- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:55:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:55:12'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2105.04387] Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2105.04387] 基于深度学习的对话系统的最新进展：系统性调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2105.04387](https://ar5iv.labs.arxiv.org/html/2105.04387)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2105.04387](https://ar5iv.labs.arxiv.org/html/2105.04387)
- en: 'Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的对话系统的最新进展：系统性调查
- en: 'Jinjie Ni Nanyang Technological University, Singapore. {jinjie001, yang0552,
    fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg Tom Young¹¹1Equal
    contribution Nanyang Technological University, Singapore. {jinjie001, yang0552,
    fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg Vlad Pandelea¹¹footnotemark:
    1 Nanyang Technological University, Singapore. {jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg,
    {vlad.pandelea, cambria}@ntu.edu.sg Fuzhao Xue Nanyang Technological University,
    Singapore. {jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg
    Erik Cambria³³3Corresponding author Nanyang Technological University, Singapore.
    {jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jinjie Ni 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Tom Young¹¹1等贡献 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Vlad Pandelea¹¹脚注标记: 1 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Fuzhao Xue 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Erik Cambria³³3通讯作者 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Dialogue systems are a popular natural language processing (NLP) task as it
    is promising in real-life applications. It is also a complicated task since many
    NLP tasks deserving study are involved. As a result, a multitude of novel works
    on this task are carried out, and most of them are deep learning based due to
    the outstanding performance. In this survey, we mainly focus on the deep learning
    based dialogue systems. We comprehensively review state-of-the-art research outcomes
    in dialogue systems and analyze them from two angles: model type and system type.
    Specifically, from the angle of model type, we discuss the principles, characteristics,
    and applications of different models that are widely used in dialogue systems.
    This will help researchers acquaint these models and see how they are applied
    in state-of-the-art frameworks, which is rather helpful when designing a new dialogue
    system. From the angle of system type, we discuss task-oriented and open-domain
    dialogue systems as two streams of research, providing insight into the hot topics
    related. Furthermore, we comprehensively review the evaluation methods and datasets
    for dialogue systems to pave the way for future research. Finally, some possible
    research trends are identified based on the recent research outcomes. To the best
    of our knowledge, this survey is the most comprehensive and up-to-date one at
    present for deep learning based dialogue systems, extensively covering the popular
    techniques¹¹1The frameworks, topics, and datasets discussed are originated from
    the extensive literature review of state-of-the-art research. We have tried our
    best to cover all but may still omit some works. Readers are welcome to provide
    suggestions regarding the omissions and mistakes in this article. We also intend
    to update this article with time as and when new approaches or definitions are
    proposed and used by the community. We speculate that this work is a good starting
    point for academics who are new to the dialogue systems or those who want to quickly
    grasp up-to-date techniques in this area.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统是一个受欢迎的自然语言处理（NLP）任务，因为它在实际应用中具有很大的潜力。由于涉及到许多值得研究的NLP任务，这也是一个复杂的任务。因此，许多关于这一任务的创新性研究被开展，其中大多数基于深度学习，因为其卓越的性能。在本次调查中，我们主要关注基于深度学习的对话系统。我们全面回顾了对话系统的最先进研究成果，并从两个角度进行分析：模型类型和系统类型。具体来说，从模型类型的角度，我们讨论了在对话系统中广泛使用的不同模型的原理、特点和应用。这将帮助研究人员了解这些模型，并看到它们如何在最先进的框架中应用，这对设计新的对话系统非常有帮助。从系统类型的角度，我们讨论了任务导向型和开放领域对话系统作为两大研究方向，提供了相关热点话题的见解。此外，我们还全面回顾了对话系统的评估方法和数据集，以为未来的研究铺平道路。最后，基于最近的研究成果，我们确定了一些可能的研究趋势。根据我们的了解，这项调查目前是针对基于深度学习的对话系统最全面、最新的调查，广泛覆盖了流行的技术¹¹1这些框架、主题和数据集源于对最先进研究的广泛文献综述。我们已尽力覆盖所有内容，但可能仍遗漏一些工作。欢迎读者对文章中的遗漏和错误提出建议。我们还打算随着时间的推移更新这篇文章，以反映社区中提出和使用的新方法或定义。我们推测，这项工作是学术界初次接触对话系统或希望迅速掌握该领域最新技术的一个良好起点。
- en: Keywords  Dialogue systems, Chatbots, Conversational AI, Natural Language Processing,
    Deep learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词  对话系统、聊天机器人、对话式人工智能、自然语言处理、深度学习
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Dialogue systems (or chatbots) are playing a bigger role in the world. People
    may still have a stereotype that chatbots are those rigid agents in their phone
    calls to a bank. However, thanks to the revival of artificial intelligence, the
    modern chatbots can converse with rich topics ranging from your birthday party
    to a speech given by Biden, and, if you want, they can even book a place for your
    party or play the speech video. At present, dialogue systems are one of the hot
    topics in NLP and are highly demanded in industry and daily life. The market size
    of chatbot is projected to grow from $2.6 billion in 2021 to $9.4 billion by 2024
    at a compound annual growth rate (CAGR) of 29.7% ²²2Statistic source: [https://markets.businessinsider.com](https://markets.businessinsider.com)
    and 80% of businesses are expected to be equipped with chatbot automation by the
    end of 2021 ³³3Statistic source: [https://outgrow.co](https://outgrow.co).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统（或聊天机器人）在全球扮演着越来越重要的角色。人们可能仍然对聊天机器人有一种刻板印象，认为它们只是那些在银行电话中出现的呆板代理。然而，得益于人工智能的复兴，现代聊天机器人可以就丰富的话题进行对话，从你的生日聚会到拜登的演讲，如果你愿意，它们甚至可以为你的聚会预订场地或播放演讲视频。目前，对话系统是自然语言处理领域的热门话题，并且在行业和日常生活中需求很高。预计聊天机器人的市场规模将从2021年的26亿美元增长到2024年的94亿美元，年复合增长率（CAGR）为29.7%²²²统计来源：[https://markets.businessinsider.com](https://markets.businessinsider.com)，而80%的企业预计将在2021年底配备聊天机器人自动化³³³统计来源：[https://outgrow.co](https://outgrow.co)。
- en: 'Dialogue systems perform chit-chat with human or serve as an assistant via
    conversations. By their applications, dialogue systems are commonly divided into
    two categories: task-oriented dialogue systems (TOD) and open-domain dialogue
    systems (OOD). Task-oriented dialogue systems solve specific problems in a certain
    domain such as movie ticket booking, restaurant table reserving, etc. Instead
    of focusing on task completion, open-domain dialogue systems aim to chat with
    users without the task and domain restrictions (Ritter et al., [2011](#bib.bib300)),
    which are usually fully data-driven. Both task-oriented and open-domain dialogue
    systems can be seen as a mapping $\varphi$ from user message $U=\{\mathrm{\mathbf{u}}^{(1)},\mathrm{\mathbf{u}}^{(2)},...,\mathrm{\mathbf{u}}^{(i)}\}$
    to agent response $R=\{\mathrm{\mathbf{r}}^{(1)},\mathrm{\mathbf{r}}^{(2)},...,\mathrm{\mathbf{r}}^{(j)}\}$:
    $R=\varphi(U)$, where $\mathrm{\mathbf{u}}^{(i)}$ and $\mathrm{\mathbf{r}}^{(j)}$
    denote the $i$th token of the user message and the $j$th token of the agent response
    respectively. In many open-domain and task-oriented dialogue systems, this mapping
    also considers a source of external knowledge/database $K$ as input: $R=\varphi(U,K)$.
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey") presents examples of inputs and
    outputs of task-oriented and open-domain dialogue systems. More specific details
    and works will be discussed in Section [3](#S3 "3 Task-oriented Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    and [4](#S4 "4 Open-Domain Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey").'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '对话系统通过对话与人类进行闲聊或作为助手服务。根据应用，对话系统通常被分为两类：任务导向型对话系统（TOD）和开放领域对话系统（OOD）。任务导向型对话系统解决特定领域中的问题，例如电影票预订、餐馆桌位预订等。开放领域对话系统则不关注任务完成，而是旨在与用户进行无任务和领域限制的对话（Ritter
    et al., [2011](#bib.bib300)），这些系统通常完全基于数据驱动。任务导向型和开放领域对话系统都可以视为从用户消息 $U=\{\mathrm{\mathbf{u}}^{(1)},\mathrm{\mathbf{u}}^{(2)},...,\mathrm{\mathbf{u}}^{(i)}\}$
    到代理响应 $R=\{\mathrm{\mathbf{r}}^{(1)},\mathrm{\mathbf{r}}^{(2)},...,\mathrm{\mathbf{r}}^{(j)}\}$
    的映射：$R=\varphi(U)$，其中 $\mathrm{\mathbf{u}}^{(i)}$ 和 $\mathrm{\mathbf{r}}^{(j)}$
    分别表示用户消息中的第 $i$ 个标记和代理响应中的第 $j$ 个标记。在许多开放领域和任务导向型对话系统中，这种映射还考虑了外部知识/数据库 $K$ 作为输入：$R=\varphi(U,K)$。表
    [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") 展示了任务导向型和开放领域对话系统的输入和输出示例。更多具体细节和相关工作将在第
    [3](#S3 "3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") 和 [4](#S4 "4 Open-Domain Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    节中讨论。'
- en: 'Table 1: Examples of inputs and outputs of task-oriented and open-domain dialogue
    systems in datasets. Some datasets provide external knowledge annotations for
    each dialogue pair, e.g., in task-oriented dialogue systems, the external knowledge
    can be retrieved from restaurant databases; in open-domain dialogue systems, it
    can be retrieved from commonsense knowledge graphs (KG).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：数据集中任务导向和开放领域对话系统的输入输出示例。一些数据集为每个对话对提供外部知识注释，例如，在任务导向对话系统中，外部知识可以从餐馆数据库中检索；在开放领域对话系统中，它可以从常识知识图谱（KG）中检索。
- en: '| Category | User message ($U$) | Agent response ($R$) | External Knowledge
    ($K$) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 用户消息 ($U$) | 代理响应 ($R$) | 外部知识 ($K$) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Task-oriented | I need to find a nice restaurant in Madrid that serves expensive
    Thai food. | There is a restaurant called Bangkok City locating at 9 Red Ave.
    | restaurant database |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 任务导向 | 我需要找到一家在马德里提供昂贵泰国菜的好餐馆。 | 有一家叫做 Bangkok City 的餐馆，位于 9 Red Ave。 | 餐馆数据库
    |'
- en: '| Open-domain | I love the grilled fish so much! | Yeah. it’s a famous Chinese
    dish. | commonsense KG |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 开放领域 | 我非常喜欢烤鱼！ | 是的，这是道著名的中式菜肴。 | 常识 KG |'
- en: 'Traditional task-oriented dialogue systems are organized in a pipeline structure
    and consist of four functional modules: Natural Language Understanding, Dialogue
    State Tracking, Policy Learning, and Natural Language Generation, which will be
    discussed in detail in Section [3](#S3 "3 Task-oriented Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey"). Many
    state-of-the-art works design end-to-end task-oriented dialogue systems to achieve
    better optimization compared with pipeline methods. Open-domain dialogue systems
    are generally divided into three categories: generative systems, retrieval-based
    systems, and ensemble systems. Generative systems apply sequence-to-sequence models
    (see Section [2.2.5](#S2.SS2.SSS5 "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder
    Models) ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence Models
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey")) to map the user message and dialogue
    history into a response sequence that may not appear in the training corpus. By
    contrast, retrieval-based systems try to select a pre-existing response from a
    certain response set. Ensemble systems combine generative methods and retrieval-based
    methods in two ways: retrieved responses can be compared with generated responses
    to choose the best among them; generative models can also be used to refine the
    retrieved responses (Zhu et al., [2018](#bib.bib466); Song et al., [2016](#bib.bib337);
    Qiu et al., [2017](#bib.bib284); Serban et al., [2017b](#bib.bib314)). Generative
    systems can produce flexible and dialogue context-related responses while sometimes
    they lack coherence ⁴⁴4The quality of being logical and consistent not only between
    words/subwords but also between responses of different timesteps. and tend to
    make dull responses (Serban et al., [2016](#bib.bib312); Vinyals and Le, [2015](#bib.bib379);
    Sordoni et al., [2015b](#bib.bib340)). Retrieval-based systems select responses
    from human response sets and thus are able to achieve better coherence in surface-level
    language. However, retrieval systems are restricted by the finiteness of the response
    sets and sometimes the responses retrieved show a weak correlation with the dialogue
    context (Zhu et al., [2018](#bib.bib466)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的任务导向对话系统采用流水线结构，包含四个功能模块：自然语言理解、对话状态跟踪、策略学习和自然语言生成，这些将在第[3](#S3 "3 Task-oriented
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey")节中详细讨论。许多先进的工作设计了端到端的任务导向对话系统，以实现比流水线方法更好的优化。开放域对话系统通常分为三类：生成系统、基于检索的系统和集成系统。生成系统应用序列到序列模型（参见第[2.2.5](#S2.SS2.SSS5
    "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder Models) ‣ 2.2 Recurrent
    Neural Networks and Vanilla Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey")节）将用户消息和对话历史映射到可能在训练语料库中未出现的响应序列。相比之下，基于检索的系统尝试从某个响应集选择一个预存在的响应。集成系统以两种方式结合生成方法和基于检索的方法：检索到的响应可以与生成的响应进行比较，以选择最佳响应；生成模型也可以用于完善检索到的响应（Zhu
    et al., [2018](#bib.bib466); Song et al., [2016](#bib.bib337); Qiu et al., [2017](#bib.bib284);
    Serban et al., [2017b](#bib.bib314)）。生成系统可以生成灵活且与对话上下文相关的响应，但有时缺乏连贯性⁴⁴4即不仅在词语/子词之间而且在不同时间步的响应之间也要逻辑一致。并且往往会产生枯燥的响应（Serban
    et al., [2016](#bib.bib312); Vinyals and Le, [2015](#bib.bib379); Sordoni et al.,
    [2015b](#bib.bib340)）。基于检索的系统从人工响应集中选择响应，因此在表层语言上能够实现更好的连贯性。然而，检索系统受到响应集有限性的限制，有时检索到的响应与对话上下文的相关性较弱（Zhu
    et al., [2018](#bib.bib466)）。'
- en: '![Refer to caption](img/183803d09e02867efd0cf03a24d4a201.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/183803d09e02867efd0cf03a24d4a201.png)'
- en: 'Figure 1: The overall diagram of this article'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本文的总体示意图
- en: For dialogue systems, existing surveys (Arora et al., [2013](#bib.bib7); Wang
    and Yuan, [2016](#bib.bib389); Mallios and Bourbakis, [2016](#bib.bib240); Chen
    et al., [2017a](#bib.bib39); Gao et al., [2018](#bib.bib98)) are either outdated
    or not comprehensive. Some definitions in these papers are no longer being used
    at present, and a lot of new works and topics are not covered. In addition, most
    of them lack a multi-angle analysis. Thus, in this survey, we comprehensively
    review high-quality works in recent years with a focus on deep learning-based
    approaches and provide insight into state-of-the-art research from both model
    angle and system angle. Moreover, this survey updates the definitions/names according
    to state-of-the-art research. E.g., we name "open-domain dialogue systems" instead
    of "chit-chat dialogue systems" because most of the articles (roughly 70% according
    to our survey) name them as the prior one. We also extensively cover the diverse
    hot topics in dialogue systems and extend some new topics that are popular in
    current research community (such as Domain Adaptation, Dialogue State Tracking
    Efficiency, End-to-end methods for task-oriented dialogue systems; Controllable
    Generation, Interactive Training, and Visual Dialogue for open-domain dialogue
    systems).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对话系统，现有的调查（Arora et al., [2013](#bib.bib7); Wang and Yuan, [2016](#bib.bib389);
    Mallios and Bourbakis, [2016](#bib.bib240); Chen et al., [2017a](#bib.bib39);
    Gao et al., [2018](#bib.bib98)）要么过时，要么不全面。这些论文中的一些定义目前已经不再使用，许多新的工作和话题没有涵盖。此外，它们大多数缺乏多角度分析。因此，在这项调查中，我们全面回顾了近年来的高质量研究，重点关注基于深度学习的方法，并从模型角度和系统角度提供了对最先进研究的见解。此外，本调查根据最先进的研究更新了定义/名称。例如，我们使用“开放领域对话系统”而不是“闲聊对话系统”，因为大多数文章（根据我们的调查，大约70%）将其称为前者。我们还广泛涵盖了对话系统中的各种热门话题，并扩展了一些在当前研究社区中流行的新话题（例如，领域适应、对话状态跟踪效率、任务导向对话系统的端到端方法；可控生成、互动训练和开放领域对话系统的视觉对话）。
- en: 'Traditional dialogue systems are mostly rule-based (Arora et al., [2013](#bib.bib7))
    and non-neural machine learning based systems. Rule-based systems are easy to
    implement and can respond naturally, which contributed to their popularity in
    earlier industry products. However, the dialogue flows of these systems are predetermined,
    which keeps the applications of the dialogue systems within certain scenarios.
    Non-neural machine learning based systems usually perform template filling to
    manage certain tasks. These systems are more flexible compared with rule-based
    systems because the dialogue flows are not predetermined. However, they cannot
    achieve high F1 scores (Powers, [2020](#bib.bib276)) in template filling⁵⁵5Template
    filling is an efficient approach to extract and structure complex information
    from text to fill in a pre-defined template. They are mostly used in task-oriented
    dialogue systems. and are also restricted in application scenarios and response
    diversity because of the fixed templates. Most if not all state-of-the-art dialogue
    systems are deep learning-based systems (neural systems). The rapid growth of
    deep learning improves the performance of dialogue systems (Chen et al., [2017a](#bib.bib39)).
    Deep learning can be viewed as representation learning with multilayer neural
    networks. Deep learning architectures are widely used in dialogue systems and
    their subtasks. Section [2](#S2 "2 Neural Models in Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey") discusses
    various popular deep learning architectures.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的对话系统大多数是基于规则的（Arora et al., [2013](#bib.bib7)）和非神经网络的机器学习系统。基于规则的系统实现简单，响应自然，这使得它们在早期行业产品中非常受欢迎。然而，这些系统的对话流程是预定的，这限制了对话系统的应用场景。非神经网络的机器学习系统通常进行模板填充来处理某些任务。与基于规则的系统相比，这些系统更具灵活性，因为对话流程不是预定的。然而，它们在模板填充中无法实现高F1分数（Powers,
    [2020](#bib.bib276)）⁵⁵5模板填充是一种高效的方法，用于从文本中提取和结构化复杂信息，以填充预定义的模板。它们主要用于任务导向的对话系统，并且由于固定模板的原因，也受到应用场景和响应多样性的限制。目前大多数（如果不是全部的话）最先进的对话系统都是基于深度学习的系统（神经系统）。深度学习的快速发展提升了对话系统的性能（Chen
    et al., [2017a](#bib.bib39)）。深度学习可以被视为具有多层神经网络的表示学习。深度学习架构被广泛应用于对话系统及其子任务。第[2](#S2
    "2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey")节讨论了各种流行的深度学习架构。'
- en: Apart from dialogue systems, there are also many dialogue-related tasks in NLP,
    including but not limited to question answering, reading comprehension, dialogue
    disentanglement, visual dialogue, visual question answering, dialogue reasoning,
    conversational semantic parsing, dialogue relation extraction, dialogue sentiment
    analysis, hate speech detection, MISC detection, etc. In this survey, we also
    touch on some works tackling these dialogue-related tasks, since the design of
    dialogue systems can benefit from advances in these related areas.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对话系统，NLP中还有许多与对话相关的任务，包括但不限于问答、阅读理解、对话解缠、视觉对话、视觉问答、对话推理、对话语义解析、对话关系提取、对话情感分析、仇恨言论检测、MISC检测等。在本调查中，我们也涉及了一些处理这些对话相关任务的研究，因为对话系统的设计可以从这些相关领域的进展中受益。
- en: 'We produced a diagram for this article to help readers familiarize the overall
    structure (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Recent Advances in Deep
    Learning Based Dialogue Systems: A Systematic Survey")). In this survey, Section [1](#S1
    "1 Introduction ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey") briefly introduces dialogue systems and deep learning; Section [2](#S2
    "2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") discusses the neural models popular in
    modern dialogue systems and the related work; Section [3](#S3 "3 Task-oriented
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey") introduces the principles and related work of task-oriented
    dialogue systems and discusses the research challenges and hot topics; Section [4](#S4
    "4 Open-Domain Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") briefly introduces the three kinds of systems and
    then focuses on hot topics in open-domain dialogue systems; Section [5](#S5 "5
    Evaluation Approaches ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey") reviews the main evaluation methods for dialogue systems;
    Section [6](#S6 "6 Datasets ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") comprehensively summarizes the datasets commonly
    used for dialogue systems; finally, Section [7](#S7 "7 Conclusions and Trends
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    concludes the paper and provides some insight on research trends.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本文制作了一张图表，以帮助读者熟悉整体结构（图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 基于深度学习的对话系统的最新进展：系统性调查")）。在本调查中，第一部分 [1](#S1
    "1 引言 ‣ 基于深度学习的对话系统的最新进展：系统性调查") 简要介绍了对话系统和深度学习；第二部分 [2](#S2 "2 神经模型在对话系统中的应用
    ‣ 基于深度学习的对话系统的最新进展：系统性调查") 讨论了现代对话系统中流行的神经模型及相关工作；第三部分 [3](#S3 "3 面向任务的对话系统 ‣
    基于深度学习的对话系统的最新进展：系统性调查") 介绍了面向任务的对话系统的原理和相关工作，并讨论了研究挑战和热点话题；第四部分 [4](#S4 "4 开放领域对话系统
    ‣ 基于深度学习的对话系统的最新进展：系统性调查") 简要介绍了三种系统，并重点关注开放领域对话系统中的热点话题；第五部分 [5](#S5 "5 评价方法
    ‣ 基于深度学习的对话系统的最新进展：系统性调查") 回顾了对话系统的主要评价方法；第六部分 [6](#S6 "6 数据集 ‣ 基于深度学习的对话系统的最新进展：系统性调查")
    综合总结了对话系统中常用的数据集；最后，第七部分 [7](#S7 "7 结论与趋势 ‣ 基于深度学习的对话系统的最新进展：系统性调查") 总结了论文并提供了一些研究趋势的见解。
- en: 2 Neural Models in Dialogue Systems
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 神经模型在对话系统中的应用
- en: 'In this section, we introduce neural models that are popular in state-of-the-art
    dialogue systems and related subtasks. We also discuss the applications of these
    models or their variants in modern dialogue systems research to provide readers
    with a picture from the model’s perspective. This will help researchers acquaint
    these models and see how they are applied in state-of-the-art frameworks, which
    is rather helpful when designing a new dialogue system. The models discussed include:
    Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Vanilla
    Sequence-to-sequence Models, Hierarchical Recurrent Encoder-Decoder (HRED), Memory
    Networks, Attention Networks, Transformer, Pointer Net and CopyNet, Deep Reinforcement
    Learning models, Generative Adversarial Networks (GANs), Knowledge Graph Augmented
    Neural Networks. We start from some classical models (e.g., CNNs and RNNs), and
    readers who are familiar with their principles and corresponding applications
    in dialogue systems can choose to read selectively.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了在最先进的对话系统及相关子任务中流行的神经模型。我们还讨论了这些模型或其变体在现代对话系统研究中的应用，以便从模型的角度为读者提供一个全景。这将帮助研究人员熟悉这些模型，并了解它们在最先进框架中的应用，这对于设计新的对话系统非常有帮助。讨论的模型包括：卷积神经网络（CNNs）、递归神经网络（RNNs）、基础序列到序列模型、层次递归编码器-解码器（HRED）、记忆网络、注意力网络、Transformer、Pointer
    Net 和 CopyNet、深度强化学习模型、生成对抗网络（GANs）、知识图谱增强神经网络。我们从一些经典模型（例如，CNNs 和 RNNs）开始，熟悉它们的原理和在对话系统中的应用的读者可以选择性阅读。
- en: 2.1 Convolutional Neural Networks
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 卷积神经网络
- en: 'Deep neural networks have been considered as one of the most powerful models.
    ‘Deep’ refers to the fact that they are multilayer, which extracts features by
    stacking feed-forward layers. Feed-forward layers can be defined as: $y=\sigma(Wx+b)$.
    Where the $\sigma$ is an activation function; $W$ and $b$ are trainable parameters.
    The feed-forward layers are powerful due to the activation function, which makes
    the otherwise linear operation, non-linear. Whereas there exist some problems
    when using feed-forward layers. Firstly, the operations of feed-forward layers
    or multilayer neural networks are just template matching, where they do not consider
    the specific structure of data. Furthermore, the fully connected mechanism of
    traditional multilayer neural networks causes an explosion in the number of parameters
    and thus leads to generalization problems. LeCun et al. ([1998](#bib.bib182))
    proposed LeNet-5, an early CNN. The invention of CNNs mitigates the above problems
    to some extent.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络被认为是最强大的模型之一。“深度”指的是它们是多层的，通过堆叠前馈层提取特征。前馈层可以定义为：$y=\sigma(Wx+b)$。其中，$\sigma$
    是激活函数；$W$ 和 $b$ 是可训练参数。前馈层因激活函数而强大，它使得本来线性的操作变为非线性。然而，使用前馈层时存在一些问题。首先，前馈层或多层神经网络的操作只是模板匹配，它们不考虑数据的具体结构。此外，传统多层神经网络的全连接机制导致参数数量激增，从而导致泛化问题。LeCun
    等人 ([1998](#bib.bib182)) 提出了早期的 CNN——LeNet-5。CNN 的发明在一定程度上缓解了上述问题。
- en: '![Refer to caption](img/0bc26bb5dea6d2ef981db472f8301895.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0bc26bb5dea6d2ef981db472f8301895.png)'
- en: 'Figure 2: A CNN architecture for text classification (Zhang and Wallace, [2017](#bib.bib450))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于文本分类的 CNN 结构（Zhang 和 Wallace，[2017](#bib.bib450)）
- en: 'CNNs (Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Convolutional Neural Networks ‣ 2 Neural
    Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")) usually consist of convolutional layers, pooling layers
    and feed-forward layers. Convolutional layers apply convolution kernels to perform
    the convolution operation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（图 [2](#S2.F2 "图 2 ‣ 2.1 卷积神经网络 ‣ 2 神经模型在对话系统中的应用 ‣ 基于深度学习的对话系统的最新进展：系统性综述")）通常由卷积层、池化层和前馈层组成。卷积层应用卷积核进行卷积操作：
- en: '|  | $G(m,n)=(f*h)(m,n)=\sum_{j}\sum_{k}h(j,k)f(m-j,n-k)$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $G(m,n)=(f*h)(m,n)=\sum_{j}\sum_{k}h(j,k)f(m-j,n-k)$ |  | (1) |'
- en: Where $m$ and $n$ are respectively the indexes of rows and columns of the result
    matrix. $f$ denotes the input matrix and $h$ denotes the convolutional kernel.
    The pooling layers perform down-sampling on the result of convolutional layers
    to get a higher level of features and the feed-forward layers map them into a
    probability distribution to predict class scores.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 和 $n$ 分别是结果矩阵的行和列索引。$f$ 表示输入矩阵，$h$ 表示卷积核。池化层对卷积层的结果进行降采样，以获得更高层次的特征，前馈层将这些特征映射到概率分布中以预测类别分数。
- en: A sliding window feature enables convolution layers to capture local features
    and the pooling layers can produce hierarchical features. These two mechanisms
    give CNNs the local perception and global perception ability, helping to capture
    some specific inner structures of data. The parameter sharing mechanism eases
    the parameter explosion problem and overfitting problem because the reduction
    of trainable parameters leads to less model complexity, improving the generalization
    ability.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口特性使卷积层能够捕捉局部特征，而池化层可以生成层次化特征。这两个机制赋予了卷积神经网络（CNNs）局部感知和全局感知的能力，有助于捕捉数据的一些特定内部结构。参数共享机制缓解了参数爆炸问题和过拟合问题，因为可训练参数的减少导致模型复杂性降低，从而提高了模型的泛化能力。
- en: Due to these good properties, CNNs have been widely applied in many works. Among
    them, the Computer Vision tasks benefit the most for that the Spatio-temporal
    data structures of images or videos are perfectly captured by CNNs. For more detailed
    mechanism illustrations and other variants of CNNs, readers can refer to these
    representative algorithm papers or surveys: (Krizhevsky et al., [2012](#bib.bib173);
    Zeiler and Fergus, [2014](#bib.bib444); Simonyan and Zisserman, [2014](#bib.bib328);
    Szegedy et al., [2015](#bib.bib355); He et al., [2016](#bib.bib126); Aloysius
    and Geetha, [2017](#bib.bib5); Rawat and Wang, [2017](#bib.bib295)). In this survey,
    we focus on dialogue systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些优良特性，CNN已广泛应用于许多工作中。其中，计算机视觉任务受益最大，因为CNN能够完美捕捉图像或视频的时空数据结构。有关更详细的机制说明和CNN的其他变体，读者可以参考以下具有代表性的算法论文或综述：
    (Krizhevsky et al., [2012](#bib.bib173); Zeiler and Fergus, [2014](#bib.bib444);
    Simonyan and Zisserman, [2014](#bib.bib328); Szegedy et al., [2015](#bib.bib355);
    He et al., [2016](#bib.bib126); Aloysius and Geetha, [2017](#bib.bib5); Rawat
    and Wang, [2017](#bib.bib295))。在本综述中，我们专注于对话系统。
- en: Recent years have seen a dramatic increase in applications of CNNs in NLP. Many
    tasks take words as basic units. However, phrases, sentences, or even paragraphs
    are also useful to semantic representations. As a result, CNNs are an ideal tool
    for the hierarchical modeling of language (Conneau et al., [2016](#bib.bib57)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，CNN在自然语言处理（NLP）中的应用显著增加。许多任务以单词作为基本单元。然而，短语、句子甚至段落对语义表示也是有用的。因此，CNN是语言的层次建模的理想工具（Conneau
    et al., [2016](#bib.bib57)）。
- en: CNNs are good textual feature extractors, but they may not be ideal sequential
    encoders. Some dialogue systems (Qiu et al., [2019](#bib.bib282); Bi et al., [2019](#bib.bib22);
    Ma et al., [2020a](#bib.bib235)) directly used CNNs as the encoder of utterances
    or knowledge, but most of the state-of-the-art dialogue systems such as Feng et al.
    ([2019](#bib.bib86)); Wu et al. ([2016](#bib.bib418)); Tao et al. ([2019](#bib.bib364));
    Wang et al. ([2019b](#bib.bib390)); Chauhan et al. ([2019](#bib.bib38)); Feldman
    and El-Yaniv ([2019](#bib.bib85)); Chen et al. ([2019c](#bib.bib45)); Lu et al.
    ([2019b](#bib.bib231)) and Coope et al. ([2020](#bib.bib58)) chose to use CNNs
    as a hierarchical feature extractor after encoding the text information, instead
    of directly applying them as encoders. This is due to the fixed input length and
    limited convolution span of CNNs. Generally, there are two main situations where
    CNNs are used to process encoded information in dialogue systems. The first situation
    is applying CNNs to extract features directly based on the feature vectors from
    the encoder (Wang et al., [2019b](#bib.bib390); Chauhan et al., [2019](#bib.bib38);
    Feldman and El-Yaniv, [2019](#bib.bib85); Chen et al., [2019c](#bib.bib45)) and Coope
    et al. ([2020](#bib.bib58)). Within the works above, Feldman and El-Yaniv ([2019](#bib.bib85))
    extracted features from character-level embeddings, illustrating the hierarchical
    extraction capability of CNNs. Another situation in which CNNs are used is extracting
    feature maps in response retrieval tasks. Some works built retrieval-based dialogue
    systems (Wu et al., [2016](#bib.bib418); Feng et al., [2019](#bib.bib86); Tao
    et al., [2019](#bib.bib364); Lu et al., [2019b](#bib.bib231)). They used separate
    encoders to encode dialogue context and candidate responses and then used a CNN
    as an extractor of the similarity matrix calculated from the encoded dialogue
    context and candidate responses. Their experiments showed that this method can
    achieve good performance in response retrieval tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 是很好的文本特征提取器，但可能不是理想的序列编码器。一些对话系统（Qiu et al., [2019](#bib.bib282); Bi et
    al., [2019](#bib.bib22); Ma et al., [2020a](#bib.bib235)）直接使用 CNNs 作为话语或知识的编码器，但大多数最先进的对话系统，如
    Feng et al. ([2019](#bib.bib86)); Wu et al. ([2016](#bib.bib418)); Tao et al.
    ([2019](#bib.bib364)); Wang et al. ([2019b](#bib.bib390)); Chauhan et al. ([2019](#bib.bib38));
    Feldman 和 El-Yaniv ([2019](#bib.bib85)); Chen et al. ([2019c](#bib.bib45)); Lu
    et al. ([2019b](#bib.bib231)) 和 Coope et al. ([2020](#bib.bib58)) 选择在编码文本信息后使用
    CNNs 作为层次特征提取器，而不是直接将其作为编码器。这是由于 CNNs 固定的输入长度和有限的卷积范围。一般来说，有两种主要情况 CNNs 被用来处理对话系统中的编码信息。第一种情况是基于编码器的特征向量直接应用
    CNNs 提取特征（Wang et al., [2019b](#bib.bib390); Chauhan et al., [2019](#bib.bib38);
    Feldman 和 El-Yaniv, [2019](#bib.bib85); Chen et al., [2019c](#bib.bib45)）和 Coope
    et al. ([2020](#bib.bib58))。在上述工作中，Feldman 和 El-Yaniv ([2019](#bib.bib85)) 从字符级嵌入中提取特征，展示了
    CNNs 的层次提取能力。CNNs 被使用的另一种情况是在响应检索任务中提取特征图。一些工作构建了基于检索的对话系统（Wu et al., [2016](#bib.bib418);
    Feng et al., [2019](#bib.bib86); Tao et al., [2019](#bib.bib364); Lu et al., [2019b](#bib.bib231)）。他们使用独立的编码器对对话上下文和候选响应进行编码，然后使用
    CNN 作为从编码的对话上下文和候选响应中计算的相似度矩阵的提取器。他们的实验表明，这种方法在响应检索任务中可以取得良好的性能。
- en: The main reason why more recent works do not choose CNNs as dialogue encoders
    is that they fail to extract the information across temporal sequence steps continuously
    and flexibly (Krizhevsky et al., [2012](#bib.bib173)). Some models introduced
    later do not process data points independently, which are desirable models for
    encoders.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的工作不选择 CNNs 作为对话编码器的主要原因是它们无法持续且灵活地提取跨时间序列步的信息（Krizhevsky et al., [2012](#bib.bib173)）。一些后续引入的模型不会独立处理数据点，这些模型是编码器所期望的。
- en: 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 循环神经网络和普通序列到序列模型
- en: NLP tasks including dialogue-related tasks try to process and analyze sequential
    language data points. Even though standard neural networks, as well as CNNs, are
    powerful learning models, they have two main limitations (Lipton et al., [2015](#bib.bib218)).
    One is that they assume the data points are independent of each other. While it
    is reasonable if the data points are produced independently, essential information
    can be missed when processing interrelated data points (e.g., text, audio, video).
    Additionally, their inputs are usually of fixed length, which is a limitation
    when processing sequential data varying in length. Thus, a sequential model being
    able to represent the sequential information flow is desirable.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 任务，包括对话相关任务，试图处理和分析顺序语言数据点。尽管标准神经网络以及 CNNs 是强大的学习模型，但它们存在两个主要的限制（Lipton
    et al.，[2015](#bib.bib218)）。其一是它们假设数据点彼此独立。如果数据点是独立产生的，这种假设是合理的，但在处理互相关联的数据点（例如文本、音频、视频）时，可能会遗漏重要信息。此外，它们的输入通常是固定长度的，这在处理长度可变的顺序数据时是一个限制。因此，能够表示顺序信息流的顺序模型是理想的。
- en: Markov models like Hidden Markov Models (HMMs) are traditional sequential models,
    but due to the time complexity of the inference algorithm (Viterbi, [1967](#bib.bib380))
    and because the size of transition matrix grows significantly with the increase
    of the discrete state space, in practice they are not applicable in dealing with
    problems involving large possible hidden states. The property that the hidden
    states of Markov models are only affected by the immediate hidden states further
    limits the power of this model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫模型如隐马尔可夫模型（HMMs）是传统的顺序模型，但由于推理算法的时间复杂度（Viterbi，[1967](#bib.bib380)）以及转移矩阵的大小随着离散状态空间的增加而显著增长，因此在处理涉及大量可能隐藏状态的问题时，实际上它们并不适用。马尔可夫模型的隐藏状态仅受当前隐藏状态的影响，这进一步限制了该模型的能力。
- en: RNN models are not proposed recently, but they greatly solve the above problems
    and some variants can amazingly achieve state-of-the-art performance in dialogue-related
    tasks as well as many other NLP tasks. The inductive bias of recurrent models
    is non-replaceable in many scenarios, and many up-to-date models incorporate the
    recurrence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 模型并非最近提出，但它们极大地解决了上述问题，一些变体在对话相关任务以及许多其他 NLP 任务中都能惊人地实现最先进的性能。递归模型的归纳偏差在许多场景中是不可替代的，许多最新的模型都融入了递归机制。
- en: 2.2.1 Jordan-Type and Elman-Type RNNs
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 Jordan 类型和 Elman 类型 RNN
- en: In 1982, Hopfield introduced an early family of RNNs to solve pattern recognition
    tasks (Hopfield, [1982](#bib.bib140)). Jordan ([1986](#bib.bib159)) and Elman
    ([1990](#bib.bib80)) introduced two kinds of RNN architectures respectively. Generally,
    modern RNNs can be classified into Jordan-type RNNs and Elman-type RNNs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1982 年，Hopfield 引入了早期的 RNN 家族来解决模式识别任务（Hopfield，[1982](#bib.bib140)）。Jordan（[1986](#bib.bib159)）和
    Elman（[1990](#bib.bib80)）分别介绍了两种 RNN 架构。一般来说，现代 RNN 可以分为 Jordan 类型 RNN 和 Elman
    类型 RNN。
- en: '![Refer to caption](img/129d8bcf18fd55212511419a63d52492.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/129d8bcf18fd55212511419a63d52492.png)'
- en: (a) Jordan-type RNNs
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Jordan 类型 RNN
- en: '![Refer to caption](img/8693660674db8944063456afc2f3cf42.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8693660674db8944063456afc2f3cf42.png)'
- en: (b) Elman-type RNNs
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Elman 类型 RNN
- en: 'Figure 3: Graphical models of two basic types of RNNs'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：两种基本 RNN 类型的图示模型
- en: 'The Jordan-type RNNs are shown in Figure [3(a)](#S2.F3.sf1 "In Figure 3 ‣ 2.2.1
    Jordan-Type and Elman-Type RNNs ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence
    Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey"). $x_{t}$, $h_{t}$, and $y_{t}$ are
    the inputs, hidden state, and output of time step $t$, respectively. $W_{h}$,
    $W_{y}$ and $U_{h}$ are weight matrixes. Each update of hidden state is decided
    by the current input and the output of last time step while each output is decided
    by current hidden state. Thus the hidden state and output of time step $t$ can
    be calculated as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Jordan 类型 RNN 如图 [3(a)](#S2.F3.sf1 "在图 3 ‣ 2.2.1 Jordan 类型和 Elman 类型 RNN ‣ 2.2
    递归神经网络和基础序列到序列模型 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统性综述") 所示。$x_{t}$、$h_{t}$
    和 $y_{t}$ 分别是时间步 $t$ 的输入、隐藏状态和输出。$W_{h}$、$W_{y}$ 和 $U_{h}$ 是权重矩阵。每次隐藏状态的更新由当前输入和上一个时间步的输出决定，而每个输出由当前隐藏状态决定。因此，时间步
    $t$ 的隐藏状态和输出可以计算为：
- en: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})$ |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})$ |  | (2) |'
- en: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (3) |'
- en: Where $b_{h}$ and $b_{y}$ are biases. $\sigma_{h}$ and $\sigma_{y}$ are activation
    functions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{h}$ 和 $b_{y}$ 是偏置。$\sigma_{h}$ 和 $\sigma_{y}$ 是激活函数。
- en: 'The Elman-type RNNs are shown in Figure [3(b)](#S2.F3.sf2 "In Figure 3 ‣ 2.2.1
    Jordan-Type and Elman-Type RNNs ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence
    Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey"). The difference is that each hidden
    state is decided by the current input and the hidden state of last time step.
    Thus the hidden state and output of time step $t$ can be calculated as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Elman 类型的 RNN 如图 [3(b)](#S2.F3.sf2 "在图 3 ‣ 2.2.1 Jordan 类型和 Elman 类型 RNN ‣ 2.2
    递归神经网络和 Vanilla 序列到序列模型 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统中的最新进展：系统综述") 所示。不同之处在于每个隐藏状态由当前输入和上一个时间步的隐藏状态决定。因此，时间步
    $t$ 的隐藏状态和输出可以计算为：
- en: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})$ |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})$ |  | (4) |'
- en: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (5) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (5) |'
- en: Simple RNNs can model long-term dependencies theoretically. But in practical
    training, long-range dependencies are difficult to learn (Bengio et al., [1994](#bib.bib20);
    Hochreiter et al., [2001](#bib.bib138)). When backpropagating errors over many
    time steps, simple RNNs suffer from problems known as gradient vanishing and gradient
    explosion (Hochreiter and Schmidhuber, [1997](#bib.bib137)). Some solutions were
    proposed to solve these problems (Williams and Zipser, [1989](#bib.bib410); Pascanu
    et al., [2013](#bib.bib271)), which led to the inventions of some variants of
    traditional recurrent networks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 RNN 理论上可以建模长期依赖关系。但在实际训练中，长范围依赖关系很难学习（Bengio et al., [1994](#bib.bib20);
    Hochreiter et al., [2001](#bib.bib138)）。在许多时间步上反向传播误差时，简单的 RNN 会遇到梯度消失和梯度爆炸的问题（Hochreiter
    和 Schmidhuber, [1997](#bib.bib137)）。一些解决方案被提出以解决这些问题（Williams 和 Zipser, [1989](#bib.bib410);
    Pascanu et al., [2013](#bib.bib271)），这导致了传统递归网络的一些变体的发明。
- en: 2.2.2 LSTM
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 LSTM
- en: 'Hochreiter and Schmidhuber ([1997](#bib.bib137)) introduced gate mechanisms
    in LSTM mainly to address the gradient vanishing problem. Input gate, forget gate
    and output gate were introduced to decide how much information from new inputs
    and past memories should be reserved. The model can be described by the following
    equations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber ([1997](#bib.bib137)) 引入了 LSTM 中的门控机制，主要是为了应对梯度消失问题。引入了输入门、遗忘门和输出门来决定应保留多少来自新输入和过去记忆的信息。该模型可以用以下方程描述：
- en: '|  | $\hat{h}^{(t)}=tanh\left(W^{\hat{h}x}x^{(t)}+W^{\hat{h}h}h^{(t-1)}+b_{\hat{h}}\right)$
    |  | (6) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{h}^{(t)}=tanh\left(W^{\hat{h}x}x^{(t)}+W^{\hat{h}h}h^{(t-1)}+b_{\hat{h}}\right)$
    |  | (6) |'
- en: '|  | $i^{(t)}=\sigma\left(W^{ix}x^{(t)}+W^{ih}h^{(t-1)}+b_{i}\right)$ |  |
    (7) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $i^{(t)}=\sigma\left(W^{ix}x^{(t)}+W^{ih}h^{(t-1)}+b_{i}\right)$ |  |
    (7) |'
- en: '|  | $f^{(t)}=\sigma\left(W^{fx}x^{(t)}+W^{fh}h^{(t-1)}+b_{f}\right)$ |  |
    (8) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $f^{(t)}=\sigma\left(W^{fx}x^{(t)}+W^{fh}h^{(t-1)}+b_{f}\right)$ |  |
    (8) |'
- en: '|  | $o^{(t)}=\sigma\left(W^{ox}x^{(t)}+W^{oh}h^{(t-1)}+b_{o}\right)$ |  |
    (9) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $o^{(t)}=\sigma\left(W^{ox}x^{(t)}+W^{oh}h^{(t-1)}+b_{o}\right)$ |  |
    (9) |'
- en: '|  | $s^{(t)}=\hat{h}^{(t)}\odot i^{(t)}+s^{(t-1)}\odot f^{(t)}$ |  | (10)
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $s^{(t)}=\hat{h}^{(t)}\odot i^{(t)}+s^{(t-1)}\odot f^{(t)}$ |  | (10)
    |'
- en: '|  | $h^{(t)}=tanh(s^{(t)})\odot o^{(t)}$ |  | (11) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{(t)}=tanh(s^{(t)})\odot o^{(t)}$ |  | (11) |'
- en: Where $t$ represents time step $t$. $i$, $f$ and $o$ are gates, denoting input
    gate, forget gate and output gate respectively. $x$, $\hat{h}$, $s$ and $h$ are
    input, short-term memory, long-term memory and output respectively. $b$ is bias
    and $W$ is weight matrix. $\odot$ denotes element-wise multiplication.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 代表时间步 $t$。$i$、$f$ 和 $o$ 是门控，分别表示输入门、遗忘门和输出门。$x$、$\hat{h}$、$s$ 和 $h$ 分别是输入、短期记忆、长期记忆和输出。$b$
    是偏置，$W$ 是权重矩阵。$\odot$ 表示逐元素乘法。
- en: The intuition of the term “Long Short-Term Memory" is that the proposed model
    applies both long-term and short-term memory vectors to encode the sequential
    data, and uses gate mechanisms to control the information flow. The performance
    of LSTM is impressive since that it achieved state-of-the-art results in many
    NLP tasks as a backbone model although this model was proposed in 1997.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: “长短期记忆”这一术语的直观意义在于，提出的模型应用了长期和短期记忆向量来编码序列数据，并使用门控机制来控制信息流。LSTM 的表现令人印象深刻，因为它在许多
    NLP 任务中作为骨干模型取得了最先进的结果，尽管该模型是在 1997 年提出的。
- en: 2.2.3 GRU
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 GRU
- en: 'Inspired by the gating mechanism, Cho et al. ([2014b](#bib.bib52)) proposed
    Gated Recurrent Unit (GRU), which can be modeled by the equations:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 受到门控机制的启发，Cho et al. ([2014b](#bib.bib52)) 提出了门控递归单元（GRU），其可以通过以下方程建模：
- en: '|  | $z^{(t)}=\sigma\left(W^{z}x^{(t)}+U^{z}h^{(t-1)}+b_{z}\right)$ |  | (12)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{(t)}=\sigma\left(W^{z}x^{(t)}+U^{z}h^{(t-1)}+b_{z}\right)$ |  | (12)
    |'
- en: '|  | $r^{(t)}=\sigma\left(W^{r}x^{(t)}+U^{r}h^{(t-1)}+b_{r}\right)$ |  | (13)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{(t)}=\sigma\left(W^{r}x^{(t)}+U^{r}h^{(t-1)}+b_{r}\right)$ |  | (13)
    |'
- en: '|  | $\hat{h}^{(t)}=tanh\left(W^{h}x^{(t)}+U^{h}(r^{(t)}\odot h^{(t-1)})+b_{h}\right)$
    |  | (14) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{h}^{(t)}=tanh\left(W^{h}x^{(t)}+U^{h}(r^{(t)}\odot h^{(t-1)})+b_{h}\right)$
    |  | (14) |'
- en: '|  | $h^{(t)}=(1-z^{(t)})\odot h^{(t-1)}+z^{(t)}\odot\hat{h}^{(t)}$ |  | (15)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{(t)}=(1-z^{(t)})\odot h^{(t-1)}+z^{(t)}\odot\hat{h}^{(t)}$ |  | (15)
    |'
- en: Where $t$ represents time step $t$. $z$ and $r$ are gates, denoting update gate
    and reset gate respectively. $x$, $\hat{h}$ and $h$ are input, candidate activation
    vector and output respectively. $b$ is bias while $W$ and $U$ are weight matrixes.
    $\odot$ denotes element-wise multiplication.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$t$代表时间步$t$。$z$和$r$是门，分别表示更新门和重置门。$x$、$\hat{h}$和$h$分别是输入、候选激活向量和输出。$b$是偏置，$W$和$U$是权重矩阵。$\odot$表示逐元素相乘。
- en: LSTM and GRU, as two types of gating units, are very similar to each other (Chung
    et al., [2014](#bib.bib54)). The most prominent common point between them is that
    from time step $t$ to time step $t+1$, an additive component is introduced to
    update the state whereas simple RNNs always replace the activation. Both LSTM
    and GRU keep certain old components and mix them with new contents. This property
    enables the units to remember the information of history steps farther back and,
    more importantly, avoid gradient vanishing problems when backpropagating the error.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM和GRU作为两种门控单元，非常相似 (Chung et al., [2014](#bib.bib54))。它们之间最显著的共同点是，从时间步$t$到时间步$t+1$，引入了一个加性组件来更新状态，而简单的RNN总是替换激活。LSTM和GRU都保留了一定的旧组件，并将其与新内容混合。这一特性使得这些单元能够记住更远历史步骤的信息，更重要的是，避免了反向传播错误时的梯度消失问题。
- en: There also exist several differences between them. LSTM exposes its memory content
    under the control of the output gate, while the same content in GRU is in an uncontrolled
    manner. Additionally, different from LSTM, GRU does not independently gate the
    amount of new memory content being added. And if looking from experimental perspective,
    GRU has fewer parameters, which contributes to its faster convergence and better
    generalization ability. It has also been shown that GRU can achieve better performance
    in smaller datasets (Chung et al., [2014](#bib.bib54)). However, Gruber and Jockisch
    ([2020](#bib.bib113)) showed that LSTM cells exhibited consistently better performance
    in a large-scale analysis of Neural Machine Translation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它们之间还存在一些差异。LSTM在输出门的控制下暴露其记忆内容，而GRU中的相同内容则以不受控制的方式存在。此外，与LSTM不同，GRU不独立控制新增记忆内容的数量。从实验角度来看，GRU参数较少，这有助于其更快的收敛和更好的泛化能力。研究表明，GRU在较小的数据集上能取得更好的性能 (Chung
    et al., [2014](#bib.bib54))。然而，Gruber和Jockisch ([2020](#bib.bib113)) 表明，在大规模神经机器翻译分析中，LSTM单元表现出持续较好的性能。
- en: 2.2.4 Bidirectional Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 双向递归神经网络
- en: 'In sequence learning, not only the past information is essential to the model
    inference, the future information should also be considered to achieve a better
    inference ability. Schuster and Paliwal ([1997](#bib.bib310)) proposed the bi-directional
    recurrent neural networks (BRNNs), which had two kinds of hidden layers: the first
    encoded information from past time steps while the second encoded information
    in a flipped direction. The model can be described using the equations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列学习中，不仅过去的信息对模型推断至关重要，未来的信息也应考虑以实现更好的推断能力。Schuster和Paliwal ([1997](#bib.bib310))
    提出了双向递归神经网络（BRNNs），它具有两种隐藏层：第一种编码过去时间步的信息，而第二种则编码反向信息。该模型可以用以下方程描述：
- en: '|  | $h^{(t)}=\sigma\left(W^{hx}x^{(t)}+W^{hh}h^{(t-1)}+b_{h}\right)$ |  |
    (16) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{(t)}=\sigma\left(W^{hx}x^{(t)}+W^{hh}h^{(t-1)}+b_{h}\right)$ |  |
    (16) |'
- en: '|  | $z^{(t)}=\sigma\left(W^{zx}x^{(t)}+W^{zz}z^{(t+1)}+b_{z}\right)$ |  |
    (17) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{(t)}=\sigma\left(W^{zx}x^{(t)}+W^{zz}z^{(t+1)}+b_{z}\right)$ |  |
    (17) |'
- en: '|  | $\hat{y}^{(t)}=softmax\left(W^{yh}h^{(t)}+W^{yz}z^{(t)}+b_{y}\right)$
    |  | (18) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}^{(t)}=softmax\left(W^{yh}h^{(t)}+W^{yz}z^{(t)}+b_{y}\right)$
    |  | (18) |'
- en: Where $h$ and $z$ are the two hidden layers. Other variables are defined in
    the same way as in the case of LSTMs and GRUs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h$和$z$是两个隐藏层。其他变量的定义与LSTM和GRU中的相同。
- en: 2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder Models)
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5 原始序列到序列模型（编码器-解码器模型）
- en: 'Sutskever et al. ([2014](#bib.bib352)) first proposed the sequence-to-sequence
    model to solve the machine translation tasks. The sequence-to-sequence model aimed
    to map an input sequence to an output sequence by first using an encoder to map
    the input sequence into an intermediate vector and a decoder further generated
    the output based on the intermediate vector and history generated by the decoder.
    The equations below illustrate the encoder-decoder model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Sutskever 等人（[2014](#bib.bib352)）首次提出了序列到序列模型来解决机器翻译任务。序列到序列模型的目的是通过首先使用编码器将输入序列映射为一个中间向量，然后由解码器基于中间向量和解码器生成的历史进一步生成输出，从而将输入序列映射到输出序列。下述方程展示了编码器-解码器模型：
- en: '|  | $Encoder:h_{t}=E(h_{t-1},x_{t})$ |  | (19) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $Encoder:h_{t}=E(h_{t-1},x_{t})$ |  | (19) |'
- en: '|  | $Decoder:y_{t}=D(h_{t},y_{t-1})$ |  | (20) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $Decoder:y_{t}=D(h_{t},y_{t-1})$ |  | (20) |'
- en: Where $t$ is the time step, $h$ is the hidden vector and $y$ is the output vector.
    $E$ and $D$ are the sequential cells used by the encoder and decoder respectively.
    The last hidden state of the encoder is the intermediate vector, and this vector
    is usually used to initialize the first hidden state of the decoder. At encoding
    time, each hidden state is decided by the hidden state of the previous time step
    and the input at the current time step, while at decoding time, each hidden state
    is decided by the current hidden state and the output of the previous time step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 是时间步，$h$ 是隐藏向量，$y$ 是输出向量。$E$ 和 $D$ 分别是编码器和解码器使用的顺序单元。编码器的最后隐藏状态是中间向量，这个向量通常用于初始化解码器的第一个隐藏状态。在编码时，每个隐藏状态由前一个时间步的隐藏状态和当前时间步的输入决定，而在解码时，每个隐藏状态由当前隐藏状态和前一个时间步的输出决定。
- en: This model is powerful because it is not restricted to fixed-length inputs and
    outputs. Instead, the length of the source sequence and target sequence can differ.
    Based on this model, many more advanced sequence-to-sequence models have been
    developed, which will be discussed in this and subsequent sections.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型之所以强大，是因为它不受限于固定长度的输入和输出。相反，源序列和目标序列的长度可以不同。基于该模型，已经开发出了许多更先进的序列到序列模型，这些模型将在本节及后续章节中讨论。
- en: RNNs play an essential role in neural dialogue systems for their strong ability
    to encode sequential text information. RNNs and their variants are found in many
    dialogue systems. Task-oriented systems apply RNNs as encoders of dialogue context,
    dialogue state, knowledge base entries, and domain tags (Moon et al., [2019](#bib.bib257);
    Chen et al., [2019b](#bib.bib44); Wu et al., [2019b](#bib.bib415), [a](#bib.bib413)).
    Open-domain systems apply RNNs as dialogue history encoders (Sankar et al., [2019](#bib.bib304);
    Du and Black, [2019](#bib.bib76); Ji et al., [2020](#bib.bib155); Chen et al.,
    [2020b](#bib.bib46)), among which retrieval-based systems model dialogue history
    and candidate responses together (Zhu et al., [2018](#bib.bib466); Tang et al.,
    [2019](#bib.bib362); Feldman and El-Yaniv, [2019](#bib.bib85); Lu et al., [2019b](#bib.bib231)).
    In knowledge-grounded systems, RNNs are encoders of outside knowledge sources
    (e.g., background, persona, topic, etc.) (Shuster et al., [2019](#bib.bib324);
    Majumder et al., [2020b](#bib.bib239); Chen et al., [2020b](#bib.bib46); Cho and
    May, [2020](#bib.bib50)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在神经对话系统中扮演了重要角色，因为它们具有强大的编码顺序文本信息的能力。RNN 及其变体在许多对话系统中都有应用。任务导向系统使用 RNN 作为对话上下文、对话状态、知识库条目和领域标签的编码器（Moon
    等人，[2019](#bib.bib257)；Chen 等人，[2019b](#bib.bib44)；Wu 等人，[2019b](#bib.bib415)，[a](#bib.bib413)）。开放域系统则将
    RNN 作为对话历史的编码器（Sankar 等人，[2019](#bib.bib304)；Du 和 Black，[2019](#bib.bib76)；Ji
    等人，[2020](#bib.bib155)；Chen 等人，[2020b](#bib.bib46)），其中基于检索的系统将对话历史和候选响应一起建模（Zhu
    等人，[2018](#bib.bib466)；Tang 等人，[2019](#bib.bib362)；Feldman 和 El-Yaniv，[2019](#bib.bib85)；Lu
    等人，[2019b](#bib.bib231)）。在知识基础系统中，RNN 作为外部知识来源的编码器（例如背景、个性、话题等）（Shuster 等人，[2019](#bib.bib324)；Majumder
    等人，[2020b](#bib.bib239)；Chen 等人，[2020b](#bib.bib46)；Cho 和 May，[2020](#bib.bib50)）。
- en: Furthermore, as the decoder of sequence-to-sequence models in dialogue systems (Huang
    et al., [2020c](#bib.bib150); Song et al., [2019](#bib.bib338); Liu et al., [2019](#bib.bib222);
    Lin et al., [2019](#bib.bib213)), RNNs usually decode the hidden state of utterance
    sequences by greedy search or beam search (Aubert et al., [1994](#bib.bib11)).
    These decoding mechanisms cause problems like generic responses, which will be
    discussed in later sections.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作为对话系统中序列到序列模型的解码器（Huang et al., [2020c](#bib.bib150); Song et al., [2019](#bib.bib338);
    Liu et al., [2019](#bib.bib222); Lin et al., [2019](#bib.bib213)），RNN 通常通过贪心搜索或束搜索来解码话语序列的隐藏状态（Aubert
    et al., [1994](#bib.bib11)）。这些解码机制会引发诸如通用响应等问题，后续章节将对此进行讨论。
- en: Some works (Liu et al., [2019](#bib.bib222); Mehri et al., [2019](#bib.bib244);
    Chen et al., [2019c](#bib.bib45); Ma et al., [2020a](#bib.bib235)) combined RNNs
    as a part of dialogue representation models to train dialogue embeddings and further
    improved the performance of dialogue-related tasks. These embedding models were
    trained on dialogue tasks and present more dialogue features. They consistently
    outperformed state-of-the-art contextual representation models (e.g., BERT, ELMo,
    and GPT) in some dialogue tasks when these contextual representation models were
    not fine-tuned for the specific tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究（Liu et al., [2019](#bib.bib222); Mehri et al., [2019](#bib.bib244); Chen
    et al., [2019c](#bib.bib45); Ma et al., [2020a](#bib.bib235)）将 RNN 结合为对话表示模型的一部分，以训练对话嵌入，并进一步提高了对话相关任务的性能。这些嵌入模型在对话任务上进行了训练，并呈现出更多对话特征。当这些上下文表示模型没有针对特定任务进行微调时，它们在某些对话任务中的表现始终优于最先进的上下文表示模型（如
    BERT、ELMo 和 GPT）。
- en: 2.3 Hierarchical Recurrent Encoder-Decoder (HRED)
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 层次递归编码器-解码器 (HRED)
- en: Hierarchical Recurrent Encoder-Decoder (HRED) is a context-aware sequence-to-sequence
    model. It was first proposed by Sordoni et al. ([2015a](#bib.bib339)) to address
    the context-aware online query suggestion problem. It was designed to be aware
    of history queries and the proposed model can provide rare and high-quality results.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 层次递归编码器-解码器 (HRED) 是一个上下文感知的序列到序列模型。它最初由 Sordoni et al. ([2015a](#bib.bib339))
    提出，旨在解决上下文感知的在线查询建议问题。它被设计为能够关注历史查询，并且提出的模型能够提供稀有且高质量的结果。
- en: With the popularity of the sequence-to-sequence model, Serban et al. ([2016](#bib.bib312))
    extended HRED to the dialogue domain and built an end-to-end context-aware dialogue
    system. HRED achieved noticeable improvements in dialogue and end-to-end question
    answering. This work attracted even more attention than the original paper for
    that dialogue systems are a perfect setting for the application of HRED. Traditional
    dialogue systems (Ritter et al., [2011](#bib.bib300)) generated responses based
    on the single-turn messages, which sacrificed the information in the dialogue
    history. Sordoni et al. ([2015b](#bib.bib340)) combined dialogue history turns
    with a window size of 3 as the input of a sequence-to-sequence model for response
    generation, which is limited as well for that they encode the dialogue history
    only in token-level. The “turn-by-turn" characteristic of dialogue indicated that
    the turn-level information also matters. The HRED learned both token-level and
    turn-level representation, thus exhibiting promising dialogue context awareness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 随着序列到序列模型的流行，Serban et al. ([2016](#bib.bib312)) 将 HRED 扩展到对话领域，并建立了一个端到端的上下文感知对话系统。HRED
    在对话和端到端问答中取得了显著的改进。这项工作比原始论文更受关注，因为对话系统是 HRED 应用的完美场景。传统对话系统（Ritter et al., [2011](#bib.bib300)）基于单轮消息生成响应，这牺牲了对话历史中的信息。Sordoni
    et al. ([2015b](#bib.bib340)) 将对话历史轮次与大小为 3 的窗口结合，作为序列到序列模型的输入进行响应生成，但这种方法也有限，因为它们仅在标记级别编码对话历史。对话的“轮次”特征表明轮次级别的信息也很重要。HRED
    学习了标记级别和轮次级别的表示，从而展示了有希望的对话上下文感知能力。
- en: '![Refer to caption](img/1ace5d3a7d1f279b2018e6f3c1bad690.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ace5d3a7d1f279b2018e6f3c1bad690.png)'
- en: 'Figure 4: The HRED model in a dialogue setting (Serban et al., [2016](#bib.bib312))'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：对话设置中的 HRED 模型（Serban et al., [2016](#bib.bib312)）
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2.3 Hierarchical Recurrent Encoder-Decoder (HRED)
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") represents the HRED in a dialogue setting.
    HRED models the token-level and turn-level sequences hierarchically with two levels
    of RNNs: a token-level RNN consisting of an encoder and a decoder, and a turn-level
    context RNN. The encoder RNN encodes the utterance of each turn token by token
    into a hidden state. This hidden state is then taken as the input of the context
    RNN at each turn-level time step. Thus the turn-level context RNN iteratively
    keeps track of the history utterances. The hidden state of context RNN at turn
    $t$ represents a summary of the utterances up to turn $t$ and is used to initialize
    the first hidden state of decoder RNN, which is similar to a standard decoder
    in sequence-to-sequence models (Sutskever et al., [2014](#bib.bib352)). All of
    the three RNNs described above apply GRU cells as the recurrent unit, and the
    parameters of encoder and decoder are shared for each utterance.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S2.F4 "图 4 ‣ 2.3 层次递归编码器-解码器 (HRED) ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统性调查")
    显示了对话环境中的 HRED。HRED 通过两个层次的 RNN 层次化建模令牌级别和轮次级别的序列：一个由编码器和解码器组成的令牌级 RNN，以及一个轮次级上下文
    RNN。编码器 RNN 将每轮的发言逐个令牌编码为隐藏状态。这个隐藏状态在每个轮次级时间步中作为上下文 RNN 的输入。因此，轮次级上下文 RNN 迭代地跟踪历史发言。轮次级上下文
    RNN 在轮次 $t$ 的隐藏状态表示了直到轮次 $t$ 的发言摘要，并用来初始化解码器 RNN 的第一个隐藏状态，这类似于序列到序列模型中的标准解码器 (Sutskever
    et al., [2014](#bib.bib352))。上述三个 RNN 都使用 GRU 单元作为递归单元，编码器和解码器的参数对于每个发言是共享的。
- en: 'Serban et al. ([2017a](#bib.bib313)) further proposed Latent Variable Hierarchical
    Recurrent Encoder-Decoder (VHRED) to model complex dependencies between sequences.
    Based on HRED, VHRED combined a latent variable into the decoder and turned the
    decoding process into a two-step generation process: sampling a latent variable
    at the first step and then generating the response conditionally. VHRED was trained
    with a variational lower bound on the log-likelihood and exhibited promising improvement
    in diversity, length, and quality of generated responses.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Serban 等人 ([2017a](#bib.bib313)) 进一步提出了潜变量层次递归编码器-解码器 (VHRED) 以建模序列之间的复杂依赖关系。在
    HRED 的基础上，VHRED 将潜变量结合到解码器中，将解码过程转变为两步生成过程：在第一步中采样潜变量，然后根据条件生成响应。VHRED 使用对数似然的变分下界进行训练，并在生成响应的多样性、长度和质量方面表现出有希望的改进。
- en: 'Many recent works in dialogue-related tasks apply HRED-based frameworks to
    capture hierarchical dialogue features. Zhang et al. ([2019a](#bib.bib446)) argued
    that standard HRED processed all contexts in dialogue history indiscriminately.
    Inspired by the architecture of Transformer (Vaswani et al., [2017](#bib.bib377)),
    they proposed ReCoSa, a self-attention-based hierarchical model. It first applied
    LSTM to encode token-level information into context hidden vectors and then calculated
    the self-attention for both the context vectors and masked response vectors. At
    the decoding stage, the encoder-decoder attention was calculated to facilitate
    the decoding. Shen et al. ([2019](#bib.bib321)) proposed a hierarchical model
    consisting of 3 hierarchies: the discourse-level which captures the global knowledge,
    the pair-level which captured the topic information in utterance pairs, and the
    utterance level which captured the content information. Such a multi-hierarchy
    structure contributed to its higher quality responses in terms of diversity, coherence,
    and fluency. Chauhan et al. ([2019](#bib.bib38)) applied HRED and VGG-19 as a
    multimodal HRED (MHRED). The HRED encoded hierarchical dialogue context while
    VGG-19 extracted visual features for all images in the corresponding turn. With
    the addition of a position-aware attention mechanism, the model showed more diverse
    and accurate responses in a visually grounded setting. Mehri et al. ([2019](#bib.bib244))
    learned dialogue context representations via four sub-tasks, three of which (next-utterance
    generation, masked-utterance retrieval, and inconsistency identification) made
    uses of HRED as the context encoder, and good performance was achieved. Cao et al.
    ([2019](#bib.bib35)) used HRED to encode the dialogue history between therapists
    and patients to categorize therapist and client MI behavioral codes and predict
    future codes. Qiu et al. ([2020](#bib.bib283)) applied an LSTM-based VHRED to
    address the two-agent and multi-agent dialogue structure induction problem in
    an unsupervised fashion. On top of that, they applied a Conditional Random Field
    model in two-agent dialogues and a non-projective dependency tree in multi-agent
    dialogues, both of them achieving better performance in dialogue structure modeling.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在对话相关任务中的许多研究应用了基于HRED的框架来捕捉层次化的对话特征。张等人（[2019a](#bib.bib446)）认为标准的HRED对对话历史中的所有上下文进行了不加区分的处理。受Transformer（Vaswani
    et al., [2017](#bib.bib377)）架构的启发，他们提出了ReCoSa，一种基于自注意力的层次化模型。该模型首先应用LSTM将标记级别的信息编码成上下文隐藏向量，然后计算上下文向量和掩蔽响应向量的自注意力。在解码阶段，计算了编码器-解码器注意力以促进解码。沈等人（[2019](#bib.bib321)）提出了一种包含3个层次的层次化模型：话语级别捕捉全球知识，配对级别捕捉话语对中的主题信息，话语级别捕捉内容信息。这种多层次结构提高了响应的质量，包括多样性、一致性和流畅性。Chauhan等人（[2019](#bib.bib38)）将HRED和VGG-19应用于多模态HRED（MHRED）。HRED对层次化对话上下文进行编码，而VGG-19提取了相应轮次中所有图像的视觉特征。通过增加位置感知注意力机制，该模型在视觉基础设置中展示了更具多样性和准确性的响应。Mehri等人（[2019](#bib.bib244)）通过四个子任务学习对话上下文表示，其中三个（下一个话语生成、掩蔽话语检索和不一致性识别）利用HRED作为上下文编码器，并取得了良好的性能。Cao等人（[2019](#bib.bib35)）使用HRED对治疗师和患者之间的对话历史进行编码，以对治疗师和客户的MI行为编码进行分类，并预测未来的编码。Qiu等人（[2020](#bib.bib283)）应用了一种基于LSTM的VHRED，以无监督的方式解决两代理和多代理对话结构归纳问题。在此基础上，他们在两代理对话中应用了条件随机场模型，在多代理对话中应用了非投影依赖树，这两者在对话结构建模方面都取得了更好的性能。
- en: 2.4 Memory Networks
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 记忆网络
- en: Memory is a crucial component when addressing problems regarding past experiences
    or outside knowledge sources. The hippocampus of human brains and the hard disk
    of computers are the components that humans and computers depend on for reading
    and writing memories. Traditional models rarely have a memory component, thus
    lacking the ability of knowledge reusing and reasoning. RNNs iteratively pass
    history information across time steps, which, to some extent, can be viewed as
    a memory model. However, even for LSTM, which is a powerful variant of RNN equipped
    with a long-term and short-term memory, the memory module is too small and facts
    are not explicitly discriminated, thus not being able to compress specific knowledge
    facts and reuse them in tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆在处理与过去经验或外部知识来源相关的问题时是一个关键组件。人脑的海马体和计算机的硬盘是人类和计算机读取和写入记忆所依赖的组件。传统模型很少有记忆组件，因此缺乏知识重用和推理的能力。RNN通过时间步迭代地传递历史信息，这在某种程度上可以被视为一种记忆模型。然而，即使是LSTM，这种具备长期和短期记忆的强大RNN变体，其记忆模块也过于简单，且事实没有被明确区分，因此无法压缩特定的知识事实并在任务中重用。
- en: 'Weston et al. ([2014](#bib.bib402)) proposed memory networks, a model that
    is endowed with a memory component. As described in their work, a memory network
    has five modules: a memory module which stores the representations of memory facts;
    an ‘I’ module which maps the input memory facts into embedded representations;
    a ‘G’ module which decides the update of the memory module; an ‘O’ module which
    generates the output conditioned on the input representation and memory representation;
    an ‘R’ module which organizes the final response based on the output of ‘O’ module.
    This model needs a strong supervision signal for each module and thus is not practical
    to train in an end-to-end fashion.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Weston等人 ([2014](#bib.bib402)) 提出了记忆网络，这是一种具有记忆组件的模型。正如他们的工作中所描述的，记忆网络有五个模块：一个存储记忆事实表示的记忆模块；一个将输入记忆事实映射到嵌入表示的‘I’模块；一个决定记忆模块更新的‘G’模块；一个根据输入表示和记忆表示生成输出的‘O’模块；一个根据‘O’模块的输出组织最终响应的‘R’模块。该模型需要对每个模块提供强监督信号，因此不适合端到端训练。
- en: Sukhbaatar et al. ([2015](#bib.bib349)) extended their prior work to an end-to-end
    memory network, which was commonly accepted as a standard memory network being
    easy to train and apply.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Sukhbaatar等人 ([2015](#bib.bib349)) 将他们之前的工作扩展到端到端的记忆网络，这被普遍接受为一种标准记忆网络，易于训练和应用。
- en: '![Refer to caption](img/8d1b3c5df55f53ec6fe5135a7a769aab.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d1b3c5df55f53ec6fe5135a7a769aab.png)'
- en: 'Figure 5: The structure of end-to-end memory networks (Sukhbaatar et al., [2015](#bib.bib349))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：端到端记忆网络的结构 (Sukhbaatar等，[2015](#bib.bib349))
- en: 'Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Memory Networks ‣ 2 Neural Models in Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey") represents the proposed end-to-end memory networks. Its architecture
    consists of three stages: weight calculation, memory selection, and final prediction.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S2.F5 "图5 ‣ 2.4 记忆网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述") 表示了提出的端到端记忆网络。其架构包括三个阶段：权重计算、记忆选择和最终预测。
- en: 'Weight calculation. The model first converts the input memory set $\{x_{i}\}$
    into memory representations $\{m_{i}\}$ using a representation model $A$. Then
    it maps the input query into its embedding space using another representation
    model $B$, obtaining an embedding vector $u$. The final weights are calculated
    as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 权重计算。模型首先使用表示模型$A$将输入记忆集$\{x_{i}\}$转换为记忆表示$\{m_{i}\}$。然后，使用另一个表示模型$B$将输入查询映射到其嵌入空间，获得嵌入向量$u$。最终的权重计算如下：
- en: '|  | $p_{i}=Softmax(u^{T}m_{i})$ |  | (21) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=Softmax(u^{T}m_{i})$ |  | (21) |'
- en: Where $p_{i}$ is the weight corresponding to each input memory $x_{i}$ conditioned
    on the query.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p_{i}$是与每个输入记忆$x_{i}$在查询条件下对应的权重。
- en: 'Memory selection. Before generating the final prediction, a selected memory
    vector is generated by first encoding the input memory $x_{i}$ into an embedded
    vector $c_{i}$ using another representation model $C$, then calculating the weighted
    sum over the $\{c_{i}\}$ using the weights calculated in the previous stage:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆选择。在生成最终预测之前，通过首先使用另一种表示模型$C$将输入记忆$x_{i}$编码为嵌入向量$c_{i}$，然后根据在上一阶段计算的权重对$\{c_{i}\}$进行加权求和，生成一个选择的记忆向量：
- en: '|  | $o=\sum_{i}p_{i}c_{i}$ |  | (22) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $o=\sum_{i}p_{i}c_{i}$ |  | (22) |'
- en: Where o represents the selected memory vector. This vector cannot be found in
    memory representations. The soft memory selection facilitates differentiability
    in gradient computing, which makes the whole model end-to-end trainable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 o 代表选择的记忆向量。这个向量在记忆表示中无法找到。软记忆选择有助于梯度计算的可微性，使得整个模型可以端到端训练。
- en: 'Final prediction. The final prediction is obtained by mapping the sum vector
    of the selected memory $o$ and the embedded query $u$ into a probability vector
    $\hat{a}$:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测。最终预测是通过将选定记忆 $o$ 和嵌入查询 $u$ 的和向量映射到概率向量 $\hat{a}$ 获得的：
- en: '|  | $\hat{\alpha}=Softmax(W(o+u))$ |  | (23) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\alpha}=Softmax(W(o+u))$ |  | (23) |'
- en: Many dialogue-related works incorporate memory networks into their framework,
    especially for tasks involving an external knowledge base like task-oriented dialogue
    systems, knowledge-grounded dialogue systems, and QA.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 许多与对话相关的工作将记忆网络纳入其框架中，特别是对于涉及外部知识库的任务，如任务导向对话系统、知识驱动对话系统和问答系统。
- en: Memory networks for task-oriented dialogue systems
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务导向对话系统的记忆网络
- en: 'Chen et al. ([2019c](#bib.bib45)) argued that state-of-the-art task-oriented
    dialogue systems tended to combine dialogue history and knowledge base entries
    in a single memory module, which influenced the response quality. They proposed
    a task-oriented system that consists of three memory modules: two long-term memory
    modules storing the dialogue history and the knowledge base respectively; a working
    memory module that memorizes two distributions and controls the final word prediction.
    He et al. ([2020a](#bib.bib128)) trained a task-oriented dialogue system with
    a “Two-teacher-one-student" framework to improve the knowledge retrieval and response
    quality of their memory networks. They first trained two teacher networks using
    reinforcement learning with complementary goal-specific reward functions respectively.
    Then with a GAN framework, they trained two discriminators to teach the student
    memory network to generate responses similar to those of the teachers, transferring
    the expert knowledge from the two teachers to the student. The advantage is that
    this training framework needs only weak supervision and the student network can
    benefit from the complementary targets of teacher networks. Kim et al. ([2019](#bib.bib170))
    solved the dialogue state tracking in task-oriented dialogue systems with a memory
    network that memorized the dialogue states. Different from other works, they did
    not update all dialogue states in the memory module from scratch. Instead, their
    model first predicted which states needed to be updated and then overwrote the
    target states. By selectively overwriting the memory module, they improved the
    efficiency of the dialogue state tracking task. Dai et al. ([2020](#bib.bib61))
    applied the MemN2N (Sukhbaatar et al., [2015](#bib.bib349)) as task-oriented utterance
    encoder, memorizing the existing responses and dialogue history. Then they used
    model-agnostic meta-learning (MAML) (Finn et al., [2017](#bib.bib92)) to train
    the framework to retrieve correct responses in a few-shot fashion.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人（[2019c](#bib.bib45)）认为，最先进的任务导向对话系统往往将对话历史和知识库条目结合在一个记忆模块中，这影响了响应质量。他们提出了一个由三个记忆模块组成的任务导向系统：两个长期记忆模块分别存储对话历史和知识库；一个工作记忆模块记忆两个分布并控制最终的词预测。何等人（[2020a](#bib.bib128)）使用“两个教师一个学生”框架训练了一个任务导向对话系统，以提高记忆网络的知识检索和响应质量。他们首先使用带有互补目标奖励函数的强化学习分别训练了两个教师网络。然后通过
    GAN 框架，训练了两个判别器来教学生记忆网络生成类似于教师的响应，将专家知识从两个教师转移到学生。其优势在于这个训练框架只需要弱监督，学生网络可以从教师网络的互补目标中受益。金等人（[2019](#bib.bib170)）通过一个记忆网络解决了任务导向对话系统中的对话状态追踪问题。与其他工作不同的是，他们并没有从头更新记忆模块中的所有对话状态。相反，他们的模型首先预测需要更新的状态，然后覆盖目标状态。通过选择性地覆盖记忆模块，他们提高了对话状态追踪任务的效率。戴等人（[2020](#bib.bib61)）应用了
    MemN2N（Sukhbaatar 等人，[2015](#bib.bib349)）作为任务导向的发言编码器，记忆现有的响应和对话历史。然后他们使用模型无关的元学习（MAML）（Finn
    等人，[2017](#bib.bib92)）训练框架，以少量样本的方式检索正确的响应。
- en: Memory networks for open-domain dialogue systems
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 开放域对话系统的记忆网络
- en: Tian et al. ([2019](#bib.bib369)) proposed a knowledge-grounded chit-chat system.
    A memory network was used to store query-response pairs and at the response generation
    stage, the generator produced the response conditioned on both the input query
    and memory pairs. It extracted key-value information from the query-response pairs
    in memory and combined them into token prediction. Xu et al. ([2019](#bib.bib425))
    proposed to use meta-words to generate responses in open-domain systems in a controllable
    way. Meta-words are phrases describing response attributes. Using a goal-tracking
    memory network, they memorized the meta-words and generated responses based on
    the user message while incorporating meta-words at the same time. Gan et al. ([2019](#bib.bib95))
    performed multi-step reasoning conditioned on a dialogue history memory module
    and a visual memory module. Two memory modules recurrently refined the representation
    to perform the next reasoning process. Experimental results illustrated the benefits
    of combining image and dialogue clues to improve the performance of visual dialogue
    systems. Han et al. ([2019](#bib.bib122)) trained a reinforcement learning agent
    to decide which memory vector can be replaced when the memory module is full to
    improve the accuracy and efficiency of the document-grounded question-answering
    task. They solved the scalability problem of memory networks by learning the query-specific
    value corresponding to each memory. Gao et al. ([2020c](#bib.bib102)) solved the
    same problem in a conversational machine reading task. They proposed an Explicit
    Memory Tracker (EMT) to decide whether the provided information in memory is enough
    for final prediction. Furthermore, a coarse-to-fine strategy was applied for the
    agent to make clarification questions to request additional information and refine
    the reasoning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Tian 等人 ([2019](#bib.bib369)) 提出了一个基于知识的聊天系统。使用了记忆网络来存储查询-响应对，在响应生成阶段，生成器根据输入查询和记忆对生成响应。它从记忆中的查询-响应对中提取关键信息，并将这些信息组合到标记预测中。Xu
    等人 ([2019](#bib.bib425)) 提议使用元词在开放域系统中以可控的方式生成响应。元词是描述响应属性的短语。通过使用目标跟踪记忆网络，他们记住了元词，并基于用户消息生成响应，同时结合了元词。Gan
    等人 ([2019](#bib.bib95)) 在对话历史记忆模块和视觉记忆模块的条件下进行了多步骤推理。这两个记忆模块反复优化表示，以进行下一步推理。实验结果表明，结合图像和对话线索有助于提高视觉对话系统的性能。Han
    等人 ([2019](#bib.bib122)) 训练了一个强化学习代理来决定当记忆模块满时可以替换哪个记忆向量，以提高文档基础问答任务的准确性和效率。他们通过学习与每个记忆相关的查询特定值解决了记忆网络的可扩展性问题。Gao
    等人 ([2020c](#bib.bib102)) 在对话机器阅读任务中解决了同样的问题。他们提出了一种显式记忆追踪器（EMT），以决定记忆中提供的信息是否足够用于最终预测。此外，代理采用了粗到细的策略，提出澄清问题以请求额外信息并完善推理。
- en: 2.5 Attention and Transformer
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 注意力机制和变换器
- en: 'As introduced in Section [2.2](#S2.SS2 "2.2 Recurrent Neural Networks and Vanilla
    Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey"), traditional sequence-to-sequence
    models decode the token conditioning on the current hidden state and output vector
    of last time step, which is formulated as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2.2](#S2.SS2 "2.2 循环神经网络和简单序列到序列模型 ‣ 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述")节中介绍的，传统的序列到序列模型在解码时依据当前隐藏状态和上一个时间步的输出向量进行解码，其公式为：
- en: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},h_{i})$ |  | (24) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},h_{i})$ |  | (24) |'
- en: Where g is a sequential model which maps the input vectors into a probability
    vector.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 g 是一个顺序模型，将输入向量映射到概率向量。
- en: 'However, such a decoding scheme is limited when the input sentence is long.
    RNNs are not able to encode all information into a fixed-length hidden vector.
    Cho et al. ([2014a](#bib.bib51)) proved via experiments that a sequence-to-sequence
    model performed worse when the input sequence got longer. Also, for the limited-expression
    ability of a fixed-length hidden vector, the performance of the decoding scheme
    in Equation ([24](#S2.E24 "In 2.5 Attention and Transformer ‣ 2 Neural Models
    in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")) largely depends on the first few steps of decoding, and
    if the decoder fails to have a good start, the whole sequence would be negatively
    affected.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当输入句子很长时，这种解码方案存在局限性。循环神经网络无法将所有信息编码为固定长度的隐藏向量。Cho等人（2014a）通过实验证明，当输入序列变长时，序列到序列模型的性能会下降。此外，由于固定长度隐藏向量的表达能力有限，方程（24）中的解码方案的性能在很大程度上取决于前几个解码步骤。如果解码器没有一个良好的启动，整个序列都会受到负面影响。
- en: 2.5.1 Attention
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1 注意力
- en: '![Refer to caption](img/4640c9ae734ac0e18a2c70d3b119c417.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: 图像 ![参考说明](img/4640c9ae734ac0e18a2c70d3b119c417.png)
- en: 'Figure 6: The attention model (Bahdanau et al., [2014](#bib.bib12))'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：注意力模型（Bahdanau等人，2014）
- en: 'Bahdanau et al. ([2014](#bib.bib12)) proposed the attention mechanism in the
    machine translation task. They described the method as “jointly align and translate",
    which illustrated the sequence-to-sequence translation model as an encoder-decoder
    model with attention. At the decoding stage, each decoding state would consider
    which parts of the encoded source sentence are correlated, instead of depending
    only on the immediate prior output token. The output probability distribution
    can be described as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau等人（2014）在机器翻译任务中提出了注意力机制。他们将该方法描述为“联合对齐和翻译”，将序列到序列翻译模型作为带有注意力的编码器-解码器模型进行了说明。在解码阶段，每个解码状态都会考虑与编码的源句子相关的部分，而不仅仅依赖于之前的输出标记。输出概率分布可以描述为：
- en: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},s_{i},c_{i})$ |  | (25) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},s_{i},c_{i})$ |  | (25) |'
- en: 'Where $i$ denotes the $i^{th}$ time step; $y_{i}$ is the output token, $s_{i}$
    is the decoder hidden state and $c_{i}$ is the weighted source sentence:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$i$表示第$i$个时间步；$y_{i}$是输出标记，$s_{i}$是解码器隐藏状态，$c_{i}$是加权的源句子：
- en: '|  | $s_{i}=f(s_{i-1},y_{i-1},c_{i})$ |  | (26) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=f(s_{i-1},y_{i-1},c_{i})$ |  | (26) |'
- en: '|  | $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (27) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (27) |'
- en: 'Where $\alpha_{ij}$ is the normalized weight score:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{ij}$是归一化的权重分数：
- en: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (28)
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (28)
    |'
- en: '$e_{ij}$ is the similarity score between $s_{i-1}$ and $j^{th}$ encoder hidden
    state $h_{j}$, where the score is predicted by the similarity model $a$:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $e_{ij}$是$s_{i-1}$和第$j$个编码器隐藏状态$h_{j}$之间的相似性分数，该分数是由相似性模型$a$预测的：
- en: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (29) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (29) |'
- en: 'Figure [6](#S2.F6 "Figure 6 ‣ 2.5.1 Attention ‣ 2.5 Attention and Transformer
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") illustrates the attention model, where
    t and T denote time steps of decoder and encoder respectively.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6]（#S2.F6 "Figure 6 ‣ 2.5.1 Attention ‣ 2.5 Attention and Transformer ‣ 2
    Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")说明了注意力模型，其中t和T分别表示解码器和编码器的时间步。'
- en: Memory networks are similar to attention networks in the way they operate, except
    for the choice of the similarity model. In memory networks, the encoded memory
    can be viewed as the encoded source sentence in attention. However, the memory
    model proposed by Sukhbaatar et al. ([2015](#bib.bib349)) chose cosine distance
    as the similarity model while the attention proposed by Bahdanau et al. ([2014](#bib.bib12))
    used a feed-forward network which is trainable together with the whole sequence-to-sequence
    model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 内存网络与注意力网络在操作方式上类似，唯一不同之处在于相似性模型的选择。在内存网络中，编码内存可以被看作是注意力中编码的源句子。然而，Sukhbaatar等人（2015）提出的内存模型选择了余弦距离作为相似性模型，而Bahdanau等人（2014）提出的注意力则使用了一个可训练的前馈网络，该网络与整个序列到序列模型一起进行训练。
- en: 2.5.2 Transformer
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2 Transformer
- en: Before transformers, most works combined attention with recurrent units, except
    for few works such as Parikh et al. ([2016](#bib.bib270)) and Gehring et al. ([2017](#bib.bib103)).
    Recurrent models condition each hidden state on the previous hidden state and
    the current input and are flexible in sequence length. However, due to their sequential
    nature, recurrent models cannot be trained in parallel, which severely undermines
    their potential. Vaswani et al. ([2017](#bib.bib377)) proposed Transformer, which
    entirely utilized attention mechanisms without any recurrent units and deployed
    more parallelization to speed up training. It applied self-attention and encoder-decoder
    attention to achieve local and global dependencies respectively.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器出现之前，大多数工作将注意力与递归单元结合，除了少数工作如Parikh等（[2016](#bib.bib270)）和Gehring等（[2017](#bib.bib103)）。递归模型将每个隐藏状态基于前一个隐藏状态和当前输入进行条件化，并且对序列长度具有灵活性。然而，由于其顺序性质，递归模型不能并行训练，这严重削弱了其潜力。Vaswani等（[2017](#bib.bib377)）提出了Transformer，该模型完全利用了注意力机制而没有任何递归单元，并且部署了更多的并行化以加快训练速度。它应用了自注意力和编码器-解码器注意力，分别实现局部和全局依赖。
- en: 'Figure [7](#S2.F7 "Figure 7 ‣ 2.5.2 Transformer ‣ 2.5 Attention and Transformer
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") represents the transformer. The following
    details its key mechanisms.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](#S2.F7 "Figure 7 ‣ 2.5.2 Transformer ‣ 2.5 Attention and Transformer ‣
    2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") 表示了变换器。以下详细介绍其关键机制。'
- en: '![Refer to caption](img/16fcebf8315258eecbe42ceccafd7253.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/16fcebf8315258eecbe42ceccafd7253.png)'
- en: 'Figure 7: The transformer model (Vaswani et al., [2017](#bib.bib377))'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：变换器模型（Vaswani等，[2017](#bib.bib377)）
- en: Encoder-decoder
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: 'The Transformer consists of an encoder and a decoder. The encoder maps the
    input sequence $(x_{1},\ldots,x_{n})$ into continuous hidden states $(z_{1},\ldots,z_{n})$.
    The decoder further generates the output sequence $(y_{1},\ldots,y_{n})$ based
    on the hidden states of the encoder. The probability model of the Transformer
    is in the same form as that of the vanilla sequence-to-sequence model introduced
    in Section [2.2.5](#S2.SS2.SSS5 "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder
    Models) ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence Models
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey"). Vaswani et al. ([2017](#bib.bib377))
    stacked 6 identical encoder layers and 6 identical decoder layers. An encoder
    layer consists of a multi-head attention component and a simple feed-forward network,
    both of which apply residual structure. The structure of a decoder layer is almost
    the same as that of an encoder layer, except for an additional encoder-decoder
    attention layer, which computes the attention between decoder hidden states of
    the current time step and the encoder output vectors. The input of the decoder
    is partially masked to make sure that each prediction is based on the previous
    tokens, avoiding predicting with the presence of future information. Both inputs
    of encoder and decoder use a positional encoding mechanism.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer包括一个编码器和一个解码器。编码器将输入序列$(x_{1},\ldots,x_{n})$映射到连续的隐藏状态$(z_{1},\ldots,z_{n})$。解码器进一步基于编码器的隐藏状态生成输出序列$(y_{1},\ldots,y_{n})$。Transformer的概率模型与第[2.2.5](#S2.SS2.SSS5
    "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder Models) ‣ 2.2 Recurrent
    Neural Networks and Vanilla Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey")节中介绍的普通序列到序列模型形式相同。Vaswani等（[2017](#bib.bib377)）堆叠了6个相同的编码器层和6个相同的解码器层。一个编码器层包括一个多头注意力组件和一个简单的前馈网络，两者都应用了残差结构。解码器层的结构几乎与编码器层相同，只是多了一个额外的编码器-解码器注意力层，该层计算当前时间步的解码器隐藏状态与编码器输出向量之间的注意力。解码器的输入被部分遮蔽，以确保每个预测基于之前的标记，避免在存在未来信息的情况下进行预测。编码器和解码器的输入都使用位置编码机制。'
- en: Self-attention
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自注意力
- en: 'For an input sentence $x=(x_{1},\ldots,x_{n})$, each token $x_{i}$ corresponds
    to three vectors: query, key, and value. The self-attention computes the attention
    weight for every token $x_{i}$ against all other tokens in $x$ by multiplying
    the query of $x_{i}$ with the keys of all the remaining tokens one-by-one. For
    parallel computing, the query, key ,and value vectors of all tokens are combined
    into three matrices: Query (Q), Key (K) ,and Value (V). The self-attention of
    an input sentence $x$ is computed by the following formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入句子 $x=(x_{1},\ldots,x_{n})$，每个 token $x_{i}$ 对应三个向量：查询、键和值。自注意力机制通过将 $x_{i}$
    的查询与 $x$ 中所有其他 token 的键逐一相乘，计算每个 token $x_{i}$ 对所有其他 token 的注意力权重。为了并行计算，将所有 token
    的查询、键和值向量合并成三个矩阵：查询（Q）、键（K）和值（V）。输入句子 $x$ 的自注意力由以下公式计算：
- en: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  | (30) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  | (30) |'
- en: Where $d_{k}$ is the dimension of queries or keys.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{k}$ 是查询或键的维度。
- en: Multi-head attention
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多头注意力
- en: 'To jointly consider the information from different subspaces of embedding,
    query, key, and value vectors are mapped into $h$ vectors of identical shapes
    by using different linear transformations, where $h$ denotes the number of heads.
    Attention is computed on each of these vectors in parallel, and the results are
    concatenated and further projected. The multi-head attention can be described
    as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了共同考虑嵌入的不同子空间的信息，查询、键和值向量通过不同的线性变换映射到 $h$ 个相同形状的向量，其中 $h$ 表示头的数量。对这些向量中的每一个并行计算注意力，然后将结果拼接起来并进一步投影。多头注意力可以描述为：
- en: '|  | $MultiHead(Q,K,V)=Concat(head_{1},...,head_{h})W^{O}$ |  | (31) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $MultiHead(Q,K,V)=Concat(head_{1},...,head_{h})W^{O}$ |  | (31) |'
- en: Where $head_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ and $W$ denotes
    the linear transformations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $head_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$，$W$ 表示线性变换。
- en: Positional encoding
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'The proposed transformer architecture has no recurrent units, which means that
    the order information of sequence is dismissed. The positional encoding is added
    with input embeddings to provide positional information. The paper chooses cosine
    functions for positional encoding:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的 Transformer 架构没有循环单元，这意味着序列的顺序信息被省略。位置编码与输入嵌入一起添加以提供位置相关信息。本文选择了余弦函数用于位置编码：
- en: '|  | $PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$ |  | (32) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$ |  | (32) |'
- en: '|  | $PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$ |  | (33) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$ |  | (33) |'
- en: Where $pos$ denotes the position of the target token and $i$ denotes the dimension,
    which means that each dimension of the positional matrix uses a different wavelength
    for encoding.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $pos$ 表示目标 token 的位置，$i$ 表示维度，这意味着位置矩阵的每个维度使用不同的波长进行编码。
- en: Transformer-based pretrain models and Transformer variants
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于 Transformer 的预训练模型和 Transformer 变体
- en: Recently, many transformer-based pretrain models have been developed. Unlike
    Embeddings from Language Model (ELMo) proposed by Peters et al. ([2018](#bib.bib273)),
    which is an LSTM-based contextual embedding model, transformer-based pretrain
    models are more powerful. Two most popular models are GPT-2 ⁶⁶6https://openai.com/blog/better-language-models/
    and BERT (Devlin et al., [2018](#bib.bib70)). GPT-2 and BERT both consist of 12
    transformer blocks and BERT is further improved by making the training bi-directional.
    They are powerful due to their capability of adapting to new tasks after pretraining.
    This property helped achieve significant improvements in many NLP tasks. There
    also evolve many Transformer variants (Zaheer et al., [2020](#bib.bib442); Dai
    et al., [2019](#bib.bib62); Guo et al., [2019](#bib.bib115)), which are designed
    to reduce the model parameters/computational complexity, or improve performance
    of the original Transformer in diverse scenarios. Lin et al. ([2021](#bib.bib212))
    and Tay et al. ([2020](#bib.bib366)) systematically summarize the state-of-the-art
    Transformer variants for academics that are interested.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多基于Transformer的预训练模型被开发出来。与Peters等人（[2018](#bib.bib273)）提出的基于LSTM的上下文嵌入模型ELMo不同，基于Transformer的预训练模型更强大。最受欢迎的两个模型是GPT-2
    [6](https://openai.com/blog/better-language-models/)和BERT（Devlin等人，[2018](#bib.bib70)）。GPT-2和BERT都由12个Transformer块组成，而BERT通过使训练双向来进一步改进。由于它们能够在预训练后适应新任务，这些模型非常强大。这一特性在许多NLP任务中取得了显著的改进。此外，还出现了许多Transformer变体（Zaheer等人，[2020](#bib.bib442)；Dai等人，[2019](#bib.bib62)；Guo等人，[2019](#bib.bib115)），这些变体旨在减少模型参数/计算复杂性，或在不同场景中提升原始Transformer的性能。Lin等人（[2021](#bib.bib212)）和Tay等人（[2020](#bib.bib366)）系统总结了最新的Transformer变体，供学术界有兴趣的研究者参考。
- en: Attention for dialogue systems
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话系统中的注意力机制
- en: Attention is a mechanism to catch the importance of different parts in the target
    sequence. Zhu et al. ([2018](#bib.bib466)) applied a two-level attention to generate
    words. Given the user message and candidate responses selected by a retrieval
    system, the generator first computes word-level attention weights, then uses sentence-level
    attention to rescale the weights. This two-level attention helps the generator
    catch different importance given the encoded context. Liu et al. ([2019](#bib.bib222))
    used an attention-based recurrent architecture to generate responses. They designed
    a multi-level encoder-decoder of which the multi-level encoder tries to map raw
    words, low-level clusters, and high-level clusters into hierarchical embedded
    representations while the multi-level decoder leveraged the hierarchical representations
    using attention and then generated responses. At each decoding stage, the model
    calculated two attention weights for the output of the higher-level decoder and
    the hidden state of the current level’s encoder. Chen et al. ([2019b](#bib.bib44))
    computed multi-head self-attention for the outputs of a dialogue act predictor.
    Unlike the transformer, which concatenates the outputs of different heads, they
    passed the outputs directly to the next multi-head layer. The stacked multi-head
    layers then generated the responses with dialogue acts as the input.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制用于捕捉目标序列中不同部分的重要性。Zhu等人（[2018](#bib.bib466)）应用了两级注意力机制来生成词语。在用户消息和由检索系统选择的候选响应的情况下，生成器首先计算词级注意力权重，然后使用句子级注意力来重新调整权重。这种两级注意力机制帮助生成器根据编码的上下文捕捉不同的重要性。Liu等人（[2019](#bib.bib222)）使用基于注意力的递归架构来生成响应。他们设计了一个多级编码器-解码器，其中多级编码器尝试将原始词汇、低级簇和高级簇映射到层次化嵌入表示，而多级解码器则利用注意力机制来利用这些层次化表示，然后生成响应。在每个解码阶段，该模型为高层解码器的输出和当前层编码器的隐藏状态计算两个注意力权重。Chen等人（[2019b](#bib.bib44)）计算了对话行为预测器输出的多头自注意力。不同于Transformer将不同头的输出连接起来，他们直接将输出传递到下一个多头层。然后，堆叠的多头层使用对话行为作为输入生成响应。
- en: Transformers for dialogue systems
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话系统中的Transformer
- en: Transformers are powerful sequence-to-sequence models and meanwhile, their encoders
    also serve as good dialogue representation models. Henderson et al. ([2019b](#bib.bib136))
    built a transformer-based response retrieval model for task-oriented dialogue
    systems. A two-channel transformer encoder was designed for encoding user messages
    and responses, both of which were initially presented as unigrams and bigrams.
    A simple cosine distance was then applied to calculate the semantic similarity
    between the user message and the candidate response. Li et al. ([2019d](#bib.bib208))
    built multiple incremental transformer encoders to encode multi-turn conversations
    and their related document knowledge. The encoded utterance and related document
    of the previous turn were treated as a part of the input of the next turn’s transformer
    encoder. The pretrained model was adaptable to multiple domains with only a small
    amount of data from the target domain. Bao et al. ([2019b](#bib.bib17)) used stacked
    transformers for dialogue generation pretraining. Besides the response generation
    task, they also pretrained the model together with a latent act prediction task.
    A latent variable was applied to solve the “one-to-many" problem in response generation.
    The multi-task training scheme improved the performance of the proposed transformer
    pretraining model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是强大的序列到序列模型，同时，它们的编码器也能作为良好的对话表示模型。Henderson 等人 ([2019b](#bib.bib136))
    构建了一个基于 Transformer 的响应检索模型，用于任务导向的对话系统。设计了一个双通道 Transformer 编码器，用于编码用户消息和响应，这些消息和响应最初以
    unigram 和 bigram 的形式呈现。随后，应用了简单的余弦距离来计算用户消息和候选响应之间的语义相似性。Li 等人 ([2019d](#bib.bib208))
    构建了多个增量 Transformer 编码器来编码多轮对话及其相关文档知识。将上一轮的编码发言和相关文档视为下一轮 Transformer 编码器的输入的一部分。预训练模型能够适应多个领域，仅需目标领域的一小部分数据。Bao
    等人 ([2019b](#bib.bib17)) 使用堆叠的 Transformer 进行对话生成预训练。除了响应生成任务外，他们还将模型与潜在行为预测任务一同预训练。应用潜在变量来解决响应生成中的“一对多”问题。多任务训练方案提高了所提出的
    Transformer 预训练模型的性能。
- en: Transformer-based pretrain models for dialogue systems
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于 Transformer 的对话系统预训练模型
- en: Large transformer-based pretrain models are adaptable to many tasks and are
    thus popular in recent works. Golovanov et al. ([2019](#bib.bib107)) used GPT
    as a sequence-to-sequence model to directly generate utterances and compared the
    performances under single- and multi-input settings. Majumder et al. ([2020b](#bib.bib239))
    first used a probability model to retrieve related news corpus and then combined
    the news corpus and dialogue context as input of a GPT-2 generator for response
    generation. They proposed that by using discourse pattern recognition and interrogative
    type prediction as two subtasks for multi-task learning, the dialogue modeling
    could be further improved. Wu et al. ([2019c](#bib.bib417)) used BERT as an encoder
    of context and candidate responses in their goal-based response retrieval system
    while Zhong et al. ([2020](#bib.bib460)) built Co-BERT, a BERT-based response
    selection model, to retrieve empathetic responses given persona-based training
    corpus. Zhao et al. ([2020b](#bib.bib459)) built a knowledge-grounded dialogue
    system in a synthesized fashion. They used both BERT and GPT-2 to perform knowledge
    selection and response generation jointly, where BERT was for knowledge selection
    and GPT-2 generated responses based on dialogue context and the selected knowledge.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 大型基于 Transformer 的预训练模型适用于许多任务，因此在近期工作中非常受欢迎。Golovanov 等人 ([2019](#bib.bib107))
    使用 GPT 作为序列到序列模型直接生成发言，并比较了单输入和多输入设置下的性能。Majumder 等人 ([2020b](#bib.bib239)) 首先使用概率模型检索相关新闻语料库，然后将新闻语料库和对话上下文结合作为
    GPT-2 生成器的输入以生成响应。他们提出，通过使用话语模式识别和疑问类型预测作为多任务学习的两个子任务，对话建模可以进一步改进。Wu 等人 ([2019c](#bib.bib417))
    使用 BERT 作为其目标导向响应检索系统中的上下文和候选响应的编码器，而 Zhong 等人 ([2020](#bib.bib460)) 构建了 Co-BERT，一种基于
    BERT 的响应选择模型，用于检索具有同理心的响应，前提是基于人格训练语料库。Zhao 等人 ([2020b](#bib.bib459)) 以合成的方式构建了一个知识驱动的对话系统。他们使用
    BERT 和 GPT-2 联合进行知识选择和响应生成，其中 BERT 用于知识选择，GPT-2 基于对话上下文和选择的知识生成响应。
- en: 2.6 Pointer Net and CopyNet
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 Pointer Net 和 CopyNet
- en: 2.6.1 Pointer Net
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1 Pointer Net
- en: 'In some NLP tasks like dialogue systems and question-answering, the agents
    sometimes need to directly quote from the user message. Pointer Net (Oriol et al.,
    [2015](#bib.bib265)) (Figure [8](#S2.F8 "Figure 8 ‣ 2.6.1 Pointer Net ‣ 2.6 Pointer
    Net and CopyNet ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep
    Learning Based Dialogue Systems: A Systematic Survey")) solved the problem of
    directly copying tokens from the input sentence.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '在一些自然语言处理任务中，如对话系统和问答系统，代理有时需要直接引用用户消息中的内容。Pointer Net（Oriol 等，[2015](#bib.bib265)）（图
    [8](#S2.F8 "Figure 8 ‣ 2.6.1 Pointer Net ‣ 2.6 Pointer Net and CopyNet ‣ 2 Neural
    Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")）解决了直接从输入句子中复制标记的问题。'
- en: '![Refer to caption](img/6879a5e0268e8d55746d25c3e89cb18a.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6879a5e0268e8d55746d25c3e89cb18a.png)'
- en: (a) Sequence-to-sequence
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 序列到序列
- en: '![Refer to caption](img/f785125339c6d05f709a9fc9001cd772.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f785125339c6d05f709a9fc9001cd772.png)'
- en: (b) Pointer Net
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Pointer Net
- en: 'Figure 8: (a) Sequence-to-sequence - The RNN (blue) processes the input sequence
    to produce a code vector, which is then used by the probability chain rule and
    another RNN to generate the output sequence (purple). The dimensionality of the
    problem determines the output dimensionality, which remains constant through training
    and inference. (b) Pointer Net - The input sequence is converted to a code (blue)
    by an encoding RNN, which is fed to the generating network (purple). The generating
    network generates a vector at each step that modulates a content-based attention
    process across inputs. The attention mechanism produces a softmax distribution
    with a dictionary size equal to the input length. (Oriol et al., [2015](#bib.bib265))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8： (a) 序列到序列 - RNN（蓝色）处理输入序列以生成一个编码向量，然后通过概率链规则和另一个 RNN 生成输出序列（紫色）。问题的维度决定了输出的维度，该维度在训练和推理过程中保持不变。
    (b) Pointer Net - 输入序列通过编码 RNN 转换为编码（蓝色），然后输入生成网络（紫色）。生成网络在每一步生成一个向量，调节基于内容的注意力过程。注意力机制产生一个
    softmax 分布，字典大小等于输入长度。（Oriol 等，[2015](#bib.bib265)）
- en: 'Traditional sequence-to-sequence models (Sutskever et al., [2014](#bib.bib352);
    Graves et al., [2014](#bib.bib111)) with an encoder-decoder structure map a source
    sentence to a target sentence. Generally, these models first map source sentence
    into hidden state vectors with an encoder, and then predict the output sequence
    based on the hidden states. The sequence prediction is accomplished step-by-step,
    each step predicting one token using greedy search or beam search. The overall
    sequence-to-sequence model can be described by the following probability model:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的序列到序列模型（Sutskever 等，[2014](#bib.bib352)；Graves 等，[2014](#bib.bib111)）采用编码器-解码器结构将源句子映射到目标句子。通常，这些模型首先使用编码器将源句子映射到隐藏状态向量，然后根据隐藏状态预测输出序列。序列预测是逐步完成的，每一步使用贪婪搜索或束搜索预测一个标记。总体的序列到序列模型可以用以下概率模型描述：
- en: '|  | $P(C^{P}&#124;P;\theta)=\prod_{i=1}^{m(P)}p(C_{i}&#124;C_{1},...,C_{i-1},P;\theta)$
    |  | (34) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(C^{P}&#124;P;\theta)=\prod_{i=1}^{m(P)}p(C_{i}&#124;C_{1},...,C_{i-1},P;\theta)$
    |  | (34) |'
- en: Where $(P,C_{p})$ constitutes a training pair, $P$ = $\{P_{1},...,P_{n}\}$ denotes
    the input sequence and $C_{p}$ = $\{C_{1},...,C_{m(p)}\}$ denotes the ground target
    sequence. $\theta$ is a decoder model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(P,C_{p})$ 构成一个训练对，$P$ = $\{P_{1},...,P_{n}\}$ 表示输入序列，而 $C_{p}$ = $\{C_{1},...,C_{m(p)}\}$
    表示目标序列。$\theta$ 是一个解码模型。
- en: 'The sequence-to-sequence models have the vanilla backbones and attention-based
    backbones. Vanilla models predict the target sequence based only on the last hidden
    state of the encoder and pass it across different decoder time steps. Such a mechanism
    restricts the information received by the decoder at each decoding stage. Attention-based
    models consider all hidden states of the encoder at each decoding step and calculate
    their importance when utilizing them. To compare the mechanism of Pointer Net
    and Attention, we present the equations explained in Section [2.2](#S2.SS2 "2.2
    Recurrent Neural Networks and Vanilla Sequence-to-sequence Models ‣ 2 Neural Models
    in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey") here again. The decoder predicts the token conditioned partially
    on the weighted sum of encoder hidden states $d_{i}$:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '序列到序列模型有普通骨干和基于注意力的骨干。普通模型仅基于编码器的最后一个隐藏状态预测目标序列，并在不同的解码时间步中传递。这样的机制限制了在每个解码阶段解码器接收的信息。基于注意力的模型在每个解码步骤中考虑编码器的所有隐藏状态，并在利用它们时计算它们的重要性。为了比较Pointer
    Net和Attention的机制，我们再次呈现了在第[2.2](#S2.SS2 "2.2 Recurrent Neural Networks and Vanilla
    Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey")节中解释的方程。解码器基于编码器隐藏状态$d_{i}$的加权和部分预测标记：'
- en: '|  | $d_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (35) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (35) |'
- en: 'Where $\alpha_{ij}$ is the normalized weight score:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{ij}$是归一化权重分数：
- en: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (36)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (36)
    |'
- en: '$e_{ij}$ is the similarity score between $s_{i-1}$ and $jth$ encoder hidden
    state $h_{j}$, where the score is predicted by the similarity model $a$:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: $e_{ij}$是$s_{i-1}$和第$j$个编码器隐藏状态$h_{j}$之间的相似度分数，该分数由相似度模型$a$预测：
- en: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (37) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (37) |'
- en: 'At each decoding step, both vanilla and attention-based sequence-to-sequence
    models predict a distribution over a fixed dictionary $X=\{x_{1},...,x_{n}\}$,
    where $x_{i}$ denotes the tokens and $n$ denotes the total count of different
    tokens in the training corpus. However, when copying words from the input sentence,
    we do not need such a large dictionary. Instead, $n$ equals to the number of tokens
    in the input sequence (including repeated ones) and is not fixed since it changes
    according to the length of the input sequence. Pointer Net made a simple change
    to the attention-based sequence-to-sequence models: instead of predicting the
    token distribution based on the weighted sum of encoder hidden states $d_{i}$,
    it directly used the normalized weights $\alpha_{i}$ as predicted distribution:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个解码步骤中，普通和基于注意力的序列到序列模型都会预测一个固定词典$X=\{x_{1},...,x_{n}\}$上的分布，其中$x_{i}$表示标记，$n$表示训练语料库中不同标记的总数。然而，在从输入句子中复制单词时，我们不需要如此大的词典。相反，$n$等于输入序列中的标记数量（包括重复的标记），并且不是固定的，因为它会根据输入序列的长度而变化。Pointer
    Net对基于注意力的序列到序列模型进行了一个简单的修改：它直接使用归一化权重$\alpha_{i}$作为预测分布，而不是基于编码器隐藏状态$d_{i}$的加权和来预测标记分布：
- en: '|  | $P(C_{i}&#124;C_{1},...,C_{i-1},P)=\alpha_{i}$ |  | (38) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(C_{i}\mid C_{1},...,C_{i-1},P)=\alpha_{i}$ |  | (38) |'
- en: Where $\alpha_{i}$ is a set of probability numbers $\{\alpha_{i}^{1},...,\alpha_{i}^{j}\}$
    which represents the probability distribution over the tokens of the input sequence.
    Obviously, the token prediction problem is now transformed into position prediction
    problem, where the model only needs to predict a position in the input sequence.
    This mechanism is like a pointer that points to its target, hence the name “Pointer
    Net".
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{i}$是一组概率数$\{\alpha_{i}^{1},...,\alpha_{i}^{j}\}$，它表示对输入序列中标记的概率分布。显然，标记预测问题现在转变为位置预测问题，模型只需要预测输入序列中的一个位置。这个机制就像一个指针指向它的目标，因此得名“Pointer
    Net”。
- en: 2.6.2 CopyNet
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.2 CopyNet
- en: '![Refer to caption](img/1264b992dafbcc189323d6c259383137.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1264b992dafbcc189323d6c259383137.png)'
- en: 'Figure 9: The overall architecture of CopyNet (Gu et al., [2016](#bib.bib114))'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：CopyNet的整体架构（Gu等，[2016](#bib.bib114)）
- en: 'In real-world applications, simply copying from the source message is not enough.
    Instead, in tasks like dialogue systems and QA, agents also require the ability
    to generate words that are not in the source sentence. CopyNet (Gu et al., [2016](#bib.bib114))
    (Figure [9](#S2.F9 "Figure 9 ‣ 2.6.2 CopyNet ‣ 2.6 Pointer Net and CopyNet ‣ 2
    Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")) was proposed to incorporate the copy mechanism
    into traditional sequence-to-sequence models. The model decides at each decoding
    stage whether to copy from the source or generate a new token not in the source.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际应用中，仅仅从源消息中复制是不够的。在对话系统和问答系统等任务中，代理还需要生成源句子中没有的单词的能力。CopyNet (Gu et al.,
    [2016](#bib.bib114)) (图 [9](#S2.F9 "Figure 9 ‣ 2.6.2 CopyNet ‣ 2.6 Pointer Net
    and CopyNet ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey")) 被提出以将复制机制融入传统的序列到序列模型。该模型在每个解码阶段决定是从源中复制还是生成一个源中不存在的新令牌。'
- en: 'The encoder of CopyNet is the same as that of a traditional sequence-to-sequence
    model, whereas the decoder has some differences compared with a traditional attention-based
    decoder. When predicting the token at time step $t$, it combines the probabilistic
    models of generate-mode and copy-mode:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: CopyNet 的编码器与传统的序列到序列模型相同，而解码器与传统的基于注意力的解码器相比有一些区别。在预测时间步 $t$ 的令牌时，它结合了生成模式和复制模式的概率模型：
- en: '|  | $P(y_{t}&#124;s_{t},y_{t-1},c_{t},M)=P_{g}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)+P_{c}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)$
    |  | (39) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{t}&#124;s_{t},y_{t-1},c_{t},M)=P_{g}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)+P_{c}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)$
    |  | (39) |'
- en: Where $t$ is the time step. $s_{t}$ is the decoder hidden state and $y_{t}$
    is the predicted token. $c_{t}$ and $M$ represent weighted sum of encoder hidden
    states and encoder hidden states respectively. $g$ and $c$ are generate-mode and
    copy-mode respectively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 是时间步。$s_{t}$ 是解码器隐藏状态，$y_{t}$ 是预测的令牌。$c_{t}$ 和 $M$ 分别表示编码器隐藏状态的加权和和编码器隐藏状态。$g$
    和 $c$ 分别表示生成模式和复制模式。
- en: Besides, though it still uses $y_{t-1}$ and weighted attention vector $c_{t}$
    to update the decoder hidden state, $y_{t-1}$ is uniquely encoded with both its
    embedding and its location-specific hidden state; also, CopyNet combines attentive
    read and selective read to capture information from the encoder hidden states,
    where the selective read is the same method used in Pointer Net. Different from
    the Neural Turing Machines (Graves et al., [2014](#bib.bib111); Kurach et al.,
    [2015](#bib.bib176)), the CopyNet has a location-based mechanism that enables
    the model to be aware of some specific details in training data in a more subtle
    way.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管它仍然使用 $y_{t-1}$ 和加权注意力向量 $c_{t}$ 来更新解码器隐藏状态，但 $y_{t-1}$ 是通过其嵌入和位置特定隐藏状态进行唯一编码的；同时，CopyNet
    结合了注意力读取和选择性读取来捕获来自编码器隐藏状态的信息，其中选择性读取是 Pointer Net 中使用的相同方法。与 Neural Turing Machines (Graves
    et al., [2014](#bib.bib111); Kurach et al., [2015](#bib.bib176)) 不同，CopyNet 具有基于位置的机制，使模型能够以更微妙的方式注意到训练数据中的一些特定细节。
- en: Copy mechanism is suitable for dialogues involving terminologies or external
    knowledge sources, and it is popular in knowledge-grounded or task-oriented dialogue
    systems.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 复制机制适用于涉及术语或外部知识源的对话，并且在知识驱动或任务导向的对话系统中很受欢迎。
- en: Copy mechanism for knowledge-grounded dialogue systems
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识驱动对话系统的复制机制
- en: 'For knowledge-grounded systems, external documents or dialogues are sources
    to copy from. Lin et al. ([2020a](#bib.bib214)) combined a recurrent knowledge
    interactive decoder with a knowledge-aware pointer network to achieve both knowledge-grounded
    generation and knowledge copy. In the proposed model, they first calculated the
    attention distribution over external knowledge, then used two pointers referring
    to dialogue context and knowledge source respectively to copy out-of-vocabulary
    (OOV) words. Wu et al. ([2020b](#bib.bib416)) applied a multi-class classifier
    to flexibly fuse three distributions: generated words, generated knowledge entities,
    and copied query words. They used Context-Knowledge Fusion and Flexible Mode Fusion
    to perform the knowledge retrieval, response generation, and copying jointly,
    making the generated responses precise, coherent, and knowledge-infused. Ji et al.
    ([2020](#bib.bib155)) proposed a Cross Copy Network to copy from internal utterance
    (dialogue history) and external utterance (similar cases) respectively. They first
    used pretrained language models for similar case retrieval, then combined the
    probability distribution of two pointers to make a prediction. They only experimented
    with court debate and customer service content generation tasks, where similar
    cases were easy to obtain.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于知识驱动的系统，外部文档或对话是复制的来源。Lin 等人（[2020a](#bib.bib214)）结合了递归知识交互解码器和知识感知指针网络，以实现知识驱动生成和知识复制。在提出的模型中，他们首先计算了对外部知识的注意力分布，然后使用两个指针分别指向对话上下文和知识源，以复制词汇表外（OOV）单词。Wu
    等人（[2020b](#bib.bib416)）应用了一个多类分类器，以灵活地融合三种分布：生成的单词、生成的知识实体和复制的查询单词。他们使用了上下文-知识融合和灵活模式融合来共同执行知识检索、响应生成和复制，使生成的响应准确、一致且富有知识性。Ji
    等人（[2020](#bib.bib155)）提出了一种交叉复制网络，分别从内部发言（对话历史）和外部发言（类似案例）中进行复制。他们首先使用预训练语言模型进行类似案例检索，然后结合两个指针的概率分布来进行预测。他们只在法庭辩论和客户服务内容生成任务中进行了实验，其中类似案例容易获得。
- en: Copy mechanism for task-oriented dialogue systems
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 针对任务导向对话系统的复制机制
- en: Many dialogue state tracking tasks generate slots and slot values using a copy
    component (Wu et al., [2019a](#bib.bib413); Ouyang et al., [2020](#bib.bib266);
    Gangadharaiah and Narayanaswamy, [2020](#bib.bib97); Chen et al., [2020a](#bib.bib41);
    Zhang et al., [2020](#bib.bib452); Li et al., [2020d](#bib.bib207)). Among them Wu
    et al. ([2019a](#bib.bib413)), Ouyang et al. ([2020](#bib.bib266)) and Chen et al.
    ([2020a](#bib.bib41)) solved the problem of multi-domain dialogue state tracking.
    Wu et al. ([2019a](#bib.bib413)) proposed TRAnsferable Dialogue statE generator
    (TRADE), a copy-based dialogue state generator. The generator decoded the slot
    value multiple times for each possible (domain, slot) pair, then a slot gate was
    applied to decide which pair belonged to the dialogue. The output distribution
    was a copy of the slot values belonging to the selected (domain, slot) pairs from
    vocabulary and dialogue history. Chen et al. ([2020a](#bib.bib41)) used a different
    copy strategy from TRADE. Instead of using the whole dialogue history as the copy
    source, they copied state values from user utterances and system messages respectively,
    which took the slot-level context as input. Ouyang et al. ([2020](#bib.bib266))
    proposed slot connection mechanism to efficiently utilize existing states from
    other domains. Attention weights were calculated to measure the connection between
    the target slot and related slot-value tuples in other domains. Three distributions
    over token generation, dialogue context copying, and past state copying were finally
    gated and fused to predict the next token. Gangadharaiah and Narayanaswamy ([2020](#bib.bib97))
    combined a pointer network with a template-based tree decoder to fill the templates
    recursively and hierarchically. Copy mechanisms also alleviated the problem of
    expensive data annotation in end-to-end task-oriented dialogue systems. Copy-augmented
    dialogue generation models were proven to perform significantly better than strong
    baselines with limited domain-specific or multi-domain data (Zhang et al., [2020](#bib.bib452);
    Li et al., [2020d](#bib.bib207); Gao et al., [2020a](#bib.bib99)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 许多对话状态追踪任务使用复制组件生成槽和槽值（Wu等，[2019a](#bib.bib413)；Ouyang等，[2020](#bib.bib266)；Gangadharaiah和Narayanaswamy，[2020](#bib.bib97)；Chen等，[2020a](#bib.bib41)；Zhang等，[2020](#bib.bib452)；Li等，[2020d](#bib.bib207)）。其中Wu等（[2019a](#bib.bib413)），Ouyang等（[2020](#bib.bib266)）和Chen等（[2020a](#bib.bib41)）解决了多领域对话状态追踪的问题。Wu等（[2019a](#bib.bib413)）提出了可转移对话状态生成器（TRADE），一种基于复制的对话状态生成器。生成器对每个可能的（领域，槽）对进行多次解码，然后应用槽门来决定哪个对属于对话。输出分布是从词汇表和对话历史中选择的（领域，槽）对的槽值的复制。Chen等（[2020a](#bib.bib41)）使用了与TRADE不同的复制策略。他们没有使用整个对话历史作为复制源，而是分别从用户话语和系统消息中复制状态值，这将槽级上下文作为输入。Ouyang等（[2020](#bib.bib266)）提出了槽连接机制，以有效利用来自其他领域的现有状态。计算了注意力权重以衡量目标槽与其他领域中相关槽值元组之间的连接。最终，对代币生成、对话上下文复制和过去状态复制的三个分布进行了门控和融合，以预测下一个代币。Gangadharaiah和Narayanaswamy（[2020](#bib.bib97)）将指针网络与基于模板的树解码器相结合，递归和分层地填充模板。复制机制还缓解了端到端任务导向对话系统中数据注释昂贵的问题。经过复制增强的对话生成模型被证明在有限的领域特定或多领域数据下表现显著优于强基线（Zhang等，[2020](#bib.bib452)；Li等，[2020d](#bib.bib207)；Gao等，[2020a](#bib.bib99)）。
- en: Copy mechanism for dialogue-related tasks
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话相关任务的复制机制
- en: Pointer networks and CopyNet are also used to solve other dialogue-related tasks.
    Yu and Joty ([2020](#bib.bib441)) applied a pointer net for online conversation
    disentanglement. The pointer module pointed to the ancestor message to which the
    current message replies and a classifier predicted whether two messages belonged
    to the same thread. In dialogue parsing tasks, the pointer net is used as the
    backbone parsing model to construct discourse trees (Aghajanyan et al., [2020](#bib.bib2);
    Lin et al., [2019](#bib.bib213)). Tay et al. ([2019](#bib.bib365)) used a pointer-generator
    framework to perform machine reading comprehension over a long span, where the
    copy mechanism reduced the demand of including target answers in context.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 指针网络和CopyNet也用于解决其他对话相关任务。Yu和Joty（[2020](#bib.bib441)）应用了指针网络进行在线对话分离。指针模块指向当前消息回复的祖先消息，分类器预测两条消息是否属于同一线程。在对话解析任务中，指针网络作为主干解析模型用于构建话语树（Aghajanyan等，[2020](#bib.bib2)；Lin等，[2019](#bib.bib213)）。Tay等（[2019](#bib.bib365)）使用了指针-生成器框架在长跨度上进行机器阅读理解，其中复制机制减少了在上下文中包含目标答案的需求。
- en: 2.7 Deep Reinforcement Learning Models and Generative Adversarial Networks
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 深度强化学习模型与生成对抗网络
- en: In recent years, two exciting approaches exhibit the potential of artificial
    intelligence. The first one is deep reinforcement learning, which outperforms
    humans in many complex problems such as large-scale games, conversations, and
    car-driving. Another technique is GAN, showing amazing capability in generation
    tasks. The data samples generated by GAN models like articles, paintings, and
    even videos, are sometimes indistinguishable from human creations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，有两种令人兴奋的方法展示了人工智能的潜力。第一种是深度强化学习，它在许多复杂问题上超越了人类，如大规模游戏、对话和汽车驾驶。另一种技术是 GAN，展现了在生成任务中的惊人能力。由
    GAN 模型生成的数据样本，如文章、画作甚至视频，有时与人类创作的难以区分。
- en: AlphaGo (Silver et al., [2016](#bib.bib327)) stimulated the research interests
    again in reinforcement learning in recent years (Graves et al., [2016](#bib.bib112);
    Mnih et al., [2016](#bib.bib253); Wang et al., [2016](#bib.bib394); Tamar et al.,
    [2016](#bib.bib358); Jaderberg et al., [2016](#bib.bib152); Mirowski et al., [2016](#bib.bib251)).
    Reinforcement learning is a branch of machine learning aiming to train agents
    to perform appropriate actions while interacting with a certain environment. It
    is one of the three fundamental machine learning branches, with supervised learning
    and unsupervised learning being the other two. It can also be seen as an intermediate
    between supervised learning and unsupervised learning because it only needs weak
    signals for training (Wang et al., [2016](#bib.bib394)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo（Silver et al., [2016](#bib.bib327)）在近年来再次激发了对强化学习的研究兴趣（Graves et al.,
    [2016](#bib.bib112)；Mnih et al., [2016](#bib.bib253)；Wang et al., [2016](#bib.bib394)；Tamar
    et al., [2016](#bib.bib358)；Jaderberg et al., [2016](#bib.bib152)；Mirowski et
    al., [2016](#bib.bib251)）。强化学习是机器学习的一个分支，旨在训练代理在与某个环境互动时执行适当的动作。它是机器学习的三个基本分支之一，另两个是监督学习和无监督学习。它也可以被视为监督学习和无监督学习之间的中间体，因为它只需要弱信号进行训练（Wang
    et al., [2016](#bib.bib394)）。
- en: '![Refer to caption](img/db8bbcf7bcb7e80032ef140aea094e73.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/db8bbcf7bcb7e80032ef140aea094e73.png)'
- en: 'Figure 10: The reinforcement learning framework'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：强化学习框架
- en: 'Figure [10](#S2.F10 "Figure 10 ‣ 2.7 Deep Reinforcement Learning Models and
    Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey") illustrates
    the reinforcement learning framework, consisting of an agent and an environment.
    The framework is a Markov Decision Process (MDP) (Puterman, [2014](#bib.bib278)),
    which can be described by a five-tuple M = $\langle S,A,P,R,\gamma\rangle$. $S$
    denotes an infinite set of environment states; $A$ denotes a set of actions that
    agent chooses from conditioned on a given environment state $s$; $P$ is the transition
    probability matrix in MDP, denoting the probability of an environment state transfer
    after agent takes an action; $R$ is an average reward the agent receives from
    the environment after taking an action under state $s$; $\gamma$ is a discount
    factor. The flow of this framework is a loop of the following two steps: the agent
    first makes an observation on the current environment state $s_{t}$ and chooses
    an action based on its policy; then according to the transition probability matrix
    $P$, the environment’s state transfers to $s_{t+1}$, and simultaneously provides
    a reward $r_{t}$.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](#S2.F10 "图 10 ‣ 2.7 深度强化学习模型与生成对抗网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述")展示了强化学习框架，包括一个代理和一个环境。该框架是一个马尔可夫决策过程（MDP）（Puterman，[2014](#bib.bib278)），可以用一个五元组
    M = $\langle S,A,P,R,\gamma\rangle$ 来描述。$S$ 代表环境状态的无限集合；$A$ 代表代理根据给定环境状态 $s$ 选择的动作集合；$P$
    是 MDP 中的转移概率矩阵，表示代理采取动作后环境状态转移的概率；$R$ 是代理在状态 $s$ 下采取动作后从环境中获得的平均奖励；$\gamma$ 是折扣因子。该框架的流程是以下两个步骤的循环：代理首先对当前环境状态
    $s_{t}$ 进行观察，并根据其策略选择一个动作；然后根据转移概率矩阵 $P$，环境的状态转移到 $s_{t+1}$，同时提供奖励 $r_{t}$。
- en: Reinforcement learning is applicable to solve many challenges in dialogue systems
    because of the agent-environment nature of a dialogue system. A two-party dialogue
    system consists of an agent, which is an intelligent chatbot, and an environment,
    which is usually a user or a user simulator. Here we mainly discuss deep reinforcement
    learning.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习适用于解决对话系统中的许多挑战，因为对话系统具有代理-环境的特性。一个双方对话系统由一个智能聊天机器人（代理）和一个用户或用户模拟器（环境）组成。在这里，我们主要讨论深度强化学习。
- en: Deep reinforcement learning means applying deep neural networks to model the
    value function or policy of the reinforcement learning framework. “Deep model"
    is in contrast to the “shallow model". The shallow model normally refers to traditional
    machine learning models like Decision Trees or KNN. Feature engineering, which
    is usually based on shallow models, is time and labor consuming, and also over-specified
    and incomplete. Different from that, deep neural models are easy to design and
    have a strong fitting capability, which contributes to many breakthroughs in recent
    research. Deep representation learning gets rid of human labor and exploits hierarchical
    features in data automatically, which strengthens the semantic expressiveness
    and domain correlations significantly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习意味着将深度神经网络应用于建模强化学习框架中的价值函数或策略。“深度模型”与“浅层模型”相对。浅层模型通常指传统的机器学习模型，如决策树或
    KNN。特征工程，通常基于浅层模型，既耗时又耗力，同时也过于具体和不完整。与此不同的是，深度神经模型设计简便，并具有强大的拟合能力，这促进了近年来许多突破。深度表示学习摆脱了人工劳动，并自动利用数据中的层次特征，显著增强了语义表达性和领域相关性。
- en: 'We discuss two typical reinforcement models: Deep Q-Networks (Mnih et al.,
    [2015](#bib.bib252)) and REINFORCE (Williams, [1992](#bib.bib409); Sutton et al.,
    [1999](#bib.bib354)). They belong to Q-learning and policy gradient respectively,
    which are two families of reinforcement learning.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了两种典型的强化学习模型：Deep Q-Networks（Mnih et al., [2015](#bib.bib252)）和 REINFORCE（Williams,
    [1992](#bib.bib409); Sutton et al., [1999](#bib.bib354)）。它们分别属于 Q-learning 和策略梯度，这两种强化学习的家族。
- en: 2.7.1 Deep Q-Networks
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.1 深度 Q 网络
- en: 'A Deep Q-Network is a value-based RL model. It determines the best policy according
    to the Q-function:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 网络是一个基于价值的强化学习模型。它根据 Q 函数确定最佳策略：
- en: '|  | $\pi^{*}(s)=arg\max_{a}Q^{*}(s,a)$ |  | (40) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}(s)=arg\max_{a}Q^{*}(s,a)$ |  | (40) |'
- en: Where $Q^{*}(s,a)$ is an optimal Q-function and $\pi^{*}(s)$ is the corresponding
    optimal policy. In Deep Q-Networks, the Q function is modeled using a deep neural
    network, such as CNNs, RNNs, etc.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q^{*}(s,a)$ 是一个最优 Q 函数，$\pi^{*}(s)$ 是相应的最优策略。在深度 Q 网络中，Q 函数使用深度神经网络建模，例如
    CNNs、RNNs 等。
- en: 'As in Gao et al. ([2018](#bib.bib98)), the parameters of the Q model are updated
    using the rule:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如 Gao et al.（[2018](#bib.bib98)）所述，Q 模型的参数使用以下规则更新：
- en: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta)-Q(s_{t},a_{t};\theta)\right)}_{\text{temporal\
    difference}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$ |  | (41) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta)-Q(s_{t},a_{t};\theta)\right)}_{\text{时间差分}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$
    |  | (41) |'
- en: Where the $(s_{t},a_{t},r_{t},s_{t+1})$ is an observed trajectory. $\alpha$
    denotes step-size and the parameter update is calculated using temporal difference (Sutton,
    [1988](#bib.bib353)). However, this update mechanism suffers from unstableness
    and demands a large number of training samples. There are two typical tricks for
    a more efficient and stable parameter update.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(s_{t},a_{t},r_{t},s_{t+1})$ 是一个观测到的轨迹。$\alpha$ 表示步长，参数更新是通过时间差分（Sutton,
    [1988](#bib.bib353)）来计算的。然而，这种更新机制存在不稳定性，并且需要大量的训练样本。为了实现更高效和稳定的参数更新，有两种典型的技巧。
- en: The first method is experience replay (Lin, [1992](#bib.bib211); Mnih et al.,
    [2015](#bib.bib252)). Instead of using one training sample at a time to update
    the parameters, it uses a buffer to store training samples, and iteratively retrieves
    training samples from the buffer pool to perform parameter updates. It avoids
    encountering training samples that change too fast in distribution during training
    time, which increases the learning stability; further, it uses each training sample
    multiple times, which improves the efficiency.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是经验回放（Lin, [1992](#bib.bib211); Mnih et al., [2015](#bib.bib252)）。它不是一次使用一个训练样本来更新参数，而是使用一个缓冲区来存储训练样本，并从缓冲池中迭代地检索训练样本进行参数更新。它避免了在训练期间遇到分布变化过快的训练样本，从而提高了学习的稳定性；此外，它多次使用每个训练样本，从而提高了效率。
- en: 'The second is two-network implementation (Mnih et al., [2015](#bib.bib252)).
    This method uses two networks in Q-function optimization, one being the Q-network,
    another being a target network. The target network is used to calculate the temporal
    difference, and its parameters $\theta_{target}$ are frozen while training, aligning
    with $\theta$ periodically. The parameters are then updated with the following
    rule:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是双网络实现（Mnih et al., [2015](#bib.bib252)）。该方法在 Q 函数优化中使用两个网络，一个是 Q 网络，另一个是目标网络。目标网络用于计算时间差异，其参数
    $\theta_{target}$ 在训练时保持不变，定期与 $\theta$ 对齐。然后使用以下规则更新参数：
- en: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta_{target})-Q(s_{t},a_{t};\theta)\right)}_{\text{temporal\
    difference\ with\ a\ target\ network}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$
    |  | (42) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta_{target})-Q(s_{t},a_{t};\theta)\right)}_{\text{带有目标网络的时间差}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$
    |  | (42) |'
- en: Since $\theta_{target}$ does not change in a period of time, the target network
    calculates the temporal difference in a stable manner, which facilitates the convergence
    of training.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\theta_{target}$ 在一段时间内不会改变，目标网络以稳定的方式计算时间差异，这有助于训练的收敛。
- en: 2.7.2 REINFORCE
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.2 REINFORCE
- en: 'REINFORCE is a policy-based RL algorithm that has no value network. It optimizes
    the policy directly. The policy is parameterized by a policy network, whose output
    is a distribution over continuous or discrete actions. A long-term reward is computed
    for evaluation of the policy network by collecting trajectory samples of length
    $H$:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 是一种基于策略的强化学习算法，没有价值网络。它直接优化策略。策略由一个策略网络参数化，其输出是对连续或离散动作的分布。通过收集长度为
    $H$ 的轨迹样本来计算长期奖励，以评估策略网络：
- en: '|  | $J(\theta)=E\left[\sum_{t=1}^{H}\gamma^{t-1}r_{t}&#124;a_{t}\sim\pi(s_{t};\theta)\right]$
    |  | (43) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=E\left[\sum_{t=1}^{H}\gamma^{t-1}r_{t}\vert a_{t}\sim\pi(s_{t};\theta)\right]$
    |  | (43) |'
- en: '$J(\theta)$ denotes a long-term reward and the goal is to optimize the policy
    network in order to maximize $J(\theta)$. Here stochastic gradient ascent⁷⁷7Stochastic
    gradient ascent simply uses the negated objective function of stochastic gradient
    descent. is used as an optimizer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: $J(\theta)$ 表示长期奖励，目标是优化策略网络以最大化 $J(\theta)$。这里使用**随机梯度上升**作为优化器：
- en: '|  | $\theta\leftarrow\theta+\alpha\bigtriangledown_{\theta}J(\theta)$ |  |
    (44) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\bigtriangledown_{\theta}J(\theta)$ |  |
    (44) |'
- en: 'Where $\bigtriangledown_{\theta}J(\theta)$ is computed by:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bigtriangledown_{\theta}J(\theta)$ 通过以下方式计算：
- en: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}&#124;s_{t};\theta)\sum_{h=t}^{H}\gamma^{h-t}r_{h}\right)$
    |  | (45) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}\vert
    s_{t};\theta)\sum_{h=t}^{H}\gamma^{h-t}r_{h}\right)$ |  | (45) |'
- en: 'Both models have their advantages: Deep Q-Networks are more sample efficient
    while REINFORCE is more stable (Li, [2017](#bib.bib205)). REINFORCE is more popular
    in recent works. Modern research involves larger action spaces, which means that
    value-based RL models like Deep Q-Networks are not suitable for problem-solving.
    Value-based methods “select an action to maximize the value", which means that
    their action sets should be discrete and moderate in scale; while policy gradient
    methods such as REINFORCE are different, they predict the action via policy networks
    directly, which sets no restriction on the action space. As a result, policy gradient
    methods are more suitable for tasks involving a larger action space.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 两种模型各有优点：深度Q网络在样本效率上更高，而REINFORCE则更稳定（Li, [2017](#bib.bib205)）。在近期的研究中，REINFORCE更为流行。现代研究涉及更大的动作空间，这意味着像深度Q网络这样的基于值的强化学习模型不适合解决这些问题。基于值的方法“选择一个动作以最大化价值”，这意味着它们的动作集合应该是离散且规模适中的；而像REINFORCE这样的策略梯度方法则不同，它们通过策略网络直接预测动作，对动作空间没有限制。因此，策略梯度方法更适合处理涉及更大动作空间的任务。
- en: 'Considering the respective benefits brought by the Q-learning and policy gradient,
    some work has been done combining the value- and policy-based methods. Actor-critic
    algorithm (Konda and Tsitsiklis, [2000](#bib.bib172); Sutton et al., [1999](#bib.bib354))
    was proposed to alleviate the severe variance problem when calculating the gradient
    in policy gradient methods. It estimates a value function for term $\sum_{h=t}^{H}\gamma^{h-t}r_{h}$
    in Equation ([45](#S2.E45 "In 2.7.2 REINFORCE ‣ 2.7 Deep Reinforcement Learning
    Models and Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey"))
    and incorporates it in policy optimization. Equation ([45](#S2.E45 "In 2.7.2 REINFORCE
    ‣ 2.7 Deep Reinforcement Learning Models and Generative Adversarial Networks ‣
    2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")) is then transformed into the formula below:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Q-learning和策略梯度各自带来的好处，一些工作已将基于值的方法与基于策略的方法结合起来。演员-评论家算法（Konda and Tsitsiklis,
    [2000](#bib.bib172); Sutton et al., [1999](#bib.bib354)）被提出以缓解在策略梯度方法中计算梯度时的严重方差问题。它为方程式中的项$\sum_{h=t}^{H}\gamma^{h-t}r_{h}$进行值函数估计，并将其纳入策略优化。方程式
    ([45](#S2.E45 "在2.7.2 REINFORCE ‣ 2.7 深度强化学习模型与生成对抗网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统评述"))
    然后被转换为如下公式：
- en: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}&#124;s_{t};\theta)\hat{Q}(s_{t},a_{t},h)\right)$
    |  | (46) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}|s_{t};\theta)\hat{Q}(s_{t},a_{t},h)\right)$
    |  | (46) |'
- en: Where $\hat{Q}(s_{t},a_{t},h)$ stands for the value function estimated.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{Q}(s_{t},a_{t},h)$ 代表估计的值函数。
- en: 2.7.3 GANs
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.3 GANs
- en: It is easy to link the actor-critic model with another framework - GANs (Goodfellow
    et al., [2014](#bib.bib108); Zhang et al., [2018c](#bib.bib451); Feng et al.,
    [2020a](#bib.bib87)) because of their similar inner structure and logic (Pfau
    and Vinyals, [2016](#bib.bib274)). Actually, there are quite a few recent works
    in dialogue systems that train GANs with reinforcement learning framework (Zhu
    et al., [2018](#bib.bib466); Wu et al., [2019b](#bib.bib415); He et al., [2020a](#bib.bib128);
    Zhu et al., [2020](#bib.bib467); Qin et al., [2020](#bib.bib281)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其相似的内部结构和逻辑，容易将演员-评论家模型与另一框架——GANs （Goodfellow et al., [2014](#bib.bib108);
    Zhang et al., [2018c](#bib.bib451); Feng et al., [2020a](#bib.bib87)）进行联系（Pfau
    and Vinyals, [2016](#bib.bib274)）。实际上，近年来有相当多的对话系统研究利用强化学习框架训练GANs（Zhu et al.,
    [2018](#bib.bib466); Wu et al., [2019b](#bib.bib415); He et al., [2020a](#bib.bib128);
    Zhu et al., [2020](#bib.bib467); Qin et al., [2020](#bib.bib281)）。
- en: '![Refer to caption](img/150fae7ed08922d0dd788e68bb1aea35.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/150fae7ed08922d0dd788e68bb1aea35.png)'
- en: 'Figure 11: The GAN framework'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：GAN框架
- en: 'Figure [11](#S2.F11 "Figure 11 ‣ 2.7.3 GANs ‣ 2.7 Deep Reinforcement Learning
    Models and Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    represents the GAN consisting of a generator and a discriminator where the training
    process can be viewed as a competition between them: the generator tries to generate
    data distributions to fool the discriminator while the discriminator attempts
    to distinguish between real data (real) and generated data (fake). During training,
    the generator takes noise as input and generates data distribution while the discriminator
    takes real and fake data as input and the binary annotation as the label. The
    whole GAN model is trained end-to-end as a connection of generator and discriminator
    to minimize the following cross-entropy losses:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#S2.F11 "Figure 11 ‣ 2.7.3 GANs ‣ 2.7 Deep Reinforcement Learning Models
    and Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")展示了包含生成器和判别器的GAN，其中训练过程可以被视为它们之间的竞争：生成器试图生成数据分布以欺骗判别器，而判别器则试图区分真实数据（真实）和生成的数据（伪造）。在训练过程中，生成器以噪声作为输入生成数据分布，而判别器则以真实和伪造的数据作为输入，二分类标注作为标签。整个GAN模型作为生成器和判别器的连接进行端到端训练，以最小化以下交叉熵损失：'
- en: '|  | $L_{1}(D,G)=-E_{\omega\sim P_{data}}[logD(\omega)]-E_{z\sim N(0,I)}[log(1-D(G(z)))]$
    |  | (47) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{1}(D,G)=-E_{\omega\sim P_{data}}[logD(\omega)]-E_{z\sim N(0,I)}[log(1-D(G(z)))]$
    |  | (47) |'
- en: '|  | $L_{2}(D,G)=-E_{z\sim N(0,I)}[logD(G(z))]$ |  | (48) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{2}(D,G)=-E_{z\sim N(0,I)}[logD(G(z))]$ |  | (48) |'
- en: Where $L_{1}$ and $L_{2}$ denote a bilevel loss, where $D$ and $G$ being discriminator
    and generator respectively. $z\sim N(0,I)$ is the noise input of the generator
    and $w$ is the input of the discriminator.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{1}$ 和 $L_{2}$ 表示一个双层损失，其中 $D$ 和 $G$ 分别为判别器和生成器。$z\sim N(0,I)$ 是生成器的噪声输入，$w$
    是判别器的输入。
- en: Relationship between RL and GAN
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RL与GAN的关系
- en: GAN can be viewed as a special actor-critic (Pfau and Vinyals, [2016](#bib.bib274)).
    In the learning architecture of GAN, the generator acts as the actor and the discriminator
    acts as the critic or environment which gives the real/fake feedback as a reward.
    However, the actions taken by the actor cannot change the states of the environment,
    which means that the learning architecture of GAN is a stateless Markov decision
    process. Also, the actor has no access to the state of the environment and generates
    data distribution simply conditioned on Gaussian noise, which means that the generator
    in the GAN framework is a blind actor/agent. In a nutshell, GAN is a special actor-critic
    where the actor is blind and the whole process is a stateless MDP.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: GAN可以被视为一种特殊的演员-评论员(Pfau and Vinyals, [2016](#bib.bib274))。在GAN的学习架构中，生成器充当演员，判别器充当评论员或环境，提供真实/伪造的反馈作为奖励。然而，演员所采取的行动不能改变环境的状态，这意味着GAN的学习架构是一个无状态的马尔可夫决策过程。同时，演员无法访问环境的状态，只是基于高斯噪声生成数据分布，这意味着GAN框架中的生成器是一个盲目的演员/代理。总而言之，GAN是一种特殊的演员-评论员，其中演员是盲目的，整个过程是一个无状态的MDP。
- en: The interactive nature of dialogue systems motivates the wide application of
    reinforcement learning and GAN models in its research.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统的互动特性激励了在其研究中广泛应用强化学习和GAN模型。
- en: RL for task-oriented dialogue systems
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 面向任务的对话系统中的RL
- en: 'One common application of reinforcement learning in dialogue systems is the
    reinforced dialogue management in task-oriented systems. Dialogue state tracking
    and policy learning are two typical modules of a dialogue manager. Huang et al.
    ([2020c](#bib.bib150)) and Li et al. ([2020d](#bib.bib207)) trained the dialogue
    state tracker with reinforcement learning. Both of them combined a reward manager
    into their tracker to enhance tracking accuracy. For the policy learning module,
    reinforcement learning seems to be the best choice since almost all recent related
    works learned policy with reinforcement learning (Zhang et al., [2019c](#bib.bib454);
    Wang et al., [2020d](#bib.bib387); Zhu et al., [2020](#bib.bib467); Wang et al.,
    [2020a](#bib.bib384); Takanobu et al., [2020](#bib.bib356); Huang et al., [2020b](#bib.bib149);
    Xu et al., [2020a](#bib.bib426)). The increasing preference of reinforcement learning
    in policy learning tasks attributes to the characteristic of them: in policy learning
    tasks, the model predicts a dialogue action (action) based on the states from
    the DST module (state), which perfectly accords with the function of the agent
    in the reinforcement learning framework.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在对话系统中的一个常见应用是任务导向系统中的强化对话管理。对话状态追踪和策略学习是对话管理器的两个典型模块。黄等人 ([2020c](#bib.bib150))
    和李等人 ([2020d](#bib.bib207)) 使用强化学习训练了对话状态追踪器。他们都在追踪器中结合了奖励管理器，以提高追踪准确性。对于策略学习模块，强化学习似乎是最佳选择，因为几乎所有近期相关的研究都通过强化学习来学习策略
    (张等人，[2019c](#bib.bib454)；王等人，[2020d](#bib.bib387)；朱等人，[2020](#bib.bib467)；王等人，[2020a](#bib.bib384)；高信等人，[2020](#bib.bib356)；黄等人，[2020b](#bib.bib149)；徐等人，[2020a](#bib.bib426))。强化学习在策略学习任务中的日益偏好归因于其特点：在策略学习任务中，模型基于来自
    DST 模块的状态 (state) 预测对话动作 (action)，这与强化学习框架中代理的功能完全一致。
- en: RL for open-domain dialogue systems
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RL 在开放域对话系统中的应用
- en: Due to the huge action space needed to generate language directly, many open-domain
    dialogue systems trained with reinforcement learning framework do not generate
    responses but instead select responses. Retrieval-based systems have a limited
    action set and are suitable to be trained in a reinforcement learning scheme.
    Some works achieved promising performance in retrieval-based dialogue tasks (Bouchacourt
    and Baroni, [2019](#bib.bib27); Li et al., [2016a](#bib.bib192); Zhao and Eskenazi,
    [2016](#bib.bib455)). However, retrieval systems fail to generalize in all user
    messages and may give unrelated responses (Qiu et al., [2017](#bib.bib284)), which
    makes generation-based dialogue systems preferable. Still considering the action
    space problem, some works build their systems combining retrieval and generative
    methods (Zhu et al., [2018](#bib.bib466); Serban et al., [2017b](#bib.bib314)).
    Zhu et al. ([2018](#bib.bib466)) chose to first retrieve a set of n-best response
    candidates and then generated responses based on the retrieved results and user
    message. Comparatively, Serban et al. ([2017b](#bib.bib314)) first generated and
    retrieved candidate responses with different dialogue models and then trained
    a scoring model with online reinforcement learning to select responses from both
    generated and retrieved responses. Since training a generative dialogue agent
    using reinforcement learning from scratch is particularly difficult, first pretraining
    the agent with supervised learning to warm-start is a good choice. Wu et al. ([2019b](#bib.bib415)), He
    et al. ([2020a](#bib.bib128)), Williams and Zweig ([2016](#bib.bib407)) and Yao
    et al. ([2016](#bib.bib433)) applied this pretrain-and-finetune strategy on dialogue
    learning and achieved outstanding performance, which proved that the reinforcement
    learning can improve the response quality of data-driven chatbots. Similarly,
    pretrain-and-finetune was also applicable to domain transfer problems. Some works
    pretrained the model in a source domain and expanded the domain area with reinforcement
    training (Mo et al., [2018](#bib.bib254); Li et al., [2016d](#bib.bib195)).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接生成语言需要庞大的动作空间，许多使用强化学习框架训练的开放域对话系统不生成回应，而是选择回应。基于检索的系统有一个有限的动作集，适合在强化学习方案中训练。一些研究在基于检索的对话任务中取得了令人鼓舞的成果（Bouchacourt
    和 Baroni，[2019](#bib.bib27)；Li 等，[2016a](#bib.bib192)；Zhao 和 Eskenazi，[2016](#bib.bib455)）。然而，检索系统无法在所有用户消息中进行泛化，可能给出无关的回应（Qiu
    等，[2017](#bib.bib284)），这使得生成式对话系统更为可取。尽管考虑到动作空间问题，一些研究通过结合检索和生成方法构建系统（Zhu 等，[2018](#bib.bib466)；Serban
    等，[2017b](#bib.bib314)）。Zhu 等（[2018](#bib.bib466)）选择首先检索一组n最佳回应候选，然后基于检索结果和用户消息生成回应。相比之下，Serban
    等（[2017b](#bib.bib314)）首先生成和检索不同对话模型的候选回应，然后用在线强化学习训练评分模型，从生成和检索的回应中选择回应。由于从头开始使用强化学习训练生成对话代理特别困难，先用监督学习进行预训练以热启动是一个不错的选择。Wu
    等（[2019b](#bib.bib415)），He 等（[2020a](#bib.bib128)），Williams 和 Zweig（[2016](#bib.bib407)）以及Yao
    等（[2016](#bib.bib433)）在对话学习中应用了这种预训练和微调策略，并取得了出色的成果，这证明了强化学习可以提高数据驱动聊天机器人的回应质量。同样，预训练和微调也适用于领域迁移问题。一些研究在源领域中预训练模型，并通过强化训练扩展领域范围（Mo
    等，[2018](#bib.bib254)；Li 等，[2016d](#bib.bib195)）。
- en: RL for knowledge grounded dialogue systems
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识驱动对话系统的强化学习
- en: Some systems use reinforcement learning to select from outside information like
    persona, document, knowledge graph, etc., and generate responses accordingly.
    Majumder et al. ([2020a](#bib.bib238)) and Jaques et al. ([2020](#bib.bib154))
    performed persona selection and persona-based response generation simultaneously
    and trained their agents with a reinforcement framework. Bao et al. ([2019a](#bib.bib16))
    and Zhao et al. ([2020b](#bib.bib459)) built document-grounded systems. Similarly,
    they used reinforcement learning to accomplish document selection and knowledge-grounded
    response generation. There were also some works combining knowledge graphs into
    the dialogue systems and treated them as outside knowledge source (Moon et al.,
    [2019](#bib.bib257); Xu et al., [2020a](#bib.bib426)). In a reinforced training
    framework, the agent chooses an edge based on the current node and state for each
    step and then combines the knowledge into the response generation process.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统使用强化学习从外部信息如个性、文档、知识图谱等中进行选择，并相应地生成响应。Majumder 等（[2020a](#bib.bib238)）和
    Jaques 等（[2020](#bib.bib154)）同时进行了个性选择和基于个性的响应生成，并通过强化学习框架训练了他们的智能体。Bao 等（[2019a](#bib.bib16)）和
    Zhao 等（[2020b](#bib.bib459)）构建了文档基础的系统。他们也使用强化学习来完成文档选择和知识基础的响应生成。同时，也有一些工作将知识图谱结合到对话系统中，并将其视为外部知识源（Moon
    等，[2019](#bib.bib257)；Xu 等，[2020a](#bib.bib426)）。在强化训练框架中，智能体根据当前节点和状态为每一步选择一条边，然后将知识结合到响应生成过程中。
- en: RL for dialogue related tasks
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话相关任务的强化学习
- en: Dialogue-related tasks like dialogue relation extraction (Li et al., [2019c](#bib.bib203)),
    question answering (Hua et al., [2020](#bib.bib146)) and machine reading comprehension (Guo
    et al., [2020](#bib.bib116)) benefit from reinforcement learning as well because
    of their interactive nature and the scarcity of annotated data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对话相关任务如对话关系提取（Li 等，[2019c](#bib.bib203)）、问答（Hua 等，[2020](#bib.bib146)）和机器阅读理解（Guo
    等，[2020](#bib.bib116)）也从强化学习中受益，因为这些任务具有交互性和注释数据的稀缺性。
- en: GAN for dialogue systems
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话系统中的GAN
- en: 'The application of GAN in dialogue systems is divided into two streams. The
    first sees the GAN framework applied to enhance response generation (Li et al.,
    [2017a](#bib.bib196); Zhu et al., [2018](#bib.bib466); Wu et al., [2019b](#bib.bib415);
    He et al., [2020a](#bib.bib128); Zhu et al., [2020](#bib.bib467); Qin et al.,
    [2020](#bib.bib281)). The discriminator distinguishes generated responses from
    human responses, which incentivizes the agent, which is also the generator in
    GAN, to generate higher-quality responses. Another stream uses GAN as an evaluation
    tool of dialogue systems (Kannan and Vinyals, [2017](#bib.bib164); Bruni and Fernandez,
    [2017](#bib.bib29)). After training the generator and discriminator as a whole
    framework, the discriminator is used separately as a scorer to evaluate the performance
    of a dialogue agent and was shown to achieve a higher correlation with human evaluation
    compared with traditional reference-based metrics like BLEU, METEOR, ROUGE-L,
    etc. We discuss the evaluation of dialogue systems as a challenge in Section [5](#S5
    "5 Evaluation Approaches ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey").'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'GAN 在对话系统中的应用分为两个方向。第一个方向将 GAN 框架应用于增强响应生成（Li 等，[2017a](#bib.bib196)；Zhu 等，[2018](#bib.bib466)；Wu
    等，[2019b](#bib.bib415)；He 等，[2020a](#bib.bib128)；Zhu 等，[2020](#bib.bib467)；Qin
    等，[2020](#bib.bib281)）。判别器将生成的响应与人类响应区分开来，这激励了智能体，即 GAN 中的生成器，生成更高质量的响应。另一个方向将
    GAN 作为对话系统的评估工具（Kannan 和 Vinyals，[2017](#bib.bib164)；Bruni 和 Fernandez，[2017](#bib.bib29)）。在将生成器和判别器作为一个整体框架进行训练后，判别器被单独用作评分器来评估对话智能体的表现，并显示出与传统的参考基准指标如
    BLEU、METEOR、ROUGE-L 等相比，与人类评估的相关性更高。我们将在第 [5](#S5 "5 Evaluation Approaches ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey") 节讨论对话系统的评估作为一个挑战。'
- en: 2.8 Knowledge Graph Augmented Neural Networks
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8 知识图谱增强神经网络
- en: Supervised training with annotated data tries to learn the knowledge distribution
    of a dataset. However, a dataset is comparatively sparse and thus learning a reliable
    knowledge distribution needs a huge amount of annotated data (Annervaz et al.,
    [2018](#bib.bib6)).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带注释的数据进行监督训练试图学习数据集的知识分布。然而，数据集相对稀疏，因此学习可靠的知识分布需要大量的注释数据（Annervaz 等，[2018](#bib.bib6)）。
- en: Knowledge Graph (KG) is attracting more and more research interests in recent
    years. KG is a structured knowledge source consisting of entities and their relationships (Ji
    et al., [2022](#bib.bib157)). In other words, KG is the knowledge facts presented
    in graph format.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KG）近年来吸引了越来越多的研究兴趣。KG是一个由实体及其关系组成的结构化知识源（Ji等，[2022](#bib.bib157)）。换句话说，KG是以图形格式呈现的知识事实。
- en: '![Refer to caption](img/cc06c397bbc31e22a1e21cb9ac8daa76.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cc06c397bbc31e22a1e21cb9ac8daa76.png)'
- en: 'Figure 12: Entities and relations in knowledge graph (Ji et al., [2022](#bib.bib157))'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：知识图谱中的实体和关系（Ji等，[2022](#bib.bib157)）
- en: 'Figure [12](#S2.F12 "Figure 12 ‣ 2.8 Knowledge Graph Augmented Neural Networks
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") shows an example of a KG consisting of
    entities and their relationships. A KG is stored in triples under the Resource
    Description Framework (RDF). For example, Albert Einstein, University of Zurich,
    and their relationship can be expressed as $(AlbertEinstein,GraduateFrom,UniversityofZurich)$.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](#S2.F12 "图12 ‣ 2.8 知识图谱增强神经网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述")展示了一个由实体及其关系组成的KG示例。KG以三元组形式存储在资源描述框架（RDF）下。例如，阿尔伯特·爱因斯坦、苏黎世大学及其关系可以表示为$(AlbertEinstein,GraduateFrom,UniversityofZurich)$。
- en: 'Knowledge graph augmented neural networks first represent the entities and
    their relations in a lower dimension space, then use a neural model to retrieve
    relevant facts (Ji et al., [2022](#bib.bib157)). Knowledge graph representation
    learning can be generally divided into two categories: structure-based representations
    and semantically-enriched representations. Structure-based representations use
    multi-dimensional vectors to represent entities and relations. Models such as
    TransE (Bordes et al., [2013](#bib.bib23)), TransR (Lin et al., [2015](#bib.bib215)),
    TransH (Wang et al., [2014](#bib.bib393)), TransD (Ji et al., [2015](#bib.bib156)),
    TransG (Xiao et al., [2015](#bib.bib420)), TransM (Fan et al., [2014](#bib.bib84)),
    HolE (Nickel et al., [2016](#bib.bib262)) and ProjE (Shi and Weninger, [2017](#bib.bib322))
    belong to this category. The semantically-enriched representation models like
    NTN (Socher et al., [2013](#bib.bib334)), SSP (Xiao et al., [2017](#bib.bib421))
    and DKRL (Xie et al., [2016](#bib.bib422)) combine semantic information into the
    representation of entities and relations. The neural retrieval models also have
    two main directions: distance-based matching model and semantic matching model.
    Distance-based matching models (Bordes et al., [2013](#bib.bib23)) consider the
    distance between projected entities while semantic matching models (Bordes et al.,
    [2014](#bib.bib24)) calculate the semantic similarity of entities and relations
    to retrieve facts.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱增强的神经网络首先在较低维度空间中表示实体及其关系，然后使用神经模型检索相关事实（Ji等，[2022](#bib.bib157)）。知识图谱表示学习通常可以分为两类：基于结构的表示和语义增强的表示。基于结构的表示使用多维向量来表示实体和关系。诸如TransE（Bordes等，[2013](#bib.bib23)）、TransR（Lin等，[2015](#bib.bib215)）、TransH（Wang等，[2014](#bib.bib393)）、TransD（Ji等，[2015](#bib.bib156)）、TransG（Xiao等，[2015](#bib.bib420)）、TransM（Fan等，[2014](#bib.bib84)）、HolE（Nickel等，[2016](#bib.bib262)）和ProjE（Shi和Weninger，[2017](#bib.bib322)）等模型属于这一类别。语义增强的表示模型如NTN（Socher等，[2013](#bib.bib334)）、SSP（Xiao等，[2017](#bib.bib421)）和DKRL（Xie等，[2016](#bib.bib422)）将语义信息融入实体和关系的表示中。神经检索模型也有两个主要方向：基于距离的匹配模型和语义匹配模型。基于距离的匹配模型（Bordes等，[2013](#bib.bib23)）考虑投影实体之间的距离，而语义匹配模型（Bordes等，[2014](#bib.bib24)）计算实体和关系的语义相似性来检索事实。
- en: Knowledge graph augmented dialogue systems
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识图谱增强对话系统
- en: Knowledge-grounded dialogue systems benefit greatly from the structured knowledge
    format of KG, where facts are widely intercorrelated. Reasoning over a KG is an
    ideal approach for combining commonsense knowledge into response generation, resulting
    in accurate and informative responses (Young et al., [2018](#bib.bib438)). Jung
    et al. ([2020](#bib.bib160)) proposed AttnIO, a bi-directional graph exploration
    model for knowledge retrieval in knowledge-grounded dialogue systems. Attention
    weights were calculated at each traversing step, and thus the model could choose
    a broader range of knowledge paths instead of choosing only one node at a time.
    In such a scheme, the model could predict adequate paths even when only having
    the destination node as the label. Zhang et al. ([2019b](#bib.bib447)) built ConceptFlow,
    a dialogue agent that guided to more meaningful future conversations. It traversed
    in a commonsense knowledge graph to explore concept-level conversation flows.
    Finally, it used a gate to decide to generate among vocabulary words, central
    concept words, and outer concept words. Majumder et al. ([2020a](#bib.bib238))
    proposed to generate persona-based responses by first using COMET (Bosselut et al.,
    [2019](#bib.bib26)) to expand a persona sentence in context along 9 relation types
    and then applied a pretrained model to generate responses based on dialogue history
    and the persona variable. Yang et al. ([2020](#bib.bib429)) used knowledge graph
    as an external knowledge source in task-oriented dialogue systems to incorporate
    domain-specified knowledge in the response. First, the dialogue history was parsed
    as a dependency tree and encoded into a fixed-length vector. Then they applied
    multi-hop reasoning over the graph using the attention mechanism. The decoder
    finally predicted tokens either by copying from graph entities or generating vocabulary
    words. Moon et al. ([2019](#bib.bib257)) proposed DialKG Walker for the conversational
    reasoning task. They computed a zero-shot relevance score between predicted KG
    embedding and ground KG embedding to facilitate cross-domain predictions. Furthermore,
    they applied an attention-based graph walker to generate graph paths based on
    the relevance scores. Huang et al. ([2020a](#bib.bib147)) evaluated the dialogue
    systems by combining the utterance-level contextualized representation and topic-level
    graph representation. They first constructed the dialogue graph based on encoded
    (context, response) pairs and then reasoned over the graph to get a topic-level
    graph representation. The final score was calculated by passing the concatenated
    vector of contextualized representation and graph representation to a feed-forward
    network.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 知识基础对话系统从KG的结构化知识格式中获益良多，其中事实之间广泛关联。对KG的推理是将常识知识融入响应生成的理想方法，从而生成准确且信息丰富的回应（Young
    et al., [2018](#bib.bib438)）。Jung et al. ([2020](#bib.bib160)) 提出了AttnIO，一种用于知识检索的双向图探索模型。注意力权重在每一步遍历时进行计算，因此模型可以选择更广泛的知识路径，而不是每次只选择一个节点。在这种方案中，即使只有目标节点作为标签，模型也能预测出合适的路径。Zhang
    et al. ([2019b](#bib.bib447)) 构建了ConceptFlow，一个引导更有意义未来对话的对话代理。它在常识知识图中遍历以探索概念级对话流。最后，它使用门控机制决定在词汇单词、中心概念词和外部概念词之间生成。Majumder
    et al. ([2020a](#bib.bib238)) 提出了基于角色生成响应的方法，通过首先使用COMET (Bosselut et al., [2019](#bib.bib26))
    在9种关系类型下扩展角色句子，然后应用预训练模型根据对话历史和角色变量生成响应。Yang et al. ([2020](#bib.bib429)) 将知识图谱作为任务导向对话系统中的外部知识源，以将领域特定知识纳入响应中。首先，将对话历史解析为依赖树并编码为固定长度的向量。然后，他们利用注意力机制在图上进行多跳推理。解码器最终通过从图实体中复制或生成词汇单词来预测标记。Moon
    et al. ([2019](#bib.bib257)) 提出了DialKG Walker用于对话推理任务。他们计算了预测KG嵌入和实际KG嵌入之间的零样本相关性分数，以促进跨领域预测。此外，他们应用基于注意力的图行走器根据相关性分数生成图路径。Huang
    et al. ([2020a](#bib.bib147)) 通过结合话语级上下文表示和主题级图表示来评估对话系统。他们首先基于编码的（上下文、回应）对构建对话图，然后在图上推理以获得主题级图表示。最终分数通过将上下文表示和图表示的串联向量传递到前馈网络中计算得出。
- en: 3 Task-oriented Dialogue Systems
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务导向对话系统
- en: This section introduces task-oriented dialogue systems including modular and
    end-to-end systems. Task-oriented systems solve specific problems in a certain
    domain such as movie ticket booking, restaurant table reserving, etc. We focus
    on deep learning-based systems due to the outstanding performance. For readers
    who want to learn more about traditional rule-based and statistical models, there
    are several surveys to refer to (Theune, [2003](#bib.bib367); Lemon and Pietquin,
    [2007](#bib.bib189); Mallios and Bourbakis, [2016](#bib.bib240); Chen et al.,
    [2017a](#bib.bib39); Santhanam and Shaikh, [2019](#bib.bib305)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了包括模块化和端到端系统的面向任务的对话系统。面向任务的系统解决特定领域中的问题，如电影票预订、餐厅桌位预订等。我们关注基于深度学习的系统，因为它们表现优异。对于那些想了解传统规则基础和统计模型的读者，可以参考一些综述文章（Theune,
    [2003](#bib.bib367); Lemon and Pietquin, [2007](#bib.bib189); Mallios and Bourbakis,
    [2016](#bib.bib240); Chen et al., [2017a](#bib.bib39); Santhanam and Shaikh, [2019](#bib.bib305)）。
- en: This section is organized as follows. We first discuss modular and end-to-end
    systems respectively by introducing the principles and reviewing recent works.
    After that, we comprehensively discuss related challenges and hot topics for task-oriented
    dialogue systems in recent research to provide some important research directions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的组织结构如下。我们首先分别讨论模块化和端到端系统，介绍其原理并回顾近期的工作。之后，我们全面讨论面向任务的对话系统在近期研究中的相关挑战和热点话题，以提供一些重要的研究方向。
- en: '![Refer to caption](img/4918f832683341a45125f7a76ec974f8.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4918f832683341a45125f7a76ec974f8.png)'
- en: 'Figure 13: Structure of a task-oriented dialogue system in the task-completion
    pipeline'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：任务完成流程中的面向任务的对话系统结构
- en: 'A task-oriented dialogue system requires stricter response constraints because
    it aims to accurately handle the user message. Therefore, modular methods were
    proposed to generate responses in a more controllable way. The architecture of
    a modular-based system is depicted in Figure [13](#S3.F13 "Figure 13 ‣ 3 Task-oriented
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey"). It consists of four modules:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '面向任务的对话系统需要更严格的响应约束，因为它旨在准确处理用户消息。因此，提出了模块化方法以更可控的方式生成响应。基于模块化的系统架构如图[13](#S3.F13
    "Figure 13 ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey")所示。它由四个模块组成：'
- en: 'Natural Language Understanding (NLU). This module converts the raw user message
    into semantic slots, together with classifications of domain and user intention.
    However, some recent modular systems omit this module and use the raw user message
    as the input of the next module, as shown in Figure [13](#S3.F13 "Figure 13 ‣
    3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey"). Such a design aims to reduce the propagation of
    errors between modules and alleviate the impact of the original error (Kim et al.,
    [2018](#bib.bib166)).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '自然语言理解（NLU）。该模块将原始用户消息转换为语义槽，同时进行领域和用户意图的分类。然而，一些近期的模块化系统省略了此模块，直接使用原始用户消息作为下一个模块的输入，如图[13](#S3.F13
    "Figure 13 ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey")所示。这种设计旨在减少模块间的错误传播，并减轻原始错误的影响（Kim
    et al., [2018](#bib.bib166)）。'
- en: Dialogue State Tracking (DST). This module iteratively calibrates the dialogue
    states based on the current input and dialogue history. The dialogue state includes
    related user actions and slot-value pairs.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态追踪（DST）。该模块根据当前输入和对话历史反复校准对话状态。对话状态包括相关的用户动作和槽值对。
- en: Dialogue Policy Learning. Based on the calibrated dialogue states from the DST
    module, this module decides the next action of a dialogue agent.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对话策略学习。基于来自DST模块的校准对话状态，该模块决定对话代理的下一个动作。
- en: Natural Language Generation (NLG). This module converts the selected dialogue
    actions into surface-level natural language, which is usually the ultimate form
    of response.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）。该模块将选定的对话动作转换为表面层次的自然语言，这通常是响应的*终极*形式。
- en: Among them, Dialogue State Tracking and Dialogue Policy Learning constitute
    the Dialogue Manager (DM), the central controller of a task-oriented dialogue
    system. Usually, a task-oriented system also interacts with an external Knowledge
    Base (KB) to retrieve essential knowledge about the target task. For example,
    in a movie ticket booking task, after understanding the requirement of the user
    message, the agent interacts with the movie knowledge base to search for movies
    with specific constraints such as movie name, time, cinema, etc.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对话状态跟踪和对话策略学习构成了对话管理器（DM），它是任务导向对话系统的中央控制器。通常，任务导向系统还与外部知识库（KB）交互，以检索有关目标任务的基本知识。例如，在电影票预订任务中，在理解用户消息的需求后，代理与电影知识库交互，以搜索符合特定约束条件的电影，如电影名称、时间、电影院等。
- en: 3.1 Natural Language Understanding
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自然语言理解
- en: 'It has been proven that the NLU module impacts the whole system significantly
    in the term of response quality (Li et al., [2017b](#bib.bib201)). The NLU module
    converts the natural language message produced by the user into semantic slots
    and performs classification. Table [2](#S3.T2 "Table 2 ‣ 3.1 Natural Language
    Understanding ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey") shows an example of the output format
    of the NLU module. The NLU module manages three tasks: domain classification,
    intent detection, and slot filling. Domain classification and intent detection
    are classification problems, which use classifiers to predict a mapping from the
    input language sequence to a predefined label set. In the given example, the predicted
    domain is “movie" and the intent is “find_movie". Slot filling is a tagging problem,
    which can be viewed as a sequence-to-sequence task. It maps a raw user message
    into a sequence of slot names. In the example, the NLU module reads the user message
    “Recommend a movie at Golden Village tonight." and outputs the corresponding tag
    sequence. It recognizes “Golden Village" as the place to go, which is tagged as
    “B_desti" and “I_desti" for the two words respectively. Similarly, the token “tonight"
    is converted into “B_time". ‘B’ represents the beginning of a chunk, and ‘I’ indicates
    that this tag is inside a target chunk. For those unrelated tokens, an ‘O’ is
    used indicating that this token is outside of any chunk of interest. This tagging
    method is called Inside-Outside-Beginning (IOB) tagging (Ramshaw and Marcus, [1999](#bib.bib290)),
    which is a common method in Named-Entity Recognition (NER) tasks.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '已证明，NLU 模块对整个系统的响应质量有显著影响（Li 等，[2017b](#bib.bib201)）。NLU 模块将用户生成的自然语言信息转换为语义槽，并进行分类。表
    [2](#S3.T2 "Table 2 ‣ 3.1 Natural Language Understanding ‣ 3 Task-oriented Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey") 展示了 NLU 模块的输出格式示例。NLU 模块管理三个任务：领域分类、意图检测和槽填充。领域分类和意图检测是分类问题，使用分类器来预测输入语言序列与预定义标签集之间的映射。在给定的示例中，预测的领域是“movie”，意图是“find_movie”。槽填充是标注问题，可以视为序列到序列的任务。它将原始用户消息映射到一系列槽名称。在这个示例中，NLU
    模块读取用户消息“Recommend a movie at Golden Village tonight.”并输出相应的标签序列。它识别“Golden Village”作为去的地方，将其标记为“B_desti”和“I_desti”。类似地，令牌“tonight”被转换为“B_time”。‘B’表示一个块的开始，‘I’表示该标签在目标块内部。对于那些无关的令牌，使用‘O’，表示该令牌在任何感兴趣的块之外。这种标注方法称为
    Inside-Outside-Beginning (IOB) 标注（Ramshaw 和 Marcus，[1999](#bib.bib290)），这是命名实体识别（NER）任务中的一种常用方法。'
- en: 'Table 2: The output example of an NLU module'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：NLU 模块的输出示例
- en: '| Sentence | Recommend | a | movie | at | Golden | Village | tonight |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | 推荐 | 一部 | 电影 | 在 | 金源 | 村 | 今晚 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Slots | O | O | O | O | B-desti | I-desti | B-time |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 槽 | O | O | O | O | B-desti | I-desti | B-time |'
- en: '| Intent |  |  |  | find_movie |  |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 意图 |  |  |  | find_movie |  |  |  |'
- en: '| Domain |  |  |  | movie |  |  |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 领域 |  |  |  | 电影 |  |  |  |'
- en: Techniques for domain classification and intent detection
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 领域分类和意图检测的技术
- en: Domain classification and intent detection belong to the same category of tasks.
    Deep learning methods are proposed to solve the classification problems of dialogue
    domain and intent. Deng et al. ([2012](#bib.bib67)) and Tur et al. ([2012](#bib.bib374))
    were the first who successfully improved the recognition accuracy of dialogue
    intent. They built deep convex networks to combine the predictions of a prior
    network and the current utterances as an integrated input of a current network.
    A deep learning framework was also used to classify the dialogue domain and intent
    in a semi-supervised fashion (Yann et al., [2014](#bib.bib430)). To solve the
    difficulty of training a deep neural network for domain and intent prediction,
    Restricted Boltzmann Machine (RBM) and Deep Belief Networks (DBNs) were applied
    to initialize the parameters of deep neural networks (Sarikaya et al., [2014](#bib.bib307)).
    To make use of the strengths of RNNs in sequence processing, some works used RNNs
    as utterance encoders and made predictions for intent and domain categories (Ravuri
    and Stolcke, [2015](#bib.bib293), [2016](#bib.bib294)). Hashemi et al. ([2016](#bib.bib124))
    used a CNN to extract hierarchical text features for intent detection and illustrated
    the sequence classification capabilities of CNNs. Lee and Dernoncourt ([2016](#bib.bib183))
    proposed a model for intent classification of short utterances. Short utterances
    are hard for intent detection because of the lack of information in a single dialogue
    turn. This paper used RNN and CNN architectures to incorporate the dialogue history,
    thus obtaining the context information as an additional input besides the current
    turn’s message. The model achieved promising performances on three intent classification
    datasets. More recently, Wu et al. ([2020a](#bib.bib414)) pretrained Task-Oriented
    Dialogue BERT (TOD-BERT) and significantly improved the accuracy in the intent
    detection sub-task. The proposed model also exhibited a strong capability of few-shot
    learning and could effectively alleviate the data insufficiency issue in a specific
    domain.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 领域分类和意图检测属于相同类别的任务。深度学习方法被提出用于解决对话领域和意图的分类问题。邓等人 ([2012](#bib.bib67)) 和 Tur
    等人 ([2012](#bib.bib374)) 是首批成功提高对话意图识别准确率的研究者。他们构建了深度凸网络，将先前网络的预测和当前话语的预测作为当前网络的综合输入。一个深度学习框架也被用来以半监督的方式分类对话领域和意图（Yann
    等人，[2014](#bib.bib430)）。为了解决训练深度神经网络进行领域和意图预测的难题，限制玻尔兹曼机（RBM）和深度置信网络（DBNs）被应用于初始化深度神经网络的参数（Sarikaya
    等人，[2014](#bib.bib307)）。为了利用递归神经网络（RNN）在序列处理中的优势，一些研究使用 RNN 作为话语编码器，并对意图和领域类别进行预测（Ravuri
    和 Stolcke，[2015](#bib.bib293)，[2016](#bib.bib294)）。Hashemi 等人 ([2016](#bib.bib124))
    使用卷积神经网络（CNN）提取层次化的文本特征进行意图检测，并展示了 CNN 的序列分类能力。Lee 和 Dernoncourt ([2016](#bib.bib183))
    提出了一个短话语意图分类模型。短话语因单个对话回合信息不足而难以进行意图检测。本文使用 RNN 和 CNN 架构来整合对话历史，从而将上下文信息作为当前回合消息之外的额外输入。该模型在三个意图分类数据集上取得了令人满意的性能。最近，Wu
    等人 ([2020a](#bib.bib414)) 预训练了任务导向对话 BERT（TOD-BERT），显著提高了意图检测子任务的准确率。该模型还展示了强大的少样本学习能力，并有效缓解了特定领域数据不足的问题。
- en: Techniques for slot filling
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 插槽填充技术
- en: The slot filling problem is also called semantic tagging, a sequence classification
    problem. It is more challenging for that the model needs to predict multiple objects
    at a time. Deep Belief Nets (DBNs) exhibit promising capabilities in the learning
    of deep architectures and have been applied in many tasks including semantic tagging.
    Sarikaya et al. ([2011](#bib.bib306)) used a DBN-initialized neural network to
    complete slot filling in the call-routing task. Deoras and Sarikaya ([2013](#bib.bib68))
    built a DBN-based sequence tagger. In addition to the NER input features used
    in traditional taggers, they also combined part of speech (POS) and syntactic
    features as a part of the input. The recurrent architectures benefited the sequence
    tagging task in that they could keep track of the information along past timesteps
    to make the most of the sequential information. Yao et al. ([2013](#bib.bib431))
    first argued that instead of simply predicting words, RNN Language Models (RNN-LMs)
    could be applied in sequence tagging. On the output side of RNN-LMs, tag labels
    were predicted instead of normal vocabularies. Mesnil et al. ([2013](#bib.bib246))
    and Mesnil et al. ([2014](#bib.bib247)) further investigated the impact of different
    recurrent architectures in the slot filling task and found that all RNNs outperformed
    the Conditional Random Field (CRF) baseline. As a powerful recurrent model, LSTM
    showed promising tagging accuracy on the ATIS dataset owing to the memory control
    of its gate mechanism (Yao et al., [2014](#bib.bib432)). Gangadharaiah and Narayanaswamy
    ([2020](#bib.bib97)) argued that the shallow output representations of traditional
    semantic tagging lacked the ability to represent the structured dialogue information.
    To improve, they treated the slot filling task as a template-based tree decoding
    task by iteratively generating and filling in the templates. Different from traditional
    sequence tagging methods, Coope et al. ([2020](#bib.bib58)) tackled the slot filling
    task by treating it as a turn-based span extraction task. They applied the conversational
    pretrained model ConveRT and utilized the rich semantic information embedded in
    the pretrained vectors to solve the problem of in-domain data insufficiency. The
    inputs of ConveRT are the requested slots and the utterance, while the output
    is a span of interest as the slot value.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 槽位填充问题也被称为语义标注，是一个序列分类问题。由于模型需要一次预测多个对象，这使得问题更加具有挑战性。深度信念网络（DBNs）在深度架构的学习中展现了良好的能力，并已被应用于包括语义标注在内的许多任务中。Sarikaya
    等人（[2011](#bib.bib306)）使用了一个以 DBN 初始化的神经网络来完成呼叫路由任务中的槽位填充。Deoras 和 Sarikaya（[2013](#bib.bib68)）建立了一个基于
    DBN 的序列标注器。除了传统标注器使用的命名实体识别（NER）输入特征外，他们还将词性（POS）和句法特征作为输入的一部分。递归架构使得序列标注任务受益，因为它们能够跟踪过去时间步的信息，以充分利用序列信息。Yao
    等人（[2013](#bib.bib431)）首次提出，相比于简单地预测单词，RNN 语言模型（RNN-LMs）可以应用于序列标注。在 RNN-LMs 的输出端，预测的是标签而不是普通词汇。Mesnil
    等人（[2013](#bib.bib246)）和 Mesnil 等人（[2014](#bib.bib247)）进一步研究了不同递归架构在槽位填充任务中的影响，发现所有
    RNN 都优于条件随机场（CRF）基线。作为一种强大的递归模型，LSTM 在 ATIS 数据集上显示出良好的标注准确性，这得益于其门控机制的记忆控制（Yao
    等人，[2014](#bib.bib432)）。Gangadharaiah 和 Narayanaswamy（[2020](#bib.bib97)）认为传统语义标注的浅层输出表示缺乏表示结构化对话信息的能力。为了改进，他们将槽位填充任务视为一个基于模板的树解码任务，通过迭代生成和填充模板来实现。与传统序列标注方法不同，Coope
    等人（[2020](#bib.bib58)）通过将槽位填充任务视为基于回合的跨度提取任务来解决这个问题。他们应用了对话预训练模型 ConveRT，并利用预训练向量中嵌入的丰富语义信息来解决领域数据不足的问题。ConveRT
    的输入是请求的槽位和话语，而输出是作为槽位值的感兴趣跨度。
- en: Unifying domain classification, intent detection, and slot filling
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 统一领域分类、意图检测和槽位填充
- en: Some works choose to combine domain classification, intent detection, and slot
    filling into a multitask learning framework to jointly optimize the shared latent
    space. Hakkani-Tür et al. ([2016](#bib.bib120)) applied a bi-directional RNN-LSTM
    architecture to jointly perform three tasks. Liu and Lane ([2016](#bib.bib220))
    augmented the traditional RNN encoder-decoder model with an attention mechanism
    to manage intent detection and slot filling. The slot filling applied explicit
    alignment. Chen et al. ([2016](#bib.bib48)) proposed an end-to-end memory network
    and used a memory module to store user intent and slot values in history utterances.
    Attention was further applied to iteratively select relevant intent and slot values
    at the decoding stage. Multi-task learning of three NLU subtasks contributed to
    the domain scaling and facilitated the zero-shot or few-shot training when transferring
    to a new domain (Bapna et al., [2017](#bib.bib18); Lee and Jha, [2019](#bib.bib186)).
    Zhang et al. ([2018a](#bib.bib445)) captured the hierarchical structure of dialogue
    semantics in NLU multi-task learning by applying a capsule-based neural network.
    With a dynamic routing-by-agreement strategy, the proposed architecture raised
    the accuracy of both intent detection and slot filling on the SNIPS-NLU and ATIS
    dataset.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究选择将领域分类、意图检测和槽填充结合到一个多任务学习框架中，以共同优化共享的潜在空间。Hakkani-Tür 等人（[2016](#bib.bib120)）应用了双向
    RNN-LSTM 架构来共同执行这三项任务。刘和 Lane（[2016](#bib.bib220)）在传统的 RNN 编码器-解码器模型中增加了注意力机制，以管理意图检测和槽填充。槽填充应用了显式对齐。Chen
    等人（[2016](#bib.bib48)）提出了一个端到端的记忆网络，并使用记忆模块存储用户意图和槽值在历史对话中。进一步应用了注意力机制来迭代选择解码阶段的相关意图和槽值。三项
    NLU 子任务的多任务学习有助于领域扩展，并在转移到新领域时促进了零样本或少样本训练（Bapna 等人，[2017](#bib.bib18)；Lee 和 Jha，[2019](#bib.bib186)）。张等人（[2018a](#bib.bib445)）通过应用基于胶囊的神经网络捕捉了对话语义在
    NLU 多任务学习中的层次结构。通过动态的协议路由策略，该架构提高了 SNIPS-NLU 和 ATIS 数据集上的意图检测和槽填充的准确性。
- en: Novel perspectives
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 新颖观点
- en: More recently, some novel ideas appear in NLU research, which provides new possibilities
    for further improvements. Traditional NLU modules rely on the text converted from
    the audio message of the user using the Automatic Speech Recognition (ASR) module.
    However, Singla et al. ([2020](#bib.bib331)) jumped over the ASR module and directly
    used audio signals as the input of NLU. They found that by reducing the module
    numbers of a pipeline system, the predictions were more robust since fewer errors
    were broadcasted. Su et al. ([2019b](#bib.bib347)) argued that Natural Language
    Understanding (NLU) and Natural Language Generation (NLG) were reversed processes.
    Thus, their dual relationship could be exploited by training with a dual-supervised
    learning framework. The experiments exhibited improvement in both tasks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些新颖的想法出现在 NLU 研究中，为进一步改进提供了新的可能性。传统的 NLU 模块依赖于通过自动语音识别（ASR）模块将用户的音频消息转换为文本。然而，Singla
    等人（[2020](#bib.bib331)）跳过了 ASR 模块，直接使用音频信号作为 NLU 的输入。他们发现，通过减少管道系统的模块数量，预测结果更为稳健，因为广播的错误更少。Su
    等人（[2019b](#bib.bib347)）认为自然语言理解（NLU）和自然语言生成（NLG）是逆向过程。因此，他们可以通过双重监督学习框架来利用它们的双重关系。实验显示两项任务都有所改进。
- en: 3.2 Dialogue State Tracking
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 对话状态跟踪
- en: Dialogue State Tracking (DST) is the first module of a dialogue manager. It
    tracks the user’s goal and related details every turn based on the whole dialogue
    history to provide the information based on which the Policy Learning module (next
    module) decides the agent action to make.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态跟踪（DST）是对话管理器的第一个模块。它基于整个对话历史跟踪用户的目标及相关细节，并提供基于此信息的信息，以便策略学习模块（下一个模块）决定代理采取的行动。
- en: Differences between NLU and DST
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: NLU 与 DST 之间的差异
- en: 'The NLU and DST modules are closely related. Both NLU and DST perform slot
    filling for the dialogue. However, they actually play different roles. The NLU
    module tries to make classifications for the current user message such as the
    intent and domain category as well as the slot each message token belongs to.
    For example, given a user message “Recommend a movie at Golden Village tonight.",
    the NLU module will convert the raw message into “$inform(domain=movie;\ destination=GoldenVillage;\
    date=today;\ time=evening)$", where the slots are usually filled by tagging each
    word of the user message as described in Section [3.1](#S3.SS1 "3.1 Natural Language
    Understanding ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey"). However, the DST module does not
    classify or tag the user message. Instead, it tries to find a slot value for each
    slot name in a pre-existing slot list based on the whole dialogue history. For
    example, there is a pre-existing slot list “$intent:\_;\ domain:\_;\ name:\_;\
    pricerange:\_;\ genre:\_;\ destination:\_;\ date:\_$", where the underscore behind
    the colon is a placeholder denoting that this place can be filled with a value.
    Every turn, the DST module will look up the whole dialogue history up to the current
    turn and decide which content can be filled in a specific slot in the slot list.
    If the user message “Recommend a movie at Golden Village tonight." is the only
    message in a dialogue, then the slot list can be filled as “$intent:inform;\ domain:movie;\
    name:None;\ pricerange:None;\ genre:None;\ destination:GoldenVillage;\ date:today$",
    where the slots unspecified by the user up to current turn can be filled with
    “$None$". To conclude, the NLU module tries to tag the user message while the
    DST module tries to find values from the user message to fill in a pre-existing
    form. Some dialogue systems took the output of the NLU module as the input of
    DST module (Williams et al., [2013](#bib.bib403); Henderson et al., [2014a](#bib.bib133),
    [b](#bib.bib134)), while others directly used raw user messages to track the state (Kim
    et al., [2019](#bib.bib170); Wang et al., [2020e](#bib.bib391); Hu et al., [2020](#bib.bib142)).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'NLU 和 DST 模块密切相关。NLU 和 DST 都为对话进行槽位填充。然而，它们实际上扮演着不同的角色。NLU 模块尝试对当前用户消息进行分类，如意图和领域类别，以及每个消息标记属于的槽位。例如，给定用户消息“推荐今晚在
    Golden Village 的电影。”，NLU 模块会将原始消息转换为“$inform(domain=movie;\ destination=GoldenVillage;\
    date=today;\ time=evening)$”，其中槽位通常通过标记用户消息中的每个单词来填充，如第 [3.1](#S3.SS1 "3.1 Natural
    Language Understanding ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in
    Deep Learning Based Dialogue Systems: A Systematic Survey") 节所述。然而，DST 模块不会对用户消息进行分类或标记。相反，它会根据整个对话历史为预先存在的槽位列表中的每个槽位名称寻找槽位值。例如，有一个预先存在的槽位列表“$intent:\_;\
    domain:\_;\ name:\_;\ pricerange:\_;\ genre:\_;\ destination:\_;\ date:\_$”，其中冒号后的下划线是占位符，表示此处可以填入一个值。每轮，DST
    模块会查看直到当前轮次的整个对话历史，并决定哪个内容可以填入槽位列表中的特定槽位。如果用户消息“推荐今晚在 Golden Village 的电影。”是对话中的唯一消息，那么槽位列表可以填充为“$intent:inform;\
    domain:movie;\ name:None;\ pricerange:None;\ genre:None;\ destination:GoldenVillage;\
    date:today$”，其中用户当前轮次未指定的槽位可以填充为“$None$”。总之，NLU 模块尝试标记用户消息，而 DST 模块尝试从用户消息中找到值来填充预先存在的表单。一些对话系统将
    NLU 模块的输出作为 DST 模块的输入（Williams et al., [2013](#bib.bib403); Henderson et al.,
    [2014a](#bib.bib133), [b](#bib.bib134)），而其他系统则直接使用原始用户消息来追踪状态（Kim et al., [2019](#bib.bib170);
    Wang et al., [2020e](#bib.bib391); Hu et al., [2020](#bib.bib142)）。'
- en: Dialogue State Tracking Challenges (DSTCs), a series of popular challenges in
    DST, provides benchmark datasets, standard evaluation frameworks, and test-beds
    for research (Williams et al., [2013](#bib.bib403); Henderson et al., [2014a](#bib.bib133),
    [b](#bib.bib134); Kim et al., [2016](#bib.bib168), [2017](#bib.bib169)). The DSTCs
    cover many domains such as restaurants, tourism, etc.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态追踪挑战（DSTCs）是一系列在DST中受欢迎的挑战，提供了基准数据集、标准评估框架和研究测试平台（Williams et al., [2013](#bib.bib403);
    Henderson et al., [2014a](#bib.bib133), [b](#bib.bib134); Kim et al., [2016](#bib.bib168),
    [2017](#bib.bib169)）。DSTCs 涵盖了许多领域，如餐馆、旅游等。
- en: 'A dialogue state contains all essential information to be conveyed in the response (Henderson,
    [2015](#bib.bib131)). As defined in DSTC2 (Henderson et al., [2014a](#bib.bib133)),
    the dialogue state of a given dialogue turn consists of informable slots Sinf
    and requestable slots Sreq. Informable slots are attributes specified by users
    to constrain the search of the database while requestable slots are attributes
    whose values are queried by the user. For example, the serial number of a movie
    ticket is usually a requestable slot because users seldom assign a specific serial
    number when booking a ticket. Specifically, the dialogue state has three components:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态包含了在响应中传达的所有基本信息（Henderson, [2015](#bib.bib131)）。根据DSTC2（Henderson等, [2014a](#bib.bib133)）的定义，给定对话回合的对话状态包括信息槽位Sinf和请求槽位Sreq。信息槽位是用户指定的属性，用于约束数据库的搜索，而请求槽位是用户查询其值的属性。例如，电影票的序列号通常是请求槽位，因为用户在订票时很少指定特定的序列号。具体来说，对话状态有三个组成部分：
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Goal constraint corresponding with informable slots. The constraints can be
    specific values mentioned by the user in the dialogue or a special value. Special
    values include Dontcare indicating the user’s indifference about the slot and
    None indicating that the user has not specified the value in the conversation
    yet.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与信息槽位对应的目标约束。约束可以是对话中用户提到的具体值或特殊值。特殊值包括Dontcare，表示用户对该槽位的无所谓，以及None，表示用户尚未在对话中指定该值。
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requested slots. It can be a list of slot names queried by the user seeking
    answers from the agent.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求的槽位。它可以是用户向系统请求答案的槽位名称列表。
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Search method of current turn. It consists of values indicating the interaction
    categories. By constraints denotes that the user tries to specify constraint information
    in his requirement; by alternatives denotes that the user requires an alternative
    entity; finished indicates that the user intends to end the conversation.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前回合的搜索方法。它由指示交互类别的值组成。通过约束表示用户尝试在其需求中指定约束信息；通过替代表示用户要求一个替代实体；完成表示用户打算结束对话。
- en: However, considering the numerous challenges such as tracking efficiency, tracking
    accuracy, domain adaptability, and end-to-end training, many alternative representations
    have been proposed recently, which will be discussed later.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到诸如跟踪效率、跟踪准确性、领域适应性和端到端训练等众多挑战，最近提出了许多替代表示方法，后续将对此进行深入探讨。
- en: '![Refer to caption](img/8fb282b87fcb7d57485f527c8ba737b8.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8fb282b87fcb7d57485f527c8ba737b8.png)'
- en: 'Figure 14: An example of DST procedure (Henderson et al., [2014a](#bib.bib133))'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '图14: DST过程的示例（Henderson等，[2014a](#bib.bib133)）'
- en: 'Figure [14](#S3.F14 "Figure 14 ‣ Differences between NLU and DST ‣ 3.2 Dialogue
    State Tracking ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey") is an example of the DST process
    for 4 dialogue turns in a restaurant table booking task. The first column includes
    the raw dialogue utterances, with $S$ denoting the system message and $U$ denoting
    the user message. The second column includes the N-best output lists of the NLU
    module and their corresponding confidence scores. The third column includes the
    labels of a turn, indicating the ground truth slot-value pairs. The fourth column
    includes the example DST outputs and their corresponding confidence scores. The
    fifth column indicates the correctness of the tracker output.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图[14](#S3.F14 "图 14 ‣ NLU 与 DST 的差异 ‣ 3.2 对话状态跟踪 ‣ 3 任务导向对话系统 ‣ 基于深度学习的对话系统的近期进展：系统综述")是一个在餐厅预订任务中4个对话回合的DST过程示例。第一列包括原始对话发言，其中$S$表示系统消息，$U$表示用户消息。第二列包括NLU模块的N-best输出列表及其对应的置信度分数。第三列包括一个回合的标签，指示真实的槽位-值对。第四列包括示例DST输出及其对应的置信度分数。第五列表示跟踪器输出的正确性。
- en: Earlier works use hand-craft rules or statistical methods to solve DST tasks.
    While widely used in industry dialogue systems, rule-based DST methods (Goddeau
    et al., [1996](#bib.bib106)) have many restrictions such as limited generalization,
    high error rate, low domain adaptability, etc (Williams, [2014](#bib.bib406)).
    Statistical methods (Lee, [2013](#bib.bib184); Lee and Eskenazi, [2013](#bib.bib185);
    Ren et al., [2013](#bib.bib297); Williams, [2013](#bib.bib405), [2014](#bib.bib406))
    also suffer from noisy conditions and ambiguity (Young et al., [2010](#bib.bib437)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的工作使用手工规则或统计方法来解决DST任务。虽然在工业对话系统中广泛使用，但基于规则的DST方法（Goddeau等人，[1996](#bib.bib106)）有许多限制，例如泛化能力有限、高错误率、领域适应性差等（Williams，[2014](#bib.bib406)）。统计方法（Lee，[2013](#bib.bib184)；Lee和Eskenazi，[2013](#bib.bib185)；Ren等人，[2013](#bib.bib297)；Williams，[2013](#bib.bib405)，[2014](#bib.bib406)）也受到噪声条件和歧义的困扰（Young等人，[2010](#bib.bib437)）。
- en: Recently, many neural trackers have emerged. Neural trackers have multiple advantages
    over rule-based and statistical trackers. In general, they are categorized into
    two streams. The first stream has predefined slot names and values, and each turn
    the DST module tries to find the most appropriate slot-value pairs based on the
    dialogue history; the second stream does not have a fixed slot value list, so
    the DST module tries to find the values directly from the dialogue context or
    generate values based on the dialogue context. Obviously, the latter one is more
    flexible and in fact, more and more works are solving DST in the second way. We
    discuss the works of both categories here.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多神经跟踪器出现了。神经跟踪器相比于基于规则和统计的跟踪器有多个优势。一般来说，它们被分为两类。第一类有预定义的槽名称和值，每轮DST模块尝试根据对话历史找到最合适的槽-值对；第二类没有固定的槽值列表，因此DST模块尝试直接从对话上下文中找到值或根据对话上下文生成值。显然，后一种方式更加灵活，实际上，越来越多的工作正在以第二种方式解决DST。我们在这里讨论这两类工作的情况。
- en: Neural trackers with predefined slot names and values
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有预定义槽名称和值的神经跟踪器
- en: The first stream can be viewed as a multi-class or multi-hop classification
    task. For multi-class classification DST, the tracker predicts the correct class
    from multiple values but this method suffers from high complexity when the value
    set grows large. On the other hand, for the multi-hop classification tasks, the
    tracker reads only one slot-value pair at a time and performs binary prediction.
    Working in this fashion reduces the model complexity but raises the system reaction
    time since for each slot there will be multiple tracking processes. Henderson
    et al. ([2013](#bib.bib132)) was the first who used a deep learning model in the
    DST tasks. They integrated many feature functions (e.g., SLU score, Rank score,
    Affirm score, etc.) as the input of a neural network, then predict the probability
    of each slot-value pair. Mrkšić et al. ([2015](#bib.bib259)) applied an RNN as
    a neural tracker to gain awareness on dialogue context. Mrkšić et al. ([2016](#bib.bib260))
    proposed a multi-hop neural tracker which took the system output and user utterances
    as the first two inputs (to model the dialogue context), and the candidate slot-value
    pairs as the third input. The tracker finally made a binary prediction on the
    current slot-value pair based on the dialogue history.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 第一类可以视为多类或多跳分类任务。对于多类分类DST，跟踪器从多个值中预测正确的类别，但当值集变大时，这种方法会受到高复杂性的困扰。另一方面，对于多跳分类任务，跟踪器每次只读取一个槽-值对并执行二元预测。这种方式降低了模型的复杂性，但由于每个槽会有多个跟踪过程，系统反应时间也会增加。Henderson等人
    ([2013](#bib.bib132)) 是首个在DST任务中使用深度学习模型的研究者。他们将许多特征函数（例如SLU分数、排名分数、肯定分数等）作为神经网络的输入，然后预测每个槽-值对的概率。Mrkšić等人
    ([2015](#bib.bib259)) 应用了RNN作为神经跟踪器，以获得对对话上下文的意识。Mrkšić等人 ([2016](#bib.bib260))
    提出了一个多跳神经跟踪器，该跟踪器将系统输出和用户话语作为前两个输入（以建模对话上下文），将候选槽-值对作为第三个输入。跟踪器最终根据对话历史对当前槽-值对进行二元预测。
- en: Neural trackers with unfixed slot names and values
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有不固定槽名称和值的神经跟踪器
- en: The second stream attracts more attention because it not only reduces the model
    and time complexity of DST tasks but also facilitates end-to-end training of task-oriented
    dialogue systems. Moreover, it is also flexible when the target domain changes.
    Lei et al. ([2018](#bib.bib188)) proposed belief span, a text span of the dialogue
    context corresponding to a specific slot. They built a two-stage CopyNet to copy
    and store slot values from the dialogue history. The slots were stored to prepare
    for neural response generation. The belief span facilitated the end-to-end training
    of dialogue systems and increased the tracking accuracy in out-of-vocabulary cases.
    Based on this, Lin et al. ([2020c](#bib.bib217)) proposed the minimal belief span
    and argued that it was not scalable to generate belief states from scratch when
    the system interacted with APIs from diverse domains. The proposed MinTL framework
    operated insertion (INS), deletion (DEL) and substitution (SUB) on the dialogue
    state of last turn based on the context and the minimal belief span. Wu et al.
    ([2019a](#bib.bib413)) proposed the TRADE model. The model also applied the copy
    mechanism and used a soft-gated pointer-generator to generate the slot value based
    on the domain-slot pair and encoded dialogue context. Quan and Xiong ([2020](#bib.bib285))
    argued that simply concatenating the dialogue context was not preferable. Alternatively,
    they used [sys] and [usr] to discriminate the system and user messages. This simple
    long context modeling method achieved a 7.03% improvement compared with the baseline.
    Cheng et al. ([2020](#bib.bib49)) proposed Tree Encoder-Decoder (TED) architecture
    which utilized a hierarchical tree structure to represent the dialogue states
    and system acts. The TED generated tree-structured dialogue states of the current
    turn based on the dialogue history, dialogue action, and dialogue state of the
    last turn. This approach led to a 20% improvement on the state-of-the-art DST
    baselines which represented dialogue states and user goals in a flat space. Chen
    et al. ([2020a](#bib.bib41)) built an interactive encoder to exploit the dependencies
    within a turn and between turns. Furthermore, they used the attention mechanism
    to construct the slot-level context for user and system respectively, which were
    embedding vectors based on which the generator copied values from the dialogue
    context. Shan et al. ([2020](#bib.bib317)) applied BERT to perform multi-task
    learning and generated the dialogue state. They first encoded word-level and turn-level
    contexts. Then they retrieved the relevant information for each slot from the
    context by applying both word-level and turn-level attention. Furthermore, the
    slot values were predicted based on the retrieved information. Similarly, Wang
    et al. ([2020e](#bib.bib391)) used BERT for slot value prediction. They performed
    Slot Attention (SA) to retrieve related spans and Value Normalization (VN) to
    convert the spans into final values. Huang et al. ([2020c](#bib.bib150)) proposed
    Meta-Reinforced MultiDomain State Generator (MERET), which was a dialogue state
    generator further finetuned with policy gradient reinforcement learning.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 第二类方法引起了更多关注，因为它不仅降低了DST任务的模型和时间复杂性，还促进了任务导向对话系统的端到端训练。此外，当目标领域发生变化时，它也很灵活。Lei
    等人（[2018](#bib.bib188)）提出了信念跨度，即对话上下文中对应特定槽位的文本跨度。他们构建了一个两阶段的 CopyNet 来从对话历史中复制和存储槽位值。槽位被存储以准备神经响应生成。信念跨度促进了对话系统的端到端训练，并提高了在词汇表外情况中的跟踪准确性。基于此，Lin
    等人（[2020c](#bib.bib217)）提出了最小信念跨度，并认为当系统与来自不同领域的 API 交互时，从头生成信念状态不可扩展。提出的 MinTL
    框架在上下文和最小信念跨度的基础上，对上一个回合的对话状态执行插入（INS）、删除（DEL）和替换（SUB）。Wu 等人（[2019a](#bib.bib413)）提出了
    TRADE 模型。该模型还应用了复制机制，并使用软门控指针生成器基于领域-槽位对和编码的对话上下文生成槽位值。Quan 和 Xiong（[2020](#bib.bib285)）认为简单地连接对话上下文并不可取。他们使用了
    [sys] 和 [usr] 来区分系统和用户消息。这种简单的长上下文建模方法与基线相比提高了 7.03%。Cheng 等人（[2020](#bib.bib49)）提出了
    Tree Encoder-Decoder (TED) 架构，该架构利用层次树结构来表示对话状态和系统行为。TED 根据对话历史、对话动作和上一个回合的对话状态生成当前回合的树状对话状态。这种方法使得对话状态和用户目标的表示在平面空间中的先进
    DST 基线上提高了 20%。Chen 等人（[2020a](#bib.bib41)）构建了一个交互编码器，以利用回合内和回合之间的依赖关系。此外，他们使用了注意机制分别构建用户和系统的槽位级上下文，这些上下文是嵌入向量，根据这些向量，生成器从对话上下文中复制值。Shan
    等人（[2020](#bib.bib317)）应用了 BERT 进行多任务学习并生成对话状态。他们首先对词级和回合级上下文进行了编码。然后，他们通过应用词级和回合级注意机制，从上下文中检索每个槽位的相关信息。此外，基于检索到的信息预测了槽位值。类似地，Wang
    等人（[2020e](#bib.bib391)）使用 BERT 进行槽位值预测。他们执行了槽位注意（SA）以检索相关跨度，并进行了值规范化（VN）以将跨度转换为最终值。Huang
    等人（[2020c](#bib.bib150)）提出了 Meta-Reinforced MultiDomain State Generator (MERET)，这是一个进一步通过策略梯度强化学习微调的对话状态生成器。
- en: 3.3 Policy Learning
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 策略学习
- en: 'The Policy learning module is the other module of a dialogue manager. This
    module controls which action will be taken by the system based on the output dialogue
    states from the DST module. Assuming that we have the dialogue state $S_{t}$ of
    the current turn and the action set $A=\{a_{1},...,a_{n}\}$, the task of this
    module is to learn a mapping function $f$: $S_{t}\to a_{i}\in A$. This module
    is comparatively simpler than other modules in the term of task definition but
    actually, the task itself is challenging (Peng et al., [2017](#bib.bib272)). For
    example, in the tasks of movie ticket and restaurant table booking, if the user
    books a two-hour movie slot and intends to go for dinner after that, then the
    agent should be aware that the time gap between movie slot and restaurant slot
    has to be more than two hours since the commuting time from the cinema to the
    restaurant should be considered.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '策略学习模块是对话管理器的另一个模块。该模块根据来自DST模块的输出对话状态控制系统将采取的动作。假设我们拥有当前轮次的对话状态$S_{t}$和动作集$A=\{a_{1},...,a_{n}\}$，该模块的任务是学习一个映射函数$f$:
    $S_{t}\to a_{i}\in A$。与其他模块相比，这个模块在任务定义上相对简单，但实际上，任务本身具有挑战性（Peng等人，[2017](#bib.bib272)）。例如，在电影票和餐厅预订的任务中，如果用户预订了一个两小时的电影时段并打算之后去晚餐，那么代理需要知道电影时段和餐厅时段之间的时间间隔必须超过两个小时，因为从电影院到餐厅的通勤时间也要考虑在内。'
- en: Supervised learning and reinforcement learning are mainstream training methods
    for dialogue policy learning (Chen et al., [2017a](#bib.bib39)). Policies learned
    in a supervised fashion exhibit great decision-making ability (Su et al., [2016](#bib.bib346);
    Dhingra et al., [2016](#bib.bib71); Williams et al., [2017](#bib.bib408); Liu
    and Lane, [2017](#bib.bib221)). In some specific tasks, the supervised policy
    model can complete tasks precisely, but the training process totally depends on
    the quality of training data. Moreover, the annotated datasets require intensive
    human labor, and the decision ability is restricted by the specific task and domain,
    showing weak transferring capability. With the prevalence of reinforcement learning
    methods, more and more task-oriented dialogue systems use reinforcement learning
    to learn the policy. The dialogue policy learning fits the reinforcement learning
    setting since the agent of reinforcement learning learns a policy to map environment
    states to actions as well.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和强化学习是对话策略学习的主流训练方法（Chen等人，[2017a](#bib.bib39)）。以监督方式学习的策略表现出出色的决策能力（Su等人，[2016](#bib.bib346);
    Dhingra等人，[2016](#bib.bib71); Williams等人，[2017](#bib.bib408); Liu和Lane，[2017](#bib.bib221)）。在一些特定任务中，监督策略模型可以准确完成任务，但训练过程完全依赖于训练数据的质量。此外，注释数据集需要大量人工劳动，而且决策能力受限于特定任务和领域，表现出较弱的迁移能力。随着强化学习方法的普及，越来越多的任务导向型对话系统使用强化学习来学习策略。对话策略学习适合强化学习的设置，因为强化学习的代理也学习将环境状态映射到动作的策略。
- en: Usually, the environment of reinforce policy learning is a user or a simulated
    user in which setting the training is called online learning. However, it is data-
    and time-consuming to learn a policy from scratch in the online learning scenario,
    so the warm-start method is needed to speed up the training process. Henderson
    et al. ([2008](#bib.bib130)) used expert data to restrict the initial action space
    exploration. Chen et al. ([2017b](#bib.bib42)) applied teacher-student learning
    framework to transfer the teacher expert knowledge to the target network in order
    to warm-start the system.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，强化学习策略的环境是用户或模拟用户，其中设置训练的过程称为在线学习。然而，在在线学习场景中从头开始学习策略是数据和时间密集型的，因此需要暖启动方法来加速训练过程。Henderson等人（[2008](#bib.bib130)）使用专家数据来限制初始动作空间的探索。Chen等人（[2017b](#bib.bib42)）应用了教师-学生学习框架，将教师专家知识转移到目标网络，以便进行暖启动。
- en: Reinforcement policy learning techniques
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强化策略学习技术
- en: Almost all recent dialogue policy learning works are based on reinforcement
    learning methods. Online learning is an ideal approach to get training samples
    iteratively for a reinforcement learning agent, but human labor is very limited.
    Zhang et al. ([2019c](#bib.bib454)) proposed Budget-Conscious Scheduling (BCS)
    to better utilize limited user interactions, where the user interaction is seen
    as the budget. The BCS used a probability scheduler to allocate the budget during
    training. Also, a controller decided whether to use real user interactions or
    simulated ones. Furthermore, a goal-based sampling model was applied to simulate
    the experiences for policy learning. Such a budget-controlling mechanism achieved
    ideal performance in the practical training process. Considering the difficulty
    of getting real online user interactions and the huge amount of annotated data
    required for training user simulators, Takanobu et al. ([2020](#bib.bib356)) proposed
    Multi-Agent Dialog Policy Learning, where they have two agents interacting with
    each other, performing both user and agent, learning the policy simultaneously.
    Furthermore, they incorporated a role-specific reward to facilitate role-based
    response generation. A High task completion rate was observed in experiments.
    Wang et al. ([2020d](#bib.bib387)) introduced Monte Carlo Tree Search with Double-q
    Dueling network (MCTS-DDU), where a decision-time planning was proposed instead
    of background planning. They used the Monte Carlo simulation to perform a tree
    search of the dialogue states. Gordon-Hall et al. ([2020](#bib.bib110)) trained
    expert demonstrators in a weakly supervised fashion to perform Deep Q-learning
    from Demonstrations (DQfD). Furthermore, Reinforced Fine-tune Learning was proposed
    to facilitate domain transfer. In reinforce dialogue policy learning, the agent
    usually receives feedback at the end of the dialogue, which is not efficient for
    learning. Huang et al. ([2020b](#bib.bib149)) proposed an innovative reward learning
    method that constrains the dialogue progress according to the expert demonstration.
    The expert demonstration could either be annotated or not, so the approach was
    not labor intensive. Wang et al. ([2020b](#bib.bib385)) proposed to co-generate
    the dialogue actions and responses to maintain the inherent semantic structures
    of dialogue. Similarly, Le et al. ([2020b](#bib.bib181)) proposed a unified framework
    to simultaneously perform dialogue state tracking, dialogue policy learning, and
    response generation. Experiments showed that unified frameworks have a better
    performance both in their sub-tasks and in their domain adaptability. Xu et al.
    ([2020a](#bib.bib426)) used a knowledge graph to provide prior knowledge of the
    action set and solved policy learning task in a graph-grounded fashion. By combining
    a knowledge graph, a long-term reward was obtained to provide the policy agent
    with a long-term vision while choosing actions. Also, the candidate actions were
    of higher quality due to prior knowledge. The policy learning was further performed
    in a more controllable way.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 目前几乎所有的对话策略学习工作都基于强化学习方法。在线学习是一种理想的方法，可以为强化学习代理迭代获取训练样本，但人工劳动非常有限。张等人（[2019c](#bib.bib454)）提出了预算意识调度（BCS）来更好地利用有限的用户交互，其中用户交互被视为预算。BCS使用概率调度器在训练期间分配预算。此外，一个控制器决定是否使用真实用户交互或模拟交互。此外，还应用了基于目标的采样模型来模拟策略学习的经验。这样的预算控制机制在实际训练过程中达到了理想的效果。考虑到获取真实在线用户交互的困难以及训练用户模拟器所需的大量注释数据，Takanobu等人（[2020](#bib.bib356)）提出了多代理对话策略学习，他们让两个代理互相交互，既执行用户角色又执行代理角色，同时学习策略。此外，他们还结合了角色特定奖励来促进基于角色的响应生成。实验中观察到高任务完成率。王等人（[2020d](#bib.bib387)）引入了带有双重Q对抗网络（MCTS-DDU）的蒙特卡洛树搜索（MCTS），提出了决策时间规划，而不是背景规划。他们使用蒙特卡洛模拟来对对话状态进行树搜索。戈登-霍尔等人（[2020](#bib.bib110)）以弱监督的方式训练专家示范者来执行来自示范的深度Q学习（DQfD）。此外，还提出了强化微调学习以促进领域迁移。在强化对话策略学习中，代理通常在对话结束时收到反馈，这对于学习来说并不高效。黄等人（[2020b](#bib.bib149)）提出了一种创新的奖励学习方法，根据专家示范来约束对话进展。专家示范可以是注释的，也可以不是，因此这种方法不是劳动密集型的。王等人（[2020b](#bib.bib385)）提出了共同生成对话动作和响应，以保持对话的固有语义结构。同样，Le等人（[2020b](#bib.bib181)）提出了一个统一的框架，同时执行对话状态跟踪、对话策略学习和响应生成。实验表明，统一框架在其子任务和领域适应性方面表现更好。徐等人（[2020a](#bib.bib426)）使用知识图谱提供动作集合的先验知识，并以图形化的方式解决策略学习任务。通过结合知识图谱，获得了长期奖励，为策略代理提供了在选择动作时的长期视野。此外，由于先验知识，候选动作的质量更高。策略学习进一步以更可控的方式进行。
- en: 3.4 Natural Language Generation
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 自然语言生成
- en: Natural Language Generation (NLG) is the last module of a task-oriented dialogue
    system pipeline. It manages to convert the dialogue actions generated from the
    dialogue manager into a final natural language representation. E.g., Assuming
    “Inform (name = Wonder Woman; genre = Action; desti = Golden Village)" to be the
    dialogue action from policy learning module, then the NLG module converts it into
    language representations such as “There is an action movie named Wonder Woman
    at Golden Village."
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）是任务导向对话系统管道的最后一个模块。它负责将从对话管理器生成的对话动作转换为最终的自然语言表示。例如，假设“Inform (name
    = Wonder Woman; genre = Action; desti = Golden Village)”是策略学习模块中的对话动作，那么NLG模块将其转换为诸如“在Golden
    Village有一部名为Wonder Woman的动作片。”的语言表示。
- en: 'Traditional NLG modules are pipeline systems. Defined by Siddharthan ([2001](#bib.bib326)),
    the standard pipeline of NLG consists of four components, as shown in Figure [15](#S3.F15
    "Figure 15 ‣ 3.4 Natural Language Generation ‣ 3 Task-oriented Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey").'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的NLG模块是管道系统。根据Siddharthan ([2001](#bib.bib326))的定义，NLG的标准管道包括四个组件，如图[15](#S3.F15
    "Figure 15 ‣ 3.4 Natural Language Generation ‣ 3 Task-oriented Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")所示。'
- en: '![Refer to caption](img/0e0556eda06576da7c8989b7bca0ecbc.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0e0556eda06576da7c8989b7bca0ecbc.png)'
- en: 'Figure 15: The pipeline NLG system'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：管道NLG系统
- en: 'The core modules of this pipeline are Content Determination, Sentence Planning,
    and Surface Realization, as proposed by Reiter ([1994](#bib.bib296)). Cahill et al.
    ([1999](#bib.bib33)) further improved the NLG pipeline by adding three more components:
    lexicalization, referring expression generation, and aggregation. However, this
    model has a drawback that the input of the system is ambiguous.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道的核心模块是内容确定、句子规划和表层实现，如Reiter ([1994](#bib.bib296))提出的那样。Cahill等人 ([1999](#bib.bib33))
    进一步改进了NLG管道，增加了三个组件：词汇化、指代表达生成和聚合。然而，这个模型的一个缺点是系统的输入存在歧义。
- en: End-to-end NLG techniques
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端到端的NLG技术
- en: Deep learning methods were further applied to enhance the NLG performance and
    the pipeline is collapsed into a single module. End-to-end natural language generation
    has achieved promising improvements and is the most popular way to perform NLG
    in recent work. Wen et al. ([2015a](#bib.bib397)) argued that language generation
    should be fully data-driven and not depend on any expert rules. They proposed
    a statistical language model based on RNNs to learn response generation with semantic
    constraints and grammar trees. Additionally, they used a CNN reranker to further
    select better responses. Similarly, an LSTM model was used by Wen et al. ([2015b](#bib.bib398))
    to learn sentence planning and surface realization simultaneously. Tran and Nguyen
    ([2017](#bib.bib372)) further improved the generation quality on multiple domains
    using GRU. The proposed generator consistently generated high-quality responses
    on multiple domains. To improve the domain adaptability of recurrent models, Wen
    et al. ([2016b](#bib.bib400)) proposed to first train the recurrent language model
    on data synthesized from out-of-domain datasets, then finetune on a comparatively
    smaller in-domain dataset. This training strategy was proved effective in human
    evaluation. Context-awareness is important in dialogue response generation because
    only depending on the dialogue action of the current turn may cause illogical
    responses. Zhou et al. ([2016](#bib.bib461)) built an attention-based Context-Aware
    LSTM (CA-LSTM) combining target user questions, all semantic values, and dialogue
    actions as input to generate context-aware responses in QA. Likewise, Dušek and
    Jurčíček ([2016a](#bib.bib77)) concatenated the preceding user utterance with
    the dialogue action vector and fed it into an LSTM model. Dušek and Jurčíček ([2016b](#bib.bib78))
    put a syntax constraint upon their neural response generator. A two-stage sequence
    generation process was proposed. First, a syntax dependency tree was generated
    to have a structured representation of the dialogue utterance. The generator in
    the second stage integrated sentence planning and surface realization and produced
    natural language representations.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习方法被进一步应用于提升NLG性能，并且将管道整合为单个模块。端到端自然语言生成取得了令人期待的进展，并且是近期研究中执行NLG的最流行方法。Wen等人（[2015a](#bib.bib397)）认为语言生成应完全基于数据，而不依赖任何专家规则。他们提出了一种基于RNN的统计语言模型来学习带有语义约束和语法树的响应生成。此外，他们使用CNN重新排序器进一步选择更好的响应。类似地，Wen等人（[2015b](#bib.bib398)）使用LSTM模型同时学习句子规划和表面实现。Tran和Nguyen（[2017](#bib.bib372)）进一步使用GRU提高了多个领域的生成质量。所提出的生成器在多个领域
    consistently 生成高质量的响应。为了提高递归模型的领域适应性，Wen等人（[2016b](#bib.bib400)）建议首先在从域外数据集合成的数据上训练递归语言模型，然后在相对较小的领域内数据集上进行微调。这一训练策略在人工评估中被证明是有效的。上下文感知在对话响应生成中非常重要，因为仅仅依赖当前轮次的对话行动可能会导致不合逻辑的响应。Zhou等人（[2016](#bib.bib461)）构建了一个基于注意力的上下文感知LSTM（CA-LSTM），将目标用户问题、所有语义值和对话行动作为输入生成上下文感知的回答。类似地，Dušek和Jurčíček（[2016a](#bib.bib77)）将前一用户话语与对话行动向量拼接，并将其输入到LSTM模型中。Dušek和Jurčíček（[2016b](#bib.bib78)）对他们的神经响应生成器施加了语法约束。提出了一个两阶段的序列生成过程。首先，生成一个语法依赖树，以对话话语进行结构化表示。在第二阶段，生成器整合了句子规划和表面实现，生成自然语言表示。
- en: Robust natural language generation
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 强大的自然语言生成
- en: More recent works have focused on the reliability and quality of generated responses.
    A tree-structured semantic representation was proposed by Balakrishnan et al.
    ([2019](#bib.bib14)) to achieve better content planning and surface realization
    performance. They further designed a novel beam search algorithm to improve the
    semantic correctness of the generated response. To avoid mistakes such as slot
    value missing or redundancy in generated responses, Li et al. ([2020d](#bib.bib207))
    proposed Iterative Rectification Network (IRN), a framework trained with supervised
    learning and finetuned with reinforcement learning. It iteratively rectified generated
    tokens by incorporating slot inconsistency penalty into its reward. Golovanov
    et al. ([2019](#bib.bib107)) applied large-scale pretrained models for NLG tasks.
    After comparing single-input and multi-input methods, they concluded that different
    types of input context will cause different inductive biases in generated responses
    and further proposed to utilize this characteristic to better adapt a pretrained
    model to a new task. Baheti et al. ([2020](#bib.bib13)) solved NLG reliability
    problem in conversational QA. Though with different pipeline structures, they
    used similar methods to increase the fluency and semantic correctness of the generated
    response. They proposed Syntactic Transformations (STs) to generate candidate
    responses and used a BERT to rank their qualities. These generated responses can
    be viewed as an augmentation of the original dataset to be further used in NLG
    model learning. Oraby et al. ([2019](#bib.bib264)) proposed a method to create
    datasets with rich style markups from easily available user reviews. They further
    trained multiple NLG models based on generated data to perform joint control of
    semantic correctness and language style. Similarly, Elder et al. ([2020](#bib.bib79))
    put forward a data augmentation approach which put a restriction on response generation.
    Though this restriction caused dull and less diverse responses, they argued that
    in task-oriented systems, reliability was more important than diversity.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的研究集中在生成响应的可靠性和质量上。Balakrishnan 等人 ([2019](#bib.bib14)) 提出了树状语义表示，以实现更好的内容规划和表面实现性能。他们进一步设计了一种新型的束搜索算法，以提高生成响应的语义正确性。为了避免诸如槽值缺失或生成响应中的冗余等错误，Li
    等人 ([2020d](#bib.bib207)) 提出了迭代修正网络（IRN），这是一种使用监督学习训练并通过强化学习微调的框架。它通过将槽不一致惩罚纳入奖励中，迭代修正生成的标记。Golovanov
    等人 ([2019](#bib.bib107)) 将大规模预训练模型应用于自然语言生成（NLG）任务。在比较了单输入和多输入方法后，他们得出结论，不同类型的输入上下文会在生成响应中造成不同的归纳偏差，并进一步提出利用这一特性来更好地将预训练模型适应新任务。Baheti
    等人 ([2020](#bib.bib13)) 解决了对话问答中的 NLG 可靠性问题。虽然有不同的流水线结构，他们使用了类似的方法来提高生成响应的流畅性和语义正确性。他们提出了句法转换（STs）来生成候选响应，并使用
    BERT 来对其质量进行排序。这些生成的响应可以视为对原始数据集的增强，以便在 NLG 模型学习中进一步使用。Oraby 等人 ([2019](#bib.bib264))
    提出了从易于获得的用户评论中创建具有丰富风格标记的数据集的方法。他们进一步基于生成的数据训练了多个 NLG 模型，以实现语义正确性和语言风格的联合控制。类似地，Elder
    等人 ([2020](#bib.bib79)) 提出了数据增强方法，该方法对响应生成施加了限制。虽然这种限制导致了乏味且多样性较差的响应，他们认为在任务导向的系统中，可靠性比多样性更重要。
- en: 3.5 End-to-end Methods
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 端到端方法
- en: 'The modules discussed above can achieve good performance in their respective
    tasks, with the help of recent relevant advances. However, there exist two significant
    drawbacks in modular systems (Zhao and Eskenazi, [2016](#bib.bib455)): (1) Modules
    in many pipeline systems are sometimes not differentiable, which means that errors
    from the end are not able to be propagated back to each module. In real dialogue
    systems training, usually the only signal is the user response, while other supervised
    signals like dialogue states and dialogue actions are scarce. (2) Though the modules
    jointly contribute to the success of a dialogue system, the improvement of one
    module may not necessarily raise the response accuracy or quality of the whole
    system. This causes additional training of other modules, which is labor intensive
    and time-consuming. Additionally, due to the handcrafted features in pipeline
    task-oriented systems such as dialogue states, it is usually hard to transfer
    modular systems to another domain, since the predefined ontologies require modification.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模块在各自的任务中可以通过最近的相关进展取得良好的性能。然而，模块化系统存在两个显著缺点（Zhao 和 Eskenazi，[2016](#bib.bib455)）：（1）许多管道系统中的模块有时不可微分，这意味着错误无法从末端传播回每个模块。在实际对话系统训练中，通常只有用户响应这一信号，而对话状态和对话动作等其他监督信号较少。（2）尽管模块共同促进了对话系统的成功，但一个模块的改进不一定会提高整个系统的响应准确性或质量。这会导致其他模块需要额外训练，这既费力又耗时。此外，由于管道任务导向系统中的手工特征，如对话状态，通常很难将模块化系统迁移到另一个领域，因为预定义的本体需要修改。
- en: There exist two main methods for the end-to-end training of task-oriented dialogue
    systems. One is to make each module of a pipeline system differentiable, then
    the whole pipeline can be viewed as a large differentiable system and the parameters
    can be optimized by back-propagation in an end-to-end fashion (Le et al., [2020b](#bib.bib181)).
    Another way is to use only one end-to-end module to perform both knowledge base
    retrieval and response generation, which is usually a multi-task learning neural
    model.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向对话系统的端到端训练存在两种主要方法。一种是使管道系统的每个模块都可微分，然后整个管道可以视为一个大型可微分系统，参数可以通过反向传播以端到端的方式进行优化（Le
    等，[2020b](#bib.bib181)）。另一种方法是使用一个端到端模块同时执行知识库检索和响应生成，这通常是一个多任务学习神经模型。
- en: End-to-end trainable pipeline TOD
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端到端可训练管道 TOD
- en: 'The increasing applications of neural models have made it possible for modules
    to be differentiable. While many modules are easily differentiable, there remains
    one task that makes differentiation challenging: the knowledge base query. Many
    task-oriented dialogue systems require an external knowledge source to retrieve
    related knowledge facts required by the user. For example, in the restaurant table
    booking task, the knowledge fact can be an available slot of one specific restaurant.
    Traditional methods use a symbolic query to match entries based on their attributes.
    The system performs semantic parsing on the user message to represent a symbolic
    query according to the user goal (Li et al., [2017b](#bib.bib201); Williams and
    Zweig, [2016](#bib.bib407); Wen et al., [2016c](#bib.bib401)). However, this retrieval
    process is not differentiable, which prevents the whole framework from being end-to-end
    trainable. With the application of key-value memory networks (Miller et al., [2016](#bib.bib250)), Eric
    and Manning ([2017](#bib.bib81)) used the key-value retrieval mechanism to retrieve
    relevant facts. The proposed architecture was augmented with the attention mechanism
    to compute the relevance between utterance representations of dialogue and key
    representations of the knowledge base. Dhingra et al. ([2016](#bib.bib71)) presented
    a soft retrieval mechanism that uses a “soft" posterior distribution over the
    knowledge base to replace the symbolic queries. They further combined this soft
    retrieval mechanism into a reinforcement learning framework to achieve complete
    end-to-end training based on user feedback. Williams et al. ([2017](#bib.bib408))
    proposed Hybrid Code Networks (HCNs), which encoded domain-specific knowledge
    into software and system action templates, achieving the differentiability of
    the knowledge retrieval module. They did not explicitly model the dialogue states
    but instead learned the latent representation and optimized the HCN using supervised
    learning and reinforcement learning jointly. Ham et al. ([2020](#bib.bib121))
    used GPT-2 to form a neural pipeline and perform domain prediction, dialogue state
    tracking, policy learning, knowledge retrieval, and response generation in a pipeline
    fashion. The system could easily interact with external systems because it outputs
    explicit intermediate results from each module and thus being interpretable. Likewise, Hosseini-Asl
    et al. ([2020](#bib.bib141)) built a neural pipeline with GPT-2 and explicitly
    generated results for each neural module as well.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 神经模型应用的增加使得模块变得可微分。尽管许多模块很容易微分，但仍有一个任务使得微分具有挑战性：知识库查询。许多面向任务的对话系统需要外部知识来源来检索用户所需的相关知识事实。例如，在餐厅预订任务中，知识事实可以是某个特定餐厅的一个可用时间段。传统方法使用符号查询来根据属性匹配条目。系统对用户消息进行语义解析，以根据用户目标表示符号查询（Li
    et al., [2017b](#bib.bib201); Williams and Zweig, [2016](#bib.bib407); Wen et
    al., [2016c](#bib.bib401)）。然而，这种检索过程是不可微分的，这使得整个框架无法实现端到端的训练。随着键值记忆网络的应用（Miller
    et al., [2016](#bib.bib250)），Eric 和 Manning（[2017](#bib.bib81)）使用键值检索机制来检索相关事实。所提出的架构通过引入注意力机制来计算对话的发话表示和知识库的键表示之间的相关性。Dhingra
    et al.（[2016](#bib.bib71)）提出了一种软检索机制，使用对知识库的“软”后验分布来替代符号查询。他们进一步将这一软检索机制结合到强化学习框架中，以实现基于用户反馈的完整端到端训练。Williams
    et al.（[2017](#bib.bib408)）提出了混合代码网络（HCNs），将领域特定知识编码到软件和系统动作模板中，从而实现知识检索模块的可微分性。他们没有明确建模对话状态，而是学习潜在表示，并通过监督学习和强化学习联合优化HCN。Ham
    et al.（[2020](#bib.bib121)）使用GPT-2形成了一个神经管道，并在管道中执行领域预测、对话状态跟踪、策略学习、知识检索和响应生成。该系统能够轻松与外部系统互动，因为它从每个模块输出显式的中间结果，因此具有可解释性。同样，Hosseini-Asl
    et al.（[2020](#bib.bib141)）也使用GPT-2构建了一个神经管道，并为每个神经模块明确生成结果。
- en: End-to-end trainable single module TOD
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端到端可训练的单模块TOD
- en: More recent works tend not to build their end-to-end systems in a pipeline fashion.
    Instead, they use complex neural models to implicitly represent the key functions
    and integrate the modules into one. Research in task-oriented end-to-end neural
    models focuses either on training methods or model architecture, which are the
    keys to response correctness and quality. Wang et al. ([2019a](#bib.bib388)) proposed
    an incremental learning framework to train their end-to-end task-oriented system.
    The main idea is to build an uncertainty estimation module to evaluate the confidence
    of appropriate responses generated. If the confidence score was higher than a
    threshold, then the response would be accepted, while a human response would be
    introduced if the confidence score was low. The agent could also learn from human
    responses using online learning. Dai et al. ([2020](#bib.bib61)) used model-agnostic
    meta-learning (MAML) to improve the adaptability and reliability jointly with
    only a handful of training samples in a real-life online service task. Similarly, Qian
    and Yu ([2019](#bib.bib280)) also trained the end-to-end neural model using MAML
    to facilitate the domain adaptation, which enables the model to first train on
    rich-resource tasks and then on new tasks with limited data. Lin et al. ([2020c](#bib.bib217))
    proposed Minimalist Transfer Learning (MinTL) to plug-and-play large-scale pretrained
    models for domain transfer in dialogue task completion. To maintain the sequential
    correctness of generated responses, Wu et al. ([2019b](#bib.bib415)) trained an
    inconsistent order detection module in an unsupervised fashion. This module detected
    whether an utterance pair is ordered or not to guide the task-completing agent
    towards generating more coherent responses. He et al. ([2020a](#bib.bib128)) proposed
    a “Two-Teacher One-Student" training framework. At the first stage, the two teacher
    models were trained in a reinforcement learning framework, with the objective
    of retrieving knowledge facts and generating human-like responses respectively.
    Then at the second stage, the student network was forced to mimic the output of
    the teacher networks. Thus, the expert knowledge of the two teacher networks was
    transferred to the student network. Balakrishnan et al. ([2019](#bib.bib14)) introduced
    a constrained decoding method to improve the semantic correctness of the responses
    generated by the proposed end-to-end system. Many end-to-end task-oriented systems
    used a memory module to store relevant knowledge facts and dialogue history. Chen
    et al. ([2019c](#bib.bib45)) argued that a single memory module was not enough
    for precise retrieval. They used two long-term memory modules to store the knowledge
    tuples and dialogue history respectively, and then a working memory was applied
    to control the token generation. Zhang et al. ([2020](#bib.bib452)) proposed LAtent
    BElief State (LABES) model, which treated the dialogue states as discrete latent
    variables to reduce the reliance on turn-level DST labels. To solve the data insufficiency
    problem in some tasks, Gao et al. ([2020a](#bib.bib99)) augmented the response
    generation model with a paraphrase model in their end-to-end system. The paraphrase
    model was jointly trained with the whole framework and it aimed to augment the
    training samples. Yang et al. ([2020](#bib.bib429)) leveraged the graph structure
    information of both a knowledge graph and the dialogue context-dependency tree.
    They proposed a recurrent cell architecture to learn representations on the graph
    and performed multi-hop reasoning to exploit the entity links in the knowledge
    graph. With the augmentation of graph information, consistent improvement was
    achieved on two task-oriented datasets.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的研究工作往往不再以流水线的方式构建端到端系统。相反，它们使用复杂的神经模型来隐式地表示关键功能，并将模块整合为一个整体。针对任务导向的端到端神经模型的研究主要集中在训练方法或模型架构上，这些是响应正确性和质量的关键。**王等人**（[2019a](#bib.bib388)）提出了一个增量学习框架来训练他们的端到端任务导向系统。其主要思想是构建一个不确定性估计模块，以评估生成的适当响应的置信度。如果置信度评分高于某个阈值，则接受该响应，否则引入人工响应。代理还可以通过在线学习从人工响应中学习。**戴等人**（[2020](#bib.bib61)）使用模型无关的元学习（MAML）来提高适应性和可靠性，并且只需少量训练样本即可在实际在线服务任务中进行改进。类似地，**钱和余**（[2019](#bib.bib280)）也使用MAML训练了端到端神经模型，以促进领域适应，这使得模型可以首先在丰富资源的任务上进行训练，然后在数据有限的新任务上进行训练。**林等人**（[2020c](#bib.bib217)）提出了简约转移学习（MinTL），以即插即用的大规模预训练模型进行对话任务完成中的领域转移。为了保持生成响应的顺序正确性，**吴等人**（[2019b](#bib.bib415)）以无监督的方式训练了一个不一致顺序检测模块。该模块检测话语对是否按顺序排列，以引导任务完成代理生成更连贯的响应。**何等人**（[2020a](#bib.bib128)）提出了一个“两个老师一个学生”的训练框架。在第一阶段，两个教师模型在强化学习框架中进行训练，目标分别是检索知识事实和生成类似人类的响应。然后，在第二阶段，学生网络被迫模仿教师网络的输出。因此，两个教师网络的专家知识被转移到学生网络。**巴拉克里希南等人**（[2019](#bib.bib14)）引入了一种受限解码方法，以提高所提端到端系统生成响应的语义正确性。许多端到端任务导向系统使用了一个记忆模块来存储相关的知识事实和对话历史。**陈等人**（[2019c](#bib.bib45)）认为单一的记忆模块不足以进行精确检索。他们使用了两个长期记忆模块分别存储知识元组和对话历史，然后应用了一个工作记忆来控制令牌生成。**张等人**（[2020](#bib.bib452)）提出了LAtent
    BElief State（LABES）模型，该模型将对话状态视为离散的潜在变量，以减少对轮次级DST标签的依赖。为了应对一些任务中的数据不足问题，**高等人**（[2020a](#bib.bib99)）在他们的端到端系统中用一个同义词模型增强了响应生成模型。该同义词模型与整个框架共同训练，旨在增加训练样本。**杨等人**（[2020](#bib.bib429)）利用了知识图谱和对话上下文依赖树的图结构信息。他们提出了一种递归单元架构，以在图上学习表示，并执行多跳推理以利用知识图谱中的实体链接。通过图信息的增强，在两个任务导向数据集上取得了一致的改进。
- en: 3.6 Research Challenges and Hot Topics
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 研究挑战与热点话题
- en: In this section, we review recent works in task-oriented dialogue systems and
    point out the frequently studied topics to provide some important research directions.
    This section can be seen as an augmentation of the literature review in previous
    sections discussing techniques developed for each module, and focuses more on
    some specific problems to be solved in the current research community.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了任务导向对话系统中的最新研究工作，并指出了频繁研究的主题，以提供一些重要的研究方向。本节可以被视为对前面章节文献综述的扩展，讨论了为每个模块开发的技术，并更关注当前研究社区中需要解决的一些特定问题。
- en: 3.6.1 Pretrained Models for NLU
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 NLU 的预训练模型
- en: The Natural Language Understanding task converts the user message into a predefined
    format of semantic slots. A popular way to perform NLU is by finetuning large-scale
    pretrained language models. Wu and Xiong ([2020](#bib.bib412)) compared many pretrained
    language models including BERT-based and GPT-based systems in three subtasks of
    task-oriented dialogue systems - domain identification, intent detection, and
    slot tagging. This empirical paper is aimed to provide insights and guidelines
    in pretrained model selection and application for related research. Wu et al.
    ([2020a](#bib.bib414)) pretrained TOD-BERT and outperformed strong baselines in
    the intent detection task. The model proposed also had a strong few-shot learning
    ability to alleviate the data insufficiency problem. Coope et al. ([2020](#bib.bib58))
    proposed Span-ConveRT, which was a pretrained model designed for slot filling
    task. It viewed the slot filling task as a turn-based span extraction problem
    and also performed well in the few-shot learning scenario.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言理解任务将用户消息转换为预定义的语义槽位格式。执行 NLU 的一种流行方式是通过微调大规模预训练语言模型。Wu 和 Xiong ([2020](#bib.bib412))
    比较了许多预训练语言模型，包括基于 BERT 和 GPT 的系统，涉及任务导向对话系统的三个子任务——领域识别、意图检测和槽位标记。这篇实证论文旨在提供关于预训练模型选择和应用的见解和指南。Wu
    等人 ([2020a](#bib.bib414)) 预训练了 TOD-BERT，并在意图检测任务中超越了强基线。该模型还具有强大的少量样本学习能力，以缓解数据不足的问题。Coope
    等人 ([2020](#bib.bib58)) 提出了 Span-ConveRT，这是一种为槽位填充任务设计的预训练模型。它将槽位填充任务视为基于轮次的跨度提取问题，并且在少量样本学习场景中表现良好。
- en: 3.6.2 Domain Transfer for NLU
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 NLU 的领域迁移
- en: Another challenge or hot topic in NLU research is the domain transfer problem,
    which is also the key issue of task-oriented dialogue systems. Hakkani-Tür et al.
    ([2016](#bib.bib120)) built an RNN-LSTM architecture for multitask learning of
    domain classification, intent detection, and slot-filling problem. Training samples
    from multiple domains were combined in a single model where respective domain
    data reinforces each other. Bapna et al. ([2017](#bib.bib18)) used a multi-task
    learning framework to leverage slot name encoding and slot description encoding,
    thus implicitly aligning the slot-filling model across domains. Likewise, Lee
    and Jha ([2019](#bib.bib186)) also applied slot description to exploit the similar
    semantic concepts between slots of different domains, which solved the sub-optimal
    concept alignment and long training time problems encountered in past works involving
    multi-domain slot-filling.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: NLU 研究中的另一个挑战或热点话题是领域迁移问题，这也是任务导向对话系统的关键问题。Hakkani-Tür 等人 ([2016](#bib.bib120))
    构建了一个 RNN-LSTM 架构用于领域分类、意图检测和槽位填充问题的多任务学习。来自多个领域的训练样本被结合在一个模型中，其中各领域的数据相互增强。Bapna
    等人 ([2017](#bib.bib18)) 使用了一个多任务学习框架来利用槽位名称编码和槽位描述编码，从而隐式地对齐跨领域的槽位填充模型。同样，Lee
    和 Jha ([2019](#bib.bib186)) 也应用了槽位描述来利用不同领域槽位之间的相似语义概念，解决了过去在多领域槽位填充中遇到的次优概念对齐和长训练时间问题。
- en: 3.6.3 Domain Transfer for DST
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.3 DST 的领域迁移
- en: 'Domain adaptability is also a significant topic for dialogue state trackers.
    The domain transfer in DST is challenging due to three main reasons (Ren et al.,
    [2018](#bib.bib298)): (1) Slot values in ontologies are different when the domain
    changes, which accounts for the incompatibility of models. (2) When the domain
    changes, the slot number will also change, causing different numbers of model
    parameters. (3) Hand-crafted lexicons make it difficult for generalization over
    domains. Mrkšić et al. ([2015](#bib.bib259)) used delexicalized n-gram features
    to solve the domain incompatibility problem by replacing all specified slot names
    and values with generic symbols. Lin et al. ([2020c](#bib.bib217)) introduced
    Levenshtein belief spans (Lev), which were short context spans relating to the
    user message. Different from previous methods which generated dialogue state from
    scratch, they performed substitution (SUB), deletion (DEL), and insertion (INS)
    based on past states to alleviate the dependency on annotated in-domain training
    samples. Huang et al. ([2020c](#bib.bib150)) applied model-agnostic meta-learning
    (MAML) to first learn on several source domains and then adapt on the target domain,
    while Campagna et al. ([2020](#bib.bib34)) improved the zero-shot transfer learning
    by synthesizing in-domain data using an abstract conversation model and the domain
    ontology. Ouyang et al. ([2020](#bib.bib266)) modeled explicit slot connections
    to exploit the existing slots appearing in other domains. Thus, the tracker could
    copy slot values from the connected slots directly, alleviating the burden of
    reasoning and learning. Wang et al. ([2020e](#bib.bib391)) proposed Value Normalization
    (VN) to convert supporting dialogue spans into state values and could achieve
    high accuracy with only 30% available ontology.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应性也是对话状态跟踪器的一个重要议题。由于以下三大主要原因，DST 中的领域转移具有挑战性（Ren et al., [2018](#bib.bib298)）：(1)
    当领域发生变化时，本体中的槽值不同，这导致模型不兼容。(2) 领域变化时，槽的数量也会改变，导致模型参数的数量不同。(3) 手工制作的词典使得跨领域泛化变得困难。Mrkšić
    et al. ([2015](#bib.bib259)) 使用了去词典化的 n-gram 特征，通过将所有指定的槽名称和槽值替换为通用符号，解决了领域不兼容问题。Lin
    et al. ([2020c](#bib.bib217)) 引入了 Levenshtein 信念范围（Lev），这是与用户消息相关的短上下文范围。与之前从零开始生成对话状态的方法不同，他们基于过去的状态进行了替换（SUB）、删除（DEL）和插入（INS），以缓解对标注的领域内训练样本的依赖。Huang
    et al. ([2020c](#bib.bib150)) 应用了模型无关的元学习（MAML），首先在几个源领域上进行学习，然后在目标领域进行适应，而 Campagna
    et al. ([2020](#bib.bib34)) 通过使用抽象对话模型和领域本体合成领域内数据，从而改善了零样本迁移学习。Ouyang et al.
    ([2020](#bib.bib266)) 建立了显式的槽连接模型，以利用其他领域中出现的现有槽。因此，跟踪器可以直接从连接的槽中复制槽值，减轻了推理和学习的负担。Wang
    et al. ([2020e](#bib.bib391)) 提出了值归一化（VN），将支持对话范围转换为状态值，并且只用 30% 的可用本体就能实现高准确率。
- en: 3.6.4 Tracking Efficiency for DST
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.4 DST 的跟踪效率
- en: Tracking efficiency is another hot topic in dialogue state tracking challenges.
    Usually, there are multiple states within a dialogue, so how to compute the slot
    values without any redundant steps becomes very significant when attempting to
    reduce the reaction time of a system. Kim et al. ([2019](#bib.bib170)) argued
    that predicting the dialogue state from scratch at every turn was not efficient.
    They proposed to first predict the operations to be taken on each of the slots
    (i.e., Carryover, Delete, Dontcare, Update), and then perform respective operations
    as predicted. Ouyang et al. ([2020](#bib.bib266)) used a slot connection mechanism
    to directly copy slot values from the source slot, which reduced the expense of
    reasoning. Hu et al. ([2020](#bib.bib142)) and Wang et al. ([2020e](#bib.bib391))
    proposed slot attention to calculate the relations between the slot and dialogue
    context, thus only focusing on the relevant slots at each turn.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪效率是对话状态跟踪挑战中的另一个热门话题。通常，一个对话中有多个状态，因此在试图减少系统反应时间时，如何在没有冗余步骤的情况下计算槽值变得非常重要。Kim
    et al. ([2019](#bib.bib170)) 认为每次都从头预测对话状态是不高效的。他们提出首先预测每个槽上要采取的操作（即 Carryover、Delete、Dontcare、Update），然后根据预测执行相应的操作。Ouyang
    et al. ([2020](#bib.bib266)) 使用了槽连接机制，直接从源槽中复制槽值，从而减少了推理开销。Hu et al. ([2020](#bib.bib142))
    和 Wang et al. ([2020e](#bib.bib391)) 提出了槽注意力机制，以计算槽与对话上下文之间的关系，从而在每次迭代时仅关注相关的槽。
- en: 3.6.5 Training Environment for PL
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.5 PL 的训练环境
- en: The environment of the Policy Learning framework has been a long-existing problem.
    Li et al. ([2017b](#bib.bib201)) built a user simulator to model the user feedback
    as the reward signal of an environment. They modeled a stack-like user agenda
    to iteratively change the user goal and thus shifting the dialogue states. While
    using a user simulator for environment modeling seems to be promising for that
    it involves less human interaction, Zhang et al. ([2019c](#bib.bib454)) argued
    that training a user simulator required a large amount of annotated data. Takanobu
    et al. ([2020](#bib.bib356)) proposed Multi-Agent Dialog Policy Learning, where
    they have two agents interact with each other, performing both user and agent,
    learning policy simultaneously. Furthermore, they incorporated a role-specific
    reward to facilitate role-based response generation and here both agents also
    acted as the environment of the other one.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 政策学习框架的环境一直是一个长期存在的问题。Li 等人（[2017b](#bib.bib201)）构建了一个用户模拟器，将用户反馈建模为环境的奖励信号。他们建模了一个类似堆栈的用户议程，以迭代地改变用户目标，从而改变对话状态。虽然使用用户模拟器进行环境建模似乎很有前途，因为它涉及比较少的人类交互，但Zhang
    等人（[2019c](#bib.bib454)）认为训练用户模拟器需要大量的带注释数据。Takanobu 等人（[2020](#bib.bib356)）提出了多智能体对话政策学习，他们让两个智能体相互交互，同时执行用户和代理，学习策略。此外，他们融入了角色特定的奖励，以促进基于角色的响应生成，这两个智能体也充当对方的环境。
- en: 3.6.6 Response Consistency for NLG
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.6 自然语言生成的响应一致性
- en: Response consistency in NLG is a challenging problem since it cannot be solved
    by simply augmenting the training samples. Instead, additional corrections or
    regulations should be designed. Wen et al. ([2015b](#bib.bib398)) proposed the
    Semantically Controlled LSTM (SC-LSTM) which used a semantic planning gate to
    control the retention or abandonment of dialogue actions thus ensuring the response
    consistency. Likewise, Tran and Nguyen ([2017](#bib.bib372)) also applied a gating
    mechanism to jointly perform sentence planning and surface realization where dialogue
    action features were gated before entering GRU cells. Li et al. ([2020d](#bib.bib207))
    proposed Iterative Rectification Network (IRN), which combined a slot inconsistency
    reward into the reinforcement learning framework. Thus, the model iteratively
    checked the correctness of slots and corresponding values.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成中的响应一致性是一个具有挑战性的问题，因为它不能仅通过增加训练样本来解决。相反，应设计额外的更正或规范。Wen 等人（[2015b](#bib.bib398)）提出了语义控制的LSTM（SC-LSTM），它使用语义规划门来控制对话动作的保留或放弃，从而确保响应的一致性。同样地，Tran
    和 Nguyen（[2017](#bib.bib372)）还应用了门控机制来共同执行句子规划和表面实现，其中对话动作特征在进入 GRU 单元之前被门控。Li
    等人（[2020d](#bib.bib207)）提出了迭代修正网络（IRN），将槽不一致奖励结合到强化学习框架中。因此，该模型循环检查槽和相应值的正确性。
- en: 3.6.7 End-to-end Task-oriented Dialogue Systems
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.7 端到端任务型对话系统
- en: 'End-to-end systems are usually fully data-driven, which contributes to their
    robust and natural responses. However, because of the finiteness of annotated
    training samples, a hot research topic is figuring out how to increase the response
    quality of end-to-end task-oriented dialogue systems with limited data. Using
    rule-based methods to constrain response generation is a way to improve response
    quality. Balakrishnan et al. ([2019](#bib.bib14)) used linearized tree-structured
    representation as input to obtain control over discourse-level and sentence-level
    semantic concepts. Kale and Rastogi ([2020](#bib.bib162)) used templates to improve
    the semantic correctness of generated responses. They broke down the response
    generation into a two-stage process: first generating semantically correct but
    possibly incoherent responses based on the slots, with the constraint of templates;
    then in the second stage, pretrained language models were applied to re-organize
    the generated utterances into coherent ones. Training the network with reinforcement
    learning was another strategy to alleviate the reliance on annotated data. He
    et al. ([2020a](#bib.bib128)) trained two teacher networks using a reinforcement
    learning framework with the objectives of knowledge retrieval and response generation
    respectively. Then the student network learns to produce responses by mimicking
    the output of teacher networks. Training the network in a supervised way, Dai
    et al. ([2020](#bib.bib61)) alternatively tried to optimize the learning strategy
    to improve the learning efficiency of models given limited data. They combined
    the meta-learning algorithm with human-machine interaction and achieved significant
    improvement compared with strong baselines not trained with the meta-learning
    algorithms. A more direct way to solve the data finiteness problem in supervised
    learning was augmenting the dataset (Elder et al., [2020](#bib.bib79)), which
    also improved the response quality to some extent. Additionally, pretraining large-scale
    models on common corpus and then applying them in a domain that lacks annotated
    data is a popular approach in recent years (Henderson et al., [2019b](#bib.bib136);
    Mehri et al., [2019](#bib.bib244); Bao et al., [2019b](#bib.bib17)).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端系统通常完全依赖数据，这有助于它们生成鲁棒且自然的回应。然而，由于标注训练样本的有限性，如何在数据有限的情况下提高端到端任务导向对话系统的回应质量是一个热门的研究话题。使用基于规则的方法来约束回应生成是提升回应质量的一种方式。Balakrishnan
    等人（[2019](#bib.bib14)）使用线性化的树结构表示作为输入，以获得对话层次和句子层次语义概念的控制。Kale 和 Rastogi（[2020](#bib.bib162)）使用模板来提高生成回应的语义正确性。他们将回应生成分解为两个阶段：首先根据槽生成语义正确但可能不连贯的回应，并使用模板进行约束；然后在第二阶段，应用预训练的语言模型将生成的发言整理成连贯的回应。用强化学习训练网络是另一个缓解对标注数据依赖的策略。He
    等人（[2020a](#bib.bib128)）使用强化学习框架训练了两个教师网络，分别以知识检索和回应生成为目标。然后，学生网络通过模仿教师网络的输出学习生成回应。Dai
    等人（[2020](#bib.bib61)）在监督学习中尝试优化学习策略，以提高有限数据下模型的学习效率。他们将元学习算法与人机交互相结合，与未使用元学习算法的强基线相比，取得了显著的改善。解决监督学习中数据有限性问题的一个更直接的方法是扩充数据集（Elder
    等人，[2020](#bib.bib79)），这在一定程度上也提高了回应质量。此外，近年来一个流行的方法是先在通用语料库上预训练大规模模型，然后将其应用于缺乏标注数据的领域（Henderson
    等人，[2019b](#bib.bib136)；Mehri 等人，[2019](#bib.bib244)；Bao 等人，[2019b](#bib.bib17)）。
- en: 3.6.8 Retrieval Methods for Task-oriented Dialogue Systems
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.8 面向任务对话系统的检索方法
- en: Retrieval-based methods are rare in task-oriented systems for the insufficiency
    of candidate entries to cover all possible responses which usually involve specific
    knowledge from external knowledge-base. However, Henderson et al. ([2019b](#bib.bib136))
    argued that in some situations not relating with specific knowledge facts, retrieval-based
    methods were more precise and effective. They first pretrained the response selection
    model on general domain corpora and then finetuned on small target domain data.
    Experiments on six datasets from different domains proved the effectiveness of
    the pretrained response selection model. Lu et al. ([2019b](#bib.bib231)) constructed
    Spatio-temporal context features to facilitate response selection, and achieved
    significant improvements on the Ubuntu IRC dataset.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的方法在任务导向的系统中较为罕见，因为候选条目的不足无法覆盖所有可能的回答，这些回答通常涉及来自外部知识库的特定知识。然而，Henderson
    等人（[2019b](#bib.bib136)）认为，在一些与特定知识事实无关的情况下，基于检索的方法更为精确和有效。他们首先在通用领域语料上预训练了响应选择模型，然后在小规模目标领域数据上进行了微调。对来自不同领域的六个数据集的实验验证了预训练响应选择模型的有效性。Lu
    等人（[2019b](#bib.bib231)）构建了时空上下文特征以促进响应选择，并在 Ubuntu IRC 数据集上取得了显著的改进。
- en: 4 Open-Domain Dialogue Systems
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 开放领域对话系统
- en: This section discusses open-domain dialogue systems, which are also called chit-chat
    dialogue systems or non-task-oriented dialogue systems. Almost all state-of-the-art
    open-domain dialogue systems are based on neural methods. We organize this section
    by first briefly introducing the concepts of different branches of open-domain
    dialogue systems, and then we focus on different research challenges and hot topics.
    We view these challenges and hot topics as different research directions in open-domain
    dialogue systems.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论开放领域对话系统，也称为闲聊对话系统或非任务导向对话系统。几乎所有最先进的开放领域对话系统都基于神经方法。我们通过简要介绍开放领域对话系统不同分支的概念来组织本节内容，然后重点讨论不同的研究挑战和热点话题。我们将这些挑战和热点话题视为开放领域对话系统中的不同研究方向。
- en: 'Instead of managing to complete tasks, open-domain dialogue systems aim to
    perform chit-chat with users without the task and domain restriction (Ritter et al.,
    [2011](#bib.bib300)) and are usually fully data-driven. Open-domain dialogue systems
    are generally divided into three categories: generative systems, retrieval-based
    systems, and ensemble systems. Generative systems apply sequence-to-sequence models
    to map the user message and dialogue history into a response sequence that may
    not appear in the training corpus. By contrast, retrieval-based systems try to
    find a pre-existing response from a certain response set. Ensemble systems combine
    generative methods and retrieval-based methods in two ways: retrieved responses
    can be compared with generated responses to choose the best among them; generative
    models can also be used to refine the retrieved responses (Zhu et al., [2018](#bib.bib466);
    Song et al., [2016](#bib.bib337); Qiu et al., [2017](#bib.bib284); Serban et al.,
    [2017b](#bib.bib314)). Generative systems can produce flexible and dialogue context-related
    responses while sometimes they lack coherence and tend to make dull responses.
    Retrieval-based systems select responses from human response sets and thus are
    able to achieve better coherence in surface-level language. However, retrieval
    systems are restricted by the finiteness of the response sets and sometimes the
    responses retrieved show a weak correlation with the dialogue context (Zhu et al.,
    [2018](#bib.bib466)).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 开放领域对话系统的目标是与用户进行闲聊，而不受任务和领域限制（Ritter 等人，[2011](#bib.bib300)），通常完全以数据驱动。开放领域对话系统一般分为三类：生成系统、基于检索的系统和集成系统。生成系统使用序列到序列模型将用户消息和对话历史映射为可能在训练语料中不存在的响应序列。相比之下，基于检索的系统则尝试从特定响应集找到预先存在的响应。集成系统通过两种方式结合生成方法和基于检索的方法：检索到的响应可以与生成的响应进行比较，以选择最佳响应；生成模型也可以用来优化检索到的响应（Zhu
    等人，[2018](#bib.bib466)；Song 等人，[2016](#bib.bib337)；Qiu 等人，[2017](#bib.bib284)；Serban
    等人，[2017b](#bib.bib314)）。生成系统能够产生灵活且与对话上下文相关的响应，但有时缺乏连贯性，并且容易产生沉闷的响应。基于检索的系统从人工响应集选择响应，因此能够在表面语言上实现更好的连贯性。然而，检索系统受限于响应集的有限性，有时检索到的响应与对话上下文的相关性较弱（Zhu
    等人，[2018](#bib.bib466)）。
- en: In the next few subsections, we discuss some research challenges and hot topics
    in open-domain dialogue systems. We aim to to help researchers quickly grasp the
    current research trends via a systematic discussion on articles solving certain
    problems.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个小节中，我们将讨论开放域对话系统中的一些研究挑战和热点话题。我们的目标是通过系统化讨论解决特定问题的文章，帮助研究人员快速掌握当前的研究趋势。
- en: 4.1 Context Awareness
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 上下文感知
- en: Dialogue context consists of user and system messages and is an important source
    of information for dialogue agents to generate responses because dialogue context
    decides the conversation topic and user goal (Serban et al., [2017a](#bib.bib313)).
    A context-aware dialogue agent responds not only depending on the current message
    but also based on the conversation history. The earlier deep learning-based systems
    added up all word representations in dialogue history or used a fixed-size window
    to focus on the recent context (Sordoni et al., [2015b](#bib.bib340); Li et al.,
    [2015](#bib.bib191)). Serban et al. ([2016](#bib.bib312)) proposed Hierarchical
    Recurrent Encoder-Decoder (HRED), which was ground-breaking in building context-awareness
    dialogue systems. They built a word-level encoder to encode utterances and a turn-level
    encoder to further summarize and deliver the topic information over past turns.
    Xing et al. ([2018](#bib.bib424)) augmented the hierarchical neural networks with
    the attention mechanism to help the model focus on more meaningful parts of dialogue
    history.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 对话上下文由用户和系统消息组成，是对话代理生成回应的重要信息来源，因为对话上下文决定了对话的主题和用户目标（Serban 等，[2017a](#bib.bib313)）。一个具有上下文感知的对话代理不仅依赖于当前消息，还基于对话历史进行回应。早期基于深度学习的系统将对话历史中的所有词表示相加，或者使用固定大小的窗口来关注最近的上下文（Sordoni
    等，[2015b](#bib.bib340)；Li 等，[2015](#bib.bib191)）。Serban 等人（[2016](#bib.bib312)）提出了层次递归编码器-解码器（HRED），这一创新在构建上下文感知对话系统方面具有突破性意义。他们构建了一个词级编码器来编码发言，并且一个轮次级编码器来进一步总结并传递过去轮次中的主题信息。Xing
    等人（[2018](#bib.bib424)）通过加入注意力机制来增强层次神经网络，帮助模型集中关注对话历史中更有意义的部分。
- en: 'Both generative and retrieval-based systems rely heavily on dialogue context
    modeling. Shen et al. ([2019](#bib.bib321)) proposed Conversational Semantic Relationship
    RNN (CSRR) to model the dialogue context in three levels: utterance-level, pair-level,
    and discourse-level, capturing content information, user-system topic, and global
    topic respectively. Zhang et al. ([2019a](#bib.bib446)) argued that the hierarchical
    encoder-decoder does not lay enough emphasis on certain parts when the decoder
    interacted with dialogue contexts. Also, they claimed that attention-based HRED
    models also suffered from position bias and relevance assumption insufficiency
    problems. Therefore, they proposed ReCoSa, whose architecture was inspired by
    the transformer. The model first used a word-level LSTM to encode dialogue contexts,
    and then self-attention was applied to update the utterance representations. In
    the final stage, an encoder-decoder attention was computed to facilitate the response
    generation process. Additionally, Mehri et al. ([2019](#bib.bib244)) examined
    several applications of large-scale pretrained models in dialogue context learning,
    providing guidance for large-scale network selection in context modeling.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 生成型和基于检索的系统都高度依赖对话上下文建模。Shen 等人（[2019](#bib.bib321)）提出了对话语义关系RNN（CSRR），在三个层级（发言级、对级、话语级）上建模对话上下文，分别捕捉内容信息、用户-系统主题和全局主题。Zhang
    等人（[2019a](#bib.bib446)）认为层次编码器-解码器在解码器与对话上下文互动时对某些部分的重视不够。此外，他们声称基于注意力的HRED模型也存在位置偏差和相关性假设不足的问题。因此，他们提出了ReCoSa，其架构受到变换器的启发。该模型首先使用词级LSTM来编码对话上下文，然后应用自注意力更新发言表示。在最终阶段，计算了编码器-解码器注意力以促进回应生成过程。此外，Mehri
    等人（[2019](#bib.bib244)）考察了大规模预训练模型在对话上下文学习中的几种应用，为上下文建模中的大规模网络选择提供了指导。
- en: Some works propose structured attention to improve context-awareness. Qiu et al.
    ([2020](#bib.bib283)) learned structured dialogue context by combining structured
    attention with a Variational Recurrent Neural Network (VRNN). Comparatively, Ferracane
    et al. ([2019](#bib.bib90)) examined the RST discourse tree model proposed by Liu
    and Lapata ([2018](#bib.bib226)) and observed little or even no discourse structures
    in the learned latent tree. Thus, they argued that structured attention did not
    benefit dialogue modeling and sometimes might even harm the performance.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作提出了结构化注意力来改善上下文感知。Qiu 等人（[2020](#bib.bib283)）通过将结构化注意力与变分递归神经网络（VRNN）相结合，学习了结构化对话背景。相比之下，Ferracane
    等人（[2019](#bib.bib90)）检查了 Liu 和 Lapata（[2018](#bib.bib226)）提出的 RST 话语树模型，并观察到学习到的潜在树中几乎没有或根本没有话语结构。因此，他们认为结构化注意力未能使对话建模受益，有时甚至可能损害性能。
- en: Interestingly, Feng et al. ([2020b](#bib.bib88)) not only utilized dialogue
    history, but also future conversations. Considering that in real inference situations
    dialogue agents cannot be explicitly aware of future information, they first trained
    a scenario-based model jointly on past and future context and then used an imitation
    framework to transfer the scenario knowledge to a target network.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，Feng 等人（[2020b](#bib.bib88)）不仅利用了对话历史，还利用了未来对话。考虑到在实际推理情境中，对话代理不能明确知道未来信息，他们首先在过去和未来背景下联合训练了一个基于场景的模型，然后使用模仿框架将场景知识转移到目标网络。
- en: Better context modeling improves the response selection performance in retrieval-based
    dialogue systems (Jia et al., [2020](#bib.bib158)). Tao et al. ([2019](#bib.bib364))
    proposed Interaction-over-Interaction network (IoI), which consisted of multiple
    interaction blocks to perform deeper interactions between dialogue context and
    candidate responses. Jia et al. ([2020](#bib.bib158)) organized the dialogue history
    into conversation threads by performing classifications on their dependency relations.
    They further used a pretrained Transformer model to encode the threads and candidate
    responses to compute the matching score. Lin et al. ([2020b](#bib.bib216)) argued
    that response-retrieval datasets should not only be annotated with relevant or
    irrelevant responses. Instead, a greyscale metric should be used to measure the
    relevance degree of a response given the dialogue context, thus increasing the
    context-awareness ability of retrieval models.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的背景建模改善了基于检索的对话系统的响应选择性能（Jia 等人，[2020](#bib.bib158)）。Tao 等人（[2019](#bib.bib364)）提出了交互-交互网络（IoI），它由多个交互块组成，以在对话背景和候选回应之间进行更深入的交互。Jia
    等人（[2020](#bib.bib158)）通过对对话历史进行依赖关系分类，将其组织为对话线程。他们进一步使用预训练的 Transformer 模型对线程和候选回应进行编码，以计算匹配分数。Lin
    等人（[2020b](#bib.bib216)）认为，响应检索数据集不应仅用相关或无关的响应进行标注。相反，应使用灰度度量来衡量给定对话背景的响应的相关程度，从而提高检索模型的上下文感知能力。
- en: Dialogue rewriting problem aims to convert several messages into a single message
    conveying the same information and dialogue context awareness is very crucial
    to this task (Xu et al., [2020b](#bib.bib427)). Su et al. ([2019a](#bib.bib343))
    modeled multi-turn dialogues via dialogue rewriting and benefited from the conciseness
    of rewritten utterances.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 对话重写问题旨在将若干条消息转换为一条传达相同信息的消息，而对话背景意识对于这一任务至关重要（Xu 等人，[2020b](#bib.bib427)）。Su
    等人（[2019a](#bib.bib343)）通过对话重写建模多轮对话，并受益于重写话语的简洁性。
- en: 4.2 Response Coherence
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 响应一致性
- en: Coherence is one of the qualities that a good generator seeks (Stent et al.,
    [2005](#bib.bib342)). Coherence means maintaining logic and consistency in a dialogue,
    which is essential in an interaction process for that a response with weak consistency
    in logic and grammar is hard to understand. Coherence is a hot topic in generative
    systems but not in retrieval-based systems because candidate responses in retrieval
    methods are usually human responses, which are naturally coherent.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一致性是一个优秀生成器所追求的品质之一（Stent 等人，[2005](#bib.bib342)）。一致性意味着在对话中保持逻辑和连贯性，这在互动过程中至关重要，因为逻辑和语法一致性差的回应难以理解。一致性在生成系统中是一个热点话题，但在基于检索的系统中却不是，因为检索方法中的候选回应通常是人工回应，天然具有一致性。
- en: Refining the order or granularity of sentence functions is a popular strategy
    for improving the language coherence. Wu et al. ([2019b](#bib.bib415)) improved
    the response coherence via the task of inconsistent order detection. The dialogue
    systems learned response generation and order detection jointly, which was self-supervised
    multi-task learning. Xu et al. ([2019](#bib.bib425)) presented the concept of
    meta-words. Meta-words were diverse attributes describing the response. Learning
    dialogue based on meta-words helped promote response generation in a more controllable
    way. Liu et al. ([2019](#bib.bib222)) used three granularities of encoders to
    encode raw words, low-level clusters, and high-level clusters. The architecture
    was called Vocabulary Pyramid Network (VPN), which performed a multi-pass encoding
    and decoding process on hierarchical vocabularies to generate coherent responses.
    Shen et al. ([2019](#bib.bib321)) also built a three-level hierarchical dialogue
    model to capture richer features and improved the response quality. Ji et al.
    ([2020](#bib.bib155)) built Cross Copy Networks (CCN), which used a copy mechanism
    to copy from similar dialogues based on the current dialogue context. Thus, the
    system benefited from the pre-existing coherent responses, which alleviated the
    need of performing the reasoning process from scratch.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼句子功能的顺序或粒度是一种提高语言连贯性的流行策略。吴等人 ([2019b](#bib.bib415)) 通过不一致顺序检测任务提高了响应连贯性。对话系统通过联合学习响应生成和顺序检测，这是一种自监督的多任务学习。徐等人
    ([2019](#bib.bib425)) 提出了元词的概念。元词是描述响应的多样化属性。基于元词的对话学习有助于以更可控的方式促进响应生成。刘等人 ([2019](#bib.bib222))
    使用三种粒度的编码器来编码原始词汇、低级别簇和高级别簇。该架构称为词汇金字塔网络（VPN），对分层词汇进行多次编码和解码过程，以生成连贯的响应。沈等人 ([2019](#bib.bib321))
    还建立了一个三级层次对话模型，以捕捉更丰富的特征，并提高了响应质量。季等人 ([2020](#bib.bib155)) 构建了交叉拷贝网络（CCN），使用拷贝机制基于当前对话上下文从相似对话中拷贝。因此，系统从预先存在的连贯响应中受益，减少了从头开始进行推理的需求。
- en: Many work employ strategies to achieve response coherence on a higher level,
    which improves the overall quality of the generated responses. Li et al. ([2019b](#bib.bib199))
    improved the logical consistency of generated utterances by incorporating an unlikelihood
    loss to control the distribution mismatches. Bao et al. ([2019a](#bib.bib16))
    proposed a Generation-Evaluation framework that evaluated the qualities, including
    coherence, of the generated response. The feedback was further seen as a reward
    signal in the reinforcement learning framework and guided to a better dialogue
    strategy via policy gradient, thus improving the response quality. Gao et al.
    ([2020b](#bib.bib101)) raised response quality by ranking generated responses
    based on user feedbacks like upvotes, downvotes, and comments on social networks.
    Zhu et al. ([2018](#bib.bib466)) built a retrieval-enhanced generation model,
    which enhanced the generated responses in two ways. First, a discriminator was
    trained with the help of a retrieval system, and then the generator was trained
    in a GAN framework under the supervision signal of a discriminator. Second, retrieved
    responses were also used as a part of the generator input to provide a coherent
    example for the generator. Xu et al. ([2020a](#bib.bib426)) achieved a global
    coherent dialogue by constructing a knowledge graph from corpora. They further
    performed graph walks to decide “what to say" and “how to say", thus improving
    the dialogue flow coherence. Mesgar et al. ([2019](#bib.bib245)) proposed an assessment
    approach for dialogue coherence evaluation by combining the dialogue act prediction
    in a multi-task learning framework and learned rich dialogue representations.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工作采用策略以在更高层次上实现响应一致性，从而提高生成响应的整体质量。Li 等（[2019b](#bib.bib199)）通过引入不相似损失来控制分布不匹配，从而提高了生成话语的逻辑一致性。Bao
    等（[2019a](#bib.bib16)）提出了一种生成-评估框架，评估生成响应的质量，包括一致性。反馈被进一步视为强化学习框架中的奖励信号，通过策略梯度指导更好的对话策略，从而提高响应质量。Gao
    等（[2020b](#bib.bib101)）通过根据用户反馈（如点赞、点踩和社交网络上的评论）对生成响应进行排名，从而提高了响应质量。Zhu 等（[2018](#bib.bib466)）构建了一个检索增强生成模型，从两个方面提升了生成响应。首先，利用检索系统训练了一个鉴别器，然后在鉴别器的监督信号下，生成器在
    GAN 框架中进行训练。其次，检索到的响应也作为生成器输入的一部分，提供了生成器的连贯示例。Xu 等（[2020a](#bib.bib426)）通过从语料库中构建知识图谱，实现了全球一致的对话。他们进一步执行图谱遍历，以决定“说什么”和“如何说”，从而改善了对话流的连贯性。Mesgar
    等（[2019](#bib.bib245)）提出了一种对话一致性评估方法，通过将对话行为预测与多任务学习框架结合，并学习丰富的对话表示。
- en: There also evolve some data-wise methods for better response coherence. Bi et al.
    ([2019](#bib.bib22)) proposed to annotate sentence functions in existing conversation
    datasets to improve the sentence logic and coherence of generated responses. Akama
    et al. ([2020](#bib.bib3)) focused on data effectiveness as well. They filtered
    out low-quality utterance pairs by scoring the relatedness and connectivity, which
    was proved to be effective in improving the response coherence. Akama et al. ([2020](#bib.bib3))
    presented a method for evaluating dataset utterance pairs’ quality in terms of
    connectedness and relatedness. The proposed scoring technique is based on research
    findings that have been widely disseminated in the conversation and linguistics
    communities. Lison and Bibauw ([2017](#bib.bib219)) included a weighting model
    in their neural architecture. The weighting model, which is based on conversation
    data, assigns a numerical weight to each training sample that reflects its intrinsic
    quality for dialogue modeling and achieved good result in experiments.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 还发展了一些数据方面的方法以改善响应一致性。Bi 等（[2019](#bib.bib22)）建议在现有对话数据集中注释句子功能，以提高生成响应的句子逻辑和一致性。Akama
    等（[2020](#bib.bib3)）也关注数据有效性。他们通过对相关性和连贯性进行评分，筛选出低质量的发言对，这被证明在提高响应一致性方面有效。Akama
    等（[2020](#bib.bib3)）提出了一种评估数据集发言对质量的方法，依据连贯性和相关性。所提出的评分技术基于已广泛传播于对话和语言学领域的研究成果。Lison
    和 Bibauw（[2017](#bib.bib219)）在他们的神经网络架构中加入了一个加权模型。该加权模型基于对话数据，为每个训练样本分配一个数值权重，反映其对对话建模的内在质量，并在实验中取得了良好效果。
- en: 4.3 Response Diversity
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 响应多样性
- en: The Bland and generic response is a long-existing problem in generative dialogue
    systems. Because of the high frequency of generic responses like “I don’t know"
    in training samples and the beam search decoding scheme of neural sequence-to-sequence
    models, generative dialogue systems tend to respond with universally acceptable
    but meaningless utterances (Serban et al., [2016](#bib.bib312); Vinyals and Le,
    [2015](#bib.bib379); Sordoni et al., [2015b](#bib.bib340)). For example, to respond
    to the user message “I really want to have a meal", the agent tends to choose
    simple responses like “It’s OK" instead of responding with more complicated sentences
    like recommendations and suggestions.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 乏味和通用的响应是生成对话系统中长期存在的问题。由于训练样本中“我不知道”等通用响应的高频率以及神经序列到序列模型的束搜索解码方案，生成对话系统往往响应通用但毫无意义的发言
    (Serban 等人, [2016](#bib.bib312); Vinyals 和 Le, [2015](#bib.bib379); Sordoni 等人,
    [2015b](#bib.bib340))。例如，回应用户消息“我真的很想吃饭”，代理通常选择简单的回应如“没问题”，而不是回应更复杂的句子，如建议和推荐。
- en: 'Earlier works solve this challenge by modifying the decoding objective or adding
    a reranking process. Li et al. ([2015](#bib.bib191)) replaced the traditional
    likelihood objective $p(R|C)$ with mutual information. The optimization of mutual
    information objective aims to achieve a Maximum Mutual Information (MMI). Specifically,
    the task is to find a best response $R$ based on the dialogue context $C$, in
    order to maximize their mutual information:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的工作通过修改解码目标或添加重新排序过程来解决这一挑战。Li 等人 ([2015](#bib.bib191)) 用互信息替代了传统的似然目标 $p(R|C)$。互信息目标的优化旨在实现最大互信息（MMI）。具体来说，任务是根据对话上下文
    $C$ 找到最佳响应 $R$，以最大化它们的互信息：
- en: '|  | $\begin{split}\hat{R}&amp;=arg\max_{R}{log\frac{P(C,R)}{P(C)P(R)}}\\ &amp;=arg\max_{R}{logP(R&#124;C)-logP(R)}\end{split}$
    |  | (49) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\hat{R}&amp;=arg\max_{R}{log\frac{P(C,R)}{P(C)P(R)}}\\ &amp;=arg\max_{R}{logP(R&#124;C)-logP(R)}\end{split}$
    |  | (49) |'
- en: 'The objective $p(R|C)$ causes the model to choose responses with high probability
    even if the response is unconditionally frequent in the dataset, thus causing
    it to ignore the content of $C$. Maximizing the mutual information as Equation
    ([49](#S4.E49 "In 4.3 Response Diversity ‣ 4 Open-Domain Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")) solves
    this issue by achieving a trade-off between safety and relativity.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '目标 $p(R|C)$ 会使模型选择高概率的响应，即使响应在数据集中无条件地频繁，从而导致模型忽略 $C$ 的内容。最大化互信息，如公式 ([49](#S4.E49
    "In 4.3 Response Diversity ‣ 4 Open-Domain Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey")) 通过在安全性和相关性之间取得平衡来解决此问题。'
- en: With a similar intuition as described above, increasing response diversity by
    modifying the decoding scheme at inference time has been explored in earlier works. Vijayakumar
    et al. ([2016](#bib.bib378)) combined a dissimilarity term into the beam search
    objective and proposed Diverse Beam Search (DBS) to promote diversity. Similarly, Shao
    et al. ([2017](#bib.bib319)) proposed a stochastic beam search algorithm by performing
    stochastic sampling when choosing top-B responses. In the beam search algorithm,
    siblings sharing the same parent nodes tended to guide to similar sequences. Inspired
    by this, Li et al. ([2016c](#bib.bib194)) penalized siblings sharing the same
    parent nodes using an additional term in the beam search objective. This encouraged
    the algorithm to search more diverse paths by expanding from different parent
    nodes. Some works further added a reranking stage to select more diverse responses
    in the generated N-best list (Li et al., [2015](#bib.bib191); Sordoni et al.,
    [2015b](#bib.bib340); Shao et al., [2017](#bib.bib319)).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述相似的直觉，通过在推理时修改解码方案以增加响应多样性已经在早期工作中得到探索。Vijayakumar 等人 ([2016](#bib.bib378))
    在束搜索目标中结合了一个不相似性项，提出了多样束搜索（DBS）以促进多样性。类似地，Shao 等人 ([2017](#bib.bib319)) 通过在选择前
    B 个响应时进行随机采样，提出了一种随机束搜索算法。在束搜索算法中，共享相同父节点的兄弟节点往往会引导到相似的序列。受到此启发，Li 等人 ([2016c](#bib.bib194))
    使用束搜索目标中的额外项对共享相同父节点的兄弟节点进行惩罚。这鼓励算法通过从不同的父节点扩展以搜索更多的多样路径。一些工作进一步添加了重新排序阶段，以在生成的
    N-best 列表中选择更多的多样响应 (Li 等人, [2015](#bib.bib191); Sordoni 等人, [2015b](#bib.bib340);
    Shao 等人, [2017](#bib.bib319))。
- en: A user message can be mapped into multiple acceptable responses, which is also
    known as the one-to-many mapping problem. Qiu et al. ([2019](#bib.bib282)) considered
    the one-to-many mapping problem in open-domain dialogue systems and proposed a
    two-stage generation model to increase response diversity - the first stage extracting
    common features of multiple ground truth responses and the second stage extracting
    the distinctive ones. Ko et al. ([2020](#bib.bib171)) solved the one-to-many mapping
    problem via a classification task to learn latent semantic representations. So
    that given one example response, different ones could be generated by exploring
    the semantically close vectors in the latent space.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 用户消息可以映射到多个可接受的响应，这也被称为一对多映射问题。Qiu 等人 ([2019](#bib.bib282)) 研究了开放域对话系统中的一对多映射问题，并提出了一个两阶段生成模型来增加响应多样性——第一阶段提取多个真实响应的共性特征，第二阶段提取独特特征。Ko
    等人 ([2020](#bib.bib171)) 通过分类任务解决了一对多映射问题，以学习潜在的语义表示。这样，给定一个示例响应，可以通过探索潜在空间中的语义相近的向量生成不同的响应。
- en: Different training strategies have been proposed to increase response diversity.
    Bao et al. ([2019a](#bib.bib16)) used human instinct or pre-defined objective
    as a reward signal in a reinforcement learning setting to prompt the agent to
    avoid generating dull responses. Still, in a reinforcement learning framework, Zhu
    et al. ([2020](#bib.bib467)) performed counterfactual reasoning to explore the
    potential response space. Given a pre-existing response, the model inferred another
    policy, which represented another possible response, thus increasing the response
    diversity. He and Glass ([2019](#bib.bib127)) used a negative training method
    to minimize the generation of bland responses. They first collected negative samples
    and then gave negative training signals based on these samples to fine-tune the
    model, impeding the model to generate bland responses. To achieve a better performance, Du
    and Black ([2019](#bib.bib76)) synthesized different dialogue models designed
    for response diversity based on boosting training. The ensemble model significantly
    outperformed each of its base models.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出不同的训练策略以增加响应的多样性。Bao 等人 ([2019a](#bib.bib16)) 在强化学习设置中使用人类本能或预定义目标作为奖励信号，促使智能体避免生成乏味的响应。然而，在强化学习框架中，Zhu
    等人 ([2020](#bib.bib467)) 进行了反事实推理，以探索潜在的响应空间。给定一个预先存在的响应，模型推断出另一个策略，代表另一种可能的响应，从而增加响应的多样性。He
    和 Glass ([2019](#bib.bib127)) 使用负训练方法来最小化平淡响应的生成。他们首先收集负样本，然后基于这些样本提供负训练信号来微调模型，阻止模型生成平淡的响应。为了获得更好的性能，Du
    和 Black ([2019](#bib.bib76)) 基于提升训练合成了不同的对话模型，以提高响应多样性。集成模型显著优于其每个基础模型。
- en: Utilizing external knowledge sources is another way to improve the diversity
    of generated responses because it can enrich the content. Wu et al. ([2020b](#bib.bib416))
    built a common-sense dialogue generation model which seeks highly related knowledge
    facts based on the dialogue history. Likewise, Su et al. ([2020](#bib.bib344))
    incorporated external knowledge sources to diversify the response generation,
    but the difference was that they utilized non-conversational texts like news articles
    as relevant knowledge facts, which were obviously easier to obtain. Tian et al.
    ([2019](#bib.bib369)) used a memory module to abstract and store useful information
    in the training corpus for generating diverse responses.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 利用外部知识来源是提高生成响应多样性的另一种方式，因为它可以丰富内容。Wu 等人 ([2020b](#bib.bib416)) 构建了一个常识对话生成模型，基于对话历史寻找高度相关的知识事实。同样，Su
    等人 ([2020](#bib.bib344)) 结合外部知识来源以多样化响应生成，但不同的是，他们利用非对话文本如新闻文章作为相关知识事实，这显然更容易获得。Tian
    等人 ([2019](#bib.bib369)) 使用记忆模块来抽象和存储训练语料库中的有用信息，以生成多样化的响应。
- en: Another approach to diversify the response generation is to make modifications
    to the training corpus. Csáky et al. ([2019](#bib.bib59)) solved the challenge
    by filtering out the generic responses in the dataset using an entropy-based algorithm,
    which was simple but effective. Augmented with human feedback data, Gao et al.
    ([2020b](#bib.bib101)) proposed that the generated responses could be reranked
    via a response ranking framework trained on the human feedback data and responses
    with higher quality including diversity were selected. Stasaski et al. ([2020](#bib.bib341))
    proposed to change the data collection pipeline by iteratively computing the diversity
    of responses from different human participants in dataset construction and selected
    those participants who tend to generate informative and diverse responses.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种多样化响应生成的方法是对训练语料库进行修改。Csáky等人（[2019](#bib.bib59)）通过使用基于熵的算法过滤掉数据集中的通用响应，解决了这个挑战，这种方法简单但有效。Gao等人（[2020b](#bib.bib101)）在加入人类反馈数据后，提出通过一个基于反馈数据训练的响应排序框架对生成的响应进行重新排序，并选择了包括多样性在内的高质量响应。Stasaski等人（[2020](#bib.bib341)）提出通过迭代计算数据集中来自不同人类参与者的响应多样性来改变数据收集流程，并选择那些倾向于生成信息丰富且多样化响应的参与者。
- en: 4.4 Speaker Consistency and Personality-based Response
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 演讲者一致性和基于个性化的响应
- en: In open-domain dialogue systems, one big issue is that the responses are entirely
    learned from training data. The inconsistent response may be received when asking
    the system about some personal facts (e.g., age, hobbies). If the dataset contains
    multiple utterance pairs about the query of age, then the response generated tends
    to be shifting, which is unacceptable because personal facts are usually not random.
    Thus, for a data-driven chatbot, it is necessary to be aware of its role and respond
    based on a fixed persona.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在开放领域对话系统中，一个大问题是响应完全基于训练数据学习。当询问系统一些个人事实（例如年龄、爱好）时，可能会收到不一致的响应。如果数据集中包含多个关于年龄的对话对，那么生成的响应往往会有所偏移，这是不可接受的，因为个人事实通常不是随机的。因此，对于数据驱动的聊天机器人来说，有必要意识到其角色，并根据固定的个性进行回应。
- en: Explicitly modeling the persona is the main strategy in recent works. Liu et al.
    ([2020b](#bib.bib225)) proposed a persona-based dialogue generator consisting
    of a Receiver and a Transmitter. The receiver was responsible for modeling the
    interlocutor’s persona through several turns’ chat while Transmitter generated
    utterances based on the persona of agent and interlocutor, together with conversation
    content. The proposed model supported conversations between two persona-based
    chatbots by modeling each other’s persona. Without training with additional Natural
    Language Inference labels, Kim et al. ([2020](#bib.bib167)) built an imaginary
    listener following a normal generator, which reasoned over the tokens generated
    by the generator and predicted a posterior distribution over the personas in a
    certain space. After that, a self-conscious speaker generated tokens aligned with
    the predicted persona. Likewise, Boyd et al. ([2020](#bib.bib28)) used an augmented
    GPT-2 to reason over the past conversations and model the target actor’s persona,
    conditioning on which persona consistency was achieved.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 显式建模个性是近期研究的主要策略。Liu等人（[2020b](#bib.bib225)）提出了一种基于个性的对话生成器，包括接收器和发射器。接收器负责通过多个回合的对话建模对话者的个性，而发射器则基于代理和对话者的个性以及对话内容生成发言。该模型通过建模彼此的个性，支持两个基于个性的聊天机器人之间的对话。Kim等人（[2020](#bib.bib167)）在没有额外自然语言推理标签的情况下，构建了一个跟随普通生成器的虚拟听众，该听众对生成器生成的标记进行推理，并预测某一空间中的个性后验分布。随后，一个自我意识的发言者生成了与预测个性一致的标记。同样，Boyd等人（[2020](#bib.bib28)）使用了增强的GPT-2对过去的对话进行推理，并建模目标演员的个性，从而实现了个性一致性。
- en: Responding with personas needs to condition on some persona descriptions. For
    example, to build a generous agent, descriptions like “I am a generous person"
    are needed as a part of the model input. However, these descriptions require hand-crafted
    feature design, which is labor intensive. Madotto et al. ([2019](#bib.bib237))
    proposed to use Model-Agnostic Meta-Learning (MAML) to adapt to new personas with
    only a few training samples and needed no persona description. Majumder et al.
    ([2020a](#bib.bib238)) relied on external knowledge sources to expand current
    persona descriptions so that richer persona descriptions were obtained, and the
    model could associate current descriptions with some commonsense facts.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 使用人格进行回应需要依据一些人格描述。例如，为了构建一个慷慨的代理，模型输入中需要包含类似“我是一位慷慨的人”的描述。然而，这些描述需要手工设计特征，这非常费力。Madotto
    et al. ([2019](#bib.bib237)) 提出了使用模型无关元学习（MAML）来适应新的人格，只需少量训练样本且不需要人格描述。Majumder
    et al. ([2020a](#bib.bib238)) 依赖于外部知识来源扩展当前的人格描述，从而获得更丰富的人格描述，使得模型能够将当前描述与一些常识事实关联起来。
- en: Song et al. ([2020a](#bib.bib335)) argued that traditional persona-based systems
    were one-stage systems and the responses they generated still contain many persona
    inconsistent words. To tackle this issue, they proposed a three-stage architecture
    to ensure persona consistency. A generate-delete-rewrite mechanism was implemented
    to remove the unacceptable words generated in prototype responses and rewrite
    them.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: Song et al. ([2020a](#bib.bib335)) 认为传统的人格系统是单阶段系统，它们生成的回应仍然包含许多人格不一致的词。为了解决这个问题，他们提出了一个三阶段架构以确保人格一致性。实现了生成-删除-重写机制，以去除原型回应中生成的不接受的词汇并进行重写。
- en: 4.5 Empathetic Response
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 同理回应
- en: Empathy means being able to sense other people’s feelings (Ma et al., [2020b](#bib.bib236)).
    An empathetic dialogue system can sense the user’s emotional changes and produce
    appropriate responses with a certain sentiment. This is an essential topic in
    chit-chat systems because it directly affects the user’s feeling and to some extent
    decides the response quality. Industry systems such as Microsoft’s Cortana, Facebook
    M, Google Assistant, and Amazon’s Alexa are all equipped with empathy modules (Wang
    et al., [2020g](#bib.bib395)).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 同理心意味着能够感知他人的感受（Ma et al., [2020b](#bib.bib236)）。一个同理对话系统能够感知用户的情感变化，并产生带有某种情感的适当回应。这是闲聊系统中的一个重要话题，因为它直接影响用户的感受，并在一定程度上决定回应的质量。像微软的
    Cortana、Facebook M、Google Assistant 和亚马逊的 Alexa 等行业系统都配备了同理心模块（Wang et al., [2020g](#bib.bib395)）。
- en: 'There are two ways to generate utterances with emotion: one is to use explicit
    sentiment words as a part of input; another is to implicitly combine neural words (Song
    et al., [2019](#bib.bib338)). Song et al. ([2019](#bib.bib338)) proposed a unified
    framework that uses a lexicon-based attention to explicitly plugin emotional words
    and a sequence-level emotion classifier to classify the output sequence, implicitly
    guiding the generator to generate emotional responses through backpropagation.
    Zhong et al. ([2020](#bib.bib460)) used CoBERT for persona-based empathetic response
    selection and further investigated the impact of persona on empathetic responses.
    Smith et al. ([2020](#bib.bib333)) blended the skills of being knowledgeable,
    empathetic, and role-aware in one open-domain conversation model and overcame
    the bias issue when blending these skills.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 生成带有情感的发言有两种方法：一种是将显式情感词作为输入的一部分；另一种是隐式地结合神经网络词汇（Song et al., [2019](#bib.bib338)）。Song
    et al. ([2019](#bib.bib338)) 提出了一个统一的框架，使用基于词典的注意力机制显式地插入情感词，并使用序列级情感分类器来分类输出序列，通过反向传播隐式地引导生成器生成情感响应。Zhong
    et al. ([2020](#bib.bib460)) 使用 CoBERT 进行基于人格的同理回应选择，并进一步研究了人格对同理回应的影响。Smith et
    al. ([2020](#bib.bib333)) 将知识渊博、同理心和角色意识的技能融合在一个开放领域对话模型中，并解决了在融合这些技能时的偏见问题。
- en: Since the available datasets for empathetic conversations are scarce, Rashkin
    et al. ([2018](#bib.bib291)) provided a new benchmark and dataset for empathetic
    dialogue systems. Oraby et al. ([2019](#bib.bib264)) constructed a dialogue dataset
    with rich emotional markups from user reviews and further proposed a novel way
    to generate similar datasets with rich markups.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可用于同理对话的数据集稀缺，Rashkin et al. ([2018](#bib.bib291)) 提供了一个新的基准和数据集用于同理对话系统。Oraby
    et al. ([2019](#bib.bib264)) 构建了一个具有丰富情感标记的对话数据集，并进一步提出了一种生成具有丰富标记的类似数据集的新方法。
- en: 4.6 Controllable Generation
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 可控生成
- en: Controllable dialogue generation is an important line of work in open-domain
    dialogue systems since solely learning from data sample distributions causes many
    uncertain responses. Some of the dialogue systems are grounded on some external
    knowledge such as knowledge graph and documents. However, grounding alone without
    explicit control and semantic targeting may induce output that is accurate but
    vague.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 可控对话生成是开放域对话系统中的一个重要方向，因为仅仅从数据样本分布中学习会导致许多不确定的回应。一些对话系统基于外部知识，如知识图谱和文档。然而，仅仅依靠这些知识而没有明确的控制和语义目标可能会导致输出准确但模糊的结果。
- en: We may get some inspirations from the prior work on language generation and
    machine translation since similarly to dialogue systems they are generation-based
    or seq-to-seq problems. Some related work aimed to enforce user-specified constraints,
    most notably using lexical constraints (Hokamp and Liu, [2017](#bib.bib139); Hu
    et al., [2019](#bib.bib143); Miao et al., [2019](#bib.bib248)). These methods
    exclusively use constraints at inference time. Constraints can be included into
    the latent space during training, resulting in better predictions. Other studies
    (See et al., [2019](#bib.bib311); Keskar et al., [2019](#bib.bib165); Tang et al.,
    [2019](#bib.bib362)) have looked at non-lexical constraints, but they haven’t
    looked into how they can help with grounding external knowledge. These publications
    also assume that the system can always be given (gold) constraints, which limits
    the ability to demonstrate larger benefits of the approaches.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从语言生成和机器翻译的先前工作中获得一些灵感，因为类似于对话系统，它们也是基于生成或序列到序列的问题。一些相关工作旨在强制用户指定的约束，最显著的是使用词汇约束（Hokamp
    and Liu, [2017](#bib.bib139); Hu et al., [2019](#bib.bib143); Miao et al., [2019](#bib.bib248)）。这些方法仅在推理时使用约束。约束可以在训练期间包含到潜在空间中，从而获得更好的预测。其他研究（See
    et al., [2019](#bib.bib311); Keskar et al., [2019](#bib.bib165); Tang et al.,
    [2019](#bib.bib362)）探讨了非词汇约束，但没有研究它们如何帮助扎根于外部知识。这些出版物还假设系统可以始终提供（黄金）约束，这限制了方法展示更大收益的能力。
- en: Controllable text generation has also been used to extract high-level style
    information from contextual information in text style transfer (Hu et al., [2017](#bib.bib144))
    and other tasks (Ficler and Goldberg, [2017](#bib.bib91); Dong et al., [2017](#bib.bib74);
    Gao et al., [2019](#bib.bib100)), allowing the former to be independently modified.
    Zhao et al. ([2018](#bib.bib457)) learns an interpretable representation for dialogue
    systems using discrete latent actions. While existing studies employ “style" descriptors
    (e.g., positive/negative, formal/informal) as control signals, Wu et al. ([2020c](#bib.bib419))
    use specific lexical constraints to regulate creation, allowing for finer semantic
    control. Content planned generation (Wiseman et al., [2017](#bib.bib411); Hua
    and Wang, [2019](#bib.bib145)) focuses response generation on a small number of
    essential words or table entries. This line of work, on the other hand, does not
    require consideration of the discourse context, which is critical for response
    generation.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 可控文本生成也被用于从文本风格迁移中的上下文信息中提取高级风格信息（Hu et al., [2017](#bib.bib144)）及其他任务（Ficler
    and Goldberg, [2017](#bib.bib91); Dong et al., [2017](#bib.bib74); Gao et al.,
    [2019](#bib.bib100)），允许前者独立修改。Zhao et al. ([2018](#bib.bib457)) 使用离散潜在动作学习对话系统的可解释表示。虽然现有研究采用“风格”描述符（例如，正面/负面，正式/非正式）作为控制信号，但Wu
    et al. ([2020c](#bib.bib419)) 使用特定的词汇约束来调节生成，从而实现更细致的语义控制。内容规划生成（Wiseman et al.,
    [2017](#bib.bib411); Hua and Wang, [2019](#bib.bib145)）将回应生成聚焦于少量关键字或表格条目。另一方面，这一方向的工作并不需要考虑话语上下文，而这对于回应生成至关重要。
- en: 4.7 Conversation Topic
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 对话主题
- en: Daily chats of people usually involve a topic or goal. Actually, a topic or
    goal is the key to keep each participant engaged in conversations and thus being
    essential to a chatbot. In real applications, a good topic model helps to retrieve
    related knowledge and guide the conversation instead of passively responding to
    the user’s message (Xing et al., [2017](#bib.bib423)). For example, if the user
    mentions “I like sunny days", a topic-aware system may reason over relevant external
    knowledge and produce responses like “I know there is a nice park near the seaside,
    have you ever been there before?". Thus, the agent pushes the conversation to
    a more engaging stage and enriches the dialogue content.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 人们的日常聊天通常涉及一个话题或目标。实际上，话题或目标是保持每个参与者参与对话的关键，因此对聊天机器人至关重要。在实际应用中，一个好的话题模型有助于检索相关知识并引导对话，而不是被动地回应用户的消息（Xing
    et al., [2017](#bib.bib423)）。例如，如果用户提到“我喜欢晴天”，一个了解话题的系统可能会根据相关的外部知识推理并产生类似“我知道海边附近有一个不错的公园，你去过那里吗？”的回应。因此，代理将对话推向一个更具吸引力的阶段，并丰富了对话内容。
- en: Almost all topic-aware dialogue agents need to model explicit topics, which
    can be entities from external knowledge-base, or topic embeddings that have some
    semantic meaning. Wu et al. ([2019c](#bib.bib417)) tried to change the traditional
    passive response fashion and radically pursue active guidance of conversation.
    The dialogue agent consists of a leader and a follower, where the leader reasons
    over a knowledge graph and decides the conversation topic. Likewise, a common-sense
    knowledge graph was used by Liu et al. ([2020c](#bib.bib227)) to lead the conversation
    topic and make recommendations. Tang et al. ([2019](#bib.bib362)) built a topic-aware
    retrieval-based chatbot. It aimed to guide the conversation topic to the target
    one step by step. It used a keyword predictor to predict turn-level keywords and
    selected the discourse-level keyword based on that. The discourse-level keyword
    was further fed into the retrieval model to retrieve responses regarding a certain
    topic. Chen and Yang ([2020](#bib.bib40)) built a multi-view sequence-to-sequence
    model to learn dialogue topics by first extracting dialogue structures of unstructured
    chit-chat dialogues, then generating topic summaries using BART decoder.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有的话题感知对话代理都需要建模显式话题，这些话题可以是来自外部知识库的实体，或具有某些语义意义的话题嵌入。Wu et al. ([2019c](#bib.bib417))
    尝试改变传统的被动响应模式，并彻底追求对话的主动引导。对话代理由一个领导者和一个跟随者组成，其中领导者对知识图谱进行推理并决定对话话题。同样，Liu et
    al. ([2020c](#bib.bib227)) 使用常识知识图谱来引导对话话题并做出推荐。Tang et al. ([2019](#bib.bib362))
    构建了一个话题感知的检索型聊天机器人。它旨在逐步将对话话题引导到目标话题上。它使用关键词预测器预测轮次级关键词，并基于此选择话语级关键词。话语级关键词进一步输入到检索模型中，以检索关于特定话题的回应。Chen
    和 Yang ([2020](#bib.bib40)) 构建了一个多视角的序列到序列模型，通过首先提取非结构化闲聊对话的对话结构，然后使用 BART 解码器生成话题总结来学习对话话题。
- en: In some applications of certain scenarios the conversation topic is essential,
    and these are where the topic-aware dialogue agents can be applied to. Zhang and
    Danescu-Niculescu-Mizil ([2020](#bib.bib448)) studied the topic-aware chatbot
    in counseling conversations. In counseling conversations, the agent led the dialogue
    topic by deciding between empathetically addressing a situation within the current
    range and moving on to a new target resolution. Cao et al. ([2019](#bib.bib35))
    studied chatbots in the psychotherapy treatment area and built a topic prediction
    model to forecast the behavior codes for upcoming conversations, thus guiding
    the dialogue.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些场景的应用中，对话话题至关重要，这些场景就是可以应用话题感知对话代理的地方。Zhang 和 Danescu-Niculescu-Mizil ([2020](#bib.bib448))
    研究了咨询对话中的话题感知聊天机器人。在咨询对话中，代理通过在当前范围内富有同情心地处理情况和转向新的目标解决方案之间进行决定，从而引导对话话题。Cao et
    al. ([2019](#bib.bib35)) 研究了心理治疗领域的聊天机器人，并建立了一个话题预测模型来预测即将到来的对话的行为代码，从而引导对话。
- en: 4.8 Knowledge-Grounded System
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8 知识驱动系统
- en: External knowledge such as common-sense knowledge is a significant source of
    information when organizing an utterance. Humans associate current conversation
    context with their experiences and memories and produce meaningful related responses,
    such capability results in the gap between human and machine chit-chat systems.
    As discussed, the earlier chit-chat systems are simply variants of machine translation
    systems, which can be viewed as sequence-to-sequence language models. However,
    dialogue generation is much more complicated than machine translation because
    of the higher freedom and vaguer constraints. Thus, chit-chat systems cannot simply
    consist of a sequence-to-sequence mapping since appropriate and informative responses
    are always related to some external common-sense knowledge. Instead, there must
    be a module incorporating world knowledge.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 外部知识，例如常识，是组织言语时的重要信息来源。人类将当前对话的上下文与他们的经验和记忆联系起来，产生有意义的相关回应，这种能力导致了人类与机器闲聊系统之间的差距。如前所述，早期的闲聊系统只是机器翻译系统的变体，可以视为序列到序列的语言模型。然而，对话生成比机器翻译要复杂得多，因为它具有更高的自由度和更模糊的约束。因此，闲聊系统不能简单地由序列到序列的映射组成，因为合适且信息丰富的回应总是与某些外部常识知识相关。相反，必须有一个模块来整合世界知识。
- en: 'Many researchers devoted their research efforts to building knowledge-grounded
    dialogue systems. A representative model is memory networks introduced in Section [2.4](#S2.SS4
    "2.4 Memory Networks ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in
    Deep Learning Based Dialogue Systems: A Systematic Survey"). Knowledge grounded
    systems use Memory Networks to store external knowledge and the generator retrieves
    relevant knowledge facts from it at the generation stage (Ghazvininejad et al.,
    [2018](#bib.bib104); Vougiouklis et al., [2016](#bib.bib381); Yin et al., [2015](#bib.bib435)). Tian
    et al. ([2019](#bib.bib369)) built a memory-augmented conversation model. The
    proposed model abstracted from the training samples and stored useful ones in
    the memory module. Zhao et al. ([2020b](#bib.bib459)) built a knowledge-grounded
    dialogue generation system based on GPT-2\. They combined a knowledge selection
    module into the language model and learned knowledge selection and response generation
    simultaneously. Lin et al. ([2020a](#bib.bib214)) proposed Knowledge-Interaction
    and knowledge Copy (KIC). They performed recurrent knowledge interactions during
    the decoding phase to compute an attention distribution over the memory. Then
    they performed knowledge copy using a knowledge-aware pointer network to copy
    knowledge words according to the attention distribution computed.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '许多研究人员致力于构建基于知识的对话系统。一种代表性的模型是内存网络，在[2.4](#S2.SS4 "2.4 Memory Networks ‣ 2
    Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")节中介绍。基于知识的系统使用内存网络来存储外部知识，生成器在生成阶段从中检索相关的知识事实（Ghazvininejad
    et al., [2018](#bib.bib104); Vougiouklis et al., [2016](#bib.bib381); Yin et al.,
    [2015](#bib.bib435)）。Tian et al. ([2019](#bib.bib369)) 构建了一个增强记忆的对话模型。该模型从训练样本中提取出有用的样本并将其存储在记忆模块中。Zhao
    et al. ([2020b](#bib.bib459)) 构建了一个基于 GPT-2 的知识基础对话生成系统。他们将知识选择模块与语言模型结合起来，同时学习知识选择和回应生成。Lin
    et al. ([2020a](#bib.bib214)) 提出了知识交互与知识复制（KIC）。他们在解码阶段执行递归知识交互，以计算记忆的注意力分布。然后，他们使用知识感知指针网络进行知识复制，根据计算出的注意力分布复制知识词汇。'
- en: Documents contain large amount of knowledge facts, but they have a drawback
    that they are usually too long to retrieve useful information from (Li et al.,
    [2019d](#bib.bib208)). Li et al. ([2019d](#bib.bib208)) built a multi-turn document-grounded
    system. They used an incremental transformer to encode multi-turns’ dialogue context
    and respective documents retrieved. In the generation phase, they designed a two-stage
    generation scheme. The first stage took dialogue context as input and generated
    coherent responses; the second stage utilized both the utterance from the first
    stage and the document retrieved for the current turn for response generation.
    In this case, selecting knowledge based on both dialogue context and generated
    response was called posterior knowledge selection, while selecting knowledge with
    only dialogue context was called prior knowledge selection, which only utilized
    prior information. Wang et al. ([2020c](#bib.bib386)) built a document quotation
    model in online conversations and investigated the consistency between quoted
    sentences and latent dialogue topics.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 文档包含大量的知识事实，但它们的一个缺点是通常过长，以至于很难从中提取有用的信息 (Li 等人，[2019d](#bib.bib208))。Li 等人
    ([2019d](#bib.bib208)) 构建了一个多轮文档基础系统。他们使用增量变换器来编码多轮对话上下文和检索到的相关文档。在生成阶段，他们设计了一个两阶段生成方案。第一阶段以对话上下文作为输入，生成连贯的响应；第二阶段利用第一阶段的发言和当前轮次的检索文档来生成响应。在这种情况下，基于对话上下文和生成的响应选择知识被称为后验知识选择，而仅基于对话上下文选择知识被称为先验知识选择，这只利用了先前的信息。Wang
    等人 ([2020c](#bib.bib386)) 构建了一个在线对话中的文档引用模型，并研究了引用句子与潜在对话主题之间的一致性。
- en: Knowledge graph is another source of external information, which is becoming
    more and more popular in knowledge-grounded systems because of their structured
    nature. Jung et al. ([2020](#bib.bib160)) proposed a dialogue-conditioned graph
    traversal model for knowledge-grounded dialogue systems. The proposed model leveraged
    attention flows of two directions and fully made use of the structured information
    of knowledge graph to flexibly decide the expanding range of nodes and edges.
    Likewise, Zhang et al. ([2019b](#bib.bib447)) applied graph attention to traverse
    the concept space, which was a common-sense knowledge graph. The graph attention
    helped to move to more meaningful nodes conditioning on dialogue context. Xu et al.
    ([2020a](#bib.bib426)) applied knowledge graphs as an external source to control
    a coarse-level utterance generation. Thus, the conversation was supported by common-sense
    knowledge, and the agent guided the dialogue topic in a more reasonable way. Moon
    et al. ([2019](#bib.bib257)) built a retrieval system retrieving responses based
    on the graph reasoning task. They used a graph walker to traverse the graph conditioning
    on symbolic transitions of the dialogue context. Huang et al. ([2020a](#bib.bib147))
    proposed Graph-enhanced Representations for Automatic Dialogue Evaluation (GRADE),
    a novel evaluation metric for open-domain dialogue systems. This metric considered
    both contextualized representations and topic-level graph representations. The
    main idea was to use an external knowledge graph to model the conversation logic
    flow as a part of the evaluation criteria.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱是另一种外部信息来源，由于其结构化的特性，在基于知识的系统中变得越来越受欢迎。Jung 等人 ([2020](#bib.bib160)) 提出了一个对话条件下的图遍历模型用于基于知识的对话系统。该模型利用了双向的注意力流，并充分利用了知识图谱的结构化信息，以灵活决定节点和边的扩展范围。同样，Zhang
    等人 ([2019b](#bib.bib447)) 将图注意力应用于遍历概念空间，这是一个常识知识图谱。图注意力有助于根据对话上下文移动到更有意义的节点。Xu
    等人 ([2020a](#bib.bib426)) 将知识图谱作为外部来源来控制粗略的发言生成。因此，对话得到了常识知识的支持，代理以更合理的方式引导对话主题。Moon
    等人 ([2019](#bib.bib257)) 构建了一个基于图推理任务检索响应的检索系统。他们使用图遍历器根据对话上下文的符号转移遍历图。Huang 等人
    ([2020a](#bib.bib147)) 提出了图增强表示用于自动对话评估（GRADE），这是一种用于开放领域对话系统的新型评估指标。该指标考虑了上下文化表示和主题级图表示。其主要思想是使用外部知识图谱来建模对话逻辑流，作为评估标准的一部分。
- en: Knowledge-grounded datasets containing context-knowledge-response triples are
    scarce and hard to obtain. Cho and May ([2020](#bib.bib50)) collected a large
    dataset consisting of more than 26000 turns of improvised dialogues which were
    further grounded with a larger movie corpus as external knowledge. Also tackling
    the data insufficiency problem, Li et al. ([2020b](#bib.bib197)) proposed a method
    that did not require context-knowledge-response triples for training and was thus
    data-efficient. They viewed knowledge as a latent variable to bridge the context
    and response. The variational approach learned the parameters of the generator
    from both a knowledge corpus and a dialogue corpus which were independent of each
    other.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 包含上下文-知识-响应三元组的知识基础数据集稀缺且难以获得。Cho 和 May ([2020](#bib.bib50)) 收集了一个包含26000多个即兴对话回合的大型数据集，并通过一个更大的电影语料库作为外部知识进一步进行了基础化。为了应对数据不足的问题，Li
    等 ([2020b](#bib.bib197)) 提出了一种不需要上下文-知识-响应三元组进行训练的方法，从而实现了数据高效性。他们将知识视为一个潜在变量，以桥接上下文和响应。变分方法从知识语料库和对话语料库中学习生成器的参数，这两个语料库是彼此独立的。
- en: 4.9 Interactive Training
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9 互动训练
- en: Interactive training, also called human-in-loop training, is a unique training
    method for dialogue systems. Annotated data is fixed and limited, not being able
    to cover all dialogue settings. Also, it takes a long time to train a good system.
    But in some industrial products, the dialogue systems need not be perfect when
    accomplishing their tasks. Thus, interactive training is desirable because the
    dialogue systems can improve themselves via interactions with users anywhere and
    anytime, which is a more flexible and cheap way to finetune the parameters.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 互动训练，也称为人机互动训练，是一种独特的对话系统训练方法。标注数据是固定且有限的，无法覆盖所有对话场景。此外，训练一个良好的系统需要很长时间。但在一些工业产品中，对话系统在完成任务时不一定需要完美。因此，互动训练是可取的，因为对话系统可以通过随时随地与用户的互动来提升自身，这是一个更灵活且便宜的调整参数的方法。
- en: Training schemes with the above intuition have been developed in recent years. Li
    et al. ([2016a](#bib.bib192)) introduced a reinforcement learning-based online
    learning framework. The agent interacted with a human dialogue partner and the
    partner provided feedback as a reward signal. Asghar et al. ([2016](#bib.bib8))
    first trained the agent with two-stage supervised learning, and then used an interaction-based
    reinforcement learning to finetune. Every time the user chose the best one from
    K responses generated by the pretrained model and then responded to this selected
    response. Instead of learning through being passively graded, Li et al. ([2016b](#bib.bib193))
    proposed a model that actively asked questions to seek improvement. Active learning
    was applicable to both offline and online learning settings. Hancock et al. ([2019](#bib.bib123))
    argued that most conversation samples an agent saw happened after it was pretrained
    and deployed. Thus, they proposed a framework to train the agent from the real
    conversations it participated in. The agent evaluated the satisfaction score of
    the user from the user’s response of each turn and explicitly requested the user
    feedback when it thought that a mistake has been made. The user feedback was further
    used for learning. Bouchacourt and Baroni ([2019](#bib.bib27)) placed the interactive
    learning in a cooperative game and tried to learn a long-term implicit strategy
    via Reinforce algorithm. Some of these work has been adopted by industry products
    and is a very promising direction for study.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年已经开发出具有上述直觉的训练方案。Li 等 ([2016a](#bib.bib192)) 介绍了一个基于强化学习的在线学习框架。代理与一个人类对话伙伴互动，伙伴提供反馈作为奖励信号。Asghar
    等 ([2016](#bib.bib8)) 首先通过两阶段监督学习训练代理，然后使用基于互动的强化学习进行微调。每次用户从预训练模型生成的 K 个响应中选择最佳一个，然后对该选中的响应进行回应。Li
    等 ([2016b](#bib.bib193)) 提出了一个主动提问寻求改进的模型，而不是通过被动评分来学习。主动学习适用于离线和在线学习环境。Hancock
    等 ([2019](#bib.bib123)) 认为，大多数代理看到的对话样本发生在其预训练和部署之后。因此，他们提出了一个从其参与的真实对话中训练代理的框架。代理从每回合用户的回应中评估用户的满意度，并在认为出现错误时明确请求用户反馈。这些用户反馈进一步用于学习。Bouchacourt
    和 Baroni ([2019](#bib.bib27)) 将互动学习放在合作游戏中，并通过 Reinforce 算法尝试学习长期隐式策略。这些工作中的一些已经被行业产品采用，是一个非常有前景的研究方向。
- en: 4.10 Visual Dialogue
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.10 视觉对话
- en: 'More and more researchers cast their eyes to a broader space and are not only
    restricted to NLP. The combination of CV and NLP giving rise to tasks like visual
    question answering attracted lots of interest. The VQA task is to answer a question
    based on the content of a picture or video. Recently, this has evolved into a
    more challenging task: visual dialogue, which conditions a dialogue on the visual
    information and dialogue history. The dialogue consists of a series of queries,
    and the query form is usually more informal, which is why it is more complicated
    than VQA.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的研究人员将目光投向更广泛的领域，不再仅仅局限于自然语言处理。计算机视觉和自然语言处理的结合催生了如视觉问答等任务，吸引了大量兴趣。视觉问答任务是根据图片或视频的内容回答问题。最近，这一任务演变为更具挑战性的任务：视觉对话，它将对话条件化于视觉信息和对话历史。对话由一系列查询组成，查询形式通常更为非正式，这使得它比视觉问答更复杂。
- en: '![Refer to caption](img/ddf47cfe252263bfc7c80cf645895937.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ddf47cfe252263bfc7c80cf645895937.png)'
- en: 'Figure 16: The architecture of VD-BERT, a state-of-the-art visual dialogue
    system (Wang et al., [2020f](#bib.bib392))'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：VD-BERT的架构，最先进的视觉对话系统（Wang et al.，[2020f](#bib.bib392)）
- en: '![Refer to caption](img/a9dbdb2a0130c37be56b263a00a91347.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a9dbdb2a0130c37be56b263a00a91347.png)'
- en: 'Figure 17: Three samples from the IMAGE-CHAT dataset (Shuster et al., [2020](#bib.bib325))'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：来自IMAGE-CHAT数据集的三个样本（Shuster et al.，[2020](#bib.bib325)）
- en: 'Visual dialogue can be seen as a multi-step reasoning process over a series
    of questions (Gan et al., [2019](#bib.bib95)). Gan et al. ([2019](#bib.bib95))
    learned semantic representation of the question based on dialogue history and
    a given image, and recurrently updated the representation. Shuster et al. ([2019](#bib.bib324))
    proposed a set of image-based tasks and provided strong baselines. Wang et al.
    ([2020f](#bib.bib392)) employed R-CNN as an image encoder and fused the visual
    and dialogue modality with a VD-BERT. The proposed architecture achieved sufficient
    interactions between multi-turn dialogue and images. The proposed architecture
    is shown as an example model for Visual Dialogue tasks in Figure [16](#S4.F16
    "Figure 16 ‣ 4.10 Visual Dialogue ‣ 4 Open-Domain Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey").'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉对话可以被视为一个多步骤的推理过程，涉及一系列问题（Gan et al.，[2019](#bib.bib95)）。Gan et al.（[2019](#bib.bib95)）基于对话历史和给定的图像学习了问题的语义表示，并反复更新该表示。Shuster
    et al.（[2019](#bib.bib324)）提出了一组基于图像的任务并提供了强有力的基线。Wang et al.（[2020f](#bib.bib392)）使用了R-CNN作为图像编码器，并通过VD-BERT融合了视觉和对话模态。所提出的架构实现了多轮对话与图像之间的充分交互。所提出的架构在图[16](#S4.F16
    "Figure 16 ‣ 4.10 Visual Dialogue ‣ 4 Open-Domain Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey")中展示了作为视觉对话任务的示例模型。'
- en: Compared with image-grounded dialogue systems, video-grounded systems are more
    interesting but also more challenging. There are two main challenges of video
    dialogue, as claimed by Le and Hoi ([2020](#bib.bib178)). One is that both spatial
    and temporal features exist in the video, which increases the difficulty of feature
    extraction. Another is that video dialogue features span across multiple conversation
    turns and thus are more complicated. A GPT-2 model was applied by Le and Hoi ([2020](#bib.bib178)),
    being able to fuse multi-modality information over different levels. Likewise, Le
    et al. ([2019](#bib.bib179)) built a multi-modal transformer network to incorporate
    information from different modalities and further applied a query-aware attention
    to extract context-related features from non-text modalities. Le et al. ([2020a](#bib.bib180))
    proposed a Bi-directional Spatio-Temporal Learning (BiST) leveraging temporal-to-spatial
    and spatial-to-temporal reasoning process and could adapt to the dynamically evolving
    semantics in the video.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于图像的对话系统相比，基于视频的对话系统更有趣但也更具挑战性。Le和Hoi（[2020](#bib.bib178)）声称视频对话面临两个主要挑战。其一是视频中存在空间和时间特征，这增加了特征提取的难度。其二是视频对话特征跨越多个对话轮次，因此更加复杂。Le和Hoi（[2020](#bib.bib178)）应用了GPT-2模型，能够融合不同层次的多模态信息。同样，Le
    et al.（[2019](#bib.bib179)）构建了一个多模态变换网络，以整合来自不同模态的信息，并进一步应用了查询感知注意力，以从非文本模态中提取上下文相关特征。Le
    et al.（[2020a](#bib.bib180)）提出了一种双向时空学习（BiST），利用时间到空间和空间到时间的推理过程，能够适应视频中动态演变的语义。
- en: Some researchers hold different opinions on the effectiveness of dialogue history
    in visual dialogue. Takmaz et al. ([2020](#bib.bib357)) proposed that many expressions
    were already mentioned in previous turns and they built a visual dialogue model
    grounded on both image and conversation history. They further proved that better
    performance was achieved when grounding the model on dialogue context. However, Agarwal
    et al. ([2020](#bib.bib1)) argued that though with dialogue history the visual
    dialogue model could achieve better results, in fact only a small proportion of
    cases benefited from the history. Furthermore, they proved that existing evaluation
    metrics for visual dialogue promoted generic responses.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究者对对话历史在视觉对话中的有效性持不同意见。Takmaz 等人 ([2020](#bib.bib357)) 提出了许多表达方式在之前的回合中已经被提及，并基于图像和对话历史构建了一个视觉对话模型。他们进一步证明了当模型基于对话上下文时，能取得更好的性能。然而，Agarwal
    等人 ([2020](#bib.bib1)) 认为尽管利用对话历史可以使视觉对话模型获得更好的结果，但实际上只有少部分案例从历史中受益。此外，他们还证明了现有的视觉对话评估指标促使了通用的回答。
- en: The visual dialogue task benefits a lot from the pretraining-based learning.
    The popularity of NLP pretraining sparked interest in multi-modal pretraining.
    VideoBERT (Sun et al., [2019b](#bib.bib351)) is widely recognized as the pioneering
    work in the field of multimodal pretraining. It’s a model that’s been pre-trained
    on video frame features and text. CBT (Sun et al., [2019a](#bib.bib350)), which
    is similarly pretrained on video-text pairs, is a contemporary work of VideoBERT.
    For video representation learning, Miech et al. ([2020](#bib.bib249)) used unlabeled
    narrated films. More researchers have focused their attention on visual-linguistic
    pretraining, inspired by the early work in multi-modal pretraining. For this objective,
    there are primarily two types of model designs. The single-stream model (Alberti
    et al., [2019](#bib.bib4); Chen et al., [2019d](#bib.bib47); Gan et al., [2020](#bib.bib96);
    Li et al., [2020a](#bib.bib190), [2019a](#bib.bib198), [c](#bib.bib204); Su et al.,
    [2019c](#bib.bib348); Zhou et al., [2020b](#bib.bib464)) is one example. (Li et al.,
    [2020a](#bib.bib190)) used a BERT model to process the concatenation of objects
    and words and pre-trained it with three standard tasks. Similar methods were proposed
    by Chen et al. ([2019d](#bib.bib47)) and Qi et al. ([2020](#bib.bib279)), but
    with more pretraining tasks and larger datasets. With an adversarial training
    technique, Gan et al. ([2020](#bib.bib96)) further enhanced the model. Su et al.
    ([2019c](#bib.bib348)) employed the same architecture, but incorporated single-modal
    data and pre-trained the object detector. Instead of using recognized objects,
    Huang et al. ([2020d](#bib.bib151)) sought to enter pixels directly. The object
    labels were used by Li et al. ([2020c](#bib.bib204)) to improve cross-modal alignment.
    Zhou et al. ([2020b](#bib.bib464)) suggested a single-stream model that learns
    both caption generation and VQA tasks at the same time. The two-stream model (Lu
    et al., [2019a](#bib.bib230), [2020](#bib.bib232); Tan and Bansal, [2019](#bib.bib359);
    Yu et al., [2020](#bib.bib440)) is another type of model architecture. Tan and
    Bansal ([2019](#bib.bib359)) suggested a two-stream model with co-attention and
    solely used in-domain data to train the model. Lu et al. ([2019a](#bib.bib230))
    introduced a similar architecture with a more complex co-attention model, which
    they pretrained with out-of-domain data, and Lu et al. ([2020](#bib.bib232)) improved
    VilBERT with multi-task learning. Yu et al. ([2020](#bib.bib440)) recently added
    the scene graph to the model, which improved performance. Aside from these studies,
    Singh et al. ([2020](#bib.bib329)) looked at the impact of pretraining dataset
    selection on downstream task performance.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉对话任务从基于预训练的学习中受益匪浅。NLP预训练的流行引发了对多模态预训练的兴趣。VideoBERT (Sun et al., [2019b](#bib.bib351))
    被广泛认可为多模态预训练领域的开创性工作。它是一个在视频帧特征和文本上进行预训练的模型。CBT (Sun et al., [2019a](#bib.bib350))，同样在视频-文本对上进行预训练，是VideoBERT的当代工作。对于视频表示学习，Miech
    et al. ([2020](#bib.bib249)) 使用了未标记的叙述影片。更多研究者将注意力集中在视觉-语言预训练上，受早期多模态预训练工作的启发。为了实现这一目标，主要有两种模型设计。一种是单流模型（Alberti
    et al., [2019](#bib.bib4); Chen et al., [2019d](#bib.bib47); Gan et al., [2020](#bib.bib96);
    Li et al., [2020a](#bib.bib190), [2019a](#bib.bib198), [c](#bib.bib204); Su et
    al., [2019c](#bib.bib348); Zhou et al., [2020b](#bib.bib464)）。例如，（Li et al., [2020a](#bib.bib190)）使用BERT模型处理对象和词的串联，并用三项标准任务对其进行预训练。Chen
    et al. ([2019d](#bib.bib47)) 和 Qi et al. ([2020](#bib.bib279)) 提出了类似的方法，但使用了更多的预训练任务和更大的数据集。通过对抗训练技术，Gan
    et al. ([2020](#bib.bib96)) 进一步增强了模型。Su et al. ([2019c](#bib.bib348)) 采用了相同的架构，但结合了单模态数据，并对目标检测器进行了预训练。Huang
    et al. ([2020d](#bib.bib151)) 试图直接进入像素，而不是使用已识别的对象。Li et al. ([2020c](#bib.bib204))
    使用对象标签来改进跨模态对齐。Zhou et al. ([2020b](#bib.bib464)) 提出了一个单流模型，该模型同时学习字幕生成和VQA任务。另一种模型架构是双流模型（Lu
    et al., [2019a](#bib.bib230), [2020](#bib.bib232); Tan and Bansal, [2019](#bib.bib359);
    Yu et al., [2020](#bib.bib440)）。Tan and Bansal ([2019](#bib.bib359)) 提出了一个具有协同注意力的双流模型，并仅使用领域内数据来训练模型。Lu
    et al. ([2019a](#bib.bib230)) 引入了一个具有更复杂协同注意力模型的类似架构，他们用领域外数据对其进行了预训练，而Lu et al.
    ([2020](#bib.bib232)) 则通过多任务学习改进了VilBERT。Yu et al. ([2020](#bib.bib440)) 最近在模型中添加了场景图，提升了性能。除了这些研究，Singh
    et al. ([2020](#bib.bib329)) 还考察了预训练数据集选择对下游任务性能的影响。
- en: 'The annotation of visual dialogue is laborious and thus the datasets are scarce.
    Recently, some researchers have tried to tackle the data insufficiency problem.
    Shuster et al. ([2020](#bib.bib325)) collected a dataset (IMAGE-CHAT, shown in
    Figure [17](#S4.F17 "Figure 17 ‣ 4.10 Visual Dialogue ‣ 4 Open-Domain Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey")) of image-grounded human-human conversations in which speakers are asked
    to perform role-playing based on an emotional mood or style offered, since the
    usage of such characteristics is also a significant factor in engagingness. Kamezawa
    et al. ([2020](#bib.bib163)) constructed a visual-grounded dialogue dataset. Interestingly,
    it additionally annotated the eye-gaze locations of the interlocutor in the image
    to provide information on what the interlocutor was paying attention to. Cogswell
    et al. ([2020](#bib.bib56)) proposed a method to utilize the VQA data when adapting
    to a new task, minimizing the requirement of dialogue data which is expensive
    to annotate.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '可视对话的标注工作繁重，因此相关数据集非常稀缺。最近，一些研究人员尝试解决数据不足的问题。Shuster 等人 ([2020](#bib.bib325))
    收集了一个数据集（IMAGE-CHAT，如图 [17](#S4.F17 "Figure 17 ‣ 4.10 Visual Dialogue ‣ 4 Open-Domain
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey") 所示），该数据集包含了基于图像的人际对话，其中发言者需要根据提供的情感氛围或风格进行角色扮演，因为这些特征的使用也是吸引力的重要因素。Kamezawa
    等人 ([2020](#bib.bib163)) 构建了一个视觉基础的对话数据集。有趣的是，它还额外标注了对话者在图像中的视线位置，以提供对话者关注的内容。Cogswell
    等人 ([2020](#bib.bib56)) 提出了在适应新任务时利用 VQA 数据的方法，减少了对昂贵的对话数据的需求。'
- en: 5 Evaluation Approaches
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估方法
- en: Evaluation is an essential part of research in dialogue systems. It is not only
    a way to assess the performance of agents, but it can also be a part of the learning
    framework which provides signals to facilitate the learning (Bao et al., [2019a](#bib.bib16)).
    This section discusses the evaluation methods in task-oriented and open-domain
    dialogue systems.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 评估是对话系统研究的一个重要部分。它不仅是评估代理性能的一种方式，还可以作为学习框架的一部分，提供信号以促进学习（Bao 等人，[2019a](#bib.bib16)）。本节讨论了任务导向和开放领域对话系统的评估方法。
- en: 5.1 Evaluation Methods for Task-oriented Dialogue Systems
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 任务导向对话系统的评估方法
- en: Task-oriented systems aim to accomplish tasks and thus have more direct metrics
    evaluating their performance such as task completion rate and task completion
    cost. Some evaluation methods also involve metrics like BLEU to compare system
    responses with human responses, which will be discussed later. In addition, human-based
    evaluation and user simulators are able to provide real conversation samples.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向系统旨在完成任务，因此有更直接的指标来评估其性能，例如任务完成率和任务完成成本。一些评估方法还涉及诸如 BLEU 的指标，以将系统响应与人类响应进行比较，这将在后续讨论。此外，人为评估和用户模拟器能够提供真实的对话样本。
- en: Task Completion Rate is the rate of successful events in all task completion
    attempts. It measures the task completion ability of a dialogue system. For example,
    in movie ticket booking tasks, the Task Completion Rate is the fraction of dialogues
    that meet all requirements specified by the user, such as movie time, cinema location,
    movie genre, etc. The task completion rate was applied in many task-oriented dialogue
    systems (Walker et al., [1997](#bib.bib382); Williams, [2007](#bib.bib404); Peng
    et al., [2017](#bib.bib272)). Additionally, some works (Singh et al., [2002](#bib.bib330);
    Yih et al., [2015](#bib.bib434)) used partial success rate.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 任务完成率是所有任务完成尝试中成功事件的比例。它衡量了对话系统的任务完成能力。例如，在电影票预订任务中，任务完成率是满足用户指定的所有要求的对话的比例，如电影时间、电影院位置、电影类型等。任务完成率已在许多任务导向对话系统中应用（Walker
    等人，[1997](#bib.bib382)；Williams，[2007](#bib.bib404)；Peng 等人，[2017](#bib.bib272)）。此外，一些研究（Singh
    等人，[2002](#bib.bib330)；Yih 等人，[2015](#bib.bib434)）使用了部分成功率。
- en: Task Completion Cost is the resources required when completing a task. Time
    efficiency is a significant metric belonging to Task Completion Cost. In dialogue-related
    tasks, the number of conversation turns is usually used to measure the time efficiency
    and dialogue with fewer turns is preferred when accomplishing the same task.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 任务完成成本是完成任务所需的资源。时间效率是属于任务完成成本的重要指标。在对话相关任务中，通常使用对话轮次来衡量时间效率，而在完成相同任务时，较少的对话轮次是更可取的。
- en: Human-based Evaluation provides user dialogues and user satisfaction scores
    for system evaluation. There are two main streams of human-based evaluation. One
    is to recruit human labor via crowdsourcing platforms to test-use a dialogue system.
    The crowdsource workers converse with the dialogue systems about predefined tasks
    and then metrics like Task Completion Rate and Task Completion Cost can be calculated.
    Another is computing the evaluation metrics in real user interactions, which means
    that evaluation is done after the system is deployed in real use.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 人工评估为系统评估提供了用户对话和用户满意度评分。人工评估主要有两种形式。一种是通过众包平台招募人工来测试对话系统。众包工人与对话系统就预定义的任务进行对话，然后可以计算诸如任务完成率和任务完成成本等指标。另一种是在真实用户交互中计算评估指标，即在系统部署到实际使用中后进行评估。
- en: User Simulator provides simulated user dialogues based on pre-defined rules
    or models. Since recruiting human labor is expensive and real user interactions
    are not available until a mature system is deployed, user simulators are able
    to provide task-oriented dialogues at a lower cost. There are two kinds of user
    simulators. One is agenda-based simulators (Schatzmann and Young, [2009](#bib.bib309);
    Li et al., [2016e](#bib.bib200); Ultes et al., [2017](#bib.bib375)), which only
    feed dialogue systems with the pre-defined user goal as a user message, without
    surface realization. Another is model-based simulators (Chandramohan et al., [2011](#bib.bib37);
    Asri et al., [2016](#bib.bib9)), which generate user utterances using language
    models given constraint information.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 用户模拟器基于预定义的规则或模型提供模拟用户对话。由于招募人工成本高且在系统成熟部署之前无法获得真实用户交互，用户模拟器能够以较低的成本提供任务导向的对话。用户模拟器有两种类型。一种是基于议程的模拟器（Schatzmann
    and Young, [2009](#bib.bib309); Li et al., [2016e](#bib.bib200); Ultes et al.,
    [2017](#bib.bib375)），它们仅通过预定义的用户目标作为用户消息来喂给对话系统，而不涉及表面实现。另一种是基于模型的模拟器（Chandramohan
    et al., [2011](#bib.bib37); Asri et al., [2016](#bib.bib9)），它们利用语言模型在给定约束信息的情况下生成用户发言。
- en: 5.2 Evaluation Methods for Open-domain Dialogue Systems
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 开放域对话系统的评估方法
- en: 'Evaluation of open-domain dialogue systems has long been a challenging problem.
    Unlike task-oriented systems, there is no clear metric like task completion rate
    or task completion cost. Both human and automatic evaluation methods are developed
    for ODD during these years. Human evaluation has been adopted by many works (Ritter
    et al., [2011](#bib.bib300); Shang et al., [2015](#bib.bib318); Sordoni et al.,
    [2015b](#bib.bib340)) to converse with and rate dialogue agents. However, human
    evaluation is not an ideal approach for that human labor is expensive and the
    evaluation results are highly subjective, varying from person to person. Researchers
    tend to hire crowd source workers (Ritter et al., [2011](#bib.bib300); Shang et al.,
    [2015](#bib.bib318); Sordoni et al., [2015b](#bib.bib340)) or random people (Moon
    et al., [2019](#bib.bib257); Jung et al., [2020](#bib.bib160)) to conduct human
    evaluation, both of which have two main drawbacks: 1\. The evaluator group is
    highly random, and there exists huge gap between people with different knowledge
    levels or from different domains. 2\. Though individual bias could be weakened
    by increasing the number of evaluators, the evaluator group cannot be very large
    because of the limited budgets (in articles mentioned above the sizes of human
    evaluator groups are usually 5-20). Thus, automatic and objective metrics are
    desirable. In general, there are two categories of automatic metrics in recent
    research: word-overlap metrics and neural metrics.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 开放域对话系统的评估一直是一个具有挑战性的问题。与任务导向系统不同，没有像任务完成率或任务完成成本这样的明确指标。在这些年里，为开放域对话系统（ODD）开发了人工和自动评估方法。许多研究（Ritter
    et al., [2011](#bib.bib300); Shang et al., [2015](#bib.bib318); Sordoni et al.,
    [2015b](#bib.bib340)）采用了人工评估与对话代理进行对话并评分。然而，人工评估并不是理想的方法，因为人工成本高且评估结果高度主观，因人而异。研究人员倾向于雇佣众包工人（Ritter
    et al., [2011](#bib.bib300); Shang et al., [2015](#bib.bib318); Sordoni et al.,
    [2015b](#bib.bib340)）或随机人员（Moon et al., [2019](#bib.bib257); Jung et al., [2020](#bib.bib160)）进行人工评估，这两种方法都有两个主要缺点：1.
    评估者组高度随机，不同知识水平或不同领域的人之间存在巨大差距。2. 尽管通过增加评估者人数可以减少个体偏见，但由于预算有限，评估者组不能太大（在上述文章中，人工评估者组的规模通常为5-20人）。因此，自动和客观的评估指标是理想的。通常，最近的研究中有两类自动评估指标：词汇重叠指标和神经网络指标。
- en: Word-overlap Metrics are widely used in Machine Translation and Summarization
    tasks, which calculate the similarity between the generated sequence and the ground
    truth sequence. Representative metrics like BLEU (Papineni et al., [2002](#bib.bib269))
    and ROUGE (Lin, [2004](#bib.bib210)) are n-gram matching metrics. METEOR (Banerjee
    and Lavie, [2005](#bib.bib15)) was further proposed with an improvement based
    on BLEU. It identified the paraphrases and synonyms between the generated sequence
    and the ground truth. Galley et al. ([2015](#bib.bib94)) extended the BLEU by
    exploiting numerical ratings of responses. Liu et al. ([2016](#bib.bib223)) argued
    that word-overlap metrics were not correlated well with human evaluation. These
    metrics are effective in Machine Translation because each source sentence has
    a ground truth to compare with, whereas in dialogues there may be many possible
    responses corresponding with one user message, and thus an acceptable response
    may receive a low score if simply computing word-overlap metrics.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 基于词重叠的度量在机器翻译和摘要任务中被广泛使用，它们计算生成序列与真实序列之间的相似性。代表性的度量如BLEU (Papineni等人, [2002](#bib.bib269))
    和ROUGE (Lin, [2004](#bib.bib210)) 是n-gram匹配度量。METEOR (Banerjee和Lavie, [2005](#bib.bib15))
    在BLEU的基础上进一步提出了改进。它识别生成序列与真实序列之间的同义词和释义。Galley等人 ([2015](#bib.bib94)) 通过利用响应的数值评分扩展了BLEU。Liu等人
    ([2016](#bib.bib223)) 认为词重叠度量与人工评价的相关性不高。这些度量在机器翻译中有效，因为每个源句子都有一个真实值可供比较，而在对话中，一个用户消息可能对应多个可能的响应，因此如果仅计算词重叠度量，接受的响应可能会得到低分。
- en: Neural Metrics are metrics computed by neural models. Neural methods improve
    the evaluation effectiveness in terms of adaptability compared with word-overlap
    metrics, but they require an additional training process. Su et al. ([2015](#bib.bib345))
    used an RNN and a CNN model to extract turn-level features in a sequence and give
    the score. Tao et al. ([2018](#bib.bib363)) proposed Ruber, which was an automatic
    metric combining referenced and unreferenced components. The referenced one computed
    the similarity between generated response representations and ground truth representations,
    while the unreferenced one learned a scoring model to rate the query-response
    pairs. Lowe et al. ([2017](#bib.bib229)) learned representations of dialogue utterances
    using an RNN and then computed the dot-product between generated response and
    ground truth response as an evaluation score. Kannan and Vinyals ([2017](#bib.bib164))
    and Bruni and Fernandez ([2017](#bib.bib29)) used the discriminator of a GAN framework
    to distinguish the generated responses from human responses. If a generated response
    achieved a high confidence score, this was indicative of a human-like response,
    thus desirable.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 神经度量是由神经模型计算的度量。与基于词重叠的度量相比，神经方法在适应性方面提高了评估效果，但需要额外的训练过程。Su等人 ([2015](#bib.bib345))
    使用了RNN和CNN模型来提取序列中的轮次级特征并给出评分。Tao等人 ([2018](#bib.bib363)) 提出了Ruber，这是一种结合了参考和非参考组件的自动度量。参考组件计算生成响应表示与真实值表示之间的相似性，而非参考组件则学习了一个评分模型来评估查询-响应对。Lowe等人
    ([2017](#bib.bib229)) 使用RNN学习对话话语的表示，然后计算生成响应与真实响应之间的点积作为评估得分。Kannan和Vinyals ([2017](#bib.bib164))
    和 Bruni与Fernandez ([2017](#bib.bib29)) 使用GAN框架的判别器来区分生成的响应和人类响应。如果生成的响应取得了高置信度得分，这表明它类似于人类响应，因此是期望的。
- en: 'Evaluation of open-domain dialogue systems is a hot topic at present and many
    researchers cast their eyes on this task recently. Some papers introduce two or
    more custom evaluation metrics for better evaluation, such as response diversity,
    response consistency, naturalness, knowledgeability, understandability, etc.,
    to study "what to evaluate". Bao et al. ([2019a](#bib.bib16)) evaluated the generated
    responses by designing two metrics. One was the informativeness metric calculating
    information utilization over turns. Another was the coherence metric, which was
    predicted by GRUs, given the response, context, and background as input. Likewise, Akama
    et al. ([2020](#bib.bib3)) designed scoring functions to compute connectivity
    of utterance pairs and content relatedness as two evaluation metrics and used
    another fusion function to combine the metrics. Pang et al. ([2020](#bib.bib268))
    combined four metrics in their automatic evaluation framework: the context coherence
    metric based on GPT-2; phrase fluency metric based on GPT-2; diversity metric
    based on n-grams; logical self-consistency metric based on textual-entailment-inference.
    Mehri and Eskenazi ([2020](#bib.bib243)) proposed a reference-free evaluation
    metric. They annotated responses considering the following qualities: Understandable
    (0-1), Maintains Context (1-3), Natural (1-3), Uses Knowledge (0-1), Interesting
    (1-3), Overall Quality (1-5). Furthermore, a transformer was trained on these
    annotated dialogues to compute the score of quality.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 开放领域对话系统的评估目前是一个热门话题，许多研究者最近将目光投向了这一任务。一些论文介绍了两种或更多自定义评估指标，以便更好地进行评估，例如响应多样性、响应一致性、自然性、知识性、可理解性等，以研究“什么需要评估”。Bao等人（[2019a](#bib.bib16)）通过设计两个指标来评估生成的响应。一个是信息量指标，用于计算回合中的信息利用情况。另一个是连贯性指标，通过GRUs预测，给定响应、上下文和背景作为输入。类似地，Akama等人（[2020](#bib.bib3)）设计了评分函数来计算发言对的连贯性和内容相关性作为两个评估指标，并使用另一个融合函数来结合这些指标。Pang等人（[2020](#bib.bib268)）在他们的自动评估框架中结合了四个指标：基于GPT-2的上下文连贯性指标；基于GPT-2的短语流畅性指标；基于n-grams的多样性指标；基于文本蕴含推理的逻辑自一致性指标。Mehri和Eskenazi（[2020](#bib.bib243)）提出了一种无参考评估指标。他们对响应进行标注，考虑以下质量：可理解性（0-1）、保持上下文（1-3）、自然性（1-3）、知识使用（0-1）、趣味性（1-3）、总体质量（1-5）。此外，还训练了一个变换器来计算这些标注对话的质量得分。
- en: Apart from "what to evaluate", there are also a multitude of papers studying
    "how to evaluate", which focus more on refining the evaluation process. Liang
    et al. ([2020](#bib.bib209)) proposed a three-stage framework to denoise the self-rating
    process. They first performed dialogue flow anomaly detection via self-supervised
    representation learning, and then the model was fine-tuned with smoothed self-reported
    user ratings. Finally, they performed a denoising procedure by calculating the
    Shapley value and removed the samples with negative values. Zhao et al. ([2020a](#bib.bib458))
    trained RoBERTa as a response scorer to achieve reference-free and semi-supervised
    evaluation. Sato et al. ([2020](#bib.bib308)) constructed a test set by first
    generating several responses based on one user message and then human evaluation
    was performed to annotate each response with a score, where the response with
    the highest score was taken as a true response and the remainder taken as false
    responses. Dialogue systems were further evaluated by comparing the response selection
    accuracy on the test set, where a cross-entropy loss was calculated between the
    generated response and candidate responses to perform the selection operation.
    Likewise, Sinha et al. ([2020](#bib.bib332)) trained a BERT-based model to discriminate
    between true and false responses, where false responses were automatically generated.
    The model was further used to predict the evaluation score of a response based
    on dialogue context. Huang et al. ([2020a](#bib.bib147)) argued that responses
    should not be simply evaluated based on their surface-level features, and instead
    the topic-level features were more essential. They incorporated a common-sense
    graph in their evaluation framework to obtain topic-level graph representations.
    The topic-level graph representation and utterance-level representation were jointly
    considered to evaluate the coherence of responses generated by open-domain dialogue
    systems.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 除了“评估什么”之外，还有大量研究“如何评估”的论文，这些论文更侧重于优化评估过程。Liang 等人 ([2020](#bib.bib209)) 提出了一个三阶段框架来去噪自我评分过程。他们首先通过自监督表示学习进行对话流异常检测，然后使用平滑的自报告用户评分对模型进行微调。最后，他们通过计算
    Shapley 值执行去噪程序，并移除具有负值的样本。Zhao 等人 ([2020a](#bib.bib458)) 训练了 RoBERTa 作为响应评分器，以实现无参考和半监督评估。Sato
    等人 ([2020](#bib.bib308)) 通过首先基于一个用户消息生成多个响应来构建测试集，然后进行人工评估，为每个响应标注分数，其中分数最高的响应被视为真实响应，其余的被视为虚假响应。对话系统进一步通过比较测试集上的响应选择准确性进行评估，其中计算了生成响应与候选响应之间的交叉熵损失以执行选择操作。同样，Sinha
    等人 ([2020](#bib.bib332)) 训练了一个基于 BERT 的模型来区分真实和虚假响应，其中虚假响应是自动生成的。该模型还被用来预测基于对话上下文的响应评估分数。Huang
    等人 ([2020a](#bib.bib147)) 认为响应不应仅仅基于表面特征进行评估，而是话题级特征更为重要。他们在评估框架中结合了常识图谱，以获得话题级图表示。话题级图表示和话语级表示被联合考虑，用于评估开放域对话系统生成响应的一致性。
- en: Ranking is also an approach that evaluates dialogue systems effectively. Gao
    et al. ([2020b](#bib.bib101)) leveraged large-scale human feedback data such as
    upvotes, downvotes, and replies to learn a GPT-2-based response ranker. Thus,
    responses were evaluated by their rankings given by the ranker. Deriu et al. ([2020](#bib.bib69))
    also evaluated the dialogue systems by ranking. They proposed a low-cost human-involved
    evaluation framework, in which different conversational agents conversed with
    each other and the human’s responsibility was to annotate whether the generated
    utterance was human-like or not. The systems were evaluated by comparing the number
    of turns their responses were judged as human-like responses.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 排序也是一种有效评估对话系统的方法。Gao 等人 ([2020b](#bib.bib101)) 利用大规模的人类反馈数据，如点赞、点踩和回复，来训练一个基于
    GPT-2 的响应排序器。因此，响应根据排序器给出的排名进行评估。Deriu 等人 ([2020](#bib.bib69)) 也通过排序来评估对话系统。他们提出了一个低成本的人类参与评估框架，其中不同的对话代理进行对话，人类的职责是标注生成的发言是否像人类一样。通过比较其响应被评判为人类响应的回合数来评估系统。
- en: 6 Datasets
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 数据集
- en: The dataset is one of the most essential components in dialogue systems study.
    Nowadays the datasets are not enough no matter for task-oriented or open-domain
    dialogue systems, especially for those tasks requiring additional annotations
    (Novikova et al., [2017](#bib.bib263)). For task-oriented dialogue systems, data
    can be collected via two main methods. One is to recruit human labor via crowdsourcing
    platforms to produce dialogues in a given task. Another is to collect dialogues
    in real task completions like film ticket booking. For open-domain dialogue systems,
    apart from dialogues collected in real interactions, social media is also a significant
    source of data. Some social media companies such as Twitter and Reddit provide
    API access to a small proportion of posts, but these services are restricted by
    many legal terms which affect the reproducibility of research. As a result, many
    recent works in dialogue systems collect their own datasets for train and test.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是对话系统研究中最重要的组成部分之一。如今，无论是任务导向还是开放领域对话系统的数据集都不够，特别是对于那些需要额外注释的任务（Novikova
    等， [2017](#bib.bib263)）。对于任务导向对话系统，数据可以通过两种主要方法收集。一种是通过众包平台招聘人工生成特定任务的对话。另一种是收集实际任务完成中的对话，如电影票预订。对于开放领域对话系统，除了在实际交互中收集的对话外，社交媒体也是一个重要的数据来源。一些社交媒体公司如
    Twitter 和 Reddit 提供对小部分帖子 API 访问，但这些服务受到许多法律条款的限制，影响了研究的可重复性。因此，许多最新的对话系统工作收集自己的数据集用于训练和测试。
- en: In this section, we review and categorize these datasets and make a comprehensive
    summary. To our best knowledge, Table LABEL:Datasets_for_Task-oriented_dialogue_systems
    and LABEL:Datasets_for_Open-domain_dialogue_systems cover almost all available
    datasets used in recent task-oriented or open-domain dialogue systems.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾并分类这些数据集，并做了全面的总结。根据我们的最佳知识，表 LABEL:Datasets_for_Task-oriented_dialogue_systems
    和 LABEL:Datasets_for_Open-domain_dialogue_systems 几乎涵盖了最近任务导向或开放领域对话系统中使用的所有可用数据集。
- en: 6.1 Datasets for Task-oriented Dialogue Systems
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 任务导向对话系统的数据集
- en: 'Table 3: Datasets for Task-oriented dialogue systems'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：任务导向对话系统的数据集
- en: '|  |  |  |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: '| Name | Description | Task | Origin |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Name | 描述 | 任务 | 来源 |'
- en: '| Schema | A dataset mainly for dialogue state tracking. | Dialogue State Tracking
    | Rastogi et al. ([2020](#bib.bib292)) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Schema | 主要用于对话状态跟踪的数据集。 | 对话状态跟踪 | Rastogi 等 ([2020](#bib.bib292)) |'
- en: '| MetaLWOZ | Collected by crowdsourcing platforms, spanning over 227 tasks
    and 47 domains. This dataset is designed for learning in unseen domains. | Domain
    Transfer | Lee et al. ([2019](#bib.bib187)) |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| MetaLWOZ | 通过众包平台收集，涵盖227个任务和47个领域。该数据集旨在用于未见领域的学习。 | 领域迁移 | Lee 等 ([2019](#bib.bib187))
    |'
- en: '| E2E | A dataset for end-to-end dialogue generation in restaurant domain.
    Data is collected in crowdsourced fashion. | End-to-end Task-oriented Dialogue
    Systems | Novikova et al. ([2017](#bib.bib263)) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| E2E | 餐厅领域的端到端对话生成数据集。数据通过众包方式收集。 | 端到端任务导向对话系统 | Novikova 等 ([2017](#bib.bib263))
    |'
- en: '| MSR-E2E | Contain dialogues spanning over 3 domains: movie-ticket booking,
    restaurant reservation, and taxi booking. | End-to-end Task-oriented Dialogue
    Systems | Li et al. ([2018](#bib.bib202)) |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| MSR-E2E | 包含涵盖3个领域的对话：电影票预订、餐厅预订和出租车预订。 | 端到端任务导向对话系统 | Li 等 ([2018](#bib.bib202))
    |'
- en: '| YELPNLG | A corpus consisting of utterances spanning over different restaurant
    attributes. | Natural Language Generation | Oraby et al. ([2019](#bib.bib264))
    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| YELPNLG | 一个由跨越不同餐厅属性的发言组成的语料库。 | 自然语言生成 | Oraby 等 ([2019](#bib.bib264))
    |'
- en: '| Clinical Conversation data set | It consists of conversations between physicians
    and participants. | Natural Language Understanding | Du et al. ([2019](#bib.bib75))
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| Clinical Conversation data set | 包含医生和参与者之间的对话。 | 自然语言理解 | Du 等 ([2019](#bib.bib75))
    |'
- en: '| OOS | A large-scale dataset for intent detection. | Natural Language Understanding
    | Larson et al. ([2019](#bib.bib177)) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| OOS | 一个用于意图检测的大规模数据集。 | 自然语言理解 | Larson 等 ([2019](#bib.bib177)) |'
- en: '| ATIS | A dataset consisting of voice calls from people who intend to make
    flight reservations. | Natural Language Understanding; Dialogue State Tracking
    | Tur et al. ([2010](#bib.bib373)) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| ATIS | 一个由打算进行航班预订的人的语音通话组成的数据集。 | 自然语言理解；对话状态跟踪 | Tur 等 ([2010](#bib.bib373))
    |'
- en: '| MultiWOZ | Human-human written conversations with rich annotations spanning
    over multi-domains. | Task-oriented Dialogue | Budzianowski et al. ([2018](#bib.bib30))
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| MultiWOZ | 涵盖多领域的富有注释的人类书面对话。 | 任务导向对话 | Budzianowski 等 ([2018](#bib.bib30))
    |'
- en: '| SNIPS-NLU | Task-oriented dialogue dataset colleted in a crowdsourced fashion.
    It was used to train voice assistant agents. | Task-oriented Dialogue | [https://github.com/snipsco/nlubenchmark](https://github.com/snipsco/nlubenchmark)
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| SNIPS-NLU | 以众包方式收集的任务导向对话数据集，用于训练语音助手。 | 任务导向对话 | [https://github.com/snipsco/nlubenchmark](https://github.com/snipsco/nlubenchmark)
    |'
- en: '| bAbI | Restaurant table reservation dialogues. | Task-oriented Dialogue |
    Bordes et al. ([2016](#bib.bib25)) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| bAbI | 餐厅桌位预订对话。 | 任务导向对话 | Bordes 等 ([2016](#bib.bib25)) |'
- en: '| JDC | A Chinese customer service dataset, consisting of context-response
    pairs. | Task-oriented Dialogue | [https://www.jddc.jd.com](https://www.jddc.jd.com)
    |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| JDC | 一个中文客服数据集，包括上下文-回应对。 | 任务导向对话 | [https://www.jddc.jd.com](https://www.jddc.jd.com)
    |'
- en: '| UbuntuV2 | It consists of dialogues collected via Ubuntu question-answering
    forum. | Task-oriented Dialogue | Lowe et al. ([2015](#bib.bib228)) |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| UbuntuV2 | 包含通过 Ubuntu 问答论坛收集的对话。 | 任务导向对话 | Lowe 等 ([2015](#bib.bib228))
    |'
- en: '| MICROSOFT DIALOGUE CHALLENGE data set | A task-oriented dataset collected
    via Amazon Mechanical Turk. | Task-oriented Dialogue | Li et al. ([2018](#bib.bib202))
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| MICROSOFT DIALOGUE CHALLENGE data set | 通过 Amazon Mechanical Turk 收集的任务导向数据集。
    | 任务导向对话 | Li 等 ([2018](#bib.bib202)) |'
- en: '| WOZ | Task-oriented data collected in crowdsourced fashion. | Task-oriented
    Dialogue | Wen et al. ([2016c](#bib.bib401)) |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| WOZ | 以众包方式收集的任务导向数据。 | 任务导向对话 | Wen 等 ([2016c](#bib.bib401)) |'
- en: '| DSTC series | Multi-domain task-oriented dataset. | Task-oriented Dialogue
    | [https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/](https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/)
    |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| DSTC series | 多领域任务导向数据集。 | 任务导向对话 | [https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/](https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/)
    |'
- en: '| SimDial | Simulated conversations spanning over multiple domains. | Task-oriented
    Dialogue | Zhao and Eskenazi ([2018](#bib.bib456)) |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| SimDial | 跨多个领域的模拟对话。 | 任务导向对话 | Zhao 和 Eskenazi ([2018](#bib.bib456)) |'
- en: '| SMD | Human-human dialogues in weather, navigation and scheduling domain.
    | Task-oriented Dialogue | Eric and Manning ([2017](#bib.bib81)) |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| SMD | 人与人之间的对话，涉及天气、导航和日程安排领域。 | 任务导向对话 | Eric 和 Manning ([2017](#bib.bib81))
    |'
- en: '| BANKING | Question-answer pairs with 77 categories in e-banking domain. |
    Task-oriented Dialogue | Henderson et al. ([2019b](#bib.bib136)) |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| BANKING | 包含 77 个电子银行领域的问答对。 | 任务导向对话 | Henderson 等 ([2019b](#bib.bib136))
    |'
- en: '| Weather forecast | A task-oriented dataset in the weather domain. | Task-oriented
    Dialogue | Balakrishnan et al. ([2019](#bib.bib14)) |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| Weather forecast | 天气领域的任务导向数据集。 | 任务导向对话 | Balakrishnan 等 ([2019](#bib.bib14))
    |'
- en: '| MedDialog-(EN,CN) | Large scale dataset in medical domain consisting of conversations
    between doctors and patients | Task-oriented Dialogue | He et al. ([2020b](#bib.bib129))
    |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| MedDialog-(EN,CN) | 医疗领域的大规模数据集，包括医生和患者之间的对话。 | 任务导向对话 | He 等 ([2020b](#bib.bib129))
    |'
- en: '| CamRest | It consists of human-human multi-turn dialogues in restaurant domain.
    | Task-oriented Dialogue | Wen et al. ([2016a](#bib.bib399)) |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| CamRest | 包含餐厅领域的人与人之间的多轮对话。 | 任务导向对话 | Wen 等 ([2016a](#bib.bib399)) |'
- en: '| Taskmaster | Contain dialogues spanning over 6 domains. It has 22.9 average
    length of conversational turns. | Task-oriented Dialogue | Byrne et al. ([2019](#bib.bib32))
    |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| Taskmaster | 包含跨越 6 个领域的对话，平均对话轮次为 22.9。 | 任务导向对话 | Byrne 等 ([2019](#bib.bib32))
    |'
- en: '| Frames | Conversational dataset with annotations of semantic frame tracking.
    | Task-oriented Dialogue | Asri et al. ([2017](#bib.bib10)) |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| Frames | 带有语义框架跟踪标注的对话数据集。 | 任务导向对话 | Asri 等 ([2017](#bib.bib10)) |'
- en: '| JDDC | A Chinese customer service dataset, consisting of context-response
    pairs. | Task-oriented Dialogue | Chen et al. ([2019a](#bib.bib43)) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| JDDC | 一个中文客服数据集，包括上下文-回应对。 | 任务导向对话 | Chen 等 ([2019a](#bib.bib43)) |'
- en: '| Court Debate Dataset | A task-oriented dataset in judicial field containing
    court debate conversations. | Task-oriented Dialogue | Ji et al. ([2020](#bib.bib155))
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| Court Debate Dataset | 法律领域的任务导向数据集，包含法庭辩论对话。 | 任务导向对话 | Ji 等 ([2020](#bib.bib155))
    |'
- en: '| TreeDST | A task-oriented dataset annotated with tree structured dialogue
    states and agent acts. | Task-oriented Dialogue | Cheng et al. ([2020](#bib.bib49))
    |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| TreeDST | 一个任务导向的数据集，带有树状结构的对话状态和代理行为标注。 | 任务导向对话 | Cheng 等 ([2020](#bib.bib49))
    |'
- en: '| RiSAWOZ | Contain utterances for 12 domains, annotated with rich semantic
    information. | Task-oriented Dialogue | Quan et al. ([2020](#bib.bib286)) |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| RiSAWOZ | 包含 12 个领域的发言，并带有丰富的语义信息。 | 任务导向对话 | Quan 等 ([2020](#bib.bib286))
    |'
- en: '| Cambridge Restaurant | A task-oriented dataset in restaurant booking field.
    | Task-oriented Dialogue | Wen et al. ([2016c](#bib.bib401)) |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| Cambridge Restaurant | 一个任务导向的数据集，涉及餐厅预订领域。 | 任务导向对话 | Wen 等 ([2016c](#bib.bib401))
    |'
- en: '| SB-TOP | A task-oriented dataset with semantic parsing annotation. It spans
    over 4 domains: Reminder, Weather, Calling and Music. | Task-oriented Dialogue
    | Aghajanyan et al. ([2020](#bib.bib2)) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| SB-TOP | 一个任务导向的数据集，带有语义解析注释。涵盖 4 个领域：提醒、天气、呼叫和音乐。 | 任务导向对话 | Aghajanyan
    等 ([2020](#bib.bib2)) |'
- en: '| GSIM | A machine-machine task-oriented dataset. It covers two domains: restaurant
    table booking and movie ticket booking. | Task-oriented Dialogue | Shah et al.
    ([2018](#bib.bib316)) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| GSIM | 一个机器间任务导向的数据集。涵盖两个领域：餐厅桌位预订和电影票预订。 | 任务导向对话 | Shah 等 ([2018](#bib.bib316))
    |'
- en: '| SGD | A schema-guided dataset spanning over multiple domains. | Task-oriented
    Dialogue | Rastogi et al. ([2020](#bib.bib292)) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| SGD | 一个覆盖多个领域的模式引导数据集。 | 任务导向对话 | Rastogi 等 ([2020](#bib.bib292)) |'
- en: '| cite-8K | A task-oriented dataset collected in restaurant booking calls.
    | Task-oriented Dialogue | Coope et al. ([2020](#bib.bib58)) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| cite-8K | 一个任务导向的数据集，收集于餐厅预订电话中。 | 任务导向对话 | Coope 等 ([2020](#bib.bib58))
    |'
- en: 6.2 Datasets for Open-domain Dialogue Systems
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 开放域对话系统的数据集
- en: 'Table 4: Datasets for Open-domain dialogue systems'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 开放域对话系统的数据集'
- en: '|  |  |  |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |'
- en: '| Name | Description | Task | Origin |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 描述 | 任务 | 来源 |'
- en: '| Large-Scale Corpus for Conversation Disentanglement | A dataset consisting
    of messages annotated with reply-structure graphs for dialogue disentanglement.
    | Conversation Disentaglement | Kummerfeld et al. ([2018](#bib.bib174)) |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 大规模对话解缠数据集 | 一个包含带有回复结构图的消息的数据集，用于对话解缠。 | 对话解缠 | Kummerfeld 等 ([2018](#bib.bib174))
    |'
- en: '| DuConv | Collected in conversations between a conversation leader and a conversation
    follower. | Conversation Topic | Wu et al. ([2019c](#bib.bib417)) |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| DuConv | 收集于对话引导者和对话跟随者之间的对话。 | 对话主题 | Wu 等 ([2019c](#bib.bib417)) |'
- en: '| PERSUASION FOR GOOD | A topic-oriented dataset annotated with persuasion
    strategies. | Conversation Topic | Wang et al. ([2019b](#bib.bib390)) |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| PERSUASION FOR GOOD | 一个以话题为导向的数据集，带有说服策略的注释。 | 对话主题 | Wang 等 ([2019b](#bib.bib390))
    |'
- en: '| MutualFriends | A topic-oriented dataset based on bot-bot stratigical conversations.
    | Conversation Topic | He et al. ([2017](#bib.bib125)) |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| MutualFriends | 一个以话题为导向的数据集，基于机器人间的策略对话。 | 对话主题 | He 等 ([2017](#bib.bib125))
    |'
- en: '| SAMSum | A large-scale dialogue summary dataset. | Conversation Topic | Gliwa
    et al. ([2019](#bib.bib105)) |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| SAMSum | 一个大规模的对话摘要数据集。 | 对话主题 | Gliwa 等 ([2019](#bib.bib105)) |'
- en: '| OpenDialKG | It consists conversations between two agents and each dialogue
    corresponds with a knowledge graph path annotation. | Conversation Topic; Dialogue
    Reasoning | Moon et al. ([2019](#bib.bib257)) |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| OpenDialKG | 它由两个代理之间的对话组成，每个对话对应一个知识图谱路径注释。 | 对话主题；对话推理 | Moon 等 ([2019](#bib.bib257))
    |'
- en: '| doc2dial | A dataset consisting of conversations annotated with goals and
    accociated documents. | Conversation Topic; Knowledge-Grounded System | Feng et al.
    ([2020c](#bib.bib89)) |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| doc2dial | 一个数据集，包含带有目标和相关文档的对话。 | 对话主题；知识基础系统 | Feng 等 ([2020c](#bib.bib89))
    |'
- en: '| DialEdit | A dataset constructed for image editing via conversational language
    instructions. | Conversational Image Editing | Manuvina-kurike et al. ([2018](#bib.bib241))
    |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| DialEdit | 一个为通过对话语言指令进行图像编辑而构建的数据集。 | 对话图像编辑 | Manuvina-kurike 等 ([2018](#bib.bib241))
    |'
- en: '| CHART DIALOGS | A dataset containing dialogues describing matplotlib plot
    features. | Conversational Plotting | Shao and Nakashole ([2020](#bib.bib320))
    |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| CHART DIALOGS | 一个包含描述 matplotlib 绘图特征的对话的数据集。 | 对话绘图 | Shao 和 Nakashole
    ([2020](#bib.bib320)) |'
- en: '| CONAN | A multilingual dataset for hate speech tackling. | Dialogue Classification
    | Chung et al. ([2019](#bib.bib55)) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| CONAN | 一个多语言的数据集，用于处理仇恨言论。 | 对话分类 | Chung 等 ([2019](#bib.bib55)) |'
- en: '| Dialogue NLI | A NLI dataset with sentences annotated with entailment (E),
    neutral (N), or contradiction (C). | Dialogue Inference | Welleck et al. ([2018](#bib.bib396))
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Dialogue NLI | 一个 NLI 数据集，句子带有蕴涵（E）、中立（N）或矛盾（C）的注释。 | 对话推理 | Welleck 等 ([2018](#bib.bib396))
    |'
- en: '| MuTual | A dialogue reasoning dataset containing English listening comprehension
    exams. | Dialogue Reasoning | Cui et al. ([2020](#bib.bib60)) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| MuTual | 一个对话推理数据集，包含英语听力理解考试。 | 对话推理 | Cui 等人 ([2020](#bib.bib60)) |'
- en: '| RST-DT | It consists of samples from 385 news articles annotated with dialogue
    features. | Discourse Parsing | Carlson et al. ([2002](#bib.bib36)) |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| RST-DT | 包含来自385篇新闻文章的样本，这些样本标注了对话特征。 | 话语解析 | Carlson 等人 ([2002](#bib.bib36))
    |'
- en: '| NLPCC | A dataset consisting of emotional classification data. | Empathetic
    Response | [http://tcci.ccf.org.cn/nlpcc.php](http://tcci.ccf.org.cn/nlpcc.php)
    |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| NLPCC | 一个包含情感分类数据的数据集。 | 具同理心的回应 | [http://tcci.ccf.org.cn/nlpcc.php](http://tcci.ccf.org.cn/nlpcc.php)
    |'
- en: '| MELD | A multi-party conversational dataset with emotion annotations. | Empathetic
    Response | Poria et al. ([2019](#bib.bib275)) |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| MELD | 一个包含情感注释的多方对话数据集。 | 具同理心的回应 | Poria 等人 ([2019](#bib.bib275)) |'
- en: '| EMPATHETIC DIALOGUES | A dataset containing conversations annotated with
    emotion labels. | Empathetic Response | Rashkin et al. ([2018](#bib.bib291)) |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| EMPATHETIC DIALOGUES | 一个包含标注了情感标签的对话的数据集。 | 具同理心的回应 | Rashkin 等人 ([2018](#bib.bib291))
    |'
- en: '| IEMOCAP | Contain multi-party dialogues. Each dialogue is annotated with
    an emotion label. | Empathetic Response | Busso et al. ([2008](#bib.bib31)) |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| IEMOCAP | 包含多方对话。每个对话标注了情感标签。 | 具同理心的回应 | Busso 等人 ([2008](#bib.bib31)) |'
- en: '| EmoryNLP | Collected from Friends’ TV series, annotated with emotion labels.
    | Empathetic Response | Zahiri and Choi ([2017](#bib.bib443)) |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| EmoryNLP | 从《老友记》电视剧中收集，标注了情感标签。 | 具同理心的回应 | Zahiri 和 Choi ([2017](#bib.bib443))
    |'
- en: '| MojiTalk | A largescale dataset collected from Twitter, including emojis.
    | Empathetic Response | Zhou and Wang ([2017](#bib.bib465)) |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| MojiTalk | 一个大规模的数据集，来自Twitter，包含表情符号。 | 具同理心的回应 | Zhou 和 Wang ([2017](#bib.bib465))
    |'
- en: '| CBET | A dialogue dataset annotated with nine emotion labels: surprise, anger,
    love, sadness, joy, fear, guilt, disgust and thankfulness | Empathetic Response
    | Yadollahi et al. ([2017](#bib.bib428)) |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| CBET | 一个对话数据集，标注了九种情感标签：惊讶、愤怒、爱、悲伤、喜悦、恐惧、内疚、厌恶和感激。 | 具同理心的回应 | Yadollahi
    等人 ([2017](#bib.bib428)) |'
- en: '| Stanford Politeness Corpus | A conversational dataset annotated with politeness
    labels. | Empathetic Response | Danescu-Niculescu-Mizil et al. ([2013](#bib.bib65))
    |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Politeness Corpus | 一个标注了礼貌标签的对话数据集。 | 具同理心的回应 | Danescu-Niculescu-Mizil
    等人 ([2013](#bib.bib65)) |'
- en: '| AIT-2018 | Collected in SemEval-2018 Task 1: Affect in Tweets. | Empathetic
    Response | Mohammad et al. ([2018](#bib.bib256)) |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| AIT-2018 | 收集自SemEval-2018任务1：推文中的情感。 | 具同理心的回应 | Mohammad 等人 ([2018](#bib.bib256))
    |'
- en: '| EMOTyDA | A dataset containing short videos about multi-party conversations,
    each annotated with respective emotion. | Empathetic Response; Visual Dialogue
    | Saha et al. ([2020](#bib.bib303)) |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| EMOTyDA | 一个包含多方对话的短视频数据集，每个视频标注了相应的情感。 | 具同理心的回应；视觉对话 | Saha 等人 ([2020](#bib.bib303))
    |'
- en: '| Wizard of Wikipedia | A large-sclale dataset consisting of conversations
    grounded with Wikipedia knowledge. | Knowledge-Grounded System | Dinan et al.
    ([2018](#bib.bib72)) |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Wizard of Wikipedia | 一个大规模的数据集，包含以维基百科知识为基础的对话。 | 知识驱动系统 | Dinan 等人 ([2018](#bib.bib72))
    |'
- en: '| CMU DoG | A dataset consisting of conversations grounded with Wikipedia articles
    about popular movies. | Knowledge-Grounded System | Zhou et al. ([2018](#bib.bib463))
    |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| CMU DoG | 一个包含以维基百科文章为基础的流行电影对话的数据集。 | 知识驱动系统 | Zhou 等人 ([2018](#bib.bib463))
    |'
- en: '| Holl-E | Contain dialogues grounded with documents. | Knowledge-Grounded
    System | Moghe et al. ([2018](#bib.bib255)) |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Holl-E | 包含基于文档的对话。 | 知识驱动系统 | Moghe 等人 ([2018](#bib.bib255)) |'
- en: '| Interview | A dataset containing multi-party conversations in the form of
    interviews. | Knowledge-Grounded System | Majumder et al. ([2020b](#bib.bib239))
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| Interview | 一个包含以采访形式进行的多方对话的数据集。 | 知识驱动系统 | Majumder 等人 ([2020b](#bib.bib239))
    |'
- en: '| Curiosity | An open-domain dataset annotated with pre-existing user knowledge
    and dialogue acts, also grounding in Wikipedia. | Knowledge-Grounded System |
    Rodriguez et al. ([2020](#bib.bib301)) |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| Curiosity | 一个开放域数据集，标注了现有用户知识和对话行为，同时以维基百科为基础。 | 知识驱动系统 | Rodriguez 等人 ([2020](#bib.bib301))
    |'
- en: '| KdConv | A chinese knowledge-grounded dialogue dataset. | Knowledge-Grounded
    System | Zhou et al. ([2020a](#bib.bib462)) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| KdConv | 一个中文知识驱动对话数据集。 | 知识驱动系统 | Zhou 等人 ([2020a](#bib.bib462)) |'
- en: '| ELI5 | A QA dataset grounded with retrieved documents. | Knowledge-Grounded
    System | Fan et al. ([2019](#bib.bib83)) |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| ELI5 | 一个基于检索文档的问答数据集。 | 知识驱动系统 | Fan 等人 ([2019](#bib.bib83)) |'
- en: '| Topical Chat | A knowledge-grounded dataset where the knowledge spans over
    eight different topics. | Knowledge-Grounded System; Conversation Topic | Gopalakrishnan
    et al. ([2019](#bib.bib109)) |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| Topical Chat | 一个知识驱动的数据集，涵盖八个不同的话题。 | 知识驱动系统；对话话题 | Gopalakrishnan 等 ([2019](#bib.bib109))
    |'
- en: '| WHERE ARE YOU? | A dialogue dataset annotated with localization information.
    | Localization Dialogue | Hahn et al. ([2020](#bib.bib119)) |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| WHERE ARE YOU? | 带有定位信息注释的对话数据集。 | 定位对话 | Hahn 等 ([2020](#bib.bib119)) |'
- en: '| MMD | A multi-modal dataset consisting of dialogues between sales agents
    and shoppers. | Multi-modal Dialogue | Saha et al. ([2018](#bib.bib302)) |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| MMD | 一个包含销售代理和购物者之间对话的多模态数据集。 | 多模态对话 | Saha 等 ([2018](#bib.bib302)) |'
- en: '| OpenSubtitles | A multilingual dataset made up of movie captions, containing
    about 8 billion words. | Open-domain Dialogue | Tiedemann ([2012](#bib.bib370))
    |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| OpenSubtitles | 一个由电影字幕组成的多语言数据集，包含约80亿个词汇。 | 开放领域对话 | Tiedemann ([2012](#bib.bib370))
    |'
- en: '| NTCIR | A social media dataset collected from Sina Weibo. | Open-domain Dialogue
    | [http://research.nii.ac.jp/ntcir/data/data-en.html](http://research.nii.ac.jp/ntcir/data/data-en.html)
    |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| NTCIR | 从新浪微博收集的社交媒体数据集。 | 开放领域对话 | [http://research.nii.ac.jp/ntcir/data/data-en.html](http://research.nii.ac.jp/ntcir/data/data-en.html)
    |'
- en: '| Twitter | A social media dataset collected from Twitter. | Open-domain Dialogue
    | [https://github.com/Marsan-Ma-zz/chatcorpus](https://github.com/Marsan-Ma-zz/chatcorpus)
    |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| Twitter | 从Twitter收集的社交媒体数据集。 | 开放领域对话 | [https://github.com/Marsan-Ma-zz/chatcorpus](https://github.com/Marsan-Ma-zz/chatcorpus)
    |'
- en: '| Douban Conversation Corpus | A social media dataset collected from Douban.
    | Open-domain Dialogue | Zhang et al. ([2018d](#bib.bib453)) |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| 豆瓣对话语料库 | 从豆瓣收集的社交媒体数据集。 | 开放领域对话 | 张等 ([2018d](#bib.bib453)) |'
- en: '| E-commerce Dialogue Corpus | It consists of conversations between customers
    and customer service staff on Taobao. | Open-domain Dialogue | Zhang et al. ([2018d](#bib.bib453))
    |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 电子商务对话语料库 | 包含淘宝上客户与客服之间的对话。 | 开放领域对话 | 张等 ([2018d](#bib.bib453)) |'
- en: '| REDDIT | A social media dataset collected from REDDIT. | Open-domain Dialogue
    | Henderson et al. ([2019a](#bib.bib135)) |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| REDDIT | 从REDDIT收集的社交媒体数据集。 | 开放领域对话 | Henderson 等 ([2019a](#bib.bib135))
    |'
- en: '| STC-SeFun | A social media dataset collected from Tieba, Zhidao, Douban and
    Weibo. | Open-domain Dialogue | Bi et al. ([2019](#bib.bib22)) |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| STC-SeFun | 从贴吧、知道、豆瓣和微博收集的社交媒体数据集。 | 开放领域对话 | Bi 等 ([2019](#bib.bib22))
    |'
- en: '| DailyDialog | A dataset consisting of daily dialogues, annotated with conversation
    intention and emotion information. | Open-domain Dialogue | Li et al. ([2017c](#bib.bib206))
    |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| DailyDialog | 包含日常对话的数据集，注释有对话意图和情感信息。 | 开放领域对话 | Li 等 ([2017c](#bib.bib206))
    |'
- en: '| PDTB | Dialogue dataset annotated with discourse relations. | Open-domain
    Dialogue | Prasad et al. ([2008](#bib.bib277)) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| PDTB | 带有话语关系注释的对话数据集。 | 开放领域对话 | Prasad 等 ([2008](#bib.bib277)) |'
- en: '| Luna | Dialogue dataset with Italian relation annotations. | Open-domain
    Dialogue | Tonelli et al. ([2010](#bib.bib371)) |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| Luna | 带有意大利语关系注释的对话数据集。 | 开放领域对话 | Tonelli 等 ([2010](#bib.bib371)) |'
- en: '| Edina-DR | Dialogue dataset with English relation annotations, which is based
    on Luna data set. | Open-domain Dialogue | Ma et al. ([2019](#bib.bib234)) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| Edina-DR | 带有英语关系注释的对话数据集，基于Luna数据集。 | 开放领域对话 | Ma 等 ([2019](#bib.bib234))
    |'
- en: '| Cornell Movie Dialog Corpus | A dialogue dataset collected via IMDB database.
    | Open-domain Dialogue | Danescu-Niculescu-Mizil and Lee ([2011](#bib.bib64))
    |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 康奈尔电影对话语料库 | 通过IMDB数据库收集的对话数据集。 | 开放领域对话 | Danescu-Niculescu-Mizil 和 Lee
    ([2011](#bib.bib64)) |'
- en: '| Reddit Movie Dialogue Dataset | A movie dialogue dataset collected from Reddit.
    | Open-domain Dialogue | Liu et al. ([2020a](#bib.bib224)) |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| Reddit电影对话数据集 | 从Reddit收集的电影对话数据集。 | 开放领域对话 | Liu 等 ([2020a](#bib.bib224))
    |'
- en: '| LIGHT | A dialogue dataset with configurable text adventure environment.
    | Open-domain Dialogue | Urbanek et al. ([2019](#bib.bib376)) |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| LIGHT | 一个具有可配置文本冒险环境的对话数据集。 | 开放领域对话 | Urbanek 等 ([2019](#bib.bib376)) |'
- en: '| This American Life | A media dialogue dataset collected in long-form expository
    podcast episodes. | Open-domain Dialogue | Mao et al. ([2020](#bib.bib242)) |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| This American Life | 在长篇说明性播客集里收集的媒体对话数据集。 | 开放领域对话 | Mao 等 ([2020](#bib.bib242))
    |'
- en: '| RadioTalk | A media dialogue dataset collected from radio transcripts. |
    Open-domain Dialogue | Beeferman et al. ([2019](#bib.bib19)) |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| RadioTalk | 从广播稿本中收集的媒体对话数据集。 | 开放领域对话 | Beeferman 等 ([2019](#bib.bib19))
    |'
- en: '| French EPAC | A media dialogue dataset collected from news. | Open-domain
    Dialogue | Esteve et al. ([2010](#bib.bib82)) |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| French EPAC | 从新闻中收集的媒体对话数据集。 | 开放领域对话 | Esteve 等人 ([2010](#bib.bib82)) |'
- en: '| TREC Conversational Assistance | An open-domain dataset spanning 30 conversation
    topics. | Open-domain Dialogue | Dalton et al. ([2020](#bib.bib63)) |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| TREC Conversational Assistance | 一个涵盖 30 个对话主题的开放领域数据集。 | 开放领域对话 | Dalton
    等人 ([2020](#bib.bib63)) |'
- en: '| Search as a Conversation | A dataset for conversations with search engines.
    | Open-domain Dialogue | Ren et al. ([2020](#bib.bib299)) |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| Search as a Conversation | 一个用于与搜索引擎对话的数据集。 | 开放领域对话 | Ren 等人 ([2020](#bib.bib299))
    |'
- en: '| Amazon Alexa Prize Competition | A dataset containing real-world conversations
    between Amazon Alexa customers and Gunrock, which is a champion chatbot. | Open-domain
    Dialogue | Ram et al. ([2018](#bib.bib288)) |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| Amazon Alexa Prize Competition | 一个包含 Amazon Alexa 客户和冠军聊天机器人 Gunrock 之间真实对话的数据集。
    | 开放领域对话 | Ram 等人 ([2018](#bib.bib288)) |'
- en: '| SwitchBoard | An open-domain dataset containing English phone conversations.
    | Open-domain Dialogue | Jurafsky ([1997](#bib.bib161)) |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| SwitchBoard | 一个包含英语电话对话的开放领域数据集。 | 开放领域对话 | Jurafsky ([1997](#bib.bib161))
    |'
- en: '| Zhihu | A Chinese social media dataset with posts and comments. | Open-domain
    Dialogue | [https://www.zhihu.com](https://www.zhihu.com) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
  zh: '| Zhihu | 一个包含帖子和评论的中文社交媒体数据集。 | 开放领域对话 | [https://www.zhihu.com](https://www.zhihu.com)
    |'
- en: '| SPOLIN | A dataset containing yes-and conversations. | Open-domain Dialogue
    | Cho and May ([2020](#bib.bib50)) |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
  zh: '| SPOLIN | 一个包含“是”和“否”对话的数据集。 | 开放领域对话 | Cho 和 May ([2020](#bib.bib50)) |'
- en: '| CRD3 | A dataset collected in the role-playing game Dungeons and Dragons.
    | Open-domain Dialogue | Rameshkumar and Bailey ([2020](#bib.bib289)) |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
  zh: '| CRD3 | 一个从角色扮演游戏《地下城与龙》中收集的数据集。 | 开放领域对话 | Rameshkumar 和 Bailey ([2020](#bib.bib289))
    |'
- en: '| Baidu Zhidao | A Chinese social media dataset with posts and comments. |
    Open-domain Dialogue | [https://zhidao.baidu.com/](https://zhidao.baidu.com/)
    |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
  zh: '| Baidu Zhidao | 一个包含帖子和评论的中文社交媒体数据集。 | 开放领域对话 | [https://zhidao.baidu.com/](https://zhidao.baidu.com/)
    |'
- en: '| Webis Gmane Email Corpus 2019 | A conversational dataset collected from 153M
    emails. | Open-domain Dialogue | Bevendorff et al. ([2020](#bib.bib21)) |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '| Webis Gmane Email Corpus 2019 | 一个从 153M 邮件中收集的对话数据集。 | 开放领域对话 | Bevendorff
    等人 ([2020](#bib.bib21)) |'
- en: '| LibreSpeech Corpus | Contain 500 hours’ speech produced by 1252 participants.
    | Open-domain Dialogue | Panayotov et al. ([2015](#bib.bib267)) |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
  zh: '| LibreSpeech Corpus | 包含 500 小时的演讲，由 1252 名参与者生成。 | 开放领域对话 | Panayotov 等人
    ([2015](#bib.bib267)) |'
- en: '| Motivational Interviewing | A dialogue dataset about conversational psychotherapy.
    | Open-domain Dialogue | Tanana et al. ([2016](#bib.bib360)) |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| Motivational Interviewing | 关于会话心理治疗的对话数据集。 | 开放领域对话 | Tanana 等人 ([2016](#bib.bib360))
    |'
- en: '| SubTle Corpus | Contact Ameixa for data. | Open-domain Dialogue | Lubis et al.
    ([2018](#bib.bib233)) |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| SubTle Corpus | 联系 Ameixa 获取数据。 | 开放领域对话 | Lubis 等人 ([2018](#bib.bib233))
    |'
- en: '| TED-LIUM | TED-talk monologues. | Open-domain Dialogue | Fung et al. ([2016](#bib.bib93))
    |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| TED-LIUM | TED 演讲独白。 | 开放领域对话 | Fung 等人 ([2016](#bib.bib93)) |'
- en: '| ECG NLPCC 2017 Data | Conversational dataset extracted from Weibo. | Open-domain
    Dialogue | Huang et al. ([2018](#bib.bib148)) |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| ECG NLPCC 2017 Data | 从微博中提取的对话数据集。 | 开放领域对话 | Huang 等人 ([2018](#bib.bib148))
    |'
- en: '| SEMEVAL15 | QA dataset with answer quality annotations via Amazon Mechanical
    Turk. | Question Answering | Nakov et al. ([2019](#bib.bib261)) |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| SEMEVAL15 | 一个通过 Amazon Mechanical Turk 提供答案质量注释的 QA 数据集。 | 问答系统 | Nakov
    等人 ([2019](#bib.bib261)) |'
- en: '| AMAZONQA | A QA dataset solving one-to-many problems. | Question Answering
    | Wan and McAuley ([2016](#bib.bib383)) |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| AMAZONQA | 一个解决一对多问题的 QA 数据集。 | 问答系统 | Wan 和 McAuley ([2016](#bib.bib383))
    |'
- en: '| TGIF-QA | A video-grounded QA dataset. | Question Answering | Jang et al.
    ([2017](#bib.bib153)) |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
  zh: '| TGIF-QA | 一个基于视频的 QA 数据集。 | 问答系统 | Jang 等人 ([2017](#bib.bib153)) |'
- en: '| QuAC | A QA dataset with 14K QA dialogues. | Question Answering | Choi et al.
    ([2018](#bib.bib53)) |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| QuAC | 一个包含 14K QA 对话的 QA 数据集。 | 问答系统 | Choi 等人 ([2018](#bib.bib53)) |'
- en: '| SQuAD | A question-answering dataset collected in crowdsourced fashion. |
    Question Answering | Rajpurkar et al. ([2018](#bib.bib287)) |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 一个通过众包方式收集的问答数据集。 | 问答系统 | Rajpurkar 等人 ([2018](#bib.bib287)) |'
- en: '| LIF | A dataset constructed based on QuAC. | Question Answering | Kundu et al.
    ([2020](#bib.bib175)) |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| LIF | 一个基于 QuAC 构建的数据集。 | 问答系统 | Kundu 等人 ([2020](#bib.bib175)) |'
- en: '| Yelp | It consists of customer reviews from Yelp Dataset Challenge | Response
    Retrieval | Tang et al. ([2015](#bib.bib361)) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| Yelp | 由 Yelp 数据集挑战中的客户评论组成。 | 回复检索 | Tang 等人 ([2015](#bib.bib361)) |'
- en: '| Debates | The dataset consists of debates on Congerssional bills. | Response
    Retrieval | Thomas et al. ([2006](#bib.bib368)) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| Debates | 数据集包括关于国会法案的辩论。 | 回答检索 | Thomas et al. ([2006](#bib.bib368)) |'
- en: '| PERSONACHAT | It provides profile information of the agents and background
    of users. | Speaker Consistency and Personality Response | Zhang et al. ([2018b](#bib.bib449))
    |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| PERSONACHAT | 提供了代理人的个人信息和用户的背景。 | 发言者一致性和个性回应 | Zhang et al. ([2018b](#bib.bib449))
    |'
- en: '| KvPI | Contain consistency annotations between response and corresponding
    key-value profiles. | Speaker Consistency and Personality Response | Song et al.
    ([2020b](#bib.bib336)) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| KvPI | 包含回应与相应键值配置文件之间的一致性注释。 | 发言者一致性和个性回应 | Song et al. ([2020b](#bib.bib336))
    |'
- en: '| ConvAI2 | A dataset constructed on the base of Persona-Chat, each conversation
    having profiles from a set containing persona candidates. | Speaker Consistency
    and Personality Response | Dinan et al. ([2019](#bib.bib73)) |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| ConvAI2 | 一个基于 Persona-Chat 构建的数据集，每个对话都有来自包含人物候选集的配置文件。 | 发言者一致性和个性回应 |
    Dinan et al. ([2019](#bib.bib73)) |'
- en: '| PEC | An open-domain dataset annotated with persona labels. | Speaker Consistency
    and Personality Response; Empathetic Response | Zhong et al. ([2020](#bib.bib460))
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| PEC | 一个带有人物标签的开放域数据集。 | 发言者一致性和个性回应；同理回应 | Zhong et al. ([2020](#bib.bib460))
    |'
- en: '| GuessWhat?! | A visual dialogue dataset for a two-player game about object
    recognition. | Visual Dialogue | De Vries et al. ([2017](#bib.bib66)) |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| GuessWhat?! | 一个关于物体识别的两人游戏的视觉对话数据集。 | 视觉对话 | De Vries et al. ([2017](#bib.bib66))
    |'
- en: '| VisDial | A visual dialogue dataset whose images are obtained from COCO data
    set. | Visual Dialogue | [https://visualdialog.org/data](https://visualdialog.org/data)
    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| VisDial | 一个视觉对话数据集，其图像来源于 COCO 数据集。 | 视觉对话 | [https://visualdialog.org/data](https://visualdialog.org/data)
    |'
- en: '| AVSD | A video-grounded dialogue dataset. | Visual Dialogue | Yoshino et al.
    ([2018](#bib.bib436)) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| AVSD | 一个视频为基础的对话数据集。 | 视觉对话 | Yoshino et al. ([2018](#bib.bib436)) |'
- en: '| VFD | A visual dialogue dataset annotated with unique eye-gaze locations.
    | Visual Dialogue | Kamezawa et al. ([2020](#bib.bib163)) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| VFD | 一个带有独特注视位置的视觉对话数据集。 | 视觉对话 | Kamezawa et al. ([2020](#bib.bib163))
    |'
- en: '| PhotoBook | A dataset for task-oriented visual dialogues. | Visual Dialogue
    | Haber et al. ([2019](#bib.bib118)) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| PhotoBook | 一个用于任务导向视觉对话的数据集。 | 视觉对话 | Haber et al. ([2019](#bib.bib118))
    |'
- en: '| IGC | A dataset containing conversations discussing a given image. | Visual
    Dialogue | Mostafazadeh et al. ([2017](#bib.bib258)) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| IGC | 一个包含讨论给定图像的对话的数据集。 | 视觉对话 | Mostafazadeh et al. ([2017](#bib.bib258))
    |'
- en: '| Image-Chat | Contain conversations grounded with images. The conversations
    are also annotated with personality. | Visual Dialogue; Speaker Consistency and
    Personality Response | Shuster et al. ([2018](#bib.bib323)) |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| Image-Chat | 包含基于图像的对话。这些对话还带有个性注释。 | 视觉对话；发言者一致性和个性回应 | Shuster et al. ([2018](#bib.bib323))
    |'
- en: 7 Conclusions and Trends
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与趋势
- en: More and more researchers are investigating conversational tasks. One factor
    contributing to the popularity of conversational tasks is the increasing demand
    for chatbots in industry and daily life. Industry agents like Apple’s Siri, Microsoft’s
    Cortana, Facebook M, Google Assistant, and Amazon’s Alexa have brought huge convenience
    to people’s lives. Another reason is that a considerable amount of natural language
    data is in the form of dialogues, which contributes to the efforts in dialogue
    research.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的研究者正在研究对话任务。对话任务受欢迎的一个因素是工业和日常生活中对聊天机器人的需求不断增加。像苹果的 Siri、微软的 Cortana、Facebook
    M、谷歌助手和亚马逊的 Alexa 等工业代理为人们的生活带来了极大的便利。另一个原因是大量的自然语言数据以对话形式存在，这也促进了对话研究的努力。
- en: 'In this paper we discuss dialogue systems from two perspectives: model and
    system type. Dialogue systems are a complicated but promising task because it
    involves the whole process of communication between agent and human. The works
    of recent years show an overwhelming preference towards neural methods, no matter
    in task-oriented or open-domain dialogue systems. Neural methods outperform traditional
    rule-based methods, statistical methods and machine learning methods for that
    neural models have the stronger fitting ability and require less hand-crafted
    feature engineering.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们从模型和系统类型两个角度讨论了对话系统。对话系统是一项复杂但充满前景的任务，因为它涉及到代理与人类之间整个交流过程。近年来的研究工作显示，无论是在任务导向的还是开放领域的对话系统中，都有一种压倒性的偏好倾向于神经方法。神经方法优于传统的规则基础方法、统计方法和机器学习方法，因为神经模型具有更强的拟合能力，并且需要更少的手工特征工程。
- en: 'We systematically summarized and categorized the latest works in dialogue systems,
    and also in other dialogue-related tasks. We hope these discussions and insights
    provide a comprehensive picture of the state-of-the-art in this area and pave
    the way for further research. Finally, we discuss some possible research trends
    arising from the works reviewed:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统地总结和分类了对话系统以及其他对话相关任务的最新工作。我们希望这些讨论和见解能提供该领域最前沿的全面图景，并为进一步的研究铺平道路。最后，我们讨论了一些从所审阅的工作中可能出现的研究趋势：
- en: Multimodal dialogue systems
  id: totrans-563
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多模态对话系统
- en: The world is multimodal and humans observe it via multiple senses such as vision,
    hearing, smell, taste, and touch. In a conversational interaction, humans tend
    to make responses not only based on text, but also on what they see and hear.
    Thus, some researchers argue that chatbots should also have such abilities to
    blend information from different modalities. There are some recent works trying
    to build multimodal dialogue systems (Le et al., [2019](#bib.bib179); Chauhan
    et al., [2019](#bib.bib38); Saha et al., [2020](#bib.bib303); Singla et al., [2020](#bib.bib331);
    Young et al., [2020](#bib.bib439)), but these systems are still far from mature.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 世界是多模态的，人类通过视觉、听觉、嗅觉、味觉和触觉等多个感官来观察它。在对话交互中，人类往往不仅根据文本做出回应，还会根据他们看到和听到的内容做出反应。因此，一些研究人员认为聊天机器人也应该具备从不同模态融合信息的能力。最近有一些工作试图建立多模态对话系统（Le
    等，[2019](#bib.bib179)；Chauhan 等，[2019](#bib.bib38)；Saha 等，[2020](#bib.bib303)；Singla
    等，[2020](#bib.bib331)；Young 等，[2020](#bib.bib439)），但这些系统仍然远未成熟。
- en: Multitask dialogue systems
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多任务对话系统
- en: Dialogue systems are categorized into task-oriented and open-domain systems.
    Such a research boundary has existed for a long time because task-oriented dialogue
    systems involve dialogue states, which constrain the decoding process. However,
    works in end-to-end task-oriented dialogue systems and knowledge-grounded open-domain
    systems provide a possibility of blending these two categories into a single framework,
    or even a single model. Such blended dialogue systems perform as assistants and
    chatbots simultaneously.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统被分为任务导向型和开放领域型。这种研究边界已经存在很长时间，因为任务导向的对话系统涉及对话状态，这限制了解码过程。然而，端到端任务导向对话系统和知识基础开放领域系统的研究工作提供了一种将这两类系统融合到一个框架甚至一个模型中的可能性。这种融合的对话系统同时作为助手和聊天机器人进行工作。
- en: Corpus exploration on Internet
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 互联网语料库探索
- en: 'In Section [6](#S6 "6 Datasets ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") we reviewed many datasets for dialogue systems
    training. However, data is still far from enough to train a perfect dialogue system.
    Many learning techniques are designed to alleviate this problem, such as reinforcement
    learning, meta-learning, transfer learning, and active learning. But many works
    ignore a significant source of information, which is the dialogue corpus on the
    Internet. There is a large volume of conversational corpus on the Internet but
    people have no access to the raw corpus because much of it is in a messy condition.
    In the future, dialogue agents should be able to explore useful corpus on the
    Internet in real-time for training. This can be achieved by standardizing online
    corpus access and their related legal terms. Moreover, real-time conversational
    corpus exploration can be an independent task that deserves study.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[6](#S6 "6 Datasets ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")节中，我们回顾了许多对话系统训练的数据集。然而，数据仍远未足够以训练一个完美的对话系统。许多学习技术被设计用来缓解这个问题，如强化学习、元学习、迁移学习和主动学习。但许多研究忽略了一个重要的信息来源，即互联网上的对话语料库。互联网上有大量的对话语料库，但人们无法访问原始语料，因为其中许多是杂乱无章的。未来，对话代理应能够实时探索互联网上有用的语料进行训练。这可以通过标准化在线语料的访问及其相关法律条款来实现。此外，实时对话语料探索可以作为一个独立的任务值得研究。'
- en: User modeling
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 用户建模
- en: User modeling is a hot topic in both dialogue generation (Gür et al., [2018](#bib.bib117);
    Serras et al., [2019](#bib.bib315)) and dialogue systems evaluation (Kannan and
    Vinyals, [2017](#bib.bib164)). Basically, the user modeling module tries to simulate
    the real decisions and actions of a human user. It makes decisions based on the
    dialogue state or dialogue history. In dialogue generation tasks, modeling the
    user helps the agent converse more coherently, based on the background information
    or even speaking habits. Besides that, a mature user simulator can provide an
    interactive training environment, which reduces the reliance on annotated training
    samples when training a dialogue system. In dialogue systems evaluation tasks,
    a user simulator provides user messages to test a dialogue agent. More recent
    user simulators also give feedback concerning the responses generated by the dialogue
    agent. However, user modeling is a challenging task since no matter explicit user
    simulation or implicit user modeling is actually the same in difficulty as response
    generation. Since response generation systems are not perfect yet, user modeling
    can still be a topic worthy of study.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 用户建模是对话生成（Gür等人，[2018](#bib.bib117)；Serras等人，[2019](#bib.bib315)）和对话系统评估（Kannan和Vinyals，[2017](#bib.bib164)）中的一个热门话题。基本上，用户建模模块试图模拟真实用户的决策和行为。它基于对话状态或对话历史做出决策。在对话生成任务中，建模用户帮助代理基于背景信息甚至说话习惯进行更连贯的对话。除此之外，一个成熟的用户模拟器可以提供一个互动的训练环境，减少对标注训练样本的依赖。在对话系统评估任务中，用户模拟器提供用户消息以测试对话代理。更新的用户模拟器还提供关于对话代理生成的回应的反馈。然而，用户建模是一项具有挑战性的任务，因为无论是显式的用户模拟还是隐式的用户建模，实际上都与回应生成同样困难。由于回应生成系统尚不完美，用户建模仍然是一个值得研究的课题。
- en: Dialogue generation with a long-term goal
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有长期目标的对话生成
- en: Most of our daily conversations are chitchats without any purpose. However,
    there are quite a few scenarios when we purposely guide the conversation content
    to achieve a specific goal. Current open-domain dialogue systems tend to model
    the conversation without a long-term goal, which does not exhibit enough intelligence.
    There are some recent works that apply reinforcement policy learning to model
    a long-term reward which encourages the agent to converse with a long-term goal,
    such as the work of Xu et al. ([2020a](#bib.bib426)). This topic will lead to
    strong artificial intelligence, which is useful in some real-life applications
    such as negotiation or story-telling chatbots.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 我们日常的大多数对话都是没有目的的闲聊。然而，也有不少场景下我们有意引导对话内容以实现特定目标。当前的开放领域对话系统往往在没有长期目标的情况下建模，这显得智能程度不够。一些近期的研究应用了强化学习策略，以建模长期奖励，鼓励代理在对话中实现长期目标，例如徐等人（[2020a](#bib.bib426)）的工作。这一主题将引导强人工智能的发展，对于一些现实应用，如谈判或讲故事的聊天机器人，是非常有用的。
- en: Acknowledgements
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research/project is supported by A*STAR under its Industry Alignment Fund
    (LOA Award I1901E0046).
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究/项目得到了 A*STAR 产业对接基金 (LOA 奖项 I1901E0046) 的支持。
- en: References
  id: totrans-575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agarwal et al. (2020) Agarwal S, Bui T, Lee JY, Konstas I, Rieser V (2020)
    History for visual dialog: Do we really need it? arXiv preprint arXiv:200507493'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等人 (2020) Agarwal S, Bui T, Lee JY, Konstas I, Rieser V (2020) 视觉对话中的历史：我们真的需要它吗？arXiv
    预印本 arXiv:200507493
- en: Aghajanyan et al. (2020) Aghajanyan A, Maillard J, Shrivastava A, Diedrick K,
    Haeger M, Li H, Mehdad Y, Stoyanov V, Kumar A, Lewis M, et al. (2020) Conversational
    semantic parsing. arXiv preprint arXiv:200913655
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aghajanyan 等人 (2020) Aghajanyan A, Maillard J, Shrivastava A, Diedrick K, Haeger
    M, Li H, Mehdad Y, Stoyanov V, Kumar A, Lewis M, 等 (2020) 对话语义解析。arXiv 预印本 arXiv:200913655
- en: Akama et al. (2020) Akama R, Yokoi S, Suzuki J, Inui K (2020) Filtering noisy
    dialogue corpora by connectivity and content relatedness. arXiv preprint arXiv:200414008
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akama 等人 (2020) Akama R, Yokoi S, Suzuki J, Inui K (2020) 通过连接性和内容相关性过滤噪声对话语料库。arXiv
    预印本 arXiv:200414008
- en: Alberti et al. (2019) Alberti C, Ling J, Collins M, Reitter D (2019) Fusion
    of detected objects in text for visual question answering. arXiv preprint arXiv:190805054
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alberti 等人 (2019) Alberti C, Ling J, Collins M, Reitter D (2019) 在视觉问答中融合检测到的文本对象。arXiv
    预印本 arXiv:190805054
- en: 'Aloysius and Geetha (2017) Aloysius N, Geetha M (2017) A review on deep convolutional
    neural networks. In: 2017 International Conference on Communication and Signal
    Processing (ICCSP), IEEE, pp 0588–0592'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aloysius 和 Geetha (2017) Aloysius N, Geetha M (2017) 深度卷积神经网络的综述。在：2017 年国际通信与信号处理会议
    (ICCSP)，IEEE，第 0588–0592 页
- en: 'Annervaz et al. (2018) Annervaz K, Chowdhury SBR, Dukkipati A (2018) Learning
    beyond datasets: Knowledge graph augmented neural networks for natural language
    processing. arXiv preprint arXiv:180205930'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Annervaz 等人 (2018) Annervaz K, Chowdhury SBR, Dukkipati A (2018) 超越数据集的学习：用于自然语言处理的知识图谱增强神经网络。arXiv
    预印本 arXiv:180205930
- en: 'Arora et al. (2013) Arora S, Batra K, Singh S (2013) Dialogue system: A brief
    review. arXiv preprint arXiv:13064134'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 等人 (2013) Arora S, Batra K, Singh S (2013) 对话系统：简要回顾。arXiv 预印本 arXiv:13064134
- en: Asghar et al. (2016) Asghar N, Poupart P, Jiang X, Li H (2016) Deep active learning
    for dialogue generation. arXiv preprint arXiv:161203929
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asghar 等人 (2016) Asghar N, Poupart P, Jiang X, Li H (2016) 对话生成的深度主动学习。arXiv
    预印本 arXiv:161203929
- en: Asri et al. (2016) Asri LE, He J, Suleman K (2016) A sequence-to-sequence model
    for user simulation in spoken dialogue systems. arXiv preprint arXiv:160700070
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asri 等人 (2016) Asri LE, He J, Suleman K (2016) 用于口语对话系统的序列到序列模型。arXiv 预印本 arXiv:160700070
- en: 'Asri et al. (2017) Asri LE, Schulz H, Sharma S, Zumer J, Harris J, Fine E,
    Mehrotra R, Suleman K (2017) Frames: a corpus for adding memory to goal-oriented
    dialogue systems. arXiv preprint arXiv:170400057'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Asri 等人 (2017) Asri LE, Schulz H, Sharma S, Zumer J, Harris J, Fine E, Mehrotra
    R, Suleman K (2017) Frames：一个用于向目标导向对话系统中添加记忆的语料库。arXiv 预印本 arXiv:170400057
- en: 'Aubert et al. (1994) Aubert X, Dugast C, Ney H, Steinbiss V (1994) Large vocabulary
    continuous speech recognition of wall street journal data. In: Proceedings of
    ICASSP’94. IEEE International Conference on Acoustics, Speech and Signal Processing,
    IEEE, vol 2, pp II–129'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aubert 等人 (1994) Aubert X, Dugast C, Ney H, Steinbiss V (1994) 华尔街日报数据的大词汇量连续语音识别。在：ICASSP’94
    会议论文集。IEEE 国际声学、语音与信号处理会议，IEEE，第 2 卷，第 II–129 页
- en: Bahdanau et al. (2014) Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation
    by jointly learning to align and translate. arXiv preprint arXiv:14090473
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等人 (2014) Bahdanau D, Cho K, Bengio Y (2014) 通过联合学习对齐和翻译的神经机器翻译。arXiv
    预印本 arXiv:14090473
- en: Baheti et al. (2020) Baheti A, Ritter A, Small K (2020) Fluent response generation
    for conversational question answering. arXiv preprint arXiv:200510464
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baheti 等人 (2020) Baheti A, Ritter A, Small K (2020) 对话问答的流畅响应生成。arXiv 预印本 arXiv:200510464
- en: Balakrishnan et al. (2019) Balakrishnan A, Rao J, Upasani K, White M, Subba
    R (2019) Constrained decoding for neural nlg from compositional representations
    in task-oriented dialogue. arXiv preprint arXiv:190607220
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balakrishnan 等人 (2019) Balakrishnan A, Rao J, Upasani K, White M, Subba R (2019)
    基于任务导向对话的组合表示的神经自然语言生成的约束解码。arXiv 预印本 arXiv:190607220
- en: 'Banerjee and Lavie (2005) Banerjee S, Lavie A (2005) Meteor: An automatic metric
    for mt evaluation with improved correlation with human judgments. In: Proceedings
    of the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization, pp 65–72'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Lavie (2005) Banerjee S, Lavie A (2005) Meteor：一种自动化的机器翻译评估指标，与人工评估的相关性有所提升。在：ACL
    研讨会论文集，讨论机器翻译和/或总结的内在和外在评估指标，第 65–72 页
- en: 'Bao et al. (2019a) Bao S, He H, Wang F, Lian R, Wu H (2019a) Know more about
    each other: Evolving dialogue strategy via compound assessment. arXiv preprint
    arXiv:190600549'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao等（2019a）Bao S, He H, Wang F, Lian R, Wu H (2019a) 了解彼此更多：通过复合评估发展对话策略。arXiv预印本
    arXiv:190600549
- en: 'Bao et al. (2019b) Bao S, He H, Wang F, Wu H, Wang H (2019b) Plato: Pre-trained
    dialogue generation model with discrete latent variable. arXiv preprint arXiv:191007931'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao等（2019b）Bao S, He H, Wang F, Wu H, Wang H (2019b) Plato：具有离散潜变量的预训练对话生成模型。arXiv预印本
    arXiv:191007931
- en: Bapna et al. (2017) Bapna A, Tur G, Hakkani-Tur D, Heck L (2017) Towards zero-shot
    frame semantic parsing for domain scaling. arXiv preprint arXiv:170702363
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bapna等（2017）Bapna A, Tur G, Hakkani-Tur D, Heck L (2017) 面向领域扩展的零样本框架语义解析。arXiv预印本
    arXiv:170702363
- en: 'Beeferman et al. (2019) Beeferman D, Brannon W, Roy D (2019) Radiotalk: A large-scale
    corpus of talk radio transcripts. arXiv preprint arXiv:190707073'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeferman等（2019）Beeferman D, Brannon W, Roy D (2019) Radiotalk：一个大规模广播电台转录语料库。arXiv预印本
    arXiv:190707073
- en: Bengio et al. (1994) Bengio Y, Simard P, Frasconi P (1994) Learning long-term
    dependencies with gradient descent is difficult. IEEE transactions on neural networks
    5(2):157–166
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio等（1994）Bengio Y, Simard P, Frasconi P (1994) 用梯度下降学习长期依赖性是困难的。IEEE神经网络事务
    5(2):157–166
- en: 'Bevendorff et al. (2020) Bevendorff J, Al Khatib K, Potthast M, Stein B (2020)
    Crawling and preprocessing mailing lists at scale for dialog analysis. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    1151–1158'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bevendorff等（2020）Bevendorff J, Al Khatib K, Potthast M, Stein B (2020) 大规模爬取和预处理邮件列表用于对话分析。
    In: 第58届计算语言学协会年会论文集，第1151–1158页'
- en: Bi et al. (2019) Bi W, Gao J, Liu X, Shi S (2019) Fine-grained sentence functions
    for short-text conversation. arXiv preprint arXiv:190710302
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi等（2019）Bi W, Gao J, Liu X, Shi S (2019) 面向短文本对话的细粒度句子功能。arXiv预印本 arXiv:190710302
- en: 'Bordes et al. (2013) Bordes A, Usunier N, Garcia-Duran A, Weston J, Yakhnenko
    O (2013) Translating embeddings for modeling multi-relational data. In: Neural
    Information Processing Systems (NIPS), pp 1–9'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bordes等（2013）Bordes A, Usunier N, Garcia-Duran A, Weston J, Yakhnenko O (2013)
    翻译嵌入以建模多关系数据。 In: 神经信息处理系统（NIPS），第1–9页'
- en: Bordes et al. (2014) Bordes A, Glorot X, Weston J, Bengio Y (2014) A semantic
    matching energy function for learning with multi-relational data. Machine Learning
    94(2):233–259
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bordes等（2014）Bordes A, Glorot X, Weston J, Bengio Y (2014) 一种用于学习多关系数据的语义匹配能量函数。机器学习
    94(2):233–259
- en: Bordes et al. (2016) Bordes A, Boureau YL, Weston J (2016) Learning end-to-end
    goal-oriented dialog. arXiv preprint arXiv:160507683
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bordes等（2016）Bordes A, Boureau YL, Weston J (2016) 端到端目标导向对话学习。arXiv预印本 arXiv:160507683
- en: 'Bosselut et al. (2019) Bosselut A, Rashkin H, Sap M, Malaviya C, Celikyilmaz
    A, Choi Y (2019) Comet: Commonsense transformers for automatic knowledge graph
    construction. arXiv preprint arXiv:190605317'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bosselut等（2019）Bosselut A, Rashkin H, Sap M, Malaviya C, Celikyilmaz A, Choi
    Y (2019) Comet：用于自动知识图谱构建的常识变换器。arXiv预印本 arXiv:190605317
- en: 'Bouchacourt and Baroni (2019) Bouchacourt D, Baroni M (2019) Miss tools and
    mr fruit: Emergent communication in agents learning about object affordances.
    arXiv preprint arXiv:190511871'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bouchacourt和Baroni（2019）Bouchacourt D, Baroni M (2019) Miss tools and mr fruit：代理学习物体可用性的紧急通信。arXiv预印本
    arXiv:190511871
- en: Boyd et al. (2020) Boyd A, Puri R, Shoeybi M, Patwary M, Catanzaro B (2020)
    Large scale multi-actor generative dialog modeling. arXiv preprint arXiv:200506114
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyd等（2020）Boyd A, Puri R, Shoeybi M, Patwary M, Catanzaro B (2020) 大规模多演员生成对话建模。arXiv预印本
    arXiv:200506114
- en: 'Bruni and Fernandez (2017) Bruni E, Fernandez R (2017) Adversarial evaluation
    for open-domain dialogue generation. In: Proceedings of the 18th Annual SIGdial
    Meeting on Discourse and Dialogue, pp 284–288'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bruni和Fernandez（2017）Bruni E, Fernandez R (2017) 开放领域对话生成的对抗评估。 In: 第18届SIGdial会议论文集，第284–288页'
- en: Budzianowski et al. (2018) Budzianowski P, Wen TH, Tseng BH, Casanueva I, Ultes
    S, Ramadan O, Gašić M (2018) Multiwoz–a large-scale multi-domain wizard-of-oz
    dataset for task-oriented dialogue modelling. arXiv preprint arXiv:181000278
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Budzianowski等（2018）Budzianowski P, Wen TH, Tseng BH, Casanueva I, Ultes S, Ramadan
    O, Gašić M (2018) Multiwoz–一个大规模多领域的Wizard-of-Oz数据集，用于任务导向对话建模。arXiv预印本 arXiv:181000278
- en: 'Busso et al. (2008) Busso C, Bulut M, Lee CC, Kazemzadeh A, Mower E, Kim S,
    Chang JN, Lee S, Narayanan SS (2008) Iemocap: Interactive emotional dyadic motion
    capture database. Language resources and evaluation 42(4):335–359'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Busso等（2008）Busso C, Bulut M, Lee CC, Kazemzadeh A, Mower E, Kim S, Chang JN,
    Lee S, Narayanan SS (2008) Iemocap：交互式情感二人运动捕捉数据库。语言资源与评估 42(4):335–359
- en: 'Byrne et al. (2019) Byrne B, Krishnamoorthi K, Sankar C, Neelakantan A, Duckworth
    D, Yavuz S, Goodrich B, Dubey A, Cedilnik A, Kim KY (2019) Taskmaster-1: Toward
    a realistic and diverse dialog dataset. arXiv preprint arXiv:190905358'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Byrne 等（2019）伯恩·B、克里希纳穆尔西·K、桑卡·C、尼拉坎坦·A、达克沃斯·D、亚武兹·S、古德里奇·B、杜比·A、塞迪尔尼克·A、金·KY（2019）Taskmaster-1：迈向现实和多样化的对话数据集。arXiv
    预印本 arXiv:190905358
- en: 'Cahill et al. (1999) Cahill L, Doran C, Evans R, Mellish C, Paiva D, Reape
    M, Scott D, Tipper N (1999) In search of a reference architecture for nlg systems.
    In: Proceedings of the 7th european workshop on natural language generation, Citeseer,
    pp 77–85'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cahill 等（1999）卡希尔·L、多兰·C、埃文斯·R、梅利什·C、帕伊瓦·D、里普·M、斯科特·D、蒂珀·N（1999）寻找 NLG 系统的参考架构。第7届欧洲自然语言生成研讨会论文集，Citeseer，pp
    77–85
- en: Campagna et al. (2020) Campagna G, Foryciarz A, Moradshahi M, Lam MS (2020)
    Zero-shot transfer learning with synthesized data for multi-domain dialogue state
    tracking. arXiv preprint arXiv:200500891
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campagna 等（2020）坎帕尼亚·G、福里齐亚兹·A、莫拉德沙希·M、拉姆·MS（2020）使用合成数据进行零样本迁移学习以进行多领域对话状态跟踪。arXiv
    预印本 arXiv:200500891
- en: 'Cao et al. (2019) Cao J, Tanana M, Imel ZE, Poitras E, Atkins DC, Srikumar
    V (2019) Observing dialogue in therapy: Categorizing and forecasting behavioral
    codes. arXiv preprint arXiv:190700326'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等（2019）曹静、塔纳娜·M、伊梅尔·ZE、普瓦特拉斯·E、阿特金斯·DC、斯里库玛·V（2019）观察治疗中的对话：分类和预测行为代码。arXiv
    预印本 arXiv:190700326
- en: Carlson et al. (2002) Carlson L, Okurowski ME, Marcu D (2002) RST discourse
    treebank. Linguistic Data Consortium, University of Pennsylvania
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlson 等（2002）卡尔森·L、奥库罗夫斯基·ME、马尔库·D（2002）RST 语篇树库。语言数据联盟，宾夕法尼亚大学
- en: 'Chandramohan et al. (2011) Chandramohan S, Geist M, Lefevre F, Pietquin O (2011)
    User simulation in dialogue systems using inverse reinforcement learning. In:
    Twelfth Annual Conference of the International Speech Communication Association'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandramohan 等（2011）钱德拉莫汉·S、盖斯特·M、勒费弗尔·F、皮特昆·O（2011）在对话系统中使用逆向强化学习的用户模拟。第十二届国际语音通信协会年会
- en: 'Chauhan et al. (2019) Chauhan H, Firdaus M, Ekbal A, Bhattacharyya P (2019)
    Ordinal and attribute aware response generation in a multimodal dialogue system.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 5437–5447'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chauhan 等（2019）乔汉·H、费尔道斯·M、埃克巴尔·A、巴塔查尔亚·P（2019）在多模态对话系统中的序数和属性感知响应生成。收录于第57届计算语言学协会年会论文集，pp
    5437–5447
- en: 'Chen et al. (2017a) Chen H, Liu X, Yin D, Tang J (2017a) A survey on dialogue
    systems: Recent advances and new frontiers. Acm Sigkdd Explorations Newsletter
    19(2):25–35'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2017a）陈华、刘鑫、尹东、唐江（2017a）对话系统调查：最新进展与新前沿。ACM Sigkdd Explorations Newsletter
    19(2):25–35
- en: Chen and Yang (2020) Chen J, Yang D (2020) Multi-view sequence-to-sequence models
    with conversational structure for abstractive dialogue summarization. arXiv preprint
    arXiv:201001672
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 和 Yang（2020）陈静、杨达（2020）具有对话结构的多视角序列到序列模型用于抽象对话摘要。arXiv 预印本 arXiv:201001672
- en: Chen et al. (2020a) Chen J, Zhang R, Mao Y, Xu J (2020a) Parallel interactive
    networks for multi-domain dialogue state generation. arXiv preprint arXiv:200907616
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020a）陈静、张锐、毛逸、徐俊（2020a）用于多领域对话状态生成的并行交互网络。arXiv 预印本 arXiv:200907616
- en: 'Chen et al. (2017b) Chen L, Zhou X, Chang C, Yang R, Yu K (2017b) Agent-aware
    dropout dqn for safe and efficient on-line dialogue policy learning. In: Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing, pp
    2454–2464'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2017b）陈林、周晓、常聪、杨锐、于凯（2017b）基于代理意识的 Dropout DQN 用于安全高效的在线对话策略学习。收录于2017年自然语言处理经验方法会议论文集，pp
    2454–2464
- en: 'Chen et al. (2019a) Chen M, Liu R, Shen L, Yuan S, Zhou J, Wu Y, He X, Zhou
    B (2019a) The jddc corpus: A large-scale multi-turn chinese dialogue dataset for
    e-commerce customer service. arXiv preprint arXiv:191109969'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019a）陈敏、刘睿、沈亮、袁爽、周杰、吴越、贺鑫、周博（2019a）JDDC 语料库：一个大规模多轮中文对话数据集，用于电子商务客服。arXiv
    预印本 arXiv:191109969
- en: Chen et al. (2019b) Chen W, Chen J, Qin P, Yan X, Wang WY (2019b) Semantically
    conditioned dialog response generation via hierarchical disentangled self-attention.
    arXiv preprint arXiv:190512866
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019b）陈伟、陈静、秦鹏、严旭、王伟瑜（2019b）通过层次化解耦自注意力生成语义条件对话响应。arXiv 预印本 arXiv:190512866
- en: 'Chen et al. (2019c) Chen X, Xu J, Xu B (2019c) A working memory model for task-oriented
    dialog response generation. In: Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, pp 2687–2693'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019c）陈晓、徐俊、徐波（2019c）用于任务导向对话响应生成的工作记忆模型。收录于第57届计算语言学协会年会论文集，pp 2687–2693
- en: 'Chen et al. (2020b) Chen X, Meng F, Li P, Chen F, Xu S, Xu B, Zhou J (2020b)
    Bridging the gap between prior and posterior knowledge selection for knowledge-grounded
    dialogue generation. In: Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pp 3426–3437'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020b）Chen X, Meng F, Li P, Chen F, Xu S, Xu B, Zhou J（2020b）弥合先验知识与后验知识选择的差距用于知识驱动的对话生成。在：2020年自然语言处理经验方法会议（EMNLP）论文集，第
    3426–3437 页
- en: 'Chen et al. (2019d) Chen YC, Li L, Yu L, El Kholy A, Ahmed F, Gan Z, Cheng
    Y, Liu J (2019d) Uniter: Learning universal image-text representations. ECCV'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2019d）Chen YC, Li L, Yu L, El Kholy A, Ahmed F, Gan Z, Cheng Y, Liu J（2019d）Uniter：学习通用的图像-文本表示。ECCV
- en: 'Chen et al. (2016) Chen YN, Hakkani-Tür D, Tür G, Gao J, Deng L (2016) End-to-end
    memory networks with knowledge carryover for multi-turn spoken language understanding.
    In: Interspeech, pp 3245–3249'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2016）Chen YN, Hakkani-Tür D, Tür G, Gao J, Deng L（2016）具有知识传递的端到端记忆网络用于多轮口语理解。在：Interspeech，第
    3245–3249 页
- en: Cheng et al. (2020) Cheng J, Agrawal D, Alonso HM, Bhargava S, Driesen J, Flego
    F, Kaplan D, Kartsaklis D, Li L, Piraviperumal D, et al. (2020) Conversational
    semantic parsing for dialog state tracking. arXiv preprint arXiv:201012770
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等（2020）Cheng J, Agrawal D, Alonso HM, Bhargava S, Driesen J, Flego F,
    Kaplan D, Kartsaklis D, Li L, Piraviperumal D, 等（2020）对话状态跟踪的对话语义解析。arXiv 预印本
    arXiv:201012770
- en: Cho and May (2020) Cho H, May J (2020) Grounding conversations with improvised
    dialogues. arXiv preprint arXiv:200409544
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 和 May（2020）Cho H, May J（2020）用即兴对话来扎根对话。arXiv 预印本 arXiv:200409544
- en: 'Cho et al. (2014a) Cho K, Van Merriënboer B, Bahdanau D, Bengio Y (2014a) On
    the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:14091259'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2014a）Cho K, Van Merriënboer B, Bahdanau D, Bengio Y（2014a）神经机器翻译的特性：编码器-解码器方法。arXiv
    预印本 arXiv:14091259
- en: Cho et al. (2014b) Cho K, Van Merriënboer B, Gulcehre C, Bahdanau D, Bougares
    F, Schwenk H, Bengio Y (2014b) Learning phrase representations using rnn encoder-decoder
    for statistical machine translation. arXiv preprint arXiv:14061078
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2014b）Cho K, Van Merriënboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk
    H, Bengio Y（2014b）使用 rnn 编码器-解码器学习短语表示用于统计机器翻译。arXiv 预印本 arXiv:14061078
- en: 'Choi et al. (2018) Choi E, He H, Iyyer M, Yatskar M, Yih Wt, Choi Y, Liang
    P, Zettlemoyer L (2018) Quac: Question answering in context. arXiv preprint arXiv:180807036'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2018）Choi E, He H, Iyyer M, Yatskar M, Yih Wt, Choi Y, Liang P, Zettlemoyer
    L（2018）Quac：上下文中的问题回答。arXiv 预印本 arXiv:180807036
- en: Chung et al. (2014) Chung J, Gulcehre C, Cho K, Bengio Y (2014) Empirical evaluation
    of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:14123555
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等（2014）Chung J, Gulcehre C, Cho K, Bengio Y（2014）对序列建模的门控递归神经网络的经验评估。arXiv
    预印本 arXiv:14123555
- en: 'Chung et al. (2019) Chung YL, Kuzmenko E, Tekiroglu SS, Guerini M (2019) Conan–counter
    narratives through nichesourcing: a multilingual dataset of responses to fight
    online hate speech. arXiv preprint arXiv:191003270'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等（2019）Chung YL, Kuzmenko E, Tekiroglu SS, Guerini M（2019）Conan–通过细分来源的反叙事：一个多语言的数据集用于对抗网络仇恨言论。arXiv
    预印本 arXiv:191003270
- en: 'Cogswell et al. (2020) Cogswell M, Lu J, Jain R, Lee S, Parikh D, Batra D (2020)
    Dialog without dialog data: Learning visual dialog agents from vqa data. arXiv
    preprint arXiv:200712750'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cogswell 等（2020）Cogswell M, Lu J, Jain R, Lee S, Parikh D, Batra D（2020）无对话数据的对话：从
    vqa 数据中学习视觉对话代理。arXiv 预印本 arXiv:200712750
- en: Conneau et al. (2016) Conneau A, Schwenk H, Barrault L, Lecun Y (2016) Very
    deep convolutional networks for text classification. arXiv preprint arXiv:160601781
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau 等（2016）Conneau A, Schwenk H, Barrault L, Lecun Y（2016）非常深度的卷积网络用于文本分类。arXiv
    预印本 arXiv:160601781
- en: 'Coope et al. (2020) Coope S, Farghly T, Gerz D, Vulić I, Henderson M (2020)
    Span-convert: Few-shot span extraction for dialog with pretrained conversational
    representations. arXiv preprint arXiv:200508866'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coope 等（2020）Coope S, Farghly T, Gerz D, Vulić I, Henderson M（2020）Span-convert：使用预训练对话表示进行少量样本的跨度提取。arXiv
    预印本 arXiv:200508866
- en: Csáky et al. (2019) Csáky R, Purgai P, Recski G (2019) Improving neural conversational
    models with entropy-based data filtering. arXiv preprint arXiv:190505471
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Csáky 等（2019）Csáky R, Purgai P, Recski G（2019）通过基于熵的数据过滤改进神经对话模型。arXiv 预印本 arXiv:190505471
- en: 'Cui et al. (2020) Cui L, Wu Y, Liu S, Zhang Y, Zhou M (2020) Mutual: A dataset
    for multi-turn dialogue reasoning. arXiv preprint arXiv:200404494'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等（2020）Cui L, Wu Y, Liu S, Zhang Y, Zhou M（2020）Mutual：一个用于多轮对话推理的数据集。arXiv
    预印本 arXiv:200404494
- en: 'Dai et al. (2020) Dai Y, Li H, Tang C, Li Y, Sun J, Zhu X (2020) Learning low-resource
    end-to-end goal-oriented dialog for fast and reliable system deployment. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    609–618'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2020）Dai Y, Li H, Tang C, Li Y, Sun J, Zhu X（2020）《学习低资源的端到端目标导向对话以实现快速和可靠的系统部署》。发表于：第58届计算语言学协会年会论文集，第609–618页
- en: 'Dai et al. (2019) Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov
    R (2019) Transformer-xl: Attentive language models beyond a fixed-length context.
    arXiv preprint arXiv:190102860'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2019）Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R（2019）《Transformer-xl：超越固定长度上下文的注意力语言模型》。arXiv
    预印本 arXiv:190102860
- en: 'Dalton et al. (2020) Dalton J, Xiong C, Callan J (2020) Trec cast 2019: The
    conversational assistance track overview. arXiv preprint arXiv:200313624'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalton 等（2020）Dalton J, Xiong C, Callan J（2020）《Trec cast 2019：对话助手轨道概述》。arXiv
    预印本 arXiv:200313624
- en: 'Danescu-Niculescu-Mizil and Lee (2011) Danescu-Niculescu-Mizil C, Lee L (2011)
    Chameleons in imagined conversations: A new approach to understanding coordination
    of linguistic style in dialogs. arXiv preprint arXiv:11063077'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danescu-Niculescu-Mizil 和 Lee（2011）Danescu-Niculescu-Mizil C, Lee L（2011）《想象对话中的变色龙：理解对话中语言风格协调的新方法》。arXiv
    预印本 arXiv:11063077
- en: Danescu-Niculescu-Mizil et al. (2013) Danescu-Niculescu-Mizil C, Sudhof M, Jurafsky
    D, Leskovec J, Potts C (2013) A computational approach to politeness with application
    to social factors. arXiv preprint arXiv:13066078
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danescu-Niculescu-Mizil 等（2013）Danescu-Niculescu-Mizil C, Sudhof M, Jurafsky
    D, Leskovec J, Potts C（2013）《一种计算方法的礼貌研究及其在社会因素中的应用》。arXiv 预印本 arXiv:13066078
- en: 'De Vries et al. (2017) De Vries H, Strub F, Chandar S, Pietquin O, Larochelle
    H, Courville A (2017) Guesswhat?! visual object discovery through multi-modal
    dialogue. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp 5503–5512'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Vries 等（2017）De Vries H, Strub F, Chandar S, Pietquin O, Larochelle H, Courville
    A（2017）《猜猜是什么？！通过多模态对话进行视觉对象发现》。发表于：IEEE计算机视觉与模式识别会议论文集，第5503–5512页
- en: 'Deng et al. (2012) Deng L, Tur G, He X, Hakkani-Tur D (2012) Use of kernel
    deep convex networks and end-to-end learning for spoken language understanding.
    In: 2012 IEEE Spoken Language Technology Workshop (SLT), IEEE, pp 210–215'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等（2012）Deng L, Tur G, He X, Hakkani-Tur D（2012）《使用内核深度凸网络和端到端学习进行口语语言理解》。发表于：2012年IEEE口语语言技术研讨会（SLT），IEEE，第210–215页
- en: 'Deoras and Sarikaya (2013) Deoras A, Sarikaya R (2013) Deep belief network
    based semantic taggers for spoken language understanding. In: Interspeech, pp
    2713–2717'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deoras 和 Sarikaya（2013）Deoras A, Sarikaya R（2013）《基于深度信念网络的语义标注器用于口语语言理解》。发表于：Interspeech，第2713–2717页
- en: 'Deriu et al. (2020) Deriu J, Tuggener D, von Däniken P, Campos JA, Rodrigo
    A, Belkacem T, Soroa A, Agirre E, Cieliebak M (2020) Spot the bot: A robust and
    efficient framework for the evaluation of conversational dialogue systems. arXiv
    preprint arXiv:201002140'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deriu 等（2020）Deriu J, Tuggener D, von Däniken P, Campos JA, Rodrigo A, Belkacem
    T, Soroa A, Agirre E, Cieliebak M（2020）《识别机器人：一种用于对话系统评估的鲁棒且高效的框架》。arXiv 预印本 arXiv:201002140
- en: 'Devlin et al. (2018) Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:181004805'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2018）Devlin J, Chang MW, Lee K, Toutanova K（2018）《BERT：用于语言理解的深度双向变换器的预训练》。arXiv
    预印本 arXiv:181004805
- en: Dhingra et al. (2016) Dhingra B, Li L, Li X, Gao J, Chen YN, Ahmed F, Deng L
    (2016) Towards end-to-end reinforcement learning of dialogue agents for information
    access. arXiv preprint arXiv:160900777
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhingra 等（2016）Dhingra B, Li L, Li X, Gao J, Chen YN, Ahmed F, Deng L（2016）《面向信息访问的对话代理的端到端强化学习》。arXiv
    预印本 arXiv:160900777
- en: 'Dinan et al. (2018) Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J (2018)
    Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:181101241'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinan 等（2018）Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J（2018）《维基百科的魔法师：知识驱动的对话代理》。arXiv
    预印本 arXiv:181101241
- en: Dinan et al. (2019) Dinan E, Logacheva V, Malykh V, Miller A, Shuster K, Urbanek
    J, Kiela D, Szlam A, Serban I, Lowe R, et al. (2019) The second conversational
    intelligence challenge (convai2). arXiv preprint arXiv:190200098
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinan 等（2019）Dinan E, Logacheva V, Malykh V, Miller A, Shuster K, Urbanek J,
    Kiela D, Szlam A, Serban I, Lowe R 等（2019）《第二届对话智能挑战赛（convai2）》。arXiv 预印本 arXiv:190200098
- en: 'Dong et al. (2017) Dong L, Huang S, Wei F, Lapata M, Zhou M, Xu K (2017) Learning
    to generate product reviews from attributes. In: Proceedings of the 15th Conference
    of the European Chapter of the Association for Computational Linguistics: Volume
    1, Long Papers, pp 623–632'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2017）Dong L, Huang S, Wei F, Lapata M, Zhou M, Xu K（2017）从属性中生成产品评价的学习。载于：第15届计算语言学协会欧洲章节会议：卷1，长篇论文，第623–632页
- en: Du et al. (2019) Du N, Chen K, Kannan A, Tran L, Chen Y, Shafran I (2019) Extracting
    symptoms and their status from clinical conversations. arXiv preprint arXiv:190602239
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等（2019）Du N, Chen K, Kannan A, Tran L, Chen Y, Shafran I（2019）从临床对话中提取症状及其状态。arXiv
    预印本 arXiv:190602239
- en: 'Du and Black (2019) Du W, Black AW (2019) Boosting dialog response generation.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 38–43'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 和 Black（2019）Du W, Black AW（2019）提升对话回应生成。载于：第57届计算语言学协会年会论文集，第38–43页
- en: Dušek and Jurčíček (2016a) Dušek O, Jurčíček F (2016a) A context-aware natural
    language generator for dialogue systems. arXiv preprint arXiv:160807076
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dušek 和 Jurčíček（2016a）Dušek O, Jurčíček F（2016a）一个上下文感知的自然语言生成器用于对话系统。arXiv
    预印本 arXiv:160807076
- en: Dušek and Jurčíček (2016b) Dušek O, Jurčíček F (2016b) Sequence-to-sequence
    generation for spoken dialogue via deep syntax trees and strings. arXiv preprint
    arXiv:160605491
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dušek 和 Jurčíček（2016b）Dušek O, Jurčíček F（2016b）通过深度语法树和字符串的序列到序列生成用于口语对话。arXiv
    预印本 arXiv:160605491
- en: 'Elder et al. (2020) Elder H, O’Connor A, Foster J (2020) How to make neural
    natural language generation as reliable as templates in task-oriented dialogue.
    In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pp 2877–2888'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elder 等（2020）Elder H, O’Connor A, Foster J（2020）如何使神经自然语言生成在任务导向对话中与模板一样可靠。载于：2020年自然语言处理实证方法会议（EMNLP）论文集，第2877–2888页
- en: Elman (1990) Elman JL (1990) Finding structure in time. Cognitive science 14(2):179–211
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elman（1990）Elman JL（1990）在时间中寻找结构。认知科学 14(2):179–211
- en: Eric and Manning (2017) Eric M, Manning CD (2017) Key-value retrieval networks
    for task-oriented dialogue. arXiv preprint arXiv:170505414
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eric 和 Manning（2017）Eric M, Manning CD（2017）面向任务的对话中的键值检索网络。arXiv 预印本 arXiv:170505414
- en: 'Esteve et al. (2010) Esteve Y, Bazillon T, Antoine JY, Béchet F, Farinas J
    (2010) The epac corpus: Manual and automatic annotations of conversational speech
    in french broadcast news. In: LREC, Citeseer'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esteve 等（2010）Esteve Y, Bazillon T, Antoine JY, Béchet F, Farinas J（2010）EPAC
    语料库：法语广播新闻对话语音的手动和自动注释。载于：LREC，Citeseer
- en: 'Fan et al. (2019) Fan A, Jernite Y, Perez E, Grangier D, Weston J, Auli M (2019)
    Eli5: Long form question answering. arXiv preprint arXiv:190709190'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2019）Fan A, Jernite Y, Perez E, Grangier D, Weston J, Auli M（2019）Eli5：长篇问答。arXiv
    预印本 arXiv:190709190
- en: 'Fan et al. (2014) Fan M, Zhou Q, Chang E, Zheng F (2014) Transition-based knowledge
    graph embedding with relational mapping properties. In: Proceedings of the 28th
    Pacific Asia conference on language, information and computing, pp 328–337'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等（2014）Fan M, Zhou Q, Chang E, Zheng F（2014）基于过渡的知识图谱嵌入与关系映射属性。载于：第28届亚太语言、信息与计算会议论文集，第328–337页
- en: Feldman and El-Yaniv (2019) Feldman Y, El-Yaniv R (2019) Multi-hop paragraph
    retrieval for open-domain question answering. arXiv preprint arXiv:190606606
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feldman 和 El-Yaniv（2019）Feldman Y, El-Yaniv R（2019）开放域问答中的多跳段落检索。arXiv 预印本 arXiv:190606606
- en: Feng et al. (2019) Feng J, Tao C, Wu W, Feng Y, Zhao D, Yan R (2019) Learning
    a matching model with co-teaching for multi-turn response selection in retrieval-based
    dialogue systems. arXiv preprint arXiv:190604413
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2019）Feng J, Tao C, Wu W, Feng Y, Zhao D, Yan R（2019）学习一个匹配模型与共教学用于基于检索的对话系统中的多轮回应选择。arXiv
    预印本 arXiv:190604413
- en: 'Feng et al. (2020a) Feng S, Chen H, Li K, Yin D (2020a) Posterior-gan: Towards
    informative and coherent response generation with posterior generative adversarial
    network. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol 34,
    pp 7708–7715'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2020a）Feng S, Chen H, Li K, Yin D（2020a）Posterior-gan：通过后验生成对抗网络实现信息丰富且连贯的回应生成。载于：AAAI
    人工智能会议论文集，第34卷，第7708–7715页
- en: Feng et al. (2020b) Feng S, Ren X, Chen H, Sun B, Li K, Sun X (2020b) Regularizing
    dialogue generation by imitating implicit scenarios. arXiv preprint arXiv:201001893
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2020b）Feng S, Ren X, Chen H, Sun B, Li K, Sun X（2020b）通过模仿隐式场景来规范对话生成。arXiv
    预印本 arXiv:201001893
- en: 'Feng et al. (2020c) Feng S, Wan H, Gunasekara C, Patel SS, Joshi S, Lastras
    LA (2020c) doc2dial: A goal-oriented document-grounded dialogue dataset. arXiv
    preprint arXiv:201106623'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng等人（2020c）Feng S, Wan H, Gunasekara C, Patel SS, Joshi S, Lastras LA（2020c）“doc2dial：一个目标导向的文档基础对话数据集。”arXiv预印本arXiv:201106623。
- en: Ferracane et al. (2019) Ferracane E, Durrett G, Li JJ, Erk K (2019) Evaluating
    discourse in structured text representations. arXiv preprint arXiv:190601472
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferracane等人（2019）Ferracane E, Durrett G, Li JJ, Erk K（2019）“评估结构化文本表示中的话语。”arXiv预印本arXiv:190601472。
- en: Ficler and Goldberg (2017) Ficler J, Goldberg Y (2017) Controlling linguistic
    style aspects in neural language generation. arXiv preprint arXiv:170702633
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ficler和Goldberg（2017）Ficler J, Goldberg Y（2017）“控制神经语言生成中的语言风格方面。”arXiv预印本arXiv:170702633。
- en: 'Finn et al. (2017) Finn C, Abbeel P, Levine S (2017) Model-agnostic meta-learning
    for fast adaptation of deep networks. In: International Conference on Machine
    Learning, PMLR, pp 1126–1135'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn等人（2017）Finn C, Abbeel P, Levine S（2017）“面向快速适应深度网络的模型无关元学习。”见于《国际机器学习会议》，PMLR，第1126–1135页。
- en: 'Fung et al. (2016) Fung P, Dey A, Siddique FB, Lin R, Yang Y, Bertero D, Wan
    Y, Chan RHY, Wu CS (2016) Zara: a virtual interactive dialogue system incorporating
    emotion, sentiment and personality recognition. In: Proceedings of COLING 2016,
    the 26th International Conference on Computational Linguistics: System Demonstrations,
    pp 278–281'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fung等人（2016）Fung P, Dey A, Siddique FB, Lin R, Yang Y, Bertero D, Wan Y, Chan
    RHY, Wu CS（2016）“Zara：一个结合情感、情绪和个性识别的虚拟互动对话系统。”见于《COLING 2016：第26届计算语言学国际会议：系统演示》，第278–281页。
- en: 'Galley et al. (2015) Galley M, Brockett C, Sordoni A, Ji Y, Auli M, Quirk C,
    Mitchell M, Gao J, Dolan B (2015) deltableu: A discriminative metric for generation
    tasks with intrinsically diverse targets. arXiv preprint arXiv:150606863'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Galley等人（2015）Galley M, Brockett C, Sordoni A, Ji Y, Auli M, Quirk C, Mitchell
    M, Gao J, Dolan B（2015）“deltableu：一种用于生成任务的判别性指标，具有内在多样的目标。”arXiv预印本arXiv:150606863。
- en: Gan et al. (2019) Gan Z, Cheng Y, Kholy AE, Li L, Liu J, Gao J (2019) Multi-step
    reasoning via recurrent dual attention for visual dialog. arXiv preprint arXiv:190200579
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan等人（2019）Gan Z, Cheng Y, Kholy AE, Li L, Liu J, Gao J（2019）“通过递归双重注意力进行多步推理用于视觉对话。”arXiv预印本arXiv:190200579。
- en: Gan et al. (2020) Gan Z, Chen YC, Li L, Zhu C, Cheng Y, Liu J (2020) Large-scale
    adversarial training for vision-and-language representation learning. arXiv preprint
    arXiv:200606195
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gan等人（2020）Gan Z, Chen YC, Li L, Zhu C, Cheng Y, Liu J（2020）“大规模对抗训练用于视觉和语言表示学习。”arXiv预印本arXiv:200606195。
- en: 'Gangadharaiah and Narayanaswamy (2020) Gangadharaiah R, Narayanaswamy B (2020)
    Recursive template-based frame generation for task oriented dialog. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    2059–2064'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gangadharaiah和Narayanaswamy（2020）Gangadharaiah R, Narayanaswamy B（2020）“基于递归模板的框架生成用于任务导向对话。”见于《第58届计算语言学协会年会论文集》，第2059–2064页。
- en: 'Gao et al. (2018) Gao J, Galley M, Li L (2018) Neural approaches to conversational
    ai. In: The 41st International ACM SIGIR Conference on Research & Development
    in Information Retrieval, pp 1371–1374'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2018）Gao J, Galley M, Li L（2018）“对话AI的神经方法。”见于《第41届国际ACM SIGIR信息检索研究与开发会议》，第1371–1374页。
- en: Gao et al. (2020a) Gao S, Zhang Y, Ou Z, Yu Z (2020a) Paraphrase augmented task-oriented
    dialog generation. arXiv preprint arXiv:200407462
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2020a）Gao S, Zhang Y, Ou Z, Yu Z（2020a）“通过释义增强的任务导向对话生成。”arXiv预印本arXiv:200407462。
- en: Gao et al. (2019) Gao X, Zhang Y, Lee S, Galley M, Brockett C, Gao J, Dolan
    B (2019) Structuring latent spaces for stylized response generation. arXiv preprint
    arXiv:190905361
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2019）Gao X, Zhang Y, Lee S, Galley M, Brockett C, Gao J, Dolan B（2019）“用于风格化响应生成的潜在空间结构化。”arXiv预印本arXiv:190905361。
- en: Gao et al. (2020b) Gao X, Zhang Y, Galley M, Brockett C, Dolan B (2020b) Dialogue
    response ranking training with large-scale human feedback data. arXiv preprint
    arXiv:200906978
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2020b）Gao X, Zhang Y, Galley M, Brockett C, Dolan B（2020b）“对话响应排序训练与大规模人类反馈数据。”arXiv预印本arXiv:200906978。
- en: 'Gao et al. (2020c) Gao Y, Wu CS, Joty S, Xiong C, Socher R, King I, Lyu M,
    Hoi SC (2020c) Explicit memory tracker with coarse-to-fine reasoning for conversational
    machine reading. In: Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics, pp 935–945'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2020c）Gao Y, Wu CS, Joty S, Xiong C, Socher R, King I, Lyu M, Hoi SC（2020c）“具有粗到细推理的显式记忆跟踪器用于对话机器阅读。”见于《第58届计算语言学协会年会论文集》，第935–945页。
- en: 'Gehring et al. (2017) Gehring J, Auli M, Grangier D, Yarats D, Dauphin YN (2017)
    Convolutional sequence to sequence learning. In: International Conference on Machine
    Learning, PMLR, pp 1243–1252'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehring 等 (2017) Gehring J, Auli M, Grangier D, Yarats D, Dauphin YN (2017)
    卷积序列到序列学习。在：国际机器学习会议，PMLR，第1243–1252页
- en: 'Ghazvininejad et al. (2018) Ghazvininejad M, Brockett C, Chang MW, Dolan B,
    Gao J, Yih Wt, Galley M (2018) A knowledge-grounded neural conversation model.
    In: Proceedings of the AAAI Conference on Artificial Intelligence, vol 32'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghazvininejad 等 (2018) Ghazvininejad M, Brockett C, Chang MW, Dolan B, Gao J,
    Yih Wt, Galley M (2018) 一个基于知识的神经对话模型。在：AAAI 人工智能会议论文集，第32卷
- en: 'Gliwa et al. (2019) Gliwa B, Mochol I, Biesek M, Wawer A (2019) Samsum corpus:
    A human-annotated dialogue dataset for abstractive summarization. arXiv preprint
    arXiv:191112237'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gliwa 等 (2019) Gliwa B, Mochol I, Biesek M, Wawer A (2019) Samsum 语料库：一个用于抽象总结的人类注释对话数据集。arXiv
    预印本 arXiv:191112237
- en: 'Goddeau et al. (1996) Goddeau D, Meng H, Polifroni J, Seneff S, Busayapongchai
    S (1996) A form-based dialogue manager for spoken language applications. In: Proceeding
    of Fourth International Conference on Spoken Language Processing. ICSLP’96, IEEE,
    vol 2, pp 701–704'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goddeau 等 (1996) Goddeau D, Meng H, Polifroni J, Seneff S, Busayapongchai S
    (1996) 用于口语语言应用的基于表单的对话管理器。在：第四届国际口语语言处理会议论文集。ICSLP’96，IEEE，第2卷，第701–704页
- en: 'Golovanov et al. (2019) Golovanov S, Kurbanov R, Nikolenko S, Truskovskyi K,
    Tselousov A, Wolf T (2019) Large-scale transfer learning for natural language
    generation. In: Proceedings of the 57th Annual Meeting of the Association for
    Computational Linguistics, pp 6053–6058'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golovanov 等 (2019) Golovanov S, Kurbanov R, Nikolenko S, Truskovskyi K, Tselousov
    A, Wolf T (2019) 用于自然语言生成的大规模迁移学习。在：第57届计算语言学协会年会论文集，第6053–6058页
- en: Goodfellow et al. (2014) Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial networks. arXiv
    preprint arXiv:14062661
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) 生成对抗网络。arXiv 预印本 arXiv:14062661
- en: 'Gopalakrishnan et al. (2019) Gopalakrishnan K, Hedayatnia B, Chen Q, Gottardi
    A, Kwatra S, Venkatesh A, Gabriel R, Hakkani-Tür D, AI AA (2019) Topical-chat:
    Towards knowledge-grounded open-domain conversations. In: INTERSPEECH, pp 1891–1895'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gopalakrishnan 等 (2019) Gopalakrishnan K, Hedayatnia B, Chen Q, Gottardi A,
    Kwatra S, Venkatesh A, Gabriel R, Hakkani-Tür D, AI AA (2019) Topical-chat：面向知识驱动的开放域对话。在：INTERSPEECH，第1891–1895页
- en: Gordon-Hall et al. (2020) Gordon-Hall G, Gorinski PJ, Cohen SB (2020) Learning
    dialog policies from weak demonstrations. arXiv preprint arXiv:200411054
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gordon-Hall 等 (2020) Gordon-Hall G, Gorinski PJ, Cohen SB (2020) 从弱演示中学习对话策略。arXiv
    预印本 arXiv:200411054
- en: Graves et al. (2014) Graves A, Wayne G, Danihelka I (2014) Neural turing machines.
    arXiv preprint arXiv:14105401
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 等 (2014) Graves A, Wayne G, Danihelka I (2014) 神经图灵机。arXiv 预印本 arXiv:14105401
- en: Graves et al. (2016) Graves A, Wayne G, Reynolds M, Harley T, Danihelka I, Grabska-Barwińska
    A, Colmenarejo SG, Grefenstette E, Ramalho T, Agapiou J, et al. (2016) Hybrid
    computing using a neural network with dynamic external memory. Nature 538(7626):471–476
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 等 (2016) Graves A, Wayne G, Reynolds M, Harley T, Danihelka I, Grabska-Barwińska
    A, Colmenarejo SG, Grefenstette E, Ramalho T, Agapiou J, 等 (2016) 使用具有动态外部记忆的神经网络进行混合计算。《自然》538(7626):471–476
- en: Gruber and Jockisch (2020) Gruber N, Jockisch A (2020) Are gru cells more specific
    and lstm cells more sensitive in motive classification of text? Frontiers in Artificial
    Intelligence 3(40):1–6
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gruber 和 Jockisch (2020) Gruber N, Jockisch A (2020) GRU 单元在文本动机分类中是否更具特异性，而
    LSTM 单元是否更具敏感性？《人工智能前沿》3(40):1–6
- en: 'Gu et al. (2016) Gu J, Lu Z, Li H, Li VO (2016) Incorporating copying mechanism
    in sequence-to-sequence learning. In: Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), Association
    for Computational Linguistics, Berlin, Germany, pp 1631–1640, DOI 10.18653/v1/P16-1154,
    URL [https://www.aclweb.org/anthology/P16-1154](https://www.aclweb.org/anthology/P16-1154)'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 (2016) Gu J, Lu Z, Li H, Li VO (2016) 在序列到序列学习中结合复制机制。在：第54届计算语言学协会年会论文集（第1卷：长篇论文），计算语言学协会，德国柏林，第1631–1640页，DOI
    10.18653/v1/P16-1154，网址 [https://www.aclweb.org/anthology/P16-1154](https://www.aclweb.org/anthology/P16-1154)
- en: Guo et al. (2019) Guo Q, Qiu X, Liu P, Shao Y, Xue X, Zhang Z (2019) Star-transformer.
    arXiv preprint arXiv:190209113
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 (2019) Guo Q, Qiu X, Liu P, Shao Y, Xue X, Zhang Z (2019) 星形变换器。arXiv
    预印本 arXiv:190209113
- en: Guo et al. (2020) Guo X, Yu M, Gao Y, Gan C, Campbell M, Chang S (2020) Interactive
    fiction game playing as multi-paragraph reading comprehension with reinforcement
    learning. arXiv preprint arXiv:201002386
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2020）Guo X, Yu M, Gao Y, Gan C, Campbell M, Chang S（2020）《将互动小说游戏作为多段落阅读理解与强化学习》。arXiv
    预印本 arXiv:201002386
- en: 'Gür et al. (2018) Gür I, Hakkani-Tür D, Tür G, Shah P (2018) User modeling
    for task oriented dialogues. In: 2018 IEEE Spoken Language Technology Workshop
    (SLT), IEEE, pp 900–906'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gür 等人（2018）Gür I, Hakkani-Tür D, Tür G, Shah P（2018）《任务导向对话的用户建模》。发表于2018年IEEE语言技术研讨会（SLT），IEEE，页
    900–906
- en: 'Haber et al. (2019) Haber J, Baumgärtner T, Takmaz E, Gelderloos L, Bruni E,
    Fernández R (2019) The photobook dataset: Building common ground through visually-grounded
    dialogue. arXiv preprint arXiv:190601530'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Haber 等人（2019）Haber J, Baumgärtner T, Takmaz E, Gelderloos L, Bruni E, Fernández
    R（2019）《照片书数据集：通过视觉基础对话建立共同基础》。arXiv 预印本 arXiv:190601530
- en: Hahn et al. (2020) Hahn M, Krantz J, Batra D, Parikh D, Rehg JM, Lee S, Anderson
    P (2020) Where are you? localization from embodied dialog. arXiv preprint arXiv:201108277
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahn 等人（2020）Hahn M, Krantz J, Batra D, Parikh D, Rehg JM, Lee S, Anderson P（2020）《你在哪里？
    从具身对话中进行定位》。arXiv 预印本 arXiv:201108277
- en: 'Hakkani-Tür et al. (2016) Hakkani-Tür D, Tür G, Celikyilmaz A, Chen YN, Gao
    J, Deng L, Wang YY (2016) Multi-domain joint semantic frame parsing using bi-directional
    rnn-lstm. In: Interspeech, pp 715–719'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hakkani-Tür 等人（2016）Hakkani-Tür D, Tür G, Celikyilmaz A, Chen YN, Gao J, Deng
    L, Wang YY（2016）《使用双向 rnn-lstm 的多领域联合语义框架解析》。发表于 Interspeech，页 715–719
- en: 'Ham et al. (2020) Ham D, Lee JG, Jang Y, Kim KE (2020) End-to-end neural pipeline
    for goal-oriented dialogue systems using gpt-2\. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 583–592'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ham 等人（2020）Ham D, Lee JG, Jang Y, Kim KE（2020）《用于目标导向对话系统的端到端神经管道，使用 gpt-2》。发表于第58届计算语言学协会年会论文集，页
    583–592
- en: 'Han et al. (2019) Han M, Kang M, Jung H, Hwang SJ (2019) Episodic memory reader:
    Learning what to remember for question answering from streaming data. arXiv preprint
    arXiv:190306164'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人（2019）Han M, Kang M, Jung H, Hwang SJ（2019）《情节记忆阅读器：从流数据中学习记住什么以回答问题》。arXiv
    预印本 arXiv:190306164
- en: 'Hancock et al. (2019) Hancock B, Bordes A, Mazare PE, Weston J (2019) Learning
    from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:190105415'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hancock 等人（2019）Hancock B, Bordes A, Mazare PE, Weston J（2019）《部署后的对话学习：喂自己，聊天机器人！》arXiv
    预印本 arXiv:190105415
- en: 'Hashemi et al. (2016) Hashemi HB, Asiaee A, Kraft R (2016) Query intent detection
    using convolutional neural networks. In: International Conference on Web Search
    and Data Mining, Workshop on Query Understanding'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hashemi 等人（2016）Hashemi HB, Asiaee A, Kraft R（2016）《使用卷积神经网络的查询意图检测》。发表于国际网络搜索与数据挖掘会议，查询理解研讨会
- en: He et al. (2017) He H, Balakrishnan A, Eric M, Liang P (2017) Learning symmetric
    collaborative dialogue agents with dynamic knowledge graph embeddings. arXiv preprint
    arXiv:170407130
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2017）He H, Balakrishnan A, Eric M, Liang P（2017）《使用动态知识图嵌入学习对称协作对话代理》。arXiv
    预印本 arXiv:170407130
- en: 'He et al. (2016) He K, Zhang X, Ren S, Sun J (2016) Deep residual learning
    for image recognition. In: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp 770–778'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2016）He K, Zhang X, Ren S, Sun J（2016）《用于图像识别的深度残差学习》。发表于 IEEE 计算机视觉与模式识别会议论文集，页
    770–778
- en: He and Glass (2019) He T, Glass J (2019) Negative training for neural dialogue
    response generation. arXiv preprint arXiv:190302134
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 和 Glass（2019）He T, Glass J（2019）《神经对话响应生成的负训练》。arXiv 预印本 arXiv:190302134
- en: 'He et al. (2020a) He W, Yang M, Yan R, Li C, Shen Y, Xu R (2020a) Amalgamating
    knowledge from two teachers for task-oriented dialogue system with adversarial
    training. In: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pp 3498–3507'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2020a）He W, Yang M, Yan R, Li C, Shen Y, Xu R（2020a）《通过对抗训练将两个教师的知识融合用于任务导向对话系统》。发表于2020年自然语言处理实证方法会议（EMNLP）论文集，页
    3498–3507
- en: 'He et al. (2020b) He X, Chen S, Ju Z, Dong X, Fang H, Wang S, Yang Y, Zeng
    J, Zhang R, Zhang R, et al. (2020b) Meddialog: Two large-scale medical dialogue
    datasets. arXiv e-prints pp arXiv–2004'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人（2020b）He X, Chen S, Ju Z, Dong X, Fang H, Wang S, Yang Y, Zeng J, Zhang
    R, Zhang R 等（2020b）《Meddialog：两个大规模医学对话数据集》。arXiv 电子预印本 pp arXiv–2004
- en: Henderson et al. (2008) Henderson J, Lemon O, Georgila K (2008) Hybrid reinforcement/supervised
    learning of dialogue policies from fixed data sets. Computational Linguistics
    34(4):487–511
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等人（2008）Henderson J, Lemon O, Georgila K（2008）《从固定数据集中学习对话策略的混合强化/监督学习》。计算语言学
    34(4):487–511
- en: 'Henderson (2015) Henderson M (2015) Machine learning for dialog state tracking:
    A review. In: Proceedings of The First International Workshop on Machine Learning
    in Spoken Language Processing'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Henderson (2015) Henderson M (2015) 对话状态追踪的机器学习：综述。载于: 第一届国际口语语言处理中的机器学习研讨会论文集'
- en: 'Henderson et al. (2013) Henderson M, Thomson B, Young S (2013) Deep neural
    network approach for the dialog state tracking challenge. In: Proceedings of the
    SIGDIAL 2013 Conference, pp 467–471'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Henderson 等 (2013) Henderson M, Thomson B, Young S (2013) 用于对话状态追踪挑战的深度神经网络方法。载于:
    SIGDIAL 2013 会议论文集，第467–471页'
- en: 'Henderson et al. (2014a) Henderson M, Thomson B, Williams JD (2014a) The second
    dialog state tracking challenge. In: Proceedings of the 15th annual meeting of
    the special interest group on discourse and dialogue (SIGDIAL), pp 263–272'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Henderson 等 (2014a) Henderson M, Thomson B, Williams JD (2014a) 第二届对话状态追踪挑战赛。载于:
    第15届话语与对话特别兴趣小组年会 (SIGDIAL) 论文集，第263–272页'
- en: 'Henderson et al. (2014b) Henderson M, Thomson B, Williams JD (2014b) The third
    dialog state tracking challenge. In: 2014 IEEE Spoken Language Technology Workshop
    (SLT), IEEE, pp 324–329'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Henderson 等 (2014b) Henderson M, Thomson B, Williams JD (2014b) 第三届对话状态追踪挑战赛。载于:
    2014 IEEE 口语语言技术研讨会 (SLT)，IEEE，第324–329页'
- en: Henderson et al. (2019a) Henderson M, Budzianowski P, Casanueva I, Coope S,
    Gerz D, Kumar G, Mrkšić N, Spithourakis G, Su PH, Vulić I, et al. (2019a) A repository
    of conversational datasets. arXiv preprint arXiv:190406472
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等 (2019a) Henderson M, Budzianowski P, Casanueva I, Coope S, Gerz
    D, Kumar G, Mrkšić N, Spithourakis G, Su PH, Vulić I, 等 (2019a) 对话数据集的存储库。arXiv
    预印本 arXiv:190406472
- en: Henderson et al. (2019b) Henderson M, Vulić I, Gerz D, Casanueva I, Budzianowski
    P, Coope S, Spithourakis G, Wen TH, Mrkšić N, Su PH (2019b) Training neural response
    selection for task-oriented dialogue systems. arXiv preprint arXiv:190601543
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson 等 (2019b) Henderson M, Vulić I, Gerz D, Casanueva I, Budzianowski
    P, Coope S, Spithourakis G, Wen TH, Mrkšić N, Su PH (2019b) 训练用于任务导向对话系统的神经响应选择。arXiv
    预印本 arXiv:190601543
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. Neural computation 9(8):1735–1780
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) 长短期记忆。神经计算
    9(8):1735–1780
- en: 'Hochreiter et al. (2001) Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J,
    et al. (2001) Gradient flow in recurrent nets: the difficulty of learning long-term
    dependencies'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 等 (2001) Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J, 等 (2001)
    递归网络中的梯度流：学习长期依赖的困难
- en: Hokamp and Liu (2017) Hokamp C, Liu Q (2017) Lexically constrained decoding
    for sequence generation using grid beam search. arXiv preprint arXiv:170407138
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hokamp 和 Liu (2017) Hokamp C, Liu Q (2017) 使用网格束搜索的词汇约束解码生成序列。arXiv 预印本 arXiv:170407138
- en: Hopfield (1982) Hopfield JJ (1982) Neural networks and physical systems with
    emergent collective computational abilities. Proceedings of the national academy
    of sciences 79(8):2554–2558
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield (1982) Hopfield JJ (1982) 神经网络和具有涌现集体计算能力的物理系统。国家科学院学报 79(8):2554–2558
- en: Hosseini-Asl et al. (2020) Hosseini-Asl E, McCann B, Wu CS, Yavuz S, Socher
    R (2020) A simple language model for task-oriented dialogue. arXiv preprint arXiv:200500796
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini-Asl 等 (2020) Hosseini-Asl E, McCann B, Wu CS, Yavuz S, Socher R (2020)
    一种简单的任务导向对话语言模型。arXiv 预印本 arXiv:200500796
- en: 'Hu et al. (2020) Hu J, Yang Y, Chen C, Yu Z, et al. (2020) Sas: Dialogue state
    tracking via slot attention and slot information sharing. In: Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, pp 6366–6375'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 (2020) Hu J, Yang Y, Chen C, Yu Z, 等 (2020) SAS: 通过槽注意力和槽信息共享进行对话状态追踪。载于:
    第58届计算语言学协会年会论文集，第6366–6375页'
- en: 'Hu et al. (2019) Hu JE, Rudinger R, Post M, Van Durme B (2019) Parabank: Monolingual
    bitext generation and sentential paraphrasing via lexically-constrained neural
    machine translation. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 33, pp 6521–6528'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 (2019) Hu JE, Rudinger R, Post M, Van Durme B (2019) Parabank: 通过词汇约束神经机器翻译生成单语双语文本和句子改写。载于:
    AAAI 人工智能会议论文集，第33卷，第6521–6528页'
- en: 'Hu et al. (2017) Hu Z, Yang Z, Liang X, Salakhutdinov R, Xing EP (2017) Toward
    controlled generation of text. In: International Conference on Machine Learning,
    PMLR, pp 1587–1596'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 (2017) Hu Z, Yang Z, Liang X, Salakhutdinov R, Xing EP (2017) 朝向受控文本生成。载于:
    国际机器学习会议，PMLR，第1587–1596页'
- en: Hua and Wang (2019) Hua X, Wang L (2019) Sentence-level content planning and
    style specification for neural text generation. arXiv preprint arXiv:190900734
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua 和 Wang (2019) Hua X, Wang L (2019) 神经文本生成的句子级内容规划和风格规范。arXiv 预印本 arXiv:190900734
- en: Hua et al. (2020) Hua Y, Li YF, Haffari G, Qi G, Wu T (2020) Few-shot complex
    knowledge base question answering via meta reinforcement learning. arXiv preprint
    arXiv:201015877
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua 等（2020）Hua Y, Li YF, Haffari G, Qi G, Wu T（2020）《通过元强化学习进行少量复杂知识库问答》。arXiv
    预印本 arXiv:201015877
- en: 'Huang et al. (2020a) Huang L, Ye Z, Qin J, Lin L, Liang X (2020a) Grade: Automatic
    graph-enhanced coherence metric for evaluating open-domain dialogue systems. arXiv
    preprint arXiv:201003994'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2020a）Huang L, Ye Z, Qin J, Lin L, Liang X（2020a）《Grade：用于评估开放域对话系统的自动图增强一致性度量》。arXiv
    预印本 arXiv:201003994
- en: 'Huang et al. (2018) Huang X, Jiang J, Zhao D, Feng Y, Hong Y (2018) Natural
    Language Processing and Chinese Computing: 6th CCF International Conference, NLPCC
    2017, Dalian, China, November 8–12, 2017, Proceedings, vol 10619\. Springer'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2018）Huang X, Jiang J, Zhao D, Feng Y, Hong Y（2018）《自然语言处理与中文计算：第6届CCF国际会议，NLPCC
    2017，大连，中国，2017年11月8–12日，会议论文集，第10619卷》。Springer
- en: Huang et al. (2020b) Huang X, Qi J, Sun Y, Zhang R (2020b) Semi-supervised dialogue
    policy learning via stochastic reward estimation. arXiv preprint arXiv:200504379
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2020b）Huang X, Qi J, Sun Y, Zhang R（2020b）《通过随机奖励估计进行半监督对话策略学习》。arXiv
    预印本 arXiv:200504379
- en: 'Huang et al. (2020c) Huang Y, Feng J, Hu M, Wu X, Du X, Ma S (2020c) Meta-reinforced
    multi-domain state generator for dialogue systems. In: Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics, pp 7109–7118'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2020c）Huang Y, Feng J, Hu M, Wu X, Du X, Ma S（2020c）《用于对话系统的元强化多领域状态生成器》。在：第58届计算语言学协会年会论文集，第7109–7118页
- en: 'Huang et al. (2020d) Huang Z, Zeng Z, Liu B, Fu D, Fu J (2020d) Pixel-bert:
    Aligning image pixels with text by deep multi-modal transformers. arXiv preprint
    arXiv:200400849'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2020d）Huang Z, Zeng Z, Liu B, Fu D, Fu J（2020d）《Pixel-bert：通过深度多模态变换器对齐图像像素与文本》。arXiv
    预印本 arXiv:200400849
- en: Jaderberg et al. (2016) Jaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo JZ,
    Silver D, Kavukcuoglu K (2016) Reinforcement learning with unsupervised auxiliary
    tasks. arXiv preprint arXiv:161105397
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaderberg 等（2016）Jaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo JZ, Silver
    D, Kavukcuoglu K（2016）《具有无监督辅助任务的强化学习》。arXiv 预印本 arXiv:161105397
- en: 'Jang et al. (2017) Jang Y, Song Y, Yu Y, Kim Y, Kim G (2017) Tgif-qa: Toward
    spatio-temporal reasoning in visual question answering. In: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp 2758–2766'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等（2017）Jang Y, Song Y, Yu Y, Kim Y, Kim G（2017）《Tgif-qa：面向视觉问答中的时空推理》。在：IEEE
    计算机视觉与模式识别会议论文集，第2758–2766页
- en: Jaques et al. (2020) Jaques N, Shen JH, Ghandeharioun A, Ferguson C, Lapedriza
    A, Jones N, Gu SS, Picard R (2020) Human-centric dialog training via offline reinforcement
    learning. arXiv preprint arXiv:201005848
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaques 等（2020）Jaques N, Shen JH, Ghandeharioun A, Ferguson C, Lapedriza A, Jones
    N, Gu SS, Picard R（2020）《通过离线强化学习进行以人为中心的对话训练》。arXiv 预印本 arXiv:201005848
- en: Ji et al. (2020) Ji C, Zhou X, Zhang Y, Liu X, Sun C, Zhu C, Zhao T (2020) Cross
    copy network for dialogue generation. arXiv preprint arXiv:201011539
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（2020）Ji C, Zhou X, Zhang Y, Liu X, Sun C, Zhu C, Zhao T（2020）《用于对话生成的交叉复制网络》。arXiv
    预印本 arXiv:201011539
- en: 'Ji et al. (2015) Ji G, He S, Xu L, Liu K, Zhao J (2015) Knowledge graph embedding
    via dynamic mapping matrix. In: Proceedings of the 53rd annual meeting of the
    association for computational linguistics and the 7th international joint conference
    on natural language processing (volume 1: Long papers), pp 687–696'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（2015）Ji G, He S, Xu L, Liu K, Zhao J（2015）《通过动态映射矩阵进行知识图谱嵌入》。在：第53届计算语言学协会年会和第7届国际自然语言处理联合会议论文集（第1卷：长篇论文），第687–696页
- en: 'Ji et al. (2022) Ji S, Pan S, Cambria E, Marttinen P, Yu PS (2022) A survey
    on knowledge graphs: Representation, acquisition and applications. IEEE Transactions
    on Neural Networks and Learning Systems 33(10)'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等（2022）Ji S, Pan S, Cambria E, Marttinen P, Yu PS（2022）《关于知识图谱的调查：表示、获取与应用》。IEEE
    神经网络与学习系统汇刊 33（10）
- en: Jia et al. (2020) Jia Q, Liu Y, Ren S, Zhu KQ, Tang H (2020) Multi-turn response
    selection using dialogue dependency relations. arXiv preprint arXiv:201001502
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等（2020）Jia Q, Liu Y, Ren S, Zhu KQ, Tang H（2020）《基于对话依赖关系的多轮响应选择》。arXiv
    预印本 arXiv:201001502
- en: 'Jordan (1986) Jordan M (1986) Serial order: a parallel distributed processing
    approach. technical report, june 1985-march 1986\. Tech. rep., California Univ.,
    San Diego, La Jolla (USA). Inst. for Cognitive Science'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan（1986）Jordan M（1986）《序列顺序：一种并行分布处理方法》。技术报告，1985年6月-1986年3月。加州大学，圣地亚哥，拉荷亚（美国）。认知科学研究所
- en: 'Jung et al. (2020) Jung J, Son B, Lyu S (2020) Attnio: Knowledge graph exploration
    with in-and-out attention flow for knowledge-grounded dialogue. In: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pp 3484–3497'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung et al. (2020) Jung J, Son B, Lyu S (2020) Attnio：通过内外注意流探索知识图谱以实现知识驱动对话。载于《2020年自然语言处理经验方法会议（EMNLP）论文集》，页
    3484–3497
- en: Jurafsky (1997) Jurafsky D (1997) Switchboard swbd-damsl shallow-discourse-function
    annotation coders manual. Institute of Cognitive Science Technical Report
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jurafsky (1997) Jurafsky D (1997) Switchboard swbd-damsl 浅层话语功能注释编码员手册。认知科学研究所技术报告
- en: 'Kale and Rastogi (2020) Kale M, Rastogi A (2020) Template guided text generation
    for task-oriented dialogue. In: Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP), Association for Computational
    Linguistics, Online, pp 6505–6520, DOI 10.18653/v1/2020.emnlp-main.527, URL [https://www.aclweb.org/anthology/2020.emnlp-main.527](https://www.aclweb.org/anthology/2020.emnlp-main.527)'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kale and Rastogi (2020) Kale M, Rastogi A (2020) 任务导向对话的模板引导文本生成。载于《2020年自然语言处理经验方法会议（EMNLP）论文集》，计算语言学协会，在线，页
    6505–6520, DOI 10.18653/v1/2020.emnlp-main.527, URL [https://www.aclweb.org/anthology/2020.emnlp-main.527](https://www.aclweb.org/anthology/2020.emnlp-main.527)
- en: 'Kamezawa et al. (2020) Kamezawa H, Nishida N, Shimizu N, Miyazaki T, Nakayama
    H (2020) A visually-grounded first-person dialogue dataset with verbal and non-verbal
    responses. In: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pp 3299–3310'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamezawa et al. (2020) Kamezawa H, Nishida N, Shimizu N, Miyazaki T, Nakayama
    H (2020) 带有言语和非言语响应的视觉基础第一人称对话数据集。载于《2020年自然语言处理经验方法会议（EMNLP）论文集》，页 3299–3310
- en: Kannan and Vinyals (2017) Kannan A, Vinyals O (2017) Adversarial evaluation
    of dialogue models. arXiv preprint arXiv:170108198
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kannan and Vinyals (2017) Kannan A, Vinyals O (2017) 对话模型的对抗性评估。arXiv 预印本 arXiv:170108198
- en: 'Keskar et al. (2019) Keskar NS, McCann B, Varshney LR, Xiong C, Socher R (2019)
    Ctrl: A conditional transformer language model for controllable generation. arXiv
    preprint arXiv:190905858'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keskar et al. (2019) Keskar NS, McCann B, Varshney LR, Xiong C, Socher R (2019)
    Ctrl：一种用于可控生成的条件变换器语言模型。arXiv 预印本 arXiv:190905858
- en: Kim et al. (2018) Kim A, Song HJ, Park SB, et al. (2018) A two-step neural dialog
    state tracker for task-oriented dialog processing. Computational intelligence
    and neuroscience 2018
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2018) Kim A, Song HJ, Park SB, 等 (2018) 一种用于任务导向对话处理的两步神经对话状态跟踪器。《计算智能与神经科学》2018
- en: 'Kim et al. (2020) Kim H, Kim B, Kim G (2020) Will i sound like me? improving
    persona consistency in dialogues through pragmatic self-consciousness. In: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pp 904–916'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2020) Kim H, Kim B, Kim G (2020) 我会听起来像我吗？通过实用自我意识改善对话中的个性一致性。载于《2020年自然语言处理经验方法会议（EMNLP）论文集》，页
    904–916
- en: 'Kim et al. (2016) Kim S, D’Haro LF, Banchs RE, Williams JD, Henderson M, Yoshino
    K (2016) The fifth dialog state tracking challenge. In: 2016 IEEE Spoken Language
    Technology Workshop (SLT), IEEE, pp 511–517'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2016) Kim S, D’Haro LF, Banchs RE, Williams JD, Henderson M, Yoshino
    K (2016) 第五届对话状态追踪挑战赛。载于《2016年IEEE语音语言技术研讨会（SLT）》，IEEE，页 511–517
- en: 'Kim et al. (2017) Kim S, D’Haro LF, Banchs RE, Williams JD, Henderson M (2017)
    The fourth dialog state tracking challenge. In: Dialogues with Social Robots,
    Springer, pp 435–449'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2017) Kim S, D’Haro LF, Banchs RE, Williams JD, Henderson M (2017)
    第四届对话状态追踪挑战赛。载于《与社交机器人对话》，Springer，页 435–449
- en: Kim et al. (2019) Kim S, Yang S, Kim G, Lee SW (2019) Efficient dialogue state
    tracking by selectively overwriting memory. arXiv preprint arXiv:191103906
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2019) Kim S, Yang S, Kim G, Lee SW (2019) 通过选择性覆盖记忆实现高效的对话状态追踪。arXiv
    预印本 arXiv:191103906
- en: Ko et al. (2020) Ko WJ, Ray A, Shen Y, Jin H (2020) Generating dialogue responses
    from a semantic latent space. arXiv preprint arXiv:201001658
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko et al. (2020) Ko WJ, Ray A, Shen Y, Jin H (2020) 从语义潜在空间生成对话响应。arXiv 预印本
    arXiv:201001658
- en: 'Konda and Tsitsiklis (2000) Konda VR, Tsitsiklis JN (2000) Actor-critic algorithms.
    In: Advances in neural information processing systems, Citeseer, pp 1008–1014'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konda and Tsitsiklis (2000) Konda VR, Tsitsiklis JN (2000) 演员-评论家算法。载于《神经信息处理系统进展》，Citeseer，页
    1008–1014
- en: Krizhevsky et al. (2012) Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet
    classification with deep convolutional neural networks. Advances in neural information
    processing systems 25:1097–1105
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2012) Krizhevsky A, Sutskever I, Hinton GE (2012) 基于深度卷积神经网络的Imagenet分类。《神经信息处理系统进展》25:1097–1105
- en: Kummerfeld et al. (2018) Kummerfeld JK, Gouravajhala SR, Peper J, Athreya V,
    Gunasekara C, Ganhotra J, Patel SS, Polymenakos L, Lasecki WS (2018) A large-scale
    corpus for conversation disentanglement. arXiv preprint arXiv:181011118
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kummerfeld 等 (2018) Kummerfeld JK, Gouravajhala SR, Peper J, Athreya V, Gunasekara
    C, Ganhotra J, Patel SS, Polymenakos L, Lasecki WS (2018) 用于对话解缠结的大规模语料库。arXiv
    预印本 arXiv:181011118
- en: 'Kundu et al. (2020) Kundu S, Lin Q, Ng HT (2020) Learning to identify follow-up
    questions in conversational question answering. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 959–968'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kundu 等 (2020) Kundu S, Lin Q, Ng HT (2020) 学习识别会话问答中的后续问题。载于：第 58 届计算语言学协会年会论文集，第
    959–968 页
- en: Kurach et al. (2015) Kurach K, Andrychowicz M, Sutskever I (2015) Neural random-access
    machines. arXiv preprint arXiv:151106392
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurach 等 (2015) Kurach K, Andrychowicz M, Sutskever I (2015) 神经随机访问机器。arXiv
    预印本 arXiv:151106392
- en: Larson et al. (2019) Larson S, Mahendran A, Peper JJ, Clarke C, Lee A, Hill
    P, Kummerfeld JK, Leach K, Laurenzano MA, Tang L, et al. (2019) An evaluation
    dataset for intent classification and out-of-scope prediction. arXiv preprint
    arXiv:190902027
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Larson 等 (2019) Larson S, Mahendran A, Peper JJ, Clarke C, Lee A, Hill P, Kummerfeld
    JK, Leach K, Laurenzano MA, Tang L, 等 (2019) 意图分类和超出范围预测的评估数据集。arXiv 预印本 arXiv:190902027
- en: Le and Hoi (2020) Le H, Hoi SC (2020) Video-grounded dialogues with pretrained
    generation language models. arXiv preprint arXiv:200615319
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 和 Hoi (2020) Le H, Hoi SC (2020) 使用预训练生成语言模型的视频基础对话。arXiv 预印本 arXiv:200615319
- en: Le et al. (2019) Le H, Sahoo D, Chen NF, Hoi SC (2019) Multimodal transformer
    networks for end-to-end video-grounded dialogue systems. arXiv preprint arXiv:190701166
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等 (2019) Le H, Sahoo D, Chen NF, Hoi SC (2019) 用于端到端视频基础对话系统的多模态 Transformer
    网络。arXiv 预印本 arXiv:190701166
- en: 'Le et al. (2020a) Le H, Sahoo D, Chen NF, Hoi SC (2020a) Bist: Bi-directional
    spatio-temporal reasoning for video-grounded dialogues. arXiv preprint arXiv:201010095'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等 (2020a) Le H, Sahoo D, Chen NF, Hoi SC (2020a) Bist：视频基础对话的双向时空推理。arXiv
    预印本 arXiv:201010095
- en: 'Le et al. (2020b) Le H, Sahoo D, Liu C, Chen NF, Hoi SC (2020b) Uniconv: A
    unified conversational neural architecture for multi-domain task-oriented dialogues.
    arXiv preprint arXiv:200414307'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等 (2020b) Le H, Sahoo D, Liu C, Chen NF, Hoi SC (2020b) Uniconv：一种统一的对话神经架构用于多领域任务导向对话。arXiv
    预印本 arXiv:200414307
- en: LeCun et al. (1998) LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based
    learning applied to document recognition. Proceedings of the IEEE 86(11):2278–2324
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (1998) LeCun Y, Bottou L, Bengio Y, Haffner P (1998) 基于梯度的学习应用于文档识别。IEEE
    会议录 86(11):2278–2324
- en: Lee and Dernoncourt (2016) Lee JY, Dernoncourt F (2016) Sequential short-text
    classification with recurrent and convolutional neural networks. arXiv preprint
    arXiv:160303827
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 Dernoncourt (2016) Lee JY, Dernoncourt F (2016) 使用递归和卷积神经网络的序列短文本分类。arXiv
    预印本 arXiv:160303827
- en: 'Lee (2013) Lee S (2013) Structured discriminative model for dialog state tracking.
    In: Proceedings of the SIGDIAL 2013 Conference, pp 442–451'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee (2013) Lee S (2013) 用于对话状态追踪的结构化判别模型。载于：SIGDIAL 2013 会议论文集，第 442–451 页
- en: 'Lee and Eskenazi (2013) Lee S, Eskenazi M (2013) Recipe for building robust
    spoken dialog state trackers: Dialog state tracking challenge system description.
    In: Proceedings of the SIGDIAL 2013 Conference, pp 414–422'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 Eskenazi (2013) Lee S, Eskenazi M (2013) 构建稳健的语音对话状态追踪器的配方：对话状态追踪挑战系统描述。载于：SIGDIAL
    2013 会议论文集，第 414–422 页
- en: 'Lee and Jha (2019) Lee S, Jha R (2019) Zero-shot adaptive transfer for conversational
    language understanding. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 33, pp 6642–6649'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 Jha (2019) Lee S, Jha R (2019) 适用于会话语言理解的零样本自适应迁移。载于：AAAI 人工智能会议论文集，第
    33 卷，第 6642–6649 页
- en: Lee et al. (2019) Lee S, Schulz H, Atkinson A, Gao J, Suleman K, El Asri L,
    Adada M, Huang M, Sharma S, Tay W, et al. (2019) Multi-domain task-completion
    dialog challenge. Dialog system technology challenges 8:9
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2019) Lee S, Schulz H, Atkinson A, Gao J, Suleman K, El Asri L, Adada
    M, Huang M, Sharma S, Tay W, 等 (2019) 多领域任务完成对话挑战。对话系统技术挑战 8:9
- en: 'Lei et al. (2018) Lei W, Jin X, Kan MY, Ren Z, He X, Yin D (2018) Sequicity:
    Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures.
    In: Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pp 1437–1447'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等 (2018) Lei W, Jin X, Kan MY, Ren Z, He X, Yin D (2018) Sequicity：通过单一的序列到序列架构简化任务导向对话系统。载于：第
    56 届计算语言学协会年会论文集（第 1 卷：长论文），第 1437–1447 页
- en: 'Lemon and Pietquin (2007) Lemon O, Pietquin O (2007) Machine learning for spoken
    dialogue systems. In: Eighth Annual Conference of the International Speech Communication
    Association'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lemon 和 Pietquin (2007) Lemon O, Pietquin O (2007) 机器学习用于语音对话系统。见：第八届国际语音通信协会年会
- en: 'Li et al. (2020a) Li G, Duan N, Fang Y, Gong M, Jiang D (2020a) Unicoder-vl:
    A universal encoder for vision and language by cross-modal pre-training. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 34, pp 11336–11344'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2020a) Li G, Duan N, Fang Y, Gong M, Jiang D (2020a) Unicoder-vl: 一种通过跨模态预训练的通用编码器用于视觉和语言。见：AAAI
    人工智能会议论文集，第34卷，第11336–11344页'
- en: Li et al. (2015) Li J, Galley M, Brockett C, Gao J, Dolan B (2015) A diversity-promoting
    objective function for neural conversation models. arXiv preprint arXiv:151003055
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2015) Li J, Galley M, Brockett C, Gao J, Dolan B (2015) 用于神经对话模型的多样性促进目标函数。arXiv
    预印本 arXiv:151003055
- en: Li et al. (2016a) Li J, Miller AH, Chopra S, Ranzato M, Weston J (2016a) Dialogue
    learning with human-in-the-loop. arXiv preprint arXiv:161109823
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016a) Li J, Miller AH, Chopra S, Ranzato M, Weston J (2016a) 人机协作的对话学习。arXiv
    预印本 arXiv:161109823
- en: Li et al. (2016b) Li J, Miller AH, Chopra S, Ranzato M, Weston J (2016b) Learning
    through dialogue interactions by asking questions. arXiv preprint arXiv:161204936
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016b) Li J, Miller AH, Chopra S, Ranzato M, Weston J (2016b) 通过提问进行对话交互学习。arXiv
    预印本 arXiv:161204936
- en: Li et al. (2016c) Li J, Monroe W, Jurafsky D (2016c) A simple, fast diverse
    decoding algorithm for neural generation. arXiv preprint arXiv:161108562
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016c) Li J, Monroe W, Jurafsky D (2016c) 一种简单快速的多样性解码算法用于神经生成。arXiv
    预印本 arXiv:161108562
- en: Li et al. (2016d) Li J, Monroe W, Ritter A, Galley M, Gao J, Jurafsky D (2016d)
    Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:160601541
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016d) Li J, Monroe W, Ritter A, Galley M, Gao J, Jurafsky D (2016d)
    对话生成的深度强化学习。arXiv 预印本 arXiv:160601541
- en: Li et al. (2017a) Li J, Monroe W, Shi T, Jean S, Ritter A, Jurafsky D (2017a)
    Adversarial learning for neural dialogue generation. arXiv preprint arXiv:170106547
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2017a) Li J, Monroe W, Shi T, Jean S, Ritter A, Jurafsky D (2017a) 对抗性学习用于神经对话生成。arXiv
    预印本 arXiv:170106547
- en: Li et al. (2020b) Li L, Xu C, Wu W, Zhao Y, Zhao X, Tao C (2020b) Zero-resource
    knowledge-grounded dialogue generation. arXiv preprint arXiv:200812918
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2020b) Li L, Xu C, Wu W, Zhao Y, Zhao X, Tao C (2020b) 零资源知识驱动对话生成。arXiv
    预印本 arXiv:200812918
- en: 'Li et al. (2019a) Li LH, Yatskar M, Yin D, Hsieh CJ, Chang KW (2019a) Visualbert:
    A simple and performant baseline for vision and language. arXiv preprint arXiv:190803557'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2019a) Li LH, Yatskar M, Yin D, Hsieh CJ, Chang KW (2019a) Visualbert:
    一种简单而高效的视觉和语言基线。arXiv 预印本 arXiv:190803557'
- en: Li et al. (2019b) Li M, Roller S, Kulikov I, Welleck S, Boureau YL, Cho K, Weston
    J (2019b) Don’t say that! making inconsistent dialogue unlikely with unlikelihood
    training. arXiv preprint arXiv:191103860
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2019b) Li M, Roller S, Kulikov I, Welleck S, Boureau YL, Cho K, Weston
    J (2019b) 别这么说！通过不可能性训练使不一致对话不太可能发生。arXiv 预印本 arXiv:191103860
- en: Li et al. (2016e) Li X, Lipton ZC, Dhingra B, Li L, Gao J, Chen YN (2016e) A
    user simulator for task-completion dialogues. arXiv preprint arXiv:161205688
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016e) Li X, Lipton ZC, Dhingra B, Li L, Gao J, Chen YN (2016e) 用于任务完成对话的用户模拟器。arXiv
    预印本 arXiv:161205688
- en: Li et al. (2017b) Li X, Chen YN, Li L, Gao J, Celikyilmaz A (2017b) End-to-end
    task-completion neural dialogue systems. arXiv preprint arXiv:170301008
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2017b) Li X, Chen YN, Li L, Gao J, Celikyilmaz A (2017b) 端到端任务完成神经对话系统。arXiv
    预印本 arXiv:170301008
- en: 'Li et al. (2018) Li X, Wang Y, Sun S, Panda S, Liu J, Gao J (2018) Microsoft
    dialogue challenge: Building end-to-end task-completion dialogue systems. arXiv
    preprint arXiv:180711125'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2018) Li X, Wang Y, Sun S, Panda S, Liu J, Gao J (2018) 微软对话挑战：构建端到端任务完成对话系统。arXiv
    预印本 arXiv:180711125
- en: Li et al. (2019c) Li X, Yin F, Sun Z, Li X, Yuan A, Chai D, Zhou M, Li J (2019c)
    Entity-relation extraction as multi-turn question answering. arXiv preprint arXiv:190505529
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2019c) Li X, Yin F, Sun Z, Li X, Yuan A, Chai D, Zhou M, Li J (2019c)
    实体-关系提取作为多轮问答。arXiv 预印本 arXiv:190505529
- en: 'Li et al. (2020c) Li X, Yin X, Li C, Zhang P, Hu X, Zhang L, Wang L, Hu H,
    Dong L, Wei F, et al. (2020c) Oscar: Object-semantics aligned pre-training for
    vision-language tasks. In: European Conference on Computer Vision, Springer, pp
    121–137'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2020c) Li X, Yin X, Li C, Zhang P, Hu X, Zhang L, Wang L, Hu H, Dong
    L, Wei F, 等 (2020c) Oscar: 面向视觉-语言任务的对象语义对齐预训练。见：欧洲计算机视觉会议，Springer，第121–137页'
- en: 'Li (2017) Li Y (2017) Deep reinforcement learning: An overview. arXiv preprint
    arXiv:170107274'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li (2017) Li Y (2017) 深度强化学习：概述。arXiv 预印本 arXiv:170107274
- en: 'Li et al. (2017c) Li Y, Su H, Shen X, Li W, Cao Z, Niu S (2017c) Dailydialog:
    A manually labelled multi-turn dialogue dataset. arXiv preprint arXiv:171003957'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2017c）李 Y, 苏 H, 沈 X, 李 W, 曹 Z, 牛 S (2017c) Dailydialog：一个手工标注的多轮对话数据集。arXiv
    预印本 arXiv:171003957
- en: 'Li et al. (2020d) Li Y, Yao K, Qin L, Che W, Li X, Liu T (2020d) Slot-consistent
    nlg for task-oriented dialogue systems with iterative rectification network. In:
    Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
    pp 97–106'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2020d）李 Y, 姚 K, 秦 L, 车 W, 李 X, 刘 T (2020d) 用于任务导向对话系统的槽一致nlg与迭代修正网络。载于：第58届计算语言学协会年会论文集，第97–106页
- en: Li et al. (2019d) Li Z, Niu C, Meng F, Feng Y, Li Q, Zhou J (2019d) Incremental
    transformer with deliberation decoder for document grounded conversations. arXiv
    preprint arXiv:190708854
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等（2019d）李 Z, 牛 C, 孟 F, 冯 Y, 李 Q, 周 J (2019d) 带有深思解码器的增量变换器用于文档基础对话。arXiv 预印本
    arXiv:190708854
- en: 'Liang et al. (2020) Liang W, Zou J, Yu Z (2020) Beyond user self-reported likert
    scale ratings: A comparison model for automatic dialog evaluation. arXiv preprint
    arXiv:200510716'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梁等（2020）梁 W, 邹 J, 于 Z (2020) 超越用户自报告的Likert量表评分：自动对话评估的比较模型。arXiv 预印本 arXiv:200510716
- en: 'Lin (2004) Lin CY (2004) Rouge: A package for automatic evaluation of summaries.
    In: Text summarization branches out, pp 74–81'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林（2004）林 CY (2004) Rouge：自动摘要评估的包。载于：文本摘要扩展，第74–81页
- en: Lin (1992) Lin LJ (1992) Self-improving reactive agents based on reinforcement
    learning, planning and teaching. Machine learning 8(3-4):293–321
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林（1992）林 LJ (1992) 基于强化学习、规划和教学的自我改进反应性代理。机器学习 8(3-4):293–321
- en: Lin et al. (2021) Lin T, Wang Y, Liu X, Qiu X (2021) A survey of transformers.
    arXiv preprint arXiv:210604554
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2021）林 T, 王 Y, 刘 X, 邱 X (2021) 转换器的综述。arXiv 预印本 arXiv:210604554
- en: Lin et al. (2019) Lin X, Joty S, Jwalapuram P, Bari MS (2019) A unified linear-time
    framework for sentence-level discourse parsing. arXiv preprint arXiv:190505682
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2019）林 X, Joty S, Jwalapuram P, Bari MS (2019) 用于句子级话语解析的统一线性时间框架。arXiv 预印本
    arXiv:190505682
- en: 'Lin et al. (2020a) Lin X, Jian W, He J, Wang T, Chu W (2020a) Generating informative
    conversational response using recurrent knowledge-interaction and knowledge-copy.
    In: Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pp 41–52'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2020a）林 X, 闵 W, 何 J, 王 T, 楚 W (2020a) 使用递归知识交互和知识复制生成信息丰富的对话响应。载于：第58届计算语言学协会年会论文集，第41–52页
- en: 'Lin et al. (2015) Lin Y, Liu Z, Sun M, Liu Y, Zhu X (2015) Learning entity
    and relation embeddings for knowledge graph completion. In: Proceedings of the
    AAAI Conference on Artificial Intelligence, vol 29'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2015）林 Y, 刘 Z, 孙 M, 刘 Y, 朱 X (2015) 学习实体和关系嵌入以完成知识图谱。载于：AAAI人工智能会议论文集，第29卷
- en: 'Lin et al. (2020b) Lin Z, Cai D, Wang Y, Liu X, Zheng H, Shi S (2020b) The
    world is not binary: Learning to rank with grayscale data for dialogue response
    selection. In: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pp 9220–9229'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2020b）林 Z, 蔡 D, 王 Y, 刘 X, 郑 H, 石 S (2020b) 世界不是二元的：使用灰度数据学习排名以进行对话响应选择。载于：2020年自然语言处理经验方法会议（EMNLP）论文集，第9220–9229页
- en: 'Lin et al. (2020c) Lin Z, Madotto A, Winata GI, Fung P (2020c) Mintl: Minimalist
    transfer learning for task-oriented dialogue systems. arXiv preprint arXiv:200912005'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等（2020c）林 Z, 马多托 A, 韦纳塔 GI, 冯 P (2020c) Mintl：任务导向对话系统的极简主义迁移学习。arXiv 预印本 arXiv:200912005
- en: Lipton et al. (2015) Lipton ZC, Berkowitz J, Elkan C (2015) A critical review
    of recurrent neural networks for sequence learning. arXiv preprint arXiv:150600019
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lipton等（2015）Lipton ZC, Berkowitz J, Elkan C (2015) 对递归神经网络在序列学习中的关键性审查。arXiv
    预印本 arXiv:150600019
- en: 'Lison and Bibauw (2017) Lison P, Bibauw S (2017) Not all dialogues are created
    equal: Instance weighting for neural conversational models. arXiv preprint arXiv:170408966'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李森和比博（2017）李森 P, 比博 S (2017) 并非所有对话都是平等的：神经对话模型的实例加权。arXiv 预印本 arXiv:170408966
- en: Liu and Lane (2016) Liu B, Lane I (2016) Attention-based recurrent neural network
    models for joint intent detection and slot filling. arXiv preprint arXiv:160901454
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘和莱恩（2016）刘 B, 莱恩 I (2016) 基于注意力的递归神经网络模型用于联合意图检测和槽填充。arXiv 预印本 arXiv:160901454
- en: 'Liu and Lane (2017) Liu B, Lane I (2017) Iterative policy learning in end-to-end
    trainable task-oriented neural dialog models. In: 2017 IEEE Automatic Speech Recognition
    and Understanding Workshop (ASRU), IEEE, pp 482–489'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘和莱恩（2017）刘 B, 莱恩 I (2017) 在端到端可训练的任务导向神经对话模型中的迭代策略学习。载于：2017 IEEE自动语音识别与理解研讨会（ASRU），IEEE，第482–489页
- en: 'Liu et al. (2019) Liu C, He S, Liu K, Zhao J (2019) Vocabulary pyramid network:
    Multi-pass encoding and decoding with multi-level vocabularies for response generation.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 3774–3783'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) 刘 C, 何 S, 刘 K, 赵 J (2019) 词汇金字塔网络：通过多层词汇进行多次编码和解码以生成响应。在：计算语言学协会第57届年会上，pp
    3774–3783
- en: 'Liu et al. (2016) Liu CW, Lowe R, Serban IV, Noseworthy M, Charlin L, Pineau
    J (2016) How not to evaluate your dialogue system: An empirical study of unsupervised
    evaluation metrics for dialogue response generation. arXiv preprint arXiv:160308023'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2016) 刘 CW, 洛威 R, 塞尔班 IV, 诺斯沃西 M, 查林 L, 皮诺 J (2016) 如何不评估你的对话系统：对对话响应生成的无监督评估指标的实证研究。arXiv
    预印本 arXiv:160308023
- en: Liu et al. (2020a) Liu H, Wang W, Wang Y, Liu H, Liu Z, Tang J (2020a) Mitigating
    gender bias for neural dialogue generation with adversarial learning. arXiv preprint
    arXiv:200913028
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020a) 刘 H, 王 W, 王 Y, 刘 H, 刘 Z, 唐 J (2020a) 使用对抗学习缓解神经对话生成中的性别偏见。arXiv
    预印本 arXiv:200913028
- en: 'Liu et al. (2020b) Liu Q, Chen Y, Chen B, Lou JG, Chen Z, Zhou B, Zhang D (2020b)
    You impress me: Dialogue generation via mutual persona perception. arXiv preprint
    arXiv:200405388'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020b) 刘 Q, 陈 Y, 陈 B, 陆 JG, 陈 Z, 周 B, 张 D (2020b) 你给我留下了深刻印象：通过互相角色感知生成对话。arXiv
    预印本 arXiv:200405388
- en: Liu and Lapata (2018) Liu Y, Lapata M (2018) Learning structured text representations.
    Transactions of the Association for Computational Linguistics 6:63–75
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu and Lapata (2018) 刘 Y, 拉帕塔 M (2018) 学习结构化文本表示。计算语言学协会会刊 6:63–75
- en: Liu et al. (2020c) Liu Z, Wang H, Niu ZY, Wu H, Che W, Liu T (2020c) Towards
    conversational recommendation over multi-type dialogs. arXiv preprint arXiv:200503954
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020c) 刘 Z, 王 H, 牛 ZY, 吴 H, 车 W, 刘 T (2020c) 迈向多类型对话中的对话推荐。arXiv
    预印本 arXiv:200503954
- en: 'Lowe et al. (2015) Lowe R, Pow N, Serban I, Pineau J (2015) The ubuntu dialogue
    corpus: A large dataset for research in unstructured multi-turn dialogue systems.
    arXiv preprint arXiv:150608909'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe et al. (2015) 洛威 R, 鲍 N, 塞尔班 I, 皮诺 J (2015) Ubuntu 对话语料库：用于非结构化多轮对话系统研究的大型数据集。arXiv
    预印本 arXiv:150608909
- en: 'Lowe et al. (2017) Lowe R, Noseworthy M, Serban IV, Angelard-Gontier N, Bengio
    Y, Pineau J (2017) Towards an automatic turing test: Learning to evaluate dialogue
    responses. arXiv preprint arXiv:170807149'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe et al. (2017) 洛威 R, 诺斯沃西 M, 塞尔班 IV, 安杰拉尔-贡蒂埃 N, 本吉奥 Y, 皮诺 J (2017) 迈向自动图灵测试：学习评估对话响应。arXiv
    预印本 arXiv:170807149
- en: 'Lu et al. (2019a) Lu J, Batra D, Parikh D, Lee S (2019a) Vilbert: Pretraining
    task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv
    preprint arXiv:190802265'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2019a) 陆 J, 巴特拉 D, 帕里克 D, 李 S (2019a) Vilbert：预训练任务无关的视觉语言表示，用于视觉和语言任务。arXiv
    预印本 arXiv:190802265
- en: 'Lu et al. (2019b) Lu J, Zhang C, Xie Z, Ling G, Zhou TC, Xu Z (2019b) Constructing
    interpretive spatio-temporal features for multi-turn responses selection. In:
    Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
    pp 44–50'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2019b) 陆 J, 张 C, 谢 Z, 凌 G, 周 TC, 徐 Z (2019b) 为多轮响应选择构建可解释的时空特征。在：计算语言学协会第57届年会上，pp
    44–50
- en: 'Lu et al. (2020) Lu J, Goswami V, Rohrbach M, Parikh D, Lee S (2020) 12-in-1:
    Multi-task vision and language representation learning. In: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10437–10446'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2020) 陆 J, 戈斯瓦米 V, 罗赫巴赫 M, 帕里克 D, 李 S (2020) 12合1：多任务视觉和语言表示学习。在：IEEE/CVF计算机视觉与模式识别会议论文集，pp
    10437–10446
- en: 'Lubis et al. (2018) Lubis N, Sakti S, Yoshino K, Nakamura S (2018) Eliciting
    positive emotion through affect-sensitive dialogue response generation: A neural
    network approach. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 32, no 1'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lubis et al. (2018) Lubis N, Sakti S, 吉野 K, 中村 S (2018) 通过情感敏感对话生成引发积极情绪：一种神经网络方法。在：AAAI人工智能大会论文集，第32卷，第1期
- en: Ma et al. (2019) Ma MD, Bowden KK, Wu J, Cui W, Walker M (2019) Implicit discourse
    relation identification for open-domain dialogues. arXiv preprint arXiv:190703975
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2019) 马 MD, 鲍登 KK, 吴 J, 崔 W, 沃克 M (2019) 开放领域对话中的隐含话语关系识别。arXiv 预印本
    arXiv:190703975
- en: Ma et al. (2020a) Ma W, Cui Y, Liu T, Wang D, Wang S, Hu G (2020a) Conversational
    word embedding for retrieval-based dialog system. arXiv preprint arXiv:200413249
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2020a) 马 W, 崔 Y, 刘 T, 王 D, 王 S, 胡 G (2020a) 基于检索的对话系统的对话词嵌入。arXiv
    预印本 arXiv:200413249
- en: Ma et al. (2020b) Ma Y, Nguyen KL, Xing FZ, Cambria E (2020b) A survey on empathetic
    dialogue systems. Information Fusion 64:50–70
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2020b) 马 Y, 阮 KL, 邢 FZ, 卡姆布里亚 E (2020b) 同情对话系统的综述。信息融合 64:50–70
- en: 'Madotto et al. (2019) Madotto A, Lin Z, Wu CS, Fung P (2019) Personalizing
    dialogue agents via meta-learning. In: Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics, pp 5454–5459'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madotto 等（2019）Madotto A, Lin Z, Wu CS, Fung P（2019）通过元学习个性化对话代理。收录于：第57届计算语言学协会年会论文集，第5454–5459页
- en: 'Majumder et al. (2020a) Majumder BP, Jhamtani H, Berg-Kirkpatrick T, McAuley
    J (2020a) Like hiking? you probably enjoy nature: Persona-grounded dialog with
    commonsense expansions. arXiv preprint arXiv:201003205'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majumder 等（2020a）Majumder BP, Jhamtani H, Berg-Kirkpatrick T, McAuley J（2020a）喜欢远足？你可能喜欢自然：基于常识扩展的个性化对话。arXiv
    预印本 arXiv:201003205
- en: 'Majumder et al. (2020b) Majumder BP, Li S, Ni J, McAuley J (2020b) Interview:
    Large-scale modeling of media dialog with discourse patterns and knowledge grounding.
    In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pp 8129–8141'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majumder 等（2020b）Majumder BP, Li S, Ni J, McAuley J（2020b）访谈：具有话语模式和知识基础的大规模媒体对话建模。收录于：2020年自然语言处理实证方法会议（EMNLP）论文集，第8129–8141页
- en: 'Mallios and Bourbakis (2016) Mallios S, Bourbakis N (2016) A survey on human
    machine dialogue systems. In: 2016 7th international conference on information,
    intelligence, systems & applications (iisa), IEEE, pp 1–7'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mallios 和 Bourbakis（2016）Mallios S, Bourbakis N（2016）关于人机对话系统的综述。收录于：2016年第七届国际信息、智能、系统与应用会议（iISA），IEEE，第1–7页
- en: 'Manuvina-kurike et al. (2018) Manuvina-kurike R, Brixey J, Bui T, Chang W,
    Artstein R, Georgila K (2018) Dialedit: Annotations for spoken conversational
    image editing. In: Proceedings 14th Joint ACL-ISO Workshop on Interoperable Semantic
    Annotation, pp 1–9'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manuvina-kurike 等（2018）Manuvina-kurike R, Brixey J, Bui T, Chang W, Artstein
    R, Georgila K（2018）Dialedit：口语对话图像编辑的注释。收录于：第14届ACL-ISO语义注释联合研讨会论文集，第1–9页
- en: Mao et al. (2020) Mao HH, Li S, McAuley J, Cottrell G (2020) Speech recognition
    and multi-speaker diarization of long conversations. arXiv preprint arXiv:200508072
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等（2020）Mao HH, Li S, McAuley J, Cottrell G（2020）长对话的语音识别和多说话人分段。arXiv 预印本
    arXiv:200508072
- en: 'Mehri and Eskenazi (2020) Mehri S, Eskenazi M (2020) Usr: An unsupervised and
    reference free evaluation metric for dialog generation. arXiv preprint arXiv:200500456'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehri 和 Eskenazi（2020）Mehri S, Eskenazi M（2020）Usr：一种无监督且无需参考的对话生成评价指标。arXiv
    预印本 arXiv:200500456
- en: Mehri et al. (2019) Mehri S, Razumovskaia E, Zhao T, Eskenazi M (2019) Pretraining
    methods for dialog context representation learning. arXiv preprint arXiv:190600414
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mehri 等（2019）Mehri S, Razumovskaia E, Zhao T, Eskenazi M（2019）对话上下文表示学习的预训练方法。arXiv
    预印本 arXiv:190600414
- en: Mesgar et al. (2019) Mesgar M, Bücker S, Gurevych I (2019) Dialogue coherence
    assessment without explicit dialogue act labels. arXiv preprint arXiv:190808486
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesgar 等（2019）Mesgar M, Bücker S, Gurevych I（2019）无显式对话行为标签的对话连贯性评估。arXiv 预印本
    arXiv:190808486
- en: 'Mesnil et al. (2013) Mesnil G, He X, Deng L, Bengio Y (2013) Investigation
    of recurrent-neural-network architectures and learning methods for spoken language
    understanding. In: Interspeech, pp 3771–3775'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesnil 等（2013）Mesnil G, He X, Deng L, Bengio Y（2013）对递归神经网络架构和学习方法在口语理解中的研究。收录于：Interspeech，第3771–3775页
- en: Mesnil et al. (2014) Mesnil G, Dauphin Y, Yao K, Bengio Y, Deng L, Hakkani-Tur
    D, He X, Heck L, Tur G, Yu D, et al. (2014) Using recurrent neural networks for
    slot filling in spoken language understanding. IEEE/ACM Transactions on Audio,
    Speech, and Language Processing 23(3):530–539
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesnil 等（2014）Mesnil G, Dauphin Y, Yao K, Bengio Y, Deng L, Hakkani-Tur D, He
    X, Heck L, Tur G, Yu D 等（2014）使用递归神经网络进行口语理解中的槽位填充。IEEE/ACM 音频、语音与语言处理汇刊 23(3):530–539
- en: 'Miao et al. (2019) Miao N, Zhou H, Mou L, Yan R, Li L (2019) Cgmh: Constrained
    sentence generation by metropolis-hastings sampling. In: Proceedings of the AAAI
    Conference on Artificial Intelligence, vol 33, pp 6834–6842'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao 等（2019）Miao N, Zhou H, Mou L, Yan R, Li L（2019）CGMH：通过Metropolis-Hastings采样的约束句子生成。收录于：AAAI人工智能会议论文集，第33卷，第6834–6842页
- en: 'Miech et al. (2020) Miech A, Alayrac JB, Smaira L, Laptev I, Sivic J, Zisserman
    A (2020) End-to-end learning of visual representations from uncurated instructional
    videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp 9879–9889'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miech 等（2020）Miech A, Alayrac JB, Smaira L, Laptev I, Sivic J, Zisserman A（2020）从未整理的教学视频中端到端学习视觉表示。收录于：IEEE/CVF
    计算机视觉与模式识别会议论文集，第9879–9889页
- en: Miller et al. (2016) Miller A, Fisch A, Dodge J, Karimi AH, Bordes A, Weston
    J (2016) Key-value memory networks for directly reading documents. arXiv preprint
    arXiv:160603126
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller 等（2016）Miller A, Fisch A, Dodge J, Karimi AH, Bordes A, Weston J（2016）用于直接读取文档的键值记忆网络。arXiv
    预印本 arXiv:160603126
- en: Mirowski et al. (2016) Mirowski P, Pascanu R, Viola F, Soyer H, Ballard AJ,
    Banino A, Denil M, Goroshin R, Sifre L, Kavukcuoglu K, et al. (2016) Learning
    to navigate in complex environments. arXiv preprint arXiv:161103673
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirowski 等人 (2016) Mirowski P, Pascanu R, Viola F, Soyer H, Ballard AJ, Banino
    A, Denil M, Goroshin R, Sifre L, Kavukcuoglu K, 等 (2016) 学习在复杂环境中导航。arXiv 预印本
    arXiv:161103673
- en: Mnih et al. (2015) Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare
    MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, et al. (2015) Human-level
    control through deep reinforcement learning. nature 518(7540):529–533
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等人 (2015) Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare
    MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, 等 (2015) 通过深度强化学习实现人类级控制。自然
    518(7540):529–533
- en: 'Mnih et al. (2016) Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley
    T, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement
    learning. In: International conference on machine learning, PMLR, pp 1928–1937'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等人 (2016) Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley T, Silver
    D, Kavukcuoglu K (2016) 深度强化学习的异步方法。发表于：国际机器学习会议，PMLR，页 1928–1937
- en: 'Mo et al. (2018) Mo K, Zhang Y, Li S, Li J, Yang Q (2018) Personalizing a dialogue
    system with transfer reinforcement learning. In: Proceedings of the AAAI Conference
    on Artificial Intelligence, vol 32, no 1'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo 等人 (2018) Mo K, Zhang Y, Li S, Li J, Yang Q (2018) 通过迁移强化学习个性化对话系统。发表于：AAAI
    人工智能会议论文集，第 32 卷，第 1 期
- en: Moghe et al. (2018) Moghe N, Arora S, Banerjee S, Khapra MM (2018) Towards exploiting
    background knowledge for building conversation systems. arXiv preprint arXiv:180908205
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moghe 等人 (2018) Moghe N, Arora S, Banerjee S, Khapra MM (2018) 利用背景知识构建对话系统。arXiv
    预印本 arXiv:180908205
- en: 'Mohammad et al. (2018) Mohammad S, Bravo-Marquez F, Salameh M, Kiritchenko
    S (2018) Semeval-2018 task 1: Affect in tweets. In: Proceedings of the 12th international
    workshop on semantic evaluation, pp 1–17'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammad 等人 (2018) Mohammad S, Bravo-Marquez F, Salameh M, Kiritchenko S (2018)
    Semeval-2018 任务 1：推特中的情感。发表于：第 12 届语义评估国际研讨会论文集，页 1–17
- en: 'Moon et al. (2019) Moon S, Shah P, Kumar A, Subba R (2019) Opendialkg: Explainable
    conversational reasoning with attention-based walks over knowledge graphs. In:
    Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
    pp 845–854'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon 等人 (2019) Moon S, Shah P, Kumar A, Subba R (2019) Opendialkg：基于注意力的知识图谱漫游的可解释对话推理。发表于：第
    57 届计算语言学协会年会论文集，页 845–854
- en: 'Mostafazadeh et al. (2017) Mostafazadeh N, Brockett C, Dolan B, Galley M, Gao
    J, Spithourakis GP, Vanderwende L (2017) Image-grounded conversations: Multimodal
    context for natural question and response generation. arXiv preprint arXiv:170108251'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafazadeh 等人 (2017) Mostafazadeh N, Brockett C, Dolan B, Galley M, Gao J,
    Spithourakis GP, Vanderwende L (2017) 图像引导对话：自然问答生成的多模态上下文。arXiv 预印本 arXiv:170108251
- en: Mrkšić et al. (2015) Mrkšić N, Séaghdha DO, Thomson B, Gašić M, Su PH, Vandyke
    D, Wen TH, Young S (2015) Multi-domain dialog state tracking using recurrent neural
    networks. arXiv preprint arXiv:150607190
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mrkšić 等人 (2015) Mrkšić N, Séaghdha DO, Thomson B, Gašić M, Su PH, Vandyke D,
    Wen TH, Young S (2015) 使用递归神经网络的多领域对话状态跟踪。arXiv 预印本 arXiv:150607190
- en: 'Mrkšić et al. (2016) Mrkšić N, Séaghdha DO, Wen TH, Thomson B, Young S (2016)
    Neural belief tracker: Data-driven dialogue state tracking. arXiv preprint arXiv:160603777'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mrkšić 等人 (2016) Mrkšić N, Séaghdha DO, Wen TH, Thomson B, Young S (2016) 神经信念跟踪器：数据驱动的对话状态跟踪。arXiv
    预印本 arXiv:160603777
- en: 'Nakov et al. (2019) Nakov P, Màrquez L, Magdy W, Moschitti A, Glass J, Randeree
    B (2019) Semeval-2015 task 3: Answer selection in community question answering.
    arXiv preprint arXiv:191111403'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakov 等人 (2019) Nakov P, Màrquez L, Magdy W, Moschitti A, Glass J, Randeree
    B (2019) Semeval-2015 任务 3：社区问答中的答案选择。arXiv 预印本 arXiv:191111403
- en: 'Nickel et al. (2016) Nickel M, Rosasco L, Poggio T (2016) Holographic embeddings
    of knowledge graphs. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 30, no 1'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nickel 等人 (2016) Nickel M, Rosasco L, Poggio T (2016) 知识图谱的全息嵌入。发表于：AAAI 人工智能会议论文集，第
    30 卷，第 1 期
- en: 'Novikova et al. (2017) Novikova J, Dušek O, Rieser V (2017) The e2e dataset:
    New challenges for end-to-end generation. arXiv preprint arXiv:170609254'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novikova 等人 (2017) Novikova J, Dušek O, Rieser V (2017) e2e 数据集：端到端生成的新挑战。arXiv
    预印本 arXiv:170609254
- en: 'Oraby et al. (2019) Oraby S, Harrison V, Ebrahimi A, Walker M (2019) Curate
    and generate: A corpus and method for joint control of semantics and style in
    neural nlg. arXiv preprint arXiv:190601334'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oraby 等人 (2019) Oraby S, Harrison V, Ebrahimi A, Walker M (2019) 策划和生成：用于神经
    NLG 的语义和风格联合控制的语料库和方法。arXiv 预印本 arXiv:190601334
- en: Oriol et al. (2015) Oriol V, Meire F, Navdeep J (2015) Pointer networks. Advances
    in neural information processing systems 28:2692–2700
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oriol等（2015）Oriol V, Meire F, Navdeep J（2015）指针网络。神经信息处理系统进展 28：2692–2700
- en: 'Ouyang et al. (2020) Ouyang Y, Chen M, Dai X, Zhao Y, Huang S, Jiajun C (2020)
    Dialogue state tracking with explicit slot connection modeling. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    34–40'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等（2020）Ouyang Y, Chen M, Dai X, Zhao Y, Huang S, Jiajun C（2020）对话状态追踪与显式槽连接建模。见：第58届计算语言学协会年会论文集，第34–40页
- en: 'Panayotov et al. (2015) Panayotov V, Chen G, Povey D, Khudanpur S (2015) Librispeech:
    an asr corpus based on public domain audio books. In: 2015 IEEE international
    conference on acoustics, speech and signal processing (ICASSP), IEEE, pp 5206–5210'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panayotov等（2015）Panayotov V, Chen G, Povey D, Khudanpur S（2015）Librispeech：一个基于公共领域有声书的ASR语料库。见：2015年IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，第5206–5210页
- en: 'Pang et al. (2020) Pang B, Nijkamp E, Han W, Zhou L, Liu Y, Tu K (2020) Towards
    holistic and automatic evaluation of open-domain dialogue generation. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    3619–3629'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庞等（2020）Pang B, Nijkamp E, Han W, Zhou L, Liu Y, Tu K（2020）朝向开放域对话生成的整体和自动化评估。见：第58届计算语言学协会年会论文集，第3619–3629页
- en: 'Papineni et al. (2002) Papineni K, Roukos S, Ward T, Zhu WJ (2002) Bleu: a
    method for automatic evaluation of machine translation. In: Proceedings of the
    40th annual meeting of the Association for Computational Linguistics, pp 311–318'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni等（2002）Papineni K, Roukos S, Ward T, Zhu WJ（2002）BLEU：一种自动评估机器翻译的方法。见：第40届计算语言学协会年会论文集，第311–318页
- en: Parikh et al. (2016) Parikh AP, Täckström O, Das D, Uszkoreit J (2016) A decomposable
    attention model for natural language inference. arXiv preprint arXiv:160601933
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parikh等（2016）Parikh AP, Täckström O, Das D, Uszkoreit J（2016）用于自然语言推理的可分解注意力模型。arXiv预印本
    arXiv:160601933
- en: 'Pascanu et al. (2013) Pascanu R, Mikolov T, Bengio Y (2013) On the difficulty
    of training recurrent neural networks. In: International conference on machine
    learning, PMLR, pp 1310–1318'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascanu等（2013）Pascanu R, Mikolov T, Bengio Y（2013）训练递归神经网络的困难。见：机器学习国际会议，PMLR，第1310–1318页
- en: Peng et al. (2017) Peng B, Li X, Li L, Gao J, Celikyilmaz A, Lee S, Wong KF
    (2017) Composite task-completion dialogue policy learning via hierarchical deep
    reinforcement learning. arXiv preprint arXiv:170403084
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彭等（2017）Peng B, Li X, Li L, Gao J, Celikyilmaz A, Lee S, Wong KF（2017）通过分层深度强化学习的复合任务完成对话策略学习。arXiv预印本
    arXiv:170403084
- en: Peters et al. (2018) Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee
    K, Zettlemoyer L (2018) Deep contextualized word representations. arXiv preprint
    arXiv:180205365
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彼得斯等（2018）Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer
    L（2018）深层上下文化的词表示。arXiv预印本 arXiv:180205365
- en: Pfau and Vinyals (2016) Pfau D, Vinyals O (2016) Connecting generative adversarial
    networks and actor-critic methods. arXiv preprint arXiv:161001945
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfau和Vinyals（2016）Pfau D, Vinyals O（2016）连接生成对抗网络和演员-评论家方法。arXiv预印本 arXiv:161001945
- en: 'Poria et al. (2019) Poria S, Hazarika D, Majumder N, Naik G, Cambria E, Mihalcea
    R (2019) MELD: A multimodal multi-party dataset for emotion recognition in conversations.
    In: ACL, pp 527–536'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poria等（2019）Poria S, Hazarika D, Majumder N, Naik G, Cambria E, Mihalcea R（2019）MELD：一个用于对话情感识别的多模态多方数据集。见：ACL，第527–536页
- en: 'Powers (2020) Powers DMW (2020) Evaluation: from precision, recall and f-measure
    to roc, informedness, markedness and correlation. ArXiv abs/2010.16061'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Powers（2020）Powers DMW（2020）评估：从精确度、召回率和F-measure到ROC、信息度、标记性和相关性。ArXiv abs/2010.16061
- en: 'Prasad et al. (2008) Prasad R, Dinesh N, Lee A, Miltsakaki E, Robaldo L, Joshi
    AK, Webber BL (2008) The penn discourse treebank 2.0\. In: LREC, Citeseer'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prasad等（2008）Prasad R, Dinesh N, Lee A, Miltsakaki E, Robaldo L, Joshi AK, Webber
    BL（2008）Penn语篇树库2.0。见：LREC，Citeseer
- en: 'Puterman (2014) Puterman ML (2014) Markov decision processes: discrete stochastic
    dynamic programming. John Wiley & Sons'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puterman（2014）Puterman ML（2014）马尔可夫决策过程：离散随机动态规划。John Wiley & Sons
- en: 'Qi et al. (2020) Qi D, Su L, Song J, Cui E, Bharti T, Sacheti A (2020) Imagebert:
    Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv
    preprint arXiv:200107966'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 齐等（2020）Qi D, Su L, Song J, Cui E, Bharti T, Sacheti A（2020）ImageBERT：使用大规模弱监督图像-文本数据的跨模态预训练。arXiv预印本
    arXiv:200107966
- en: Qian and Yu (2019) Qian K, Yu Z (2019) Domain adaptive dialog generation via
    meta learning. arXiv preprint arXiv:190603520
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钱和于（2019）Qian K, Yu Z（2019）通过元学习的领域自适应对话生成。arXiv预印本 arXiv:190603520
- en: Qin et al. (2020) Qin L, Xu X, Che W, Zhang Y, Liu T (2020) Dynamic fusion network
    for multi-domain end-to-end task-oriented dialog. arXiv preprint arXiv:200411019
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人（2020）Qin L, Xu X, Che W, Zhang Y, Liu T（2020）用于多领域端到端任务导向对话的动态融合网络。arXiv
    预印本 arXiv:200411019
- en: 'Qiu et al. (2019) Qiu L, Li J, Bi W, Zhao D, Yan R (2019) Are training samples
    correlated? learning to generate dialogue responses with multiple references.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 3826–3835'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等人（2019）Qiu L, Li J, Bi W, Zhao D, Yan R（2019）训练样本是否相关？学习生成带有多个参考的对话响应。在：第57届计算语言学协会年会论文集，第3826–3835页
- en: Qiu et al. (2020) Qiu L, Zhao Y, Shi W, Liang Y, Shi F, Yuan T, Yu Z, Zhu SC
    (2020) Structured attention for unsupervised dialogue structure induction. arXiv
    preprint arXiv:200908552
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等人（2020）Qiu L, Zhao Y, Shi W, Liang Y, Shi F, Yuan T, Yu Z, Zhu SC（2020）无监督对话结构诱导的结构化注意力。arXiv
    预印本 arXiv:200908552
- en: 'Qiu et al. (2017) Qiu M, Li FL, Wang S, Gao X, Chen Y, Zhao W, Chen H, Huang
    J, Chu W (2017) Alime chat: A sequence to sequence and rerank based chatbot engine.
    In: Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers), pp 498–503'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiu 等人（2017）Qiu M, Li FL, Wang S, Gao X, Chen Y, Zhao W, Chen H, Huang J, Chu
    W（2017）Alime chat：一个基于序列到序列和重排序的聊天机器人引擎。在：第55届计算语言学协会年会论文集（卷2：短文），第498–503页
- en: Quan and Xiong (2020) Quan J, Xiong D (2020) Modeling long context for task-oriented
    dialogue state generation. arXiv preprint arXiv:200414080
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quan 和 Xiong（2020）Quan J, Xiong D（2020）建模任务导向对话状态生成的长上下文。arXiv 预印本 arXiv:200414080
- en: 'Quan et al. (2020) Quan J, Zhang S, Cao Q, Li Z, Xiong D (2020) Risawoz: A
    large-scale multi-domain wizard-of-oz dataset with rich semantic annotations for
    task-oriented dialogue modeling. arXiv preprint arXiv:201008738'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Quan 等人（2020）Quan J, Zhang S, Cao Q, Li Z, Xiong D（2020）Risawoz：一个大规模多领域的 Wizard-of-Oz
    数据集，具有丰富的语义注释，用于任务导向对话建模。arXiv 预印本 arXiv:201008738
- en: 'Rajpurkar et al. (2018) Rajpurkar P, Jia R, Liang P (2018) Know what you don’t
    know: Unanswerable questions for squad. arXiv preprint arXiv:180603822'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajpurkar 等人（2018）Rajpurkar P, Jia R, Liang P（2018）了解你不知道的：SQUAD 的不可回答问题。arXiv
    预印本 arXiv:180603822
- en: 'Ram et al. (2018) Ram A, Prasad R, Khatri C, Venkatesh A, Gabriel R, Liu Q,
    Nunn J, Hedayatnia B, Cheng M, Nagar A, et al. (2018) Conversational ai: The science
    behind the alexa prize. arXiv preprint arXiv:180103604'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram 等人（2018）Ram A, Prasad R, Khatri C, Venkatesh A, Gabriel R, Liu Q, Nunn J,
    Hedayatnia B, Cheng M, Nagar A 等（2018）对话 AI：Alexa 奖背后的科学。arXiv 预印本 arXiv:180103604
- en: 'Rameshkumar and Bailey (2020) Rameshkumar R, Bailey P (2020) Storytelling with
    dialogue: A critical role dungeons and dragons dataset. In: Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, pp 5121–5134'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rameshkumar 和 Bailey（2020）Rameshkumar R, Bailey P（2020）通过对话讲故事：一个关键角色的《龙与地下城》数据集。在：第58届计算语言学协会年会论文集，第5121–5134页
- en: 'Ramshaw and Marcus (1999) Ramshaw LA, Marcus MP (1999) Text chunking using
    transformation-based learning. In: Natural language processing using very large
    corpora, Springer, pp 157–176'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramshaw 和 Marcus（1999）Ramshaw LA, Marcus MP（1999）使用基于转换的学习进行文本块划分。在：使用超大型语料库的自然语言处理，Springer，第157–176页
- en: 'Rashkin et al. (2018) Rashkin H, Smith EM, Li M, Boureau YL (2018) Towards
    empathetic open-domain conversation models: A new benchmark and dataset. arXiv
    preprint arXiv:181100207'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashkin 等人（2018）Rashkin H, Smith EM, Li M, Boureau YL（2018）朝向具有同理心的开放域对话模型：一个新的基准和数据集。arXiv
    预印本 arXiv:181100207
- en: 'Rastogi et al. (2020) Rastogi A, Zang X, Sunkara S, Gupta R, Khaitan P (2020)
    Towards scalable multi-domain conversational agents: The schema-guided dialogue
    dataset. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol
    34, no 5, pp 8689–8696'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rastogi 等人（2020）Rastogi A, Zang X, Sunkara S, Gupta R, Khaitan P（2020）朝向可扩展的多领域对话代理：基于
    schema 的对话数据集。在：AAAI 人工智能会议论文集，第34卷，第5期，第8689–8696页
- en: 'Ravuri and Stolcke (2015) Ravuri S, Stolcke A (2015) Recurrent neural network
    and lstm models for lexical utterance classification. In: Sixteenth Annual Conference
    of the International Speech Communication Association'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravuri 和 Stolcke（2015）Ravuri S, Stolcke A（2015）用于词汇发话分类的递归神经网络和 LSTM 模型。在：第十六届国际语音通信协会年会
- en: 'Ravuri and Stolcke (2016) Ravuri S, Stolcke A (2016) A comparative study of
    recurrent neural network models for lexical domain classification. In: 2016 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, pp 6075–6079'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravuri 和 Stolcke（2016）Ravuri S, Stolcke A（2016）递归神经网络模型在词汇领域分类中的比较研究。在：2016
    IEEE 国际声学、语音和信号处理会议（ICASSP），IEEE，第6075–6079页
- en: 'Rawat and Wang (2017) Rawat W, Wang Z (2017) Deep convolutional neural networks
    for image classification: A comprehensive review. Neural computation 29(9):2352–2449'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rawat 和 Wang (2017) Rawat W, Wang Z (2017) 用于图像分类的深度卷积神经网络：一项全面的综述。神经计算 29(9):2352–2449
- en: Reiter (1994) Reiter E (1994) Has a consensus nl generation architecture appeared,
    and is it psycholinguistically plausible? arXiv preprint cmp-lg/9411032
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiter (1994) Reiter E (1994) 是否出现了共识的自然语言生成架构，并且它是否心理语言学上合理？arXiv 预印本 cmp-lg/9411032
- en: 'Ren et al. (2013) Ren H, Xu W, Zhang Y, Yan Y (2013) Dialog state tracking
    using conditional random fields. In: Proceedings of the SIGDIAL 2013 Conference,
    pp 457–461'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 (2013) Ren H, Xu W, Zhang Y, Yan Y (2013) 使用条件随机场的对话状态追踪。在：SIGDIAL 2013
    会议论文集，pp 457–461
- en: Ren et al. (2018) Ren L, Xie K, Chen L, Yu K (2018) Towards universal dialogue
    state tracking. arXiv preprint arXiv:181009587
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 (2018) Ren L, Xie K, Chen L, Yu K (2018) 朝着通用对话状态追踪迈进。arXiv 预印本 arXiv:181009587
- en: Ren et al. (2020) Ren P, Chen Z, Ren Z, Kanoulas E, Monz C, de Rijke M (2020)
    Conversations with search engines. arXiv preprint arXiv:200414162
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等 (2020) Ren P, Chen Z, Ren Z, Kanoulas E, Monz C, de Rijke M (2020) 与搜索引擎的对话。arXiv
    预印本 arXiv:200414162
- en: 'Ritter et al. (2011) Ritter A, Cherry C, Dolan WB (2011) Data-driven response
    generation in social media. In: Proceedings of the 2011 Conference on Empirical
    Methods in Natural Language Processing, pp 583–593'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritter 等 (2011) Ritter A, Cherry C, Dolan WB (2011) 社交媒体中的数据驱动响应生成。在：2011年自然语言处理经验方法会议论文集，pp
    583–593
- en: 'Rodriguez et al. (2020) Rodriguez P, Crook P, Moon S, Wang Z (2020) Information
    seeking in the spirit of learning: a dataset for conversational curiosity. arXiv
    preprint arXiv:200500172'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rodriguez 等 (2020) Rodriguez P, Crook P, Moon S, Wang Z (2020) 以学习精神进行信息寻求：一个对话好奇心数据集。arXiv
    预印本 arXiv:200500172
- en: 'Saha et al. (2018) Saha A, Khapra M, Sankaranarayanan K (2018) Towards building
    large scale multimodal domain-aware conversation systems. In: Proceedings of the
    AAAI Conference on Artificial Intelligence, vol 32, no 1'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha 等 (2018) Saha A, Khapra M, Sankaranarayanan K (2018) 朝着构建大规模多模态领域感知对话系统迈进。在：AAAI
    人工智能会议论文集，第32卷，第1期
- en: 'Saha et al. (2020) Saha T, Patra A, Saha S, Bhattacharyya P (2020) Towards
    emotion-aided multi-modal dialogue act classification. In: Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, pp 4361–4372'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha 等 (2020) Saha T, Patra A, Saha S, Bhattacharyya P (2020) 朝着情感辅助的多模态对话行为分类迈进。在：第58届计算语言学协会年会论文集，pp
    4361–4372
- en: Sankar et al. (2019) Sankar C, Subramanian S, Pal C, Chandar S, Bengio Y (2019)
    Do neural dialog systems use the conversation history effectively? an empirical
    study. arXiv preprint arXiv:190601603
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sankar 等 (2019) Sankar C, Subramanian S, Pal C, Chandar S, Bengio Y (2019) 神经对话系统是否有效利用对话历史？一项实证研究。arXiv
    预印本 arXiv:190601603
- en: Santhanam and Shaikh (2019) Santhanam S, Shaikh S (2019) A survey of natural
    language generation techniques with a focus on dialogue systems-past, present
    and future directions. arXiv preprint arXiv:190600500
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santhanam 和 Shaikh (2019) Santhanam S, Shaikh S (2019) 自然语言生成技术的调查，重点关注对话系统——过去、现在及未来方向。arXiv
    预印本 arXiv:190600500
- en: 'Sarikaya et al. (2011) Sarikaya R, Hinton GE, Ramabhadran B (2011) Deep belief
    nets for natural language call-routing. In: 2011 IEEE International conference
    on acoustics, speech and signal processing (ICASSP), IEEE, pp 5680–5683'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarikaya 等 (2011) Sarikaya R, Hinton GE, Ramabhadran B (2011) 用于自然语言呼叫路由的深度信念网络。在：2011年IEEE国际声学、语音与信号处理会议（ICASSP），IEEE，pp
    5680–5683
- en: Sarikaya et al. (2014) Sarikaya R, Hinton GE, Deoras A (2014) Application of
    deep belief networks for natural language understanding. IEEE/ACM Transactions
    on Audio, Speech, and Language Processing 22(4):778–784
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarikaya 等 (2014) Sarikaya R, Hinton GE, Deoras A (2014) 深度信念网络在自然语言理解中的应用。IEEE/ACM
    音频、语音与语言处理学报 22(4):778–784
- en: Sato et al. (2020) Sato S, Akama R, Ouchi H, Suzuki J, Inui K (2020) Evaluating
    dialogue generation systems via response selection. arXiv preprint arXiv:200414302
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sato 等 (2020) Sato S, Akama R, Ouchi H, Suzuki J, Inui K (2020) 通过响应选择评估对话生成系统。arXiv
    预印本 arXiv:200414302
- en: Schatzmann and Young (2009) Schatzmann J, Young S (2009) The hidden agenda user
    simulation model. IEEE transactions on audio, speech, and language processing
    17(4):733–747
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schatzmann 和 Young (2009) Schatzmann J, Young S (2009) 隐藏议程用户模拟模型。IEEE 音频、语音与语言处理学报
    17(4):733–747
- en: Schuster and Paliwal (1997) Schuster M, Paliwal KK (1997) Bidirectional recurrent
    neural networks. IEEE transactions on Signal Processing 45(11):2673–2681
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schuster 和 Paliwal (1997) Schuster M, Paliwal KK (1997) 双向递归神经网络。IEEE 信号处理学报
    45(11):2673–2681
- en: See et al. (2019) See A, Roller S, Kiela D, Weston J (2019) What makes a good
    conversation? how controllable attributes affect human judgments. arXiv preprint
    arXiv:190208654
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: See等（2019）See A, Roller S, Kiela D, Weston J（2019）什么样的对话是好的？可控属性如何影响人类判断。arXiv预印本
    arXiv:190208654
- en: 'Serban et al. (2016) Serban I, Sordoni A, Bengio Y, Courville A, Pineau J (2016)
    Building end-to-end dialogue systems using generative hierarchical neural network
    models. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol
    30, no 1'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serban等（2016）Serban I, Sordoni A, Bengio Y, Courville A, Pineau J（2016）使用生成层次神经网络模型构建端到端对话系统。在：AAAI人工智能会议论文集，第30卷，第1期
- en: 'Serban et al. (2017a) Serban I, Sordoni A, Lowe R, Charlin L, Pineau J, Courville
    A, Bengio Y (2017a) A hierarchical latent variable encoder-decoder model for generating
    dialogues. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 31, no 1'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serban等（2017a）Serban I, Sordoni A, Lowe R, Charlin L, Pineau J, Courville A,
    Bengio Y（2017a）用于生成对话的层次潜变量编码器-解码器模型。在：AAAI人工智能会议论文集，第31卷，第1期
- en: Serban et al. (2017b) Serban IV, Sankar C, Germain M, Zhang S, Lin Z, Subramanian
    S, Kim T, Pieper M, Chandar S, Ke NR, et al. (2017b) A deep reinforcement learning
    chatbot. arXiv preprint arXiv:170902349
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serban等（2017b）Serban IV, Sankar C, Germain M, Zhang S, Lin Z, Subramanian S,
    Kim T, Pieper M, Chandar S, Ke NR 等（2017b）一种深度强化学习聊天机器人。arXiv预印本 arXiv:170902349
- en: 'Serras et al. (2019) Serras M, Torres MI, del Pozo A (2019) Goal-conditioned
    user modeling for dialogue systems using stochastic bi-automata. In: ICPRAM, pp
    128–134'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serras等（2019）Serras M, Torres MI, del Pozo A（2019）基于目标条件的对话系统用户建模，采用随机双自动机。在：ICPRAM，第128–134页
- en: Shah et al. (2018) Shah P, Hakkani-Tür D, Tür G, Rastogi A, Bapna A, Nayak N,
    Heck L (2018) Building a conversational agent overnight with dialogue self-play.
    arXiv preprint arXiv:180104871
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah等（2018）Shah P, Hakkani-Tür D, Tür G, Rastogi A, Bapna A, Nayak N, Heck L（2018）通过对话自我游戏在一夜之间构建对话代理。arXiv预印本
    arXiv:180104871
- en: Shan et al. (2020) Shan Y, Li Z, Zhang J, Meng F, Feng Y, Niu C, Zhou J (2020)
    A contextual hierarchical attention network with adaptive objective for dialogue
    state tracking. arXiv preprint arXiv:200601554
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shan等（2020）Shan Y, Li Z, Zhang J, Meng F, Feng Y, Niu C, Zhou J（2020）一种具有自适应目标的上下文层次注意力网络，用于对话状态跟踪。arXiv预印本
    arXiv:200601554
- en: Shang et al. (2015) Shang L, Lu Z, Li H (2015) Neural responding machine for
    short-text conversation. arXiv preprint arXiv:150302364
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang等（2015）Shang L, Lu Z, Li H（2015）用于短文本对话的神经响应机器。arXiv预印本 arXiv:150302364
- en: Shao et al. (2017) Shao L, Gouws S, Britz D, Goldie A, Strope B, Kurzweil R
    (2017) Generating long and diverse responses with neural conversation models.
    ArXiv abs/1701.03185
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao等（2017）Shao L, Gouws S, Britz D, Goldie A, Strope B, Kurzweil R（2017）利用神经对话模型生成长且多样的回应。ArXiv
    abs/1701.03185
- en: 'Shao and Nakashole (2020) Shao Y, Nakashole N (2020) Chartdialogs: Plotting
    from natural language instructions. In: Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics, pp 3559–3574'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao和Nakashole（2020）Shao Y, Nakashole N（2020）Chartdialogs：从自然语言指令绘制图表。在：第58届计算语言学协会年会论文集，第3559–3574页
- en: Shen et al. (2019) Shen L, Feng Y, Zhan H (2019) Modeling semantic relationship
    in multi-turn conversations with hierarchical latent variables. arXiv preprint
    arXiv:190607429
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen等（2019）Shen L, Feng Y, Zhan H（2019）在多轮对话中利用层次潜变量建模语义关系。arXiv预印本 arXiv:190607429
- en: 'Shi and Weninger (2017) Shi B, Weninger T (2017) Proje: Embedding projection
    for knowledge graph completion. In: Proceedings of the AAAI Conference on Artificial
    Intelligence, vol 31, no 1'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi和Weninger（2017）Shi B, Weninger T（2017）Proje：用于知识图谱补全的嵌入投影。在：AAAI人工智能会议论文集，第31卷，第1期
- en: 'Shuster et al. (2018) Shuster K, Humeau S, Bordes A, Weston J (2018) Image
    chat: Engaging grounded conversations. arXiv preprint arXiv:181100945'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuster等（2018）Shuster K, Humeau S, Bordes A, Weston J（2018）图像聊天：引人入胜的基础对话。arXiv预印本
    arXiv:181100945
- en: 'Shuster et al. (2019) Shuster K, Ju D, Roller S, Dinan E, Boureau YL, Weston
    J (2019) The dialogue dodecathlon: Open-domain knowledge and image grounded conversational
    agents. arXiv preprint arXiv:191103768'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuster等（2019）Shuster K, Ju D, Roller S, Dinan E, Boureau YL, Weston J（2019）对话十二项全能：开放领域知识和图像基础的对话代理。arXiv预印本
    arXiv:191103768
- en: 'Shuster et al. (2020) Shuster K, Humeau S, Bordes A, Weston J (2020) Image-chat:
    Engaging grounded conversations. In: Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics, Association for Computational Linguistics,
    Online, pp 2414–2429, DOI 10.18653/v1/2020.acl-main.219, URL [https://www.aclweb.org/anthology/2020.acl-main.219](https://www.aclweb.org/anthology/2020.acl-main.219)'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shuster et al. (2020) Shuster K, Humeau S, Bordes A, Weston J (2020) 图像聊天：引人入胜的现场对话。在：第58届计算语言学年会论文集，计算语言学协会，在线，2414-2429页，DOI 10.18653/v1/2020.acl-main.219，URL
    [https://www.aclweb.org/anthology/2020.acl-main.219](https://www.aclweb.org/anthology/2020.acl-main.219)
- en: Siddharthan (2001) Siddharthan A (2001) Ehud reiter and robert dale. building
    natural language generation systems. Natural Language Engineering 7(3):271
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Siddharthan (2001) Siddharthan A (2001) Ehud Reiter和Robert Dale. 构建自然语言生成系统。《自然语言工程》7(3)：271
- en: Silver et al. (2016) Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche
    G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M, et al. (2016) Mastering
    the game of go with deep neural networks and tree search. nature 529(7587):484–489
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver et al. (2016) Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche
    G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M等（2016）通过深度神经网络和树搜索掌握围棋。自然529(7587)：484-489
- en: Simonyan and Zisserman (2014) Simonyan K, Zisserman A (2014) Very deep convolutional
    networks for large-scale image recognition. arXiv preprint arXiv:14091556
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan and Zisserman (2014) Simonyan K, Zisserman A (2014) 大规模图像识别的非常深层卷积网络。arXiv预印本arXiv:14091556
- en: Singh et al. (2020) Singh A, Goswami V, Parikh D (2020) Are we pretraining it
    right? digging deeper into visio-linguistic pretraining. arXiv preprint arXiv:200408744
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2020) Singh A, Goswami V, Parikh D (2020) 我们是否正确地预训练了它？更深入地挖掘视觉语言预训练。arXiv预印本arXiv:200408744
- en: 'Singh et al. (2002) Singh S, Litman D, Kearns M, Walker M (2002) Optimizing
    dialogue management with reinforcement learning: Experiments with the njfun system.
    Journal of Artificial Intelligence Research 16:105–133'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. (2002) Singh S, Litman D, Kearns M, Walker M (2002) 用强化学习优化对话管理：njfun系统的实验。《人工智能研究杂志》16：105-133
- en: 'Singla et al. (2020) Singla K, Chen Z, Atkins D, Narayanan S (2020) Towards
    end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy
    conversations. In: Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, pp 3797–3803'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singla et al. (2020) Singla K, Chen Z, Atkins D, Narayanan S (2020) 从心理治疗对话中预测行为代码的端到端学习近在眼前。在：第58届计算语言学年会论文集，3797-3803页
- en: Sinha et al. (2020) Sinha K, Parthasarathi P, Wang J, Lowe R, Hamilton WL, Pineau
    J (2020) Learning an unreferenced metric for online dialogue evaluation. arXiv
    preprint arXiv:200500583
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sinha et al. (2020) Sinha K, Parthasarathi P, Wang J, Lowe R, Hamilton WL, Pineau
    J (2020) 学习无参考度量进行在线对话评估。arXiv预印本arXiv：200500583
- en: 'Smith et al. (2020) Smith EM, Williamson M, Shuster K, Weston J, Boureau YL
    (2020) Can you put it all together: Evaluating conversational agents’ ability
    to blend skills. arXiv preprint arXiv:200408449'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith et al. (2020) Smith EM, Williamson M, Shuster K, Weston J, Boureau YL
    (2020) 能否将其整合起来：评估会话代理在技能融合方面的能力。arXiv预印本arXiv:200408449
- en: 'Socher et al. (2013) Socher R, Chen D, Manning CD, Ng A (2013) Reasoning with
    neural tensor networks for knowledge base completion. In: Advances in neural information
    processing systems, Citeseer, pp 926–934'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher et al. (2013) Socher R, Chen D, Manning CD, Ng A (2013) 用神经张量网络推理完成知识库补全。在：神经信息处理系统的进展，Citeseer，926-934页
- en: 'Song et al. (2020a) Song H, Wang Y, Zhang WN, Liu X, Liu T (2020a) Generate,
    delete and rewrite: A three-stage framework for improving persona consistency
    of dialogue generation. arXiv preprint arXiv:200407672'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020a) Song H, Wang Y, Zhang WN, Liu X, Liu T (2020a) 生成、删除和重写：提高对话生成的个人一致性的三阶段框架。arXiv预印本arXiv:200407672
- en: Song et al. (2020b) Song H, Wang Y, Zhang WN, Zhao Z, Liu T, Liu X (2020b) Profile
    consistency identification for open-domain dialogue agents. arXiv preprint arXiv:200909680
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2020b) Song H, Wang Y, Zhang WN, Zhao Z, Liu T, Liu X (2020b) 用于开放域对话代理的个人一致性识别。arXiv预印本arXiv:200909680
- en: 'Song et al. (2016) Song Y, Yan R, Li X, Zhao D, Zhang M (2016) Two are better
    than one: An ensemble of retrieval-and generation-based dialog systems. arXiv
    preprint arXiv:161007149'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2016) Song Y, Yan R, Li X, Zhao D, Zhang M (2016) 优于一的是两个：一个基于检索和生成的对话系统的整体。arXiv预印本arXiv:161007149
- en: 'Song et al. (2019) Song Z, Zheng X, Liu L, Xu M, Huang XJ (2019) Generating
    responses with a specific emotion in dialog. In: Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics, pp 3685–3695'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2019) Song Z, Zheng X, Liu L, Xu M, Huang XJ (2019) 在对话中生成具有特定情感的回应。载于：第57届计算语言学协会年会论文集，第3685–3695页
- en: 'Sordoni et al. (2015a) Sordoni A, Bengio Y, Vahabi H, Lioma C, Grue Simonsen
    J, Nie JY (2015a) A hierarchical recurrent encoder-decoder for generative context-aware
    query suggestion. In: Proceedings of the 24th ACM International on Conference
    on Information and Knowledge Management, pp 553–562'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sordoni et al. (2015a) Sordoni A, Bengio Y, Vahabi H, Lioma C, Grue Simonsen
    J, Nie JY (2015a) 一种层次递归编码器-解码器用于生成上下文感知查询建议。载于：第24届ACM国际信息与知识管理会议论文集，第553–562页
- en: Sordoni et al. (2015b) Sordoni A, Galley M, Auli M, Brockett C, Ji Y, Mitchell
    M, Nie JY, Gao J, Dolan B (2015b) A neural network approach to context-sensitive
    generation of conversational responses. arXiv preprint arXiv:150606714
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sordoni et al. (2015b) Sordoni A, Galley M, Auli M, Brockett C, Ji Y, Mitchell
    M, Nie JY, Gao J, Dolan B (2015b) 基于神经网络的上下文敏感对话生成。arXiv 预印本 arXiv:150606714
- en: 'Stasaski et al. (2020) Stasaski K, Yang GH, Hearst MA (2020) More diverse dialogue
    datasets via diversity-informed data collection. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 4958–4968'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stasaski et al. (2020) Stasaski K, Yang GH, Hearst MA (2020) 通过多样性信息数据收集获取更多多样化对话数据集。载于：第58届计算语言学协会年会论文集，第4958–4968页
- en: 'Stent et al. (2005) Stent A, Marge M, Singhai M (2005) Evaluating evaluation
    methods for generation in the presence of variation. In: international conference
    on intelligent text processing and computational linguistics, Springer, pp 341–351'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stent et al. (2005) Stent A, Marge M, Singhai M (2005) 评估生成中的评估方法以应对变异。载于：国际智能文本处理与计算语言学会议，Springer，第341–351页
- en: Su et al. (2019a) Su H, Shen X, Zhang R, Sun F, Hu P, Niu C, Zhou J (2019a)
    Improving multi-turn dialogue modelling with utterance rewriter. arXiv preprint
    arXiv:190607004
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2019a) Su H, Shen X, Zhang R, Sun F, Hu P, Niu C, Zhou J (2019a)
    通过话语重写器提升多轮对话建模。arXiv 预印本 arXiv:190607004
- en: Su et al. (2020) Su H, Shen X, Zhao S, Zhou X, Hu P, Zhong R, Niu C, Zhou J
    (2020) Diversifying dialogue generation with non-conversational text. arXiv preprint
    arXiv:200504346
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2020) Su H, Shen X, Zhao S, Zhou X, Hu P, Zhong R, Niu C, Zhou J
    (2020) 通过非对话文本多样化对话生成。arXiv 预印本 arXiv:200504346
- en: 'Su et al. (2015) Su PH, Vandyke D, Gasic M, Kim D, Mrksic N, Wen TH, Young
    S (2015) Learning from real users: Rating dialogue success with neural networks
    for reinforcement learning in spoken dialogue systems. arXiv preprint arXiv:150803386'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2015) Su PH, Vandyke D, Gasic M, Kim D, Mrksic N, Wen TH, Young S
    (2015) 从真实用户学习：通过神经网络在语音对话系统中进行强化学习的对话成功率评估。arXiv 预印本 arXiv:150803386
- en: Su et al. (2016) Su PH, Gasic M, Mrksic N, Rojas-Barahona L, Ultes S, Vandyke
    D, Wen TH, Young S (2016) Continuously learning neural dialogue management. arXiv
    preprint arXiv:160602689
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2016) Su PH, Gasic M, Mrksic N, Rojas-Barahona L, Ultes S, Vandyke
    D, Wen TH, Young S (2016) 持续学习的神经对话管理。arXiv 预印本 arXiv:160602689
- en: Su et al. (2019b) Su SY, Huang CW, Chen YN (2019b) Dual supervised learning
    for natural language understanding and generation. arXiv preprint arXiv:190506196
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2019b) Su SY, Huang CW, Chen YN (2019b) 用于自然语言理解和生成的双重监督学习。arXiv
    预印本 arXiv:190506196
- en: 'Su et al. (2019c) Su W, Zhu X, Cao Y, Li B, Lu L, Wei F, Dai J (2019c) Vl-bert:
    Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:190808530'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Su et al. (2019c) Su W, Zhu X, Cao Y, Li B, Lu L, Wei F, Dai J (2019c) Vl-bert:
    预训练通用视觉语言表示。arXiv 预印本 arXiv:190808530'
- en: Sukhbaatar et al. (2015) Sukhbaatar S, Szlam A, Weston J, Fergus R (2015) End-to-end
    memory networks. arXiv preprint arXiv:150308895
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar et al. (2015) Sukhbaatar S, Szlam A, Weston J, Fergus R (2015) 端到端记忆网络。arXiv
    预印本 arXiv:150308895
- en: Sun et al. (2019a) Sun C, Baradel F, Murphy K, Schmid C (2019a) Learning video
    representations using contrastive bidirectional transformer. arXiv preprint arXiv:190605743
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019a) Sun C, Baradel F, Murphy K, Schmid C (2019a) 使用对比双向变换器学习视频表示。arXiv
    预印本 arXiv:190605743
- en: 'Sun et al. (2019b) Sun C, Myers A, Vondrick C, Murphy K, Schmid C (2019b) Videobert:
    A joint model for video and language representation learning. In: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp 7464–7473'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2019b) Sun C, Myers A, Vondrick C, Murphy K, Schmid C (2019b) Videobert:
    视频与语言表示学习的联合模型。载于：IEEE/CVF国际计算机视觉会议论文集，第7464–7473页'
- en: Sutskever et al. (2014) Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence
    learning with neural networks. arXiv preprint arXiv:14093215
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2014）Sutskever I, Vinyals O, Le QV（2014）使用神经网络的序列到序列学习。arXiv 预印本
    arXiv:14093215
- en: Sutton (1988) Sutton RS (1988) Learning to predict by the methods of temporal
    differences. Machine learning 3(1):9–44
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton（1988）Sutton RS（1988）通过时间差分方法进行预测学习。机器学习 3（1）：第9–44页
- en: 'Sutton et al. (1999) Sutton RS, McAllester DA, Singh SP, Mansour Y, et al.
    (1999) Policy gradient methods for reinforcement learning with function approximation.
    In: NIPs, Citeseer, vol 99, pp 1057–1063'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 等（1999）Sutton RS, McAllester DA, Singh SP, Mansour Y 等（1999）带有函数逼近的强化学习中的策略梯度方法。发表于NIPs，Citeseer，第99卷，第1057–1063页
- en: 'Szegedy et al. (2015) Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov
    D, Erhan D, Vanhoucke V, Rabinovich A (2015) Going deeper with convolutions. In:
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp 1–9'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2015）Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan
    D, Vanhoucke V, Rabinovich A（2015）通过卷积深入探究。发表于IEEE计算机视觉与模式识别会议论文集，第1–9页
- en: Takanobu et al. (2020) Takanobu R, Liang R, Huang M (2020) Multi-agent task-oriented
    dialog policy learning with role-aware reward decomposition. arXiv preprint arXiv:200403809
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takanobu 等（2020）Takanobu R, Liang R, Huang M（2020）基于角色感知奖励分解的多智能体任务导向对话策略学习。arXiv
    预印本 arXiv:200403809
- en: 'Takmaz et al. (2020) Takmaz E, Giulianelli M, Pezzelle S, Sinclair A, Fernández
    R (2020) Refer, reuse, reduce: Generating subsequent references in visual and
    conversational contexts. arXiv preprint arXiv:201104554'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takmaz 等（2020）Takmaz E, Giulianelli M, Pezzelle S, Sinclair A, Fernández R（2020）参考、重用、减少：在视觉和对话上下文中生成后续引用。arXiv
    预印本 arXiv:201104554
- en: Tamar et al. (2016) Tamar A, Wu Y, Thomas G, Levine S, Abbeel P (2016) Value
    iteration networks. arXiv preprint arXiv:160202867
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tamar 等（2016）Tamar A, Wu Y, Thomas G, Levine S, Abbeel P（2016）价值迭代网络。arXiv 预印本
    arXiv:160202867
- en: 'Tan and Bansal (2019) Tan H, Bansal M (2019) Lxmert: Learning cross-modality
    encoder representations from transformers. arXiv preprint arXiv:190807490'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 和 Bansal（2019）Tan H, Bansal M（2019）Lxmert：从变换器学习跨模态编码器表示。arXiv 预印本 arXiv:190807490
- en: Tanana et al. (2016) Tanana M, Hallgren KA, Imel ZE, Atkins DC, Srikumar V (2016)
    A comparison of natural language processing methods for automated coding of motivational
    interviewing. Journal of substance abuse treatment 65:43–50
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanana 等（2016）Tanana M, Hallgren KA, Imel ZE, Atkins DC, Srikumar V（2016）自动编码动机面谈的自然语言处理方法比较。物质滥用治疗杂志
    65：第43–50页
- en: 'Tang et al. (2015) Tang D, Qin B, Liu T (2015) Learning semantic representations
    of users and products for document level sentiment classification. In: Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), pp 1014–1023'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等（2015）Tang D, Qin B, Liu T（2015）学习用户和产品的语义表示用于文档级情感分类。发表于第53届计算语言学协会年会暨第7届国际自然语言处理联合会议论文集（第1卷：长篇论文），第1014–1023页
- en: Tang et al. (2019) Tang J, Zhao T, Xiong C, Liang X, Xing EP, Hu Z (2019) Target-guided
    open-domain conversation. arXiv preprint arXiv:190511553
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等（2019）Tang J, Zhao T, Xiong C, Liang X, Xing EP, Hu Z（2019）目标引导的开放域对话。arXiv
    预印本 arXiv:190511553
- en: 'Tao et al. (2018) Tao C, Mou L, Zhao D, Yan R (2018) Ruber: An unsupervised
    method for automatic evaluation of open-domain dialog systems. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 32, no 1'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等（2018）Tao C, Mou L, Zhao D, Yan R（2018）Ruber：一种用于开放域对话系统自动评估的无监督方法。发表于AAAI人工智能会议论文集，第32卷，第1期
- en: 'Tao et al. (2019) Tao C, Wu W, Xu C, Hu W, Zhao D, Yan R (2019) One time of
    interaction may not be enough: Go deep with an interaction-over-interaction network
    for response selection in dialogues. In: Proceedings of the 57th annual meeting
    of the association for computational linguistics, pp 1–11'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等（2019）Tao C, Wu W, Xu C, Hu W, Zhao D, Yan R（2019）一次交互可能不够：通过交互-交互网络深入探讨对话中的回应选择。发表于第57届计算语言学协会年会论文集，第1–11页
- en: Tay et al. (2019) Tay Y, Wang S, Tuan LA, Fu J, Phan MC, Yuan X, Rao J, Hui
    SC, Zhang A (2019) Simple and effective curriculum pointer-generator networks
    for reading comprehension over long narratives. arXiv preprint arXiv:190510847
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay 等（2019）Tay Y, Wang S, Tuan LA, Fu J, Phan MC, Yuan X, Rao J, Hui SC, Zhang
    A（2019）简单而有效的课程指针生成网络用于长篇叙事阅读理解。arXiv 预印本 arXiv:190510847
- en: 'Tay et al. (2020) Tay Y, Dehghani M, Bahri D, Metzler D (2020) Efficient transformers:
    A survey. arXiv preprint arXiv:200906732'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tay 等（2020）Tay Y, Dehghani M, Bahri D, Metzler D（2020）高效的变换器：综述。arXiv 预印本 arXiv:200906732
- en: 'Theune (2003) Theune M (2003) Natural language generation for dialogue: system
    survey. Centre for Telematics and Information Technology, University of Twente'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theune (2003) Theune M (2003) 《对话的自然语言生成：系统综述》。特门大学电信与信息技术中心
- en: 'Thomas et al. (2006) Thomas M, Pang B, Lee L (2006) Get out the vote: Determining
    support or opposition from congressional floor-debate transcripts. arXiv preprint
    cs/0607062'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomas 等人 (2006) Thomas M, Pang B, Lee L (2006) 《投票支持或反对：从国会辩论记录中确定立场》。arXiv
    预印本 cs/0607062
- en: 'Tian et al. (2019) Tian Z, Bi W, Li X, Zhang NL (2019) Learning to abstract
    for memory-augmented conversational response generation. In: Proceedings of the
    57th Annual Meeting of the Association for Computational Linguistics, pp 3816–3825'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian 等人 (2019) Tian Z, Bi W, Li X, Zhang NL (2019) 《学习抽象以增强记忆的对话回应生成》。发表于：第
    57 届计算语言学协会年会论文集，第 3816–3825 页
- en: 'Tiedemann (2012) Tiedemann J (2012) Parallel data, tools and interfaces in
    opus. In: Lrec, vol 2012, pp 2214–2218'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiedemann (2012) Tiedemann J (2012) 《Opus 中的平行数据、工具和接口》。发表于：Lrec，卷 2012，第 2214–2218
    页
- en: 'Tonelli et al. (2010) Tonelli S, Riccardi G, Prasad R, Joshi AK (2010) Annotation
    of discourse relations for conversational spoken dialogs. In: LREC'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tonelli 等人 (2010) Tonelli S, Riccardi G, Prasad R, Joshi AK (2010) 《对话发言中的话语关系注释》。发表于：LREC
- en: 'Tran and Nguyen (2017) Tran VK, Nguyen LM (2017) Semantic refinement gru-based
    neural language generation for spoken dialogue systems. In: International Conference
    of the Pacific Association for Computational Linguistics, Springer, pp 63–75'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran 和 Nguyen (2017) Tran VK, Nguyen LM (2017) 《语义细化的基于 GRU 的神经语言生成用于口语对话系统》。发表于：国际计算语言学协会太平洋会议，Springer，第
    63–75 页
- en: 'Tur et al. (2010) Tur G, Hakkani-Tür D, Heck L (2010) What is left to be understood
    in atis? In: 2010 IEEE Spoken Language Technology Workshop, IEEE, pp 19–24'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tur 等人 (2010) Tur G, Hakkani-Tür D, Heck L (2010) 《在 ATIS 中还剩下什么需要理解的？》。发表于：2010
    年 IEEE 语音语言技术研讨会，IEEE，第 19–24 页
- en: 'Tur et al. (2012) Tur G, Deng L, Hakkani-Tür D, He X (2012) Towards deeper
    understanding: Deep convex networks for semantic utterance classification. In:
    2012 IEEE international conference on acoustics, speech and signal processing
    (ICASSP), IEEE, pp 5045–5048'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tur 等人 (2012) Tur G, Deng L, Hakkani-Tür D, He X (2012) 《迈向更深的理解：用于语义发言分类的深度凸网络》。发表于：2012
    年 IEEE 国际声学、语音与信号处理会议（ICASSP），IEEE，第 5045–5048 页
- en: 'Ultes et al. (2017) Ultes S, Barahona LMR, Su PH, Vandyke D, Kim D, Casanueva
    I, Budzianowski P, Mrkšić N, Wen TH, Gasic M, et al. (2017) Pydial: A multi-domain
    statistical dialogue system toolkit. In: Proceedings of ACL 2017, System Demonstrations,
    pp 73–78'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ultes 等人 (2017) Ultes S, Barahona LMR, Su PH, Vandyke D, Kim D, Casanueva I,
    Budzianowski P, Mrkšić N, Wen TH, Gasic M, 等 (2017) 《Pydial：多领域统计对话系统工具包》。发表于：ACL
    2017 会议论文集，系统演示，第 73–78 页
- en: Urbanek et al. (2019) Urbanek J, Fan A, Karamcheti S, Jain S, Humeau S, Dinan
    E, Rocktäschel T, Kiela D, Szlam A, Weston J (2019) Learning to speak and act
    in a fantasy text adventure game. arXiv preprint arXiv:190303094
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urbanek 等人 (2019) Urbanek J, Fan A, Karamcheti S, Jain S, Humeau S, Dinan E,
    Rocktäschel T, Kiela D, Szlam A, Weston J (2019) 《学习在幻想文字冒险游戏中说话和行动》。arXiv 预印本
    arXiv:190303094
- en: 'Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser Lu, Polosukhin I (2017) Attention is all you need. In: Guyon
    I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds)
    Advances in Neural Information Processing Systems, Curran Associates, Inc., vol 30,
    URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez
    AN, Kaiser Lu, Polosukhin I (2017) 《注意力即一切》。发表于：Guyon I, Luxburg UV, Bengio S,
    Wallach H, Fergus R, Vishwanathan S, Garnett R (编) 《神经信息处理系统进展》，Curran Associates,
    Inc., 卷 30，URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- en: 'Vijayakumar et al. (2016) Vijayakumar AK, Cogswell M, Selvaraju RR, Sun Q,
    Lee S, Crandall D, Batra D (2016) Diverse beam search: Decoding diverse solutions
    from neural sequence models. arXiv preprint arXiv:161002424'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vijayakumar 等人 (2016) Vijayakumar AK, Cogswell M, Selvaraju RR, Sun Q, Lee S,
    Crandall D, Batra D (2016) 《多样化束搜索：从神经序列模型中解码多样化解决方案》。arXiv 预印本 arXiv:161002424
- en: Vinyals and Le (2015) Vinyals O, Le Q (2015) A neural conversational model.
    arXiv preprint arXiv:150605869
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 和 Le (2015) Vinyals O, Le Q (2015) 《一种神经对话模型》。arXiv 预印本 arXiv:150605869
- en: Viterbi (1967) Viterbi A (1967) Error bounds for convolutional codes and an
    asymptotically optimum decoding algorithm. IEEE transactions on Information Theory
    13(2):260–269
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Viterbi (1967) Viterbi A (1967) 《卷积码的误差界限和渐近最优解码算法》。IEEE 信息理论交易 13(2):260–269
- en: 'Vougiouklis et al. (2016) Vougiouklis P, Hare J, Simperl E (2016) A neural
    network approach for knowledge-driven response generation. In: Proceedings of
    COLING 2016, the 26th International Conference on Computational Linguistics: Technical
    Papers, The COLING 2016 Organizing Committee, Osaka, Japan, pp 3370–3380, URL
    [https://www.aclweb.org/anthology/C16-1318](https://www.aclweb.org/anthology/C16-1318)'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vougiouklis et al. (2016) Vougiouklis P, Hare J, Simperl E (2016) 基于知识驱动的回应生成的神经网络方法。见：COLING
    2016，第26届计算语言学国际会议：技术论文，COLING 2016组织委员会，大阪，日本，第3370–3380页，网址 [https://www.aclweb.org/anthology/C16-1318](https://www.aclweb.org/anthology/C16-1318)。
- en: 'Walker et al. (1997) Walker MA, Litman DJ, Kamm CA, Abella A (1997) Paradise:
    A framework for evaluating spoken dialogue agents. arXiv preprint cmp-lg/9704004'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walker et al. (1997) Walker MA, Litman DJ, Kamm CA, Abella A (1997) Paradise：评估口语对话代理的框架。arXiv预印本
    cmp-lg/9704004。
- en: 'Wan and McAuley (2016) Wan M, McAuley J (2016) Modeling ambiguity, subjectivity,
    and diverging viewpoints in opinion question answering systems. In: 2016 IEEE
    16th international conference on data mining (ICDM), IEEE, pp 489–498'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan and McAuley (2016) Wan M, McAuley J (2016) 在意见问答系统中建模歧义、主观性和不同观点。见：2016年IEEE第16届数据挖掘国际会议（ICDM），IEEE，第489–498页。
- en: 'Wang et al. (2020a) Wang H, Peng B, Wong KF (2020a) Learning efficient dialogue
    policy from demonstrations through shaping. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 6355–6365'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Wang H, Peng B, Wong KF (2020a) 从示范中学习高效的对话策略。见：第58届计算语言学协会年会论文集，第6355–6365页。
- en: Wang et al. (2020b) Wang K, Tian J, Wang R, Quan X, Yu J (2020b) Multi-domain
    dialogue acts and response co-generation. arXiv preprint arXiv:200412363
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020b) Wang K, Tian J, Wang R, Quan X, Yu J (2020b) 多领域对话行为和回应共同生成。arXiv预印本
    arXiv:200412363。
- en: 'Wang et al. (2020c) Wang L, Li J, Zeng X, Zhang H, Wong KF (2020c) Continuity
    of topic, interaction, and query: Learning to quote in online conversations. In:
    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP), pp 6640–6650'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020c) Wang L, Li J, Zeng X, Zhang H, Wong KF (2020c) 话题、互动与查询的连续性：学习在在线对话中引用。见：2020年自然语言处理实证方法会议论文集（EMNLP），第6640–6650页。
- en: 'Wang et al. (2020d) Wang S, Zhou K, Lai K, Shen J (2020d) Task-completion dialogue
    policy learning via Monte Carlo tree search with dueling network. In: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    Association for Computational Linguistics, Online, pp 3461–3471, DOI 10.18653/v1/2020.emnlp-main.278,
    URL [https://www.aclweb.org/anthology/2020.emnlp-main.278](https://www.aclweb.org/anthology/2020.emnlp-main.278)'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020d) Wang S, Zhou K, Lai K, Shen J (2020d) 通过蒙特卡罗树搜索与对抗网络进行任务完成对话策略学习。见：2020年自然语言处理实证方法会议论文集（EMNLP），计算语言学协会，在线，第3461–3471页，DOI
    10.18653/v1/2020.emnlp-main.278，网址 [https://www.aclweb.org/anthology/2020.emnlp-main.278](https://www.aclweb.org/anthology/2020.emnlp-main.278)。
- en: Wang et al. (2019a) Wang W, Zhang J, Li Q, Hwang MY, Zong C, Li Z (2019a) Incremental
    learning from scratch for task-oriented dialogue systems. arXiv preprint arXiv:190604991
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019a) Wang W, Zhang J, Li Q, Hwang MY, Zong C, Li Z (2019a) 从头开始的增量学习用于任务导向对话系统。arXiv预印本
    arXiv:190604991。
- en: Wang and Yuan (2016) Wang X, Yuan C (2016) Recent advances on human-computer
    dialogue. CAAI Transactions on Intelligence Technology 1(4):303–312
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Yuan (2016) Wang X, Yuan C (2016) 人机对话的最新进展。CAAI智能技术学报 1(4):303–312。
- en: 'Wang et al. (2019b) Wang X, Shi W, Kim R, Oh Y, Yang S, Zhang J, Yu Z (2019b)
    Persuasion for good: Towards a personalized persuasive dialogue system for social
    good. arXiv preprint arXiv:190606725'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019b) Wang X, Shi W, Kim R, Oh Y, Yang S, Zhang J, Yu Z (2019b)
    为善而劝说：面向社会公益的个性化劝说对话系统。arXiv预印本 arXiv:190606725。
- en: 'Wang et al. (2020e) Wang Y, Guo Y, Zhu S (2020e) Slot attention with value
    normalization for multi-domain dialogue state tracking. In: Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp
    3019–3028'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020e) Wang Y, Guo Y, Zhu S (2020e) 用于多领域对话状态跟踪的槽注意力与值归一化。见：2020年自然语言处理实证方法会议论文集（EMNLP），第3019–3028页。
- en: 'Wang et al. (2020f) Wang Y, Joty S, Lyu M, King I, Xiong C, Hoi SC (2020f)
    VD-BERT: A Unified Vision and Dialog Transformer with BERT. In: Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    Association for Computational Linguistics, Online, pp 3325–3338, DOI 10.18653/v1/2020.emnlp-main.269,
    URL [https://www.aclweb.org/anthology/2020.emnlp-main.269](https://www.aclweb.org/anthology/2020.emnlp-main.269)'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020f）Wang Y, Joty S, Lyu M, King I, Xiong C, Hoi SC（2020f）VD-BERT：一个统一的视觉和对话
    Transformer 与 BERT。在：2020年自然语言处理实证方法会议（EMNLP）论文集，计算语言学协会，线上，页码 3325–3338，DOI 10.18653/v1/2020.emnlp-main.269，网址
    [https://www.aclweb.org/anthology/2020.emnlp-main.269](https://www.aclweb.org/anthology/2020.emnlp-main.269)
- en: 'Wang et al. (2014) Wang Z, Zhang J, Feng J, Chen Z (2014) Knowledge graph embedding
    by translating on hyperplanes. In: Proceedings of the AAAI Conference on Artificial
    Intelligence, vol 28, no 1'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2014）Wang Z, Zhang J, Feng J, Chen Z（2014）通过在超平面上转换进行知识图谱嵌入。在：AAAI 人工智能会议论文集，第28卷，第1期
- en: 'Wang et al. (2016) Wang Z, Schaul T, Hessel M, Hasselt H, Lanctot M, Freitas
    N (2016) Dueling network architectures for deep reinforcement learning. In: International
    conference on machine learning, PMLR, pp 1995–2003'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2016）Wang Z, Schaul T, Hessel M, Hasselt H, Lanctot M, Freitas N（2016）用于深度强化学习的对抗网络架构。在：国际机器学习会议，PMLR，页码
    1995–2003
- en: 'Wang et al. (2020g) Wang Z, Ho S, Cambria E (2020g) A review of emotion sensing:
    Categorization models and algorithms. Multimedia Tools and Applications 79:35553–35582'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2020g）Wang Z, Ho S, Cambria E（2020g）情感感知综述：分类模型与算法。Multimedia Tools and
    Applications 79:35553–35582
- en: Welleck et al. (2018) Welleck S, Weston J, Szlam A, Cho K (2018) Dialogue natural
    language inference. arXiv preprint arXiv:181100671
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Welleck 等（2018）Welleck S, Weston J, Szlam A, Cho K（2018）对话自然语言推理。arXiv 预印本 arXiv:181100671
- en: Wen et al. (2015a) Wen TH, Gasic M, Kim D, Mrksic N, Su PH, Vandyke D, Young
    S (2015a) Stochastic language generation in dialogue using recurrent neural networks
    with convolutional sentence reranking. arXiv preprint arXiv:150801755
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2015a）Wen TH, Gasic M, Kim D, Mrksic N, Su PH, Vandyke D, Young S（2015a）使用卷积句子重排序的递归神经网络中的随机语言生成。arXiv
    预印本 arXiv:150801755
- en: Wen et al. (2015b) Wen TH, Gasic M, Mrksic N, Su PH, Vandyke D, Young S (2015b)
    Semantically conditioned lstm-based natural language generation for spoken dialogue
    systems. arXiv preprint arXiv:150801745
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2015b）Wen TH, Gasic M, Mrksic N, Su PH, Vandyke D, Young S（2015b）用于口语对话系统的语义条件
    LSTM 基自然语言生成。arXiv 预印本 arXiv:150801745
- en: Wen et al. (2016a) Wen TH, Gasic M, Mrksic N, Rojas-Barahona LM, Su PH, Ultes
    S, Vandyke D, Young S (2016a) Conditional generation and snapshot learning in
    neural dialogue systems. arXiv preprint arXiv:160603352
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2016a）Wen TH, Gasic M, Mrksic N, Rojas-Barahona LM, Su PH, Ultes S, Vandyke
    D, Young S（2016a）神经对话系统中的条件生成与快照学习。arXiv 预印本 arXiv:160603352
- en: Wen et al. (2016b) Wen TH, Gasic M, Mrksic N, Rojas-Barahona LM, Su PH, Vandyke
    D, Young S (2016b) Multi-domain neural network language generation for spoken
    dialogue systems. arXiv preprint arXiv:160301232
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2016b）Wen TH, Gasic M, Mrksic N, Rojas-Barahona LM, Su PH, Vandyke D,
    Young S（2016b）用于口语对话系统的多领域神经网络语言生成。arXiv 预印本 arXiv:160301232
- en: Wen et al. (2016c) Wen TH, Vandyke D, Mrksic N, Gasic M, Rojas-Barahona LM,
    Su PH, Ultes S, Young S (2016c) A network-based end-to-end trainable task-oriented
    dialogue system. arXiv preprint arXiv:160404562
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等（2016c）Wen TH, Vandyke D, Mrksic N, Gasic M, Rojas-Barahona LM, Su PH,
    Ultes S, Young S（2016c）基于网络的端到端可训练任务导向对话系统。arXiv 预印本 arXiv:160404562
- en: Weston et al. (2014) Weston J, Chopra S, Bordes A (2014) Memory networks. arXiv
    preprint arXiv:14103916
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weston 等（2014）Weston J, Chopra S, Bordes A（2014）记忆网络。arXiv 预印本 arXiv:14103916
- en: 'Williams et al. (2013) Williams J, Raux A, Ramachandran D, Black A (2013) The
    dialog state tracking challenge. In: Proceedings of the SIGDIAL 2013 Conference,
    Association for Computational Linguistics, Metz, France, pp 404–413, URL [https://www.aclweb.org/anthology/W13-4065](https://www.aclweb.org/anthology/W13-4065)'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams 等（2013）Williams J, Raux A, Ramachandran D, Black A（2013）对话状态跟踪挑战。在：SIGDIAL
    2013 会议论文集，计算语言学协会，法国美茵兹，页码 404–413，网址 [https://www.aclweb.org/anthology/W13-4065](https://www.aclweb.org/anthology/W13-4065)
- en: Williams (2007) Williams JD (2007) Partially observable markov decision processes
    for spoken dialogue management. PhD thesis, University of Cambridge
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams（2007）Williams JD（2007）用于口语对话管理的部分可观察马尔可夫决策过程。博士论文，剑桥大学
- en: 'Williams (2013) Williams JD (2013) Multi-domain learning and generalization
    in dialog state tracking. In: Proceedings of the SIGDIAL 2013 Conference, pp 433–441'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams（2013）Williams JD（2013）对话状态跟踪中的多领域学习与泛化。在：SIGDIAL 2013 会议论文集，页码 433–441
- en: 'Williams (2014) Williams JD (2014) Web-style ranking and slu combination for
    dialog state tracking. In: Proceedings of the 15th Annual Meeting of the Special
    Interest Group on Discourse and Dialogue (SIGDIAL), pp 282–291'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯（2014）威廉姆斯 JD (2014) 用于对话状态跟踪的Web风格排名和SLU组合。载于：第15届话语和对话特别兴趣小组年会论文集（SIGDIAL），第282–291页
- en: Williams and Zweig (2016) Williams JD, Zweig G (2016) End-to-end lstm-based
    dialog control optimized with supervised and reinforcement learning. arXiv preprint
    arXiv:160601269
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯和齐普瑟（2016）威廉姆斯 JD, 齐普瑟 G (2016) 通过监督和强化学习优化的端到端 LSTM 对话控制。arXiv预印本 arXiv:160601269
- en: 'Williams et al. (2017) Williams JD, Asadi K, Zweig G (2017) Hybrid code networks:
    practical and efficient end-to-end dialog control with supervised and reinforcement
    learning. arXiv preprint arXiv:170203274'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯等（2017）威廉姆斯 JD, 阿萨迪 K, 齐普瑟 G (2017) 混合代码网络：具有监督学习和强化学习的实用高效端到端对话控制。arXiv预印本
    arXiv:170203274
- en: Williams (1992) Williams RJ (1992) Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Machine learning 8(3-4):229–256
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯（1992）威廉姆斯 RJ (1992) 用于连接主义强化学习的简单统计梯度跟随算法。机器学习 8(3-4):229–256
- en: Williams and Zipser (1989) Williams RJ, Zipser D (1989) A learning algorithm
    for continually running fully recurrent neural networks. Neural computation 1(2):270–280
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯和齐普瑟（1989）威廉姆斯 RJ, 齐普瑟 D (1989) 一种用于持续运行的完全递归神经网络的学习算法。神经计算 1(2):270–280
- en: Wiseman et al. (2017) Wiseman S, Shieber SM, Rush AM (2017) Challenges in data-to-document
    generation. arXiv preprint arXiv:170708052
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威斯曼等（2017）威斯曼 S, 希伯 SM, 拉什 AM (2017) 数据到文档生成中的挑战。arXiv预印本 arXiv:170708052
- en: Wu and Xiong (2020) Wu CS, Xiong C (2020) Probing task-oriented dialogue representation
    from language models. arXiv preprint arXiv:201013912
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴和熊（2020）吴 CS, 熊 C (2020) 从语言模型中探测任务导向对话表示。arXiv预印本 arXiv:201013912
- en: Wu et al. (2019a) Wu CS, Madotto A, Hosseini-Asl E, Xiong C, Socher R, Fung
    P (2019a) Transferable multi-domain state generator for task-oriented dialogue
    systems. arXiv preprint arXiv:190508743
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019a）吴 CS, 马多托 A, 霍塞尼-阿斯尔 E, 熊 C, 索切 R, 冯 P (2019a) 适用于任务导向对话系统的可转移多领域状态生成器。arXiv预印本
    arXiv:190508743
- en: 'Wu et al. (2020a) Wu CS, Hoi S, Socher R, Xiong C (2020a) Tod-bert: Pre-trained
    natural language understanding for task-oriented dialogues. arXiv preprint arXiv:200406871'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2020a）吴 CS, 侯 S, 索切 R, 熊 C (2020a) Tod-bert：用于任务导向对话的预训练自然语言理解。arXiv预印本 arXiv:200406871
- en: Wu et al. (2019b) Wu J, Wang X, Wang WY (2019b) Self-supervised dialogue learning.
    arXiv preprint arXiv:190700448
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019b）吴 J, 王 X, 王 WY (2019b) 自监督对话学习。arXiv预印本 arXiv:190700448
- en: 'Wu et al. (2020b) Wu S, Li Y, Zhang D, Zhou Y, Wu Z (2020b) Diverse and informative
    dialogue generation with context-specific commonsense knowledge awareness. In:
    Proceedings of the 58th annual meeting of the association for computational linguistics,
    pp 5811–5820'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2020b）吴 S, 李 Y, 张 D, 周 Y, 吴 Z (2020b) 具有上下文特定常识知识意识的多样化和信息丰富的对话生成。载于：第58届计算语言学协会年会论文集，第5811–5820页
- en: Wu et al. (2019c) Wu W, Guo Z, Zhou X, Wu H, Zhang X, Lian R, Wang H (2019c)
    Proactive human-machine conversation with explicit conversation goals. arXiv preprint
    arXiv:190605572
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019c）吴 W, 郭 Z, 周 X, 吴 H, 张 X, 连 R, 王 H (2019c) 具有明确对话目标的主动人机对话。arXiv预印本
    arXiv:190605572
- en: 'Wu et al. (2016) Wu Y, Wu W, Xing C, Zhou M, Li Z (2016) Sequential matching
    network: A new architecture for multi-turn response selection in retrieval-based
    chatbots. arXiv preprint arXiv:161201627'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2016）吴 Y, 吴 W, 邢 C, 周 M, 李 Z (2016) 序列匹配网络：用于基于检索的聊天机器人的多轮响应选择的新架构。arXiv预印本
    arXiv:161201627
- en: Wu et al. (2020c) Wu Z, Galley M, Brockett C, Zhang Y, Gao X, Quirk C, Koncel-Kedziorski
    R, Gao J, Hajishirzi H, Ostendorf M, et al. (2020c) A controllable model of grounded
    response generation. arXiv preprint arXiv:200500613
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2020c）吴 Z, 加利 M, 布罗凯特 C, 张 Y, 高 X, 奎克 C, 孔采尔-凯奇奥尔斯基 R, 高 J, 哈吉什尔兹 H, 奥斯滕多夫
    M, 等 (2020c) 一种可控的基础响应生成模型。arXiv预印本 arXiv:200500613
- en: 'Xiao et al. (2015) Xiao H, Huang M, Hao Y, Zhu X (2015) Transg: A generative
    mixture model for knowledge graph embedding. arXiv preprint arXiv:150905488'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2015）肖 H, 黄 M, 郝 Y, 朱 X (2015) Transg：用于知识图谱嵌入的生成混合模型。arXiv预印本 arXiv:150905488
- en: 'Xiao et al. (2017) Xiao H, Huang M, Meng L, Zhu X (2017) Ssp: semantic space
    projection for knowledge graph embedding with text descriptions. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 31, no 1'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等（2017）肖 H, 黄 M, 孟 L, 朱 X (2017) Ssp：用于知识图谱嵌入的语义空间投影，结合文本描述。载于：AAAI人工智能会议论文集，第31卷，第1期
- en: 'Xie et al. (2016) Xie R, Liu Z, Jia J, Luan H, Sun M (2016) Representation
    learning of knowledge graphs with entity descriptions. In: Proceedings of the
    AAAI Conference on Artificial Intelligence, vol 30, no 1'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2016）Xie R, Liu Z, Jia J, Luan H, Sun M（2016）带实体描述的知识图谱表示学习。在：AAAI人工智能会议论文集，第30卷，第1期
- en: 'Xing et al. (2017) Xing C, Wu W, Wu Y, Liu J, Huang Y, Zhou M, Ma WY (2017)
    Topic aware neural response generation. In: Proceedings of the AAAI Conference
    on Artificial Intelligence, vol 31, no 1'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing 等（2017）Xing C, Wu W, Wu Y, Liu J, Huang Y, Zhou M, Ma WY（2017）话题感知神经响应生成。在：AAAI人工智能会议论文集，第31卷，第1期
- en: 'Xing et al. (2018) Xing C, Wu Y, Wu W, Huang Y, Zhou M (2018) Hierarchical
    recurrent attention network for response generation. In: Proceedings of the AAAI
    Conference on Artificial Intelligence, vol 32, no 1'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xing 等（2018）Xing C, Wu Y, Wu W, Huang Y, Zhou M（2018）用于响应生成的层次递归注意力网络。在：AAAI人工智能会议论文集，第32卷，第1期
- en: Xu et al. (2019) Xu C, Wu W, Tao C, Hu H, Schuerman M, Wang Y (2019) Neural
    response generation with meta-words. arXiv preprint arXiv:190606050
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2019）Xu C, Wu W, Tao C, Hu H, Schuerman M, Wang Y（2019）带元词的神经响应生成。arXiv
    预印本 arXiv:190606050
- en: 'Xu et al. (2020a) Xu J, Wang H, Niu ZY, Wu H, Che W, Liu T (2020a) Conversational
    graph grounded policy learning for open-domain conversation generation. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    1835–1845'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020a）Xu J, Wang H, Niu ZY, Wu H, Che W, Liu T（2020a）基于对话图的策略学习用于开放领域对话生成。在：第58届计算语言学协会年会论文集，第1835–1845页
- en: Xu et al. (2020b) Xu K, Tan H, Song L, Wu H, Zhang H, Song L, Yu D (2020b) Semantic
    role labeling guided multi-turn dialogue rewriter. arXiv preprint arXiv:201001417
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020b）Xu K, Tan H, Song L, Wu H, Zhang H, Song L, Yu D（2020b）语义角色标注引导的多轮对话重写器。arXiv
    预印本 arXiv:201001417
- en: Yadollahi et al. (2017) Yadollahi A, Shahraki AG, Zaiane OR (2017) Current state
    of text sentiment analysis from opinion to emotion mining. ACM Computing Surveys
    (CSUR) 50(2):1–33
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yadollahi 等（2017）Yadollahi A, Shahraki AG, Zaiane OR（2017）从观点到情感挖掘的文本情感分析现状。ACM计算调查（CSUR）50(2):1–33
- en: 'Yang et al. (2020) Yang S, Zhang R, Erfani S (2020) Graphdialog: Integrating
    graph knowledge into end-to-end task-oriented dialogue systems. arXiv preprint
    arXiv:201001447'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2020）Yang S, Zhang R, Erfani S（2020）Graphdialog：将图知识集成到端到端任务导向对话系统中。arXiv
    预印本 arXiv:201001447
- en: 'Yann et al. (2014) Yann D, Tur G, Hakkani-Tur D, Heck L (2014) Zero-shot learning
    and clustering for semantic utterance classification using deep learning. In:
    International Conference on Learning Representations (cited on page 28)'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yann 等（2014）Yann D, Tur G, Hakkani-Tur D, Heck L（2014）利用深度学习进行语义话语分类的零样本学习和聚类。在：国际学习表征会议（引用于第28页）
- en: 'Yao et al. (2013) Yao K, Zweig G, Hwang MY, Shi Y, Yu D (2013) Recurrent neural
    networks for language understanding. In: Interspeech, pp 2524–2528'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2013）Yao K, Zweig G, Hwang MY, Shi Y, Yu D（2013）用于语言理解的递归神经网络。在：Interspeech，第2524–2528页
- en: 'Yao et al. (2014) Yao K, Peng B, Zhang Y, Yu D, Zweig G, Shi Y (2014) Spoken
    language understanding using long short-term memory neural networks. In: 2014
    IEEE Spoken Language Technology Workshop (SLT), IEEE, pp 189–194'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2014）Yao K, Peng B, Zhang Y, Yu D, Zweig G, Shi Y（2014）使用长短期记忆神经网络进行口语理解。在：2014
    IEEE口语语言技术研讨会（SLT），IEEE，第189–194页
- en: Yao et al. (2016) Yao K, Peng B, Zweig G, Wong KF (2016) An attentional neural
    conversation model with improved specificity. arXiv preprint arXiv:160601292
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2016）Yao K, Peng B, Zweig G, Wong KF（2016）具有改进特异性的注意力神经对话模型。arXiv 预印本
    arXiv:160601292
- en: 'Yih et al. (2015) Yih Wt, He X, Gao J (2015) Deep learning and continuous representations
    for natural language processing. In: Proceedings of the 2015 Conference of the
    North American Chapter of the Association for Computational Linguistics: Tutorial
    Abstracts, pp 6–8'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yih 等（2015）Yih Wt, He X, Gao J（2015）深度学习和连续表示在自然语言处理中的应用。在：第2015届北美计算语言学协会年会论文集：教程摘要，第6–8页
- en: Yin et al. (2015) Yin J, Jiang X, Lu Z, Shang L, Li H, Li X (2015) Neural generative
    question answering. arXiv preprint arXiv:151201337
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2015）Yin J, Jiang X, Lu Z, Shang L, Li H, Li X（2015）神经生成问答。arXiv 预印本 arXiv:151201337
- en: Yoshino et al. (2018) Yoshino K, Hori C, Perez J, D’Haro LF, Polymenakos L,
    Gunasekara C, Lasecki WS, Kummerfeld J, Galley M, Brockett C, et al. (2018) The
    7th dialog system technology challenge. arXiv preprint
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoshino 等（2018）Yoshino K, Hori C, Perez J, D’Haro LF, Polymenakos L, Gunasekara
    C, Lasecki WS, Kummerfeld J, Galley M, Brockett C 等（2018）第七届对话系统技术挑战。arXiv 预印本
- en: 'Young et al. (2010) Young S, Gašić M, Keizer S, Mairesse F, Schatzmann J, Thomson
    B, Yu K (2010) The hidden information state model: A practical framework for pomdp-based
    spoken dialogue management. Computer Speech & Language 24(2):150–174'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young 等人 (2010) Young S, Gašić M, Keizer S, Mairesse F, Schatzmann J, Thomson
    B, Yu K (2010) 隐藏信息状态模型：基于POMDP的语音对话管理实用框架。计算机语音与语言 24(2):150–174
- en: 'Young et al. (2018) Young T, Cambria E, Chaturvedi I, Zhou H, Biswas S, Huang
    M (2018) Augmenting end-to-end dialogue systems with commonsense knowledge. In:
    AAAI, pp 4970–4977'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young 等人 (2018) Young T, Cambria E, Chaturvedi I, Zhou H, Biswas S, Huang M
    (2018) 通过常识知识增强端到端对话系统。在：AAAI，页 4970–4977
- en: Young et al. (2020) Young T, Pandelea V, Poria S, Cambria E (2020) Dialogue
    systems with audio context. Neurocomputing 388:102–109
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Young 等人 (2020) Young T, Pandelea V, Poria S, Cambria E (2020) 带有音频上下文的对话系统。Neurocomputing
    388:102–109
- en: 'Yu et al. (2020) Yu F, Tang J, Yin W, Sun Y, Tian H, Wu H, Wang H (2020) Ernie-vil:
    Knowledge enhanced vision-language representations through scene graph. arXiv
    preprint arXiv:200616934 1:12'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 (2020) Yu F, Tang J, Yin W, Sun Y, Tian H, Wu H, Wang H (2020) Ernie-vil：通过场景图增强的知识视觉语言表示。arXiv
    预印本 arXiv:200616934 1:12
- en: Yu and Joty (2020) Yu T, Joty S (2020) Online conversation disentanglement with
    pointer networks. arXiv preprint arXiv:201011080
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Joty (2020) Yu T, Joty S (2020) 基于指针网络的在线对话解缠。arXiv 预印本 arXiv:201011080
- en: 'Zaheer et al. (2020) Zaheer M, Guruganesh G, Dubey KA, Ainslie J, Alberti C,
    Ontanon S, Pham P, Ravula A, Wang Q, Yang L, et al. (2020) Big bird: Transformers
    for longer sequences. Advances in Neural Information Processing Systems 33:17283–17297'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zaheer 等人 (2020) Zaheer M, Guruganesh G, Dubey KA, Ainslie J, Alberti C, Ontanon
    S, Pham P, Ravula A, Wang Q, Yang L, 等 (2020) Big bird: 适用于长序列的变换器。神经信息处理系统进展
    33:17283–17297'
- en: Zahiri and Choi (2017) Zahiri SM, Choi JD (2017) Emotion detection on tv show
    transcripts with sequence-based convolutional neural networks. arXiv preprint
    arXiv:170804299
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zahiri 和 Choi (2017) Zahiri SM, Choi JD (2017) 基于序列的卷积神经网络在电视节目转录中的情感检测。arXiv
    预印本 arXiv:170804299
- en: 'Zeiler and Fergus (2014) Zeiler MD, Fergus R (2014) Visualizing and understanding
    convolutional networks. In: European conference on computer vision, Springer,
    pp 818–833'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler 和 Fergus (2014) Zeiler MD, Fergus R (2014) 卷积网络的可视化与理解。在：欧洲计算机视觉会议，Springer，页
    818–833
- en: Zhang et al. (2018a) Zhang C, Li Y, Du N, Fan W, Yu PS (2018a) Joint slot filling
    and intent detection via capsule neural networks. arXiv preprint arXiv:181209471
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2018a) Zhang C, Li Y, Du N, Fan W, Yu PS (2018a) 通过胶囊神经网络联合插槽填充和意图检测。arXiv
    预印本 arXiv:181209471
- en: 'Zhang et al. (2019a) Zhang H, Lan Y, Pang L, Guo J, Cheng X (2019a) Recosa:
    Detecting the relevant contexts with self-attention for multi-turn dialogue generation.
    arXiv preprint arXiv:190705339'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2019a) Zhang H, Lan Y, Pang L, Guo J, Cheng X (2019a) Recosa: 利用自注意力检测多轮对话生成中的相关上下文。arXiv
    预印本 arXiv:190705339'
- en: Zhang et al. (2019b) Zhang H, Liu Z, Xiong C, Liu Z (2019b) Grounded conversation
    generation as guided traverses in commonsense knowledge graphs. arXiv preprint
    arXiv:191102707
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2019b) Zhang H, Liu Z, Xiong C, Liu Z (2019b) 以常识知识图谱为指导的基础对话生成。arXiv
    预印本 arXiv:191102707
- en: 'Zhang and Danescu-Niculescu-Mizil (2020) Zhang J, Danescu-Niculescu-Mizil C
    (2020) Balancing objectives in counseling conversations: Advancing forwards or
    looking backwards. arXiv preprint arXiv:200504245'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Danescu-Niculescu-Mizil (2020) Zhang J, Danescu-Niculescu-Mizil C (2020)
    在咨询对话中平衡目标：向前推进还是回顾过去。arXiv 预印本 arXiv:200504245
- en: 'Zhang et al. (2018b) Zhang S, Dinan E, Urbanek J, Szlam A, Kiela D, Weston
    J (2018b) Personalizing dialogue agents: I have a dog, do you have pets too? arXiv
    preprint arXiv:180107243'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2018b) Zhang S, Dinan E, Urbanek J, Szlam A, Kiela D, Weston J (2018b)
    个性化对话代理：我有一只狗，你也有宠物吗？arXiv 预印本 arXiv:180107243
- en: 'Zhang and Wallace (2017) Zhang Y, Wallace B (2017) A sensitivity analysis of
    (and practitioners’ guide to) convolutional neural networks for sentence classification.
    In: Proceedings of the Eighth International Joint Conference on Natural Language
    Processing (Volume 1: Long Papers), Asian Federation of Natural Language Processing,
    Taipei, Taiwan, pp 253–263, URL [https://www.aclweb.org/anthology/I17-1026](https://www.aclweb.org/anthology/I17-1026)'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Wallace (2017) Zhang Y, Wallace B (2017) 卷积神经网络在句子分类中的敏感性分析（及实践指南）。在：第八届国际自然语言处理联合会议论文集（第1卷：长篇论文），亚洲自然语言处理联合会，台北，台湾，页
    253–263，网址 [https://www.aclweb.org/anthology/I17-1026](https://www.aclweb.org/anthology/I17-1026)
- en: Zhang et al. (2018c) Zhang Y, Galley M, Gao J, Gan Z, Li X, Brockett C, Dolan
    B (2018c) Generating informative and diverse conversational responses via adversarial
    information maximization. arXiv preprint arXiv:180905972
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018c) Zhang Y, Galley M, Gao J, Gan Z, Li X, Brockett C, Dolan B (2018c)
    通过对抗信息最大化生成信息丰富且多样化的对话响应。arXiv 预印本 arXiv:180905972
- en: Zhang et al. (2020) Zhang Y, Ou Z, Wang H, Feng J (2020) A probabilistic end-to-end
    task-oriented dialog model with latent belief states towards semi-supervised learning.
    arXiv preprint arXiv:200908115
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2020) Zhang Y, Ou Z, Wang H, Feng J (2020) 一种概率性端到端任务导向对话模型，具备潜在信念状态，面向半监督学习。arXiv
    预印本 arXiv:200908115
- en: Zhang et al. (2018d) Zhang Z, Li J, Zhu P, Zhao H, Liu G (2018d) Modeling multi-turn
    conversation with deep utterance aggregation. arXiv preprint arXiv:180609102
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2018d) Zhang Z, Li J, Zhu P, Zhao H, Liu G (2018d) 通过深度发话聚合建模多轮对话。arXiv
    预印本 arXiv:180609102
- en: Zhang et al. (2019c) Zhang Z, Li X, Gao J, Chen E (2019c) Budgeted policy learning
    for task-oriented dialogue systems. arXiv preprint arXiv:190600499
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019c) Zhang Z, Li X, Gao J, Chen E (2019c) 面向任务导向对话系统的预算策略学习。arXiv
    预印本 arXiv:190600499
- en: Zhao and Eskenazi (2016) Zhao T, Eskenazi M (2016) Towards end-to-end learning
    for dialog state tracking and management using deep reinforcement learning. arXiv
    preprint arXiv:160602560
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Eskenazi (2016) Zhao T, Eskenazi M (2016) 面向对话状态跟踪和管理的端到端学习，使用深度强化学习。arXiv
    预印本 arXiv:160602560
- en: Zhao and Eskenazi (2018) Zhao T, Eskenazi M (2018) Zero-shot dialog generation
    with cross-domain latent actions. arXiv preprint arXiv:180504803
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 和 Eskenazi (2018) Zhao T, Eskenazi M (2018) 跨领域潜在动作的零-shot对话生成。arXiv 预印本
    arXiv:180504803
- en: Zhao et al. (2018) Zhao T, Lee K, Eskenazi M (2018) Unsupervised discrete sentence
    representation learning for interpretable neural dialog generation. arXiv preprint
    arXiv:180408069
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2018) Zhao T, Lee K, Eskenazi M (2018) 无监督离散句子表示学习，用于可解释的神经对话生成。arXiv
    预印本 arXiv:180408069
- en: Zhao et al. (2020a) Zhao T, Lala D, Kawahara T (2020a) Designing precise and
    robust dialogue response evaluators. arXiv preprint arXiv:200404908
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2020a) Zhao T, Lala D, Kawahara T (2020a) 设计精准且鲁棒的对话响应评估器。arXiv 预印本
    arXiv:200404908
- en: Zhao et al. (2020b) Zhao X, Wu W, Xu C, Tao C, Zhao D, Yan R (2020b) Knowledge-grounded
    dialogue generation with pre-trained language models. arXiv preprint arXiv:201008824
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2020b) Zhao X, Wu W, Xu C, Tao C, Zhao D, Yan R (2020b) 基于知识的对话生成与预训练语言模型。arXiv
    预印本 arXiv:201008824
- en: 'Zhong et al. (2020) Zhong P, Zhang C, Wang H, Liu Y, Miao C (2020) Towards
    persona-based empathetic conversational models. In: Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pp 6556–6566, DOI 10.18653/v1/2020.emnlp-main.531'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等 (2020) Zhong P, Zhang C, Wang H, Liu Y, Miao C (2020) 面向个性化同理心对话模型。In:
    2020年自然语言处理经验方法会议（EMNLP）论文集，第6556–6566页，DOI 10.18653/v1/2020.emnlp-main.531'
- en: 'Zhou et al. (2016) Zhou H, Huang M, Zhu X (2016) Context-aware natural language
    generation for spoken dialogue systems. In: Proceedings of COLING 2016, the 26th
    International Conference on Computational Linguistics: Technical Papers, pp 2032–2041'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 (2016) Zhou H, Huang M, Zhu X (2016) 面向口语对话系统的上下文感知自然语言生成。In: COLING
    2016 会议论文集，第26届国际计算语言学大会技术论文，第2032–2041页'
- en: 'Zhou et al. (2020a) Zhou H, Zheng C, Huang K, Huang M, Zhu X (2020a) Kdconv:
    a chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation.
    arXiv preprint arXiv:200404100'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 (2020a) Zhou H, Zheng C, Huang K, Huang M, Zhu X (2020a) Kdconv: 一个面向多轮知识驱动对话的中文多领域对话数据集。arXiv
    预印本 arXiv:200404100'
- en: Zhou et al. (2018) Zhou K, Prabhumoye S, Black AW (2018) A dataset for document
    grounded conversations. arXiv preprint arXiv:180907358
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2018) Zhou K, Prabhumoye S, Black AW (2018) 一个文档基础对话的数据集。arXiv 预印本 arXiv:180907358
- en: 'Zhou et al. (2020b) Zhou L, Palangi H, Zhang L, Hu H, Corso J, Gao J (2020b)
    Unified vision-language pre-training for image captioning and vqa. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 34, pp 13041–13049'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 (2020b) Zhou L, Palangi H, Zhang L, Hu H, Corso J, Gao J (2020b) 统一的视觉-语言预训练，用于图像描述和视觉问答。In:
    AAAI 人工智能会议论文集，第34卷，第13041–13049页'
- en: 'Zhou and Wang (2017) Zhou X, Wang WY (2017) Mojitalk: Generating emotional
    responses at scale. arXiv preprint arXiv:171104090'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 和 Wang (2017) Zhou X, Wang WY (2017) Mojitalk: 大规模生成情感响应。arXiv 预印本 arXiv:171104090'
- en: Zhu et al. (2018) Zhu Q, Cui L, Zhang W, Wei F, Liu T (2018) Retrieval-enhanced
    adversarial training for neural response generation. arXiv preprint arXiv:180904276
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2018) Zhu Q, Cui L, Zhang W, Wei F, Liu T (2018) 检索增强的对抗训练用于神经响应生成。arXiv
    预印本 arXiv:180904276
- en: 'Zhu et al. (2020) Zhu Q, Zhang W, Liu T, Wang WY (2020) Counterfactual off-policy
    training for neural dialogue generation. In: Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pp 3438–3448'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2020）**朱庆**，**张伟**，**刘婷**，**王伟阳**（2020）《用于神经对话生成的反事实离线策略训练》。载于：2020年自然语言处理实证方法会议（EMNLP）论文集，第3438–3448页。
