- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-06 19:55:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:55:12'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2105.04387] Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2105.04387] 基于深度学习的对话系统的最新进展：系统性调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2105.04387](https://ar5iv.labs.arxiv.org/html/2105.04387)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2105.04387](https://ar5iv.labs.arxiv.org/html/2105.04387)
- en: 'Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的对话系统的最新进展：系统性调查
- en: 'Jinjie Ni Nanyang Technological University, Singapore. {jinjie001, yang0552,
    fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg Tom Young¹¹1Equal
    contribution Nanyang Technological University, Singapore. {jinjie001, yang0552,
    fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg Vlad Pandelea¹¹footnotemark:
    1 Nanyang Technological University, Singapore. {jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg,
    {vlad.pandelea, cambria}@ntu.edu.sg Fuzhao Xue Nanyang Technological University,
    Singapore. {jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg
    Erik Cambria³³3Corresponding author Nanyang Technological University, Singapore.
    {jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg, {vlad.pandelea, cambria}@ntu.edu.sg'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'Jinjie Ni 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Tom Young¹¹1等贡献 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Vlad Pandelea¹¹脚注标记: 1 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Fuzhao Xue 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg Erik Cambria³³3通讯作者 南洋理工大学，新加坡。{jinjie001, yang0552, fuzhao001}@e.ntu.edu.sg，{vlad.pandelea,
    cambria}@ntu.edu.sg'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Dialogue systems are a popular natural language processing (NLP) task as it
    is promising in real-life applications. It is also a complicated task since many
    NLP tasks deserving study are involved. As a result, a multitude of novel works
    on this task are carried out, and most of them are deep learning based due to
    the outstanding performance. In this survey, we mainly focus on the deep learning
    based dialogue systems. We comprehensively review state-of-the-art research outcomes
    in dialogue systems and analyze them from two angles: model type and system type.
    Specifically, from the angle of model type, we discuss the principles, characteristics,
    and applications of different models that are widely used in dialogue systems.
    This will help researchers acquaint these models and see how they are applied
    in state-of-the-art frameworks, which is rather helpful when designing a new dialogue
    system. From the angle of system type, we discuss task-oriented and open-domain
    dialogue systems as two streams of research, providing insight into the hot topics
    related. Furthermore, we comprehensively review the evaluation methods and datasets
    for dialogue systems to pave the way for future research. Finally, some possible
    research trends are identified based on the recent research outcomes. To the best
    of our knowledge, this survey is the most comprehensive and up-to-date one at
    present for deep learning based dialogue systems, extensively covering the popular
    techniques¹¹1The frameworks, topics, and datasets discussed are originated from
    the extensive literature review of state-of-the-art research. We have tried our
    best to cover all but may still omit some works. Readers are welcome to provide
    suggestions regarding the omissions and mistakes in this article. We also intend
    to update this article with time as and when new approaches or definitions are
    proposed and used by the community. We speculate that this work is a good starting
    point for academics who are new to the dialogue systems or those who want to quickly
    grasp up-to-date techniques in this area.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统是一个受欢迎的自然语言处理（NLP）任务，因为它在实际应用中具有很大的潜力。由于涉及到许多值得研究的NLP任务，这也是一个复杂的任务。因此，许多关于这一任务的创新性研究被开展，其中大多数基于深度学习，因为其卓越的性能。在本次调查中，我们主要关注基于深度学习的对话系统。我们全面回顾了对话系统的最先进研究成果，并从两个角度进行分析：模型类型和系统类型。具体来说，从模型类型的角度，我们讨论了在对话系统中广泛使用的不同模型的原理、特点和应用。这将帮助研究人员了解这些模型，并看到它们如何在最先进的框架中应用，这对设计新的对话系统非常有帮助。从系统类型的角度，我们讨论了任务导向型和开放领域对话系统作为两大研究方向，提供了相关热点话题的见解。此外，我们还全面回顾了对话系统的评估方法和数据集，以为未来的研究铺平道路。最后，基于最近的研究成果，我们确定了一些可能的研究趋势。根据我们的了解，这项调查目前是针对基于深度学习的对话系统最全面、最新的调查，广泛覆盖了流行的技术¹¹1这些框架、主题和数据集源于对最先进研究的广泛文献综述。我们已尽力覆盖所有内容，但可能仍遗漏一些工作。欢迎读者对文章中的遗漏和错误提出建议。我们还打算随着时间的推移更新这篇文章，以反映社区中提出和使用的新方法或定义。我们推测，这项工作是学术界初次接触对话系统或希望迅速掌握该领域最新技术的一个良好起点。
- en: Keywords  Dialogue systems, Chatbots, Conversational AI, Natural Language Processing,
    Deep learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词  对话系统、聊天机器人、对话式人工智能、自然语言处理、深度学习
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Dialogue systems (or chatbots) are playing a bigger role in the world. People
    may still have a stereotype that chatbots are those rigid agents in their phone
    calls to a bank. However, thanks to the revival of artificial intelligence, the
    modern chatbots can converse with rich topics ranging from your birthday party
    to a speech given by Biden, and, if you want, they can even book a place for your
    party or play the speech video. At present, dialogue systems are one of the hot
    topics in NLP and are highly demanded in industry and daily life. The market size
    of chatbot is projected to grow from $2.6 billion in 2021 to $9.4 billion by 2024
    at a compound annual growth rate (CAGR) of 29.7% ²²2Statistic source: [https://markets.businessinsider.com](https://markets.businessinsider.com)
    and 80% of businesses are expected to be equipped with chatbot automation by the
    end of 2021 ³³3Statistic source: [https://outgrow.co](https://outgrow.co).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统（或聊天机器人）在全球扮演着越来越重要的角色。人们可能仍然对聊天机器人有一种刻板印象，认为它们只是那些在银行电话中出现的呆板代理。然而，得益于人工智能的复兴，现代聊天机器人可以就丰富的话题进行对话，从你的生日聚会到拜登的演讲，如果你愿意，它们甚至可以为你的聚会预订场地或播放演讲视频。目前，对话系统是自然语言处理领域的热门话题，并且在行业和日常生活中需求很高。预计聊天机器人的市场规模将从2021年的26亿美元增长到2024年的94亿美元，年复合增长率（CAGR）为29.7%²²²统计来源：[https://markets.businessinsider.com](https://markets.businessinsider.com)，而80%的企业预计将在2021年底配备聊天机器人自动化³³³统计来源：[https://outgrow.co](https://outgrow.co)。
- en: 'Dialogue systems perform chit-chat with human or serve as an assistant via
    conversations. By their applications, dialogue systems are commonly divided into
    two categories: task-oriented dialogue systems (TOD) and open-domain dialogue
    systems (OOD). Task-oriented dialogue systems solve specific problems in a certain
    domain such as movie ticket booking, restaurant table reserving, etc. Instead
    of focusing on task completion, open-domain dialogue systems aim to chat with
    users without the task and domain restrictions (Ritter et al., [2011](#bib.bib300)),
    which are usually fully data-driven. Both task-oriented and open-domain dialogue
    systems can be seen as a mapping $\varphi$ from user message $U=\{\mathrm{\mathbf{u}}^{(1)},\mathrm{\mathbf{u}}^{(2)},...,\mathrm{\mathbf{u}}^{(i)}\}$
    to agent response $R=\{\mathrm{\mathbf{r}}^{(1)},\mathrm{\mathbf{r}}^{(2)},...,\mathrm{\mathbf{r}}^{(j)}\}$:
    $R=\varphi(U)$, where $\mathrm{\mathbf{u}}^{(i)}$ and $\mathrm{\mathbf{r}}^{(j)}$
    denote the $i$th token of the user message and the $j$th token of the agent response
    respectively. In many open-domain and task-oriented dialogue systems, this mapping
    also considers a source of external knowledge/database $K$ as input: $R=\varphi(U,K)$.
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey") presents examples of inputs and
    outputs of task-oriented and open-domain dialogue systems. More specific details
    and works will be discussed in Section [3](#S3 "3 Task-oriented Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    and [4](#S4 "4 Open-Domain Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey").'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '对话系统通过对话与人类进行闲聊或作为助手服务。根据应用，对话系统通常被分为两类：任务导向型对话系统（TOD）和开放领域对话系统（OOD）。任务导向型对话系统解决特定领域中的问题，例如电影票预订、餐馆桌位预订等。开放领域对话系统则不关注任务完成，而是旨在与用户进行无任务和领域限制的对话（Ritter
    et al., [2011](#bib.bib300)），这些系统通常完全基于数据驱动。任务导向型和开放领域对话系统都可以视为从用户消息 $U=\{\mathrm{\mathbf{u}}^{(1)},\mathrm{\mathbf{u}}^{(2)},...,\mathrm{\mathbf{u}}^{(i)}\}$
    到代理响应 $R=\{\mathrm{\mathbf{r}}^{(1)},\mathrm{\mathbf{r}}^{(2)},...,\mathrm{\mathbf{r}}^{(j)}\}$
    的映射：$R=\varphi(U)$，其中 $\mathrm{\mathbf{u}}^{(i)}$ 和 $\mathrm{\mathbf{r}}^{(j)}$
    分别表示用户消息中的第 $i$ 个标记和代理响应中的第 $j$ 个标记。在许多开放领域和任务导向型对话系统中，这种映射还考虑了外部知识/数据库 $K$ 作为输入：$R=\varphi(U,K)$。表
    [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") 展示了任务导向型和开放领域对话系统的输入和输出示例。更多具体细节和相关工作将在第
    [3](#S3 "3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") 和 [4](#S4 "4 Open-Domain Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    节中讨论。'
- en: 'Table 1: Examples of inputs and outputs of task-oriented and open-domain dialogue
    systems in datasets. Some datasets provide external knowledge annotations for
    each dialogue pair, e.g., in task-oriented dialogue systems, the external knowledge
    can be retrieved from restaurant databases; in open-domain dialogue systems, it
    can be retrieved from commonsense knowledge graphs (KG).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：数据集中任务导向和开放领域对话系统的输入输出示例。一些数据集为每个对话对提供外部知识注释，例如，在任务导向对话系统中，外部知识可以从餐馆数据库中检索；在开放领域对话系统中，它可以从常识知识图谱（KG）中检索。
- en: '| Category | User message ($U$) | Agent response ($R$) | External Knowledge
    ($K$) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 用户消息 ($U$) | 代理响应 ($R$) | 外部知识 ($K$) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Task-oriented | I need to find a nice restaurant in Madrid that serves expensive
    Thai food. | There is a restaurant called Bangkok City locating at 9 Red Ave.
    | restaurant database |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 任务导向 | 我需要找到一家在马德里提供昂贵泰国菜的好餐馆。 | 有一家叫做 Bangkok City 的餐馆，位于 9 Red Ave。 | 餐馆数据库
    |'
- en: '| Open-domain | I love the grilled fish so much! | Yeah. it’s a famous Chinese
    dish. | commonsense KG |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 开放领域 | 我非常喜欢烤鱼！ | 是的，这是道著名的中式菜肴。 | 常识 KG |'
- en: 'Traditional task-oriented dialogue systems are organized in a pipeline structure
    and consist of four functional modules: Natural Language Understanding, Dialogue
    State Tracking, Policy Learning, and Natural Language Generation, which will be
    discussed in detail in Section [3](#S3 "3 Task-oriented Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey"). Many
    state-of-the-art works design end-to-end task-oriented dialogue systems to achieve
    better optimization compared with pipeline methods. Open-domain dialogue systems
    are generally divided into three categories: generative systems, retrieval-based
    systems, and ensemble systems. Generative systems apply sequence-to-sequence models
    (see Section [2.2.5](#S2.SS2.SSS5 "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder
    Models) ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence Models
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey")) to map the user message and dialogue
    history into a response sequence that may not appear in the training corpus. By
    contrast, retrieval-based systems try to select a pre-existing response from a
    certain response set. Ensemble systems combine generative methods and retrieval-based
    methods in two ways: retrieved responses can be compared with generated responses
    to choose the best among them; generative models can also be used to refine the
    retrieved responses (Zhu et al., [2018](#bib.bib466); Song et al., [2016](#bib.bib337);
    Qiu et al., [2017](#bib.bib284); Serban et al., [2017b](#bib.bib314)). Generative
    systems can produce flexible and dialogue context-related responses while sometimes
    they lack coherence ⁴⁴4The quality of being logical and consistent not only between
    words/subwords but also between responses of different timesteps. and tend to
    make dull responses (Serban et al., [2016](#bib.bib312); Vinyals and Le, [2015](#bib.bib379);
    Sordoni et al., [2015b](#bib.bib340)). Retrieval-based systems select responses
    from human response sets and thus are able to achieve better coherence in surface-level
    language. However, retrieval systems are restricted by the finiteness of the response
    sets and sometimes the responses retrieved show a weak correlation with the dialogue
    context (Zhu et al., [2018](#bib.bib466)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的任务导向对话系统采用流水线结构，包含四个功能模块：自然语言理解、对话状态跟踪、策略学习和自然语言生成，这些将在第[3](#S3 "3 Task-oriented
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey")节中详细讨论。许多先进的工作设计了端到端的任务导向对话系统，以实现比流水线方法更好的优化。开放域对话系统通常分为三类：生成系统、基于检索的系统和集成系统。生成系统应用序列到序列模型（参见第[2.2.5](#S2.SS2.SSS5
    "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder Models) ‣ 2.2 Recurrent
    Neural Networks and Vanilla Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey")节）将用户消息和对话历史映射到可能在训练语料库中未出现的响应序列。相比之下，基于检索的系统尝试从某个响应集选择一个预存在的响应。集成系统以两种方式结合生成方法和基于检索的方法：检索到的响应可以与生成的响应进行比较，以选择最佳响应；生成模型也可以用于完善检索到的响应（Zhu
    et al., [2018](#bib.bib466); Song et al., [2016](#bib.bib337); Qiu et al., [2017](#bib.bib284);
    Serban et al., [2017b](#bib.bib314)）。生成系统可以生成灵活且与对话上下文相关的响应，但有时缺乏连贯性⁴⁴4即不仅在词语/子词之间而且在不同时间步的响应之间也要逻辑一致。并且往往会产生枯燥的响应（Serban
    et al., [2016](#bib.bib312); Vinyals and Le, [2015](#bib.bib379); Sordoni et al.,
    [2015b](#bib.bib340)）。基于检索的系统从人工响应集中选择响应，因此在表层语言上能够实现更好的连贯性。然而，检索系统受到响应集有限性的限制，有时检索到的响应与对话上下文的相关性较弱（Zhu
    et al., [2018](#bib.bib466)）。'
- en: '![Refer to caption](img/183803d09e02867efd0cf03a24d4a201.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/183803d09e02867efd0cf03a24d4a201.png)'
- en: 'Figure 1: The overall diagram of this article'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本文的总体示意图
- en: For dialogue systems, existing surveys (Arora et al., [2013](#bib.bib7); Wang
    and Yuan, [2016](#bib.bib389); Mallios and Bourbakis, [2016](#bib.bib240); Chen
    et al., [2017a](#bib.bib39); Gao et al., [2018](#bib.bib98)) are either outdated
    or not comprehensive. Some definitions in these papers are no longer being used
    at present, and a lot of new works and topics are not covered. In addition, most
    of them lack a multi-angle analysis. Thus, in this survey, we comprehensively
    review high-quality works in recent years with a focus on deep learning-based
    approaches and provide insight into state-of-the-art research from both model
    angle and system angle. Moreover, this survey updates the definitions/names according
    to state-of-the-art research. E.g., we name "open-domain dialogue systems" instead
    of "chit-chat dialogue systems" because most of the articles (roughly 70% according
    to our survey) name them as the prior one. We also extensively cover the diverse
    hot topics in dialogue systems and extend some new topics that are popular in
    current research community (such as Domain Adaptation, Dialogue State Tracking
    Efficiency, End-to-end methods for task-oriented dialogue systems; Controllable
    Generation, Interactive Training, and Visual Dialogue for open-domain dialogue
    systems).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对话系统，现有的调查（Arora et al., [2013](#bib.bib7); Wang and Yuan, [2016](#bib.bib389);
    Mallios and Bourbakis, [2016](#bib.bib240); Chen et al., [2017a](#bib.bib39);
    Gao et al., [2018](#bib.bib98)）要么过时，要么不全面。这些论文中的一些定义目前已经不再使用，许多新的工作和话题没有涵盖。此外，它们大多数缺乏多角度分析。因此，在这项调查中，我们全面回顾了近年来的高质量研究，重点关注基于深度学习的方法，并从模型角度和系统角度提供了对最先进研究的见解。此外，本调查根据最先进的研究更新了定义/名称。例如，我们使用“开放领域对话系统”而不是“闲聊对话系统”，因为大多数文章（根据我们的调查，大约70%）将其称为前者。我们还广泛涵盖了对话系统中的各种热门话题，并扩展了一些在当前研究社区中流行的新话题（例如，领域适应、对话状态跟踪效率、任务导向对话系统的端到端方法；可控生成、互动训练和开放领域对话系统的视觉对话）。
- en: 'Traditional dialogue systems are mostly rule-based (Arora et al., [2013](#bib.bib7))
    and non-neural machine learning based systems. Rule-based systems are easy to
    implement and can respond naturally, which contributed to their popularity in
    earlier industry products. However, the dialogue flows of these systems are predetermined,
    which keeps the applications of the dialogue systems within certain scenarios.
    Non-neural machine learning based systems usually perform template filling to
    manage certain tasks. These systems are more flexible compared with rule-based
    systems because the dialogue flows are not predetermined. However, they cannot
    achieve high F1 scores (Powers, [2020](#bib.bib276)) in template filling⁵⁵5Template
    filling is an efficient approach to extract and structure complex information
    from text to fill in a pre-defined template. They are mostly used in task-oriented
    dialogue systems. and are also restricted in application scenarios and response
    diversity because of the fixed templates. Most if not all state-of-the-art dialogue
    systems are deep learning-based systems (neural systems). The rapid growth of
    deep learning improves the performance of dialogue systems (Chen et al., [2017a](#bib.bib39)).
    Deep learning can be viewed as representation learning with multilayer neural
    networks. Deep learning architectures are widely used in dialogue systems and
    their subtasks. Section [2](#S2 "2 Neural Models in Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey") discusses
    various popular deep learning architectures.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的对话系统大多数是基于规则的（Arora et al., [2013](#bib.bib7)）和非神经网络的机器学习系统。基于规则的系统实现简单，响应自然，这使得它们在早期行业产品中非常受欢迎。然而，这些系统的对话流程是预定的，这限制了对话系统的应用场景。非神经网络的机器学习系统通常进行模板填充来处理某些任务。与基于规则的系统相比，这些系统更具灵活性，因为对话流程不是预定的。然而，它们在模板填充中无法实现高F1分数（Powers,
    [2020](#bib.bib276)）⁵⁵5模板填充是一种高效的方法，用于从文本中提取和结构化复杂信息，以填充预定义的模板。它们主要用于任务导向的对话系统，并且由于固定模板的原因，也受到应用场景和响应多样性的限制。目前大多数（如果不是全部的话）最先进的对话系统都是基于深度学习的系统（神经系统）。深度学习的快速发展提升了对话系统的性能（Chen
    et al., [2017a](#bib.bib39)）。深度学习可以被视为具有多层神经网络的表示学习。深度学习架构被广泛应用于对话系统及其子任务。第[2](#S2
    "2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey")节讨论了各种流行的深度学习架构。'
- en: Apart from dialogue systems, there are also many dialogue-related tasks in NLP,
    including but not limited to question answering, reading comprehension, dialogue
    disentanglement, visual dialogue, visual question answering, dialogue reasoning,
    conversational semantic parsing, dialogue relation extraction, dialogue sentiment
    analysis, hate speech detection, MISC detection, etc. In this survey, we also
    touch on some works tackling these dialogue-related tasks, since the design of
    dialogue systems can benefit from advances in these related areas.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对话系统，NLP中还有许多与对话相关的任务，包括但不限于问答、阅读理解、对话解缠、视觉对话、视觉问答、对话推理、对话语义解析、对话关系提取、对话情感分析、仇恨言论检测、MISC检测等。在本调查中，我们也涉及了一些处理这些对话相关任务的研究，因为对话系统的设计可以从这些相关领域的进展中受益。
- en: 'We produced a diagram for this article to help readers familiarize the overall
    structure (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Recent Advances in Deep
    Learning Based Dialogue Systems: A Systematic Survey")). In this survey, Section [1](#S1
    "1 Introduction ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey") briefly introduces dialogue systems and deep learning; Section [2](#S2
    "2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") discusses the neural models popular in
    modern dialogue systems and the related work; Section [3](#S3 "3 Task-oriented
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey") introduces the principles and related work of task-oriented
    dialogue systems and discusses the research challenges and hot topics; Section [4](#S4
    "4 Open-Domain Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") briefly introduces the three kinds of systems and
    then focuses on hot topics in open-domain dialogue systems; Section [5](#S5 "5
    Evaluation Approaches ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey") reviews the main evaluation methods for dialogue systems;
    Section [6](#S6 "6 Datasets ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") comprehensively summarizes the datasets commonly
    used for dialogue systems; finally, Section [7](#S7 "7 Conclusions and Trends
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    concludes the paper and provides some insight on research trends.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本文制作了一张图表，以帮助读者熟悉整体结构（图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 基于深度学习的对话系统的最新进展：系统性调查")）。在本调查中，第一部分 [1](#S1
    "1 引言 ‣ 基于深度学习的对话系统的最新进展：系统性调查") 简要介绍了对话系统和深度学习；第二部分 [2](#S2 "2 神经模型在对话系统中的应用
    ‣ 基于深度学习的对话系统的最新进展：系统性调查") 讨论了现代对话系统中流行的神经模型及相关工作；第三部分 [3](#S3 "3 面向任务的对话系统 ‣
    基于深度学习的对话系统的最新进展：系统性调查") 介绍了面向任务的对话系统的原理和相关工作，并讨论了研究挑战和热点话题；第四部分 [4](#S4 "4 开放领域对话系统
    ‣ 基于深度学习的对话系统的最新进展：系统性调查") 简要介绍了三种系统，并重点关注开放领域对话系统中的热点话题；第五部分 [5](#S5 "5 评价方法
    ‣ 基于深度学习的对话系统的最新进展：系统性调查") 回顾了对话系统的主要评价方法；第六部分 [6](#S6 "6 数据集 ‣ 基于深度学习的对话系统的最新进展：系统性调查")
    综合总结了对话系统中常用的数据集；最后，第七部分 [7](#S7 "7 结论与趋势 ‣ 基于深度学习的对话系统的最新进展：系统性调查") 总结了论文并提供了一些研究趋势的见解。
- en: 2 Neural Models in Dialogue Systems
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 神经模型在对话系统中的应用
- en: 'In this section, we introduce neural models that are popular in state-of-the-art
    dialogue systems and related subtasks. We also discuss the applications of these
    models or their variants in modern dialogue systems research to provide readers
    with a picture from the model’s perspective. This will help researchers acquaint
    these models and see how they are applied in state-of-the-art frameworks, which
    is rather helpful when designing a new dialogue system. The models discussed include:
    Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Vanilla
    Sequence-to-sequence Models, Hierarchical Recurrent Encoder-Decoder (HRED), Memory
    Networks, Attention Networks, Transformer, Pointer Net and CopyNet, Deep Reinforcement
    Learning models, Generative Adversarial Networks (GANs), Knowledge Graph Augmented
    Neural Networks. We start from some classical models (e.g., CNNs and RNNs), and
    readers who are familiar with their principles and corresponding applications
    in dialogue systems can choose to read selectively.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了在最先进的对话系统及相关子任务中流行的神经模型。我们还讨论了这些模型或其变体在现代对话系统研究中的应用，以便从模型的角度为读者提供一个全景。这将帮助研究人员熟悉这些模型，并了解它们在最先进框架中的应用，这对于设计新的对话系统非常有帮助。讨论的模型包括：卷积神经网络（CNNs）、递归神经网络（RNNs）、基础序列到序列模型、层次递归编码器-解码器（HRED）、记忆网络、注意力网络、Transformer、Pointer
    Net 和 CopyNet、深度强化学习模型、生成对抗网络（GANs）、知识图谱增强神经网络。我们从一些经典模型（例如，CNNs 和 RNNs）开始，熟悉它们的原理和在对话系统中的应用的读者可以选择性阅读。
- en: 2.1 Convolutional Neural Networks
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 卷积神经网络
- en: 'Deep neural networks have been considered as one of the most powerful models.
    ‘Deep’ refers to the fact that they are multilayer, which extracts features by
    stacking feed-forward layers. Feed-forward layers can be defined as: $y=\sigma(Wx+b)$.
    Where the $\sigma$ is an activation function; $W$ and $b$ are trainable parameters.
    The feed-forward layers are powerful due to the activation function, which makes
    the otherwise linear operation, non-linear. Whereas there exist some problems
    when using feed-forward layers. Firstly, the operations of feed-forward layers
    or multilayer neural networks are just template matching, where they do not consider
    the specific structure of data. Furthermore, the fully connected mechanism of
    traditional multilayer neural networks causes an explosion in the number of parameters
    and thus leads to generalization problems. LeCun et al. ([1998](#bib.bib182))
    proposed LeNet-5, an early CNN. The invention of CNNs mitigates the above problems
    to some extent.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络被认为是最强大的模型之一。“深度”指的是它们是多层的，通过堆叠前馈层提取特征。前馈层可以定义为：$y=\sigma(Wx+b)$。其中，$\sigma$
    是激活函数；$W$ 和 $b$ 是可训练参数。前馈层因激活函数而强大，它使得本来线性的操作变为非线性。然而，使用前馈层时存在一些问题。首先，前馈层或多层神经网络的操作只是模板匹配，它们不考虑数据的具体结构。此外，传统多层神经网络的全连接机制导致参数数量激增，从而导致泛化问题。LeCun
    等人 ([1998](#bib.bib182)) 提出了早期的 CNN——LeNet-5。CNN 的发明在一定程度上缓解了上述问题。
- en: '![Refer to caption](img/0bc26bb5dea6d2ef981db472f8301895.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0bc26bb5dea6d2ef981db472f8301895.png)'
- en: 'Figure 2: A CNN architecture for text classification (Zhang and Wallace, [2017](#bib.bib450))'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：用于文本分类的 CNN 结构（Zhang 和 Wallace，[2017](#bib.bib450)）
- en: 'CNNs (Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Convolutional Neural Networks ‣ 2 Neural
    Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")) usually consist of convolutional layers, pooling layers
    and feed-forward layers. Convolutional layers apply convolution kernels to perform
    the convolution operation:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（图 [2](#S2.F2 "图 2 ‣ 2.1 卷积神经网络 ‣ 2 神经模型在对话系统中的应用 ‣ 基于深度学习的对话系统的最新进展：系统性综述")）通常由卷积层、池化层和前馈层组成。卷积层应用卷积核进行卷积操作：
- en: '|  | $G(m,n)=(f*h)(m,n)=\sum_{j}\sum_{k}h(j,k)f(m-j,n-k)$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $G(m,n)=(f*h)(m,n)=\sum_{j}\sum_{k}h(j,k)f(m-j,n-k)$ |  | (1) |'
- en: Where $m$ and $n$ are respectively the indexes of rows and columns of the result
    matrix. $f$ denotes the input matrix and $h$ denotes the convolutional kernel.
    The pooling layers perform down-sampling on the result of convolutional layers
    to get a higher level of features and the feed-forward layers map them into a
    probability distribution to predict class scores.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 和 $n$ 分别是结果矩阵的行和列索引。$f$ 表示输入矩阵，$h$ 表示卷积核。池化层对卷积层的结果进行降采样，以获得更高层次的特征，前馈层将这些特征映射到概率分布中以预测类别分数。
- en: A sliding window feature enables convolution layers to capture local features
    and the pooling layers can produce hierarchical features. These two mechanisms
    give CNNs the local perception and global perception ability, helping to capture
    some specific inner structures of data. The parameter sharing mechanism eases
    the parameter explosion problem and overfitting problem because the reduction
    of trainable parameters leads to less model complexity, improving the generalization
    ability.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口特性使卷积层能够捕捉局部特征，而池化层可以生成层次化特征。这两个机制赋予了卷积神经网络（CNNs）局部感知和全局感知的能力，有助于捕捉数据的一些特定内部结构。参数共享机制缓解了参数爆炸问题和过拟合问题，因为可训练参数的减少导致模型复杂性降低，从而提高了模型的泛化能力。
- en: Due to these good properties, CNNs have been widely applied in many works. Among
    them, the Computer Vision tasks benefit the most for that the Spatio-temporal
    data structures of images or videos are perfectly captured by CNNs. For more detailed
    mechanism illustrations and other variants of CNNs, readers can refer to these
    representative algorithm papers or surveys: (Krizhevsky et al., [2012](#bib.bib173);
    Zeiler and Fergus, [2014](#bib.bib444); Simonyan and Zisserman, [2014](#bib.bib328);
    Szegedy et al., [2015](#bib.bib355); He et al., [2016](#bib.bib126); Aloysius
    and Geetha, [2017](#bib.bib5); Rawat and Wang, [2017](#bib.bib295)). In this survey,
    we focus on dialogue systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些优良特性，CNN已广泛应用于许多工作中。其中，计算机视觉任务受益最大，因为CNN能够完美捕捉图像或视频的时空数据结构。有关更详细的机制说明和CNN的其他变体，读者可以参考以下具有代表性的算法论文或综述：
    (Krizhevsky et al., [2012](#bib.bib173); Zeiler and Fergus, [2014](#bib.bib444);
    Simonyan and Zisserman, [2014](#bib.bib328); Szegedy et al., [2015](#bib.bib355);
    He et al., [2016](#bib.bib126); Aloysius and Geetha, [2017](#bib.bib5); Rawat
    and Wang, [2017](#bib.bib295))。在本综述中，我们专注于对话系统。
- en: Recent years have seen a dramatic increase in applications of CNNs in NLP. Many
    tasks take words as basic units. However, phrases, sentences, or even paragraphs
    are also useful to semantic representations. As a result, CNNs are an ideal tool
    for the hierarchical modeling of language (Conneau et al., [2016](#bib.bib57)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，CNN在自然语言处理（NLP）中的应用显著增加。许多任务以单词作为基本单元。然而，短语、句子甚至段落对语义表示也是有用的。因此，CNN是语言的层次建模的理想工具（Conneau
    et al., [2016](#bib.bib57)）。
- en: CNNs are good textual feature extractors, but they may not be ideal sequential
    encoders. Some dialogue systems (Qiu et al., [2019](#bib.bib282); Bi et al., [2019](#bib.bib22);
    Ma et al., [2020a](#bib.bib235)) directly used CNNs as the encoder of utterances
    or knowledge, but most of the state-of-the-art dialogue systems such as Feng et al.
    ([2019](#bib.bib86)); Wu et al. ([2016](#bib.bib418)); Tao et al. ([2019](#bib.bib364));
    Wang et al. ([2019b](#bib.bib390)); Chauhan et al. ([2019](#bib.bib38)); Feldman
    and El-Yaniv ([2019](#bib.bib85)); Chen et al. ([2019c](#bib.bib45)); Lu et al.
    ([2019b](#bib.bib231)) and Coope et al. ([2020](#bib.bib58)) chose to use CNNs
    as a hierarchical feature extractor after encoding the text information, instead
    of directly applying them as encoders. This is due to the fixed input length and
    limited convolution span of CNNs. Generally, there are two main situations where
    CNNs are used to process encoded information in dialogue systems. The first situation
    is applying CNNs to extract features directly based on the feature vectors from
    the encoder (Wang et al., [2019b](#bib.bib390); Chauhan et al., [2019](#bib.bib38);
    Feldman and El-Yaniv, [2019](#bib.bib85); Chen et al., [2019c](#bib.bib45)) and Coope
    et al. ([2020](#bib.bib58)). Within the works above, Feldman and El-Yaniv ([2019](#bib.bib85))
    extracted features from character-level embeddings, illustrating the hierarchical
    extraction capability of CNNs. Another situation in which CNNs are used is extracting
    feature maps in response retrieval tasks. Some works built retrieval-based dialogue
    systems (Wu et al., [2016](#bib.bib418); Feng et al., [2019](#bib.bib86); Tao
    et al., [2019](#bib.bib364); Lu et al., [2019b](#bib.bib231)). They used separate
    encoders to encode dialogue context and candidate responses and then used a CNN
    as an extractor of the similarity matrix calculated from the encoded dialogue
    context and candidate responses. Their experiments showed that this method can
    achieve good performance in response retrieval tasks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 是很好的文本特征提取器，但可能不是理想的序列编码器。一些对话系统（Qiu et al., [2019](#bib.bib282); Bi et
    al., [2019](#bib.bib22); Ma et al., [2020a](#bib.bib235)）直接使用 CNNs 作为话语或知识的编码器，但大多数最先进的对话系统，如
    Feng et al. ([2019](#bib.bib86)); Wu et al. ([2016](#bib.bib418)); Tao et al.
    ([2019](#bib.bib364)); Wang et al. ([2019b](#bib.bib390)); Chauhan et al. ([2019](#bib.bib38));
    Feldman 和 El-Yaniv ([2019](#bib.bib85)); Chen et al. ([2019c](#bib.bib45)); Lu
    et al. ([2019b](#bib.bib231)) 和 Coope et al. ([2020](#bib.bib58)) 选择在编码文本信息后使用
    CNNs 作为层次特征提取器，而不是直接将其作为编码器。这是由于 CNNs 固定的输入长度和有限的卷积范围。一般来说，有两种主要情况 CNNs 被用来处理对话系统中的编码信息。第一种情况是基于编码器的特征向量直接应用
    CNNs 提取特征（Wang et al., [2019b](#bib.bib390); Chauhan et al., [2019](#bib.bib38);
    Feldman 和 El-Yaniv, [2019](#bib.bib85); Chen et al., [2019c](#bib.bib45)）和 Coope
    et al. ([2020](#bib.bib58))。在上述工作中，Feldman 和 El-Yaniv ([2019](#bib.bib85)) 从字符级嵌入中提取特征，展示了
    CNNs 的层次提取能力。CNNs 被使用的另一种情况是在响应检索任务中提取特征图。一些工作构建了基于检索的对话系统（Wu et al., [2016](#bib.bib418);
    Feng et al., [2019](#bib.bib86); Tao et al., [2019](#bib.bib364); Lu et al., [2019b](#bib.bib231)）。他们使用独立的编码器对对话上下文和候选响应进行编码，然后使用
    CNN 作为从编码的对话上下文和候选响应中计算的相似度矩阵的提取器。他们的实验表明，这种方法在响应检索任务中可以取得良好的性能。
- en: The main reason why more recent works do not choose CNNs as dialogue encoders
    is that they fail to extract the information across temporal sequence steps continuously
    and flexibly (Krizhevsky et al., [2012](#bib.bib173)). Some models introduced
    later do not process data points independently, which are desirable models for
    encoders.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 更近期的工作不选择 CNNs 作为对话编码器的主要原因是它们无法持续且灵活地提取跨时间序列步的信息（Krizhevsky et al., [2012](#bib.bib173)）。一些后续引入的模型不会独立处理数据点，这些模型是编码器所期望的。
- en: 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence Models
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 循环神经网络和普通序列到序列模型
- en: NLP tasks including dialogue-related tasks try to process and analyze sequential
    language data points. Even though standard neural networks, as well as CNNs, are
    powerful learning models, they have two main limitations (Lipton et al., [2015](#bib.bib218)).
    One is that they assume the data points are independent of each other. While it
    is reasonable if the data points are produced independently, essential information
    can be missed when processing interrelated data points (e.g., text, audio, video).
    Additionally, their inputs are usually of fixed length, which is a limitation
    when processing sequential data varying in length. Thus, a sequential model being
    able to represent the sequential information flow is desirable.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 任务，包括对话相关任务，试图处理和分析顺序语言数据点。尽管标准神经网络以及 CNNs 是强大的学习模型，但它们存在两个主要的限制（Lipton
    et al.，[2015](#bib.bib218)）。其一是它们假设数据点彼此独立。如果数据点是独立产生的，这种假设是合理的，但在处理互相关联的数据点（例如文本、音频、视频）时，可能会遗漏重要信息。此外，它们的输入通常是固定长度的，这在处理长度可变的顺序数据时是一个限制。因此，能够表示顺序信息流的顺序模型是理想的。
- en: Markov models like Hidden Markov Models (HMMs) are traditional sequential models,
    but due to the time complexity of the inference algorithm (Viterbi, [1967](#bib.bib380))
    and because the size of transition matrix grows significantly with the increase
    of the discrete state space, in practice they are not applicable in dealing with
    problems involving large possible hidden states. The property that the hidden
    states of Markov models are only affected by the immediate hidden states further
    limits the power of this model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫模型如隐马尔可夫模型（HMMs）是传统的顺序模型，但由于推理算法的时间复杂度（Viterbi，[1967](#bib.bib380)）以及转移矩阵的大小随着离散状态空间的增加而显著增长，因此在处理涉及大量可能隐藏状态的问题时，实际上它们并不适用。马尔可夫模型的隐藏状态仅受当前隐藏状态的影响，这进一步限制了该模型的能力。
- en: RNN models are not proposed recently, but they greatly solve the above problems
    and some variants can amazingly achieve state-of-the-art performance in dialogue-related
    tasks as well as many other NLP tasks. The inductive bias of recurrent models
    is non-replaceable in many scenarios, and many up-to-date models incorporate the
    recurrence.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 模型并非最近提出，但它们极大地解决了上述问题，一些变体在对话相关任务以及许多其他 NLP 任务中都能惊人地实现最先进的性能。递归模型的归纳偏差在许多场景中是不可替代的，许多最新的模型都融入了递归机制。
- en: 2.2.1 Jordan-Type and Elman-Type RNNs
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 Jordan 类型和 Elman 类型 RNN
- en: In 1982, Hopfield introduced an early family of RNNs to solve pattern recognition
    tasks (Hopfield, [1982](#bib.bib140)). Jordan ([1986](#bib.bib159)) and Elman
    ([1990](#bib.bib80)) introduced two kinds of RNN architectures respectively. Generally,
    modern RNNs can be classified into Jordan-type RNNs and Elman-type RNNs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1982 年，Hopfield 引入了早期的 RNN 家族来解决模式识别任务（Hopfield，[1982](#bib.bib140)）。Jordan（[1986](#bib.bib159)）和
    Elman（[1990](#bib.bib80)）分别介绍了两种 RNN 架构。一般来说，现代 RNN 可以分为 Jordan 类型 RNN 和 Elman
    类型 RNN。
- en: '![Refer to caption](img/129d8bcf18fd55212511419a63d52492.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/129d8bcf18fd55212511419a63d52492.png)'
- en: (a) Jordan-type RNNs
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Jordan 类型 RNN
- en: '![Refer to caption](img/8693660674db8944063456afc2f3cf42.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8693660674db8944063456afc2f3cf42.png)'
- en: (b) Elman-type RNNs
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Elman 类型 RNN
- en: 'Figure 3: Graphical models of two basic types of RNNs'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：两种基本 RNN 类型的图示模型
- en: 'The Jordan-type RNNs are shown in Figure [3(a)](#S2.F3.sf1 "In Figure 3 ‣ 2.2.1
    Jordan-Type and Elman-Type RNNs ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence
    Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey"). $x_{t}$, $h_{t}$, and $y_{t}$ are
    the inputs, hidden state, and output of time step $t$, respectively. $W_{h}$,
    $W_{y}$ and $U_{h}$ are weight matrixes. Each update of hidden state is decided
    by the current input and the output of last time step while each output is decided
    by current hidden state. Thus the hidden state and output of time step $t$ can
    be calculated as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Jordan 类型 RNN 如图 [3(a)](#S2.F3.sf1 "在图 3 ‣ 2.2.1 Jordan 类型和 Elman 类型 RNN ‣ 2.2
    递归神经网络和基础序列到序列模型 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统性综述") 所示。$x_{t}$、$h_{t}$
    和 $y_{t}$ 分别是时间步 $t$ 的输入、隐藏状态和输出。$W_{h}$、$W_{y}$ 和 $U_{h}$ 是权重矩阵。每次隐藏状态的更新由当前输入和上一个时间步的输出决定，而每个输出由当前隐藏状态决定。因此，时间步
    $t$ 的隐藏状态和输出可以计算为：
- en: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})$ |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}y_{t-1}+b_{h})$ |  | (2) |'
- en: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (3) |'
- en: Where $b_{h}$ and $b_{y}$ are biases. $\sigma_{h}$ and $\sigma_{y}$ are activation
    functions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{h}$ 和 $b_{y}$ 是偏置。$\sigma_{h}$ 和 $\sigma_{y}$ 是激活函数。
- en: 'The Elman-type RNNs are shown in Figure [3(b)](#S2.F3.sf2 "In Figure 3 ‣ 2.2.1
    Jordan-Type and Elman-Type RNNs ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence
    Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey"). The difference is that each hidden
    state is decided by the current input and the hidden state of last time step.
    Thus the hidden state and output of time step $t$ can be calculated as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Elman 类型的 RNN 如图 [3(b)](#S2.F3.sf2 "在图 3 ‣ 2.2.1 Jordan 类型和 Elman 类型 RNN ‣ 2.2
    递归神经网络和 Vanilla 序列到序列模型 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统中的最新进展：系统综述") 所示。不同之处在于每个隐藏状态由当前输入和上一个时间步的隐藏状态决定。因此，时间步
    $t$ 的隐藏状态和输出可以计算为：
- en: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})$ |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{t}=\sigma_{h}(W_{h}x_{t}+U_{h}h_{t-1}+b_{h})$ |  | (4) |'
- en: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (5) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{t}=\sigma_{y}(W_{y}h_{t}+b_{y})$ |  | (5) |'
- en: Simple RNNs can model long-term dependencies theoretically. But in practical
    training, long-range dependencies are difficult to learn (Bengio et al., [1994](#bib.bib20);
    Hochreiter et al., [2001](#bib.bib138)). When backpropagating errors over many
    time steps, simple RNNs suffer from problems known as gradient vanishing and gradient
    explosion (Hochreiter and Schmidhuber, [1997](#bib.bib137)). Some solutions were
    proposed to solve these problems (Williams and Zipser, [1989](#bib.bib410); Pascanu
    et al., [2013](#bib.bib271)), which led to the inventions of some variants of
    traditional recurrent networks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的 RNN 理论上可以建模长期依赖关系。但在实际训练中，长范围依赖关系很难学习（Bengio et al., [1994](#bib.bib20);
    Hochreiter et al., [2001](#bib.bib138)）。在许多时间步上反向传播误差时，简单的 RNN 会遇到梯度消失和梯度爆炸的问题（Hochreiter
    和 Schmidhuber, [1997](#bib.bib137)）。一些解决方案被提出以解决这些问题（Williams 和 Zipser, [1989](#bib.bib410);
    Pascanu et al., [2013](#bib.bib271)），这导致了传统递归网络的一些变体的发明。
- en: 2.2.2 LSTM
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 LSTM
- en: 'Hochreiter and Schmidhuber ([1997](#bib.bib137)) introduced gate mechanisms
    in LSTM mainly to address the gradient vanishing problem. Input gate, forget gate
    and output gate were introduced to decide how much information from new inputs
    and past memories should be reserved. The model can be described by the following
    equations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber ([1997](#bib.bib137)) 引入了 LSTM 中的门控机制，主要是为了应对梯度消失问题。引入了输入门、遗忘门和输出门来决定应保留多少来自新输入和过去记忆的信息。该模型可以用以下方程描述：
- en: '|  | $\hat{h}^{(t)}=tanh\left(W^{\hat{h}x}x^{(t)}+W^{\hat{h}h}h^{(t-1)}+b_{\hat{h}}\right)$
    |  | (6) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{h}^{(t)}=tanh\left(W^{\hat{h}x}x^{(t)}+W^{\hat{h}h}h^{(t-1)}+b_{\hat{h}}\right)$
    |  | (6) |'
- en: '|  | $i^{(t)}=\sigma\left(W^{ix}x^{(t)}+W^{ih}h^{(t-1)}+b_{i}\right)$ |  |
    (7) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $i^{(t)}=\sigma\left(W^{ix}x^{(t)}+W^{ih}h^{(t-1)}+b_{i}\right)$ |  |
    (7) |'
- en: '|  | $f^{(t)}=\sigma\left(W^{fx}x^{(t)}+W^{fh}h^{(t-1)}+b_{f}\right)$ |  |
    (8) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $f^{(t)}=\sigma\left(W^{fx}x^{(t)}+W^{fh}h^{(t-1)}+b_{f}\right)$ |  |
    (8) |'
- en: '|  | $o^{(t)}=\sigma\left(W^{ox}x^{(t)}+W^{oh}h^{(t-1)}+b_{o}\right)$ |  |
    (9) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $o^{(t)}=\sigma\left(W^{ox}x^{(t)}+W^{oh}h^{(t-1)}+b_{o}\right)$ |  |
    (9) |'
- en: '|  | $s^{(t)}=\hat{h}^{(t)}\odot i^{(t)}+s^{(t-1)}\odot f^{(t)}$ |  | (10)
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $s^{(t)}=\hat{h}^{(t)}\odot i^{(t)}+s^{(t-1)}\odot f^{(t)}$ |  | (10)
    |'
- en: '|  | $h^{(t)}=tanh(s^{(t)})\odot o^{(t)}$ |  | (11) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{(t)}=tanh(s^{(t)})\odot o^{(t)}$ |  | (11) |'
- en: Where $t$ represents time step $t$. $i$, $f$ and $o$ are gates, denoting input
    gate, forget gate and output gate respectively. $x$, $\hat{h}$, $s$ and $h$ are
    input, short-term memory, long-term memory and output respectively. $b$ is bias
    and $W$ is weight matrix. $\odot$ denotes element-wise multiplication.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 代表时间步 $t$。$i$、$f$ 和 $o$ 是门控，分别表示输入门、遗忘门和输出门。$x$、$\hat{h}$、$s$ 和 $h$ 分别是输入、短期记忆、长期记忆和输出。$b$
    是偏置，$W$ 是权重矩阵。$\odot$ 表示逐元素乘法。
- en: The intuition of the term “Long Short-Term Memory" is that the proposed model
    applies both long-term and short-term memory vectors to encode the sequential
    data, and uses gate mechanisms to control the information flow. The performance
    of LSTM is impressive since that it achieved state-of-the-art results in many
    NLP tasks as a backbone model although this model was proposed in 1997.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: “长短期记忆”这一术语的直观意义在于，提出的模型应用了长期和短期记忆向量来编码序列数据，并使用门控机制来控制信息流。LSTM 的表现令人印象深刻，因为它在许多
    NLP 任务中作为骨干模型取得了最先进的结果，尽管该模型是在 1997 年提出的。
- en: 2.2.3 GRU
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 GRU
- en: 'Inspired by the gating mechanism, Cho et al. ([2014b](#bib.bib52)) proposed
    Gated Recurrent Unit (GRU), which can be modeled by the equations:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 受到门控机制的启发，Cho et al. ([2014b](#bib.bib52)) 提出了门控递归单元（GRU），其可以通过以下方程建模：
- en: '|  | $z^{(t)}=\sigma\left(W^{z}x^{(t)}+U^{z}h^{(t-1)}+b_{z}\right)$ |  | (12)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{(t)}=\sigma\left(W^{z}x^{(t)}+U^{z}h^{(t-1)}+b_{z}\right)$ |  | (12)
    |'
- en: '|  | $r^{(t)}=\sigma\left(W^{r}x^{(t)}+U^{r}h^{(t-1)}+b_{r}\right)$ |  | (13)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $r^{(t)}=\sigma\left(W^{r}x^{(t)}+U^{r}h^{(t-1)}+b_{r}\right)$ |  | (13)
    |'
- en: '|  | $\hat{h}^{(t)}=tanh\left(W^{h}x^{(t)}+U^{h}(r^{(t)}\odot h^{(t-1)})+b_{h}\right)$
    |  | (14) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{h}^{(t)}=tanh\left(W^{h}x^{(t)}+U^{h}(r^{(t)}\odot h^{(t-1)})+b_{h}\right)$
    |  | (14) |'
- en: '|  | $h^{(t)}=(1-z^{(t)})\odot h^{(t-1)}+z^{(t)}\odot\hat{h}^{(t)}$ |  | (15)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{(t)}=(1-z^{(t)})\odot h^{(t-1)}+z^{(t)}\odot\hat{h}^{(t)}$ |  | (15)
    |'
- en: Where $t$ represents time step $t$. $z$ and $r$ are gates, denoting update gate
    and reset gate respectively. $x$, $\hat{h}$ and $h$ are input, candidate activation
    vector and output respectively. $b$ is bias while $W$ and $U$ are weight matrixes.
    $\odot$ denotes element-wise multiplication.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$t$代表时间步$t$。$z$和$r$是门，分别表示更新门和重置门。$x$、$\hat{h}$和$h$分别是输入、候选激活向量和输出。$b$是偏置，$W$和$U$是权重矩阵。$\odot$表示逐元素相乘。
- en: LSTM and GRU, as two types of gating units, are very similar to each other (Chung
    et al., [2014](#bib.bib54)). The most prominent common point between them is that
    from time step $t$ to time step $t+1$, an additive component is introduced to
    update the state whereas simple RNNs always replace the activation. Both LSTM
    and GRU keep certain old components and mix them with new contents. This property
    enables the units to remember the information of history steps farther back and,
    more importantly, avoid gradient vanishing problems when backpropagating the error.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM和GRU作为两种门控单元，非常相似 (Chung et al., [2014](#bib.bib54))。它们之间最显著的共同点是，从时间步$t$到时间步$t+1$，引入了一个加性组件来更新状态，而简单的RNN总是替换激活。LSTM和GRU都保留了一定的旧组件，并将其与新内容混合。这一特性使得这些单元能够记住更远历史步骤的信息，更重要的是，避免了反向传播错误时的梯度消失问题。
- en: There also exist several differences between them. LSTM exposes its memory content
    under the control of the output gate, while the same content in GRU is in an uncontrolled
    manner. Additionally, different from LSTM, GRU does not independently gate the
    amount of new memory content being added. And if looking from experimental perspective,
    GRU has fewer parameters, which contributes to its faster convergence and better
    generalization ability. It has also been shown that GRU can achieve better performance
    in smaller datasets (Chung et al., [2014](#bib.bib54)). However, Gruber and Jockisch
    ([2020](#bib.bib113)) showed that LSTM cells exhibited consistently better performance
    in a large-scale analysis of Neural Machine Translation.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 它们之间还存在一些差异。LSTM在输出门的控制下暴露其记忆内容，而GRU中的相同内容则以不受控制的方式存在。此外，与LSTM不同，GRU不独立控制新增记忆内容的数量。从实验角度来看，GRU参数较少，这有助于其更快的收敛和更好的泛化能力。研究表明，GRU在较小的数据集上能取得更好的性能 (Chung
    et al., [2014](#bib.bib54))。然而，Gruber和Jockisch ([2020](#bib.bib113)) 表明，在大规模神经机器翻译分析中，LSTM单元表现出持续较好的性能。
- en: 2.2.4 Bidirectional Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 双向递归神经网络
- en: 'In sequence learning, not only the past information is essential to the model
    inference, the future information should also be considered to achieve a better
    inference ability. Schuster and Paliwal ([1997](#bib.bib310)) proposed the bi-directional
    recurrent neural networks (BRNNs), which had two kinds of hidden layers: the first
    encoded information from past time steps while the second encoded information
    in a flipped direction. The model can be described using the equations:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列学习中，不仅过去的信息对模型推断至关重要，未来的信息也应考虑以实现更好的推断能力。Schuster和Paliwal ([1997](#bib.bib310))
    提出了双向递归神经网络（BRNNs），它具有两种隐藏层：第一种编码过去时间步的信息，而第二种则编码反向信息。该模型可以用以下方程描述：
- en: '|  | $h^{(t)}=\sigma\left(W^{hx}x^{(t)}+W^{hh}h^{(t-1)}+b_{h}\right)$ |  |
    (16) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $h^{(t)}=\sigma\left(W^{hx}x^{(t)}+W^{hh}h^{(t-1)}+b_{h}\right)$ |  |
    (16) |'
- en: '|  | $z^{(t)}=\sigma\left(W^{zx}x^{(t)}+W^{zz}z^{(t+1)}+b_{z}\right)$ |  |
    (17) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $z^{(t)}=\sigma\left(W^{zx}x^{(t)}+W^{zz}z^{(t+1)}+b_{z}\right)$ |  |
    (17) |'
- en: '|  | $\hat{y}^{(t)}=softmax\left(W^{yh}h^{(t)}+W^{yz}z^{(t)}+b_{y}\right)$
    |  | (18) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{y}^{(t)}=softmax\left(W^{yh}h^{(t)}+W^{yz}z^{(t)}+b_{y}\right)$
    |  | (18) |'
- en: Where $h$ and $z$ are the two hidden layers. Other variables are defined in
    the same way as in the case of LSTMs and GRUs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h$和$z$是两个隐藏层。其他变量的定义与LSTM和GRU中的相同。
- en: 2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder Models)
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5 原始序列到序列模型（编码器-解码器模型）
- en: 'Sutskever et al. ([2014](#bib.bib352)) first proposed the sequence-to-sequence
    model to solve the machine translation tasks. The sequence-to-sequence model aimed
    to map an input sequence to an output sequence by first using an encoder to map
    the input sequence into an intermediate vector and a decoder further generated
    the output based on the intermediate vector and history generated by the decoder.
    The equations below illustrate the encoder-decoder model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Sutskever 等人（[2014](#bib.bib352)）首次提出了序列到序列模型来解决机器翻译任务。序列到序列模型的目的是通过首先使用编码器将输入序列映射为一个中间向量，然后由解码器基于中间向量和解码器生成的历史进一步生成输出，从而将输入序列映射到输出序列。下述方程展示了编码器-解码器模型：
- en: '|  | $Encoder:h_{t}=E(h_{t-1},x_{t})$ |  | (19) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $Encoder:h_{t}=E(h_{t-1},x_{t})$ |  | (19) |'
- en: '|  | $Decoder:y_{t}=D(h_{t},y_{t-1})$ |  | (20) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $Decoder:y_{t}=D(h_{t},y_{t-1})$ |  | (20) |'
- en: Where $t$ is the time step, $h$ is the hidden vector and $y$ is the output vector.
    $E$ and $D$ are the sequential cells used by the encoder and decoder respectively.
    The last hidden state of the encoder is the intermediate vector, and this vector
    is usually used to initialize the first hidden state of the decoder. At encoding
    time, each hidden state is decided by the hidden state of the previous time step
    and the input at the current time step, while at decoding time, each hidden state
    is decided by the current hidden state and the output of the previous time step.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 是时间步，$h$ 是隐藏向量，$y$ 是输出向量。$E$ 和 $D$ 分别是编码器和解码器使用的顺序单元。编码器的最后隐藏状态是中间向量，这个向量通常用于初始化解码器的第一个隐藏状态。在编码时，每个隐藏状态由前一个时间步的隐藏状态和当前时间步的输入决定，而在解码时，每个隐藏状态由当前隐藏状态和前一个时间步的输出决定。
- en: This model is powerful because it is not restricted to fixed-length inputs and
    outputs. Instead, the length of the source sequence and target sequence can differ.
    Based on this model, many more advanced sequence-to-sequence models have been
    developed, which will be discussed in this and subsequent sections.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型之所以强大，是因为它不受限于固定长度的输入和输出。相反，源序列和目标序列的长度可以不同。基于该模型，已经开发出了许多更先进的序列到序列模型，这些模型将在本节及后续章节中讨论。
- en: RNNs play an essential role in neural dialogue systems for their strong ability
    to encode sequential text information. RNNs and their variants are found in many
    dialogue systems. Task-oriented systems apply RNNs as encoders of dialogue context,
    dialogue state, knowledge base entries, and domain tags (Moon et al., [2019](#bib.bib257);
    Chen et al., [2019b](#bib.bib44); Wu et al., [2019b](#bib.bib415), [a](#bib.bib413)).
    Open-domain systems apply RNNs as dialogue history encoders (Sankar et al., [2019](#bib.bib304);
    Du and Black, [2019](#bib.bib76); Ji et al., [2020](#bib.bib155); Chen et al.,
    [2020b](#bib.bib46)), among which retrieval-based systems model dialogue history
    and candidate responses together (Zhu et al., [2018](#bib.bib466); Tang et al.,
    [2019](#bib.bib362); Feldman and El-Yaniv, [2019](#bib.bib85); Lu et al., [2019b](#bib.bib231)).
    In knowledge-grounded systems, RNNs are encoders of outside knowledge sources
    (e.g., background, persona, topic, etc.) (Shuster et al., [2019](#bib.bib324);
    Majumder et al., [2020b](#bib.bib239); Chen et al., [2020b](#bib.bib46); Cho and
    May, [2020](#bib.bib50)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 在神经对话系统中扮演了重要角色，因为它们具有强大的编码顺序文本信息的能力。RNN 及其变体在许多对话系统中都有应用。任务导向系统使用 RNN 作为对话上下文、对话状态、知识库条目和领域标签的编码器（Moon
    等人，[2019](#bib.bib257)；Chen 等人，[2019b](#bib.bib44)；Wu 等人，[2019b](#bib.bib415)，[a](#bib.bib413)）。开放域系统则将
    RNN 作为对话历史的编码器（Sankar 等人，[2019](#bib.bib304)；Du 和 Black，[2019](#bib.bib76)；Ji
    等人，[2020](#bib.bib155)；Chen 等人，[2020b](#bib.bib46)），其中基于检索的系统将对话历史和候选响应一起建模（Zhu
    等人，[2018](#bib.bib466)；Tang 等人，[2019](#bib.bib362)；Feldman 和 El-Yaniv，[2019](#bib.bib85)；Lu
    等人，[2019b](#bib.bib231)）。在知识基础系统中，RNN 作为外部知识来源的编码器（例如背景、个性、话题等）（Shuster 等人，[2019](#bib.bib324)；Majumder
    等人，[2020b](#bib.bib239)；Chen 等人，[2020b](#bib.bib46)；Cho 和 May，[2020](#bib.bib50)）。
- en: Furthermore, as the decoder of sequence-to-sequence models in dialogue systems (Huang
    et al., [2020c](#bib.bib150); Song et al., [2019](#bib.bib338); Liu et al., [2019](#bib.bib222);
    Lin et al., [2019](#bib.bib213)), RNNs usually decode the hidden state of utterance
    sequences by greedy search or beam search (Aubert et al., [1994](#bib.bib11)).
    These decoding mechanisms cause problems like generic responses, which will be
    discussed in later sections.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作为对话系统中序列到序列模型的解码器（Huang et al., [2020c](#bib.bib150); Song et al., [2019](#bib.bib338);
    Liu et al., [2019](#bib.bib222); Lin et al., [2019](#bib.bib213)），RNN 通常通过贪心搜索或束搜索来解码话语序列的隐藏状态（Aubert
    et al., [1994](#bib.bib11)）。这些解码机制会引发诸如通用响应等问题，后续章节将对此进行讨论。
- en: Some works (Liu et al., [2019](#bib.bib222); Mehri et al., [2019](#bib.bib244);
    Chen et al., [2019c](#bib.bib45); Ma et al., [2020a](#bib.bib235)) combined RNNs
    as a part of dialogue representation models to train dialogue embeddings and further
    improved the performance of dialogue-related tasks. These embedding models were
    trained on dialogue tasks and present more dialogue features. They consistently
    outperformed state-of-the-art contextual representation models (e.g., BERT, ELMo,
    and GPT) in some dialogue tasks when these contextual representation models were
    not fine-tuned for the specific tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究（Liu et al., [2019](#bib.bib222); Mehri et al., [2019](#bib.bib244); Chen
    et al., [2019c](#bib.bib45); Ma et al., [2020a](#bib.bib235)）将 RNN 结合为对话表示模型的一部分，以训练对话嵌入，并进一步提高了对话相关任务的性能。这些嵌入模型在对话任务上进行了训练，并呈现出更多对话特征。当这些上下文表示模型没有针对特定任务进行微调时，它们在某些对话任务中的表现始终优于最先进的上下文表示模型（如
    BERT、ELMo 和 GPT）。
- en: 2.3 Hierarchical Recurrent Encoder-Decoder (HRED)
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 层次递归编码器-解码器 (HRED)
- en: Hierarchical Recurrent Encoder-Decoder (HRED) is a context-aware sequence-to-sequence
    model. It was first proposed by Sordoni et al. ([2015a](#bib.bib339)) to address
    the context-aware online query suggestion problem. It was designed to be aware
    of history queries and the proposed model can provide rare and high-quality results.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 层次递归编码器-解码器 (HRED) 是一个上下文感知的序列到序列模型。它最初由 Sordoni et al. ([2015a](#bib.bib339))
    提出，旨在解决上下文感知的在线查询建议问题。它被设计为能够关注历史查询，并且提出的模型能够提供稀有且高质量的结果。
- en: With the popularity of the sequence-to-sequence model, Serban et al. ([2016](#bib.bib312))
    extended HRED to the dialogue domain and built an end-to-end context-aware dialogue
    system. HRED achieved noticeable improvements in dialogue and end-to-end question
    answering. This work attracted even more attention than the original paper for
    that dialogue systems are a perfect setting for the application of HRED. Traditional
    dialogue systems (Ritter et al., [2011](#bib.bib300)) generated responses based
    on the single-turn messages, which sacrificed the information in the dialogue
    history. Sordoni et al. ([2015b](#bib.bib340)) combined dialogue history turns
    with a window size of 3 as the input of a sequence-to-sequence model for response
    generation, which is limited as well for that they encode the dialogue history
    only in token-level. The “turn-by-turn" characteristic of dialogue indicated that
    the turn-level information also matters. The HRED learned both token-level and
    turn-level representation, thus exhibiting promising dialogue context awareness.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 随着序列到序列模型的流行，Serban et al. ([2016](#bib.bib312)) 将 HRED 扩展到对话领域，并建立了一个端到端的上下文感知对话系统。HRED
    在对话和端到端问答中取得了显著的改进。这项工作比原始论文更受关注，因为对话系统是 HRED 应用的完美场景。传统对话系统（Ritter et al., [2011](#bib.bib300)）基于单轮消息生成响应，这牺牲了对话历史中的信息。Sordoni
    et al. ([2015b](#bib.bib340)) 将对话历史轮次与大小为 3 的窗口结合，作为序列到序列模型的输入进行响应生成，但这种方法也有限，因为它们仅在标记级别编码对话历史。对话的“轮次”特征表明轮次级别的信息也很重要。HRED
    学习了标记级别和轮次级别的表示，从而展示了有希望的对话上下文感知能力。
- en: '![Refer to caption](img/1ace5d3a7d1f279b2018e6f3c1bad690.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ace5d3a7d1f279b2018e6f3c1bad690.png)'
- en: 'Figure 4: The HRED model in a dialogue setting (Serban et al., [2016](#bib.bib312))'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：对话设置中的 HRED 模型（Serban et al., [2016](#bib.bib312)）
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2.3 Hierarchical Recurrent Encoder-Decoder (HRED)
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") represents the HRED in a dialogue setting.
    HRED models the token-level and turn-level sequences hierarchically with two levels
    of RNNs: a token-level RNN consisting of an encoder and a decoder, and a turn-level
    context RNN. The encoder RNN encodes the utterance of each turn token by token
    into a hidden state. This hidden state is then taken as the input of the context
    RNN at each turn-level time step. Thus the turn-level context RNN iteratively
    keeps track of the history utterances. The hidden state of context RNN at turn
    $t$ represents a summary of the utterances up to turn $t$ and is used to initialize
    the first hidden state of decoder RNN, which is similar to a standard decoder
    in sequence-to-sequence models (Sutskever et al., [2014](#bib.bib352)). All of
    the three RNNs described above apply GRU cells as the recurrent unit, and the
    parameters of encoder and decoder are shared for each utterance.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S2.F4 "图 4 ‣ 2.3 层次递归编码器-解码器 (HRED) ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统性调查")
    显示了对话环境中的 HRED。HRED 通过两个层次的 RNN 层次化建模令牌级别和轮次级别的序列：一个由编码器和解码器组成的令牌级 RNN，以及一个轮次级上下文
    RNN。编码器 RNN 将每轮的发言逐个令牌编码为隐藏状态。这个隐藏状态在每个轮次级时间步中作为上下文 RNN 的输入。因此，轮次级上下文 RNN 迭代地跟踪历史发言。轮次级上下文
    RNN 在轮次 $t$ 的隐藏状态表示了直到轮次 $t$ 的发言摘要，并用来初始化解码器 RNN 的第一个隐藏状态，这类似于序列到序列模型中的标准解码器 (Sutskever
    et al., [2014](#bib.bib352))。上述三个 RNN 都使用 GRU 单元作为递归单元，编码器和解码器的参数对于每个发言是共享的。
- en: 'Serban et al. ([2017a](#bib.bib313)) further proposed Latent Variable Hierarchical
    Recurrent Encoder-Decoder (VHRED) to model complex dependencies between sequences.
    Based on HRED, VHRED combined a latent variable into the decoder and turned the
    decoding process into a two-step generation process: sampling a latent variable
    at the first step and then generating the response conditionally. VHRED was trained
    with a variational lower bound on the log-likelihood and exhibited promising improvement
    in diversity, length, and quality of generated responses.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Serban 等人 ([2017a](#bib.bib313)) 进一步提出了潜变量层次递归编码器-解码器 (VHRED) 以建模序列之间的复杂依赖关系。在
    HRED 的基础上，VHRED 将潜变量结合到解码器中，将解码过程转变为两步生成过程：在第一步中采样潜变量，然后根据条件生成响应。VHRED 使用对数似然的变分下界进行训练，并在生成响应的多样性、长度和质量方面表现出有希望的改进。
- en: 'Many recent works in dialogue-related tasks apply HRED-based frameworks to
    capture hierarchical dialogue features. Zhang et al. ([2019a](#bib.bib446)) argued
    that standard HRED processed all contexts in dialogue history indiscriminately.
    Inspired by the architecture of Transformer (Vaswani et al., [2017](#bib.bib377)),
    they proposed ReCoSa, a self-attention-based hierarchical model. It first applied
    LSTM to encode token-level information into context hidden vectors and then calculated
    the self-attention for both the context vectors and masked response vectors. At
    the decoding stage, the encoder-decoder attention was calculated to facilitate
    the decoding. Shen et al. ([2019](#bib.bib321)) proposed a hierarchical model
    consisting of 3 hierarchies: the discourse-level which captures the global knowledge,
    the pair-level which captured the topic information in utterance pairs, and the
    utterance level which captured the content information. Such a multi-hierarchy
    structure contributed to its higher quality responses in terms of diversity, coherence,
    and fluency. Chauhan et al. ([2019](#bib.bib38)) applied HRED and VGG-19 as a
    multimodal HRED (MHRED). The HRED encoded hierarchical dialogue context while
    VGG-19 extracted visual features for all images in the corresponding turn. With
    the addition of a position-aware attention mechanism, the model showed more diverse
    and accurate responses in a visually grounded setting. Mehri et al. ([2019](#bib.bib244))
    learned dialogue context representations via four sub-tasks, three of which (next-utterance
    generation, masked-utterance retrieval, and inconsistency identification) made
    uses of HRED as the context encoder, and good performance was achieved. Cao et al.
    ([2019](#bib.bib35)) used HRED to encode the dialogue history between therapists
    and patients to categorize therapist and client MI behavioral codes and predict
    future codes. Qiu et al. ([2020](#bib.bib283)) applied an LSTM-based VHRED to
    address the two-agent and multi-agent dialogue structure induction problem in
    an unsupervised fashion. On top of that, they applied a Conditional Random Field
    model in two-agent dialogues and a non-projective dependency tree in multi-agent
    dialogues, both of them achieving better performance in dialogue structure modeling.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在对话相关任务中的许多研究应用了基于HRED的框架来捕捉层次化的对话特征。张等人（[2019a](#bib.bib446)）认为标准的HRED对对话历史中的所有上下文进行了不加区分的处理。受Transformer（Vaswani
    et al., [2017](#bib.bib377)）架构的启发，他们提出了ReCoSa，一种基于自注意力的层次化模型。该模型首先应用LSTM将标记级别的信息编码成上下文隐藏向量，然后计算上下文向量和掩蔽响应向量的自注意力。在解码阶段，计算了编码器-解码器注意力以促进解码。沈等人（[2019](#bib.bib321)）提出了一种包含3个层次的层次化模型：话语级别捕捉全球知识，配对级别捕捉话语对中的主题信息，话语级别捕捉内容信息。这种多层次结构提高了响应的质量，包括多样性、一致性和流畅性。Chauhan等人（[2019](#bib.bib38)）将HRED和VGG-19应用于多模态HRED（MHRED）。HRED对层次化对话上下文进行编码，而VGG-19提取了相应轮次中所有图像的视觉特征。通过增加位置感知注意力机制，该模型在视觉基础设置中展示了更具多样性和准确性的响应。Mehri等人（[2019](#bib.bib244)）通过四个子任务学习对话上下文表示，其中三个（下一个话语生成、掩蔽话语检索和不一致性识别）利用HRED作为上下文编码器，并取得了良好的性能。Cao等人（[2019](#bib.bib35)）使用HRED对治疗师和患者之间的对话历史进行编码，以对治疗师和客户的MI行为编码进行分类，并预测未来的编码。Qiu等人（[2020](#bib.bib283)）应用了一种基于LSTM的VHRED，以无监督的方式解决两代理和多代理对话结构归纳问题。在此基础上，他们在两代理对话中应用了条件随机场模型，在多代理对话中应用了非投影依赖树，这两者在对话结构建模方面都取得了更好的性能。
- en: 2.4 Memory Networks
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 记忆网络
- en: Memory is a crucial component when addressing problems regarding past experiences
    or outside knowledge sources. The hippocampus of human brains and the hard disk
    of computers are the components that humans and computers depend on for reading
    and writing memories. Traditional models rarely have a memory component, thus
    lacking the ability of knowledge reusing and reasoning. RNNs iteratively pass
    history information across time steps, which, to some extent, can be viewed as
    a memory model. However, even for LSTM, which is a powerful variant of RNN equipped
    with a long-term and short-term memory, the memory module is too small and facts
    are not explicitly discriminated, thus not being able to compress specific knowledge
    facts and reuse them in tasks.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆在处理与过去经验或外部知识来源相关的问题时是一个关键组件。人脑的海马体和计算机的硬盘是人类和计算机读取和写入记忆所依赖的组件。传统模型很少有记忆组件，因此缺乏知识重用和推理的能力。RNN通过时间步迭代地传递历史信息，这在某种程度上可以被视为一种记忆模型。然而，即使是LSTM，这种具备长期和短期记忆的强大RNN变体，其记忆模块也过于简单，且事实没有被明确区分，因此无法压缩特定的知识事实并在任务中重用。
- en: 'Weston et al. ([2014](#bib.bib402)) proposed memory networks, a model that
    is endowed with a memory component. As described in their work, a memory network
    has five modules: a memory module which stores the representations of memory facts;
    an ‘I’ module which maps the input memory facts into embedded representations;
    a ‘G’ module which decides the update of the memory module; an ‘O’ module which
    generates the output conditioned on the input representation and memory representation;
    an ‘R’ module which organizes the final response based on the output of ‘O’ module.
    This model needs a strong supervision signal for each module and thus is not practical
    to train in an end-to-end fashion.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Weston等人 ([2014](#bib.bib402)) 提出了记忆网络，这是一种具有记忆组件的模型。正如他们的工作中所描述的，记忆网络有五个模块：一个存储记忆事实表示的记忆模块；一个将输入记忆事实映射到嵌入表示的‘I’模块；一个决定记忆模块更新的‘G’模块；一个根据输入表示和记忆表示生成输出的‘O’模块；一个根据‘O’模块的输出组织最终响应的‘R’模块。该模型需要对每个模块提供强监督信号，因此不适合端到端训练。
- en: Sukhbaatar et al. ([2015](#bib.bib349)) extended their prior work to an end-to-end
    memory network, which was commonly accepted as a standard memory network being
    easy to train and apply.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Sukhbaatar等人 ([2015](#bib.bib349)) 将他们之前的工作扩展到端到端的记忆网络，这被普遍接受为一种标准记忆网络，易于训练和应用。
- en: '![Refer to caption](img/8d1b3c5df55f53ec6fe5135a7a769aab.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d1b3c5df55f53ec6fe5135a7a769aab.png)'
- en: 'Figure 5: The structure of end-to-end memory networks (Sukhbaatar et al., [2015](#bib.bib349))'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：端到端记忆网络的结构 (Sukhbaatar等，[2015](#bib.bib349))
- en: 'Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Memory Networks ‣ 2 Neural Models in Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey") represents the proposed end-to-end memory networks. Its architecture
    consists of three stages: weight calculation, memory selection, and final prediction.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S2.F5 "图5 ‣ 2.4 记忆网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述") 表示了提出的端到端记忆网络。其架构包括三个阶段：权重计算、记忆选择和最终预测。
- en: 'Weight calculation. The model first converts the input memory set $\{x_{i}\}$
    into memory representations $\{m_{i}\}$ using a representation model $A$. Then
    it maps the input query into its embedding space using another representation
    model $B$, obtaining an embedding vector $u$. The final weights are calculated
    as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 权重计算。模型首先使用表示模型$A$将输入记忆集$\{x_{i}\}$转换为记忆表示$\{m_{i}\}$。然后，使用另一个表示模型$B$将输入查询映射到其嵌入空间，获得嵌入向量$u$。最终的权重计算如下：
- en: '|  | $p_{i}=Softmax(u^{T}m_{i})$ |  | (21) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $p_{i}=Softmax(u^{T}m_{i})$ |  | (21) |'
- en: Where $p_{i}$ is the weight corresponding to each input memory $x_{i}$ conditioned
    on the query.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p_{i}$是与每个输入记忆$x_{i}$在查询条件下对应的权重。
- en: 'Memory selection. Before generating the final prediction, a selected memory
    vector is generated by first encoding the input memory $x_{i}$ into an embedded
    vector $c_{i}$ using another representation model $C$, then calculating the weighted
    sum over the $\{c_{i}\}$ using the weights calculated in the previous stage:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆选择。在生成最终预测之前，通过首先使用另一种表示模型$C$将输入记忆$x_{i}$编码为嵌入向量$c_{i}$，然后根据在上一阶段计算的权重对$\{c_{i}\}$进行加权求和，生成一个选择的记忆向量：
- en: '|  | $o=\sum_{i}p_{i}c_{i}$ |  | (22) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | $o=\sum_{i}p_{i}c_{i}$ |  | (22) |'
- en: Where o represents the selected memory vector. This vector cannot be found in
    memory representations. The soft memory selection facilitates differentiability
    in gradient computing, which makes the whole model end-to-end trainable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 o 代表选择的记忆向量。这个向量在记忆表示中无法找到。软记忆选择有助于梯度计算的可微性，使得整个模型可以端到端训练。
- en: 'Final prediction. The final prediction is obtained by mapping the sum vector
    of the selected memory $o$ and the embedded query $u$ into a probability vector
    $\hat{a}$:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最终预测。最终预测是通过将选定记忆 $o$ 和嵌入查询 $u$ 的和向量映射到概率向量 $\hat{a}$ 获得的：
- en: '|  | $\hat{\alpha}=Softmax(W(o+u))$ |  | (23) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\alpha}=Softmax(W(o+u))$ |  | (23) |'
- en: Many dialogue-related works incorporate memory networks into their framework,
    especially for tasks involving an external knowledge base like task-oriented dialogue
    systems, knowledge-grounded dialogue systems, and QA.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 许多与对话相关的工作将记忆网络纳入其框架中，特别是对于涉及外部知识库的任务，如任务导向对话系统、知识驱动对话系统和问答系统。
- en: Memory networks for task-oriented dialogue systems
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务导向对话系统的记忆网络
- en: 'Chen et al. ([2019c](#bib.bib45)) argued that state-of-the-art task-oriented
    dialogue systems tended to combine dialogue history and knowledge base entries
    in a single memory module, which influenced the response quality. They proposed
    a task-oriented system that consists of three memory modules: two long-term memory
    modules storing the dialogue history and the knowledge base respectively; a working
    memory module that memorizes two distributions and controls the final word prediction.
    He et al. ([2020a](#bib.bib128)) trained a task-oriented dialogue system with
    a “Two-teacher-one-student" framework to improve the knowledge retrieval and response
    quality of their memory networks. They first trained two teacher networks using
    reinforcement learning with complementary goal-specific reward functions respectively.
    Then with a GAN framework, they trained two discriminators to teach the student
    memory network to generate responses similar to those of the teachers, transferring
    the expert knowledge from the two teachers to the student. The advantage is that
    this training framework needs only weak supervision and the student network can
    benefit from the complementary targets of teacher networks. Kim et al. ([2019](#bib.bib170))
    solved the dialogue state tracking in task-oriented dialogue systems with a memory
    network that memorized the dialogue states. Different from other works, they did
    not update all dialogue states in the memory module from scratch. Instead, their
    model first predicted which states needed to be updated and then overwrote the
    target states. By selectively overwriting the memory module, they improved the
    efficiency of the dialogue state tracking task. Dai et al. ([2020](#bib.bib61))
    applied the MemN2N (Sukhbaatar et al., [2015](#bib.bib349)) as task-oriented utterance
    encoder, memorizing the existing responses and dialogue history. Then they used
    model-agnostic meta-learning (MAML) (Finn et al., [2017](#bib.bib92)) to train
    the framework to retrieve correct responses in a few-shot fashion.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人（[2019c](#bib.bib45)）认为，最先进的任务导向对话系统往往将对话历史和知识库条目结合在一个记忆模块中，这影响了响应质量。他们提出了一个由三个记忆模块组成的任务导向系统：两个长期记忆模块分别存储对话历史和知识库；一个工作记忆模块记忆两个分布并控制最终的词预测。何等人（[2020a](#bib.bib128)）使用“两个教师一个学生”框架训练了一个任务导向对话系统，以提高记忆网络的知识检索和响应质量。他们首先使用带有互补目标奖励函数的强化学习分别训练了两个教师网络。然后通过
    GAN 框架，训练了两个判别器来教学生记忆网络生成类似于教师的响应，将专家知识从两个教师转移到学生。其优势在于这个训练框架只需要弱监督，学生网络可以从教师网络的互补目标中受益。金等人（[2019](#bib.bib170)）通过一个记忆网络解决了任务导向对话系统中的对话状态追踪问题。与其他工作不同的是，他们并没有从头更新记忆模块中的所有对话状态。相反，他们的模型首先预测需要更新的状态，然后覆盖目标状态。通过选择性地覆盖记忆模块，他们提高了对话状态追踪任务的效率。戴等人（[2020](#bib.bib61)）应用了
    MemN2N（Sukhbaatar 等人，[2015](#bib.bib349)）作为任务导向的发言编码器，记忆现有的响应和对话历史。然后他们使用模型无关的元学习（MAML）（Finn
    等人，[2017](#bib.bib92)）训练框架，以少量样本的方式检索正确的响应。
- en: Memory networks for open-domain dialogue systems
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 开放域对话系统的记忆网络
- en: Tian et al. ([2019](#bib.bib369)) proposed a knowledge-grounded chit-chat system.
    A memory network was used to store query-response pairs and at the response generation
    stage, the generator produced the response conditioned on both the input query
    and memory pairs. It extracted key-value information from the query-response pairs
    in memory and combined them into token prediction. Xu et al. ([2019](#bib.bib425))
    proposed to use meta-words to generate responses in open-domain systems in a controllable
    way. Meta-words are phrases describing response attributes. Using a goal-tracking
    memory network, they memorized the meta-words and generated responses based on
    the user message while incorporating meta-words at the same time. Gan et al. ([2019](#bib.bib95))
    performed multi-step reasoning conditioned on a dialogue history memory module
    and a visual memory module. Two memory modules recurrently refined the representation
    to perform the next reasoning process. Experimental results illustrated the benefits
    of combining image and dialogue clues to improve the performance of visual dialogue
    systems. Han et al. ([2019](#bib.bib122)) trained a reinforcement learning agent
    to decide which memory vector can be replaced when the memory module is full to
    improve the accuracy and efficiency of the document-grounded question-answering
    task. They solved the scalability problem of memory networks by learning the query-specific
    value corresponding to each memory. Gao et al. ([2020c](#bib.bib102)) solved the
    same problem in a conversational machine reading task. They proposed an Explicit
    Memory Tracker (EMT) to decide whether the provided information in memory is enough
    for final prediction. Furthermore, a coarse-to-fine strategy was applied for the
    agent to make clarification questions to request additional information and refine
    the reasoning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Tian 等人 ([2019](#bib.bib369)) 提出了一个基于知识的聊天系统。使用了记忆网络来存储查询-响应对，在响应生成阶段，生成器根据输入查询和记忆对生成响应。它从记忆中的查询-响应对中提取关键信息，并将这些信息组合到标记预测中。Xu
    等人 ([2019](#bib.bib425)) 提议使用元词在开放域系统中以可控的方式生成响应。元词是描述响应属性的短语。通过使用目标跟踪记忆网络，他们记住了元词，并基于用户消息生成响应，同时结合了元词。Gan
    等人 ([2019](#bib.bib95)) 在对话历史记忆模块和视觉记忆模块的条件下进行了多步骤推理。这两个记忆模块反复优化表示，以进行下一步推理。实验结果表明，结合图像和对话线索有助于提高视觉对话系统的性能。Han
    等人 ([2019](#bib.bib122)) 训练了一个强化学习代理来决定当记忆模块满时可以替换哪个记忆向量，以提高文档基础问答任务的准确性和效率。他们通过学习与每个记忆相关的查询特定值解决了记忆网络的可扩展性问题。Gao
    等人 ([2020c](#bib.bib102)) 在对话机器阅读任务中解决了同样的问题。他们提出了一种显式记忆追踪器（EMT），以决定记忆中提供的信息是否足够用于最终预测。此外，代理采用了粗到细的策略，提出澄清问题以请求额外信息并完善推理。
- en: 2.5 Attention and Transformer
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 注意力机制和变换器
- en: 'As introduced in Section [2.2](#S2.SS2 "2.2 Recurrent Neural Networks and Vanilla
    Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey"), traditional sequence-to-sequence
    models decode the token conditioning on the current hidden state and output vector
    of last time step, which is formulated as:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2.2](#S2.SS2 "2.2 循环神经网络和简单序列到序列模型 ‣ 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述")节中介绍的，传统的序列到序列模型在解码时依据当前隐藏状态和上一个时间步的输出向量进行解码，其公式为：
- en: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},h_{i})$ |  | (24) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},h_{i})$ |  | (24) |'
- en: Where g is a sequential model which maps the input vectors into a probability
    vector.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 g 是一个顺序模型，将输入向量映射到概率向量。
- en: 'However, such a decoding scheme is limited when the input sentence is long.
    RNNs are not able to encode all information into a fixed-length hidden vector.
    Cho et al. ([2014a](#bib.bib51)) proved via experiments that a sequence-to-sequence
    model performed worse when the input sequence got longer. Also, for the limited-expression
    ability of a fixed-length hidden vector, the performance of the decoding scheme
    in Equation ([24](#S2.E24 "In 2.5 Attention and Transformer ‣ 2 Neural Models
    in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")) largely depends on the first few steps of decoding, and
    if the decoder fails to have a good start, the whole sequence would be negatively
    affected.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当输入句子很长时，这种解码方案存在局限性。循环神经网络无法将所有信息编码为固定长度的隐藏向量。Cho等人（2014a）通过实验证明，当输入序列变长时，序列到序列模型的性能会下降。此外，由于固定长度隐藏向量的表达能力有限，方程（24）中的解码方案的性能在很大程度上取决于前几个解码步骤。如果解码器没有一个良好的启动，整个序列都会受到负面影响。
- en: 2.5.1 Attention
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1 注意力
- en: '![Refer to caption](img/4640c9ae734ac0e18a2c70d3b119c417.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: 图像 ![参考说明](img/4640c9ae734ac0e18a2c70d3b119c417.png)
- en: 'Figure 6: The attention model (Bahdanau et al., [2014](#bib.bib12))'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：注意力模型（Bahdanau等人，2014）
- en: 'Bahdanau et al. ([2014](#bib.bib12)) proposed the attention mechanism in the
    machine translation task. They described the method as “jointly align and translate",
    which illustrated the sequence-to-sequence translation model as an encoder-decoder
    model with attention. At the decoding stage, each decoding state would consider
    which parts of the encoded source sentence are correlated, instead of depending
    only on the immediate prior output token. The output probability distribution
    can be described as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Bahdanau等人（2014）在机器翻译任务中提出了注意力机制。他们将该方法描述为“联合对齐和翻译”，将序列到序列翻译模型作为带有注意力的编码器-解码器模型进行了说明。在解码阶段，每个解码状态都会考虑与编码的源句子相关的部分，而不仅仅依赖于之前的输出标记。输出概率分布可以描述为：
- en: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},s_{i},c_{i})$ |  | (25) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{i}&#124;y_{1},...,y_{i-1},x)=g(y_{i-1},s_{i},c_{i})$ |  | (25) |'
- en: 'Where $i$ denotes the $i^{th}$ time step; $y_{i}$ is the output token, $s_{i}$
    is the decoder hidden state and $c_{i}$ is the weighted source sentence:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$i$表示第$i$个时间步；$y_{i}$是输出标记，$s_{i}$是解码器隐藏状态，$c_{i}$是加权的源句子：
- en: '|  | $s_{i}=f(s_{i-1},y_{i-1},c_{i})$ |  | (26) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=f(s_{i-1},y_{i-1},c_{i})$ |  | (26) |'
- en: '|  | $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (27) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $c_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (27) |'
- en: 'Where $\alpha_{ij}$ is the normalized weight score:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{ij}$是归一化的权重分数：
- en: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (28)
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (28)
    |'
- en: '$e_{ij}$ is the similarity score between $s_{i-1}$ and $j^{th}$ encoder hidden
    state $h_{j}$, where the score is predicted by the similarity model $a$:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $e_{ij}$是$s_{i-1}$和第$j$个编码器隐藏状态$h_{j}$之间的相似性分数，该分数是由相似性模型$a$预测的：
- en: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (29) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (29) |'
- en: 'Figure [6](#S2.F6 "Figure 6 ‣ 2.5.1 Attention ‣ 2.5 Attention and Transformer
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") illustrates the attention model, where
    t and T denote time steps of decoder and encoder respectively.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6]（#S2.F6 "Figure 6 ‣ 2.5.1 Attention ‣ 2.5 Attention and Transformer ‣ 2
    Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")说明了注意力模型，其中t和T分别表示解码器和编码器的时间步。'
- en: Memory networks are similar to attention networks in the way they operate, except
    for the choice of the similarity model. In memory networks, the encoded memory
    can be viewed as the encoded source sentence in attention. However, the memory
    model proposed by Sukhbaatar et al. ([2015](#bib.bib349)) chose cosine distance
    as the similarity model while the attention proposed by Bahdanau et al. ([2014](#bib.bib12))
    used a feed-forward network which is trainable together with the whole sequence-to-sequence
    model.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 内存网络与注意力网络在操作方式上类似，唯一不同之处在于相似性模型的选择。在内存网络中，编码内存可以被看作是注意力中编码的源句子。然而，Sukhbaatar等人（2015）提出的内存模型选择了余弦距离作为相似性模型，而Bahdanau等人（2014）提出的注意力则使用了一个可训练的前馈网络，该网络与整个序列到序列模型一起进行训练。
- en: 2.5.2 Transformer
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2 Transformer
- en: Before transformers, most works combined attention with recurrent units, except
    for few works such as Parikh et al. ([2016](#bib.bib270)) and Gehring et al. ([2017](#bib.bib103)).
    Recurrent models condition each hidden state on the previous hidden state and
    the current input and are flexible in sequence length. However, due to their sequential
    nature, recurrent models cannot be trained in parallel, which severely undermines
    their potential. Vaswani et al. ([2017](#bib.bib377)) proposed Transformer, which
    entirely utilized attention mechanisms without any recurrent units and deployed
    more parallelization to speed up training. It applied self-attention and encoder-decoder
    attention to achieve local and global dependencies respectively.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器出现之前，大多数工作将注意力与递归单元结合，除了少数工作如Parikh等（[2016](#bib.bib270)）和Gehring等（[2017](#bib.bib103)）。递归模型将每个隐藏状态基于前一个隐藏状态和当前输入进行条件化，并且对序列长度具有灵活性。然而，由于其顺序性质，递归模型不能并行训练，这严重削弱了其潜力。Vaswani等（[2017](#bib.bib377)）提出了Transformer，该模型完全利用了注意力机制而没有任何递归单元，并且部署了更多的并行化以加快训练速度。它应用了自注意力和编码器-解码器注意力，分别实现局部和全局依赖。
- en: 'Figure [7](#S2.F7 "Figure 7 ‣ 2.5.2 Transformer ‣ 2.5 Attention and Transformer
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") represents the transformer. The following
    details its key mechanisms.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](#S2.F7 "Figure 7 ‣ 2.5.2 Transformer ‣ 2.5 Attention and Transformer ‣
    2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") 表示了变换器。以下详细介绍其关键机制。'
- en: '![Refer to caption](img/16fcebf8315258eecbe42ceccafd7253.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/16fcebf8315258eecbe42ceccafd7253.png)'
- en: 'Figure 7: The transformer model (Vaswani et al., [2017](#bib.bib377))'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：变换器模型（Vaswani等，[2017](#bib.bib377)）
- en: Encoder-decoder
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 编码器-解码器
- en: 'The Transformer consists of an encoder and a decoder. The encoder maps the
    input sequence $(x_{1},\ldots,x_{n})$ into continuous hidden states $(z_{1},\ldots,z_{n})$.
    The decoder further generates the output sequence $(y_{1},\ldots,y_{n})$ based
    on the hidden states of the encoder. The probability model of the Transformer
    is in the same form as that of the vanilla sequence-to-sequence model introduced
    in Section [2.2.5](#S2.SS2.SSS5 "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder
    Models) ‣ 2.2 Recurrent Neural Networks and Vanilla Sequence-to-sequence Models
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey"). Vaswani et al. ([2017](#bib.bib377))
    stacked 6 identical encoder layers and 6 identical decoder layers. An encoder
    layer consists of a multi-head attention component and a simple feed-forward network,
    both of which apply residual structure. The structure of a decoder layer is almost
    the same as that of an encoder layer, except for an additional encoder-decoder
    attention layer, which computes the attention between decoder hidden states of
    the current time step and the encoder output vectors. The input of the decoder
    is partially masked to make sure that each prediction is based on the previous
    tokens, avoiding predicting with the presence of future information. Both inputs
    of encoder and decoder use a positional encoding mechanism.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 'Transformer包括一个编码器和一个解码器。编码器将输入序列$(x_{1},\ldots,x_{n})$映射到连续的隐藏状态$(z_{1},\ldots,z_{n})$。解码器进一步基于编码器的隐藏状态生成输出序列$(y_{1},\ldots,y_{n})$。Transformer的概率模型与第[2.2.5](#S2.SS2.SSS5
    "2.2.5 Vanilla Sequence-to-sequence Models (Encoder-decoder Models) ‣ 2.2 Recurrent
    Neural Networks and Vanilla Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey")节中介绍的普通序列到序列模型形式相同。Vaswani等（[2017](#bib.bib377)）堆叠了6个相同的编码器层和6个相同的解码器层。一个编码器层包括一个多头注意力组件和一个简单的前馈网络，两者都应用了残差结构。解码器层的结构几乎与编码器层相同，只是多了一个额外的编码器-解码器注意力层，该层计算当前时间步的解码器隐藏状态与编码器输出向量之间的注意力。解码器的输入被部分遮蔽，以确保每个预测基于之前的标记，避免在存在未来信息的情况下进行预测。编码器和解码器的输入都使用位置编码机制。'
- en: Self-attention
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自注意力
- en: 'For an input sentence $x=(x_{1},\ldots,x_{n})$, each token $x_{i}$ corresponds
    to three vectors: query, key, and value. The self-attention computes the attention
    weight for every token $x_{i}$ against all other tokens in $x$ by multiplying
    the query of $x_{i}$ with the keys of all the remaining tokens one-by-one. For
    parallel computing, the query, key ,and value vectors of all tokens are combined
    into three matrices: Query (Q), Key (K) ,and Value (V). The self-attention of
    an input sentence $x$ is computed by the following formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入句子 $x=(x_{1},\ldots,x_{n})$，每个 token $x_{i}$ 对应三个向量：查询、键和值。自注意力机制通过将 $x_{i}$
    的查询与 $x$ 中所有其他 token 的键逐一相乘，计算每个 token $x_{i}$ 对所有其他 token 的注意力权重。为了并行计算，将所有 token
    的查询、键和值向量合并成三个矩阵：查询（Q）、键（K）和值（V）。输入句子 $x$ 的自注意力由以下公式计算：
- en: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  | (30) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | $Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V$ |  | (30) |'
- en: Where $d_{k}$ is the dimension of queries or keys.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{k}$ 是查询或键的维度。
- en: Multi-head attention
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多头注意力
- en: 'To jointly consider the information from different subspaces of embedding,
    query, key, and value vectors are mapped into $h$ vectors of identical shapes
    by using different linear transformations, where $h$ denotes the number of heads.
    Attention is computed on each of these vectors in parallel, and the results are
    concatenated and further projected. The multi-head attention can be described
    as:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了共同考虑嵌入的不同子空间的信息，查询、键和值向量通过不同的线性变换映射到 $h$ 个相同形状的向量，其中 $h$ 表示头的数量。对这些向量中的每一个并行计算注意力，然后将结果拼接起来并进一步投影。多头注意力可以描述为：
- en: '|  | $MultiHead(Q,K,V)=Concat(head_{1},...,head_{h})W^{O}$ |  | (31) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $MultiHead(Q,K,V)=Concat(head_{1},...,head_{h})W^{O}$ |  | (31) |'
- en: Where $head_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$ and $W$ denotes
    the linear transformations.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $head_{i}=Attention(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V})$，$W$ 表示线性变换。
- en: Positional encoding
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 位置编码
- en: 'The proposed transformer architecture has no recurrent units, which means that
    the order information of sequence is dismissed. The positional encoding is added
    with input embeddings to provide positional information. The paper chooses cosine
    functions for positional encoding:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的 Transformer 架构没有循环单元，这意味着序列的顺序信息被省略。位置编码与输入嵌入一起添加以提供位置相关信息。本文选择了余弦函数用于位置编码：
- en: '|  | $PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$ |  | (32) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})$ |  | (32) |'
- en: '|  | $PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$ |  | (33) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | $PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$ |  | (33) |'
- en: Where $pos$ denotes the position of the target token and $i$ denotes the dimension,
    which means that each dimension of the positional matrix uses a different wavelength
    for encoding.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $pos$ 表示目标 token 的位置，$i$ 表示维度，这意味着位置矩阵的每个维度使用不同的波长进行编码。
- en: Transformer-based pretrain models and Transformer variants
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于 Transformer 的预训练模型和 Transformer 变体
- en: Recently, many transformer-based pretrain models have been developed. Unlike
    Embeddings from Language Model (ELMo) proposed by Peters et al. ([2018](#bib.bib273)),
    which is an LSTM-based contextual embedding model, transformer-based pretrain
    models are more powerful. Two most popular models are GPT-2 ⁶⁶6https://openai.com/blog/better-language-models/
    and BERT (Devlin et al., [2018](#bib.bib70)). GPT-2 and BERT both consist of 12
    transformer blocks and BERT is further improved by making the training bi-directional.
    They are powerful due to their capability of adapting to new tasks after pretraining.
    This property helped achieve significant improvements in many NLP tasks. There
    also evolve many Transformer variants (Zaheer et al., [2020](#bib.bib442); Dai
    et al., [2019](#bib.bib62); Guo et al., [2019](#bib.bib115)), which are designed
    to reduce the model parameters/computational complexity, or improve performance
    of the original Transformer in diverse scenarios. Lin et al. ([2021](#bib.bib212))
    and Tay et al. ([2020](#bib.bib366)) systematically summarize the state-of-the-art
    Transformer variants for academics that are interested.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多基于Transformer的预训练模型被开发出来。与Peters等人（[2018](#bib.bib273)）提出的基于LSTM的上下文嵌入模型ELMo不同，基于Transformer的预训练模型更强大。最受欢迎的两个模型是GPT-2
    [6](https://openai.com/blog/better-language-models/)和BERT（Devlin等人，[2018](#bib.bib70)）。GPT-2和BERT都由12个Transformer块组成，而BERT通过使训练双向来进一步改进。由于它们能够在预训练后适应新任务，这些模型非常强大。这一特性在许多NLP任务中取得了显著的改进。此外，还出现了许多Transformer变体（Zaheer等人，[2020](#bib.bib442)；Dai等人，[2019](#bib.bib62)；Guo等人，[2019](#bib.bib115)），这些变体旨在减少模型参数/计算复杂性，或在不同场景中提升原始Transformer的性能。Lin等人（[2021](#bib.bib212)）和Tay等人（[2020](#bib.bib366)）系统总结了最新的Transformer变体，供学术界有兴趣的研究者参考。
- en: Attention for dialogue systems
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话系统中的注意力机制
- en: Attention is a mechanism to catch the importance of different parts in the target
    sequence. Zhu et al. ([2018](#bib.bib466)) applied a two-level attention to generate
    words. Given the user message and candidate responses selected by a retrieval
    system, the generator first computes word-level attention weights, then uses sentence-level
    attention to rescale the weights. This two-level attention helps the generator
    catch different importance given the encoded context. Liu et al. ([2019](#bib.bib222))
    used an attention-based recurrent architecture to generate responses. They designed
    a multi-level encoder-decoder of which the multi-level encoder tries to map raw
    words, low-level clusters, and high-level clusters into hierarchical embedded
    representations while the multi-level decoder leveraged the hierarchical representations
    using attention and then generated responses. At each decoding stage, the model
    calculated two attention weights for the output of the higher-level decoder and
    the hidden state of the current level’s encoder. Chen et al. ([2019b](#bib.bib44))
    computed multi-head self-attention for the outputs of a dialogue act predictor.
    Unlike the transformer, which concatenates the outputs of different heads, they
    passed the outputs directly to the next multi-head layer. The stacked multi-head
    layers then generated the responses with dialogue acts as the input.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制用于捕捉目标序列中不同部分的重要性。Zhu等人（[2018](#bib.bib466)）应用了两级注意力机制来生成词语。在用户消息和由检索系统选择的候选响应的情况下，生成器首先计算词级注意力权重，然后使用句子级注意力来重新调整权重。这种两级注意力机制帮助生成器根据编码的上下文捕捉不同的重要性。Liu等人（[2019](#bib.bib222)）使用基于注意力的递归架构来生成响应。他们设计了一个多级编码器-解码器，其中多级编码器尝试将原始词汇、低级簇和高级簇映射到层次化嵌入表示，而多级解码器则利用注意力机制来利用这些层次化表示，然后生成响应。在每个解码阶段，该模型为高层解码器的输出和当前层编码器的隐藏状态计算两个注意力权重。Chen等人（[2019b](#bib.bib44)）计算了对话行为预测器输出的多头自注意力。不同于Transformer将不同头的输出连接起来，他们直接将输出传递到下一个多头层。然后，堆叠的多头层使用对话行为作为输入生成响应。
- en: Transformers for dialogue systems
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话系统中的Transformer
- en: Transformers are powerful sequence-to-sequence models and meanwhile, their encoders
    also serve as good dialogue representation models. Henderson et al. ([2019b](#bib.bib136))
    built a transformer-based response retrieval model for task-oriented dialogue
    systems. A two-channel transformer encoder was designed for encoding user messages
    and responses, both of which were initially presented as unigrams and bigrams.
    A simple cosine distance was then applied to calculate the semantic similarity
    between the user message and the candidate response. Li et al. ([2019d](#bib.bib208))
    built multiple incremental transformer encoders to encode multi-turn conversations
    and their related document knowledge. The encoded utterance and related document
    of the previous turn were treated as a part of the input of the next turn’s transformer
    encoder. The pretrained model was adaptable to multiple domains with only a small
    amount of data from the target domain. Bao et al. ([2019b](#bib.bib17)) used stacked
    transformers for dialogue generation pretraining. Besides the response generation
    task, they also pretrained the model together with a latent act prediction task.
    A latent variable was applied to solve the “one-to-many" problem in response generation.
    The multi-task training scheme improved the performance of the proposed transformer
    pretraining model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 是强大的序列到序列模型，同时，它们的编码器也能作为良好的对话表示模型。Henderson 等人 ([2019b](#bib.bib136))
    构建了一个基于 Transformer 的响应检索模型，用于任务导向的对话系统。设计了一个双通道 Transformer 编码器，用于编码用户消息和响应，这些消息和响应最初以
    unigram 和 bigram 的形式呈现。随后，应用了简单的余弦距离来计算用户消息和候选响应之间的语义相似性。Li 等人 ([2019d](#bib.bib208))
    构建了多个增量 Transformer 编码器来编码多轮对话及其相关文档知识。将上一轮的编码发言和相关文档视为下一轮 Transformer 编码器的输入的一部分。预训练模型能够适应多个领域，仅需目标领域的一小部分数据。Bao
    等人 ([2019b](#bib.bib17)) 使用堆叠的 Transformer 进行对话生成预训练。除了响应生成任务外，他们还将模型与潜在行为预测任务一同预训练。应用潜在变量来解决响应生成中的“一对多”问题。多任务训练方案提高了所提出的
    Transformer 预训练模型的性能。
- en: Transformer-based pretrain models for dialogue systems
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于 Transformer 的对话系统预训练模型
- en: Large transformer-based pretrain models are adaptable to many tasks and are
    thus popular in recent works. Golovanov et al. ([2019](#bib.bib107)) used GPT
    as a sequence-to-sequence model to directly generate utterances and compared the
    performances under single- and multi-input settings. Majumder et al. ([2020b](#bib.bib239))
    first used a probability model to retrieve related news corpus and then combined
    the news corpus and dialogue context as input of a GPT-2 generator for response
    generation. They proposed that by using discourse pattern recognition and interrogative
    type prediction as two subtasks for multi-task learning, the dialogue modeling
    could be further improved. Wu et al. ([2019c](#bib.bib417)) used BERT as an encoder
    of context and candidate responses in their goal-based response retrieval system
    while Zhong et al. ([2020](#bib.bib460)) built Co-BERT, a BERT-based response
    selection model, to retrieve empathetic responses given persona-based training
    corpus. Zhao et al. ([2020b](#bib.bib459)) built a knowledge-grounded dialogue
    system in a synthesized fashion. They used both BERT and GPT-2 to perform knowledge
    selection and response generation jointly, where BERT was for knowledge selection
    and GPT-2 generated responses based on dialogue context and the selected knowledge.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 大型基于 Transformer 的预训练模型适用于许多任务，因此在近期工作中非常受欢迎。Golovanov 等人 ([2019](#bib.bib107))
    使用 GPT 作为序列到序列模型直接生成发言，并比较了单输入和多输入设置下的性能。Majumder 等人 ([2020b](#bib.bib239)) 首先使用概率模型检索相关新闻语料库，然后将新闻语料库和对话上下文结合作为
    GPT-2 生成器的输入以生成响应。他们提出，通过使用话语模式识别和疑问类型预测作为多任务学习的两个子任务，对话建模可以进一步改进。Wu 等人 ([2019c](#bib.bib417))
    使用 BERT 作为其目标导向响应检索系统中的上下文和候选响应的编码器，而 Zhong 等人 ([2020](#bib.bib460)) 构建了 Co-BERT，一种基于
    BERT 的响应选择模型，用于检索具有同理心的响应，前提是基于人格训练语料库。Zhao 等人 ([2020b](#bib.bib459)) 以合成的方式构建了一个知识驱动的对话系统。他们使用
    BERT 和 GPT-2 联合进行知识选择和响应生成，其中 BERT 用于知识选择，GPT-2 基于对话上下文和选择的知识生成响应。
- en: 2.6 Pointer Net and CopyNet
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 Pointer Net 和 CopyNet
- en: 2.6.1 Pointer Net
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1 Pointer Net
- en: 'In some NLP tasks like dialogue systems and question-answering, the agents
    sometimes need to directly quote from the user message. Pointer Net (Oriol et al.,
    [2015](#bib.bib265)) (Figure [8](#S2.F8 "Figure 8 ‣ 2.6.1 Pointer Net ‣ 2.6 Pointer
    Net and CopyNet ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep
    Learning Based Dialogue Systems: A Systematic Survey")) solved the problem of
    directly copying tokens from the input sentence.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '在一些自然语言处理任务中，如对话系统和问答系统，代理有时需要直接引用用户消息中的内容。Pointer Net（Oriol 等，[2015](#bib.bib265)）（图
    [8](#S2.F8 "Figure 8 ‣ 2.6.1 Pointer Net ‣ 2.6 Pointer Net and CopyNet ‣ 2 Neural
    Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey")）解决了直接从输入句子中复制标记的问题。'
- en: '![Refer to caption](img/6879a5e0268e8d55746d25c3e89cb18a.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6879a5e0268e8d55746d25c3e89cb18a.png)'
- en: (a) Sequence-to-sequence
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 序列到序列
- en: '![Refer to caption](img/f785125339c6d05f709a9fc9001cd772.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f785125339c6d05f709a9fc9001cd772.png)'
- en: (b) Pointer Net
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Pointer Net
- en: 'Figure 8: (a) Sequence-to-sequence - The RNN (blue) processes the input sequence
    to produce a code vector, which is then used by the probability chain rule and
    another RNN to generate the output sequence (purple). The dimensionality of the
    problem determines the output dimensionality, which remains constant through training
    and inference. (b) Pointer Net - The input sequence is converted to a code (blue)
    by an encoding RNN, which is fed to the generating network (purple). The generating
    network generates a vector at each step that modulates a content-based attention
    process across inputs. The attention mechanism produces a softmax distribution
    with a dictionary size equal to the input length. (Oriol et al., [2015](#bib.bib265))'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8： (a) 序列到序列 - RNN（蓝色）处理输入序列以生成一个编码向量，然后通过概率链规则和另一个 RNN 生成输出序列（紫色）。问题的维度决定了输出的维度，该维度在训练和推理过程中保持不变。
    (b) Pointer Net - 输入序列通过编码 RNN 转换为编码（蓝色），然后输入生成网络（紫色）。生成网络在每一步生成一个向量，调节基于内容的注意力过程。注意力机制产生一个
    softmax 分布，字典大小等于输入长度。（Oriol 等，[2015](#bib.bib265)）
- en: 'Traditional sequence-to-sequence models (Sutskever et al., [2014](#bib.bib352);
    Graves et al., [2014](#bib.bib111)) with an encoder-decoder structure map a source
    sentence to a target sentence. Generally, these models first map source sentence
    into hidden state vectors with an encoder, and then predict the output sequence
    based on the hidden states. The sequence prediction is accomplished step-by-step,
    each step predicting one token using greedy search or beam search. The overall
    sequence-to-sequence model can be described by the following probability model:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的序列到序列模型（Sutskever 等，[2014](#bib.bib352)；Graves 等，[2014](#bib.bib111)）采用编码器-解码器结构将源句子映射到目标句子。通常，这些模型首先使用编码器将源句子映射到隐藏状态向量，然后根据隐藏状态预测输出序列。序列预测是逐步完成的，每一步使用贪婪搜索或束搜索预测一个标记。总体的序列到序列模型可以用以下概率模型描述：
- en: '|  | $P(C^{P}&#124;P;\theta)=\prod_{i=1}^{m(P)}p(C_{i}&#124;C_{1},...,C_{i-1},P;\theta)$
    |  | (34) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(C^{P}&#124;P;\theta)=\prod_{i=1}^{m(P)}p(C_{i}&#124;C_{1},...,C_{i-1},P;\theta)$
    |  | (34) |'
- en: Where $(P,C_{p})$ constitutes a training pair, $P$ = $\{P_{1},...,P_{n}\}$ denotes
    the input sequence and $C_{p}$ = $\{C_{1},...,C_{m(p)}\}$ denotes the ground target
    sequence. $\theta$ is a decoder model.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(P,C_{p})$ 构成一个训练对，$P$ = $\{P_{1},...,P_{n}\}$ 表示输入序列，而 $C_{p}$ = $\{C_{1},...,C_{m(p)}\}$
    表示目标序列。$\theta$ 是一个解码模型。
- en: 'The sequence-to-sequence models have the vanilla backbones and attention-based
    backbones. Vanilla models predict the target sequence based only on the last hidden
    state of the encoder and pass it across different decoder time steps. Such a mechanism
    restricts the information received by the decoder at each decoding stage. Attention-based
    models consider all hidden states of the encoder at each decoding step and calculate
    their importance when utilizing them. To compare the mechanism of Pointer Net
    and Attention, we present the equations explained in Section [2.2](#S2.SS2 "2.2
    Recurrent Neural Networks and Vanilla Sequence-to-sequence Models ‣ 2 Neural Models
    in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey") here again. The decoder predicts the token conditioned partially
    on the weighted sum of encoder hidden states $d_{i}$:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '序列到序列模型有普通骨干和基于注意力的骨干。普通模型仅基于编码器的最后一个隐藏状态预测目标序列，并在不同的解码时间步中传递。这样的机制限制了在每个解码阶段解码器接收的信息。基于注意力的模型在每个解码步骤中考虑编码器的所有隐藏状态，并在利用它们时计算它们的重要性。为了比较Pointer
    Net和Attention的机制，我们再次呈现了在第[2.2](#S2.SS2 "2.2 Recurrent Neural Networks and Vanilla
    Sequence-to-sequence Models ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey")节中解释的方程。解码器基于编码器隐藏状态$d_{i}$的加权和部分预测标记：'
- en: '|  | $d_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (35) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{i}=\sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}$ |  | (35) |'
- en: 'Where $\alpha_{ij}$ is the normalized weight score:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{ij}$是归一化权重分数：
- en: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (36)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$ |  | (36)
    |'
- en: '$e_{ij}$ is the similarity score between $s_{i-1}$ and $jth$ encoder hidden
    state $h_{j}$, where the score is predicted by the similarity model $a$:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: $e_{ij}$是$s_{i-1}$和第$j$个编码器隐藏状态$h_{j}$之间的相似度分数，该分数由相似度模型$a$预测：
- en: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (37) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{ij}=a(s_{i-1},h_{j})$ |  | (37) |'
- en: 'At each decoding step, both vanilla and attention-based sequence-to-sequence
    models predict a distribution over a fixed dictionary $X=\{x_{1},...,x_{n}\}$,
    where $x_{i}$ denotes the tokens and $n$ denotes the total count of different
    tokens in the training corpus. However, when copying words from the input sentence,
    we do not need such a large dictionary. Instead, $n$ equals to the number of tokens
    in the input sequence (including repeated ones) and is not fixed since it changes
    according to the length of the input sequence. Pointer Net made a simple change
    to the attention-based sequence-to-sequence models: instead of predicting the
    token distribution based on the weighted sum of encoder hidden states $d_{i}$,
    it directly used the normalized weights $\alpha_{i}$ as predicted distribution:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个解码步骤中，普通和基于注意力的序列到序列模型都会预测一个固定词典$X=\{x_{1},...,x_{n}\}$上的分布，其中$x_{i}$表示标记，$n$表示训练语料库中不同标记的总数。然而，在从输入句子中复制单词时，我们不需要如此大的词典。相反，$n$等于输入序列中的标记数量（包括重复的标记），并且不是固定的，因为它会根据输入序列的长度而变化。Pointer
    Net对基于注意力的序列到序列模型进行了一个简单的修改：它直接使用归一化权重$\alpha_{i}$作为预测分布，而不是基于编码器隐藏状态$d_{i}$的加权和来预测标记分布：
- en: '|  | $P(C_{i}&#124;C_{1},...,C_{i-1},P)=\alpha_{i}$ |  | (38) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(C_{i}\mid C_{1},...,C_{i-1},P)=\alpha_{i}$ |  | (38) |'
- en: Where $\alpha_{i}$ is a set of probability numbers $\{\alpha_{i}^{1},...,\alpha_{i}^{j}\}$
    which represents the probability distribution over the tokens of the input sequence.
    Obviously, the token prediction problem is now transformed into position prediction
    problem, where the model only needs to predict a position in the input sequence.
    This mechanism is like a pointer that points to its target, hence the name “Pointer
    Net".
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{i}$是一组概率数$\{\alpha_{i}^{1},...,\alpha_{i}^{j}\}$，它表示对输入序列中标记的概率分布。显然，标记预测问题现在转变为位置预测问题，模型只需要预测输入序列中的一个位置。这个机制就像一个指针指向它的目标，因此得名“Pointer
    Net”。
- en: 2.6.2 CopyNet
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.2 CopyNet
- en: '![Refer to caption](img/1264b992dafbcc189323d6c259383137.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1264b992dafbcc189323d6c259383137.png)'
- en: 'Figure 9: The overall architecture of CopyNet (Gu et al., [2016](#bib.bib114))'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：CopyNet的整体架构（Gu等，[2016](#bib.bib114)）
- en: 'In real-world applications, simply copying from the source message is not enough.
    Instead, in tasks like dialogue systems and QA, agents also require the ability
    to generate words that are not in the source sentence. CopyNet (Gu et al., [2016](#bib.bib114))
    (Figure [9](#S2.F9 "Figure 9 ‣ 2.6.2 CopyNet ‣ 2.6 Pointer Net and CopyNet ‣ 2
    Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")) was proposed to incorporate the copy mechanism
    into traditional sequence-to-sequence models. The model decides at each decoding
    stage whether to copy from the source or generate a new token not in the source.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际应用中，仅仅从源消息中复制是不够的。在对话系统和问答系统等任务中，代理还需要生成源句子中没有的单词的能力。CopyNet (Gu et al.,
    [2016](#bib.bib114)) (图 [9](#S2.F9 "Figure 9 ‣ 2.6.2 CopyNet ‣ 2.6 Pointer Net
    and CopyNet ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey")) 被提出以将复制机制融入传统的序列到序列模型。该模型在每个解码阶段决定是从源中复制还是生成一个源中不存在的新令牌。'
- en: 'The encoder of CopyNet is the same as that of a traditional sequence-to-sequence
    model, whereas the decoder has some differences compared with a traditional attention-based
    decoder. When predicting the token at time step $t$, it combines the probabilistic
    models of generate-mode and copy-mode:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: CopyNet 的编码器与传统的序列到序列模型相同，而解码器与传统的基于注意力的解码器相比有一些区别。在预测时间步 $t$ 的令牌时，它结合了生成模式和复制模式的概率模型：
- en: '|  | $P(y_{t}&#124;s_{t},y_{t-1},c_{t},M)=P_{g}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)+P_{c}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)$
    |  | (39) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(y_{t}&#124;s_{t},y_{t-1},c_{t},M)=P_{g}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)+P_{c}(y_{t}&#124;s_{t},y_{t-1},c_{t},M)$
    |  | (39) |'
- en: Where $t$ is the time step. $s_{t}$ is the decoder hidden state and $y_{t}$
    is the predicted token. $c_{t}$ and $M$ represent weighted sum of encoder hidden
    states and encoder hidden states respectively. $g$ and $c$ are generate-mode and
    copy-mode respectively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $t$ 是时间步。$s_{t}$ 是解码器隐藏状态，$y_{t}$ 是预测的令牌。$c_{t}$ 和 $M$ 分别表示编码器隐藏状态的加权和和编码器隐藏状态。$g$
    和 $c$ 分别表示生成模式和复制模式。
- en: Besides, though it still uses $y_{t-1}$ and weighted attention vector $c_{t}$
    to update the decoder hidden state, $y_{t-1}$ is uniquely encoded with both its
    embedding and its location-specific hidden state; also, CopyNet combines attentive
    read and selective read to capture information from the encoder hidden states,
    where the selective read is the same method used in Pointer Net. Different from
    the Neural Turing Machines (Graves et al., [2014](#bib.bib111); Kurach et al.,
    [2015](#bib.bib176)), the CopyNet has a location-based mechanism that enables
    the model to be aware of some specific details in training data in a more subtle
    way.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，尽管它仍然使用 $y_{t-1}$ 和加权注意力向量 $c_{t}$ 来更新解码器隐藏状态，但 $y_{t-1}$ 是通过其嵌入和位置特定隐藏状态进行唯一编码的；同时，CopyNet
    结合了注意力读取和选择性读取来捕获来自编码器隐藏状态的信息，其中选择性读取是 Pointer Net 中使用的相同方法。与 Neural Turing Machines (Graves
    et al., [2014](#bib.bib111); Kurach et al., [2015](#bib.bib176)) 不同，CopyNet 具有基于位置的机制，使模型能够以更微妙的方式注意到训练数据中的一些特定细节。
- en: Copy mechanism is suitable for dialogues involving terminologies or external
    knowledge sources, and it is popular in knowledge-grounded or task-oriented dialogue
    systems.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 复制机制适用于涉及术语或外部知识源的对话，并且在知识驱动或任务导向的对话系统中很受欢迎。
- en: Copy mechanism for knowledge-grounded dialogue systems
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识驱动对话系统的复制机制
- en: 'For knowledge-grounded systems, external documents or dialogues are sources
    to copy from. Lin et al. ([2020a](#bib.bib214)) combined a recurrent knowledge
    interactive decoder with a knowledge-aware pointer network to achieve both knowledge-grounded
    generation and knowledge copy. In the proposed model, they first calculated the
    attention distribution over external knowledge, then used two pointers referring
    to dialogue context and knowledge source respectively to copy out-of-vocabulary
    (OOV) words. Wu et al. ([2020b](#bib.bib416)) applied a multi-class classifier
    to flexibly fuse three distributions: generated words, generated knowledge entities,
    and copied query words. They used Context-Knowledge Fusion and Flexible Mode Fusion
    to perform the knowledge retrieval, response generation, and copying jointly,
    making the generated responses precise, coherent, and knowledge-infused. Ji et al.
    ([2020](#bib.bib155)) proposed a Cross Copy Network to copy from internal utterance
    (dialogue history) and external utterance (similar cases) respectively. They first
    used pretrained language models for similar case retrieval, then combined the
    probability distribution of two pointers to make a prediction. They only experimented
    with court debate and customer service content generation tasks, where similar
    cases were easy to obtain.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于知识驱动的系统，外部文档或对话是复制的来源。Lin 等人（[2020a](#bib.bib214)）结合了递归知识交互解码器和知识感知指针网络，以实现知识驱动生成和知识复制。在提出的模型中，他们首先计算了对外部知识的注意力分布，然后使用两个指针分别指向对话上下文和知识源，以复制词汇表外（OOV）单词。Wu
    等人（[2020b](#bib.bib416)）应用了一个多类分类器，以灵活地融合三种分布：生成的单词、生成的知识实体和复制的查询单词。他们使用了上下文-知识融合和灵活模式融合来共同执行知识检索、响应生成和复制，使生成的响应准确、一致且富有知识性。Ji
    等人（[2020](#bib.bib155)）提出了一种交叉复制网络，分别从内部发言（对话历史）和外部发言（类似案例）中进行复制。他们首先使用预训练语言模型进行类似案例检索，然后结合两个指针的概率分布来进行预测。他们只在法庭辩论和客户服务内容生成任务中进行了实验，其中类似案例容易获得。
- en: Copy mechanism for task-oriented dialogue systems
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 针对任务导向对话系统的复制机制
- en: Many dialogue state tracking tasks generate slots and slot values using a copy
    component (Wu et al., [2019a](#bib.bib413); Ouyang et al., [2020](#bib.bib266);
    Gangadharaiah and Narayanaswamy, [2020](#bib.bib97); Chen et al., [2020a](#bib.bib41);
    Zhang et al., [2020](#bib.bib452); Li et al., [2020d](#bib.bib207)). Among them Wu
    et al. ([2019a](#bib.bib413)), Ouyang et al. ([2020](#bib.bib266)) and Chen et al.
    ([2020a](#bib.bib41)) solved the problem of multi-domain dialogue state tracking.
    Wu et al. ([2019a](#bib.bib413)) proposed TRAnsferable Dialogue statE generator
    (TRADE), a copy-based dialogue state generator. The generator decoded the slot
    value multiple times for each possible (domain, slot) pair, then a slot gate was
    applied to decide which pair belonged to the dialogue. The output distribution
    was a copy of the slot values belonging to the selected (domain, slot) pairs from
    vocabulary and dialogue history. Chen et al. ([2020a](#bib.bib41)) used a different
    copy strategy from TRADE. Instead of using the whole dialogue history as the copy
    source, they copied state values from user utterances and system messages respectively,
    which took the slot-level context as input. Ouyang et al. ([2020](#bib.bib266))
    proposed slot connection mechanism to efficiently utilize existing states from
    other domains. Attention weights were calculated to measure the connection between
    the target slot and related slot-value tuples in other domains. Three distributions
    over token generation, dialogue context copying, and past state copying were finally
    gated and fused to predict the next token. Gangadharaiah and Narayanaswamy ([2020](#bib.bib97))
    combined a pointer network with a template-based tree decoder to fill the templates
    recursively and hierarchically. Copy mechanisms also alleviated the problem of
    expensive data annotation in end-to-end task-oriented dialogue systems. Copy-augmented
    dialogue generation models were proven to perform significantly better than strong
    baselines with limited domain-specific or multi-domain data (Zhang et al., [2020](#bib.bib452);
    Li et al., [2020d](#bib.bib207); Gao et al., [2020a](#bib.bib99)).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 许多对话状态追踪任务使用复制组件生成槽和槽值（Wu等，[2019a](#bib.bib413)；Ouyang等，[2020](#bib.bib266)；Gangadharaiah和Narayanaswamy，[2020](#bib.bib97)；Chen等，[2020a](#bib.bib41)；Zhang等，[2020](#bib.bib452)；Li等，[2020d](#bib.bib207)）。其中Wu等（[2019a](#bib.bib413)），Ouyang等（[2020](#bib.bib266)）和Chen等（[2020a](#bib.bib41)）解决了多领域对话状态追踪的问题。Wu等（[2019a](#bib.bib413)）提出了可转移对话状态生成器（TRADE），一种基于复制的对话状态生成器。生成器对每个可能的（领域，槽）对进行多次解码，然后应用槽门来决定哪个对属于对话。输出分布是从词汇表和对话历史中选择的（领域，槽）对的槽值的复制。Chen等（[2020a](#bib.bib41)）使用了与TRADE不同的复制策略。他们没有使用整个对话历史作为复制源，而是分别从用户话语和系统消息中复制状态值，这将槽级上下文作为输入。Ouyang等（[2020](#bib.bib266)）提出了槽连接机制，以有效利用来自其他领域的现有状态。计算了注意力权重以衡量目标槽与其他领域中相关槽值元组之间的连接。最终，对代币生成、对话上下文复制和过去状态复制的三个分布进行了门控和融合，以预测下一个代币。Gangadharaiah和Narayanaswamy（[2020](#bib.bib97)）将指针网络与基于模板的树解码器相结合，递归和分层地填充模板。复制机制还缓解了端到端任务导向对话系统中数据注释昂贵的问题。经过复制增强的对话生成模型被证明在有限的领域特定或多领域数据下表现显著优于强基线（Zhang等，[2020](#bib.bib452)；Li等，[2020d](#bib.bib207)；Gao等，[2020a](#bib.bib99)）。
- en: Copy mechanism for dialogue-related tasks
  id: totrans-201
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话相关任务的复制机制
- en: Pointer networks and CopyNet are also used to solve other dialogue-related tasks.
    Yu and Joty ([2020](#bib.bib441)) applied a pointer net for online conversation
    disentanglement. The pointer module pointed to the ancestor message to which the
    current message replies and a classifier predicted whether two messages belonged
    to the same thread. In dialogue parsing tasks, the pointer net is used as the
    backbone parsing model to construct discourse trees (Aghajanyan et al., [2020](#bib.bib2);
    Lin et al., [2019](#bib.bib213)). Tay et al. ([2019](#bib.bib365)) used a pointer-generator
    framework to perform machine reading comprehension over a long span, where the
    copy mechanism reduced the demand of including target answers in context.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 指针网络和CopyNet也用于解决其他对话相关任务。Yu和Joty（[2020](#bib.bib441)）应用了指针网络进行在线对话分离。指针模块指向当前消息回复的祖先消息，分类器预测两条消息是否属于同一线程。在对话解析任务中，指针网络作为主干解析模型用于构建话语树（Aghajanyan等，[2020](#bib.bib2)；Lin等，[2019](#bib.bib213)）。Tay等（[2019](#bib.bib365)）使用了指针-生成器框架在长跨度上进行机器阅读理解，其中复制机制减少了在上下文中包含目标答案的需求。
- en: 2.7 Deep Reinforcement Learning Models and Generative Adversarial Networks
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 深度强化学习模型与生成对抗网络
- en: In recent years, two exciting approaches exhibit the potential of artificial
    intelligence. The first one is deep reinforcement learning, which outperforms
    humans in many complex problems such as large-scale games, conversations, and
    car-driving. Another technique is GAN, showing amazing capability in generation
    tasks. The data samples generated by GAN models like articles, paintings, and
    even videos, are sometimes indistinguishable from human creations.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，有两种令人兴奋的方法展示了人工智能的潜力。第一种是深度强化学习，它在许多复杂问题上超越了人类，如大规模游戏、对话和汽车驾驶。另一种技术是 GAN，展现了在生成任务中的惊人能力。由
    GAN 模型生成的数据样本，如文章、画作甚至视频，有时与人类创作的难以区分。
- en: AlphaGo (Silver et al., [2016](#bib.bib327)) stimulated the research interests
    again in reinforcement learning in recent years (Graves et al., [2016](#bib.bib112);
    Mnih et al., [2016](#bib.bib253); Wang et al., [2016](#bib.bib394); Tamar et al.,
    [2016](#bib.bib358); Jaderberg et al., [2016](#bib.bib152); Mirowski et al., [2016](#bib.bib251)).
    Reinforcement learning is a branch of machine learning aiming to train agents
    to perform appropriate actions while interacting with a certain environment. It
    is one of the three fundamental machine learning branches, with supervised learning
    and unsupervised learning being the other two. It can also be seen as an intermediate
    between supervised learning and unsupervised learning because it only needs weak
    signals for training (Wang et al., [2016](#bib.bib394)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaGo（Silver et al., [2016](#bib.bib327)）在近年来再次激发了对强化学习的研究兴趣（Graves et al.,
    [2016](#bib.bib112)；Mnih et al., [2016](#bib.bib253)；Wang et al., [2016](#bib.bib394)；Tamar
    et al., [2016](#bib.bib358)；Jaderberg et al., [2016](#bib.bib152)；Mirowski et
    al., [2016](#bib.bib251)）。强化学习是机器学习的一个分支，旨在训练代理在与某个环境互动时执行适当的动作。它是机器学习的三个基本分支之一，另两个是监督学习和无监督学习。它也可以被视为监督学习和无监督学习之间的中间体，因为它只需要弱信号进行训练（Wang
    et al., [2016](#bib.bib394)）。
- en: '![Refer to caption](img/db8bbcf7bcb7e80032ef140aea094e73.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/db8bbcf7bcb7e80032ef140aea094e73.png)'
- en: 'Figure 10: The reinforcement learning framework'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：强化学习框架
- en: 'Figure [10](#S2.F10 "Figure 10 ‣ 2.7 Deep Reinforcement Learning Models and
    Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey") illustrates
    the reinforcement learning framework, consisting of an agent and an environment.
    The framework is a Markov Decision Process (MDP) (Puterman, [2014](#bib.bib278)),
    which can be described by a five-tuple M = $\langle S,A,P,R,\gamma\rangle$. $S$
    denotes an infinite set of environment states; $A$ denotes a set of actions that
    agent chooses from conditioned on a given environment state $s$; $P$ is the transition
    probability matrix in MDP, denoting the probability of an environment state transfer
    after agent takes an action; $R$ is an average reward the agent receives from
    the environment after taking an action under state $s$; $\gamma$ is a discount
    factor. The flow of this framework is a loop of the following two steps: the agent
    first makes an observation on the current environment state $s_{t}$ and chooses
    an action based on its policy; then according to the transition probability matrix
    $P$, the environment’s state transfers to $s_{t+1}$, and simultaneously provides
    a reward $r_{t}$.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](#S2.F10 "图 10 ‣ 2.7 深度强化学习模型与生成对抗网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述")展示了强化学习框架，包括一个代理和一个环境。该框架是一个马尔可夫决策过程（MDP）（Puterman，[2014](#bib.bib278)），可以用一个五元组
    M = $\langle S,A,P,R,\gamma\rangle$ 来描述。$S$ 代表环境状态的无限集合；$A$ 代表代理根据给定环境状态 $s$ 选择的动作集合；$P$
    是 MDP 中的转移概率矩阵，表示代理采取动作后环境状态转移的概率；$R$ 是代理在状态 $s$ 下采取动作后从环境中获得的平均奖励；$\gamma$ 是折扣因子。该框架的流程是以下两个步骤的循环：代理首先对当前环境状态
    $s_{t}$ 进行观察，并根据其策略选择一个动作；然后根据转移概率矩阵 $P$，环境的状态转移到 $s_{t+1}$，同时提供奖励 $r_{t}$。
- en: Reinforcement learning is applicable to solve many challenges in dialogue systems
    because of the agent-environment nature of a dialogue system. A two-party dialogue
    system consists of an agent, which is an intelligent chatbot, and an environment,
    which is usually a user or a user simulator. Here we mainly discuss deep reinforcement
    learning.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习适用于解决对话系统中的许多挑战，因为对话系统具有代理-环境的特性。一个双方对话系统由一个智能聊天机器人（代理）和一个用户或用户模拟器（环境）组成。在这里，我们主要讨论深度强化学习。
- en: Deep reinforcement learning means applying deep neural networks to model the
    value function or policy of the reinforcement learning framework. “Deep model"
    is in contrast to the “shallow model". The shallow model normally refers to traditional
    machine learning models like Decision Trees or KNN. Feature engineering, which
    is usually based on shallow models, is time and labor consuming, and also over-specified
    and incomplete. Different from that, deep neural models are easy to design and
    have a strong fitting capability, which contributes to many breakthroughs in recent
    research. Deep representation learning gets rid of human labor and exploits hierarchical
    features in data automatically, which strengthens the semantic expressiveness
    and domain correlations significantly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习意味着将深度神经网络应用于建模强化学习框架中的价值函数或策略。“深度模型”与“浅层模型”相对。浅层模型通常指传统的机器学习模型，如决策树或
    KNN。特征工程，通常基于浅层模型，既耗时又耗力，同时也过于具体和不完整。与此不同的是，深度神经模型设计简便，并具有强大的拟合能力，这促进了近年来许多突破。深度表示学习摆脱了人工劳动，并自动利用数据中的层次特征，显著增强了语义表达性和领域相关性。
- en: 'We discuss two typical reinforcement models: Deep Q-Networks (Mnih et al.,
    [2015](#bib.bib252)) and REINFORCE (Williams, [1992](#bib.bib409); Sutton et al.,
    [1999](#bib.bib354)). They belong to Q-learning and policy gradient respectively,
    which are two families of reinforcement learning.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了两种典型的强化学习模型：Deep Q-Networks（Mnih et al., [2015](#bib.bib252)）和 REINFORCE（Williams,
    [1992](#bib.bib409); Sutton et al., [1999](#bib.bib354)）。它们分别属于 Q-learning 和策略梯度，这两种强化学习的家族。
- en: 2.7.1 Deep Q-Networks
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.1 深度 Q 网络
- en: 'A Deep Q-Network is a value-based RL model. It determines the best policy according
    to the Q-function:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 深度 Q 网络是一个基于价值的强化学习模型。它根据 Q 函数确定最佳策略：
- en: '|  | $\pi^{*}(s)=arg\max_{a}Q^{*}(s,a)$ |  | (40) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}(s)=arg\max_{a}Q^{*}(s,a)$ |  | (40) |'
- en: Where $Q^{*}(s,a)$ is an optimal Q-function and $\pi^{*}(s)$ is the corresponding
    optimal policy. In Deep Q-Networks, the Q function is modeled using a deep neural
    network, such as CNNs, RNNs, etc.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q^{*}(s,a)$ 是一个最优 Q 函数，$\pi^{*}(s)$ 是相应的最优策略。在深度 Q 网络中，Q 函数使用深度神经网络建模，例如
    CNNs、RNNs 等。
- en: 'As in Gao et al. ([2018](#bib.bib98)), the parameters of the Q model are updated
    using the rule:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如 Gao et al.（[2018](#bib.bib98)）所述，Q 模型的参数使用以下规则更新：
- en: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta)-Q(s_{t},a_{t};\theta)\right)}_{\text{temporal\
    difference}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$ |  | (41) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta)-Q(s_{t},a_{t};\theta)\right)}_{\text{时间差分}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$
    |  | (41) |'
- en: Where the $(s_{t},a_{t},r_{t},s_{t+1})$ is an observed trajectory. $\alpha$
    denotes step-size and the parameter update is calculated using temporal difference (Sutton,
    [1988](#bib.bib353)). However, this update mechanism suffers from unstableness
    and demands a large number of training samples. There are two typical tricks for
    a more efficient and stable parameter update.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(s_{t},a_{t},r_{t},s_{t+1})$ 是一个观测到的轨迹。$\alpha$ 表示步长，参数更新是通过时间差分（Sutton,
    [1988](#bib.bib353)）来计算的。然而，这种更新机制存在不稳定性，并且需要大量的训练样本。为了实现更高效和稳定的参数更新，有两种典型的技巧。
- en: The first method is experience replay (Lin, [1992](#bib.bib211); Mnih et al.,
    [2015](#bib.bib252)). Instead of using one training sample at a time to update
    the parameters, it uses a buffer to store training samples, and iteratively retrieves
    training samples from the buffer pool to perform parameter updates. It avoids
    encountering training samples that change too fast in distribution during training
    time, which increases the learning stability; further, it uses each training sample
    multiple times, which improves the efficiency.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是经验回放（Lin, [1992](#bib.bib211); Mnih et al., [2015](#bib.bib252)）。它不是一次使用一个训练样本来更新参数，而是使用一个缓冲区来存储训练样本，并从缓冲池中迭代地检索训练样本进行参数更新。它避免了在训练期间遇到分布变化过快的训练样本，从而提高了学习的稳定性；此外，它多次使用每个训练样本，从而提高了效率。
- en: 'The second is two-network implementation (Mnih et al., [2015](#bib.bib252)).
    This method uses two networks in Q-function optimization, one being the Q-network,
    another being a target network. The target network is used to calculate the temporal
    difference, and its parameters $\theta_{target}$ are frozen while training, aligning
    with $\theta$ periodically. The parameters are then updated with the following
    rule:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是双网络实现（Mnih et al., [2015](#bib.bib252)）。该方法在 Q 函数优化中使用两个网络，一个是 Q 网络，另一个是目标网络。目标网络用于计算时间差异，其参数
    $\theta_{target}$ 在训练时保持不变，定期与 $\theta$ 对齐。然后使用以下规则更新参数：
- en: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta_{target})-Q(s_{t},a_{t};\theta)\right)}_{\text{temporal\
    difference\ with\ a\ target\ network}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$
    |  | (42) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\underbrace{\left(r_{t}+\gamma\max_{a_{t+1}}Q(s_{t+1},a_{t+1};\theta_{target})-Q(s_{t},a_{t};\theta)\right)}_{\text{带有目标网络的时间差}}\bigtriangledown_{\theta}Q(s_{t},a_{t};\theta)$
    |  | (42) |'
- en: Since $\theta_{target}$ does not change in a period of time, the target network
    calculates the temporal difference in a stable manner, which facilitates the convergence
    of training.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\theta_{target}$ 在一段时间内不会改变，目标网络以稳定的方式计算时间差异，这有助于训练的收敛。
- en: 2.7.2 REINFORCE
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.2 REINFORCE
- en: 'REINFORCE is a policy-based RL algorithm that has no value network. It optimizes
    the policy directly. The policy is parameterized by a policy network, whose output
    is a distribution over continuous or discrete actions. A long-term reward is computed
    for evaluation of the policy network by collecting trajectory samples of length
    $H$:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: REINFORCE 是一种基于策略的强化学习算法，没有价值网络。它直接优化策略。策略由一个策略网络参数化，其输出是对连续或离散动作的分布。通过收集长度为
    $H$ 的轨迹样本来计算长期奖励，以评估策略网络：
- en: '|  | $J(\theta)=E\left[\sum_{t=1}^{H}\gamma^{t-1}r_{t}&#124;a_{t}\sim\pi(s_{t};\theta)\right]$
    |  | (43) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=E\left[\sum_{t=1}^{H}\gamma^{t-1}r_{t}\vert a_{t}\sim\pi(s_{t};\theta)\right]$
    |  | (43) |'
- en: '$J(\theta)$ denotes a long-term reward and the goal is to optimize the policy
    network in order to maximize $J(\theta)$. Here stochastic gradient ascent⁷⁷7Stochastic
    gradient ascent simply uses the negated objective function of stochastic gradient
    descent. is used as an optimizer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: $J(\theta)$ 表示长期奖励，目标是优化策略网络以最大化 $J(\theta)$。这里使用**随机梯度上升**作为优化器：
- en: '|  | $\theta\leftarrow\theta+\alpha\bigtriangledown_{\theta}J(\theta)$ |  |
    (44) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta\leftarrow\theta+\alpha\bigtriangledown_{\theta}J(\theta)$ |  |
    (44) |'
- en: 'Where $\bigtriangledown_{\theta}J(\theta)$ is computed by:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bigtriangledown_{\theta}J(\theta)$ 通过以下方式计算：
- en: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}&#124;s_{t};\theta)\sum_{h=t}^{H}\gamma^{h-t}r_{h}\right)$
    |  | (45) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}\vert
    s_{t};\theta)\sum_{h=t}^{H}\gamma^{h-t}r_{h}\right)$ |  | (45) |'
- en: 'Both models have their advantages: Deep Q-Networks are more sample efficient
    while REINFORCE is more stable (Li, [2017](#bib.bib205)). REINFORCE is more popular
    in recent works. Modern research involves larger action spaces, which means that
    value-based RL models like Deep Q-Networks are not suitable for problem-solving.
    Value-based methods “select an action to maximize the value", which means that
    their action sets should be discrete and moderate in scale; while policy gradient
    methods such as REINFORCE are different, they predict the action via policy networks
    directly, which sets no restriction on the action space. As a result, policy gradient
    methods are more suitable for tasks involving a larger action space.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 两种模型各有优点：深度Q网络在样本效率上更高，而REINFORCE则更稳定（Li, [2017](#bib.bib205)）。在近期的研究中，REINFORCE更为流行。现代研究涉及更大的动作空间，这意味着像深度Q网络这样的基于值的强化学习模型不适合解决这些问题。基于值的方法“选择一个动作以最大化价值”，这意味着它们的动作集合应该是离散且规模适中的；而像REINFORCE这样的策略梯度方法则不同，它们通过策略网络直接预测动作，对动作空间没有限制。因此，策略梯度方法更适合处理涉及更大动作空间的任务。
- en: 'Considering the respective benefits brought by the Q-learning and policy gradient,
    some work has been done combining the value- and policy-based methods. Actor-critic
    algorithm (Konda and Tsitsiklis, [2000](#bib.bib172); Sutton et al., [1999](#bib.bib354))
    was proposed to alleviate the severe variance problem when calculating the gradient
    in policy gradient methods. It estimates a value function for term $\sum_{h=t}^{H}\gamma^{h-t}r_{h}$
    in Equation ([45](#S2.E45 "In 2.7.2 REINFORCE ‣ 2.7 Deep Reinforcement Learning
    Models and Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey"))
    and incorporates it in policy optimization. Equation ([45](#S2.E45 "In 2.7.2 REINFORCE
    ‣ 2.7 Deep Reinforcement Learning Models and Generative Adversarial Networks ‣
    2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey")) is then transformed into the formula below:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Q-learning和策略梯度各自带来的好处，一些工作已将基于值的方法与基于策略的方法结合起来。演员-评论家算法（Konda and Tsitsiklis,
    [2000](#bib.bib172); Sutton et al., [1999](#bib.bib354)）被提出以缓解在策略梯度方法中计算梯度时的严重方差问题。它为方程式中的项$\sum_{h=t}^{H}\gamma^{h-t}r_{h}$进行值函数估计，并将其纳入策略优化。方程式
    ([45](#S2.E45 "在2.7.2 REINFORCE ‣ 2.7 深度强化学习模型与生成对抗网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统评述"))
    然后被转换为如下公式：
- en: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}&#124;s_{t};\theta)\hat{Q}(s_{t},a_{t},h)\right)$
    |  | (46) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bigtriangledown_{\theta}J(\theta)=\sum_{t=1}^{H-1}\gamma^{t-1}\left(\bigtriangledown_{\theta}log\pi(a_{t}|s_{t};\theta)\hat{Q}(s_{t},a_{t},h)\right)$
    |  | (46) |'
- en: Where $\hat{Q}(s_{t},a_{t},h)$ stands for the value function estimated.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{Q}(s_{t},a_{t},h)$ 代表估计的值函数。
- en: 2.7.3 GANs
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.7.3 GANs
- en: It is easy to link the actor-critic model with another framework - GANs (Goodfellow
    et al., [2014](#bib.bib108); Zhang et al., [2018c](#bib.bib451); Feng et al.,
    [2020a](#bib.bib87)) because of their similar inner structure and logic (Pfau
    and Vinyals, [2016](#bib.bib274)). Actually, there are quite a few recent works
    in dialogue systems that train GANs with reinforcement learning framework (Zhu
    et al., [2018](#bib.bib466); Wu et al., [2019b](#bib.bib415); He et al., [2020a](#bib.bib128);
    Zhu et al., [2020](#bib.bib467); Qin et al., [2020](#bib.bib281)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其相似的内部结构和逻辑，容易将演员-评论家模型与另一框架——GANs （Goodfellow et al., [2014](#bib.bib108);
    Zhang et al., [2018c](#bib.bib451); Feng et al., [2020a](#bib.bib87)）进行联系（Pfau
    and Vinyals, [2016](#bib.bib274)）。实际上，近年来有相当多的对话系统研究利用强化学习框架训练GANs（Zhu et al.,
    [2018](#bib.bib466); Wu et al., [2019b](#bib.bib415); He et al., [2020a](#bib.bib128);
    Zhu et al., [2020](#bib.bib467); Qin et al., [2020](#bib.bib281)）。
- en: '![Refer to caption](img/150fae7ed08922d0dd788e68bb1aea35.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/150fae7ed08922d0dd788e68bb1aea35.png)'
- en: 'Figure 11: The GAN framework'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：GAN框架
- en: 'Figure [11](#S2.F11 "Figure 11 ‣ 2.7.3 GANs ‣ 2.7 Deep Reinforcement Learning
    Models and Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")
    represents the GAN consisting of a generator and a discriminator where the training
    process can be viewed as a competition between them: the generator tries to generate
    data distributions to fool the discriminator while the discriminator attempts
    to distinguish between real data (real) and generated data (fake). During training,
    the generator takes noise as input and generates data distribution while the discriminator
    takes real and fake data as input and the binary annotation as the label. The
    whole GAN model is trained end-to-end as a connection of generator and discriminator
    to minimize the following cross-entropy losses:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#S2.F11 "Figure 11 ‣ 2.7.3 GANs ‣ 2.7 Deep Reinforcement Learning Models
    and Generative Adversarial Networks ‣ 2 Neural Models in Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")展示了包含生成器和判别器的GAN，其中训练过程可以被视为它们之间的竞争：生成器试图生成数据分布以欺骗判别器，而判别器则试图区分真实数据（真实）和生成的数据（伪造）。在训练过程中，生成器以噪声作为输入生成数据分布，而判别器则以真实和伪造的数据作为输入，二分类标注作为标签。整个GAN模型作为生成器和判别器的连接进行端到端训练，以最小化以下交叉熵损失：'
- en: '|  | $L_{1}(D,G)=-E_{\omega\sim P_{data}}[logD(\omega)]-E_{z\sim N(0,I)}[log(1-D(G(z)))]$
    |  | (47) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{1}(D,G)=-E_{\omega\sim P_{data}}[logD(\omega)]-E_{z\sim N(0,I)}[log(1-D(G(z)))]$
    |  | (47) |'
- en: '|  | $L_{2}(D,G)=-E_{z\sim N(0,I)}[logD(G(z))]$ |  | (48) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{2}(D,G)=-E_{z\sim N(0,I)}[logD(G(z))]$ |  | (48) |'
- en: Where $L_{1}$ and $L_{2}$ denote a bilevel loss, where $D$ and $G$ being discriminator
    and generator respectively. $z\sim N(0,I)$ is the noise input of the generator
    and $w$ is the input of the discriminator.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{1}$ 和 $L_{2}$ 表示一个双层损失，其中 $D$ 和 $G$ 分别为判别器和生成器。$z\sim N(0,I)$ 是生成器的噪声输入，$w$
    是判别器的输入。
- en: Relationship between RL and GAN
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RL与GAN的关系
- en: GAN can be viewed as a special actor-critic (Pfau and Vinyals, [2016](#bib.bib274)).
    In the learning architecture of GAN, the generator acts as the actor and the discriminator
    acts as the critic or environment which gives the real/fake feedback as a reward.
    However, the actions taken by the actor cannot change the states of the environment,
    which means that the learning architecture of GAN is a stateless Markov decision
    process. Also, the actor has no access to the state of the environment and generates
    data distribution simply conditioned on Gaussian noise, which means that the generator
    in the GAN framework is a blind actor/agent. In a nutshell, GAN is a special actor-critic
    where the actor is blind and the whole process is a stateless MDP.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: GAN可以被视为一种特殊的演员-评论员(Pfau and Vinyals, [2016](#bib.bib274))。在GAN的学习架构中，生成器充当演员，判别器充当评论员或环境，提供真实/伪造的反馈作为奖励。然而，演员所采取的行动不能改变环境的状态，这意味着GAN的学习架构是一个无状态的马尔可夫决策过程。同时，演员无法访问环境的状态，只是基于高斯噪声生成数据分布，这意味着GAN框架中的生成器是一个盲目的演员/代理。总而言之，GAN是一种特殊的演员-评论员，其中演员是盲目的，整个过程是一个无状态的MDP。
- en: The interactive nature of dialogue systems motivates the wide application of
    reinforcement learning and GAN models in its research.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 对话系统的互动特性激励了在其研究中广泛应用强化学习和GAN模型。
- en: RL for task-oriented dialogue systems
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 面向任务的对话系统中的RL
- en: 'One common application of reinforcement learning in dialogue systems is the
    reinforced dialogue management in task-oriented systems. Dialogue state tracking
    and policy learning are two typical modules of a dialogue manager. Huang et al.
    ([2020c](#bib.bib150)) and Li et al. ([2020d](#bib.bib207)) trained the dialogue
    state tracker with reinforcement learning. Both of them combined a reward manager
    into their tracker to enhance tracking accuracy. For the policy learning module,
    reinforcement learning seems to be the best choice since almost all recent related
    works learned policy with reinforcement learning (Zhang et al., [2019c](#bib.bib454);
    Wang et al., [2020d](#bib.bib387); Zhu et al., [2020](#bib.bib467); Wang et al.,
    [2020a](#bib.bib384); Takanobu et al., [2020](#bib.bib356); Huang et al., [2020b](#bib.bib149);
    Xu et al., [2020a](#bib.bib426)). The increasing preference of reinforcement learning
    in policy learning tasks attributes to the characteristic of them: in policy learning
    tasks, the model predicts a dialogue action (action) based on the states from
    the DST module (state), which perfectly accords with the function of the agent
    in the reinforcement learning framework.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习在对话系统中的一个常见应用是任务导向系统中的强化对话管理。对话状态追踪和策略学习是对话管理器的两个典型模块。黄等人 ([2020c](#bib.bib150))
    和李等人 ([2020d](#bib.bib207)) 使用强化学习训练了对话状态追踪器。他们都在追踪器中结合了奖励管理器，以提高追踪准确性。对于策略学习模块，强化学习似乎是最佳选择，因为几乎所有近期相关的研究都通过强化学习来学习策略
    (张等人，[2019c](#bib.bib454)；王等人，[2020d](#bib.bib387)；朱等人，[2020](#bib.bib467)；王等人，[2020a](#bib.bib384)；高信等人，[2020](#bib.bib356)；黄等人，[2020b](#bib.bib149)；徐等人，[2020a](#bib.bib426))。强化学习在策略学习任务中的日益偏好归因于其特点：在策略学习任务中，模型基于来自
    DST 模块的状态 (state) 预测对话动作 (action)，这与强化学习框架中代理的功能完全一致。
- en: RL for open-domain dialogue systems
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RL 在开放域对话系统中的应用
- en: Due to the huge action space needed to generate language directly, many open-domain
    dialogue systems trained with reinforcement learning framework do not generate
    responses but instead select responses. Retrieval-based systems have a limited
    action set and are suitable to be trained in a reinforcement learning scheme.
    Some works achieved promising performance in retrieval-based dialogue tasks (Bouchacourt
    and Baroni, [2019](#bib.bib27); Li et al., [2016a](#bib.bib192); Zhao and Eskenazi,
    [2016](#bib.bib455)). However, retrieval systems fail to generalize in all user
    messages and may give unrelated responses (Qiu et al., [2017](#bib.bib284)), which
    makes generation-based dialogue systems preferable. Still considering the action
    space problem, some works build their systems combining retrieval and generative
    methods (Zhu et al., [2018](#bib.bib466); Serban et al., [2017b](#bib.bib314)).
    Zhu et al. ([2018](#bib.bib466)) chose to first retrieve a set of n-best response
    candidates and then generated responses based on the retrieved results and user
    message. Comparatively, Serban et al. ([2017b](#bib.bib314)) first generated and
    retrieved candidate responses with different dialogue models and then trained
    a scoring model with online reinforcement learning to select responses from both
    generated and retrieved responses. Since training a generative dialogue agent
    using reinforcement learning from scratch is particularly difficult, first pretraining
    the agent with supervised learning to warm-start is a good choice. Wu et al. ([2019b](#bib.bib415)), He
    et al. ([2020a](#bib.bib128)), Williams and Zweig ([2016](#bib.bib407)) and Yao
    et al. ([2016](#bib.bib433)) applied this pretrain-and-finetune strategy on dialogue
    learning and achieved outstanding performance, which proved that the reinforcement
    learning can improve the response quality of data-driven chatbots. Similarly,
    pretrain-and-finetune was also applicable to domain transfer problems. Some works
    pretrained the model in a source domain and expanded the domain area with reinforcement
    training (Mo et al., [2018](#bib.bib254); Li et al., [2016d](#bib.bib195)).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接生成语言需要庞大的动作空间，许多使用强化学习框架训练的开放域对话系统不生成回应，而是选择回应。基于检索的系统有一个有限的动作集，适合在强化学习方案中训练。一些研究在基于检索的对话任务中取得了令人鼓舞的成果（Bouchacourt
    和 Baroni，[2019](#bib.bib27)；Li 等，[2016a](#bib.bib192)；Zhao 和 Eskenazi，[2016](#bib.bib455)）。然而，检索系统无法在所有用户消息中进行泛化，可能给出无关的回应（Qiu
    等，[2017](#bib.bib284)），这使得生成式对话系统更为可取。尽管考虑到动作空间问题，一些研究通过结合检索和生成方法构建系统（Zhu 等，[2018](#bib.bib466)；Serban
    等，[2017b](#bib.bib314)）。Zhu 等（[2018](#bib.bib466)）选择首先检索一组n最佳回应候选，然后基于检索结果和用户消息生成回应。相比之下，Serban
    等（[2017b](#bib.bib314)）首先生成和检索不同对话模型的候选回应，然后用在线强化学习训练评分模型，从生成和检索的回应中选择回应。由于从头开始使用强化学习训练生成对话代理特别困难，先用监督学习进行预训练以热启动是一个不错的选择。Wu
    等（[2019b](#bib.bib415)），He 等（[2020a](#bib.bib128)），Williams 和 Zweig（[2016](#bib.bib407)）以及Yao
    等（[2016](#bib.bib433)）在对话学习中应用了这种预训练和微调策略，并取得了出色的成果，这证明了强化学习可以提高数据驱动聊天机器人的回应质量。同样，预训练和微调也适用于领域迁移问题。一些研究在源领域中预训练模型，并通过强化训练扩展领域范围（Mo
    等，[2018](#bib.bib254)；Li 等，[2016d](#bib.bib195)）。
- en: RL for knowledge grounded dialogue systems
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识驱动对话系统的强化学习
- en: Some systems use reinforcement learning to select from outside information like
    persona, document, knowledge graph, etc., and generate responses accordingly.
    Majumder et al. ([2020a](#bib.bib238)) and Jaques et al. ([2020](#bib.bib154))
    performed persona selection and persona-based response generation simultaneously
    and trained their agents with a reinforcement framework. Bao et al. ([2019a](#bib.bib16))
    and Zhao et al. ([2020b](#bib.bib459)) built document-grounded systems. Similarly,
    they used reinforcement learning to accomplish document selection and knowledge-grounded
    response generation. There were also some works combining knowledge graphs into
    the dialogue systems and treated them as outside knowledge source (Moon et al.,
    [2019](#bib.bib257); Xu et al., [2020a](#bib.bib426)). In a reinforced training
    framework, the agent chooses an edge based on the current node and state for each
    step and then combines the knowledge into the response generation process.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统使用强化学习从外部信息如个性、文档、知识图谱等中进行选择，并相应地生成响应。Majumder 等（[2020a](#bib.bib238)）和
    Jaques 等（[2020](#bib.bib154)）同时进行了个性选择和基于个性的响应生成，并通过强化学习框架训练了他们的智能体。Bao 等（[2019a](#bib.bib16)）和
    Zhao 等（[2020b](#bib.bib459)）构建了文档基础的系统。他们也使用强化学习来完成文档选择和知识基础的响应生成。同时，也有一些工作将知识图谱结合到对话系统中，并将其视为外部知识源（Moon
    等，[2019](#bib.bib257)；Xu 等，[2020a](#bib.bib426)）。在强化训练框架中，智能体根据当前节点和状态为每一步选择一条边，然后将知识结合到响应生成过程中。
- en: RL for dialogue related tasks
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话相关任务的强化学习
- en: Dialogue-related tasks like dialogue relation extraction (Li et al., [2019c](#bib.bib203)),
    question answering (Hua et al., [2020](#bib.bib146)) and machine reading comprehension (Guo
    et al., [2020](#bib.bib116)) benefit from reinforcement learning as well because
    of their interactive nature and the scarcity of annotated data.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对话相关任务如对话关系提取（Li 等，[2019c](#bib.bib203)）、问答（Hua 等，[2020](#bib.bib146)）和机器阅读理解（Guo
    等，[2020](#bib.bib116)）也从强化学习中受益，因为这些任务具有交互性和注释数据的稀缺性。
- en: GAN for dialogue systems
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对话系统中的GAN
- en: 'The application of GAN in dialogue systems is divided into two streams. The
    first sees the GAN framework applied to enhance response generation (Li et al.,
    [2017a](#bib.bib196); Zhu et al., [2018](#bib.bib466); Wu et al., [2019b](#bib.bib415);
    He et al., [2020a](#bib.bib128); Zhu et al., [2020](#bib.bib467); Qin et al.,
    [2020](#bib.bib281)). The discriminator distinguishes generated responses from
    human responses, which incentivizes the agent, which is also the generator in
    GAN, to generate higher-quality responses. Another stream uses GAN as an evaluation
    tool of dialogue systems (Kannan and Vinyals, [2017](#bib.bib164); Bruni and Fernandez,
    [2017](#bib.bib29)). After training the generator and discriminator as a whole
    framework, the discriminator is used separately as a scorer to evaluate the performance
    of a dialogue agent and was shown to achieve a higher correlation with human evaluation
    compared with traditional reference-based metrics like BLEU, METEOR, ROUGE-L,
    etc. We discuss the evaluation of dialogue systems as a challenge in Section [5](#S5
    "5 Evaluation Approaches ‣ Recent Advances in Deep Learning Based Dialogue Systems:
    A Systematic Survey").'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'GAN 在对话系统中的应用分为两个方向。第一个方向将 GAN 框架应用于增强响应生成（Li 等，[2017a](#bib.bib196)；Zhu 等，[2018](#bib.bib466)；Wu
    等，[2019b](#bib.bib415)；He 等，[2020a](#bib.bib128)；Zhu 等，[2020](#bib.bib467)；Qin
    等，[2020](#bib.bib281)）。判别器将生成的响应与人类响应区分开来，这激励了智能体，即 GAN 中的生成器，生成更高质量的响应。另一个方向将
    GAN 作为对话系统的评估工具（Kannan 和 Vinyals，[2017](#bib.bib164)；Bruni 和 Fernandez，[2017](#bib.bib29)）。在将生成器和判别器作为一个整体框架进行训练后，判别器被单独用作评分器来评估对话智能体的表现，并显示出与传统的参考基准指标如
    BLEU、METEOR、ROUGE-L 等相比，与人类评估的相关性更高。我们将在第 [5](#S5 "5 Evaluation Approaches ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey") 节讨论对话系统的评估作为一个挑战。'
- en: 2.8 Knowledge Graph Augmented Neural Networks
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8 知识图谱增强神经网络
- en: Supervised training with annotated data tries to learn the knowledge distribution
    of a dataset. However, a dataset is comparatively sparse and thus learning a reliable
    knowledge distribution needs a huge amount of annotated data (Annervaz et al.,
    [2018](#bib.bib6)).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用带注释的数据进行监督训练试图学习数据集的知识分布。然而，数据集相对稀疏，因此学习可靠的知识分布需要大量的注释数据（Annervaz 等，[2018](#bib.bib6)）。
- en: Knowledge Graph (KG) is attracting more and more research interests in recent
    years. KG is a structured knowledge source consisting of entities and their relationships (Ji
    et al., [2022](#bib.bib157)). In other words, KG is the knowledge facts presented
    in graph format.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱（KG）近年来吸引了越来越多的研究兴趣。KG是一个由实体及其关系组成的结构化知识源（Ji等，[2022](#bib.bib157)）。换句话说，KG是以图形格式呈现的知识事实。
- en: '![Refer to caption](img/cc06c397bbc31e22a1e21cb9ac8daa76.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cc06c397bbc31e22a1e21cb9ac8daa76.png)'
- en: 'Figure 12: Entities and relations in knowledge graph (Ji et al., [2022](#bib.bib157))'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：知识图谱中的实体和关系（Ji等，[2022](#bib.bib157)）
- en: 'Figure [12](#S2.F12 "Figure 12 ‣ 2.8 Knowledge Graph Augmented Neural Networks
    ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in Deep Learning Based
    Dialogue Systems: A Systematic Survey") shows an example of a KG consisting of
    entities and their relationships. A KG is stored in triples under the Resource
    Description Framework (RDF). For example, Albert Einstein, University of Zurich,
    and their relationship can be expressed as $(AlbertEinstein,GraduateFrom,UniversityofZurich)$.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](#S2.F12 "图12 ‣ 2.8 知识图谱增强神经网络 ‣ 2 对话系统中的神经模型 ‣ 基于深度学习的对话系统的最新进展：系统综述")展示了一个由实体及其关系组成的KG示例。KG以三元组形式存储在资源描述框架（RDF）下。例如，阿尔伯特·爱因斯坦、苏黎世大学及其关系可以表示为$(AlbertEinstein,GraduateFrom,UniversityofZurich)$。
- en: 'Knowledge graph augmented neural networks first represent the entities and
    their relations in a lower dimension space, then use a neural model to retrieve
    relevant facts (Ji et al., [2022](#bib.bib157)). Knowledge graph representation
    learning can be generally divided into two categories: structure-based representations
    and semantically-enriched representations. Structure-based representations use
    multi-dimensional vectors to represent entities and relations. Models such as
    TransE (Bordes et al., [2013](#bib.bib23)), TransR (Lin et al., [2015](#bib.bib215)),
    TransH (Wang et al., [2014](#bib.bib393)), TransD (Ji et al., [2015](#bib.bib156)),
    TransG (Xiao et al., [2015](#bib.bib420)), TransM (Fan et al., [2014](#bib.bib84)),
    HolE (Nickel et al., [2016](#bib.bib262)) and ProjE (Shi and Weninger, [2017](#bib.bib322))
    belong to this category. The semantically-enriched representation models like
    NTN (Socher et al., [2013](#bib.bib334)), SSP (Xiao et al., [2017](#bib.bib421))
    and DKRL (Xie et al., [2016](#bib.bib422)) combine semantic information into the
    representation of entities and relations. The neural retrieval models also have
    two main directions: distance-based matching model and semantic matching model.
    Distance-based matching models (Bordes et al., [2013](#bib.bib23)) consider the
    distance between projected entities while semantic matching models (Bordes et al.,
    [2014](#bib.bib24)) calculate the semantic similarity of entities and relations
    to retrieve facts.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱增强的神经网络首先在较低维度空间中表示实体及其关系，然后使用神经模型检索相关事实（Ji等，[2022](#bib.bib157)）。知识图谱表示学习通常可以分为两类：基于结构的表示和语义增强的表示。基于结构的表示使用多维向量来表示实体和关系。诸如TransE（Bordes等，[2013](#bib.bib23)）、TransR（Lin等，[2015](#bib.bib215)）、TransH（Wang等，[2014](#bib.bib393)）、TransD（Ji等，[2015](#bib.bib156)）、TransG（Xiao等，[2015](#bib.bib420)）、TransM（Fan等，[2014](#bib.bib84)）、HolE（Nickel等，[2016](#bib.bib262)）和ProjE（Shi和Weninger，[2017](#bib.bib322)）等模型属于这一类别。语义增强的表示模型如NTN（Socher等，[2013](#bib.bib334)）、SSP（Xiao等，[2017](#bib.bib421)）和DKRL（Xie等，[2016](#bib.bib422)）将语义信息融入实体和关系的表示中。神经检索模型也有两个主要方向：基于距离的匹配模型和语义匹配模型。基于距离的匹配模型（Bordes等，[2013](#bib.bib23)）考虑投影实体之间的距离，而语义匹配模型（Bordes等，[2014](#bib.bib24)）计算实体和关系的语义相似性来检索事实。
- en: Knowledge graph augmented dialogue systems
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 知识图谱增强对话系统
- en: Knowledge-grounded dialogue systems benefit greatly from the structured knowledge
    format of KG, where facts are widely intercorrelated. Reasoning over a KG is an
    ideal approach for combining commonsense knowledge into response generation, resulting
    in accurate and informative responses (Young et al., [2018](#bib.bib438)). Jung
    et al. ([2020](#bib.bib160)) proposed AttnIO, a bi-directional graph exploration
    model for knowledge retrieval in knowledge-grounded dialogue systems. Attention
    weights were calculated at each traversing step, and thus the model could choose
    a broader range of knowledge paths instead of choosing only one node at a time.
    In such a scheme, the model could predict adequate paths even when only having
    the destination node as the label. Zhang et al. ([2019b](#bib.bib447)) built ConceptFlow,
    a dialogue agent that guided to more meaningful future conversations. It traversed
    in a commonsense knowledge graph to explore concept-level conversation flows.
    Finally, it used a gate to decide to generate among vocabulary words, central
    concept words, and outer concept words. Majumder et al. ([2020a](#bib.bib238))
    proposed to generate persona-based responses by first using COMET (Bosselut et al.,
    [2019](#bib.bib26)) to expand a persona sentence in context along 9 relation types
    and then applied a pretrained model to generate responses based on dialogue history
    and the persona variable. Yang et al. ([2020](#bib.bib429)) used knowledge graph
    as an external knowledge source in task-oriented dialogue systems to incorporate
    domain-specified knowledge in the response. First, the dialogue history was parsed
    as a dependency tree and encoded into a fixed-length vector. Then they applied
    multi-hop reasoning over the graph using the attention mechanism. The decoder
    finally predicted tokens either by copying from graph entities or generating vocabulary
    words. Moon et al. ([2019](#bib.bib257)) proposed DialKG Walker for the conversational
    reasoning task. They computed a zero-shot relevance score between predicted KG
    embedding and ground KG embedding to facilitate cross-domain predictions. Furthermore,
    they applied an attention-based graph walker to generate graph paths based on
    the relevance scores. Huang et al. ([2020a](#bib.bib147)) evaluated the dialogue
    systems by combining the utterance-level contextualized representation and topic-level
    graph representation. They first constructed the dialogue graph based on encoded
    (context, response) pairs and then reasoned over the graph to get a topic-level
    graph representation. The final score was calculated by passing the concatenated
    vector of contextualized representation and graph representation to a feed-forward
    network.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 知识基础对话系统从KG的结构化知识格式中获益良多，其中事实之间广泛关联。对KG的推理是将常识知识融入响应生成的理想方法，从而生成准确且信息丰富的回应（Young
    et al., [2018](#bib.bib438)）。Jung et al. ([2020](#bib.bib160)) 提出了AttnIO，一种用于知识检索的双向图探索模型。注意力权重在每一步遍历时进行计算，因此模型可以选择更广泛的知识路径，而不是每次只选择一个节点。在这种方案中，即使只有目标节点作为标签，模型也能预测出合适的路径。Zhang
    et al. ([2019b](#bib.bib447)) 构建了ConceptFlow，一个引导更有意义未来对话的对话代理。它在常识知识图中遍历以探索概念级对话流。最后，它使用门控机制决定在词汇单词、中心概念词和外部概念词之间生成。Majumder
    et al. ([2020a](#bib.bib238)) 提出了基于角色生成响应的方法，通过首先使用COMET (Bosselut et al., [2019](#bib.bib26))
    在9种关系类型下扩展角色句子，然后应用预训练模型根据对话历史和角色变量生成响应。Yang et al. ([2020](#bib.bib429)) 将知识图谱作为任务导向对话系统中的外部知识源，以将领域特定知识纳入响应中。首先，将对话历史解析为依赖树并编码为固定长度的向量。然后，他们利用注意力机制在图上进行多跳推理。解码器最终通过从图实体中复制或生成词汇单词来预测标记。Moon
    et al. ([2019](#bib.bib257)) 提出了DialKG Walker用于对话推理任务。他们计算了预测KG嵌入和实际KG嵌入之间的零样本相关性分数，以促进跨领域预测。此外，他们应用基于注意力的图行走器根据相关性分数生成图路径。Huang
    et al. ([2020a](#bib.bib147)) 通过结合话语级上下文表示和主题级图表示来评估对话系统。他们首先基于编码的（上下文、回应）对构建对话图，然后在图上推理以获得主题级图表示。最终分数通过将上下文表示和图表示的串联向量传递到前馈网络中计算得出。
- en: 3 Task-oriented Dialogue Systems
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 任务导向对话系统
- en: This section introduces task-oriented dialogue systems including modular and
    end-to-end systems. Task-oriented systems solve specific problems in a certain
    domain such as movie ticket booking, restaurant table reserving, etc. We focus
    on deep learning-based systems due to the outstanding performance. For readers
    who want to learn more about traditional rule-based and statistical models, there
    are several surveys to refer to (Theune, [2003](#bib.bib367); Lemon and Pietquin,
    [2007](#bib.bib189); Mallios and Bourbakis, [2016](#bib.bib240); Chen et al.,
    [2017a](#bib.bib39); Santhanam and Shaikh, [2019](#bib.bib305)).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了包括模块化和端到端系统的面向任务的对话系统。面向任务的系统解决特定领域中的问题，如电影票预订、餐厅桌位预订等。我们关注基于深度学习的系统，因为它们表现优异。对于那些想了解传统规则基础和统计模型的读者，可以参考一些综述文章（Theune,
    [2003](#bib.bib367); Lemon and Pietquin, [2007](#bib.bib189); Mallios and Bourbakis,
    [2016](#bib.bib240); Chen et al., [2017a](#bib.bib39); Santhanam and Shaikh, [2019](#bib.bib305)）。
- en: This section is organized as follows. We first discuss modular and end-to-end
    systems respectively by introducing the principles and reviewing recent works.
    After that, we comprehensively discuss related challenges and hot topics for task-oriented
    dialogue systems in recent research to provide some important research directions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的组织结构如下。我们首先分别讨论模块化和端到端系统，介绍其原理并回顾近期的工作。之后，我们全面讨论面向任务的对话系统在近期研究中的相关挑战和热点话题，以提供一些重要的研究方向。
- en: '![Refer to caption](img/4918f832683341a45125f7a76ec974f8.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4918f832683341a45125f7a76ec974f8.png)'
- en: 'Figure 13: Structure of a task-oriented dialogue system in the task-completion
    pipeline'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：任务完成流程中的面向任务的对话系统结构
- en: 'A task-oriented dialogue system requires stricter response constraints because
    it aims to accurately handle the user message. Therefore, modular methods were
    proposed to generate responses in a more controllable way. The architecture of
    a modular-based system is depicted in Figure [13](#S3.F13 "Figure 13 ‣ 3 Task-oriented
    Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A
    Systematic Survey"). It consists of four modules:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '面向任务的对话系统需要更严格的响应约束，因为它旨在准确处理用户消息。因此，提出了模块化方法以更可控的方式生成响应。基于模块化的系统架构如图[13](#S3.F13
    "Figure 13 ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey")所示。它由四个模块组成：'
- en: 'Natural Language Understanding (NLU). This module converts the raw user message
    into semantic slots, together with classifications of domain and user intention.
    However, some recent modular systems omit this module and use the raw user message
    as the input of the next module, as shown in Figure [13](#S3.F13 "Figure 13 ‣
    3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey"). Such a design aims to reduce the propagation of
    errors between modules and alleviate the impact of the original error (Kim et al.,
    [2018](#bib.bib166)).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '自然语言理解（NLU）。该模块将原始用户消息转换为语义槽，同时进行领域和用户意图的分类。然而，一些近期的模块化系统省略了此模块，直接使用原始用户消息作为下一个模块的输入，如图[13](#S3.F13
    "Figure 13 ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey")所示。这种设计旨在减少模块间的错误传播，并减轻原始错误的影响（Kim
    et al., [2018](#bib.bib166)）。'
- en: Dialogue State Tracking (DST). This module iteratively calibrates the dialogue
    states based on the current input and dialogue history. The dialogue state includes
    related user actions and slot-value pairs.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态追踪（DST）。该模块根据当前输入和对话历史反复校准对话状态。对话状态包括相关的用户动作和槽值对。
- en: Dialogue Policy Learning. Based on the calibrated dialogue states from the DST
    module, this module decides the next action of a dialogue agent.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对话策略学习。基于来自DST模块的校准对话状态，该模块决定对话代理的下一个动作。
- en: Natural Language Generation (NLG). This module converts the selected dialogue
    actions into surface-level natural language, which is usually the ultimate form
    of response.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）。该模块将选定的对话动作转换为表面层次的自然语言，这通常是响应的*终极*形式。
- en: Among them, Dialogue State Tracking and Dialogue Policy Learning constitute
    the Dialogue Manager (DM), the central controller of a task-oriented dialogue
    system. Usually, a task-oriented system also interacts with an external Knowledge
    Base (KB) to retrieve essential knowledge about the target task. For example,
    in a movie ticket booking task, after understanding the requirement of the user
    message, the agent interacts with the movie knowledge base to search for movies
    with specific constraints such as movie name, time, cinema, etc.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对话状态跟踪和对话策略学习构成了对话管理器（DM），它是任务导向对话系统的中央控制器。通常，任务导向系统还与外部知识库（KB）交互，以检索有关目标任务的基本知识。例如，在电影票预订任务中，在理解用户消息的需求后，代理与电影知识库交互，以搜索符合特定约束条件的电影，如电影名称、时间、电影院等。
- en: 3.1 Natural Language Understanding
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 自然语言理解
- en: 'It has been proven that the NLU module impacts the whole system significantly
    in the term of response quality (Li et al., [2017b](#bib.bib201)). The NLU module
    converts the natural language message produced by the user into semantic slots
    and performs classification. Table [2](#S3.T2 "Table 2 ‣ 3.1 Natural Language
    Understanding ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey") shows an example of the output format
    of the NLU module. The NLU module manages three tasks: domain classification,
    intent detection, and slot filling. Domain classification and intent detection
    are classification problems, which use classifiers to predict a mapping from the
    input language sequence to a predefined label set. In the given example, the predicted
    domain is “movie" and the intent is “find_movie". Slot filling is a tagging problem,
    which can be viewed as a sequence-to-sequence task. It maps a raw user message
    into a sequence of slot names. In the example, the NLU module reads the user message
    “Recommend a movie at Golden Village tonight." and outputs the corresponding tag
    sequence. It recognizes “Golden Village" as the place to go, which is tagged as
    “B_desti" and “I_desti" for the two words respectively. Similarly, the token “tonight"
    is converted into “B_time". ‘B’ represents the beginning of a chunk, and ‘I’ indicates
    that this tag is inside a target chunk. For those unrelated tokens, an ‘O’ is
    used indicating that this token is outside of any chunk of interest. This tagging
    method is called Inside-Outside-Beginning (IOB) tagging (Ramshaw and Marcus, [1999](#bib.bib290)),
    which is a common method in Named-Entity Recognition (NER) tasks.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '已证明，NLU 模块对整个系统的响应质量有显著影响（Li 等，[2017b](#bib.bib201)）。NLU 模块将用户生成的自然语言信息转换为语义槽，并进行分类。表
    [2](#S3.T2 "Table 2 ‣ 3.1 Natural Language Understanding ‣ 3 Task-oriented Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey") 展示了 NLU 模块的输出格式示例。NLU 模块管理三个任务：领域分类、意图检测和槽填充。领域分类和意图检测是分类问题，使用分类器来预测输入语言序列与预定义标签集之间的映射。在给定的示例中，预测的领域是“movie”，意图是“find_movie”。槽填充是标注问题，可以视为序列到序列的任务。它将原始用户消息映射到一系列槽名称。在这个示例中，NLU
    模块读取用户消息“Recommend a movie at Golden Village tonight.”并输出相应的标签序列。它识别“Golden Village”作为去的地方，将其标记为“B_desti”和“I_desti”。类似地，令牌“tonight”被转换为“B_time”。‘B’表示一个块的开始，‘I’表示该标签在目标块内部。对于那些无关的令牌，使用‘O’，表示该令牌在任何感兴趣的块之外。这种标注方法称为
    Inside-Outside-Beginning (IOB) 标注（Ramshaw 和 Marcus，[1999](#bib.bib290)），这是命名实体识别（NER）任务中的一种常用方法。'
- en: 'Table 2: The output example of an NLU module'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：NLU 模块的输出示例
- en: '| Sentence | Recommend | a | movie | at | Golden | Village | tonight |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 句子 | 推荐 | 一部 | 电影 | 在 | 金源 | 村 | 今晚 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Slots | O | O | O | O | B-desti | I-desti | B-time |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 槽 | O | O | O | O | B-desti | I-desti | B-time |'
- en: '| Intent |  |  |  | find_movie |  |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 意图 |  |  |  | find_movie |  |  |  |'
- en: '| Domain |  |  |  | movie |  |  |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 领域 |  |  |  | 电影 |  |  |  |'
- en: Techniques for domain classification and intent detection
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 领域分类和意图检测的技术
- en: Domain classification and intent detection belong to the same category of tasks.
    Deep learning methods are proposed to solve the classification problems of dialogue
    domain and intent. Deng et al. ([2012](#bib.bib67)) and Tur et al. ([2012](#bib.bib374))
    were the first who successfully improved the recognition accuracy of dialogue
    intent. They built deep convex networks to combine the predictions of a prior
    network and the current utterances as an integrated input of a current network.
    A deep learning framework was also used to classify the dialogue domain and intent
    in a semi-supervised fashion (Yann et al., [2014](#bib.bib430)). To solve the
    difficulty of training a deep neural network for domain and intent prediction,
    Restricted Boltzmann Machine (RBM) and Deep Belief Networks (DBNs) were applied
    to initialize the parameters of deep neural networks (Sarikaya et al., [2014](#bib.bib307)).
    To make use of the strengths of RNNs in sequence processing, some works used RNNs
    as utterance encoders and made predictions for intent and domain categories (Ravuri
    and Stolcke, [2015](#bib.bib293), [2016](#bib.bib294)). Hashemi et al. ([2016](#bib.bib124))
    used a CNN to extract hierarchical text features for intent detection and illustrated
    the sequence classification capabilities of CNNs. Lee and Dernoncourt ([2016](#bib.bib183))
    proposed a model for intent classification of short utterances. Short utterances
    are hard for intent detection because of the lack of information in a single dialogue
    turn. This paper used RNN and CNN architectures to incorporate the dialogue history,
    thus obtaining the context information as an additional input besides the current
    turn’s message. The model achieved promising performances on three intent classification
    datasets. More recently, Wu et al. ([2020a](#bib.bib414)) pretrained Task-Oriented
    Dialogue BERT (TOD-BERT) and significantly improved the accuracy in the intent
    detection sub-task. The proposed model also exhibited a strong capability of few-shot
    learning and could effectively alleviate the data insufficiency issue in a specific
    domain.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 领域分类和意图检测属于相同类别的任务。深度学习方法被提出用于解决对话领域和意图的分类问题。邓等人 ([2012](#bib.bib67)) 和 Tur
    等人 ([2012](#bib.bib374)) 是首批成功提高对话意图识别准确率的研究者。他们构建了深度凸网络，将先前网络的预测和当前话语的预测作为当前网络的综合输入。一个深度学习框架也被用来以半监督的方式分类对话领域和意图（Yann
    等人，[2014](#bib.bib430)）。为了解决训练深度神经网络进行领域和意图预测的难题，限制玻尔兹曼机（RBM）和深度置信网络（DBNs）被应用于初始化深度神经网络的参数（Sarikaya
    等人，[2014](#bib.bib307)）。为了利用递归神经网络（RNN）在序列处理中的优势，一些研究使用 RNN 作为话语编码器，并对意图和领域类别进行预测（Ravuri
    和 Stolcke，[2015](#bib.bib293)，[2016](#bib.bib294)）。Hashemi 等人 ([2016](#bib.bib124))
    使用卷积神经网络（CNN）提取层次化的文本特征进行意图检测，并展示了 CNN 的序列分类能力。Lee 和 Dernoncourt ([2016](#bib.bib183))
    提出了一个短话语意图分类模型。短话语因单个对话回合信息不足而难以进行意图检测。本文使用 RNN 和 CNN 架构来整合对话历史，从而将上下文信息作为当前回合消息之外的额外输入。该模型在三个意图分类数据集上取得了令人满意的性能。最近，Wu
    等人 ([2020a](#bib.bib414)) 预训练了任务导向对话 BERT（TOD-BERT），显著提高了意图检测子任务的准确率。该模型还展示了强大的少样本学习能力，并有效缓解了特定领域数据不足的问题。
- en: Techniques for slot filling
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 插槽填充技术
- en: The slot filling problem is also called semantic tagging, a sequence classification
    problem. It is more challenging for that the model needs to predict multiple objects
    at a time. Deep Belief Nets (DBNs) exhibit promising capabilities in the learning
    of deep architectures and have been applied in many tasks including semantic tagging.
    Sarikaya et al. ([2011](#bib.bib306)) used a DBN-initialized neural network to
    complete slot filling in the call-routing task. Deoras and Sarikaya ([2013](#bib.bib68))
    built a DBN-based sequence tagger. In addition to the NER input features used
    in traditional taggers, they also combined part of speech (POS) and syntactic
    features as a part of the input. The recurrent architectures benefited the sequence
    tagging task in that they could keep track of the information along past timesteps
    to make the most of the sequential information. Yao et al. ([2013](#bib.bib431))
    first argued that instead of simply predicting words, RNN Language Models (RNN-LMs)
    could be applied in sequence tagging. On the output side of RNN-LMs, tag labels
    were predicted instead of normal vocabularies. Mesnil et al. ([2013](#bib.bib246))
    and Mesnil et al. ([2014](#bib.bib247)) further investigated the impact of different
    recurrent architectures in the slot filling task and found that all RNNs outperformed
    the Conditional Random Field (CRF) baseline. As a powerful recurrent model, LSTM
    showed promising tagging accuracy on the ATIS dataset owing to the memory control
    of its gate mechanism (Yao et al., [2014](#bib.bib432)). Gangadharaiah and Narayanaswamy
    ([2020](#bib.bib97)) argued that the shallow output representations of traditional
    semantic tagging lacked the ability to represent the structured dialogue information.
    To improve, they treated the slot filling task as a template-based tree decoding
    task by iteratively generating and filling in the templates. Different from traditional
    sequence tagging methods, Coope et al. ([2020](#bib.bib58)) tackled the slot filling
    task by treating it as a turn-based span extraction task. They applied the conversational
    pretrained model ConveRT and utilized the rich semantic information embedded in
    the pretrained vectors to solve the problem of in-domain data insufficiency. The
    inputs of ConveRT are the requested slots and the utterance, while the output
    is a span of interest as the slot value.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 槽位填充问题也被称为语义标注，是一个序列分类问题。由于模型需要一次预测多个对象，这使得问题更加具有挑战性。深度信念网络（DBNs）在深度架构的学习中展现了良好的能力，并已被应用于包括语义标注在内的许多任务中。Sarikaya
    等人（[2011](#bib.bib306)）使用了一个以 DBN 初始化的神经网络来完成呼叫路由任务中的槽位填充。Deoras 和 Sarikaya（[2013](#bib.bib68)）建立了一个基于
    DBN 的序列标注器。除了传统标注器使用的命名实体识别（NER）输入特征外，他们还将词性（POS）和句法特征作为输入的一部分。递归架构使得序列标注任务受益，因为它们能够跟踪过去时间步的信息，以充分利用序列信息。Yao
    等人（[2013](#bib.bib431)）首次提出，相比于简单地预测单词，RNN 语言模型（RNN-LMs）可以应用于序列标注。在 RNN-LMs 的输出端，预测的是标签而不是普通词汇。Mesnil
    等人（[2013](#bib.bib246)）和 Mesnil 等人（[2014](#bib.bib247)）进一步研究了不同递归架构在槽位填充任务中的影响，发现所有
    RNN 都优于条件随机场（CRF）基线。作为一种强大的递归模型，LSTM 在 ATIS 数据集上显示出良好的标注准确性，这得益于其门控机制的记忆控制（Yao
    等人，[2014](#bib.bib432)）。Gangadharaiah 和 Narayanaswamy（[2020](#bib.bib97)）认为传统语义标注的浅层输出表示缺乏表示结构化对话信息的能力。为了改进，他们将槽位填充任务视为一个基于模板的树解码任务，通过迭代生成和填充模板来实现。与传统序列标注方法不同，Coope
    等人（[2020](#bib.bib58)）通过将槽位填充任务视为基于回合的跨度提取任务来解决这个问题。他们应用了对话预训练模型 ConveRT，并利用预训练向量中嵌入的丰富语义信息来解决领域数据不足的问题。ConveRT
    的输入是请求的槽位和话语，而输出是作为槽位值的感兴趣跨度。
- en: Unifying domain classification, intent detection, and slot filling
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 统一领域分类、意图检测和槽位填充
- en: Some works choose to combine domain classification, intent detection, and slot
    filling into a multitask learning framework to jointly optimize the shared latent
    space. Hakkani-Tür et al. ([2016](#bib.bib120)) applied a bi-directional RNN-LSTM
    architecture to jointly perform three tasks. Liu and Lane ([2016](#bib.bib220))
    augmented the traditional RNN encoder-decoder model with an attention mechanism
    to manage intent detection and slot filling. The slot filling applied explicit
    alignment. Chen et al. ([2016](#bib.bib48)) proposed an end-to-end memory network
    and used a memory module to store user intent and slot values in history utterances.
    Attention was further applied to iteratively select relevant intent and slot values
    at the decoding stage. Multi-task learning of three NLU subtasks contributed to
    the domain scaling and facilitated the zero-shot or few-shot training when transferring
    to a new domain (Bapna et al., [2017](#bib.bib18); Lee and Jha, [2019](#bib.bib186)).
    Zhang et al. ([2018a](#bib.bib445)) captured the hierarchical structure of dialogue
    semantics in NLU multi-task learning by applying a capsule-based neural network.
    With a dynamic routing-by-agreement strategy, the proposed architecture raised
    the accuracy of both intent detection and slot filling on the SNIPS-NLU and ATIS
    dataset.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Novel perspectives
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: More recently, some novel ideas appear in NLU research, which provides new possibilities
    for further improvements. Traditional NLU modules rely on the text converted from
    the audio message of the user using the Automatic Speech Recognition (ASR) module.
    However, Singla et al. ([2020](#bib.bib331)) jumped over the ASR module and directly
    used audio signals as the input of NLU. They found that by reducing the module
    numbers of a pipeline system, the predictions were more robust since fewer errors
    were broadcasted. Su et al. ([2019b](#bib.bib347)) argued that Natural Language
    Understanding (NLU) and Natural Language Generation (NLG) were reversed processes.
    Thus, their dual relationship could be exploited by training with a dual-supervised
    learning framework. The experiments exhibited improvement in both tasks.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Dialogue State Tracking
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dialogue State Tracking (DST) is the first module of a dialogue manager. It
    tracks the user’s goal and related details every turn based on the whole dialogue
    history to provide the information based on which the Policy Learning module (next
    module) decides the agent action to make.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Differences between NLU and DST
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The NLU and DST modules are closely related. Both NLU and DST perform slot
    filling for the dialogue. However, they actually play different roles. The NLU
    module tries to make classifications for the current user message such as the
    intent and domain category as well as the slot each message token belongs to.
    For example, given a user message “Recommend a movie at Golden Village tonight.",
    the NLU module will convert the raw message into “$inform(domain=movie;\ destination=GoldenVillage;\
    date=today;\ time=evening)$", where the slots are usually filled by tagging each
    word of the user message as described in Section [3.1](#S3.SS1 "3.1 Natural Language
    Understanding ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey"). However, the DST module does not
    classify or tag the user message. Instead, it tries to find a slot value for each
    slot name in a pre-existing slot list based on the whole dialogue history. For
    example, there is a pre-existing slot list “$intent:\_;\ domain:\_;\ name:\_;\
    pricerange:\_;\ genre:\_;\ destination:\_;\ date:\_$", where the underscore behind
    the colon is a placeholder denoting that this place can be filled with a value.
    Every turn, the DST module will look up the whole dialogue history up to the current
    turn and decide which content can be filled in a specific slot in the slot list.
    If the user message “Recommend a movie at Golden Village tonight." is the only
    message in a dialogue, then the slot list can be filled as “$intent:inform;\ domain:movie;\
    name:None;\ pricerange:None;\ genre:None;\ destination:GoldenVillage;\ date:today$",
    where the slots unspecified by the user up to current turn can be filled with
    “$None$". To conclude, the NLU module tries to tag the user message while the
    DST module tries to find values from the user message to fill in a pre-existing
    form. Some dialogue systems took the output of the NLU module as the input of
    DST module (Williams et al., [2013](#bib.bib403); Henderson et al., [2014a](#bib.bib133),
    [b](#bib.bib134)), while others directly used raw user messages to track the state (Kim
    et al., [2019](#bib.bib170); Wang et al., [2020e](#bib.bib391); Hu et al., [2020](#bib.bib142)).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 'NLU 和 DST 模块密切相关。NLU 和 DST 都为对话进行槽位填充。然而，它们实际上扮演着不同的角色。NLU 模块尝试对当前用户消息进行分类，如意图和领域类别，以及每个消息标记属于的槽位。例如，给定用户消息“推荐今晚在
    Golden Village 的电影。”，NLU 模块会将原始消息转换为“$inform(domain=movie;\ destination=GoldenVillage;\
    date=today;\ time=evening)$”，其中槽位通常通过标记用户消息中的每个单词来填充，如第 [3.1](#S3.SS1 "3.1 Natural
    Language Understanding ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in
    Deep Learning Based Dialogue Systems: A Systematic Survey") 节所述。然而，DST 模块不会对用户消息进行分类或标记。相反，它会根据整个对话历史为预先存在的槽位列表中的每个槽位名称寻找槽位值。例如，有一个预先存在的槽位列表“$intent:\_;\
    domain:\_;\ name:\_;\ pricerange:\_;\ genre:\_;\ destination:\_;\ date:\_$”，其中冒号后的下划线是占位符，表示此处可以填入一个值。每轮，DST
    模块会查看直到当前轮次的整个对话历史，并决定哪个内容可以填入槽位列表中的特定槽位。如果用户消息“推荐今晚在 Golden Village 的电影。”是对话中的唯一消息，那么槽位列表可以填充为“$intent:inform;\
    domain:movie;\ name:None;\ pricerange:None;\ genre:None;\ destination:GoldenVillage;\
    date:today$”，其中用户当前轮次未指定的槽位可以填充为“$None$”。总之，NLU 模块尝试标记用户消息，而 DST 模块尝试从用户消息中找到值来填充预先存在的表单。一些对话系统将
    NLU 模块的输出作为 DST 模块的输入（Williams et al., [2013](#bib.bib403); Henderson et al.,
    [2014a](#bib.bib133), [b](#bib.bib134)），而其他系统则直接使用原始用户消息来追踪状态（Kim et al., [2019](#bib.bib170);
    Wang et al., [2020e](#bib.bib391); Hu et al., [2020](#bib.bib142)）。'
- en: Dialogue State Tracking Challenges (DSTCs), a series of popular challenges in
    DST, provides benchmark datasets, standard evaluation frameworks, and test-beds
    for research (Williams et al., [2013](#bib.bib403); Henderson et al., [2014a](#bib.bib133),
    [b](#bib.bib134); Kim et al., [2016](#bib.bib168), [2017](#bib.bib169)). The DSTCs
    cover many domains such as restaurants, tourism, etc.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态追踪挑战（DSTCs）是一系列在DST中受欢迎的挑战，提供了基准数据集、标准评估框架和研究测试平台（Williams et al., [2013](#bib.bib403);
    Henderson et al., [2014a](#bib.bib133), [b](#bib.bib134); Kim et al., [2016](#bib.bib168),
    [2017](#bib.bib169)）。DSTCs 涵盖了许多领域，如餐馆、旅游等。
- en: 'A dialogue state contains all essential information to be conveyed in the response (Henderson,
    [2015](#bib.bib131)). As defined in DSTC2 (Henderson et al., [2014a](#bib.bib133)),
    the dialogue state of a given dialogue turn consists of informable slots Sinf
    and requestable slots Sreq. Informable slots are attributes specified by users
    to constrain the search of the database while requestable slots are attributes
    whose values are queried by the user. For example, the serial number of a movie
    ticket is usually a requestable slot because users seldom assign a specific serial
    number when booking a ticket. Specifically, the dialogue state has three components:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 对话状态包含了在响应中传达的所有基本信息（Henderson, [2015](#bib.bib131)）。根据DSTC2（Henderson等, [2014a](#bib.bib133)）的定义，给定对话回合的对话状态包括信息槽位Sinf和请求槽位Sreq。信息槽位是用户指定的属性，用于约束数据库的搜索，而请求槽位是用户查询其值的属性。例如，电影票的序列号通常是请求槽位，因为用户在订票时很少指定特定的序列号。具体来说，对话状态有三个组成部分：
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Goal constraint corresponding with informable slots. The constraints can be
    specific values mentioned by the user in the dialogue or a special value. Special
    values include Dontcare indicating the user’s indifference about the slot and
    None indicating that the user has not specified the value in the conversation
    yet.
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与信息槽位对应的目标约束。约束可以是对话中用户提到的具体值或特殊值。特殊值包括Dontcare，表示用户对该槽位的无所谓，以及None，表示用户尚未在对话中指定该值。
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Requested slots. It can be a list of slot names queried by the user seeking
    answers from the agent.
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请求的槽位。它可以是用户向系统请求答案的槽位名称列表。
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Search method of current turn. It consists of values indicating the interaction
    categories. By constraints denotes that the user tries to specify constraint information
    in his requirement; by alternatives denotes that the user requires an alternative
    entity; finished indicates that the user intends to end the conversation.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前回合的搜索方法。它由指示交互类别的值组成。通过约束表示用户尝试在其需求中指定约束信息；通过替代表示用户要求一个替代实体；完成表示用户打算结束对话。
- en: However, considering the numerous challenges such as tracking efficiency, tracking
    accuracy, domain adaptability, and end-to-end training, many alternative representations
    have been proposed recently, which will be discussed later.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到诸如跟踪效率、跟踪准确性、领域适应性和端到端训练等众多挑战，最近提出了许多替代表示方法，后续将对此进行深入探讨。
- en: '![Refer to caption](img/8fb282b87fcb7d57485f527c8ba737b8.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8fb282b87fcb7d57485f527c8ba737b8.png)'
- en: 'Figure 14: An example of DST procedure (Henderson et al., [2014a](#bib.bib133))'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '图14: DST过程的示例（Henderson等，[2014a](#bib.bib133)）'
- en: 'Figure [14](#S3.F14 "Figure 14 ‣ Differences between NLU and DST ‣ 3.2 Dialogue
    State Tracking ‣ 3 Task-oriented Dialogue Systems ‣ Recent Advances in Deep Learning
    Based Dialogue Systems: A Systematic Survey") is an example of the DST process
    for 4 dialogue turns in a restaurant table booking task. The first column includes
    the raw dialogue utterances, with $S$ denoting the system message and $U$ denoting
    the user message. The second column includes the N-best output lists of the NLU
    module and their corresponding confidence scores. The third column includes the
    labels of a turn, indicating the ground truth slot-value pairs. The fourth column
    includes the example DST outputs and their corresponding confidence scores. The
    fifth column indicates the correctness of the tracker output.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图[14](#S3.F14 "图 14 ‣ NLU 与 DST 的差异 ‣ 3.2 对话状态跟踪 ‣ 3 任务导向对话系统 ‣ 基于深度学习的对话系统的近期进展：系统综述")是一个在餐厅预订任务中4个对话回合的DST过程示例。第一列包括原始对话发言，其中$S$表示系统消息，$U$表示用户消息。第二列包括NLU模块的N-best输出列表及其对应的置信度分数。第三列包括一个回合的标签，指示真实的槽位-值对。第四列包括示例DST输出及其对应的置信度分数。第五列表示跟踪器输出的正确性。
- en: Earlier works use hand-craft rules or statistical methods to solve DST tasks.
    While widely used in industry dialogue systems, rule-based DST methods (Goddeau
    et al., [1996](#bib.bib106)) have many restrictions such as limited generalization,
    high error rate, low domain adaptability, etc (Williams, [2014](#bib.bib406)).
    Statistical methods (Lee, [2013](#bib.bib184); Lee and Eskenazi, [2013](#bib.bib185);
    Ren et al., [2013](#bib.bib297); Williams, [2013](#bib.bib405), [2014](#bib.bib406))
    also suffer from noisy conditions and ambiguity (Young et al., [2010](#bib.bib437)).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Recently, many neural trackers have emerged. Neural trackers have multiple advantages
    over rule-based and statistical trackers. In general, they are categorized into
    two streams. The first stream has predefined slot names and values, and each turn
    the DST module tries to find the most appropriate slot-value pairs based on the
    dialogue history; the second stream does not have a fixed slot value list, so
    the DST module tries to find the values directly from the dialogue context or
    generate values based on the dialogue context. Obviously, the latter one is more
    flexible and in fact, more and more works are solving DST in the second way. We
    discuss the works of both categories here.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Neural trackers with predefined slot names and values
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first stream can be viewed as a multi-class or multi-hop classification
    task. For multi-class classification DST, the tracker predicts the correct class
    from multiple values but this method suffers from high complexity when the value
    set grows large. On the other hand, for the multi-hop classification tasks, the
    tracker reads only one slot-value pair at a time and performs binary prediction.
    Working in this fashion reduces the model complexity but raises the system reaction
    time since for each slot there will be multiple tracking processes. Henderson
    et al. ([2013](#bib.bib132)) was the first who used a deep learning model in the
    DST tasks. They integrated many feature functions (e.g., SLU score, Rank score,
    Affirm score, etc.) as the input of a neural network, then predict the probability
    of each slot-value pair. Mrkšić et al. ([2015](#bib.bib259)) applied an RNN as
    a neural tracker to gain awareness on dialogue context. Mrkšić et al. ([2016](#bib.bib260))
    proposed a multi-hop neural tracker which took the system output and user utterances
    as the first two inputs (to model the dialogue context), and the candidate slot-value
    pairs as the third input. The tracker finally made a binary prediction on the
    current slot-value pair based on the dialogue history.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Neural trackers with unfixed slot names and values
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The second stream attracts more attention because it not only reduces the model
    and time complexity of DST tasks but also facilitates end-to-end training of task-oriented
    dialogue systems. Moreover, it is also flexible when the target domain changes.
    Lei et al. ([2018](#bib.bib188)) proposed belief span, a text span of the dialogue
    context corresponding to a specific slot. They built a two-stage CopyNet to copy
    and store slot values from the dialogue history. The slots were stored to prepare
    for neural response generation. The belief span facilitated the end-to-end training
    of dialogue systems and increased the tracking accuracy in out-of-vocabulary cases.
    Based on this, Lin et al. ([2020c](#bib.bib217)) proposed the minimal belief span
    and argued that it was not scalable to generate belief states from scratch when
    the system interacted with APIs from diverse domains. The proposed MinTL framework
    operated insertion (INS), deletion (DEL) and substitution (SUB) on the dialogue
    state of last turn based on the context and the minimal belief span. Wu et al.
    ([2019a](#bib.bib413)) proposed the TRADE model. The model also applied the copy
    mechanism and used a soft-gated pointer-generator to generate the slot value based
    on the domain-slot pair and encoded dialogue context. Quan and Xiong ([2020](#bib.bib285))
    argued that simply concatenating the dialogue context was not preferable. Alternatively,
    they used [sys] and [usr] to discriminate the system and user messages. This simple
    long context modeling method achieved a 7.03% improvement compared with the baseline.
    Cheng et al. ([2020](#bib.bib49)) proposed Tree Encoder-Decoder (TED) architecture
    which utilized a hierarchical tree structure to represent the dialogue states
    and system acts. The TED generated tree-structured dialogue states of the current
    turn based on the dialogue history, dialogue action, and dialogue state of the
    last turn. This approach led to a 20% improvement on the state-of-the-art DST
    baselines which represented dialogue states and user goals in a flat space. Chen
    et al. ([2020a](#bib.bib41)) built an interactive encoder to exploit the dependencies
    within a turn and between turns. Furthermore, they used the attention mechanism
    to construct the slot-level context for user and system respectively, which were
    embedding vectors based on which the generator copied values from the dialogue
    context. Shan et al. ([2020](#bib.bib317)) applied BERT to perform multi-task
    learning and generated the dialogue state. They first encoded word-level and turn-level
    contexts. Then they retrieved the relevant information for each slot from the
    context by applying both word-level and turn-level attention. Furthermore, the
    slot values were predicted based on the retrieved information. Similarly, Wang
    et al. ([2020e](#bib.bib391)) used BERT for slot value prediction. They performed
    Slot Attention (SA) to retrieve related spans and Value Normalization (VN) to
    convert the spans into final values. Huang et al. ([2020c](#bib.bib150)) proposed
    Meta-Reinforced MultiDomain State Generator (MERET), which was a dialogue state
    generator further finetuned with policy gradient reinforcement learning.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Policy Learning
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Policy learning module is the other module of a dialogue manager. This
    module controls which action will be taken by the system based on the output dialogue
    states from the DST module. Assuming that we have the dialogue state $S_{t}$ of
    the current turn and the action set $A=\{a_{1},...,a_{n}\}$, the task of this
    module is to learn a mapping function $f$: $S_{t}\to a_{i}\in A$. This module
    is comparatively simpler than other modules in the term of task definition but
    actually, the task itself is challenging (Peng et al., [2017](#bib.bib272)). For
    example, in the tasks of movie ticket and restaurant table booking, if the user
    books a two-hour movie slot and intends to go for dinner after that, then the
    agent should be aware that the time gap between movie slot and restaurant slot
    has to be more than two hours since the commuting time from the cinema to the
    restaurant should be considered.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning and reinforcement learning are mainstream training methods
    for dialogue policy learning (Chen et al., [2017a](#bib.bib39)). Policies learned
    in a supervised fashion exhibit great decision-making ability (Su et al., [2016](#bib.bib346);
    Dhingra et al., [2016](#bib.bib71); Williams et al., [2017](#bib.bib408); Liu
    and Lane, [2017](#bib.bib221)). In some specific tasks, the supervised policy
    model can complete tasks precisely, but the training process totally depends on
    the quality of training data. Moreover, the annotated datasets require intensive
    human labor, and the decision ability is restricted by the specific task and domain,
    showing weak transferring capability. With the prevalence of reinforcement learning
    methods, more and more task-oriented dialogue systems use reinforcement learning
    to learn the policy. The dialogue policy learning fits the reinforcement learning
    setting since the agent of reinforcement learning learns a policy to map environment
    states to actions as well.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the environment of reinforce policy learning is a user or a simulated
    user in which setting the training is called online learning. However, it is data-
    and time-consuming to learn a policy from scratch in the online learning scenario,
    so the warm-start method is needed to speed up the training process. Henderson
    et al. ([2008](#bib.bib130)) used expert data to restrict the initial action space
    exploration. Chen et al. ([2017b](#bib.bib42)) applied teacher-student learning
    framework to transfer the teacher expert knowledge to the target network in order
    to warm-start the system.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement policy learning techniques
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Almost all recent dialogue policy learning works are based on reinforcement
    learning methods. Online learning is an ideal approach to get training samples
    iteratively for a reinforcement learning agent, but human labor is very limited.
    Zhang et al. ([2019c](#bib.bib454)) proposed Budget-Conscious Scheduling (BCS)
    to better utilize limited user interactions, where the user interaction is seen
    as the budget. The BCS used a probability scheduler to allocate the budget during
    training. Also, a controller decided whether to use real user interactions or
    simulated ones. Furthermore, a goal-based sampling model was applied to simulate
    the experiences for policy learning. Such a budget-controlling mechanism achieved
    ideal performance in the practical training process. Considering the difficulty
    of getting real online user interactions and the huge amount of annotated data
    required for training user simulators, Takanobu et al. ([2020](#bib.bib356)) proposed
    Multi-Agent Dialog Policy Learning, where they have two agents interacting with
    each other, performing both user and agent, learning the policy simultaneously.
    Furthermore, they incorporated a role-specific reward to facilitate role-based
    response generation. A High task completion rate was observed in experiments.
    Wang et al. ([2020d](#bib.bib387)) introduced Monte Carlo Tree Search with Double-q
    Dueling network (MCTS-DDU), where a decision-time planning was proposed instead
    of background planning. They used the Monte Carlo simulation to perform a tree
    search of the dialogue states. Gordon-Hall et al. ([2020](#bib.bib110)) trained
    expert demonstrators in a weakly supervised fashion to perform Deep Q-learning
    from Demonstrations (DQfD). Furthermore, Reinforced Fine-tune Learning was proposed
    to facilitate domain transfer. In reinforce dialogue policy learning, the agent
    usually receives feedback at the end of the dialogue, which is not efficient for
    learning. Huang et al. ([2020b](#bib.bib149)) proposed an innovative reward learning
    method that constrains the dialogue progress according to the expert demonstration.
    The expert demonstration could either be annotated or not, so the approach was
    not labor intensive. Wang et al. ([2020b](#bib.bib385)) proposed to co-generate
    the dialogue actions and responses to maintain the inherent semantic structures
    of dialogue. Similarly, Le et al. ([2020b](#bib.bib181)) proposed a unified framework
    to simultaneously perform dialogue state tracking, dialogue policy learning, and
    response generation. Experiments showed that unified frameworks have a better
    performance both in their sub-tasks and in their domain adaptability. Xu et al.
    ([2020a](#bib.bib426)) used a knowledge graph to provide prior knowledge of the
    action set and solved policy learning task in a graph-grounded fashion. By combining
    a knowledge graph, a long-term reward was obtained to provide the policy agent
    with a long-term vision while choosing actions. Also, the candidate actions were
    of higher quality due to prior knowledge. The policy learning was further performed
    in a more controllable way.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Natural Language Generation
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 自然语言生成
- en: Natural Language Generation (NLG) is the last module of a task-oriented dialogue
    system pipeline. It manages to convert the dialogue actions generated from the
    dialogue manager into a final natural language representation. E.g., Assuming
    “Inform (name = Wonder Woman; genre = Action; desti = Golden Village)" to be the
    dialogue action from policy learning module, then the NLG module converts it into
    language representations such as “There is an action movie named Wonder Woman
    at Golden Village."
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言生成（NLG）是任务导向对话系统管道的最后一个模块。它负责将从对话管理器生成的对话动作转换为最终的自然语言表示。例如，假设“Inform (name
    = Wonder Woman; genre = Action; desti = Golden Village)”是策略学习模块中的对话动作，那么NLG模块将其转换为诸如“在Golden
    Village有一部名为Wonder Woman的动作片。”的语言表示。
- en: 'Traditional NLG modules are pipeline systems. Defined by Siddharthan ([2001](#bib.bib326)),
    the standard pipeline of NLG consists of four components, as shown in Figure [15](#S3.F15
    "Figure 15 ‣ 3.4 Natural Language Generation ‣ 3 Task-oriented Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey").'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的NLG模块是管道系统。根据Siddharthan ([2001](#bib.bib326))的定义，NLG的标准管道包括四个组件，如图[15](#S3.F15
    "Figure 15 ‣ 3.4 Natural Language Generation ‣ 3 Task-oriented Dialogue Systems
    ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")所示。'
- en: '![Refer to caption](img/0e0556eda06576da7c8989b7bca0ecbc.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0e0556eda06576da7c8989b7bca0ecbc.png)'
- en: 'Figure 15: The pipeline NLG system'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：管道NLG系统
- en: 'The core modules of this pipeline are Content Determination, Sentence Planning,
    and Surface Realization, as proposed by Reiter ([1994](#bib.bib296)). Cahill et al.
    ([1999](#bib.bib33)) further improved the NLG pipeline by adding three more components:
    lexicalization, referring expression generation, and aggregation. However, this
    model has a drawback that the input of the system is ambiguous.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这个管道的核心模块是内容确定、句子规划和表层实现，如Reiter ([1994](#bib.bib296))提出的那样。Cahill等人 ([1999](#bib.bib33))
    进一步改进了NLG管道，增加了三个组件：词汇化、指代表达生成和聚合。然而，这个模型的一个缺点是系统的输入存在歧义。
- en: End-to-end NLG techniques
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端到端的NLG技术
- en: Deep learning methods were further applied to enhance the NLG performance and
    the pipeline is collapsed into a single module. End-to-end natural language generation
    has achieved promising improvements and is the most popular way to perform NLG
    in recent work. Wen et al. ([2015a](#bib.bib397)) argued that language generation
    should be fully data-driven and not depend on any expert rules. They proposed
    a statistical language model based on RNNs to learn response generation with semantic
    constraints and grammar trees. Additionally, they used a CNN reranker to further
    select better responses. Similarly, an LSTM model was used by Wen et al. ([2015b](#bib.bib398))
    to learn sentence planning and surface realization simultaneously. Tran and Nguyen
    ([2017](#bib.bib372)) further improved the generation quality on multiple domains
    using GRU. The proposed generator consistently generated high-quality responses
    on multiple domains. To improve the domain adaptability of recurrent models, Wen
    et al. ([2016b](#bib.bib400)) proposed to first train the recurrent language model
    on data synthesized from out-of-domain datasets, then finetune on a comparatively
    smaller in-domain dataset. This training strategy was proved effective in human
    evaluation. Context-awareness is important in dialogue response generation because
    only depending on the dialogue action of the current turn may cause illogical
    responses. Zhou et al. ([2016](#bib.bib461)) built an attention-based Context-Aware
    LSTM (CA-LSTM) combining target user questions, all semantic values, and dialogue
    actions as input to generate context-aware responses in QA. Likewise, Dušek and
    Jurčíček ([2016a](#bib.bib77)) concatenated the preceding user utterance with
    the dialogue action vector and fed it into an LSTM model. Dušek and Jurčíček ([2016b](#bib.bib78))
    put a syntax constraint upon their neural response generator. A two-stage sequence
    generation process was proposed. First, a syntax dependency tree was generated
    to have a structured representation of the dialogue utterance. The generator in
    the second stage integrated sentence planning and surface realization and produced
    natural language representations.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: Robust natural language generation
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: More recent works have focused on the reliability and quality of generated responses.
    A tree-structured semantic representation was proposed by Balakrishnan et al.
    ([2019](#bib.bib14)) to achieve better content planning and surface realization
    performance. They further designed a novel beam search algorithm to improve the
    semantic correctness of the generated response. To avoid mistakes such as slot
    value missing or redundancy in generated responses, Li et al. ([2020d](#bib.bib207))
    proposed Iterative Rectification Network (IRN), a framework trained with supervised
    learning and finetuned with reinforcement learning. It iteratively rectified generated
    tokens by incorporating slot inconsistency penalty into its reward. Golovanov
    et al. ([2019](#bib.bib107)) applied large-scale pretrained models for NLG tasks.
    After comparing single-input and multi-input methods, they concluded that different
    types of input context will cause different inductive biases in generated responses
    and further proposed to utilize this characteristic to better adapt a pretrained
    model to a new task. Baheti et al. ([2020](#bib.bib13)) solved NLG reliability
    problem in conversational QA. Though with different pipeline structures, they
    used similar methods to increase the fluency and semantic correctness of the generated
    response. They proposed Syntactic Transformations (STs) to generate candidate
    responses and used a BERT to rank their qualities. These generated responses can
    be viewed as an augmentation of the original dataset to be further used in NLG
    model learning. Oraby et al. ([2019](#bib.bib264)) proposed a method to create
    datasets with rich style markups from easily available user reviews. They further
    trained multiple NLG models based on generated data to perform joint control of
    semantic correctness and language style. Similarly, Elder et al. ([2020](#bib.bib79))
    put forward a data augmentation approach which put a restriction on response generation.
    Though this restriction caused dull and less diverse responses, they argued that
    in task-oriented systems, reliability was more important than diversity.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 End-to-end Methods
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The modules discussed above can achieve good performance in their respective
    tasks, with the help of recent relevant advances. However, there exist two significant
    drawbacks in modular systems (Zhao and Eskenazi, [2016](#bib.bib455)): (1) Modules
    in many pipeline systems are sometimes not differentiable, which means that errors
    from the end are not able to be propagated back to each module. In real dialogue
    systems training, usually the only signal is the user response, while other supervised
    signals like dialogue states and dialogue actions are scarce. (2) Though the modules
    jointly contribute to the success of a dialogue system, the improvement of one
    module may not necessarily raise the response accuracy or quality of the whole
    system. This causes additional training of other modules, which is labor intensive
    and time-consuming. Additionally, due to the handcrafted features in pipeline
    task-oriented systems such as dialogue states, it is usually hard to transfer
    modular systems to another domain, since the predefined ontologies require modification.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 上述模块在各自的任务中可以通过最近的相关进展取得良好的性能。然而，模块化系统存在两个显著缺点（Zhao 和 Eskenazi，[2016](#bib.bib455)）：（1）许多管道系统中的模块有时不可微分，这意味着错误无法从末端传播回每个模块。在实际对话系统训练中，通常只有用户响应这一信号，而对话状态和对话动作等其他监督信号较少。（2）尽管模块共同促进了对话系统的成功，但一个模块的改进不一定会提高整个系统的响应准确性或质量。这会导致其他模块需要额外训练，这既费力又耗时。此外，由于管道任务导向系统中的手工特征，如对话状态，通常很难将模块化系统迁移到另一个领域，因为预定义的本体需要修改。
- en: There exist two main methods for the end-to-end training of task-oriented dialogue
    systems. One is to make each module of a pipeline system differentiable, then
    the whole pipeline can be viewed as a large differentiable system and the parameters
    can be optimized by back-propagation in an end-to-end fashion (Le et al., [2020b](#bib.bib181)).
    Another way is to use only one end-to-end module to perform both knowledge base
    retrieval and response generation, which is usually a multi-task learning neural
    model.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 任务导向对话系统的端到端训练存在两种主要方法。一种是使管道系统的每个模块都可微分，然后整个管道可以视为一个大型可微分系统，参数可以通过反向传播以端到端的方式进行优化（Le
    等，[2020b](#bib.bib181)）。另一种方法是使用一个端到端模块同时执行知识库检索和响应生成，这通常是一个多任务学习神经模型。
- en: End-to-end trainable pipeline TOD
  id: totrans-332
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 端到端可训练管道 TOD
- en: 'The increasing applications of neural models have made it possible for modules
    to be differentiable. While many modules are easily differentiable, there remains
    one task that makes differentiation challenging: the knowledge base query. Many
    task-oriented dialogue systems require an external knowledge source to retrieve
    related knowledge facts required by the user. For example, in the restaurant table
    booking task, the knowledge fact can be an available slot of one specific restaurant.
    Traditional methods use a symbolic query to match entries based on their attributes.
    The system performs semantic parsing on the user message to represent a symbolic
    query according to the user goal (Li et al., [2017b](#bib.bib201); Williams and
    Zweig, [2016](#bib.bib407); Wen et al., [2016c](#bib.bib401)). However, this retrieval
    process is not differentiable, which prevents the whole framework from being end-to-end
    trainable. With the application of key-value memory networks (Miller et al., [2016](#bib.bib250)), Eric
    and Manning ([2017](#bib.bib81)) used the key-value retrieval mechanism to retrieve
    relevant facts. The proposed architecture was augmented with the attention mechanism
    to compute the relevance between utterance representations of dialogue and key
    representations of the knowledge base. Dhingra et al. ([2016](#bib.bib71)) presented
    a soft retrieval mechanism that uses a “soft" posterior distribution over the
    knowledge base to replace the symbolic queries. They further combined this soft
    retrieval mechanism into a reinforcement learning framework to achieve complete
    end-to-end training based on user feedback. Williams et al. ([2017](#bib.bib408))
    proposed Hybrid Code Networks (HCNs), which encoded domain-specific knowledge
    into software and system action templates, achieving the differentiability of
    the knowledge retrieval module. They did not explicitly model the dialogue states
    but instead learned the latent representation and optimized the HCN using supervised
    learning and reinforcement learning jointly. Ham et al. ([2020](#bib.bib121))
    used GPT-2 to form a neural pipeline and perform domain prediction, dialogue state
    tracking, policy learning, knowledge retrieval, and response generation in a pipeline
    fashion. The system could easily interact with external systems because it outputs
    explicit intermediate results from each module and thus being interpretable. Likewise, Hosseini-Asl
    et al. ([2020](#bib.bib141)) built a neural pipeline with GPT-2 and explicitly
    generated results for each neural module as well.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end trainable single module TOD
  id: totrans-334
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: More recent works tend not to build their end-to-end systems in a pipeline fashion.
    Instead, they use complex neural models to implicitly represent the key functions
    and integrate the modules into one. Research in task-oriented end-to-end neural
    models focuses either on training methods or model architecture, which are the
    keys to response correctness and quality. Wang et al. ([2019a](#bib.bib388)) proposed
    an incremental learning framework to train their end-to-end task-oriented system.
    The main idea is to build an uncertainty estimation module to evaluate the confidence
    of appropriate responses generated. If the confidence score was higher than a
    threshold, then the response would be accepted, while a human response would be
    introduced if the confidence score was low. The agent could also learn from human
    responses using online learning. Dai et al. ([2020](#bib.bib61)) used model-agnostic
    meta-learning (MAML) to improve the adaptability and reliability jointly with
    only a handful of training samples in a real-life online service task. Similarly, Qian
    and Yu ([2019](#bib.bib280)) also trained the end-to-end neural model using MAML
    to facilitate the domain adaptation, which enables the model to first train on
    rich-resource tasks and then on new tasks with limited data. Lin et al. ([2020c](#bib.bib217))
    proposed Minimalist Transfer Learning (MinTL) to plug-and-play large-scale pretrained
    models for domain transfer in dialogue task completion. To maintain the sequential
    correctness of generated responses, Wu et al. ([2019b](#bib.bib415)) trained an
    inconsistent order detection module in an unsupervised fashion. This module detected
    whether an utterance pair is ordered or not to guide the task-completing agent
    towards generating more coherent responses. He et al. ([2020a](#bib.bib128)) proposed
    a “Two-Teacher One-Student" training framework. At the first stage, the two teacher
    models were trained in a reinforcement learning framework, with the objective
    of retrieving knowledge facts and generating human-like responses respectively.
    Then at the second stage, the student network was forced to mimic the output of
    the teacher networks. Thus, the expert knowledge of the two teacher networks was
    transferred to the student network. Balakrishnan et al. ([2019](#bib.bib14)) introduced
    a constrained decoding method to improve the semantic correctness of the responses
    generated by the proposed end-to-end system. Many end-to-end task-oriented systems
    used a memory module to store relevant knowledge facts and dialogue history. Chen
    et al. ([2019c](#bib.bib45)) argued that a single memory module was not enough
    for precise retrieval. They used two long-term memory modules to store the knowledge
    tuples and dialogue history respectively, and then a working memory was applied
    to control the token generation. Zhang et al. ([2020](#bib.bib452)) proposed LAtent
    BElief State (LABES) model, which treated the dialogue states as discrete latent
    variables to reduce the reliance on turn-level DST labels. To solve the data insufficiency
    problem in some tasks, Gao et al. ([2020a](#bib.bib99)) augmented the response
    generation model with a paraphrase model in their end-to-end system. The paraphrase
    model was jointly trained with the whole framework and it aimed to augment the
    training samples. Yang et al. ([2020](#bib.bib429)) leveraged the graph structure
    information of both a knowledge graph and the dialogue context-dependency tree.
    They proposed a recurrent cell architecture to learn representations on the graph
    and performed multi-hop reasoning to exploit the entity links in the knowledge
    graph. With the augmentation of graph information, consistent improvement was
    achieved on two task-oriented datasets.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Research Challenges and Hot Topics
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we review recent works in task-oriented dialogue systems and
    point out the frequently studied topics to provide some important research directions.
    This section can be seen as an augmentation of the literature review in previous
    sections discussing techniques developed for each module, and focuses more on
    some specific problems to be solved in the current research community.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1 Pretrained Models for NLU
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Natural Language Understanding task converts the user message into a predefined
    format of semantic slots. A popular way to perform NLU is by finetuning large-scale
    pretrained language models. Wu and Xiong ([2020](#bib.bib412)) compared many pretrained
    language models including BERT-based and GPT-based systems in three subtasks of
    task-oriented dialogue systems - domain identification, intent detection, and
    slot tagging. This empirical paper is aimed to provide insights and guidelines
    in pretrained model selection and application for related research. Wu et al.
    ([2020a](#bib.bib414)) pretrained TOD-BERT and outperformed strong baselines in
    the intent detection task. The model proposed also had a strong few-shot learning
    ability to alleviate the data insufficiency problem. Coope et al. ([2020](#bib.bib58))
    proposed Span-ConveRT, which was a pretrained model designed for slot filling
    task. It viewed the slot filling task as a turn-based span extraction problem
    and also performed well in the few-shot learning scenario.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Domain Transfer for NLU
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another challenge or hot topic in NLU research is the domain transfer problem,
    which is also the key issue of task-oriented dialogue systems. Hakkani-Tür et al.
    ([2016](#bib.bib120)) built an RNN-LSTM architecture for multitask learning of
    domain classification, intent detection, and slot-filling problem. Training samples
    from multiple domains were combined in a single model where respective domain
    data reinforces each other. Bapna et al. ([2017](#bib.bib18)) used a multi-task
    learning framework to leverage slot name encoding and slot description encoding,
    thus implicitly aligning the slot-filling model across domains. Likewise, Lee
    and Jha ([2019](#bib.bib186)) also applied slot description to exploit the similar
    semantic concepts between slots of different domains, which solved the sub-optimal
    concept alignment and long training time problems encountered in past works involving
    multi-domain slot-filling.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.3 Domain Transfer for DST
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Domain adaptability is also a significant topic for dialogue state trackers.
    The domain transfer in DST is challenging due to three main reasons (Ren et al.,
    [2018](#bib.bib298)): (1) Slot values in ontologies are different when the domain
    changes, which accounts for the incompatibility of models. (2) When the domain
    changes, the slot number will also change, causing different numbers of model
    parameters. (3) Hand-crafted lexicons make it difficult for generalization over
    domains. Mrkšić et al. ([2015](#bib.bib259)) used delexicalized n-gram features
    to solve the domain incompatibility problem by replacing all specified slot names
    and values with generic symbols. Lin et al. ([2020c](#bib.bib217)) introduced
    Levenshtein belief spans (Lev), which were short context spans relating to the
    user message. Different from previous methods which generated dialogue state from
    scratch, they performed substitution (SUB), deletion (DEL), and insertion (INS)
    based on past states to alleviate the dependency on annotated in-domain training
    samples. Huang et al. ([2020c](#bib.bib150)) applied model-agnostic meta-learning
    (MAML) to first learn on several source domains and then adapt on the target domain,
    while Campagna et al. ([2020](#bib.bib34)) improved the zero-shot transfer learning
    by synthesizing in-domain data using an abstract conversation model and the domain
    ontology. Ouyang et al. ([2020](#bib.bib266)) modeled explicit slot connections
    to exploit the existing slots appearing in other domains. Thus, the tracker could
    copy slot values from the connected slots directly, alleviating the burden of
    reasoning and learning. Wang et al. ([2020e](#bib.bib391)) proposed Value Normalization
    (VN) to convert supporting dialogue spans into state values and could achieve
    high accuracy with only 30% available ontology.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.4 Tracking Efficiency for DST
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tracking efficiency is another hot topic in dialogue state tracking challenges.
    Usually, there are multiple states within a dialogue, so how to compute the slot
    values without any redundant steps becomes very significant when attempting to
    reduce the reaction time of a system. Kim et al. ([2019](#bib.bib170)) argued
    that predicting the dialogue state from scratch at every turn was not efficient.
    They proposed to first predict the operations to be taken on each of the slots
    (i.e., Carryover, Delete, Dontcare, Update), and then perform respective operations
    as predicted. Ouyang et al. ([2020](#bib.bib266)) used a slot connection mechanism
    to directly copy slot values from the source slot, which reduced the expense of
    reasoning. Hu et al. ([2020](#bib.bib142)) and Wang et al. ([2020e](#bib.bib391))
    proposed slot attention to calculate the relations between the slot and dialogue
    context, thus only focusing on the relevant slots at each turn.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.5 Training Environment for PL
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The environment of the Policy Learning framework has been a long-existing problem.
    Li et al. ([2017b](#bib.bib201)) built a user simulator to model the user feedback
    as the reward signal of an environment. They modeled a stack-like user agenda
    to iteratively change the user goal and thus shifting the dialogue states. While
    using a user simulator for environment modeling seems to be promising for that
    it involves less human interaction, Zhang et al. ([2019c](#bib.bib454)) argued
    that training a user simulator required a large amount of annotated data. Takanobu
    et al. ([2020](#bib.bib356)) proposed Multi-Agent Dialog Policy Learning, where
    they have two agents interact with each other, performing both user and agent,
    learning policy simultaneously. Furthermore, they incorporated a role-specific
    reward to facilitate role-based response generation and here both agents also
    acted as the environment of the other one.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.6 Response Consistency for NLG
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Response consistency in NLG is a challenging problem since it cannot be solved
    by simply augmenting the training samples. Instead, additional corrections or
    regulations should be designed. Wen et al. ([2015b](#bib.bib398)) proposed the
    Semantically Controlled LSTM (SC-LSTM) which used a semantic planning gate to
    control the retention or abandonment of dialogue actions thus ensuring the response
    consistency. Likewise, Tran and Nguyen ([2017](#bib.bib372)) also applied a gating
    mechanism to jointly perform sentence planning and surface realization where dialogue
    action features were gated before entering GRU cells. Li et al. ([2020d](#bib.bib207))
    proposed Iterative Rectification Network (IRN), which combined a slot inconsistency
    reward into the reinforcement learning framework. Thus, the model iteratively
    checked the correctness of slots and corresponding values.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.7 End-to-end Task-oriented Dialogue Systems
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'End-to-end systems are usually fully data-driven, which contributes to their
    robust and natural responses. However, because of the finiteness of annotated
    training samples, a hot research topic is figuring out how to increase the response
    quality of end-to-end task-oriented dialogue systems with limited data. Using
    rule-based methods to constrain response generation is a way to improve response
    quality. Balakrishnan et al. ([2019](#bib.bib14)) used linearized tree-structured
    representation as input to obtain control over discourse-level and sentence-level
    semantic concepts. Kale and Rastogi ([2020](#bib.bib162)) used templates to improve
    the semantic correctness of generated responses. They broke down the response
    generation into a two-stage process: first generating semantically correct but
    possibly incoherent responses based on the slots, with the constraint of templates;
    then in the second stage, pretrained language models were applied to re-organize
    the generated utterances into coherent ones. Training the network with reinforcement
    learning was another strategy to alleviate the reliance on annotated data. He
    et al. ([2020a](#bib.bib128)) trained two teacher networks using a reinforcement
    learning framework with the objectives of knowledge retrieval and response generation
    respectively. Then the student network learns to produce responses by mimicking
    the output of teacher networks. Training the network in a supervised way, Dai
    et al. ([2020](#bib.bib61)) alternatively tried to optimize the learning strategy
    to improve the learning efficiency of models given limited data. They combined
    the meta-learning algorithm with human-machine interaction and achieved significant
    improvement compared with strong baselines not trained with the meta-learning
    algorithms. A more direct way to solve the data finiteness problem in supervised
    learning was augmenting the dataset (Elder et al., [2020](#bib.bib79)), which
    also improved the response quality to some extent. Additionally, pretraining large-scale
    models on common corpus and then applying them in a domain that lacks annotated
    data is a popular approach in recent years (Henderson et al., [2019b](#bib.bib136);
    Mehri et al., [2019](#bib.bib244); Bao et al., [2019b](#bib.bib17)).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.8 Retrieval Methods for Task-oriented Dialogue Systems
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Retrieval-based methods are rare in task-oriented systems for the insufficiency
    of candidate entries to cover all possible responses which usually involve specific
    knowledge from external knowledge-base. However, Henderson et al. ([2019b](#bib.bib136))
    argued that in some situations not relating with specific knowledge facts, retrieval-based
    methods were more precise and effective. They first pretrained the response selection
    model on general domain corpora and then finetuned on small target domain data.
    Experiments on six datasets from different domains proved the effectiveness of
    the pretrained response selection model. Lu et al. ([2019b](#bib.bib231)) constructed
    Spatio-temporal context features to facilitate response selection, and achieved
    significant improvements on the Ubuntu IRC dataset.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 4 Open-Domain Dialogue Systems
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section discusses open-domain dialogue systems, which are also called chit-chat
    dialogue systems or non-task-oriented dialogue systems. Almost all state-of-the-art
    open-domain dialogue systems are based on neural methods. We organize this section
    by first briefly introducing the concepts of different branches of open-domain
    dialogue systems, and then we focus on different research challenges and hot topics.
    We view these challenges and hot topics as different research directions in open-domain
    dialogue systems.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of managing to complete tasks, open-domain dialogue systems aim to
    perform chit-chat with users without the task and domain restriction (Ritter et al.,
    [2011](#bib.bib300)) and are usually fully data-driven. Open-domain dialogue systems
    are generally divided into three categories: generative systems, retrieval-based
    systems, and ensemble systems. Generative systems apply sequence-to-sequence models
    to map the user message and dialogue history into a response sequence that may
    not appear in the training corpus. By contrast, retrieval-based systems try to
    find a pre-existing response from a certain response set. Ensemble systems combine
    generative methods and retrieval-based methods in two ways: retrieved responses
    can be compared with generated responses to choose the best among them; generative
    models can also be used to refine the retrieved responses (Zhu et al., [2018](#bib.bib466);
    Song et al., [2016](#bib.bib337); Qiu et al., [2017](#bib.bib284); Serban et al.,
    [2017b](#bib.bib314)). Generative systems can produce flexible and dialogue context-related
    responses while sometimes they lack coherence and tend to make dull responses.
    Retrieval-based systems select responses from human response sets and thus are
    able to achieve better coherence in surface-level language. However, retrieval
    systems are restricted by the finiteness of the response sets and sometimes the
    responses retrieved show a weak correlation with the dialogue context (Zhu et al.,
    [2018](#bib.bib466)).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: In the next few subsections, we discuss some research challenges and hot topics
    in open-domain dialogue systems. We aim to to help researchers quickly grasp the
    current research trends via a systematic discussion on articles solving certain
    problems.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Context Awareness
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dialogue context consists of user and system messages and is an important source
    of information for dialogue agents to generate responses because dialogue context
    decides the conversation topic and user goal (Serban et al., [2017a](#bib.bib313)).
    A context-aware dialogue agent responds not only depending on the current message
    but also based on the conversation history. The earlier deep learning-based systems
    added up all word representations in dialogue history or used a fixed-size window
    to focus on the recent context (Sordoni et al., [2015b](#bib.bib340); Li et al.,
    [2015](#bib.bib191)). Serban et al. ([2016](#bib.bib312)) proposed Hierarchical
    Recurrent Encoder-Decoder (HRED), which was ground-breaking in building context-awareness
    dialogue systems. They built a word-level encoder to encode utterances and a turn-level
    encoder to further summarize and deliver the topic information over past turns.
    Xing et al. ([2018](#bib.bib424)) augmented the hierarchical neural networks with
    the attention mechanism to help the model focus on more meaningful parts of dialogue
    history.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: 'Both generative and retrieval-based systems rely heavily on dialogue context
    modeling. Shen et al. ([2019](#bib.bib321)) proposed Conversational Semantic Relationship
    RNN (CSRR) to model the dialogue context in three levels: utterance-level, pair-level,
    and discourse-level, capturing content information, user-system topic, and global
    topic respectively. Zhang et al. ([2019a](#bib.bib446)) argued that the hierarchical
    encoder-decoder does not lay enough emphasis on certain parts when the decoder
    interacted with dialogue contexts. Also, they claimed that attention-based HRED
    models also suffered from position bias and relevance assumption insufficiency
    problems. Therefore, they proposed ReCoSa, whose architecture was inspired by
    the transformer. The model first used a word-level LSTM to encode dialogue contexts,
    and then self-attention was applied to update the utterance representations. In
    the final stage, an encoder-decoder attention was computed to facilitate the response
    generation process. Additionally, Mehri et al. ([2019](#bib.bib244)) examined
    several applications of large-scale pretrained models in dialogue context learning,
    providing guidance for large-scale network selection in context modeling.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Some works propose structured attention to improve context-awareness. Qiu et al.
    ([2020](#bib.bib283)) learned structured dialogue context by combining structured
    attention with a Variational Recurrent Neural Network (VRNN). Comparatively, Ferracane
    et al. ([2019](#bib.bib90)) examined the RST discourse tree model proposed by Liu
    and Lapata ([2018](#bib.bib226)) and observed little or even no discourse structures
    in the learned latent tree. Thus, they argued that structured attention did not
    benefit dialogue modeling and sometimes might even harm the performance.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, Feng et al. ([2020b](#bib.bib88)) not only utilized dialogue
    history, but also future conversations. Considering that in real inference situations
    dialogue agents cannot be explicitly aware of future information, they first trained
    a scenario-based model jointly on past and future context and then used an imitation
    framework to transfer the scenario knowledge to a target network.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Better context modeling improves the response selection performance in retrieval-based
    dialogue systems (Jia et al., [2020](#bib.bib158)). Tao et al. ([2019](#bib.bib364))
    proposed Interaction-over-Interaction network (IoI), which consisted of multiple
    interaction blocks to perform deeper interactions between dialogue context and
    candidate responses. Jia et al. ([2020](#bib.bib158)) organized the dialogue history
    into conversation threads by performing classifications on their dependency relations.
    They further used a pretrained Transformer model to encode the threads and candidate
    responses to compute the matching score. Lin et al. ([2020b](#bib.bib216)) argued
    that response-retrieval datasets should not only be annotated with relevant or
    irrelevant responses. Instead, a greyscale metric should be used to measure the
    relevance degree of a response given the dialogue context, thus increasing the
    context-awareness ability of retrieval models.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: Dialogue rewriting problem aims to convert several messages into a single message
    conveying the same information and dialogue context awareness is very crucial
    to this task (Xu et al., [2020b](#bib.bib427)). Su et al. ([2019a](#bib.bib343))
    modeled multi-turn dialogues via dialogue rewriting and benefited from the conciseness
    of rewritten utterances.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Response Coherence
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coherence is one of the qualities that a good generator seeks (Stent et al.,
    [2005](#bib.bib342)). Coherence means maintaining logic and consistency in a dialogue,
    which is essential in an interaction process for that a response with weak consistency
    in logic and grammar is hard to understand. Coherence is a hot topic in generative
    systems but not in retrieval-based systems because candidate responses in retrieval
    methods are usually human responses, which are naturally coherent.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Refining the order or granularity of sentence functions is a popular strategy
    for improving the language coherence. Wu et al. ([2019b](#bib.bib415)) improved
    the response coherence via the task of inconsistent order detection. The dialogue
    systems learned response generation and order detection jointly, which was self-supervised
    multi-task learning. Xu et al. ([2019](#bib.bib425)) presented the concept of
    meta-words. Meta-words were diverse attributes describing the response. Learning
    dialogue based on meta-words helped promote response generation in a more controllable
    way. Liu et al. ([2019](#bib.bib222)) used three granularities of encoders to
    encode raw words, low-level clusters, and high-level clusters. The architecture
    was called Vocabulary Pyramid Network (VPN), which performed a multi-pass encoding
    and decoding process on hierarchical vocabularies to generate coherent responses.
    Shen et al. ([2019](#bib.bib321)) also built a three-level hierarchical dialogue
    model to capture richer features and improved the response quality. Ji et al.
    ([2020](#bib.bib155)) built Cross Copy Networks (CCN), which used a copy mechanism
    to copy from similar dialogues based on the current dialogue context. Thus, the
    system benefited from the pre-existing coherent responses, which alleviated the
    need of performing the reasoning process from scratch.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Many work employ strategies to achieve response coherence on a higher level,
    which improves the overall quality of the generated responses. Li et al. ([2019b](#bib.bib199))
    improved the logical consistency of generated utterances by incorporating an unlikelihood
    loss to control the distribution mismatches. Bao et al. ([2019a](#bib.bib16))
    proposed a Generation-Evaluation framework that evaluated the qualities, including
    coherence, of the generated response. The feedback was further seen as a reward
    signal in the reinforcement learning framework and guided to a better dialogue
    strategy via policy gradient, thus improving the response quality. Gao et al.
    ([2020b](#bib.bib101)) raised response quality by ranking generated responses
    based on user feedbacks like upvotes, downvotes, and comments on social networks.
    Zhu et al. ([2018](#bib.bib466)) built a retrieval-enhanced generation model,
    which enhanced the generated responses in two ways. First, a discriminator was
    trained with the help of a retrieval system, and then the generator was trained
    in a GAN framework under the supervision signal of a discriminator. Second, retrieved
    responses were also used as a part of the generator input to provide a coherent
    example for the generator. Xu et al. ([2020a](#bib.bib426)) achieved a global
    coherent dialogue by constructing a knowledge graph from corpora. They further
    performed graph walks to decide “what to say" and “how to say", thus improving
    the dialogue flow coherence. Mesgar et al. ([2019](#bib.bib245)) proposed an assessment
    approach for dialogue coherence evaluation by combining the dialogue act prediction
    in a multi-task learning framework and learned rich dialogue representations.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: There also evolve some data-wise methods for better response coherence. Bi et al.
    ([2019](#bib.bib22)) proposed to annotate sentence functions in existing conversation
    datasets to improve the sentence logic and coherence of generated responses. Akama
    et al. ([2020](#bib.bib3)) focused on data effectiveness as well. They filtered
    out low-quality utterance pairs by scoring the relatedness and connectivity, which
    was proved to be effective in improving the response coherence. Akama et al. ([2020](#bib.bib3))
    presented a method for evaluating dataset utterance pairs’ quality in terms of
    connectedness and relatedness. The proposed scoring technique is based on research
    findings that have been widely disseminated in the conversation and linguistics
    communities. Lison and Bibauw ([2017](#bib.bib219)) included a weighting model
    in their neural architecture. The weighting model, which is based on conversation
    data, assigns a numerical weight to each training sample that reflects its intrinsic
    quality for dialogue modeling and achieved good result in experiments.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Response Diversity
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Bland and generic response is a long-existing problem in generative dialogue
    systems. Because of the high frequency of generic responses like “I don’t know"
    in training samples and the beam search decoding scheme of neural sequence-to-sequence
    models, generative dialogue systems tend to respond with universally acceptable
    but meaningless utterances (Serban et al., [2016](#bib.bib312); Vinyals and Le,
    [2015](#bib.bib379); Sordoni et al., [2015b](#bib.bib340)). For example, to respond
    to the user message “I really want to have a meal", the agent tends to choose
    simple responses like “It’s OK" instead of responding with more complicated sentences
    like recommendations and suggestions.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'Earlier works solve this challenge by modifying the decoding objective or adding
    a reranking process. Li et al. ([2015](#bib.bib191)) replaced the traditional
    likelihood objective $p(R|C)$ with mutual information. The optimization of mutual
    information objective aims to achieve a Maximum Mutual Information (MMI). Specifically,
    the task is to find a best response $R$ based on the dialogue context $C$, in
    order to maximize their mutual information:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\hat{R}&amp;=arg\max_{R}{log\frac{P(C,R)}{P(C)P(R)}}\\ &amp;=arg\max_{R}{logP(R&#124;C)-logP(R)}\end{split}$
    |  | (49) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: 'The objective $p(R|C)$ causes the model to choose responses with high probability
    even if the response is unconditionally frequent in the dataset, thus causing
    it to ignore the content of $C$. Maximizing the mutual information as Equation
    ([49](#S4.E49 "In 4.3 Response Diversity ‣ 4 Open-Domain Dialogue Systems ‣ Recent
    Advances in Deep Learning Based Dialogue Systems: A Systematic Survey")) solves
    this issue by achieving a trade-off between safety and relativity.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: With a similar intuition as described above, increasing response diversity by
    modifying the decoding scheme at inference time has been explored in earlier works. Vijayakumar
    et al. ([2016](#bib.bib378)) combined a dissimilarity term into the beam search
    objective and proposed Diverse Beam Search (DBS) to promote diversity. Similarly, Shao
    et al. ([2017](#bib.bib319)) proposed a stochastic beam search algorithm by performing
    stochastic sampling when choosing top-B responses. In the beam search algorithm,
    siblings sharing the same parent nodes tended to guide to similar sequences. Inspired
    by this, Li et al. ([2016c](#bib.bib194)) penalized siblings sharing the same
    parent nodes using an additional term in the beam search objective. This encouraged
    the algorithm to search more diverse paths by expanding from different parent
    nodes. Some works further added a reranking stage to select more diverse responses
    in the generated N-best list (Li et al., [2015](#bib.bib191); Sordoni et al.,
    [2015b](#bib.bib340); Shao et al., [2017](#bib.bib319)).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: A user message can be mapped into multiple acceptable responses, which is also
    known as the one-to-many mapping problem. Qiu et al. ([2019](#bib.bib282)) considered
    the one-to-many mapping problem in open-domain dialogue systems and proposed a
    two-stage generation model to increase response diversity - the first stage extracting
    common features of multiple ground truth responses and the second stage extracting
    the distinctive ones. Ko et al. ([2020](#bib.bib171)) solved the one-to-many mapping
    problem via a classification task to learn latent semantic representations. So
    that given one example response, different ones could be generated by exploring
    the semantically close vectors in the latent space.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Different training strategies have been proposed to increase response diversity.
    Bao et al. ([2019a](#bib.bib16)) used human instinct or pre-defined objective
    as a reward signal in a reinforcement learning setting to prompt the agent to
    avoid generating dull responses. Still, in a reinforcement learning framework, Zhu
    et al. ([2020](#bib.bib467)) performed counterfactual reasoning to explore the
    potential response space. Given a pre-existing response, the model inferred another
    policy, which represented another possible response, thus increasing the response
    diversity. He and Glass ([2019](#bib.bib127)) used a negative training method
    to minimize the generation of bland responses. They first collected negative samples
    and then gave negative training signals based on these samples to fine-tune the
    model, impeding the model to generate bland responses. To achieve a better performance, Du
    and Black ([2019](#bib.bib76)) synthesized different dialogue models designed
    for response diversity based on boosting training. The ensemble model significantly
    outperformed each of its base models.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing external knowledge sources is another way to improve the diversity
    of generated responses because it can enrich the content. Wu et al. ([2020b](#bib.bib416))
    built a common-sense dialogue generation model which seeks highly related knowledge
    facts based on the dialogue history. Likewise, Su et al. ([2020](#bib.bib344))
    incorporated external knowledge sources to diversify the response generation,
    but the difference was that they utilized non-conversational texts like news articles
    as relevant knowledge facts, which were obviously easier to obtain. Tian et al.
    ([2019](#bib.bib369)) used a memory module to abstract and store useful information
    in the training corpus for generating diverse responses.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to diversify the response generation is to make modifications
    to the training corpus. Csáky et al. ([2019](#bib.bib59)) solved the challenge
    by filtering out the generic responses in the dataset using an entropy-based algorithm,
    which was simple but effective. Augmented with human feedback data, Gao et al.
    ([2020b](#bib.bib101)) proposed that the generated responses could be reranked
    via a response ranking framework trained on the human feedback data and responses
    with higher quality including diversity were selected. Stasaski et al. ([2020](#bib.bib341))
    proposed to change the data collection pipeline by iteratively computing the diversity
    of responses from different human participants in dataset construction and selected
    those participants who tend to generate informative and diverse responses.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Speaker Consistency and Personality-based Response
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In open-domain dialogue systems, one big issue is that the responses are entirely
    learned from training data. The inconsistent response may be received when asking
    the system about some personal facts (e.g., age, hobbies). If the dataset contains
    multiple utterance pairs about the query of age, then the response generated tends
    to be shifting, which is unacceptable because personal facts are usually not random.
    Thus, for a data-driven chatbot, it is necessary to be aware of its role and respond
    based on a fixed persona.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly modeling the persona is the main strategy in recent works. Liu et al.
    ([2020b](#bib.bib225)) proposed a persona-based dialogue generator consisting
    of a Receiver and a Transmitter. The receiver was responsible for modeling the
    interlocutor’s persona through several turns’ chat while Transmitter generated
    utterances based on the persona of agent and interlocutor, together with conversation
    content. The proposed model supported conversations between two persona-based
    chatbots by modeling each other’s persona. Without training with additional Natural
    Language Inference labels, Kim et al. ([2020](#bib.bib167)) built an imaginary
    listener following a normal generator, which reasoned over the tokens generated
    by the generator and predicted a posterior distribution over the personas in a
    certain space. After that, a self-conscious speaker generated tokens aligned with
    the predicted persona. Likewise, Boyd et al. ([2020](#bib.bib28)) used an augmented
    GPT-2 to reason over the past conversations and model the target actor’s persona,
    conditioning on which persona consistency was achieved.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Responding with personas needs to condition on some persona descriptions. For
    example, to build a generous agent, descriptions like “I am a generous person"
    are needed as a part of the model input. However, these descriptions require hand-crafted
    feature design, which is labor intensive. Madotto et al. ([2019](#bib.bib237))
    proposed to use Model-Agnostic Meta-Learning (MAML) to adapt to new personas with
    only a few training samples and needed no persona description. Majumder et al.
    ([2020a](#bib.bib238)) relied on external knowledge sources to expand current
    persona descriptions so that richer persona descriptions were obtained, and the
    model could associate current descriptions with some commonsense facts.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Song et al. ([2020a](#bib.bib335)) argued that traditional persona-based systems
    were one-stage systems and the responses they generated still contain many persona
    inconsistent words. To tackle this issue, they proposed a three-stage architecture
    to ensure persona consistency. A generate-delete-rewrite mechanism was implemented
    to remove the unacceptable words generated in prototype responses and rewrite
    them.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Empathetic Response
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Empathy means being able to sense other people’s feelings (Ma et al., [2020b](#bib.bib236)).
    An empathetic dialogue system can sense the user’s emotional changes and produce
    appropriate responses with a certain sentiment. This is an essential topic in
    chit-chat systems because it directly affects the user’s feeling and to some extent
    decides the response quality. Industry systems such as Microsoft’s Cortana, Facebook
    M, Google Assistant, and Amazon’s Alexa are all equipped with empathy modules (Wang
    et al., [2020g](#bib.bib395)).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two ways to generate utterances with emotion: one is to use explicit
    sentiment words as a part of input; another is to implicitly combine neural words (Song
    et al., [2019](#bib.bib338)). Song et al. ([2019](#bib.bib338)) proposed a unified
    framework that uses a lexicon-based attention to explicitly plugin emotional words
    and a sequence-level emotion classifier to classify the output sequence, implicitly
    guiding the generator to generate emotional responses through backpropagation.
    Zhong et al. ([2020](#bib.bib460)) used CoBERT for persona-based empathetic response
    selection and further investigated the impact of persona on empathetic responses.
    Smith et al. ([2020](#bib.bib333)) blended the skills of being knowledgeable,
    empathetic, and role-aware in one open-domain conversation model and overcame
    the bias issue when blending these skills.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Since the available datasets for empathetic conversations are scarce, Rashkin
    et al. ([2018](#bib.bib291)) provided a new benchmark and dataset for empathetic
    dialogue systems. Oraby et al. ([2019](#bib.bib264)) constructed a dialogue dataset
    with rich emotional markups from user reviews and further proposed a novel way
    to generate similar datasets with rich markups.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Controllable Generation
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Controllable dialogue generation is an important line of work in open-domain
    dialogue systems since solely learning from data sample distributions causes many
    uncertain responses. Some of the dialogue systems are grounded on some external
    knowledge such as knowledge graph and documents. However, grounding alone without
    explicit control and semantic targeting may induce output that is accurate but
    vague.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: We may get some inspirations from the prior work on language generation and
    machine translation since similarly to dialogue systems they are generation-based
    or seq-to-seq problems. Some related work aimed to enforce user-specified constraints,
    most notably using lexical constraints (Hokamp and Liu, [2017](#bib.bib139); Hu
    et al., [2019](#bib.bib143); Miao et al., [2019](#bib.bib248)). These methods
    exclusively use constraints at inference time. Constraints can be included into
    the latent space during training, resulting in better predictions. Other studies
    (See et al., [2019](#bib.bib311); Keskar et al., [2019](#bib.bib165); Tang et al.,
    [2019](#bib.bib362)) have looked at non-lexical constraints, but they haven’t
    looked into how they can help with grounding external knowledge. These publications
    also assume that the system can always be given (gold) constraints, which limits
    the ability to demonstrate larger benefits of the approaches.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Controllable text generation has also been used to extract high-level style
    information from contextual information in text style transfer (Hu et al., [2017](#bib.bib144))
    and other tasks (Ficler and Goldberg, [2017](#bib.bib91); Dong et al., [2017](#bib.bib74);
    Gao et al., [2019](#bib.bib100)), allowing the former to be independently modified.
    Zhao et al. ([2018](#bib.bib457)) learns an interpretable representation for dialogue
    systems using discrete latent actions. While existing studies employ “style" descriptors
    (e.g., positive/negative, formal/informal) as control signals, Wu et al. ([2020c](#bib.bib419))
    use specific lexical constraints to regulate creation, allowing for finer semantic
    control. Content planned generation (Wiseman et al., [2017](#bib.bib411); Hua
    and Wang, [2019](#bib.bib145)) focuses response generation on a small number of
    essential words or table entries. This line of work, on the other hand, does not
    require consideration of the discourse context, which is critical for response
    generation.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Conversation Topic
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Daily chats of people usually involve a topic or goal. Actually, a topic or
    goal is the key to keep each participant engaged in conversations and thus being
    essential to a chatbot. In real applications, a good topic model helps to retrieve
    related knowledge and guide the conversation instead of passively responding to
    the user’s message (Xing et al., [2017](#bib.bib423)). For example, if the user
    mentions “I like sunny days", a topic-aware system may reason over relevant external
    knowledge and produce responses like “I know there is a nice park near the seaside,
    have you ever been there before?". Thus, the agent pushes the conversation to
    a more engaging stage and enriches the dialogue content.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Almost all topic-aware dialogue agents need to model explicit topics, which
    can be entities from external knowledge-base, or topic embeddings that have some
    semantic meaning. Wu et al. ([2019c](#bib.bib417)) tried to change the traditional
    passive response fashion and radically pursue active guidance of conversation.
    The dialogue agent consists of a leader and a follower, where the leader reasons
    over a knowledge graph and decides the conversation topic. Likewise, a common-sense
    knowledge graph was used by Liu et al. ([2020c](#bib.bib227)) to lead the conversation
    topic and make recommendations. Tang et al. ([2019](#bib.bib362)) built a topic-aware
    retrieval-based chatbot. It aimed to guide the conversation topic to the target
    one step by step. It used a keyword predictor to predict turn-level keywords and
    selected the discourse-level keyword based on that. The discourse-level keyword
    was further fed into the retrieval model to retrieve responses regarding a certain
    topic. Chen and Yang ([2020](#bib.bib40)) built a multi-view sequence-to-sequence
    model to learn dialogue topics by first extracting dialogue structures of unstructured
    chit-chat dialogues, then generating topic summaries using BART decoder.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: In some applications of certain scenarios the conversation topic is essential,
    and these are where the topic-aware dialogue agents can be applied to. Zhang and
    Danescu-Niculescu-Mizil ([2020](#bib.bib448)) studied the topic-aware chatbot
    in counseling conversations. In counseling conversations, the agent led the dialogue
    topic by deciding between empathetically addressing a situation within the current
    range and moving on to a new target resolution. Cao et al. ([2019](#bib.bib35))
    studied chatbots in the psychotherapy treatment area and built a topic prediction
    model to forecast the behavior codes for upcoming conversations, thus guiding
    the dialogue.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 4.8 Knowledge-Grounded System
  id: totrans-397
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: External knowledge such as common-sense knowledge is a significant source of
    information when organizing an utterance. Humans associate current conversation
    context with their experiences and memories and produce meaningful related responses,
    such capability results in the gap between human and machine chit-chat systems.
    As discussed, the earlier chit-chat systems are simply variants of machine translation
    systems, which can be viewed as sequence-to-sequence language models. However,
    dialogue generation is much more complicated than machine translation because
    of the higher freedom and vaguer constraints. Thus, chit-chat systems cannot simply
    consist of a sequence-to-sequence mapping since appropriate and informative responses
    are always related to some external common-sense knowledge. Instead, there must
    be a module incorporating world knowledge.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'Many researchers devoted their research efforts to building knowledge-grounded
    dialogue systems. A representative model is memory networks introduced in Section [2.4](#S2.SS4
    "2.4 Memory Networks ‣ 2 Neural Models in Dialogue Systems ‣ Recent Advances in
    Deep Learning Based Dialogue Systems: A Systematic Survey"). Knowledge grounded
    systems use Memory Networks to store external knowledge and the generator retrieves
    relevant knowledge facts from it at the generation stage (Ghazvininejad et al.,
    [2018](#bib.bib104); Vougiouklis et al., [2016](#bib.bib381); Yin et al., [2015](#bib.bib435)). Tian
    et al. ([2019](#bib.bib369)) built a memory-augmented conversation model. The
    proposed model abstracted from the training samples and stored useful ones in
    the memory module. Zhao et al. ([2020b](#bib.bib459)) built a knowledge-grounded
    dialogue generation system based on GPT-2\. They combined a knowledge selection
    module into the language model and learned knowledge selection and response generation
    simultaneously. Lin et al. ([2020a](#bib.bib214)) proposed Knowledge-Interaction
    and knowledge Copy (KIC). They performed recurrent knowledge interactions during
    the decoding phase to compute an attention distribution over the memory. Then
    they performed knowledge copy using a knowledge-aware pointer network to copy
    knowledge words according to the attention distribution computed.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: Documents contain large amount of knowledge facts, but they have a drawback
    that they are usually too long to retrieve useful information from (Li et al.,
    [2019d](#bib.bib208)). Li et al. ([2019d](#bib.bib208)) built a multi-turn document-grounded
    system. They used an incremental transformer to encode multi-turns’ dialogue context
    and respective documents retrieved. In the generation phase, they designed a two-stage
    generation scheme. The first stage took dialogue context as input and generated
    coherent responses; the second stage utilized both the utterance from the first
    stage and the document retrieved for the current turn for response generation.
    In this case, selecting knowledge based on both dialogue context and generated
    response was called posterior knowledge selection, while selecting knowledge with
    only dialogue context was called prior knowledge selection, which only utilized
    prior information. Wang et al. ([2020c](#bib.bib386)) built a document quotation
    model in online conversations and investigated the consistency between quoted
    sentences and latent dialogue topics.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge graph is another source of external information, which is becoming
    more and more popular in knowledge-grounded systems because of their structured
    nature. Jung et al. ([2020](#bib.bib160)) proposed a dialogue-conditioned graph
    traversal model for knowledge-grounded dialogue systems. The proposed model leveraged
    attention flows of two directions and fully made use of the structured information
    of knowledge graph to flexibly decide the expanding range of nodes and edges.
    Likewise, Zhang et al. ([2019b](#bib.bib447)) applied graph attention to traverse
    the concept space, which was a common-sense knowledge graph. The graph attention
    helped to move to more meaningful nodes conditioning on dialogue context. Xu et al.
    ([2020a](#bib.bib426)) applied knowledge graphs as an external source to control
    a coarse-level utterance generation. Thus, the conversation was supported by common-sense
    knowledge, and the agent guided the dialogue topic in a more reasonable way. Moon
    et al. ([2019](#bib.bib257)) built a retrieval system retrieving responses based
    on the graph reasoning task. They used a graph walker to traverse the graph conditioning
    on symbolic transitions of the dialogue context. Huang et al. ([2020a](#bib.bib147))
    proposed Graph-enhanced Representations for Automatic Dialogue Evaluation (GRADE),
    a novel evaluation metric for open-domain dialogue systems. This metric considered
    both contextualized representations and topic-level graph representations. The
    main idea was to use an external knowledge graph to model the conversation logic
    flow as a part of the evaluation criteria.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge-grounded datasets containing context-knowledge-response triples are
    scarce and hard to obtain. Cho and May ([2020](#bib.bib50)) collected a large
    dataset consisting of more than 26000 turns of improvised dialogues which were
    further grounded with a larger movie corpus as external knowledge. Also tackling
    the data insufficiency problem, Li et al. ([2020b](#bib.bib197)) proposed a method
    that did not require context-knowledge-response triples for training and was thus
    data-efficient. They viewed knowledge as a latent variable to bridge the context
    and response. The variational approach learned the parameters of the generator
    from both a knowledge corpus and a dialogue corpus which were independent of each
    other.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 4.9 Interactive Training
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactive training, also called human-in-loop training, is a unique training
    method for dialogue systems. Annotated data is fixed and limited, not being able
    to cover all dialogue settings. Also, it takes a long time to train a good system.
    But in some industrial products, the dialogue systems need not be perfect when
    accomplishing their tasks. Thus, interactive training is desirable because the
    dialogue systems can improve themselves via interactions with users anywhere and
    anytime, which is a more flexible and cheap way to finetune the parameters.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Training schemes with the above intuition have been developed in recent years. Li
    et al. ([2016a](#bib.bib192)) introduced a reinforcement learning-based online
    learning framework. The agent interacted with a human dialogue partner and the
    partner provided feedback as a reward signal. Asghar et al. ([2016](#bib.bib8))
    first trained the agent with two-stage supervised learning, and then used an interaction-based
    reinforcement learning to finetune. Every time the user chose the best one from
    K responses generated by the pretrained model and then responded to this selected
    response. Instead of learning through being passively graded, Li et al. ([2016b](#bib.bib193))
    proposed a model that actively asked questions to seek improvement. Active learning
    was applicable to both offline and online learning settings. Hancock et al. ([2019](#bib.bib123))
    argued that most conversation samples an agent saw happened after it was pretrained
    and deployed. Thus, they proposed a framework to train the agent from the real
    conversations it participated in. The agent evaluated the satisfaction score of
    the user from the user’s response of each turn and explicitly requested the user
    feedback when it thought that a mistake has been made. The user feedback was further
    used for learning. Bouchacourt and Baroni ([2019](#bib.bib27)) placed the interactive
    learning in a cooperative game and tried to learn a long-term implicit strategy
    via Reinforce algorithm. Some of these work has been adopted by industry products
    and is a very promising direction for study.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 4.10 Visual Dialogue
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More and more researchers cast their eyes to a broader space and are not only
    restricted to NLP. The combination of CV and NLP giving rise to tasks like visual
    question answering attracted lots of interest. The VQA task is to answer a question
    based on the content of a picture or video. Recently, this has evolved into a
    more challenging task: visual dialogue, which conditions a dialogue on the visual
    information and dialogue history. The dialogue consists of a series of queries,
    and the query form is usually more informal, which is why it is more complicated
    than VQA.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ddf47cfe252263bfc7c80cf645895937.png)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The architecture of VD-BERT, a state-of-the-art visual dialogue
    system (Wang et al., [2020f](#bib.bib392))'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a9dbdb2a0130c37be56b263a00a91347.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Three samples from the IMAGE-CHAT dataset (Shuster et al., [2020](#bib.bib325))'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual dialogue can be seen as a multi-step reasoning process over a series
    of questions (Gan et al., [2019](#bib.bib95)). Gan et al. ([2019](#bib.bib95))
    learned semantic representation of the question based on dialogue history and
    a given image, and recurrently updated the representation. Shuster et al. ([2019](#bib.bib324))
    proposed a set of image-based tasks and provided strong baselines. Wang et al.
    ([2020f](#bib.bib392)) employed R-CNN as an image encoder and fused the visual
    and dialogue modality with a VD-BERT. The proposed architecture achieved sufficient
    interactions between multi-turn dialogue and images. The proposed architecture
    is shown as an example model for Visual Dialogue tasks in Figure [16](#S4.F16
    "Figure 16 ‣ 4.10 Visual Dialogue ‣ 4 Open-Domain Dialogue Systems ‣ Recent Advances
    in Deep Learning Based Dialogue Systems: A Systematic Survey").'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Compared with image-grounded dialogue systems, video-grounded systems are more
    interesting but also more challenging. There are two main challenges of video
    dialogue, as claimed by Le and Hoi ([2020](#bib.bib178)). One is that both spatial
    and temporal features exist in the video, which increases the difficulty of feature
    extraction. Another is that video dialogue features span across multiple conversation
    turns and thus are more complicated. A GPT-2 model was applied by Le and Hoi ([2020](#bib.bib178)),
    being able to fuse multi-modality information over different levels. Likewise, Le
    et al. ([2019](#bib.bib179)) built a multi-modal transformer network to incorporate
    information from different modalities and further applied a query-aware attention
    to extract context-related features from non-text modalities. Le et al. ([2020a](#bib.bib180))
    proposed a Bi-directional Spatio-Temporal Learning (BiST) leveraging temporal-to-spatial
    and spatial-to-temporal reasoning process and could adapt to the dynamically evolving
    semantics in the video.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers hold different opinions on the effectiveness of dialogue history
    in visual dialogue. Takmaz et al. ([2020](#bib.bib357)) proposed that many expressions
    were already mentioned in previous turns and they built a visual dialogue model
    grounded on both image and conversation history. They further proved that better
    performance was achieved when grounding the model on dialogue context. However, Agarwal
    et al. ([2020](#bib.bib1)) argued that though with dialogue history the visual
    dialogue model could achieve better results, in fact only a small proportion of
    cases benefited from the history. Furthermore, they proved that existing evaluation
    metrics for visual dialogue promoted generic responses.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: The visual dialogue task benefits a lot from the pretraining-based learning.
    The popularity of NLP pretraining sparked interest in multi-modal pretraining.
    VideoBERT (Sun et al., [2019b](#bib.bib351)) is widely recognized as the pioneering
    work in the field of multimodal pretraining. It’s a model that’s been pre-trained
    on video frame features and text. CBT (Sun et al., [2019a](#bib.bib350)), which
    is similarly pretrained on video-text pairs, is a contemporary work of VideoBERT.
    For video representation learning, Miech et al. ([2020](#bib.bib249)) used unlabeled
    narrated films. More researchers have focused their attention on visual-linguistic
    pretraining, inspired by the early work in multi-modal pretraining. For this objective,
    there are primarily two types of model designs. The single-stream model (Alberti
    et al., [2019](#bib.bib4); Chen et al., [2019d](#bib.bib47); Gan et al., [2020](#bib.bib96);
    Li et al., [2020a](#bib.bib190), [2019a](#bib.bib198), [c](#bib.bib204); Su et al.,
    [2019c](#bib.bib348); Zhou et al., [2020b](#bib.bib464)) is one example. (Li et al.,
    [2020a](#bib.bib190)) used a BERT model to process the concatenation of objects
    and words and pre-trained it with three standard tasks. Similar methods were proposed
    by Chen et al. ([2019d](#bib.bib47)) and Qi et al. ([2020](#bib.bib279)), but
    with more pretraining tasks and larger datasets. With an adversarial training
    technique, Gan et al. ([2020](#bib.bib96)) further enhanced the model. Su et al.
    ([2019c](#bib.bib348)) employed the same architecture, but incorporated single-modal
    data and pre-trained the object detector. Instead of using recognized objects,
    Huang et al. ([2020d](#bib.bib151)) sought to enter pixels directly. The object
    labels were used by Li et al. ([2020c](#bib.bib204)) to improve cross-modal alignment.
    Zhou et al. ([2020b](#bib.bib464)) suggested a single-stream model that learns
    both caption generation and VQA tasks at the same time. The two-stream model (Lu
    et al., [2019a](#bib.bib230), [2020](#bib.bib232); Tan and Bansal, [2019](#bib.bib359);
    Yu et al., [2020](#bib.bib440)) is another type of model architecture. Tan and
    Bansal ([2019](#bib.bib359)) suggested a two-stream model with co-attention and
    solely used in-domain data to train the model. Lu et al. ([2019a](#bib.bib230))
    introduced a similar architecture with a more complex co-attention model, which
    they pretrained with out-of-domain data, and Lu et al. ([2020](#bib.bib232)) improved
    VilBERT with multi-task learning. Yu et al. ([2020](#bib.bib440)) recently added
    the scene graph to the model, which improved performance. Aside from these studies,
    Singh et al. ([2020](#bib.bib329)) looked at the impact of pretraining dataset
    selection on downstream task performance.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 'The annotation of visual dialogue is laborious and thus the datasets are scarce.
    Recently, some researchers have tried to tackle the data insufficiency problem.
    Shuster et al. ([2020](#bib.bib325)) collected a dataset (IMAGE-CHAT, shown in
    Figure [17](#S4.F17 "Figure 17 ‣ 4.10 Visual Dialogue ‣ 4 Open-Domain Dialogue
    Systems ‣ Recent Advances in Deep Learning Based Dialogue Systems: A Systematic
    Survey")) of image-grounded human-human conversations in which speakers are asked
    to perform role-playing based on an emotional mood or style offered, since the
    usage of such characteristics is also a significant factor in engagingness. Kamezawa
    et al. ([2020](#bib.bib163)) constructed a visual-grounded dialogue dataset. Interestingly,
    it additionally annotated the eye-gaze locations of the interlocutor in the image
    to provide information on what the interlocutor was paying attention to. Cogswell
    et al. ([2020](#bib.bib56)) proposed a method to utilize the VQA data when adapting
    to a new task, minimizing the requirement of dialogue data which is expensive
    to annotate.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluation Approaches
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation is an essential part of research in dialogue systems. It is not only
    a way to assess the performance of agents, but it can also be a part of the learning
    framework which provides signals to facilitate the learning (Bao et al., [2019a](#bib.bib16)).
    This section discusses the evaluation methods in task-oriented and open-domain
    dialogue systems.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Evaluation Methods for Task-oriented Dialogue Systems
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Task-oriented systems aim to accomplish tasks and thus have more direct metrics
    evaluating their performance such as task completion rate and task completion
    cost. Some evaluation methods also involve metrics like BLEU to compare system
    responses with human responses, which will be discussed later. In addition, human-based
    evaluation and user simulators are able to provide real conversation samples.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Task Completion Rate is the rate of successful events in all task completion
    attempts. It measures the task completion ability of a dialogue system. For example,
    in movie ticket booking tasks, the Task Completion Rate is the fraction of dialogues
    that meet all requirements specified by the user, such as movie time, cinema location,
    movie genre, etc. The task completion rate was applied in many task-oriented dialogue
    systems (Walker et al., [1997](#bib.bib382); Williams, [2007](#bib.bib404); Peng
    et al., [2017](#bib.bib272)). Additionally, some works (Singh et al., [2002](#bib.bib330);
    Yih et al., [2015](#bib.bib434)) used partial success rate.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Task Completion Cost is the resources required when completing a task. Time
    efficiency is a significant metric belonging to Task Completion Cost. In dialogue-related
    tasks, the number of conversation turns is usually used to measure the time efficiency
    and dialogue with fewer turns is preferred when accomplishing the same task.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Human-based Evaluation provides user dialogues and user satisfaction scores
    for system evaluation. There are two main streams of human-based evaluation. One
    is to recruit human labor via crowdsourcing platforms to test-use a dialogue system.
    The crowdsource workers converse with the dialogue systems about predefined tasks
    and then metrics like Task Completion Rate and Task Completion Cost can be calculated.
    Another is computing the evaluation metrics in real user interactions, which means
    that evaluation is done after the system is deployed in real use.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: User Simulator provides simulated user dialogues based on pre-defined rules
    or models. Since recruiting human labor is expensive and real user interactions
    are not available until a mature system is deployed, user simulators are able
    to provide task-oriented dialogues at a lower cost. There are two kinds of user
    simulators. One is agenda-based simulators (Schatzmann and Young, [2009](#bib.bib309);
    Li et al., [2016e](#bib.bib200); Ultes et al., [2017](#bib.bib375)), which only
    feed dialogue systems with the pre-defined user goal as a user message, without
    surface realization. Another is model-based simulators (Chandramohan et al., [2011](#bib.bib37);
    Asri et al., [2016](#bib.bib9)), which generate user utterances using language
    models given constraint information.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evaluation Methods for Open-domain Dialogue Systems
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluation of open-domain dialogue systems has long been a challenging problem.
    Unlike task-oriented systems, there is no clear metric like task completion rate
    or task completion cost. Both human and automatic evaluation methods are developed
    for ODD during these years. Human evaluation has been adopted by many works (Ritter
    et al., [2011](#bib.bib300); Shang et al., [2015](#bib.bib318); Sordoni et al.,
    [2015b](#bib.bib340)) to converse with and rate dialogue agents. However, human
    evaluation is not an ideal approach for that human labor is expensive and the
    evaluation results are highly subjective, varying from person to person. Researchers
    tend to hire crowd source workers (Ritter et al., [2011](#bib.bib300); Shang et al.,
    [2015](#bib.bib318); Sordoni et al., [2015b](#bib.bib340)) or random people (Moon
    et al., [2019](#bib.bib257); Jung et al., [2020](#bib.bib160)) to conduct human
    evaluation, both of which have two main drawbacks: 1\. The evaluator group is
    highly random, and there exists huge gap between people with different knowledge
    levels or from different domains. 2\. Though individual bias could be weakened
    by increasing the number of evaluators, the evaluator group cannot be very large
    because of the limited budgets (in articles mentioned above the sizes of human
    evaluator groups are usually 5-20). Thus, automatic and objective metrics are
    desirable. In general, there are two categories of automatic metrics in recent
    research: word-overlap metrics and neural metrics.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Word-overlap Metrics are widely used in Machine Translation and Summarization
    tasks, which calculate the similarity between the generated sequence and the ground
    truth sequence. Representative metrics like BLEU (Papineni et al., [2002](#bib.bib269))
    and ROUGE (Lin, [2004](#bib.bib210)) are n-gram matching metrics. METEOR (Banerjee
    and Lavie, [2005](#bib.bib15)) was further proposed with an improvement based
    on BLEU. It identified the paraphrases and synonyms between the generated sequence
    and the ground truth. Galley et al. ([2015](#bib.bib94)) extended the BLEU by
    exploiting numerical ratings of responses. Liu et al. ([2016](#bib.bib223)) argued
    that word-overlap metrics were not correlated well with human evaluation. These
    metrics are effective in Machine Translation because each source sentence has
    a ground truth to compare with, whereas in dialogues there may be many possible
    responses corresponding with one user message, and thus an acceptable response
    may receive a low score if simply computing word-overlap metrics.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Neural Metrics are metrics computed by neural models. Neural methods improve
    the evaluation effectiveness in terms of adaptability compared with word-overlap
    metrics, but they require an additional training process. Su et al. ([2015](#bib.bib345))
    used an RNN and a CNN model to extract turn-level features in a sequence and give
    the score. Tao et al. ([2018](#bib.bib363)) proposed Ruber, which was an automatic
    metric combining referenced and unreferenced components. The referenced one computed
    the similarity between generated response representations and ground truth representations,
    while the unreferenced one learned a scoring model to rate the query-response
    pairs. Lowe et al. ([2017](#bib.bib229)) learned representations of dialogue utterances
    using an RNN and then computed the dot-product between generated response and
    ground truth response as an evaluation score. Kannan and Vinyals ([2017](#bib.bib164))
    and Bruni and Fernandez ([2017](#bib.bib29)) used the discriminator of a GAN framework
    to distinguish the generated responses from human responses. If a generated response
    achieved a high confidence score, this was indicative of a human-like response,
    thus desirable.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation of open-domain dialogue systems is a hot topic at present and many
    researchers cast their eyes on this task recently. Some papers introduce two or
    more custom evaluation metrics for better evaluation, such as response diversity,
    response consistency, naturalness, knowledgeability, understandability, etc.,
    to study "what to evaluate". Bao et al. ([2019a](#bib.bib16)) evaluated the generated
    responses by designing two metrics. One was the informativeness metric calculating
    information utilization over turns. Another was the coherence metric, which was
    predicted by GRUs, given the response, context, and background as input. Likewise, Akama
    et al. ([2020](#bib.bib3)) designed scoring functions to compute connectivity
    of utterance pairs and content relatedness as two evaluation metrics and used
    another fusion function to combine the metrics. Pang et al. ([2020](#bib.bib268))
    combined four metrics in their automatic evaluation framework: the context coherence
    metric based on GPT-2; phrase fluency metric based on GPT-2; diversity metric
    based on n-grams; logical self-consistency metric based on textual-entailment-inference.
    Mehri and Eskenazi ([2020](#bib.bib243)) proposed a reference-free evaluation
    metric. They annotated responses considering the following qualities: Understandable
    (0-1), Maintains Context (1-3), Natural (1-3), Uses Knowledge (0-1), Interesting
    (1-3), Overall Quality (1-5). Furthermore, a transformer was trained on these
    annotated dialogues to compute the score of quality.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: Apart from "what to evaluate", there are also a multitude of papers studying
    "how to evaluate", which focus more on refining the evaluation process. Liang
    et al. ([2020](#bib.bib209)) proposed a three-stage framework to denoise the self-rating
    process. They first performed dialogue flow anomaly detection via self-supervised
    representation learning, and then the model was fine-tuned with smoothed self-reported
    user ratings. Finally, they performed a denoising procedure by calculating the
    Shapley value and removed the samples with negative values. Zhao et al. ([2020a](#bib.bib458))
    trained RoBERTa as a response scorer to achieve reference-free and semi-supervised
    evaluation. Sato et al. ([2020](#bib.bib308)) constructed a test set by first
    generating several responses based on one user message and then human evaluation
    was performed to annotate each response with a score, where the response with
    the highest score was taken as a true response and the remainder taken as false
    responses. Dialogue systems were further evaluated by comparing the response selection
    accuracy on the test set, where a cross-entropy loss was calculated between the
    generated response and candidate responses to perform the selection operation.
    Likewise, Sinha et al. ([2020](#bib.bib332)) trained a BERT-based model to discriminate
    between true and false responses, where false responses were automatically generated.
    The model was further used to predict the evaluation score of a response based
    on dialogue context. Huang et al. ([2020a](#bib.bib147)) argued that responses
    should not be simply evaluated based on their surface-level features, and instead
    the topic-level features were more essential. They incorporated a common-sense
    graph in their evaluation framework to obtain topic-level graph representations.
    The topic-level graph representation and utterance-level representation were jointly
    considered to evaluate the coherence of responses generated by open-domain dialogue
    systems.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Ranking is also an approach that evaluates dialogue systems effectively. Gao
    et al. ([2020b](#bib.bib101)) leveraged large-scale human feedback data such as
    upvotes, downvotes, and replies to learn a GPT-2-based response ranker. Thus,
    responses were evaluated by their rankings given by the ranker. Deriu et al. ([2020](#bib.bib69))
    also evaluated the dialogue systems by ranking. They proposed a low-cost human-involved
    evaluation framework, in which different conversational agents conversed with
    each other and the human’s responsibility was to annotate whether the generated
    utterance was human-like or not. The systems were evaluated by comparing the number
    of turns their responses were judged as human-like responses.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: 6 Datasets
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset is one of the most essential components in dialogue systems study.
    Nowadays the datasets are not enough no matter for task-oriented or open-domain
    dialogue systems, especially for those tasks requiring additional annotations
    (Novikova et al., [2017](#bib.bib263)). For task-oriented dialogue systems, data
    can be collected via two main methods. One is to recruit human labor via crowdsourcing
    platforms to produce dialogues in a given task. Another is to collect dialogues
    in real task completions like film ticket booking. For open-domain dialogue systems,
    apart from dialogues collected in real interactions, social media is also a significant
    source of data. Some social media companies such as Twitter and Reddit provide
    API access to a small proportion of posts, but these services are restricted by
    many legal terms which affect the reproducibility of research. As a result, many
    recent works in dialogue systems collect their own datasets for train and test.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we review and categorize these datasets and make a comprehensive
    summary. To our best knowledge, Table LABEL:Datasets_for_Task-oriented_dialogue_systems
    and LABEL:Datasets_for_Open-domain_dialogue_systems cover almost all available
    datasets used in recent task-oriented or open-domain dialogue systems.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Datasets for Task-oriented Dialogue Systems
  id: totrans-435
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: Datasets for Task-oriented dialogue systems'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| Name | Description | Task | Origin |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| Schema | A dataset mainly for dialogue state tracking. | Dialogue State Tracking
    | Rastogi et al. ([2020](#bib.bib292)) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| MetaLWOZ | Collected by crowdsourcing platforms, spanning over 227 tasks
    and 47 domains. This dataset is designed for learning in unseen domains. | Domain
    Transfer | Lee et al. ([2019](#bib.bib187)) |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| E2E | A dataset for end-to-end dialogue generation in restaurant domain.
    Data is collected in crowdsourced fashion. | End-to-end Task-oriented Dialogue
    Systems | Novikova et al. ([2017](#bib.bib263)) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| MSR-E2E | Contain dialogues spanning over 3 domains: movie-ticket booking,
    restaurant reservation, and taxi booking. | End-to-end Task-oriented Dialogue
    Systems | Li et al. ([2018](#bib.bib202)) |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| YELPNLG | A corpus consisting of utterances spanning over different restaurant
    attributes. | Natural Language Generation | Oraby et al. ([2019](#bib.bib264))
    |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| Clinical Conversation data set | It consists of conversations between physicians
    and participants. | Natural Language Understanding | Du et al. ([2019](#bib.bib75))
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| OOS | A large-scale dataset for intent detection. | Natural Language Understanding
    | Larson et al. ([2019](#bib.bib177)) |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| ATIS | A dataset consisting of voice calls from people who intend to make
    flight reservations. | Natural Language Understanding; Dialogue State Tracking
    | Tur et al. ([2010](#bib.bib373)) |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| MultiWOZ | Human-human written conversations with rich annotations spanning
    over multi-domains. | Task-oriented Dialogue | Budzianowski et al. ([2018](#bib.bib30))
    |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| SNIPS-NLU | Task-oriented dialogue dataset colleted in a crowdsourced fashion.
    It was used to train voice assistant agents. | Task-oriented Dialogue | [https://github.com/snipsco/nlubenchmark](https://github.com/snipsco/nlubenchmark)
    |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: '| bAbI | Restaurant table reservation dialogues. | Task-oriented Dialogue |
    Bordes et al. ([2016](#bib.bib25)) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| JDC | A Chinese customer service dataset, consisting of context-response
    pairs. | Task-oriented Dialogue | [https://www.jddc.jd.com](https://www.jddc.jd.com)
    |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| UbuntuV2 | It consists of dialogues collected via Ubuntu question-answering
    forum. | Task-oriented Dialogue | Lowe et al. ([2015](#bib.bib228)) |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| MICROSOFT DIALOGUE CHALLENGE data set | A task-oriented dataset collected
    via Amazon Mechanical Turk. | Task-oriented Dialogue | Li et al. ([2018](#bib.bib202))
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| WOZ | Task-oriented data collected in crowdsourced fashion. | Task-oriented
    Dialogue | Wen et al. ([2016c](#bib.bib401)) |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| DSTC series | Multi-domain task-oriented dataset. | Task-oriented Dialogue
    | [https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/](https://www.microsoft.com/en-us/research/event/dialog-state-tracking-challenge/)
    |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| SimDial | Simulated conversations spanning over multiple domains. | Task-oriented
    Dialogue | Zhao and Eskenazi ([2018](#bib.bib456)) |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| SMD | Human-human dialogues in weather, navigation and scheduling domain.
    | Task-oriented Dialogue | Eric and Manning ([2017](#bib.bib81)) |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| BANKING | Question-answer pairs with 77 categories in e-banking domain. |
    Task-oriented Dialogue | Henderson et al. ([2019b](#bib.bib136)) |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| Weather forecast | A task-oriented dataset in the weather domain. | Task-oriented
    Dialogue | Balakrishnan et al. ([2019](#bib.bib14)) |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| MedDialog-(EN,CN) | Large scale dataset in medical domain consisting of conversations
    between doctors and patients | Task-oriented Dialogue | He et al. ([2020b](#bib.bib129))
    |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| CamRest | It consists of human-human multi-turn dialogues in restaurant domain.
    | Task-oriented Dialogue | Wen et al. ([2016a](#bib.bib399)) |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| Taskmaster | Contain dialogues spanning over 6 domains. It has 22.9 average
    length of conversational turns. | Task-oriented Dialogue | Byrne et al. ([2019](#bib.bib32))
    |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| Frames | Conversational dataset with annotations of semantic frame tracking.
    | Task-oriented Dialogue | Asri et al. ([2017](#bib.bib10)) |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| JDDC | A Chinese customer service dataset, consisting of context-response
    pairs. | Task-oriented Dialogue | Chen et al. ([2019a](#bib.bib43)) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: '| Court Debate Dataset | A task-oriented dataset in judicial field containing
    court debate conversations. | Task-oriented Dialogue | Ji et al. ([2020](#bib.bib155))
    |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: '| TreeDST | A task-oriented dataset annotated with tree structured dialogue
    states and agent acts. | Task-oriented Dialogue | Cheng et al. ([2020](#bib.bib49))
    |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
- en: '| RiSAWOZ | Contain utterances for 12 domains, annotated with rich semantic
    information. | Task-oriented Dialogue | Quan et al. ([2020](#bib.bib286)) |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: '| Cambridge Restaurant | A task-oriented dataset in restaurant booking field.
    | Task-oriented Dialogue | Wen et al. ([2016c](#bib.bib401)) |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
- en: '| SB-TOP | A task-oriented dataset with semantic parsing annotation. It spans
    over 4 domains: Reminder, Weather, Calling and Music. | Task-oriented Dialogue
    | Aghajanyan et al. ([2020](#bib.bib2)) |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
- en: '| GSIM | A machine-machine task-oriented dataset. It covers two domains: restaurant
    table booking and movie ticket booking. | Task-oriented Dialogue | Shah et al.
    ([2018](#bib.bib316)) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
- en: '| SGD | A schema-guided dataset spanning over multiple domains. | Task-oriented
    Dialogue | Rastogi et al. ([2020](#bib.bib292)) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '| cite-8K | A task-oriented dataset collected in restaurant booking calls.
    | Task-oriented Dialogue | Coope et al. ([2020](#bib.bib58)) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
- en: 6.2 Datasets for Open-domain Dialogue Systems
  id: totrans-472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Datasets for Open-domain dialogue systems'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
- en: '| Name | Description | Task | Origin |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
- en: '| Large-Scale Corpus for Conversation Disentanglement | A dataset consisting
    of messages annotated with reply-structure graphs for dialogue disentanglement.
    | Conversation Disentaglement | Kummerfeld et al. ([2018](#bib.bib174)) |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
- en: '| DuConv | Collected in conversations between a conversation leader and a conversation
    follower. | Conversation Topic | Wu et al. ([2019c](#bib.bib417)) |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
- en: '| PERSUASION FOR GOOD | A topic-oriented dataset annotated with persuasion
    strategies. | Conversation Topic | Wang et al. ([2019b](#bib.bib390)) |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
- en: '| MutualFriends | A topic-oriented dataset based on bot-bot stratigical conversations.
    | Conversation Topic | He et al. ([2017](#bib.bib125)) |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
- en: '| SAMSum | A large-scale dialogue summary dataset. | Conversation Topic | Gliwa
    et al. ([2019](#bib.bib105)) |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
- en: '| OpenDialKG | It consists conversations between two agents and each dialogue
    corresponds with a knowledge graph path annotation. | Conversation Topic; Dialogue
    Reasoning | Moon et al. ([2019](#bib.bib257)) |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
- en: '| doc2dial | A dataset consisting of conversations annotated with goals and
    accociated documents. | Conversation Topic; Knowledge-Grounded System | Feng et al.
    ([2020c](#bib.bib89)) |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
- en: '| DialEdit | A dataset constructed for image editing via conversational language
    instructions. | Conversational Image Editing | Manuvina-kurike et al. ([2018](#bib.bib241))
    |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
- en: '| CHART DIALOGS | A dataset containing dialogues describing matplotlib plot
    features. | Conversational Plotting | Shao and Nakashole ([2020](#bib.bib320))
    |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
- en: '| CONAN | A multilingual dataset for hate speech tackling. | Dialogue Classification
    | Chung et al. ([2019](#bib.bib55)) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
- en: '| Dialogue NLI | A NLI dataset with sentences annotated with entailment (E),
    neutral (N), or contradiction (C). | Dialogue Inference | Welleck et al. ([2018](#bib.bib396))
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
- en: '| MuTual | A dialogue reasoning dataset containing English listening comprehension
    exams. | Dialogue Reasoning | Cui et al. ([2020](#bib.bib60)) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
- en: '| RST-DT | It consists of samples from 385 news articles annotated with dialogue
    features. | Discourse Parsing | Carlson et al. ([2002](#bib.bib36)) |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '| NLPCC | A dataset consisting of emotional classification data. | Empathetic
    Response | [http://tcci.ccf.org.cn/nlpcc.php](http://tcci.ccf.org.cn/nlpcc.php)
    |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
- en: '| MELD | A multi-party conversational dataset with emotion annotations. | Empathetic
    Response | Poria et al. ([2019](#bib.bib275)) |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
- en: '| EMPATHETIC DIALOGUES | A dataset containing conversations annotated with
    emotion labels. | Empathetic Response | Rashkin et al. ([2018](#bib.bib291)) |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
- en: '| IEMOCAP | Contain multi-party dialogues. Each dialogue is annotated with
    an emotion label. | Empathetic Response | Busso et al. ([2008](#bib.bib31)) |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
- en: '| EmoryNLP | Collected from Friends’ TV series, annotated with emotion labels.
    | Empathetic Response | Zahiri and Choi ([2017](#bib.bib443)) |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
- en: '| MojiTalk | A largescale dataset collected from Twitter, including emojis.
    | Empathetic Response | Zhou and Wang ([2017](#bib.bib465)) |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
- en: '| CBET | A dialogue dataset annotated with nine emotion labels: surprise, anger,
    love, sadness, joy, fear, guilt, disgust and thankfulness | Empathetic Response
    | Yadollahi et al. ([2017](#bib.bib428)) |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| Stanford Politeness Corpus | A conversational dataset annotated with politeness
    labels. | Empathetic Response | Danescu-Niculescu-Mizil et al. ([2013](#bib.bib65))
    |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| AIT-2018 | Collected in SemEval-2018 Task 1: Affect in Tweets. | Empathetic
    Response | Mohammad et al. ([2018](#bib.bib256)) |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| EMOTyDA | A dataset containing short videos about multi-party conversations,
    each annotated with respective emotion. | Empathetic Response; Visual Dialogue
    | Saha et al. ([2020](#bib.bib303)) |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| Wizard of Wikipedia | A large-sclale dataset consisting of conversations
    grounded with Wikipedia knowledge. | Knowledge-Grounded System | Dinan et al.
    ([2018](#bib.bib72)) |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| CMU DoG | A dataset consisting of conversations grounded with Wikipedia articles
    about popular movies. | Knowledge-Grounded System | Zhou et al. ([2018](#bib.bib463))
    |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| Holl-E | Contain dialogues grounded with documents. | Knowledge-Grounded
    System | Moghe et al. ([2018](#bib.bib255)) |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| Interview | A dataset containing multi-party conversations in the form of
    interviews. | Knowledge-Grounded System | Majumder et al. ([2020b](#bib.bib239))
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: '| Curiosity | An open-domain dataset annotated with pre-existing user knowledge
    and dialogue acts, also grounding in Wikipedia. | Knowledge-Grounded System |
    Rodriguez et al. ([2020](#bib.bib301)) |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
- en: '| KdConv | A chinese knowledge-grounded dialogue dataset. | Knowledge-Grounded
    System | Zhou et al. ([2020a](#bib.bib462)) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
- en: '| ELI5 | A QA dataset grounded with retrieved documents. | Knowledge-Grounded
    System | Fan et al. ([2019](#bib.bib83)) |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
- en: '| Topical Chat | A knowledge-grounded dataset where the knowledge spans over
    eight different topics. | Knowledge-Grounded System; Conversation Topic | Gopalakrishnan
    et al. ([2019](#bib.bib109)) |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
- en: '| WHERE ARE YOU? | A dialogue dataset annotated with localization information.
    | Localization Dialogue | Hahn et al. ([2020](#bib.bib119)) |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
- en: '| MMD | A multi-modal dataset consisting of dialogues between sales agents
    and shoppers. | Multi-modal Dialogue | Saha et al. ([2018](#bib.bib302)) |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
- en: '| OpenSubtitles | A multilingual dataset made up of movie captions, containing
    about 8 billion words. | Open-domain Dialogue | Tiedemann ([2012](#bib.bib370))
    |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
- en: '| NTCIR | A social media dataset collected from Sina Weibo. | Open-domain Dialogue
    | [http://research.nii.ac.jp/ntcir/data/data-en.html](http://research.nii.ac.jp/ntcir/data/data-en.html)
    |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
- en: '| Twitter | A social media dataset collected from Twitter. | Open-domain Dialogue
    | [https://github.com/Marsan-Ma-zz/chatcorpus](https://github.com/Marsan-Ma-zz/chatcorpus)
    |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
- en: '| Douban Conversation Corpus | A social media dataset collected from Douban.
    | Open-domain Dialogue | Zhang et al. ([2018d](#bib.bib453)) |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
- en: '| E-commerce Dialogue Corpus | It consists of conversations between customers
    and customer service staff on Taobao. | Open-domain Dialogue | Zhang et al. ([2018d](#bib.bib453))
    |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
- en: '| REDDIT | A social media dataset collected from REDDIT. | Open-domain Dialogue
    | Henderson et al. ([2019a](#bib.bib135)) |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
- en: '| STC-SeFun | A social media dataset collected from Tieba, Zhidao, Douban and
    Weibo. | Open-domain Dialogue | Bi et al. ([2019](#bib.bib22)) |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
- en: '| DailyDialog | A dataset consisting of daily dialogues, annotated with conversation
    intention and emotion information. | Open-domain Dialogue | Li et al. ([2017c](#bib.bib206))
    |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
- en: '| PDTB | Dialogue dataset annotated with discourse relations. | Open-domain
    Dialogue | Prasad et al. ([2008](#bib.bib277)) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
- en: '| Luna | Dialogue dataset with Italian relation annotations. | Open-domain
    Dialogue | Tonelli et al. ([2010](#bib.bib371)) |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
- en: '| Edina-DR | Dialogue dataset with English relation annotations, which is based
    on Luna data set. | Open-domain Dialogue | Ma et al. ([2019](#bib.bib234)) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
- en: '| Cornell Movie Dialog Corpus | A dialogue dataset collected via IMDB database.
    | Open-domain Dialogue | Danescu-Niculescu-Mizil and Lee ([2011](#bib.bib64))
    |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| Reddit Movie Dialogue Dataset | A movie dialogue dataset collected from Reddit.
    | Open-domain Dialogue | Liu et al. ([2020a](#bib.bib224)) |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| LIGHT | A dialogue dataset with configurable text adventure environment.
    | Open-domain Dialogue | Urbanek et al. ([2019](#bib.bib376)) |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
- en: '| This American Life | A media dialogue dataset collected in long-form expository
    podcast episodes. | Open-domain Dialogue | Mao et al. ([2020](#bib.bib242)) |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
- en: '| RadioTalk | A media dialogue dataset collected from radio transcripts. |
    Open-domain Dialogue | Beeferman et al. ([2019](#bib.bib19)) |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
- en: '| French EPAC | A media dialogue dataset collected from news. | Open-domain
    Dialogue | Esteve et al. ([2010](#bib.bib82)) |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
- en: '| TREC Conversational Assistance | An open-domain dataset spanning 30 conversation
    topics. | Open-domain Dialogue | Dalton et al. ([2020](#bib.bib63)) |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
- en: '| Search as a Conversation | A dataset for conversations with search engines.
    | Open-domain Dialogue | Ren et al. ([2020](#bib.bib299)) |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
- en: '| Amazon Alexa Prize Competition | A dataset containing real-world conversations
    between Amazon Alexa customers and Gunrock, which is a champion chatbot. | Open-domain
    Dialogue | Ram et al. ([2018](#bib.bib288)) |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
- en: '| SwitchBoard | An open-domain dataset containing English phone conversations.
    | Open-domain Dialogue | Jurafsky ([1997](#bib.bib161)) |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
- en: '| Zhihu | A Chinese social media dataset with posts and comments. | Open-domain
    Dialogue | [https://www.zhihu.com](https://www.zhihu.com) |'
  id: totrans-530
  prefs: []
  type: TYPE_TB
- en: '| SPOLIN | A dataset containing yes-and conversations. | Open-domain Dialogue
    | Cho and May ([2020](#bib.bib50)) |'
  id: totrans-531
  prefs: []
  type: TYPE_TB
- en: '| CRD3 | A dataset collected in the role-playing game Dungeons and Dragons.
    | Open-domain Dialogue | Rameshkumar and Bailey ([2020](#bib.bib289)) |'
  id: totrans-532
  prefs: []
  type: TYPE_TB
- en: '| Baidu Zhidao | A Chinese social media dataset with posts and comments. |
    Open-domain Dialogue | [https://zhidao.baidu.com/](https://zhidao.baidu.com/)
    |'
  id: totrans-533
  prefs: []
  type: TYPE_TB
- en: '| Webis Gmane Email Corpus 2019 | A conversational dataset collected from 153M
    emails. | Open-domain Dialogue | Bevendorff et al. ([2020](#bib.bib21)) |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
- en: '| LibreSpeech Corpus | Contain 500 hours’ speech produced by 1252 participants.
    | Open-domain Dialogue | Panayotov et al. ([2015](#bib.bib267)) |'
  id: totrans-535
  prefs: []
  type: TYPE_TB
- en: '| Motivational Interviewing | A dialogue dataset about conversational psychotherapy.
    | Open-domain Dialogue | Tanana et al. ([2016](#bib.bib360)) |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
- en: '| SubTle Corpus | Contact Ameixa for data. | Open-domain Dialogue | Lubis et al.
    ([2018](#bib.bib233)) |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
- en: '| TED-LIUM | TED-talk monologues. | Open-domain Dialogue | Fung et al. ([2016](#bib.bib93))
    |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
- en: '| ECG NLPCC 2017 Data | Conversational dataset extracted from Weibo. | Open-domain
    Dialogue | Huang et al. ([2018](#bib.bib148)) |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
- en: '| SEMEVAL15 | QA dataset with answer quality annotations via Amazon Mechanical
    Turk. | Question Answering | Nakov et al. ([2019](#bib.bib261)) |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
- en: '| AMAZONQA | A QA dataset solving one-to-many problems. | Question Answering
    | Wan and McAuley ([2016](#bib.bib383)) |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
- en: '| TGIF-QA | A video-grounded QA dataset. | Question Answering | Jang et al.
    ([2017](#bib.bib153)) |'
  id: totrans-542
  prefs: []
  type: TYPE_TB
- en: '| QuAC | A QA dataset with 14K QA dialogues. | Question Answering | Choi et al.
    ([2018](#bib.bib53)) |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
- en: '| SQuAD | A question-answering dataset collected in crowdsourced fashion. |
    Question Answering | Rajpurkar et al. ([2018](#bib.bib287)) |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
- en: '| LIF | A dataset constructed based on QuAC. | Question Answering | Kundu et al.
    ([2020](#bib.bib175)) |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
- en: '| Yelp | It consists of customer reviews from Yelp Dataset Challenge | Response
    Retrieval | Tang et al. ([2015](#bib.bib361)) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
- en: '| Debates | The dataset consists of debates on Congerssional bills. | Response
    Retrieval | Thomas et al. ([2006](#bib.bib368)) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
- en: '| PERSONACHAT | It provides profile information of the agents and background
    of users. | Speaker Consistency and Personality Response | Zhang et al. ([2018b](#bib.bib449))
    |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
- en: '| KvPI | Contain consistency annotations between response and corresponding
    key-value profiles. | Speaker Consistency and Personality Response | Song et al.
    ([2020b](#bib.bib336)) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
- en: '| ConvAI2 | A dataset constructed on the base of Persona-Chat, each conversation
    having profiles from a set containing persona candidates. | Speaker Consistency
    and Personality Response | Dinan et al. ([2019](#bib.bib73)) |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
- en: '| PEC | An open-domain dataset annotated with persona labels. | Speaker Consistency
    and Personality Response; Empathetic Response | Zhong et al. ([2020](#bib.bib460))
    |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
- en: '| GuessWhat?! | A visual dialogue dataset for a two-player game about object
    recognition. | Visual Dialogue | De Vries et al. ([2017](#bib.bib66)) |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
- en: '| VisDial | A visual dialogue dataset whose images are obtained from COCO data
    set. | Visual Dialogue | [https://visualdialog.org/data](https://visualdialog.org/data)
    |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
- en: '| AVSD | A video-grounded dialogue dataset. | Visual Dialogue | Yoshino et al.
    ([2018](#bib.bib436)) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
- en: '| VFD | A visual dialogue dataset annotated with unique eye-gaze locations.
    | Visual Dialogue | Kamezawa et al. ([2020](#bib.bib163)) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
- en: '| PhotoBook | A dataset for task-oriented visual dialogues. | Visual Dialogue
    | Haber et al. ([2019](#bib.bib118)) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
- en: '| IGC | A dataset containing conversations discussing a given image. | Visual
    Dialogue | Mostafazadeh et al. ([2017](#bib.bib258)) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
- en: '| Image-Chat | Contain conversations grounded with images. The conversations
    are also annotated with personality. | Visual Dialogue; Speaker Consistency and
    Personality Response | Shuster et al. ([2018](#bib.bib323)) |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
- en: 7 Conclusions and Trends
  id: totrans-559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More and more researchers are investigating conversational tasks. One factor
    contributing to the popularity of conversational tasks is the increasing demand
    for chatbots in industry and daily life. Industry agents like Apple’s Siri, Microsoft’s
    Cortana, Facebook M, Google Assistant, and Amazon’s Alexa have brought huge convenience
    to people’s lives. Another reason is that a considerable amount of natural language
    data is in the form of dialogues, which contributes to the efforts in dialogue
    research.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper we discuss dialogue systems from two perspectives: model and
    system type. Dialogue systems are a complicated but promising task because it
    involves the whole process of communication between agent and human. The works
    of recent years show an overwhelming preference towards neural methods, no matter
    in task-oriented or open-domain dialogue systems. Neural methods outperform traditional
    rule-based methods, statistical methods and machine learning methods for that
    neural models have the stronger fitting ability and require less hand-crafted
    feature engineering.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: 'We systematically summarized and categorized the latest works in dialogue systems,
    and also in other dialogue-related tasks. We hope these discussions and insights
    provide a comprehensive picture of the state-of-the-art in this area and pave
    the way for further research. Finally, we discuss some possible research trends
    arising from the works reviewed:'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal dialogue systems
  id: totrans-563
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The world is multimodal and humans observe it via multiple senses such as vision,
    hearing, smell, taste, and touch. In a conversational interaction, humans tend
    to make responses not only based on text, but also on what they see and hear.
    Thus, some researchers argue that chatbots should also have such abilities to
    blend information from different modalities. There are some recent works trying
    to build multimodal dialogue systems (Le et al., [2019](#bib.bib179); Chauhan
    et al., [2019](#bib.bib38); Saha et al., [2020](#bib.bib303); Singla et al., [2020](#bib.bib331);
    Young et al., [2020](#bib.bib439)), but these systems are still far from mature.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: Multitask dialogue systems
  id: totrans-565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dialogue systems are categorized into task-oriented and open-domain systems.
    Such a research boundary has existed for a long time because task-oriented dialogue
    systems involve dialogue states, which constrain the decoding process. However,
    works in end-to-end task-oriented dialogue systems and knowledge-grounded open-domain
    systems provide a possibility of blending these two categories into a single framework,
    or even a single model. Such blended dialogue systems perform as assistants and
    chatbots simultaneously.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Corpus exploration on Internet
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Section [6](#S6 "6 Datasets ‣ Recent Advances in Deep Learning Based Dialogue
    Systems: A Systematic Survey") we reviewed many datasets for dialogue systems
    training. However, data is still far from enough to train a perfect dialogue system.
    Many learning techniques are designed to alleviate this problem, such as reinforcement
    learning, meta-learning, transfer learning, and active learning. But many works
    ignore a significant source of information, which is the dialogue corpus on the
    Internet. There is a large volume of conversational corpus on the Internet but
    people have no access to the raw corpus because much of it is in a messy condition.
    In the future, dialogue agents should be able to explore useful corpus on the
    Internet in real-time for training. This can be achieved by standardizing online
    corpus access and their related legal terms. Moreover, real-time conversational
    corpus exploration can be an independent task that deserves study.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: User modeling
  id: totrans-569
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: User modeling is a hot topic in both dialogue generation (Gür et al., [2018](#bib.bib117);
    Serras et al., [2019](#bib.bib315)) and dialogue systems evaluation (Kannan and
    Vinyals, [2017](#bib.bib164)). Basically, the user modeling module tries to simulate
    the real decisions and actions of a human user. It makes decisions based on the
    dialogue state or dialogue history. In dialogue generation tasks, modeling the
    user helps the agent converse more coherently, based on the background information
    or even speaking habits. Besides that, a mature user simulator can provide an
    interactive training environment, which reduces the reliance on annotated training
    samples when training a dialogue system. In dialogue systems evaluation tasks,
    a user simulator provides user messages to test a dialogue agent. More recent
    user simulators also give feedback concerning the responses generated by the dialogue
    agent. However, user modeling is a challenging task since no matter explicit user
    simulation or implicit user modeling is actually the same in difficulty as response
    generation. Since response generation systems are not perfect yet, user modeling
    can still be a topic worthy of study.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: Dialogue generation with a long-term goal
  id: totrans-571
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most of our daily conversations are chitchats without any purpose. However,
    there are quite a few scenarios when we purposely guide the conversation content
    to achieve a specific goal. Current open-domain dialogue systems tend to model
    the conversation without a long-term goal, which does not exhibit enough intelligence.
    There are some recent works that apply reinforcement policy learning to model
    a long-term reward which encourages the agent to converse with a long-term goal,
    such as the work of Xu et al. ([2020a](#bib.bib426)). This topic will lead to
    strong artificial intelligence, which is useful in some real-life applications
    such as negotiation or story-telling chatbots.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research/project is supported by A*STAR under its Industry Alignment Fund
    (LOA Award I1901E0046).
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-575
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agarwal et al. (2020) Agarwal S, Bui T, Lee JY, Konstas I, Rieser V (2020)
    History for visual dialog: Do we really need it? arXiv preprint arXiv:200507493'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aghajanyan et al. (2020) Aghajanyan A, Maillard J, Shrivastava A, Diedrick K,
    Haeger M, Li H, Mehdad Y, Stoyanov V, Kumar A, Lewis M, et al. (2020) Conversational
    semantic parsing. arXiv preprint arXiv:200913655
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akama et al. (2020) Akama R, Yokoi S, Suzuki J, Inui K (2020) Filtering noisy
    dialogue corpora by connectivity and content relatedness. arXiv preprint arXiv:200414008
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alberti et al. (2019) Alberti C, Ling J, Collins M, Reitter D (2019) Fusion
    of detected objects in text for visual question answering. arXiv preprint arXiv:190805054
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aloysius and Geetha (2017) Aloysius N, Geetha M (2017) A review on deep convolutional
    neural networks. In: 2017 International Conference on Communication and Signal
    Processing (ICCSP), IEEE, pp 0588–0592'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Annervaz et al. (2018) Annervaz K, Chowdhury SBR, Dukkipati A (2018) Learning
    beyond datasets: Knowledge graph augmented neural networks for natural language
    processing. arXiv preprint arXiv:180205930'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arora et al. (2013) Arora S, Batra K, Singh S (2013) Dialogue system: A brief
    review. arXiv preprint arXiv:13064134'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asghar et al. (2016) Asghar N, Poupart P, Jiang X, Li H (2016) Deep active learning
    for dialogue generation. arXiv preprint arXiv:161203929
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asri et al. (2016) Asri LE, He J, Suleman K (2016) A sequence-to-sequence model
    for user simulation in spoken dialogue systems. arXiv preprint arXiv:160700070
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Asri et al. (2017) Asri LE, Schulz H, Sharma S, Zumer J, Harris J, Fine E,
    Mehrotra R, Suleman K (2017) Frames: a corpus for adding memory to goal-oriented
    dialogue systems. arXiv preprint arXiv:170400057'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aubert et al. (1994) Aubert X, Dugast C, Ney H, Steinbiss V (1994) Large vocabulary
    continuous speech recognition of wall street journal data. In: Proceedings of
    ICASSP’94. IEEE International Conference on Acoustics, Speech and Signal Processing,
    IEEE, vol 2, pp II–129'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2014) Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation
    by jointly learning to align and translate. arXiv preprint arXiv:14090473
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baheti et al. (2020) Baheti A, Ritter A, Small K (2020) Fluent response generation
    for conversational question answering. arXiv preprint arXiv:200510464
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balakrishnan et al. (2019) Balakrishnan A, Rao J, Upasani K, White M, Subba
    R (2019) Constrained decoding for neural nlg from compositional representations
    in task-oriented dialogue. arXiv preprint arXiv:190607220
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie (2005) Banerjee S, Lavie A (2005) Meteor: An automatic metric
    for mt evaluation with improved correlation with human judgments. In: Proceedings
    of the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization, pp 65–72'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2019a) Bao S, He H, Wang F, Lian R, Wu H (2019a) Know more about
    each other: Evolving dialogue strategy via compound assessment. arXiv preprint
    arXiv:190600549'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bao et al. (2019b) Bao S, He H, Wang F, Wu H, Wang H (2019b) Plato: Pre-trained
    dialogue generation model with discrete latent variable. arXiv preprint arXiv:191007931'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bapna et al. (2017) Bapna A, Tur G, Hakkani-Tur D, Heck L (2017) Towards zero-shot
    frame semantic parsing for domain scaling. arXiv preprint arXiv:170702363
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beeferman et al. (2019) Beeferman D, Brannon W, Roy D (2019) Radiotalk: A large-scale
    corpus of talk radio transcripts. arXiv preprint arXiv:190707073'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (1994) Bengio Y, Simard P, Frasconi P (1994) Learning long-term
    dependencies with gradient descent is difficult. IEEE transactions on neural networks
    5(2):157–166
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bevendorff et al. (2020) Bevendorff J, Al Khatib K, Potthast M, Stein B (2020)
    Crawling and preprocessing mailing lists at scale for dialog analysis. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    1151–1158'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bi et al. (2019) Bi W, Gao J, Liu X, Shi S (2019) Fine-grained sentence functions
    for short-text conversation. arXiv preprint arXiv:190710302
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bordes et al. (2013) Bordes A, Usunier N, Garcia-Duran A, Weston J, Yakhnenko
    O (2013) Translating embeddings for modeling multi-relational data. In: Neural
    Information Processing Systems (NIPS), pp 1–9'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bordes et al. (2014) Bordes A, Glorot X, Weston J, Bengio Y (2014) A semantic
    matching energy function for learning with multi-relational data. Machine Learning
    94(2):233–259
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bordes et al. (2016) Bordes A, Boureau YL, Weston J (2016) Learning end-to-end
    goal-oriented dialog. arXiv preprint arXiv:160507683
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bosselut et al. (2019) Bosselut A, Rashkin H, Sap M, Malaviya C, Celikyilmaz
    A, Choi Y (2019) Comet: Commonsense transformers for automatic knowledge graph
    construction. arXiv preprint arXiv:190605317'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bouchacourt and Baroni (2019) Bouchacourt D, Baroni M (2019) Miss tools and
    mr fruit: Emergent communication in agents learning about object affordances.
    arXiv preprint arXiv:190511871'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boyd et al. (2020) Boyd A, Puri R, Shoeybi M, Patwary M, Catanzaro B (2020)
    Large scale multi-actor generative dialog modeling. arXiv preprint arXiv:200506114
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bruni and Fernandez (2017) Bruni E, Fernandez R (2017) Adversarial evaluation
    for open-domain dialogue generation. In: Proceedings of the 18th Annual SIGdial
    Meeting on Discourse and Dialogue, pp 284–288'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budzianowski et al. (2018) Budzianowski P, Wen TH, Tseng BH, Casanueva I, Ultes
    S, Ramadan O, Gašić M (2018) Multiwoz–a large-scale multi-domain wizard-of-oz
    dataset for task-oriented dialogue modelling. arXiv preprint arXiv:181000278
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Busso et al. (2008) Busso C, Bulut M, Lee CC, Kazemzadeh A, Mower E, Kim S,
    Chang JN, Lee S, Narayanan SS (2008) Iemocap: Interactive emotional dyadic motion
    capture database. Language resources and evaluation 42(4):335–359'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Byrne et al. (2019) Byrne B, Krishnamoorthi K, Sankar C, Neelakantan A, Duckworth
    D, Yavuz S, Goodrich B, Dubey A, Cedilnik A, Kim KY (2019) Taskmaster-1: Toward
    a realistic and diverse dialog dataset. arXiv preprint arXiv:190905358'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cahill et al. (1999) Cahill L, Doran C, Evans R, Mellish C, Paiva D, Reape
    M, Scott D, Tipper N (1999) In search of a reference architecture for nlg systems.
    In: Proceedings of the 7th european workshop on natural language generation, Citeseer,
    pp 77–85'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campagna et al. (2020) Campagna G, Foryciarz A, Moradshahi M, Lam MS (2020)
    Zero-shot transfer learning with synthesized data for multi-domain dialogue state
    tracking. arXiv preprint arXiv:200500891
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2019) Cao J, Tanana M, Imel ZE, Poitras E, Atkins DC, Srikumar
    V (2019) Observing dialogue in therapy: Categorizing and forecasting behavioral
    codes. arXiv preprint arXiv:190700326'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlson et al. (2002) Carlson L, Okurowski ME, Marcu D (2002) RST discourse
    treebank. Linguistic Data Consortium, University of Pennsylvania
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chandramohan et al. (2011) Chandramohan S, Geist M, Lefevre F, Pietquin O (2011)
    User simulation in dialogue systems using inverse reinforcement learning. In:
    Twelfth Annual Conference of the International Speech Communication Association'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chauhan et al. (2019) Chauhan H, Firdaus M, Ekbal A, Bhattacharyya P (2019)
    Ordinal and attribute aware response generation in a multimodal dialogue system.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 5437–5447'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017a) Chen H, Liu X, Yin D, Tang J (2017a) A survey on dialogue
    systems: Recent advances and new frontiers. Acm Sigkdd Explorations Newsletter
    19(2):25–35'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Yang (2020) Chen J, Yang D (2020) Multi-view sequence-to-sequence models
    with conversational structure for abstractive dialogue summarization. arXiv preprint
    arXiv:201001672
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020a) Chen J, Zhang R, Mao Y, Xu J (2020a) Parallel interactive
    networks for multi-domain dialogue state generation. arXiv preprint arXiv:200907616
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2017b) Chen L, Zhou X, Chang C, Yang R, Yu K (2017b) Agent-aware
    dropout dqn for safe and efficient on-line dialogue policy learning. In: Proceedings
    of the 2017 Conference on Empirical Methods in Natural Language Processing, pp
    2454–2464'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019a) Chen M, Liu R, Shen L, Yuan S, Zhou J, Wu Y, He X, Zhou
    B (2019a) The jddc corpus: A large-scale multi-turn chinese dialogue dataset for
    e-commerce customer service. arXiv preprint arXiv:191109969'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2019b) Chen W, Chen J, Qin P, Yan X, Wang WY (2019b) Semantically
    conditioned dialog response generation via hierarchical disentangled self-attention.
    arXiv preprint arXiv:190512866
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019c) Chen X, Xu J, Xu B (2019c) A working memory model for task-oriented
    dialog response generation. In: Proceedings of the 57th Annual Meeting of the
    Association for Computational Linguistics, pp 2687–2693'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2020b) Chen X, Meng F, Li P, Chen F, Xu S, Xu B, Zhou J (2020b)
    Bridging the gap between prior and posterior knowledge selection for knowledge-grounded
    dialogue generation. In: Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pp 3426–3437'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2019d) Chen YC, Li L, Yu L, El Kholy A, Ahmed F, Gan Z, Cheng
    Y, Liu J (2019d) Uniter: Learning universal image-text representations. ECCV'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2016) Chen YN, Hakkani-Tür D, Tür G, Gao J, Deng L (2016) End-to-end
    memory networks with knowledge carryover for multi-turn spoken language understanding.
    In: Interspeech, pp 3245–3249'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2020) Cheng J, Agrawal D, Alonso HM, Bhargava S, Driesen J, Flego
    F, Kaplan D, Kartsaklis D, Li L, Piraviperumal D, et al. (2020) Conversational
    semantic parsing for dialog state tracking. arXiv preprint arXiv:201012770
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho and May (2020) Cho H, May J (2020) Grounding conversations with improvised
    dialogues. arXiv preprint arXiv:200409544
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2014a) Cho K, Van Merriënboer B, Bahdanau D, Bengio Y (2014a) On
    the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:14091259'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2014b) Cho K, Van Merriënboer B, Gulcehre C, Bahdanau D, Bougares
    F, Schwenk H, Bengio Y (2014b) Learning phrase representations using rnn encoder-decoder
    for statistical machine translation. arXiv preprint arXiv:14061078
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2018) Choi E, He H, Iyyer M, Yatskar M, Yih Wt, Choi Y, Liang
    P, Zettlemoyer L (2018) Quac: Question answering in context. arXiv preprint arXiv:180807036'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2014) Chung J, Gulcehre C, Cho K, Bengio Y (2014) Empirical evaluation
    of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:14123555
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chung et al. (2019) Chung YL, Kuzmenko E, Tekiroglu SS, Guerini M (2019) Conan–counter
    narratives through nichesourcing: a multilingual dataset of responses to fight
    online hate speech. arXiv preprint arXiv:191003270'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cogswell et al. (2020) Cogswell M, Lu J, Jain R, Lee S, Parikh D, Batra D (2020)
    Dialog without dialog data: Learning visual dialog agents from vqa data. arXiv
    preprint arXiv:200712750'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conneau et al. (2016) Conneau A, Schwenk H, Barrault L, Lecun Y (2016) Very
    deep convolutional networks for text classification. arXiv preprint arXiv:160601781
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coope et al. (2020) Coope S, Farghly T, Gerz D, Vulić I, Henderson M (2020)
    Span-convert: Few-shot span extraction for dialog with pretrained conversational
    representations. arXiv preprint arXiv:200508866'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Csáky et al. (2019) Csáky R, Purgai P, Recski G (2019) Improving neural conversational
    models with entropy-based data filtering. arXiv preprint arXiv:190505471
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cui et al. (2020) Cui L, Wu Y, Liu S, Zhang Y, Zhou M (2020) Mutual: A dataset
    for multi-turn dialogue reasoning. arXiv preprint arXiv:200404494'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2020) Dai Y, Li H, Tang C, Li Y, Sun J, Zhu X (2020) Learning low-resource
    end-to-end goal-oriented dialog for fast and reliable system deployment. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    609–618'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov
    R (2019) Transformer-xl: Attentive language models beyond a fixed-length context.
    arXiv preprint arXiv:190102860'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dalton et al. (2020) Dalton J, Xiong C, Callan J (2020) Trec cast 2019: The
    conversational assistance track overview. arXiv preprint arXiv:200313624'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Danescu-Niculescu-Mizil and Lee (2011) Danescu-Niculescu-Mizil C, Lee L (2011)
    Chameleons in imagined conversations: A new approach to understanding coordination
    of linguistic style in dialogs. arXiv preprint arXiv:11063077'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Danescu-Niculescu-Mizil et al. (2013) Danescu-Niculescu-Mizil C, Sudhof M, Jurafsky
    D, Leskovec J, Potts C (2013) A computational approach to politeness with application
    to social factors. arXiv preprint arXiv:13066078
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Vries et al. (2017) De Vries H, Strub F, Chandar S, Pietquin O, Larochelle
    H, Courville A (2017) Guesswhat?! visual object discovery through multi-modal
    dialogue. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pp 5503–5512'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2012) Deng L, Tur G, He X, Hakkani-Tur D (2012) Use of kernel
    deep convex networks and end-to-end learning for spoken language understanding.
    In: 2012 IEEE Spoken Language Technology Workshop (SLT), IEEE, pp 210–215'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deoras and Sarikaya (2013) Deoras A, Sarikaya R (2013) Deep belief network
    based semantic taggers for spoken language understanding. In: Interspeech, pp
    2713–2717'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deriu et al. (2020) Deriu J, Tuggener D, von Däniken P, Campos JA, Rodrigo
    A, Belkacem T, Soroa A, Agirre E, Cieliebak M (2020) Spot the bot: A robust and
    efficient framework for the evaluation of conversational dialogue systems. arXiv
    preprint arXiv:201002140'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:181004805'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhingra et al. (2016) Dhingra B, Li L, Li X, Gao J, Chen YN, Ahmed F, Deng L
    (2016) Towards end-to-end reinforcement learning of dialogue agents for information
    access. arXiv preprint arXiv:160900777
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dinan et al. (2018) Dinan E, Roller S, Shuster K, Fan A, Auli M, Weston J (2018)
    Wizard of wikipedia: Knowledge-powered conversational agents. arXiv preprint arXiv:181101241'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dinan et al. (2019) Dinan E, Logacheva V, Malykh V, Miller A, Shuster K, Urbanek
    J, Kiela D, Szlam A, Serban I, Lowe R, et al. (2019) The second conversational
    intelligence challenge (convai2). arXiv preprint arXiv:190200098
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dong et al. (2017) Dong L, Huang S, Wei F, Lapata M, Zhou M, Xu K (2017) Learning
    to generate product reviews from attributes. In: Proceedings of the 15th Conference
    of the European Chapter of the Association for Computational Linguistics: Volume
    1, Long Papers, pp 623–632'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2019) Du N, Chen K, Kannan A, Tran L, Chen Y, Shafran I (2019) Extracting
    symptoms and their status from clinical conversations. arXiv preprint arXiv:190602239
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du and Black (2019) Du W, Black AW (2019) Boosting dialog response generation.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 38–43'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dušek and Jurčíček (2016a) Dušek O, Jurčíček F (2016a) A context-aware natural
    language generator for dialogue systems. arXiv preprint arXiv:160807076
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dušek and Jurčíček (2016b) Dušek O, Jurčíček F (2016b) Sequence-to-sequence
    generation for spoken dialogue via deep syntax trees and strings. arXiv preprint
    arXiv:160605491
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elder et al. (2020) Elder H, O’Connor A, Foster J (2020) How to make neural
    natural language generation as reliable as templates in task-oriented dialogue.
    In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pp 2877–2888'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elman (1990) Elman JL (1990) Finding structure in time. Cognitive science 14(2):179–211
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eric and Manning (2017) Eric M, Manning CD (2017) Key-value retrieval networks
    for task-oriented dialogue. arXiv preprint arXiv:170505414
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Esteve et al. (2010) Esteve Y, Bazillon T, Antoine JY, Béchet F, Farinas J
    (2010) The epac corpus: Manual and automatic annotations of conversational speech
    in french broadcast news. In: LREC, Citeseer'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2019) Fan A, Jernite Y, Perez E, Grangier D, Weston J, Auli M (2019)
    Eli5: Long form question answering. arXiv preprint arXiv:190709190'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2014) Fan M, Zhou Q, Chang E, Zheng F (2014) Transition-based knowledge
    graph embedding with relational mapping properties. In: Proceedings of the 28th
    Pacific Asia conference on language, information and computing, pp 328–337'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feldman and El-Yaniv (2019) Feldman Y, El-Yaniv R (2019) Multi-hop paragraph
    retrieval for open-domain question answering. arXiv preprint arXiv:190606606
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2019) Feng J, Tao C, Wu W, Feng Y, Zhao D, Yan R (2019) Learning
    a matching model with co-teaching for multi-turn response selection in retrieval-based
    dialogue systems. arXiv preprint arXiv:190604413
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020a) Feng S, Chen H, Li K, Yin D (2020a) Posterior-gan: Towards
    informative and coherent response generation with posterior generative adversarial
    network. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol 34,
    pp 7708–7715'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2020b) Feng S, Ren X, Chen H, Sun B, Li K, Sun X (2020b) Regularizing
    dialogue generation by imitating implicit scenarios. arXiv preprint arXiv:201001893
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020c) Feng S, Wan H, Gunasekara C, Patel SS, Joshi S, Lastras
    LA (2020c) doc2dial: A goal-oriented document-grounded dialogue dataset. arXiv
    preprint arXiv:201106623'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferracane et al. (2019) Ferracane E, Durrett G, Li JJ, Erk K (2019) Evaluating
    discourse in structured text representations. arXiv preprint arXiv:190601472
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ficler and Goldberg (2017) Ficler J, Goldberg Y (2017) Controlling linguistic
    style aspects in neural language generation. arXiv preprint arXiv:170702633
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finn et al. (2017) Finn C, Abbeel P, Levine S (2017) Model-agnostic meta-learning
    for fast adaptation of deep networks. In: International Conference on Machine
    Learning, PMLR, pp 1126–1135'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fung et al. (2016) Fung P, Dey A, Siddique FB, Lin R, Yang Y, Bertero D, Wan
    Y, Chan RHY, Wu CS (2016) Zara: a virtual interactive dialogue system incorporating
    emotion, sentiment and personality recognition. In: Proceedings of COLING 2016,
    the 26th International Conference on Computational Linguistics: System Demonstrations,
    pp 278–281'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Galley et al. (2015) Galley M, Brockett C, Sordoni A, Ji Y, Auli M, Quirk C,
    Mitchell M, Gao J, Dolan B (2015) deltableu: A discriminative metric for generation
    tasks with intrinsically diverse targets. arXiv preprint arXiv:150606863'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gan et al. (2019) Gan Z, Cheng Y, Kholy AE, Li L, Liu J, Gao J (2019) Multi-step
    reasoning via recurrent dual attention for visual dialog. arXiv preprint arXiv:190200579
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gan et al. (2020) Gan Z, Chen YC, Li L, Zhu C, Cheng Y, Liu J (2020) Large-scale
    adversarial training for vision-and-language representation learning. arXiv preprint
    arXiv:200606195
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gangadharaiah and Narayanaswamy (2020) Gangadharaiah R, Narayanaswamy B (2020)
    Recursive template-based frame generation for task oriented dialog. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    2059–2064'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2018) Gao J, Galley M, Li L (2018) Neural approaches to conversational
    ai. In: The 41st International ACM SIGIR Conference on Research & Development
    in Information Retrieval, pp 1371–1374'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020a) Gao S, Zhang Y, Ou Z, Yu Z (2020a) Paraphrase augmented task-oriented
    dialog generation. arXiv preprint arXiv:200407462
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2019) Gao X, Zhang Y, Lee S, Galley M, Brockett C, Gao J, Dolan
    B (2019) Structuring latent spaces for stylized response generation. arXiv preprint
    arXiv:190905361
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2020b) Gao X, Zhang Y, Galley M, Brockett C, Dolan B (2020b) Dialogue
    response ranking training with large-scale human feedback data. arXiv preprint
    arXiv:200906978
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2020c) Gao Y, Wu CS, Joty S, Xiong C, Socher R, King I, Lyu M,
    Hoi SC (2020c) Explicit memory tracker with coarse-to-fine reasoning for conversational
    machine reading. In: Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics, pp 935–945'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gehring et al. (2017) Gehring J, Auli M, Grangier D, Yarats D, Dauphin YN (2017)
    Convolutional sequence to sequence learning. In: International Conference on Machine
    Learning, PMLR, pp 1243–1252'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ghazvininejad et al. (2018) Ghazvininejad M, Brockett C, Chang MW, Dolan B,
    Gao J, Yih Wt, Galley M (2018) A knowledge-grounded neural conversation model.
    In: Proceedings of the AAAI Conference on Artificial Intelligence, vol 32'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gliwa et al. (2019) Gliwa B, Mochol I, Biesek M, Wawer A (2019) Samsum corpus:
    A human-annotated dialogue dataset for abstractive summarization. arXiv preprint
    arXiv:191112237'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goddeau et al. (1996) Goddeau D, Meng H, Polifroni J, Seneff S, Busayapongchai
    S (1996) A form-based dialogue manager for spoken language applications. In: Proceeding
    of Fourth International Conference on Spoken Language Processing. ICSLP’96, IEEE,
    vol 2, pp 701–704'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Golovanov et al. (2019) Golovanov S, Kurbanov R, Nikolenko S, Truskovskyi K,
    Tselousov A, Wolf T (2019) Large-scale transfer learning for natural language
    generation. In: Proceedings of the 57th Annual Meeting of the Association for
    Computational Linguistics, pp 6053–6058'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley
    D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial networks. arXiv
    preprint arXiv:14062661
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gopalakrishnan et al. (2019) Gopalakrishnan K, Hedayatnia B, Chen Q, Gottardi
    A, Kwatra S, Venkatesh A, Gabriel R, Hakkani-Tür D, AI AA (2019) Topical-chat:
    Towards knowledge-grounded open-domain conversations. In: INTERSPEECH, pp 1891–1895'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gordon-Hall et al. (2020) Gordon-Hall G, Gorinski PJ, Cohen SB (2020) Learning
    dialog policies from weak demonstrations. arXiv preprint arXiv:200411054
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves et al. (2014) Graves A, Wayne G, Danihelka I (2014) Neural turing machines.
    arXiv preprint arXiv:14105401
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves et al. (2016) Graves A, Wayne G, Reynolds M, Harley T, Danihelka I, Grabska-Barwińska
    A, Colmenarejo SG, Grefenstette E, Ramalho T, Agapiou J, et al. (2016) Hybrid
    computing using a neural network with dynamic external memory. Nature 538(7626):471–476
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gruber and Jockisch (2020) Gruber N, Jockisch A (2020) Are gru cells more specific
    and lstm cells more sensitive in motive classification of text? Frontiers in Artificial
    Intelligence 3(40):1–6
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2016) Gu J, Lu Z, Li H, Li VO (2016) Incorporating copying mechanism
    in sequence-to-sequence learning. In: Proceedings of the 54th Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), Association
    for Computational Linguistics, Berlin, Germany, pp 1631–1640, DOI 10.18653/v1/P16-1154,
    URL [https://www.aclweb.org/anthology/P16-1154](https://www.aclweb.org/anthology/P16-1154)'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019) Guo Q, Qiu X, Liu P, Shao Y, Xue X, Zhang Z (2019) Star-transformer.
    arXiv preprint arXiv:190209113
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2020) Guo X, Yu M, Gao Y, Gan C, Campbell M, Chang S (2020) Interactive
    fiction game playing as multi-paragraph reading comprehension with reinforcement
    learning. arXiv preprint arXiv:201002386
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gür et al. (2018) Gür I, Hakkani-Tür D, Tür G, Shah P (2018) User modeling
    for task oriented dialogues. In: 2018 IEEE Spoken Language Technology Workshop
    (SLT), IEEE, pp 900–906'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haber et al. (2019) Haber J, Baumgärtner T, Takmaz E, Gelderloos L, Bruni E,
    Fernández R (2019) The photobook dataset: Building common ground through visually-grounded
    dialogue. arXiv preprint arXiv:190601530'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hahn et al. (2020) Hahn M, Krantz J, Batra D, Parikh D, Rehg JM, Lee S, Anderson
    P (2020) Where are you? localization from embodied dialog. arXiv preprint arXiv:201108277
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hakkani-Tür et al. (2016) Hakkani-Tür D, Tür G, Celikyilmaz A, Chen YN, Gao
    J, Deng L, Wang YY (2016) Multi-domain joint semantic frame parsing using bi-directional
    rnn-lstm. In: Interspeech, pp 715–719'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ham et al. (2020) Ham D, Lee JG, Jang Y, Kim KE (2020) End-to-end neural pipeline
    for goal-oriented dialogue systems using gpt-2\. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 583–592'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2019) Han M, Kang M, Jung H, Hwang SJ (2019) Episodic memory reader:
    Learning what to remember for question answering from streaming data. arXiv preprint
    arXiv:190306164'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hancock et al. (2019) Hancock B, Bordes A, Mazare PE, Weston J (2019) Learning
    from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:190105415'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hashemi et al. (2016) Hashemi HB, Asiaee A, Kraft R (2016) Query intent detection
    using convolutional neural networks. In: International Conference on Web Search
    and Data Mining, Workshop on Query Understanding'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2017) He H, Balakrishnan A, Eric M, Liang P (2017) Learning symmetric
    collaborative dialogue agents with dynamic knowledge graph embeddings. arXiv preprint
    arXiv:170407130
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2016) He K, Zhang X, Ren S, Sun J (2016) Deep residual learning
    for image recognition. In: Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp 770–778'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He and Glass (2019) He T, Glass J (2019) Negative training for neural dialogue
    response generation. arXiv preprint arXiv:190302134
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020a) He W, Yang M, Yan R, Li C, Shen Y, Xu R (2020a) Amalgamating
    knowledge from two teachers for task-oriented dialogue system with adversarial
    training. In: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pp 3498–3507'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2020b) He X, Chen S, Ju Z, Dong X, Fang H, Wang S, Yang Y, Zeng
    J, Zhang R, Zhang R, et al. (2020b) Meddialog: Two large-scale medical dialogue
    datasets. arXiv e-prints pp arXiv–2004'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson et al. (2008) Henderson J, Lemon O, Georgila K (2008) Hybrid reinforcement/supervised
    learning of dialogue policies from fixed data sets. Computational Linguistics
    34(4):487–511
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henderson (2015) Henderson M (2015) Machine learning for dialog state tracking:
    A review. In: Proceedings of The First International Workshop on Machine Learning
    in Spoken Language Processing'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henderson et al. (2013) Henderson M, Thomson B, Young S (2013) Deep neural
    network approach for the dialog state tracking challenge. In: Proceedings of the
    SIGDIAL 2013 Conference, pp 467–471'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henderson et al. (2014a) Henderson M, Thomson B, Williams JD (2014a) The second
    dialog state tracking challenge. In: Proceedings of the 15th annual meeting of
    the special interest group on discourse and dialogue (SIGDIAL), pp 263–272'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henderson et al. (2014b) Henderson M, Thomson B, Williams JD (2014b) The third
    dialog state tracking challenge. In: 2014 IEEE Spoken Language Technology Workshop
    (SLT), IEEE, pp 324–329'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson et al. (2019a) Henderson M, Budzianowski P, Casanueva I, Coope S,
    Gerz D, Kumar G, Mrkšić N, Spithourakis G, Su PH, Vulić I, et al. (2019a) A repository
    of conversational datasets. arXiv preprint arXiv:190406472
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henderson et al. (2019b) Henderson M, Vulić I, Gerz D, Casanueva I, Budzianowski
    P, Coope S, Spithourakis G, Wen TH, Mrkšić N, Su PH (2019b) Training neural response
    selection for task-oriented dialogue systems. arXiv preprint arXiv:190601543
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) Hochreiter S, Schmidhuber J (1997) Long short-term
    memory. Neural computation 9(8):1735–1780
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter et al. (2001) Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J,
    et al. (2001) Gradient flow in recurrent nets: the difficulty of learning long-term
    dependencies'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hokamp and Liu (2017) Hokamp C, Liu Q (2017) Lexically constrained decoding
    for sequence generation using grid beam search. arXiv preprint arXiv:170407138
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopfield (1982) Hopfield JJ (1982) Neural networks and physical systems with
    emergent collective computational abilities. Proceedings of the national academy
    of sciences 79(8):2554–2558
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosseini-Asl et al. (2020) Hosseini-Asl E, McCann B, Wu CS, Yavuz S, Socher
    R (2020) A simple language model for task-oriented dialogue. arXiv preprint arXiv:200500796
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020) Hu J, Yang Y, Chen C, Yu Z, et al. (2020) Sas: Dialogue state
    tracking via slot attention and slot information sharing. In: Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, pp 6366–6375'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2019) Hu JE, Rudinger R, Post M, Van Durme B (2019) Parabank: Monolingual
    bitext generation and sentential paraphrasing via lexically-constrained neural
    machine translation. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 33, pp 6521–6528'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2017) Hu Z, Yang Z, Liang X, Salakhutdinov R, Xing EP (2017) Toward
    controlled generation of text. In: International Conference on Machine Learning,
    PMLR, pp 1587–1596'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hua and Wang (2019) Hua X, Wang L (2019) Sentence-level content planning and
    style specification for neural text generation. arXiv preprint arXiv:190900734
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hua et al. (2020) Hua Y, Li YF, Haffari G, Qi G, Wu T (2020) Few-shot complex
    knowledge base question answering via meta reinforcement learning. arXiv preprint
    arXiv:201015877
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020a) Huang L, Ye Z, Qin J, Lin L, Liang X (2020a) Grade: Automatic
    graph-enhanced coherence metric for evaluating open-domain dialogue systems. arXiv
    preprint arXiv:201003994'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) Huang X, Jiang J, Zhao D, Feng Y, Hong Y (2018) Natural
    Language Processing and Chinese Computing: 6th CCF International Conference, NLPCC
    2017, Dalian, China, November 8–12, 2017, Proceedings, vol 10619\. Springer'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2020b) Huang X, Qi J, Sun Y, Zhang R (2020b) Semi-supervised dialogue
    policy learning via stochastic reward estimation. arXiv preprint arXiv:200504379
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020c) Huang Y, Feng J, Hu M, Wu X, Du X, Ma S (2020c) Meta-reinforced
    multi-domain state generator for dialogue systems. In: Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics, pp 7109–7118'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020d) Huang Z, Zeng Z, Liu B, Fu D, Fu J (2020d) Pixel-bert:
    Aligning image pixels with text by deep multi-modal transformers. arXiv preprint
    arXiv:200400849'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. (2016) Jaderberg M, Mnih V, Czarnecki WM, Schaul T, Leibo JZ,
    Silver D, Kavukcuoglu K (2016) Reinforcement learning with unsupervised auxiliary
    tasks. arXiv preprint arXiv:161105397
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jang et al. (2017) Jang Y, Song Y, Yu Y, Kim Y, Kim G (2017) Tgif-qa: Toward
    spatio-temporal reasoning in visual question answering. In: Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition, pp 2758–2766'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaques et al. (2020) Jaques N, Shen JH, Ghandeharioun A, Ferguson C, Lapedriza
    A, Jones N, Gu SS, Picard R (2020) Human-centric dialog training via offline reinforcement
    learning. arXiv preprint arXiv:201005848
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ji et al. (2020) Ji C, Zhou X, Zhang Y, Liu X, Sun C, Zhu C, Zhao T (2020) Cross
    copy network for dialogue generation. arXiv preprint arXiv:201011539
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2015) Ji G, He S, Xu L, Liu K, Zhao J (2015) Knowledge graph embedding
    via dynamic mapping matrix. In: Proceedings of the 53rd annual meeting of the
    association for computational linguistics and the 7th international joint conference
    on natural language processing (volume 1: Long papers), pp 687–696'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ji et al. (2022) Ji S, Pan S, Cambria E, Marttinen P, Yu PS (2022) A survey
    on knowledge graphs: Representation, acquisition and applications. IEEE Transactions
    on Neural Networks and Learning Systems 33(10)'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2020) Jia Q, Liu Y, Ren S, Zhu KQ, Tang H (2020) Multi-turn response
    selection using dialogue dependency relations. arXiv preprint arXiv:201001502
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jordan (1986) Jordan M (1986) Serial order: a parallel distributed processing
    approach. technical report, june 1985-march 1986\. Tech. rep., California Univ.,
    San Diego, La Jolla (USA). Inst. for Cognitive Science'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jung et al. (2020) Jung J, Son B, Lyu S (2020) Attnio: Knowledge graph exploration
    with in-and-out attention flow for knowledge-grounded dialogue. In: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pp 3484–3497'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jurafsky (1997) Jurafsky D (1997) Switchboard swbd-damsl shallow-discourse-function
    annotation coders manual. Institute of Cognitive Science Technical Report
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kale and Rastogi (2020) Kale M, Rastogi A (2020) Template guided text generation
    for task-oriented dialogue. In: Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP), Association for Computational
    Linguistics, Online, pp 6505–6520, DOI 10.18653/v1/2020.emnlp-main.527, URL [https://www.aclweb.org/anthology/2020.emnlp-main.527](https://www.aclweb.org/anthology/2020.emnlp-main.527)'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamezawa et al. (2020) Kamezawa H, Nishida N, Shimizu N, Miyazaki T, Nakayama
    H (2020) A visually-grounded first-person dialogue dataset with verbal and non-verbal
    responses. In: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pp 3299–3310'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kannan and Vinyals (2017) Kannan A, Vinyals O (2017) Adversarial evaluation
    of dialogue models. arXiv preprint arXiv:170108198
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keskar et al. (2019) Keskar NS, McCann B, Varshney LR, Xiong C, Socher R (2019)
    Ctrl: A conditional transformer language model for controllable generation. arXiv
    preprint arXiv:190905858'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2018) Kim A, Song HJ, Park SB, et al. (2018) A two-step neural dialog
    state tracker for task-oriented dialog processing. Computational intelligence
    and neuroscience 2018
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020) Kim H, Kim B, Kim G (2020) Will i sound like me? improving
    persona consistency in dialogues through pragmatic self-consciousness. In: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    pp 904–916'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2016) Kim S, D’Haro LF, Banchs RE, Williams JD, Henderson M, Yoshino
    K (2016) The fifth dialog state tracking challenge. In: 2016 IEEE Spoken Language
    Technology Workshop (SLT), IEEE, pp 511–517'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2017) Kim S, D’Haro LF, Banchs RE, Williams JD, Henderson M (2017)
    The fourth dialog state tracking challenge. In: Dialogues with Social Robots,
    Springer, pp 435–449'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2019) Kim S, Yang S, Kim G, Lee SW (2019) Efficient dialogue state
    tracking by selectively overwriting memory. arXiv preprint arXiv:191103906
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ko et al. (2020) Ko WJ, Ray A, Shen Y, Jin H (2020) Generating dialogue responses
    from a semantic latent space. arXiv preprint arXiv:201001658
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Konda and Tsitsiklis (2000) Konda VR, Tsitsiklis JN (2000) Actor-critic algorithms.
    In: Advances in neural information processing systems, Citeseer, pp 1008–1014'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky A, Sutskever I, Hinton GE (2012) Imagenet
    classification with deep convolutional neural networks. Advances in neural information
    processing systems 25:1097–1105
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kummerfeld et al. (2018) Kummerfeld JK, Gouravajhala SR, Peper J, Athreya V,
    Gunasekara C, Ganhotra J, Patel SS, Polymenakos L, Lasecki WS (2018) A large-scale
    corpus for conversation disentanglement. arXiv preprint arXiv:181011118
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kundu et al. (2020) Kundu S, Lin Q, Ng HT (2020) Learning to identify follow-up
    questions in conversational question answering. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 959–968'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurach et al. (2015) Kurach K, Andrychowicz M, Sutskever I (2015) Neural random-access
    machines. arXiv preprint arXiv:151106392
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larson et al. (2019) Larson S, Mahendran A, Peper JJ, Clarke C, Lee A, Hill
    P, Kummerfeld JK, Leach K, Laurenzano MA, Tang L, et al. (2019) An evaluation
    dataset for intent classification and out-of-scope prediction. arXiv preprint
    arXiv:190902027
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le and Hoi (2020) Le H, Hoi SC (2020) Video-grounded dialogues with pretrained
    generation language models. arXiv preprint arXiv:200615319
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Le et al. (2019) Le H, Sahoo D, Chen NF, Hoi SC (2019) Multimodal transformer
    networks for end-to-end video-grounded dialogue systems. arXiv preprint arXiv:190701166
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. (2020a) Le H, Sahoo D, Chen NF, Hoi SC (2020a) Bist: Bi-directional
    spatio-temporal reasoning for video-grounded dialogues. arXiv preprint arXiv:201010095'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le et al. (2020b) Le H, Sahoo D, Liu C, Chen NF, Hoi SC (2020b) Uniconv: A
    unified conversational neural architecture for multi-domain task-oriented dialogues.
    arXiv preprint arXiv:200414307'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based
    learning applied to document recognition. Proceedings of the IEEE 86(11):2278–2324
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee and Dernoncourt (2016) Lee JY, Dernoncourt F (2016) Sequential short-text
    classification with recurrent and convolutional neural networks. arXiv preprint
    arXiv:160303827
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee (2013) Lee S (2013) Structured discriminative model for dialog state tracking.
    In: Proceedings of the SIGDIAL 2013 Conference, pp 442–451'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee and Eskenazi (2013) Lee S, Eskenazi M (2013) Recipe for building robust
    spoken dialog state trackers: Dialog state tracking challenge system description.
    In: Proceedings of the SIGDIAL 2013 Conference, pp 414–422'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee and Jha (2019) Lee S, Jha R (2019) Zero-shot adaptive transfer for conversational
    language understanding. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 33, pp 6642–6649'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2019) Lee S, Schulz H, Atkinson A, Gao J, Suleman K, El Asri L,
    Adada M, Huang M, Sharma S, Tay W, et al. (2019) Multi-domain task-completion
    dialog challenge. Dialog system technology challenges 8:9
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei et al. (2018) Lei W, Jin X, Kan MY, Ren Z, He X, Yin D (2018) Sequicity:
    Simplifying task-oriented dialogue systems with single sequence-to-sequence architectures.
    In: Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pp 1437–1447'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lemon and Pietquin (2007) Lemon O, Pietquin O (2007) Machine learning for spoken
    dialogue systems. In: Eighth Annual Conference of the International Speech Communication
    Association'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020a) Li G, Duan N, Fang Y, Gong M, Jiang D (2020a) Unicoder-vl:
    A universal encoder for vision and language by cross-modal pre-training. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 34, pp 11336–11344'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2015) Li J, Galley M, Brockett C, Gao J, Dolan B (2015) A diversity-promoting
    objective function for neural conversation models. arXiv preprint arXiv:151003055
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016a) Li J, Miller AH, Chopra S, Ranzato M, Weston J (2016a) Dialogue
    learning with human-in-the-loop. arXiv preprint arXiv:161109823
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016b) Li J, Miller AH, Chopra S, Ranzato M, Weston J (2016b) Learning
    through dialogue interactions by asking questions. arXiv preprint arXiv:161204936
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016c) Li J, Monroe W, Jurafsky D (2016c) A simple, fast diverse
    decoding algorithm for neural generation. arXiv preprint arXiv:161108562
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016d) Li J, Monroe W, Ritter A, Galley M, Gao J, Jurafsky D (2016d)
    Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:160601541
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017a) Li J, Monroe W, Shi T, Jean S, Ritter A, Jurafsky D (2017a)
    Adversarial learning for neural dialogue generation. arXiv preprint arXiv:170106547
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020b) Li L, Xu C, Wu W, Zhao Y, Zhao X, Tao C (2020b) Zero-resource
    knowledge-grounded dialogue generation. arXiv preprint arXiv:200812918
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Li LH, Yatskar M, Yin D, Hsieh CJ, Chang KW (2019a) Visualbert:
    A simple and performant baseline for vision and language. arXiv preprint arXiv:190803557'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019b) Li M, Roller S, Kulikov I, Welleck S, Boureau YL, Cho K, Weston
    J (2019b) Don’t say that! making inconsistent dialogue unlikely with unlikelihood
    training. arXiv preprint arXiv:191103860
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2016e) Li X, Lipton ZC, Dhingra B, Li L, Gao J, Chen YN (2016e) A
    user simulator for task-completion dialogues. arXiv preprint arXiv:161205688
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017b) Li X, Chen YN, Li L, Gao J, Celikyilmaz A (2017b) End-to-end
    task-completion neural dialogue systems. arXiv preprint arXiv:170301008
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Li X, Wang Y, Sun S, Panda S, Liu J, Gao J (2018) Microsoft
    dialogue challenge: Building end-to-end task-completion dialogue systems. arXiv
    preprint arXiv:180711125'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019c) Li X, Yin F, Sun Z, Li X, Yuan A, Chai D, Zhou M, Li J (2019c)
    Entity-relation extraction as multi-turn question answering. arXiv preprint arXiv:190505529
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020c) Li X, Yin X, Li C, Zhang P, Hu X, Zhang L, Wang L, Hu H,
    Dong L, Wei F, et al. (2020c) Oscar: Object-semantics aligned pre-training for
    vision-language tasks. In: European Conference on Computer Vision, Springer, pp
    121–137'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li (2017) Li Y (2017) Deep reinforcement learning: An overview. arXiv preprint
    arXiv:170107274'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2017c) Li Y, Su H, Shen X, Li W, Cao Z, Niu S (2017c) Dailydialog:
    A manually labelled multi-turn dialogue dataset. arXiv preprint arXiv:171003957'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020d) Li Y, Yao K, Qin L, Che W, Li X, Liu T (2020d) Slot-consistent
    nlg for task-oriented dialogue systems with iterative rectification network. In:
    Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
    pp 97–106'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019d) Li Z, Niu C, Meng F, Feng Y, Li Q, Zhou J (2019d) Incremental
    transformer with deliberation decoder for document grounded conversations. arXiv
    preprint arXiv:190708854
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2020) Liang W, Zou J, Yu Z (2020) Beyond user self-reported likert
    scale ratings: A comparison model for automatic dialog evaluation. arXiv preprint
    arXiv:200510716'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Lin CY (2004) Rouge: A package for automatic evaluation of summaries.
    In: Text summarization branches out, pp 74–81'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin (1992) Lin LJ (1992) Self-improving reactive agents based on reinforcement
    learning, planning and teaching. Machine learning 8(3-4):293–321
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) Lin T, Wang Y, Liu X, Qiu X (2021) A survey of transformers.
    arXiv preprint arXiv:210604554
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2019) Lin X, Joty S, Jwalapuram P, Bari MS (2019) A unified linear-time
    framework for sentence-level discourse parsing. arXiv preprint arXiv:190505682
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020a) Lin X, Jian W, He J, Wang T, Chu W (2020a) Generating informative
    conversational response using recurrent knowledge-interaction and knowledge-copy.
    In: Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pp 41–52'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2015) Lin Y, Liu Z, Sun M, Liu Y, Zhu X (2015) Learning entity
    and relation embeddings for knowledge graph completion. In: Proceedings of the
    AAAI Conference on Artificial Intelligence, vol 29'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020b) Lin Z, Cai D, Wang Y, Liu X, Zheng H, Shi S (2020b) The
    world is not binary: Learning to rank with grayscale data for dialogue response
    selection. In: Proceedings of the 2020 Conference on Empirical Methods in Natural
    Language Processing (EMNLP), pp 9220–9229'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020c) Lin Z, Madotto A, Winata GI, Fung P (2020c) Mintl: Minimalist
    transfer learning for task-oriented dialogue systems. arXiv preprint arXiv:200912005'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lipton et al. (2015) Lipton ZC, Berkowitz J, Elkan C (2015) A critical review
    of recurrent neural networks for sequence learning. arXiv preprint arXiv:150600019
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lison and Bibauw (2017) Lison P, Bibauw S (2017) Not all dialogues are created
    equal: Instance weighting for neural conversational models. arXiv preprint arXiv:170408966'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Lane (2016) Liu B, Lane I (2016) Attention-based recurrent neural network
    models for joint intent detection and slot filling. arXiv preprint arXiv:160901454
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Lane (2017) Liu B, Lane I (2017) Iterative policy learning in end-to-end
    trainable task-oriented neural dialog models. In: 2017 IEEE Automatic Speech Recognition
    and Understanding Workshop (ASRU), IEEE, pp 482–489'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Liu C, He S, Liu K, Zhao J (2019) Vocabulary pyramid network:
    Multi-pass encoding and decoding with multi-level vocabularies for response generation.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 3774–3783'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2016) Liu CW, Lowe R, Serban IV, Noseworthy M, Charlin L, Pineau
    J (2016) How not to evaluate your dialogue system: An empirical study of unsupervised
    evaluation metrics for dialogue response generation. arXiv preprint arXiv:160308023'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Liu H, Wang W, Wang Y, Liu H, Liu Z, Tang J (2020a) Mitigating
    gender bias for neural dialogue generation with adversarial learning. arXiv preprint
    arXiv:200913028
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2020b) Liu Q, Chen Y, Chen B, Lou JG, Chen Z, Zhou B, Zhang D (2020b)
    You impress me: Dialogue generation via mutual persona perception. arXiv preprint
    arXiv:200405388'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Lapata (2018) Liu Y, Lapata M (2018) Learning structured text representations.
    Transactions of the Association for Computational Linguistics 6:63–75
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020c) Liu Z, Wang H, Niu ZY, Wu H, Che W, Liu T (2020c) Towards
    conversational recommendation over multi-type dialogs. arXiv preprint arXiv:200503954
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lowe et al. (2015) Lowe R, Pow N, Serban I, Pineau J (2015) The ubuntu dialogue
    corpus: A large dataset for research in unstructured multi-turn dialogue systems.
    arXiv preprint arXiv:150608909'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lowe et al. (2017) Lowe R, Noseworthy M, Serban IV, Angelard-Gontier N, Bengio
    Y, Pineau J (2017) Towards an automatic turing test: Learning to evaluate dialogue
    responses. arXiv preprint arXiv:170807149'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2019a) Lu J, Batra D, Parikh D, Lee S (2019a) Vilbert: Pretraining
    task-agnostic visiolinguistic representations for vision-and-language tasks. arXiv
    preprint arXiv:190802265'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2019b) Lu J, Zhang C, Xie Z, Ling G, Zhou TC, Xu Z (2019b) Constructing
    interpretive spatio-temporal features for multi-turn responses selection. In:
    Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
    pp 44–50'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2020) Lu J, Goswami V, Rohrbach M, Parikh D, Lee S (2020) 12-in-1:
    Multi-task vision and language representation learning. In: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp 10437–10446'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lubis et al. (2018) Lubis N, Sakti S, Yoshino K, Nakamura S (2018) Eliciting
    positive emotion through affect-sensitive dialogue response generation: A neural
    network approach. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 32, no 1'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2019) Ma MD, Bowden KK, Wu J, Cui W, Walker M (2019) Implicit discourse
    relation identification for open-domain dialogues. arXiv preprint arXiv:190703975
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020a) Ma W, Cui Y, Liu T, Wang D, Wang S, Hu G (2020a) Conversational
    word embedding for retrieval-based dialog system. arXiv preprint arXiv:200413249
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2020b) Ma Y, Nguyen KL, Xing FZ, Cambria E (2020b) A survey on empathetic
    dialogue systems. Information Fusion 64:50–70
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madotto et al. (2019) Madotto A, Lin Z, Wu CS, Fung P (2019) Personalizing
    dialogue agents via meta-learning. In: Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics, pp 5454–5459'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Majumder et al. (2020a) Majumder BP, Jhamtani H, Berg-Kirkpatrick T, McAuley
    J (2020a) Like hiking? you probably enjoy nature: Persona-grounded dialog with
    commonsense expansions. arXiv preprint arXiv:201003205'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Majumder et al. (2020b) Majumder BP, Li S, Ni J, McAuley J (2020b) Interview:
    Large-scale modeling of media dialog with discourse patterns and knowledge grounding.
    In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP), pp 8129–8141'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mallios and Bourbakis (2016) Mallios S, Bourbakis N (2016) A survey on human
    machine dialogue systems. In: 2016 7th international conference on information,
    intelligence, systems & applications (iisa), IEEE, pp 1–7'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manuvina-kurike et al. (2018) Manuvina-kurike R, Brixey J, Bui T, Chang W,
    Artstein R, Georgila K (2018) Dialedit: Annotations for spoken conversational
    image editing. In: Proceedings 14th Joint ACL-ISO Workshop on Interoperable Semantic
    Annotation, pp 1–9'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. (2020) Mao HH, Li S, McAuley J, Cottrell G (2020) Speech recognition
    and multi-speaker diarization of long conversations. arXiv preprint arXiv:200508072
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehri and Eskenazi (2020) Mehri S, Eskenazi M (2020) Usr: An unsupervised and
    reference free evaluation metric for dialog generation. arXiv preprint arXiv:200500456'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehri et al. (2019) Mehri S, Razumovskaia E, Zhao T, Eskenazi M (2019) Pretraining
    methods for dialog context representation learning. arXiv preprint arXiv:190600414
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mesgar et al. (2019) Mesgar M, Bücker S, Gurevych I (2019) Dialogue coherence
    assessment without explicit dialogue act labels. arXiv preprint arXiv:190808486
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mesnil et al. (2013) Mesnil G, He X, Deng L, Bengio Y (2013) Investigation
    of recurrent-neural-network architectures and learning methods for spoken language
    understanding. In: Interspeech, pp 3771–3775'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mesnil et al. (2014) Mesnil G, Dauphin Y, Yao K, Bengio Y, Deng L, Hakkani-Tur
    D, He X, Heck L, Tur G, Yu D, et al. (2014) Using recurrent neural networks for
    slot filling in spoken language understanding. IEEE/ACM Transactions on Audio,
    Speech, and Language Processing 23(3):530–539
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miao et al. (2019) Miao N, Zhou H, Mou L, Yan R, Li L (2019) Cgmh: Constrained
    sentence generation by metropolis-hastings sampling. In: Proceedings of the AAAI
    Conference on Artificial Intelligence, vol 33, pp 6834–6842'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miech et al. (2020) Miech A, Alayrac JB, Smaira L, Laptev I, Sivic J, Zisserman
    A (2020) End-to-end learning of visual representations from uncurated instructional
    videos. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp 9879–9889'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miller et al. (2016) Miller A, Fisch A, Dodge J, Karimi AH, Bordes A, Weston
    J (2016) Key-value memory networks for directly reading documents. arXiv preprint
    arXiv:160603126
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirowski et al. (2016) Mirowski P, Pascanu R, Viola F, Soyer H, Ballard AJ,
    Banino A, Denil M, Goroshin R, Sifre L, Kavukcuoglu K, et al. (2016) Learning
    to navigate in complex environments. arXiv preprint arXiv:161103673
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare
    MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski G, et al. (2015) Human-level
    control through deep reinforcement learning. nature 518(7540):529–533
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mnih et al. (2016) Mnih V, Badia AP, Mirza M, Graves A, Lillicrap T, Harley
    T, Silver D, Kavukcuoglu K (2016) Asynchronous methods for deep reinforcement
    learning. In: International conference on machine learning, PMLR, pp 1928–1937'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mo et al. (2018) Mo K, Zhang Y, Li S, Li J, Yang Q (2018) Personalizing a dialogue
    system with transfer reinforcement learning. In: Proceedings of the AAAI Conference
    on Artificial Intelligence, vol 32, no 1'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moghe et al. (2018) Moghe N, Arora S, Banerjee S, Khapra MM (2018) Towards exploiting
    background knowledge for building conversation systems. arXiv preprint arXiv:180908205
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mohammad et al. (2018) Mohammad S, Bravo-Marquez F, Salameh M, Kiritchenko
    S (2018) Semeval-2018 task 1: Affect in tweets. In: Proceedings of the 12th international
    workshop on semantic evaluation, pp 1–17'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moon et al. (2019) Moon S, Shah P, Kumar A, Subba R (2019) Opendialkg: Explainable
    conversational reasoning with attention-based walks over knowledge graphs. In:
    Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
    pp 845–854'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mostafazadeh et al. (2017) Mostafazadeh N, Brockett C, Dolan B, Galley M, Gao
    J, Spithourakis GP, Vanderwende L (2017) Image-grounded conversations: Multimodal
    context for natural question and response generation. arXiv preprint arXiv:170108251'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mrkšić et al. (2015) Mrkšić N, Séaghdha DO, Thomson B, Gašić M, Su PH, Vandyke
    D, Wen TH, Young S (2015) Multi-domain dialog state tracking using recurrent neural
    networks. arXiv preprint arXiv:150607190
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mrkšić et al. (2016) Mrkšić N, Séaghdha DO, Wen TH, Thomson B, Young S (2016)
    Neural belief tracker: Data-driven dialogue state tracking. arXiv preprint arXiv:160603777'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nakov et al. (2019) Nakov P, Màrquez L, Magdy W, Moschitti A, Glass J, Randeree
    B (2019) Semeval-2015 task 3: Answer selection in community question answering.
    arXiv preprint arXiv:191111403'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nickel et al. (2016) Nickel M, Rosasco L, Poggio T (2016) Holographic embeddings
    of knowledge graphs. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 30, no 1'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novikova et al. (2017) Novikova J, Dušek O, Rieser V (2017) The e2e dataset:
    New challenges for end-to-end generation. arXiv preprint arXiv:170609254'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oraby et al. (2019) Oraby S, Harrison V, Ebrahimi A, Walker M (2019) Curate
    and generate: A corpus and method for joint control of semantics and style in
    neural nlg. arXiv preprint arXiv:190601334'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oriol et al. (2015) Oriol V, Meire F, Navdeep J (2015) Pointer networks. Advances
    in neural information processing systems 28:2692–2700
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2020) Ouyang Y, Chen M, Dai X, Zhao Y, Huang S, Jiajun C (2020)
    Dialogue state tracking with explicit slot connection modeling. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    34–40'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Panayotov et al. (2015) Panayotov V, Chen G, Povey D, Khudanpur S (2015) Librispeech:
    an asr corpus based on public domain audio books. In: 2015 IEEE international
    conference on acoustics, speech and signal processing (ICASSP), IEEE, pp 5206–5210'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang et al. (2020) Pang B, Nijkamp E, Han W, Zhou L, Liu Y, Tu K (2020) Towards
    holistic and automatic evaluation of open-domain dialogue generation. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    3619–3629'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Papineni K, Roukos S, Ward T, Zhu WJ (2002) Bleu: a
    method for automatic evaluation of machine translation. In: Proceedings of the
    40th annual meeting of the Association for Computational Linguistics, pp 311–318'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parikh et al. (2016) Parikh AP, Täckström O, Das D, Uszkoreit J (2016) A decomposable
    attention model for natural language inference. arXiv preprint arXiv:160601933
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pascanu et al. (2013) Pascanu R, Mikolov T, Bengio Y (2013) On the difficulty
    of training recurrent neural networks. In: International conference on machine
    learning, PMLR, pp 1310–1318'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng et al. (2017) Peng B, Li X, Li L, Gao J, Celikyilmaz A, Lee S, Wong KF
    (2017) Composite task-completion dialogue policy learning via hierarchical deep
    reinforcement learning. arXiv preprint arXiv:170403084
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee
    K, Zettlemoyer L (2018) Deep contextualized word representations. arXiv preprint
    arXiv:180205365
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pfau and Vinyals (2016) Pfau D, Vinyals O (2016) Connecting generative adversarial
    networks and actor-critic methods. arXiv preprint arXiv:161001945
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Poria et al. (2019) Poria S, Hazarika D, Majumder N, Naik G, Cambria E, Mihalcea
    R (2019) MELD: A multimodal multi-party dataset for emotion recognition in conversations.
    In: ACL, pp 527–536'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Powers (2020) Powers DMW (2020) Evaluation: from precision, recall and f-measure
    to roc, informedness, markedness and correlation. ArXiv abs/2010.16061'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al. (2008) Prasad R, Dinesh N, Lee A, Miltsakaki E, Robaldo L, Joshi
    AK, Webber BL (2008) The penn discourse treebank 2.0\. In: LREC, Citeseer'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Puterman (2014) Puterman ML (2014) Markov decision processes: discrete stochastic
    dynamic programming. John Wiley & Sons'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2020) Qi D, Su L, Song J, Cui E, Bharti T, Sacheti A (2020) Imagebert:
    Cross-modal pre-training with large-scale weak-supervised image-text data. arXiv
    preprint arXiv:200107966'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian and Yu (2019) Qian K, Yu Z (2019) Domain adaptive dialog generation via
    meta learning. arXiv preprint arXiv:190603520
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2020) Qin L, Xu X, Che W, Zhang Y, Liu T (2020) Dynamic fusion network
    for multi-domain end-to-end task-oriented dialog. arXiv preprint arXiv:200411019
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2019) Qiu L, Li J, Bi W, Zhao D, Yan R (2019) Are training samples
    correlated? learning to generate dialogue responses with multiple references.
    In: Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pp 3826–3835'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiu et al. (2020) Qiu L, Zhao Y, Shi W, Liang Y, Shi F, Yuan T, Yu Z, Zhu SC
    (2020) Structured attention for unsupervised dialogue structure induction. arXiv
    preprint arXiv:200908552
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qiu et al. (2017) Qiu M, Li FL, Wang S, Gao X, Chen Y, Zhao W, Chen H, Huang
    J, Chu W (2017) Alime chat: A sequence to sequence and rerank based chatbot engine.
    In: Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers), pp 498–503'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quan and Xiong (2020) Quan J, Xiong D (2020) Modeling long context for task-oriented
    dialogue state generation. arXiv preprint arXiv:200414080
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quan et al. (2020) Quan J, Zhang S, Cao Q, Li Z, Xiong D (2020) Risawoz: A
    large-scale multi-domain wizard-of-oz dataset with rich semantic annotations for
    task-oriented dialogue modeling. arXiv preprint arXiv:201008738'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajpurkar et al. (2018) Rajpurkar P, Jia R, Liang P (2018) Know what you don’t
    know: Unanswerable questions for squad. arXiv preprint arXiv:180603822'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ram et al. (2018) Ram A, Prasad R, Khatri C, Venkatesh A, Gabriel R, Liu Q,
    Nunn J, Hedayatnia B, Cheng M, Nagar A, et al. (2018) Conversational ai: The science
    behind the alexa prize. arXiv preprint arXiv:180103604'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rameshkumar and Bailey (2020) Rameshkumar R, Bailey P (2020) Storytelling with
    dialogue: A critical role dungeons and dragons dataset. In: Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, pp 5121–5134'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramshaw and Marcus (1999) Ramshaw LA, Marcus MP (1999) Text chunking using
    transformation-based learning. In: Natural language processing using very large
    corpora, Springer, pp 157–176'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rashkin et al. (2018) Rashkin H, Smith EM, Li M, Boureau YL (2018) Towards
    empathetic open-domain conversation models: A new benchmark and dataset. arXiv
    preprint arXiv:181100207'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastogi et al. (2020) Rastogi A, Zang X, Sunkara S, Gupta R, Khaitan P (2020)
    Towards scalable multi-domain conversational agents: The schema-guided dialogue
    dataset. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol
    34, no 5, pp 8689–8696'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ravuri and Stolcke (2015) Ravuri S, Stolcke A (2015) Recurrent neural network
    and lstm models for lexical utterance classification. In: Sixteenth Annual Conference
    of the International Speech Communication Association'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ravuri and Stolcke (2016) Ravuri S, Stolcke A (2016) A comparative study of
    recurrent neural network models for lexical domain classification. In: 2016 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    IEEE, pp 6075–6079'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rawat and Wang (2017) Rawat W, Wang Z (2017) Deep convolutional neural networks
    for image classification: A comprehensive review. Neural computation 29(9):2352–2449'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reiter (1994) Reiter E (1994) Has a consensus nl generation architecture appeared,
    and is it psycholinguistically plausible? arXiv preprint cmp-lg/9411032
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2013) Ren H, Xu W, Zhang Y, Yan Y (2013) Dialog state tracking
    using conditional random fields. In: Proceedings of the SIGDIAL 2013 Conference,
    pp 457–461'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2018) Ren L, Xie K, Chen L, Yu K (2018) Towards universal dialogue
    state tracking. arXiv preprint arXiv:181009587
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2020) Ren P, Chen Z, Ren Z, Kanoulas E, Monz C, de Rijke M (2020)
    Conversations with search engines. arXiv preprint arXiv:200414162
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ritter et al. (2011) Ritter A, Cherry C, Dolan WB (2011) Data-driven response
    generation in social media. In: Proceedings of the 2011 Conference on Empirical
    Methods in Natural Language Processing, pp 583–593'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rodriguez et al. (2020) Rodriguez P, Crook P, Moon S, Wang Z (2020) Information
    seeking in the spirit of learning: a dataset for conversational curiosity. arXiv
    preprint arXiv:200500172'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. (2018) Saha A, Khapra M, Sankaranarayanan K (2018) Towards building
    large scale multimodal domain-aware conversation systems. In: Proceedings of the
    AAAI Conference on Artificial Intelligence, vol 32, no 1'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. (2020) Saha T, Patra A, Saha S, Bhattacharyya P (2020) Towards
    emotion-aided multi-modal dialogue act classification. In: Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, pp 4361–4372'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sankar et al. (2019) Sankar C, Subramanian S, Pal C, Chandar S, Bengio Y (2019)
    Do neural dialog systems use the conversation history effectively? an empirical
    study. arXiv preprint arXiv:190601603
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santhanam and Shaikh (2019) Santhanam S, Shaikh S (2019) A survey of natural
    language generation techniques with a focus on dialogue systems-past, present
    and future directions. arXiv preprint arXiv:190600500
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarikaya et al. (2011) Sarikaya R, Hinton GE, Ramabhadran B (2011) Deep belief
    nets for natural language call-routing. In: 2011 IEEE International conference
    on acoustics, speech and signal processing (ICASSP), IEEE, pp 5680–5683'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarikaya et al. (2014) Sarikaya R, Hinton GE, Deoras A (2014) Application of
    deep belief networks for natural language understanding. IEEE/ACM Transactions
    on Audio, Speech, and Language Processing 22(4):778–784
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sato et al. (2020) Sato S, Akama R, Ouchi H, Suzuki J, Inui K (2020) Evaluating
    dialogue generation systems via response selection. arXiv preprint arXiv:200414302
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schatzmann and Young (2009) Schatzmann J, Young S (2009) The hidden agenda user
    simulation model. IEEE transactions on audio, speech, and language processing
    17(4):733–747
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schuster and Paliwal (1997) Schuster M, Paliwal KK (1997) Bidirectional recurrent
    neural networks. IEEE transactions on Signal Processing 45(11):2673–2681
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See et al. (2019) See A, Roller S, Kiela D, Weston J (2019) What makes a good
    conversation? how controllable attributes affect human judgments. arXiv preprint
    arXiv:190208654
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Serban et al. (2016) Serban I, Sordoni A, Bengio Y, Courville A, Pineau J (2016)
    Building end-to-end dialogue systems using generative hierarchical neural network
    models. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol
    30, no 1'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Serban et al. (2017a) Serban I, Sordoni A, Lowe R, Charlin L, Pineau J, Courville
    A, Bengio Y (2017a) A hierarchical latent variable encoder-decoder model for generating
    dialogues. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    vol 31, no 1'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serban et al. (2017b) Serban IV, Sankar C, Germain M, Zhang S, Lin Z, Subramanian
    S, Kim T, Pieper M, Chandar S, Ke NR, et al. (2017b) A deep reinforcement learning
    chatbot. arXiv preprint arXiv:170902349
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Serras et al. (2019) Serras M, Torres MI, del Pozo A (2019) Goal-conditioned
    user modeling for dialogue systems using stochastic bi-automata. In: ICPRAM, pp
    128–134'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shah et al. (2018) Shah P, Hakkani-Tür D, Tür G, Rastogi A, Bapna A, Nayak N,
    Heck L (2018) Building a conversational agent overnight with dialogue self-play.
    arXiv preprint arXiv:180104871
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shan et al. (2020) Shan Y, Li Z, Zhang J, Meng F, Feng Y, Niu C, Zhou J (2020)
    A contextual hierarchical attention network with adaptive objective for dialogue
    state tracking. arXiv preprint arXiv:200601554
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shang et al. (2015) Shang L, Lu Z, Li H (2015) Neural responding machine for
    short-text conversation. arXiv preprint arXiv:150302364
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shao et al. (2017) Shao L, Gouws S, Britz D, Goldie A, Strope B, Kurzweil R
    (2017) Generating long and diverse responses with neural conversation models.
    ArXiv abs/1701.03185
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao and Nakashole (2020) Shao Y, Nakashole N (2020) Chartdialogs: Plotting
    from natural language instructions. In: Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics, pp 3559–3574'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2019) Shen L, Feng Y, Zhan H (2019) Modeling semantic relationship
    in multi-turn conversations with hierarchical latent variables. arXiv preprint
    arXiv:190607429
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi and Weninger (2017) Shi B, Weninger T (2017) Proje: Embedding projection
    for knowledge graph completion. In: Proceedings of the AAAI Conference on Artificial
    Intelligence, vol 31, no 1'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuster et al. (2018) Shuster K, Humeau S, Bordes A, Weston J (2018) Image
    chat: Engaging grounded conversations. arXiv preprint arXiv:181100945'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuster et al. (2019) Shuster K, Ju D, Roller S, Dinan E, Boureau YL, Weston
    J (2019) The dialogue dodecathlon: Open-domain knowledge and image grounded conversational
    agents. arXiv preprint arXiv:191103768'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuster et al. (2020) Shuster K, Humeau S, Bordes A, Weston J (2020) Image-chat:
    Engaging grounded conversations. In: Proceedings of the 58th Annual Meeting of
    the Association for Computational Linguistics, Association for Computational Linguistics,
    Online, pp 2414–2429, DOI 10.18653/v1/2020.acl-main.219, URL [https://www.aclweb.org/anthology/2020.acl-main.219](https://www.aclweb.org/anthology/2020.acl-main.219)'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siddharthan (2001) Siddharthan A (2001) Ehud reiter and robert dale. building
    natural language generation systems. Natural Language Engineering 7(3):271
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2016) Silver D, Huang A, Maddison CJ, Guez A, Sifre L, Van Den Driessche
    G, Schrittwieser J, Antonoglou I, Panneershelvam V, Lanctot M, et al. (2016) Mastering
    the game of go with deep neural networks and tree search. nature 529(7587):484–489
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2014) Simonyan K, Zisserman A (2014) Very deep convolutional
    networks for large-scale image recognition. arXiv preprint arXiv:14091556
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2020) Singh A, Goswami V, Parikh D (2020) Are we pretraining it
    right? digging deeper into visio-linguistic pretraining. arXiv preprint arXiv:200408744
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. (2002) Singh S, Litman D, Kearns M, Walker M (2002) Optimizing
    dialogue management with reinforcement learning: Experiments with the njfun system.
    Journal of Artificial Intelligence Research 16:105–133'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singla et al. (2020) Singla K, Chen Z, Atkins D, Narayanan S (2020) Towards
    end-2-end learning for predicting behavior codes from spoken utterances in psychotherapy
    conversations. In: Proceedings of the 58th Annual Meeting of the Association for
    Computational Linguistics, pp 3797–3803'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sinha et al. (2020) Sinha K, Parthasarathi P, Wang J, Lowe R, Hamilton WL, Pineau
    J (2020) Learning an unreferenced metric for online dialogue evaluation. arXiv
    preprint arXiv:200500583
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith et al. (2020) Smith EM, Williamson M, Shuster K, Weston J, Boureau YL
    (2020) Can you put it all together: Evaluating conversational agents’ ability
    to blend skills. arXiv preprint arXiv:200408449'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Socher et al. (2013) Socher R, Chen D, Manning CD, Ng A (2013) Reasoning with
    neural tensor networks for knowledge base completion. In: Advances in neural information
    processing systems, Citeseer, pp 926–934'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2020a) Song H, Wang Y, Zhang WN, Liu X, Liu T (2020a) Generate,
    delete and rewrite: A three-stage framework for improving persona consistency
    of dialogue generation. arXiv preprint arXiv:200407672'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020b) Song H, Wang Y, Zhang WN, Zhao Z, Liu T, Liu X (2020b) Profile
    consistency identification for open-domain dialogue agents. arXiv preprint arXiv:200909680
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2016) Song Y, Yan R, Li X, Zhao D, Zhang M (2016) Two are better
    than one: An ensemble of retrieval-and generation-based dialog systems. arXiv
    preprint arXiv:161007149'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2019) Song Z, Zheng X, Liu L, Xu M, Huang XJ (2019) Generating
    responses with a specific emotion in dialog. In: Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics, pp 3685–3695'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sordoni et al. (2015a) Sordoni A, Bengio Y, Vahabi H, Lioma C, Grue Simonsen
    J, Nie JY (2015a) A hierarchical recurrent encoder-decoder for generative context-aware
    query suggestion. In: Proceedings of the 24th ACM International on Conference
    on Information and Knowledge Management, pp 553–562'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sordoni et al. (2015b) Sordoni A, Galley M, Auli M, Brockett C, Ji Y, Mitchell
    M, Nie JY, Gao J, Dolan B (2015b) A neural network approach to context-sensitive
    generation of conversational responses. arXiv preprint arXiv:150606714
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stasaski et al. (2020) Stasaski K, Yang GH, Hearst MA (2020) More diverse dialogue
    datasets via diversity-informed data collection. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 4958–4968'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stent et al. (2005) Stent A, Marge M, Singhai M (2005) Evaluating evaluation
    methods for generation in the presence of variation. In: international conference
    on intelligent text processing and computational linguistics, Springer, pp 341–351'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2019a) Su H, Shen X, Zhang R, Sun F, Hu P, Niu C, Zhou J (2019a)
    Improving multi-turn dialogue modelling with utterance rewriter. arXiv preprint
    arXiv:190607004
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2020) Su H, Shen X, Zhao S, Zhou X, Hu P, Zhong R, Niu C, Zhou J
    (2020) Diversifying dialogue generation with non-conversational text. arXiv preprint
    arXiv:200504346
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2015) Su PH, Vandyke D, Gasic M, Kim D, Mrksic N, Wen TH, Young
    S (2015) Learning from real users: Rating dialogue success with neural networks
    for reinforcement learning in spoken dialogue systems. arXiv preprint arXiv:150803386'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2016) Su PH, Gasic M, Mrksic N, Rojas-Barahona L, Ultes S, Vandyke
    D, Wen TH, Young S (2016) Continuously learning neural dialogue management. arXiv
    preprint arXiv:160602689
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2019b) Su SY, Huang CW, Chen YN (2019b) Dual supervised learning
    for natural language understanding and generation. arXiv preprint arXiv:190506196
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2019c) Su W, Zhu X, Cao Y, Li B, Lu L, Wei F, Dai J (2019c) Vl-bert:
    Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:190808530'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar et al. (2015) Sukhbaatar S, Szlam A, Weston J, Fergus R (2015) End-to-end
    memory networks. arXiv preprint arXiv:150308895
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019a) Sun C, Baradel F, Murphy K, Schmid C (2019a) Learning video
    representations using contrastive bidirectional transformer. arXiv preprint arXiv:190605743
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2019b) Sun C, Myers A, Vondrick C, Murphy K, Schmid C (2019b) Videobert:
    A joint model for video and language representation learning. In: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp 7464–7473'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence
    learning with neural networks. arXiv preprint arXiv:14093215
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton (1988) Sutton RS (1988) Learning to predict by the methods of temporal
    differences. Machine learning 3(1):9–44
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton et al. (1999) Sutton RS, McAllester DA, Singh SP, Mansour Y, et al.
    (1999) Policy gradient methods for reinforcement learning with function approximation.
    In: NIPs, Citeseer, vol 99, pp 1057–1063'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Szegedy et al. (2015) Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov
    D, Erhan D, Vanhoucke V, Rabinovich A (2015) Going deeper with convolutions. In:
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp 1–9'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takanobu et al. (2020) Takanobu R, Liang R, Huang M (2020) Multi-agent task-oriented
    dialog policy learning with role-aware reward decomposition. arXiv preprint arXiv:200403809
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takmaz et al. (2020) Takmaz E, Giulianelli M, Pezzelle S, Sinclair A, Fernández
    R (2020) Refer, reuse, reduce: Generating subsequent references in visual and
    conversational contexts. arXiv preprint arXiv:201104554'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tamar et al. (2016) Tamar A, Wu Y, Thomas G, Levine S, Abbeel P (2016) Value
    iteration networks. arXiv preprint arXiv:160202867
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Bansal (2019) Tan H, Bansal M (2019) Lxmert: Learning cross-modality
    encoder representations from transformers. arXiv preprint arXiv:190807490'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanana et al. (2016) Tanana M, Hallgren KA, Imel ZE, Atkins DC, Srikumar V (2016)
    A comparison of natural language processing methods for automated coding of motivational
    interviewing. Journal of substance abuse treatment 65:43–50
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2015) Tang D, Qin B, Liu T (2015) Learning semantic representations
    of users and products for document level sentiment classification. In: Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), pp 1014–1023'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al. (2019) Tang J, Zhao T, Xiong C, Liang X, Xing EP, Hu Z (2019) Target-guided
    open-domain conversation. arXiv preprint arXiv:190511553
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2018) Tao C, Mou L, Zhao D, Yan R (2018) Ruber: An unsupervised
    method for automatic evaluation of open-domain dialog systems. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 32, no 1'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2019) Tao C, Wu W, Xu C, Hu W, Zhao D, Yan R (2019) One time of
    interaction may not be enough: Go deep with an interaction-over-interaction network
    for response selection in dialogues. In: Proceedings of the 57th annual meeting
    of the association for computational linguistics, pp 1–11'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tay et al. (2019) Tay Y, Wang S, Tuan LA, Fu J, Phan MC, Yuan X, Rao J, Hui
    SC, Zhang A (2019) Simple and effective curriculum pointer-generator networks
    for reading comprehension over long narratives. arXiv preprint arXiv:190510847
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tay et al. (2020) Tay Y, Dehghani M, Bahri D, Metzler D (2020) Efficient transformers:
    A survey. arXiv preprint arXiv:200906732'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Theune (2003) Theune M (2003) Natural language generation for dialogue: system
    survey. Centre for Telematics and Information Technology, University of Twente'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thomas et al. (2006) Thomas M, Pang B, Lee L (2006) Get out the vote: Determining
    support or opposition from congressional floor-debate transcripts. arXiv preprint
    cs/0607062'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tian et al. (2019) Tian Z, Bi W, Li X, Zhang NL (2019) Learning to abstract
    for memory-augmented conversational response generation. In: Proceedings of the
    57th Annual Meeting of the Association for Computational Linguistics, pp 3816–3825'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tiedemann (2012) Tiedemann J (2012) Parallel data, tools and interfaces in
    opus. In: Lrec, vol 2012, pp 2214–2218'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tonelli et al. (2010) Tonelli S, Riccardi G, Prasad R, Joshi AK (2010) Annotation
    of discourse relations for conversational spoken dialogs. In: LREC'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran and Nguyen (2017) Tran VK, Nguyen LM (2017) Semantic refinement gru-based
    neural language generation for spoken dialogue systems. In: International Conference
    of the Pacific Association for Computational Linguistics, Springer, pp 63–75'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tur et al. (2010) Tur G, Hakkani-Tür D, Heck L (2010) What is left to be understood
    in atis? In: 2010 IEEE Spoken Language Technology Workshop, IEEE, pp 19–24'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tur et al. (2012) Tur G, Deng L, Hakkani-Tür D, He X (2012) Towards deeper
    understanding: Deep convex networks for semantic utterance classification. In:
    2012 IEEE international conference on acoustics, speech and signal processing
    (ICASSP), IEEE, pp 5045–5048'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ultes et al. (2017) Ultes S, Barahona LMR, Su PH, Vandyke D, Kim D, Casanueva
    I, Budzianowski P, Mrkšić N, Wen TH, Gasic M, et al. (2017) Pydial: A multi-domain
    statistical dialogue system toolkit. In: Proceedings of ACL 2017, System Demonstrations,
    pp 73–78'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urbanek et al. (2019) Urbanek J, Fan A, Karamcheti S, Jain S, Humeau S, Dinan
    E, Rocktäschel T, Kiela D, Szlam A, Weston J (2019) Learning to speak and act
    in a fantasy text adventure game. arXiv preprint arXiv:190303094
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vaswani et al. (2017) Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,
    Gomez AN, Kaiser Lu, Polosukhin I (2017) Attention is all you need. In: Guyon
    I, Luxburg UV, Bengio S, Wallach H, Fergus R, Vishwanathan S, Garnett R (eds)
    Advances in Neural Information Processing Systems, Curran Associates, Inc., vol 30,
    URL [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vijayakumar et al. (2016) Vijayakumar AK, Cogswell M, Selvaraju RR, Sun Q,
    Lee S, Crandall D, Batra D (2016) Diverse beam search: Decoding diverse solutions
    from neural sequence models. arXiv preprint arXiv:161002424'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals and Le (2015) Vinyals O, Le Q (2015) A neural conversational model.
    arXiv preprint arXiv:150605869
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Viterbi (1967) Viterbi A (1967) Error bounds for convolutional codes and an
    asymptotically optimum decoding algorithm. IEEE transactions on Information Theory
    13(2):260–269
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vougiouklis et al. (2016) Vougiouklis P, Hare J, Simperl E (2016) A neural
    network approach for knowledge-driven response generation. In: Proceedings of
    COLING 2016, the 26th International Conference on Computational Linguistics: Technical
    Papers, The COLING 2016 Organizing Committee, Osaka, Japan, pp 3370–3380, URL
    [https://www.aclweb.org/anthology/C16-1318](https://www.aclweb.org/anthology/C16-1318)'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Walker et al. (1997) Walker MA, Litman DJ, Kamm CA, Abella A (1997) Paradise:
    A framework for evaluating spoken dialogue agents. arXiv preprint cmp-lg/9704004'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan and McAuley (2016) Wan M, McAuley J (2016) Modeling ambiguity, subjectivity,
    and diverging viewpoints in opinion question answering systems. In: 2016 IEEE
    16th international conference on data mining (ICDM), IEEE, pp 489–498'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020a) Wang H, Peng B, Wong KF (2020a) Learning efficient dialogue
    policy from demonstrations through shaping. In: Proceedings of the 58th Annual
    Meeting of the Association for Computational Linguistics, pp 6355–6365'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020b) Wang K, Tian J, Wang R, Quan X, Yu J (2020b) Multi-domain
    dialogue acts and response co-generation. arXiv preprint arXiv:200412363
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020c) Wang L, Li J, Zeng X, Zhang H, Wong KF (2020c) Continuity
    of topic, interaction, and query: Learning to quote in online conversations. In:
    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
    (EMNLP), pp 6640–6650'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020d) Wang S, Zhou K, Lai K, Shen J (2020d) Task-completion dialogue
    policy learning via Monte Carlo tree search with dueling network. In: Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    Association for Computational Linguistics, Online, pp 3461–3471, DOI 10.18653/v1/2020.emnlp-main.278,
    URL [https://www.aclweb.org/anthology/2020.emnlp-main.278](https://www.aclweb.org/anthology/2020.emnlp-main.278)'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019a) Wang W, Zhang J, Li Q, Hwang MY, Zong C, Li Z (2019a) Incremental
    learning from scratch for task-oriented dialogue systems. arXiv preprint arXiv:190604991
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Yuan (2016) Wang X, Yuan C (2016) Recent advances on human-computer
    dialogue. CAAI Transactions on Intelligence Technology 1(4):303–312
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Wang X, Shi W, Kim R, Oh Y, Yang S, Zhang J, Yu Z (2019b)
    Persuasion for good: Towards a personalized persuasive dialogue system for social
    good. arXiv preprint arXiv:190606725'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020e) Wang Y, Guo Y, Zhu S (2020e) Slot attention with value
    normalization for multi-domain dialogue state tracking. In: Proceedings of the
    2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp
    3019–3028'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020f) Wang Y, Joty S, Lyu M, King I, Xiong C, Hoi SC (2020f)
    VD-BERT: A Unified Vision and Dialog Transformer with BERT. In: Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
    Association for Computational Linguistics, Online, pp 3325–3338, DOI 10.18653/v1/2020.emnlp-main.269,
    URL [https://www.aclweb.org/anthology/2020.emnlp-main.269](https://www.aclweb.org/anthology/2020.emnlp-main.269)'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2014) Wang Z, Zhang J, Feng J, Chen Z (2014) Knowledge graph embedding
    by translating on hyperplanes. In: Proceedings of the AAAI Conference on Artificial
    Intelligence, vol 28, no 1'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Wang Z, Schaul T, Hessel M, Hasselt H, Lanctot M, Freitas
    N (2016) Dueling network architectures for deep reinforcement learning. In: International
    conference on machine learning, PMLR, pp 1995–2003'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020g) Wang Z, Ho S, Cambria E (2020g) A review of emotion sensing:
    Categorization models and algorithms. Multimedia Tools and Applications 79:35553–35582'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2018) Welleck S, Weston J, Szlam A, Cho K (2018) Dialogue natural
    language inference. arXiv preprint arXiv:181100671
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2015a) Wen TH, Gasic M, Kim D, Mrksic N, Su PH, Vandyke D, Young
    S (2015a) Stochastic language generation in dialogue using recurrent neural networks
    with convolutional sentence reranking. arXiv preprint arXiv:150801755
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2015b) Wen TH, Gasic M, Mrksic N, Su PH, Vandyke D, Young S (2015b)
    Semantically conditioned lstm-based natural language generation for spoken dialogue
    systems. arXiv preprint arXiv:150801745
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016a) Wen TH, Gasic M, Mrksic N, Rojas-Barahona LM, Su PH, Ultes
    S, Vandyke D, Young S (2016a) Conditional generation and snapshot learning in
    neural dialogue systems. arXiv preprint arXiv:160603352
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016b) Wen TH, Gasic M, Mrksic N, Rojas-Barahona LM, Su PH, Vandyke
    D, Young S (2016b) Multi-domain neural network language generation for spoken
    dialogue systems. arXiv preprint arXiv:160301232
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016c) Wen TH, Vandyke D, Mrksic N, Gasic M, Rojas-Barahona LM,
    Su PH, Ultes S, Young S (2016c) A network-based end-to-end trainable task-oriented
    dialogue system. arXiv preprint arXiv:160404562
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weston et al. (2014) Weston J, Chopra S, Bordes A (2014) Memory networks. arXiv
    preprint arXiv:14103916
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. (2013) Williams J, Raux A, Ramachandran D, Black A (2013) The
    dialog state tracking challenge. In: Proceedings of the SIGDIAL 2013 Conference,
    Association for Computational Linguistics, Metz, France, pp 404–413, URL [https://www.aclweb.org/anthology/W13-4065](https://www.aclweb.org/anthology/W13-4065)'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (2007) Williams JD (2007) Partially observable markov decision processes
    for spoken dialogue management. PhD thesis, University of Cambridge
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams (2013) Williams JD (2013) Multi-domain learning and generalization
    in dialog state tracking. In: Proceedings of the SIGDIAL 2013 Conference, pp 433–441'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams (2014) Williams JD (2014) Web-style ranking and slu combination for
    dialog state tracking. In: Proceedings of the 15th Annual Meeting of the Special
    Interest Group on Discourse and Dialogue (SIGDIAL), pp 282–291'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams and Zweig (2016) Williams JD, Zweig G (2016) End-to-end lstm-based
    dialog control optimized with supervised and reinforcement learning. arXiv preprint
    arXiv:160601269
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams et al. (2017) Williams JD, Asadi K, Zweig G (2017) Hybrid code networks:
    practical and efficient end-to-end dialog control with supervised and reinforcement
    learning. arXiv preprint arXiv:170203274'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams (1992) Williams RJ (1992) Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Machine learning 8(3-4):229–256
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams and Zipser (1989) Williams RJ, Zipser D (1989) A learning algorithm
    for continually running fully recurrent neural networks. Neural computation 1(2):270–280
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wiseman et al. (2017) Wiseman S, Shieber SM, Rush AM (2017) Challenges in data-to-document
    generation. arXiv preprint arXiv:170708052
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Xiong (2020) Wu CS, Xiong C (2020) Probing task-oriented dialogue representation
    from language models. arXiv preprint arXiv:201013912
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019a) Wu CS, Madotto A, Hosseini-Asl E, Xiong C, Socher R, Fung
    P (2019a) Transferable multi-domain state generator for task-oriented dialogue
    systems. arXiv preprint arXiv:190508743
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020a) Wu CS, Hoi S, Socher R, Xiong C (2020a) Tod-bert: Pre-trained
    natural language understanding for task-oriented dialogues. arXiv preprint arXiv:200406871'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019b) Wu J, Wang X, Wang WY (2019b) Self-supervised dialogue learning.
    arXiv preprint arXiv:190700448
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2020b) Wu S, Li Y, Zhang D, Zhou Y, Wu Z (2020b) Diverse and informative
    dialogue generation with context-specific commonsense knowledge awareness. In:
    Proceedings of the 58th annual meeting of the association for computational linguistics,
    pp 5811–5820'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019c) Wu W, Guo Z, Zhou X, Wu H, Zhang X, Lian R, Wang H (2019c)
    Proactive human-machine conversation with explicit conversation goals. arXiv preprint
    arXiv:190605572
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2016) Wu Y, Wu W, Xing C, Zhou M, Li Z (2016) Sequential matching
    network: A new architecture for multi-turn response selection in retrieval-based
    chatbots. arXiv preprint arXiv:161201627'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020c) Wu Z, Galley M, Brockett C, Zhang Y, Gao X, Quirk C, Koncel-Kedziorski
    R, Gao J, Hajishirzi H, Ostendorf M, et al. (2020c) A controllable model of grounded
    response generation. arXiv preprint arXiv:200500613
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2015) Xiao H, Huang M, Hao Y, Zhu X (2015) Transg: A generative
    mixture model for knowledge graph embedding. arXiv preprint arXiv:150905488'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2017) Xiao H, Huang M, Meng L, Zhu X (2017) Ssp: semantic space
    projection for knowledge graph embedding with text descriptions. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 31, no 1'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2016) Xie R, Liu Z, Jia J, Luan H, Sun M (2016) Representation
    learning of knowledge graphs with entity descriptions. In: Proceedings of the
    AAAI Conference on Artificial Intelligence, vol 30, no 1'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xing et al. (2017) Xing C, Wu W, Wu Y, Liu J, Huang Y, Zhou M, Ma WY (2017)
    Topic aware neural response generation. In: Proceedings of the AAAI Conference
    on Artificial Intelligence, vol 31, no 1'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xing et al. (2018) Xing C, Wu Y, Wu W, Huang Y, Zhou M (2018) Hierarchical
    recurrent attention network for response generation. In: Proceedings of the AAAI
    Conference on Artificial Intelligence, vol 32, no 1'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Xu C, Wu W, Tao C, Hu H, Schuerman M, Wang Y (2019) Neural
    response generation with meta-words. arXiv preprint arXiv:190606050
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020a) Xu J, Wang H, Niu ZY, Wu H, Che W, Liu T (2020a) Conversational
    graph grounded policy learning for open-domain conversation generation. In: Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pp
    1835–1845'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020b) Xu K, Tan H, Song L, Wu H, Zhang H, Song L, Yu D (2020b) Semantic
    role labeling guided multi-turn dialogue rewriter. arXiv preprint arXiv:201001417
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yadollahi et al. (2017) Yadollahi A, Shahraki AG, Zaiane OR (2017) Current state
    of text sentiment analysis from opinion to emotion mining. ACM Computing Surveys
    (CSUR) 50(2):1–33
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Yang S, Zhang R, Erfani S (2020) Graphdialog: Integrating
    graph knowledge into end-to-end task-oriented dialogue systems. arXiv preprint
    arXiv:201001447'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yann et al. (2014) Yann D, Tur G, Hakkani-Tur D, Heck L (2014) Zero-shot learning
    and clustering for semantic utterance classification using deep learning. In:
    International Conference on Learning Representations (cited on page 28)'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2013) Yao K, Zweig G, Hwang MY, Shi Y, Yu D (2013) Recurrent neural
    networks for language understanding. In: Interspeech, pp 2524–2528'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2014) Yao K, Peng B, Zhang Y, Yu D, Zweig G, Shi Y (2014) Spoken
    language understanding using long short-term memory neural networks. In: 2014
    IEEE Spoken Language Technology Workshop (SLT), IEEE, pp 189–194'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yao et al. (2016) Yao K, Peng B, Zweig G, Wong KF (2016) An attentional neural
    conversation model with improved specificity. arXiv preprint arXiv:160601292
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yih et al. (2015) Yih Wt, He X, Gao J (2015) Deep learning and continuous representations
    for natural language processing. In: Proceedings of the 2015 Conference of the
    North American Chapter of the Association for Computational Linguistics: Tutorial
    Abstracts, pp 6–8'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2015) Yin J, Jiang X, Lu Z, Shang L, Li H, Li X (2015) Neural generative
    question answering. arXiv preprint arXiv:151201337
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoshino et al. (2018) Yoshino K, Hori C, Perez J, D’Haro LF, Polymenakos L,
    Gunasekara C, Lasecki WS, Kummerfeld J, Galley M, Brockett C, et al. (2018) The
    7th dialog system technology challenge. arXiv preprint
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Young et al. (2010) Young S, Gašić M, Keizer S, Mairesse F, Schatzmann J, Thomson
    B, Yu K (2010) The hidden information state model: A practical framework for pomdp-based
    spoken dialogue management. Computer Speech & Language 24(2):150–174'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Young et al. (2018) Young T, Cambria E, Chaturvedi I, Zhou H, Biswas S, Huang
    M (2018) Augmenting end-to-end dialogue systems with commonsense knowledge. In:
    AAAI, pp 4970–4977'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Young et al. (2020) Young T, Pandelea V, Poria S, Cambria E (2020) Dialogue
    systems with audio context. Neurocomputing 388:102–109
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2020) Yu F, Tang J, Yin W, Sun Y, Tian H, Wu H, Wang H (2020) Ernie-vil:
    Knowledge enhanced vision-language representations through scene graph. arXiv
    preprint arXiv:200616934 1:12'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu and Joty (2020) Yu T, Joty S (2020) Online conversation disentanglement with
    pointer networks. arXiv preprint arXiv:201011080
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Zaheer M, Guruganesh G, Dubey KA, Ainslie J, Alberti C,
    Ontanon S, Pham P, Ravula A, Wang Q, Yang L, et al. (2020) Big bird: Transformers
    for longer sequences. Advances in Neural Information Processing Systems 33:17283–17297'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zahiri and Choi (2017) Zahiri SM, Choi JD (2017) Emotion detection on tv show
    transcripts with sequence-based convolutional neural networks. arXiv preprint
    arXiv:170804299
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeiler and Fergus (2014) Zeiler MD, Fergus R (2014) Visualizing and understanding
    convolutional networks. In: European conference on computer vision, Springer,
    pp 818–833'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) Zhang C, Li Y, Du N, Fan W, Yu PS (2018a) Joint slot filling
    and intent detection via capsule neural networks. arXiv preprint arXiv:181209471
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2019a) Zhang H, Lan Y, Pang L, Guo J, Cheng X (2019a) Recosa:
    Detecting the relevant contexts with self-attention for multi-turn dialogue generation.
    arXiv preprint arXiv:190705339'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019b) Zhang H, Liu Z, Xiong C, Liu Z (2019b) Grounded conversation
    generation as guided traverses in commonsense knowledge graphs. arXiv preprint
    arXiv:191102707
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Danescu-Niculescu-Mizil (2020) Zhang J, Danescu-Niculescu-Mizil C
    (2020) Balancing objectives in counseling conversations: Advancing forwards or
    looking backwards. arXiv preprint arXiv:200504245'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018b) Zhang S, Dinan E, Urbanek J, Szlam A, Kiela D, Weston
    J (2018b) Personalizing dialogue agents: I have a dog, do you have pets too? arXiv
    preprint arXiv:180107243'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang and Wallace (2017) Zhang Y, Wallace B (2017) A sensitivity analysis of
    (and practitioners’ guide to) convolutional neural networks for sentence classification.
    In: Proceedings of the Eighth International Joint Conference on Natural Language
    Processing (Volume 1: Long Papers), Asian Federation of Natural Language Processing,
    Taipei, Taiwan, pp 253–263, URL [https://www.aclweb.org/anthology/I17-1026](https://www.aclweb.org/anthology/I17-1026)'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018c) Zhang Y, Galley M, Gao J, Gan Z, Li X, Brockett C, Dolan
    B (2018c) Generating informative and diverse conversational responses via adversarial
    information maximization. arXiv preprint arXiv:180905972
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Zhang Y, Ou Z, Wang H, Feng J (2020) A probabilistic end-to-end
    task-oriented dialog model with latent belief states towards semi-supervised learning.
    arXiv preprint arXiv:200908115
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018d) Zhang Z, Li J, Zhu P, Zhao H, Liu G (2018d) Modeling multi-turn
    conversation with deep utterance aggregation. arXiv preprint arXiv:180609102
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2019c) Zhang Z, Li X, Gao J, Chen E (2019c) Budgeted policy learning
    for task-oriented dialogue systems. arXiv preprint arXiv:190600499
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Eskenazi (2016) Zhao T, Eskenazi M (2016) Towards end-to-end learning
    for dialog state tracking and management using deep reinforcement learning. arXiv
    preprint arXiv:160602560
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Eskenazi (2018) Zhao T, Eskenazi M (2018) Zero-shot dialog generation
    with cross-domain latent actions. arXiv preprint arXiv:180504803
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhao T, Lee K, Eskenazi M (2018) Unsupervised discrete sentence
    representation learning for interpretable neural dialog generation. arXiv preprint
    arXiv:180408069
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020a) Zhao T, Lala D, Kawahara T (2020a) Designing precise and
    robust dialogue response evaluators. arXiv preprint arXiv:200404908
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020b) Zhao X, Wu W, Xu C, Tao C, Zhao D, Yan R (2020b) Knowledge-grounded
    dialogue generation with pre-trained language models. arXiv preprint arXiv:201008824
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2020) Zhong P, Zhang C, Wang H, Liu Y, Miao C (2020) Towards
    persona-based empathetic conversational models. In: Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pp 6556–6566, DOI 10.18653/v1/2020.emnlp-main.531'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2016) Zhou H, Huang M, Zhu X (2016) Context-aware natural language
    generation for spoken dialogue systems. In: Proceedings of COLING 2016, the 26th
    International Conference on Computational Linguistics: Technical Papers, pp 2032–2041'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020a) Zhou H, Zheng C, Huang K, Huang M, Zhu X (2020a) Kdconv:
    a chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation.
    arXiv preprint arXiv:200404100'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2018) Zhou K, Prabhumoye S, Black AW (2018) A dataset for document
    grounded conversations. arXiv preprint arXiv:180907358
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2020b) Zhou L, Palangi H, Zhang L, Hu H, Corso J, Gao J (2020b)
    Unified vision-language pre-training for image captioning and vqa. In: Proceedings
    of the AAAI Conference on Artificial Intelligence, vol 34, pp 13041–13049'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou and Wang (2017) Zhou X, Wang WY (2017) Mojitalk: Generating emotional
    responses at scale. arXiv preprint arXiv:171104090'
  id: totrans-1040
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2018) Zhu Q, Cui L, Zhang W, Wei F, Liu T (2018) Retrieval-enhanced
    adversarial training for neural response generation. arXiv preprint arXiv:180904276
  id: totrans-1041
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Zhu Q, Zhang W, Liu T, Wang WY (2020) Counterfactual off-policy
    training for neural dialogue generation. In: Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), pp 3438–3448'
  id: totrans-1042
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
