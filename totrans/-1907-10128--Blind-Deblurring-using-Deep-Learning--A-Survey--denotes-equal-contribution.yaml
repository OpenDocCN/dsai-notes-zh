- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:05:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1907.10128] Blind Deblurring using Deep Learning: A Survey *denotes equal
    contribution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.10128](https://ar5iv.labs.arxiv.org/html/1907.10128)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Blind Deblurring using Deep Learning: A Survey ^†^†thanks: *denotes equal contribution'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Siddhant Sahu ^* dept.Computer Science and Engineering
  prefs: []
  type: TYPE_NORMAL
- en: KIIT University Bhubansewar, India
  prefs: []
  type: TYPE_NORMAL
- en: hello@siddhantsahu.com    Manoj Kumar Lenka ^* dept.Computer Science and Engineering
  prefs: []
  type: TYPE_NORMAL
- en: KIIT University Bhubansewar, India
  prefs: []
  type: TYPE_NORMAL
- en: manojlenka1998@gmail.com    Pankaj Kumar Sa dept.Computer Science and Engineering
  prefs: []
  type: TYPE_NORMAL
- en: NIT Rourkela Rourkela, India
  prefs: []
  type: TYPE_NORMAL
- en: pankajksa@nitrkl.ac.in
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We inspect all the deep learning based solutions and provide holistic understanding
    of various architectures that have evolved over the past few years to solve blind
    deblurring. The introductory work used deep learning to estimate some features
    of the blur kernel and then moved onto predicting the blur kernel entirely, which
    converts the problem into non-blind deblurring. The recent state of the art techniques
    are end to end i.e they don’t estimate the blur kernel rather try to estimate
    the latent sharp image directly from the blurred image. The benchmarking PSNR
    and SSIM values on standard datasets of GOPRO and Köhler using various architectures
    are also provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deblurring, Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Present day imaging systems for instance consumer level photography cameras,
    medical imaging equipments, scientific astronomical imaging systems, microscopy
    and more may experience blurring due to various intrinsic (diffraction, lens chromatic
    aberration, anti-aliasing filters etc.) or extrinsic (object motion, camera shake,
    out of focus, atmospheric turbulence etc.) factors which results in loss of image
    information. To overcome this problem and to recover lost information, deblurring
    is of great interest. From an artistic perspective blur is sometimes intentional
    in photography but for majority of the image analysis applications blurs ruins
    useful data.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of deblurring is restoring a latent sharp image from a blurred image
    alone or at times with some statistical information about the blur kernel. This
    has attracted many researchers who have given many different solutions. These
    solutions can be broadly divided into statistical methods like,
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bayesian Inference Framework
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Variational Methods
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sparse Representation based method
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Homography based modeling
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Region based methods
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where we try to estimate the blur kernel from just a single given blurred image
    and learning based methods ([[1](#bib.bib1)], [[2](#bib.bib2)], [[3](#bib.bib3)],
    [[4](#bib.bib4)], [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)]) which is data driven and the blur kernel
    is learned by providing not just one but several examples of blur and its corresponding
    sharp images as ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: A blurred image can be modeled using equation,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $B=K*I+N$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $B$ is the blurred image, $K$ is the kernel, $I$ is the sharp image and
    $N$ is the additive noise. In blind deblurring we are given $B$ only, and our
    goal is to predict a latent image $L$ which is the closest approximation to the
    sharp image $I$. This is an ill-posed problem, as we have to predict both $L$
    and $K$. Predicting the kernel accurately is essential, else it may lead to various
    artifacts [[11](#bib.bib11)], using learning based approach gives an accurate
    estimate of blur kernel compared to statistical approaches or skips the kernel
    estimation process altogether (i.e end-to-end). After estimation of blur kernel
    the problem converts to non-blind deconvolution, which can be solved using methods([[12](#bib.bib12)],
    [[13](#bib.bib13)])
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods have another limitation i.e their inability to parallelize
    because a majority of them rely on coarse to fine iterative methods. Although
    deep learning models are significantly harder to train but once trained their
    inference time is comparatively fast. Moreover, deep learning methods have shown
    better on benchmarking metrics (PSNR and SSIM).
  prefs: []
  type: TYPE_NORMAL
- en: In this paper we have divided the deep learning methods into two broad categories
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimation of Kernel - Here the proposed deep learning architectures are used
    to estimate some features (Fourier coefficients[[1](#bib.bib1)], motion flow [[2](#bib.bib2)][[8](#bib.bib8)])
    of the blur kernel or deriving the deconvolution filter [[1](#bib.bib1)] which
    can be used to get back the sharp image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: End to End - These methods are kernel free, that means we don’t estimate the
    blur kernel, rather only the blurred image is required and the model generates
    the predicted restored image. Some of these methods rely on generative models
    ([[3](#bib.bib3)], [[6](#bib.bib6)], [[4](#bib.bib4)]) which are trained in an
    adversarial method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The emphasis of this paper is on the “architecture” proposed by several author
    instead of the specific details of the architecture and to foster further research
    in blind deblurring using learning based methods.
  prefs: []
  type: TYPE_NORMAL
- en: II Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Estimation of Kernel and its Attributes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-A1 Extraction of Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a optimal deblurring we require global information i.e data from different
    parts of the image, but to do so we need to have connectivity with all the pixels
    of image which will lead to a huge parameter space making it difficult to train
    and converge, hence Schuler et. al.[[7](#bib.bib7)] uses CNNs to extract features
    locally and then combine them to estimate the kernel. For this they use a multi
    scale (for different kernel sizes), multi stage architecture, where each stage
    consists of three modules feature extraction, kernel estimation, latent image
    estimation (Fig. [1](#S2.F1 "Figure 1 ‣ II-A1 Extraction of Features ‣ II-A Estimation
    of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")). In the first stage given a blurry image,
    a sharp image is estimated, for later stages they gave the blurry image concatenated
    with the estimated sharp image of the previous stage as input.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/770bcff05adadea94ed7d93de1734ccd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Shows the multi stage architecture used by Schuler et. al.[[7](#bib.bib7)],
    here the different modules in a stage are shown for the first stage only. The
    latter stages are identical to the first, except the input which is concatenation
    of blurred image and the restored image of the previous layer'
  prefs: []
  type: TYPE_NORMAL
- en: In feature extraction module they used a convolutional layer to extract features
    using filters $f_{j}$, then they used $\tanh$ to introduce non-linearity and finally
    these hidden features are linearly recombined using cofficients $\alpha_{ij}$
    and $\beta_{ij}$ to form hidden images $x_{i}$ and $y_{i}$ for stage $i$ used
    for kernel estimation, formally,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}x_{i}=\sum_{j}\alpha_{ij}\tanh(f_{j}*y)\\ y_{i}=\sum_{j}\beta_{ij}\tanh(f_{j}*y)\end{split}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $y$ is blurred image $B$ for first stage or concatenation of $B$ and predicted
    sharp image $L$ for later stages.
  prefs: []
  type: TYPE_NORMAL
- en: Given $x_{i}$ and $y_{i}$ the kernel estimation module estimates the kernel
    $K$ by minimizing,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{i}\lVert K*x_{i}-y_{i}\rVert^{2}+\beta_{k}\lVert K\rVert^{2}$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: for $K$. Given $K$ we can find the latent (restored) image $L$ by solving the
    equation,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lVert K*L-B\rVert^{2}+\beta_{x}\lVert L\rVert^{2}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'for $L$, where both $\beta_{k}$ and $\beta_{x}$ are regularization weights.
    Both (Eqn.[3](#S2.E3 "In II-A1 Extraction of Features ‣ II-A Estimation of Kernel
    and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) and (Eqn.[4](#S2.E4 "In II-A1 Extraction of Features
    ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) can be solved in
    one step in Fourier space.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Estimation of Fourier Coefficients
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given a blurry image $B[n]$ where $n\in\mathbb{Z}^{2}$ are the indexes of pixels.
    We need to find a latent sharp image $L[n]$ such that it resembles the sharp image
    $I[n]$ closely where,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $B[n]=(I*K)[n]+N[n]$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $K[n]$ is the blur kernel such that $K[n]\geq 0$ (positivity constraint),
    $\sum_{n}K[n]=1$ (unit sum constraint) and $N[n]$ the noise.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c8c370d05d6c25d340d85e60b2ffd0d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Architecture used by Chakrabati[[1](#bib.bib1)] for prediction of
    Fourier coefficients for the deconvolution filter. Here $H$ is high pass, $B_{2},B_{1}$
    are band pass, while $L$ is low pass frequency band. The letters in bold are Fourier
    transforms of the corresponding symbols.'
  prefs: []
  type: TYPE_NORMAL
- en: In the method given by Chakrabarti [[1](#bib.bib1)] a blurry image $B[n]$ is
    divided into several overlapping patches. Given a blurry patch $B_{p}=\{B[n]:n\in
    p\}$ they considered the surrounding pixels of the patch while finding its Fourier
    coefficients for better results, let the blurry image with the neighboring pixels
    be $B_{p^{+}}=\{B[n]:n\in p^{+}\}$ where $p\subset p^{+}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then they used a neural network (Fig. [2](#S2.F2 "Figure 2 ‣ II-A2 Estimation
    of Fourier Coefficients ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods
    ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution"))
    to predict the Complex Fourier Coefficients of the Deconvolution Filter $\textbf{G}_{p^{+}}[z]$
    for the blurry patch $B_{p^{+}}$, where $z$ is the two dimensional spatial frequencies
    in DFT (Discrete Fourier Transform). Then the filter is applied to the DFT of
    $B_{p^{+}}$ i.e $\textbf{B}_{p^{+}}[z]$ giving us the DFT of latent sharp image
    $\textbf{L}_{p^{+}}[z]$,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{L}_{p^{+}}[z]=\textbf{B}_{p^{+}}[z]\times\textbf{G}_{p^{+}}[z]$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: After getting $\textbf{L}_{p^{+}}[z]$, we can use a inverse discrete Fourier
    transform (IDFT) to get the latent image patch $L_{p^{+}}$ from which we can extract
    $L_{p}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate coefficients of the filter they used the architecture shown in
    (Fig. [2](#S2.F2 "Figure 2 ‣ II-A2 Estimation of Fourier Coefficients ‣ II-A Estimation
    of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")). The architecture uses multi-resolution
    decomposition strategy i.e the initial layers of the neural network are connected
    to only adjacent bands of frequency and not fully connected (here they are considering
    locality in the frequency domain, in contrast to CNNs which consider locality
    in the spatial domain). The image is sampled into patches of various resolution
    and a lower resolution patch is used to sample a higher frequency band using DFT.
    The loss function for the network is,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\dfrac{1}{&#124;p&#124;}\sum_{n\in p}(L_{p}[n]-I_{p}[n])^{2}$ |  |
    (7) |'
  prefs: []
  type: TYPE_TB
- en: They combined all the restored patches to get the first estimate of the latent
    image $L_{N}[n]$. It is assumed that the entire image is blurred by the same motion
    kernel(uniform blur), but they predicted different motion kernels for different
    patches, hence to find a global motion kernel $K_{\lambda}[n]$ they used the first
    estimate $L_{N}[n]$ as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $K_{\lambda}=arg\text{ min}\sum_{i}\lVert(K*(f_{i}*L_{N}))-(f_{i}*B)\rVert^{2}+\lambda\sum_{n}&#124;K[n]&#124;$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Here $f_{i}$ are different derivative filters. They use $L1$ regularization.
    In classical statistical methods refining latent image from a previous estimate
    is an iterative step, while here they only do it once to estimate the global blur
    kernel. After estimation of the global blur kernel, the problem becomes that of
    a non-blind deblurring and latent sharp image can be estimated using deconvolution.
  prefs: []
  type: TYPE_NORMAL
- en: II-A3 Estimation of Motion Vector for each Patch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/366b4c63b6765a9c97ae65a1ded15b1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Network architecture for predicting the motion kernel of a given
    blurred patch used by Sun et. al.[[8](#bib.bib8)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this method proposed by Sun et. al.[[8](#bib.bib8)] an image is divided
    into several overlapping patches. For each patch a CNN with a fully connected
    layer and softmax layer is used to find the probability distribution of motion
    kernels for that patch (Fig.[3](#S2.F3 "Figure 3 ‣ II-A3 Estimation of Motion
    Vector for each Patch ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods
    ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")).
    Given a patch $\Psi_{p}$ centered at pixel p, the network finds a probability
    distribution,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(m=(l,o)&#124;\Psi_{p})$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $m=(l,o)$ is the motion kernel with length $l$ and orientation $o$. Here
    $l\in S^{l}$ and $o\in S^{o}$ both $S^{l}$ and $S^{o}$ are discretized sets of
    length and orientation. Due to discretization the number of motion kernels is
    limited which leads to blocky artifacts. Hence, they rotated the image and its
    corresponding motion kernel by the same amount to get new data entry, which is
    then used in training this increases the range of $S^{o}$ that is given a patch
    $\Psi_{p}(I)$ of image $I$ and its corresponding motion kernel $m=(l,o)$, if image
    is rotated by an angel of $\theta$ then for patch $\Psi_{p}(I_{\theta})$ they
    got the motion kernel as $m=(l,o-\theta)$. Since they are doing a multicalss classification(where
    each class is a motion kernel) they use cross entropy loss given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P(m=(l,o)&#124;\Psi)=\dfrac{\exp(z_{i})}{\sum_{k=1}^{n}\exp(z_{k})}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $z$ is the output of the last fully connected layer and $n=|S^{l}|\times|S^{o}|$
    i.e $n$ is the total number of motion kernels. Since the patches are overlapping
    many patches may contain the same pixel, in such case the confidence of motion
    kernel $m$ at a pixel $p$ is given as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C(m_{p}=(l,o))=\dfrac{1}{Z}\sum_{q:p\in\Psi_{q}}G_{\sigma}(\lVert x_{p}-x_{q}\rVert^{2})P(m=(l,o)&#124;\Psi_{q})$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $q$ is center pixel of patch $\Psi_{q}$ such that $p\in\Psi_{q}$. $x_{p}$
    and $x_{q}$ are the coordinates of $p$ and $q$ respectively. $G_{\sigma}$ is a
    Gaussian function that gives more weight to patches whose center pixel $q$ is
    closest to $p$. $Z$ is a normalization constant.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5b092044feb6aff493d2484eb3acf5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: This shows given a pixel $p$ (in yellow) how MRF smoothen its value
    based on $N(p)$ i.e its neighboring pixels'
  prefs: []
  type: TYPE_NORMAL
- en: 'After estimating the motion kernels for all the patches they are concatenated
    and a Markov Random Function (MRF) is used to merge them all together, smoothen
    the transition of motion kernels amongst neighboring pixels (Fig.[4](#S2.F4 "Figure
    4 ‣ II-A3 Estimation of Motion Vector for each Patch ‣ II-A Estimation of Kernel
    and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) and generates a dense motion field by minimizing
    energy function,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sum_{p\in\Omega}[-C(m_{p}=(l_{p},o_{p}))+\sum_{q\in N(p)}\lambda[(u_{p}-u_{q})^{2}+(v_{p}-v_{q})^{2}]]$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $\Omega$ is a image region. $u_{p},u_{q},v_{p},v_{q}$ are defined as $u_{i}=l_{i}\cos(o_{i})$
    and $v_{i}=l_{i}\sin(o_{i})$ for $i=p,q$. $N(p)$ is the neighborhood of $p$. The
    first term gives more weight to using the motion kernel that the CNN chose with
    the highest confidence, while the second term looks at the neighboring pixels
    and tries to smoothen it. After predicting the motion field they deconvolve the
    blurred image with it to get the deblurred image.
  prefs: []
  type: TYPE_NORMAL
- en: II-A4 Estimation of Dense Motion Flow for entire Image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous approach of Sun et. al[[8](#bib.bib8)] a motion kernel was predicted
    for each patch using a CNN classifier and then all the motion kernels were smoothened
    using Markov Random Field (MRF) to get the dense motion field. In the method used
    by Gong et. al.[[2](#bib.bib2)] they also predict a dense motion field, but a
    pixel wise dense motion field is generated for the entire image directly (i.e
    image is not divided into patches).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3ca9c291a4eb84e69c9ba281a9ddb4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Architecture used by Gong et. al.[[2](#bib.bib2)] to predict the
    motion field given a blurry image'
  prefs: []
  type: TYPE_NORMAL
- en: In Sun et. al[[8](#bib.bib8)] they assumeded uniform motion blur within a single
    patch as only one motion kernel was chosen for a patch. This does not generalize
    to real life data properly were we can have a heterogeneous motion blur i.e motion
    may vary from pixel to pixel. In such cases an end to end approach of generating
    motion field [[2](#bib.bib2)] can give better results as they are considering
    the entire image (larger spatial context) instead of a single patch. Hence, this
    method is suitable for heterogeneous motion blurs. It does not require any post
    processing like MRF.
  prefs: []
  type: TYPE_NORMAL
- en: If the network is represented by a function of $f$. Then given a blurred image
    $B$, the goal of the network is to generate the motion field $M$, i.e
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(B)=M$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where the motion field $M$ can be represented as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M=(U,V)$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $U$ and $V$ are the horizontal and vertical motion maps respectively.
    Now given a pixel $p=(i,j)$ where $(i,j)$ are the coordinates of pixel, then we
    get,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M(i,j)=(U(i,j),V(i,j))$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: let $M(i,j)=m_{p}$, $U(i,j)=u_{p}$ and $V(i,j)=v_{p}$ then we get,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m_{p}=(u_{p},v_{p})$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $u_{p}\in\mathbb{D}_{u}$ and $v_{p}\in\mathbb{D}_{v}$. Here $\mathbb{D}_{u}$
    and $\mathbb{D}_{v}$ are the discretized motion vectors in the horizontal and
    vertical directions respectively, they are defined as, $\mathbb{D}_{u}=\{u|u\in\mathbb{Z},|u|\leq
    u_{max}\}$ and $\mathbb{D}_{v}=\{v|v\in\mathbb{Z},|v|\leq v_{max}\}$.
  prefs: []
  type: TYPE_NORMAL
- en: But, two motion vectors of opposite directions and same magnitude would generate
    the same blur pattern i.e $m_{p}=(u_{p},v_{p})$ and $-m_{p}=(-u_{p},-v_{p})$ would
    give the same blur, hence they restrict the horizontal motion vector to be positive
    only i.e $u_{p}\in\mathbb{D}_{u}^{+}$ where $\mathbb{D}_{u}^{+}=\{u|u\in\mathbb{Z}^{+},|u|\leq
    u_{max}\}$, this is done by letting $(u_{p},v_{p})=\phi(u_{p},v_{p})$ where,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\phi(u_{p},v_{p})=\begin{cases}(-u_{p},-v_{p})\text{ if }u_{p}<0\\ (u_{p},v_{p})\text{
    if }u_{p}\geq 0\end{cases}$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: If an image of size $P\times Q$ is sent trough the network(excluding the softmax)
    it generates a feature map of size $P\times Q\times D$ where $D=|\mathbb{D}_{u}^{+}|+|\mathbb{D}_{v}|$.
    This feature map is then divided into two parts of shape $P\times Q\times|\mathbb{D}_{u}^{+}|$
    and $P\times Q\times|\mathbb{D}_{v}|$. These two features pass through separate
    softmax layers to generate the horizontal and vertical motion maps $U$ and $V$
    receptively. Using these two vector maps they generate the final motion field
    $M$. After getting the motion field $M$ this becomes a non-blind debluring problem
    and a deconvolution is used to get the sharp image.
  prefs: []
  type: TYPE_NORMAL
- en: II-B End to End
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: II-B1 Without Adversarial Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deblurring requires a large receptive field (global knowledge), but CNNs provide
    local knowledge and do not show the long range dependencies properly, for this
    reason Nah et. al[[4](#bib.bib4)] (refer [II-B2](#S2.SS2.SSS2 "II-B2 With Adversarial
    Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")) uses a scaled structure and a large number
    of convolutional layers with residual connections to improve the receptive field
    of the structure, but this also makes it harder to converge due to the large number
    of parameters. Hence (Tao et. al.[[9](#bib.bib9)]) use a scale recurrent structure
    where they still use a scaled network, but significantly reduce the number of
    parameters by using a smaller encoder-decoder type network with a recurrent module
    and also share the weights between scales.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ce8db4df8ac55ef52c59d1547946bf41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Scale Recurrent Network Architecture used by (Tao et. al.[[9](#bib.bib9)])'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scale recurrent network (Fig.[6](#S2.F6 "Figure 6 ‣ II-B1 Without Adversarial
    Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")) [[9](#bib.bib9)] consists of three parts,
    encoder ($Net_{E}$), recurrent ($Net_{R}$) and decoder ($Net_{D}$) modules. This
    can be represented as,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\'
  prefs: []
  type: TYPE_NORMAL
- en: L^{i}=Net_{D}(g^{i};\theta_{D})\\
  prefs: []
  type: TYPE_NORMAL
- en: \end{split}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd 
    columnalign="right" ><mrow ><msup
    ><mi  >f</mi><mi
     >i</mi></msup><mo
     >=</mo><mrow ><mi
     >N</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi
     >E</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" 
    >(</mo><msup ><mi
     >B</mi><mi 
    >i</mi></msup><mo 
    >,</mo><msup ><mi
     >L</mi><mrow
     ><mrow
     ><mi
     >i</mi><mo
     >−</mo><mn
     >1</mn></mrow><mo
    stretchy="false"  >↑</mo></mrow></msup><mo
     >;</mo><msub ><mi
     >θ</mi><mi
     >E</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow
    ><mrow ><msup
    ><mi  >h</mi><mi
     >i</mi></msup><mo
     >,</mo><msup ><mi
     >g</mi><mi 
    >i</mi></msup></mrow><mo 
    >=</mo><mrow ><mi
     >N</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi
     >R</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" 
    >(</mo><msup ><mi
     >h</mi><mrow
     ><mrow
     ><mi
     >i</mi><mo
     >−</mo><mn
     >1</mn></mrow><mo
    stretchy="false"  >↑</mo></mrow></msup><mo
     >,</mo><msup ><mi
     >f</mi><mi
     >i</mi></msup><mo
     >;</mo><msub ><mi
     >θ</mi><mi
     >R</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow
    ><msup ><mi
     >L</mi><mi 
    >i</mi></msup><mo 
    >=</mo><mrow ><mi
     >N</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi
     >D</mi></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" 
    >(</mo><msup ><mi
     >g</mi><mi 
    >i</mi></msup><mo 
    >;</mo><msub ><mi
     >θ</mi><mi
     >D</mi></msub><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><csymbol cd="ambiguous" 
    >formulae-sequence</csymbol><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑓</ci><ci  >𝑖</ci></apply><apply
     ><ci 
    >𝑁</ci><ci  >𝑒</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑡</ci><ci 
    >𝐸</ci></apply><vector 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐵</ci><ci 
    >𝑖</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝐿</ci><apply 
    ><ci 
    >↑</ci><apply 
    ><ci 
    >𝑖</ci><cn type="integer" 
    >1</cn></apply><csymbol cd="latexml" 
    >absent</csymbol></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝜃</ci><ci
     >𝐸</ci></apply></vector><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >ℎ</ci><ci 
    >𝑖</ci></apply></apply></apply><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑔</ci><ci 
    >𝑖</ci></apply><apply 
    ><ci 
    >𝑁</ci><ci  >𝑒</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑡</ci><ci 
    >𝑅</ci></apply><vector 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >ℎ</ci><apply 
    ><ci 
    >↑</ci><apply 
    ><ci 
    >𝑖</ci><cn type="integer" 
    >1</cn></apply><csymbol cd="latexml" 
    >absent</csymbol></apply></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝑓</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝜃</ci><ci
     >𝑅</ci></apply></vector><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝐿</ci><ci 
    >𝑖</ci></apply></apply></apply><apply 
    ><apply 
    ><ci 
    >𝑁</ci><ci  >𝑒</ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑡</ci><ci 
    >𝐷</ci></apply><list 
    ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><ci 
    >𝑔</ci><ci 
    >𝑖</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝜃</ci><ci 
    >𝐷</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\ L^{i}=Net_{D}(g^{i};\theta_{D})\\
    \end{split}</annotation></semantics></math> |  | (18) |
  prefs: []
  type: TYPE_NORMAL
- en: $\theta_{E},\theta_{R},\theta_{D}$ are the weights of their respective modules.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder module consists of convolutional layers with residual connections.
    For the first scale, only the blurred image is used as input, for all the subsequent
    layers both the blurred image $B^{i}$ and the restored image of the previous scale
    $L^{i-1\uparrow}$ are concatenated and both are sent as input. The encoder module
    is used to extract features $f^{i}$, it gradually decreases the length and breadth
    but increases the number of channels.
  prefs: []
  type: TYPE_NORMAL
- en: The recurrent module can be a vanilla RNN, GRU or LSTM, in Tao et. al they used
    convolutional LSTM (ConvLSTM) which gave the best results. They also trained a
    network without any Recurrent module and it gave lower performance compared to
    the one which included a recurrent module. It takes as input the hidden features
    of the previous scale’s recurrent module $h^{i-1\uparrow}$ and the features generated
    by the current scales encoder $f^{i}$. The hidden features of the previous scale
    passes the intermediate results and blur patterns of the previous scale which
    benefits the current scale. Gradient clipping is used for this module only. It
    gives as output a modified set of features $g^{i}$ and the hidden features of
    the current scale $h^{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder module consists of a few convolutional layers with residual connections
    (same dimensions are maintained using padding) followed by a deconvolutional layer
    which increases the spatial dimensions and decreases the number of channels until
    the we get latent image for the scale $L^{i}$. $\uparrow$ operator (Eqn. [18](#S2.E18
    "In II-B1 Without Adversarial Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) is used to adapt
    the dimensions of features or images to that of the next scale. $\uparrow$ can
    be deconvolution, sub-pixel convolution, image resizing, bilinear interpolation,
    etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining all the three modules a single scale in the network can be represented
    as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L^{i},h^{i}=Net_{SR}(B^{i},L^{i-1\uparrow},h^{i-1\uparrow};\theta_{SR})$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where $\theta_{SR}$ is the weight shared across all scales.
  prefs: []
  type: TYPE_NORMAL
- en: Scaled Recurrent Network uses Euclidean Loss given below,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\sum_{i=1}^{n}\dfrac{\kappa_{i}}{N_{i}}\lVert L^{i}-I^{i}\rVert^{2}_{2}$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: where $L^{i}$ is the latent restored image, $I^{i}$ is the ground truth sharp
    image. $\{\kappa_{i}\}$ are weights for each scale, and $N_{i}$ is the number
    of elements in $L^{i}$ to be normalized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Noorozi et. al.[[5](#bib.bib5)] also uses a three pyramid stages chained together,
    each consisting of several convolutional and deconvolutional layers $(N_{1},N_{2},\text{
    and }N_{3})$ (Eqn. [21](#S2.E21 "In II-B1 Without Adversarial Loss ‣ II-B End
    to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes
    equal contribution")) which recreates a multiscale pyramid schemes which were
    previously used in the classical methods. The key idea of this pyramid structure
    is that the downsampled version of the blurred image has less amount of blur and
    it is easier for removal. Hence the goal of each respective network (or stage)
    is to mitigate the blur effect at that corresponding scale. It also helps break
    down the complex problem of deblurring into smaller units.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de378c90eab5299a5bd7854ce25f6d5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Architecture used by Noorozi et. al. [[5](#bib.bib5)]. Here the three
    CNNs starting from the left denotes $N_{1},N_{2},N_{3}$ respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly the blurred image is given as input to the first network $N_{1}$ (pure
    convolution) without any downsampling and it’s output is added with the downsampled
    version of the same blurred image by a factor of four. After this the first loss
    $L_{1}$ is calculated using [21](#S2.E21 "In II-B1 Without Adversarial Loss ‣
    II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution") by calculating the difference (or MSE) between the
    downsampled sharp image and the added sum of the network $N_{1}$’s output and
    downsampled blurred image. This same process is repeated for network $N_{2}$ and
    $N_{3}$ for calculating losses $L_{2}$ and $L_{3}$ but the downsampling factor
    are two and one (no downsampling) respectively. The these three computed losses
    are added resulting in final loss function for this model.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: L_{3}=\sum_{(B,I)}&#124;N_{3}(N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B))+B-I&#124;^{2}\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
     ><mtr 
    ><mtd  columnalign="right"
     ><mrow 
    ><msub  ><mi
     >L</mi><mn 
    >1</mn></msub><mo rspace="0.111em" 
    >=</mo><mrow 
    ><munder 
    ><mo movablelimits="false" rspace="0em" 
    >∑</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo
     >,</mo><mi
     >I</mi><mo
    stretchy="false"  >)</mo></mrow></munder><msup
     ><mrow 
    ><mo stretchy="false" 
    >&#124;</mo><mrow 
    ><mrow 
    ><mrow 
    ><msub 
    ><mi  >N</mi><mn
     >1</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
     >−</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >I</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mn
     >2</mn></msup></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><msub 
    ><mi  >L</mi><mn
     >2</mn></msub><mo
    rspace="0.111em"  >=</mo><mrow
     ><munder 
    ><mo movablelimits="false" rspace="0em" 
    >∑</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mi 
    >B</mi><mo 
    >,</mo><mi 
    >I</mi><mo stretchy="false" 
    >)</mo></mrow></munder><msup 
    ><mrow 
    ><mo stretchy="false" 
    >&#124;</mo><mrow 
    ><mrow 
    ><mrow 
    ><msub 
    ><mi  >N</mi><mn
     >2</mn></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><mrow
     ><msub
     ><mi
     >N</mi><mn
     >1</mn></msub><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >2</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
     >−</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >2</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >I</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mn
     >2</mn></msup></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><msub 
    ><mi  >L</mi><mn
     >3</mn></msub><mo
    rspace="0.111em"  >=</mo><mrow
     ><munder
     ><mo movablelimits="false"
    rspace="0em"  >∑</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo
     >,</mo><mi
     >I</mi><mo
    stretchy="false"  >)</mo></mrow></munder><msup
     ><mrow
     ><mo
    stretchy="false"  >&#124;</mo><mrow
     ><mrow
     ><mrow
     ><msub
     ><mi
     >N</mi><mn 
    >3</mn></msub><mo lspace="0em" rspace="0em"
     >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><mrow
     ><msub
     ><mi
     >N</mi><mn
     >2</mn></msub><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mrow 
    ><mrow 
    ><msub 
    ><mi  >N</mi><mn
     >1</mn></msub><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mrow
     ><msub
     ><mi
     >d</mi><mrow
     ><mn
     >1</mn><mo
     >/</mo><mn
     >2</mn></mrow></msub><mo
    lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >+</mo><mi
     >B</mi></mrow><mo
     >−</mo><mi
     >I</mi></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mn
     >2</mn></msup></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐿</ci><cn type="integer"
     >1</cn></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><interval closure="open" 
    ><ci  >𝐵</ci><ci
     >𝐼</ci></interval></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><apply  ><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑁</ci><cn type="integer" 
    >1</cn></apply><ci 
    >𝐵</ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >4</cn></apply></apply><ci 
    >𝐵</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >4</cn></apply></apply><ci 
    >𝐼</ci></apply></apply></apply><cn type="integer"
     >2</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐿</ci><cn type="integer"
     >2</cn></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><interval
    closure="open"  ><ci
     >𝐵</ci><ci
     >𝐼</ci></interval></apply><apply
     ><apply 
    ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><apply  ><apply
     ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑁</ci><cn type="integer" 
    >2</cn></apply><apply 
    ><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑁</ci><cn type="integer" 
    >1</cn></apply><ci 
    >𝐵</ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >4</cn></apply></apply><ci 
    >𝐵</ci></apply></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >2</cn></apply></apply><ci 
    >𝐵</ci></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >2</cn></apply></apply><ci 
    >𝐼</ci></apply></apply></apply><cn type="integer"
     >2</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐿</ci><cn type="integer"
     >3</cn></apply></apply></apply></apply><apply
     ><apply 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><interval
    closure="open"  ><ci
     >𝐵</ci><ci
     >𝐼</ci></interval></apply><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><apply
     ><apply 
    ><apply  ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑁</ci><cn type="integer"
     >3</cn></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑁</ci><cn
    type="integer"  >2</cn></apply><apply
     ><apply
     ><apply
     ><csymbol
    cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑁</ci><cn type="integer" 
    >1</cn></apply><ci 
    >𝐵</ci></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >4</cn></apply></apply><ci 
    >𝐵</ci></apply></apply></apply><apply 
    ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑑</ci><apply 
    ><cn type="integer" 
    >1</cn><cn type="integer" 
    >2</cn></apply></apply><ci 
    >𝐵</ci></apply></apply></apply><ci 
    >𝐵</ci></apply><ci 
    >𝐼</ci></apply></apply><cn type="integer" 
    >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\
    L_{3}=\sum_{(B,I)}&#124;N_{3}(N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B))+B-I&#124;^{2}\end{split}</annotation></semantics></math>
    |  | (21) |
  prefs: []
  type: TYPE_NORMAL
- en: The problem in this architecture is when there are extreme blurs,the network
    leaves the images untouched but it does not suffer from artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: Spatially variant blurs in dynamic scenes require a large receptive field, while
    CNN have a local knowledge (small receptive field) and spatially invariant weights,
    to remove this problem they have to use larger networks with more convolutional
    layer, leading to more parameters which is difficult to train. Hence, the challenge
    is to have a small architecture with a large receptive field, to this end Zhang
    et. al[[10](#bib.bib10)] proposes the use of Recurrent Neural Network as a deconvolutional
    operator which increases the receptive field (long range dependencies).
  prefs: []
  type: TYPE_NORMAL
- en: 'The network proposed by Zhang et. al.[[10](#bib.bib10)] uses three CNN and
    one RNN. The CNN are used for feature extraction, image reconstruction, and pixel-wise
    weight generation (for the RNN). While the RNN is used as a deconvolutional operator
    with a large receptive field (Fig. [8](#S2.F8 "Figure 8 ‣ II-B1 Without Adversarial
    Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")). Weight generation for the RNN is done by
    passing an image through a encoder-decoder architecture CNN. For both the decoder
    part of the weight generation module and the image restoration module they use
    bilinear interpolation (no deconvolution) to avoid checkerboard artifact [[11](#bib.bib11)]
    The RNN generates receptive fields in one direction (single dimension), hence
    they use a convolutional layer after every RNN to fuse the receptive fields together
    to get a two dimensional structure. Skip connections are added to avoid vanishing
    gradient problem and for faster training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d52d0a2d620c339837c08921ac8205b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Architecture used by Zhang et. al. [[10](#bib.bib10)]'
  prefs: []
  type: TYPE_NORMAL
- en: If $f$ is the feature extraction module, $rnn$ is the RNN module, $w$ is the
    weight generation module, and $r$ is the restoration module, then the network
    proposed by Zhang et. al.[[10](#bib.bib10)] can be summarized as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math   alttext="\begin{split}F=f(B)\\
    \theta=w(B)\\'
  prefs: []
  type: TYPE_NORMAL
- en: F^{\prime}=rnn(F;\theta)\\
  prefs: []
  type: TYPE_NORMAL
- en: L=r(F^{\prime})\end{split}" display="block"><semantics ><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><mi 
    >F</mi><mo  >=</mo><mrow
     ><mi 
    >f</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><mi 
    >θ</mi><mo  >=</mo><mrow
     ><mi 
    >w</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi
     >B</mi><mo stretchy="false"
     >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><msup 
    ><mi  >F</mi><mo
     >′</mo></msup><mo
     >=</mo><mrow
     ><mi 
    >r</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi  >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >n</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mrow
     ><mo stretchy="false"
     >(</mo><mi 
    >F</mi><mo  >;</mo><mi
     >θ</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="right"  ><mrow
     ><mi 
    >L</mi><mo  >=</mo><mrow
     ><mi 
    >r</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mrow 
    ><mo stretchy="false" 
    >(</mo><msup 
    ><mi  >F</mi><mo
     >′</mo></msup><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><ci
     >𝐹</ci><apply 
    ><ci  >𝑓</ci><ci
     >𝐵</ci><ci 
    >𝜃</ci></apply></apply><apply 
    ><apply  ><ci
     >𝑤</ci><ci 
    >𝐵</ci><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci
     >𝐹</ci><ci 
    >′</ci></apply></apply></apply><apply 
    ><apply  ><ci
     >𝑟</ci><ci 
    >𝑛</ci><ci  >𝑛</ci><list
     ><ci 
    >𝐹</ci><ci 
    >𝜃</ci></list><ci 
    >𝐿</ci></apply></apply><apply 
    ><apply  ><ci
     >𝑟</ci><apply
     ><csymbol cd="ambiguous"
     >superscript</csymbol><ci
     >𝐹</ci><ci 
    >′</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}F=f(B)\\ \theta=w(B)\\
    F^{\prime}=rnn(F;\theta)\\ L=r(F^{\prime})\end{split}</annotation></semantics></math>
    |  | (22) |
  prefs: []
  type: TYPE_NORMAL
- en: where $B$ is the blurry image, $F$ is the extracted features, $\theta$ is the
    pixel-wise generated weights, $F^{\prime}$ are the modified features after passing
    through the RNN, and $L$ is the latent (predicted) deblurred image.
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 With Adversarial Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90b4f0df280268f575fe334d2f970fb0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The basic structure of a GAN, where $G$ denotes the Generator and
    $D$ denotes the discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: Blind Deblurring can also be solved end-to-end by generative models like Generative
    Adversarial Networks [[14](#bib.bib14)][[15](#bib.bib15)][[16](#bib.bib16)]. The
    approach Generative Adversarial Networks take is to have two different agents
    play a game against each other. One of the agents is a generator network which
    tries to generate data and the other is a discriminator network which examines
    data and checks whether it came from the real distribution (ground truth sharp
    image) or model distribution (restored blurred image). The goal of the generator
    is to fool the discriminator into believing that its output is from the real distribution.
    These generator and discriminator modules are neural networks whose parameters
    can be tuned by backpropagation and as both players get better at their job over
    time eventually the generator is forced to create data as realistic as possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87deaf31d77889391a6730a67a788709.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Multiscale architecture used by Nah. et. al [[4](#bib.bib4)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nah et. al.[[4](#bib.bib4)] also uses a Multiscale Convolutional Neural Network
    i.e coarse(low resolution) to fine(high resolution) structure. The blurred and
    sharp images are scaled down to form a Gaussian pyramid structure, this is done
    because convolution can only capture local information, hence lower resolution
    images are used to capture the long range dependencies, whereas the high resolution
    images are used to capture the fine grained details. Each of these scaled blurred
    images passes through a layer of multiple convolutional and residual blocks (residual
    blocks enable training in large networks without over fitting) to generate the
    corresponding latent image for that scale, then for each scale MSE(Mean Squared
    Error) with the sharp image is calculated and back propagation is done. The MSE
    for all the scales are averaged together to give the content loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{content}=\dfrac{1}{2K}\sum_{k=1}^{K}\dfrac{1}{c_{k}h_{k}w_{k}}\lVert
    L_{k}-I_{k}\rVert^{2}$ |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: Here $K$ is the total number of scales, $c_{k},h_{k},w_{k}$ are the channels,
    height and width of the $k^{th}$ scale, and $L_{k}$ and $I_{k}$ are the latent
    and sharp images of the $k^{th}$ scale respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of current scale is given as input to the next scale. The next scale
    is of a higher resolution, hence the latent image of the current scale is passed
    through a upconvolutional(transpose convolution) layer and is concatenated with
    the blurred image input of the next layer. Except for the last layer whose output
    latent image is the same size as the original image, hence does not need any upconvolution.
    This generated deblurred image of the last scale is given as input to a discriminator
    or some sharp image is given as input, and the discriminator tells weather the
    image given was originally sharp or was deblurred by the Multi-Scaled Network.
    Discriminator loss function (Adversarial Loss) [[14](#bib.bib14)] is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{adv}=\mathbb{E}_{S\sim p_{sharp}}[\log(D(S))]+\mathbb{E}_{B\sim p_{blurred}}[1-\log(G(B))]$
    |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: Here $D$ is the Discriminator i.e a CNN classifier and $G$ is the Generator
    i.e our Multi-Scaled CNN. Generator $G$ for each scale can be defined as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L^{i}=G(B^{i},L^{i-1\uparrow};\theta_{i})$ |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: where $L^{i},B^{i}$ is the generated and blurred image for $i^{th}$ scale respectively.
    $L^{i-1\uparrow}$ is the generated image of the previous scale where $\uparrow$
    denotes the upconvolution function used to upscale the dimension of $L^{i-1}$
    to be same as $L^{i}$. $\theta_{i}$ are the weights of $i^{th}$ scale.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combing both loss functions i.e Content and Adversarial Loss we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{total}=L_{content}+\lambda L_{adv}$ |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: Where $\lambda$ is a weight constant.
  prefs: []
  type: TYPE_NORMAL
- en: Ramamkrishnan et. al[[6](#bib.bib6)] also uses an adversarial way of training,
    but the generator uses a structure similar to DenseNet[[17](#bib.bib17)] with
    a global skip connection. Similar to Nah et. al.[[4](#bib.bib4)], here dimensions
    are maintained throughout the convolutional layers, so that no deconvolution module
    needs to be used, preventing checkerboard effect [[11](#bib.bib11)]. Using a densely
    connected CNN in generator reduces the vanishing gradient problem, strengthens
    feature propagation and reuse and reduces the number of parameters, all of which
    in turn allows us to use a smaller network with smoother training and faster inference
    time.
  prefs: []
  type: TYPE_NORMAL
- en: The generator is divided into three parts, head, dense field and tail. Head
    creates sufficient activations for the dense field using convolutional layer.
    The dense field consists of several dense blocks, each dense block has a ReLU
    to add non linearity, a 1$\times$1 convolution to limit the number of activaitons
    (or channels), a convolutional layer (3 $\times$ 3), and batch normalizations.
    In DenseNet[[17](#bib.bib17)] $l^{th}$ layer of convolution is connected to the
    features of all the previous layers as opposed to the immediately previous layer
    (like in classic CNN). This dense connectivity is achieved in the generator by
    concatenating the output of $l^{th}$ layer with the output of $(l+1)^{th}$ layer,
    which in turn concatenates its output with the output of next layer i.e $(l+2)^{th}$
    layer and so on. All the convolutional layers in the dense filed use spatial and
    dilated convolution alternatively, this increases the receptive field of the network
    while still restraining the number of parameters to be learned. The rate of dilation
    increases till the middle layer and then decreases till the tail is reached. Tail
    adds non-linearity and uses $1\times 1$ convolution to reduce the number of activations.
  prefs: []
  type: TYPE_NORMAL
- en: They added the output of head to the output of tail to form a global skip connection,
    this allows gradients to flow back to the first layers of convolution which helps
    in learning (gradient updates) of the lower layers. Shorter connections between
    layers close to the output and layers close to the input, results in better accuracy
    and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to Nah et. al.[[4](#bib.bib4)] here the loss functions is also divided
    into two parts but with slight differences i.e instead of finding the MSE between
    predicted and sharp image (content loss), they find the MSE between the features
    (taken from end layers of a pretrained VGG16 network) of the predicted and sharp
    image. This is known as Perceptual Loss [[18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{precep}=\dfrac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}(\phi(I)_{x,y}-\phi(L)_{x,y})$
    |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: where $\phi$ denotes the function used to generate the features. $W,H$ are dimensions
    of the features. $L$ is the predicted latent image i.e $L=G(B)$ for generator
    $G$ and blurry image $B$.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of simple adversarial loss they use a conditional adversarial loss [[15](#bib.bib15)]
    i.e with every sharp or predicted image, they also send the corresponding blurred
    image. Then calculate the probability of weather the image is deblurred or sharp
    given the blurred image.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{adv_{con}}=-\mathbb{E}_{b\in B}[\log D(G(B)&#124;B)]$ |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where $D$ is the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: While combining both the losses Zhang et. al[[10](#bib.bib10)] also adds a $L1$
    loss which was not present in Nah et. al[[4](#bib.bib4)],
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{total}=L_{precep}+K1\times L_{adv_{con}}+K2\times L_{L1}$ |  | (29)
    |'
  prefs: []
  type: TYPE_TB
- en: where $L_{L1}$ is the $L1$ loss. $K1,K2$ are the weight constant.
  prefs: []
  type: TYPE_NORMAL
- en: Kupyn et. al.[[3](#bib.bib3)] uses a method also based on conditional GANs [[15](#bib.bib15)]
    similar to Ramakrishnan et. al.[[6](#bib.bib6)], the number of layer are significantly
    less compared to Nah et. al.[[4](#bib.bib4)], decreasing the number of trainable
    parameters and hence resulting in decrease training time and faster inference
    time. Instead of using the conventional loss function of GANs, they used the wasserstein
    (or called Earth-Mover) distance with gradient penalty [[16](#bib.bib16)] which
    has proved to show stability from vanila GANs[[14](#bib.bib14)] which suffer from
    mode collapse and vanishing gradients. The generator architecture is different
    from Ramakrishnan et. al.[[6](#bib.bib6)] as they use some convlutional layers
    followed by a series of residual blocks and finally some deconvolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator takes the blurred image as input and produces it’s sharp estimate.
    The discriminator then tries to model the differences in sharp images (real distribution)
    and restored image (model distribution) by the generator by computing the Wasserstein
    distance (Earth mover distance)[[16](#bib.bib16)]. The perceptual loss is same
    as in (Eqn. [27](#S2.E27 "In II-B2 With Adversarial Loss ‣ II-B End to End ‣ II
    Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to minimize this entire loss function (which is same as (Eqn. [29](#S2.E29
    "In II-B2 With Adversarial Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) but without the $L1$
    loss) such that the generator is producing well enough restored image from the
    blurred image and the discriminator network to unable to distinguish the real
    sharp image (real data distribution) and restored image (model distribution) resulting
    in output of ½ probability by the discriminator most of the time. This is when
    the model is said to have converged.'
  prefs: []
  type: TYPE_NORMAL
- en: III Performance Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The metrics used to measure similarity between the restored image and the blurred
    image are Peak Signal to Noise Ratio (PSNR) and Structural Similarity(SSIM). We
    also compare time taken by different architectures to deblur a blurry image after
    they are trained (inference time) (Table [III](#S3.T3 "TABLE III ‣ III-C GoPro
    Dataset ‣ III Performance Evaluation ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")).'
  prefs: []
  type: TYPE_NORMAL
- en: PSNR can be thought of as the reciprocal of MSE (Mean Squared Error). MSE can
    be calculated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MSE=\dfrac{\sum_{P,Q}(I-L)^{2}}{P\times Q}$ |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: where $P,Q$ are the dimensions of the image. $I$ and $L$ are the sharp and deblurred
    image respectively. Given MSE, PSNR can be calculated using,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $PSNR=\dfrac{m^{2}}{MSE}$ |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: where $m$ is the maximum possible intensity value, since we are using 8-bit
    integer to represent a pixel in channel, m = 255.
  prefs: []
  type: TYPE_NORMAL
- en: SSIM helps us to find the structural similarity between two image, it can be
    calculated using,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SSIM(x,y)=\dfrac{(2\mu_{x}\mu_{y}+c_{1})(2\sigma_{xy}+c_{2})}{(\mu_{x}^{2}+\mu_{y}^{2}+c_{1})(\sigma_{x}^{2}+\sigma_{y}^{2}+c_{2})}$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: where $x,y$ are windows of equal dimension for $B,I$ respectively. $\mu_{x},\mu_{y}$
    denotes mean of $x,y$ respectively. $\sigma_{x},\sigma_{y}$ denotes variance for
    $x,y$ respectively, whereas $\sigma_{xy}$ is the covariance between $x$ and $y$.
    $c_{1}$ and $c_{2}$ are constants used to stabilize the division.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Köhler Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Köhler Dataset [[19](#bib.bib19)] consists of 4 images which are blurred using
    12 different blur kernels giving us a total of 48 blurred images. To generate
    the blurred kernels, 6D camera motion is recored and then replayed using a robot,
    for each image. While replying, the 6D motion is approximated into a 3D motion
    by considering translation in one plane, and rotation on the plane perpendicular
    to it. This helps us to approximate actual camera shakes that occur in real life.
    For more details refer to [[19](#bib.bib19)]. The PSNR and SSIM for different
    deblurring architecture in Köhler dataset is shown in (Table [I](#S3.T1 "TABLE
    I ‣ III-B Köhler Dataset ‣ III Performance Evaluation ‣ Blind Deblurring using
    Deep Learning: A Survey *denotes equal contribution")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Köhler Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | PSNR | SSIM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Kupyn et. al. | 26.10 | 0.816 |'
  prefs: []
  type: TYPE_TB
- en: '| Tao et. al. | 26.80 | 0.838 |'
  prefs: []
  type: TYPE_TB
- en: '| Nah et. al. | 26.48 | 0.812 |'
  prefs: []
  type: TYPE_TB
- en: '| Gong et. al. | 26.59 | 0.742 |'
  prefs: []
  type: TYPE_TB
- en: '| Ramakrishnan et. al. | 27.08 | 0.751 |'
  prefs: []
  type: TYPE_TB
- en: '| Sun et. al. | 25.22 | 0.774 |'
  prefs: []
  type: TYPE_TB
- en: III-C GoPro Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, a high resolution (1280 $\times$ 720), high frame rate (240 frames per
    second) camera (GoPro Hero5 Black) is used to capture video outdoors. To generate
    blurred image an average of a few frames (odd number picked randomly from 7 to
    23) is taken, while the central frame is considered as the corresponding sharp
    image. To reduce the magnitude of relative motion across frames they are down
    sampled and to avoid artifacts caused by averaging we only consider frames were
    the optical flow is at most 1. The PSNR and SSIM for different deblurring architecture
    in GoPro dataset is shown in (Table [II](#S3.T2 "TABLE II ‣ III-C GoPro Dataset
    ‣ III Performance Evaluation ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: GoPro Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | PSNR | SSIM |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Kupyn et. al. | 28.7 | 0.958 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et. al. | 29.2 | 0.931 |'
  prefs: []
  type: TYPE_TB
- en: '| Tao et. al. | 30.1 | 0.932 |'
  prefs: []
  type: TYPE_TB
- en: '| Nah et. al. | 29.2 | 0.916 |'
  prefs: []
  type: TYPE_TB
- en: '| Gong et. al. | 26.1 | 0.863 |'
  prefs: []
  type: TYPE_TB
- en: '| Noorozi et. al. | 28.1 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Sun et. al. | 24.6 | 0.842 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE III: Inference Time'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Time(sec) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Kupyn et. al. | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Tao et. al. | 1.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Nah et. al. | 4.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et. al. | 1.4 |'
  prefs: []
  type: TYPE_TB
- en: IV Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We observe that end-to-end methods ([[3](#bib.bib3)], [[4](#bib.bib4)], [[6](#bib.bib6)],
    [[5](#bib.bib5)], [[9](#bib.bib9)], [[10](#bib.bib10)]) have higher PSNR and SSIM
    compared to methods that estimate the blur kernel ([[2](#bib.bib2)], [[8](#bib.bib8)]),
    this is because an error in kernel estimation can lead to various artifacts in
    image, degrading the restoration.
  prefs: []
  type: TYPE_NORMAL
- en: We also observed that most of the methods tried to increase their receptive
    field, which allowed long range spatial dependencies, essential for non uniform
    blurs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another motivation was to reduce the size of network and the number of parameters,
    resulting in faster inference, as it can be clearly seen in (Table [III](#S3.T3
    "TABLE III ‣ III-C GoPro Dataset ‣ III Performance Evaluation ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) that Nah et. al.[[4](#bib.bib4)]
    which has a large network size is slower compared to other networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Decreasing the size of network, while maintaining a large receptive field is
    one of the biggest challenge in learning based deblurring methods.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Chakrabarti, “A neural approach to blind motion deblurring,” *CoRR*,
    vol. abs/1603.04771, 2016\. [Online]. Available: [http://arxiv.org/abs/1603.04771](http://arxiv.org/abs/1603.04771)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel,
    and Q. Shi, “From motion blur to motion flow: A deep learning solution for removing
    heterogeneous motion blur,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] O. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, and J. Matas, “Deblurgan:
    Blind motion deblurring using conditional adversarial networks,” in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Nah, T. Hyun Kim, and K. Mu Lee, “Deep multi-scale convolutional neural
    network for dynamic scene deblurring,” in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Noroozi, P. Chandramouli, and P. Favaro, “Motion deblurring in the wild,”
    *CoRR*, vol. abs/1701.01486, 2017\. [Online]. Available: [http://arxiv.org/abs/1701.01486](http://arxiv.org/abs/1701.01486)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Ramakrishnan, S. Pachori, A. Gangopadhyay, and S. Raman, “Deep generative
    filter for motion deblurring,” *CoRR*, vol. abs/1709.03481, 2017. [Online]. Available:
    [http://arxiv.org/abs/1709.03481](http://arxiv.org/abs/1709.03481)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Schölkopf, “Learning to
    deblur,” *CoRR*, vol. abs/1406.7444, 2014\. [Online]. Available: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Sun, W. Cao, Z. Xu, and J. Ponce, “Learning a convolutional neural network
    for non-uniform motion blur removal,” in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] X. Tao, H. Gao, X. Shen, J. Wang, and J. Jia, “Scale-recurrent network
    for deep image deblurring,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Zhang, J. Pan, J. Ren, Y. Song, L. Bao, R. W. Lau, and M.-H. Yang,
    “Dynamic scene deblurring using spatially variant recurrent neural networks,”
    in *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Odena, V. Dumoulin, and C. Olah, “Deconvolution and checkerboard artifacts,”
    *Distill*, 2016\. [Online]. Available: [http://distill.pub/2016/deconv-checkerboard](http://distill.pub/2016/deconv-checkerboard)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] U. Schmidt, C. Rother, S. Nowozin, J. Jancsary, and S. Roth, “Discriminative
    non-blind deblurring,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] C. J. Schuler, H. Christopher Burger, S. Harmeling, and B. Scholkopf,
    “A machine learning approach for non-blind image deconvolution,” in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems 27*, Z. Ghahramani, M. Welling, C. Cortes, N. D.
    Lawrence, and K. Q. Weinberger, Eds.   Curran Associates, Inc., 2014, pp. 2672–2680.
    [Online]. Available: [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” in *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, 2017, pp. 214–223\. [Online].
    Available: [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
    style transfer and super-resolution,” in *European Conference on Computer Vision*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. B. S. B. H. S. Köhler R., Hirsch M., “Recording and playback of camera
    shake: Benchmarking blind deconvolution with a real-world database,” in *European
    Conference on Computer Vision (ECCV)*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
