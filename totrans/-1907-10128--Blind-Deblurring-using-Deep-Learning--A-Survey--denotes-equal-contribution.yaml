- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: æœªåˆ†ç±»'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: æœªåˆ†ç±»'
- en: 'date: 2024-09-06 20:05:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¥æœŸï¼š2024-09-06 20:05:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.10128] Blind Deblurring using Deep Learning: A Survey *denotes equal
    contribution'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.10128] åŸºäºæ·±åº¦å­¦ä¹ çš„ç›²ç›®å»æ¨¡ç³Šï¼šç»¼è¿° *è¡¨ç¤ºè´¡çŒ®ç›¸ç­‰'
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/1907.10128](https://ar5iv.labs.arxiv.org/html/1907.10128)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/1907.10128](https://ar5iv.labs.arxiv.org/html/1907.10128)
- en: 'Blind Deblurring using Deep Learning: A Survey ^â€ ^â€ thanks: *denotes equal contribution'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åŸºäºæ·±åº¦å­¦ä¹ çš„ç›²ç›®å»æ¨¡ç³Šï¼šç»¼è¿° ^â€ ^â€ æ„Ÿè°¢ï¼š*è¡¨ç¤ºè´¡çŒ®ç›¸ç­‰
- en: Siddhant Sahu ^* dept.Computer Science and Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Siddhant Sahu ^* dept.Computer Science and Engineering
- en: KIIT University Bhubansewar, India
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: KIIT University Bhubansewar, India
- en: hello@siddhantsahu.com â€ƒâ€ƒ Manoj Kumar Lenka ^* dept.Computer Science and Engineering
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: hello@siddhantsahu.com â€ƒâ€ƒ Manoj Kumar Lenka ^* dept.Computer Science and Engineering
- en: KIIT University Bhubansewar, India
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KIIT University Bhubansewar, India
- en: manojlenka1998@gmail.com â€ƒâ€ƒ Pankaj Kumar Sa dept.Computer Science and Engineering
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: manojlenka1998@gmail.com â€ƒâ€ƒ Pankaj Kumar Sa dept.Computer Science and Engineering
- en: NIT Rourkela Rourkela, India
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NIT Rourkela Rourkela, India
- en: pankajksa@nitrkl.ac.in
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: pankajksa@nitrkl.ac.in
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: We inspect all the deep learning based solutions and provide holistic understanding
    of various architectures that have evolved over the past few years to solve blind
    deblurring. The introductory work used deep learning to estimate some features
    of the blur kernel and then moved onto predicting the blur kernel entirely, which
    converts the problem into non-blind deblurring. The recent state of the art techniques
    are end to end i.e they donâ€™t estimate the blur kernel rather try to estimate
    the latent sharp image directly from the blurred image. The benchmarking PSNR
    and SSIM values on standard datasets of GOPRO and KÃ¶hler using various architectures
    are also provided.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ£€æŸ¥äº†æ‰€æœ‰åŸºäºæ·±åº¦å­¦ä¹ çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶æä¾›äº†å¯¹è¿‡å»å‡ å¹´ä¸­å‡ºç°çš„å„ç§æ¶æ„çš„å…¨é¢ç†è§£ï¼Œä»¥è§£å†³ç›²ç›®å»æ¨¡ç³Šçš„é—®é¢˜ã€‚æœ€åˆçš„å·¥ä½œåˆ©ç”¨æ·±åº¦å­¦ä¹ æ¥ä¼°è®¡æ¨¡ç³Šæ ¸çš„ä¸€äº›ç‰¹å¾ï¼Œç„¶åè½¬å‘å®Œå…¨é¢„æµ‹æ¨¡ç³Šæ ¸ï¼Œè¿™å°†é—®é¢˜è½¬æ¢ä¸ºéç›²å»æ¨¡ç³Šã€‚æœ€è¿‘çš„æœ€å…ˆè¿›æŠ€æœ¯æ˜¯ç«¯åˆ°ç«¯çš„ï¼Œå³å®ƒä»¬ä¸ä¼°è®¡æ¨¡ç³Šæ ¸ï¼Œè€Œæ˜¯ç›´æ¥ä»æ¨¡ç³Šå›¾åƒä¸­ä¼°è®¡æ½œåœ¨çš„æ¸…æ™°å›¾åƒã€‚è¿˜æä¾›äº†ä½¿ç”¨å„ç§æ¶æ„åœ¨æ ‡å‡†æ•°æ®é›†GOPROå’ŒKÃ¶hlerä¸Šçš„PSNRå’ŒSSIMå€¼çš„åŸºå‡†æµ‹è¯•ã€‚
- en: 'Index Terms:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Index Terms:'
- en: Deblurring, Deep Learning
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å»æ¨¡ç³Šï¼Œæ·±åº¦å­¦ä¹ 
- en: I Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I ä»‹ç»
- en: Present day imaging systems for instance consumer level photography cameras,
    medical imaging equipments, scientific astronomical imaging systems, microscopy
    and more may experience blurring due to various intrinsic (diffraction, lens chromatic
    aberration, anti-aliasing filters etc.) or extrinsic (object motion, camera shake,
    out of focus, atmospheric turbulence etc.) factors which results in loss of image
    information. To overcome this problem and to recover lost information, deblurring
    is of great interest. From an artistic perspective blur is sometimes intentional
    in photography but for majority of the image analysis applications blurs ruins
    useful data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰çš„æˆåƒç³»ç»Ÿï¼Œå¦‚æ¶ˆè´¹çº§æ‘„å½±ç›¸æœºã€åŒ»å­¦æˆåƒè®¾å¤‡ã€ç§‘å­¦å¤©æ–‡æˆåƒç³»ç»Ÿã€æ˜¾å¾®é•œç­‰ï¼Œå¯èƒ½ä¼šå› å„ç§å†…åœ¨ï¼ˆè¡å°„ã€é•œå¤´è‰²å·®ã€æŠ—é”¯é½¿æ»¤é•œç­‰ï¼‰æˆ–å¤–åœ¨ï¼ˆç‰©ä½“è¿åŠ¨ã€ç›¸æœºæŠ–åŠ¨ã€å¯¹ç„¦ä¸å‡†ã€å¤§æ°”æ‰°åŠ¨ç­‰ï¼‰å› ç´ è€Œå‡ºç°æ¨¡ç³Šï¼Œä»è€Œå¯¼è‡´å›¾åƒä¿¡æ¯çš„ä¸¢å¤±ã€‚ä¸ºäº†å…‹æœè¿™ä¸€é—®é¢˜å¹¶æ¢å¤ä¸¢å¤±çš„ä¿¡æ¯ï¼Œå»æ¨¡ç³Šæ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶æ–¹å‘ã€‚ä»è‰ºæœ¯è§’åº¦æ¥çœ‹ï¼Œæ¨¡ç³Šåœ¨æ‘„å½±ä¸­æœ‰æ—¶æ˜¯æ•…æ„çš„ï¼Œä½†å¯¹äºå¤§å¤šæ•°å›¾åƒåˆ†æåº”ç”¨æ¥è¯´ï¼Œæ¨¡ç³Šä¼šç ´åæœ‰ç”¨çš„æ•°æ®ã€‚
- en: The problem of deblurring is restoring a latent sharp image from a blurred image
    alone or at times with some statistical information about the blur kernel. This
    has attracted many researchers who have given many different solutions. These
    solutions can be broadly divided into statistical methods like,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å»æ¨¡ç³Šçš„é—®é¢˜æ˜¯ä»æ¨¡ç³Šå›¾åƒå•ç‹¬æ¢å¤æ½œåœ¨çš„æ¸…æ™°å›¾åƒï¼Œæœ‰æ—¶ä¹Ÿéœ€è¦ä¸€äº›å…³äºæ¨¡ç³Šæ ¸çš„ç»Ÿè®¡ä¿¡æ¯ã€‚è¿™å¸å¼•äº†è®¸å¤šç ”ç©¶äººå‘˜ï¼Œä»–ä»¬æä¾›äº†è®¸å¤šä¸åŒçš„è§£å†³æ–¹æ¡ˆã€‚è¿™äº›è§£å†³æ–¹æ¡ˆå¯ä»¥å¤§è‡´åˆ†ä¸ºç»Ÿè®¡æ–¹æ³•ï¼Œå¦‚ï¼Œ
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Bayesian Inference Framework
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è´å¶æ–¯æ¨æ–­æ¡†æ¶
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Variational Methods
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å˜åˆ†æ–¹æ³•
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Sparse Representation based method
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç¨€ç–è¡¨ç¤ºæ³•åŸºç¡€çš„æ–¹æ³•
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Homography based modeling
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºå•åº”æ€§çš„å»ºæ¨¡
- en: '5.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Region based methods
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åŸºäºåŒºåŸŸçš„æ–¹æ³•
- en: where we try to estimate the blur kernel from just a single given blurred image
    and learning based methods ([[1](#bib.bib1)], [[2](#bib.bib2)], [[3](#bib.bib3)],
    [[4](#bib.bib4)], [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)]) which is data driven and the blur kernel
    is learned by providing not just one but several examples of blur and its corresponding
    sharp images as ground truth.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°è¯•ä»ä»…ç»™å®šçš„æ¨¡ç³Šå›¾åƒä¸­ä¼°è®¡æ¨¡ç³Šæ ¸ï¼Œå­¦ä¹ åŸºç¡€çš„æ–¹æ³•ï¼ˆ[[1](#bib.bib1)], [[2](#bib.bib2)], [[3](#bib.bib3)],
    [[4](#bib.bib4)], [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)])æ˜¯æ•°æ®é©±åŠ¨çš„ï¼Œé€šè¿‡æä¾›å¤šä¸ªæ¨¡ç³Šå®ä¾‹åŠå…¶ç›¸åº”çš„æ¸…æ™°å›¾åƒä½œä¸ºçœŸå®å€¼æ¥å­¦ä¹ æ¨¡ç³Šæ ¸ã€‚
- en: A blurred image can be modeled using equation,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡ç³Šå›¾åƒå¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹å»ºæ¨¡ï¼Œ
- en: '|  | $B=K*I+N$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $B=K*I+N$ |  | (1) |'
- en: where $B$ is the blurred image, $K$ is the kernel, $I$ is the sharp image and
    $N$ is the additive noise. In blind deblurring we are given $B$ only, and our
    goal is to predict a latent image $L$ which is the closest approximation to the
    sharp image $I$. This is an ill-posed problem, as we have to predict both $L$
    and $K$. Predicting the kernel accurately is essential, else it may lead to various
    artifacts [[11](#bib.bib11)], using learning based approach gives an accurate
    estimate of blur kernel compared to statistical approaches or skips the kernel
    estimation process altogether (i.e end-to-end). After estimation of blur kernel
    the problem converts to non-blind deconvolution, which can be solved using methods([[12](#bib.bib12)],
    [[13](#bib.bib13)])
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $B$ æ˜¯æ¨¡ç³Šå›¾åƒï¼Œ$K$ æ˜¯å†…æ ¸ï¼Œ$I$ æ˜¯æ¸…æ™°å›¾åƒï¼Œ$N$ æ˜¯é™„åŠ å™ªå£°ã€‚åœ¨ç›²å»æ¨¡ç³Šä¸­ï¼Œæˆ‘ä»¬åªç»™å®š $B$ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é¢„æµ‹ä¸€ä¸ªæ½œåœ¨å›¾åƒ $L$ï¼Œå®ƒæ˜¯æœ€æ¥è¿‘æ¸…æ™°å›¾åƒ
    $I$ çš„è¿‘ä¼¼å€¼ã€‚è¿™æ˜¯ä¸€ä¸ªä¸é€‚å®šçš„é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬å¿…é¡»åŒæ—¶é¢„æµ‹ $L$ å’Œ $K$ã€‚å‡†ç¡®é¢„æµ‹å†…æ ¸æ˜¯è‡³å…³é‡è¦çš„ï¼Œå¦åˆ™å¯èƒ½å¯¼è‡´å„ç§ä¼ªå½±[[11](#bib.bib11)]ã€‚ä½¿ç”¨åŸºäºå­¦ä¹ çš„æ–¹æ³•ä¸ç»Ÿè®¡æ–¹æ³•ç›¸æ¯”ï¼Œæä¾›äº†æ›´å‡†ç¡®çš„æ¨¡ç³Šå†…æ ¸ä¼°è®¡ï¼Œæˆ–å®Œå…¨è·³è¿‡å†…æ ¸ä¼°è®¡è¿‡ç¨‹ï¼ˆå³ç«¯åˆ°ç«¯ï¼‰ã€‚åœ¨ä¼°è®¡æ¨¡ç³Šå†…æ ¸åï¼Œé—®é¢˜è½¬å˜ä¸ºéç›²å»å·ç§¯ï¼Œè¿™å¯ä»¥é€šè¿‡æ–¹æ³•([[12](#bib.bib12)],
    [[13](#bib.bib13)])æ¥è§£å†³ã€‚
- en: Statistical methods have another limitation i.e their inability to parallelize
    because a majority of them rely on coarse to fine iterative methods. Although
    deep learning models are significantly harder to train but once trained their
    inference time is comparatively fast. Moreover, deep learning methods have shown
    better on benchmarking metrics (PSNR and SSIM).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ç»Ÿè®¡æ–¹æ³•è¿˜æœ‰å¦ä¸€ä¸ªé™åˆ¶ï¼Œå³æ— æ³•å¹¶è¡ŒåŒ–ï¼Œå› ä¸ºå¤§å¤šæ•°æ–¹æ³•ä¾èµ–äºç²—åˆ°ç»†çš„è¿­ä»£æ–¹æ³•ã€‚å°½ç®¡æ·±åº¦å­¦ä¹ æ¨¡å‹è®­ç»ƒèµ·æ¥æ˜¾è‘—æ›´éš¾ï¼Œä½†è®­ç»ƒå®Œæˆåå…¶æ¨æ–­æ—¶é—´ç›¸å¯¹è¾ƒå¿«ã€‚æ­¤å¤–ï¼Œæ·±åº¦å­¦ä¹ æ–¹æ³•åœ¨åŸºå‡†æŒ‡æ ‡ï¼ˆPSNR
    å’Œ SSIMï¼‰ä¸Šè¡¨ç°æ›´ä½³ã€‚
- en: In this paper we have divided the deep learning methods into two broad categories
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡å°†æ·±åº¦å­¦ä¹ æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªå¹¿æ³›ç±»åˆ«
- en: '1.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Estimation of Kernel - Here the proposed deep learning architectures are used
    to estimate some features (Fourier coefficients[[1](#bib.bib1)], motion flow [[2](#bib.bib2)][[8](#bib.bib8)])
    of the blur kernel or deriving the deconvolution filter [[1](#bib.bib1)] which
    can be used to get back the sharp image.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å†…æ ¸ä¼°è®¡ - åœ¨è¿™é‡Œï¼Œæå‡ºçš„æ·±åº¦å­¦ä¹ æ¶æ„ç”¨äºä¼°è®¡æ¨¡ç³Šå†…æ ¸çš„ä¸€äº›ç‰¹å¾ï¼ˆå‚…é‡Œå¶ç³»æ•°[[1](#bib.bib1)]ï¼Œè¿åŠ¨æµ[[2](#bib.bib2)][[8](#bib.bib8)]ï¼‰æˆ–æ¨å¯¼å»å·ç§¯æ»¤æ³¢å™¨[[1](#bib.bib1)]ï¼Œå¯ä»¥ç”¨æ¥æ¢å¤æ¸…æ™°å›¾åƒã€‚
- en: '2.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: End to End - These methods are kernel free, that means we donâ€™t estimate the
    blur kernel, rather only the blurred image is required and the model generates
    the predicted restored image. Some of these methods rely on generative models
    ([[3](#bib.bib3)], [[6](#bib.bib6)], [[4](#bib.bib4)]) which are trained in an
    adversarial method.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ç«¯åˆ°ç«¯ - è¿™äº›æ–¹æ³•ä¸ä¾èµ–äºå†…æ ¸ï¼Œå³æˆ‘ä»¬ä¸ä¼°è®¡æ¨¡ç³Šå†…æ ¸ï¼Œåªéœ€æä¾›æ¨¡ç³Šå›¾åƒï¼Œæ¨¡å‹å°±ä¼šç”Ÿæˆé¢„æµ‹çš„æ¢å¤å›¾åƒã€‚å…¶ä¸­ä¸€äº›æ–¹æ³•ä¾èµ–äºç”Ÿæˆæ¨¡å‹ï¼ˆ[[3](#bib.bib3)],
    [[6](#bib.bib6)], [[4](#bib.bib4)]ï¼‰ï¼Œè¿™äº›æ¨¡å‹é€šè¿‡å¯¹æŠ—æ€§æ–¹æ³•è¿›è¡Œè®­ç»ƒã€‚
- en: The emphasis of this paper is on the â€œarchitectureâ€ proposed by several author
    instead of the specific details of the architecture and to foster further research
    in blind deblurring using learning based methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬æ–‡é‡ç‚¹è®¨è®ºäº†å‡ ä½ä½œè€…æå‡ºçš„â€œæ¶æ„â€ï¼Œè€Œä¸æ˜¯æ¶æ„çš„å…·ä½“ç»†èŠ‚ï¼Œå¹¶ä¿ƒè¿›äº†ä½¿ç”¨åŸºäºå­¦ä¹ çš„æ–¹æ³•è¿›è¡Œç›²å»æ¨¡ç³Šçš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚
- en: II Methods
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II æ–¹æ³•
- en: II-A Estimation of Kernel and its Attributes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A å†…æ ¸åŠå…¶å±æ€§çš„ä¼°è®¡
- en: II-A1 Extraction of Features
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 ç‰¹å¾æå–
- en: 'For a optimal deblurring we require global information i.e data from different
    parts of the image, but to do so we need to have connectivity with all the pixels
    of image which will lead to a huge parameter space making it difficult to train
    and converge, hence Schuler et. al.[[7](#bib.bib7)] uses CNNs to extract features
    locally and then combine them to estimate the kernel. For this they use a multi
    scale (for different kernel sizes), multi stage architecture, where each stage
    consists of three modules feature extraction, kernel estimation, latent image
    estimation (Fig. [1](#S2.F1 "Figure 1 â€£ II-A1 Extraction of Features â€£ II-A Estimation
    of Kernel and its Attributes â€£ II Methods â€£ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")). In the first stage given a blurry image,
    a sharp image is estimated, for later stages they gave the blurry image concatenated
    with the estimated sharp image of the previous stage as input.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å®ç°æœ€ä½³å»æ¨¡ç³Šï¼Œæˆ‘ä»¬éœ€è¦å…¨å±€ä¿¡æ¯ï¼Œå³æ¥è‡ªå›¾åƒä¸åŒéƒ¨åˆ†çš„æ•°æ®ï¼Œä½†ä¸ºæ­¤æˆ‘ä»¬éœ€è¦ä¸å›¾åƒçš„æ‰€æœ‰åƒç´ è¿æ¥ï¼Œè¿™å°†å¯¼è‡´åºå¤§çš„å‚æ•°ç©ºé—´ï¼Œä½¿å¾—è®­ç»ƒå’Œæ”¶æ•›å˜å¾—å›°éš¾ï¼Œå› æ­¤
    Schuler ç­‰äºº[[7](#bib.bib7)] ä½¿ç”¨ CNNs åœ¨å±€éƒ¨æå–ç‰¹å¾ï¼Œç„¶åå°†å…¶ç»“åˆèµ·æ¥ä¼°è®¡æ ¸ã€‚ä¸ºæ­¤ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå¤šå°ºåº¦ï¼ˆé’ˆå¯¹ä¸åŒçš„æ ¸å°ºå¯¸ï¼‰å¤šé˜¶æ®µæ¶æ„ï¼Œå…¶ä¸­æ¯ä¸ªé˜¶æ®µåŒ…å«ä¸‰ä¸ªæ¨¡å—ï¼šç‰¹å¾æå–ã€æ ¸ä¼°è®¡ã€æ½œåœ¨å›¾åƒä¼°è®¡ï¼ˆå›¾
    [1](#S2.F1 "å›¾ 1 â€£ II-A1 ç‰¹å¾æå– â€£ II-A æ ¸ä¼°è®¡åŠå…¶å±æ€§ â€£ II æ–¹æ³• â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„ç›²å»æ¨¡ç³Šï¼šç»¼è¿° *è¡¨ç¤ºç›¸ç­‰è´¡çŒ®")ï¼‰ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œç»™å®šä¸€ä¸ªæ¨¡ç³Šå›¾åƒï¼Œä¼°è®¡å‡ºä¸€ä¸ªæ¸…æ™°å›¾åƒï¼›åœ¨åç»­é˜¶æ®µï¼Œä»–ä»¬å°†æ¨¡ç³Šå›¾åƒä¸å‰ä¸€é˜¶æ®µçš„ä¼°è®¡æ¸…æ™°å›¾åƒä¸²è”ä½œä¸ºè¾“å…¥ã€‚
- en: '![Refer to caption](img/770bcff05adadea94ed7d93de1734ccd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/770bcff05adadea94ed7d93de1734ccd.png)'
- en: 'Figure 1: Shows the multi stage architecture used by Schuler et. al.[[7](#bib.bib7)],
    here the different modules in a stage are shown for the first stage only. The
    latter stages are identical to the first, except the input which is concatenation
    of blurred image and the restored image of the previous layer'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 1ï¼šå±•ç¤ºäº† Schuler ç­‰äºº[[7](#bib.bib7)] ä½¿ç”¨çš„å¤šé˜¶æ®µæ¶æ„ï¼Œè¿™é‡Œä»…å±•ç¤ºäº†ç¬¬ä¸€é˜¶æ®µçš„ä¸åŒæ¨¡å—ã€‚åç»­é˜¶æ®µä¸ç¬¬ä¸€é˜¶æ®µç›¸åŒï¼Œå”¯ä¸€ä¸åŒçš„æ˜¯è¾“å…¥ä¸ºæ¨¡ç³Šå›¾åƒå’Œå‰ä¸€å±‚æ¢å¤å›¾åƒçš„ä¸²è”
- en: In feature extraction module they used a convolutional layer to extract features
    using filters $f_{j}$, then they used $\tanh$ to introduce non-linearity and finally
    these hidden features are linearly recombined using cofficients $\alpha_{ij}$
    and $\beta_{ij}$ to form hidden images $x_{i}$ and $y_{i}$ for stage $i$ used
    for kernel estimation, formally,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç‰¹å¾æå–æ¨¡å—ä¸­ï¼Œä»–ä»¬ä½¿ç”¨äº†ä¸€ä¸ªå·ç§¯å±‚ï¼Œé€šè¿‡æ»¤æ³¢å™¨ $f_{j}$ æå–ç‰¹å¾ï¼Œç„¶åä½¿ç”¨ $\tanh$ å¼•å…¥éçº¿æ€§ï¼Œæœ€åè¿™äº›éšè—ç‰¹å¾ä½¿ç”¨ç³»æ•° $\alpha_{ij}$
    å’Œ $\beta_{ij}$ çº¿æ€§é‡ç»„ï¼Œå½¢æˆç”¨äºæ ¸ä¼°è®¡çš„éšè—å›¾åƒ $x_{i}$ å’Œ $y_{i}$ï¼Œæ­£å¼åœ°ï¼Œ
- en: '|  | $\begin{split}x_{i}=\sum_{j}\alpha_{ij}\tanh(f_{j}*y)\\ y_{i}=\sum_{j}\beta_{ij}\tanh(f_{j}*y)\end{split}$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}x_{i}=\sum_{j}\alpha_{ij}\tanh(f_{j}*y)\\ y_{i}=\sum_{j}\beta_{ij}\tanh(f_{j}*y)\end{split}$
    |  | (2) |'
- en: where $y$ is blurred image $B$ for first stage or concatenation of $B$ and predicted
    sharp image $L$ for later stages.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $y$ æ˜¯ç¬¬ä¸€é˜¶æ®µçš„æ¨¡ç³Šå›¾åƒ $B$ï¼Œæˆ–è€…æ˜¯åç»­é˜¶æ®µä¸­ $B$ å’Œé¢„æµ‹çš„æ¸…æ™°å›¾åƒ $L$ çš„ä¸²è”ã€‚
- en: Given $x_{i}$ and $y_{i}$ the kernel estimation module estimates the kernel
    $K$ by minimizing,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®š $x_{i}$ å’Œ $y_{i}$ï¼Œæ ¸ä¼°è®¡æ¨¡å—é€šè¿‡æœ€å°åŒ–æ¥ä¼°è®¡æ ¸ $K$ï¼Œ
- en: '|  | $\sum_{i}\lVert K*x_{i}-y_{i}\rVert^{2}+\beta_{k}\lVert K\rVert^{2}$ |  |
    (3) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{i}\lVert K*x_{i}-y_{i}\rVert^{2}+\beta_{k}\lVert K\rVert^{2}$ |  |
    (3) |'
- en: for $K$. Given $K$ we can find the latent (restored) image $L$ by solving the
    equation,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $K$ã€‚ç»™å®š $K$ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‚è§£æ–¹ç¨‹æ¥æ‰¾åˆ°æ½œåœ¨ï¼ˆæ¢å¤çš„ï¼‰å›¾åƒ $L$ï¼Œ
- en: '|  | $\lVert K*L-B\rVert^{2}+\beta_{x}\lVert L\rVert^{2}$ |  | (4) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lVert K*L-B\rVert^{2}+\beta_{x}\lVert L\rVert^{2}$ |  | (4) |'
- en: 'for $L$, where both $\beta_{k}$ and $\beta_{x}$ are regularization weights.
    Both (Eqn.[3](#S2.E3 "In II-A1 Extraction of Features â€£ II-A Estimation of Kernel
    and its Attributes â€£ II Methods â€£ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) and (Eqn.[4](#S2.E4 "In II-A1 Extraction of Features
    â€£ II-A Estimation of Kernel and its Attributes â€£ II Methods â€£ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) can be solved in
    one step in Fourier space.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äº $L$ï¼Œå…¶ä¸­ $\beta_{k}$ å’Œ $\beta_{x}$ éƒ½æ˜¯æ­£åˆ™åŒ–æƒé‡ã€‚æ–¹ç¨‹ï¼ˆEqn.[3](#S2.E3 "åœ¨ II-A1 ç‰¹å¾æå–
    â€£ II-A æ ¸ä¼°è®¡åŠå…¶å±æ€§ â€£ II æ–¹æ³• â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„ç›²å»æ¨¡ç³Šï¼šç»¼è¿° *è¡¨ç¤ºç›¸ç­‰è´¡çŒ®"ï¼‰å’Œï¼ˆEqn.[4](#S2.E4 "åœ¨ II-A1 ç‰¹å¾æå–
    â€£ II-A æ ¸ä¼°è®¡åŠå…¶å±æ€§ â€£ II æ–¹æ³• â€£ åŸºäºæ·±åº¦å­¦ä¹ çš„ç›²å»æ¨¡ç³Šï¼šç»¼è¿° *è¡¨ç¤ºç›¸ç­‰è´¡çŒ®"ï¼‰å¯ä»¥åœ¨å‚…é‡Œå¶ç©ºé—´ä¸­ä¸€æ­¥æ±‚è§£ã€‚
- en: II-A2 Estimation of Fourier Coefficients
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 å‚…é‡Œå¶ç³»æ•°ä¼°è®¡
- en: Given a blurry image $B[n]$ where $n\in\mathbb{Z}^{2}$ are the indexes of pixels.
    We need to find a latent sharp image $L[n]$ such that it resembles the sharp image
    $I[n]$ closely where,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šä¸€ä¸ªæ¨¡ç³Šå›¾åƒ $B[n]$ï¼Œå…¶ä¸­ $n\in\mathbb{Z}^{2}$ æ˜¯åƒç´ çš„ç´¢å¼•ã€‚æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ä¸ªæ½œåœ¨çš„æ¸…æ™°å›¾åƒ $L[n]$ï¼Œä½¿å…¶ä¸æ¸…æ™°å›¾åƒ
    $I[n]$ å…·æœ‰é«˜åº¦ç›¸ä¼¼æ€§ï¼Œå…¶ä¸­ï¼Œ
- en: '|  | $B[n]=(I*K)[n]+N[n]$ |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $B[n]=(I*K)[n]+N[n]$ |  | (5) |'
- en: where $K[n]$ is the blur kernel such that $K[n]\geq 0$ (positivity constraint),
    $\sum_{n}K[n]=1$ (unit sum constraint) and $N[n]$ the noise.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$K[n]$æ˜¯æ¨¡ç³Šæ ¸ï¼Œä½¿å¾—$K[n]\geq 0$ï¼ˆéè´Ÿæ€§çº¦æŸï¼‰ï¼Œ$\sum_{n}K[n]=1$ï¼ˆå•ä½å’Œçº¦æŸï¼‰ï¼Œ$N[n]$ä¸ºå™ªå£°ã€‚
- en: '![Refer to caption](img/c8c370d05d6c25d340d85e60b2ffd0d1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/c8c370d05d6c25d340d85e60b2ffd0d1.png)'
- en: 'Figure 2: Architecture used by Chakrabati[[1](#bib.bib1)] for prediction of
    Fourier coefficients for the deconvolution filter. Here $H$ is high pass, $B_{2},B_{1}$
    are band pass, while $L$ is low pass frequency band. The letters in bold are Fourier
    transforms of the corresponding symbols.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2ï¼šChakrabarti[[1](#bib.bib1)]ç”¨äºé¢„æµ‹å»å·ç§¯æ»¤æ³¢å™¨å‚…é‡Œå¶ç³»æ•°çš„æ¶æ„ã€‚è¿™é‡Œ$H$æ˜¯é«˜é€šï¼Œ$B_{2},B_{1}$æ˜¯å¸¦é€šï¼Œè€Œ$L$æ˜¯ä½é€šé¢‘ç‡å¸¦ã€‚åŠ ç²—çš„å­—æ¯æ˜¯å¯¹åº”ç¬¦å·çš„å‚…é‡Œå¶å˜æ¢ã€‚
- en: In the method given by Chakrabarti [[1](#bib.bib1)] a blurry image $B[n]$ is
    divided into several overlapping patches. Given a blurry patch $B_{p}=\{B[n]:n\in
    p\}$ they considered the surrounding pixels of the patch while finding its Fourier
    coefficients for better results, let the blurry image with the neighboring pixels
    be $B_{p^{+}}=\{B[n]:n\in p^{+}\}$ where $p\subset p^{+}$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Chakrabartiçš„æ–¹æ³•[[1](#bib.bib1)]ä¸­ï¼Œæ¨¡ç³Šå›¾åƒ$B[n]$è¢«åˆ’åˆ†ä¸ºå‡ ä¸ªé‡å çš„å›¾å—ã€‚å¯¹äºä¸€ä¸ªæ¨¡ç³Šå›¾å—$B_{p}=\{B[n]:n\in
    p\}$ï¼Œåœ¨å¯»æ‰¾å…¶å‚…é‡Œå¶ç³»æ•°æ—¶ï¼Œä»–ä»¬è€ƒè™‘äº†å›¾å—çš„å‘¨å›´åƒç´ ä»¥è·å¾—æ›´å¥½çš„ç»“æœï¼Œè®©æ¨¡ç³Šå›¾åƒä¸é‚»è¿‘åƒç´ ä¸€èµ·è¡¨ç¤ºä¸º$B_{p^{+}}=\{B[n]:n\in p^{+}\}$ï¼Œå…¶ä¸­$p\subset
    p^{+}$ã€‚
- en: 'Then they used a neural network (Fig. [2](#S2.F2 "Figure 2 â€£ II-A2 Estimation
    of Fourier Coefficients â€£ II-A Estimation of Kernel and its Attributes â€£ II Methods
    â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution"))
    to predict the Complex Fourier Coefficients of the Deconvolution Filter $\textbf{G}_{p^{+}}[z]$
    for the blurry patch $B_{p^{+}}$, where $z$ is the two dimensional spatial frequencies
    in DFT (Discrete Fourier Transform). Then the filter is applied to the DFT of
    $B_{p^{+}}$ i.e $\textbf{B}_{p^{+}}[z]$ giving us the DFT of latent sharp image
    $\textbf{L}_{p^{+}}[z]$,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç„¶åä»–ä»¬ä½¿ç”¨äº†ä¸€ä¸ªç¥ç»ç½‘ç»œï¼ˆå›¾ [2](#S2.F2 "Figure 2 â€£ II-A2 Estimation of Fourier Coefficients
    â€£ II-A Estimation of Kernel and its Attributes â€£ II Methods â€£ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")ï¼‰æ¥é¢„æµ‹æ¨¡ç³Šå›¾å—$B_{p^{+}}$çš„å»å·ç§¯æ»¤æ³¢å™¨çš„å¤å‚…é‡Œå¶ç³»æ•°$\textbf{G}_{p^{+}}[z]$ï¼Œå…¶ä¸­$z$æ˜¯ç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼ˆDFTï¼‰ä¸­çš„äºŒç»´ç©ºé—´é¢‘ç‡ã€‚ç„¶åå°†æ»¤æ³¢å™¨åº”ç”¨äº$B_{p^{+}}$çš„DFTï¼Œå³$\textbf{B}_{p^{+}}[z]$ï¼Œä»è€Œå¾—åˆ°æ½œåœ¨æ¸…æ™°å›¾åƒçš„DFT
    $\textbf{L}_{p^{+}}[z]$ï¼Œ'
- en: '|  | $\textbf{L}_{p^{+}}[z]=\textbf{B}_{p^{+}}[z]\times\textbf{G}_{p^{+}}[z]$
    |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{L}_{p^{+}}[z]=\textbf{B}_{p^{+}}[z]\times\textbf{G}_{p^{+}}[z]$
    |  | (6) |'
- en: After getting $\textbf{L}_{p^{+}}[z]$, we can use a inverse discrete Fourier
    transform (IDFT) to get the latent image patch $L_{p^{+}}$ from which we can extract
    $L_{p}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è·å¾—$\textbf{L}_{p^{+}}[z]$åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€†ç¦»æ•£å‚…é‡Œå¶å˜æ¢ï¼ˆIDFTï¼‰å¾—åˆ°æ½œåœ¨å›¾å—$L_{p^{+}}$ï¼Œä»ä¸­æå–$L_{p}$ã€‚
- en: 'To generate coefficients of the filter they used the architecture shown in
    (Fig. [2](#S2.F2 "Figure 2 â€£ II-A2 Estimation of Fourier Coefficients â€£ II-A Estimation
    of Kernel and its Attributes â€£ II Methods â€£ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")). The architecture uses multi-resolution
    decomposition strategy i.e the initial layers of the neural network are connected
    to only adjacent bands of frequency and not fully connected (here they are considering
    locality in the frequency domain, in contrast to CNNs which consider locality
    in the spatial domain). The image is sampled into patches of various resolution
    and a lower resolution patch is used to sample a higher frequency band using DFT.
    The loss function for the network is,'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä¸ºäº†ç”Ÿæˆæ»¤æ³¢å™¨çš„ç³»æ•°ï¼Œä»–ä»¬ä½¿ç”¨äº†å›¾ä¸­æ‰€ç¤ºçš„æ¶æ„ï¼ˆå›¾ [2](#S2.F2 "Figure 2 â€£ II-A2 Estimation of Fourier
    Coefficients â€£ II-A Estimation of Kernel and its Attributes â€£ II Methods â€£ Blind
    Deblurring using Deep Learning: A Survey *denotes equal contribution")ï¼‰ã€‚è¯¥æ¶æ„ä½¿ç”¨äº†å¤šåˆ†è¾¨ç‡åˆ†è§£ç­–ç•¥ï¼Œå³ç¥ç»ç½‘ç»œçš„åˆå§‹å±‚ä»…è¿æ¥åˆ°ç›¸é‚»çš„é¢‘ç‡å¸¦ï¼Œè€Œä¸æ˜¯å®Œå…¨è¿æ¥ï¼ˆåœ¨è¿™é‡Œï¼Œä»–ä»¬è€ƒè™‘äº†é¢‘ç‡åŸŸä¸­çš„å±€éƒ¨æ€§ï¼Œä¸è€ƒè™‘ç©ºé—´åŸŸä¸­å±€éƒ¨æ€§çš„CNNså½¢æˆå¯¹æ¯”ï¼‰ã€‚å›¾åƒè¢«é‡‡æ ·ä¸ºä¸åŒåˆ†è¾¨ç‡çš„å›¾å—ï¼Œå¹¶ä¸”ä½åˆ†è¾¨ç‡å›¾å—ç”¨äºä½¿ç”¨DFTé‡‡æ ·æ›´é«˜çš„é¢‘ç‡å¸¦ã€‚ç½‘ç»œçš„æŸå¤±å‡½æ•°ä¸ºï¼Œ'
- en: '|  | $L=\dfrac{1}{&#124;p&#124;}\sum_{n\in p}(L_{p}[n]-I_{p}[n])^{2}$ |  |
    (7) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\dfrac{1}{&#124;p&#124;}\sum_{n\in p}(L_{p}[n]-I_{p}[n])^{2}$ |  |
    (7) |'
- en: They combined all the restored patches to get the first estimate of the latent
    image $L_{N}[n]$. It is assumed that the entire image is blurred by the same motion
    kernel(uniform blur), but they predicted different motion kernels for different
    patches, hence to find a global motion kernel $K_{\lambda}[n]$ they used the first
    estimate $L_{N}[n]$ as follows,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬å°†æ‰€æœ‰æ¢å¤çš„å›¾å—ç»“åˆèµ·æ¥ï¼Œå¾—åˆ°æ½œåœ¨å›¾åƒ$L_{N}[n]$çš„åˆæ­¥ä¼°è®¡ã€‚å‡è®¾æ•´ä¸ªå›¾åƒæ˜¯ç”±ç›¸åŒçš„è¿åŠ¨æ ¸ï¼ˆå‡åŒ€æ¨¡ç³Šï¼‰æ¨¡ç³Šçš„ï¼Œä½†ä»–ä»¬ä¸ºä¸åŒçš„å›¾å—é¢„æµ‹äº†ä¸åŒçš„è¿åŠ¨æ ¸ï¼Œå› æ­¤ä¸ºäº†æ‰¾åˆ°å…¨å±€è¿åŠ¨æ ¸$K_{\lambda}[n]$ï¼Œä»–ä»¬ä½¿ç”¨äº†åˆæ­¥ä¼°è®¡$L_{N}[n]$ï¼Œå…¶æ–¹å¼å¦‚ä¸‹ï¼Œ
- en: '|  | $K_{\lambda}=arg\text{ min}\sum_{i}\lVert(K*(f_{i}*L_{N}))-(f_{i}*B)\rVert^{2}+\lambda\sum_{n}&#124;K[n]&#124;$
    |  | (8) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $K_{\lambda}=arg\text{ min}\sum_{i}\lVert(K*(f_{i}*L_{N}))-(f_{i}*B)\rVert^{2}+\lambda\sum_{n}&#124;K[n]&#124;$
    |  | (8) |'
- en: Here $f_{i}$ are different derivative filters. They use $L1$ regularization.
    In classical statistical methods refining latent image from a previous estimate
    is an iterative step, while here they only do it once to estimate the global blur
    kernel. After estimation of the global blur kernel, the problem becomes that of
    a non-blind deblurring and latent sharp image can be estimated using deconvolution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ$f_{i}$æ˜¯ä¸åŒçš„å¯¼æ•°æ»¤æ³¢å™¨ã€‚å®ƒä»¬ä½¿ç”¨$L1$æ­£åˆ™åŒ–ã€‚åœ¨ç»å…¸ç»Ÿè®¡æ–¹æ³•ä¸­ï¼Œä»å…ˆå‰ä¼°è®¡ä¸­æç‚¼æ½œåœ¨å›¾åƒæ˜¯ä¸€ä¸ªè¿­ä»£æ­¥éª¤ï¼Œè€Œåœ¨è¿™é‡Œï¼Œå®ƒä»¬åªè¿›è¡Œä¸€æ¬¡ä»¥ä¼°è®¡å…¨å±€æ¨¡ç³Šæ ¸ã€‚å…¨å±€æ¨¡ç³Šæ ¸ä¼°è®¡åï¼Œé—®é¢˜å˜æˆäº†éç›²å»æ¨¡ç³Šé—®é¢˜ï¼Œæ½œåœ¨çš„æ¸…æ™°å›¾åƒå¯ä»¥é€šè¿‡å»å·ç§¯æ¥ä¼°è®¡ã€‚
- en: II-A3 Estimation of Motion Vector for each Patch
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 æ¯ä¸ªè¡¥ä¸çš„è¿åŠ¨å‘é‡ä¼°è®¡
- en: '![Refer to caption](img/366b4c63b6765a9c97ae65a1ded15b1c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/366b4c63b6765a9c97ae65a1ded15b1c.png)'
- en: 'Figure 3: Network architecture for predicting the motion kernel of a given
    blurred patch used by Sun et. al.[[8](#bib.bib8)]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾3ï¼šSunç­‰äººä½¿ç”¨çš„é¢„æµ‹ç»™å®šæ¨¡ç³Šè¡¥ä¸çš„è¿åŠ¨æ ¸çš„ç½‘ç»œæ¶æ„[[8](#bib.bib8)]
- en: 'In this method proposed by Sun et. al.[[8](#bib.bib8)] an image is divided
    into several overlapping patches. For each patch a CNN with a fully connected
    layer and softmax layer is used to find the probability distribution of motion
    kernels for that patch (Fig.[3](#S2.F3 "Figure 3 â€£ II-A3 Estimation of Motion
    Vector for each Patch â€£ II-A Estimation of Kernel and its Attributes â€£ II Methods
    â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")).
    Given a patch $\Psi_{p}$ centered at pixel p, the network finds a probability
    distribution,'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨Sunç­‰äººæå‡ºçš„æ–¹æ³•[[8](#bib.bib8)]ä¸­ï¼Œä¸€å¹…å›¾åƒè¢«åˆ†æˆå‡ ä¸ªé‡å çš„è¡¥ä¸ã€‚å¯¹äºæ¯ä¸ªè¡¥ä¸ï¼Œä½¿ç”¨ä¸€ä¸ªå…·æœ‰å…¨è¿æ¥å±‚å’Œsoftmaxå±‚çš„CNNæ¥æ‰¾åˆ°è¯¥è¡¥ä¸çš„è¿åŠ¨æ ¸æ¦‚ç‡åˆ†å¸ƒï¼ˆè§å›¾[3](#S2.F3
    "Figure 3 â€£ II-A3 Estimation of Motion Vector for each Patch â€£ II-A Estimation
    of Kernel and its Attributes â€£ II Methods â€£ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")ï¼‰ã€‚ç»™å®šä»¥åƒç´ $p$ä¸ºä¸­å¿ƒçš„è¡¥ä¸$\Psi_{p}$ï¼Œç½‘ç»œæ‰¾å‡ºä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œ'
- en: '|  | $P(m=(l,o)&#124;\Psi_{p})$ |  | (9) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(m=(l,o)&#124;\Psi_{p})$ |  | (9) |'
- en: where $m=(l,o)$ is the motion kernel with length $l$ and orientation $o$. Here
    $l\in S^{l}$ and $o\in S^{o}$ both $S^{l}$ and $S^{o}$ are discretized sets of
    length and orientation. Due to discretization the number of motion kernels is
    limited which leads to blocky artifacts. Hence, they rotated the image and its
    corresponding motion kernel by the same amount to get new data entry, which is
    then used in training this increases the range of $S^{o}$ that is given a patch
    $\Psi_{p}(I)$ of image $I$ and its corresponding motion kernel $m=(l,o)$, if image
    is rotated by an angel of $\theta$ then for patch $\Psi_{p}(I_{\theta})$ they
    got the motion kernel as $m=(l,o-\theta)$. Since they are doing a multicalss classification(where
    each class is a motion kernel) they use cross entropy loss given as,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$m=(l,o)$æ˜¯å…·æœ‰é•¿åº¦$l$å’Œæ–¹å‘$o$çš„è¿åŠ¨æ ¸ã€‚è¿™é‡Œ$l\in S^{l}$å’Œ$o\in S^{o}$ï¼Œ$S^{l}$å’Œ$S^{o}$éƒ½æ˜¯ç¦»æ•£åŒ–çš„é•¿åº¦å’Œæ–¹å‘é›†åˆã€‚ç”±äºç¦»æ•£åŒ–ï¼Œè¿åŠ¨æ ¸çš„æ•°é‡æœ‰é™ï¼Œè¿™å¯¼è‡´å—çŠ¶ä¼ªå½±ã€‚å› æ­¤ï¼Œä»–ä»¬é€šè¿‡ç›¸åŒçš„è§’åº¦æ—‹è½¬å›¾åƒåŠå…¶å¯¹åº”çš„è¿åŠ¨æ ¸æ¥è·å¾—æ–°çš„æ•°æ®æ¡ç›®ï¼Œç„¶åç”¨äºè®­ç»ƒï¼Œè¿™å¢åŠ äº†$S^{o}$çš„èŒƒå›´ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç»™å®šå›¾åƒ$I$çš„è¡¥ä¸$\Psi_{p}(I)$åŠå…¶å¯¹åº”çš„è¿åŠ¨æ ¸$m=(l,o)$ï¼Œå¦‚æœå›¾åƒæ—‹è½¬äº†è§’åº¦$\theta$ï¼Œé‚£ä¹ˆå¯¹äºè¡¥ä¸$\Psi_{p}(I_{\theta})$ï¼Œä»–ä»¬å¾—åˆ°äº†è¿åŠ¨æ ¸$m=(l,o-\theta)$ã€‚ç”±äºä»–ä»¬æ­£åœ¨è¿›è¡Œå¤šç±»åˆ†ç±»ï¼ˆæ¯ä¸ªç±»åˆ«æ˜¯ä¸€ä¸ªè¿åŠ¨æ ¸ï¼‰ï¼Œä»–ä»¬ä½¿ç”¨çš„äº¤å‰ç†µæŸå¤±å®šä¹‰ä¸ºï¼š
- en: '|  | $P(m=(l,o)&#124;\Psi)=\dfrac{\exp(z_{i})}{\sum_{k=1}^{n}\exp(z_{k})}$
    |  | (10) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(m=(l,o)&#124;\Psi)=\dfrac{\exp(z_{i})}{\sum_{k=1}^{n}\exp(z_{k})}$
    |  | (10) |'
- en: where $z$ is the output of the last fully connected layer and $n=|S^{l}|\times|S^{o}|$
    i.e $n$ is the total number of motion kernels. Since the patches are overlapping
    many patches may contain the same pixel, in such case the confidence of motion
    kernel $m$ at a pixel $p$ is given as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$z$æ˜¯æœ€åä¸€ä¸ªå…¨è¿æ¥å±‚çš„è¾“å‡ºï¼Œ$n=|S^{l}|\times|S^{o}|$ï¼Œå³$n$æ˜¯è¿åŠ¨æ ¸çš„æ€»æ•°ã€‚ç”±äºè¡¥ä¸æ˜¯é‡å çš„ï¼Œè®¸å¤šè¡¥ä¸å¯èƒ½åŒ…å«ç›¸åŒçš„åƒç´ ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåƒç´ $p$å¤„è¿åŠ¨æ ¸$m$çš„ç½®ä¿¡åº¦è®¡ç®—å¦‚ä¸‹ï¼š
- en: '|  | $C(m_{p}=(l,o))=\dfrac{1}{Z}\sum_{q:p\in\Psi_{q}}G_{\sigma}(\lVert x_{p}-x_{q}\rVert^{2})P(m=(l,o)&#124;\Psi_{q})$
    |  | (11) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $C(m_{p}=(l,o))=\dfrac{1}{Z}\sum_{q:p\in\Psi_{q}}G_{\sigma}(\lVert x_{p}-x_{q}\rVert^{2})P(m=(l,o)&#124;\Psi_{q})$
    |  | (11) |'
- en: where $q$ is center pixel of patch $\Psi_{q}$ such that $p\in\Psi_{q}$. $x_{p}$
    and $x_{q}$ are the coordinates of $p$ and $q$ respectively. $G_{\sigma}$ is a
    Gaussian function that gives more weight to patches whose center pixel $q$ is
    closest to $p$. $Z$ is a normalization constant.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­$q$æ˜¯è¡¥ä¸$\Psi_{q}$çš„ä¸­å¿ƒåƒç´ ï¼Œä½¿$p\in\Psi_{q}$ã€‚$x_{p}$å’Œ$x_{q}$åˆ†åˆ«æ˜¯$p$å’Œ$q$çš„åæ ‡ã€‚$G_{\sigma}$æ˜¯ä¸€ä¸ªé«˜æ–¯å‡½æ•°ï¼Œå®ƒå¯¹ä¸­å¿ƒåƒç´ $q$æœ€æ¥è¿‘$p$çš„è¡¥ä¸èµ‹äºˆæ›´å¤šçš„æƒé‡ã€‚$Z$æ˜¯å½’ä¸€åŒ–å¸¸æ•°ã€‚
- en: '![Refer to caption](img/a5b092044feb6aff493d2484eb3acf5a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/a5b092044feb6aff493d2484eb3acf5a.png)'
- en: 'Figure 4: This shows given a pixel $p$ (in yellow) how MRF smoothen its value
    based on $N(p)$ i.e its neighboring pixels'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 4ï¼šè¿™æ˜¾ç¤ºäº†ç»™å®šä¸€ä¸ªåƒç´  $p$ï¼ˆé»„è‰²ï¼‰æ—¶ï¼ŒMRF å¦‚ä½•æ ¹æ® $N(p)$ å³å…¶é‚»è¿‘åƒç´ æ¥å¹³æ»‘å…¶å€¼
- en: 'After estimating the motion kernels for all the patches they are concatenated
    and a Markov Random Function (MRF) is used to merge them all together, smoothen
    the transition of motion kernels amongst neighboring pixels (Fig.[4](#S2.F4 "Figure
    4 â€£ II-A3 Estimation of Motion Vector for each Patch â€£ II-A Estimation of Kernel
    and its Attributes â€£ II Methods â€£ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) and generates a dense motion field by minimizing
    energy function,'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 'åœ¨ä¼°è®¡äº†æ‰€æœ‰è¡¥ä¸çš„è¿åŠ¨æ ¸åï¼Œå°†å®ƒä»¬è¿æ¥èµ·æ¥ï¼Œå¹¶ä½¿ç”¨é©¬å°”å¯å¤«éšæœºå‡½æ•° (MRF) å°†å®ƒä»¬åˆå¹¶åœ¨ä¸€èµ·ï¼Œå¹³æ»‘é‚»è¿‘åƒç´ ä¹‹é—´è¿åŠ¨æ ¸çš„è¿‡æ¸¡ï¼ˆå›¾ã€‚[4](#S2.F4
    "Figure 4 â€£ II-A3 Estimation of Motion Vector for each Patch â€£ II-A Estimation
    of Kernel and its Attributes â€£ II Methods â€£ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")ï¼‰ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–èƒ½é‡å‡½æ•°ç”Ÿæˆå¯†é›†è¿åŠ¨åœºã€‚'
- en: '|  | $\sum_{p\in\Omega}[-C(m_{p}=(l_{p},o_{p}))+\sum_{q\in N(p)}\lambda[(u_{p}-u_{q})^{2}+(v_{p}-v_{q})^{2}]]$
    |  | (12) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{p\in\Omega}[-C(m_{p}=(l_{p},o_{p}))+\sum_{q\in N(p)}\lambda[(u_{p}-u_{q})^{2}+(v_{p}-v_{q})^{2}]]$
    |  | (12) |'
- en: where $\Omega$ is a image region. $u_{p},u_{q},v_{p},v_{q}$ are defined as $u_{i}=l_{i}\cos(o_{i})$
    and $v_{i}=l_{i}\sin(o_{i})$ for $i=p,q$. $N(p)$ is the neighborhood of $p$. The
    first term gives more weight to using the motion kernel that the CNN chose with
    the highest confidence, while the second term looks at the neighboring pixels
    and tries to smoothen it. After predicting the motion field they deconvolve the
    blurred image with it to get the deblurred image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\Omega$ æ˜¯å›¾åƒåŒºåŸŸã€‚ $u_{p},u_{q},v_{p},v_{q}$ å®šä¹‰ä¸º $u_{i}=l_{i}\cos(o_{i})$ å’Œ
    $v_{i}=l_{i}\sin(o_{i})$ å¯¹äº $i=p,q$ã€‚ $N(p)$ æ˜¯ $p$ çš„é‚»åŸŸã€‚ç¬¬ä¸€ä¸ªé¡¹èµ‹äºˆ CNN é€‰æ‹©çš„å…·æœ‰æœ€é«˜ä¿¡å¿ƒçš„è¿åŠ¨æ ¸æ›´å¤šçš„æƒé‡ï¼Œè€Œç¬¬äºŒä¸ªé¡¹æŸ¥çœ‹é‚»è¿‘åƒç´ å¹¶å°è¯•å¹³æ»‘ã€‚é¢„æµ‹è¿åŠ¨åœºåï¼Œä»–ä»¬ä½¿ç”¨å…¶å¯¹æ¨¡ç³Šå›¾åƒè¿›è¡Œåå·ç§¯ï¼Œä»¥è·å¾—å»æ¨¡ç³Šå›¾åƒã€‚
- en: II-A4 Estimation of Dense Motion Flow for entire Image
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 æ•´å¹…å›¾åƒçš„å¯†é›†è¿åŠ¨æµä¼°è®¡
- en: In the previous approach of Sun et. al[[8](#bib.bib8)] a motion kernel was predicted
    for each patch using a CNN classifier and then all the motion kernels were smoothened
    using Markov Random Field (MRF) to get the dense motion field. In the method used
    by Gong et. al.[[2](#bib.bib2)] they also predict a dense motion field, but a
    pixel wise dense motion field is generated for the entire image directly (i.e
    image is not divided into patches).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Sun ç­‰äºº[[8](#bib.bib8)] çš„å‰ä¸€ç§æ–¹æ³•ä¸­ï¼Œä½¿ç”¨ CNN åˆ†ç±»å™¨é¢„æµ‹æ¯ä¸ªè¡¥ä¸çš„è¿åŠ¨æ ¸ï¼Œç„¶ååˆ©ç”¨é©¬å°”å¯å¤«éšæœºåœº (MRF) å¹³æ»‘æ‰€æœ‰è¿åŠ¨æ ¸ï¼Œä»¥è·å¾—å¯†é›†çš„è¿åŠ¨åœºã€‚åœ¨
    Gong ç­‰äºº[[2](#bib.bib2)] ä½¿ç”¨çš„æ–¹æ³•ä¸­ï¼Œä»–ä»¬ä¹Ÿé¢„æµ‹äº†å¯†é›†çš„è¿åŠ¨åœºï¼Œä½†ä¸ºæ•´ä¸ªå›¾åƒç›´æ¥ç”Ÿæˆäº†é€åƒç´ çš„å¯†é›†è¿åŠ¨åœºï¼ˆå³å›¾åƒæœªè¢«åˆ’åˆ†ä¸ºè¡¥ä¸ï¼‰ã€‚
- en: '![Refer to caption](img/a3ca9c291a4eb84e69c9ba281a9ddb4b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§è¯´æ˜](img/a3ca9c291a4eb84e69c9ba281a9ddb4b.png)'
- en: 'Figure 5: Architecture used by Gong et. al.[[2](#bib.bib2)] to predict the
    motion field given a blurry image'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 5ï¼šGong ç­‰äºº[[2](#bib.bib2)] ç”¨äºé¢„æµ‹ç»™å®šæ¨¡ç³Šå›¾åƒçš„è¿åŠ¨åœºçš„æ¶æ„
- en: In Sun et. al[[8](#bib.bib8)] they assumeded uniform motion blur within a single
    patch as only one motion kernel was chosen for a patch. This does not generalize
    to real life data properly were we can have a heterogeneous motion blur i.e motion
    may vary from pixel to pixel. In such cases an end to end approach of generating
    motion field [[2](#bib.bib2)] can give better results as they are considering
    the entire image (larger spatial context) instead of a single patch. Hence, this
    method is suitable for heterogeneous motion blurs. It does not require any post
    processing like MRF.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Sun ç­‰äºº[[8](#bib.bib8)] çš„æ–¹æ³•ä¸­ï¼Œä»–ä»¬å‡è®¾å•ä¸ªè¡¥ä¸å†…çš„è¿åŠ¨æ¨¡ç³Šæ˜¯å‡åŒ€çš„ï¼Œå› ä¸ºä¸€ä¸ªè¡¥ä¸åªé€‰æ‹©äº†ä¸€ä¸ªè¿åŠ¨æ ¸ã€‚è¿™å¹¶ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å®é™…æ•°æ®ä¸­ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½ä¼šæœ‰å¼‚è´¨è¿åŠ¨æ¨¡ç³Šï¼Œå³è¿åŠ¨å¯èƒ½å› åƒç´ è€Œå¼‚ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç”Ÿæˆè¿åŠ¨åœºçš„ç«¯åˆ°ç«¯æ–¹æ³•[[2](#bib.bib2)]
    å¯ä»¥æä¾›æ›´å¥½çš„ç»“æœï¼Œå› ä¸ºå®ƒä»¬è€ƒè™‘äº†æ•´ä¸ªå›¾åƒï¼ˆæ›´å¤§çš„ç©ºé—´èƒŒæ™¯ï¼‰ï¼Œè€Œä¸æ˜¯å•ä¸ªè¡¥ä¸ã€‚å› æ­¤ï¼Œè¿™ç§æ–¹æ³•é€‚ç”¨äºå¼‚è´¨è¿åŠ¨æ¨¡ç³Šã€‚ä¸éœ€è¦åƒ MRF è¿™æ ·çš„åå¤„ç†ã€‚
- en: If the network is represented by a function of $f$. Then given a blurred image
    $B$, the goal of the network is to generate the motion field $M$, i.e
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœç½‘ç»œç”±å‡½æ•° $f$ è¡¨ç¤ºã€‚åˆ™ç»™å®šæ¨¡ç³Šå›¾åƒ $B$ï¼Œç½‘ç»œçš„ç›®æ ‡æ˜¯ç”Ÿæˆè¿åŠ¨åœº $M$ï¼Œå³
- en: '|  | $f(B)=M$ |  | (13) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(B)=M$ |  | (13) |'
- en: where the motion field $M$ can be represented as,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿åŠ¨åœº $M$ å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ
- en: '|  | $M=(U,V)$ |  | (14) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $M=(U,V)$ |  | (14) |'
- en: where $U$ and $V$ are the horizontal and vertical motion maps respectively.
    Now given a pixel $p=(i,j)$ where $(i,j)$ are the coordinates of pixel, then we
    get,
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $U$ å’Œ $V$ åˆ†åˆ«æ˜¯æ°´å¹³å’Œå‚ç›´è¿åŠ¨å›¾ã€‚ç°åœ¨ç»™å®šä¸€ä¸ªåƒç´  $p=(i,j)$ï¼Œå…¶ä¸­ $(i,j)$ æ˜¯åƒç´ çš„åæ ‡ï¼Œåˆ™æˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: '|  | $M(i,j)=(U(i,j),V(i,j))$ |  | (15) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $M(i,j)=(U(i,j),V(i,j))$ |  | (15) |'
- en: let $M(i,j)=m_{p}$, $U(i,j)=u_{p}$ and $V(i,j)=v_{p}$ then we get,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¤ $M(i,j)=m_{p}$, $U(i,j)=u_{p}$ å’Œ $V(i,j)=v_{p}$ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼Œ
- en: '|  | $m_{p}=(u_{p},v_{p})$ |  | (16) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $m_{p}=(u_{p},v_{p})$ |  | (16) |'
- en: where $u_{p}\in\mathbb{D}_{u}$ and $v_{p}\in\mathbb{D}_{v}$. Here $\mathbb{D}_{u}$
    and $\mathbb{D}_{v}$ are the discretized motion vectors in the horizontal and
    vertical directions respectively, they are defined as, $\mathbb{D}_{u}=\{u|u\in\mathbb{Z},|u|\leq
    u_{max}\}$ and $\mathbb{D}_{v}=\{v|v\in\mathbb{Z},|v|\leq v_{max}\}$.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $u_{p}\in\mathbb{D}_{u}$ å’Œ $v_{p}\in\mathbb{D}_{v}$ã€‚è¿™é‡Œ $\mathbb{D}_{u}$ å’Œ
    $\mathbb{D}_{v}$ åˆ†åˆ«æ˜¯æ°´å¹³æ–¹å‘å’Œå‚ç›´æ–¹å‘çš„ç¦»æ•£åŒ–è¿åŠ¨çŸ¢é‡ï¼Œå®ƒä»¬å®šä¹‰ä¸ºï¼Œ$\mathbb{D}_{u}=\{u|u\in\mathbb{Z},|u|\leq
    u_{max}\}$ å’Œ $\mathbb{D}_{v}=\{v|v\in\mathbb{Z},|v|\leq v_{max}\}$ã€‚
- en: But, two motion vectors of opposite directions and same magnitude would generate
    the same blur pattern i.e $m_{p}=(u_{p},v_{p})$ and $-m_{p}=(-u_{p},-v_{p})$ would
    give the same blur, hence they restrict the horizontal motion vector to be positive
    only i.e $u_{p}\in\mathbb{D}_{u}^{+}$ where $\mathbb{D}_{u}^{+}=\{u|u\in\mathbb{Z}^{+},|u|\leq
    u_{max}\}$, this is done by letting $(u_{p},v_{p})=\phi(u_{p},v_{p})$ where,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œç›¸åæ–¹å‘ä¸”å¹…åº¦ç›¸åŒçš„ä¸¤ä¸ªè¿åŠ¨çŸ¢é‡ä¼šç”Ÿæˆç›¸åŒçš„æ¨¡ç³Šæ¨¡å¼ï¼Œå³ $m_{p}=(u_{p},v_{p})$ å’Œ $-m_{p}=(-u_{p},-v_{p})$
    ä¼šäº§ç”Ÿç›¸åŒçš„æ¨¡ç³Šï¼Œå› æ­¤ä»–ä»¬é™åˆ¶æ°´å¹³è¿åŠ¨çŸ¢é‡åªèƒ½ä¸ºæ­£ï¼Œå³ $u_{p}\in\mathbb{D}_{u}^{+}$ï¼Œå…¶ä¸­ $\mathbb{D}_{u}^{+}=\{u|u\in\mathbb{Z}^{+},|u|\leq
    u_{max}\}$ï¼Œè¿™æ˜¯é€šè¿‡è®© $(u_{p},v_{p})=\phi(u_{p},v_{p})$ å®ç°çš„ï¼Œå…¶ä¸­ï¼Œ
- en: '|  | $\phi(u_{p},v_{p})=\begin{cases}(-u_{p},-v_{p})\text{ if }u_{p}<0\\ (u_{p},v_{p})\text{
    if }u_{p}\geq 0\end{cases}$ |  | (17) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi(u_{p},v_{p})=\begin{cases}(-u_{p},-v_{p})\text{ å¦‚æœ }u_{p}<0\\ (u_{p},v_{p})\text{
    å¦‚æœ }u_{p}\geq 0\end{cases}$ |  | (17) |'
- en: If an image of size $P\times Q$ is sent trough the network(excluding the softmax)
    it generates a feature map of size $P\times Q\times D$ where $D=|\mathbb{D}_{u}^{+}|+|\mathbb{D}_{v}|$.
    This feature map is then divided into two parts of shape $P\times Q\times|\mathbb{D}_{u}^{+}|$
    and $P\times Q\times|\mathbb{D}_{v}|$. These two features pass through separate
    softmax layers to generate the horizontal and vertical motion maps $U$ and $V$
    receptively. Using these two vector maps they generate the final motion field
    $M$. After getting the motion field $M$ this becomes a non-blind debluring problem
    and a deconvolution is used to get the sharp image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¸€å¼  $P\times Q$ å°ºå¯¸çš„å›¾åƒé€šè¿‡ç½‘ç»œï¼ˆä¸åŒ…æ‹¬ softmaxï¼‰ï¼Œå®ƒä¼šç”Ÿæˆä¸€ä¸ª $P\times Q\times D$ å°ºå¯¸çš„ç‰¹å¾å›¾ï¼Œå…¶ä¸­
    $D=|\mathbb{D}_{u}^{+}|+|\mathbb{D}_{v}|$ã€‚è¯¥ç‰¹å¾å›¾éšåè¢«åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œå½¢çŠ¶ä¸º $P\times Q\times|\mathbb{D}_{u}^{+}|$
    å’Œ $P\times Q\times|\mathbb{D}_{v}|$ã€‚è¿™ä¸¤ä¸ªç‰¹å¾é€šè¿‡ç‹¬ç«‹çš„ softmax å±‚ç”Ÿæˆæ°´å¹³å’Œå‚ç›´è¿åŠ¨å›¾ $U$ å’Œ $V$ã€‚åˆ©ç”¨è¿™ä¸¤ä¸ªçŸ¢é‡å›¾ç”Ÿæˆæœ€ç»ˆçš„è¿åŠ¨åœº
    $M$ã€‚åœ¨å¾—åˆ°è¿åŠ¨åœº $M$ åï¼Œè¿™å˜æˆäº†ä¸€ä¸ªéç›²å»æ¨¡ç³Šé—®é¢˜ï¼Œä½¿ç”¨åå·ç§¯æ¥è·å–æ¸…æ™°å›¾åƒã€‚
- en: II-B End to End
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B ç«¯åˆ°ç«¯
- en: II-B1 Without Adversarial Loss
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 æ— å¯¹æŠ—æŸå¤±
- en: 'Deblurring requires a large receptive field (global knowledge), but CNNs provide
    local knowledge and do not show the long range dependencies properly, for this
    reason Nah et. al[[4](#bib.bib4)] (refer [II-B2](#S2.SS2.SSS2 "II-B2 With Adversarial
    Loss â€£ II-B End to End â€£ II Methods â€£ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")) uses a scaled structure and a large number
    of convolutional layers with residual connections to improve the receptive field
    of the structure, but this also makes it harder to converge due to the large number
    of parameters. Hence (Tao et. al.[[9](#bib.bib9)]) use a scale recurrent structure
    where they still use a scaled network, but significantly reduce the number of
    parameters by using a smaller encoder-decoder type network with a recurrent module
    and also share the weights between scales.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 'å»æ¨¡ç³Šéœ€è¦å¤§èŒƒå›´çš„æ„Ÿå—é‡ï¼ˆå…¨å±€çŸ¥è¯†ï¼‰ï¼Œä½†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æä¾›çš„æ˜¯å±€éƒ¨çŸ¥è¯†ï¼Œä¸èƒ½å¾ˆå¥½åœ°æ˜¾ç¤ºé•¿æœŸä¾èµ–æ€§ã€‚å› æ­¤ï¼ŒNah ç­‰äºº[[4](#bib.bib4)]ï¼ˆå‚è§
    [II-B2](#S2.SS2.SSS2 "II-B2 With Adversarial Loss â€£ II-B End to End â€£ II Methods
    â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")ï¼‰ä½¿ç”¨äº†ç¼©æ”¾ç»“æ„å’Œå¤§é‡çš„å·ç§¯å±‚åŠæ®‹å·®è¿æ¥æ¥æé«˜ç»“æ„çš„æ„Ÿå—é‡ï¼Œä½†è¿™ä¹Ÿå› å‚æ•°ä¼—å¤šè€Œä½¿å¾—æ”¶æ•›æ›´å›°éš¾ã€‚å› æ­¤
    (Tao et. al.[[9](#bib.bib9)]) ä½¿ç”¨äº†å°ºåº¦é€’å½’ç»“æ„ï¼Œåœ¨æ­¤ç»“æ„ä¸­ï¼Œä»–ä»¬ä»ä½¿ç”¨ç¼©æ”¾ç½‘ç»œï¼Œä½†é€šè¿‡ä½¿ç”¨ä¸€ä¸ªè¾ƒå°çš„ç¼–ç å™¨-è§£ç å™¨ç±»å‹ç½‘ç»œå’Œé€’å½’æ¨¡å—ï¼Œæ˜¾è‘—å‡å°‘äº†å‚æ•°æ•°é‡ï¼ŒåŒæ—¶åœ¨å°ºåº¦ä¹‹é—´å…±äº«æƒé‡ã€‚'
- en: '![Refer to caption](img/ce8db4df8ac55ef52c59d1547946bf41.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/ce8db4df8ac55ef52c59d1547946bf41.png)'
- en: 'Figure 6: Scale Recurrent Network Architecture used by (Tao et. al.[[9](#bib.bib9)])'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾6ï¼šç”± (Tao et. al.[[9](#bib.bib9)]) ä½¿ç”¨çš„å°ºåº¦é€’å½’ç½‘ç»œç»“æ„
- en: 'Scale recurrent network (Fig.[6](#S2.F6 "Figure 6 â€£ II-B1 Without Adversarial
    Loss â€£ II-B End to End â€£ II Methods â€£ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")) [[9](#bib.bib9)] consists of three parts,
    encoder ($Net_{E}$), recurrent ($Net_{R}$) and decoder ($Net_{D}$) modules. This
    can be represented as,'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 'ç¼©æ”¾é€’å½’ç½‘ç»œï¼ˆå›¾ã€‚[6](#S2.F6 "Figure 6 â€£ II-B1 Without Adversarial Loss â€£ II-B End to
    End â€£ II Methods â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal
    contribution")ï¼‰[[9](#bib.bib9)] ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šç¼–ç å™¨ï¼ˆ`$Net_{E}$`ï¼‰ã€é€’å½’ï¼ˆ`$Net_{R}$`ï¼‰å’Œè§£ç å™¨ï¼ˆ`$Net_{D}$`ï¼‰æ¨¡å—ã€‚å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ'
- en: '|  | <math   alttext="\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`|  | <math   alttext="\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\"`'
- en: L^{i}=Net_{D}(g^{i};\theta_{D})\\
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`L^{i}=Net_{D}(g^{i};\theta_{D})\\`'
- en: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><msup ><mi  >f</mi><mi >i</mi></msup><mo
    >=</mo><mrow ><mi >N</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi  >t</mi><mi >E</mi></msub><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi >B</mi><mi
    >i</mi></msup><mo >,</mo><msup ><mi >L</mi><mrow ><mrow ><mi >i</mi><mo >âˆ’</mo><mn
    >1</mn></mrow><mo stretchy="false"  >â†‘</mo></mrow></msup><mo >;</mo><msub ><mi
    >Î¸</mi><mi >E</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><msup ><mi  >h</mi><mi >i</mi></msup><mo
    >,</mo><msup ><mi >g</mi><mi >i</mi></msup></mrow><mo >=</mo><mrow ><mi >N</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi  >t</mi><mi >R</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >h</mi><mrow ><mrow ><mi >i</mi><mo >âˆ’</mo><mn
    >1</mn></mrow><mo stretchy="false"  >â†‘</mo></mrow></msup><mo >,</mo><msup ><mi
    >f</mi><mi >i</mi></msup><mo >;</mo><msub ><mi >Î¸</mi><mi >R</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mi >L</mi><mi >i</mi></msup><mo >=</mo><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><msub ><mi  >t</mi><mi >D</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><msup ><mi >g</mi><mi >i</mi></msup><mo >;</mo><msub
    ><mi >Î¸</mi><mi >D</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘“</ci><ci  >ğ‘–</ci></apply><apply
    ><ci >ğ‘</ci><ci  >ğ‘’</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘¡</ci><ci >ğ¸</ci></apply><vector ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğµ</ci><ci >ğ‘–</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ¿</ci><apply ><ci >â†‘</ci><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply><csymbol
    cd="latexml" >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğœƒ</ci><ci >ğ¸</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >â„</ci><ci >ğ‘–</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ğ‘”</ci><ci >ğ‘–</ci></apply><apply ><ci >ğ‘</ci><ci  >ğ‘’</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¡</ci><ci >ğ‘…</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >â„</ci><apply ><ci
    >â†‘</ci><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply><csymbol cd="latexml"
    >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ğ‘“</ci><ci >ğ‘–</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğœƒ</ci><ci >ğ‘…</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ğ¿</ci><ci >ğ‘–</ci></apply></apply></apply><apply ><apply ><ci >ğ‘</ci><ci  >ğ‘’</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¡</ci><ci >ğ·</ci></apply><list
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘”</ci><ci >ğ‘–</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğœƒ</ci><ci >ğ·</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\ L^{i}=Net_{D}(g^{i};\theta_{D})\\
    \end{split}</annotation></semantics></math> |  | (18) |
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><msup ><mi  >f</mi><mi >i</mi></msup><mo
    >=</mo><mrow ><mi >N</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><msub ><mi  >t</mi><mi >E</mi></msub><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi >B</mi><mi
    >i</mi></msup><mo >,</mo><msup ><mi >L</mi><mrow ><mrow ><mi >i</mi><mo >âˆ’</mo><mn
    >1</mn></mrow><mo stretchy="false"  >â†‘</mo></mrow></msup><mo >;</mo><msub ><mi
    >Î¸</mi><mi >E</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><msup ><mi  >h</mi><mi >i</mi></msup><mo
    >,</mo><msup ><mi >g</mi><mi >i</mi></msup></mrow><mo >=</mo><mrow ><mi >N</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><msub
    ><mi  >t</mi><mi >R</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >h</mi><mrow ><mrow ><mi >i</mi><mo >âˆ’</mo><mn
    >1</mn></mrow><mo stretchy="false"  >â†‘</mo></mrow></msup><mo >,</mo><msup ><mi
    >f</mi><mi >i</mi></msup><mo >;</mo><msub ><mi >Î¸</mi><mi >R</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mi >L</mi><mi >i</mi></msup><mo >=</mo><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >â€‹</mo><msub ><mi  >t</mi><mi >D</mi></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><msup ><mi >g</mi><mi >i</mi></msup><mo >;</mo><msub
    ><mi >Î¸</mi><mi >D</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘“</ci><ci  >ğ‘–</ci></apply><apply
    ><ci >ğ‘</ci><ci  >ğ‘’</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘¡</ci><ci >ğ¸</ci></apply><vector ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğµ</ci><ci >ğ‘–</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ğ¿</ci><apply ><ci >â†‘</ci><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply><csymbol
    cd="latexml" >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğœƒ</ci><ci >ğ¸</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >â„</ci><ci >ğ‘–</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ğ‘”</ci><ci >ğ‘–</ci></apply><apply ><ci >ğ‘</ci><ci  >ğ‘’</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¡</ci><ci >ğ‘…</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >â„</ci><apply ><ci
    >â†‘</ci><apply ><ci >ğ‘–</ci><cn type="integer" >1</cn></apply><csymbol cd="latexml"
    >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ğ‘“</ci><ci >ğ‘–</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğœƒ</ci><ci >ğ‘…</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ğ¿</ci><ci >ğ‘–</ci></apply></apply></apply><apply ><ci >ğ‘</ci><ci  >ğ‘’</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ğ‘¡</ci><ci >ğ·</ci></apply><list
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ğ‘”</ci><ci >ğ‘–</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğœƒ</ci><ci >ğ·</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\ L^{i}=Net_{D}(g^{i};\theta_{D})\\
    \end{split}</annotation></semantics></math> |  | (18) |
- en: $\theta_{E},\theta_{R},\theta_{D}$ are the weights of their respective modules.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta_{E},\theta_{R},\theta_{D}$ æ˜¯å„è‡ªæ¨¡å—çš„æƒé‡ã€‚
- en: The encoder module consists of convolutional layers with residual connections.
    For the first scale, only the blurred image is used as input, for all the subsequent
    layers both the blurred image $B^{i}$ and the restored image of the previous scale
    $L^{i-1\uparrow}$ are concatenated and both are sent as input. The encoder module
    is used to extract features $f^{i}$, it gradually decreases the length and breadth
    but increases the number of channels.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç å™¨æ¨¡å—ç”±å¸¦æœ‰æ®‹å·®è¿æ¥çš„å·ç§¯å±‚ç»„æˆã€‚å¯¹äºç¬¬ä¸€ä¸ªå°ºåº¦ï¼Œä»…ä½¿ç”¨æ¨¡ç³Šå›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¯¹äºæ‰€æœ‰åç»­å±‚ï¼Œæ¨¡ç³Šå›¾åƒ $B^{i}$ å’Œå‰ä¸€ä¸ªå°ºåº¦çš„æ¢å¤å›¾åƒ $L^{i-1\uparrow}$
    éƒ½ä¼šè¿æ¥åœ¨ä¸€èµ·ä½œä¸ºè¾“å…¥ã€‚ç¼–ç å™¨æ¨¡å—ç”¨äºæå–ç‰¹å¾ $f^{i}$ï¼Œå®ƒé€æ¸å‡å°‘é•¿åº¦å’Œå®½åº¦ï¼Œä½†å¢åŠ é€šé“æ•°é‡ã€‚
- en: The recurrent module can be a vanilla RNN, GRU or LSTM, in Tao et. al they used
    convolutional LSTM (ConvLSTM) which gave the best results. They also trained a
    network without any Recurrent module and it gave lower performance compared to
    the one which included a recurrent module. It takes as input the hidden features
    of the previous scaleâ€™s recurrent module $h^{i-1\uparrow}$ and the features generated
    by the current scales encoder $f^{i}$. The hidden features of the previous scale
    passes the intermediate results and blur patterns of the previous scale which
    benefits the current scale. Gradient clipping is used for this module only. It
    gives as output a modified set of features $g^{i}$ and the hidden features of
    the current scale $h^{i}$.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: é€’å½’æ¨¡å—å¯ä»¥æ˜¯æ™®é€šçš„ RNNã€GRU æˆ– LSTMï¼Œåœ¨ Tao ç­‰äººçš„ç ”ç©¶ä¸­ï¼Œä»–ä»¬ä½¿ç”¨äº†å·ç§¯ LSTMï¼ˆConvLSTMï¼‰ï¼Œæ•ˆæœæœ€ä½³ã€‚ä»–ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ªæ²¡æœ‰é€’å½’æ¨¡å—çš„ç½‘ç»œï¼Œæ€§èƒ½ä½äºåŒ…å«é€’å½’æ¨¡å—çš„ç½‘ç»œã€‚å®ƒä»¥å‰ä¸€å°ºåº¦é€’å½’æ¨¡å—çš„éšè—ç‰¹å¾
    $h^{i-1\uparrow}$ å’Œå½“å‰å°ºåº¦ç¼–ç å™¨ç”Ÿæˆçš„ç‰¹å¾ $f^{i}$ ä½œä¸ºè¾“å…¥ã€‚å‰ä¸€å°ºåº¦çš„éšè—ç‰¹å¾ä¼ é€’ä¸­é—´ç»“æœå’Œæ¨¡ç³Šæ¨¡å¼ï¼Œè¿™å¯¹å½“å‰å°ºåº¦æœ‰ç›Šã€‚ä»…å¯¹è¯¥æ¨¡å—ä½¿ç”¨æ¢¯åº¦è£å‰ªã€‚å®ƒè¾“å‡ºä¸€ä¸ªä¿®æ”¹åçš„ç‰¹å¾é›†
    $g^{i}$ å’Œå½“å‰å°ºåº¦çš„éšè—ç‰¹å¾ $h^{i}$ã€‚
- en: 'The decoder module consists of a few convolutional layers with residual connections
    (same dimensions are maintained using padding) followed by a deconvolutional layer
    which increases the spatial dimensions and decreases the number of channels until
    the we get latent image for the scale $L^{i}$. $\uparrow$ operator (Eqn. [18](#S2.E18
    "In II-B1 Without Adversarial Loss â€£ II-B End to End â€£ II Methods â€£ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) is used to adapt
    the dimensions of features or images to that of the next scale. $\uparrow$ can
    be deconvolution, sub-pixel convolution, image resizing, bilinear interpolation,
    etc.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 'è§£ç å™¨æ¨¡å—ç”±ä¸€äº›å·ç§¯å±‚å’Œæ®‹å·®è¿æ¥ç»„æˆï¼ˆé€šè¿‡å¡«å……ä¿æŒç›¸åŒçš„ç»´åº¦ï¼‰ï¼Œæ¥ç€æ˜¯ä¸€ä¸ªåå·ç§¯å±‚ï¼Œè¯¥å±‚å¢åŠ äº†ç©ºé—´ç»´åº¦å¹¶å‡å°‘äº†é€šé“æ•°é‡ï¼Œç›´åˆ°æˆ‘ä»¬è·å¾—å°ºåº¦$L^{i}$çš„æ½œåœ¨å›¾åƒã€‚$\uparrow$
    è¿ç®—ç¬¦ï¼ˆå…¬å¼ [18](#S2.E18 "In II-B1 Without Adversarial Loss â€£ II-B End to End â€£ II
    Methods â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")ï¼‰ç”¨äºè°ƒæ•´ç‰¹å¾æˆ–å›¾åƒçš„ç»´åº¦åˆ°ä¸‹ä¸€ä¸ªå°ºåº¦çš„ç»´åº¦ã€‚$\uparrow$
    å¯ä»¥æ˜¯åå·ç§¯ã€å­åƒç´ å·ç§¯ã€å›¾åƒç¼©æ”¾ã€åŒçº¿æ€§æ’å€¼ç­‰ã€‚'
- en: Combining all the three modules a single scale in the network can be represented
    as,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ‰€æœ‰ä¸‰ä¸ªæ¨¡å—ç»„åˆèµ·æ¥ï¼Œç½‘ç»œä¸­çš„å•ä¸€å°ºåº¦å¯ä»¥è¡¨ç¤ºä¸ºï¼Œ
- en: '|  | $L^{i},h^{i}=Net_{SR}(B^{i},L^{i-1\uparrow},h^{i-1\uparrow};\theta_{SR})$
    |  | (19) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{i},h^{i}=Net_{SR}(B^{i},L^{i-1\uparrow},h^{i-1\uparrow};\theta_{SR})$
    |  | (19) |'
- en: where $\theta_{SR}$ is the weight shared across all scales.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\theta_{SR}$ æ˜¯åœ¨æ‰€æœ‰å°ºåº¦ä¸­å…±äº«çš„æƒé‡ã€‚
- en: Scaled Recurrent Network uses Euclidean Loss given below,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼©æ”¾é€’å½’ç½‘ç»œä½¿ç”¨ä»¥ä¸‹çš„æ¬§å‡ é‡Œå¾—æŸå¤±ï¼Œ
- en: '|  | $L=\sum_{i=1}^{n}\dfrac{\kappa_{i}}{N_{i}}\lVert L^{i}-I^{i}\rVert^{2}_{2}$
    |  | (20) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\sum_{i=1}^{n}\dfrac{\kappa_{i}}{N_{i}}\lVert L^{i}-I^{i}\rVert^{2}_{2}$
    |  | (20) |'
- en: where $L^{i}$ is the latent restored image, $I^{i}$ is the ground truth sharp
    image. $\{\kappa_{i}\}$ are weights for each scale, and $N_{i}$ is the number
    of elements in $L^{i}$ to be normalized.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $L^{i}$ æ˜¯æ½œåœ¨çš„æ¢å¤å›¾åƒï¼Œ$I^{i}$ æ˜¯çœŸå®çš„é”åŒ–å›¾åƒã€‚$\{\kappa_{i}\}$ æ˜¯æ¯ä¸ªå°ºåº¦çš„æƒé‡ï¼Œ$N_{i}$ æ˜¯éœ€è¦å½’ä¸€åŒ–çš„
    $L^{i}$ ä¸­çš„å…ƒç´ æ•°é‡ã€‚
- en: 'Noorozi et. al.[[5](#bib.bib5)] also uses a three pyramid stages chained together,
    each consisting of several convolutional and deconvolutional layers $(N_{1},N_{2},\text{
    and }N_{3})$ (Eqn. [21](#S2.E21 "In II-B1 Without Adversarial Loss â€£ II-B End
    to End â€£ II Methods â€£ Blind Deblurring using Deep Learning: A Survey *denotes
    equal contribution")) which recreates a multiscale pyramid schemes which were
    previously used in the classical methods. The key idea of this pyramid structure
    is that the downsampled version of the blurred image has less amount of blur and
    it is easier for removal. Hence the goal of each respective network (or stage)
    is to mitigate the blur effect at that corresponding scale. It also helps break
    down the complex problem of deblurring into smaller units.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'Noorozi ç­‰äºº[[5](#bib.bib5)] ä¹Ÿä½¿ç”¨äº†ä¸‰ä¸ªé‡‘å­—å¡”é˜¶æ®µä¸²è”åœ¨ä¸€èµ·ï¼Œæ¯ä¸ªé˜¶æ®µåŒ…å«è‹¥å¹²å·ç§¯å±‚å’Œåå·ç§¯å±‚`(N_{1},N_{2},\text{
    å’Œ }N_{3})`ï¼ˆå…¬å¼ [21](#S2.E21 "In II-B1 Without Adversarial Loss â€£ II-B End to End
    â€£ II Methods â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")ï¼‰ï¼Œè¿™äº›é‡‘å­—å¡”é˜¶æ®µé‡å»ºäº†å…ˆå‰åœ¨ç»å…¸æ–¹æ³•ä¸­ä½¿ç”¨çš„å¤šå°ºåº¦é‡‘å­—å¡”æ–¹æ¡ˆã€‚è¿™ä¸ªé‡‘å­—å¡”ç»“æ„çš„å…³é”®æ€æƒ³æ˜¯ï¼Œæ¨¡ç³Šå›¾åƒçš„é™é‡‡æ ·ç‰ˆæœ¬æ¨¡ç³Šç¨‹åº¦è¾ƒè½»ï¼Œæ›´å®¹æ˜“å»é™¤ã€‚å› æ­¤ï¼Œæ¯ä¸ªç›¸åº”çš„ç½‘ç»œï¼ˆæˆ–é˜¶æ®µï¼‰çš„ç›®æ ‡æ˜¯åœ¨å¯¹åº”çš„å°ºåº¦ä¸Šå‡è½»æ¨¡ç³Šæ•ˆæœã€‚è¿™ä¹Ÿæœ‰åŠ©äºå°†å¤æ‚çš„å»æ¨¡ç³Šé—®é¢˜åˆ†è§£æˆæ›´å°çš„å•å…ƒã€‚'
- en: '![Refer to caption](img/de378c90eab5299a5bd7854ce25f6d5f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒæ ‡é¢˜](img/de378c90eab5299a5bd7854ce25f6d5f.png)'
- en: 'Figure 7: Architecture used by Noorozi et. al. [[5](#bib.bib5)]. Here the three
    CNNs starting from the left denotes $N_{1},N_{2},N_{3}$ respectively.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 7ï¼šNoorozi ç­‰äººä½¿ç”¨çš„æ¶æ„[[5](#bib.bib5)]ã€‚è¿™é‡Œï¼Œå·¦ä¾§çš„ä¸‰ä¸ªå·ç§¯ç¥ç»ç½‘ç»œåˆ†åˆ«è¡¨ç¤º`N_{1}`ã€`N_{2}`å’Œ`N_{3}`ã€‚
- en: 'Firstly the blurred image is given as input to the first network $N_{1}$ (pure
    convolution) without any downsampling and itâ€™s output is added with the downsampled
    version of the same blurred image by a factor of four. After this the first loss
    $L_{1}$ is calculated using [21](#S2.E21 "In II-B1 Without Adversarial Loss â€£
    II-B End to End â€£ II Methods â€£ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution") by calculating the difference (or MSE) between the
    downsampled sharp image and the added sum of the network $N_{1}$â€™s output and
    downsampled blurred image. This same process is repeated for network $N_{2}$ and
    $N_{3}$ for calculating losses $L_{2}$ and $L_{3}$ but the downsampling factor
    are two and one (no downsampling) respectively. The these three computed losses
    are added resulting in final loss function for this model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'é¦–å…ˆï¼Œå°†æ¨¡ç³Šå›¾åƒä½œä¸ºè¾“å…¥ä¼ é€’ç»™ç¬¬ä¸€ä¸ªç½‘ç»œ`N_{1}`ï¼ˆçº¯å·ç§¯ï¼‰ï¼Œä¸è¿›è¡Œä»»ä½•é™é‡‡æ ·ï¼Œå¹¶å°†å…¶è¾“å‡ºä¸ç›¸åŒæ¨¡ç³Šå›¾åƒçš„é™é‡‡æ ·ç‰ˆæœ¬ï¼ˆé™é‡‡æ ·å› å­ä¸ºå››ï¼‰ç›¸åŠ ã€‚ä¹‹åï¼Œä½¿ç”¨[21](#S2.E21
    "In II-B1 Without Adversarial Loss â€£ II-B End to End â€£ II Methods â€£ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")è®¡ç®—ç¬¬ä¸€ä¸ªæŸå¤±`L_{1}`ï¼Œé€šè¿‡è®¡ç®—é™é‡‡æ ·é”åŒ–å›¾åƒä¸ç½‘ç»œ`N_{1}`è¾“å‡ºä¸é™é‡‡æ ·æ¨¡ç³Šå›¾åƒçš„å’Œä¹‹é—´çš„å·®å¼‚ï¼ˆæˆ–å‡æ–¹è¯¯å·®ï¼‰ã€‚è¿™ä¸ªè¿‡ç¨‹å¯¹ç½‘ç»œ`N_{2}`å’Œ`N_{3}`ä¹Ÿè¿›è¡Œäº†é‡å¤ï¼Œä»¥è®¡ç®—æŸå¤±`L_{2}`å’Œ`L_{3}`ï¼Œä½†é™é‡‡æ ·å› å­åˆ†åˆ«ä¸ºäºŒå’Œä¸€ï¼ˆå³ä¸é™é‡‡æ ·ï¼‰ã€‚è¿™ä¸‰ç§è®¡ç®—å¾—åˆ°çš„æŸå¤±è¢«ç›¸åŠ ï¼Œå¾—åˆ°è¯¥æ¨¡å‹çš„æœ€ç»ˆæŸå¤±å‡½æ•°ã€‚'
- en: '|  | <math   alttext="\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\'
- en: L_{3}=\sum_{(B,I)}&#124;N_{3}(N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B))+B-I&#124;^{2}\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><msub  ><mi >L</mi><mn >1</mn></msub><mo rspace="0.111em"
    >=</mo><mrow ><munder ><mo movablelimits="false" rspace="0em" >âˆ‘</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi >B</mi><mo >,</mo><mi >I</mi><mo stretchy="false"  >)</mo></mrow></munder><msup
    ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><mrow ><mrow ><msub ><mi  >N</mi><mn
    >1</mn></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >B</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><msub ><mi
    >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo lspace="0em"
    rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi >B</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    >âˆ’</mo><mrow ><msub ><mi >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi >I</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >&#124;</mo></mrow><mn
    >2</mn></msup></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow
    ><msub ><mi  >L</mi><mn >2</mn></msub><mo rspace="0.111em"  >=</mo><mrow ><munder
    ><mo movablelimits="false" rspace="0em" >âˆ‘</mo><mrow ><mo stretchy="false" >(</mo><mi
    >B</mi><mo >,</mo><mi >I</mi><mo stretchy="false" >)</mo></mrow></munder><msup
    ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><mrow ><mrow ><msub ><mi  >N</mi><mn
    >2</mn></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><mrow ><msub ><mi >N</mi><mn >1</mn></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><mi  >B</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo><mrow ><msub ><mi >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo><mrow ><msub ><mi >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >2</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mi >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo >âˆ’</mo><mrow ><msub ><mi >d</mi><mrow
    ><mn >1</mn><mo >/</mo><mn >2</mn></mrow></msub><mo lspace="0em" rspace="0em"  >â€‹</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >I</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mn >2</mn></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi  >L</mi><mn >3</mn></msub><mo rspace="0.111em"  >=</mo><mrow
    ><munder ><mo movablelimits="false" rspace="0em"  >âˆ‘</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >B</mi><mo >,</mo><mi >I</mi><mo stretchy="false"  >)</mo></mrow></munder><msup
    ><mrow ><mo stretchy="false"  >&#124;</mo><mrow ><mrow ><mrow ><msub ><mi >N</mi><mn
    >3</mn></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><mrow ><msub ><mi >N</mi><mn >2</mn></msub><mo lspace="0em" rspace="0em" >â€‹</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi  >N</mi><mn >1</mn></msub><mo
    lspace="0em" rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><msub ><mi >d</mi><mrow
    ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><msub ><mi >d</mi><mrow
    ><mn >1</mn><mo >/</mo><mn >2</mn></mrow></msub><mo lspace="0em" rspace="0em"
    >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mi >B</mi></mrow><mo >âˆ’</mo><mi
    >I</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow><mn >2</mn></msup></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ¿</ci><cn type="integer" >1</cn></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><interval closure="open" ><ci  >ğµ</ci><ci >ğ¼</ci></interval></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><apply  ><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘</ci><cn type="integer"
    >1</cn></apply><ci >ğµ</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘‘</ci><apply ><cn type="integer" >1</cn><cn type="integer" >4</cn></apply></apply><ci
    >ğµ</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘‘</ci><apply ><cn type="integer" >1</cn><cn type="integer" >4</cn></apply></apply><ci
    >ğ¼</ci></apply></apply></apply><cn type="integer" >2</cn></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ğ¿</ci><cn type="integer" >2</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><interval closure="open"  ><ci
    >ğµ</ci><ci >ğ¼</ci></interval></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><apply  ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ‘</ci><cn type="integer" >2</cn></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘</ci><cn type="integer" >1</cn></apply><ci >ğµ</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘‘</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >4</cn></apply></apply><ci >ğµ</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘‘</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply><ci >ğµ</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘‘</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply><ci >ğ¼</ci></apply></apply></apply><cn
    type="integer" >2</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >ğ¿</ci><cn type="integer" >3</cn></apply></apply></apply></apply><apply ><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><interval closure="open"  ><ci
    >ğµ</ci><ci >ğ¼</ci></interval></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><apply ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><cn type="integer" >3</cn></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >ğ‘</ci><cn type="integer"  >2</cn></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ğ‘</ci><cn type="integer" >1</cn></apply><ci >ğµ</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘‘</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >4</cn></apply></apply><ci >ğµ</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >ğ‘‘</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply><ci >ğµ</ci></apply></apply></apply><ci
    >ğµ</ci></apply><ci >ğ¼</ci></apply></apply><cn type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\
    L_{3}=\sum_{(B,I)}&#124;N_{3}(N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B))+B-I&#124;^{2}\end{split}</annotation></semantics></math>
    |  | (21) |
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The problem in this architecture is when there are extreme blurs,the network
    leaves the images untouched but it does not suffer from artifacts.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ¶æ„ä¸­çš„é—®é¢˜æ˜¯å½“å‡ºç°æç«¯æ¨¡ç³Šæ—¶ï¼Œç½‘ç»œä¼šä½¿å›¾åƒä¿æŒä¸å˜ï¼Œä½†ä¸ä¼šå‡ºç°ä¼ªå½±ã€‚
- en: Spatially variant blurs in dynamic scenes require a large receptive field, while
    CNN have a local knowledge (small receptive field) and spatially invariant weights,
    to remove this problem they have to use larger networks with more convolutional
    layer, leading to more parameters which is difficult to train. Hence, the challenge
    is to have a small architecture with a large receptive field, to this end Zhang
    et. al[[10](#bib.bib10)] proposes the use of Recurrent Neural Network as a deconvolutional
    operator which increases the receptive field (long range dependencies).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ¨æ€åœºæ™¯ä¸­çš„ç©ºé—´å˜åŒ–æ¨¡ç³Šéœ€è¦è¾ƒå¤§çš„æ„Ÿå—é‡ï¼Œè€ŒCNNå…·æœ‰å±€éƒ¨çŸ¥è¯†ï¼ˆå°æ„Ÿå—é‡ï¼‰å’Œç©ºé—´ä¸å˜çš„æƒé‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒCNNå¿…é¡»ä½¿ç”¨æ›´å¤§çš„ç½‘ç»œå’Œæ›´å¤šçš„å·ç§¯å±‚ï¼Œè¿™å¯¼è‡´æ›´å¤šçš„å‚æ•°ï¼Œè®­ç»ƒèµ·æ¥å›°éš¾ã€‚å› æ­¤ï¼ŒæŒ‘æˆ˜åœ¨äºæ‹¥æœ‰ä¸€ä¸ªå°å‹æ¶æ„è€Œåˆå…·æœ‰å¤§æ„Ÿå—é‡ã€‚ä¸ºæ­¤ï¼Œå¼ ç­‰äºº[[10](#bib.bib10)]æå‡ºäº†ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œä½œä¸ºå»å·ç§¯ç®—å­ï¼Œä»¥å¢åŠ æ„Ÿå—é‡ï¼ˆé•¿è·ç¦»ä¾èµ–ï¼‰ã€‚
- en: 'The network proposed by Zhang et. al.[[10](#bib.bib10)] uses three CNN and
    one RNN. The CNN are used for feature extraction, image reconstruction, and pixel-wise
    weight generation (for the RNN). While the RNN is used as a deconvolutional operator
    with a large receptive field (Fig. [8](#S2.F8 "Figure 8 â€£ II-B1 Without Adversarial
    Loss â€£ II-B End to End â€£ II Methods â€£ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")). Weight generation for the RNN is done by
    passing an image through a encoder-decoder architecture CNN. For both the decoder
    part of the weight generation module and the image restoration module they use
    bilinear interpolation (no deconvolution) to avoid checkerboard artifact [[11](#bib.bib11)]
    The RNN generates receptive fields in one direction (single dimension), hence
    they use a convolutional layer after every RNN to fuse the receptive fields together
    to get a two dimensional structure. Skip connections are added to avoid vanishing
    gradient problem and for faster training.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¼ ç­‰äººæå‡ºçš„ç½‘ç»œ[[10](#bib.bib10)]ä½¿ç”¨äº†ä¸‰ä¸ªCNNå’Œä¸€ä¸ªRNNã€‚CNNç”¨äºç‰¹å¾æå–ã€å›¾åƒé‡å»ºå’Œé€åƒç´ æƒé‡ç”Ÿæˆï¼ˆç”¨äºRNNï¼‰ã€‚è€ŒRNNä½œä¸ºå»å·ç§¯ç®—å­ï¼Œå…·æœ‰è¾ƒå¤§çš„æ„Ÿå—é‡ï¼ˆè§å›¾
    [8](#S2.F8 "Figure 8 â€£ II-B1 Without Adversarial Loss â€£ II-B End to End â€£ II Methods
    â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")ï¼‰ã€‚RNNçš„æƒé‡ç”Ÿæˆé€šè¿‡å›¾åƒé€šè¿‡ç¼–ç -è§£ç æ¶æ„çš„CNNå®Œæˆã€‚å¯¹äºæƒé‡ç”Ÿæˆæ¨¡å—çš„è§£ç å™¨éƒ¨åˆ†å’Œå›¾åƒæ¢å¤æ¨¡å—ï¼Œä»–ä»¬ä½¿ç”¨åŒçº¿æ€§æ’å€¼ï¼ˆæ²¡æœ‰å»å·ç§¯ï¼‰ä»¥é¿å…æ£‹ç›˜çŠ¶ä¼ªå½±
    [[11](#bib.bib11)]ã€‚RNNåœ¨ä¸€ä¸ªæ–¹å‘ä¸Šç”Ÿæˆæ„Ÿå—é‡ï¼ˆå•ç»´ï¼‰ï¼Œå› æ­¤åœ¨æ¯ä¸ªRNNä¹‹åä½¿ç”¨å·ç§¯å±‚å°†æ„Ÿå—é‡èåˆåœ¨ä¸€èµ·ï¼Œä»¥è·å¾—äºŒç»´ç»“æ„ã€‚æ·»åŠ è·³è·ƒè¿æ¥ä»¥é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜å¹¶åŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚'
- en: '![Refer to caption](img/d52d0a2d620c339837c08921ac8205b1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/d52d0a2d620c339837c08921ac8205b1.png)'
- en: 'Figure 8: Architecture used by Zhang et. al. [[10](#bib.bib10)]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾8ï¼šå¼ ç­‰äººä½¿ç”¨çš„æ¶æ„ [[10](#bib.bib10)]
- en: If $f$ is the feature extraction module, $rnn$ is the RNN module, $w$ is the
    weight generation module, and $r$ is the restoration module, then the network
    proposed by Zhang et. al.[[10](#bib.bib10)] can be summarized as,
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœ$f$æ˜¯ç‰¹å¾æå–æ¨¡å—ï¼Œ$rnn$æ˜¯RNNæ¨¡å—ï¼Œ$w$æ˜¯æƒé‡ç”Ÿæˆæ¨¡å—ï¼Œ$r$æ˜¯æ¢å¤æ¨¡å—ï¼Œåˆ™å¼ ç­‰äººæå‡ºçš„ç½‘ç»œ[[10](#bib.bib10)]å¯ä»¥æ€»ç»“ä¸ºï¼Œ
- en: '|  | <math   alttext="\begin{split}F=f(B)\\ \theta=w(B)\\'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}F=f(B)\\ \theta=w(B)\\'
- en: F^{\prime}=rnn(F;\theta)\\
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: F^{\prime}=rnn(F;\theta)\\
- en: L=r(F^{\prime})\end{split}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mrow ><mi >F</mi><mo  >=</mo><mrow
    ><mi >f</mi><mo lspace="0em" rspace="0em" >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mi
    >B</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mrow ><mi >Î¸</mi><mo  >=</mo><mrow ><mi >w</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow  ><mo stretchy="false"  >(</mo><mi >B</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow
    ><msup ><mi  >F</mi><mo >â€²</mo></msup><mo >=</mo><mrow ><mi >r</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mi  >n</mi><mo lspace="0em" rspace="0em"  >â€‹</mo><mi >n</mi><mo
    lspace="0em" rspace="0em"  >â€‹</mo><mrow ><mo stretchy="false" >(</mo><mi >F</mi><mo  >;</mo><mi
    >Î¸</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mrow ><mi >L</mi><mo  >=</mo><mrow ><mi >r</mi><mo lspace="0em"
    rspace="0em" >â€‹</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi  >F</mi><mo
    >â€²</mo></msup><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >ğ¹</ci><apply ><ci  >ğ‘“</ci><ci
    >ğµ</ci><ci >ğœƒ</ci></apply></apply><apply ><apply  ><ci >ğ‘¤</ci><ci >ğµ</ci><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci >ğ¹</ci><ci >â€²</ci></apply></apply></apply><apply
    ><apply  ><ci >ğ‘Ÿ</ci><ci >ğ‘›</ci><ci  >ğ‘›</ci><list ><ci >ğ¹</ci><ci >ğœƒ</ci></list><ci
    >ğ¿</ci></apply></apply><apply ><apply  ><ci >ğ‘Ÿ</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ğ¹</ci><ci >â€²</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}F=f(B)\\ \theta=w(B)\\ F^{\prime}=rnn(F;\theta)\\
    L=r(F^{\prime})\end{split}</annotation></semantics></math> |  | (22) |
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $L=r(F^{\prime})$
- en: where $B$ is the blurry image, $F$ is the extracted features, $\theta$ is the
    pixel-wise generated weights, $F^{\prime}$ are the modified features after passing
    through the RNN, and $L$ is the latent (predicted) deblurred image.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $B$ æ˜¯æ¨¡ç³Šå›¾åƒï¼Œ$F$ æ˜¯æå–çš„ç‰¹å¾ï¼Œ$\theta$ æ˜¯é€åƒç´ ç”Ÿæˆçš„æƒé‡ï¼Œ$F^{\prime}$ æ˜¯é€šè¿‡ RNN å¤„ç†åçš„ä¿®æ”¹ç‰¹å¾ï¼Œ$L$
    æ˜¯æ½œåœ¨çš„ï¼ˆé¢„æµ‹çš„ï¼‰å»æ¨¡ç³Šå›¾åƒã€‚
- en: II-B2 With Adversarial Loss
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 ä½¿ç”¨å¯¹æŠ—æŸå¤±
- en: '![Refer to caption](img/90b4f0df280268f575fe334d2f970fb0.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è§æ ‡é¢˜](img/90b4f0df280268f575fe334d2f970fb0.png)'
- en: 'Figure 9: The basic structure of a GAN, where $G$ denotes the Generator and
    $D$ denotes the discriminator.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9ï¼šGAN çš„åŸºæœ¬ç»“æ„ï¼Œå…¶ä¸­ $G$ è¡¨ç¤ºç”Ÿæˆå™¨ï¼Œ$D$ è¡¨ç¤ºåˆ¤åˆ«å™¨ã€‚
- en: Blind Deblurring can also be solved end-to-end by generative models like Generative
    Adversarial Networks [[14](#bib.bib14)][[15](#bib.bib15)][[16](#bib.bib16)]. The
    approach Generative Adversarial Networks take is to have two different agents
    play a game against each other. One of the agents is a generator network which
    tries to generate data and the other is a discriminator network which examines
    data and checks whether it came from the real distribution (ground truth sharp
    image) or model distribution (restored blurred image). The goal of the generator
    is to fool the discriminator into believing that its output is from the real distribution.
    These generator and discriminator modules are neural networks whose parameters
    can be tuned by backpropagation and as both players get better at their job over
    time eventually the generator is forced to create data as realistic as possible.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ç›²å»æ¨¡ç³Šä¹Ÿå¯ä»¥é€šè¿‡ç”Ÿæˆå¯¹æŠ—ç½‘ç»œç­‰ç”Ÿæˆæ¨¡å‹è¿›è¡Œç«¯åˆ°ç«¯çš„è§£å†³[[14](#bib.bib14)][[15](#bib.bib15)][[16](#bib.bib16)]ã€‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œé‡‡ç”¨çš„æ–¹æ³•æ˜¯è®©ä¸¤ä¸ªä¸åŒçš„ä»£ç†ç›¸äº’å¯¹æŠ—ã€‚å…¶ä¸­ä¸€ä¸ªä»£ç†æ˜¯ç”Ÿæˆç½‘ç»œï¼Œå®ƒå°è¯•ç”Ÿæˆæ•°æ®ï¼Œå¦ä¸€ä¸ªæ˜¯é‰´åˆ«ç½‘ç»œï¼Œå®ƒæ£€æŸ¥æ•°æ®å¹¶åˆ¤æ–­å…¶æ˜¯å¦æ¥è‡ªçœŸå®åˆ†å¸ƒï¼ˆåŸå§‹æ¸…æ™°å›¾åƒï¼‰è¿˜æ˜¯æ¨¡å‹åˆ†å¸ƒï¼ˆæ¢å¤åçš„æ¨¡ç³Šå›¾åƒï¼‰ã€‚ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯æ¬ºéª—é‰´åˆ«å™¨ï¼Œä½¿å…¶ç›¸ä¿¡ç”Ÿæˆçš„æ•°æ®æ¥è‡ªçœŸå®åˆ†å¸ƒã€‚è¿™äº›ç”Ÿæˆå™¨å’Œé‰´åˆ«å™¨æ¨¡å—æ˜¯å¯ä»¥é€šè¿‡åå‘ä¼ æ’­è°ƒæ•´å‚æ•°çš„ç¥ç»ç½‘ç»œï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸¤è€…åœ¨å„è‡ªä»»åŠ¡ä¸Šé€æ¸å˜å¾—æ›´åŠ é«˜æ•ˆï¼Œæœ€ç»ˆç”Ÿæˆå™¨è¢«è¿«åˆ›å»ºå°½å¯èƒ½çœŸå®çš„æ•°æ®ã€‚
- en: '![Refer to caption](img/87deaf31d77889391a6730a67a788709.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![å‚è€ƒè¯´æ˜](img/87deaf31d77889391a6730a67a788709.png)'
- en: 'Figure 10: Multiscale architecture used by Nah. et. al [[4](#bib.bib4)]'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'å›¾ 10: Nah ç­‰äººä½¿ç”¨çš„å¤šå°ºåº¦æ¶æ„ [[4](#bib.bib4)]'
- en: 'Nah et. al.[[4](#bib.bib4)] also uses a Multiscale Convolutional Neural Network
    i.e coarse(low resolution) to fine(high resolution) structure. The blurred and
    sharp images are scaled down to form a Gaussian pyramid structure, this is done
    because convolution can only capture local information, hence lower resolution
    images are used to capture the long range dependencies, whereas the high resolution
    images are used to capture the fine grained details. Each of these scaled blurred
    images passes through a layer of multiple convolutional and residual blocks (residual
    blocks enable training in large networks without over fitting) to generate the
    corresponding latent image for that scale, then for each scale MSE(Mean Squared
    Error) with the sharp image is calculated and back propagation is done. The MSE
    for all the scales are averaged together to give the content loss as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Nah ç­‰äºº[[4](#bib.bib4)] ä¹Ÿä½¿ç”¨äº†å¤šå°ºåº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå³ä»ç²—åˆ°ç»†ï¼ˆä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡ï¼‰çš„ç»“æ„ã€‚æ¨¡ç³Šå›¾åƒå’Œæ¸…æ™°å›¾åƒè¢«ç¼©å°ä»¥å½¢æˆé«˜æ–¯é‡‘å­—å¡”ç»“æ„ï¼Œè¿™æ ·åšæ˜¯å› ä¸ºå·ç§¯åªèƒ½æ•æ‰å±€éƒ¨ä¿¡æ¯ï¼Œå› æ­¤ä½¿ç”¨ä½åˆ†è¾¨ç‡å›¾åƒæ¥æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ï¼Œè€Œé«˜åˆ†è¾¨ç‡å›¾åƒåˆ™ç”¨äºæ•æ‰ç»†ç²’åº¦çš„ç»†èŠ‚ã€‚è¿™äº›ç¼©å°çš„æ¨¡ç³Šå›¾åƒç»è¿‡å¤šä¸ªå·ç§¯å’Œæ®‹å·®å—çš„å±‚ï¼ˆæ®‹å·®å—ä½¿å¾—å¤§ç½‘ç»œè®­ç»ƒæ—¶ä¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼‰ï¼Œä»¥ç”Ÿæˆè¯¥å°ºåº¦å¯¹åº”çš„æ½œåœ¨å›¾åƒï¼Œç„¶åå¯¹äºæ¯ä¸ªå°ºåº¦è®¡ç®—ä¸æ¸…æ™°å›¾åƒçš„å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ï¼Œå¹¶è¿›è¡Œåå‘ä¼ æ’­ã€‚æ‰€æœ‰å°ºåº¦çš„MSEå¹³å‡åç»™å‡ºå†…å®¹æŸå¤±ï¼Œå¦‚ä¸‹ï¼š
- en: '|  | $L_{content}=\dfrac{1}{2K}\sum_{k=1}^{K}\dfrac{1}{c_{k}h_{k}w_{k}}\lVert
    L_{k}-I_{k}\rVert^{2}$ |  | (23) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{content}=\dfrac{1}{2K}\sum_{k=1}^{K}\dfrac{1}{c_{k}h_{k}w_{k}}\lVert
    L_{k}-I_{k}\rVert^{2}$ |  | (23) |'
- en: Here $K$ is the total number of scales, $c_{k},h_{k},w_{k}$ are the channels,
    height and width of the $k^{th}$ scale, and $L_{k}$ and $I_{k}$ are the latent
    and sharp images of the $k^{th}$ scale respectively.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„ $K$ æ˜¯å°ºåº¦çš„æ€»æ•°ï¼Œ$c_{k},h_{k},w_{k}$ åˆ†åˆ«æ˜¯ç¬¬ $k^{th}$ å°ºåº¦çš„é€šé“ã€é«˜åº¦å’Œå®½åº¦ï¼Œ$L_{k}$ å’Œ $I_{k}$
    åˆ†åˆ«æ˜¯ç¬¬ $k^{th}$ å°ºåº¦çš„æ½œåœ¨å›¾åƒå’Œæ¸…æ™°å›¾åƒã€‚
- en: 'The output of current scale is given as input to the next scale. The next scale
    is of a higher resolution, hence the latent image of the current scale is passed
    through a upconvolutional(transpose convolution) layer and is concatenated with
    the blurred image input of the next layer. Except for the last layer whose output
    latent image is the same size as the original image, hence does not need any upconvolution.
    This generated deblurred image of the last scale is given as input to a discriminator
    or some sharp image is given as input, and the discriminator tells weather the
    image given was originally sharp or was deblurred by the Multi-Scaled Network.
    Discriminator loss function (Adversarial Loss) [[14](#bib.bib14)] is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å‰å°ºåº¦çš„è¾“å‡ºä½œä¸ºè¾“å…¥ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå°ºåº¦ã€‚ä¸‹ä¸€ä¸ªå°ºåº¦å…·æœ‰æ›´é«˜çš„åˆ†è¾¨ç‡ï¼Œå› æ­¤å½“å‰å°ºåº¦çš„æ½œåœ¨å›¾åƒé€šè¿‡ä¸Šå·ç§¯ï¼ˆè½¬ç½®å·ç§¯ï¼‰å±‚è¿›è¡Œä¼ é€’ï¼Œå¹¶ä¸ä¸‹ä¸€ä¸ªå±‚çš„æ¨¡ç³Šå›¾åƒè¾“å…¥è¿›è¡Œæ‹¼æ¥ã€‚é™¤äº†æœ€åä¸€å±‚ï¼Œå…¶è¾“å‡ºçš„æ½œåœ¨å›¾åƒä¸åŸå§‹å›¾åƒå¤§å°ç›¸åŒï¼Œå› æ­¤ä¸éœ€è¦ä»»ä½•ä¸Šå·ç§¯ã€‚æœ€åå°ºåº¦ç”Ÿæˆçš„å»æ¨¡ç³Šå›¾åƒä½œä¸ºè¾“å…¥ä¼ é€’ç»™é‰´åˆ«å™¨ï¼Œæˆ–è€…è¾“å…¥ä¸€äº›æ¸…æ™°çš„å›¾åƒï¼Œé‰´åˆ«å™¨ä¼šåˆ¤æ–­å›¾åƒæ˜¯å¦åŸæœ¬æ¸…æ™°è¿˜æ˜¯è¢«å¤šå°ºåº¦ç½‘ç»œå»æ¨¡ç³Šçš„ã€‚é‰´åˆ«å™¨æŸå¤±å‡½æ•°ï¼ˆå¯¹æŠ—æŸå¤±ï¼‰[[14](#bib.bib14)]å¦‚ä¸‹ï¼š
- en: '|  | $L_{adv}=\mathbb{E}_{S\sim p_{sharp}}[\log(D(S))]+\mathbb{E}_{B\sim p_{blurred}}[1-\log(G(B))]$
    |  | (24) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{adv}=\mathbb{E}_{S\sim p_{sharp}}[\log(D(S))]+\mathbb{E}_{B\sim p_{blurred}}[1-\log(G(B))]$
    |  | (24) |'
- en: Here $D$ is the Discriminator i.e a CNN classifier and $G$ is the Generator
    i.e our Multi-Scaled CNN. Generator $G$ for each scale can be defined as,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œ $D$ æ˜¯é‰´åˆ«å™¨ï¼Œå³ä¸€ä¸ª CNN åˆ†ç±»å™¨ï¼Œ$G$ æ˜¯ç”Ÿæˆå™¨ï¼Œå³æˆ‘ä»¬çš„å¤šå°ºåº¦ CNNã€‚æ¯ä¸ªå°ºåº¦çš„ç”Ÿæˆå™¨ $G$ å¯ä»¥å®šä¹‰ä¸ºï¼Œ
- en: '|  | $L^{i}=G(B^{i},L^{i-1\uparrow};\theta_{i})$ |  | (25) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{i}=G(B^{i},L^{i-1\uparrow};\theta_{i})$ |  | (25) |'
- en: where $L^{i},B^{i}$ is the generated and blurred image for $i^{th}$ scale respectively.
    $L^{i-1\uparrow}$ is the generated image of the previous scale where $\uparrow$
    denotes the upconvolution function used to upscale the dimension of $L^{i-1}$
    to be same as $L^{i}$. $\theta_{i}$ are the weights of $i^{th}$ scale.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $L^{i},B^{i}$ åˆ†åˆ«æ˜¯ç¬¬ $i^{th}$ å°ºåº¦çš„ç”Ÿæˆå›¾åƒå’Œæ¨¡ç³Šå›¾åƒã€‚$L^{i-1\uparrow}$ æ˜¯å‰ä¸€ä¸ªå°ºåº¦çš„ç”Ÿæˆå›¾åƒï¼Œå…¶ä¸­
    $\uparrow$ è¡¨ç¤ºç”¨äºå°† $L^{i-1}$ çš„ç»´åº¦æ”¾å¤§åˆ°ä¸ $L^{i}$ ç›¸åŒçš„ä¸Šå·ç§¯å‡½æ•°ã€‚$\theta_{i}$ æ˜¯ç¬¬ $i^{th}$ å°ºåº¦çš„æƒé‡ã€‚
- en: 'Combing both loss functions i.e Content and Adversarial Loss we get:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“åˆä¸¤ä¸ªæŸå¤±å‡½æ•°ï¼Œå³å†…å®¹æŸå¤±å’Œå¯¹æŠ—æŸå¤±ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š
- en: '|  | $L_{total}=L_{content}+\lambda L_{adv}$ |  | (26) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{total}=L_{content}+\lambda L_{adv}$ |  | (26) |'
- en: Where $\lambda$ is a weight constant.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\lambda$ æ˜¯ä¸€ä¸ªæƒé‡å¸¸æ•°ã€‚
- en: Ramamkrishnan et. al[[6](#bib.bib6)] also uses an adversarial way of training,
    but the generator uses a structure similar to DenseNet[[17](#bib.bib17)] with
    a global skip connection. Similar to Nah et. al.[[4](#bib.bib4)], here dimensions
    are maintained throughout the convolutional layers, so that no deconvolution module
    needs to be used, preventing checkerboard effect [[11](#bib.bib11)]. Using a densely
    connected CNN in generator reduces the vanishing gradient problem, strengthens
    feature propagation and reuse and reduces the number of parameters, all of which
    in turn allows us to use a smaller network with smoother training and faster inference
    time.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Ramamkrishnan ç­‰[[6](#bib.bib6)] ä¹Ÿä½¿ç”¨äº†ä¸€ç§å¯¹æŠ—è®­ç»ƒçš„æ–¹æ³•ï¼Œä½†ç”Ÿæˆå™¨ä½¿ç”¨äº†ç±»ä¼¼äº DenseNet[[17](#bib.bib17)]
    çš„ç»“æ„ï¼Œå¹¶å…·æœ‰å…¨å±€è·³è·ƒè¿æ¥ã€‚ä¸ Nah ç­‰[[4](#bib.bib4)] ç±»ä¼¼ï¼Œè¿™é‡Œåœ¨æ•´ä¸ªå·ç§¯å±‚ä¸­ä¿æŒäº†ç»´åº¦ï¼Œå› æ­¤ä¸éœ€è¦ä½¿ç”¨å»å·ç§¯æ¨¡å—ï¼Œé˜²æ­¢äº†æ£‹ç›˜æ•ˆåº” [[11](#bib.bib11)]ã€‚ä½¿ç”¨å¯†é›†è¿æ¥çš„
    CNN ç”Ÿæˆå™¨å‡å°‘äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ŒåŠ å¼ºäº†ç‰¹å¾ä¼ æ’­å’Œé‡ç”¨ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡ï¼Œä»è€Œå…è®¸æˆ‘ä»¬ä½¿ç”¨æ›´å°çš„ç½‘ç»œï¼Œå®ç°æ›´å¹³æ»‘çš„è®­ç»ƒå’Œæ›´å¿«çš„æ¨ç†æ—¶é—´ã€‚
- en: The generator is divided into three parts, head, dense field and tail. Head
    creates sufficient activations for the dense field using convolutional layer.
    The dense field consists of several dense blocks, each dense block has a ReLU
    to add non linearity, a 1$\times$1 convolution to limit the number of activaitons
    (or channels), a convolutional layer (3 $\times$ 3), and batch normalizations.
    In DenseNet[[17](#bib.bib17)] $l^{th}$ layer of convolution is connected to the
    features of all the previous layers as opposed to the immediately previous layer
    (like in classic CNN). This dense connectivity is achieved in the generator by
    concatenating the output of $l^{th}$ layer with the output of $(l+1)^{th}$ layer,
    which in turn concatenates its output with the output of next layer i.e $(l+2)^{th}$
    layer and so on. All the convolutional layers in the dense filed use spatial and
    dilated convolution alternatively, this increases the receptive field of the network
    while still restraining the number of parameters to be learned. The rate of dilation
    increases till the middle layer and then decreases till the tail is reached. Tail
    adds non-linearity and uses $1\times 1$ convolution to reduce the number of activations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨åˆ†ä¸ºä¸‰éƒ¨åˆ†ï¼šå¤´éƒ¨ã€å¯†é›†åŒºåŸŸå’Œå°¾éƒ¨ã€‚å¤´éƒ¨é€šè¿‡å·ç§¯å±‚ä¸ºå¯†é›†åŒºåŸŸåˆ›é€ è¶³å¤Ÿçš„æ¿€æ´»ã€‚å¯†é›†åŒºåŸŸç”±å‡ ä¸ªå¯†é›†å—ç»„æˆï¼Œæ¯ä¸ªå¯†é›†å—æœ‰ä¸€ä¸ª ReLU æ·»åŠ éçº¿æ€§ï¼Œä¸€ä¸ª
    $1\times1$ å·ç§¯é™åˆ¶æ¿€æ´»æ•°ï¼ˆæˆ–é€šé“æ•°ï¼‰ï¼Œä¸€ä¸ªå·ç§¯å±‚ï¼ˆ$3 \times 3$ï¼‰ï¼Œä»¥åŠæ‰¹å½’ä¸€åŒ–ã€‚åœ¨ DenseNet[[17](#bib.bib17)]
    ä¸­ï¼Œç¬¬ $l^{th}$ å±‚çš„å·ç§¯è¿æ¥åˆ°æ‰€æœ‰å…ˆå‰å±‚çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯ç«‹å³å‰ä¸€å±‚ï¼ˆå¦‚åœ¨ç»å…¸ CNN ä¸­ï¼‰ã€‚è¿™ç§å¯†é›†è¿æ¥åœ¨ç”Ÿæˆå™¨ä¸­é€šè¿‡å°†ç¬¬ $l^{th}$ å±‚çš„è¾“å‡ºä¸ç¬¬
    $(l+1)^{th}$ å±‚çš„è¾“å‡ºè¿æ¥å®ç°ï¼Œåè€…åˆå°†å…¶è¾“å‡ºä¸ä¸‹ä¸€å±‚å³ç¬¬ $(l+2)^{th}$ å±‚çš„è¾“å‡ºè¿æ¥ï¼Œä¾æ­¤ç±»æ¨ã€‚å¯†é›†åŒºåŸŸä¸­çš„æ‰€æœ‰å·ç§¯å±‚äº¤æ›¿ä½¿ç”¨ç©ºé—´å·ç§¯å’Œè†¨èƒ€å·ç§¯ï¼Œè¿™å¢åŠ äº†ç½‘ç»œçš„æ„Ÿå—é‡ï¼ŒåŒæ—¶ä»ç„¶é™åˆ¶äº†éœ€è¦å­¦ä¹ çš„å‚æ•°æ•°é‡ã€‚è†¨èƒ€ç‡åœ¨ä¸­é—´å±‚å¢åŠ ï¼Œç„¶ååœ¨è¾¾åˆ°å°¾éƒ¨æ—¶å‡å°‘ã€‚å°¾éƒ¨æ·»åŠ éçº¿æ€§å¹¶ä½¿ç”¨
    $1\times 1$ å·ç§¯å‡å°‘æ¿€æ´»æ•°ã€‚
- en: They added the output of head to the output of tail to form a global skip connection,
    this allows gradients to flow back to the first layers of convolution which helps
    in learning (gradient updates) of the lower layers. Shorter connections between
    layers close to the output and layers close to the input, results in better accuracy
    and efficiency.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬å°†å¤´éƒ¨çš„è¾“å‡ºä¸å°¾éƒ¨çš„è¾“å‡ºç›¸åŠ ï¼Œå½¢æˆå…¨å±€è·³è·ƒè¿æ¥ï¼Œè¿™ä½¿å¾—æ¢¯åº¦èƒ½å¤Ÿæµå›å·ç§¯çš„ç¬¬ä¸€å±‚ï¼Œæœ‰åŠ©äºå­¦ä¹ ï¼ˆæ¢¯åº¦æ›´æ–°ï¼‰è¾ƒä½å±‚çš„å†…å®¹ã€‚å±‚ä¹‹é—´çš„çŸ­è¿æ¥ï¼Œç‰¹åˆ«æ˜¯é è¿‘è¾“å‡ºå’Œè¾“å…¥çš„å±‚ä¹‹é—´ï¼Œæœ‰åŠ©äºæé«˜å‡†ç¡®æ€§å’Œæ•ˆç‡ã€‚
- en: Similar to Nah et. al.[[4](#bib.bib4)] here the loss functions is also divided
    into two parts but with slight differences i.e instead of finding the MSE between
    predicted and sharp image (content loss), they find the MSE between the features
    (taken from end layers of a pretrained VGG16 network) of the predicted and sharp
    image. This is known as Perceptual Loss [[18](#bib.bib18)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ Nah ç­‰äºº[[4](#bib.bib4)] ç±»ä¼¼ï¼Œè¿™é‡Œçš„æŸå¤±å‡½æ•°ä¹Ÿåˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œä½†ç•¥æœ‰ä¸åŒï¼Œå³ä¸æ˜¯å¯»æ‰¾é¢„æµ‹å›¾åƒå’Œæ¸…æ™°å›¾åƒä¹‹é—´çš„ MSEï¼ˆå†…å®¹æŸå¤±ï¼‰ï¼Œè€Œæ˜¯å¯»æ‰¾é¢„æµ‹å›¾åƒå’Œæ¸…æ™°å›¾åƒçš„ç‰¹å¾ï¼ˆå–è‡ªé¢„è®­ç»ƒ
    VGG16 ç½‘ç»œçš„æœ«å±‚ï¼‰ä¹‹é—´çš„ MSEã€‚è¿™è¢«ç§°ä¸ºæ„ŸçŸ¥æŸå¤± [[18](#bib.bib18)]ã€‚
- en: '|  | $L_{precep}=\dfrac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}(\phi(I)_{x,y}-\phi(L)_{x,y})$
    |  | (27) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{precep}=\dfrac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}(\phi(I)_{x,y}-\phi(L)_{x,y})$
    |  | (27) |'
- en: where $\phi$ denotes the function used to generate the features. $W,H$ are dimensions
    of the features. $L$ is the predicted latent image i.e $L=G(B)$ for generator
    $G$ and blurry image $B$.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $\phi$ è¡¨ç¤ºç”¨äºç”Ÿæˆç‰¹å¾çš„å‡½æ•°ã€‚$W,H$ æ˜¯ç‰¹å¾çš„ç»´åº¦ã€‚$L$ æ˜¯é¢„æµ‹çš„æ½œåœ¨å›¾åƒï¼Œå³ $L=G(B)$ï¼Œå…¶ä¸­ $G$ æ˜¯ç”Ÿæˆå™¨ï¼Œ$B$ æ˜¯æ¨¡ç³Šå›¾åƒã€‚
- en: Instead of simple adversarial loss they use a conditional adversarial loss [[15](#bib.bib15)]
    i.e with every sharp or predicted image, they also send the corresponding blurred
    image. Then calculate the probability of weather the image is deblurred or sharp
    given the blurred image.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ä½¿ç”¨çš„ä¸æ˜¯ç®€å•çš„å¯¹æŠ—æŸå¤±ï¼Œè€Œæ˜¯æ¡ä»¶å¯¹æŠ—æŸå¤± [[15](#bib.bib15)]ï¼Œå³å¯¹äºæ¯å¼ æ¸…æ™°æˆ–é¢„æµ‹çš„å›¾åƒï¼Œä»–ä»¬ä¹Ÿä¼šå‘é€ç›¸åº”çš„æ¨¡ç³Šå›¾åƒã€‚ç„¶åè®¡ç®—å›¾åƒæ˜¯å¦å·²å»æ¨¡ç³Šæˆ–æ¸…æ™°çš„æ¦‚ç‡ï¼Œç»™å®šæ¨¡ç³Šå›¾åƒã€‚
- en: '|  | $L_{adv_{con}}=-\mathbb{E}_{b\in B}[\log D(G(B)&#124;B)]$ |  | (28) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{adv_{con}}=-\mathbb{E}_{b\in B}[\log D(G(B)\mid B)]$ |  | (28) |'
- en: where $D$ is the discriminator.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $D$ æ˜¯åˆ¤åˆ«å™¨ã€‚
- en: While combining both the losses Zhang et. al[[10](#bib.bib10)] also adds a $L1$
    loss which was not present in Nah et. al[[4](#bib.bib4)],
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ ç­‰äºº[[10](#bib.bib10)] åœ¨ç»“åˆè¿™ä¸¤ç§æŸå¤±æ—¶ï¼Œè¿˜æ·»åŠ äº†ä¸€ç§åœ¨ Nah ç­‰äºº[[4](#bib.bib4)] ä¸­æœªå‡ºç°çš„ $L1$ æŸå¤±ï¼Œ
- en: '|  | $L_{total}=L_{precep}+K1\times L_{adv_{con}}+K2\times L_{L1}$ |  | (29)
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{total}=L_{precep}+K1\times L_{adv_{con}}+K2\times L_{L1}$ |  | (29)
    |'
- en: where $L_{L1}$ is the $L1$ loss. $K1,K2$ are the weight constant.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $L_{L1}$ æ˜¯ $L1$ æŸå¤±ã€‚$K1,K2$ æ˜¯æƒé‡å¸¸æ•°ã€‚
- en: Kupyn et. al.[[3](#bib.bib3)] uses a method also based on conditional GANs [[15](#bib.bib15)]
    similar to Ramakrishnan et. al.[[6](#bib.bib6)], the number of layer are significantly
    less compared to Nah et. al.[[4](#bib.bib4)], decreasing the number of trainable
    parameters and hence resulting in decrease training time and faster inference
    time. Instead of using the conventional loss function of GANs, they used the wasserstein
    (or called Earth-Mover) distance with gradient penalty [[16](#bib.bib16)] which
    has proved to show stability from vanila GANs[[14](#bib.bib14)] which suffer from
    mode collapse and vanishing gradients. The generator architecture is different
    from Ramakrishnan et. al.[[6](#bib.bib6)] as they use some convlutional layers
    followed by a series of residual blocks and finally some deconvolution layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kupyn ç­‰äºº[[3](#bib.bib3)] é‡‡ç”¨çš„æ–¹æ³•ä¹ŸåŸºäºæ¡ä»¶ GANs [[15](#bib.bib15)]ï¼Œç±»ä¼¼äº Ramakrishnan
    ç­‰äºº[[6](#bib.bib6)]ï¼Œä½†ä¸ Nah ç­‰äºº[[4](#bib.bib4)] ç›¸æ¯”ï¼Œå±‚æ•°æ˜¾è‘—å‡å°‘ï¼Œä»è€Œå‡å°‘äº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ï¼Œä»è€Œå‡å°‘äº†è®­ç»ƒæ—¶é—´å’ŒåŠ å¿«äº†æ¨ç†é€Ÿåº¦ã€‚ä»–ä»¬æ²¡æœ‰ä½¿ç”¨ä¼ ç»Ÿçš„
    GANs æŸå¤±å‡½æ•°ï¼Œè€Œæ˜¯ä½¿ç”¨äº† wassersteinï¼ˆæˆ–ç§° Earth-Moverï¼‰è·ç¦»ä¸æ¢¯åº¦æƒ©ç½š [[16](#bib.bib16)]ï¼Œè¿™å·²è¢«è¯æ˜åœ¨è§£å†³
    vanilla GANs[[14](#bib.bib14)] é‡åˆ°çš„æ¨¡å¼å´©æºƒå’Œæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ä¸Šè¡¨ç°å‡ºç¨³å®šæ€§ã€‚ç”Ÿæˆå™¨æ¶æ„ä¸åŒäº Ramakrishnan ç­‰äºº[[6](#bib.bib6)]ï¼Œå› ä¸ºä»–ä»¬ä½¿ç”¨äº†ä¸€äº›å·ç§¯å±‚ï¼Œåè·Ÿä¸€ç³»åˆ—æ®‹å·®å—ï¼Œæœ€åæ˜¯ä¸€äº›åå·ç§¯å±‚ã€‚
- en: 'The generator takes the blurred image as input and produces itâ€™s sharp estimate.
    The discriminator then tries to model the differences in sharp images (real distribution)
    and restored image (model distribution) by the generator by computing the Wasserstein
    distance (Earth mover distance)[[16](#bib.bib16)]. The perceptual loss is same
    as in (Eqn. [27](#S2.E27 "In II-B2 With Adversarial Loss â€£ II-B End to End â€£ II
    Methods â€£ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ç”Ÿæˆå™¨ä»¥æ¨¡ç³Šå›¾åƒä½œä¸ºè¾“å…¥ï¼Œç”Ÿæˆå…¶é”åŒ–ä¼°è®¡ã€‚ç„¶åï¼Œåˆ¤åˆ«å™¨é€šè¿‡è®¡ç®— Wasserstein è·ç¦»ï¼ˆEarth mover distanceï¼‰[[16](#bib.bib16)]
    å°è¯•å»ºæ¨¡é”åŒ–å›¾åƒï¼ˆçœŸå®åˆ†å¸ƒï¼‰å’Œç”Ÿæˆå™¨æ¢å¤å›¾åƒï¼ˆæ¨¡å‹åˆ†å¸ƒï¼‰ä¹‹é—´çš„å·®å¼‚ã€‚æ„ŸçŸ¥æŸå¤±ä¸ (Eqn. [27](#S2.E27 "åœ¨ II-B2 å¯¹æŠ—æ€§æŸå¤± â€£ II-B
    ç«¯åˆ°ç«¯ â€£ II æ–¹æ³• â€£ ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„ç›²å»æ¨¡ç³Šï¼šç»¼è¿° *è¡¨ç¤ºåŒç­‰è´¡çŒ®")) ä¸­çš„ç›¸åŒã€‚
- en: 'The goal is to minimize this entire loss function (which is same as (Eqn. [29](#S2.E29
    "In II-B2 With Adversarial Loss â€£ II-B End to End â€£ II Methods â€£ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) but without the $L1$
    loss) such that the generator is producing well enough restored image from the
    blurred image and the discriminator network to unable to distinguish the real
    sharp image (real data distribution) and restored image (model distribution) resulting
    in output of Â½ probability by the discriminator most of the time. This is when
    the model is said to have converged.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯æœ€å°åŒ–æ•´ä¸ªæŸå¤±å‡½æ•°ï¼ˆè¿™ä¸ (Eqn. [29](#S2.E29 "åœ¨ II-B2 å¯¹æŠ—æ€§æŸå¤± â€£ II-B ç«¯åˆ°ç«¯ â€£ II æ–¹æ³• â€£ ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„ç›²å»æ¨¡ç³Šï¼šç»¼è¿°
    *è¡¨ç¤ºåŒç­‰è´¡çŒ®")) ä¸­çš„ç›¸åŒï¼Œä½†æ²¡æœ‰ $L1$ æŸå¤±ï¼‰ï¼Œä»¥ä½¿ç”Ÿæˆå™¨èƒ½å¤Ÿä»æ¨¡ç³Šå›¾åƒä¸­ç”Ÿæˆè¶³å¤Ÿå¥½çš„æ¢å¤å›¾åƒï¼Œå¹¶ä¸”åˆ¤åˆ«ç½‘ç»œæ— æ³•åŒºåˆ†çœŸå®çš„é”åŒ–å›¾åƒï¼ˆçœŸå®æ•°æ®åˆ†å¸ƒï¼‰å’Œæ¢å¤å›¾åƒï¼ˆæ¨¡å‹åˆ†å¸ƒï¼‰ï¼Œä½¿åˆ¤åˆ«å™¨å¤§å¤šæ•°æ—¶é—´çš„è¾“å‡ºä¸º
    Â½ æ¦‚ç‡ã€‚è¿™æ—¶ï¼Œæ¨¡å‹è¢«è®¤ä¸ºå·²ç»æ”¶æ•›ã€‚
- en: III Performance Evaluation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III æ€§èƒ½è¯„ä¼°
- en: III-A Metric
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A è¡¡é‡æ ‡å‡†
- en: 'The metrics used to measure similarity between the restored image and the blurred
    image are Peak Signal to Noise Ratio (PSNR) and Structural Similarity(SSIM). We
    also compare time taken by different architectures to deblur a blurry image after
    they are trained (inference time) (Table [III](#S3.T3 "TABLE III â€£ III-C GoPro
    Dataset â€£ III Performance Evaluation â€£ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨äºè¡¡é‡æ¢å¤å›¾åƒå’Œæ¨¡ç³Šå›¾åƒä¹‹é—´ç›¸ä¼¼æ€§çš„åº¦é‡æ˜¯å³°å€¼ä¿¡å™ªæ¯”ï¼ˆPSNRï¼‰å’Œç»“æ„ç›¸ä¼¼æ€§ï¼ˆSSIMï¼‰ã€‚æˆ‘ä»¬è¿˜æ¯”è¾ƒäº†ä¸åŒæ¶æ„åœ¨è®­ç»ƒåå»æ¨¡ç³Šæ¨¡ç³Šå›¾åƒæ‰€éœ€çš„æ—¶é—´ï¼ˆæ¨ç†æ—¶é—´ï¼‰ï¼ˆè¡¨
    [III](#S3.T3 "TABLE III â€£ III-C GoPro æ•°æ®é›† â€£ III æ€§èƒ½è¯„ä¼° â€£ ä½¿ç”¨æ·±åº¦å­¦ä¹ çš„ç›²å»æ¨¡ç³Šï¼šç»¼è¿° *è¡¨ç¤ºåŒç­‰è´¡çŒ®"))ã€‚
- en: PSNR can be thought of as the reciprocal of MSE (Mean Squared Error). MSE can
    be calculated as,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PSNR å¯ä»¥è¢«è§†ä¸º MSEï¼ˆå‡æ–¹è¯¯å·®ï¼‰çš„å€’æ•°ã€‚MSE å¯ä»¥è®¡ç®—ä¸ºï¼Œ
- en: '|  | $MSE=\dfrac{\sum_{P,Q}(I-L)^{2}}{P\times Q}$ |  | (30) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSE=\dfrac{\sum_{P,Q}(I-L)^{2}}{P\times Q}$ |  | (30) |'
- en: where $P,Q$ are the dimensions of the image. $I$ and $L$ are the sharp and deblurred
    image respectively. Given MSE, PSNR can be calculated using,
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $P,Q$ æ˜¯å›¾åƒçš„ç»´åº¦ã€‚ $I$ å’Œ $L$ åˆ†åˆ«æ˜¯é”åŒ–å’Œå»æ¨¡ç³Šå›¾åƒã€‚ç»™å®š MSEï¼ŒPSNR å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼Œ
- en: '|  | $PSNR=\dfrac{m^{2}}{MSE}$ |  | (31) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSNR=\dfrac{m^{2}}{MSE}$ |  | (31) |'
- en: where $m$ is the maximum possible intensity value, since we are using 8-bit
    integer to represent a pixel in channel, m = 255.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $m$ æ˜¯æœ€å¤§å¯èƒ½çš„å¼ºåº¦å€¼ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨ 8 ä½æ•´æ•°æ¥è¡¨ç¤ºé€šé“ä¸­çš„åƒç´ ï¼Œm = 255ã€‚
- en: SSIM helps us to find the structural similarity between two image, it can be
    calculated using,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM å¸®åŠ©æˆ‘ä»¬æ‰¾å‡ºä¸¤å¹…å›¾åƒä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å…¬å¼è®¡ç®—ï¼Œ
- en: '|  | $SSIM(x,y)=\dfrac{(2\mu_{x}\mu_{y}+c_{1})(2\sigma_{xy}+c_{2})}{(\mu_{x}^{2}+\mu_{y}^{2}+c_{1})(\sigma_{x}^{2}+\sigma_{y}^{2}+c_{2})}$
    |  | (32) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $SSIM(x,y)=\dfrac{(2\mu_{x}\mu_{y}+c_{1})(2\sigma_{xy}+c_{2})}{(\mu_{x}^{2}+\mu_{y}^{2}+c_{1})(\sigma_{x}^{2}+\sigma_{y}^{2}+c_{2})}$
    |  | (32) |'
- en: where $x,y$ are windows of equal dimension for $B,I$ respectively. $\mu_{x},\mu_{y}$
    denotes mean of $x,y$ respectively. $\sigma_{x},\sigma_{y}$ denotes variance for
    $x,y$ respectively, whereas $\sigma_{xy}$ is the covariance between $x$ and $y$.
    $c_{1}$ and $c_{2}$ are constants used to stabilize the division.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ $x,y$ åˆ†åˆ«æ˜¯ $B,I$ çš„ç›¸ç­‰å°ºå¯¸çª—å£ã€‚ $\mu_{x},\mu_{y}$ åˆ†åˆ«è¡¨ç¤º $x,y$ çš„å‡å€¼ã€‚ $\sigma_{x},\sigma_{y}$
    åˆ†åˆ«è¡¨ç¤º $x,y$ çš„æ–¹å·®ï¼Œè€Œ $\sigma_{xy}$ æ˜¯ $x$ å’Œ $y$ ä¹‹é—´çš„åæ–¹å·®ã€‚ $c_{1}$ å’Œ $c_{2}$ æ˜¯ç”¨äºç¨³å®šé™¤æ³•çš„å¸¸æ•°ã€‚
- en: III-B KÃ¶hler Dataset
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B KÃ¶hler æ•°æ®é›†
- en: 'KÃ¶hler Dataset [[19](#bib.bib19)] consists of 4 images which are blurred using
    12 different blur kernels giving us a total of 48 blurred images. To generate
    the blurred kernels, 6D camera motion is recored and then replayed using a robot,
    for each image. While replying, the 6D motion is approximated into a 3D motion
    by considering translation in one plane, and rotation on the plane perpendicular
    to it. This helps us to approximate actual camera shakes that occur in real life.
    For more details refer to [[19](#bib.bib19)]. The PSNR and SSIM for different
    deblurring architecture in KÃ¶hler dataset is shown in (Table [I](#S3.T1 "TABLE
    I â€£ III-B KÃ¶hler Dataset â€£ III Performance Evaluation â€£ Blind Deblurring using
    Deep Learning: A Survey *denotes equal contribution")).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'KÃ¶hler æ•°æ®é›† [[19](#bib.bib19)] åŒ…å« 4 å¼ å›¾åƒï¼Œè¿™äº›å›¾åƒä½¿ç”¨ 12 ç§ä¸åŒçš„æ¨¡ç³Šæ ¸æ¨¡ç³Šå¤„ç†ï¼Œå…±ç”Ÿæˆäº† 48 å¼ æ¨¡ç³Šå›¾åƒã€‚ä¸ºäº†ç”Ÿæˆæ¨¡ç³Šæ ¸ï¼Œè®°å½•äº†
    6D ç›¸æœºè¿åŠ¨ï¼Œç„¶åé€šè¿‡æœºå™¨äººé‡æ”¾æ¯å¼ å›¾åƒã€‚åœ¨é‡æ”¾è¿‡ç¨‹ä¸­ï¼Œå°† 6D è¿åŠ¨è¿‘ä¼¼ä¸º 3D è¿åŠ¨ï¼Œé€šè¿‡è€ƒè™‘å¹³é¢ä¸Šçš„å¹³ç§»å’Œå‚ç›´äºè¯¥å¹³é¢çš„æ—‹è½¬ã€‚è¿™æœ‰åŠ©äºæˆ‘ä»¬è¿‘ä¼¼å®é™…ç”Ÿæ´»ä¸­çš„ç›¸æœºæŠ–åŠ¨ã€‚æ›´å¤šç»†èŠ‚è¯·å‚é˜…
    [[19](#bib.bib19)]ã€‚KÃ¶hler æ•°æ®é›†ä¸­ä¸åŒå»æ¨¡ç³Šæ¶æ„çš„ PSNR å’Œ SSIM å¦‚ (è¡¨ [I](#S3.T1 "TABLE I â€£
    III-B KÃ¶hler Dataset â€£ III Performance Evaluation â€£ Blind Deblurring using Deep
    Learning: A Survey *denotes equal contribution")) æ‰€ç¤ºã€‚'
- en: 'TABLE I: KÃ¶hler Dataset'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ I: KÃ¶hler æ•°æ®é›†'
- en: '| Methods | PSNR | SSIM |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | PSNR | SSIM |'
- en: '| --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kupyn et. al. | 26.10 | 0.816 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Kupyn ç­‰äºº | 26.10 | 0.816 |'
- en: '| Tao et. al. | 26.80 | 0.838 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Tao ç­‰äºº | 26.80 | 0.838 |'
- en: '| Nah et. al. | 26.48 | 0.812 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Nah ç­‰äºº | 26.48 | 0.812 |'
- en: '| Gong et. al. | 26.59 | 0.742 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Gong ç­‰äºº | 26.59 | 0.742 |'
- en: '| Ramakrishnan et. al. | 27.08 | 0.751 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Ramakrishnan ç­‰äºº | 27.08 | 0.751 |'
- en: '| Sun et. al. | 25.22 | 0.774 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Sun ç­‰äºº | 25.22 | 0.774 |'
- en: III-C GoPro Dataset
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C GoPro æ•°æ®é›†
- en: 'Here, a high resolution (1280 $\times$ 720), high frame rate (240 frames per
    second) camera (GoPro Hero5 Black) is used to capture video outdoors. To generate
    blurred image an average of a few frames (odd number picked randomly from 7 to
    23) is taken, while the central frame is considered as the corresponding sharp
    image. To reduce the magnitude of relative motion across frames they are down
    sampled and to avoid artifacts caused by averaging we only consider frames were
    the optical flow is at most 1. The PSNR and SSIM for different deblurring architecture
    in GoPro dataset is shown in (Table [II](#S3.T2 "TABLE II â€£ III-C GoPro Dataset
    â€£ III Performance Evaluation â€£ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¿™é‡Œä½¿ç”¨é«˜åˆ†è¾¨ç‡ (1280 $\times$ 720)ã€é«˜å¸§ç‡ (240 å¸§æ¯ç§’) çš„ç›¸æœº (GoPro Hero5 Black) åœ¨æˆ·å¤–æ‹æ‘„è§†é¢‘ã€‚ä¸ºäº†ç”Ÿæˆæ¨¡ç³Šå›¾åƒï¼Œå–å‡ ä¸ªå¸§çš„å¹³å‡å€¼
    (ä» 7 åˆ° 23 ä¸­éšæœºæŒ‘é€‰çš„å¥‡æ•°)ï¼Œè€Œä¸­å¤®å¸§è¢«è§†ä¸ºå¯¹åº”çš„æ¸…æ™°å›¾åƒã€‚ä¸ºäº†å‡å°‘å¸§ä¹‹é—´çš„ç›¸å¯¹è¿åŠ¨å¹…åº¦ï¼Œå®ƒä»¬è¢«ä¸‹é‡‡æ ·ï¼Œå¹¶ä¸”ä¸ºäº†é¿å…å¹³å‡é€ æˆçš„ä¼ªå½±ï¼Œæˆ‘ä»¬åªè€ƒè™‘å…‰æµæœ€å¤§ä¸º
    1 çš„å¸§ã€‚GoPro æ•°æ®é›†ä¸­ä¸åŒå»æ¨¡ç³Šæ¶æ„çš„ PSNR å’Œ SSIM å¦‚ (è¡¨ [II](#S3.T2 "TABLE II â€£ III-C GoPro Dataset
    â€£ III Performance Evaluation â€£ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) æ‰€ç¤ºã€‚'
- en: 'TABLE II: GoPro Dataset'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ II: GoPro æ•°æ®é›†'
- en: '| Methods | PSNR | SSIM |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | PSNR | SSIM |'
- en: '| --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kupyn et. al. | 28.7 | 0.958 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Kupyn ç­‰äºº | 28.7 | 0.958 |'
- en: '| Zhang et. al. | 29.2 | 0.931 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Zhang ç­‰äºº | 29.2 | 0.931 |'
- en: '| Tao et. al. | 30.1 | 0.932 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Tao ç­‰äºº | 30.1 | 0.932 |'
- en: '| Nah et. al. | 29.2 | 0.916 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Nah ç­‰äºº | 29.2 | 0.916 |'
- en: '| Gong et. al. | 26.1 | 0.863 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Gong ç­‰äºº | 26.1 | 0.863 |'
- en: '| Noorozi et. al. | 28.1 | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Noorozi ç­‰äºº | 28.1 | - |'
- en: '| Sun et. al. | 24.6 | 0.842 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Sun ç­‰äºº | 24.6 | 0.842 |'
- en: 'TABLE III: Inference Time'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 'è¡¨ III: æ¨æ–­æ—¶é—´'
- en: '| Methods | Time(sec) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| æ–¹æ³• | æ—¶é—´ (ç§’) |'
- en: '| --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Kupyn et. al. | 0.8 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Kupyn ç­‰äºº | 0.8 |'
- en: '| Tao et. al. | 1.6 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Tao ç­‰äºº | 1.6 |'
- en: '| Nah et. al. | 4.3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Nah ç­‰äºº | 4.3 |'
- en: '| Zhang et. al. | 1.4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Zhang ç­‰äºº | 1.4 |'
- en: IV Conclusion
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV ç»“è®º
- en: We observe that end-to-end methods ([[3](#bib.bib3)], [[4](#bib.bib4)], [[6](#bib.bib6)],
    [[5](#bib.bib5)], [[9](#bib.bib9)], [[10](#bib.bib10)]) have higher PSNR and SSIM
    compared to methods that estimate the blur kernel ([[2](#bib.bib2)], [[8](#bib.bib8)]),
    this is because an error in kernel estimation can lead to various artifacts in
    image, degrading the restoration.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œç«¯åˆ°ç«¯çš„æ–¹æ³• ([[3](#bib.bib3)], [[4](#bib.bib4)], [[6](#bib.bib6)], [[5](#bib.bib5)],
    [[9](#bib.bib9)], [[10](#bib.bib10)]) æ¯”ä¼°è®¡æ¨¡ç³Šæ ¸çš„æ–¹æ³• ([[2](#bib.bib2)], [[8](#bib.bib8)])
    çš„ PSNR å’Œ SSIM æ›´é«˜ï¼Œå› ä¸ºæ ¸ä¼°è®¡ä¸­çš„é”™è¯¯å¯èƒ½å¯¼è‡´å›¾åƒä¸­çš„å„ç§ä¼ªå½±ï¼Œé™ä½äº†ä¿®å¤æ•ˆæœã€‚
- en: We also observed that most of the methods tried to increase their receptive
    field, which allowed long range spatial dependencies, essential for non uniform
    blurs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œå¤§å¤šæ•°æ–¹æ³•å°è¯•æ‰©å¤§æ¥æ”¶åœºï¼Œè¿™å…è®¸é•¿è·ç¦»ç©ºé—´ä¾èµ–ï¼Œå¯¹éå‡åŒ€æ¨¡ç³Šè‡³å…³é‡è¦ã€‚
- en: 'Another motivation was to reduce the size of network and the number of parameters,
    resulting in faster inference, as it can be clearly seen in (Table [III](#S3.T3
    "TABLE III â€£ III-C GoPro Dataset â€£ III Performance Evaluation â€£ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) that Nah et. al.[[4](#bib.bib4)]
    which has a large network size is slower compared to other networks.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'å¦ä¸€ä¸ªåŠ¨æœºæ˜¯å‡å°‘ç½‘ç»œçš„å¤§å°å’Œå‚æ•°æ•°é‡ï¼Œä»è€ŒåŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œæ­£å¦‚åœ¨ (è¡¨ [III](#S3.T3 "TABLE III â€£ III-C GoPro Dataset
    â€£ III Performance Evaluation â€£ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) ä¸­å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼ŒNah ç­‰äºº[[4](#bib.bib4)] çš„ç½‘ç»œè§„æ¨¡è¾ƒå¤§ï¼Œç›¸è¾ƒäºå…¶ä»–ç½‘ç»œé€Ÿåº¦è¾ƒæ…¢ã€‚'
- en: Decreasing the size of network, while maintaining a large receptive field is
    one of the biggest challenge in learning based deblurring methods.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘ç½‘ç»œå¤§å°ï¼ŒåŒæ—¶ä¿æŒå¤§æ¥æ”¶åŸŸæ˜¯åŸºäºå­¦ä¹ çš„å»æ¨¡ç³Šæ–¹æ³•ä¸­çš„ä¸€å¤§æŒ‘æˆ˜ã€‚
- en: References
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‚è€ƒæ–‡çŒ®
- en: '[1] A.Â Chakrabarti, â€œA neural approach to blind motion deblurring,â€ *CoRR*,
    vol. abs/1603.04771, 2016\. [Online]. Available: [http://arxiv.org/abs/1603.04771](http://arxiv.org/abs/1603.04771)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Chakrabarti, â€œä¸€ç§ç›²è¿åŠ¨å»æ¨¡ç³Šçš„ç¥ç»æ–¹æ³•,â€ *CoRR*, å· abs/1603.04771, 2016\. [åœ¨çº¿].
    å¯ç”¨: [http://arxiv.org/abs/1603.04771](http://arxiv.org/abs/1603.04771)'
- en: '[2] D.Â Gong, J.Â Yang, L.Â Liu, Y.Â Zhang, I.Â Reid, C.Â Shen, A.Â vanÂ den Hengel,
    and Q.Â Shi, â€œFrom motion blur to motion flow: A deep learning solution for removing
    heterogeneous motion blur,â€ in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, July 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel,
    å’Œ Q. Shi, â€œä»è¿åŠ¨æ¨¡ç³Šåˆ°è¿åŠ¨æµï¼šä¸€ç§å»é™¤å¼‚è´¨è¿åŠ¨æ¨¡ç³Šçš„æ·±åº¦å­¦ä¹ è§£å†³æ–¹æ¡ˆ,â€ å‘è¡¨åœ¨ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2017å¹´7æœˆã€‚'
- en: '[3] O.Â Kupyn, V.Â Budzan, M.Â Mykhailych, D.Â Mishkin, and J.Â Matas, â€œDeblurgan:
    Blind motion deblurring using conditional adversarial networks,â€ in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] O. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, å’Œ J. Matas, â€œDeblurgan:
    ä½¿ç”¨æ¡ä»¶å¯¹æŠ—ç½‘ç»œè¿›è¡Œç›²è¿åŠ¨å»æ¨¡ç³Š,â€ å‘è¡¨åœ¨ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2018å¹´6æœˆã€‚'
- en: '[4] S.Â Nah, T.Â HyunÂ Kim, and K.Â MuÂ Lee, â€œDeep multi-scale convolutional neural
    network for dynamic scene deblurring,â€ in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, July 2017.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Nah, T. Hyun Kim, å’Œ K. Mu Lee, â€œç”¨äºåŠ¨æ€åœºæ™¯å»æ¨¡ç³Šçš„æ·±åº¦å¤šå°ºåº¦å·ç§¯ç¥ç»ç½‘ç»œ,â€ å‘è¡¨åœ¨ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2017å¹´7æœˆã€‚'
- en: '[5] M.Â Noroozi, P.Â Chandramouli, and P.Â Favaro, â€œMotion deblurring in the wild,â€
    *CoRR*, vol. abs/1701.01486, 2017\. [Online]. Available: [http://arxiv.org/abs/1701.01486](http://arxiv.org/abs/1701.01486)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Noroozi, P. Chandramouli, å’Œ P. Favaro, â€œé‡å¤–è¿åŠ¨å»æ¨¡ç³Š,â€ *CoRR*, å· abs/1701.01486,
    2017\. [åœ¨çº¿]. å¯ç”¨: [http://arxiv.org/abs/1701.01486](http://arxiv.org/abs/1701.01486)'
- en: '[6] S.Â Ramakrishnan, S.Â Pachori, A.Â Gangopadhyay, and S.Â Raman, â€œDeep generative
    filter for motion deblurring,â€ *CoRR*, vol. abs/1709.03481, 2017. [Online]. Available:
    [http://arxiv.org/abs/1709.03481](http://arxiv.org/abs/1709.03481)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Ramakrishnan, S. Pachori, A. Gangopadhyay, å’Œ S. Raman, â€œç”¨äºè¿åŠ¨å»æ¨¡ç³Šçš„æ·±åº¦ç”Ÿæˆæ»¤æ³¢å™¨,â€
    *CoRR*, å· abs/1709.03481, 2017. [åœ¨çº¿]. å¯ç”¨: [http://arxiv.org/abs/1709.03481](http://arxiv.org/abs/1709.03481)'
- en: '[7] C.Â J. Schuler, M.Â Hirsch, S.Â Harmeling, and B.Â SchÃ¶lkopf, â€œLearning to
    deblur,â€ *CoRR*, vol. abs/1406.7444, 2014\. [Online]. Available: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. J. Schuler, M. Hirsch, S. Harmeling, å’Œ B. SchÃ¶lkopf, â€œå­¦ä¹ å»æ¨¡ç³Š,â€ *CoRR*,
    å· abs/1406.7444, 2014\. [åœ¨çº¿]. å¯ç”¨: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)'
- en: '[8] J.Â Sun, W.Â Cao, Z.Â Xu, and J.Â Ponce, â€œLearning a convolutional neural network
    for non-uniform motion blur removal,â€ in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2015.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Sun, W. Cao, Z. Xu, å’Œ J. Ponce, â€œå­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œä»¥å»é™¤éå‡åŒ€è¿åŠ¨æ¨¡ç³Š,â€ å‘è¡¨åœ¨ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2015å¹´6æœˆã€‚'
- en: '[9] X.Â Tao, H.Â Gao, X.Â Shen, J.Â Wang, and J.Â Jia, â€œScale-recurrent network
    for deep image deblurring,â€ in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Tao, H. Gao, X. Shen, J. Wang, å’Œ J. Jia, â€œç”¨äºæ·±åº¦å›¾åƒå»æ¨¡ç³Šçš„å°ºåº¦é€’å½’ç½‘ç»œ,â€ å‘è¡¨åœ¨ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2018å¹´6æœˆã€‚'
- en: '[10] J.Â Zhang, J.Â Pan, J.Â Ren, Y.Â Song, L.Â Bao, R.Â W. Lau, and M.-H. Yang,
    â€œDynamic scene deblurring using spatially variant recurrent neural networks,â€
    in *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. å¼ , J. æ½˜, J. ä»», Y. å®‹, L. å®, R. W. åŠ³, å’Œ M.-H. æ¨, â€œåˆ©ç”¨ç©ºé—´å˜å¼‚é€’å½’ç¥ç»ç½‘ç»œè¿›è¡ŒåŠ¨æ€åœºæ™¯å»æ¨¡ç³Š,â€
    å‘è¡¨åœ¨ *IEEEè®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®®ï¼ˆCVPRï¼‰*ï¼Œ2018å¹´6æœˆã€‚'
- en: '[11] A.Â Odena, V.Â Dumoulin, and C.Â Olah, â€œDeconvolution and checkerboard artifacts,â€
    *Distill*, 2016\. [Online]. Available: [http://distill.pub/2016/deconv-checkerboard](http://distill.pub/2016/deconv-checkerboard)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Odena, V. Dumoulin, å’Œ C. Olah, â€œåå·ç§¯å’Œæ£‹ç›˜æ ¼ä¼ªå½±,â€ *Distill*, 2016\. [åœ¨çº¿].
    å¯ç”¨: [http://distill.pub/2016/deconv-checkerboard](http://distill.pub/2016/deconv-checkerboard)'
- en: '[12] U.Â Schmidt, C.Â Rother, S.Â Nowozin, J.Â Jancsary, and S.Â Roth, â€œDiscriminative
    non-blind deblurring,â€ in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2013.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] U. Schmidt, C. Rother, S. Nowozin, J. Jancsary å’Œ S. Rothï¼Œâ€œåˆ¤åˆ«å¼éç›²å»æ¨¡ç³Šâ€ï¼Œå‘è¡¨äº
    *IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2013 å¹´ 6 æœˆã€‚'
- en: '[13] C.Â J. Schuler, H.Â ChristopherÂ Burger, S.Â Harmeling, and B.Â Scholkopf,
    â€œA machine learning approach for non-blind image deconvolution,â€ in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2013.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. J. Schuler, H. Christopher Burger, S. Harmeling å’Œ B. Scholkopfï¼Œâ€œä¸€ç§ç”¨äºéç›²å›¾åƒè§£å·ç§¯çš„æœºå™¨å­¦ä¹ æ–¹æ³•â€ï¼Œå‘è¡¨äº
    *IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2013 å¹´ 6 æœˆã€‚'
- en: '[14] I.Â Goodfellow, J.Â Pouget-Abadie, M.Â Mirza, B.Â Xu, D.Â Warde-Farley, S.Â Ozair,
    A.Â Courville, and Y.Â Bengio, â€œGenerative adversarial nets,â€ in *Advances in Neural
    Information Processing Systems 27*, Z.Â Ghahramani, M.Â Welling, C.Â Cortes, N.Â D.
    Lawrence, and K.Â Q. Weinberger, Eds.Â Â Â Curran Associates, Inc., 2014, pp. 2672â€“2680.
    [Online]. Available: [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville å’Œ Y. Bengioï¼Œâ€œç”Ÿæˆå¯¹æŠ—ç½‘ç»œâ€ï¼Œå‘è¡¨äº *ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±• 27*ï¼ŒZ. Ghahramani, M.
    Welling, C. Cortes, N. D. Lawrence å’Œ K. Q. Weinberger ç¼–ï¼ŒCurran Associates, Inc.ï¼Œ2014
    å¹´ï¼Œç¬¬ 2672â€“2680 é¡µã€‚[åœ¨çº¿]. å¯ç”¨ï¼š [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)'
- en: '[15] P.Â Isola, J.-Y. Zhu, T.Â Zhou, and A.Â A. Efros, â€œImage-to-image translation
    with conditional adversarial networks,â€ in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, July 2017.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. Isola, J.-Y. Zhu, T. Zhou å’Œ A. A. Efrosï¼Œâ€œä½¿ç”¨æ¡ä»¶å¯¹æŠ—ç½‘ç»œçš„å›¾åƒåˆ°å›¾åƒç¿»è¯‘â€ï¼Œå‘è¡¨äº *IEEE
    è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2017 å¹´ 7 æœˆã€‚'
- en: '[16] M.Â Arjovsky, S.Â Chintala, and L.Â Bottou, â€œWasserstein generative adversarial
    networks,â€ in *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, 2017, pp. 214â€“223\. [Online].
    Available: [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Arjovsky, S. Chintala å’Œ L. Bottouï¼Œâ€œWasserstein ç”Ÿæˆå¯¹æŠ—ç½‘ç»œâ€ï¼Œå‘è¡¨äº *ç¬¬34å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®ï¼ˆICML
    2017ï¼‰ï¼Œæ‚‰å°¼ï¼Œæ–°å—å¨å°”å£«ï¼Œæ¾³å¤§åˆ©äºšï¼Œ2017 å¹´ 8 æœˆ 6-11 æ—¥*ï¼Œ2017 å¹´ï¼Œç¬¬ 214â€“223 é¡µã€‚[åœ¨çº¿]. å¯ç”¨ï¼š [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)'
- en: '[17] G.Â Huang, Z.Â Liu, L.Â vanÂ der Maaten, and K.Â Q. Weinberger, â€œDensely connected
    convolutional networks,â€ in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, July 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] G. Huang, Z. Liu, L. van der Maaten å’Œ K. Q. Weinbergerï¼Œâ€œå¯†é›†è¿æ¥å·ç§¯ç½‘ç»œâ€ï¼Œå‘è¡¨äº
    *IEEE è®¡ç®—æœºè§†è§‰ä¸æ¨¡å¼è¯†åˆ«ä¼šè®® (CVPR)*ï¼Œ2017 å¹´ 7 æœˆã€‚'
- en: '[18] J.Â Johnson, A.Â Alahi, and L.Â Fei-Fei, â€œPerceptual losses for real-time
    style transfer and super-resolution,â€ in *European Conference on Computer Vision*,
    2016.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Johnson, A. Alahi å’Œ L. Fei-Feiï¼Œâ€œç”¨äºå®æ—¶é£æ ¼è¿ç§»å’Œè¶…åˆ†è¾¨ç‡çš„æ„ŸçŸ¥æŸå¤±â€ï¼Œå‘è¡¨äº *æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®®*ï¼Œ2016
    å¹´ã€‚'
- en: '[19] M.Â B. S. B. H.Â S. KÃ¶hlerÂ R., HirschÂ M., â€œRecording and playback of camera
    shake: Benchmarking blind deconvolution with a real-world database,â€ in *European
    Conference on Computer Vision (ECCV)*, 2012.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. B. S. B. H. S. KÃ¶hler R.ï¼ŒHirsch M.ï¼Œâ€œç›¸æœºæŠ–åŠ¨çš„è®°å½•ä¸æ’­æ”¾ï¼šä½¿ç”¨çœŸå®ä¸–ç•Œæ•°æ®åº“è¿›è¡Œç›²è§£å·ç§¯åŸºå‡†æµ‹è¯•â€ï¼Œå‘è¡¨äº
    *æ¬§æ´²è®¡ç®—æœºè§†è§‰ä¼šè®® (ECCV)*ï¼Œ2012 å¹´ã€‚'
