- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:05:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.10128] Blind Deblurring using Deep Learning: A Survey *denotes equal
    contribution'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.10128] 基于深度学习的盲目去模糊：综述 *表示贡献相等'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.10128](https://ar5iv.labs.arxiv.org/html/1907.10128)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.10128](https://ar5iv.labs.arxiv.org/html/1907.10128)
- en: 'Blind Deblurring using Deep Learning: A Survey ^†^†thanks: *denotes equal contribution'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的盲目去模糊：综述 ^†^†感谢：*表示贡献相等
- en: Siddhant Sahu ^* dept.Computer Science and Engineering
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Siddhant Sahu ^* dept.Computer Science and Engineering
- en: KIIT University Bhubansewar, India
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: KIIT University Bhubansewar, India
- en: hello@siddhantsahu.com    Manoj Kumar Lenka ^* dept.Computer Science and Engineering
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: hello@siddhantsahu.com    Manoj Kumar Lenka ^* dept.Computer Science and Engineering
- en: KIIT University Bhubansewar, India
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: KIIT University Bhubansewar, India
- en: manojlenka1998@gmail.com    Pankaj Kumar Sa dept.Computer Science and Engineering
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: manojlenka1998@gmail.com    Pankaj Kumar Sa dept.Computer Science and Engineering
- en: NIT Rourkela Rourkela, India
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NIT Rourkela Rourkela, India
- en: pankajksa@nitrkl.ac.in
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: pankajksa@nitrkl.ac.in
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We inspect all the deep learning based solutions and provide holistic understanding
    of various architectures that have evolved over the past few years to solve blind
    deblurring. The introductory work used deep learning to estimate some features
    of the blur kernel and then moved onto predicting the blur kernel entirely, which
    converts the problem into non-blind deblurring. The recent state of the art techniques
    are end to end i.e they don’t estimate the blur kernel rather try to estimate
    the latent sharp image directly from the blurred image. The benchmarking PSNR
    and SSIM values on standard datasets of GOPRO and Köhler using various architectures
    are also provided.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了所有基于深度学习的解决方案，并提供了对过去几年中出现的各种架构的全面理解，以解决盲目去模糊的问题。最初的工作利用深度学习来估计模糊核的一些特征，然后转向完全预测模糊核，这将问题转换为非盲去模糊。最近的最先进技术是端到端的，即它们不估计模糊核，而是直接从模糊图像中估计潜在的清晰图像。还提供了使用各种架构在标准数据集GOPRO和Köhler上的PSNR和SSIM值的基准测试。
- en: 'Index Terms:'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 'Index Terms:'
- en: Deblurring, Deep Learning
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 去模糊，深度学习
- en: I Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 介绍
- en: Present day imaging systems for instance consumer level photography cameras,
    medical imaging equipments, scientific astronomical imaging systems, microscopy
    and more may experience blurring due to various intrinsic (diffraction, lens chromatic
    aberration, anti-aliasing filters etc.) or extrinsic (object motion, camera shake,
    out of focus, atmospheric turbulence etc.) factors which results in loss of image
    information. To overcome this problem and to recover lost information, deblurring
    is of great interest. From an artistic perspective blur is sometimes intentional
    in photography but for majority of the image analysis applications blurs ruins
    useful data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的成像系统，如消费级摄影相机、医学成像设备、科学天文成像系统、显微镜等，可能会因各种内在（衍射、镜头色差、抗锯齿滤镜等）或外在（物体运动、相机抖动、对焦不准、大气扰动等）因素而出现模糊，从而导致图像信息的丢失。为了克服这一问题并恢复丢失的信息，去模糊是一个重要的研究方向。从艺术角度来看，模糊在摄影中有时是故意的，但对于大多数图像分析应用来说，模糊会破坏有用的数据。
- en: The problem of deblurring is restoring a latent sharp image from a blurred image
    alone or at times with some statistical information about the blur kernel. This
    has attracted many researchers who have given many different solutions. These
    solutions can be broadly divided into statistical methods like,
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 去模糊的问题是从模糊图像单独恢复潜在的清晰图像，有时也需要一些关于模糊核的统计信息。这吸引了许多研究人员，他们提供了许多不同的解决方案。这些解决方案可以大致分为统计方法，如，
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Bayesian Inference Framework
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯推断框架
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Variational Methods
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变分方法
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Sparse Representation based method
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稀疏表示法基础的方法
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Homography based modeling
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于单应性的建模
- en: '5.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Region based methods
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于区域的方法
- en: where we try to estimate the blur kernel from just a single given blurred image
    and learning based methods ([[1](#bib.bib1)], [[2](#bib.bib2)], [[3](#bib.bib3)],
    [[4](#bib.bib4)], [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)]) which is data driven and the blur kernel
    is learned by providing not just one but several examples of blur and its corresponding
    sharp images as ground truth.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试从仅给定的模糊图像中估计模糊核，学习基础的方法（[[1](#bib.bib1)], [[2](#bib.bib2)], [[3](#bib.bib3)],
    [[4](#bib.bib4)], [[5](#bib.bib5)], [[6](#bib.bib6)], [[7](#bib.bib7)], [[8](#bib.bib8)],
    [[9](#bib.bib9)], [[10](#bib.bib10)])是数据驱动的，通过提供多个模糊实例及其相应的清晰图像作为真实值来学习模糊核。
- en: A blurred image can be modeled using equation,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊图像可以使用以下方程建模，
- en: '|  | $B=K*I+N$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $B=K*I+N$ |  | (1) |'
- en: where $B$ is the blurred image, $K$ is the kernel, $I$ is the sharp image and
    $N$ is the additive noise. In blind deblurring we are given $B$ only, and our
    goal is to predict a latent image $L$ which is the closest approximation to the
    sharp image $I$. This is an ill-posed problem, as we have to predict both $L$
    and $K$. Predicting the kernel accurately is essential, else it may lead to various
    artifacts [[11](#bib.bib11)], using learning based approach gives an accurate
    estimate of blur kernel compared to statistical approaches or skips the kernel
    estimation process altogether (i.e end-to-end). After estimation of blur kernel
    the problem converts to non-blind deconvolution, which can be solved using methods([[12](#bib.bib12)],
    [[13](#bib.bib13)])
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $B$ 是模糊图像，$K$ 是内核，$I$ 是清晰图像，$N$ 是附加噪声。在盲去模糊中，我们只给定 $B$，我们的目标是预测一个潜在图像 $L$，它是最接近清晰图像
    $I$ 的近似值。这是一个不适定的问题，因为我们必须同时预测 $L$ 和 $K$。准确预测内核是至关重要的，否则可能导致各种伪影[[11](#bib.bib11)]。使用基于学习的方法与统计方法相比，提供了更准确的模糊内核估计，或完全跳过内核估计过程（即端到端）。在估计模糊内核后，问题转变为非盲去卷积，这可以通过方法([[12](#bib.bib12)],
    [[13](#bib.bib13)])来解决。
- en: Statistical methods have another limitation i.e their inability to parallelize
    because a majority of them rely on coarse to fine iterative methods. Although
    deep learning models are significantly harder to train but once trained their
    inference time is comparatively fast. Moreover, deep learning methods have shown
    better on benchmarking metrics (PSNR and SSIM).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 统计方法还有另一个限制，即无法并行化，因为大多数方法依赖于粗到细的迭代方法。尽管深度学习模型训练起来显著更难，但训练完成后其推断时间相对较快。此外，深度学习方法在基准指标（PSNR
    和 SSIM）上表现更佳。
- en: In this paper we have divided the deep learning methods into two broad categories
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将深度学习方法分为两个广泛类别
- en: '1.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Estimation of Kernel - Here the proposed deep learning architectures are used
    to estimate some features (Fourier coefficients[[1](#bib.bib1)], motion flow [[2](#bib.bib2)][[8](#bib.bib8)])
    of the blur kernel or deriving the deconvolution filter [[1](#bib.bib1)] which
    can be used to get back the sharp image.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内核估计 - 在这里，提出的深度学习架构用于估计模糊内核的一些特征（傅里叶系数[[1](#bib.bib1)]，运动流[[2](#bib.bib2)][[8](#bib.bib8)]）或推导去卷积滤波器[[1](#bib.bib1)]，可以用来恢复清晰图像。
- en: '2.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: End to End - These methods are kernel free, that means we don’t estimate the
    blur kernel, rather only the blurred image is required and the model generates
    the predicted restored image. Some of these methods rely on generative models
    ([[3](#bib.bib3)], [[6](#bib.bib6)], [[4](#bib.bib4)]) which are trained in an
    adversarial method.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端到端 - 这些方法不依赖于内核，即我们不估计模糊内核，只需提供模糊图像，模型就会生成预测的恢复图像。其中一些方法依赖于生成模型（[[3](#bib.bib3)],
    [[6](#bib.bib6)], [[4](#bib.bib4)]），这些模型通过对抗性方法进行训练。
- en: The emphasis of this paper is on the “architecture” proposed by several author
    instead of the specific details of the architecture and to foster further research
    in blind deblurring using learning based methods.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点讨论了几位作者提出的“架构”，而不是架构的具体细节，并促进了使用基于学习的方法进行盲去模糊的进一步研究。
- en: II Methods
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 方法
- en: II-A Estimation of Kernel and its Attributes
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 内核及其属性的估计
- en: II-A1 Extraction of Features
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A1 特征提取
- en: 'For a optimal deblurring we require global information i.e data from different
    parts of the image, but to do so we need to have connectivity with all the pixels
    of image which will lead to a huge parameter space making it difficult to train
    and converge, hence Schuler et. al.[[7](#bib.bib7)] uses CNNs to extract features
    locally and then combine them to estimate the kernel. For this they use a multi
    scale (for different kernel sizes), multi stage architecture, where each stage
    consists of three modules feature extraction, kernel estimation, latent image
    estimation (Fig. [1](#S2.F1 "Figure 1 ‣ II-A1 Extraction of Features ‣ II-A Estimation
    of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")). In the first stage given a blurry image,
    a sharp image is estimated, for later stages they gave the blurry image concatenated
    with the estimated sharp image of the previous stage as input.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳去模糊，我们需要全局信息，即来自图像不同部分的数据，但为此我们需要与图像的所有像素连接，这将导致庞大的参数空间，使得训练和收敛变得困难，因此
    Schuler 等人[[7](#bib.bib7)] 使用 CNNs 在局部提取特征，然后将其结合起来估计核。为此，他们使用了一个多尺度（针对不同的核尺寸）多阶段架构，其中每个阶段包含三个模块：特征提取、核估计、潜在图像估计（图
    [1](#S2.F1 "图 1 ‣ II-A1 特征提取 ‣ II-A 核估计及其属性 ‣ II 方法 ‣ 基于深度学习的盲去模糊：综述 *表示相等贡献")）。在第一阶段，给定一个模糊图像，估计出一个清晰图像；在后续阶段，他们将模糊图像与前一阶段的估计清晰图像串联作为输入。
- en: '![Refer to caption](img/770bcff05adadea94ed7d93de1734ccd.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/770bcff05adadea94ed7d93de1734ccd.png)'
- en: 'Figure 1: Shows the multi stage architecture used by Schuler et. al.[[7](#bib.bib7)],
    here the different modules in a stage are shown for the first stage only. The
    latter stages are identical to the first, except the input which is concatenation
    of blurred image and the restored image of the previous layer'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了 Schuler 等人[[7](#bib.bib7)] 使用的多阶段架构，这里仅展示了第一阶段的不同模块。后续阶段与第一阶段相同，唯一不同的是输入为模糊图像和前一层恢复图像的串联
- en: In feature extraction module they used a convolutional layer to extract features
    using filters $f_{j}$, then they used $\tanh$ to introduce non-linearity and finally
    these hidden features are linearly recombined using cofficients $\alpha_{ij}$
    and $\beta_{ij}$ to form hidden images $x_{i}$ and $y_{i}$ for stage $i$ used
    for kernel estimation, formally,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征提取模块中，他们使用了一个卷积层，通过滤波器 $f_{j}$ 提取特征，然后使用 $\tanh$ 引入非线性，最后这些隐藏特征使用系数 $\alpha_{ij}$
    和 $\beta_{ij}$ 线性重组，形成用于核估计的隐藏图像 $x_{i}$ 和 $y_{i}$，正式地，
- en: '|  | $\begin{split}x_{i}=\sum_{j}\alpha_{ij}\tanh(f_{j}*y)\\ y_{i}=\sum_{j}\beta_{ij}\tanh(f_{j}*y)\end{split}$
    |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}x_{i}=\sum_{j}\alpha_{ij}\tanh(f_{j}*y)\\ y_{i}=\sum_{j}\beta_{ij}\tanh(f_{j}*y)\end{split}$
    |  | (2) |'
- en: where $y$ is blurred image $B$ for first stage or concatenation of $B$ and predicted
    sharp image $L$ for later stages.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y$ 是第一阶段的模糊图像 $B$，或者是后续阶段中 $B$ 和预测的清晰图像 $L$ 的串联。
- en: Given $x_{i}$ and $y_{i}$ the kernel estimation module estimates the kernel
    $K$ by minimizing,
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 $x_{i}$ 和 $y_{i}$，核估计模块通过最小化来估计核 $K$，
- en: '|  | $\sum_{i}\lVert K*x_{i}-y_{i}\rVert^{2}+\beta_{k}\lVert K\rVert^{2}$ |  |
    (3) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{i}\lVert K*x_{i}-y_{i}\rVert^{2}+\beta_{k}\lVert K\rVert^{2}$ |  |
    (3) |'
- en: for $K$. Given $K$ we can find the latent (restored) image $L$ by solving the
    equation,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $K$。给定 $K$，我们可以通过求解方程来找到潜在（恢复的）图像 $L$，
- en: '|  | $\lVert K*L-B\rVert^{2}+\beta_{x}\lVert L\rVert^{2}$ |  | (4) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\lVert K*L-B\rVert^{2}+\beta_{x}\lVert L\rVert^{2}$ |  | (4) |'
- en: 'for $L$, where both $\beta_{k}$ and $\beta_{x}$ are regularization weights.
    Both (Eqn.[3](#S2.E3 "In II-A1 Extraction of Features ‣ II-A Estimation of Kernel
    and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) and (Eqn.[4](#S2.E4 "In II-A1 Extraction of Features
    ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) can be solved in
    one step in Fourier space.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $L$，其中 $\beta_{k}$ 和 $\beta_{x}$ 都是正则化权重。方程（Eqn.[3](#S2.E3 "在 II-A1 特征提取
    ‣ II-A 核估计及其属性 ‣ II 方法 ‣ 基于深度学习的盲去模糊：综述 *表示相等贡献"）和（Eqn.[4](#S2.E4 "在 II-A1 特征提取
    ‣ II-A 核估计及其属性 ‣ II 方法 ‣ 基于深度学习的盲去模糊：综述 *表示相等贡献"）可以在傅里叶空间中一步求解。
- en: II-A2 Estimation of Fourier Coefficients
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A2 傅里叶系数估计
- en: Given a blurry image $B[n]$ where $n\in\mathbb{Z}^{2}$ are the indexes of pixels.
    We need to find a latent sharp image $L[n]$ such that it resembles the sharp image
    $I[n]$ closely where,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个模糊图像 $B[n]$，其中 $n\in\mathbb{Z}^{2}$ 是像素的索引。我们需要找到一个潜在的清晰图像 $L[n]$，使其与清晰图像
    $I[n]$ 具有高度相似性，其中，
- en: '|  | $B[n]=(I*K)[n]+N[n]$ |  | (5) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $B[n]=(I*K)[n]+N[n]$ |  | (5) |'
- en: where $K[n]$ is the blur kernel such that $K[n]\geq 0$ (positivity constraint),
    $\sum_{n}K[n]=1$ (unit sum constraint) and $N[n]$ the noise.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$K[n]$是模糊核，使得$K[n]\geq 0$（非负性约束），$\sum_{n}K[n]=1$（单位和约束），$N[n]$为噪声。
- en: '![Refer to caption](img/c8c370d05d6c25d340d85e60b2ffd0d1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c8c370d05d6c25d340d85e60b2ffd0d1.png)'
- en: 'Figure 2: Architecture used by Chakrabati[[1](#bib.bib1)] for prediction of
    Fourier coefficients for the deconvolution filter. Here $H$ is high pass, $B_{2},B_{1}$
    are band pass, while $L$ is low pass frequency band. The letters in bold are Fourier
    transforms of the corresponding symbols.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Chakrabarti[[1](#bib.bib1)]用于预测去卷积滤波器傅里叶系数的架构。这里$H$是高通，$B_{2},B_{1}$是带通，而$L$是低通频率带。加粗的字母是对应符号的傅里叶变换。
- en: In the method given by Chakrabarti [[1](#bib.bib1)] a blurry image $B[n]$ is
    divided into several overlapping patches. Given a blurry patch $B_{p}=\{B[n]:n\in
    p\}$ they considered the surrounding pixels of the patch while finding its Fourier
    coefficients for better results, let the blurry image with the neighboring pixels
    be $B_{p^{+}}=\{B[n]:n\in p^{+}\}$ where $p\subset p^{+}$.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在Chakrabarti的方法[[1](#bib.bib1)]中，模糊图像$B[n]$被划分为几个重叠的图块。对于一个模糊图块$B_{p}=\{B[n]:n\in
    p\}$，在寻找其傅里叶系数时，他们考虑了图块的周围像素以获得更好的结果，让模糊图像与邻近像素一起表示为$B_{p^{+}}=\{B[n]:n\in p^{+}\}$，其中$p\subset
    p^{+}$。
- en: 'Then they used a neural network (Fig. [2](#S2.F2 "Figure 2 ‣ II-A2 Estimation
    of Fourier Coefficients ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods
    ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution"))
    to predict the Complex Fourier Coefficients of the Deconvolution Filter $\textbf{G}_{p^{+}}[z]$
    for the blurry patch $B_{p^{+}}$, where $z$ is the two dimensional spatial frequencies
    in DFT (Discrete Fourier Transform). Then the filter is applied to the DFT of
    $B_{p^{+}}$ i.e $\textbf{B}_{p^{+}}[z]$ giving us the DFT of latent sharp image
    $\textbf{L}_{p^{+}}[z]$,'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '然后他们使用了一个神经网络（图 [2](#S2.F2 "Figure 2 ‣ II-A2 Estimation of Fourier Coefficients
    ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")）来预测模糊图块$B_{p^{+}}$的去卷积滤波器的复傅里叶系数$\textbf{G}_{p^{+}}[z]$，其中$z$是离散傅里叶变换（DFT）中的二维空间频率。然后将滤波器应用于$B_{p^{+}}$的DFT，即$\textbf{B}_{p^{+}}[z]$，从而得到潜在清晰图像的DFT
    $\textbf{L}_{p^{+}}[z]$，'
- en: '|  | $\textbf{L}_{p^{+}}[z]=\textbf{B}_{p^{+}}[z]\times\textbf{G}_{p^{+}}[z]$
    |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{L}_{p^{+}}[z]=\textbf{B}_{p^{+}}[z]\times\textbf{G}_{p^{+}}[z]$
    |  | (6) |'
- en: After getting $\textbf{L}_{p^{+}}[z]$, we can use a inverse discrete Fourier
    transform (IDFT) to get the latent image patch $L_{p^{+}}$ from which we can extract
    $L_{p}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得$\textbf{L}_{p^{+}}[z]$后，我们可以使用逆离散傅里叶变换（IDFT）得到潜在图块$L_{p^{+}}$，从中提取$L_{p}$。
- en: 'To generate coefficients of the filter they used the architecture shown in
    (Fig. [2](#S2.F2 "Figure 2 ‣ II-A2 Estimation of Fourier Coefficients ‣ II-A Estimation
    of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")). The architecture uses multi-resolution
    decomposition strategy i.e the initial layers of the neural network are connected
    to only adjacent bands of frequency and not fully connected (here they are considering
    locality in the frequency domain, in contrast to CNNs which consider locality
    in the spatial domain). The image is sampled into patches of various resolution
    and a lower resolution patch is used to sample a higher frequency band using DFT.
    The loss function for the network is,'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '为了生成滤波器的系数，他们使用了图中所示的架构（图 [2](#S2.F2 "Figure 2 ‣ II-A2 Estimation of Fourier
    Coefficients ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods ‣ Blind
    Deblurring using Deep Learning: A Survey *denotes equal contribution")）。该架构使用了多分辨率分解策略，即神经网络的初始层仅连接到相邻的频率带，而不是完全连接（在这里，他们考虑了频率域中的局部性，与考虑空间域中局部性的CNNs形成对比）。图像被采样为不同分辨率的图块，并且低分辨率图块用于使用DFT采样更高的频率带。网络的损失函数为，'
- en: '|  | $L=\dfrac{1}{&#124;p&#124;}\sum_{n\in p}(L_{p}[n]-I_{p}[n])^{2}$ |  |
    (7) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\dfrac{1}{&#124;p&#124;}\sum_{n\in p}(L_{p}[n]-I_{p}[n])^{2}$ |  |
    (7) |'
- en: They combined all the restored patches to get the first estimate of the latent
    image $L_{N}[n]$. It is assumed that the entire image is blurred by the same motion
    kernel(uniform blur), but they predicted different motion kernels for different
    patches, hence to find a global motion kernel $K_{\lambda}[n]$ they used the first
    estimate $L_{N}[n]$ as follows,
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将所有恢复的图块结合起来，得到潜在图像$L_{N}[n]$的初步估计。假设整个图像是由相同的运动核（均匀模糊）模糊的，但他们为不同的图块预测了不同的运动核，因此为了找到全局运动核$K_{\lambda}[n]$，他们使用了初步估计$L_{N}[n]$，其方式如下，
- en: '|  | $K_{\lambda}=arg\text{ min}\sum_{i}\lVert(K*(f_{i}*L_{N}))-(f_{i}*B)\rVert^{2}+\lambda\sum_{n}&#124;K[n]&#124;$
    |  | (8) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $K_{\lambda}=arg\text{ min}\sum_{i}\lVert(K*(f_{i}*L_{N}))-(f_{i}*B)\rVert^{2}+\lambda\sum_{n}&#124;K[n]&#124;$
    |  | (8) |'
- en: Here $f_{i}$ are different derivative filters. They use $L1$ regularization.
    In classical statistical methods refining latent image from a previous estimate
    is an iterative step, while here they only do it once to estimate the global blur
    kernel. After estimation of the global blur kernel, the problem becomes that of
    a non-blind deblurring and latent sharp image can be estimated using deconvolution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$f_{i}$是不同的导数滤波器。它们使用$L1$正则化。在经典统计方法中，从先前估计中提炼潜在图像是一个迭代步骤，而在这里，它们只进行一次以估计全局模糊核。全局模糊核估计后，问题变成了非盲去模糊问题，潜在的清晰图像可以通过去卷积来估计。
- en: II-A3 Estimation of Motion Vector for each Patch
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A3 每个补丁的运动向量估计
- en: '![Refer to caption](img/366b4c63b6765a9c97ae65a1ded15b1c.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/366b4c63b6765a9c97ae65a1ded15b1c.png)'
- en: 'Figure 3: Network architecture for predicting the motion kernel of a given
    blurred patch used by Sun et. al.[[8](#bib.bib8)]'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Sun等人使用的预测给定模糊补丁的运动核的网络架构[[8](#bib.bib8)]
- en: 'In this method proposed by Sun et. al.[[8](#bib.bib8)] an image is divided
    into several overlapping patches. For each patch a CNN with a fully connected
    layer and softmax layer is used to find the probability distribution of motion
    kernels for that patch (Fig.[3](#S2.F3 "Figure 3 ‣ II-A3 Estimation of Motion
    Vector for each Patch ‣ II-A Estimation of Kernel and its Attributes ‣ II Methods
    ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")).
    Given a patch $\Psi_{p}$ centered at pixel p, the network finds a probability
    distribution,'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '在Sun等人提出的方法[[8](#bib.bib8)]中，一幅图像被分成几个重叠的补丁。对于每个补丁，使用一个具有全连接层和softmax层的CNN来找到该补丁的运动核概率分布（见图[3](#S2.F3
    "Figure 3 ‣ II-A3 Estimation of Motion Vector for each Patch ‣ II-A Estimation
    of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")）。给定以像素$p$为中心的补丁$\Psi_{p}$，网络找出一个概率分布，'
- en: '|  | $P(m=(l,o)&#124;\Psi_{p})$ |  | (9) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(m=(l,o)&#124;\Psi_{p})$ |  | (9) |'
- en: where $m=(l,o)$ is the motion kernel with length $l$ and orientation $o$. Here
    $l\in S^{l}$ and $o\in S^{o}$ both $S^{l}$ and $S^{o}$ are discretized sets of
    length and orientation. Due to discretization the number of motion kernels is
    limited which leads to blocky artifacts. Hence, they rotated the image and its
    corresponding motion kernel by the same amount to get new data entry, which is
    then used in training this increases the range of $S^{o}$ that is given a patch
    $\Psi_{p}(I)$ of image $I$ and its corresponding motion kernel $m=(l,o)$, if image
    is rotated by an angel of $\theta$ then for patch $\Psi_{p}(I_{\theta})$ they
    got the motion kernel as $m=(l,o-\theta)$. Since they are doing a multicalss classification(where
    each class is a motion kernel) they use cross entropy loss given as,
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$m=(l,o)$是具有长度$l$和方向$o$的运动核。这里$l\in S^{l}$和$o\in S^{o}$，$S^{l}$和$S^{o}$都是离散化的长度和方向集合。由于离散化，运动核的数量有限，这导致块状伪影。因此，他们通过相同的角度旋转图像及其对应的运动核来获得新的数据条目，然后用于训练，这增加了$S^{o}$的范围。也就是说，给定图像$I$的补丁$\Psi_{p}(I)$及其对应的运动核$m=(l,o)$，如果图像旋转了角度$\theta$，那么对于补丁$\Psi_{p}(I_{\theta})$，他们得到了运动核$m=(l,o-\theta)$。由于他们正在进行多类分类（每个类别是一个运动核），他们使用的交叉熵损失定义为：
- en: '|  | $P(m=(l,o)&#124;\Psi)=\dfrac{\exp(z_{i})}{\sum_{k=1}^{n}\exp(z_{k})}$
    |  | (10) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(m=(l,o)&#124;\Psi)=\dfrac{\exp(z_{i})}{\sum_{k=1}^{n}\exp(z_{k})}$
    |  | (10) |'
- en: where $z$ is the output of the last fully connected layer and $n=|S^{l}|\times|S^{o}|$
    i.e $n$ is the total number of motion kernels. Since the patches are overlapping
    many patches may contain the same pixel, in such case the confidence of motion
    kernel $m$ at a pixel $p$ is given as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$z$是最后一个全连接层的输出，$n=|S^{l}|\times|S^{o}|$，即$n$是运动核的总数。由于补丁是重叠的，许多补丁可能包含相同的像素，在这种情况下，像素$p$处运动核$m$的置信度计算如下：
- en: '|  | $C(m_{p}=(l,o))=\dfrac{1}{Z}\sum_{q:p\in\Psi_{q}}G_{\sigma}(\lVert x_{p}-x_{q}\rVert^{2})P(m=(l,o)&#124;\Psi_{q})$
    |  | (11) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $C(m_{p}=(l,o))=\dfrac{1}{Z}\sum_{q:p\in\Psi_{q}}G_{\sigma}(\lVert x_{p}-x_{q}\rVert^{2})P(m=(l,o)&#124;\Psi_{q})$
    |  | (11) |'
- en: where $q$ is center pixel of patch $\Psi_{q}$ such that $p\in\Psi_{q}$. $x_{p}$
    and $x_{q}$ are the coordinates of $p$ and $q$ respectively. $G_{\sigma}$ is a
    Gaussian function that gives more weight to patches whose center pixel $q$ is
    closest to $p$. $Z$ is a normalization constant.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$q$是补丁$\Psi_{q}$的中心像素，使$p\in\Psi_{q}$。$x_{p}$和$x_{q}$分别是$p$和$q$的坐标。$G_{\sigma}$是一个高斯函数，它对中心像素$q$最接近$p$的补丁赋予更多的权重。$Z$是归一化常数。
- en: '![Refer to caption](img/a5b092044feb6aff493d2484eb3acf5a.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5b092044feb6aff493d2484eb3acf5a.png)'
- en: 'Figure 4: This shows given a pixel $p$ (in yellow) how MRF smoothen its value
    based on $N(p)$ i.e its neighboring pixels'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：这显示了给定一个像素 $p$（黄色）时，MRF 如何根据 $N(p)$ 即其邻近像素来平滑其值
- en: 'After estimating the motion kernels for all the patches they are concatenated
    and a Markov Random Function (MRF) is used to merge them all together, smoothen
    the transition of motion kernels amongst neighboring pixels (Fig.[4](#S2.F4 "Figure
    4 ‣ II-A3 Estimation of Motion Vector for each Patch ‣ II-A Estimation of Kernel
    and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) and generates a dense motion field by minimizing
    energy function,'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在估计了所有补丁的运动核后，将它们连接起来，并使用马尔可夫随机函数 (MRF) 将它们合并在一起，平滑邻近像素之间运动核的过渡（图。[4](#S2.F4
    "Figure 4 ‣ II-A3 Estimation of Motion Vector for each Patch ‣ II-A Estimation
    of Kernel and its Attributes ‣ II Methods ‣ Blind Deblurring using Deep Learning:
    A Survey *denotes equal contribution")），并通过最小化能量函数生成密集运动场。'
- en: '|  | $\sum_{p\in\Omega}[-C(m_{p}=(l_{p},o_{p}))+\sum_{q\in N(p)}\lambda[(u_{p}-u_{q})^{2}+(v_{p}-v_{q})^{2}]]$
    |  | (12) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{p\in\Omega}[-C(m_{p}=(l_{p},o_{p}))+\sum_{q\in N(p)}\lambda[(u_{p}-u_{q})^{2}+(v_{p}-v_{q})^{2}]]$
    |  | (12) |'
- en: where $\Omega$ is a image region. $u_{p},u_{q},v_{p},v_{q}$ are defined as $u_{i}=l_{i}\cos(o_{i})$
    and $v_{i}=l_{i}\sin(o_{i})$ for $i=p,q$. $N(p)$ is the neighborhood of $p$. The
    first term gives more weight to using the motion kernel that the CNN chose with
    the highest confidence, while the second term looks at the neighboring pixels
    and tries to smoothen it. After predicting the motion field they deconvolve the
    blurred image with it to get the deblurred image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Omega$ 是图像区域。 $u_{p},u_{q},v_{p},v_{q}$ 定义为 $u_{i}=l_{i}\cos(o_{i})$ 和
    $v_{i}=l_{i}\sin(o_{i})$ 对于 $i=p,q$。 $N(p)$ 是 $p$ 的邻域。第一个项赋予 CNN 选择的具有最高信心的运动核更多的权重，而第二个项查看邻近像素并尝试平滑。预测运动场后，他们使用其对模糊图像进行反卷积，以获得去模糊图像。
- en: II-A4 Estimation of Dense Motion Flow for entire Image
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-A4 整幅图像的密集运动流估计
- en: In the previous approach of Sun et. al[[8](#bib.bib8)] a motion kernel was predicted
    for each patch using a CNN classifier and then all the motion kernels were smoothened
    using Markov Random Field (MRF) to get the dense motion field. In the method used
    by Gong et. al.[[2](#bib.bib2)] they also predict a dense motion field, but a
    pixel wise dense motion field is generated for the entire image directly (i.e
    image is not divided into patches).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sun 等人[[8](#bib.bib8)] 的前一种方法中，使用 CNN 分类器预测每个补丁的运动核，然后利用马尔可夫随机场 (MRF) 平滑所有运动核，以获得密集的运动场。在
    Gong 等人[[2](#bib.bib2)] 使用的方法中，他们也预测了密集的运动场，但为整个图像直接生成了逐像素的密集运动场（即图像未被划分为补丁）。
- en: '![Refer to caption](img/a3ca9c291a4eb84e69c9ba281a9ddb4b.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a3ca9c291a4eb84e69c9ba281a9ddb4b.png)'
- en: 'Figure 5: Architecture used by Gong et. al.[[2](#bib.bib2)] to predict the
    motion field given a blurry image'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Gong 等人[[2](#bib.bib2)] 用于预测给定模糊图像的运动场的架构
- en: In Sun et. al[[8](#bib.bib8)] they assumeded uniform motion blur within a single
    patch as only one motion kernel was chosen for a patch. This does not generalize
    to real life data properly were we can have a heterogeneous motion blur i.e motion
    may vary from pixel to pixel. In such cases an end to end approach of generating
    motion field [[2](#bib.bib2)] can give better results as they are considering
    the entire image (larger spatial context) instead of a single patch. Hence, this
    method is suitable for heterogeneous motion blurs. It does not require any post
    processing like MRF.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sun 等人[[8](#bib.bib8)] 的方法中，他们假设单个补丁内的运动模糊是均匀的，因为一个补丁只选择了一个运动核。这并不能很好地推广到实际数据中，因为我们可能会有异质运动模糊，即运动可能因像素而异。在这种情况下，生成运动场的端到端方法[[2](#bib.bib2)]
    可以提供更好的结果，因为它们考虑了整个图像（更大的空间背景），而不是单个补丁。因此，这种方法适用于异质运动模糊。不需要像 MRF 这样的后处理。
- en: If the network is represented by a function of $f$. Then given a blurred image
    $B$, the goal of the network is to generate the motion field $M$, i.e
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果网络由函数 $f$ 表示。则给定模糊图像 $B$，网络的目标是生成运动场 $M$，即
- en: '|  | $f(B)=M$ |  | (13) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(B)=M$ |  | (13) |'
- en: where the motion field $M$ can be represented as,
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 运动场 $M$ 可以表示为，
- en: '|  | $M=(U,V)$ |  | (14) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $M=(U,V)$ |  | (14) |'
- en: where $U$ and $V$ are the horizontal and vertical motion maps respectively.
    Now given a pixel $p=(i,j)$ where $(i,j)$ are the coordinates of pixel, then we
    get,
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $U$ 和 $V$ 分别是水平和垂直运动图。现在给定一个像素 $p=(i,j)$，其中 $(i,j)$ 是像素的坐标，则我们得到，
- en: '|  | $M(i,j)=(U(i,j),V(i,j))$ |  | (15) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $M(i,j)=(U(i,j),V(i,j))$ |  | (15) |'
- en: let $M(i,j)=m_{p}$, $U(i,j)=u_{p}$ and $V(i,j)=v_{p}$ then we get,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $M(i,j)=m_{p}$, $U(i,j)=u_{p}$ 和 $V(i,j)=v_{p}$，我们得到，
- en: '|  | $m_{p}=(u_{p},v_{p})$ |  | (16) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $m_{p}=(u_{p},v_{p})$ |  | (16) |'
- en: where $u_{p}\in\mathbb{D}_{u}$ and $v_{p}\in\mathbb{D}_{v}$. Here $\mathbb{D}_{u}$
    and $\mathbb{D}_{v}$ are the discretized motion vectors in the horizontal and
    vertical directions respectively, they are defined as, $\mathbb{D}_{u}=\{u|u\in\mathbb{Z},|u|\leq
    u_{max}\}$ and $\mathbb{D}_{v}=\{v|v\in\mathbb{Z},|v|\leq v_{max}\}$.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $u_{p}\in\mathbb{D}_{u}$ 和 $v_{p}\in\mathbb{D}_{v}$。这里 $\mathbb{D}_{u}$ 和
    $\mathbb{D}_{v}$ 分别是水平方向和垂直方向的离散化运动矢量，它们定义为，$\mathbb{D}_{u}=\{u|u\in\mathbb{Z},|u|\leq
    u_{max}\}$ 和 $\mathbb{D}_{v}=\{v|v\in\mathbb{Z},|v|\leq v_{max}\}$。
- en: But, two motion vectors of opposite directions and same magnitude would generate
    the same blur pattern i.e $m_{p}=(u_{p},v_{p})$ and $-m_{p}=(-u_{p},-v_{p})$ would
    give the same blur, hence they restrict the horizontal motion vector to be positive
    only i.e $u_{p}\in\mathbb{D}_{u}^{+}$ where $\mathbb{D}_{u}^{+}=\{u|u\in\mathbb{Z}^{+},|u|\leq
    u_{max}\}$, this is done by letting $(u_{p},v_{p})=\phi(u_{p},v_{p})$ where,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，相反方向且幅度相同的两个运动矢量会生成相同的模糊模式，即 $m_{p}=(u_{p},v_{p})$ 和 $-m_{p}=(-u_{p},-v_{p})$
    会产生相同的模糊，因此他们限制水平运动矢量只能为正，即 $u_{p}\in\mathbb{D}_{u}^{+}$，其中 $\mathbb{D}_{u}^{+}=\{u|u\in\mathbb{Z}^{+},|u|\leq
    u_{max}\}$，这是通过让 $(u_{p},v_{p})=\phi(u_{p},v_{p})$ 实现的，其中，
- en: '|  | $\phi(u_{p},v_{p})=\begin{cases}(-u_{p},-v_{p})\text{ if }u_{p}<0\\ (u_{p},v_{p})\text{
    if }u_{p}\geq 0\end{cases}$ |  | (17) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\phi(u_{p},v_{p})=\begin{cases}(-u_{p},-v_{p})\text{ 如果 }u_{p}<0\\ (u_{p},v_{p})\text{
    如果 }u_{p}\geq 0\end{cases}$ |  | (17) |'
- en: If an image of size $P\times Q$ is sent trough the network(excluding the softmax)
    it generates a feature map of size $P\times Q\times D$ where $D=|\mathbb{D}_{u}^{+}|+|\mathbb{D}_{v}|$.
    This feature map is then divided into two parts of shape $P\times Q\times|\mathbb{D}_{u}^{+}|$
    and $P\times Q\times|\mathbb{D}_{v}|$. These two features pass through separate
    softmax layers to generate the horizontal and vertical motion maps $U$ and $V$
    receptively. Using these two vector maps they generate the final motion field
    $M$. After getting the motion field $M$ this becomes a non-blind debluring problem
    and a deconvolution is used to get the sharp image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一张 $P\times Q$ 尺寸的图像通过网络（不包括 softmax），它会生成一个 $P\times Q\times D$ 尺寸的特征图，其中
    $D=|\mathbb{D}_{u}^{+}|+|\mathbb{D}_{v}|$。该特征图随后被分为两个部分，形状为 $P\times Q\times|\mathbb{D}_{u}^{+}|$
    和 $P\times Q\times|\mathbb{D}_{v}|$。这两个特征通过独立的 softmax 层生成水平和垂直运动图 $U$ 和 $V$。利用这两个矢量图生成最终的运动场
    $M$。在得到运动场 $M$ 后，这变成了一个非盲去模糊问题，使用反卷积来获取清晰图像。
- en: II-B End to End
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 端到端
- en: II-B1 Without Adversarial Loss
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 无对抗损失
- en: 'Deblurring requires a large receptive field (global knowledge), but CNNs provide
    local knowledge and do not show the long range dependencies properly, for this
    reason Nah et. al[[4](#bib.bib4)] (refer [II-B2](#S2.SS2.SSS2 "II-B2 With Adversarial
    Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")) uses a scaled structure and a large number
    of convolutional layers with residual connections to improve the receptive field
    of the structure, but this also makes it harder to converge due to the large number
    of parameters. Hence (Tao et. al.[[9](#bib.bib9)]) use a scale recurrent structure
    where they still use a scaled network, but significantly reduce the number of
    parameters by using a smaller encoder-decoder type network with a recurrent module
    and also share the weights between scales.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '去模糊需要大范围的感受野（全局知识），但卷积神经网络（CNN）提供的是局部知识，不能很好地显示长期依赖性。因此，Nah 等人[[4](#bib.bib4)]（参见
    [II-B2](#S2.SS2.SSS2 "II-B2 With Adversarial Loss ‣ II-B End to End ‣ II Methods
    ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")）使用了缩放结构和大量的卷积层及残差连接来提高结构的感受野，但这也因参数众多而使得收敛更困难。因此
    (Tao et. al.[[9](#bib.bib9)]) 使用了尺度递归结构，在此结构中，他们仍使用缩放网络，但通过使用一个较小的编码器-解码器类型网络和递归模块，显著减少了参数数量，同时在尺度之间共享权重。'
- en: '![Refer to caption](img/ce8db4df8ac55ef52c59d1547946bf41.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ce8db4df8ac55ef52c59d1547946bf41.png)'
- en: 'Figure 6: Scale Recurrent Network Architecture used by (Tao et. al.[[9](#bib.bib9)])'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：由 (Tao et. al.[[9](#bib.bib9)]) 使用的尺度递归网络结构
- en: 'Scale recurrent network (Fig.[6](#S2.F6 "Figure 6 ‣ II-B1 Without Adversarial
    Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")) [[9](#bib.bib9)] consists of three parts,
    encoder ($Net_{E}$), recurrent ($Net_{R}$) and decoder ($Net_{D}$) modules. This
    can be represented as,'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '缩放递归网络（图。[6](#S2.F6 "Figure 6 ‣ II-B1 Without Adversarial Loss ‣ II-B End to
    End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal
    contribution")）[[9](#bib.bib9)] 由三个部分组成：编码器（`$Net_{E}$`）、递归（`$Net_{R}$`）和解码器（`$Net_{D}$`）模块。可以表示为，'
- en: '|  | <math   alttext="\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`|  | <math   alttext="\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\"`'
- en: L^{i}=Net_{D}(g^{i};\theta_{D})\\
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`L^{i}=Net_{D}(g^{i};\theta_{D})\\`'
- en: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><msup ><mi  >f</mi><mi >i</mi></msup><mo
    >=</mo><mrow ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi  >t</mi><mi >E</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi >B</mi><mi
    >i</mi></msup><mo >,</mo><msup ><mi >L</mi><mrow ><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow><mo stretchy="false"  >↑</mo></mrow></msup><mo >;</mo><msub ><mi
    >θ</mi><mi >E</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><msup ><mi  >h</mi><mi >i</mi></msup><mo
    >,</mo><msup ><mi >g</mi><mi >i</mi></msup></mrow><mo >=</mo><mrow ><mi >N</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi >R</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >h</mi><mrow ><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow><mo stretchy="false"  >↑</mo></mrow></msup><mo >,</mo><msup ><mi
    >f</mi><mi >i</mi></msup><mo >;</mo><msub ><mi >θ</mi><mi >R</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mi >L</mi><mi >i</mi></msup><mo >=</mo><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi  >t</mi><mi >D</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msup ><mi >g</mi><mi >i</mi></msup><mo >;</mo><msub
    ><mi >θ</mi><mi >D</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑓</ci><ci  >𝑖</ci></apply><apply
    ><ci >𝑁</ci><ci  >𝑒</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑡</ci><ci >𝐸</ci></apply><vector ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐵</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐿</ci><apply ><ci >↑</ci><apply ><ci >𝑖</ci><cn type="integer" >1</cn></apply><csymbol
    cd="latexml" >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝐸</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ℎ</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑔</ci><ci >𝑖</ci></apply><apply ><ci >𝑁</ci><ci  >𝑒</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝑅</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℎ</ci><apply ><ci
    >↑</ci><apply ><ci >𝑖</ci><cn type="integer" >1</cn></apply><csymbol cd="latexml"
    >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑓</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝑅</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝐿</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply ><ci >𝑁</ci><ci  >𝑒</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝐷</ci></apply><list
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑔</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝐷</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\ L^{i}=Net_{D}(g^{i};\theta_{D})\\
    \end{split}</annotation></semantics></math> |  | (18) |
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: \end{split}" display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mrow ><msup ><mi  >f</mi><mi >i</mi></msup><mo
    >=</mo><mrow ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><msub ><mi  >t</mi><mi >E</mi></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi >B</mi><mi
    >i</mi></msup><mo >,</mo><msup ><mi >L</mi><mrow ><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow><mo stretchy="false"  >↑</mo></mrow></msup><mo >;</mo><msub ><mi
    >θ</mi><mi >E</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><mrow ><msup ><mi  >h</mi><mi >i</mi></msup><mo
    >,</mo><msup ><mi >g</mi><mi >i</mi></msup></mrow><mo >=</mo><mrow ><mi >N</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi  >t</mi><mi >R</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo
    stretchy="false" >(</mo><msup ><mi >h</mi><mrow ><mrow ><mi >i</mi><mo >−</mo><mn
    >1</mn></mrow><mo stretchy="false"  >↑</mo></mrow></msup><mo >,</mo><msup ><mi
    >f</mi><mi >i</mi></msup><mo >;</mo><msub ><mi >θ</mi><mi >R</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="right" ><mrow ><msup ><mi >L</mi><mi >i</mi></msup><mo >=</mo><mrow
    ><mi >N</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi  >t</mi><mi >D</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false" >(</mo><msup ><mi >g</mi><mi >i</mi></msup><mo >;</mo><msub
    ><mi >θ</mi><mi >D</mi></msub><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑓</ci><ci  >𝑖</ci></apply><apply
    ><ci >𝑁</ci><ci  >𝑒</ci><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑡</ci><ci >𝐸</ci></apply><vector ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐵</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝐿</ci><apply ><ci >↑</ci><apply ><ci >𝑖</ci><cn type="integer" >1</cn></apply><csymbol
    cd="latexml" >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝐸</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >ℎ</ci><ci >𝑖</ci></apply></apply></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝑔</ci><ci >𝑖</ci></apply><apply ><ci >𝑁</ci><ci  >𝑒</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝑅</ci></apply><vector
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℎ</ci><apply ><ci
    >↑</ci><apply ><ci >𝑖</ci><cn type="integer" >1</cn></apply><csymbol cd="latexml"
    >absent</csymbol></apply></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑓</ci><ci >𝑖</ci></apply><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝜃</ci><ci >𝑅</ci></apply></vector><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝐿</ci><ci >𝑖</ci></apply></apply></apply><apply ><ci >𝑁</ci><ci  >𝑒</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝑡</ci><ci >𝐷</ci></apply><list
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑔</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝜃</ci><ci >𝐷</ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}f^{i}=Net_{E}(B^{i},L^{i-1\uparrow};\theta_{E})\\
    h^{i},g^{i}=Net_{R}(h^{i-1\uparrow},f^{i};\theta_{R})\\ L^{i}=Net_{D}(g^{i};\theta_{D})\\
    \end{split}</annotation></semantics></math> |  | (18) |
- en: $\theta_{E},\theta_{R},\theta_{D}$ are the weights of their respective modules.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta_{E},\theta_{R},\theta_{D}$ 是各自模块的权重。
- en: The encoder module consists of convolutional layers with residual connections.
    For the first scale, only the blurred image is used as input, for all the subsequent
    layers both the blurred image $B^{i}$ and the restored image of the previous scale
    $L^{i-1\uparrow}$ are concatenated and both are sent as input. The encoder module
    is used to extract features $f^{i}$, it gradually decreases the length and breadth
    but increases the number of channels.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器模块由带有残差连接的卷积层组成。对于第一个尺度，仅使用模糊图像作为输入，对于所有后续层，模糊图像 $B^{i}$ 和前一个尺度的恢复图像 $L^{i-1\uparrow}$
    都会连接在一起作为输入。编码器模块用于提取特征 $f^{i}$，它逐渐减少长度和宽度，但增加通道数量。
- en: The recurrent module can be a vanilla RNN, GRU or LSTM, in Tao et. al they used
    convolutional LSTM (ConvLSTM) which gave the best results. They also trained a
    network without any Recurrent module and it gave lower performance compared to
    the one which included a recurrent module. It takes as input the hidden features
    of the previous scale’s recurrent module $h^{i-1\uparrow}$ and the features generated
    by the current scales encoder $f^{i}$. The hidden features of the previous scale
    passes the intermediate results and blur patterns of the previous scale which
    benefits the current scale. Gradient clipping is used for this module only. It
    gives as output a modified set of features $g^{i}$ and the hidden features of
    the current scale $h^{i}$.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 递归模块可以是普通的 RNN、GRU 或 LSTM，在 Tao 等人的研究中，他们使用了卷积 LSTM（ConvLSTM），效果最佳。他们还训练了一个没有递归模块的网络，性能低于包含递归模块的网络。它以前一尺度递归模块的隐藏特征
    $h^{i-1\uparrow}$ 和当前尺度编码器生成的特征 $f^{i}$ 作为输入。前一尺度的隐藏特征传递中间结果和模糊模式，这对当前尺度有益。仅对该模块使用梯度裁剪。它输出一个修改后的特征集
    $g^{i}$ 和当前尺度的隐藏特征 $h^{i}$。
- en: 'The decoder module consists of a few convolutional layers with residual connections
    (same dimensions are maintained using padding) followed by a deconvolutional layer
    which increases the spatial dimensions and decreases the number of channels until
    the we get latent image for the scale $L^{i}$. $\uparrow$ operator (Eqn. [18](#S2.E18
    "In II-B1 Without Adversarial Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) is used to adapt
    the dimensions of features or images to that of the next scale. $\uparrow$ can
    be deconvolution, sub-pixel convolution, image resizing, bilinear interpolation,
    etc.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '解码器模块由一些卷积层和残差连接组成（通过填充保持相同的维度），接着是一个反卷积层，该层增加了空间维度并减少了通道数量，直到我们获得尺度$L^{i}$的潜在图像。$\uparrow$
    运算符（公式 [18](#S2.E18 "In II-B1 Without Adversarial Loss ‣ II-B End to End ‣ II
    Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")）用于调整特征或图像的维度到下一个尺度的维度。$\uparrow$
    可以是反卷积、子像素卷积、图像缩放、双线性插值等。'
- en: Combining all the three modules a single scale in the network can be represented
    as,
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有三个模块组合起来，网络中的单一尺度可以表示为，
- en: '|  | $L^{i},h^{i}=Net_{SR}(B^{i},L^{i-1\uparrow},h^{i-1\uparrow};\theta_{SR})$
    |  | (19) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{i},h^{i}=Net_{SR}(B^{i},L^{i-1\uparrow},h^{i-1\uparrow};\theta_{SR})$
    |  | (19) |'
- en: where $\theta_{SR}$ is the weight shared across all scales.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\theta_{SR}$ 是在所有尺度中共享的权重。
- en: Scaled Recurrent Network uses Euclidean Loss given below,
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放递归网络使用以下的欧几里得损失，
- en: '|  | $L=\sum_{i=1}^{n}\dfrac{\kappa_{i}}{N_{i}}\lVert L^{i}-I^{i}\rVert^{2}_{2}$
    |  | (20) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $L=\sum_{i=1}^{n}\dfrac{\kappa_{i}}{N_{i}}\lVert L^{i}-I^{i}\rVert^{2}_{2}$
    |  | (20) |'
- en: where $L^{i}$ is the latent restored image, $I^{i}$ is the ground truth sharp
    image. $\{\kappa_{i}\}$ are weights for each scale, and $N_{i}$ is the number
    of elements in $L^{i}$ to be normalized.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L^{i}$ 是潜在的恢复图像，$I^{i}$ 是真实的锐化图像。$\{\kappa_{i}\}$ 是每个尺度的权重，$N_{i}$ 是需要归一化的
    $L^{i}$ 中的元素数量。
- en: 'Noorozi et. al.[[5](#bib.bib5)] also uses a three pyramid stages chained together,
    each consisting of several convolutional and deconvolutional layers $(N_{1},N_{2},\text{
    and }N_{3})$ (Eqn. [21](#S2.E21 "In II-B1 Without Adversarial Loss ‣ II-B End
    to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes
    equal contribution")) which recreates a multiscale pyramid schemes which were
    previously used in the classical methods. The key idea of this pyramid structure
    is that the downsampled version of the blurred image has less amount of blur and
    it is easier for removal. Hence the goal of each respective network (or stage)
    is to mitigate the blur effect at that corresponding scale. It also helps break
    down the complex problem of deblurring into smaller units.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 'Noorozi 等人[[5](#bib.bib5)] 也使用了三个金字塔阶段串联在一起，每个阶段包含若干卷积层和反卷积层`(N_{1},N_{2},\text{
    和 }N_{3})`（公式 [21](#S2.E21 "In II-B1 Without Adversarial Loss ‣ II-B End to End
    ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")），这些金字塔阶段重建了先前在经典方法中使用的多尺度金字塔方案。这个金字塔结构的关键思想是，模糊图像的降采样版本模糊程度较轻，更容易去除。因此，每个相应的网络（或阶段）的目标是在对应的尺度上减轻模糊效果。这也有助于将复杂的去模糊问题分解成更小的单元。'
- en: '![Refer to caption](img/de378c90eab5299a5bd7854ce25f6d5f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/de378c90eab5299a5bd7854ce25f6d5f.png)'
- en: 'Figure 7: Architecture used by Noorozi et. al. [[5](#bib.bib5)]. Here the three
    CNNs starting from the left denotes $N_{1},N_{2},N_{3}$ respectively.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：Noorozi 等人使用的架构[[5](#bib.bib5)]。这里，左侧的三个卷积神经网络分别表示`N_{1}`、`N_{2}`和`N_{3}`。
- en: 'Firstly the blurred image is given as input to the first network $N_{1}$ (pure
    convolution) without any downsampling and it’s output is added with the downsampled
    version of the same blurred image by a factor of four. After this the first loss
    $L_{1}$ is calculated using [21](#S2.E21 "In II-B1 Without Adversarial Loss ‣
    II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution") by calculating the difference (or MSE) between the
    downsampled sharp image and the added sum of the network $N_{1}$’s output and
    downsampled blurred image. This same process is repeated for network $N_{2}$ and
    $N_{3}$ for calculating losses $L_{2}$ and $L_{3}$ but the downsampling factor
    are two and one (no downsampling) respectively. The these three computed losses
    are added resulting in final loss function for this model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '首先，将模糊图像作为输入传递给第一个网络`N_{1}`（纯卷积），不进行任何降采样，并将其输出与相同模糊图像的降采样版本（降采样因子为四）相加。之后，使用[21](#S2.E21
    "In II-B1 Without Adversarial Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")计算第一个损失`L_{1}`，通过计算降采样锐化图像与网络`N_{1}`输出与降采样模糊图像的和之间的差异（或均方误差）。这个过程对网络`N_{2}`和`N_{3}`也进行了重复，以计算损失`L_{2}`和`L_{3}`，但降采样因子分别为二和一（即不降采样）。这三种计算得到的损失被相加，得到该模型的最终损失函数。'
- en: '|  | <math   alttext="\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\'
- en: L_{3}=\sum_{(B,I)}&#124;N_{3}(N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B))+B-I&#124;^{2}\end{split}"
    display="block"><semantics ><mtable displaystyle="true" rowspacing="0pt" ><mtr
    ><mtd  columnalign="right" ><mrow ><msub  ><mi >L</mi><mn >1</mn></msub><mo rspace="0.111em"
    >=</mo><mrow ><munder ><mo movablelimits="false" rspace="0em" >∑</mo><mrow  ><mo
    stretchy="false"  >(</mo><mi >B</mi><mo >,</mo><mi >I</mi><mo stretchy="false"  >)</mo></mrow></munder><msup
    ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><mrow ><mrow ><msub ><mi  >N</mi><mn
    >1</mn></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >B</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><msub ><mi
    >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >B</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    >−</mo><mrow ><msub ><mi >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >I</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >&#124;</mo></mrow><mn
    >2</mn></msup></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow
    ><msub ><mi  >L</mi><mn >2</mn></msub><mo rspace="0.111em"  >=</mo><mrow ><munder
    ><mo movablelimits="false" rspace="0em" >∑</mo><mrow ><mo stretchy="false" >(</mo><mi
    >B</mi><mo >,</mo><mi >I</mi><mo stretchy="false" >)</mo></mrow></munder><msup
    ><mrow ><mo stretchy="false" >&#124;</mo><mrow ><mrow ><mrow ><msub ><mi  >N</mi><mn
    >2</mn></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><mrow ><msub ><mi >N</mi><mn >1</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi  >B</mi><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo><mrow ><msub ><mi >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo stretchy="false"  >)</mo></mrow></mrow><mo
    >+</mo><mrow ><msub ><mi >d</mi><mrow ><mn >1</mn><mo >/</mo><mn >2</mn></mrow></msub><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"  >(</mo><mi >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow></mrow><mo >−</mo><mrow ><msub ><mi >d</mi><mrow
    ><mn >1</mn><mo >/</mo><mn >2</mn></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><mrow
    ><mo stretchy="false"  >(</mo><mi >I</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >&#124;</mo></mrow><mn >2</mn></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="right"  ><mrow ><msub ><mi  >L</mi><mn >3</mn></msub><mo rspace="0.111em"  >=</mo><mrow
    ><munder ><mo movablelimits="false" rspace="0em"  >∑</mo><mrow ><mo stretchy="false"  >(</mo><mi
    >B</mi><mo >,</mo><mi >I</mi><mo stretchy="false"  >)</mo></mrow></munder><msup
    ><mrow ><mo stretchy="false"  >&#124;</mo><mrow ><mrow ><mrow ><msub ><mi >N</mi><mn
    >3</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"  >(</mo><mrow
    ><mrow ><msub ><mi >N</mi><mn >2</mn></msub><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow ><mrow ><msub ><mi  >N</mi><mn >1</mn></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><msub ><mi >d</mi><mrow
    ><mn >1</mn><mo >/</mo><mn >4</mn></mrow></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mrow ><msub ><mi >d</mi><mrow
    ><mn >1</mn><mo >/</mo><mn >2</mn></mrow></msub><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi  >B</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow><mo
    stretchy="false"  >)</mo></mrow></mrow><mo >+</mo><mi >B</mi></mrow><mo >−</mo><mi
    >I</mi></mrow><mo stretchy="false"  >&#124;</mo></mrow><mn >2</mn></msup></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐿</ci><cn type="integer" >1</cn></apply><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><interval closure="open" ><ci  >𝐵</ci><ci >𝐼</ci></interval></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply ><apply  ><apply
    ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑁</ci><cn type="integer"
    >1</cn></apply><ci >𝐵</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑑</ci><apply ><cn type="integer" >1</cn><cn type="integer" >4</cn></apply></apply><ci
    >𝐵</ci></apply></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑑</ci><apply ><cn type="integer" >1</cn><cn type="integer" >4</cn></apply></apply><ci
    >𝐼</ci></apply></apply></apply><cn type="integer" >2</cn></apply><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >𝐿</ci><cn type="integer" >2</cn></apply></apply></apply></apply><apply
    ><apply ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><interval closure="open"  ><ci
    >𝐵</ci><ci >𝐼</ci></interval></apply><apply ><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><apply  ><apply ><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑁</ci><cn type="integer" >2</cn></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑁</ci><cn type="integer" >1</cn></apply><ci >𝐵</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >4</cn></apply></apply><ci >𝐵</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply><ci >𝐵</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply><ci >𝐼</ci></apply></apply></apply><cn
    type="integer" >2</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐿</ci><cn type="integer" >3</cn></apply></apply></apply></apply><apply ><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><interval closure="open"  ><ci
    >𝐵</ci><ci >𝐼</ci></interval></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><apply ><apply  ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑁</ci><cn type="integer" >3</cn></apply><apply ><apply ><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑁</ci><cn type="integer"  >2</cn></apply><apply ><apply ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑁</ci><cn type="integer" >1</cn></apply><ci >𝐵</ci></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >4</cn></apply></apply><ci >𝐵</ci></apply></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑑</ci><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply><ci >𝐵</ci></apply></apply></apply><ci
    >𝐵</ci></apply><ci >𝐼</ci></apply></apply><cn type="integer" >2</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}L_{1}=\sum_{(B,I)}&#124;N_{1}(B)+d_{1/4}(B)-d_{1/4}(I)&#124;^{2}\\
    L_{2}=\sum_{(B,I)}&#124;N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B)-d_{1/2}(I)&#124;^{2}\\
    L_{3}=\sum_{(B,I)}&#124;N_{3}(N_{2}(N_{1}(B)+d_{1/4}(B))+d_{1/2}(B))+B-I&#124;^{2}\end{split}</annotation></semantics></math>
    |  | (21) |
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: The problem in this architecture is when there are extreme blurs,the network
    leaves the images untouched but it does not suffer from artifacts.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中的问题是当出现极端模糊时，网络会使图像保持不变，但不会出现伪影。
- en: Spatially variant blurs in dynamic scenes require a large receptive field, while
    CNN have a local knowledge (small receptive field) and spatially invariant weights,
    to remove this problem they have to use larger networks with more convolutional
    layer, leading to more parameters which is difficult to train. Hence, the challenge
    is to have a small architecture with a large receptive field, to this end Zhang
    et. al[[10](#bib.bib10)] proposes the use of Recurrent Neural Network as a deconvolutional
    operator which increases the receptive field (long range dependencies).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 动态场景中的空间变化模糊需要较大的感受野，而CNN具有局部知识（小感受野）和空间不变的权重。为了解决这个问题，CNN必须使用更大的网络和更多的卷积层，这导致更多的参数，训练起来困难。因此，挑战在于拥有一个小型架构而又具有大感受野。为此，张等人[[10](#bib.bib10)]提出了使用递归神经网络作为去卷积算子，以增加感受野（长距离依赖）。
- en: 'The network proposed by Zhang et. al.[[10](#bib.bib10)] uses three CNN and
    one RNN. The CNN are used for feature extraction, image reconstruction, and pixel-wise
    weight generation (for the RNN). While the RNN is used as a deconvolutional operator
    with a large receptive field (Fig. [8](#S2.F8 "Figure 8 ‣ II-B1 Without Adversarial
    Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")). Weight generation for the RNN is done by
    passing an image through a encoder-decoder architecture CNN. For both the decoder
    part of the weight generation module and the image restoration module they use
    bilinear interpolation (no deconvolution) to avoid checkerboard artifact [[11](#bib.bib11)]
    The RNN generates receptive fields in one direction (single dimension), hence
    they use a convolutional layer after every RNN to fuse the receptive fields together
    to get a two dimensional structure. Skip connections are added to avoid vanishing
    gradient problem and for faster training.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '张等人提出的网络[[10](#bib.bib10)]使用了三个CNN和一个RNN。CNN用于特征提取、图像重建和逐像素权重生成（用于RNN）。而RNN作为去卷积算子，具有较大的感受野（见图
    [8](#S2.F8 "Figure 8 ‣ II-B1 Without Adversarial Loss ‣ II-B End to End ‣ II Methods
    ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")）。RNN的权重生成通过图像通过编码-解码架构的CNN完成。对于权重生成模块的解码器部分和图像恢复模块，他们使用双线性插值（没有去卷积）以避免棋盘状伪影
    [[11](#bib.bib11)]。RNN在一个方向上生成感受野（单维），因此在每个RNN之后使用卷积层将感受野融合在一起，以获得二维结构。添加跳跃连接以避免梯度消失问题并加快训练速度。'
- en: '![Refer to caption](img/d52d0a2d620c339837c08921ac8205b1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d52d0a2d620c339837c08921ac8205b1.png)'
- en: 'Figure 8: Architecture used by Zhang et. al. [[10](#bib.bib10)]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：张等人使用的架构 [[10](#bib.bib10)]
- en: If $f$ is the feature extraction module, $rnn$ is the RNN module, $w$ is the
    weight generation module, and $r$ is the restoration module, then the network
    proposed by Zhang et. al.[[10](#bib.bib10)] can be summarized as,
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$f$是特征提取模块，$rnn$是RNN模块，$w$是权重生成模块，$r$是恢复模块，则张等人提出的网络[[10](#bib.bib10)]可以总结为，
- en: '|  | <math   alttext="\begin{split}F=f(B)\\ \theta=w(B)\\'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{split}F=f(B)\\ \theta=w(B)\\'
- en: F^{\prime}=rnn(F;\theta)\\
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: F^{\prime}=rnn(F;\theta)\\
- en: L=r(F^{\prime})\end{split}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt"  ><mtr ><mtd columnalign="right"  ><mrow ><mi >F</mi><mo  >=</mo><mrow
    ><mi >f</mi><mo lspace="0em" rspace="0em" >​</mo><mrow  ><mo stretchy="false"  >(</mo><mi
    >B</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mrow ><mi >θ</mi><mo  >=</mo><mrow ><mi >w</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow  ><mo stretchy="false"  >(</mo><mi >B</mi><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd columnalign="right"  ><mrow
    ><msup ><mi  >F</mi><mo >′</mo></msup><mo >=</mo><mrow ><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi  >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false" >(</mo><mi >F</mi><mo  >;</mo><mi
    >θ</mi><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    columnalign="right"  ><mrow ><mi >L</mi><mo  >=</mo><mrow ><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msup ><mi  >F</mi><mo
    >′</mo></msup><mo stretchy="false"  >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >𝐹</ci><apply ><ci  >𝑓</ci><ci
    >𝐵</ci><ci >𝜃</ci></apply></apply><apply ><apply  ><ci >𝑤</ci><ci >𝐵</ci><apply  ><csymbol
    cd="ambiguous"  >superscript</csymbol><ci >𝐹</ci><ci >′</ci></apply></apply></apply><apply
    ><apply  ><ci >𝑟</ci><ci >𝑛</ci><ci  >𝑛</ci><list ><ci >𝐹</ci><ci >𝜃</ci></list><ci
    >𝐿</ci></apply></apply><apply ><apply  ><ci >𝑟</ci><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >𝐹</ci><ci >′</ci></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}F=f(B)\\ \theta=w(B)\\ F^{\prime}=rnn(F;\theta)\\
    L=r(F^{\prime})\end{split}</annotation></semantics></math> |  | (22) |
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $L=r(F^{\prime})$
- en: where $B$ is the blurry image, $F$ is the extracted features, $\theta$ is the
    pixel-wise generated weights, $F^{\prime}$ are the modified features after passing
    through the RNN, and $L$ is the latent (predicted) deblurred image.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $B$ 是模糊图像，$F$ 是提取的特征，$\theta$ 是逐像素生成的权重，$F^{\prime}$ 是通过 RNN 处理后的修改特征，$L$
    是潜在的（预测的）去模糊图像。
- en: II-B2 With Adversarial Loss
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 使用对抗损失
- en: '![Refer to caption](img/90b4f0df280268f575fe334d2f970fb0.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/90b4f0df280268f575fe334d2f970fb0.png)'
- en: 'Figure 9: The basic structure of a GAN, where $G$ denotes the Generator and
    $D$ denotes the discriminator.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：GAN 的基本结构，其中 $G$ 表示生成器，$D$ 表示判别器。
- en: Blind Deblurring can also be solved end-to-end by generative models like Generative
    Adversarial Networks [[14](#bib.bib14)][[15](#bib.bib15)][[16](#bib.bib16)]. The
    approach Generative Adversarial Networks take is to have two different agents
    play a game against each other. One of the agents is a generator network which
    tries to generate data and the other is a discriminator network which examines
    data and checks whether it came from the real distribution (ground truth sharp
    image) or model distribution (restored blurred image). The goal of the generator
    is to fool the discriminator into believing that its output is from the real distribution.
    These generator and discriminator modules are neural networks whose parameters
    can be tuned by backpropagation and as both players get better at their job over
    time eventually the generator is forced to create data as realistic as possible.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 盲去模糊也可以通过生成对抗网络等生成模型进行端到端的解决[[14](#bib.bib14)][[15](#bib.bib15)][[16](#bib.bib16)]。生成对抗网络采用的方法是让两个不同的代理相互对抗。其中一个代理是生成网络，它尝试生成数据，另一个是鉴别网络，它检查数据并判断其是否来自真实分布（原始清晰图像）还是模型分布（恢复后的模糊图像）。生成器的目标是欺骗鉴别器，使其相信生成的数据来自真实分布。这些生成器和鉴别器模块是可以通过反向传播调整参数的神经网络，随着时间的推移，两者在各自任务上逐渐变得更加高效，最终生成器被迫创建尽可能真实的数据。
- en: '![Refer to caption](img/87deaf31d77889391a6730a67a788709.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87deaf31d77889391a6730a67a788709.png)'
- en: 'Figure 10: Multiscale architecture used by Nah. et. al [[4](#bib.bib4)]'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: Nah 等人使用的多尺度架构 [[4](#bib.bib4)]'
- en: 'Nah et. al.[[4](#bib.bib4)] also uses a Multiscale Convolutional Neural Network
    i.e coarse(low resolution) to fine(high resolution) structure. The blurred and
    sharp images are scaled down to form a Gaussian pyramid structure, this is done
    because convolution can only capture local information, hence lower resolution
    images are used to capture the long range dependencies, whereas the high resolution
    images are used to capture the fine grained details. Each of these scaled blurred
    images passes through a layer of multiple convolutional and residual blocks (residual
    blocks enable training in large networks without over fitting) to generate the
    corresponding latent image for that scale, then for each scale MSE(Mean Squared
    Error) with the sharp image is calculated and back propagation is done. The MSE
    for all the scales are averaged together to give the content loss as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Nah 等人[[4](#bib.bib4)] 也使用了多尺度卷积神经网络，即从粗到细（低分辨率到高分辨率）的结构。模糊图像和清晰图像被缩小以形成高斯金字塔结构，这样做是因为卷积只能捕捉局部信息，因此使用低分辨率图像来捕捉长距离依赖关系，而高分辨率图像则用于捕捉细粒度的细节。这些缩小的模糊图像经过多个卷积和残差块的层（残差块使得大网络训练时不容易过拟合），以生成该尺度对应的潜在图像，然后对于每个尺度计算与清晰图像的均方误差（MSE），并进行反向传播。所有尺度的MSE平均后给出内容损失，如下：
- en: '|  | $L_{content}=\dfrac{1}{2K}\sum_{k=1}^{K}\dfrac{1}{c_{k}h_{k}w_{k}}\lVert
    L_{k}-I_{k}\rVert^{2}$ |  | (23) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{content}=\dfrac{1}{2K}\sum_{k=1}^{K}\dfrac{1}{c_{k}h_{k}w_{k}}\lVert
    L_{k}-I_{k}\rVert^{2}$ |  | (23) |'
- en: Here $K$ is the total number of scales, $c_{k},h_{k},w_{k}$ are the channels,
    height and width of the $k^{th}$ scale, and $L_{k}$ and $I_{k}$ are the latent
    and sharp images of the $k^{th}$ scale respectively.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 $K$ 是尺度的总数，$c_{k},h_{k},w_{k}$ 分别是第 $k^{th}$ 尺度的通道、高度和宽度，$L_{k}$ 和 $I_{k}$
    分别是第 $k^{th}$ 尺度的潜在图像和清晰图像。
- en: 'The output of current scale is given as input to the next scale. The next scale
    is of a higher resolution, hence the latent image of the current scale is passed
    through a upconvolutional(transpose convolution) layer and is concatenated with
    the blurred image input of the next layer. Except for the last layer whose output
    latent image is the same size as the original image, hence does not need any upconvolution.
    This generated deblurred image of the last scale is given as input to a discriminator
    or some sharp image is given as input, and the discriminator tells weather the
    image given was originally sharp or was deblurred by the Multi-Scaled Network.
    Discriminator loss function (Adversarial Loss) [[14](#bib.bib14)] is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当前尺度的输出作为输入传递给下一个尺度。下一个尺度具有更高的分辨率，因此当前尺度的潜在图像通过上卷积（转置卷积）层进行传递，并与下一个层的模糊图像输入进行拼接。除了最后一层，其输出的潜在图像与原始图像大小相同，因此不需要任何上卷积。最后尺度生成的去模糊图像作为输入传递给鉴别器，或者输入一些清晰的图像，鉴别器会判断图像是否原本清晰还是被多尺度网络去模糊的。鉴别器损失函数（对抗损失）[[14](#bib.bib14)]如下：
- en: '|  | $L_{adv}=\mathbb{E}_{S\sim p_{sharp}}[\log(D(S))]+\mathbb{E}_{B\sim p_{blurred}}[1-\log(G(B))]$
    |  | (24) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{adv}=\mathbb{E}_{S\sim p_{sharp}}[\log(D(S))]+\mathbb{E}_{B\sim p_{blurred}}[1-\log(G(B))]$
    |  | (24) |'
- en: Here $D$ is the Discriminator i.e a CNN classifier and $G$ is the Generator
    i.e our Multi-Scaled CNN. Generator $G$ for each scale can be defined as,
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $D$ 是鉴别器，即一个 CNN 分类器，$G$ 是生成器，即我们的多尺度 CNN。每个尺度的生成器 $G$ 可以定义为，
- en: '|  | $L^{i}=G(B^{i},L^{i-1\uparrow};\theta_{i})$ |  | (25) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{i}=G(B^{i},L^{i-1\uparrow};\theta_{i})$ |  | (25) |'
- en: where $L^{i},B^{i}$ is the generated and blurred image for $i^{th}$ scale respectively.
    $L^{i-1\uparrow}$ is the generated image of the previous scale where $\uparrow$
    denotes the upconvolution function used to upscale the dimension of $L^{i-1}$
    to be same as $L^{i}$. $\theta_{i}$ are the weights of $i^{th}$ scale.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L^{i},B^{i}$ 分别是第 $i^{th}$ 尺度的生成图像和模糊图像。$L^{i-1\uparrow}$ 是前一个尺度的生成图像，其中
    $\uparrow$ 表示用于将 $L^{i-1}$ 的维度放大到与 $L^{i}$ 相同的上卷积函数。$\theta_{i}$ 是第 $i^{th}$ 尺度的权重。
- en: 'Combing both loss functions i.e Content and Adversarial Loss we get:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 结合两个损失函数，即内容损失和对抗损失，我们得到：
- en: '|  | $L_{total}=L_{content}+\lambda L_{adv}$ |  | (26) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{total}=L_{content}+\lambda L_{adv}$ |  | (26) |'
- en: Where $\lambda$ is a weight constant.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是一个权重常数。
- en: Ramamkrishnan et. al[[6](#bib.bib6)] also uses an adversarial way of training,
    but the generator uses a structure similar to DenseNet[[17](#bib.bib17)] with
    a global skip connection. Similar to Nah et. al.[[4](#bib.bib4)], here dimensions
    are maintained throughout the convolutional layers, so that no deconvolution module
    needs to be used, preventing checkerboard effect [[11](#bib.bib11)]. Using a densely
    connected CNN in generator reduces the vanishing gradient problem, strengthens
    feature propagation and reuse and reduces the number of parameters, all of which
    in turn allows us to use a smaller network with smoother training and faster inference
    time.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Ramamkrishnan 等[[6](#bib.bib6)] 也使用了一种对抗训练的方法，但生成器使用了类似于 DenseNet[[17](#bib.bib17)]
    的结构，并具有全局跳跃连接。与 Nah 等[[4](#bib.bib4)] 类似，这里在整个卷积层中保持了维度，因此不需要使用去卷积模块，防止了棋盘效应 [[11](#bib.bib11)]。使用密集连接的
    CNN 生成器减少了梯度消失问题，加强了特征传播和重用，减少了参数数量，从而允许我们使用更小的网络，实现更平滑的训练和更快的推理时间。
- en: The generator is divided into three parts, head, dense field and tail. Head
    creates sufficient activations for the dense field using convolutional layer.
    The dense field consists of several dense blocks, each dense block has a ReLU
    to add non linearity, a 1$\times$1 convolution to limit the number of activaitons
    (or channels), a convolutional layer (3 $\times$ 3), and batch normalizations.
    In DenseNet[[17](#bib.bib17)] $l^{th}$ layer of convolution is connected to the
    features of all the previous layers as opposed to the immediately previous layer
    (like in classic CNN). This dense connectivity is achieved in the generator by
    concatenating the output of $l^{th}$ layer with the output of $(l+1)^{th}$ layer,
    which in turn concatenates its output with the output of next layer i.e $(l+2)^{th}$
    layer and so on. All the convolutional layers in the dense filed use spatial and
    dilated convolution alternatively, this increases the receptive field of the network
    while still restraining the number of parameters to be learned. The rate of dilation
    increases till the middle layer and then decreases till the tail is reached. Tail
    adds non-linearity and uses $1\times 1$ convolution to reduce the number of activations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器分为三部分：头部、密集区域和尾部。头部通过卷积层为密集区域创造足够的激活。密集区域由几个密集块组成，每个密集块有一个 ReLU 添加非线性，一个
    $1\times1$ 卷积限制激活数（或通道数），一个卷积层（$3 \times 3$），以及批归一化。在 DenseNet[[17](#bib.bib17)]
    中，第 $l^{th}$ 层的卷积连接到所有先前层的特征，而不是立即前一层（如在经典 CNN 中）。这种密集连接在生成器中通过将第 $l^{th}$ 层的输出与第
    $(l+1)^{th}$ 层的输出连接实现，后者又将其输出与下一层即第 $(l+2)^{th}$ 层的输出连接，依此类推。密集区域中的所有卷积层交替使用空间卷积和膨胀卷积，这增加了网络的感受野，同时仍然限制了需要学习的参数数量。膨胀率在中间层增加，然后在达到尾部时减少。尾部添加非线性并使用
    $1\times 1$ 卷积减少激活数。
- en: They added the output of head to the output of tail to form a global skip connection,
    this allows gradients to flow back to the first layers of convolution which helps
    in learning (gradient updates) of the lower layers. Shorter connections between
    layers close to the output and layers close to the input, results in better accuracy
    and efficiency.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将头部的输出与尾部的输出相加，形成全局跳跃连接，这使得梯度能够流回卷积的第一层，有助于学习（梯度更新）较低层的内容。层之间的短连接，特别是靠近输出和输入的层之间，有助于提高准确性和效率。
- en: Similar to Nah et. al.[[4](#bib.bib4)] here the loss functions is also divided
    into two parts but with slight differences i.e instead of finding the MSE between
    predicted and sharp image (content loss), they find the MSE between the features
    (taken from end layers of a pretrained VGG16 network) of the predicted and sharp
    image. This is known as Perceptual Loss [[18](#bib.bib18)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Nah 等人[[4](#bib.bib4)] 类似，这里的损失函数也分为两部分，但略有不同，即不是寻找预测图像和清晰图像之间的 MSE（内容损失），而是寻找预测图像和清晰图像的特征（取自预训练
    VGG16 网络的末层）之间的 MSE。这被称为感知损失 [[18](#bib.bib18)]。
- en: '|  | $L_{precep}=\dfrac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}(\phi(I)_{x,y}-\phi(L)_{x,y})$
    |  | (27) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{precep}=\dfrac{1}{W\times H}\sum_{x=1}^{W}\sum_{y=1}^{H}(\phi(I)_{x,y}-\phi(L)_{x,y})$
    |  | (27) |'
- en: where $\phi$ denotes the function used to generate the features. $W,H$ are dimensions
    of the features. $L$ is the predicted latent image i.e $L=G(B)$ for generator
    $G$ and blurry image $B$.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 表示用于生成特征的函数。$W,H$ 是特征的维度。$L$ 是预测的潜在图像，即 $L=G(B)$，其中 $G$ 是生成器，$B$ 是模糊图像。
- en: Instead of simple adversarial loss they use a conditional adversarial loss [[15](#bib.bib15)]
    i.e with every sharp or predicted image, they also send the corresponding blurred
    image. Then calculate the probability of weather the image is deblurred or sharp
    given the blurred image.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用的不是简单的对抗损失，而是条件对抗损失 [[15](#bib.bib15)]，即对于每张清晰或预测的图像，他们也会发送相应的模糊图像。然后计算图像是否已去模糊或清晰的概率，给定模糊图像。
- en: '|  | $L_{adv_{con}}=-\mathbb{E}_{b\in B}[\log D(G(B)&#124;B)]$ |  | (28) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{adv_{con}}=-\mathbb{E}_{b\in B}[\log D(G(B)\mid B)]$ |  | (28) |'
- en: where $D$ is the discriminator.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 是判别器。
- en: While combining both the losses Zhang et. al[[10](#bib.bib10)] also adds a $L1$
    loss which was not present in Nah et. al[[4](#bib.bib4)],
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 张等人[[10](#bib.bib10)] 在结合这两种损失时，还添加了一种在 Nah 等人[[4](#bib.bib4)] 中未出现的 $L1$ 损失，
- en: '|  | $L_{total}=L_{precep}+K1\times L_{adv_{con}}+K2\times L_{L1}$ |  | (29)
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{total}=L_{precep}+K1\times L_{adv_{con}}+K2\times L_{L1}$ |  | (29)
    |'
- en: where $L_{L1}$ is the $L1$ loss. $K1,K2$ are the weight constant.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{L1}$ 是 $L1$ 损失。$K1,K2$ 是权重常数。
- en: Kupyn et. al.[[3](#bib.bib3)] uses a method also based on conditional GANs [[15](#bib.bib15)]
    similar to Ramakrishnan et. al.[[6](#bib.bib6)], the number of layer are significantly
    less compared to Nah et. al.[[4](#bib.bib4)], decreasing the number of trainable
    parameters and hence resulting in decrease training time and faster inference
    time. Instead of using the conventional loss function of GANs, they used the wasserstein
    (or called Earth-Mover) distance with gradient penalty [[16](#bib.bib16)] which
    has proved to show stability from vanila GANs[[14](#bib.bib14)] which suffer from
    mode collapse and vanishing gradients. The generator architecture is different
    from Ramakrishnan et. al.[[6](#bib.bib6)] as they use some convlutional layers
    followed by a series of residual blocks and finally some deconvolution layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Kupyn 等人[[3](#bib.bib3)] 采用的方法也基于条件 GANs [[15](#bib.bib15)]，类似于 Ramakrishnan
    等人[[6](#bib.bib6)]，但与 Nah 等人[[4](#bib.bib4)] 相比，层数显著减少，从而减少了可训练参数的数量，从而减少了训练时间和加快了推理速度。他们没有使用传统的
    GANs 损失函数，而是使用了 wasserstein（或称 Earth-Mover）距离与梯度惩罚 [[16](#bib.bib16)]，这已被证明在解决
    vanilla GANs[[14](#bib.bib14)] 遇到的模式崩溃和梯度消失问题上表现出稳定性。生成器架构不同于 Ramakrishnan 等人[[6](#bib.bib6)]，因为他们使用了一些卷积层，后跟一系列残差块，最后是一些反卷积层。
- en: 'The generator takes the blurred image as input and produces it’s sharp estimate.
    The discriminator then tries to model the differences in sharp images (real distribution)
    and restored image (model distribution) by the generator by computing the Wasserstein
    distance (Earth mover distance)[[16](#bib.bib16)]. The perceptual loss is same
    as in (Eqn. [27](#S2.E27 "In II-B2 With Adversarial Loss ‣ II-B End to End ‣ II
    Methods ‣ Blind Deblurring using Deep Learning: A Survey *denotes equal contribution")).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器以模糊图像作为输入，生成其锐化估计。然后，判别器通过计算 Wasserstein 距离（Earth mover distance）[[16](#bib.bib16)]
    尝试建模锐化图像（真实分布）和生成器恢复图像（模型分布）之间的差异。感知损失与 (Eqn. [27](#S2.E27 "在 II-B2 对抗性损失 ‣ II-B
    端到端 ‣ II 方法 ‣ 使用深度学习的盲去模糊：综述 *表示同等贡献")) 中的相同。
- en: 'The goal is to minimize this entire loss function (which is same as (Eqn. [29](#S2.E29
    "In II-B2 With Adversarial Loss ‣ II-B End to End ‣ II Methods ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) but without the $L1$
    loss) such that the generator is producing well enough restored image from the
    blurred image and the discriminator network to unable to distinguish the real
    sharp image (real data distribution) and restored image (model distribution) resulting
    in output of ½ probability by the discriminator most of the time. This is when
    the model is said to have converged.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是最小化整个损失函数（这与 (Eqn. [29](#S2.E29 "在 II-B2 对抗性损失 ‣ II-B 端到端 ‣ II 方法 ‣ 使用深度学习的盲去模糊：综述
    *表示同等贡献")) 中的相同，但没有 $L1$ 损失），以使生成器能够从模糊图像中生成足够好的恢复图像，并且判别网络无法区分真实的锐化图像（真实数据分布）和恢复图像（模型分布），使判别器大多数时间的输出为
    ½ 概率。这时，模型被认为已经收敛。
- en: III Performance Evaluation
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 性能评估
- en: III-A Metric
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 衡量标准
- en: 'The metrics used to measure similarity between the restored image and the blurred
    image are Peak Signal to Noise Ratio (PSNR) and Structural Similarity(SSIM). We
    also compare time taken by different architectures to deblur a blurry image after
    they are trained (inference time) (Table [III](#S3.T3 "TABLE III ‣ III-C GoPro
    Dataset ‣ III Performance Evaluation ‣ Blind Deblurring using Deep Learning: A
    Survey *denotes equal contribution")).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量恢复图像和模糊图像之间相似性的度量是峰值信噪比（PSNR）和结构相似性（SSIM）。我们还比较了不同架构在训练后去模糊模糊图像所需的时间（推理时间）（表
    [III](#S3.T3 "TABLE III ‣ III-C GoPro 数据集 ‣ III 性能评估 ‣ 使用深度学习的盲去模糊：综述 *表示同等贡献"))。
- en: PSNR can be thought of as the reciprocal of MSE (Mean Squared Error). MSE can
    be calculated as,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PSNR 可以被视为 MSE（均方误差）的倒数。MSE 可以计算为，
- en: '|  | $MSE=\dfrac{\sum_{P,Q}(I-L)^{2}}{P\times Q}$ |  | (30) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | $MSE=\dfrac{\sum_{P,Q}(I-L)^{2}}{P\times Q}$ |  | (30) |'
- en: where $P,Q$ are the dimensions of the image. $I$ and $L$ are the sharp and deblurred
    image respectively. Given MSE, PSNR can be calculated using,
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P,Q$ 是图像的维度。 $I$ 和 $L$ 分别是锐化和去模糊图像。给定 MSE，PSNR 可以使用以下公式计算，
- en: '|  | $PSNR=\dfrac{m^{2}}{MSE}$ |  | (31) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $PSNR=\dfrac{m^{2}}{MSE}$ |  | (31) |'
- en: where $m$ is the maximum possible intensity value, since we are using 8-bit
    integer to represent a pixel in channel, m = 255.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 是最大可能的强度值，因为我们使用 8 位整数来表示通道中的像素，m = 255。
- en: SSIM helps us to find the structural similarity between two image, it can be
    calculated using,
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM 帮助我们找出两幅图像之间的结构相似性，可以使用以下公式计算，
- en: '|  | $SSIM(x,y)=\dfrac{(2\mu_{x}\mu_{y}+c_{1})(2\sigma_{xy}+c_{2})}{(\mu_{x}^{2}+\mu_{y}^{2}+c_{1})(\sigma_{x}^{2}+\sigma_{y}^{2}+c_{2})}$
    |  | (32) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $SSIM(x,y)=\dfrac{(2\mu_{x}\mu_{y}+c_{1})(2\sigma_{xy}+c_{2})}{(\mu_{x}^{2}+\mu_{y}^{2}+c_{1})(\sigma_{x}^{2}+\sigma_{y}^{2}+c_{2})}$
    |  | (32) |'
- en: where $x,y$ are windows of equal dimension for $B,I$ respectively. $\mu_{x},\mu_{y}$
    denotes mean of $x,y$ respectively. $\sigma_{x},\sigma_{y}$ denotes variance for
    $x,y$ respectively, whereas $\sigma_{xy}$ is the covariance between $x$ and $y$.
    $c_{1}$ and $c_{2}$ are constants used to stabilize the division.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x,y$ 分别是 $B,I$ 的相等尺寸窗口。 $\mu_{x},\mu_{y}$ 分别表示 $x,y$ 的均值。 $\sigma_{x},\sigma_{y}$
    分别表示 $x,y$ 的方差，而 $\sigma_{xy}$ 是 $x$ 和 $y$ 之间的协方差。 $c_{1}$ 和 $c_{2}$ 是用于稳定除法的常数。
- en: III-B Köhler Dataset
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B Köhler 数据集
- en: 'Köhler Dataset [[19](#bib.bib19)] consists of 4 images which are blurred using
    12 different blur kernels giving us a total of 48 blurred images. To generate
    the blurred kernels, 6D camera motion is recored and then replayed using a robot,
    for each image. While replying, the 6D motion is approximated into a 3D motion
    by considering translation in one plane, and rotation on the plane perpendicular
    to it. This helps us to approximate actual camera shakes that occur in real life.
    For more details refer to [[19](#bib.bib19)]. The PSNR and SSIM for different
    deblurring architecture in Köhler dataset is shown in (Table [I](#S3.T1 "TABLE
    I ‣ III-B Köhler Dataset ‣ III Performance Evaluation ‣ Blind Deblurring using
    Deep Learning: A Survey *denotes equal contribution")).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 'Köhler 数据集 [[19](#bib.bib19)] 包含 4 张图像，这些图像使用 12 种不同的模糊核模糊处理，共生成了 48 张模糊图像。为了生成模糊核，记录了
    6D 相机运动，然后通过机器人重放每张图像。在重放过程中，将 6D 运动近似为 3D 运动，通过考虑平面上的平移和垂直于该平面的旋转。这有助于我们近似实际生活中的相机抖动。更多细节请参阅
    [[19](#bib.bib19)]。Köhler 数据集中不同去模糊架构的 PSNR 和 SSIM 如 (表 [I](#S3.T1 "TABLE I ‣
    III-B Köhler Dataset ‣ III Performance Evaluation ‣ Blind Deblurring using Deep
    Learning: A Survey *denotes equal contribution")) 所示。'
- en: 'TABLE I: Köhler Dataset'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: Köhler 数据集'
- en: '| Methods | PSNR | SSIM |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PSNR | SSIM |'
- en: '| --- | --- | --- |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kupyn et. al. | 26.10 | 0.816 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Kupyn 等人 | 26.10 | 0.816 |'
- en: '| Tao et. al. | 26.80 | 0.838 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Tao 等人 | 26.80 | 0.838 |'
- en: '| Nah et. al. | 26.48 | 0.812 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Nah 等人 | 26.48 | 0.812 |'
- en: '| Gong et. al. | 26.59 | 0.742 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Gong 等人 | 26.59 | 0.742 |'
- en: '| Ramakrishnan et. al. | 27.08 | 0.751 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Ramakrishnan 等人 | 27.08 | 0.751 |'
- en: '| Sun et. al. | 25.22 | 0.774 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Sun 等人 | 25.22 | 0.774 |'
- en: III-C GoPro Dataset
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C GoPro 数据集
- en: 'Here, a high resolution (1280 $\times$ 720), high frame rate (240 frames per
    second) camera (GoPro Hero5 Black) is used to capture video outdoors. To generate
    blurred image an average of a few frames (odd number picked randomly from 7 to
    23) is taken, while the central frame is considered as the corresponding sharp
    image. To reduce the magnitude of relative motion across frames they are down
    sampled and to avoid artifacts caused by averaging we only consider frames were
    the optical flow is at most 1. The PSNR and SSIM for different deblurring architecture
    in GoPro dataset is shown in (Table [II](#S3.T2 "TABLE II ‣ III-C GoPro Dataset
    ‣ III Performance Evaluation ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '这里使用高分辨率 (1280 $\times$ 720)、高帧率 (240 帧每秒) 的相机 (GoPro Hero5 Black) 在户外拍摄视频。为了生成模糊图像，取几个帧的平均值
    (从 7 到 23 中随机挑选的奇数)，而中央帧被视为对应的清晰图像。为了减少帧之间的相对运动幅度，它们被下采样，并且为了避免平均造成的伪影，我们只考虑光流最大为
    1 的帧。GoPro 数据集中不同去模糊架构的 PSNR 和 SSIM 如 (表 [II](#S3.T2 "TABLE II ‣ III-C GoPro Dataset
    ‣ III Performance Evaluation ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) 所示。'
- en: 'TABLE II: GoPro Dataset'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: GoPro 数据集'
- en: '| Methods | PSNR | SSIM |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PSNR | SSIM |'
- en: '| --- | --- | --- |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Kupyn et. al. | 28.7 | 0.958 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Kupyn 等人 | 28.7 | 0.958 |'
- en: '| Zhang et. al. | 29.2 | 0.931 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 | 29.2 | 0.931 |'
- en: '| Tao et. al. | 30.1 | 0.932 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Tao 等人 | 30.1 | 0.932 |'
- en: '| Nah et. al. | 29.2 | 0.916 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Nah 等人 | 29.2 | 0.916 |'
- en: '| Gong et. al. | 26.1 | 0.863 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Gong 等人 | 26.1 | 0.863 |'
- en: '| Noorozi et. al. | 28.1 | - |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| Noorozi 等人 | 28.1 | - |'
- en: '| Sun et. al. | 24.6 | 0.842 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Sun 等人 | 24.6 | 0.842 |'
- en: 'TABLE III: Inference Time'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 推断时间'
- en: '| Methods | Time(sec) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 时间 (秒) |'
- en: '| --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Kupyn et. al. | 0.8 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Kupyn 等人 | 0.8 |'
- en: '| Tao et. al. | 1.6 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Tao 等人 | 1.6 |'
- en: '| Nah et. al. | 4.3 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Nah 等人 | 4.3 |'
- en: '| Zhang et. al. | 1.4 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Zhang 等人 | 1.4 |'
- en: IV Conclusion
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 结论
- en: We observe that end-to-end methods ([[3](#bib.bib3)], [[4](#bib.bib4)], [[6](#bib.bib6)],
    [[5](#bib.bib5)], [[9](#bib.bib9)], [[10](#bib.bib10)]) have higher PSNR and SSIM
    compared to methods that estimate the blur kernel ([[2](#bib.bib2)], [[8](#bib.bib8)]),
    this is because an error in kernel estimation can lead to various artifacts in
    image, degrading the restoration.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，端到端的方法 ([[3](#bib.bib3)], [[4](#bib.bib4)], [[6](#bib.bib6)], [[5](#bib.bib5)],
    [[9](#bib.bib9)], [[10](#bib.bib10)]) 比估计模糊核的方法 ([[2](#bib.bib2)], [[8](#bib.bib8)])
    的 PSNR 和 SSIM 更高，因为核估计中的错误可能导致图像中的各种伪影，降低了修复效果。
- en: We also observed that most of the methods tried to increase their receptive
    field, which allowed long range spatial dependencies, essential for non uniform
    blurs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到，大多数方法尝试扩大接收场，这允许长距离空间依赖，对非均匀模糊至关重要。
- en: 'Another motivation was to reduce the size of network and the number of parameters,
    resulting in faster inference, as it can be clearly seen in (Table [III](#S3.T3
    "TABLE III ‣ III-C GoPro Dataset ‣ III Performance Evaluation ‣ Blind Deblurring
    using Deep Learning: A Survey *denotes equal contribution")) that Nah et. al.[[4](#bib.bib4)]
    which has a large network size is slower compared to other networks.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个动机是减少网络的大小和参数数量，从而加快推理速度，正如在 (表 [III](#S3.T3 "TABLE III ‣ III-C GoPro Dataset
    ‣ III Performance Evaluation ‣ Blind Deblurring using Deep Learning: A Survey
    *denotes equal contribution")) 中可以清楚地看到，Nah 等人[[4](#bib.bib4)] 的网络规模较大，相较于其他网络速度较慢。'
- en: Decreasing the size of network, while maintaining a large receptive field is
    one of the biggest challenge in learning based deblurring methods.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 减少网络大小，同时保持大接收域是基于学习的去模糊方法中的一大挑战。
- en: References
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] A. Chakrabarti, “A neural approach to blind motion deblurring,” *CoRR*,
    vol. abs/1603.04771, 2016\. [Online]. Available: [http://arxiv.org/abs/1603.04771](http://arxiv.org/abs/1603.04771)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] A. Chakrabarti, “一种盲运动去模糊的神经方法,” *CoRR*, 卷 abs/1603.04771, 2016\. [在线].
    可用: [http://arxiv.org/abs/1603.04771](http://arxiv.org/abs/1603.04771)'
- en: '[2] D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel,
    and Q. Shi, “From motion blur to motion flow: A deep learning solution for removing
    heterogeneous motion blur,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, July 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Gong, J. Yang, L. Liu, Y. Zhang, I. Reid, C. Shen, A. van den Hengel,
    和 Q. Shi, “从运动模糊到运动流：一种去除异质运动模糊的深度学习解决方案,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）*，2017年7月。'
- en: '[3] O. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, and J. Matas, “Deblurgan:
    Blind motion deblurring using conditional adversarial networks,” in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2018.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] O. Kupyn, V. Budzan, M. Mykhailych, D. Mishkin, 和 J. Matas, “Deblurgan:
    使用条件对抗网络进行盲运动去模糊,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）*，2018年6月。'
- en: '[4] S. Nah, T. Hyun Kim, and K. Mu Lee, “Deep multi-scale convolutional neural
    network for dynamic scene deblurring,” in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, July 2017.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Nah, T. Hyun Kim, 和 K. Mu Lee, “用于动态场景去模糊的深度多尺度卷积神经网络,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）*，2017年7月。'
- en: '[5] M. Noroozi, P. Chandramouli, and P. Favaro, “Motion deblurring in the wild,”
    *CoRR*, vol. abs/1701.01486, 2017\. [Online]. Available: [http://arxiv.org/abs/1701.01486](http://arxiv.org/abs/1701.01486)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Noroozi, P. Chandramouli, 和 P. Favaro, “野外运动去模糊,” *CoRR*, 卷 abs/1701.01486,
    2017\. [在线]. 可用: [http://arxiv.org/abs/1701.01486](http://arxiv.org/abs/1701.01486)'
- en: '[6] S. Ramakrishnan, S. Pachori, A. Gangopadhyay, and S. Raman, “Deep generative
    filter for motion deblurring,” *CoRR*, vol. abs/1709.03481, 2017. [Online]. Available:
    [http://arxiv.org/abs/1709.03481](http://arxiv.org/abs/1709.03481)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Ramakrishnan, S. Pachori, A. Gangopadhyay, 和 S. Raman, “用于运动去模糊的深度生成滤波器,”
    *CoRR*, 卷 abs/1709.03481, 2017. [在线]. 可用: [http://arxiv.org/abs/1709.03481](http://arxiv.org/abs/1709.03481)'
- en: '[7] C. J. Schuler, M. Hirsch, S. Harmeling, and B. Schölkopf, “Learning to
    deblur,” *CoRR*, vol. abs/1406.7444, 2014\. [Online]. Available: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. J. Schuler, M. Hirsch, S. Harmeling, 和 B. Schölkopf, “学习去模糊,” *CoRR*,
    卷 abs/1406.7444, 2014\. [在线]. 可用: [http://arxiv.org/abs/1406.7444](http://arxiv.org/abs/1406.7444)'
- en: '[8] J. Sun, W. Cao, Z. Xu, and J. Ponce, “Learning a convolutional neural network
    for non-uniform motion blur removal,” in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, June 2015.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Sun, W. Cao, Z. Xu, 和 J. Ponce, “学习卷积神经网络以去除非均匀运动模糊,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）*，2015年6月。'
- en: '[9] X. Tao, H. Gao, X. Shen, J. Wang, and J. Jia, “Scale-recurrent network
    for deep image deblurring,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] X. Tao, H. Gao, X. Shen, J. Wang, 和 J. Jia, “用于深度图像去模糊的尺度递归网络,” 发表在 *IEEE计算机视觉与模式识别会议（CVPR）*，2018年6月。'
- en: '[10] J. Zhang, J. Pan, J. Ren, Y. Song, L. Bao, R. W. Lau, and M.-H. Yang,
    “Dynamic scene deblurring using spatially variant recurrent neural networks,”
    in *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. 张, J. 潘, J. 任, Y. 宋, L. 宝, R. W. 劳, 和 M.-H. 杨, “利用空间变异递归神经网络进行动态场景去模糊,”
    发表在 *IEEE计算机视觉与模式识别会议（CVPR）*，2018年6月。'
- en: '[11] A. Odena, V. Dumoulin, and C. Olah, “Deconvolution and checkerboard artifacts,”
    *Distill*, 2016\. [Online]. Available: [http://distill.pub/2016/deconv-checkerboard](http://distill.pub/2016/deconv-checkerboard)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Odena, V. Dumoulin, 和 C. Olah, “反卷积和棋盘格伪影,” *Distill*, 2016\. [在线].
    可用: [http://distill.pub/2016/deconv-checkerboard](http://distill.pub/2016/deconv-checkerboard)'
- en: '[12] U. Schmidt, C. Rother, S. Nowozin, J. Jancsary, and S. Roth, “Discriminative
    non-blind deblurring,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2013.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] U. Schmidt, C. Rother, S. Nowozin, J. Jancsary 和 S. Roth，“判别式非盲去模糊”，发表于
    *IEEE 计算机视觉与模式识别会议 (CVPR)*，2013 年 6 月。'
- en: '[13] C. J. Schuler, H. Christopher Burger, S. Harmeling, and B. Scholkopf,
    “A machine learning approach for non-blind image deconvolution,” in *The IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2013.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. J. Schuler, H. Christopher Burger, S. Harmeling 和 B. Scholkopf，“一种用于非盲图像解卷积的机器学习方法”，发表于
    *IEEE 计算机视觉与模式识别会议 (CVPR)*，2013 年 6 月。'
- en: '[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems 27*, Z. Ghahramani, M. Welling, C. Cortes, N. D.
    Lawrence, and K. Q. Weinberger, Eds.   Curran Associates, Inc., 2014, pp. 2672–2680.
    [Online]. Available: [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio，“生成对抗网络”，发表于 *神经信息处理系统进展 27*，Z. Ghahramani, M.
    Welling, C. Cortes, N. D. Lawrence 和 K. Q. Weinberger 编，Curran Associates, Inc.，2014
    年，第 2672–2680 页。[在线]. 可用： [http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)'
- en: '[15] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation
    with conditional adversarial networks,” in *The IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)*, July 2017.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] P. Isola, J.-Y. Zhu, T. Zhou 和 A. A. Efros，“使用条件对抗网络的图像到图像翻译”，发表于 *IEEE
    计算机视觉与模式识别会议 (CVPR)*，2017 年 7 月。'
- en: '[16] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” in *Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017*, 2017, pp. 214–223\. [Online].
    Available: [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Arjovsky, S. Chintala 和 L. Bottou，“Wasserstein 生成对抗网络”，发表于 *第34届国际机器学习会议（ICML
    2017），悉尼，新南威尔士，澳大利亚，2017 年 8 月 6-11 日*，2017 年，第 214–223 页。[在线]. 可用： [http://proceedings.mlr.press/v70/arjovsky17a.html](http://proceedings.mlr.press/v70/arjovsky17a.html)'
- en: '[17] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *The IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, July 2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] G. Huang, Z. Liu, L. van der Maaten 和 K. Q. Weinberger，“密集连接卷积网络”，发表于
    *IEEE 计算机视觉与模式识别会议 (CVPR)*，2017 年 7 月。'
- en: '[18] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
    style transfer and super-resolution,” in *European Conference on Computer Vision*,
    2016.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Johnson, A. Alahi 和 L. Fei-Fei，“用于实时风格迁移和超分辨率的感知损失”，发表于 *欧洲计算机视觉会议*，2016
    年。'
- en: '[19] M. B. S. B. H. S. Köhler R., Hirsch M., “Recording and playback of camera
    shake: Benchmarking blind deconvolution with a real-world database,” in *European
    Conference on Computer Vision (ECCV)*, 2012.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] M. B. S. B. H. S. Köhler R.，Hirsch M.，“相机抖动的记录与播放：使用真实世界数据库进行盲解卷积基准测试”，发表于
    *欧洲计算机视觉会议 (ECCV)*，2012 年。'
