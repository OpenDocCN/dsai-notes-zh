- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1808.07256] A Survey of Modern Object Detection Literature using Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1808.07256] 现代目标检测文献综述：深度学习方法'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1808.07256](https://ar5iv.labs.arxiv.org/html/1808.07256)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1808.07256](https://ar5iv.labs.arxiv.org/html/1808.07256)
- en: A Survey of Modern Object Detection Literature using Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现代目标检测文献综述：深度学习方法
- en: Karanbir Chahal¹ and Kuntal Dey² *This work was not supported by any organization¹
    Karanbir is an independent researcher ² Kuntal Dey is a Member of IBM Academy
    of Technology (AoT)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Karanbir Chahal¹ 和 Kuntal Dey² *本研究未获得任何组织的支持¹ Karanbir 是一名独立研究员 ² Kuntal Dey
    是 IBM 技术学院 (AoT) 的成员*
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Object detection is the identification of an object in the image along with
    its localization and classification. It has wide spread applications and is a
    critical component for vision based software systems. This paper seeks to perform
    a rigorous survey of modern object detection algorithms that use deep learning.
    As part of the survey, the topics explored include various algorithms, quality
    metrics, speed/size trade offs and training methodologies. This paper focuses
    on the two types of object detection algorithms- the SSD class of single step
    detectors and the Faster R-CNN class of two step detectors. Techniques to construct
    detectors that are portable and fast on low powered devices are also addressed
    by exploring new lightweight convolutional base architectures. Ultimately, a rigorous
    review of the strengths and weaknesses of each detector leads us to the present
    state of the art.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是识别图像中的物体及其定位和分类。它有广泛的应用，是基于视觉的软件系统的关键组件。本文旨在对使用深度学习的现代目标检测算法进行严格的综述。作为综述的一部分，探索的主题包括各种算法、质量指标、速度/大小权衡和训练方法。本文重点关注两类目标检测算法——SSD
    类单步检测器和 Faster R-CNN 类两步检测器。通过探索新的轻量级卷积基础架构，本文还解决了在低功耗设备上构建便携且快速检测器的技术。最终，对每种检测器的优缺点进行严格审查，得出了目前的最新技术状态。
- en: I Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: I-A Background and Motivation
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 背景和动机
- en: Deep Learning has revolutionized the computing landscape leading to a fundamental
    change in how applications are being created. Andrej Karpathy, rightfully coined
    it as Software 2.0\. Applications are fast becoming intelligent and capable of
    performing complex tasks- tasks that were initially thought of being out of reach
    for a computer. Examples of these complex tasks include detecting and classifying
    objects in an image, summarizing large amounts of text, answering questions from
    a passage, generating art and defeating human players at complex games like Go
    and Chess. The human brain processes large amounts of data of varying patterns.
    It identifies these patterns, reasons about them and takes some action specific
    to that pattern. Artificial Intelligence aims to replicate this approach through
    Deep Learning. Deep Learning has proven to have been quite instrumental in understanding
    data of varying patterns at an accurate rate. This capability is responsible for
    most of the innovations in understanding language and images. With Deep Learning
    research moving forward at a fast pace, new discoveries and algorithms have led
    to disruption of numerous fields. One such field that has been affected by Deep
    Learning in a substantial way is object detection.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经彻底改变了计算领域，从而根本性地改变了应用程序的创建方式。Andrej Karpathy 正确地将其称为软件 2.0\. 应用程序正在快速变得智能，能够执行复杂的任务——这些任务最初被认为超出了计算机的能力。这些复杂任务的例子包括在图像中检测和分类物体、总结大量文本、回答段落中的问题、生成艺术作品以及在复杂的游戏如围棋和国际象棋中战胜人类玩家。人脑处理大量不同模式的数据。它识别这些模式，对其进行推理，并采取特定于该模式的行动。人工智能旨在通过深度学习复制这种方法。深度学习在准确率上对不同模式的数据理解方面被证明非常有效。这一能力是理解语言和图像的大多数创新的关键。随着深度学习研究的快速推进，新发现和算法导致了众多领域的颠覆。其中一个受到深度学习显著影响的领域是目标检测。
- en: I-B Object Detection
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 目标检测
- en: Object detection is the identification of an object in an image along with its
    localization and classification. Software systems that can perform these tasks
    are called object detectors. Object Detection has important applications. Numerous
    tasks which require human supervision can be automated with a software system
    that can detect objects in images. These include surveillance, disease identification
    and driving. The advent of deep learning has brought a profound change in how
    we implement computer vision nowadays. [[22](#bib.bib22)]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测是在图像中识别对象及其定位和分类的过程。能够执行这些任务的软件系统称为物体检测器。物体检测具有重要的应用。许多需要人工监督的任务可以通过能够在图像中检测对象的软件系统实现自动化。这些任务包括监控、疾病识别和驾驶。深度学习的出现对我们实施计算机视觉的方式带来了深远的变化。
    [[22](#bib.bib22)]
- en: Unfortunately, this technology has a high potential for irresponsible use. Military
    applications of object detectors are particularly worrying. Hence, in spite of
    its considerable useful applications, caution and responsible usage should always
    be kept in mind.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这项技术有着高度的不负责任使用的潜力。特别是物体检测器在军事应用中的使用令人担忧。因此，尽管其有着相当有用的应用，仍需时刻保持谨慎和负责任的使用。
- en: I-C Progress and Future Work
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-C 进展与未来工作
- en: Object Detectors have been making fast strides in accuracy, speed and memory
    footprint. The field has come a long way since 2015, when the first viable deep
    learning based object detector was introduced. The earliest deep learning object
    detector took $47s$ to process an image, now it takes less than $30ms$ which is
    better than real time. Similar to speed, accuracy has also steadily improved.
    From a detection accuracy of $29$ mAP (mixed average precision), modern object
    detectors have achieved $43$ mAP. Object detectors have also improved upon their
    size. Detectors can run well on low powered phones, thanks to the intelligent
    and conservative design of the models. Support for running models on phones has
    improved thanks to frameworks like Tensorflow[[34](#bib.bib34)] and Caffe[[35](#bib.bib35)]
    among others. A decent argument can be made that object detectors have achieved
    close to human parity. Conversely, like any deep learning model, these detectors
    are still open to adversarial attacks and can misclassify objects if the image
    is adversarial in nature. Work is being done to make object detectors and deep
    learning models in general more robust to these attacks. Accuracy, speed and size
    will constantly be improved upon, but that is no longer the most pressing goal.
    Detectors have attained a respectable quality, allowing them to be put into production
    today. The goal now should be to make these models robust against hacks and ensure
    that this technology is being used responsibly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测器在准确性、速度和内存占用方面取得了快速进展。自2015年首次引入基于深度学习的物体检测器以来，该领域已取得长足的进步。最早的深度学习物体检测器处理一张图像需要$47s$，而现在处理时间不到$30ms$，已经优于实时。与速度类似，准确性也在稳步提高。从$29$
    mAP（混合平均精度）的检测准确率，现代物体检测器已达到$43$ mAP。物体检测器在体积上也有所改进。得益于智能而保守的模型设计，检测器可以在低功耗手机上良好运行。由于Tensorflow[[34](#bib.bib34)]和Caffe[[35](#bib.bib35)]等框架的支持，模型在手机上的运行得到了改进。可以合理地认为，物体检测器已经接近人类水平。然而，与任何深度学习模型一样，这些检测器仍然容易受到对抗性攻击，如果图像具有对抗性质，则可能会误分类。正在努力使物体检测器和深度学习模型在这些攻击面前更为鲁棒。准确性、速度和大小将不断改进，但这已不再是最紧迫的目标。检测器已达到令人尊敬的质量，使其能够在今天投入生产。现在的目标应该是使这些模型在对抗攻击面前更为鲁棒，并确保这项技术得到负责任的使用。
- en: II Object Detection Models
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 物体检测模型
- en: II-A Early Work
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 早期工作
- en: The first object detector came out in 2001 and was called the Viola Jones Object
    Detector [[7](#bib.bib7)]. Although, it was technically classified as an object
    detector, it’s primary use case was for facial detection. It provided a real time
    solution and was adopted by many computer vision libraries at the time. The field
    was substantially accelerated with the advent of Deep Learning. The first Deep
    Learning object detector model was called the Overfeat Network [[13](#bib.bib13)]
    which used Convolutional Neural Networks (CNNs) along with a sliding window approach.
    It classified each part of the image as an object/non object and subsequently
    combined the results to generate the final set of predictions. This method of
    using CNNs to solve detection led to new networks being introduced which pushed
    the state of the art even further. We shall explore these networks in the next
    section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一款物体检测器于2001年问世，称为 Viola-Jones 物体检测器 [[7](#bib.bib7)]。尽管它在技术上被归类为物体检测器，但其主要用途是面部检测。它提供了实时解决方案，并被当时的许多计算机视觉库所采用。随着深度学习的出现，该领域得到了实质性的加速。第一个深度学习物体检测器模型称为
    Overfeat 网络 [[13](#bib.bib13)]，它使用了卷积神经网络（CNNs）以及滑动窗口方法。它将图像的每一部分分类为物体/非物体，并随后将结果结合生成最终的预测集。这种使用
    CNNs 进行检测的方法引入了新的网络，使技术水平更上一层楼。我们将在下一节中探讨这些网络。
- en: II-B Recent Work
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 最近的工作
- en: There are currently two methods of constructing object detectors- the single
    step approach and the two step approach. The two step approach has achieved a
    better accuracy than the former whereas the single step approach has been faster
    and shown higher memory efficiency. The single step approach classifies objects
    in images along with their locations in a single step. The two step approach on
    the other hand divides this process into two steps. The first step generates a
    set of regions in the image that have a high probability of being an object. The
    second step then performs the final detection and classification of objects by
    taking these regions as input. These two steps are named the Region Proposal Step
    and the Object Detection Step respectively. Alternatively, the single step approach
    combines these two steps to directly predict the class probabilities and object
    locations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有两种构建物体检测器的方法——单步方法和两步方法。两步方法的准确率优于前者，而单步方法则更快且显示出更高的内存效率。单步方法在一步中同时分类图像中的物体及其位置。另一方面，两步方法将这个过程分为两步。第一步生成图像中具有高概率为物体的区域。第二步则通过将这些区域作为输入来执行最终的检测和分类。这两步分别称为区域提议步骤和物体检测步骤。或者，单步方法将这两个步骤结合在一起，直接预测类别概率和物体位置。
- en: Object detector models have gone through various changes throughout the years
    since 2012\. The first breakthrough in object detection was the RCNN [[1](#bib.bib1)]
    which resulted in an improvement of nearly 30% over the previous state of the
    art. We shall start the survey by exploring this detector first.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从2012年以来，物体检测模型经历了各种变化。物体检测的第一次突破是 RCNN [[1](#bib.bib1)]，这使得相对于之前的技术水平提高了近30%。我们将通过首先探讨这个检测器来开始调查。
- en: II-C Problem Statement for Object Detection
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 物体检测问题陈述
- en: There are always two components to constructing a deep learning model. The first
    component is responsible for dividing the training data into input and targets.
    The second component is deciding upon the neural network architecture and training
    regime. The input for these models is an image. The targets are a list of object
    classes relaying what class the object belongs to and their corresponding coordinates.
    These coordinates signify where in the image the object exist. There are 4 types
    of coordinates- the center x and y coordinates and the height and width of the
    bounding box. We shall use the term bounding box to denote the box formed by applying
    these 4 coordinates on the image. The network is trained to predict a list of
    objects with their corresponding locations in the form of bounding box coordinates.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 构建深度学习模型总是包括两个组件。第一个组件负责将训练数据划分为输入和目标。第二个组件则是确定神经网络架构和训练方案。这些模型的输入是图像。目标是一个对象类别的列表，表示对象所属的类别及其对应的坐标。这些坐标标示出对象在图像中的位置。有4种坐标——中心的
    x 和 y 坐标以及边界框的高度和宽度。我们将使用边界框一词来表示通过在图像上应用这4个坐标形成的框。网络被训练以预测一个物体列表及其对应的边界框坐标位置。
- en: Two Step Models
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两步模型
- en: III Region Convolutional Network (R-CNN)
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 区域卷积网络（R-CNN）
- en: '![Refer to caption](img/eb1a03479b5bbbb6ebf690944e08d00e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb1a03479b5bbbb6ebf690944e08d00e.png)'
- en: 'Figure 1: Region Convolutional Network'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：区域卷积网络
- en: The RCNN Model [[1](#bib.bib1)] was a highly influential model that has shaped
    the structure of modern object detectors. It was the first detector which proposed
    the two step approach. We shall first look at the Region Proposal Model now.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RCNN 模型 [[1](#bib.bib1)] 是一个具有高度影响力的模型，它塑造了现代目标检测器的结构。它是第一个提出两步法的检测器。我们将首先查看区域提议模型。
- en: III-A Region Proposal Model
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 区域提议模型
- en: In this model, the image is the input. A region proposal system finds a set
    of blobs or regions in the image which have a high degree of probability of being
    objects. The Region Proposal System for the R-CNN uses a non deep learning model
    called Selective Search [[12](#bib.bib12)]. Selective Search finds a list of regions
    that it deems most plausible of having an object in them. It finds a large number
    of regions which are then cropped from the input image and resized to a size of
    7 by 7 pixels. These blobs are then fed into the Object Detector Model. The Selective
    Search outputs around  2,000 region proposals of various scales and takes approximately
     27 seconds to execute.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在该模型中，图像作为输入。区域提议系统会在图像中找到一组具有高概率为对象的斑点或区域。R-CNN 的区域提议系统使用一个非深度学习模型，称为选择性搜索
    [[12](#bib.bib12)]。选择性搜索找到一系列其认为最有可能包含对象的区域。它找到了大量的区域，这些区域然后从输入图像中裁剪出来，并调整为 7×7
    像素的大小。这些斑点随后被送入目标检测模型。选择性搜索输出约 2,000 个不同尺度的区域提议，执行时间大约为 27 秒。
- en: III-B Object Detector Model
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 目标检测模型
- en: Each deep learning model is broken down into 5 subsections in this paper. These
    are the input to the model, the targets for the model to learn on, the architecture,
    the loss function and the training procedure used to train the model.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将每个深度学习模型分为 5 个子部分。这些部分包括模型的输入、模型需要学习的目标、架构、损失函数以及用于训练模型的训练过程。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input: The object detector model takes the 7 by 7 sized regions calculated
    by the region proposal model as input.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：目标检测模型将区域提议模型计算出的 7×7 尺寸的区域作为输入。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Targets: The targets for the RCNN network are a list of class probabilities
    and offset coordinates for each region proposed by the Selective Search. The box
    coordinate offsets are calculated to allow the network to learn to fit objects
    better. In other words, offsets are used to modify the original shape of the bounding
    box such that it encloses the object exactly. The region is assigned a class by
    calculating the Intersection Over Union (IOU) between it and the ground truth.
    If the IOU $\geq$ 0.7, the region is assigned the class. If multiple ground truth’s
    have an IOU $\geq$ 0.7 with the region, the ground truth with the highest IOU
    is assigned to that region. If the IOU $\leq$ 0.3, the region is assigned the
    background class. The other regions are not used in calculation of the loss and
    are hence ignored during training. The offsets between the ground truth and that
    region are calculated only if a region is allotted a foreground class. The method
    for calculating these offsets varies. In the RCNN series, they are calculated
    as follows:'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：RCNN 网络的目标是一个包含每个区域的类别概率和偏移坐标的列表，这些区域由选择性搜索提出。盒子坐标的偏移量被计算出来，以使网络能够更好地拟合对象。换句话说，偏移量用于修改边界框的原始形状，以便准确地包围对象。通过计算区域与真实标注之间的交并比（IOU）来分配类别。如果
    IOU $\geq$ 0.7，则为该区域分配类别。如果多个真实标注与区域的 IOU $\geq$ 0.7，则将具有最高 IOU 的真实标注分配给该区域。如果
    IOU $\leq$ 0.3，则该区域被分配为背景类。其他区域不用于损失计算，因此在训练过程中被忽略。只有在区域被分配前景类时，才计算真实标注与该区域之间的偏移量。计算这些偏移量的方法有所不同。在
    RCNN 系列中，它们的计算方式如下：
- en: '|  | $t_{x}=(x_{g}-x_{a})/w_{a}$ |  | <math id="S3.Ex1.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex1.1.1.m1.1b"><mrow id="S3.Ex1.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex1.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex1.1.1.m1.1.1" xref="S3.Ex1.1.1.m1.1.1.cmml">1</mn><mo
    stretchy="false" id="S3.Ex1.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex1.1.1.m1.1c"><cn type="integer" id="S3.Ex1.1.1.m1.1.1.cmml" xref="S3.Ex1.1.1.m1.1.1">1</cn></annotation-xml></semantics></math>
    |'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{x}=(x_{g}-x_{a})/w_{a}$ |  | <math id="S3.Ex1.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex1.1.1.m1.1b"><mrow id="S3.Ex1.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex1.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex1.1.1.m1.1.1" xref="S3.Ex1.1.1.m1.1.1.cmml">1</mn><mo
    stretchy="false" id="S3.Ex1.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex1.1.1.m1.1c"><cn type="integer" id="S3.Ex1.1.1.m1.1.1.cmml" xref="S3.Ex1.1.1.m1.1.1">1</cn></annotation-xml></semantics></math>
    |'
- en: '|  | $t_{y}=(y_{g}-y_{a})/h_{a}$ |  | <math id="S3.Ex2.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex2.1.1.m1.1b"><mrow id="S3.Ex2.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex2.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex2.1.1.m1.1.1" xref="S3.Ex2.1.1.m1.1.1.cmml">2</mn><mo
    stretchy="false" id="S3.Ex2.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex2.1.1.m1.1c"><cn type="integer" id="S3.Ex2.1.1.m1.1.1.cmml" xref="S3.Ex2.1.1.m1.1.1">2</cn></annotation-xml></semantics></math>
    |'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{y}=(y_{g}-y_{a})/h_{a}$ |  | <math id="S3.Ex2.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex2.1.1.m1.1b"><mrow id="S3.Ex2.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex2.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex2.1.1.m1.1.1" xref="S3.Ex2.1.1.m1.1.1.cmml">2</mn><mo
    stretchy="false" id="S3.Ex2.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex2.1.1.m1.1c"><cn type="integer" id="S3.Ex2.1.1.m1.1.1.cmml" xref="S3.Ex2.1.1.m1.1.1">2</cn></annotation-xml></semantics></math>
    |'
- en: '|  | $t_{w}=log(w_{g}/w_{a})$ |  | <math id="S3.Ex3.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex3.1.1.m1.1b"><mrow id="S3.Ex3.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex3.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex3.1.1.m1.1.1" xref="S3.Ex3.1.1.m1.1.1.cmml">3</mn><mo
    stretchy="false" id="S3.Ex3.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex3.1.1.m1.1c"><cn type="integer" id="S3.Ex3.1.1.m1.1.1.cmml" xref="S3.Ex3.1.1.m1.1.1">3</cn></annotation-xml></semantics></math>
    |'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{w}=log(w_{g}/w_{a})$ |  | <math id="S3.Ex3.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex3.1.1.m1.1b"><mrow id="S3.Ex3.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex3.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex3.1.1.m1.1.1" xref="S3.Ex3.1.1.m1.1.1.cmml">3</mn><mo
    stretchy="false" id="S3.Ex3.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex3.1.1.m1.1c"><cn type="integer" id="S3.Ex3.1.1.m1.1.1.cmml" xref="S3.Ex3.1.1.m1.1.1">3</cn></annotation-xml></semantics></math>
    |'
- en: '|  | $t_{h}=log(h_{g}/h_{a})$ |  | <math id="S3.Ex4.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex4.1.1.m1.1b"><mrow id="S3.Ex4.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex4.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex4.1.1.m1.1.1" xref="S3.Ex4.1.1.m1.1.1.cmml">4</mn><mo
    stretchy="false" id="S3.Ex4.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex4.1.1.m1.1c"><cn type="integer" id="S3.Ex4.1.1.m1.1.1.cmml" xref="S3.Ex4.1.1.m1.1.1">4</cn></annotation-xml></semantics></math>
    |'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{h}=log(h_{g}/h_{a})$ |  | <math id="S3.Ex4.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S3.Ex4.1.1.m1.1b"><mrow id="S3.Ex4.1.1.m1.1.2.2"><mo
    stretchy="false" id="S3.Ex4.1.1.m1.1.2.2.1">(</mo><mn id="S3.Ex4.1.1.m1.1.1" xref="S3.Ex4.1.1.m1.1.1.cmml">4</mn><mo
    stretchy="false" id="S3.Ex4.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex4.1.1.m1.1c"><cn type="integer" id="S3.Ex4.1.1.m1.1.1.cmml" xref="S3.Ex4.1.1.m1.1.1">4</cn></annotation-xml></semantics></math>
    |'
- en: where $x_{g}$, $y_{g}$, $w_{g}$ and $h_{g}$ are the x,y coordinates, width and
    height of the ground truth box and $x_{a}$, $y_{a}$, $w_{a}$ and $h_{a}$ are the
    x,y coordinates, width and height of the region. The four values $t_{x}$, $t_{y}$,
    $t_{w}$ and $t_{h}$ are the target offsets.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $x_{g}$、$y_{g}$、$w_{g}$ 和 $h_{g}$ 是实际框的 x、y 坐标、宽度和高度，而 $x_{a}$、$y_{a}$、$w_{a}$
    和 $h_{a}$ 是区域的 x、y 坐标、宽度和高度。四个值 $t_{x}$、$t_{y}$、$t_{w}$ 和 $t_{h}$ 是目标偏移量。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Architecture: The architecture of the Object Detector Model consists of a series
    of convolutional and max pooling layers with activation functions. A region from
    the above step is run through these layers to generate a feature map. This feature
    map denotes a high level format of the region that is interpretable by the model.
    The feature map is unrolled and fed into two fully connected layers to generate
    a 4,078 dimensional vector. This vector is then input into two separate small
    SVM networks [[41](#bib.bib41)] - the classification network head and the regression
    network head. The classification head is responsible for predicting the class
    of object that the region belongs to and the regression head is responsible for
    predicting the offsets to the coordinates of the region to better fit the object.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 架构：目标检测器模型的架构包括一系列卷积层和最大池化层，配有激活函数。上述步骤中的一个区域经过这些层处理后生成一个特征图。该特征图表示区域的高级格式，模型可以解释。特征图被展开并输入到两个全连接层中，生成一个
    4,078 维的向量。然后，将该向量输入到两个独立的小型 SVM 网络 [[41](#bib.bib41)] - 分类网络头和回归网络头。分类头负责预测区域所属的对象类别，而回归头负责预测区域坐标的偏移量，以更好地拟合对象。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss: There are two losses computed in the RCNN- the classification loss and
    the regression loss. The classification loss is the cross entropy loss and the
    regression loss is an L2 loss. The RCNN uses a multi step training pipeline to
    train the network using these two losses.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 损失：RCNN 中计算了两个损失 - 分类损失和回归损失。分类损失是交叉熵损失，回归损失是 L2 损失。RCNN 使用多步训练管道来使用这两个损失训练网络。
- en: 'The cross entropy loss is given as:'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 交叉熵损失定义为：
- en: '|  | $CE({\boldsymbol{y}},\hat{\boldsymbol{y}})=-\sum^{\textrm{N}_{c}}_{i=1}y_{i}\log(\hat{y}_{i})$
    |  | <math id="S3.Ex5.1.1.m1.1" class="ltx_Math" display="inline"><semantics id="S3.Ex5.1.1.m1.1b"><mrow
    id="S3.Ex5.1.1.m1.1.2.2"><mo stretchy="false" id="S3.Ex5.1.1.m1.1.2.2.1">(</mo><mn
    id="S3.Ex5.1.1.m1.1.1" xref="S3.Ex5.1.1.m1.1.1.cmml">5</mn><mo stretchy="false"
    id="S3.Ex5.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex5.1.1.m1.1c"><cn type="integer" id="S3.Ex5.1.1.m1.1.1.cmml" xref="S3.Ex5.1.1.m1.1.1">5</cn></annotation-xml></semantics></math>
    |'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $CE({\boldsymbol{y}},\hat{\boldsymbol{y}})=-\sum^{\textrm{N}_{c}}_{i=1}y_{i}\log(\hat{y}_{i})$
    |  | <math id="S3.Ex5.1.1.m1.1" class="ltx_Math" display="inline"><semantics id="S3.Ex5.1.1.m1.1b"><mrow
    id="S3.Ex5.1.1.m1.1.2.2"><mo stretchy="false" id="S3.Ex5.1.1.m1.1.2.2.1">(</mo><mn
    id="S3.Ex5.1.1.m1.1.1" xref="S3.Ex5.1.1.m1.1.1.cmml">5</mn><mo stretchy="false"
    id="S3.Ex5.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex5.1.1.m1.1c"><cn type="integer" id="S3.Ex5.1.1.m1.1.1.cmml" xref="S3.Ex5.1.1.m1.1.1">5</cn></annotation-xml></semantics></math>
    |'
- en: where ${\boldsymbol{y}}\in\mathbb{R}^{5}$ is a one-hot label vector and $\textrm{N}_{c}$
    is the number of classes.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 ${\boldsymbol{y}}\in\mathbb{R}^{5}$ 是一个独热编码标签向量，$\textrm{N}_{c}$ 是类别的数量。
- en: 'The L2 or the Mean Square Error (MSE) Loss is given as :'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L2 或均方误差（MSE）损失定义为：
- en: '|  | $L2(x_{1},x_{2})=\frac{1}{n}\left\&#124;x_{1}-x_{2}\right\&#124;_{2}^{2}=\frac{1}{n}\sum_{i}(x_{1_{i}}-x_{2_{i}})^{2}$
    |  | <math id="S3.Ex6.1.1.m1.1" class="ltx_Math" display="inline"><semantics id="S3.Ex6.1.1.m1.1b"><mrow
    id="S3.Ex6.1.1.m1.1.2.2"><mo stretchy="false" id="S3.Ex6.1.1.m1.1.2.2.1">(</mo><mn
    id="S3.Ex6.1.1.m1.1.1" xref="S3.Ex6.1.1.m1.1.1.cmml">6</mn><mo stretchy="false"
    id="S3.Ex6.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex6.1.1.m1.1c"><cn type="integer" id="S3.Ex6.1.1.m1.1.1.cmml" xref="S3.Ex6.1.1.m1.1.1">6</cn></annotation-xml></semantics></math>
    |'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $L2(x_{1},x_{2})=\frac{1}{n}\left\|x_{1}-x_{2}\right\|_{2}^{2}=\frac{1}{n}\sum_{i}(x_{1_{i}}-x_{2_{i}})^{2}$
    |  | <math id="S3.Ex6.1.1.m1.1" class="ltx_Math" display="inline"><semantics id="S3.Ex6.1.1.m1.1b"><mrow
    id="S3.Ex6.1.1.m1.1.2.2"><mo stretchy="false" id="S3.Ex6.1.1.m1.1.2.2.1">(</mo><mn
    id="S3.Ex6.1.1.m1.1.1" xref="S3.Ex6.1.1.m1.1.1.cmml">6</mn><mo stretchy="false"
    id="S3.Ex6.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S3.Ex6.1.1.m1.1c"><cn type="integer" id="S3.Ex6.1.1.m1.1.1.cmml" xref="S3.Ex6.1.1.m1.1.1">6</cn></annotation-xml></semantics></math>
    |'
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Training Procedure: The model is trained using a two step procedure.
    Before training, a convolutional base pre-trained on ImageNet is used. The first
    step includes training the SVM classification head using the cross entropy loss.
    The weights for the regression head are not updated. In the second step, the regression
    head is trained with the L2 loss. The weights for the classification head are
    fixed. This process takes approximately 84 hours as features are computed and
    stored for each region proposal. The high number of regions occupy a large amount
    of space and the input/output operations add a substantial overhead.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练流程：模型使用两步流程进行训练。在训练之前，使用在 ImageNet 上预训练的卷积基础。第一步包括使用交叉熵损失训练 SVM 分类头部。回归头部的权重不进行更新。在第二步中，使用
    L2 损失训练回归头部。分类头部的权重是固定的。这个过程大约需要 84 小时，因为计算并存储每个区域提案的特征。大量的区域占用大量空间，输入/输出操作增加了相当大的开销。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Salient Features: The RCNN model takes 47 seconds to process a single image
    since it has a complex multistep training pipeline which requires careful tweaking
    of parameters. Training is expensive in both time and space. The features computed
    for the dataset occupy hundreds of gigabytes and take around 84 hours to train.
    The RCNN provided a good base to iterate upon by providing a structure to solve
    the object detection problem. However, due to its time and space constraints,
    a better model was needed.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 突出特点：RCNN 模型处理单张图像需要 47 秒，因为它拥有复杂的多步骤训练流程，需要仔细调整参数。训练在时间和空间上都非常昂贵。计算的数据集特征占用数百吉字节，训练时间大约为
    84 小时。RCNN 提供了一个很好的基础，通过提供结构来解决目标检测问题。然而，由于其时间和空间的限制，需要一个更好的模型。
- en: IV Fast RCNN
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 快速 RCNN
- en: '![Refer to caption](img/289f1d4cdba74e06c96032e67cec86fa.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/289f1d4cdba74e06c96032e67cec86fa.png)'
- en: 'Figure 2: Fast RCNN'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：快速 RCNN
- en: The Fast RCNN [[3](#bib.bib3)] came out soon after the RCNN and was a substantial
    improvement upon the original. The Fast RCNN is also a two step model which is
    quite similar to the RCNN, in that it uses selective search to find some regions
    and then runs each region through the object detector network. This network consists
    of a convolutional base and two SVM heads for classification and regression. Predictions
    are made for the class and offsets of each region. The RCNN Model takes every
    region proposal and runs them through the convolutional base. This is quite inefficient
    as an overhead of running a region proposal through the convolutional base is
    added, everytime a region proposal is processed. The Fast RCNN aims to reduce
    this overhead by running the convolutional base just once. It runs the convolutional
    base over the entire image to generate a feature map. The regions are cropped
    from this feature map instead of the input image. Hence, features are shared leading
    to a reduction in both space and time. This cropping procedure is done using a
    new algorithm called ROI Pooling.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 快速 RCNN [[3](#bib.bib3)] 在 RCNN 之后不久推出，并在原始模型上进行了显著改进。快速 RCNN 也是一个两步模型，与 RCNN
    十分相似，它使用选择性搜索来找到一些区域，然后将每个区域通过目标检测网络。该网络由一个卷积基础和两个用于分类和回归的 SVM 头部组成。对每个区域的类别和偏移量进行预测。RCNN
    模型对每个区域提案都通过卷积基础进行处理。由于每次处理区域提案时都增加了通过卷积基础的开销，这种方法效率较低。快速 RCNN 旨在通过仅运行一次卷积基础来减少这一开销。它在整个图像上运行卷积基础以生成特征图。区域从该特征图中裁剪，而不是从输入图像中裁剪。因此，特征被共享，从而在空间和时间上都得到了减少。这一裁剪过程是通过一种称为
    ROI 池化的新算法完成的。
- en: The Fast RCNN also introduced a single step training pipeline and a multitask
    loss, enabling the classification and regression heads to be trained simultaneously.
    These changes led to substantial decrease in training time and memory needed.
    Fast RCNN is more of a speed improvement than an accuracy improvement. It takes
    the ideas of RCNN and packages it in a more compact architecture. The improvements
    in the model included a single step end-to-end training pipeline instead of a
    multi step pipeline and reduced training time from 84 hours to 9 hours. It also
    had a reduced memory footprint, no longer requiring features to be stored on disk.
    The major innovation of Fast RCNN was in sharing the features of a convolutional
    net. Constructing a single step training pipeline instead of a multistep training
    pipeline by using a multitask loss was also a novel and elegant solution.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Fast RCNN还引入了单步训练管道和多任务损失，使分类和回归头能够同时训练。这些变化导致训练时间和所需内存的大幅减少。Fast RCNN更多的是速度的提升，而非准确性的提升。它将RCNN的理念打包到一个更紧凑的架构中。模型的改进包括使用单步端到端训练管道代替多步骤管道，将训练时间从84小时减少到9小时。它还减少了内存占用，不再需要将特征存储在磁盘上。Fast
    RCNN的主要创新在于共享卷积网络的特征。通过使用多任务损失构建单步训练管道，而不是多步骤训练管道，也是一个新颖而优雅的解决方案。
- en: IV-A ROI Pooling
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A ROI池化
- en: Region of Interest (ROI) Pooling is an algorithm that takes the coordinates
    of the regions obtained via the Selective Search and directly crops it out from
    the feature map of the original image. ROI Pooling allows for computation to be
    shared for all regions as the convolutional base need not be run for each region.
    The convolutional base is run only once for the input image to generate a single
    feature map. Features for various regions are computed by cropping this feature
    map.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 区域兴趣（ROI）池化是一种算法，它利用通过选择性搜索获得的区域坐标，直接从原始图像的特征图中裁剪出这些区域。ROI池化允许对所有区域共享计算，因为卷积基不需要为每个区域单独运行。卷积基仅对输入图像运行一次，以生成一个特征图。通过裁剪这个特征图来计算各个区域的特征。
- en: In the ROI Pooling algorithm, the coordinates of the regions proposed are divided
    by a factor of $h$, the compression factor. The compression factor is the amount
    by which the image is compressed after it is run through the convolutional base.
    The value for $h$ is 16 if the VGGNet [[47](#bib.bib47)] is used as the convolutional
    base. This value was chosen because the VGGNet compresses the image to $1/16^{th}$
    of its original width and height.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在ROI池化算法中，提出的区域坐标通过一个压缩因子$h$来进行划分。压缩因子是图像经过卷积基处理后被压缩的程度。如果使用VGGNet [[47](#bib.bib47)]
    作为卷积基，则$h$的值为16。这个值是因为VGGNet将图像压缩到其原始宽度和高度的$1/16^{th}$。
- en: 'The compressed coordinates are calculated as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩坐标的计算如下：
- en: '|  | $x_{new}=x_{old}/h$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{new}=x_{old}/h$ |  |'
- en: '|  | $y_{new}=y_{old}/h$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{new}=y_{old}/h$ |  |'
- en: '|  | $w_{new}=w_{old}/h$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{new}=w_{old}/h$ |  |'
- en: '|  | $h_{new}=h_{old}/h$ |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{new}=h_{old}/h$ |  |'
- en: where $x_{new}$, $y_{new}$, $w_{new}$ and $h_{new}$ are the compressed x,y coordinates,
    width and height.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$x_{new}$、$y_{new}$、$w_{new}$ 和 $h_{new}$ 是压缩后的 x、y 坐标、宽度和高度。
- en: Once the compressed coordinates are calculated, they are plotted on the image
    feature map. The region is cropped and resized from the feature map to a size
    of 7 by 7\. This resizing is done using various methods in practice. In ROI Pooling,
    the region plotted on the feature map is divided into 7 by 7 bins. Max pooling
    is performed on the cells in each bin. Often in practice, a variation of ROI Pooling
    called ROI Averaging is used. It simply replaces the max pooling with average
    pooling. The procedure to divide this feature map into 49 bins is approximate.
    It is not uncommon for one bin to contain more number of cells than the other
    bins.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦压缩坐标计算完成，它们将被绘制在图像特征图上。区域从特征图上裁剪并调整大小为7乘7。实际操作中，这种调整大小是通过各种方法完成的。在ROI池化中，将绘制在特征图上的区域划分为7乘7的网格。对每个网格中的单元进行最大池化。在实际操作中，常常使用一种称为ROI平均的ROI池化变体。它简单地将最大池化替换为平均池化。将特征图划分为49个网格的过程是近似的。一个网格包含比其他网格更多单元并不罕见。
- en: Some object detectors simply resize the cropped region from the feature map
    to a size of 7 by 7 using common image algorithms instead of going through the
    ROI Pooling step. In practice, accuracy isn’t affected substantially by doing
    this. Once these regions are cropped, they are ready to be input into the object
    detector model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一些物体检测器简单地使用常见的图像算法将特征图上的裁剪区域调整为 7x7 的大小，而不是经过 ROI Pooling 步骤。在实际应用中，这样做对准确性影响不大。一旦这些区域被裁剪出来，就可以输入到物体检测器模型中。
- en: IV-B Object Detector Model
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 物体检测器模型
- en: The object detector model in the Fast RCNN is very similar to the detector used
    in the RCNN.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Fast RCNN 中的物体检测器模型与 RCNN 中使用的检测器非常相似。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input: The object detector model takes in the region proposals received from
    the region proposal model. These region proposals are cropped using ROI Pooling.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：物体检测器模型接收来自区域提议模型的区域提议。这些区域提议通过 ROI Pooling 进行裁剪。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Targets: The class targets and box regression offsets are calculated in the
    exact same way as done in the RCNN.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：类别目标和框回归偏移量的计算方式与 RCNN 中完全相同。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Architecture: After the ROI Pooling step, the cropped regions are run through
    a small convolutional network. This network consists of a set of convolutional
    and fully connected layers. These layers output a 4,078 dimensional vector which
    is in turn used as input for the classification and regression SVM heads for class
    and offsets predictions respectively.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 架构：在 ROI Pooling 步骤之后，裁剪后的区域会通过一个小型卷积网络。该网络由一组卷积层和全连接层组成。这些层输出一个 4,078 维的向量，该向量进一步作为分类和回归
    SVM 头部的输入，用于分类和偏移量预测。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss: The model uses a multitask loss which is given as:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 损失：该模型使用一个多任务损失，其定义如下：
- en: '|  | $L=l_{c}+\alpha*\lambda*l_{r}$ |  | <math id="S4.Ex11.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S4.Ex11.1.1.m1.1b"><mrow id="S4.Ex11.1.1.m1.1.2.2"><mo
    stretchy="false" id="S4.Ex11.1.1.m1.1.2.2.1">(</mo><mn id="S4.Ex11.1.1.m1.1.1"
    xref="S4.Ex11.1.1.m1.1.1.cmml">7</mn><mo stretchy="false" id="S4.Ex11.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S4.Ex11.1.1.m1.1c"><cn type="integer" id="S4.Ex11.1.1.m1.1.1.cmml"
    xref="S4.Ex11.1.1.m1.1.1">7</cn></annotation-xml></semantics></math> |'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $L=l_{c}+\alpha*\lambda*l_{r}$ |  | <math id="S4.Ex11.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S4.Ex11.1.1.m1.1b"><mrow id="S4.Ex11.1.1.m1.1.2.2"><mo
    stretchy="false" id="S4.Ex11.1.1.m1.1.2.2.1">(</mo><mn id="S4.Ex11.1.1.m1.1.1"
    xref="S4.Ex11.1.1.m1.1.1.cmml">7</mn><mo stretchy="false" id="S4.Ex11.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S4.Ex11.1.1.m1.1c"><cn type="integer" id="S4.Ex11.1.1.m1.1.1.cmml"
    xref="S4.Ex11.1.1.m1.1.1">7</cn></annotation-xml></semantics></math> |'
- en: where $\alpha$ and $\lambda$ are hyperparameters. The $\alpha$ hyperparameter
    is switched to 1 if the region was classified with a foreground class and 0 if
    the region was classified as a background class. The intuition is that the loss
    generated via the regression head should only be taken into account if the region
    actually has an object in it. The $\lambda$ hyperparameter is a weighting factor
    which controls the weight given to each of these losses. It is set to 1 in training
    the network. This loss enables joint training. The loss of classification ($l_{c}$)
    is a regular log loss and the loss of regression ($l_{r}$) is a Smooth $L_{1}$
    loss. The Smooth $L_{1}$ loss is an improvement over the $L_{2}$ loss used for
    regression in RCNN. It is found to be less sensitive to outliers as training with
    unbounded regression targets leads to gradient explosion. Hence, a carefully tuned
    learning rate needs to be followed. Using the Smooth $L_{1}$ loss removes this
    problem.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 和 $\lambda$ 是超参数。如果区域被分类为前景类别，则将 $\alpha$ 超参数设置为 1；如果区域被分类为背景类别，则将其设置为
    0。直观上，只有当区域中实际存在物体时，回归头生成的损失才应被考虑。$\lambda$ 超参数是一个权重因子，用于控制这些损失的权重。在训练网络时，$\lambda$
    被设置为 1。这个损失使得联合训练成为可能。分类损失 ($l_{c}$) 是常规的对数损失，回归损失 ($l_{r}$) 是平滑的 $L_{1}$ 损失。平滑的
    $L_{1}$ 损失是对 RCNN 中用于回归的 $L_{2}$ 损失的改进。研究发现，平滑的 $L_{1}$ 损失对异常值的敏感性较低，因为训练时使用无界回归目标会导致梯度爆炸。因此，需要仔细调整学习率。使用平滑的
    $L_{1}$ 损失可以解决这个问题。
- en: 'The Smooth $L_{1}$ loss is given as follows:'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平滑的 $L_{1}$ 损失定义如下：
- en: '|  | $\textrm{Smooth}_{L_{1}}(x)=\begin{cases}0.5x^{2}&amp;\text{if }&#124;x&#124;<1\\
    &#124;x&#124;-0.5&amp;\text{otherwise}\end{cases}$ |  | <math id="S4.Ex12.1.1.m1.1"
    class="ltx_Math" display="inline"><semantics id="S4.Ex12.1.1.m1.1b"><mrow id="S4.Ex12.1.1.m1.1.2.2"><mo
    stretchy="false" id="S4.Ex12.1.1.m1.1.2.2.1">(</mo><mn id="S4.Ex12.1.1.m1.1.1"
    xref="S4.Ex12.1.1.m1.1.1.cmml">8</mn><mo stretchy="false" id="S4.Ex12.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S4.Ex12.1.1.m1.1c"><cn type="integer" id="S4.Ex12.1.1.m1.1.1.cmml"
    xref="S4.Ex12.1.1.m1.1.1">8</cn></annotation-xml></semantics></math> |'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\textrm{Smooth}_{L_{1}}(x)=\begin{cases}0.5x^{2}&\text{如果 }|x|<1\\ |x|-0.5&\text{否则}\end{cases}$
    |  | <math id="S4.Ex12.1.1.m1.1" class="ltx_Math" display="inline"><semantics
    id="S4.Ex12.1.1.m1.1b"><mrow id="S4.Ex12.1.1.m1.1.2.2"><mo stretchy="false" id="S4.Ex12.1.1.m1.1.2.2.1">(</mo><mn
    id="S4.Ex12.1.1.m1.1.1" xref="S4.Ex12.1.1.m1.1.1.cmml">8</mn><mo stretchy="false"
    id="S4.Ex12.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S4.Ex12.1.1.m1.1c"><cn type="integer" id="S4.Ex12.1.1.m1.1.1.cmml" xref="S4.Ex12.1.1.m1.1.1">8</cn></annotation-xml></semantics></math>
    |'
- en: This is a robust $L_{1}$ loss that is less sensitive to outliers than the $L_{2}$
    loss used in R-CNN.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一种稳健的$L_{1}$损失，相较于R-CNN中使用的$L_{2}$损失对离群值的敏感性较低。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Training Procedure: In the RCNN, training had two distinct steps. The
    Fast RCNN introduces a single step training pipeline where the classification
    and regression subnetworks can be trained together using the multitask loss described
    above. The network is trained with Synchronous Gradient Descent (SGD) with a mini
    batch size of 2 images. 64 random region proposals are taken from each image resulting
    in a mini batch size of 128 region proposals.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型训练过程：在RCNN中，训练有两个不同的步骤。Fast RCNN引入了一个单步训练管道，其中分类和回归子网络可以使用上述的多任务损失一起训练。网络使用同步梯度下降（SGD）进行训练，迷你批次大小为2张图像。从每张图像中随机取64个区域建议，结果是迷你批次大小为128个区域建议。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Salient Features: Firstly, Fast RCNN shares computation through the ROI Pooling
    step hence leading to dramatic increases in speed and memory efficiency. More
    specifically, it reduces training time exponentially from 84 hrs to 9 hrs and
    also reduces inference time from 47 seconds to 0.32 seconds. Secondly, it introduces
    a simpler single step training pipeline and a new loss function. This loss function
    is easier to train and does not suffer with the gradient explosion problem.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 显著特征：首先，Fast RCNN通过ROI Pooling步骤共享计算，从而显著提高了速度和内存效率。更具体地说，它将训练时间从84小时减少到9小时，同时将推理时间从47秒减少到0.32秒。其次，它引入了一个更简单的单步训练管道和一个新的损失函数。这个损失函数更易于训练，不会遇到梯度爆炸问题。
- en: The objector detector step reports real time speeds. However, the region proposal
    step proves to be a bottleneck. More specifically, a better solution than Selective
    Search was needed as it was deemed too computationally demanding for a real time
    system. The next model aimed to do just that.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标检测步骤报告实时速度。然而，区域建议步骤证明是一个瓶颈。更具体地说，比选择性搜索更好的解决方案是必要的，因为它被认为对实时系统计算要求过高。下一个模型旨在解决这一点。
- en: V Faster RCNN
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V Faster RCNN
- en: '![Refer to caption](img/64f1f8c134017dab66bffaa631481f15.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/64f1f8c134017dab66bffaa631481f15.png)'
- en: 'Figure 3: Faster RCNN'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: Faster RCNN'
- en: The Faster RCNN [[6](#bib.bib6)] came out soon after the Fast RCNN paper. It
    was meant to represent the final stage of what the RCNN set out to do. It proposed
    a detector that was learnt end to end. This entailed doing away with the algorithmic
    region proposal selection method and constructing a network that learned to predict
    good region proposals. Selective Search was serviceable but took a lot of time
    and set a bottleneck for accuracy. A network that learnt to predict higher quality
    regions would theoretically have higher quality predictions.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Faster RCNN [[6](#bib.bib6)]在Fast RCNN论文发布后不久出现。它旨在代表RCNN设定的最终阶段。它提出了一个端到端学习的检测器。这意味着不再使用算法区域建议选择方法，而是构建一个学习预测良好区域建议的网络。选择性搜索虽然可用，但耗时较长，并且成为精度的瓶颈。理论上，学习预测更高质量区域的网络将具有更高质量的预测。
- en: The Faster RCNN introduced the Region Proposal Network (RPN) to replace Selective
    Search. The RPN needed to have the capability of predicting regions of multiple
    scales and aspect ratios across the image. This was achieved using a novel concept
    of anchors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Faster RCNN引入了区域建议网络（RPN）来替代选择性搜索。RPN需要能够预测图像中多尺度和纵横比的区域。这是通过一种新颖的锚点概念实现的。
- en: V-A Anchors
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 锚点
- en: Anchors are a set of regions in an image of a predefined shape and size i.e
    anchors are simply rectangular crops of an image. To model for objects of all
    shapes and sizes, they have a diverse set of dimensions. These multiple shapes
    are decided by coming up with a set of aspect ratios and scales. The authors use
    scales of 32px, 64px, 128px and aspect ratios of 1:1, 1:2, 2:1 resulting in 9
    types of anchors. Once a location in the image is decided upon, these 9 anchors
    are cropped from that location.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点是图像中具有预定义形状和大小的一组区域，即锚点只是图像的矩形裁剪。为了对各种形状和大小的物体进行建模，锚点具有多样的维度。这些不同的形状通过设定一组长宽比和尺度来决定。作者使用
    32px、64px、128px 的尺度和 1:1、1:2、2:1 的长宽比，结果生成了 9 种锚点。一旦确定了图像中的一个位置，就从该位置裁剪出这 9 个锚点。
- en: Anchors are cropped out after every $x$ number of pixels in the image. This
    process starts at the top left and ends at the bottom right of the image. The
    window slides from left to right, $x$ pixels at a time, moving $x$ pixels towards
    the bottom after each horizontal scan is done. The process is also called a sliding
    window.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点在图像中的每 $x$ 像素后被裁剪。该过程从左上角开始，到图像的右下角结束。窗口从左到右滑动，每次滑动 $x$ 像素，每完成一次水平扫描后，向下移动
    $x$ 像素。这个过程也称为滑动窗口。
- en: The number of pixels after which the set of anchors are cropped out are decided
    by the compression factor ($h$) of the feature map described above. In VGGNet,
    that number is 16\. In the Faster RCNN paper, 9 crops of different sizes are cropped
    out after every 16 pixels (in height and width) in a sliding window fashion across
    the image. In this way, anchors cover the image quite well.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 锚点被裁剪的像素数由上述特征图的压缩因子（$h$）决定。在 VGGNet 中，这个数字是 16。在 Faster RCNN 论文中，图像中每 16 像素（在高度和宽度方向）裁剪
    9 个不同尺寸的锚点，采用滑动窗口的方式。通过这种方式，锚点可以很好地覆盖图像。
- en: V-B Region Proposal Model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 区域提议模型
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input: The input to the model is the input image. Images are of a fixed size,
    224 by 224 and the training data for the model is augmented by using standard
    tricks like horizontal flipping. To decrease training time, batches of images
    are fed into the network. The GPU can parallelize matrix multiplications and can
    therefore process multiple images at a time.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：模型的输入是输入图像。图像的大小固定为 224 x 224，模型的训练数据通过使用标准技巧如水平翻转来增强。为了减少训练时间，将图像批量输入网络。GPU
    可以并行处理矩阵乘法，因此可以同时处理多张图像。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Targets: Once the entire set of anchors are cropped out of the image, two parameters
    are calculated for each anchor- the class probability target and the box coordinate
    offset target. The class probability target for each anchor is calculated by taking
    the IOU of the ground truth with the anchor. If the IOU $\geq$ 0.7, we assign
    the anchor the class of the ground truth object. If there are multiple ground
    truths with an IOU $\geq$ 0.7, we take the highest one. If the IOU $\leq$ 0.3,
    we assign it the background class. If 0.3 $\leq$ IOU $\leq$ 0.7, we fill in 0
    values as those anchors will not be considered when the loss is calculated, thereby,
    not affecting training. If an anchor is allotted a foreground class, offsets between
    the ground truth and the anchor are calculated. These box offsets targets are
    calculated to make the network learn how to better fit the ground truth by modifying
    the shape of the anchor. The offsets are calculated in the same way as in the
    RCNN model.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：一旦图像中所有的锚点被裁剪出来，将为每个锚点计算两个参数——类别概率目标和框坐标偏移目标。每个锚点的类别概率目标通过计算锚点与真实值的 IOU 来确定。如果
    IOU $\geq$ 0.7，则将锚点分配为真实值对象的类别。如果有多个真实值的 IOU $\geq$ 0.7，则取最大值。如果 IOU $\leq$ 0.3，则将其分配为背景类别。如果
    0.3 $\leq$ IOU $\leq$ 0.7，则填入 0 值，因为这些锚点在计算损失时不会被考虑，从而不会影响训练。如果锚点被分配了前景类别，则计算真实值与锚点之间的偏移量。这些框偏移目标被计算出来，以使网络学习如何通过修改锚点的形状来更好地拟合真实值。偏移量的计算方式与
    RCNN 模型相同。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Architecture: The RPN network consists of a convolutional base which is similar
    to the one used in the RCNN object detector model. This convolutional base outputs
    a feature map which is in turn fed into two subnetworks- a classification subnetwork
    and a regression subnetwork.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 架构：RPN 网络由一个卷积基础组成，该卷积基础与 RCNN 目标检测模型中使用的类似。这个卷积基础输出一个特征图，然后将其输入到两个子网络中——一个分类子网络和一个回归子网络。
- en: The classification and regression head consist of few convolutional layers which
    generate a classification and regression feature map respectively. The only difference
    in the architecture of the two networks is the shape of the final feature map.
    The classification feature map has dimensions $w*h*(k*m)$, where $w$, $h$ and
    $k*m$ are the width, height and depth. The value of $k$ denotes the number of
    anchors per pixel point, which in this case is 9 and $m$ represents the number
    of classes. The feature map for the regression head has the dimensions of $w*h*(k*4)$.
    The value of 4 is meant to represent the predictions for the four offset coordinates
    for each anchor. The cells in the feature map denote the set of pixels out of
    which the anchors are cropped out of. Each cell has a depth which represents the
    box regression offsets of each anchor type. Similar to the classification head,
    the regression head has a few convolutional layers which generate a regression
    feature map.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分类和回归头由少量卷积层组成，这些层分别生成分类和回归特征图。两个网络的架构唯一的不同是最终特征图的形状。分类特征图的维度为 $w*h*(k*m)$，其中
    $w$、$h$ 和 $k*m$ 是宽度、高度和深度。 $k$ 表示每个像素点的锚点数量，在本例中为9，$m$ 代表类别数量。回归头的特征图的维度为 $w*h*(k*4)$。4
    表示每个锚点的四个偏移坐标的预测。特征图中的单元表示从中裁剪出锚点的像素集。每个单元具有一个深度，表示每种锚点类型的框回归偏移。与分类头类似，回归头也有几个卷积层，这些层生成回归特征图。
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss : The anchors are meant to denote the good region proposals that the object
    detector model will further classify on. The model is trained with simple log
    loss for the classification head and the Smooth L1 loss for the regression head.
    There is a weighting factor of $\lambda$ that balances the weight of the loss
    generated by both the heads in a similar way to the Fast RCNN loss. The model
    doesn’t converge when trained on the loss computed across all anchors. The reason
    for this is that the training set is dominated by foreground/background examples,
    this problem is also termed class imbalance. To evade this problem, the training
    set is made by collecting 256 anchors to train on, 128 of these are foreground
    anchors and the rest are background anchors. Challenges have been encountered
    keeping this training set balanced making it an active area of research.'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 损失：锚点旨在表示对象检测器模型将进一步分类的良好区域提议。该模型使用简单的对数损失进行分类头训练，并使用Smooth L1损失进行回归头训练。存在一个加权因子
    $\lambda$，以类似于Fast RCNN损失的方式平衡两个头生成的损失的权重。模型在所有锚点计算的损失上训练时无法收敛。原因是训练集被前景/背景样本主导，这个问题也称为类别不平衡。为避免这个问题，训练集通过收集256个锚点来进行，其中128个是前景锚点，其余的是背景锚点。保持这个训练集平衡已经遇到了一些挑战，使其成为一个活跃的研究领域。
- en: V-C Object detector model
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 目标检测器模型
- en: The object detector model is the same model as the one used in Fast RCNN. The
    only difference is that the input to the model comes from the proposals generated
    by the RPN instead of the Selective Search.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测器模型与Fast RCNN中使用的模型相同。唯一的区别是模型的输入来自RPN生成的提议，而不是Selective Search。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Salient Features: The Faster RCNN is faster and has end to end deep learning
    pipeline. The network improved state of the art accuracy due to the introduction
    of the RPN which improved region proposal quality.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 突出特征：Faster RCNN更快，并且具有端到端的深度学习管道。由于引入了RPN，网络提高了最先进的准确性，从而改进了区域提议质量。
- en: VI Extensions to Faster RCNN
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI Faster RCNN的扩展
- en: There have been various extensions to Faster RCNN network that have made it
    faster and possess greater scale invariance. To make the Faster RCNN scale invariant,
    the original paper took the input image and resized it to various sizes. These
    images were then run through the network. This approach wasn’t ideal as the network
    ran through one image multiple times, making the object detector slower. Feature
    Pyramid Networks provide a robust way to deal with images of different scales
    while still retaining real time speeds. Another extension to the Faster RCNN framework
    is the Region Fully Convolutional Network (R-FCN). It refactors the original network
    by making it fully convolutional thus yielding greater speeds. We shall elaborate
    on both of these architectures in the coming sections.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对 Faster RCNN 网络进行了多种扩展，使其更快且具有更好的尺度不变性。为了使 Faster RCNN 具有尺度不变性，原始论文将输入图像调整为不同的尺寸，然后将这些图像输入网络中。该方法并不理想，因为网络对一张图像进行了多次处理，从而使目标检测器变得更慢。特征金字塔网络提供了一种强大的方法来处理不同尺度的图像，同时仍能保持实时速度。Faster
    RCNN 框架的另一个扩展是区域全卷积网络（R-FCN）。它通过将原始网络重构为全卷积网络，从而实现更快的速度。我们将在接下来的章节中详细讲解这两种架构。
- en: VI-A Feature Pyramid Networks (FPN)
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 特征金字塔网络（FPN）
- en: '![Refer to caption](img/f6e7c77c23bf5e33a3b131d640e1995c.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f6e7c77c23bf5e33a3b131d640e1995c.png)'
- en: 'Figure 4: Feature Pyramid Network'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：特征金字塔网络
- en: Scale invariance is an important property of computer vision systems. The system
    should be able to recognize an object up close and also from far away. The Feature
    Pyramid Network [[9](#bib.bib9)] (FPN) provides such a neural network architecture.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度不变性是计算机视觉系统的一个重要特性。系统应该能够在近距离和远距离识别物体。特征金字塔网络 [[9](#bib.bib9)]（FPN）提供了这样一种神经网络架构。
- en: '![Refer to caption](img/0faaf2a094494cc83f7e49dfc595a20e.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0faaf2a094494cc83f7e49dfc595a20e.png)'
- en: 'Figure 5: Region - Fully Convolutional Network (R-FCN)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：区域全卷积网络（R-FCN）
- en: In the original Faster RCNN, a single feature map was created. A classification
    head and a regression head were attached to this feature map. However, with a
    FPN there are multiple feature maps that are designed to represent the image at
    different scales. The regression and classification heads are run across these
    multi scale feature maps. Anchors no longer have to take care of scale. They can
    only represent various aspect ratios, as scale is handled by these multi scale
    feature maps implicitly.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始 Faster RCNN 中，创建了单个特征图。一个分类头和一个回归头附加到这个特征图上。然而，使用 FPN 时，有多个特征图设计用来表示不同尺度的图像。回归头和分类头在这些多尺度特征图上运行。锚点不再需要处理尺度问题。它们只需表示各种长宽比，因为尺度由这些多尺度特征图隐式处理。
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Architecture: The FPN model takes in an input image and runs it through the
    convolutional base. The convolutional base takes the input through various scales,
    steadily transforming the image to be smaller in height and width but deeper in
    channel depth. This process is also called the bottom up pathway. For example,
    in a ResNet the image goes through five scales which are 224, 56, 28, 14 and 7\.
    This corresponds to four feature maps in the author’s version of FPN (the first
    feature map is ignored as it occupies too much space). These feature maps are
    responsible for the anchors having scales of 32px, 64px, 128px and 256px. These
    feature maps are taken from the last layer of each scale- the intuition being
    that the deepest features contain the most salient information for that scale.
    Each of the four feature maps goes through a 1 by 1 convolution to bring the channel
    depth to $C$. The authors used $C=256$ channels in their implementation. These
    maps are then added element wise to the upsampled version of the feature map one
    scale above them. This procedure is also called a lateral connection. The upsampling
    is performed using nearest neighbour sampling with a factor of 2\. This upsampling
    procedure is also called a top down pathway. Once lateral connections have been
    performed for each scale, the updated feature maps go through a 3 by 3 convolution
    to generate the final set of feature maps. This procedure of lateral connections
    that merge bottom up pathways and top down pathways, ensures that the feature
    maps have a high level of information while still retaining the low level localization
    information for each pixel. It provides a good compromise between getting more
    salient features while still retaining the overall structure of the image at that
    scale.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 架构：FPN 模型接收输入图像并通过卷积基进行处理。卷积基将输入图像通过不同的尺度，逐步将图像的高度和宽度缩小，但通道深度增加。这一过程也称为自下而上的路径。例如，在
    ResNet 中，图像经过五个尺度，分别是 224、56、28、14 和 7。这对应于作者版本中的四个特征图（第一个特征图被忽略，因为它占用了太多空间）。这些特征图负责锚点的尺度为
    32px、64px、128px 和 256px。这些特征图取自每个尺度的最后一层——直觉上，最深层的特征包含了该尺度的最显著信息。每个特征图经过 1x1 卷积，将通道深度变为
    $C$。作者在其实现中使用了 $C=256$ 通道。这些图像然后与上一个尺度的上采样版本逐元素相加。这一过程也称为横向连接。上采样使用邻近采样，因子为 2。这一上采样过程也称为自上而下的路径。完成每个尺度的横向连接后，更新后的特征图经过
    3x3 卷积以生成最终的特征图集。这种将自下而上的路径和自上而下的路径融合的横向连接过程，确保了特征图具有高水平的信息，同时保留了每个像素的低层定位信息。这在获取更显著的特征的同时，还能保持该尺度下图像的整体结构，提供了良好的折中。
- en: After generating these multiple feature maps, the Faster RCNN network runs on
    each scale. Predictions for each scale are generated, the major change being that
    the regression and classification heads now run on multiple feature maps instead
    of one. FPN’s allow for scale invariance in testing and training images. Previously,
    Faster RCNN was trained on multi scaled images but testing on images was done
    on a single scale. Now, due to the structure of FPN, multi scale testing is done
    implicitly.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在生成这些多个特征图后，Faster RCNN 网络在每个尺度上运行。每个尺度生成预测，主要变化在于回归和分类头现在在多个特征图上运行而不是一个。FPN
    允许在测试和训练图像中实现尺度不变性。以前，Faster RCNN 在多尺度图像上进行训练，但测试是在单一尺度图像上进行的。现在，由于 FPN 的结构，多尺度测试隐式完成。
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Salient Points: New state of the art results were obtained in object detection,
    segmentation and classification by integrating FPNs into the pre-existing models.'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 突出点：通过将 FPNs 集成到预先存在的模型中，在目标检测、分割和分类方面取得了新的最先进的结果。
- en: '![Refer to caption](img/e89ec410b5cf1cf3aa2b4c504786a846.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e89ec410b5cf1cf3aa2b4c504786a846.png)'
- en: 'Figure 6: Single Shot MultiBox Detector (SSD)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：单次检测多框检测器（SSD）
- en: VI-B Region - Fully Convolutional Network (R-FCN)
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 区域 - 全卷积网络（R-FCN）
- en: In the Faster RCNN after the RPN stage, each region proposal had to be cropped
    out and resized from the feature map and then fed into the Fast RCNN network.
    This proved to be the most time consuming step in the model and the research community
    focused on improving this. The R-FCN is an attempt to make the Faster RCNN network
    faster by making it fully convolutional and delaying this cropping step.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Faster RCNN 的 RPN 阶段之后，每个区域提议必须从特征图中裁剪出来并调整大小，然后输入到 Fast RCNN 网络中。这被证明是模型中最耗时的步骤，研究界集中在改进这一点。R-FCN
    尝试通过使 Faster RCNN 网络完全卷积化并推迟这个裁剪步骤来提高其速度。
- en: There are several benefits of a fully convolutional network. One of them is
    speed- computing convolutions is faster than computing a fully connected layer.
    The other benefit is that the network becomes scale invariant. Images of various
    sizes can be input into the network without modifying the architecture because
    of the absence of a fully connected layer. Fully convolutional networks first
    gained popularity with segmentation networks [[48](#bib.bib48)]. The R-FCN refactors
    the Faster RCNN network such that it becomes fully convolutional.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 完全卷积网络有几个优点。其中之一是速度：计算卷积比计算全连接层更快。另一个优点是网络变得尺度不变。由于缺少全连接层，可以将不同大小的图像输入网络而无需修改架构。完全卷积网络首次因分割网络而受到关注
    [[48](#bib.bib48)]。R-FCN 通过使 Faster RCNN 网络完全卷积化来重构它。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Architecture: Instead of cropping each region proposal out of the feature map,
    the R-FCN model inputs the entire feature map into the regression and classification
    heads, bringing their depth to a size of $z_{r}$ and $z_{c}$ respectively. The
    value of $z_{c}$ is $k*k*(x)$, where $k$ is 7, which was the size of the side
    of the crop after ROI Pooling and $x$ represents the total number of classes.
    The value of $z_{r}$ is $k*k*4$, where 4 represents the number of box offsets.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 架构：R-FCN 模型并不裁剪特征图中的每个区域提议，而是将整个特征图输入到回归和分类头中，使其深度分别达到 $z_{r}$ 和 $z_{c}$。$z_{c}$
    的值为 $k*k*(x)$，其中 $k$ 为 7，代表 ROI Pooling 后裁剪的边长，而 $x$ 代表类别总数。$z_{r}$ 的值为 $k*k*4$，其中
    4 代表框偏移的数量。
- en: The process of cropping a region is similar to ROI Pooling. However, instead
    of max pooling the values from each bin on the single feature map, max pooling
    is performed on different feature maps. These feature maps are chosen depending
    on the position of the bin. For example, if max pooling is needed to be done for
    the $i^{th}$ bin out of $k*k$ bins, the $i^{th}$ feature map would be used for
    each class. The coordinates for that bin would be mapped on the feature map and
    a single max pooled value will be calculated. Using this method an ROI map is
    created out of the feature map. The probability value for an ROI is then calculated
    by simply finding out the average or maximum value for this ROI map. A softmax
    is computed on the classification head to give the final class probabilities.
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 裁剪一个区域的过程类似于 ROI Pooling。然而，与在单一特征图上对每个 bin 的值进行最大池化不同，最大池化是在不同的特征图上进行的。这些特征图的选择取决于
    bin 的位置。例如，如果需要对 $i^{th}$ 个 bin （在 $k*k$ 个 bin 中）进行最大池化，则每个类别将使用第 $i^{th}$ 个特征图。该
    bin 的坐标将被映射到特征图上，并计算出一个最大池化值。使用这种方法，可以从特征图中创建一个 ROI 图。然后，通过简单地找出该 ROI 图的平均值或最大值来计算
    ROI 的概率值。分类头上计算出一个 softmax，以给出最终的类别概率。
- en: Hence, an R-FCN modifies the ROI Pooling and does it at the end of the convolutional
    operations. There is no extra convolution layer that a region goes through after
    the ROI Pooling. The R-FCN shares features in a better way than the Faster RCNN
    while also reporting speed improvements. It retains the same accuracy as the Faster
    RCNN.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，R-FCN 修改了 ROI Pooling，并将其置于卷积操作的末尾。ROI Pooling 后没有额外的卷积层。R-FCN 在共享特征方面优于
    Faster RCNN，同时也报告了速度上的改进。它保留了与 Faster RCNN 相同的准确性。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Salient Features: The R-FCN sets a new state of the art in the speed of two
    step detectors. It achieves an inference speed of 170ms, which is 2.5 to 20x faster
    than it’s Faster RCNN counterpart.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 突出特征：R-FCN 在两步检测器的速度上设立了新的技术标准。它实现了 170ms 的推理速度，比 Faster RCNN 快 2.5 到 20 倍。
- en: Single Step Object Detectors
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单步目标检测器
- en: Single Step Object Detectors have been popular for some time now. Their simplicity
    and speed coupled with reasonable accuracy have been powerful reasons for their
    popularity. Single step detectors are similar to the RPN network, however instead
    of predicting objects/non objects they directly predict object classes and coordinate
    offsets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 单步目标检测器已经流行了一段时间。它们的简单性和速度，加上合理的准确性，是它们受欢迎的强大理由。单步检测器类似于RPN网络，但不是预测对象/非对象，而是直接预测对象类别和坐标偏移。
- en: VII Single Shot MultiBox Detector (SSD)
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 单次多框检测器（SSD）
- en: Single Shot MultiBox Detector [[5](#bib.bib5)] [[18](#bib.bib18)] came out in
    2015, boasting state of the art results at the time and real time speeds. The
    SSD uses anchors to define the number of default regions in an image. As explained
    before, these anchors predict the class scores and the box coordinates offsets.
    A backbone convolutional base (VGG16) is used and a multitask loss is computed
    to train the network. This loss is similar to the Faster RCNN loss function- a
    smooth L1 loss to predict the box offsets is used along with the cross entropy
    loss to train for the class probabilities. The major difference between the SSD
    from other architectures is that it was the first model to propose training on
    a feature pyramid.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 单次多框检测器[[5](#bib.bib5)] [[18](#bib.bib18)]于2015年推出，当时以最先进的结果和实时速度而自豪。SSD使用锚点来定义图像中的默认区域数量。如前所述，这些锚点预测类别分数和框坐标偏移。使用了一个骨干卷积基础（VGG16），并计算了多任务损失来训练网络。这个损失类似于Faster
    RCNN的损失函数——使用平滑L1损失来预测框的偏移量，以及交叉熵损失来训练类别概率。SSD与其他架构的主要区别在于它是第一个提出在特征金字塔上进行训练的模型。
- en: The network is trained on $n$ number of feature maps, instead of just one. These
    feature maps, taken from each layer are similar to the FPN network but with one
    important difference. They do not use top down pathways to enrich the feature
    map with higher level information. A feature map is taken from each scale and
    a loss is computed and back propagated. Studies have shown that the top down pathway
    is important in ablation studies. Modern object detectors modify the original
    SSD architecture by replacing the SSD feature pyramid with the FPN. The SSD network
    computes the anchors for each scale in a unique way. The network uses a concept
    of aspect ratios and scales, each cell on the feature map generates 6 types of
    anchors, similar to the Faster RCNN. These anchors vary in aspect ratio and the
    scale is captured by the multiple feature maps, in a similar fashion as the FPN.
    SSD uses this feature pyramid to achieve a high accuracy, while remaining the
    fastest detector on the market. It’s variants are used in production systems today,
    where there is a need for fast low memory object detectors. Recently, a tweak
    to the SSD architecture was introduced which further improves on the memory consumption
    and speed of the model without sacrificing on accuracy. The new network is called
    the Pyramid Pooling Network [[38](#bib.bib38)]. The PPN replaces the convolution
    layers needed to compute feature maps with max pooling layers which are faster
    to compute.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在$n$个特征图上进行训练，而不仅仅是一个。这些特征图来自每一层，类似于FPN网络，但有一个重要的区别。它们不使用自上而下的路径来丰富特征图的高层信息。每个尺度都提取一个特征图，并计算损失并进行反向传播。研究表明，自上而下的路径在消融研究中是重要的。现代目标检测器通过用FPN替换SSD特征金字塔来修改原始的SSD架构。SSD网络以独特的方式计算每个尺度的锚点。网络使用纵横比和尺度的概念，每个特征图上的单元生成6种类型的锚点，类似于Faster
    RCNN。这些锚点在纵横比上有所不同，尺度由多个特征图捕捉，类似于FPN。SSD使用这个特征金字塔来实现高准确率，同时保持市场上最快的检测器。它的变体如今在需要快速低内存目标检测器的生产系统中被使用。最近，SSD架构的一个调整被引入，进一步改善了模型的内存消耗和速度，而不牺牲准确性。新网络称为金字塔池化网络[[38](#bib.bib38)]。PPN用计算更快的最大池化层替换了计算特征图所需的卷积层。
- en: '![Refer to caption](img/ba89075434cde711dc06d23a7589ad13.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ba89075434cde711dc06d23a7589ad13.png)'
- en: 'Figure 7: You Only Look Once (YOLO)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：你只看一次（YOLO）
- en: VIII You Only Look Once (YOLO)
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 你只看一次（YOLO）
- en: The YOLO [[10](#bib.bib10)][[11](#bib.bib11)] group of architectures were constructed
    in the same vein as the SSD architectures. The image was run through a few convolutional
    layers to construct a feature map. The concept of anchors was used here too, with
    every grid cell acting as a pixel point on the original image. The YOLO algorithm
    generated 2 anchors for each grid cell. Unlike the Fast RCNN, Yolo has only one
    head. The head outputs feature map of size 7 by 7 by $(x+1+5*(k))$, $k$ is the
    number of anchors, $x+1$ is the total number of classes including the background
    class. The number 5 comes from the four offsets of $x$, $y$, height, width and
    an extra parameter that detects if the region contains an object or not. YOLO
    coins it as the “objectness” of the anchor.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO [[10](#bib.bib10)][[11](#bib.bib11)] 架构组的构建方式与 SSD 架构类似。图像经过几个卷积层处理以构建特征图。这里也使用了锚点的概念，每个网格单元充当原始图像上的一个像素点。YOLO
    算法为每个网格单元生成 2 个锚点。与 Fast RCNN 不同，YOLO 只有一个头部。该头部输出大小为 7 乘 7 乘 $(x+1+5*(k))$ 的特征图，其中
    $k$ 是锚点的数量，$x+1$ 是包括背景类在内的总类数。数字 5 来源于 $x$、$y$、高度、宽度的四个偏移量和一个额外的参数，该参数检测区域是否包含物体。YOLO
    将其称为锚点的“物体性”。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Offset Calculation: Yolo uses a different formulation to calculate offsets
    than the Faster RCNN and SSD architectures. The Faster RCNN uses the following
    formulation:'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 偏移量计算：YOLO 使用与 Faster RCNN 和 SSD 架构不同的公式来计算偏移量。Faster RCNN 使用以下公式：
- en: '|  | $t_{x}=(x_{g}-x_{a})/w_{a}$ |  | <math id="S8.Ex13.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex13.1.1.m1.1b"><mrow id="S8.Ex13.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex13.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex13.1.1.m1.1.1"
    xref="S8.Ex13.1.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S8.Ex13.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex13.1.1.m1.1c"><cn type="integer" id="S8.Ex13.1.1.m1.1.1.cmml"
    xref="S8.Ex13.1.1.m1.1.1">1</cn></annotation-xml></semantics></math> |'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{x}=(x_{g}-x_{a})/w_{a}$ |  | <math id="S8.Ex13.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex13.1.1.m1.1b"><mrow id="S8.Ex13.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex13.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex13.1.1.m1.1.1"
    xref="S8.Ex13.1.1.m1.1.1.cmml">1</mn><mo stretchy="false" id="S8.Ex13.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex13.1.1.m1.1c"><cn type="integer" id="S8.Ex13.1.1.m1.1.1.cmml"
    xref="S8.Ex13.1.1.m1.1.1">1</cn></annotation-xml></semantics></math> |'
- en: '|  | $t_{y}=(y_{g}-y_{a})/h_{a}$ |  | <math id="S8.Ex14.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex14.1.1.m1.1b"><mrow id="S8.Ex14.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex14.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex14.1.1.m1.1.1"
    xref="S8.Ex14.1.1.m1.1.1.cmml">2</mn><mo stretchy="false" id="S8.Ex14.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex14.1.1.m1.1c"><cn type="integer" id="S8.Ex14.1.1.m1.1.1.cmml"
    xref="S8.Ex14.1.1.m1.1.1">2</cn></annotation-xml></semantics></math> |'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{y}=(y_{g}-y_{a})/h_{a}$ |  | <math id="S8.Ex14.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex14.1.1.m1.1b"><mrow id="S8.Ex14.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex14.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex14.1.1.m1.1.1"
    xref="S8.Ex14.1.1.m1.1.1.cmml">2</mn><mo stretchy="false" id="S8.Ex14.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex14.1.1.m1.1c"><cn type="integer" id="S8.Ex14.1.1.m1.1.1.cmml"
    xref="S8.Ex14.1.1.m1.1.1">2</cn></annotation-xml></semantics></math> |'
- en: '|  | $t_{w}=log(w_{g}/w_{a})$ |  | <math id="S8.Ex15.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex15.1.1.m1.1b"><mrow id="S8.Ex15.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex15.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex15.1.1.m1.1.1"
    xref="S8.Ex15.1.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="S8.Ex15.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex15.1.1.m1.1c"><cn type="integer" id="S8.Ex15.1.1.m1.1.1.cmml"
    xref="S8.Ex15.1.1.m1.1.1">3</cn></annotation-xml></semantics></math> |'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{w}=log(w_{g}/w_{a})$ |  | <math id="S8.Ex15.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex15.1.1.m1.1b"><mrow id="S8.Ex15.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex15.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex15.1.1.m1.1.1"
    xref="S8.Ex15.1.1.m1.1.1.cmml">3</mn><mo stretchy="false" id="S8.Ex15.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex15.1.1.m1.1c"><cn type="integer" id="S8.Ex15.1.1.m1.1.1.cmml"
    xref="S8.Ex15.1.1.m1.1.1">3</cn></annotation-xml></semantics></math> |'
- en: '|  | $t_{h}=log(h_{g}/h_{a})$ |  | <math id="S8.Ex16.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex16.1.1.m1.1b"><mrow id="S8.Ex16.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex16.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex16.1.1.m1.1.1"
    xref="S8.Ex16.1.1.m1.1.1.cmml">4</mn><mo stretchy="false" id="S8.Ex16.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex16.1.1.m1.1c"><cn type="integer" id="S8.Ex16.1.1.m1.1.1.cmml"
    xref="S8.Ex16.1.1.m1.1.1">4</cn></annotation-xml></semantics></math> |'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $t_{h}=log(h_{g}/h_{a})$ |  | <math id="S8.Ex16.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex16.1.1.m1.1b"><mrow id="S8.Ex16.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex16.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex16.1.1.m1.1.1"
    xref="S8.Ex16.1.1.m1.1.1.cmml">4</mn><mo stretchy="false" id="S8.Ex16.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex16.1.1.m1.1c"><cn type="integer" id="S8.Ex16.1.1.m1.1.1.cmml"
    xref="S8.Ex16.1.1.m1.1.1">4</cn></annotation-xml></semantics></math> |'
- en: This formulation worked well, but the authors of YOLO point out that this formulation
    is unconstrained. The offsets can be predicted in such a way that they can modify
    the anchor to lie anywhere in the image. Using this formulation, training took
    a long time for the model to start predicting sensible offsets. YOLO hypothesized
    that this was not needed as an anchor for a particular position would only be
    responsible for modifying its structure around that position and not to any location
    in the entire image.
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种表述效果很好，但YOLO的作者指出，这种表述是无约束的。偏移量可以预测，使其能够修改锚点以任意位置的图像。使用这种表述，模型训练时间长，才开始预测合理的偏移量。YOLO假设，不需要锚点的位置，只需负责修改其周围结构，而不是整个图像中的任何位置。
- en: YOLO introduced a new formulation for offsets that constrained the predictions
    of these offsets to near the anchor box. The new formulation modified the above
    objective by training the network to predict these 5 values.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: YOLO为偏移引入了一种新的表述，约束这些偏移量的预测接近锚框。新的表述通过训练网络来预测这5个值，修改了上述目标。
- en: '|  | $b_{x}=\sigma(t_{x})+c_{x}$ |  | <math id="S8.Ex17.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex17.1.1.m1.1b"><mrow id="S8.Ex17.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex17.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex17.1.1.m1.1.1"
    xref="S8.Ex17.1.1.m1.1.1.cmml">21</mn><mo stretchy="false" id="S8.Ex17.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex17.1.1.m1.1c"><cn type="integer" id="S8.Ex17.1.1.m1.1.1.cmml"
    xref="S8.Ex17.1.1.m1.1.1">21</cn></annotation-xml></semantics></math> |'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $b_{x}=\sigma(t_{x})+c_{x}$ |  | <math id="S8.Ex17.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex17.1.1.m1.1b"><mrow id="S8.Ex17.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex17.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex17.1.1.m1.1.1"
    xref="S8.Ex17.1.1.m1.1.1.cmml">21</mn><mo stretchy="false" id="S8.Ex17.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex17.1.1.m1.1c"><cn type="integer" id="S8.Ex17.1.1.m1.1.1.cmml"
    xref="S8.Ex17.1.1.m1.1.1">21</cn></annotation-xml></semantics></math> |'
- en: '|  | $b_{y}=\sigma(t_{y})+c_{y}$ |  | <math id="S8.Ex18.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex18.1.1.m1.1b"><mrow id="S8.Ex18.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex18.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex18.1.1.m1.1.1"
    xref="S8.Ex18.1.1.m1.1.1.cmml">22</mn><mo stretchy="false" id="S8.Ex18.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex18.1.1.m1.1c"><cn type="integer" id="S8.Ex18.1.1.m1.1.1.cmml"
    xref="S8.Ex18.1.1.m1.1.1">22</cn></annotation-xml></semantics></math> |'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $b_{y}=\sigma(t_{y})+c_{y}$ |  | <math id="S8.Ex18.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex18.1.1.m1.1b"><mrow id="S8.Ex18.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex18.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex18.1.1.m1.1.1"
    xref="S8.Ex18.1.1.m1.1.1.cmml">22</mn><mo stretchy="false" id="S8.Ex18.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex18.1.1.m1.1c"><cn type="integer" id="S8.Ex18.1.1.m1.1.1.cmml"
    xref="S8.Ex18.1.1.m1.1.1">22</cn></annotation-xml></semantics></math> |'
- en: '|  | $b_{w}=w_{a}*e^{t_{w}}$ |  | <math id="S8.Ex19.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex19.1.1.m1.1b"><mrow id="S8.Ex19.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex19.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex19.1.1.m1.1.1"
    xref="S8.Ex19.1.1.m1.1.1.cmml">23</mn><mo stretchy="false" id="S8.Ex19.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex19.1.1.m1.1c"><cn type="integer" id="S8.Ex19.1.1.m1.1.1.cmml"
    xref="S8.Ex19.1.1.m1.1.1">23</cn></annotation-xml></semantics></math> |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $b_{w}=w_{a}*e^{t_{w}}$ |  | <math id="S8.Ex19.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex19.1.1.m1.1b"><mrow id="S8.Ex19.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex19.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex19.1.1.m1.1.1"
    xref="S8.Ex19.1.1.m1.1.1.cmml">23</mn><mo stretchy="false" id="S8.Ex19.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex19.1.1.m1.1c"><cn type="integer" id="S8.Ex19.1.1.m1.1.1.cmml"
    xref="S8.Ex19.1.1.m1.1.1">23</cn></annotation-xml></semantics></math> |'
- en: '|  | $b_{h}=h_{a}*e^{t_{h}}$ |  | <math id="S8.Ex20.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex20.1.1.m1.1b"><mrow id="S8.Ex20.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex20.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex20.1.1.m1.1.1"
    xref="S8.Ex20.1.1.m1.1.1.cmml">24</mn><mo stretchy="false" id="S8.Ex20.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex20.1.1.m1.1c"><cn type="integer" id="S8.Ex20.1.1.m1.1.1.cmml"
    xref="S8.Ex20.1.1.m1.1.1">24</cn></annotation-xml></semantics></math> |'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $b_{h}=h_{a}*e^{t_{h}}$ |  | <math id="S8.Ex20.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex20.1.1.m1.1b"><mrow id="S8.Ex20.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex20.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex20.1.1.m1.1.1"
    xref="S8.Ex20.1.1.m1.1.1.cmml">24</mn><mo stretchy="false" id="S8.Ex20.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex20.1.1.m1.1c"><cn type="integer" id="S8.Ex20.1.1.m1.1.1.cmml"
    xref="S8.Ex20.1.1.m1.1.1">24</cn></annotation-xml></semantics></math> |'
- en: '|  | $b_{o}=\sigma(p_{o})$ |  | <math id="S8.Ex21.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex21.1.1.m1.1b"><mrow id="S8.Ex21.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex21.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex21.1.1.m1.1.1"
    xref="S8.Ex21.1.1.m1.1.1.cmml">25</mn><mo stretchy="false" id="S8.Ex21.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex21.1.1.m1.1c"><cn type="integer" id="S8.Ex21.1.1.m1.1.1.cmml"
    xref="S8.Ex21.1.1.m1.1.1">25</cn></annotation-xml></semantics></math> |'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $b_{o}=\sigma(p_{o})$ |  | <math id="S8.Ex21.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S8.Ex21.1.1.m1.1b"><mrow id="S8.Ex21.1.1.m1.1.2.2"><mo
    stretchy="false" id="S8.Ex21.1.1.m1.1.2.2.1">(</mo><mn id="S8.Ex21.1.1.m1.1.1"
    xref="S8.Ex21.1.1.m1.1.1.cmml">25</mn><mo stretchy="false" id="S8.Ex21.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S8.Ex21.1.1.m1.1c"><cn type="integer" id="S8.Ex21.1.1.m1.1.1.cmml"
    xref="S8.Ex21.1.1.m1.1.1">25</cn></annotation-xml></semantics></math> |'
- en: Where $b_{x}$, $b_{y}$, $b_{w}$ , $b_{h}$, $b_{x}$ $b_{o}$ are the target x,y
    coordinates, width, height and objectness. The value of $p_{o}$ represents the
    prediction for objectness. The values for $c_{x}$ and $c_{y}$ represent the offsets
    of the cell in the feature map for the x and y axis. This new formulation constrains
    the prediction to around the anchor box and is reported to decrease training time.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $b_{x}$、$b_{y}$、$b_{w}$、$b_{h}$、$b_{x}$ 和 $b_{o}$ 分别是目标的 x、y 坐标、宽度、高度和对象性。$p_{o}$
    的值表示对象性的预测。$c_{x}$ 和 $c_{y}$ 的值表示特征图中 x 和 y 轴的单元格偏移。这种新公式将预测约束在锚框周围，并报告称可以减少训练时间。
- en: '![Refer to caption](img/0d217464534b391595d352d9b3e4dd6e.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d217464534b391595d352d9b3e4dd6e.png)'
- en: 'Figure 8: Retina Net'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Retina Net
- en: IX Retina Net
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX Retina Net
- en: The RetinaNet is a single step object detector which boasts the state of the
    art results at this point in time by introducing a novel loss function [[15](#bib.bib15)].
    This model represents the first instance where one step detectors have surpassed
    two step detectors in accuracy while retaining superior speed.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 是一种单步目标检测器，目前通过引入一种新颖的损失函数[[15](#bib.bib15)]，展现了最先进的结果。该模型代表了单步检测器首次在准确度上超越双步检测器，同时保持了更快的速度。
- en: The authors realized that the reason why one step detectors have lagged behind
    2 step detectors in accuracy was an implicit class imbalance problem that was
    encountered while training. The RetinaNet sought to solve this problem by introducing
    a loss function coined Focal Loss.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作者意识到单步检测器在准确度上落后于双步检测器的原因是训练过程中遇到的隐性类别不平衡问题。RetinaNet 通过引入名为 Focal Loss 的损失函数来解决这个问题。
- en: IX-A Class Imbalance
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-A 类别不平衡
- en: Class imbalance occurs when the types of training examples are not equal in
    number. In the case of object detection, single step detectors suffer from an
    extreme foreground/background imbalance, with the data heavily biased towards
    background examples.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡发生在训练样本类型数量不等时。在目标检测的情况下，单步检测器遭受极端的前景/背景不平衡，数据严重偏向背景示例。
- en: Class imbalance occurs because a single step detector densely samples regions
    from all over the image. This leads to a high majority of regions belonging to
    the background class. The two step detectors avoid this problem by using an attention
    mechanism (RPN) which focuses the network to train on a small set of examples.
    SSD [[5](#bib.bib5)] tries to solve this problem by using techniques like a fixed
    foreground-background ratio of 1:3, online hard example mining [[8](#bib.bib8)][[21](#bib.bib21)][[13](#bib.bib13)]
    or bootstrapping [[19](#bib.bib19)] [[20](#bib.bib20)]. These techniques are performed
    in single step implementations to maintain a manageable balance between foreground
    and background examples. However, they are inefficient as even after applying
    these techniques, the training data is still dominated by easily classified background
    examples.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡发生在单阶段检测器密集地从图像的各个区域进行采样时。这导致大多数区域属于背景类别。两阶段检测器通过使用注意力机制（RPN）来避免这个问题，RPN
    使网络集中训练在少量的样本上。SSD [[5](#bib.bib5)] 尝试通过使用固定的前景背景比例 1:3、在线困难样本挖掘 [[8](#bib.bib8)][[21](#bib.bib21)][[13](#bib.bib13)]
    或自举 [[19](#bib.bib19)] [[20](#bib.bib20)] 等技术来解决这个问题。这些技术在单阶段实现中执行，以维持前景和背景样本之间的可管理平衡。然而，即使在应用这些技术后，训练数据仍然被易于分类的背景样本所主导，这使得它们效率低下。
- en: Retina Net proposes a dynamic loss function which down weights the loss contributed
    by easily classified examples. The scaling factor decays to zero when the confidence
    in predicting a certain class increases. This loss function can automatically
    down weight the contribution of easy examples during training and rapidly focus
    the model on hard examples.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 提出了一个动态损失函数，该函数降低了易于分类的样本所贡献的损失。缩放因子在预测某个类别的信心增加时衰减为零。该损失函数可以在训练过程中自动降低易样本的贡献，并迅速将模型的重点放在难样本上。
- en: As RetinaNet is a single step detector, it consists of only one model, the object
    detector model.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RetinaNet 是一个单阶段检测器，它仅包含一个模型，即物体检测器模型。
- en: IX-B The Object Detector Model
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IX-B 物体检测器模型
- en: RetinaNet uses a simple object detector by combining the best practices gained
    from previous research.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: RetinaNet 通过结合从先前研究中获得的最佳实践来使用简单的物体检测器。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input: An input image is fed in as the input to the model.'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：将输入图像作为模型的输入。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Targets: The network uses the concept of anchors to predict regions. As an
    FPN is integrated with the model, the anchor sizes do not need to account for
    different scales as that is handled by the multiple feature maps. Each level on
    the feature pyramid uses 9 anchor shapes at each location. The original set of
    three aspect ratios ${{1:1,1:2,2:1}}$ have been augmented by the factors of ${{1,2^{1}/3,2^{2}/3}}$
    for a more diverse selection of bounding box shapes. Similar to the RPN, each
    anchor predicts a class probability out of a set of $K$ object classes (including
    the background class) and 4 bounding box offsets.'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标：网络使用锚点的概念来预测区域。由于模型中集成了 FPN，锚点尺寸不需要考虑不同的尺度，因为多个特征图处理了这个问题。特征金字塔的每一层在每个位置使用
    9 种锚点形状。原始的三种长宽比 ${{1:1,1:2,2:1}}$ 已通过 ${{1,2^{1}/3,2^{2}/3}}$ 这些因子进行了扩展，以获得更丰富的边界框形状选择。类似于
    RPN，每个锚点从 $K$ 个物体类别（包括背景类别）中预测一个类别概率和 4 个边界框偏移量。
- en: Targets for the network are calculated for each anchor as follows. The anchor
    is assigned the ground truth class if the IOU of the ground truth box and the
    anchor $\geq$ 0.5\. A background class is assigned if the IOU $\leq$ 0.4 and if
    the 0.4 $\leq$ IOU $\leq$ 0.5, the anchor is ignored during training. Box regression
    targets are computed by calculating the offsets between each anchor and its assigned
    ground truth box using the same method used by the Faster RCNN. No targets are
    calculated for the anchors belonging to the background class, as the model is
    not trained to predict offsets for a background region.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网络的目标是按如下方式为每个锚点计算。若地面真值框与锚点的 IOU $\geq$ 0.5，则将地面真值类别分配给锚点。如果 IOU $\leq$ 0.4，则分配为背景类别，如果
    0.4 $\leq$ IOU $\leq$ 0.5，则在训练过程中忽略该锚点。框回归目标通过计算每个锚点与其分配的地面真值框之间的偏移量来确定，使用的方法与
    Faster RCNN 相同。对于背景类别的锚点不计算目标，因为模型不训练预测背景区域的偏移量。
- en: 'TABLE I: Object Detection Comparison Table'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 I：物体检测比较表
- en: '| Object Detector Type | backbone | $AP$ | $AP_{50}$ | $AP_{75}$ | $AP_{S}$
    | $AP_{M}$ | $AP_{L}$ |'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 物体检测器类型 | 主干网络 | $AP$ | $AP_{50}$ | $AP_{75}$ | $AP_{S}$ | $AP_{M}$ | $AP_{L}$
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  Faster R-CNN+++ [[6](#bib.bib6)] | ResNet-101-C4 | 34.9 | 55.7 | 37.4 |
    15.6 | 38.7 | 50.9 |'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  Faster R-CNN+++ [[6](#bib.bib6)] | ResNet-101-C4 | 34.9 | 55.7 | 37.4 |
    15.6 | 38.7 | 50.9 |'
- en: '|  Faster R-CNN w FPN [[6](#bib.bib6)] | ResNet-101-FPN | 36.2 | 59.1 | 39.0
    | 18.2 | 39.0 | 48.2 |'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  Faster R-CNN w FPN [[6](#bib.bib6)] | ResNet-101-FPN | 36.2 | 59.1 | 39.0
    | 18.2 | 39.0 | 48.2 |'
- en: '|  Faster R-CNN by G-RMI [[6](#bib.bib6)] | Inception-ResNet-v2 [[3](#bib.bib3)]
    | 34.7 | 55.5 | 36.7 | 13.5 | 38.1 | 52.0 |'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  Faster R-CNN by G-RMI [[6](#bib.bib6)] | Inception-ResNet-v2 [[3](#bib.bib3)]
    | 34.7 | 55.5 | 36.7 | 13.5 | 38.1 | 52.0 |'
- en: '|  Faster R-CNN w TDM [[6](#bib.bib6)] | Inception-ResNet-v2-TDM | 36.8 | 57.7
    | 39.2 | 16.2 | 39.8 | 52.1 |'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  Faster R-CNN w TDM [[6](#bib.bib6)] | Inception-ResNet-v2-TDM | 36.8 | 57.7
    | 39.2 | 16.2 | 39.8 | 52.1 |'
- en: '|  YOLOv2 [[11](#bib.bib11)] | DarkNet-19 [[11](#bib.bib11)] | 21.6 | 44.0
    | 19.2 | 5.0 | 22.4 | 35.5 |'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  YOLOv2 [[11](#bib.bib11)] | DarkNet-19 [[11](#bib.bib11)] | 21.6 | 44.0
    | 19.2 | 5.0 | 22.4 | 35.5 |'
- en: '|  SSD513 [[5](#bib.bib5), [4](#bib.bib4)] | ResNet-101-SSD | 31.2 | 50.4 |
    33.3 | 10.2 | 34.5 | 49.8 |'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  SSD513 [[5](#bib.bib5), [4](#bib.bib4)] | ResNet-101-SSD | 31.2 | 50.4 |
    33.3 | 10.2 | 34.5 | 49.8 |'
- en: '|  DSSD513 [[18](#bib.bib18), [4](#bib.bib4)] | ResNet-101-DSSD | 33.2 | 53.3
    | 35.2 | 13.0 | 35.4 | 51.1 |'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  DSSD513 [[18](#bib.bib18), [4](#bib.bib4)] | ResNet-101-DSSD | 33.2 | 53.3
    | 35.2 | 13.0 | 35.4 | 51.1 |'
- en: '|  RetinaNet[[16](#bib.bib16), [4](#bib.bib4)] | ResNet-101-FPN | 39.1 | 59.1
    | 42.3 | 21.8 | 42.7 | 50.2 |'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  RetinaNet[[16](#bib.bib16), [4](#bib.bib4)] | ResNet-101-FPN | 39.1 | 59.1
    | 42.3 | 21.8 | 42.7 | 50.2 |'
- en: '|  RetinaNet[[16](#bib.bib16), [17](#bib.bib17)] | ResNeXt-101-FPN | 40.8 |
    61.1 | 44.1 | 24.1 | 44.2 | 51.2 |'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  RetinaNet[[16](#bib.bib16), [17](#bib.bib17)] | ResNeXt-101-FPN | 40.8 |
    61.1 | 44.1 | 24.1 | 44.2 | 51.2 |'
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Architecture: The detector uses a single unified network composed of a backbone
    network and two task specific subnetworks. The first subnetwork predicts the class
    of the region and the second subnetwork predicts the coordinate offsets. The architecture
    is similar to an RPN augmented by an FPN.'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 架构：检测器使用由骨干网络和两个任务特定子网络组成的单一统一网络。第一个子网络预测区域的类别，第二个子网络预测坐标偏移。该架构类似于通过FPN增强的RPN。
- en: In the paper, the authors use a Resnet for the convolutional base which is augmented
    by an FPN to create a rich feature pyramid of the image. The classification and
    the regression subnets are quite similar in structure. Each pyramid level is attached
    with these subnetworks, weights of the heads are shared across all levels. The
    architecture of the classification subnet consists of a small FCN consisting of
    4 convolutional layers of filter size 3 by 3\. Each convolutional layer has a
    relu [[49](#bib.bib49)] activation function attached to it and maintains the same
    channel size as the input feature map. Finally, sigmoid activations are attached
    to output a feature map of depth $A*K$. The value for $A=9$ and it represents
    the number of aspect ratios per anchor, $K$ represents the number of object classes.
    The box regression subnet is identical to the classification subnet except for
    the last layer. The last layer has the depth of $4*A$. The 4 indicates the width,
    height and x and y coordinate offsets. The authors claim that a class agnostic
    box regressor like the one described above is equally accurate inspite of having
    having fewer parameters.
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在论文中，作者使用了一个Resnet作为卷积基础，通过FPN增强以创建图像的丰富特征金字塔。分类和回归子网络在结构上非常相似。每个金字塔层都附有这些子网络，头部的权重在所有层之间共享。分类子网络的架构由一个小型FCN组成，该FCN包含4个卷积层，滤波器大小为3x3。每个卷积层附有relu
    [[49](#bib.bib49)] 激活函数，并保持与输入特征图相同的通道大小。最后，sigmoid激活函数被附加以输出深度为$A*K$的特征图。$A=9$表示每个锚点的长宽比数量，$K$表示对象类别的数量。盒子回归子网络与分类子网络完全相同，除了最后一层。最后一层的深度为$4*A$。4表示宽度、高度以及x和y坐标偏移。作者声称，尽管具有较少的参数，像上述的无类别盒子回归器依然准确。
- en: •
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Loss: The paper pioneered a new loss function called the focal loss. This loss
    is used to train the entire network and is the central innovation of RetinaNet.
    It is due to this loss that the network is able to achieve state of the art accuracies
    while still retaining real time speeds. Before describing the loss, a brief word
    on backpropogation. Backpropogation is the algorithm through which neural networks
    learn. It tweaks the weights of the network slightly such that the loss is minimized.
    Hence, the loss controls the amount by which gradients are tweaked. A high loss
    for an object makes the network more sensitive for that object and vice versa.'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 损失：该论文首创了一种称为焦点损失的新损失函数。此损失用于训练整个网络，是RetinaNet的核心创新。正是由于这个损失，网络能够在保持实时速度的同时达到最先进的准确性。在描述损失之前，简要介绍一下反向传播。反向传播是神经网络学习的算法。它略微调整网络的权重，使损失最小化。因此，损失控制梯度调整的程度。一个物体的高损失使得网络对该物体更敏感，反之亦然。
- en: 'The focal loss was introduced to solve the class imbalance problem. Methods
    to combat class imbalance have been used in the past with the most common being
    the balanced cross entropy loss. The balanced cross entropy loss function down
    weights the loss generated by the background class hence reducing it’s effect
    on the parameters of the network. This is done using a hyper parameter called
    $\alpha$. The balanced cross entropy, $B_{c}$ is given as follows:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 焦点损失的引入是为了解决类别不平衡问题。以往用于应对类别不平衡的方法有很多，其中最常见的是平衡交叉熵损失。平衡交叉熵损失函数降低背景类别生成的损失，从而减少它对网络参数的影响。这是通过一个称为$\alpha$的超参数来实现的。平衡交叉熵$B_{c}$的表达式如下：
- en: '|  | $B_{c}=\alpha*c$ |  | <math id="S9.Ex22.1.1.m1.1" class="ltx_Math" display="inline"><semantics
    id="S9.Ex22.1.1.m1.1b"><mrow id="S9.Ex22.1.1.m1.1.2.2"><mo stretchy="false" id="S9.Ex22.1.1.m1.1.2.2.1">(</mo><mn
    id="S9.Ex22.1.1.m1.1.1" xref="S9.Ex22.1.1.m1.1.1.cmml">26</mn><mo stretchy="false"
    id="S9.Ex22.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S9.Ex22.1.1.m1.1c"><cn type="integer" id="S9.Ex22.1.1.m1.1.1.cmml" xref="S9.Ex22.1.1.m1.1.1">26</cn></annotation-xml></semantics></math>
    |'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $B_{c}=\alpha*c$ |  | <math id="S9.Ex22.1.1.m1.1" class="ltx_Math" display="inline"><semantics
    id="S9.Ex22.1.1.m1.1b"><mrow id="S9.Ex22.1.1.m1.1.2.2"><mo stretchy="false" id="S9.Ex22.1.1.m1.1.2.2.1">(</mo><mn
    id="S9.Ex22.1.1.m1.1.1" xref="S9.Ex22.1.1.m1.1.1.cmml">26</mn><mo stretchy="false"
    id="S9.Ex22.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml encoding="MathML-Content"
    id="S9.Ex22.1.1.m1.1c"><cn type="integer" id="S9.Ex22.1.1.m1.1.1.cmml" xref="S9.Ex22.1.1.m1.1.1">26</cn></annotation-xml></semantics></math>
    |'
- en: where $c$ is cross entropy. The value for $\alpha$ is as is for the foreground
    class and $1-\alpha$ for the background class. The value for $\alpha$ could be
    the inverse class frequency or can be treated as a hyperparameter to be set during
    cross validation. It is used to balance between foreground and background. The
    problem however is that this loss does not differentiate between easy/hard examples
    although it does balance the importance of positive/negative examples.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中$c$是交叉熵。$\alpha$的值对前景类别保持不变，对背景类别为$1-\alpha$。$\alpha$的值可以是逆类频率，也可以视为在交叉验证过程中设置的超参数。它用于平衡前景和背景。然而，问题在于，这种损失没有区分简单/困难样本，尽管它确实平衡了正负样本的重要性。
- en: The authors discovered that the gradients are mostly dominated by easily classified
    examples. Hence, they decided to down weight the loss for a prediction of high
    confidence. This allows the network to focus on hard examples and learn how to
    classify them. To achieve this, the authors combined the balanced cross entropy
    loss and this discovery of down weighting the easily classified examples to form
    the focal loss $F_{L}$.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者发现梯度主要由容易分类的样本主导。因此，他们决定降低高置信度预测的损失。这使得网络能够关注困难样本并学习如何对它们进行分类。为实现这一目标，作者将平衡交叉熵损失与这种降低容易分类样本权重的发现相结合，形成了焦点损失$F_{L}$。
- en: '|  | $F_{L}=(1-p)^{\gamma}*B_{c}$ |  | <math id="S9.Ex23.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S9.Ex23.1.1.m1.1b"><mrow id="S9.Ex23.1.1.m1.1.2.2"><mo
    stretchy="false" id="S9.Ex23.1.1.m1.1.2.2.1">(</mo><mn id="S9.Ex23.1.1.m1.1.1"
    xref="S9.Ex23.1.1.m1.1.1.cmml">27</mn><mo stretchy="false" id="S9.Ex23.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S9.Ex23.1.1.m1.1c"><cn type="integer" id="S9.Ex23.1.1.m1.1.1.cmml"
    xref="S9.Ex23.1.1.m1.1.1">27</cn></annotation-xml></semantics></math> |'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $F_{L}=(1-p)^{\gamma}*B_{c}$ |  | <math id="S9.Ex23.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S9.Ex23.1.1.m1.1b"><mrow id="S9.Ex23.1.1.m1.1.2.2"><mo
    stretchy="false" id="S9.Ex23.1.1.m1.1.2.2.1">(</mo><mn id="S9.Ex23.1.1.m1.1.1"
    xref="S9.Ex23.1.1.m1.1.1.cmml">27</mn><mo stretchy="false" id="S9.Ex23.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S9.Ex23.1.1.m1.1c"><cn type="integer" id="S9.Ex23.1.1.m1.1.1.cmml"
    xref="S9.Ex23.1.1.m1.1.1">27</cn></annotation-xml></semantics></math> |'
- en: where $(1-p)^{\gamma}$ is called the modulating factor of $F_{L}$. The modulating
    factor down weights the effect of the loss if the examples are easy to predict.
    The factor of $\gamma$ adds an exponential factor to the scale, it is also called
    the scaling factor and is generally set to 2\. For example, the focal loss generated
    by a example predicted to be of 0.9 confidence, will be down weighted by a factor
    of 100 while a example predicted to be 0.99 will be down weighted by a factor
    of 1000.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $(1-p)^{\gamma}$ 被称为 $F_{L}$ 的调节因子。调节因子降低了当样本易于预测时损失的影响。$\gamma$ 因子为尺度添加了一个指数因子，也称为缩放因子，通常设置为
    2。例如，对于一个预测置信度为 0.9 的样本，焦点损失会被一个 100 的因子缩小，而对于预测置信度为 0.99 的样本，焦点损失会被一个 1000 的因子缩小。
- en: •
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Training and Inference: Training is performed using Stochastic Gradient Descent
    (SGD), using a initial learning rate of 0.01, which is divided by 10 after 60k
    examples and and again after 80k examples. SGD is initialized with a weight decay
    of 0.0001 and a momentum of 0.9\. Horizontal image flipping is the only data augmentation
    technique used. During the start of training, the focal loss fails to converge
    and diverges early in training. To combat this, in the beginning the network predicts
    a probability of 0.01 for each foreground class, to stabilize training.'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练和推断：训练使用随机梯度下降 (SGD)，初始学习率为 0.01，在 60k 样本后除以 10，再在 80k 样本后除以 10。SGD 初始化时的权重衰减为
    0.0001，动量为 0.9。水平翻转图像是唯一使用的数据增强技术。在训练开始时，焦点损失无法收敛，并在训练初期发散。为了解决这个问题，在开始时，网络对每个前景类别预测
    0.01 的概率，以稳定训练。
- en: Inference is performed by running the image through the network. Only the top
    1k predictions are taken at each feature level after thresholding the detector
    confidence at 0.05\. Non Maximum Suppression (NMS) [[1](#bib.bib1)] is performed
    using a threshold of 0.5 and the boxes are overlaid on the image to form the final
    output. This technique is seen to improve training stability for both cross entropy
    and focal loss in the case of heavy class imbalance.
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推断通过将图像传递到网络中来执行。仅在阈值为 0.05 后取每个特征级别的前 1k 个预测。使用 0.5 的阈值进行非极大值抑制 (NMS) [[1](#bib.bib1)]，并将框叠加在图像上以形成最终输出。这种技术被认为可以改善在严重类别不平衡情况下交叉熵和焦点损失的训练稳定性。
- en: •
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Salient Points
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重点内容
- en: The RetinaNet boasts state of the art accuracy presently and operates at around
    60 FPS. It’s use of the focal loss allows it to have a simple design that is easy
    to implement. Table 1 compares and contrasts different object detectors.
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RetinaNet 目前拥有最先进的准确度，并以约 60 FPS 运行。它使用焦点损失，使得其设计简单且易于实现。表 1 对不同目标检测器进行了比较和对比。
- en: X Metrics To Evaluate Object Recognition Models
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 评估目标识别模型的指标
- en: There have been several metrics that the research community uses to evaluate
    object detection models. The most important being the Mixed Average Precision
    and Average Precision metrics.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 研究界使用了几种指标来评估目标检测模型。最重要的指标是混合平均精度和平均精度。
- en: X-A Precision & Recall
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-A 精度与召回率
- en: '|  | $T_{P}=TruePositive$ |  | <math id="S10.Ex24.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex24.1.1.m1.1b"><mrow id="S10.Ex24.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex24.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex24.1.1.m1.1.1"
    xref="S10.Ex24.1.1.m1.1.1.cmml">28</mn><mo stretchy="false" id="S10.Ex24.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex24.1.1.m1.1c"><cn type="integer" id="S10.Ex24.1.1.m1.1.1.cmml"
    xref="S10.Ex24.1.1.m1.1.1">28</cn></annotation-xml></semantics></math> |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{P}=TruePositive$ |  | <math id="S10.Ex24.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex24.1.1.m1.1b"><mrow id="S10.Ex24.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex24.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex24.1.1.m1.1.1"
    xref="S10.Ex24.1.1.m1.1.1.cmml">28</mn><mo stretchy="false" id="S10.Ex24.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex24.1.1.m1.1c"><cn type="integer" id="S10.Ex24.1.1.m1.1.1.cmml"
    xref="S10.Ex24.1.1.m1.1.1">28</cn></annotation-xml></semantics></math> |'
- en: '|  | $T_{N}=TrueNegative$ |  | <math id="S10.Ex25.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex25.1.1.m1.1b"><mrow id="S10.Ex25.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex25.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex25.1.1.m1.1.1"
    xref="S10.Ex25.1.1.m1.1.1.cmml">29</mn><mo stretchy="false" id="S10.Ex25.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex25.1.1.m1.1c"><cn type="integer" id="S10.Ex25.1.1.m1.1.1.cmml"
    xref="S10.Ex25.1.1.m1.1.1">29</cn></annotation-xml></semantics></math> |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{N}=TrueNegative$ |  | <math id="S10.Ex25.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex25.1.1.m1.1b"><mrow id="S10.Ex25.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex25.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex25.1.1.m1.1.1"
    xref="S10.Ex25.1.1.m1.1.1.cmml">29</mn><mo stretchy="false" id="S10.Ex25.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex25.1.1.m1.1c"><cn type="integer" id="S10.Ex25.1.1.m1.1.1.cmml"
    xref="S10.Ex25.1.1.m1.1.1">29</cn></annotation-xml></semantics></math> |'
- en: '|  | $F_{P}=FalsePositive$ |  | <math id="S10.Ex26.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex26.1.1.m1.1b"><mrow id="S10.Ex26.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex26.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex26.1.1.m1.1.1"
    xref="S10.Ex26.1.1.m1.1.1.cmml">30</mn><mo stretchy="false" id="S10.Ex26.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex26.1.1.m1.1c"><cn type="integer" id="S10.Ex26.1.1.m1.1.1.cmml"
    xref="S10.Ex26.1.1.m1.1.1">30</cn></annotation-xml></semantics></math> |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{P}=FalsePositive$ |  | <math id="S10.Ex26.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex26.1.1.m1.1b"><mrow id="S10.Ex26.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex26.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex26.1.1.m1.1.1"
    xref="S10.Ex26.1.1.m1.1.1.cmml">30</mn><mo stretchy="false" id="S10.Ex26.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex26.1.1.m1.1c"><cn type="integer" id="S10.Ex26.1.1.m1.1.1.cmml"
    xref="S10.Ex26.1.1.m1.1.1">30</cn></annotation-xml></semantics></math> |'
- en: '|  | $F_{N}=FalseNegative$ |  | <math id="S10.Ex27.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex27.1.1.m1.1b"><mrow id="S10.Ex27.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex27.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex27.1.1.m1.1.1"
    xref="S10.Ex27.1.1.m1.1.1.cmml">31</mn><mo stretchy="false" id="S10.Ex27.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex27.1.1.m1.1c"><cn type="integer" id="S10.Ex27.1.1.m1.1.1.cmml"
    xref="S10.Ex27.1.1.m1.1.1">31</cn></annotation-xml></semantics></math> |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{N}=FalseNegative$ |  | <math id="S10.Ex27.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex27.1.1.m1.1b"><mrow id="S10.Ex27.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex27.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex27.1.1.m1.1.1"
    xref="S10.Ex27.1.1.m1.1.1.cmml">31</mn><mo stretchy="false" id="S10.Ex27.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex27.1.1.m1.1c"><cn type="integer" id="S10.Ex27.1.1.m1.1.1.cmml"
    xref="S10.Ex27.1.1.m1.1.1">31</cn></annotation-xml></semantics></math> |'
- en: '|  | $P=T_{P}/(T_{P}+F_{P})$ |  | <math id="S10.Ex28.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex28.1.1.m1.1b"><mrow id="S10.Ex28.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex28.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex28.1.1.m1.1.1"
    xref="S10.Ex28.1.1.m1.1.1.cmml">32</mn><mo stretchy="false" id="S10.Ex28.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex28.1.1.m1.1c"><cn type="integer" id="S10.Ex28.1.1.m1.1.1.cmml"
    xref="S10.Ex28.1.1.m1.1.1">32</cn></annotation-xml></semantics></math> |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $P=T_{P}/(T_{P}+F_{P})$ |  | <math id="S10.Ex28.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex28.1.1.m1.1b"><mrow id="S10.Ex28.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex28.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex28.1.1.m1.1.1"
    xref="S10.Ex28.1.1.m1.1.1.cmml">32</mn><mo stretchy="false" id="S10.Ex28.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex28.1.1.m1.1c"><cn type="integer" id="S10.Ex28.1.1.m1.1.1.cmml"
    xref="S10.Ex28.1.1.m1.1.1">32</cn></annotation-xml></semantics></math> |'
- en: '|  | $R=T_{P}/(T_{P}+F_{N})$ |  | <math id="S10.Ex29.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex29.1.1.m1.1b"><mrow id="S10.Ex29.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex29.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex29.1.1.m1.1.1"
    xref="S10.Ex29.1.1.m1.1.1.cmml">33</mn><mo stretchy="false" id="S10.Ex29.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex29.1.1.m1.1c"><cn type="integer" id="S10.Ex29.1.1.m1.1.1.cmml"
    xref="S10.Ex29.1.1.m1.1.1">33</cn></annotation-xml></semantics></math> |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $R=T_{P}/(T_{P}+F_{N})$ |  | <math id="S10.Ex29.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex29.1.1.m1.1b"><mrow id="S10.Ex29.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex29.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex29.1.1.m1.1.1"
    xref="S10.Ex29.1.1.m1.1.1.cmml">33</mn><mo stretchy="false" id="S10.Ex29.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex29.1.1.m1.1c"><cn type="integer" id="S10.Ex29.1.1.m1.1.1.cmml"
    xref="S10.Ex29.1.1.m1.1.1">33</cn></annotation-xml></semantics></math> |'
- en: where Precision is $P$ and Recall is $R$
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中精度是 $P$，召回率是 $R$
- en: Intuitively, precision measures how accurate the predictions are and recall
    measures the quality of the positive predictions made by the model. There is generally
    a trade off while constructing machine learning models with the ideal scenario
    being a model with a high precision and a high recall. However, some use cases
    call for greater precision than recall or vice versa.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 从直观上讲，精度衡量预测的准确性，而召回率衡量模型做出的正预测的质量。在构建机器学习模型时通常会存在权衡，理想的情况是模型具有高精度和高召回率。然而，有些应用场景要求更高的精度或召回率，或两者之一。
- en: X-B Average Precision
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-B 平均精度
- en: Average precision is calculated by taking the top 11 predictions made by the
    model for an object. For these 11 predictions, the precision and recall for each
    prediction is measured given that $T_{P}$ is known for the experiment. The prediction
    is said to be correct if it’s IOU is above a certain threshold. These IOU values
    generally vary between 0.5 and 0.95.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度是通过取模型对一个对象的前 11 个预测来计算的。对于这 11 个预测，测量每个预测的精度和召回率，前提是已知实验的 $T_{P}$。如果预测的
    IOU 高于某个阈值，则认为预测是正确的。这些 IOU 值通常在 0.5 和 0.95 之间变化。
- en: Once precision and recall are calculated each of these 11 steps, the maximum
    precision is calculated for the recall values that range from 0 to 1 with a step
    size of 0.1\. Using these values, the Average Precision is calculated by taking
    the average over all these maximum precision values.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了这 11 个步骤中的精度和召回率，就会计算从 0 到 1 的召回值范围内的最大精度，步长为 0.1。使用这些值，通过对所有这些最大精度值取平均来计算平均精度。
- en: The following formula is used to calculate the average precision of the model.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式用于计算模型的平均精度。
- en: '|  | $AP_{r}(i)=\max_{i\leq j}(P_{j})$ |  | <math id="S10.Ex30.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex30.1.1.m1.1b"><mrow id="S10.Ex30.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex30.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex30.1.1.m1.1.1"
    xref="S10.Ex30.1.1.m1.1.1.cmml">34</mn><mo stretchy="false" id="S10.Ex30.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex30.1.1.m1.1c"><cn type="integer" id="S10.Ex30.1.1.m1.1.1.cmml"
    xref="S10.Ex30.1.1.m1.1.1">34</cn></annotation-xml></semantics></math> |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP_{r}(i)=\max_{i\leq j}(P_{j})$ |  | <math id="S10.Ex30.1.1.m1.1" class="ltx_Math"
    display="inline"><semantics id="S10.Ex30.1.1.m1.1b"><mrow id="S10.Ex30.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex30.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex30.1.1.m1.1.1"
    xref="S10.Ex30.1.1.m1.1.1.cmml">34</mn><mo stretchy="false" id="S10.Ex30.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex30.1.1.m1.1c"><cn type="integer" id="S10.Ex30.1.1.m1.1.1.cmml"
    xref="S10.Ex30.1.1.m1.1.1">34</cn></annotation-xml></semantics></math> |'
- en: '|  | $AP=(1/11)*\sum_{r=0}^{1}(AP_{r}(i))$ |  | <math id="S10.Ex31.1.1.m1.1"
    class="ltx_Math" display="inline"><semantics id="S10.Ex31.1.1.m1.1b"><mrow id="S10.Ex31.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex31.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex31.1.1.m1.1.1"
    xref="S10.Ex31.1.1.m1.1.1.cmml">35</mn><mo stretchy="false" id="S10.Ex31.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex31.1.1.m1.1c"><cn type="integer" id="S10.Ex31.1.1.m1.1.1.cmml"
    xref="S10.Ex31.1.1.m1.1.1">35</cn></annotation-xml></semantics></math> |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP=(1/11)*\sum_{r=0}^{1}(AP_{r}(i))$ |  | <math id="S10.Ex31.1.1.m1.1"
    class="ltx_Math" display="inline"><semantics id="S10.Ex31.1.1.m1.1b"><mrow id="S10.Ex31.1.1.m1.1.2.2"><mo
    stretchy="false" id="S10.Ex31.1.1.m1.1.2.2.1">(</mo><mn id="S10.Ex31.1.1.m1.1.1"
    xref="S10.Ex31.1.1.m1.1.1.cmml">35</mn><mo stretchy="false" id="S10.Ex31.1.1.m1.1.2.2.2">)</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S10.Ex31.1.1.m1.1c"><cn type="integer" id="S10.Ex31.1.1.m1.1.1.cmml"
    xref="S10.Ex31.1.1.m1.1.1">35</cn></annotation-xml></semantics></math> |'
- en: where $AP$ is Average Precision. It is averaged over 11 quantities because the
    step size of $i$ is 0.1.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $AP$ 是平均精度。它在 11 个量上取平均，因为 $i$ 的步长为 0.1。
- en: X-C Mean Average Precision (mAP)
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: X-C 均值平均精度 (mAP)
- en: The Mean Average Precision is one of the more popular metrics used to judge
    object detectors. In recent papers, object detectors are compared via their $mAP$
    score. Unfortunately the metric has been used to varying meanings. The YOLO paper
    as well as the PASCAL VOC [[36](#bib.bib36)] dataset details $mAP$ to be the same
    quantity as AP. The COCO dataset [[37](#bib.bib37)] however, uses a modification
    to this metric called the Mixed Average Metric. The AP values for different IOU
    values are calculated for the COCO $mAP$. The IOU values range from 0.5 to 0.95
    with a step size of 0.05\. These AP values are then averaged over to get the COCO
    Mixed Average Precision metric. The YOLO model reports better accuracies in the
    simple AP of 0.5 metric but not with the $mAP$ metric. This paper treats the COCO
    $mAP$ as the Mixed Average Precision.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 平均平均精度（Mean Average Precision）是评判目标检测器的流行指标之一。在最近的论文中，目标检测器通过其`$mAP$`分数进行比较。不幸的是，这一指标被赋予了不同的含义。YOLO论文和PASCAL
    VOC [[36](#bib.bib36)] 数据集将 `$mAP$` 视为与AP相同的量。而COCO数据集 [[37](#bib.bib37)] 使用了一种称为混合平均度量（Mixed
    Average Metric）的指标修正方法。COCO `$mAP$` 的AP值是针对不同IOU值计算的。这些IOU值范围从0.5到0.95，步长为0.05。然后将这些AP值进行平均，以得到COCO混合平均精度指标。YOLO模型在简单的AP
    0.5指标上报告了更好的准确率，但在 `$mAP$` 指标上则不然。本文将COCO `$mAP$` 视为混合平均精度。
- en: XI Convolutional Bases
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XI 卷积基
- en: All modern object detectors have a convolutional base. This base is responsible
    for creating a feature map that is embedded with salient information about the
    image. The accuracy for the object detector is highly related to how well the
    convolutional base can capture meaningful information about the image. The base
    takes the image through a series of convolutions that make the image smaller and
    deeper. This process allows the network to make sense of the various shapes in
    the image.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 所有现代目标检测器都有一个卷积基。这个基负责创建一个包含图像显著信息的特征图。目标检测器的准确性与卷积基捕捉图像有意义信息的能力密切相关。基通过一系列卷积处理图像，使图像变得更小、更深。这一过程使网络能够理解图像中的各种形状。
- en: Convolutional networks form the backbone of most modern computer vision models.
    A lot of convolutional networks with different architectures have come out in
    the past few years. They are roughly judged on three factors namely accuracy,
    speed and memory.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络构成了大多数现代计算机视觉模型的基础。过去几年中出现了许多具有不同架构的卷积网络。它们大致根据三个因素进行评估，即准确性、速度和内存。
- en: Convolutional bases are selected according to the use case. For example, object
    detector’s on the phone will require the base to be small and fast. Alternatively,
    larger bases will be used by the powerful GPU’s on the cloud. A lot of research
    has gone into making these convolutional nets faster and more accurate. A few
    popular bases are described in the coming section. Bigger nets have led in accuracy
    however, advancements have been made to compress and optimize neural networks
    with a minimal tradeoff on accuracy.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积基根据使用案例进行选择。例如，手机上的目标检测器需要基数小且快速。相反，云上的强大GPU将使用更大的基数。大量研究致力于使这些卷积网络更快、更准确。一些流行的基数将在接下来的部分中描述。虽然更大的网络在准确率上领先，但已取得进展以压缩和优化神经网络，在准确率上做最小的权衡。
- en: XI-A Resnet
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XI-A Resnet
- en: Resnet [[4](#bib.bib4)] is an extremely popular convolutional network. It popularized
    the concept of skip connections in convolutional networks. Skip connections add
    or concatenate features of the previous layer to the current layer. This leads
    to the network propagating gradients much more effectively during backpropogation.
    Resnets were the state of the art at the time they were released and are still
    quite popular today.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Resnet [[4](#bib.bib4)] 是一个极其流行的卷积网络。它推广了卷积网络中跳跃连接的概念。跳跃连接将前一层的特征添加到当前层。这样可以使网络在反向传播过程中更有效地传播梯度。Resnets
    在发布时是当时的最先进技术，至今仍然相当受欢迎。
- en: The innovation of introducing skip connections resulted in the training of extremely
    deep networks without over fitting. Resnets are usually used with powerful GPU’s
    as their processing takes substantially more time on a CPU. These networks are
    a good choice for a convolutional base on a powerful cloud server.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 引入跳跃连接的创新使得训练极其深的网络而不会过拟合。Resnets 通常与强大的GPU一起使用，因为在CPU上处理的时间要长得多。这些网络是强大云服务器上卷积基的不错选择。
- en: XI-B Resnext
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XI-B Resnext
- en: Resnext [[17](#bib.bib17)] networks are an evolution to the Resnet. They introduce
    a new concept called grouped convolutions. Traditional convolutions operate in
    three dimensions- width, height and depth. Grouped convolutions introduce a new
    dimension called cardinality.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Resnext [[17](#bib.bib17)] 网络是 Resnet 的一种进化。它们引入了一个新的概念，称为分组卷积。传统卷积在三个维度上操作——宽度、高度和深度。分组卷积引入了一个新的维度，称为
    cardinality。
- en: Cardinality espouses dividing a task into $n$ number of smaller subtasks. Each
    block of the network goes through a 1 by 1 convolution similar to the Resnet,
    to reduce dimensionality. The next step is slightly different. Instead of running
    the map through a 3 by 3 convolutional layer, the network splits the $m$ channels
    (where $m$ is the depth of the feature map after the 1 by 1 convolution) into
    groups of $n$, where $n$ is the cardinality. A 3 by 3 convolution is performed
    for each of these $n$ groups (with $m/n$ channels each) after which, the $n$ groups
    are concatenated together. After concatenation, this aggregation undergoes another
    1 by 1 convolution layer to adjust the channel size. Similar to the Resnet, a
    skip connection is added to this result.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Cardinality 提倡将任务划分为 $n$ 个较小的子任务。网络的每个块都经过类似于 Resnet 的 1x1 卷积，以减少维度。下一步略有不同。网络将
    $m$ 个通道（其中 $m$ 是 1x1 卷积后特征图的深度）分成 $n$ 组，其中 $n$ 是 cardinality。对每个这些 $n$ 组（每组有 $m/n$
    个通道）执行 3x3 卷积，之后将这 $n$ 组连接在一起。连接后，这个聚合经过另一个 1x1 卷积层以调整通道大小。类似于 Resnet，这个结果还添加了一个跳过连接。
- en: The usage of grouped convolutions in Resnext networks led to a better classification
    accuracy while still maintaining the speed of a Resnet network. They are indeed
    the next version of Resnets.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Resnext 网络中使用分组卷积提高了分类准确性，同时仍保持 Resnet 网络的速度。它们确实是 Resnets 的下一版本。
- en: XI-C MobileNets
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XI-C MobileNets
- en: These networks are a series of convolutional networks made with speed and memory
    efficiency in mind. Like the name suggests, the MobileNet [[14](#bib.bib14)][[16](#bib.bib16)]
    class of networks are used on low powered devices like smart phones and embedded
    systems.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这些网络是一系列以速度和内存效率为重点的卷积网络。正如名称所示，MobileNet [[14](#bib.bib14)][[16](#bib.bib16)]
    类网络用于低功耗设备，如智能手机和嵌入式系统。
- en: MobileNets introduce depth wise separable convolutions that lead to a major
    loss in floating point operations while still retaining accuracy. The traditional
    procedure of converting 16 channels to 32 channels is to do it in one go. The
    floating point operations are $(w*h*3*3*16*32)$ for a 3 by 3 convolution.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNets 引入了深度可分离卷积，这导致浮点操作的大量减少，同时仍然保持准确性。将 16 个通道转换为 32 个通道的传统方法是一次完成。对于
    3x3 卷积，浮点操作是 $(w*h*3*3*16*32)$。
- en: Depth wise convolutions make the feature map go through a 3 by 3 convolution
    by not merging anything, resulting in 16 feature maps. To these 16 feature maps,
    a 1 by 1 filter with 32 channels is applied, resulting in 32 feature maps. Hence,
    the total computation is $(3*3*16+16*32*1*1)$ , which is far less than the previous
    approach.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积使特征图通过 3x3 卷积而不合并任何东西， resulting in 16 个特征图。对这 16 个特征图应用 32 通道的 1x1 滤波器，得到
    32 个特征图。因此，总计算量为 $(3*3*16+16*32*1*1)$，远低于以前的方法。
- en: Depth wise separable convolutions form the backbone of MobileNets and have led
    to a speedup in computing the feature map without sacrificing the overall quality.
    The next generation of MobileNets have been released recently are are aptly named
    MobileNetsV2 [[16](#bib.bib16)]. Apart from using depth wise convolutions they
    add two new architectural modifications- a linear bottleneck and skip connections
    on these linear bottlenecks. Linear bottleneck layers are layers that compress
    the number of channels from the previous layer. This compression has been experimentally
    proven to help, they make MobileNetsV2 smaller but just as accurate. The other
    innovation in MobileNetV2 [[16](#bib.bib16)] over the V1 [[14](#bib.bib14)] is
    skip connections which were popularized by the Resnet. Studies have shown that
    MobileNetsV2 are 30-40% faster then the previous iteration.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积构成了 MobileNets 的基础，并在不牺牲整体质量的情况下加速了特征图的计算。最近发布了新一代 MobileNets，名为 MobileNetsV2
    [[16](#bib.bib16)]。除了使用深度卷积外，它们还增加了两个新的结构修改——线性瓶颈和这些线性瓶颈上的跳跃连接。线性瓶颈层是压缩来自上一层的通道数的层。这种压缩已被实验证明有效，它使
    MobileNetsV2 更小但同样准确。MobileNetV2 [[16](#bib.bib16)] 相对于 V1 [[14](#bib.bib14)]
    的另一个创新是跳跃连接，这些连接由 Resnet 推广。研究表明，MobileNetsV2 比以前的版本快 30-40%。
- en: Papers have coined object detection systems using MobileNets as the SSDLite
    Framework. The SSDLite outperforms the Yolo architecture by being 20% more efficient
    and 10% smaller. Networks like the Effnet [[39](#bib.bib39)] and Shufflenet [[40](#bib.bib40)]
    are also some examples of promising steps in that direction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 论文提出了使用 MobileNets 的物体检测系统，称为 SSDLite 框架。SSDLite 在效率上比 Yolo 架构高出 20%，且体积缩小了
    10%。像 Effnet [[39](#bib.bib39)] 和 Shufflenet [[40](#bib.bib40)] 等网络也是朝着这个方向的有前途的步骤。
- en: XII Training
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XII 训练
- en: There have been significant innovations in training neural networks in the past
    few years. This section provides a brief overview of some new promising techniques.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来在训练神经网络方面取得了显著创新。本节简要概述了一些新的有前途的技术。
- en: XII-A Superconvergence
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XII-A 超收敛
- en: Superconvergence is observed when a model is trained to the same level of accuracy
    in exponentially lesser time than the usual way using the same hardware. An example
    of Superconvergence is observed when training the Cifar10 network to an accuracy
    of 94% in around 70 epochs, compared to the original paper which took around 700
    epochs. Superconvergence is possible through the use of the 1 Cycle Policy to
    train neural networks.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 超收敛是指在相同硬件下，模型在指数减少的时间内达到相同的准确度。超收敛的一个例子是在约 70 个 epoch 内将 Cifar10 网络的准确度提升到
    94%，而原始论文需要约 700 个 epoch。通过使用 1 Cycle 策略来训练神经网络，可以实现超收敛。
- en: 'To use the 1 Cycle Policy [[27](#bib.bib27)], the concept of a learning rate
    finder needs to be explained. The learning rate finder seeks to find the highest
    learning rate to train the model without divergence. It is helpful as one can
    now be sure that the training of the model is performed at the fastest rate possible.
    To implement the learning rate finder, the following steps are followed:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 1 Cycle 策略 [[27](#bib.bib27)]，需要解释学习率查找器的概念。学习率查找器旨在找到最高的学习率以训练模型而不发生发散。这是有帮助的，因为这样可以确保模型训练在可能的最快速率下进行。实现学习率查找器时，按照以下步骤进行：
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Start with an extremely small Learning Rate (around 1e-8) and increase the Learning
    Rate linearly.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从极小的学习率（大约 1e-8）开始，并线性增加学习率。
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Plot the loss at each step of the Learning Rate.
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制每一步的学习率损失图。
- en: •
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Stop the learning rate finder when the loss stops decreasing and starts increasing.
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当损失不再减少而开始增加时，停止学习率查找器。
- en: After observing the graph, a value for the Learning Rate is decided upon by
    taking the maximum value of the Learning Rate when the loss is still decreasing.
    Now once the optimal Learning Rate (L) is discovered, the 1 Cycle Policy can be
    used.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图表后，通过取损失仍在减少时的学习率最大值来决定学习率值。现在，一旦发现最优学习率（L），就可以使用 1 Cycle 策略。
- en: '![Refer to caption](img/47d29aaf9cad8b4871c66afe365082c4.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/47d29aaf9cad8b4871c66afe365082c4.png)'
- en: 'Figure 9: Learning Rate Finder'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：学习率查找器
- en: 'The 1 Cycle Policy [[27](#bib.bib27)] [[46](#bib.bib46)] states that to train
    the model, the following steps should be observed:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 1 Cycle 策略 [[27](#bib.bib27)] [[46](#bib.bib46)] 指出，为了训练模型，应该遵循以下步骤：
- en: •
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Start with a Learning Rate L/10 than the learning rate found out by the Learning
    Rate finder.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从比学习率查找器找到的学习率低 L/10 开始。
- en: •
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Train the model, while increasing the Learning Rate linearly to L after each
    epoch.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在每个训练周期结束后，将学习率线性增加到 L。
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: After reaching L, start decreasing the Learning Rate back till L/10.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 达到 L 后，开始将学习率逐步降低回 L/10。
- en: '![Refer to caption](img/95b0b3e7a8f40796aed8c781b24f1cc6.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/95b0b3e7a8f40796aed8c781b24f1cc6.png)'
- en: 'Figure 10: Feature Pyramid Network'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：特征金字塔网络
- en: XII-B Distributed Training
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XII-B 分布式训练
- en: Training huge models on a single machine is not feasible. Nowadays, training
    is distributed on various machines. Distribution aids parallelism which leads
    to substantial improvements in training time. Distributed Training [[42](#bib.bib42)][[43](#bib.bib43)][[44](#bib.bib44)]
    has led to datasets like Imagenet being trained in as little a time as 4 minutes.
    This has been made possible by using techniques like Layer-wise Adaptive Rate
    Scaling (LARS). The major intuition is that all the layers of a neural network
    shouldn’t be trained at the same rate. The initial layers should be trained faster
    than the last layers at the beginning of training, with this being reversed when
    the model has been training for some time. Using adaptive learning rates has also
    led to substantial improvements in the training process. The most popular way
    of training networks presently is to use a small learning rate to warm up the
    network after which higher learning rates are used with a decay policy.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在单台机器上训练巨大的模型是不切实际的。如今，训练工作分布在多台机器上。分布式训练有助于并行计算，从而显著提高训练时间。分布式训练 [[42](#bib.bib42)][[43](#bib.bib43)][[44](#bib.bib44)]
    使得像 ImageNet 这样的数据集能够在短至 4 分钟内完成训练。这得益于使用了诸如逐层自适应学习率缩放（LARS）等技术。主要的直觉是神经网络的所有层不应以相同的速率进行训练。在训练初期，前面几层应该比后面的层训练得更快，而当模型训练了一段时间后则相反。使用自适应学习率也大大改善了训练过程。目前，训练网络的最流行方法是使用较小的学习率来预热网络，然后使用较高的学习率并应用衰减策略。
- en: As of this time, techniques like LARS and the 1 Cycle Policy haven’t been used
    to train object detection algorithms. A paper exploring these new training techniques
    for the use case of object detection would be quite useful.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，LARS 和 1 Cycle 策略等技术尚未用于训练目标检测算法。探索这些新训练技术在目标检测用例中的应用将是非常有用的。
- en: XIII Future Work
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XIII 未来工作
- en: Object Detection has reached a mature stage in it’s development. The algorithms
    described in this paper boast state of the art accuracy that match human capability
    in most cases. Although like all neural networks, they are susceptible to adversarial
    examples. Better explainability and interpretability [[33](#bib.bib33)] are open
    research topics as of now.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测已经达到了其发展的成熟阶段。这篇论文中描述的算法在大多数情况下具有与人类能力匹敌的最先进的准确性。尽管如此，像所有神经网络一样，它们对对抗样本仍然敏感。更好的可解释性和可解释性
    [[33](#bib.bib33)] 仍然是当前的开放研究课题。
- en: Techniques to make models faster and less resource intensive can be explored.
    Reducing time to train these models is another research avenue. Applying new techniques
    like super convergence [[27](#bib.bib27)], cyclic learning rates [[23](#bib.bib23)]
    and SGDR [[28](#bib.bib28)] to these models, could reveal new state of the art
    training times. Apart from reducing training time, reducing inference time can
    also be explored by using quantization [[24](#bib.bib24)][[25](#bib.bib25)] techniques
    and experimenting with new architectures. Automated architecture search is a promising
    step in that direction.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可以探索使模型更快且资源消耗更少的技术。减少训练这些模型的时间是另一项研究方向。将超收敛 [[27](#bib.bib27)]、循环学习率 [[23](#bib.bib23)]
    和 SGDR [[28](#bib.bib28)] 等新技术应用于这些模型，可能会揭示新的最先进的训练时间。除了减少训练时间外，还可以通过使用量化 [[24](#bib.bib24)][[25](#bib.bib25)]
    技术和实验新架构来减少推理时间。自动化架构搜索是朝着这个方向迈出的有希望的一步。
- en: Neural Architecture Search (NAS) [[29](#bib.bib29)] [[30](#bib.bib30)] [[31](#bib.bib31)]
    tries out various combinations of architectures to get the best architecture for
    a task. NASNets have achieved state of the art results in various Computer Vision
    Tasks and exploring NASNets for the object detection problem could reveal some
    new insights. A caveat of NASNets is that they take an extremely long time to
    discover these architectures, hence, a faster search would need to be employed
    to get results in a meaningful time frame.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构搜索（NAS）[[29](#bib.bib29)] [[30](#bib.bib30)] [[31](#bib.bib31)] 尝试各种架构组合，以获得适用于任务的最佳架构。NASNets
    在各种计算机视觉任务中实现了最先进的结果，探索 NASNets 用于物体检测问题可能会揭示一些新的见解。NASNets 的一个警告是，它们需要极长的时间来发现这些架构，因此，需要采用更快的搜索方法来在有意义的时间范围内获得结果。
- en: Weakly supervised training techniques [[32](#bib.bib32)] to train models in
    the absence of labels is another research direction that holds promise. In the
    real world, good object detection datasets are rare. Papers combining unlabeled
    data to train detectors have been coming out recently. Techniques such as using
    different transformations of unlabeled data to get automatic labels have been
    researched. These automatically generated labels have been trained along with
    known labels to get state of the art results in object detection. Some variations
    to this technique can be explored such as using saliency maps to aggregate predictions,
    which could prove to generate better quality labels. The deep learning field is
    growing rapidly or rather, exploding. There shall be consistent improvements to
    the above techniques in the coming years.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督训练技术 [[32](#bib.bib32)] 用于在没有标签的情况下训练模型是另一个有前景的研究方向。在现实世界中，好的物体检测数据集是稀缺的。最近出现了将未标记数据结合起来训练检测器的论文。研究了使用不同的未标记数据转换来获取自动标签的技术。这些自动生成的标签与已知标签一起训练，以在物体检测中获得最先进的结果。可以探索这种技术的一些变体，例如使用显著性图来汇总预测，这可能会生成更高质量的标签。深度学习领域正在快速发展或更确切地说，爆炸式增长。在未来几年中，上述技术将会持续改进。
- en: Conclusion
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In conclusion, the paper has surveyed various object detection Deep Learning
    models and techniques, of which the Retina Net is noted as the best till date.
    The paper has also explored techniques to make networks portable through the use
    of lighter convolutional bases. In closing, various training techniques that make
    these networks easier to train and converge have also been surveyed. Techniques
    such as Cyclic Learning Rates, Stochastic Weight Averaging [[45](#bib.bib45)]
    and Super Convergence, will lead to better training times for a single machine.
    For training in a distributed setting, techniques such as LARS [[43](#bib.bib43)]
    and Linear batch size scaling [[44](#bib.bib44)][[42](#bib.bib42)] have proven
    to give substantial efficiencies.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，本文综述了各种物体检测深度学习模型和技术，其中 Retina Net 被认为是迄今为止最好的。本文还探讨了通过使用更轻量的卷积基础使网络可移植的技术。最后，还综述了使这些网络更容易训练和收敛的各种训练技术。诸如循环学习率、随机权重平均
    [[45](#bib.bib45)] 和超级收敛等技术，将导致单台机器的训练时间更短。对于分布式设置的训练，诸如 LARS [[43](#bib.bib43)]
    和线性批量大小缩放 [[44](#bib.bib44)][[42](#bib.bib42)] 等技术已被证明能显著提高效率。
- en: References
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies
    for accurate object detection and semantic segmentation. In CVPR, 2014.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Girshick, J. Donahue, T. Darrell 和 J. Malik. 丰富的特征层次结构用于准确的物体检测和语义分割。发表于
    CVPR, 2014。'
- en: '[2] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region based convolutional
    networks for accurate object detection and segmentation. TPAMI, 2015.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] R. Girshick, J. Donahue, T. Darrell 和 J. Malik. 基于区域的卷积网络用于准确的物体检测和分割。TPAMI,
    2015。'
- en: '[3] R. Girshick. Fast R-CNN. ICCV 2015'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] R. Girshick. Fast R-CNN。ICCV 2015。'
- en: '[4] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition.
    In CVPR, 2016'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] K. He, X. Zhang, S. Ren 和 J. Sun. 深度残差学习用于图像识别。发表于 CVPR, 2016。'
- en: '[5] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. Reed. SSD: Single shot
    multibox detector. arXiv:1512.02325v2, 2015.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] W. Liu, D. Anguelov, D. Erhan, C. Szegedy 和 S. Reed. SSD: 单次多框检测器。arXiv:1512.02325v2,
    2015。'
- en: '[6] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time
    object detection with region proposal networks. In NIPS, 2015.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Ren, K. He, R. Girshick 和 J. Sun. Faster R-CNN: 基于区域提议网络的实时物体检测。发表于
    NIPS, 2015。'
- en: '[7] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat:
    Integrated recognition, localization and detection using convolutional networks.
    In ICLR, 2014.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus 和 Y. LeCun. Overfeat:
    使用卷积网络进行集成识别、定位和检测。在 ICLR，2014。'
- en: '[8] A. Shrivastava, A. Gupta, and R. Girshick. Training region-based object
    detectors with online hard example mining. In CVPR, 2016.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. Shrivastava, A. Gupta 和 R. Girshick. 通过在线难例挖掘训练基于区域的物体检测器。在 CVPR，2016。'
- en: '[9] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and ´ S. Belongie.
    Feature pyramid networks for object detection. In CVPR, 2017.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan 和 ´ S. Belongie.
    用于物体检测的特征金字塔网络。在 CVPR，2017。'
- en: '[10] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once:
    Unified, real-time object detection. In CVPR, 2016\. 1'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Redmon, S. Divvala, R. Girshick 和 A. Farhadi. 你只看一次: 统一的实时物体检测。在 CVPR，2016。'
- en: '[11] J. Redmon and A. Farhadi. YOLO9000: Better, faster, stronger. In CVPR,
    2017.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Redmon 和 A. Farhadi. YOLO9000: 更好、更快、更强。在 CVPR，2017。'
- en: '[12] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W. Smeulders. Selective
    search for object recognition. IJCV, 2013.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. R. Uijlings, K. E. van de Sande, T. Gevers 和 A. W. Smeulders. 物体识别的选择性搜索。IJCV，2013。'
- en: '[13] P. Viola and M. Jones. Rapid object detection using a boosted cascade
    of simple features. In CVPR, 2001.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] P. Viola 和 M. Jones. 使用增强级联简单特征的快速物体检测。在 CVPR，2001。'
- en: '[14] MobileNets: Efficient Convolutional Neural Networks for Mobile Vision
    Applications'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] MobileNets: 用于移动视觉应用的高效卷积神经网络'
- en: '[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar. Focal
    Loss for Dense Object Detection'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollar. 用于密集物体检测的焦点损失'
- en: '[16] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh
    Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh
    Chen. MobileNetV2: 反向残差和线性瓶颈'
- en: '[17] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He. Aggregated
    Residual Transformations for Deep Neural Networks'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, Kaiming He. 深度神经网络的聚合残差变换'
- en: '[18] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi, and A. C. Berg. DSSD: Deconvolutional
    single shot detector. arXiv:1701.06659, 2016.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C.-Y. Fu, W. Liu, A. Ranga, A. Tyagi 和 A. C. Berg. DSSD: 解卷积单次检测器。arXiv:1701.06659，2016。'
- en: '[19] K.-K. Sung and T. Poggio. Learning and Example Selection for Object and
    Pattern Detection. In MIT A.I. Memo No. 1521, 1994.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] K.-K. Sung 和 T. Poggio. 用于物体和模式检测的学习和例子选择。在 MIT A.I. Memo No. 1521，1994。'
- en: '[20] H. Rowley, S. Baluja, and T. Kanade. Human face detection in visual scenes.
    Technical Report CMU-CS-95-158R, Carnegie Mellon University, 1995.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. Rowley, S. Baluja 和 T. Kanade. 视觉场景中的人脸检测。技术报告 CMU-CS-95-158R，卡内基梅隆大学，1995。'
- en: '[21] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Cascade object
    detection with deformable part models. In CVPR, 2010.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. F. Felzenszwalb, R. B. Girshick 和 D. McAllester. 使用可变形部件模型的级联物体检测。在
    CVPR，2010。'
- en: '[22] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with
    deep convolutional neural networks. In NIPS, 2012.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Krizhevsky, I. Sutskever 和 G. Hinton. 使用深度卷积神经网络进行 ImageNet 分类。在 NIPS，2012。'
- en: '[23] Cyclical Learning Rates for Training Neural Networks. Leslie N. Smith
    .2015'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 用于训练神经网络的循环学习率。Leslie N. Smith，2015'
- en: '[24] Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only
    Inference Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, Dmitry Kalenichenko; The IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), 2018, pp. 2704-2713'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 神经网络的量化和训练以实现高效的整数算术推理 Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong
    Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, Dmitry Kalenichenko; IEEE 计算机视觉与模式识别会议（CVPR），2018，第2704-2713页'
- en: '[25] Quantizing deep convolutional networks for efficient inference: A whitepaper
    Raghuraman Krishnamoorthi, 2018'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 对深度卷积网络进行量化以实现高效推理: 一份白皮书 Raghuraman Krishnamoorthi，2018'
- en: '[26] Pooling Pyramid Network for Object Detection Pengchong Jin, Vivek Rathod,
    Xiangxin Zhu, 2018'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 用于物体检测的池化金字塔网络 Pengchong Jin, Vivek Rathod, Xiangxin Zhu, 2018'
- en: '[27] Super-Convergence: Very Fast Training of Neural Networks Using Large Learning
    Rates Leslie N. Smith, Nicholay Topin, 2017'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 超收敛：使用大学习率的神经网络快速训练 Leslie N. Smith, Nicholay Topin，2017'
- en: '[28] SGDR: Stochastic Gradient Descent with Warm Restarts. Ilya Loshchilov,
    Frank Hutter. ICLR 2017 conference paper'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] SGDR: 带有热启动的随机梯度下降。Ilya Loshchilov, Frank Hutter。ICLR 2017 会议论文'
- en: '[29] DARTS: Differentiable Architecture Search. Hanxiao Liu, Karen Simonyan,
    Yiming Yang'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] DARTS: 可微分架构搜索。Hanxiao Liu, Karen Simonyan, Yiming Yang'
- en: '[30] Neural Architecture Search with Reinforcement Learning Barret Zoph, Quoc
    V. Le'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 通过强化学习进行神经网络架构搜索 Barret Zoph, Quoc V. Le'
- en: '[31] Efficient Neural Architecture Search via Parameter Sharing Hieu Pham,
    Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 高效的神经网络架构搜索通过参数共享 Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le,
    Jeff Dean'
- en: '[32] Data Distillation: Towards Omni-Supervised Learning. Ilija Radosavovic,
    Piotr Dollár, Ross Girshick, Georgia Gkioxari, and Kaiming He. Tech report, arXiv,
    Dec. 2017'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 数据蒸馏：迈向全监督学习 Ilija Radosavovic, Piotr Dollár, Ross Girshick, Georgia Gkioxari,
    和 Kaiming He。技术报告，arXiv，2017年12月'
- en: '[33] https://distill.pub/ , Chris Olah'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] https://distill.pub/，Chris Olah'
- en: '[34] TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed
    Systems Mart´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
    Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay
    Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing
    Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mane,
    Rajat Monga, Sherry Moore, Derek Murray, ´ Chris Olah, Mike Schuster, Jonathon
    Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke,
    Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, ´ Pete Warden, Martin Wattenberg,
    Martin Wicke, Yuan Yu, and Xiaoqiang Zheng'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] TensorFlow：在异构分布式系统上的大规模机器学习 Mart´ın Abadi, Ashish Agarwal, Paul Barham,
    Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey
    Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving,
    Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur,
    Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, ´ Chris Olah,
    Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,
    Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals,
    ´ Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, 和 Xiaoqiang Zheng'
- en: '[35] Caffe: Convolutional Architecture for Fast Feature Embedding∗ Yangqing
    Jia∗ , Evan Shelhamer∗ , Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick,
    Sergio Guadarrama, Trevor Darrell'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Caffe：用于快速特征嵌入的卷积架构∗ Yangqing Jia∗ , Evan Shelhamer∗ , Jeff Donahue, Sergey
    Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell'
- en: '[36] The PASCAL Visual Object Classes (VOC) Challenge Mark Everingham · Luc
    Van Gool · Christopher K. I. Williams · John Winn · Andrew Zisserman'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] PASCAL视觉对象类别（VOC）挑战 Mark Everingham, Luc Van Gool, Christopher K. I. Williams,
    John Winn, Andrew Zisserman'
- en: '[37] Microsoft COCO: Common Objects in Context Tsung-Yi Lin Michael Maire Serge
    Belongie Lubomir Bourdev Ross Girshick James Hays Pietro Perona Deva Ramanan C.
    Lawrence Zitnick Piotr Dollar'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Microsoft COCO：上下文中的常见对象 Tsung-Yi Lin, Michael Maire, Serge Belongie,
    Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence
    Zitnick, Piotr Dollar'
- en: '[38] Pooling Pyramid Network for Object Detection Pengchong Jin, Vivek Rathod,
    Xiangxin Zhu'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 用于目标检测的池化金字塔网络 Pengchong Jin, Vivek Rathod, Xiangxin Zhu'
- en: '[39] ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile
    Devices Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] ShuffleNet：一种极为高效的移动设备卷积神经网络 Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin,
    Jian Sun'
- en: '[40] EffNet: An Efficient Structure for Convolutional Neural Networks Ido Freeman,
    Lutz Roese-Koerner, Anton Kummert'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] EffNet：卷积神经网络的高效结构 Ido Freeman, Lutz Roese-Koerner, Anton Kummert'
- en: '[41] Support vector machines Marti A. Hearst University of California, Berkeley'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 支持向量机 Marti A. Hearst 加州大学伯克利分校'
- en: '[42] Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. Priya Goyal,
    Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
    Andrew Tulloch, Yangqing Jia, Kaiming He'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 精确的大规模小批量SGD：1小时内训练ImageNet Priya Goyal, Piotr Dollár, Ross Girshick,
    Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia,
    Kaiming He'
- en: '[43] LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS .Yang You,Igor Gitman,Boris
    Ginsburg'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 大批量训练卷积网络 Yang You, Igor Gitman, Boris Ginsburg'
- en: '[44] Highly Scalable Deep Learning Training System with Mixed-Precision: Training
    ImageNet in Four Minutes Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong
    Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen,
    Guangxiao Hu, Shaohuai Shi, Xiaowen Chu'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 高度可扩展的深度学习训练系统与混合精度：四分钟内训练 ImageNet Xianyan Jia, Shutao Song, Wei He,
    Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang,
    Liwei Yu, Tiegang Chen, Guangxiao Hu, Shaohuai Shi, Xiaowen Chu'
- en: '[45] Averaging Weights Leads to Wider Optima and Better Generalization Pavel
    Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, Andrew Gordon Wilson'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] 平均权重导致更广的最优解和更好的泛化 Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,
    Dmitry Vetrov, Andrew Gordon Wilson'
- en: '[46] A disciplined approach to neural network hyper-parameters: Part 1 – learning
    rate, batch size, momentum, and weight decay. Leslie Smith.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 对神经网络超参数的有序方法：第1部分 – 学习率、批量大小、动量和权重衰减 Leslie Smith'
- en: '[47] VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION Karen
    Simonyan, Andrew Zisserman'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 非常深度的卷积网络用于大规模图像识别 **卡伦·西蒙尼扬**、**安德鲁·齐瑟曼**'
- en: '[48] Fully Convolutional Networks for Semantic Segmentation Jonathan Long,
    Evan Shelhamer, Trevor Darrell'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 用于语义分割的全卷积网络 **乔纳森·隆**、**埃文·谢尔哈默**、**特雷弗·达雷尔**'
- en: '[49] ImageNet Classification with Deep Convolutional Neural Networks Alex Krizhevsky,
    Ilya Sutskever, Geoffrey E. Hinton'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 使用深度卷积神经网络的 ImageNet 分类 **亚历克斯·克里日夫斯基**、**伊利亚·苏茨克夫**、**杰弗里·E·辛顿**'
