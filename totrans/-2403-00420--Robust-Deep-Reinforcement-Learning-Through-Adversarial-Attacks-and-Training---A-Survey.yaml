- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:11'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2403.00420] Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00420](https://ar5iv.labs.arxiv.org/html/2403.00420)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \equalcont
  prefs: []
  type: TYPE_NORMAL
- en: These authors contributed equally to this work.
  prefs: []
  type: TYPE_NORMAL
- en: \equalcont
  prefs: []
  type: TYPE_NORMAL
- en: These authors contributed equally to this work.
  prefs: []
  type: TYPE_NORMAL
- en: 1]\orgnameIRT SystemX, \orgaddress\cityPalaiseau, \postcode91120, \countryFrance
  prefs: []
  type: TYPE_NORMAL
- en: 2]\orgdivMLIA, ISIR, \orgnameSorbonne Université, \orgaddress\cityParis, \postcode75005,
    \countryFrance
  prefs: []
  type: TYPE_NORMAL
- en: 3]\orgnamePolytechnique Montréal, \orgaddress\cityMontréal, \postcode6079, \stateQuébec,
    \countryCanada
  prefs: []
  type: TYPE_NORMAL
- en: 4]\orgnameSafran Tech, \orgaddress\cityChâteaufort, \postcode78117, \countryFrance
  prefs: []
  type: TYPE_NORMAL
- en: 5]\orgdivLERIA, \orgnameUniversité d’Angers, \orgaddress\cityAngers, \postcode49000,
    \countryFrance
  prefs: []
  type: TYPE_NORMAL
- en: 'Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: \fnmLucas \surSchott    \fnmJoséphine \surDelas    \fnmHatem \surHajri    \fnmElies
    \surGherbi    \fnmReda \surYaich    \fnmNora \surBoulahia-Cuppens    \fnmFrederic
    \surCuppens    \fnmSylvain \surLamprier [ [ [ [ [
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning (DRL) is an approach for training autonomous agents
    across various complex environments. Despite its significant performance in well
    known environments, it remains susceptible to minor conditions variations, raising
    concerns about its reliability in real-world applications. To improve usability,
    DRL must demonstrate trustworthiness and robustness. A way to improve robustness
    of DRL to unknown changes in the conditions is through Adversarial Training, by
    training the agent against well suited adversarial attacks on the dynamics of
    the environment. Addressing this critical issue, our work presents an in-depth
    analysis of contemporary adversarial attack methodologies, systematically categorizing
    them and comparing their objectives and operational mechanisms. This classification
    offers a detailed insight into how adversarial attacks effectively act for evaluating
    the resilience of DRL agents, thereby paving the way for enhancing their robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning, Robustness, Adversarial Attacks, Adversarial Training
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The advent of Deep Reinforcement Learning (DRL) has marked a significant shift
    in various fields, including games [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    autonomous robotics [[4](#bib.bib4)], autonomous driving [[5](#bib.bib5)], and
    energy management [[6](#bib.bib6)]. By integrating Reinforcement Learning (RL)
    with Deep Neural Networks (DNN), DRL can leverages high dimensional continuous
    observations and rewards to train neural policies, without the need for supervised
    example trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: While DRL achieves remarkable performances in well known controlled environments,
    it also encounter challenges in ensuring robust performance amid diverse condition
    changes and real-world perturbations. It particularly struggle to bridge the reality
    gap [[7](#bib.bib7), [8](#bib.bib8)], often DRL agents are trained in simulation
    that remains an imitation of the real-world, resulting in a gap between the performance
    of a trained agent in the simulation and its performance once transferred to the
    real-world application. Even without trying to bridge the reality gap, agents
    can be trained in the first place in some conditions, and be deployed later and
    the conditions may have changed since. This pose the problem of robustness, which
    refers to the agent’s ability to maintain performance in deployment despite slight
    conditions changes in the environment or minor perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover the emergence of adversarial attacks that generate perturbation in
    the inputs and disturbances in the dynamics of the environment, which are deliberately
    designed to mislead neural network decisions, poses unique challenges in RL [[9](#bib.bib9),
    [10](#bib.bib10)] and can be a key to resolve the robustness problem in RL, necessitating
    further exploration and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: This survey aims to address these critical areas of concern. It focuses on key
    issues by presenting a comprehensive framework for understanding the concept of
    robustness of DRL agent. It covers both robustness to perturbed inputs as well
    as robustness to perturbed dynamics of the environment. Additionally, it introduces
    a new classification system that organizes every type of perturbation affecting
    robustness into a unified model. It also offers a review of the existing literature
    on adversarial methods for robust DRL agents and classify the existing methods
    in the proposed taxonomy. The goal is to provide a deeper understanding of various
    adversarial techniques, including their strengths, limitations, and the impact
    they have on the performance, robustness and generalization capabilities of DRL
    agents.
  prefs: []
  type: TYPE_NORMAL
- en: Historically, the primary focus on adversarial examples has been in the realm
    of supervised learning [[11](#bib.bib11)]. Attempts to extend this scope to RL
    have been made, but these have primarily concentrated on adversarial evasion methods
    and robustness-oriented classification [[12](#bib.bib12), [9](#bib.bib9)]. To
    bridge this gap, our work introduces a robustness-centric study of adversarial
    methods in DRL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key contributions of this work include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formalizing the concept of Robustness in DRL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a taxonomy and classification for adversarial attack in DRL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing existing adversarial attack, characterized using our proposed taxonomy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing how adversarial attacks can be used to improve robustness of DRL agents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The structure of the survey is organized as follows: Section [2](#S2 "2 Background
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") provides an introduction to RL and the security implications of DNNs,
    as well as the mathematical prerequisites for analyzing RL Robustness. Section
    [3](#S3 "3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") introduces a formalization of the
    notion of Robustness in DRL. Section [4](#S4 "4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") presents a taxonomy for categorizing adversarial attack methods as
    shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey"). Sections [5.1](#S5.SS1
    "5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") and [5.2](#S5.SS2
    "5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") explore observation and
    dynamic alterations attacks, respectively. Finally section [6](#S6 "6 Strategies
    of Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and
    Training : A Survey") focuses on strategies for applying adversarial attacks and
    adversarial training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fdddcac16768e24135975a9ef3bf9d1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Categorization of the adversarial attacks of the literature as described
    in Section[5](#S5 "5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") with the taxonomy introduced
    Section [4](#S4 "4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RL focuses on decision-making in dynamic environments [[13](#bib.bib13)]. RL
    agents learn by interacting with an environment: they take actions and receive
    feedback in terms of numerical rewards. The objective of a RL agent is to learn
    a policy, a mapping from states to actions, which maximizes the expected cumulative
    reward over time.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Partially Observable Markov Decision Process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Markov Decision Process (MDP) is a mathematical framework for modeling decision-making
    problems where an agent interacts with an environment over discrete time steps.
    In most real-world applications, the agent may not have access to the environment’s
    complete states and instead receives partial observations. This scenario is known
    as a Partially Observable Markov Decision Process (POMDP), which is a generalization
    of the MDP framework, represented by the tuple $\Omega=(S,A,T,R,X,O)$, where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $S$ is the set of states in the environment,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $A$ is the set of actions available to the agent,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $T:S\times S\times A\rightarrow[0,1]$ is the stochastic transition function,
    with $T(s_{+}|s,a)$ denoting the probability of transitioning to state $s_{+}$
    given state $s$ and action $a$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $R:S\times A\times S\rightarrow\mathbb{R}$ is the reward function. $R(s,a,s_{+})$
    is received by the agent for taking action $a$ in state $s$ and moving to state
    $s_{+}$,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $X$ is the set of observations as perceived by the agent,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $O:S\times X\rightarrow[0,1]$ is the observation function, with $O(x|s)$ denoting
    the probability of observing $x$ given state $s$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A step in the environment represented by the POMDP $\Omega$ is represented by
    the transition $(s_{t},x_{t},a_{t},s_{t+1})$, where $s_{t}$ stands for the sate,
    $x_{t}$ the observation of this state, $a_{t}$ the action applied by the agent,
    $s_{t+1}$ the next state after transition. In this paper, we will use the POMDP
    framework as a general model, even though some environments could be described
    as MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Fundamentals of Reinforcement Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In RL, the goal is to learn a policy $\pi:A\times S\rightarrow[0,1]$, $\pi(a|s)$
    denoting the probability of selecting the action $a$ given state $s$. The optimal
    policy, denoted as $\pi^{*}$, therefore maximizes the expected cumulative discounted
    reward :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)]$ |  |'
  prefs: []
  type: TYPE_TB
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(\tau)=\sum_{t=0}^{&#124;\tau&#124;}\gamma^{t}R(s_{t},a_{t},s_{t+1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau=(s_{0},a_{0},s_{1},...,s_{|\tau|})$ is sampled from the distribution
    $\pi^{\Omega}$ of trajectories obtained by executing policy $\pi$ in environment
    $\Omega$. The discount factor $\gamma$, ranging from 0 to 1, weights the importance
    of future rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important criterion for defining optimality is the state value function,
    denoted as $V^{\pi}:S\rightarrow\mathbb{R}$. For a state $s$, the value $V^{\pi}(s)$
    represents the expected cumulative discounted reward starting from $s$ and following
    the policy $\pi$ thereafter. This can be formally expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V^{\pi}(s)=\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)&#124;s_{0}=s]$ |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: 'It can be expressed recursively with the Bellman equation :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}V^{\pi}(s)=\\ \scalebox{0.95}{$\displaystyle\sum_{a}\pi(a&#124;s)\sum_{s_{+}}T(s_{+}&#124;s,a)\Big{(}R(s,a,s_{+})+\gamma
    V^{\pi}(s_{+})\Big{)}$}\end{gathered}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, the state-action value function $Q^{\pi}:S\times A\rightarrow R$ is
    used in many algorithms as an alternative to $V^{\pi}$. The Q-value function of
    a state $s$ and action $a$ is the expected cumulative discounted reward, starting
    from $s$, taking $a$, and following $\pi$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{\tau\sim\pi^{\Omega}}[R(\tau)&#124;s_{0}=s,a_{0}=a]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'It can be expressed recursively with the equation :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{gathered}Q^{\pi}(s,a)=\\ \scalebox{0.95}{$\displaystyle\sum_{s_{+}}T(s_{+}&#124;s,a)\Big{(}R(s,a,s_{+})+\gamma\sum_{a_{+}}\pi(a_{+}&#124;s_{+})\big{[}Q^{\pi}(s_{+},a_{+})\big{]}\Big{)}$}\end{gathered}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'In the POMDP setting, since states are not directly observable by agents, the
    practice is to base policies and value functions on the history of observations
    (i.e., $x_{0:t}$ at step $t$) in place of the true state of the system (i.e.,
    $s_{t}$). For the ease of notations, we consider in the following policies and
    value functions defined with only the last observation as input (i.e., $x_{t}$),
    while every approach presented below can be extended to methods leveraging full
    histories of observations. More specifically, we consider in the following policies
    defined as $\pi:A\times X\rightarrow[0;1]$ and action-value functions as $Q:A\times
    X\rightarrow\mathbb{R}$. Figure [2](#S2.F2 "Figure 2 ‣ 2.1.2 Fundamentals of Reinforcement
    Learning ‣ 2.1 Reinforcement Learning ‣ 2 Background ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") shows the flowchart
    of an agent with a policy function $\pi$ interacting with a POMDP environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29f2cd9575252cbc21ef8ad7edc3ecda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Flowchart of an agent with a policy function $\pi$ interacting with
    a POMDP environment'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Neural Networks and Deep Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To solve the complex task of RL problems in a large input space and enable generalization,
    RL methods are combined with DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Deep Neural Networks (DNNs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A neural network is a system of interconnected nodes (neurons) that process
    and transmit signals. DNNs are models utilizing multiple layers of neurons, featuring
    varying degrees of architecture complexity, to analyze intricate data patterns.
    Training involves adjusting inter-neuron weights parameters to reduce errors (called
    loss function) between the network’s predictions and actual outcomes, often employing
    Stochastic Gradient Descent (SGD) inspired algorithms. This training refines the
    network’s ability to recognize and respond to input data accurately. The update
    rule of the parameters $\theta$ of the model $f_{\theta}$ in this context, given
    inputs $x$, labels $y$, learning rate $\alpha$ and loss function $\mathcal{L}$,
    is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta=\theta-\alpha\cdot\nabla_{\theta}\mathcal{L}(f_{\theta}(x),y)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 2.2.2 Deep Reinforcement Learning (DRL)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DRL combines the principles of RL with the capabilities of DNNs. The central
    concept in DRL is to construct a policy $\pi$ using a DNN. This can be achieved
    either by approximating the Q-function (as in Equation ([2](#S2.E2 "In 2.1.2 Fundamentals
    of Reinforcement Learning ‣ 2.1 Reinforcement Learning ‣ 2 Background ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey"))),
    the V-function (as in Equation ([1](#S2.E1 "In 2.1.2 Fundamentals of Reinforcement
    Learning ‣ 2.1 Reinforcement Learning ‣ 2 Background ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey"))), or by directly
    inferring the policy from experiences. There are several popular DRL algorithms,
    each with their specific strengths and weaknesses, some are better suited for
    specific context like discrete or continuous action space, or depending on possibility
    to train the DNNs on- or off-policy. The fundamental DRL algorithms are PG [[14](#bib.bib14)],
    DQN [[15](#bib.bib15)] and DDPG [[16](#bib.bib16)], but the most effective contemporary
    algorithms are Rainbow [[17](#bib.bib17)], PPO [[18](#bib.bib18)], SAC [[19](#bib.bib19)]
    or TQC [[20](#bib.bib20)] depending on the context.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Security challenges in DNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DNNs are a powerful tool now used in numerous real-world applications. However,
    their complex and highly non-linear structure make them hard to control, raising
    growing concerns about their reliabilities. Adv ML recently emerged to exhibit
    vulnerabilities of DNNs by processing attacks on their inputs that modify outcomes.
    NIST’s National Cybersecurity Center of Excellence (NCCE) [[21](#bib.bib21)] and
    its European counterpart, ETSI Standards [[22](#bib.bib22)], provide terminologies
    and ontologies to frame the study of these adversarial methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Adversarial Machine Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adversarial attacks are initially designed to exploit vulnerabilities in DNNs,
    threatening their privacy, availability, or integrity [[23](#bib.bib23)]. If the
    term adversarial attack suggests a malicious intention, it is to be noted that
    these methods may also be used by a model’s owner to improve its performances
    and assess its vulnerability. Indeed, adversarial ML aims to analyze the capabilities
    of potential attackers, comprehend the impact of their attacks and develop ML
    algorithms able to withstand these security threats. An adversary may act during
    the learning phase by poisoning the training data, or the inference phase modifying
    the inputs to evade decision. In this paper we consider robustness of already
    trained models, as well as leveraging adversarial examples as a defense method
    during the training phase in order to improve robustness at the inference phase,
    therefore we focuses on discussing model robustness to evasion methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Adversarial Examples [[21](#bib.bib21)]
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The large number of dimensions of a DNNs input space inevitably leads to blind
    spots and high sensitivity to small perturbations. In the restrained domain of
    classification, Adversarial examples are slightly altered data instances, carefully
    crafted to trick the model into misclassification while staying undetected. Computation
    techniques range from costly hand-made modifications [[24](#bib.bib24)] to perturbations
    generated by complex algorithms, yet the fundamental objective of adversarial
    example generation remains simple and can be summarized in Equation ([3](#S2.E3
    "In 2.3.2 Adversarial Examples [21] ‣ 2.3 Security challenges in DNNs ‣ 2 Background
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")): given the original instance $x$, find the closest example $x^{\prime}$
    relative to the chosen metric $||.||$ that leads a model’s function $f_{\theta}$
    to change its output.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{x^{\prime}}&#124;&#124;x-x^{\prime}&#124;&#124;\quad s.t.\quad
    f_{\theta}(x)\neq f_{\theta}(x^{\prime})$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: A variety of perturbation methods exist for the supervised classification problem,
    depending on the adversary’s objective and the model’s constraints. A extensive
    overview of these methods can be found together with defense strategies in [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Security challenges in DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DRL enables agents to learn complex behaviors by interacting with their environment.
    However, this interaction introduces unique security challenges not fully encountered
    in traditional deep learning contexts. The dynamic nature of DRL, combined with
    the necessity for long-term strategic decision-making, exposes DRL systems to
    a range of security threats that can compromise their learning process, decision-making
    integrity, and overall effectiveness. These challenges are further exacerbated
    by the adversarial landscape, where attackers can manipulate the environment or
    the agent’s perception to induce faulty learning or decision-making. Addressing
    these challenges is crucial for deploying DRL in security-sensitive applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Safe RL Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Formulating the challenge of safe control in RL [[25](#bib.bib25)] merges insights
    from the realms of control theory and reinforcement learning, aiming to optimize
    a solution that balances task achievement with stringent safety standards. At
    the heart of this approach lies three critical elements: the dynamic system behavior
    encapsulated within a model of the agent, the objectives or targets of the control
    task expressed through a cost function, and a set of safety constraints that the
    solution must adhere to. The goal is to develop a policy or controller that is
    capable of producing the necessary actions to navigate the system towards its
    objectives, all while strictly complying with the predefined safety protocols.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Reality Gap, Real World Perturbations and Generalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Reality Gap [[7](#bib.bib7), [8](#bib.bib8)] refers to the divergence between
    the simulated training environments of DRL agents and the complex, unpredictable
    conditions they encounter in real-world applications. This discrepancy challenges
    not only the agent’s ability to generalize across different contexts but also
    presents a profound security vulnerability. Real-world perturbations—unexpected
    changes in the environment—can lead to degraded performance or entirely erroneous
    actions by the DRL agent, particularly when these agents are confronted with scenarios
    slightly different from their training conditions. Such perturbations may arise
    naturally, from unmodeled aspects of the environment, or be adversarially crafted,
    with the intent to exploit these generalization weaknesses and induce failures.
    Addressing the Reality Gap, thereby enhancing the agents’ ability to generalize
    effectively and securing them against both natural and adversarial perturbations,
    is crucial for the safe and reliable deployment of DRL systems in environments
    demanding high levels of security and robust decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3 Robust RL Control
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Robust RL control introduces an advanced framework for RL by incorporating elements
    of uncertainty, such as parametric variations and external disturbances, into
    the system dynamics [[26](#bib.bib26)]. This approach shifts the optimization
    focus towards minimizing the maximum possible loss, essentially preparing the
    system to handle the worst-case scenario efficiently. It does so through a min-max
    optimization strategy, where the goal is to find a control policy that minimizes
    the maximum expected cost.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\pi}\max_{\delta\in\Delta}J(\pi,\delta)$ |  |'
  prefs: []
  type: TYPE_TB
- en: Where $J(\pi,\delta)$ represents the expected cost (or loss) of policy $\pi$
    when subjected to perturbations $\delta$ introduced by the adversary. The set
    $\Delta$ defines the allowable perturbations or disturbances.
  prefs: []
  type: TYPE_NORMAL
- en: This methodology ensures that the control system remains effective and reliable
    even when faced with unpredictable changes or adverse conditions, thereby enhancing
    its robustness and resilience in uncertain environments. This framework for enhancing
    robust control in RL, can participate for generalization of the policies across
    conditions changes thus helping to bridge the reality gap and overcome real world
    perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.4 Adversarial Attacks of DRL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If adversarial attacks were historically developed for supervised image classification
    models, they were proven equally effective for DRL agents. [[27](#bib.bib27)]
    first established the vulnerability of DQNs to adversarial perturbations, their
    statement soon supported by further studies [[23](#bib.bib23)]. Moreover, the
    RL framework offers more adversarial possibilities than the simple adaptation
    of supervised methods. Indeed, various components of the POMDP can be found vulnerable
    (like observation or transition function) through various elements which could
    be critic entry points (observations, states, actions), while the long-term dependencies
    inherent to DRL raise complex security challenges. On the other hand, this higher
    level of adversarial latitude enables new defense strategies for improving the
    agents robustness. This survey explores how adversarial attacks in RL can be used
    to generate perturbations that result in the worst-case scenarios crucial for
    Robust RL Control.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Formalization and Scope
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The existence of adversarial examples poses a significant threat for DRL agents,
    particularly in applications where incorrect predictions can have serious consequences,
    such as autonomous driving or medical diagnosis. Developing robust DRL algorithms
    that can defend against adversarial attacks and bridge the reality gap is an important
    and active research area in the field.
  prefs: []
  type: TYPE_NORMAL
- en: This survey aims to identify and assess how using adversarial examples during
    policy training can improve agent robustness. More specifically, we discuss the
    ability of various types of adversarial generation strategies to help anticipate
    the reality gap, which refers to a discrepancy between the training environment
    (e.g., a simulator) and the deployment one (which can include perturbations, whether
    they can be adversarially generated or not).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The problem of Robustness in RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally speaking, we are interested in the following optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\Omega\sim\Phi(\Omega&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\Omega}(\tau)}[R(\tau)]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\Phi$ corresponds to the distribution of environments to which the agent
    is likely to be confronted when deployed (whether it adversarially considers $\pi$
    or not at test time), $\pi^{\Omega}(\tau)$ is the distribution of trajectories
    using the policy $\pi$ and the dynamics from $\Omega$, and $R(\tau)$ is the cumulative
    reward collected in $\tau$. While this formulation suggests meta-reinforcement
    learning, in our setting $\Phi(\Omega|\pi)$ is unknown at train time. The training
    setup is composed of a unique MDP on which the policy can be learned, which is
    usually the case for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a unique training POMDP $\Omega$, the problem of robustness we are interested
    in can be reformulated by means of an alteration distribution $\Phi(\phi|\pi)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\phi\sim\Phi(\phi&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\phi,{\Omega}}(\tau)}[R(\tau)]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\pi^{\phi,\Omega}$ is the distribution of trajectories using policy
    $\pi$ on $\phi(\Omega)$, standing as the MDP $\Omega$ altered by $\phi$. Generally
    speaking, we can set $\phi$ as a function that can alter any component of $\Omega$
    as $\phi(\Omega)=(\phi_{S}(S^{\Omega}),\phi_{A}(A^{\Omega}),\phi_{T}(T^{\Omega}),\phi_{R}(R^{\Omega}),\phi_{X}(X^{\Omega}),\phi_{O}(O^{\Omega}))$.
    As discussed below and also in [[28](#bib.bib28)], while $\phi$ can simultaneously
    affect any of these components, we particularly focus on two crucial components
    for robustness:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Observation alterations: $\phi_{O}$ denotes alterations of the observation
    function of $\Omega$. In the corresponding altered environment $\widetilde{\Omega}=(S^{\Omega},A^{\Omega},T^{\Omega},R^{\Omega},X^{\Omega},\phi_{O}(O^{\Omega}))$,
    the observation obtained from a state $s\in S^{\Omega}$ could differ from that
    in $\Omega$. This can result from an adversarial attacker, that perturb signals
    from sensors to induce failures, observation abilities from real world that might
    be different than in simulation, or even unexpected failures of some sensors.
    These perturbations only induce perception alterations for $\pi$, without any
    effect on the true internal state of the system in the environment. Occurring
    at a specific step $t$ of a trajectory $\tau$, such alteration thus only impacts
    the future of $\tau$ if it induces changes in the policy decision at $t$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynamics alterations: $\phi_{T}$ denotes alterations of the transition function
    of $\Omega$. In the corresponding altered environment $\widetilde{\Omega}=(S^{\Omega},A^{\Omega},\phi_{T}(T^{\Omega}),R^{\Omega},X^{\Omega},O^{\Omega})$,
    dynamics are modified, such that actions have not the exactly same effect as in
    $\Omega$. This can result from an adversarial attacker, that modifies components
    of the environment to induce failures, from real world physics, that might be
    different than those from the training simulator, or from external events, that
    can incur unexpected situations. Dynamics alterations act on trajectories by modifying
    the resulting state $s_{t+1}$ emitted by the transition function $T$ at any step
    $t$. Even when localized at a single specific step $t$ of a trajectory, they thus
    impact its whole future.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this work, we do not explicitly address variation of other components ($S$,
    $A$, $R$ and $X$), as they usually pertain to different problem areas. $\phi_{S}$
    (resp. $\phi_{A}$) denotes alterations of the state (resp. action) set, where
    states (resp. actions) can be removed or introduced in $S^{\Omega}$ (resp. $A^{\Omega}$).
    $\phi_{X}$ denotes alterations of the observation support $X^{\Omega}$. While
    some perturbations of dynamics $\phi_{T}$ or observations $\phi_{O}$ can lead
    the agent to reach new states or observations never considered during training
    (which corresponds to implicit $\phi_{S}$ or $\phi_{X}$ perturbations), $\phi_{S}$,
    $\phi_{A}$, and $\phi_{X}$ all correspond to support shifts, related to static
    Out-Of-Domain issues, which we do not specifically focus on in this work. $\phi_{R}$
    denotes alterations of the reward function, which does not pose problem of robustness
    in usage, since the reward function is only used during training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Adversarial Attacks for Robust RL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Following distributionally robust optimization (DRO) principles [[29](#bib.bib29)],
    unknown distribution shifts can be anticipated by considering worst-case settings
    in some uncertainty sets ${\cal R}$. In our robust RL setting, this comes down
    to the following max-min optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{*}=\arg\max_{\pi}\min_{\tilde{\Phi}\in{\cal R}}\mathbb{E}_{\phi\sim\tilde{\Phi}(\phi&#124;\pi)}\mathbb{E}_{\tau\sim\pi^{\phi,{\Omega}}(\tau)}[R(\tau)]$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where ${\cal R}$ is a set of perturbation distributions. As well-known in DRO
    literature for supervised regression problems, the shape of ${\cal R}$ has a strong
    impact on the corresponding optimal decision system. In our RL setting, increasing
    the level of disparities allowed by the set ${\cal R}$ constrains the resulting
    policy $\pi$ to have to perform simultaneously over a broader spectrum of environmental
    conditions. While this enables better generalization for environmental shifts,
    it also implies to deal with various highly unrealistic scenarios if the set $\cal
    R$ is not restricted on reasonable levels of perturbations. With extremely large
    sets ${\cal R}$, the policy $\pi$ is expected to be equally effective for any
    possible environment, eventually converging to a trivial uniform policy, that
    allocates equal probability to every action for any state from $S^{\Omega}$. The
    shape of $\cal R$ has thus to be controlled to find an accurate trade-off between
    generalization and effectiveness. This is done in the following by setting restricted
    supports of perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our setting, dealing with worst-case distributions of perturbations defined
    over full supports of $\Omega$ is highly intractable in most realistic applications.
    In this survey, we rather focus on adversarial training that leverage the simultaneous
    optimization of an attacker agent $\xi$, that produces perturbations for situations
    reached by the protagonist $\pi$, by acting on adversarial actions $A_{\xi}^{\Omega}$
    that the environment $\Omega$ permits:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\pi^{*}=\arg\max_{\pi}\mathbb{E}_{\tau\sim\pi^{\xi^{*},\Omega}(\tau)}[R(\tau)]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.\qquad\xi^{*}=\arg\min_{\xi}\Delta^{\pi,\Omega}(\xi)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\Delta^{\pi,\Omega}(\xi)$ stands as the optimization objective of the
    adversarial agent given $\pi$ and the training environment $\Omega$, which ranges
    from adverse reward functions to divergence metrics (c.f., section [4.3](#S4.SS3
    "4.3 Adversarial Objective ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")),
    and $\pi^{\xi,\Omega}(\tau)$ corresponds to the probability of a trajectory following
    policy $\pi$ in a POMDP dynamically modified by an adversarial agent $\xi$, given
    a set of actions $A_{\xi}^{\Omega}=(A_{\xi,X}^{\Omega},A_{\xi,A}^{\Omega},A_{\xi,S}^{\Omega},A_{\xi,T}^{\Omega},A_{\xi,S+}^{\Omega})$.
    The action $a^{\xi}_{t}=(a^{\xi,X}_{t},a^{\xi,A}_{t},a^{\xi,S}_{t},a^{\xi,T}_{t},a^{\xi,S+}_{t})$
    of adversary $\xi$ can target any element of any transition $\tau_{t}=(s_{t},x_{t},a_{t},s_{t+1})$
    of trajectories in $\Omega$. While any perturbation of $x_{t}$ induce an alteration
    of the observation function $O^{\Omega}$, any perturbation of $s_{t}$, $a_{t}$
    or $s_{t+1}$ induce an alteration of the transition function $T^{\Omega}$ (either
    directly, through its internal dynamics, or indirectly via the modification of
    its inputs or outputs).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this setting, any trajectory is composed as a sequence of adversary-augmented
    transitions $\widetilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$,
    where the elements $x^{\prime}_{t}$ (resp. $a^{\prime}_{t}$) stands for the perturbed
    observation (resp. action) produced by the application of the adversary action
    $a^{\xi,X}_{t}$ (resp. $a^{\xi,A}_{t}$) at step $t$. $\widetilde{s}_{t}$ (resp.
    $\widetilde{s}_{t+1}$) stands for the intermediary state produced by the application
    of the adversary action $a^{\xi,S}_{t}$ (resp. $a^{\xi,T}_{t}$) at step $t$ before
    (resp. during) the transition function, and $\widetilde{x}_{t}$ (resp. $\widetilde{x}_{t+1}$)
    is the observation of this state. Finally $s_{t+1}$ stands for the final next
    state produced by the application of the adversary action $a^{\xi,S+}_{t}$ after
    the transition function, its observation is $x_{t+1}$. The support and scope of
    adversarial actions define the level of perturbations allowed in the corresponding
    uncertainty set ${\cal R}$ from ([4](#S3.E4 "In 3.2 Adversarial Attacks for Robust
    RL ‣ 3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey")), with impacts on the generalization/accuracy
    trade-off of the resulting policy $\pi$. While the protagonist agent $\pi$ acts
    from $x_{t}$ with $a_{t}\sim\pi(\cdot|x_{t})$, in the following, we consider the
    general case of adversaries $\xi$ that act from $s_{t}$, $x_{t}$ and $a_{t}$,
    that is $\xi:S^{\Omega}\times X^{\Omega}\times A^{\Omega}\times A_{\xi}^{\Omega}\rightarrow[0;1]$
    where $a^{\xi}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$. By doing this we consider
    adversaries $\xi$ that have full knowledge of the environment, observation and
    action, while this could be easily limited to adversarial policies $\xi$ that
    act only from partial information.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Taxonomy of Adversarial Attacks of DRL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conduct a systematic analysis of adversarial attacks for RL agents, with
    a focus on their purposes and applications. To better grasp the variety of methods
    available and their specificities, we propose a taxonomy of adversarial attacks
    for DRL. This taxonomy is used to categorize the adversarial attack as previously
    shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") and later described
    in Table [1](#S5.T1 "Table 1 ‣ Classical Adversarial Policy Learning Based Methods
    ‣ 5.2.3 Other Objective ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").
    This section discusses the different components of adversarial approaches for
    robust RL, before developing main approaches in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Perturbed Element
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An adversarial attacks is a method that use an adversarial action $a^{\xi}_{t}\in
    A_{\xi}^{\Omega}$ emitted by the adversary agent $\xi$ at step $t$, to produce
    a perturbation in the simulation during the trajectory of an agent. Given the
    type of attack, an action $a^{\xi}_{t}$ can directly perturb different elements
    :'
  prefs: []
  type: TYPE_NORMAL
- en: The observations $x_{t}$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Via a perturbation function $O^{\xi}:X^{\Omega}\times X^{\Omega}\times A_{\xi,X}^{\Omega}\rightarrow[0;1]$,
    where $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: The actions $a_{t}$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Via a perturbation function $A^{\xi}:A^{\Omega}\times A^{\Omega}\times A_{\xi,A}^{\Omega}\rightarrow[0;1]$
    where $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: The current state $s_{t}$ (before transition)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Via an additional transition functions $T_{\xi-}^{\Omega}:S^{\Omega}\times S^{\Omega}\times
    A_{\xi,S}^{\Omega}\rightarrow[0;1]$ where $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    is applied after the decision $a_{t}$ of the agent is taken and before the main
    transition function of the environment is applied.
  prefs: []
  type: TYPE_NORMAL
- en: The transition function $T^{\Omega}$
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Via an adversarially augmented transition functions $T_{\xi}^{\Omega}:S^{\Omega}\times
    S^{\Omega}\times A^{\Omega}\times A_{\xi,T}^{\Omega}\rightarrow[0;1]$ where $\widetilde{s}_{t+1}\sim
    T_{\xi}^{\Omega}(\cdot|s_{t},a_{t},a^{\xi,T}_{t})$ is applied as substitute of
    the main transition function of the environment $T^{\Omega}$.
  prefs: []
  type: TYPE_NORMAL
- en: The next state $s_{t+1}$ (after transition)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Via an additional transition functions $T_{\xi+}^{\Omega}:S^{\Omega}\times S^{\Omega}\times
    A_{\xi,S+}^{\Omega}\rightarrow[0;1]$ where $s_{t+1}\sim T_{\xi+}^{\Omega}(\cdot|s_{t+1},a^{\xi,S+}_{t})$
    is applied after the main transition function of the environment and before the
    next decision $a_{t+1}$ of the agent is taken.
  prefs: []
  type: TYPE_NORMAL
- en: The perturbations on the two first types of elements (observation and action)
    require just to modify a vector which will be fed as input of another function,
    so they are easy to implement in any environment. The perturbations on the three
    last types of elements (state, transition function and next state) are more complex
    and require to modify the environment itself, either by being able to modify the
    state with an additional transition function, or being able to modify the main
    transition function itself by incorporating the effect of the adversary action.
  prefs: []
  type: TYPE_NORMAL
- en: Here and in the following, we use the term perturb to denote direct modification
    of an element by an adversarial action $a^{\xi}_{t}$. For example, direct perturbation
    of an observation $x_{t}$, perturbation of a state $s_{t}$, perturbations of an
    action $a_{t}$. We do not use the term perturbations for indirect modifications,
    for example by directly perturbing an observation $x_{t}$, the action $a_{t}$
    chosen by the agent could be modifies, this new action cannot be seen as a perturbed
    action but results from the application of the policy $\pi$ of the agent given
    a perturbed observation $x^{\prime}_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following we use the term disturb to denote any perturbation of one
    of the following elements : action, state, transition function or next state.
    More generally the term disturb is used to denote perturbations that modifies
    the dynamics of the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Altered POMDP Component
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the two main types of alterations $\phi$ that are discussed in previous
    section, the main axis of the taxonomy of approaches concerns the impact on the
    POMDP of actions that are emitted by adversary agents during training of $\pi$.
    Given adversarial elements defined in the previous section, we specify each possible
    perturbation independently to discuss each specific adversarial impact on the
    POMDP.
  prefs: []
  type: TYPE_NORMAL
- en: Here and in the following, we use the term alter to denote the modification
    of a component of the POMDP from the POV of the agent. For example, adding an
    adversarial attack that perturb the observations is an alteration of the observation
    function $O^{\Omega}$ of the POMDP from the POV of the agent. Alternatively, adding
    an adversarial attack that perturb the action, the state, the transition function
    or the next state, is an alteration of the transition function $T^{\Omega}$ of
    the POMDP (dynamics of the environment) from the POV of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Alteration of the Observations Function $O^{\Omega}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first type of component alteration is the alteration of the observation
    function $O^{\Omega}$ of the POMDP. Directly inspired from adversarial attacks
    in supervised machine learning, many methods are designed to modify the inputs
    that are perceived by the protagonist agent $\pi$. The principle is to modify
    the input vector of an agent, which can correspond for instance to the outputs
    of a sensor of a physical agent, like an autonomous vehicle. The observation is
    perturbed before the agent take any decision, so the agent get the perturbed observation
    and can be fooled.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, in the setting of an observation attack, the adversary $\xi$
    acts to produce a perturbed observation $x^{\prime}_{t}$ before it is fed as input
    to $\pi$, by perturbing the observation $x_{t}$ via the specific perturbation
    function $O^{\xi}(x^{\prime}_{t}|x_{t},a^{\xi,X}_{t})$ introduced in Section [4.1](#S4.SS1
    "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case, $\xi$ can be regarded as an adversary agent that acts by emitting
    adversarial actions $a^{\xi,X}_{t}\sim\xi(\cdot|s_{t},x_{t})$ with $a^{\xi,X}_{t}\in
    A^{\Omega}_{\xi,X}$, given $\pi$ in a POMDP defined as $\Omega^{\pi}=(S^{\Omega},A_{\xi,X}^{\Omega},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,X}_{t})$ is performed
    in four steps, as shown in Algorithm [1](#alg1 "Algorithm 1 ‣ 4.2.1 Alteration
    of the Observations Function 𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of
    Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,X}_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t},a^{\xi,X}_{t}$ $\triangleright$ state and adversary action2:sample
    $x_{t}\sim O^{\Omega}(\cdot|s_{t})$ $\triangleright$ observation3:sample $x^{\prime}_{t}\sim
    O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$ $\triangleright$ perturbed observation4:sample
    $a_{t}\sim\pi(\cdot|x^{\prime}_{t})$ $\triangleright$ agent action5:sample $s_{t+1}\sim
    T^{\Omega}(\cdot|s_{t},a_{t})$ $\triangleright$ next state after transition6:return
    $s_{t+1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Reversely, agent $\pi$ acts on an altered POMDP $\Omega^{\xi}=(S^{\Omega},A^{\Omega},T^{\Omega},R^{\Omega},X^{\Omega},O^{\xi,\Omega})$
    where the input observation $x^{\prime}_{t}\sim O^{\xi,\Omega}(\cdot|s_{t})$ is
    performed in three steps, as shown in Algorithm [2](#alg2 "Algorithm 2 ‣ 4.2.1
    Alteration of the Observations Function 𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4
    Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 $x^{\prime}_{t}\sim O^{\xi,\Omega}(\cdot|s_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t}$ $\triangleright$ state2:sample $x_{t}\sim O^{\Omega}(\cdot|s_{t})$
    $\triangleright$ observation3:sample $a^{\xi,X}_{t}\sim\xi(\cdot|s_{t},x_{t})$
    $\triangleright$ adversary action4:sample $x^{\prime}_{t}\sim O^{\xi}(\cdot|x_{t},a^{\xi,X}_{t})$
    $\triangleright$ perturbed observation5:return $x^{\prime}_{t}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ 4.2.1 Alteration of the Observations Function
    𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey") presents a flowchart illustrating how the observation perturbation
    integrates into the POMDP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0399df22e568e31d7598a77da8d98ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Flowchart of the perturbation of the observation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, the probability of an adversary-augmented transition $\tilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$
    given past $\tilde{\tau}_{0:t-1}$, is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\pi^{\xi,\Omega}(\tau_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\xi(a^{\xi,X}_{t}&#124;s_{t},x_{t})O^{\xi}(x^{\prime}_{t}&#124;x_{t},a^{\xi,X}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\delta_{a_{t}}(a^{\prime}_{t})\\
    \delta_{s_{t}}(\widetilde{s}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})T^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t})O^{\Omega}(\widetilde{x}_{t+1}&#124;s_{t+1})\delta_{\widetilde{s}_{t+1}}(s_{t+1})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\delta_{x}$ stands for a Dirac distribution centered on $x$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Alteration of the Transition Function $T^{\Omega}$ (Environment Dynamics)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The other type of component alteration is the alteration of the transition function
    $T^{\Omega}$ of the POMDP (altering the dynamics of the environment). The principle
    is to modify effects of actions of the protagonist in the environment. For example,
    this can include moving or modifying the behavior of some physical objects in
    the environment, like modifying the positions or speed of some vehicles in autonomous
    driving simulator, or modifying the way the protagonist’s actions are affecting
    the environment (e.g. by amplifying or reversing actions).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is done by emitting adversarial actions $A^{\Omega}_{\xi}$, that are allowed
    by the environment $\Omega$ through a specific adversary function $A^{\xi}$, $T_{\xi-}^{\Omega}$,
    $T_{\xi}^{\Omega}$ or $T_{\xi+}^{\Omega}$, creating an altered transition function
    $T^{\xi,\Omega}$ for the protagonist actions. In that setting, four types of adversaries
    can be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: Transition Perturbation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this setting, the process begins with the agent in an initial state. The
    agent then chooses an action, which is applied to the environment. This should
    leads to a transition to a new state, according to the environment’s transition
    function. However, this transition function is perturbed, effectively altering
    the dynamics of the environment. Resulting in a different new subsequent state
    than if the transition had not been perturbed.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the context of an autonomous vehicle, the vehicle might decide
    to change lanes (action) based on the existing traffic setup (state). As this
    action leads to a transition, the behavior of surrounding vehicles is unpredictably
    modified (perturbed transition). Consequently, the vehicle emerges in a new traffic
    configuration (next state) that is different from what would typically result
    from the chosen action.
  prefs: []
  type: TYPE_NORMAL
- en: This process introduces variability into the environment’s dynamics by directly
    changing the environment’s inherent transition function.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, the adversary $\xi$ acts to induce an altered next state $s^{\prime}_{t+1}$
    by modifying the transition function itself, replacing it with the perturbed transition
    function $T_{\xi}^{\Omega}(s_{t+1}|(s_{t},a_{t}),a^{\xi,T}_{t})$ introduced in
    Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). In that case, $\xi$ can be regarded as an agent that acts by emitting
    adversarial actions $a^{\xi,T}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$, given $\pi$
    in a POMDP defined as: $\Omega^{\pi}=((S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,T},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,T}_{t})$
    is performed in three steps, as shown in Algorithm [3](#alg3 "Algorithm 3 ‣ Transition
    Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics)
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,T}_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t},a_{t},a^{\xi,T}_{t}$ $\triangleright$ state, agent action and
    adversary action2:sample $s_{t+1}\sim T_{\xi}^{\Omega}(\cdot|s_{t},a_{t},a^{\xi,T}_{t})$
    $\triangleright$ next state after perturbed transition3:sample $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$
    $\triangleright$ next observation4:sample $a_{t+1}\sim\pi(\cdot|x_{t+1})$ $\triangleright$
    next agent action5:return $s_{t+1}$, $a_{t+1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ Transition Perturbation ‣ 4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the transition perturbation integrates into the POMDP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3891f8756f6314bc9bf6551b3524f64c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Flowchart of the perturbation of the transition function'
  prefs: []
  type: TYPE_NORMAL
- en: Current State Perturbation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this setting, the process begins with the agent in an initial state. The
    agent then chooses an action to be applied within the environment. However, before
    this action is applied, the current state is subjected to a perturbation. This
    perturbation alters the initial state, leading to a modified state in which the
    chosen action is applied. The application of the action in this perturbed state
    results in a transition, resulting in a new subsequent state according to the
    environment’s transition function.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an autonomous vehicle deciding to change lanes (action)
    based on the prevailing traffic configuration (state). Prior to executing this
    maneuver, the traffic configuration is altered (perturbed state), such as by adjusting
    the positions of nearby vehicles. Consequently, when the vehicle executes its
    lane change, it does so in this adjusted traffic scenario, leading to a different
    traffic configuration (next state) than if the original state had not been modified.
  prefs: []
  type: TYPE_NORMAL
- en: This process introduces variability into the environment’s dynamics without
    necessitating a direct modification of the environment’s transition function.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, the adversary $\xi$ acts to induce an altered next state $s^{\prime}_{t+1}$
    by perturbing the state before the transition function via the prior transition
    function $T_{\xi-}^{\Omega}(\widetilde{s}_{t}|s_{t},a^{\xi,S}_{t})$ introduced
    in Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). In that case, $\xi$ can be regarded as an agent that acts by emitting
    adversarial actions $a^{\xi,S}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$, given $\pi$
    in a POMDP defined as: $\Omega^{\pi}=((S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,S},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,S}_{t})$
    is performed in four steps, as shown in Algorithm [4](#alg4 "Algorithm 4 ‣ Current
    State Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment
    Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of
    DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,S}_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t},a_{t},a^{\xi,S}_{t}$ $\triangleright$ state, agent action and
    adversary action2:sample $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    $\triangleright$ perturbed state3:sample $s_{t+1}\sim T^{\Omega}(\cdot|\widetilde{s}_{t},a_{t})$
    $\triangleright$ next state after transition4:sample $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$
    $\triangleright$ next observation5:sample $a_{t+1}\sim\pi(\cdot|x_{t+1})$ $\triangleright$
    next agent action6:return $s_{t+1}$, $a_{t+1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ Current State Perturbation ‣ 4.2.2 Alteration
    of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the current state perturbation integrates into the POMDP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f3a3e2a17a360fe350d608e70031d62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Flowchart of the perturbation of the current state'
  prefs: []
  type: TYPE_NORMAL
- en: Next State Perturbation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this setting, the process begins with the agent in an initial state. The
    agent chooses an action which is then applied in the environment. This leads to
    transition to a new subsequent state, according to the environment’s transition
    function. However, before the agent can choose its next action, this new state
    is perturbed.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the case of an autonomous vehicle, the vehicle might choose
    to change lanes (action) based on the current traffic configuration (state). After
    the action is executed, the vehicle finds itself in a new traffic configuration
    (next state). Before choosing the next action, this new state is perturbed, for
    example, by altering the positions of surrounding vehicles. This means the vehicle
    now faces a modified traffic configuration (perturbed next state) from which it
    must decide its next move.
  prefs: []
  type: TYPE_NORMAL
- en: This process introduces variability into the environment’s dynamics without
    necessitating a direct modification of the environment’s transition function.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference between perturbing the current state and perturbing the next
    state lies in the agent’s awareness of its situation. In current state perturbation,
    the agent lacks true knowledge of its precise state when choosing the action because
    this state is modified just before the action is applied. However, in next state
    perturbation, the agent has full awareness of its current state when choosing
    the action.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, the adversary $\xi$ acts to produce an altered next state $s^{\prime}_{t+1}$
    by perturbing the state after the transition function via the posterior transition
    function $T_{\xi+}^{\Omega}(\widetilde{s}_{t+1}|s_{t+1},a^{\xi,S+}_{t})$ introduced
    in Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial Attacks
    of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). In that case, $\xi$ can be regarded as an agent that acts by emitting
    adversarial actions $a^{\xi,S+}_{t}\sim\xi(\cdot|s_{t},x_{t})$, given $\pi$ in
    a POMDP defined as: $\Omega^{\pi}=(S^{\Omega},A^{\Omega}_{\xi,S+},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega})$,
    where sampling $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$ is performed
    in three steps, as shown in Algorithm [5](#alg5 "Algorithm 5 ‣ Next State Perturbation
    ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2
    Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5 $s_{t+1}\sim T^{\pi,\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t},a^{\xi,S+}_{t}$ $\triangleright$ state, adversary action2:sample
    $\widetilde{s}_{t}\sim T_{\xi+}^{\Omega}(\cdot|s_{t},a^{\xi,S+}_{t})$ $\triangleright$
    perturbed state3:sample $\widetilde{x}_{t}\sim O^{\Omega}(\cdot|\widetilde{s}_{t})$
    $\triangleright$ observation4:sample $a_{t}\sim\pi(\cdot|x_{t})$ $\triangleright$
    agent action5:sample $s_{t+1}\sim T^{\Omega}(\cdot|\widetilde{s}_{t},a_{t})$ $\triangleright$
    next state after transition6:return $s_{t+1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ Next State Perturbation ‣ 4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the next state perturbation integrates into the POMDP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56e72c58ab14589a358011dd14f38c12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Flowchart of the perturbation of the next state'
  prefs: []
  type: TYPE_NORMAL
- en: Action Perturbation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this setting, the process starts with the agent in an initial state. The
    agent chooses an action, which is intended to be applied in the environment. However,
    before this action can be applied, it undergoes a perturbation, resulting in a
    perturbed action. This perturbed action is then applied, leading to transition
    to a new state according to the environment’s transition function.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider an autonomous vehicle that decides to steer at an angle
    of $\alpha$ (action) based on the current traffic configuration (state). Before
    the steering action is executed, it is perturbed, so the actual steering angle
    applied to the vehicle becomes $\alpha+\epsilon$ (perturbed action). As a result,
    the vehicle transitions into a new traffic configuration (next state) that reflects
    the outcome of the perturbed steering action.
  prefs: []
  type: TYPE_NORMAL
- en: This process introduces variability into the environment’s dynamics without
    necessitating a direct modification of the environment’s transition function or
    modification of the state of the environment. However, this approach to modifying
    dynamics, while introducing variability, is confined to the scope of action perturbation,
    limiting the diversity of potential dynamics alterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, the adversary $\xi$ acts to induce an altered next state $s^{\prime}_{t+1}$
    by perturbing the action decided by the agent $a_{t}\sim\pi(\cdot|x_{t})$ via
    the specific perturbation function $A^{\xi}(a^{\prime}_{t}|a_{t},a^{\xi,A}_{t})$
    introduced in Section [4.1](#S4.SS1 "4.1 Perturbed Element ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In that case $\xi$ can be regarded as an agent that acts by emitting adversarial
    actions $a^{\xi,A}_{t}\sim\xi(\cdot|s_{t},x_{t},a_{t})$, given $\pi$ in a POMDP
    defined as: $\Omega^{\pi}=\big{(}(S^{\Omega},A^{\Omega}),A^{\Omega}_{\xi,A},T^{\pi,\Omega},R_{\xi}^{\Omega},X^{\Omega},O^{\Omega}\big{)}$,
    where sampling $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,A}_{t})$
    is performed in four steps, as shown in Algorithm [6](#alg6 "Algorithm 6 ‣ Action
    Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics)
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 6 $(s_{t+1},a_{t+1})\sim T^{\pi,\Omega}(\cdot|(s_{t},a_{t}),a^{\xi,A}_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t},a_{t},a^{\xi,A}_{t}$ $\triangleright$ state, agent action and
    adversary action2:sample $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$
    $\triangleright$ perturbed agent action3:sample $s_{t+1}\sim T^{\Omega}(\cdot|s_{t},a^{\prime}_{t})$
    $\triangleright$ next state after transition4:sample $x_{t+1}\sim O^{\Omega}(\cdot|s_{t+1})$
    $\triangleright$ next observation5:sample $a_{t+1}\sim\pi(\cdot|x_{t+1})$ $\triangleright$
    next agent action6:return $s_{t+1}$, $a_{t+1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the
    Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣
    4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") presents a flowchart illustrating
    how the action perturbation integrates into the POMDP.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8baa29bcab52d8570993df69d7d30163.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Flowchart of the perturbation of the action'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reversely, we can gather the point of view of the protagonist agent $\pi$ in
    a single example combining all four possible attacks just described by denoting
    the adversaries $\xi_{A}$, $\xi_{S-}$, $\xi_{T}$ and $\xi_{S+}$. The agent $\pi$
    acts on an altered POMDP $\Omega^{\xi}=(S^{\Omega},A^{\Omega},T^{\xi,\Omega},R^{\Omega},X^{\Omega},O^{\Omega})$,
    where $s_{t+1}\sim T^{\xi,\Omega}(\cdot|s_{t},a_{t})$ is performed in eleven steps,
    as shown in Algorithm [7](#alg7 "Algorithm 7 ‣ Action Perturbation ‣ 4.2.2 Alteration
    of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 7 $s_{t+1}\sim T^{\xi,\Omega}(\cdot|s_{t},a_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 1:Input $s_{t},a_{t}$ $\triangleright$ state and agent action2:sample $x_{t}\sim
    O^{\Omega}(\cdot|s_{t})$ $\triangleright$ observation3:sample $a^{\xi,A}_{t}\sim\xi_{A}(\cdot|s_{t},x_{t},a_{t})$
    $\triangleright$ adversary action A4:sample $a^{\xi,S}_{t}\sim\xi_{T-}(\cdot|s_{t},x_{t},a_{t})$
    $\triangleright$ adversary action S5:sample $a^{\prime}_{t}\sim A^{\xi}(\cdot|a_{t},a^{\xi,A}_{t})$
    $\triangleright$ perturbed action6:sample $\widetilde{s}_{t}\sim T_{\xi-}^{\Omega}(\cdot|s_{t},a^{\xi,S}_{t})$
    $\triangleright$ perturbed state7:sample $\widetilde{x}_{t}\sim O^{\Omega}(\cdot|\widetilde{s}_{t})$
    $\triangleright$ observation of the perturbed state8:sample $a^{\xi,T}_{t}\sim\xi_{T}(\cdot|\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})$
    $\triangleright$ adversary action T9:sample $\widetilde{s}_{t+1}\sim T_{\xi}^{\Omega}(\cdot|\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})$
    $\triangleright$ next state after perturbed transition10:sample $\widetilde{x}_{t+1}\sim
    O^{\Omega}(\cdot|\widetilde{s}_{t+1})$ $\triangleright$ observation of the next
    state11:sample $a^{\xi,S+}_{t}\sim\xi_{T+}(\cdot|\widetilde{s}_{t+1},\widetilde{x}_{t+1})$
    $\triangleright$ adversary action S+12:sample $s_{t+1}\sim T_{\xi+}^{\Omega}(\cdot|\widetilde{s}_{t+1},a^{\xi,S+}_{t})$
    $\triangleright$ perturbed next state13:return $s_{t+1}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, the probability of an adversary-augmented transition $\tilde{\tau}_{t}=(s_{t},x_{t},a_{t},a^{\xi}_{t},x^{\prime}_{t},a^{\prime}_{t},\widetilde{s}_{t},\widetilde{x}_{t},\widetilde{s}_{t+1},\widetilde{x}_{t+1},s_{t+1})$
    given past $\tilde{\tau}_{0:t-1}$, is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.SS2.SSS2.Px4.p9.1.m1.203" class="ltx_Math" alttext="\pi^{\xi,\Omega}(\tilde{\tau}_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\delta_{x_{t}}(x^{\prime}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\xi_{A}(a^{\xi,A}_{t}&#124;s_{t},x_{t},a_{t})A^{\xi}(a^{\prime}_{t}&#124;a_{t},a^{\xi,A}_{t})\\
    \xi_{S-}(a^{\xi,S}_{t}&#124;s_{t},x_{t},a_{t})T_{\xi-}^{\Omega}(\widetilde{s}_{t}&#124;s_{t},a^{\xi,S}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})\xi_{T}(a^{\xi,T}_{t}&#124;\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})T_{\xi}^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})\\'
  prefs: []
  type: TYPE_NORMAL
- en: O^{\Omega}(\widetilde{x}_{t+1}&#124;\widetilde{s}_{t+1})\xi_{S+}(a^{\xi,S+}_{t}&#124;\widetilde{s}_{t+1},\widetilde{x}_{t+1})T_{\xi+}^{\Omega}(s_{t+1}&#124;\widetilde{s}_{t+1},a^{\xi,S+}_{t})"
    display="block"><semantics id="S4.SS2.SSS2.Px4.p9.1.m1.203a"><mtable displaystyle="true"
    rowspacing="0pt" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mtr
    id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28a" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28b"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msup id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.1.1.1.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.1.1.1.1.1.1.cmml">π</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.3.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.1.cmml">ξ</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.4.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.3.cmml">,</mo><mi mathvariant="normal"
    id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.2.cmml">Ω</mi></mrow></msup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.3.3.3.3.3.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66.1.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.2.cmml">τ</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.1.cmml">~</mo></mover><mi id="S4.SS2.SSS2.Px4.p9.1.m1.5.5.5.5.5.5.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.5.5.5.5.5.5.1.cmml">t</mi></msub><mo fence="false"
    id="S4.SS2.SSS2.Px4.p9.1.m1.6.6.6.6.6.6" xref="S4.SS2.SSS2.Px4.p9.1.m1.6.6.6.6.6.6.cmml">&#124;</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.190.190.15.176.66.66.66.1.1.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.7.7.7.7.7.7" xref="S4.SS2.SSS2.Px4.p9.1.m1.7.7.7.7.7.7.cmml">s</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.8.8.8.8.8.8.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.8.8.8.8.8.8.1.cmml">t</mi></msub></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.9.9.9.9.9.9" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow></mrow><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.10.10.10.10.10.10" xref="S4.SS2.SSS2.Px4.p9.1.m1.10.10.10.10.10.10.cmml">=</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msup
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.7" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.11.11.11.11.11.11" xref="S4.SS2.SSS2.Px4.p9.1.m1.11.11.11.11.11.11.cmml">O</mi><mi
    mathvariant="normal" id="S4.SS2.SSS2.Px4.p9.1.m1.12.12.12.12.12.12.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.12.12.12.12.12.12.1.cmml">Ω</mi></msup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.191.191.16.177.67.67.67.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.13.13.13.13.13.13"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.191.191.16.177.67.67.67.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.191.191.16.177.67.67.67.1.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.14.14.14.14.14.14"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.14.14.14.14.14.14.cmml">x</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.15.15.15.15.15.15.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.15.15.15.15.15.15.1.cmml">t</mi></msub><mo fence="false"
    id="S4.SS2.SSS2.Px4.p9.1.m1.16.16.16.16.16.16" xref="S4.SS2.SSS2.Px4.p9.1.m1.16.16.16.16.16.16.cmml">&#124;</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.191.191.16.177.67.67.67.1.1.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.17.17.17.17.17.17" xref="S4.SS2.SSS2.Px4.p9.1.m1.17.17.17.17.17.17.cmml">s</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.18.18.18.18.18.18.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.18.18.18.18.18.18.1.cmml">t</mi></msub></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.19.19.19.19.19.19" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6a"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msub id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.8"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.20.20.20.20.20.20"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.20.20.20.20.20.20.cmml">δ</mi><msub id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.2.cmml">x</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.3.cmml">t</mi></msub></msub><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6b"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.192.192.17.178.68.68.68.2.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.22.22.22.22.22.22"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><msubsup id="S4.SS2.SSS2.Px4.p9.1.m1.192.192.17.178.68.68.68.2.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.23.23.23.23.23.23"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.23.23.23.23.23.23.cmml">x</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.25.25.25.25.25.25.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.25.25.25.25.25.25.1.cmml">t</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.24.24.24.24.24.24.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.24.24.24.24.24.24.1.cmml">′</mo></msubsup><mo stretchy="false"
    id="S4.SS2.SSS2.Px4.p9.1.m1.26.26.26.26.26.26" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6c"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mi id="S4.SS2.SSS2.Px4.p9.1.m1.27.27.27.27.27.27"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.27.27.27.27.27.27.cmml">π</mi><mo lspace="0em" rspace="0em"
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6d" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.193.193.18.179.69.69.69.3.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.28.28.28.28.28.28" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.193.193.18.179.69.69.69.3.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.193.193.18.179.69.69.69.3.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.29.29.29.29.29.29" xref="S4.SS2.SSS2.Px4.p9.1.m1.29.29.29.29.29.29.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.30.30.30.30.30.30.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.30.30.30.30.30.30.1.cmml">t</mi></msub><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.31.31.31.31.31.31" xref="S4.SS2.SSS2.Px4.p9.1.m1.31.31.31.31.31.31.cmml">&#124;</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.193.193.18.179.69.69.69.3.1.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.32.32.32.32.32.32" xref="S4.SS2.SSS2.Px4.p9.1.m1.32.32.32.32.32.32.cmml">x</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.34.34.34.34.34.34.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.34.34.34.34.34.34.1.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.33.33.33.33.33.33.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.33.33.33.33.33.33.1.cmml">′</mo></msubsup></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.35.35.35.35.35.35" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6e"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msub id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.9"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.36.36.36.36.36.36"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.36.36.36.36.36.36.cmml">ξ</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.37.37.37.37.37.37.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.37.37.37.37.37.37.1.cmml">A</mi></msub><mo lspace="0em"
    rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6f" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.38.38.38.38.38.38" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.39.39.39.39.39.39" xref="S4.SS2.SSS2.Px4.p9.1.m1.39.39.39.39.39.39.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.41.41.41.41.41.41.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.41.41.41.41.41.41.1.cmml">t</mi><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.3.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.1.cmml">ξ</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.4.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.3.cmml">,</mo><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.2.cmml">A</mi></mrow></msubsup><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.42.42.42.42.42.42" xref="S4.SS2.SSS2.Px4.p9.1.m1.42.42.42.42.42.42.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1.1.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.43.43.43.43.43.43" xref="S4.SS2.SSS2.Px4.p9.1.m1.43.43.43.43.43.43.cmml">s</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.44.44.44.44.44.44.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.44.44.44.44.44.44.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.45.45.45.45.45.45" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.46.46.46.46.46.46" xref="S4.SS2.SSS2.Px4.p9.1.m1.46.46.46.46.46.46.cmml">x</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.47.47.47.47.47.47.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.47.47.47.47.47.47.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.48.48.48.48.48.48" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.194.194.19.180.70.70.70.4.1.1.3.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.49.49.49.49.49.49" xref="S4.SS2.SSS2.Px4.p9.1.m1.49.49.49.49.49.49.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.50.50.50.50.50.50.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.50.50.50.50.50.50.1.cmml">t</mi></msub></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.51.51.51.51.51.51" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6g"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msup id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.10"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.52.52.52.52.52.52"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.52.52.52.52.52.52.cmml">A</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.53.53.53.53.53.53.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.53.53.53.53.53.53.1.cmml">ξ</mi></msup><mo lspace="0em"
    rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.6h" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.5.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.54.54.54.54.54.54" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.5.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.5.1.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.55.55.55.55.55.55" xref="S4.SS2.SSS2.Px4.p9.1.m1.55.55.55.55.55.55.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.57.57.57.57.57.57.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.57.57.57.57.57.57.1.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.56.56.56.56.56.56.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.56.56.56.56.56.56.1.cmml">′</mo></msubsup><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.58.58.58.58.58.58" xref="S4.SS2.SSS2.Px4.p9.1.m1.58.58.58.58.58.58.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.5.1.1.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.5.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.59.59.59.59.59.59" xref="S4.SS2.SSS2.Px4.p9.1.m1.59.59.59.59.59.59.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.60.60.60.60.60.60.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.60.60.60.60.60.60.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.61.61.61.61.61.61" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.195.195.20.181.71.71.71.5.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.62.62.62.62.62.62" xref="S4.SS2.SSS2.Px4.p9.1.m1.62.62.62.62.62.62.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.64.64.64.64.64.64.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.64.64.64.64.64.64.1.cmml">t</mi><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.3.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.1.cmml">ξ</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.4.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.3.cmml">,</mo><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.2.cmml">A</mi></mrow></msubsup></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.65.65.65.65.65.65" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28c" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mtd
    class="ltx_align_right" columnalign="right" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28d"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.81"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.66.66.66.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.66.66.66.1.1.1.cmml">ξ</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.2.cmml">S</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.3.cmml">−</mo></mrow></msub><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.68.68.68.3.3.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msubsup id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1.1.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.69.69.69.4.4.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.69.69.69.4.4.4.cmml">a</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.71.71.71.6.6.6.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.71.71.71.6.6.6.1.cmml">t</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.3.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.1.cmml">ξ</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.4.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.3.cmml">,</mo><mi id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.2.cmml">S</mi></mrow></msubsup><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.72.72.72.7.7.7" xref="S4.SS2.SSS2.Px4.p9.1.m1.72.72.72.7.7.7.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1.1.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.73.73.73.8.8.8" xref="S4.SS2.SSS2.Px4.p9.1.m1.73.73.73.8.8.8.cmml">s</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.74.74.74.9.9.9.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.74.74.74.9.9.9.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.75.75.75.10.10.10" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.76.76.76.11.11.11" xref="S4.SS2.SSS2.Px4.p9.1.m1.76.76.76.11.11.11.cmml">x</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.77.77.77.12.12.12.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.77.77.77.12.12.12.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.78.78.78.13.13.13" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.196.196.21.182.75.75.75.1.1.3.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.79.79.79.14.14.14" xref="S4.SS2.SSS2.Px4.p9.1.m1.79.79.79.14.14.14.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.80.80.80.15.15.15.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.80.80.80.15.15.15.1.cmml">t</mi></msub></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.81.81.81.16.16.16" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80a"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msubsup id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.82"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.82.82.82.17.17.17"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.82.82.82.17.17.17.cmml">T</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.2.cmml">ξ</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.3.cmml">−</mo></mrow><mi mathvariant="normal"
    id="S4.SS2.SSS2.Px4.p9.1.m1.84.84.84.19.19.19.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.84.84.84.19.19.19.1.cmml">Ω</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80b"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.197.197.22.183.76.76.76.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.85.85.85.20.20.20"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.197.197.22.183.76.76.76.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.197.197.22.183.76.76.76.1.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.2.cmml">s</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.1.cmml">~</mo></mover><mi id="S4.SS2.SSS2.Px4.p9.1.m1.87.87.87.22.22.22.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.87.87.87.22.22.22.1.cmml">t</mi></msub><mo fence="false"
    id="S4.SS2.SSS2.Px4.p9.1.m1.88.88.88.23.23.23" xref="S4.SS2.SSS2.Px4.p9.1.m1.88.88.88.23.23.23.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.197.197.22.183.76.76.76.1.1.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.197.197.22.183.76.76.76.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.89.89.89.24.24.24" xref="S4.SS2.SSS2.Px4.p9.1.m1.89.89.89.24.24.24.cmml">s</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.90.90.90.25.25.25.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.90.90.90.25.25.25.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.91.91.91.26.26.26" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.197.197.22.183.76.76.76.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.92.92.92.27.27.27" xref="S4.SS2.SSS2.Px4.p9.1.m1.92.92.92.27.27.27.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.94.94.94.29.29.29.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.94.94.94.29.29.29.1.cmml">t</mi><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.3.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.1.cmml">ξ</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.4.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.3.cmml">,</mo><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.2.cmml">S</mi></mrow></msubsup></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.95.95.95.30.30.30" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80c"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msup id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.83"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.96.96.96.31.31.31"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.96.96.96.31.31.31.cmml">O</mi><mi mathvariant="normal"
    id="S4.SS2.SSS2.Px4.p9.1.m1.97.97.97.32.32.32.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.97.97.97.32.32.32.1.cmml">Ω</mi></msup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80d"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.198.198.23.184.77.77.77.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.98.98.98.33.33.33"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.198.198.23.184.77.77.77.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.198.198.23.184.77.77.77.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.2.cmml">x</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.1.cmml">~</mo></mover><mi id="S4.SS2.SSS2.Px4.p9.1.m1.100.100.100.35.35.35.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.100.100.100.35.35.35.1.cmml">t</mi></msub><mo fence="false"
    id="S4.SS2.SSS2.Px4.p9.1.m1.101.101.101.36.36.36" xref="S4.SS2.SSS2.Px4.p9.1.m1.101.101.101.36.36.36.cmml">&#124;</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.198.198.23.184.77.77.77.1.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37" xref="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.2.cmml">s</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.1.cmml">~</mo></mover><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.103.103.103.38.38.38.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.103.103.103.38.38.38.1.cmml">t</mi></msub></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.104.104.104.39.39.39" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80e"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msub id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.84"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.105.105.105.40.40.40"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.105.105.105.40.40.40.cmml">ξ</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.106.106.106.41.41.41.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.106.106.106.41.41.41.1.cmml">T</mi></msub><mo lspace="0em"
    rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80f" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.107.107.107.42.42.42" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.108.108.108.43.43.43" xref="S4.SS2.SSS2.Px4.p9.1.m1.108.108.108.43.43.43.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.110.110.110.45.45.45.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.110.110.110.45.45.45.1.cmml">t</mi><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.3.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.1.cmml">ξ</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.4.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.3.cmml">,</mo><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.2.cmml">T</mi></mrow></msubsup><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.111.111.111.46.46.46" xref="S4.SS2.SSS2.Px4.p9.1.m1.111.111.111.46.46.46.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1.1.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47" xref="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.2.cmml">s</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.1.cmml">~</mo></mover><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.113.113.113.48.48.48.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.113.113.113.48.48.48.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.114.114.114.49.49.49" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50" xref="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.2.cmml">x</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.1.cmml">~</mo></mover><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.116.116.116.51.51.51.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.116.116.116.51.51.51.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.117.117.117.52.52.52" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.199.199.24.185.78.78.78.1.1.3.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.118.118.118.53.53.53" xref="S4.SS2.SSS2.Px4.p9.1.m1.118.118.118.53.53.53.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.120.120.120.55.55.55.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.120.120.120.55.55.55.1.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.119.119.119.54.54.54.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.119.119.119.54.54.54.1.cmml">′</mo></msubsup></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.121.121.121.56.56.56" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80g"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msubsup id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.85"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.122.122.122.57.57.57"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.122.122.122.57.57.57.cmml">T</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.123.123.123.58.58.58.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.123.123.123.58.58.58.1.cmml">ξ</mi><mi mathvariant="normal"
    id="S4.SS2.SSS2.Px4.p9.1.m1.124.124.124.59.59.59.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.124.124.124.59.59.59.1.cmml">Ω</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.80h"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.125.125.125.60.60.60"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1.1.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.2.cmml">s</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.1.cmml">~</mo></mover><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.2.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.1.cmml">+</mo><mn
    id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.3.cmml">1</mn></mrow></msub><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.128.128.128.63.63.63" xref="S4.SS2.SSS2.Px4.p9.1.m1.128.128.128.63.63.63.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1.1.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64" xref="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.2.cmml">s</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.1.cmml">~</mo></mover><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.130.130.130.65.65.65.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.130.130.130.65.65.65.1.cmml">t</mi></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.131.131.131.66.66.66" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.132.132.132.67.67.67" xref="S4.SS2.SSS2.Px4.p9.1.m1.132.132.132.67.67.67.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.134.134.134.69.69.69.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.134.134.134.69.69.69.1.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.133.133.133.68.68.68.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.133.133.133.68.68.68.1.cmml">′</mo></msubsup><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.135.135.135.70.70.70" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.200.200.25.186.79.79.79.1.1.3.3.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.136.136.136.71.71.71" xref="S4.SS2.SSS2.Px4.p9.1.m1.136.136.136.71.71.71.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.138.138.138.73.73.73.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.138.138.138.73.73.73.1.cmml">t</mi><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.4" xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.3.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.1.cmml">ξ</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.4.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.3.cmml">,</mo><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.2.cmml">T</mi></mrow></msubsup></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.139.139.139.74.74.74" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow></mrow></mtd></mtr><mtr
    id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28e" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mtd
    class="ltx_align_right" columnalign="right" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28f"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msup id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.41"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.140.140.140.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.140.140.140.1.1.1.cmml">O</mi><mi mathvariant="normal"
    id="S4.SS2.SSS2.Px4.p9.1.m1.141.141.141.2.2.2.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.141.141.141.2.2.2.1.cmml">Ω</mi></msup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.40"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.201.201.26.187.37.37.37.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.142.142.142.3.3.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.201.201.26.187.37.37.37.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.201.201.26.187.37.37.37.1.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.2.cmml">x</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.1.cmml">~</mo></mover><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.2.cmml">t</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.1.cmml">+</mo><mn id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.3.cmml">1</mn></mrow></msub><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.145.145.145.6.6.6" xref="S4.SS2.SSS2.Px4.p9.1.m1.145.145.145.6.6.6.cmml">&#124;</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.201.201.26.187.37.37.37.1.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7" xref="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.2.cmml">s</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.1.cmml">~</mo></mover><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.2.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.1.cmml">+</mo><mn
    id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.3.cmml">1</mn></mrow></msub></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.148.148.148.9.9.9" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.40a"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msub id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.42"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.149.149.149.10.10.10"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.149.149.149.10.10.10.cmml">ξ</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.2.cmml">S</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.3.cmml">+</mo></mrow></msub><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.40b"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.202.202.27.188.38.38.38.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.151.151.151.12.12.12"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.202.202.27.188.38.38.38.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msubsup id="S4.SS2.SSS2.Px4.p9.1.m1.202.202.27.188.38.38.38.1.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.152.152.152.13.13.13"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.152.152.152.13.13.13.cmml">a</mi><mi id="S4.SS2.SSS2.Px4.p9.1.m1.154.154.154.15.15.15.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.154.154.154.15.15.15.1.cmml">t</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.3.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.1.cmml">ξ</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.3.cmml">,</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.2.cmml">S</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.3.cmml">+</mo></mrow></mrow></msubsup><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.155.155.155.16.16.16" xref="S4.SS2.SSS2.Px4.p9.1.m1.155.155.155.16.16.16.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.202.202.27.188.38.38.38.1.1.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.202.202.27.188.38.38.38.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17" xref="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.2.cmml">s</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.1.cmml">~</mo></mover><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.2.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.1.cmml">+</mo><mn
    id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.3.cmml">1</mn></mrow></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.158.158.158.19.19.19" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.202.202.27.188.38.38.38.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20" xref="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.2.cmml">x</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.1.cmml">~</mo></mover><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.2.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.1.cmml">+</mo><mn
    id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.3.cmml">1</mn></mrow></msub></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.161.161.161.22.22.22" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.40c"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><msubsup id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.43"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.162.162.162.23.23.23"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.162.162.162.23.23.23.cmml">T</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.2.cmml">ξ</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.3.cmml">+</mo></mrow><mi
    mathvariant="normal" id="S4.SS2.SSS2.Px4.p9.1.m1.164.164.164.25.25.25.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.164.164.164.25.25.25.1.cmml">Ω</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.40d"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">​</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.39.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mo stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.165.165.165.26.26.26"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">(</mo><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.39.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.39.1.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.166.166.166.27.27.27"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.166.166.166.27.27.27.cmml">s</mi><mrow id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.cmml"><mi id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.2"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.2.cmml">t</mi><mo id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.1"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.1.cmml">+</mo><mn id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.3"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.3.cmml">1</mn></mrow></msub><mo
    fence="false" id="S4.SS2.SSS2.Px4.p9.1.m1.168.168.168.29.29.29" xref="S4.SS2.SSS2.Px4.p9.1.m1.168.168.168.29.29.29.cmml">&#124;</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.39.1.1.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><msub
    id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.39.1.1.1.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mover
    accent="true" id="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30" xref="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.2.cmml">s</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.1.cmml">~</mo></mover><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.2.cmml">t</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.1.cmml">+</mo><mn
    id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.3.cmml">1</mn></mrow></msub><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.171.171.171.32.32.32" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">,</mo><msubsup
    id="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28.189.39.39.39.1.1.2.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.172.172.172.33.33.33" xref="S4.SS2.SSS2.Px4.p9.1.m1.172.172.172.33.33.33.cmml">a</mi><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.174.174.174.35.35.35.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.174.174.174.35.35.35.1.cmml">t</mi><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.3.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.1.cmml">ξ</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.3.cmml">,</mo><mrow
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.cmml"><mi
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.2" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.2.cmml">S</mi><mo
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.3" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.3.cmml">+</mo></mrow></mrow></msubsup></mrow></mrow><mo
    stretchy="false" id="S4.SS2.SSS2.Px4.p9.1.m1.175.175.175.36.36.36" xref="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml">)</mo></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S4.SS2.SSS2.Px4.p9.1.m1.203b"><apply id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.3.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.3.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci id="S4.SS2.SSS2.Px4.p9.1.m1.1.1.1.1.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.1.1.1.1.1.1">𝜋</ci><list id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.3.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.4"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.1">𝜉</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.2.2.2.2.2.2.1.2">Ω</ci></list></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.1.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.6.6.6.6.6.6.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.6.6.6.6.6.6">conditional</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.1.1.1.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.1.1.1.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.1">~</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.4.4.4.4.4.4.2">𝜏</ci></apply><ci id="S4.SS2.SSS2.Px4.p9.1.m1.5.5.5.5.5.5.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.5.5.5.5.5.5.1">𝑡</ci></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.1.1.1.3.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.176.176.1.1.1.1.1.3.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci id="S4.SS2.SSS2.Px4.p9.1.m1.7.7.7.7.7.7.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.7.7.7.7.7.7">𝑠</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.8.8.8.8.8.8.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.8.8.8.8.8.8.1">𝑡</ci></apply></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.15.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.15.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.11.11.11.11.11.11.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.11.11.11.11.11.11">𝑂</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.12.12.12.12.12.12.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.12.12.12.12.12.12.1">Ω</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.177.177.2.2.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.16.16.16.16.16.16.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.16.16.16.16.16.16">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.177.177.2.2.1.1.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.177.177.2.2.1.1.1.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.14.14.14.14.14.14.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.14.14.14.14.14.14">𝑥</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.15.15.15.15.15.15.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.15.15.15.15.15.15.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.177.177.2.2.1.1.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.177.177.2.2.1.1.1.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.17.17.17.17.17.17.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.17.17.17.17.17.17">𝑠</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.18.18.18.18.18.18.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.18.18.18.18.18.18.1">𝑡</ci></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.16.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.16.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.20.20.20.20.20.20.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.20.20.20.20.20.20">𝛿</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.2">𝑥</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.21.21.21.21.21.21.1.3">𝑡</ci></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.178.178.3.3.2.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.178.178.3.3.2.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.178.178.3.3.2.1.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.178.178.3.3.2.1.1.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.23.23.23.23.23.23.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.23.23.23.23.23.23">𝑥</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.24.24.24.24.24.24.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.24.24.24.24.24.24.1">′</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.25.25.25.25.25.25.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.25.25.25.25.25.25.1">𝑡</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.27.27.27.27.27.27.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.27.27.27.27.27.27">𝜋</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.31.31.31.31.31.31.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.31.31.31.31.31.31">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.29.29.29.29.29.29.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.29.29.29.29.29.29">𝑎</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.30.30.30.30.30.30.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.30.30.30.30.30.30.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.3.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.179.179.4.4.3.1.1.3.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.32.32.32.32.32.32.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.32.32.32.32.32.32">𝑥</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.33.33.33.33.33.33.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.33.33.33.33.33.33.1">′</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.34.34.34.34.34.34.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.34.34.34.34.34.34.1">𝑡</ci></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.18.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.18.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.36.36.36.36.36.36.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.36.36.36.36.36.36">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.37.37.37.37.37.37.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.37.37.37.37.37.37.1">𝐴</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.42.42.42.42.42.42.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.42.42.42.42.42.42">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.5.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.5.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.5.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.5.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.39.39.39.39.39.39.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.39.39.39.39.39.39">𝑎</ci><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.4"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.1">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.40.40.40.40.40.40.1.2">𝐴</ci></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.41.41.41.41.41.41.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.41.41.41.41.41.41.1">𝑡</ci></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.3.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.43.43.43.43.43.43.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.43.43.43.43.43.43">𝑠</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.44.44.44.44.44.44.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.44.44.44.44.44.44.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.46.46.46.46.46.46.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.46.46.46.46.46.46">𝑥</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.47.47.47.47.47.47.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.47.47.47.47.47.47.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.3.3.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.180.180.5.5.4.1.1.3.3.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.49.49.49.49.49.49.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.49.49.49.49.49.49">𝑎</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.50.50.50.50.50.50.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.50.50.50.50.50.50.1">𝑡</ci></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.19.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.19.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.52.52.52.52.52.52.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.52.52.52.52.52.52">𝐴</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.53.53.53.53.53.53.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.53.53.53.53.53.53.1">𝜉</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.58.58.58.58.58.58.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.58.58.58.58.58.58">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.4.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.4.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.4.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.55.55.55.55.55.55.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.55.55.55.55.55.55">𝑎</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.56.56.56.56.56.56.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.56.56.56.56.56.56.1">′</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.57.57.57.57.57.57.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.57.57.57.57.57.57.1">𝑡</ci></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.2.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.59.59.59.59.59.59.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.59.59.59.59.59.59">𝑎</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.60.60.60.60.60.60.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.60.60.60.60.60.60.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.2.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.181.181.6.6.5.1.1.2.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.62.62.62.62.62.62.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.62.62.62.62.62.62">𝑎</ci><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.4"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.1">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.63.63.63.63.63.63.1.2">𝐴</ci></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.64.64.64.64.64.64.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.64.64.64.64.64.64.1">𝑡</ci></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.20.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.20.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.66.66.66.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.66.66.66.1.1.1">𝜉</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1">limit-from</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.67.67.67.2.2.2.1.2">𝑆</ci></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.72.72.72.7.7.7.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.72.72.72.7.7.7">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.5.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.5.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.5.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.5.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.69.69.69.4.4.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.69.69.69.4.4.4">𝑎</ci><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.4"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.1">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.70.70.70.5.5.5.1.2">𝑆</ci></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.71.71.71.6.6.6.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.71.71.71.6.6.6.1">𝑡</ci></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.3.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.73.73.73.8.8.8.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.73.73.73.8.8.8">𝑠</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.74.74.74.9.9.9.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.74.74.74.9.9.9.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.76.76.76.11.11.11.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.76.76.76.11.11.11">𝑥</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.77.77.77.12.12.12.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.77.77.77.12.12.12.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.3.3.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.182.182.7.7.6.1.1.3.3.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.79.79.79.14.14.14.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.79.79.79.14.14.14">𝑎</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.80.80.80.15.15.15.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.80.80.80.15.15.15.1">𝑡</ci></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.21.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.21.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.21.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.21.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.82.82.82.17.17.17.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.82.82.82.17.17.17">𝑇</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1">limit-from</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.83.83.83.18.18.18.1.2">𝜉</ci></apply></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.84.84.84.19.19.19.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.84.84.84.19.19.19.1">Ω</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.88.88.88.23.23.23.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.88.88.88.23.23.23">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.4.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.86.86.86.21.21.21.2">𝑠</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.87.87.87.22.22.22.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.87.87.87.22.22.22.1">𝑡</ci></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.2.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.89.89.89.24.24.24.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.89.89.89.24.24.24">𝑠</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.90.90.90.25.25.25.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.90.90.90.25.25.25.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.2.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.183.183.8.8.7.1.1.2.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.92.92.92.27.27.27.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.92.92.92.27.27.27">𝑎</ci><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.4"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.1">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.93.93.93.28.28.28.1.2">𝑆</ci></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.94.94.94.29.29.29.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.94.94.94.29.29.29.1">𝑡</ci></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.22.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.22.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.96.96.96.31.31.31.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.96.96.96.31.31.31">𝑂</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.97.97.97.32.32.32.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.97.97.97.32.32.32.1">Ω</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.184.184.9.9.8.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.101.101.101.36.36.36.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.101.101.101.36.36.36">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.184.184.9.9.8.1.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.184.184.9.9.8.1.1.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.99.99.99.34.34.34.2">𝑥</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.100.100.100.35.35.35.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.100.100.100.35.35.35.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.184.184.9.9.8.1.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.184.184.9.9.8.1.1.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.102.102.102.37.37.37.2">𝑠</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.103.103.103.38.38.38.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.103.103.103.38.38.38.1">𝑡</ci></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.23.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.23.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.105.105.105.40.40.40.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.105.105.105.40.40.40">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.106.106.106.41.41.41.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.106.106.106.41.41.41.1">𝑇</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.111.111.111.46.46.46.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.111.111.111.46.46.46">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.5.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.5.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.5.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.5.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.108.108.108.43.43.43.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.108.108.108.43.43.43">𝑎</ci><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.4"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.1">𝜉</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.109.109.109.44.44.44.1.2">𝑇</ci></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.110.110.110.45.45.45.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.110.110.110.45.45.45.1">𝑡</ci></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.3.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.112.112.112.47.47.47.2">𝑠</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.113.113.113.48.48.48.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.113.113.113.48.48.48.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.115.115.115.50.50.50.2">𝑥</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.116.116.116.51.51.51.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.116.116.116.51.51.51.1">𝑡</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.3.3.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.3.3.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.3.3.3.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.185.185.10.10.9.1.1.3.3.3.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci id="S4.SS2.SSS2.Px4.p9.1.m1.118.118.118.53.53.53.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.118.118.118.53.53.53">𝑎</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.119.119.119.54.54.54.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.119.119.119.54.54.54.1">′</ci></apply><ci id="S4.SS2.SSS2.Px4.p9.1.m1.120.120.120.55.55.55.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.120.120.120.55.55.55.1">𝑡</ci></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.24.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.24.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.24.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.24.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.122.122.122.57.57.57.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.122.122.122.57.57.57">𝑇</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.123.123.123.58.58.58.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.123.123.123.58.58.58.1">𝜉</ci></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.124.124.124.59.59.59.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.124.124.124.59.59.59.1">Ω</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.128.128.128.63.63.63.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.128.128.128.63.63.63">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.5.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.5.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.126.126.126.61.61.61.2">𝑠</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.2">𝑡</ci><cn
    type="integer" id="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.127.127.127.62.62.62.1.3">1</cn></apply></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.3.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.1.1.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.1">~</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.129.129.129.64.64.64.2">𝑠</ci></apply><ci id="S4.SS2.SSS2.Px4.p9.1.m1.130.130.130.65.65.65.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.130.130.130.65.65.65.1">𝑡</ci></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.2.2.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.2.2.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.2.2.2.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.2.2.2.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci id="S4.SS2.SSS2.Px4.p9.1.m1.132.132.132.67.67.67.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.132.132.132.67.67.67">𝑎</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.133.133.133.68.68.68.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.133.133.133.68.68.68.1">′</ci></apply><ci id="S4.SS2.SSS2.Px4.p9.1.m1.134.134.134.69.69.69.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.134.134.134.69.69.69.1">𝑡</ci></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.3.3.3.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.3.3.3.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.3.3.3.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.186.186.11.11.10.1.1.3.3.3.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci id="S4.SS2.SSS2.Px4.p9.1.m1.136.136.136.71.71.71.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.136.136.136.71.71.71">𝑎</ci><list id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.3.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.4"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.1">𝜉</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.137.137.137.72.72.72.1.2">𝑇</ci></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.138.138.138.73.73.73.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.138.138.138.73.73.73.1">𝑡</ci></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.25.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.25.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.140.140.140.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.140.140.140.1.1.1">𝑂</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.141.141.141.2.2.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.141.141.141.2.2.2.1">Ω</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.187.187.12.12.11.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.145.145.145.6.6.6.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.145.145.145.6.6.6">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.187.187.12.12.11.1.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.187.187.12.12.11.1.1.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.143.143.143.4.4.4.2">𝑥</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.2">𝑡</ci><cn
    type="integer" id="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.144.144.144.5.5.5.1.3">1</cn></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.187.187.12.12.11.1.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.187.187.12.12.11.1.1.3.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.1">~</ci><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.146.146.146.7.7.7.2">𝑠</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.2">𝑡</ci><cn
    type="integer" id="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.147.147.147.8.8.8.1.3">1</cn></apply></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.26.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.26.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.149.149.149.10.10.10.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.149.149.149.10.10.10">𝜉</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1">limit-from</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.150.150.150.11.11.11.1.2">𝑆</ci></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.155.155.155.16.16.16.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.155.155.155.16.16.16">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.4.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.4.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.4.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.152.152.152.13.13.13.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.152.152.152.13.13.13">𝑎</ci><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.1">𝜉</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1">limit-from</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.153.153.153.14.14.14.1.2.1.2">𝑆</ci></apply></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.154.154.154.15.15.15.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.154.154.154.15.15.15.1">𝑡</ci></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.2.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.1.1.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.1">~</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.156.156.156.17.17.17.2">𝑠</ci></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.2">𝑡</ci><cn type="integer"
    id="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.157.157.157.18.18.18.1.3">1</cn></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.188.188.13.13.12.1.1.2.2.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.1">~</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.159.159.159.20.20.20.2">𝑥</ci></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.2">𝑡</ci><cn type="integer"
    id="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.160.160.160.21.21.21.1.3">1</cn></apply></apply></list></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.27.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.27.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.27.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.27.2.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.162.162.162.23.23.23.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.162.162.162.23.23.23">𝑇</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1">limit-from</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.163.163.163.24.24.24.1.2">𝜉</ci></apply></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.164.164.164.25.25.25.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.164.164.164.25.25.25.1">Ω</ci></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="latexml" id="S4.SS2.SSS2.Px4.p9.1.m1.168.168.168.29.29.29.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.168.168.168.29.29.29">conditional</csymbol><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.4.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.4.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.166.166.166.27.27.27.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.166.166.166.27.27.27">𝑠</ci><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1"><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.2">𝑡</ci><cn
    type="integer" id="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.167.167.167.28.28.28.1.3">1</cn></apply></apply><list
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.2.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.1.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.1.1.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.1">~</ci><ci id="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.169.169.169.30.30.30.2">𝑠</ci></apply><apply id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.2">𝑡</ci><cn type="integer"
    id="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.3.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.170.170.170.31.31.31.1.3">1</cn></apply></apply><apply
    id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.2.2.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol
    cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.2.2.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">subscript</csymbol><apply id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.2.2.2.2.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px4.p9.1.m1.189.189.14.14.13.1.1.2.2.2.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.203.203.28">superscript</csymbol><ci id="S4.SS2.SSS2.Px4.p9.1.m1.172.172.172.33.33.33.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.172.172.172.33.33.33">𝑎</ci><list id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.3.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2"><ci id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.1">𝜉</ci><apply id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.cmml"
    xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1"><csymbol cd="latexml"
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1">limit-from</csymbol><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.2.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.173.173.173.34.34.34.1.2.1.2">𝑆</ci></apply></list></apply><ci
    id="S4.SS2.SSS2.Px4.p9.1.m1.174.174.174.35.35.35.1.cmml" xref="S4.SS2.SSS2.Px4.p9.1.m1.174.174.174.35.35.35.1">𝑡</ci></apply></list></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S4.SS2.SSS2.Px4.p9.1.m1.203c">\pi^{\xi,\Omega}(\tilde{\tau}_{t}&#124;s_{t})=O^{\Omega}(x_{t}&#124;s_{t})\delta_{x_{t}}(x^{\prime}_{t})\pi(a_{t}&#124;x^{\prime}_{t})\xi_{A}(a^{\xi,A}_{t}&#124;s_{t},x_{t},a_{t})A^{\xi}(a^{\prime}_{t}&#124;a_{t},a^{\xi,A}_{t})\\
    \xi_{S-}(a^{\xi,S}_{t}&#124;s_{t},x_{t},a_{t})T_{\xi-}^{\Omega}(\widetilde{s}_{t}&#124;s_{t},a^{\xi,S}_{t})O^{\Omega}(\widetilde{x}_{t}&#124;\widetilde{s}_{t})\xi_{T}(a^{\xi,T}_{t}&#124;\widetilde{s}_{t},\widetilde{x}_{t},a^{\prime}_{t})T_{\xi}^{\Omega}(\widetilde{s}_{t+1}&#124;\widetilde{s}_{t},a^{\prime}_{t},a^{\xi,T}_{t})\\
    O^{\Omega}(\widetilde{x}_{t+1}&#124;\widetilde{s}_{t+1})\xi_{S+}(a^{\xi,S+}_{t}&#124;\widetilde{s}_{t+1},\widetilde{x}_{t+1})T_{\xi+}^{\Omega}(s_{t+1}&#124;\widetilde{s}_{t+1},a^{\xi,S+}_{t})</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: In all settings, the reward functions $R^{\Omega}$ and $R_{\xi}^{\Omega}$ are
    defined on environment transitions $(s_{t},a_{t},s_{t+1})$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Adversarial Objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial attacks in RL are strategically designed to compromise specific
    aspects of agent behavior or environment dynamics. In general, they aim to prevent
    the agent from acting optimally, but the attacks vary in their objectives and
    methodologies. Even if the general goal of any adversarial attack is to reduce
    the performances of the agent, methods to achieve this can primarily have different
    objective function, for specific performance reductions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Deviate Policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The primary goal is to deviate the agent from its initial, typically optimal,
    policy. We can deviate the policy to make it diverge from the original policy
    : in that case the adversary $\xi$ is designed to maximize the agent’s expected
    loss over the pairs of policy given perturbed observation $x^{\prime}$ and original
    observation $x$, this is often referred as untargeted attacks. Or, we can also
    deviate the policy to make it converge to a target policy: in that case the adversary
    $\xi$ is designed to minimize the agent’s expected loss over the pairs of policy
    given perturbed observation $x^{\prime}$ and another policy $g$ given original
    observation $x$, this is often referred as targeted attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For adversarial attacks that alter the observation function, assuming the following
    shorthand notations: $\>\mathbb{E}_{x}=\mathbb{E}_{x\sim X}$; $\>\mathbb{E}_{a}=\mathbb{E}_{a^{\xi}\sim\xi(\cdot|x)}$;
    $\>\mathbb{E}_{x^{\prime}}=\mathbb{E}_{x^{\prime}\sim O^{\xi}(\cdot|x,a^{\xi}),||x^{\prime}-x||<\varepsilon}$,
    the objective function for untargeted attacks (policy divergence) is :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\xi^{*}=\arg\max_{\xi}\mathbb{E}_{x}\mathbb{E}_{a}\mathbb{E}_{x^{\prime}}\Big{[}\>\mathcal{L}\big{(}\,\pi(x^{\prime}),\pi(x)\,\big{)}\>\Big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The objective function for targeted attacks (policy convergence) is :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{x}\mathbb{E}_{a}E_{x^{\prime}}\Big{[}\>\mathcal{L}\big{(}\,\pi(x^{\prime}),g(x)\,\big{)}\>\Big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: These formulations seek for the optimal adversarial strategy $\xi^{*}$ that
    maximizes (resp. minimizes) the expected loss $\mathcal{L}$, computed over a distribution
    of original observations $x$, actions $a^{\xi}$ according to the adversary policy,
    and perturbed observations $x^{\prime}$ resulting from the altered observation
    function $O^{\xi}$, constrained by the condition that the perturbation in observation
    is less than $\varepsilon$. The loss $\mathcal{L}$ measures the difference between
    the policy’s output over the perturbed observations $\pi(x^{\prime})$ and the
    policy’s output over the original observations $\pi(x)$ (resp. the target policy’s
    output over the original observations $g(x)$).
  prefs: []
  type: TYPE_NORMAL
- en: 'For adversarial attacks that alter the dynamics of the environment, assuming
    the following shorthand notations: $\mathbb{E}_{s,a}=\mathbb{E}_{(s,a)\sim(S,A)}$;
    $\>\mathbb{E}_{x}=\mathbb{E}_{x\sim O^{\Omega}(\cdot|s)}$; $\>\mathbb{E}_{a^{\xi}}=\mathbb{E}_{a^{\xi}\sim\xi(\cdot|x)}$;
    $\>\mathbb{E}_{s_{t+1}}=\mathbb{E}_{s_{t+1}\sim T^{\Omega}(\cdot|s,a)}$; $\>\mathbb{E}_{x_{t+1}}=\mathbb{E}_{x_{t+1}\sim
    O(\cdot|s_{t+1})}$; $\>\mathbb{E}_{\widetilde{s}_{t+1}}=\mathbb{E}_{\widetilde{s}_{t+1}\sim
    T^{\xi,\Omega}(\cdot|s,a),||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon}$; $\>\mathbb{E}_{\widetilde{x}_{t+1}}=\mathbb{E}_{\widetilde{x}_{t+1}\sim
    O(\cdot|\widetilde{s}_{t+1})}$, the objective function for untargeted attacks
    (policy divergence) is :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\xi^{*}=\arg\max_{\xi}\mathbb{E}_{s,a}\mathbb{E}_{x}\mathbb{E}_{a^{\xi}}\mathbb{E}_{s_{t+1}}\mathbb{E}_{x_{t+1}}\mathbb{E}_{\widetilde{s}_{t+1}}\mathbb{E}_{\widetilde{x}_{t+1}}\Big{[}\>\mathcal{L}\big{(}\pi(\widetilde{x}_{t+1}),\pi(x_{t+1})\big{)}\>\Big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The objective function for targeted attacks (policy convergence) is :'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{s,a}\mathbb{E}_{x}\mathbb{E}_{a^{\xi}}\mathbb{E}_{s_{t+1}}\mathbb{E}_{x_{t+1}}\mathbb{E}_{\widetilde{s}_{t+1}}\mathbb{E}_{\widetilde{x}_{t+1}}\Big{[}\>\mathcal{L}\big{(}\pi(\widetilde{x}_{t+1}),g(x_{t+1})\big{)}\>\Big{]}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\xi^{*}$ is the optimal adversarial strategy that maximizes (resp. minimizes)
    the expected loss, considering the altered state dynamics and resulting observations,
    with the constraint that the perturbation in the state is less than $\varepsilon$.
    The expectations are over the distribution of original states $s$ and their observations
    $x$, adversarial actions $a^{\xi}$, perturbed states $\widetilde{s}_{t+1}$ from
    the modified transition function $T^{\xi,\Omega}$, and observations $\widetilde{x}_{t+1}$
    from the perturbed states. The loss $\mathcal{L}$ measures the difference between
    the policy’s output over the observations of perturbed next states $\pi(\widetilde{x}_{t+1})$
    and the policy’s output over the observations of original non perturbed next states
    $\pi(x_{t+1})$ (resp. the target policy’s output over the observations of original
    non perturbed next states $g(x_{t+1})$).
  prefs: []
  type: TYPE_NORMAL
- en: Even if the goal of applying these attacks can be to reduce the performances
    of the agent, the attacks themselves are designed to maximize the divergence (resp.
    convergence) of the policy, effectively causing the policy to produce significantly
    different actions or decisions based on the manipulated environment dynamics and
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Reward Minimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast, some adversarial attacks focus on leading the agent to less favorable
    states or decisions, thereby minimizing the total expected reward the agent accrues.
    These attacks are often targeted and seek for reduction of the efficacy or efficiency
    of the agent’s behavior by altering its reward acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function optimized in such attacks, subject to the relevant perturbation
    constraints, is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\xi^{*}=\arg\min_{\xi}\mathbb{E}_{\widetilde{\tau}\sim\pi^{\xi,\Omega}(\widetilde{\tau})}\Big{[}R(\tau)\Big{]}\\
    \footnotesize{\widetilde{\tau}\text{ subject to }&#124;&#124;x^{\prime}-x&#124;&#124;<\varepsilon\text{
    or }&#124;&#124;\widetilde{s}_{t+1}-s_{t+1}&#124;&#124;<\varepsilon}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\xi^{*}$ represents the adversarial strategy aimed at minimizing the
    agent’s total expected reward. The expectation is taken over the trajectories
    $\tau$ sampled according to a policy $\pi$ perturbed by the adversary in the environment
    $\Omega$. The function $R(\tau)$ calculates the discounted sum of rewards for
    each trajectory, with the goal of the adversary being to minimize this quantity
    through interventions, while adhering to the specified constraints on the perturbations.
    For attacks altering the observation, the difference between the perturbed observation
    $x^{\prime}$ and the original observation $x$ is constrained such that $||x^{\prime}-x||<\varepsilon$.
    Similarly, for attacks that alter the dynamics of the environment, the perturbation
    in the state is constrained with $||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon$.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it is hard to verify $||\widetilde{s}_{t+1}-s_{t+1}||<\varepsilon$
    since once the transition function applied an the next state get, in most of the
    simulation environment available, it is not possible to redo the transition with
    some other inputs to get the alternative next state, to be able to compare them.
    Therefore often, the actually verified constraint is $||a^{\xi}_{t}||<\varepsilon$,
    this makes it hard to compare different Dynamic Attacks that do not alter the
    same element $s_{t}$, $a_{t}$, $T^{\Omega}$, or $s_{t+1}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Others
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some methods have other objectives, for example to lead the agent to a specific
    target state. The objective is then to minimize the distance between the current
    state and the target state. Some other specific objectives can exist.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Knowledge Requirement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the realm of adversarial attacks against DRL agents, the extent and nature
    of the attacker’s knowledge about the agent significantly influence the strategy
    and effectiveness of the attack. Broadly, these can be categorized into White
    Box and Black Box approaches, each with its own set of strategies, challenges,
    and considerations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1 White Box
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this scenario, the adversary has complete knowledge of the agent’s architecture,
    parameters, and training data. This scenario represents the most informed type
    of attacks, where the adversary has access to all the inner workings of the agent,
    including its policy, value function, and possibly even the environment model.
  prefs: []
  type: TYPE_NORMAL
- en: '$-$ Policy and Model Access: The adversary knows the exact policy and decision-making
    process of the agent. This includes access to the policy’s parameters, algorithm
    type, and architecture. In model-based RL, the attacker might also know the transition
    dynamics and reward function.'
  prefs: []
  type: TYPE_NORMAL
- en: '$-$ Optimization and Perturbation: With complete knowledge, the attacker can
    craft precise and potent perturbations to the agent’s inputs or environment to
    maximize the deviation from desired behaviors or minimize rewards. They can calculate
    the exact gradients or other relevant information needed to optimize their attack
    strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: '$-$ Challenges and Implications: While white box attacks represent an idealized
    scenario with maximal knowledge, they provide a comprehensive framework for testing
    the agent’s robustness. By simulating the most extreme conditions an agent could
    face, developers can identify and reinforce potential vulnerabilities, leading
    to policies that are not only effective but also resilient to a wide range of
    scenarios, including unexpected environmental changes. This approach is particularly
    valuable in safety-critical applications where ensuring reliability against all
    possible disturbances is crucial.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Black Box
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this scenario, the adversary has limited or no knowledge of the internal
    workings of the agent. They may not know the specific policy, parameters, or architecture
    of the RL agent. Instead, they must rely on observable behaviors or outputs to
    infer information and craft their attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '$-$ Observational Inference: The attacker observes the agent’s actions and
    possibly some aspects of the state transitions to infer patterns, weaknesses,
    or predict future actions. This process often involves probing the agent with
    different inputs and analyzing the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '$-$ Surrogate Models and Transferability: Attackers might train a surrogate
    model to approximate the agent’s behavior or policy. If an attack is successful
    on the surrogate, it might also be effective on the target agent due to transferability,
    especially if both are trained in similar environments or tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '$-$ Challenges and Implications: The use of black box methods in enhancing
    robustness is not directly about realism of adversarial intent but rather about
    preparing for a variety of uncertain conditions and environmental changes. These
    methods encourage the development of general defense mechanisms that improve the
    agent’s adaptability and resilience. While the adversarial mindset might not reflect
    the typical operational challenges, the diversity and unpredictability of black
    box approaches help ensure that RL systems are robust not only against potential
    adversaries but also against a wide array of non-adversarial issues that could
    arise in dynamic and uncertain environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Both white and black box attacks paradigms play crucial roles in the study and
    development of adversarial strategies in RL. They help researchers and practitioners
    understand the spectrum of threats and devise more robust algorithms and defenses.
    By considering these different knowledge scenarios, one can better prepare RL
    agents to withstand or recover from adversarial attacks in various real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Category of Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section delineates the various methodologies utilized in crafting adversarial
    attacks, each with distinct strategies and theoretical underpinnings. It primarily
    divides into direct optimization-based and adversarial policy learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Direct Optimization Based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: They focus on directly manipulating the input or parameters of a model to induce
    misbehavior. These methods are subdivided into first-order and zeroth-order techniques,
    depending on the availability and usage of gradient information.
  prefs: []
  type: TYPE_NORMAL
- en: 'First Order Optimization approaches (White Box) : Gradient Attacks'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: They utilize the gradient information of the model to craft adversarial examples,
    efficiently targeting the model’s weaknesses. Common in white-box scenarios, gradient
    attacks are powerful when model internals are accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Zeroth Order Optimization approaches (Black Box)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Or derivative-free methods, optimize the adversarial objective without requiring
    gradient information, making them suitable for black-box scenarios. Techniques
    include simulated annealing, genetic algorithms, and random search.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Adversarial Policy Learning Based Approaches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These approaches involve training a separate model or policy to generate adversarial
    attacks. The adversarial model learns an optimal attack strategy through interaction
    with the target system, often using RL techniques. To train an Adversarial Policy
    (AP), optimization methods are used and could also be classified as First and
    Zeroth Order methods, but on the contrary to direct optimization methods, the
    optimization is used to train the adversary, not to directly craft the perturbation.
    Adversarial Policy Learning Based Approaches can be divided into two categories
    :'
  prefs: []
  type: TYPE_NORMAL
- en: Classical Adversarial Policies
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Learned via RL or any other method, only need a black box access to the model
    of the agent since they are policies sufficient in themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented Adversarial Policies
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Learned via RL or any other approach, are augmented either with a white Box
    access to the agent’s model during training or inference phase, or with some Direct
    Optimization method are added besides the Adversarial Policy to improve its performances.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we will use this taxonomy as a framework to examine
    recent research on adversarial examples for DRL. Section [5.1](#S5.SS1 "5.1 Observation
    Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") focuses on input-space perturbations,
    and Section [5.2](#S5.SS2 "5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")
    on environment-space perturbations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Adversarial Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we conduct a comprehensive review of contemporary adversarial
    attacks as documented in current literature, presented in a hierarchical, tree-like
    structure (refer to Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey")).
    The review categorizes these attacks first based on the type of alteration induced
    in the POMDP: either Observation Alteration or Dynamic Alteration. Next, the categorization
    considers the underlying objective driving these attacks, which could be either
    to Deviate Policy or Minimize Reward. Lastly, the classification focuses on the
    computational approach employed: Direct Optimization (First or Zeroth Order) or
    Adversarial Policy Learning. For each method in this classification tree, we will
    provide a detailed description, ensuring to consistently include the following
    critical information: the nature of the perturbation support (whether it’s an
    observation, state, action, or transition function), the level of knowledge about
    the model required to execute the attack (white-box or black-box), and any specific
    constraints or potential limitations associated with the method.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Observation Alteration Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section delves into the analysis of Observation Alteration Attacks targeting
    RL agents. These attacks specifically modify the observation function in the POMDP
    framework. Such methods are instrumental in simulating sensor errors in an agent,
    creating discrepancies between the agent’s perceived observations and the actual
    underlying state. These techniques can be particularly beneficial during an agent’s
    training phase, enhancing its resilience to potential observation discrepancies
    that might be encountered in real-world deployment scenarios. Observation Alteration
    Attacks generate a perturbation $a^{\xi,X}_{\epsilon}$ for a given observation
    $x$, resulting in a perturbed observation $x^{\prime}=x+a^{\xi,X}_{\epsilon}$.
    The perturbation $a^{\xi,X}_{\epsilon}$ is constrained within an $\epsilon$-ball
    of a specified norm $p\in{L_{1},L_{2},\ldots,L_{\infty}}$. This constraint can
    be achieved from any arbitrary perturbation $a^{\xi,X}$ by computing $a^{\xi,X}_{\epsilon}=\dfrac{\epsilon}{||a^{\xi,X}||_{p}}\cdot
    a^{\xi,X}$. Alternatively, in the context of a $L_{0}$ $\epsilon$-ball, $a^{\xi,X}_{\epsilon}$
    is defined by assigning the top-$\epsilon$ values of $a^{\xi,X}$ as 1 and setting
    all other values to 0.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Deviate Policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the field of adversarial attacks on observations, most methods developed
    primarily focus on optimizing the deviation of the policy. These methods function
    by crafting a perturbed observation $x^{\prime}$ that replaces the original observation
    $x$. Consequently, the policy $\pi$ generates a different output $\pi(x^{\prime})\neq\pi(x)$.
    In the untargeted scenario, the goal is to maximize divergence from the original
    policy; this is achieved by maximizing a specific loss function between the policy
    output on the altered observation and the original observation, formulated as
    $\arg\max_{x^{\prime}}L(\pi(x^{\prime}),\pi(x))$. Conversely, in targeted attacks,
    the objective is to guide the policy towards a particular behavior. This is done
    by minimizing a defined loss between the policy output on the altered observation
    and a target policy $g$ on the original observation, expressed as $\arg\min_{x^{\prime}}L(\pi(x^{\prime}),g(x))$.
    While the primary focus of these optimization functions is the deviation of the
    policy, this often results in a corresponding reduction in the reward garnered
    by the agent. Although Adversarial Policy Learning Based Methods could theoretically
    be employed to create observation attacks intended to deviate policy, the prevailing
    methods in practice are predominantly Direct Optimization Methods.
  prefs: []
  type: TYPE_NORMAL
- en: Direct Optimization Methods refer to techniques that directly compute an adversarial
    perturbation directly with optimization approaches. These approaches include gradient
    descent, evolutionary methods, stochastic optimization, among others. They are
    particularly effective for generating perturbations in observations with the aim
    of altering the policy’s behavior. A key advantage of these methods is their ability
    to be applied directly to a given agent model, without the necessity for extensive
    prior knowledge or preliminary computations. However, it is important to note
    that some of these methods might entail significant computational resources for
    each perturbation calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 'First Order Optimization Methods: Gradient Attacks (White Box)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'They are methods initially introduced in the context of supervised classification.
    They utilize the gradient of the attacked model to compute a perturbation $a^{\xi,X}$
    for a given input $x$, thereby crafting a perturbed input $x^{\prime}$. Consequently,
    these methods require white-box access to the model being attacked. In the realm
    of supervised classification, they are typically defined using the general formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x^{\prime}=x+a^{\xi,X}\quad\text{with}\quad a^{\xi,X}=\varepsilon\times\ldots\nabla_{x}L\big{(}f(x),y\big{)}\ldots$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\varepsilon$ represents the magnitude of the perturbation, $f(x)$ is
    the model output, $y$ denotes the ground truth label for untargeted attacks or
    the target class for targeted attacks, and $L$ is a loss function (often the same
    one used in training, but not exclusively). The term $\nabla_{x}L$ signifies the
    gradient of the loss function $L$ with respect to the input $x$, and the \say…
    indicates that additional operations can be applied to this core equation to tailor
    the update function to the specific optimization problem at hand. When adapted
    to RL, the formula essentially remains unchanged, except $f(x)$ is replaced by
    $\pi(x)$, the output of the agent’s policy function. In this context, $y$ no longer
    represents the ground truth but rather the current action $a$ for untargeted attacks,
    or a targeted action for targeted attacks. Numerous gradient attack methods exist,
    with the most notable being FGSM and its extensions (BIM, PGD, C&W, DeepFool,
    …), as well as JSMA and its extensions (XSMA, VFGA, …). All these methods, initially
    designed for supervised classification, are applicable in RL. Their primary objective
    is to deviate the agent’s policy by creating adversarial observations. As they
    are designed to generate minimal perturbations around a given observation, they
    are generally suited for agents and environments with continuous observation spaces,
    such as images, feature vectors, and signals. These attacks employ first-order
    optimization methods as they utilize the direction directly provided by the model’s
    gradient, thus categorizing them as white-box methods, which necessitate complete
    knowledge of the agent model’s architecture to obtain the gradient. These methods
    can be employed either in an untargeted manner, by maximizing the loss between
    the chosen action and itself, or in a targeted manner, by minimizing the loss
    between the chosen action and a specific target action.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ FGSM [[31](#bib.bib31)] is a fast computing method for crafting effective
    perturbed observations with $x^{\prime}=x+\varepsilon\cdot a^{\xi,X}$ with $a^{\xi,X}=\text{sign}\big{(}\nabla_{x}L(f(x),y)\big{)}$.
    In some case a variant is preferred with $a^{\xi,X}=\nabla_{x}L\big{(}f(x),y\big{)}$.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ BIM [[31](#bib.bib31)] and PGD [[32](#bib.bib32)] are iterative versions
    of FGSM, BIM simply applies FGSM multiple times with small steps, while PGD is
    more refined and projects the adversarial example back into a feasible set after
    each iteration. They are more computation needing, since they are iterative methods
    that computes the gradient several times to craft more precise adversarial observations.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ DeepFool [[33](#bib.bib33)] is an iterative method. In each iteration, it
    linearizes the classifier’s decision boundary around the current input and then
    computes the perturbation to cross this linearized boundary.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ C&W [[34](#bib.bib34)] is a method that seeks to minimize the perturbation
    while ensuring that the perturbed input is classified as a specific target class.
  prefs: []
  type: TYPE_NORMAL
- en: These methods can be used on any agent having whether their type of action space
    (discrete or continuous).
  prefs: []
  type: TYPE_NORMAL
- en: $-$ JSMA [[35](#bib.bib35)] is another gradient attack. It is more computationally
    expensive than FGSM, since it is an iterative method that craft perturbation with
    several iteration of computation of a Jacobian matrix for each output. It applies
    perturbation pixel by pixel, this make it particularly suitable for $L_{0}$ bounded
    perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ XSMA [[36](#bib.bib36)], VFGA [[37](#bib.bib37)], are methods based on JSMA,
    improving its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Theses methods have been applied to RL in untargeted way [[27](#bib.bib27),
    [23](#bib.bib23)] in various type of environments, and in targeted way [[38](#bib.bib38)].
  prefs: []
  type: TYPE_NORMAL
- en: 'A representation of the integration and application of Gradient Attacks in
    crafting observation perturbations within an RL framework is shown in Figure [8](#S5.F8
    "Figure 8 ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.1.1
    Deviate Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5e4440ef3b51db8c44bf06a3f43d881.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Gradient Observation Attacks : The adversarial attack intercept the
    observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$ by back-propagating
    the gradient of a loss in the neural network of a copy of the agent $\pi$, this
    perturbation is used to craft a perturbed observation $x^{\prime}_{t}$, which
    is sent to the agent'
  prefs: []
  type: TYPE_NORMAL
- en: Zeroth Order Optimization Methods (Black Box)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: They represent an alternative category of direct adversarial attacks on observations.
    Unlike methods that rely on gradient computation, these attacks use optimization
    techniques that do not depend on gradient information to generate perturbations.
    Their primary objective is to alter the agent’s policy by producing a perturbed
    observation.
  prefs: []
  type: TYPE_NORMAL
- en: These methods employ various search techniques, such as random search, meta-heuristic
    optimization, or methods for estimating the gradient without direct computation.
    They operate in a black box setting, where the attacker has access only to the
    inputs and outputs of the model, without any internal knowledge of the agent.
  prefs: []
  type: TYPE_NORMAL
- en: Square Attack [[39](#bib.bib39)] is one such method that performs a random search
    within an $\epsilon$-ball to discover adversarial examples. Although computationally
    demanding due to the number of iterations required for effective perturbation
    discovery, this method is applicable to any continuous observation space, including
    feature vectors, images, and signals, and to both discrete and continuous action
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Finite Difference [[40](#bib.bib40), [28](#bib.bib28)] offers a technique for
    Gradient Estimation through querying the agent’s model, bypassing the need for
    white-box access. This estimated gradient is then utilized to craft a perturbed
    observation. This approach necessitates querying the neural network $2\times N$
    times for an input of size N.
  prefs: []
  type: TYPE_NORMAL
- en: 'A representation of the integration and application of Zeroth Order Optimization
    Methods in crafting observation perturbations within an RL framework is shown
    in Figure [9](#S5.F9 "Figure 9 ‣ Zeroth Order Optimization Methods (Black Box)
    ‣ 5.1.1 Deviate Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/575c97be67134e374b751210601ddd0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Zeroth Order Optimization Observation Attacks : The adversarial attack
    intercept the observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$ by
    querying the neural network of the agent $\pi$ through an interface and applying
    a zeroth order optimization algorithm to maximize a loss, this perturbation is
    used to craft a perturbed observation $x^{\prime}_{t}$, which is sent to the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Minimize Reward
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several methods have been developed with the specific aim of producing perturbations
    in observations that directly minimize the reward obtained by an agent. These
    methods generate an adversarial action $a^{\xi}$ that induces perturbations $a^{\xi,X}$
    on the observation $x^{\prime}$, thereby replacing the original observation $x$.
    As a result, the policy $\pi$ yields a different output $\pi(x^{\prime})\neq\pi(x)$,
    leading to undesirable situations characterized by lower rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these methods fall under the category of Adversarial Policy Learning
    Based Methods. As pointed out by [[41](#bib.bib41)], learning an optimal adversary
    to perturb observations is equivalent to learning a optimal policy in a new POMDP
    from its point of view as described in [4.2.1](#S4.SS2.SSS1 "4.2.1 Alteration
    of the Observations Function 𝑂^Ω ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of
    Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey"). The effectiveness of these approaches is largely
    due to their ability to leverage the sequential nature of the environment and
    the anticipation of future rewards, which aids in the development of effective
    Adversarial Policies. In contrast, Direct Optimization Methods are generally not
    used for this purpose, as they often struggle to capture the sequential dynamics
    of the environment and the implications for future rewards.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Policy Learning Based Methods involve training an adversarial agent
    that initially learns to generate perturbations and subsequently uses this knowledge
    to produce perturbations during inference. Typically, these methods employ an
    adversarial policy to generate perturbations $a^{\xi,X}$ from observations $x$,
    creating a perturbed observation $x^{\prime}$. These methods require a training
    phase prior to being deployed as an attack, making them computationally intensive
    initially. However, once trained, these policies can be directly applied to generate
    perturbations with a significantly reduced computational cost when used in an
    attack scenario. In scenarios where the agent and the adversary are trained concurrently,
    these methods resemble techniques used in Generative Adversarial Networks (GAN).
    In this setup, the agent learns to perform its task while also becoming robust
    to the perturbations generated by the adversary. Simultaneously, the adversary
    refines its ability to create more effective perturbations to hinder the agent’s
    task performance. This co-learning approach enhances the adaptability and effectiveness
    of both the agent and the adversarial policy.
  prefs: []
  type: TYPE_NORMAL
- en: Classical Adversarial Policies (Black Box)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'They are methods that employ an adversarial RL agent trained to create perturbations
    in observations. These methods function as black-box attacks, particularly during
    inference, and do not require comprehensive knowledge of the agent’s model to
    produce perturbations. Instead, their only requirement during the training phase
    is the ability to query the model for outputs based on various inputs. In the
    subsequent attack phase, the already-trained adversarial agent no longer needs
    additional information except for its own policy model parameters. To launch an
    attack, the agent simply performs a forward pass through its policy, generating
    a perturbation that results in a perturbed observation. The methods that follow
    this principle are Optimal Attack on Reinforcement Learning Policies OARLP [[42](#bib.bib42),
    [43](#bib.bib43)] and State Adversarial SA-MDP [[44](#bib.bib44)]. ATLA [[45](#bib.bib45)]
    also use this principle, but it is more focused on how to use it effectively in
    adversarial training, this is discussed further in Section [7.1](#S7.SS1 "7.1
    Robustness Strategies with Adversarial Training ‣ 7 Adversarial and Robust Training
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey"). The adversary can use the same observation as the agent or augment
    its input with additional data, such as the agent’s action based on the original
    observation. This approach is highly flexible, allowing application across various
    observation spaces, including tabular data, feature vectors, images, and signals,
    and suitable for both discrete and continuous action spaces. Being black-box in
    nature, these methods only require the output of the agent model for a given input
    and do not need further information from the agent model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A representation of the integration and application of Adversarial Policies
    in crafting observation perturbations within an RL framework is shown in Figure
    [10](#S5.F10 "Figure 10 ‣ Classical Adversarial Policies (Black Box) ‣ 5.1.2 Minimize
    Reward ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb2a964ae4fe648bbd174b8e11fdd502.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Classical Adversarial Policy Observation Attacks : The adversarial
    policy intercept the observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$
    by a forward pass in its neural network, this perturbation is used to craft a
    perturbed observation $x^{\prime}_{t}$, which is sent to the agent. The opposite
    reward as the agent is send to the adversarial policy to be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: Augmented Adversarial Policies (White Box)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Adversarial Policies can also be augmented with specific white-box techniques
    that can improve their performances to be more effective.
  prefs: []
  type: TYPE_NORMAL
- en: The Adversarial Transformer Network (ATN) method, as used and studied by [[46](#bib.bib46),
    [47](#bib.bib47)] shows that training an adversarial policy can also involve utilizing
    the gradients of the agent model. In this approach, the adversary is trained by
    back-propagating the agent’s loss through its input, which corresponds to the
    adversary’s output, and subsequently updating the adversary’s parameters. This
    technique effectively trains the adversary to generate perturbations that counteract
    the agent’s tendencies. During training, this method is considered white-box as
    it relies on the agent model’s gradients. However, at inference time, it functions
    as a black-box method since the trained policy alone is sufficient for operation.
    This methods are highly adaptable, applicable to any type of observation space,
    including tables, feature vectors, images, and signals, as well as to both discrete
    and continuous action spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 'A representation of the integration and application of the Augmented Adversarial
    Policy ATN in crafting observation perturbations within an RL framework is shown
    in Figure [11](#S5.F11 "Figure 11 ‣ Augmented Adversarial Policies (White Box)
    ‣ 5.1.2 Minimize Reward ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ece355c7a48f140aff22ca4b8ec4b190.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Adversarial Transformer Network ATN Observation Attacks : The adversarial
    policy intercept the observation $x_{t}$, computes a perturbation $a^{\xi,X}_{t}$
    by a forward pass in its neural network, this perturbation is used to craft a
    perturbed observation $x^{\prime}_{t}$, which is sent to the agent. The opposite
    reward is sent to the agent, which back-propagate the loss to its input, then
    this loss is back-propagated in the adversarial policy network to be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another augmented adversarial policy approach is PA-AD [[48](#bib.bib48)],
    this method craft an attack in two steps: First an RL based adversary, the Director
    adversary, is gives the direction of the perturbation wanted in the policy space,
    then this direction is given as target to the Actor adversary which is a direct
    Optimization method that compute the perturbation in the observation space to
    produce in order to make the agent choose the action wanted by the adversary.
    The director adversary in trained by RL by getting the opposite reward as the
    agent, thus improving its direction given to the actor adversary. The actor adversary
    cannot be learn, it only apply a direct optimization algorithm by optimizing the
    direction given by the actor. A representation of the integration and application
    of the Augmented Adversarial Policy PA-AD in crafting observation perturbations
    within an RL framework is shown in Figure [12](#S5.F12 "Figure 12 ‣ Augmented
    Adversarial Policies (White Box) ‣ 5.1.2 Minimize Reward ‣ 5.1 Observation Alteration
    Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4c4d21b91ead774994c0cfbe2226a92b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: PA-AD Observation Attacks : The adversarial policy intercept the
    observation $x_{t}$, the director adversary computes a direction in the policy
    space by forward pass in its neural network, the actor adversary computes a perturbation
    $a^{\xi,X}_{t}$ by direct optimization, this perturbation is used to craft a perturbed
    observation $x^{\prime}_{t}$, which is sent to the agent. The opposite reward
    as the agent is send to the director adversary to be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Dynamic Alteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section presents an analysis of Dynamics Alteration Attacks for RL agents,
    which are methods that alter the transition function of the POMDP, they are useful
    to simulate mismatch between the dynamics of a deployment environment compared
    to the dynamics of the training environment, and they can be used to apply during
    the training of the agent to improve its robustness to unpredictable changes in
    the dynamics of the environment. Their goal is to produce an alteration of the
    transition function by producing a perturbation $a^{\xi}$ at a certain state $t$
    with the current state being $s_{t}$. This perturbation $a^{\xi}$ applied to any
    element of the transition function will have the consequence to lead to an alternative
    next state $\widetilde{s}_{t+1}$, which is different than the original next $s_{t+1}$
    that would have been produced without alteration. To achieve this goal the attack
    can either :'
  prefs: []
  type: TYPE_NORMAL
- en: $-$ compute a perturbation $a^{\xi,S}$ to craft a perturbed state $\widetilde{s}_{t}=s_{t}+a^{\xi,S}$.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ compute a perturbation $a^{\xi,A}$ to craft a perturbed action $a^{\prime}_{t}=a+a^{\xi,A}$.
  prefs: []
  type: TYPE_NORMAL
- en: $-$ compute a perturbation $a^{\xi,T}$ to directly alter the transition function
    $T^{\Omega}$ to induce alternative next state $T^{\Omega}_{\xi}(\widetilde{s}_{t+1}|...,a^{\xi,T}$).
  prefs: []
  type: TYPE_NORMAL
- en: $-$ compute a perturbation $a^{\xi,S+}$ to craft a perturbed next state $\widetilde{s}_{t+1}=s_{t+1}+a^{\xi,S+}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'As previously shown in Figures [5](#S4.F5 "Figure 5 ‣ Current State Perturbation
    ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2
    Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") [7](#S4.F7
    "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") [4](#S4.F4 "Figure 4 ‣ Transition Perturbation ‣ 4.2.2
    Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered
    POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") [6](#S4.F6 "Figure
    6 ‣ Next State Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω
    (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") in Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey") All these methods have the
    consequence to lead to an alternative next state $\widetilde{s}_{t+1}$. Theoretically,
    the amount of perturbation produced by such attacks should be measured as a distance
    between the original next state that would have been produced without alteration
    and the alternative next state given a norm $L_{p}$, $||s_{t+1}-\widetilde{s}_{t+1}||_{p}$.
    But since it is often hard to redo several times the transition function of an
    environment, in practice often only the difference to the directly perturbed element
    is measured : either $||s_{t}-\widetilde{s}_{t}||_{p}$, $||a_{t}-a^{\prime}_{t}||_{p}$,
    $||a^{\xi,T}||_{p}$, or $||\widetilde{s}_{t+1}-s_{t+1}||_{p}$ given the type of
    attack, thus making hard to compare disturbance magnitude between two different
    type of attacks that does not perturb the same element.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tampering with the transition is a completely different approach than with
    the observation. The methods developed in this section assume that the environments
    simulate physical, real-life settings : the perturbations are more restricted,
    ruling out gradient-based methods, and their effects on the agent is now indirect.
    In this section we first discuss methods that are designed to minimize the rewards
    obtained by the agent by altering the transition, then we discuss methods that
    are designed to deviate the policy of the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Minimize Reward
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Certain methods are specifically designed to modify the dynamics of an environment,
    aiming to reduce the rewards an agent receives. These methods achieve this by
    generating an adversarial action $a^{\xi}$, which leads to an altered subsequent
    state $\widetilde{s}_{t+1}$, diverging from the original next state $s_{t+1}$.
    As a result, the agent finds itself in an unfavorable situation $\widetilde{s}_{t+1}$
    at the next step, potentially leading to reduced immediate or future rewards.
    Most of these methods fall under the category of Adversarial Policy Learning Based
    Methods. This is because they are well-suited to exploit the sequential nature
    of the environment and the anticipation of future rewards, which aids in developing
    effective Adversarial Policies.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Policies involve methods where an adversarial agent is trained to
    create perturbations. Initially, this agent learns how to generate these perturbations,
    and once trained, it can efficiently produce them during inference. Typically,
    these methods utilize an adversarial policy to create an adversarial action $a^{\xi}$,
    aimed at altering the environment’s transitions. The training of these methods
    must be completed before they are deployed for attacks, which makes the initial
    phase computationally demanding. However, once the training phase is over, the
    adversarial policy can be used directly to generate perturbations at a significantly
    reduced computational cost in attack scenarios. In such a setup, the primary agent
    learns to perform its task while also becoming resilient to the adversary’s perturbations.
    Concurrently, the adversarial agent refines its skills in creating more effective
    perturbations to hinder the primary agent’s task performance.
  prefs: []
  type: TYPE_NORMAL
- en: Classical Adversarial Policies (Black Box)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'They utilize an adversarial RL agent trained to create adversarial actions
    that modify transitions within the environment. Functioning primarily as black-box
    attacks, especially during the inference phase, these methods do not necessitate
    comprehensive understanding of the agent’s model to produce perturbations. Instead,
    their main requirement during the training phase is the ability to query the model
    and obtain its outputs for various inputs. Once the adversarial agent completes
    its training, no additional information is needed beyond the parameters of its
    own policy model. During the attack phase, generating a perturbation involves
    simply executing a forward pass through the adversarial policy to create a perturbed
    observation. Examples of methods adhering to this approach include Robust Adversarial
    Reinforcement Learning (RARL) [[49](#bib.bib49)], along with its advancements
    such as Risk Averse (RA-RARL) [[50](#bib.bib50)], and Semi-Competitive (SC-RARL)
    [[51](#bib.bib51)]. These methods exemplify the application of Adversarial Policies
    in crafting effective black-box attacks in RL contexts. FSP [[52](#bib.bib52),
    [53](#bib.bib53)] also use this approach, but its points is more focused on how
    to use it effectively in adversarial training, this is discussed further in Section
    [7.1](#S7.SS1 "7.1 Robustness Strategies with Adversarial Training ‣ 7 Adversarial
    and Robust Training ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey"). These methods have been introduced as adding an adversarial
    action to an augmented version of the transition function $T_{\xi}^{\Omega}$,
    a representation of the integration and usage of Adversarial Policies Attacks
    to add transition perturbations in an RL context is shown in Figure [13](#S5.F13
    "Figure 13 ‣ Classical Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey"). The perturbation is added
    in the environment as previously shown in Figure [4](#S4.F4 "Figure 4 ‣ Transition
    Perturbation ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics)
    ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").
    These methods can also be used to generate perturbations on the state $s_{t}$,
    the next state $s_{t+1}$, and also the action $a_{t}$ as previously shown in Figures
    [5](#S4.F5 "Figure 5 ‣ Current State Perturbation ‣ 4.2.2 Alteration of the Transition
    Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy
    of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") [6](#S4.F6 "Figure 6 ‣ Next State Perturbation
    ‣ 4.2.2 Alteration of the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2
    Altered POMDP Component ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") [7](#S4.F7
    "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition Function
    𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy of Adversarial
    Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks
    and Training : A Survey") in Section [4.2.2](#S4.SS2.SSS2 "4.2.2 Alteration of
    the Transition Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component
    ‣ 4 Taxonomy of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey"). The Probabilistic Action
    Robust PR-MDP method [[54](#bib.bib54)], follows the same principle as the other
    adversarial policy methods, the goal is to train an adversarial policy to generate
    perturbation, but specifically on the action space as previously shown in Figure
    [7](#S4.F7 "Figure 7 ‣ Action Perturbation ‣ 4.2.2 Alteration of the Transition
    Function 𝑇^Ω (Environment Dynamics) ‣ 4.2 Altered POMDP Component ‣ 4 Taxonomy
    of Adversarial Attacks of DRL ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey"). These methods are very versatile, they can
    be applied to any observation space (table, features vector, images, signals)
    and action space (discrete or continuous). When adding disturbances in the transition
    function directly or in the states there is the constraint of having activatable
    lever in the environment to be used by the adversarial policy add disturbances,
    this constraint is not prevent when disturbing the action since the action itself
    is already the lever, but the attacks perturbing the action may have less means
    for disturbing the dynamics than methods that perturb state or transition. A representation
    of the integration and usage of Adversarial Policies Attacks to add transition
    perturbations in an RL context is shown in Figure [13](#S5.F13 "Figure 13 ‣ Classical
    Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/841d32f5dd14ed7e486382e7f53d7e3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Classical Adversarial Policy Dynamic Attack : The agent and the
    adversary get an observation $x_{t}$ of the environment. The agent chooses the
    action $a_{t}$ to apply and the adversary chooses the perturbation $a^{\xi}_{t}$
    to apply to alter the dynamics. The step function of the environment is run incorporating
    both agent action $a_{t}$ and adversarial action $a^{\xi}_{t}$. The reward opposite
    to that of the agent is sent to the adversarial policy to be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: Other works like APDRL [[55](#bib.bib55)], A-MCTS [[56](#bib.bib56)], APT [[57](#bib.bib57)]
    and ICMCTS-BR [[58](#bib.bib58)] have used the concept of adversarial policy or
    adversarial agent, but more in the context of a real two players game, where an
    agent learns to do a task and an adversarial agent learns to make the agent fail.
    The key difference with methods previously discussed like RARL, is that here the
    environment itself is a two players game. The agent and the adversary are always
    here, contrary to methods previously discussed like RARL where the original setup
    is an agent alone learning to do a task and the adversary comes to challenge the
    agent and make it improve its performances for itself. So the methods presented
    in these works are very similar to RARL with some more specificities, and even
    if there were not presented in the context of the robustness for a single agent
    they can also be adapted and used in this setup.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented Adversarial Policies (White Box)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Another possibility is to augment the information observed by the adversary
    with internal state of the agent like latent spaces or others. This is done by
    White-Box Adversarial Policy WB-AP [[59](#bib.bib59)] which is very similar to
    RARL except that the adversary has white box access to the agent internal data.
    This enables to improve the attack effectiveness of the perturbations since the
    adversary can learn to adapt the perturbations to the internal states of the agent
    that is attacked. This method is white box since it requires access to the internal
    state of the agent. It can be applied on any observation, and action spaces. A
    representation of the integration and usage of Adversarial Policies Attacks to
    add transition perturbations in an RL context is shown in Figure [14](#S5.F14
    "Figure 14 ‣ Augmented Adversarial Policies (White Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey")'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/60fc18b0783aeb4245d3d634a0cb5837.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: White Box Adversarial Policy Dynamic Attack : The agent get an observation
    $x_{t}$ of the environment and chooses the action $a_{t}$ to apply. The adversary
    get the observation and some white box internal state of the agent and chooses
    the perturbation $a^{\xi}_{t}$ to apply to alter the dynamics. The step function
    of the environment is run incorporating both agent action $a_{t}$ and adversarial
    action $a^{\xi}_{t}$. The reward opposite to that of the agent is sent to the
    adversarial policy to be trained.'
  prefs: []
  type: TYPE_NORMAL
- en: Direct Optimization Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: They use direct optimization to generate adversarial action $a^{\xi}$ to alter
    the transition of the environment. These methods on the contrary to Adversarial
    Policies does not require a prior training before usage in attacks. Although,
    they can be more computation needing when used in attacks since they have to solve
    an optimization problem at each perturbation rather than doing a forward pass
    in a neural network of an adversary policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First Order Optimization Methods: Gradient Attacks (White Box)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'MAS, LAS [[60](#bib.bib60), [61](#bib.bib61)] are methods that alter the action
    $a$ of the agent with a perturbation $a^{\xi,A}$ to make a perturbed action $a^{\prime}=a+a^{\xi,A}$
    that is less effective that the original action $a$. This method work only on
    RL agent that works with an actor network that produce the action and a Q-critic
    network that estimate the Q-value of the observation-action tuple. The method
    is to apply a gradient attack on the Q-Critic network by computing the gradient
    of the Q-value with respect to the action $a^{\xi,A}=\epsilon\nabla_{a}Q(x_{t},a_{t})$
    to minimize the Q-value estimated by perturbing the action. LAS is an extension
    of the MAS method which computes perturbation to apply over a sequence of future
    states to improve the long term impact of the attacks. This requires specific
    conditions to be applied such as an environment with repayable sequences, because
    the agent must be able to be reset at the specific state after the next pre-computed
    steps used to craft the perturbation. The scheme of the integration and usage
    of MAS attack in an RL context is shown in Figure [15](#S5.F15 "Figure 15 ‣ First
    Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d859325ddf400eb7e59527156e53fff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: MAS Dynamic Attacks on the Action: The agent and the adversarial
    attack get the observation $x_{t}$ of the environment. The agent chooses the action
    $a_{t}$ to apply, the adversarial attack computes the gradient of a copy of the
    Q-Critic Network of the agent with respect to the action $a_{t}$, this gradient
    is used as disturbance $a^{\xi,A}_{t}$ to add in the action $a^{\prime}_{t}=a_{t}+a^{\xi,A}_{t}$.
    The step function of the environment is ran starting from the perturbed state
    $\widetilde{s}_{t}$, with the agent action $a_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Environment Attack based on the value-Critic Network EACN [[62](#bib.bib62)]
    is a method that apply a gradient attack to compute a perturbed observation $x^{\prime}$
    and then use it to craft a perturbed state $\widetilde{s}$. The general idea is
    to use the knowledge of the critic network’s gradient to progressively increase
    the complexity of the task during training. At step $t$ with state $s$ and observation
    $x=O(s)$, EACN computes the gradient of the input of the Value-Critic Network
    $V$ to minimize the output value. The value-Critic Network evaluates the Value
    function of the environment given the policy of the agent. So the method generates
    a perturbed input $x^{\prime}$ with $V(x^{\prime})<V(x)$, this perturbed input
    is then used to create a perturbed state $\widetilde{s}$ with the property $x^{\prime}=O(\widetilde{s})$.
    Then the value estimated by the agent of the state is less than what it would
    have been without perturbation $V(O(\widetilde{s}))<V(O(s))$. This attack can
    either be applied before or after the transition function. The method requires
    a Value-Critic Network as in the PPO algorithm, but any other RL approaches can
    be considered by just adding the training of a Value-Critic Network on the resulting
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantage of EACN is that is avoid training an adversarial policy.
    But it is less versatile than an adversarial policy since it needs one-to-one
    correspondence between the observations and the disturbances available, since
    the disturbance are computed from gradient on the observations. So most of the
    time EACN is applicable only on environments where the observation space is a
    feature space, with modifiable features. The method works with an agent with any
    action space (discrete or continuous). A scheme of the integration and usage of
    EACN Gradient method on dynamic attacks in an RL context is shown in Figure [16](#S5.F16
    "Figure 16 ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣
    5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da4c9ef3af3d84703b79cb075c043351.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Environment Attack based on the Critic Network EACN Dynamic Attacks
    : The agent and the adversarial attack get the observation $x_{t}$ of the environment.
    The agent chooses the action $a_{t}$ to apply and while the adversarial attack
    computes the gradient of a loss in a copy of the Value-Critic Network of the agent,
    this gradient is used as disturbance $a^{\xi,S}_{t}$ to add in the state $\widetilde{s}_{t}=s_{t}+a^{\xi,S}_{t}$.
    The step function of the environment is ran starting from the perturbed state
    $\widetilde{s}_{t}$, with the agent action $a_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Some other approaches have been proposed like to generate perturbations in gridworld
    environments. These methods are WBA [[63](#bib.bib63)] which works by analyzing
    the Q table, and CDG [[64](#bib.bib64)] which works by analyzing the gradient
    of the Q network on the grid. But these methods add new obstacles in the grid,
    then changing the states-set. But our definition of perturbation of environments
    relies on dynamic alterations, by perturbing the transition function which is
    not the case for these methods.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Deviate Policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Certain methods are designed to alter the transitions and to produce perturbation
    optimized to deviate the policy of the agent. By generating an adversarial action
    $a^{\xi}$ that will induce an alternative next state $\widetilde{s}_{t+1}$ to
    replace the original next state $s_{t+1}$. The agent will be placed at the next
    step in a situation $\widetilde{s}_{t+1}$ where it should change the action that
    it would initially have chosen.
  prefs: []
  type: TYPE_NORMAL
- en: 'First Order Optimization Methods: Gradient Attacks (White Box)'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Environment Attack based on the Actor Network EAAN [[62](#bib.bib62)]. At step
    $t$ with state $s$ and observation $x=O(s)$, EAAN is used to compute the gradient
    of a loss on the input of the model of the agent to generate a perturbed input
    $x^{\prime}$ with $\pi(x^{\prime})\neq\pi(x)$, this perturbed input is then used
    to create a perturbed state $\widetilde{s}$ with the property $x^{\prime}=O(\widetilde{s})$.
    Then, the action chosen by the agent at this step is different than what it would
    have been without perturbation $\pi(O(\widetilde{s}))\neq\pi(O(s))$. This attack
    can either be applied before or after the transition function. EACN, this method
    is a bit less versatile than with an adversarial policy, since in addition to
    have activatable lever in the environment to be used to add disturbances, these
    lever need to have one-to-one correspondence with the observations used as inputs
    of the agent. So most of the time EAAN is applicable only on environments where
    the observation space is a feature space, with modifiable features. The method
    works with any action space (discrete or continuous). The scheme of the integration
    and usage of EAAN Gradient method on dynamic attacks in an RL context is shown
    exactly the same as for EACN which is shown in Figure [16](#S5.F16 "Figure 16
    ‣ First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.2.1 Minimize
    Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey"), the only difference
    is that for EAAN the gradient is computed on the actor network $\pi$, rather than
    on the value-critic network $V$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Other Objective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classical Adversarial Policy Learning Based Methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Environment-Search Attack ESA : [[28](#bib.bib28)] train an adversary to perturb
    the environment transition model $M$ with an adversarial reward based on the distance
    between the disturbed state and the original state: the goal is to make small
    changes to $M$ but to induce a completely different disturbed state. The scheme
    of the integration and usage of ESA on dynamic attacks in an RL context is shown
    exactly the same as for Adversarial Policy which is shown in Figure [13](#S5.F13
    "Figure 13 ‣ Classical Adversarial Policies (Black Box) ‣ 5.2.1 Minimize Reward
    ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning
    Through Adversarial Attacks and Training : A Survey"), the only difference is
    that in ESA reward get by the adversary is not the opposite of the reward of the
    agent, but is a reward based on the state distance.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Component &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Alteration &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Objective | Category |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Perturbed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Element &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Knowledge &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method |'
  prefs: []
  type: TYPE_TB
- en: '| Observations Alteration see [5.1](#S5.SS1 "5.1 Observation Alteration Attacks
    ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") | Deviate Policy | Gradient Attack see [5.1.1](#S5.SS1.SSS1.Px1
    "First Order Optimization Methods: Gradient Attacks (White Box) ‣ 5.1.1 Deviate
    Policy ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") |
    observation $x$ | white-box |  | FGSM [[65](#bib.bib65)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | BIM [[31](#bib.bib31)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | PGD [[32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeepFool [[33](#bib.bib33)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | C&W [[34](#bib.bib34)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | JSMA [[35](#bib.bib35)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | XSMA [[36](#bib.bib36)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | VFGA [[37](#bib.bib37)] |'
  prefs: []
  type: TYPE_TB
- en: '| Zeroth Order Optimization Attack see [5.1.1](#S5.SS1.SSS1.Px2 "Zeroth Order
    Optimization Methods (Black Box) ‣ 5.1.1 Deviate Policy ‣ 5.1 Observation Alteration
    Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") | observation $x$ | black-box |  | FD [[40](#bib.bib40),
    [28](#bib.bib28)] SA [[39](#bib.bib39)] |'
  prefs: []
  type: TYPE_TB
- en: '| Minimize Reward | Adversarial Policy see [5.1.2](#S5.SS1.SSS2 "5.1.2 Minimize
    Reward ‣ 5.1 Observation Alteration Attacks ‣ 5 Adversarial Attacks ‣ Robust Deep
    Reinforcement Learning Through Adversarial Attacks and Training : A Survey") |
    observation $x$ | black-box |  | OARLP [[42](#bib.bib42), [43](#bib.bib43)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | SA-MDP [[44](#bib.bib44)] |'
  prefs: []
  type: TYPE_TB
- en: '| white-box |  | ATN [[46](#bib.bib46), [47](#bib.bib47)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | PA-AD [[48](#bib.bib48)] |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamics Alteration see [5.2](#S5.SS2 "5.2 Dynamic Alteration ‣ 5 Adversarial
    Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") | Minimize Reward | Adversarial Policy see [5.2.1](#S5.SS2.SSS1.Px2
    "Augmented Adversarial Policies (White Box) ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic
    Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") | transition $T^{\Omega}$ state
    $s$ or action $a$ | black-box |  | RARL [[49](#bib.bib49)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | RA-RARL [[50](#bib.bib50)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | SC-RARL [[51](#bib.bib51)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-MCTS [[56](#bib.bib56)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | APT [[57](#bib.bib57)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | ICMCTS-BR [[58](#bib.bib58)] |'
  prefs: []
  type: TYPE_TB
- en: '| white-box |  | WB-AP [[59](#bib.bib59)] |'
  prefs: []
  type: TYPE_TB
- en: '| action $a$ | black-box |  | PR-MDP [[54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient Attack see [5.2.1](#S5.SS2.SSS1.Px3 "Direct Optimization Methods
    ‣ 5.2.1 Minimize Reward ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust
    Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey")
    | action $a$ | white-box |  | MAS, LAS [[60](#bib.bib60), [61](#bib.bib61)] |'
  prefs: []
  type: TYPE_TB
- en: '| state $s$ | white-box |  | EACN [[62](#bib.bib62)] |'
  prefs: []
  type: TYPE_TB
- en: '| Deviate Policy | Gradient Attack see [5.2.2](#S5.SS2.SSS2 "5.2.2 Deviate
    Policy ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") | state $s$ | white-box
    |  | EAAN [[62](#bib.bib62)] |'
  prefs: []
  type: TYPE_TB
- en: '| Other Objective | Adversarial Policy see [5.2.3](#S5.SS2.SSS3 "5.2.3 Other
    Objective ‣ 5.2 Dynamic Alteration ‣ 5 Adversarial Attacks ‣ Robust Deep Reinforcement
    Learning Through Adversarial Attacks and Training : A Survey") | state $s$ | black-box
    |  | ESA [[28](#bib.bib28)] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Characterization and Constraints of Adversarial Attack Methods for
    Reinforcement Learning : Summary of the Content of Section [5](#S5 "5 Adversarial
    Attacks ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey")'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Strategies of Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Adversarial attack can be used in many different ways and there can be various
    approaches to use them. Often adversarial attack are introduced with a specific
    application strategy. In this section we develop about how various strategies
    are applied to use adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Strategies for using White Box Attacks in Black Box Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some strategies leverage white box attack methods in scenarios where the adversary
    has limited knowledge about the target agent (black box scenarios). Techniques
    in this category, like imitation learning, often involve understanding or approximating
    the internal mechanisms of the target system without complete access to its architecture
    or training data. This approach is particularly challenging as it requires the
    adversary to make accurate guesses or approximations about the target’s behavior
    based on limited observable outputs or effects. The AEPI strategy [[66](#bib.bib66)]
    targets black-box environments where the attacker has limited knowledge about
    the victim’s system. It uses Deep Q-Learning from Demonstration (DQfD) to imitate
    the victim’s Q-function, to the apply white box adversarial attacks. The RS strategy
    [[44](#bib.bib44)] is designed to not relying on the victim’s Q-function’s accuracy.
    RS focuses on learning the Q-function of the victim’s policy and them applying
    any white box adversarial attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Strategies for Timing and Stealthiness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some Strategies are characterized by their focus on the timing of attacks and
    maintaining stealthiness. These strategies are designed to minimize detection
    while maximizing impact, often by carefully choosing when to launch an attack
    or by subtly altering agent behavior. This approach is particularly relevant in
    scenarios where avoiding detection is crucial, either for the success of the attack
    or to study the system’s vulnerabilities without triggering alarms.
  prefs: []
  type: TYPE_NORMAL
- en: K&S [[67](#bib.bib67)], and Strategically-Timed Attack STA [[68](#bib.bib68)],
    both concentrate on the timing of perturbations. [[67](#bib.bib67)] found that
    altering the frequency of FGSM perturbation injections can maintain effectiveness
    while reducing computational costs. Similarly, STA employs a preference metric
    to determine the optimal moments for launching perturbations, targeting only 25%
    of the states.
  prefs: []
  type: TYPE_NORMAL
- en: Weighted Majority Algorithm WMA [[69](#bib.bib69)] and Critical Point and Antagonist
    Attack CPA and AA [[70](#bib.bib70)]. Take the concept further by introducing
    more sophisticated timing strategies. WMA uses real-time calculations to select
    the most sensitive timeframes for attacks, while CPA and AA focus on identifying
    critical moments for injections, predicting the next environment state or using
    an adversarial agent to decide the timing.
  prefs: []
  type: TYPE_NORMAL
- en: Static Reward Impact Maps SRIMA [[71](#bib.bib71)] and Minimalistic Attack MA
    [[72](#bib.bib72)] emphasize efficiency and stealth. SRIMA method selects only
    the most influential features for perturbation, reducing the gradient computation
    cost, and is suitable for time-constrained environments. MA pushes stealth to
    the limits by altering only a small fraction of state features and timeframes,
    making it hard for the victim to detect the attack.
  prefs: []
  type: TYPE_NORMAL
- en: The Enchanting Attack EA [[68](#bib.bib68)] is a strategy that use a model that
    predict the future state depending on the action applied, and then based on the
    possible future states, apply an adversarial attack targeting the action that
    lead to the target state.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Strategies for Real Time and Resource Constraint Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some strategies are optimized for efficiency and speed, making them suitable
    for real-time applications or scenarios with significant resource constraints.
    Universal attacks, which create perturbations that are broadly effective across
    different inputs or states, are a key part of this category. These techniques
    are valuable in environments where the computational power is limited or where
    decisions need to be made swiftly, such as in certain gaming or real-time decision-making
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the input-space adversarial examples, universal methods are particularly
    interesting when dealing with time-constrained environments: state-independent
    perturbations are computed offline, thus allowing for online injections with very
    small delay. These perturbations are short-term by nature, but may lead to controlling
    the victim’s policy and are therefore usually combined with adversarial policies.'
  prefs: []
  type: TYPE_NORMAL
- en: CopyCAT [[73](#bib.bib73)] use an additive mask $\delta_{a}$ is computed offline
    for each action $a$, maximizing the expected $\pi(a,o^{k}_{t}+\delta_{a})$ over
    a set of pre-collected observations $(o^{k}_{t})_{k,t}$. Once each mask is computed,
    the chosen perturbation can be applied to the last observation $o_{t}$ to make
    the agent follow any target policy. The authors do not focus on how to compute
    such policy, but there existing techniques were described earlier in the survey
    [[74](#bib.bib74)]. Experiments are conducted on Atari games, and CopyCAT provides
    better result than the targeted version of FGSM.
  prefs: []
  type: TYPE_NORMAL
- en: Universal Adversarial Perturbations UAP-S, UAP-O [[75](#bib.bib75)] inspired
    from a known DL-based universal method, UAP [[76](#bib.bib76)]. The adversary
    first gathers a set of observed states $D_{train}$ and sanitizes it, keeping only
    the ones having a critical influence on the episode. Then, the unique additive
    mask is computed using the UAP algorithm [[76](#bib.bib76)]. In the case of Atari
    games, where a state $s_{t}$ consists in several consecutive observations ${o_{t-N+1},...,o_{t}}$,
    the perturbation can be state-agnostic (UAP-S), i.e. unique for all states but
    different for each $o_{i}$ within a state, or observation-agnostic (UAP-O), i.e.
    unique for all observations. Experiments show promising results for both methods
    (with an advantage for UAP-S) on DQN, PPO and A2C agents in three different Atari
    games. Authors show that the SA-MDP method [[44](#bib.bib44)] is not sufficient
    to mitigate these attacks, and introduce an effective detection technique called
    $AD^{3}$.
  prefs: []
  type: TYPE_NORMAL
- en: DAP [[74](#bib.bib74)] decoupled adversarial policy attack is also universal,
    though the paper is more focused on the adversarial policy aspect. Here each additive
    mask $\delta_{a,a^{\prime}}$ is specific to the victim’s initial action $a$ as
    well as the targeted action $a^{\prime}$. States are then classified into the
    right $(a,a^{\prime})$ category using both the victim’s policy and the decoupled
    policy, and the designated mask is added to the observation if the switch policy
    allows it.
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Strategies |'
  prefs: []
  type: TYPE_TB
- en: '| White Box Attacks in Black Box Scenarios see [6.1](#S6.SS1 "6.1 Strategies
    for using White Box Attacks in Black Box Scenarios ‣ 6 Strategies of Attacks ‣
    Robust Deep Reinforcement Learning Through Adversarial Attacks and Training :
    A Survey") | AEPI [[66](#bib.bib66)] RS [[44](#bib.bib44)] |'
  prefs: []
  type: TYPE_TB
- en: '| Timing and Stealthiness see [6.2](#S6.SS2 "6.2 Strategies for Timing and
    Stealthiness ‣ 6 Strategies of Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey") | K&S [[67](#bib.bib67)] |'
  prefs: []
  type: TYPE_TB
- en: '| STA [[68](#bib.bib68)] |'
  prefs: []
  type: TYPE_TB
- en: '| WMA [[69](#bib.bib69)] |'
  prefs: []
  type: TYPE_TB
- en: '| CPA, AA [[70](#bib.bib70)] |'
  prefs: []
  type: TYPE_TB
- en: '| SRIMA [[71](#bib.bib71)] |'
  prefs: []
  type: TYPE_TB
- en: '| MA [[72](#bib.bib72)] |'
  prefs: []
  type: TYPE_TB
- en: '| EA [[68](#bib.bib68)] |'
  prefs: []
  type: TYPE_TB
- en: '| Real Time and Resource Constraint Scenarios Methods see [6.3](#S6.SS3 "6.3
    Strategies for Real Time and Resource Constraint Scenarios ‣ 6 Strategies of Attacks
    ‣ Robust Deep Reinforcement Learning Through Adversarial Attacks and Training
    : A Survey") | CopyCAT [[73](#bib.bib73)] |'
  prefs: []
  type: TYPE_TB
- en: '| UAP-S / O [[75](#bib.bib75), [76](#bib.bib76)] |'
  prefs: []
  type: TYPE_TB
- en: '| DAP [[74](#bib.bib74)] |'
  prefs: []
  type: TYPE_TB
- en: '| \botrule |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Strategies for Adversarial Attack : Summary of the Content of Section
    [6](#S6 "6 Strategies of Attacks ‣ Robust Deep Reinforcement Learning Through
    Adversarial Attacks and Training : A Survey")'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Adversarial and Robust Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Robustness Strategies with Adversarial Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adversarial training in RL is a technique aimed at improving the robustness
    of RL agents against adversarial attacks. The general principle of adversarial
    training involves repeatedly exposing the agent to adversarial examples or perturbations
    during its training phase. This process is akin to inoculating the agent against
    potential attacks it might encounter in the real world or more complex environments.
    This method is conceptually akin to the principles of robust control [[77](#bib.bib77)],
    where the focus is on ensuring that control systems maintain stability and performance
    despite uncertainties and external disturbances. In RL, adversarial training can
    take various forms, but the core idea is consistent: the RL agent is trained not
    only with normal experiences drawn from its interaction with the environment but
    also with experiences that have been modified by adversarial perturbations [[10](#bib.bib10)].
    These perturbations are typically generated using methods akin to those used in
    adversarial attacks – for example, by altering the agent’s observations or by
    modifying the dynamics of the environment. By training in such an adversarially
    challenging environment, the RL agent learns to perform its task effectively even
    when faced with manipulated inputs or altered state transitions. This makes the
    agent more robust and less susceptible to potential adversarial manipulations
    post-deployment. As detailed in [3.2](#S3.SS2 "3.2 Adversarial Attacks for Robust
    RL ‣ 3 Formalization and Scope ‣ Robust Deep Reinforcement Learning Through Adversarial
    Attacks and Training : A Survey") the training process often involves a sort of
    min-max game, where one tries to minimize the maximum possible loss that an adversary
    can induce. This approach mirrors robust control’s emphasis on preparing systems
    to handle worst-case scenarios and uncertainties, such as variations in system
    dynamics or external noise. Adversarial training in RL can also be related to
    GANs in supervised learning, where models are trained to be robust against an
    adversary that tries to generate examples to fool the model. In the RL context,
    this approach helps the agent to not only optimize its policy for the given task
    but also to harden it against unexpected changes or adversarial strategies that
    might be encountered, thereby enhancing its overall performance and reliability.
    Any adversarial attack method can be used in adversarial training, certain are
    better than others to improve robustness of the agent. And specific strategies
    fro applying attacks can be used to better improve robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following the describe the different adversarial training strategies
    that can be applied :'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.1 Initial Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It Consists on training an adversary against an agent until convergence, and
    then adversarially train the agent against this agent for it to become robust.
    This technique works with adversarial attacks based on adversarial policies as
    done with the method RARL [[49](#bib.bib49)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2 Continuous Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It consists on training the adversary to converge. And then co-train the adversary
    and the agent simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach of adversarial training is the one used for attack techniques
    based Direct Optimization such agent gradient or zeroth order. Since the attacks
    are based on the policy of the agent, the attack automatically re-adapt to the
    new policy, for example gradient attacks automatically compute gradient on the
    new parameters of the agents policy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, for attack techniques based on adversarial policy this techniques is
    less usable since it is hard to maintain the effectiveness of the adversarial
    policy since both agent and adversarial policies are learned simultaneously, one
    agent could learn faster than the other and the the whole processes can be less
    effective. So for adversarial policies the previous approaches of alternate training
    is more preferable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 7.1.3 Alternate Adversarial Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It consists on alternately training an adversary against an agent until convergence,
    and then train the agent against this adversary until convergence. And repeat
    the loop convergence of both agent and adversary. This technique works well with
    adversarial attacks based on adversarial policies as done with the method ATLA
    [[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.4 Fictitious Self Play
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: FSP [[52](#bib.bib52), [53](#bib.bib53)] is a strategy for applying adversarial
    policies attacks during the adversarial training to maximize the gain in robustness
    of the agent. The adversarial training is done alternatively between the agent
    and the adversary each until convergence. The idea is to not always apply fully
    effective attacks during the training phases of the agent, but sometimes applying
    a random or average disturbance. This strategy enhance RARL [[49](#bib.bib49)]
    to improve its effectiveness for improving generalization of the policy adversarially
    trained.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Other Robustness Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Leveraging adversarial training for more robust and reliable DRL algorithms
    is the most used defense against adversarial attacks, the variety of methods available
    fitting each specific use case. However, the issue raised by [[78](#bib.bib78)]
    remain: the high amount of possible adversarial inputs makes the design of a unique
    and adaptive defense method unlikely. Combining design-specific adversarial training
    methods with more classic defensive measures is therefore an interesting lead
    to protect DRL models from malicious behaviour, as countermeasures can be implemented
    at each stage of the models’ construction process.'
  prefs: []
  type: TYPE_NORMAL
- en: First, a defendant may focus on the design stage to choose a robust network
    architecture for the agent’s policy. [[79](#bib.bib79)] works on the impact of
    network architecture to a model’s robustness shows that within a same parameters
    budget, some architectural configurations are more robust than others. In addition,
    the authors show that reducing the deeper layers capacity also improves robustness.
    Though their findings focus on supervised models, they could be extend to DRL.
    Robust architecture may also be achieved through defensive distillation [[80](#bib.bib80)],
    i.e. training a smaller student network to imitate a larger teacher network with
    class probability outputs, resulting in a lighter network with more regularization.
    Distillation was shown to be effective for DRL [[81](#bib.bib81)], and [[82](#bib.bib82)]
    studied the relevance of distillation approaches according to different contexts.
    Observation alterations automatically transform any MDP into a POMDP, since the
    observation is not anymore deterministic, it has been shown that for POMDP, recurrent
    architecture improve performances of the policies [[83](#bib.bib83)], so do defend
    against observation alteration recurrent policies can be useful [[45](#bib.bib45)].
  prefs: []
  type: TYPE_NORMAL
- en: For improving robustness, the defendant may also improve regularization through
    noisy training rewards [[84](#bib.bib84), [85](#bib.bib85)]. Or increase exploration
    with noisy actions [[30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: Worst-Case-Aware Robust Reinforcement Learning WocaR-RL [[86](#bib.bib86)] is
    a method that improve robustness to observations perturbation without attacking.
    It only works for adversarial attacks on the observations, it consist of constructing
    a $\epsilon$-ball around the observation and computing the upper and lower bound
    of the action $\hat{A}$ of the agent by convex relaxation of neural network. The
    the worst-attack action value network $\underline{Q}(x_{t},a_{t})$ is then learned
    based on $Q(x_{t},\hat{a}_{t})$, and then the agent is trained to maximize the
    worst-attack action value $\underline{Q}(x_{t},a_{t})$
  prefs: []
  type: TYPE_NORMAL
- en: Some other methods improve robustness defining a distribution of transition
    function for training the agent, the method RAMU [[87](#bib.bib87)] design a architecture
    enabling to learn policies across a distribution of transition function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a classic method to add a layer of security during the testing stage
    is adversarial detection: identifying modified observations for denoising or removal.
    Detection may be done through successor representation [[88](#bib.bib88)], or
    using a separately trained model [[89](#bib.bib89)]. Adversarial detection is
    a widespread technique in supervised learning [[90](#bib.bib90), [91](#bib.bib91)],
    but it is limited when it comes to the RL context: indeed, environment-space perturbations
    produce legitimate (though unlikely) observations and are therefore very difficult
    to detect. In addition [[92](#bib.bib92)] showed that detection methods are usually
    not very versatile, and easily subject to small changes in the attack methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a wide spectrum of defense methods for DRL algorithms, each with their
    benefits and limitations. Combining several methods allows to cover different
    aspects of the adversarial threat, but the defendant must keep in mind that simply
    stacking defense layers does not necessarily improve robustness [[93](#bib.bib93)]:
    each method must be analyzed and selected with care according to the given context.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 8.1 Current issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial methods for reinforcement learning is a recent yet booming research
    field. We encountered a few shortcomings in the tools and literature, the resolving
    of which would substantially help future usage and research.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.1 Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: New methods to compute RL-based adversarial examples are published regularly,
    and the number and variety of existing malicious or defensive techniques is constantly
    increasing. Each paper addresses a specific aspect of adversarial RL, however
    the positioning of new methods amongst the pre-existing techniques is often unclear,
    and a more global vision is usually missing. We aim to address this need of a
    generic framework for classification, by introducing a precise and clear taxonomy
    and mapping existing methods onto it. Following a similar structure for presenting
    future work would highly improve the field’s coherence and clarity. This also
    applies to the metrics used in the experiments and to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2 Code availability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A key element for the ongoing research is reproducibility. Indeed, a method’s
    validity relies heavily on its comparison with state-of-the-art techniques. Yet
    as some articles do not provide the corresponding code to replicate their experiments,
    reproducing existing methods becomes a tedious process for researchers, with sometimes
    unreliable results. We therefore wish to highlight the importance of publicly
    available code to make the best use of published works and progress further into
    the field.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3 Common tools and methodology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Comparing the performance of the various existing methods is challenging. The
    metrics used in the articles may differ depending on the authors’ objective, threat
    model, and chosen RL environments. Even if the metrics are similar, the results
    are also dependent on the chosen RL algorithm and its hyperparameters. To properly
    assess a methods efficiency, a standard methodology is needed: [[94](#bib.bib94)]
    propose such approach for RL robustness evaluation, and a few other usage-specific
    methodologies can be found [[95](#bib.bib95), [96](#bib.bib96)]. From a broader
    perspective, authors can follow a simple rule of thumb: using open source and
    peer-reviewed toolkits that provide reliable implementations of RL algorithms
    (Stable-baselines3 [[97](#bib.bib97)], Tianshou [[98](#bib.bib98)]), attacks methods
    (DRL-oriented [[99](#bib.bib99)] or not [[100](#bib.bib100)]), and robust training
    algorithms [[101](#bib.bib101)].'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Open challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This survey is also an opportunity to highlight the major research directions
    to be explored in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1 Explainability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The question of robustness and vulnerability to adversarial behaviour in RL
    boils down to the issue of trustworthy artificial intelligence. Explainable reinforcement
    learning (XLR) techniques are developed in order to deeply understand the RL decision-making
    process and improve its transparency [[102](#bib.bib102), [103](#bib.bib103)].
    Recent works focus on the correlation between explainability and robustness for
    machine learning in general [[104](#bib.bib104)], and extending these concept
    to DRL may lead to significant progress in both domains.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2 Attack feasability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another interesting lead for the robustness of DL algorithms, and particularly
    for DRL, is the study of the practicality of adversarial examples. It is now acknowledge
    that an all-mighty, omniscient adversary can fool about any model: however, RL
    environments often model real-life situations with physical constraints. Recent
    works on the compatibility of adversarial examples with such constraint show promising
    results in the defense of DL models [[105](#bib.bib105)]. A thorough study on
    RL-based adversarial methods’ feasability could help improve the challenge of
    universally robust models.'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This survey has provided an extensive examination of the robustness challenges
    in RL and the various adversarial training methods aimed at enhancing this robustness.
    Our work highlighted the susceptibility of RL agents to dynamic and observation
    alterations with adversarial attacks, underscoring a significant gap in their
    application in real-world scenarios where reliability and safety are paramount.
  prefs: []
  type: TYPE_NORMAL
- en: We have presented a novel taxonomy of adversarial attacks, categorizing them
    based on their impact on the dynamics and observations within the RL environment.
    This classification system not only aids in understanding the nature of these
    attacks but also serves as a guide for researchers and practitioners in identifying
    appropriate adversarial training strategies tailored to specific types of vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Our formalization of the robustness problem in RL, drawing from the principles
    of distributionally robust optimization for bot observation and dynamic alterations,
    provides a foundational framework for future research. By considering the worst-case
    scenarios within a controlled uncertainty set, we can develop RL agents that are
    not only robust to known adversarial attacks but also equipped to handle unexpected
    variations in real-world environments.
  prefs: []
  type: TYPE_NORMAL
- en: The exploration of adversarial training strategies in this survey emphasizes
    the importance of simulating realistic adversarial conditions during the training
    phase. By doing so, RL agents can be better prepared for the complexities and
    uncertainties of real-world operations, leading to more reliable and effective
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, while our work sheds light on the current state of adversarial
    methods in DRL and their role in enhancing agent robustness, it also opens the
    door for further exploration. Future research should focus on refining adversarial
    training techniques, exploring new forms of attacks, and expanding the taxonomy
    as the field evolves. Additionally, there is a need for developing more sophisticated
    models that can balance the trade-off between robustness and performance efficiency.
    As DRL continues to evolve, the pursuit of robust, reliable, and safe autonomous
    agents remains a critical objective, ensuring their applicability and trustworthiness
    in a wide range of real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: \bmhead
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements This work has been supported by the French government under
    the \sayFrance 2030 program, as part of the SystemX Technological Research Institute
    within the Confiance.ai program.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \bibcommenthead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mnih et al. [2015] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness,
    J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G.,
    Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
    D., Legg, S., Hassabis, D.: Human-level control through deep reinforcement learning.
    Nature (2015). Accessed 2023-02-13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silver et al. [2016] Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre,
    L., Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot,
    M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap,
    T., Leach, M., Kavukcuoglu, K., Graepel, T., Hassabis, D.: Mastering the game
    of go with deep neural networks and tree search. Nature (2016). Accessed 2023-02-13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. [2019] Vinyals, O., Babuschkin, I., Czarnecki, W.M., Mathieu,
    M., Dudzik, A., Chung, J., Choi, D.H., Powell, R., Ewalds, T., Georgiev, P., Oh,
    J., Horgan, D., Kroiss, M., Danihelka, I., Huang, A., Sifre, L., Cai, T., Agapiou,
    J.P., Jaderberg, M., Vezhnevets, A.S., Leblond, R., Pohlen, T., Dalibard, V.,
    Budden, D., Sulsky, Y., Molloy, J., Paine, T.L., Gulcehre, C., Wang, Z., Pfaff,
    T., Wu, Y., Ring, R., Yogatama, D., Wünsch, D., McKinney, K., Smith, O., Schaul,
    T., Lillicrap, T., Kavukcuoglu, K., Hassabis, D., Apps, C., Silver, D.: Grandmaster
    level in starcraft ii using multi-agent reinforcement learning. Nature (2019).
    Accessed 2023-02-13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. [2016] Levine, S., Finn, C., Darrell, T., Abbeel, P.: End-to-End
    Training of Deep Visuomotor Policies. arXiv (2016). [http://arxiv.org/abs/1504.00702](http://arxiv.org/abs/1504.00702)
    Accessed 2023-02-13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kiran et al. [2021] Kiran, B.R., Sobh, I., Talpaert, V., Mannion, P., Sallab,
    A.A.A., Yogamani, S., Pérez, P.: Deep Reinforcement Learning for Autonomous Driving:
    A Survey. arXiv (2021). [http://arxiv.org/abs/2002.00444](http://arxiv.org/abs/2002.00444)
    Accessed 2023-02-13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2018] Zhang, D., Han, X., Deng, C.: Review on the research and
    practice of deep learning and reinforcement learning in smart grids. CSEE Journal
    of Power and Energy Systems (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Höfer et al. [2020] Höfer, S., Bekris, K., Handa, A., Gamboa, J.C., Golemo,
    F., Mozifian, M., Atkeson, C., Fox, D., Goldberg, K., Leonard, J., et al.: Perspectives
    on sim2real transfer for robotics: A summary of the r: Ss 2020 workshop. arXiv
    preprint arXiv:2012.03806 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Collins et al. [2019] Collins, J., Howard, D., Leitner, J.: Quantifying the
    reality gap in robotic manipulation tasks. In: 2019 International Conference on
    Robotics and Automation (ICRA), pp. 6706–6712 (2019). IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ilahi et al. [2022] Ilahi, I., Usama, M., Qadir, J., Janjua, M.U., Al-Fuqaha,
    A., Hoang, D.T., Niyato, D.: Challenges and countermeasures for adversarial attacks
    on deep reinforcement learning. IEEE Transactions on Artificial Intelligence (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moos et al. [2022] Moos, J., Hansel, K., Abdulsamad, H., Stark, S., Clever,
    D., Peters, J.: Robust reinforcement learning: A review of foundations and recent
    advances. Machine Learning and Knowledge Extraction 4(1), 276–315 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2019] Yuan, X., He, P., Zhu, Q., Li, X.: Adversarial examples:
    Attacks and defenses for deep learning. IEEE Transactions on Neural Networks and
    Learning Systems (2019). Accessed 2022-10-12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2019] Chen, T., Liu, J., Xiang, Y., Niu, W., Tong, E., Han, Z.:
    Adversarial attack and defense in reinforcement learning-from ai security view.
    Cybersecurity (2019). Accessed 2022-10-12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto [1998] Sutton, R.S., Barto, A.G.: Reinforcement Learning,
    Second Edition: An Introduction. MIT Press, ??? (1998)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Williams [1992] Williams, R.J.: Simple statistical gradient-following algorithms
    for connectionist reinforcement learning. Machine Learning (1992). Accessed 2023-02-22'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mnih et al. [2013] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., Riedmiller, M.: Playing Atari with Deep Reinforcement Learning.
    arXiv (2013). [http://arxiv.org/abs/1312.5602](http://arxiv.org/abs/1312.5602)
    Accessed 2022-11-10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lillicrap et al. [2015] Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N.,
    Erez, T., Tassa, Y., Silver, D., Wierstra, D.: Continuous control with deep reinforcement
    learning. arXiv preprint arXiv:1509.02971 (2015)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. [2018] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T.,
    Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., Silver, D.: Rainbow:
    Combining improvements in deep reinforcement learning. In: Proceedings of the
    AAAI Conference on Artificial Intelligence (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schulman et al. [2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    Klimov, O.: Proximal Policy Optimization Algorithms. arXiv (2017). [http://arxiv.org/abs/1707.06347](http://arxiv.org/abs/1707.06347)
    Accessed 2022-11-10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haarnoja et al. [2018] Haarnoja, T., Zhou, A., Abbeel, P., Levine, S.: Soft
    actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic
    actor. In: International Conference on Machine Learning (2018). PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kuznetsov et al. [2020] Kuznetsov, A., Shvechikov, P., Grishin, A., Vetrov,
    D.: Controlling overestimation bias with truncated mixture of continuous distributional
    quantile critics. In: International Conference on Machine Learning (2020). PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vassilev et al. [2024] Vassilev, A., Oprea, A., Fordyce, A., Andersen, H.:
    Adversarial machine learning: A taxonomy and terminology of attacks and mitigations
    (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Dahmen-Lhuissier, S.: ETSI - Best Security Standards | ETSI Security Standards.
    [https://www.etsi.org/technologies/securing-artificial-intelligence](https://www.etsi.org/technologies/securing-artificial-intelligence)
    Accessed 2022-12-02'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2017] Huang, S., Papernot, N., Goodfellow, I., Duan, Y., Abbeel,
    P.: Adversarial Attacks on Neural Network Policies. arXiv (2017). [http://arxiv.org/abs/1702.02284](http://arxiv.org/abs/1702.02284)
    Accessed 2022-11-22'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barreno et al. [2010] Barreno, M., Nelson, B., Joseph, A.D., Tygar, J.D.: The
    security of machine learning. Machine Learning (2010). Accessed 2022-11-10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brunke et al. [2022] Brunke, L., Greeff, M., Hall, A.W., Yuan, Z., Zhou, S.,
    Panerati, J., Schoellig, A.P.: Safe learning in robotics: From learning-based
    control to safe reinforcement learning. Annual Review of Control, Robotics, and
    Autonomous Systems 5, 411–444 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Morimoto and Doya [2005] Morimoto, J., Doya, K.: Robust reinforcement learning.
    Neural computation 17(2), 335–359 (2005)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behzadan and Munir [2017] Behzadan, V., Munir, A.: Vulnerability of deep reinforcement
    learning to policy induction attacks. In: Perner, P. (ed.) Machine Learning and
    Data Mining in Pattern Recognition. Springer, ??? (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2022] Pan, X., Xiao, C., He, W., Yang, S., Peng, J., Sun, M., Yi,
    J., Yang, Z., Liu, M., Li, B., Song, D.: Characterizing Attacks on Deep Reinforcement
    Learning. arXiv (2022). [http://arxiv.org/abs/1907.09470](http://arxiv.org/abs/1907.09470)
    Accessed 2022-11-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahimian and Mehrotra [2019] Rahimian, H., Mehrotra, S.: Distributionally robust
    optimization: A review. arXiv preprint arXiv:1908.05659 (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hollenstein et al. [2022] Hollenstein, J., Auddy, S., Saveriano, M., Renaudo,
    E., Piater, J.: Action Noise in Off-Policy Deep Reinforcement Learning: Impact
    on Exploration and Performance. arXiv (2022). [http://arxiv.org/abs/2206.03787](http://arxiv.org/abs/2206.03787)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kurakin et al. [2017] Kurakin, A., Goodfellow, I., Bengio, S.: Adversarial
    examples in the physical world. arXiv (2017). [http://arxiv.org/abs/1607.02533](http://arxiv.org/abs/1607.02533)
    Accessed 2022-12-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madry et al. [2017] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu,
    A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint
    arXiv:1706.06083 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi-Dezfooli et al. [2016] Moosavi-Dezfooli, S.-M., Fawzi, A., Frossard,
    P.: Deepfool: a simple and accurate method to fool deep neural networks. In: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2574–2582
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini and Wagner [2017] Carlini, N., Wagner, D.: Towards evaluating the robustness
    of neural networks. In: 2017 IEEE Symposium on Security and Privacy (SP) (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papernot et al. [2016] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M.,
    Celik, Z.B., Swami, A.: The limitations of deep learning in adversarial settings.
    In: 2016 IEEE European Symposium on Security and Privacy (EuroS&P) (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Césaire et al. [2021] Césaire, M., Schott, L., Hajri, H., Lamprier, S., Gallinari,
    P.: Stochastic sparse adversarial attacks. In: 2021 IEEE 33rd International Conference
    on Tools with Artificial Intelligence (ICTAI), pp. 1247–1254 (2021). IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hajri et al. [2022] Hajri, H., Cesaire, M., Schott, L., Lamprier, S., Gallinari,
    P.: Neural adversarial attacks with random noises. International Journal on Artificial
    Intelligence Tools (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattanaik et al. [2017] Pattanaik, A., Tang, Z., Liu, S., Bommannan, G., Chowdhary,
    G.: Robust Deep Reinforcement Learning with Adversarial Attacks. arXiv (2017).
    [http://arxiv.org/abs/1712.03632](http://arxiv.org/abs/1712.03632) Accessed 2022-11-15'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andriushchenko et al. [2020] Andriushchenko, M., Croce, F., Flammarion, N.,
    Hein, M.: Square attack: a query-efficient black-box adversarial attack via random
    search. In: European Conference on Computer Vision, pp. 484–501 (2020). Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhagoji et al. [2017] Bhagoji, A.N., He, W., Li, B., Song, D.: Exploring the
    space of black-box attacks on deep neural networks. arXiv preprint arXiv:1712.09491
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning,
    D., Hsieh, C.-J.: Robust deep reinforcement learning against adversarial perturbations
    on state observations. Advances in Neural Information Processing Systems 33, 21024–21037
    (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Russo and Proutiere [2019] Russo, A., Proutiere, A.: Optimal Attacks on Reinforcement
    Learning Policies. arXiv (2019). [http://arxiv.org/abs/1907.13548](http://arxiv.org/abs/1907.13548)
    Accessed 2022-10-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Russo and Proutiere [2021] Russo, A., Proutiere, A.: Towards optimal attacks
    on reinforcement learning policies. In: 2021 American Control Conference (ACC)
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2020] Zhang, H., Chen, H., Xiao, C., Li, B., Liu, M., Boning,
    D., Hsieh, C.-J.: Robust deep reinforcement learning against adversarial perturbations
    on state observations. In: Advances in Neural Information Processing Systems.
    Curran Associates, Inc., ??? (2020). [https://proceedings.neurips.cc/paper/2020/hash/f0eb6568ea114ba6e293f903c34d7488-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/f0eb6568ea114ba6e293f903c34d7488-Abstract.html)
    Accessed 2022-11-15'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2021] Zhang, H., Chen, H., Boning, D., Hsieh, C.-J.: Robust reinforcement
    learning on state observations with learned optimal adversary. arXiv preprint
    arXiv:2101.08452 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tretschk et al. [2018] Tretschk, E., Oh, S.J., Fritz, M.: Sequential Attacks
    on Agents for Long-Term Adversarial Goals. arXiv (2018). [http://arxiv.org/abs/1805.12487](http://arxiv.org/abs/1805.12487)
    Accessed 2022-10-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baluja and Fischer [2018] Baluja, S., Fischer, I.: Learning to attack: Adversarial
    transformation networks. Proceedings of the AAAI Conference on Artificial Intelligence
    (2018). Accessed 2022-10-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2022] Sun, Y., Zheng, R., Liang, Y., Huang, F.: Who is the strongest
    enemy? towards optimal and efficient evasion attacks in deep rl. International
    Conference on Learning Representations (ICLR) (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinto et al. [2017] Pinto, L., Davidson, J., Sukthankar, R., Gupta, A.: Robust
    adversarial reinforcement learning. In: Proceedings of the 34th International
    Conference on Machine Learning. PMLR, ??? (2017). [https://proceedings.mlr.press/v70/pinto17a.html](https://proceedings.mlr.press/v70/pinto17a.html)
    Accessed 2022-10-18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. [2019] Pan, X., Seita, D., Gao, Y., Canny, J.: Risk averse robust
    adversarial reinforcement learning. In: 2019 International Conference on Robotics
    and Automation (ICRA) (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. [2018] Ma, X., Driggs-Campbell, K., Kochenderfer, M.J.: Improved
    robustness and safety for autonomous vehicle control with adversarial reinforcement
    learning. In: 2018 IEEE Intelligent Vehicles Symposium (IV) (2018). IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich et al. [2015] Heinrich, J., Lanctot, M., Silver, D.: Fictitious self-play
    in extensive-form games. In: International Conference on Machine Learning (2015).
    PMLR'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heinrich and Silver [2016] Heinrich, J., Silver, D.: Deep reinforcement learning
    from self-play in imperfect-information games. arXiv preprint arXiv:1603.01121
    (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tessler et al. [2019] Tessler, C., Efroni, Y., Mannor, S.: Action robust reinforcement
    learning and applications in continuous control. In: Proceedings of the 36th International
    Conference on Machine Learning. PMLR, ??? (2019). [https://proceedings.mlr.press/v97/tessler19a.html](https://proceedings.mlr.press/v97/tessler19a.html)
    Accessed 2022-10-24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gleave et al. [2021] Gleave, A., Dennis, M., Wild, C., Kant, N., Levine, S.,
    Russell, S.: Adversarial Policies: Attacking Deep Reinforcement Learning. arXiv
    (2021). [http://arxiv.org/abs/1905.10615](http://arxiv.org/abs/1905.10615) Accessed
    2022-11-09'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022] Wang, T.T., Gleave, A., Belrose, N., Tseng, T., Miller,
    J., Dennis, M.D., Duan, Y., Pogrebniak, V., Levine, S., Russell, S.: Adversarial
    Policies Beat Professional-Level Go AIs. arXiv (2022). [http://arxiv.org/abs/2211.00241](http://arxiv.org/abs/2211.00241)
    Accessed 2022-11-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2021] Wu, X., Guo, W., Wei, H., Xing, X.: Adversarial policy training
    against deep reinforcement learning. In: 30th USENIX Security Symposium (USENIX
    Security 21) (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timbers et al. [2022] Timbers, F., Bard, N., Lockhart, E., Lanctot, M., Schmid,
    M., Burch, N., Schrittwieser, J., Hubert, T., Bowling, M.: Approximate exploitability:
    learning a best response. In: Proceedings of the International Joint Conference
    on Artificial Intelligence (IJCAI) (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Casper et al. [2022] Casper, S., Killian, T., Kreiman, G., Hadfield-Menell,
    D.: Red teaming with mind reading: White-box adversarial policies against rl agents.
    arXiv preprint arXiv:2209.02167 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2020] Lee, X.Y., Ghadai, S., Tan, K.L., Hegde, C., Sarkar, S.:
    Spatiotemporally constrained action space attacks on deep reinforcement learning
    agents. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol.
    34, pp. 4577–4584 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan et al. [2020] Tan, K.L., Esfandiari, Y., Lee, X.Y., Sarkar, S., et al.:
    Robustifying reinforcement learning agents via action space adversarial training.
    In: 2020 American Control Conference (ACC), pp. 3959–3964 (2020). IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schott et al. [2022] Schott, L., Hajri, H., Lamprier, S.: Improving robustness
    of deep reinforcement learning agents: Environment attack based on the critic
    network. In: 2022 International Joint Conference on Neural Networks (IJCNN) (2022).
    [http://arxiv.org/abs/2104.03154](http://arxiv.org/abs/2104.03154) Accessed 2022-11-08'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. [2018] Bai, X., Niu, W., Liu, J., Gao, X., Xiang, Y., Liu, J.: Adversarial
    examples construction towards white-box q table variation in dqn pathfinding training.
    In: 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)
    (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2018] Chen, T., Niu, W., Xiang, Y., Bai, X., Liu, J., Han, Z.,
    Li, G.: Gradient Band-based Adversarial Training for Generalized Attack Immunity
    of A3C Path Finding. arXiv (2018). [http://arxiv.org/abs/1807.06752](http://arxiv.org/abs/1807.06752)
    Accessed 2022-10-20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow et al. [2015] Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining
    and Harnessing Adversarial Examples. arXiv (2015). [http://arxiv.org/abs/1412.6572](http://arxiv.org/abs/1412.6572)
    Accessed 2022-12-05'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behzadan and Hsu [2019] Behzadan, V., Hsu, W.: Adversarial Exploitation of
    Policy Imitation. arXiv (2019). [http://arxiv.org/abs/1906.01121](http://arxiv.org/abs/1906.01121)
    Accessed 2022-10-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kos and Song [2017] Kos, J., Song, D.: Delving into adversarial attacks on
    deep policies. arXiv (2017). [http://arxiv.org/abs/1705.06452](http://arxiv.org/abs/1705.06452)
    Accessed 2022-11-22'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2019] Lin, Y.-C., Hong, Z.-W., Liao, Y.-H., Shih, M.-L., Liu, M.-Y.,
    Sun, M.: Tactics of Adversarial Attack on Deep Reinforcement Learning Agents.
    arXiv (2019). [http://arxiv.org/abs/1703.06748](http://arxiv.org/abs/1703.06748)
    Accessed 2022-10-14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2020] Yang, C.-H.H., Qi, J., Chen, P.-Y., Ouyang, Y., Hung, I.-T.D.,
    Lee, C.-H., Ma, X.: Enhanced adversarial strategically-timed attacks against deep
    reinforcement learning. In: ICASSP 2020 - 2020 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP) (2020). [http://arxiv.org/abs/2002.09027](http://arxiv.org/abs/2002.09027)
    Accessed 2022-10-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2020] Sun, J., Zhang, T., Xie, X., Ma, L., Zheng, Y., Chen, K.,
    Liu, Y.: Stealthy and Efficient Adversarial Attacks against Deep Reinforcement
    Learning. arXiv (2020). [http://arxiv.org/abs/2005.07099](http://arxiv.org/abs/2005.07099)
    Accessed 2022-11-22'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chan et al. [2020] Chan, P.P.K., Wang, Y., Yeung, D.S.: Adversarial attack
    against deep reinforcement learning with static reward impact map. In: Proceedings
    of the 15th ACM Asia Conference on Computer and Communications Security. Association
    for Computing Machinery, ??? (2020). [https://doi.org/10.1145/3320269.3384715](https://doi.org/10.1145/3320269.3384715)
    Accessed 2022-11-22'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qu et al. [2021] Qu, X., Sun, Z., Ong, Y.-S., Gupta, A., Wei, P.: Minimalistic
    attacks: How little it takes to fool deep reinforcement learning policies. IEEE
    Transactions on Cognitive and Developmental Systems (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hussenot et al. [2020] Hussenot, L., Geist, M., Pietquin, O.: CopyCAT: Taking
    Control of Neural Policies with Constant Attacks. arXiv (2020). [http://arxiv.org/abs/1905.12282](http://arxiv.org/abs/1905.12282)
    Accessed 2022-10-20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mo et al. [2022] Mo, K., Tang, W., Li, J., Yuan, X.: Attacking deep reinforcement
    learning with decoupled adversarial policy. IEEE Transactions on Dependable and
    Secure Computing (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tekgul et al. [2022] Tekgul, B.G.A., Wang, S., Marchal, S., Asokan, N.: Real-time
    Adversarial Perturbations against Deep Reinforcement Learning Policies: Attacks
    and Defenses. arXiv (2022). [http://arxiv.org/abs/2106.08746](http://arxiv.org/abs/2106.08746)
    Accessed 2022-10-24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moosavi-Dezfooli et al. [2017] Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O.,
    Frossard, P.: Universal adversarial perturbations. arXiv (2017). [http://arxiv.org/abs/1610.08401](http://arxiv.org/abs/1610.08401)
    Accessed 2023-01-12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dorato [1987] Dorato, P.: A historical review of robust control. IEEE Control
    Systems Magazine 7(2), 44–47 (1987)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu [2017] Wu, D.J.: Is attacking machine learning easier than defending it?
    (2017). [http://cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html](http://cleverhans.io/security/privacy/ml/2017/02/15/why-attacking-machine-learning-is-easier-than-defending-it.html)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2021] Huang, H., Wang, Y., Erfani, S., Gu, Q., Bailey, J., Ma,
    X.: Exploring architectural ingredients of adversarially robust deep neural networks.
    In: Advances in Neural Information Processing Systems. Curran Associates, Inc.,
    ??? (2021). [https://proceedings.neurips.cc/paper/2021/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/2bd7f907b7f5b6bbd91822c0c7b835f6-Abstract.html)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papernot et al. [2016] Papernot, N., McDaniel, P., Wu, X., Jha, S., Swami,
    A.: Distillation as a Defense to Adversarial Perturbations against Deep Neural
    Networks. arXiv (2016). [http://arxiv.org/abs/1511.04508](http://arxiv.org/abs/1511.04508)
    Accessed 2023-02-14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rusu et al. [2016] Rusu, A.A., Colmenarejo, S.G., Gulcehre, C., Desjardins,
    G., Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu, K., Hadsell, R.: Policy
    Distillation. arXiv (2016). [http://arxiv.org/abs/1511.06295](http://arxiv.org/abs/1511.06295)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Czarnecki et al. [2019] Czarnecki, W.M., Pascanu, R., Osindero, S., Jayakumar,
    S., Swirszcz, G., Jaderberg, M.: Distilling policy distillation. In: Proceedings
    of the Twenty-Second International Conference on Artificial Intelligence And Statistics.
    PMLR, ??? (2019). [https://proceedings.mlr.press/v89/czarnecki19a.html](https://proceedings.mlr.press/v89/czarnecki19a.html)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wierstra et al. [2007] Wierstra, D., Foerster, A., Peters, J., Schmidhuber,
    J.: Solving deep memory pomdps with recurrent policy gradients. In: Artificial
    Neural Networks–ICANN 2007: 17th International Conference, Porto, Portugal, September
    9-13, 2007, Proceedings, Part I 17, pp. 697–706 (2007). Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar [2019] Kumar, A.: Enhancing performance of reinforcement learning models
    in the presence of noisy rewards. Thesis (April 2019). [https://repositories.lib.utexas.edu/handle/2152/75758](https://repositories.lib.utexas.edu/handle/2152/75758)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2020] Wang, J., Liu, Y., Li, B.: Reinforcement Learning with Perturbed
    Rewards. arXiv (2020). [http://arxiv.org/abs/1810.01032](http://arxiv.org/abs/1810.01032)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. [2022] Liang, Y., Sun, Y., Zheng, R., Huang, F.: Efficient adversarial
    training without attacking: Worst-case-aware robust reinforcement learning. Advances
    in Neural Information Processing Systems 35, 22547–22561 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Queeney and Benosman [2023] Queeney, J., Benosman, M.: Risk-averse model uncertainty
    for distributionally robust safe reinforcement learning. arXiv preprint arXiv:2301.12593
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2017] Lin, Y.-C., Liu, M.-Y., Sun, M., Huang, J.-B.: Detecting
    Adversarial Attacks on Neural Network Policies with Visual Foresight. arXiv (2017).
    [http://arxiv.org/abs/1710.00814](http://arxiv.org/abs/1710.00814) Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hickling et al. [2022] Hickling, T., Aouf, N., Spencer, P.: Robust Adversarial
    Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance
    and Planning. arXiv (2022). [http://arxiv.org/abs/2206.02670](http://arxiv.org/abs/2206.02670)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metzen et al. [2017] Metzen, J.H., Genewein, T., Fischer, V., Bischoff, B.:
    On Detecting Adversarial Perturbations. arXiv (2017). [http://arxiv.org/abs/1702.04267](http://arxiv.org/abs/1702.04267)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang et al. [2018] Pang, T., Du, C., Dong, Y., Zhu, J.: Towards robust detection
    of adversarial examples. In: Advances in Neural Information Processing Systems.
    Curran Associates, Inc., ??? (2018). [https://proceedings.neurips.cc/paper/2018/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/e0f7a4d0ef9b84b83b693bbf3feb8e6e-Abstract.html)
    Accessed 2023-01-23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carlini and Wagner [2017] Carlini, N., Wagner, D.: Adversarial Examples Are
    Not Easily Detected: Bypassing Ten Detection Methods. arXiv (2017). [http://arxiv.org/abs/1705.07263](http://arxiv.org/abs/1705.07263)
    Accessed 2023-02-15'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. [2017] He, W., Wei, J., Chen, X., Carlini, N., Song, D.: Adversarial
    example defense: Ensembles of weak defenses are not strong. In: 11th USENIX Workshop
    on Offensive Technologies (WOOT 17) (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behzadan and Hsu [2019] Behzadan, V., Hsu, W.: RL-Based Method for Benchmarking
    the Adversarial Resilience and Robustness of Deep Reinforcement Learning Policies.
    arXiv (2019). [http://arxiv.org/abs/1906.01110](http://arxiv.org/abs/1906.01110)
    Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behzadan and Munir [2018] Behzadan, V., Munir, A.: Adversarial Reinforcement
    Learning Framework for Benchmarking Collision Avoidance Mechanisms in Autonomous
    Vehicles. arXiv (2018). [http://arxiv.org/abs/1806.01368](http://arxiv.org/abs/1806.01368)
    Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behzadan and Hsu [2019] Behzadan, V., Hsu, W.: Sequential Triggers for Watermarking
    of Deep Reinforcement Learning Policies. arXiv (2019). [http://arxiv.org/abs/1906.01126](http://arxiv.org/abs/1906.01126)
    Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Raffin et al. [2021] Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus,
    M., Dormann, N.: Stable-baselines3: Reliable reinforcement learning implementations.
    Journal of Machine Learning Research (2021). Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng et al. [2022] Weng, J., Chen, H., Yan, D., You, K., Duburcq, A., Zhang,
    M., Su, Y., Su, H., Zhu, J.: Tianshou: a Highly Modularized Deep Reinforcement
    Learning Library. arXiv (2022). [http://arxiv.org/abs/2107.14171](http://arxiv.org/abs/2107.14171)
    Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behzadan and Munir [2017] Behzadan, V., Munir, A.: Whatever Does Not Kill Deep
    Reinforcement Learning, Makes It Stronger. arXiv (2017). [http://arxiv.org/abs/1712.09344](http://arxiv.org/abs/1712.09344)
    Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papernot et al. [2018] Papernot, N., Faghri, F., Carlini, N., Goodfellow, I.,
    Feinman, R., Kurakin, A., Xie, C., Sharma, Y., Brown, T., Roy, A., Matyasko, A.,
    Behzadan, V., Hambardzumyan, K., Zhang, Z., Juang, Y.-L., Li, Z., Sheatsley, R.,
    Garg, A., Uesato, J., Gierke, W., Dong, Y., Berthelot, D., Hendricks, P., Rauber,
    J., Long, R., McDaniel, P.: Technical Report on the CleverHans v2.1.0 Adversarial
    Examples Library. arXiv (2018). [http://arxiv.org/abs/1610.00768](http://arxiv.org/abs/1610.00768)
    Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. [2022] Yuan, Z., Hall, A.W., Zhou, S., Brunke, L., Greeff, M.,
    Panerati, J., Schoellig, A.P.: safe-control-gym: a Unified Benchmark Suite for
    Safe Learning-based Control and Reinforcement Learning in Robotics. arXiv (2022).
    [http://arxiv.org/abs/2109.06325](http://arxiv.org/abs/2109.06325) Accessed 2023-01-26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heuillet et al. [2021] Heuillet, A., Couthouis, F., Díaz-Rodríguez, N.: Explainability
    in deep reinforcement learning. Knowledge-Based Systems (2021). Accessed 2023-01-27'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vouros [2022] Vouros, G.A.: Explainable deep reinforcement learning: State
    of the art and challenges. ACM Computing Surveys (2022). Accessed 2023-01-27'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Datta et al. [2021] Datta, A., Fredrikson, M., Leino, K., Lu, K., Sen, S.,
    Wang, Z.: Machine learning explainability and robustness: Connected at the hip.
    In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data
    Mining. Association for Computing Machinery, ??? (2021). [https://doi.org/10.1145/3447548.3470806](https://doi.org/10.1145/3447548.3470806)
    Accessed 2023-01-27'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2018] Sun, L., Tan, M., Zhou, Z.: A survey of practical adversarial
    example attacks. Cybersecurity (2018). Accessed 2023-01-27'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
