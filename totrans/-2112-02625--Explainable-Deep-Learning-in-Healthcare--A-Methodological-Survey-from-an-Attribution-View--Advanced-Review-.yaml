- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:49:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.02625] Explainable Deep Learning in Healthcare: A Methodological Survey
    from an Attribution View [Advanced Review]'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.02625] 可解释深度学习在医疗保健中的应用：从归因视角的研究方法综述 [高级综述]'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02625](https://ar5iv.labs.arxiv.org/html/2112.02625)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02625](https://ar5iv.labs.arxiv.org/html/2112.02625)
- en: 'Explainable Deep Learning in Healthcare: A Methodological Survey from an Attribution
    View [Advanced Review]'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释深度学习在医疗保健中的应用：从归因视角的研究方法综述 [高级综述]
- en: 'Di Jin Equal contributions Elena Sergeeva^†^†footnotemark: Wei-Hung Weng^†^†footnotemark:
    Geeticka Chauhan^†^†footnotemark: Peter Szolovits Corresponding author: psz@mit.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '贡献作者：Di Jin Equal contributions Elena Sergeeva^†^†footnotemark: Wei-Hung Weng^†^†footnotemark:
    Geeticka Chauhan^†^†footnotemark: Peter Szolovits 通讯作者：psz@mit.edu'
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The increasing availability of large collections of electronic health record
    (EHR) data and unprecedented technical advances in deep learning (DL) have sparked
    a surge of research interest in developing DL based clinical decision support
    systems for diagnosis, prognosis, and treatment. Despite the recognition of the
    value of deep learning in healthcare, impediments to further adoption in real
    healthcare settings remain due to the black-box nature of DL. Therefore, there
    is an emerging need for interpretable DL, which allows end users to evaluate the
    model decision making to know whether to accept or reject predictions and recommendations
    before an action is taken. In this review, we focus on the interpretability of
    the DL models in healthcare. We start by introducing the methods for interpretability
    in depth and comprehensively as a methodological reference for future researchers
    or clinical practitioners in this field. Besides the methods’ details, we also
    include a discussion of advantages and disadvantages of these methods and which
    scenarios each of them is suitable for, so that interested readers can know how
    to compare and choose among them for use. Moreover, we discuss how these methods,
    originally developed for solving general-domain problems, have been adapted and
    applied to healthcare problems and how they can help physicians better understand
    these data-driven technologies. Overall, we hope this survey can help researchers
    and practitioners in both artificial intelligence (AI) and clinical fields understand
    what methods we have for enhancing the interpretability of their DL models and
    choose the optimal one accordingly.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大量电子健康记录（EHR）数据的日益丰富以及深度学习（DL）技术的空前进展引发了在诊断、预后和治疗方面开发基于DL的临床决策支持系统的研究热潮。尽管深度学习在医疗保健中的价值已经得到认可，但由于DL的黑箱特性，进一步在实际医疗环境中的采用仍然存在障碍。因此，出现了对可解释性深度学习的需求，这使得最终用户能够评估模型的决策过程，以了解在采取行动之前是否接受或拒绝预测和建议。在这篇综述中，我们重点关注医疗保健中DL模型的可解释性。我们首先深入全面地介绍了可解释性的方法，作为未来研究人员或临床从业者的研究方法参考。除了方法的详细信息外，我们还讨论了这些方法的优缺点以及每种方法适用的场景，以便感兴趣的读者能够了解如何比较和选择使用这些方法。此外，我们讨论了这些方法如何从解决通用领域问题的最初发展中适应并应用于医疗保健问题，以及它们如何帮助医生更好地理解这些数据驱动的技术。总体而言，我们希望这篇综述能够帮助人工智能（AI）和临床领域的研究人员和从业者了解我们在增强DL模型可解释性方面拥有的方法，并据此选择最佳方法。
- en: 'This article is categorized under:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文分类为：
- en: 'Keywords: Interpretable Deep Learning, Deep Learning in medicine'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：可解释深度学习，医学中的深度学习
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, the wide adoption of electronic health record (EHR) systems
    by healthcare organizations and subsequent availability of large collections of
    EHR data have made the application of Artificial Intelligence (AI) techniques
    in healthcare more feasible. The EHR data contain rich, longitudinal, and patient-specific
    information including both structured data (e.g., patient demographics, diagnoses,
    procedures) as well as unstructured data, such as physician notes and medical
    images [[Mesko, 2017](#bib.bibx108)]. Meanwhile, deep learning (DL), a family
    of machine learning (ML) models based on deep neural networks, has achieved remarkable
    progress in the last decade on various datasets for different modalities including
    images, natural language, and structured time series data [[LeCun et al., 2015](#bib.bibx89)].
    The availability of large-scale data and unprecedented technical advances have
    come together to spark a surge of research interest in developing a variety of
    deep learning based clinical decision support systems for diagnosis, prognosis
    and treatment [[Murdoch and Detsky, 2013](#bib.bibx116)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，医疗保健组织广泛采用电子健康记录（EHR）系统，以及随之而来的大量EHR数据，使得在医疗保健中应用人工智能（AI）技术变得更加可行。EHR数据包含丰富的、长期的和患者特定的信息，包括结构化数据（例如患者人口统计学、诊断、程序）以及非结构化数据，如医生笔记和医学影像[[Mesko,
    2017](#bib.bibx108)]。与此同时，基于深度神经网络的深度学习（DL）模型在过去十年中在各种数据集上取得了显著进展，包括图像、自然语言和结构化时间序列数据[[LeCun
    et al., 2015](#bib.bibx89)]。大规模数据的可用性和前所未有的技术进步共同激发了对开发多种深度学习基础的临床决策支持系统的研究兴趣，用于诊断、预后和治疗[[Murdoch
    and Detsky, 2013](#bib.bibx116)]。
- en: 'Despite the recognition of the value of deep learning in healthcare, impediments
    to further adoption in real healthcare settings remain [[Tonekaboni et al., 2019a](#bib.bibx166)].
    One pivotal impediment relates to the black box nature, or opacity, of deep learning
    algorithms, in which there is no easily discernible logic connecting the data
    about a case to the decisions of the model. Healthcare abounds with possible “high
    stakes” applications of deep learning algorithms: predicting a patient’s likelihood
    of readmission to the hospital [[Ashfaq et al., 2019](#bib.bibx12)], making the
    diagnosis of a patient’s disease [[Esteva et al., 2017](#bib.bibx43)], suggesting
    the optimal drug prescription and therapy plan [[Rough et al., 2020](#bib.bibx136)],
    just to name a few. In these critical use cases that include clinical decision
    making, there is some hesitation in the deployment of such models because the
    cost of model mis-classification is potentially high [[Mozaffari-Kermani et al.,
    2014](#bib.bibx113)]. Moreover, it has been widely demonstrated that deep learning
    models are not robust and may easily encounter failures in the face of both artificial
    and natural noise [[Szegedy et al., 2014](#bib.bibx163), [Finlayson et al., 2019a](#bib.bibx48),
    [Jin et al., 2020](#bib.bibx75)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在医疗保健中的价值已经得到认可，但在实际医疗环境中进一步采用仍面临障碍[[Tonekaboni et al., 2019a](#bib.bibx166)]。一个关键障碍与深度学习算法的黑箱性质或不透明性有关，在这种情况下，没有容易辨识的数据逻辑将案例数据与模型的决策联系起来。医疗保健中充满了深度学习算法可能的“高风险”应用：预测患者的再入院可能性[[Ashfaq
    et al., 2019](#bib.bibx12)]，诊断患者的疾病[[Esteva et al., 2017](#bib.bibx43)]，建议最佳的药物处方和治疗计划[[Rough
    et al., 2020](#bib.bibx136)]，仅举几例。在这些包括临床决策的关键使用案例中，由于模型误分类的成本可能很高，部署此类模型时存在一些犹豫[[Mozaffari-Kermani
    et al., 2014](#bib.bibx113)]。此外，已有广泛证明，深度学习模型并不稳健，可能在面对人工和自然噪声时容易出现失败[[Szegedy
    et al., 2014](#bib.bibx163), [Finlayson et al., 2019a](#bib.bibx48), [Jin et al.,
    2020](#bib.bibx75)]。
- en: Artificial intelligence (AI) systems are, on the whole, not expected to act
    autonomously in patient care, but to serve as decision support for human clinicians.
    To support the required communication between such systems and people, and to
    allow the person to assess the reliability of the system’s advice, we seek to
    build systems that are interpretable. Interpretable DL allows algorithm designers
    to interrogate, understand, debug, and even improve the systems to be deployed
    by analyzing and interpreting the behavior of black-box DL systems. From the end
    user perspective, interpretable DL allows end users to evaluate the model decision
    making to determine whether to accept or reject predictions and recommendations
    before an action is taken.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）系统通常不期望在患者护理中自主行动，而是作为人类临床医生的决策支持。为了支持这些系统与人之间的必要沟通，并让人能够评估系统建议的可靠性，我们力求构建可解释的系统。可解释的深度学习（DL）允许算法设计者通过分析和解释黑箱DL系统的行为来审问、理解、调试甚至改进待部署的系统。从最终用户的角度来看，可解释的DL使最终用户能够评估模型的决策过程，以确定是否接受或拒绝预测和建议，进而决定是否采取行动。
- en: In particular, in this review we focus on the interpretability of the DL models
    in health care. Such models are known for both their complexity and high performance
    on a variety of tasks, yet the decisions and recommendations of deep learning
    systems may be biased [[Gianfrancesco et al., 2018](#bib.bibx54)]. Interpretability
    can offer one effective approach to ensuring that such systems are free from bias
    and fair in scoring different ethnic and social groups [[Hajian et al., 2016](#bib.bibx58)].
    Many DL systems have already been deployed to make decisions and recommendations
    in non-healthcare settings for tens of millions of people around the world (e.g.,
    Netflix, Google, Amazon) and we hope that DL applied in healthcare will also become
    widespread [[Esteva et al., 2019](#bib.bibx44)]. To this end, we need help from
    interpretability to better understand the resulting models to help prevent potential
    negative impacts. Lastly, there are some legal regulations such as the European
    Union (EU)’s General Data Protection Regulation (GDPR) that require organizations
    that use patient data for predictions and recommendations to provide on demand
    explanations for an output of the algorithm, which is called a “right to explanation” [[Tesfay
    et al., 2018](#bib.bibx165), [Edwards and Veale, 2018](#bib.bibx39)]. The inability
    to provide such explanations on demand may result in large penalties for the organizations
    involved.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在这篇综述中，我们专注于健康护理领域DL模型的可解释性。这些模型因其复杂性和在各种任务上的高性能而著称，但深度学习系统的决策和建议可能存在偏见[[Gianfrancesco
    et al., 2018](#bib.bibx54)]。可解释性可以提供一种有效的方法，以确保这些系统没有偏见，并在对不同种族和社会群体进行评分时公平[[Hajian
    et al., 2016](#bib.bibx58)]。许多DL系统已经在非医疗环境中部署，用于对全球数千万人的决策和建议（例如，Netflix、Google、Amazon），我们希望DL在医疗保健中的应用也能变得普及[[Esteva
    et al., 2019](#bib.bibx44)]。为此，我们需要借助可解释性更好地理解模型，以帮助防止潜在的负面影响。最后，有一些法律法规，如欧盟（EU）的《通用数据保护条例》（GDPR），要求使用患者数据进行预测和建议的组织提供算法输出的按需解释，这被称为“解释权”[[Tesfay
    et al., 2018](#bib.bibx165), [Edwards and Veale, 2018](#bib.bibx39)]。无法提供这种按需解释可能会导致相关组织面临巨额罚款。
- en: 'It should be noted that the notion of explanation of a decision in itself is
    not a very well defined concept: indeed the original EU GDPR Recital 71 does not
    provide a clear definition beyond stating a person’s right to obtain it. There
    have been active discussions in the community on this notion [[Lipton, 2018](#bib.bibx97)];
    for instance, [[Muggleton et al., 2018](#bib.bibx114)] proposed an operational
    definition of comprehensibility and interpretability based on the ultra-strong
    criteria for Machine Learning proposed by [[Michie, 1988](#bib.bibx109)] and Inductive
    Logic Programming [[Kovalerchuk et al., 2021](#bib.bibx84)]. However, no single
    uniform definition has been reached: for any complex model with no superficial
    components, any simple explanation is inherently unfaithful to the underlying
    model. The decision on what definition of explanation to use necessarily affects
    the properties of the methods used to produce them: for example focusing on producing
    a per example explanation vs. the structure of the network analysis favors local
    (why this particular example resulted in a given prediction) explanation over
    global ones (what kinds of knowledge are encoded in the model and how they affect
    predictions). In this work, we first cover the most common type of interpretation
    method, in which an explanation is an assignment of a score to each input element
    that reflects its importance to a model’s conclusions. We also briefly discuss
    example based explanation methods. Other approaches to interpretability include
    a more recent focus on feature interactions for neural networks [[Sundararajan
    et al., 2020](#bib.bibx158), [Tsang et al., 2018](#bib.bibx168), [Tsang et al.,
    2020](#bib.bibx169)] and whole network behavior analysis [[Carter et al., 2019](#bib.bibx23)].'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，决策解释的概念本身并不是一个定义明确的概念：实际上，原始的欧盟 GDPR 第71条并没有提供一个清晰的定义，只是声明了一个人有权获得这种解释。关于这一概念，社区内有过积极的讨论[[Lipton,
    2018](#bib.bibx97)]；例如，[[Muggleton et al., 2018](#bib.bibx114)] 提出了一个基于[[Michie,
    1988](#bib.bibx109)] 提出的超强标准的可理解性和可解释性的操作定义，以及归纳逻辑编程[[Kovalerchuk et al., 2021](#bib.bibx84)]。然而，尚未达成统一的定义：对于任何复杂模型，任何简单的解释都本质上不忠于基础模型。决定使用何种解释定义必然会影响生成这些解释的方法的属性：例如，集中于生成单个示例解释与网络分析结构的关注会使得局部解释（为什么这个特定示例导致了给定的预测）优于全局解释（模型中编码了什么样的知识及其如何影响预测）。在这项工作中，我们首先涵盖了最常见的解释方法类型，其中解释是对每个输入元素分配一个分数，反映其对模型结论的重要性。我们还简要讨论了基于示例的解释方法。其他可解释性的方法包括对神经网络特征交互的最新关注[[Sundararajan
    et al., 2020](#bib.bibx158), [Tsang et al., 2018](#bib.bibx168), [Tsang et al.,
    2020](#bib.bibx169)]和整个网络行为分析[[Carter et al., 2019](#bib.bibx23)]。
- en: It is conventionally thought that there is a trade-off between model interpretability
    and performance (e.g., F1, accuracy). For example, more interpretable models such
    as regression models and decision trees often perform less well on many prediction
    tasks compared to less interpretable models such as deep learning models. With
    this constraint, researchers have to balance the desire for the most highly performing
    model against adequate interpretability. Fortunately in the last few years, researchers
    have proposed many new methods that can maintain the model performance while producing
    good explanations, such as LIME [[Ribeiro et al., 2016a](#bib.bibx131)], RETAIN [[Choi
    et al., 2016](#bib.bibx30)], and SHAP [[Lundberg and Lee, 2017](#bib.bibx101)],
    described below. And many of them have been adapted and applied to healthcare
    problems with good interpretability achieved. This survey aims to provide a comprehensive
    and in-depth summary and discussion over such methods.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为，模型可解释性与性能（例如 F1、准确性）之间存在权衡。例如，更可解释的模型，如回归模型和决策树，在许多预测任务上的表现通常不如较少可解释的模型，如深度学习模型。在这种限制下，研究人员必须平衡对性能最高模型的渴望与适当的可解释性。幸运的是，近年来，研究人员提出了许多新方法，这些方法可以在保持模型性能的同时提供良好的解释，例如
    LIME[[Ribeiro et al., 2016a](#bib.bibx131)]、RETAIN[[Choi et al., 2016](#bib.bibx30)]和
    SHAP[[Lundberg and Lee, 2017](#bib.bibx101)]，如下面所述。许多这些方法已被调整并应用于医疗保健问题，取得了良好的可解释性。这项调查旨在提供这些方法的全面而深入的总结和讨论。
- en: 'Previous surveys on explainable ML for healthcare [[Ahmad et al., 2018](#bib.bibx2),
    [Holzinger et al., 2019](#bib.bibx66), [Wiens et al., 2019](#bib.bibx178), [Tonekaboni
    et al., 2019a](#bib.bibx166), [Vellido, 2019](#bib.bibx172), [Payrovnaziri et al.,
    2020](#bib.bibx122)] mainly discuss the definition, concept, importance, application,
    evaluation, and high-level overview of methods for interpretability. In contrast,
    we will focus on introducing the methods for interpretability in depth so as to
    provide methodological guidance for future researchers or clinical practitioners
    in this field. Besides the methods’ details, we will also include a discussion
    of advantages and disadvantages of these methods and which scenarios each of them
    is suitable for, so that interested readers can know how to compare and choose
    among them for use. Moreover, we will discuss how these methods originally developed
    for solving general-domain problems have been adapted and applied to healthcare
    problems and how they can help physicians better understand these data-driven
    technologies. Overall, we hope this survey can help researchers and practitioners
    in both AI and clinical fields understand what methods we have for enhancing the
    interpretability of their DL models and choose the optimal one accordingly based
    on a deep and thorough understanding. For readers’ convenience, we have provided
    a map between all abbreviations to be used and their corresponding full names
    in Table [2](#Sx1.T2 "Table 2 ‣ Funding Information ‣ Explainable Deep Learning
    in Healthcare: A Methodological Survey from an Attribution View [Advanced Review]").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '以前关于医疗保健的可解释性机器学习的调查[[Ahmad et al., 2018](#bib.bibx2), [Holzinger et al., 2019](#bib.bibx66),
    [Wiens et al., 2019](#bib.bibx178), [Tonekaboni et al., 2019a](#bib.bibx166),
    [Vellido, 2019](#bib.bibx172), [Payrovnaziri et al., 2020](#bib.bibx122)]主要讨论了可解释性方法的定义、概念、重要性、应用、评估和高层次概述。相比之下，我们将重点介绍可解释性方法的细节，以便为未来的研究人员或临床实践者提供方法论指导。除了方法的详细信息，我们还将讨论这些方法的优缺点以及它们适合的场景，以便感兴趣的读者可以了解如何比较和选择使用。此外，我们将讨论这些最初用于解决通用领域问题的方法如何被调整和应用于医疗保健问题，以及它们如何帮助医生更好地理解这些数据驱动的技术。总体而言，我们希望这项调查能够帮助人工智能和临床领域的研究人员和实践者了解我们为增强深度学习模型可解释性所拥有的方法，并根据深入和全面的理解选择最优的方法。为了方便读者，我们在表[2](#Sx1.T2
    "Table 2 ‣ Funding Information ‣ Explainable Deep Learning in Healthcare: A Methodological
    Survey from an Attribution View [Advanced Review]")中提供了所有缩写与其对应全名的对照表。'
- en: 'Paper Selection:'
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 论文选择：
- en: 'We first conducted a systematic search of papers using MEDLINE, IEEE Xplore,
    Association for Computing Machinery (ACM), and ACL Anthology databases, several
    prestigious clinical journals’ websites such as Nature, JAMA, JAMIA, BMC, Elsevier,
    Springer, Plos One, etc., as well as the top AI conferences such as NeurIPS, ICML,
    ICLR, AAAI, KDD, etc.. The keywords for our searches are: (explainable OR explainability
    OR interpretable OR interpretability OR understandable OR understandability OR
    comprehensible OR comprehensibility) AND (machine learning OR artificial intelligence
    OR deep learning OR AI OR neural network). After initial searching, we conducted
    manual filtering by reading titles and abstracts and only retained three types
    of works for subsequent careful reading: interpretability methods developed for
    general domain problems, interpretability methods specifically developed for healthcare
    problems, and healthcare applications that involve interpretability. We only covered
    the methods that can interpret DL models. The literature of explanation methods
    for DL grows rapidly, so any review of this type is captive to its date of completion.
    Searching the above-mentioned sources with the keywords we used for recent articles
    should help to bring an appreciation of the field up to date.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用MEDLINE、IEEE Xplore、计算机协会（ACM）和ACL Anthology数据库，以及几个知名临床期刊网站（如Nature、JAMA、JAMIA、BMC、Elsevier、Springer、Plos
    One等）和顶级人工智能会议（如NeurIPS、ICML、ICLR、AAAI、KDD等）进行系统搜索。我们的搜索关键词是：（explainable OR explainability
    OR interpretable OR interpretability OR understandable OR understandability OR
    comprehensible OR comprehensibility）AND（machine learning OR artificial intelligence
    OR deep learning OR AI OR neural network）。在初步搜索后，我们通过阅读标题和摘要进行手动筛选，仅保留三类作品以供后续仔细阅读：为通用领域问题开发的可解释性方法、专门为医疗保健问题开发的可解释性方法以及涉及可解释性的医疗保健应用。我们仅涵盖能够解释深度学习模型的方法。深度学习解释方法的文献快速增长，因此任何此类综述都受限于其完成日期。使用我们用于搜索最近文章的关键词在上述来源中进行搜索应有助于更新对该领域的认识。
- en: 2 Interpretability Methods
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 可解释性方法
- en: In this section, we will introduce various kinds of interpretability methods,
    which aim to assign an attribution value, sometimes also called ”relevance” or
    ”contribution”, to each input feature of a network. Such interpretability methods
    can thus be called attribution methods. More formally, consider a deep neural
    network (DNN) that takes an input $x=[x_{1},...,x_{N}]$ and produces an output
    $S(x)=[S_{1}(x),...,S_{C}(x)]$, where $C$ is the total number of output neurons.
    Given a specific target neuron $c$, the goal of an attribution method is to determine
    the contribution $R^{c}=[R^{c}_{1},...,R^{c}_{N}]$ of each input feature $x_{i}$
    to the output $S_{c}$. For a classification task, the target neuron of interest
    is usually the output neuron associated with the correct class for a given sample.
    The obtained attribution maps are usually displayed as heatmaps, where one color
    indicates features that contribute positively to the activation of the target
    output while another color indicates features that have a suppressing effect on
    it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍各种可解释性方法，旨在为网络的每个输入特征分配一个归因值，有时也称为“相关性”或“贡献”。这些可解释性方法因此可以称为归因方法。更正式地说，考虑一个深度神经网络（DNN），它接受一个输入`$x=[x_{1},...,x_{N}]$`并生成一个输出`$S(x)=[S_{1}(x),...,S_{C}(x)]$`，其中`$C$`是输出神经元的总数。给定一个特定的目标神经元`$c$`，归因方法的目标是确定每个输入特征`$x_{i}$`对输出`$S_{c}$`的贡献`$R^{c}=[R^{c}_{1},...,R^{c}_{N}]$`。对于分类任务，感兴趣的目标神经元通常是与给定样本的正确类别相关联的输出神经元。得到的归因图通常显示为热图，其中一种颜色表示对目标输出激活有正面贡献的特征，而另一种颜色表示对其有抑制作用的特征。
- en: 'To organize our presentation, we classify all attribution methods into the
    following categories: back-propagation based, attention based, feature perturbation
    based, model distillation based, and game theory based. We also include example
    and generative based interpretation for DL methods for completeness. More technical
    details for each category will be elaborated below.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了组织我们的展示，我们将所有归因方法分类为以下几类：基于反向传播的、基于注意力的、基于特征扰动的、基于模型蒸馏的和基于博弈论的。我们还包括了深度学习方法的示例和生成解释，以确保完整性。每个类别的更多技术细节将在下文中详细说明。
- en: 2.1 Back-propagation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 反向传播
- en: 'The most popularly used interpretability method is based on back-propagation
    of either gradients [[Simonyan et al., 2014](#bib.bibx153)] or activation values [[Bach
    et al., 2015](#bib.bibx14)]. This line of methods starts from the Saliency Map [[Simonyan
    et al., 2014](#bib.bibx153)], which follows the normal gradient back-propagation
    process and constructs attributions by taking the absolute value of the partial
    derivative of the target output $S_{c}$ with respect to the input features $x_{i}$,
    i.e., $|\frac{\partial S_{c}(x)}{\partial x_{i}}|$. Intuitively, the absolute
    value of the gradient indicates those input features that can be perturbed the
    least in order for the target output to change the most. However, the absolute
    value prevents the detection of positive and negative evidence that might be present
    in the input. To make the reconstructed heatmaps significantly more accurate for
    convolutional neural network (CNN) models, Deconvolution [[Zeiler and Fergus,
    2014a](#bib.bibx190)] and Guided Back-propagation [[Springenberg et al., 2015](#bib.bibx156)]
    were proposed and these two methods and the Saliency Map method differ mainly
    in the way they handle back-propagation through the rectified linear (ReLU) non-linearity.
    As illustrated in Figure [1(a)](#S2.F1.sf1 "In Figure 1 ‣ 2.1 Back-propagation
    ‣ 2 Interpretability Methods ‣ Explainable Deep Learning in Healthcare: A Methodological
    Survey from an Attribution View [Advanced Review]"), for normal gradient back-propagation
    in the Saliency Map method, when the activation values in the lower layer are
    negative, the corresponding back-propagated gradients are masked out. In contrast,
    the Deconvolution method masks out the gradients when they themselves are negative,
    while the Guided Back-propagation approach combines these two methods: those gradients
    are masked out for which at least one of these two values is negative.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的可解释性方法基于梯度的反向传播[[Simonyan et al., 2014](#bib.bibx153)]或激活值的反向传播[[Bach et
    al., 2015](#bib.bibx14)]。这类方法始于显著性图[[Simonyan et al., 2014](#bib.bibx153)]，该方法遵循常规梯度反向传播过程，通过取目标输出$S_{c}$相对于输入特征$x_{i}$的偏导数的绝对值来构建归因，即$|\frac{\partial
    S_{c}(x)}{\partial x_{i}}|$。直观地，梯度的绝对值指示了那些对输入特征影响最小的特征。然而，绝对值阻碍了检测输入中可能存在的正负证据。为了使卷积神经网络（CNN）模型的重建热图显著更准确，提出了反卷积[[Zeiler
    and Fergus, 2014a](#bib.bibx190)]和引导反向传播[[Springenberg et al., 2015](#bib.bibx156)]，这两种方法与显著性图方法的主要区别在于它们处理通过修正线性（ReLU）非线性反向传播的方式。如图[1(a)](#S2.F1.sf1
    "在图1 ‣ 2.1 反向传播 ‣ 2 可解释性方法 ‣ 医疗中的可解释深度学习：一种归因视角的方法论调查 [高级评论]")所示，对于显著性图方法中的常规梯度反向传播，当下层的激活值为负时，相应的反向传播梯度会被屏蔽。相比之下，反卷积方法在梯度本身为负时屏蔽梯度，而引导反向传播方法则结合了这两种方法：那些梯度被屏蔽的条件是这两种值中至少有一个为负。
- en: 'Gradient * Input [[Shrikumar et al., 2017b](#bib.bibx152)] was proposed as
    a technique to improve the sharpness of the attribution maps. The attribution
    is computed taking the (signed) partial derivatives of the output with respect
    to the input and multiplying them with the input itself. Integrated Gradients [[Sundararajan
    et al., 2017](#bib.bibx160)] is similar to Gradient * Input, with the main difference
    being that Integrated Gradients computes the average gradient as the input varies
    along a linear path from a baseline $\tilde{x}$ to $x$. The baseline is defined
    by the user and often chosen to be zero. Please refer to Figure [1(b)](#S2.F1.sf2
    "In Figure 1 ‣ 2.1 Back-propagation ‣ 2 Interpretability Methods ‣ Explainable
    Deep Learning in Healthcare: A Methodological Survey from an Attribution View
    [Advanced Review]") for the mathematical definition for both methods.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度 * 输入[[Shrikumar et al., 2017b](#bib.bibx152)]被提出作为提高归因图清晰度的一种技术。归因是通过取输出相对于输入的（有符号）偏导数并将其与输入本身相乘来计算的。积分梯度[[Sundararajan
    et al., 2017](#bib.bibx160)]与梯度 * 输入类似，主要区别在于积分梯度计算平均梯度，因为输入沿着从基线$\tilde{x}$到$x$的线性路径变化。基线由用户定义，通常选择为零。有关这两种方法的数学定义，请参见图[1(b)](#S2.F1.sf2
    "在图1 ‣ 2.1 反向传播 ‣ 2 可解释性方法 ‣ 医疗中的可解释深度学习：一种归因视角的方法论调查 [高级评论]")。
- en: Pixel-space gradient visualizations such as the above-mentioned Guided Back-propagation
    and Deconvolution are high-resolution and highlight fine-grained details in the
    image, but are not class-discriminative, i.e., the attribution value plots for
    different classes may look similar. In contrast, localization approaches like
    Class Activation Mapping (CAM) [[Zhou et al., 2016](#bib.bibx194)] are highly
    class-discriminative (e.g., the ‘cat’ explanation exclusively highlights the ‘cat’
    regions but not ‘dog’ regions in an image containing both a cat and dog). This
    approach modifies image classification CNN architectures by replacing fully-connected
    layers with convolutional layers and global average pooling, thus achieving class-specific
    feature maps. A drawback of CAM is that it requires feature maps to directly precede
    softmax layers, so it is only applicable to particular kinds of CNN architectures.
    To solve this shortcoming, Grad-CAM [[Selvaraju et al., 2017](#bib.bibx141)] was
    introduced as a generalization to CAM, which uses the gradient information flowing
    into the last convolutional layer of the CNN to understand the importance of each
    neuron for a decision of interest. Furthermore, it is combined with existing pixel-space
    gradient visualizations to create Guided Grad-CAM visualizations that are both
    high-resolution and class-discriminative.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 像上面提到的引导反向传播和去卷积这样的像素空间梯度可视化具有高分辨率，并突出了图像中的细节，但它们不具备类别判别性，即不同类别的归因值图可能看起来相似。相比之下，像类别激活映射（CAM）[[Zhou
    et al., 2016](#bib.bibx194)]这样的定位方法具有很强的类别判别性（例如，‘cat’的解释专门突出显示图像中‘cat’的区域，而不是包含猫和狗的图像中的‘dog’区域）。这种方法通过用卷积层和全局平均池化替代全连接层来修改图像分类CNN架构，从而实现类别特定的特征图。CAM的一个缺点是它要求特征图直接位于softmax层之前，因此仅适用于特定类型的CNN架构。为了解决这一不足，引入了Grad-CAM
    [[Selvaraju et al., 2017](#bib.bibx141)]，它作为CAM的泛化，通过利用流入CNN最后一个卷积层的梯度信息来理解每个神经元对感兴趣决策的重要性。此外，它与现有的像素空间梯度可视化相结合，创造了既高分辨率又具类别判别性的引导Grad-CAM可视化。
- en: 'Besides gradients, back-propagation of activation values can also be leveraged
    as an interpretability approach. Layer-wise Relevance Propagation (LRP) [[Bach
    et al., 2015](#bib.bibx14)] is the first to adopt this method, where the algorithm
    starts at the output layer $L$ and assigns the relevance of the target neuron
    equal to the output of the neuron itself (i.e., the activation value of the neuron)
    and the relevance of all other neurons to zero, as shown in Eq. [1](#S2.E1 "In
    2.1 Back-propagation ‣ 2 Interpretability Methods ‣ Explainable Deep Learning
    in Healthcare: A Methodological Survey from an Attribution View [Advanced Review]").
    Then the recursive back-propagation rule (called the $\epsilon$-rule) for the
    redistribution of a layer’s relevance to the preceding layer is described in Eq. [2](#S2.E2
    "In 2.1 Back-propagation ‣ 2 Interpretability Methods ‣ Explainable Deep Learning
    in Healthcare: A Methodological Survey from an Attribution View [Advanced Review]"),
    where we define $z_{ji}=w_{ji}^{(l+1,l)}x_{i}^{(l)}$ to be the weighted activation
    of a neuron $i$ onto neuron $j$ in the next layer and $b_{j}$ the additive bias
    of unit $j$. Once the back-propagation reaches the input layer, the final attributions
    are defined as $R_{i}^{c}(x)=r_{i}^{(1)}$. As an alternative, DeepLIFT [[Shrikumar
    et al., 2017a](#bib.bibx151)] proceeds in a backward fashion similar to LRP but
    calibrates all relevance scores by subtracting reference values that are determined
    by running a forward pass through the network using the baseline $\bar{x}$ as
    input and recording the activation of each unit. Although LRP and DeepLIFT were
    invented based on back-propagation of activation values, it has been demonstrated
    in [[Ancona et al., 2018](#bib.bibx7)] that they can also be computed by applying
    the chain rule for gradients and the converted equations are summarized in Figure
    [1(b)](#S2.F1.sf2 "In Figure 1 ‣ 2.1 Back-propagation ‣ 2 Interpretability Methods
    ‣ Explainable Deep Learning in Healthcare: A Methodological Survey from an Attribution
    View [Advanced Review]").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了梯度，激活值的反向传播也可以作为解释性方法。逐层相关传播（LRP）[[Bach et al., 2015](#bib.bibx14)] 是首个采用这种方法的算法，其中算法从输出层
    $L$ 开始，将目标神经元的相关性设置为神经元自身的输出（即神经元的激活值），而将其他所有神经元的相关性设置为零，如方程 [1](#S2.E1 "在 2.1
    反向传播 ‣ 2 解释方法 ‣ 医疗保健中的可解释深度学习：从归因视角的研究 [高级综述]") 所示。然后，第 $2$ 个方程 [2](#S2.E2 "在
    2.1 反向传播 ‣ 2 解释方法 ‣ 医疗保健中的可解释深度学习：从归因视角的研究 [高级综述]") 描述了将层的相关性重新分配到前一层的递归反向传播规则（称为
    $\epsilon$-规则），其中我们定义 $z_{ji}=w_{ji}^{(l+1,l)}x_{i}^{(l)}$ 为神经元 $i$ 对下一层神经元 $j$
    的加权激活值，$b_{j}$ 为单元 $j$ 的附加偏差。一旦反向传播到达输入层，最终的归因定义为 $R_{i}^{c}(x)=r_{i}^{(1)}$。作为替代，DeepLIFT
    [[Shrikumar et al., 2017a](#bib.bibx151)] 以类似于 LRP 的向后方式进行，但通过减去通过使用基线 $\bar{x}$
    作为输入并记录每个单元激活值的前向传播确定的参考值来校准所有相关性分数。尽管 LRP 和 DeepLIFT 是基于激活值的反向传播发明的，但在 [[Ancona
    et al., 2018](#bib.bibx7)] 中已经证明，它们也可以通过应用梯度链规则来计算，转换后的方程总结在图 [1(b)](#S2.F1.sf2
    "在图 1 ‣ 2.1 反向传播 ‣ 2 解释方法 ‣ 医疗保健中的可解释深度学习：从归因视角的研究 [高级综述]") 中。
- en: '|  | $r_{i}^{(L)}=\begin{cases}S_{i}(x)&amp;\text{if unit}\ i\ \text{is the
    target unit of interest}\\ 0&amp;\text{otherwise}\end{cases}$ |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{i}^{(L)}=\begin{cases}S_{i}(x)&amp;\text{如果单元}\ i\ \text{是目标单元}\\
    0&amp;\text{否则}\end{cases}$ |  | (1) |'
- en: '|  | $r_{i}^{l}=\sum_{j}\frac{z_{ji}}{\sum_{i^{\prime}}({z_{ji^{\prime}}+b_{j}})+\epsilon\cdot\mathrm{sign}(\sum_{i^{\prime}}(z_{ji^{\prime}}+b_{j}))}r_{j}^{l+1}$
    |  | (2) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{i}^{l}=\sum_{j}\frac{z_{ji}}{\sum_{i^{\prime}}({z_{ji^{\prime}}+b_{j}})+\epsilon\cdot\mathrm{sign}(\sum_{i^{\prime}}(z_{ji^{\prime}}+b_{j}))}r_{j}^{l+1}$
    |  | (2) |'
- en: '![Refer to caption](img/6cc9d58c4123c2822c5e4486a46caf68.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6cc9d58c4123c2822c5e4486a46caf68.png)'
- en: (a) Comparison among normal gradient back-propagation, Deconvolution, and Guided
    Back-propagation in terms of how they handle back-propagation through the rectified
    linear (ReLU) non-linearity.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 关于普通梯度反向传播、去卷积和引导反向传播在如何处理通过修正线性（ReLU）非线性进行反向传播方面的比较。
- en: '![Refer to caption](img/d3a3da004b42b836fbd660d9a2263268.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/d3a3da004b42b836fbd660d9a2263268.png)'
- en: (b) Mathematical formulation of four back-propagation based attribution methods.
    The original equations of $\epsilon$-LRP and DeepLIFT are transformed so that
    they can be calcuated based on gradients.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 四种基于反向传播的归因方法的数学公式。$\epsilon$-LRP 和 DeepLIFT 的原始方程被转换，使其可以基于梯度进行计算。
- en: 'Figure 1: Mathematical formulation of different back-propagation based interpretability
    methods.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 不同基于反向传播的可解释性方法的数学公式。'
- en: 2.2 Feature perturbation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 特征扰动
- en: Compared to back-propagation based methods, which compute gradients of outputs
    with respect to input features, feature perturbation methods explicitly examine
    the change in model confidence resulting from occluding or ablating certain features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于基于反向传播的方法，这些方法计算输出相对于输入特征的梯度，特征扰动方法显式地检查由于遮挡或消除某些特征所导致的模型置信度变化。
- en: The idea of masking parts of the input and measuring the model’s change in confidence
    was introduced in a model agnostic context, pre-DL by works such as [[Štrumbelj
    et al., 2009](#bib.bibx174)] and [[Robnik-Šikonja and Kononenko, 2008](#bib.bibx135)].
    Based on some of these works, there have been multiple methods in DL for feature
    perturbation, attempting to explain the model based on the change in output classification
    confidence upon perturbation of features. These include model agnostic works based
    on conditional multivariate analysis and deep visualization [[Zintgraf et al.,
    2017](#bib.bibx197)] (based on the instance-specific method known as prediction
    difference analysis) and explicit erasure of parts of input representations [[Li
    et al., 2016](#bib.bibx92)]; as well as convolution neural network specific identification
    of image regions for which the model reacts most to perturbation [[Zeiler and
    Fergus, 2014b](#bib.bibx191)] and image masking models that are trained to manipulate
    scores outputted by the predictive model by masking salient parts of the input
    image [[Dabkowski and Gal, 2017](#bib.bibx37)]. Similar to image masking models,
    recent model-agnostic methods use generative models to sample plausible in-fills
    (as opposed to full masking) and optimize to find image regions that most change
    the classifier decision after in-filling [[Chang et al., 2019](#bib.bibx26)].
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 遮蔽输入部分并测量模型置信度变化的想法在深度学习之前的模型不可知背景中被提出，如[[Štrumbelj et al., 2009](#bib.bibx174)]和[[Robnik-Šikonja
    和 Kononenko, 2008](#bib.bibx135)]。基于这些工作的基础上，深度学习中有多种特征扰动方法，试图通过特征扰动后输出分类置信度的变化来解释模型。这些方法包括基于条件多变量分析和深度可视化的模型不可知工作[[Zintgraf
    et al., 2017](#bib.bibx197)]（基于称为预测差异分析的实例特定方法）以及输入表示部分的显式擦除[[Li et al., 2016](#bib.bibx92)]；以及卷积神经网络特定的图像区域识别，该区域对扰动的反应最强[[Zeiler
    和 Fergus, 2014b](#bib.bibx191)]和通过遮蔽输入图像的显著部分来操控预测模型输出分数的图像遮蔽模型[[Dabkowski 和 Gal,
    2017](#bib.bibx37)]。与图像遮蔽模型类似，最近的模型不可知方法使用生成模型来采样合理的填充（与完全遮蔽相对）并优化以找到在填充后最改变分类器决策的图像区域[[Chang
    et al., 2019](#bib.bibx26)]。
- en: In the direction of more theoretically grounded variable importance-based techniques,
    [[Fisher et al., 2019](#bib.bibx51)] measure the model prediction difference upon
    adding noise to the features. Additionally, various adversarial perturbation techniques
    have been introduced that add noise to the feature representations, falling in
    the category of Evasion Attacks [[Tabassi et al., 2019](#bib.bibx164)]. Evasion
    Attacks involve finding small input perturbations that cause large changes in
    the loss function and lead to mispredictions. These input perturbations are usually
    found by solving constrained optimization problems. These include gradient-based
    search algorithms like Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)
    [[Szegedy et al., 2013](#bib.bibx162)], Fast Gradient Sign Method (FGSM) [[Goodfellow
    et al., 2015](#bib.bibx55)], Jacobian-based Saliency Map Attack (JSMA) [[Papernot
    et al., 2016a](#bib.bibx120)] and Projected Gradient Descent (PGD) [[Madry et al.,
    2018](#bib.bibx105)] among others. For detailed surveys on adversarial perturbation
    techniques in computer vision see [[Akhtar and Mian, 2018](#bib.bibx3)]; for surveys
    on adversarial attacks in general see [[Chakraborty et al., 2018](#bib.bibx25)]
    and [[Yuan et al., 2019](#bib.bibx188)]. While the goal of these methods is to
    actively change model confidence for the purpose of attacking the model, they
    take advantage of the black box nature of DL models and have led to creation of
    techniques that can be used to deploy more robust and interpretable models.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在更具理论基础的变量重要性技术方面，[[Fisher et al., 2019](#bib.bibx51)] 通过对特征添加噪声来衡量模型预测差异。此外，还引入了各种对抗扰动技术，这些技术通过向特征表示添加噪声，属于规避攻击的范畴[[Tabassi
    et al., 2019](#bib.bibx164)]。规避攻击涉及寻找小的输入扰动，这些扰动会导致损失函数发生巨大变化，从而导致误预测。这些输入扰动通常通过解决约束优化问题来发现，包括像有限记忆Broyden-Fletcher-Goldfarb-Shanno（L-BFGS）[[Szegedy
    et al., 2013](#bib.bibx162)]、快速梯度符号法（FGSM）[[Goodfellow et al., 2015](#bib.bibx55)]、Jacobian基的显著性图攻击（JSMA）[[Papernot
    et al., 2016a](#bib.bibx120)]和投影梯度下降（PGD）[[Madry et al., 2018](#bib.bibx105)]等基于梯度的搜索算法。有关计算机视觉中对抗扰动技术的详细调查，请参见[[Akhtar
    and Mian, 2018](#bib.bibx3)]；有关对抗攻击的一般调查，请参见[[Chakraborty et al., 2018](#bib.bibx25)]和[[Yuan
    et al., 2019](#bib.bibx188)]。虽然这些方法的目标是主动改变模型的信心以攻击模型，但它们利用了深度学习模型的黑箱特性，并导致了可以用于部署更强大且可解释模型的技术的创建。
- en: 2.3 Attention
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 注意力
- en: Attention mechanisms have played an important role in model interpretations
    and the attention weights have been widely adopted as a proxy to explain a given
    model’s decision making [[Xu et al., 2015](#bib.bibx183), [Xie et al., 2017](#bib.bibx182),
    [Clark et al., 2019](#bib.bibx32), [Voita et al., 2019](#bib.bibx173)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在模型解释中发挥了重要作用，注意力权重已被广泛采用作为解释给定模型决策的代理[[Xu et al., 2015](#bib.bibx183),
    [Xie et al., 2017](#bib.bibx182), [Clark et al., 2019](#bib.bibx32), [Voita et
    al., 2019](#bib.bibx173)]。
- en: Historically, attention mechanisms have been introduced in the context of sequence
    to sequence text model alignment as the way to directly incorporate the importance
    of the context to any given word representation. Each input word in a given context
    is represented by a weighted sum of the representations of other words. Naturally,
    the dynamic weights for each word can be interpreted as the contribution (or importance)
    of the words to a given word representation.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，注意力机制是在序列到序列文本模型对齐的背景下引入的，作为直接将上下文的重要性纳入到任何给定单词表示的方法。在给定上下文中的每个输入单词由其他单词表示的加权和表示。自然地，每个单词的动态权重可以被解释为单词对给定单词表示的贡献（或重要性）。
- en: 'While the exact architecture of the attention-utilizing models differs from
    model to model, all of them make use of the set of computations known as the attention
    mechanism. The basic building block of attention is a generalized trainable function
    [[Bahdanau et al., 2014](#bib.bibx15), [Vaswani et al., 2017](#bib.bibx171)]:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用注意力机制的模型的确切架构因模型而异，但所有模型都利用了称为注意力机制的计算集。注意力的基本构建块是一个通用的可训练函数[[Bahdanau et
    al., 2014](#bib.bibx15), [Vaswani et al., 2017](#bib.bibx171)]：
- en: '|  | $Attention(V,Q,K)=Score(W_{q}Q,W_{k}K)\odot W_{v}V$ |  | (3) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $Attention(V,Q,K)=Score(W_{q}Q,W_{k}K)\odot W_{v}V$ |  | (3) |'
- en: where $Q$ and $K$ represent the context of a given element, $V$ the unmodified
    element contribution to the representation without the context being taken into
    an account, and the set of weights $W_{k},W_{q},W_{v}$ are the adaptable weights
    that represent the learned elements’ contributions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Q$ 和 $K$ 代表给定元素的上下文，$V$ 是未考虑上下文的元素对表示的未修改贡献，而权重集 $W_{k},W_{q},W_{v}$ 是表示学习到的元素贡献的可调权重。
- en: The output of the Score function is known as the attention weights and represents
    the contributions of the other elements of the input to the representation of
    the given element or sequence as a whole; in the naive interpretation setting,
    a high post-training attention weight of an input feature or a set of features
    corresponds to a higher importance of the given feature value in producing a prediction.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 评分函数的输出称为注意力权重，表示输入的其他元素对给定元素或序列整体表示的贡献；在简单的解释设置中，输入特征或特征集的高后训练注意力权重对应于在产生预测时给定特征值的重要性。
- en: Note that this methods of producing interpretation is intrinsically linked to
    the model itself and constitutes a direct interpretation of the outputs of the
    parts of a given attention-utilizing model (attention scores) as an explanation
    for the prediction. Since attention scores are often computed over already pooled
    representations of the elements and sequences, the element scores do not necessarily
    represent the direct feature contributions [[Jain and Wallace, 2019](#bib.bibx73),
    [Brunner et al., 2019](#bib.bibx21), [Zhong et al., 2019](#bib.bibx193)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种产生解释的方法本质上与模型本身紧密相关，并直接解释了给定利用注意力的模型（注意力分数）部分输出的预测。由于注意力分数通常计算在已汇聚的元素和序列表示上，元素分数不一定代表直接的特征贡献
    [[Jain and Wallace, 2019](#bib.bibx73), [Brunner et al., 2019](#bib.bibx21), [Zhong
    et al., 2019](#bib.bibx193)]。
- en: The majority of work on attention based interpretability has been in the general
    time-series processing field due to both the success of the attention-using models
    and the natural idea of viewing the contribution of the other elements of the
    sequence to the current state [[Sezer et al., 2020](#bib.bibx143), [Fawaz et al.,
    2019](#bib.bibx45), [Wang et al., 2019](#bib.bibx176), [Ardabili et al., 2019](#bib.bibx10)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于注意力使用模型的成功以及将序列中其他元素对当前状态的贡献自然地视为观点，大多数关于注意力基于解释性的工作集中在一般时间序列处理领域 [[Sezer
    et al., 2020](#bib.bibx143), [Fawaz et al., 2019](#bib.bibx45), [Wang et al.,
    2019](#bib.bibx176), [Ardabili et al., 2019](#bib.bibx10)]。
- en: Fully-attention based models and attention based interpretations are also popular
    in natural language processing (NLP) due to the compositional nature of syntax
    and meaning [[Wolf et al., 2020](#bib.bibx180)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的模型和注意力基于的解释在自然语言处理（NLP）中也很流行，因为语法和意义的组成性质 [[Wolf et al., 2020](#bib.bibx180)]。
- en: 2.4 Model distillation
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 模型蒸馏
- en: Model distillation (also known as Network distillation) is a model compression
    technique where a simpler model (student) is “taught” by a more complex model
    (teacher). While the original use of the technique focused on the improved performance
    or compactness of the student model [[Hinton et al., 2015](#bib.bibx64)], it is
    important to note that if the simpler model is naturally interpretable, the transfer
    results in an “interpretable” model explaining the behavior of the more complex
    teacher model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 模型蒸馏（也称为网络蒸馏）是一种模型压缩技术，其中一个简单模型（学生）通过更复杂的模型（教师）进行“教学”。尽管该技术最初的用途侧重于学生模型的性能或紧凑性
    [[Hinton et al., 2015](#bib.bibx64)]，但重要的是要注意，如果简单模型本身是自然可解释的，那么转移结果将是一个“可解释的”模型，用来解释更复杂的教师模型的行为。
- en: A complex model’s behavior may be approximated either locally, by fitting a
    simpler model around a given example to produce an explanation for a given point
    [[Ribeiro et al., 2016a](#bib.bibx131)], or globally, by fitting one simple model
    directly to the teacher model, using all the training data [[Lakkaraju et al.,
    2017](#bib.bibx87)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂模型的行为可以通过局部方式近似，即通过在给定示例周围拟合一个简单模型来为某一点产生解释 [[Ribeiro et al., 2016a](#bib.bibx131)]，或通过全局方式，即通过将一个简单模型直接拟合到教师模型中，使用所有训练数据
    [[Lakkaraju et al., 2017](#bib.bibx87)]。
- en: Due to the explicit “interpretation” use case of the technique, the student
    models are in general limited to either generalized linear models [[Ribeiro et al.,
    2016a](#bib.bibx131)], decision trees [[Craven and Shavlik, 1995](#bib.bibx34),
    [Schmitz et al., 1999](#bib.bibx139), [Plumb et al., 2018](#bib.bibx126)] or direct
    rules or set inductions [[Sethi et al., 2012](#bib.bibx142), [Lakkaraju et al.,
    2017](#bib.bibx87), [Ribeiro et al., 2018](#bib.bibx133), [Zilke et al., 2016](#bib.bibx196)]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该技术的明确“解释”用例，学生模型通常限制为广义线性模型 [[Ribeiro et al., 2016a](#bib.bibx131)]、决策树 [[Craven
    and Shavlik, 1995](#bib.bibx34), [Schmitz et al., 1999](#bib.bibx139), [Plumb
    et al., 2018](#bib.bibx126)] 或直接规则或集合归纳 [[Sethi et al., 2012](#bib.bibx142), [Lakkaraju
    et al., 2017](#bib.bibx87), [Ribeiro et al., 2018](#bib.bibx133), [Zilke et al.,
    2016](#bib.bibx196)]。
- en: The most influential member of this family of interpretation producing techniques
    is LIME [[Ribeiro et al., 2016a](#bib.bibx131)], a general method for generating
    local explanation for a specific input case. The local model that serves as an
    explanation for a given point is obtained by minimizing
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这一解释生成技术家族中最有影响力的成员是 LIME [[Ribeiro et al., 2016a](#bib.bibx131)]，它是一种为特定输入案例生成局部解释的一般方法。作为给定点解释的局部模型是通过最小化以下公式获得的
- en: '|  | $\xi(x^{*})=\operatorname*{argmin}_{g\in\mathcal{G}}\mathcal{L}(f,g,\pi_{x^{*}})+\Omega(g)$
    |  | (4) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\xi(x^{*})=\operatorname*{argmin}_{g\in\mathcal{G}}\mathcal{L}(f,g,\pi_{x^{*}})+\Omega(g)$
    |  | (4) |'
- en: where $\mathcal{G}$ is a class of the interpretable models used to produce an
    explanation, $\pi_{x^{*}}$ defines the neighborhood of points near $x^{*}$, $\mathcal{L}$
    is the measure of the difference between the original model and the explanation
    model prediction in that neighborhood, and $\Omega(g)$ is a complexity measure
    of the explanation model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{G}$ 是用于生成解释的可解释模型类，$\pi_{x^{*}}$ 定义了靠近 $x^{*}$ 的点的邻域，$\mathcal{L}$
    是该邻域中原始模型和解释模型预测之间差异的度量，而 $\Omega(g)$ 是解释模型的复杂度度量。
- en: In practice, in the classical LIME use, $\mathcal{L}$ is set to be the distance
    weighted squared loss between the original model and the explanation model prediction
    computed over a randomly sampled set of data points biased to lie near $x^{*}$
    by $\pi_{x^{*}}$. The explanation model class $\mathcal{G}$ is the class of all
    linear models and $\Omega(g)$ is a regularization term to prevent overfitting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，在经典的 LIME 使用中，$\mathcal{L}$ 被设定为原始模型和解释模型预测之间的距离加权平方损失，该损失是在一个随机采样的数据点集上计算的，这些数据点集由于
    $\pi_{x^{*}}$ 的偏向而靠近 $x^{*}$。解释模型类 $\mathcal{G}$ 是所有线性模型的集合，$\Omega(g)$ 是一个正则化项，用于防止过拟合。
- en: The vast majority of local knowledge distillation for interpretability models
    are the result of modifying Lime in either the neighborhood construction (ALIME
    [[Shankaranarayana and Runje, 2019](#bib.bibx145)]), sampling (MPS-LIME [[Shi
    et al., 2020](#bib.bibx149)]) and input structure constraining procedure (GraphLime
    [[Huang et al., 2020](#bib.bibx69)]) or the nature of the explanation model (SurvLIME
    [[Kovalev et al., 2020](#bib.bibx85)], GRAPHLime [[Huang et al., 2020](#bib.bibx69)]).
    Another popular trend is producing semi-global explanation models through LIME-like
    fitting procedures (LIME-SUP [[Hu et al., 2018](#bib.bibx68)], Klime [[Hall et al.,
    2017](#bib.bibx59)], NormLime [[Ahern et al., 2019](#bib.bibx1)], DLIME [[Zafar
    and Khan, 2019](#bib.bibx189)], ILIME [[Shawi et al., 2019](#bib.bibx147)]).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 绝大多数用于可解释性模型的局部知识蒸馏结果是对 Lime 的修改，包括邻域构造（ALIME [[Shankaranarayana and Runje,
    2019](#bib.bibx145)]）、采样（MPS-LIME [[Shi et al., 2020](#bib.bibx149)]）和输入结构约束程序（GraphLime
    [[Huang et al., 2020](#bib.bibx69)]）或解释模型的性质（SurvLIME [[Kovalev et al., 2020](#bib.bibx85)],
    GRAPHLime [[Huang et al., 2020](#bib.bibx69)]）。另一种流行趋势是通过类似 LIME 的拟合程序生成半全局解释模型（LIME-SUP
    [[Hu et al., 2018](#bib.bibx68)], Klime [[Hall et al., 2017](#bib.bibx59)], NormLime
    [[Ahern et al., 2019](#bib.bibx1)], DLIME [[Zafar and Khan, 2019](#bib.bibx189)],
    ILIME [[Shawi et al., 2019](#bib.bibx147)]）。
- en: 2.5 Game theory based Interpretability Methods
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 基于博弈论的可解释性方法
- en: DL models can also be interpreted via Shapley value, a game theory concept inspired
    by local surrogate models [[Lundberg and Lee, 2017](#bib.bibx101)]. Shapley value
    is a concept of fair distribution of gains and losses to several unequal players
    in a cooperative game [[Shapley, 1953](#bib.bibx146)]. It is an average value
    of all marginal contributions to all possible interactions of features (i.e.,
    players in the game) given a particular example. Therefore, the Shapley value
    can explain how feature values contribute to the model prediction of the given
    example by comparing against the average prediction for the whole dataset. Nevertheless,
    the Shapley value approximation is not easy to compute when the learning model
    becomes complicated.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: DL模型也可以通过Shapley值进行解释，这是一种受局部替代模型启发的博弈理论概念[[Lundberg and Lee, 2017](#bib.bibx101)]。Shapley值是将收益和损失公平分配给合作博弈中的多个不平等参与者的概念[[Shapley,
    1953](#bib.bibx146)]。它是所有可能特征（即博弈中的参与者）交互的边际贡献的平均值。 因此，Shapley值可以通过与整个数据集的平均预测进行比较，来解释特征值如何对给定示例的模型预测做出贡献。然而，当学习模型变得复杂时，Shapley值的近似计算并不容易。
- en: Recently, researchers proposed a unified framework, SHAP (SHapley Additive exPlanations)
    values, to approximate the classical Shapley values with conditional expectations
    for various kinds of machine learning models, which include linear models, tree
    models [[Lundberg et al., 2018a](#bib.bibx100)], and even complicated deep neural
    networks [[Lundberg and Lee, 2017](#bib.bibx101)]. SHAP has been widely used recently
    for DL interpretation, yet researchers also admit to concerns about this popular
    interpretability method.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究人员提出了一个统一框架，即SHAP（SHapley Additive exPlanations）值，用于近似各种机器学习模型的经典Shapley值及其条件期望，这些模型包括线性模型、树模型[[Lundberg
    et al., 2018a](#bib.bibx100)]，甚至复杂的深度神经网络[[Lundberg and Lee, 2017](#bib.bibx101)]。SHAP最近在DL解释中被广泛使用，但研究人员也承认对这一流行解释方法的担忧。
- en: First, the SHAP for neural networks (KernelSHAP) is based on an assumption of
    model linearity. To mitigate the problem, [[Ancona et al., 2019](#bib.bibx8)]
    propose a polynomial-time approximation algorithm of Shapley values, Deep Approximate
    Shapley Propagation (DASP), to learn a better Shapley value approximation in non-linear
    models, especially deeper neural networks. DASP is a perturbation-based method
    using uncertainty propagation in the neural networks. It requires a polynomial
    number of network evaluations, which is faster than other sampling-based methods,
    without losing approximation performance. Also, [[Sundararajan and Najmi, 2020](#bib.bibx159)]
    show that SHAP, or other methods using Shapley values with conditional expectations,
    can be sensitive to data sparsity and yield counterintuitive attributions that
    make an incorrect model interpretation. They propose a technique, Baseline Shapley,
    to provide a good unique result.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，神经网络的SHAP（KernelSHAP）基于模型线性的假设。为了解决这个问题，[[Ancona et al., 2019](#bib.bibx8)]
    提出了一个多项式时间近似算法，即深度近似Shapley传播（DASP），用于在非线性模型，特别是更深的神经网络中学习更好的Shapley值近似。DASP是一种基于扰动的方法，利用神经网络中的不确定性传播。它需要多项式数量的网络评估，比其他基于采样的方法更快，并且不失去近似性能。此外，[[Sundararajan
    and Najmi, 2020](#bib.bibx159)] 表明，SHAP或其他使用Shapley值和条件期望的方法对数据稀疏性可能敏感，可能导致产生反直觉的归因，从而使模型解释不准确。他们提出了一种技术，基线Shapley，提供一个良好的唯一结果。
- en: 2.6 Example based Interpretability Methods
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 基于示例的可解释性方法
- en: Instead of explaining the model using the attributive contribution of input
    data points, example based methods interpret the model behavior using only the
    particular training data points that are representative or influential for the
    model prediction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过输入数据点的属性贡献来解释模型不同，基于示例的方法使用仅对模型预测具有代表性或影响力的特定训练数据点来解释模型行为。
- en: 'For DL models, there are several interpretation methods based on example-level
    information. For example, the influence function [[Koh and Liang, 2017](#bib.bibx83)],
    example-level feature selection [[Chen et al., 2018](#bib.bibx28)], contextual
    decomposition (CD) [[Murdoch et al., 2018](#bib.bibx117)], and the combination
    of both prototypes and criticism samples—data points that can’t be represented
    by prototypes [[Kim et al., 2016](#bib.bibx80)]. Other popular methods for interpretation,
    such as LIME [[Ribeiro et al., 2016a](#bib.bibx131)] (Section [2.4](#S2.SS4 "2.4
    Model distillation ‣ 2 Interpretability Methods ‣ Explainable Deep Learning in
    Healthcare: A Methodological Survey from an Attribution View [Advanced Review]"))
    and SHAP [[Lundberg and Lee, 2017](#bib.bibx101)] (Section [2.5](#S2.SS5 "2.5
    Game theory based Interpretability Methods ‣ 2 Interpretability Methods ‣ Explainable
    Deep Learning in Healthcare: A Methodological Survey from an Attribution View
    [Advanced Review]")), also provide example-level model interpretability.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '对于DL模型，有几种基于样本级信息的解释方法。例如，影响函数[[Koh and Liang, 2017](#bib.bibx83)]、样本级特征选择[[Chen
    et al., 2018](#bib.bibx28)]、上下文分解（CD）[[Murdoch et al., 2018](#bib.bibx117)]，以及原型和批评样本的组合——无法通过原型表示的数据点[[Kim
    et al., 2016](#bib.bibx80)]。其他流行的解释方法，如LIME[[Ribeiro et al., 2016a](#bib.bibx131)]（第[2.4节](#S2.SS4
    "2.4 Model distillation ‣ 2 Interpretability Methods ‣ Explainable Deep Learning
    in Healthcare: A Methodological Survey from an Attribution View [Advanced Review]")）和SHAP[[Lundberg
    and Lee, 2017](#bib.bibx101)]（第[2.5节](#S2.SS5 "2.5 Game theory based Interpretability
    Methods ‣ 2 Interpretability Methods ‣ Explainable Deep Learning in Healthcare:
    A Methodological Survey from an Attribution View [Advanced Review]")），也提供样本级模型解释性。'
- en: The influence function is an example of example-based interpretability [[Koh
    and Liang, 2017](#bib.bibx83)], which can be used in both computer vision [[Koh
    and Liang, 2017](#bib.bibx83)], and NLP [[Han et al., 2020b](#bib.bibx61)]. The
    goal of the influence function is to measure the change in the loss function as
    we add a small perturbation, weight, or remove a influence instance, which is
    a representative, influential training point. Under the smoothness assumptions,
    the influence function can be computed using the inverse of the Hessian matrix
    of the loss function or by using the Hessian-vector products to approximate the
    result. The influence function can also be used to generate an adversarial attack.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 影响函数是基于样本的解释性示例[[Koh and Liang, 2017](#bib.bibx83)]，可用于计算机视觉[[Koh and Liang,
    2017](#bib.bibx83)]和自然语言处理[[Han et al., 2020b](#bib.bibx61)]。影响函数的目标是测量我们添加小扰动、权重或移除一个影响实例时，损失函数的变化，该实例是一个具有代表性和影响力的训练点。在平滑性假设下，影响函数可以使用损失函数的Hessian矩阵的逆或通过使用Hessian-向量积来近似结果。影响函数也可以用于生成对抗攻击。
- en: Researchers developed DL-based instance-wise feature selection at the example-level
    for feature importance measurement [[Chen et al., 2018](#bib.bibx28)]. Instance-wise
    feature selection (L2X, Learning to Explain) measures feature importance locally
    for each specific example and therefore indicates which features are the key for
    the model to make its prediction on that instance. L2X is trained to maximize
    the mutual information between selected features and the response variable, where
    the conditional distribution of the response variable given the input is the model
    to be explained. To solve an intractable issue of direct estimation of mutual
    information and discrete feature subset sampling, the authors apply a variational
    approximation for mutual information, then develop a continuous reparameterization
    of the sampling distribution. The method has been applied to CNN and hierarchical
    long short-term memory (LSTM) on different datasets and yields a better explanation
    performance quantitatively and qualitatively.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员开发了基于DL的样本级特征选择方法来测量特征重要性[[Chen et al., 2018](#bib.bibx28)]。样本级特征选择（L2X，学习解释）在每个特定样本上局部测量特征重要性，因此指示哪些特征对模型在该实例上做出预测至关重要。L2X被训练以最大化所选特征和响应变量之间的互信息，其中响应变量的条件分布给定输入是要解释的模型。为了解决直接估计互信息和离散特征子集采样的不可处理问题，作者应用了互信息的变分近似，然后开发了采样分布的连续重参数化。该方法已在不同数据集上的CNN和分层长短期记忆（LSTM）中应用，并在定量和定性上都获得了更好的解释性能。
- en: 'CD is an interpretation method to analyze individual predictions by decomposing
    the output of LSTMs without any changes to the underlying model [[Murdoch et al.,
    2018](#bib.bibx117)]. In NLP, it decomposes an LSTM into a sum of two contributions:
    those resulting solely from the given phrase and those involving other factors.
    CD captures the contributions of combinations of words or variables to the final
    prediction of an LSTM. In the study, researchers demonstrate that CD can explain
    both NLP and general LSTM applications. For example, they model for sentiment
    analysis by identifying words and phrases of differing sentiment within a given
    review and extracting positive and negative words from the model. The CD method
    can be further extended to a more general version, contextual decomposition explanation
    penalization (CDEP) [[Rieger et al., 2020](#bib.bibx134)]. CDEP is a method that
    allows the insertion of domain knowledge into a model to ignore spurious correlations,
    correct errors and generalize to different types of dataset shifts. It is general
    and can be applied to different neural network architectures.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CD 是一种解释方法，通过对 LSTMs 的输出进行分解而不对底层模型进行任何更改 [[Murdoch et al., 2018](#bib.bibx117)].
    在 NLP 中，它将 LSTM 分解为两部分的总和：仅由给定短语导致的部分和其他因素所涉及的部分。 CD 捕捉了词语或变量组合对于 LSTM 的最终预测的贡献。在研究中，研究人员证明了
    CD 可以解释 NLP 和一般 LSTM 应用。例如，他们通过识别评论中情感不同的词语和短语，并从模型中提取积极和消极的词语来建立情感分析模型。CD 方法还可以进一步扩展为更一般的版本，即上下文分解解释惩罚（CDEP），[[Rieger
    et al., 2020](#bib.bibx134)]. CDEP 是一种允许将领域知识插入到模型中以忽略虚假的相关性、纠正错误并推广到不同类型的数据集转换的方法。它是通用的，并可以应用于不同的神经网络结构。
- en: For graph neural networks, [[Ying et al., 2019](#bib.bibx186)] further propose
    a model-agnostic GnnExplainer to provide interpretability on graph-based tasks,
    such as node and graph classification. By identifying the prediction-relevant
    edges, GnnExplainer can highlight local subgraph structures and small subsets
    of important features to the prediction. The method can be used for single and
    multiple instance explanations in a graph.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图神经网络, [[Ying et al., 2019](#bib.bibx186)] 进一步提出了一个模型无关的 GnnExplainer 来提供对基于图的任务（如节点分类和图分类）的解释性。通过识别与预测相关的边缘，GnnExplainer
    可以突出显示局部子图结构和重要特征的小子集对于预测的重要性。该方法可以用于图的单个和多个示例的解释。
- en: To tackle the real-world data, which may not have a set of prototypical examples
    representing the data well, we can also utilize both the prototypical examples
    and criticism samples that don’t fit the model well [[Kim et al., 2016](#bib.bibx80)].
    The MMD-critic (maximum mean discrepancy-critic) method uses a Bayesian approach
    to select the prototype and criticism samples and to provide explanations that
    can facilitate human reasoning and understanding of the model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决现实世界中的数据问题，可能无法具有一组典型的示例来很好地表示数据，我们还可以利用既有典型示例又有不适合模型的批评样本 [[Kim et al.,
    2016](#bib.bibx80)]. MMD-critic（最大均值差异-批评家）方法使用贝叶斯方法选择原型和批评样本，并提供有助于人类推理和理解模型的解释。
- en: 2.7 Generative based Interpretability Methods
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 生成式的解释方法
- en: The basis of generative based methods for explaining a model’s behavior uses
    information that does not occur explicitly in attributes of the input, but is
    derived from external knowledge sources, from a causal model, or from explainable
    probabilistic modeling.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成式的方法解释模型行为的基础是使用不显式出现在输入属性中的信息，而是来自外部知识来源、因果模型或可解释的概率建模。
- en: For example, the state-of-the-art general domain neural question answering (QA)
    system attempts to provide human-understandable explanations for better commonsense
    reasoning, yet to interpret how the model utilizes common sense knowledge, a common-sense
    explanation generation framework is required [[Rajani et al., 2019](#bib.bibx128)].
    Researchers collect human narrative explanations for common sense reasoning and
    pretrain language models [[Rajani et al., 2019](#bib.bibx128)], which can generate
    explanations and be used concurrently with the QA system (Commonsense Auto-Generated
    Explanations (CAGE) framework). They further transfer knowledge (generated explanations)
    to out-of-domain tasks and demonstrate the capacity of pretrained language models
    for common sense reasoning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，最先进的通用领域神经问答（QA）系统尝试提供人类可理解的解释，以便更好地进行常识推理。然而，要解读模型如何利用常识知识，需要一个常识解释生成框架 [[Rajani
    et al., 2019](#bib.bibx128)]。研究人员收集了用于常识推理的人类叙述解释，并预训练语言模型 [[Rajani et al., 2019](#bib.bibx128)]，这些模型可以生成解释，并与QA系统同时使用（常识自动生成解释（CAGE）框架）。他们进一步将知识（生成的解释）转移到域外任务，并展示了预训练语言模型在常识推理中的能力。
- en: Generative Explanation Framework (GEF) is another hybrid generative-discriminative
    method that explicitly captures the information inferred from raw texts, generates
    abstractive, fine-grained explanations (attributes), and simultaneously conducts
    classification tasks. It can interpret the predicted classification results and
    improve the overall performance at the same time [[Liu et al., 2019](#bib.bibx98)].
    More specifically, the authors introduce the explainable factor (EF) and the minimum
    risk training (MRT) approach that learn to generate more reasonable explanations.
    They pretrain a classifier using explanations as inputs to classify texts, then
    adopt the classifier to jointly train a text encoder by computing EF, which is
    the semantic distance between generated explanations, gold standard explanations,
    and inputs, and then minimizing MRT loss that considers both the distance between
    predicted overall labels and ground truth labels, as well as the semantic distance
    represented in EF. GEF is a model-agnostic method that can be used in different
    neural network architectures.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 生成解释框架（GEF）是一种混合生成-判别方法，它明确捕捉从原始文本中推断的信息，生成抽象的、细粒度的解释（属性），并同时进行分类任务。它可以解释预测的分类结果，并在提高整体性能的同时 [[Liu
    et al., 2019](#bib.bibx98)]。更具体地说，作者引入了可解释因素（EF）和最小风险训练（MRT）方法，以生成更合理的解释。他们通过使用解释作为输入来预训练分类器，以对文本进行分类，然后采用分类器通过计算EF来联合训练文本编码器，EF是生成的解释、标准答案解释与输入之间的语义距离，然后最小化MRT损失，该损失考虑了预测整体标签与真实标签之间的距离，以及EF中表示的语义距离。GEF是一种模型无关的方法，可以用于不同的神经网络架构。
- en: '[[Madumal et al., 2020](#bib.bibx106)] introduced action influence models that
    utilize the structural causal model to generate the explanation of the behavior
    of model-free reinforcement learning agents through knowing the cause-effect relationships
    using counterfactual analysis. The proposed model has been evaluated on deep reinforcement
    learning algorithms, such as Deep Q Network (DQN) [[Mnih et al., 2013](#bib.bibx111)],
    Double DQN (DDQN) [[Van Hasselt et al., 2016](#bib.bibx170)], Proximal Policy
    Optimization (PPO) [[Schulman et al., 2017](#bib.bibx140)], and Advantage Actor
    Critic (A2C) [[Mnih et al., 2016](#bib.bibx110)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Madumal et al., 2020](#bib.bibx106)] 介绍了利用结构因果模型生成模型无关的强化学习代理行为解释的动作影响模型，通过使用反事实分析了解因果关系。该模型已在深度强化学习算法上进行了评估，如深度Q网络（DQN） [[Mnih
    et al., 2013](#bib.bibx111)]、双重DQN（DDQN） [[Van Hasselt et al., 2016](#bib.bibx170)]、近端策略优化（PPO） [[Schulman
    et al., 2017](#bib.bibx140)]和优势演员评论家（A2C） [[Mnih et al., 2016](#bib.bibx110)]。'
- en: '[[Wisdom et al., 2016](#bib.bibx179)] developed a model-based interpretation
    method, sequential iterative soft-thresholding algorithm (SISTA), to construct
    recurrent neural network (RNN) without black-box components like LSTMs, via the
    trained weights of the explicit probabilistic model.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Wisdom et al., 2016](#bib.bibx179)] 开发了一种基于模型的解释方法，即序列迭代软阈值算法（SISTA），通过显式概率模型的训练权重构建递归神经网络（RNN），避免使用黑箱组件如LSTM。'
- en: 3 Methods for Interpretability in Healthcare
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3种医疗领域可解释性方法
- en: In the last section, we have summarized the methodology for each class of interpretation
    methods. Most of these methods were initially proposed for general domain applications.
    In order to deploy them to healthcare problems, some customization needs to be
    performed. Therefore in this section, we discuss how each class of interpretation
    methods can be adapted to healthcare systems. We also discuss what kinds of clinical/medical
    observations and findings we can make with the help of these interpretation methods.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，我们总结了每类解释方法的工作原理。这些方法大多最初是针对通用领域应用提出的。为了将其部署到医疗保健问题上，需要进行一些定制。因此，在本节中，我们讨论了每类解释方法如何适应医疗系统。我们还讨论了借助这些解释方法可以进行哪些临床/医学观察和发现。
- en: 3.1 Back-propagation
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 反向传播
- en: Back-propagation based interpretability methods have been widely used to help
    visualize and analyze those DL models adopted for healthcare problems, which include
    computer vision, NLP [[Gehrmann et al., 2018](#bib.bibx53)], time series analysis,
    and static features-based predictive modeling. We would like to summarize these
    successful applications and categorize them based on the applied task types.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 基于反向传播的可解释性方法已被广泛用于帮助可视化和分析用于医疗保健问题的深度学习模型，这些问题包括计算机视觉、自然语言处理[[Gehrmann et al.,
    2018](#bib.bibx53)]、时间序列分析以及基于静态特征的预测建模。我们将总结这些成功的应用，并根据应用任务类型进行分类。
- en: 'In computer vision tasks, many powerful DL models have achieved close to expert
    doctor performance [[Esteva et al., 2017](#bib.bibx43), [Ran et al., 2020](#bib.bibx130)]
    and thus it is very meaningful to study how these models can accomplish such great
    success [[Singh et al., 2020b](#bib.bibx155)]. [[Xie et al., 2019](#bib.bibx181)]
    adopted CAM [[Zhou et al., 2016](#bib.bibx194)] to generate heatmaps separately
    for melanoma and nevus cells in skin cancer histology images so that the morphological
    difference between these two types of cells can be visualized: the melanoma cells
    are of irregular shape and the nevus cells are distinctly shaped and regularly
    distributed. [[Zhang et al., 2021](#bib.bibx192)] used Grad-CAM to provide an
    explainable heatmap for an attention network built for classifying chest CT images
    for COVID-19 diagnosis, while Grad-CAM was also used to explain a graph convolutional
    network for secondary pulmonary tuberculosis diagnosis based on chest CT images
     [[Wang et al., 2021](#bib.bibx177)]. Integrated Gradients [[Sundararajan et al.,
    2017](#bib.bibx160)] was used to visualize the features of a CNN model used for
    classifying estrogen receptor status from breast magnetic resonance imaging (MRI)
    images [[Pereira et al., 2018](#bib.bibx124)], where the model was found to have
    learned relevant features in both spatial and dynamic domains with different contributions
    from both. Overall, back-propagation based methods have been used to visualize
    and interpret various medical imaging modalities such as brain MRI [[Eitel et al.,
    2019](#bib.bibx40)], retinal imaging [[Sayres et al., 2019](#bib.bibx138), [Singh
    et al., 2020a](#bib.bibx154)], breast imaging [[Papanastasopoulos et al., 2020](#bib.bibx119),
    [Kim et al., 2018](#bib.bibx81)], skin imaging [[Young et al., 2019](#bib.bibx187)],
    computed tomography (CT) scans [[Couteaux et al., 2019](#bib.bibx33)], and chest
    X-rays [[Linda, 2020](#bib.bibx96)].'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉任务中，许多强大的深度学习模型已实现接近专家医生的表现[[Esteva et al., 2017](#bib.bibx43), [Ran et
    al., 2020](#bib.bibx130)]，因此研究这些模型如何取得如此伟大成功是非常有意义的[[Singh et al., 2020b](#bib.bibx155)]。[[Xie
    et al., 2019](#bib.bibx181)] 采用了 CAM [[Zhou et al., 2016](#bib.bibx194)] 分别生成了皮肤癌组织图像中的黑色素瘤细胞和痣细胞的热图，从而可以可视化这两种细胞之间的形态差异：黑色素瘤细胞形状不规则，而痣细胞形状明显且分布规律。[[Zhang
    et al., 2021](#bib.bibx192)] 使用 Grad-CAM 为用于分类胸部 CT 图像的注意力网络提供了可解释的热图，同时 Grad-CAM
    也被用于解释基于胸部 CT 图像的图卷积网络，用于二次肺结核诊断[[Wang et al., 2021](#bib.bibx177)]。Integrated
    Gradients [[Sundararajan et al., 2017](#bib.bibx160)] 被用于可视化用于从乳腺磁共振成像 (MRI) 图像中分类雌激素受体状态的
    CNN 模型的特征[[Pereira et al., 2018](#bib.bibx124)]，其中发现该模型在空间和动态领域学习了相关特征，并且两者的贡献各有不同。总体而言，基于反向传播的方法已被用于可视化和解释各种医学影像模态，如脑
    MRI [[Eitel et al., 2019](#bib.bibx40)]、视网膜成像[[Sayres et al., 2019](#bib.bibx138),
    [Singh et al., 2020a](#bib.bibx154)]、乳腺成像[[Papanastasopoulos et al., 2020](#bib.bibx119),
    [Kim et al., 2018](#bib.bibx81)]、皮肤成像[[Young et al., 2019](#bib.bibx187)]、计算机断层扫描
    (CT) [[Couteaux et al., 2019](#bib.bibx33)] 和胸部 X 光[[Linda, 2020](#bib.bibx96)]。
- en: For features-based predictive modeling, back-propagation based interpretability
    methods can be applied to both static and time-series analysis. For static analysis
    (e.g., therapy recommendation based on a fixed set of features), fully connected
    neural networks are typically utilized for modeling and thus are the target to
    be interpreted. Commonly-used interpretability methods include DeepLIFT [[Fiosina
    et al., 2020](#bib.bibx50)], LRP [[Li et al., 2018](#bib.bibx94), [Zihni et al.,
    2020](#bib.bibx195)], etc. For time-series analysis, besides being able to analyze
    which features are more important or relevant to the prediction among all features
    used [[Yang et al., 2018](#bib.bibx184)], it is noteworthy that we can also analyze
    what temporal patterns are more influential to the final model decision [[Mayampurath
    et al., 2019](#bib.bibx107), [Suresh et al., 2017](#bib.bibx161)].
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于特征的预测建模，基于反向传播的可解释性方法可以应用于静态分析和时间序列分析。对于静态分析（例如，基于固定特征集的治疗推荐），通常使用全连接神经网络进行建模，因此这些网络是需要被解释的目标。常用的可解释性方法包括
    DeepLIFT [[Fiosina et al., 2020](#bib.bibx50)]、LRP [[Li et al., 2018](#bib.bibx94),
    [Zihni et al., 2020](#bib.bibx195)] 等。对于时间序列分析，除了能够分析所有特征中哪些特征对预测更重要或相关 [[Yang
    et al., 2018](#bib.bibx184)]，还值得注意的是我们还可以分析哪些时间模式对最终模型决策影响更大 [[Mayampurath et
    al., 2019](#bib.bibx107), [Suresh et al., 2017](#bib.bibx161)]。
- en: 3.2 Feature perturbation
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 特征扰动
- en: Feature perturbation methods have primarily been discussed in the context of
    adversarial attacks in the healthcare domain [[Finlayson et al., 2019a](#bib.bibx48)],
    mainly as potential future risks due to the ready acceptance of machine learning
    in diagnosis and insurance claims approval. Nevertheless, the features that are
    most influential if altered by an attacker are also the ones to which the model’s
    responses are most sensitive [[Finlayson et al., 2019a](#bib.bibx48)].
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 特征扰动方法主要在医疗领域的对抗性攻击背景下进行讨论 [[Finlayson et al., 2019a](#bib.bibx48)]，主要作为由于机器学习在诊断和保险理赔审批中被广泛接受而导致的潜在未来风险。然而，如果被攻击者改变的特征对模型的响应最为敏感，这些特征也是最具影响力的
    [[Finlayson et al., 2019a](#bib.bibx48)]。
- en: '[[Finlayson et al., 2019b](#bib.bibx49)] perform adversarial perturbations
    (a variation on FGSM attack [[Goodfellow et al., 2015](#bib.bibx55)]) by addition
    of gradient-based noise to three highly accurate deep learning systems for medical
    imaging. By attacking models that classify diabetic retinopathy, pneumothorax
    and melanoma, they show vulnerabilities in three of the most highly visible successes
    for medical deep learning. In addition, they discuss hypothetical scenarios of
    how attackers could take advantage of the vulnerabilities the systems demonstrate.
    More broadly, they comment on industries and scenarios that could be affected
    by adversarial attacks in the future: insurance fraud and determining pharmaceutical
    and device approvals. They discuss the challenging tradeoff between forestalling
    approval until a resilient algorithm is built and the harm that delaying the deployment
    of a technology impacting healthcare delivery for millions could entail.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Finlayson et al., 2019b](#bib.bibx49)] 通过向三种高精度的医学成像深度学习系统添加基于梯度的噪声，执行了对抗性扰动（FGSM
    攻击的变体 [[Goodfellow et al., 2015](#bib.bibx55)]）。通过攻击分类糖尿病视网膜病变、气胸和黑色素瘤的模型，他们展示了医学深度学习中三大最显著成功的脆弱性。此外，他们讨论了攻击者如何利用系统展示的脆弱性进行攻击的假设场景。更广泛地，他们评论了未来可能受到对抗性攻击影响的行业和情境：保险欺诈以及药物和设备审批。他们讨论了在建设具有韧性的算法之前推迟审批与延迟部署可能对影响数百万人的医疗服务技术所造成的伤害之间的挑战性权衡。'
- en: '[[Iqtidar Newaz et al., 2020](#bib.bibx70)] show vulnerability in smart healthcare
    systems (SHS) by manipulating device readings to alter patient status. By performing
    two types of attacks, including Evasion Attacks [[Tabassi et al., 2019](#bib.bibx164)],
    they identify flaws in an underlying ML model in a SHS. Employing feature perturbation
    methods such as FGSM [[Goodfellow et al., 2015](#bib.bibx55)], randomized gradient-free
    attacks based on [[Carlini and Wagner, 2017](#bib.bibx22)], [[Croce et al., 2019](#bib.bibx36)],
    and [[Croce and Hein, 2018](#bib.bibx35)] and zeroth order optimization based
    attacks [[Chen et al., 2017](#bib.bibx29)], they are able to alter patient status
    for ML models based on patient vital signs.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Iqtidar Newaz et al., 2020](#bib.bibx70)] 通过操控设备读数以改变患者状态，展示了智能医疗系统（SHS）的脆弱性。通过执行包括回避攻击[[Tabassi
    et al., 2019](#bib.bibx164)] 在内的两种攻击，他们识别了SHS中基础机器学习模型的缺陷。采用如FGSM [[Goodfellow
    et al., 2015](#bib.bibx55)]、基于[[Carlini and Wagner, 2017](#bib.bibx22)]、[[Croce
    et al., 2019](#bib.bibx36)] 和 [[Croce and Hein, 2018](#bib.bibx35)] 的随机梯度无关攻击，以及基于零阶优化的攻击[[Chen
    et al., 2017](#bib.bibx29)]，他们能够改变基于患者生命体征的机器学习模型的患者状态。'
- en: '[[Chen et al., 2020](#bib.bibx27)] generate adversarial examples based on perturbation
    techniques for electrocardiograms. They use techniques from [[Carlini and Wagner,
    2017](#bib.bibx22)] and [[Athalye et al., 2018](#bib.bibx13)] to misguide arrhythmia
    classification. In a similar application, [[Han et al., 2020a](#bib.bibx60)] introduce
    a smooth method for perturbing input features to misclassify arrhythmia.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Chen et al., 2020](#bib.bibx27)] 基于对心电图的扰动技术生成对抗样本。他们使用[[Carlini and Wagner,
    2017](#bib.bibx22)] 和 [[Athalye et al., 2018](#bib.bibx13)] 的技术来误导心律失常分类。在类似的应用中，[[Han
    et al., 2020a](#bib.bibx60)] 引入了一种平滑的方法来扰动输入特征，以误分类心律失常。'
- en: For temporal data in EHR, [[Sun et al., 2018](#bib.bibx157)] introduce an optimization
    based attack strategy, similar to [[Chen et al., 2017](#bib.bibx29)], to perturb
    EHR input data. [[An et al., 2019](#bib.bibx6)] introduce a JSMA and attention-based
    attack by jointly modeling the saliency map and attention mechanism. Finally,
    in a domain agnostic setting, [[Naseer et al., 2019](#bib.bibx118)] introduce
    cross-domain transferability of adversarial perturbations using a generative adversarial
    network (GAN)-based framework. They show how networks trained on medical imaging
    datasets can be used to fool ImageNet based classifiers. Successful transferability
    of adversarial perturbations can make it even simpler to fool healthcare models
    across multiple task domains, and potentially modalities. One such paper examines
    the effect of universal adversarial perturbations in the medical imaging space
    [[Hirano et al., 2021](#bib.bibx65)].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电子健康记录（EHR）中的时间数据，[[Sun et al., 2018](#bib.bibx157)] 引入了一种基于优化的攻击策略，类似于[[Chen
    et al., 2017](#bib.bibx29)]，以扰动EHR输入数据。[[An et al., 2019](#bib.bibx6)] 通过联合建模显著性图和注意力机制，引入了JSMA和基于注意力的攻击。最后，在领域无关的设置中，[[Naseer
    et al., 2019](#bib.bibx118)] 引入了基于生成对抗网络（GAN）的对抗扰动的跨领域可转移性。他们展示了如何利用在医学影像数据集上训练的网络来欺骗基于ImageNet的分类器。对抗扰动的成功转移性可能会使欺骗多个任务领域和潜在模态的医疗模型变得更加简单。一个相关的研究探讨了在医学影像领域中普遍对抗扰动的效果[[Hirano
    et al., 2021](#bib.bibx65)]。
- en: Several methods have been proposed in the general domain to counter these adversarial
    attacks, namely proactive defense ([[Cisse et al., 2017](#bib.bibx31), [Gu and
    Rigazio, 2014](#bib.bibx57), [Papernot et al., 2016b](#bib.bibx121)], [[Shaham
    et al., 2018](#bib.bibx144)]) and reactive defense ([[Feinman et al., 2017](#bib.bibx46),
    [Grosse et al., 2017](#bib.bibx56), [Lu et al., 2017](#bib.bibx99)]). Proactive
    defense methods increase the robustness of models retroactively, whereas reactive
    defense models detect the adversarial examples. There are also other methods,
    such as using collaborative multi-task learning ([[Wang et al., 2020](#bib.bibx175)]).
    While it may seem that the possibility of adversarial perturbations works against
    the recommendation of using deep learning in healthcare settings, there are recent
    works pushing the boundaries by actively examining the reasons for the susceptibility
    of healthcare data to attacks ([[Ma et al., 2021](#bib.bibx104)]). Since feature
    perturbation techniques have strong policy-level implications in healthcare, it
    is also imperative to tailor general domain defense methods to the healthcare
    setting.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般领域中，已经提出了几种方法来应对这些对抗攻击，即前瞻性防御([[Cisse et al., 2017](#bib.bibx31), [Gu and
    Rigazio, 2014](#bib.bibx57), [Papernot et al., 2016b](#bib.bibx121)], [[Shaham
    et al., 2018](#bib.bibx144)])和反应性防御([[Feinman et al., 2017](#bib.bibx46), [Grosse
    et al., 2017](#bib.bibx56), [Lu et al., 2017](#bib.bibx99)])。前瞻性防御方法通过提高模型的鲁棒性来对抗攻击，而反应性防御模型则用于检测对抗样本。还有其他方法，如使用协作多任务学习([[Wang
    et al., 2020](#bib.bibx175)])。虽然对抗扰动的可能性似乎与在医疗保健环境中使用深度学习的推荐相悖，但最近的研究通过积极检验医疗数据易受攻击的原因来推动这一领域的界限([[Ma
    et al., 2021](#bib.bibx104)])。由于特征扰动技术在医疗保健中具有强烈的政策级影响，因此也必须将一般领域的防御方法调整到医疗保健环境中。
- en: 3.3 Attention
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 注意力
- en: Attention architectures designed with a special consideration for interpretability
    are routinely used for EHR-based longitudinal prediction tasks such as heart failure
    prediction [[Choi et al., 2016](#bib.bibx30), [Kaji et al., 2019](#bib.bibx77)],
    sepsis [[Kaji et al., 2019](#bib.bibx77)], intensive care unit (ICU) mortality
    [[Shi et al., 2019](#bib.bibx150)], automated diagnosis, and disease progression
    modeling [[Gao et al., 2019](#bib.bibx52), [Mullenbach et al., 2018](#bib.bibx115),
    [Ma et al., 2017](#bib.bibx103), [Bai et al., 2018](#bib.bibx16), [Alaa and van der
    Schaar, 2019](#bib.bibx4)]. The underlying representation of such a model is often
    produced by an LSTM variant with the attention used to compute the contribution
    of a given feature or time step element of the sequence to the prediction. The
    best known model of this kind is RETAIN [[Choi et al., 2016](#bib.bibx30)], which
    includes computing the attention weights over both the time-step of the time-series
    and the individual features of the inputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 设计时特别考虑解释性的注意力架构被广泛应用于基于EHR的纵向预测任务，例如心力衰竭预测[[Choi et al., 2016](#bib.bibx30),
    [Kaji et al., 2019](#bib.bibx77)]、脓毒症[[Kaji et al., 2019](#bib.bibx77)]、重症监护病房（ICU）死亡率[[Shi
    et al., 2019](#bib.bibx150)]、自动诊断和疾病进展建模[[Gao et al., 2019](#bib.bibx52), [Mullenbach
    et al., 2018](#bib.bibx115), [Ma et al., 2017](#bib.bibx103), [Bai et al., 2018](#bib.bibx16),
    [Alaa and van der Schaar, 2019](#bib.bibx4)]。这类模型的基础表示通常由一种LSTM变体生成，注意力用于计算序列中给定特征或时间步元素对预测的贡献。最著名的此类模型是RETAIN[[Choi
    et al., 2016](#bib.bibx30)]，它包括计算时间序列的时间步和输入特征的注意力权重。
- en: Pure attention-based architectures such as the Transformer have revolutionized
    NLP-based modeling, allowing the use of massive unlabeled medical text for pretraining
    [[Lee et al., 2020](#bib.bibx90), [Alsentzer et al., 2019](#bib.bibx5), [Beltagy
    et al., 2019](#bib.bibx17)]. Adoption of such models for non-text data is still
    relatively rare [[Li et al., 2020b](#bib.bibx95), [Rajan et al., 2017](#bib.bibx127)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 纯注意力架构如Transformer已经彻底改变了基于NLP的建模，允许使用大规模未标注的医学文本进行预训练[[Lee et al., 2020](#bib.bibx90),
    [Alsentzer et al., 2019](#bib.bibx5), [Beltagy et al., 2019](#bib.bibx17)]。这类模型在非文本数据中的应用仍然相对较少[[Li
    et al., 2020b](#bib.bibx95), [Rajan et al., 2017](#bib.bibx127)]。
- en: A special variant of the attention mechanism that seeks to address interpretability
    allows the model to output uncertainty on each input feature and use the aggregated
    uncertainty information for prediction [[Heo et al., 2018](#bib.bibx63)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一种寻求解决解释性问题的特殊注意力机制变体允许模型对每个输入特征输出不确定性，并使用聚合的不确定性信息进行预测[[Heo et al., 2018](#bib.bibx63)]。
- en: Despite the widespread use of the attention maps as explanations, we would caution
    against the direct interpretation of attention as an element’s contributions to
    the prediction in the medical domain. More studies are needed to disentangle self-attention
    produced representations from the context contribution itself.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管注意力图作为解释的使用非常广泛，但我们还是要谨慎对待将注意力直接解释为元素对预测的贡献，特别是在医疗领域。需要更多的研究来解开自注意力生成的表示与上下文贡献本身的关系。
- en: 3.4 Model distillation
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 模型蒸馏
- en: LIME [[Ribeiro et al., 2016a](#bib.bibx131)] is one of the most popular techniques
    used to produce instance-level explanations for black-box model predictions in
    medical AI. The model-agnostic nature of the technique has led to its use in a
    diverse set of longitudinal EHR-based prediction tasks such as heart failure prediction
    [[Khedkar et al., 2020](#bib.bibx79)], cancer type and severity inferences [[Moreira
    et al., 2020](#bib.bibx112)], breast cancer survival prediction [[Hendriks et al.,
    2020](#bib.bibx62)], and predicting development of hypertension [[Elshawi et al.,
    2019](#bib.bibx41)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: LIME [[Ribeiro et al., 2016a](#bib.bibx131)] 是一种非常流行的技术，用于为医疗AI中的黑箱模型预测生成实例级解释。该技术的模型无关特性使其在多种基于长期电子健康记录的预测任务中得到了应用，如心力衰竭预测
    [[Khedkar et al., 2020](#bib.bibx79)]、癌症类型和严重性推断 [[Moreira et al., 2020](#bib.bibx112)]、乳腺癌生存预测
    [[Hendriks et al., 2020](#bib.bibx62)] 和高血压发展预测 [[Elshawi et al., 2019](#bib.bibx41)]。
- en: It should be noted that the LIME variants are not widely used, despite the potential
    clinical usefulness of such interpretation methods. Among the potentially useful
    variants for ML in medicine are SurvLIME [[Kovalev et al., 2020](#bib.bibx85)],
    introduced specifically for producing Cox proportional hazards explanations for
    black-box survival models, and DLIME [[Zafar and Khan, 2019](#bib.bibx189)], a
    hierarchical clustering neighborhood based semi-global LIME variant for producing
    more consistent explanations for predictions over similar inputs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 应当注意的是，尽管这些解释方法在临床上具有潜在的有用性，LIME的变体并未得到广泛使用。在医学机器学习中，潜在有用的变体包括SurvLIME [[Kovalev
    et al., 2020](#bib.bibx85)]，专门用于为黑箱生存模型生成Cox比例风险解释，以及DLIME [[Zafar and Khan, 2019](#bib.bibx189)]，一种基于层次聚类邻域的半全局LIME变体，用于生成对类似输入的预测更一致的解释。
- en: 3.5 Game theory based Interpretability
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 基于博弈论的可解释性
- en: The game theory based SHAP algorithm has been widely applied in the medical
    domain for feature contribution analysis due to its ability to explain not only
    individual predictions but also global model behavior via the aggregation of Shapley
    values. SHAP is also model-agnostic, so that it can be applied to various machine
    learning algorithms [[Lundberg and Lee, 2017](#bib.bibx101), [Lundberg et al.,
    2018b](#bib.bibx102)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 基于博弈论的SHAP算法由于能够通过Shapley值的聚合来解释不仅是个别预测，还包括全局模型行为，因此在医学领域得到了广泛应用。SHAP也是模型无关的，因此可以应用于各种机器学习算法
    [[Lundberg and Lee, 2017](#bib.bibx101), [Lundberg et al., 2018b](#bib.bibx102)]。
- en: In the direct usage of SHAP for deep learning in healthcare, [[Arcadu et al.,
    2019](#bib.bibx9)] applied SHAP to find the crucial regions, which are peripheral
    fields, for identifying diabetic retinopathy progression. Also, for the interpretation
    of medical imaging,  [[Young et al., 2019](#bib.bibx187)] and [[Pianpanit et al.,
    2019](#bib.bibx125)] utilized KernelSHAP to generate the saliency maps for interpreting
    the deep neural networks for melanoma prediction and Parkinson’s disease prediction,
    respectively. [[Levy et al., 2019](#bib.bibx91)] also adopted SHAP to interpret
    the portal region prediction in pathology slide imaging. Beyond medical imaging, [[Boshra
    et al., 2019](#bib.bibx20)] used SHAP to investigate the features’ influence on
    concussion identification given the electroencephalography (EEG) signals.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习在医疗保健中的直接使用中，[[Arcadu et al., 2019](#bib.bibx9)] 应用了SHAP来寻找关键区域，即外围领域，以识别糖尿病视网膜病变的进展。此外，对于医学影像的解释，[[Young
    et al., 2019](#bib.bibx187)] 和 [[Pianpanit et al., 2019](#bib.bibx125)] 使用了KernelSHAP来生成显著性图，以解释深度神经网络对黑色素瘤预测和帕金森病预测的结果。[[Levy
    et al., 2019](#bib.bibx91)] 还采用了SHAP来解释病理幻灯片影像中的门区域预测。除了医学影像，[[Boshra et al.,
    2019](#bib.bibx20)] 使用SHAP来调查特征对脑震荡识别的影响，该研究基于脑电图（EEG）信号。
- en: '[[Ancona et al., 2019](#bib.bibx8)] uses the DASP algorithm to approximate
    the Shapley values and yields the explanation of the deep learning models and
    applies it to a fully-connected network model for predicting the Parkinson’s disease
    rating scale (UPDRS), which is a regression task to predict the severity of Parkinson’s
    disease based on 18 clinical features in a telemonitoring dataset.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Ancona et al., 2019](#bib.bibx8)] 使用DASP算法来近似Shapley值，并提供深度学习模型的解释，并将其应用于全连接网络模型，用于预测帕金森病评分量表（UPDRS），这是一个回归任务，基于18个临床特征在远程监测数据集中预测帕金森病的严重程度。'
- en: In [[Lundberg et al., 2018b](#bib.bibx102)], they also have anesthesiologists
    consulted to ensure that their model explanations are clinically meaningful. The
    anesthesiologists were asked to justify the SHAP explanations with the change
    in model output when a feature is perturbed.  [[Li et al., 2020a](#bib.bibx93)]
    also shows that it is possible to use SHAP for modeling and visualizing nonlinear
    relationship between prostate-specific antigen and Gleason score in prostate cancer
    that is consistent with the prior knowledge in the medical literature. Such clinical
    evaluations help the medical community to accept the interpretation method better.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[Lundberg et al., 2018b](#bib.bibx102)]中，他们还请了麻醉师进行咨询，以确保他们的模型解释在临床上有意义。麻醉师被要求根据特征扰动时模型输出的变化来验证SHAP解释。[[Li
    et al., 2020a](#bib.bibx93)]也显示，使用SHAP建模和可视化前列腺特异性抗原与前列腺癌Gleason评分之间的非线性关系是可能的，这与医学文献中的先前知识一致。这些临床评估有助于医学界更好地接受解释方法。
- en: Other works mentioned in this section also provide explanations that are aligned
    with prior knowledge and ground truth given by the dataset via visualization or
    computing quantitative metrics, yet none of them are justified by a formal clinical
    user study. Further study is needed for these methods and applications in healthcare.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中提到的其他工作也通过可视化或计算定量指标提供了与先前知识和数据集提供的真实情况一致的解释，但其中没有经过正式的临床用户研究验证。这些方法和在医疗保健中的应用需要进一步研究。
- en: One major concern of using SHAP in the medical domain is that the Shapley value
    and SHAP was originally derived from economics tasks, where the cost is additive.
    However, clinical features are usually heterogeneous, and the Shapley values derived
    from the model may not be meaningful in the domain [[Kovalerchuk et al., 2021](#bib.bibx84)].
    Further investigation is needed to justify real-world clinical use of SHAP-based
    interpretations.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在医学领域使用SHAP的一个主要关注点是Shapley值和SHAP最初源于经济学任务，其中成本是可加的。然而，临床特征通常是异质的，从模型中得出的Shapley值在该领域可能没有意义[[Kovalerchuk
    et al., 2021](#bib.bibx84)]。需要进一步调查以证明SHAP基于解释在实际临床中的使用。
- en: 3.6 Example based Interpretability
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 基于示例的可解释性
- en: Example-based model interpretation provides a mental model that allows clinicians
    to refer to some similar cases, prototypes, or clusters given a new case.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 基于示例的模型解释提供了一种心理模型，使临床医生可以在遇到新病例时参考一些类似的案例、原型或聚类。
- en: Researchers utilize CDEP to ignore spurious confounders in skin cancer diagnosis [[Rieger
    et al., 2020](#bib.bibx134)]. The study uses a publicly available image dataset
    from ISIC (International Skin Imaging Collaboration), which has colorful patches
    present in approximately 50% of the non-cancerous images but not in the cancerous
    images. It can be problematic if the learned model uses such spurious patch features
    as an indicator but not the critical underlying information for skin cancer prediction.
    The CDEP helps penalize the patches for having zero importance during training
    and mitigates the issue.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员利用CDEP来忽略皮肤癌诊断中的虚假混杂因素[[Rieger et al., 2020](#bib.bibx134)]。该研究使用了来自ISIC（国际皮肤影像合作组织）的公开图像数据集，其中约50%的非癌症图像中存在彩色斑块，但癌症图像中没有。如果学习到的模型将这些虚假斑块特征作为指标，而不是皮肤癌预测的关键底层信息，则可能会出现问题。CDEP有助于在训练过程中惩罚这些斑块，使其重要性为零，从而减轻这一问题。
- en: Although yielding better model performance with a quasi-explanation with a skin
    cancer classification example, CDEP has not yet been justified by a formal clinical
    user study and not yet been accepted by the medical community. It is still at
    the research rather than the deployment stage.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在皮肤癌分类示例中产生了更好的模型性能并提供了准解释，但CDEP尚未通过正式的临床用户研究得到证明，也未被医学界接受。它仍处于研究阶段而非部署阶段。
- en: 3.7 Generative based Interpretability
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 基于生成的可解释性
- en: DL interpretability can also be learned based on expert-interpretable features
    provided during the learning process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的可解释性也可以通过在学习过程中提供的专家可解释特征来获得。
- en: To provide visually interpretable evidence for breast cancer diagnostic decisions,
    [[Kim et al., 2018](#bib.bibx81)] developed an interpretability framework that
    includes a breast imaging reporting and data system (BIRADS) guided diagnosis
    network and a BIRADS critic network. The interpretable 2D BIRADS guide map, which
    is generated from the visual feature encoder, can help the diagnosis network focus
    on the critical areas related to the human-understandable BIRADS lexicon via the
    critic network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供对乳腺癌诊断决策的可视化解释证据，[[Kim et al., 2018](#bib.bibx81)] 开发了一个可解释性框架，该框架包括一个乳腺影像报告和数据系统（BIRADS）指导的诊断网络和一个BIRADS评论网络。由视觉特征编码器生成的可解释2D
    BIRADS指导图可以帮助诊断网络通过评论网络专注于与人类可理解的BIRADS词汇相关的关键区域。
- en: The study shows that with the BIRADS guide map, the performance is significantly
    higher than the network without the guide map. This finding also indicates the
    critical role and necessity of integrating medical domain knowledge while deploying
    machine learning models in healthcare.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，使用BIRADS指导图的性能显著高于没有指导图的网络。这一发现也表明，在医疗保健中部署机器学习模型时，整合医学领域知识的关键作用和必要性。
- en: For radiology, [[Shen et al., 2019](#bib.bibx148)] proposed an interpretable
    deep hierarchical semantic convolutional neural network (HSCNN) for pulmonary
    nodule malignancy prediction on CT images. HSCNN generates the binarized low-level
    expert-interpretable diagnostic semantic features that are commonly used by radiologists,
    such as sphericity, margin, and calcification; these are inputs to the high-level
    classification model, along with the latent representations learned from the visual
    encoder.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于放射学，[[Shen et al., 2019](#bib.bibx148)] 提出了一个可解释的深度层次语义卷积神经网络（HSCNN），用于对CT图像中的肺结节恶性预测。HSCNN生成的二值化低级专家可解释的诊断语义特征，如球形度、边缘和钙化，这些特征通常被放射科医生使用；这些特征作为高层分类模型的输入，同时还包括从视觉编码器学习到的潜在表示。
- en: Both [[Kim et al., 2018](#bib.bibx81)] and [[Shen et al., 2019](#bib.bibx148)]
    demonstrate that the image guide map and label generation process may help clinicians
    curate the raw image information to high-level diagnostic criteria, yet the method
    is not yet justified by formal clinical user studies. Further study is needed
    for these methods to be accepted by the medical community.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[[Kim et al., 2018](#bib.bibx81)] 和 [[Shen et al., 2019](#bib.bibx148)] 都展示了图像指导图和标签生成过程可能帮助临床医生将原始图像信息整理为高级诊断标准，但该方法尚未通过正式的临床用户研究证明。需要进一步研究这些方法以便被医学界接受。'
- en: 4 Discussion
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: 4.1 Dimensions of different interpretability methods
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 不同可解释性方法的维度
- en: 'The current literature presents several different classification schemes for
    interpretation methods in DL [[Lipton, 2018](#bib.bibx97), [Doshi-Velez and Kim,
    2017](#bib.bibx38), [Pedreschi et al., 2019](#bib.bibx123)]. In addition to the
    methodology motivated classification used in the Interpretability methods section
    of the paper, we present two different questions that every interpretation producing
    method naturally poses:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现有文献提出了几种不同的深度学习解释方法的分类方案 [[Lipton, 2018](#bib.bibx97), [Doshi-Velez and Kim,
    2017](#bib.bibx38), [Pedreschi et al., 2019](#bib.bibx123)]。除了本文解释方法部分使用的基于方法论的分类，我们还提出了每种解释生成方法自然会面临的两个不同问题：
- en: '1.'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Model Dependence: Does the explanation model depend on the internal structure
    of the model it is explaining or can it be used for producing an explanation of
    any “black-box” model?'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型依赖性：解释模型是否依赖于它所解释的模型的内部结构，还是可以用于生成任何“黑箱”模型的解释？
- en: '2.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Explanation Scope: Does the explanation model focus on producing an explanation
    for a given input-prediction pair or is it attempting to create a unified global
    explanation of the model’s behavior?'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释范围：解释模型是专注于生成给定输入-预测对的解释，还是试图创建对模型行为的统一全局解释？
- en: 'A characterization of the most commonly used methods to produce interpretations
    in health care with respect to these aspects is presented in Table [1](#S4.T1
    "Table 1 ‣ 4.1 Dimensions of different interpretability methods ‣ 4 Discussion
    ‣ Explainable Deep Learning in Healthcare: A Methodological Survey from an Attribution
    View [Advanced Review]").'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '关于这些方面的最常用方法在健康护理中的解释特征在表[1](#S4.T1 "Table 1 ‣ 4.1 Dimensions of different
    interpretability methods ‣ 4 Discussion ‣ Explainable Deep Learning in Healthcare:
    A Methodological Survey from an Attribution View [Advanced Review]")中进行了描述。'
- en: In general, the vast majority of the methods are explicitly local, producing
    the explanation for a given decision only, with some attempts at aggregation of
    the local explanation into patterns [[Ramamurthy et al., 2020](#bib.bibx129),
    [Lakkaraju et al., 2019](#bib.bibx88)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，绝大多数方法都是明确局部的，仅对给定决策提供解释，并尝试将局部解释聚合成模式 [[Ramamurthy et al., 2020](#bib.bibx129),
    [Lakkaraju et al., 2019](#bib.bibx88)]。
- en: The community appears to be deeply split on the issue of model dependence, with
    the proponents citing the necessity of explanation fidelity [[Rudin, 2019](#bib.bibx137)],
    while opponents doubt the inherent fidelity of the directly model-dependent explanations
    [[Jacovi and Goldberg, 2020](#bib.bibx72)] and stress the need for flexible model-independent
    explanation methods [[Ribeiro et al., 2016b](#bib.bibx132)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 社区在模型依赖性问题上似乎存在深刻分歧，支持者引用了解释真实性的必要性 [[Rudin, 2019](#bib.bibx137)]，而反对者质疑直接依赖模型的解释的固有真实性
    [[Jacovi and Goldberg, 2020](#bib.bibx72)]，并强调需要灵活的模型无关解释方法 [[Ribeiro et al.,
    2016b](#bib.bibx132)]。
- en: '| Class: | Model | Scope | Dep. | Potential Issues | Ref. |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 模型 | 范围 | 依赖 | 潜在问题 | 参考文献 |'
- en: '| Back-prop. | [Integrated Gradients](https://github.com/ankurtaly/Integrated-Gradients)
    | L | I | More computationally expensive than Gradients * Inputs, the baseline
    needs to be carefully selected/tuned for some cases | [[Sundararajan et al., 2017](#bib.bibx160)]
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 反向传播 | [集成梯度](https://github.com/ankurtaly/Integrated-Gradients) | L | I
    | 比梯度*输入*计算开销更大，基线需要在某些情况下仔细选择/调整 | [[Sundararajan et al., 2017](#bib.bibx160)]
    |'
- en: '| [CAM](https://github.com/ramprs/grad-cam) | L | I | Label/class discriminative
    features revealed by this method may not be convincing and accurate for some data
    samples | [[Zhou et al., 2016](#bib.bibx194)] |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [CAM](https://github.com/ramprs/grad-cam) | L | I | 这种方法揭示的标签/类别区分特征对于某些数据样本可能不够令人信服和准确
    | [[Zhou et al., 2016](#bib.bibx194)] |'
- en: '|  | [LRP](https://github.com/alewarne/Layerwise-Relevance-Propagation-for-LSTMs)
    | L | I | Initially proposed for interpreting multi-layer perceptions and hard
    to generalize well to more complex neural networks such as LSTM and Transformers
    | [[Bach et al., 2015](#bib.bibx14)] |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | [LRP](https://github.com/alewarne/Layerwise-Relevance-Propagation-for-LSTMs)
    | L | I | 最初提出用于解释多层感知器，并且难以很好地推广到更复杂的神经网络，如LSTM和Transformers | [[Bach et al.,
    2015](#bib.bibx14)] |'
- en: '| Feat perturbation | [Prediction Difference Analysis](https://github.com/lmzintgraf/DeepVis-PredDiff)
    | L | I | Computationally expensive. Simulates the absence of feature via marginalization,
    rather than exact knowledge of model behavior without the feature present. | [[Zintgraf
    et al., 2017](#bib.bibx197)] |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 特征扰动 | [预测差异分析](https://github.com/lmzintgraf/DeepVis-PredDiff) | L | I |
    计算开销大。通过边际化模拟特征的缺失，而不是对缺失特征时模型行为的精确了解。 | [[Zintgraf et al., 2017](#bib.bibx197)]
    |'
- en: '| [Representation Erasure](https://arxiv.org/pdf/1612.08220.pdf) | G | I |
    Computationally expensive, requires several steps of probing. Injects random noise
    into input for representation erasure. | [[Li et al., 2016](#bib.bibx92)] |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [表示消除](https://arxiv.org/pdf/1612.08220.pdf) | G | I | 计算开销大，需要多个探测步骤。通过在输入中注入随机噪声进行表示消除。
    | [[Li et al., 2016](#bib.bibx92)] |'
- en: '|  | [Counterfactual Generation](https://github.com/zzzace2000/FIDO-saliency)
    | L | I | Computationally expensive due to intermediate generative stage for injecting
    noise respecting data distribution. Involves similar marginalization approximations
    as Prediction Difference Analysis. | [[Chang et al., 2019](#bib.bibx26)] |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | [反事实生成](https://github.com/zzzace2000/FIDO-saliency) | L | I | 由于中间生成阶段用于注入噪声，计算开销大。涉及与预测差异分析类似的边际化近似。
    | [[Chang et al., 2019](#bib.bibx26)] |'
- en: '| Attention | [RETAIN](https://github.com/mp2893/retain) | L | D | Attention
    weight correspond to the importance of the intermediate representations to the
    final representation, not the input elements directly. | [[Choi et al., 2016](#bib.bibx30)]
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | [RETAIN](https://github.com/mp2893/retain) | L | D | 注意力权重对应于中间表示对最终表示的重要性，而不是直接对应于输入元素。
    | [[Choi et al., 2016](#bib.bibx30)] |'
- en: '| [Attend and Diagnose](https://github.com/khirotaka/SAnD) | L | D | Same issues
    as retain, exacerbated by the use of the fully-additional architecture of the
    model | [[Rajan et al., 2017](#bib.bibx127)] |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [Attend and Diagnose](https://github.com/khirotaka/SAnD) | L | D | 存在与保留相同的问题，并且由于模型完全附加架构的使用而加剧
    | [[Rajan 等, 2017](#bib.bibx127)] |'
- en: '| Model distillation | [LIME](https://github.com/marcotcr/lime) | L | D | The
    explanation is not cross instances consistent and might vary wildly for even for
    very similar instances. Possible issues on discontinuous data. Might produce inconsistent
    results across the multiple runs | [[Ribeiro et al., 2016a](#bib.bibx131)] |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 模型蒸馏 | [LIME](https://github.com/marcotcr/lime) | L | D | 解释在实例间不一致，即使对于非常相似的实例也可能大相径庭。可能存在离散数据的问题。可能在多次运行中产生不一致的结果
    | [[Ribeiro 等, 2016a](#bib.bibx131)] |'
- en: '| [Anchors](https://github.com/marcotcr/anchor) | S | I | Might produce inconsistent
    results across multiple runs. The explanation might be too specific and not very
    robust at the decision boundary | [[Ribeiro et al., 2018](#bib.bibx133)] |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [Anchors](https://github.com/marcotcr/anchor) | S | I | 可能在多次运行中产生不一致的结果。解释可能过于具体，决策边界处的鲁棒性不强
    | [[Ribeiro 等, 2018](#bib.bibx133)] |'
- en: '| Game theory | [SHAP](https://github.com/slundberg/shap) | L | I | Computationally
    expensive. Require the access to training data for interpretation. | [[Lundberg
    and Lee, 2017](#bib.bibx101)] |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 博弈论 | [SHAP](https://github.com/slundberg/shap) | L | I | 计算成本高。需要访问训练数据以进行解释。
    | [[Lundberg 和 Lee, 2017](#bib.bibx101)] |'
- en: '| Example | [Influence function](https://github.com/kohpangwei/influence-release)
    | L | I | Won’t work for models without differentiable parameters and losses.
    Only an approximation. No clear cut of “influential” and “non-influential”. May
    not be human-interpretable if there are too many feature values in a prototype.
    | [[Koh and Liang, 2017](#bib.bibx83)] |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | [影响函数](https://github.com/kohpangwei/influence-release) | L | I | 对于没有可微分参数和损失的模型不起作用。仅为近似值。没有明确的“有影响”和“无影响”区分。如果原型中的特征值过多，可能难以被人类解释。
    | [[Koh 和 Liang, 2017](#bib.bibx83)] |'
- en: '| [Contextual decomposition](https://github.com/jamie-murdoch/ContextualDecomposition)
    | L | D | Only for LSTM. Require further algorithm modifications to extend the
    method for other network architecture. | [[Murdoch et al., 2018](#bib.bibx117)]
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [上下文分解](https://github.com/jamie-murdoch/ContextualDecomposition) | L | D
    | 仅适用于 LSTM。需要进一步的算法修改以扩展到其他网络架构。 | [[Murdoch 等, 2018](#bib.bibx117)] |'
- en: '| Generative | CAGE | L | D | Require high-quality external knowledge resources.
    Task-specific method. | [[Rajani et al., 2019](#bib.bibx128)] |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 生成式 | CAGE | L | D | 需要高质量的外部知识资源。特定任务的方法。 | [[Rajani 等, 2019](#bib.bibx128)]
    |'
- en: 'Table 1: Most popular methods intended for providing interpretation (cited
    more than 50 times), Scope : Local, Semi-Global, Global. Model Dependence: Dependent,
    Independent'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：用于提供解释的最受欢迎的方法（引用超过 50 次），范围：局部、半全局、全局。模型依赖性：依赖、独立
- en: 4.2 Credibility and Trustworthiness of Interpretability Methods
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 可解释性方法的可信度和可靠性
- en: 'In this section we will discuss two aspects of the methods used to produce
    interpretations of decision models used in health care:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将探讨用于生成健康护理决策模型解释的两种方法：
- en: '1.'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: How faithful is the interpretation to the underlying decision making model?
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释对基础决策模型的忠实度如何？
- en: '2.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: How understandable are the interpretations to human expert users?
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释对人类专家用户的可理解性如何？
- en: 'The two aspects are often at odds with each other: A complex model decision
    might require a rather complex explanation to cover all of the possible aspects
    of the model’s behaviors on different inputs, which might not look easy to understand
    to humans.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法常常相互冲突：复杂的模型决策可能需要相当复杂的解释来涵盖模型在不同输入下的所有可能行为，这可能对人类来说并不容易理解。
- en: 4.2.1 Faithfulness of the interpretation
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 解释的可信度
- en: We first discuss the direct correspondence between the produced interpretation
    and the model’s decision making, known in the literature under the terms Fidelity [[Jacovi
    and Goldberg, 2020](#bib.bibx72)] or Faithfulness [[Rudin, 2019](#bib.bibx137)].
    A perfectly faithful interpretation accurately represents the decision making
    of the model being explained. If the interpretation is constrained to agree with
    the model’s behavior on all possible inputs, then no simpler explanation than
    the original model is possible. Even model dependent explanation producing methods
    may not be faithful to the original model because, as a simplified model, they
    may not include all parts of the original decision making process [[Jain and Wallace,
    2019](#bib.bibx73)].
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先讨论生成的解释与模型决策之间的直接对应关系，文献中称为**忠实度**[[Jacovi and Goldberg, 2020](#bib.bibx72)]或**真实度**[[Rudin,
    2019](#bib.bibx137)]。一个完全真实的解释准确地反映了被解释模型的决策。如果解释必须与模型在所有可能输入下的行为一致，则没有比原始模型更简单的解释方法。即使是依赖模型的解释生成方法也可能不忠实于原始模型，因为作为简化模型，它们可能未包含原始决策过程的所有部分[[Jain
    and Wallace, 2019](#bib.bibx73)]。
- en: When using an explanation producing model for black-box models trained on complex
    healthcare data, we recommend the user to consider the following issues to gain
    more insight into the explanation model’s faithfulness.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用解释生成模型对复杂医疗数据训练的黑箱模型进行分析时，我们建议用户考虑以下问题，以深入了解解释模型的真实性。
- en: '1.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'For explanations that, in themselves, are predictive models, look at the prediction
    agreement between the explanation model and the original: if the concordance is
    low, then the model is not faithful.'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于本身就是预测模型的解释，请查看解释模型和原始模型之间的预测一致性：如果一致性低，则模型不真实。
- en: '2.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: While it is hard to estimate the fidelity of an explanation method, consider
    computing recently proposed fidelity measures over the set of the explanation
    methods you are planning to use [[Yeh et al., 2019](#bib.bibx185)].
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然估计解释方法的忠实度很困难，但可以考虑计算最近提出的忠实度度量，以评估你计划使用的一组解释方法[[Yeh et al., 2019](#bib.bibx185)]。
- en: '3.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Consider running “feature occlusion” sanity checks, to check if changing those
    model elements according to the explanation change the original predictions [[Hooker
    et al., 2018](#bib.bibx67)].
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑运行“特征遮蔽”检查，以验证根据解释改变模型元素是否会改变原始预测[[Hooker et al., 2018](#bib.bibx67)]。
- en: '4.'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Due to the nature of some interpretation producing models, the same model might
    produce different explanations for the same pair of input-outputs over multiple
    runs.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于某些解释生成模型的特性，相同模型可能在多次运行中为相同的输入-输出对生成不同的解释。
- en: 4.2.2 Plausibility of the interpretation as defined by the expert user
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 由专家用户定义的解释的合理性
- en: Traditionally, clinicians tend to embrace expert-curated models, such as the
    APACHE (Acute Physiology and Chronic Health Evaluation) score for evaluating the
    patient severity in the ICU [[Knaus et al., 1985](#bib.bibx82)], due to the consistency
    between used model features and domain knowledge. In contrast, machine learning
    approaches for healthcare problems aim to further improve performance by learning
    a much more complex representations from raw features while sacrificing model
    transparency. Machine learning interpretability methods may provide human-understandable
    explanations, yet it is crucial that the explanations should be aligned with our
    knowledge to be trustable, especially for real-world deployment in the healthcare
    domain. However, current deployments with interpretability methods mainly focus
    only on helping to debug the model for engineers, but not the real-world use for
    end users [[Bhatt et al., 2020b](#bib.bibx19)]. The appropriate interpretability
    methods should be selected and evaluated both to help model developers (data scientists
    and machine learning practitioners) understand how their models behave, and to
    assist clinicians to understand the rationale for model predictions for decision
    making.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，临床医生倾向于采用专家策划的模型，如用于评估ICU患者严重程度的APACHE（急性生理和慢性健康评估）评分[[Knaus et al., 1985](#bib.bibx82)]，因为所用模型特征与领域知识之间的一致性。相比之下，针对医疗保健问题的机器学习方法旨在通过从原始特征中学习更复杂的表示来进一步提高性能，同时牺牲模型透明度。机器学习解释性方法可能提供易于理解的解释，但重要的是，这些解释应与我们的知识对齐，以便值得信赖，特别是在医疗领域的实际部署中。然而，目前使用解释性方法的部署主要关注于帮助工程师调试模型，而非实际用户[[Bhatt
    et al., 2020b](#bib.bibx19)]。应选择并评估适当的解释性方法，以帮助模型开发者（数据科学家和机器学习从业者）理解其模型的行为，并协助临床医生理解模型预测的依据，以便进行决策。
- en: For model developers, researchers evaluate their use of interpretability methods
    with different levels of model transparency (generalized additive models (GAMs)
    and SHAP), from both quantitative (machine-learned interpretability) and qualitative
    (visualization) perspectives using interviews and surveys [[Kaur et al., 2020](#bib.bibx78)].
    The results, however, show that developers usually over-trust the methods and
    this may lead to their misuse, especially over-relying on their “thinking fast
    (system 1)” [[Kahneman, 2011](#bib.bibx76)] since the good visualization may sway
    human thought, but may not fully explain the behavior of the system and may be
    incorrectly interpreted by developers. Moreover, visualization sometimes is not
    able to be fully understood and interpreted correctly by the model developers.
    The authors point out that developers usually just focus on superficial values
    for model debugging instead of using explanations to dig deeper into data or model
    problems. They also enumerate the common issues faced by developers, which include
    missing values, data change over time, data duplication, redundant features, ad-hoc
    categorization, and difficulties of debugging the methods based on explanations.
    The developers are also shown to be biased toward model deployment even after
    recognizing suspicious aspects of the models.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型开发者，研究人员从量化（机器学习解释性）和质性（可视化）两个角度，通过访谈和调查评估其使用不同水平的模型透明度（广义加性模型（GAMs）和SHAP）的解释性方法[[Kaur
    et al., 2020](#bib.bibx78)]。然而，结果显示，开发者通常过于信任这些方法，这可能导致误用，特别是过度依赖他们的“快速思维（系统1）”[[Kahneman,
    2011](#bib.bibx76)]，因为良好的可视化可能影响人的思维，但可能无法完全解释系统的行为，并且可能被开发者误解。此外，可视化有时无法被模型开发者完全理解和正确解释。作者指出，开发者通常只关注模型调试中的表面值，而不是使用解释来深入挖掘数据或模型问题。他们还列举了开发者常遇到的问题，包括缺失值、数据随时间变化、数据重复、冗余特征、临时分类以及基于解释调试方法的困难。即使在识别模型中的可疑方面后，开发者也显示出对模型部署的偏见。
- en: From a clinical perspective, it is necessary and critical to have clinically
    relevant features that align with medical knowledge and clinical practice [[Caruana
    et al., 2015](#bib.bibx24)], while under-performing models may still be acceptable
    as long as the errors are explainable. In [[Tonekaboni et al., 2019b](#bib.bibx167)],
    the authors survey clinicians in the ICU and emergency department to understand
    the clinicians’ need for explanation, which is mainly to justify their clinical
    decision-making to patients and colleagues.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从临床角度来看，具有与医学知识和临床实践一致的临床相关特征是必要且关键的[[Caruana et al., 2015](#bib.bibx24)]，而表现不佳的模型只要错误是可解释的，仍然是可以接受的。在[[Tonekaboni
    et al., 2019b](#bib.bibx167)]中，作者对ICU和急诊科的临床医生进行了调查，以了解临床医生对解释的需求，主要是为了向患者和同事证明他们的临床决策。
- en: Depending on the problem scope, different levels of interpretability may be
    considered by clinicians. [[Elshawi et al., 2019](#bib.bibx41)] conduct a case
    study of the hypertension risk prediction problem using the random forest algorithm
    and explore the important factors with different model-agnostic interpretability
    techniques at either global or local-level interpretation. They find that different
    interpretability methods in general provide insights from different perspectives
    to assist clinicians to have a better understanding of the model behavior depending
    on clinical applications. Global methods can generalize over the whole cohort
    while local methods show the explanation for specific instances. Thus, applications
    such as the hypertension risk prediction problem may focus on global risk factors
    derived from either global interpretability methods, mainly non-DL based techniques
    such as feature importance and partial dependence plot, or the aggregation of
    local explainers (e.g. SHAP, LIME) [[Elshawi et al., 2019](#bib.bibx41)], while
    disease progression prediction requires integrated interpretations at local, cohort-specific
    and global levels [[Ahmad et al., 2018](#bib.bibx2)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题范围，临床医生可能会考虑不同级别的可解释性。[[Elshawi et al., 2019](#bib.bibx41)] 对高血压风险预测问题进行了案例研究，使用随机森林算法，并探讨了不同模型无关可解释性技术在全局或局部层面上的重要因素。他们发现，不同的可解释性方法通常从不同的角度提供见解，帮助临床医生更好地理解模型行为，具体取决于临床应用。全局方法可以对整个队列进行泛化，而局部方法则展示特定实例的解释。因此，像高血压风险预测问题这样的应用可能会集中于从全局可解释性方法中获得的全局风险因素，这些方法主要是基于非深度学习的技术，如特征重要性和部分依赖图，或者局部解释器（如SHAP、LIME）的汇总[[Elshawi
    et al., 2019](#bib.bibx41)]，而疾病进展预测则需要在局部、队列特定和全球层面上进行综合解释[[Ahmad et al., 2018](#bib.bibx2)]。
- en: 'However, different interpretability methods may yield a different subset of
    clinically relevant important features due to their ways to obtain feature importance.
    For instance, SHAP, coefficient of regression models, and permutation-based feature
    importance may provide completely different interpretations even if they are all
    at the global level. With some clinical examples, researchers found that the local
    interpretation methods (LIME and SHAP) of the correctly predicted samples are
    in general intuitive and follow common patterns, yet for the incorrectly predicted
    cases (either false positive or false negative cases), these local methods can
    be less consistent and more difficult to interpret [[Elshawi et al., 2019](#bib.bibx41)].
    Nevertheless, the users may not be aware of the assumption of using the model
    and how it makes the decision: e.g., the additivity assumption of the SHAP algorithm.
    Interpretability can be quite subjective, and the computerized techniques for
    producing interpretations lack the interactivity that is often crucial when one
    human expert is trying to convince another [[Lahav et al., 2018](#bib.bibx86)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于不同的可解释性方法获得特征重要性的方式不同，可能会产生不同的临床相关重要特征子集。例如，即使SHAP、回归模型系数和基于置换的特征重要性都在全局层面，它们也可能提供完全不同的解释。通过一些临床实例，研究人员发现，正确预测样本的局部解释方法（LIME和SHAP）通常是直观的并遵循常见模式，而对于错误预测的情况（无论是假阳性还是假阴性），这些局部方法可能不那么一致，更难以解释[[Elshawi
    et al., 2019](#bib.bibx41)]。然而，用户可能没有意识到使用模型的假设以及它如何做出决策：例如，SHAP算法的可加性假设。可解释性可以是相当主观的，计算机化的解释技术缺乏在一个专家试图说服另一个专家时通常至关重要的互动性[[Lahav
    et al., 2018](#bib.bibx86)]。
- en: Studies also show shortcomings of some interpretability methods while adopting
    them for real-world clinical settings [[Tonekaboni et al., 2019b](#bib.bibx167),
    [Elshawi et al., 2019](#bib.bibx41)]. For example, the complex correlation between
    features in feature importance-based methods, the weak correlation between feature
    importance and learned attention weights for recurrent neural encoders [[Jain
    and Wallace, 2019](#bib.bibx73)], and the trade-off between performance and explainability
    for rule-based methods, are all potential problems of using global interpretability
    methods [[Tonekaboni et al., 2019b](#bib.bibx167)]. For local interpretability
    methods, researchers also show that clinicians can easily conclude the explanation
    at the feature-level using LIME, but the main problem is that the LIME explanation
    can be quite unstable, where patients with similar patterns may have very different
    interpretations [[Elshawi et al., 2019](#bib.bibx41)]. Instead, the advantage
    of the Shapley value interpretation method is that it makes the instance prediction
    considering all feature values of the instance, and therefore the patients with
    similar feature values will also have similar interpretations [[Elshawi et al.,
    2019](#bib.bibx41)]. But the cons of Shapley value-based methods are that they
    can be computationally expensive and that they need to access the training data
    while building model explainers [[Lundberg and Lee, 2017](#bib.bibx101), [Janzing
    et al., 2020](#bib.bibx74)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 研究还显示了一些可解释性方法在应用于实际临床环境中的不足之处[[Tonekaboni et al., 2019b](#bib.bibx167), [Elshawi
    et al., 2019](#bib.bibx41)]。例如，特征重要性方法中复杂的特征间相关性、特征重要性与循环神经编码器中学习的注意力权重之间的弱相关性[[Jain
    and Wallace, 2019](#bib.bibx73)]，以及基于规则的方法在性能和可解释性之间的权衡，都是使用全局可解释性方法的潜在问题[[Tonekaboni
    et al., 2019b](#bib.bibx167)]。对于局部可解释性方法，研究人员还指出，临床医生可以使用LIME在特征层面轻松得出解释，但主要问题在于LIME解释可能非常不稳定，具有相似模式的患者可能会有非常不同的解释[[Elshawi
    et al., 2019](#bib.bibx41)]。相比之下，Shapley值解释方法的优势在于它考虑了实例的所有特征值，从而具有相似特征值的患者也会有类似的解释[[Elshawi
    et al., 2019](#bib.bibx41)]。但基于Shapley值的方法的缺点是它们可能计算开销大，并且在构建模型解释器时需要访问训练数据[[Lundberg
    and Lee, 2017](#bib.bibx101), [Janzing et al., 2020](#bib.bibx74)]。
- en: It is not trivial to select appropriate interpretability methods for real-world
    healthcare applications. Researchers therefore provide a list of metrics, including
    identity, stability, separability, similarity, time, bias detection and trust,
    to evaluate different interpretability methods when considering real-world deployment [[ElShawi
    et al., 2020](#bib.bibx42)]. However, they find that there is no consistent winning
    method for all metrics across various interpretability methods, such as LIME,
    SHAP and Anchors. Thus, it is essential to make a clear plan and think more about
    the clinical application and interpretability focus in order to select the reasonable
    and effective interpretability methods and metrics for real-world use.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为实际的医疗保健应用选择合适的可解释性方法并非易事。因此，研究人员提供了一系列指标，包括身份、稳定性、可分离性、相似性、时间、偏差检测和信任，以评估在实际部署时考虑的不同可解释性方法[[ElShawi
    et al., 2020](#bib.bibx42)]。然而，他们发现对于各种可解释性方法（如LIME、SHAP和Anchors）来说，并没有一种在所有指标上都表现一致的获胜方法。因此，制定清晰的计划，并更多考虑临床应用和可解释性重点，以选择合理有效的可解释性方法和指标对于实际应用至关重要。
- en: To further achieve the potential clinical impact of deployed models, we should
    not only focus on advancing machine learning techniques, but also need to consider
    human-computer interaction (HCI), which investigates complex systems from the
    user viewpoint, and propose better designs to bridge the gap between users and
    machines. End users’ involvement in the design of machine learning tools is also
    critical to understand the skills and real needs of end users and how they will
    utilize the model outputs [[Ahmad et al., 2018](#bib.bibx2), [Feng and Boyd-Graber,
    2019](#bib.bibx47)]. [[Kaur et al., 2020](#bib.bibx78)] suggest that it may be
    beneficial to design interpretability tools that allow back-and-forth communication
    (human-in-the-loop) to make interpretability a bidirectional exploration, and
    also to build tools that can activate thinking via “system 2” for deeper reasoning [[Kahneman,
    2011](#bib.bibx76)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步实现部署模型的潜在临床影响，我们不仅应专注于推进机器学习技术，还需要考虑人机交互（HCI），该领域从用户的角度研究复杂系统，并提出更好的设计以弥合用户与机器之间的差距。最终用户在机器学习工具设计中的参与对于理解最终用户的技能和实际需求以及他们如何利用模型输出也至关重要[[Ahmad
    et al., 2018](#bib.bibx2), [Feng and Boyd-Graber, 2019](#bib.bibx47)]。[[Kaur et
    al., 2020](#bib.bibx78)]建议，设计能够进行双向沟通（人机交互）的可解释性工具可能会有益，使可解释性成为一个双向探索的过程，并且构建能够通过“系统2”激发思考的工具，以便进行更深入的推理[[Kahneman,
    2011](#bib.bibx76)]。
- en: 4.3 Benchmarking Interpretation Methods
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准测试解释方法
- en: Now we have many different kinds of interpretation methods to choose when we
    want to analyze a neural model, although they are still in need of further improvement.
    At the current state of the art, which method we should choose still does not
    have a definite answer. The choice of the right interpretation method should depend
    on the specific model type we want to interpret; however, such a detailed and
    comprehensive guideline for all kinds of models to be analyzed is currently not
    available. Several recent studies started to look into this problem by benchmarking
    some popularly used interpretation methods applied to some neural models such
    as CNN, RNN, and transformer. For example, [[Arras et al., 2019](#bib.bibx11)]
    first use four interpretation methods, namely LRP, Gradient*Input, occlusion-based
    explanation [[Li et al., 2016](#bib.bibx92)], and CD [[Murdoch et al., 2018](#bib.bibx117)],
    to obtain the relevance scores of each word in the text for the LSTM model for
    text classification tasks, and then measure the change of accuracy after removing
    two or three words in decreasing order of their relevance. By comparing the percentage
    of accuracy decrement, they observe that LRP and CD perform on-par with the occlusion-based
    relevance, with near 100% accuracy change, followed by Gradient*Input which leads
    to only 66% accuracy change. This experiment indicates that LRP, CD, and occlusion-based
    methods can better identify the most relevant words than Gradient*Input. As a
    counterpart, [[Ismail et al., 2020](#bib.bibx71)] argue that one should not compare
    interpretation methods solely on the loss of accuracy after masking since the
    removal of two or three features may not be sufficient for the model to behave
    incorrectly. Instead, they choose to measure the precision and recall of features
    identified as salient by comparing against ground truth important features and
    report the weighted precision and recall as the benchmarking metric. However,
    their annotations of which features are important are synthesized rather than
    collected by human annotation, which is not that convincing. In a more theoretical
    way, [[Bhatt et al., 2020a](#bib.bibx18)] propose several equations as quantitative
    evaluation criteria to measure and compare the sensitivity, faithfulness, and
    complexity of feature-based explanation methods.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有很多不同类型的解释方法可供选择，当我们想要分析一个神经模型时，尽管它们仍然需要进一步改进。在当前的技术水平下，我们应该选择哪种方法仍然没有明确的答案。选择正确的解释方法应该取决于我们想要解释的具体模型类型；然而，目前还没有关于分析各种模型的详细和全面的指导方针。最近的一些研究开始通过对一些常用的解释方法在
    CNN、RNN 和 transformer 等神经模型上的应用进行基准测试来研究这个问题。例如，[[Arras et al.，2019](#bib.bibx11)]
    首先使用了四种解释方法，即 LRP、Gradient*Input、基于遮挡的解释 [[Li et al.，2016](#bib.bibx92)] 和 CD
    [[Murdoch et al.，2018](#bib.bibx117)]，来获取用于文本分类任务的 LSTM 模型中每个单词的相关性分数，然后按照相关性的降序测量去除两个或三个单词后的准确性变化。通过比较准确性降低的百分比，他们观察到
    LRP 和 CD 与基于遮挡的相关性表现相当，准确性变化接近 100%，其次是 Gradient*Input，准确性变化只有 66%。这个实验表明，LRP、CD
    和基于遮挡的方法可以更好地识别出最相关的单词，而不是 Gradient*Input。作为对比，[[Ismail et al.，2020](#bib.bibx71)]
    认为，在屏蔽后准确性的损失上单独比较解释方法是不合理的，因为去除两个或三个特征可能对模型产生不正确的行为不足够。相反，他们选择通过将被识别为显著特征与地面真实重要特征进行比较，以测量特征的精确率和召回率，并报告加权精确率和召回率作为基准指标。然而，他们对哪些特征是重要的进行的注释是合成的，而不是由人工注释收集的，这并不太令人信服。在更理论的角度上，[[Bhatt
    et al.，2020a](#bib.bibx18)] 提出了几个方程作为用于量化评估和比较基于特征解释方法的敏感性、忠实度和复杂性的评估标准。
- en: Through these benchmarking evaluations, we find that different interpretation
    methods may vary a lot in their advantages and disadvantages. To make use of this
    fact, some studies propose to aggregate two kinds of interpretation methods so
    that they can complement each other [[Ismail et al., 2020](#bib.bibx71)]. For
    instance, [[Bhatt et al., 2020a](#bib.bibx18)] develop an aggregation scheme for
    learning combinations of various explanation functions, and devise schemes to
    learn explanations with lower complexity and lower sensitivity. We hope to see
    more efforts along this direction to generalize such an aggregation scheme to
    a broader range of interpretation methods.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些基准评估，我们发现不同的解释方法在优缺点上可能差异很大。为利用这一事实，一些研究提出了聚合两种解释方法，以便它们能够互补[[Ismail et
    al., 2020](#bib.bibx71)]。例如，[[Bhatt et al., 2020a](#bib.bibx18)]开发了一种聚合方案，用于学习各种解释函数的组合，并设计了学习具有较低复杂性和较低敏感性的解释的方案。我们希望看到更多的努力在这一方向上推广这种聚合方案，以适用于更广泛的解释方法。
- en: 5 Conclusion
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this review, we provided a broad overview of interpretation methods for
    interpreting the black-box DL models deployed for healthcare problems. We started
    by summarizing the methodologies of seven classes of interpretation methods in
    Section 2\. Then we proceeded to discuss how these methods, which were initially
    proposed for general domain applications, are adapted for solving healthcare problem
    in Section 3\. Finally in Section 4, we continued discussing three important aspects
    in the process of applying these interpretation methods to medical/clinical problems:
    1\. Are these interpretation methods model agnostic? 2\. How good are their credibility
    and trustworthiness? 3\. How to compare the performance of the methods so as to
    choose the most appropriate one for use? We hope these summaries and discussions
    can throw some light onto the field of explainable DL in healthcare and help healthcare
    researchers and clinical practitioners build both high-performing and explainable
    models.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次综述中，我们提供了对解释黑箱深度学习模型在医疗问题中的应用的广泛概述。我们首先在第2节总结了七类解释方法的研究方法。然后，我们在第3节讨论了这些最初为通用领域应用提出的方法如何适应解决医疗问题。最后，在第4节，我们继续讨论了将这些解释方法应用于医疗/临床问题的过程中三个重要方面：1.
    这些解释方法是否模型无关？2. 其可信度和可靠性如何？3. 如何比较这些方法的性能，以选择最合适的方法？我们希望这些总结和讨论能够为医疗领域的可解释深度学习提供一些启示，并帮助医疗研究人员和临床从业者建立高性能且可解释的模型。
- en: Funding Information
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 资助信息
- en: The authors’ work was supported in part by collaborative research agreements
    with IBM, Wistron, and Bayer Pharmaceuticals, and by NIH grant 1R01LM013337 from
    the National Library of Medicine. The authors declare no conflicts of interest.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 作者的研究部分得到了与IBM、纬创资通和拜耳制药的合作研究协议的支持，以及来自国家医学图书馆的NIH资助1R01LM013337。作者声明没有利益冲突。
- en: '| Abbreviation | Full Form |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 完整形式 |'
- en: '| --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| A2C | Advantage Actor Critic |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| A2C | 优势演员评论家 |'
- en: '| AI | Artificial Intelligence |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| AI | 人工智能 |'
- en: '| BIRADS | Breast Imaging Reporting And Data System |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| BIRADS | 乳腺影像报告和数据系统 |'
- en: '| CAGE | Commonsense Auto-Generated Explanations |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| CAGE | 常识自动生成解释 |'
- en: '| CAM | Class Activation Mapping |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| CAM | 类激活映射 |'
- en: '| CD | Contextual Decomposition |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| CD | 上下文分解 |'
- en: '| CDEP | Contextual Decomposition Explanation Penalization |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CDEP | 上下文分解解释惩罚 |'
- en: '| CNN | Convolutional Neural Network |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 |'
- en: '| CT | Computed Tomography |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| CT | 计算机断层扫描 |'
- en: '| DASP | Deep Approximate Shapley Propagation |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| DASP | 深度近似Shapley传播 |'
- en: '| DDQN | Double Deep Q Network |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| DDQN | 双深度Q网络 |'
- en: '| DL | Deep Learning |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| DL | 深度学习 |'
- en: '| DNN | Deep Neural Network |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| DNN | 深度神经网络 |'
- en: '| DQN | Deep Q Network |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| DQN | 深度Q网络 |'
- en: '| EEG | Electroencephalography |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| EEG | 脑电图 |'
- en: '| EF | Explainable Factor |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| EF | 可解释因子 |'
- en: '| EHR | Electronic Health Record |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| EHR | 电子健康记录 |'
- en: '| EU | European Union |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| EU | 欧盟 |'
- en: '| FGSM | Fast Gradient Sign Method |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| FGSM | 快速梯度符号法 |'
- en: '| GAM | Generalized Additive Models |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| GAM | 广义加性模型 |'
- en: '| GAN | Generative Adversarial Network |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| GAN | 生成对抗网络 |'
- en: '| GDPR | General Data Protection Regulation |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| GDPR | 通用数据保护条例 |'
- en: '| GEF | Generative Explanation Framework |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| GEF | 生成解释框架 |'
- en: '| HCI | Human-Computer Interaction |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| HCI | 人机交互 |'
- en: '| HSCNN | Hierarchical Semantic Convolutional Neural Network |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| HSCNN | 分层语义卷积神经网络 |'
- en: '| ICU | Intensive Care Unit |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| ICU | 重症监护室 |'
- en: '| ISIC | International Skin Imaging Collaboration |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| ISIC | 国际皮肤影像合作 |'
- en: '| JSMA | Jacobian-based Saliency Map Attack |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| JSMA | 基于雅可比矩阵的显著性图攻击 |'
- en: '| L-BFGS | Limited-memory Broyden-Fletcher-Goldfarb-Shanno |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| L-BFGS | 有限记忆Broyden-Fletcher-Goldfarb-Shanno |'
- en: '| L2X | Learning to Explain |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| L2X | 学习解释 |'
- en: '| LIME | Local Interpretable Model-agnostic Explanations |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| LIME | 局部可解释模型无关解释 |'
- en: '| LRP | Layer-wise Relevance Propagation |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| LRP | 层级相关传播 |'
- en: '| LSTM | Long Short-Term Memory |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| LSTM | 长短期记忆 |'
- en: '| ML | Machine Learning |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| ML | 机器学习 |'
- en: '| MRI | Magnetic Resonance Imaging |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| MRI | 磁共振成像 |'
- en: '| MRT | Minimum Risk Training |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| MRT | 最小风险训练 |'
- en: '| NLP | Natural Language Processing |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| NLP | 自然语言处理 |'
- en: '| PGD | Projected Gradient Descent |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| PGD | 投影梯度下降 |'
- en: '| PPO | Proximal Policy Optimization |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| PPO | 近端策略优化 |'
- en: '| QA | Question Answering |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| QA | 问答系统 |'
- en: '| ReLU | Rectified Linear Unit |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | 修正线性单元 |'
- en: '| RETAIN | Reverse Time Attention Model |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| RETAIN | 反向时间注意模型 |'
- en: '| RNN | Recurrent Neural Network |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| RNN | 循环神经网络 |'
- en: '| SHAP | Shapley Additive Explanations |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| SHAP | Shapley加法解释 |'
- en: '| SHS | Smart Healthcare Systems |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| SHS | 智能医疗系统 |'
- en: '| SISTA | Sequential Iterative Soft-Thresholding Algorithm |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SISTA | 顺序迭代软阈值算法 |'
- en: 'Table 2: Glossary of abbreviations and acronyms.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：缩写词和首字母缩略词词汇表。
- en: References
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: '[Ahern et al., 2019] Ahern, I., Noack, A., Guzman-Nateras, L., Dou, D., Li,
    B., and Huan, J. (2019). Normlime: A new feature importance metric for explaining
    deep neural networks. arXiv preprint arXiv:1909.04200.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ahern et al., 2019] Ahern, I., Noack, A., Guzman-Nateras, L., Dou, D., Li,
    B., and Huan, J. (2019). Normlime: 一种新的特征重要性度量，用于解释深度神经网络。arXiv预印本arXiv:1909.04200。'
- en: '[Ahmad et al., 2018] Ahmad, M. A., Eckert, C., and Teredesai, A. (2018). Interpretable
    machine learning in healthcare. In Proceedings of the 2018 ACM international conference
    on bioinformatics, computational biology, and health informatics, pages 559–560.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ahmad et al., 2018] Ahmad, M. A., Eckert, C., and Teredesai, A. (2018). 医疗领域的可解释机器学习。2018年ACM国际生物信息学、计算生物学和健康信息学会议论文集，第559–560页。'
- en: '[Akhtar and Mian, 2018] Akhtar, N. and Mian, A. (2018). Threat of adversarial
    attacks on deep learning in computer vision: A survey. IEEE Access, 6:14410–14430.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Akhtar and Mian, 2018] Akhtar, N. and Mian, A. (2018). 深度学习在计算机视觉中的对抗攻击威胁：一项调查。IEEE
    Access，6:14410–14430。'
- en: '[Alaa and van der Schaar, 2019] Alaa, A. M. and van der Schaar, M. (2019).
    Attentive state-space modeling of disease progression. In Advances in Neural Information
    Processing Systems, pages 11338–11348.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Alaa and van der Schaar, 2019] Alaa, A. M. and van der Schaar, M. (2019).
    关注状态空间建模的疾病进展。在神经信息处理系统进展中，第11338–11348页。'
- en: '[Alsentzer et al., 2019] Alsentzer, E., Murphy, J. R., Boag, W., Weng, W.-H.,
    Jin, D., Naumann, T., Redmond, W., and McDermott, M. B. (2019). Publicly available
    clinical bert embeddings. NAACL HLT 2019, page 72.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Alsentzer et al., 2019] Alsentzer, E., Murphy, J. R., Boag, W., Weng, W.-H.,
    Jin, D., Naumann, T., Redmond, W., and McDermott, M. B. (2019). 公开可用的临床BERT嵌入。NAACL
    HLT 2019，第72页。'
- en: '[An et al., 2019] An, S., Xiao, C., Stewart, W. F., and Sun, J. (2019). Longitudinal
    adversarial attack on electronic health records data. In The World Wide Web Conference,
    pages 2558–2564.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[An et al., 2019] An, S., Xiao, C., Stewart, W. F., and Sun, J. (2019). 对电子健康记录数据的纵向对抗攻击。在万维网会议上，第2558–2564页。'
- en: '[Ancona et al., 2018] Ancona, M., Ceolini, E., Öztireli, C., and Gross, M.
    (2018). Towards better understanding of gradient-based attribution methods for
    deep neural networks. In International Conference on Learning Representations.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ancona et al., 2018] Ancona, M., Ceolini, E., Öztireli, C., and Gross, M.
    (2018). 更好地理解基于梯度的深度神经网络归因方法。在国际学习表示会议上。'
- en: '[Ancona et al., 2019] Ancona, M., Oztireli, C., and Gross, M. (2019). Explaining
    deep neural networks with a polynomial time algorithm for shapley value approximation.
    In International Conference on Machine Learning, pages 272–281.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ancona et al., 2019] Ancona, M., Oztireli, C., and Gross, M. (2019). 使用多项式时间算法解释深度神经网络中的Shapley值近似。在国际机器学习会议上，第272–281页。'
- en: '[Arcadu et al., 2019] Arcadu, F., Benmansour, F., Maunz, A., Willis, J., Haskova,
    Z., and Prunotto, M. (2019). Deep learning algorithm predicts diabetic retinopathy
    progression in individual patients. NPJ digital medicine, 2(1):1–9.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arcadu et al., 2019] Arcadu, F., Benmansour, F., Maunz, A., Willis, J., Haskova,
    Z., and Prunotto, M. (2019). 深度学习算法预测个体患者糖尿病视网膜病变的进展。NPJ数字医学，2(1):1–9。'
- en: '[Ardabili et al., 2019] Ardabili, S., Mosavi, A., Dehghani, M., and Várkonyi-Kóczy,
    A. R. (2019). Deep learning and machine learning in hydrological processes climate
    change and earth systems a systematic review. In International Conference on Global
    Research and Education, pages 52–62\. Springer.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ardabili 等, 2019] Ardabili, S., Mosavi, A., Dehghani, M., 和 Várkonyi-Kóczy,
    A. R. (2019). 气候变化和地球系统中的水文过程中的深度学习和机器学习：系统评审。在全球研究与教育国际会议的会议录中，第52–62页。Springer。'
- en: '[Arras et al., 2019] Arras, L., Osman, A., Müller, K.-R., and Samek, W. (2019).
    Evaluating recurrent neural network explanations. In Proceedings of the 2019 ACL
    Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages
    113–126, Florence, Italy. Association for Computational Linguistics.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Arras 等, 2019] Arras, L., Osman, A., Müller, K.-R., 和 Samek, W. (2019). 评估递归神经网络解释。在2019
    ACL 研讨会 BlackboxNLP: 分析和解释自然语言处理中的神经网络的会议录中，第113–126页，意大利佛罗伦萨。计算语言学协会。'
- en: '[Ashfaq et al., 2019] Ashfaq, A., Sant’Anna, A., Lingman, M., and Nowaczyk,
    S. (2019). Readmission prediction using deep learning on electronic health records.
    Journal of biomedical informatics, 97:103256.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ashfaq 等, 2019] Ashfaq, A., Sant’Anna, A., Lingman, M., 和 Nowaczyk, S. (2019).
    使用深度学习对电子健康记录进行再入院预测。生物医学信息学杂志, 97:103256。'
- en: '[Athalye et al., 2018] Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. (2018).
    Synthesizing robust adversarial examples. In International conference on machine
    learning, pages 284–293\. PMLR.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Athalye 等, 2018] Athalye, A., Engstrom, L., Ilyas, A., 和 Kwok, K. (2018).
    合成鲁棒的对抗样本。在国际机器学习会议中，第284–293页。PMLR。'
- en: '[Bach et al., 2015] Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller,
    K.-R., and Samek, W. (2015). On pixel-wise explanations for non-linear classifier
    decisions by layer-wise relevance propagation. PloS one, 10(7):e0130140.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bach 等, 2015] Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.-R.,
    和 Samek, W. (2015). 关于通过层次相关传播进行的像素级解释。PloS one, 10(7):e0130140。'
- en: '[Bahdanau et al., 2014] Bahdanau, D., Cho, K., and Bengio, Y. (2014). Neural
    machine translation by jointly learning to align and translate. ICLR.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bahdanau 等, 2014] Bahdanau, D., Cho, K., 和 Bengio, Y. (2014). 通过联合学习对齐和翻译的神经机器翻译。ICLR。'
- en: '[Bai et al., 2018] Bai, T., Zhang, S., Egleston, B. L., and Vucetic, S. (2018).
    Interpretable representation learning for healthcare via capturing disease progression
    through time. In Proceedings of the 24th ACM SIGKDD International Conference on
    Knowledge Discovery & Data Mining, pages 43–51.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bai 等, 2018] Bai, T., Zhang, S., Egleston, B. L., 和 Vucetic, S. (2018). 通过捕捉疾病进展进行可解释的医疗保健表示学习。在第24届
    ACM SIGKDD 国际知识发现与数据挖掘会议的会议录中，第43–51页。'
- en: '[Beltagy et al., 2019] Beltagy, I., Lo, K., and Cohan, A. (2019). Scibert:
    A pretrained language model for scientific text. In Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3606–3611.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Beltagy 等, 2019] Beltagy, I., Lo, K., 和 Cohan, A. (2019). Scibert：用于科学文本的预训练语言模型。在2019年自然语言处理经验方法会议和第九届国际自然语言处理联合会议（EMNLP-IJCNLP）的会议录中，第3606–3611页。'
- en: '[Bhatt et al., 2020a] Bhatt, U., Weller, A., and Moura, J. M. (2020a). Evaluating
    and aggregating feature-based model explanations. Proceedings of the Twenty-Ninth
    International Joint Conference on Artificial Intelligence.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bhatt 等, 2020a] Bhatt, U., Weller, A., 和 Moura, J. M. (2020a). 评估和聚合基于特征的模型解释。在第二十九届国际人工智能联合会议的会议录中。'
- en: '[Bhatt et al., 2020b] Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A.,
    Jia, Y., Ghosh, J., Puri, R., Moura, J. M., and Eckersley, P. (2020b). Explainable
    machine learning in deployment. In Proceedings of the 2020 Conference on Fairness,
    Accountability, and Transparency, pages 648–657.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Bhatt 等, 2020b] Bhatt, U., Xiang, A., Sharma, S., Weller, A., Taly, A., Jia,
    Y., Ghosh, J., Puri, R., Moura, J. M., 和 Eckersley, P. (2020b). 可解释的机器学习在部署中的应用。在2020年公平性、问责制与透明度会议的会议录中，第648–657页。'
- en: '[Boshra et al., 2019] Boshra, R., Ruiter, K. I., DeMatteo, C., Reilly, J. P.,
    and Connolly, J. F. (2019). neurophysiological correlates of concussion: Deep
    learning for clinical assessment. Scientific reports, 9(1):1–10.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Boshra 等, 2019] Boshra, R., Ruiter, K. I., DeMatteo, C., Reilly, J. P., 和
    Connolly, J. F. (2019). 脑震荡的神经生理学相关性：用于临床评估的深度学习。科学报告, 9(1):1–10。'
- en: '[Brunner et al., 2019] Brunner, G., Liu, Y., Pascual, D., Richter, O., Ciaramita,
    M., and Wattenhofer, R. (2019). On identifiability in transformers. In International
    Conference on Learning Representations.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Brunner 等, 2019] Brunner, G., Liu, Y., Pascual, D., Richter, O., Ciaramita,
    M., 和 Wattenhofer, R. (2019). 关于变换器中的可识别性。在国际学习表征会议中。'
- en: '[Carlini and Wagner, 2017] Carlini, N. and Wagner, D. (2017). Towards evaluating
    the robustness of neural networks. In 2017 ieee symposium on security and privacy
    (sp), pages 39–57\. IEEE.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Carlini 和 Wagner, 2017] Carlini, N. 和 Wagner, D. (2017). 评估神经网络的鲁棒性。 在2017年
    IEEE 安全与隐私研讨会（SP）上，第39–57页。 IEEE。'
- en: '[Carter et al., 2019] Carter, S., Armstrong, Z., Schubert, L., Johnson, I.,
    and Olah, C. (2019). Activation atlas. Distill. https://distill.pub/2019/activation-atlas.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Carter 等, 2019] Carter, S., Armstrong, Z., Schubert, L., Johnson, I., 和 Olah,
    C. (2019). 激活图谱。Distill。https://distill.pub/2019/activation-atlas。'
- en: '[Caruana et al., 2015] Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M.,
    and Elhadad, N. (2015). Intelligible models for healthcare: Predicting pneumonia
    risk and hospital 30-day readmission. In Proceedings of the 21th ACM SIGKDD international
    conference on knowledge discovery and data mining, pages 1721–1730.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Caruana 等, 2015] Caruana, R., Lou, Y., Gehrke, J., Koch, P., Sturm, M., 和
    Elhadad, N. (2015). 医疗保健的可理解模型：预测肺炎风险和医院30天再入院。 在第21届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集中，第1721–1730页。'
- en: '[Chakraborty et al., 2018] Chakraborty, A., Alam, M., Dey, V., Chattopadhyay,
    A., and Mukhopadhyay, D. (2018). Adversarial attacks and defences: A survey. arXiv
    preprint arXiv:1810.00069.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chakraborty 等, 2018] Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A.,
    和 Mukhopadhyay, D. (2018). 对抗性攻击与防御：综述。arXiv 预印本 arXiv:1810.00069。'
- en: '[Chang et al., 2019] Chang, C.-H., Creager, E., Goldenberg, A., and Duvenaud,
    D. (2019). Explaining image classifiers by counterfactual generation. In International
    Conference on Learning Representations.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chang 等, 2019] Chang, C.-H., Creager, E., Goldenberg, A., 和 Duvenaud, D. (2019).
    通过反事实生成解释图像分类器。在国际学习表征会议上。'
- en: '[Chen et al., 2020] Chen, H., Huang, C., Huang, Q., Zhang, Q., and Wang, W.
    (2020). Ecgadv: Generating adversarial electrocardiogram to misguide arrhythmia
    classification system. In AAAI, pages 3446–3453.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等, 2020] Chen, H., Huang, C., Huang, Q., Zhang, Q., 和 Wang, W. (2020).
    ECGADV：生成对抗性心电图以误导心律失常分类系统。在 AAAI 上，第3446–3453页。'
- en: '[Chen et al., 2018] Chen, J., Song, L., Wainwright, M., and Jordan, M. (2018).
    Learning to explain: An information-theoretic perspective on model interpretation.
    In International Conference on Machine Learning, pages 883–892.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等, 2018] Chen, J., Song, L., Wainwright, M., 和 Jordan, M. (2018). 学习解释：从信息理论的角度看模型解释。在国际机器学习会议上，第883–892页。'
- en: '[Chen et al., 2017] Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh,
    C.-J. (2017). Zoo: Zeroth order optimization based black-box attacks to deep neural
    networks without training substitute models. In Proceedings of the 10th ACM Workshop
    on Artificial Intelligence and Security, pages 15–26.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Chen 等, 2017] Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., 和 Hsieh, C.-J. (2017).
    Zoo: 基于零阶优化的黑箱攻击深度神经网络，无需训练替代模型。在第十届 ACM 人工智能与安全研讨会论文集中，第15–26页。'
- en: '[Choi et al., 2016] Choi, E., Bahadori, M. T., Sun, J., Kulas, J., Schuetz,
    A., and Stewart, W. (2016). Retain: An interpretable predictive model for healthcare
    using reverse time attention mechanism. In Advances in Neural Information Processing
    Systems, pages 3504–3512.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Choi 等, 2016] Choi, E., Bahadori, M. T., Sun, J., Kulas, J., Schuetz, A.,
    和 Stewart, W. (2016). Retain: 一种使用逆向时间注意机制的医疗保健可解释预测模型。在神经信息处理系统的进展中，第3504–3512页。'
- en: '[Cisse et al., 2017] Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and
    Usunier, N. (2017). Parsevasl networks: Improving robustness to adversarial examples.
    In International Conference on Machine Learning, pages 854–863\. PMLR.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cisse 等, 2017] Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., 和 Usunier,
    N. (2017). Parsevasl 网络：提高对对抗样本的鲁棒性。在国际机器学习会议上，第854–863页。PMLR。'
- en: '[Clark et al., 2019] Clark, K., Khandelwal, U., Levy, O., and Manning, C. D.
    (2019). What does bert look at? an analysis of bert’s attention. BlackBoxNLP@ACL.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Clark 等, 2019] Clark, K., Khandelwal, U., Levy, O., 和 Manning, C. D. (2019).
    BERT 看了什么？对 BERT 注意力的分析。BlackBoxNLP@ACL。'
- en: '[Couteaux et al., 2019] Couteaux, V., Nempont, O., Pizaine, G., and Bloch,
    I. (2019). Towards interpretability of segmentation networks by analyzing deepdreams.
    In Interpretability of Machine Intelligence in Medical Image Computing and Multimodal
    Learning for Clinical Decision Support, pages 56–63\. Springer.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Couteaux 等, 2019] Couteaux, V., Nempont, O., Pizaine, G., 和 Bloch, I. (2019).
    通过分析 DeepDreams 实现分割网络的可解释性。在医学图像计算和临床决策支持的机器智能可解释性中，第56–63页。Springer。'
- en: '[Craven and Shavlik, 1995] Craven, M. and Shavlik, J. (1995). Extracting tree-structured
    representations of trained networks. Advances in neural information processing
    systems, 8:24–30.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Craven 和 Shavlik, 1995] Craven, M. 和 Shavlik, J. (1995). 提取训练网络的树状结构表示。神经信息处理系统的进展，8:24–30。'
- en: '[Croce and Hein, 2018] Croce, F. and Hein, M. (2018). A randomized gradient-free
    attack on relu networks. In German Conference on Pattern Recognition, pages 215–227.
    Springer.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Croce 和 Hein，2018] Croce, F. 和 Hein, M. (2018)。对 ReLU 网络的随机梯度无关攻击。见《德国模式识别会议》，第
    215–227 页。Springer。'
- en: '[Croce et al., 2019] Croce, F., Rauber, J., and Hein, M. (2019). Scaling up
    the randomized gradient-free adversarial attack reveals overestimation of robustness
    using established attacks. International Journal of Computer Vision, pages 1–19.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Croce 等，2019] Croce, F., Rauber, J., 和 Hein, M. (2019)。扩大随机梯度无关对抗攻击揭示了使用既定攻击方法对稳健性的高估。International
    Journal of Computer Vision, 第 1–19 页。'
- en: '[Dabkowski and Gal, 2017] Dabkowski, P. and Gal, Y. (2017). Real time image
    saliency for black box classifiers. In Advances in Neural Information Processing
    Systems, pages 6967–6976.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Dabkowski 和 Gal，2017] Dabkowski, P. 和 Gal, Y. (2017)。用于黑箱分类器的实时图像显著性。见《神经信息处理系统进展》，第
    6967–6976 页。'
- en: '[Doshi-Velez and Kim, 2017] Doshi-Velez, F. and Kim, B. (2017). Towards a rigorous
    science of interpretable machine learning. stat, 1050:2.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Doshi-Velez 和 Kim，2017] Doshi-Velez, F. 和 Kim, B. (2017)。朝着严谨的可解释机器学习科学迈进。stat,
    1050:2。'
- en: '[Edwards and Veale, 2018] Edwards, L. and Veale, M. (2018). Enslaving the algorithm:
    From a “right to an explanation” to a “right to better decisions”? IEEE Security
    & Privacy, 16(3):46–54.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Edwards 和 Veale，2018] Edwards, L. 和 Veale, M. (2018)。奴役算法：从“解释权”到“更好决策的权利”？IEEE
    Security & Privacy, 16(3):46–54。'
- en: '[Eitel et al., 2019] Eitel, F., Ritter, K., (ADNI, A. D. N. I., et al. (2019).
    Testing the robustness of attribution methods for convolutional neural networks
    in mri-based alzheimer’s disease classification. In Interpretability of Machine
    Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision
    Support, pages 3–11\. Springer.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Eitel 等，2019] Eitel, F., Ritter, K., (ADNI, A. D. N. I., 等，2019)。测试卷积神经网络在基于
    MRI 的阿尔茨海默病分类中的归因方法的稳健性。见《医学图像计算与临床决策支持的机器智能可解释性》，第 3–11 页。Springer。'
- en: '[Elshawi et al., 2019] Elshawi, R., Al-Mallah, M. H., and Sakr, S. (2019).
    On the interpretability of machine learning-based model for predicting hypertension.
    BMC medical informatics and decision making, 19(1):146.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Elshawi 等，2019] Elshawi, R., Al-Mallah, M. H., 和 Sakr, S. (2019)。关于基于机器学习的预测高血压模型的可解释性。BMC
    medical informatics and decision making, 19(1):146。'
- en: '[ElShawi et al., 2020] ElShawi, R., Sherif, Y., Al-Mallah, M., and Sakr, S.
    (2020). Interpretability in healthcare: A comparative study of local machine learning
    interpretability techniques. Computational Intelligence.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ElShawi 等，2020] ElShawi, R., Sherif, Y., Al-Mallah, M., 和 Sakr, S. (2020)。医疗保健中的可解释性：本地机器学习可解释性技术的比较研究。Computational
    Intelligence。'
- en: '[Esteva et al., 2017] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter,
    S. M., Blau, H. M., and Thrun, S. (2017). Dermatologist-level classification of
    skin cancer with deep neural networks. nature, 542(7639):115–118.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Esteva 等，2017] Esteva, A., Kuprel, B., Novoa, R. A., Ko, J., Swetter, S. M.,
    Blau, H. M., 和 Thrun, S. (2017)。使用深度神经网络进行皮肤癌的皮肤科医生级分类。nature, 542(7639):115–118。'
- en: '[Esteva et al., 2019] Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V.,
    DePristo, M., Chou, K., Cui, C., Corrado, G., Thrun, S., and Dean, J. (2019).
    A guide to deep learning in healthcare. Nature medicine, 25(1):24–29.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Esteva 等，2019] Esteva, A., Robicquet, A., Ramsundar, B., Kuleshov, V., DePristo,
    M., Chou, K., Cui, C., Corrado, G., Thrun, S., 和 Dean, J. (2019)。医疗保健中的深度学习指南。Nature
    medicine, 25(1):24–29。'
- en: '[Fawaz et al., 2019] Fawaz, H. I., Forestier, G., Weber, J., Idoumghar, L.,
    and Muller, P.-A. (2019). Deep learning for time series classification: a review.
    Data Mining and Knowledge Discovery, 33(4):917–963.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fawaz 等，2019] Fawaz, H. I., Forestier, G., Weber, J., Idoumghar, L., 和 Muller,
    P.-A. (2019)。时间序列分类的深度学习：综述。Data Mining and Knowledge Discovery, 33(4):917–963。'
- en: '[Feinman et al., 2017] Feinman, R., Curtin, R. R., Shintre, S., and Gardner,
    A. B. (2017). Detecting adversarial samples from artifacts. arXiv preprint arXiv:1703.00410.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Feinman 等，2017] Feinman, R., Curtin, R. R., Shintre, S., 和 Gardner, A. B.
    (2017)。从伪影中检测对抗样本。arXiv 预印本 arXiv:1703.00410。'
- en: '[Feng and Boyd-Graber, 2019] Feng, S. and Boyd-Graber, J. (2019). What can
    AI do for me? evaluating machine learning interpretations in cooperative play.
    In Proceedings of the 24th International Conference on Intelligent User Interfaces,
    pages 229–239.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Feng 和 Boyd-Graber，2019] Feng, S. 和 Boyd-Graber, J. (2019)。人工智能能为我做什么？评估合作游戏中的机器学习解释。见《第
    24 届国际智能用户界面会议论文集》，第 229–239 页。'
- en: '[Finlayson et al., 2019a] Finlayson, S. G., Bowers, J. D., Ito, J., Zittrain,
    J. L., Beam, A. L., and Kohane, I. S. (2019a). Adversarial attacks on medical
    machine learning. Science, 363(6433):1287–1289.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Finlayson et al., 2019a] Finlayson, S. G., Bowers, J. D., Ito, J., Zittrain,
    J. L., Beam, A. L., 和 Kohane, I. S. (2019a). 医疗机器学习的对抗攻击。科学，363(6433):1287–1289。'
- en: '[Finlayson et al., 2019b] Finlayson, S. G., Chung, H. W., Kohane, I. S., and
    Beam, A. L. (2019b). Adversarial attacks against medical deep learning systems.
    Science, pages 1287–1289.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Finlayson et al., 2019b] Finlayson, S. G., Chung, H. W., Kohane, I. S., 和
    Beam, A. L. (2019b). 针对医疗深度学习系统的对抗攻击。科学，页码 1287–1289。'
- en: '[Fiosina et al., 2020] Fiosina, J., Fiosins, M., and Bonn, S. (2020). Explainable
    deep learning for augmentation of small RNA expression profiles. Journal of Computational
    Biology, 27(2):234–247.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fiosina et al., 2020] Fiosina, J., Fiosins, M., 和 Bonn, S. (2020). 解释性深度学习用于小
    RNA 表达谱的增强。计算生物学杂志，27(2):234–247。'
- en: '[Fisher et al., 2019] Fisher, A., Rudin, C., and Dominici, F. (2019). All models
    are wrong, but many are useful: Learning a variable’s importance by studying an
    entire class of prediction models simultaneously. Journal of Machine Learning
    Research, 20(177):1–81.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Fisher et al., 2019] Fisher, A., Rudin, C., 和 Dominici, F. (2019). 所有模型都有误，但许多模型有用：通过同时研究整个预测模型类来学习变量的重要性。机器学习研究期刊，20(177):1–81。'
- en: '[Gao et al., 2019] Gao, J., Wang, X., Wang, Y., Yang, Z., Gao, J., Wang, J.,
    Tang, W., and Xie, X. (2019). Camp: Co-attention memory networks for diagnosis
    prediction in healthcare. In 2019 IEEE International Conference on Data Mining
    (ICDM), pages 1036–1041\. IEEE.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gao et al., 2019] Gao, J., Wang, X., Wang, Y., Yang, Z., Gao, J., Wang, J.,
    Tang, W., 和 Xie, X. (2019). Camp：用于医疗诊断预测的共同关注记忆网络。在2019年IEEE国际数据挖掘会议（ICDM），页码
    1036–1041。IEEE。'
- en: '[Gehrmann et al., 2018] Gehrmann, S., Dernoncourt, F., Li, Y., Carlson, E. T.,
    Wu, J. T., Welt, J., Foote Jr, J., Moseley, E. T., Grant, D. W., Tyler, P. D.,
    et al. (2018). Comparing deep learning and concept extraction based methods for
    patient phenotyping from clinical narratives. PloS one, 13(2):e0192360.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gehrmann et al., 2018] Gehrmann, S., Dernoncourt, F., Li, Y., Carlson, E.
    T., Wu, J. T., Welt, J., Foote Jr, J., Moseley, E. T., Grant, D. W., Tyler, P.
    D., 等 (2018). 比较基于深度学习和概念提取的方法用于临床叙事中的患者表型识别。PloS one，13(2):e0192360。'
- en: '[Gianfrancesco et al., 2018] Gianfrancesco, M. A., Tamang, S., Yazdany, J.,
    and Schmajuk, G. (2018). Potential biases in machine learning algorithms using
    electronic health record data. JAMA internal medicine, 178(11):1544–1547.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gianfrancesco et al., 2018] Gianfrancesco, M. A., Tamang, S., Yazdany, J.,
    和 Schmajuk, G. (2018). 使用电子健康记录数据的机器学习算法潜在偏见。JAMA 内科，178(11):1544–1547。'
- en: '[Goodfellow et al., 2015] Goodfellow, I. J., Shlens, J., and Szegedy, C. (2015).
    Explaining and harnessing adversarial examples. ICLR.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Goodfellow et al., 2015] Goodfellow, I. J., Shlens, J., 和 Szegedy, C. (2015).
    解释和利用对抗样本。ICLR。'
- en: '[Grosse et al., 2017] Grosse, K., Manoharan, P., Papernot, N., Backes, M.,
    and McDaniel, P. (2017). On the (statistical) detection of adversarial examples.
    arXiv preprint arXiv:1702.06280.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Grosse et al., 2017] Grosse, K., Manoharan, P., Papernot, N., Backes, M.,
    和 McDaniel, P. (2017). 关于（统计）对抗样本的检测。arXiv 预印本 arXiv:1702.06280。'
- en: '[Gu and Rigazio, 2014] Gu, S. and Rigazio, L. (2014). Towards deep neural network
    architectures robust to adversarial examples. arXiv preprint arXiv:1412.5068.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Gu and Rigazio, 2014] Gu, S. 和 Rigazio, L. (2014). 朝着对抗样本鲁棒的深度神经网络架构发展。arXiv
    预印本 arXiv:1412.5068。'
- en: '[Hajian et al., 2016] Hajian, S., Bonchi, F., and Castillo, C. (2016). Algorithmic
    bias: From discrimination discovery to fairness-aware data mining. In Proceedings
    of the 22nd ACM SIGKDD international conference on knowledge discovery and data
    mining, pages 2125–2126.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hajian et al., 2016] Hajian, S., Bonchi, F., 和 Castillo, C. (2016). 算法偏见：从歧视发现到公平感知数据挖掘。在第22届ACM
    SIGKDD国际知识发现与数据挖掘会议论文集，页码 2125–2126。'
- en: '[Hall et al., 2017] Hall, P., Gill, N., Kurka, M., and Phan, W. (2017). Machine
    learning interpretability with H2O driverless ai. H2O. ai. URL: http://docs. h2o.
    ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet. pdf.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hall et al., 2017] Hall, P., Gill, N., Kurka, M., 和 Phan, W. (2017). 使用 H2O
    无驱动 AI 进行机器学习解释性。H2O.ai。网址：http://docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/MLIBooklet.pdf。'
- en: '[Han et al., 2020a] Han, X., Hu, Y., Foschini, L., Chinitz, L., Jankelson,
    L., and Ranganath, R. (2020a). Deep learning models for electrocardiograms are
    susceptible to adversarial attack. Nature Medicine, pages 1–4.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Han et al., 2020a] Han, X., Hu, Y., Foschini, L., Chinitz, L., Jankelson,
    L., 和 Ranganath, R. (2020a). 深度学习模型对心电图的对抗攻击敏感。自然医学，页码 1–4。'
- en: '[Han et al., 2020b] Han, X., Wallace, B. C., and Tsvetkov, Y. (2020b). Explaining
    black box predictions and unveiling data artifacts through influence functions.
    In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pages 5553–5563, Online. Association for Computational Linguistics.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Han 等, 2020b] Han, X., Wallace, B. C., 和 Tsvetkov, Y. (2020b). 通过影响函数解释黑箱预测并揭示数据伪影。在第58届计算语言学协会年会论文集中，页面
    5553–5563，在线。计算语言学协会。'
- en: '[Hendriks et al., 2020] Hendriks, M. P., Ten Teije, A., and Moncada-Torres,
    A. (2020). Machine learning explainability in breast cancer survival. Digital
    Personalized Health and Medicine: Proceedings of MIE 2020, 270:307.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hendriks 等, 2020] Hendriks, M. P., Ten Teije, A., 和 Moncada-Torres, A. (2020).
    乳腺癌生存中的机器学习可解释性。数字个性化健康与医学：MIE 2020 论文集，270:307。'
- en: '[Heo et al., 2018] Heo, J., Lee, H. B., Kim, S., Lee, J., Kim, K. J., Yang,
    E., and Hwang, S. J. (2018). Uncertainty-aware attention for reliable interpretation
    and prediction. In Advances in neural information processing systems, pages 909–918.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Heo 等, 2018] Heo, J., Lee, H. B., Kim, S., Lee, J., Kim, K. J., Yang, E.,
    和 Hwang, S. J. (2018). 具有不确定性感知的注意力，用于可靠的解释和预测。在神经信息处理系统进展中，页面 909–918。'
- en: '[Hinton et al., 2015] Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling
    the knowledge in a neural network. stat, 1050:9.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hinton 等, 2015] Hinton, G., Vinyals, O., 和 Dean, J. (2015). 提炼神经网络中的知识。stat,
    1050:9。'
- en: '[Hirano et al., 2021] Hirano, H., Minagi, A., and Takemoto, K. (2021). Universal
    adversarial attacks on deep neural networks for medical image classification.
    BMC medical imaging, 21(1):1–13.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hirano 等, 2021] Hirano, H., Minagi, A., 和 Takemoto, K. (2021). 对深度神经网络进行通用对抗性攻击，以进行医学图像分类。BMC
    医学影像学, 21(1):1–13。'
- en: '[Holzinger et al., 2019] Holzinger, A., Langs, G., Denk, H., Zatloukal, K.,
    and Müller, H. (2019). Causability and explainability of artificial intelligence
    in medicine. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,
    9(4):e1312.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Holzinger 等, 2019] Holzinger, A., Langs, G., Denk, H., Zatloukal, K., 和 Müller,
    H. (2019). 医学中人工智能的因果性和可解释性。Wiley 跨学科评论：数据挖掘与知识发现, 9(4):e1312。'
- en: '[Hooker et al., 2018] Hooker, S., Erhan, D., Kindermans, P.-J., and Kim, B.
    (2018). Evaluating feature importance estimates. GoogleResearch.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hooker 等, 2018] Hooker, S., Erhan, D., Kindermans, P.-J., 和 Kim, B. (2018).
    评估特征重要性估计。GoogleResearch。'
- en: '[Hu et al., 2018] Hu, L., Chen, J. J., Nair, V., and Sudjianto, A. (2018).
    Locally interpretable models and effects based on supervised partitioning (LIME-SUP).
    ArXiv, abs/1806.00663.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Hu 等, 2018] Hu, L., Chen, J. J., Nair, V., 和 Sudjianto, A. (2018). 基于监督分区的局部可解释模型和效果（LIME-SUP）。ArXiv,
    abs/1806.00663。'
- en: '[Huang et al., 2020] Huang, Q., Yamada, M., Tian, Y., Singh, D., Yin, D., and
    Chang, Y. (2020). Graphlime: Local interpretable model explanations for graph
    neural networks. arXiv preprint arXiv:2001.06216.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Huang 等, 2020] Huang, Q., Yamada, M., Tian, Y., Singh, D., Yin, D., 和 Chang,
    Y. (2020). Graphlime：图神经网络的局部可解释模型解释。arXiv 预印本 arXiv:2001.06216。'
- en: '[Iqtidar Newaz et al., 2020] Iqtidar Newaz, A., Imtiazul Haque, N., Sikder,
    A. K., Ashiqur Rahman, M., and Selcuk Uluagac, A. (2020). Adversarial attacks
    to machine learning-based smart healthcare systems. In Proceedings of the IEEE
    Global Communications Conference.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Iqtidar Newaz 等, 2020] Iqtidar Newaz, A., Imtiazul Haque, N., Sikder, A. K.,
    Ashiqur Rahman, M., 和 Selcuk Uluagac, A. (2020). 对基于机器学习的智能医疗系统的对抗性攻击。发表于 IEEE
    全球通信会议论文集。'
- en: '[Ismail et al., 2020] Ismail, A. A., Gunady, M., Bravo, H. C., and Feizi, S.
    (2020). Benchmarking deep learning interpretability in time series predictions.
    Advances in Neural Information Processing Systems 33.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ismail 等, 2020] Ismail, A. A., Gunady, M., Bravo, H. C., 和 Feizi, S. (2020).
    时间序列预测中的深度学习可解释性基准测试。神经信息处理系统进展 33。'
- en: '[Jacovi and Goldberg, 2020] Jacovi, A. and Goldberg, Y. (2020). Towards faithfully
    interpretable NLP systems: How should we define and evaluate faithfulness? In
    Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,
    pages 4198–4205, Online. Association for Computational Linguistics.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jacovi 和 Goldberg, 2020] Jacovi, A. 和 Goldberg, Y. (2020). 朝着真实可解释的自然语言处理系统迈进：我们应该如何定义和评估真实度？在第58届计算语言学协会年会论文集中，页面
    4198–4205，在线。计算语言学协会。'
- en: '[Jain and Wallace, 2019] Jain, S. and Wallace, B. C. (2019). Attention is not
    explanation. In Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers), pages 3543–3556.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jain 和 Wallace, 2019] Jain, S. 和 Wallace, B. C. (2019). 注意力不是解释。在第2019年北美计算语言学协会年会：人类语言技术会议论文集中，第1卷（长短论文），页面
    3543–3556。'
- en: '[Janzing et al., 2020] Janzing, D., Minorics, L., and Blöbaum, P. (2020). Feature
    relevance quantification in explainable ai: A causal problem. In International
    Conference on Artificial Intelligence and Statistics, pages 2907–2916\. PMLR.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Janzing 等, 2020] Janzing, D., Minorics, L., 和 Blöbaum, P. (2020). 可解释人工智能中的特征相关性量化：一个因果问题。见于国际人工智能与统计会议，页码
    2907–2916。PMLR。'
- en: '[Jin et al., 2020] Jin, D., Jin, Z., Zhou, J. T., and Szolovits, P. (2020).
    Is BERT really robust? a strong baseline for natural language attack on text classification
    and entailment. In AAAI.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Jin 等, 2020] Jin, D., Jin, Z., Zhou, J. T., 和 Szolovits, P. (2020). BERT真的健壮吗？文本分类和推理的自然语言攻击的强基线。见于AAAI。'
- en: '[Kahneman, 2011] Kahneman, D. (2011). Thinking, fast and slow. Macmillan.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kahneman, 2011] Kahneman, D. (2011). 《思考，快与慢》。Macmillan。'
- en: '[Kaji et al., 2019] Kaji, D. A., Zech, J. R., Kim, J. S., Cho, S. K., Dangayach,
    N. S., Costa, A. B., and Oermann, E. K. (2019). An attention based deep learning
    model of clinical events in the intensive care unit. PloS one, 14(2):e0211057.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaji 等, 2019] Kaji, D. A., Zech, J. R., Kim, J. S., Cho, S. K., Dangayach,
    N. S., Costa, A. B., 和 Oermann, E. K. (2019). 一种基于注意力的深度学习模型，用于重症监护室的临床事件。PloS
    one, 14(2):e0211057。'
- en: '[Kaur et al., 2020] Kaur, H., Nori, H., Jenkins, S., Caruana, R., Wallach,
    H., and Wortman Vaughan, J. (2020). Interpreting interpretability: Understanding
    data scientists’ use of interpretability tools for machine learning. In Proceedings
    of the 2020 CHI Conference on Human Factors in Computing Systems, pages 1–14.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kaur 等, 2020] Kaur, H., Nori, H., Jenkins, S., Caruana, R., Wallach, H., 和
    Wortman Vaughan, J. (2020). 解读可解释性：理解数据科学家如何使用可解释性工具进行机器学习。见于2020年CHI计算机系统人因会议论文集，页码
    1–14。'
- en: '[Khedkar et al., 2020] Khedkar, S., Gandhi, P., Shinde, G., and Subramanian,
    V. (2020). Deep learning and explainable ai in healthcare using ehr. In Deep Learning
    Techniques for Biomedical and Health Informatics, pages 129–148\. Springer.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Khedkar 等, 2020] Khedkar, S., Gandhi, P., Shinde, G., 和 Subramanian, V. (2020).
    使用电子健康记录的深度学习与可解释人工智能在医疗保健中的应用。见于《生物医学与健康信息学中的深度学习技术》，页码 129–148。Springer。'
- en: '[Kim et al., 2016] Kim, B., Khanna, R., and Koyejo, O. O. (2016). Examples
    are not enough, learn to criticize! criticism for interpretability. In Advances
    in neural information processing systems, pages 2280–2288.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kim 等, 2016] Kim, B., Khanna, R., 和 Koyejo, O. O. (2016). 示例是不够的，学会批评！批评以提高可解释性。见于《神经信息处理系统的进展》，页码
    2280–2288。'
- en: '[Kim et al., 2018] Kim, S. T., Lee, J.-H., Lee, H., and Ro, Y. M. (2018). Visually
    interpretable deep network for diagnosis of breast masses on mammograms. Physics
    in Medicine &amp; Biology, 63(23):235025.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kim 等, 2018] Kim, S. T., Lee, J.-H., Lee, H., 和 Ro, Y. M. (2018). 可视化的深度网络用于乳腺X光片上乳腺肿块的诊断。医学与生物学物理学，63(23):235025。'
- en: '[Knaus et al., 1985] Knaus, W. A., Draper, E. A., Wagner, D. P., and Zimmerman,
    J. E. (1985). Apache ii: a severity of disease classification system. Critical
    care medicine, 13(10):818–829.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Knaus 等, 1985] Knaus, W. A., Draper, E. A., Wagner, D. P., 和 Zimmerman, J.
    E. (1985). Apache ii：一种疾病严重程度分类系统。重症监护医学，13(10):818–829。'
- en: '[Koh and Liang, 2017] Koh, P. W. and Liang, P. (2017). Understanding black-box
    predictions via influence functions. In International Conference on Machine Learning,
    pages 1885–1894.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Koh 和 Liang, 2017] Koh, P. W. 和 Liang, P. (2017). 通过影响函数理解黑箱预测。见于国际机器学习会议，页码
    1885–1894。'
- en: '[Kovalerchuk et al., 2021] Kovalerchuk, B., Ahmad, M. A., and Teredesai, A.
    (2021). Survey of explainable machine learning with visual and granular methods
    beyond quasi-explanations. Interpretable Artificial Intelligence: A Perspective
    of Granular Computing (Eds. W. Pedrycz, SM Chen), Springer, pages 217–267.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kovalerchuk 等, 2021] Kovalerchuk, B., Ahmad, M. A., 和 Teredesai, A. (2021).
    超越准解释的可视化和细粒度方法的可解释机器学习调查。《可解释的人工智能：细粒度计算的视角》（编辑 W. Pedrycz, SM Chen），Springer，页码
    217–267。'
- en: '[Kovalev et al., 2020] Kovalev, M. S., Utkin, L. V., and Kasimov, E. M. (2020).
    An explanation method for black-box machine learning survival models using the
    chebyshev distance. AINL 2020: Artificial Intelligence and Natural Language.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Kovalev 等, 2020] Kovalev, M. S., Utkin, L. V., 和 Kasimov, E. M. (2020). 使用切比雪夫距离的黑箱机器学习生存模型解释方法。AINL
    2020：人工智能与自然语言。'
- en: '[Lahav et al., 2018] Lahav, O., Mastronarde, N., and van der Schaar, M. (2018).
    What is interpretable? using machine learning to design interpretable decision-support
    systems. arXiv preprint arXiv:1811.10799.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lahav 等, 2018] Lahav, O., Mastronarde, N., 和 van der Schaar, M. (2018). 什么是可解释的？利用机器学习设计可解释的决策支持系统。arXiv
    预印本 arXiv:1811.10799。'
- en: '[Lakkaraju et al., 2017] Lakkaraju, H., Kamar, E., Caruana, R., and Leskovec,
    J. (2017). Interpretable & explorable approximations of black box models. 2017
    Workshop on Fairness, Accountability, and Transparency in Machine Learning.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lakkaraju 等，2017] Lakkaraju, H., Kamar, E., Caruana, R., 和 Leskovec, J. (2017).
    可解释和可探索的黑箱模型近似。2017 年公平性、问责制和透明度在机器学习中的研讨会。'
- en: '[Lakkaraju et al., 2019] Lakkaraju, H., Kamar, E., Caruana, R., and Leskovec,
    J. (2019). Faithful and customizable explanations of black box models. In Proceedings
    of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 131–138.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lakkaraju 等，2019] Lakkaraju, H., Kamar, E., Caruana, R., 和 Leskovec, J. (2019).
    对黑箱模型的忠实和可定制的解释。在 2019 年 AAAI/ACM 人工智能、伦理与社会会议论文集中，页码 131–138。'
- en: '[LeCun et al., 2015] LeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.
    nature, 521(7553):436–444.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[LeCun 等，2015] LeCun, Y., Bengio, Y., 和 Hinton, G. (2015). 深度学习。自然，521(7553):436–444。'
- en: '[Lee et al., 2020] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H.,
    and Kang, J. (2020). Biobert: a pre-trained biomedical language representation
    model for biomedical text mining. Bioinformatics, 36(4):1234–1240.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lee 等，2020] Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., 和 Kang,
    J. (2020). Biobert: 用于生物医学文本挖掘的预训练生物医学语言表示模型。生物信息学，36(4):1234–1240。'
- en: '[Levy et al., 2019] Levy, J., Salas, L. A., Christensen, B. C., Sriharan, A.,
    and Vaickus, L. J. (2019). Pathflowai: A high-throughput workflow for preprocessing,
    deep learning and interpretation in digital pathology. medRxiv, page 19003897.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Levy 等，2019] Levy, J., Salas, L. A., Christensen, B. C., Sriharan, A., 和 Vaickus,
    L. J. (2019). Pathflowai: 高通量数字病理学预处理、深度学习和解释工作流。medRxiv, 页码 19003897。'
- en: '[Li et al., 2016] Li, J., Monroe, W., and Jurafsky, D. (2016). Understanding
    neural networks through representation erasure. arXiv preprint arXiv:1612.08220.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等，2016] Li, J., Monroe, W., 和 Jurafsky, D. (2016). 通过表示擦除理解神经网络。arXiv 预印本
    arXiv:1612.08220。'
- en: '[Li et al., 2020a] Li, R., Shinde, A., Liu, A., Glaser, S., Lyou, Y., Yuh,
    B., Wong, J., and Amini, A. (2020a). Machine learning–based interpretation and
    visualization of nonlinear interactions in prostate cancer survival. JCO Clinical
    Cancer Informatics, 4:637–646.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等，2020a] Li, R., Shinde, A., Liu, A., Glaser, S., Lyou, Y., Yuh, B., Wong,
    J., 和 Amini, A. (2020a). 基于机器学习的前列腺癌生存的非线性交互解释和可视化。JCO 临床癌症信息学，4:637–646。'
- en: '[Li et al., 2018] Li, X., Zhu, D., and Levy, P. (2018). Leveraging auxiliary
    measures: a deep multi-task neural network for predictive modeling in clinical
    research. BMC medical informatics and decision making, 18(4):45–53.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等，2018] Li, X., Zhu, D., 和 Levy, P. (2018). 利用辅助测量：用于临床研究预测建模的深度多任务神经网络。BMC
    医学信息学与决策制定，18(4):45–53。'
- en: '[Li et al., 2020b] Li, Y., Rao, S., Solares, J. R. A., Hassaine, A., Ramakrishnan,
    R., Canoy, D., Zhu, Y., Rahimi, K., and Salimi-Khorshidi, G. (2020b). Behrt: transformer
    for electronic health records. Scientific Reports, 10(1):1–12.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Li 等，2020b] Li, Y., Rao, S., Solares, J. R. A., Hassaine, A., Ramakrishnan,
    R., Canoy, D., Zhu, Y., Rahimi, K., 和 Salimi-Khorshidi, G. (2020b). Behrt: 用于电子健康记录的变换器。Scientific
    Reports, 10(1):1–12。'
- en: '[Linda, 2020] Linda, W. (2020). A tailored deep convolutional neural network
    design for detection of covid-19 cases from chest radiography images. Journal
    of Network and Computer Applications.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Linda, 2020] Linda, W. (2020). 针对从胸部 X 射线图像中检测 covid-19 病例的深度卷积神经网络设计。网络与计算机应用期刊。'
- en: '[Lipton, 2018] Lipton, Z. C. (2018). The mythos of model interpretability.
    Queue, 16(3):31–57.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lipton, 2018] Lipton, Z. C. (2018). 模型可解释性的神话。Queue, 16(3):31–57。'
- en: '[Liu et al., 2019] Liu, H., Yin, Q., and Wang, W. Y. (2019). Towards explainable
    nlp: A generative explanation framework for text classification. In Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics, pages
    5570–5581.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Liu 等，2019] Liu, H., Yin, Q., 和 Wang, W. Y. (2019). 朝着可解释的自然语言处理：用于文本分类的生成解释框架。在第
    57 届计算语言学协会年会论文集中，页码 5570–5581。'
- en: '[Lu et al., 2017] Lu, J., Issaranon, T., and Forsyth, D. (2017). Safetynet:
    Detecting and rejecting adversarial examples robustly. In Proceedings of the IEEE
    International Conference on Computer Vision, pages 446–454.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lu 等，2017] Lu, J., Issaranon, T., 和 Forsyth, D. (2017). Safetynet: 可靠地检测和拒绝对抗样本。在
    IEEE 国际计算机视觉会议论文集中，页码 446–454。'
- en: '[Lundberg et al., 2018a] Lundberg, S. M., Erion, G. G., and Lee, S.-I. (2018a).
    Consistent individualized feature attribution for tree ensembles. International
    Conference on MachineLearning.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lundberg 等，2018a] Lundberg, S. M., Erion, G. G., 和 Lee, S.-I. (2018a). 树集成的一致个性化特征归因。国际机器学习会议。'
- en: '[Lundberg and Lee, 2017] Lundberg, S. M. and Lee, S.-I. (2017). A unified approach
    to interpreting model predictions. In Advances in neural information processing
    systems, pages 4765–4774.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lundberg 和 Lee, 2017] Lundberg, S. M. 和 Lee, S.-I. (2017). 统一的模型预测解释方法。收录于神经信息处理系统进展，第4765–4774页。'
- en: '[Lundberg et al., 2018b] Lundberg, S. M., Nair, B., Vavilala, M. S., Horibe,
    M., Eisses, M. J., Adams, T., Liston, D. E., Low, D. K.-W., Newman, S.-F., Kim,
    J., et al. (2018b). Explainable machine-learning predictions for the prevention
    of hypoxaemia during surgery. Nature biomedical engineering, 2(10):749–760.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Lundberg 等，2018b] Lundberg, S. M., Nair, B., Vavilala, M. S., Horibe, M.,
    Eisses, M. J., Adams, T., Liston, D. E., Low, D. K.-W., Newman, S.-F., Kim, J.,
    等. (2018b). 可解释的机器学习预测用于防止手术中的低氧血症。自然生物医学工程，2(10):749–760。'
- en: '[Ma et al., 2017] Ma, F., Chitta, R., Zhou, J., You, Q., Sun, T., and Gao,
    J. (2017). Dipole: Diagnosis prediction in healthcare via attention-based bidirectional
    recurrent neural networks. In Proceedings of the 23rd ACM SIGKDD international
    conference on knowledge discovery and data mining, pages 1903–1911.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ma 等，2017] Ma, F., Chitta, R., Zhou, J., You, Q., Sun, T., 和 Gao, J. (2017).
    Dipole：通过基于注意力的双向递归神经网络进行医疗诊断预测。收录于第23届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集，第1903–1911页。'
- en: '[Ma et al., 2021] Ma, X., Niu, Y., Gu, L., Wang, Y., Zhao, Y., Bailey, J.,
    and Lu, F. (2021). Understanding adversarial attacks on deep learning based medical
    image analysis systems. Pattern Recognition, 110:107332.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ma 等，2021] Ma, X., Niu, Y., Gu, L., Wang, Y., Zhao, Y., Bailey, J., 和 Lu,
    F. (2021). 理解对深度学习医学图像分析系统的对抗攻击。模式识别，110:107332。'
- en: '[Madry et al., 2018] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
    Vladu, A. (2018). Towards deep learning models resistant to adversarial attacks.
    ICLR.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Madry 等，2018] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., 和 Vladu, A.
    (2018). 朝着对抗攻击鲁棒的深度学习模型迈进。ICLR。'
- en: '[Madumal et al., 2020] Madumal, P., Miller, T., Sonenberg, L., and Vetere,
    F. (2020). Explainable reinforcement learning through a causal lens. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 34, pages 2493–2500.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Madumal 等，2020] Madumal, P., Miller, T., Sonenberg, L., 和 Vetere, F. (2020).
    通过因果视角解释可解释的强化学习。收录于 AAAI 人工智能会议论文集，第34卷，第2493–2500页。'
- en: '[Mayampurath et al., 2019] Mayampurath, A., Sanchez-Pinto, L. N., Carey, K. A.,
    Venable, L.-R., and Churpek, M. (2019). Combining patient visual timelines with
    deep learning to predict mortality. PloS one, 14(7):e0220640.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mayampurath 等，2019] Mayampurath, A., Sanchez-Pinto, L. N., Carey, K. A., Venable,
    L.-R., 和 Churpek, M. (2019). 将患者视觉时间线与深度学习结合以预测死亡率。PloS one, 14(7):e0220640。'
- en: '[Mesko, 2017] Mesko, B. (2017). The role of artificial intelligence in precision
    medicine.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mesko, 2017] Mesko, B. (2017). 人工智能在精准医学中的作用。'
- en: '[Michie, 1988] Michie, D. (1988). Machine learning in the next five years.
    In Proceedings of the 3rd European Conference on European Working Session on Learning,
    pages 107–122.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Michie, 1988] Michie, D. (1988). 未来五年中的机器学习。收录于第3届欧洲机器学习工作会议论文集，第107–122页。'
- en: '[Mnih et al., 2016] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
    T., Harley, T., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for
    deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937\. PMLR.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mnih 等，2016] Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap, T.,
    Harley, T., Silver, D., 和 Kavukcuoglu, K. (2016). 用于深度强化学习的异步方法。收录于国际机器学习会议论文集，第1928–1937页。PMLR。'
- en: '[Mnih et al., 2013] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement
    learning. arXiv preprint arXiv:1312.5602.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mnih 等，2013] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,
    I., Wierstra, D., 和 Riedmiller, M. (2013). 使用深度强化学习玩 Atari 游戏。arXiv 预印本 arXiv:1312.5602。'
- en: '[Moreira et al., 2020] Moreira, C., Sindhgatta, R., Ouyang, C., Bruza, P.,
    and Wichert, A. (2020). An investigation of interpretability techniques for deep
    learning in predictive process analytics. arXiv preprint arXiv:2002.09192.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Moreira 等，2020] Moreira, C., Sindhgatta, R., Ouyang, C., Bruza, P., 和 Wichert,
    A. (2020). 对预测过程分析中的深度学习可解释性技术的调查。arXiv 预印本 arXiv:2002.09192。'
- en: '[Mozaffari-Kermani et al., 2014] Mozaffari-Kermani, M., Sur-Kolay, S., Raghunathan,
    A., and Jha, N. K. (2014). Systematic poisoning attacks on and defenses for machine
    learning in healthcare. IEEE journal of biomedical and health informatics, 19(6):1893–1905.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Mozaffari-Kermani 等，2014] Mozaffari-Kermani, M., Sur-Kolay, S., Raghunathan,
    A., 和 Jha, N. K. (2014). 对医疗保健中机器学习的系统性毒化攻击和防御。IEEE 生物医学与健康信息学期刊，19(6):1893–1905。'
- en: '[Muggleton et al., 2018] Muggleton, S. H., Schmid, U., Zeller, C., Tamaddoni-Nezhad,
    A., and Besold, T. (2018). Ultra-strong machine learning: comprehensibility of
    programs learned with ilp. Machine Learning, 107(7):1119–1140.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Mullenbach et al., 2018] Mullenbach, J., Wiegreffe, S., Duke, J., Sun, J.,
    and Eisenstein, J. (2018). Explainable prediction of medical codes from clinical
    text. In Proceedings of the 2018 Conference of the North American Chapter of the
    Association for Computational Linguistics: Human Language Technologies, Volume
    1 (Long Papers), pages 1101–1111.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Murdoch and Detsky, 2013] Murdoch, T. B. and Detsky, A. S. (2013). The inevitable
    application of big data to health care. Jama, 309(13):1351–1352.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Murdoch et al., 2018] Murdoch, W. J., Liu, P. J., and Yu, B. (2018). Beyond
    word importance: Contextual decomposition to extract interactions from lstms.
    In International Conference on Learning Representations.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Naseer et al., 2019] Naseer, M. M., Khan, S. H., Khan, M. H., Shahbaz Khan,
    F., and Porikli, F. (2019). Cross-domain transferability of adversarial perturbations.
    Advances in Neural Information Processing Systems, 32:12905–12915.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Papanastasopoulos et al., 2020] Papanastasopoulos, Z., Samala, R. K., Chan,
    H.-P., Hadjiiski, L., Paramagul, C., Helvie, M. A., and Neal, C. H. (2020). Explainable
    ai for medical imaging: deep-learning cnn ensemble for classification of estrogen
    receptor status from breast mri. In Medical Imaging 2020: Computer-Aided Diagnosis,
    volume 11314, page 113140Z. International Society for Optics and Photonics.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Papernot et al., 2016a] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M.,
    Celik, Z. B., and Swami, A. (2016a). The limitations of deep learning in adversarial
    settings. In 2016 IEEE European symposium on security and privacy (EuroS&P), pages
    372–387\. IEEE.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Papernot et al., 2016b] Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
    A. (2016b). Distillation as a defense to adversarial perturbations against deep
    neural networks. In 2016 IEEE symposium on security and privacy (SP), pages 582–597\.
    IEEE.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Payrovnaziri et al., 2020] Payrovnaziri, S. N., Chen, Z., Rengifo-Moreno,
    P., Miller, T., Bian, J., Chen, J. H., Liu, X., and He, Z. (2020). Explainable
    artificial intelligence models using real-world electronic health record data:
    a systematic scoping review. Journal of the American Medical Informatics Association.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pedreschi et al., 2019] Pedreschi, D., Giannotti, F., Guidotti, R., Monreale,
    A., Ruggieri, S., and Turini, F. (2019). Meaningful explanations of black box
    ai decision systems. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 33, pages 9780–9784.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pereira et al., 2018] Pereira, S., Meier, R., Alves, V., Reyes, M., and Silva,
    C. A. (2018). Automatic brain tumor grading from mri data using convolutional
    neural networks and quality assessment. In Understanding and interpreting machine
    learning in medical image computing applications, pages 106–114\. Springer.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Pianpanit et al., 2019] Pianpanit, T., Lolak, S., Sawangjai, P., Ditthapron,
    A., Leelaarporn, P., Marukatat, S., Chuangsuwanich, E., and Wilaiprasitporn, T.
    (2019). Neural network interpretation of the parkinson’s disease diagnosis from
    spect imaging. IEEE Transactions and Journals.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Pianpanit et al., 2019] Pianpanit, T., Lolak, S., Sawangjai, P., Ditthapron,
    A., Leelaarporn, P., Marukatat, S., Chuangsuwanich, E., 和 Wilaiprasitporn, T.
    (2019). 从光谱成像中解析帕金森病的神经网络解释。IEEE Transactions and Journals.'
- en: '[Plumb et al., 2018] Plumb, G., Molitor, D., and Talwalkar, A. S. (2018). Model
    agnostic supervised local explanations. Advances in Neural Information Processing
    Systems, 31:2515–2524.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Plumb et al., 2018] Plumb, G., Molitor, D., 和 Talwalkar, A. S. (2018). 模型无关的监督本地解释。神经信息处理系统进展，31:2515–2524。'
- en: '[Rajan et al., 2017] Rajan, D., Song, H., Spanias, A., and Thiagarajan, J.
    (2017). Attend and diagnose: Clinical time series analysis using attention models.
    Technical report, Lawrence Livermore National Lab.(LLNL), Livermore, CA (United
    States).'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rajan et al., 2017] Rajan, D., Song, H., Spanias, A., 和 Thiagarajan, J. (2017).
    注意并诊断：使用注意力模型的临床时间序列分析。技术报告，劳伦斯利弗莫尔国家实验室（LLNL），加利福尼亚州利弗莫尔（美国）。'
- en: '[Rajani et al., 2019] Rajani, N. F., McCann, B., Xiong, C., and Socher, R.
    (2019). Explain yourself! leveraging language models for commonsense reasoning.
    In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pages 4932–4942.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rajani et al., 2019] Rajani, N. F., McCann, B., Xiong, C., 和 Socher, R. (2019).
    解释一下你自己！利用语言模型进行常识推理。发表于第57届计算语言学协会年会论文集，页码 4932–4942。'
- en: '[Ramamurthy et al., 2020] Ramamurthy, K. N., Vinzamuri, B., Zhang, Y., and
    Dhurandhar, A. (2020). Model agnostic multilevel explanations. Advances in Neural
    Information Processing Systems, 33.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ramamurthy et al., 2020] Ramamurthy, K. N., Vinzamuri, B., Zhang, Y., 和 Dhurandhar,
    A. (2020). 模型无关的多层次解释。神经信息处理系统进展，33。'
- en: '[Ran et al., 2020] Ran, A. R., Tham, C. C., Chan, P. P., Cheng, C.-Y., Tham,
    Y.-C., Rim, T. H., and Cheung, C. Y. (2020). Deep learning in glaucoma with optical
    coherence tomography: a review. Eye, pages 1–14.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ran et al., 2020] Ran, A. R., Tham, C. C., Chan, P. P., Cheng, C.-Y., Tham,
    Y.-C., Rim, T. H., 和 Cheung, C. Y. (2020). 使用光学相干断层扫描的青光眼深度学习：综述。Eye，页码 1–14。'
- en: '[Ribeiro et al., 2016a] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016a).
    ” why should i trust you?” explaining the predictions of any classifier. In Proceedings
    of the 22nd ACM SIGKDD international conference on knowledge discovery and data
    mining, pages 1135–1144.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ribeiro et al., 2016a] Ribeiro, M. T., Singh, S., 和 Guestrin, C. (2016a).
    “我为什么要相信你？” 解释任何分类器的预测。发表于第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集，页码 1135–1144。'
- en: '[Ribeiro et al., 2016b] Ribeiro, M. T., Singh, S., and Guestrin, C. (2016b).
    Model-agnostic interpretability of machine learning. 2016 ICML Workshop on Human
    Interpretability in Machine Learning.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ribeiro et al., 2016b] Ribeiro, M. T., Singh, S., 和 Guestrin, C. (2016b).
    机器学习的模型无关解释性。2016 ICML 人工智能可解释性研讨会。'
- en: '[Ribeiro et al., 2018] Ribeiro, M. T., Singh, S., and Guestrin, C. (2018).
    Anchors: High-precision model-agnostic explanations. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 32.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ribeiro et al., 2018] Ribeiro, M. T., Singh, S., 和 Guestrin, C. (2018). Anchors:
    高精度的模型无关解释。在人工智能的AAAI会议论文集中，第32卷。'
- en: '[Rieger et al., 2020] Rieger, L., Singh, C., Murdoch, W., and Yu, B. (2020).
    Interpretations are useful: penalizing explanations to align neural networks with
    prior knowledge. In International Conference on Machine Learning, pages 8116–8126\.
    PMLR.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rieger et al., 2020] Rieger, L., Singh, C., Murdoch, W., 和 Yu, B. (2020).
    解释是有用的：惩罚解释以将神经网络与先验知识对齐。在国际机器学习会议论文集，页码 8116–8126。PMLR。'
- en: '[Robnik-Šikonja and Kononenko, 2008] Robnik-Šikonja, M. and Kononenko, I. (2008).
    Explaining classifications for individual instances. IEEE Transactions on Knowledge
    and Data Engineering, 20(5):589–600.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Robnik-Šikonja and Kononenko, 2008] Robnik-Šikonja, M. 和 Kononenko, I. (2008).
    为个体实例解释分类。IEEE知识与数据工程学报，20(5):589–600。'
- en: '[Rough et al., 2020] Rough, K., Dai, A. M., Zhang, K., Xue, Y., Vardoulakis,
    L. M., Cui, C., Butte, A. J., Howell, M. D., and Rajkomar, A. (2020). Predicting
    inpatient medication orders from electronic health record data. Clinical Pharmacology
    & Therapeutics.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rough et al., 2020] Rough, K., Dai, A. M., Zhang, K., Xue, Y., Vardoulakis,
    L. M., Cui, C., Butte, A. J., Howell, M. D., 和 Rajkomar, A. (2020). 从电子健康记录数据中预测住院药物订单。临床药理学与治疗学。'
- en: '[Rudin, 2019] Rudin, C. (2019). Stop explaining black box machine learning
    models for high stakes decisions and use interpretable models instead. Nature
    Machine Intelligence, 1(5):206–215.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Rudin, 2019] Rudin, C. (2019). 停止解释用于高风险决策的黑箱机器学习模型，而改用可解释的模型。自然机器智能，1(5):206–215。'
- en: '[Sayres et al., 2019] Sayres, R., Taly, A., Rahimy, E., Blumer, K., Coz, D.,
    Hammel, N., Krause, J., Narayanaswamy, A., Rastegar, Z., Wu, D., et al. (2019).
    Using a deep learning algorithm and integrated gradients explanation to assist
    grading for diabetic retinopathy. Ophthalmology, 126(4):552–564.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sayres et al., 2019] Sayres, R., Taly, A., Rahimy, E., Blumer, K., Coz, D.,
    Hammel, N., Krause, J., Narayanaswamy, A., Rastegar, Z., Wu, D., 等 (2019). 利用深度学习算法和集成梯度解释来辅助糖尿病视网膜病变的评分。眼科学，126(4):552–564。'
- en: '[Schmitz et al., 1999] Schmitz, G. P., Aldrich, C., and Gouws, F. S. (1999).
    Ann-dt: an algorithm for extraction of decision trees from artificial neural networks.
    IEEE Transactions on Neural Networks, 10(6):1392–1401.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Schmitz et al., 1999] Schmitz, G. P., Aldrich, C., and Gouws, F. S. (1999).
    Ann-dt: 从人工神经网络中提取决策树的算法。IEEE神经网络交易，10(6):1392–1401。'
- en: '[Schulman et al., 2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint
    arXiv:1707.06347.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Schulman et al., 2017] Schulman, J., Wolski, F., Dhariwal, P., Radford, A.,
    and Klimov, O. (2017). 近端策略优化算法。arXiv预印本 arXiv:1707.06347。'
- en: '[Selvaraju et al., 2017] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam,
    R., Parikh, D., and Batra, D. (2017). Grad-cam: Visual explanations from deep
    networks via gradient-based localization. In Proceedings of the IEEE international
    conference on computer vision, pages 618–626.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Selvaraju et al., 2017] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam,
    R., Parikh, D., and Batra, D. (2017). Grad-cam: 基于梯度的定位生成深度网络的视觉解释。IEEE国际计算机视觉会议论文集，页码618–626。'
- en: '[Sethi et al., 2012] Sethi, K. K., Mishra, D. K., and Mishra, B. (2012). Kdruleex:
    A novel approach for enhancing user comprehensibility using rule extraction. In
    2012 Third International Conference on Intelligent Systems Modelling and Simulation,
    pages 55–60\. IEEE.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sethi et al., 2012] Sethi, K. K., Mishra, D. K., and Mishra, B. (2012). Kdruleex:
    一种通过规则提取增强用户理解的创新方法。在2012年第三届国际智能系统建模与仿真会议，页码55–60。IEEE。'
- en: '[Sezer et al., 2020] Sezer, O. B., Gudelek, M. U., and Ozbayoglu, A. M. (2020).
    Financial time series forecasting with deep learning: A systematic literature
    review: 2005–2019. Applied Soft Computing, 90:106181.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sezer et al., 2020] Sezer, O. B., Gudelek, M. U., and Ozbayoglu, A. M. (2020).
    利用深度学习的金融时间序列预测：系统文献综述：2005–2019。应用软计算，90:106181。'
- en: '[Shaham et al., 2018] Shaham, U., Yamada, Y., and Negahban, S. (2018). Understanding
    adversarial training: Increasing local stability of supervised models through
    robust optimization. Neurocomputing, 307:195–204.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shaham et al., 2018] Shaham, U., Yamada, Y., and Negahban, S. (2018). 理解对抗训练：通过鲁棒优化提高监督模型的局部稳定性。神经计算，307:195–204。'
- en: '[Shankaranarayana and Runje, 2019] Shankaranarayana, S. M. and Runje, D. (2019).
    Alime: Autoencoder based approach for local interpretability. In International
    Conference on Intelligent Data Engineering and Automated Learning, pages 454–463\.
    Springer.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shankaranarayana and Runje, 2019] Shankaranarayana, S. M. 和 Runje, D. (2019).
    Alime: 基于自编码器的局部可解释性方法。在国际智能数据工程与自动化学习会议，页码454–463。Springer。'
- en: '[Shapley, 1953] Shapley, L. S. (1953). A value for n-person games. Contributions
    to the Theory of Games, 2(28):307–317.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shapley, 1953] Shapley, L. S. (1953). 一种用于n人游戏的价值。游戏理论的贡献，2(28):307–317。'
- en: '[Shawi et al., 2019] Shawi, R. E., Sherif, Y., Al-Mallah, M. H., and Sakr,
    S. (2019). ILIME: local and global interpretable model-agnostic explainer of black-box
    decision. In Welzer, T., Eder, J., Podgorelec, V., and Latific, A. K., editors,
    Advances in Databases and Information Systems - 23rd European Conference, ADBIS
    2019, Bled, Slovenia, September 8-11, 2019, Proceedings, volume 11695 of Lecture
    Notes in Computer Science, pages 53–68. Springer.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shawi et al., 2019] Shawi, R. E., Sherif, Y., Al-Mallah, M. H., and Sakr,
    S. (2019). ILIME: 局部和全局可解释的模型无关黑箱决策解释器。在Welzer, T., Eder, J., Podgorelec, V.,
    和 Latific, A. K. 编辑的《数据库与信息系统进展 - 第23届欧洲会议，ADBIS 2019，斯洛文尼亚布雷德，2019年9月8-11日，会议录》，计算机科学讲义第11695卷，页码53–68。Springer。'
- en: '[Shen et al., 2019] Shen, S., Han, S. X., Aberle, D. R., Bui, A. A., and Hsu,
    W. (2019). An interpretable deep hierarchical semantic convolutional neural network
    for lung nodule malignancy classification. Expert systems with applications, 128:84–95.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shen et al., 2019] Shen, S., Han, S. X., Aberle, D. R., Bui, A. A., and Hsu,
    W. (2019). 一种可解释的深层次层次语义卷积神经网络用于肺结节恶性分类。专家系统与应用，128:84–95。'
- en: '[Shi et al., 2020] Shi, S., Zhang, X., and Fan, W. (2020). A modified perturbed
    sampling method for local interpretable model-agnostic explanation. arXiv preprint
    arXiv:2002.07434.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shi et al., 2020] Shi, S., Zhang, X., and Fan, W. (2020). 一种用于局部可解释模型无关解释的改进扰动采样方法。arXiv预印本
    arXiv:2002.07434。'
- en: '[Shi et al., 2019] Shi, Z., Chen, W., Liang, S., Zuo, W., Yue, L., and Wang,
    S. (2019). Deep interpretable mortality model for intensive care unit risk prediction.
    In International Conference on Advanced Data Mining and Applications, pages 617–631\.
    Springer.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shi 等，2019] Shi, Z., Chen, W., Liang, S., Zuo, W., Yue, L., 和 Wang, S.（2019）。用于重症监护室风险预测的深度可解释死亡率模型。见于国际先进数据挖掘与应用大会，第617–631页。Springer。'
- en: '[Shrikumar et al., 2017a] Shrikumar, A., Greenside, P., and Kundaje, A. (2017a).
    Learning important features through propagating activation differences. In Precup,
    D. and Teh, Y. W., editors, Proceedings of the 34th International Conference on
    Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages
    3145–3153, International Convention Centre, Sydney, Australia. PMLR.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shrikumar 等，2017a] Shrikumar, A., Greenside, P., 和 Kundaje, A.（2017a）。通过传播激活差异学习重要特征。见于
    Precup, D. 和 Teh, Y. W. 主编的《第34届国际机器学习大会论文集》，卷70，《机器学习研究论文集》，第3145–3153页，国际会议中心，悉尼，澳大利亚。PMLR。'
- en: '[Shrikumar et al., 2017b] Shrikumar, A., Greenside, P., Shcherbina, A., and
    Kundaje, A. (2017b). Not just a black box: Learning important features through
    propagating activation differences. Proceedings of the 34th International Conference
    on Machine Learning, PMLR.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Shrikumar 等，2017b] Shrikumar, A., Greenside, P., Shcherbina, A., 和 Kundaje,
    A.（2017b）。不仅仅是黑箱：通过传播激活差异学习重要特征。第34届国际机器学习大会论文集，PMLR。'
- en: '[Simonyan et al., 2014] Simonyan, K., Vedaldi, A., and Zisserman, A. (2014).
    Deep inside convolutional networks: Visualising image classification models and
    saliency maps. Workshop at International Conference on Learning Representations.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Simonyan 等，2014] Simonyan, K., Vedaldi, A., 和 Zisserman, A.（2014）。深度卷积网络内部：可视化图像分类模型和显著性图。国际学习表征大会工作坊。'
- en: '[Singh et al., 2020a] Singh, A., Mohammed, A. R., Zelek, J., et al. (2020a).
    Interpretation of deep learning using attributions: application to ophthalmic
    diagnosis. In Applications of Machine Learning 2020, volume 11511, page 115110A.
    International Society for Optics and Photonics.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Singh 等，2020a] Singh, A., Mohammed, A. R., Zelek, J. 等（2020a）。利用归因解释深度学习：在眼科诊断中的应用。见于《2020年机器学习应用》，卷11511，第115110A页。国际光学与光子学学会。'
- en: '[Singh et al., 2020b] Singh, A., Sengupta, S., and Lakshminarayanan, V. (2020b).
    Explainable deep learning models in medical image analysis. arXiv preprint arXiv:2005.13799.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Singh 等，2020b] Singh, A., Sengupta, S., 和 Lakshminarayanan, V.（2020b）。医疗图像分析中的可解释深度学习模型。arXiv
    预印本 arXiv:2005.13799。'
- en: '[Springenberg et al., 2015] Springenberg, J. T., Dosovitskiy, A., Brox, T.,
    and Riedmiller, M. (2015). Striving for simplicity: The all convolutional net.
    ICLR (workshop track.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Springenberg 等，2015] Springenberg, J. T., Dosovitskiy, A., Brox, T., 和 Riedmiller,
    M.（2015）。追求简洁：全卷积网络。ICLR（工作坊跟踪）。'
- en: '[Sun et al., 2018] Sun, M., Tang, F., Yi, J., Wang, F., and Zhou, J. (2018).
    Identify susceptible locations in medical records via adversarial attacks on deep
    predictive models. In Proceedings of the 24th ACM SIGKDD International Conference
    on Knowledge Discovery & Data Mining, pages 793–801.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sun 等，2018] Sun, M., Tang, F., Yi, J., Wang, F., 和 Zhou, J.（2018）。通过对深度预测模型的对抗攻击识别医疗记录中的易感位置。见于《第24届
    ACM SIGKDD 国际知识发现与数据挖掘大会论文集》，第793–801页。'
- en: '[Sundararajan et al., 2020] Sundararajan, M., Dhamdhere, K., and Agarwal, A.
    (2020). The shapley taylor interaction index. In International Conference on Machine
    Learning, pages 9259–9268\. PMLR.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sundararajan 等，2020] Sundararajan, M., Dhamdhere, K., 和 Agarwal, A.（2020）。Shapley
    Taylor 交互指数。见于国际机器学习大会，第9259–9268页。PMLR。'
- en: '[Sundararajan and Najmi, 2020] Sundararajan, M. and Najmi, A. (2020). The many
    shapley values for model explanation. In International Conference on Machine Learning,
    pages 9269–9278\. PMLR.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sundararajan 和 Najmi，2020] Sundararajan, M. 和 Najmi, A.（2020）。模型解释的多种 Shapley
    值。见于国际机器学习大会，第9269–9278页。PMLR。'
- en: '[Sundararajan et al., 2017] Sundararajan, M., Taly, A., and Yan, Q. (2017).
    Axiomatic attribution for deep networks. In Proceedings of the 34th International
    Conference on Machine Learning-Volume 70, pages 3319–3328.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Sundararajan 等，2017] Sundararajan, M., Taly, A., 和 Yan, Q.（2017）。深度网络的公理化归因。见于《第34届国际机器学习大会论文集-卷70》，第3319–3328页。'
- en: '[Suresh et al., 2017] Suresh, H., Hunt, N., Johnson, A., Celi, L. A., Szolovits,
    P., and Ghassemi, M. (2017). Clinical intervention prediction and understanding
    with deep neural networks. In Machine Learning for Healthcare Conference, pages
    322–337. PMLR.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Suresh 等，2017] Suresh, H., Hunt, N., Johnson, A., Celi, L. A., Szolovits,
    P., 和 Ghassemi, M.（2017）。使用深度神经网络进行临床干预预测和理解。见于《医疗保健会议机器学习》，第322–337页。PMLR。'
- en: '[Szegedy et al., 2013] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J.,
    Erhan, D., Goodfellow, I., and Fergus, R. (2013). Intriguing properties of neural
    networks. ICLR.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Szegedy et al., 2013] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J.,
    Erhan, D., Goodfellow, I., 和 Fergus, R. (2013). 神经网络的迷人属性。ICLR。'
- en: '[Szegedy et al., 2014] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J.,
    Erhan, D., Goodfellow, I., and Fergus, R. (2014). Intriguing properties of neural
    networks. In 2nd International Conference on Learning Representations, ICLR 2014.
    Conference date: 14-04-2014 Through 16-04-2014.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Szegedy et al., 2014] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J.,
    Erhan, D., Goodfellow, I., 和 Fergus, R. (2014). 神经网络的迷人属性。在第二届国际学习表征会议，ICLR 2014。会议日期：2014年4月14日至2014年4月16日。'
- en: '[Tabassi et al., 2019] Tabassi, E., Burns, K. J., Hadjimichael, M., Molina-Markham,
    A. D., and Sexton, J. T. (2019). A taxonomy and terminology of adversarial machine
    learning.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tabassi et al., 2019] Tabassi, E., Burns, K. J., Hadjimichael, M., Molina-Markham,
    A. D., 和 Sexton, J. T. (2019). 对抗性机器学习的分类法和术语。'
- en: '[Tesfay et al., 2018] Tesfay, W. B., Hofmann, P., Nakamura, T., Kiyomoto, S.,
    and Serna, J. (2018). I read but don’t agree: Privacy policy benchmarking using
    machine learning and the eu gdpr. In Companion Proceedings of the The Web Conference
    2018, pages 163–166.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tesfay et al., 2018] Tesfay, W. B., Hofmann, P., Nakamura, T., Kiyomoto, S.,
    和 Serna, J. (2018). 我阅读但不同意：使用机器学习和欧盟GDPR的隐私政策基准测试。在2018年网络会议伴随论文集中，第163–166页。'
- en: '[Tonekaboni et al., 2019a] Tonekaboni, S., Joshi, S., McCradden, M. D., and
    Goldenberg, A. (2019a). What clinicians want: Contextualizing explainable machine
    learning for clinical end use. In Machine Learning for Healthcare Conference,
    pages 359–380.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tonekaboni et al., 2019a] Tonekaboni, S., Joshi, S., McCradden, M. D., 和 Goldenberg,
    A. (2019a). 临床医生的需求：将可解释的机器学习应用于临床终端。 在医疗保健会议中，第359–380页。'
- en: '[Tonekaboni et al., 2019b] Tonekaboni, S., Joshi, S., McCradden, M. D., and
    Goldenberg, A. (2019b). What clinicians want: contextualizing explainable machine
    learning for clinical end use. arXiv preprint arXiv:1905.05134.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tonekaboni et al., 2019b] Tonekaboni, S., Joshi, S., McCradden, M. D., 和 Goldenberg,
    A. (2019b). 临床医生的需求：将可解释的机器学习应用于临床终端。arXiv预印本 arXiv:1905.05134。'
- en: '[Tsang et al., 2018] Tsang, M., Cheng, D., and Liu, Y. (2018). Detecting statistical
    interactions from neural network weights. In International Conference on Learning
    Representations.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tsang et al., 2018] Tsang, M., Cheng, D., 和 Liu, Y. (2018). 从神经网络权重中检测统计交互。在国际学习表征会议中。'
- en: '[Tsang et al., 2020] Tsang, M., Rambhatla, S., and Liu, Y. (2020). How does
    this interaction affect me? interpretable attribution for feature interactions.
    Advances in Neural Information Processing Systems.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Tsang et al., 2020] Tsang, M., Rambhatla, S., 和 Liu, Y. (2020). 这种交互如何影响我？特征交互的可解释归因。《神经信息处理系统进展》。'
- en: '[Van Hasselt et al., 2016] Van Hasselt, H., Guez, A., and Silver, D. (2016).
    Deep reinforcement learning with double q-learning. In Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 30.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Van Hasselt et al., 2016] Van Hasselt, H., Guez, A., 和 Silver, D. (2016).
    使用双重Q学习的深度强化学习。在AAAI人工智能会议论文集中，第30卷。'
- en: '[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all
    you need. Advances in neural information processing systems, 30:5998–6008.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
    Jones, L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. (2017). 注意力即你所需要的。神经信息处理系统进展，30:5998–6008。'
- en: '[Vellido, 2019] Vellido, A. (2019). The importance of interpretability and
    visualization in machine learning for applications in medicine and health care.
    Neural Computing and Applications, pages 1–15.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Vellido, 2019] Vellido, A. (2019). 机器学习在医学和医疗保健应用中的可解释性和可视化的重要性。《神经计算与应用》，第1–15页。'
- en: '[Voita et al., 2019] Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and
    Titov, I. (2019). Analyzing multi-head self-attention: Specialized heads do the
    heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting
    of the Association for Computational Linguistics, pages 5797–5808, Florence, Italy.
    Association for Computational Linguistics.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Voita et al., 2019] Voita, E., Talbot, D., Moiseev, F., Sennrich, R., 和 Titov,
    I. (2019). 分析多头自注意力：专门的头部进行繁重的工作，其余的可以被修剪。在第57届计算语言学协会年会论文集中，第5797–5808页，意大利佛罗伦萨。计算语言学协会。'
- en: '[Štrumbelj et al., 2009] Štrumbelj, E., Kononenko, I., and Robnik Šikonja,
    M. (2009). Explaining instance classifications with interactions of subsets of
    feature values. Data Knowledge and Engineering, 68(10):886–904.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Štrumbelj et al., 2009] Štrumbelj, E., Kononenko, I., 和 Robnik-Šikonja, M.
    (2009). 通过特征值子集的交互解释实例分类。《数据知识与工程》，68(10):886–904。'
- en: '[Wang et al., 2020] Wang, D., Li, C., Wen, S., Nepal, S., and Xiang, Y. (2020).
    Defending against adversarial attack towards deep neural networks via collaborative
    multi-task training. IEEE Transactions on Dependable and Secure Computing.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang 等，2020] Wang, D., Li, C., Wen, S., Nepal, S., 和 Xiang, Y. (2020). 通过协作多任务训练防御针对深度神经网络的对抗攻击。IEEE
    可靠与安全计算汇刊。'
- en: '[Wang et al., 2019] Wang, H., Lei, Z., Zhang, X., Zhou, B., and Peng, J. (2019).
    A review of deep learning for renewable energy forecasting. Energy Conversion
    and Management, 198:111799.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang 等，2019] Wang, H., Lei, Z., Zhang, X., Zhou, B., 和 Peng, J. (2019). 深度学习在可再生能源预测中的综述。能源转换与管理，198:111799。'
- en: '[Wang et al., 2021] Wang, S.-H., Govindaraj, V., Gorriz, J. M., Zhang, X.,
    and Zhang, Y.-D. (2021). Explainable diagnosis of secondary pulmonary tuberculosis
    by graph rank-based average pooling neural network. Journal of Ambient Intelligence
    and Humanized Computing, pages 1–14.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wang 等，2021] Wang, S.-H., Govindaraj, V., Gorriz, J. M., Zhang, X., 和 Zhang,
    Y.-D. (2021). 通过基于图的排序池神经网络解释继发性肺结核的诊断。环境智能与人性化计算杂志，页 1–14。'
- en: '[Wiens et al., 2019] Wiens, J., Saria, S., Sendak, M., Ghassemi, M., Liu, V. X.,
    Doshi-Velez, F., Jung, K., Heller, K., Kale, D., Saeed, M., et al. (2019). Do
    no harm: a roadmap for responsible machine learning for health care. Nature medicine,
    25(9):1337–1340.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wiens 等，2019] Wiens, J., Saria, S., Sendak, M., Ghassemi, M., Liu, V. X.,
    Doshi-Velez, F., Jung, K., Heller, K., Kale, D., Saeed, M., 等 (2019). 不要造成伤害：健康护理负责任机器学习的路线图。自然医学，25(9):1337–1340。'
- en: '[Wisdom et al., 2016] Wisdom, S., Powers, T., Pitton, J., and Atlas, L. (2016).
    Interpretable recurrent neural networks using sequential sparse recovery. NIPS
    2016 Workshop on Interpretable Machine Learning in Complex Systems.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wisdom 等，2016] Wisdom, S., Powers, T., Pitton, J., 和 Atlas, L. (2016). 使用顺序稀疏恢复的可解释递归神经网络。NIPS
    2016 复杂系统中可解释机器学习研讨会。'
- en: '[Wolf et al., 2020] Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue,
    C., Moi, A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S., et al. (2020).
    Transformers: State-of-the-art natural language processing. In Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations, pages 38–45.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Wolf 等，2020] Wolf, T., Chaumond, J., Debut, L., Sanh, V., Delangue, C., Moi,
    A., Cistac, P., Funtowicz, M., Davison, J., Shleifer, S., 等 (2020). Transformers:
    先进的自然语言处理技术。发表于 2020 年经验方法自然语言处理会议：系统演示，页 38–45。'
- en: '[Xie et al., 2019] Xie, P., Zuo, K., Zhang, Y., Li, F., Yin, M., and Lu, K.
    (2019). Interpretable classification from skin cancer histology slides using deep
    learning: A retrospective multicenter study. arXiv preprint arXiv:1904.06156.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xie 等，2019] Xie, P., Zuo, K., Zhang, Y., Li, F., Yin, M., 和 Lu, K. (2019).
    利用深度学习从皮肤癌组织切片中进行可解释分类：一项回顾性多中心研究。arXiv 预印本 arXiv:1904.06156。'
- en: '[Xie et al., 2017] Xie, Q., Ma, X., Dai, Z., and Hovy, E. (2017). An interpretable
    knowledge transfer model for knowledge base completion. In Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers), pages 950–962, Vancouver, Canada. Association for Computational
    Linguistics.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xie 等，2017] Xie, Q., Ma, X., Dai, Z., 和 Hovy, E. (2017). 一种可解释的知识迁移模型用于知识库补全。发表于第
    55 届计算语言学协会年会 (第一卷：长篇论文)，页 950–962，加拿大温哥华。计算语言学协会。'
- en: '[Xu et al., 2015] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov,
    R., Zemel, R., and Bengio, Y. (2015). Show, attend and tell: Neural image caption
    generation with visual attention. In International conference on machine learning,
    pages 2048–2057.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Xu 等，2015] Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov,
    R., Zemel, R., 和 Bengio, Y. (2015). 展示、关注与讲述：使用视觉注意力的神经图像描述生成。发表于国际机器学习会议，页 2048–2057。'
- en: '[Yang et al., 2018] Yang, Y., Tresp, V., Wunderle, M., and Fasching, P. A.
    (2018). Explaining therapy predictions with layer-wise relevance propagation in
    neural networks. In 2018 IEEE International Conference on Healthcare Informatics
    (ICHI), pages 152–162\. IEEE.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yang 等，2018] Yang, Y., Tresp, V., Wunderle, M., 和 Fasching, P. A. (2018).
    用神经网络的逐层相关传播解释治疗预测。发表于 2018 年 IEEE 国际医疗信息学大会 (ICHI)，页 152–162。IEEE。'
- en: '[Yeh et al., 2019] Yeh, C.-K., Hsieh, C.-Y., Suggala, A., Inouye, D. I., and
    Ravikumar, P. K. (2019). On the (in) fidelity and sensitivity of explanations.
    In Advances in Neural Information Processing Systems, pages 10967–10978.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Yeh 等，2019] Yeh, C.-K., Hsieh, C.-Y., Suggala, A., Inouye, D. I., 和 Ravikumar,
    P. K. (2019). 关于（不）忠实性和解释的敏感性。发表于神经信息处理系统进展，页 10967–10978。'
- en: '[Ying et al., 2019] Ying, R., Bourgeois, D., You, J., Zitnik, M., and Leskovec,
    J. (2019). Gnn explainer: A tool for post-hoc explanation of graph neural networks.
    Neural Information Processing Systems (NeurIPS).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Young et al., 2019] Young, K., Booth, G., Simpson, B., Dutton, R., and Shrapnel,
    S. (2019). Deep neural network or dermatologist? In Interpretability of Machine
    Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision
    Support, pages 48–55\. Springer.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Yuan et al., 2019] Yuan, X., He, P., Zhu, Q., and Li, X. (2019). Adversarial
    examples: Attacks and defenses for deep learning. IEEE transactions on neural
    networks and learning systems, 30(9):2805–2824.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zafar and Khan, 2019] Zafar, M. R. and Khan, N. M. (2019). Dlime: A deterministic
    local interpretable model-agnostic explanations approach for computer-aided diagnosis
    systems. Proceedings ofAnchorage ’19: ACM SIGKDD Workshop on Explainable AI/ML
    (XAI) for Accountability,Fairness, and Transparency (Anchorage ’19).'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zeiler and Fergus, 2014a] Zeiler, M. D. and Fergus, R. (2014a). Visualizing
    and understanding convolutional networks. In European conference on computer vision,
    pages 818–833. Springer.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zeiler and Fergus, 2014b] Zeiler, M. D. and Fergus, R. (2014b). Visualizing
    and understanding convolutional neural networks. In Proceedings of the 13th European
    Conference Computer Vision and Pattern Recognition, Zurich, Switzerland, pages
    6–12.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zhang et al., 2021] Zhang, Y., Zhang, X., and Zhu, W. (2021). Anc: attention
    network for covid-19 explainable diagnosis based on convolutional block attention
    module. Computer Modeling in Engineering & Sciences, pages 1037–1058.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zhong et al., 2019] Zhong, R., Shao, S., and McKeown, K. (2019). Fine-grained
    sentiment analysis with faithful attention. arXiv preprint arXiv:1908.06870.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zhou et al., 2016] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., and Torralba,
    A. (2016). Learning deep features for discriminative localization. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2921–2929.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zihni et al., 2020] Zihni, E., Madai, V. I., Livne, M., Galinovic, I., Khalil,
    A. A., Fiebach, J. B., and Frey, D. (2020). Opening the black box of artificial
    intelligence for clinical decision support: A study predicting stroke outcome.
    Plos one, 15(4):e0231166.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zilke et al., 2016] Zilke, J. R., Mencía, E. L., and Janssen, F. (2016). Deepred–rule
    extraction from deep neural networks. In International Conference on Discovery
    Science, pages 457–473\. Springer.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Zintgraf et al., 2017] Zintgraf, L. M., Cohen, T. S., Adel, T., and Welling,
    M. (2017). Visualizing deep neural network decisions: Prediction difference analysis.
    In International Conference on Learning Representations.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
