["```py\nnet2 = LogReg().cuda()\nloss=nn.NLLLoss()\nlearning_rate = 1e-2\noptimizer=optim.SGD(net2.parameters(), lr=learning_rate)**for** epoch **in** range(1):\n    losses=[]\n    dl = iter(md.trn_dl)\n    **for** t **in** range(len(dl)):\n        *# Forward pass: compute predicted y and loss by passing x to\n        # the model.*\n        xt, yt = next(dl)\n        y_pred = net2(V(xt))\n        l = loss(y_pred, V(yt))\n        losses.append(l) *# Before the backward pass, use the optimizer object to zero\n        # all of the gradients for the variables it will update\n        # (which are the learnable weights of the model)*\n        optimizer.zero_grad() *# Backward pass: compute gradient of the loss with respect\n        # to model parameters*\n        l.backward() *# Calling the step function on an Optimizer makes an update\n        # to its parameters*\n        optimizer.step()\n\n    val_dl = iter(md.val_dl)\n    val_scores = [score(*next(val_dl)) **for** i **in** range(len(val_dl))]\n    print(np.mean(val_scores))\n```", "```py\n**def** score(x, y):\n    y_pred = to_np(net2(V(x)))\n    **return** np.sum(y_pred.argmax(axis=1) == to_np(y))/len(y_pred)\n```", "```py\nnet2 = LogReg().cuda()\nloss_fn=nn.NLLLoss()\nlr = 1e-2\nw,b = net2.l1_w,net2.l1_b\n\n**for** epoch **in** range(1):\n    losses=[]\n    dl = iter(md.trn_dl)\n    **for** t **in** range(len(dl)):\n        xt, yt = next(dl)\n        y_pred = net2(V(xt))\n        l = loss(y_pred, Variable(yt).cuda())\n        losses.append(loss)\n\n        *# Backward pass: compute gradient of the loss with respect \n        # to model parameters*\n        l.backward()\n        w.data -= w.grad.data * lr\n        b.data -= b.grad.data * lr\n\n        w.grad.data.zero_()\n        b.grad.data.zero_()   \n\n    val_dl = iter(md.val_dl)\n    val_scores = [score(*next(val_dl)) **for** i **in** range(len(val_dl))]\n    print(np.mean(val_scores))\n```", "```py\nd = Dataset(...)\nlen(d)\nd[i]\n```", "```py\nTo get the dataset, in your terminal run the following commands:wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gzgunzip aclImdb_v1.tar.gztar -xvf aclImdb_v1.tar\n```", "```py\nPATH='data/aclImdb/'\nnames = ['neg','pos']%ls {PATH}*aclImdb_v1.tar.gz  imdbEr.txt  imdb.vocab  models/  README  test/  tmp/  train/*%ls {PATH}train*aclImdb/  all_val/         neg/  tmp/    unsupBow.feat  urls_pos.txt\nall/      labeledBow.feat  pos/  unsup/  urls_neg.txt   urls_unsup.txt*%ls {PATH}train/pos | head*0_9.txt\n10000_8.txt\n10001_10.txt\n10002_7.txt\n10003_8.txt\n10004_8.txt\n10005_7.txt\n10006_7.txt\n10007_7.txt\n10008_7.txt\n...*\n```", "```py\ntrn[0]*\"Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\"*\n```", "```py\ntrn_y[0]*0*\n```", "```py\ntrn,trn_y = texts_labels_from_folders(f'**{PATH}**train',names)\nval,val_y = texts_labels_from_folders(f'**{PATH}**test',names)\n```", "```py\nveczr = CountVectorizer(tokenizer=tokenize)\n```", "```py\ntrn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)\n```", "```py\ntrn_term_doc<25000x75132 sparse matrix of type '<class 'numpy.int64'>'\n     with 3749745 stored elements in Compressed Sparse Row format>\n```", "```py\n(1, 4) \u2192 4\n(1, 123) \u2192 1\n```", "```py\ntrn_term_doc[0]<1x75132 sparse matrix of type '<class 'numpy.int64'>'\n\twith 93 stored elements in Compressed Sparse Row format>\n```", "```py\nvocab = veczr.get_feature_names(); vocab[5000:5005]['aussie', 'aussies', 'austen', 'austeniana', 'austens']\n```", "```py\nw0 = set([o.lower() **for** o **in** trn[0].split(' ')]); w0*{'a',\n 'absurd',\n 'an',\n 'and',\n 'audience',\n 'be',\n 'better',\n 'briefly.',\n 'by',\n 'can',\n ...\n}*len(w0)*91*\n```", "```py\nveczr.vocabulary_['absurd']*1297*\n```", "```py\ntrn_term_doc[0,1297]*2*\n```", "```py\ntrn_term_doc[0,5000]0\n```", "```py\n**def** pr(y_i):\n    p = x[y==y_i].sum(0)\n    **return** (p+1) / ((y==y_i).sum()+1)x=trn_term_doc\ny=trn_yp = x[y==1].sum(0)+1 \nq = x[y==0].sum(0)+1\nr = np.log((p/p.sum())/(q/q.sum()))\nb = np.log(len(p)/len(q))\n```", "```py\npre_preds = val_term_doc @ r.T + b\npreds = pre_preds.T>0\n(preds==val_y).mean()0.80691999999999997\n```", "```py\npre_preds = val_term_doc.sign() @ r.T + b\npreds = pre_preds.T>0\n(preds==val_y).mean()0.82623999999999997\n```", "```py\nm = LogisticRegression(C=1e8, dual=**True**)\nm.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()0.85504000000000002\n```", "```py\nm = LogisticRegression(C=1e8, dual=**True**)\nm.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()0.85487999999999997\n```", "```py\nm = LogisticRegression(C=0.1, dual=**True**)\nm.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()0.88275999999999999\n```", "```py\nm = LogisticRegression(C=0.1, dual=**True**)\nm.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()0.88404000000000005\n```", "```py\nveczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize,\n                         max_features=800000)\ntrn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)trn_term_doc.shape*(25000, 800000)*vocab = veczr.get_feature_names()vocab[200000:200005]['by vast', 'by vengeance', 'by vengeance .', 'by vera', 'by vera miles']\n```"]