- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:33:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2404.06114] Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.06114](https://ar5iv.labs.arxiv.org/html/2404.06114)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Feng Liang, , Zhen Zhang, Haifeng Lu, Victor C. M. Leung, , Yanyi Guo^∗, Xiping Hu^∗
    ^∗ Corresponding authors.Feng Liang and Xiping Hu are with a) Artificial Intelligence
    Research Institute, Shenzhen MSU-BIT University, Shenzhen 518000, China, and b)
    Guangdong-Hong Kong-Macao Joint Laboratory for Emotional Intelligence and Pervasive
    Computing, Shenzhen MSU-BIT University, Shenzhen 518107, China. (e-mail: {fliang,
    huxp}@smbu.edu.cn)Zhen Zhang and Haifeng Lu are with a) Gansu Provincial Key Laboratory
    of Wearable Computing, School of information Science and Engineering, Lanzhou
    University, Gansu 730000, China, and b) Guangdong-Hong Kong-Macao Joint Laboratory
    for Emotional Intelligence and Pervasive Computing, Shenzhen MSU-BIT University,
    Shenzhen 518107, China. (e-mail: {zhangzhen19, luhf18}@lzu.edu.cn)Victor C. M.
    Leung is with a) Artificial Intelligence Research Institute, Shenzhen MSU-BIT
    University, Shenzhen 518000, China, and b) the Department of Electrical and Computer
    Engineering, The University of British Columbia, Vancouver, Canada. (e-mail: vleung@ieee.org)Yanyi
    Guo is with a) Frontier Cross Disciplinary Research Institute, Shenzhen MSU-BIT
    University, Shenzhen 518000, China, and b) the School of Mechanical and Electrical
    Engineering, Beijing Institute of Technology, Beijing 10081, China. (e-mail: guoyy@smbu.edu.cn)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the rapid growth in the volume of data sets, models, and devices in the
    domain of deep learning, there is increasing attention on large-scale distributed
    deep learning. In contrast to traditional distributed deep learning, the large-scale
    scenario poses new challenges that include fault tolerance, scalability of algorithms
    and infrastructures, and heterogeneity in data sets, models, and resources. Due
    to intensive synchronization of models and sharing of data across GPUs and computing
    nodes during distributed training and inference processes, communication efficiency
    becomes the bottleneck for achieving high performance at a large scale. This article
    surveys the literature over the period of 2018-2023 on algorithms and technologies
    aimed at achieving efficient communication in large-scale distributed DL at various
    levels, including algorithms, frameworks, and infrastructures. Specifically, we
    first introduce efficient algorithms for model synchronization and communication
    data compression in the context of large-scale distributed training. Next, we
    introduce efficient strategies related to resource allocation and task scheduling
    for use in distributed training and inference. After that, we present the latest
    technologies pertaining to modern communication infrastructures used in distributed
    deep learning, including GPU interconnects, programmable network devices, collective
    communication protocols, and communication topologies. We focus our discussion
    of these topics on examining the impact of the communication overhead in a large-scale
    and heterogeneous setting. Finally, we conduct a case study on the distributed
    training of large language models at a large scale to illustrate how to apply
    these technologies in real cases. This article aims to offer researchers a comprehensive
    understanding of the current landscape of large-scale distributed deep learning
    and to reveal promising future research directions toward communication-efficient
    solutions in this scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Communication-efficient, distributed deep learning, scalability, federated learning,
    heterogeneity, LLM, large models, large scale, pipeline parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the rapid advancements in computational power of GPUs [[1](#bib.bib1)]
    and the development of foundation artificial neural network models such as ResNet [[2](#bib.bib2)]
    and Transformer [[3](#bib.bib3)], deep learning (DL) has become the state-of-the-art
    approach across diverse fields over the past decade. The fields include natural
    language processing (NLP) [[4](#bib.bib4), [5](#bib.bib5)], multimedia processing [[6](#bib.bib6),
    [7](#bib.bib7)], biomedical engineering [[8](#bib.bib8), [9](#bib.bib9)], and
    autonomous driving solutions [[10](#bib.bib10), [11](#bib.bib11)]. Traditional
    DL models and the associated training data sets can run on a single GPU or server
    node without inter-GPU or inter-node communication. However, with the increase
    in the sizes of data sets and DL models, a standalone GPU or node cannot handle
    DL tasks efficiently, and distributed DL emerges to help.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed DL has become the state-of-the-art solution for addressing challenges
    present in complex scenarios of artificial intelligence. Distributed DL entails
    the training or inference of deep neural network (DNN) models on multiple CPUs
    or GPUs in one or multiple computing nodes to handle large training data sets
    and extensive learning models. The benefits of distributed DL are threefold. Firstly,
    distributed DL enhances the training parallelism on large data sets and optimizes
    hyperparameters tuning. For instance, numerous DL models used in the fields of
    remote sensing [[12](#bib.bib12), [13](#bib.bib13)] require processing vast volumes
    of high-revolution multimedia data across multiple modalities to improve classification
    accuracy. To enhance training parallelism, data sets can be distributed across
    multiple nodes, with each node training models independently and sharing its efforts
    through a specific synchronization mechanism. Secondly, distributed DL facilitates
    the training and inference of large artificial neural network models. In particular,
    to train a large language model (LLM) [[14](#bib.bib14)] with tens of billions
    of parameters, a large number of GPUs and nodes are required to collaborate in
    accommodating the entire model and to perform parallel training with distributed
    data for rapid convergence. Thirdly, the evolution of Internet of Things (IoT)
    and Internet of Vehicles (IoV) has led to intricate scenarios that require use
    of distributed DL. Distributed DL empowers IoT and IoV solutions [[15](#bib.bib15)]
    by enabling them to make intelligent decisions by leveraging the computational
    and communication capabilities of the servers, network infrastructures, and end
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Communications and distributed DL are intricately intertwine themes, forming
    integral components of each other’s core functionalities. On the one hand, distributed
    DL has emerged as a prominent technique for addressing and optimizing diverse
    communications problems. With the surge in communication devices and network traffic
    volume, distributed DL offers a real-time and agile approach for tasks encompassing
    traffic analytics, routing, and network resource management across diverse communications
    domains, such as wireless communications, IoT, and network security. The inherent
    distributed nature of this approach also contributes to robustness and fault tolerance,
    thereby ensuring reliable communication in dynamic environments. On the other
    hand, efficient communications technologies play a pivotal role in achieving high-performance
    distributed DL. Given the collaborative and coordination-driven nature of distributed
    DL, communications permeate nearly all aspects of this domain and act as its driving
    force. Technologies aimed at optimizing high-performance distributed DL fall predominantly
    into the category of efficient communications technologies. Because of this mutual
    dependence, advancements in one domain impact significantly the capability of
    the other. While the former case, exploring distributed DL for communications,
    has been investigated extensively in numerous studies [[16](#bib.bib16), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)], this article emphasizes
    strategically the latter case, communications for distributed DL, for focused
    discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient communication is crucial for achieving high performance at different
    levels in distributed DL. 1) At the algorithm level, synchronizing models during
    distributed training involves intensive inter-GPU and inter-node communication
    to ensure model consistency and convergence performance [[21](#bib.bib21)]. Optimizing
    communication frequency and traffic volume to reduce communication overhead can
    help models converge swiftly with significantly less training time. 2) At the
    framework level, underutilizing communication resources and having inferior task
    scheduling cause communication traffic congestion and straggler problems in distributed
    DL [[22](#bib.bib22)]. The performance of distributed DL can be enhanced greatly
    by fully utilizing communication resources, balancing the allocation of computational
    and communication resources, and overlapping computational and communication tasks
    to prevent blocking communication. 3) At the infrastructure level, low-performance
    communication links, devices, protocols, and topologies can offset the power of
    high-performance computing units and are prone to becoming the bottleneck of the
    overall system of distributed DL [[23](#bib.bib23)]. A high-performance and cost-effective
    solution for all these communication infrastructure layers [[24](#bib.bib24)]
    can maximize the computational and communication capacity of the entire distributed
    DL system. Addressing these communication challenges at various levels in diverse
    environments is crucial for high-performance distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the growing number of computational and communication devices in systems
    has led to the emergence of large-scale distributed DL and posed additional challenges
    for this context. The first challenge pertains to algorithmic complexity. In large-scale
    distributed DL, the increased number of devices and volume of workloads introduce
    additional computational and communication overhead. Efficient algorithms used
    for model synchronization and communication data compression that can scale linearly
    are crucial. Otherwise, large-scale distributed DL can introduce more overhead
    than benefits. Simultaneously, given that the optimization solution search space
    for distributed DL algorithm can grow exponentially with the scale, designing
    optimal algorithms for larger-scale distributed DL can be significantly more challenging.
    The second challenge concerns heterogeneity. In large-scale distributed DL, heterogeneity
    is prevalent in various aspects, including the data distribution, model specification,
    and geographical location and resource capacity of computational and communication
    devices. The heterogeneity not only degrades the convergence performance of distributed
    DL, but also impacts communication efficiency, and is exacerbated by problems
    of resource underutilization and stragglers. The heterogeneous factors further
    contribute to the algorithmic complexity in designing communication-efficient
    distributed DL algorithms. The third challenge concerns large models. The size
    of large models has grown exponentially compared to previous sizes. Distributed
    training of large models at scale has raised both theoretical and practical concerns
    about various factors, including convergence performance, training efficiency,
    and the costs of computational and communication resources. Given the increased
    number of devices and extended training time required, fault tolerance has also
    become more critical for large-model training compared to traditional cases with
    medium-sized models.
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid development of large-scale distributed DL, there is an urgent
    need for conducting a comprehensive survey on communication-efficient large-scale
    distributed DL technologies that aim to empower researchers in the fields of communications,
    computer science, and artificial intelligence to understand critical research
    problems in this domain and to make valuable contributions. Existing surveys on
    distributed DL do express concerns about communication acting as a bottleneck
    at various levels. However, they lack a systematic and comprehensive investigation
    into the communication problems and solutions in the large-scale scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'This article surveys the literature over the period of 2018-2023 on communication-efficient
    technologies that operate at various levels of large-scale distributed DL, including
    the algorithm, framework, and infrastructure levels. For a comprehensive understanding
    of the development history of a specific topic, some milestone works before 2018
    may be included. The topics covered in this article include model-synchronization
    and communication-data-compression algorithms, resource-allocation and task-scheduling
    strategies, and communication infrastructures. The discussion of these topics
    focuses on two dimensions: 1) how resolving the bottleneck of communication can
    enhance the performance of distributed DL; and 2) how they can be high-performance
    in large-scale settings. We also summarize some lessons learned from each topic
    to emphasize promising future research directions toward high-performance large-scale
    distributed DL. At the end this article, we conduct a case study on the large-scale
    distributed training of LLM to explore how these communication-efficient solutions
    can be applied practically in real cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: A Comparison of Related Surveys'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Ref. | Year | Comm-Efficient Synchro-nization | Comm Data Compression |
    Comm-Efficient Resource Allocation | Comm-Efficient Task Scheduling | Comm Infrastructure
    | Edge-Cloud Heterogeneity | Large-Scale Distributed DL | Distributed Training
    of Large Models |'
  prefs: []
  type: TYPE_TB
- en: '|   [[25](#bib.bib25)] | 2020 | ✓ | ✓ |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bib26)] | 2019 | ✓ | ✓ |  | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] | 2020 | ✓ | ✓ |  | ✓ | ✓ |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] | 2016 | ✓ |  |  | ✓ |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] | 2021 | ✓ | ✓ |  | ✓ | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[30](#bib.bib30)] | 2023 | ✓ | ✓ |  | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[31](#bib.bib31)] | 2023 | ✓ | ✓ | ✓ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[32](#bib.bib32)] | 2023 | ✓ | ✓ |  | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[33](#bib.bib33)] | 2019 | ✓ |  |  |  | ✓ | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[34](#bib.bib34)] | 2019 | ✓ | ✓ | ✓ |  |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bib35)] | 2020 | ✓ | ✓ |  | ✓ |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[36](#bib.bib36)] | 2020 | ✓ | ✓ |  |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[37](#bib.bib37)] | 2021 | ✓ | ✓ | ✓ |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[38](#bib.bib38)] | 2021 | ✓ | ✓ |  |  | ✓ | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[39](#bib.bib39)] | 2022 | ✓ | ✓ |  |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[40](#bib.bib40)] | 2022 | ✓ | ✓ |  |  |  | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bib41)] | 2022 |  | ✓ |  |  | ✓ | ✓ | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '|   Ours | - | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: I-A Related Surveys
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [I](#S1.T1 "TABLE I ‣ I Introduction ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") compares our survey with other
    related surveys on various topics. An empty cell denotes insufficient coverage
    of the topic in the corresponding survey. Existing surveys explore distributed
    DL from various perspectives, but do not address thoroughly key aspects regarding
    the large-scale setting, especially the critical role of communication in handling
    large volumes of data, models, devices, and infrastructures. They typically do
    not explore sufficiently the scalability of algorithms and infrastructures at
    different levels of the distributed DL ecosystems in the large-scale setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, existing surveys on distributed DL have the following drawbacks
    regarding the large-scale setting:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About heterogeneity. Few surveys, when discussing technologies for communication
    optimization in distributed DL, highlight the impacts of heterogeneity, including
    model synchronization and compression of models for communication. The issue of
    heterogeneity in data, models, devices, and infrastructures is pervasive and salient
    in modern distributed DL scenarios, especially in large-scale distributed DL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About resource allocation. Existing surveys lack a thorough study on communication-efficient
    resource-allocation strategies. In a multi-tenant environment with a large number
    of servers and devices for computing and network infrastructures for communication,
    optimizing the allocation of computational and communication resources for better
    resource utilization is critical to the performance of large-scale distributed
    DL.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About communication infrastructures. Few surveys study the infrastructure for
    efficient communication in large-scale distributed DL. Communication infrastructures,
    encompassing interconnects, network devices, collective communication libraries,
    and network topologies, are of paramount importance for high-performance large-scale
    distributed DL. The rapid development of these communication infrastructure technologies
    deserves timely attention and follow-up.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About large models. Existing surveys lack a focus on distributed DL with large
    models. Given the success of extremely large foundation models in fields such
    as NLP, there is an urgent need for the study of various technologies for distribute
    DL in the scenario of large models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Specifically, several surveys [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)]
    have focused on distributed DL from various perspectives. In particular, Verbraeken
    et al. [[25](#bib.bib25)] provide a review of distributed DL algorithms and frameworks.
    Ben-Nun and Hoefler [[26](#bib.bib26)] provide an analysis on the concurrency
    of parallel and distributed DL architectures and models. Mayer and Jocobsen [[27](#bib.bib27)]
    focus on parallelization methods to enable scalable distributed training and cover
    various topics such as distributed resource and multi-tenant management. However,
    these surveys do not emphasize adequately the communication as the critical bottleneck
    when discussing the ecosystem of distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: Several surveys [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31),
    [32](#bib.bib32)] have addressed communication topics in distributed DL at various
    levels. Particularly, Xing et al. [[28](#bib.bib28)] provide a survey on strategies
    and principles for distributing DL algorithms in a cluster, aiming to enhance
    inter-node communication efficiency. Ouyang et al. [[29](#bib.bib29)] focus on
    communication strategies aimed at reducing network communication traffic through
    algorithm-level optimization and accelerating network communication speed through
    network-level optimization. Both Yu et al. [[30](#bib.bib30)] and Cao et al. [[31](#bib.bib31)]
    further summarize and categorize communication-optimization strategies for distributed
    DL such as communication frequency reduction and communication data compression,
    at the algorithm level. In addition, Cao et al. [[31](#bib.bib31)] also explore
    radio-resource-management strategies and game theory approaches for improving
    communication efficiency in distributed DL. Tang et al. [[32](#bib.bib32)] conduct
    a survey of efficient communication technologies used in distributed DL at the
    architecture and application levels. Organized in a presentation structure that
    is partially similar to this paper, the survey focuses on topics like communication
    synchronization, system architectures, compression techniques, and parallelism
    of computational and communication tasks. However, it do not tackle the influence
    of heterogeneity issues and the scalability of the related technologies when discussing
    these topics. It also overlooks certain important topics for large-scale distributed
    DL, such as resource allocation and communication infrastructures.
  prefs: []
  type: TYPE_NORMAL
- en: Several surveys address the communication challenges in specific distributed
    DL paradigms, including cloud-edge-based [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)], P2P-based [[39](#bib.bib39)],
    and hybrid [[40](#bib.bib40), [41](#bib.bib41)] paradigms. Notably, surveys that
    focus on the edge-based paradigm cover different aspects of communication-efficient
    technologies, including edge-specific architectures [[33](#bib.bib33), [34](#bib.bib34)],
    algorithms [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)], and infrastructures [[38](#bib.bib38)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To fill the gap in existing surveys on distributed DL, this article particularly
    surveys communication-efficient technologies in large-scale scenarios. Specifically,
    this article focuses on:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communication-efficient distributed DL algorithms in heterogeneous settings.
    We highlight the impact of heterogeneity in data, model, and resources when presenting
    various optimization and compression algorithms, discussing how to improve communication
    performance in this context.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for resource allocation and task scheduling aimed at fully utilizing
    computational and communication resources and increasing distributed DL throughput
    in large-scale settings. We explore various resource-allocation strategies for
    large-scale distributed training and inference, considering framework and container
    levels. We also study various task-scheduling strategies for overlapping computational
    and communication tasks, aiming to increase distributed training and inference
    throughput with diverse heterogeneous resources and workloads.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contemporary communication infrastructures for high-performance communication.
    We study various state-of-the-art communication infrastructure technologies on
    different layers, including GPU interconnects, programmable network devices, collective
    communication interfaces, and communication topologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A case study of various technologies for large foundation DL models in large-scale
    distributed DL. We use a question-and-answer approach to discuss applying these
    communication-efficient technologies to the distributed training of LLMs in real
    cases. This case study helps researchers identify practical, high-performance,
    and cost-effective solutions for large-model training in a large-scale and heterogeneous
    environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/320df778c60a115accab99f338312dfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The organization of the survey'
  prefs: []
  type: TYPE_NORMAL
- en: I-B Survey Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fig. [1](#S1.F1 "Figure 1 ‣ I-A Related Surveys ‣ I Introduction ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") outlines the detailed
    organization of the remaining sections in this survey. Section [II](#S2 "II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey") provides fundamental knowledge about distributed
    DL. Sections [III](#S3 "III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") and [IV](#S4 "IV
    Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") present works on communication-efficient
    algorithms for model synchronization and communication data compression in large-scale
    distributed DL, respectively. Section [V](#S5 "V Large-Scale Resource Allocation
    and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") examines various communication-efficient strategies for
    resource allocation and task scheduling in large-scale distributed training and
    inference. Section [VI](#S6 "VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") introduces works
    on communication infrastructure technologies at different system layers for high-performance
    communication in large-scale DL clusters. We present large-scale distributed training
    of large foundation DL models as a case study in Section [VII](#S7 "VII Large-Scale
    Distributed Training of Large Models: A Case Study on LLMs ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"), and conclude
    this survey in Section [VIII](#S8 "VIII Conclusion ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: A List of Common Abbreviations'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Abbreviation | Description |'
  prefs: []
  type: TYPE_TB
- en: '|   AIMD | Additive-Increase Multiplicative-Decrease |'
  prefs: []
  type: TYPE_TB
- en: '| CNN | Convolutional Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| DL | Deep Learning |'
  prefs: []
  type: TYPE_TB
- en: '| DLRM | Deep Learning Recommendation Model |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | Deep Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| FL | Federated Learning |'
  prefs: []
  type: TYPE_TB
- en: '| FPGA | Field Programmable Gate Array |'
  prefs: []
  type: TYPE_TB
- en: '| GPU | Graphics Processing Unit |'
  prefs: []
  type: TYPE_TB
- en: '| GRU | Gated Recurrent Unit |'
  prefs: []
  type: TYPE_TB
- en: '| IID | Independent and Identically Distributed |'
  prefs: []
  type: TYPE_TB
- en: '| INA | In-Network Aggregation |'
  prefs: []
  type: TYPE_TB
- en: '| IoT | Internet of Things |'
  prefs: []
  type: TYPE_TB
- en: '| IoV | Internet of Vehicles |'
  prefs: []
  type: TYPE_TB
- en: '| LARS | Layer-wise Adaptive learning Rate Scaling |'
  prefs: []
  type: TYPE_TB
- en: '| LLM | Large Language Model |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | Long Short-Term Memory |'
  prefs: []
  type: TYPE_TB
- en: '| MPI | Message Passing Interface |'
  prefs: []
  type: TYPE_TB
- en: '| MPS | Multiple Process Sharing |'
  prefs: []
  type: TYPE_TB
- en: '| NCCL | NVIDIA Collective Communications Library |'
  prefs: []
  type: TYPE_TB
- en: '| NIC | Network Interface Card |'
  prefs: []
  type: TYPE_TB
- en: '| NLP | Natural Language Processing |'
  prefs: []
  type: TYPE_TB
- en: '| PaaS | Platform as a Service |'
  prefs: []
  type: TYPE_TB
- en: '| PCIe | Peripheral Component Interconnect Express |'
  prefs: []
  type: TYPE_TB
- en: '| PS | Parameter Server |'
  prefs: []
  type: TYPE_TB
- en: '| P2P | Peer-to-Peer |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | Recurrent Neural Network |'
  prefs: []
  type: TYPE_TB
- en: '| SGD | Stochastic Gradient Descent |'
  prefs: []
  type: TYPE_TB
- en: '| SiP | Silicon Photonic |'
  prefs: []
  type: TYPE_TB
- en: '| SLO | Service-Level Objective |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |'
  prefs: []
  type: TYPE_TB
- en: II Fundamentals of DL and Distributed DL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we present the fundamentals of DL and distributed DL. Table [II](#S1.T2
    "TABLE II ‣ I-B Survey Organization ‣ I Introduction ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") includes common
    abbreviations used in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Deep Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL is a subfield of machine learning that utilizes deep artificial neural networks,
    also known as deep neural networks (DNN), to extract complex patterns from training
    data in a hierarchical manner. The trained DNN is capable to recognize/predict
    patterns in unseen data. DL has been used in various fields, including NLP [[4](#bib.bib4)],
    computer vision [[6](#bib.bib6)], and biomedical engineering [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a25cbc22bede4616288487fe93ddc64b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Fully Connected DNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98c3f9e1520fa6974b027fdd882c3c4a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) CNN
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a89c1a36e066e144d13474eac1c9ef35.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) RNN
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Common artificial neural network models for DL'
  prefs: []
  type: TYPE_NORMAL
- en: II-A1 DL models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A DNN consists of multiple hidden layers. Each layer is comprised of neurons,
    which are typically activated by non-linear functions. Based on the connections
    between neurons within and between layers, there can be various types of DNN models.
    In this survey, when referring to models or DL models, we mean DNNs unless the
    context otherwise specifies. Fig. [2](#S2.F2 "Figure 2 ‣ II-A Deep Learning ‣
    II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") illustrates three basic DNN
    models: the fully connected DNN, convolutional neural network (CNN), and recurrent
    neural network (RNN).'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Fully connected DNN: The fully connected DNN, also known as the feedforward
    neural network, constitutes a dense network with an input layer, a number of hidden
    layers, and an output layer, as depicted in Fig. [2a](#S2.F2.sf1 "In Figure 2
    ‣ II-A Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"). Neurons in a
    preceding layer connect to all neurons in the subsequent layer, and each connection
    has a learnable weight parameter indicating the strength of the connection. This
    architecture enables the fully connected DNN to capture complex relationships
    within data, finding extensive application in tasks such as classification [[42](#bib.bib42)],
    regression [[43](#bib.bib43)], and feature representation embedding [[44](#bib.bib44)].'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ CNN: CNN stands as a prevalent model designed for feature extraction
    and classification, primarily tailored for image and video data. As depicted in
    Fig. [2b](#S2.F2.sf2 "In Figure 2 ‣ II-A Deep Learning ‣ II Fundamentals of DL
    and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey"), in addition to the input and output layers, CNN comprises
    a stack of convolutional layers and pooling layers for feature extraction, succeeded
    by fully connected (FC) layers for classification. Unlike the fully connected
    layer, which assigns a weight parameter to each neuron connection, the convolutional
    layer substantially reduces the number of weight parameters by utilizing a number
    of kernels, or filters, each containing shared weights for feature extraction.
    The feature extraction process of the convolutional layer’s feature extraction
    process is empowered by the convolution operation, wherein kernels traverse the
    receptive fields of an image, extracting new features through weighted summations
    followed by a non-linear activation function. The pooling layer, typically using
    max-pooling or average-pooling functions, downsamples the data in the convolutional
    layer to reduce feature dimensions and alleviate overfitting issues. CNN has found
    widespread applications in various computer vision tasks, including image classification [[45](#bib.bib45)],
    semantic segmentation [[46](#bib.bib46)], and object detection [[47](#bib.bib47)].'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ RNN: RNN, a DL model that deals with sequential data like time-series
    data, natural language, and speech audio, is illustrated in Fig. [2c](#S2.F2.sf3
    "In Figure 2 ‣ II-A Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey"). The general architecture of RNN includes hidden units that capture and
    propagate temporal context from the input sequence to subsequent hidden unites.
    It updates continuously and utilizes the temporal context based on the current
    input and previous temporal context to make predictions. To address the challenge
    of capturing long-range temporal dependencies, two common variants of RNNs, known
    as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), have been developed,
    providing a trade-off between modeling such dependencies and reducing computation
    complexity effectively. Common applications of RNN include tasks such as time
    series forecasting [[48](#bib.bib48)], NLP [[49](#bib.bib49)], and automated planning [[50](#bib.bib50)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-A2 Training and inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training of a DL model is the process of optimizing its parameters to minimize
    the prediction error on a training data set, as determined by a specified loss
    function, or objective function. Loss functions can be either convex or non-convex,
    leading to convex or non-convex optimization problems. Training can be decomposed
    into two key processes: feedforward and backpropagation. In the feedforward process,
    training data are passed into the model’s input layer, and the output prediction
    is computed by forwarding data through the network using the current model parameters.
    In the backpropagation process, the prediction error and gradients are calculated
    with respect to the loss function, and trainable parameters are updated iteratively
    in a backward manner, optimizing the model for the minimum loss. Common optimizers
    for backpropagation updating include minibatch Stochastic Gradient Descent (SGD) [[51](#bib.bib51)],
    SGD with momentum [[52](#bib.bib52)], Adagrad [[53](#bib.bib53)], and Adam [[54](#bib.bib54)].
    The training process usually operates on batches of training data iteratively
    over multiple epochs until the model converges. A model is said to have converged
    when the training error settles to within a predefined error range, and additional
    training will not further decrease the error. After completing the training process,
    the weight parameters in the DL model are learned and fixed. Following the training
    process, there is typically a validation process for validating the performance
    of the trained model, providing information for fine-tuning hyperparameters and
    retraining the model for better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The inference process passes forward unseen data through the trained DL model
    to make predictions. Depending on the specific requirements of an application,
    the resulting prediction can be extracted either either from the output layer
    or from the predicted latent representation in an intermediate hidden layer. For
    example, in the context of network traffic analysis, an end-to-end DNN model may
    be trained to classify traffic types directly [[55](#bib.bib55)], Alternatively,
    an encoder-decoder model trained on traffic data can utilize the latent representation
    generated by the encoder for subsequent tasks such as attack detection [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: Computing tasks related to a specific portion of the DL model for specific epochs
    during the training or inference process are generally referred to as DL tasks
    in this survey, when it is not necessary to distinguish training tasks and inference
    tasks in the context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/698f7e231a9db2be5cfc57e1a365625b.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Residual Block
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/61707a34cded52657d18bc04e1f0e5df.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Transformer Block
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Fundamental blocks for deep neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: II-A3 Fundamental neural network blocks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'DL models have experienced an exponential increase in terms of both depth and
    scale, with some models comprising thousands of neural layers [[57](#bib.bib57)]
    or tens of billions of parameters [[14](#bib.bib14)]. However, the growth in the
    complexity of models has introduced various challenges, including the vanishing
    gradient problem [[58](#bib.bib58)] and issues related to training and inference
    efficiency. To overcome these challenges, certain neural network structures have
    been developed as fundamental blocks for building various DNN models that can
    capture complex patterns efficiently. In practice, a DNN model can be built easily
    by stacking these fundamental blocks on top of each other. Fig. [3](#S2.F3 "Figure
    3 ‣ II-A2 Training and inference ‣ II-A Deep Learning ‣ II Fundamentals of DL
    and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") illustrates two widely recognized fundamental neural
    network blocks: the Residual [[2](#bib.bib2)] and Transformer [[3](#bib.bib3)]
    blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Residual: The Residual block, depicted in Fig. [3a](#S2.F3.sf1 "In
    Figure 3 ‣ II-A2 Training and inference ‣ II-A Deep Learning ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey"), features a shortcut connection that adds the
    identity input to its mapping after passing through several layers. The shortcut
    connection facilitates the smooth flow of data through multiple layers, ensuring
    that important gradient updates can be propagated efficiently back to shallower
    layers. As a result, the Residual block addresses the issue of vanishing gradients
    efficiently, enabling efficient training in very deep models.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Transformer: The Transformer block is a ubiquitous choice for DL
    models in learning tasks with sequential data, such as NLP problems. It adopts
    an autoencoder architecture characterized by a combined encoder and decoder, as
    shown in Fig. [3b](#S2.F3.sf2 "In Figure 3 ‣ II-A2 Training and inference ‣ II-A
    Deep Learning ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"). The Transformer
    block benefits from the self-attention mechanism, aligning the data with the context
    and enabling the data to attend to the important parts within that context. This
    mechanism can be calculated in parallel, making the Transformer block less time-consuming
    to train compared to previous RNN architectures such as LSTM and GRU. This facilitates
    the building and training of large DL models, particularly giving rise to LLMs [[59](#bib.bib59),
    [14](#bib.bib14)], which serve as foundation models for various NLP tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Distributed DL Parallelism Modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In distributed DL, data, models, and training or inference tasks are partitioned
    across multiple processing units (commonly GPUs in the DL context) within a single
    computing node or across multiple nodes in a cluster. Training DNN models in distributed
    environments involves intensive computation and communication within a computing
    cluster, making distributed training a focus of significant research. During the
    distributed training process, leveraging computational capabilities of available
    GPUs and nodes enhances training parallelism, thereby improving efficiency. Furthermore,
    it enables the successful completion of training tasks that involve extensive
    data or models exceeding the capacity of a single GPU or node. Distributed Training
    requires intensive exchanges of model parameters or gradients across GPUs and
    nodes, where the communication process typically becomes the performance bottleneck.
    Crucially, designing communication-efficient algorithms for model synchronization,
    resource management, task scheduling, and infrastructures is essential for making
    distributed training a versatile solution for large-scale distributed DL tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06db9eb7fe5bd5e6719facce29580841.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Data Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/869697b20f758da91d67cb387ef90fb5.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Model Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fa9061f75f6c03b74707013b9a36c82.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Pipeline Parallelism
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Parallelism modes of distributed DL'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various partitioning strategies in distributed DL result in three prevalent
    parallelism modes, as illustrated in Fig. [4](#S2.F4 "Figure 4 ‣ II-B Distributed
    DL Parallelism Modes ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey"): data parallelism
    (Fig. [4a](#S2.F4.sf1 "In Figure 4 ‣ II-B Distributed DL Parallelism Modes ‣ II
    Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")), model parallelism (Fig. [4b](#S2.F4.sf2
    "In Figure 4 ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals of DL and
    Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")), and pipeline parallelism (Fig. [4c](#S2.F4.sf3 "In
    Figure 4 ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals of DL and Distributed
    DL ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: II-B1 Data parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this mode, the entire training data set is divided into several splits, which
    are then distributed across multiple GPUs within a cluster [[60](#bib.bib60)],
    aiming to increase the level of training parallelism and reduce training time.
    The DL model is replicated on each GPU, and each GPU trains a local model with
    an identical structure, concentrating on a specific partition of the data set.
    Throughout the distributed training process, these local models share their knowledge
    to update a global model using a specific model-synchronization mechanism. The
    convergence rate of distributed training is tied to the communication efficiency
    of the synchronization mechanism, considering factors such as the communication
    pattern and the underlying network infrastructure. Moreover, it is influenced
    significantly by the trade-off between the model computation and synchronization
    communication performance, including factors such as communication frequency and
    the volume of data to be transferred across GPUs and computing nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5cf8782c9df2a1990a7f56b628c2b05.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Centralized
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ece317b4c258456fbb08a2f42e9c7db4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Decentralized
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Distributed SGD'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed SGD is a ubiquitous model-synchronization mechanism employed for
    distributed training in the context of data parallelism. Fig. [5](#S2.F5 "Figure
    5 ‣ II-B1 Data parallelism ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey") illustrates two common architectures of distributed
    SGD: parameter-server-based centralized SGD and gossip-based decentralized SGD.
    In centralized SGD with a parameter server (PS) [[61](#bib.bib61)] and multiple
    workers, SGD updates from local workers are transmitted to the parameter servers,
    either synchronously or asynchronously, to determine up-to-date parameters. Subsequently,
    these updated parameters are sent back to the workers for local model updating.
    In decentralized SGD without a central PS, workers synchronize their models with
    other workers either in a gossip manner or via a hierarchical structure of multiple
    parameter servers.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B2 Model parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When an entire DL model exceeds the capacity of a single GPU or node, the model
    parallelism mode [[62](#bib.bib62)] employs a specific model-division strategy
    to separate the model into multiple neural network chunks, or called submodels.
    These submodels are then distributed across multiple GPUs within a cluster, with
    connections maintaining data communication for feedforward and backpropagation.
    This enables a seamless flow of data and gradients throughout the entire model
    between GPUs and nodes, framing the training and inference process as if it were
    operating within a unified, black-box GPU. This parallelism mode introduces several
    crucial communication considerations. Initially, there is a need for optimizing
    the division of the DL model to minimize communication overhead. Additionally,
    establishing an efficient strategy to locate submodels is essential to ensure
    an optimized communication pattern with respect to submodel dependency.
  prefs: []
  type: TYPE_NORMAL
- en: II-B3 Pipeline parallelism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pipeline parallelism mode further enhances the parallelism of DL tasks by
    reducing the complexity of submodel dependency in the model parallelism mode and
    preventing computational resources from idling [[63](#bib.bib63), [64](#bib.bib64),
    [65](#bib.bib65)]. In this mode, distributed training tasks are decomposed layer-by-layer
    into multiple subtasks, with their completions depending on previous layers. The
    associated submodels for handling the subtasks are distributed across the GPU
    cluster. Thus, when multiple batches of data flow through all submodels in a pipeline
    manner, each submodel is trained by different batches simultaneously. This mode
    is particularly applicable in the domains of edge-computing, IoT, and IoV, where
    devices have heterogeneous computational and communication capabilities to handle
    various distributed DL subtasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, these parallelism modes can be combined [[66](#bib.bib66)] to
    tackle complex DL tasks with large model structures, e.g., LLMs, which we will
    explore in a case study in Section [VII](#S7 "VII Large-Scale Distributed Training
    of Large Models: A Case Study on LLMs ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9463636999ab4fbb00db602309fb998c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Cluster/Cloud
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5cb13007afc3fb94d5fff47800201670.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Edge/Federated
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b87a41339c6b286c01d64fd32e7f47b3.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) P2P
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Typical distributed DL paradigms'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Distributed DL Paradigms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We introduce mainstream paradigms of distributed DL, discussing their characteristics,
    research challenges, and applications. Based on the communication pattern, training
    and inference locality, and hardware platform, we classify these paradigms into
    three types: cluster/cloud-based, edge-based, and peer-to-peer-based (P2P-based).
    Fig. [6](#S2.F6 "Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism
    Modes ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") depicts each paradigm, using
    solid and dashed arrows to represent inter-node communications and intra-node
    communications, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 Cluster/Cloud-based Centralized Distributed DL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The cluster-based paradigm, depicted in Fig. [6a](#S2.F6.sf1 "In Figure 6 ‣
    II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey"), is a traditional scale-out solution for centralized
    distributed DL. A cluster of computing servers, interconnected via local networks,
    provide extended computing capability for training and inference beyond what a
    single computing server can achieve. End devices, such as desktop computers, perceive
    the cluster as a unified entity, accessing its computational resources through
    unified network interfaces without being concerned with the communication mechanisms
    within the cluster. The cluster comprises four key modules: the resource manager,
    data storage manager, task scheduler, and computing framework. The resource manager
    oversees the computational resources (e.g., GPU, CPU, and memory) and network
    communication resources (e.g., network topology and bandwidth) in the cluster,
    allocating them to DL tasks. The data storage manager partitions and manages large
    training and testing data in a distributed manner, considering factors such as
    storage resource and network topology to ensure data availability, efficiency,
    and reliability. The task scheduler breaks down DL tasks into various levels of
    granularity and schedules them within the cluster, taking into account resource
    consumption, data locality, workloads, and task characteristics to improve parallelism
    and failure tolerance. The computing framework implements the kernel and API to
    run the distributed DL algorithms efficiently with available resources allocated
    by the resource manager. This paradigm is commonly employed for accelerating model
    training [[67](#bib.bib67)], accommodating large models [[62](#bib.bib62)], and
    facilitating hyperparameter tuning [[68](#bib.bib68)]. In this context, efficient
    strategies for resource management and task scheduling are essential, aiming to
    maximize computational and communication-resource utilization, as well as the
    parallelism of task execution. This optimization is crucial for achieving high
    performance in this domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cloud-based distributed DL paradigm can be viewed as an extension of the
    cluster-based paradigm, as shown in Fig. [6a](#S2.F6.sf1 "In Figure 6 ‣ II-B3
    Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes ‣ II Fundamentals
    of DL and Distributed DL ‣ Communication-Efficient Large-Scale Distributed Deep
    Learning: A Comprehensive Survey"). Cloud service providers encapsulate the cluster-based
    distributed DL platform into Platform-as-a-Service (PaaS) products and deliver
    the distributed DL services in a pay-as-you-go manner. End devices are charged
    by the resources they actually use, including storage for the training data, computational
    resources for training and inference, and traffic for network communication. Compared
    to the cluster-based paradigm, the advantages of the cloud-based paradigm are
    threefold: virtualization, elasticity, and utility. First, the cloud employs virtualization
    technologies that include resource isolation and network virtualization to facilitate
    the sharing of computational and communication resources among multiple tenants
    in the cloud, thus reducing the resource costs for tenants. Second, the cloud
    can adjust elastically resource capacities based on runtime learning workloads.
    This allows the distributed DL platform to expand to a larger scale to handle
    larger models and peak workloads or contract when not necessary to conserve resources.
    Third, the cloud typically provides more utility tools for managing and monitoring
    data, models, resources, and tasks. For instance, AWS SageMaker [[69](#bib.bib69)]
    offers visualization and model versioning tools for rapidly fine-tuning distributed
    DL models. The cloud-based paradigm is commonly applied in enterprise solutions
    across various industries, such as image and video intelligence [[70](#bib.bib70)],
    medical diagnosis [[71](#bib.bib71)], and cloud manufacturing [[72](#bib.bib72),
    [73](#bib.bib73)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Edge-based Distributed DL and Federated Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the rapid development of IoT and IoV technologies, the edge-based distributed
    DL paradigm emerges. Compared to the cluster/cloud-based paradigm, the edge-based
    paradigm follows a hierarchical architecture, which includes an additional edge
    layer connecting the cluster/cloud and heterogeneous end devices (as shown in
    Fig. [6b](#S2.F6.sf2 "In Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed
    DL Parallelism Modes ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")). The key idea
    of this paradigm is to place training and inference capabilities close to data
    to enhance the efficiency and latency of distributed DL. End devices, such as
    mobile devices and vehicles, typically have limited storage and computational
    capacities and unstable wireless network connections but require real-time data
    collection and DL inference results. The cloud paradigm may not be the ideal solution
    for such real-time tasks due to the high communication latency. The edge layer
    places edge servers close to the end devices to expand the storage, training,
    and inference capability of the end devices in an efficient manner for communication.
    The edge-based paradigm is applied widely in scenarios that demand high resource
    efficiency and low latency, such as AR/VR [[74](#bib.bib74)], intelligent transportation
    systems [[75](#bib.bib75)], and smart industry and manufacturing [[76](#bib.bib76)].'
  prefs: []
  type: TYPE_NORMAL
- en: Federated Learning (FL) has recently emerged as a rapidly growing research topic,
    primarily exploiting the capacity of edge-based paradigms in collaborative DL.
    To uphold privacy and data security while multiple parties train a global model
    collaboratively, each party trains a model with local data on its edge servers
    and shares only gradients, avoiding sharing raw data with other parties and the
    central cluster. FL finds applications in various domains, with a particular focus
    on privacy and data security issues across multiple parties, including healthcare [[77](#bib.bib77),
    [78](#bib.bib78)], finance [[79](#bib.bib79)], and autonomous driving [[80](#bib.bib80)].
    Nevertheless, FL is characterized by the heterogeneity of 1) geographical locations
    and computational and communication capacities of devices, 2) model structures,
    and 3) non-independent-and-identically-distributed (non-IID) data within each
    party. This heterogeneity poses a major challenge for achieving high-performance
    FL.
  prefs: []
  type: TYPE_NORMAL
- en: II-C3 P2P-based Decentralized Distributed DL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The P2P-based distributed DL paradigm (as shown in Fig. [6c](#S2.F6.sf3 "In
    Figure 6 ‣ II-B3 Pipeline parallelism ‣ II-B Distributed DL Parallelism Modes
    ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) utilizes decentralized networks
    for collaborative DL model training. Unlike the above paradigms that follow a
    between-layer communication pattern, the P2P-based distributed DL paradigm is
    distinguished by the communication between peers at each layer. Peers collaborate
    to complete training tasks by exchanging learning information, such as the data,
    models, parameters, or gradients, in a gossip manner. Due to its decentralized
    nature, the P2P-based paradigm is expected to demonstrate scalability and fault
    tolerance. Addressing concerns related to privacy protection, data integrity,
    and model security is crucial in both design and implementation of this paradigm.
    It finds applications in decentralized trading systems [[81](#bib.bib81)], swarm
    intelligence of autonomous vehicles [[82](#bib.bib82)], and mobile robotic systems [[83](#bib.bib83)].'
  prefs: []
  type: TYPE_NORMAL
- en: There are also hybrid distributed DL paradigms, which are mixed architectures
    combining the above paradigms for complex DL scenarios [[84](#bib.bib84)]. In
    a hybrid paradigm, communication can happen between adjacent or skipping layers,
    or among peers within each layer. It capitalizes on the computational power of
    the cluster and cloud, while maintaining low-latency through the edge and ensuring
    privacy and network robustness through the P2P-based component.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Studies on communication-efficient model-synchronization algorithms
    for large-scale distributed DL'
  prefs: []
  type: TYPE_NORMAL
- en: '|                                                  Category | Strategy&Ref.
    | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|   Distributed SGD variants ([III-A](#S3.SS1 "III-A Synchronous, Asynchronous,
    and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Synchronous ([III-A1](#S3.SS1.SSS1 "III-A1 Synchronous SGD ‣ III-A
    Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | OSP [[85](#bib.bib85)] | 2023 | Overlapping unimportant
    gradient synchronization with computation of the next iteration. |'
  prefs: []
  type: TYPE_TB
- en: '| Asynchronous ([III-A2](#S3.SS1.SSS2 "III-A2 Asynchronous SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Downpour SGD [[86](#bib.bib86)] | 2012 | Asynchronous with independent
    workers and PS shards. |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ [[87](#bib.bib87)] | 2022 | Pulling the global model eagerly before pushing
    the latest local updates. |'
  prefs: []
  type: TYPE_TB
- en: '| Stale synchronous ([III-A3](#S3.SS1.SSS3 "III-A3 Stale synchronous SGD ‣
    III-A Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | SSP [[88](#bib.bib88)] | 2013 | Allowing asynchronous
    model synchronization within a limited staleness bound. |'
  prefs: []
  type: TYPE_TB
- en: '| AdaptiveRevision [[89](#bib.bib89)] | 2014 | Adaptive learning rate for minimum
    regret bound in stale SGD. |'
  prefs: []
  type: TYPE_TB
- en: '|  | R²P [[90](#bib.bib90)] | 2019 | Workers synchronize in a fixed round-robin
    order with adaptive minibatch size to avoid network contention. |'
  prefs: []
  type: TYPE_TB
- en: '|  | HSP [[91](#bib.bib91)] | 2022 | Workers synchronize in a fixed round-robin
    order and switch between synchronous and stale modes. |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive local ([III-A4](#S3.SS1.SSS4 "III-A4 Local SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | EASGD [[92](#bib.bib92)] | 2015 | Incrementing synchronization frequencies
    iteratively. |'
  prefs: []
  type: TYPE_TB
- en: '| AdaComm [[93](#bib.bib93)] | 2019 | Low synchronization frequency first for
    fast convergence and high frequency later for lower errors. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Post-local [[94](#bib.bib94)] | 2020 | Adding large-minibatch local SGD
    as the second stage; hierarchical local SGD at different infrastructure levels.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | SlowMo [[95](#bib.bib95)] | 2020 | Local SGD with momentum updates. |'
  prefs: []
  type: TYPE_TB
- en: '| Event-triggered local ([III-A4](#S3.SS1.SSS4 "III-A4 Local SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | LAG [[96](#bib.bib96)] | 2018 | Triggering synchronization when accumulated
    parameter changes are smaller than current parameter changes. |'
  prefs: []
  type: TYPE_TB
- en: '| DETSGRAD [[97](#bib.bib97)] | 2020 | Triggering synchronization when the
    current parameter change in the PS is larger than a threshold. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FSP [[98](#bib.bib98)] | 2023 | Fitting loss into an empirical two-stage
    pattern and triggering synchronization at the end of the first stage. |'
  prefs: []
  type: TYPE_TB
- en: '| Decentralized ([III-A5](#S3.SS1.SSS5 "III-A5 Decentralized SGD ‣ III-A Synchronous,
    Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | D-PSGD [[99](#bib.bib99)] | 2017 | Theoretically and experimentally
    showing that decentralized SGD has linear speedup performance for convergence.
    |'
  prefs: []
  type: TYPE_TB
- en: '| D² [[100](#bib.bib100)] | 2018 | Aggregating gradients based on differences
    between parameters and between gradients for heterogeneous data. |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid ([III-A6](#S3.SS1.SSS6 "III-A6 Hybrid SGD for the straggler problem
    ‣ III-A Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | GSSP [[101](#bib.bib101)] | 2022 | Clustering workers
    with similar performance into multiple groups; stale SGD within a group and decentralized
    SGD across groups. |'
  prefs: []
  type: TYPE_TB
- en: '| A2S [[102](#bib.bib102)] | 2022 | Stale SGD for fast workers and asynchronous
    SGD for slow workers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASHL [[103](#bib.bib103)] | 2023 | Asynchronous local SGD for faster convergence
    at an early stage and stale SGD for model consistency at a late stage. |'
  prefs: []
  type: TYPE_TB
- en: '|   Guarantee | Local ([III-B1](#S3.SS2.SSS1 "III-B1 Convergence of local SGD
    ‣ III-B Convergence Guarantees of Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | [[104](#bib.bib104), [105](#bib.bib105), [106](#bib.bib106),
    [107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110),
    [111](#bib.bib111)] | 2018-2021 | Convergence guarantees of local SGD w.r.t. the
    number of workers, local updating iterations, entire iterations, etc. |'
  prefs: []
  type: TYPE_TB
- en: '| Asynchronous ([III-B2](#S3.SS2.SSS2 "III-B2 Convergence of asynchronous SGD
    ‣ III-B Convergence Guarantees of Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114),
    [115](#bib.bib115), [116](#bib.bib116)] | 2020-2022 | Convergence guarantees of
    asynchronous SGD w.r.t. the gradient delay, gradient noise, stationary point,
    etc. |'
  prefs: []
  type: TYPE_TB
- en: '|   Heterogeneous FL ([III-C](#S3.SS3 "III-C Model Synchronization in FL ‣
    III Communication-Efficient Model Synchronization ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Random workers ([III-C1](#S3.SS3.SSS1
    "III-C1 Randomly selected workers ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | FedAvg [[117](#bib.bib117)] | 2017 | Randomly selecting
    a fraction of workers in every synchronization round. |'
  prefs: []
  type: TYPE_TB
- en: '| NetMax [[118](#bib.bib118)] | 2021 | Optimizing probabilities of selecting
    a peer w.r.t network bandwidth status in decentralized SGD. |'
  prefs: []
  type: TYPE_TB
- en: '| Model breaking-down ([III-C2](#S3.SS3.SSS2 "III-C2 Breaking down the model
    ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | ASTW_FedAvg [[21](#bib.bib21)] | 2020 | Synchronizing shallow layers
    more frequently than deep layers. |'
  prefs: []
  type: TYPE_TB
- en: '| APF [[119](#bib.bib119)] | 2021 | Freezing stable parameters with AIMD frozen
    periods. |'
  prefs: []
  type: TYPE_TB
- en: '|  | APF# [[120](#bib.bib120)] | 2023 | Aggressive APF by freezing unstable
    parameters with a probability. |'
  prefs: []
  type: TYPE_TB
- en: '|  | YOGA [[121](#bib.bib121)] | 2023 | Selecting certain layers and peers
    based on data discrepancy and bandwidth for synchronization in decentralized SGD.
    |'
  prefs: []
  type: TYPE_TB
- en: '| FL-tailored aggregation ([III-C3](#S3.SS3.SSS3 "III-C3 FL-tailored aggregation
    strategies ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient Model
    Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | FedProx [[122](#bib.bib122)] | 2020 | Finding local
    parameters that minimize the local loss and magnitude of local updates inexactly.
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedNova [[123](#bib.bib123)] | 2020 | Computing global gradients as a scaled
    weighted sum of normalized local gradients of randomly selected workers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | CGA [[124](#bib.bib124)] | 2021 | Addressing non-IID data in decentralized
    SGD by training local and neighbor models at each worker. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AsyNG [[125](#bib.bib125)] | 2023 | Addresses non-IID data in decentralized
    SGD by selecting appropriate neighbors based on loss difference and model staleness.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hierarchical aggregation ([III-C4](#S3.SS3.SSS4 "III-C4 Hierarchical gradient
    aggregation ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Two-level [[126](#bib.bib126)] | 2019 | A global aggregator
    spawning sub-aggregators for membership maintenance and encrypted aggregation.
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedCH [[127](#bib.bib127)] | 2023 | Optimizing allocation of workers to groups
    in two-level aggregation to minimize loss with resource and time constraints.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | TT-HF [[128](#bib.bib128)] | 2021 | Adaptive learning rate and synchronization
    frequency for decentralized SGD within an edge and local SGD across edges. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Moshoit SGD [[129](#bib.bib129)] | 2021 | Clustering pairs of peers into
    different groups iteratively for decentralized SGD in environments with unstable
    networks. |'
  prefs: []
  type: TYPE_TB
- en: '| Adaptive training ([III-C5](#S3.SS3.SSS5 "III-C5 Adaptive hyperparameters
    throughout training ‣ III-C Model Synchronization in FL ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Adaptive FL [[130](#bib.bib130)] | 2019 | Optimizing
    synchronization frequency to minimize errors with heterogeneous resource constraints.
    |'
  prefs: []
  type: TYPE_TB
- en: '| FedLamp [[131](#bib.bib131)] | 2023 | Jointly optimizing synchronization
    frequency and compression ratio with convergence and resource constraints. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AdaSFL [[132](#bib.bib132)] | 2023 | Jointly optimizing synchronization
    frequency and minibatch size with convergence and resource constraints. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AAFL [[133](#bib.bib133)] | 2023 | Optimizing synchronization frequency
    with bandwidth and convergence constraints via deep reinforcement learning. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FAST [[134](#bib.bib134)] | 2023 | Optimizing synchronization frequency
    and data sampling jointly with resource and time constraints. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AMBLE [[135](#bib.bib135)] | 2022 | Different learning rate, minibatch
    size, and synchronization frequency for fast and slow workers empirically. |'
  prefs: []
  type: TYPE_TB
- en: '| Unlearning ([III-C6](#S3.SS3.SSS6 "III-C6 Ability to forget over time ‣ III-C
    Model Synchronization in FL ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | FL unlearning [[136](#bib.bib136)] | 2022 | Efficient unlearning for
    non-stationary data distribution over time. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: III Communication-Efficient Model Synchronization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Distributed SGD stands as the state-of-art algorithm for model synchronization
    in the distributed training of DNN models. The performance of different algorithms
    of distributed SGD is primarily associated to the trade-off between communication
    efficiency and model consistency. In this section, we first introduce various
    distributed SGD algorithms. Subsequently, we present theoretical analyses on the
    convergence guarantee of distributed SGD. Moreover, we discuss distributed SGD
    algorithms in large-scale FL environments, where heterogeneity across various
    aspects dominates the research focus. Finally, we summarize key lessons learned
    from existing literature, guiding future endeavors in achieving high-performance
    model synchronization for large-scale distributed DL. Table [III](#S2.T3 "TABLE
    III ‣ II-C3 P2P-based Decentralized Distributed DL ‣ II-C Distributed DL Paradigms
    ‣ II Fundamentals of DL and Distributed DL ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") provides a summary of the
    related studies.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e528e7dcf292620aa33bdcdbae7492f1.png)![Refer to caption](img/a417be9e1048ad275a00eef657b50ff4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Synchronous SGD
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/557b3bbee3e96d946575eae6f3e52dba.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Asynchronous SGD
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9fa751f4ac091a65a6769c3cef369980.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Stale SGD
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec3ac60c58892d649d87fa9fbccc6153.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Local SGD
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Typical variants of distributed SGD'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Synchronous, Asynchronous, and Other Distributed SGD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection introduces optimizations for various distributed SGD algorithms,
    including synchronous, asynchronous, stale, local, decentralized, and hybrid SGD
    algorithms. Fig. [7](#S3.F7 "Figure 7 ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") illustrates some typical distributed SGD algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Synchronous SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In fully synchronous SGD, also known as minibatch distributed SGD, every gradient
    iteration serves as a synchronization round (see Fig. [7a](#S3.F7.sf1 "In Figure
    7 ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")). Workers retrieve
    the most up-to-date model parameters from the PS during every gradient iteration,
    where the PS updates the global model based on the gradients from all workers,
    resulting in more consistent models across all workers and a more deterministic
    and coordinated training progress. However, since the training process on any
    worker is blocked until all other workers have synchronized to the same progress,
    this strictly consistent-pace strategy introduces significant communication overhead,
    such as idle time caused by straggling workers. Some work, such as Overlapped
    Synchronization Parallel (OSP) [[85](#bib.bib85)], introduces a technique of overlapping
    computation and communication aimed at alleviating this blocking communication
    overhead of fully synchronous SGD. OSP divides the synchronization communication
    of synchronous SGD into two stages, a predecessor synchronization stage for important
    gradients and a successor stage for unimportant gradients. It enables the successor
    communication stage to overlap with the feedforward and backpropagation computation
    of the next training iteration. However, the blocking communication overhead cannot
    be eliminated completely in synchronous SGD because of the existence of a strict
    consistency constraint. This motivates the exploration of numerous variants of
    distributed SGD algorithms that relax the consistency constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Asynchronous SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The impact of blocking communication overhead of synchronous SGD can be challenging
    in large-scale distributed DL when many heterogeneous workers participate in the
    synchronization. Asynchronous SGD addresses the blocking communication overhead
    by allowing workers to synchronize their models with the PS independently, without
    waiting for gradient updates from other workers (see Fig. [7b](#S3.F7.sf2 "In
    Figure 7 ‣ III Communication-Efficient Model Synchronization ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")). Compared to
    synchronous SGD, asynchronous SGD eliminates the synchronization blocking time
    and is more suitable for large-scale distributed training in heterogeneous environments.
    To enhance asynchronous SGD for large data, Downpour SGD [[86](#bib.bib86)], a
    fully asynchronous SGD algorithm for data-parallel training with a centralized
    PS, segments the PS into multiple shards, each of which is responsible for storing
    and applying SGD updates to a portion of the global model parameters. Downpour
    SGD is asynchronous in two aspects: model replicas in workers operate independently
    of each other, which is the common form of asynchronous SGD, and PS shards also
    operate independently of each other. Downpour SGD has been verified experimentally
    to utilize network bandwidth efficiently and stabilize volatile parameters using
    the Adagrad optimizer for non-convex objectives.'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient asynchronous SGD is facilitated by a PS that optimizes computation
    and communication parallelism. PS+ [[87](#bib.bib87)] accelerates asynchronous
    SGD by decoupling gradient pushing and the successive parameter pulling operations,
    enabling workers to pull the global parameters eagerly from the PS for the next
    gradient iteration even before pushing the latest local gradients. This early-pulling
    strategy overlaps computational and communication workloads in every synchronization
    round, though it introduces additional model staleness. Nevertheless, the impact
    of staleness can be mitigated by a delay-adaptive strategy for the global learning
    rate, adjusting it smaller dynamically when the staleness grows large throughout
    the training process.
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Stale synchronous SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'However, asynchronous SGD introduces staleness among models in different workers,
    leading to less consistent models across workers, causing oscillations during
    the training process potentially. Stale synchronous SGD, a family of distributed
    SGD variants, relaxes model consistency among workers within a bounded model staleness
    (Fig. [7c](#S3.F7.sf3 "In Figure 7 ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")). This trade-off between model consistency and communication overhead
    aims for improved synchronization performance. In Stale Synchronous Parallel (SSP) [[88](#bib.bib88)],
    the PS maintains historical model parameters and gradients, enabling workers to
    retrieve a historical global model that is a bounded number of synchronization
    intervals ago. Consequently, faster workers can proceed without waiting for slower
    workers in every synchronization round. They use the latest updated model, as
    long as slower workers are not falling behind beyond the bounded interval. Ensuring
    a theoretical convergence guarantee, the bounded staleness reduces synchronization
    waiting time among workers and results in a faster convergence rate than fully
    synchronous SGD. To limit the impact of model staleness, AdaptiveRevision [[89](#bib.bib89)]
    adjusts the learning rate appropriately and efficiently for gradient updates between
    periodic synchronization rounds. This adjustment minimizes the regret bound, defined
    based on the difference between the actual and optimal model parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Workers synchronizing models with the PS at the consistent pace can lead to
    the thundering herd problem in bulk synchronization of synchronous SGD, causing
    network contention in the PS bandwidth. To address this network contention issue,
    R²P [[90](#bib.bib90)] and HSP [[91](#bib.bib91)] introduce similar schemes for
    stale synchronous SGD, where workers update models to the PS in a fixed round-robin
    order. Consequently, this minimizes contention in the PS network bandwidth, and
    models in workers are evenly staggered, indicating bounded model staleness. Additionally,
    to utilize computational resources efficiently in heterogeneous computing clusters,
    R²P assigns larger minibatch sizes for faster workers dynamically, keeping them
    engaged throughout the round-robin iteration when it is other worker’s turn to
    synchronize with the PS. In contrast, HSP allows workers to switch between synchronous
    and stale modes based on the resource utilization status.
  prefs: []
  type: TYPE_NORMAL
- en: III-A4 Local SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The above synchronous and asynchronous SGD algorithms demand frequent communication
    between workers and the PS, leading to significant communication overhead, particularly
    in large-scale clusters. To decrease communication frequency in large-scale distributed
    training, an alternative model-synchronization algorithm, known as local distributed
    SGD or local SGD, has been proposed. Local SGD allows workers to update local
    models for multiple iterations and only synchronize models with the PS periodically
    to maintain the most current global model (Fig. [7d](#S3.F7.sf4 "In Figure 7 ‣
    III Communication-Efficient Model Synchronization ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")). In addition to concerns
    about synchronization overhead, allowing local updates in the worker implies more
    exploration in the existence of local optima with distributed SGD, potentially
    resulting in enhanced optimization results. A general form of local SGD can be
    characterized by four parameters: synchronous or asynchronous synchronization
    patterns, the faction of workers performing updates in each synchronization round,
    the number of local gradient update iterations in each synchronization round (corresponding
    to the synchronization frequency), and the local minibatch size and learning rate
    used for each local iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Adaptive local SGD: Various local SGD algorithms use all workers
    in each synchronization round and adaptive settings for the synchronization frequency
    and minibatch size, whether synchronously or asynchronously. In asynchronous Elastic
    Averaging SGD (EASGD) [[92](#bib.bib92)], the synchronization frequency of each
    worker is controlled by a locally maintained clock, which is incremented by one
    after every local update iteration. The PS updates the global model in a moving
    average style for every synchronization from a worker. The stability of asynchronous
    EASGD is guaranteed theoretically, and experimental results demonstrate its significantly
    faster convergence compared to Downpour SGD. By exploring the trade-off between
    convergence and wall-clock runtime with different numbers of local update iterations,
    AdaComm [[93](#bib.bib93)] adjusts the number of local update iterations over
    time to minimize the optimization error at a given wall-clock time. AdaComm suggests
    an optimal strategy for local SGD convergence concerning synchronization frequency:
    starting with infrequent synchronization can reduce communication delay and improve
    convergence rate initially; increasing synchronization frequency gradually over
    time contributes to achieving lower optimization error. Lin et al. [[94](#bib.bib94)]
    propose two local SGD variants: Post-local SGD, aiming to reach high generalization,
    and Hierarchical Local SGD, to optimize the trade-off between computation and
    communication. Large-minibatch distributed SGD has advantages in training speed,
    but it often does not generalize as well as local SGD [[137](#bib.bib137)]. Post-local
    SGD closes this generalization gap by adding large-minibatch local SGD as the
    second training phase after the initial phase of standard minibatch distributed
    SGD. The initial phase can be viewed as the warm-up phase for tuning the learning
    rate. Hierarchical Local SGD employs local SGD as an inner loop on each layer
    of a hierarchical system comprising GPUs, computing nodes, racks, or even data
    centers. This hierarchical approach limits intensive model synchronization within
    each layer and mitigates communication overhead between different layers. SlowMo [[95](#bib.bib95)]
    is a momentum-based variant of local SGD, where the global model in the PS is
    updated through SGD with local update momentum. Supported by experimental results
    on various image and NLP benchmarks, the momentum-based local SGD is believed
    to converge faster and generalize better than vanilla local SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Event-triggered local SGD: One category of asynchronous local SGD
    algorithms is event-triggered SGD. Representative works in this category include
    Lazily Aggregated Gradient (LAG) [[96](#bib.bib96)] and Distributed Event-Triggered
    Stochastic GRAdient Descent (DETSGRAD) [[97](#bib.bib97)], with model synchronization
    triggered by a specific condition on gradients. Workers and the PS synchronize
    models only when the gradients of the local or global model change significantly.
    In the case of LAG, the synchronization is triggered in the PS or a worker when
    the accumulated parameter change within a specific period is smaller than the
    most recent parameter change in the PS or gradient change in a worker. LAG is
    proven theoretically to converge for objectives that are smooth and strongly convex,
    convex, or non-convex. In the case of DETSGRAD, the triggering condition is met
    in the PS when the current parameter change exceeds a threshold, which decreases
    as the training iteration increases. The convergence of DETSGRAD is proved under
    a series of assumptions for the non-convex objectives. Rather than using a synchronization
    trigger conditioned on gradient changes, Flexible Synchronous Parallel (FSP) [[98](#bib.bib98)]
    uses a trigger conditioned on optimization gains. This is based on an empirical
    observation that the loss change w.r.t the objective function can be divided into
    two stages, where the loss decreases linearly faster in the former stage than
    in the latter stage. FSP fits the loss pattern of the local SGD process into this
    two-stage pattern and identifies the barrier between these two stages as the optimal
    moment for model synchronization, as further local model synchronization in the
    latter stage delivers inferior optimization gain. FSP provides convergence guarantee
    for smooth and strongly convex objectives with Lipschitz-continuous gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A5 Decentralized SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The distributed SGD algorithms mentioned above are centralized approaches, featuring
    a central PS. This centralized setup is susceptible to traffic jam and single-point
    failure in a large-scale distributed DL setting. Decentralized SGD addresses this
    bottleneck by eliminating the central PS during model synchronization and typically
    relies on gossip-based communication among workers. To explore the advantage of
    decentralized SGD over centralized SGD, Lian et al. [[99](#bib.bib99)] conduct
    a theoretical and experimental comparison of the convergence performance between
    these two approaches. The results demonstrate that decentralized SGD converges
    as effectively as centralized SGD but outperforms it in terms of the maximum communication
    cost at any single point. To the best of our knowledge, this is the first work
    to offer a theoretical analysis of the linear speedup performance concerning the
    number of workers for decentralized SGD. Nevertheless, this result assumes similar
    data distributions across workers and its robustness to data in heterogeneous
    distributions remains unknown. By investigating the convergence performance of
    decentralized SGD with data variance among workers, D² [[100](#bib.bib100)] is
    proposed as a decentralized SGD variant where the parameters shared among peers
    for aggregation are calculated based on the difference between local parameters
    and the difference between gradients of consecutive iterations. This parameter
    updating strategy yields a theoretically improved convergence rate for decentralized
    SGD, particularly when dealing with heterogeneous data across workers, assuming
    Lipschitz-continuous and variance-bounded gradients.
  prefs: []
  type: TYPE_NORMAL
- en: III-A6 Hybrid SGD for the straggler problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In heterogeneous large-scale distributed DL, the issue of straggling worker
    is exacerbated, impacting both synchronous and asynchronous SGD. Workers lagging
    significantly behind others can impede the synchronization in the case of synchronous
    SGD and contribute to increased model staleness in asynchronous SGD. To address
    the straggler problem, a few studies have employed hybrid SGD approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The straggler problem can be mitigated by identifying stragglers during training
    and organizing workers into groups with varied model-synchronization strategies.
    Grouping Stale Synchronous Parallel (GSSP) [[101](#bib.bib101)] categorizes workers
    with comparable performance into specific groups and implements distinct strategies
    for intra- and inter-group model synchronization. Within each group, workers apply
    stale synchronous SGD with the group PS, whereas each group synchronizes its model
    with other groups using an asynchronous gossip approach. Consequently, GSSP mitigates
    stragglers during intra-group synchronization and improves convergence performance
    through inter-group synchronization. In contrast, A2S [[102](#bib.bib102)] applies
    distinct model-synchronization strategies to fast and slow workers. These groups
    are identified based on maintaining synchronization speeds of workers in the PS,
    fast and slow workers are separated dynamically into synchronous and asynchronous
    groups, respectively. Fast workers apply synchronous SGD with a relaxed adaptive
    synchronization barrier, akin to stale synchronous SGD. In contrast, slow workers
    apply asynchronous SGD. This approach limits the model staleness through stale
    synchronous SGD and mitigates the communication delay caused by stragglers through
    asynchronous SGD. The convergence performance of these grouping SGD approaches
    is guaranteed theoretically and experimentally.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies address the straggler problem by implementing various model-synchronization
    strategies at different training stages. To accelerate distributed training in
    heterogeneous environments with stragglers, ASHL [[103](#bib.bib103)] divides
    the training process into coarse- and fine-grained stages depending on whether
    the global loss exceeds a predefined threshold. In the coarse-grained stage, where
    the model needs to converge quickly to a certain extent, ASHL employs asynchronous
    local SGD to reduce communication frequency and waiting time for faster convergence;
    in the fine-grained stage, where the model should be refined to enhance the convergence
    bound, ASHL employs stale synchronous SGD with bounded staleness to ensure model
    consistency. The synchronization frequency of each worker in both stages are determined
    by profiling the updating speeds of workers, aiming for a roughly consistent synchronization
    pace for both fast and slow workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: A Comparison of convergence analyses of local SGD'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Ref. | Convergence Rate | Synchronization Round (R) | Setting |'
  prefs: []
  type: TYPE_TB
- en: '|   Zhou and Cong (2018) [[104](#bib.bib104)] | $\mathcal{O}(\frac{1}{\sqrt{NT}})$
    | $\Omega(T)$ | Non-convex; Lipschitz-continuous gradients |'
  prefs: []
  type: TYPE_TB
- en: '| Stich (2019) [[105](#bib.bib105)] | $\mathcal{O}(\frac{G^{2}}{NT})$ | $\Omega(N^{\frac{1}{2}}T^{\frac{1}{2}})$
    | Bounded gradient; strongly convex |'
  prefs: []
  type: TYPE_TB
- en: '| Yu et al. (2019) [[106](#bib.bib106)] | $\mathcal{O}(\frac{G^{2}}{\sqrt{NT}})$
    | $\Omega(N^{\frac{3}{4}}T^{\frac{3}{4}})$ | Bounded gradient; non-convex |'
  prefs: []
  type: TYPE_TB
- en: '| Haddadpour et al. (2019) [[107](#bib.bib107)] | $\mathcal{O}(\frac{1}{NT})$
    | $\Omega(N^{\frac{1}{3}}T^{\frac{1}{3}})$ | Non-convex under PL Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Yu and Jin (2019) [[108](#bib.bib108)] | $\mathcal{O}(\frac{1}{NT})$ | $\Omega(logT)$
    | Increasing minibatch size; non-convex under PL Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Woodworth et al. (2020) [[109](#bib.bib109)] | $\mathcal{O}(\frac{1}{\sqrt{NT}}+\frac{1}{\sqrt[3]{TR}})$
    | $\Omega(N\times poly(logT))$ | Convex |'
  prefs: []
  type: TYPE_TB
- en: '| Spiridonoff et al. (2021) [[110](#bib.bib110)] | $\mathcal{O}(\frac{1}{NT})$
    | $\Omega(N)$ | Smooth and strongly convex under PL Condition |'
  prefs: []
  type: TYPE_TB
- en: '| Wang and Joshi (2021) [[111](#bib.bib111)] | $\mathcal{O}(\frac{1}{\sqrt{NT}})$
    | $\Omega(N^{\frac{3}{2}}T^{\frac{1}{2}})$ | Non-convex |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Convergence rate, the expected different between the optimization objectives
    with the averaged model and minibatch SGD: $\mathbb{E}[f(\bar{x}_{T})-f(x^{\ast})]$,
    where $f$ is the objective function, $x_{t}$ is the local model after $t$ local
    SGD update iterations, and $x^{\ast}$ is the optimal model;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $G$, the uniform upper bound for the L2-norm of gradients;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $N$, the number of worker;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $T$, the total number of SGD update iterations at each worker.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III-B Convergence Guarantees of Distributed SGD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many studies analyze theoretical convergence guarantees of different distributed
    SGD algorithms across various optimization objectives. Two major branches of these
    analyses focus on local SGD and asynchronous SGD,
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Convergence of local SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A significant portion of these studies focus on local SGD. These works formalize
    local SGD across various optimization objective settings, studying the convergence
    performance concerning the number of workers ($N$), number of local updating iterations
    within each synchronization round, entire updating iterations ($T$) at each worker,
    and minibatch size. Therefore, they prove the scalability advantage of local SGD
    over other distributed SGD algorithms when applied in large-scale distributed
    DL. Table [IV](#S3.T4 "TABLE IV ‣ III-A6 Hybrid SGD for the straggler problem
    ‣ III-A Synchronous, Asynchronous, and Other Distributed SGD ‣ III Communication-Efficient
    Model Synchronization ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") presents a comparison of analysis results of local SGD
    in various objective settings.'
  prefs: []
  type: TYPE_NORMAL
- en: These analyses provides bounds on the convergence rate and synchronization round.
    Zhou and Cong [[104](#bib.bib104)] analyze the convergence of local SGD for non-convex
    objectives with Lipschitz-continuous gradients, under various settings of the
    learning rate and minibatch size. The analysis shows that local SGD allows larger
    learning rate, which is a common practice in large-batch training for large-scale
    distributed DL, and scales more efficiently than asynchronous SGD. Stich [[105](#bib.bib105)]
    demonstrates that both synchronous and asynchronous local SGD algorithms share
    the same convergence rate but reduce synchronization rounds by a factor of the
    root square of the number of local gradient updates compared to synchronous SGD
    for strongly convex objectives. Assuming a uniform upper bound on the L2-norm
    of gradients, local SGD can scale with a linear speedup w.r.t the number of workers
    and minibatch size. With the same bounded gradient assumption as in [[105](#bib.bib105)],
    Yu et al. [[106](#bib.bib106)] ensure convergence for non-convex objectives. However,
    the convergence rate is not proven a linearly speedup w.r.t the number of workers.
    Eliminating this bounded gradient assumption, both Haddadpour et al. [[107](#bib.bib107)]
    and Yu and Jin [[108](#bib.bib108)] enhance the analysis of [[106](#bib.bib106)].
    They offer tighter bounds on both the convergence rate and synchronization round
    than previous analyses and prove a linear speedup for non-convex objectives under
    the Polyak-Łojasiewicz (PL) condition [[138](#bib.bib138)], which can be viewed
    as a generalization of strong convexity for non-convex optimization. Compared
    to [[107](#bib.bib107)], whose asymptotic lower bound on the synchronization round
    is $\Omega(N^{\frac{1}{3}}T^{\frac{1}{3}})$, the analysis of [[108](#bib.bib108)]
    provides a tighter bound of $\Omega(log(T))$ for local SGD with dynamically increasing
    minibatch size, implying a promising direction to scale local SGD. Woodworth et
    al. [[109](#bib.bib109)] concentrate on comparing the convergence of minibatch
    distributed SGD and local SGD through a theoretical analysis. In comparison to
    minibatch distributed SGD, local SGD exhibits superior convergence performance
    for quadratic objectives. Yet, for general convex objectives, local SGD demonstrates
    better convergence than minibatch distributed SGD with a large number of workers,
    but worse with a large minibatch size.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies investigate the convergence guarantee of an extreme case of local
    SGD known as one-shot averaging. This method uses very few, or even only a single,
    synchronization rounds at the very end of the distributed training process. In
    smooth and strongly convex problems under the PL condition, Spiridonoff et al.
    (2021) [[110](#bib.bib110)] demonstrate that one-shot averaging can converge in
    $\Omega(N)$ synchronization rounds, irrespective of the total number of gradient
    iterations $T$. This finding suggests the potential application of one-shot averaging
    to large models, typically requiring numerous training iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Some studies focus on building a unified framework for the convergence analysis
    of local SGD. Wang and Joshi [[111](#bib.bib111)] present a unified framework
    named Cooperative SGD to design variants and analyze convergence performance of
    local SGD, expanding the design space significantly by incorporating various model
    averaging protocols and adjusting the number of local updates. Cooperative SGD
    reveals that vanilla local SGD, EASGD [[92](#bib.bib92)], and decentralized local
    SGD are all special cases within this unified framework. Leveraging the convergence
    analysis of this unified framework, Cooperative SGD offers the first convergence
    guarantee for EASGD with non-convex objective functions, applicable to both IID
    and non-IID data.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: A Comparison of convergence analyses of asynchronous SGD'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Ref. | Synchronization Iteration | Setting |'
  prefs: []
  type: TYPE_TB
- en: '|   Stich and Karimireddy (2020) [[112](#bib.bib112)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{max}}{\epsilon})$
    | General quasi-convex and smooth non-convex |'
  prefs: []
  type: TYPE_TB
- en: '| Aviv et al. (2021) [[113](#bib.bib113)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | Strongly convex; delay-adaptive learning rate |'
  prefs: []
  type: TYPE_TB
- en: '| Cohen et al. (2021) [[114](#bib.bib114)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{4}}+\frac{\tau_{avg}}{\epsilon^{2}})$
    | Smooth non-convex |'
  prefs: []
  type: TYPE_TB
- en: '| Koloskova et al. (2022) [[115](#bib.bib115)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | Smooth non-convex; delay-adaptive learning rate |'
  prefs: []
  type: TYPE_TB
- en: '| Mishchenko et al. (2022) [[116](#bib.bib116)] | $\Omega(\frac{\sigma^{2}}{\epsilon^{2}}+\frac{\tau_{avg}}{\epsilon})$
    | Non-convex, convex, or strongly convex; Lipschitz-continuous gradients; |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | delay-adaptive learning rate |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\tau_{max}$, the maximum synchronization delay;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\tau_{avg}$, the average synchronization delay;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\sigma^{2}$, the upper bound on the gradient variance within a worker;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\epsilon$, the approximate stationary point, which bounds the squared gradient
    norm.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: III-B2 Convergence of asynchronous SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several studies offer theoretical convergence guarantees for asynchronous SGD,
    emphasizing the the synchronization iteration bound concerning the gradient delay,
    upper bound on gradient noise, and approximate stationary point for training termination.
    Table [V](#S3.T5 "TABLE V ‣ III-B1 Convergence of local SGD ‣ III-B Convergence
    Guarantees of Distributed SGD ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey") presents a comparison of analysis results of various asynchronous SGD
    algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Stich and Karimireddy [[112](#bib.bib112)] conduct a convergence analysis for
    asynchronous SGD involving gradient compression and error feedback, whose mechanisms
    will be introduced in Section [IV](#S4 "IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey"), and incorporating bounded gradient noise in the model for both general
    quasi-convex and smooth non-convex objectives. The complexity of synchronization
    iteration is bounded linearly by the maximum synchronization delay, indicating
    the maximum model staleness. However, these findings lack robustness in heterogeneous
    environments, where the maximum delay can surpass the average delay significantly.
    Aviv et al. [[113](#bib.bib113)] introduce a delay-adaptive learning rate scheme
    for asynchronous SGD. Smaller learning rates are assigned to updates with a larger
    delay, providing a tighter synchronization iteration bound proportional to the
    average synchronization delay rather than the maximum delay. However, the result
    is confined to strongly convex problems, and the proof relies heavily on the assumption
    of an upper bound on the variance of delays, which is can be closely relevant
    to the maximum delay. In contrast, Cohen et al. [[114](#bib.bib114)] focus on
    smooth non-convex objectives and present a synchronization iteration bound proportional
    to the average synchronization delay. However, this analysis requires twice as
    many communication rounds at every step and relies on the appropriate tuning of
    hyperparameters. Eliminating the assumption of the upper bound on the delay variance
    in [[113](#bib.bib113)] and the hyperparameter tuning in [[114](#bib.bib114)],
    Koloskova et al. [[115](#bib.bib115)] apply another delay-adaptive learning rate
    scheme, achieving the same synchronization iteration bound as in [[113](#bib.bib113)].
    Mishchenko et al. [[116](#bib.bib116)] employ analyzing techniques and achieve
    convergence guarantees similar to those of [[115](#bib.bib115)], but cover a broader
    range of optimization problems assuming Lipschitz-continuous gradients, including
    non-convex, convex, and strongly convex.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Model Synchronization in FL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model synchronization in the domain of FL presents additional challenges. Firstly,
    and most importantly, there can be a huge amount of workers, represented by various
    types of devices, exhibit heterogeneity in geographical locations, computational
    and communication-resource capacities, model structures and objectives, and data
    distributions. Secondly, the membership of workers participating in the synchronization
    can be dynamic and unstable. Thirdly, concerns arise regarding privacy and data
    security issues. These challenges are addressed by numerous studies employing
    approaches in various directions.
  prefs: []
  type: TYPE_NORMAL
- en: III-C1 Randomly selected workers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To reduce communication overhead among numerous heterogeneous workers in FL,
    FedAvg [[117](#bib.bib117)] randomly selects only a fraction of workers in each
    synchronization round of local SGD. In the context of decentralized SGD, NetMax [[118](#bib.bib118)]
    enables each worker to select a peer stochastically based on a fine-tuned probability
    for model synchronization in each round. The optimal selection probabilities for
    all workers are derived by a network monitor, aiming to minimize the total convergence
    time w.r.t the network bandwidth capacity and utilization status. While the method
    of selecting workers randomly has become the state-of-the-art practice for distributed
    SGD in FL, it does not address various other heterogeneity issues such as non-IID
    data and heterogeneous models. This limitation has prompted a series of studies
    on this topic, focusing on different strategies tailored to cope with heterogeneity
    in FL, some of which are variants of FedAvg.
  prefs: []
  type: TYPE_NORMAL
- en: III-C2 Breaking down the model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A branch of variants [[21](#bib.bib21), [119](#bib.bib119), [120](#bib.bib120),
    [121](#bib.bib121)] distinguishes model components and synchronizes specific components
    intermittently, enabling varied synchronization frequencies for different components
    and enhancing communication efficiency for large models. Leveraging the observation
    that shallow layers of DNN models capture general features, while deep layers
    acquire ad-hoc features specific to data sets, ASTW_FedAvg [[21](#bib.bib21)]
    synchronizes shallow layers more frequently than deep layers, excluding deep layers
    in certain synchronization rounds. This approach enhances FedAvg for heterogeneous
    data sets; however, the coarse division of the so-called shallow and deep layers
    is empirical and not robust. Exploiting the observation that many parameters stabilize
    before the model reaches its ultimate convergence, Adaptive Parameter Freezing
    (APF) [[119](#bib.bib119)] introduces the parameter-wise freezing scheme. In this
    scheme, specific stable local parameters are fixed in local feedforward and backpropagation
    iterations and excluded during synchronization for specified periods. The periods
    of parameter-wise freezing are not predetermined based on prior knowledge of model
    convergence behaviors. Instead, they are adjusted dynamically in an Additive-Increase
    Multiplicative-Decrease (AIMD) manner, depending on the stability of parameters
    that were previously frozen in subsequent iterations. Nevertheless, empirical
    observations [[139](#bib.bib139)] suggest that some parameters remain unstable
    even when the model reaches convergence, especially in the case of over-parameterized
    large models. This instability weakens the ability of these parameters to freeze
    and to reduce communication overhead in APF. APF# and APF++ [[120](#bib.bib120)]
    extend APF to tackle the jittering issue in over-parameterized large models by
    employing aggressive methods for parameter freezing. In APF#, unstable parameters
    have a probability of being frozen for one round, and in the more more aggressive
    APF++, both the probability and the number of freezing rounds increase over time.
  prefs: []
  type: TYPE_NORMAL
- en: However, these fine-grained distributed SGD algorithms can be computationally
    expensive. APF and its extensions offer precise controls over the trade-off between
    model consistency and communication overhead. However, maintaining parameter-wise
    freezing period introduces memory and computation overhead, with complexity is
    linear to the model size. This suggests a careful consideration of these methods
    for large models. To achieve efficient and fine-grained SGD for FL, YOGA [[121](#bib.bib121)]
    adopts a layer-wise gradient aggregation approach based on ranks in decentralized
    SGD. YOGA assigns ranks to both layers and peers, employing a greedy algorithm
    to aggregate gradients from layers and peers selectively. Layers are ranked according
    to their learning speed and discrepancy, identifying the significance of each
    layers effectively. Peers are ranked based on data distribution divergence and
    available bandwidth, addressing the issues of non-IID data and heterogeneous resource
    capacities in FL.
  prefs: []
  type: TYPE_NORMAL
- en: III-C3 FL-tailored aggregation strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some studies focus [[122](#bib.bib122), [123](#bib.bib123), [124](#bib.bib124),
    [125](#bib.bib125)] on diverse aggregation strategies to determine local and global
    parameters, addressing challenges related to heterogeneity in both data and resources
    within FL environments. FedProx [[122](#bib.bib122)], akin to FedAvg, presents
    a new local updating strategy among workers. Instead of updating local parameters
    through SGD, FedProx identifies local parameters that minimize a local proximal
    objective inexactly, which restricts the magnitude of local updates to alleviate
    model inconsistency arising from data heterogeneity. Addressing resource heterogeneity
    involves adjusting the inexact level of local parameters in each worker. This
    adjustment correlates with the number of local iterations and reduces computation
    and communication overhead of each worker during a synchronization round. However,
    the computation of finding these inexact local parameters in FedProx remains sophisticated,
    hindering its convergence rate in large-scale FL. FedNova [[123](#bib.bib123)]
    addresses this issue by dealing with inconsistent local objectives arising from
    the heterogeneity in the number of local updates and non-IID data in FL. Gradients
    for updating the global model are computed as a scaled weighted sum of the normalized
    local gradients from randomly selected workers. These weights serve as flexible
    hyperparameters unique to each worker, and the scale is associated with the number
    of local update iterations in each worker. An illustrative example of the scale
    is the weighted sum of the numbers of local update iterations.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the studies above that focus on centralized SGD, some studies
    address FL heterogeneity through decentralized SGD approaches. Cross-Gradient
    Aggregation (CGA) [[124](#bib.bib124)] tackles non-IID data by computing the so-called
    cross gradients during model aggregation. In addition to computing local gradients
    by training local models at each worker, each worker retrieves neighboring models
    and derives cross gradients, which are computed by the neighboring models using
    local data on this worker. These local and cross gradients are then projected
    into aggregated gradients using quadratic programming and updated into each model
    via SGD with momentum. CGA exhibits superior performance over many other decentralized
    SGD algorithms, especially for non-IID data. On the other hand, AsyNG [[125](#bib.bib125)]
    tackles non-IID data through the dynamical selection of suitable neighbors for
    P2P model synchronization. Selection of neighbors relies on a priority-based algorithm,
    wherein the priority of a neighbor is determined by the difference in loss and
    the gap in training iterations compared to the worker. A neighbor is deemed more
    favorable for synchronization where there is a larger loss difference and a smaller
    training iteration gap.
  prefs: []
  type: TYPE_NORMAL
- en: III-C4 Hierarchical gradient aggregation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some of studies [[126](#bib.bib126), [127](#bib.bib127), [128](#bib.bib128),
    [129](#bib.bib129)] focus on clustering workers into groups for gradient aggregation,
    similar to [[101](#bib.bib101), [102](#bib.bib102)], but with considerations for
    the heterogeneity issues in FL. To address membership and data security issues,
    Bonawitz et al. [[126](#bib.bib126)] propose a two-level hierarchical aggregating
    scheme with encrypted computation for model synchronization among numerous mobile
    devices in FL, based on FedAvg. This scheme employs a global aggregator that is
    capable of spawning several sub-aggregators, each being responsible for membership
    maintenance and model synchronization within a subset of devices. The scheme adopts
    the Secure Aggregation protocol [[140](#bib.bib140)], performing model aggregation
    in a logical incorruptible third party. This ensures encrypted model updating,
    preventing the exposure of the model to untrusted devices and cloud providers.
    To optimize the topology for hierarchical SGD, FedCH [[127](#bib.bib127)] proposes
    a two-level gradient aggregating architecture comprising a specific number of
    groups. Each group aggregates gradients from different workers within it through
    local SGD. Additionally, there is a global PS that aggregates gradients from different
    groups using asynchronous SGD. FedCH aims to determine the optimal allocation
    of workers to groups, known as the cluster topology, to minimize training loss
    while considering resource and time budget constraints within this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the grouping methods above that focus on the centralized SGD,
    certain studies focus on decentralized SGD, wherein each group employs gossip-based
    gradient aggregation. TT-HF [[128](#bib.bib128)] examines FL across wireless edge
    devices, where devices within an edge function naturally as workers belonging
    to the same group. Workers apply local SGD, synchronize their models aperiodically
    through gossip within an edge, and synchronizes models aperiodically across edges
    via a global PS. Drawing on the convergence analysis of this synchronization strategy,
    TT-HF adjusts the learning rate and synchronization frequencies within the edge
    and globally adaptively to achieve the optimal trade-off among delay, model accuracy,
    and energy consumption. To mitigate the impact of constantly joining, leaving,
    and failing participating workers with convergence guarantees, Moshoit SGD [[129](#bib.bib129)]
    clusters them into different groups for gradient aggregation iteratively in every
    synchronization round. This ensures that any pair of workers will not be clustered
    into the same group in consecutive rounds. Moshoit SGD is designed for cloud-based
    and edge-based distributed DL environments with unreliable devices and unstable
    networks. It is theoretically and experimental proven to be more effective than
    the completely gossip-based approach in these environments.
  prefs: []
  type: TYPE_NORMAL
- en: III-C5 Adaptive hyperparameters throughout training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Various studies [[130](#bib.bib130), [131](#bib.bib131), [132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135)] address issues of
    heterogeneous data and resources in FL by focusing on the adapting hyperparameters
    of distributed SGD, including the synchronization frequency, minibatch size, learning
    rate, and data-sampling strategy. Wang et al. [[130](#bib.bib130)] analyze the
    convergence bound of local SGD incorporating non-IID data initially. Based on
    this analysis, they propose a local SGD algorithm with an adaptive synchronization
    frequency, optimized to minimize errors in optimization with heterogeneous resource
    constraints in workers and the PS, common in edge-based distributed DL paradigms.
    FedLamp [[131](#bib.bib131)] and AdaSFL [[132](#bib.bib132)] employ approaches
    similar to [[130](#bib.bib130)] for determining the optimal synchronization frequency
    based on convergence bounds within heterogeneous resource constraints adaptively.
    The difference is that FedLamp optimizes the synchronization frequency and data
    compression ratio jointly, as detailed in Section [IV](#S4 "IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey"), aimed at minimizing training time while considering
    convergence and resource constraints. Conversely, AdaSFL optimizes the synchronization
    frequency and minibatch size jointly to achieve the same objective under identical
    constraints. In Adaptive Asynchronous Federated Learning (AAFL) [[133](#bib.bib133)],
    the number of local updates in each synchronization round is determined adaptively
    by an experience-driven deep reinforcement learning algorithm, minimizes the maximum
    local accumulated training time of workers while considering bandwidth budget
    and convergence performance constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, several studies have focused on reconstructing data based on sampled
    data on each worker to address the challenge of non-IID heterogeneous data in
    FL. For instance, FAST [[134](#bib.bib134)] has developed an online learning algorithm
    that optimizes the synchronization frequency and data sampling on each device
    jointly to minimize global training loss within resource and time budgets. This
    online algorithm can offer the optimized data sampling results in each iteration
    dynamically without prior knowledge of the data distribution on each worker.
  prefs: []
  type: TYPE_NORMAL
- en: There are empirical approaches in addition to the aforementioned optimization-based
    methods. In AMBLE [[135](#bib.bib135)], the learning rate, minibatch size, and
    synchronization frequency are adjusted concurrently for local updating in heterogeneous
    workers, employing empirical strategies. AMBLE formulates the local updating time
    for workers, assigning greater values to the number of local updating iterations
    and minibatch size for faster workers. The learning rate is then adjusted linearly
    in relation to the adaptive synchronization frequency and minibatch size. AMBLE
    demonstrates superior experimental prediction results compared to FedAvg in both
    IDD and non-IDD scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: III-C6 Ability to forget over time
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some studies [[136](#bib.bib136)] focus on FL heterogeneity in the temporal
    direction, distinct from the heterogeneity observed in the spatial direction.
    During incremental training, where the model retains previously learned knowledge
    while acquiring new information, the data distribution may be non-stationary over
    time. The model must learn to forget old knowledge during the retraining to acquire
    new knowledge; this ability to forget is known as unlearning. However, unlearning
    the FL model is challenging because training data are not shared with a central
    PS server. Investigating the unlearning problem in FL, Liu et al. [[136](#bib.bib136)]
    introduce a time-saving and energy-efficient retraining method. They utilize the
    first-order Taylor expansion to approximate the objective and the diagonal empirical
    Fisher Information Matrix [[141](#bib.bib141)] to approximate the inverse Hessian
    in the Quasi-Newton method. This rapid retraining method allows different FL parties
    to collaborate efficiently in the unlearning process without sharing data, thus
    erasing data samples from a trained FL model completely.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Lessons Learned toward Communication-efficient Large-scale Model Synchronization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we summarize lessons learned from developing communication-efficient
    algorithms for model synchronization in large-scale distributed DL. These insights
    have ensued a number of interesting research topics.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ The bottleneck lies in communication overhead, and employing large-batch
    training can accelerate large-scale distributed DL. When dealing with large data
    sets and models in distributed DL with numerous powerful workers, using a minibatch
    size that is significantly greater than those traditionally used can diminish
    communication overhead and expedite training. Because distributed SGD has been
    demonstrated to exhibit linear convergence speedup w.r.t the number of workers
    in various optimization settings, large-batch training can fully exploit the computational
    power offered by high parallelism in a large-scale cluster. However, large-batch
    training poses challenges that need attention, including increased memory requirements,
    potential decay in model generalization, and the need for more complicated hyperparameter
    tuning. For instance, a staleness-aware learning rate is proposed to adjust the
    learning rate of different models adaptively during different training stages,
    aiming to coordinate the model consistency across workers in large-batch distributed
    training.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Heterogeneity is inevitable, and distributed SGD must adapt to heterogeneous
    environments dynamically. Various forms of heterogeneity are unavoidable in large-scale
    distributed DL. For optimal performance, distributed SGD should adjust its hyperparameters
    to match the characteristics of data, models, resources, and even the temporal
    dynamics of these factors. The analysis of theoretical convergence guarantees
    for new adaptive distributed SGD algorithms can follow existing analyzing frameworks
    presented in other literature, controlling certain hyperparameters as fixed variables
    while keeping other dynamic. Challenges of this approach include efficient profiling
    of these characteristics, the development of efficient algorithms to search for
    optimal solutions in dynamic training environment, and ensuring robustness across
    diverse workloads, including those with non-IID and non-stationary data sets.
    The isolation and opacity of environmental characteristics in FL scenarios further
    complicate finding a practical solution to address these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Stragglers pose obstacles, and hierarchical SGD can alleviate the
    straggler problem caused by heterogeneous device resources effectively. In the
    large-scale distributed DL scenario with a considerable number of heterogeneous
    devices having diverse computational and communication resources, devices can
    be clustered into different groups based on their resource capacities, localities,
    and data profiles. Hierarchical SGD, which applies different model-synchronization
    strategies for different groups at different levels, can limit the impact of stragglers
    to a minimal extent. However, this strategy also poses challenges pending to be
    tackled. Examples include the complexity in devising the optimal clustering topology,
    balancing workload, and ensuring model consistency across groups and levels. Additionally,
    the strategy must demonstrate adaptability to dynamic device membership throughout
    the entire training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Studies on communication-efficient compression techniques for large-scale
    distributed DL'
  prefs: []
  type: TYPE_NORMAL
- en: '|                                                 Category | Strategy&Ref.
    | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|   Quantization ([IV-A](#S4.SS1 "IV-A Gradient Quantization ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Error-feedback ([IV-A1](#S4.SS1.SSS1 "IV-A1 Deterministic
    quantization with error feedback ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | 1-bit SGD [[142](#bib.bib142)] | 2014 | The first
    quantization for distributed DL, reducing gradients to 1 bit with error feedback.
    |'
  prefs: []
  type: TYPE_TB
- en: '| SignSGD [[143](#bib.bib143)] | 2018 | Quantizing gradients to 1-bit signs
    based on majority votes among workers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | EF-SignSGD [[144](#bib.bib144)] | 2019 | SignSGD with error feedback.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 1-bit Adam [[145](#bib.bib145)] | 2021 | Applying 1-bit quantization with
    error feedback to the Adam optimizer. |'
  prefs: []
  type: TYPE_TB
- en: '| Stochastic ([IV-A2](#S4.SS1.SSS2 "IV-A2 Stochastic quantization ‣ IV-A Gradient
    Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Gupta et al. [[146](#bib.bib146)]
    | 2015 | Stochastically rounding to 16-bit computation. |'
  prefs: []
  type: TYPE_TB
- en: '| DoReFa-Net [[147](#bib.bib147)] | 2016 | I ntroducing noise in uniform distribution
    to compensate quantization errors. |'
  prefs: []
  type: TYPE_TB
- en: '|  | QNN [[148](#bib.bib148)] | 2017 | Stochastic binary quantization based
    on a hard sigmoid probability function. |'
  prefs: []
  type: TYPE_TB
- en: '|  | ZipML [[149](#bib.bib149)] | 2017 | Stochastic quantization based on empirical
    distributions derived via double sampling. |'
  prefs: []
  type: TYPE_TB
- en: '|  | NaturalComp [[150](#bib.bib150)] | 2022 | Stochastically rounding gradients
    to the upper or lower nearest powers of two. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Suresh et al. [[151](#bib.bib151)] | 2017 | Quantizing rotated gradients
    multiplied by a random rotation matrix, eliminating gradient distribution assumption.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Matrix decomposition ([IV-A3](#S4.SS1.SSS3 "IV-A3 Matrix decomposition quantization
    ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | ATOMO [[152](#bib.bib152)]
    | 2018 | Applying entry-wise or singular value decomposition (SVD) on gradients.
    |'
  prefs: []
  type: TYPE_TB
- en: '| PowerSGD [[153](#bib.bib153)] | 2019 | Applying two low-rank matrices for
    decomposing and composing gradients efficiently. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Vogels et al. [[154](#bib.bib154)] | 2020 | Increasing the number of rank
    1 power iterations to enhance PowerSGD for higher rank quantization approximation.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PCA-AWFL [[155](#bib.bib155)] | 2023 | Leveraging principle component
    analysis to reduce uplink gradients in wireless FL scenarios. |'
  prefs: []
  type: TYPE_TB
- en: '| Guarantee ([IV-A4](#S4.SS1.SSS4 "IV-A4 Convergence guarantees for quantization
    ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | [[156](#bib.bib156),
    [157](#bib.bib157), [158](#bib.bib158), [159](#bib.bib159), [160](#bib.bib160),
    [161](#bib.bib161), [162](#bib.bib162)] | 2017-2022 | Analyses of theoretical
    convergence guarantees of quantization w.r.t. the quantization level. |'
  prefs: []
  type: TYPE_TB
- en: '| Quantization for FL ([IV-A5](#S4.SS1.SSS5 "IV-A5 Gradient quantization in
    FL ‣ IV-A Gradient Quantization ‣ IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | AQG [[163](#bib.bib163)] | 2022 | Adaptive quantization level; skipping
    stable quantized gradients and amplifying certain quantized gradient regarding
    device dropouts. |'
  prefs: []
  type: TYPE_TB
- en: '| AdaGQ [[164](#bib.bib164)] | 2023 | Adaptive quantization level for each
    training iteration on each device. |'
  prefs: []
  type: TYPE_TB
- en: '|   Sparsification ([IV-B](#S4.SS2 "IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Threshold ([IV-B1](#S4.SS2.SSS1 "IV-B1 Sparsification
    with a threshold ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient Data
    Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A
    Comprehensive Survey")) | Strom [[165](#bib.bib165)] | 2015 | The first sparsification
    method for DL, selecting gradients beyond a fixed threshold bound and computing
    residuals of these gradients and the threshold. |'
  prefs: []
  type: TYPE_TB
- en: '|  | top-$k$ [[166](#bib.bib166)] | 2017 | top-$k$ sparsification dropping
    a significantly large portion of gradients. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mem-SGD [[167](#bib.bib167)] | 2018 | Sparsification with error feedback
    for distributed SGD. |'
  prefs: []
  type: TYPE_TB
- en: '|  | DGC [[168](#bib.bib168)] | 2018 | top-$k$ sparsification with momentum
    correction, local gradient clipping, momentum factor masking, and warmup training
    for SGD with momentum. |'
  prefs: []
  type: TYPE_TB
- en: '|  | EGC [[169](#bib.bib169)] | 2021 | Sparsification based on layer-wise thresholds
    determined by the entropy of gradient bins of each layer. |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIPD [[170](#bib.bib170)] | 2022 | Sparsification based on layer-wise
    thresholds determined by the L2-Norm of gradients of each layer. |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability considerations ([IV-B2](#S4.SS2.SSS2 "IV-B2 Scalability considerations
    for sparsification ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Global top-$k$ [[171](#bib.bib171)] | 2019 | Selecting
    a portion of globally largest gradients across all workers and aggregating sparsified
    gradients hierarchically. |'
  prefs: []
  type: TYPE_TB
- en: '| ScaleCom [[172](#bib.bib172)] | 2020 | Workers taking turns to be the leading
    worker and using its local top-$k$ selection indices for the selection in all
    other workers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | SIDCo [[173](#bib.bib173)] | 2021 | Sampling and matching gradients to
    some heuristic distribution, whose optimal sparsification threshold is determined
    empirically. |'
  prefs: []
  type: TYPE_TB
- en: '|  | MSTopK [[174](#bib.bib174)] | 2021 | Using binary search to determine
    an approximate sparsification threshold efficiently. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ok-Topk [[175](#bib.bib175)] | 2022 | Global top-$k$ with adaptive threshold.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | JointSpar [[176](#bib.bib176)] | 2022 | Dropping certain layers based
    on a probability distribution. |'
  prefs: []
  type: TYPE_TB
- en: '| Guarantee ([IV-B3](#S4.SS2.SSS3 "IV-B3 Convergence guarantees for sparsification
    ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient Data Compression ‣
    Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | [[177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180)]
    | 2018-2021 | Analyses of theoretical convergence guarantees of sparsification
    w.r.t. the sparsification level. |'
  prefs: []
  type: TYPE_TB
- en: '| Communication-computation trade-off ([IV-B4](#S4.SS2.SSS4 "IV-B4 Communication-computation
    trade-off optimization ‣ IV-B Gradient Sparsification ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | OMGS-SGD [[181](#bib.bib181)] | 2020 | Optimizing
    scheduling of layer-wise aggregation of top-$k$ sparsification to overlap communication
    with computation. |'
  prefs: []
  type: TYPE_TB
- en: '| DRAGONN [[182](#bib.bib182)] | 2022 | Applying sparsification when the communication
    benefit surpasses computation overhead. |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsification for FL ([IV-B5](#S4.SS2.SSS5 "IV-B5 Sparsification in FL ‣
    IV-B Gradient Sparsification ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | STC [[183](#bib.bib183)]
    | 2020 | top-$k$ sparsification and ternary quantization for bidirectional communication
    data compression in FL. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GossipFL [[184](#bib.bib184)] | 2022 | Synchronizing sparsified gradients
    among workers in a gossip manner based on a bandwidth-aware matrix. |'
  prefs: []
  type: TYPE_TB
- en: '|  | QSFL [[185](#bib.bib185)] | 2022 | Worker-level sparsification and model-level
    sparsification. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FAB-top-$k$ [[186](#bib.bib186)] | 2020 | Optimizing the sparsification
    level to minimize training time with constraints of the fairness of sparsity among
    workers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FedDD [[187](#bib.bib187)] | 2023 | Optimizing the sparsification level
    to minimize training time with constraints of heterogeneous resources, data, and
    models. |'
  prefs: []
  type: TYPE_TB
- en: '|   Others ([IV-C](#S4.SS3 "IV-C Other Gradient Compression Technologies ‣
    IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Combined methods ([IV-C1](#S4.SS3.SSS1
    "IV-C1 Combining quantization and sparsification ‣ IV-C Other Gradient Compression
    Technologies ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | [[165](#bib.bib165),
    [166](#bib.bib166), [188](#bib.bib188), [179](#bib.bib179), [170](#bib.bib170)]
    | 2015-2022 | Sequentially applying sparsification and quantization. |'
  prefs: []
  type: TYPE_TB
- en: '| Guarantee ([IV-C2](#S4.SS3.SSS2 "IV-C2 Convergence guarantees for quantization
    and sparsification combination ‣ IV-C Other Gradient Compression Technologies
    ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | [[189](#bib.bib189), [190](#bib.bib190),
    [191](#bib.bib191)] | 2018-2022 | Analyses of theoretical convergence guarantees
    of combining quantization and sparsification. |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsification with encoding ([IV-C3](#S4.SS3.SSS3 "IV-C3 Combining sparsification
    with compression encoding ‣ IV-C Other Gradient Compression Technologies ‣ IV
    Communication-Efficient Data Compression ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | 3LC [[192](#bib.bib192)]
    | 2019 | Combining sparsification and quantization with lossless encoding algorithms.
    |'
  prefs: []
  type: TYPE_TB
- en: '| DFS [[193](#bib.bib193)] | 2023 | Encoding sparsified gradient blocks into
    a zero-compacted format for bidirectional communication. |'
  prefs: []
  type: TYPE_TB
- en: '| Residual ([IV-C4](#S4.SS3.SSS4 "IV-C4 Residual compression ‣ IV-C Other Gradient
    Compression Technologies ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | ResFed [[194](#bib.bib194)]
    | 2023 | Compressing residuals based on the difference of a sequence of updated
    model for bidirectional communication. |'
  prefs: []
  type: TYPE_TB
- en: '| Autoencoder ([IV-C5](#S4.SS3.SSS5 "IV-C5 Autoencoder compression ‣ IV-C Other
    Gradient Compression Technologies ‣ IV Communication-Efficient Data Compression
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | LGC [[195](#bib.bib195)] | 2021 | Auto-encoding the common component
    of sparsified gradients. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: IV Communication-Efficient Data Compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compressing data, including model parameters and gradients, can reduce communication
    overhead across workers and the PS during distributed training and inference effectively.
    This process entails two crucial trade-offs: the balance between computation and
    communication, and that between accuracy and workloads. The former trade-off expects
    that allocating computational resources for compression can significantly enhance
    communication performance and consequently, overall efficiency. This requires
    compression algorithms to be computationally efficient. The latter anticipates
    reduced communication overhead or training and inference time, without significantly
    compromising model convergence performance and prediction accuracy. As listed
    in Table [VI](#S3.T6 "TABLE VI ‣ III-D Lessons Learned toward Communication-efficient
    Large-scale Model Synchronization ‣ III Communication-Efficient Model Synchronization
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey"), communication data compression technologies for large-scale distributed
    DL mainly fall into the following categories: gradient quantization, gradient
    sparsification, and others. Fig. [8](#S4.F8 "Figure 8 ‣ IV Communication-Efficient
    Data Compression ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey") illustrates an example of these compression technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: This section reviews existing works in these categories, with a special focus
    on applying communication-efficient data compression technologies for distributed
    DL in large-scale scenarios. We also summarize lessons learned from these works
    for future high-performance communication data compression technologies in large-scale
    distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0692c6d921595d8266f26ffc5fd336ce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An example of applying various communication data compression technologies
    to the data in distributed DL'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Gradient Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient quantization [[196](#bib.bib196)], or quantization, is a lossy compression
    technique used in distributed training, particularly the data-parallel training
    with distributed SGD. Exchanging high-precision gradients across the cluster and
    updating the model iteratively consume computational and communication resources
    intensively. Quantization methods reduce the data, typically gradients, originally
    represented by 32- or 64-bit floating-point values into lower-precision values
    with fewer bits. This helps mitigate computation and communication overhead for
    large-scale distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: Various quantization methods primarily differ in the number of bits used and
    the data-rounding strategy. Common bit numbers for quantized gradients, also referred
    to as quantization levels, include 16 [[146](#bib.bib146)], 9 [[150](#bib.bib150)],
    8 [[197](#bib.bib197)], 2 [[158](#bib.bib158)], 1 [[143](#bib.bib143)], or of
    variable length [[151](#bib.bib151)]. Typical data-rounding strategies encompass
    deterministic rounding with error feedback, stochastic rounding, and matrix decomposition.
    This subsection initially introduces quantization methods categorized by various
    data-rounding strategies, subsequently presents theoretical analyses on the convergence
    performance of quantization, and finally, discusses special considerations for
    applying quantization to FL in large-scale and heterogeneous settings.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A1 Deterministic quantization with error feedback
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some quantization methods utilize deterministic functions to reduce model parameters
    or gradients into lower-precision values. The 1-bit SGD method [[142](#bib.bib142)]
    is recognized as the first quantization method with error feedback, or error compensation,
    for distributed DL. 1-bit SGD condenses 32-bit gradients into one bit per value
    by using a constant quantization threshold of 0. To uphold model accuracy, its
    error-feedback mechanism ensures that previous quantization errors are reflected
    in the subsequent minibatch gradient quantization. Similarly, SignSGD [[143](#bib.bib143)]
    transmits only 1-bit signs of gradients. It employs a majority-vote strategy to
    aggregate these gradient signs in the PS before broadcasting them back to workers.
    SignSGD demonstrates efficient convergence in non-convex problems using optimizers
    of SGD and SGD with momentum, particularly when gradients exhibit density comparable
    to, or greater than than stochasticity and curvature. However, Karimireddy et
    al. [[144](#bib.bib144)] show that SignSGD may fail to converge in certain convex
    problems or exhibit poor generalization due to the biased nature of the sign quantization.
    To address this bias, they introduce EF-SignSGD, incorporating an error-feedback
    mechanism akin to 1-bit SGD [[142](#bib.bib142)] into SignSGD. This modification
    results in the model converging as rapidly as SGD and demonstrate superior generalization
    compared to SignSGD.
  prefs: []
  type: TYPE_NORMAL
- en: 'The quantization with error feedback needs adaptation when applied to large
    models. Though error feedback can compensate for accumulated gradient errors for
    SGD and SGD with momentum, it cannot address the non-linear gradient errors in
    the Adam optimizer. The Adam optimizer is utilized widely in Transformer-based
    models such as Bert [[5](#bib.bib5)] and LLMs [[14](#bib.bib14)]. To overcome
    this limitation in quantization for Adam optimizers, 1-bit Adam [[145](#bib.bib145)]
    capitalizes the observation that Adam’s variance remains stable in the early stage
    of distributed training. It divides the distributed training process into two
    phases: Adam warmup and momentum quantization. The warmup phase employs vanilla
    Adam for a few epochs. Subsequently, the Adam’s variance term becomes fixed, and
    the momentum quantization phase utilizes the SGD with momentum optimizer, applying
    1-bit quantization with error feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: Without randomization, deterministic quantization methods are easy to compute.
    However, biased quantization errors are commonly acknowledged as a prevalent issue
    in most deterministic methods. To mitigate quantization bias, the error-feedback
    mechanism is one approach; nevertheless, the focus of numerous studies is on stochastic
    quantization methods to address this problem.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A2 Stochastic quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Stochastic quantization methods usually make distribution assumptions on the
    data and employ stochastic functions to ensure that quantized gradients serve
    as unbiased approximations to the original gradients. To the best of our knowledge,
    Gupta et al. [[146](#bib.bib146)] are the first to introduce a stochastic rounding
    scheme to gradient quantization, which condenses distributed training into 16-bit
    fixed point computation. The likelihood of rounding a floating-point value to
    a specific fixed-point value is proportional to the proximity of these two values.
    Deterministic and stochastic methods can also be combined. DoReFa-Net [[147](#bib.bib147)]
    quantizes CNN model parameters and activations deterministically, while quantizing
    gradients stochastically. When quantizing gradients, DoReFa-Net introduces gradient
    noise in a uniform distribution to compensate for the quantization error. However,
    assuming a uniform distribution for gradient errors in the stochastic functions
    of these methods may not fully eliminate quantization bias.
  prefs: []
  type: TYPE_NORMAL
- en: To address the limitation of the uniform distribution assumption on gradient
    errors, some studies opt for alternative distributions to ensure unbiased quantization.
    QNN [[148](#bib.bib148)] also quantizes gradients into binary values, similar
    to [[142](#bib.bib142)], but bases it on a hard sigmoid probability function.
    This stochastic binarization method is verified to reduce training time and memory
    consumption significantly without compromising the model prediction accuracy.
    ZipML [[149](#bib.bib149)] employs an empirical distribution of gradients, derived
    through a double sampling strategy, to minimize stochastic quantization variance
    in linear models. Nevertheless, its performance on non-linear models is not well
    investigated. Natural Compression [[150](#bib.bib150)] rounds gradients stochastically
    to the nearest powers of two, whether upper or lower. In contrast, Suresh et al. [[151](#bib.bib151)]
    propose a stochastic rotated quantization method that makes no assumption on the
    data distribution. This method generates a random rotation matrix to rotate gradients
    and quantizes the rotated results, yielding a lower mean square error (MSE) between
    the original and quantized gradients. To further reduce the MSE, a variable-length
    coding approach is approach, encoding quantized data into variable-length formats.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A3 Matrix decomposition quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, some studies have proposed to apply quantization to the decomposed
    matrices of gradients. ATOMO [[152](#bib.bib152)] applies entry-wise or singular
    value decomposition (SVD) on gradients, and the decomposed results are sparsified
    subsequently. ATOMO is an unbiased compression scheme and represents a generalized
    version of QSGD [[156](#bib.bib156)] and TernGrad [[158](#bib.bib158)]. However,
    deriving SVD iteratively is computationally expensive. To address this issue,
    PowerSGD [[153](#bib.bib153)] employs a low-rank gradient compressor based on
    the power iteration [[198](#bib.bib198)]. This method introduces two low-rank
    matrices for quantization and the decomposition and composition involve only low-rank
    matrix multiplication and orthogonalization, ensuring computation efficiency.
    Claiming to be the pioneering gradient-quantization method, it achieves end-to-end
    wall-clock training speedup compared to vanilla SGD, enhancing scalability to
    large clusters, and exhibits superior generalization performance. On one hand,
    in contrast to prior works such as sign-based quantization with a majority vote [[143](#bib.bib143),
    [144](#bib.bib144)], which require a gather operation, PowerSGD allows the hierarchical
    addition of the decomposed matrix. This approach enables efficient leverage of
    communication optimization for the allreduce primitive in a large-scale cluster.
    On the other hand, a low-rank gradient update can be interpreted as a spectral
    regularization [[199](#bib.bib199)], contributing to the model’s enhanced generalization.
    A follow-up study by Vogels et al. [[154](#bib.bib154)] enhances the practicality
    of PowerSGD by achieving a higher-rank quantization approximation through increasing
    the number of rank-1 power iterations. In the scenarios of wireless federated
    learning, where the uplink transmission from the worker to the PS is a communication
    bottleneck, PCA-AWFL [[155](#bib.bib155)] leverages principle component analysis
    to reduce the dimension of uploaded gradients of Nesterov’s momentum.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A4 Convergence guarantees for quantization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lower-precision computation through quantization can degrade the convergence
    performance of DL models, thereby reducing prediction accuracy. Consequently,
    minimizing communication costs when ensuring theoretic guarantees on convergence
    performance becomes a primary focus of quantization methods. Alistarh et al. [[156](#bib.bib156)]
    analyze tight bounds on the trade-off between precision and quantization variance
    trade-off, and propose QSGD, a family of stochastic gradient-quantization algorithms.
    QSGD provides theoretical guarantees on convergence rate with a bound on the gradient
    precision. In line with QSGD, Konečnỳ and Richtárik [[157](#bib.bib157)] investigate
    the trade-off between the communication cost and MSE through a family of stochastic
    quantization methods. These methods vary in terms of variable-size or fixed-size
    encoding, as well as dense or sparse communication protocols. This study provides
    quantization MSE bounds associated with various communication costs. Wen et al. [[158](#bib.bib158)]
    further prove the convergence from the perspective of statistical bound on gradients.
    Based on this convergence analysis, they propose a special case of QSGD, known
    as TernGrad, which map gradients stochastically into ternary values: positive,
    zero, and negative. Workers and the PS synchronize these ternary values along
    with a gradient scalar. Subsequently, this scalar multiplies the ternary values
    in each worker, producing actual gradients for updating models. ECQ-SGD [[159](#bib.bib159)]
    further combines the error-feedback mechanism with stochastic gradient quantization,
    ensuring a tighter bound on the worst-case error compared to stochastic quantization
    methods without error feedback, such as QSGD. NUQSGD [[160](#bib.bib160)] offers
    strictly tighter bounds on the trade-off between communication cost and quantization
    variance than QSGD. This improvement is achieved by substituting the uniform quantization
    scheme in QSGD with an unbiased nonuniform logarithmic scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: In the pursuit of adaptive quantization, Faghri et al. [[161](#bib.bib161)]
    suggest quantization methods that adjust quantization levels among different iterations
    dynamically based on runtime statistics of data distribution. IntSGD [[162](#bib.bib162)]
    employs an adaptive scaling factor to multiply gradients before quantizing them
    stochastically into integer values. These values are then scaled back for model
    updating by the reciprocal of the scaling factor. This scaling factor is calibrated
    using the moving average of model parameters, facilitating computation efficiency
    of quantization. Theoretical and empirical demonstrations show that these adaptive
    methods offer tighter variance and precision bounds than non-adaptive ones.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A5 Gradient quantization in FL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, emerging studies have focuses on gradient quantization in FL, which
    presents unique challenges. First, the heterogeneity of computing devices and
    non-IID nature of data in FL amplify the challenges for unbiased quantization.
    Second, as the norm of gradients varies during training in FL, a fixed quantization
    level failed to minimize the communication cost while guaranteeing convergence
    throughout the training process. Third, ensuring the reliability of devices in
    FL is challenging, and frequent device dropouts pose a significant concern. To
    tackle these challenges, Mao et al. [[163](#bib.bib163)] introduce Augmented Adaptive
    Quantized Gradient (Augmented AQG). Augmented AQG enables devices to skip transmitting
    slowly varying quantized gradients, addressing the heterogeneity issue. It determines
    the quantization level during training adaptively to aim for the minimum communication
    cost and amplifies certain quantized gradients appropriately to mitigate potential
    device dropouts. On the other hand, AdaGQ [[164](#bib.bib164)] addresses the fluctuating
    gradient norm problem on diverse edge devices in FL. It employs an adaptive quantization
    method, allowing each device to decide its own suitable quantization level for
    every training iteration. Both approaches improve the distributed training performance
    when dealing with non-IID data.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Gradient Sparsification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gradient sparsification, or sparsification, is another lossy mechanism orthogonal
    to gradient quantization for communication data compression in distributed training,
    particularly the data-parallel training with distributed SGD. Sparsification takes
    advantage of the observation known as gradient sparsity during backpropagation
    when updating model parameters, where gradients exhibit positive skewness, with
    many being close to zero or insignificant. It involves discarding less significant
    gradients and sparsifying more substantial gradients, aiming to reduce communication
    costs in distributed training. Typically, sparse data are encoded into index and
    values pairs, which can undergo additional quantization and compression.
  prefs: []
  type: TYPE_NORMAL
- en: In this subsection, we introduce various sparsification methods, consider their
    scalability issues, provide theoretical analyses of convergence guarantees, and
    examine the trade-off between computation and communication. Additionally, we
    discuss optimizations for sparsification in high-performance FL within large-scale
    and heterogeneous settings.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B1 Sparsification with a threshold
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, Strom [[165](#bib.bib165)] introduces the first
    gradient-sparsification method for distributed training. This method only selects
    gradients with absolute values above a fixed threshold and sends sparse residual
    values calculated by the difference between these gradients and the threshold.
    This threshold represents the sparsification level, indicating the level of sparsity
    after sparsification. Since an appropriate fixed threshold is hard to determine
    in practice, Aji and Heafield [[166](#bib.bib166)] propose an approach known as
    top-$k$ sparsification, whereby a large portion (e.g., 99%) of gradients with
    smaller values are dropped. Similar to the quantization error, sparsification
    can also accumulate gradient errors during iterative training. Mem-SGD [[167](#bib.bib167)]
    incorporates the error-feedback mechanism into sparsification to compensate for
    sparsification errors, thus avoiding error explosion. However, these methods are
    designed for SGD but may result in significant gradient errors if applied to SGD
    with momentum, which is a pervasive replacement of vanilla SGD. To ensure the
    convergence performance of sparsification when applied to SGD with momentum, DGC [[168](#bib.bib168)]
    utilizes four other techniques on top of the top-$k$ sparsification: momentum
    correction, local gradient clipping [[200](#bib.bib200)], momentum factor masking [[201](#bib.bib201)],
    and warmup training. The momentum correction technique sparsifies momentum terms
    instead of gradients to maintain the accumulated discounting factor in SGD with
    momentum. In addition, DGC applies local gradient clipping to prevent exploding
    gradients by limiting the upper bounds of gradients. It employs momentum factor
    masking to address staleness problems by preventing momentum for stale gradients.
    Furthermore, warmup training accelerates training by utilizing a smaller learning
    rate and less aggressive gradient sparsity during the early stage.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the top-$k$ sparsification methods above have limitations in identifying
    the significance of gradients. Initially, the threshold is determined based on
    the gradient distribution of the entire DNN model, which is a coarse metric because
    gradients of different layers of the model can follow different distributions.
    Moreover, the magnitude of gradients may not be the best measurement for gradient
    significance. Addressing these limitations, EGC [[169](#bib.bib169)] employs the
    layer-wise threshold, determined by the entropy of gradient bins of each layer,
    indicating the significance of that layer. In contrast, MIPD [[170](#bib.bib170)]
    determines the layer-wise sparsification threshold adaptively across different
    layers and employs the layer-wise L2-Norm as the indicator of gradient significance.
    These finer-grained approaches for determining the sparsification threshold are
    expected to contain aggregated sparsification errors within a small range and
    preserve prediction accuracy; however, calibrating the threshold for every layer
    in every training iteration adds to computation complexity, which may exacerbate
    scalability problems.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B2 Scalability considerations for sparsification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Threshold-based sparsification methods may encounter scalability problems when
    applied in large-scale clusters for three main reasons. Firstly, the local top-$k$
    methods mandate every worker to select a specific portion of gradients, resulting
    in an accumulated gradient volume that increases linearly with the number of workers
    and becomes non-negligible in large clusters. Secondly, frequent determination
    of the dynamic threshold in top-$k$ sparsification is computationally expensive,
    which hinders its application in large DNN models. Thirdly, as training with large
    batch sizes [[202](#bib.bib202)] becomes popular for large models in large clusters,
    issues such as the increasing computation cost per iteration and Layer-wise Adaptive
    learning Rate Scaling (LARS) [[203](#bib.bib203), [204](#bib.bib204)] pose new
    challenges to gradient sparsification; however, there is a lack of evidences that
    these sparsification methods work well in this scenario.
  prefs: []
  type: TYPE_NORMAL
- en: To tackle the first issue of large gradient volume, Shi et al. [[171](#bib.bib171)]
    propose a global top-$k$ sparsification method to reduce the communication cost
    in large-cluster environments. This method selects a portion of globally largest
    gradients among all workers and synchronizes through a hierarchical aggregation
    strategy. Notably, this method demonstrate significant improvements in scalability
    efficiency compared to earlier top-$k$ sparsification methods. Another relevant
    development is ScaleCom [[172](#bib.bib172)], which capitalizes on the observation
    that distributions of sparsification errors are similar among workers. ScaleCom
    employs a Cyclic Local top-$k$ (CLT-k) method, where workers take turns as the
    leading worker, utilizing its local top-$k$ selection indices for the gradient
    selection in all other workers. The calculation of CLT-k is commutative, ensuring
    efficiency in allreduce implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle the second issue of calculating overhead, SIDCo [[173](#bib.bib173)]
    employs the heuristic that DNN gradients can be approximated by three representative
    sparsity-inducing distributions. The thresholds of these heuristic distributions
    can be determined efficiently at a designated compression ratio. SIDCo concentrates
    on sampling gradients and matching them to one of the distributions, thus obtaining
    the threshold for sparsification naturally. On the other hand, MSTopK [[174](#bib.bib174)]
    introduces an approximate top-$k$ sparsification approach, which uses binary search
    to determine an approximate sparsification threshold efficiently. Similar to [[171](#bib.bib171)],
    Ok-Topk [[175](#bib.bib175)] also adopts the global top-$k$ method but only updates
    the threshold every few iterations instead of every iteration to further reduce
    the threshold computation overhead. The rationale of this periodic strategy derives
    from empirical observations that gradients change very slowly across multiple
    consecutive training iterations. In addition, collective communication libraries
    tailored for sparse data enhance collective operations of sparsified gradients
    during distributed training, as detailed in Section [VI-C3](#S6.SS3.SSS3 "VI-C3
    Collective communication for sparse gradients ‣ VI-C Inter-GPU Collective Communication
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: To address the third issue arising with LARS in large-batch training, where
    learning rates scale differently in different model layers, ScaleCom additionally
    applies low-pass filtering to accumulated sparsification errors locally to accommodate
    rapid model changes and large gradient noise caused by the scaled learning rate.
    The scalability of ScaleCom is ensured because its sparsification performance
    is independent of the number of workers and the batch size per worker. To reduce
    computation costs in large-batch training, JointSpar [[176](#bib.bib176)] sparsifies
    gradient computation and communication jointly to avoid redundant gradient computation
    for those deemed to be dropped. JointSpar first decomposes gradients into gradient
    blocks layer by layer, and then decides which gradient blocks to drop based on
    a probability distribution set over gradient blocks, where each element indicates
    the probability of dropping a specific gradient block. JointSpar avoids both computation
    and communication of these blocks to drop. The convergence performance of JointSpar
    has been demonstrated theoretically and experimentally for large-batch training
    with LARS.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B3 Convergence guarantees for sparsification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to quantization, there is a trade-off between convergence and communication
    in sparsification. The communication performance is associated with the sparsification
    compression ratio, i.e., sparsification level. The convergence performance of
    the sparsification methods above is supported by experimental results on commodity
    cloud infrastructures, but many methods lack theoretical guarantees. In an effort
    to develop sparsification methods with theoretical convergence guarantees, GSpar [[177](#bib.bib177)]
    adopts a random approach, which drops gradients with probabilities and amplifies
    the remaining gradients to ensure unbiased sparsification. The probabilities are
    calibrated to minimize the communication cost after sparsification while adhering
    to a specific gradient error variance constraint. Alistarh et al. [[178](#bib.bib178)]
    provide the first convergence theoretical analysis of the top-$k$ sparsification.
    By giving non-trivial upper bounds on the convergence rate, this analysis demonstrates
    that top-$k$ sparsification methods provide convergence guarantees for distributed
    SGD with either convex or non-convex training objectives. Wang et al. [[179](#bib.bib179)]
    analyze the convergence performance of top-$k$ sparsification with respect to
    the compression ratio. They propose a Fast-Fourier-Transform-based (FFT-based)
    method for top-$k$ sparsification, employing FFT to transform gradients into the
    frequency domain and drops low-energy gradients according to a compression ratio
    that is bound to converge. Compared to previous gradient quantization and top-$k$
    sparsification methods, the distribution of reconstructed gradients using this
    FFT-based method is closer to that of the original gradients prior to sparsification,
    indicating that the FFT-based method preserves more relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: When local or global top-$k$ sparsification methods apply constant thresholds
    across different training iterations, they result in sub-optimal convergence or
    communication performance. To guarantee the convergence performance in the presence
    of varying thresholds across different training iterations, Sahu et al. [[180](#bib.bib180)]
    propose an adaptive top-$k$ sparsification method with a communication constraint.
    Given a communication budget for the entire training, this method selects the
    threshold adaptively for each iteration, with the goal of minimizing the accumulated
    sparsification error of the entire training. A theoretical analysis shows that
    it converges for both convex and non-convex objectives.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B4 Communication-computation trade-off optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides concerns about convergence, there are also concerns about the computation
    overhead introduced by sparsification, such as the additional encoding and synchronization
    overhead for sparsified gradients. To tackle the trade-off between computation
    and communication in gradient sparsification, OMGS-SGD [[181](#bib.bib181)] defines
    top-$k$ sparsification as an optimization problem of scheduling a set of layer-wise
    aggregation tasks of sparsified gradients. These tasks of different gradient layers
    involve computation and communication stages and can be scheduled in a pipeline,
    allowing the communication stage of an earlier task can overlap with the computation
    stage of subsequent tasks. OMGS-SGD find the optimal scheduling of layer-wise
    gradient aggregation, aiming to minimize the training time. From an analytical
    perspective, DRAGONN [[182](#bib.bib182)] implements sparsification selectively,
    activating it only when the estimated communication time saved exceeds the computing
    overhead associated with sparsification. DRAGONN also minimizes the sparsification
    computing overhead through the use of an efficient hashing algorithm to obtain
    gradients above the threshold, instead of relying on the mask-based (e.g., Hadamard-product-based)
    selection algorithm widely adopted by other sparsification methods.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B5 Sparsification in FL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similar to quantization, challenges regarding heterogeneity of computing devices,
    non-IID data, and communication patterns in FL also present in gradient sparsification.
    Numerous gradient-sparsification methods have been developed to address the FL
    heterogeneity from various perspectives. STC [[183](#bib.bib183)] represents the
    pioneering work in addressing bidirectional communication data compression in
    FL. When many studies focus gradient quantization in the worker-to-server direction,
    STC employs top-$k$ sparsification and ternary quantization for compressing communication
    in both the worker-to-server uploading and server-to-worker downloading. Focusing
    on the heterogeneity in bandwidth and communication pattern in FL, GossipFL [[184](#bib.bib184)]
    utilizes a bandwidth-aware gossip matrix to guide how a peer worker synchronizes
    sparsified gradients with a single other peer in a gossip-based P2P communication
    pattern in FL. This gossip matrix is designed based on the bandwidth information
    among peer workers for high-bandwidth communication, thus requiring little communication
    time in the synchronization. Concerning the heterogeneity of gradient contributions
    among workers in FL, QSFL [[185](#bib.bib185)] employs a two-level gradient-sparsification
    method: worker-level sparsification for worker-to-server communication and model-level
    sparsification for server-to-worker communication. On the worker level, QSFL selects
    qualified workers for uploading gradients to the PS server considering both the
    worker’s contribution to the loss and the relevance between the local worker model
    and the global server model. On the model level, QSFL divides the global synchronized
    model into segments and sends one segment to each qualified worker. This worker-level
    sparsification is considered exclusive for FL scenarios where worker models can
    be heterogeneous, distinguishing it from the standard tensor-level [[166](#bib.bib166)]
    or layer-level [[176](#bib.bib176)] sparsification used in homogeneous model synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the trade-off between computation and communication in FL, Han et
    al. [[186](#bib.bib186)] present a Fairness-Aware Bidirectional top-$k$ (FAB-top-$k$)
    sparsification method, which formulates sparsification as an online learning algorithm.
    This approach determines the optimal sparsification level to minimize overall
    training time, considering factors including the estimated derivative sign, adjustable
    search interval, and fairness of sparsity among workers, thus addressing non-IID
    data issues. Similarly, FedDD [[187](#bib.bib187)] formulates sparsification in
    FL as a convex optimization problem to find the optimal sparsification level for
    minimizing the overall training time, considering the heterogeneity of device
    computational resources, data distribution and quality, and model size and structure.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Other Gradient Compression Technologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Other gradient compression technologies mainly include combining quantization
    and sparsification, integrating sparsification with compression encoding methods,
    residual compression, and autoencoder compression. This subsection introduces
    recent advancements in these technologies and provides an analysis of the convergence
    guarantees on some hybrid compression methods.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C1 Combining quantization and sparsification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gradient sparsification is usually applied in conjunction with other orthogonal
    compression technologies, such as gradient quantization. In [[165](#bib.bib165),
    [166](#bib.bib166)], sparsified gradients are quantized using 1-bit [[142](#bib.bib142)]
    and 2-bit quantization, respectively, to further reduce communication costs. Similarly,
    RedSync [[188](#bib.bib188)] applies gradient sparsification sequentially with
    a binary-searched threshold, similar to [[174](#bib.bib174)], and 1-bit quantization,
    similar to [[142](#bib.bib142)]. In [[179](#bib.bib179)], after FFT-based sparsification,
    the sparsified gradient frequency data are packed into a dense vector with another
    status vector, indicating the index locations. The dense vector is further quantized
    by a range-based quantization method, which converts 32-bit IEEE-754-format data
    to a lower-precision format with a range that matches the original format and
    has a number of bits constrained by training convergence requirements. In [[170](#bib.bib170)],
    following layer-wise sparsification, MIPD further applies quantization based on
    layer-wise gradient distributions, aiming to reduce the aggregated approximation
    error based on the gradient distribution of the entire model.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C2 Convergence guarantees for quantization and sparsification combination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Theoretical analyses are used demonstrate the convergence performance of the
    combination of sparsification and quantization. PQASGD [[189](#bib.bib189)] presents
    a special combination form of sparsification and quantization, where the synchronization
    of quantized gradients is triggered only periodically but not iteratively. This
    periodic synchronization can be perceived as the sparsification over training
    iterations but not over the model, as discussed earlier. Therefore, PAQSGD can
    be viewed as a combination of quantization and iteration-wise sparsification.
    Theoretically, this combination has been proven to converge at a rate inversely
    proportional to the square root of the product of the total minibatch size across
    all workers and the number of iterations within a period, indicating its ability
    to scale linearly as the number of worker increases. Similar to PAQSGD, Qsparse-local-SGD [[190](#bib.bib190)]
    also synchronizes gradients periodically. The difference is that in the local
    computation between synchronization intervals, each worker additionally undergoes
    ordinary gradient sparsification and quantization sequentially, while maintaining
    a local error memory for the combined error compensation. Qsparse-local-SGD has
    been demonstrated theoretically to converge for smooth non-convex objectives with
    a fixed learning rate and smooth convex objectives with a decaying learning rate.
    To adjust the compression level of the combined methods adaptively to meet the
    convergence guarantee, AC-SGD [[191](#bib.bib191)] optimizes the quantization
    and sparsification level jointly to minimize the gradient variance, regarding
    factors including the communication budget, gradient norm, and remaining number
    of iterations. This jointly optimized compression level guarantees convergence
    for non-convex and quadratic objectives.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C3 Combining sparsification with compression encoding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gradient sparsification is also usually combined with efficient encoding algorithms.
    To maximize the compression ratio, both STC [[183](#bib.bib183)] (which has been
    introduced) and 3LC [[192](#bib.bib192)] combine sparsification and quantization
    with lossless encoding algorithms. 3LC first quantizes gradients into ternary
    values using a sparsity multiplier, which indicates the threshold for rounding
    gradients to zero, one of the ternary values. It then uses lossless based-3⁵ encoding
    to compact every five ternary values into a one-byte representation, followed
    subsequently by another lossless zero-run-length encoding [[205](#bib.bib205)]
    to shorten consecutive runs of the same bytes. DFS [[193](#bib.bib193)] segments
    sparsified gradients dynamically into data blocks and encodes them into a zero-compacted
    format for bidirectional communication of model synchronization. Given the increased
    compression complexity, designing algorithms that integrate various lossy and
    lossless compression technologies for distributed SGD requires a careful trade-off
    between computation and communication performance. It is essential to ensure that
    the communication benefit provided by compression surpasses its computation overhead.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C4 Residual compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some compression technologies for distributed training adopt the residual-based
    approach. This approach exploits the heuristic that residuals calculated from
    the difference between updated and predicted models within a specific period contain
    denser information than gradients. ResFed [[194](#bib.bib194)] predicts a model
    in local iterations in the worker through memorizing the model trajectory, which
    consists of a sequence of updated models across multiple training updates, and
    then derives the residuals. It communicates residuals between workers and the
    PS server through compression methods such as sparsification and quantization.
    This residual-based compression is used for both uploading and downloading communications,
    making it appropriate for FL scenarios with constrained bandwidth. The residual-based
    sparsification and quantization can be perceived as a generalized version of the
    gradient sparsification and quantization, because when the predicted model is
    set to the last updated model, residuals essentially become gradients.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C5 Autoencoder compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some gradient-compression methods adopt a DNN autoencoder to encode gradients.
    By exploiting the observation that gradients share similarities across workers
    during distributed training [[172](#bib.bib172)], LGC [[195](#bib.bib195)] conceptually
    decomposes gradient values into two components: the common and innovation components,
    which represents the parts making gradients in a worker similar to or distinguished
    from those in other workers, respectively. The innovation component is captured
    by sparsified gradients with a very aggressive sparsification rate, e.g., 0.001%,
    The common component is generated by inputting 0.1% spasified gradients into the
    encoder side of a lightweight autoencoder, which has been trained for extracting
    the similarity among gradients of different workers during the early stage of
    distributed training. During gradient synchronization, workers and the PS server
    exchange the innovation and encoded common components, which are highly compacted
    for fast communication and can be decoded by the decoder side of the lightweight
    autoencoder efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Lessons Learned toward Communication-efficient Large-scale DL Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we explore lessons learned from developing communication-efficient
    compression algorithms for large-scale distributed DL, highlighting potential
    research problems for future studies in this area.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ The environment is complex and dynamic, and environment-aware adaptive
    compression can optimize the communication overhead in diverse settings. The compression
    ratio can be adjusted based on diverse environment information dynamically, including
    but not limited to the data distribution, model architecture, and communication
    topology. However, the diversity and heterogeneity of these factors pose challenges
    to finding efficient methods for discovering the optimal solution and providing
    real-time adaption.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Gradients vary in significance, and finer-grained assessment of gradient
    significance leads to improved overall performance in terms of convergence and
    communication in distributed DL with large models. In large models with different
    learning characteristics in different components, a coarse criterion for gradient
    significance can fail to capture important information for model updating. Utilizing
    different strategies to calibrate gradient significance at the granularity of
    edges, workers, model components, or even model layers can optimize the convergence
    performance with the minimal communication overhead. Yet, fine-grained compression
    entails a high computation overhead. The primary challenge is finding a balance
    between the computation and communication overhead.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Various compression methods can be compatible with each other, and
    hybrid and hierarchical compression methods can collaborate to reduce communication
    overhead significantly. For large-scale distributed DL, a combination of various
    lossy and lossless compression algorithms can be utilized to optimize the communication
    at different hierarchical levels of model synchronization. The convergence performance
    of the hybrid and hierarchical approach is guaranteed theoretically in certain
    optimization settings. However, there is a lack of detailed studies on the specific
    contributions of each component in the hybrid and hierarchical approach to the
    overall convergence and communication performance. This approach approach usually
    relies on empirical combinations of different compression technologies, and achieving
    the optimal combination is challenging due to the large search space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/179305ee3d0d4bfda1fafd0a1183230c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: An illustration of resource-management and task-scheduling mechanisms
    in large-scale distributed DL'
  prefs: []
  type: TYPE_NORMAL
- en: V Large-Scale Resource Allocation and Task Scheduling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section introduces resource-allocation and task-scheduling strategies
    for high-performance distributed DL at a large scale. These strategies are typically
    integrated in cluster-level and distributed-DL-level frameworks. Fig. [9](#S4.F9
    "Figure 9 ‣ IV-D Lessons Learned toward Communication-efficient Large-scale DL
    Compression ‣ IV Communication-Efficient Data Compression ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") illustrates a
    procedure of resource-allocation and task-scheduling mechanisms for large-scale
    distributed DL within a cluster. This procedure comprises six major steps. First,
    for a queue of distributed DL jobs, the task scheduler conducts job profiling
    based on various workload characters, such as resource-utilization status and
    working progress. Second, the resource manager allocates GPU and network resources
    distributed within the cluster for jobs based on their characteristic profiling.
    The resources can be represented in a physical or virtual manner, and virtual
    resources can be encapsulated in virtual machines or containers. Third, the job-level
    scheduling determines job-execution priorities based on resource constraints and
    job performance estimation. Fourth, the task-pipeline-level scheduling divides
    the job into subtasks and locates them onto available resources for the pipeline
    execution of the subtasks, aiming to increase task parallelism and overlap computational
    and communication workloads. Fifth, the network-flow-level scheduling optimizes
    the coflows of numerous subtasks by considering the relation and dependency of
    network flows. Sixth, the scheduled jobs, task pipelines, and network flows run
    efficiently on the allocated GPU and network resources within the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first introduce large-scale GPU management aimed at achieving
    efficient computational and communication-resource utilization in distributed
    DL. We then introduce task scheduling to overlap computational and communication
    tasks of distributed DL, thereby increasing parallelism. During the discussion
    of these strategies, we focus on their applications in the large-scale setting,
    with heterogeneous resource capacities and task workloads. Finally, we summarize
    some lessons learned from these strategies to assist in uncovering promising research
    directions within this scope.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Studies on resource-allocation strategies for large-scale distributed
    DL'
  prefs: []
  type: TYPE_NORMAL
- en: '|                                                  Category | Technology&Ref.
    | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|   Training ([V-A1](#S5.SS1.SSS1 "V-A1 Resource management for distributed
    training ‣ V-A Resource Management ‣ V Large-Scale Resource Allocation and Task
    Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A
    Comprehensive Survey")) | GPU Sharing. Focus: (1) production cluster; (2) context
    switching; (3) performance estimate; (4) elasticity; (5) hyperparameter tuning
    | Gandiva [[206](#bib.bib206)] | 2018 | (1) Using the profiles of the DL workload
    to improve efficiency of training DL models and latency in a GPU cluster. |'
  prefs: []
  type: TYPE_TB
- en: '| AntMan [[207](#bib.bib207)] | 2020 | (1) Introducing co-designing the cluster
    scheduler and dynamic scaling mechanisms. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FGD [[208](#bib.bib208)] | 2023 | (1) Monitoring the individual evaluation
    functions of DL jobs at runtime to make placement decisions and resource allocations
    elastically. |'
  prefs: []
  type: TYPE_TB
- en: '|  | TGS [[209](#bib.bib209)] | 2023 | (1) Designing adaptive rate-control
    and transparent unified-memory mechanisms . |'
  prefs: []
  type: TYPE_TB
- en: '|  | Salus [[210](#bib.bib210)] | 2019 | (2) Achieving fine-grained GPU sharing
    among multiple DL applications. |'
  prefs: []
  type: TYPE_TB
- en: '|  | PipeSwitch [[211](#bib.bib211)] | 2020 | (2)Exploiting the profiles of
    DL applications to achieve millisecond-scale context switching. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimus [[212](#bib.bib212)] | 2018 | (3) Estimating a DL task’s remaining
    execution time and designing a marginal gain-based allocation algorithm. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Harmony [[213](#bib.bib213)] | 2019 | (3) Placing training jobs in a manner
    that minimizes interference and maximizes performance. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Horus [[214](#bib.bib214)] | 2021 | (3) Proposing a data-driven approach
    to predict the GPU utilization of heterogeneous DL tasks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pollux [[215](#bib.bib215)] | 2021 | (4) Combining system throughput with
    statistical efficiency and introducing a formulation of goodput. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zico [[216](#bib.bib216)] | 2021 | (4) Monitoring the memory-usage pattern
    of individual DL jobs by tracking computational progress of training jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AFS [[217](#bib.bib217)] | 2021 | (4) Handling future jobs requires proactive
    preparation based on current share calculations. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FlowCon [[218](#bib.bib218)] | 2023 | (4) Minimizing the growth of GPU
    fragmentation through packing tasks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fluid [[219](#bib.bib219)] | 2021 | (5) Utilizing a water-filling approach
    to accelerate the hyperparameter optimization process. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Titan [[220](#bib.bib220)] | 2022 | (5) Merging several fine-tuning workloads
    into one to improve resource utilization. |'
  prefs: []
  type: TYPE_TB
- en: '|  | DISC [[221](#bib.bib221)] | 2022 | (5) Leveraging adaptive scaling to
    adjust the size of GPU time slices and formalizing the dynamic allocation of GPU
    time slices into an optimization problem. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hydro [[222](#bib.bib222)] | 2023 | (5) Extending resources of hyperparameter
    tuning workloads by interleaving them with pipeline-enabled large-model training
    tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| Network Bandwidth Sharing. Granularity: (1) job; (2) gradient block; (3)
    coflow | Liquid [[223](#bib.bib223)] | 2021 | (1) Proposing intelligent cluster
    network-efficient scheduling methods in both immediate and batch modes. |'
  prefs: []
  type: TYPE_TB
- en: '| Prophet [[224](#bib.bib224)] | 2021 | (2) Employing the monitored network
    bandwidth and the profiled gradient time interval to predict the number of gradients
    into gradient blocks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Parrot [[225](#bib.bib225)] | 2020 | (3) Using a linear program (LP) solution
    to derive a weighted bandwidth scaling strategy to minimize the time cost in the
    communication stage. |'
  prefs: []
  type: TYPE_TB
- en: '|   Inference ([V-A2](#S5.SS1.SSS2 "V-A2 Resource management for distributed
    inference ‣ V-A Resource Management ‣ V Large-Scale Resource Allocation and Task
    Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A
    Comprehensive Survey")) | Spatial Sharing | GSLICE [[226](#bib.bib226)] | 2020
    | Developing self-learning and adaptive GPU-resource allocation and batching schemes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| iGniter [[227](#bib.bib227)] | 2022 | Leveraging inference performance model
    to calculate the appropriate batch size and lower bound of allocated GPU resources.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | SLO-aware [[228](#bib.bib228)] | 2022 | Distributing inference requests
    to the deployed functions based on the autoscaling decision. |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal Sharing | Nexus [[229](#bib.bib229)] | 2019 | Applying a heuristic
    approach to select the requests. |'
  prefs: []
  type: TYPE_TB
- en: '| INFaaS [[230](#bib.bib230)] | 2021 | Identifying the colocation interference
    caused by the shared hardware resources. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cocktail [[231](#bib.bib231)] | 2022 | Building a distributed-weighted
    auto-scaling policy that utilizes the importance sampling technique. |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Sharing | Gpulet [[232](#bib.bib232)] | 2022 | Allowing heterogeneous
    ML models to be mapped to multiple gpulets in the most cost-effective way. |'
  prefs: []
  type: TYPE_TB
- en: '| FaST-GShare [[233](#bib.bib233)] | 2023 | Introducing the FaST-Manager to
    limit and isolate spatio-temporal resources. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: V-A Resource Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to the different stages of DL, we categorize distinct resource-management
    techniques applicable to training and inference stages of DL. Table [VII](#S5.T7
    "TABLE VII ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") provides a summary
    of relevant computational and communication-resource-management technologies in
    the domain of large-scale distributed DL.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Resource management for distributed training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The training process of distributed DL requires an intensive consumption of
    computing power and memory of GPUs and network communication bandwidth across
    GPUs. Therefore, GPU and network bandwidth sharing is the focus of our discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ GPU Sharing: Although GPUs have found extensive applications in distributed
    DL, a prevalent issue of underutilization is observed in production clusters.
    The recorded low GPU utilization typically ranges from 25% to below 50% [[234](#bib.bib234),
    [235](#bib.bib235), [236](#bib.bib236), [237](#bib.bib237), [208](#bib.bib208),
    [238](#bib.bib238)]. This concern is particularly noteworthy in large-scale distributed
    computing environments. To address this issue, various distributed techniques
    have been developed to enable DL tasks to run efficiently on numerous devices [[239](#bib.bib239)].'
  prefs: []
  type: TYPE_NORMAL
- en: GPU-sharing techniques leverage partial resource allocation through virtualization,
    presenting a feasible strategy to mitigate the challenge of low GPU utilization
    in large-scale distributed DL. NVIDIA, acknowledged as the leading GPU provider,
    introduces Multiple Process Sharing (MPS) [[240](#bib.bib240)] that offers an
    operating-system-level virtualization solution. Nevertheless, its implementation
    requires application-specific expertise to define resource limits for ensuring
    performance isolation. Moreover, MPS lacks compatibility with various DL frameworks.
    To address the performance isolation issue with MPS, another NVIDIA technology,
    Multi-Instance GPU (MIG) [[241](#bib.bib241)], enables the partitioning of a GPU
    into multiple discrete instances, each with dedicated resources. However, MIG
    is exclusively available to only a certain specifications of GPUs (e.g., A100
    and H100) and primarily supports resource sharing at a coarse-grained level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some comprehensive solutions have been proposed to tackle complex characteristics
    of production clusters for large-scale distributed DL. Gandiva [[206](#bib.bib206)]
    leverages the profiles of distributed DL tasks and addresses the issue of GPU
    underutilization in three key ways. Initially, Gandiva allows incoming jobs to
    time-share GPUs with existing jobs when overloaded. Then, it permits time-sliced
    jobs to migrate to other GPUs. Lastly, it supports elastic GPU capacity, increasing
    the number of GPUs during idle times and reducing the number of GPUs as the load
    grows dynamically, thereby utilizing idle GPUs effectively. The performance of
    Gandiva is demonstrated on production clusters at Microsoft. AntMan [[207](#bib.bib207)]
    is a production solution for distributed DL clusters at Alibaba. It analyzes the
    cause of GPU underutilization in distributed DL clusters for production use in
    three aspects: hardware, cluster scheduling, and job behavior. Exploiting the
    profiles of fluctuating resource demands from distributed training jobs, AntMan
    co-designs the cluster scheduler and distributed DL framework with dynamic scaling
    mechanisms for GPU resources during job execution. This approach ensures jobs’
    service-level objectives (SLOs) in large-scale clusters while enhancing cluster
    utilization through opportunistic scheduling. Leveraging the analysis of the production
    trace at Alibaba, Fragmentation Gradient Descent (FGD) [[208](#bib.bib208)] addresses
    severe GPU fragmentation in large clusters. FGD minimizes GPU fragmentation growth
    through task packing to achieve maximum GPU allocation rates. TGS [[209](#bib.bib209)]
    provides transparent GPU sharing at OS layer for distributed DL tasks in production
    clusters of containers. TGS addresses challenges of the lack of application profiling
    knowledge and the potential oversubscription of GPU memory during the sharing
    of GPU resources. TGS tackles the first challenge by monitoring and controlling
    the rate of sending GPU kernels to the GPU for each container adaptively, aiming
    to maximize the rate of opportunistic jobs while not affecting that of production
    jobs. TGS tackles the second challenge by unifying GPU memory and host memory
    in a single address space via CUDA unified-memory allocation [[242](#bib.bib242)]
    that enables both performance isolation and transparency of GPU memory allocation.
    Oversubscribed memory of opportunistic jobs is evicted to the host memory automatically,
    ensuring the performance of production jobs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some works use fast-context switching to improve GPU utilization. Salus [[210](#bib.bib210)]
    achieves fine-grained GPU sharing with flexible scheduling policies, exposing
    two GPU-sharing primitives: fast job switching and memory sharing. The former
    enables rapid preemption and efficient time sharing for the currently active DL
    job on a GPU, whereas the latter packs smaller distributed DL tasks on the same
    device to ensure high memory utilization and prevent memory fragmentation. In
    contrast, PipeSwitch [[211](#bib.bib211)] supports fast-context switching for
    task pipelines of distributed DL jobs. PipeSwitch optimizes the context switching
    overhead through model-aware grouping for pipelines and proactive allocating of
    GPU memory. The model-aware grouping of layers aims to minimize the overhead of
    transferring the model between CPUs and GPUs during context switching. The proactive
    allocation of GPU memory for standby workers before it should be active expedites
    the speed of context switching. To prevent job interference, PipeSwitch enforces
    process-level isolation, by initialing a new separate process for each active-worker
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: Some works employ performance-estimate-guided approaches to enhance GPU-resource
    allocation. Both the performance-estimation methods and the performance goals
    can vary in these approaches. Optimus [[212](#bib.bib212)] introduces a dynamic
    allocation algorithm based on marginal gains, estimating the remaining execution
    time of a distributed DL task. In this greedy policy, a job’s larger marginal
    gain results in a higher allocation of resources. Harmony [[213](#bib.bib213)]
    uses a deep reinforcement learning algorithm to place distributed DL jobs on allocated
    GPU resources with minimum interference and minimum training time. The learning
    rewards for unseen placements are guided by historical allocation samples. Horus [[214](#bib.bib214)]
    builds a model to predict GPU utilization of heterogeneous distributed DL tasks
    from the computation graph features. It identifies GPU utilization as a general
    proxy metric for making optimal placement decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Elastic training, which involves extending and shrinking resource capacity dynamically,
    is an important strategy to improve resource utilization and save costs for distributed
    DL in the cloud environment. Some studies focus on elastic resources at the granularity
    of GPU memory. Pollux [[215](#bib.bib215)] adjusts the GPU resources available
    to distributed DL jobs dynamically, aiming to maximize the overall training goodput
    within the cluster. To improve the efficiency of GPU memory sharing, Zico [[216](#bib.bib216)]
    monitors the memory-usage patterns of individual distributed DL jobs by tracking
    computational progress during training. Based on this monitoring, Zico allocates
    and deallocates memory among concurrent jobs automatically, ensuring no exceeding
    of the memory budget. AFS [[217](#bib.bib217)] points out that handling future
    jobs requires proactive preparation of resources based on current share calculations.
    When the resource scheduler estimates that the resource contention will be heavy
    in the future, it allocates more resources to long-lasting jobs; otherwise it
    allocates more resources to short jobs. In contrast, some studies focus on elastic-container
    resources. For instance, FlowCon [[218](#bib.bib218)] introduces a container-placement
    strategy based on growth efficiency and dynamic resource configuration for elastic
    allocation and withdrawal of resources during runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Several studies explore strategies for improving resource utilization during
    hyperparameter tuning in distributed DL clusters. Fluid [[219](#bib.bib219)] is
    a distributed DL hyperparameter tuning execution engine that abstracts the hyperparameter
    tuning process as a sequence of trial groups. It employs a water-filling approach
    to expedite the hyperparameter tuning process, thereby enhancing resource utilization.
    Titan [[220](#bib.bib220)] adopts a different approach by consolidating multiple
    fine-tuning workloads into one, aiming to improve resource utilization. This consolidation
    is particularly advantageous given that multiple fine-tuning workloads often share
    the same model parameters. DISC [[221](#bib.bib221)] leverages adaptive scaling
    to adjust the size of GPU time slices occupied by hyperparameter-tuning jobs at
    runtime. This dynamic allocation of GPU time slices for each hyperparameter tuning
    job is based on its potential to create a steep increase in the inference accuracy.
    Hydro [[222](#bib.bib222)] addresses cluster-wide resource utilization and tuning
    efficiency by incorporating a heterogeneity-aware allocation strategy. This method
    extends the resources of hyperparameter-tuning workloads by interleaving them
    with pipeline-enabled large-model training tasks. By effectively utilizing idle
    time intervals on each node, caused by the gaps between the forward and backward
    processing of microbatches, Hydro enhances overall resource utilization and tuning
    efficiency in large-scale distributed DL clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Network bandwidth sharing: In large-scale distributed environments,
    network bandwidth is another significant factor determining the efficiency of
    distributed training. The focus of network bandwidth sharing can be in various
    granularities, including jobs and tasks, gradient blocks, and network coflows.'
  prefs: []
  type: TYPE_NORMAL
- en: Some work focuses on optimizing network bandwidth sharing in the granularity
    of jobs and tasks. Liquid [[223](#bib.bib223)] proposes a computational and communication-resource-estimation
    algorithm and a network-efficient job-placement strategy for distributed training
    jobs. The resource-estimation algorithm models resource requirements of distributed
    training jobs, including GPU computing power, GPU memory, and network-bandwidth
    requirements. The job-placement strategy assigns distributed training jobs to
    a cluster of computing nodes and containers, finding a best-fit job placement
    solution that satisfies the estimated computational and communication-resource
    requirements and exhibits less GPU fragmentation and network communication cost
    across containers.
  prefs: []
  type: TYPE_NORMAL
- en: Some work focuses on network bandwidth sharing in the granularity of gradient
    blocks. For instance, Prophet [[224](#bib.bib224)] groups into certain gradient
    blocks based on the profiled time interval and models the distributed training
    time in terms of the network bandwidth and order of network transfers of gradient
    blocks. Based on this model, Prophet searches for an optimal order of the network
    transfers of gradient blocks, aiming to minimize the distributed training time.
    This optimal order of gradient block transfers optimizes both the network bandwidth
    sharing among gradient blocks and the overlapping between network transfers and
    GPU computation.
  prefs: []
  type: TYPE_NORMAL
- en: Some work focuses on network bandwidth sharing in the granularity of coflows.
    For instance, Parrot [[225](#bib.bib225)] perceives the communication pattern
    of a distributed training job as a series of dependent coflows and estimates the
    remaining processing time of distributed training jobs based on the amount of
    information carried per coflow. Parrot allocates network bandwidth to active coflows
    of concurrent jobs within the cluster, so that the effective completion time of
    coflows of the job with a shorter remaining processing time has a higher priority
    to be minimized.
  prefs: []
  type: TYPE_NORMAL
- en: V-A2 Resource management for distributed inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In contrast to distributed training that caters to long-term offline workloads,
    distributed inference necessitates real-time execution with more stringent requirements
    on latency and accuracy. This difference in demands necessitates the development
    of novel resource management solutions to address the distinct characteristics
    of inference workloads effectively. In the distributed inference process, GPU
    sharing is the focus of various resource-allocation approaches that can be divided
    into two major categories: spatial sharing and temporal sharing. In the context
    of multiple distributed DL jobs, the spatial sharing of GPUs involves the sharing
    of GPU space partitions while the temporal sharing involves the sharing of computation
    time slices of an entire GPU. Hybrid approaches combine techniques from these
    two categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Spatial Sharing: Many existing works exploit spatial sharing of GPUs
    to optimize the performance of distributed inference tasks. GSLICE [[226](#bib.bib226)]
    introduces an inference system that achieves safe and efficient GPU sharing through
    spatial GPU multiplexing systematically. It utilizes MPS [[240](#bib.bib240)],
    a GPU spatial-multiplexing framework with virtualization, to handle various inference
    requests. iGniter [[227](#bib.bib227)] employs an inference performance model
    to calculate an appropriate batch size and the lower bound of allocated GPU resources.
    Subsequently, it allocates GPU resources for each inference workload by employing
    a greedy approach to identify the placement GPU devices that can achieve minimal
    performance interference. The SLO-aware ML Inference Framework [[228](#bib.bib228)]
    designs a resource auto-scaling strategy in the cloud by leveraging rich and precise
    workload-specific metrics, with a special consideration of the heterogeneity in
    the GPU computational capability. This effective and elastic management of resources
    ensures meeting the SLO for diverse inference workloads in the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Temporal Sharing: Recent temporal-sharing approaches designed for
    specific distributed inference systems have shown improvements in GPU utilization,
    especially in cloud environments shared by numerous tenants. Nexus [[229](#bib.bib229)]
    employs a heuristic approach to select requests for co-location on the same GPU.
    Initially, it determines the most suitable batch size to meet throughput and SLO
    requirements for the existing inference workloads. Subsequently, Nexus identifies
    all possible combinations within a GPU’s duty cycle on a single GPU in a best-fit
    manner, maximizing utilization without violating latency requirements. Focusing
    on inference services in the cloud, INFaaS [[230](#bib.bib230)] addresses the
    problem of co-location interference arising from shared hardware resources. It
    allocates available resources to interfered instances through workload migration
    or virtual-machine-level scaling, aiming to reduce monetary costs through GPU
    sharing while meeting latency requirements via virtual-machine-level scaling.
    Cocktail [[231](#bib.bib231)] scales the virtual machine resources for various
    inference models in the cloud automatically and proactively based on the predicted
    workload and popularity of these models. This approach enhances the efficiency
    of resource allocation in distributed DL inference systems with a specific set
    of supported inference models.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Hybrid Sharing: Several works study the hybrid GPU-sharing approaches,
    considering both spatial and temporal sharing. Gpulet [[232](#bib.bib232)] supports
    spatial sharing of GPUs via the abstraction of virtual GPUs that are split partitions
    derived from physical GPUs. Given allocated virtual GPU resources, Gpulet supports
    temporal sharing by scheduling the batch sizes of inference jobs of multiple tenants,
    with a goal to guarantee the SLO. This hybrid design enables cost-effective cloud-resource
    allocation for the inference of numerous heterogeneous DL models. FaST-GShare [[233](#bib.bib233)]
    utilizes spatial and temporal sharing of GPUs to maximize inference function throughput
    in the Function-as-a-Service serverless architecture for distributed DL. It supports
    auto-scaling of inference resources in the cloud based on the profiling of function
    throughput and resource allocation, maximizing GPU utilization while ensuring
    the SLO.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Studies on Task-Scheduling Strategies for Large-scale Distributed
    DL'
  prefs: []
  type: TYPE_NORMAL
- en: '|                                                  Category | Technology&Ref.
    | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|   Training Scheduling ([V-B](#S5.SS2 "V-B Training-Task Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Efficiency: Job-level Scheduling
    ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency ‣ V-B Training-Task Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Amaral et al. [[243](#bib.bib243)]
    | 2017 | Proposing a new topology-aware workload-placement strategy to schedule
    DL jobs on multi-GPU systems. |'
  prefs: []
  type: TYPE_TB
- en: '| Tiresias [[244](#bib.bib244)] | 2019 | Using LAS algorithm to prioritize
    jobs based on their service, a metric defined as the multiplication of requested
    GPU resources and execution time. |'
  prefs: []
  type: TYPE_TB
- en: '|  | FfDL [[245](#bib.bib245)] | 2019 | Using the operating lessons from the
    industry practice to guide the balance dependability with scalability, elasticity,
    flexibility, and efficiency. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Philly [[246](#bib.bib246)] | 2019 | Correlating scheduler logs with logs
    from individual jobs and conducting a thorough analysis about the impact of gang
    scheduling and locality constraints on the queuing delay and job runtime. |'
  prefs: []
  type: TYPE_TB
- en: '|  | E-LAS [[247](#bib.bib247)] | 2020 | Using real-time epoch progress rates
    specific to distributed training jobs, as well as services obtained from the temporal
    and spatial domains, to guide scheduling decisions |'
  prefs: []
  type: TYPE_TB
- en: '|  | SMD [[248](#bib.bib248)] | 2021 | Allowing multiple jobs to compete for
    the communication bandwidth |'
  prefs: []
  type: TYPE_TB
- en: '|  | OSDL [[249](#bib.bib249)] | 2022 | Designing job-placement and scheduling
    algorithms in networks that involve both OCS and EPS. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sched² [[250](#bib.bib250)] | 2019 | Using DRL to perform smart locality-aware
    scheduling of DLT jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | MLFS [[251](#bib.bib251)] | 2020 | Leveraging the data from the heuristic
    scheduling method for training a DRL model and making decisions on job scheduling
    using this trained DRL model automatically. |'
  prefs: []
  type: TYPE_TB
- en: '| Efficiency: Task-pipeline-level Scheduling ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency
    ‣ V-B Training-Task Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | PipeDream [[252](#bib.bib252)] | 2019 | Partitioning DNN layers among
    workers automatically to balance workload and reduce communication. |'
  prefs: []
  type: TYPE_TB
- en: '| GPipe [[253](#bib.bib253)] | 2019 | Employs an innovative batch-splitting
    pipelining algorithm, achieving nearly linear convergence speedups when a model
    is distributed across multiple accelerators. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Piper [[66](#bib.bib66)] | 2021 | Leveraging tensor parallelization techniques
    within a single layer to reduce the search space significantly. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Chimera [[63](#bib.bib63)] | 2021 | Integrating bidirectional pipelines
    for the efficient training of large-scale models, striking an optimal balance
    among pipeline efficiency, memory cost, and convergence friendliness. |'
  prefs: []
  type: TYPE_TB
- en: '|  | AutoPipe [[64](#bib.bib64)] | 2022 | Introducing a method for achieving
    balanced partitioning and reducing startup overhead automatically. |'
  prefs: []
  type: TYPE_TB
- en: '|  | OOO BackProp [[65](#bib.bib65)] | 2022 | Leveraging gradient computation
    dependencies to reorder pipeline executions, which maximize GPU resource utilization.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeAR [[254](#bib.bib254)] | 2023 | Addressing the issues of startup latency
    and sub-optimal training performance. |'
  prefs: []
  type: TYPE_TB
- en: '|  | HetPipe [[255](#bib.bib255)] | 2020 | Presenting a DNN training system
    that integrates PMP with DP. |'
  prefs: []
  type: TYPE_TB
- en: '| Efficiency: Network-flow-level Scheduling ([V-B1](#S5.SS2.SSS1 "V-B1 Efficiency
    ‣ V-B Training-Task Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | JPAS [[256](#bib.bib256)] | 2020 | Using a simple greedy mechanism
    to order all DDL jobs periodically. |'
  prefs: []
  type: TYPE_TB
- en: '| Geryon [[257](#bib.bib257)] | 2020 | Employing multiple flows with varying
    priorities to transfer parameters of different urgency levels. |'
  prefs: []
  type: TYPE_TB
- en: '|  | TensorExpress [[258](#bib.bib258)] | 2020 | Enables each switch to transmit
    tensor packets according to their priority using multiple queues. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Beamer [[259](#bib.bib259)] | 2021 | Reducing the SCT by considering stage
    information in its scheduling approach. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Tereis [[260](#bib.bib260)] | 2023 | Exploring the utilization of idle
    GPU computational resources during data transmission periods. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mercury [[261](#bib.bib261)] | 2023 | Working on packet granularity to
    shift priority scheduling to the transport layer. |'
  prefs: []
  type: TYPE_TB
- en: '| Cost-Effective ([V-B2](#S5.SS2.SSS2 "V-B2 Cost-Effective ‣ V-B Training-Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Cynthia [[262](#bib.bib262)]
    | 2019 | Providing predictable distributed training performance and reducing the
    training budget. |'
  prefs: []
  type: TYPE_TB
- en: '| FC² [[263](#bib.bib263)] | 2019 | A scheduler that recommends cost-effective
    cloud-resource allocations for distributed training tasks with a PS. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Jahani [[264](#bib.bib264)] | 2019 | Modeling the scheduling process as
    a MILP problem to reduce the leasing cost in a global manner while maintaining
    the job latency. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPOEO [[265](#bib.bib265)] | 2022 | Saving power in GPU data centers and
    using a customized scheduler to orchestrate jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | STS [[266](#bib.bib266)] | 2023 | Exploiting the probability distribution
    of early termination and adapting the resource assignment during the execution
    of the jobs to minimize the expected energy cost |'
  prefs: []
  type: TYPE_TB
- en: '| Deadline Guarantee ([V-B3](#S5.SS2.SSS3 "V-B3 Deadline Guarantee ‣ V-B Training-Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | GENIE [[267](#bib.bib267)]
    | 2020 | Proposing a prediction model derived from lightweight profiling to estimate
    the processing rate and response latency for diverse DL workloads. |'
  prefs: []
  type: TYPE_TB
- en: '| Chronus [[268](#bib.bib268)] | 2021 | Providing deadline guarantee for SLO
    jobs and maximizing the performance of best-effort jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hydra [[269](#bib.bib269)] | 2023 | Adopting a sampling approach that
    exploits the inherent iterative periodicity of DL jobs to estimate job completion
    times accurately on heterogeneous GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '|   Inference Scheduling ([V-C](#S5.SS3 "V-C Inference Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Efficiency ([V-C1](#S5.SS3.SSS1
    "V-C1 Efficiency ‣ V-C Inference Scheduling ‣ V Large-Scale Resource Allocation
    and Task Scheduling ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | Sniper [[270](#bib.bib270)] | 2022 | Using non-invasive
    performance characterization network based on neural network similarity (NNS)
    to predict the inference time of DNNs accurately. |'
  prefs: []
  type: TYPE_TB
- en: '| AutoDeep [[271](#bib.bib271)] | 2020 | Leveraging Bayesian Optimization and
    Deep Reinforcement Learning to unearth the optimal cloud configuration and device
    placement with limited search time adaptively. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Clipper [[272](#bib.bib272)] | 2017 | Observing the corresponding accuracy
    and latency feedback to make a choice to use a best-effort search approach. |'
  prefs: []
  type: TYPE_TB
- en: '| Throughout capability ([V-C2](#S5.SS3.SSS2 "V-C2 Throughout capability ‣
    V-C Inference Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | Rafiki [[273](#bib.bib273)] | 2018 | Using a practical AIMD algorithm
    to adjust inference batch size. |'
  prefs: []
  type: TYPE_TB
- en: '| Nanily [[274](#bib.bib274)] | 2019 | Deriving the corresponding batch size
    so that the inference execution time is equal to or close to the maximum remaining
    time. |'
  prefs: []
  type: TYPE_TB
- en: '|  | RRL [[275](#bib.bib275)] | 2019 | Focusing on optimizing parallel configurations
    at different levels. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Morphling [[276](#bib.bib276)] | 2021 | Adapting the meta-model to a new
    inference service by sampling a small number of configurations and using it to
    find the optimal one. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: V-B Training-Task Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In large-scale GPU clusters with complex network connections, scheduling distributed
    DL workloads effectively is critical for ensuring the high performance of task
    execution, optimal hardware utilization, and achievement of various scheduling
    objectives. Training and inference stages of distributed DL are widely recognized
    as particularly resource-intensive [[239](#bib.bib239)]. The following subsections
    study task-scheduling strategies on the training and inference workloads, respectively,
    and focus on providing efficient communication or overlapping computation and
    communication for overall efficiency in large-scale distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the distributed training of a model can consume a lot of computational and
    communication resources, efficient task-scheduling strategies for training workloads
    are crucial. The performance goals of scheduling can include efficiency, cost-effective,
    and deadline guarantees while the granularity levels can be on jobs, task pipelines,
    and network flows. In this section, we survey the task scheduling of large-scale
    distributed training with various performance goals and granularity levels. Table [VIII](#S5.T8
    "TABLE VIII ‣ V-A2 Resource management for distributed inference ‣ V-A Resource
    Management ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") summarizes the
    related literature.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B1 Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Efficiency is one of the most critical performance goals of the scheduling of
    distributed training [[239](#bib.bib239)]. This subsection categorizes task-scheduling
    strategies based on various scheduling granularity levels, including jobs, task
    pipelines, and network flows.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Job-level scheduling is one of the most common and effective scheduling
    methods that focus on reordering priorities of jobs [[244](#bib.bib244), [245](#bib.bib245)].
    Amaral et al. [[243](#bib.bib243)] introduce a novel topology-aware workload-placement
    strategy, designed for scheduling distributed DL jobs on multi-GPU systems. This
    strategy satisfies workload requirements efficiently while minimizing interference
    simultaneously. Building upon this foundation, Tiresias [[244](#bib.bib244)],
    drawing inspiration from the classic Multi-Level Feedback Queue (MLFQ) algorithm [[277](#bib.bib277)],
    develops a priority discretization approach to mitigate issues related to frequent
    preemption. A Least Attained Service (LAS) algorithm is conceptualized to prioritize
    jobs based on their service level that is quantified by the product of requested
    GPU resources and execution time. FfDL [[245](#bib.bib245)], an open-source scheduling
    platform developed by IBM, incorporates operational insights from industry practices
    to strike a balance between dependability and scalability, while maintaining elasticity,
    flexibility, and efficiency. In a related study, Philly [[246](#bib.bib246)] performs
    a comprehensive analysis by correlating scheduler logs with logs from individual
    jobs, examining the impact of gang scheduling and locality constraints on queuing
    delay and job runtime. Drawing on insights from this analysis, Philly advocates
    relaxing locality constraints to enhance job-time efficiency. E-LAS [[247](#bib.bib247)],
    with the objective of reducing the average completion time for distributed training
    jobs, shifts the focus away from reliance on job runtime estimates or prior knowledge.
    Instead, it utilizes real-time epoch progress rates specific to distributed training
    jobs, combined with service metrics derived from temporal and spatial domains,
    to inform scheduling decisions. This innovative approach enables E-LAS to surpass
    the efficiency of Tiresias. Additionally, SMD [[248](#bib.bib248)] presents a
    resource-scheduling analytical model that accommodates multiple jobs vying for
    communication bandwidth. This model treats the scheduling problem as a non-convex
    integer non-linear program with bin-packing constraints and introduces an $\epsilon$-approximation
    algorithm, termed the sum-of-retios multi-dimensional-knapsack decomposition,
    for its resolution. OSDL [[249](#bib.bib249)] designs and proposes algorithms
    for job placement and scheduling in networks that involve both optical circuit
    switching (OCS) and electrical packet switching (EPS). Simulation results demonstrate
    that OSDL surpasses multiple well-established scheduling methods in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Several scheduling methods have integrated machine-learning techniques, particularly
    deep reinforcement learning (DRL), to enhance task-scheduling efficiency. Sched² [[250](#bib.bib250)]
    utilizes DRL to schedule distributed training jobs with a locality-aware approach
    intelligently. This method is capable of understanding both the locality sensitivity
    of jobs and the fragmentation condition of clusters comprehensively within the
    entire learning stack. Through this heightened awareness, the DRL model adjusts
    its scheduling decisions dynamically and adaptively, responding effectively to
    the varying locality sensitivities of individual jobs and the evolving state of
    cluster fragmentation. MLFS [[251](#bib.bib251)] employs data from heuristic scheduling
    methods to train a DRL model, subsequently using this model to make informed decisions
    about job scheduling autonomously.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Task-pipeline-level scheduling is specialized in managing the sequential
    processing of tasks within a pipeline architecture, especially for large distributed
    training jobs with large models. It orchestrates the flow of data and tasks across
    various stages or processes expertly in a pre-defined order. By ensuring each
    task is executed efficiently and in adherence to the specified sequence, the scheduler
    optimizes overall pipeline performance. For example, PipeDream [[252](#bib.bib252)]
    introduces a system that incorporates inter-batch pipelining into intra-batch
    parallelism. This approach enhances parallel training throughput by overlapping
    computation with communication and minimizing communication when feasible. Moreover,
    PipeDream partitions DNN layers among workers automatically to balance workload
    and reduce communication. Addressing the need for efficient and task-independent
    model parallelism, GPipe [[253](#bib.bib253)] emerges as a pipeline parallelism
    library. It facilitates scaling of any network describable as a sequence of layers.
    GPipe employs an innovative batch-splitting and pipelining algorithm, achieving
    nearly linear convergence speedups when a model is distributed across multiple
    accelerators. This method offers the flexibility to scale various DNN models to
    immense sizes efficiently. Piper [[66](#bib.bib66)] aims to partition the DNN
    computation graph across numerous accelerators optimally while combining various
    parallelism modes and optimizations. As an efficient optimization algorithm based
    on two-level dynamic programming, Piper leverages tensor parallelization within
    a single layer to reduce the search space significantly. Chimera [[63](#bib.bib63)]
    integrates bidirectional pipelines for efficient training of large-scale models,
    striking an optimal balance among pipeline efficiency, memory cost, and convergence
    friendliness. AutoPipe [[64](#bib.bib64)] introduces a method for achieving balanced
    partitioning and reducing startup overhead automatically. It includes a planner
    for rapid and automated generation of balanced pipeline partition schemes and
    a micro-batch slicer that adjusts micro-batches in line with planner outcomes
    to minimize pipeline startup overhead. Out-Of-Order (OOO) BackProp [[65](#bib.bib65)]
    leverages gradient computation dependencies to reorder pipeline executions, maximizing
    GPU-resource utilization. In data-parallel training, OOO reorders the sequence
    of gradient computations to optimize the overlap between computation and parameter
    communication. In pipeline-parallel training, it prioritizes critical gradient
    computations to minimize pipeline stalls. DeAR [[254](#bib.bib254)] proposes decoupling
    the all-reduce primitive into two continuous operations, enabling both backpropagation
    and feed-forward computations without extra communication. This method addresses
    the issues of startup latency and sub-optimal training performance. For heterogeneous
    pipelines, HetPipe [[255](#bib.bib255)] presents a DNN training system that integrates
    pipelined model parallelism (PMP) with data parallelism (DP). In this system,
    a wave-synchronous-parallel approach is proposed to support both PMP and DP for
    virtual workers.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Network-flow-level scheduling manages network bandwidth effectively,
    reduces latency, and ensures efficient utilization of network resources for network
    flows related to distributed DL jobs. By adeptly balancing these critical elements,
    the scheduler enhances both performance and reliability of network operations
    significantly. JPAS [[256](#bib.bib256)] implements a straightforward greedy mechanism
    to organize all distributed training jobs periodically. This approach enables
    each host machine to prioritize its network flows according to the established
    job order, delegating the task of flow scheduling and rate allocation to the underlying
    priority-enabled networks. Geryon [[257](#bib.bib257)] employs multiple flows
    with varying priorities to transfer parameters of different urgency levels. This
    approach coordinates multiple parameter servers effectively and gives precedence
    to urgent parameter transfers across the entire network fabric. To address in-network
    delays, such as queuing delays, TensorExpress [[258](#bib.bib258)] enables each
    switch to transmit tensor packets according to their priorities using multiple
    queues. This method ensures that high-priority data packets are handled efficiently
    to minimize delays. Beamer [[259](#bib.bib259)] focuses on reducing the stage-completion
    time (SCT) by considering stage information in its scheduling approach. It proposes
    a stage-aware coflow-scheduling method to minimize the average SCT. Tereis [[260](#bib.bib260)]
    explores the utilization of idle GPU computational resources during data transmission
    periods. It predicts the execution time for a distributed DL job and its corresponding
    data transmission time, allowing for the simultaneous packaging of two jobs on
    the same GPU. This ensures that one job is completed before the other concludes
    its data transfer. Mercury [[261](#bib.bib261)] shifts priority scheduling to
    the transport layer innovatively, focusing on packet granularity. In this system,
    packets in the Mercury buffer with the highest priority are transmitted first.
    Additionally, Mercury incorporates immediate aggregation at the transport layer,
    enabling full overlapping of gradient push-and-pull operations. This approach
    not only streamlines data flow but also maximizes the efficiency of network resource
    utilization.
  prefs: []
  type: TYPE_NORMAL
- en: V-B2 Cost-Effective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The primary objective of cost-effective scheduling is to minimize operational
    costs while ensuring optimal performance for distributed training. It achieves
    a balance between resource utilization, energy consumption, and monetary expenditures
    in the scheduling decisions. During the training phase, cost efficiency emerges
    as a significant goal for the scheduling of training workloads. Cynthia [[262](#bib.bib262)]
    offers predictable distributed training performance while reducing the training
    budget. This scheduler identifies the optimal resource type and maintains training
    throughput effectively, thereby minimizing monetary costs. FC² [[263](#bib.bib263)],
    similar to Cynthia, is a scheduler that recommends cost-effective cloud resource
    allocations for parameter servers in distributed training tasks. It prioritizes
    instances with the largest network bandwidth within the budget to circumvent communication
    bottlenecks. Furthermore, it introduces a heuristic named Scale-Opt for determining
    worker instances, ensuring job throughput, and maximizing cost savings. Jahani [[264](#bib.bib264)]
    considers computing nodes with varying numbers of GPUs as distinct virtual machines.
    The scheduling process is modeled as a mixed-integer linear programming (MILP)
    problem, aiming to reduce leasing costs globally while maintaining job latency.
    GPOEO [[265](#bib.bib265)] achieves significant power savings for training workloads.
    It can be integrated into GPU data centers easily, utilizing a customized scheduler
    to manage job orchestration. STS [[266](#bib.bib266)] optimizes the scheduling
    of distributed training jobs from the perspective of cloud service providers operating
    data centers. It leverages the probability distribution of early job termination
    to adapt resource assignments during job execution, with the aim of minimizing
    the expected energy cost.
  prefs: []
  type: TYPE_NORMAL
- en: V-B3 Deadline Guarantee
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diverging from the aforementioned scheduling methods, deadline-guaranteed scheduling
    focuses on ensuring that jobs are completed before a specified deadline. This
    approach is critical for tasks for which timing is a key factor. GENIE [[267](#bib.bib267)],
    a trailblazing deadline-aware scheduler for distributed training workloads, explores
    the key factors that impact the performance of distributed DL tasks. It introduces
    a predictive model based on lightweight profiling, enabling the accurate estimation
    of processing rates and response latencies for a variety of distributed DL workloads.
    A significant limitation of GENIE, however, is its inability to handle mixed workloads
    that include both deadline-sensitive tasks and best-effort tasks simultaneously [[239](#bib.bib239)].
    Chronus [[268](#bib.bib268)], an end-to-end scheduling system, meets SLOs by guaranteeing
    deadlines for SLO-aware jobs while also enhancing the performance of best-effort
    jobs. This dual-focused strategy enables Chronus to manage a wide range of workload
    requirements. Furthering these developments, Hydra [[269](#bib.bib269)] emerges
    as a dynamic and multifaceted scheduler, equipped to tackle various scheduling
    challenges, including adhering to deadlines and reducing job completion times.
    Hydra introduces an innovative sampling approach that capitalizes on the iterative
    periodicity inherent in distributed DL jobs. This technique allows for the precise
    estimation of job completion times in heterogeneous GPU environments, thereby
    elevating the efficiency and effectiveness of scheduling for various distributed
    DL workloads.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Inference Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As distributed-DL-based applications permeate our daily lives increasingly
    in the form of online services, the management and scheduling of large-scale inference
    workloads on GPUs have become critical. These inference jobs, distinct from the
    resource-intensive and long-duration training workloads mentioned earlier, possess
    unique characteristics and requirements. Consequently, they necessitate novel
    scheduling solutions tailored to their specific needs [[239](#bib.bib239), [278](#bib.bib278)].
    In alignment with the approach of the preceding section, this section categorizes
    various scheduling strategies with a particular focus on two primary goals: efficiency
    and throughput. This categorization facilitates a comprehensive understanding
    of the different methodologies employed in inference scheduling, highlighting
    their significance in enhancing the overall performance of distributed inference
    workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C1 Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike the training phase, where the focus is often on maximizing accuracy and
    model robustness, the inference phase primarily emphasizes the reduction of latency
    and cost.
  prefs: []
  type: TYPE_NORMAL
- en: To maintain satisfactory latency, inference schedulers are designed to scale
    resources proactively in response to request density and to reorder execution
    sequences strategically at the job level. Sniper [[270](#bib.bib270)] stands out
    as a self-updating cloud-edge collaborative inference scheduling system with a
    focus on time awareness. It employs a non-invasive performance characterization
    network based on neural network similarity to predict the inference time of DNNs
    accurately. This system achieves a stable increase in throughput successfully,
    demonstrating its effectiveness in optimizing the scheduling process in dynamic
    cloud-edge environments.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, cost efficiency is another critical factor in the inference phase,
    prompting some schedulers to incorporate various mechanisms aimed at achieving
    cost-effective inference. AutoDeep [[271](#bib.bib271)] endeavors to automate
    cloud deployment for real-time online DNN inference, focusing on minimizing costs
    while maintaining acceptably low latency. To achieve this, AutoDeep utilizes Bayesian
    optimization combined with DRL. This innovative approach enables the adaptive
    discovery of the optimal cloud configuration and device placement, reducing the
    required search time significantly. Through this method, AutoDeep balances the
    trade-off between operational costs and latency in DNN inference workloads efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, latency and cost are recognized as interdependent factors in system
    design. Improving one aspect may inadvertently compromise the other if the solution
    is not designed meticulously. This interplay motivates researchers to develop
    scheduler systems that strike a balance between these objectives. Clipper [[272](#bib.bib272)]
    introduces an innovative model-selection abstraction, accommodating both single-model
    selection and model-integration selection. This system conducts inferences across
    all models and integrates their results. Clipper monitors accuracy and latency
    feedback continuously, employing a best-effort search approach to optimize model
    selection based on these parameters.
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 Throughout capability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Throughput capability represents another crucial objective for scheduling inference
    workloads. Generally, the scheduling system is fine-tuned to enhance throughput
    through strategic batch execution and configuration adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: Batching inference has been identified as an efficient method to enhance utilization
    and reduce system overhead [[239](#bib.bib239)]. In recent times, various schedulers
    have incorporated heuristic methods to fine-tune the batch size for optimal performance.
    For instance, Rafiki [[273](#bib.bib273)] employs a practical Additive-Increase
    Multiplicative-Decrease (AIMD) algorithm to adjust the inference batch size dynamically.
    This approach allows for responsive adaptation to varying workload conditions.
    Nanily [[274](#bib.bib274)] establishes an upper limit on the batch size by calculating
    the maximum remaining time for a request. This is determined by subtracting the
    minimum queuing time of available resources from the remaining time. It then computes
    an appropriate batch size such that the inference execution time is equal to or
    approximates this maximum remaining time.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned approaches, certain schedulers employ end-to-end
    configuration tuning to enhance system throughput. RRL [[275](#bib.bib275)] emphasizes
    the optimization of parallel configurations at various levels, including both
    request-level parallelism and intra-request level parallelism. These optimizations
    play a significant role in reducing the overall system latency and improving throughput.
    Morphling [[276](#bib.bib276)], on the other hand, presents a rapid and near-optimal
    auto-configuration framework designed specifically for cloud-native model serving.
    This framework adapts to new inference services by sampling a limited set of configurations
    and then employs a meta-model to identify the most optimal configuration. This
    strategy allows Morphling to adjust quickly and efficiently to varying service
    requirements while maintaining high system performance.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Lessons Learned toward Large-scale Resource Allocation and Task Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we discuss some lessons learned from designing resource-allocation
    and task-scheduling strategies for large-scale distributed DL. These lessons learned
    help to reveal promising directions for future studies.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Fine-grained and elastic GPU memory and bandwidth sharing strategies
    are critical to improve GPU utilization. Conventional GPU-allocation methods often
    assign individual distributed DL jobs exclusive access to a GPU and can lead to
    extremely low utilization. Exploring the resource-allocation optimization for
    diverse distributed training and inference workloads running on heterogeneous
    GPU networks in large-scale clusters comprehensively is necessary. GPU-sharing
    strategies should consider various important factors, including performance isolation,
    elastic allocation, orchestration of computational and communication-resource
    allocation, and resource fragmentation.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ High-performance large-scale distributed DL requires the orchestration
    of efficient allocation of GPU and network resources. The allocation of network
    resources can frequently be overlooked as a bottleneck for efficient resource
    utilization in distributed DL. Many resource-allocation strategies of distributed
    DL focus on addressing computation issues, such as low utilization, load imbalance,
    and long queuing delays. However, with the increase of the cluster scale, the
    complexity of GPU network connections increases exponentially, and lack of consideration
    to efficient network-resource allocation can result in significant low job-execution
    performance of large-scale distributed DL. Efficient network-bandwidth-allocation
    strategies can alleviate communication contention. Fully utilizing resources of
    both GPU and network bandwidth leads to enhanced overall performance of distributed
    training and inference at a large scale.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ These framework-level strategies can orchestrate with optimizations
    for distributed DL algorithms to meet common and distinct requirements of distributed
    training and inference. Allocation and scheduling efficiencies are important for
    both distributed training and inference. Examples of distinct performance objectives
    of distributed training include promoting fairness and increasing training throughput.
    In contrast, distributed inference requires achieving low latency and ensuring
    the SLO. Efficient distributed DL is the result of the orchestration of efficient
    resource-allocation and task-scheduling strategies. To tackle complex challenges
    posed by large data sets, large cluster scale, and large models, distributed DL
    also requires the orchestration of these strategies and contemporary distributed
    DL algorithms, for developing more efficient, effective, and comprehensive distributed
    DL solutions for various large-scale scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Studies on communication-efficient infrastructures for large-scale
    distributed DL'
  prefs: []
  type: TYPE_NORMAL
- en: '|                                                  Category | Technology&Ref.
    | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '|   GPU interconnects ([VI-A](#S6.SS1 "VI-A GPU Interconnects ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | Intra-node ([VI-A1](#S6.SS1.SSS1 "VI-A1
    Intra-node ‣ VI-A GPU Interconnects ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | PCIe [[279](#bib.bib279)] | - | Wired point-to-point link connection
    for high-speed serial communication between GPUs. |'
  prefs: []
  type: TYPE_TB
- en: '| NVLink [[280](#bib.bib280)] | - | Direct GPU-to-GPU interconnects based on
    wired mesh networking. |'
  prefs: []
  type: TYPE_TB
- en: '| Inter-node ([VI-A2](#S6.SS1.SSS2 "VI-A2 Inter-node ‣ VI-A GPU Interconnects
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | GPUDirect-RDMA [[281](#bib.bib281)]
    | - | Direct GPU memory access between nodes within a cluster via PCIe. |'
  prefs: []
  type: TYPE_TB
- en: '| NVSwitch [[280](#bib.bib280)] | - | All-to-all GPU communication across multiple
    nodes. |'
  prefs: []
  type: TYPE_TB
- en: '|   Programmable network devices ([VI-B](#S6.SS2 "VI-B Programmable Network
    Devices ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Switch ([VI-B1](#S6.SS2.SSS1
    "VI-B1 Programmable switches ‣ VI-B Programmable Network Devices ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | SwitchML [[282](#bib.bib282)] | 2021
    | Resource: reusing a fixed-size memory pool for INA within a rack. |'
  prefs: []
  type: TYPE_TB
- en: '| ATP [[283](#bib.bib283)] | 2021 | Resource: allocating memory dynamically
    upon their arrival for multiple training jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | INAlloc [[284](#bib.bib284)] | 2023 | Resource: an additional switch memory
    manager to manage switch memory allocation for multiple jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GRID [[285](#bib.bib285)] | 2023 | Routing: randomized-rounding-based
    gradient routing with INA to maximize the gradient sending rate with resource
    constraints. |'
  prefs: []
  type: TYPE_TB
- en: '|  | GOAT [[286](#bib.bib286)] | 2023 | Routing: dividing the model into submodels
    and finding the optimal submodel gradient routing to minimize communication overhead.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PANAMA [[287](#bib.bib287)] | 2021 | Congestion: multiple aggregation
    tree to disperse gradient traffic and ensure fairness among multiple jobs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | A²TP [[288](#bib.bib288)] | 2023 | Congestion: two congestion windows
    for independent congestion control of switch resources and link bandwidth. |'
  prefs: []
  type: TYPE_TB
- en: '|  | NetReduce [[289](#bib.bib289)] | 2023 | Congestion: transport-transparent
    gradient INA, reusing transport layer’s congestion control. |'
  prefs: []
  type: TYPE_TB
- en: '| SmartNIC ([VI-B2](#S6.SS2.SSS2 "VI-B2 Programmable NICs ‣ VI-B Programmable
    Network Devices ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | Guo et al. [[290](#bib.bib290)]
    | 2023 | Multi-level in-SmartNIC caching and computing system for recommendation
    models. |'
  prefs: []
  type: TYPE_TB
- en: '| FCsN [[291](#bib.bib291)] | 2022 | Offloading control logic, scheduling,
    and computation of distributed inference jobs to SmartNICs. |'
  prefs: []
  type: TYPE_TB
- en: '|   Collective communication ([VI-C](#S6.SS3 "VI-C Inter-GPU Collective Communication
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | Interface ([VI-C1](#S6.SS3.SSS1
    "VI-C1 Collective communication libraries ‣ VI-C Inter-GPU Collective Communication
    ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey")) | [[292](#bib.bib292), [293](#bib.bib293),
    [294](#bib.bib294)] | - | Point-to-point and collective communication interfaces
    for gradient computation and communication via GPU interconnects. |'
  prefs: []
  type: TYPE_TB
- en: '| NCCL [[295](#bib.bib295)] | - | A high-bandwidth and low-latency collective
    communication library tailored for NVLink. |'
  prefs: []
  type: TYPE_TB
- en: '|  | MSCCLang [[296](#bib.bib296)] | 2023 | A unified framework for writing
    customized collective communication algorithms. |'
  prefs: []
  type: TYPE_TB
- en: '| Heterogeneous networks ([VI-C2](#S6.SS3.SSS2 "VI-C2 Collective communication
    in heterogeneous networks ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | BlueConnect [[297](#bib.bib297)] |
    2019 | Decomposing allreduce into multiple reduce-scatter and all-gather primitives
    in symmetric topologies. |'
  prefs: []
  type: TYPE_TB
- en: '| Blink [[298](#bib.bib298)] | 2020 | Leveraging packing spanning trees to
    decomposing allreduce for optimal bandwidth utilization in asymmetric topologies.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Plink [[299](#bib.bib299)] | 2020 | Probing the network to capture the
    physical network topology and bandwidth information to optimize collective communication.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sparse data ([VI-C3](#S6.SS3.SSS3 "VI-C3 Collective communication for sparse
    gradients ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | SparcML [[300](#bib.bib300)] | 2019 | Supporting sparse
    data streaming and sparse-to-dense switching for the minimal collective communication
    cost. |'
  prefs: []
  type: TYPE_TB
- en: '| OmniReduce [[301](#bib.bib301)] | 2021 | Splitting sparse data into blocks
    to increase INA parallelism for non-zero blocks. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Ok-Topk [[175](#bib.bib175)] | 2022 | Balancing sparse collective communication
    overhead across workers depending on consecutive buffer size. |'
  prefs: []
  type: TYPE_TB
- en: '|  | CommLib [[174](#bib.bib174)] | 2021 | A hierarchical aggregation and caching
    scheme for sparse collective communication to fully utilize GPU bandwidth and
    reduce I/O. |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeepReduce [[302](#bib.bib302)] | 2021 | Decoupling indices and values
    of sparse data to apply different compression algorithms. |'
  prefs: []
  type: TYPE_TB
- en: '| Synthesis ([VI-C4](#S6.SS3.SSS4 "VI-C4 Synthesizing optimal collective communication
    ‣ VI-C Inter-GPU Collective Communication ‣ VI Large-Scale Communication Infrastructures
    ‣ Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive
    Survey")) | SCCL [[303](#bib.bib303)] | 2021 | Synthesizing latency- and bandwidth-optimal
    collective implementations given a network topology with bandwidth constraints.
    |'
  prefs: []
  type: TYPE_TB
- en: '| TACCL [[304](#bib.bib304)] | 2023 | Synthesizing collective algorithms for
    heterogeneous multi-rack networks efficiently by dividing the problem into two
    independent sub-problems: routing and scheduling. |'
  prefs: []
  type: TYPE_TB
- en: '|   Topology ([VI-D](#S6.SS4 "VI-D Communication Topologies ‣ VI Large-Scale
    Communication Infrastructures ‣ Communication-Efficient Large-Scale Distributed
    Deep Learning: A Comprehensive Survey")) | Fixed ([VI-D1](#S6.SS4.SSS1 "VI-D1
    Fixed topologies ‣ VI-D Communication Topologies ‣ VI Large-Scale Communication
    Infrastructures ‣ Communication-Efficient Large-Scale Distributed Deep Learning:
    A Comprehensive Survey")) | BML [[305](#bib.bib305)] | 2020 | A BCube topology
    for fully distributed data-parallel training. |'
  prefs: []
  type: TYPE_TB
- en: '| HammingMesh [[306](#bib.bib306)] | 2022 | Local connectivity by affordable
    PCB-mesh interconnects and global connectivity by sparsely connected switches.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Configurable ([VI-D2](#S6.SS4.SSS2 "VI-D2 Reconfigurable topologies ‣ VI-D
    Communication Topologies ‣ VI Large-Scale Communication Infrastructures ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey")) | SiP-ML [[307](#bib.bib307)]
    | 2021 | Commercially affordable fully-connected and ring-based topologies powered
    by SiP interfaces. |'
  prefs: []
  type: TYPE_TB
- en: '| TopoOpt[[308](#bib.bib308)] | 2023 | Offline permuting ring-allreduce topologies
    for the optimal topology of multi-NIC nodes. |'
  prefs: []
  type: TYPE_TB
- en: '|   |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/dd26c39d9a617b76d6be45297ef6f79a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: A Torus topology for large-scale distributed DL with GPU interconnects,
    as well as programmable network interface cards and switches'
  prefs: []
  type: TYPE_NORMAL
- en: VI Large-Scale Communication Infrastructures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we introduce high-performance communication infrastructures
    for large-scale distributed DL, including GPU interconnects, programmable network
    devices, collective communication protocols, and communication topologies. Table [IX](#S5.T9
    "TABLE IX ‣ V-D Lessons Learned toward Large-scale Resource Allocation and Task
    Scheduling ‣ V Large-Scale Resource Allocation and Task Scheduling ‣ Communication-Efficient
    Large-Scale Distributed Deep Learning: A Comprehensive Survey") summarizes the
    related technologies and studies on these topics. We conclude this section with
    some lessons learned in this domain. Fig. [10](#S5.F10 "Figure 10 ‣ V-D Lessons
    Learned toward Large-scale Resource Allocation and Task Scheduling ‣ V Large-Scale
    Resource Allocation and Task Scheduling ‣ Communication-Efficient Large-Scale
    Distributed Deep Learning: A Comprehensive Survey") depicts an example Torus [[309](#bib.bib309)]
    topology with high-bandwidth intra- and inter-node GPU interconnects, as well
    as programmable network devices. Implementing efficient collective communication
    built upon the high-performance communication connection and topology enhances
    distributed DL at a large scale.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-A GPU Interconnects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The communication between GPUs within a computing node and across nodes is significantly
    accelerated by high-speed GPU interconnects [[310](#bib.bib310), [311](#bib.bib311)].
  prefs: []
  type: TYPE_NORMAL
- en: VI-A1 Intra-node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a scale-up solution, Peripheral Component Interconnect Express (PCIe) [[279](#bib.bib279)]
    and NVLink [[280](#bib.bib280)] are prominent data link protocols for inter-GPU
    communication within a node. PCIe serves as a high-speed serial communication
    interface between GPUs via a wired point-to-point link connection. It supports
    up to 16 lanes per GPU, with the maximum bandwidth of 242 GBps for each GPU in
    PCIe version 7.0. NVLink is a direct GPU-to-GPU interconnect based on wired mesh
    networking. It achieves significantly higher throughput compared to PCIe, supporting
    up to 18 links per GPU and providing a maximum bandwidth of 900 GBps for each
    GPU in NVLink version 4.0. This elevated bandwidth enhances the speed and efficiency
    of inter-GPU communication, making NVLink particularly advantageous in scenarios
    demanding extensive inter-GPU data exchange.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A2 Inter-node
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In a scale-out solution, the scalability of high-speed inter-node GPU interconnects
    can be achieved through the technologies of GPUDirect-RDMA [[281](#bib.bib281)]
    and NVSwitch [[280](#bib.bib280)]. GPUDirect-RDMA facilitates direct GPU memory
    access between nodes within a cluster via PCIe, thereby eliminating unnecessary
    memory copies through the CPU. NVSwitch extends the capability of NVLink by supporting
    all-to-all GPU communication across multiple nodes. NVSwitch can empower an ultra-large
    GPU server cluster with a capacity of up to 256 GPUs, delivering a staggering
    57.6 TBps all-to-all bandwidth and supporting very large DL models with trillions
    of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Programmable Network Devices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Programmable network devices, including programmable switches and network interface
    cards (NICs), facilitate software-hardware co-design to improve the communication
    efficiency of distributed DL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B1 Programmable switches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, interest has been growing in leveraging programmable switches to execute
    in-network aggregation (INA) [[312](#bib.bib312)] for allreduce operations on
    gradients, aiming to mitigate network traffic during distributed training. Key
    considerations within this domain include on-switch resource utilization [[282](#bib.bib282),
    [283](#bib.bib283), [284](#bib.bib284)], gradient routing [[285](#bib.bib285),
    [286](#bib.bib286)], and congestion control [[287](#bib.bib287), [288](#bib.bib288),
    [289](#bib.bib289)].
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ On-switch resource utilization: The implementation of INA for gradients
    in the switch is significantly constrained by limited on-switch memory. SwitchML [[282](#bib.bib282)]
    employs a fixed-size memory pool to aggregate gradients during allreduce communication
    within a rack switch. The reuse mechanism of the memory pool enables SwitchML
    to accommodate streaming gradients in a switch with limited computational and
    storage capabilities. However, SwitchML falls short in addressing challenges presented
    by practical scenarios featuring more complex topologies and workloads, such as
    a multi-rack cluster shared by multiple DL tenants. This hinders its scalability
    across multiple racks accommodating multiple training jobs. ATP [[283](#bib.bib283)]
    extends upon the work of SwitchML to enhance INA for multiple training jobs in
    a multi-rack setting by sharing limited switch resources across concurrently running
    training jobs efficiently. ATP employs a decentralized memory-allocation mechanism
    to allocate memory space on each switch to gradient packets from multiple training
    jobs dynamically upon their arrival at the switch. In contrast to SwitchML that
    conducts entire gradient aggregation in the rack switch, potentially encountering
    network bandwidth underutilization during heavy resource contention, ATP employs
    a best-effort approach to send gradients for aggregation. To further enhance the
    efficient utilization of switch resources for INA in a cluster, INAlloc [[284](#bib.bib284)]
    introduces a additional switch memory management layer. This layer enables the
    distributed training job scheduler to optimize the management of physical memory
    on switches, thus improving job completion time.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Gradient routing: Optimized gradient routing algorithms yield benefits
    for INA in multi-rack environments, including improved load balancing, mitigation
    of bandwidth bottlenecks, and the optimal utilization of switch resources. However,
    gradient routing paths in SwitchML and ATP are fixed and not optimized. To tackle
    this issue, GRID [[285](#bib.bib285)] presents a randomized-rounding-based algorithm
    for gradient routing with INA, aiming to maximize the gradient sending rate of
    workers with resource constraints. GOAT [[286](#bib.bib286)] introduces a comparable
    approach with a focus on minimizing the network communication overhead among workers,
    switches, and the PS. This approach first partitions the DL model into submodels
    and then utilizes a knapsack-based randomized rounding algorithm to determine
    the switch responsible for aggregating specific submodel gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ Congestion control: Because of the absence of an end-to-end byte
    stream abstraction in the transport layer for INA of distributed training, traditional
    congestion-control strategies at the transport layer, such as DCTCP [[313](#bib.bib313)]
    and pFabric [[314](#bib.bib314)], are not readily applicable. Many existing INA
    methods opt to implement their own straightforward congestion-control strategies.
    For example, SwitchML implements basic retransmission timeout and self-clock mechanisms,
    while ATP utilizes the Explicit Congestion Notification (ECN) flag and Additive-Increase
    Multiplicative-Decrease (AIMD) mechanism, akin to TCP. However, these simplified
    congestion control implementations often fall short in optimizing performance
    for multiple jobs within large-scale shared cluster scenarios, or tend to couple
    the congestion control of switch resources and link bandwidth resources. To tackle
    the former issue, PANAMA [[287](#bib.bib287)] employs multiple aggregation trees
    to disperse traffic, thus mitigating congestion hotspots and ensuring fair sharing
    of network resources across multiple jobs. To tackle the latter issue, A²TP [[288](#bib.bib288)]
    employs two congestion windows for independent congestion control of switch resources
    and link bandwidth resources. Decoupling INA with the transport layer provides
    another direction in tackling the congestion control issue in this context. NetReduce [[289](#bib.bib289)]
    preserves end-to-end abstracted connections by keeping packet data volume unchanged
    within the switch. As a result, the INA for gradients becomes transport-transparent,
    allowing the reuse of advanced congestion-control strategies of the transport
    layer, such as RoCEv2 [[315](#bib.bib315)].'
  prefs: []
  type: TYPE_NORMAL
- en: VI-B2 Programmable NICs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Commercial Field-Programmable-Gate-Array-based (FPGA-based) SmartNICs [[316](#bib.bib316),
    [317](#bib.bib317)] can server as computational resources for offloading domain-specific
    tasks from GPUs, as well as mitigating network communication overhead in distributed
    training and inference scenarios. Guo et al. [[290](#bib.bib290)] have developed
    a heterogeneous SmartNIC system to address communication, memory, and computation
    bottlenecks inherent in the distributed training of Deep Learning Recommendation
    Models (DLRMs). To tackle the challenge of all-to-all communication in exchanging
    massive embedding tables within DLRMs, this system introduces a remote caching
    mechanism that buffers frequently used remote embedding lookup results on the
    SmartNIC, thereby reducing communication overhead during the feedforward process.
    This system further integrates local cache and in-SmartNIC computation mechanisms
    to access local embedding tables and execute irregular computations within the
    SmartNIC, obviating GPU intervention. These mechanisms alleviate memory bandwidth
    burdens and kernel launching overheads of GPUs effectively, optimizing both feedforward
    and backpropagation processing during the distributed training of DLRMs. On the
    other hand, FCsN [[291](#bib.bib291)] enhances distributed inference by offloading
    control logic, system scheduling, network communication, and neural network kernel
    computation to SmartNICs, eliminating CPU intervention. The strategic offloading
    maximizes the overlap of computation and network communication during distributed
    inference, facilitating non-conflict streaming neural network kernel executions
    at line rate.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Inter-GPU Collective Communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The efficiency of collective communication among GPUs plays a pivotal role
    in determining the overall performance of distributed DL. We introduce this topic
    in four dimensions: the collective communication library, optimization for heterogeneous
    network, optimization for sparse gradients, and the synthesis approach.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-C1 Collective communication libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Collective communication libraries, such as Message Passing Interface (MPI) [[292](#bib.bib292)],
    Gloo [[293](#bib.bib293)], and Horovod [[294](#bib.bib294)], provide convenient
    programming interfaces for point-to-point and collective communication to exchange
    gradients on top of GPU interconnects. Specifically tailored to NVIDIA GPUs, NVIDIA
    Collective Communications Library (NCCL) [[295](#bib.bib295)] is designed for
    compatibility with MPI and optimized for delivering high bandwidth and low latency
    for multi-GPU and multi-node interconnects over PCIe and NVLink. In addition to
    point-to-point communication, NCCL supports various collective communication primitives,
    including but not limited to allgather, allreduce, and broadcast. Microsoft MSCCLang [[296](#bib.bib296)]
    provides a unified library for writing customized collective communication algorithms
    that can be compiled into NCCL built-in primitives. It enables high-performance
    customized collective implementations for various network environments. However,
    these collective communication libraries lack native-optimization for heterogeneous
    network environments and sparse gradient scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C2 Collective communication in heterogeneous networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The performance of collective communication can be improved by fully utilizing
    network bandwidth in heterogeneous networks. BlueConnect [[297](#bib.bib297)]
    decomposes an allreduce operation into a series of reduce-scatter and all-gather
    primitives by leveraging the hierarchical topology information of communication
    bandwidth in a cluster. As a result, it minimizes the communication overhead for
    distributed training in heterogeneous networks and outperforms NCCL. However,
    BlueConnect is limited to symmetric topologies. Blink [[298](#bib.bib298)] further
    considers asymmetric topologies and heterogeneous links through the employment
    of packing spanning trees, optimizing the utilization of high-link bandwidth in
    the presence of topology heterogeneity. In the cloud-based network environment
    with an opaque network topology, Plink [[299](#bib.bib299)] probes the network
    as to capture physical network topology and bandwidth/latency constraints that
    are then utilized to optimize collective communication in data center networks.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C3 Collective communication for sparse gradients
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gradient sparsity is a prevalent and substantial characteristic in numerous
    distributed DL models, and treating sparse gradients as dense data in collective
    communication can result in a waste of communication bandwidth. Unfortunately,
    most existing collective communication libraries are primarily designed for dense
    data communication and lack native supports for sparse data, including considerations
    for sparse data representation and aggregation in communication streams. Addressing
    this limitation, SparcML [[300](#bib.bib300)] supports streaming sparse data in
    index-value pair representations natively and designs an efficient method for
    sparse-sparse and sparse-dense summation. Sparse representations can switch to
    dense ones to ensure minimal communication cost during collective communication.
    To increase the parallelism for sparse data aggregation, OmniReduce [[301](#bib.bib301)]
    introduces a streaming-aggregation mechanism that splits sparse data into blocks
    and achieves high INA parallelism for blocks containing non-zero values, thereby
    reducing communication overhead and maximizing communication bandwidth utilization
    effectively. Imbalanced workloads for key-value pair collective operations within
    the cluster lead to straggler performance for sparse gradients. To handle imbalance
    allreduce operations for top-$k$ sparsified gradients, Ok-Topk [[175](#bib.bib175)]
    generates a workload balancing scheme across workers based on the information
    about consecutive buffer sizes on all workers. This workload-balancing approach
    makes collective communication for sparse gradients more scalable, whereas the
    additional communication overhead is narrowly bounded.
  prefs: []
  type: TYPE_NORMAL
- en: The heterogeneity of cloud and FL environments poses new challenges to collective
    communication of sparse data. For cloud environments in which interconnects within
    a node are fast and those across nodes are slow in the GPU cluster, CommLib [[174](#bib.bib174)]
    adopts a hierarchical communication architecture for aggregating top-$k$ sparsified
    gradients to better utilize the GPU bandwidth within and across nodes. It also
    introduces a multi-level data caching scheme to reduce I/O on public clouds. For
    FL environments in which devices have constrained computational and communication
    resources and are separated geographically, the data volume of collective communication
    operations is a critical performance factor. To compress sparse gradients in FL
    environments, DeepReduce [[302](#bib.bib302)] decouples indices and values of
    sparse gradients into two sets, so each set can apply compressors that are optimized
    for its type independently. Specifically, DeepReduce introduces a Bloom-filter
    based compressor for indices and a curve-fitting based compressor for values,
    and significantly reduces the gradient volume compared to the traditional key-value
    pair representation.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C4 Synthesizing optimal collective communication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recently, a rising trend of synthesis approaches aims at crafting collective
    communication algorithms for distributed DL. Given a network topology with bandwidth
    constraints on GPUs and edges, SCCL [[303](#bib.bib303)] synthesizes Pareto-efficient
    latency- and bandwidth-optimal collective implementations from scratch. However,
    SCCL lacks consideration for multi-rack network environments, impeding its scalability
    within a large-scale GPU cluster. To address this scalability issue, TACCL [[304](#bib.bib304)]
    enhances the synthesizer for multi-rack networks with heterogeneous links. On
    one hand, it uses a communication sketch containing information about the logical
    network topology and switch-hyperedge policy as the synthesizer input. On the
    other hand, TACCL adopts a heuristic approach, breaking down the problem into
    two independent steps: routing and scheduling. This division significantly reduces
    the search space, offering a pragmatic synthesis solution for collective communication
    in large-scale GPU clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-D Communication Topologies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enhancing communication topologies is crucial for leveraging full computational
    and communication capabilities of a large-scale cluster in distributed DL. Conventional
    high-performance computing topologies for data centers can be used for reasonable-scale
    distributed DL [[318](#bib.bib318)] that include fix topologies such as Fat-tree [[319](#bib.bib319)],
    BCube [[320](#bib.bib320)], Jellyfish [[321](#bib.bib321)], and DCell [[322](#bib.bib322)],
    and reconfigurable topologies such as Helios [[323](#bib.bib323)], c-Through [[324](#bib.bib324)],
    and OSA [[325](#bib.bib325)]. However, these topologies are primary designed for
    electrical and optical networks and encounter challenges related to bandwidth
    and cost when dealing with all-to-all communication patterns inherent in distributed
    DL scenarios at ultra-scale, involving hundreds or even thousands of GPUs [[23](#bib.bib23),
    [236](#bib.bib236)]. Recently, a number of communication topologies that address
    these challenges considering communication patterns of distributed DL and high
    bandwidth among GPU interconnects have been proposed.
  prefs: []
  type: TYPE_NORMAL
- en: VI-D1 Fixed topologies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To leverage the advantage of multiple NICs per computing node and provide a
    scalable and fault-tolerant network over Ethernet and commodity devices, BML [[305](#bib.bib305)]
    introduces a fully distributed gradient-synchronization algorithm on top of the
    BCube topology. However, it is optimized exclusively for data-parallel training.
  prefs: []
  type: TYPE_NORMAL
- en: To address the constraints of BML and pursue a communication topology that is
    both high-bandwidth while cost-effective for large-scale distributed training
    involving data and model parallelisms, HammingMesh [[306](#bib.bib306)] integrates
    concepts from Fat-tree and Torus [[309](#bib.bib309)]. This results in a two-dimensional
    topology with local and global connectivity. The local connectivity ensures high
    local bandwidth at low cost through the utilization of affordable Printed-Circuit-Board-mesh
    (PCB-mesh) interconnects, while the global connectivity can establish a global
    network by using a small number of sparsely connected switches to connect the
    meshes in rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: VI-D2 Reconfigurable topologies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The iterative nature of distributed training requires high bandwidth and low
    reconfiguration latency in reconfigurable topologies. To address these challenges,
    SiP-ML [[307](#bib.bib307)] proposes two topologies driven by Silicon Photonic
    (SiP) interfaces: SiP-OCS and SiP-Ring. SiP-OCS adopts a fully connected topology
    to deliver high bandwidth using commercially available optical circuit switches,
    connecting GPUs to all switches through Tbps SiP interfaces. SiP-Ring employs
    a switch-less ring topology, minimizing reconfiguration latency through the use
    of Micro-ring resonators embedded in SiP interfaces. On the other hand, TopoOpt[[308](#bib.bib308)]
    adopts an offline method to find the best communication topology by exploring
    from ring-allreduce permutations across a set of computing nodes, each equipped
    with multiple NICs, and reconfigures switches to realize this target topology.
    TopoOpt aims to co-optimize the network-topology and parallelism strategy for
    efficient large allreduce operations in both data and model parallelism modes.
    By employing alternating optimization, TopoOpt narrows down the search space for
    co-optimization targets strategically, ensuring a network topology that provides
    ample bandwidth and a parallelism strategy that requires only a small hop count
    during model-parallelism transfers.'
  prefs: []
  type: TYPE_NORMAL
- en: VI-E Lessons Learned toward High-Performance Large-scale Communication Infrastructures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We discuss some lessons learned from existing technologies introduced in this
    section, helping researchers working toward high-performance communication infrastructures
    for large-scale distributed DL.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ High performance communication is monetarily costly, and economically
    affordable devices and topologies with best cost benefit are more practical. The
    ultimate pursuit of the highest performance, regardless of cost, is only applicable
    in some specialized cases. In most cases, we aim to achieve the best training
    throughput within a monetary budget for computational and communication resources.
    The optimal solution requires a co-design that involves adopting cost-effective
    communication devices and topologies for the high communication throughput and
    balancing computational and communication capacities for the overall training
    throughput.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ INA is efficient, and co-designing communication infrastructures and
    algorithms can reduce communication overhead and increase scheduling efficiency.
    Many modern communication devices also possess non-negligible computational capacity.
    With a thorough understanding of the characteristics of model synchronization
    in large-scale distributed DL, designing a solution incorporating in-network computation
    to reduce communication traffic and implementing efficient transport scheduling
    at the package, flow, or coflow level to enhance communication throughput is feasible.
    Challenges of such a solution include congestion-control strategies for INA, workload
    balancing among devices, fairness among multiple distributed DL jobs and tenants,
    and interference isolation of distributed DL traffic and other network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Heterogeneous networks and sparse data are prevalent, and collective
    communication protocols should be fine-tuned for these scenarios. Collective communication
    implementations should take into account the heterogeneity in the bandwidth of
    network links, the computational capacity of network devices, and the topology
    to reduce communication overhead and enhance workload balancing, particularly
    in large-scale clusters. Given the widespread occurrence of data sparsity in large
    DL models, implementing efficient collective communication for sparse data atop
    heterogeneous networks is imperative. Implementing such efficient collective communication
    algorithms poses challenges, including discovering the true status of the underlying
    networks, routing and scheduling gradient aggregations in complex networks efficiently,
    and devising compatible libraries tailored for specialized high-performance interconnects.
  prefs: []
  type: TYPE_NORMAL
- en: 'VII Large-Scale Distributed Training of Large Models: A Case Study on LLMs'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, LLMs [[59](#bib.bib59), [14](#bib.bib14), [326](#bib.bib326), [327](#bib.bib327),
    [328](#bib.bib328)] have been used successfully in various applications of NLP.
    They have also initiated the trend of developing ultra-large foundation models
    for domain-specific applications [[329](#bib.bib329), [330](#bib.bib330)] in diverse
    fields, including communications, computer science, and artificial intelligence.
    As ultra-large foundation models can contain tens of billions of parameters and
    take hundreds of GPUs and tens of days to train, the distributed training of these
    models at scale encounters more and tougher challenges than conventional distributed
    DL. By inspecting the cases of training LLMs to examine how to apply those communication-efficient
    technologies in practical scenarios, researchers can identify more intrinsic and
    literal research challenges for achieving high performance in large-scale distributed
    DL.
  prefs: []
  type: TYPE_NORMAL
- en: This section discusses practical aspects of the distributed training of LLMs
    by examining the real cases in [[331](#bib.bib331), [332](#bib.bib332), [333](#bib.bib333),
    [334](#bib.bib334), [335](#bib.bib335), [336](#bib.bib336), [337](#bib.bib337),
    [338](#bib.bib338)]. The goal is to revisit key themes introduced earlier in this
    article and provide insights derived from related practices. Additionally, future
    trends in the use of large foundation models within the domain of communications
    are discussed. The presentation adopts a question-and-answer style.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Model Synchronization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ Can data-parallel training be communication-efficient for LLMs? Data-parallel
    training alone is not a good choice for LLMs with slow GPU interconnects, but
    is acceptable with high-performance interconnects. According to [[336](#bib.bib336)],
    concerning the volume of data to be transmitted across GPUs in data-parallel model
    synchronization, the time spent on communication can dominate the training time
    and leaves little chance for overlapping with the computation time. For example,
    training a 100-billion-parameter model on 100 GPUs in the data-parallel mode,
    the volume of transmitted data can reach 10 trillion parameters in one model synchronization
    round. Using 16-bit precision with slow interconnects of 1000 Mbps bandwidth,
    the time for one synchronization round is 1,600 seconds, which is completely unacceptable.
    But for very fast interconnects such as NVLink with 900 GBps point-to-point bandwidth,
    the synchronization time reduces to only 0.22 seconds, making it negligible if
    the synchronization frequency is reduced by local SGD.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Communication Data Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ What is the effect of communication data compression on LLMs? Though
    applying simple implementations of quantization and sparsification algorithms
    can reduce the communication overhead and training time of LLM, the effect of
    adaptive and fine-grained compression methods remains insufficiently explored.
    According to SWARM [[336](#bib.bib336)], in the previous example with 100 billion
    parameters, the 16-bit precision can be reduced to 8-bit, resulting in the communication
    volume and time of model synchronization being only half of the origin values.
    On the contrary, adaptive and fine-grained compression algorithms can introduce
    significant additional decision and computation overhead to LLMs. Considering
    this complexity, caution should be exercised when we applying these algorithms
    to the distributed training of LLMs. The trade-off between computation and communication
    becomes more critical and requires further exploration in the large model scenario.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Can hybrid communication-data-compression algorithms perform effectively
    on LLMs? We can utilize hybrid communication-data-compression algorithms to reduce
    communication overhead on slow networks aggressively. For instance, CocktailSGD [[335](#bib.bib335)]
    employs random sparsification, top-$k$ sparsification, and quantization on the
    gradients sequentially when training an LLM with 20 billion parameters on a 500
    Mbps network. This hybrid method, achieving 117$\times$ compression, is only slightly
    slower than training on high-performance data center networks.
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Resource Allocation and Task Scheduling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ How crucial is pipeline parallelism for LLM training? Pipeline parallelism
    is essential for LLM training. As indicated in [[331](#bib.bib331)], by diminishing
    the impact of the communication volume and worker idle time during pipeline flushes,
    heuristic pipeline-parallelism proves effective in practice with trillion-scale
    LLMs on more than 3,000 GPU. In contrast to layer-slicing parallelism, multiple-layer-slicing
    pipeline parallelism only communicates end-of-layer activations and gradients,
    which can be 300$times$ smaller in the communication volume in a 2.2-billion-parameter
    example [[333](#bib.bib333)]. It is a common practice to use what is known as
    3D parallelism [[334](#bib.bib334)] for LLM training, which combines data, pipeline,
    and layer-slicing parallelisms, to maximize the training throughput.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ How to schedule tasks of LLM training on heterogeneous and low-bandwidth
    networks efficiently? LLMs can be trained on heterogeneous and low-bandwidth networks
    efficiently even with unstable worker devices, by employing fault-tolerant pipelines
    and redistributing tasks among workers. Tackling the issue of unstable links between
    workers, SWARM parallelism [[336](#bib.bib336)] employs decentralized model-parallelism
    to randomize fault-tolerant pipelines and rebalance workers between pipeline stages
    dynamically. The use of randomized fault-tolerant pipelines permits a worker in
    one pipeline stage to establish a connection to a stochastic worker in the next
    pipeline stage between different iterations. This approach enables the rerouting
    of tasks from a disconnected worker to other workers within the same swarm in
    the next iteration. This allows for the tolerance of worker failures in the pipeline,
    ensuring continuity in task execution. The dynamic-rebalancing strategy enables
    workers to switch between pipeline stages for the purpose of balancing the throughput
    of different stages in instances of dynamic membership with unstable workers.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ How crucial is fault-tolerant scheduling for LLM training? Given the
    involvement of a large number of workers in prolonged training sessions for LLMs,
    ensuring fault-tolerance is of utmost importance for efficient scheduling. Frequent
    failure in devices or networks can potentially block the training process, degrade
    the convergence performance, and necessitate redundant restarting of failed tasks
    and pipelines. SWARM parallelism [[336](#bib.bib336)] incorporates the dynamic
    membership of unstable workers into account for efficient pipeline scheduling.
    According to Oobleck [[337](#bib.bib337)], a failed pipeline can be recovered
    by using pipeline replicas and templates swiftly. We can instantiate some logically
    equivalent pipeline replicas, which possess replicated model states. Additionally,
    we define pipeline templates, which include information about the number of workers
    and stages in the pipeline, as well as the mapping of stages to GPUs. Once a pipeline
    failure occurs, a new pipeline can be restored based on the pipeline template
    and replicas instantly.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ Can we utilize GPU resources in decentralized networks for LLM training?
    LLMs can undergo training on GPUs in decentralized, heterogeneous, and low-bandwidth
    networks using optimized data and model partitioning scheme, along with pipeline
    scheduling. According to [[332](#bib.bib332)], there are numerous idle GPU clusters
    dispersed globally that can be utilized collectively for LLM training. We can
    divide the entire model into submodels, utilize a cost model to formulate the
    communication cost of both data and pipeline parallelisms based on the partitioning
    of these submodels, and partition submodels with a goal of minimizing this cost.
    For instance, when training GPT-3 [[59](#bib.bib59)] with 1.3 billion parameters,
    this scheduling strategy can achieve about 4$\times$ speedup in a global cloud
    across 8 worldwide regions, and is only 1.7-3.5$\times$ slower compared to the
    centralized data center solution with 100$\times$ faster networks.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Communication Infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ How can we construct cost-effective communication infrastructures
    for LLM training? Commodity communication infrastructures can be cost-effective
    for LLM training when we employ optimized technologies at various levels for distributed
    DL. High-performance specialized communication infrastructures, such as NVIDIA
    A100 with NVLink interconnects, are frequently used for high-performance LLM training.
    However, considering the monetary cost, many researchers also use commodity networks,
    including data center networks, decentralized cloud infrastructures, and other
    low-bandwidth networks. When data, model, and pipeline parallelism modes are combined,
    and optimized algorithms for communication data compression, resource allocation,
    and task scheduling are applied, commodity communication infrastructures can also
    achieve comparable performance to some specialized ones regarding the cost.
  prefs: []
  type: TYPE_NORMAL
- en: VII-E Large foundation Models for Communications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: $\bullet$ What is the future trend of large foundation models within the domain
    of communications? Large foundation models can be trained with abundant domain-specific
    knowledge of communications to generate optimized strategies for diverse topics,
    including IoT, wireless networks, and network cybersecurity. We refer to such
    models as large communications models here. In the context of industrial IoT,
    large communications models can be exploited effectively in mobile edge computing
    (MEC) clusters to generate optimized codes for communication and scheduling strategies
    autonomously [[338](#bib.bib338)]. In the context of wireless networks, possessing
    capability to handle diverse multi-modal data, large communications models can
    conduct intricate data analyses for wireless sensor networks (WSN) based on unstructured
    mobile network logs, radio signals, and multimedia data, which are challenging
    for alternative models [[16](#bib.bib16)]. Leveraging rich domain knowledge, large
    communications models can also be utilized to improve solutions for various optimization
    tasks of communications, such as routing, network-resource allocation, and radio
    control. In the context of network cybersecurity, particularly within the topics
    of the Internet, WSN, and IoT, large communications models can be used to detect
    anomalies, intrusions, and Distributed Denial of Service (DDoS) attacks [[18](#bib.bib18)].
    They can also detect botnet attacks and generate verification strategies for anti-crawler
    protection. Overall, large communications models have the promising potential
    to extend their applications in various communications topics, enhancing the performance
    of existing technologies and innovating new solutions to address contemporary
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper surveys communication-efficient technologies at the algorithm, framework,
    and infrastructure levels for high-performance distributed DL in large-scale scenarios
    involving a large number of devices, large data, and large models, where heterogeneity
    and scalability are major research concerns. We cover various topics, including
    distributed model synchronization, communication data compression, resource allocation,
    task scheduling, as well as communication interconnects, devices, protocols, and
    topologies. During the discussion, We highlight the pros and cons of applying
    these technologies in a large-scale setting. We present promising future research
    trends for each topic. Moreover, a case study on the distributed training of large
    language models is showcased to illustrate the practical use of these communication-efficient
    technologies in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew, “Deep
    learning with cots hpc systems,” in *International conference on machine learning*.   PMLR,
    2013, pp. 1337–1345.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *Advances in neural
    information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Bahdanau, K. H. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” in *3rd International Conference on Learning
    Representations, ICLR 2015*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep
    bidirectional transformers for language understanding,” in *Proceedings of NAACL-HLT*,
    2019, pp. 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *International Conference
    on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework
    for self-supervised learning of speech representations,” *Advances in neural information
    processing systems*, vol. 33, pp. 12 449–12 460, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] T. Hollon, C. Jiang, A. Chowdury, M. Nasir-Moin, A. Kondepudi, A. Aabedi,
    A. Adapa, W. Al-Holou, J. Heth, O. Sagher *et al.*, “Artificial-intelligence-based
    molecular classification of diffuse gliomas using rapid, label-free optical imaging,”
    *Nature Medicine*, vol. 29, no. 4, pp. 828–832, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. H. Thieme, Y. Zheng, G. Machiraju, C. Sadee, M. Mittermaier, M. Gertler,
    J. L. Salinas, K. Srinivasan, P. Gyawali, F. Carrillo-Perez *et al.*, “A deep-learning
    algorithm to classify skin lesions from mpox virus infection,” *Nature medicine*,
    vol. 29, no. 3, pp. 738–747, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] C. Yan, J. Qin, Q. Liu, Q. Ma, and Y. Kang, “Mapless navigation with safety-enhanced
    imitation learning,” *IEEE Transactions on Industrial Electronics*, vol. 70, no. 7,
    pp. 7073–7081, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] K. Lee, D. Isele, E. A. Theodorou, and S. Bae, “Spatiotemporal costmap
    inference for mpc via deep inverse reinforcement learning,” *IEEE Robotics and
    Automation Letters*, vol. 7, no. 2, pp. 3194–3201, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Aspri, G. Tsagkatakis, and P. Tsakalides, “Distributed training and
    inference of deep learning models for multi-modal land cover classification,”
    *Remote Sensing*, vol. 12, no. 17, p. 2670, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. M. Haut, M. E. Paoletti, S. Moreno-Álvarez, J. Plaza, J.-A. Rico-Gallego,
    and A. Plaza, “Distributed deep learning for remote sensing data interpretation,”
    *Proceedings of the IEEE*, vol. 109, no. 8, pp. 1320–1349, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.
    Cheng, A. Jin, T. Bos, L. Baker, Y. Du *et al.*, “Lamda: Language models for dialog
    applications,” *arXiv preprint arXiv:2201.08239*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] T.-C. Chiu, Y.-Y. Shih, A.-C. Pang, C.-S. Wang, W. Weng, and C.-T. Chou,
    “Semisupervised distributed learning with non-iid data for aiot service platform,”
    *IEEE Internet of Things Journal*, vol. 7, no. 10, pp. 9266–9277, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. Zhang, P. Patras, and H. Haddadi, “Deep learning in mobile and wireless
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 3,
    pp. 2224–2287, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim, “Applications of deep reinforcement learning in communications and
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 4,
    pp. 3133–3174, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] E. Rodríguez, B. Otero, N. Gutiérrez, and R. Canal, “A survey of deep
    learning techniques for cybersecurity in mobile networks,” *IEEE Communications
    Surveys & Tutorials*, vol. 23, no. 3, pp. 1920–1955, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. A. Al-Garadi, A. Mohamed, A. K. Al-Ali, X. Du, I. Ali, and M. Guizani,
    “A survey of machine and deep learning methods for internet of things (iot) security,”
    *IEEE Communications Surveys & Tutorials*, vol. 22, no. 3, pp. 1646–1685, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] E. Baccour, N. Mhaisen, A. A. Abdellatif, A. Erbad, A. Mohamed, M. Hamdi,
    and M. Guizani, “Pervasive ai for iot applications: A survey on resource-efficient
    distributed artificial intelligence,” *IEEE Communications Surveys & Tutorials*,
    vol. 24, no. 4, pp. 2366–2418, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Chen, X. Sun, and Y. Jin, “Communication-efficient federated deep learning
    with layerwise asynchronous model update and temporally weighted aggregation,”
    *IEEE Transactions on Neural Networks and Learning Systems*, vol. 31, no. 10,
    pp. 4229–4238, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Z. Yang, M. Chen, W. Saad, C. S. Hong, and M. Shikh-Bahaei, “Energy efficient
    federated learning over wireless communication networks,” *IEEE Transactions on
    Wireless Communications*, vol. 20, no. 3, pp. 1935–1949, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] S. Wang, D. Li, J. Geng, Y. Gu, and Y. Cheng, “Impact of network topology
    on the performance of dml: Theoretical analysis and practical factors,” in *IEEE
    INFOCOM 2019-IEEE conference on computer communications*.   IEEE, 2019, pp. 1729–1737.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Xue, Y. Miao, C. Chen, M. Wu, L. Zhang, and L. Zhou, “Fast distributed
    deep learning over rdma,” in *Proceedings of the Fourteenth EuroSys Conference
    2019*, 2019, pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J. Verbraeken, M. Wolting, J. Katzy, J. Kloppenburg, T. Verbelen, and
    J. S. Rellermeyer, “A survey on distributed machine learning,” *Acm computing
    surveys (csur)*, vol. 53, no. 2, pp. 1–33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] T. Ben-Nun and T. Hoefler, “Demystifying parallel and distributed deep
    learning: An in-depth concurrency analysis,” *ACM Computing Surveys (CSUR)*, vol. 52,
    no. 4, pp. 1–43, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] R. Mayer and H.-A. Jacobsen, “Scalable deep learning on distributed infrastructures:
    Challenges, techniques, and tools,” *ACM Computing Surveys (CSUR)*, vol. 53, no. 1,
    pp. 1–37, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] E. P. Xing, Q. Ho, P. Xie, and D. Wei, “Strategies and principles of distributed
    machine learning on big data,” *Engineering*, vol. 2, no. 2, pp. 179–195, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Ouyang, D. Dong, Y. Xu, and L. Xiao, “Communication optimization strategies
    for distributed deep neural network training: A survey,” *Journal of Parallel
    and Distributed Computing*, vol. 149, pp. 52–65, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] E. Yu, D. Dong, and X. Liao, “Communication optimization algorithms for
    distributed deep learning systems: A survey,” *IEEE Transactions on Parallel and
    Distributed Systems*, vol. 34, no. 12, pp. 3294–3308, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] X. Cao, T. Başar, S. Diggavi, Y. C. Eldar, K. B. Letaief, H. V. Poor,
    and J. Zhang, “Communication-efficient distributed learning: An overview,” *IEEE
    journal on selected areas in communications*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Z. Tang, S. Shi, X. Chu, W. Wang, and B. Li, “Communication-efficient
    distributed deep learning: A comprehensive survey,” *arXiv preprint arXiv:2003.06307*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Park, S. Samarakoon, M. Bennis, and M. Debbah, “Wireless network intelligence
    at the edge,” *Proceedings of the IEEE*, vol. 107, no. 11, pp. 2204–2239, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Z. Zhou, X. Chen, E. Li, L. Zeng, K. Luo, and J. Zhang, “Edge intelligence:
    Paving the last mile of artificial intelligence with edge computing,” *Proceedings
    of the IEEE*, vol. 107, no. 8, pp. 1738–1762, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Shi, K. Yang, T. Jiang, J. Zhang, and K. B. Letaief, “Communication-efficient
    edge ai: Algorithms and systems,” *IEEE Communications Surveys & Tutorials*, vol. 22,
    no. 4, pp. 2167–2191, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Deng, H. Zhao, W. Fang, J. Yin, S. Dustdar, and A. Y. Zomaya, “Edge
    intelligence: The confluence of edge computing and artificial intelligence,” *IEEE
    Internet of Things Journal*, vol. 7, no. 8, pp. 7457–7469, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] M. Chen, N. Shlezinger, H. V. Poor, Y. C. Eldar, and S. Cui, “Communication-efficient
    federated learning,” *Proceedings of the National Academy of Sciences*, vol. 118,
    no. 17, p. e2024789118, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] M. S. Murshed, C. Murphy, D. Hou, N. Khan, G. Ananthanarayanan, and F. Hussain,
    “Machine learning at the network edge: A survey,” *ACM Computing Surveys (CSUR)*,
    vol. 54, no. 8, pp. 1–37, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Liu, J. Huang, Y. Zhou, X. Li, S. Ji, H. Xiong, and D. Dou, “From distributed
    machine learning to federated learning: A survey,” *Knowledge and Information
    Systems*, vol. 64, no. 4, pp. 885–917, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Duan, D. Wang, J. Ren, F. Lyu, Y. Zhang, H. Wu, and X. Shen, “Distributed
    artificial intelligence empowered by end-edge-cloud computing: A survey,” *IEEE
    Communications Surveys & Tutorials*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Yao, S. Zhang, Y. Yao, F. Wang, J. Ma, J. Zhang, Y. Chu, L. Ji, K. Jia,
    T. Shen *et al.*, “Edge-cloud polarization and collaboration: A comprehensive
    survey for ai,” *IEEE Transactions on Knowledge and Data Engineering*, vol. 35,
    no. 7, pp. 6866–6886, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Y. E. Sagduyu, S. Ulukus, and A. Yener, “Task-oriented communications
    for nextg: End-to-end deep learning and ai security aspects,” *IEEE Wireless Communications*,
    vol. 30, no. 3, pp. 52–60, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] P. Saikia, S. Biswas, K. Singh, and C.-P. Li, “Signal detection in gsm-based
    in-band full-duplex communication using dnn,” *IEEE Transactions on Vehicular
    Technology*, vol. 72, no. 2, pp. 2661–2666, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] P. Ferrand, A. Decurninge, and M. Guillaud, “Dnn-based localization from
    channel estimates: Feature design and experimental results,” in *GLOBECOM 2020-2020
    IEEE Global Communications Conference*.   IEEE, 2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] I. Amerini, C.-T. Li, and R. Caldelli, “Social network identification
    through image classification with cnn,” *IEEE access*, vol. 7, pp. 35 264–35 273,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] C. Zhang, W. Jiang, Y. Zhang, W. Wang, Q. Zhao, and C. Wang, “Transformer
    and cnn hybrid deep neural network for semantic segmentation of very-high-resolution
    remote sensing imagery,” *IEEE Transactions on Geoscience and Remote Sensing*,
    vol. 60, pp. 1–20, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Bi, J. Xiong, Y. Tian, Q. Li, and K.-K. R. Choo, “Achieving lightweight
    and privacy-preserving object detection for connected autonomous vehicles,” *IEEE
    Internet of Things Journal*, vol. 10, no. 3, pp. 2314–2329, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Hua, Z. Zhao, R. Li, X. Chen, Z. Liu, and H. Zhang, “Deep learning
    with long short-term memory for time series prediction,” *IEEE Communications
    Magazine*, vol. 57, no. 6, pp. 114–119, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Q. Feng, D. He, Z. Liu, H. Wang, and K.-K. R. Choo, “Securenlp: A system
    for multi-party privacy-preserving natural language processing,” *IEEE Transactions
    on Information Forensics and Security*, vol. 15, pp. 3709–3721, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B. Say, “A unified framework for planning with learned neural network
    transition models,” in *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 35, no. 6, 2021, pp. 5016–5024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] M. Li, T. Zhang, Y. Chen, and A. J. Smola, “Efficient mini-batch training
    for stochastic optimization,” in *Proceedings of the 20th ACM SIGKDD international
    conference on Knowledge discovery and data mining*, 2014, pp. 661–670.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
    initialization and momentum in deep learning,” in *Proceedings of the 30th International
    Conference on Machine Learning*, vol. 28, no. 3.   Atlanta, Georgia, USA: PMLR,
    17–19 Jun 2013, pp. 1139–1147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online
    learning and stochastic optimization.” *Journal of machine learning research*,
    vol. 12, no. 7, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
    in *3rd International Conference on Learning Representations, ICLR 2015, San Diego,
    CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] M. Lotfollahi, M. Jafari Siavoshani, R. Shirali Hossein Zade, and M. Saberian,
    “Deep packet: A novel approach for encrypted traffic classification using deep
    learning,” *Soft Computing*, vol. 24, no. 3, pp. 1999–2012, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] L. Vu, Q. U. Nguyen, D. N. Nguyen, D. T. Hoang, E. Dutkiewicz *et al.*,
    “Learning latent representation for iot anomaly detection,” *IEEE Transactions
    on Cybernetics*, vol. 52, no. 5, pp. 3769–3782, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] G. Li, M. Müller, B. Ghanem, and V. Koltun, “Training graph neural networks
    with 1000 layers,” in *International conference on machine learning*.   PMLR,
    2021, pp. 6437–6449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] B. Hanin, “Which neural net architectures give rise to exploding and vanishing
    gradients?” *Advances in neural information processing systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] S. Li, Y. Zhao, R. Varma, O. Salpekar, P. Noordhuis, T. Li, A. Paszke,
    J. Smith, B. Vaughan, P. Damania, and S. Chintala, “Pytorch distributed: Experiences
    on accelerating data parallel training,” *Proc. VLDB Endow.*, vol. 13, no. 12,
    p. 3005–3018, aug 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] M. Li, D. G. Andersen, A. J. Smola, and K. Yu, “Communication efficient
    distributed machine learning with the parameter server,” in *Advances in Neural
    Information Processing Systems*, vol. 27, 2014, pp. 19–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] A. N. Gomez, O. Key, K. Perlin, S. Gou, N. Frosst, J. Dean, and Y. Gal,
    “Interlocking backpropagation: Improving depthwise model-parallelism,” *J. Mach.
    Learn. Res.*, vol. 23, no. 1, jan 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Li and T. Hoefler, “Chimera: efficiently training large-scale neural
    networks with bidirectional pipelines,” in *Proceedings of the International Conference
    for High Performance Computing, Networking, Storage and Analysis*, 2021, pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] W. Liu, Z. Lai, S. Li, Y. Duan, K. Ge, and D. Li, “Autopipe: A fast pipeline
    parallelism approach with balanced partitioning and micro-batch slicing,” in *2022
    IEEE International Conference on Cluster Computing (CLUSTER)*.   IEEE, 2022, pp.
    301–312.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] H. Oh, J. Lee, H. Kim, and J. Seo, “Out-of-order backprop: An effective
    scheduling technique for deep learning,” in *Proceedings of the Seventeenth European
    Conference on Computer Systems*, 2022, pp. 435–452.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. M. Tarnawski, D. Narayanan, and A. Phanishayee, “Piper: Multidimensional
    planner for dnn parallelization,” *Advances in Neural Information Processing Systems*,
    vol. 34, pp. 24 829–24 840, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. Unger, Z. Jia, W. Wu, S. Lin, M. Baines, C. E. Q. Narvaez, V. Ramakrishnaiah,
    N. Prajapati, P. McCormick, J. Mohd-Yusof *et al.*, “Unity: Accelerating dnn training
    through joint optimization of algebraic transformations and parallelization,”
    in *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    22)*, 2022, pp. 267–284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Zhou, Q. Shi, Y. Ding, L. Wang, L. Li, and F. Zhu, “Anttune: An efficient
    distributed hyperparameter optimization system for large-scale data,” in *International
    Conference on Database Systems for Advanced Applications*.   Springer, 2023, pp.
    477–489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. V. Joshi and A. V. Joshi, “Amazon’s machine learning toolkit: Sagemaker,”
    *Machine learning and artificial intelligence*, pp. 233–243, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] W. Ma, T. Zhou, J. Qin, X. Xiang, Y. Tan, and Z. Cai, “A privacy-preserving
    content-based image retrieval method based on deep learning in cloud computing,”
    *Expert Systems with Applications*, vol. 203, p. 117508, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] F. Desai, D. Chowdhury, R. Kaur, M. Peeters, R. C. Arya, G. S. Wander,
    S. S. Gill, and R. Buyya, “Healthcloud: A system for monitoring health status
    of heart patients using machine learning and cloud computing,” *Internet of Things*,
    vol. 17, p. 100485, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] X. Wang, L. Zhang, Y. Liu, C. Zhao, and K. Wang, “Solving task scheduling
    problems in cloud manufacturing via attention mechanism and deep reinforcement
    learning,” *Journal of Manufacturing Systems*, vol. 65, pp. 452–468, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] L. Zhang, C. Yang, Y. Yan, and Y. Hu, “Distributed real-time scheduling
    in cloud manufacturing by deep reinforcement learning,” *IEEE Transactions on
    Industrial Informatics*, vol. 18, no. 12, pp. 8999–9007, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Xu, W. C. Ng, W. Y. B. Lim, J. Kang, Z. Xiong, D. Niyato, Q. Yang,
    X. S. Shen, and C. Miao, “A full dive into realizing the edge-enabled metaverse:
    Visions, enabling technologies, and challenges,” *IEEE Communications Surveys
    & Tutorials*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] P. Arthurs, L. Gillam, P. Krause, N. Wang, K. Halder, and A. Mouzakitis,
    “A taxonomy and survey of edge cloud computing for intelligent transportation
    systems and connected vehicles,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 23, no. 7, pp. 6206–6221, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Wu, B. Yang, D. Zhu, Q. Liu, C. Li, C. Chen, and X. Guan, “To transmit
    or predict: An efficient industrial data transmission scheme with deep learning
    and cloud-edge collaboration,” *IEEE Transactions on Industrial Informatics*,
    vol. 19, no. 11, pp. 11 322–11 332, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Q. Wu, X. Chen, Z. Zhou, and J. Zhang, “Fedhome: Cloud-edge based personalized
    federated learning for in-home health monitoring,” *IEEE Transactions on Mobile
    Computing*, vol. 21, no. 8, pp. 2818–2832, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. A. Alzubi, O. A. Alzubi, A. Singh, and M. Ramachandran, “Cloud-iiot-based
    electronic health record privacy-preserving by cnn and blockchain-enabled federated
    learning,” *IEEE Transactions on Industrial Informatics*, vol. 19, no. 1, pp.
    1080–1087, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] D. C. Nguyen, M. Ding, Q.-V. Pham, P. N. Pathirana, L. B. Le, A. Seneviratne,
    J. Li, D. Niyato, and H. V. Poor, “Federated learning meets blockchain in edge
    computing: Opportunities and challenges,” *IEEE Internet of Things Journal*, vol. 8,
    no. 16, pp. 12 806–12 825, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Li, X. Tao, X. Zhang, J. Liu, and J. Xu, “Privacy-preserved federated
    learning for autonomous driving,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 23, no. 7, pp. 8423–8434, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] O. Bouachir, M. Aloqaily, Ö. Özkasap, and F. Ali, “Federatedgrids: Federated
    learning and blockchain-assisted p2p energy sharing,” *IEEE Transactions on Green
    Communications and Networking*, vol. 6, no. 1, pp. 424–436, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. Batra, Z. Huang, A. Petrenko, T. Kumar, A. Molchanov, and G. S. Sukhatme,
    “Decentralized control of quadrotor swarms with end-to-end deep reinforcement
    learning,” in *Proceedings of the 5th Conference on Robot Learning*, vol. 164.   PMLR,
    08–11 Nov 2022, pp. 576–586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] X. Zhou, W. Liang, K. I.-K. Wang, Z. Yan, L. T. Yang, W. Wei, J. Ma, and
    Q. Jin, “Decentralized p2p federated learning for privacy-preserving and resilient
    mobile robotic systems,” *IEEE Wireless Communications*, vol. 30, no. 2, pp. 82–89,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] D. Liu, X. Chen, Z. Zhou, and Q. Ling, “Hiertrain: Fast hierarchical edge
    ai learning with hybrid parallelism in mobile-edge-cloud computing,” *IEEE Open
    Journal of the Communications Society*, vol. 1, pp. 634–645, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Z. Chen, L. Shi, X. Liu, J. Li, S. Liu, and Y. Xu, “Osp: Boosting distributed
    model training with 2-stage synchronization,” in *Proceedings of the 52nd International
    Conference on Parallel Processing*, New York, NY, USA, 2023, p. 102–111.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. a. Ranzato,
    A. Senior, P. Tucker, K. Yang, Q. Le, and A. Ng, “Large scale distributed deep
    networks,” in *Advances in Neural Information Processing Systems*, vol. 25, 2012,
    pp. 1223–1231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A.-L. Jin, W. Xu, S. Guo, B. Hu, and K. Yeung, “Ps+: A simple yet effective
    framework for fast training on parameter server,” *IEEE Transactions on Parallel
    and Distributed Systems*, vol. 33, no. 12, pp. 4625–4637, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Q. Ho, J. Cipar, H. Cui, S. Lee, J. K. Kim, P. B. Gibbons, G. A. Gibson,
    G. Ganger, and E. P. Xing, “More effective distributed ml via a stale synchronous
    parallel parameter server,” in *Advances in Neural Information Processing Systems*,
    vol. 26, 2013, pp. 1223–1231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] B. McMahan and M. Streeter, “Delay-tolerant algorithms for asynchronous
    distributed online learning,” in *Advances in Neural Information Processing Systems*,
    vol. 27, 2014, pp. 2915–2923.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] C. Chen, W. Wang, and B. Li, “Round-robin synchronization: Mitigating
    communication bottlenecks in parameter servers,” in *IEEE INFOCOM 2019-IEEE Conference
    on Computer Communications*.   IEEE, 2019, pp. 532–540.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Li, J. Huang, Z. Li, S. Zhou, W. Jiang, and J. Wang, “Hsp: Hybrid synchronous
    parallelism for fast distributed deep learning,” in *Proceedings of the 51st International
    Conference on Parallel Processing*, New York, NY, USA, 2022, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] S. Zhang, A. E. Choromanska, and Y. LeCun, “Deep learning with elastic
    averaging sgd,” in *Advances in Neural Information Processing Systems*, vol. 28,
    2015, pp. 685–693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Wang and G. Joshi, “Adaptive communication strategies to achieve the
    best error-runtime trade-off in local-update sgd,” in *Proceedings of Machine
    Learning and Systems*, vol. 1, 2019, pp. 212–229.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi, “Don’t use large mini-batches,
    use local sgd,” in *International Conference on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Wang, V. Tantia, N. Ballas, and M. Rabbat, “Slowmo: Improving communication-efficient
    distributed sgd with slow momentum,” in *International Conference on Learning
    Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] T. Chen, G. Giannakis, T. Sun, and W. Yin, “Lag: Lazily aggregated gradient
    for communication-efficient distributed learning,” *Advances in neural information
    processing systems*, vol. 31, p. 5055–5065, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. George and P. Gurram, “Distributed stochastic gradient descent with
    event-triggered communication,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 34, no. 05, 2020, pp. 7169–7178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Z. Wang, Y. Tu, N. Wang, L. Gao, J. Nie, Z. Wei, Y. Gu, and G. Yu, “Fsp:
    Towards flexible synchronous parallel frameworks for distributed machine learning,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 34, no. 2, pp. 687–703,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu, “Can decentralized
    algorithms outperform centralized algorithms? a case study for decentralized parallel
    stochastic gradient descent,” in *Advances in Neural Information Processing Systems*,
    vol. 30, 2017, pp. 5330–5340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu, “D²: Decentralized training
    over decentralized data,” in *International Conference on Machine Learning*.   PMLR,
    2018, pp. 4848–4856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] H. Sun, Z. Gui, S. Guo, Q. Qi, J. Wang, and J. Liao, “Gssp: Eliminating
    stragglers through grouping synchronous for distributed deep learning in heterogeneous
    cluster,” *IEEE Transactions on Cloud Computing*, vol. 10, no. 4, pp. 2637–2648,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Tan, W.-X. Liu, J. Luo, H. Chen, and Z.-Z. Guo, “Adaptive synchronous
    strategy for distributed machine learning,” *International Journal of Intelligent
    Systems*, vol. 37, no. 12, pp. 11 713–11 741, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Z. Shen, Q. Tang, T. Zhou, Y. Zhang, Z. Jia, D. Yu, Z. Zhang, and B. Li,
    “Ashl: An adaptive multi-stage distributed deep learning training scheme for heterogeneous
    environments,” *IEEE Transactions on Computers*, pp. 1–14, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] F. Zhou and G. Cong, “On the convergence properties of a k-step averaging
    stochastic gradient descent algorithm for nonconvex optimization,” in *Proceedings
    of the 27th International Joint Conference on Artificial Intelligence*, 2018,
    pp. 3219–3227.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. U. Stich, “Local sgd converges fast and communicates little,” in *ICLR
    2019-International Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] H. Yu, S. Yang, and S. Zhu, “Parallel restarted sgd with faster convergence
    and less communication: Demystifying why model averaging works for deep learning,”
    ser. AAAI’19/IAAI’19/EAAI’19.   AAAI Press, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe, “Local sgd with
    periodic averaging: Tighter analysis and adaptive synchronization,” in *Advances
    in Neural Information Processing Systems*, vol. 32, 2019, pp. 11 082–11 094.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] H. Yu and R. Jin, “On the computation and communication complexity of
    parallel SGD with dynamic batch sizes for stochastic non-convex optimization,”
    in *Proceedings of the 36th International Conference on Machine Learning*, vol. 97.   PMLR,
    09–15 Jun 2019, pp. 7174–7183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] B. Woodworth, K. K. Patel, S. Stich, Z. Dai, B. Bullins, B. Mcmahan,
    O. Shamir, and N. Srebro, “Is local SGD better than minibatch SGD?” in *Proceedings
    of the 37th International Conference on Machine Learning*, vol. 119.   PMLR, 13–18
    Jul 2020, pp. 10 334–10 343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] A. Spiridonoff, A. Olshevsky, and Y. Paschalidis, “Communication-efficient
    sgd: From local sgd to one-shot averaging,” in *Advances in Neural Information
    Processing Systems*, vol. 34, 2021, pp. 24 313–24 326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] J. Wang and G. Joshi, “Cooperative sgd: A unified framework for the design
    and analysis of local-update sgd algorithms,” vol. 22, no. 1, jan 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. U. Stich and S. P. Karimireddy, “The error-feedback framework: Better
    rates for sgd with delayed gradients and compressed updates,” *The Journal of
    Machine Learning Research*, vol. 21, no. 1, pp. 9613–9648, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] R. Z. Aviv, I. Hakimi, A. Schuster, and K. Y. Levy, “Asynchronous distributed
    learning : Adapting to gradient delays without prior knowledge,” in *Proceedings
    of the 38th International Conference on Machine Learning*, vol. 139.   PMLR, 18–24
    Jul 2021, pp. 436–445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] A. Cohen, A. Daniely, Y. Drori, T. Koren, and M. Schain, “Asynchronous
    stochastic optimization robust to arbitrary delays,” *Advances in Neural Information
    Processing Systems*, vol. 34, pp. 9024–9035, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A. Koloskova, S. U. Stich, and M. Jaggi, “Sharper convergence guarantees
    for asynchronous sgd for distributed and federated learning,” *Advances in Neural
    Information Processing Systems*, vol. 35, pp. 17 202–17 215, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] K. Mishchenko, F. Bach, M. Even, and B. E. Woodworth, “Asynchronous sgd
    beats minibatch sgd under arbitrary delays,” *Advances in Neural Information Processing
    Systems*, vol. 35, pp. 420–433, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
    learning of deep networks from decentralized data,” in *Artificial intelligence
    and statistics*.   PMLR, 2017, pp. 1273–1282.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] P. Zhou, Q. Lin, D. Loghin, B. C. Ooi, Y. Wu, and H. Yu, “Communication-efficient
    decentralized machine learning over heterogeneous networks,” in *2021 IEEE 37th
    International Conference on Data Engineering (ICDE)*, 2021, pp. 384–395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] C. Chen, H. Xu, W. Wang, B. Li, B. Li, L. Chen, and G. Zhang, “Communication-efficient
    federated learning with adaptive parameter freezing,” in *2021 IEEE 41st International
    Conference on Distributed Computing Systems (ICDCS)*, 2021, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] ——, “Synchronize only the immature parameters: Communication-efficient
    federated learning by freezing parameters adaptively,” *IEEE Transactions on Parallel
    and Distributed Systems*, pp. 1–18, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Liu, J. Liu, H. Xu, Y. Liao, Z. Wang, and Q. Ma, “Yoga: Adaptive layer-wise
    model aggregation for decentralized federated learning,” *IEEE/ACM Transactions
    on Networking*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
    “Federated optimization in heterogeneous networks,” in *Proceedings of Machine
    Learning and Systems*, vol. 2, 2020, pp. 429–450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, “Tackling the objective
    inconsistency problem in heterogeneous federated optimization,” *Advances in neural
    information processing systems*, vol. 33, pp. 7611–7623, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Esfandiari, S. Y. Tan, Z. Jiang, A. Balu, E. Herron, C. Hegde, and
    S. Sarkar, “Cross-gradient aggregation for decentralized learning from non-iid
    data,” in *Proceedings of the 38th International Conference on Machine Learning*,
    vol. 139.   PMLR, 18–24 Jul 2021, pp. 3036–3046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] M. Chen, Y. Xu, H. Xu, and L. Huang, “Enhancing decentralized federated
    learning for non-iid data on heterogeneous devices,” in *2023 IEEE 39th International
    Conference on Data Engineering (ICDE)*, 2023, pp. 2289–2302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] K. Bonawitz, H. Eichner, W. Grieskamp, D. Huba, A. Ingerman, V. Ivanov,
    C. Kiddon, J. Konečný, S. Mazzocchi, B. McMahan, T. Van Overveldt, D. Petrou,
    D. Ramage, and J. Roselander, “Towards federated learning at scale: System design,”
    in *Proceedings of Machine Learning and Systems*, vol. 1, 2019, pp. 374–388.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Z. Wang, H. Xu, J. Liu, Y. Xu, H. Huang, and Y. Zhao, “Accelerating federated
    learning with cluster construction and hierarchical aggregation,” *IEEE Transactions
    on Mobile Computing*, vol. 22, no. 7, pp. 3805–3822, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] F. P.-C. Lin, S. Hosseinalipour, S. S. Azam, C. G. Brinton, and N. Michelusi,
    “Semi-decentralized federated learning with cooperative d2d local model aggregations,”
    *IEEE Journal on Selected Areas in Communications*, vol. 39, no. 12, pp. 3851–3869,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] M. Ryabinin, E. Gorbunov, V. Plokhotnyuk, and G. Pekhimenko, “Moshpit
    sgd: Communication-efficient decentralized training on heterogeneous unreliable
    devices,” in *Advances in Neural Information Processing Systems*, vol. 34, 2021,
    pp. 18 195–18 211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and K. Chan,
    “Adaptive federated learning in resource constrained edge computing systems,”
    *IEEE Journal on Selected Areas in Communications*, vol. 37, no. 6, pp. 1205–1221,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Y. Xu, Y. Liao, H. Xu, Z. Ma, L. Wang, and J. Liu, “Adaptive control
    of local updating and model compression for efficient federated learning,” *IEEE
    Transactions on Mobile Computing*, vol. 22, no. 10, pp. 5675–5689, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Y. Liao, Y. Xu, H. Xu, Z. Yao, L. Wang, and C. Qiao, “Accelerating federated
    learning with data and model parallelism in edge computing,” *IEEE/ACM Transactions
    on Networking*, pp. 1–15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Wang, H. Xu, Y. Xu, Z. Jiang, J. Liu, and S. Chen, “Fast: Enhancing
    federated learning through adaptive data sampling and local training,” *IEEE Transactions
    on Parallel and Distributed Systems*, pp. 1–15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] J. Park, D. Yoon, S. Yeo, and S. Oh, “Amble: Adjusting mini-batch and
    local epoch for federated learning with heterogeneous devices,” *Journal of Parallel
    and Distributed Computing*, vol. 170, pp. 13–23, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Y. Liu, L. Xu, X. Yuan, C. Wang, and B. Li, “The right to be forgotten
    in federated learning: An efficient realization with rapid retraining,” in *IEEE
    INFOCOM 2022 - IEEE Conference on Computer Communications*, 2022, pp. 1749–1758.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and
    G. E. Dahl, “Measuring the effects of data parallelism on neural network training,”
    *Journal of Machine Learning Research*, vol. 20, no. 112, pp. 1–49, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] H. Karimi, J. Nutini, and M. Schmidt, “Linear convergence of gradient
    and proximal-gradient methods under the polyak-łojasiewicz condition,” in *Machine
    Learning and Knowledge Discovery in Databases*, 2016, pp. 795–811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] B. Neyshabur, Z. Li, S. Bhojanapalli, Y. LeCun, and N. Srebro, “The role
    of over-parametrization in generalization of neural networks,” in *International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
    D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for privacy-preserving
    machine learning,” in *Proceedings of the 2017 ACM SIGSAC Conference on Computer
    and Communications Security*, New York, NY, USA, 2017, p. 1175–1191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Z. Yao, A. Gholami, S. Shen, M. Mustafa, K. Keutzer, and M. Mahoney,
    “Adahessian: An adaptive second order optimizer for machine learning,” in *proceedings
    of the AAAI conference on artificial intelligence*, vol. 35, no. 12, 2021, pp.
    10 665–10 673.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu, “1-bit stochastic gradient
    descent and its application to data-parallel distributed training of speech dnns,”
    in *Fifteenth annual conference of the international speech communication association*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, “signsgd:
    Compressed optimisation for non-convex problems,” in *International Conference
    on Machine Learning*.   PMLR, 2018, pp. 560–569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi, “Error feedback
    fixes signsgd and other gradient compression schemes,” in *International Conference
    on Machine Learning*.   PMLR, 2019, pp. 3252–3261.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu,
    C. Zhang, and Y. He, “1-bit adam: Communication efficient large-scale training
    with adam’s convergence speed,” in *Proceedings of the 38th International Conference
    on Machine Learning*, vol. 139.   PMLR, 18–24 Jul 2021, pp. 10 118–10 129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan, “Deep learning
    with limited numerical precision,” in *International conference on machine learning*.   PMLR,
    2015, pp. 1737–1746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou, “Dorefa-net: Training
    low bitwidth convolutional neural networks with low bitwidth gradients,” *arXiv
    preprint arXiv:1606.06160*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio, “Quantized
    neural networks: Training neural networks with low precision weights and activations,”
    *The Journal of Machine Learning Research*, vol. 18, no. 1, pp. 6869–6898, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang, “Zipml:
    Training linear models with end-to-end low precision, and a little bit of deep
    learning,” in *International Conference on Machine Learning*.   PMLR, 2017, pp.
    4035–4043.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Horvóth, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtárik,
    “Natural compression for distributed deep learning,” in *Mathematical and Scientific
    Machine Learning*.   PMLR, 2022, pp. 129–141.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. T. Suresh, X. Y. Felix, S. Kumar, and H. B. McMahan, “Distributed
    mean estimation with limited communication,” in *International conference on machine
    learning*.   PMLR, 2017, pp. 3329–3337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] H. Wang, S. Sievert, S. Liu, Z. Charles, D. Papailiopoulos, and S. Wright,
    “Atomo: Communication-efficient learning via atomic sparsification,” *Advances
    in neural information processing systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] T. Vogels, S. P. Karimireddy, and M. Jaggi, “Powersgd: Practical low-rank
    gradient compression for distributed optimization,” *Advances in Neural Information
    Processing Systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] ——, “Practical low-rank communication compression in decentralized deep
    learning,” *Advances in Neural Information Processing Systems*, vol. 33, pp. 14 171–14 181,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Y. Dong, L. Wang, J. Wang, X. Hu, H. Zhang, F. R. Yu, and V. C. M. Leung,
    “Accelerating wireless federated learning via nesterov’s momentum and distributed
    principle component analysis,” *IEEE Transactions on Wireless Communications*,
    pp. 1–1, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] D. Alistarh, D. Grubic, J. Li, R. Tomioka, and M. Vojnovic, “Qsgd: Communication-efficient
    sgd via gradient quantization and encoding,” *Advances in neural information processing
    systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] J. Konečnỳ and P. Richtárik, “Randomized distributed mean estimation:
    Accuracy vs. communication,” *Frontiers in Applied Mathematics and Statistics*,
    vol. 4, p. 62, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li, “Terngrad:
    Ternary gradients to reduce communication in distributed deep learning,” *Advances
    in neural information processing systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J. Wu, W. Huang, J. Huang, and T. Zhang, “Error compensated quantized
    sgd and its applications to large-scale distributed optimization,” in *International
    Conference on Machine Learning*.   PMLR, 2018, pp. 5325–5333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Ramezani-Kebrya, F. Faghri, I. Markov, V. Aksenov, D. Alistarh, and
    D. M. Roy, “Nuqsgd: Provably communication-efficient data-parallel sgd via nonuniform
    quantization,” *Journal of Machine Learning Research*, vol. 22, no. 1, jan 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya,
    “Adaptive gradient quantization for data-parallel sgd,” *Advances in neural information
    processing systems*, vol. 33, pp. 3174–3185, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] K. Mishchenko, B. Wang, D. Kovalev, and P. Richtárik, “IntSGD: Adaptive
    floatless compression of stochastic gradients,” in *International Conference on
    Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Y. Mao, Z. Zhao, G. Yan, Y. Liu, T. Lan, L. Song, and W. Ding, “Communication-efficient
    federated learning with adaptive quantization,” *ACM Transactions on Intelligent
    Systems and Technology (TIST)*, vol. 13, no. 4, pp. 1–26, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] H. Liu, F. He, and G. Cao, “Communication-efficient federated learning
    for heterogeneous edge devices based on adaptive gradient quantization,” in *IEEE
    INFOCOM 2023-IEEE Conference on Computer Communications*.   IEEE, 2023, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] N. Ström, “Scalable distributed dnn training using commodity gpu cloud
    computing,” in *Interspeech 2015*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] A. F. Aji and K. Heafield, “Sparse communication for distributed gradient
    descent,” in *Proceedings of the 2017 Conference on Empirical Methods in Natural
    Language Processing*, 2017, pp. 440–445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] S. U. Stich, J.-B. Cordonnier, and M. Jaggi, “Sparsified sgd with memory,”
    in *Advances in Neural Information Processing Systems*, vol. 31.   Curran Associates,
    Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally, “Deep gradient compression:
    Reducing the communication bandwidth for distributed training,” in *International
    Conference on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] D. Xiao, Y. Mei, D. Kuang, M. Chen, B. Guo, and W. Wu, “Egc: Entropy-based
    gradient compression for distributed deep learning,” *Information Sciences*, vol.
    548, pp. 118–134, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Z. Zhang and C. Wang, “Mipd: An adaptive gradient sparsification framework
    for distributed dnns training,” *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 33, no. 11, pp. 3053–3066, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, and X. Chu, “A
    distributed synchronous sgd algorithm with global top-k sparsification for low
    bandwidth networks,” in *2019 IEEE 39th International Conference on Distributed
    Computing Systems (ICDCS)*, 2019, pp. 2238–2247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] C.-Y. Chen, J. Ni, S. Lu, X. Cui, P.-Y. Chen, X. Sun, N. Wang, S. Venkataramani,
    V. V. Srinivasan, W. Zhang *et al.*, “Scalecom: Scalable sparsified gradient compression
    for communication-efficient distributed training,” *Advances in Neural Information
    Processing Systems*, vol. 33, pp. 13 551–13 563, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. M Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini, “An efficient
    statistical-based gradient compression technique for distributed training systems,”
    *Proceedings of Machine Learning and Systems*, vol. 3, pp. 297–322, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Shi, X. Zhou, S. Song, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou,
    Z. Guo, L. Xie *et al.*, “Towards scalable distributed training of deep learning
    on public cloud clusters,” *Proceedings of Machine Learning and Systems*, vol. 3,
    pp. 401–412, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Li and T. Hoefler, “Near-optimal sparse allreduce for distributed
    deep learning,” in *Proceedings of the 27th ACM SIGPLAN Symposium on Principles
    and Practice of Parallel Programming*, 2022, pp. 135–149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] R. Liu and B. Mozafari, “Communication-efficient distributed learning
    for large batch optimization,” in *International Conference on Machine Learning*.   PMLR,
    2022, pp. 13 925–13 946.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Wangni, J. Wang, J. Liu, and T. Zhang, “Gradient sparsification for
    communication-efficient distributed optimization,” *Advances in Neural Information
    Processing Systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] D. Alistarh, T. Hoefler, M. Johansson, N. Konstantinov, S. Khirirat,
    and C. Renggli, “The convergence of sparsified gradient methods,” *Advances in
    Neural Information Processing Systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] L. Wang, W. Wu, J. Zhang, H. Liu, G. Bosilca, M. Herlihy, and R. Fonseca,
    “Fft-based gradient sparsification for the distributed training of deep neural
    networks,” in *Proceedings of the 29th International Symposium on High-Performance
    Parallel and Distributed Computing*, 2020, pp. 113–124.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Sahu, A. Dutta, A. M. Abdelmoniem, T. Banerjee, M. Canini, and P. Kalnis,
    “Rethinking gradient sparsification as total error minimization,” in *Advances
    in Neural Information Processing Systems*, vol. 34, 2021, pp. 8133–8146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] S. Shi, Q. Wang, X. Chu, B. Li, Y. Qin, R. Liu, and X. Zhao, “Communication-efficient
    distributed deep learning with merged gradient sparsification on gpus,” in *IEEE
    INFOCOM 2020-IEEE Conference on Computer Communications*.   IEEE, 2020, pp. 406–415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Z. Wang, Z. Xu, X. Wu, A. Shrivastava, and T. S. E. Ng, “DRAGONN: Distributed
    randomized approximate gradients of neural networks,” in *Proceedings of the 39th
    International Conference on Machine Learning*, vol. 162.   PMLR, 17–23 Jul 2022,
    pp. 23 274–23 291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek, “Robust and communication-efficient
    federated learning from non-i.i.d. data,” *IEEE Transactions on Neural Networks
    and Learning Systems*, vol. 31, no. 9, pp. 3400–3413, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Z. Tang, S. Shi, B. Li, and X. Chu, “Gossipfl: A decentralized federated
    learning framework with sparsified and adaptive communication,” *IEEE Transactions
    on Parallel and Distributed Systems*, vol. 34, no. 3, pp. 909–922, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] L. Yi, W. Gang, and L. Xiaoguang, “QSFL: A two-level uplink communication
    optimization framework for federated learning,” in *Proceedings of the 39th International
    Conference on Machine Learning*, vol. 162.   PMLR, 17–23 Jul 2022, pp. 25 501–25 513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] P. Han, S. Wang, and K. K. Leung, “Adaptive gradient sparsification for
    efficient federated learning: An online learning approach,” in *2020 IEEE 40th
    international conference on distributed computing systems (ICDCS)*.   IEEE, 2020,
    pp. 300–310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Z. Feng, X. Chen, Q. Wu, W. Wu, X. Zhang, and Q. Huang, “Feddd: Toward
    communication-efficient federated learning with differential parameter dropout,”
    *IEEE Transactions on Mobile Computing*, pp. 1–18, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] J. Fang, H. Fu, G. Yang, and C.-J. Hsieh, “Redsync: reducing synchronization
    bandwidth for distributed deep learning training system,” *Journal of Parallel
    and Distributed Computing*, vol. 133, pp. 30–39, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] P. Jiang and G. Agrawal, “A linear speedup analysis of distributed deep
    learning with sparse and quantized communication,” *Advances in Neural Information
    Processing Systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] D. Basu, D. Data, C. Karakus, and S. Diggavi, “Qsparse-local-sgd: Distributed
    sgd with quantization, sparsification and local computations,” *Advances in Neural
    Information Processing Systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] G. Yan, T. Li, S.-L. Huang, T. Lan, and L. Song, “Ac-sgd: Adaptively
    compressed sgd for communication-efficient distributed learning,” *IEEE Journal
    on Selected Areas in Communications*, vol. 40, no. 9, pp. 2678–2693, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] H. Lim, D. G. Andersen, and M. Kaminsky, “3lc: Lightweight and effective
    traffic compression for distributed machine learning,” *Proceedings of Machine
    Learning and Systems*, vol. 1, pp. 53–64, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] C. Yang, Y. Zhao, G. Zhao, and H. Xu, “Dfs: Joint data formatting and
    sparsification for efficient communication in distributed machine learning,” *Computer
    Networks*, vol. 229, p. 109777, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] R. Song, L. Zhou, L. Lyu, A. Festag, and A. Knoll, “Resfed: Communication
    efficient federated learning with deep compressed residuals,” *IEEE Internet of
    Things Journal*, pp. 1–1, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] L. Abrahamyan, Y. Chen, G. Bekoulis, and N. Deligiannis, “Learned gradient
    compression for distributed deep learning,” *IEEE Transactions on Neural Networks
    and Learning Systems*, vol. 33, no. 12, pp. 7330–7344, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Z. Wang, M. Wen, Y. Xu, Y. Zhou, J. H. Wang, and L. Zhang, “Communication
    compression techniques in distributed deep learning: A survey,” *Journal of Systems
    Architecture*, p. 102927, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] R. Banner, I. Hubara, E. Hoffer, and D. Soudry, “Scalable methods for
    8-bit training of neural networks,” in *Advances in Neural Information Processing
    Systems*, vol. 31.   Curran Associates, Inc., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] G. Stewart and J. Miller, “Methods of simultaneous iteration for calculating
    eigenvectors of matrices,” *Topics in Numerical Analysis II*, vol. 2, 1975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] S. Gunasekar, J. Lee, D. Soudry, and N. Srebro, “Characterizing implicit
    bias in terms of optimization geometry,” in *International Conference on Machine
    Learning*.   PMLR, 2018, pp. 1832–1841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training
    recurrent neural networks,” in *International conference on machine learning*.   Pmlr,
    2013, pp. 1310–1318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] I. Mitliagkas, C. Zhang, S. Hadjis, and C. Ré, “Asynchrony begets momentum,
    with an application to deep learning,” in *2016 54th Annual Allerton Conference
    on Communication, Control, and Computing (Allerton)*.   IEEE, 2016, pp. 997–1004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, and K. He, “Accurate, large minibatch sgd: Training imagenet
    in 1 hour,” *arXiv preprint arXiv:1706.02677*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Y. You, I. Gitman, and B. Ginsburg, “Scaling sgd batch size to 32k for
    imagenet training,” *arXiv preprint arXiv:1708.03888*, vol. 6, no. 12, p. 6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch optimization for deep learning:
    Training bert in 76 minutes,” *arXiv preprint arXiv:1904.00962*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] A. H. Robinson and C. Cherry, “Results of a prototype television bandwidth
    compression scheme,” *Proceedings of the IEEE*, vol. 55, no. 3, pp. 356–364, 1967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] W. Xiao, R. Bhardwaj, R. Ramjee, M. Sivathanu, N. Kwatra, Z. Han, P. Patel,
    X. Peng, H. Zhao, Q. Zhang *et al.*, “Gandiva: Introspective cluster scheduling
    for deep learning,” in *13th USENIX Symposium on Operating Systems Design and
    Implementation (OSDI 18)*, 2018, pp. 595–610.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] W. Xiao, S. Ren, Y. Li, Y. Zhang, P. Hou, Z. Li, Y. Feng, W. Lin, and
    Y. Jia, “Antman: Dynamic scaling on gpu clusters for deep learning,” in *14th
    USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)*, 2020,
    pp. 533–548.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Q. Weng, L. Yang, Y. Yu, W. Wang, X. Tang, G. Yang, and L. Zhang, “Beware
    of fragmentation: Scheduling gpu-sharing workloads with fragmentation gradient
    descent,” in *2023 USENIX Annual Technical Conference (USENIX ATC 23)*, 2023,
    pp. 995–1008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] B. Wu, Z. Zhang, Z. Bai, X. Liu, and X. Jin, “Transparent gpu sharing
    in container clouds for deep learning workloads,” in *20th USENIX Symposium on
    Networked Systems Design and Implementation (NSDI 23)*, 2023, pp. 69–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P. Yu and M. Chowdhury, “Salus: Fine-grained gpu sharing primitives for
    deep learning applications,” in *Proceedings of the 3rd MLSys Conference*, 2020,
    pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Z. Bai, Z. Zhang, Y. Zhu, and X. Jin, “Pipeswitch: Fast pipelined context
    switching for deep learning applications,” in *14th USENIX Symposium on Operating
    Systems Design and Implementation (OSDI 20)*, 2020, pp. 499–514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Y. Peng, Y. Bao, Y. Chen, C. Wu, and C. Guo, “Optimus: an efficient dynamic
    resource scheduler for deep learning clusters,” in *Proceedings of the Thirteenth
    EuroSys Conference*, 2018, pp. 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Y. Bao, Y. Peng, and C. Wu, “Deep learning-based job placement in distributed
    machine learning clusters,” in *IEEE INFOCOM 2019 - IEEE Conference on Computer
    Communications*, 2019, pp. 505–513.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] G. Yeung, D. Borowiec, R. Yang, A. Friday, R. Harper, and P. Garraghan,
    “Horus: Interference-aware and prediction-based scheduling in deep learning systems,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 33, no. 1, pp. 88–100,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] A. Qiao, S. K. Choe, S. J. Subramanya, W. Neiswanger, Q. Ho, H. Zhang,
    G. R. Ganger, and E. P. Xing, “Pollux: Co-adaptive cluster scheduling for goodput-optimized
    deep learning,” in *15th USENIX Symposium on Operating Systems Design and Implementation
    (OSDI 21)*, Jul. 2021, pp. 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] G. Lim, J. Ahn, W. Xiao, Y. Kwon, and M. Jeon, “Zico: Efficient GPU memory
    sharing for concurrent DNN training,” in *2021 USENIX Annual Technical Conference
    (USENIX ATC 21)*, Jul. 2021, pp. 161–175.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] C. Hwang, T. Kim, S. Kim, J. Shin, and K. Park, “Elastic resource sharing
    for distributed deep learning,” in *18th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 21)*, 2021, pp. 721–739.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Y. Mao, V. Sharma, W. Zheng, L. Cheng, Q. Guan, and A. Li, “Elastic resource
    management for deep learning applications in a container cluster,” *IEEE Transactions
    on Cloud Computing*, vol. 11, no. 2, pp. 2204–2216, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] P. Yu, J. Liu, and M. Chowdhury, “Fluid: Resource-aware hyperparameter
    tuning engine,” in *Proceedings of Machine Learning and Systems*, vol. 3, 2021,
    pp. 502–516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] W. Gao, P. Sun, Y. Wen, and T. Zhang, “Titan: a scheduler for foundation
    model fine-tuning workloads,” in *Proceedings of the 13th Symposium on Cloud Computing*,
    2022, pp. 348–354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] L. Liu, J. Yu, and Z. Ding, “Adaptive and efficient gpu time sharing
    for hyperparameter tuning in cloud,” in *Proceedings of the 51st International
    Conference on Parallel Processing*, 2022, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Q. Hu, Z. Ye, M. Zhang, Q. Chen, P. Sun, Y. Wen, and T. Zhang, “Hydro:
    Surrogate-based hyperparameter tuning service in datacenters,” in *17th USENIX
    Symposium on Operating Systems Design and Implementation (OSDI 23)*, 2023, pp.
    757–777.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] R. Gu, Y. Chen, S. Liu, H. Dai, G. Chen, K. Zhang, Y. Che, and Y. Huang,
    “Liquid: Intelligent resource estimation and network-efficient scheduling for
    deep learning jobs on distributed gpu clusters,” *IEEE Transactions on Parallel
    and Distributed Systems*, vol. 33, no. 11, pp. 2808–2820, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Z. Zhang, Q. Qi, R. Shang, L. Chen, and F. Xu, “Prophet: Speeding up
    distributed dnn training with predictable communication scheduling,” in *Proceedings
    of the 50th International Conference on Parallel Processing*, 2021, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] W. Li, S. Chen, K. Li, H. Qi, R. Xu, and S. Zhang, “Efficient online
    scheduling for coflow-aware machine learning clusters,” *IEEE Transactions on
    Cloud Computing*, vol. 10, no. 4, pp. 2564–2579, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] A. Dhakal, S. G. Kulkarni, and K. Ramakrishnan, “Gslice: controlled spatial
    sharing of gpus for a scalable inference platform,” in *Proceedings of the 11th
    ACM Symposium on Cloud Computing*, 2020, pp. 492–506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] F. Xu, J. Xu, J. Chen, L. Chen, R. Shang, Z. Zhou, and F. Liu, “igniter:
    Interference-aware gpu resource provisioning for predictable dnn inference in
    the cloud,” *IEEE Transactions on Parallel and Distributed Systems*, vol. 34,
    no. 3, pp. 812–827, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] J. Cho, D. Zad Tootaghaj, L. Cao, and P. Sharma, “Sla-driven ml inference
    framework for clouds with heterogeneous accelerators,” *Proceedings of Machine
    Learning and Systems*, vol. 4, pp. 20–32, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] H. Shen, L. Chen, Y. Jin, L. Zhao, B. Kong, M. Philipose, A. Krishnamurthy,
    and R. Sundaram, “Nexus: A gpu cluster engine for accelerating dnn-based video
    analysis,” in *Proceedings of the 27th ACM Symposium on Operating Systems Principles*,
    2019, pp. 322–337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] F. Romero, Q. Li, N. J. Yadwadkar, and C. Kozyrakis, “Infaas: Automated
    model-less inference serving,” in *2021 USENIX Annual Technical Conference (USENIX
    ATC 21)*, 2021, pp. 397–411.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] J. R. Gunasekaran, C. S. Mishra, P. Thinakaran, B. Sharma, M. T. Kandemir,
    and C. R. Das, “Cocktail: A multidimensional optimization for model serving in
    cloud,” in *19th USENIX Symposium on Networked Systems Design and Implementation
    (NSDI 22)*, 2022, pp. 1041–1057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] S. Choi, S. Lee, Y. Kim, J. Park, Y. Kwon, and J. Huh, “Serving heterogeneous
    machine learning models on multi-gpu servers with spatio-temporal sharing,” in
    *2022 USENIX Annual Technical Conference (USENIX ATC 22)*, 2022, pp. 199–216.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] J. Gu, Y. Zhu, P. Wang, M. Chadha, and M. Gerndt, “Fast-gshare: Enabling
    efficient spatio-temporal gpu sharing in serverless computing for deep learning
    inference,” in *Proceedings of the 52nd International Conference on Parallel Processing*,
    2023, pp. 635–644.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] D. Narayanan, K. Santhanam, F. Kazhamiaka, A. Phanishayee, and M. Zaharia,
    “Heterogeneity-aware cluster scheduling policies for deep learning workloads,”
    in *14th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    20)*, 2020, pp. 481–498.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Q. Hu, P. Sun, S. Yan, Y. Wen, and T. Zhang, “Characterization and prediction
    of deep learning workloads in large-scale gpu datacenters,” in *Proceedings of
    the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2021, pp. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Q. Weng, W. Xiao, Y. Yu, W. Wang, C. Wang, J. He, Y. Li, L. Zhang, W. Lin,
    and Y. Ding, “Mlaas in the wild: Workload analysis and scheduling in large-scale
    heterogeneous gpu clusters,” in *19th USENIX Symposium on Networked Systems Design
    and Implementation (NSDI 22)*, 2022, pp. 945–960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] J. Li, H. Xu, Y. Zhu, Z. Liu, C. Guo, and C. Wang, “Lyra: Elastic scheduling
    for deep learning clusters,” in *Proceedings of the Eighteenth European Conference
    on Computer Systems*, 2023, pp. 835–850.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] R. Cheng, C. Cai, S. Yilmaz, R. Mitra, M. Bag, M. Ghosh, and T. Xu, “Towards
    gpu memory efficiency for distributed training at scale,” in *Proceedings of the
    2023 ACM Symposium on Cloud Computing*, 2023, pp. 281–297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Z. Ye, W. Gao, Q. Hu, P. Sun, X. Wang, Y. Luo, T. Zhang, and Y. Wen,
    “Deep learning workload scheduling in gpu datacenters: A survey,” *ACM Computing
    Surveys*, vol. 56, no. 6, pp. 1–38, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] “Nvidia multi-process service,” 2024\. [Online]. Available: [https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf](https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] “Nvidia multi-instance gpu.” 2024\. [Online]. Available: [https://www.nvidia.com/en-us/technologies/multi-instance-gpu/](https://www.nvidia.com/en-us/technologies/multi-instance-gpu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] “Unified memory for cuda,” 2024\. [Online]. Available: [https://developer.nvidia.com/blog/unified-memory-cuda-beginners/](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] M. Amaral, J. Polo, D. Carrera, S. Seelam, and M. Steinder, “Topology-aware
    gpu scheduling for learning workloads in cloud environments,” in *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2017, pp. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] J. Gu, M. Chowdhury, K. Shin, Y. Zhu, M. Jeon, J. Qian, H. Liu, and C. Guo,
    “Tiresias: A GPU cluster manager for distributed deep learning,” *Networked Systems
    Design and Implementation,Networked Systems Design and Implementation*, pp. 485–500,
    Jan 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] K. R. Jayaram, V. Muthusamy, P. Dube, V. Ishakian, C. Wang, B. Herta,
    S. Boag, D. Arroyo, A. Tantawi, A. Verma, F. Pollok, and R. Khalaf, “Ffdl: A flexible
    multi-tenant deep learning platform.” in *Proceedings of the 20th International
    Middleware Conference*, Dec 2019, pp. 82–95.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] M. Jeon, S. Venkataraman, A. Phanishayee, J. Qian, W. Xiao, and F. Yang,
    “Analysis of large-scalemulti-tenantgpu clusters for dnn training workloads,”
    in *2019 USENIX Annual Technical Conference (USENIX ATC 19)*, 2019, pp. 947–960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] A. Sultana, L. Chen, F. Xu, and X. Yuan, “E-las: Design and analysis
    of completion-time agnostic scheduling for distributed deep learning cluster,”
    in *49th International Conference on Parallel Processing - ICPP*, Aug 2020, pp.
    1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] M. Yu, C. Wu, B. Ji, and J. Liu, “A sum-of-ratios multi-dimensional-knapsack
    decomposition for dnn resource scheduling,” in *IEEE INFOCOM 2021 - IEEE Conference
    on Computer Communications*, May 2021, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] C. Wang, N. Yoshikane, F. Balasis, and T. Tsuritani, “Osdl: Dedicated
    optical slice provisioning in support of distributed deep learning,” *Computer
    Networks*, vol. 214, p. 109191, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Y. Luan, X. Chen, H. Zhao, Z. Yang, and Y. Dai, “Sched²: Scheduling deep
    learning training via deep reinforcement learning,” in *2019 IEEE Global Communications
    Conference (GLOBECOM)*.   IEEE, 2019, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] H. Wang, Z. Liu, and H. Shen, “Job scheduling for large-scale machine
    learning clusters,” in *Proceedings of the 16th International Conference on emerging
    Networking EXperiments and Technologies*, Nov 2020, pp. 108–120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,
    G. R. Ganger, P. B. Gibbons, and M. Zaharia, “Pipedream: Generalized pipeline
    parallelism for dnn training,” in *Proceedings of the 27th ACM Symposium on Operating
    Systems Principles*, 2019, pp. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam,
    Q. V. Le, Y. Wu *et al.*, “Gpipe: Efficient training of giant neural networks
    using pipeline parallelism,” *Advances in neural information processing systems*,
    vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, and C. Liu, “Dear: Accelerating
    distributed deep learning with fine-grained all-reduce pipelining,” in *2023 IEEE
    43rd International Conference on Distributed Computing Systems (ICDCS)*.   IEEE,
    2023, pp. 142–153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] J. H. Park, G. Yun, M. Y. Chang, N. T. Nguyen, S. Lee, J. Choi, S. H.
    Noh, and Y.-r. Choi, “Hetpipe: Enabling large dnn training on (whimpy) heterogeneous
    gpu clusters through integration of pipelined model parallelism and data parallelism,”
    in *2020 USENIX Annual Technical Conference (USENIX ATC 20)*, 2020, pp. 307–321.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] P. Zhou, X. He, S. Luo, H. Yu, and G. Sun, “Jpas: Job-progress-aware
    flow scheduling for deep learning clusters,” *Journal of Network and Computer
    Applications*, p. 102590, May 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] S. Wang, D. Li, and J. Geng, “Geryon: Accelerating distributed cnn training
    by network-level flow scheduling,” in *IEEE INFOCOM 2020-IEEE Conference on Computer
    Communications*.   IEEE, 2020, pp. 1678–1687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] M. Kang, G. Yang, Y. Yoo, and C. Yoo, “Tensorexpress: In-network communication
    scheduling for distributed deep learning,” in *2020 IEEE 13th international conference
    on cloud computing (CLOUD)*.   IEEE, 2020, pp. 25–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Y. He, W. Cai, P. Zhou, G. Sun, S. Luo, H. Yu, and M. Guizani, “Beamer:
    Stage-aware coflow scheduling to accelerate hyper-parameter tuning in deep learning
    clusters,” *IEEE Transactions on Network and Service Management*, vol. 19, no. 2,
    pp. 1083–1097, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] C. Chen, S. Wang, Y. Chen, and J. Han, “Tereis: A package-based scheduling
    in deep learning systems,” in *2022 IEEE 28th International Conference on Parallel
    and Distributed Systems (ICPADS)*.   IEEE, 2023, pp. 867–874.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Q. Duan, C. Peng, Z. Wang, Y. Xu, S. Liu, J. Wu, and J. C. Lui, “Accelerating
    distributed dnn training via transport layer scheduling,” *IEEE Transactions on
    Parallel and Distributed Systems*, vol. 34, no. 5, pp. 1650–1666, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] H. Zheng, F. Xu, L. Chen, Z. Zhou, and F. Liu, “Cynthia: Cost-efficient
    cloud resource provisioning for predictable distributed deep neural network training,”
    in *Proceedings of the 48th International Conference on Parallel Processing*,
    2019, pp. 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] N. B. D. Ta, “$fc^{2}$: cloud-based cluster provisioning for distributed
    machine learning,” *Cluster Computing*, p. 1299–1315, Dec 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] A. Jahani, M. Lattuada, M. Ciavotta, D. Ardagna, E. Amaldi, and L. Zhang,
    “Optimizing on-demand gpus in the cloud for deep learning applications training,”
    in *2019 4th International Conference on Computing, Communications and Security
    (ICCCS)*, Oct 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] F. Wang, W. Zhang, S. Lai, M. Hao, and Z. Wang, “Dynamic gpu energy optimization
    for machine learning training workloads,” *IEEE Transactions on Parallel and Distributed
    Systems*, p. 1–1, Jan 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] F. Filippini, J. Anselmi, D. Ardagna, and B. Gaujal, “A stochastic approach
    for scheduling ai training jobs in gpu-based systems,” *IEEE Transactions on Cloud
    Computing*, pp. 1–17, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Z. Chen, W. Quan, M. Wen, J. Fang, J. Yu, C. Zhang, and L. Luo, “Deep
    learning research and development platform: Characterizing and scheduling with
    qos guarantees on gpu clusters,” *IEEE Transactions on Parallel and Distributed
    Systems*, vol. 31, no. 1, pp. 34–50, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] W. Gao, Z. Ye, P. Sun, Y. Wen, and T. Zhang, “Chronus: A novel deadline-aware
    scheduler for deep learning training jobs,” in *Proceedings of the ACM Symposium
    on Cloud Computing*, 2021, pp. 609–623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Z. Yang, H. Wu, Y. Xu, Y. Wu, H. Zhong, and W. Zhang, “Hydra: Deadline-aware
    and efficiency-oriented scheduling for deep learning jobs on heterogeneous gpus,”
    *IEEE Transactions on Computers*, vol. 72, no. 8, pp. 2224–2236, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] W. Liu, J. Geng, Z. Zhu, J. Cao, and Z. Lian, “Sniper: Cloud-edge collaborative
    inference scheduling with neural network similarity modeling,” in *Proceedings
    of the 59th ACM/IEEE Design Automation Conference*, ser. DAC ’22, New York, NY,
    USA, 2022, p. 505–510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Y. Li, Z. Han, Q. Zhang, Z. Li, and H. Tan, “Automating cloud deployment
    for deep learning inference of real-time online services,” in *IEEE INFOCOM 2020-IEEE
    Conference on Computer Communications*.   IEEE, 2020, pp. 1668–1677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] D. Crankshaw, X. Wang, G. Zhou, M. J. Franklin, J. E. Gonzalez, and I. Stoica,
    “Clipper: A low-latency online prediction serving system,” in *14th USENIX Symposium
    on Networked Systems Design and Implementation (NSDI 17)*, 2017, pp. 613–627.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] W. Wang, J. Gao, M. Zhang, S. Wang, G. Chen, T. K. Ng, B. C. Ooi, J. Shao,
    and M. Reyad, “Rafiki,” *Proceedings of the VLDB Endowment*, p. 128–140, Oct 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] X. Tang, P. Wang, Q. Liu, W. Wang, and J. Han, “Nanily: A qos-aware scheduling
    for dnn inference workload in clouds,” in *2019 IEEE 21st International Conference
    on High Performance Computing and Communications; IEEE 17th International Conference
    on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS)*,
    Aug 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] H. Qin, S. Zawad, Y. Zhou, L. Yang, D. Zhao, and F. Yan, “Swift machine
    learning model serving scheduling: a region based reinforcement learning approach,”
    in *Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis*, 2019, pp. 1–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] L. Wang, L. Yang, Y. Yu, W. Wang, B. Li, X. Sun, J. He, and L. Zhang,
    “Morphling: fast, near-optimal auto-configuration for cloud-native model serving,”
    in *Proceedings of the ACM Symposium on Cloud Computing*, 2021, pp. 639–653.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] M. Chowdhury and I. Stoica, “Efficient coflow scheduling without prior
    knowledge,” *ACM SIGCOMM Computer Communication Review*, vol. 45, no. 4, pp. 393–406,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] S. Tang, Y. Yu, H. Wang, G. Wang, W. Chen, Z. Xu, S. Guo, and W. Gao,
    “A survey on scheduling techniques in computing and network convergence,” *IEEE
    Communications Surveys & Tutorials*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] A. Nukada, “Performance optimization of allreduce operation for multi-gpu
    systems,” in *2021 IEEE International Conference on Big Data (Big Data)*.   IEEE,
    2021, pp. 3107–3112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] (2024) Nvlink and nvswitch. [Online]. Available: [https://www.nvidia.com/en-us/data-center/nvlink/](https://www.nvidia.com/en-us/data-center/nvlink/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] (2024) Gpudirect rdma. [Online]. Available: [https://developer.nvidia.com/gpudirect](https://developer.nvidia.com/gpudirect)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy,
    M. Moshref, D. Ports, and P. Richtárik, “Scaling distributed machine learning
    with in-network aggregation,” in *18th USENIX Symposium on Networked Systems Design
    and Implementation (NSDI 21)*, 2021, pp. 785–808.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] C. Lao, Y. Le, K. Mahajan, Y. Chen, W. Wu, A. Akella, and M. Swift, “Atp:
    In-network aggregation for multi-tenant learning,” in *18th USENIX Symposium on
    Networked Systems Design and Implementation (NSDI 21)*, 2021, pp. 741–761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] B. Zhao, C. Liu, J. Dong, Z. Cao, W. Nie, and W. Wu, “Enabling switch
    memory management for distributed training with in-network aggregation,” in *IEEE
    INFOCOM 2023-IEEE Conference on Computer Communications*.   IEEE, 2023, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] J. Fang, G. Zhao, H. Xu, C. Wu, and Z. Yu, “Grid: Gradient routing with
    in-network aggregation for distributed training,” *IEEE/ACM Transactions on Networking*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] J. Fang, G. Zhao, H. Xu, Z. Yu, B. Shen, and L. Xie, “Goat: Gradient
    scheduling with collaborative in-network aggregation for distributed training,”
    in *2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)*.   IEEE,
    2023, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] N. Gebara, M. Ghobadi, and P. Costa, “In-network aggregation for shared
    machine learning clusters,” *Proceedings of Machine Learning and Systems*, vol. 3,
    pp. 829–844, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Z. Li, J. Huang, Y. Li, A. Xu, S. Zhou, J. Liu, and J. Wang, “A2tp: Aggregator-aware
    in-network aggregation for multi-tenant learning,” in *Proceedings of the Eighteenth
    European Conference on Computer Systems*, 2023, pp. 639–653.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] S. Liu, Q. Wang, J. Zhang, W. Wu, Q. Lin, Y. Liu, M. Xu, M. Canini, R. C.
    Cheung, and J. He, “In-network aggregation with transport transparency for distributed
    training,” in *Proceedings of the 28th ACM International Conference on Architectural
    Support for Programming Languages and Operating Systems, Volume 3*, 2023, pp.
    376–391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] A. Guo, Y. Hao, C. Wu, P. Haghi, Z. Pan, M. Si, D. Tao, A. Li, M. Herbordt,
    and T. Geng, “Software-hardware co-design of heterogeneous smartnic system for
    recommendation models inference and training,” in *Proceedings of the 37th International
    Conference on Supercomputing*, 2023, pp. 336–347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] A. Guo, T. Geng, Y. Zhang, P. Haghi, C. Wu, C. Tan, Y. Lin, A. Li, and
    M. Herbordt, “A framework for neural network inference on fpga-centric smartnics,”
    in *2022 32nd International Conference on Field-Programmable Logic and Applications
    (FPL)*.   IEEE, 2022, pp. 01–08.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] (2024) Open mpi. [Online]. Available: [https://www.open-mpi.org/](https://www.open-mpi.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] (2024) Facebook gloo. [Online]. Available: [https://github.com/facebookincubator/gloo](https://github.com/facebookincubator/gloo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] (2024) Horovod. [Online]. Available: [https://horovod.readthedocs.io/en/stable/](https://horovod.readthedocs.io/en/stable/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] (2024) Nccl. [Online]. Available: [https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] M. Cowan, S. Maleki, M. Musuvathi, O. Saarikivi, and Y. Xiong, “Mscclang:
    Microsoft collective communication language,” in *Proceedings of the 28th ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems, Volume 2*, 2023, pp. 502–514.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] M. Cho, U. Finkler, D. Kung, and H. Hunter, “Blueconnect: Decomposing
    all-reduce for deep learning on heterogeneous network hierarchy,” *Proceedings
    of Machine Learning and Systems*, vol. 1, pp. 241–251, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] G. Wang, S. Venkataraman, A. Phanishayee, N. Devanur, J. Thelin, and
    I. Stoica, “Blink: Fast and generic collectives for distributed ml,” in *Proceedings
    of Machine Learning and Systems*, vol. 2, 2020, pp. 172–186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] L. Luo, P. West, J. Nelson, A. Krishnamurthy, and L. Ceze, “Plink: Discovering
    and exploiting locality for accelerated distributed training on the public cloud,”
    *Proceedings of Machine Learning and Systems*, vol. 2, pp. 82–97, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] C. Renggli, S. Ashkboos, M. Aghagolzadeh, D. Alistarh, and T. Hoefler,
    “Sparcml: High-performance sparse communication for machine learning,” in *Proceedings
    of the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2019, pp. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, and A. Sapio, “Efficient sparse
    collective communication and its application to accelerate distributed deep learning,”
    in *Proceedings of the 2021 ACM SIGCOMM 2021 Conference*, 2021, pp. 676–691.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] H. Xu, K. Kostopoulou, A. Dutta, X. Li, A. Ntoulas, and P. Kalnis, “Deepreduce:
    A sparse-tensor communication framework for federated deep learning,” in *Advances
    in Neural Information Processing Systems*, vol. 34, 2021, pp. 21 150–21 163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Z. Cai, Z. Liu, S. Maleki, M. Musuvathi, T. Mytkowicz, J. Nelson, and
    O. Saarikivi, “Synthesizing optimal collective algorithms,” in *Proceedings of
    the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming*,
    2021, pp. 62–75.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] A. Shah, V. Chidambaram, M. Cowan, S. Maleki, M. Musuvathi, T. Mytkowicz,
    J. Nelson, O. Saarikivi, and R. Singh, “TACCL: Guiding collective algorithm synthesis
    using communication sketches,” in *20th USENIX Symposium on Networked Systems
    Design and Implementation (NSDI 23)*.   Boston, MA: USENIX Association, Apr. 2023,
    pp. 593–612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] S. Wang, D. Li, Y. Cheng, J. Geng, Y. Wang, S. Wang, S. Xia, and J. Wu,
    “A scalable, high-performance, and fault-tolerant network architecture for distributed
    machine learning,” *IEEE/ACM Transactions on Networking*, vol. 28, no. 4, pp.
    1752–1764, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] T. Hoefler, T. Bonato, D. De Sensi, S. Di Girolamo, S. Li, M. Heddes,
    J. Belk, D. Goel, M. Castro, and S. Scott, “Hammingmesh: A network topology for
    large-scale deep learning,” in *SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis*.   IEEE, 2022, pp. 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] M. Khani, M. Ghobadi, M. Alizadeh, Z. Zhu, M. Glick, K. Bergman, A. Vahdat,
    B. Klenk, and E. Ebrahimi, “Sip-ml: high-bandwidth optical network interconnects
    for machine learning training,” in *Proceedings of the 2021 ACM SIGCOMM 2021 Conference*,
    2021, pp. 657–675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] W. Wang, M. Khazraee, Z. Zhong, M. Ghobadi, Z. Jia, D. Mudigere, Y. Zhang,
    and A. Kewitsch, “Topoopt: Co-optimizing network topology and parallelization
    strategy for distributed training jobs,” in *20th USENIX Symposium on Networked
    Systems Design and Implementation (NSDI 23)*, 2023, pp. 739–767.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] N. R. Adiga, M. A. Blumrich, D. Chen, P. Coteus, A. Gara, M. E. Giampapa,
    P. Heidelberger, S. Singh, B. D. Steinmacher-Burow, T. Takken *et al.*, “Blue
    gene/l torus interconnection network,” *IBM Journal of Research and Development*,
    vol. 49, no. 2.3, pp. 265–276, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] A. Li, S. L. Song, J. Chen, J. Li, X. Liu, N. R. Tallent, and K. J. Barker,
    “Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect,”
    *IEEE Transactions on Parallel and Distributed Systems*, vol. 31, no. 1, pp. 94–110,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] S. M. Nabavinejad, M. Baharloo, K.-C. Chen, M. Palesi, T. Kogel, and
    M. Ebrahimi, “An overview of efficient interconnection networks for deep neural
    network accelerators,” *IEEE Journal on Emerging and Selected Topics in Circuits
    and Systems*, vol. 10, no. 3, pp. 268–282, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] A. Feng, D. Dong, F. Lei, J. Ma, E. Yu, and R. Wang, “In-network aggregation
    for data center networks: A survey,” *Computer Communications*, vol. 198, pp.
    63–76, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar,
    S. Sengupta, and M. Sridharan, “Data center tcp (dctcp),” in *Proceedings of the
    ACM SIGCOMM 2010 Conference*, 2010, pp. 63–74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] M. Alizadeh, S. Yang, M. Sharif, S. Katti, N. McKeown, B. Prabhakar,
    and S. Shenker, “pfabric: Minimal near-optimal datacenter transport,” *ACM SIGCOMM
    Computer Communication Review*, vol. 43, no. 4, pp. 435–446, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] N. Schelten, F. Steinert, A. Schulte, and B. Stabernack, “A high-throughput,
    resource-efficient implementation of the rocev2 remote dma protocol for network-attached
    hardware accelerators,” in *2020 International Conference on Field-Programmable
    Technology (ICFPT)*.   IEEE, 2020, pp. 241–249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] (2024) Nvidia connectx smartnic. [Online]. Available: [https://www.nvidia.com/en-us/networking/ethernet-adapters/](https://www.nvidia.com/en-us/networking/ethernet-adapters/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] (2024) Intel ipu. [Online]. Available: [https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html](https://www.intel.com/content/www/us/en/products/details/network-io/ipu.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] L. Liu, P. Zhou, G. Sun, X. Chen, T. Wu, H. Yu, and M. Guizani, “Topologies
    in distributed machine learning: Comprehensive survey, recommendations and future
    directions,” *Neurocomputing*, p. 127009, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] M. Al-Fares, A. Loukissas, and A. Vahdat, “A scalable, commodity data
    center network architecture,” *ACM SIGCOMM computer communication review*, vol. 38,
    no. 4, pp. 63–74, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and
    S. Lu, “Bcube: a high performance, server-centric network architecture for modular
    data centers,” in *Proceedings of the ACM SIGCOMM 2009 conference on Data communication*,
    2009, pp. 63–74.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] A. Singla, C.-Y. Hong, L. Popa, and P. B. Godfrey, “Jellyfish: Networking
    data centers randomly,” in *9th USENIX Symposium on Networked Systems Design and
    Implementation (NSDI 12)*, 2012, pp. 225–238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu, “Dcell: a scalable
    and fault-tolerant network structure for data centers,” in *Proceedings of the
    ACM SIGCOMM 2008 conference on Data communication*, 2008, pp. 75–86.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] N. Farrington, G. Porter, S. Radhakrishnan, H. H. Bazzaz, V. Subramanya,
    Y. Fainman, G. Papen, and A. Vahdat, “Helios: a hybrid electrical/optical switch
    architecture for modular data centers,” in *Proceedings of the ACM SIGCOMM 2010
    Conference*, 2010, pp. 339–350.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] G. Wang, D. G. Andersen, M. Kaminsky, K. Papagiannaki, T. E. Ng, M. Kozuch,
    and M. Ryan, “c-through: Part-time optics in data centers,” in *Proceedings of
    the ACM SIGCOMM 2010 Conference*, 2010, pp. 327–338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] K. Chen, A. Singla, A. Singh, K. Ramachandran, L. Xu, Y. Zhang, X. Wen,
    and Y. Chen, “Osa: An optical switching architecture for data center networks
    with unprecedented flexibility,” *IEEE/ACM Transactions on networking*, vol. 22,
    no. 2, pp. 498–511, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *et al.*, “Llama: Open and efficient
    foundation language models,” *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,
    N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation
    and fine-tuned chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko,
    J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du,
    B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin,
    T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra,
    K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov,
    R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat,
    A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta,
    M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov,
    and N. Fiedel, “Palm: Scaling language modeling with pathways,” *Journal of Machine
    Learning Research*, vol. 24, no. 240, pp. 1–113, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese,
    and C. Xiong, “Codegen: An open large language model for code with multi-turn
    program synthesis,” in *The Eleventh International Conference on Learning Representations*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann, P. Kambadur,
    D. Rosenberg, and G. Mann, “Bloomberggpt: A large language model for finance,”
    *arXiv preprint arXiv:2303.17564*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti,
    D. Vainbrand, P. Kashinkunti, J. Bernauer, B. Catanzaro *et al.*, “Efficient large-scale
    language model training on gpu clusters using megatron-lm,” in *Proceedings of
    the International Conference for High Performance Computing, Networking, Storage
    and Analysis*, 2021, pp. 1–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] B. Yuan, Y. He, J. Davis, T. Zhang, T. Dao, B. Chen, P. S. Liang, C. Re,
    and C. Zhang, “Decentralized training of foundation models in heterogeneous environments,”
    *Advances in Neural Information Processing Systems*, vol. 35, pp. 25 464–25 477,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] S. Athlur, N. Saran, M. Sivathanu, R. Ramjee, and N. Kwatra, “Varuna:
    Scalable, low-cost training of massive deep learning models,” in *Proceedings
    of the Seventeenth European Conference on Computer Systems*, New York, NY, USA,
    2022, p. 472–487.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper,
    Z. Liu, S. Prabhumoye, G. Zerveas, V. Korthikanti *et al.*, “Using deepspeed and
    megatron to train megatron-turing nlg 530b, a large-scale generative language
    model,” *arXiv preprint arXiv:2201.11990*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] J. Wang, Y. Lu, B. Yuan, B. Chen, P. Liang, C. De Sa, C. Re, and C. Zhang,
    “CocktailSGD: Fine-tuning foundation models over 500Mbps networks,” in *Proceedings
    of the 40th International Conference on Machine Learning*, vol. 202.   PMLR, 23–29
    Jul 2023, pp. 36 058–36 076.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] M. Ryabinin, T. Dettmers, M. Diskin, and A. Borzunov, “Swarm parallelism:
    Training large models can be surprisingly communication-efficient,” in *Proceedings
    of the 40th International Conference on Machine Learning*.   JMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] I. Jang, Z. Yang, Z. Zhang, X. Jin, and M. Chowdhury, “Oobleck: Resilient
    distributed training of large models using pipeline templates,” in *Proceedings
    of the 29th Symposium on Operating Systems Principles*, New York, NY, USA, 2023,
    p. 382–395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] Y. Shen, J. Shao, X. Zhang, Z. Lin, H. Pan, D. Li, J. Zhang, and K. B.
    Letaief, “Large language models empowered autonomous edge ai for connected intelligence,”
    *IEEE Communications Magazine*, pp. 1–7, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/0bc11e175fe17e7b57b9294560a83db2.png) | Feng Liang
    (Member, IEEE) received his Ph.D. degree in computer science from the University
    of Hong Kong. He was a senior research engineer at Huawei and CTO of Jiufeng Tech
    Co., Ltd and is currently an associate professor with Shenzhen MSU-BIT University.
    His research interests are in broad areas of distributed computing, including
    distributed databases and computing systems, distributed machine learning, and
    interdisciplinary studies in bioinformatics. He has published papers in prestigious
    venues including IEEE TPDS, IEEE TNSM, Information Sciences, HPDC, IPDPS, and
    ACSAC. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/90794743754d90640cb7754bf0504ae5.png) | Zhen Zhang
    is currently working toward the Ph.D. degree with the Gansu Provincial Key Laboratory
    of Wearable Computing, School of Information Science and Engineering, Lanzhou
    University, Lanzhou, China. He is also a guest student with the Shenzhen MSU-BIT
    University, Shenzhen, China. His research interests includes affective computing,
    person re-identification, and deep learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/dde7d6f205a3f731a44a9ead4f6a4323.png) | Haifeng
    Lu received the the M.Sc. degree in School of Information Science and Engineering
    in 2021 from Lanzhou University, China. He is currently pursuing the Ph.D. degree
    with with the Gansu Provincial Key Laboratory of Wearable Computing, School of
    information Science and Engineering, Lanzhou University, Lanzhou, China. He is
    also a guest student with the Shenzhen MSU-BIT University, Shenzhen, China. His
    research interests include affective computing and machine learning. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/dacad7a7089a56831b7f72c998c3760e.png) | Victor
    C. M. Leung (Life Fellow, IEEE) is currently a Distinguished Professor of computer
    science and software engineering with Shenzhen University, China. He is also an
    Emeritus Professor of electrical and computer engineering and the Director of
    the Laboratory for Wireless Networks and Mobile Systems, The University of British
    Columbia (UBC), Canada. His research interests include wireless networks and mobile
    systems. He has published widely in these areas. He is a Fellow of the Royal Society
    of Canada, the Canadian Academy of Engineering, and the Engineering Institute
    of Canada. He received the 1977 APEBC Gold Medal, the 1977–1981 NSERC Postgraduate
    Scholarships, the IEEE Vancouver Section Centennial Award, the 2011 UBC Killam
    Research Prize, the 2017 Canadian Award for Telecommunications Research, the 2018
    IEEE TCGCC Distinguished Technical Achievement Recognition Award, and the 2018
    ACM MSWiM Reginald Fessenden Award. He has coauthored papers that won the 2017
    IEEE ComSoc Fred W. Ellersick Prize, the 2017 IEEE Systems Journal Best Paper
    Award, the 2018 IEEE CSIM Best Journal Paper Award, and the 2019 IEEE TCGCC Best
    Journal Paper Award. He has been serving on the editorial boards of the IEEE Transactions
    on Green Communications and Networking, IEEE Transactions on Cloud Computing,
    IEEE Access, IEEE Network, and several other journals. He is named in the current
    Clarivate Analytics list of “Highly Cited Researchers.” |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4202586ff654c13edfe925fecf17d421.png) | Yanyi
    Guo received his Ph.D. degree in safety system engineering from Beiing Institute
    of Technology. He was an associate professor with BIT and is currently a senior
    researcher at Shenzhen MSU-BIT University. His research interests are mainly in
    the areas of safety modelling and evaluation. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/36af73c0c2bfe8679f4954cc106e2468.png) | Xiping
    Hu (Member, IEEE) received the Ph.D. degree from the University of British Columbia,
    Vancouver, BC, Canada. He is currently a professor with Beijing Institute of Technology,
    and with Shenzhen MSU-BIT University, China. He has more than 150 papers published
    and presented in prestigious conferences and journals, such as IEEE TPAMI/TMC/TPDS/TIP/JSAC,
    IEEE COMST, ACM MobiCom/MM/SIGIR/WWW, AAAI, and IJCAI. He has been serving as
    associate editor of IEEE TCSS, and the lead guest editors of IEEE IoT Journal
    and IEEE TASE etc. He has been granted several key national research projects
    as principal investigator. He was the Co-Founder and CTO of Bravolol Ltd., Hong
    Kong, a leading language learning mobile application company with over 100 million
    users, and listed as the top 2 language education platform globally. His research
    areas consist of mobile cyber-physical systems, crowd sensing and affective computing.
    |'
  prefs: []
  type: TYPE_TB
