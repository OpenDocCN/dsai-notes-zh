- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:08:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1711.09223] Malaria Likelihood Prediction By Effectively Surveying Households
    Using Deep Reinforcement Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1711.09223] 通过深度强化学习有效调查家庭来预测疟疾可能性'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1711.09223](https://ar5iv.labs.arxiv.org/html/1711.09223)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1711.09223](https://ar5iv.labs.arxiv.org/html/1711.09223)
- en: Malaria Likelihood Prediction By Effectively Surveying Households Using Deep
    Reinforcement Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过深度强化学习有效调查家庭来预测疟疾可能性
- en: Pranav Rajpurkar
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Pranav Rajpurkar
- en: Department of Computer Science
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Stanford University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学
- en: pranavsr@cs.stanford.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: pranavsr@cs.stanford.edu
- en: \AndVinaya Polamreddi
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \And Vinaya Polamreddi
- en: Department of Computer Science
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Stanford University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学
- en: vinaya.polamreddi@cs.stanford.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: vinaya.polamreddi@cs.stanford.edu
- en: \ANDAnusha Balakrishnan
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \AND Anusha Balakrishnan
- en: Department of Computer Science
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Stanford University
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福大学
- en: anusha@cs.stanford.edu
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: anusha@cs.stanford.edu
- en: Abstract
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We build a deep reinforcement learning (RL) agent that can predict the likelihood
    of an individual testing positive for malaria by asking questions about their
    household. The RL agent learns to determine which survey question to ask next
    and when to stop to make a prediction about their likelihood of malaria based
    on their responses hitherto. The agent incurs a small penalty for each question
    asked, and a large reward/penalty for making the correct/wrong prediction; it
    thus has to learn to balance the length of the survey with the accuracy of its
    final predictions. Our RL agent is a Deep Q-network that learns a policy directly
    from the responses to the questions, with an action defined for each possible
    survey question and for each possible prediction class. We focus on Kenya, where
    malaria is a massive health burden, and train the RL agent on a dataset of 6481
    households from the Kenya Malaria Indicator Survey 2015\. To investigate the importance
    of having survey questions be adaptive to responses, we compare our RL agent to
    a supervised learning (SL) baseline that fixes its set of survey questions a priori.
    We evaluate on prediction accuracy and on the number of survey questions asked
    on a holdout set and find that the RL agent is able to predict with 80% accuracy,
    using only 2.5 questions on average. In addition, the RL agent learns to survey
    adaptively to responses and is able to match the SL baseline in prediction accuracy
    while significantly reducing survey length.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个深度强化学习（RL）代理，能够通过询问有关家庭的问题来预测个体感染疟疾的可能性。RL代理学习确定接下来应该问哪个调查问题，以及何时停止以根据迄今为止的回答做出关于疟疾可能性的预测。代理每问一个问题会遭受小的惩罚，并且在做出正确/错误预测时会获得大的奖励/惩罚；因此，它必须学会平衡调查的长度与最终预测的准确性。我们的RL代理是一个深度Q网络，它直接从问题的回答中学习策略，为每个可能的调查问题和每个可能的预测类别定义一个动作。我们重点关注肯尼亚，在那里疟疾是一项巨大的健康负担，并在2015年肯尼亚疟疾指标调查的6481个家庭数据集上训练RL代理。为了调查调查问题适应响应的重要性，我们将我们的RL代理与一个先验固定调查问题集的监督学习（SL）基线进行比较。我们在一个保留集上评估预测准确性和问卷问题数量，发现RL代理能够以80%的准确率进行预测，平均只使用2.5个问题。此外，RL代理学会了根据响应进行自适应调查，并且能够在减少调查长度的同时与SL基线匹配预测准确性。
- en: '![Refer to caption](img/aa517faaba9beaf4bff16872a0c842fb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aa517faaba9beaf4bff16872a0c842fb.png)'
- en: 'Figure 1: The RL agent is able to effectively survey individuals to determine
    their likelihood of testing positive for malaria by asking them questions about
    their households. On this example, the agent correctly predicts that the individual
    being surveyed is malaria positive.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：RL代理能够通过询问有关家庭的问题，有效地调查个体，以确定他们感染疟疾的可能性。在这个例子中，代理正确预测被调查者感染了疟疾。
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: We build a short mobile survey questionnaire that informs individuals of their
    risk of malaria, aiming to improve malaria management at the national and individual
    level. Making surveys short and available on mobile allows for quicker, cheaper
    and more timely disease monitoring (Game, [2013](#bib.bib2)). In addition, making
    people aware of their risk of having malaria promotes health seeking behaviors
    (Torfin, [2017](#bib.bib9)). We focus on the Malaria Indicator Survey (MIS) in
    Kenya, which plays a central role in assessing and guiding the public policy for
    targeted malaria interventions (Torfin, [2017](#bib.bib9)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'We use reinforcement learning (RL) to train an agent that learns to survey
    individuals to accurately predict their likelihood of testing positive for malaria.
    The agent is allowed a total of $k$ sequential decisions: at each time it can
    decide whether to ask the individual another question or make a prediction about
    the result of their malaria blood smear test based on the survey questions answered.
    The agent incurs a small negative reward for every question asked, a large negative
    reward for incorrect predictions, and a large positive reward for correct predictions.
    The agent must thus learn to balance the cost of asking questions with the benefit
    of making an accurate prediction.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'Our RL agent is a deep Q-network that learns a policy directly from the state
    of the survey: the responses to the questions posed so far serve as input to a
    convolutional neural network, which outputs a $Q$ value for every action. We define
    the action space such that there is an action associated with each possible question
    that can be asked and with each possible prediction class. To train the RL agent,
    we use the Kenya MIS 2015, containing detailed surveys with individuals in their
    households and biomarkers that include the results of their malaria blood smear
    tests.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: We investigate the benefits of a survey that is adaptive to responses by comparing
    the RL agent against a supervised learning (SL) baseline that fixes the length
    and the content of the survey beforehand. The SL baseline uses the response to
    a fixed set of $k$ questions as input to a convolutional neural network, which
    outputs the probability of testing negative and positive on the malaria test.
    As $k$ increases the SL model has higher accuracy though it accumulates a higher
    cost for asking more questions. On a holdout set, we evaluate agents on their
    prediction accuracies and average survey length, and find that the RL agent learns
    to predict the result of malaria test with $80\%$ accuracy asking only $2.5$ questions
    on average, matching the accuracy of the SL baseline, but outperforming it on
    survey length.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: 2 Data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use the Kenya Malaria Indicator Survey 2015, a nationally representative
    survey done to assess the progress of malaria interventions and monitor prevalence
    across the target population (NMCP & Kenya National Bureau of Statistics, [KNBS](#bib.bib6)).
    The survey contains data about 6,481 households, including information the use
    of malaria nets in the household, ownership of household assets such as TV and
    access to electricity, biomarker test results including a rapid diagnostic test
    and a microscopic blood smear test that detect the presence of malaria parasites.
    We focus on the household member recode survey, which has a record for every person
    in the household.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用2015年肯尼亚疟疾指标调查，这是一项全国性代表性调查，旨在评估疟疾干预措施的进展并监测目标人群中的流行情况（NMCP和肯尼亚国家统计局，[KNBS](#bib.bib6)）。调查包含有关6,481户家庭的数据，包括家庭中疟疾网的使用情况、家庭资产如电视的拥有情况以及电力供应情况、生物标志物测试结果，包括快速诊断测试和显微镜血涂片测试，以检测疟疾寄生虫的存在。我们关注的是家庭成员重新编码调查，该调查记录了家庭中每个人的信息。
- en: Processing
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理
- en: We process the data to keep only categorical variables which have no more than
    10 classes, and records for which there is no missing data. Because our focus
    is on data that can be collected remotely through mobile surveys, we filter out
    variables which can only be acquired in person such as biomarker test results.
    We fix our output variable to be the result of the microscopy blood smear test
    for malaria.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们处理数据以仅保留类别变量，这些变量的类别数不超过10个，并且记录中没有缺失数据。由于我们的重点是可以通过移动调查远程收集的数据，因此我们排除了只能面对面获取的变量，例如生物标志物测试结果。我们将输出变量固定为显微镜血涂片测试结果，用于检测疟疾。
- en: We then use Pearson’s chi-squared test of independence between each of the categorical
    variables and the output variable (result of an individual’s blood smear test)
    to retain the $8$ most correlated variables. The processed categorical variables
    are shown in the appendix.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用皮尔逊的卡方独立性检验来筛选每个类别变量与输出变量（个人血涂片测试结果）之间的相关性，保留$8$个最相关的变量。处理后的类别变量见附录。
- en: '![Refer to caption](img/0af10ad3a1853a7284ac9b1579252382.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0af10ad3a1853a7284ac9b1579252382.png)'
- en: 'Figure 2: The agent interacts with the environment, deciding whether to perform
    additional queries before making a prediction.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：代理与环境互动，决定是否在做出预测之前执行额外查询。
- en: 3 Setup
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 设置
- en: The agent is tasked with a $c=2$ class prediction problem, but does not have
    access to any of its input features (all initially $0$). Instead, it can query
    any of its input features $F_{n}=[f_{0},..,f_{m}],m=8$, one at a time from the
    environment at a cost $cost_{query}=-0.05$. The agent querying its input feature
    $f_{i}$ is analogous to asking and getting the response to survey question $q_{i}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 代理任务是一个$c=2$类预测问题，但无法访问任何输入特征（所有特征初始为$0$）。相反，它可以从环境中一次查询一个输入特征$F_{n}=[f_{0},..,f_{m}],m=8$，查询的成本为$cost_{query}=-0.05$。代理查询输入特征$f_{i}$类似于询问并获得调查问题$q_{i}$的回答。
- en: Within $T=k$ queries, the agent must make its prediction, and incurs a reward
    of $r_{correct}=+1$ if correct, and $r_{wrong}=-1$ if wrong. Making a prediction
    terminates the episode. If the agent makes a prediction before $2$ timesteps,
    or does not make a prediction before the episode ends at $t=k$, it receives the
    penalty of making a wrong prediction. Finally, we setup the problem such that
    future rewards are undiscounted ($\gamma=1$).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在$T=k$次查询内，代理必须做出预测，如果正确则获得$ r_{correct}=+1 $的奖励，如果错误则获得$ r_{wrong}=-1 $的奖励。做出预测将终止这一回合。如果代理在$2$个时间步之前做出预测，或在回合结束时$t=k$之前没有做出预测，则会受到错误预测的惩罚。最后，我们设置问题，使得未来的奖励不打折扣（$
    \gamma=1 $）。
- en: 4 Reinforcement Learning Agent
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 强化学习代理
- en: We train a deep Q-Network (DQN) Mnih et al. ([2013](#bib.bib5)) to learn policies
    directly from the results of the input feature queries made so far. The DQN outputs
    a $Q$ value for every action in the action space, defined such that there is an
    action associated with each possible query ($k$), and one with each possible prediction
    class ($c$), a total of $k+c$ actions. The DQN uses a neural network architecture
    with a $1\times 1$ convolutional layer Szegedy et al. ([2014](#bib.bib8)) followed
    by $3$ fully connected layers. Each layer has $k+c$ hidden units, with a ReLU
    activation after each hidden layer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练深度 Q 网络 (DQN) Mnih 等人 ([2013](#bib.bib5)) 以直接从到目前为止进行的输入特征查询结果中学习策略。DQN
    为动作空间中的每个动作输出一个 $Q$ 值，定义为每个可能的查询 ($k$) 和每个可能的预测类别 ($c$) 都有一个动作，总共 $k+c$ 个动作。DQN
    使用一个 $1\times 1$ 卷积层 Szegedy 等人 ([2014](#bib.bib8))，后接 $3$ 个全连接层。每个层有 $k+c$ 个隐藏单元，每个隐藏层之后都有一个
    ReLU 激活函数。
- en: We train the DQN following the work of Mnih et al. ([2013](#bib.bib5)), using
    experience replay and a target network to smooth out learning and avoid divergence
    of the parameters. We use the Adam optimizer Kingma & Ba ([2014](#bib.bib4)) with
    minibatches of size $32$, and a learning rate that is annealed linearly from $0.00025$
    to $0.00005$. The behavior policy during training is $\epsilon-$greedy with $\epsilon$
    linearly annealed from $1$ to $0.01$ over the first $50k$ training samples, and
    fixed at $0.01$ thereafter. We train for a total of $100k$ samples, using a replay
    memory of the most recent $5k$ samples.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照 Mnih 等人 ([2013](#bib.bib5)) 的工作训练 DQN，使用经验回放和目标网络来平滑学习并避免参数的发散。我们使用 Adam
    优化器 Kingma & Ba ([2014](#bib.bib4))，小批量大小为 $32$，学习率线性衰减从 $0.00025$ 到 $0.00005$。训练期间的行为策略是
    $\epsilon$-贪婪的，其中 $\epsilon$ 在线性衰减从 $1$ 到 $0.01$，持续 $50k$ 训练样本，然后固定为 $0.01$。我们总共训练了
    $100k$ 个样本，使用最新的 $5k$ 个样本作为回放记忆。
- en: 5 Supervised Learning Baseline
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 监督学习基线
- en: 'The SL baseline fixes the set of queries that it uses beforehand, and can be
    interpreted as a non-adaptive policy in which the survey question responses do
    not affect either the choice or the number of questions asked. It uses a neural
    network with the same architecture as the RL network, except that the final layer
    only has $c$ outputs: the network learns to map from the query results to the
    correct prediction class.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SL 基线方法预先固定了它所使用的查询集，并且可以被解释为一种非自适应策略，其中调查问题的回答不会影响问题的选择或数量。它使用与 RL 网络相同架构的神经网络，只是最终层只有
    $c$ 个输出：网络学习将查询结果映射到正确的预测类别。
- en: We train the network to minimize the cross-entropy loss between the predictions
    and the ground truth. Training examples are sampled from the training split of
    $~{}8k$ examples with equal probability assigned to each class. We use the Adam
    optimizer (Kingma & Ba, [2014](#bib.bib4)) with minibatches of size $32$, with
    a learning rate that is annealed linearly from $0.0025$ to $0.0005$ over $20$
    epochs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练网络以最小化预测与真实标签之间的交叉熵损失。训练样本从 $~{}8k$ 个样本的训练集抽取，每个类别分配相等的概率。我们使用 Adam 优化器
    (Kingma & Ba, [2014](#bib.bib4))，小批量大小为 $32$，学习率在线性衰减从 $0.0025$ 到 $0.0005$，共 $20$
    个周期。
- en: '| Model | Accuracy | Avg. Queries | Avg. Episode Reward |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 准确率 | 平均查询次数 | 平均回合奖励 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SL network ($k=2$) | 0.73 | 2 | +0.36 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| SL 网络 ($k=2$) | 0.73 | 2 | +0.36 |'
- en: '| SL network ($k=4$) | 0.77 | 4 | +0.35 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| SL 网络 ($k=4$) | 0.77 | 4 | +0.35 |'
- en: '| SL network ($k=8$) | 0.80 | 8 | +0.20 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SL 网络 ($k=8$) | 0.80 | 8 | +0.20 |'
- en: '| RL network ($kmax=2$) | 0.78 | 2 | +0.45 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| RL 网络 ($kmax=2$) | 0.78 | 2 | +0.45 |'
- en: '| RL network ($kmax=4$) | 0.78 | 2.30 | +0.45 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| RL 网络 ($kmax=4$) | 0.78 | 2.30 | +0.45 |'
- en: '| RL network ($kmax=8$) | 0.80 | 2.35 | +0.47 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| RL 网络 ($kmax=8$) | 0.80 | 2.35 | +0.47 |'
- en: 'Table 1: The RL networks outperform the SL networks, not only being able to
    make more accurate predictions, but also with fewer number of queries on average.
    Unlike the SL networks, which fix the set of k queries they make a priori, the
    RL networks are able to effectively balance the cost of making additional queries
    with the benefits of increased prediction accuracy.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：RL 网络表现优于 SL 网络，不仅能做出更准确的预测，而且平均查询数量更少。与预先固定的 SL 网络不同，RL 网络能够有效地平衡额外查询的成本与增加的预测准确性带来的好处。
- en: 6 Experiments and Results
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验与结果
- en: We evaluate our models on prediction accuracy and the number of queries made.
    We also report the mean episode reward as defined by our setup earlier. Models
    are evaluated on a test set of $2k$ examples, resampled such that samples are
    drawn with equal probability from each class.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: We investigate the effects of limiting the number of queries the agents can
    make. In the RL approach, we set the maximum number of queries ($kmax={2,4,8}$),
    while in the SL approach we fix the number of queries ($k={2,4,8}$). Queries are
    ordered by decreasing correlation with the output variable, so using $k$ queries
    refers to using the top $k$ correlated variables.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: As $k$ increases, the SL baselines reach higher accuracies, but at the cost
    of an increase in the number of queries. The tradeoff at $k=2$ and $k=4$ produces
    comparable average episode scores, but the cost of making even more queries at
    $k=8$ outweighs the benefits of increased accuracy.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: As $kmax$ increases, the RL agents also reach higher accuracies; they are, however,
    able to keep the average number of queries small. The agents are therefore able
    to reach higher episode rewards.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of training RL agents to query before making predictions has recently
    been explored in the reinforcement learning literature. Shen et al. ([2016](#bib.bib7))
    train an agent to dynamically decide whether to continue or to terminate the inference
    process in machine comprehension tasks. Buck et al. ([2017](#bib.bib1)) train
    an agent to systematically perturb a question to aggregate evidence until it can
    confidently answer it. Guo & Brunskill ([2017](#bib.bib3)) propose an algorithm
    to find which features are necessary solve a particular task.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We build a deep reinforcement learning (RL) agent that can predict the likelihood
    of an individual testing positive for malaria by asking questions about their
    household. We propose an algorithm to adaptively determine which survey question
    to ask next and when to stop to make a prediction about their likelihood of malaria
    based on their responses. We show that we can make predictions with high accuracy,
    and that adaptive surveys can significantly reduce the number of questions needed
    to attain the high accuracy, enabling large-scale and cost-effective monitoring
    of malaria through SMS surveys.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Buck et al. (2017) Buck, Christian, Bulian, Jannis, Ciaramita, Massimiliano,
    Gesmundo, Andrea, Houlsby, Neil, Gajewski, Wojciech, and Wang, Wei. Ask the right
    questions: Active question reformulation with reinforcement learning. *CoRR*,
    abs/1705.07830, 2017. URL [http://arxiv.org/abs/1705.07830](http://arxiv.org/abs/1705.07830).'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Game (2013) Game, Eddie. Mobile text surveys: A smarter way to measure conservation’s
    impacts on people? *Nature*, 2013. URL [http://blog.nature.org/science/2013/09/30/mobile-text-survey-conservation-people-human-wellbeing/](http://blog.nature.org/science/2013/09/30/mobile-text-survey-conservation-people-human-wellbeing/).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo & Brunskill (2017) Guo, Zhaohan Daniel and Brunskill, Emma. Sample efficient
    feature selection for factored mdps. *CoRR*, abs/1703.03454, 2017. URL [http://arxiv.org/abs/1703.03454](http://arxiv.org/abs/1703.03454).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo & Brunskill (2017) Guo, Zhaohan Daniel 和 Brunskill, Emma. 针对因式分解 MDP 的样本高效特征选择。
    *CoRR*, abs/1703.03454, 2017。网址 [http://arxiv.org/abs/1703.03454](http://arxiv.org/abs/1703.03454)。
- en: 'Kingma & Ba (2014) Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Ba (2014) Kingma, Diederik 和 Ba, Jimmy. Adam: 一种随机优化方法。 *arXiv 预印本
    arXiv:1412.6980*, 2014。'
- en: Mnih et al. (2013) Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves,
    Alex, Antonoglou, Ioannis, Wierstra, Daan, and Riedmiller, Martin. Playing atari
    with deep reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 (2013) Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex,
    Antonoglou, Ioannis, Wierstra, Daan, 和 Riedmiller, Martin. 使用深度强化学习玩 Atari 游戏。
    *arXiv 预印本 arXiv:1312.5602*, 2013。
- en: NMCP & Kenya National Bureau of Statistics (KNBS) NMCP and Kenya National Bureau of
    Statistics (KNBS) and, ICF International. *Kenya Malaria Indicator Survey 2015*.
    NMCP, KNBS, ICF International, Nairobi, Kenya and Rockville, Maryland.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NMCP & 肯尼亚国家统计局 (KNBS) NMCP 和肯尼亚国家统计局 (KNBS) 及 ICF 国际。*肯尼亚疟疾指标调查 2015*。NMCP,
    KNBS, ICF 国际，肯尼亚内罗毕和马里兰州洛克维尔。
- en: 'Shen et al. (2016) Shen, Yelong, Huang, Po-Sen, Gao, Jianfeng, and Chen, Weizhu.
    Reasonet: Learning to stop reading in machine comprehension. *CoRR*, abs/1609.05284,
    2016. URL [http://arxiv.org/abs/1609.05284](http://arxiv.org/abs/1609.05284).'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等 (2016) Shen, Yelong, Huang, Po-Sen, Gao, Jianfeng, 和 Chen, Weizhu. Reasonet:
    学习在机器理解中停止阅读。 *CoRR*, abs/1609.05284, 2016。网址 [http://arxiv.org/abs/1609.05284](http://arxiv.org/abs/1609.05284)。'
- en: Szegedy et al. (2014) Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,
    Pierre, Reed, Scott E., Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent,
    and Rabinovich, Andrew. Going deeper with convolutions. *CoRR*, abs/1409.4842,
    2014. URL [http://arxiv.org/abs/1409.4842](http://arxiv.org/abs/1409.4842).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等 (2014) Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre,
    Reed, Scott E., Anguelov, Dragomir, Erhan, Dumitru, Vanhoucke, Vincent, 和 Rabinovich,
    Andrew. 深入卷积。 *CoRR*, abs/1409.4842, 2014。网址 [http://arxiv.org/abs/1409.4842](http://arxiv.org/abs/1409.4842)。
- en: Torfin (2017) Torfin, Svenn. In kenya, the path to elimination of malaria is
    lined with good preventions. *WHO*, 2017. URL [http://www.who.int/features/2017/vector-control-kenya/en/](http://www.who.int/features/2017/vector-control-kenya/en/).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torfin (2017) Torfin, Svenn. 在肯尼亚，消除疟疾的道路上布满了良好的预防措施。 *WHO*, 2017。网址 [http://www.who.int/features/2017/vector-control-kenya/en/](http://www.who.int/features/2017/vector-control-kenya/en/)。
- en: Appendix A Appendix
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: '| Question | Variable Name | Num Categories |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 变量名称 | 类别数量 |'
- en: '| --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Region | 8 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 区域 | 8 |'
- en: '| 2 | Main floor material | 9 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 主层材料 | 9 |'
- en: '| 3 | Owns land usable for agriculture | 2 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 是否拥有可用于农业的土地 | 2 |'
- en: '| 4 | Has electricity | 2 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 是否有电 | 2 |'
- en: '| 5 | Has television | 2 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 是否有电视 | 2 |'
- en: '| 6 | Whether nets are used alternatively in this community | 2 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 该社区是否交替使用网 | 2 |'
- en: '| 7 | Most people in the community sleep under an ITN all the time | 4 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 社区中的大多数人是否总是使用 ITN | 4 |'
- en: '| 8 | Type of place of residence | 2 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 居住地点的类型 | 2 |'
- en: 'Table 2: The list of processed categorical variables, sorted by decreasing
    correlation with the output variable (the result of the blood smear test).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 处理过的分类变量列表，按与输出变量（血涂片测试结果）的相关性降序排列。'
- en: '![Refer to caption](img/f7b57685eacb8df4610d67887f04ba22.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f7b57685eacb8df4610d67887f04ba22.png)'
- en: 'Figure 3: The DQN outputs a Q value for each of the possible queries and prediction
    outputs from its input state.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: DQN 为每一个可能的查询和从其输入状态的预测输出提供一个 Q 值。'
- en: '![Refer to caption](img/da400fc0414669b5b4a994adfb75b15e.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/da400fc0414669b5b4a994adfb75b15e.png)'
- en: 'Figure 4: The average training (top) and evaluation (bottom) episode rewards
    of the RL agents during the training process. The behavior policy for training
    episodes is $\epsilon-$greedy with a linearly annealed $\epsilon$; for test episodes,
    we set $\epsilon=0$, and act greedily with respect to the $Q$ values outputted
    by the DQN.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: RL 代理在训练过程中平均训练（上）和评估（下）回报。训练回合的行为策略是 $\epsilon-$贪婪的，$\epsilon$ 线性退火；对于测试回合，我们设定
    $\epsilon=0$，并根据 DQN 输出的 $Q$ 值进行贪婪动作。'
