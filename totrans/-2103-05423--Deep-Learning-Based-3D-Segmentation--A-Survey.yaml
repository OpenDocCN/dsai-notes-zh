- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:56:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2103.05423] Deep Learning Based 3D Segmentation: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05423](https://ar5iv.labs.arxiv.org/html/2103.05423)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[type=editor, auid=000, bioid=1, orcid= 0000-0003-2916-3068]'
  prefs: []
  type: TYPE_NORMAL
- en: '[type=editor,auid=000,bioid=1, orcid = 0000-0003-1973-6766]'
  prefs: []
  type: TYPE_NORMAL
- en: '[type=editor,auid=000,bioid=1,] [type=editor,auid=000,bioid=1,] [type=editor,auid=000,bioid=1,]
    [type=editor,auid=000,bioid=1,]'
  prefs: []
  type: TYPE_NORMAL
- en: 1]organization=Hunan University, addressline=Lushan South Rd., Yuelu Dist.,
    city=Changsha, postcode=410082, state=Hunan, country=China
  prefs: []
  type: TYPE_NORMAL
- en: 2]organization=University of Western Australia, addressline=35 Stirling Hwy,
    city=Perth, postcode=6009, state=WA, country=Australia
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning Based 3D Segmentation: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yong He h.yong@hnu.edu.cn    Hongshan Yu yuhongshancn@hotmail.com    Xiaoyan
    Liu xiaoyan.liu@hnu.edu.cn    Zhengeng Yang yzg050215@163.com    Wei Sun david-sun@126.com
       Ajaml Mian ajmal.mian@uwa.edu.au [ [
  prefs: []
  type: TYPE_NORMAL
- en: A B S T R A C T
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 3D segmentation is a fundamental and challenging problem in computer vision
    with applications in autonomous driving, robotics, augmented reality and medical
    image analysis. It has received significant attention from the computer vision,
    graphics and machine learning communities. Conventional methods for 3D segmentation,
    based on hand-crafted features and machine learning classifiers, lack generalization
    ability. Driven by their success in 2D computer vision, deep learning techniques
    have recently become the tool of choice for 3D segmentation tasks. This has led
    to an influx of a large number of methods in the literature that have been evaluated
    on different benchmark datasets. Whereas survey papers on RGB-D and point cloud
    segmentation exist, there is a lack of an in-depth and recent survey that covers
    all 3D data modalities and application domains. This paper fills the gap and provides
    a comprehensive survey of the recent progress made in deep learning based 3D segmentation.
    It covers over 180 works, analyzes their strengths and limitations and discusses
    their competitive results on benchmark datasets. The survey provides a summary
    of the most commonly used pipelines and finally highlights promising research
    directions for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 3D data\sep3D semantic segmentation\sep3D instance segmentation\sep3D part segmentation\sep3D
    video segmentation\sep3D semantic map\sepDeep learning\sep
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Segmentation of 3D scenes is a fundamental and challenging problem in computer
    vision as well as computer graphics. The objective of 3D segmentation is to build
    computational techniques that predict the fine-grained labels of objects in a
    3D scene for a wide range of applications such as autonomous driving, mobile robots,
    industrial control, augmented reality and medical image analysis. As illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey"), 3D segmentation can be divided into three types: semantic, instance
    and part segmentation. Semantic segmentation aims to predict object class labels
    such as table and chair. Instance segmentation additionally distinguishes between
    different instances of the same class labels e.g. table one/two and chair one/two.
    Part segmentation aims to decompose instances further into their different components
    such as armrests, legs and backrest of the same chair.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to conventional single view 2D segmentation, 3D segmentation gives
    a more comprehensive understanding of a scene, since 3D data (e.g. RGB-D, point
    cloud, voxel, mesh, 3D video) contain richer geometric, shape, and scale information
    with less background noise. Moreover, the representation of 3D data, for example
    in the form of projected images, has more semantic information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5adef999a7403ba147c6589c55015740.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The main five types of 3D data: (a) RGB-D image, (b) projected images,
    (c) voxels, (d) mesh, and (d) point. Types of 3D segmentation: (f) 3D semantic
    segmentation, (g) 3D instance segmentation, and (h) 3D part segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/938b87a3bbb92dfc3903f056d414a72b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Complete overview of the survey paper.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, deep learning techniques have dominated many research areas including
    computer vision and natural language processing. Motivated by its success in learning
    powerful features, deep learning for 3D segmentation has also attracted a growing
    interest from the research community over the past decade. However, 3D deep learning
    methods still face many unsolved challenges. For example, irregularity of point
    clouds makes it difficult to exploit local features and converting them to high-resolution
    voxels comes with a huge computational burden.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper provides a comprehensive survey of recent progress in deep learning
    methods for 3D segmentation. It focuses on analyzing commonly used building blocks,
    convolution kernels and complete architectures pointing out the pros and cons
    in each case. The survey covers over 180 representative papers published in the
    last five years. Although some notable 3D segmentation surveys have been released
    including RGB-D semantic segmentation Fooladgar and Kasaei ([2020](#bib.bib30)),
    remote sensing imagery segmentation Yuan et al. ([2021](#bib.bib186)), point clouds
    segmentation Xie et al. ([2020a](#bib.bib170)), Guo et al. ([2020](#bib.bib36)),
    Liu et al. ([2019a](#bib.bib95)), Bello et al. ([2020](#bib.bib4)), Naseer et al.
    ([2018](#bib.bib109)), Ioannidou et al. ([2017](#bib.bib56)), these surveys do
    not comprehensively cover all 3D data types and typical application domains. Most
    importantly, these surveys do not focus on 3D segmentation but give a general
    survey of deep learning from point clouds Guo et al. ([2020](#bib.bib36)), Liu
    et al. ([2019a](#bib.bib95)), Bello et al. ([2020](#bib.bib4)), Naseer et al.
    ([2018](#bib.bib109)), Ioannidou et al. ([2017](#bib.bib56)). Given the importance
    of the three segmentation tasks, this paper focuses exclusively on deep learning
    techniques for 3D segmentation. The contributions of this paper are summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the first survey paper to comprehensively
    cover deep learning methods on 3D segmentation covering all 3D data representations,
    including RGB-D, projected images, voxels, point clouds, meshes, and 3D videos.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This survey provides an in-depth analysis of the relative advantages and disadvantages
    of different types of 3D data segmentation methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike existing reviews, this survey papers focuses on deep learning methods
    designed specifically for 3D segmentation and also discusses typical segmentation
    pipelines as well as application domains.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, this survey provides comprehensive comparisons of existing methods
    on several public benchmark 3D datasets, draw interesting conclusions and identify
    promising future research directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey") shows a snapshot of how this survey is organized. Section [2](#S2 "2
    Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation: A Survey")
    introduces some terminology and background concepts, including popular 3D datasets
    and evaluation metrics for 3D segmentation. Section [3](#S3 "3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey") reviews methods for 3D semantic
    segmentation whereas Section [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey") reviews methods for 3D instance segmentation.
    Section [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") provides a survey of existing methods for 3D part segmentation. Section
    [6](#S6 "6 Applications of 3D Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") reviews the 3D segmentation methods used in some common application
    areas including 3D video segmentation and 3D semantic map. Section [7](#S7 "7
    Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey") presents
    performance comparison between 3D segmentation methods on several popular datasets,
    and gives corresponding data analysis. Finally, Section [8](#S8 "8 Discussion
    and Conclusion ‣ Deep Learning Based 3D Segmentation: A Survey") identifies promising
    future research directions and concludes the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87380b46ede615306a8ddc74295a6939.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Annotated examples from (a) S3DIS, (b) Semantic3D, (c) SemanticKITTI
    for 3D semantic segmentation, (d) ScanNet for 3D instance segmentation, and (e)
    ShapeNet for 3D part segmentation. See Table [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation
    Dataset ‣ 2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation:
    A Survey") for a summary of these datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Terminology and Background Concept
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces some terminologies and background concepts, including
    3D data representation, popular 3D segmentation datasets and evaluation metrics
    to help the reader easily navigate through the field of 3D segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 3D Segmentation Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets are critical to train and test 3D segmentation algorithms using deep
    learning. However, it is cumbersome and expensive to privately gather and annotate
    datasets as it needs domain expertise, high quality sensors and processing equipment.
    Thus, building on public datasets is an ideal way to reduce the cost. Following
    this way has another advantage for the community that it provides a fair comparison
    between algorithms. Table [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation Dataset ‣
    2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation: A
    Survey") summarizes some of the most popular and typical datasets with respect
    to the sensor type, data size and format, scene class and annotation method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These datasets are acquired for *3D semantic segmentation* by different type
    of sensors, including RGB-D cameras Silberman and Fergus ([2011](#bib.bib129)),
    Silberman et al. ([2012](#bib.bib130)), Song et al. ([2015](#bib.bib132)), Hua
    et al. ([2016](#bib.bib51)), Dai et al. ([2017](#bib.bib18)), mobile laser scanner
    Roynard et al. ([2018](#bib.bib125)), Behley et al. ([2019](#bib.bib3)), static
    terrestrial scanner Hackel et al. ([2017](#bib.bib38)) and unreal engine Brodeur
    et al. ([2017](#bib.bib7)), Wu et al. ([2018b](#bib.bib164)) and other 3D scanners
    Armeni et al. ([2016](#bib.bib2)), Chang et al. ([2017](#bib.bib9)). Among these,
    the ones obtained from unreal engine are synthetic datasets Brodeur et al. ([2017](#bib.bib7))
    Wu et al. ([2018b](#bib.bib164)) that do not require expensive equipment or annotation
    time. These are also rich in categories and quantities of objects. Synthetic datasets
    have complete 360 degree 3D objects with no occlusion effects or noise compared
    to the real-world datasets which are noisy and contain occlusions Silberman and
    Fergus ([2011](#bib.bib129)), Silberman et al. ([2012](#bib.bib130)), Song et al.
    ([2015](#bib.bib132)), Hua et al. ([2016](#bib.bib51)), Dai et al. ([2017](#bib.bib18)),
    Roynard et al. ([2018](#bib.bib125)), Behley et al. ([2019](#bib.bib3)), Armeni
    et al. ([2016](#bib.bib2)), Hackel et al. ([2017](#bib.bib38)), Chang et al. ([2017](#bib.bib9)).
    For *3D instance segmentation*, there are limited 3D datasets, such as ScanNet
    Dai et al. ([2017](#bib.bib18)) and S3DIS Armeni et al. ([2016](#bib.bib2)). These
    two datasets contain scans of real-world indoor scenes obtained by RGB-D cameras
    or Matterport separately. For *3D part segmentation*, the Princeton Segmentation
    Benchmark (PSB) Chen et al. ([2009](#bib.bib11)), COSEG Wang et al. ([2012](#bib.bib153))
    and ShapeNet Yi et al. ([2016](#bib.bib181)) are three of the most popular datasets.
    Below, we introduce five famous segmentation datasets in detail, including S3DIS
    Armeni et al. ([2016](#bib.bib2)), ScanNet Dai et al. ([2017](#bib.bib18)), Semantic3D
    Hackel et al. ([2017](#bib.bib38)), SemanticKITTI Chang et al. ([2017](#bib.bib9))
    and ShapeNet Yi et al. ([2016](#bib.bib181)). Some examples with annotation from
    these datasets are shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep
    Learning Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'S3DIS: In this dataset, the complete point clouds are obtained without any
    manual intervention using the Matterport scanner. The dataset consists of 271
    rooms belonging to 6 large-scale indoor scenes from 3 different buildings (total
    of 6020 square meters). These areas mainly include offices, educational and exhibition
    spaces, and conference rooms etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic3D comprises a total of around 4 billion 3D points acquired with static
    terrestrial laser scanners, covering up to 160×240×30 meters in real-world 3D
    space. Point clouds belong to 8 classes (e.g. urban and rural) and contain 3D
    coordinates, RGB information, and intensity. Unlike 2D annotation strategies,
    3D data labeling is easily amenable to over-segmentation where each point is individually
    assigned to a class label.
  prefs: []
  type: TYPE_NORMAL
- en: SemanticKITTI is a large outdoor dataset containing detailed point-wise annotation
    of 28 classes. Building on the KITTI vision benchmark Geiger et al. ([2012](#bib.bib31)),
    SemanticKITTI contains annotations of all 22 sequences of this benchmark consisting
    of 43K scans. Moreover, the dataset contains labels for the complete horizontal
    360 filed-of-view of the rotating laser sensor.
  prefs: []
  type: TYPE_NORMAL
- en: ScanNet dataset is particularly valuable for research in scene understanding
    as its annotations contain estimated calibration parameters, camera poses, 3D
    surface reconstruction, textured meshes, dense object level semantic segmentation,
    and CAD models. The dataset comprises annotated RGB-D scans of real-world environments.
    There are 2.5M RGB-D images in 1513 scans acquired in 707 distinct places. After
    RGB-D image processing, annotation human intelligence tasks were performed using
    the Amazon Mechanical Turk.
  prefs: []
  type: TYPE_NORMAL
- en: ShapeNet dataset has a novel scalable method for efficient and accurate geometric
    annotation of massive 3D shape collections. The novel technical innovations explicitly
    model and lessen the human cost of the annotation effort. Researchers create detailed
    point-wise labeling of 31963 models in shape categories in ShapeNetCore and combine
    feature-based classifiers, point-to-point correspondences, and shape-to-shape
    similarities into a single CRF optimization over the network of shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of popular datasets for 3D segmentation datasets including
    the sensor, type, size, object class, number of classes (shown in brackets), and
    annotation method. S←synthetic environment. R←real-world environment. Kf←thousand
    frames. s←scan. Mp←million points. the symbol ‘–’ means information unavailable.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Sensors | Type | Size | Scene class (number) | Annotation method
    |'
  prefs: []
  type: TYPE_TB
- en: '| Datasets for 3D semantic segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| NYUv1 Silberman and Fergus ([2011](#bib.bib129)) | Microsoft Kinect v1 |
    R | 2347f | bedroom, cafe, kitchen, etc. (7) | Condition Random Field-based model
    |'
  prefs: []
  type: TYPE_TB
- en: '| NYUv2 Silberman et al. ([2012](#bib.bib130)) | Microsoft Kinect v1 | R |
    1449f | bedroom, cafe, kitchen, etc. (26) | 2D annotation from AMK |'
  prefs: []
  type: TYPE_TB
- en: '| SUN RGB-D Song et al. ([2015](#bib.bib132)) | RealSense, Xtion LIVE PRO,
    MKv1/2 | R | 10355f | objects, room layouts, etc.(47) | 2D/3Dpolygons +3D bounding
    box |'
  prefs: []
  type: TYPE_TB
- en: '| SceneNN Hua et al. ([2016](#bib.bib51)) | Asus Xtion PRO, MK v2 | R | 100s
    | bedroom, office, apartment, etc.(-) | 3D Labels project to 2D frames |'
  prefs: []
  type: TYPE_TB
- en: '| RueMonge2014 Riemenschneider et al. ([2014](#bib.bib123)) | – | R | 428s
    | window, wall, balcony, door, etc(7) | Multi-view semantic labelling + CRF |'
  prefs: []
  type: TYPE_TB
- en: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital structure sensor | R
    | 2.5Mf | office, apartment, bathroom, etc(19) | 3D labels project to 2D frames
    |'
  prefs: []
  type: TYPE_TB
- en: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport camera | R | 70496f
    | conference rooms, offices, etc(11) | Hierarchical labeling |'
  prefs: []
  type: TYPE_TB
- en: '| Semantic3D Hackel et al. ([2017](#bib.bib38)) | Terrestrial laser scanner
    | R | 1660Mp | farms, town hall, sport fields, etc (8) | Three baseline methods
    |'
  prefs: []
  type: TYPE_TB
- en: '| NPM3D Roynard et al. ([2018](#bib.bib125)) | Velodyne HDL-32E LiDAR | R |
    143.1Mp | ground, vehicle, hunman, etc (50) | Human labeling |'
  prefs: []
  type: TYPE_TB
- en: '| SemanticKITTI Behley et al. ([2019](#bib.bib3)) | Velodyne HDL-64E | R |
    43Ks | ground, vehicle, hunman, etc(28) | Multi-scans semantic labelling |'
  prefs: []
  type: TYPE_TB
- en: '| Matterport3D Chang et al. ([2017](#bib.bib9)) | Matterport camera | R | 194.4Kf
    | various rooms (90) | Hierarchical labeling |'
  prefs: []
  type: TYPE_TB
- en: '| HoME Brodeur et al. ([2017](#bib.bib7)) | Planner5D platform | S | 45622f
    | rooms, object and etc.(84) | SSCNet+ a short text description |'
  prefs: []
  type: TYPE_TB
- en: '| House3D Wu et al. ([2018b](#bib.bib164)) | Planner5D platform | S | 45622f
    | rooms, object and etc.(84) | SSCNet+3 ways |'
  prefs: []
  type: TYPE_TB
- en: '| Datasets for 3D instance segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital structure sensor | R
    | 2.5Mf | office, apartment, bathroom, etc(19) | 3D labels project to 2D frames
    |'
  prefs: []
  type: TYPE_TB
- en: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport camera | R | 70496f
    | conference rooms, offices, etc(11) | Active learning method |'
  prefs: []
  type: TYPE_TB
- en: '| Datasets for 3D part segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| ShapeNet Yi et al. ([2016](#bib.bib181)) | – | S | 31963s | transportation,
    tool, etc.(16) | Propagating human label to shapes |'
  prefs: []
  type: TYPE_TB
- en: '| PSB Chen et al. ([2009](#bib.bib11)) | Amazon’s Mechanical Turk | S | 380s
    | human,cup, glasses airplane,etc(19) | Interactive segmentation tool |'
  prefs: []
  type: TYPE_TB
- en: '| COSEG Wang et al. ([2012](#bib.bib153)) | – | S | 1090s | vase, lamp, guiter,
    etc (11) | semi-supervised learning method |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different evaluation metrics can assert the validity and superiority of segmentation
    methods including the execution time, memory footprint and accuracy. However,
    few authors provide detailed information about the execution time and memory footprint
    of their method. This paper introduces the accuracy metrics mainly.
  prefs: []
  type: TYPE_NORMAL
- en: For *3D semantic segmentation*, Overall Accuracy (OAcc), mean class Accuracy
    (mAcc) and mean class Intersection over Union (mIoU) are the most frequently used
    metrics to measure the accuracy of segmentation methods. For the sake of explanation,
    we assume that there are a total of $K+1$ classes, and $p_{ij}$ is the minimum
    unit (e.g. pixel, voxel, mesh, point) of class $i$ implied to belong to class
    $j$. In other words, $p_{ii}$ represents true positives, while $p_{ij}$ and $p_{ji}$
    represent false positives and false negatives respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Overall Accuracy is a simple metric that computes the ratio between the number
    of truly classified samples and the total number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $OAcc=\sum_{i=0}^{K}{\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Mean Accuracy is an extension of OAcc, computing OAcc in a per-class and then
    averaging over the total number of classes $K$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $mAcc=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Mean Intersection over Union is a standard metric for semantic segmentation.
    It computes the intersection ratio between ground truth and predicted value averaged
    over the total number of classes $K$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $mIoU=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}+\sum_{i=0}^{K}p_{ji}-p_{ii}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For *3D instance segmentation*, Average Precision (AP) and mean class Average
    Precision (mAP) are also frequently used. Assuming $L_{I},I\in[0,K]$ instance
    in every class, and $c_{ij}$ is the amount of point of instance $i$ inferred to
    belong to instance $j$ ($i=j$ represents correct and $i\neq j$ represents incorrect
    segmentations).
  prefs: []
  type: TYPE_NORMAL
- en: Average Precision is another simple metric for segmentation that computes the
    ratio between true positives and the total number of positive samples.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $AP=\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Mean Average precision is an extension of AP which computes per-class AP and
    then averages over the total number of classes $K$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $mAP=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: For *3D part segmentation*, overall average category Intersection over Union
    ($mIoU_{cat}$) and overall average instance Intersection over Union ($mIoU_{ins}$)
    are most frequently used. For the sake of explanation, we assume $M_{J},J\in[0,L_{I}]$
    parts in every instance, and $q_{ij}$ as the total number of points in part $i$
    inferred to belong to part $j$. Hence, $q_{ii}$ represents the number of true
    positive, while $q_{ij}$ and $q_{ji}$ are false positives and false negative respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Overall average category Intersection over Union is an evaluation metric for
    part segmentation that measures the mean IoU averaged across K classes.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $mIoU_{cat}=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Overall average instance Intersection over Union, for part segmentation, measures
    the mean IoU across all instances.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $mIoU_{ins}=\frac{1}{\sum_{I=0}^{K}L_{I}+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 3 3D Semantic segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many deep learning methods on 3D semantic segmentation have been proposed in
    the literature. These methods can be divided into five categories according to
    the data representation used, namely, RGB-D image based, projected images based,
    voxel based, point based, 3D video and other representations based. Point based
    methods can be further categorized, based on the network architecture, into Multiple
    Layer Perceptron (MLP) based, Point Convolution based and Graph Convolution based
    and Point Transformer based methods. Figure [4](#S3.F4 "Figure 4 ‣ 3 3D Semantic
    segmentation ‣ Deep Learning Based 3D Segmentation: A Survey") shows the milestones
    of deep learning on 3D semantic segmentation in recent years.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1f5a4979d0c6a9c762dd7dce7e3a4b5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Milestones of deep learning based 3D semantic segmentation methods.
    Note that the arrow (timeline) goes anti-clockwise'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 RGB-D Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The depth map in an RGB-D image contains geometric information about the real-world
    which is useful to distinguish foreground objects from background, hence providing
    opportunities to improve the segmentation accuracy. In this category, generally
    the classical two-channel network is used to extract features from RGB and depth
    images separately. However, this simple framework is not powerful enough to extract
    rich and refined features. To this end, researchers have integrated several additional
    modules into the above simple two-channel framework to improve the performance
    by learning rich *context* and *geometric* information that are crucial for semantic
    segmentation. These modules can be roughly divided into six categories: multi-task
    learning, depth encoding, multi-scale network, novel neural network architectures,
    data/feature/score level fusion and post-processing (see Figure [5](#S3.F5 "Figure
    5 ‣ 3.1 RGB-D Based ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")). RGB-D image based semantic segmentation methods are summarized in
    Table [2](#S3.T2 "Table 2 ‣ 3.1 RGB-D Based ‣ 3 3D Semantic segmentation ‣ Deep
    Learning Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-tasks learning: *Depth estimation* and semantic segmentation are two fundamental
    challenging tasks in computer vision. These tasks are also somewhat related as
    depth variation within an object is small compared to depth variation between
    different objects. Hence, many researchers choose to unite depth estimation task
    and semantic segmentation task. From the view of relationship of the two tasks,
    there are two main types of multi-task leaning framework, cascade and parallel
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: As for the cascade framework, depth estimation task provides depth images for
    semantic segmentation task. For example, Cao et al.  Cao et al. ([2016](#bib.bib8))
    used the deep convolutional neural fields (DCNF) introduced by Liu et al.  Liu
    et al. ([2015](#bib.bib92)) for depth estimation. The estimated depth images and
    RGB images are fed into a two-channel FCN for semantic segmentation. Similarly,
    Guo et al.  Guo and Chen ([2018](#bib.bib35)) adopted the deep network proposed
    by Ivanecky  Ivaneckỳ ([2016](#bib.bib57)) for automatic generating depth images
    from single RGB images, and then proposed a two-channel FCN model on the image
    pair of RGB and predicted depth map for pixel labeling.
  prefs: []
  type: TYPE_NORMAL
- en: The cascade framework performs depth estimation and semantic segmentation separately,
    which is simultaneously unable to perform end-to-end training for two tasks. Consequently,
    depth estimation task does not get any benefit from semantic segmentation task.
    In contrast, the *parallel* framework performs these two tasks in an unify network,
    which allows two tasks get benefits each other. For instance, Wang et al.  Wang
    et al. ([2015](#bib.bib147)) used Joint Global CNN to exploit pixel-wise depth
    values and semantic labels from RGB images to provide accurate global scale and
    semantic guidance. As well as, they use Joint Region CNN to extract region-wise
    depth values and semantic map from RGB to learn detailed depth and semantic boundaries.
    Mousavian et al.  Mousavian et al. ([2016](#bib.bib107)) presented a multi-scale
    FCN comprising five streams that simultaneously explore depth and semantic features
    at different scales, where the two tasks share the underlying feature representation.
    Liu et al.  Liu et al. ([2018b](#bib.bib94)) proposed a collaborative deconvolutional
    neural network(C-DCNN) to jointly model the two tasks. However, the quality of
    depth maps estimated from RGB images is not as good as the one acquired directly
    from depth sensors. This multi-task learning pipeline has been gradually abandoned
    in RGB-D semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depth Encoding: Conventional 2D CNNs are unable to exploit rich geometric features
    from raw depth images. An alternative way is to encode raw depth images into other
    representations that are suitable to 2D CNN. Hoft et al. Höft et al. ([2014](#bib.bib47))
    used a simplified version of the histogram of oriented gradients (HOG) to represent
    depth channel from RGB-D scenes. Gupta et al. Gupta et al. ([2014](#bib.bib37))
    and Aman et al. Lin et al. ([2017](#bib.bib89)) calculated three new channels
    named horizontal disparity, height above ground and angle with gravity (HHA) from
    the raw depth images. Liu et al. Liu et al. ([2018a](#bib.bib93)) point out a
    limitation of HHA that some scenes may not be enough horizontal and vertical planes.
    Hence, they propose a novel gravity direction detection method with vertical lines
    fitted to learn better representation. Hazirbas et al. Hazirbas et al. ([2016](#bib.bib41))
    also argue that HHA representation has a high computational cost and contains
    less information than the raw depth images. They propose an architecture called
    FuseNet that consists of two encoder-decoder branches, including a depth branch
    and an RGB branch, which directly encodes depth information with a lower computational
    load.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-scale Network: The context information learned by multi-scale networks
    is useful for small objects and detailed region segmentation. Couprie et al. Couprie
    et al. ([2013](#bib.bib17)) applied a multi-scale convolutional network to learn
    features directly from the RGB images and the depth images. Aman et al. Raj et al.
    ([2015](#bib.bib119)) proposed a multi-scale deep ConvNet for segmentation where
    the coarse predictions of VGG16-FC net are up sampled in a Scale-2 module and
    then concatenated with the low-level predictions of VGG-M net in Scale-1 module
    to get both high and low level features. However, this method is sensitive to
    clutter in the scene resulting in output errors. Lin et al. Lin et al. ([2017](#bib.bib89))
    exploit the fact that lower scene-resolution regions have higher depth, and higher
    scene-resolution regions have lower depth. They use depth maps to split the corresponding
    color images into multiple scene-resolution regions, and introduce context-aware
    receptive field (CaRF) which focuses on semantic segmentation of certain scene-resolution
    regions. This makes their pipeline a multi-scale network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa0ffb98f08b256b59d1380f2cefaee2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Typical two-channel framework with six improvement modules, including
    (a) multi-tasks learning, (b) depth encoding, (c) multi-scale network, (d) novel
    neural network architecture, (e) feature/score level fusion, and (f) post-processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Novel Neural Networks: Given the fixed grid computation of CNNs, their ability
    to process and exploit geometric information is limited. Therefore, researchers
    have proposed other novel neural network architectures to better exploit geometric
    features and the relationships between RGB and depth images. These architectures
    can be divided into five main categories.
  prefs: []
  type: TYPE_NORMAL
- en: '*Improved 2D Convolutional Neural Networks* (2D CNNs) Inspired from cascaded
    feature networks  Lin et al. ([2017](#bib.bib89)), Jiang et al. Jiang et al. ([2017](#bib.bib62))
    proposed a novel Dense-Sensitive Fully Convolutional Neural Network (DFCN) which
    incorporates depth information into the early layers of the network using feature
    fusion tactics. This is followed by several dilated convolutional layers for context
    information exploitation. Similarly, Wang et al. Wang and Neumann ([2018](#bib.bib150))
    proposed a depth-aware 2D CNN by introducing two novel layers, depth aware convolution
    layer and depth-aware pooling layer, which are based on the prior that pixels
    with the same semantic label and similar depth should have more impact on one
    another.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deconvolutional Neural Networks*(DeconvNets) are a simple yet effective and
    efficient solution for the refinement of segmentation map. Liu et al. Liu et al.
    ([2018b](#bib.bib94)) and Wang et al. Wang et al. ([2016](#bib.bib145)) all adopt
    the DeconvNet for RGB-D semantic segmentation because of good performance. However,
    the potential of DeconvNet is limited since the high-level prediction map aggregates
    large context for dense prediction. To this end, Cheng et al. Cheng et al. ([2017](#bib.bib13))
    proposed a locality-sensitive DeconvNet (LS-DenconvNet) to refine the boundary
    segmentation over depth and color images. LS-DeconvNet incorporates local visual
    and geometric cues from the raw RGB-D data into each DeconvNet, which is able
    to up sample the coarse convolutional maps with large context while recovering
    sharp object boundaries.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Recurrent Neural Networks* (RNNs) can capture long-range dependencies between
    pixels but are mainly suited to a single data channel (e.g. RGB). Fan et al. Fan
    et al. ([2017](#bib.bib27)) extended the single-modal RNNs to multimodal RNNs
    (MM-RNNs) for application to RGB-D scene labeling. The MM-RNNs allow ‘memory’
    sharing across depth and color channels. Each channel not only possess its own
    features but also has the attributes of other channel making the learned features
    more discriminative for semantic segmentation. Li et al. Li et al. ([2016](#bib.bib84))
    proposed a novel Long Short-Term Memorized Context Fusion (LSTM-CF) model to capture
    and fuse contextual information from multiple channels of RGB and depth images.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Graph Neural Networks* (GNNs) were first used for RGB-D semantic segmentation
    by Qi et al. Qi et al. ([2017c](#bib.bib117)) who cast the 2D RGB pixels into
    3D space based on depth information and associated the 3D points with semantic
    information. Nest, they built a k-nearest neighbor graph from the 3D points and
    applied a 3D graph neural network (3DGNN) to perform pixelwise predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Transformers* have gained popularity in RGB image segmentation and have also
    been extended to RGB-D segmentation. Researchers have proposed various approaches
    to leverage Transformers for this purpose. One notable work by Ying et al. Ying
    and Chuah ([2022](#bib.bib184)) introduces the concept of Uncertainty-Aware Self-Attention,
    which explicitly manages the information flow from unreliable depth pixels to
    confident depth pixels during feature extraction. This approach aims to address
    the challenges posed by noisy or uncertain depth information in RGB-D segmentation.
    Another study by Wu et al. Wu et al. ([2022c](#bib.bib166)) adopts the Swin-Transformer
    directly to exploit both the RGB and depth features. By leveraging the self-attention
    mechanism, this approach captures long-range dependencies and enables effective
    fusion of RGB and depth information for segmentation. Inspired by the success
    of the Swin-Transformer, Yang et al. Yang et al. ([2022](#bib.bib179)) proposes
    a hierarchical Swin-RGBD Transformer. This model incorporates and leverages depth
    information to complement and enhance the ambiguous and obscured features in RGB
    images. The hierarchical architecture allows for multi-scale feature learning
    and enables more effective integration of RGB and depth information.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data/Feature/Score Fusion: Optimal fusion of the texture (RGB channels) and
    geometric (depth channel) information is important for accurate semantic segmentation.
    There are three fusion tactics: data level, feature level and score level, referring
    to early, middle and late fusion respectively. A simple *data level fusion* strategy
    is to concatenate the RGB and depth images into four channels for direct input
    to a CNN model e.g. as performed by Couprie et al. Couprie et al. ([2013](#bib.bib17)).
    However, such a data level fusion does not exploit the strong correlations between
    depth and photometric channels. *Feature level fusion*, on the other hand, captures
    these correlations. For example, Li et al. Li et al. ([2016](#bib.bib84)) proposed
    a memorized fusion layer to adaptively fuse vertical depth and RGB contexts in
    a data-driven manner. Their method performs bidirectional propagation along the
    horizontal direction to hold true 2D global contexts. Similarly, Wang et al. Wang
    et al. ([2016](#bib.bib145)) proposed a feature transformation network that correlates
    the depth and color channels, and bridges the convolutional networks and deconvolutional
    networks in a single channel. The feature transformation network can discover
    specific features in a single channel as well as common features between two channels,
    allowing the two branches to share features to improve the representation power
    of shared information. The above complex feature level fusion models are inserted
    in a specific same layer between RGB and depth channels, which is difficult to
    train and ignores other same layer feature fusion. To this end, Hazirbas et al.
    Hazirbas et al. ([2016](#bib.bib41)) and Jiang et al. Jiang et al. ([2017](#bib.bib62))
    carry out fusion as an element-wise summation to fuse feature of multiple same
    layers between the two channels. Wu et al. Wu et al. ([2022c](#bib.bib166)) proposea
    novel Transformer-based fusion scheme, named TransD-Fusion to better model long-range
    contextual information.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Score level fusion* is commonly performed using the simple averaging strategy.
    However, the contributions of RGB model and depth model for semantic segmentation
    are different. Liu et al. Liu et al. ([2018a](#bib.bib93)) proposed a score level
    fusion layer with weighted summation that uses a convolution layer to learn the
    weights from the two channels. Similarly, Cheng et al. Cheng et al. ([2017](#bib.bib13))
    proposed a gated fusion layer to learn the varying performance of RGB and depth
    channels for different class recognition in different scenes. Both techniques
    improved the results over the simple averaging strategy at the cost of additional
    learnable parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Post-Processing: The results of CNN or DCNN used for RGB-D semantic segmentation
    are generally very coarse resulting in rough boundaries and the vanishing of small
    objects. A common method to address this problem is to couple the CNN with a Conditional
    Random Field (CRF). Wang et al. Wang et al. ([2015](#bib.bib147)) further boost
    the mutual interactions between the two channels by the joint inference of Hierarchical
    CRF (HCRF). It enforces synergy between global and local predictions, where the
    global layouts are used to guide the local predictions and reduce local ambiguities,
    as well as local results provide detailed regional structures and boundaries.
    Mousavian et al. Mousavian et al. ([2016](#bib.bib107)), Liu et al. Liu et al.
    ([2018b](#bib.bib94)), and Long et al. Liu et al. ([2018a](#bib.bib93)) adopt
    a Fully Connected CRF (FC-CRF) for post-processing, where the pixel-wise label
    prediction jointly considers geometric constraint, such as pixel-wise normal information,
    pixel position, intensity and depth, to promote the consistency of pixel-wise
    labeling. Similarly, Jiang et al. Jiang et al. ([2017](#bib.bib62)) proposed Dense-sensitive
    CRF (DCRF) that integrates the depth information with FC-CRF.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of RGB-D based methods with deep learning. Est.←depth estimation.
    Enc.←depth encoding. Mul.←multi-scale networks. Nov.←novel neural networks. Fus.←data/feature/score
    fusion. Pos.←post-processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Est. | Enc. | Mul. | Nov. | Fus. | Pos. | Architecture(2-stream)
    | Contribution |'
  prefs: []
  type: TYPE_TB
- en: '| Cao et al. ([2016](#bib.bib8)) | $\checkmark$ | $\checkmark$ | $\times$ |
    $\times$ | $\checkmark$ | $\times$ | FCNs | Estimating depth images+a unified
    network for two tasks |'
  prefs: []
  type: TYPE_TB
- en: '| Guo and Chen ([2018](#bib.bib35)) | $\checkmark$ | $\times$ | $\times$ |
    $\times$ | $\checkmark$ | $\times$ | FCNs | Incorporating depth & gradient for
    depth estim. |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2015](#bib.bib147)) | $\checkmark$ | $\times$ | $\times$ |
    $\times$ | $\times$ | $\checkmark$ | Region./Global CNN | HCRF for fusion and
    refining + two tasks by a network |'
  prefs: []
  type: TYPE_TB
- en: '| Mousavian et al. ([2016](#bib.bib107)) | $\checkmark$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\checkmark$ | FCN | FC-CRF for refining + Mutual
    improvement for two tasks |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2018b](#bib.bib94)) | $\checkmark$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | S/D-DCNN | PBL for two feature maps integration +
    FC-CRF |'
  prefs: []
  type: TYPE_TB
- en: '| Höft et al. ([2014](#bib.bib47)) | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | A embedding for depth images |'
  prefs: []
  type: TYPE_TB
- en: '| Gupta et al. ([2014](#bib.bib37)) | $\times$ | $\checkmark$ | $\times$ |
    $\times$ | $\times$ | $\times$ | CNNs | HHA for depth images |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2018a](#bib.bib93)) | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\checkmark$ | DCNNs | New depth encoding+ FC-CRF for refining
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hazirbas et al. ([2016](#bib.bib41)) | $\times$ | $\checkmark$ | $\times$
    | $\times$ | $\checkmark$ | $\times$ | Encoder-decoder | Semantic and depth feature
    fusion at each layer |'
  prefs: []
  type: TYPE_TB
- en: '| Couprie et al. ([2013](#bib.bib17)) | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | ConvNets | RGB laplacian pyramid for multi-scale
    features |'
  prefs: []
  type: TYPE_TB
- en: '| Raj et al. ([2015](#bib.bib119)) | $\times$ | $\checkmark$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | VGG-M | New multi-scale deep CNN |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. ([2017](#bib.bib89)) | $\times$ | $\times$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | $\times$ | CFN | CaRF for multi-resolution features |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. ([2017](#bib.bib62)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\checkmark$ | RGB-FCN | Semantic & depth feature fusion at each
    layer + DCRF |'
  prefs: []
  type: TYPE_TB
- en: '| Wang and Neumann ([2018](#bib.bib150)) | $\times$ | $\times$ | $\times$ |
    $\checkmark$ | $\times$ | $\times$ | Depth-aware CNN | Depth-aware Conv. and depth
    aware average pooling |'
  prefs: []
  type: TYPE_TB
- en: '| Cheng et al. ([2017](#bib.bib13)) | $\times$ | $\checkmark$ | $\times$ |
    $\checkmark$ | $\checkmark$ | $\times$ | FCN + Deconv | LS-DeconvNet + novel gated
    fusion |'
  prefs: []
  type: TYPE_TB
- en: '| Fan et al. ([2017](#bib.bib27)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | MM-RNNs | Multimodal RNN |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2016](#bib.bib84)) | $\times$ | $\checkmark$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | LSTM-CF | LSTM-CF for capturing and fusing contextual
    inf. |'
  prefs: []
  type: TYPE_TB
- en: '| Qi et al. ([2017c](#bib.bib117)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | 3DGNN | GNN for RGB-D semantic segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2016](#bib.bib145)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | ConvNet-DeconvNet | MK-MMD for assessing the similarity
    between common features |'
  prefs: []
  type: TYPE_TB
- en: '| Ying and Chuah ([2022](#bib.bib184)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformer | Effective and scalable fusion module
    based on aross-attention |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. ([2022c](#bib.bib166)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformers | Transformer-based fusion module
    |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. ([2022](#bib.bib179)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | Swin-Transformer+ResNet | Swin-RGB-D Transformer |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Projected Images Based Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The core idea of projected images based semantic segmentation is to use 2D CNNs
    to exploit features from projected images of 3D scenes/shapes and then fuse these
    features for label prediction. This pipeline not only exploits more semantic information
    from large-scale scenes compared to a single-view image, but also reduces the
    data size of a 3D scene compared to a point cloud. The projected images mainly
    include *multi-view images* or *spherical images*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among, multi-view images projection is usually employed on RGB-D datasets Dai
    et al. ([2017](#bib.bib18)), and statics terrestrial scanning datasets Hackel
    et al. ([2017](#bib.bib38)). Spherical images projection is usually employed on
    self-driving mobile laser scanning datasets Behley et al. ([2019](#bib.bib3)).
    Projected images based semantic segmentation methods are summarized in Table [3](#S3.T3
    "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected Images Based
    Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Multi-View Images Based Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'MVCNN Su et al. ([2015](#bib.bib135)) uses a unified network to combine features
    from multiple views of a 3D shape, formed by a virtual camera, into a single and
    compact shape descriptor to get improved classification performance. This inspired
    researchers to take the same idea into 3D semantic segmentation (see Figure [6](#S3.F6
    "Figure 6 ‣ 3.2.1 Multi-View Images Based Segmentation ‣ 3.2 Projected Images
    Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")). For example, Lawin et al. Lawin et al. ([2017](#bib.bib75)) project
    point clouds into multi-view synthetic images, including RGB, depth and surface
    normal images. The prediction score of all multi-view images is fused into a single
    representation and back-projected into each point. However, the snapshot can erroneously
    catch the points behind the observed structure if the density of the point cloud
    is low, which makes the deep network to misinterpret the multiple views. To this
    end, SnapNet Boulch et al. ([2017](#bib.bib6)), Boulch et al. ([2018](#bib.bib5))
    preprocesses point clouds for computing point features(like normal or local noise)
    and generating a mesh, which is similar to point cloud densification. From the
    mesh and point clouds, they generate RGB and depth images by suitable snapshot.
    Then, they perform a pixel-wise labeling of 2D snapshots using FCN and fast back-project
    these labels into 3D points by efficient buffering. Above methods need obtain
    the whole point clouds of 3D scene in advance to provide a complete spatial structure
    for back-projection. However, the multi-view images directly obtained from real-world
    scene would lose much spatial information. some works attempt to unite 3D scene
    reconstruction with semantic segmentation, where scene reconstruction could make
    up for spatial information. For example, Guerry et al. Guerry et al. ([2017](#bib.bib34))
    reconstruct 3D scene with global multi-view RGB and Gray stereo images. Then,
    the labels of 2D snapshots are back-projected onto the reconstructed scene. But,
    simple back-projection can not optimally fuse semantic and spatial geometric features.
    Along the line, Pham et al. Pham et al. ([2019a](#bib.bib113)) proposed a novel
    Higher-order CRF, following back-projection, to further develop the initial segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9199e7695799d6dd521bc6661680a8c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Illustration of basic frameworks for projected images based segmentation
    methods. Top: Multi-view images based framework. Bottom: Spherical images based
    framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Spherical Images Based Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Selecting snapshots from a 3D scene is not straight forward. Snapshots must
    be taken after giving due consideration to the number of viewpoints, viewing distance
    and angle of the virtual cameras to get an optimal representation of the complete
    scene. To avoid these complexities, researchers project the complete point cloud
    onto a sphere (see Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1 Multi-View Images Based
    Segmentation ‣ 3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").Bottom). For example, Wu et
    al. Wu et al. ([2018a](#bib.bib159)) proposed an end-to-end pipeline called SqueezeSeg,
    inspired from SqueezeNet Iandola et al. ([2016](#bib.bib55)), to learn features
    from spherical images which are then refined by CRF implemented as a recurrent
    layer. Similarly, PointSeg Wang et al. ([2018e](#bib.bib154)) extends the SqueezeNet
    by integrating the feature-wise and channel-wise attention to learn robust representation.
    SqueezeSegv2 Wu et al. ([2019a](#bib.bib160)) improves the structure of SqueezeSeg
    with Context Aggregation Module (CAM), adding LiDAR mask as a channel to increase
    robustness to noise. RangNet++ Milioto et al. ([2019](#bib.bib105)) transfers
    the semantic labels to 3D point clouds, avoiding discarding points regardless
    of the level of discretization used in CNN. Despite the likeness between regular
    RGB and LiDAR images, the feature distribution of LiDAR images changes at different
    locations. SqueezeSegv3 Xu et al. ([2020](#bib.bib172)) has a spatially-adaptive
    and context-aware convolution, termed Spatially-Adaptive Convolution (SAC) to
    adopt different filters for different locations. Inspired by the success of 2D
    vision Transformer, RangViT Ando et al. ([2023](#bib.bib1)) leverage ViTs pre-trained
    on long natural image datasets by adding the down and up module on the top and
    bottom of ViTs, and achieves a good performance comparing to the projection based
    methods. Similarly, to make the long projection image to suit the ViTs, RangeFormer
    Kong et al. ([2023](#bib.bib70)) adopts a scalable training strategy that splits
    the whole projection image into several sub-images, and puts them into ViTs for
    training. After training, the predictions are merged sequentially to form the
    complete scene.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of projected images/voxel/other representation based methods
    with deep learning. M←multi-view image. S←spherical image. V←voxel. T←tangent
    images. L←lattice. P←point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Input | Architecture | Feature extractor | Contribution
    |'
  prefs: []
  type: TYPE_TB
- en: '| projection | Lawin et al. Lawin et al. ([2017](#bib.bib75)) | M | multi-stream
    | VGG-16 | Investigate the impact of different input modalities |'
  prefs: []
  type: TYPE_TB
- en: '| Boulch et al. Boulch et al. ([2017](#bib.bib6))  Boulch et al. ([2018](#bib.bib5))
    | M | SegNet/U-Net | VGG-16 | New and efficient framework SnapNet |'
  prefs: []
  type: TYPE_TB
- en: '| Guerry et al. Guerry et al. ([2017](#bib.bib34)) | M | SegNet/U-Net | VGG-16
    | Improved MVCNN+3D consistent data augment. |'
  prefs: []
  type: TYPE_TB
- en: '| Pham et al. Pham et al. ([2019a](#bib.bib113)) | M | Two-stream | 2DConv
    | High-order CRF+ real-time reconstruction pipeline |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) | S | AlexNet | Firemodules |
    End-to-end pipeline SqueezeSeg + real time |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) | S | AlexNet | Firemodules
    | Quite light-weight framework PointSeg + real time |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) | S | AlexNet | Firemodules |
    Robust framework SqueezeSegV2 |'
  prefs: []
  type: TYPE_TB
- en: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) | S | DarkNet | Residual
    block | GPU-accelerated post-processing + RangNet++ |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. Xu et al. ([2020](#bib.bib172)) | S | RangeNet | SAC | Adopting
    different filters for different locations |'
  prefs: []
  type: TYPE_TB
- en: '| Ando et al. Ando et al. ([2023](#bib.bib1)) | S | U-Net | ViTs | Decreasing
    the gaps between image and point domain. |'
  prefs: []
  type: TYPE_TB
- en: '| Kong et al. Kong et al. ([2023](#bib.bib70)) | S | U-Net | ViTs | Introducing
    a scalable training from range view strategy |'
  prefs: []
  type: TYPE_TB
- en: '| voxel | Huang et al. Huang and You ([2016](#bib.bib53)) | V | 3D CNN | 3DConv
    | Efficiently handling large data |'
  prefs: []
  type: TYPE_TB
- en: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | V | 3D FCNN | 3DConv
    | Combining 3D FCNN with fine-represen. |'
  prefs: []
  type: TYPE_TB
- en: '| Meng et al. Meng et al. ([2019](#bib.bib103)) | V | VAE | RBF | A novel voxel-based
    representation + RBF |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. Liu et al. ([2017](#bib.bib91)) | V | 3D CNN/DQN/RNN | 3DConv
    | Integrating three vision tasks into one frame. |'
  prefs: []
  type: TYPE_TB
- en: '| Rethage et at. Rethage et al. ([2018](#bib.bib121)) | V | 3D FCNN | FPConv
    | First fully-convolutional network on raw point sets |'
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. Dai et al. ([2018](#bib.bib20)) | V | 3D FCNN | 3DConv | Combing
    scene completion and semantic labeling |'
  prefs: []
  type: TYPE_TB
- en: '| Riegler et al. Riegler et al. ([2017](#bib.bib122)) | V | Octree | 3DConv
    | Making DL with high-resolution voxels |'
  prefs: []
  type: TYPE_TB
- en: '| Graham et al. Graham et al. ([2018](#bib.bib32)) | V | FCN/U-Net | SSConv
    | SSConv with less computation |'
  prefs: []
  type: TYPE_TB
- en: '| others | TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | T | U-Net
    | TConv | Tangent convolution + Parsing large scenes |'
  prefs: []
  type: TYPE_TB
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) | L | DeepLab | BConv | Hierarchical
    and spatially-aware feature learning |'
  prefs: []
  type: TYPE_TB
- en: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) | L | U-Net | PN+3DConv | Hybrid
    architecture + novel slicing operator |'
  prefs: []
  type: TYPE_TB
- en: '| 3DMV Dai and Nießner ([2018](#bib.bib19)) | M+V | Cascade frame. | ENet+3DConv
    | Inferring 3D semantics from both 3D and 2D input |'
  prefs: []
  type: TYPE_TB
- en: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) | V+M+P | Parallel frame.
    | SSCNet/DeepLab/PN | Leveraging 2D and 3D features |'
  prefs: []
  type: TYPE_TB
- en: '| PVCNN Liu et al. ([2019b](#bib.bib97)) | V+P | POintNet | PVConv | Both memory
    and computation efficient |'
  prefs: []
  type: TYPE_TB
- en: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) | M+P | Cascade frame. | U-Net+PointNet++
    | Leveraging 2D and 3D features |'
  prefs: []
  type: TYPE_TB
- en: '| LaserNet++ Meyer et al. ([2019](#bib.bib104)) | M+P | Cascade frame. | ResNet+LNet
    | Unified network for two tasks |'
  prefs: []
  type: TYPE_TB
- en: '| BPNet Hu et al. ([2021](#bib.bib50)) | M+P | Cascade frame. | 2/3DUNet |
    Bidirection projection module |'
  prefs: []
  type: TYPE_TB
- en: 3.3 Voxel Based Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to pixels, voxels divide the 3D space into many volumetric grids with
    a specific size and discrete coordinates. It contains more geometric information
    of the scene compared to projected images. 3D ShapeNets Wu et al. ([2015](#bib.bib165))
    and VoxNet Maturana and Scherer ([2015](#bib.bib101)) take volumetric occupancy
    grid representation as input to a 3D convolutional neural network for object recognition,
    which guides 3D semantic segmentation based on voxels. Voxel based semantic segmentation
    methods are summarized in Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based
    Segmentation ‣ 3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3D CNN is a common architecture used to process uniform voxels for label prediction.
    Huang et al. Huang and You ([2016](#bib.bib53)) presented a 3D FCN for coarse
    voxel level predictions. Their method is limited by spatial inconsistency between
    predictions and provide a coarse labeling. Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138))
    introduce a novel network SEGCloud to produce fine-grained predictions. It up
    samples the coarse voxel-wise prediction obtained from a 3D FCN to the original
    3D point space resolution by trilinear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: With fixed resolution voxels, the computational complexity grows linearly with
    the increase of the scene scale. Large voxels can lower the computational cost
    of large-scale scene parsing. Liu et al. Liu et al. ([2017](#bib.bib91)) introduced
    a novel network called 3D CNN-DQN-RNN. Like the sliding windows in 2D semantic
    segmentation, this network proposes eye window that traverses the whole data for
    fast localizing and segmenting class objects under the control of 3D CNN and deep
    Q-Network (DQN). The 3D CNN and Residual RNN further refine features in the eye
    window. The pipeline learns key features of interesting regions efficiently to
    enhance the accuracy of large-scale scene parsing with less computational cost.
    Rethage et at. Rethage et al. ([2018](#bib.bib121)) present a novel fully convolutional
    point network (FCPN), sensitive to multi-scale input , to parse large-scale scene
    without pro- or post-process steps. Particularly, FCPN is able to learn memory
    efficient representations that scale well to larger volumes. Similarly, Dai et
    al. Dai et al. ([2018](#bib.bib20)) design a novel 3D CNN to train on scene subvolumes
    but deploy on arbitrarily large scenes at test time, as it is able to handle large
    scenes with varying spatial extent. Additionally, their network adopts a coarse-to-fine
    tactic to predict multiple resolution scenes to handle the resolution growth in
    data size as the scene increases in size. Traditionally, the voxel representation
    only comprises Boolean occupancy information which loses much geometric informatin.
    Meng et al. Meng et al. ([2019](#bib.bib103)) develop a novel information-rich
    voxel representation by using a variational auto-encoder(VAE) taking radial basis
    function(RBF) to capture the distribution of points within each voxel. Further,
    they proposed a group equivariant convolution to exploit feature.
  prefs: []
  type: TYPE_NORMAL
- en: In fixed scale scenes, the computational complexity grows cubically as the voxel
    resolution increases. However, the volumetric representation is naturally sparse,
    resulting in unnecessary computations when applying 3D dense convolution on the
    sparse data. To aleviate this problem, OctNet Riegler et al. ([2017](#bib.bib122))
    divides the space hierarchically into nonniform voxels using a series of unbalanced
    octrees. Tree structure allows memory allocation and computation to focus on relevant
    dense voxels without sacrificing resolution. However, empty space still imposes
    computational and memory burden in OctNet. In contrast, Graham et al. Graham et al.
    ([2018](#bib.bib32)) proposed a novel submanifold sparse convolution (SSC) that
    does not perform computations in empty regions, making up for the drawback of
    OctNet.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Point Based Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Point clouds are scattered irregularly in 3D space, lacking any canonical order
    and translation invariance, which restricts the use of conventional 2D/3D convolutional
    neural networks. Recently, a series of point-based semantic segmentation networks
    have been proposed. These methods can be roughly subdivided into four categories:
    MLP based, point convolution based, graph convolution based and Transformer based.
    These methods are summarized in Table [4](#S3.T4 "Table 4 ‣ 3.4.4 Transformer
    Based ‣ 3.4 Point Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 MLP Based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These methods apply a Multi Layer Perceptron directly on the points to learn
    features. The PointNet Qi et al. ([2017a](#bib.bib115)) is a pioneering work that
    directly processes point clouds. It uses shared MLP to exploit points-wise features
    and adopts a symmetric function such as max-pooling to collect these features
    into a global feature representation. Because the max-pooling layer only captures
    the maximum activation across global points, PointNet cannot learn to exploit
    local features. Building on PointNet, PointNet++ Qi et al. ([2017b](#bib.bib116))
    defines a hierarchical learning architecture. It hierarchically samples points
    using farthest point sampling (FPS) and groups local regions using k nearest neighbor
    search as well as ball search. Progressively, a simplified PointNet exploits features
    in local regions at multiple scales or multiple resolutions. Similarly, Engelmann
    et al. Engelmann et al. ([2018](#bib.bib26)) define local regions by KNN clustering
    and K-means clustering and use a simplified PointNet to extract local features.
  prefs: []
  type: TYPE_NORMAL
- en: To learn the short and long-range dependencies, some works introduce the Recurrent
    Neural Networks (RNN) to MLP-based methods. For example, ESC Engelmann et al.
    ([2017](#bib.bib24)) divides global points into multi-scale/grid blocks. The concatenated
    (local) block features are appended to the point-wise features and passed through
    Recurrent Consolidation Units (RCUs) to further learn global context features.
    Similarly, HRNN Ye et al. ([2018](#bib.bib180)) uses Pointwise Pyramid Pooling
    (3P) to extract local features on the multi-size local regions. Point-wise features
    and local features are concatenated and a two-direction hierarchical RNN explores
    context features on these concatenated features. However, the local features learned
    are not sufficient because the deeper layer features do not cover a larger spatial
    extent.
  prefs: []
  type: TYPE_NORMAL
- en: Another technology, some works integrate the hand-craft point representation
    into PointNet or PointNet++ network to enhance the point representation ability
    with less learnable netowrk parameters. Inspired by SIFT representation Lowe ([2004](#bib.bib98)),
    PointSIFT Jiang et al. ([2018](#bib.bib64)) inserts a PointSIFT module layer learn
    local shape information. This module transforms each point into a new shape representation
    by encoding information of different orientations. PointWeb Zhao et al. ([2019a](#bib.bib193))
    propose a adaptive feature adjustment (AFA) module to learning the interactive
    information between local points to enhance the point representation. Similarly,
    RepSurf Ran et al. ([2022](#bib.bib120)) introduces two novel point representations,
    namely triangular and umbrella representative surfaces, to establish connections
    and enhance the representation capability of learned point-wise features. This
    approach effectively improves feature representation with fewer learnable network
    parameters, drawing significant attention from the research community. In contrast
    to the aforementioned methods, PointNeXt Qian et al. ([2022](#bib.bib118)) takes
    a different approach by revisiting the classical PointNet++ architecture through
    a systematic study of model training and scaling strategies. It proposes a set
    of improved training strategies that lead to a significant performance boost for
    PointNet++. Additionally, PointNeXt introduces an inverted residual bottleneck
    design and employs separable MLPs to enable efficient and effective model scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Point Convolution Based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point convolution based methods perform convolution operations directly on the
    points. Different from 2D convolution , the weight function of point convolution
    need learn from point geometric information adaptively. Early convolution networks
    focus on the convolution weight function design. For example, RSNet Huang et al.
    ([2018](#bib.bib54)) exploit point-wise features using 1x1 convolution and then
    pass them through the local dependency module (LDM) to exploit local context features.
    However, it does not define the neighborhood for each point in order to learn
    local features. On the other hand, PointwiseCNN  Hua et al. ([2018](#bib.bib52))
    sorts points in a specific order, e.g. XYZ coordinate or Morton cureve Morton
    ([1966](#bib.bib106)), and queries nearest neighbors dynamically and bins them
    into 3x3x3 kernel cells before convolving with the same kernel weights.
  prefs: []
  type: TYPE_NORMAL
- en: Gradually, some point convolution works approximate the convolution weight function
    as MLP to learn weights from point coordinates. PCCN  Wang et al. ([2018c](#bib.bib149))
    performs Parametric CNN, where the kernel is estimated as an MLP, on KD-tree neighborhood
    to learn local features. PointCNN  Li et al. ([2018b](#bib.bib82)) coarsens the
    input points with farthest point sampling. The convolution layer learns an $\chi$-transformation
    from local points by MLP to simultaneously weight and permute the features, subsequently
    applying a standard convolution on these transformed features.
  prefs: []
  type: TYPE_NORMAL
- en: Some works associates a coefficient (derived from point coordinates) with the
    weight function to adjust the learned convolutional weights. An extension of Monte
    Carlo approximation for convolution called PointConv  Wu et al. ([2019b](#bib.bib161))
    takes the point density into account. It uses MLP to approximate a weight function
    of the convolution kernel, and applies an inverse density scale to reweight the
    learned weight function. Similarly, MCC  Hermosilla et al. ([2018](#bib.bib46))
    phrases convolution as a Monte Carlo integration problem by relying on point probability
    density function (PDF), where the convolution kernel is also represented by an
    MLP. Moreover, it introduces Possion Disk Sampling (PDS) Wei ([2008](#bib.bib157))
    to construct a point hierarchy instead of FPS, which provides an opportunity to
    get the maximal number of samples in a receptive field.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of works use other function instead of MLP to approximate the convolution
    weight function. Flex-Convolution  Groh et al. ([2018](#bib.bib33)) uses a linear
    function with fewer parameters to model a convolution kernel and adapts inverse
    density importance sub-sampling (IDISS) to coarsen the points. KPConv Thomas et al.
    ([2019](#bib.bib139)) and KCNet Shen et al. ([2018](#bib.bib126)) fixed the convolution
    kernel for robustness to varying point density. These networks predefine the kernel
    points on local region and learn convolutional weights on the kernel points from
    their geometric connections to local points using linear and Gaussian correlation
    functions, respectively. Here, the number and position of kernel points need be
    optimized for different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Point convolution on limited local receptive field could not exploit long-range
    features. Therefore, some works introduce the dilated mechanism into point convolution.
    Dilated point convolution(DPC)  Engelmann et al. ([2020b](#bib.bib25)) adapts
    standard point convolution on neighborhood points of each point where the neighborhood
    points are determined though a dilated KNN search. similarly, A-CNN Komarichev
    et al. ([2019](#bib.bib69)) defines a new local ring-shaped region by dilated
    KNN, and projects points on a tangent plane to further order neighbor points in
    local regions. Then, the standard point convolutions are performed on these ordered
    neighbors represented as a closed loop array.
  prefs: []
  type: TYPE_NORMAL
- en: In the large-scale point clouds semantic segmentation area, RandLA-Net  Hu et al.
    ([2020](#bib.bib49)) uses random point sampling instead of the more complex point
    selection approach. It introduces a novel local feature aggregation module (LFAM)
    to progressively increase the receptive field and effectively preserve geometric
    details. Another technology, PolarNet  Zhang et al. ([2020](#bib.bib190)) first
    partitions a large point cloud into smaller grids (local regions) along their
    polar bird’s-eye-view (BEV) coordinates. It then abstracts local region points
    into a fixed-length representation by a simplified PointNet and these representations
    are passed through a standard convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Graph Convolution Based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The graph convolution based methods perform convolution on points connected
    with a graph structure, where the graph help the feature aggregation exploit the
    structure information between points. the graphs can be divided into spectral
    graph and spatial graph. In the spectral graph, LS-GCN  Wang et al. ([2018a](#bib.bib143))
    adopts the basic architecture of PointNet++, replaces MLPs with a spectral graph
    convolution using standard unparametrized Fourier kernels, as well as a novel
    recursive spectral cluster pooling substitute for max-pooling. However, transformation
    from spatial to spectral domain incurs a high computational cost. Besides that,
    spectral graph networks are usually defined on a fixed graph structure and are
    thus unable to directly process data with varying graph structures.
  prefs: []
  type: TYPE_NORMAL
- en: In the spatial graph category, ECC  Simonovsky and Komodakis ([2017](#bib.bib131))
    is among of the pioneer methods to apply spatial graph network to extract features
    from point clouds. It dynamically generates edge-conditioned filters to learn
    edge features that describe the relationships between a point and its neighbors.
    Based on PointNet architecture, DGCNN  Wang et al. ([2019b](#bib.bib155)) implements
    dynamic edge convolution called EdgeConv on the neighborhood of each point. The
    convolution is approximated by a simplified PointNet. SPG  Landrieu and Simonovsky
    ([2018](#bib.bib74)) parts the point clouds into a number of simple geometrical
    shapes (termed super-points) and builts super graph on global super-points. Furthermore,
    this network adopts PointNet to embed these points and refine the embedding by
    Gated Recurrent Unit (GRU). Based on the basic architecture of PoinNet++, Li et
    al.  Li et al. ([2019b](#bib.bib83)) proposed Geometric Graph Convolution (TGCov),
    its filters defined as products of local point-wise features with local geometric
    connection features expressed by Gaussian weighted Taylor kernels. Feng et al. 
    Feng et al. ([2020](#bib.bib29)) constructed a local graph on neighborhood points
    searched along multi-directions and explore local features by a local attention-edge
    convolution (LAE-Conv). These features are imported into a point-wise spatial
    attention module to capture accurate and robust local geometric details. Lei et
    al. designs a fuzzy coefficient to times weight function, to enable the convolution
    weights robust.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous graph convolution also incurs a high computational cost and generally
    suffer from the vanishing gradient problem. Inspired by the separable convolution
    strategy in Xception  Chollet ([2017](#bib.bib15)) that significantly reduces
    parameters and computation burden, HDGCN  Liang et al. ([2019a](#bib.bib87)) designed
    a DGConv that composes depth-wise graph convolution followed by a point-wise convolution,
    and add DGConv into the hierarchical structure to extract local and global features.
    DeepGCNs  Li et al. ([2019a](#bib.bib80)) borrows some concepts from 2D CNN such
    as residual connections between different layers (ResNet) to alleviate the vanishing
    gradient problem, and dilation mechanism to allow the GCN to go deeper. Lei et
    al.  Lei et al. ([2020](#bib.bib78)) propose a discrete spherical convolution
    kernel (SPH3D kernel) that consists of the spherical convolution learning depth-wise
    features and point-wise convolution learning point-wise features.
  prefs: []
  type: TYPE_NORMAL
- en: Tree structures such as KD-tree and Octree can be viewed as a special type of
    graph, allowing to share convolution layers depending on the tree splitting orientation.
    3DContextNet  Zeng and Gevers ([2018](#bib.bib188)) adopts a KD-tree structure
    to hierarchically represent points where the nodes of different tree layers represent
    local regions at different scales, and employs a simplified PointNet with a gating
    function on nodes to explore local features. However, their performance depends
    heavily on the randomization of the tree construction. Lei et al.  Lei et al.
    ([2019](#bib.bib77)) built an Octree based hierarchical structure on global points
    to guide the spherical convolution computation in per layer of the network. The
    spherical convolution kernel systematically partitions a 3D spherical region into
    multiple bins that specifies learnable parameters to weight the points falling
    within the corresponding bin.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Transformer Based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention mechanism has recently become popular for improving point cloud segmentation
    accuracy. Compared to point convolution, Transformer introduces the point features
    into the weight learning. For example, Ma et al.  Ma et al. ([2020](#bib.bib100))
    use the channel self-attention mechanism to learn independence between any two
    point-wise feature channels, and further define a Channel Graph where the channel
    maps are presented as nodes and the independencies are represented as graph edges.
    AGCN  Xie et al. ([2020b](#bib.bib171)) integrates attention mechanism with GCN
    for analyzing the relationships between local features of points and introduces
    a global point graph to compensate for the relative information of individual
    points. PointANSL Yan et al. ([2020](#bib.bib176)) use the general self-attention
    mechanism for group feature updating, and propose a adaptive sampling (AS) module
    to overcome the issues of FPS.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer model, which employs self-attention as a fundamental component,
    includes position encoding to capture the sequential order of input tokens. Position
    encoding is crucial to ensure that the model understands the relative positions
    of tokens within a sequence. Point Transformer Zhao et al. ([2021](#bib.bib194))
    introduce MLP-based position encoding into vector attention, and use a KNN-based
    downsampling module to decrease the point resolution. Follow up work, Point Transformer
    v2 Wu et al. ([2022a](#bib.bib162)) strengthens the position encoding mechanism
    by applying an additional encoding multiplier to the relation vector, and designs
    a partition-based pooling strategy to align the geometric information.
  prefs: []
  type: TYPE_NORMAL
- en: Point Transformers are generally computationally expensive because the original
    self-attention module needs to generate a huge attention map. To address this
    problem, PatchFormer Zhang et al. ([2022](#bib.bib189)) calculates the attention
    map via low-rank approximation. Similarly, FastPointTransformer Park et al. ([2022](#bib.bib111))
    introduces a lightweight local self-attention module that learns continuous positional
    information while reducing the space complexity. Inspired by the success of window-based
    Transformer in the 2D domain, Stratified Transformer Lai et al. ([2022](#bib.bib73))
    designs a cubic window and samples distant points as keys, but in a sparser way,
    to expand the receptive field. Similarly, SphereFormer Lai et al. ([2023](#bib.bib72))
    designs radial window self-attention that partitions that space into several non-overlapping
    narrow and long windows for exploiting long-range dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of point based semantic segmentation methods with deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Neighb. search | Feature abstraction | Coarsening | Contribution
    |'
  prefs: []
  type: TYPE_TB
- en: '| MLP | PointNet Qi et al. ([2017a](#bib.bib115)) | None | MLP | None | Pioneering
    processing points directly |'
  prefs: []
  type: TYPE_TB
- en: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) | None | MLP | None | Two local
    definition+local/global pathway |'
  prefs: []
  type: TYPE_TB
- en: '| ESC Engelmann et al. ([2017](#bib.bib24)) | None | MLP | None | MC/Grid Block
    for local defini.+RCUs for context exploit. |'
  prefs: []
  type: TYPE_TB
- en: '| HRNN Ye et al. ([2018](#bib.bib180)) | None | MLP | None | 3P for local feature
    exploit..+HRNN for local context exploit. |'
  prefs: []
  type: TYPE_TB
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) | Ball/KNN | PointNet | FPS |
    Proposing hierarchical learning framework |'
  prefs: []
  type: TYPE_TB
- en: '| PointSIFT Jiang et al. ([2018](#bib.bib64)) | KNN | PointNet | FPS | PointSIFT
    module for local shape information |'
  prefs: []
  type: TYPE_TB
- en: '| PointWeb Zhao et al. ([2019a](#bib.bib193)) | KNN | PointNet | FPS | AFA
    for interactive feature exploitation |'
  prefs: []
  type: TYPE_TB
- en: '| Repsurf Ran et al. ([2022](#bib.bib120)) | KNN | PointNet | FPS | Local triangular
    orientation + local umbrella orientation |'
  prefs: []
  type: TYPE_TB
- en: '| PointNeXt Qian et al. ([2022](#bib.bib118)) | KNN | InvResMLP | FPS | Next
    version of PointNet |'
  prefs: []
  type: TYPE_TB
- en: '| Point Convolution | RSNet Huang et al. ([2018](#bib.bib54)) | None | 1x1
    Conv | None | LDM for local context exploitation |'
  prefs: []
  type: TYPE_TB
- en: '| DPC Engelmann et al. ([2020b](#bib.bib25)) | DKNN | PointConv | None | Dilated
    KNN for expanding the receptive field |'
  prefs: []
  type: TYPE_TB
- en: '| PointWiseCNN Hua et al. ([2018](#bib.bib52)) | Grid | PWConv. | None | Novel
    point convolution |'
  prefs: []
  type: TYPE_TB
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) | KD index | PCConv. | None | KD-tree
    index for neigh. search+novel point Conv. |'
  prefs: []
  type: TYPE_TB
- en: '| KPConv Thomas et al. ([2019](#bib.bib139)) | Ball | KPConv. | Grid sampling
    | Novel point convolution |'
  prefs: []
  type: TYPE_TB
- en: '| FlexConv Groh et al. ([2018](#bib.bib33)) | KD index | flexConv. | IDISS
    | Novel point Conv.+flex-maxpooling without subsampling |'
  prefs: []
  type: TYPE_TB
- en: '| PointCNN Li et al. ([2018b](#bib.bib82)) | DKNN | $\chi$-Conv | FPS | Novel
    point convolution |'
  prefs: []
  type: TYPE_TB
- en: '| MCC Hermosilla et al. ([2018](#bib.bib46)) | Ball | MCConv. | PDS | Novel
    coarsening layer+point convolution |'
  prefs: []
  type: TYPE_TB
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) | KNN | PointConv | FPS | Novel
    point convolution considering point density |'
  prefs: []
  type: TYPE_TB
- en: '| A-CNN Komarichev et al. ([2019](#bib.bib69)) | DKNN | AConv | FPS | Novel
    neighborhood search+point convolution |'
  prefs: []
  type: TYPE_TB
- en: '| RandLA-Net Hu et al. ([2020](#bib.bib49)) | KNN | LocSE | RPS | LFAM with
    large receptive field and keeping geometric details |'
  prefs: []
  type: TYPE_TB
- en: '| PolarNet Zhang et al. ([2020](#bib.bib190)) | None | PointNet | PolarGrid
    | Novel local regions definition + RingConv |'
  prefs: []
  type: TYPE_TB
- en: '| Graph Convolution | DGCNN Wang et al. ([2019b](#bib.bib155)) | KNN | EdgeConv
    | None | Novel graph convolution + updating graph |'
  prefs: []
  type: TYPE_TB
- en: '| SPG Landrieu and Simonovsky ([2018](#bib.bib74)) | partition | PointNet |
    None | Superpoint graph + parsing large-scale scene |'
  prefs: []
  type: TYPE_TB
- en: '| DeepGCNs Li et al. ([2019a](#bib.bib80)) | DKNN | DGConv | RPS | Adapting
    residual connections between layers |'
  prefs: []
  type: TYPE_TB
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) | Ball | SPH3D-GConv | FPS | Novel
    graph convolution + pooling + uppooling |'
  prefs: []
  type: TYPE_TB
- en: '| LS-GCN Wang et al. ([2018a](#bib.bib143)) | KNN | Spec.Conv. | FPS | Local
    spectral graph + Novel graph convolution |'
  prefs: []
  type: TYPE_TB
- en: '| PAN Feng et al. ([2020](#bib.bib29)) | Multi-direct. | LAE-Conv | PFS | Point-wise
    spatial attention+local graph Conv. |'
  prefs: []
  type: TYPE_TB
- en: '| TGNet Li et al. ([2019b](#bib.bib83)) | Ball | TGConv | PFS | Novel graph
    Conv.+multi-scale features explo. |'
  prefs: []
  type: TYPE_TB
- en: '| HDGCN Liang et al. ([2019a](#bib.bib87)) | KNN | DGConv | FPS | Depthwise
    graph Conv. + Pointwise Conv. |'
  prefs: []
  type: TYPE_TB
- en: '| 3DCon.Net Zeng and Gevers ([2018](#bib.bib188)) | KNN | PointNet | Tree layer
    | KD tree structure |'
  prefs: []
  type: TYPE_TB
- en: '| $\psi$-CNN Lei et al. ([2019](#bib.bib77)) | Octree neig. | $\psi$-Conv |
    Tree layer | Octree structure+ Novel graph convolution |'
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer | PGCRNet Ma et al. ([2020](#bib.bib100)) | None | Conv1D
    | None | PointGCR to model context dependencies |'
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) | KNN | MLP | None | Point attention
    layer for aggregating local features |'
  prefs: []
  type: TYPE_TB
- en: '| PointANSL Yan et al. ([2020](#bib.bib176)) | KNN | local-nonlocal module
    | AS | Local-nonlocal module + adaptive sampling |'
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer Zhao et al. ([2021](#bib.bib194)) | KNN | Point Transformer
    | Pooling | MLP-based relative position encoding + vector attention |'
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer v2 Wu et al. ([2022a](#bib.bib162)) | Grid partition |
    PointTransformerv2 | pooling | Novel position encoding + grid Pooling |'
  prefs: []
  type: TYPE_TB
- en: '| PatchFormer Zhang et al. ([2022](#bib.bib189)) | Boxes partition | Patch
    Transformer | DWConv | First linear attention + Lightweight multi-scale Transformer
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fast Point Transformer Park et al. ([2022](#bib.bib111)) | Voxel partition
    | Fast point Transformer | Voxel-based sampl. | Lightweight local self-attention
    + novel position encoding |'
  prefs: []
  type: TYPE_TB
- en: '| Stratified Transformer Lai et al. ([2022](#bib.bib73)) | Voxel partition
    | Stratified Transformer | PFS | Contextual relative position encoding |'
  prefs: []
  type: TYPE_TB
- en: '| SphereFormer Lai et al. ([2023](#bib.bib72)) | Voxel partition | Sphereformer
    + cubicformer | Maxpooling | Novel spherical window for LIDAR points |'
  prefs: []
  type: TYPE_TB
- en: 3.5 Other Representation Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some methods transform the original point cloud to representations other than
    projected images, voxels and points. Examples of such representations include
    *tangent images* Tatarchenko et al. ([2018](#bib.bib137)) and *lattice* Su et al.
    ([2018](#bib.bib134)), Rosu et al. ([2019](#bib.bib124)). In the former case,
    Tatarchenko et al. Tatarchenko et al. ([2018](#bib.bib137)) project local surfaces
    around each-point to a series of 2D tangent images and develop a tangent convolution
    based U-Net to extract features. In the latter case, SPLATNet Su et al. ([2018](#bib.bib134))
    adapts the bilateral convolution layers (BCLs) proposed by Jampani et al. Jampani
    et al. ([2016](#bib.bib58)) to smoothly map disordered points onto a sparse lattice.
    Similarly, LatticeNet Rosu et al. ([2019](#bib.bib124)) uses a hybrid architecture
    that combines PointNet, which obtains low-level features, with sparse 3D convolution,
    which explores global context features. These features are embedded into a sparse
    lattice that allows the application of standard 2D convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the above methods have achieved significant progress in 3D semantic
    segmentation, each has its own drawbacks. For instance, multi-view images have
    more spectral information like color/intensity but less geometric information
    of the scene. On the other hand, voxels have more geometric information but less
    spectral information. To get the best of both worlds, some methods adopt *hybrid
    representations* as input to learn comprehensive features of a scene. Dai et al.
    Dai and Nießner ([2018](#bib.bib19)) map 2D semantic features obtained by multi-view
    networks into 3D grids of scene. These pipelines make 3D grids attach rich 2D
    semantic as well as 3D geometric information so that the scene can get better
    segmentation by a 3D CNN. Similarly, Hung et al. Chiang et al. ([2019](#bib.bib14))
    back-project 2D multi-view image features on to the 3D point cloud space and use
    a unified network to extract local details and global context from sub-volumes
    and the global scene respectively. Liu et al. Liu et al. ([2019b](#bib.bib97))
    argue that voxel-based and point-based NN are computationally inefficient in high-resolution
    and data structuring respectively. To overcome these challenges, they propose
    Point-Voxel CNN (PVCNN) that represents the 3D input data as point clouds to take
    advantage of the sparsity to lower the memory footprint, and leverage the voxel-based
    convolution to obtain a contiguous memory access pattern. Jaritz et al. Jaritz
    et al. ([2019](#bib.bib59)) proposed MVPNet that collect 2D multi-view dense image
    features into 3D sparse point clouds and then use a unified network to fuse the
    semantic and geometric features. Also, Meyer et al. Meyer et al. ([2019](#bib.bib104))
    fuse 2D image and point clouds to address 3D object detection and semantic segmentation
    by a unifying network. BPNet Hu et al. ([2021](#bib.bib50)) consists of 2D and
    3D sub-networks with symmetric architectures, connected through a bidirectional
    projection module (BPM). This allows the interaction of complementary information
    from both visual domains at multiple architectural levels, leading to improved
    scene recognition by leveraging the advantages of both 2D and 3D information.
    The other representations based semantic segmentation methods are summarized in
    Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected
    Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4 3D Instance Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3D instance segmentation methods additionally distinguish between different
    instances of the same class. Being a more informative task for scene understanding,
    3D instance segmentation is receiving increased interest from the research community.
    3D instance segmentation methods are roughly divided into two directions: *proposal-based*
    and *proposal-free*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac3e01783ced0e190ace2f88a82545f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of two different approaches for 3D instance segmentation.
    Top: proposal-based framework. Bottom: proposal-free framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Proposal Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Proposal-based methods first predict object proposals and then refine them
    to generate final instance masks (see Figure [7](#S4.F7 "Figure 7 ‣ 4 3D Instance
    Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey")), breaking down
    the task into two main challenges. Hence, from the proposal generation point of
    view, these methods can be grouped into *detection-based* and *detection-free*
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Detection-based methods sometimes define object proposals as a 3D bounding box
    regression problem. 3D-SIS Hou et al. ([2019](#bib.bib48)) incorporates high-resolution
    RGB images with voxels, based on the pose alignment of the 3D reconstruction,
    and jointly learns color and geometric features by a 3D detection backbone to
    predict 3D bounding box proposals. In these proposals, a 3D mask backbone predicts
    the final instance masks. Similarly, GPSN Yi et al. ([2019](#bib.bib183)) introduces
    a 3D object proposal network termed Generative Shape Proposal Network (GPSN) that
    reconstructs object shapes from shape noisy observations to enforce geometric
    understanding. GPSN is further embedded into a 3D instance segmentation network
    named Region-based PointNet (R-PointNet) to reject, receive and refine proposals.
    Training of these networks needs to be performed step-by-step and the object proposal
    refinement requires expensive suppression operation. To this end, Yang et al.
    Yang et al. ([2019](#bib.bib177)) introduced a novel end-to-end network named
    3D-BoNet to directly learn a fixed number of 3D bounding boxes without any rejection,
    and then estimate an instance mask in each bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: Detection-free methods include SGPN Wang et al. ([2018d](#bib.bib151)) which
    assumes that the points belonging to the same object instance should have very
    similar features. Hence, it learns a similarity matrix to predict proposals. The
    proposals are pruned by confidence scores of the points to generate highly credible
    instance proposals. However, this simple distance similarity metric learning is
    not informative and is unable to segment adjacent objects of the same class. To
    this end, 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) learns object proposals
    from sampled and grouped point features that vote for the same object center,
    and then consolidates the proposal features using a graph convolutional network
    enabling higher-level interactions between proposals which result in refined proposal
    features. AS-Net Jiang et al. ([2020a](#bib.bib61)) uses an assignment module
    to assign proposal candidates and then eliminates redundant candidates by a suppression
    network. SoftGroup Vu et al. ([2022](#bib.bib142)) proposes top-down refinement
    to refine the instance proposal. SSTNet Liang et al. ([2021](#bib.bib86)) proposes
    an end-to-end solution of Semantic Superpoint Tree Network (SSTNet) to generate
    object instance proposals from scene points. A key contribution in SSTNet is an
    intermediate semantic superpoint tree (SST), which is constructed based on the
    learned semantic features of superpoints. The tree is traversed and split at intermediate
    nodes to generate proposals of object instances.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Proposal Free
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Proposal-free methods learn feature embedding for each point and then apply
    clustering to obtain defintive 3D instance labels (see Figure [7](#S4.F7 "Figure
    7 ‣ 4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey"))
    breaking down the task into two main challenges. From the embedding learning point
    of a view, these methods can be roughly subdivided into four categories: *2D embedding
    based* *multi-tasks learning*, *clustering based*, and *dynamic convolution based*.'
  prefs: []
  type: TYPE_NORMAL
- en: 2D embedding based: An example of these methods is the 3D-BEVIS Elich et al.
    ([2019](#bib.bib21)) that learns 2D global instance embedding with a bird’s-eye-view
    of the full scene. It then propagates the learned embedding onto point clouds
    by DGCNN Wang et al. ([2019b](#bib.bib155)). Another example is PanopticFusion
    Narita et al. ([2019](#bib.bib108)) which predicts pixel-wise instance labels
    by 2D instance segmentation network Mask R-CNN He et al. ([2017a](#bib.bib42))
    for RGB frames and integrates the learned labels into 3D volumes.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-tasks learning: 3D semantic segmentation and 3D instance segmentation
    can influence each other. For example objects with different classes must be different
    instances, and objects with the same instance label must be the same class. Based
    on this, ASIS Wang et al. ([2019a](#bib.bib152)) designs an encoder-decoder network,
    termed ASIS, to learn semantic-aware instance embeddings for boosting the performance
    of the two tasks. Similarly, JSIS3D Pham et al. ([2019b](#bib.bib114)) uses a
    unified network namely MT-PNet to predict the semantic labels of points and embedding
    the points into high-dimensional feature vectors, and further propose a MV-CRF
    to jointly optimize object classes and instance labels. Similarly Liu et al. Liu
    and Furukawa ([2019](#bib.bib90)) and 3D-GEL Liang et al. ([2019b](#bib.bib88))
    adopt SSCN to generate semantic predictions and instance embeddings simultaneously,
    then use two GCNs to refine the instance labels. OccuSeg Han et al. ([2020](#bib.bib39))
    uses a multi-task learning network to produce both occupancy signal and spatial
    embedding. The occupancy signal represents the number of voxel occupied by per
    voxel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering based: methods like MASC Liu and Furukawa ([2019](#bib.bib90)) rely
    on high performance of the SSCN Graham et al. ([2018](#bib.bib32)) to predict
    the similarity embedding between neighboring points at multiple scales and semantic
    topology. A simple yet effective clustering Liu et al. ([2018c](#bib.bib96)) is
    adapted to segment points into instances based on the two types of learned embeddings.
    MTML Lahoud et al. ([2019](#bib.bib71)) learns two sets of feature embeddings,
    including the feature embedding unique to every instance and the direction embedding
    that orients the instance center, which provides a stronger grouping force. Similarly,
    PointGroup Jiang et al. ([2020b](#bib.bib63)) groups points into different clusters
    based on the original coordinate embedding space and the shifted coordinate embedding
    space. In addition, the proposed ScoreNet guides the proper cluster selection.
    The above methods usually group points according to point-level embeddings, without
    the instance-level corrections. HAIS Chen et al. ([2021](#bib.bib10)) introduce
    the set aggregation and intra-instance prediction to refine the instance at the
    object level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic convolution based: These methods overcome the limitations of clustering
    based methods by generating kernels and then using them to convolve with the point
    features to generate instance masks. Dyco3D He et al. ([2021](#bib.bib43)) adopts
    the clustering algorithm to generate a kernel for convolution. Similarly, PointInst3D
    He et al. ([2022](#bib.bib44)) uses FPS to generate kernels. DKNet Wu et al. ([2022b](#bib.bib163))
    introduces candidate mining and candidate aggregation to generate more instance
    kernels. Moreover, ISBNet Ngo et al. ([2023](#bib.bib110)) proposes a new instance
    encoder combining instance-aware PFS with a point aggregation layer to generate
    kernels to replace clustering in DyCo3D. 3D instance segmentation methods are
    summarized in Table [5](#S4.T5 "Table 5 ‣ 4.2 Proposal Free ‣ 4 3D Instance Segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of 3D instance segmentation methods with deep learning. M←multi-view
    image; Me←mesh;V←voxel; P←point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Input | Propo./Embed. Prediction | Refining/Grouping | Contribution
    |'
  prefs: []
  type: TYPE_TB
- en: '| proposal based | GSPN Yi et al. ([2019](#bib.bib183)) | P | GSPN | R-PointNet
    | New proposal generation methods |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | M+V | 3D-RPN+3D-RoI | 3DFCN | Learning
    bounding box on geometry and RGB |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | P | Bounding box regression
    | Point mask prediction | Directly regressing 3D bounding box |'
  prefs: []
  type: TYPE_TB
- en: '| SGPN Wang et al. ([2018d](#bib.bib151)) | P | SM + SCM + PN | Non-Maximum
    suppression | New group proposal |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | p | SSCNet | Graph ConvNet
    | Multi proposal aggregation strategy |'
  prefs: []
  type: TYPE_TB
- en: '| AS-Net Jiang et al. ([2020a](#bib.bib61)) | p | Four branches with MLPs |
    Candidate proposal suppression | Novel Algorithm mapping labels to candidates
    |'
  prefs: []
  type: TYPE_TB
- en: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | P | Soft-grouping module | top-down
    refinment | Novel clustering algorithm based on dual coordinate sets |'
  prefs: []
  type: TYPE_TB
- en: '| SSTNet Liang et al. ([2021](#bib.bib86)) | p | Tree traversal + splitting
    | CliqueNet | Constructing the superpoint tree for instance segmentation |'
  prefs: []
  type: TYPE_TB
- en: '| proposal free | 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | M | U-Net/FCN
    + 3D prop. | Mean-shift clustering | Joint 2D-3D feature |'
  prefs: []
  type: TYPE_TB
- en: '| PanopticFus Narita et al. ([2019](#bib.bib108)) | M | PSPNet/Mask R-CNN |
    FC-CRF | Coopering with semantic mapping |'
  prefs: []
  type: TYPE_TB
- en: '| ASIS Wang et al. ([2019a](#bib.bib152)) | P | 1 encoder+ 2 decoders | ASIS
    module | Simultaneously performing sem./ins. segmentation tasks |'
  prefs: []
  type: TYPE_TB
- en: '| JSIS3D Pham et al. ([2019b](#bib.bib114)) | P | MT-PNet | MV-CRF | Simultaneously
    performing sem./ins. segmentation tasks |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-GEL Liang et al. ([2019b](#bib.bib88)) | P | SSCNet | GCN | Structure-aware
    loss function + attention-based GCN |'
  prefs: []
  type: TYPE_TB
- en: '| OccuSeg Han et al. ([2020](#bib.bib39)) | P | 3D-UNet | Graph-based clustering
    | Proposing a novel occupancy signal |'
  prefs: []
  type: TYPE_TB
- en: '| MASC Liu and Furukawa ([2019](#bib.bib90)) | Me | U-Net with SSConv | Clustering
    algorithm | Novel clustering based on affinity and mesh topology |'
  prefs: []
  type: TYPE_TB
- en: '| MTML Lahoud et al. ([2019](#bib.bib71)) | V | SSCNet | Mean-shift clustering
    | Multi-task learning |'
  prefs: []
  type: TYPE_TB
- en: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | P | U-Net with SSConv | Point
    clustering + ScoreNet | Novel clustering algorithm based on dual coordinate sets
    |'
  prefs: []
  type: TYPE_TB
- en: '| HAIS Chen et al. ([2021](#bib.bib10)) | P | 3D U-Net | Set aggregation |
    Hierarchical aggregation for fine-grained predictions |'
  prefs: []
  type: TYPE_TB
- en: '| Dyco3D He et al. ([2021](#bib.bib43)) | P | 3D U-Net | Dynamic conv. | Generating
    kernel by clustering for convolution |'
  prefs: []
  type: TYPE_TB
- en: '| PointInst3D He et al. ([2022](#bib.bib44)) | P | 3D U-Net | MLP | Generating
    kernel by FPS |'
  prefs: []
  type: TYPE_TB
- en: '| DKNet Wu et al. ([2022b](#bib.bib163)) | P | 3D U-Net | MLP | Generating
    kernel by candidate mining and aggregation |'
  prefs: []
  type: TYPE_TB
- en: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | P | 3D U-Net | Box-aware dynamic
    conv | Generating kernel by instance aware FPS and point aggrega. |'
  prefs: []
  type: TYPE_TB
- en: 5 3D Part Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3D part segmentation is the next finer level, after instance segmentation,
    where the aim is to label different parts of an instance. The pipeline of part
    segmentation is quite similar to that of semantic segmentation except that the
    labels are now for individual parts. Therefore, some existing 3D semantic segmentation
    networks Meng et al. ([2019](#bib.bib103)), Graham et al. ([2018](#bib.bib32)),
    Qi et al. ([2017a](#bib.bib115)), Qi et al. ([2017b](#bib.bib116)), Zeng and Gevers
    ([2018](#bib.bib188)), Huang et al. ([2018](#bib.bib54)), Thomas et al. ([2019](#bib.bib139)),
    Hua et al. ([2018](#bib.bib52)), Hermosilla et al. ([2018](#bib.bib46)), Wu et al.
    ([2019b](#bib.bib161)), Li et al. ([2018b](#bib.bib82)), Wang et al. ([2019b](#bib.bib155)),
    Lei et al. ([2020](#bib.bib78)), Xie et al. ([2020b](#bib.bib171)), Wang et al.
    ([2018c](#bib.bib149)), Groh et al. ([2018](#bib.bib33)), Lei et al. ([2019](#bib.bib77)),
    Su et al. ([2018](#bib.bib134)), Rosu et al. ([2019](#bib.bib124)) can also be
    trained for part segmentation. However, these networks can not entirely tackle
    the difficulties of part segmentation. For example, various parts with the same
    semantic label might have diverse shapes, and the number of parts for an instance
    with the same semantic label may be different. We subdivide 3D part segmentation
    methods into two categories: *regular data* based and *irregular data* based as
    follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Regular Data Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular data usually includes projected images Kalogerakis et al. ([2017](#bib.bib65)),
    voxels Wang and Lu ([2019](#bib.bib156)), Le and Duan ([2018](#bib.bib76)), Song
    et al. ([2017](#bib.bib133)). As for projected images, Kalogerakis et al. Kalogerakis
    et al. ([2017](#bib.bib65)) obtain a set of images from multiple views that optimally
    cover object surface, and then use multi-view Fully Convolutional Networks(FCNs)
    and surface-based Conditional Random Fields (CRFs) to predict and refine part
    labels separately. Voxel is a useful representation of geometric data. However,
    fine-grained tasks like part segmentation require high resolution voxels with
    more detailed structure information, which leads to high computation cost. Wang
    et al. Wang and Lu ([2019](#bib.bib156)) proposed VoxSegNet to exploit more detailed
    information from voxels with limited resolution. They use spatial dense extraction
    to preserve the spatial resolution during the sub-sampling process and an attention
    feature aggregation (AFA) module to adaptively select scale features. Le et al.
    Le and Duan ([2018](#bib.bib76)) introduced a novel 3D CNN called PointGrid, to
    incorporate a constant number of points with each cell allowing the network to
    learn better local geometry shape details. Furthermore, multiple model fusion
    can enhance the segmentation performance. Combining the advantages of images and
    voxels, Song et al. Song et al. ([2017](#bib.bib133)) proposed a two-stream FCN,
    termed AppNet and GeoNet, to explore 2D appearance and 3D geometric features from
    2D images. In particular, their VolNet extracts 3D geometric features from 3D
    volumes guiding GeoNet to extract features from a single image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Summary of 3D part segmentation methods. M←multi-view image; Me←mesh;V←voxel;P←point
    clouds; reg.←regular data; irreg.←irregular data.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Input | Architecture | Feature extractor | Contribution
    |'
  prefs: []
  type: TYPE_TB
- en: '| regular | ShapePFCN Kalogerakis et al. ([2017](#bib.bib65)) | M | Multi-stream
    FCN | 2DConv | Per-label confidence maps + surface-based CRF |'
  prefs: []
  type: TYPE_TB
- en: '| VoxSegNet Wang and Lu ([2019](#bib.bib156)) | V | 3DU-Net | AtrousConv |
    SDE for preserving the spatial resolution AFA for feature selecting |'
  prefs: []
  type: TYPE_TB
- en: '| Pointgrid Le and Duan ([2018](#bib.bib76)) | V | Conv-deconv | 3DConv | Learning
    higher order local geometry shape. |'
  prefs: []
  type: TYPE_TB
- en: '| SubvolumeSup Song et al. ([2017](#bib.bib133)) | M+V | 2-stream FCN | 2D/3DConv
    | GeoNet/AppNet for 3/2D features exploi. + DCT for aligning. |'
  prefs: []
  type: TYPE_TB
- en: '| irregular | DCN Xu et al. ([2017](#bib.bib173)) | Me | 2-tream DCN & NN |
    DirectionalConv | DCN/NN for local feature and global feature. |'
  prefs: []
  type: TYPE_TB
- en: '| MeshCNN Hanocka et al. ([2019](#bib.bib40)) | Me | 2D CNN | MeshConv | Novel
    mesh convolution and pooling |'
  prefs: []
  type: TYPE_TB
- en: '| PartNet Yu et al. ([2019](#bib.bib185)) | P | RNN | PN | Part feature learning
    scheme for context and geometry feature exploitation |'
  prefs: []
  type: TYPE_TB
- en: '| SSCNN Yi et al. ([2017](#bib.bib182)) | P | FCN | SpectralConv | STN for
    allowing weight sharing + spectral multi-scale kernel |'
  prefs: []
  type: TYPE_TB
- en: '| KCNet Shen et al. ([2018](#bib.bib126)) | P | PN | MLP | KNN graph on points
    + kernel correlation for measuring geometric affinity |'
  prefs: []
  type: TYPE_TB
- en: '| SFCN Wang et al. ([2018b](#bib.bib146)) | P | FCN | SFConv | Novel point
    convolution |'
  prefs: []
  type: TYPE_TB
- en: '| SpiderCNN Xu et al. ([2018](#bib.bib175)) | P | PN | SpiderConv | Novel point
    convolution |'
  prefs: []
  type: TYPE_TB
- en: '| FeaStNet Verma et al. ([2018](#bib.bib141)) | P | U-Net | GConv | Dynamic
    graph convolution filters |'
  prefs: []
  type: TYPE_TB
- en: '| Kd-Net Klokov and Lempitsky ([2017](#bib.bib67)) | P | Kd-tree | Affine Transformation
    | Useing Kd-tree to build graphs and share learnable parameters |'
  prefs: []
  type: TYPE_TB
- en: '| O-CNN Wang et al. ([2017](#bib.bib148)) | P | Octree | 3DConv | Making 3D-CNN
    feasible for high-resolu. voxels |'
  prefs: []
  type: TYPE_TB
- en: '| PointCapsNet Zhao et al. ([2019b](#bib.bib195)) | P | Encoder-decoder | PN
    | Semi-supervision learning |'
  prefs: []
  type: TYPE_TB
- en: '| SO-Net Li et al. ([2018a](#bib.bib81)) | P | Encoder-decoder | FC layers
    | SOM for modeling spatial distribution + un-supervision learning |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Irregular Data Based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Irregular data representations usually includes meshes Xu et al. ([2017](#bib.bib173)),
    Hanocka et al. ([2019](#bib.bib40)) and point clouds Li et al. ([2018a](#bib.bib81)),
    Shen et al. ([2018](#bib.bib126)), Yi et al. ([2017](#bib.bib182)), Verma et al.
    ([2018](#bib.bib141)), Wang et al. ([2018b](#bib.bib146)), Yu et al. ([2019](#bib.bib185)),
    Zhao et al. ([2019b](#bib.bib195)) Yue et al. ([2022](#bib.bib187)). Mesh provides
    an efficient approximation to a 3D shape because it captures the flat, sharp and
    intricate of surface shape surface and topology. Xu et al. Xu et al. ([2017](#bib.bib173))
    put the face normal and face distance histogram as the input of a two-stream framework
    and use the CRF to optimize the final labels. Inspired by traditional CNN, Hanocka
    et al. Hanocka et al. ([2019](#bib.bib40)) design novel mesh convolution and pooling
    to operate on the mesh edges.
  prefs: []
  type: TYPE_NORMAL
- en: As for point clouds, the graph convolution is the most commonly used pipeline.
    In the spectral graph domain, SyncSpecCNN Yi et al. ([2017](#bib.bib182)) introduces
    a Sychronized Spectral CNN to process irregular data. Specially, multichannel
    convolution and parametrized dilated convolution kernels are proposed to solve
    multi-scale analysis and information sharing across shapes respectively. In spatial
    graph domain, in analogy to a convolution kernel for images, KCNet Shen et al.
    ([2018](#bib.bib126)) present point-set kernel and nearest-neighbor-graph to improve
    PointNet with an efficient local feature exploitation structure. Similarly, Wang
    et al. Wang et al. ([2018b](#bib.bib146)) design Shape Fully Convolutional Networks
    (SFCN) based on graph convolution and pooling operation, similar to FCN on images.
    SpiderCNN Xu et al. ([2018](#bib.bib175)) applies a special family of convolutional
    filters that combine simple step function with Taylor polynomial, making the filters
    to effectively capture intricate local geometric variations. Furthermore, FeastNet
    Verma et al. ([2018](#bib.bib141)) uses dynamic graph convolution operator to
    build relationships between filter weights and graph neighborhoods instead of
    relying on static graph of the above network.
  prefs: []
  type: TYPE_NORMAL
- en: A special kind of graphs, the trees (e.g. Kd-tree and Octree), work on 3D shapes
    with different representations and can support various CNN architectures. Kd-Net
    Klokov and Lempitsky ([2017](#bib.bib67)) uses a kd-tree data structure to represent
    point cloud connectivity. However, the networks have high computational cost.
    O-CNN Wang et al. ([2017](#bib.bib148)) designs an Octree data structure from
    3D shapes. However, the computational cost of the O-CNN grows quadratically as
    the depth of tree increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'SO-Net Li et al. ([2018a](#bib.bib81)) sets up a Self-Organization Map (SOM)
    from point clouds, and hierarchically learns node-wise features on this map using
    the PointNet architecture. However, it fails to fully exploit local features.
    PartNet Yu et al. ([2019](#bib.bib185)) decomposes 3D shapes in a top-down fashion,
    and proposes a Recursive Neural Network (RvNN) for learning the hierarchy of fine-grained
    parts. Zhao et al. Zhao et al. ([2019b](#bib.bib195)) introduce an encoder-decoder
    network, 3D-PointCapsNet, to tackle several common point cloud-related tasks.
    The dynamic routing scheme and the peculiar 2D latent space deployed by capsule
    networks, deployed in their model, bring improved performance. The 3D part segmentation
    methods are summarized in Table [6](#S5.T6 "Table 6 ‣ 5.1 Regular Data Based ‣
    5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Applications of 3D Segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We review 3D semantic segmentation methods for two main applications, unmanned
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Unmanned Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As LIDAR scanners and depth cameras become widely available and more affordable,
    they are increasingly being deployed in unmanned systems such as autonomous driving
    and mobile robots. These sensors provide realtime 3D video, generally at 30 frames
    per second (fps), as direct input to the system making *3D video semantic segmentation*
    as the primary task to understand the scene. Furthermore, in order to interact
    more effectively with the environment, unmanned systems generally build a *3D
    semantic map* of the scene. Below we review 3D video based semantic segmentation
    and 3D semantic map construction.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 3D video semantic segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to the 3D single frame/scan semantic segmentation methods reviewed
    in Section 3.1, 3D video (continuous frames/scans) semantic segmentation methods
    take into account the connecting spatio-temporal information between frames which
    is more powerful at parsing the scene robustly and continuously. Conventional
    convolutional neural networks (CNNs) are not designed to exploit the temporal
    information between frames. A common strategy is to adapt Recurrent Neural Networks
    or Spatio-temporal convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network based: RNNs generally work in combination with 2D CNNs
    to process RGB-D videos. The 2D CNN learns to extract the frame-wise spatial information
    and the RNN learns to extract the temporal information between the frames. Valipour
    et al. Valipour et al. ([2017](#bib.bib140)) proposed Recurrent Fully Neural Network
    to operate over a sliding window over the RGB-D video frames. Specifically, the
    convolutional gated recurrent unit preserves the spatial information and reduces
    the parameters. Similarly, Yurdakul et al. Emre Yurdakul and Yemez ([2017](#bib.bib22))
    combine fully convolutional and recurrent neural network to investigate the contribution
    of depth and temporal information separately in the synthetic RGB-D video.
  prefs: []
  type: TYPE_NORMAL
- en: Spatio-temporal convolution based: Nearby video frames provide diverse viewpoints
    and additional context of objects and scenes. STD2P He et al. ([2017b](#bib.bib45))
    uses a novel spatio-temporal pooling layer to aggregate region correspondences
    computed by optical flow and image boundary-based super-pixels. Choy et al. Choy
    et al. ([2019](#bib.bib16)) proposed 4D Spatio-Temporary ConvNet, to directly
    process a 3D point cloud video. To overcome challenges in the high-dimensional
    4D space (3D space and time), they introduced the 4D sptio-temporal convolution,
    a generalized sparse convolution, and the trilateral-stationary conditional random
    field that keeps spatio-temporal consistency. Similarly, based on 3D sparse convolution,
    Shi et al. Shi et al. ([2020](#bib.bib127)) proposed SpSequenceNet that contains
    two novel modules, a cross frame-global attention module and a cross-frame local
    interpolation module to exploit spatial and temporal feature in 4D point clouds.
    PointMotionNet Wang et al. ([2022](#bib.bib144)) proposes a spatio-temporal convolution
    that exploits a time-invariant spatial neighboring space and extracts spatio-temporal
    features, to distinguish the moving and static objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatio-temporal Transformer based : To capture the dynamics in point cloud
    video, point tracking is usually employed. However, P4Transformer Fan et al. ([2021](#bib.bib28))
    proposes a 4D convolution to embed the spatio-temporal local structures in point
    cloud video and further introduces a Transformer to leverage the motion information
    across the entire video by performing the self-attention on these embedded local
    features. Similarly, PST² Wei et al. ([2022](#bib.bib158)) performs spatio-temporal
    self attention across adjacent frames to capture the spatio-temporal context,
    and proposes a resolution embedding mudule to enhance the resolution of feature
    maps by aggregating features.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 3D semantic map construction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unmanned systems do not just need to avoid obstacles but also need to establish
    a deeper understanding of the scene such as object parsing, self localization
    etc. To facilitate such tasks, unmanned systems build a 3D semantic map of the
    scene which includes two key problems: geometric reconstruction and semantic segmentation.
    3D scene reconstruction has conventionally relied on simultaneous localization
    and mapping system (SLAM) to obtain a 3D map without semantic information. This
    is followed by 2D semantic segmentation with a 2D CNN and then the 2D labels are
    transferred to the 3D map following an optimization (e.g. conditional random field)
    to obtain a 3D semantic map Yang et al. ([2017](#bib.bib178)). This common pipeline
    does not guarantee high performance of 3D semantic maps in complex, large-scale,
    and dynamic scenes. Efforts have been made to enhance the robustness using association
    information exploitation from multiple frames, multi-model fusion and novel post-processing
    operations. These efforts are explained below.'
  prefs: []
  type: TYPE_NORMAL
- en: Association information exploitation: mainly depends on SLAM trajectory, recurrent
    neural networks or scene flow. Ma et al. Ma et al. ([2017](#bib.bib99)) enforce
    consistency by warping CNN feature maps from multi-views into a common reference
    view by using the SLAM trajectory and to supervise training at multiple scales.
    SemanticFusion McCormac et al. ([2017](#bib.bib102)) incorporates deconvolutional
    neural networks with a state-of-the-art dense SLAM system, ElasticFusion, which
    provides long-term correspondence between frames of a video. These correspondences
    allow label predictions from multi-views to be probabilistically fused into a
    map. Similarly, using the connection information between frames provided by a
    recurrent unit on RGB-D videos, Xiang et al. Xiang and Fox ([2017](#bib.bib168))
    proposed a data associated recurrent neural networks (DA-RNN) and integrated the
    output of the DA-RNN with KinnectFusion, which provides a consistent semantic
    labeling of the 3D scene. Cheng et al. Cheng et al. ([2020](#bib.bib12)) use a
    CRF-RNN-based semantic segmentation to generate the corresponding labels. Specifically,
    the authors proposed an optical flow-based method to deal with the dynamic factors
    for accurate localization. Kochanov et al. Kochanov et al. ([2016](#bib.bib68))
    also use scene flow to propagate dynamic objects within the 3D semantic maps.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple model fusion: Jeong et al. Jeong et al. ([2018](#bib.bib60)) build
    a 3D map by estimating odometry based on GPS and IMU, and use a 2D CNN for semantic
    segmentation. They integrate the 3D map with semantic labels using a coordinate
    transformation and Bayes’ update scheme. Zhao et al. Zhao et al. ([2018](#bib.bib192))
    use PixelNet and VoxelNet to exploit global context information and local shape
    information separately and then fuse the score maps with a softmax weighted fusion
    that adaptively learns the contribution of different data streams. The final dense
    3D semantic maps are generated with visual odometry and recursive Bayesian update.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below we summarize the quantitative results of the segmentation methods discussed
    in Sections [3](#S3 "3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey"), [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") and [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") on some typical public datasets, as well as analyze these results qualitatively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Evaluation performance regarding for RGB-D semantic segmentation methods
    on the SUN-RGB-D and NYUDv2\. Note that the ‘%’ after the value is omitted and
    the symbol ‘–’ means the results are unavailable.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | NYUDv2 | SUN-RGB-D |'
  prefs: []
  type: TYPE_TB
- en: '| mAcc | mIoU | mAcc | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| Guo and Chen ([2018](#bib.bib35)) | 46.3 | 34.8 | 45.7 | 33.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2015](#bib.bib147)) | – | 44.2 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Mousavian et al. ([2016](#bib.bib107)) | 52.3 | 39.2 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2018b](#bib.bib94)) | 50.8 | 39.8 | 50.0 | 39.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Gupta et al. ([2014](#bib.bib37)) | 35.1 | 28.6 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2018a](#bib.bib93)) | 51.7 | 41.2 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Hazirbas et al. ([2016](#bib.bib41)) | – | – | 48.3 | 37.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. ([2017](#bib.bib89)) | – | 47.7 | – | 48.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. ([2017](#bib.bib62)) | – | – | 50.6 | 39.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang and Neumann ([2018](#bib.bib150)) | 47.3 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Cheng et al. ([2017](#bib.bib13)) | 60.7 | 45.9 | 58.0 | – |'
  prefs: []
  type: TYPE_TB
- en: '| Fan et al. ([2017](#bib.bib27)) | 50.2 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2016](#bib.bib84)) | 49.4 | – | 48.1 | – |'
  prefs: []
  type: TYPE_TB
- en: '| Qi et al. ([2017c](#bib.bib117)) | 55.7 | 43.1 | 57.0 | 45.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2016](#bib.bib145)) | 60.6 | 38.3 | 50.1 | 33.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Evaluation performance regarding for projected images, voxel, point
    clouds and other representation semantic segmentation methods on the S3DIS, ScanNet,
    Semantic3D and SemanticKITTI. Note: the ‘%’ after the value is omitted, the symbol
    ‘–’ means the results are unavailable, the dotted line means the subdivision of
    methods according to the type of architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | S3DIS | ScanNet | Semantic3D | SemanticKITTI |'
  prefs: []
  type: TYPE_TB
- en: '|  | Area5 | 6-fold | test set | reduced-8 | only xyz |'
  prefs: []
  type: TYPE_TB
- en: '|  | mAcc | mIoU | mIoU | oAcc | mIoU | oAcc | mIoU | mAcc | mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| Lawin et al. Lawin et al. ([2017](#bib.bib75)) | projection | – | – | – |
    – | – | 88.9 | 58.5 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Boulch et al. Boulch et al. ([2017](#bib.bib6)) |  | – | – | – | – | – |
    91.0 | 67.4 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) |  | – | – | – | – | – | – | –
    | – | 37.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) |  | – | – | – | – | – | –
    | – | – | 39.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) |  | – | – | – | – | – | – | –
    | – | 44.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) |  | – | – | – | – |
    – | – | – | – | 52.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. Xu et al. ([2020](#bib.bib172)) |  | – | – | – | – | – | – | –
    | – | 55.9 |'
  prefs: []
  type: TYPE_TB
- en: '| RangViT Ando et al. ([2023](#bib.bib1)) |  | – | – | – | – | – | – | – |
    – | 55.9 |'
  prefs: []
  type: TYPE_TB
- en: '| RangFormer Kong et al. ([2023](#bib.bib70)) |  | – | – | – | – | – | – |
    – | – | 64.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | voxel | 57.35 | 48.92
    | 48.92 | – | – | 88.1 | 61.30 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Meng et al. Meng et al. ([2019](#bib.bib103)) |  | – | 78.22 | – | – | –
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. Liu et al. ([2017](#bib.bib91)) |  | – | 70.76 | – | – | – | –
    | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointNet Qi et al. ([2017a](#bib.bib115)) | point | 48.98 | 41.09 | 47.71
    | – | 14.69 | – | – | 29.9 | 17.9 |'
  prefs: []
  type: TYPE_TB
- en: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) |  | 59.10 | 52.17 | 58.27 |
    75.53 | – | – | – | 57.59 | 29.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ESC Engelmann et al. ([2017](#bib.bib24)) |  | 54.06 | 45.14 | 49.7 | 63.4
    | – | – | – | 40.9 | 26.4 |'
  prefs: []
  type: TYPE_TB
- en: '| HRNN Ye et al. ([2018](#bib.bib180)) |  | 71.3 | 53.4 | – | 76.5 | – | –
    | – | 49.2 | 34.5 |'
  prefs: []
  type: TYPE_TB
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) |  | – | 50.04 | 54.4 | 71.40
    | 34.26 | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointWeb Zhao et al. ([2019a](#bib.bib193)) |  | 66.64 | 60.28 | 66.7 | 85.9
    | – | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointSIFT Jiang et al. ([2018](#bib.bib64)) |  | – | 70.23 | 70.2 | – | 41.5
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Resurf Ran et al. ([2022](#bib.bib120)) |  | 76.0 | 68.9 | 74.3 | – | 70.0
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointNeXt Qian et al. ([2022](#bib.bib118)) |  | – | 70.5 | 74.9 | – | 71.2
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| RSNet Huang et al. ([2018](#bib.bib54)) |  | 59.42 | 56.5 | 56.47 | – | 39.35
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| DPC Engelmann et al. ([2020b](#bib.bib25)) |  | 68.38 | 61.28 | – | – | 59.2
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointwiseCNN Hua et al. ([2018](#bib.bib52)) |  | 56.5 | – | – | – | – |
    – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) |  | 67.01 | 58.27 | – | – | 49.8
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointCNN Li et al. ([2018b](#bib.bib82)) |  | 63.86 | 57.26 | 65.3 | 85.1
    | 45.8 | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| KPConv Thomas et al. ([2019](#bib.bib139)) |  | – | 67.1 | 70.6 | – | 66.6
    | 92.9 | 74.6 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) |  | – | 50.34 | – | – | 55.6
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| A-CNN Komarichev et al. ([2019](#bib.bib69)) |  | – | – | – | 85.4 | – |
    – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| RandLA-Net Hu et al. ([2020](#bib.bib49)) |  | – | – | 70.0 | – | – | 94.8
    | 77.4 | – | 53.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PolarNet Zhang et al. ([2020](#bib.bib190)) |  | – | – | – | – | – | – |
    – | – | 54.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DGCNN Wang et al. ([2019b](#bib.bib155)) |  | – | 56.1 | 56.1 | – | – | –
    | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| SPG Landrieu and Simonovsky ([2018](#bib.bib74)) |  | 66.50 | 58.04 | 62.1
    | – | – | 94.0 | 73.2 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) |  | 65.9 | 59.5 | 68.9 | – | 61.0
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| DeepGCNs Li et al. ([2019a](#bib.bib80)) |  | – | 60.0 | – | – | – | – |
    – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointGCRNet Ma et al. ([2020](#bib.bib100)) |  | – | 52.43 | – | – | 60.8
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) |  | – | – | 56.63 | – | – | – | –
    | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PAN Feng et al. ([2020](#bib.bib29)) |  | – | 66.3 | – | 86.7 | 42.1 | –
    | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| TGNet Li et al. ([2019b](#bib.bib83)) |  | – | 58.7 | – | 66.2 | – | – |
    – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| HDGCN Liang et al. ([2019a](#bib.bib87)) |  | 65.81 | 59.33 | 66.85 | – |
    – | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| 3DContextNet Zeng and Gevers ([2018](#bib.bib188)) |  | 74.5 | 55.6 | 55.6
    | – | – | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PGCRNet Ma et al. ([2020](#bib.bib100)) |  | – | 54.4 | – | – | – | – | 69.5
    | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) |  | 74.5 | 55.6 | 55.6 | – | – |
    – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PointANSL Yan et al. ([2020](#bib.bib176)) |  | – | 62.6 | 68.7 | – | – |
    66.6 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer Zhao et al. ([2021](#bib.bib194)) |  | 76.5 | 70.4 | 73.5
    | – | – | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer v2 Wu et al. ([2022a](#bib.bib162)) |  | 77.9 | 71.6 |
    – | – | 75.2 | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PatchFormer Zhang et al. ([2022](#bib.bib189)) |  | – | 68.1 | – | – | –
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Fast Point Transformer Park et al. ([2022](#bib.bib111)) |  | 77.3 | 70.1
    | – | – | – | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Stratify Transformer Lai et al. ([2022](#bib.bib73)) |  | 78.1 | 72.0 | –
    | – | 73.7 | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| SphereFormer Lai et al. ([2023](#bib.bib72)) |  | – | – | – | – | – | – |
    – | – | 78.4 |'
  prefs: []
  type: TYPE_TB
- en: '| TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | others | 62.2 | 52.8
    | – | 80.1 | 40.9 | 89.3 | 66.4 | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) |  | – | – | – | – | 39.3 | – |
    – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) |  | – | – | – | – | 64.0 |
    – | – | – | 52.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) |  | – | – | – | – | 63.4
    | – | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| PVCNN Liu et al. ([2019b](#bib.bib97)) |  | 87.12 | 58.98 | – | – | – | –
    | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) |  | – | – | – | – | 66.4 | – |
    – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| BPNet Hu et al. ([2021](#bib.bib50)) |  | – | – | – | – | 74.9 | – | – |
    – | – |'
  prefs: []
  type: TYPE_TB
- en: 7.1 Results for 3D Semantic Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We report the results of RGB-D based semantic segmentation methods on SUN-RGB-D
    Song et al. ([2015](#bib.bib132)) and NYUDv2 Silberman et al. ([2012](#bib.bib130))
    datasets using mAcc (mean Accuracy) and mIoU (mean Intersection over Union) as
    the evaluation metrics. These results of various methods are taken from the original
    papers and they are shown in Table [7](#S7.T7 "Table 7 ‣ 7 Experimental Results
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We report the results of projected images/voxel/point clouds/other representation
    semantic segmentation methods on S3DIS Armeni et al. ([2016](#bib.bib2)) (both
    Area 5 and 6-fold cross validation), ScanNet Dai et al. ([2017](#bib.bib18)) (test
    sets), Semantic3D Hackel et al. ([2017](#bib.bib38)) (reduced-8 subsets) and SemanticKITTI
    Behley et al. ([2019](#bib.bib3)) (only xyz without RGB). We use mAcc, oAcc (overall
    accuracy) and mIoU as the evaluation metrics. These results of various methods
    are taken from the original papers. Table [8](#S7.T8 "Table 8 ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey") reports the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectures of point cloud semantic segmentation typically focus on five
    main components: basic framework, neighborhood search, features abstraction, coarsening,
    and pre-processing. Below, we provide a more detailed discussion of each component.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic framework: Basic networks are one of the main driving forces behind the
    development of 3D segmentation. Generally, there are two main basic frameworks
    including PointNet and PointNet++. The PointNet framework utilizes shared Multi-Layer
    Perceptrons (MLPs) to capture point-wise features and employs max-pooling to aggregate
    these features into a global representation. However, it lacks the ability to
    learn local features due to the absence of a defined local neighborhood. Additionally,
    the fixed resolution of the feature map makes it challenging to adapt to deep
    architectures. In contrast, the PointNet++ framework introduces a novel hierarchical
    learning architecture. It defines local regions in a hierarchical manner and to
    progressively extract features from these regions. This approach enables the network
    to capture both local and global information, leading to improved performance.
    As a result, many current networks adopt the PointNet++ framework or similar variations
    (such as 3D U-Net). This framework significantly reduces computational and memory
    complexities, particularly in high-level tasks like semantic segmentation, instance
    segmentation, and detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neighborhood search: To exploit the local features of point clouds, the neighborhood
    point search is introduced into networks, including the K nearest neighbors (KNN)
    Zhao et al. ([2021](#bib.bib194)), Ran et al. ([2022](#bib.bib120)), Qian et al.
    ([2022](#bib.bib118)), Ball search Hermosilla et al. ([2018](#bib.bib46)), Thomas
    et al. ([2019](#bib.bib139)), Lei et al. ([2020](#bib.bib78)), grid-based search
    Hua et al. ([2018](#bib.bib52)), Wu et al. ([2022a](#bib.bib162)) and tree based
    search Lei et al. ([2019](#bib.bib77)). KNN search retrieves the K closest neighbors
    to a query point based on a distance metric, and hence lacks robustness to point
    clouds with varying densities. Some works integrate the dilated mechanism with
    the neighbor search to expand the receptive field Komarichev et al. ([2019](#bib.bib69)),
    Li et al. ([2018b](#bib.bib82)), Li et al. ([2019a](#bib.bib80)). Ball search
    involves finding all points within a specified radius (ball) around a query point.
    Similarly, grid-based search divides the point cloud space into a regular grid
    structure. Ball search and grid-based search are both useful for effectively capturing
    local structures and neighborhoods of varying densities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Features abstraction: In feature abstraction, commonly used methods include
    MLP-based, Convolution-based, and Transformer-based approaches. MLP is often used
    to extract features from individual points in point cloud data. By passing the
    feature vectors of each point through multiple fully connected layers, MLP learns
    nonlinear point-level feature representations. MLP offers flexibility and scalability
    in point cloud processing. Convolution operations on point clouds typically involve
    aggregating (low-level) information from local points to capture local structures
    and contextual information. In contrast, Transformer based methods establish correlations
    between high-level point information through the attention mechanism, which is
    more helpful for high-level tasks such as point cloud segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: The essence of MLP based, convolution based, and Transformer based methods is
    to learn the relationships between points, obtaining the robust weights. In the
    context of similar baseline architecture, the more comprehensive the learned point
    cloud relationship in the feature abstraction process, the stronger the robustness
    of the model becomes. Recently, MLP-based methods (e.g. Resurf Ran et al. ([2022](#bib.bib120))
    and PointNeXt Qian et al. ([2022](#bib.bib118))) exhibit better accuracy and efficiency,
    encouraging researchers to re-examine and further explore the potential of MLP-based
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Evaluation performance regarding for 3D instance segmentation methods
    on the ScanNet. Note: the ‘%’ after the value is omitted.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | mAP | bath. | bed | book. | cabi. | chair | count. | curt. | desk
    | door | other | pict. | refr. | shower. | sink | sofa | table | toilet | wind.
    |'
  prefs: []
  type: TYPE_TB
- en: '| GSPN Yi et al. ([2019](#bib.bib183)) | 30.6 | 50.0 | 40.5 | 31.1 | 34.8 |
    58.9 | 5.4 | 6.8 | 12.6 | 28.3 | 29.0 | 2.8 | 21.9 | 21.4 | 33.1 | 39.6 | 27.5
    | 82.1 | 24.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | 38.2 | 100 | 43.2 | 24.5 | 19.0
    | 57.7 | 1.3 | 26.3 | 3.3 | 32.0 | 24.0 | 7.5 | 42.2 | 85.7 | 11.7 | 69.9 | 27.1
    | 88.3 | 23.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | 48.8 | 100 | 67.2 | 59.0 | 30.1
    | 48.4 | 9.8 | 62.0 | 30.6 | 34.1 | 25.9 | 12.5 | 43.4 | 79.6 | 40.2 | 49.9 |
    51.3 | 90.9 | 43.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SGPN Wang et al. ([2018d](#bib.bib151)) | 14.3 | 20.8 | 39.0 | 16.9 | 6.5
    | 27.5 | 2.9 | 6.9 | 0 | 8.7 | 4.3 | 1.4 | 2.7 | 0 | 11.2 | 35.1 | 16.8 | 43.8
    | 13.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | 61.1 | 100 | 83.3 | 76.5
    | 52.6 | 75.6 | 13.6 | 58.8 | 47.0 | 43.8 | 43.2 | 35.8 | 65.0 | 85.7 | 42.9 |
    76.5 | 55.7 | 100 | 43.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | 76.1 | 100 | 80.8 | 84.5 | 71.6
    | 86.2 | 24.3 | 82.4 | 65.5 | 62.0 | 73.4 | 69.9 | 79.1 | 98.1 | 71.6 | 84.4 |
    76.9 | 100 | 59.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SSTNet Liang et al. ([2021](#bib.bib86)) | 69.8 | 100 | 69.7 | 88.8 | 55.6
    | 80.3 | 38.7 | 62.6 | 41.7 | 55.6 | 58.5 | 70.2 | 60.0 | 100 | 82.4 | 72.0 |
    69.2 | 100 | 50.9 |'
  prefs: []
  type: TYPE_TB
- en: '| 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | 24.8 | 66.7 | 56.6 | 7.6 | 3.5
    | 39.4 | 2.7 | 3.5 | 9.8 | 9.8 | 3.0 | 2.5 | 9.8 | 37.5 | 12.6 | 60.4 | 18.1 |
    85.4 | 17.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PanopticFus. Narita et al. ([2019](#bib.bib108)) | 47.8 | 66.7 | 71.2 | 59.5
    | 25.9 | 55.0 | 0 | 61.3 | 17.5 | 25.0 | 43.4 | 43.7 | 41.1 | 85.7 | 48.5 | 59.1
    | 26.7 | 94.4 | 35.9 |'
  prefs: []
  type: TYPE_TB
- en: '| OccuSeg Han et al. ([2020](#bib.bib39)) | 67.2 | 100 | 75.8 | 68.2 | 57.6
    | 84.2 | 47.7 | 50.4 | 52.4 | 56.7 | 58.5 | 45.1 | 55.7 | 100 | 75.1 | 79.7 |
    56.3 | 100 | 46.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MTML Lahoud et al. ([2019](#bib.bib71)) | 54.9 | 100 | 80.7 | 58.8 | 32.7
    | 64.7 | 0.4 | 81.5 | 18.0 | 41.8 | 36.4 | 18.2 | 44.5 | 100 | 44.2 | 68.8 | 57.1
    | 100 | 39.6 |'
  prefs: []
  type: TYPE_TB
- en: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | 63.6 | 100 | 76.5 | 62.4
    | 50.5 | 79.7 | 11.6 | 69.6 | 38.4 | 44.1 | 55.9 | 47.6 | 59.6 | 100 | 66.6 |
    75.6 | 55.6 | 99.7 | 51.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HAIS Chen et al. ([2021](#bib.bib10)) | 69.9 | 100 | 84.9 | 82.0 | 67.5 |
    80.8 | 27.9 | 75.7 | 46.5 | 51.7 | 59.6 | 55.9 | 60.0 | 100 | 65.4 | 76.7 | 67.6
    | 99.4 | 56.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Dyco3D He et al. ([2021](#bib.bib43)) | 64.1 | 100 | 84.1 | 89.3 | 53.1 |
    80.2 | 11.5 | 58.8 | 44.8 | 43.8 | 53.7 | 43.0 | 55.0 | 85.7 | 53.4 | 76.4 | 65.7
    | 98.7 | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DKNet Wu et al. ([2022b](#bib.bib163)) | 71.8 | 100 | 81.4 | 78.2 | 61.9
    | 87.2 | 22.4 | 75.1 | 56.9 | 67.7 | 58.5 | 72.4 | 63.3 | 98.1 | 51.5 | 81.9 |
    73.6 | 100 | 61.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | 76.3 | 100 | 87.3 | 71.7 | 66.6
    | 85.8 | 50.8 | 66.7 | 76.4 | 64.3 | 67.6 | 68.8 | 82.5 | 100 | 77.3 | 74.1 |
    77.7 | 100 | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Coarsening: Coarsening, also known as downsampling or subsampling, involves
    reducing the number of points in the point cloud while preserving the essential
    structures and features. Coarsening techniques include random sampling Hu et al.
    ([2020](#bib.bib49)), farthest point sampling Qi et al. ([2017a](#bib.bib115),
    [b](#bib.bib116)), tree-based methods Lei et al. ([2019](#bib.bib77)) and mesh-based
    decimation Lei et al. ([2023](#bib.bib79)). This step helps to reduce computational
    complexity and improve efficiency in subsequent stages of the segmentation process.
    Random sampling is simple and computationally efficient but may not select the
    most optimal points in maintaining local and global structure. This can potentially
    lead to information loss in feature rich regions. Farthest point sampling is widely
    used in networks as it ensures a more even spatial distribution of the selected
    points and can help preserve global structures. However, local structures can
    still get destroyed with farthest point sampling. Tree-based methods leverage
    hierarchical tree structures, such as an octree, to partition the point cloud
    and perform coarsening. Mesh based methods must convert the point cloud to a mesh
    first before it can decimate it. This adds a computational overhead to the already
    expensive mesh decimation process. Moreover, creating a mesh from complex and
    sparse point clouds obtained from LiDAR sensors is not always possible Lei et al.
    ([2023](#bib.bib79)).'
  prefs: []
  type: TYPE_NORMAL
- en: The above methods are hand-crafted or engineered techniques that do not involve
    learning parameters directly from the data, which determines the sub-sampling
    pattern based on predefined rules or heuristics, without explicitly optimizing
    for the task at hand. Therefore, some works propose learnable coarsening methods
    that integrate a learnable layer into the coarsening module such as pooling Groh
    et al. ([2018](#bib.bib33)), Lai et al. ([2023](#bib.bib72)), Wu et al. ([2022a](#bib.bib162)),
    Zhao et al. ([2021](#bib.bib194)), attention mechanism Yan et al. ([2020](#bib.bib176)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing: Pre-processing is an essential step in point cloud semantic
    segmentation that involves preparing and transforming the raw point cloud data
    before feeding it into the segmentation network. Pre-processing aims to enhance
    the quality, consistency, and suitability of the data for the segmentation task.
    Some common aspects of pre-processing in point cloud segmentation include data
    normalization, data augmentation, outlier removal, and point registration.'
  prefs: []
  type: TYPE_NORMAL
- en: Point clouds often have varying scales, which can negatively affect the performance
    of segmentation networks. Data normalization involves scaling the point cloud
    data to a standard range or unit sphere to ensure consistent scales across different
    point. For example, the number of ShapNet object point is generally fixed as 4096\.
    For the complexity scene, early works Xu et al. ([2021](#bib.bib174)), Qi et al.
    ([2017a](#bib.bib115)) divided raw point clouds into smaller ones (e.g. 4096 points,
    1m³ blocks) so that the processing does not require large memory. However, this
    strategy might break down the semantic continuity of the scene. Recent works Qian
    et al. ([2022](#bib.bib118)), Lai et al. ([2023](#bib.bib72)), Wu et al. ([2022a](#bib.bib162)),
    Zhao et al. ([2021](#bib.bib194)), Lei et al. ([2023](#bib.bib79)) input the complete
    scene into the network, but that requires more computational sources. Moreover,
    these works tend to down sample the point cloud in the pre-processing stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/050ff7383543ce0cbd52abeb5414ad7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Evaluation performance regarding for 3D instance segmentation architecture,
    including proposal based and proposal free, on the different class of ScanNet.
    For simplicity, we omit the ‘%’ after the value.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Results for 3D Instance Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We report the results of 3D instance segmentation methods on ScanNet Dai et al.
    ([2017](#bib.bib18)) datasets, and choose mAP as the evaluation metrics. These
    results of these methods are taken from the ScanNet Benchmark Challenge website,
    and they are shown in Table [9](#S7.T9 "Table 9 ‣ 7.1 Results for 3D Semantic
    Segmentation ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A
    Survey") and summarized in Figure [8](#S7.F8 "Figure 8 ‣ 7.1 Results for 3D Semantic
    Segmentation ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A
    Survey"). The table and figure shows that:'
  prefs: []
  type: TYPE_NORMAL
- en: ISBNet Ngo et al. ([2023](#bib.bib110)) has the state-of-the-art performance,
    with 76.3% average precision on ScanNet dataset at the time of this view. It also
    achieves the best instance segmentation performance on most classes, including
    ‘bathtub’, ‘counter’, ‘shower curtain’, ‘table’, ‘toilet’ and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Most methods have better segmentation performance on large scale classes such
    as ‘bathtub’ and ‘toilet’, and have poor segmentation performance on small scale
    classes such as ‘counter’, ‘desk’ and ‘picture’. Therefore, the instance segmentation
    of small objects is a prominent challenge.
  prefs: []
  type: TYPE_NORMAL
- en: In proposal-based methods, specifically the 2D embedding propagating-based methods
    such as 3D-BEVIS Elich et al. ([2019](#bib.bib21)) and PanoticFusion Narita et al.
    ([2019](#bib.bib108)), they tend to exhibit poorer performance compared to other
    proposal-free methods. This is primarily because simple embedding propagation
    techniques are more susceptible to error labels, leading to inaccuracies in the
    instance segmentation results.
  prefs: []
  type: TYPE_NORMAL
- en: Proposal-free methods demonstrate superior performance compared to proposal-based
    methods in instance segmentation across all classes, particularly for small objects
    like ’curtain,’ ’picture,’ ’shower curtain,’ and ’sink.’ Unlike proposal-based
    methods that rely on the accuracy of proposal generation, proposal-free methods
    circumvent this issue entirely. They directly consider the entire point cloud
    and its global features, enabling more precise and comprehensive instance segmentation.
    By avoiding the need for proposal generation, proposal-free methods can achieve
    better results by taking into account the overall context and characteristics
    of the point cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Results for 3D Part Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We report the results of 3D part segmentation methods on ShapeNet Yi et al.
    ([2016](#bib.bib181)) datasets and use Ins. mIoU as the evaluation metric. These
    results of various methods are taken from the original papers and they are shown
    in Table [10](#S7.T10 "Table 10 ‣ 7.3 Results for 3D Part Segmentation ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey"). We can find that part
    segmentation performance of all methods is quite similar. one underlying assumption
    is that objects in ShapeNet datasets are synthetic, normalized in scale, aligned
    in pose, and lack scene context. This makes part segmentation network difficult
    to extract rich context features. Another underlying assumption is that the point
    clouds in synthetic scene without background noise is more simpler and cleaner
    than ones in real scene, so that the geometric features of point clouds is easy
    to exploitation. The accuracy performance of various part segmentation network
    is difficult to be effectively distinguished.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Evaluation performance regarding for 3D part segmentation on the
    ShapeNet. Note: the ‘%’ after the value is omitted, the symbol ‘–’ means the results
    are unavailable.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Ins. mIoU | Methods | Ins. mIoU |'
  prefs: []
  type: TYPE_TB
- en: '| VV-Net Meng et al. ([2019](#bib.bib103)) | 87.4 | LatticeNet Su et al. ([2018](#bib.bib134))
    | 83.9 |'
  prefs: []
  type: TYPE_TB
- en: '| SSCNet Graham et al. ([2018](#bib.bib32)) | 86.0 | SGPN Wang et al. ([2018d](#bib.bib151))
    | 85.8 |'
  prefs: []
  type: TYPE_TB
- en: '| PointNet Qi et al. ([2017a](#bib.bib115)) | 83.7 | ShapePFCN Kalogerakis
    et al. ([2017](#bib.bib65)) | 88.4 |'
  prefs: []
  type: TYPE_TB
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) | 85.1 | VoxSegNet Wang and Lu
    ([2019](#bib.bib156)) | 87.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DContextNet Zeng and Gevers ([2018](#bib.bib188)) | 84.3 | PointgridLe and
    Duan ([2018](#bib.bib76)) | 86.4 |'
  prefs: []
  type: TYPE_TB
- en: '| RSNet Huang et al. ([2018](#bib.bib54)) | 84.9 | KPConv Thomas et al. ([2019](#bib.bib139))
    | 86.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MCC Hermosilla et al. ([2018](#bib.bib46)) | 85.9 | SO-Net Li et al. ([2018a](#bib.bib81))
    | 84.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) | 85.7 | PartNet Yu et al. ([2019](#bib.bib185))
    | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DGCNN Wang et al. ([2019b](#bib.bib155)) | 85.1 | SyncSpecCNN Yi et al. ([2017](#bib.bib182))
    | 84.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) | 86.8 | KCNet Yi et al. ([2017](#bib.bib182))
    | 84.7 |'
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) | 85.4 | PointCNN Li et al. ([2018b](#bib.bib82))
    | 86.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) | 85.9 | SpiderCNN Xu et al. ([2018](#bib.bib175))
    | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Flex-Conv Groh et al. ([2018](#bib.bib33)) | 85.0 | FeaStNet Verma et al.
    ([2018](#bib.bib141)) | 81.5 |'
  prefs: []
  type: TYPE_TB
- en: '| $\psi$-CNN Lei et al. ([2019](#bib.bib77)) | 86.8 | Kd-Net Klokov and Lempitsky
    ([2017](#bib.bib67)) | 82.3 |'
  prefs: []
  type: TYPE_TB
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) | 84.6 | O-CNN Wang et al. ([2017](#bib.bib148))
    | 85.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DRGCNN Yue et al. ([2022](#bib.bib187)) | 86.2 |  |  |'
  prefs: []
  type: TYPE_TB
- en: 8 Discussion and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3D segmentation using deep learning techniques has made significant progress
    during recent years. However, this is just the beginning and significant developments
    lie ahead of us. Below, we present some outstanding issues and identify potential
    research directions.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Synthetic datasets with richer information for multiple tasks: Synthetic datasets
    gradually play an important role on semantic segmentation due to the low cost
    and diverse scenes that can be generated Brodeur et al. ([2017](#bib.bib7)), Wu
    et al. ([2018b](#bib.bib164)) compared to real datasets Dai et al. ([2017](#bib.bib18)),
    Armeni et al. ([2016](#bib.bib2)), Hackel et al. ([2017](#bib.bib38)). It is well
    known that the information contained in training data determine the upper limit
    of the scene parsing accuracy. Existing datasets lack important semantic information,
    such as material, and texture information, which is more crucial for segmentation
    with similar color or geometric information. Besides, most exiting datasets are
    generally designed for a single task. Currently, only a few semantic segmentation
    datasets also contain labels for instances Dai et al. ([2017](#bib.bib18)) and
    scene layout Song et al. ([2015](#bib.bib132)) to meet the multi-task objective.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unified network for multiple tasks: It is expensive and impractical for a system
    to accomplish different computer vision tasks by various deep learning networks.
    Towards fundamental feature exploitation of scene, Semantic segmentation has strong
    consistency with some tasks, such as depth estimation Meyer et al. ([2019](#bib.bib104)),
    Liu et al. ([2015](#bib.bib92)), Guo and Chen ([2018](#bib.bib35)), Liu et al.
    ([2018b](#bib.bib94)), scene completion Dai et al. ([2018](#bib.bib20)), Xia et al.
    ([2023](#bib.bib167)), Zhang et al. ([2023](#bib.bib191)), instance segmentation
    Liang et al. ([2019b](#bib.bib88)), Pham et al. ([2019b](#bib.bib114)), Han et al.
    ([2020](#bib.bib39)), and object detection Meyer et al. ([2019](#bib.bib104)),
    Lian et al. ([2022](#bib.bib85)). These tasks could cooperate with each other
    to improve performance in a unified network because they exhibit certain correlations
    and shared feature representations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple modals for segmentation: Semantic segmentation using multiple representations,
    such as projected images, voxels, and point clouds, has the potential to achieve
    higher accuracy. Single representation limits segmentation accuracy due to the
    limited scene information in some practical scenarios. For instance, LiDAR measurements
    become sparser as the distance increases, and incorporating high-resolution image
    data can improve performance on distant objects. Therefore, utilizing multiple
    representations, also known as multiple modalities, can be an alternative way
    to enhance segmentation performance Dai and Nießner ([2018](#bib.bib19)), Chiang
    et al. ([2019](#bib.bib14)), Liu et al. ([2019b](#bib.bib97)), Hu et al. ([2021](#bib.bib50)).
    Moreover, segmenting point cloud with large image models (such as SAM Kirillov
    et al. ([2023](#bib.bib66))) and natural language models like ChatGPT can be popular
    approaches. The advanced capabilities of large models enable them to capture intricate
    patterns and semantic relationships, leading to improved performance and accuracy
    in segmentation tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretable and sparse feature abstraction: Various features abstraction,
    including MLP, Convolution and Transformer, have undergone significant development.
    Feature abstraction modules may prioritize generating interpretable feature representations,
    enabling them to provide explanations for model decisions, visualizations of points
    of interest, and other interpretability functions. Moreover, in scenarios involving
    large-scale data and limited resources, the feature abstraction module improve
    computational efficiency.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weakly-supervised and unsupervised segmentation: Deep learning has gained significant
    success in 3D segmentation, but heavily hinges on large-scale labelled training
    samples. Weakly-supervised learning refers to a training approach where the model
    is trained with limited or incomplete supervision. Unsupervised learning only
    use unlabelled training samples. weakly-supervised Su et al. ([2023](#bib.bib136)),
    Shi et al. ([2022](#bib.bib128)) and unsupervised Xiao et al. ([2023](#bib.bib169))
    paradigms are considered as an alternative to relax the impractical requirement
    of large-scale labelled datasets.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Real-time and incremental segmentation: Real-time 3D scene parsing is crucial
    for some applications such as autonomous driving and mobile robots. However, most
    existing 3D semantic segmentation methods mainly focus on the improvement of segmentation
    accuracy but rarely focus on real-time performance. A few lightweight 3D semantic
    segmentation networks realize real-time by pre-processing point clouds into other
    presentations such as projected images Wu et al. ([2018a](#bib.bib159)), Wu et al.
    ([2019a](#bib.bib160)), Park et al. ([2023](#bib.bib112)), Ando et al. ([2023](#bib.bib1)).
    Additionally, incremental segmentation will become an important research direction,
    allowing models to incrementally update and adapt in dynamic scenes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3D video semantic segmentation: Like 2D video semantic segmentation, A handful
    of works try to exploit 4D spatio-temporal features on 3D videos (also call 4D
    point clouds) Wei et al. ([2022](#bib.bib158)), Fan et al. ([2021](#bib.bib28)).
    From these works, it can be seen that the spatio-temporal features can help improve
    the robustness of 3D video or dynamic 3D scene semantic segmentation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We provided a comprehensive survey of the recent development in 3D segmentation
    using deep learning techniques, including 3D semantic segmentation, 3D instance
    segmentation and 3D part segmentation. We presented a comprehensive performance
    comparison and merit of various methods in each category, with potential research
    directions being listed.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ando et al. (2023) Ando, A., Gidaris, S., Bursuc, A., Puy, G., Boulch, A.,
    Marlet, R., 2023. Rangevit: Towards vision transformers for 3d semantic segmentation
    in autonomous driving, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 5240–5250.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Armeni et al. (2016) Armeni, I., Sener, O., Zamir, A., Jiang, H., Brilakis,
    I., Fischer, M., Savarese, S., 2016. 3d semantic parsing of large-scale indoor
    spaces, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 1534–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behley et al. (2019) Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke,
    S., Stachniss, C., Gall, J., 2019. Semantickitti: A dataset for semantic scene
    understanding of lidar sequences, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, IEEE. pp. 9297–9307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bello et al. (2020) Bello, S., Yu, S., Wang, C., Adam, J., Li, J., 2020. deep
    learning on 3d point clouds. Remote Sensing 12, 1729.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boulch et al. (2018) Boulch, A., Guerry, J., Le Saux, B., Audebert, N., 2018.
    Snapnet: 3d point cloud semantic labeling with 2d deep segmentation networks.
    Computers & Graphics 71, 189–198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boulch et al. (2017) Boulch, A., Le Saux, B., Audebert, N., 2017. Unstructured
    point cloud semantic labeling using deep segmentation networks. 3dor@ eurographics
    2, 7.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brodeur et al. (2017) Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti,
    L., Strub, F., Rouat, J., Larochelle, H., Courville, A., 2017. Home: A household
    multimodal environment. arXiv preprint arXiv:1711.11017 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2016) Cao, Y., Shen, C., Shen, H., 2016. Exploiting depth from single
    monocular images for object detection and semantic segmentation. IEEE Transactions
    on Image Processing 26, 836–846.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2017) Chang, A., Dai, A., Funkhouser, T., Halber, M., Niebner,
    M., Savva, M., Song, S., Zeng, A., Zhang, Y., 2017. Matterport3d: Learning from
    rgb-d data in indoor environments, in: 2017 International Conference on 3D Vision
    (3DV), IEEE. pp. 667–676.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen, S., Fang, J., Zhang, Q., Liu, W., Wang, X., 2021.
    Hierarchical aggregation for 3d instance segmentation, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 15467–15476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2009) Chen, X., Golovinskiy, A., Funkhouser, T., 2009. A benchmark
    for 3d mesh segmentation. ACM Transactions on Graphics 28, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2020) Cheng, J., Sun, Y., Meng, M.Q.H., 2020. Robust semantic
    mapping in challenging environments. Robotica 38, 256–270.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2017) Cheng, Y., Cai, R., Li, Z., Zhao, X., Huang, K., 2017.
    Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 3029–3037.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2019) Chiang, H.Y., Lin, Y.L., Liu, Y.C., Hsu, W.H., 2019. A
    unified point-based framework for 3d segmentation, in: Processing of the International
    Conference on 3D Vision, IEEE. pp. 155–163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) Chollet, F., 2017. Xception: Deep learning with depthwise separable
    convolutions, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 1251–1258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choy et al. (2019) Choy, C., Gwak, J., Savarese, S., 2019. 4d spatio-temporal
    convnets: Minkowski convolutional neural networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 3075–3084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Couprie et al. (2013) Couprie, C., Farabet, C., Najman, L., LeCun, Y., 2013.
    Indoor semantic segmentation using depth information. arXiv preprint arXiv:1301.3572
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2017) Dai, A., Chang, A., Savva, M., Halber, M., Funkhouser, T.,
    Nießner, M., 2017. Scannet: Richly-annotated 3d reconstructions of indoor scenes,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 5828–5839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai and Nießner (2018) Dai, A., Nießner, M., 2018. 3dmv: Joint 3d-multi-view
    prediction for 3d semantic scene segmentation, in: Proceedings of the European
    Conference on Computer Vision, pp. 452–468.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2018) Dai, A., Ritchie, D., Bokeloh, M., Reed, S., Sturm, J., Nießner,
    M., 2018. Scancomplete: Large-scale scene completion and semantic segmentation
    for 3d scans, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 4578–4587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elich et al. (2019) Elich, C., Engelmann, F., Kontogianni, T., Leibe, B., 2019.
    3d bird’s-eye-view instance segmentation, in: German Conference on Pattern Recognition,
    Springer. pp. 48–61.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Emre Yurdakul and Yemez (2017) Emre Yurdakul, E., Yemez, Y., 2017. Semantic
    segmentation of rgbd videos with recurrent fully convolutional neural networks,
    in: Processings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 367–374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2020a) Engelmann, F., Bokeloh, M., Fathi, A., Leibe, B.,
    Nießner, M., 2020a. 3d-mpa: Multi-proposal aggregation for 3d semantic instance
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 9031–9040.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2017) Engelmann, F., Kontogianni, T., Hermans, A., Leibe,
    B., 2017. Exploring spatial context for 3d semantic segmentation of point clouds,
    in: Processings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 716–724.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2020b) Engelmann, F., Kontogianni, T., Leibe, B., 2020b.
    Dilated point convolutions: On the receptive field size of point convolutions
    on 3d point clouds, in: 2020 IEEE International Conference on Robotics and Automation
    (ICRA), IEEE. pp. 9463–9469.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2018) Engelmann, F., Kontogianni, T., Schult, J., Leibe,
    B., 2018. Know what your neighbors do: 3d semantic segmentation of point clouds,
    in: Proceedings of the European Conference on Computer Vision, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2017) Fan, H., Mei, X., Prokhorov, D., Ling, H., 2017. Rgb-d scene
    labeling with multimodal recurrent neural networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops, pp. 9–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2021) Fan, H., Yang, Y., Kankanhalli, M., 2021. Point 4d transformer
    networks for spatio-temporal modeling in point cloud videos, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, pp. 14204–14213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2020) Feng, M., Zhang, L., Lin, X., Gilani, S.Z., Mian, A., 2020.
    Point attention network for semantic segmentation of 3d point clouds. Pattern
    Recognition , 107446.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fooladgar and Kasaei (2020) Fooladgar, F., Kasaei, S., 2020. A survey on indoor
    rgb-d semantic segmentation: from hand-crafted features to deep convolutional
    neural networks. Multimedia Tools and Applications 79, 4499–4524.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geiger et al. (2012) Geiger, A., Lenz, P., Urtasun, R., 2012. Are we ready
    for autonomous driving? the kitti vision benchmark suite, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE. pp. 3354–3361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graham et al. (2018) Graham, B., Engelcke, M., Van Der Maaten, L., 2018. 3d
    semantic segmentation with submanifold sparse convolutional networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9224–9232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Groh et al. (2018) Groh, F., Wieschollek, P., Lensch, H.P., 2018. Flex-convolution,
    in: Asian Conference on Computer Vision, Springer. pp. 105–122.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guerry et al. (2017) Guerry, J., Boulch, A., Le Saux, B., Moras, J., Plyer,
    A., Filliat, D., 2017. Snapnet-r: Consistent 3d multi-view semantic labeling for
    robotics, in: Processings of the IEEE/CVF International Conference on Computer
    Vision Workshops, pp. 669–678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo and Chen (2018) Guo, Y., Chen, T., 2018. Semantic segmentation of rgbd images
    based on deep depth regression. Pattern Recognit. Lett. 109, 55–64.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.,
    2020. Deep learning for 3d point clouds: A survey. IEEE Transactions on Pattern
    Analysis and Machine Intelligence .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2014) Gupta, S., Girshick, R., Arbeláez, P., Malik, J., 2014.
    Learning rich features from rgb-d images for object detection and segmentation,
    in: Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part VII 13, Springer. pp. 345–360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hackel et al. (2017) Hackel, T., Savinov, N., Ladicky, L., Wegner, J., Schindler,
    K., Pollefeys, M., 2017. Semantic3d. net: A new large-scale point cloud classification
    benchmark. arXiv preprint arXiv:1704.03847 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2020) Han, L., Zheng, T., Xu, L., Fang, L., 2020. Occuseg: Occupancy-aware
    3d instance segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 2940–2949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hanocka et al. (2019) Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman,
    S., D., C.O., 2019. Meshcnn: a network with an edge. ACM Transactions on Graphics
    38, 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hazirbas et al. (2016) Hazirbas, C., Ma, L., Domokos, C., Cremers, D., 2016.
    Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture,
    in: Proc. Asian Conf. Compu. Vis., Springer. pp. 213–228.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2017a) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017a. Mask
    r-cnn, in: Processings of the IEEE/CVF International Conference on Computer Vision,
    pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) He, T., Shen, C., Van Den Hengel, A., 2021. Dyco3d: Robust
    instance segmentation of 3d point clouds through dynamic convolution, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pp. 354–363.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022) He, T., Yin, W., Shen, C., van den Hengel, A., 2022. Pointinst3d:
    Segmenting 3d instances by points, in: Proceedings of the European Conference
    on Computer Vision, Springer. pp. 286–302.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2017b) He, Y., Chiu, W.C., Keuper, M., Fritz, M., 2017b. Std2p:
    Rgbd semantic segmentation using spatio-temporal data-driven pooling, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4837–4846.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermosilla et al. (2018) Hermosilla, P., Ritschel, T., Vázquez, P.P., Vinacua,
    À., Ropinski, T., 2018. Monte carlo convolution for learning on non-uniformly
    sampled point clouds. ACM Transactions on Graphics 37, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Höft et al. (2014) Höft, N., Schulz, H., Behnke, S., 2014. Fast semantic segmentation
    of rgb-d scenes with gpu-accelerated deep neural networks, in: Joint German/Austrian
    Conference on Artificial Intelligence, Springer. pp. 80–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2019) Hou, J., Dai, A., Nießner, M., 2019. 3d-sis: 3d semantic
    instance segmentation of rgb-d scans, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 4421–4430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020) Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni,
    N., Markham, A., 2020. Randla-net: Efficient semantic segmentation of large-scale
    point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 11108–11117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, W., Zhao, H., Jiang, L., Jia, J., Wong, T.T., 2021. Bidirectional
    projection network for cross dimension scene understanding, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14373–14382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua et al. (2016) Hua, B., Pham, Q., Nguyen, D., Tran, M., Yu, L., Yeung, S.,
    2016. Scenenn: A scene meshes dataset with annotations, in: Processing of the
    International Conference on 3D Vision, IEEE. pp. 92–101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua et al. (2018) Hua, B.S., Tran, M.K., Yeung, S.K., 2018. Pointwise convolutional
    neural networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 984–993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and You (2016) Huang, J., You, S., 2016. Point cloud labeling using 3d
    convolutional neural network, in: 2016 23rd International Conference on Pattern
    Recognition (ICPR), IEEE. pp. 2670–2675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) Huang, Q., Wang, W., Neumann, U., 2018. Recurrent slice
    networks for 3d segmentation of point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2626–2635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2016) Iandola, F., Han, S., Moskewicz, M., Ashraf, K., Dally,
    W., Keutzer, K., 2016. Squeezenet: Alexnet-level accuracy with 50x fewer parameters
    and< 0.5 mb model size. arXiv preprint arXiv:1602.07360 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioannidou et al. (2017) Ioannidou, A., Chatzilari, E., Nikolopoulos, S., Kompatsiaris,
    I., 2017. Deep learning advances in computer vision with 3d data: A survey. ACM
    Computing Surveys 50, 1–38.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivaneckỳ (2016) Ivaneckỳ, B.J., 2016. Depth estimation by convolutional neural
    networks. Ph.D. thesis. Master thesis, Brno University of Technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jampani et al. (2016) Jampani, V., Kiefel, M., Gehler, P.V., 2016. Learning
    sparse high dimensional filters: Image filtering, dense crfs and bilateral neural
    networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4452–4461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaritz et al. (2019) Jaritz, M., Gu, J., Su, H., 2019. Multi-view pointnet
    for 3d scene understanding, in: Processings of the IEEE/CVF International Conference
    on Computer Vision Workshops, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeong et al. (2018) Jeong, J., Yoon, T.S., Park, J.B., 2018. Multimodal sensor-based
    semantic 3d mapping for a large-scale environment. Expert Systems with Applications
    105, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020a) Jiang, H., Yan, F., Cai, J., Zheng, J., Xiao, J., 2020a.
    End-to-end 3d point cloud instance segmentation without detection, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12796–12805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2017) Jiang, J., Zhang, Z., Huang, Y., Zheng, L., 2017. Incorporating
    depth into both cnn and crf for indoor semantic segmentation, in: Processing of
    the IEEE International Conference on Software Engineering and Service Science,
    IEEE. pp. 525–530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020b) Jiang, L., Zhao, H., Shi, S., Liu, S., Fu, C., Jia, J.,
    2020b. Pointgroup: Dual-set point grouping for 3d instance segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4867–4876.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2018) Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., 2018. Pointsift:
    A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint
    arXiv:1807.00652 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalogerakis et al. (2017) Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri,
    S., 2017. 3d shape segmentation with projective convolutional networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3779–3788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland,
    C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al., 2023.
    Segment anything. arXiv preprint arXiv:2304.02643 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klokov and Lempitsky (2017) Klokov, R., Lempitsky, V., 2017. Escape from cells:
    Deep kd-networks for the recognition of 3d point cloud models, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 863–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kochanov et al. (2016) Kochanov, D., Ošep, A., Stückler, J., Leibe, B., 2016.
    Scene flow propagation for semantic mapping and object discovery in dynamic street
    scenes, in: Proc. IEEE Int. Conf. Intell. Rob. Syst., IEEE. pp. 1785–1792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Komarichev et al. (2019) Komarichev, A., Zhong, Z., Hua, J., 2019. A-cnn: Annularly
    convolutional neural networks on point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 7421–7430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. (2023) Kong, L., Liu, Y., Chen, R., Ma, Y., Zhu, X., Li, Y., Hou,
    Y., Qiao, Y., Liu, Z., 2023. Rethinking range view representation for lidar segmentation.
    arXiv preprint arXiv:2303.05367 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lahoud et al. (2019) Lahoud, J., Ghanem, B., Pollefeys, M., Oswald, M., 2019.
    3d instance segmentation via multi-task metric learning, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 9256–9266.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2023) Lai, X., Chen, Y., Lu, F., Liu, J., Jia, J., 2023. Spherical
    transformer for lidar-based 3d recognition, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 17545–17555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2022) Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S.,
    Qi, X., Jia, J., 2022. Stratified transformer for 3d point cloud segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 8500–8509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Landrieu and Simonovsky (2018) Landrieu, L., Simonovsky, M., 2018. Large-scale
    point cloud semantic segmentation with superpoint graphs, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4558–4567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lawin et al. (2017) Lawin, F., Danelljan, M., Tosteberg, P., Bhat, G., Khan,
    F., Felsberg, M., 2017. Deep projective 3d semantic segmentation, in: Computer
    Analysis of Images and Patterns, Springer. pp. 95–107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le and Duan (2018) Le, T., Duan, Y., 2018. Pointgrid: A deep network for 3d
    shape understanding, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 9204–9214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei et al. (2019) Lei, H., Akhtar, N., Mian, A., 2019. Octree guided cnn with
    spherical kernels for 3d point clouds, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9631–9640.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2020) Lei, H., Akhtar, N., Mian, A., 2020. Spherical kernel for
    efficient graph convolution on 3d point clouds. IEEE Transactions on Pattern Analysis
    and Machine Intelligence .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2023) Lei, H., Akhtar, N., Shah, M., Mian, A., 2023. Mesh convolution
    with continuous filters for 3-d surface parsing. IEEE Transactions on Neural Networks
    and Learning Systems .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Li, G., Muller, M., Thabet, A., Ghanem, B., 2019a. Deepgcns:
    Can gcns go as deep as cnns?, in: Processings of the IEEE/CVF International Conference
    on Computer Vision, pp. 9267–9276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018a) Li, J., Chen, B.M., Hee Lee, G., 2018a. So-net: Self-organizing
    network for point cloud analysis, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 9397–9406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018b) Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., 2018b.
    Pointcnn: Convolution on x-transformed points. Advances in Neural Information
    Processing Systemsms 31, 820–830.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. Tgnet:
    Geometric graph cnn on 3-d point cloud segmentation. IEEE Transactions on Geoscience
    and Remote Sensing 58, 3588–3600.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2016) Li, Z., Gan, Y., Liang, X., Yu, Y., Cheng, H., Lin, L., 2016.
    Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling,
    in: Proceedings of the European Conference on Computer Vision, Springer. pp. 541–557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. (2022) Lian, Q., Li, P., Chen, X., 2022. Monojsg: Joint semantic
    and geometric cost volume for monocular 3d object detection, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1070–1079.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Liang, Z., Li, Z., Xu, S., Tan, M., Jia, K., 2021. Instance
    segmentation in 3d scenes using semantic superpoint tree networks, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 2783–2792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2019a) Liang, Z., Yang, M., Deng, L., Wang, C., Wang, B., 2019a.
    Hierarchical depthwise graph convolutional neural network for 3d semantic segmentation
    of point clouds, in: Processing of the IEEE International Conference on Robotics
    and Automation, IEEE. pp. 8152–8158.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2019b) Liang, Z., Yang, M., Wang, C., 2019b. 3d graph embedding
    learning with a structure-aware loss function for point cloud semantic instance
    segmentation. arXiv preprint arXiv:1902.05247 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2017) Lin, D., Chen, G., Cohen-Or, D., Heng, P., Huang, H., 2017.
    Cascaded feature network for semantic segmentation of rgb-d images, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 1311–1319.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Furukawa (2019) Liu, C., Furukawa, Y., 2019. Masc: multi-scale affinity
    with sparse convolution for 3d instance segmentation. arXiv preprint arXiv:1902.04478
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2017) Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu,
    J., 2017. 3dcnn-dqn-rnn: A deep reinforcement learning framework for semantic
    parsing of large-scale 3d point clouds, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, pp. 5678–5687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2015) Liu, F., Shen, C., Lin, G., Reid, I., 2015. Learning depth
    from single monocular images using deep convolutional neural fields. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 38, 2024–2039.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Liu, H., Wu, W., Wang, X., Qian, Y., 2018a. Rgb-d joint modelling
    with scene geometric information for indoor semantic segmentation. Multimed. Tools.
    Appl. 77, 22475–22488.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Liu, J., Wang, Y., Li, Y., Fu, J., Li, J., Lu, H., 2018b.
    Collaborative deconvolutional neural networks for joint depth estimation and semantic
    segmentation. IEEE Trans. Neural Netw. Learn. Syst. 29, 5655–5666.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Liu, W., Sun, J., Li, W., Hu, T., Wang, P., 2019a. Deep
    learning on point clouds and its application: A survey. Sensors 19, 4188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018c) Liu, Y., Yang, S., Li, B., Zhou, W., Xu, J., Li, H., Lu,
    Y., 2018c. Affinity derivation and graph merge for instance segmentation, in:
    Proceedings of the European Conference on Computer Vision, pp. 686–703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Liu, Z., Tang, H., Lin, Y., Han, S., 2019b. Point-voxel
    cnn for efficient 3d deep learning, in: Advances in Neural Information Processing
    Systemsms, pp. 965–975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe (2004) Lowe, D.G., 2004. Distinctive image features from scale-invariant
    keypoints. International journal of computer vision 60, 91–110.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2017) Ma, L., Stückler, J., Kerl, C., Cremers, D., 2017. Multi-view
    deep learning for consistent semantic mapping with rgb-d cameras, in: Proc. IEEE
    Int. Conf. Intell. Rob. Syst., IEEE. pp. 598–605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020) Ma, Y., Guo, Y., Liu, H., Lei, Y., Wen, G., 2020. Global context
    reasoning for semantic segmentation of 3d point clouds, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2931–2940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maturana and Scherer (2015) Maturana, D., Scherer, S., 2015. Voxnet: A 3d convolutional
    neural network for real-time object recognition, in: Proc. IEEE Int. Conf. Intell.
    Rob. Syst., IEEE. pp. 922–928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCormac et al. (2017) McCormac, J., Handa, A., Davison, A., Leutenegger, S.,
    2017. Semanticfusion: Dense 3d semantic mapping with convolutional neural networks,
    in: Proceedings of the IEEE International Conference on Robotics and Automation,
    IEEE. pp. 4628–4635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meng et al. (2019) Meng, H., Gao, L., Lai, Y., Manocha, D., 2019. Vv-net: Voxel
    vae net with group convolutions for point cloud segmentation, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 8500–8508.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meyer et al. (2019) Meyer, G.P., Charland, J., Hegde, D., Laddha, A., Vallespi-Gonzalez,
    C., 2019. Sensor fusion for joint 3d object detection and semantic segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Milioto et al. (2019) Milioto, A., Vizzo, I., Behley, J., Stachniss, C., 2019.
    Rangenet++: Fast and accurate lidar semantic segmentation, in: Proc. IEEE Int.
    Conf. Intell. Rob. Syst., IEEE. pp. 4213–4220.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morton (1966) Morton, G.M., 1966. A computer oriented geodetic data base and
    a new technique in file sequencing .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mousavian et al. (2016) Mousavian, A., Pirsiavash, H., Košecká, J., 2016. Joint
    semantic segmentation and depth estimation with deep convolutional networks, in:
    Processing of the International Conference on 3D Vision, IEEE. pp. 611–619.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narita et al. (2019) Narita, G., Seno, T., Ishikawa, T., Kaji, Y., 2019. Panopticfusion:
    Online volumetric semantic mapping at the level of stuff and things. arXiv preprint
    arXiv:1903.01177 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naseer et al. (2018) Naseer, M., Khan, S., Porikli, F., 2018. Indoor scene
    understanding in 2.5/3d for autonomous agents: A survey. IEEE Access 7, 1859–1887.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ngo et al. (2023) Ngo, T.D., Hua, B.S., Nguyen, K., 2023. Isbnet: a 3d point
    cloud instance segmentation network with instance-aware sampling and box-aware
    dynamic convolution, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 13550–13559.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2022) Park, C., Jeong, Y., Cho, M., Park, J., 2022. Fast point
    transformer, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 16949–16958.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Park, J., Kim, C., Kim, S., Jo, K., 2023. Pcscnet: Fast
    3d semantic segmentation of lidar point cloud for autonomous car using point convolution
    and sparse convolution network. Expert Systems with Applications 212, 118815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2019a) Pham, Q., Hua, B., Nguyen, T., Yeung, S., 2019a. Real-time
    progressive 3d semantic segmentation for indoor scenes, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, IEEE. pp. 1089–1098.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2019b) Pham, Q.H., Nguyen, T., Hua, B.S., Roig, G., Yeung, S.K.,
    2019b. Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task
    pointwise networks and multi-value conditional random fields, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8827–8836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017a) Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017a. Pointnet:
    Deep learning on point sets for 3d classification and segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017b) Qi, C.R., Yi, L., Su, H., Guibas, L.J., 2017b. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. Advances in
    Neural Information Processing Systemsms 30, 5099–5108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017c) Qi, X., Liao, R., Jia, J., Fidler, S., Urtasun, R., 2017c.
    3d graph neural networks for rgbd semantic segmentation, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 5199–5208.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2022) Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H.A.A.K.,
    Elhoseiny, M., Ghanem, B., 2022. Pointnext: Revisiting pointnet++ with improved
    training and scaling strategies. arXiv preprint arXiv:2206.04670 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raj et al. (2015) Raj, A., Maturana, D., Scherer, S., 2015. Multi-scale convolutional
    architecture for semantic segmentation. Robotics Institute, Carnegie Mellon University,
    Tech. Rep. CMU-RITR-15-21 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ran et al. (2022) Ran, H., Liu, J., Wang, C., 2022. Surface representation
    for point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 18942–18952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rethage et al. (2018) Rethage, D., Wald, J., Sturm, J., Navab, N., Tombari,
    F., 2018. Fully-convolutional point networks for large-scale point clouds, in:
    Proceedings of the European Conference on Computer Vision, pp. 596–611.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riegler et al. (2017) Riegler, G., Osman Ulusoy, A., Geiger, A., 2017. Octnet:
    Learning deep 3d representations at high resolutions, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 3577–3586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riemenschneider et al. (2014) Riemenschneider, H., Bódis-Szomorú, A., Weissenberg,
    J., Van Gool, L., 2014. Learning where to classify in multi-view semantic segmentation,
    in: Proceedings of the European Conference on Computer Vision, Springer. pp. 516–532.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosu et al. (2019) Rosu, R.A., Schütt, P., Quenzel, J., Behnke, S., 2019. Latticenet:
    Fast point cloud segmentation using permutohedral lattices. arXiv preprint arXiv:1912.05905
    .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roynard et al. (2018) Roynard, X., Deschaud, J., Goulette, F., 2018. Paris-lille-3d:
    A large and high-quality ground-truth urban point cloud dataset for automatic
    segmentation and classification. The International Journal of Robotics Research
    37, 545–557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2018) Shen, Y., Feng, C., Yang, Y., Tian, D., 2018. Mining point
    cloud local structures by kernel correlation and graph pooling, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4548–4557.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2020) Shi, H., Lin, G., Wang, H., Hung, T.Y., Wang, Z., 2020. Spsequencenet:
    Semantic segmentation network on 4d point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 4574–4583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2022) Shi, H., Wei, J., Li, R., Liu, F., Lin, G., 2022. Weakly
    supervised segmentation on outdoor 4d point clouds with temporal matching and
    spatial graph propagation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 11840–11849.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silberman and Fergus (2011) Silberman, N., Fergus, R., 2011. Indoor scene segmentation
    using a structured light sensor, in: Processings of the IEEE/CVF International
    Conference on Computer Vision Worksh., IEEE. pp. 601–608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silberman et al. (2012) Silberman, N., Hoiem, D., Kohli, P., Fergus, R., 2012.
    Indoor segmentation and support inference from rgbd images, in: Proceedings of
    the European Conference on Computer Vision, Springer. pp. 746–760.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonovsky and Komodakis (2017) Simonovsky, M., Komodakis, N., 2017. Dynamic
    edge-conditioned filters in convolutional neural networks on graphs, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3693–3702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2015) Song, S., Lichtenberg, S.P., Xiao, J., 2015. Sun rgb-d:
    A rgb-d scene understanding benchmark suite, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 567–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2017) Song, Y., Chen, X., Li, J., Zhao, Q., 2017. Embedding 3d
    geometric features for rigid object part segmentation, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 580–588.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2018) Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang,
    M.H., Kautz, J., 2018. Splatnet: Sparse lattice networks for point cloud processing,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2530–2539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2015) Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015.
    Multi-view convolutional neural networks for 3d shape recognition, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 945–953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2023) Su, Y., Xu, X., Jia, K., 2023. Weakly supervised 3d point cloud
    segmentation via multi-prototype learning. IEEE Transactions on Circuits and Systems
    for Video Technology .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tatarchenko et al. (2018) Tatarchenko, M., Park, J., Koltun, V., Zhou, Q.Y.,
    2018. Tangent convolutions for dense prediction in 3d, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3887–3896.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tchapmi et al. (2017) Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese,
    S., 2017. Segcloud: Semantic segmentation of 3d point clouds, in: Processing of
    the International Conference on 3D Vision, IEEE. pp. 537–547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thomas et al. (2019) Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B.,
    Goulette, F., Guibas, L.J., 2019. Kpconv: Flexible and deformable convolution
    for point clouds, in: Processings of the IEEE/CVF International Conference on
    Computer Vision, pp. 6411–6420.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valipour et al. (2017) Valipour, S., Siam, M., Jagersand, M., Ray, N., 2017.
    Recurrent fully convolutional networks for video segmentation, in: Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, IEEE. pp.
    29–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verma et al. (2018) Verma, N., Boyer, E., Verbeek, J., 2018. Feastnet: Feature-steered
    graph convolutions for 3d shape analysis, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 2598–2606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2022) Vu, T., Kim, K., Luu, T.M., Nguyen, T., Yoo, C.D., 2022. Softgroup
    for 3d instance segmentation on point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2708–2717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018a) Wang, C., Samari, B., Siddiqi, K., 2018a. Local spectral
    graph convolution for point set feature learning, in: Proceedings of the European
    Conference on Computer Vision, pp. 52–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Wang, J., Li, X., Sullivan, A., Abbott, L., Chen, S., 2022.
    Pointmotionnet: Point-wise motion learning for large-scale lidar point clouds
    sequences, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4419–4428.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Wang, J., Wang, Z., Tao, D., See, S., Wang, G., 2016. Learning
    common and specific features for rgb-d semantic segmentation with deconvolutional
    networks, in: Proceedings of the European Conference on Computer Vision, Springer.
    pp. 664–679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) Wang, P., Gan, Y., Shui, P., Yu, F., Zhang, Y., Chen, S.,
    Sun, Z., 2018b. 3d shape segmentation via shape fully convolutional networks.
    Computers & Graphics 70, 128–139.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2015) Wang, P., Shen, X., Lin, Z., Cohen, S., Price, B., Yuille,
    A., 2015. Towards unified depth and semantic prediction from a single image, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2800–2809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., 2017.
    O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM Transactions
    on Graphics 36, 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018c) Wang, S., Suo, S., Ma, W.C., Pokrovsky, A., Urtasun, R.,
    2018c. Deep parametric continuous convolutional neural networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2589–2597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Neumann (2018) Wang, W., Neumann, U., 2018. Depth-aware cnn for rgb-d
    segmentation, in: Proceedings of the European Conference on Computer Vision, pp.
    135–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018d) Wang, W., Yu, R., Huang, Q., Neumann, U., 2018d. Sgpn:
    Similarity group proposal network for 3d point cloud instance segmentation, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2569–2578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Wang, X., Liu, S., Shen, X., Shen, C., Jia, J., 2019a.
    Associatively segmenting instances and semantics in point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4096–4105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2012) Wang, Y., Asafi, S., Van Kaick, O., Zhang, H., Cohen-Or,
    D., Chen, B., 2012. Active co-analysis of a set of shapes. ACM Transactions on
    Graphics 31, 1–10.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018e) Wang, Y., Shi, T., Yun, P., Tai, L., Liu, M., 2018e. Pointseg:
    Real-time semantic segmentation based on 3d lidar point cloud. arXiv preprint
    arXiv:1807.06288 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M.,
    Solomon, J.M., 2019b. Dynamic graph cnn for learning on point clouds. ACM Transactions
    on Graphics 38, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Lu (2019) Wang, Z., Lu, F., 2019. Voxsegnet: Volumetric cnns for semantic
    part segmentation of 3d shapes. IEEE Transactions on Visualization and Computer
    Graphics .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei (2008) Wei, L.Y., 2008. Parallel poisson disk sampling. ACM Transactions
    on Graphics 27, 1–9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Wei, Y., Liu, H., Xie, T., Ke, Q., Guo, Y., 2022. Spatial-temporal
    transformer for 3d point cloud sequences, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, pp. 1171–1180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018a) Wu, B., Wan, A., Yue, X., Keutzer, K., 2018a. Squeezeseg:
    Convolutional neural nets with recurrent crf for real-time road-object segmentation
    from 3d lidar point cloud, in: Proceedings of the IEEE International Conference
    on Robotics and Automation, IEEE. pp. 1887–1893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019a) Wu, B., Zhou, X., Zhao, S., Yue, X., Keutzer, K., 2019a.
    Squeezesegv2: Improved model structure and unsupervised domain adaptation for
    road-object segmentation from a lidar point cloud, in: Proceedings of the IEEE
    International Conference on Robotics and Automation, IEEE. pp. 4376–4382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019b) Wu, W., Qi, Z., Fuxin, L., 2019b. Pointconv: Deep convolutional
    networks on 3d point clouds, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 9621–9630.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022a) Wu, X., Lao, Y., Jiang, L., Liu, X., Zhao, H., 2022a. Point
    transformer v2: Grouped vector attention and partition-based pooling. Advances
    in Neural Information Processing Systemsms 35, 33330–33342.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022b) Wu, Y., Shi, M., Du, S., Lu, H., Cao, Z., Zhong, W., 2022b.
    3d instances as 1d kernels, in: Proceedings of the European Conference on Computer
    Vision, Springer. pp. 235–252.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018b) Wu, Y., Wu, Y., Gkioxari, G., Tian, Y., 2018b. Building generalizable
    agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2015) Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X.,
    Xiao, J., 2015. 3d shapenets: A deep representation for volumetric shapes, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 1912–1920.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022c) Wu, Z., Zhou, Z., Allibert, G., Stolz, C., Demonceaux, C.,
    Ma, C., 2022c. Transformer fusion for indoor rgb-d semantic segmentation. Available
    at SSRN 4251286 .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2023) Xia, Z., Liu, Y., Li, X., Zhu, X., Ma, Y., Li, Y., Hou, Y.,
    Qiao, Y., 2023. Scpnet: Semantic scene completion on point cloud, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17642–17651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang and Fox (2017) Xiang, Y., Fox, D., 2017. Da-rnn: Semantic mapping with
    data associated recurrent neural networks. arXiv preprint arXiv:1703.03098 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Xiao, A., Huang, J., Guan, D., Zhang, X., Lu, S., Shao,
    L., 2023. Unsupervised point cloud representation learning with deep neural networks:
    A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020a) Xie, Y., Jiaojiao, T., Zhu, X., 2020a. Linking points with
    labels in 3d: A review of point cloud semantic segmentation. IEEE Geoscience and
    Remote Sensing Magazine .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020b) Xie, Z., Chen, J., Peng, B., 2020b. Point clouds learning
    with attention-based graph convolution networks. Neurocomputing .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020) Xu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K.,
    Tomizuka, M., 2020. Squeezesegv3: Spatially-adaptive convolution for efficient
    point-cloud segmentation. arXiv preprint arXiv:2004.01803 .'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2017) Xu, H., Dong, M., Zhong, Z., 2017. Directionally convolutional
    networks for 3d shape segmentation, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, pp. 2698–2707.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Xu, M., Ding, R., Zhao, H., Qi, X., 2021. Paconv: Position
    adaptive convolution with dynamic kernel assembling on point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3173–3182.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., 2018. Spidercnn:
    Deep learning on point sets with parameterized convolutional filters, in: Proceedings
    of the European Conference on Computer Vision, pp. 87–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2020) Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., 2020. Pointasnl:
    Robust point clouds processing using nonlocal neural networks with adaptive sampling,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    pp. 5589–5598.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2019) Yang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham,
    A., Trigoni, N., 2019. Learning object bounding boxes for 3d instance segmentation
    on point clouds. Advances in Neural Information Processing Systemsms 32, 6740–6749.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2017) Yang, S., Huang, Y., Scherer, S., 2017. Semantic 3d occupancy
    mapping through efficient high order crfs, in: Proc. IEEE Int. Conf. Intell. Rob.
    Syst., IEEE. pp. 590–597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Yang, Y., Xu, Y., Zhang, C., Xu, Z., Huang, J., 2022. Hierarchical
    vision transformer with channel attention for rgb-d image segmentation, in: Proceedings
    of the 4th International Symposium on Signal Processing Systems, pp. 68–73.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2018) Ye, X., Li, J., Huang, H., Du, L., Zhang, X., 2018. 3d recurrent
    neural networks with context fusion for point cloud semantic segmentation, in:
    Proceedings of the European Conference on Computer Vision, pp. 403–417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2016) Yi, L., Kim, V., Ceylan, D., Shen, I., Yan, M., Su, H., Lu,
    C., Huang, Q., Sheffer, A., Guibas, L., 2016. A scalable active framework for
    region annotation in 3d shape collections. ACM Transactions on Graphics 35, 1–12.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2017) Yi, L., Su, H., Guo, X., Guibas, L.J., 2017. Syncspeccnn:
    Synchronized spectral cnn for 3d shape segmentation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2282–2290.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2019) Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., 2019.
    Gspn: Generative shape proposal network for 3d instance segmentation in point
    cloud, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 3947–3956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying and Chuah (2022) Ying, X., Chuah, M.C., 2022. Uctnet: Uncertainty-aware
    cross-modal transformer network for indoor rgb-d semantic segmentation, in: Proceedings
    of the European Conference on Computer Vision, Springer. pp. 20–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019) Yu, F., Liu, K., Zhang, Y., Zhu, C., Xu, K., 2019. Partnet:
    A recursive part decomposition network for fine-grained and hierarchical shape
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 9491–9500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2021) Yuan, X., Shi, J., Gu, L., 2021. A review of deep learning
    methods for semantic segmentation of remote sensing imagery. Expert Systems with
    Applications 169, 114417.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yue et al. (2022) Yue, C., Wang, Y., Tang, X., Chen, Q., 2022. Drgcnn: Dynamic
    region graph convolutional neural network for point clouds. Expert Systems with
    Applications 205, 117663.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng and Gevers (2018) Zeng, W., Gevers, T., 2018. 3dcontextnet: Kd tree guided
    hierarchical learning of point clouds using local and global contextual cues,
    in: Proceedings of the European Conference on Computer Vision, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, C., Wan, H., Shen, X., Wu, Z., 2022. Patchformer:
    An efficient point transformer with patch attention, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 11799–11808.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Zhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong,
    B., Foroosh, H., 2020. Polarnet: An improved grid representation for online lidar
    point clouds semantic segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9601–9610.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, Z., Han, X., Dong, B., Li, T., Yin, B., Yang, X.,
    2023. Point cloud scene completion with joint color and semantic estimation from
    single rgb-d image. IEEE Transactions on Pattern Analysis and Machine Intelligence
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhao, C., Sun, L., Purkait, P., Duckett, T., Stolkin, R.,
    2018. Dense rgb-d semantic mapping with pixel-voxel neural network. Sensors 18,
    3099.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019a) Zhao, H., Jiang, L., Fu, C.W., Jia, J., 2019a. Pointweb:
    Enhancing local neighborhood features for point cloud processing, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5565–5573.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V., 2021.
    Point transformer, in: Proceedings of the IEEE/CVF International Conference on
    Computer Vision, pp. 16259–16268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019b) Zhao, Y., Birdal, T., Deng, H., Tombari, F., 2019b. 3d
    point capsule networks, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1009–1018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )Yong He received the M.S. degree from China University of Mining and Technology,
    Xuzhou, Jiangsu, China, in 2018\. He is currently pursing the Ph.D. degree with
    Hunan University, Changsha, China, and he is also currently a Visiting Scholar
    with University of Western Australia, Perth, Australia. His research interests
    include computer vision, point clouds analysis, and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: )Hongshan Yu received the B.S., M.S. and Ph.D. degrees of Control Science and
    Technology from electrical and information engineering of Hunan University, Changsha,
    China, in 2001, 2004 and 2007 respectively. From 2011 to 2012, he worked as a
    postdoctoral researcher in Laboratory for Computational Neuroscience of University
    of Pittsburgh, USA. He is currently a professor of Hunan University and associate
    dean of National Engineering Laboratory for Robot Visual Perception and Control.
    His research interests include autonomous mobile robot and machine vision.
  prefs: []
  type: TYPE_NORMAL
- en: )Xiaoyan Liu received her Ph.D. degree of Process and System Engineering in
    2005 from Otto-von-Guericke University Magdeburg, Germany. She is currently a
    professor of Hunan University. Her research interests include machine vision and
    pattern recognition.
  prefs: []
  type: TYPE_NORMAL
- en: )Zhengeng Yang received the B.S. and M.S. degrees from Central South University,
    Changsha, China, in 2009 and 2012, respectively. He received the Phd degree from
    Hunan University, Changsha, China, in 2020\. He is currently a post-doctor researcher
    at Hunan University, Changsha. He was a Visiting Scholar with the University of
    Pittsburgh, Pittsburgh, PA during 2018 -2020\. His research interests include
    computer vision, image analysis, and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: )Wei Sun received the M.S. and Ph.D. degrees of Control Science and Technology
    from the Hunan University, Changsha, China, in 1999 and 2002, respectively. He
    is currently a Professor at Hunan University. His research interests include artificial
    intelligence, robot control, complex mechanical and electrical control systems,
    and automotive electronics.
  prefs: []
  type: TYPE_NORMAL
- en: )Ajmal Mian is a Professor of computer science with The University of Western
    Australia. His research interests include 3D computer vision, machine learning,
    point cloud analysis, human action recognition, and video description. He is Fellow
    of the International Association for Pattern Recognition and has received several
    awards including the HBF Mid-Career Scientist of the Year Award, the West Australian
    Early Career Scientist of the Year Award, the Aspire Professional Development
    Award, the Vice-Chancellors Mid-Career Research Award, the Outstanding Young Investigator
    Award, the IAPR Best Scientific Paper Award, the EH Thompson Award, and excellence
    in Research Supervision Award. He has received three prestigious fellowships and
    several major research grants from the Australian Research Council, the National
    Health and Medical Research Council of Australia and the US Dept of Defense DARPA
    with a total funding of over $40 Million. He serves as a Senior Editor for the
    IEEE Transactions on Neural Networks and Learning Systems, and Associate Editor
    for the IEEE Transactions on Image Processing, and the Pattern Recognition Journal.
  prefs: []
  type: TYPE_NORMAL
