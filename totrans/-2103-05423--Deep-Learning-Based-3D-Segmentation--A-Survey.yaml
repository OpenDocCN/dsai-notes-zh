- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:56:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.05423] Deep Learning Based 3D Segmentation: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.05423] 基于深度学习的3D分割：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05423](https://ar5iv.labs.arxiv.org/html/2103.05423)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05423](https://ar5iv.labs.arxiv.org/html/2103.05423)
- en: '[type=editor, auid=000, bioid=1, orcid= 0000-0003-2916-3068]'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[类型=编辑，auid=000，bioid=1，orcid=0000-0003-2916-3068]'
- en: '[type=editor,auid=000,bioid=1, orcid = 0000-0003-1973-6766]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[类型=编辑，auid=000，bioid=1，orcid=0000-0003-1973-6766]'
- en: '[type=editor,auid=000,bioid=1,] [type=editor,auid=000,bioid=1,] [type=editor,auid=000,bioid=1,]
    [type=editor,auid=000,bioid=1,]'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[类型=编辑, auid=000, bioid=1,] [类型=编辑, auid=000, bioid=1,] [类型=编辑, auid=000, bioid=1,]
    [类型=编辑, auid=000, bioid=1,]'
- en: 1]organization=Hunan University, addressline=Lushan South Rd., Yuelu Dist.,
    city=Changsha, postcode=410082, state=Hunan, country=China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1] 机构=湖南大学，地址=岳麓区麓山南路，城市=长沙，邮政编码=410082，省份=湖南，国家=中国
- en: 2]organization=University of Western Australia, addressline=35 Stirling Hwy,
    city=Perth, postcode=6009, state=WA, country=Australia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2] 机构=西澳大利亚大学，地址=35 Stirling Hwy，城市=珀斯，邮政编码=6009，省份=WA，国家=澳大利亚
- en: 'Deep Learning Based 3D Segmentation: A Survey'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的3D分割：综述
- en: Yong He h.yong@hnu.edu.cn    Hongshan Yu yuhongshancn@hotmail.com    Xiaoyan
    Liu xiaoyan.liu@hnu.edu.cn    Zhengeng Yang yzg050215@163.com    Wei Sun david-sun@126.com
       Ajaml Mian ajmal.mian@uwa.edu.au [ [
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Yong He h.yong@hnu.edu.cn    Hongshan Yu yuhongshancn@hotmail.com    Xiaoyan
    Liu xiaoyan.liu@hnu.edu.cn    Zhengeng Yang yzg050215@163.com    Wei Sun david-sun@126.com
       Ajaml Mian ajmal.mian@uwa.edu.au [ [
- en: A B S T R A C T
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 3D segmentation is a fundamental and challenging problem in computer vision
    with applications in autonomous driving, robotics, augmented reality and medical
    image analysis. It has received significant attention from the computer vision,
    graphics and machine learning communities. Conventional methods for 3D segmentation,
    based on hand-crafted features and machine learning classifiers, lack generalization
    ability. Driven by their success in 2D computer vision, deep learning techniques
    have recently become the tool of choice for 3D segmentation tasks. This has led
    to an influx of a large number of methods in the literature that have been evaluated
    on different benchmark datasets. Whereas survey papers on RGB-D and point cloud
    segmentation exist, there is a lack of an in-depth and recent survey that covers
    all 3D data modalities and application domains. This paper fills the gap and provides
    a comprehensive survey of the recent progress made in deep learning based 3D segmentation.
    It covers over 180 works, analyzes their strengths and limitations and discusses
    their competitive results on benchmark datasets. The survey provides a summary
    of the most commonly used pipelines and finally highlights promising research
    directions for the future.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 3D分割是计算机视觉中的一个基础而具有挑战性的问题，应用于自动驾驶、机器人、增强现实和医学图像分析。它已经引起了计算机视觉、图形学和机器学习社区的广泛关注。基于手工特征和机器学习分类器的传统3D分割方法缺乏泛化能力。受益于在2D计算机视觉中的成功，深度学习技术近年来已成为3D分割任务的首选工具。这导致了大量文献中提出了许多方法，并在不同的基准数据集上进行了评估。尽管RGB-D和点云分割的综述文章已有存在，但缺乏对所有3D数据模态和应用领域进行深入和最新综述的文献。本文填补了这一空白，提供了基于深度学习的3D分割的最新进展的全面综述。它涵盖了超过180篇工作，分析了它们的优缺点，并讨论了它们在基准数据集上的竞争性结果。该综述总结了最常用的处理流程，并最终突出显示了未来有前景的研究方向。
- en: 'keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 3D data\sep3D semantic segmentation\sep3D instance segmentation\sep3D part segmentation\sep3D
    video segmentation\sep3D semantic map\sepDeep learning\sep
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 3D数据\sep3D语义分割\sep3D实例分割\sep3D部件分割\sep3D视频分割\sep3D语义地图\sep深度学习\sep
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Segmentation of 3D scenes is a fundamental and challenging problem in computer
    vision as well as computer graphics. The objective of 3D segmentation is to build
    computational techniques that predict the fine-grained labels of objects in a
    3D scene for a wide range of applications such as autonomous driving, mobile robots,
    industrial control, augmented reality and medical image analysis. As illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey"), 3D segmentation can be divided into three types: semantic, instance
    and part segmentation. Semantic segmentation aims to predict object class labels
    such as table and chair. Instance segmentation additionally distinguishes between
    different instances of the same class labels e.g. table one/two and chair one/two.
    Part segmentation aims to decompose instances further into their different components
    such as armrests, legs and backrest of the same chair.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '3D 场景的分割是计算机视觉和计算机图形学中的一个基础且具有挑战性的问题。3D 分割的目标是建立计算技术，以预测 3D 场景中对象的细粒度标签，应用范围广泛，如自动驾驶、移动机器人、工业控制、增强现实和医学图像分析。如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation: A
    Survey") 所示，3D 分割可以分为三种类型：语义分割、实例分割和部件分割。语义分割旨在预测对象类别标签，如桌子和椅子。实例分割额外区分相同类别标签的不同实例，例如桌子一/二和椅子一/二。部件分割旨在将实例进一步分解为其不同组件，例如同一椅子的扶手、腿和靠背。'
- en: Compared to conventional single view 2D segmentation, 3D segmentation gives
    a more comprehensive understanding of a scene, since 3D data (e.g. RGB-D, point
    cloud, voxel, mesh, 3D video) contain richer geometric, shape, and scale information
    with less background noise. Moreover, the representation of 3D data, for example
    in the form of projected images, has more semantic information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的单视图 2D 分割相比，3D 分割提供了对场景的更全面理解，因为 3D 数据（例如 RGB-D、点云、体素、网格、3D 视频）包含更丰富的几何、形状和尺度信息，并且背景噪声较少。此外，3D
    数据的表示，例如以投影图像的形式，包含更多的语义信息。
- en: '![Refer to caption](img/5adef999a7403ba147c6589c55015740.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5adef999a7403ba147c6589c55015740.png)'
- en: 'Figure 1: The main five types of 3D data: (a) RGB-D image, (b) projected images,
    (c) voxels, (d) mesh, and (d) point. Types of 3D segmentation: (f) 3D semantic
    segmentation, (g) 3D instance segmentation, and (h) 3D part segmentation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：主要的五种 3D 数据类型：(a) RGB-D 图像，(b) 投影图像，(c) 体素，(d) 网格，以及 (e) 点。3D 分割的类型：(f)
    3D 语义分割，(g) 3D 实例分割，以及 (h) 3D 部件分割。
- en: '![Refer to caption](img/938b87a3bbb92dfc3903f056d414a72b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/938b87a3bbb92dfc3903f056d414a72b.png)'
- en: 'Figure 2: Complete overview of the survey paper.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：调查论文的完整概述。
- en: Recently, deep learning techniques have dominated many research areas including
    computer vision and natural language processing. Motivated by its success in learning
    powerful features, deep learning for 3D segmentation has also attracted a growing
    interest from the research community over the past decade. However, 3D deep learning
    methods still face many unsolved challenges. For example, irregularity of point
    clouds makes it difficult to exploit local features and converting them to high-resolution
    voxels comes with a huge computational burden.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习技术在包括计算机视觉和自然语言处理在内的许多研究领域中占据了主导地位。受到其在学习强大特征方面成功的激励，过去十年间，3D 分割的深度学习也引起了研究界日益增长的兴趣。然而，3D
    深度学习方法仍面临许多未解决的挑战。例如，点云的不规则性使得利用局部特征变得困难，并且将其转换为高分辨率体素会带来巨大的计算负担。
- en: 'This paper provides a comprehensive survey of recent progress in deep learning
    methods for 3D segmentation. It focuses on analyzing commonly used building blocks,
    convolution kernels and complete architectures pointing out the pros and cons
    in each case. The survey covers over 180 representative papers published in the
    last five years. Although some notable 3D segmentation surveys have been released
    including RGB-D semantic segmentation Fooladgar and Kasaei ([2020](#bib.bib30)),
    remote sensing imagery segmentation Yuan et al. ([2021](#bib.bib186)), point clouds
    segmentation Xie et al. ([2020a](#bib.bib170)), Guo et al. ([2020](#bib.bib36)),
    Liu et al. ([2019a](#bib.bib95)), Bello et al. ([2020](#bib.bib4)), Naseer et al.
    ([2018](#bib.bib109)), Ioannidou et al. ([2017](#bib.bib56)), these surveys do
    not comprehensively cover all 3D data types and typical application domains. Most
    importantly, these surveys do not focus on 3D segmentation but give a general
    survey of deep learning from point clouds Guo et al. ([2020](#bib.bib36)), Liu
    et al. ([2019a](#bib.bib95)), Bello et al. ([2020](#bib.bib4)), Naseer et al.
    ([2018](#bib.bib109)), Ioannidou et al. ([2017](#bib.bib56)). Given the importance
    of the three segmentation tasks, this paper focuses exclusively on deep learning
    techniques for 3D segmentation. The contributions of this paper are summarized
    as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了有关3D分割的深度学习方法的最新进展的全面调查。它专注于分析常用的构建模块、卷积核和完整架构，指出每种情况的优缺点。该调查涵盖了过去五年中发表的180多篇具有代表性的论文。尽管已经发布了一些显著的3D分割调查，包括RGB-D语义分割Fooladgar和Kasaei（[2020](#bib.bib30)）、遥感图像分割Yuan等（[2021](#bib.bib186)）、点云分割Xie等（[2020a](#bib.bib170)）、Guo等（[2020](#bib.bib36)）、Liu等（[2019a](#bib.bib95)）、Bello等（[2020](#bib.bib4)）、Naseer等（[2018](#bib.bib109)）、Ioannidou等（[2017](#bib.bib56)），这些调查并未全面涵盖所有3D数据类型和典型应用领域。最重要的是，这些调查并未专注于3D分割，而是对从点云Guo等（[2020](#bib.bib36)）、Liu等（[2019a](#bib.bib95)）、Bello等（[2020](#bib.bib4)）、Naseer等（[2018](#bib.bib109)）、Ioannidou等（[2017](#bib.bib56)）等给出的深度学习进行了总体调查。鉴于这三个分割任务的重要性，本文专注于3D分割的深度学习技术。本文的贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first survey paper to comprehensively
    cover deep learning methods on 3D segmentation covering all 3D data representations,
    including RGB-D, projected images, voxels, point clouds, meshes, and 3D videos.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇全面涵盖3D分割中深度学习方法的调查论文，涵盖了所有3D数据表示，包括RGB-D、投影图像、体素、点云、网格和3D视频。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey provides an in-depth analysis of the relative advantages and disadvantages
    of different types of 3D data segmentation methods.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查对不同类型的3D数据分割方法的相对优缺点进行了深入分析。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unlike existing reviews, this survey papers focuses on deep learning methods
    designed specifically for 3D segmentation and also discusses typical segmentation
    pipelines as well as application domains.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有的评述不同，本调查论文专注于专门为3D分割设计的深度学习方法，并讨论了典型的分割流程以及应用领域。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Finally, this survey provides comprehensive comparisons of existing methods
    on several public benchmark 3D datasets, draw interesting conclusions and identify
    promising future research directions.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，这项调查对多个公共基准3D数据集上的现有方法进行了全面比较，得出了有趣的结论，并确定了有前景的未来研究方向。
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey") shows a snapshot of how this survey is organized. Section [2](#S2 "2
    Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation: A Survey")
    introduces some terminology and background concepts, including popular 3D datasets
    and evaluation metrics for 3D segmentation. Section [3](#S3 "3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey") reviews methods for 3D semantic
    segmentation whereas Section [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey") reviews methods for 3D instance segmentation.
    Section [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") provides a survey of existing methods for 3D part segmentation. Section
    [6](#S6 "6 Applications of 3D Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") reviews the 3D segmentation methods used in some common application
    areas including 3D video segmentation and 3D semantic map. Section [7](#S7 "7
    Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey") presents
    performance comparison between 3D segmentation methods on several popular datasets,
    and gives corresponding data analysis. Finally, Section [8](#S8 "8 Discussion
    and Conclusion ‣ Deep Learning Based 3D Segmentation: A Survey") identifies promising
    future research directions and concludes the paper.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/87380b46ede615306a8ddc74295a6939.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Annotated examples from (a) S3DIS, (b) Semantic3D, (c) SemanticKITTI
    for 3D semantic segmentation, (d) ScanNet for 3D instance segmentation, and (e)
    ShapeNet for 3D part segmentation. See Table [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation
    Dataset ‣ 2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation:
    A Survey") for a summary of these datasets.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 2 Terminology and Background Concept
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section introduces some terminologies and background concepts, including
    3D data representation, popular 3D segmentation datasets and evaluation metrics
    to help the reader easily navigate through the field of 3D segmentation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 3D Segmentation Dataset
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets are critical to train and test 3D segmentation algorithms using deep
    learning. However, it is cumbersome and expensive to privately gather and annotate
    datasets as it needs domain expertise, high quality sensors and processing equipment.
    Thus, building on public datasets is an ideal way to reduce the cost. Following
    this way has another advantage for the community that it provides a fair comparison
    between algorithms. Table [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation Dataset ‣
    2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation: A
    Survey") summarizes some of the most popular and typical datasets with respect
    to the sensor type, data size and format, scene class and annotation method.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'These datasets are acquired for *3D semantic segmentation* by different type
    of sensors, including RGB-D cameras Silberman and Fergus ([2011](#bib.bib129)),
    Silberman et al. ([2012](#bib.bib130)), Song et al. ([2015](#bib.bib132)), Hua
    et al. ([2016](#bib.bib51)), Dai et al. ([2017](#bib.bib18)), mobile laser scanner
    Roynard et al. ([2018](#bib.bib125)), Behley et al. ([2019](#bib.bib3)), static
    terrestrial scanner Hackel et al. ([2017](#bib.bib38)) and unreal engine Brodeur
    et al. ([2017](#bib.bib7)), Wu et al. ([2018b](#bib.bib164)) and other 3D scanners
    Armeni et al. ([2016](#bib.bib2)), Chang et al. ([2017](#bib.bib9)). Among these,
    the ones obtained from unreal engine are synthetic datasets Brodeur et al. ([2017](#bib.bib7))
    Wu et al. ([2018b](#bib.bib164)) that do not require expensive equipment or annotation
    time. These are also rich in categories and quantities of objects. Synthetic datasets
    have complete 360 degree 3D objects with no occlusion effects or noise compared
    to the real-world datasets which are noisy and contain occlusions Silberman and
    Fergus ([2011](#bib.bib129)), Silberman et al. ([2012](#bib.bib130)), Song et al.
    ([2015](#bib.bib132)), Hua et al. ([2016](#bib.bib51)), Dai et al. ([2017](#bib.bib18)),
    Roynard et al. ([2018](#bib.bib125)), Behley et al. ([2019](#bib.bib3)), Armeni
    et al. ([2016](#bib.bib2)), Hackel et al. ([2017](#bib.bib38)), Chang et al. ([2017](#bib.bib9)).
    For *3D instance segmentation*, there are limited 3D datasets, such as ScanNet
    Dai et al. ([2017](#bib.bib18)) and S3DIS Armeni et al. ([2016](#bib.bib2)). These
    two datasets contain scans of real-world indoor scenes obtained by RGB-D cameras
    or Matterport separately. For *3D part segmentation*, the Princeton Segmentation
    Benchmark (PSB) Chen et al. ([2009](#bib.bib11)), COSEG Wang et al. ([2012](#bib.bib153))
    and ShapeNet Yi et al. ([2016](#bib.bib181)) are three of the most popular datasets.
    Below, we introduce five famous segmentation datasets in detail, including S3DIS
    Armeni et al. ([2016](#bib.bib2)), ScanNet Dai et al. ([2017](#bib.bib18)), Semantic3D
    Hackel et al. ([2017](#bib.bib38)), SemanticKITTI Chang et al. ([2017](#bib.bib9))
    and ShapeNet Yi et al. ([2016](#bib.bib181)). Some examples with annotation from
    these datasets are shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep
    Learning Based 3D Segmentation: A Survey").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集用于*3D语义分割*，由不同类型的传感器获取，包括RGB-D摄像头 Silberman 和 Fergus ([2011](#bib.bib129))、Silberman
    等人 ([2012](#bib.bib130))、Song 等人 ([2015](#bib.bib132))、Hua 等人 ([2016](#bib.bib51))、Dai
    等人 ([2017](#bib.bib18))、移动激光扫描仪 Roynard 等人 ([2018](#bib.bib125))、Behley 等人 ([2019](#bib.bib3))、静态地面扫描仪
    Hackel 等人 ([2017](#bib.bib38)) 和虚幻引擎 Brodeur 等人 ([2017](#bib.bib7))、Wu 等人 ([2018b](#bib.bib164))
    及其他 3D 扫描仪 Armeni 等人 ([2016](#bib.bib2))、Chang 等人 ([2017](#bib.bib9))。其中，通过虚幻引擎获得的数据集是合成数据集
    Brodeur 等人 ([2017](#bib.bib7)) 和 Wu 等人 ([2018b](#bib.bib164))，这些数据集无需昂贵的设备或标注时间，且在对象类别和数量上也很丰富。与真实世界数据集相比，合成数据集具有完整的360度3D对象，没有遮挡效应或噪声，而真实数据集则通常有噪声和遮挡效应
    Silberman 和 Fergus ([2011](#bib.bib129))、Silberman 等人 ([2012](#bib.bib130))、Song
    等人 ([2015](#bib.bib132))、Hua 等人 ([2016](#bib.bib51))、Dai 等人 ([2017](#bib.bib18))、Roynard
    等人 ([2018](#bib.bib125))、Behley 等人 ([2019](#bib.bib3))、Armeni 等人 ([2016](#bib.bib2))、Hackel
    等人 ([2017](#bib.bib38))、Chang 等人 ([2017](#bib.bib9))。对于*3D实例分割*，可用的3D数据集有限，例如
    ScanNet Dai 等人 ([2017](#bib.bib18)) 和 S3DIS Armeni 等人 ([2016](#bib.bib2))。这两个数据集分别包含由RGB-D摄像头或Matterport扫描仪获取的真实世界室内场景扫描。对于*3D部件分割*，普林斯顿分割基准（PSB）Chen
    等人 ([2009](#bib.bib11))、COSEG Wang 等人 ([2012](#bib.bib153)) 和 ShapeNet Yi 等人 ([2016](#bib.bib181))
    是三个最受欢迎的数据集。下面，我们详细介绍五个著名的分割数据集，包括 S3DIS Armeni 等人 ([2016](#bib.bib2))、ScanNet
    Dai 等人 ([2017](#bib.bib18))、Semantic3D Hackel 等人 ([2017](#bib.bib38))、SemanticKITTI
    Chang 等人 ([2017](#bib.bib9)) 和 ShapeNet Yi 等人 ([2016](#bib.bib181))。图[3](#S1.F3
    "图 3 ‣ 1 引言 ‣ 基于深度学习的3D分割：综述")展示了这些数据集的注释示例。
- en: 'S3DIS: In this dataset, the complete point clouds are obtained without any
    manual intervention using the Matterport scanner. The dataset consists of 271
    rooms belonging to 6 large-scale indoor scenes from 3 different buildings (total
    of 6020 square meters). These areas mainly include offices, educational and exhibition
    spaces, and conference rooms etc.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: S3DIS：在此数据集中，完整的点云数据是通过Matterport扫描仪自动获取的，无需人工干预。该数据集包括来自3座不同建筑物的6个大规模室内场景中的271个房间（总面积6020平方米）。这些区域主要包括办公室、教育和展览空间、会议室等。
- en: Semantic3D comprises a total of around 4 billion 3D points acquired with static
    terrestrial laser scanners, covering up to 160×240×30 meters in real-world 3D
    space. Point clouds belong to 8 classes (e.g. urban and rural) and contain 3D
    coordinates, RGB information, and intensity. Unlike 2D annotation strategies,
    3D data labeling is easily amenable to over-segmentation where each point is individually
    assigned to a class label.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Semantic3D 总共包含约 40 亿个 3D 点，这些点是通过静态地面激光扫描仪获得的，覆盖了真实世界 3D 空间中的最大范围为 160×240×30
    米。点云属于 8 个类别（如城市和乡村），并包含 3D 坐标、RGB 信息和强度。与 2D 注释策略不同，3D 数据标注很容易出现过度分割，每个点都会被单独分配到一个类别标签。
- en: SemanticKITTI is a large outdoor dataset containing detailed point-wise annotation
    of 28 classes. Building on the KITTI vision benchmark Geiger et al. ([2012](#bib.bib31)),
    SemanticKITTI contains annotations of all 22 sequences of this benchmark consisting
    of 43K scans. Moreover, the dataset contains labels for the complete horizontal
    360 filed-of-view of the rotating laser sensor.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SemanticKITTI 是一个大型户外数据集，包含28个类别的详细逐点注释。基于 KITTI 视觉基准 Geiger 等人（[2012](#bib.bib31)），SemanticKITTI
    包含这个基准的所有22个序列的注释，共有43K个扫描。此外，该数据集包含了旋转激光传感器完整水平360度视场的标签。
- en: ScanNet dataset is particularly valuable for research in scene understanding
    as its annotations contain estimated calibration parameters, camera poses, 3D
    surface reconstruction, textured meshes, dense object level semantic segmentation,
    and CAD models. The dataset comprises annotated RGB-D scans of real-world environments.
    There are 2.5M RGB-D images in 1513 scans acquired in 707 distinct places. After
    RGB-D image processing, annotation human intelligence tasks were performed using
    the Amazon Mechanical Turk.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ScanNet 数据集对于场景理解研究特别有价值，因为其注释包含估算的校准参数、相机姿态、3D 表面重建、纹理网格、密集的物体级语义分割和 CAD 模型。该数据集包含了真实世界环境的
    RGB-D 扫描注释。共有 2.5M 个 RGB-D 图像，分布在 707 个不同的地方，共 1513 个扫描。经过 RGB-D 图像处理后，使用 Amazon
    Mechanical Turk 执行了注释的人工智能任务。
- en: ShapeNet dataset has a novel scalable method for efficient and accurate geometric
    annotation of massive 3D shape collections. The novel technical innovations explicitly
    model and lessen the human cost of the annotation effort. Researchers create detailed
    point-wise labeling of 31963 models in shape categories in ShapeNetCore and combine
    feature-based classifiers, point-to-point correspondences, and shape-to-shape
    similarities into a single CRF optimization over the network of shapes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ShapeNet 数据集具有一种新颖的可扩展方法，用于高效且准确地对大规模 3D 形状集合进行几何注释。这些新颖的技术创新显著减少了注释工作的人工成本。研究人员在
    ShapeNetCore 中创建了 31963 个模型的详细逐点标注，并将基于特征的分类器、点对点对应关系和形状对形状的相似性结合到一个 CRF 优化模型中。
- en: 'Table 1: Summary of popular datasets for 3D segmentation datasets including
    the sensor, type, size, object class, number of classes (shown in brackets), and
    annotation method. S←synthetic environment. R←real-world environment. Kf←thousand
    frames. s←scan. Mp←million points. the symbol ‘–’ means information unavailable.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：流行的 3D 分割数据集的总结，包括传感器、类型、尺寸、物体类别、类别数量（括号中显示）和注释方法。S←合成环境。R←真实世界环境。Kf←千帧。s←扫描。Mp←百万点。符号
    ‘–’ 表示信息不可用。
- en: '| Dataset | Sensors | Type | Size | Scene class (number) | Annotation method
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 传感器 | 类型 | 尺寸 | 场景类别（数量） | 注释方法 |'
- en: '| Datasets for 3D semantic segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 用于 3D 语义分割的数据集 |'
- en: '| NYUv1 Silberman and Fergus ([2011](#bib.bib129)) | Microsoft Kinect v1 |
    R | 2347f | bedroom, cafe, kitchen, etc. (7) | Condition Random Field-based model
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| NYUv1 Silberman 和 Fergus（[2011](#bib.bib129)） | Microsoft Kinect v1 | R |
    2347f | 卧室、咖啡馆、厨房等（7） | 基于条件随机场的模型 |'
- en: '| NYUv2 Silberman et al. ([2012](#bib.bib130)) | Microsoft Kinect v1 | R |
    1449f | bedroom, cafe, kitchen, etc. (26) | 2D annotation from AMK |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| NYUv2 Silberman 等人（[2012](#bib.bib130)） | Microsoft Kinect v1 | R | 1449f
    | 卧室、咖啡馆、厨房等（26） | 来自 AMK 的 2D 注释 |'
- en: '| SUN RGB-D Song et al. ([2015](#bib.bib132)) | RealSense, Xtion LIVE PRO,
    MKv1/2 | R | 10355f | objects, room layouts, etc.(47) | 2D/3Dpolygons +3D bounding
    box |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D Song 等人（[2015](#bib.bib132)） | RealSense, Xtion LIVE PRO, MKv1/2
    | R | 10355f | 物体、房间布局等（47） | 2D/3D多边形 + 3D 边界框 |'
- en: '| SceneNN Hua et al. ([2016](#bib.bib51)) | Asus Xtion PRO, MK v2 | R | 100s
    | bedroom, office, apartment, etc.(-) | 3D Labels project to 2D frames |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| SceneNN Hua 等人（[2016](#bib.bib51)） | Asus Xtion PRO, MK v2 | R | 100s | 卧室、办公室、公寓等（-）
    | 3D 标签投影到 2D 帧 |'
- en: '| RueMonge2014 Riemenschneider et al. ([2014](#bib.bib123)) | – | R | 428s
    | window, wall, balcony, door, etc(7) | Multi-view semantic labelling + CRF |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| RueMonge2014 Riemenschneider et al. ([2014](#bib.bib123)) | – | R | 428s
    | 窗户、墙壁、阳台、门等(7) | 多视角语义标注 + CRF |'
- en: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital structure sensor | R
    | 2.5Mf | office, apartment, bathroom, etc(19) | 3D labels project to 2D frames
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital 结构传感器 | R | 2.5Mf | 办公室、公寓、浴室等(19)
    | 3D 标签投影到 2D 框架上 |'
- en: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport camera | R | 70496f
    | conference rooms, offices, etc(11) | Hierarchical labeling |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport 摄像头 | R | 70496f | 会议室、办公室等(11)
    | 层级标注 |'
- en: '| Semantic3D Hackel et al. ([2017](#bib.bib38)) | Terrestrial laser scanner
    | R | 1660Mp | farms, town hall, sport fields, etc (8) | Three baseline methods
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Semantic3D Hackel et al. ([2017](#bib.bib38)) | 地面激光扫描仪 | R | 1660Mp | 农场、市政厅、运动场等(8)
    | 三种基准方法 |'
- en: '| NPM3D Roynard et al. ([2018](#bib.bib125)) | Velodyne HDL-32E LiDAR | R |
    143.1Mp | ground, vehicle, hunman, etc (50) | Human labeling |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| NPM3D Roynard et al. ([2018](#bib.bib125)) | Velodyne HDL-32E LiDAR | R |
    143.1Mp | 地面、车辆、人等(50) | 人工标注 |'
- en: '| SemanticKITTI Behley et al. ([2019](#bib.bib3)) | Velodyne HDL-64E | R |
    43Ks | ground, vehicle, hunman, etc(28) | Multi-scans semantic labelling |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SemanticKITTI Behley et al. ([2019](#bib.bib3)) | Velodyne HDL-64E | R |
    43Ks | 地面、车辆、人等(28) | 多扫描语义标注 |'
- en: '| Matterport3D Chang et al. ([2017](#bib.bib9)) | Matterport camera | R | 194.4Kf
    | various rooms (90) | Hierarchical labeling |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Matterport3D Chang et al. ([2017](#bib.bib9)) | Matterport 摄像头 | R | 194.4Kf
    | 各种房间 (90) | 层级标注 |'
- en: '| HoME Brodeur et al. ([2017](#bib.bib7)) | Planner5D platform | S | 45622f
    | rooms, object and etc.(84) | SSCNet+ a short text description |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| HoME Brodeur et al. ([2017](#bib.bib7)) | Planner5D 平台 | S | 45622f | 房间、物体等(84)
    | SSCNet+ 简短文本描述 |'
- en: '| House3D Wu et al. ([2018b](#bib.bib164)) | Planner5D platform | S | 45622f
    | rooms, object and etc.(84) | SSCNet+3 ways |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| House3D Wu et al. ([2018b](#bib.bib164)) | Planner5D 平台 | S | 45622f | 房间、物体等(84)
    | SSCNet+3 种方式 |'
- en: '| Datasets for 3D instance segmentation |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 3D 实例分割的数据集 |'
- en: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital structure sensor | R
    | 2.5Mf | office, apartment, bathroom, etc(19) | 3D labels project to 2D frames
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital 结构传感器 | R | 2.5Mf | 办公室、公寓、浴室等(19)
    | 3D 标签投影到 2D 框架上 |'
- en: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport camera | R | 70496f
    | conference rooms, offices, etc(11) | Active learning method |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport 摄像头 | R | 70496f | 会议室、办公室等(11)
    | 主动学习方法 |'
- en: '| Datasets for 3D part segmentation |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 3D 部件分割的数据集 |'
- en: '| ShapeNet Yi et al. ([2016](#bib.bib181)) | – | S | 31963s | transportation,
    tool, etc.(16) | Propagating human label to shapes |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet Yi et al. ([2016](#bib.bib181)) | – | S | 31963s | 交通工具、工具等(16)
    | 将人工标签传播到形状上 |'
- en: '| PSB Chen et al. ([2009](#bib.bib11)) | Amazon’s Mechanical Turk | S | 380s
    | human,cup, glasses airplane,etc(19) | Interactive segmentation tool |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| PSB Chen et al. ([2009](#bib.bib11)) | 亚马逊 Mechanical Turk | S | 380s | 人、杯子、眼镜、飞机等(19)
    | 交互式分割工具 |'
- en: '| COSEG Wang et al. ([2012](#bib.bib153)) | – | S | 1090s | vase, lamp, guiter,
    etc (11) | semi-supervised learning method |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| COSEG Wang et al. ([2012](#bib.bib153)) | – | S | 1090s | 花瓶、灯、吉他等(11) |
    半监督学习方法 |'
- en: 2.2 Evaluation Metrics
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 评估指标
- en: Different evaluation metrics can assert the validity and superiority of segmentation
    methods including the execution time, memory footprint and accuracy. However,
    few authors provide detailed information about the execution time and memory footprint
    of their method. This paper introduces the accuracy metrics mainly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的评估指标可以验证分割方法的有效性和优越性，包括执行时间、内存占用和准确性。然而，很少有作者提供他们方法的执行时间和内存占用的详细信息。本文主要介绍准确性指标。
- en: For *3D semantic segmentation*, Overall Accuracy (OAcc), mean class Accuracy
    (mAcc) and mean class Intersection over Union (mIoU) are the most frequently used
    metrics to measure the accuracy of segmentation methods. For the sake of explanation,
    we assume that there are a total of $K+1$ classes, and $p_{ij}$ is the minimum
    unit (e.g. pixel, voxel, mesh, point) of class $i$ implied to belong to class
    $j$. In other words, $p_{ii}$ represents true positives, while $p_{ij}$ and $p_{ji}$
    represent false positives and false negatives respectively.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *3D 语义分割*，总体准确率 (OAcc)、均值类别准确率 (mAcc) 和均值类别交并比 (mIoU) 是衡量分割方法准确性的最常用指标。为了说明，我们假设共有
    $K+1$ 个类别，$p_{ij}$ 是类别 $i$ 被暗示为属于类别 $j$ 的最小单元（例如像素、体素、网格、点）。换句话说，$p_{ii}$ 代表真正例，而
    $p_{ij}$ 和 $p_{ji}$ 分别代表假正例和假负例。
- en: Overall Accuracy is a simple metric that computes the ratio between the number
    of truly classified samples and the total number of samples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总体准确率（Overall Accuracy）是一个简单的指标，它计算真正分类样本数与总样本数之间的比例。
- en: '|  | $OAcc=\sum_{i=0}^{K}{\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}}$ |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $OAcc=\sum_{i=0}^{K}{\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}}$ |  |'
- en: Mean Accuracy is an extension of OAcc, computing OAcc in a per-class and then
    averaging over the total number of classes $K$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 平均准确率（Mean Accuracy）是 OAcc 的扩展，它计算每个类别的 OAcc，然后对总类别数 $K$ 取平均。
- en: '|  | $mAcc=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}$
    |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $mAcc=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}$
    |  |'
- en: Mean Intersection over Union is a standard metric for semantic segmentation.
    It computes the intersection ratio between ground truth and predicted value averaged
    over the total number of classes $K$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 平均交并比（Mean Intersection over Union, mIoU）是语义分割的标准指标。它计算地面真实值和预测值之间的交集比例，并对总类别数
    $K$ 取平均。
- en: '|  | $mIoU=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}+\sum_{i=0}^{K}p_{ji}-p_{ii}}$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}+\sum_{i=0}^{K}p_{ji}-p_{ii}}$
    |  |'
- en: For *3D instance segmentation*, Average Precision (AP) and mean class Average
    Precision (mAP) are also frequently used. Assuming $L_{I},I\in[0,K]$ instance
    in every class, and $c_{ij}$ is the amount of point of instance $i$ inferred to
    belong to instance $j$ ($i=j$ represents correct and $i\neq j$ represents incorrect
    segmentations).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *3D 实例分割*，平均精度（AP）和平均类别平均精度（mAP）也经常使用。假设每个类别中有 $L_{I},I\in[0,K]$ 个实例，$c_{ij}$
    是实例 $i$ 被推断为实例 $j$ 的点数（$i=j$ 代表正确，$i\neq j$ 代表错误分割）。
- en: Average Precision is another simple metric for segmentation that computes the
    ratio between true positives and the total number of positive samples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度（Average Precision, AP）是另一种简单的分割指标，它计算真正例与正样本总数之间的比例。
- en: '|  | $AP=\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP=\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
- en: Mean Average precision is an extension of AP which computes per-class AP and
    then averages over the total number of classes $K$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 平均平均精度（Mean Average Precision, AP）是 AP 的扩展，它计算每个类别的 AP，然后对总类别数 $K$ 取平均。
- en: '|  | $mAP=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $mAP=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
- en: For *3D part segmentation*, overall average category Intersection over Union
    ($mIoU_{cat}$) and overall average instance Intersection over Union ($mIoU_{ins}$)
    are most frequently used. For the sake of explanation, we assume $M_{J},J\in[0,L_{I}]$
    parts in every instance, and $q_{ij}$ as the total number of points in part $i$
    inferred to belong to part $j$. Hence, $q_{ii}$ represents the number of true
    positive, while $q_{ij}$ and $q_{ji}$ are false positives and false negative respectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *3D 部分分割*，总体类别平均交并比（$mIoU_{cat}$）和总体实例平均交并比（$mIoU_{ins}$）是最常用的指标。为了说明，我们假设每个实例中有
    $M_{J},J\in[0,L_{I}]$ 个部分，并且 $q_{ij}$ 是推断属于部分 $j$ 的部分 $i$ 的点总数。因此，$q_{ii}$ 代表真正例，而
    $q_{ij}$ 和 $q_{ji}$ 分别代表假阳性和假阴性。
- en: Overall average category Intersection over Union is an evaluation metric for
    part segmentation that measures the mean IoU averaged across K classes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总体类别平均交并比（Overall average category Intersection over Union）是部分分割的评估指标，它测量跨 K
    类的平均 IoU。
- en: '|  | $mIoU_{cat}=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU_{cat}=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
- en: Overall average instance Intersection over Union, for part segmentation, measures
    the mean IoU across all instances.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总体实例平均交并比（$mIoU_{ins}$）在部分分割中测量所有实例的平均 IoU。
- en: '|  | $mIoU_{ins}=\frac{1}{\sum_{I=0}^{K}L_{I}+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU_{ins}=\frac{1}{\sum_{I=0}^{K}L_{I}+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
- en: 3 3D Semantic segmentation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D 语义分割
- en: 'Many deep learning methods on 3D semantic segmentation have been proposed in
    the literature. These methods can be divided into five categories according to
    the data representation used, namely, RGB-D image based, projected images based,
    voxel based, point based, 3D video and other representations based. Point based
    methods can be further categorized, based on the network architecture, into Multiple
    Layer Perceptron (MLP) based, Point Convolution based and Graph Convolution based
    and Point Transformer based methods. Figure [4](#S3.F4 "Figure 4 ‣ 3 3D Semantic
    segmentation ‣ Deep Learning Based 3D Segmentation: A Survey") shows the milestones
    of deep learning on 3D semantic segmentation in recent years.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了许多基于深度学习的 3D 语义分割方法。这些方法可以根据使用的数据表示分为五类，即基于 RGB-D 图像、基于投影图像、基于体素、基于点云、基于
    3D 视频和其他表示方法。基于点云的方法可以进一步分类，根据网络架构分为多层感知机（MLP）基、点卷积基、图卷积基和点变换器基的方法。图 [4](#S3.F4
    "图 4 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述") 显示了近年来深度学习在 3D 语义分割方面的里程碑。
- en: '![Refer to caption](img/1f5a4979d0c6a9c762dd7dce7e3a4b5c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1f5a4979d0c6a9c762dd7dce7e3a4b5c.png)'
- en: 'Figure 4: Milestones of deep learning based 3D semantic segmentation methods.
    Note that the arrow (timeline) goes anti-clockwise'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于深度学习的 3D 语义分割方法的里程碑。注意箭头（时间线）是逆时针方向
- en: 3.1 RGB-D Based
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于 RGB-D 的
- en: 'The depth map in an RGB-D image contains geometric information about the real-world
    which is useful to distinguish foreground objects from background, hence providing
    opportunities to improve the segmentation accuracy. In this category, generally
    the classical two-channel network is used to extract features from RGB and depth
    images separately. However, this simple framework is not powerful enough to extract
    rich and refined features. To this end, researchers have integrated several additional
    modules into the above simple two-channel framework to improve the performance
    by learning rich *context* and *geometric* information that are crucial for semantic
    segmentation. These modules can be roughly divided into six categories: multi-task
    learning, depth encoding, multi-scale network, novel neural network architectures,
    data/feature/score level fusion and post-processing (see Figure [5](#S3.F5 "Figure
    5 ‣ 3.1 RGB-D Based ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")). RGB-D image based semantic segmentation methods are summarized in
    Table [2](#S3.T2 "Table 2 ‣ 3.1 RGB-D Based ‣ 3 3D Semantic segmentation ‣ Deep
    Learning Based 3D Segmentation: A Survey").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-D 图像中的深度图包含了关于现实世界的几何信息，这对区分前景物体和背景非常有用，从而提供了提高分割准确性的机会。在这一类别中，通常使用经典的双通道网络分别从
    RGB 和深度图像中提取特征。然而，这种简单的框架并不够强大，无法提取丰富和精细的特征。为此，研究人员在上述简单的双通道框架中集成了几个附加模块，通过学习对语义分割至关重要的丰富*上下文*和*几何*信息来提高性能。这些模块大致可以分为六类：多任务学习、深度编码、多尺度网络、新型神经网络架构、数据/特征/得分级融合和后处理（见图[5](#S3.F5
    "图 5 ‣ 3.1 基于 RGB-D 的 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述")）。基于 RGB-D 图像的语义分割方法总结见表[2](#S3.T2
    "表 2 ‣ 3.1 基于 RGB-D 的 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述")。
- en: Multi-tasks learning: *Depth estimation* and semantic segmentation are two fundamental
    challenging tasks in computer vision. These tasks are also somewhat related as
    depth variation within an object is small compared to depth variation between
    different objects. Hence, many researchers choose to unite depth estimation task
    and semantic segmentation task. From the view of relationship of the two tasks,
    there are two main types of multi-task leaning framework, cascade and parallel
    framework.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习：*深度估计*和语义分割是计算机视觉中的两个基本且具有挑战性的任务。这些任务之间也有一定的关联，因为在一个物体内部的深度变化相对于不同物体之间的深度变化较小。因此，许多研究人员选择将深度估计任务和语义分割任务结合起来。从这两个任务的关系来看，多任务学习框架主要有两种类型：级联框架和并行框架。
- en: As for the cascade framework, depth estimation task provides depth images for
    semantic segmentation task. For example, Cao et al.  Cao et al. ([2016](#bib.bib8))
    used the deep convolutional neural fields (DCNF) introduced by Liu et al.  Liu
    et al. ([2015](#bib.bib92)) for depth estimation. The estimated depth images and
    RGB images are fed into a two-channel FCN for semantic segmentation. Similarly,
    Guo et al.  Guo and Chen ([2018](#bib.bib35)) adopted the deep network proposed
    by Ivanecky  Ivaneckỳ ([2016](#bib.bib57)) for automatic generating depth images
    from single RGB images, and then proposed a two-channel FCN model on the image
    pair of RGB and predicted depth map for pixel labeling.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 关于级联框架，深度估计任务为语义分割任务提供深度图像。例如，Cao 等人 Cao et al. ([2016](#bib.bib8)) 使用了 Liu
    等人 Liu et al. ([2015](#bib.bib92)) 提出的深度卷积神经场（DCNF）进行深度估计。估计的深度图像和 RGB 图像被输入到一个双通道
    FCN 进行语义分割。同样，Guo 等人 Guo and Chen ([2018](#bib.bib35)) 采用了 Ivanecky Ivaneckỳ ([2016](#bib.bib57))
    提出的深度网络，从单个 RGB 图像中自动生成深度图像，然后提出了一个双通道 FCN 模型用于 RGB 和预测深度图的图像对进行像素标记。
- en: The cascade framework performs depth estimation and semantic segmentation separately,
    which is simultaneously unable to perform end-to-end training for two tasks. Consequently,
    depth estimation task does not get any benefit from semantic segmentation task.
    In contrast, the *parallel* framework performs these two tasks in an unify network,
    which allows two tasks get benefits each other. For instance, Wang et al.  Wang
    et al. ([2015](#bib.bib147)) used Joint Global CNN to exploit pixel-wise depth
    values and semantic labels from RGB images to provide accurate global scale and
    semantic guidance. As well as, they use Joint Region CNN to extract region-wise
    depth values and semantic map from RGB to learn detailed depth and semantic boundaries.
    Mousavian et al.  Mousavian et al. ([2016](#bib.bib107)) presented a multi-scale
    FCN comprising five streams that simultaneously explore depth and semantic features
    at different scales, where the two tasks share the underlying feature representation.
    Liu et al.  Liu et al. ([2018b](#bib.bib94)) proposed a collaborative deconvolutional
    neural network(C-DCNN) to jointly model the two tasks. However, the quality of
    depth maps estimated from RGB images is not as good as the one acquired directly
    from depth sensors. This multi-task learning pipeline has been gradually abandoned
    in RGB-D semantic segmentation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 级联框架将深度估计和语义分割任务分开进行，这使得无法对这两个任务进行端到端的训练。因此，深度估计任务没有从语义分割任务中获得任何好处。相比之下，*并行*框架在一个统一的网络中执行这两个任务，使得两个任务能够互相受益。例如，Wang
    等人 Wang et al. ([2015](#bib.bib147)) 使用联合全局 CNN 从 RGB 图像中提取像素级深度值和语义标签，以提供准确的全局尺度和语义指导。同时，他们使用联合区域
    CNN 从 RGB 中提取区域级深度值和语义图，以学习详细的深度和语义边界。Mousavian 等人 Mousavian et al. ([2016](#bib.bib107))
    提出了一个多尺度 FCN，由五个流组成，同时在不同尺度上探索深度和语义特征，其中两个任务共享底层特征表示。Liu 等人 Liu et al. ([2018b](#bib.bib94))
    提出了一个协作反卷积神经网络（C-DCNN）来联合建模这两个任务。然而，从 RGB 图像估计的深度图质量不如直接从深度传感器获取的深度图。这种多任务学习流程在
    RGB-D 语义分割中逐渐被弃用。
- en: 'Depth Encoding: Conventional 2D CNNs are unable to exploit rich geometric features
    from raw depth images. An alternative way is to encode raw depth images into other
    representations that are suitable to 2D CNN. Hoft et al. Höft et al. ([2014](#bib.bib47))
    used a simplified version of the histogram of oriented gradients (HOG) to represent
    depth channel from RGB-D scenes. Gupta et al. Gupta et al. ([2014](#bib.bib37))
    and Aman et al. Lin et al. ([2017](#bib.bib89)) calculated three new channels
    named horizontal disparity, height above ground and angle with gravity (HHA) from
    the raw depth images. Liu et al. Liu et al. ([2018a](#bib.bib93)) point out a
    limitation of HHA that some scenes may not be enough horizontal and vertical planes.
    Hence, they propose a novel gravity direction detection method with vertical lines
    fitted to learn better representation. Hazirbas et al. Hazirbas et al. ([2016](#bib.bib41))
    also argue that HHA representation has a high computational cost and contains
    less information than the raw depth images. They propose an architecture called
    FuseNet that consists of two encoder-decoder branches, including a depth branch
    and an RGB branch, which directly encodes depth information with a lower computational
    load.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度编码：传统的2D卷积神经网络（CNN）无法从原始深度图像中提取丰富的几何特征。另一种方法是将原始深度图像编码为适合2D CNN的其他表示形式。Hoft等人
    Höft et al. ([2014](#bib.bib47)) 使用了简化版本的方向梯度直方图（HOG）来表示RGB-D场景中的深度通道。Gupta等人
    Gupta et al. ([2014](#bib.bib37)) 和Aman等人 Lin et al. ([2017](#bib.bib89)) 从原始深度图像中计算了三个新的通道，分别是水平差异、高度和重力角度（HHA）。刘等人
    Liu et al. ([2018a](#bib.bib93)) 指出HHA的一个局限性是某些场景可能没有足够的水平和垂直平面。因此，他们提出了一种新的重力方向检测方法，通过拟合垂直线来学习更好的表示。Hazirbas等人
    Hazirbas et al. ([2016](#bib.bib41)) 还认为HHA表示具有较高的计算成本，并且信息量少于原始深度图像。他们提出了一种名为FuseNet的架构，由两个编码-解码分支组成，包括一个深度分支和一个RGB分支，直接以较低的计算负担编码深度信息。
- en: Multi-scale Network: The context information learned by multi-scale networks
    is useful for small objects and detailed region segmentation. Couprie et al. Couprie
    et al. ([2013](#bib.bib17)) applied a multi-scale convolutional network to learn
    features directly from the RGB images and the depth images. Aman et al. Raj et al.
    ([2015](#bib.bib119)) proposed a multi-scale deep ConvNet for segmentation where
    the coarse predictions of VGG16-FC net are up sampled in a Scale-2 module and
    then concatenated with the low-level predictions of VGG-M net in Scale-1 module
    to get both high and low level features. However, this method is sensitive to
    clutter in the scene resulting in output errors. Lin et al. Lin et al. ([2017](#bib.bib89))
    exploit the fact that lower scene-resolution regions have higher depth, and higher
    scene-resolution regions have lower depth. They use depth maps to split the corresponding
    color images into multiple scene-resolution regions, and introduce context-aware
    receptive field (CaRF) which focuses on semantic segmentation of certain scene-resolution
    regions. This makes their pipeline a multi-scale network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度网络：多尺度网络学到的上下文信息对于小物体和详细区域分割非常有用。Couprie等人 Couprie et al. ([2013](#bib.bib17))
    应用多尺度卷积网络直接从RGB图像和深度图像中学习特征。Aman等人 Raj et al. ([2015](#bib.bib119)) 提出了一个多尺度深度卷积网络进行分割，其中VGG16-FC网络的粗略预测在Scale-2模块中上采样，然后与VGG-M网络在Scale-1模块中的低级预测进行拼接，以获得高低级特征。然而，这种方法对场景中的杂乱物体很敏感，导致输出错误。Lin等人
    Lin et al. ([2017](#bib.bib89)) 利用低场景分辨率区域具有较高深度和高场景分辨率区域具有较低深度的事实。他们使用深度图将相应的彩色图像分割成多个场景分辨率区域，并引入了上下文感知接收场（CaRF），关注某些场景分辨率区域的语义分割。这使得他们的管道成为一个多尺度网络。
- en: '![Refer to caption](img/fa0ffb98f08b256b59d1380f2cefaee2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa0ffb98f08b256b59d1380f2cefaee2.png)'
- en: 'Figure 5: Typical two-channel framework with six improvement modules, including
    (a) multi-tasks learning, (b) depth encoding, (c) multi-scale network, (d) novel
    neural network architecture, (e) feature/score level fusion, and (f) post-processing.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：典型的双通道框架，包括六个改进模块：(a) 多任务学习，(b) 深度编码，(c) 多尺度网络，(d) 新颖的神经网络架构，(e) 特征/评分级融合，和
    (f) 后处理。
- en: Novel Neural Networks: Given the fixed grid computation of CNNs, their ability
    to process and exploit geometric information is limited. Therefore, researchers
    have proposed other novel neural network architectures to better exploit geometric
    features and the relationships between RGB and depth images. These architectures
    can be divided into five main categories.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '*Improved 2D Convolutional Neural Networks* (2D CNNs) Inspired from cascaded
    feature networks  Lin et al. ([2017](#bib.bib89)), Jiang et al. Jiang et al. ([2017](#bib.bib62))
    proposed a novel Dense-Sensitive Fully Convolutional Neural Network (DFCN) which
    incorporates depth information into the early layers of the network using feature
    fusion tactics. This is followed by several dilated convolutional layers for context
    information exploitation. Similarly, Wang et al. Wang and Neumann ([2018](#bib.bib150))
    proposed a depth-aware 2D CNN by introducing two novel layers, depth aware convolution
    layer and depth-aware pooling layer, which are based on the prior that pixels
    with the same semantic label and similar depth should have more impact on one
    another.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '*Deconvolutional Neural Networks*(DeconvNets) are a simple yet effective and
    efficient solution for the refinement of segmentation map. Liu et al. Liu et al.
    ([2018b](#bib.bib94)) and Wang et al. Wang et al. ([2016](#bib.bib145)) all adopt
    the DeconvNet for RGB-D semantic segmentation because of good performance. However,
    the potential of DeconvNet is limited since the high-level prediction map aggregates
    large context for dense prediction. To this end, Cheng et al. Cheng et al. ([2017](#bib.bib13))
    proposed a locality-sensitive DeconvNet (LS-DenconvNet) to refine the boundary
    segmentation over depth and color images. LS-DeconvNet incorporates local visual
    and geometric cues from the raw RGB-D data into each DeconvNet, which is able
    to up sample the coarse convolutional maps with large context while recovering
    sharp object boundaries.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '*Recurrent Neural Networks* (RNNs) can capture long-range dependencies between
    pixels but are mainly suited to a single data channel (e.g. RGB). Fan et al. Fan
    et al. ([2017](#bib.bib27)) extended the single-modal RNNs to multimodal RNNs
    (MM-RNNs) for application to RGB-D scene labeling. The MM-RNNs allow ‘memory’
    sharing across depth and color channels. Each channel not only possess its own
    features but also has the attributes of other channel making the learned features
    more discriminative for semantic segmentation. Li et al. Li et al. ([2016](#bib.bib84))
    proposed a novel Long Short-Term Memorized Context Fusion (LSTM-CF) model to capture
    and fuse contextual information from multiple channels of RGB and depth images.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '*Graph Neural Networks* (GNNs) were first used for RGB-D semantic segmentation
    by Qi et al. Qi et al. ([2017c](#bib.bib117)) who cast the 2D RGB pixels into
    3D space based on depth information and associated the 3D points with semantic
    information. Nest, they built a k-nearest neighbor graph from the 3D points and
    applied a 3D graph neural network (3DGNN) to perform pixelwise predictions.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*图神经网络*（GNNs）首次由 Qi 等人 ([2017c](#bib.bib117)) 用于 RGB-D 语义分割，他们将基于深度信息的 2D RGB
    像素投射到 3D 空间，并将 3D 点与语义信息关联。接下来，他们从 3D 点构建了一个 k 最近邻图，并应用了 3D 图神经网络（3DGNN）进行像素级预测。'
- en: '*Transformers* have gained popularity in RGB image segmentation and have also
    been extended to RGB-D segmentation. Researchers have proposed various approaches
    to leverage Transformers for this purpose. One notable work by Ying et al. Ying
    and Chuah ([2022](#bib.bib184)) introduces the concept of Uncertainty-Aware Self-Attention,
    which explicitly manages the information flow from unreliable depth pixels to
    confident depth pixels during feature extraction. This approach aims to address
    the challenges posed by noisy or uncertain depth information in RGB-D segmentation.
    Another study by Wu et al. Wu et al. ([2022c](#bib.bib166)) adopts the Swin-Transformer
    directly to exploit both the RGB and depth features. By leveraging the self-attention
    mechanism, this approach captures long-range dependencies and enables effective
    fusion of RGB and depth information for segmentation. Inspired by the success
    of the Swin-Transformer, Yang et al. Yang et al. ([2022](#bib.bib179)) proposes
    a hierarchical Swin-RGBD Transformer. This model incorporates and leverages depth
    information to complement and enhance the ambiguous and obscured features in RGB
    images. The hierarchical architecture allows for multi-scale feature learning
    and enables more effective integration of RGB and depth information.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*变压器* 在 RGB 图像分割中获得了广泛应用，并且也被扩展到 RGB-D 分割。研究人员提出了多种方法来利用变压器实现这一目的。Ying 等人 ([2022](#bib.bib184))
    的一项显著工作引入了不确定性感知自注意力的概念，该方法在特征提取过程中显式地管理从不可靠的深度像素到自信深度像素的信息流。这种方法旨在解决 RGB-D 分割中噪声或不确定深度信息带来的挑战。Wu
    等人 ([2022c](#bib.bib166)) 的另一项研究直接采用了 Swin-Transformer 来利用 RGB 和深度特征。通过利用自注意力机制，该方法捕捉了长距离依赖性，并有效融合了
    RGB 和深度信息进行分割。受到 Swin-Transformer 成功的启发，Yang 等人 ([2022](#bib.bib179)) 提出了一个层次化的
    Swin-RGBD 变压器。该模型结合并利用深度信息来补充和增强 RGB 图像中的模糊和遮挡特征。层次化架构允许进行多尺度特征学习，并实现了 RGB 和深度信息的更有效集成。'
- en: 'Data/Feature/Score Fusion: Optimal fusion of the texture (RGB channels) and
    geometric (depth channel) information is important for accurate semantic segmentation.
    There are three fusion tactics: data level, feature level and score level, referring
    to early, middle and late fusion respectively. A simple *data level fusion* strategy
    is to concatenate the RGB and depth images into four channels for direct input
    to a CNN model e.g. as performed by Couprie et al. Couprie et al. ([2013](#bib.bib17)).
    However, such a data level fusion does not exploit the strong correlations between
    depth and photometric channels. *Feature level fusion*, on the other hand, captures
    these correlations. For example, Li et al. Li et al. ([2016](#bib.bib84)) proposed
    a memorized fusion layer to adaptively fuse vertical depth and RGB contexts in
    a data-driven manner. Their method performs bidirectional propagation along the
    horizontal direction to hold true 2D global contexts. Similarly, Wang et al. Wang
    et al. ([2016](#bib.bib145)) proposed a feature transformation network that correlates
    the depth and color channels, and bridges the convolutional networks and deconvolutional
    networks in a single channel. The feature transformation network can discover
    specific features in a single channel as well as common features between two channels,
    allowing the two branches to share features to improve the representation power
    of shared information. The above complex feature level fusion models are inserted
    in a specific same layer between RGB and depth channels, which is difficult to
    train and ignores other same layer feature fusion. To this end, Hazirbas et al.
    Hazirbas et al. ([2016](#bib.bib41)) and Jiang et al. Jiang et al. ([2017](#bib.bib62))
    carry out fusion as an element-wise summation to fuse feature of multiple same
    layers between the two channels. Wu et al. Wu et al. ([2022c](#bib.bib166)) proposea
    novel Transformer-based fusion scheme, named TransD-Fusion to better model long-range
    contextual information.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据/特征/得分融合：纹理（RGB 通道）和几何（深度通道）信息的最佳融合对于准确的语义分割至关重要。融合策略有三种：数据层融合、特征层融合和得分层融合，分别指的是早期、中期和晚期融合。一个简单的*数据层融合*策略是将
    RGB 图像和深度图像拼接成四个通道，直接输入 CNN 模型，例如 Couprie 等人所做的。Couprie 等人 ([2013](#bib.bib17))。然而，这种数据层融合并没有利用深度通道和光度通道之间的强相关性。另一方面，*特征层融合*可以捕捉这些相关性。例如，Li
    等人 Li 等人 ([2016](#bib.bib84)) 提出了一个记忆融合层，以数据驱动的方式自适应地融合垂直深度和 RGB 上下文。他们的方法在水平方向上进行双向传播，以保持真实的
    2D 全局上下文。类似地，Wang 等人 Wang 等人 ([2016](#bib.bib145)) 提出了一个特征转换网络，该网络将深度和颜色通道关联起来，并在单个通道中连接卷积网络和反卷积网络。特征转换网络可以在单个通道中发现特定特征以及两个通道之间的共性特征，使两个分支可以共享特征，从而提高共享信息的表示能力。上述复杂的特征层融合模型被插入到
    RGB 和深度通道之间的特定相同层中，这使得训练变得困难，并且忽略了其他相同层特征融合。为此，Hazirbas 等人 Hazirbas 等人 ([2016](#bib.bib41))
    和 Jiang 等人 Jiang 等人 ([2017](#bib.bib62)) 通过逐元素求和的方式进行融合，以融合两个通道之间多个相同层的特征。Wu 等人
    Wu 等人 ([2022c](#bib.bib166)) 提出了一个新颖的基于 Transformer 的融合方案，称为 TransD-Fusion，以更好地建模远程上下文信息。
- en: '*Score level fusion* is commonly performed using the simple averaging strategy.
    However, the contributions of RGB model and depth model for semantic segmentation
    are different. Liu et al. Liu et al. ([2018a](#bib.bib93)) proposed a score level
    fusion layer with weighted summation that uses a convolution layer to learn the
    weights from the two channels. Similarly, Cheng et al. Cheng et al. ([2017](#bib.bib13))
    proposed a gated fusion layer to learn the varying performance of RGB and depth
    channels for different class recognition in different scenes. Both techniques
    improved the results over the simple averaging strategy at the cost of additional
    learnable parameters.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*得分层融合* 通常采用简单的平均策略。然而，RGB 模型和深度模型对语义分割的贡献是不同的。Liu 等人 Liu 等人 ([2018a](#bib.bib93))
    提出了一个具有加权求和的得分层融合层，该层使用卷积层从两个通道中学习权重。类似地，Cheng 等人 Cheng 等人 ([2017](#bib.bib13))
    提出了一个门控融合层，以学习 RGB 和深度通道在不同场景中对不同类别识别的不同性能。这两种技术在增加额外可学习参数的代价下，改善了简单平均策略的结果。'
- en: Post-Processing: The results of CNN or DCNN used for RGB-D semantic segmentation
    are generally very coarse resulting in rough boundaries and the vanishing of small
    objects. A common method to address this problem is to couple the CNN with a Conditional
    Random Field (CRF). Wang et al. Wang et al. ([2015](#bib.bib147)) further boost
    the mutual interactions between the two channels by the joint inference of Hierarchical
    CRF (HCRF). It enforces synergy between global and local predictions, where the
    global layouts are used to guide the local predictions and reduce local ambiguities,
    as well as local results provide detailed regional structures and boundaries.
    Mousavian et al. Mousavian et al. ([2016](#bib.bib107)), Liu et al. Liu et al.
    ([2018b](#bib.bib94)), and Long et al. Liu et al. ([2018a](#bib.bib93)) adopt
    a Fully Connected CRF (FC-CRF) for post-processing, where the pixel-wise label
    prediction jointly considers geometric constraint, such as pixel-wise normal information,
    pixel position, intensity and depth, to promote the consistency of pixel-wise
    labeling. Similarly, Jiang et al. Jiang et al. ([2017](#bib.bib62)) proposed Dense-sensitive
    CRF (DCRF) that integrates the depth information with FC-CRF.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理：用于 RGB-D 语义分割的 CNN 或 DCNN 的结果通常非常粗糙，导致边界模糊和小物体消失。解决这个问题的一个常用方法是将 CNN 与条件随机场（CRF）结合。Wang
    等人（[2015](#bib.bib147)）通过层次 CRF（HCRF）的联合推理进一步增强了两个通道之间的互相作用。它在全局和局部预测之间强制协同，其中全局布局用于指导局部预测并减少局部模糊，同时局部结果提供详细的区域结构和边界。Mousavian
    等人（[2016](#bib.bib107)）、Liu 等人（[2018b](#bib.bib94)）和 Long 等人（[2018a](#bib.bib93)）采用了完全连接
    CRF（FC-CRF）进行后处理，其中像素级标签预测联合考虑几何约束，如像素级法线信息、像素位置、强度和深度，以促进像素级标记的一致性。类似地，Jiang
    等人（[2017](#bib.bib62)）提出了密集敏感 CRF（DCRF），将深度信息与 FC-CRF 相结合。
- en: 'Table 2: Summary of RGB-D based methods with deep learning. Est.←depth estimation.
    Enc.←depth encoding. Mul.←multi-scale networks. Nov.←novel neural networks. Fus.←data/feature/score
    fusion. Pos.←post-processing.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于深度学习的 RGB-D 方法总结。估计←深度估计。编码←深度编码。多尺度←多尺度网络。新颖←新颖神经网络。融合←数据/特征/得分融合。后处理←后处理。
- en: '| Methods | Est. | Enc. | Mul. | Nov. | Fus. | Pos. | Architecture(2-stream)
    | Contribution |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 估计 | 编码 | 多尺度 | 新颖 | 融合 | 后处理 | 架构（2-stream） | 贡献 |'
- en: '| Cao et al. ([2016](#bib.bib8)) | $\checkmark$ | $\checkmark$ | $\times$ |
    $\times$ | $\checkmark$ | $\times$ | FCNs | Estimating depth images+a unified
    network for two tasks |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Cao 等人（[2016](#bib.bib8)） | $\checkmark$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\times$ | FCNs | 深度图像估计 + 统一网络用于两个任务 |'
- en: '| Guo and Chen ([2018](#bib.bib35)) | $\checkmark$ | $\times$ | $\times$ |
    $\times$ | $\checkmark$ | $\times$ | FCNs | Incorporating depth & gradient for
    depth estim. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Guo 和 Chen（[2018](#bib.bib35)） | $\checkmark$ | $\times$ | $\times$ | $\times$
    | $\checkmark$ | $\times$ | FCNs | 结合深度和梯度用于深度估计 |'
- en: '| Wang et al. ([2015](#bib.bib147)) | $\checkmark$ | $\times$ | $\times$ |
    $\times$ | $\times$ | $\checkmark$ | Region./Global CNN | HCRF for fusion and
    refining + two tasks by a network |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人（[2015](#bib.bib147)） | $\checkmark$ | $\times$ | $\times$ | $\times$
    | $\times$ | $\checkmark$ | 区域/全局 CNN | HCRF 用于融合和精炼 + 通过网络完成两个任务 |'
- en: '| Mousavian et al. ([2016](#bib.bib107)) | $\checkmark$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\checkmark$ | FCN | FC-CRF for refining + Mutual
    improvement for two tasks |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Mousavian 等人（[2016](#bib.bib107)） | $\checkmark$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\checkmark$ | FCN | FC-CRF 用于精炼 + 两个任务的互相改进 |'
- en: '| Liu et al. ([2018b](#bib.bib94)) | $\checkmark$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | S/D-DCNN | PBL for two feature maps integration +
    FC-CRF |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人（[2018b](#bib.bib94)） | $\checkmark$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | S/D-DCNN | PBL 用于两个特征图的集成 + FC-CRF |'
- en: '| Höft et al. ([2014](#bib.bib47)) | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | A embedding for depth images |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Höft 等人（[2014](#bib.bib47)） | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | 深度图像的嵌入 |'
- en: '| Gupta et al. ([2014](#bib.bib37)) | $\times$ | $\checkmark$ | $\times$ |
    $\times$ | $\times$ | $\times$ | CNNs | HHA for depth images |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Gupta 等人（[2014](#bib.bib37)） | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | 深度图像的 HHA |'
- en: '| Liu et al. ([2018a](#bib.bib93)) | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\checkmark$ | DCNNs | New depth encoding+ FC-CRF for refining
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人（[2018a](#bib.bib93)） | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\checkmark$ | DCNNs | 新的深度编码 + 用于精炼的 FC-CRF |'
- en: '| Hazirbas et al. ([2016](#bib.bib41)) | $\times$ | $\checkmark$ | $\times$
    | $\times$ | $\checkmark$ | $\times$ | Encoder-decoder | Semantic and depth feature
    fusion at each layer |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Hazirbas et al. ([2016](#bib.bib41)) | $\times$ | $\checkmark$ | $\times$
    | $\times$ | $\checkmark$ | $\times$ | Encoder-decoder | 每层的语义和深度特征融合 |'
- en: '| Couprie et al. ([2013](#bib.bib17)) | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | ConvNets | RGB laplacian pyramid for multi-scale
    features |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Couprie et al. ([2013](#bib.bib17)) | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | ConvNets | 用于多尺度特征的 RGB 拉普拉斯金字塔 |'
- en: '| Raj et al. ([2015](#bib.bib119)) | $\times$ | $\checkmark$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | VGG-M | New multi-scale deep CNN |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Raj et al. ([2015](#bib.bib119)) | $\times$ | $\checkmark$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | VGG-M | 新型多尺度深度 CNN |'
- en: '| Lin et al. ([2017](#bib.bib89)) | $\times$ | $\times$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | $\times$ | CFN | CaRF for multi-resolution features |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al. ([2017](#bib.bib89)) | $\times$ | $\times$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | $\times$ | CFN | 用于多分辨率特征的 CaRF |'
- en: '| Jiang et al. ([2017](#bib.bib62)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\checkmark$ | RGB-FCN | Semantic & depth feature fusion at each
    layer + DCRF |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Jiang et al. ([2017](#bib.bib62)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\checkmark$ | RGB-FCN | 每层的语义和深度特征融合 + DCRF |'
- en: '| Wang and Neumann ([2018](#bib.bib150)) | $\times$ | $\times$ | $\times$ |
    $\checkmark$ | $\times$ | $\times$ | Depth-aware CNN | Depth-aware Conv. and depth
    aware average pooling |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Wang and Neumann ([2018](#bib.bib150)) | $\times$ | $\times$ | $\times$ |
    $\checkmark$ | $\times$ | $\times$ | Depth-aware CNN | 深度感知卷积和深度感知平均池化 |'
- en: '| Cheng et al. ([2017](#bib.bib13)) | $\times$ | $\checkmark$ | $\times$ |
    $\checkmark$ | $\checkmark$ | $\times$ | FCN + Deconv | LS-DeconvNet + novel gated
    fusion |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Cheng et al. ([2017](#bib.bib13)) | $\times$ | $\checkmark$ | $\times$ |
    $\checkmark$ | $\checkmark$ | $\times$ | FCN + Deconv | LS-DeconvNet + 新颖的门控融合
    |'
- en: '| Fan et al. ([2017](#bib.bib27)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | MM-RNNs | Multimodal RNN |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Fan et al. ([2017](#bib.bib27)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | MM-RNNs | 多模态 RNN |'
- en: '| Li et al. ([2016](#bib.bib84)) | $\times$ | $\checkmark$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | LSTM-CF | LSTM-CF for capturing and fusing contextual
    inf. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. ([2016](#bib.bib84)) | $\times$ | $\checkmark$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | LSTM-CF | 用于捕捉和融合上下文信息的 LSTM-CF |'
- en: '| Qi et al. ([2017c](#bib.bib117)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | 3DGNN | GNN for RGB-D semantic segmentation |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Qi et al. ([2017c](#bib.bib117)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | 3DGNN | 用于 RGB-D 语义分割的 GNN |'
- en: '| Wang et al. ([2016](#bib.bib145)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | ConvNet-DeconvNet | MK-MMD for assessing the similarity
    between common features |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2016](#bib.bib145)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | ConvNet-DeconvNet | 用于评估常见特征相似性的 MK-MMD |'
- en: '| Ying and Chuah ([2022](#bib.bib184)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformer | Effective and scalable fusion module
    based on aross-attention |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Ying and Chuah ([2022](#bib.bib184)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformer | 基于跨注意力的有效且可扩展的融合模块 |'
- en: '| Wu et al. ([2022c](#bib.bib166)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformers | Transformer-based fusion module
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. ([2022c](#bib.bib166)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformers | 基于 Transformer 的融合模块 |'
- en: '| Yang et al. ([2022](#bib.bib179)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | Swin-Transformer+ResNet | Swin-RGB-D Transformer |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. ([2022](#bib.bib179)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | Swin-Transformer+ResNet | Swin-RGB-D Transformer |'
- en: 3.2 Projected Images Based Segmentation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 投影图像基础的分割
- en: The core idea of projected images based semantic segmentation is to use 2D CNNs
    to exploit features from projected images of 3D scenes/shapes and then fuse these
    features for label prediction. This pipeline not only exploits more semantic information
    from large-scale scenes compared to a single-view image, but also reduces the
    data size of a 3D scene compared to a point cloud. The projected images mainly
    include *multi-view images* or *spherical images*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 投影图像基础的语义分割的核心思想是使用 2D CNN 从 3D 场景/形状的投影图像中提取特征，然后融合这些特征进行标签预测。该流程不仅相比单视角图像可以利用更多来自大规模场景的语义信息，而且相比点云也减少了
    3D 场景的数据量。投影图像主要包括*多视角图像*或*球面图像*。
- en: 'Among, multi-view images projection is usually employed on RGB-D datasets Dai
    et al. ([2017](#bib.bib18)), and statics terrestrial scanning datasets Hackel
    et al. ([2017](#bib.bib38)). Spherical images projection is usually employed on
    self-driving mobile laser scanning datasets Behley et al. ([2019](#bib.bib3)).
    Projected images based semantic segmentation methods are summarized in Table [3](#S3.T3
    "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected Images Based
    Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在其中，多视角图像投影通常应用于 RGB-D 数据集 Dai 等人 ([2017](#bib.bib18))，以及静态地面扫描数据集 Hackel 等人
    ([2017](#bib.bib38))。球面图像投影通常应用于自动驾驶移动激光扫描数据集 Behley 等人 ([2019](#bib.bib3))。基于投影图像的语义分割方法总结在表
    [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected
    Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey")。'
- en: 3.2.1 Multi-View Images Based Segmentation
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 多视角图像基于的分割
- en: 'MVCNN Su et al. ([2015](#bib.bib135)) uses a unified network to combine features
    from multiple views of a 3D shape, formed by a virtual camera, into a single and
    compact shape descriptor to get improved classification performance. This inspired
    researchers to take the same idea into 3D semantic segmentation (see Figure [6](#S3.F6
    "Figure 6 ‣ 3.2.1 Multi-View Images Based Segmentation ‣ 3.2 Projected Images
    Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")). For example, Lawin et al. Lawin et al. ([2017](#bib.bib75)) project
    point clouds into multi-view synthetic images, including RGB, depth and surface
    normal images. The prediction score of all multi-view images is fused into a single
    representation and back-projected into each point. However, the snapshot can erroneously
    catch the points behind the observed structure if the density of the point cloud
    is low, which makes the deep network to misinterpret the multiple views. To this
    end, SnapNet Boulch et al. ([2017](#bib.bib6)), Boulch et al. ([2018](#bib.bib5))
    preprocesses point clouds for computing point features(like normal or local noise)
    and generating a mesh, which is similar to point cloud densification. From the
    mesh and point clouds, they generate RGB and depth images by suitable snapshot.
    Then, they perform a pixel-wise labeling of 2D snapshots using FCN and fast back-project
    these labels into 3D points by efficient buffering. Above methods need obtain
    the whole point clouds of 3D scene in advance to provide a complete spatial structure
    for back-projection. However, the multi-view images directly obtained from real-world
    scene would lose much spatial information. some works attempt to unite 3D scene
    reconstruction with semantic segmentation, where scene reconstruction could make
    up for spatial information. For example, Guerry et al. Guerry et al. ([2017](#bib.bib34))
    reconstruct 3D scene with global multi-view RGB and Gray stereo images. Then,
    the labels of 2D snapshots are back-projected onto the reconstructed scene. But,
    simple back-projection can not optimally fuse semantic and spatial geometric features.
    Along the line, Pham et al. Pham et al. ([2019a](#bib.bib113)) proposed a novel
    Higher-order CRF, following back-projection, to further develop the initial segmentation.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'MVCNN Su 等人 ([2015](#bib.bib135)) 使用统一网络将来自虚拟摄像机的多个视角的 3D 形状特征合并为单一紧凑的形状描述符，以提高分类性能。这启发了研究人员将相同的想法应用于
    3D 语义分割（见图 [6](#S3.F6 "Figure 6 ‣ 3.2.1 Multi-View Images Based Segmentation ‣
    3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey")）。例如，Lawin 等人 ([2017](#bib.bib75)) 将点云投影到多视角合成图像中，包括
    RGB、深度和表面法线图像。所有多视角图像的预测分数融合成单一表示，并反向投影到每个点。然而，如果点云的密度较低，快照可能会错误地捕捉到观察结构后的点，这使得深度网络误解多个视角。为此，SnapNet
    Boulch 等人 ([2017](#bib.bib6))，Boulch 等人 ([2018](#bib.bib5)) 对点云进行预处理以计算点特征（如法线或局部噪声）并生成网格，这类似于点云密集化。通过网格和点云，他们通过合适的快照生成
    RGB 和深度图像。然后，他们使用 FCN 对 2D 快照进行逐像素标记，并通过高效缓冲将这些标签快速反向投影到 3D 点上。上述方法需要提前获得 3D 场景的完整点云，以提供完整的空间结构用于反向投影。然而，直接从现实世界场景获得的多视角图像会丢失大量空间信息。一些研究试图将
    3D 场景重建与语义分割结合起来，其中场景重建可以弥补空间信息。例如，Guerry 等人 ([2017](#bib.bib34)) 使用全局多视角 RGB
    和灰度立体图像重建 3D 场景。然后，将 2D 快照的标签反向投影到重建的场景上。但是，简单的反向投影不能最佳地融合语义和空间几何特征。沿此思路，Pham
    等人 ([2019a](#bib.bib113)) 提出了一个新的高阶 CRF，在反向投影之后，进一步发展初步分割。'
- en: '![Refer to caption](img/c9199e7695799d6dd521bc6661680a8c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Illustration of basic frameworks for projected images based segmentation
    methods. Top: Multi-view images based framework. Bottom: Spherical images based
    framework.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Spherical Images Based Segmentation
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Selecting snapshots from a 3D scene is not straight forward. Snapshots must
    be taken after giving due consideration to the number of viewpoints, viewing distance
    and angle of the virtual cameras to get an optimal representation of the complete
    scene. To avoid these complexities, researchers project the complete point cloud
    onto a sphere (see Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1 Multi-View Images Based
    Segmentation ‣ 3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").Bottom). For example, Wu et
    al. Wu et al. ([2018a](#bib.bib159)) proposed an end-to-end pipeline called SqueezeSeg,
    inspired from SqueezeNet Iandola et al. ([2016](#bib.bib55)), to learn features
    from spherical images which are then refined by CRF implemented as a recurrent
    layer. Similarly, PointSeg Wang et al. ([2018e](#bib.bib154)) extends the SqueezeNet
    by integrating the feature-wise and channel-wise attention to learn robust representation.
    SqueezeSegv2 Wu et al. ([2019a](#bib.bib160)) improves the structure of SqueezeSeg
    with Context Aggregation Module (CAM), adding LiDAR mask as a channel to increase
    robustness to noise. RangNet++ Milioto et al. ([2019](#bib.bib105)) transfers
    the semantic labels to 3D point clouds, avoiding discarding points regardless
    of the level of discretization used in CNN. Despite the likeness between regular
    RGB and LiDAR images, the feature distribution of LiDAR images changes at different
    locations. SqueezeSegv3 Xu et al. ([2020](#bib.bib172)) has a spatially-adaptive
    and context-aware convolution, termed Spatially-Adaptive Convolution (SAC) to
    adopt different filters for different locations. Inspired by the success of 2D
    vision Transformer, RangViT Ando et al. ([2023](#bib.bib1)) leverage ViTs pre-trained
    on long natural image datasets by adding the down and up module on the top and
    bottom of ViTs, and achieves a good performance comparing to the projection based
    methods. Similarly, to make the long projection image to suit the ViTs, RangeFormer
    Kong et al. ([2023](#bib.bib70)) adopts a scalable training strategy that splits
    the whole projection image into several sub-images, and puts them into ViTs for
    training. After training, the predictions are merged sequentially to form the
    complete scene.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of projected images/voxel/other representation based methods
    with deep learning. M←multi-view image. S←spherical image. V←voxel. T←tangent
    images. L←lattice. P←point clouds.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Input | Architecture | Feature extractor | Contribution
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| projection | Lawin et al. Lawin et al. ([2017](#bib.bib75)) | M | multi-stream
    | VGG-16 | Investigate the impact of different input modalities |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| Boulch et al. Boulch et al. ([2017](#bib.bib6))  Boulch et al. ([2018](#bib.bib5))
    | M | SegNet/U-Net | VGG-16 | New and efficient framework SnapNet |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| Guerry et al. Guerry et al. ([2017](#bib.bib34)) | M | SegNet/U-Net | VGG-16
    | Improved MVCNN+3D consistent data augment. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| Pham et al. Pham et al. ([2019a](#bib.bib113)) | M | Two-stream | 2DConv
    | High-order CRF+ real-time reconstruction pipeline |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) | S | AlexNet | Firemodules |
    End-to-end pipeline SqueezeSeg + real time |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) | S | AlexNet | Firemodules
    | Quite light-weight framework PointSeg + real time |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) | S | AlexNet | Firemodules |
    Robust framework SqueezeSegV2 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) | S | DarkNet | Residual
    block | GPU-accelerated post-processing + RangNet++ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. Xu et al. ([2020](#bib.bib172)) | S | RangeNet | SAC | Adopting
    different filters for different locations |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| Ando et al. Ando et al. ([2023](#bib.bib1)) | S | U-Net | ViTs | Decreasing
    the gaps between image and point domain. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Kong et al. Kong et al. ([2023](#bib.bib70)) | S | U-Net | ViTs | Introducing
    a scalable training from range view strategy |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| voxel | Huang et al. Huang and You ([2016](#bib.bib53)) | V | 3D CNN | 3DConv
    | Efficiently handling large data |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | V | 3D FCNN | 3DConv
    | Combining 3D FCNN with fine-represen. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| Meng et al. Meng et al. ([2019](#bib.bib103)) | V | VAE | RBF | A novel voxel-based
    representation + RBF |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. Liu et al. ([2017](#bib.bib91)) | V | 3D CNN/DQN/RNN | 3DConv
    | Integrating three vision tasks into one frame. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| Rethage et at. Rethage et al. ([2018](#bib.bib121)) | V | 3D FCNN | FPConv
    | First fully-convolutional network on raw point sets |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Dai et al. Dai et al. ([2018](#bib.bib20)) | V | 3D FCNN | 3DConv | Combing
    scene completion and semantic labeling |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Riegler et al. Riegler et al. ([2017](#bib.bib122)) | V | Octree | 3DConv
    | Making DL with high-resolution voxels |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Graham et al. Graham et al. ([2018](#bib.bib32)) | V | FCN/U-Net | SSConv
    | SSConv with less computation |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| others | TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | T | U-Net
    | TConv | Tangent convolution + Parsing large scenes |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) | L | DeepLab | BConv | Hierarchical
    and spatially-aware feature learning |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) | L | U-Net | PN+3DConv | Hybrid
    architecture + novel slicing operator |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| 3DMV Dai and Nießner ([2018](#bib.bib19)) | M+V | Cascade frame. | ENet+3DConv
    | Inferring 3D semantics from both 3D and 2D input |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) | V+M+P | Parallel frame.
    | SSCNet/DeepLab/PN | Leveraging 2D and 3D features |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| PVCNN Liu et al. ([2019b](#bib.bib97)) | V+P | POintNet | PVConv | Both memory
    and computation efficient |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) | M+P | Cascade frame. | U-Net+PointNet++
    | Leveraging 2D and 3D features |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| LaserNet++ Meyer et al. ([2019](#bib.bib104)) | M+P | Cascade frame. | ResNet+LNet
    | Unified network for two tasks |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| BPNet Hu et al. ([2021](#bib.bib50)) | M+P | Cascade frame. | 2/3DUNet |
    Bidirection projection module |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: 3.3 Voxel Based Segmentation
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to pixels, voxels divide the 3D space into many volumetric grids with
    a specific size and discrete coordinates. It contains more geometric information
    of the scene compared to projected images. 3D ShapeNets Wu et al. ([2015](#bib.bib165))
    and VoxNet Maturana and Scherer ([2015](#bib.bib101)) take volumetric occupancy
    grid representation as input to a 3D convolutional neural network for object recognition,
    which guides 3D semantic segmentation based on voxels. Voxel based semantic segmentation
    methods are summarized in Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based
    Segmentation ‣ 3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 3D CNN is a common architecture used to process uniform voxels for label prediction.
    Huang et al. Huang and You ([2016](#bib.bib53)) presented a 3D FCN for coarse
    voxel level predictions. Their method is limited by spatial inconsistency between
    predictions and provide a coarse labeling. Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138))
    introduce a novel network SEGCloud to produce fine-grained predictions. It up
    samples the coarse voxel-wise prediction obtained from a 3D FCN to the original
    3D point space resolution by trilinear interpolation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: With fixed resolution voxels, the computational complexity grows linearly with
    the increase of the scene scale. Large voxels can lower the computational cost
    of large-scale scene parsing. Liu et al. Liu et al. ([2017](#bib.bib91)) introduced
    a novel network called 3D CNN-DQN-RNN. Like the sliding windows in 2D semantic
    segmentation, this network proposes eye window that traverses the whole data for
    fast localizing and segmenting class objects under the control of 3D CNN and deep
    Q-Network (DQN). The 3D CNN and Residual RNN further refine features in the eye
    window. The pipeline learns key features of interesting regions efficiently to
    enhance the accuracy of large-scale scene parsing with less computational cost.
    Rethage et at. Rethage et al. ([2018](#bib.bib121)) present a novel fully convolutional
    point network (FCPN), sensitive to multi-scale input , to parse large-scale scene
    without pro- or post-process steps. Particularly, FCPN is able to learn memory
    efficient representations that scale well to larger volumes. Similarly, Dai et
    al. Dai et al. ([2018](#bib.bib20)) design a novel 3D CNN to train on scene subvolumes
    but deploy on arbitrarily large scenes at test time, as it is able to handle large
    scenes with varying spatial extent. Additionally, their network adopts a coarse-to-fine
    tactic to predict multiple resolution scenes to handle the resolution growth in
    data size as the scene increases in size. Traditionally, the voxel representation
    only comprises Boolean occupancy information which loses much geometric informatin.
    Meng et al. Meng et al. ([2019](#bib.bib103)) develop a novel information-rich
    voxel representation by using a variational auto-encoder(VAE) taking radial basis
    function(RBF) to capture the distribution of points within each voxel. Further,
    they proposed a group equivariant convolution to exploit feature.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In fixed scale scenes, the computational complexity grows cubically as the voxel
    resolution increases. However, the volumetric representation is naturally sparse,
    resulting in unnecessary computations when applying 3D dense convolution on the
    sparse data. To aleviate this problem, OctNet Riegler et al. ([2017](#bib.bib122))
    divides the space hierarchically into nonniform voxels using a series of unbalanced
    octrees. Tree structure allows memory allocation and computation to focus on relevant
    dense voxels without sacrificing resolution. However, empty space still imposes
    computational and memory burden in OctNet. In contrast, Graham et al. Graham et al.
    ([2018](#bib.bib32)) proposed a novel submanifold sparse convolution (SSC) that
    does not perform computations in empty regions, making up for the drawback of
    OctNet.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Point Based Segmentation
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Point clouds are scattered irregularly in 3D space, lacking any canonical order
    and translation invariance, which restricts the use of conventional 2D/3D convolutional
    neural networks. Recently, a series of point-based semantic segmentation networks
    have been proposed. These methods can be roughly subdivided into four categories:
    MLP based, point convolution based, graph convolution based and Transformer based.
    These methods are summarized in Table [4](#S3.T4 "Table 4 ‣ 3.4.4 Transformer
    Based ‣ 3.4 Point Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 MLP Based
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These methods apply a Multi Layer Perceptron directly on the points to learn
    features. The PointNet Qi et al. ([2017a](#bib.bib115)) is a pioneering work that
    directly processes point clouds. It uses shared MLP to exploit points-wise features
    and adopts a symmetric function such as max-pooling to collect these features
    into a global feature representation. Because the max-pooling layer only captures
    the maximum activation across global points, PointNet cannot learn to exploit
    local features. Building on PointNet, PointNet++ Qi et al. ([2017b](#bib.bib116))
    defines a hierarchical learning architecture. It hierarchically samples points
    using farthest point sampling (FPS) and groups local regions using k nearest neighbor
    search as well as ball search. Progressively, a simplified PointNet exploits features
    in local regions at multiple scales or multiple resolutions. Similarly, Engelmann
    et al. Engelmann et al. ([2018](#bib.bib26)) define local regions by KNN clustering
    and K-means clustering and use a simplified PointNet to extract local features.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: To learn the short and long-range dependencies, some works introduce the Recurrent
    Neural Networks (RNN) to MLP-based methods. For example, ESC Engelmann et al.
    ([2017](#bib.bib24)) divides global points into multi-scale/grid blocks. The concatenated
    (local) block features are appended to the point-wise features and passed through
    Recurrent Consolidation Units (RCUs) to further learn global context features.
    Similarly, HRNN Ye et al. ([2018](#bib.bib180)) uses Pointwise Pyramid Pooling
    (3P) to extract local features on the multi-size local regions. Point-wise features
    and local features are concatenated and a two-direction hierarchical RNN explores
    context features on these concatenated features. However, the local features learned
    are not sufficient because the deeper layer features do not cover a larger spatial
    extent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Another technology, some works integrate the hand-craft point representation
    into PointNet or PointNet++ network to enhance the point representation ability
    with less learnable netowrk parameters. Inspired by SIFT representation Lowe ([2004](#bib.bib98)),
    PointSIFT Jiang et al. ([2018](#bib.bib64)) inserts a PointSIFT module layer learn
    local shape information. This module transforms each point into a new shape representation
    by encoding information of different orientations. PointWeb Zhao et al. ([2019a](#bib.bib193))
    propose a adaptive feature adjustment (AFA) module to learning the interactive
    information between local points to enhance the point representation. Similarly,
    RepSurf Ran et al. ([2022](#bib.bib120)) introduces two novel point representations,
    namely triangular and umbrella representative surfaces, to establish connections
    and enhance the representation capability of learned point-wise features. This
    approach effectively improves feature representation with fewer learnable network
    parameters, drawing significant attention from the research community. In contrast
    to the aforementioned methods, PointNeXt Qian et al. ([2022](#bib.bib118)) takes
    a different approach by revisiting the classical PointNet++ architecture through
    a systematic study of model training and scaling strategies. It proposes a set
    of improved training strategies that lead to a significant performance boost for
    PointNet++. Additionally, PointNeXt introduces an inverted residual bottleneck
    design and employs separable MLPs to enable efficient and effective model scaling.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Point Convolution Based
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Point convolution based methods perform convolution operations directly on the
    points. Different from 2D convolution , the weight function of point convolution
    need learn from point geometric information adaptively. Early convolution networks
    focus on the convolution weight function design. For example, RSNet Huang et al.
    ([2018](#bib.bib54)) exploit point-wise features using 1x1 convolution and then
    pass them through the local dependency module (LDM) to exploit local context features.
    However, it does not define the neighborhood for each point in order to learn
    local features. On the other hand, PointwiseCNN  Hua et al. ([2018](#bib.bib52))
    sorts points in a specific order, e.g. XYZ coordinate or Morton cureve Morton
    ([1966](#bib.bib106)), and queries nearest neighbors dynamically and bins them
    into 3x3x3 kernel cells before convolving with the same kernel weights.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Gradually, some point convolution works approximate the convolution weight function
    as MLP to learn weights from point coordinates. PCCN  Wang et al. ([2018c](#bib.bib149))
    performs Parametric CNN, where the kernel is estimated as an MLP, on KD-tree neighborhood
    to learn local features. PointCNN  Li et al. ([2018b](#bib.bib82)) coarsens the
    input points with farthest point sampling. The convolution layer learns an $\chi$-transformation
    from local points by MLP to simultaneously weight and permute the features, subsequently
    applying a standard convolution on these transformed features.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Some works associates a coefficient (derived from point coordinates) with the
    weight function to adjust the learned convolutional weights. An extension of Monte
    Carlo approximation for convolution called PointConv  Wu et al. ([2019b](#bib.bib161))
    takes the point density into account. It uses MLP to approximate a weight function
    of the convolution kernel, and applies an inverse density scale to reweight the
    learned weight function. Similarly, MCC  Hermosilla et al. ([2018](#bib.bib46))
    phrases convolution as a Monte Carlo integration problem by relying on point probability
    density function (PDF), where the convolution kernel is also represented by an
    MLP. Moreover, it introduces Possion Disk Sampling (PDS) Wei ([2008](#bib.bib157))
    to construct a point hierarchy instead of FPS, which provides an opportunity to
    get the maximal number of samples in a receptive field.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Another line of works use other function instead of MLP to approximate the convolution
    weight function. Flex-Convolution  Groh et al. ([2018](#bib.bib33)) uses a linear
    function with fewer parameters to model a convolution kernel and adapts inverse
    density importance sub-sampling (IDISS) to coarsen the points. KPConv Thomas et al.
    ([2019](#bib.bib139)) and KCNet Shen et al. ([2018](#bib.bib126)) fixed the convolution
    kernel for robustness to varying point density. These networks predefine the kernel
    points on local region and learn convolutional weights on the kernel points from
    their geometric connections to local points using linear and Gaussian correlation
    functions, respectively. Here, the number and position of kernel points need be
    optimized for different datasets.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Point convolution on limited local receptive field could not exploit long-range
    features. Therefore, some works introduce the dilated mechanism into point convolution.
    Dilated point convolution(DPC)  Engelmann et al. ([2020b](#bib.bib25)) adapts
    standard point convolution on neighborhood points of each point where the neighborhood
    points are determined though a dilated KNN search. similarly, A-CNN Komarichev
    et al. ([2019](#bib.bib69)) defines a new local ring-shaped region by dilated
    KNN, and projects points on a tangent plane to further order neighbor points in
    local regions. Then, the standard point convolutions are performed on these ordered
    neighbors represented as a closed loop array.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: In the large-scale point clouds semantic segmentation area, RandLA-Net  Hu et al.
    ([2020](#bib.bib49)) uses random point sampling instead of the more complex point
    selection approach. It introduces a novel local feature aggregation module (LFAM)
    to progressively increase the receptive field and effectively preserve geometric
    details. Another technology, PolarNet  Zhang et al. ([2020](#bib.bib190)) first
    partitions a large point cloud into smaller grids (local regions) along their
    polar bird’s-eye-view (BEV) coordinates. It then abstracts local region points
    into a fixed-length representation by a simplified PointNet and these representations
    are passed through a standard convolution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.3 Graph Convolution Based
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The graph convolution based methods perform convolution on points connected
    with a graph structure, where the graph help the feature aggregation exploit the
    structure information between points. the graphs can be divided into spectral
    graph and spatial graph. In the spectral graph, LS-GCN  Wang et al. ([2018a](#bib.bib143))
    adopts the basic architecture of PointNet++, replaces MLPs with a spectral graph
    convolution using standard unparametrized Fourier kernels, as well as a novel
    recursive spectral cluster pooling substitute for max-pooling. However, transformation
    from spatial to spectral domain incurs a high computational cost. Besides that,
    spectral graph networks are usually defined on a fixed graph structure and are
    thus unable to directly process data with varying graph structures.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: In the spatial graph category, ECC  Simonovsky and Komodakis ([2017](#bib.bib131))
    is among of the pioneer methods to apply spatial graph network to extract features
    from point clouds. It dynamically generates edge-conditioned filters to learn
    edge features that describe the relationships between a point and its neighbors.
    Based on PointNet architecture, DGCNN  Wang et al. ([2019b](#bib.bib155)) implements
    dynamic edge convolution called EdgeConv on the neighborhood of each point. The
    convolution is approximated by a simplified PointNet. SPG  Landrieu and Simonovsky
    ([2018](#bib.bib74)) parts the point clouds into a number of simple geometrical
    shapes (termed super-points) and builts super graph on global super-points. Furthermore,
    this network adopts PointNet to embed these points and refine the embedding by
    Gated Recurrent Unit (GRU). Based on the basic architecture of PoinNet++, Li et
    al.  Li et al. ([2019b](#bib.bib83)) proposed Geometric Graph Convolution (TGCov),
    its filters defined as products of local point-wise features with local geometric
    connection features expressed by Gaussian weighted Taylor kernels. Feng et al. 
    Feng et al. ([2020](#bib.bib29)) constructed a local graph on neighborhood points
    searched along multi-directions and explore local features by a local attention-edge
    convolution (LAE-Conv). These features are imported into a point-wise spatial
    attention module to capture accurate and robust local geometric details. Lei et
    al. designs a fuzzy coefficient to times weight function, to enable the convolution
    weights robust.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Continuous graph convolution also incurs a high computational cost and generally
    suffer from the vanishing gradient problem. Inspired by the separable convolution
    strategy in Xception  Chollet ([2017](#bib.bib15)) that significantly reduces
    parameters and computation burden, HDGCN  Liang et al. ([2019a](#bib.bib87)) designed
    a DGConv that composes depth-wise graph convolution followed by a point-wise convolution,
    and add DGConv into the hierarchical structure to extract local and global features.
    DeepGCNs  Li et al. ([2019a](#bib.bib80)) borrows some concepts from 2D CNN such
    as residual connections between different layers (ResNet) to alleviate the vanishing
    gradient problem, and dilation mechanism to allow the GCN to go deeper. Lei et
    al.  Lei et al. ([2020](#bib.bib78)) propose a discrete spherical convolution
    kernel (SPH3D kernel) that consists of the spherical convolution learning depth-wise
    features and point-wise convolution learning point-wise features.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Tree structures such as KD-tree and Octree can be viewed as a special type of
    graph, allowing to share convolution layers depending on the tree splitting orientation.
    3DContextNet  Zeng and Gevers ([2018](#bib.bib188)) adopts a KD-tree structure
    to hierarchically represent points where the nodes of different tree layers represent
    local regions at different scales, and employs a simplified PointNet with a gating
    function on nodes to explore local features. However, their performance depends
    heavily on the randomization of the tree construction. Lei et al.  Lei et al.
    ([2019](#bib.bib77)) built an Octree based hierarchical structure on global points
    to guide the spherical convolution computation in per layer of the network. The
    spherical convolution kernel systematically partitions a 3D spherical region into
    multiple bins that specifies learnable parameters to weight the points falling
    within the corresponding bin.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.4 Transformer Based
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Attention mechanism has recently become popular for improving point cloud segmentation
    accuracy. Compared to point convolution, Transformer introduces the point features
    into the weight learning. For example, Ma et al.  Ma et al. ([2020](#bib.bib100))
    use the channel self-attention mechanism to learn independence between any two
    point-wise feature channels, and further define a Channel Graph where the channel
    maps are presented as nodes and the independencies are represented as graph edges.
    AGCN  Xie et al. ([2020b](#bib.bib171)) integrates attention mechanism with GCN
    for analyzing the relationships between local features of points and introduces
    a global point graph to compensate for the relative information of individual
    points. PointANSL Yan et al. ([2020](#bib.bib176)) use the general self-attention
    mechanism for group feature updating, and propose a adaptive sampling (AS) module
    to overcome the issues of FPS.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer model, which employs self-attention as a fundamental component,
    includes position encoding to capture the sequential order of input tokens. Position
    encoding is crucial to ensure that the model understands the relative positions
    of tokens within a sequence. Point Transformer Zhao et al. ([2021](#bib.bib194))
    introduce MLP-based position encoding into vector attention, and use a KNN-based
    downsampling module to decrease the point resolution. Follow up work, Point Transformer
    v2 Wu et al. ([2022a](#bib.bib162)) strengthens the position encoding mechanism
    by applying an additional encoding multiplier to the relation vector, and designs
    a partition-based pooling strategy to align the geometric information.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Point Transformers are generally computationally expensive because the original
    self-attention module needs to generate a huge attention map. To address this
    problem, PatchFormer Zhang et al. ([2022](#bib.bib189)) calculates the attention
    map via low-rank approximation. Similarly, FastPointTransformer Park et al. ([2022](#bib.bib111))
    introduces a lightweight local self-attention module that learns continuous positional
    information while reducing the space complexity. Inspired by the success of window-based
    Transformer in the 2D domain, Stratified Transformer Lai et al. ([2022](#bib.bib73))
    designs a cubic window and samples distant points as keys, but in a sparser way,
    to expand the receptive field. Similarly, SphereFormer Lai et al. ([2023](#bib.bib72))
    designs radial window self-attention that partitions that space into several non-overlapping
    narrow and long windows for exploiting long-range dependencies.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of point based semantic segmentation methods with deep learning.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Neighb. search | Feature abstraction | Coarsening | Contribution
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| MLP | PointNet Qi et al. ([2017a](#bib.bib115)) | None | MLP | None | Pioneering
    processing points directly |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) | None | MLP | None | Two local
    definition+local/global pathway |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| ESC Engelmann et al. ([2017](#bib.bib24)) | None | MLP | None | MC/Grid Block
    for local defini.+RCUs for context exploit. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| HRNN Ye et al. ([2018](#bib.bib180)) | None | MLP | None | 3P for local feature
    exploit..+HRNN for local context exploit. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) | Ball/KNN | PointNet | FPS |
    Proposing hierarchical learning framework |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| PointSIFT Jiang et al. ([2018](#bib.bib64)) | KNN | PointNet | FPS | PointSIFT
    module for local shape information |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| PointWeb Zhao et al. ([2019a](#bib.bib193)) | KNN | PointNet | FPS | AFA
    for interactive feature exploitation |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Repsurf Ran et al. ([2022](#bib.bib120)) | KNN | PointNet | FPS | Local triangular
    orientation + local umbrella orientation |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| PointNeXt Qian et al. ([2022](#bib.bib118)) | KNN | InvResMLP | FPS | Next
    version of PointNet |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| Point Convolution | RSNet Huang et al. ([2018](#bib.bib54)) | None | 1x1
    Conv | None | LDM for local context exploitation |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| DPC Engelmann et al. ([2020b](#bib.bib25)) | DKNN | PointConv | None | Dilated
    KNN for expanding the receptive field |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| PointWiseCNN Hua et al. ([2018](#bib.bib52)) | Grid | PWConv. | None | Novel
    point convolution |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) | KD index | PCConv. | None | KD-tree
    index for neigh. search+novel point Conv. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| KPConv Thomas et al. ([2019](#bib.bib139)) | Ball | KPConv. | Grid sampling
    | Novel point convolution |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| FlexConv Groh et al. ([2018](#bib.bib33)) | KD index | flexConv. | IDISS
    | Novel point Conv.+flex-maxpooling without subsampling |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| PointCNN Li et al. ([2018b](#bib.bib82)) | DKNN | $\chi$-Conv | FPS | Novel
    point convolution |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| MCC Hermosilla et al. ([2018](#bib.bib46)) | Ball | MCConv. | PDS | Novel
    coarsening layer+point convolution |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) | KNN | PointConv | FPS | Novel
    point convolution considering point density |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| A-CNN Komarichev et al. ([2019](#bib.bib69)) | DKNN | AConv | FPS | Novel
    neighborhood search+point convolution |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| RandLA-Net Hu et al. ([2020](#bib.bib49)) | KNN | LocSE | RPS | LFAM with
    large receptive field and keeping geometric details |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| PolarNet Zhang et al. ([2020](#bib.bib190)) | None | PointNet | PolarGrid
    | Novel local regions definition + RingConv |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| Graph Convolution | DGCNN Wang et al. ([2019b](#bib.bib155)) | KNN | EdgeConv
    | None | Novel graph convolution + updating graph |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| SPG Landrieu and Simonovsky ([2018](#bib.bib74)) | partition | PointNet |
    None | Superpoint graph + parsing large-scale scene |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| DeepGCNs Li et al. ([2019a](#bib.bib80)) | DKNN | DGConv | RPS | Adapting
    residual connections between layers |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) | Ball | SPH3D-GConv | FPS | Novel
    graph convolution + pooling + uppooling |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| LS-GCN Wang et al. ([2018a](#bib.bib143)) | KNN | Spec.Conv. | FPS | Local
    spectral graph + Novel graph convolution |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| PAN Feng et al. ([2020](#bib.bib29)) | Multi-direct. | LAE-Conv | PFS | Point-wise
    spatial attention+local graph Conv. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| TGNet Li et al. ([2019b](#bib.bib83)) | Ball | TGConv | PFS | Novel graph
    Conv.+multi-scale features explo. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| HDGCN Liang et al. ([2019a](#bib.bib87)) | KNN | DGConv | FPS | Depthwise
    graph Conv. + Pointwise Conv. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '| 3DCon.Net Zeng and Gevers ([2018](#bib.bib188)) | KNN | PointNet | Tree layer
    | KD tree structure |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| $\psi$-CNN Lei et al. ([2019](#bib.bib77)) | Octree neig. | $\psi$-Conv |
    Tree layer | Octree structure+ Novel graph convolution |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer | PGCRNet Ma et al. ([2020](#bib.bib100)) | None | Conv1D
    | None | PointGCR to model context dependencies |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) | KNN | MLP | None | Point attention
    layer for aggregating local features |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| PointANSL Yan et al. ([2020](#bib.bib176)) | KNN | local-nonlocal module
    | AS | Local-nonlocal module + adaptive sampling |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer Zhao et al. ([2021](#bib.bib194)) | KNN | Point Transformer
    | Pooling | MLP-based relative position encoding + vector attention |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer v2 Wu et al. ([2022a](#bib.bib162)) | Grid partition |
    PointTransformerv2 | pooling | Novel position encoding + grid Pooling |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| PatchFormer Zhang et al. ([2022](#bib.bib189)) | Boxes partition | Patch
    Transformer | DWConv | First linear attention + Lightweight multi-scale Transformer
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| Fast Point Transformer Park et al. ([2022](#bib.bib111)) | Voxel partition
    | Fast point Transformer | Voxel-based sampl. | Lightweight local self-attention
    + novel position encoding |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: '| Stratified Transformer Lai et al. ([2022](#bib.bib73)) | Voxel partition
    | Stratified Transformer | PFS | Contextual relative position encoding |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
- en: '| SphereFormer Lai et al. ([2023](#bib.bib72)) | Voxel partition | Sphereformer
    + cubicformer | Maxpooling | Novel spherical window for LIDAR points |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
- en: 3.5 Other Representation Based
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some methods transform the original point cloud to representations other than
    projected images, voxels and points. Examples of such representations include
    *tangent images* Tatarchenko et al. ([2018](#bib.bib137)) and *lattice* Su et al.
    ([2018](#bib.bib134)), Rosu et al. ([2019](#bib.bib124)). In the former case,
    Tatarchenko et al. Tatarchenko et al. ([2018](#bib.bib137)) project local surfaces
    around each-point to a series of 2D tangent images and develop a tangent convolution
    based U-Net to extract features. In the latter case, SPLATNet Su et al. ([2018](#bib.bib134))
    adapts the bilateral convolution layers (BCLs) proposed by Jampani et al. Jampani
    et al. ([2016](#bib.bib58)) to smoothly map disordered points onto a sparse lattice.
    Similarly, LatticeNet Rosu et al. ([2019](#bib.bib124)) uses a hybrid architecture
    that combines PointNet, which obtains low-level features, with sparse 3D convolution,
    which explores global context features. These features are embedded into a sparse
    lattice that allows the application of standard 2D convolutions.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Although the above methods have achieved significant progress in 3D semantic
    segmentation, each has its own drawbacks. For instance, multi-view images have
    more spectral information like color/intensity but less geometric information
    of the scene. On the other hand, voxels have more geometric information but less
    spectral information. To get the best of both worlds, some methods adopt *hybrid
    representations* as input to learn comprehensive features of a scene. Dai et al.
    Dai and Nießner ([2018](#bib.bib19)) map 2D semantic features obtained by multi-view
    networks into 3D grids of scene. These pipelines make 3D grids attach rich 2D
    semantic as well as 3D geometric information so that the scene can get better
    segmentation by a 3D CNN. Similarly, Hung et al. Chiang et al. ([2019](#bib.bib14))
    back-project 2D multi-view image features on to the 3D point cloud space and use
    a unified network to extract local details and global context from sub-volumes
    and the global scene respectively. Liu et al. Liu et al. ([2019b](#bib.bib97))
    argue that voxel-based and point-based NN are computationally inefficient in high-resolution
    and data structuring respectively. To overcome these challenges, they propose
    Point-Voxel CNN (PVCNN) that represents the 3D input data as point clouds to take
    advantage of the sparsity to lower the memory footprint, and leverage the voxel-based
    convolution to obtain a contiguous memory access pattern. Jaritz et al. Jaritz
    et al. ([2019](#bib.bib59)) proposed MVPNet that collect 2D multi-view dense image
    features into 3D sparse point clouds and then use a unified network to fuse the
    semantic and geometric features. Also, Meyer et al. Meyer et al. ([2019](#bib.bib104))
    fuse 2D image and point clouds to address 3D object detection and semantic segmentation
    by a unifying network. BPNet Hu et al. ([2021](#bib.bib50)) consists of 2D and
    3D sub-networks with symmetric architectures, connected through a bidirectional
    projection module (BPM). This allows the interaction of complementary information
    from both visual domains at multiple architectural levels, leading to improved
    scene recognition by leveraging the advantages of both 2D and 3D information.
    The other representations based semantic segmentation methods are summarized in
    Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected
    Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey").'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 4 3D Instance Segmentation
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3D instance segmentation methods additionally distinguish between different
    instances of the same class. Being a more informative task for scene understanding,
    3D instance segmentation is receiving increased interest from the research community.
    3D instance segmentation methods are roughly divided into two directions: *proposal-based*
    and *proposal-free*.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac3e01783ced0e190ace2f88a82545f9.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of two different approaches for 3D instance segmentation.
    Top: proposal-based framework. Bottom: proposal-free framework.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Proposal Based
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Proposal-based methods first predict object proposals and then refine them
    to generate final instance masks (see Figure [7](#S4.F7 "Figure 7 ‣ 4 3D Instance
    Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey")), breaking down
    the task into two main challenges. Hence, from the proposal generation point of
    view, these methods can be grouped into *detection-based* and *detection-free*
    methods.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Detection-based methods sometimes define object proposals as a 3D bounding box
    regression problem. 3D-SIS Hou et al. ([2019](#bib.bib48)) incorporates high-resolution
    RGB images with voxels, based on the pose alignment of the 3D reconstruction,
    and jointly learns color and geometric features by a 3D detection backbone to
    predict 3D bounding box proposals. In these proposals, a 3D mask backbone predicts
    the final instance masks. Similarly, GPSN Yi et al. ([2019](#bib.bib183)) introduces
    a 3D object proposal network termed Generative Shape Proposal Network (GPSN) that
    reconstructs object shapes from shape noisy observations to enforce geometric
    understanding. GPSN is further embedded into a 3D instance segmentation network
    named Region-based PointNet (R-PointNet) to reject, receive and refine proposals.
    Training of these networks needs to be performed step-by-step and the object proposal
    refinement requires expensive suppression operation. To this end, Yang et al.
    Yang et al. ([2019](#bib.bib177)) introduced a novel end-to-end network named
    3D-BoNet to directly learn a fixed number of 3D bounding boxes without any rejection,
    and then estimate an instance mask in each bounding box.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Detection-free methods include SGPN Wang et al. ([2018d](#bib.bib151)) which
    assumes that the points belonging to the same object instance should have very
    similar features. Hence, it learns a similarity matrix to predict proposals. The
    proposals are pruned by confidence scores of the points to generate highly credible
    instance proposals. However, this simple distance similarity metric learning is
    not informative and is unable to segment adjacent objects of the same class. To
    this end, 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) learns object proposals
    from sampled and grouped point features that vote for the same object center,
    and then consolidates the proposal features using a graph convolutional network
    enabling higher-level interactions between proposals which result in refined proposal
    features. AS-Net Jiang et al. ([2020a](#bib.bib61)) uses an assignment module
    to assign proposal candidates and then eliminates redundant candidates by a suppression
    network. SoftGroup Vu et al. ([2022](#bib.bib142)) proposes top-down refinement
    to refine the instance proposal. SSTNet Liang et al. ([2021](#bib.bib86)) proposes
    an end-to-end solution of Semantic Superpoint Tree Network (SSTNet) to generate
    object instance proposals from scene points. A key contribution in SSTNet is an
    intermediate semantic superpoint tree (SST), which is constructed based on the
    learned semantic features of superpoints. The tree is traversed and split at intermediate
    nodes to generate proposals of object instances.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Proposal Free
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Proposal-free methods learn feature embedding for each point and then apply
    clustering to obtain defintive 3D instance labels (see Figure [7](#S4.F7 "Figure
    7 ‣ 4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey"))
    breaking down the task into two main challenges. From the embedding learning point
    of a view, these methods can be roughly subdivided into four categories: *2D embedding
    based* *multi-tasks learning*, *clustering based*, and *dynamic convolution based*.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 2D embedding based: An example of these methods is the 3D-BEVIS Elich et al.
    ([2019](#bib.bib21)) that learns 2D global instance embedding with a bird’s-eye-view
    of the full scene. It then propagates the learned embedding onto point clouds
    by DGCNN Wang et al. ([2019b](#bib.bib155)). Another example is PanopticFusion
    Narita et al. ([2019](#bib.bib108)) which predicts pixel-wise instance labels
    by 2D instance segmentation network Mask R-CNN He et al. ([2017a](#bib.bib42))
    for RGB frames and integrates the learned labels into 3D volumes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Multi-tasks learning: 3D semantic segmentation and 3D instance segmentation
    can influence each other. For example objects with different classes must be different
    instances, and objects with the same instance label must be the same class. Based
    on this, ASIS Wang et al. ([2019a](#bib.bib152)) designs an encoder-decoder network,
    termed ASIS, to learn semantic-aware instance embeddings for boosting the performance
    of the two tasks. Similarly, JSIS3D Pham et al. ([2019b](#bib.bib114)) uses a
    unified network namely MT-PNet to predict the semantic labels of points and embedding
    the points into high-dimensional feature vectors, and further propose a MV-CRF
    to jointly optimize object classes and instance labels. Similarly Liu et al. Liu
    and Furukawa ([2019](#bib.bib90)) and 3D-GEL Liang et al. ([2019b](#bib.bib88))
    adopt SSCN to generate semantic predictions and instance embeddings simultaneously,
    then use two GCNs to refine the instance labels. OccuSeg Han et al. ([2020](#bib.bib39))
    uses a multi-task learning network to produce both occupancy signal and spatial
    embedding. The occupancy signal represents the number of voxel occupied by per
    voxel.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering based: methods like MASC Liu and Furukawa ([2019](#bib.bib90)) rely
    on high performance of the SSCN Graham et al. ([2018](#bib.bib32)) to predict
    the similarity embedding between neighboring points at multiple scales and semantic
    topology. A simple yet effective clustering Liu et al. ([2018c](#bib.bib96)) is
    adapted to segment points into instances based on the two types of learned embeddings.
    MTML Lahoud et al. ([2019](#bib.bib71)) learns two sets of feature embeddings,
    including the feature embedding unique to every instance and the direction embedding
    that orients the instance center, which provides a stronger grouping force. Similarly,
    PointGroup Jiang et al. ([2020b](#bib.bib63)) groups points into different clusters
    based on the original coordinate embedding space and the shifted coordinate embedding
    space. In addition, the proposed ScoreNet guides the proper cluster selection.
    The above methods usually group points according to point-level embeddings, without
    the instance-level corrections. HAIS Chen et al. ([2021](#bib.bib10)) introduce
    the set aggregation and intra-instance prediction to refine the instance at the
    object level.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Dynamic convolution based: These methods overcome the limitations of clustering
    based methods by generating kernels and then using them to convolve with the point
    features to generate instance masks. Dyco3D He et al. ([2021](#bib.bib43)) adopts
    the clustering algorithm to generate a kernel for convolution. Similarly, PointInst3D
    He et al. ([2022](#bib.bib44)) uses FPS to generate kernels. DKNet Wu et al. ([2022b](#bib.bib163))
    introduces candidate mining and candidate aggregation to generate more instance
    kernels. Moreover, ISBNet Ngo et al. ([2023](#bib.bib110)) proposes a new instance
    encoder combining instance-aware PFS with a point aggregation layer to generate
    kernels to replace clustering in DyCo3D. 3D instance segmentation methods are
    summarized in Table [5](#S4.T5 "Table 5 ‣ 4.2 Proposal Free ‣ 4 3D Instance Segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Summary of 3D instance segmentation methods with deep learning. M←multi-view
    image; Me←mesh;V←voxel; P←point clouds.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Input | Propo./Embed. Prediction | Refining/Grouping | Contribution
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| proposal based | GSPN Yi et al. ([2019](#bib.bib183)) | P | GSPN | R-PointNet
    | New proposal generation methods |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | M+V | 3D-RPN+3D-RoI | 3DFCN | Learning
    bounding box on geometry and RGB |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | P | Bounding box regression
    | Point mask prediction | Directly regressing 3D bounding box |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| SGPN Wang et al. ([2018d](#bib.bib151)) | P | SM + SCM + PN | Non-Maximum
    suppression | New group proposal |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | p | SSCNet | Graph ConvNet
    | Multi proposal aggregation strategy |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '| AS-Net Jiang et al. ([2020a](#bib.bib61)) | p | Four branches with MLPs |
    Candidate proposal suppression | Novel Algorithm mapping labels to candidates
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
- en: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | P | Soft-grouping module | top-down
    refinment | Novel clustering algorithm based on dual coordinate sets |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| SSTNet Liang et al. ([2021](#bib.bib86)) | p | Tree traversal + splitting
    | CliqueNet | Constructing the superpoint tree for instance segmentation |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| proposal free | 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | M | U-Net/FCN
    + 3D prop. | Mean-shift clustering | Joint 2D-3D feature |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| PanopticFus Narita et al. ([2019](#bib.bib108)) | M | PSPNet/Mask R-CNN |
    FC-CRF | Coopering with semantic mapping |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| ASIS Wang et al. ([2019a](#bib.bib152)) | P | 1 encoder+ 2 decoders | ASIS
    module | Simultaneously performing sem./ins. segmentation tasks |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| JSIS3D Pham et al. ([2019b](#bib.bib114)) | P | MT-PNet | MV-CRF | Simultaneously
    performing sem./ins. segmentation tasks |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| 3D-GEL Liang et al. ([2019b](#bib.bib88)) | P | SSCNet | GCN | Structure-aware
    loss function + attention-based GCN |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| OccuSeg Han et al. ([2020](#bib.bib39)) | P | 3D-UNet | Graph-based clustering
    | Proposing a novel occupancy signal |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: '| MASC Liu and Furukawa ([2019](#bib.bib90)) | Me | U-Net with SSConv | Clustering
    algorithm | Novel clustering based on affinity and mesh topology |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
- en: '| MTML Lahoud et al. ([2019](#bib.bib71)) | V | SSCNet | Mean-shift clustering
    | Multi-task learning |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | P | U-Net with SSConv | Point
    clustering + ScoreNet | Novel clustering algorithm based on dual coordinate sets
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| HAIS Chen et al. ([2021](#bib.bib10)) | P | 3D U-Net | Set aggregation |
    Hierarchical aggregation for fine-grained predictions |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| Dyco3D He et al. ([2021](#bib.bib43)) | P | 3D U-Net | Dynamic conv. | Generating
    kernel by clustering for convolution |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| PointInst3D He et al. ([2022](#bib.bib44)) | P | 3D U-Net | MLP | Generating
    kernel by FPS |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| DKNet Wu et al. ([2022b](#bib.bib163)) | P | 3D U-Net | MLP | Generating
    kernel by candidate mining and aggregation |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | P | 3D U-Net | Box-aware dynamic
    conv | Generating kernel by instance aware FPS and point aggrega. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: 5 3D Part Segmentation
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '3D part segmentation is the next finer level, after instance segmentation,
    where the aim is to label different parts of an instance. The pipeline of part
    segmentation is quite similar to that of semantic segmentation except that the
    labels are now for individual parts. Therefore, some existing 3D semantic segmentation
    networks Meng et al. ([2019](#bib.bib103)), Graham et al. ([2018](#bib.bib32)),
    Qi et al. ([2017a](#bib.bib115)), Qi et al. ([2017b](#bib.bib116)), Zeng and Gevers
    ([2018](#bib.bib188)), Huang et al. ([2018](#bib.bib54)), Thomas et al. ([2019](#bib.bib139)),
    Hua et al. ([2018](#bib.bib52)), Hermosilla et al. ([2018](#bib.bib46)), Wu et al.
    ([2019b](#bib.bib161)), Li et al. ([2018b](#bib.bib82)), Wang et al. ([2019b](#bib.bib155)),
    Lei et al. ([2020](#bib.bib78)), Xie et al. ([2020b](#bib.bib171)), Wang et al.
    ([2018c](#bib.bib149)), Groh et al. ([2018](#bib.bib33)), Lei et al. ([2019](#bib.bib77)),
    Su et al. ([2018](#bib.bib134)), Rosu et al. ([2019](#bib.bib124)) can also be
    trained for part segmentation. However, these networks can not entirely tackle
    the difficulties of part segmentation. For example, various parts with the same
    semantic label might have diverse shapes, and the number of parts for an instance
    with the same semantic label may be different. We subdivide 3D part segmentation
    methods into two categories: *regular data* based and *irregular data* based as
    follows.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Regular Data Based
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular data usually includes projected images Kalogerakis et al. ([2017](#bib.bib65)),
    voxels Wang and Lu ([2019](#bib.bib156)), Le and Duan ([2018](#bib.bib76)), Song
    et al. ([2017](#bib.bib133)). As for projected images, Kalogerakis et al. Kalogerakis
    et al. ([2017](#bib.bib65)) obtain a set of images from multiple views that optimally
    cover object surface, and then use multi-view Fully Convolutional Networks(FCNs)
    and surface-based Conditional Random Fields (CRFs) to predict and refine part
    labels separately. Voxel is a useful representation of geometric data. However,
    fine-grained tasks like part segmentation require high resolution voxels with
    more detailed structure information, which leads to high computation cost. Wang
    et al. Wang and Lu ([2019](#bib.bib156)) proposed VoxSegNet to exploit more detailed
    information from voxels with limited resolution. They use spatial dense extraction
    to preserve the spatial resolution during the sub-sampling process and an attention
    feature aggregation (AFA) module to adaptively select scale features. Le et al.
    Le and Duan ([2018](#bib.bib76)) introduced a novel 3D CNN called PointGrid, to
    incorporate a constant number of points with each cell allowing the network to
    learn better local geometry shape details. Furthermore, multiple model fusion
    can enhance the segmentation performance. Combining the advantages of images and
    voxels, Song et al. Song et al. ([2017](#bib.bib133)) proposed a two-stream FCN,
    termed AppNet and GeoNet, to explore 2D appearance and 3D geometric features from
    2D images. In particular, their VolNet extracts 3D geometric features from 3D
    volumes guiding GeoNet to extract features from a single image.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Summary of 3D part segmentation methods. M←multi-view image; Me←mesh;V←voxel;P←point
    clouds; reg.←regular data; irreg.←irregular data.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Methods | Input | Architecture | Feature extractor | Contribution
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| regular | ShapePFCN Kalogerakis et al. ([2017](#bib.bib65)) | M | Multi-stream
    FCN | 2DConv | Per-label confidence maps + surface-based CRF |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| VoxSegNet Wang and Lu ([2019](#bib.bib156)) | V | 3DU-Net | AtrousConv |
    SDE for preserving the spatial resolution AFA for feature selecting |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| Pointgrid Le and Duan ([2018](#bib.bib76)) | V | Conv-deconv | 3DConv | Learning
    higher order local geometry shape. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| SubvolumeSup Song et al. ([2017](#bib.bib133)) | M+V | 2-stream FCN | 2D/3DConv
    | GeoNet/AppNet for 3/2D features exploi. + DCT for aligning. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| irregular | DCN Xu et al. ([2017](#bib.bib173)) | Me | 2-tream DCN & NN |
    DirectionalConv | DCN/NN for local feature and global feature. |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| MeshCNN Hanocka et al. ([2019](#bib.bib40)) | Me | 2D CNN | MeshConv | Novel
    mesh convolution and pooling |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| PartNet Yu et al. ([2019](#bib.bib185)) | P | RNN | PN | Part feature learning
    scheme for context and geometry feature exploitation |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| SSCNN Yi et al. ([2017](#bib.bib182)) | P | FCN | SpectralConv | STN for
    allowing weight sharing + spectral multi-scale kernel |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| KCNet Shen et al. ([2018](#bib.bib126)) | P | PN | MLP | KNN graph on points
    + kernel correlation for measuring geometric affinity |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| SFCN Wang et al. ([2018b](#bib.bib146)) | P | FCN | SFConv | Novel point
    convolution |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| SpiderCNN Xu et al. ([2018](#bib.bib175)) | P | PN | SpiderConv | Novel point
    convolution |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| FeaStNet Verma et al. ([2018](#bib.bib141)) | P | U-Net | GConv | Dynamic
    graph convolution filters |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| Kd-Net Klokov and Lempitsky ([2017](#bib.bib67)) | P | Kd-tree | Affine Transformation
    | Useing Kd-tree to build graphs and share learnable parameters |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| O-CNN Wang et al. ([2017](#bib.bib148)) | P | Octree | 3DConv | Making 3D-CNN
    feasible for high-resolu. voxels |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| PointCapsNet Zhao et al. ([2019b](#bib.bib195)) | P | Encoder-decoder | PN
    | Semi-supervision learning |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| SO-Net Li et al. ([2018a](#bib.bib81)) | P | Encoder-decoder | FC layers
    | SOM for modeling spatial distribution + un-supervision learning |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: 5.2 Irregular Data Based
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Irregular data representations usually includes meshes Xu et al. ([2017](#bib.bib173)),
    Hanocka et al. ([2019](#bib.bib40)) and point clouds Li et al. ([2018a](#bib.bib81)),
    Shen et al. ([2018](#bib.bib126)), Yi et al. ([2017](#bib.bib182)), Verma et al.
    ([2018](#bib.bib141)), Wang et al. ([2018b](#bib.bib146)), Yu et al. ([2019](#bib.bib185)),
    Zhao et al. ([2019b](#bib.bib195)) Yue et al. ([2022](#bib.bib187)). Mesh provides
    an efficient approximation to a 3D shape because it captures the flat, sharp and
    intricate of surface shape surface and topology. Xu et al. Xu et al. ([2017](#bib.bib173))
    put the face normal and face distance histogram as the input of a two-stream framework
    and use the CRF to optimize the final labels. Inspired by traditional CNN, Hanocka
    et al. Hanocka et al. ([2019](#bib.bib40)) design novel mesh convolution and pooling
    to operate on the mesh edges.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: As for point clouds, the graph convolution is the most commonly used pipeline.
    In the spectral graph domain, SyncSpecCNN Yi et al. ([2017](#bib.bib182)) introduces
    a Sychronized Spectral CNN to process irregular data. Specially, multichannel
    convolution and parametrized dilated convolution kernels are proposed to solve
    multi-scale analysis and information sharing across shapes respectively. In spatial
    graph domain, in analogy to a convolution kernel for images, KCNet Shen et al.
    ([2018](#bib.bib126)) present point-set kernel and nearest-neighbor-graph to improve
    PointNet with an efficient local feature exploitation structure. Similarly, Wang
    et al. Wang et al. ([2018b](#bib.bib146)) design Shape Fully Convolutional Networks
    (SFCN) based on graph convolution and pooling operation, similar to FCN on images.
    SpiderCNN Xu et al. ([2018](#bib.bib175)) applies a special family of convolutional
    filters that combine simple step function with Taylor polynomial, making the filters
    to effectively capture intricate local geometric variations. Furthermore, FeastNet
    Verma et al. ([2018](#bib.bib141)) uses dynamic graph convolution operator to
    build relationships between filter weights and graph neighborhoods instead of
    relying on static graph of the above network.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: A special kind of graphs, the trees (e.g. Kd-tree and Octree), work on 3D shapes
    with different representations and can support various CNN architectures. Kd-Net
    Klokov and Lempitsky ([2017](#bib.bib67)) uses a kd-tree data structure to represent
    point cloud connectivity. However, the networks have high computational cost.
    O-CNN Wang et al. ([2017](#bib.bib148)) designs an Octree data structure from
    3D shapes. However, the computational cost of the O-CNN grows quadratically as
    the depth of tree increases.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'SO-Net Li et al. ([2018a](#bib.bib81)) sets up a Self-Organization Map (SOM)
    from point clouds, and hierarchically learns node-wise features on this map using
    the PointNet architecture. However, it fails to fully exploit local features.
    PartNet Yu et al. ([2019](#bib.bib185)) decomposes 3D shapes in a top-down fashion,
    and proposes a Recursive Neural Network (RvNN) for learning the hierarchy of fine-grained
    parts. Zhao et al. Zhao et al. ([2019b](#bib.bib195)) introduce an encoder-decoder
    network, 3D-PointCapsNet, to tackle several common point cloud-related tasks.
    The dynamic routing scheme and the peculiar 2D latent space deployed by capsule
    networks, deployed in their model, bring improved performance. The 3D part segmentation
    methods are summarized in Table [6](#S5.T6 "Table 6 ‣ 5.1 Regular Data Based ‣
    5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 6 Applications of 3D Segmentation
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We review 3D semantic segmentation methods for two main applications, unmanned
    systems.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Unmanned Systems
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As LIDAR scanners and depth cameras become widely available and more affordable,
    they are increasingly being deployed in unmanned systems such as autonomous driving
    and mobile robots. These sensors provide realtime 3D video, generally at 30 frames
    per second (fps), as direct input to the system making *3D video semantic segmentation*
    as the primary task to understand the scene. Furthermore, in order to interact
    more effectively with the environment, unmanned systems generally build a *3D
    semantic map* of the scene. Below we review 3D video based semantic segmentation
    and 3D semantic map construction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 3D video semantic segmentation
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to the 3D single frame/scan semantic segmentation methods reviewed
    in Section 3.1, 3D video (continuous frames/scans) semantic segmentation methods
    take into account the connecting spatio-temporal information between frames which
    is more powerful at parsing the scene robustly and continuously. Conventional
    convolutional neural networks (CNNs) are not designed to exploit the temporal
    information between frames. A common strategy is to adapt Recurrent Neural Networks
    or Spatio-temporal convolutional network.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural network based: RNNs generally work in combination with 2D CNNs
    to process RGB-D videos. The 2D CNN learns to extract the frame-wise spatial information
    and the RNN learns to extract the temporal information between the frames. Valipour
    et al. Valipour et al. ([2017](#bib.bib140)) proposed Recurrent Fully Neural Network
    to operate over a sliding window over the RGB-D video frames. Specifically, the
    convolutional gated recurrent unit preserves the spatial information and reduces
    the parameters. Similarly, Yurdakul et al. Emre Yurdakul and Yemez ([2017](#bib.bib22))
    combine fully convolutional and recurrent neural network to investigate the contribution
    of depth and temporal information separately in the synthetic RGB-D video.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Spatio-temporal convolution based: Nearby video frames provide diverse viewpoints
    and additional context of objects and scenes. STD2P He et al. ([2017b](#bib.bib45))
    uses a novel spatio-temporal pooling layer to aggregate region correspondences
    computed by optical flow and image boundary-based super-pixels. Choy et al. Choy
    et al. ([2019](#bib.bib16)) proposed 4D Spatio-Temporary ConvNet, to directly
    process a 3D point cloud video. To overcome challenges in the high-dimensional
    4D space (3D space and time), they introduced the 4D sptio-temporal convolution,
    a generalized sparse convolution, and the trilateral-stationary conditional random
    field that keeps spatio-temporal consistency. Similarly, based on 3D sparse convolution,
    Shi et al. Shi et al. ([2020](#bib.bib127)) proposed SpSequenceNet that contains
    two novel modules, a cross frame-global attention module and a cross-frame local
    interpolation module to exploit spatial and temporal feature in 4D point clouds.
    PointMotionNet Wang et al. ([2022](#bib.bib144)) proposes a spatio-temporal convolution
    that exploits a time-invariant spatial neighboring space and extracts spatio-temporal
    features, to distinguish the moving and static objects.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatio-temporal Transformer based : To capture the dynamics in point cloud
    video, point tracking is usually employed. However, P4Transformer Fan et al. ([2021](#bib.bib28))
    proposes a 4D convolution to embed the spatio-temporal local structures in point
    cloud video and further introduces a Transformer to leverage the motion information
    across the entire video by performing the self-attention on these embedded local
    features. Similarly, PST² Wei et al. ([2022](#bib.bib158)) performs spatio-temporal
    self attention across adjacent frames to capture the spatio-temporal context,
    and proposes a resolution embedding mudule to enhance the resolution of feature
    maps by aggregating features.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 3D semantic map construction
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unmanned systems do not just need to avoid obstacles but also need to establish
    a deeper understanding of the scene such as object parsing, self localization
    etc. To facilitate such tasks, unmanned systems build a 3D semantic map of the
    scene which includes two key problems: geometric reconstruction and semantic segmentation.
    3D scene reconstruction has conventionally relied on simultaneous localization
    and mapping system (SLAM) to obtain a 3D map without semantic information. This
    is followed by 2D semantic segmentation with a 2D CNN and then the 2D labels are
    transferred to the 3D map following an optimization (e.g. conditional random field)
    to obtain a 3D semantic map Yang et al. ([2017](#bib.bib178)). This common pipeline
    does not guarantee high performance of 3D semantic maps in complex, large-scale,
    and dynamic scenes. Efforts have been made to enhance the robustness using association
    information exploitation from multiple frames, multi-model fusion and novel post-processing
    operations. These efforts are explained below.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Association information exploitation: mainly depends on SLAM trajectory, recurrent
    neural networks or scene flow. Ma et al. Ma et al. ([2017](#bib.bib99)) enforce
    consistency by warping CNN feature maps from multi-views into a common reference
    view by using the SLAM trajectory and to supervise training at multiple scales.
    SemanticFusion McCormac et al. ([2017](#bib.bib102)) incorporates deconvolutional
    neural networks with a state-of-the-art dense SLAM system, ElasticFusion, which
    provides long-term correspondence between frames of a video. These correspondences
    allow label predictions from multi-views to be probabilistically fused into a
    map. Similarly, using the connection information between frames provided by a
    recurrent unit on RGB-D videos, Xiang et al. Xiang and Fox ([2017](#bib.bib168))
    proposed a data associated recurrent neural networks (DA-RNN) and integrated the
    output of the DA-RNN with KinnectFusion, which provides a consistent semantic
    labeling of the 3D scene. Cheng et al. Cheng et al. ([2020](#bib.bib12)) use a
    CRF-RNN-based semantic segmentation to generate the corresponding labels. Specifically,
    the authors proposed an optical flow-based method to deal with the dynamic factors
    for accurate localization. Kochanov et al. Kochanov et al. ([2016](#bib.bib68))
    also use scene flow to propagate dynamic objects within the 3D semantic maps.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Multiple model fusion: Jeong et al. Jeong et al. ([2018](#bib.bib60)) build
    a 3D map by estimating odometry based on GPS and IMU, and use a 2D CNN for semantic
    segmentation. They integrate the 3D map with semantic labels using a coordinate
    transformation and Bayes’ update scheme. Zhao et al. Zhao et al. ([2018](#bib.bib192))
    use PixelNet and VoxelNet to exploit global context information and local shape
    information separately and then fuse the score maps with a softmax weighted fusion
    that adaptively learns the contribution of different data streams. The final dense
    3D semantic maps are generated with visual odometry and recursive Bayesian update.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 7 Experimental Results
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Below we summarize the quantitative results of the segmentation methods discussed
    in Sections [3](#S3 "3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey"), [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") and [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") on some typical public datasets, as well as analyze these results qualitatively.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Evaluation performance regarding for RGB-D semantic segmentation methods
    on the SUN-RGB-D and NYUDv2\. Note that the ‘%’ after the value is omitted and
    the symbol ‘–’ means the results are unavailable.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | NYUDv2 | SUN-RGB-D |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| mAcc | mIoU | mAcc | mIoU |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Guo and Chen ([2018](#bib.bib35)) | 46.3 | 34.8 | 45.7 | 33.7 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2015](#bib.bib147)) | – | 44.2 | – | – |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| Mousavian et al. ([2016](#bib.bib107)) | 52.3 | 39.2 | – | – |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2018b](#bib.bib94)) | 50.8 | 39.8 | 50.0 | 39.4 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| Gupta et al. ([2014](#bib.bib37)) | 35.1 | 28.6 | – | – |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2018a](#bib.bib93)) | 51.7 | 41.2 | – | – |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| Hazirbas et al. ([2016](#bib.bib41)) | – | – | 48.3 | 37.3 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. ([2017](#bib.bib89)) | – | 47.7 | – | 48.1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| Jiang et al. ([2017](#bib.bib62)) | – | – | 50.6 | 39.3 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| Wang and Neumann ([2018](#bib.bib150)) | 47.3 | – | – | – |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: '| Cheng et al. ([2017](#bib.bib13)) | 60.7 | 45.9 | 58.0 | – |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
- en: '| Fan et al. ([2017](#bib.bib27)) | 50.2 | – | – | – |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
- en: '| Li et al. ([2016](#bib.bib84)) | 49.4 | – | 48.1 | – |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '| Qi et al. ([2017c](#bib.bib117)) | 55.7 | 43.1 | 57.0 | 45.9 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2016](#bib.bib145)) | 60.6 | 38.3 | 50.1 | 33.5 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Evaluation performance regarding for projected images, voxel, point
    clouds and other representation semantic segmentation methods on the S3DIS, ScanNet,
    Semantic3D and SemanticKITTI. Note: the ‘%’ after the value is omitted, the symbol
    ‘–’ means the results are unavailable, the dotted line means the subdivision of
    methods according to the type of architecture.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Type | S3DIS | ScanNet | Semantic3D | SemanticKITTI |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
- en: '|  | Area5 | 6-fold | test set | reduced-8 | only xyz |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: '|  | mAcc | mIoU | mIoU | oAcc | mIoU | oAcc | mIoU | mAcc | mIoU |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
- en: '| Lawin et al. Lawin et al. ([2017](#bib.bib75)) | projection | – | – | – |
    – | – | 88.9 | 58.5 | – | – |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
- en: '| Boulch et al. Boulch et al. ([2017](#bib.bib6)) |  | – | – | – | – | – |
    91.0 | 67.4 | – | – |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) |  | – | – | – | – | – | – | –
    | – | 37.2 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) |  | – | – | – | – | – | –
    | – | – | 39.8 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) |  | – | – | – | – | – | – | –
    | – | 44.9 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) |  | – | – | – | – |
    – | – | – | – | 52.2 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. Xu et al. ([2020](#bib.bib172)) |  | – | – | – | – | – | – | –
    | – | 55.9 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| RangViT Ando et al. ([2023](#bib.bib1)) |  | – | – | – | – | – | – | – |
    – | 55.9 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| RangFormer Kong et al. ([2023](#bib.bib70)) |  | – | – | – | – | – | – |
    – | – | 64.0 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | voxel | 57.35 | 48.92
    | 48.92 | – | – | 88.1 | 61.30 | – | – |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Meng et al. Meng et al. ([2019](#bib.bib103)) |  | – | 78.22 | – | – | –
    | – | – | – | – |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. Liu et al. ([2017](#bib.bib91)) |  | – | 70.76 | – | – | – | –
    | – | – | – |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| PointNet Qi et al. ([2017a](#bib.bib115)) | point | 48.98 | 41.09 | 47.71
    | – | 14.69 | – | – | 29.9 | 17.9 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) |  | 59.10 | 52.17 | 58.27 |
    75.53 | – | – | – | 57.59 | 29.9 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| ESC Engelmann et al. ([2017](#bib.bib24)) |  | 54.06 | 45.14 | 49.7 | 63.4
    | – | – | – | 40.9 | 26.4 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| HRNN Ye et al. ([2018](#bib.bib180)) |  | 71.3 | 53.4 | – | 76.5 | – | –
    | – | 49.2 | 34.5 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) |  | – | 50.04 | 54.4 | 71.40
    | 34.26 | – | – | – | – |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| PointWeb Zhao et al. ([2019a](#bib.bib193)) |  | 66.64 | 60.28 | 66.7 | 85.9
    | – | – | – | – | – |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| PointSIFT Jiang et al. ([2018](#bib.bib64)) |  | – | 70.23 | 70.2 | – | 41.5
    | – | – | – | – |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| Resurf Ran et al. ([2022](#bib.bib120)) |  | 76.0 | 68.9 | 74.3 | – | 70.0
    | – | – | – | – |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: '| PointNeXt Qian et al. ([2022](#bib.bib118)) |  | – | 70.5 | 74.9 | – | 71.2
    | – | – | – | – |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
- en: '| RSNet Huang et al. ([2018](#bib.bib54)) |  | 59.42 | 56.5 | 56.47 | – | 39.35
    | – | – | – | – |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
- en: '| DPC Engelmann et al. ([2020b](#bib.bib25)) |  | 68.38 | 61.28 | – | – | 59.2
    | – | – | – | – |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
- en: '| PointwiseCNN Hua et al. ([2018](#bib.bib52)) |  | 56.5 | – | – | – | – |
    – | – | – | – |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) |  | 67.01 | 58.27 | – | – | 49.8
    | – | – | – | – |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
- en: '| PointCNN Li et al. ([2018b](#bib.bib82)) |  | 63.86 | 57.26 | 65.3 | 85.1
    | 45.8 | – | – | – | – |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
- en: '| KPConv Thomas et al. ([2019](#bib.bib139)) |  | – | 67.1 | 70.6 | – | 66.6
    | 92.9 | 74.6 | – | – |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) |  | – | 50.34 | – | – | 55.6
    | – | – | – | – |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
- en: '| A-CNN Komarichev et al. ([2019](#bib.bib69)) |  | – | – | – | 85.4 | – |
    – | – | – | – |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
- en: '| RandLA-Net Hu et al. ([2020](#bib.bib49)) |  | – | – | 70.0 | – | – | 94.8
    | 77.4 | – | 53.9 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| PolarNet Zhang et al. ([2020](#bib.bib190)) |  | – | – | – | – | – | – |
    – | – | 54.3 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| DGCNN Wang et al. ([2019b](#bib.bib155)) |  | – | 56.1 | 56.1 | – | – | –
    | – | – | – |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| SPG Landrieu and Simonovsky ([2018](#bib.bib74)) |  | 66.50 | 58.04 | 62.1
    | – | – | 94.0 | 73.2 | – | – |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) |  | 65.9 | 59.5 | 68.9 | – | 61.0
    | – | – | – | – |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| DeepGCNs Li et al. ([2019a](#bib.bib80)) |  | – | 60.0 | – | – | – | – |
    – | – | – |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| PointGCRNet Ma et al. ([2020](#bib.bib100)) |  | – | 52.43 | – | – | 60.8
    | – | – | – | – |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) |  | – | – | 56.63 | – | – | – | –
    | – | – |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| PAN Feng et al. ([2020](#bib.bib29)) |  | – | 66.3 | – | 86.7 | 42.1 | –
    | – | – | – |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '| TGNet Li et al. ([2019b](#bib.bib83)) |  | – | 58.7 | – | 66.2 | – | – |
    – | – | – |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| HDGCN Liang et al. ([2019a](#bib.bib87)) |  | 65.81 | 59.33 | 66.85 | – |
    – | – | – | – | – |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '| 3DContextNet Zeng and Gevers ([2018](#bib.bib188)) |  | 74.5 | 55.6 | 55.6
    | – | – | – | – | – | – |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '| PGCRNet Ma et al. ([2020](#bib.bib100)) |  | – | 54.4 | – | – | – | – | 69.5
    | – | – |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) |  | 74.5 | 55.6 | 55.6 | – | – |
    – | – | – | – |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| PointANSL Yan et al. ([2020](#bib.bib176)) |  | – | 62.6 | 68.7 | – | – |
    66.6 | – | – | – |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer Zhao et al. ([2021](#bib.bib194)) |  | 76.5 | 70.4 | 73.5
    | – | – | – | – | – | – |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Point Transformer v2 Wu et al. ([2022a](#bib.bib162)) |  | 77.9 | 71.6 |
    – | – | 75.2 | – | – | – | – |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| PatchFormer Zhang et al. ([2022](#bib.bib189)) |  | – | 68.1 | – | – | –
    | – | – | – | – |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Fast Point Transformer Park et al. ([2022](#bib.bib111)) |  | 77.3 | 70.1
    | – | – | – | – | – | – | – |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '| Stratify Transformer Lai et al. ([2022](#bib.bib73)) |  | 78.1 | 72.0 | –
    | – | 73.7 | – | – | – | – |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
- en: '| SphereFormer Lai et al. ([2023](#bib.bib72)) |  | – | – | – | – | – | – |
    – | – | 78.4 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
- en: '| TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | others | 62.2 | 52.8
    | – | 80.1 | 40.9 | 89.3 | 66.4 | – | – |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) |  | – | – | – | – | 39.3 | – |
    – | – | – |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) |  | – | – | – | – | 64.0 |
    – | – | – | 52.9 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) |  | – | – | – | – | 63.4
    | – | – | – | – |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| PVCNN Liu et al. ([2019b](#bib.bib97)) |  | 87.12 | 58.98 | – | – | – | –
    | – | – | – |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) |  | – | – | – | – | 66.4 | – |
    – | – | – |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| BPNet Hu et al. ([2021](#bib.bib50)) |  | – | – | – | – | 74.9 | – | – |
    – | – |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: 7.1 Results for 3D Semantic Segmentation
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We report the results of RGB-D based semantic segmentation methods on SUN-RGB-D
    Song et al. ([2015](#bib.bib132)) and NYUDv2 Silberman et al. ([2012](#bib.bib130))
    datasets using mAcc (mean Accuracy) and mIoU (mean Intersection over Union) as
    the evaluation metrics. These results of various methods are taken from the original
    papers and they are shown in Table [7](#S7.T7 "Table 7 ‣ 7 Experimental Results
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'We report the results of projected images/voxel/point clouds/other representation
    semantic segmentation methods on S3DIS Armeni et al. ([2016](#bib.bib2)) (both
    Area 5 and 6-fold cross validation), ScanNet Dai et al. ([2017](#bib.bib18)) (test
    sets), Semantic3D Hackel et al. ([2017](#bib.bib38)) (reduced-8 subsets) and SemanticKITTI
    Behley et al. ([2019](#bib.bib3)) (only xyz without RGB). We use mAcc, oAcc (overall
    accuracy) and mIoU as the evaluation metrics. These results of various methods
    are taken from the original papers. Table [8](#S7.T8 "Table 8 ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey") reports the results.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: 'The architectures of point cloud semantic segmentation typically focus on five
    main components: basic framework, neighborhood search, features abstraction, coarsening,
    and pre-processing. Below, we provide a more detailed discussion of each component.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: 'Basic framework: Basic networks are one of the main driving forces behind the
    development of 3D segmentation. Generally, there are two main basic frameworks
    including PointNet and PointNet++. The PointNet framework utilizes shared Multi-Layer
    Perceptrons (MLPs) to capture point-wise features and employs max-pooling to aggregate
    these features into a global representation. However, it lacks the ability to
    learn local features due to the absence of a defined local neighborhood. Additionally,
    the fixed resolution of the feature map makes it challenging to adapt to deep
    architectures. In contrast, the PointNet++ framework introduces a novel hierarchical
    learning architecture. It defines local regions in a hierarchical manner and to
    progressively extract features from these regions. This approach enables the network
    to capture both local and global information, leading to improved performance.
    As a result, many current networks adopt the PointNet++ framework or similar variations
    (such as 3D U-Net). This framework significantly reduces computational and memory
    complexities, particularly in high-level tasks like semantic segmentation, instance
    segmentation, and detection.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'Neighborhood search: To exploit the local features of point clouds, the neighborhood
    point search is introduced into networks, including the K nearest neighbors (KNN)
    Zhao et al. ([2021](#bib.bib194)), Ran et al. ([2022](#bib.bib120)), Qian et al.
    ([2022](#bib.bib118)), Ball search Hermosilla et al. ([2018](#bib.bib46)), Thomas
    et al. ([2019](#bib.bib139)), Lei et al. ([2020](#bib.bib78)), grid-based search
    Hua et al. ([2018](#bib.bib52)), Wu et al. ([2022a](#bib.bib162)) and tree based
    search Lei et al. ([2019](#bib.bib77)). KNN search retrieves the K closest neighbors
    to a query point based on a distance metric, and hence lacks robustness to point
    clouds with varying densities. Some works integrate the dilated mechanism with
    the neighbor search to expand the receptive field Komarichev et al. ([2019](#bib.bib69)),
    Li et al. ([2018b](#bib.bib82)), Li et al. ([2019a](#bib.bib80)). Ball search
    involves finding all points within a specified radius (ball) around a query point.
    Similarly, grid-based search divides the point cloud space into a regular grid
    structure. Ball search and grid-based search are both useful for effectively capturing
    local structures and neighborhoods of varying densities.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Features abstraction: In feature abstraction, commonly used methods include
    MLP-based, Convolution-based, and Transformer-based approaches. MLP is often used
    to extract features from individual points in point cloud data. By passing the
    feature vectors of each point through multiple fully connected layers, MLP learns
    nonlinear point-level feature representations. MLP offers flexibility and scalability
    in point cloud processing. Convolution operations on point clouds typically involve
    aggregating (low-level) information from local points to capture local structures
    and contextual information. In contrast, Transformer based methods establish correlations
    between high-level point information through the attention mechanism, which is
    more helpful for high-level tasks such as point cloud segmentation.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: The essence of MLP based, convolution based, and Transformer based methods is
    to learn the relationships between points, obtaining the robust weights. In the
    context of similar baseline architecture, the more comprehensive the learned point
    cloud relationship in the feature abstraction process, the stronger the robustness
    of the model becomes. Recently, MLP-based methods (e.g. Resurf Ran et al. ([2022](#bib.bib120))
    and PointNeXt Qian et al. ([2022](#bib.bib118))) exhibit better accuracy and efficiency,
    encouraging researchers to re-examine and further explore the potential of MLP-based
    approaches.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Evaluation performance regarding for 3D instance segmentation methods
    on the ScanNet. Note: the ‘%’ after the value is omitted.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | mAP | bath. | bed | book. | cabi. | chair | count. | curt. | desk
    | door | other | pict. | refr. | shower. | sink | sofa | table | toilet | wind.
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: '| GSPN Yi et al. ([2019](#bib.bib183)) | 30.6 | 50.0 | 40.5 | 31.1 | 34.8 |
    58.9 | 5.4 | 6.8 | 12.6 | 28.3 | 29.0 | 2.8 | 21.9 | 21.4 | 33.1 | 39.6 | 27.5
    | 82.1 | 24.5 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
- en: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | 38.2 | 100 | 43.2 | 24.5 | 19.0
    | 57.7 | 1.3 | 26.3 | 3.3 | 32.0 | 24.0 | 7.5 | 42.2 | 85.7 | 11.7 | 69.9 | 27.1
    | 88.3 | 23.5 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | 48.8 | 100 | 67.2 | 59.0 | 30.1
    | 48.4 | 9.8 | 62.0 | 30.6 | 34.1 | 25.9 | 12.5 | 43.4 | 79.6 | 40.2 | 49.9 |
    51.3 | 90.9 | 43.9 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| SGPN Wang et al. ([2018d](#bib.bib151)) | 14.3 | 20.8 | 39.0 | 16.9 | 6.5
    | 27.5 | 2.9 | 6.9 | 0 | 8.7 | 4.3 | 1.4 | 2.7 | 0 | 11.2 | 35.1 | 16.8 | 43.8
    | 13.8 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | 61.1 | 100 | 83.3 | 76.5
    | 52.6 | 75.6 | 13.6 | 58.8 | 47.0 | 43.8 | 43.2 | 35.8 | 65.0 | 85.7 | 42.9 |
    76.5 | 55.7 | 100 | 43.0 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | 76.1 | 100 | 80.8 | 84.5 | 71.6
    | 86.2 | 24.3 | 82.4 | 65.5 | 62.0 | 73.4 | 69.9 | 79.1 | 98.1 | 71.6 | 84.4 |
    76.9 | 100 | 59.4 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| SSTNet Liang et al. ([2021](#bib.bib86)) | 69.8 | 100 | 69.7 | 88.8 | 55.6
    | 80.3 | 38.7 | 62.6 | 41.7 | 55.6 | 58.5 | 70.2 | 60.0 | 100 | 82.4 | 72.0 |
    69.2 | 100 | 50.9 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '| 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | 24.8 | 66.7 | 56.6 | 7.6 | 3.5
    | 39.4 | 2.7 | 3.5 | 9.8 | 9.8 | 3.0 | 2.5 | 9.8 | 37.5 | 12.6 | 60.4 | 18.1 |
    85.4 | 17.1 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
- en: '| PanopticFus. Narita et al. ([2019](#bib.bib108)) | 47.8 | 66.7 | 71.2 | 59.5
    | 25.9 | 55.0 | 0 | 61.3 | 17.5 | 25.0 | 43.4 | 43.7 | 41.1 | 85.7 | 48.5 | 59.1
    | 26.7 | 94.4 | 35.9 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
- en: '| OccuSeg Han et al. ([2020](#bib.bib39)) | 67.2 | 100 | 75.8 | 68.2 | 57.6
    | 84.2 | 47.7 | 50.4 | 52.4 | 56.7 | 58.5 | 45.1 | 55.7 | 100 | 75.1 | 79.7 |
    56.3 | 100 | 46.7 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| MTML Lahoud et al. ([2019](#bib.bib71)) | 54.9 | 100 | 80.7 | 58.8 | 32.7
    | 64.7 | 0.4 | 81.5 | 18.0 | 41.8 | 36.4 | 18.2 | 44.5 | 100 | 44.2 | 68.8 | 57.1
    | 100 | 39.6 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | 63.6 | 100 | 76.5 | 62.4
    | 50.5 | 79.7 | 11.6 | 69.6 | 38.4 | 44.1 | 55.9 | 47.6 | 59.6 | 100 | 66.6 |
    75.6 | 55.6 | 99.7 | 51.3 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| HAIS Chen et al. ([2021](#bib.bib10)) | 69.9 | 100 | 84.9 | 82.0 | 67.5 |
    80.8 | 27.9 | 75.7 | 46.5 | 51.7 | 59.6 | 55.9 | 60.0 | 100 | 65.4 | 76.7 | 67.6
    | 99.4 | 56.0 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| Dyco3D He et al. ([2021](#bib.bib43)) | 64.1 | 100 | 84.1 | 89.3 | 53.1 |
    80.2 | 11.5 | 58.8 | 44.8 | 43.8 | 53.7 | 43.0 | 55.0 | 85.7 | 53.4 | 76.4 | 65.7
    | 98.7 | 56.8 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| DKNet Wu et al. ([2022b](#bib.bib163)) | 71.8 | 100 | 81.4 | 78.2 | 61.9
    | 87.2 | 22.4 | 75.1 | 56.9 | 67.7 | 58.5 | 72.4 | 63.3 | 98.1 | 51.5 | 81.9 |
    73.6 | 100 | 61.7 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | 76.3 | 100 | 87.3 | 71.7 | 66.6
    | 85.8 | 50.8 | 66.7 | 76.4 | 64.3 | 67.6 | 68.8 | 82.5 | 100 | 77.3 | 74.1 |
    77.7 | 100 | 55.6 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
- en: 'Coarsening: Coarsening, also known as downsampling or subsampling, involves
    reducing the number of points in the point cloud while preserving the essential
    structures and features. Coarsening techniques include random sampling Hu et al.
    ([2020](#bib.bib49)), farthest point sampling Qi et al. ([2017a](#bib.bib115),
    [b](#bib.bib116)), tree-based methods Lei et al. ([2019](#bib.bib77)) and mesh-based
    decimation Lei et al. ([2023](#bib.bib79)). This step helps to reduce computational
    complexity and improve efficiency in subsequent stages of the segmentation process.
    Random sampling is simple and computationally efficient but may not select the
    most optimal points in maintaining local and global structure. This can potentially
    lead to information loss in feature rich regions. Farthest point sampling is widely
    used in networks as it ensures a more even spatial distribution of the selected
    points and can help preserve global structures. However, local structures can
    still get destroyed with farthest point sampling. Tree-based methods leverage
    hierarchical tree structures, such as an octree, to partition the point cloud
    and perform coarsening. Mesh based methods must convert the point cloud to a mesh
    first before it can decimate it. This adds a computational overhead to the already
    expensive mesh decimation process. Moreover, creating a mesh from complex and
    sparse point clouds obtained from LiDAR sensors is not always possible Lei et al.
    ([2023](#bib.bib79)).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: The above methods are hand-crafted or engineered techniques that do not involve
    learning parameters directly from the data, which determines the sub-sampling
    pattern based on predefined rules or heuristics, without explicitly optimizing
    for the task at hand. Therefore, some works propose learnable coarsening methods
    that integrate a learnable layer into the coarsening module such as pooling Groh
    et al. ([2018](#bib.bib33)), Lai et al. ([2023](#bib.bib72)), Wu et al. ([2022a](#bib.bib162)),
    Zhao et al. ([2021](#bib.bib194)), attention mechanism Yan et al. ([2020](#bib.bib176)).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: 'Pre-processing: Pre-processing is an essential step in point cloud semantic
    segmentation that involves preparing and transforming the raw point cloud data
    before feeding it into the segmentation network. Pre-processing aims to enhance
    the quality, consistency, and suitability of the data for the segmentation task.
    Some common aspects of pre-processing in point cloud segmentation include data
    normalization, data augmentation, outlier removal, and point registration.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Point clouds often have varying scales, which can negatively affect the performance
    of segmentation networks. Data normalization involves scaling the point cloud
    data to a standard range or unit sphere to ensure consistent scales across different
    point. For example, the number of ShapNet object point is generally fixed as 4096\.
    For the complexity scene, early works Xu et al. ([2021](#bib.bib174)), Qi et al.
    ([2017a](#bib.bib115)) divided raw point clouds into smaller ones (e.g. 4096 points,
    1m³ blocks) so that the processing does not require large memory. However, this
    strategy might break down the semantic continuity of the scene. Recent works Qian
    et al. ([2022](#bib.bib118)), Lai et al. ([2023](#bib.bib72)), Wu et al. ([2022a](#bib.bib162)),
    Zhao et al. ([2021](#bib.bib194)), Lei et al. ([2023](#bib.bib79)) input the complete
    scene into the network, but that requires more computational sources. Moreover,
    these works tend to down sample the point cloud in the pre-processing stage.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/050ff7383543ce0cbd52abeb5414ad7c.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Evaluation performance regarding for 3D instance segmentation architecture,
    including proposal based and proposal free, on the different class of ScanNet.
    For simplicity, we omit the ‘%’ after the value.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Results for 3D Instance Segmentation
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We report the results of 3D instance segmentation methods on ScanNet Dai et al.
    ([2017](#bib.bib18)) datasets, and choose mAP as the evaluation metrics. These
    results of these methods are taken from the ScanNet Benchmark Challenge website,
    and they are shown in Table [9](#S7.T9 "Table 9 ‣ 7.1 Results for 3D Semantic
    Segmentation ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A
    Survey") and summarized in Figure [8](#S7.F8 "Figure 8 ‣ 7.1 Results for 3D Semantic
    Segmentation ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A
    Survey"). The table and figure shows that:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: ISBNet Ngo et al. ([2023](#bib.bib110)) has the state-of-the-art performance,
    with 76.3% average precision on ScanNet dataset at the time of this view. It also
    achieves the best instance segmentation performance on most classes, including
    ‘bathtub’, ‘counter’, ‘shower curtain’, ‘table’, ‘toilet’ and so on.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: Most methods have better segmentation performance on large scale classes such
    as ‘bathtub’ and ‘toilet’, and have poor segmentation performance on small scale
    classes such as ‘counter’, ‘desk’ and ‘picture’. Therefore, the instance segmentation
    of small objects is a prominent challenge.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: In proposal-based methods, specifically the 2D embedding propagating-based methods
    such as 3D-BEVIS Elich et al. ([2019](#bib.bib21)) and PanoticFusion Narita et al.
    ([2019](#bib.bib108)), they tend to exhibit poorer performance compared to other
    proposal-free methods. This is primarily because simple embedding propagation
    techniques are more susceptible to error labels, leading to inaccuracies in the
    instance segmentation results.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: Proposal-free methods demonstrate superior performance compared to proposal-based
    methods in instance segmentation across all classes, particularly for small objects
    like ’curtain,’ ’picture,’ ’shower curtain,’ and ’sink.’ Unlike proposal-based
    methods that rely on the accuracy of proposal generation, proposal-free methods
    circumvent this issue entirely. They directly consider the entire point cloud
    and its global features, enabling more precise and comprehensive instance segmentation.
    By avoiding the need for proposal generation, proposal-free methods can achieve
    better results by taking into account the overall context and characteristics
    of the point cloud.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Results for 3D Part Segmentation
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We report the results of 3D part segmentation methods on ShapeNet Yi et al.
    ([2016](#bib.bib181)) datasets and use Ins. mIoU as the evaluation metric. These
    results of various methods are taken from the original papers and they are shown
    in Table [10](#S7.T10 "Table 10 ‣ 7.3 Results for 3D Part Segmentation ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey"). We can find that part
    segmentation performance of all methods is quite similar. one underlying assumption
    is that objects in ShapeNet datasets are synthetic, normalized in scale, aligned
    in pose, and lack scene context. This makes part segmentation network difficult
    to extract rich context features. Another underlying assumption is that the point
    clouds in synthetic scene without background noise is more simpler and cleaner
    than ones in real scene, so that the geometric features of point clouds is easy
    to exploitation. The accuracy performance of various part segmentation network
    is difficult to be effectively distinguished.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Evaluation performance regarding for 3D part segmentation on the
    ShapeNet. Note: the ‘%’ after the value is omitted, the symbol ‘–’ means the results
    are unavailable.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Ins. mIoU | Methods | Ins. mIoU |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| VV-Net Meng et al. ([2019](#bib.bib103)) | 87.4 | LatticeNet Su et al. ([2018](#bib.bib134))
    | 83.9 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '| SSCNet Graham et al. ([2018](#bib.bib32)) | 86.0 | SGPN Wang et al. ([2018d](#bib.bib151))
    | 85.8 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
- en: '| PointNet Qi et al. ([2017a](#bib.bib115)) | 83.7 | ShapePFCN Kalogerakis
    et al. ([2017](#bib.bib65)) | 88.4 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) | 85.1 | VoxSegNet Wang and Lu
    ([2019](#bib.bib156)) | 87.5 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
- en: '| 3DContextNet Zeng and Gevers ([2018](#bib.bib188)) | 84.3 | PointgridLe and
    Duan ([2018](#bib.bib76)) | 86.4 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| RSNet Huang et al. ([2018](#bib.bib54)) | 84.9 | KPConv Thomas et al. ([2019](#bib.bib139))
    | 86.4 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
- en: '| MCC Hermosilla et al. ([2018](#bib.bib46)) | 85.9 | SO-Net Li et al. ([2018a](#bib.bib81))
    | 84.9 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) | 85.7 | PartNet Yu et al. ([2019](#bib.bib185))
    | 87.4 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
- en: '| DGCNN Wang et al. ([2019b](#bib.bib155)) | 85.1 | SyncSpecCNN Yi et al. ([2017](#bib.bib182))
    | 84.7 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) | 86.8 | KCNet Yi et al. ([2017](#bib.bib182))
    | 84.7 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) | 85.4 | PointCNN Li et al. ([2018b](#bib.bib82))
    | 86.1 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) | 85.9 | SpiderCNN Xu et al. ([2018](#bib.bib175))
    | 85.3 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| Flex-Conv Groh et al. ([2018](#bib.bib33)) | 85.0 | FeaStNet Verma et al.
    ([2018](#bib.bib141)) | 81.5 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| $\psi$-CNN Lei et al. ([2019](#bib.bib77)) | 86.8 | Kd-Net Klokov and Lempitsky
    ([2017](#bib.bib67)) | 82.3 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) | 84.6 | O-CNN Wang et al. ([2017](#bib.bib148))
    | 85.9 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| DRGCNN Yue et al. ([2022](#bib.bib187)) | 86.2 |  |  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: 8 Discussion and Conclusion
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3D segmentation using deep learning techniques has made significant progress
    during recent years. However, this is just the beginning and significant developments
    lie ahead of us. Below, we present some outstanding issues and identify potential
    research directions.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Synthetic datasets with richer information for multiple tasks: Synthetic datasets
    gradually play an important role on semantic segmentation due to the low cost
    and diverse scenes that can be generated Brodeur et al. ([2017](#bib.bib7)), Wu
    et al. ([2018b](#bib.bib164)) compared to real datasets Dai et al. ([2017](#bib.bib18)),
    Armeni et al. ([2016](#bib.bib2)), Hackel et al. ([2017](#bib.bib38)). It is well
    known that the information contained in training data determine the upper limit
    of the scene parsing accuracy. Existing datasets lack important semantic information,
    such as material, and texture information, which is more crucial for segmentation
    with similar color or geometric information. Besides, most exiting datasets are
    generally designed for a single task. Currently, only a few semantic segmentation
    datasets also contain labels for instances Dai et al. ([2017](#bib.bib18)) and
    scene layout Song et al. ([2015](#bib.bib132)) to meet the multi-task objective.'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unified network for multiple tasks: It is expensive and impractical for a system
    to accomplish different computer vision tasks by various deep learning networks.
    Towards fundamental feature exploitation of scene, Semantic segmentation has strong
    consistency with some tasks, such as depth estimation Meyer et al. ([2019](#bib.bib104)),
    Liu et al. ([2015](#bib.bib92)), Guo and Chen ([2018](#bib.bib35)), Liu et al.
    ([2018b](#bib.bib94)), scene completion Dai et al. ([2018](#bib.bib20)), Xia et al.
    ([2023](#bib.bib167)), Zhang et al. ([2023](#bib.bib191)), instance segmentation
    Liang et al. ([2019b](#bib.bib88)), Pham et al. ([2019b](#bib.bib114)), Han et al.
    ([2020](#bib.bib39)), and object detection Meyer et al. ([2019](#bib.bib104)),
    Lian et al. ([2022](#bib.bib85)). These tasks could cooperate with each other
    to improve performance in a unified network because they exhibit certain correlations
    and shared feature representations.'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple modals for segmentation: Semantic segmentation using multiple representations,
    such as projected images, voxels, and point clouds, has the potential to achieve
    higher accuracy. Single representation limits segmentation accuracy due to the
    limited scene information in some practical scenarios. For instance, LiDAR measurements
    become sparser as the distance increases, and incorporating high-resolution image
    data can improve performance on distant objects. Therefore, utilizing multiple
    representations, also known as multiple modalities, can be an alternative way
    to enhance segmentation performance Dai and Nießner ([2018](#bib.bib19)), Chiang
    et al. ([2019](#bib.bib14)), Liu et al. ([2019b](#bib.bib97)), Hu et al. ([2021](#bib.bib50)).
    Moreover, segmenting point cloud with large image models (such as SAM Kirillov
    et al. ([2023](#bib.bib66))) and natural language models like ChatGPT can be popular
    approaches. The advanced capabilities of large models enable them to capture intricate
    patterns and semantic relationships, leading to improved performance and accuracy
    in segmentation tasks.'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretable and sparse feature abstraction: Various features abstraction,
    including MLP, Convolution and Transformer, have undergone significant development.
    Feature abstraction modules may prioritize generating interpretable feature representations,
    enabling them to provide explanations for model decisions, visualizations of points
    of interest, and other interpretability functions. Moreover, in scenarios involving
    large-scale data and limited resources, the feature abstraction module improve
    computational efficiency.'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weakly-supervised and unsupervised segmentation: Deep learning has gained significant
    success in 3D segmentation, but heavily hinges on large-scale labelled training
    samples. Weakly-supervised learning refers to a training approach where the model
    is trained with limited or incomplete supervision. Unsupervised learning only
    use unlabelled training samples. weakly-supervised Su et al. ([2023](#bib.bib136)),
    Shi et al. ([2022](#bib.bib128)) and unsupervised Xiao et al. ([2023](#bib.bib169))
    paradigms are considered as an alternative to relax the impractical requirement
    of large-scale labelled datasets.'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Real-time and incremental segmentation: Real-time 3D scene parsing is crucial
    for some applications such as autonomous driving and mobile robots. However, most
    existing 3D semantic segmentation methods mainly focus on the improvement of segmentation
    accuracy but rarely focus on real-time performance. A few lightweight 3D semantic
    segmentation networks realize real-time by pre-processing point clouds into other
    presentations such as projected images Wu et al. ([2018a](#bib.bib159)), Wu et al.
    ([2019a](#bib.bib160)), Park et al. ([2023](#bib.bib112)), Ando et al. ([2023](#bib.bib1)).
    Additionally, incremental segmentation will become an important research direction,
    allowing models to incrementally update and adapt in dynamic scenes.'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3D video semantic segmentation: Like 2D video semantic segmentation, A handful
    of works try to exploit 4D spatio-temporal features on 3D videos (also call 4D
    point clouds) Wei et al. ([2022](#bib.bib158)), Fan et al. ([2021](#bib.bib28)).
    From these works, it can be seen that the spatio-temporal features can help improve
    the robustness of 3D video or dynamic 3D scene semantic segmentation.'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We provided a comprehensive survey of the recent development in 3D segmentation
    using deep learning techniques, including 3D semantic segmentation, 3D instance
    segmentation and 3D part segmentation. We presented a comprehensive performance
    comparison and merit of various methods in each category, with potential research
    directions being listed.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ando et al. (2023) Ando, A., Gidaris, S., Bursuc, A., Puy, G., Boulch, A.,
    Marlet, R., 2023. Rangevit: Towards vision transformers for 3d semantic segmentation
    in autonomous driving, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 5240–5250.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Armeni et al. (2016) Armeni, I., Sener, O., Zamir, A., Jiang, H., Brilakis,
    I., Fischer, M., Savarese, S., 2016. 3d semantic parsing of large-scale indoor
    spaces, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 1534–1543.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Behley et al. (2019) Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke,
    S., Stachniss, C., Gall, J., 2019. Semantickitti: A dataset for semantic scene
    understanding of lidar sequences, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, IEEE. pp. 9297–9307.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bello et al. (2020) Bello, S., Yu, S., Wang, C., Adam, J., Li, J., 2020. deep
    learning on 3d point clouds. Remote Sensing 12, 1729.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Boulch et al. (2018) Boulch, A., Guerry, J., Le Saux, B., Audebert, N., 2018.
    Snapnet: 3d point cloud semantic labeling with 2d deep segmentation networks.
    Computers & Graphics 71, 189–198.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boulch et al. (2017) Boulch, A., Le Saux, B., Audebert, N., 2017. Unstructured
    point cloud semantic labeling using deep segmentation networks. 3dor@ eurographics
    2, 7.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brodeur et al. (2017) Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti,
    L., Strub, F., Rouat, J., Larochelle, H., Courville, A., 2017. Home: A household
    multimodal environment. arXiv preprint arXiv:1711.11017 .'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2016) Cao, Y., Shen, C., Shen, H., 2016. Exploiting depth from single
    monocular images for object detection and semantic segmentation. IEEE Transactions
    on Image Processing 26, 836–846.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2017) Chang, A., Dai, A., Funkhouser, T., Halber, M., Niebner,
    M., Savva, M., Song, S., Zeng, A., Zhang, Y., 2017. Matterport3d: Learning from
    rgb-d data in indoor environments, in: 2017 International Conference on 3D Vision
    (3DV), IEEE. pp. 667–676.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Chen, S., Fang, J., Zhang, Q., Liu, W., Wang, X., 2021.
    Hierarchical aggregation for 3d instance segmentation, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 15467–15476.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2009) Chen, X., Golovinskiy, A., Funkhouser, T., 2009. A benchmark
    for 3d mesh segmentation. ACM Transactions on Graphics 28, 1–12.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2020) Cheng, J., Sun, Y., Meng, M.Q.H., 2020. Robust semantic
    mapping in challenging environments. Robotica 38, 256–270.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2017) Cheng, Y., Cai, R., Li, Z., Zhao, X., Huang, K., 2017.
    Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 3029–3037.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2019) Chiang, H.Y., Lin, Y.L., Liu, Y.C., Hsu, W.H., 2019. A
    unified point-based framework for 3d segmentation, in: Processing of the International
    Conference on 3D Vision, IEEE. pp. 155–163.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chollet (2017) Chollet, F., 2017. Xception: Deep learning with depthwise separable
    convolutions, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 1251–1258.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choy et al. (2019) Choy, C., Gwak, J., Savarese, S., 2019. 4d spatio-temporal
    convnets: Minkowski convolutional neural networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 3075–3084.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Couprie et al. (2013) Couprie, C., Farabet, C., Najman, L., LeCun, Y., 2013.
    Indoor semantic segmentation using depth information. arXiv preprint arXiv:1301.3572
    .
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2017) Dai, A., Chang, A., Savva, M., Halber, M., Funkhouser, T.,
    Nießner, M., 2017. Scannet: Richly-annotated 3d reconstructions of indoor scenes,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 5828–5839.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai and Nießner (2018) Dai, A., Nießner, M., 2018. 3dmv: Joint 3d-multi-view
    prediction for 3d semantic scene segmentation, in: Proceedings of the European
    Conference on Computer Vision, pp. 452–468.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2018) Dai, A., Ritchie, D., Bokeloh, M., Reed, S., Sturm, J., Nießner,
    M., 2018. Scancomplete: Large-scale scene completion and semantic segmentation
    for 3d scans, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 4578–4587.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elich et al. (2019) Elich, C., Engelmann, F., Kontogianni, T., Leibe, B., 2019.
    3d bird’s-eye-view instance segmentation, in: German Conference on Pattern Recognition,
    Springer. pp. 48–61.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Emre Yurdakul and Yemez (2017) Emre Yurdakul, E., Yemez, Y., 2017. Semantic
    segmentation of rgbd videos with recurrent fully convolutional neural networks,
    in: Processings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 367–374.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2020a) Engelmann, F., Bokeloh, M., Fathi, A., Leibe, B.,
    Nießner, M., 2020a. 3d-mpa: Multi-proposal aggregation for 3d semantic instance
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 9031–9040.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2017) Engelmann, F., Kontogianni, T., Hermans, A., Leibe,
    B., 2017. Exploring spatial context for 3d semantic segmentation of point clouds,
    in: Processings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 716–724.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2020b) Engelmann, F., Kontogianni, T., Leibe, B., 2020b.
    Dilated point convolutions: On the receptive field size of point convolutions
    on 3d point clouds, in: 2020 IEEE International Conference on Robotics and Automation
    (ICRA), IEEE. pp. 9463–9469.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Engelmann et al. (2018) Engelmann, F., Kontogianni, T., Schult, J., Leibe,
    B., 2018. Know what your neighbors do: 3d semantic segmentation of point clouds,
    in: Proceedings of the European Conference on Computer Vision, pp. 0–0.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2017) Fan, H., Mei, X., Prokhorov, D., Ling, H., 2017. Rgb-d scene
    labeling with multimodal recurrent neural networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops, pp. 9–17.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2021) Fan, H., Yang, Y., Kankanhalli, M., 2021. Point 4d transformer
    networks for spatio-temporal modeling in point cloud videos, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, pp. 14204–14213.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2020) Feng, M., Zhang, L., Lin, X., Gilani, S.Z., Mian, A., 2020.
    Point attention network for semantic segmentation of 3d point clouds. Pattern
    Recognition , 107446.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fooladgar and Kasaei (2020) Fooladgar, F., Kasaei, S., 2020. A survey on indoor
    rgb-d semantic segmentation: from hand-crafted features to deep convolutional
    neural networks. Multimedia Tools and Applications 79, 4499–4524.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geiger et al. (2012) Geiger, A., Lenz, P., Urtasun, R., 2012. Are we ready
    for autonomous driving? the kitti vision benchmark suite, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE. pp. 3354–3361.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Graham et al. (2018) Graham, B., Engelcke, M., Van Der Maaten, L., 2018. 3d
    semantic segmentation with submanifold sparse convolutional networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9224–9232.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Groh et al. (2018) Groh, F., Wieschollek, P., Lensch, H.P., 2018. Flex-convolution,
    in: Asian Conference on Computer Vision, Springer. pp. 105–122.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guerry et al. (2017) Guerry, J., Boulch, A., Le Saux, B., Moras, J., Plyer,
    A., Filliat, D., 2017. Snapnet-r: Consistent 3d multi-view semantic labeling for
    robotics, in: Processings of the IEEE/CVF International Conference on Computer
    Vision Workshops, pp. 669–678.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo and Chen (2018) Guo, Y., Chen, T., 2018. Semantic segmentation of rgbd images
    based on deep depth regression. Pattern Recognit. Lett. 109, 55–64.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2020) Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.,
    2020. Deep learning for 3d point clouds: A survey. IEEE Transactions on Pattern
    Analysis and Machine Intelligence .'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. (2014) Gupta, S., Girshick, R., Arbeláez, P., Malik, J., 2014.
    Learning rich features from rgb-d images for object detection and segmentation,
    in: Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part VII 13, Springer. pp. 345–360.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hackel et al. (2017) Hackel, T., Savinov, N., Ladicky, L., Wegner, J., Schindler,
    K., Pollefeys, M., 2017. Semantic3d. net: A new large-scale point cloud classification
    benchmark. arXiv preprint arXiv:1704.03847 .'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2020) Han, L., Zheng, T., Xu, L., Fang, L., 2020. Occuseg: Occupancy-aware
    3d instance segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 2940–2949.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hanocka et al. (2019) Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman,
    S., D., C.O., 2019. Meshcnn: a network with an edge. ACM Transactions on Graphics
    38, 1–12.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hazirbas et al. (2016) Hazirbas, C., Ma, L., Domokos, C., Cremers, D., 2016.
    Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture,
    in: Proc. Asian Conf. Compu. Vis., Springer. pp. 213–228.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2017a) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017a. Mask
    r-cnn, in: Processings of the IEEE/CVF International Conference on Computer Vision,
    pp. 2961–2969.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) He, T., Shen, C., Van Den Hengel, A., 2021. Dyco3d: Robust
    instance segmentation of 3d point clouds through dynamic convolution, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pp. 354–363.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022) He, T., Yin, W., Shen, C., van den Hengel, A., 2022. Pointinst3d:
    Segmenting 3d instances by points, in: Proceedings of the European Conference
    on Computer Vision, Springer. pp. 286–302.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2017b) He, Y., Chiu, W.C., Keuper, M., Fritz, M., 2017b. Std2p:
    Rgbd semantic segmentation using spatio-temporal data-driven pooling, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4837–4846.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hermosilla et al. (2018) Hermosilla, P., Ritschel, T., Vázquez, P.P., Vinacua,
    À., Ropinski, T., 2018. Monte carlo convolution for learning on non-uniformly
    sampled point clouds. ACM Transactions on Graphics 37, 1–12.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Höft et al. (2014) Höft, N., Schulz, H., Behnke, S., 2014. Fast semantic segmentation
    of rgb-d scenes with gpu-accelerated deep neural networks, in: Joint German/Austrian
    Conference on Artificial Intelligence, Springer. pp. 80–85.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. (2019) Hou, J., Dai, A., Nießner, M., 2019. 3d-sis: 3d semantic
    instance segmentation of rgb-d scans, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 4421–4430.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2020) Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni,
    N., Markham, A., 2020. Randla-net: Efficient semantic segmentation of large-scale
    point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 11108–11117.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) Hu, W., Zhao, H., Jiang, L., Jia, J., Wong, T.T., 2021. Bidirectional
    projection network for cross dimension scene understanding, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14373–14382.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua et al. (2016) Hua, B., Pham, Q., Nguyen, D., Tran, M., Yu, L., Yeung, S.,
    2016. Scenenn: A scene meshes dataset with annotations, in: Processing of the
    International Conference on 3D Vision, IEEE. pp. 92–101.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hua et al. (2018) Hua, B.S., Tran, M.K., Yeung, S.K., 2018. Pointwise convolutional
    neural networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 984–993.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and You (2016) Huang, J., You, S., 2016. Point cloud labeling using 3d
    convolutional neural network, in: 2016 23rd International Conference on Pattern
    Recognition (ICPR), IEEE. pp. 2670–2675.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2018) Huang, Q., Wang, W., Neumann, U., 2018. Recurrent slice
    networks for 3d segmentation of point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2626–2635.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iandola et al. (2016) Iandola, F., Han, S., Moskewicz, M., Ashraf, K., Dally,
    W., Keutzer, K., 2016. Squeezenet: Alexnet-level accuracy with 50x fewer parameters
    and< 0.5 mb model size. arXiv preprint arXiv:1602.07360 .'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioannidou et al. (2017) Ioannidou, A., Chatzilari, E., Nikolopoulos, S., Kompatsiaris,
    I., 2017. Deep learning advances in computer vision with 3d data: A survey. ACM
    Computing Surveys 50, 1–38.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivaneckỳ (2016) Ivaneckỳ, B.J., 2016. Depth estimation by convolutional neural
    networks. Ph.D. thesis. Master thesis, Brno University of Technology.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jampani et al. (2016) Jampani, V., Kiefel, M., Gehler, P.V., 2016. Learning
    sparse high dimensional filters: Image filtering, dense crfs and bilateral neural
    networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4452–4461.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaritz et al. (2019) Jaritz, M., Gu, J., Su, H., 2019. Multi-view pointnet
    for 3d scene understanding, in: Processings of the IEEE/CVF International Conference
    on Computer Vision Workshops, pp. 0–0.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeong et al. (2018) Jeong, J., Yoon, T.S., Park, J.B., 2018. Multimodal sensor-based
    semantic 3d mapping for a large-scale environment. Expert Systems with Applications
    105, 1–10.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020a) Jiang, H., Yan, F., Cai, J., Zheng, J., Xiao, J., 2020a.
    End-to-end 3d point cloud instance segmentation without detection, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12796–12805.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2017) Jiang, J., Zhang, Z., Huang, Y., Zheng, L., 2017. Incorporating
    depth into both cnn and crf for indoor semantic segmentation, in: Processing of
    the IEEE International Conference on Software Engineering and Service Science,
    IEEE. pp. 525–530.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2020b) Jiang, L., Zhao, H., Shi, S., Liu, S., Fu, C., Jia, J.,
    2020b. Pointgroup: Dual-set point grouping for 3d instance segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4867–4876.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2018) Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., 2018. Pointsift:
    A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint
    arXiv:1807.00652 .'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kalogerakis et al. (2017) Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri,
    S., 2017. 3d shape segmentation with projective convolutional networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3779–3788.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland,
    C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al., 2023.
    Segment anything. arXiv preprint arXiv:2304.02643 .
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klokov and Lempitsky (2017) Klokov, R., Lempitsky, V., 2017. Escape from cells:
    Deep kd-networks for the recognition of 3d point cloud models, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 863–872.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kochanov et al. (2016) Kochanov, D., Ošep, A., Stückler, J., Leibe, B., 2016.
    Scene flow propagation for semantic mapping and object discovery in dynamic street
    scenes, in: Proc. IEEE Int. Conf. Intell. Rob. Syst., IEEE. pp. 1785–1792.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Komarichev et al. (2019) Komarichev, A., Zhong, Z., Hua, J., 2019. A-cnn: Annularly
    convolutional neural networks on point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 7421–7430.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. (2023) Kong, L., Liu, Y., Chen, R., Ma, Y., Zhu, X., Li, Y., Hou,
    Y., Qiao, Y., Liu, Z., 2023. Rethinking range view representation for lidar segmentation.
    arXiv preprint arXiv:2303.05367 .
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lahoud et al. (2019) Lahoud, J., Ghanem, B., Pollefeys, M., Oswald, M., 2019.
    3d instance segmentation via multi-task metric learning, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 9256–9266.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2023) Lai, X., Chen, Y., Lu, F., Liu, J., Jia, J., 2023. Spherical
    transformer for lidar-based 3d recognition, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 17545–17555.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lai et al. (2022) Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S.,
    Qi, X., Jia, J., 2022. Stratified transformer for 3d point cloud segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 8500–8509.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Landrieu and Simonovsky (2018) Landrieu, L., Simonovsky, M., 2018. Large-scale
    point cloud semantic segmentation with superpoint graphs, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4558–4567.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lawin et al. (2017) Lawin, F., Danelljan, M., Tosteberg, P., Bhat, G., Khan,
    F., Felsberg, M., 2017. Deep projective 3d semantic segmentation, in: Computer
    Analysis of Images and Patterns, Springer. pp. 95–107.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le and Duan (2018) Le, T., Duan, Y., 2018. Pointgrid: A deep network for 3d
    shape understanding, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 9204–9214.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lei et al. (2019) Lei, H., Akhtar, N., Mian, A., 2019. Octree guided cnn with
    spherical kernels for 3d point clouds, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9631–9640.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2020) Lei, H., Akhtar, N., Mian, A., 2020. Spherical kernel for
    efficient graph convolution on 3d point clouds. IEEE Transactions on Pattern Analysis
    and Machine Intelligence .
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lei et al. (2023) Lei, H., Akhtar, N., Shah, M., Mian, A., 2023. Mesh convolution
    with continuous filters for 3-d surface parsing. IEEE Transactions on Neural Networks
    and Learning Systems .
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019a) Li, G., Muller, M., Thabet, A., Ghanem, B., 2019a. Deepgcns:
    Can gcns go as deep as cnns?, in: Processings of the IEEE/CVF International Conference
    on Computer Vision, pp. 9267–9276.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018a) Li, J., Chen, B.M., Hee Lee, G., 2018a. So-net: Self-organizing
    network for point cloud analysis, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 9397–9406.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018b) Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., 2018b.
    Pointcnn: Convolution on x-transformed points. Advances in Neural Information
    Processing Systemsms 31, 820–830.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2019b) Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. Tgnet:
    Geometric graph cnn on 3-d point cloud segmentation. IEEE Transactions on Geoscience
    and Remote Sensing 58, 3588–3600.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2016) Li, Z., Gan, Y., Liang, X., Yu, Y., Cheng, H., Lin, L., 2016.
    Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling,
    in: Proceedings of the European Conference on Computer Vision, Springer. pp. 541–557.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. (2022) Lian, Q., Li, P., Chen, X., 2022. Monojsg: Joint semantic
    and geometric cost volume for monocular 3d object detection, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1070–1079.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2021) Liang, Z., Li, Z., Xu, S., Tan, M., Jia, K., 2021. Instance
    segmentation in 3d scenes using semantic superpoint tree networks, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 2783–2792.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2019a) Liang, Z., Yang, M., Deng, L., Wang, C., Wang, B., 2019a.
    Hierarchical depthwise graph convolutional neural network for 3d semantic segmentation
    of point clouds, in: Processing of the IEEE International Conference on Robotics
    and Automation, IEEE. pp. 8152–8158.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2019b) Liang, Z., Yang, M., Wang, C., 2019b. 3d graph embedding
    learning with a structure-aware loss function for point cloud semantic instance
    segmentation. arXiv preprint arXiv:1902.05247 .
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2017) Lin, D., Chen, G., Cohen-Or, D., Heng, P., Huang, H., 2017.
    Cascaded feature network for semantic segmentation of rgb-d images, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 1311–1319.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Furukawa (2019) Liu, C., Furukawa, Y., 2019. Masc: multi-scale affinity
    with sparse convolution for 3d instance segmentation. arXiv preprint arXiv:1902.04478
    .'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2017) Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu,
    J., 2017. 3dcnn-dqn-rnn: A deep reinforcement learning framework for semantic
    parsing of large-scale 3d point clouds, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, pp. 5678–5687.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2015) Liu, F., Shen, C., Lin, G., Reid, I., 2015. Learning depth
    from single monocular images using deep convolutional neural fields. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 38, 2024–2039.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018a) Liu, H., Wu, W., Wang, X., Qian, Y., 2018a. Rgb-d joint modelling
    with scene geometric information for indoor semantic segmentation. Multimed. Tools.
    Appl. 77, 22475–22488.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2018b) Liu, J., Wang, Y., Li, Y., Fu, J., Li, J., Lu, H., 2018b.
    Collaborative deconvolutional neural networks for joint depth estimation and semantic
    segmentation. IEEE Trans. Neural Netw. Learn. Syst. 29, 5655–5666.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019a) Liu, W., Sun, J., Li, W., Hu, T., Wang, P., 2019a. Deep
    learning on point clouds and its application: A survey. Sensors 19, 4188.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2018c) Liu, Y., Yang, S., Li, B., Zhou, W., Xu, J., Li, H., Lu,
    Y., 2018c. Affinity derivation and graph merge for instance segmentation, in:
    Proceedings of the European Conference on Computer Vision, pp. 686–703.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Liu, Z., Tang, H., Lin, Y., Han, S., 2019b. Point-voxel
    cnn for efficient 3d deep learning, in: Advances in Neural Information Processing
    Systemsms, pp. 965–975.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe (2004) Lowe, D.G., 2004. Distinctive image features from scale-invariant
    keypoints. International journal of computer vision 60, 91–110.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2017) Ma, L., Stückler, J., Kerl, C., Cremers, D., 2017. Multi-view
    deep learning for consistent semantic mapping with rgb-d cameras, in: Proc. IEEE
    Int. Conf. Intell. Rob. Syst., IEEE. pp. 598–605.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020) Ma, Y., Guo, Y., Liu, H., Lei, Y., Wen, G., 2020. Global context
    reasoning for semantic segmentation of 3d point clouds, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2931–2940.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maturana and Scherer (2015) Maturana, D., Scherer, S., 2015. Voxnet: A 3d convolutional
    neural network for real-time object recognition, in: Proc. IEEE Int. Conf. Intell.
    Rob. Syst., IEEE. pp. 922–928.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCormac et al. (2017) McCormac, J., Handa, A., Davison, A., Leutenegger, S.,
    2017. Semanticfusion: Dense 3d semantic mapping with convolutional neural networks,
    in: Proceedings of the IEEE International Conference on Robotics and Automation,
    IEEE. pp. 4628–4635.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meng et al. (2019) Meng, H., Gao, L., Lai, Y., Manocha, D., 2019. Vv-net: Voxel
    vae net with group convolutions for point cloud segmentation, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 8500–8508.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meyer et al. (2019) Meyer, G.P., Charland, J., Hegde, D., Laddha, A., Vallespi-Gonzalez,
    C., 2019. Sensor fusion for joint 3d object detection and semantic segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 0–0.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Milioto et al. (2019) Milioto, A., Vizzo, I., Behley, J., Stachniss, C., 2019.
    Rangenet++: Fast and accurate lidar semantic segmentation, in: Proc. IEEE Int.
    Conf. Intell. Rob. Syst., IEEE. pp. 4213–4220.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morton (1966) Morton, G.M., 1966. A computer oriented geodetic data base and
    a new technique in file sequencing .
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mousavian et al. (2016) Mousavian, A., Pirsiavash, H., Košecká, J., 2016. Joint
    semantic segmentation and depth estimation with deep convolutional networks, in:
    Processing of the International Conference on 3D Vision, IEEE. pp. 611–619.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narita et al. (2019) Narita, G., Seno, T., Ishikawa, T., Kaji, Y., 2019. Panopticfusion:
    Online volumetric semantic mapping at the level of stuff and things. arXiv preprint
    arXiv:1903.01177 .'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naseer et al. (2018) Naseer, M., Khan, S., Porikli, F., 2018. Indoor scene
    understanding in 2.5/3d for autonomous agents: A survey. IEEE Access 7, 1859–1887.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ngo et al. (2023) Ngo, T.D., Hua, B.S., Nguyen, K., 2023. Isbnet: a 3d point
    cloud instance segmentation network with instance-aware sampling and box-aware
    dynamic convolution, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 13550–13559.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2022) Park, C., Jeong, Y., Cho, M., Park, J., 2022. Fast point
    transformer, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 16949–16958.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2023) Park, J., Kim, C., Kim, S., Jo, K., 2023. Pcscnet: Fast
    3d semantic segmentation of lidar point cloud for autonomous car using point convolution
    and sparse convolution network. Expert Systems with Applications 212, 118815.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2019a) Pham, Q., Hua, B., Nguyen, T., Yeung, S., 2019a. Real-time
    progressive 3d semantic segmentation for indoor scenes, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, IEEE. pp. 1089–1098.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2019b) Pham, Q.H., Nguyen, T., Hua, B.S., Roig, G., Yeung, S.K.,
    2019b. Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task
    pointwise networks and multi-value conditional random fields, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8827–8836.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017a) Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017a. Pointnet:
    Deep learning on point sets for 3d classification and segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 652–660.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017b) Qi, C.R., Yi, L., Su, H., Guibas, L.J., 2017b. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. Advances in
    Neural Information Processing Systemsms 30, 5099–5108.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2017c) Qi, X., Liao, R., Jia, J., Fidler, S., Urtasun, R., 2017c.
    3d graph neural networks for rgbd semantic segmentation, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 5199–5208.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2022) Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H.A.A.K.,
    Elhoseiny, M., Ghanem, B., 2022. Pointnext: Revisiting pointnet++ with improved
    training and scaling strategies. arXiv preprint arXiv:2206.04670 .'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raj et al. (2015) Raj, A., Maturana, D., Scherer, S., 2015. Multi-scale convolutional
    architecture for semantic segmentation. Robotics Institute, Carnegie Mellon University,
    Tech. Rep. CMU-RITR-15-21 .
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ran et al. (2022) Ran, H., Liu, J., Wang, C., 2022. Surface representation
    for point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 18942–18952.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rethage et al. (2018) Rethage, D., Wald, J., Sturm, J., Navab, N., Tombari,
    F., 2018. Fully-convolutional point networks for large-scale point clouds, in:
    Proceedings of the European Conference on Computer Vision, pp. 596–611.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riegler et al. (2017) Riegler, G., Osman Ulusoy, A., Geiger, A., 2017. Octnet:
    Learning deep 3d representations at high resolutions, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 3577–3586.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riemenschneider et al. (2014) Riemenschneider, H., Bódis-Szomorú, A., Weissenberg,
    J., Van Gool, L., 2014. Learning where to classify in multi-view semantic segmentation,
    in: Proceedings of the European Conference on Computer Vision, Springer. pp. 516–532.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosu et al. (2019) Rosu, R.A., Schütt, P., Quenzel, J., Behnke, S., 2019. Latticenet:
    Fast point cloud segmentation using permutohedral lattices. arXiv preprint arXiv:1912.05905
    .'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roynard et al. (2018) Roynard, X., Deschaud, J., Goulette, F., 2018. Paris-lille-3d:
    A large and high-quality ground-truth urban point cloud dataset for automatic
    segmentation and classification. The International Journal of Robotics Research
    37, 545–557.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2018) Shen, Y., Feng, C., Yang, Y., Tian, D., 2018. Mining point
    cloud local structures by kernel correlation and graph pooling, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4548–4557.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2020) Shi, H., Lin, G., Wang, H., Hung, T.Y., Wang, Z., 2020. Spsequencenet:
    Semantic segmentation network on 4d point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 4574–4583.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2022) Shi, H., Wei, J., Li, R., Liu, F., Lin, G., 2022. Weakly
    supervised segmentation on outdoor 4d point clouds with temporal matching and
    spatial graph propagation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 11840–11849.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silberman and Fergus (2011) Silberman, N., Fergus, R., 2011. Indoor scene segmentation
    using a structured light sensor, in: Processings of the IEEE/CVF International
    Conference on Computer Vision Worksh., IEEE. pp. 601–608.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silberman et al. (2012) Silberman, N., Hoiem, D., Kohli, P., Fergus, R., 2012.
    Indoor segmentation and support inference from rgbd images, in: Proceedings of
    the European Conference on Computer Vision, Springer. pp. 746–760.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonovsky and Komodakis (2017) Simonovsky, M., Komodakis, N., 2017. Dynamic
    edge-conditioned filters in convolutional neural networks on graphs, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3693–3702.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2015) Song, S., Lichtenberg, S.P., Xiao, J., 2015. Sun rgb-d:
    A rgb-d scene understanding benchmark suite, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 567–576.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2017) Song, Y., Chen, X., Li, J., Zhao, Q., 2017. Embedding 3d
    geometric features for rigid object part segmentation, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 580–588.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2018) Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang,
    M.H., Kautz, J., 2018. Splatnet: Sparse lattice networks for point cloud processing,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2530–2539.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Su et al. (2015) Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015.
    Multi-view convolutional neural networks for 3d shape recognition, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 945–953.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2023) Su, Y., Xu, X., Jia, K., 2023. Weakly supervised 3d point cloud
    segmentation via multi-prototype learning. IEEE Transactions on Circuits and Systems
    for Video Technology .
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tatarchenko et al. (2018) Tatarchenko, M., Park, J., Koltun, V., Zhou, Q.Y.,
    2018. Tangent convolutions for dense prediction in 3d, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3887–3896.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tchapmi et al. (2017) Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese,
    S., 2017. Segcloud: Semantic segmentation of 3d point clouds, in: Processing of
    the International Conference on 3D Vision, IEEE. pp. 537–547.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thomas et al. (2019) Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B.,
    Goulette, F., Guibas, L.J., 2019. Kpconv: Flexible and deformable convolution
    for point clouds, in: Processings of the IEEE/CVF International Conference on
    Computer Vision, pp. 6411–6420.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Valipour et al. (2017) Valipour, S., Siam, M., Jagersand, M., Ray, N., 2017.
    Recurrent fully convolutional networks for video segmentation, in: Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, IEEE. pp.
    29–36.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Verma et al. (2018) Verma, N., Boyer, E., Verbeek, J., 2018. Feastnet: Feature-steered
    graph convolutions for 3d shape analysis, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 2598–2606.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vu et al. (2022) Vu, T., Kim, K., Luu, T.M., Nguyen, T., Yoo, C.D., 2022. Softgroup
    for 3d instance segmentation on point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2708–2717.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018a) Wang, C., Samari, B., Siddiqi, K., 2018a. Local spectral
    graph convolution for point set feature learning, in: Proceedings of the European
    Conference on Computer Vision, pp. 52–66.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Wang, J., Li, X., Sullivan, A., Abbott, L., Chen, S., 2022.
    Pointmotionnet: Point-wise motion learning for large-scale lidar point clouds
    sequences, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4419–4428.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2016) Wang, J., Wang, Z., Tao, D., See, S., Wang, G., 2016. Learning
    common and specific features for rgb-d semantic segmentation with deconvolutional
    networks, in: Proceedings of the European Conference on Computer Vision, Springer.
    pp. 664–679.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) Wang, P., Gan, Y., Shui, P., Yu, F., Zhang, Y., Chen, S.,
    Sun, Z., 2018b. 3d shape segmentation via shape fully convolutional networks.
    Computers & Graphics 70, 128–139.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2015) Wang, P., Shen, X., Lin, Z., Cohen, S., Price, B., Yuille,
    A., 2015. Towards unified depth and semantic prediction from a single image, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2800–2809.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2017) Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., 2017.
    O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM Transactions
    on Graphics 36, 1–11.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018c) Wang, S., Suo, S., Ma, W.C., Pokrovsky, A., Urtasun, R.,
    2018c. Deep parametric continuous convolutional neural networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2589–2597.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Neumann (2018) Wang, W., Neumann, U., 2018. Depth-aware cnn for rgb-d
    segmentation, in: Proceedings of the European Conference on Computer Vision, pp.
    135–150.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018d) Wang, W., Yu, R., Huang, Q., Neumann, U., 2018d. Sgpn:
    Similarity group proposal network for 3d point cloud instance segmentation, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2569–2578.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019a) Wang, X., Liu, S., Shen, X., Shen, C., Jia, J., 2019a.
    Associatively segmenting instances and semantics in point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4096–4105.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2012) Wang, Y., Asafi, S., Van Kaick, O., Zhang, H., Cohen-Or,
    D., Chen, B., 2012. Active co-analysis of a set of shapes. ACM Transactions on
    Graphics 31, 1–10.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2018e) Wang, Y., Shi, T., Yun, P., Tai, L., Liu, M., 2018e. Pointseg:
    Real-time semantic segmentation based on 3d lidar point cloud. arXiv preprint
    arXiv:1807.06288 .'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019b) Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M.,
    Solomon, J.M., 2019b. Dynamic graph cnn for learning on point clouds. ACM Transactions
    on Graphics 38, 1–12.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Lu (2019) Wang, Z., Lu, F., 2019. Voxsegnet: Volumetric cnns for semantic
    part segmentation of 3d shapes. IEEE Transactions on Visualization and Computer
    Graphics .'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei (2008) Wei, L.Y., 2008. Parallel poisson disk sampling. ACM Transactions
    on Graphics 27, 1–9.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Wei, Y., Liu, H., Xie, T., Ke, Q., Guo, Y., 2022. Spatial-temporal
    transformer for 3d point cloud sequences, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, pp. 1171–1180.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018a) Wu, B., Wan, A., Yue, X., Keutzer, K., 2018a. Squeezeseg:
    Convolutional neural nets with recurrent crf for real-time road-object segmentation
    from 3d lidar point cloud, in: Proceedings of the IEEE International Conference
    on Robotics and Automation, IEEE. pp. 1887–1893.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019a) Wu, B., Zhou, X., Zhao, S., Yue, X., Keutzer, K., 2019a.
    Squeezesegv2: Improved model structure and unsupervised domain adaptation for
    road-object segmentation from a lidar point cloud, in: Proceedings of the IEEE
    International Conference on Robotics and Automation, IEEE. pp. 4376–4382.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019b) Wu, W., Qi, Z., Fuxin, L., 2019b. Pointconv: Deep convolutional
    networks on 3d point clouds, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 9621–9630.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022a) Wu, X., Lao, Y., Jiang, L., Liu, X., Zhao, H., 2022a. Point
    transformer v2: Grouped vector attention and partition-based pooling. Advances
    in Neural Information Processing Systemsms 35, 33330–33342.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2022b) Wu, Y., Shi, M., Du, S., Lu, H., Cao, Z., Zhong, W., 2022b.
    3d instances as 1d kernels, in: Proceedings of the European Conference on Computer
    Vision, Springer. pp. 235–252.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2018b) Wu, Y., Wu, Y., Gkioxari, G., Tian, Y., 2018b. Building generalizable
    agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209
    .
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2015) Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X.,
    Xiao, J., 2015. 3d shapenets: A deep representation for volumetric shapes, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 1912–1920.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022c) Wu, Z., Zhou, Z., Allibert, G., Stolz, C., Demonceaux, C.,
    Ma, C., 2022c. Transformer fusion for indoor rgb-d semantic segmentation. Available
    at SSRN 4251286 .
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2023) Xia, Z., Liu, Y., Li, X., Zhu, X., Ma, Y., Li, Y., Hou, Y.,
    Qiao, Y., 2023. Scpnet: Semantic scene completion on point cloud, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17642–17651.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiang and Fox (2017) Xiang, Y., Fox, D., 2017. Da-rnn: Semantic mapping with
    data associated recurrent neural networks. arXiv preprint arXiv:1703.03098 .'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2023) Xiao, A., Huang, J., Guan, D., Zhang, X., Lu, S., Shao,
    L., 2023. Unsupervised point cloud representation learning with deep neural networks:
    A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence .'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2020a) Xie, Y., Jiaojiao, T., Zhu, X., 2020a. Linking points with
    labels in 3d: A review of point cloud semantic segmentation. IEEE Geoscience and
    Remote Sensing Magazine .'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020b) Xie, Z., Chen, J., Peng, B., 2020b. Point clouds learning
    with attention-based graph convolution networks. Neurocomputing .
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2020) Xu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K.,
    Tomizuka, M., 2020. Squeezesegv3: Spatially-adaptive convolution for efficient
    point-cloud segmentation. arXiv preprint arXiv:2004.01803 .'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2017) Xu, H., Dong, M., Zhong, Z., 2017. Directionally convolutional
    networks for 3d shape segmentation, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, pp. 2698–2707.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Xu, M., Ding, R., Zhao, H., Qi, X., 2021. Paconv: Position
    adaptive convolution with dynamic kernel assembling on point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3173–3182.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., 2018. Spidercnn:
    Deep learning on point sets with parameterized convolutional filters, in: Proceedings
    of the European Conference on Computer Vision, pp. 87–102.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2020) Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., 2020. Pointasnl:
    Robust point clouds processing using nonlocal neural networks with adaptive sampling,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    pp. 5589–5598.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2019) Yang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham,
    A., Trigoni, N., 2019. Learning object bounding boxes for 3d instance segmentation
    on point clouds. Advances in Neural Information Processing Systemsms 32, 6740–6749.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2017) Yang, S., Huang, Y., Scherer, S., 2017. Semantic 3d occupancy
    mapping through efficient high order crfs, in: Proc. IEEE Int. Conf. Intell. Rob.
    Syst., IEEE. pp. 590–597.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Yang, Y., Xu, Y., Zhang, C., Xu, Z., Huang, J., 2022. Hierarchical
    vision transformer with channel attention for rgb-d image segmentation, in: Proceedings
    of the 4th International Symposium on Signal Processing Systems, pp. 68–73.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2018) Ye, X., Li, J., Huang, H., Du, L., Zhang, X., 2018. 3d recurrent
    neural networks with context fusion for point cloud semantic segmentation, in:
    Proceedings of the European Conference on Computer Vision, pp. 403–417.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2016) Yi, L., Kim, V., Ceylan, D., Shen, I., Yan, M., Su, H., Lu,
    C., Huang, Q., Sheffer, A., Guibas, L., 2016. A scalable active framework for
    region annotation in 3d shape collections. ACM Transactions on Graphics 35, 1–12.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2017) Yi, L., Su, H., Guo, X., Guibas, L.J., 2017. Syncspeccnn:
    Synchronized spectral cnn for 3d shape segmentation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2282–2290.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2019) Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., 2019.
    Gspn: Generative shape proposal network for 3d instance segmentation in point
    cloud, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 3947–3956.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying and Chuah (2022) Ying, X., Chuah, M.C., 2022. Uctnet: Uncertainty-aware
    cross-modal transformer network for indoor rgb-d semantic segmentation, in: Proceedings
    of the European Conference on Computer Vision, Springer. pp. 20–37.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2019) Yu, F., Liu, K., Zhang, Y., Zhu, C., Xu, K., 2019. Partnet:
    A recursive part decomposition network for fine-grained and hierarchical shape
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 9491–9500.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yuan et al. (2021) Yuan, X., Shi, J., Gu, L., 2021. A review of deep learning
    methods for semantic segmentation of remote sensing imagery. Expert Systems with
    Applications 169, 114417.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yue et al. (2022) Yue, C., Wang, Y., Tang, X., Chen, Q., 2022. Drgcnn: Dynamic
    region graph convolutional neural network for point clouds. Expert Systems with
    Applications 205, 117663.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng and Gevers (2018) Zeng, W., Gevers, T., 2018. 3dcontextnet: Kd tree guided
    hierarchical learning of point clouds using local and global contextual cues,
    in: Proceedings of the European Conference on Computer Vision, pp. 0–0.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Zhang, C., Wan, H., Shen, X., Wu, Z., 2022. Patchformer:
    An efficient point transformer with patch attention, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 11799–11808.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Zhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong,
    B., Foroosh, H., 2020. Polarnet: An improved grid representation for online lidar
    point clouds semantic segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9601–9610.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Zhang, Z., Han, X., Dong, B., Li, T., Yin, B., Yang, X.,
    2023. Point cloud scene completion with joint color and semantic estimation from
    single rgb-d image. IEEE Transactions on Pattern Analysis and Machine Intelligence
    .
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2018) Zhao, C., Sun, L., Purkait, P., Duckett, T., Stolkin, R.,
    2018. Dense rgb-d semantic mapping with pixel-voxel neural network. Sensors 18,
    3099.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019a) Zhao, H., Jiang, L., Fu, C.W., Jia, J., 2019a. Pointweb:
    Enhancing local neighborhood features for point cloud processing, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5565–5573.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V., 2021.
    Point transformer, in: Proceedings of the IEEE/CVF International Conference on
    Computer Vision, pp. 16259–16268.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2019b) Zhao, Y., Birdal, T., Deng, H., Tombari, F., 2019b. 3d
    point capsule networks, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1009–1018.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )Yong He received the M.S. degree from China University of Mining and Technology,
    Xuzhou, Jiangsu, China, in 2018\. He is currently pursing the Ph.D. degree with
    Hunan University, Changsha, China, and he is also currently a Visiting Scholar
    with University of Western Australia, Perth, Australia. His research interests
    include computer vision, point clouds analysis, and deep learning.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: )Hongshan Yu received the B.S., M.S. and Ph.D. degrees of Control Science and
    Technology from electrical and information engineering of Hunan University, Changsha,
    China, in 2001, 2004 and 2007 respectively. From 2011 to 2012, he worked as a
    postdoctoral researcher in Laboratory for Computational Neuroscience of University
    of Pittsburgh, USA. He is currently a professor of Hunan University and associate
    dean of National Engineering Laboratory for Robot Visual Perception and Control.
    His research interests include autonomous mobile robot and machine vision.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: )Xiaoyan Liu received her Ph.D. degree of Process and System Engineering in
    2005 from Otto-von-Guericke University Magdeburg, Germany. She is currently a
    professor of Hunan University. Her research interests include machine vision and
    pattern recognition.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: )Zhengeng Yang received the B.S. and M.S. degrees from Central South University,
    Changsha, China, in 2009 and 2012, respectively. He received the Phd degree from
    Hunan University, Changsha, China, in 2020\. He is currently a post-doctor researcher
    at Hunan University, Changsha. He was a Visiting Scholar with the University of
    Pittsburgh, Pittsburgh, PA during 2018 -2020\. His research interests include
    computer vision, image analysis, and machine learning.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: )Wei Sun received the M.S. and Ph.D. degrees of Control Science and Technology
    from the Hunan University, Changsha, China, in 1999 and 2002, respectively. He
    is currently a Professor at Hunan University. His research interests include artificial
    intelligence, robot control, complex mechanical and electrical control systems,
    and automotive electronics.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: )Ajmal Mian is a Professor of computer science with The University of Western
    Australia. His research interests include 3D computer vision, machine learning,
    point cloud analysis, human action recognition, and video description. He is Fellow
    of the International Association for Pattern Recognition and has received several
    awards including the HBF Mid-Career Scientist of the Year Award, the West Australian
    Early Career Scientist of the Year Award, the Aspire Professional Development
    Award, the Vice-Chancellors Mid-Career Research Award, the Outstanding Young Investigator
    Award, the IAPR Best Scientific Paper Award, the EH Thompson Award, and excellence
    in Research Supervision Award. He has received three prestigious fellowships and
    several major research grants from the Australian Research Council, the National
    Health and Medical Research Council of Australia and the US Dept of Defense DARPA
    with a total funding of over $40 Million. He serves as a Senior Editor for the
    IEEE Transactions on Neural Networks and Learning Systems, and Associate Editor
    for the IEEE Transactions on Image Processing, and the Pattern Recognition Journal.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
