- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:56:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.05423] Deep Learning Based 3D Segmentation: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.05423] 基于深度学习的3D分割：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05423](https://ar5iv.labs.arxiv.org/html/2103.05423)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.05423](https://ar5iv.labs.arxiv.org/html/2103.05423)
- en: '[type=editor, auid=000, bioid=1, orcid= 0000-0003-2916-3068]'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[类型=编辑，auid=000，bioid=1，orcid=0000-0003-2916-3068]'
- en: '[type=editor,auid=000,bioid=1, orcid = 0000-0003-1973-6766]'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '[类型=编辑，auid=000，bioid=1，orcid=0000-0003-1973-6766]'
- en: '[type=editor,auid=000,bioid=1,] [type=editor,auid=000,bioid=1,] [type=editor,auid=000,bioid=1,]
    [type=editor,auid=000,bioid=1,]'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[类型=编辑, auid=000, bioid=1,] [类型=编辑, auid=000, bioid=1,] [类型=编辑, auid=000, bioid=1,]
    [类型=编辑, auid=000, bioid=1,]'
- en: 1]organization=Hunan University, addressline=Lushan South Rd., Yuelu Dist.,
    city=Changsha, postcode=410082, state=Hunan, country=China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 1] 机构=湖南大学，地址=岳麓区麓山南路，城市=长沙，邮政编码=410082，省份=湖南，国家=中国
- en: 2]organization=University of Western Australia, addressline=35 Stirling Hwy,
    city=Perth, postcode=6009, state=WA, country=Australia
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2] 机构=西澳大利亚大学，地址=35 Stirling Hwy，城市=珀斯，邮政编码=6009，省份=WA，国家=澳大利亚
- en: 'Deep Learning Based 3D Segmentation: A Survey'
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的3D分割：综述
- en: Yong He h.yong@hnu.edu.cn    Hongshan Yu yuhongshancn@hotmail.com    Xiaoyan
    Liu xiaoyan.liu@hnu.edu.cn    Zhengeng Yang yzg050215@163.com    Wei Sun david-sun@126.com
       Ajaml Mian ajmal.mian@uwa.edu.au [ [
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Yong He h.yong@hnu.edu.cn    Hongshan Yu yuhongshancn@hotmail.com    Xiaoyan
    Liu xiaoyan.liu@hnu.edu.cn    Zhengeng Yang yzg050215@163.com    Wei Sun david-sun@126.com
       Ajaml Mian ajmal.mian@uwa.edu.au [ [
- en: A B S T R A C T
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 3D segmentation is a fundamental and challenging problem in computer vision
    with applications in autonomous driving, robotics, augmented reality and medical
    image analysis. It has received significant attention from the computer vision,
    graphics and machine learning communities. Conventional methods for 3D segmentation,
    based on hand-crafted features and machine learning classifiers, lack generalization
    ability. Driven by their success in 2D computer vision, deep learning techniques
    have recently become the tool of choice for 3D segmentation tasks. This has led
    to an influx of a large number of methods in the literature that have been evaluated
    on different benchmark datasets. Whereas survey papers on RGB-D and point cloud
    segmentation exist, there is a lack of an in-depth and recent survey that covers
    all 3D data modalities and application domains. This paper fills the gap and provides
    a comprehensive survey of the recent progress made in deep learning based 3D segmentation.
    It covers over 180 works, analyzes their strengths and limitations and discusses
    their competitive results on benchmark datasets. The survey provides a summary
    of the most commonly used pipelines and finally highlights promising research
    directions for the future.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 3D分割是计算机视觉中的一个基础而具有挑战性的问题，应用于自动驾驶、机器人、增强现实和医学图像分析。它已经引起了计算机视觉、图形学和机器学习社区的广泛关注。基于手工特征和机器学习分类器的传统3D分割方法缺乏泛化能力。受益于在2D计算机视觉中的成功，深度学习技术近年来已成为3D分割任务的首选工具。这导致了大量文献中提出了许多方法，并在不同的基准数据集上进行了评估。尽管RGB-D和点云分割的综述文章已有存在，但缺乏对所有3D数据模态和应用领域进行深入和最新综述的文献。本文填补了这一空白，提供了基于深度学习的3D分割的最新进展的全面综述。它涵盖了超过180篇工作，分析了它们的优缺点，并讨论了它们在基准数据集上的竞争性结果。该综述总结了最常用的处理流程，并最终突出显示了未来有前景的研究方向。
- en: 'keywords:'
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 3D data\sep3D semantic segmentation\sep3D instance segmentation\sep3D part segmentation\sep3D
    video segmentation\sep3D semantic map\sepDeep learning\sep
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 3D数据\sep3D语义分割\sep3D实例分割\sep3D部件分割\sep3D视频分割\sep3D语义地图\sep深度学习\sep
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Segmentation of 3D scenes is a fundamental and challenging problem in computer
    vision as well as computer graphics. The objective of 3D segmentation is to build
    computational techniques that predict the fine-grained labels of objects in a
    3D scene for a wide range of applications such as autonomous driving, mobile robots,
    industrial control, augmented reality and medical image analysis. As illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey"), 3D segmentation can be divided into three types: semantic, instance
    and part segmentation. Semantic segmentation aims to predict object class labels
    such as table and chair. Instance segmentation additionally distinguishes between
    different instances of the same class labels e.g. table one/two and chair one/two.
    Part segmentation aims to decompose instances further into their different components
    such as armrests, legs and backrest of the same chair.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '3D 场景的分割是计算机视觉和计算机图形学中的一个基础且具有挑战性的问题。3D 分割的目标是建立计算技术，以预测 3D 场景中对象的细粒度标签，应用范围广泛，如自动驾驶、移动机器人、工业控制、增强现实和医学图像分析。如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation: A
    Survey") 所示，3D 分割可以分为三种类型：语义分割、实例分割和部件分割。语义分割旨在预测对象类别标签，如桌子和椅子。实例分割额外区分相同类别标签的不同实例，例如桌子一/二和椅子一/二。部件分割旨在将实例进一步分解为其不同组件，例如同一椅子的扶手、腿和靠背。'
- en: Compared to conventional single view 2D segmentation, 3D segmentation gives
    a more comprehensive understanding of a scene, since 3D data (e.g. RGB-D, point
    cloud, voxel, mesh, 3D video) contain richer geometric, shape, and scale information
    with less background noise. Moreover, the representation of 3D data, for example
    in the form of projected images, has more semantic information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的单视图 2D 分割相比，3D 分割提供了对场景的更全面理解，因为 3D 数据（例如 RGB-D、点云、体素、网格、3D 视频）包含更丰富的几何、形状和尺度信息，并且背景噪声较少。此外，3D
    数据的表示，例如以投影图像的形式，包含更多的语义信息。
- en: '![Refer to caption](img/5adef999a7403ba147c6589c55015740.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5adef999a7403ba147c6589c55015740.png)'
- en: 'Figure 1: The main five types of 3D data: (a) RGB-D image, (b) projected images,
    (c) voxels, (d) mesh, and (d) point. Types of 3D segmentation: (f) 3D semantic
    segmentation, (g) 3D instance segmentation, and (h) 3D part segmentation.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：主要的五种 3D 数据类型：(a) RGB-D 图像，(b) 投影图像，(c) 体素，(d) 网格，以及 (e) 点。3D 分割的类型：(f)
    3D 语义分割，(g) 3D 实例分割，以及 (h) 3D 部件分割。
- en: '![Refer to caption](img/938b87a3bbb92dfc3903f056d414a72b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/938b87a3bbb92dfc3903f056d414a72b.png)'
- en: 'Figure 2: Complete overview of the survey paper.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：调查论文的完整概述。
- en: Recently, deep learning techniques have dominated many research areas including
    computer vision and natural language processing. Motivated by its success in learning
    powerful features, deep learning for 3D segmentation has also attracted a growing
    interest from the research community over the past decade. However, 3D deep learning
    methods still face many unsolved challenges. For example, irregularity of point
    clouds makes it difficult to exploit local features and converting them to high-resolution
    voxels comes with a huge computational burden.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习技术在包括计算机视觉和自然语言处理在内的许多研究领域中占据了主导地位。受到其在学习强大特征方面成功的激励，过去十年间，3D 分割的深度学习也引起了研究界日益增长的兴趣。然而，3D
    深度学习方法仍面临许多未解决的挑战。例如，点云的不规则性使得利用局部特征变得困难，并且将其转换为高分辨率体素会带来巨大的计算负担。
- en: 'This paper provides a comprehensive survey of recent progress in deep learning
    methods for 3D segmentation. It focuses on analyzing commonly used building blocks,
    convolution kernels and complete architectures pointing out the pros and cons
    in each case. The survey covers over 180 representative papers published in the
    last five years. Although some notable 3D segmentation surveys have been released
    including RGB-D semantic segmentation Fooladgar and Kasaei ([2020](#bib.bib30)),
    remote sensing imagery segmentation Yuan et al. ([2021](#bib.bib186)), point clouds
    segmentation Xie et al. ([2020a](#bib.bib170)), Guo et al. ([2020](#bib.bib36)),
    Liu et al. ([2019a](#bib.bib95)), Bello et al. ([2020](#bib.bib4)), Naseer et al.
    ([2018](#bib.bib109)), Ioannidou et al. ([2017](#bib.bib56)), these surveys do
    not comprehensively cover all 3D data types and typical application domains. Most
    importantly, these surveys do not focus on 3D segmentation but give a general
    survey of deep learning from point clouds Guo et al. ([2020](#bib.bib36)), Liu
    et al. ([2019a](#bib.bib95)), Bello et al. ([2020](#bib.bib4)), Naseer et al.
    ([2018](#bib.bib109)), Ioannidou et al. ([2017](#bib.bib56)). Given the importance
    of the three segmentation tasks, this paper focuses exclusively on deep learning
    techniques for 3D segmentation. The contributions of this paper are summarized
    as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了有关3D分割的深度学习方法的最新进展的全面调查。它专注于分析常用的构建模块、卷积核和完整架构，指出每种情况的优缺点。该调查涵盖了过去五年中发表的180多篇具有代表性的论文。尽管已经发布了一些显著的3D分割调查，包括RGB-D语义分割Fooladgar和Kasaei（[2020](#bib.bib30)）、遥感图像分割Yuan等（[2021](#bib.bib186)）、点云分割Xie等（[2020a](#bib.bib170)）、Guo等（[2020](#bib.bib36)）、Liu等（[2019a](#bib.bib95)）、Bello等（[2020](#bib.bib4)）、Naseer等（[2018](#bib.bib109)）、Ioannidou等（[2017](#bib.bib56)），这些调查并未全面涵盖所有3D数据类型和典型应用领域。最重要的是，这些调查并未专注于3D分割，而是对从点云Guo等（[2020](#bib.bib36)）、Liu等（[2019a](#bib.bib95)）、Bello等（[2020](#bib.bib4)）、Naseer等（[2018](#bib.bib109)）、Ioannidou等（[2017](#bib.bib56)）等给出的深度学习进行了总体调查。鉴于这三个分割任务的重要性，本文专注于3D分割的深度学习技术。本文的贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the first survey paper to comprehensively
    cover deep learning methods on 3D segmentation covering all 3D data representations,
    including RGB-D, projected images, voxels, point clouds, meshes, and 3D videos.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一篇全面涵盖3D分割中深度学习方法的调查论文，涵盖了所有3D数据表示，包括RGB-D、投影图像、体素、点云、网格和3D视频。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey provides an in-depth analysis of the relative advantages and disadvantages
    of different types of 3D data segmentation methods.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查对不同类型的3D数据分割方法的相对优缺点进行了深入分析。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unlike existing reviews, this survey papers focuses on deep learning methods
    designed specifically for 3D segmentation and also discusses typical segmentation
    pipelines as well as application domains.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与现有的评述不同，本调查论文专注于专门为3D分割设计的深度学习方法，并讨论了典型的分割流程以及应用领域。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Finally, this survey provides comprehensive comparisons of existing methods
    on several public benchmark 3D datasets, draw interesting conclusions and identify
    promising future research directions.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，这项调查对多个公共基准3D数据集上的现有方法进行了全面比较，得出了有趣的结论，并确定了有前景的未来研究方向。
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey") shows a snapshot of how this survey is organized. Section [2](#S2 "2
    Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation: A Survey")
    introduces some terminology and background concepts, including popular 3D datasets
    and evaluation metrics for 3D segmentation. Section [3](#S3 "3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey") reviews methods for 3D semantic
    segmentation whereas Section [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey") reviews methods for 3D instance segmentation.
    Section [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") provides a survey of existing methods for 3D part segmentation. Section
    [6](#S6 "6 Applications of 3D Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") reviews the 3D segmentation methods used in some common application
    areas including 3D video segmentation and 3D semantic map. Section [7](#S7 "7
    Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey") presents
    performance comparison between 3D segmentation methods on several popular datasets,
    and gives corresponding data analysis. Finally, Section [8](#S8 "8 Discussion
    and Conclusion ‣ Deep Learning Based 3D Segmentation: A Survey") identifies promising
    future research directions and concludes the paper.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning Based 3D Segmentation:
    A Survey") 展示了本调查的组织结构。第 [2](#S2 "2 Terminology and Background Concept ‣ Deep
    Learning Based 3D Segmentation: A Survey") 节介绍了一些术语和背景概念，包括流行的3D数据集和3D分割的评估指标。第
    [3](#S3 "3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation: A Survey")
    节回顾了3D语义分割的方法，而第 [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey") 节回顾了3D实例分割的方法。第 [5](#S5 "5 3D Part Segmentation ‣ Deep
    Learning Based 3D Segmentation: A Survey") 节提供了现有3D部件分割方法的综述。第 [6](#S6 "6 Applications
    of 3D Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey") 节回顾了在一些常见应用领域中使用的3D分割方法，包括3D视频分割和3D语义地图。第
    [7](#S7 "7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey")
    节展示了在几个受欢迎的数据集上对3D分割方法的性能比较，并给出了相应的数据分析。最后，第 [8](#S8 "8 Discussion and Conclusion
    ‣ Deep Learning Based 3D Segmentation: A Survey") 节确定了有前景的未来研究方向并总结了论文内容。'
- en: '![Refer to caption](img/87380b46ede615306a8ddc74295a6939.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87380b46ede615306a8ddc74295a6939.png)'
- en: 'Figure 3: Annotated examples from (a) S3DIS, (b) Semantic3D, (c) SemanticKITTI
    for 3D semantic segmentation, (d) ScanNet for 3D instance segmentation, and (e)
    ShapeNet for 3D part segmentation. See Table [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation
    Dataset ‣ 2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation:
    A Survey") for a summary of these datasets.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：来自(a) S3DIS、(b) Semantic3D、(c) SemanticKITTI的3D语义分割标注示例，(d) ScanNet的3D实例分割，以及(e)
    ShapeNet的3D部件分割。有关这些数据集的总结，请参见表格 [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation Dataset
    ‣ 2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation:
    A Survey")。'
- en: 2 Terminology and Background Concept
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 术语与背景概念
- en: This section introduces some terminologies and background concepts, including
    3D data representation, popular 3D segmentation datasets and evaluation metrics
    to help the reader easily navigate through the field of 3D segmentation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一些术语和背景概念，包括3D数据表示、流行的3D分割数据集和评估指标，以帮助读者轻松导航3D分割领域。
- en: 2.1 3D Segmentation Dataset
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 3D分割数据集
- en: 'Datasets are critical to train and test 3D segmentation algorithms using deep
    learning. However, it is cumbersome and expensive to privately gather and annotate
    datasets as it needs domain expertise, high quality sensors and processing equipment.
    Thus, building on public datasets is an ideal way to reduce the cost. Following
    this way has another advantage for the community that it provides a fair comparison
    between algorithms. Table [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation Dataset ‣
    2 Terminology and Background Concept ‣ Deep Learning Based 3D Segmentation: A
    Survey") summarizes some of the most popular and typical datasets with respect
    to the sensor type, data size and format, scene class and annotation method.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集对于使用深度学习训练和测试3D分割算法至关重要。然而，私下收集和标注数据集既繁琐又昂贵，因为这需要领域专业知识、高质量传感器和处理设备。因此，基于公共数据集构建是降低成本的理想方式。遵循这种方式还有一个社区层面的好处，那就是提供了算法之间的公平比较。表格
    [1](#S2.T1 "Table 1 ‣ 2.1 3D Segmentation Dataset ‣ 2 Terminology and Background
    Concept ‣ Deep Learning Based 3D Segmentation: A Survey") 总结了一些最受欢迎和典型的数据集，涵盖了传感器类型、数据大小和格式、场景类别以及标注方法。'
- en: 'These datasets are acquired for *3D semantic segmentation* by different type
    of sensors, including RGB-D cameras Silberman and Fergus ([2011](#bib.bib129)),
    Silberman et al. ([2012](#bib.bib130)), Song et al. ([2015](#bib.bib132)), Hua
    et al. ([2016](#bib.bib51)), Dai et al. ([2017](#bib.bib18)), mobile laser scanner
    Roynard et al. ([2018](#bib.bib125)), Behley et al. ([2019](#bib.bib3)), static
    terrestrial scanner Hackel et al. ([2017](#bib.bib38)) and unreal engine Brodeur
    et al. ([2017](#bib.bib7)), Wu et al. ([2018b](#bib.bib164)) and other 3D scanners
    Armeni et al. ([2016](#bib.bib2)), Chang et al. ([2017](#bib.bib9)). Among these,
    the ones obtained from unreal engine are synthetic datasets Brodeur et al. ([2017](#bib.bib7))
    Wu et al. ([2018b](#bib.bib164)) that do not require expensive equipment or annotation
    time. These are also rich in categories and quantities of objects. Synthetic datasets
    have complete 360 degree 3D objects with no occlusion effects or noise compared
    to the real-world datasets which are noisy and contain occlusions Silberman and
    Fergus ([2011](#bib.bib129)), Silberman et al. ([2012](#bib.bib130)), Song et al.
    ([2015](#bib.bib132)), Hua et al. ([2016](#bib.bib51)), Dai et al. ([2017](#bib.bib18)),
    Roynard et al. ([2018](#bib.bib125)), Behley et al. ([2019](#bib.bib3)), Armeni
    et al. ([2016](#bib.bib2)), Hackel et al. ([2017](#bib.bib38)), Chang et al. ([2017](#bib.bib9)).
    For *3D instance segmentation*, there are limited 3D datasets, such as ScanNet
    Dai et al. ([2017](#bib.bib18)) and S3DIS Armeni et al. ([2016](#bib.bib2)). These
    two datasets contain scans of real-world indoor scenes obtained by RGB-D cameras
    or Matterport separately. For *3D part segmentation*, the Princeton Segmentation
    Benchmark (PSB) Chen et al. ([2009](#bib.bib11)), COSEG Wang et al. ([2012](#bib.bib153))
    and ShapeNet Yi et al. ([2016](#bib.bib181)) are three of the most popular datasets.
    Below, we introduce five famous segmentation datasets in detail, including S3DIS
    Armeni et al. ([2016](#bib.bib2)), ScanNet Dai et al. ([2017](#bib.bib18)), Semantic3D
    Hackel et al. ([2017](#bib.bib38)), SemanticKITTI Chang et al. ([2017](#bib.bib9))
    and ShapeNet Yi et al. ([2016](#bib.bib181)). Some examples with annotation from
    these datasets are shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Deep
    Learning Based 3D Segmentation: A Survey").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集用于*3D语义分割*，由不同类型的传感器获取，包括RGB-D摄像头 Silberman 和 Fergus ([2011](#bib.bib129))、Silberman
    等人 ([2012](#bib.bib130))、Song 等人 ([2015](#bib.bib132))、Hua 等人 ([2016](#bib.bib51))、Dai
    等人 ([2017](#bib.bib18))、移动激光扫描仪 Roynard 等人 ([2018](#bib.bib125))、Behley 等人 ([2019](#bib.bib3))、静态地面扫描仪
    Hackel 等人 ([2017](#bib.bib38)) 和虚幻引擎 Brodeur 等人 ([2017](#bib.bib7))、Wu 等人 ([2018b](#bib.bib164))
    及其他 3D 扫描仪 Armeni 等人 ([2016](#bib.bib2))、Chang 等人 ([2017](#bib.bib9))。其中，通过虚幻引擎获得的数据集是合成数据集
    Brodeur 等人 ([2017](#bib.bib7)) 和 Wu 等人 ([2018b](#bib.bib164))，这些数据集无需昂贵的设备或标注时间，且在对象类别和数量上也很丰富。与真实世界数据集相比，合成数据集具有完整的360度3D对象，没有遮挡效应或噪声，而真实数据集则通常有噪声和遮挡效应
    Silberman 和 Fergus ([2011](#bib.bib129))、Silberman 等人 ([2012](#bib.bib130))、Song
    等人 ([2015](#bib.bib132))、Hua 等人 ([2016](#bib.bib51))、Dai 等人 ([2017](#bib.bib18))、Roynard
    等人 ([2018](#bib.bib125))、Behley 等人 ([2019](#bib.bib3))、Armeni 等人 ([2016](#bib.bib2))、Hackel
    等人 ([2017](#bib.bib38))、Chang 等人 ([2017](#bib.bib9))。对于*3D实例分割*，可用的3D数据集有限，例如
    ScanNet Dai 等人 ([2017](#bib.bib18)) 和 S3DIS Armeni 等人 ([2016](#bib.bib2))。这两个数据集分别包含由RGB-D摄像头或Matterport扫描仪获取的真实世界室内场景扫描。对于*3D部件分割*，普林斯顿分割基准（PSB）Chen
    等人 ([2009](#bib.bib11))、COSEG Wang 等人 ([2012](#bib.bib153)) 和 ShapeNet Yi 等人 ([2016](#bib.bib181))
    是三个最受欢迎的数据集。下面，我们详细介绍五个著名的分割数据集，包括 S3DIS Armeni 等人 ([2016](#bib.bib2))、ScanNet
    Dai 等人 ([2017](#bib.bib18))、Semantic3D Hackel 等人 ([2017](#bib.bib38))、SemanticKITTI
    Chang 等人 ([2017](#bib.bib9)) 和 ShapeNet Yi 等人 ([2016](#bib.bib181))。图[3](#S1.F3
    "图 3 ‣ 1 引言 ‣ 基于深度学习的3D分割：综述")展示了这些数据集的注释示例。
- en: 'S3DIS: In this dataset, the complete point clouds are obtained without any
    manual intervention using the Matterport scanner. The dataset consists of 271
    rooms belonging to 6 large-scale indoor scenes from 3 different buildings (total
    of 6020 square meters). These areas mainly include offices, educational and exhibition
    spaces, and conference rooms etc.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: S3DIS：在此数据集中，完整的点云数据是通过Matterport扫描仪自动获取的，无需人工干预。该数据集包括来自3座不同建筑物的6个大规模室内场景中的271个房间（总面积6020平方米）。这些区域主要包括办公室、教育和展览空间、会议室等。
- en: Semantic3D comprises a total of around 4 billion 3D points acquired with static
    terrestrial laser scanners, covering up to 160×240×30 meters in real-world 3D
    space. Point clouds belong to 8 classes (e.g. urban and rural) and contain 3D
    coordinates, RGB information, and intensity. Unlike 2D annotation strategies,
    3D data labeling is easily amenable to over-segmentation where each point is individually
    assigned to a class label.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Semantic3D 总共包含约 40 亿个 3D 点，这些点是通过静态地面激光扫描仪获得的，覆盖了真实世界 3D 空间中的最大范围为 160×240×30
    米。点云属于 8 个类别（如城市和乡村），并包含 3D 坐标、RGB 信息和强度。与 2D 注释策略不同，3D 数据标注很容易出现过度分割，每个点都会被单独分配到一个类别标签。
- en: SemanticKITTI is a large outdoor dataset containing detailed point-wise annotation
    of 28 classes. Building on the KITTI vision benchmark Geiger et al. ([2012](#bib.bib31)),
    SemanticKITTI contains annotations of all 22 sequences of this benchmark consisting
    of 43K scans. Moreover, the dataset contains labels for the complete horizontal
    360 filed-of-view of the rotating laser sensor.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SemanticKITTI 是一个大型户外数据集，包含28个类别的详细逐点注释。基于 KITTI 视觉基准 Geiger 等人（[2012](#bib.bib31)），SemanticKITTI
    包含这个基准的所有22个序列的注释，共有43K个扫描。此外，该数据集包含了旋转激光传感器完整水平360度视场的标签。
- en: ScanNet dataset is particularly valuable for research in scene understanding
    as its annotations contain estimated calibration parameters, camera poses, 3D
    surface reconstruction, textured meshes, dense object level semantic segmentation,
    and CAD models. The dataset comprises annotated RGB-D scans of real-world environments.
    There are 2.5M RGB-D images in 1513 scans acquired in 707 distinct places. After
    RGB-D image processing, annotation human intelligence tasks were performed using
    the Amazon Mechanical Turk.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ScanNet 数据集对于场景理解研究特别有价值，因为其注释包含估算的校准参数、相机姿态、3D 表面重建、纹理网格、密集的物体级语义分割和 CAD 模型。该数据集包含了真实世界环境的
    RGB-D 扫描注释。共有 2.5M 个 RGB-D 图像，分布在 707 个不同的地方，共 1513 个扫描。经过 RGB-D 图像处理后，使用 Amazon
    Mechanical Turk 执行了注释的人工智能任务。
- en: ShapeNet dataset has a novel scalable method for efficient and accurate geometric
    annotation of massive 3D shape collections. The novel technical innovations explicitly
    model and lessen the human cost of the annotation effort. Researchers create detailed
    point-wise labeling of 31963 models in shape categories in ShapeNetCore and combine
    feature-based classifiers, point-to-point correspondences, and shape-to-shape
    similarities into a single CRF optimization over the network of shapes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ShapeNet 数据集具有一种新颖的可扩展方法，用于高效且准确地对大规模 3D 形状集合进行几何注释。这些新颖的技术创新显著减少了注释工作的人工成本。研究人员在
    ShapeNetCore 中创建了 31963 个模型的详细逐点标注，并将基于特征的分类器、点对点对应关系和形状对形状的相似性结合到一个 CRF 优化模型中。
- en: 'Table 1: Summary of popular datasets for 3D segmentation datasets including
    the sensor, type, size, object class, number of classes (shown in brackets), and
    annotation method. S←synthetic environment. R←real-world environment. Kf←thousand
    frames. s←scan. Mp←million points. the symbol ‘–’ means information unavailable.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：流行的 3D 分割数据集的总结，包括传感器、类型、尺寸、物体类别、类别数量（括号中显示）和注释方法。S←合成环境。R←真实世界环境。Kf←千帧。s←扫描。Mp←百万点。符号
    ‘–’ 表示信息不可用。
- en: '| Dataset | Sensors | Type | Size | Scene class (number) | Annotation method
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 传感器 | 类型 | 尺寸 | 场景类别（数量） | 注释方法 |'
- en: '| Datasets for 3D semantic segmentation |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 用于 3D 语义分割的数据集 |'
- en: '| NYUv1 Silberman and Fergus ([2011](#bib.bib129)) | Microsoft Kinect v1 |
    R | 2347f | bedroom, cafe, kitchen, etc. (7) | Condition Random Field-based model
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| NYUv1 Silberman 和 Fergus（[2011](#bib.bib129)） | Microsoft Kinect v1 | R |
    2347f | 卧室、咖啡馆、厨房等（7） | 基于条件随机场的模型 |'
- en: '| NYUv2 Silberman et al. ([2012](#bib.bib130)) | Microsoft Kinect v1 | R |
    1449f | bedroom, cafe, kitchen, etc. (26) | 2D annotation from AMK |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| NYUv2 Silberman 等人（[2012](#bib.bib130)） | Microsoft Kinect v1 | R | 1449f
    | 卧室、咖啡馆、厨房等（26） | 来自 AMK 的 2D 注释 |'
- en: '| SUN RGB-D Song et al. ([2015](#bib.bib132)) | RealSense, Xtion LIVE PRO,
    MKv1/2 | R | 10355f | objects, room layouts, etc.(47) | 2D/3Dpolygons +3D bounding
    box |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| SUN RGB-D Song 等人（[2015](#bib.bib132)） | RealSense, Xtion LIVE PRO, MKv1/2
    | R | 10355f | 物体、房间布局等（47） | 2D/3D多边形 + 3D 边界框 |'
- en: '| SceneNN Hua et al. ([2016](#bib.bib51)) | Asus Xtion PRO, MK v2 | R | 100s
    | bedroom, office, apartment, etc.(-) | 3D Labels project to 2D frames |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| SceneNN Hua 等人（[2016](#bib.bib51)） | Asus Xtion PRO, MK v2 | R | 100s | 卧室、办公室、公寓等（-）
    | 3D 标签投影到 2D 帧 |'
- en: '| RueMonge2014 Riemenschneider et al. ([2014](#bib.bib123)) | – | R | 428s
    | window, wall, balcony, door, etc(7) | Multi-view semantic labelling + CRF |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| RueMonge2014 Riemenschneider et al. ([2014](#bib.bib123)) | – | R | 428s
    | 窗户、墙壁、阳台、门等(7) | 多视角语义标注 + CRF |'
- en: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital structure sensor | R
    | 2.5Mf | office, apartment, bathroom, etc(19) | 3D labels project to 2D frames
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital 结构传感器 | R | 2.5Mf | 办公室、公寓、浴室等(19)
    | 3D 标签投影到 2D 框架上 |'
- en: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport camera | R | 70496f
    | conference rooms, offices, etc(11) | Hierarchical labeling |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport 摄像头 | R | 70496f | 会议室、办公室等(11)
    | 层级标注 |'
- en: '| Semantic3D Hackel et al. ([2017](#bib.bib38)) | Terrestrial laser scanner
    | R | 1660Mp | farms, town hall, sport fields, etc (8) | Three baseline methods
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Semantic3D Hackel et al. ([2017](#bib.bib38)) | 地面激光扫描仪 | R | 1660Mp | 农场、市政厅、运动场等(8)
    | 三种基准方法 |'
- en: '| NPM3D Roynard et al. ([2018](#bib.bib125)) | Velodyne HDL-32E LiDAR | R |
    143.1Mp | ground, vehicle, hunman, etc (50) | Human labeling |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| NPM3D Roynard et al. ([2018](#bib.bib125)) | Velodyne HDL-32E LiDAR | R |
    143.1Mp | 地面、车辆、人等(50) | 人工标注 |'
- en: '| SemanticKITTI Behley et al. ([2019](#bib.bib3)) | Velodyne HDL-64E | R |
    43Ks | ground, vehicle, hunman, etc(28) | Multi-scans semantic labelling |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SemanticKITTI Behley et al. ([2019](#bib.bib3)) | Velodyne HDL-64E | R |
    43Ks | 地面、车辆、人等(28) | 多扫描语义标注 |'
- en: '| Matterport3D Chang et al. ([2017](#bib.bib9)) | Matterport camera | R | 194.4Kf
    | various rooms (90) | Hierarchical labeling |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Matterport3D Chang et al. ([2017](#bib.bib9)) | Matterport 摄像头 | R | 194.4Kf
    | 各种房间 (90) | 层级标注 |'
- en: '| HoME Brodeur et al. ([2017](#bib.bib7)) | Planner5D platform | S | 45622f
    | rooms, object and etc.(84) | SSCNet+ a short text description |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| HoME Brodeur et al. ([2017](#bib.bib7)) | Planner5D 平台 | S | 45622f | 房间、物体等(84)
    | SSCNet+ 简短文本描述 |'
- en: '| House3D Wu et al. ([2018b](#bib.bib164)) | Planner5D platform | S | 45622f
    | rooms, object and etc.(84) | SSCNet+3 ways |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| House3D Wu et al. ([2018b](#bib.bib164)) | Planner5D 平台 | S | 45622f | 房间、物体等(84)
    | SSCNet+3 种方式 |'
- en: '| Datasets for 3D instance segmentation |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 3D 实例分割的数据集 |'
- en: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital structure sensor | R
    | 2.5Mf | office, apartment, bathroom, etc(19) | 3D labels project to 2D frames
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ScanNet Dai et al. ([2017](#bib.bib18)) | Occipital 结构传感器 | R | 2.5Mf | 办公室、公寓、浴室等(19)
    | 3D 标签投影到 2D 框架上 |'
- en: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport camera | R | 70496f
    | conference rooms, offices, etc(11) | Active learning method |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| S3DIS Armeni et al. ([2016](#bib.bib2)) | Matterport 摄像头 | R | 70496f | 会议室、办公室等(11)
    | 主动学习方法 |'
- en: '| Datasets for 3D part segmentation |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 3D 部件分割的数据集 |'
- en: '| ShapeNet Yi et al. ([2016](#bib.bib181)) | – | S | 31963s | transportation,
    tool, etc.(16) | Propagating human label to shapes |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ShapeNet Yi et al. ([2016](#bib.bib181)) | – | S | 31963s | 交通工具、工具等(16)
    | 将人工标签传播到形状上 |'
- en: '| PSB Chen et al. ([2009](#bib.bib11)) | Amazon’s Mechanical Turk | S | 380s
    | human,cup, glasses airplane,etc(19) | Interactive segmentation tool |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| PSB Chen et al. ([2009](#bib.bib11)) | 亚马逊 Mechanical Turk | S | 380s | 人、杯子、眼镜、飞机等(19)
    | 交互式分割工具 |'
- en: '| COSEG Wang et al. ([2012](#bib.bib153)) | – | S | 1090s | vase, lamp, guiter,
    etc (11) | semi-supervised learning method |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| COSEG Wang et al. ([2012](#bib.bib153)) | – | S | 1090s | 花瓶、灯、吉他等(11) |
    半监督学习方法 |'
- en: 2.2 Evaluation Metrics
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 评估指标
- en: Different evaluation metrics can assert the validity and superiority of segmentation
    methods including the execution time, memory footprint and accuracy. However,
    few authors provide detailed information about the execution time and memory footprint
    of their method. This paper introduces the accuracy metrics mainly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的评估指标可以验证分割方法的有效性和优越性，包括执行时间、内存占用和准确性。然而，很少有作者提供他们方法的执行时间和内存占用的详细信息。本文主要介绍准确性指标。
- en: For *3D semantic segmentation*, Overall Accuracy (OAcc), mean class Accuracy
    (mAcc) and mean class Intersection over Union (mIoU) are the most frequently used
    metrics to measure the accuracy of segmentation methods. For the sake of explanation,
    we assume that there are a total of $K+1$ classes, and $p_{ij}$ is the minimum
    unit (e.g. pixel, voxel, mesh, point) of class $i$ implied to belong to class
    $j$. In other words, $p_{ii}$ represents true positives, while $p_{ij}$ and $p_{ji}$
    represent false positives and false negatives respectively.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *3D 语义分割*，总体准确率 (OAcc)、均值类别准确率 (mAcc) 和均值类别交并比 (mIoU) 是衡量分割方法准确性的最常用指标。为了说明，我们假设共有
    $K+1$ 个类别，$p_{ij}$ 是类别 $i$ 被暗示为属于类别 $j$ 的最小单元（例如像素、体素、网格、点）。换句话说，$p_{ii}$ 代表真正例，而
    $p_{ij}$ 和 $p_{ji}$ 分别代表假正例和假负例。
- en: Overall Accuracy is a simple metric that computes the ratio between the number
    of truly classified samples and the total number of samples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 总体准确率（Overall Accuracy）是一个简单的指标，它计算真正分类样本数与总样本数之间的比例。
- en: '|  | $OAcc=\sum_{i=0}^{K}{\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}}$ |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $OAcc=\sum_{i=0}^{K}{\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}}$ |  |'
- en: Mean Accuracy is an extension of OAcc, computing OAcc in a per-class and then
    averaging over the total number of classes $K$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 平均准确率（Mean Accuracy）是 OAcc 的扩展，它计算每个类别的 OAcc，然后对总类别数 $K$ 取平均。
- en: '|  | $mAcc=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}$
    |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $mAcc=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}$
    |  |'
- en: Mean Intersection over Union is a standard metric for semantic segmentation.
    It computes the intersection ratio between ground truth and predicted value averaged
    over the total number of classes $K$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 平均交并比（Mean Intersection over Union, mIoU）是语义分割的标准指标。它计算地面真实值和预测值之间的交集比例，并对总类别数
    $K$ 取平均。
- en: '|  | $mIoU=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}+\sum_{i=0}^{K}p_{ji}-p_{ii}}$
    |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}+\sum_{i=0}^{K}p_{ji}-p_{ii}}$
    |  |'
- en: For *3D instance segmentation*, Average Precision (AP) and mean class Average
    Precision (mAP) are also frequently used. Assuming $L_{I},I\in[0,K]$ instance
    in every class, and $c_{ij}$ is the amount of point of instance $i$ inferred to
    belong to instance $j$ ($i=j$ represents correct and $i\neq j$ represents incorrect
    segmentations).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *3D 实例分割*，平均精度（AP）和平均类别平均精度（mAP）也经常使用。假设每个类别中有 $L_{I},I\in[0,K]$ 个实例，$c_{ij}$
    是实例 $i$ 被推断为实例 $j$ 的点数（$i=j$ 代表正确，$i\neq j$ 代表错误分割）。
- en: Average Precision is another simple metric for segmentation that computes the
    ratio between true positives and the total number of positive samples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 平均精度（Average Precision, AP）是另一种简单的分割指标，它计算真正例与正样本总数之间的比例。
- en: '|  | $AP=\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $AP=\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
- en: Mean Average precision is an extension of AP which computes per-class AP and
    then averages over the total number of classes $K$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 平均平均精度（Mean Average Precision, AP）是 AP 的扩展，它计算每个类别的 AP，然后对总类别数 $K$ 取平均。
- en: '|  | $mAP=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $mAP=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{i=0}^{L_{I}}\frac{c_{ii}}{c_{ii}+{\sum_{j=0}^{L_{I}}c_{ij}}}$
    |  |'
- en: For *3D part segmentation*, overall average category Intersection over Union
    ($mIoU_{cat}$) and overall average instance Intersection over Union ($mIoU_{ins}$)
    are most frequently used. For the sake of explanation, we assume $M_{J},J\in[0,L_{I}]$
    parts in every instance, and $q_{ij}$ as the total number of points in part $i$
    inferred to belong to part $j$. Hence, $q_{ii}$ represents the number of true
    positive, while $q_{ij}$ and $q_{ji}$ are false positives and false negative respectively.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *3D 部分分割*，总体类别平均交并比（$mIoU_{cat}$）和总体实例平均交并比（$mIoU_{ins}$）是最常用的指标。为了说明，我们假设每个实例中有
    $M_{J},J\in[0,L_{I}]$ 个部分，并且 $q_{ij}$ 是推断属于部分 $j$ 的部分 $i$ 的点总数。因此，$q_{ii}$ 代表真正例，而
    $q_{ij}$ 和 $q_{ji}$ 分别代表假阳性和假阴性。
- en: Overall average category Intersection over Union is an evaluation metric for
    part segmentation that measures the mean IoU averaged across K classes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 总体类别平均交并比（Overall average category Intersection over Union）是部分分割的评估指标，它测量跨 K
    类的平均 IoU。
- en: '|  | $mIoU_{cat}=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU_{cat}=\frac{1}{K+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
- en: Overall average instance Intersection over Union, for part segmentation, measures
    the mean IoU across all instances.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 总体实例平均交并比（$mIoU_{ins}$）在部分分割中测量所有实例的平均 IoU。
- en: '|  | $mIoU_{ins}=\frac{1}{\sum_{I=0}^{K}L_{I}+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $mIoU_{ins}=\frac{1}{\sum_{I=0}^{K}L_{I}+1}\sum_{I=0}^{K}\sum_{J=0}^{L_{I}}\sum_{i=0}^{M_{J}}\frac{q_{ii}}{\sum_{j=0}^{M_{j}}q_{ij}+\sum_{i=0}^{M_{j}}q_{ji}-q_{ii}}$
    |  |'
- en: 3 3D Semantic segmentation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D 语义分割
- en: 'Many deep learning methods on 3D semantic segmentation have been proposed in
    the literature. These methods can be divided into five categories according to
    the data representation used, namely, RGB-D image based, projected images based,
    voxel based, point based, 3D video and other representations based. Point based
    methods can be further categorized, based on the network architecture, into Multiple
    Layer Perceptron (MLP) based, Point Convolution based and Graph Convolution based
    and Point Transformer based methods. Figure [4](#S3.F4 "Figure 4 ‣ 3 3D Semantic
    segmentation ‣ Deep Learning Based 3D Segmentation: A Survey") shows the milestones
    of deep learning on 3D semantic segmentation in recent years.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了许多基于深度学习的 3D 语义分割方法。这些方法可以根据使用的数据表示分为五类，即基于 RGB-D 图像、基于投影图像、基于体素、基于点云、基于
    3D 视频和其他表示方法。基于点云的方法可以进一步分类，根据网络架构分为多层感知机（MLP）基、点卷积基、图卷积基和点变换器基的方法。图 [4](#S3.F4
    "图 4 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述") 显示了近年来深度学习在 3D 语义分割方面的里程碑。
- en: '![Refer to caption](img/1f5a4979d0c6a9c762dd7dce7e3a4b5c.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/1f5a4979d0c6a9c762dd7dce7e3a4b5c.png)'
- en: 'Figure 4: Milestones of deep learning based 3D semantic segmentation methods.
    Note that the arrow (timeline) goes anti-clockwise'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：基于深度学习的 3D 语义分割方法的里程碑。注意箭头（时间线）是逆时针方向
- en: 3.1 RGB-D Based
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于 RGB-D 的
- en: 'The depth map in an RGB-D image contains geometric information about the real-world
    which is useful to distinguish foreground objects from background, hence providing
    opportunities to improve the segmentation accuracy. In this category, generally
    the classical two-channel network is used to extract features from RGB and depth
    images separately. However, this simple framework is not powerful enough to extract
    rich and refined features. To this end, researchers have integrated several additional
    modules into the above simple two-channel framework to improve the performance
    by learning rich *context* and *geometric* information that are crucial for semantic
    segmentation. These modules can be roughly divided into six categories: multi-task
    learning, depth encoding, multi-scale network, novel neural network architectures,
    data/feature/score level fusion and post-processing (see Figure [5](#S3.F5 "Figure
    5 ‣ 3.1 RGB-D Based ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")). RGB-D image based semantic segmentation methods are summarized in
    Table [2](#S3.T2 "Table 2 ‣ 3.1 RGB-D Based ‣ 3 3D Semantic segmentation ‣ Deep
    Learning Based 3D Segmentation: A Survey").'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: RGB-D 图像中的深度图包含了关于现实世界的几何信息，这对区分前景物体和背景非常有用，从而提供了提高分割准确性的机会。在这一类别中，通常使用经典的双通道网络分别从
    RGB 和深度图像中提取特征。然而，这种简单的框架并不够强大，无法提取丰富和精细的特征。为此，研究人员在上述简单的双通道框架中集成了几个附加模块，通过学习对语义分割至关重要的丰富*上下文*和*几何*信息来提高性能。这些模块大致可以分为六类：多任务学习、深度编码、多尺度网络、新型神经网络架构、数据/特征/得分级融合和后处理（见图[5](#S3.F5
    "图 5 ‣ 3.1 基于 RGB-D 的 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述")）。基于 RGB-D 图像的语义分割方法总结见表[2](#S3.T2
    "表 2 ‣ 3.1 基于 RGB-D 的 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述")。
- en: Multi-tasks learning: *Depth estimation* and semantic segmentation are two fundamental
    challenging tasks in computer vision. These tasks are also somewhat related as
    depth variation within an object is small compared to depth variation between
    different objects. Hence, many researchers choose to unite depth estimation task
    and semantic segmentation task. From the view of relationship of the two tasks,
    there are two main types of multi-task leaning framework, cascade and parallel
    framework.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习：*深度估计*和语义分割是计算机视觉中的两个基本且具有挑战性的任务。这些任务之间也有一定的关联，因为在一个物体内部的深度变化相对于不同物体之间的深度变化较小。因此，许多研究人员选择将深度估计任务和语义分割任务结合起来。从这两个任务的关系来看，多任务学习框架主要有两种类型：级联框架和并行框架。
- en: As for the cascade framework, depth estimation task provides depth images for
    semantic segmentation task. For example, Cao et al.  Cao et al. ([2016](#bib.bib8))
    used the deep convolutional neural fields (DCNF) introduced by Liu et al.  Liu
    et al. ([2015](#bib.bib92)) for depth estimation. The estimated depth images and
    RGB images are fed into a two-channel FCN for semantic segmentation. Similarly,
    Guo et al.  Guo and Chen ([2018](#bib.bib35)) adopted the deep network proposed
    by Ivanecky  Ivaneckỳ ([2016](#bib.bib57)) for automatic generating depth images
    from single RGB images, and then proposed a two-channel FCN model on the image
    pair of RGB and predicted depth map for pixel labeling.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 关于级联框架，深度估计任务为语义分割任务提供深度图像。例如，Cao 等人 Cao et al. ([2016](#bib.bib8)) 使用了 Liu
    等人 Liu et al. ([2015](#bib.bib92)) 提出的深度卷积神经场（DCNF）进行深度估计。估计的深度图像和 RGB 图像被输入到一个双通道
    FCN 进行语义分割。同样，Guo 等人 Guo and Chen ([2018](#bib.bib35)) 采用了 Ivanecky Ivaneckỳ ([2016](#bib.bib57))
    提出的深度网络，从单个 RGB 图像中自动生成深度图像，然后提出了一个双通道 FCN 模型用于 RGB 和预测深度图的图像对进行像素标记。
- en: The cascade framework performs depth estimation and semantic segmentation separately,
    which is simultaneously unable to perform end-to-end training for two tasks. Consequently,
    depth estimation task does not get any benefit from semantic segmentation task.
    In contrast, the *parallel* framework performs these two tasks in an unify network,
    which allows two tasks get benefits each other. For instance, Wang et al.  Wang
    et al. ([2015](#bib.bib147)) used Joint Global CNN to exploit pixel-wise depth
    values and semantic labels from RGB images to provide accurate global scale and
    semantic guidance. As well as, they use Joint Region CNN to extract region-wise
    depth values and semantic map from RGB to learn detailed depth and semantic boundaries.
    Mousavian et al.  Mousavian et al. ([2016](#bib.bib107)) presented a multi-scale
    FCN comprising five streams that simultaneously explore depth and semantic features
    at different scales, where the two tasks share the underlying feature representation.
    Liu et al.  Liu et al. ([2018b](#bib.bib94)) proposed a collaborative deconvolutional
    neural network(C-DCNN) to jointly model the two tasks. However, the quality of
    depth maps estimated from RGB images is not as good as the one acquired directly
    from depth sensors. This multi-task learning pipeline has been gradually abandoned
    in RGB-D semantic segmentation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 级联框架将深度估计和语义分割任务分开进行，这使得无法对这两个任务进行端到端的训练。因此，深度估计任务没有从语义分割任务中获得任何好处。相比之下，*并行*框架在一个统一的网络中执行这两个任务，使得两个任务能够互相受益。例如，Wang
    等人 Wang et al. ([2015](#bib.bib147)) 使用联合全局 CNN 从 RGB 图像中提取像素级深度值和语义标签，以提供准确的全局尺度和语义指导。同时，他们使用联合区域
    CNN 从 RGB 中提取区域级深度值和语义图，以学习详细的深度和语义边界。Mousavian 等人 Mousavian et al. ([2016](#bib.bib107))
    提出了一个多尺度 FCN，由五个流组成，同时在不同尺度上探索深度和语义特征，其中两个任务共享底层特征表示。Liu 等人 Liu et al. ([2018b](#bib.bib94))
    提出了一个协作反卷积神经网络（C-DCNN）来联合建模这两个任务。然而，从 RGB 图像估计的深度图质量不如直接从深度传感器获取的深度图。这种多任务学习流程在
    RGB-D 语义分割中逐渐被弃用。
- en: 'Depth Encoding: Conventional 2D CNNs are unable to exploit rich geometric features
    from raw depth images. An alternative way is to encode raw depth images into other
    representations that are suitable to 2D CNN. Hoft et al. Höft et al. ([2014](#bib.bib47))
    used a simplified version of the histogram of oriented gradients (HOG) to represent
    depth channel from RGB-D scenes. Gupta et al. Gupta et al. ([2014](#bib.bib37))
    and Aman et al. Lin et al. ([2017](#bib.bib89)) calculated three new channels
    named horizontal disparity, height above ground and angle with gravity (HHA) from
    the raw depth images. Liu et al. Liu et al. ([2018a](#bib.bib93)) point out a
    limitation of HHA that some scenes may not be enough horizontal and vertical planes.
    Hence, they propose a novel gravity direction detection method with vertical lines
    fitted to learn better representation. Hazirbas et al. Hazirbas et al. ([2016](#bib.bib41))
    also argue that HHA representation has a high computational cost and contains
    less information than the raw depth images. They propose an architecture called
    FuseNet that consists of two encoder-decoder branches, including a depth branch
    and an RGB branch, which directly encodes depth information with a lower computational
    load.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 深度编码：传统的2D卷积神经网络（CNN）无法从原始深度图像中提取丰富的几何特征。另一种方法是将原始深度图像编码为适合2D CNN的其他表示形式。Hoft等人
    Höft et al. ([2014](#bib.bib47)) 使用了简化版本的方向梯度直方图（HOG）来表示RGB-D场景中的深度通道。Gupta等人
    Gupta et al. ([2014](#bib.bib37)) 和Aman等人 Lin et al. ([2017](#bib.bib89)) 从原始深度图像中计算了三个新的通道，分别是水平差异、高度和重力角度（HHA）。刘等人
    Liu et al. ([2018a](#bib.bib93)) 指出HHA的一个局限性是某些场景可能没有足够的水平和垂直平面。因此，他们提出了一种新的重力方向检测方法，通过拟合垂直线来学习更好的表示。Hazirbas等人
    Hazirbas et al. ([2016](#bib.bib41)) 还认为HHA表示具有较高的计算成本，并且信息量少于原始深度图像。他们提出了一种名为FuseNet的架构，由两个编码-解码分支组成，包括一个深度分支和一个RGB分支，直接以较低的计算负担编码深度信息。
- en: Multi-scale Network: The context information learned by multi-scale networks
    is useful for small objects and detailed region segmentation. Couprie et al. Couprie
    et al. ([2013](#bib.bib17)) applied a multi-scale convolutional network to learn
    features directly from the RGB images and the depth images. Aman et al. Raj et al.
    ([2015](#bib.bib119)) proposed a multi-scale deep ConvNet for segmentation where
    the coarse predictions of VGG16-FC net are up sampled in a Scale-2 module and
    then concatenated with the low-level predictions of VGG-M net in Scale-1 module
    to get both high and low level features. However, this method is sensitive to
    clutter in the scene resulting in output errors. Lin et al. Lin et al. ([2017](#bib.bib89))
    exploit the fact that lower scene-resolution regions have higher depth, and higher
    scene-resolution regions have lower depth. They use depth maps to split the corresponding
    color images into multiple scene-resolution regions, and introduce context-aware
    receptive field (CaRF) which focuses on semantic segmentation of certain scene-resolution
    regions. This makes their pipeline a multi-scale network.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度网络：多尺度网络学到的上下文信息对于小物体和详细区域分割非常有用。Couprie等人 Couprie et al. ([2013](#bib.bib17))
    应用多尺度卷积网络直接从RGB图像和深度图像中学习特征。Aman等人 Raj et al. ([2015](#bib.bib119)) 提出了一个多尺度深度卷积网络进行分割，其中VGG16-FC网络的粗略预测在Scale-2模块中上采样，然后与VGG-M网络在Scale-1模块中的低级预测进行拼接，以获得高低级特征。然而，这种方法对场景中的杂乱物体很敏感，导致输出错误。Lin等人
    Lin et al. ([2017](#bib.bib89)) 利用低场景分辨率区域具有较高深度和高场景分辨率区域具有较低深度的事实。他们使用深度图将相应的彩色图像分割成多个场景分辨率区域，并引入了上下文感知接收场（CaRF），关注某些场景分辨率区域的语义分割。这使得他们的管道成为一个多尺度网络。
- en: '![Refer to caption](img/fa0ffb98f08b256b59d1380f2cefaee2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa0ffb98f08b256b59d1380f2cefaee2.png)'
- en: 'Figure 5: Typical two-channel framework with six improvement modules, including
    (a) multi-tasks learning, (b) depth encoding, (c) multi-scale network, (d) novel
    neural network architecture, (e) feature/score level fusion, and (f) post-processing.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：典型的双通道框架，包括六个改进模块：(a) 多任务学习，(b) 深度编码，(c) 多尺度网络，(d) 新颖的神经网络架构，(e) 特征/评分级融合，和
    (f) 后处理。
- en: Novel Neural Networks: Given the fixed grid computation of CNNs, their ability
    to process and exploit geometric information is limited. Therefore, researchers
    have proposed other novel neural network architectures to better exploit geometric
    features and the relationships between RGB and depth images. These architectures
    can be divided into five main categories.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖神经网络：鉴于CNN的固定网格计算，其处理和利用几何信息的能力有限。因此，研究人员提出了其他新颖的神经网络架构，以更好地利用几何特征及RGB与深度图像之间的关系。这些架构可以分为五大类。
- en: '*Improved 2D Convolutional Neural Networks* (2D CNNs) Inspired from cascaded
    feature networks  Lin et al. ([2017](#bib.bib89)), Jiang et al. Jiang et al. ([2017](#bib.bib62))
    proposed a novel Dense-Sensitive Fully Convolutional Neural Network (DFCN) which
    incorporates depth information into the early layers of the network using feature
    fusion tactics. This is followed by several dilated convolutional layers for context
    information exploitation. Similarly, Wang et al. Wang and Neumann ([2018](#bib.bib150))
    proposed a depth-aware 2D CNN by introducing two novel layers, depth aware convolution
    layer and depth-aware pooling layer, which are based on the prior that pixels
    with the same semantic label and similar depth should have more impact on one
    another.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*改进的2D卷积神经网络*（2D CNNs）受到级联特征网络的启发，Lin等人（[2017](#bib.bib89)）和Jiang等人（[2017](#bib.bib62)）提出了一种新颖的深度敏感全卷积神经网络（DFCN），该网络通过特征融合策略将深度信息引入网络的早期层。接着，使用若干扩张卷积层来利用上下文信息。同样，Wang等人（Wang
    and Neumann [2018](#bib.bib150)）提出了一种深度感知的2D CNN，通过引入两个新层——深度感知卷积层和深度感知池化层——基于这样的前提：具有相同语义标签和相似深度的像素应对彼此产生更多影响。'
- en: '*Deconvolutional Neural Networks*(DeconvNets) are a simple yet effective and
    efficient solution for the refinement of segmentation map. Liu et al. Liu et al.
    ([2018b](#bib.bib94)) and Wang et al. Wang et al. ([2016](#bib.bib145)) all adopt
    the DeconvNet for RGB-D semantic segmentation because of good performance. However,
    the potential of DeconvNet is limited since the high-level prediction map aggregates
    large context for dense prediction. To this end, Cheng et al. Cheng et al. ([2017](#bib.bib13))
    proposed a locality-sensitive DeconvNet (LS-DenconvNet) to refine the boundary
    segmentation over depth and color images. LS-DeconvNet incorporates local visual
    and geometric cues from the raw RGB-D data into each DeconvNet, which is able
    to up sample the coarse convolutional maps with large context while recovering
    sharp object boundaries.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '*反卷积神经网络*（DeconvNets）是细化分割图的一个简单而有效的解决方案。Liu等人（[2018b](#bib.bib94)）和Wang等人（[2016](#bib.bib145)）都采用DeconvNet进行RGB-D语义分割，因为它的表现良好。然而，由于高层预测图聚合了大量上下文用于密集预测，DeconvNet的潜力有限。为此，Cheng等人（[2017](#bib.bib13)）提出了一种局部敏感的DeconvNet（LS-DeconvNet），用于细化深度和颜色图像的边界分割。LS-DeconvNet将来自原始RGB-D数据的局部视觉和几何线索融入每个DeconvNet，能够在恢复锐利的物体边界的同时，上采样具有大上下文的粗略卷积图。'
- en: '*Recurrent Neural Networks* (RNNs) can capture long-range dependencies between
    pixels but are mainly suited to a single data channel (e.g. RGB). Fan et al. Fan
    et al. ([2017](#bib.bib27)) extended the single-modal RNNs to multimodal RNNs
    (MM-RNNs) for application to RGB-D scene labeling. The MM-RNNs allow ‘memory’
    sharing across depth and color channels. Each channel not only possess its own
    features but also has the attributes of other channel making the learned features
    more discriminative for semantic segmentation. Li et al. Li et al. ([2016](#bib.bib84))
    proposed a novel Long Short-Term Memorized Context Fusion (LSTM-CF) model to capture
    and fuse contextual information from multiple channels of RGB and depth images.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*递归神经网络*（RNNs）能够捕捉像素之间的长程依赖，但主要适用于单一数据通道（例如RGB）。Fan等人（[2017](#bib.bib27)）将单模态RNN扩展到多模态RNN（MM-RNNs），以应用于RGB-D场景标注。MM-RNNs允许在深度和颜色通道之间共享‘记忆’。每个通道不仅拥有自己的特征，还具有其他通道的属性，使得学习到的特征在语义分割中更加具备区分性。Li等人（[2016](#bib.bib84)）提出了一种新颖的长短期记忆上下文融合（LSTM-CF）模型，用于捕捉和融合来自RGB和深度图像的多个通道的上下文信息。'
- en: '*Graph Neural Networks* (GNNs) were first used for RGB-D semantic segmentation
    by Qi et al. Qi et al. ([2017c](#bib.bib117)) who cast the 2D RGB pixels into
    3D space based on depth information and associated the 3D points with semantic
    information. Nest, they built a k-nearest neighbor graph from the 3D points and
    applied a 3D graph neural network (3DGNN) to perform pixelwise predictions.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*图神经网络*（GNNs）首次由 Qi 等人 ([2017c](#bib.bib117)) 用于 RGB-D 语义分割，他们将基于深度信息的 2D RGB
    像素投射到 3D 空间，并将 3D 点与语义信息关联。接下来，他们从 3D 点构建了一个 k 最近邻图，并应用了 3D 图神经网络（3DGNN）进行像素级预测。'
- en: '*Transformers* have gained popularity in RGB image segmentation and have also
    been extended to RGB-D segmentation. Researchers have proposed various approaches
    to leverage Transformers for this purpose. One notable work by Ying et al. Ying
    and Chuah ([2022](#bib.bib184)) introduces the concept of Uncertainty-Aware Self-Attention,
    which explicitly manages the information flow from unreliable depth pixels to
    confident depth pixels during feature extraction. This approach aims to address
    the challenges posed by noisy or uncertain depth information in RGB-D segmentation.
    Another study by Wu et al. Wu et al. ([2022c](#bib.bib166)) adopts the Swin-Transformer
    directly to exploit both the RGB and depth features. By leveraging the self-attention
    mechanism, this approach captures long-range dependencies and enables effective
    fusion of RGB and depth information for segmentation. Inspired by the success
    of the Swin-Transformer, Yang et al. Yang et al. ([2022](#bib.bib179)) proposes
    a hierarchical Swin-RGBD Transformer. This model incorporates and leverages depth
    information to complement and enhance the ambiguous and obscured features in RGB
    images. The hierarchical architecture allows for multi-scale feature learning
    and enables more effective integration of RGB and depth information.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*变压器* 在 RGB 图像分割中获得了广泛应用，并且也被扩展到 RGB-D 分割。研究人员提出了多种方法来利用变压器实现这一目的。Ying 等人 ([2022](#bib.bib184))
    的一项显著工作引入了不确定性感知自注意力的概念，该方法在特征提取过程中显式地管理从不可靠的深度像素到自信深度像素的信息流。这种方法旨在解决 RGB-D 分割中噪声或不确定深度信息带来的挑战。Wu
    等人 ([2022c](#bib.bib166)) 的另一项研究直接采用了 Swin-Transformer 来利用 RGB 和深度特征。通过利用自注意力机制，该方法捕捉了长距离依赖性，并有效融合了
    RGB 和深度信息进行分割。受到 Swin-Transformer 成功的启发，Yang 等人 ([2022](#bib.bib179)) 提出了一个层次化的
    Swin-RGBD 变压器。该模型结合并利用深度信息来补充和增强 RGB 图像中的模糊和遮挡特征。层次化架构允许进行多尺度特征学习，并实现了 RGB 和深度信息的更有效集成。'
- en: 'Data/Feature/Score Fusion: Optimal fusion of the texture (RGB channels) and
    geometric (depth channel) information is important for accurate semantic segmentation.
    There are three fusion tactics: data level, feature level and score level, referring
    to early, middle and late fusion respectively. A simple *data level fusion* strategy
    is to concatenate the RGB and depth images into four channels for direct input
    to a CNN model e.g. as performed by Couprie et al. Couprie et al. ([2013](#bib.bib17)).
    However, such a data level fusion does not exploit the strong correlations between
    depth and photometric channels. *Feature level fusion*, on the other hand, captures
    these correlations. For example, Li et al. Li et al. ([2016](#bib.bib84)) proposed
    a memorized fusion layer to adaptively fuse vertical depth and RGB contexts in
    a data-driven manner. Their method performs bidirectional propagation along the
    horizontal direction to hold true 2D global contexts. Similarly, Wang et al. Wang
    et al. ([2016](#bib.bib145)) proposed a feature transformation network that correlates
    the depth and color channels, and bridges the convolutional networks and deconvolutional
    networks in a single channel. The feature transformation network can discover
    specific features in a single channel as well as common features between two channels,
    allowing the two branches to share features to improve the representation power
    of shared information. The above complex feature level fusion models are inserted
    in a specific same layer between RGB and depth channels, which is difficult to
    train and ignores other same layer feature fusion. To this end, Hazirbas et al.
    Hazirbas et al. ([2016](#bib.bib41)) and Jiang et al. Jiang et al. ([2017](#bib.bib62))
    carry out fusion as an element-wise summation to fuse feature of multiple same
    layers between the two channels. Wu et al. Wu et al. ([2022c](#bib.bib166)) proposea
    novel Transformer-based fusion scheme, named TransD-Fusion to better model long-range
    contextual information.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据/特征/得分融合：纹理（RGB 通道）和几何（深度通道）信息的最佳融合对于准确的语义分割至关重要。融合策略有三种：数据层融合、特征层融合和得分层融合，分别指的是早期、中期和晚期融合。一个简单的*数据层融合*策略是将
    RGB 图像和深度图像拼接成四个通道，直接输入 CNN 模型，例如 Couprie 等人所做的。Couprie 等人 ([2013](#bib.bib17))。然而，这种数据层融合并没有利用深度通道和光度通道之间的强相关性。另一方面，*特征层融合*可以捕捉这些相关性。例如，Li
    等人 Li 等人 ([2016](#bib.bib84)) 提出了一个记忆融合层，以数据驱动的方式自适应地融合垂直深度和 RGB 上下文。他们的方法在水平方向上进行双向传播，以保持真实的
    2D 全局上下文。类似地，Wang 等人 Wang 等人 ([2016](#bib.bib145)) 提出了一个特征转换网络，该网络将深度和颜色通道关联起来，并在单个通道中连接卷积网络和反卷积网络。特征转换网络可以在单个通道中发现特定特征以及两个通道之间的共性特征，使两个分支可以共享特征，从而提高共享信息的表示能力。上述复杂的特征层融合模型被插入到
    RGB 和深度通道之间的特定相同层中，这使得训练变得困难，并且忽略了其他相同层特征融合。为此，Hazirbas 等人 Hazirbas 等人 ([2016](#bib.bib41))
    和 Jiang 等人 Jiang 等人 ([2017](#bib.bib62)) 通过逐元素求和的方式进行融合，以融合两个通道之间多个相同层的特征。Wu 等人
    Wu 等人 ([2022c](#bib.bib166)) 提出了一个新颖的基于 Transformer 的融合方案，称为 TransD-Fusion，以更好地建模远程上下文信息。
- en: '*Score level fusion* is commonly performed using the simple averaging strategy.
    However, the contributions of RGB model and depth model for semantic segmentation
    are different. Liu et al. Liu et al. ([2018a](#bib.bib93)) proposed a score level
    fusion layer with weighted summation that uses a convolution layer to learn the
    weights from the two channels. Similarly, Cheng et al. Cheng et al. ([2017](#bib.bib13))
    proposed a gated fusion layer to learn the varying performance of RGB and depth
    channels for different class recognition in different scenes. Both techniques
    improved the results over the simple averaging strategy at the cost of additional
    learnable parameters.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*得分层融合* 通常采用简单的平均策略。然而，RGB 模型和深度模型对语义分割的贡献是不同的。Liu 等人 Liu 等人 ([2018a](#bib.bib93))
    提出了一个具有加权求和的得分层融合层，该层使用卷积层从两个通道中学习权重。类似地，Cheng 等人 Cheng 等人 ([2017](#bib.bib13))
    提出了一个门控融合层，以学习 RGB 和深度通道在不同场景中对不同类别识别的不同性能。这两种技术在增加额外可学习参数的代价下，改善了简单平均策略的结果。'
- en: Post-Processing: The results of CNN or DCNN used for RGB-D semantic segmentation
    are generally very coarse resulting in rough boundaries and the vanishing of small
    objects. A common method to address this problem is to couple the CNN with a Conditional
    Random Field (CRF). Wang et al. Wang et al. ([2015](#bib.bib147)) further boost
    the mutual interactions between the two channels by the joint inference of Hierarchical
    CRF (HCRF). It enforces synergy between global and local predictions, where the
    global layouts are used to guide the local predictions and reduce local ambiguities,
    as well as local results provide detailed regional structures and boundaries.
    Mousavian et al. Mousavian et al. ([2016](#bib.bib107)), Liu et al. Liu et al.
    ([2018b](#bib.bib94)), and Long et al. Liu et al. ([2018a](#bib.bib93)) adopt
    a Fully Connected CRF (FC-CRF) for post-processing, where the pixel-wise label
    prediction jointly considers geometric constraint, such as pixel-wise normal information,
    pixel position, intensity and depth, to promote the consistency of pixel-wise
    labeling. Similarly, Jiang et al. Jiang et al. ([2017](#bib.bib62)) proposed Dense-sensitive
    CRF (DCRF) that integrates the depth information with FC-CRF.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 后处理：用于 RGB-D 语义分割的 CNN 或 DCNN 的结果通常非常粗糙，导致边界模糊和小物体消失。解决这个问题的一个常用方法是将 CNN 与条件随机场（CRF）结合。Wang
    等人（[2015](#bib.bib147)）通过层次 CRF（HCRF）的联合推理进一步增强了两个通道之间的互相作用。它在全局和局部预测之间强制协同，其中全局布局用于指导局部预测并减少局部模糊，同时局部结果提供详细的区域结构和边界。Mousavian
    等人（[2016](#bib.bib107)）、Liu 等人（[2018b](#bib.bib94)）和 Long 等人（[2018a](#bib.bib93)）采用了完全连接
    CRF（FC-CRF）进行后处理，其中像素级标签预测联合考虑几何约束，如像素级法线信息、像素位置、强度和深度，以促进像素级标记的一致性。类似地，Jiang
    等人（[2017](#bib.bib62)）提出了密集敏感 CRF（DCRF），将深度信息与 FC-CRF 相结合。
- en: 'Table 2: Summary of RGB-D based methods with deep learning. Est.←depth estimation.
    Enc.←depth encoding. Mul.←multi-scale networks. Nov.←novel neural networks. Fus.←data/feature/score
    fusion. Pos.←post-processing.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基于深度学习的 RGB-D 方法总结。估计←深度估计。编码←深度编码。多尺度←多尺度网络。新颖←新颖神经网络。融合←数据/特征/得分融合。后处理←后处理。
- en: '| Methods | Est. | Enc. | Mul. | Nov. | Fus. | Pos. | Architecture(2-stream)
    | Contribution |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 估计 | 编码 | 多尺度 | 新颖 | 融合 | 后处理 | 架构（2-stream） | 贡献 |'
- en: '| Cao et al. ([2016](#bib.bib8)) | $\checkmark$ | $\checkmark$ | $\times$ |
    $\times$ | $\checkmark$ | $\times$ | FCNs | Estimating depth images+a unified
    network for two tasks |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Cao 等人（[2016](#bib.bib8)） | $\checkmark$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\times$ | FCNs | 深度图像估计 + 统一网络用于两个任务 |'
- en: '| Guo and Chen ([2018](#bib.bib35)) | $\checkmark$ | $\times$ | $\times$ |
    $\times$ | $\checkmark$ | $\times$ | FCNs | Incorporating depth & gradient for
    depth estim. |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Guo 和 Chen（[2018](#bib.bib35)） | $\checkmark$ | $\times$ | $\times$ | $\times$
    | $\checkmark$ | $\times$ | FCNs | 结合深度和梯度用于深度估计 |'
- en: '| Wang et al. ([2015](#bib.bib147)) | $\checkmark$ | $\times$ | $\times$ |
    $\times$ | $\times$ | $\checkmark$ | Region./Global CNN | HCRF for fusion and
    refining + two tasks by a network |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人（[2015](#bib.bib147)） | $\checkmark$ | $\times$ | $\times$ | $\times$
    | $\times$ | $\checkmark$ | 区域/全局 CNN | HCRF 用于融合和精炼 + 通过网络完成两个任务 |'
- en: '| Mousavian et al. ([2016](#bib.bib107)) | $\checkmark$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\checkmark$ | FCN | FC-CRF for refining + Mutual
    improvement for two tasks |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Mousavian 等人（[2016](#bib.bib107)） | $\checkmark$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\checkmark$ | FCN | FC-CRF 用于精炼 + 两个任务的互相改进 |'
- en: '| Liu et al. ([2018b](#bib.bib94)) | $\checkmark$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | S/D-DCNN | PBL for two feature maps integration +
    FC-CRF |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人（[2018b](#bib.bib94)） | $\checkmark$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | S/D-DCNN | PBL 用于两个特征图的集成 + FC-CRF |'
- en: '| Höft et al. ([2014](#bib.bib47)) | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | A embedding for depth images |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Höft 等人（[2014](#bib.bib47)） | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | 深度图像的嵌入 |'
- en: '| Gupta et al. ([2014](#bib.bib37)) | $\times$ | $\checkmark$ | $\times$ |
    $\times$ | $\times$ | $\times$ | CNNs | HHA for depth images |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Gupta 等人（[2014](#bib.bib37)） | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\times$ | $\times$ | CNNs | 深度图像的 HHA |'
- en: '| Liu et al. ([2018a](#bib.bib93)) | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\checkmark$ | DCNNs | New depth encoding+ FC-CRF for refining
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Liu 等人（[2018a](#bib.bib93)） | $\times$ | $\checkmark$ | $\times$ | $\times$
    | $\checkmark$ | $\checkmark$ | DCNNs | 新的深度编码 + 用于精炼的 FC-CRF |'
- en: '| Hazirbas et al. ([2016](#bib.bib41)) | $\times$ | $\checkmark$ | $\times$
    | $\times$ | $\checkmark$ | $\times$ | Encoder-decoder | Semantic and depth feature
    fusion at each layer |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Hazirbas et al. ([2016](#bib.bib41)) | $\times$ | $\checkmark$ | $\times$
    | $\times$ | $\checkmark$ | $\times$ | Encoder-decoder | 每层的语义和深度特征融合 |'
- en: '| Couprie et al. ([2013](#bib.bib17)) | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | ConvNets | RGB laplacian pyramid for multi-scale
    features |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Couprie et al. ([2013](#bib.bib17)) | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | ConvNets | 用于多尺度特征的 RGB 拉普拉斯金字塔 |'
- en: '| Raj et al. ([2015](#bib.bib119)) | $\times$ | $\checkmark$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | VGG-M | New multi-scale deep CNN |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Raj et al. ([2015](#bib.bib119)) | $\times$ | $\checkmark$ | $\checkmark$
    | $\times$ | $\checkmark$ | $\times$ | VGG-M | 新型多尺度深度 CNN |'
- en: '| Lin et al. ([2017](#bib.bib89)) | $\times$ | $\times$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | $\times$ | CFN | CaRF for multi-resolution features |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al. ([2017](#bib.bib89)) | $\times$ | $\times$ | $\checkmark$ | $\checkmark$
    | $\checkmark$ | $\times$ | CFN | 用于多分辨率特征的 CaRF |'
- en: '| Jiang et al. ([2017](#bib.bib62)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\checkmark$ | RGB-FCN | Semantic & depth feature fusion at each
    layer + DCRF |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Jiang et al. ([2017](#bib.bib62)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\checkmark$ | RGB-FCN | 每层的语义和深度特征融合 + DCRF |'
- en: '| Wang and Neumann ([2018](#bib.bib150)) | $\times$ | $\times$ | $\times$ |
    $\checkmark$ | $\times$ | $\times$ | Depth-aware CNN | Depth-aware Conv. and depth
    aware average pooling |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Wang and Neumann ([2018](#bib.bib150)) | $\times$ | $\times$ | $\times$ |
    $\checkmark$ | $\times$ | $\times$ | Depth-aware CNN | 深度感知卷积和深度感知平均池化 |'
- en: '| Cheng et al. ([2017](#bib.bib13)) | $\times$ | $\checkmark$ | $\times$ |
    $\checkmark$ | $\checkmark$ | $\times$ | FCN + Deconv | LS-DeconvNet + novel gated
    fusion |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Cheng et al. ([2017](#bib.bib13)) | $\times$ | $\checkmark$ | $\times$ |
    $\checkmark$ | $\checkmark$ | $\times$ | FCN + Deconv | LS-DeconvNet + 新颖的门控融合
    |'
- en: '| Fan et al. ([2017](#bib.bib27)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | MM-RNNs | Multimodal RNN |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Fan et al. ([2017](#bib.bib27)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | MM-RNNs | 多模态 RNN |'
- en: '| Li et al. ([2016](#bib.bib84)) | $\times$ | $\checkmark$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | LSTM-CF | LSTM-CF for capturing and fusing contextual
    inf. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. ([2016](#bib.bib84)) | $\times$ | $\checkmark$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | LSTM-CF | 用于捕捉和融合上下文信息的 LSTM-CF |'
- en: '| Qi et al. ([2017c](#bib.bib117)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | 3DGNN | GNN for RGB-D semantic segmentation |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Qi et al. ([2017c](#bib.bib117)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | 3DGNN | 用于 RGB-D 语义分割的 GNN |'
- en: '| Wang et al. ([2016](#bib.bib145)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | ConvNet-DeconvNet | MK-MMD for assessing the similarity
    between common features |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2016](#bib.bib145)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | ConvNet-DeconvNet | 用于评估常见特征相似性的 MK-MMD |'
- en: '| Ying and Chuah ([2022](#bib.bib184)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformer | Effective and scalable fusion module
    based on aross-attention |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Ying and Chuah ([2022](#bib.bib184)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformer | 基于跨注意力的有效且可扩展的融合模块 |'
- en: '| Wu et al. ([2022c](#bib.bib166)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformers | Transformer-based fusion module
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. ([2022c](#bib.bib166)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\checkmark$ | $\times$ | Swin-Transformers | 基于 Transformer 的融合模块 |'
- en: '| Yang et al. ([2022](#bib.bib179)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | Swin-Transformer+ResNet | Swin-RGB-D Transformer |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al. ([2022](#bib.bib179)) | $\times$ | $\times$ | $\times$ | $\checkmark$
    | $\times$ | $\times$ | Swin-Transformer+ResNet | Swin-RGB-D Transformer |'
- en: 3.2 Projected Images Based Segmentation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 投影图像基础的分割
- en: The core idea of projected images based semantic segmentation is to use 2D CNNs
    to exploit features from projected images of 3D scenes/shapes and then fuse these
    features for label prediction. This pipeline not only exploits more semantic information
    from large-scale scenes compared to a single-view image, but also reduces the
    data size of a 3D scene compared to a point cloud. The projected images mainly
    include *multi-view images* or *spherical images*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 投影图像基础的语义分割的核心思想是使用 2D CNN 从 3D 场景/形状的投影图像中提取特征，然后融合这些特征进行标签预测。该流程不仅相比单视角图像可以利用更多来自大规模场景的语义信息，而且相比点云也减少了
    3D 场景的数据量。投影图像主要包括*多视角图像*或*球面图像*。
- en: 'Among, multi-view images projection is usually employed on RGB-D datasets Dai
    et al. ([2017](#bib.bib18)), and statics terrestrial scanning datasets Hackel
    et al. ([2017](#bib.bib38)). Spherical images projection is usually employed on
    self-driving mobile laser scanning datasets Behley et al. ([2019](#bib.bib3)).
    Projected images based semantic segmentation methods are summarized in Table [3](#S3.T3
    "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected Images Based
    Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey").'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在其中，多视角图像投影通常应用于 RGB-D 数据集 Dai 等人 ([2017](#bib.bib18))，以及静态地面扫描数据集 Hackel 等人
    ([2017](#bib.bib38))。球面图像投影通常应用于自动驾驶移动激光扫描数据集 Behley 等人 ([2019](#bib.bib3))。基于投影图像的语义分割方法总结在表
    [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected
    Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey")。'
- en: 3.2.1 Multi-View Images Based Segmentation
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 多视角图像基于的分割
- en: 'MVCNN Su et al. ([2015](#bib.bib135)) uses a unified network to combine features
    from multiple views of a 3D shape, formed by a virtual camera, into a single and
    compact shape descriptor to get improved classification performance. This inspired
    researchers to take the same idea into 3D semantic segmentation (see Figure [6](#S3.F6
    "Figure 6 ‣ 3.2.1 Multi-View Images Based Segmentation ‣ 3.2 Projected Images
    Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")). For example, Lawin et al. Lawin et al. ([2017](#bib.bib75)) project
    point clouds into multi-view synthetic images, including RGB, depth and surface
    normal images. The prediction score of all multi-view images is fused into a single
    representation and back-projected into each point. However, the snapshot can erroneously
    catch the points behind the observed structure if the density of the point cloud
    is low, which makes the deep network to misinterpret the multiple views. To this
    end, SnapNet Boulch et al. ([2017](#bib.bib6)), Boulch et al. ([2018](#bib.bib5))
    preprocesses point clouds for computing point features(like normal or local noise)
    and generating a mesh, which is similar to point cloud densification. From the
    mesh and point clouds, they generate RGB and depth images by suitable snapshot.
    Then, they perform a pixel-wise labeling of 2D snapshots using FCN and fast back-project
    these labels into 3D points by efficient buffering. Above methods need obtain
    the whole point clouds of 3D scene in advance to provide a complete spatial structure
    for back-projection. However, the multi-view images directly obtained from real-world
    scene would lose much spatial information. some works attempt to unite 3D scene
    reconstruction with semantic segmentation, where scene reconstruction could make
    up for spatial information. For example, Guerry et al. Guerry et al. ([2017](#bib.bib34))
    reconstruct 3D scene with global multi-view RGB and Gray stereo images. Then,
    the labels of 2D snapshots are back-projected onto the reconstructed scene. But,
    simple back-projection can not optimally fuse semantic and spatial geometric features.
    Along the line, Pham et al. Pham et al. ([2019a](#bib.bib113)) proposed a novel
    Higher-order CRF, following back-projection, to further develop the initial segmentation.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 'MVCNN Su 等人 ([2015](#bib.bib135)) 使用统一网络将来自虚拟摄像机的多个视角的 3D 形状特征合并为单一紧凑的形状描述符，以提高分类性能。这启发了研究人员将相同的想法应用于
    3D 语义分割（见图 [6](#S3.F6 "Figure 6 ‣ 3.2.1 Multi-View Images Based Segmentation ‣
    3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey")）。例如，Lawin 等人 ([2017](#bib.bib75)) 将点云投影到多视角合成图像中，包括
    RGB、深度和表面法线图像。所有多视角图像的预测分数融合成单一表示，并反向投影到每个点。然而，如果点云的密度较低，快照可能会错误地捕捉到观察结构后的点，这使得深度网络误解多个视角。为此，SnapNet
    Boulch 等人 ([2017](#bib.bib6))，Boulch 等人 ([2018](#bib.bib5)) 对点云进行预处理以计算点特征（如法线或局部噪声）并生成网格，这类似于点云密集化。通过网格和点云，他们通过合适的快照生成
    RGB 和深度图像。然后，他们使用 FCN 对 2D 快照进行逐像素标记，并通过高效缓冲将这些标签快速反向投影到 3D 点上。上述方法需要提前获得 3D 场景的完整点云，以提供完整的空间结构用于反向投影。然而，直接从现实世界场景获得的多视角图像会丢失大量空间信息。一些研究试图将
    3D 场景重建与语义分割结合起来，其中场景重建可以弥补空间信息。例如，Guerry 等人 ([2017](#bib.bib34)) 使用全局多视角 RGB
    和灰度立体图像重建 3D 场景。然后，将 2D 快照的标签反向投影到重建的场景上。但是，简单的反向投影不能最佳地融合语义和空间几何特征。沿此思路，Pham
    等人 ([2019a](#bib.bib113)) 提出了一个新的高阶 CRF，在反向投影之后，进一步发展初步分割。'
- en: '![Refer to caption](img/c9199e7695799d6dd521bc6661680a8c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9199e7695799d6dd521bc6661680a8c.png)'
- en: 'Figure 6: Illustration of basic frameworks for projected images based segmentation
    methods. Top: Multi-view images based framework. Bottom: Spherical images based
    framework.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：基于投影图像分割方法的基本框架说明。顶部：基于多视角图像的框架。底部：基于球面图像的框架。
- en: 3.2.2 Spherical Images Based Segmentation
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 球面图像基础的分割
- en: 'Selecting snapshots from a 3D scene is not straight forward. Snapshots must
    be taken after giving due consideration to the number of viewpoints, viewing distance
    and angle of the virtual cameras to get an optimal representation of the complete
    scene. To avoid these complexities, researchers project the complete point cloud
    onto a sphere (see Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1 Multi-View Images Based
    Segmentation ‣ 3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").Bottom). For example, Wu et
    al. Wu et al. ([2018a](#bib.bib159)) proposed an end-to-end pipeline called SqueezeSeg,
    inspired from SqueezeNet Iandola et al. ([2016](#bib.bib55)), to learn features
    from spherical images which are then refined by CRF implemented as a recurrent
    layer. Similarly, PointSeg Wang et al. ([2018e](#bib.bib154)) extends the SqueezeNet
    by integrating the feature-wise and channel-wise attention to learn robust representation.
    SqueezeSegv2 Wu et al. ([2019a](#bib.bib160)) improves the structure of SqueezeSeg
    with Context Aggregation Module (CAM), adding LiDAR mask as a channel to increase
    robustness to noise. RangNet++ Milioto et al. ([2019](#bib.bib105)) transfers
    the semantic labels to 3D point clouds, avoiding discarding points regardless
    of the level of discretization used in CNN. Despite the likeness between regular
    RGB and LiDAR images, the feature distribution of LiDAR images changes at different
    locations. SqueezeSegv3 Xu et al. ([2020](#bib.bib172)) has a spatially-adaptive
    and context-aware convolution, termed Spatially-Adaptive Convolution (SAC) to
    adopt different filters for different locations. Inspired by the success of 2D
    vision Transformer, RangViT Ando et al. ([2023](#bib.bib1)) leverage ViTs pre-trained
    on long natural image datasets by adding the down and up module on the top and
    bottom of ViTs, and achieves a good performance comparing to the projection based
    methods. Similarly, to make the long projection image to suit the ViTs, RangeFormer
    Kong et al. ([2023](#bib.bib70)) adopts a scalable training strategy that splits
    the whole projection image into several sub-images, and puts them into ViTs for
    training. After training, the predictions are merged sequentially to form the
    complete scene.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '从 3D 场景中选择快照并非简单。必须在考虑视点数量、视距和虚拟相机的角度之后进行拍摄，以获得完整场景的最佳表示。为避免这些复杂性，研究人员将完整的点云投影到一个球面上（见图 [6](#S3.F6
    "Figure 6 ‣ 3.2.1 Multi-View Images Based Segmentation ‣ 3.2 Projected Images
    Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey").底部）。例如，Wu 等人 Wu et al. ([2018a](#bib.bib159)) 提出了一个称为 SqueezeSeg 的端到端管道，灵感来源于
    SqueezeNet Iandola et al. ([2016](#bib.bib55))，用于从球面图像中学习特征，然后通过作为递归层实现的 CRF 进行优化。同样，PointSeg
    Wang 等人 ([2018e](#bib.bib154)) 通过集成特征级和通道级注意力来扩展 SqueezeNet，从而学习鲁棒的表示。SqueezeSegv2
    Wu 等人 ([2019a](#bib.bib160)) 通过上下文聚合模块 (CAM) 改进了 SqueezeSeg 的结构，增加了 LiDAR 掩码作为通道，以提高对噪声的鲁棒性。RangNet++
    Milioto 等人 ([2019](#bib.bib105)) 将语义标签转移到 3D 点云中，避免丢弃点而不管 CNN 使用的离散化水平。尽管常规 RGB
    和 LiDAR 图像之间有相似性，但 LiDAR 图像的特征分布在不同位置会发生变化。SqueezeSegv3 Xu 等人 ([2020](#bib.bib172))
    具有空间自适应和上下文感知卷积，称为空间自适应卷积 (SAC)，以便为不同位置采用不同的滤波器。受到 2D 视觉 Transformer 成功的启发，RangViT
    Ando 等人 ([2023](#bib.bib1)) 通过在 ViTs 的顶部和底部添加下采样和上采样模块，利用在大规模自然图像数据集上预训练的 ViTs，并取得了相对于投影方法的良好表现。同样，为了使长投影图像适应
    ViTs，RangeFormer Kong 等人 ([2023](#bib.bib70)) 采用了一种可扩展的训练策略，将整个投影图像拆分为多个子图像，并将其输入
    ViTs 进行训练。训练后，预测结果被顺序合并以形成完整的场景。'
- en: 'Table 3: Summary of projected images/voxel/other representation based methods
    with deep learning. M←multi-view image. S←spherical image. V←voxel. T←tangent
    images. L←lattice. P←point clouds.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于深度学习的投影图像/体素/其他表示方法的总结。M←多视角图像。S←球面图像。V←体素。T←切线图像。L←格点。P←点云。
- en: '| Type | Methods | Input | Architecture | Feature extractor | Contribution
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法 | 输入 | 架构 | 特征提取器 | 贡献 |'
- en: '| projection | Lawin et al. Lawin et al. ([2017](#bib.bib75)) | M | multi-stream
    | VGG-16 | Investigate the impact of different input modalities |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 投影 | Lawin 等人 Lawin et al. ([2017](#bib.bib75)) | M | 多流 | VGG-16 | 研究不同输入模式的影响
    |'
- en: '| Boulch et al. Boulch et al. ([2017](#bib.bib6))  Boulch et al. ([2018](#bib.bib5))
    | M | SegNet/U-Net | VGG-16 | New and efficient framework SnapNet |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Boulch et al. Boulch et al. ([2017](#bib.bib6))  Boulch et al. ([2018](#bib.bib5))
    | M | SegNet/U-Net | VGG-16 | 新的高效框架SnapNet |'
- en: '| Guerry et al. Guerry et al. ([2017](#bib.bib34)) | M | SegNet/U-Net | VGG-16
    | Improved MVCNN+3D consistent data augment. |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| Guerry et al. Guerry et al. ([2017](#bib.bib34)) | M | SegNet/U-Net | VGG-16
    | 改进的MVCNN+3D一致数据增强 |'
- en: '| Pham et al. Pham et al. ([2019a](#bib.bib113)) | M | Two-stream | 2DConv
    | High-order CRF+ real-time reconstruction pipeline |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Pham et al. Pham et al. ([2019a](#bib.bib113)) | M | Two-stream | 2DConv
    | 高阶CRF+实时重建管道 |'
- en: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) | S | AlexNet | Firemodules |
    End-to-end pipeline SqueezeSeg + real time |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) | S | AlexNet | Firemodules |
    端到端管道SqueezeSeg + 实时 |'
- en: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) | S | AlexNet | Firemodules
    | Quite light-weight framework PointSeg + real time |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) | S | AlexNet | Firemodules
    | 相当轻量的框架PointSeg + 实时 |'
- en: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) | S | AlexNet | Firemodules |
    Robust framework SqueezeSegV2 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) | S | AlexNet | Firemodules |
    强大的框架SqueezeSegV2 |'
- en: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) | S | DarkNet | Residual
    block | GPU-accelerated post-processing + RangNet++ |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) | S | DarkNet | Residual
    block | GPU加速后处理+ RangNet++ |'
- en: '| Xu et al. Xu et al. ([2020](#bib.bib172)) | S | RangeNet | SAC | Adopting
    different filters for different locations |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. Xu et al. ([2020](#bib.bib172)) | S | RangeNet | SAC | 为不同位置采用不同的滤波器
    |'
- en: '| Ando et al. Ando et al. ([2023](#bib.bib1)) | S | U-Net | ViTs | Decreasing
    the gaps between image and point domain. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Ando et al. Ando et al. ([2023](#bib.bib1)) | S | U-Net | ViTs | 减少图像和点域之间的差距
    |'
- en: '| Kong et al. Kong et al. ([2023](#bib.bib70)) | S | U-Net | ViTs | Introducing
    a scalable training from range view strategy |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Kong et al. Kong et al. ([2023](#bib.bib70)) | S | U-Net | ViTs | 引入可扩展的范围视图训练策略
    |'
- en: '| voxel | Huang et al. Huang and You ([2016](#bib.bib53)) | V | 3D CNN | 3DConv
    | Efficiently handling large data |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| voxel | Huang et al. Huang and You ([2016](#bib.bib53)) | V | 3D CNN | 3DConv
    | 高效处理大数据 |'
- en: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | V | 3D FCNN | 3DConv
    | Combining 3D FCNN with fine-represen. |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | V | 3D FCNN | 3DConv
    | 结合3D FCNN与细致表示 |'
- en: '| Meng et al. Meng et al. ([2019](#bib.bib103)) | V | VAE | RBF | A novel voxel-based
    representation + RBF |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Meng et al. Meng et al. ([2019](#bib.bib103)) | V | VAE | RBF | 一种新颖的体素表示+RBF
    |'
- en: '| Liu et al. Liu et al. ([2017](#bib.bib91)) | V | 3D CNN/DQN/RNN | 3DConv
    | Integrating three vision tasks into one frame. |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. Liu et al. ([2017](#bib.bib91)) | V | 3D CNN/DQN/RNN | 3DConv
    | 将三种视觉任务整合到一个框架中 |'
- en: '| Rethage et at. Rethage et al. ([2018](#bib.bib121)) | V | 3D FCNN | FPConv
    | First fully-convolutional network on raw point sets |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Rethage et al. Rethage et al. ([2018](#bib.bib121)) | V | 3D FCNN | FPConv
    | 首个在原始点集上完全卷积网络 |'
- en: '| Dai et al. Dai et al. ([2018](#bib.bib20)) | V | 3D FCNN | 3DConv | Combing
    scene completion and semantic labeling |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Dai et al. Dai et al. ([2018](#bib.bib20)) | V | 3D FCNN | 3DConv | 结合场景完成和语义标注
    |'
- en: '| Riegler et al. Riegler et al. ([2017](#bib.bib122)) | V | Octree | 3DConv
    | Making DL with high-resolution voxels |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Riegler et al. Riegler et al. ([2017](#bib.bib122)) | V | Octree | 3DConv
    | 使用高分辨率体素进行深度学习 |'
- en: '| Graham et al. Graham et al. ([2018](#bib.bib32)) | V | FCN/U-Net | SSConv
    | SSConv with less computation |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Graham et al. Graham et al. ([2018](#bib.bib32)) | V | FCN/U-Net | SSConv
    | 计算量更少的SSConv |'
- en: '| others | TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | T | U-Net
    | TConv | Tangent convolution + Parsing large scenes |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| others | TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | T | U-Net
    | TConv | 切线卷积+解析大场景 |'
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) | L | DeepLab | BConv | Hierarchical
    and spatially-aware feature learning |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| SPLATNet Su et al. ([2018](#bib.bib134)) | L | DeepLab | BConv | 分层和空间感知特征学习
    |'
- en: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) | L | U-Net | PN+3DConv | Hybrid
    architecture + novel slicing operator |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) | L | U-Net | PN+3DConv | 混合架构+新型切片操作符
    |'
- en: '| 3DMV Dai and Nießner ([2018](#bib.bib19)) | M+V | Cascade frame. | ENet+3DConv
    | Inferring 3D semantics from both 3D and 2D input |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 3DMV Dai and Nießner ([2018](#bib.bib19)) | M+V | Cascade frame. | ENet+3DConv
    | 从3D和2D输入推断3D语义 |'
- en: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) | V+M+P | Parallel frame.
    | SSCNet/DeepLab/PN | Leveraging 2D and 3D features |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) | V+M+P | Parallel frame.
    | SSCNet/DeepLab/PN | 利用2D和3D特征 |'
- en: '| PVCNN Liu et al. ([2019b](#bib.bib97)) | V+P | POintNet | PVConv | Both memory
    and computation efficient |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| PVCNN Liu et al. ([2019b](#bib.bib97)) | V+P | POintNet | PVConv | 既高效又节省内存和计算
    |'
- en: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) | M+P | Cascade frame. | U-Net+PointNet++
    | Leveraging 2D and 3D features |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) | M+P | 级联框架。 | U-Net+PointNet++
    | 利用2D和3D特征 |'
- en: '| LaserNet++ Meyer et al. ([2019](#bib.bib104)) | M+P | Cascade frame. | ResNet+LNet
    | Unified network for two tasks |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LaserNet++ Meyer et al. ([2019](#bib.bib104)) | M+P | 级联框架。 | ResNet+LNet
    | 用于两个任务的统一网络 |'
- en: '| BPNet Hu et al. ([2021](#bib.bib50)) | M+P | Cascade frame. | 2/3DUNet |
    Bidirection projection module |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| BPNet Hu et al. ([2021](#bib.bib50)) | M+P | 级联框架。 | 2/3DUNet | 双向投影模块 |'
- en: 3.3 Voxel Based Segmentation
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于体素的分割
- en: 'Similar to pixels, voxels divide the 3D space into many volumetric grids with
    a specific size and discrete coordinates. It contains more geometric information
    of the scene compared to projected images. 3D ShapeNets Wu et al. ([2015](#bib.bib165))
    and VoxNet Maturana and Scherer ([2015](#bib.bib101)) take volumetric occupancy
    grid representation as input to a 3D convolutional neural network for object recognition,
    which guides 3D semantic segmentation based on voxels. Voxel based semantic segmentation
    methods are summarized in Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based
    Segmentation ‣ 3.2 Projected Images Based Segmentation ‣ 3 3D Semantic segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于像素，体素将3D空间划分为许多具有特定大小和离散坐标的体积网格。与投影图像相比，它包含了场景的更多几何信息。3D ShapeNets Wu et
    al. ([2015](#bib.bib165)) 和 VoxNet Maturana 和 Scherer ([2015](#bib.bib101)) 将体积占用网格表示作为3D卷积神经网络的输入，用于物体识别，这指导了基于体素的3D语义分割。基于体素的语义分割方法总结在表
    [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected
    Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey")。'
- en: 3D CNN is a common architecture used to process uniform voxels for label prediction.
    Huang et al. Huang and You ([2016](#bib.bib53)) presented a 3D FCN for coarse
    voxel level predictions. Their method is limited by spatial inconsistency between
    predictions and provide a coarse labeling. Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138))
    introduce a novel network SEGCloud to produce fine-grained predictions. It up
    samples the coarse voxel-wise prediction obtained from a 3D FCN to the original
    3D point space resolution by trilinear interpolation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 3D CNN是一种常用的架构，用于处理均匀体素以进行标签预测。Huang et al. Huang 和 You ([2016](#bib.bib53))
    提出了用于粗略体素级预测的3D FCN。他们的方法受限于预测之间的空间不一致，并提供了粗略的标记。Tchapmi et al. Tchapmi et al.
    ([2017](#bib.bib138)) 引入了一种新颖的网络SEGCloud以产生细粒度预测。它通过三线性插值将从3D FCN获得的粗体素预测上采样到原始3D点空间分辨率。
- en: With fixed resolution voxels, the computational complexity grows linearly with
    the increase of the scene scale. Large voxels can lower the computational cost
    of large-scale scene parsing. Liu et al. Liu et al. ([2017](#bib.bib91)) introduced
    a novel network called 3D CNN-DQN-RNN. Like the sliding windows in 2D semantic
    segmentation, this network proposes eye window that traverses the whole data for
    fast localizing and segmenting class objects under the control of 3D CNN and deep
    Q-Network (DQN). The 3D CNN and Residual RNN further refine features in the eye
    window. The pipeline learns key features of interesting regions efficiently to
    enhance the accuracy of large-scale scene parsing with less computational cost.
    Rethage et at. Rethage et al. ([2018](#bib.bib121)) present a novel fully convolutional
    point network (FCPN), sensitive to multi-scale input , to parse large-scale scene
    without pro- or post-process steps. Particularly, FCPN is able to learn memory
    efficient representations that scale well to larger volumes. Similarly, Dai et
    al. Dai et al. ([2018](#bib.bib20)) design a novel 3D CNN to train on scene subvolumes
    but deploy on arbitrarily large scenes at test time, as it is able to handle large
    scenes with varying spatial extent. Additionally, their network adopts a coarse-to-fine
    tactic to predict multiple resolution scenes to handle the resolution growth in
    data size as the scene increases in size. Traditionally, the voxel representation
    only comprises Boolean occupancy information which loses much geometric informatin.
    Meng et al. Meng et al. ([2019](#bib.bib103)) develop a novel information-rich
    voxel representation by using a variational auto-encoder(VAE) taking radial basis
    function(RBF) to capture the distribution of points within each voxel. Further,
    they proposed a group equivariant convolution to exploit feature.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 固定分辨率的体素中，计算复杂度随着场景规模的增加而线性增长。大体素可以降低大规模场景解析的计算成本。**刘等人** ([2017](#bib.bib91))
    引入了一种新颖的网络，称为 3D CNN-DQN-RNN。与 2D 语义分割中的滑动窗口类似，该网络提出了一个遍历整个数据的视窗，用于在 3D CNN 和深度
    Q 网络 (DQN) 的控制下快速定位和分割类对象。3D CNN 和 Residual RNN 进一步在视窗中细化特征。该流程有效学习有趣区域的关键特征，以提高大规模场景解析的准确性，同时减少计算成本。**Rethage
    等人** ([2018](#bib.bib121)) 提出了一个新颖的全卷积点网络 (FCPN)，对多尺度输入敏感，以解析大规模场景而无需前处理或后处理步骤。特别是，FCPN
    能够学习内存高效的表示，能够很好地扩展到更大的体积。类似地，**戴等人** ([2018](#bib.bib20)) 设计了一种新型的 3D CNN，用于在场景子体积上进行训练，但在测试时可应用于任意大的场景，因为它能够处理具有不同空间范围的大型场景。此外，他们的网络采用了从粗到细的策略来预测多个分辨率的场景，以应对数据大小随场景增加而增长的分辨率。传统的体素表示仅包括布尔占用信息，丢失了大量的几何信息。**孟等人**
    ([2019](#bib.bib103)) 通过使用变分自编码器 (VAE) 和径向基函数 (RBF) 捕捉每个体素内点的分布，开发了一种新颖的信息丰富的体素表示。此外，他们提出了一种群体等变卷积以利用特征。
- en: In fixed scale scenes, the computational complexity grows cubically as the voxel
    resolution increases. However, the volumetric representation is naturally sparse,
    resulting in unnecessary computations when applying 3D dense convolution on the
    sparse data. To aleviate this problem, OctNet Riegler et al. ([2017](#bib.bib122))
    divides the space hierarchically into nonniform voxels using a series of unbalanced
    octrees. Tree structure allows memory allocation and computation to focus on relevant
    dense voxels without sacrificing resolution. However, empty space still imposes
    computational and memory burden in OctNet. In contrast, Graham et al. Graham et al.
    ([2018](#bib.bib32)) proposed a novel submanifold sparse convolution (SSC) that
    does not perform computations in empty regions, making up for the drawback of
    OctNet.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定尺度场景中，计算复杂度随着体素分辨率的增加而立方增长。然而，体积表示自然是稀疏的，这在对稀疏数据应用 3D 密集卷积时会导致不必要的计算。为了解决这个问题，**Riegler
    等人** ([2017](#bib.bib122)) 使用一系列不均匀八叉树将空间层次化地划分为非均匀体素。树结构使得内存分配和计算能够集中在相关的密集体素上，而不牺牲分辨率。然而，空闲空间仍然对
    OctNet 施加了计算和内存负担。相比之下，**Graham 等人** ([2018](#bib.bib32)) 提出了一个新颖的子流形稀疏卷积 (SSC)，它在空闲区域不进行计算，弥补了
    OctNet 的缺陷。
- en: 3.4 Point Based Segmentation
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 基于点的分割
- en: 'Point clouds are scattered irregularly in 3D space, lacking any canonical order
    and translation invariance, which restricts the use of conventional 2D/3D convolutional
    neural networks. Recently, a series of point-based semantic segmentation networks
    have been proposed. These methods can be roughly subdivided into four categories:
    MLP based, point convolution based, graph convolution based and Transformer based.
    These methods are summarized in Table [4](#S3.T4 "Table 4 ‣ 3.4.4 Transformer
    Based ‣ 3.4 Point Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning
    Based 3D Segmentation: A Survey").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 点云在 3D 空间中分布不规则，缺乏任何标准顺序和平移不变性，这限制了传统 2D/3D 卷积神经网络的使用。最近，提出了一系列基于点的语义分割网络。这些方法大致可以分为四类：基于
    MLP 的、基于点卷积的、基于图卷积的和基于 Transformer 的。这些方法总结在表 [4](#S3.T4 "表 4 ‣ 3.4.4 基于 Transformer
    的 ‣ 3.4 基于点的分割 ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：综述") 中。
- en: 3.4.1 MLP Based
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 基于 MLP 的方法
- en: These methods apply a Multi Layer Perceptron directly on the points to learn
    features. The PointNet Qi et al. ([2017a](#bib.bib115)) is a pioneering work that
    directly processes point clouds. It uses shared MLP to exploit points-wise features
    and adopts a symmetric function such as max-pooling to collect these features
    into a global feature representation. Because the max-pooling layer only captures
    the maximum activation across global points, PointNet cannot learn to exploit
    local features. Building on PointNet, PointNet++ Qi et al. ([2017b](#bib.bib116))
    defines a hierarchical learning architecture. It hierarchically samples points
    using farthest point sampling (FPS) and groups local regions using k nearest neighbor
    search as well as ball search. Progressively, a simplified PointNet exploits features
    in local regions at multiple scales or multiple resolutions. Similarly, Engelmann
    et al. Engelmann et al. ([2018](#bib.bib26)) define local regions by KNN clustering
    and K-means clustering and use a simplified PointNet to extract local features.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法直接在点上应用多层感知机（MLP）以学习特征。PointNet Qi 等人 ([2017a](#bib.bib115)) 是一项开创性工作，直接处理点云。它使用共享
    MLP 来利用点级特征，并采用诸如最大池化这样的对称函数将这些特征收集到全局特征表示中。由于最大池化层仅捕获全局点的最大激活，PointNet 无法学习利用局部特征。在
    PointNet 的基础上，PointNet++ Qi 等人 ([2017b](#bib.bib116)) 定义了一个层次化学习架构。它通过最远点采样（FPS）逐层采样点，并使用
    k 最近邻搜索以及球形搜索来分组局部区域。逐步地，简化的 PointNet 在多个尺度或多个分辨率下提取局部区域的特征。类似地，Engelmann 等人 ([2018](#bib.bib26))
    通过 KNN 聚类和 K-means 聚类定义局部区域，并使用简化的 PointNet 提取局部特征。
- en: To learn the short and long-range dependencies, some works introduce the Recurrent
    Neural Networks (RNN) to MLP-based methods. For example, ESC Engelmann et al.
    ([2017](#bib.bib24)) divides global points into multi-scale/grid blocks. The concatenated
    (local) block features are appended to the point-wise features and passed through
    Recurrent Consolidation Units (RCUs) to further learn global context features.
    Similarly, HRNN Ye et al. ([2018](#bib.bib180)) uses Pointwise Pyramid Pooling
    (3P) to extract local features on the multi-size local regions. Point-wise features
    and local features are concatenated and a two-direction hierarchical RNN explores
    context features on these concatenated features. However, the local features learned
    are not sufficient because the deeper layer features do not cover a larger spatial
    extent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习短期和长期依赖，一些研究将递归神经网络（RNN）引入到基于 MLP 的方法中。例如，ESC Engelmann 等人 ([2017](#bib.bib24))
    将全局点划分为多尺度/网格块。将（局部）块特征连接到点级特征中，并通过递归整合单元（RCU）进一步学习全局上下文特征。类似地，HRNN Ye 等人 ([2018](#bib.bib180))
    使用点级金字塔池化（3P）在多尺度局部区域中提取局部特征。点级特征和局部特征被连接起来，双向层次 RNN 在这些连接的特征上探索上下文特征。然而，所学习的局部特征仍然不足，因为深层特征无法覆盖更大的空间范围。
- en: Another technology, some works integrate the hand-craft point representation
    into PointNet or PointNet++ network to enhance the point representation ability
    with less learnable netowrk parameters. Inspired by SIFT representation Lowe ([2004](#bib.bib98)),
    PointSIFT Jiang et al. ([2018](#bib.bib64)) inserts a PointSIFT module layer learn
    local shape information. This module transforms each point into a new shape representation
    by encoding information of different orientations. PointWeb Zhao et al. ([2019a](#bib.bib193))
    propose a adaptive feature adjustment (AFA) module to learning the interactive
    information between local points to enhance the point representation. Similarly,
    RepSurf Ran et al. ([2022](#bib.bib120)) introduces two novel point representations,
    namely triangular and umbrella representative surfaces, to establish connections
    and enhance the representation capability of learned point-wise features. This
    approach effectively improves feature representation with fewer learnable network
    parameters, drawing significant attention from the research community. In contrast
    to the aforementioned methods, PointNeXt Qian et al. ([2022](#bib.bib118)) takes
    a different approach by revisiting the classical PointNet++ architecture through
    a systematic study of model training and scaling strategies. It proposes a set
    of improved training strategies that lead to a significant performance boost for
    PointNet++. Additionally, PointNeXt introduces an inverted residual bottleneck
    design and employs separable MLPs to enable efficient and effective model scaling.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项技术中，一些工作将手工制作的点表示集成到PointNet或PointNet++网络中，以提高点表示能力，同时减少可学习的网络参数。受到SIFT表示的启发，Lowe
    ([2004](#bib.bib98))，PointSIFT Jiang et al. ([2018](#bib.bib64)) 插入了一个PointSIFT模块层来学习局部形状信息。该模块通过编码不同方向的信息，将每个点转换为新的形状表示。PointWeb
    Zhao et al. ([2019a](#bib.bib193)) 提出了一个自适应特征调整（AFA）模块，以学习局部点之间的交互信息，从而增强点表示能力。同样，RepSurf
    Ran et al. ([2022](#bib.bib120)) 引入了两种新型点表示，即三角形和伞形代表面，以建立连接并增强所学点特征的表示能力。这种方法在减少可学习网络参数的同时有效提升了特征表示，引起了研究界的广泛关注。与上述方法相比，PointNeXt
    Qian et al. ([2022](#bib.bib118)) 采取了不同的方法，通过系统地研究模型训练和扩展策略重新审视经典的PointNet++架构。它提出了一套改进的训练策略，显著提升了PointNet++的性能。此外，PointNeXt引入了倒置残差瓶颈设计，并采用可分离的MLP以实现高效和有效的模型扩展。
- en: 3.4.2 Point Convolution Based
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 基于点卷积
- en: Point convolution based methods perform convolution operations directly on the
    points. Different from 2D convolution , the weight function of point convolution
    need learn from point geometric information adaptively. Early convolution networks
    focus on the convolution weight function design. For example, RSNet Huang et al.
    ([2018](#bib.bib54)) exploit point-wise features using 1x1 convolution and then
    pass them through the local dependency module (LDM) to exploit local context features.
    However, it does not define the neighborhood for each point in order to learn
    local features. On the other hand, PointwiseCNN  Hua et al. ([2018](#bib.bib52))
    sorts points in a specific order, e.g. XYZ coordinate or Morton cureve Morton
    ([1966](#bib.bib106)), and queries nearest neighbors dynamically and bins them
    into 3x3x3 kernel cells before convolving with the same kernel weights.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 基于点卷积的方法直接对点进行卷积操作。与2D卷积不同，点卷积的权重函数需要从点的几何信息中自适应学习。早期的卷积网络关注于卷积权重函数的设计。例如，RSNet
    Huang et al. ([2018](#bib.bib54)) 利用1x1卷积提取点特征，然后通过局部依赖模块（LDM）来挖掘局部上下文特征。然而，它没有定义每个点的邻域来学习局部特征。另一方面，PointwiseCNN
    Hua et al. ([2018](#bib.bib52)) 将点按特定顺序排序，例如XYZ坐标或Morton曲线 Morton ([1966](#bib.bib106))，并动态查询最近邻点，将其分组到3x3x3的内核单元中，然后使用相同的内核权重进行卷积。
- en: Gradually, some point convolution works approximate the convolution weight function
    as MLP to learn weights from point coordinates. PCCN  Wang et al. ([2018c](#bib.bib149))
    performs Parametric CNN, where the kernel is estimated as an MLP, on KD-tree neighborhood
    to learn local features. PointCNN  Li et al. ([2018b](#bib.bib82)) coarsens the
    input points with farthest point sampling. The convolution layer learns an $\chi$-transformation
    from local points by MLP to simultaneously weight and permute the features, subsequently
    applying a standard convolution on these transformed features.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 渐渐地，一些点卷积工作将卷积权重函数近似为 MLP，以从点坐标中学习权重。PCCN  Wang 等人（[2018c](#bib.bib149)）执行参数化
    CNN，其中卷积核被估计为 MLP，在 KD-tree 邻域上学习局部特征。PointCNN  Li 等人（[2018b](#bib.bib82)）通过最远点采样来粗化输入点。卷积层通过
    MLP 从局部点学习一个 $\chi$-变换，以同时加权和排列特征，然后对这些变换后的特征应用标准卷积。
- en: Some works associates a coefficient (derived from point coordinates) with the
    weight function to adjust the learned convolutional weights. An extension of Monte
    Carlo approximation for convolution called PointConv  Wu et al. ([2019b](#bib.bib161))
    takes the point density into account. It uses MLP to approximate a weight function
    of the convolution kernel, and applies an inverse density scale to reweight the
    learned weight function. Similarly, MCC  Hermosilla et al. ([2018](#bib.bib46))
    phrases convolution as a Monte Carlo integration problem by relying on point probability
    density function (PDF), where the convolution kernel is also represented by an
    MLP. Moreover, it introduces Possion Disk Sampling (PDS) Wei ([2008](#bib.bib157))
    to construct a point hierarchy instead of FPS, which provides an opportunity to
    get the maximal number of samples in a receptive field.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究将系数（由点坐标得出）与权重函数相关联，以调整学习到的卷积权重。PointConv  Wu 等人（[2019b](#bib.bib161)）是卷积的蒙特卡罗近似的扩展，考虑了点密度。它使用
    MLP 来近似卷积核的权重函数，并应用逆密度缩放来重新加权学习到的权重函数。类似地，MCC  Hermosilla 等人（[2018](#bib.bib46)）将卷积描述为一个蒙特卡罗积分问题，依赖于点的概率密度函数（PDF），其中卷积核也由
    MLP 表示。此外，它引入了 Possion Disk Sampling（PDS） Wei（[2008](#bib.bib157)）来构建点层次，而不是 FPS，从而提供了在接收场中获取最大样本数量的机会。
- en: Another line of works use other function instead of MLP to approximate the convolution
    weight function. Flex-Convolution  Groh et al. ([2018](#bib.bib33)) uses a linear
    function with fewer parameters to model a convolution kernel and adapts inverse
    density importance sub-sampling (IDISS) to coarsen the points. KPConv Thomas et al.
    ([2019](#bib.bib139)) and KCNet Shen et al. ([2018](#bib.bib126)) fixed the convolution
    kernel for robustness to varying point density. These networks predefine the kernel
    points on local region and learn convolutional weights on the kernel points from
    their geometric connections to local points using linear and Gaussian correlation
    functions, respectively. Here, the number and position of kernel points need be
    optimized for different datasets.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究使用其他函数代替多层感知机（MLP）来近似卷积权重函数。Flex-Convolution  Groh 等人（[2018](#bib.bib33)）使用具有较少参数的线性函数来建模卷积核，并适配逆密度重要性子采样（IDISS）来粗化点。KPConv
    Thomas 等人（[2019](#bib.bib139)）和 KCNet Shen 等人（[2018](#bib.bib126)）为提高对点密度变化的鲁棒性，固定了卷积核。这些网络在局部区域上预定义核点，并通过线性和高斯相关函数分别学习核点的卷积权重。这里，核点的数量和位置需要根据不同的数据集进行优化。
- en: Point convolution on limited local receptive field could not exploit long-range
    features. Therefore, some works introduce the dilated mechanism into point convolution.
    Dilated point convolution(DPC)  Engelmann et al. ([2020b](#bib.bib25)) adapts
    standard point convolution on neighborhood points of each point where the neighborhood
    points are determined though a dilated KNN search. similarly, A-CNN Komarichev
    et al. ([2019](#bib.bib69)) defines a new local ring-shaped region by dilated
    KNN, and projects points on a tangent plane to further order neighbor points in
    local regions. Then, the standard point convolutions are performed on these ordered
    neighbors represented as a closed loop array.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在有限的局部接收场上，点卷积无法利用长距离特征。因此，一些工作将膨胀机制引入点卷积。膨胀点卷积（DPC）  Engelmann 等人（[2020b](#bib.bib25)）将标准点卷积适配于每个点的邻域点，邻域点通过膨胀
    KNN 搜索来确定。类似地，A-CNN Komarichev 等人（[2019](#bib.bib69)）通过膨胀 KNN 定义了一个新的局部环形区域，并将点投影到切平面上，以进一步排序局部区域中的邻居点。然后，在这些排序后的邻居点上执行标准点卷积，这些邻居点表示为一个闭环数组。
- en: In the large-scale point clouds semantic segmentation area, RandLA-Net  Hu et al.
    ([2020](#bib.bib49)) uses random point sampling instead of the more complex point
    selection approach. It introduces a novel local feature aggregation module (LFAM)
    to progressively increase the receptive field and effectively preserve geometric
    details. Another technology, PolarNet  Zhang et al. ([2020](#bib.bib190)) first
    partitions a large point cloud into smaller grids (local regions) along their
    polar bird’s-eye-view (BEV) coordinates. It then abstracts local region points
    into a fixed-length representation by a simplified PointNet and these representations
    are passed through a standard convolution.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模点云语义分割领域，RandLA-Net Hu 等人（[2020](#bib.bib49)）使用随机点采样方法替代了更复杂的点选择方法。它引入了一种新颖的局部特征聚合模块（LFAM），以逐步扩大感受野，并有效保留几何细节。另一种技术，PolarNet
    Zhang 等人（[2020](#bib.bib190)）首先将大规模点云按其极坐标鸟瞰视角（BEV）坐标划分为较小的网格（局部区域）。然后，它通过简化的
    PointNet 将局部区域的点抽象为固定长度的表示，并将这些表示通过标准卷积处理。
- en: 3.4.3 Graph Convolution Based
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 基于图卷积
- en: The graph convolution based methods perform convolution on points connected
    with a graph structure, where the graph help the feature aggregation exploit the
    structure information between points. the graphs can be divided into spectral
    graph and spatial graph. In the spectral graph, LS-GCN  Wang et al. ([2018a](#bib.bib143))
    adopts the basic architecture of PointNet++, replaces MLPs with a spectral graph
    convolution using standard unparametrized Fourier kernels, as well as a novel
    recursive spectral cluster pooling substitute for max-pooling. However, transformation
    from spatial to spectral domain incurs a high computational cost. Besides that,
    spectral graph networks are usually defined on a fixed graph structure and are
    thus unable to directly process data with varying graph structures.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图卷积的方法在连接图结构的点上执行卷积，其中图结构有助于特征聚合，利用点之间的结构信息。图可以分为谱图和空间图。在谱图中，LS-GCN Wang 等人（[2018a](#bib.bib143)）采用了
    PointNet++ 的基本架构，将 MLP 替换为使用标准非参数傅里叶核的谱图卷积，并用新颖的递归谱聚类池化替代最大池化。然而，从空间域到谱域的转换会带来较高的计算成本。此外，谱图网络通常定义在固定的图结构上，因此无法直接处理具有不同图结构的数据。
- en: In the spatial graph category, ECC  Simonovsky and Komodakis ([2017](#bib.bib131))
    is among of the pioneer methods to apply spatial graph network to extract features
    from point clouds. It dynamically generates edge-conditioned filters to learn
    edge features that describe the relationships between a point and its neighbors.
    Based on PointNet architecture, DGCNN  Wang et al. ([2019b](#bib.bib155)) implements
    dynamic edge convolution called EdgeConv on the neighborhood of each point. The
    convolution is approximated by a simplified PointNet. SPG  Landrieu and Simonovsky
    ([2018](#bib.bib74)) parts the point clouds into a number of simple geometrical
    shapes (termed super-points) and builts super graph on global super-points. Furthermore,
    this network adopts PointNet to embed these points and refine the embedding by
    Gated Recurrent Unit (GRU). Based on the basic architecture of PoinNet++, Li et
    al.  Li et al. ([2019b](#bib.bib83)) proposed Geometric Graph Convolution (TGCov),
    its filters defined as products of local point-wise features with local geometric
    connection features expressed by Gaussian weighted Taylor kernels. Feng et al. 
    Feng et al. ([2020](#bib.bib29)) constructed a local graph on neighborhood points
    searched along multi-directions and explore local features by a local attention-edge
    convolution (LAE-Conv). These features are imported into a point-wise spatial
    attention module to capture accurate and robust local geometric details. Lei et
    al. designs a fuzzy coefficient to times weight function, to enable the convolution
    weights robust.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在空间图类别中，ECC  Simonovsky 和 Komodakis ([2017](#bib.bib131)) 是应用空间图网络从点云中提取特征的先驱方法之一。它动态生成边缘条件滤波器以学习描述点与其邻域之间关系的边缘特征。基于
    PointNet 架构，DGCNN  Wang et al. ([2019b](#bib.bib155)) 在每个点的邻域上实现了动态边缘卷积，称为 EdgeConv。卷积通过简化的
    PointNet 进行近似。SPG  Landrieu 和 Simonovsky ([2018](#bib.bib74)) 将点云划分为多个简单几何形状（称为超级点）并在全局超级点上构建超级图。此外，该网络采用
    PointNet 嵌入这些点，并通过门控递归单元（GRU）来精炼嵌入。基于 PoinNet++ 的基本架构，Li et al.  Li et al. ([2019b](#bib.bib83))
    提出了几何图卷积（TGCov），其滤波器定义为局部逐点特征与局部几何连接特征的高斯加权泰勒核的乘积。Feng et al.  Feng et al. ([2020](#bib.bib29))
    在沿多方向搜索的邻域点上构建了一个局部图，并通过局部注意力边缘卷积（LAE-Conv）探索局部特征。这些特征被导入到逐点空间注意力模块中，以捕捉准确且鲁棒的局部几何细节。Lei
    et al. 设计了一个模糊系数来乘以权重函数，以增强卷积权重的鲁棒性。
- en: Continuous graph convolution also incurs a high computational cost and generally
    suffer from the vanishing gradient problem. Inspired by the separable convolution
    strategy in Xception  Chollet ([2017](#bib.bib15)) that significantly reduces
    parameters and computation burden, HDGCN  Liang et al. ([2019a](#bib.bib87)) designed
    a DGConv that composes depth-wise graph convolution followed by a point-wise convolution,
    and add DGConv into the hierarchical structure to extract local and global features.
    DeepGCNs  Li et al. ([2019a](#bib.bib80)) borrows some concepts from 2D CNN such
    as residual connections between different layers (ResNet) to alleviate the vanishing
    gradient problem, and dilation mechanism to allow the GCN to go deeper. Lei et
    al.  Lei et al. ([2020](#bib.bib78)) propose a discrete spherical convolution
    kernel (SPH3D kernel) that consists of the spherical convolution learning depth-wise
    features and point-wise convolution learning point-wise features.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 连续图卷积也会产生较高的计算成本，并且通常会遇到梯度消失问题。受到 Xception  Chollet ([2017](#bib.bib15)) 中可分离卷积策略的启发，该策略显著减少了参数和计算负担，HDGCN 
    Liang et al. ([2019a](#bib.bib87)) 设计了一个 DGConv，结合了深度图卷积和逐点卷积，并将 DGConv 添加到分层结构中以提取局部和全局特征。DeepGCNs 
    Li et al. ([2019a](#bib.bib80)) 借鉴了 2D CNN 的一些概念，如层间残差连接（ResNet）来缓解梯度消失问题，并使用扩张机制使
    GCN 能够更深。Lei et al.  Lei et al. ([2020](#bib.bib78)) 提出了一个离散球面卷积核（SPH3D 核），该卷积核包括球面卷积用于学习深度特征和逐点卷积用于学习逐点特征。
- en: Tree structures such as KD-tree and Octree can be viewed as a special type of
    graph, allowing to share convolution layers depending on the tree splitting orientation.
    3DContextNet  Zeng and Gevers ([2018](#bib.bib188)) adopts a KD-tree structure
    to hierarchically represent points where the nodes of different tree layers represent
    local regions at different scales, and employs a simplified PointNet with a gating
    function on nodes to explore local features. However, their performance depends
    heavily on the randomization of the tree construction. Lei et al.  Lei et al.
    ([2019](#bib.bib77)) built an Octree based hierarchical structure on global points
    to guide the spherical convolution computation in per layer of the network. The
    spherical convolution kernel systematically partitions a 3D spherical region into
    multiple bins that specifies learnable parameters to weight the points falling
    within the corresponding bin.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: KD-tree 和 Octree 等树结构可以视为一种特殊类型的图，允许根据树分割方向共享卷积层。3DContextNet Zeng 和 Gevers
    ([2018](#bib.bib188)) 采用 KD-tree 结构以分层方式表示点，其中不同树层的节点代表不同尺度的局部区域，并采用具有门控功能的简化
    PointNet 来探索局部特征。然而，他们的性能严重依赖于树构建的随机化。Lei 等人 ([2019](#bib.bib77)) 在全局点上构建了基于 Octree
    的分层结构，以指导网络每层的球形卷积计算。球形卷积核将 3D 球形区域系统地分割成多个箱子，这些箱子指定了可学习的参数来加权落在对应箱子中的点。
- en: 3.4.4 Transformer Based
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4 基于 Transformer
- en: Attention mechanism has recently become popular for improving point cloud segmentation
    accuracy. Compared to point convolution, Transformer introduces the point features
    into the weight learning. For example, Ma et al.  Ma et al. ([2020](#bib.bib100))
    use the channel self-attention mechanism to learn independence between any two
    point-wise feature channels, and further define a Channel Graph where the channel
    maps are presented as nodes and the independencies are represented as graph edges.
    AGCN  Xie et al. ([2020b](#bib.bib171)) integrates attention mechanism with GCN
    for analyzing the relationships between local features of points and introduces
    a global point graph to compensate for the relative information of individual
    points. PointANSL Yan et al. ([2020](#bib.bib176)) use the general self-attention
    mechanism for group feature updating, and propose a adaptive sampling (AS) module
    to overcome the issues of FPS.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制最近已成为提高点云分割精度的热门方法。与点卷积相比，Transformer 将点特征引入权重学习。例如，Ma 等人 ([2020](#bib.bib100))
    使用通道自注意力机制学习任意两个点特征通道之间的独立性，并进一步定义了一个通道图，其中通道映射被表示为节点，独立性被表示为图边。AGCN Xie 等人 ([2020b](#bib.bib171))
    将注意力机制与 GCN 集成，用于分析点的局部特征之间的关系，并引入了全局点图来补偿单个点的相对信息。PointANSL Yan 等人 ([2020](#bib.bib176))
    使用通用自注意力机制进行组特征更新，并提出了一个自适应采样 (AS) 模块以克服 FPS 的问题。
- en: The Transformer model, which employs self-attention as a fundamental component,
    includes position encoding to capture the sequential order of input tokens. Position
    encoding is crucial to ensure that the model understands the relative positions
    of tokens within a sequence. Point Transformer Zhao et al. ([2021](#bib.bib194))
    introduce MLP-based position encoding into vector attention, and use a KNN-based
    downsampling module to decrease the point resolution. Follow up work, Point Transformer
    v2 Wu et al. ([2022a](#bib.bib162)) strengthens the position encoding mechanism
    by applying an additional encoding multiplier to the relation vector, and designs
    a partition-based pooling strategy to align the geometric information.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型采用自注意力作为基本组件，包括位置编码以捕捉输入标记的顺序。位置编码对于确保模型理解序列中标记的相对位置至关重要。Point
    Transformer Zhao 等人 ([2021](#bib.bib194)) 在向量注意力中引入了基于 MLP 的位置编码，并使用基于 KNN 的下采样模块来降低点的分辨率。后续工作
    Point Transformer v2 Wu 等人 ([2022a](#bib.bib162)) 通过对关系向量应用额外的编码乘法器来加强位置编码机制，并设计了基于分区的池化策略以对齐几何信息。
- en: Point Transformers are generally computationally expensive because the original
    self-attention module needs to generate a huge attention map. To address this
    problem, PatchFormer Zhang et al. ([2022](#bib.bib189)) calculates the attention
    map via low-rank approximation. Similarly, FastPointTransformer Park et al. ([2022](#bib.bib111))
    introduces a lightweight local self-attention module that learns continuous positional
    information while reducing the space complexity. Inspired by the success of window-based
    Transformer in the 2D domain, Stratified Transformer Lai et al. ([2022](#bib.bib73))
    designs a cubic window and samples distant points as keys, but in a sparser way,
    to expand the receptive field. Similarly, SphereFormer Lai et al. ([2023](#bib.bib72))
    designs radial window self-attention that partitions that space into several non-overlapping
    narrow and long windows for exploiting long-range dependencies.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 点变换器通常计算开销较大，因为原始的自注意力模块需要生成一个巨大的注意力图。为了解决这个问题，PatchFormer Zhang 等 ([2022](#bib.bib189))
    通过低秩近似计算注意力图。类似地，FastPointTransformer Park 等 ([2022](#bib.bib111)) 引入了一种轻量级的局部自注意力模块，该模块在减少空间复杂度的同时学习连续的位置信息。受到二维领域窗口型
    Transformer 成功的启发，Stratified Transformer Lai 等 ([2022](#bib.bib73)) 设计了一个立方体窗口，并以更稀疏的方式将远处的点作为键，以扩展感受野。类似地，SphereFormer
    Lai 等 ([2023](#bib.bib72)) 设计了径向窗口自注意力，将空间划分为几个不重叠的狭长窗口，以利用长程依赖性。
- en: 'Table 4: Summary of point based semantic segmentation methods with deep learning.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：基于点的深度学习语义分割方法总结。
- en: '| Type | Methods | Neighb. search | Feature abstraction | Coarsening | Contribution
    |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法 | 邻域搜索 | 特征抽象 | 粗化 | 贡献 |'
- en: '| MLP | PointNet Qi et al. ([2017a](#bib.bib115)) | None | MLP | None | Pioneering
    processing points directly |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| MLP | PointNet Qi 等 ([2017a](#bib.bib115)) | 无 | MLP | 无 | 开创性地直接处理点 |'
- en: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) | None | MLP | None | Two local
    definition+local/global pathway |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| G+RCU Engelmann 等 ([2018](#bib.bib26)) | 无 | MLP | 无 | 两种局部定义+局部/全局路径 |'
- en: '| ESC Engelmann et al. ([2017](#bib.bib24)) | None | MLP | None | MC/Grid Block
    for local defini.+RCUs for context exploit. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| ESC Engelmann 等 ([2017](#bib.bib24)) | 无 | MLP | 无 | 用于局部定义的MC/Grid Block+用于上下文利用的RCUs
    |'
- en: '| HRNN Ye et al. ([2018](#bib.bib180)) | None | MLP | None | 3P for local feature
    exploit..+HRNN for local context exploit. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| HRNN Ye 等 ([2018](#bib.bib180)) | 无 | MLP | 无 | 用于局部特征利用的3P+用于局部上下文利用的HRNN
    |'
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) | Ball/KNN | PointNet | FPS |
    Proposing hierarchical learning framework |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| PointNet++ Qi 等 ([2017b](#bib.bib116)) | 球体/KNN | PointNet | FPS | 提出的层次学习框架
    |'
- en: '| PointSIFT Jiang et al. ([2018](#bib.bib64)) | KNN | PointNet | FPS | PointSIFT
    module for local shape information |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| PointSIFT Jiang 等 ([2018](#bib.bib64)) | KNN | PointNet | FPS | 用于局部形状信息的PointSIFT模块
    |'
- en: '| PointWeb Zhao et al. ([2019a](#bib.bib193)) | KNN | PointNet | FPS | AFA
    for interactive feature exploitation |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb Zhao 等 ([2019a](#bib.bib193)) | KNN | PointNet | FPS | 用于交互特征利用的AFA
    |'
- en: '| Repsurf Ran et al. ([2022](#bib.bib120)) | KNN | PointNet | FPS | Local triangular
    orientation + local umbrella orientation |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Repsurf Ran 等 ([2022](#bib.bib120)) | KNN | PointNet | FPS | 局部三角形方向+局部伞形方向
    |'
- en: '| PointNeXt Qian et al. ([2022](#bib.bib118)) | KNN | InvResMLP | FPS | Next
    version of PointNet |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| PointNeXt Qian 等 ([2022](#bib.bib118)) | KNN | InvResMLP | FPS | PointNet
    的下一版本 |'
- en: '| Point Convolution | RSNet Huang et al. ([2018](#bib.bib54)) | None | 1x1
    Conv | None | LDM for local context exploitation |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| Point Convolution | RSNet Huang 等 ([2018](#bib.bib54)) | 无 | 1x1 卷积 | 无 |
    用于局部上下文利用的LDM |'
- en: '| DPC Engelmann et al. ([2020b](#bib.bib25)) | DKNN | PointConv | None | Dilated
    KNN for expanding the receptive field |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| DPC Engelmann 等 ([2020b](#bib.bib25)) | DKNN | PointConv | 无 | 扩展感受野的膨胀KNN
    |'
- en: '| PointWiseCNN Hua et al. ([2018](#bib.bib52)) | Grid | PWConv. | None | Novel
    point convolution |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| PointWiseCNN Hua 等 ([2018](#bib.bib52)) | 网格 | PWConv. | 无 | 新颖的点卷积 |'
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) | KD index | PCConv. | None | KD-tree
    index for neigh. search+novel point Conv. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| PCCN Wang 等 ([2018c](#bib.bib149)) | KD 索引 | PCConv. | 无 | KD-tree 索引用于邻域搜索+新颖的点卷积
    |'
- en: '| KPConv Thomas et al. ([2019](#bib.bib139)) | Ball | KPConv. | Grid sampling
    | Novel point convolution |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| KPConv Thomas 等 ([2019](#bib.bib139)) | 球体 | KPConv. | 网格采样 | 新颖的点卷积 |'
- en: '| FlexConv Groh et al. ([2018](#bib.bib33)) | KD index | flexConv. | IDISS
    | Novel point Conv.+flex-maxpooling without subsampling |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| FlexConv Groh 等 ([2018](#bib.bib33)) | KD 索引 | flexConv. | IDISS | 新颖的点卷积+不进行下采样的flex-maxpooling
    |'
- en: '| PointCNN Li et al. ([2018b](#bib.bib82)) | DKNN | $\chi$-Conv | FPS | Novel
    point convolution |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| PointCNN Li 等 ([2018b](#bib.bib82)) | DKNN | $\chi$-卷积 | FPS | 新颖的点卷积 |'
- en: '| MCC Hermosilla et al. ([2018](#bib.bib46)) | Ball | MCConv. | PDS | Novel
    coarsening layer+point convolution |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| MCC Hermosilla 等人 ([2018](#bib.bib46)) | 球形 | MC卷积 | PDS | 新型粗化层 + 点卷积 |'
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) | KNN | PointConv | FPS | Novel
    point convolution considering point density |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| PointConv Wu 等人 ([2019b](#bib.bib161)) | KNN | PointConv | FPS | 考虑点密度的新型点卷积
    |'
- en: '| A-CNN Komarichev et al. ([2019](#bib.bib69)) | DKNN | AConv | FPS | Novel
    neighborhood search+point convolution |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN Komarichev 等人 ([2019](#bib.bib69)) | DKNN | A卷积 | FPS | 新型邻域搜索 + 点卷积
    |'
- en: '| RandLA-Net Hu et al. ([2020](#bib.bib49)) | KNN | LocSE | RPS | LFAM with
    large receptive field and keeping geometric details |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| RandLA-Net Hu 等人 ([2020](#bib.bib49)) | KNN | LocSE | RPS | 大感受野的LFAM且保留几何细节
    |'
- en: '| PolarNet Zhang et al. ([2020](#bib.bib190)) | None | PointNet | PolarGrid
    | Novel local regions definition + RingConv |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| PolarNet Zhang 等人 ([2020](#bib.bib190)) | None | PointNet | 极坐标网格 | 新型局部区域定义
    + 环卷积 |'
- en: '| Graph Convolution | DGCNN Wang et al. ([2019b](#bib.bib155)) | KNN | EdgeConv
    | None | Novel graph convolution + updating graph |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 图卷积 | DGCNN Wang 等人 ([2019b](#bib.bib155)) | KNN | 边卷积 | None | 新型图卷积 + 更新图
    |'
- en: '| SPG Landrieu and Simonovsky ([2018](#bib.bib74)) | partition | PointNet |
    None | Superpoint graph + parsing large-scale scene |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| SPG Landrieu 和 Simonovsky ([2018](#bib.bib74)) | 分区 | PointNet | None | 超点图
    + 大规模场景解析 |'
- en: '| DeepGCNs Li et al. ([2019a](#bib.bib80)) | DKNN | DGConv | RPS | Adapting
    residual connections between layers |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| DeepGCNs Li 等人 ([2019a](#bib.bib80)) | DKNN | DGConv | RPS | 适应层间残差连接 |'
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) | Ball | SPH3D-GConv | FPS | Novel
    graph convolution + pooling + uppooling |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| SPH3D-GCN Lei 等人 ([2020](#bib.bib78)) | 球形 | SPH3D-G卷积 | FPS | 新型图卷积 + 池化
    + 上采样 |'
- en: '| LS-GCN Wang et al. ([2018a](#bib.bib143)) | KNN | Spec.Conv. | FPS | Local
    spectral graph + Novel graph convolution |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LS-GCN Wang 等人 ([2018a](#bib.bib143)) | KNN | 谱卷积 | FPS | 局部谱图 + 新型图卷积 |'
- en: '| PAN Feng et al. ([2020](#bib.bib29)) | Multi-direct. | LAE-Conv | PFS | Point-wise
    spatial attention+local graph Conv. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| PAN Feng 等人 ([2020](#bib.bib29)) | 多方向 | LAE-卷积 | PFS | 点位空间注意力 + 局部图卷积 |'
- en: '| TGNet Li et al. ([2019b](#bib.bib83)) | Ball | TGConv | PFS | Novel graph
    Conv.+multi-scale features explo. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| TGNet Li 等人 ([2019b](#bib.bib83)) | 球形 | TG卷积 | PFS | 新型图卷积 + 多尺度特征探索 |'
- en: '| HDGCN Liang et al. ([2019a](#bib.bib87)) | KNN | DGConv | FPS | Depthwise
    graph Conv. + Pointwise Conv. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| HDGCN Liang 等人 ([2019a](#bib.bib87)) | KNN | DGConv | FPS | 深度图卷积 + 点卷积 |'
- en: '| 3DCon.Net Zeng and Gevers ([2018](#bib.bib188)) | KNN | PointNet | Tree layer
    | KD tree structure |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 3DCon.Net Zeng 和 Gevers ([2018](#bib.bib188)) | KNN | PointNet | 树层 | KD树结构
    |'
- en: '| $\psi$-CNN Lei et al. ([2019](#bib.bib77)) | Octree neig. | $\psi$-Conv |
    Tree layer | Octree structure+ Novel graph convolution |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| $\psi$-CNN Lei 等人 ([2019](#bib.bib77)) | 八叉树邻域 | $\psi$-卷积 | 树层 | 八叉树结构 +
    新型图卷积 |'
- en: '| Point Transformer | PGCRNet Ma et al. ([2020](#bib.bib100)) | None | Conv1D
    | None | PointGCR to model context dependencies |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 点变换器 | PGCRNet Ma 等人 ([2020](#bib.bib100)) | None | Conv1D | None | 用于建模上下文依赖的PointGCR
    |'
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) | KNN | MLP | None | Point attention
    layer for aggregating local features |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| AGCN Xie 等人 ([2020b](#bib.bib171)) | KNN | MLP | None | 用于聚合局部特征的点注意力层 |'
- en: '| PointANSL Yan et al. ([2020](#bib.bib176)) | KNN | local-nonlocal module
    | AS | Local-nonlocal module + adaptive sampling |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| PointANSL Yan 等人 ([2020](#bib.bib176)) | KNN | 局部-非局部模块 | AS | 局部-非局部模块 +
    自适应采样 |'
- en: '| Point Transformer Zhao et al. ([2021](#bib.bib194)) | KNN | Point Transformer
    | Pooling | MLP-based relative position encoding + vector attention |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 点变换器 Zhao 等人 ([2021](#bib.bib194)) | KNN | 点变换器 | 池化 | 基于MLP的相对位置编码 + 向量注意力
    |'
- en: '| Point Transformer v2 Wu et al. ([2022a](#bib.bib162)) | Grid partition |
    PointTransformerv2 | pooling | Novel position encoding + grid Pooling |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Point Transformer v2 Wu 等人 ([2022a](#bib.bib162)) | 网格分区 | 点变换器v2 | 池化 |
    新型位置编码 + 网格池化 |'
- en: '| PatchFormer Zhang et al. ([2022](#bib.bib189)) | Boxes partition | Patch
    Transformer | DWConv | First linear attention + Lightweight multi-scale Transformer
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| PatchFormer Zhang 等人 ([2022](#bib.bib189)) | 盒子分区 | Patch 变换器 | DW卷积 | 首次线性注意力
    + 轻量级多尺度变换器 |'
- en: '| Fast Point Transformer Park et al. ([2022](#bib.bib111)) | Voxel partition
    | Fast point Transformer | Voxel-based sampl. | Lightweight local self-attention
    + novel position encoding |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 快速点变换器 Park 等人 ([2022](#bib.bib111)) | 体素分区 | 快速点变换器 | 体素基础采样 | 轻量级局部自注意力
    + 新型位置编码 |'
- en: '| Stratified Transformer Lai et al. ([2022](#bib.bib73)) | Voxel partition
    | Stratified Transformer | PFS | Contextual relative position encoding |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 分层变换器 Lai 等人 ([2022](#bib.bib73)) | 体素分区 | 分层变换器 | PFS | 上下文相对位置编码 |'
- en: '| SphereFormer Lai et al. ([2023](#bib.bib72)) | Voxel partition | Sphereformer
    + cubicformer | Maxpooling | Novel spherical window for LIDAR points |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| SphereFormer Lai 等人 ([2023](#bib.bib72)) | 体素划分 | Sphereformer + cubicformer
    | 最大池化 | 针对 LIDAR 点的新型球面窗口 |'
- en: 3.5 Other Representation Based
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 其他基于表示的方法
- en: Some methods transform the original point cloud to representations other than
    projected images, voxels and points. Examples of such representations include
    *tangent images* Tatarchenko et al. ([2018](#bib.bib137)) and *lattice* Su et al.
    ([2018](#bib.bib134)), Rosu et al. ([2019](#bib.bib124)). In the former case,
    Tatarchenko et al. Tatarchenko et al. ([2018](#bib.bib137)) project local surfaces
    around each-point to a series of 2D tangent images and develop a tangent convolution
    based U-Net to extract features. In the latter case, SPLATNet Su et al. ([2018](#bib.bib134))
    adapts the bilateral convolution layers (BCLs) proposed by Jampani et al. Jampani
    et al. ([2016](#bib.bib58)) to smoothly map disordered points onto a sparse lattice.
    Similarly, LatticeNet Rosu et al. ([2019](#bib.bib124)) uses a hybrid architecture
    that combines PointNet, which obtains low-level features, with sparse 3D convolution,
    which explores global context features. These features are embedded into a sparse
    lattice that allows the application of standard 2D convolutions.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法将原始点云转换为其他表示方式，而不是投影图像、体素和点。这些表示方式的例子包括*切线图像* Tatarchenko 等人 ([2018](#bib.bib137))
    和*格点* Su 等人 ([2018](#bib.bib134))，Rosu 等人 ([2019](#bib.bib124))。在前一种情况下，Tatarchenko
    等人 ([2018](#bib.bib137)) 将每个点周围的局部表面投影到一系列 2D 切线图像中，并开发了基于切线卷积的 U-Net 来提取特征。在后一种情况下，SPLATNet
    Su 等人 ([2018](#bib.bib134)) 采用了 Jampani 等人 ([2016](#bib.bib58)) 提出的双边卷积层（BCLs），以平滑地将无序点映射到稀疏格点上。类似地，LatticeNet
    Rosu 等人 ([2019](#bib.bib124)) 使用了结合了获得低层次特征的 PointNet 和探索全局上下文特征的稀疏 3D 卷积的混合架构。这些特征被嵌入到一个稀疏格点中，从而允许应用标准的
    2D 卷积。
- en: 'Although the above methods have achieved significant progress in 3D semantic
    segmentation, each has its own drawbacks. For instance, multi-view images have
    more spectral information like color/intensity but less geometric information
    of the scene. On the other hand, voxels have more geometric information but less
    spectral information. To get the best of both worlds, some methods adopt *hybrid
    representations* as input to learn comprehensive features of a scene. Dai et al.
    Dai and Nießner ([2018](#bib.bib19)) map 2D semantic features obtained by multi-view
    networks into 3D grids of scene. These pipelines make 3D grids attach rich 2D
    semantic as well as 3D geometric information so that the scene can get better
    segmentation by a 3D CNN. Similarly, Hung et al. Chiang et al. ([2019](#bib.bib14))
    back-project 2D multi-view image features on to the 3D point cloud space and use
    a unified network to extract local details and global context from sub-volumes
    and the global scene respectively. Liu et al. Liu et al. ([2019b](#bib.bib97))
    argue that voxel-based and point-based NN are computationally inefficient in high-resolution
    and data structuring respectively. To overcome these challenges, they propose
    Point-Voxel CNN (PVCNN) that represents the 3D input data as point clouds to take
    advantage of the sparsity to lower the memory footprint, and leverage the voxel-based
    convolution to obtain a contiguous memory access pattern. Jaritz et al. Jaritz
    et al. ([2019](#bib.bib59)) proposed MVPNet that collect 2D multi-view dense image
    features into 3D sparse point clouds and then use a unified network to fuse the
    semantic and geometric features. Also, Meyer et al. Meyer et al. ([2019](#bib.bib104))
    fuse 2D image and point clouds to address 3D object detection and semantic segmentation
    by a unifying network. BPNet Hu et al. ([2021](#bib.bib50)) consists of 2D and
    3D sub-networks with symmetric architectures, connected through a bidirectional
    projection module (BPM). This allows the interaction of complementary information
    from both visual domains at multiple architectural levels, leading to improved
    scene recognition by leveraging the advantages of both 2D and 3D information.
    The other representations based semantic segmentation methods are summarized in
    Table [3](#S3.T3 "Table 3 ‣ 3.2.2 Spherical Images Based Segmentation ‣ 3.2 Projected
    Images Based Segmentation ‣ 3 3D Semantic segmentation ‣ Deep Learning Based 3D
    Segmentation: A Survey").'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述方法在 3D 语义分割方面取得了显著进展，但各自都有缺陷。例如，多视角图像具有更多的光谱信息，如颜色/强度，但场景的几何信息较少。另一方面，体素具有更多的几何信息但光谱信息较少。为了兼顾这两方面的优点，一些方法采用*混合表示*作为输入，以学习场景的全面特征。Dai
    等人 ([2018](#bib.bib19)) 将通过多视角网络获得的 2D 语义特征映射到场景的 3D 网格中。这些管道使得 3D 网格附加了丰富的 2D
    语义以及 3D 几何信息，从而通过 3D CNN 实现更好的场景分割。同样，Hung 等人 ([2019](#bib.bib14)) 将 2D 多视角图像特征反投影到
    3D 点云空间，并使用统一网络从子体积和全局场景中提取局部细节和全局上下文。Liu 等人 ([2019b](#bib.bib97)) 认为，基于体素和基于点的神经网络在高分辨率和数据结构化方面的计算效率较低。为了克服这些挑战，他们提出了
    Point-Voxel CNN (PVCNN)，将 3D 输入数据表示为点云，利用稀疏性降低内存占用，并利用基于体素的卷积获得连续的内存访问模式。Jaritz
    等人 ([2019](#bib.bib59)) 提出了 MVPNet，将 2D 多视角密集图像特征收集到 3D 稀疏点云中，然后使用统一网络融合语义和几何特征。此外，Meyer
    等人 ([2019](#bib.bib104)) 融合 2D 图像和点云，通过统一网络解决 3D 对象检测和语义分割问题。BPNet Hu 等人 ([2021](#bib.bib50))
    包括具有对称结构的 2D 和 3D 子网络，通过双向投影模块 (BPM) 连接。这允许在多个架构层次上交互来自两个视觉领域的互补信息，从而利用 2D 和 3D
    信息的优势提高场景识别能力。其他基于表示的语义分割方法在表 [3](#S3.T3 "表 3 ‣ 3.2.2 球面图像基础分割 ‣ 3.2 投影图像基础分割
    ‣ 3 3D 语义分割 ‣ 基于深度学习的 3D 分割：一项调查") 中进行了总结。
- en: 4 3D Instance Segmentation
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 3D 实例分割
- en: '3D instance segmentation methods additionally distinguish between different
    instances of the same class. Being a more informative task for scene understanding,
    3D instance segmentation is receiving increased interest from the research community.
    3D instance segmentation methods are roughly divided into two directions: *proposal-based*
    and *proposal-free*.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 实例分割方法还区分了同一类别的不同实例。作为一种更具信息性的场景理解任务，3D 实例分割正受到研究界的日益关注。3D 实例分割方法大致分为两种方向：*基于提议的*和*无提议的*。
- en: '![Refer to caption](img/ac3e01783ced0e190ace2f88a82545f9.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ac3e01783ced0e190ace2f88a82545f9.png)'
- en: 'Figure 7: Illustration of two different approaches for 3D instance segmentation.
    Top: proposal-based framework. Bottom: proposal-free framework.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：展示了三维实例分割的两种不同方法。顶部：基于提议的框架。底部：无提议框架。
- en: 4.1 Proposal Based
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基于提议的方法
- en: 'Proposal-based methods first predict object proposals and then refine them
    to generate final instance masks (see Figure [7](#S4.F7 "Figure 7 ‣ 4 3D Instance
    Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey")), breaking down
    the task into two main challenges. Hence, from the proposal generation point of
    view, these methods can be grouped into *detection-based* and *detection-free*
    methods.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '基于提议的方法首先预测对象提议，然后对其进行细化以生成最终实例掩码（见图[7](#S4.F7 "Figure 7 ‣ 4 3D Instance Segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey")），将任务分解为两个主要挑战。因此，从提议生成的角度来看，这些方法可以分为*检测基于*和*检测无*方法。'
- en: Detection-based methods sometimes define object proposals as a 3D bounding box
    regression problem. 3D-SIS Hou et al. ([2019](#bib.bib48)) incorporates high-resolution
    RGB images with voxels, based on the pose alignment of the 3D reconstruction,
    and jointly learns color and geometric features by a 3D detection backbone to
    predict 3D bounding box proposals. In these proposals, a 3D mask backbone predicts
    the final instance masks. Similarly, GPSN Yi et al. ([2019](#bib.bib183)) introduces
    a 3D object proposal network termed Generative Shape Proposal Network (GPSN) that
    reconstructs object shapes from shape noisy observations to enforce geometric
    understanding. GPSN is further embedded into a 3D instance segmentation network
    named Region-based PointNet (R-PointNet) to reject, receive and refine proposals.
    Training of these networks needs to be performed step-by-step and the object proposal
    refinement requires expensive suppression operation. To this end, Yang et al.
    Yang et al. ([2019](#bib.bib177)) introduced a novel end-to-end network named
    3D-BoNet to directly learn a fixed number of 3D bounding boxes without any rejection,
    and then estimate an instance mask in each bounding box.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 检测基于的方法有时将对象提议定义为三维边界框回归问题。3D-SIS Hou et al. ([2019](#bib.bib48))结合了高分辨率RGB图像和体素，基于三维重建的姿态对齐，通过三维检测主干网络共同学习颜色和几何特征，以预测三维边界框提议。在这些提议中，三维掩码主干网络预测最终的实例掩码。类似地，GPSN
    Yi et al. ([2019](#bib.bib183))引入了一种称为生成形状提议网络（GPSN）的三维对象提议网络，该网络从形状噪声观测中重建对象形状以加强几何理解。GPSN进一步嵌入到一种名为基于区域的PointNet（R-PointNet）的三维实例分割网络中，以拒绝、接收和细化提议。这些网络的训练需要逐步进行，并且对象提议的细化需要昂贵的抑制操作。为此，Yang
    et al. Yang et al. ([2019](#bib.bib177)) 引入了一种新颖的端到端网络，称为3D-BoNet，直接学习固定数量的三维边界框而无需任何拒绝，然后在每个边界框中估计实例掩码。
- en: Detection-free methods include SGPN Wang et al. ([2018d](#bib.bib151)) which
    assumes that the points belonging to the same object instance should have very
    similar features. Hence, it learns a similarity matrix to predict proposals. The
    proposals are pruned by confidence scores of the points to generate highly credible
    instance proposals. However, this simple distance similarity metric learning is
    not informative and is unable to segment adjacent objects of the same class. To
    this end, 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) learns object proposals
    from sampled and grouped point features that vote for the same object center,
    and then consolidates the proposal features using a graph convolutional network
    enabling higher-level interactions between proposals which result in refined proposal
    features. AS-Net Jiang et al. ([2020a](#bib.bib61)) uses an assignment module
    to assign proposal candidates and then eliminates redundant candidates by a suppression
    network. SoftGroup Vu et al. ([2022](#bib.bib142)) proposes top-down refinement
    to refine the instance proposal. SSTNet Liang et al. ([2021](#bib.bib86)) proposes
    an end-to-end solution of Semantic Superpoint Tree Network (SSTNet) to generate
    object instance proposals from scene points. A key contribution in SSTNet is an
    intermediate semantic superpoint tree (SST), which is constructed based on the
    learned semantic features of superpoints. The tree is traversed and split at intermediate
    nodes to generate proposals of object instances.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 无检测方法包括SGPN Wang等（[2018d](#bib.bib151)），该方法假设属于同一物体实例的点应具有非常相似的特征。因此，它学习一个相似性矩阵以预测提案。提案通过点的置信度分数进行修剪，以生成高度可信的实例提案。然而，这种简单的距离相似性度量学习并不具有信息量，无法分割相同类别的邻近物体。为此，3D-MPA
    Engelmann等（[2020a](#bib.bib23)）从采样和分组的点特征中学习物体提案，这些特征投票给同一物体中心，然后使用图卷积网络整合提案特征，从而实现提案之间的高级交互，生成更精细的提案特征。AS-Net
    Jiang等（[2020a](#bib.bib61)）使用分配模块来分配提案候选，然后通过抑制网络消除冗余候选。SoftGroup Vu等（[2022](#bib.bib142)）提出了自上而下的细化方法来优化实例提案。SSTNet
    Liang等（[2021](#bib.bib86)）提出了一种端到端的解决方案，即语义超级点树网络（SSTNet），用于从场景点生成物体实例提案。SSTNet的一个关键贡献是中间语义超级点树（SST），它是基于学习到的超级点语义特征构建的。该树在中间节点处遍历和拆分，以生成物体实例提案。
- en: 4.2 Proposal Free
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 无提案
- en: 'Proposal-free methods learn feature embedding for each point and then apply
    clustering to obtain defintive 3D instance labels (see Figure [7](#S4.F7 "Figure
    7 ‣ 4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey"))
    breaking down the task into two main challenges. From the embedding learning point
    of a view, these methods can be roughly subdivided into four categories: *2D embedding
    based* *multi-tasks learning*, *clustering based*, and *dynamic convolution based*.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 无提案方法为每个点学习特征嵌入，然后应用聚类以获得明确的3D实例标签（参见图[7](#S4.F7 "图 7 ‣ 4 3D 实例分割 ‣ 基于深度学习的3D分割：综述")），将任务分解为两个主要挑战。从嵌入学习的角度来看，这些方法大致可以分为四类：*基于2D嵌入*的*多任务学习*、*基于聚类*的方法，以及*基于动态卷积*的方法。
- en: 2D embedding based: An example of these methods is the 3D-BEVIS Elich et al.
    ([2019](#bib.bib21)) that learns 2D global instance embedding with a bird’s-eye-view
    of the full scene. It then propagates the learned embedding onto point clouds
    by DGCNN Wang et al. ([2019b](#bib.bib155)). Another example is PanopticFusion
    Narita et al. ([2019](#bib.bib108)) which predicts pixel-wise instance labels
    by 2D instance segmentation network Mask R-CNN He et al. ([2017a](#bib.bib42))
    for RGB frames and integrates the learned labels into 3D volumes.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 基于2D嵌入的方法：这些方法的一个例子是3D-BEVIS Elich等（[2019](#bib.bib21)），它通过全场景的鸟瞰图学习2D全局实例嵌入。然后，它通过DGCNN
    Wang等（[2019b](#bib.bib155)）将学习到的嵌入传播到点云上。另一个例子是PanopticFusion Narita等（[2019](#bib.bib108)），它通过2D实例分割网络Mask
    R-CNN He等（[2017a](#bib.bib42)）对RGB帧进行像素级实例标签预测，并将学习到的标签整合到3D体积中。
- en: Multi-tasks learning: 3D semantic segmentation and 3D instance segmentation
    can influence each other. For example objects with different classes must be different
    instances, and objects with the same instance label must be the same class. Based
    on this, ASIS Wang et al. ([2019a](#bib.bib152)) designs an encoder-decoder network,
    termed ASIS, to learn semantic-aware instance embeddings for boosting the performance
    of the two tasks. Similarly, JSIS3D Pham et al. ([2019b](#bib.bib114)) uses a
    unified network namely MT-PNet to predict the semantic labels of points and embedding
    the points into high-dimensional feature vectors, and further propose a MV-CRF
    to jointly optimize object classes and instance labels. Similarly Liu et al. Liu
    and Furukawa ([2019](#bib.bib90)) and 3D-GEL Liang et al. ([2019b](#bib.bib88))
    adopt SSCN to generate semantic predictions and instance embeddings simultaneously,
    then use two GCNs to refine the instance labels. OccuSeg Han et al. ([2020](#bib.bib39))
    uses a multi-task learning network to produce both occupancy signal and spatial
    embedding. The occupancy signal represents the number of voxel occupied by per
    voxel.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习：3D语义分割和3D实例分割可以相互影响。例如，不同类别的对象必须是不同的实例，而具有相同实例标签的对象必须是相同类别。基于此，ASIS Wang等
    ([2019a](#bib.bib152)) 设计了一个称为ASIS的编码器-解码器网络，用于学习语义感知的实例嵌入，以提高这两项任务的性能。类似地，JSIS3D
    Pham等 ([2019b](#bib.bib114)) 使用了一个统一的网络，即MT-PNet，来预测点的语义标签并将点嵌入到高维特征向量中，并进一步提出了一个MV-CRF来联合优化对象类别和实例标签。类似地，Liu等
    Liu和Furukawa ([2019](#bib.bib90)) 和3D-GEL Liang等 ([2019b](#bib.bib88)) 采用SSCN来同时生成语义预测和实例嵌入，然后使用两个GCN来细化实例标签。OccuSeg
    Han等 ([2020](#bib.bib39)) 使用了一个多任务学习网络来产生占用信号和空间嵌入。占用信号表示每个体素占据的体素数量。
- en: 'Clustering based: methods like MASC Liu and Furukawa ([2019](#bib.bib90)) rely
    on high performance of the SSCN Graham et al. ([2018](#bib.bib32)) to predict
    the similarity embedding between neighboring points at multiple scales and semantic
    topology. A simple yet effective clustering Liu et al. ([2018c](#bib.bib96)) is
    adapted to segment points into instances based on the two types of learned embeddings.
    MTML Lahoud et al. ([2019](#bib.bib71)) learns two sets of feature embeddings,
    including the feature embedding unique to every instance and the direction embedding
    that orients the instance center, which provides a stronger grouping force. Similarly,
    PointGroup Jiang et al. ([2020b](#bib.bib63)) groups points into different clusters
    based on the original coordinate embedding space and the shifted coordinate embedding
    space. In addition, the proposed ScoreNet guides the proper cluster selection.
    The above methods usually group points according to point-level embeddings, without
    the instance-level corrections. HAIS Chen et al. ([2021](#bib.bib10)) introduce
    the set aggregation and intra-instance prediction to refine the instance at the
    object level.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 基于聚类的方法：像MASC Liu和Furukawa ([2019](#bib.bib90)) 这样的方法依赖于SSCN Graham等 ([2018](#bib.bib32))
    的高性能，来预测多尺度和语义拓扑中相邻点之间的相似性嵌入。一个简单但有效的聚类方法 Liu等 ([2018c](#bib.bib96)) 被用于根据两种类型的学习嵌入将点分割为实例。MTML
    Lahoud等 ([2019](#bib.bib71)) 学习了两组特征嵌入，包括每个实例独特的特征嵌入和指向实例中心的方向嵌入，这提供了更强的分组力量。类似地，PointGroup
    Jiang等 ([2020b](#bib.bib63)) 基于原始坐标嵌入空间和偏移坐标嵌入空间将点分组为不同的簇。此外，提出的ScoreNet指导了正确的聚类选择。这些方法通常根据点级嵌入对点进行分组，而没有实例级的校正。HAIS
    Chen等 ([2021](#bib.bib10)) 引入了集合聚合和实例内部预测来在对象级别上细化实例。
- en: 'Dynamic convolution based: These methods overcome the limitations of clustering
    based methods by generating kernels and then using them to convolve with the point
    features to generate instance masks. Dyco3D He et al. ([2021](#bib.bib43)) adopts
    the clustering algorithm to generate a kernel for convolution. Similarly, PointInst3D
    He et al. ([2022](#bib.bib44)) uses FPS to generate kernels. DKNet Wu et al. ([2022b](#bib.bib163))
    introduces candidate mining and candidate aggregation to generate more instance
    kernels. Moreover, ISBNet Ngo et al. ([2023](#bib.bib110)) proposes a new instance
    encoder combining instance-aware PFS with a point aggregation layer to generate
    kernels to replace clustering in DyCo3D. 3D instance segmentation methods are
    summarized in Table [5](#S4.T5 "Table 5 ‣ 4.2 Proposal Free ‣ 4 3D Instance Segmentation
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 基于动态卷积：这些方法通过生成核来克服基于聚类的方法的局限性，然后使用这些核与点特征卷积以生成实例掩码。Dyco3D He 等人 ([2021](#bib.bib43))
    采用聚类算法生成用于卷积的核。类似地，PointInst3D He 等人 ([2022](#bib.bib44)) 使用FPS生成核。DKNet Wu 等人
    ([2022b](#bib.bib163)) 引入候选挖掘和候选聚合，以生成更多实例核。此外，ISBNet Ngo 等人 ([2023](#bib.bib110))
    提出了结合实例感知PFS和点聚合层的新实例编码器，以生成替代DyCo3D中聚类的核。3D实例分割方法总结见表 [5](#S4.T5 "表 5 ‣ 4.2 提议自由
    ‣ 4 3D 实例分割 ‣ 基于深度学习的3D分割：综述")。
- en: 'Table 5: Summary of 3D instance segmentation methods with deep learning. M←multi-view
    image; Me←mesh;V←voxel; P←point clouds.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：基于深度学习的3D实例分割方法总结。M←多视角图像；Me←网格；V←体素；P←点云。
- en: '| Type | Methods | Input | Propo./Embed. Prediction | Refining/Grouping | Contribution
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: 类型 | 方法 | 输入 | 提议/嵌入预测 | 精炼/分组 | 贡献 |
- en: '| proposal based | GSPN Yi et al. ([2019](#bib.bib183)) | P | GSPN | R-PointNet
    | New proposal generation methods |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| proposal based | GSPN Yi 等人 ([2019](#bib.bib183)) | P | GSPN | R-PointNet
    | 新的提议生成方法 |'
- en: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | M+V | 3D-RPN+3D-RoI | 3DFCN | Learning
    bounding box on geometry and RGB |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 3D-SIS Hou 等人 ([2019](#bib.bib48)) | M+V | 3D-RPN+3D-RoI | 3DFCN | 学习几何和RGB上的边界框
    |'
- en: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | P | Bounding box regression
    | Point mask prediction | Directly regressing 3D bounding box |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 3D-BoNet Yang 等人 ([2019](#bib.bib177)) | P | 边界框回归 | 点掩码预测 | 直接回归3D边界框 |'
- en: '| SGPN Wang et al. ([2018d](#bib.bib151)) | P | SM + SCM + PN | Non-Maximum
    suppression | New group proposal |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| SGPN Wang 等人 ([2018d](#bib.bib151)) | P | SM + SCM + PN | 非最大抑制 | 新的组提议 |'
- en: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | p | SSCNet | Graph ConvNet
    | Multi proposal aggregation strategy |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 3D-MPA Engelmann 等人 ([2020a](#bib.bib23)) | p | SSCNet | 图卷积网络 | 多提议聚合策略
    |'
- en: '| AS-Net Jiang et al. ([2020a](#bib.bib61)) | p | Four branches with MLPs |
    Candidate proposal suppression | Novel Algorithm mapping labels to candidates
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| AS-Net Jiang 等人 ([2020a](#bib.bib61)) | p | 四个MLP分支 | 候选提议抑制 | 新算法将标签映射到候选项
    |'
- en: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | P | Soft-grouping module | top-down
    refinment | Novel clustering algorithm based on dual coordinate sets |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| SoftGroup Vu 等人 ([2022](#bib.bib142)) | P | 软分组模块 | 自顶向下的精炼 | 基于双坐标集的新聚类算法
    |'
- en: '| SSTNet Liang et al. ([2021](#bib.bib86)) | p | Tree traversal + splitting
    | CliqueNet | Constructing the superpoint tree for instance segmentation |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| SSTNet Liang 等人 ([2021](#bib.bib86)) | p | 树遍历 + 分裂 | CliqueNet | 为实例分割构建超级点树
    |'
- en: '| proposal free | 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | M | U-Net/FCN
    + 3D prop. | Mean-shift clustering | Joint 2D-3D feature |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| proposal free | 3D-BEVIS Elich 等人 ([2019](#bib.bib21)) | M | U-Net/FCN +
    3D prop. | 均值漂移聚类 | 联合2D-3D特征 |'
- en: '| PanopticFus Narita et al. ([2019](#bib.bib108)) | M | PSPNet/Mask R-CNN |
    FC-CRF | Coopering with semantic mapping |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| PanopticFus Narita 等人 ([2019](#bib.bib108)) | M | PSPNet/Mask R-CNN | FC-CRF
    | 与语义映射合作 |'
- en: '| ASIS Wang et al. ([2019a](#bib.bib152)) | P | 1 encoder+ 2 decoders | ASIS
    module | Simultaneously performing sem./ins. segmentation tasks |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| ASIS Wang 等人 ([2019a](#bib.bib152)) | P | 1 编码器 + 2 解码器 | ASIS模块 | 同时执行语义/实例分割任务
    |'
- en: '| JSIS3D Pham et al. ([2019b](#bib.bib114)) | P | MT-PNet | MV-CRF | Simultaneously
    performing sem./ins. segmentation tasks |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| JSIS3D Pham 等人 ([2019b](#bib.bib114)) | P | MT-PNet | MV-CRF | 同时执行语义/实例分割任务
    |'
- en: '| 3D-GEL Liang et al. ([2019b](#bib.bib88)) | P | SSCNet | GCN | Structure-aware
    loss function + attention-based GCN |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 3D-GEL Liang 等人 ([2019b](#bib.bib88)) | P | SSCNet | GCN | 结构感知损失函数 + 基于注意力的GCN
    |'
- en: '| OccuSeg Han et al. ([2020](#bib.bib39)) | P | 3D-UNet | Graph-based clustering
    | Proposing a novel occupancy signal |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| OccuSeg Han 等人 ([2020](#bib.bib39)) | P | 3D-UNet | 基于图的聚类 | 提出一种新颖的占用信号
    |'
- en: '| MASC Liu and Furukawa ([2019](#bib.bib90)) | Me | U-Net with SSConv | Clustering
    algorithm | Novel clustering based on affinity and mesh topology |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| MASC Liu和Furukawa ([2019](#bib.bib90)) | Me | 带SSConv的U-Net | 聚类算法 | 基于亲和力和网格拓扑的新型聚类
    |'
- en: '| MTML Lahoud et al. ([2019](#bib.bib71)) | V | SSCNet | Mean-shift clustering
    | Multi-task learning |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| MTML Lahoud et al. ([2019](#bib.bib71)) | V | SSCNet | 均值漂移聚类 | 多任务学习 |'
- en: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | P | U-Net with SSConv | Point
    clustering + ScoreNet | Novel clustering algorithm based on dual coordinate sets
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | P | 带SSConv的U-Net | 点聚类 +
    ScoreNet | 基于双坐标集的新型聚类算法 |'
- en: '| HAIS Chen et al. ([2021](#bib.bib10)) | P | 3D U-Net | Set aggregation |
    Hierarchical aggregation for fine-grained predictions |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| HAIS Chen et al. ([2021](#bib.bib10)) | P | 3D U-Net | 集合聚合 | 用于精细预测的分层聚合
    |'
- en: '| Dyco3D He et al. ([2021](#bib.bib43)) | P | 3D U-Net | Dynamic conv. | Generating
    kernel by clustering for convolution |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Dyco3D He et al. ([2021](#bib.bib43)) | P | 3D U-Net | 动态卷积 | 通过聚类生成卷积核 |'
- en: '| PointInst3D He et al. ([2022](#bib.bib44)) | P | 3D U-Net | MLP | Generating
    kernel by FPS |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| PointInst3D He et al. ([2022](#bib.bib44)) | P | 3D U-Net | MLP | 通过FPS生成核
    |'
- en: '| DKNet Wu et al. ([2022b](#bib.bib163)) | P | 3D U-Net | MLP | Generating
    kernel by candidate mining and aggregation |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| DKNet Wu et al. ([2022b](#bib.bib163)) | P | 3D U-Net | MLP | 通过候选挖掘和聚合生成核
    |'
- en: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | P | 3D U-Net | Box-aware dynamic
    conv | Generating kernel by instance aware FPS and point aggrega. |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | P | 3D U-Net | Box-aware动态卷积 |
    通过实例感知FPS和点聚合生成核 |'
- en: 5 3D Part Segmentation
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 3D部件分割
- en: '3D part segmentation is the next finer level, after instance segmentation,
    where the aim is to label different parts of an instance. The pipeline of part
    segmentation is quite similar to that of semantic segmentation except that the
    labels are now for individual parts. Therefore, some existing 3D semantic segmentation
    networks Meng et al. ([2019](#bib.bib103)), Graham et al. ([2018](#bib.bib32)),
    Qi et al. ([2017a](#bib.bib115)), Qi et al. ([2017b](#bib.bib116)), Zeng and Gevers
    ([2018](#bib.bib188)), Huang et al. ([2018](#bib.bib54)), Thomas et al. ([2019](#bib.bib139)),
    Hua et al. ([2018](#bib.bib52)), Hermosilla et al. ([2018](#bib.bib46)), Wu et al.
    ([2019b](#bib.bib161)), Li et al. ([2018b](#bib.bib82)), Wang et al. ([2019b](#bib.bib155)),
    Lei et al. ([2020](#bib.bib78)), Xie et al. ([2020b](#bib.bib171)), Wang et al.
    ([2018c](#bib.bib149)), Groh et al. ([2018](#bib.bib33)), Lei et al. ([2019](#bib.bib77)),
    Su et al. ([2018](#bib.bib134)), Rosu et al. ([2019](#bib.bib124)) can also be
    trained for part segmentation. However, these networks can not entirely tackle
    the difficulties of part segmentation. For example, various parts with the same
    semantic label might have diverse shapes, and the number of parts for an instance
    with the same semantic label may be different. We subdivide 3D part segmentation
    methods into two categories: *regular data* based and *irregular data* based as
    follows.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 3D部件分割是实例分割之后的更精细层次，目的是为实例的不同部件打上标签。部件分割的流程与语义分割相似，只是标签现在是针对单独的部件。因此，一些现有的3D语义分割网络
    Meng et al. ([2019](#bib.bib103)), Graham et al. ([2018](#bib.bib32)), Qi et al.
    ([2017a](#bib.bib115)), Qi et al. ([2017b](#bib.bib116)), Zeng和Gevers ([2018](#bib.bib188)),
    Huang et al. ([2018](#bib.bib54)), Thomas et al. ([2019](#bib.bib139)), Hua et
    al. ([2018](#bib.bib52)), Hermosilla et al. ([2018](#bib.bib46)), Wu et al. ([2019b](#bib.bib161)),
    Li et al. ([2018b](#bib.bib82)), Wang et al. ([2019b](#bib.bib155)), Lei et al.
    ([2020](#bib.bib78)), Xie et al. ([2020b](#bib.bib171)), Wang et al. ([2018c](#bib.bib149)),
    Groh et al. ([2018](#bib.bib33)), Lei et al. ([2019](#bib.bib77)), Su et al. ([2018](#bib.bib134)),
    Rosu et al. ([2019](#bib.bib124)) 也可以用于部件分割训练。然而，这些网络不能完全解决部件分割的困难。例如，相同语义标签的不同部件可能具有不同的形状，并且一个实例中相同语义标签的部件数量可能不同。我们将3D部件分割方法细分为两类：*规则数据*基础和*不规则数据*基础，如下所示。
- en: 5.1 Regular Data Based
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 规则数据基础
- en: Regular data usually includes projected images Kalogerakis et al. ([2017](#bib.bib65)),
    voxels Wang and Lu ([2019](#bib.bib156)), Le and Duan ([2018](#bib.bib76)), Song
    et al. ([2017](#bib.bib133)). As for projected images, Kalogerakis et al. Kalogerakis
    et al. ([2017](#bib.bib65)) obtain a set of images from multiple views that optimally
    cover object surface, and then use multi-view Fully Convolutional Networks(FCNs)
    and surface-based Conditional Random Fields (CRFs) to predict and refine part
    labels separately. Voxel is a useful representation of geometric data. However,
    fine-grained tasks like part segmentation require high resolution voxels with
    more detailed structure information, which leads to high computation cost. Wang
    et al. Wang and Lu ([2019](#bib.bib156)) proposed VoxSegNet to exploit more detailed
    information from voxels with limited resolution. They use spatial dense extraction
    to preserve the spatial resolution during the sub-sampling process and an attention
    feature aggregation (AFA) module to adaptively select scale features. Le et al.
    Le and Duan ([2018](#bib.bib76)) introduced a novel 3D CNN called PointGrid, to
    incorporate a constant number of points with each cell allowing the network to
    learn better local geometry shape details. Furthermore, multiple model fusion
    can enhance the segmentation performance. Combining the advantages of images and
    voxels, Song et al. Song et al. ([2017](#bib.bib133)) proposed a two-stream FCN,
    termed AppNet and GeoNet, to explore 2D appearance and 3D geometric features from
    2D images. In particular, their VolNet extracts 3D geometric features from 3D
    volumes guiding GeoNet to extract features from a single image.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 常规数据通常包括投影图像 Kalogerakis 等人 ([2017](#bib.bib65))、体素 Wang 和 Lu ([2019](#bib.bib156))、Le
    和 Duan ([2018](#bib.bib76))、Song 等人 ([2017](#bib.bib133))。至于投影图像，Kalogerakis 等人
    ([2017](#bib.bib65)) 从多个视角获取了一组最佳覆盖物体表面的图像，然后使用多视角全卷积网络（FCNs）和基于表面的条件随机场（CRFs）分别预测和细化部件标签。体素是几何数据的一种有用表示。然而，像部件分割这样的精细任务需要高分辨率的体素以获得更详细的结构信息，这导致了高计算成本。Wang
    等人 ([2019](#bib.bib156)) 提出了 VoxSegNet 来从有限分辨率的体素中挖掘更多详细信息。他们使用空间密集提取来保持子采样过程中的空间分辨率，并通过注意力特征聚合（AFA）模块自适应地选择尺度特征。Le
    等人 ([2018](#bib.bib76)) 引入了一种新颖的 3D CNN，称为 PointGrid，通过为每个单元格引入固定数量的点，允许网络更好地学习局部几何形状细节。此外，多模型融合可以增强分割性能。结合图像和体素的优点，Song
    等人 ([2017](#bib.bib133)) 提出了一个双流 FCN，称为 AppNet 和 GeoNet，以从 2D 图像中探索 2D 外观和 3D
    几何特征。特别是，他们的 VolNet 从 3D 体积中提取 3D 几何特征，引导 GeoNet 从单幅图像中提取特征。
- en: 'Table 6: Summary of 3D part segmentation methods. M←multi-view image; Me←mesh;V←voxel;P←point
    clouds; reg.←regular data; irreg.←irregular data.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：3D 部件分割方法总结。M←多视角图像；Me←网格；V←体素；P←点云；reg.←常规数据；irreg.←不规则数据。
- en: '| Type | Methods | Input | Architecture | Feature extractor | Contribution
    |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 方法 | 输入 | 架构 | 特征提取器 | 贡献 |'
- en: '| regular | ShapePFCN Kalogerakis et al. ([2017](#bib.bib65)) | M | Multi-stream
    FCN | 2DConv | Per-label confidence maps + surface-based CRF |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 常规 | ShapePFCN Kalogerakis 等人 ([2017](#bib.bib65)) | M | 多流 FCN | 2DConv
    | 每标签置信度图 + 基于表面的 CRF |'
- en: '| VoxSegNet Wang and Lu ([2019](#bib.bib156)) | V | 3DU-Net | AtrousConv |
    SDE for preserving the spatial resolution AFA for feature selecting |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| VoxSegNet Wang 和 Lu ([2019](#bib.bib156)) | V | 3DU-Net | AtrousConv | SDE
    用于保持空间分辨率 AFA 用于特征选择 |'
- en: '| Pointgrid Le and Duan ([2018](#bib.bib76)) | V | Conv-deconv | 3DConv | Learning
    higher order local geometry shape. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| Pointgrid Le 和 Duan ([2018](#bib.bib76)) | V | Conv-deconv | 3DConv | 学习更高阶的局部几何形状。
    |'
- en: '| SubvolumeSup Song et al. ([2017](#bib.bib133)) | M+V | 2-stream FCN | 2D/3DConv
    | GeoNet/AppNet for 3/2D features exploi. + DCT for aligning. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| SubvolumeSup Song 等人 ([2017](#bib.bib133)) | M+V | 双流 FCN | 2D/3DConv | GeoNet/AppNet
    用于 3/2D 特征挖掘 + DCT 用于对齐。'
- en: '| irregular | DCN Xu et al. ([2017](#bib.bib173)) | Me | 2-tream DCN & NN |
    DirectionalConv | DCN/NN for local feature and global feature. |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 不规则 | DCN Xu 等人 ([2017](#bib.bib173)) | Me | 2-stream DCN & NN | DirectionalConv
    | DCN/NN 用于局部特征和全局特征。 |'
- en: '| MeshCNN Hanocka et al. ([2019](#bib.bib40)) | Me | 2D CNN | MeshConv | Novel
    mesh convolution and pooling |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| MeshCNN Hanocka 等人 ([2019](#bib.bib40)) | Me | 2D CNN | MeshConv | 新颖的网格卷积和池化
    |'
- en: '| PartNet Yu et al. ([2019](#bib.bib185)) | P | RNN | PN | Part feature learning
    scheme for context and geometry feature exploitation |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| PartNet Yu 等人 ([2019](#bib.bib185)) | P | RNN | PN | 部件特征学习方案用于上下文和几何特征挖掘
    |'
- en: '| SSCNN Yi et al. ([2017](#bib.bib182)) | P | FCN | SpectralConv | STN for
    allowing weight sharing + spectral multi-scale kernel |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| SSCNN Yi 等人 ([2017](#bib.bib182)) | P | FCN | SpectralConv | STN 允许权重共享 +
    光谱多尺度核 |'
- en: '| KCNet Shen et al. ([2018](#bib.bib126)) | P | PN | MLP | KNN graph on points
    + kernel correlation for measuring geometric affinity |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| KCNet Shen 等 ([2018](#bib.bib126)) | P | PN | MLP | 基于点的KNN图 + 用于测量几何相似度的核相关性
    |'
- en: '| SFCN Wang et al. ([2018b](#bib.bib146)) | P | FCN | SFConv | Novel point
    convolution |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| SFCN Wang 等 ([2018b](#bib.bib146)) | P | FCN | SFConv | 新颖的点卷积 |'
- en: '| SpiderCNN Xu et al. ([2018](#bib.bib175)) | P | PN | SpiderConv | Novel point
    convolution |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| SpiderCNN Xu 等 ([2018](#bib.bib175)) | P | PN | SpiderConv | 新颖的点卷积 |'
- en: '| FeaStNet Verma et al. ([2018](#bib.bib141)) | P | U-Net | GConv | Dynamic
    graph convolution filters |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| FeaStNet Verma 等 ([2018](#bib.bib141)) | P | U-Net | GConv | 动态图卷积滤波器 |'
- en: '| Kd-Net Klokov and Lempitsky ([2017](#bib.bib67)) | P | Kd-tree | Affine Transformation
    | Useing Kd-tree to build graphs and share learnable parameters |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Kd-Net Klokov 和 Lempitsky ([2017](#bib.bib67)) | P | Kd-tree | 仿射变换 | 使用Kd-tree构建图形并共享可学习参数
    |'
- en: '| O-CNN Wang et al. ([2017](#bib.bib148)) | P | Octree | 3DConv | Making 3D-CNN
    feasible for high-resolu. voxels |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| O-CNN Wang 等 ([2017](#bib.bib148)) | P | Octree | 3DConv | 使高分辨率体素的3D-CNN成为可能
    |'
- en: '| PointCapsNet Zhao et al. ([2019b](#bib.bib195)) | P | Encoder-decoder | PN
    | Semi-supervision learning |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| PointCapsNet Zhao 等 ([2019b](#bib.bib195)) | P | 编码器-解码器 | PN | 半监督学习 |'
- en: '| SO-Net Li et al. ([2018a](#bib.bib81)) | P | Encoder-decoder | FC layers
    | SOM for modeling spatial distribution + un-supervision learning |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| SO-Net Li 等 ([2018a](#bib.bib81)) | P | 编码器-解码器 | FC 层 | 用于建模空间分布的SOM + 无监督学习
    |'
- en: 5.2 Irregular Data Based
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 不规则数据基础
- en: Irregular data representations usually includes meshes Xu et al. ([2017](#bib.bib173)),
    Hanocka et al. ([2019](#bib.bib40)) and point clouds Li et al. ([2018a](#bib.bib81)),
    Shen et al. ([2018](#bib.bib126)), Yi et al. ([2017](#bib.bib182)), Verma et al.
    ([2018](#bib.bib141)), Wang et al. ([2018b](#bib.bib146)), Yu et al. ([2019](#bib.bib185)),
    Zhao et al. ([2019b](#bib.bib195)) Yue et al. ([2022](#bib.bib187)). Mesh provides
    an efficient approximation to a 3D shape because it captures the flat, sharp and
    intricate of surface shape surface and topology. Xu et al. Xu et al. ([2017](#bib.bib173))
    put the face normal and face distance histogram as the input of a two-stream framework
    and use the CRF to optimize the final labels. Inspired by traditional CNN, Hanocka
    et al. Hanocka et al. ([2019](#bib.bib40)) design novel mesh convolution and pooling
    to operate on the mesh edges.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 不规则数据表示通常包括网格 Xu 等 ([2017](#bib.bib173))、Hanocka 等 ([2019](#bib.bib40)) 和点云
    Li 等 ([2018a](#bib.bib81))、Shen 等 ([2018](#bib.bib126))、Yi 等 ([2017](#bib.bib182))、Verma
    等 ([2018](#bib.bib141))、Wang 等 ([2018b](#bib.bib146))、Yu 等 ([2019](#bib.bib185))、Zhao
    等 ([2019b](#bib.bib195))、Yue 等 ([2022](#bib.bib187))。网格提供了对3D形状的有效近似，因为它捕捉了表面形状和拓扑的平面、锐利和复杂特征。Xu
    等 ([2017](#bib.bib173)) 将面法线和面距离直方图作为双流框架的输入，并使用CRF优化最终标签。受传统CNN的启发，Hanocka 等
    ([2019](#bib.bib40)) 设计了新颖的网格卷积和池化操作，用于处理网格边缘。
- en: As for point clouds, the graph convolution is the most commonly used pipeline.
    In the spectral graph domain, SyncSpecCNN Yi et al. ([2017](#bib.bib182)) introduces
    a Sychronized Spectral CNN to process irregular data. Specially, multichannel
    convolution and parametrized dilated convolution kernels are proposed to solve
    multi-scale analysis and information sharing across shapes respectively. In spatial
    graph domain, in analogy to a convolution kernel for images, KCNet Shen et al.
    ([2018](#bib.bib126)) present point-set kernel and nearest-neighbor-graph to improve
    PointNet with an efficient local feature exploitation structure. Similarly, Wang
    et al. Wang et al. ([2018b](#bib.bib146)) design Shape Fully Convolutional Networks
    (SFCN) based on graph convolution and pooling operation, similar to FCN on images.
    SpiderCNN Xu et al. ([2018](#bib.bib175)) applies a special family of convolutional
    filters that combine simple step function with Taylor polynomial, making the filters
    to effectively capture intricate local geometric variations. Furthermore, FeastNet
    Verma et al. ([2018](#bib.bib141)) uses dynamic graph convolution operator to
    build relationships between filter weights and graph neighborhoods instead of
    relying on static graph of the above network.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于点云数据，图卷积是最常用的处理管道。在谱图领域，SyncSpecCNN Yi 等（[2017](#bib.bib182)）引入了一种同步谱卷积网络来处理不规则数据。特别地，提出了多通道卷积和参数化膨胀卷积核，分别解决多尺度分析和跨形状的信息共享问题。在空间图领域，类似于图像的卷积核，KCNet
    Shen 等（[2018](#bib.bib126)）提出了点集核和最近邻图，以通过高效的局部特征利用结构来改进 PointNet。类似地，Wang 等（[2018b](#bib.bib146)）设计了基于图卷积和池化操作的形状全卷积网络（SFCN），类似于图像上的
    FCN。SpiderCNN Xu 等（[2018](#bib.bib175)）应用了一种特殊的卷积滤波器族，该滤波器结合了简单的阶跃函数和泰勒多项式，使滤波器能够有效捕捉复杂的局部几何变化。此外，FeastNet
    Verma 等（[2018](#bib.bib141)）使用动态图卷积操作符建立滤波器权重与图邻域之间的关系，而不是依赖于上述网络的静态图。
- en: A special kind of graphs, the trees (e.g. Kd-tree and Octree), work on 3D shapes
    with different representations and can support various CNN architectures. Kd-Net
    Klokov and Lempitsky ([2017](#bib.bib67)) uses a kd-tree data structure to represent
    point cloud connectivity. However, the networks have high computational cost.
    O-CNN Wang et al. ([2017](#bib.bib148)) designs an Octree data structure from
    3D shapes. However, the computational cost of the O-CNN grows quadratically as
    the depth of tree increases.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊的图，即树（例如 Kd 树和八叉树），在具有不同表示形式的 3D 形状上工作，并可以支持各种 CNN 架构。Kd-Net Klokov 和 Lempitsky（[2017](#bib.bib67)）使用
    kd-tree 数据结构表示点云连通性。然而，这些网络具有较高的计算成本。O-CNN Wang 等（[2017](#bib.bib148)）从 3D 形状中设计了一种八叉树数据结构。然而，O-CNN
    的计算成本随着树的深度增加而呈平方增长。
- en: 'SO-Net Li et al. ([2018a](#bib.bib81)) sets up a Self-Organization Map (SOM)
    from point clouds, and hierarchically learns node-wise features on this map using
    the PointNet architecture. However, it fails to fully exploit local features.
    PartNet Yu et al. ([2019](#bib.bib185)) decomposes 3D shapes in a top-down fashion,
    and proposes a Recursive Neural Network (RvNN) for learning the hierarchy of fine-grained
    parts. Zhao et al. Zhao et al. ([2019b](#bib.bib195)) introduce an encoder-decoder
    network, 3D-PointCapsNet, to tackle several common point cloud-related tasks.
    The dynamic routing scheme and the peculiar 2D latent space deployed by capsule
    networks, deployed in their model, bring improved performance. The 3D part segmentation
    methods are summarized in Table [6](#S5.T6 "Table 6 ‣ 5.1 Regular Data Based ‣
    5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 'SO-Net Li 等（[2018a](#bib.bib81)）从点云中建立了一个自组织映射（SOM），并使用 PointNet 架构在该映射上分层学习节点特征。然而，它未能充分利用局部特征。PartNet
    Yu 等（[2019](#bib.bib185)）以自上而下的方式对 3D 形状进行分解，并提出了递归神经网络（RvNN）来学习细粒度部件的层次结构。Zhao
    等（[2019b](#bib.bib195)）引入了一种编码器-解码器网络 3D-PointCapsNet，以解决多个常见的点云相关任务。胶囊网络中部署的动态路由方案和特殊的
    2D 潜在空间带来了性能提升。3D 部件分割方法的总结见表 [6](#S5.T6 "Table 6 ‣ 5.1 Regular Data Based ‣ 5
    3D Part Segmentation ‣ Deep Learning Based 3D Segmentation: A Survey")。'
- en: 6 Applications of 3D Segmentation
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 3D 分割的应用
- en: We review 3D semantic segmentation methods for two main applications, unmanned
    systems.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了用于两个主要应用领域的 3D 语义分割方法，即无人系统。
- en: 6.1 Unmanned Systems
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 无人系统
- en: As LIDAR scanners and depth cameras become widely available and more affordable,
    they are increasingly being deployed in unmanned systems such as autonomous driving
    and mobile robots. These sensors provide realtime 3D video, generally at 30 frames
    per second (fps), as direct input to the system making *3D video semantic segmentation*
    as the primary task to understand the scene. Furthermore, in order to interact
    more effectively with the environment, unmanned systems generally build a *3D
    semantic map* of the scene. Below we review 3D video based semantic segmentation
    and 3D semantic map construction.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 随着激光雷达扫描仪和深度相机变得越来越普及和经济，它们越来越多地应用于无人系统，如自动驾驶和移动机器人。这些传感器提供实时的 3D 视频，通常以每秒 30
    帧（fps）的速度作为系统的直接输入，使*3D 视频语义分割*成为理解场景的主要任务。此外，为了更有效地与环境互动，无人系统通常会构建场景的*3D 语义地图*。以下是对基于
    3D 视频的语义分割和 3D 语义地图构建的回顾。
- en: 6.1.1 3D video semantic segmentation
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 3D 视频语义分割
- en: Compared to the 3D single frame/scan semantic segmentation methods reviewed
    in Section 3.1, 3D video (continuous frames/scans) semantic segmentation methods
    take into account the connecting spatio-temporal information between frames which
    is more powerful at parsing the scene robustly and continuously. Conventional
    convolutional neural networks (CNNs) are not designed to exploit the temporal
    information between frames. A common strategy is to adapt Recurrent Neural Networks
    or Spatio-temporal convolutional network.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 与第 3.1 节中回顾的 3D 单帧/扫描语义分割方法相比，3D 视频（连续帧/扫描）语义分割方法考虑了帧间连接的时空信息，这在强健和连续地解析场景时更为有效。传统的卷积神经网络（CNN）并未设计用来利用帧间的时间信息。一种常见策略是适配递归神经网络或时空卷积网络。
- en: Recurrent neural network based: RNNs generally work in combination with 2D CNNs
    to process RGB-D videos. The 2D CNN learns to extract the frame-wise spatial information
    and the RNN learns to extract the temporal information between the frames. Valipour
    et al. Valipour et al. ([2017](#bib.bib140)) proposed Recurrent Fully Neural Network
    to operate over a sliding window over the RGB-D video frames. Specifically, the
    convolutional gated recurrent unit preserves the spatial information and reduces
    the parameters. Similarly, Yurdakul et al. Emre Yurdakul and Yemez ([2017](#bib.bib22))
    combine fully convolutional and recurrent neural network to investigate the contribution
    of depth and temporal information separately in the synthetic RGB-D video.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 基于递归神经网络：RNN 通常与 2D CNN 结合使用来处理 RGB-D 视频。2D CNN 学习提取逐帧的空间信息，而 RNN 学习提取帧间的时间信息。Valipour
    等人 ([2017](#bib.bib140)) 提出了递归全神经网络，以在 RGB-D 视频帧上滑动窗口的方式进行操作。具体来说，卷积门控递归单元保持空间信息并减少参数。同样，Yurdakul
    等人（Emre Yurdakul 和 Yemez ([2017](#bib.bib22))）结合了全卷积和递归神经网络，以分别研究深度和时间信息在合成 RGB-D
    视频中的贡献。
- en: Spatio-temporal convolution based: Nearby video frames provide diverse viewpoints
    and additional context of objects and scenes. STD2P He et al. ([2017b](#bib.bib45))
    uses a novel spatio-temporal pooling layer to aggregate region correspondences
    computed by optical flow and image boundary-based super-pixels. Choy et al. Choy
    et al. ([2019](#bib.bib16)) proposed 4D Spatio-Temporary ConvNet, to directly
    process a 3D point cloud video. To overcome challenges in the high-dimensional
    4D space (3D space and time), they introduced the 4D sptio-temporal convolution,
    a generalized sparse convolution, and the trilateral-stationary conditional random
    field that keeps spatio-temporal consistency. Similarly, based on 3D sparse convolution,
    Shi et al. Shi et al. ([2020](#bib.bib127)) proposed SpSequenceNet that contains
    two novel modules, a cross frame-global attention module and a cross-frame local
    interpolation module to exploit spatial and temporal feature in 4D point clouds.
    PointMotionNet Wang et al. ([2022](#bib.bib144)) proposes a spatio-temporal convolution
    that exploits a time-invariant spatial neighboring space and extracts spatio-temporal
    features, to distinguish the moving and static objects.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时空卷积：相邻视频帧提供了多样的视角和额外的对象与场景上下文。STD2P He et al. ([2017b](#bib.bib45)) 使用了一种新型的时空池化层，通过光流和图像边界基础超像素来聚合区域对应关系。Choy
    et al. Choy et al. ([2019](#bib.bib16)) 提出了 4D 时空 ConvNet，以直接处理 3D 点云视频。为了克服高维
    4D 空间（3D 空间和时间）中的挑战，他们引入了 4D 时空卷积、广义稀疏卷积以及保持时空一致性的三边形平稳条件随机场。类似地，基于 3D 稀疏卷积，Shi
    et al. Shi et al. ([2020](#bib.bib127)) 提出了 SpSequenceNet，包含两个新模块，一个是跨帧全局注意力模块，另一个是跨帧局部插值模块，用于挖掘
    4D 点云中的空间和时间特征。PointMotionNet Wang et al. ([2022](#bib.bib144)) 提出了一个时空卷积，利用时间不变的空间邻域，提取时空特征，以区分移动物体和静态物体。
- en: 'Spatio-temporal Transformer based : To capture the dynamics in point cloud
    video, point tracking is usually employed. However, P4Transformer Fan et al. ([2021](#bib.bib28))
    proposes a 4D convolution to embed the spatio-temporal local structures in point
    cloud video and further introduces a Transformer to leverage the motion information
    across the entire video by performing the self-attention on these embedded local
    features. Similarly, PST² Wei et al. ([2022](#bib.bib158)) performs spatio-temporal
    self attention across adjacent frames to capture the spatio-temporal context,
    and proposes a resolution embedding mudule to enhance the resolution of feature
    maps by aggregating features.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 基于时空 Transformer：为了捕捉点云视频中的动态，通常会使用点跟踪。然而，P4Transformer Fan et al. ([2021](#bib.bib28))
    提出了一个 4D 卷积来嵌入点云视频中的时空局部结构，并进一步引入了 Transformer，通过对这些嵌入的局部特征进行自注意力操作，以利用整个视频中的运动信息。类似地，PST²
    Wei et al. ([2022](#bib.bib158)) 在相邻帧之间执行时空自注意力，以捕捉时空上下文，并提出了一个分辨率嵌入模块，通过聚合特征来增强特征图的分辨率。
- en: 6.1.2 3D semantic map construction
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 3D 语义地图构建
- en: 'Unmanned systems do not just need to avoid obstacles but also need to establish
    a deeper understanding of the scene such as object parsing, self localization
    etc. To facilitate such tasks, unmanned systems build a 3D semantic map of the
    scene which includes two key problems: geometric reconstruction and semantic segmentation.
    3D scene reconstruction has conventionally relied on simultaneous localization
    and mapping system (SLAM) to obtain a 3D map without semantic information. This
    is followed by 2D semantic segmentation with a 2D CNN and then the 2D labels are
    transferred to the 3D map following an optimization (e.g. conditional random field)
    to obtain a 3D semantic map Yang et al. ([2017](#bib.bib178)). This common pipeline
    does not guarantee high performance of 3D semantic maps in complex, large-scale,
    and dynamic scenes. Efforts have been made to enhance the robustness using association
    information exploitation from multiple frames, multi-model fusion and novel post-processing
    operations. These efforts are explained below.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 无人系统不仅需要避开障碍物，还需要对场景建立更深刻的理解，如对象解析、自我定位等。为了促进这些任务，无人系统构建了一个包含两个关键问题的场景 3D 语义地图：几何重建和语义分割。传统上，3D
    场景重建依赖于同时定位与地图构建系统 (SLAM) 来获取没有语义信息的 3D 地图。随后，使用 2D CNN 进行 2D 语义分割，并通过优化（例如条件随机场）将
    2D 标签转移到 3D 地图上，从而获得 3D 语义地图 Yang et al. ([2017](#bib.bib178))。这种常见的流程并不能保证在复杂、大规模和动态场景中
    3D 语义地图的高性能。为提高鲁棒性，已采取了利用多帧关联信息、多模态融合和新颖后处理操作的措施。下面将对这些努力进行详细解释。
- en: Association information exploitation: mainly depends on SLAM trajectory, recurrent
    neural networks or scene flow. Ma et al. Ma et al. ([2017](#bib.bib99)) enforce
    consistency by warping CNN feature maps from multi-views into a common reference
    view by using the SLAM trajectory and to supervise training at multiple scales.
    SemanticFusion McCormac et al. ([2017](#bib.bib102)) incorporates deconvolutional
    neural networks with a state-of-the-art dense SLAM system, ElasticFusion, which
    provides long-term correspondence between frames of a video. These correspondences
    allow label predictions from multi-views to be probabilistically fused into a
    map. Similarly, using the connection information between frames provided by a
    recurrent unit on RGB-D videos, Xiang et al. Xiang and Fox ([2017](#bib.bib168))
    proposed a data associated recurrent neural networks (DA-RNN) and integrated the
    output of the DA-RNN with KinnectFusion, which provides a consistent semantic
    labeling of the 3D scene. Cheng et al. Cheng et al. ([2020](#bib.bib12)) use a
    CRF-RNN-based semantic segmentation to generate the corresponding labels. Specifically,
    the authors proposed an optical flow-based method to deal with the dynamic factors
    for accurate localization. Kochanov et al. Kochanov et al. ([2016](#bib.bib68))
    also use scene flow to propagate dynamic objects within the 3D semantic maps.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 关联信息的利用：主要依赖于 SLAM 轨迹、递归神经网络或场景流。Ma et al. Ma et al. ([2017](#bib.bib99)) 通过使用
    SLAM 轨迹将多视角的 CNN 特征图扭曲到一个公共参考视图中，从而强制一致性，并在多个尺度上监督训练。SemanticFusion McCormac et
    al. ([2017](#bib.bib102)) 将反卷积神经网络与最先进的密集 SLAM 系统 ElasticFusion 结合起来，后者提供了视频帧之间的长期对应关系。这些对应关系允许从多视角的标签预测以概率方式融合到地图中。类似地，利用
    RGB-D 视频中递归单元提供的帧之间的连接信息，Xiang et al. Xiang and Fox ([2017](#bib.bib168)) 提出了数据关联递归神经网络（DA-RNN），并将
    DA-RNN 的输出与 KinnectFusion 集成，后者提供了一致的 3D 场景语义标注。Cheng et al. Cheng et al. ([2020](#bib.bib12))
    使用基于 CRF-RNN 的语义分割来生成相应的标签。具体而言，作者提出了一种基于光流的方法来处理动态因素，以实现准确定位。Kochanov et al.
    Kochanov et al. ([2016](#bib.bib68)) 还使用场景流在 3D 语义地图中传播动态物体。
- en: Multiple model fusion: Jeong et al. Jeong et al. ([2018](#bib.bib60)) build
    a 3D map by estimating odometry based on GPS and IMU, and use a 2D CNN for semantic
    segmentation. They integrate the 3D map with semantic labels using a coordinate
    transformation and Bayes’ update scheme. Zhao et al. Zhao et al. ([2018](#bib.bib192))
    use PixelNet and VoxelNet to exploit global context information and local shape
    information separately and then fuse the score maps with a softmax weighted fusion
    that adaptively learns the contribution of different data streams. The final dense
    3D semantic maps are generated with visual odometry and recursive Bayesian update.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型融合：Jeong et al. Jeong et al. ([2018](#bib.bib60)) 通过基于 GPS 和 IMU 的里程计估计构建
    3D 地图，并使用 2D CNN 进行语义分割。他们使用坐标变换和贝叶斯更新方案将 3D 地图与语义标签结合起来。Zhao et al. Zhao et al.
    ([2018](#bib.bib192)) 使用 PixelNet 和 VoxelNet 分别挖掘全局上下文信息和局部形状信息，然后通过软最大加权融合将得分图融合，适应性地学习不同数据流的贡献。最终通过视觉里程计和递归贝叶斯更新生成密集的
    3D 语义地图。
- en: 7 Experimental Results
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 实验结果
- en: 'Below we summarize the quantitative results of the segmentation methods discussed
    in Sections [3](#S3 "3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey"), [4](#S4 "4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") and [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") on some typical public datasets, as well as analyze these results qualitatively.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '以下是我们对在[3](#S3 "3 3D Semantic segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")、[4](#S4 "4 3D Instance Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey") 和 [5](#S5 "5 3D Part Segmentation ‣ Deep Learning Based 3D Segmentation:
    A Survey")中讨论的分割方法在一些典型公共数据集上的定量结果的总结，以及对这些结果的定性分析。'
- en: 'Table 7: Evaluation performance regarding for RGB-D semantic segmentation methods
    on the SUN-RGB-D and NYUDv2\. Note that the ‘%’ after the value is omitted and
    the symbol ‘–’ means the results are unavailable.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：关于 SUN-RGB-D 和 NYUDv2 上 RGB-D 语义分割方法的评估性能。请注意，数值后的“%”被省略，符号“–”表示结果不可用。
- en: '| Methods | NYUDv2 | SUN-RGB-D |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | NYUDv2 | SUN-RGB-D |'
- en: '| mAcc | mIoU | mAcc | mIoU |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| mAcc | mIoU | mAcc | mIoU |'
- en: '| Guo and Chen ([2018](#bib.bib35)) | 46.3 | 34.8 | 45.7 | 33.7 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| Guo and Chen ([2018](#bib.bib35)) | 46.3 | 34.8 | 45.7 | 33.7 |'
- en: '| Wang et al. ([2015](#bib.bib147)) | – | 44.2 | – | – |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2015](#bib.bib147)) | – | 44.2 | – | – |'
- en: '| Mousavian et al. ([2016](#bib.bib107)) | 52.3 | 39.2 | – | – |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Mousavian et al. ([2016](#bib.bib107)) | 52.3 | 39.2 | – | – |'
- en: '| Liu et al. ([2018b](#bib.bib94)) | 50.8 | 39.8 | 50.0 | 39.4 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. ([2018b](#bib.bib94)) | 50.8 | 39.8 | 50.0 | 39.4 |'
- en: '| Gupta et al. ([2014](#bib.bib37)) | 35.1 | 28.6 | – | – |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Gupta et al. ([2014](#bib.bib37)) | 35.1 | 28.6 | – | – |'
- en: '| Liu et al. ([2018a](#bib.bib93)) | 51.7 | 41.2 | – | – |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. ([2018a](#bib.bib93)) | 51.7 | 41.2 | – | – |'
- en: '| Hazirbas et al. ([2016](#bib.bib41)) | – | – | 48.3 | 37.3 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| Hazirbas et al. ([2016](#bib.bib41)) | – | – | 48.3 | 37.3 |'
- en: '| Lin et al. ([2017](#bib.bib89)) | – | 47.7 | – | 48.1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al. ([2017](#bib.bib89)) | – | 47.7 | – | 48.1 |'
- en: '| Jiang et al. ([2017](#bib.bib62)) | – | – | 50.6 | 39.3 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Jiang et al. ([2017](#bib.bib62)) | – | – | 50.6 | 39.3 |'
- en: '| Wang and Neumann ([2018](#bib.bib150)) | 47.3 | – | – | – |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Wang and Neumann ([2018](#bib.bib150)) | 47.3 | – | – | – |'
- en: '| Cheng et al. ([2017](#bib.bib13)) | 60.7 | 45.9 | 58.0 | – |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Cheng et al. ([2017](#bib.bib13)) | 60.7 | 45.9 | 58.0 | – |'
- en: '| Fan et al. ([2017](#bib.bib27)) | 50.2 | – | – | – |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Fan et al. ([2017](#bib.bib27)) | 50.2 | – | – | – |'
- en: '| Li et al. ([2016](#bib.bib84)) | 49.4 | – | 48.1 | – |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. ([2016](#bib.bib84)) | 49.4 | – | 48.1 | – |'
- en: '| Qi et al. ([2017c](#bib.bib117)) | 55.7 | 43.1 | 57.0 | 45.9 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Qi et al. ([2017c](#bib.bib117)) | 55.7 | 43.1 | 57.0 | 45.9 |'
- en: '| Wang et al. ([2016](#bib.bib145)) | 60.6 | 38.3 | 50.1 | 33.5 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2016](#bib.bib145)) | 60.6 | 38.3 | 50.1 | 33.5 |'
- en: 'Table 8: Evaluation performance regarding for projected images, voxel, point
    clouds and other representation semantic segmentation methods on the S3DIS, ScanNet,
    Semantic3D and SemanticKITTI. Note: the ‘%’ after the value is omitted, the symbol
    ‘–’ means the results are unavailable, the dotted line means the subdivision of
    methods according to the type of architecture.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 关于投影图像、体素、点云和其他表示语义分割方法在 S3DIS、ScanNet、Semantic3D 和 SemanticKITTI 数据集上的评估性能。注:
    数值后的‘%’被省略，符号‘–’表示结果不可用，虚线表示按架构类型划分的方法。'
- en: '| Method | Type | S3DIS | ScanNet | Semantic3D | SemanticKITTI |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 类型 | S3DIS | ScanNet | Semantic3D | SemanticKITTI |'
- en: '|  | Area5 | 6-fold | test set | reduced-8 | only xyz |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | Area5 | 6-fold | 测试集 | reduced-8 | 仅 xyz |'
- en: '|  | mAcc | mIoU | mIoU | oAcc | mIoU | oAcc | mIoU | mAcc | mIoU |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '|  | mAcc | mIoU | mIoU | oAcc | mIoU | oAcc | mIoU | mAcc | mIoU |'
- en: '| Lawin et al. Lawin et al. ([2017](#bib.bib75)) | projection | – | – | – |
    – | – | 88.9 | 58.5 | – | – |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Lawin et al. Lawin et al. ([2017](#bib.bib75)) | 投影 | – | – | – | – | – |
    88.9 | 58.5 | – | – |'
- en: '| Boulch et al. Boulch et al. ([2017](#bib.bib6)) |  | – | – | – | – | – |
    91.0 | 67.4 | – | – |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Boulch et al. Boulch et al. ([2017](#bib.bib6)) |  | – | – | – | – | – |
    91.0 | 67.4 | – | – |'
- en: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) |  | – | – | – | – | – | – | –
    | – | 37.2 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. Wu et al. ([2018a](#bib.bib159)) |  | – | – | – | – | – | – | –
    | – | 37.2 |'
- en: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) |  | – | – | – | – | – | –
    | – | – | 39.8 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. Wang et al. ([2018e](#bib.bib154)) |  | – | – | – | – | – | –
    | – | – | 39.8 |'
- en: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) |  | – | – | – | – | – | – | –
    | – | 44.9 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. Wu et al. ([2019a](#bib.bib160)) |  | – | – | – | – | – | – | –
    | – | 44.9 |'
- en: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) |  | – | – | – | – |
    – | – | – | – | 52.2 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Milioto et al. Milioto et al. ([2019](#bib.bib105)) |  | – | – | – | – |
    – | – | – | – | 52.2 |'
- en: '| Xu et al. Xu et al. ([2020](#bib.bib172)) |  | – | – | – | – | – | – | –
    | – | 55.9 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. Xu et al. ([2020](#bib.bib172)) |  | – | – | – | – | – | – | –
    | – | 55.9 |'
- en: '| RangViT Ando et al. ([2023](#bib.bib1)) |  | – | – | – | – | – | – | – |
    – | 55.9 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| RangViT Ando et al. ([2023](#bib.bib1)) |  | – | – | – | – | – | – | – |
    – | 55.9 |'
- en: '| RangFormer Kong et al. ([2023](#bib.bib70)) |  | – | – | – | – | – | – |
    – | – | 64.0 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| RangFormer Kong et al. ([2023](#bib.bib70)) |  | – | – | – | – | – | – |
    – | – | 64.0 |'
- en: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | voxel | 57.35 | 48.92
    | 48.92 | – | – | 88.1 | 61.30 | – | – |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| Tchapmi et al. Tchapmi et al. ([2017](#bib.bib138)) | 体素 | 57.35 | 48.92
    | 48.92 | – | – | 88.1 | 61.30 | – | – |'
- en: '| Meng et al. Meng et al. ([2019](#bib.bib103)) |  | – | 78.22 | – | – | –
    | – | – | – | – |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| Meng et al. Meng et al. ([2019](#bib.bib103)) |  | – | 78.22 | – | – | –
    | – | – | – | – |'
- en: '| Liu et al. Liu et al. ([2017](#bib.bib91)) |  | – | 70.76 | – | – | – | –
    | – | – | – |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. Liu et al. ([2017](#bib.bib91)) |  | – | 70.76 | – | – | – | –
    | – | – | – |'
- en: '| PointNet Qi et al. ([2017a](#bib.bib115)) | point | 48.98 | 41.09 | 47.71
    | – | 14.69 | – | – | 29.9 | 17.9 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| PointNet Qi et al. ([2017a](#bib.bib115)) | 点 | 48.98 | 41.09 | 47.71 | –
    | 14.69 | – | – | 29.9 | 17.9 |'
- en: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) |  | 59.10 | 52.17 | 58.27 |
    75.53 | – | – | – | 57.59 | 29.9 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| G+RCU Engelmann et al. ([2018](#bib.bib26)) |  | 59.10 | 52.17 | 58.27 |
    75.53 | – | – | – | 57.59 | 29.9 |'
- en: '| ESC Engelmann et al. ([2017](#bib.bib24)) |  | 54.06 | 45.14 | 49.7 | 63.4
    | – | – | – | 40.9 | 26.4 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| ESC Engelmann et al. ([2017](#bib.bib24)) |  | 54.06 | 45.14 | 49.7 | 63.4
    | – | – | – | 40.9 | 26.4 |'
- en: '| HRNN Ye et al. ([2018](#bib.bib180)) |  | 71.3 | 53.4 | – | 76.5 | – | –
    | – | 49.2 | 34.5 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| HRNN Ye et al. ([2018](#bib.bib180)) |  | 71.3 | 53.4 | – | 76.5 | – | –
    | – | 49.2 | 34.5 |'
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) |  | – | 50.04 | 54.4 | 71.40
    | 34.26 | – | – | – | – |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) |  | – | 50.04 | 54.4 | 71.40
    | 34.26 | – | – | – | – |'
- en: '| PointWeb Zhao et al. ([2019a](#bib.bib193)) |  | 66.64 | 60.28 | 66.7 | 85.9
    | – | – | – | – | – |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| PointWeb Zhao 等人 ([2019a](#bib.bib193)) |  | 66.64 | 60.28 | 66.7 | 85.9
    | – | – | – | – | – |'
- en: '| PointSIFT Jiang et al. ([2018](#bib.bib64)) |  | – | 70.23 | 70.2 | – | 41.5
    | – | – | – | – |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| PointSIFT Jiang 等人 ([2018](#bib.bib64)) |  | – | 70.23 | 70.2 | – | 41.5
    | – | – | – | – |'
- en: '| Resurf Ran et al. ([2022](#bib.bib120)) |  | 76.0 | 68.9 | 74.3 | – | 70.0
    | – | – | – | – |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Resurf Ran 等人 ([2022](#bib.bib120)) |  | 76.0 | 68.9 | 74.3 | – | 70.0 |
    – | – | – | – |'
- en: '| PointNeXt Qian et al. ([2022](#bib.bib118)) |  | – | 70.5 | 74.9 | – | 71.2
    | – | – | – | – |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| PointNeXt Qian 等人 ([2022](#bib.bib118)) |  | – | 70.5 | 74.9 | – | 71.2 |
    – | – | – | – |'
- en: '| RSNet Huang et al. ([2018](#bib.bib54)) |  | 59.42 | 56.5 | 56.47 | – | 39.35
    | – | – | – | – |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| RSNet Huang 等人 ([2018](#bib.bib54)) |  | 59.42 | 56.5 | 56.47 | – | 39.35
    | – | – | – | – |'
- en: '| DPC Engelmann et al. ([2020b](#bib.bib25)) |  | 68.38 | 61.28 | – | – | 59.2
    | – | – | – | – |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| DPC Engelmann 等人 ([2020b](#bib.bib25)) |  | 68.38 | 61.28 | – | – | 59.2
    | – | – | – | – |'
- en: '| PointwiseCNN Hua et al. ([2018](#bib.bib52)) |  | 56.5 | – | – | – | – |
    – | – | – | – |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| PointwiseCNN Hua 等人 ([2018](#bib.bib52)) |  | 56.5 | – | – | – | – | – |
    – | – | – |'
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) |  | 67.01 | 58.27 | – | – | 49.8
    | – | – | – | – |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| PCCN Wang 等人 ([2018c](#bib.bib149)) |  | 67.01 | 58.27 | – | – | 49.8 | –
    | – | – | – |'
- en: '| PointCNN Li et al. ([2018b](#bib.bib82)) |  | 63.86 | 57.26 | 65.3 | 85.1
    | 45.8 | – | – | – | – |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| PointCNN Li 等人 ([2018b](#bib.bib82)) |  | 63.86 | 57.26 | 65.3 | 85.1 | 45.8
    | – | – | – | – |'
- en: '| KPConv Thomas et al. ([2019](#bib.bib139)) |  | – | 67.1 | 70.6 | – | 66.6
    | 92.9 | 74.6 | – | – |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| KPConv Thomas 等人 ([2019](#bib.bib139)) |  | – | 67.1 | 70.6 | – | 66.6 |
    92.9 | 74.6 | – | – |'
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) |  | – | 50.34 | – | – | 55.6
    | – | – | – | – |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| PointConv Wu 等人 ([2019b](#bib.bib161)) |  | – | 50.34 | – | – | 55.6 | –
    | – | – | – |'
- en: '| A-CNN Komarichev et al. ([2019](#bib.bib69)) |  | – | – | – | 85.4 | – |
    – | – | – | – |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| A-CNN Komarichev 等人 ([2019](#bib.bib69)) |  | – | – | – | 85.4 | – | – |
    – | – | – |'
- en: '| RandLA-Net Hu et al. ([2020](#bib.bib49)) |  | – | – | 70.0 | – | – | 94.8
    | 77.4 | – | 53.9 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| RandLA-Net Hu 等人 ([2020](#bib.bib49)) |  | – | – | 70.0 | – | – | 94.8 |
    77.4 | – | 53.9 |'
- en: '| PolarNet Zhang et al. ([2020](#bib.bib190)) |  | – | – | – | – | – | – |
    – | – | 54.3 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| PolarNet Zhang 等人 ([2020](#bib.bib190)) |  | – | – | – | – | – | – | – |
    – | 54.3 |'
- en: '| DGCNN Wang et al. ([2019b](#bib.bib155)) |  | – | 56.1 | 56.1 | – | – | –
    | – | – | – |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN Wang 等人 ([2019b](#bib.bib155)) |  | – | 56.1 | 56.1 | – | – | – | –
    | – | – |'
- en: '| SPG Landrieu and Simonovsky ([2018](#bib.bib74)) |  | 66.50 | 58.04 | 62.1
    | – | – | 94.0 | 73.2 | – | – |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| SPG Landrieu 和 Simonovsky ([2018](#bib.bib74)) |  | 66.50 | 58.04 | 62.1
    | – | – | 94.0 | 73.2 | – | – |'
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) |  | 65.9 | 59.5 | 68.9 | – | 61.0
    | – | – | – | – |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| SPH3D-GCN Lei 等人 ([2020](#bib.bib78)) |  | 65.9 | 59.5 | 68.9 | – | 61.0
    | – | – | – | – |'
- en: '| DeepGCNs Li et al. ([2019a](#bib.bib80)) |  | – | 60.0 | – | – | – | – |
    – | – | – |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| DeepGCNs Li 等人 ([2019a](#bib.bib80)) |  | – | 60.0 | – | – | – | – | – |
    – | – |'
- en: '| PointGCRNet Ma et al. ([2020](#bib.bib100)) |  | – | 52.43 | – | – | 60.8
    | – | – | – | – |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| PointGCRNet Ma 等人 ([2020](#bib.bib100)) |  | – | 52.43 | – | – | 60.8 | –
    | – | – | – |'
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) |  | – | – | 56.63 | – | – | – | –
    | – | – |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| AGCN Xie 等人 ([2020b](#bib.bib171)) |  | – | – | 56.63 | – | – | – | – | –
    | – |'
- en: '| PAN Feng et al. ([2020](#bib.bib29)) |  | – | 66.3 | – | 86.7 | 42.1 | –
    | – | – | – |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| PAN Feng 等人 ([2020](#bib.bib29)) |  | – | 66.3 | – | 86.7 | 42.1 | – | –
    | – | – |'
- en: '| TGNet Li et al. ([2019b](#bib.bib83)) |  | – | 58.7 | – | 66.2 | – | – |
    – | – | – |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| TGNet Li 等人 ([2019b](#bib.bib83)) |  | – | 58.7 | – | 66.2 | – | – | – |
    – | – |'
- en: '| HDGCN Liang et al. ([2019a](#bib.bib87)) |  | 65.81 | 59.33 | 66.85 | – |
    – | – | – | – | – |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| HDGCN Liang 等人 ([2019a](#bib.bib87)) |  | 65.81 | 59.33 | 66.85 | – | – |
    – | – | – | – |'
- en: '| 3DContextNet Zeng and Gevers ([2018](#bib.bib188)) |  | 74.5 | 55.6 | 55.6
    | – | – | – | – | – | – |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 3DContextNet Zeng 和 Gevers ([2018](#bib.bib188)) |  | 74.5 | 55.6 | 55.6
    | – | – | – | – | – | – |'
- en: '| PGCRNet Ma et al. ([2020](#bib.bib100)) |  | – | 54.4 | – | – | – | – | 69.5
    | – | – |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| PGCRNet Ma 等人 ([2020](#bib.bib100)) |  | – | 54.4 | – | – | – | – | 69.5
    | – | – |'
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) |  | 74.5 | 55.6 | 55.6 | – | – |
    – | – | – | – |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| AGCN Xie 等人 ([2020b](#bib.bib171)) |  | 74.5 | 55.6 | 55.6 | – | – | – |
    – | – | – |'
- en: '| PointANSL Yan et al. ([2020](#bib.bib176)) |  | – | 62.6 | 68.7 | – | – |
    66.6 | – | – | – |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| PointANSL Yan 等人 ([2020](#bib.bib176)) |  | – | 62.6 | 68.7 | – | – | 66.6
    | – | – | – |'
- en: '| Point Transformer Zhao et al. ([2021](#bib.bib194)) |  | 76.5 | 70.4 | 73.5
    | – | – | – | – | – | – |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| Point Transformer Zhao 等人 ([2021](#bib.bib194)) |  | 76.5 | 70.4 | 73.5 |
    – | – | – | – | – | – |'
- en: '| Point Transformer v2 Wu et al. ([2022a](#bib.bib162)) |  | 77.9 | 71.6 |
    – | – | 75.2 | – | – | – | – |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| Point Transformer v2 Wu 等人 ([2022a](#bib.bib162)) |  | 77.9 | 71.6 | – |
    – | 75.2 | – | – | – | – |'
- en: '| PatchFormer Zhang et al. ([2022](#bib.bib189)) |  | – | 68.1 | – | – | –
    | – | – | – | – |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| PatchFormer Zhang 等人 ([2022](#bib.bib189)) |  | – | 68.1 | – | – | – | –
    | – | – | – |'
- en: '| Fast Point Transformer Park et al. ([2022](#bib.bib111)) |  | 77.3 | 70.1
    | – | – | – | – | – | – | – |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| Fast Point Transformer Park 等人 ([2022](#bib.bib111)) |  | 77.3 | 70.1 | –
    | – | – | – | – | – | – |'
- en: '| Stratify Transformer Lai et al. ([2022](#bib.bib73)) |  | 78.1 | 72.0 | –
    | – | 73.7 | – | – | – | – |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Stratify Transformer Lai et al. ([2022](#bib.bib73)) |  | 78.1 | 72.0 | –
    | – | 73.7 | – | – | – | – |'
- en: '| SphereFormer Lai et al. ([2023](#bib.bib72)) |  | – | – | – | – | – | – |
    – | – | 78.4 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| SphereFormer Lai et al. ([2023](#bib.bib72)) |  | – | – | – | – | – | – |
    – | – | 78.4 |'
- en: '| TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | others | 62.2 | 52.8
    | – | 80.1 | 40.9 | 89.3 | 66.4 | – | – |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| TangentConv Tatarchenko et al. ([2018](#bib.bib137)) | 其他 | 62.2 | 52.8 |
    – | 80.1 | 40.9 | 89.3 | 66.4 | – | – |'
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) |  | – | – | – | – | 39.3 | – |
    – | – | – |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| SPLATNet Su et al. ([2018](#bib.bib134)) |  | – | – | – | – | 39.3 | – |
    – | – | – |'
- en: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) |  | – | – | – | – | 64.0 |
    – | – | – | 52.9 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| LatticeNet Rosu et al. ([2019](#bib.bib124)) |  | – | – | – | – | 64.0 |
    – | – | – | 52.9 |'
- en: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) |  | – | – | – | – | 63.4
    | – | – | – | – |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| Hung et al. Chiang et al. ([2019](#bib.bib14)) |  | – | – | – | – | 63.4
    | – | – | – | – |'
- en: '| PVCNN Liu et al. ([2019b](#bib.bib97)) |  | 87.12 | 58.98 | – | – | – | –
    | – | – | – |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| PVCNN Liu et al. ([2019b](#bib.bib97)) |  | 87.12 | 58.98 | – | – | – | –
    | – | – | – |'
- en: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) |  | – | – | – | – | 66.4 | – |
    – | – | – |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| MVPNet Jaritz et al. ([2019](#bib.bib59)) |  | – | – | – | – | 66.4 | – |
    – | – | – |'
- en: '| BPNet Hu et al. ([2021](#bib.bib50)) |  | – | – | – | – | 74.9 | – | – |
    – | – |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| BPNet Hu et al. ([2021](#bib.bib50)) |  | – | – | – | – | 74.9 | – | – |
    – | – |'
- en: 7.1 Results for 3D Semantic Segmentation
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 3D语义分割的结果
- en: 'We report the results of RGB-D based semantic segmentation methods on SUN-RGB-D
    Song et al. ([2015](#bib.bib132)) and NYUDv2 Silberman et al. ([2012](#bib.bib130))
    datasets using mAcc (mean Accuracy) and mIoU (mean Intersection over Union) as
    the evaluation metrics. These results of various methods are taken from the original
    papers and they are shown in Table [7](#S7.T7 "Table 7 ‣ 7 Experimental Results
    ‣ Deep Learning Based 3D Segmentation: A Survey").'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '我们报告了基于RGB-D的语义分割方法在SUN-RGB-D Song et al. ([2015](#bib.bib132)) 和 NYUDv2 Silberman
    et al. ([2012](#bib.bib130)) 数据集上的结果，使用mAcc（平均准确率）和mIoU（平均交并比）作为评估指标。这些方法的结果取自原始论文，并展示在表[7](#S7.T7
    "Table 7 ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey")中。'
- en: 'We report the results of projected images/voxel/point clouds/other representation
    semantic segmentation methods on S3DIS Armeni et al. ([2016](#bib.bib2)) (both
    Area 5 and 6-fold cross validation), ScanNet Dai et al. ([2017](#bib.bib18)) (test
    sets), Semantic3D Hackel et al. ([2017](#bib.bib38)) (reduced-8 subsets) and SemanticKITTI
    Behley et al. ([2019](#bib.bib3)) (only xyz without RGB). We use mAcc, oAcc (overall
    accuracy) and mIoU as the evaluation metrics. These results of various methods
    are taken from the original papers. Table [8](#S7.T8 "Table 8 ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey") reports the results.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '我们报告了投影图像/体素/点云/其他表示的语义分割方法在S3DIS Armeni et al. ([2016](#bib.bib2))（包括第5区和6折交叉验证）、ScanNet
    Dai et al. ([2017](#bib.bib18))（测试集）、Semantic3D Hackel et al. ([2017](#bib.bib38))（减少的8个子集）和SemanticKITTI
    Behley et al. ([2019](#bib.bib3))（仅xyz无RGB）上的结果。我们使用mAcc、oAcc（总体准确率）和mIoU作为评估指标。这些方法的结果取自原始论文。表[8](#S7.T8
    "Table 8 ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey")报告了结果。'
- en: 'The architectures of point cloud semantic segmentation typically focus on five
    main components: basic framework, neighborhood search, features abstraction, coarsening,
    and pre-processing. Below, we provide a more detailed discussion of each component.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 点云语义分割的架构通常集中于五个主要组件：基本框架、邻域搜索、特征抽象、粗化和预处理。下面，我们将对每个组件进行更详细的讨论。
- en: 'Basic framework: Basic networks are one of the main driving forces behind the
    development of 3D segmentation. Generally, there are two main basic frameworks
    including PointNet and PointNet++. The PointNet framework utilizes shared Multi-Layer
    Perceptrons (MLPs) to capture point-wise features and employs max-pooling to aggregate
    these features into a global representation. However, it lacks the ability to
    learn local features due to the absence of a defined local neighborhood. Additionally,
    the fixed resolution of the feature map makes it challenging to adapt to deep
    architectures. In contrast, the PointNet++ framework introduces a novel hierarchical
    learning architecture. It defines local regions in a hierarchical manner and to
    progressively extract features from these regions. This approach enables the network
    to capture both local and global information, leading to improved performance.
    As a result, many current networks adopt the PointNet++ framework or similar variations
    (such as 3D U-Net). This framework significantly reduces computational and memory
    complexities, particularly in high-level tasks like semantic segmentation, instance
    segmentation, and detection.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 基本框架：基本网络是3D分割发展的主要驱动力之一。通常，主要有两个基本框架，包括PointNet和PointNet++。PointNet框架利用共享的多层感知机（MLP）来捕捉点特征，并采用最大池化将这些特征聚合成全局表示。然而，由于缺乏定义的局部邻域，它无法学习局部特征。此外，特征图的固定分辨率使得其难以适应深层架构。相比之下，PointNet++框架引入了一种新颖的层次化学习架构。它以层次化的方式定义局部区域，并逐步从这些区域中提取特征。这种方法使网络能够捕捉到局部和全局信息，从而提高性能。因此，许多当前的网络采用了PointNet++框架或类似的变体（例如3D
    U-Net）。该框架显著减少了计算和内存复杂度，特别是在语义分割、实例分割和检测等高级任务中。
- en: 'Neighborhood search: To exploit the local features of point clouds, the neighborhood
    point search is introduced into networks, including the K nearest neighbors (KNN)
    Zhao et al. ([2021](#bib.bib194)), Ran et al. ([2022](#bib.bib120)), Qian et al.
    ([2022](#bib.bib118)), Ball search Hermosilla et al. ([2018](#bib.bib46)), Thomas
    et al. ([2019](#bib.bib139)), Lei et al. ([2020](#bib.bib78)), grid-based search
    Hua et al. ([2018](#bib.bib52)), Wu et al. ([2022a](#bib.bib162)) and tree based
    search Lei et al. ([2019](#bib.bib77)). KNN search retrieves the K closest neighbors
    to a query point based on a distance metric, and hence lacks robustness to point
    clouds with varying densities. Some works integrate the dilated mechanism with
    the neighbor search to expand the receptive field Komarichev et al. ([2019](#bib.bib69)),
    Li et al. ([2018b](#bib.bib82)), Li et al. ([2019a](#bib.bib80)). Ball search
    involves finding all points within a specified radius (ball) around a query point.
    Similarly, grid-based search divides the point cloud space into a regular grid
    structure. Ball search and grid-based search are both useful for effectively capturing
    local structures and neighborhoods of varying densities.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域搜索：为了利用点云的局部特征，将邻域点搜索引入到网络中，包括K最近邻（KNN）Zhao等人（[2021](#bib.bib194)）、Ran等人（[2022](#bib.bib120)）、Qian等人（[2022](#bib.bib118)）、球形搜索Hermosilla等人（[2018](#bib.bib46)）、Thomas等人（[2019](#bib.bib139)）、Lei等人（[2020](#bib.bib78)）、基于网格的搜索Hua等人（[2018](#bib.bib52)）、Wu等人（[2022a](#bib.bib162)）和基于树的搜索Lei等人（[2019](#bib.bib77)）。KNN搜索基于距离度量检索离查询点最近的K个邻居，因此对密度变化的点云缺乏鲁棒性。一些工作将膨胀机制与邻域搜索结合，以扩展感受野Komarichev等人（[2019](#bib.bib69)）、Li等人（[2018b](#bib.bib82)）、Li等人（[2019a](#bib.bib80)）。球形搜索涉及在查询点周围的指定半径（球）内寻找所有点。类似地，基于网格的搜索将点云空间划分为规则的网格结构。球形搜索和基于网格的搜索都有效地捕捉到了局部结构和不同密度的邻域。
- en: 'Features abstraction: In feature abstraction, commonly used methods include
    MLP-based, Convolution-based, and Transformer-based approaches. MLP is often used
    to extract features from individual points in point cloud data. By passing the
    feature vectors of each point through multiple fully connected layers, MLP learns
    nonlinear point-level feature representations. MLP offers flexibility and scalability
    in point cloud processing. Convolution operations on point clouds typically involve
    aggregating (low-level) information from local points to capture local structures
    and contextual information. In contrast, Transformer based methods establish correlations
    between high-level point information through the attention mechanism, which is
    more helpful for high-level tasks such as point cloud segmentation.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 特征抽象：在特征抽象中，常用的方法包括基于 MLP 的、基于卷积的和基于 Transformer 的方法。MLP 通常用于从点云数据中的单个点提取特征。通过将每个点的特征向量传递通过多个全连接层，MLP
    学习非线性的点级特征表示。MLP 在点云处理中提供了灵活性和可扩展性。对点云的卷积操作通常涉及从局部点聚合（低级）信息，以捕捉局部结构和上下文信息。相比之下，基于
    Transformer 的方法通过注意力机制建立高层点信息之间的相关性，这对点云分割等高层任务更为有用。
- en: The essence of MLP based, convolution based, and Transformer based methods is
    to learn the relationships between points, obtaining the robust weights. In the
    context of similar baseline architecture, the more comprehensive the learned point
    cloud relationship in the feature abstraction process, the stronger the robustness
    of the model becomes. Recently, MLP-based methods (e.g. Resurf Ran et al. ([2022](#bib.bib120))
    and PointNeXt Qian et al. ([2022](#bib.bib118))) exhibit better accuracy and efficiency,
    encouraging researchers to re-examine and further explore the potential of MLP-based
    approaches.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MLP、基于卷积和基于 Transformer 的方法的本质是学习点之间的关系，从而获得鲁棒的权重。在相似基线架构的背景下，特征抽象过程中的点云关系学得越全面，模型的鲁棒性就越强。近年来，基于
    MLP 的方法（如 Resurf Ran et al. ([2022](#bib.bib120)) 和 PointNeXt Qian et al. ([2022](#bib.bib118)))
    展现了更好的准确性和效率，鼓励研究人员重新审视并进一步探索基于 MLP 的方法的潜力。
- en: 'Table 9: Evaluation performance regarding for 3D instance segmentation methods
    on the ScanNet. Note: the ‘%’ after the value is omitted.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：关于 ScanNet 上 3D 实例分割方法的评估性能。注意：值后的“%”被省略。
- en: '| Methods | mAP | bath. | bed | book. | cabi. | chair | count. | curt. | desk
    | door | other | pict. | refr. | shower. | sink | sofa | table | toilet | wind.
    |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | mAP | 浴室 | 床 | 书 | 柜子 | 椅子 | 数量 | 窗帘 | 桌子 | 门 | 其他 | 图片 | 冰箱 | 淋浴 |
    水槽 | 沙发 | 桌子 | 厕所 | 风扇 |'
- en: '| GSPN Yi et al. ([2019](#bib.bib183)) | 30.6 | 50.0 | 40.5 | 31.1 | 34.8 |
    58.9 | 5.4 | 6.8 | 12.6 | 28.3 | 29.0 | 2.8 | 21.9 | 21.4 | 33.1 | 39.6 | 27.5
    | 82.1 | 24.5 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| GSPN Yi et al. ([2019](#bib.bib183)) | 30.6 | 50.0 | 40.5 | 31.1 | 34.8 |
    58.9 | 5.4 | 6.8 | 12.6 | 28.3 | 29.0 | 2.8 | 21.9 | 21.4 | 33.1 | 39.6 | 27.5
    | 82.1 | 24.5 |'
- en: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | 38.2 | 100 | 43.2 | 24.5 | 19.0
    | 57.7 | 1.3 | 26.3 | 3.3 | 32.0 | 24.0 | 7.5 | 42.2 | 85.7 | 11.7 | 69.9 | 27.1
    | 88.3 | 23.5 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 3D-SIS Hou et al. ([2019](#bib.bib48)) | 38.2 | 100 | 43.2 | 24.5 | 19.0
    | 57.7 | 1.3 | 26.3 | 3.3 | 32.0 | 24.0 | 7.5 | 42.2 | 85.7 | 11.7 | 69.9 | 27.1
    | 88.3 | 23.5 |'
- en: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | 48.8 | 100 | 67.2 | 59.0 | 30.1
    | 48.4 | 9.8 | 62.0 | 30.6 | 34.1 | 25.9 | 12.5 | 43.4 | 79.6 | 40.2 | 49.9 |
    51.3 | 90.9 | 43.9 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 3D-BoNet Yang et al. ([2019](#bib.bib177)) | 48.8 | 100 | 67.2 | 59.0 | 30.1
    | 48.4 | 9.8 | 62.0 | 30.6 | 34.1 | 25.9 | 12.5 | 43.4 | 79.6 | 40.2 | 49.9 |
    51.3 | 90.9 | 43.9 |'
- en: '| SGPN Wang et al. ([2018d](#bib.bib151)) | 14.3 | 20.8 | 39.0 | 16.9 | 6.5
    | 27.5 | 2.9 | 6.9 | 0 | 8.7 | 4.3 | 1.4 | 2.7 | 0 | 11.2 | 35.1 | 16.8 | 43.8
    | 13.8 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| SGPN Wang et al. ([2018d](#bib.bib151)) | 14.3 | 20.8 | 39.0 | 16.9 | 6.5
    | 27.5 | 2.9 | 6.9 | 0 | 8.7 | 4.3 | 1.4 | 2.7 | 0 | 11.2 | 35.1 | 16.8 | 43.8
    | 13.8 |'
- en: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | 61.1 | 100 | 83.3 | 76.5
    | 52.6 | 75.6 | 13.6 | 58.8 | 47.0 | 43.8 | 43.2 | 35.8 | 65.0 | 85.7 | 42.9 |
    76.5 | 55.7 | 100 | 43.0 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 3D-MPA Engelmann et al. ([2020a](#bib.bib23)) | 61.1 | 100 | 83.3 | 76.5
    | 52.6 | 75.6 | 13.6 | 58.8 | 47.0 | 43.8 | 43.2 | 35.8 | 65.0 | 85.7 | 42.9 |
    76.5 | 55.7 | 100 | 43.0 |'
- en: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | 76.1 | 100 | 80.8 | 84.5 | 71.6
    | 86.2 | 24.3 | 82.4 | 65.5 | 62.0 | 73.4 | 69.9 | 79.1 | 98.1 | 71.6 | 84.4 |
    76.9 | 100 | 59.4 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| SoftGroup Vu et al. ([2022](#bib.bib142)) | 76.1 | 100 | 80.8 | 84.5 | 71.6
    | 86.2 | 24.3 | 82.4 | 65.5 | 62.0 | 73.4 | 69.9 | 79.1 | 98.1 | 71.6 | 84.4 |
    76.9 | 100 | 59.4 |'
- en: '| SSTNet Liang et al. ([2021](#bib.bib86)) | 69.8 | 100 | 69.7 | 88.8 | 55.6
    | 80.3 | 38.7 | 62.6 | 41.7 | 55.6 | 58.5 | 70.2 | 60.0 | 100 | 82.4 | 72.0 |
    69.2 | 100 | 50.9 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| SSTNet Liang et al. ([2021](#bib.bib86)) | 69.8 | 100 | 69.7 | 88.8 | 55.6
    | 80.3 | 38.7 | 62.6 | 41.7 | 55.6 | 58.5 | 70.2 | 60.0 | 100 | 82.4 | 72.0 |
    69.2 | 100 | 50.9 |'
- en: '| 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | 24.8 | 66.7 | 56.6 | 7.6 | 3.5
    | 39.4 | 2.7 | 3.5 | 9.8 | 9.8 | 3.0 | 2.5 | 9.8 | 37.5 | 12.6 | 60.4 | 18.1 |
    85.4 | 17.1 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 3D-BEVIS Elich et al. ([2019](#bib.bib21)) | 24.8 | 66.7 | 56.6 | 7.6 | 3.5
    | 39.4 | 2.7 | 3.5 | 9.8 | 9.8 | 3.0 | 2.5 | 9.8 | 37.5 | 12.6 | 60.4 | 18.1 |
    85.4 | 17.1 |'
- en: '| PanopticFus. Narita et al. ([2019](#bib.bib108)) | 47.8 | 66.7 | 71.2 | 59.5
    | 25.9 | 55.0 | 0 | 61.3 | 17.5 | 25.0 | 43.4 | 43.7 | 41.1 | 85.7 | 48.5 | 59.1
    | 26.7 | 94.4 | 35.9 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| PanopticFus. Narita et al. ([2019](#bib.bib108)) | 47.8 | 66.7 | 71.2 | 59.5
    | 25.9 | 55.0 | 0 | 61.3 | 17.5 | 25.0 | 43.4 | 43.7 | 41.1 | 85.7 | 48.5 | 59.1
    | 26.7 | 94.4 | 35.9 |'
- en: '| OccuSeg Han et al. ([2020](#bib.bib39)) | 67.2 | 100 | 75.8 | 68.2 | 57.6
    | 84.2 | 47.7 | 50.4 | 52.4 | 56.7 | 58.5 | 45.1 | 55.7 | 100 | 75.1 | 79.7 |
    56.3 | 100 | 46.7 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| OccuSeg Han et al. ([2020](#bib.bib39)) | 67.2 | 100 | 75.8 | 68.2 | 57.6
    | 84.2 | 47.7 | 50.4 | 52.4 | 56.7 | 58.5 | 45.1 | 55.7 | 100 | 75.1 | 79.7 |
    56.3 | 100 | 46.7 |'
- en: '| MTML Lahoud et al. ([2019](#bib.bib71)) | 54.9 | 100 | 80.7 | 58.8 | 32.7
    | 64.7 | 0.4 | 81.5 | 18.0 | 41.8 | 36.4 | 18.2 | 44.5 | 100 | 44.2 | 68.8 | 57.1
    | 100 | 39.6 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| MTML Lahoud et al. ([2019](#bib.bib71)) | 54.9 | 100 | 80.7 | 58.8 | 32.7
    | 64.7 | 0.4 | 81.5 | 18.0 | 41.8 | 36.4 | 18.2 | 44.5 | 100 | 44.2 | 68.8 | 57.1
    | 100 | 39.6 |'
- en: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | 63.6 | 100 | 76.5 | 62.4
    | 50.5 | 79.7 | 11.6 | 69.6 | 38.4 | 44.1 | 55.9 | 47.6 | 59.6 | 100 | 66.6 |
    75.6 | 55.6 | 99.7 | 51.3 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| PointGroup Jiang et al. ([2020b](#bib.bib63)) | 63.6 | 100 | 76.5 | 62.4
    | 50.5 | 79.7 | 11.6 | 69.6 | 38.4 | 44.1 | 55.9 | 47.6 | 59.6 | 100 | 66.6 |
    75.6 | 55.6 | 99.7 | 51.3 |'
- en: '| HAIS Chen et al. ([2021](#bib.bib10)) | 69.9 | 100 | 84.9 | 82.0 | 67.5 |
    80.8 | 27.9 | 75.7 | 46.5 | 51.7 | 59.6 | 55.9 | 60.0 | 100 | 65.4 | 76.7 | 67.6
    | 99.4 | 56.0 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| HAIS Chen et al. ([2021](#bib.bib10)) | 69.9 | 100 | 84.9 | 82.0 | 67.5 |
    80.8 | 27.9 | 75.7 | 46.5 | 51.7 | 59.6 | 55.9 | 60.0 | 100 | 65.4 | 76.7 | 67.6
    | 99.4 | 56.0 |'
- en: '| Dyco3D He et al. ([2021](#bib.bib43)) | 64.1 | 100 | 84.1 | 89.3 | 53.1 |
    80.2 | 11.5 | 58.8 | 44.8 | 43.8 | 53.7 | 43.0 | 55.0 | 85.7 | 53.4 | 76.4 | 65.7
    | 98.7 | 56.8 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| Dyco3D He et al. ([2021](#bib.bib43)) | 64.1 | 100 | 84.1 | 89.3 | 53.1 |
    80.2 | 11.5 | 58.8 | 44.8 | 43.8 | 53.7 | 43.0 | 55.0 | 85.7 | 53.4 | 76.4 | 65.7
    | 98.7 | 56.8 |'
- en: '| DKNet Wu et al. ([2022b](#bib.bib163)) | 71.8 | 100 | 81.4 | 78.2 | 61.9
    | 87.2 | 22.4 | 75.1 | 56.9 | 67.7 | 58.5 | 72.4 | 63.3 | 98.1 | 51.5 | 81.9 |
    73.6 | 100 | 61.7 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| DKNet Wu et al. ([2022b](#bib.bib163)) | 71.8 | 100 | 81.4 | 78.2 | 61.9
    | 87.2 | 22.4 | 75.1 | 56.9 | 67.7 | 58.5 | 72.4 | 63.3 | 98.1 | 51.5 | 81.9 |
    73.6 | 100 | 61.7 |'
- en: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | 76.3 | 100 | 87.3 | 71.7 | 66.6
    | 85.8 | 50.8 | 66.7 | 76.4 | 64.3 | 67.6 | 68.8 | 82.5 | 100 | 77.3 | 74.1 |
    77.7 | 100 | 55.6 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| ISBNet Ngo et al. ([2023](#bib.bib110)) | 76.3 | 100 | 87.3 | 71.7 | 66.6
    | 85.8 | 50.8 | 66.7 | 76.4 | 64.3 | 67.6 | 68.8 | 82.5 | 100 | 77.3 | 74.1 |
    77.7 | 100 | 55.6 |'
- en: 'Coarsening: Coarsening, also known as downsampling or subsampling, involves
    reducing the number of points in the point cloud while preserving the essential
    structures and features. Coarsening techniques include random sampling Hu et al.
    ([2020](#bib.bib49)), farthest point sampling Qi et al. ([2017a](#bib.bib115),
    [b](#bib.bib116)), tree-based methods Lei et al. ([2019](#bib.bib77)) and mesh-based
    decimation Lei et al. ([2023](#bib.bib79)). This step helps to reduce computational
    complexity and improve efficiency in subsequent stages of the segmentation process.
    Random sampling is simple and computationally efficient but may not select the
    most optimal points in maintaining local and global structure. This can potentially
    lead to information loss in feature rich regions. Farthest point sampling is widely
    used in networks as it ensures a more even spatial distribution of the selected
    points and can help preserve global structures. However, local structures can
    still get destroyed with farthest point sampling. Tree-based methods leverage
    hierarchical tree structures, such as an octree, to partition the point cloud
    and perform coarsening. Mesh based methods must convert the point cloud to a mesh
    first before it can decimate it. This adds a computational overhead to the already
    expensive mesh decimation process. Moreover, creating a mesh from complex and
    sparse point clouds obtained from LiDAR sensors is not always possible Lei et al.
    ([2023](#bib.bib79)).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 粗化：粗化，也称为降采样或子采样，涉及在保持点云中基本结构和特征的同时减少点的数量。粗化技术包括随机采样 Hu 等（[2020](#bib.bib49)）、最远点采样
    Qi 等（[2017a](#bib.bib115)，[b](#bib.bib116)）、基于树的方法 Lei 等（[2019](#bib.bib77)）和基于网格的简化
    Lei 等（[2023](#bib.bib79)）。这一步有助于降低计算复杂度，并提高后续分割过程的效率。随机采样简单且计算效率高，但可能不会选择在保持局部和全局结构方面最优的点。这可能导致在特征丰富的区域信息丢失。最远点采样在网络中被广泛使用，因为它确保了所选点的空间分布更均匀，并有助于保持全局结构。然而，最远点采样仍可能破坏局部结构。基于树的方法利用分层树结构，如八叉树，来划分点云并进行粗化。基于网格的方法必须先将点云转换为网格，然后才能简化网格。这为已经昂贵的网格简化过程增加了计算开销。此外，从LiDAR传感器获得的复杂且稀疏的点云创建网格并不总是可能的
    Lei 等（[2023](#bib.bib79)）。
- en: The above methods are hand-crafted or engineered techniques that do not involve
    learning parameters directly from the data, which determines the sub-sampling
    pattern based on predefined rules or heuristics, without explicitly optimizing
    for the task at hand. Therefore, some works propose learnable coarsening methods
    that integrate a learnable layer into the coarsening module such as pooling Groh
    et al. ([2018](#bib.bib33)), Lai et al. ([2023](#bib.bib72)), Wu et al. ([2022a](#bib.bib162)),
    Zhao et al. ([2021](#bib.bib194)), attention mechanism Yan et al. ([2020](#bib.bib176)).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法是手工制作或工程化技术，不直接从数据中学习参数，这决定了基于预定义规则或启发式的方法的子采样模式，而没有明确针对任务进行优化。因此，一些工作提出了可学习的粗化方法，将可学习层集成到粗化模块中，如池化
    Groh 等（[2018](#bib.bib33)）、Lai 等（[2023](#bib.bib72)）、Wu 等（[2022a](#bib.bib162)）、Zhao
    等（[2021](#bib.bib194)）、注意机制 Yan 等（[2020](#bib.bib176)）。
- en: 'Pre-processing: Pre-processing is an essential step in point cloud semantic
    segmentation that involves preparing and transforming the raw point cloud data
    before feeding it into the segmentation network. Pre-processing aims to enhance
    the quality, consistency, and suitability of the data for the segmentation task.
    Some common aspects of pre-processing in point cloud segmentation include data
    normalization, data augmentation, outlier removal, and point registration.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理：预处理是点云语义分割中的一个重要步骤，涉及在将原始点云数据输入分割网络之前对其进行准备和转换。预处理的目的是提高数据的质量、一致性和适用性，以便进行分割任务。点云分割中的一些常见预处理方面包括数据标准化、数据增强、异常值去除和点配准。
- en: Point clouds often have varying scales, which can negatively affect the performance
    of segmentation networks. Data normalization involves scaling the point cloud
    data to a standard range or unit sphere to ensure consistent scales across different
    point. For example, the number of ShapNet object point is generally fixed as 4096\.
    For the complexity scene, early works Xu et al. ([2021](#bib.bib174)), Qi et al.
    ([2017a](#bib.bib115)) divided raw point clouds into smaller ones (e.g. 4096 points,
    1m³ blocks) so that the processing does not require large memory. However, this
    strategy might break down the semantic continuity of the scene. Recent works Qian
    et al. ([2022](#bib.bib118)), Lai et al. ([2023](#bib.bib72)), Wu et al. ([2022a](#bib.bib162)),
    Zhao et al. ([2021](#bib.bib194)), Lei et al. ([2023](#bib.bib79)) input the complete
    scene into the network, but that requires more computational sources. Moreover,
    these works tend to down sample the point cloud in the pre-processing stage.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 点云通常具有不同的尺度，这可能会对分割网络的性能产生负面影响。数据归一化涉及将点云数据缩放到标准范围或单位球体，以确保不同点之间的一致尺度。例如，ShapNet
    对象点的数量通常固定为 4096。对于复杂场景，早期的工作 Xu 等人（[2021](#bib.bib174)），Qi 等人（[2017a](#bib.bib115)）将原始点云划分为更小的点云（例如
    4096 个点，1m³ 块），以便处理不需要大量内存。然而，这种策略可能会破坏场景的语义连续性。最近的工作 Qian 等人（[2022](#bib.bib118)），Lai
    等人（[2023](#bib.bib72)），Wu 等人（[2022a](#bib.bib162)），Zhao 等人（[2021](#bib.bib194)），Lei
    等人（[2023](#bib.bib79)）将完整场景输入网络，但这需要更多的计算资源。此外，这些工作通常在预处理阶段对点云进行下采样。
- en: '![Refer to caption](img/050ff7383543ce0cbd52abeb5414ad7c.png)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/050ff7383543ce0cbd52abeb5414ad7c.png)'
- en: 'Figure 8: Evaluation performance regarding for 3D instance segmentation architecture,
    including proposal based and proposal free, on the different class of ScanNet.
    For simplicity, we omit the ‘%’ after the value.'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：关于不同类别的 ScanNet 上 3D 实例分割架构的评估性能，包括基于提案和无提案的方法。为简便起见，我们省略了值后面的“%”。
- en: 7.2 Results for 3D Instance Segmentation
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 3D 实例分割结果
- en: 'We report the results of 3D instance segmentation methods on ScanNet Dai et al.
    ([2017](#bib.bib18)) datasets, and choose mAP as the evaluation metrics. These
    results of these methods are taken from the ScanNet Benchmark Challenge website,
    and they are shown in Table [9](#S7.T9 "Table 9 ‣ 7.1 Results for 3D Semantic
    Segmentation ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A
    Survey") and summarized in Figure [8](#S7.F8 "Figure 8 ‣ 7.1 Results for 3D Semantic
    Segmentation ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A
    Survey"). The table and figure shows that:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '我们报告了在 ScanNet Dai 等人（[2017](#bib.bib18)）数据集上的 3D 实例分割方法的结果，并选择 mAP 作为评估指标。这些方法的结果取自
    ScanNet 基准挑战网站，显示在表 [9](#S7.T9 "Table 9 ‣ 7.1 Results for 3D Semantic Segmentation
    ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey") 中，并在图
    [8](#S7.F8 "Figure 8 ‣ 7.1 Results for 3D Semantic Segmentation ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey") 中总结。表格和图形显示：'
- en: ISBNet Ngo et al. ([2023](#bib.bib110)) has the state-of-the-art performance,
    with 76.3% average precision on ScanNet dataset at the time of this view. It also
    achieves the best instance segmentation performance on most classes, including
    ‘bathtub’, ‘counter’, ‘shower curtain’, ‘table’, ‘toilet’ and so on.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: ISBNet Ngo 等人（[2023](#bib.bib110)）具有最先进的性能，在查看时 ScanNet 数据集的平均精度为 76.3%。它还在大多数类别中（包括‘浴缸’，‘柜台’，‘淋浴帘’，‘桌子’，‘厕所’等）达到了最佳的实例分割性能。
- en: Most methods have better segmentation performance on large scale classes such
    as ‘bathtub’ and ‘toilet’, and have poor segmentation performance on small scale
    classes such as ‘counter’, ‘desk’ and ‘picture’. Therefore, the instance segmentation
    of small objects is a prominent challenge.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法在大规模类别（如‘浴缸’和‘厕所’）上的分割性能较好，而在小规模类别（如‘柜台’，‘桌子’和‘图片’）上的分割性能较差。因此，小物体的实例分割是一个突出的挑战。
- en: In proposal-based methods, specifically the 2D embedding propagating-based methods
    such as 3D-BEVIS Elich et al. ([2019](#bib.bib21)) and PanoticFusion Narita et al.
    ([2019](#bib.bib108)), they tend to exhibit poorer performance compared to other
    proposal-free methods. This is primarily because simple embedding propagation
    techniques are more susceptible to error labels, leading to inaccuracies in the
    instance segmentation results.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于提案的方法中，特别是 2D 嵌入传播方法，如 3D-BEVIS Elich 等人（[2019](#bib.bib21)）和 PanoticFusion
    Narita 等人（[2019](#bib.bib108)），其性能通常较其他无提案方法较差。这主要是因为简单的嵌入传播技术更容易受到错误标签的影响，从而导致实例分割结果不准确。
- en: Proposal-free methods demonstrate superior performance compared to proposal-based
    methods in instance segmentation across all classes, particularly for small objects
    like ’curtain,’ ’picture,’ ’shower curtain,’ and ’sink.’ Unlike proposal-based
    methods that rely on the accuracy of proposal generation, proposal-free methods
    circumvent this issue entirely. They directly consider the entire point cloud
    and its global features, enabling more precise and comprehensive instance segmentation.
    By avoiding the need for proposal generation, proposal-free methods can achieve
    better results by taking into account the overall context and characteristics
    of the point cloud.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 无提议方法在所有类别的实例分割中相对于基于提议的方法表现出更优的性能，特别是对于小物体如‘窗帘’，‘画’，‘淋浴帘’，和‘水槽’。不同于依赖提议生成准确性的基于提议的方法，无提议方法完全绕过了这个问题。它们直接考虑整个点云及其全局特征，实现了更精确和全面的实例分割。通过避免提议生成的需要，无提议方法可以通过考虑点云的整体上下文和特征来取得更好的结果。
- en: 7.3 Results for 3D Part Segmentation
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 3D 部分分割的结果
- en: 'We report the results of 3D part segmentation methods on ShapeNet Yi et al.
    ([2016](#bib.bib181)) datasets and use Ins. mIoU as the evaluation metric. These
    results of various methods are taken from the original papers and they are shown
    in Table [10](#S7.T10 "Table 10 ‣ 7.3 Results for 3D Part Segmentation ‣ 7 Experimental
    Results ‣ Deep Learning Based 3D Segmentation: A Survey"). We can find that part
    segmentation performance of all methods is quite similar. one underlying assumption
    is that objects in ShapeNet datasets are synthetic, normalized in scale, aligned
    in pose, and lack scene context. This makes part segmentation network difficult
    to extract rich context features. Another underlying assumption is that the point
    clouds in synthetic scene without background noise is more simpler and cleaner
    than ones in real scene, so that the geometric features of point clouds is easy
    to exploitation. The accuracy performance of various part segmentation network
    is difficult to be effectively distinguished.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '我们报告了 ShapeNet Yi 等人 ([2016](#bib.bib181)) 数据集上 3D 部分分割方法的结果，并使用 Ins. mIoU
    作为评估指标。这些方法的结果来自原始论文，并显示在表 [10](#S7.T10 "Table 10 ‣ 7.3 Results for 3D Part Segmentation
    ‣ 7 Experimental Results ‣ Deep Learning Based 3D Segmentation: A Survey")。我们可以发现所有方法的部分分割性能非常相似。一个基本假设是
    ShapeNet 数据集中的对象是合成的、经过尺度归一化、姿态对齐的，并且缺乏场景上下文。这使得部分分割网络难以提取丰富的上下文特征。另一个基本假设是合成场景中的点云没有背景噪声，比实际场景中的点云更简单、更干净，因此点云的几何特征更容易被利用。各种部分分割网络的准确性表现很难被有效区分。'
- en: 'Table 10: Evaluation performance regarding for 3D part segmentation on the
    ShapeNet. Note: the ‘%’ after the value is omitted, the symbol ‘–’ means the results
    are unavailable.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：ShapeNet 上 3D 部分分割的评估性能。注：值后面的‘%’被省略，符号‘–’表示结果不可用。
- en: '| Methods | Ins. mIoU | Methods | Ins. mIoU |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Ins. mIoU | 方法 | Ins. mIoU |'
- en: '| VV-Net Meng et al. ([2019](#bib.bib103)) | 87.4 | LatticeNet Su et al. ([2018](#bib.bib134))
    | 83.9 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| VV-Net Meng 等人 ([2019](#bib.bib103)) | 87.4 | LatticeNet Su 等人 ([2018](#bib.bib134))
    | 83.9 |'
- en: '| SSCNet Graham et al. ([2018](#bib.bib32)) | 86.0 | SGPN Wang et al. ([2018d](#bib.bib151))
    | 85.8 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| SSCNet Graham 等人 ([2018](#bib.bib32)) | 86.0 | SGPN Wang 等人 ([2018d](#bib.bib151))
    | 85.8 |'
- en: '| PointNet Qi et al. ([2017a](#bib.bib115)) | 83.7 | ShapePFCN Kalogerakis
    et al. ([2017](#bib.bib65)) | 88.4 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| PointNet Qi 等人 ([2017a](#bib.bib115)) | 83.7 | ShapePFCN Kalogerakis 等人 ([2017](#bib.bib65))
    | 88.4 |'
- en: '| PointNet++ Qi et al. ([2017b](#bib.bib116)) | 85.1 | VoxSegNet Wang and Lu
    ([2019](#bib.bib156)) | 87.5 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| PointNet++ Qi 等人 ([2017b](#bib.bib116)) | 85.1 | VoxSegNet Wang 和 Lu ([2019](#bib.bib156))
    | 87.5 |'
- en: '| 3DContextNet Zeng and Gevers ([2018](#bib.bib188)) | 84.3 | PointgridLe and
    Duan ([2018](#bib.bib76)) | 86.4 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 3DContextNet Zeng 和 Gevers ([2018](#bib.bib188)) | 84.3 | PointgridLe 和 Duan
    ([2018](#bib.bib76)) | 86.4 |'
- en: '| RSNet Huang et al. ([2018](#bib.bib54)) | 84.9 | KPConv Thomas et al. ([2019](#bib.bib139))
    | 86.4 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| RSNet Huang 等人 ([2018](#bib.bib54)) | 84.9 | KPConv Thomas 等人 ([2019](#bib.bib139))
    | 86.4 |'
- en: '| MCC Hermosilla et al. ([2018](#bib.bib46)) | 85.9 | SO-Net Li et al. ([2018a](#bib.bib81))
    | 84.9 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| MCC Hermosilla 等人 ([2018](#bib.bib46)) | 85.9 | SO-Net Li 等人 ([2018a](#bib.bib81))
    | 84.9 |'
- en: '| PointConv Wu et al. ([2019b](#bib.bib161)) | 85.7 | PartNet Yu et al. ([2019](#bib.bib185))
    | 87.4 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| PointConv Wu 等人 ([2019b](#bib.bib161)) | 85.7 | PartNet Yu 等人 ([2019](#bib.bib185))
    | 87.4 |'
- en: '| DGCNN Wang et al. ([2019b](#bib.bib155)) | 85.1 | SyncSpecCNN Yi et al. ([2017](#bib.bib182))
    | 84.7 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| DGCNN Wang 等人 ([2019b](#bib.bib155)) | 85.1 | SyncSpecCNN Yi 等人 ([2017](#bib.bib182))
    | 84.7 |'
- en: '| SPH3D-GCN Lei et al. ([2020](#bib.bib78)) | 86.8 | KCNet Yi et al. ([2017](#bib.bib182))
    | 84.7 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| SPH3D-GCN Lei 等人 ([2020](#bib.bib78)) | 86.8 | KCNet Yi 等人 ([2017](#bib.bib182))
    | 84.7 |'
- en: '| AGCN Xie et al. ([2020b](#bib.bib171)) | 85.4 | PointCNN Li et al. ([2018b](#bib.bib82))
    | 86.1 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| AGCN Xie 等人 ([2020b](#bib.bib171)) | 85.4 | PointCNN Li 等人 ([2018b](#bib.bib82))
    | 86.1 |'
- en: '| PCCN Wang et al. ([2018c](#bib.bib149)) | 85.9 | SpiderCNN Xu et al. ([2018](#bib.bib175))
    | 85.3 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| PCCN Wang 等人 ([2018c](#bib.bib149)) | 85.9 | SpiderCNN Xu 等人 ([2018](#bib.bib175))
    | 85.3 |'
- en: '| Flex-Conv Groh et al. ([2018](#bib.bib33)) | 85.0 | FeaStNet Verma et al.
    ([2018](#bib.bib141)) | 81.5 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| Flex-Conv Groh 等人 ([2018](#bib.bib33)) | 85.0 | FeaStNet Verma 等人 ([2018](#bib.bib141))
    | 81.5 |'
- en: '| $\psi$-CNN Lei et al. ([2019](#bib.bib77)) | 86.8 | Kd-Net Klokov and Lempitsky
    ([2017](#bib.bib67)) | 82.3 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| $\psi$-CNN Lei 等人 ([2019](#bib.bib77)) | 86.8 | Kd-Net Klokov 和 Lempitsky
    ([2017](#bib.bib67)) | 82.3 |'
- en: '| SPLATNet Su et al. ([2018](#bib.bib134)) | 84.6 | O-CNN Wang et al. ([2017](#bib.bib148))
    | 85.9 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| SPLATNet Su 等人 ([2018](#bib.bib134)) | 84.6 | O-CNN Wang 等人 ([2017](#bib.bib148))
    | 85.9 |'
- en: '| DRGCNN Yue et al. ([2022](#bib.bib187)) | 86.2 |  |  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| DRGCNN Yue 等人 ([2022](#bib.bib187)) | 86.2 |  |  |'
- en: 8 Discussion and Conclusion
  id: totrans-463
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论与结论
- en: 3D segmentation using deep learning techniques has made significant progress
    during recent years. However, this is just the beginning and significant developments
    lie ahead of us. Below, we present some outstanding issues and identify potential
    research directions.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度学习技术的3D分割在近年来取得了显著进展。然而，这仅仅是开始，未来还有重大发展。下面，我们提出了一些突出问题，并确定了潜在的研究方向。
- en: •
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Synthetic datasets with richer information for multiple tasks: Synthetic datasets
    gradually play an important role on semantic segmentation due to the low cost
    and diverse scenes that can be generated Brodeur et al. ([2017](#bib.bib7)), Wu
    et al. ([2018b](#bib.bib164)) compared to real datasets Dai et al. ([2017](#bib.bib18)),
    Armeni et al. ([2016](#bib.bib2)), Hackel et al. ([2017](#bib.bib38)). It is well
    known that the information contained in training data determine the upper limit
    of the scene parsing accuracy. Existing datasets lack important semantic information,
    such as material, and texture information, which is more crucial for segmentation
    with similar color or geometric information. Besides, most exiting datasets are
    generally designed for a single task. Currently, only a few semantic segmentation
    datasets also contain labels for instances Dai et al. ([2017](#bib.bib18)) and
    scene layout Song et al. ([2015](#bib.bib132)) to meet the multi-task objective.'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用于多任务的更丰富信息的合成数据集：由于低成本和能够生成的多样场景，合成数据集在语义分割中逐渐发挥了重要作用。Brodeur 等人 ([2017](#bib.bib7))
    和 Wu 等人 ([2018b](#bib.bib164)) 相较于真实数据集 Dai 等人 ([2017](#bib.bib18))、Armeni 等人
    ([2016](#bib.bib2)) 和 Hackel 等人 ([2017](#bib.bib38)) 发现，训练数据中包含的信息决定了场景解析精度的上限。现有的数据集缺乏重要的语义信息，例如材料和纹理信息，这对于具有相似颜色或几何信息的分割更加关键。此外，大多数现有的数据集通常设计为单任务。目前，只有少数语义分割数据集同时包含实例标签
    Dai 等人 ([2017](#bib.bib18)) 和场景布局 Song 等人 ([2015](#bib.bib132)) 以满足多任务目标。
- en: •
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unified network for multiple tasks: It is expensive and impractical for a system
    to accomplish different computer vision tasks by various deep learning networks.
    Towards fundamental feature exploitation of scene, Semantic segmentation has strong
    consistency with some tasks, such as depth estimation Meyer et al. ([2019](#bib.bib104)),
    Liu et al. ([2015](#bib.bib92)), Guo and Chen ([2018](#bib.bib35)), Liu et al.
    ([2018b](#bib.bib94)), scene completion Dai et al. ([2018](#bib.bib20)), Xia et al.
    ([2023](#bib.bib167)), Zhang et al. ([2023](#bib.bib191)), instance segmentation
    Liang et al. ([2019b](#bib.bib88)), Pham et al. ([2019b](#bib.bib114)), Han et al.
    ([2020](#bib.bib39)), and object detection Meyer et al. ([2019](#bib.bib104)),
    Lian et al. ([2022](#bib.bib85)). These tasks could cooperate with each other
    to improve performance in a unified network because they exhibit certain correlations
    and shared feature representations.'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多任务统一网络：通过各种深度学习网络来完成不同计算机视觉任务是昂贵且不切实际的。对于场景的基本特征开发，语义分割与一些任务具有较强的一致性，如深度估计
    Meyer 等人 ([2019](#bib.bib104))、Liu 等人 ([2015](#bib.bib92))、Guo 和 Chen ([2018](#bib.bib35))、Liu
    等人 ([2018b](#bib.bib94))、场景完成 Dai 等人 ([2018](#bib.bib20))、Xia 等人 ([2023](#bib.bib167))、Zhang
    等人 ([2023](#bib.bib191))、实例分割 Liang 等人 ([2019b](#bib.bib88))、Pham 等人 ([2019b](#bib.bib114))、Han
    等人 ([2020](#bib.bib39)) 和目标检测 Meyer 等人 ([2019](#bib.bib104))、Lian 等人 ([2022](#bib.bib85))。这些任务可以在统一网络中互相配合，以提高性能，因为它们表现出某些相关性和共享的特征表示。
- en: •
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multiple modals for segmentation: Semantic segmentation using multiple representations,
    such as projected images, voxels, and point clouds, has the potential to achieve
    higher accuracy. Single representation limits segmentation accuracy due to the
    limited scene information in some practical scenarios. For instance, LiDAR measurements
    become sparser as the distance increases, and incorporating high-resolution image
    data can improve performance on distant objects. Therefore, utilizing multiple
    representations, also known as multiple modalities, can be an alternative way
    to enhance segmentation performance Dai and Nießner ([2018](#bib.bib19)), Chiang
    et al. ([2019](#bib.bib14)), Liu et al. ([2019b](#bib.bib97)), Hu et al. ([2021](#bib.bib50)).
    Moreover, segmenting point cloud with large image models (such as SAM Kirillov
    et al. ([2023](#bib.bib66))) and natural language models like ChatGPT can be popular
    approaches. The advanced capabilities of large models enable them to capture intricate
    patterns and semantic relationships, leading to improved performance and accuracy
    in segmentation tasks.'
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模态分割：利用多种表示（如投影图像、体素和点云）的语义分割具有实现更高准确度的潜力。单一表示在某些实际场景中由于场景信息有限，限制了分割的准确性。例如，LiDAR测量在距离增加时变得更加稀疏，结合高分辨率图像数据可以提高远处物体的性能。因此，利用多种表示，也称为多模态，可以成为提升分割性能的替代方法Dai和Nießner（[2018](#bib.bib19)）、Chiang等（[2019](#bib.bib14)）、Liu等（[2019b](#bib.bib97)）、Hu等（[2021](#bib.bib50)）。此外，使用大型图像模型（如SAM
    Kirillov等（[2023](#bib.bib66)））和自然语言模型如ChatGPT进行点云分割也可以是流行的方法。大型模型的先进能力使其能够捕捉复杂的模式和语义关系，从而在分割任务中提高性能和准确度。
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Interpretable and sparse feature abstraction: Various features abstraction,
    including MLP, Convolution and Transformer, have undergone significant development.
    Feature abstraction modules may prioritize generating interpretable feature representations,
    enabling them to provide explanations for model decisions, visualizations of points
    of interest, and other interpretability functions. Moreover, in scenarios involving
    large-scale data and limited resources, the feature abstraction module improve
    computational efficiency.'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可解释性和稀疏特征抽象：各种特征抽象方法，包括MLP、卷积和Transformer，已经取得了显著的发展。特征抽象模块可能会优先生成可解释的特征表示，从而提供对模型决策的解释、兴趣点的可视化以及其他可解释性功能。此外，在涉及大规模数据和有限资源的场景中，特征抽象模块能够提高计算效率。
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weakly-supervised and unsupervised segmentation: Deep learning has gained significant
    success in 3D segmentation, but heavily hinges on large-scale labelled training
    samples. Weakly-supervised learning refers to a training approach where the model
    is trained with limited or incomplete supervision. Unsupervised learning only
    use unlabelled training samples. weakly-supervised Su et al. ([2023](#bib.bib136)),
    Shi et al. ([2022](#bib.bib128)) and unsupervised Xiao et al. ([2023](#bib.bib169))
    paradigms are considered as an alternative to relax the impractical requirement
    of large-scale labelled datasets.'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 弱监督和无监督分割：深度学习在3D分割中取得了显著成功，但主要依赖于大规模标注训练样本。弱监督学习指的是一种训练方法，其中模型在有限或不完整的监督下进行训练。无监督学习仅使用未标注的训练样本。弱监督Su等（[2023](#bib.bib136)）、Shi等（[2022](#bib.bib128)）和无监督Xiao等（[2023](#bib.bib169)）范式被认为是放宽大规模标注数据集不切实际要求的替代方案。
- en: •
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Real-time and incremental segmentation: Real-time 3D scene parsing is crucial
    for some applications such as autonomous driving and mobile robots. However, most
    existing 3D semantic segmentation methods mainly focus on the improvement of segmentation
    accuracy but rarely focus on real-time performance. A few lightweight 3D semantic
    segmentation networks realize real-time by pre-processing point clouds into other
    presentations such as projected images Wu et al. ([2018a](#bib.bib159)), Wu et al.
    ([2019a](#bib.bib160)), Park et al. ([2023](#bib.bib112)), Ando et al. ([2023](#bib.bib1)).
    Additionally, incremental segmentation will become an important research direction,
    allowing models to incrementally update and adapt in dynamic scenes.'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时和增量分割：实时3D场景解析对于某些应用（如自动驾驶和移动机器人）至关重要。然而，大多数现有的3D语义分割方法主要关注分割准确性的提升，但很少关注实时性能。一些轻量级的3D语义分割网络通过将点云预处理成其他表示（如投影图像）来实现实时Wu等（[2018a](#bib.bib159)）、Wu等（[2019a](#bib.bib160)）、Park等（[2023](#bib.bib112)）、Ando等（[2023](#bib.bib1)）。此外，增量分割将成为一个重要的研究方向，使模型能够在动态场景中逐步更新和适应。
- en: •
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '3D video semantic segmentation: Like 2D video semantic segmentation, A handful
    of works try to exploit 4D spatio-temporal features on 3D videos (also call 4D
    point clouds) Wei et al. ([2022](#bib.bib158)), Fan et al. ([2021](#bib.bib28)).
    From these works, it can be seen that the spatio-temporal features can help improve
    the robustness of 3D video or dynamic 3D scene semantic segmentation.'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D 视频语义分割：与 2D 视频语义分割类似，一些研究尝试利用 4D 时空特征进行 3D 视频（也称为 4D 点云）的分析 Wei 等（[2022](#bib.bib158)），Fan
    等（[2021](#bib.bib28)）。从这些研究可以看出，时空特征可以帮助提高 3D 视频或动态 3D 场景语义分割的鲁棒性。
- en: We provided a comprehensive survey of the recent development in 3D segmentation
    using deep learning techniques, including 3D semantic segmentation, 3D instance
    segmentation and 3D part segmentation. We presented a comprehensive performance
    comparison and merit of various methods in each category, with potential research
    directions being listed.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了使用深度学习技术在 3D 分割领域的最新进展的全面综述，包括 3D 语义分割、3D 实例分割和 3D 部分分割。我们展示了各种方法在每个类别中的全面性能比较和优点，并列出了潜在的研究方向。
- en: References
  id: totrans-480
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ando et al. (2023) Ando, A., Gidaris, S., Bursuc, A., Puy, G., Boulch, A.,
    Marlet, R., 2023. Rangevit: Towards vision transformers for 3d semantic segmentation
    in autonomous driving, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 5240–5250.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ando 等（2023）Ando, A., Gidaris, S., Bursuc, A., Puy, G., Boulch, A., Marlet,
    R., 2023. Rangevit: 面向自动驾驶的 3D 语义分割的视觉变换器，见：IEEE/CVF 计算机视觉与模式识别会议论文集，第 5240–5250
    页。'
- en: 'Armeni et al. (2016) Armeni, I., Sener, O., Zamir, A., Jiang, H., Brilakis,
    I., Fischer, M., Savarese, S., 2016. 3d semantic parsing of large-scale indoor
    spaces, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 1534–1543.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Armeni 等（2016）Armeni, I., Sener, O., Zamir, A., Jiang, H., Brilakis, I., Fischer,
    M., Savarese, S., 2016. 大规模室内空间的 3D 语义解析，见：IEEE/CVF 计算机视觉与模式识别会议论文集，第 1534–1543
    页。
- en: 'Behley et al. (2019) Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke,
    S., Stachniss, C., Gall, J., 2019. Semantickitti: A dataset for semantic scene
    understanding of lidar sequences, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, IEEE. pp. 9297–9307.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Behley 等（2019）Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S.,
    Stachniss, C., Gall, J., 2019. Semantickitti: 用于激光雷达序列语义场景理解的数据集，见：IEEE/CVF 国际计算机视觉会议论文集，IEEE。第
    9297–9307 页。'
- en: Bello et al. (2020) Bello, S., Yu, S., Wang, C., Adam, J., Li, J., 2020. deep
    learning on 3d point clouds. Remote Sensing 12, 1729.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bello 等（2020）Bello, S., Yu, S., Wang, C., Adam, J., Li, J., 2020. 深度学习在 3D 点云上的应用。遥感
    12, 1729。
- en: 'Boulch et al. (2018) Boulch, A., Guerry, J., Le Saux, B., Audebert, N., 2018.
    Snapnet: 3d point cloud semantic labeling with 2d deep segmentation networks.
    Computers & Graphics 71, 189–198.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Boulch 等（2018）Boulch, A., Guerry, J., Le Saux, B., Audebert, N., 2018. Snapnet:
    使用 2D 深度分割网络进行 3D 点云语义标注。Computers & Graphics 71, 189–198。'
- en: Boulch et al. (2017) Boulch, A., Le Saux, B., Audebert, N., 2017. Unstructured
    point cloud semantic labeling using deep segmentation networks. 3dor@ eurographics
    2, 7.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boulch 等（2017）Boulch, A., Le Saux, B., Audebert, N., 2017. 使用深度分割网络进行无结构点云语义标注。3dor@eurographics
    2, 7。
- en: 'Brodeur et al. (2017) Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti,
    L., Strub, F., Rouat, J., Larochelle, H., Courville, A., 2017. Home: A household
    multimodal environment. arXiv preprint arXiv:1711.11017 .'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brodeur 等（2017）Brodeur, S., Perez, E., Anand, A., Golemo, F., Celotti, L.,
    Strub, F., Rouat, J., Larochelle, H., Courville, A., 2017. Home: 一种家庭多模态环境。arXiv
    预印本 arXiv:1711.11017。'
- en: Cao et al. (2016) Cao, Y., Shen, C., Shen, H., 2016. Exploiting depth from single
    monocular images for object detection and semantic segmentation. IEEE Transactions
    on Image Processing 26, 836–846.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等（2016）Cao, Y., Shen, C., Shen, H., 2016. 利用单目图像深度进行物体检测和语义分割。IEEE 图像处理学报
    26, 836–846。
- en: 'Chang et al. (2017) Chang, A., Dai, A., Funkhouser, T., Halber, M., Niebner,
    M., Savva, M., Song, S., Zeng, A., Zhang, Y., 2017. Matterport3d: Learning from
    rgb-d data in indoor environments, in: 2017 International Conference on 3D Vision
    (3DV), IEEE. pp. 667–676.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等（2017）Chang, A., Dai, A., Funkhouser, T., Halber, M., Niebner, M., Savva,
    M., Song, S., Zeng, A., Zhang, Y., 2017. Matterport3d: 从室内环境中的 RGB-D 数据学习，见：2017
    国际 3D 视觉会议（3DV），IEEE。第 667–676 页。'
- en: 'Chen et al. (2021) Chen, S., Fang, J., Zhang, Q., Liu, W., Wang, X., 2021.
    Hierarchical aggregation for 3d instance segmentation, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pp. 15467–15476.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021）Chen, S., Fang, J., Zhang, Q., Liu, W., Wang, X., 2021. 用于 3D 实例分割的层次聚合，见：IEEE/CVF
    国际计算机视觉会议论文集，第 15467–15476 页。
- en: Chen et al. (2009) Chen, X., Golovinskiy, A., Funkhouser, T., 2009. A benchmark
    for 3d mesh segmentation. ACM Transactions on Graphics 28, 1–12.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2009) Chen, X., Golovinskiy, A., Funkhouser, T., 2009. 3D 网格分割的基准。ACM
    图形学会会刊 28, 1–12。
- en: Cheng et al. (2020) Cheng, J., Sun, Y., Meng, M.Q.H., 2020. Robust semantic
    mapping in challenging environments. Robotica 38, 256–270.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 (2020) Cheng, J., Sun, Y., Meng, M.Q.H., 2020. 在挑战性环境中的鲁棒语义映射。Robotica
    38, 256–270。
- en: 'Cheng et al. (2017) Cheng, Y., Cai, R., Li, Z., Zhao, X., Huang, K., 2017.
    Locality-sensitive deconvolution networks with gated fusion for rgb-d indoor semantic
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 3029–3037.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 (2017) Cheng, Y., Cai, R., Li, Z., Zhao, X., Huang, K., 2017. 带门控融合的局部敏感去卷积网络用于
    RGB-D 室内语义分割，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 3029–3037 页。
- en: 'Chiang et al. (2019) Chiang, H.Y., Lin, Y.L., Liu, Y.C., Hsu, W.H., 2019. A
    unified point-based framework for 3d segmentation, in: Processing of the International
    Conference on 3D Vision, IEEE. pp. 155–163.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等人 (2019) Chiang, H.Y., Lin, Y.L., Liu, Y.C., Hsu, W.H., 2019. 统一的基于点的
    3D 分割框架，发表于国际 3D 视觉会议论文集，IEEE，第 155–163 页。
- en: 'Chollet (2017) Chollet, F., 2017. Xception: Deep learning with depthwise separable
    convolutions, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 1251–1258.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chollet (2017) Chollet, F., 2017. Xception: 使用深度可分离卷积的深度学习，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第
    1251–1258 页。'
- en: 'Choy et al. (2019) Choy, C., Gwak, J., Savarese, S., 2019. 4d spatio-temporal
    convnets: Minkowski convolutional neural networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 3075–3084.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choy 等人 (2019) Choy, C., Gwak, J., Savarese, S., 2019. 4D 时空卷积网络: Minkowski
    卷积神经网络，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 3075–3084 页。'
- en: Couprie et al. (2013) Couprie, C., Farabet, C., Najman, L., LeCun, Y., 2013.
    Indoor semantic segmentation using depth information. arXiv preprint arXiv:1301.3572
    .
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Couprie 等人 (2013) Couprie, C., Farabet, C., Najman, L., LeCun, Y., 2013. 使用深度信息的室内语义分割。arXiv
    预印本 arXiv:1301.3572。
- en: 'Dai et al. (2017) Dai, A., Chang, A., Savva, M., Halber, M., Funkhouser, T.,
    Nießner, M., 2017. Scannet: Richly-annotated 3d reconstructions of indoor scenes,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 5828–5839.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等人 (2017) Dai, A., Chang, A., Savva, M., Halber, M., Funkhouser, T., Nießner,
    M., 2017. Scannet: 丰富注释的室内场景 3D 重建，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 5828–5839 页。'
- en: 'Dai and Nießner (2018) Dai, A., Nießner, M., 2018. 3dmv: Joint 3d-multi-view
    prediction for 3d semantic scene segmentation, in: Proceedings of the European
    Conference on Computer Vision, pp. 452–468.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 和 Nießner (2018) Dai, A., Nießner, M., 2018. 3DMV: 联合 3D 多视图预测用于 3D 语义场景分割，发表于欧洲计算机视觉会议论文集，第
    452–468 页。'
- en: 'Dai et al. (2018) Dai, A., Ritchie, D., Bokeloh, M., Reed, S., Sturm, J., Nießner,
    M., 2018. Scancomplete: Large-scale scene completion and semantic segmentation
    for 3d scans, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 4578–4587.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等人 (2018) Dai, A., Ritchie, D., Bokeloh, M., Reed, S., Sturm, J., Nießner,
    M., 2018. Scancomplete: 大规模场景补全和 3D 扫描的语义分割，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 4578–4587
    页。'
- en: 'Elich et al. (2019) Elich, C., Engelmann, F., Kontogianni, T., Leibe, B., 2019.
    3d bird’s-eye-view instance segmentation, in: German Conference on Pattern Recognition,
    Springer. pp. 48–61.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elich 等人 (2019) Elich, C., Engelmann, F., Kontogianni, T., Leibe, B., 2019.
    3D 鸟瞰图实例分割，发表于德国模式识别会议，Springer，第 48–61 页。
- en: 'Emre Yurdakul and Yemez (2017) Emre Yurdakul, E., Yemez, Y., 2017. Semantic
    segmentation of rgbd videos with recurrent fully convolutional neural networks,
    in: Processings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 367–374.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Emre Yurdakul 和 Yemez (2017) Emre Yurdakul, E., Yemez, Y., 2017. 使用递归全卷积神经网络的
    RGBD 视频语义分割，发表于 IEEE/CVF 国际计算机视觉会议研讨会论文集，第 367–374 页。
- en: 'Engelmann et al. (2020a) Engelmann, F., Bokeloh, M., Fathi, A., Leibe, B.,
    Nießner, M., 2020a. 3d-mpa: Multi-proposal aggregation for 3d semantic instance
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 9031–9040.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Engelmann 等人 (2020a) Engelmann, F., Bokeloh, M., Fathi, A., Leibe, B., Nießner,
    M., 2020a. 3D-MPA: 3D 语义实例分割的多提议聚合，发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 9031–9040 页。'
- en: 'Engelmann et al. (2017) Engelmann, F., Kontogianni, T., Hermans, A., Leibe,
    B., 2017. Exploring spatial context for 3d semantic segmentation of point clouds,
    in: Processings of the IEEE/CVF International Conference on Computer Vision Workshops,
    pp. 716–724.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engelmann等（2017）Engelmann, F., Kontogianni, T., Hermans, A., Leibe, B., 2017。探索点云的3D语义分割的空间上下文，见：IEEE/CVF国际计算机视觉会议论文集，第716–724页。
- en: 'Engelmann et al. (2020b) Engelmann, F., Kontogianni, T., Leibe, B., 2020b.
    Dilated point convolutions: On the receptive field size of point convolutions
    on 3d point clouds, in: 2020 IEEE International Conference on Robotics and Automation
    (ICRA), IEEE. pp. 9463–9469.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engelmann等（2020b）Engelmann, F., Kontogianni, T., Leibe, B., 2020b。膨胀点卷积：3D点云上点卷积的感受野大小，见：2020
    IEEE国际机器人与自动化大会（ICRA），IEEE，第9463–9469页。
- en: 'Engelmann et al. (2018) Engelmann, F., Kontogianni, T., Schult, J., Leibe,
    B., 2018. Know what your neighbors do: 3d semantic segmentation of point clouds,
    in: Proceedings of the European Conference on Computer Vision, pp. 0–0.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engelmann等（2018）Engelmann, F., Kontogianni, T., Schult, J., Leibe, B., 2018。了解你邻居的行为：点云的3D语义分割，见：欧洲计算机视觉会议论文集，第0–0页。
- en: 'Fan et al. (2017) Fan, H., Mei, X., Prokhorov, D., Ling, H., 2017. Rgb-d scene
    labeling with multimodal recurrent neural networks, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops, pp. 9–17.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等（2017）Fan, H., Mei, X., Prokhorov, D., Ling, H., 2017。利用多模态递归神经网络进行RGB-D场景标注，见：IEEE/CVF计算机视觉与模式识别会议论文集，第9–17页。
- en: 'Fan et al. (2021) Fan, H., Yang, Y., Kankanhalli, M., 2021. Point 4d transformer
    networks for spatio-temporal modeling in point cloud videos, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, pp. 14204–14213.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan等（2021）Fan, H., Yang, Y., Kankanhalli, M., 2021。用于点云视频的时空建模的点4D变换器网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，第14204–14213页。
- en: Feng et al. (2020) Feng, M., Zhang, L., Lin, X., Gilani, S.Z., Mian, A., 2020.
    Point attention network for semantic segmentation of 3d point clouds. Pattern
    Recognition , 107446.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng等（2020）Feng, M., Zhang, L., Lin, X., Gilani, S.Z., Mian, A., 2020。点注意力网络用于3D点云的语义分割。模式识别，107446。
- en: 'Fooladgar and Kasaei (2020) Fooladgar, F., Kasaei, S., 2020. A survey on indoor
    rgb-d semantic segmentation: from hand-crafted features to deep convolutional
    neural networks. Multimedia Tools and Applications 79, 4499–4524.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fooladgar和Kasaei（2020）Fooladgar, F., Kasaei, S., 2020。室内RGB-D语义分割的综述：从手工特征到深度卷积神经网络。多媒体工具与应用，79，4499–4524。
- en: 'Geiger et al. (2012) Geiger, A., Lenz, P., Urtasun, R., 2012. Are we ready
    for autonomous driving? the kitti vision benchmark suite, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE. pp. 3354–3361.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiger等（2012）Geiger, A., Lenz, P., Urtasun, R., 2012。我们准备好进行自动驾驶了吗？kitti视觉基准套件，见：IEEE/CVF计算机视觉与模式识别会议论文集，IEEE，第3354–3361页。
- en: 'Graham et al. (2018) Graham, B., Engelcke, M., Van Der Maaten, L., 2018. 3d
    semantic segmentation with submanifold sparse convolutional networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9224–9232.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graham等（2018）Graham, B., Engelcke, M., Van Der Maaten, L., 2018。利用子流形稀疏卷积网络进行3D语义分割，见：IEEE/CVF计算机视觉与模式识别会议论文集，第9224–9232页。
- en: 'Groh et al. (2018) Groh, F., Wieschollek, P., Lensch, H.P., 2018. Flex-convolution,
    in: Asian Conference on Computer Vision, Springer. pp. 105–122.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Groh等（2018）Groh, F., Wieschollek, P., Lensch, H.P., 2018。Flex-convolution，见：亚洲计算机视觉大会，Springer，第105–122页。
- en: 'Guerry et al. (2017) Guerry, J., Boulch, A., Le Saux, B., Moras, J., Plyer,
    A., Filliat, D., 2017. Snapnet-r: Consistent 3d multi-view semantic labeling for
    robotics, in: Processings of the IEEE/CVF International Conference on Computer
    Vision Workshops, pp. 669–678.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guerry等（2017）Guerry, J., Boulch, A., Le Saux, B., Moras, J., Plyer, A., Filliat,
    D., 2017。Snapnet-r：一致的3D多视角语义标注用于机器人，见：IEEE/CVF国际计算机视觉会议论文集，第669–678页。
- en: Guo and Chen (2018) Guo, Y., Chen, T., 2018. Semantic segmentation of rgbd images
    based on deep depth regression. Pattern Recognit. Lett. 109, 55–64.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo和Chen（2018）Guo, Y., Chen, T., 2018。基于深度回归的RGBD图像语义分割。模式识别快报，109，55–64。
- en: 'Guo et al. (2020) Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.,
    2020. Deep learning for 3d point clouds: A survey. IEEE Transactions on Pattern
    Analysis and Machine Intelligence .'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2020）Guo, Y., Wang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M., 2020。深度学习在3D点云中的应用：综述。IEEE模式分析与机器智能汇刊。
- en: 'Gupta et al. (2014) Gupta, S., Girshick, R., Arbeláez, P., Malik, J., 2014.
    Learning rich features from rgb-d images for object detection and segmentation,
    in: Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part VII 13, Springer. pp. 345–360.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等（2014）Gupta, S., Girshick, R., Arbeláez, P., Malik, J., 2014. 从 RGB-D
    图像中学习丰富的特征以进行目标检测和分割，发表于计算机视觉–ECCV 2014：第 13 届欧洲会议，瑞士苏黎世，2014 年 9 月 6-12 日，论文集，第
    VII 部分 13，Springer. 页码 345–360。
- en: 'Hackel et al. (2017) Hackel, T., Savinov, N., Ladicky, L., Wegner, J., Schindler,
    K., Pollefeys, M., 2017. Semantic3d. net: A new large-scale point cloud classification
    benchmark. arXiv preprint arXiv:1704.03847 .'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hackel 等（2017）Hackel, T., Savinov, N., Ladicky, L., Wegner, J., Schindler,
    K., Pollefeys, M., 2017. Semantic3d. net: 一个新的大规模点云分类基准。arXiv 预印本 arXiv:1704.03847。'
- en: 'Han et al. (2020) Han, L., Zheng, T., Xu, L., Fang, L., 2020. Occuseg: Occupancy-aware
    3d instance segmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 2940–2949.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Han 等（2020）Han, L., Zheng, T., Xu, L., Fang, L., 2020. Occuseg: 关注占用的 3D 实例分割，发表于
    IEEE/CVF 计算机视觉与模式识别大会论文集，页码 2940–2949。'
- en: 'Hanocka et al. (2019) Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman,
    S., D., C.O., 2019. Meshcnn: a network with an edge. ACM Transactions on Graphics
    38, 1–12.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hanocka 等（2019）Hanocka, R., Hertz, A., Fish, N., Giryes, R., Fleishman, S.,
    D., C.O., 2019. Meshcnn: 一个边缘网络。ACM 图形学汇刊 38, 1–12。'
- en: 'Hazirbas et al. (2016) Hazirbas, C., Ma, L., Domokos, C., Cremers, D., 2016.
    Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture,
    in: Proc. Asian Conf. Compu. Vis., Springer. pp. 213–228.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hazirbas 等（2016）Hazirbas, C., Ma, L., Domokos, C., Cremers, D., 2016. Fusenet:
    通过基于融合的 CNN 架构将深度融入语义分割，发表于亚洲计算机视觉会议论文集，Springer. 页码 213–228。'
- en: 'He et al. (2017a) He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017a. Mask
    r-cnn, in: Processings of the IEEE/CVF International Conference on Computer Vision,
    pp. 2961–2969.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2017a）He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017a. Mask R-CNN，发表于
    IEEE/CVF 国际计算机视觉大会论文集，页码 2961–2969。
- en: 'He et al. (2021) He, T., Shen, C., Van Den Hengel, A., 2021. Dyco3d: Robust
    instance segmentation of 3d point clouds through dynamic convolution, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pp. 354–363.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等（2021）He, T., Shen, C., Van Den Hengel, A., 2021. Dyco3d: 通过动态卷积进行鲁棒的 3D
    点云实例分割，发表于 IEEE/CVF 计算机视觉与模式识别大会论文集，页码 354–363。'
- en: 'He et al. (2022) He, T., Yin, W., Shen, C., van den Hengel, A., 2022. Pointinst3d:
    Segmenting 3d instances by points, in: Proceedings of the European Conference
    on Computer Vision, Springer. pp. 286–302.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等（2022）He, T., Yin, W., Shen, C., van den Hengel, A., 2022. Pointinst3d:
    通过点进行 3D 实例分割，发表于欧洲计算机视觉大会论文集，Springer. 页码 286–302。'
- en: 'He et al. (2017b) He, Y., Chiu, W.C., Keuper, M., Fritz, M., 2017b. Std2p:
    Rgbd semantic segmentation using spatio-temporal data-driven pooling, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4837–4846.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等（2017b）He, Y., Chiu, W.C., Keuper, M., Fritz, M., 2017b. Std2p: 使用时空数据驱动池进行
    RGB-D 语义分割，发表于 IEEE/CVF 计算机视觉与模式识别大会论文集，页码 4837–4846。'
- en: Hermosilla et al. (2018) Hermosilla, P., Ritschel, T., Vázquez, P.P., Vinacua,
    À., Ropinski, T., 2018. Monte carlo convolution for learning on non-uniformly
    sampled point clouds. ACM Transactions on Graphics 37, 1–12.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hermosilla 等（2018）Hermosilla, P., Ritschel, T., Vázquez, P.P., Vinacua, À.,
    Ropinski, T., 2018. 非均匀采样点云的蒙特卡罗卷积学习。ACM 图形学汇刊 37, 1–12。
- en: 'Höft et al. (2014) Höft, N., Schulz, H., Behnke, S., 2014. Fast semantic segmentation
    of rgb-d scenes with gpu-accelerated deep neural networks, in: Joint German/Austrian
    Conference on Artificial Intelligence, Springer. pp. 80–85.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Höft 等（2014）Höft, N., Schulz, H., Behnke, S., 2014. 使用 GPU 加速深度神经网络进行 RGB-D
    场景的快速语义分割，发表于德国/奥地利人工智能联合会议，Springer. 页码 80–85。
- en: 'Hou et al. (2019) Hou, J., Dai, A., Nießner, M., 2019. 3d-sis: 3d semantic
    instance segmentation of rgb-d scans, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 4421–4430.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou 等（2019）Hou, J., Dai, A., Nießner, M., 2019. 3d-sis: RGB-D 扫描的 3D 语义实例分割，发表于
    IEEE/CVF 计算机视觉与模式识别大会论文集，页码 4421–4430。'
- en: 'Hu et al. (2020) Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni,
    N., Markham, A., 2020. Randla-net: Efficient semantic segmentation of large-scale
    point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 11108–11117.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2020）Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., Trigoni,
    N., Markham, A., 2020. Randla-net: 大规模点云的高效语义分割，发表于 IEEE/CVF 计算机视觉与模式识别大会论文集，页码
    11108–11117。'
- en: 'Hu et al. (2021) Hu, W., Zhao, H., Jiang, L., Jia, J., Wong, T.T., 2021. Bidirectional
    projection network for cross dimension scene understanding, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14373–14382.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等（2021）Hu, W., Zhao, H., Jiang, L., Jia, J., Wong, T.T., 2021. 用于跨维度场景理解的双向投影网络，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：14373–14382。
- en: 'Hua et al. (2016) Hua, B., Pham, Q., Nguyen, D., Tran, M., Yu, L., Yeung, S.,
    2016. Scenenn: A scene meshes dataset with annotations, in: Processing of the
    International Conference on 3D Vision, IEEE. pp. 92–101.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua等（2016）Hua, B., Pham, Q., Nguyen, D., Tran, M., Yu, L., Yeung, S., 2016.
    SceneNN：带注释的场景网格数据集，发表于：国际3D视觉会议论文集，IEEE，页码：92–101。
- en: 'Hua et al. (2018) Hua, B.S., Tran, M.K., Yeung, S.K., 2018. Pointwise convolutional
    neural networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 984–993.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua等（2018）Hua, B.S., Tran, M.K., Yeung, S.K., 2018. 点wise卷积神经网络，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：984–993。
- en: 'Huang and You (2016) Huang, J., You, S., 2016. Point cloud labeling using 3d
    convolutional neural network, in: 2016 23rd International Conference on Pattern
    Recognition (ICPR), IEEE. pp. 2670–2675.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang和You（2016）Huang, J., You, S., 2016. 使用3D卷积神经网络进行点云标注，发表于：2016年第23届国际模式识别大会（ICPR），IEEE，页码：2670–2675。
- en: 'Huang et al. (2018) Huang, Q., Wang, W., Neumann, U., 2018. Recurrent slice
    networks for 3d segmentation of point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2626–2635.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等（2018）Huang, Q., Wang, W., Neumann, U., 2018. 用于点云3D分割的递归切片网络，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：2626–2635。
- en: 'Iandola et al. (2016) Iandola, F., Han, S., Moskewicz, M., Ashraf, K., Dally,
    W., Keutzer, K., 2016. Squeezenet: Alexnet-level accuracy with 50x fewer parameters
    and< 0.5 mb model size. arXiv preprint arXiv:1602.07360 .'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iandola等（2016）Iandola, F., Han, S., Moskewicz, M., Ashraf, K., Dally, W., Keutzer,
    K., 2016. SqueezeNet：AlexNet级别的准确性，参数减少50倍且模型大小< 0.5 MB。arXiv预印本 arXiv:1602.07360。
- en: 'Ioannidou et al. (2017) Ioannidou, A., Chatzilari, E., Nikolopoulos, S., Kompatsiaris,
    I., 2017. Deep learning advances in computer vision with 3d data: A survey. ACM
    Computing Surveys 50, 1–38.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioannidou等（2017）Ioannidou, A., Chatzilari, E., Nikolopoulos, S., Kompatsiaris,
    I., 2017. 深度学习在计算机视觉与3D数据中的进展：综述。ACM计算机调查 50, 1–38。
- en: Ivaneckỳ (2016) Ivaneckỳ, B.J., 2016. Depth estimation by convolutional neural
    networks. Ph.D. thesis. Master thesis, Brno University of Technology.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivaneckỳ（2016）Ivaneckỳ, B.J., 2016. 通过卷积神经网络进行深度估计。博士论文。布尔诺科技大学硕士论文。
- en: 'Jampani et al. (2016) Jampani, V., Kiefel, M., Gehler, P.V., 2016. Learning
    sparse high dimensional filters: Image filtering, dense crfs and bilateral neural
    networks, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4452–4461.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jampani等（2016）Jampani, V., Kiefel, M., Gehler, P.V., 2016. 学习稀疏高维滤波器：图像滤波、稠密CRFs和双边神经网络，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：4452–4461。
- en: 'Jaritz et al. (2019) Jaritz, M., Gu, J., Su, H., 2019. Multi-view pointnet
    for 3d scene understanding, in: Processings of the IEEE/CVF International Conference
    on Computer Vision Workshops, pp. 0–0.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaritz等（2019）Jaritz, M., Gu, J., Su, H., 2019. 多视角PointNet用于3D场景理解，发表于：IEEE/CVF国际计算机视觉会议工作坊论文集，页码：0–0。
- en: Jeong et al. (2018) Jeong, J., Yoon, T.S., Park, J.B., 2018. Multimodal sensor-based
    semantic 3d mapping for a large-scale environment. Expert Systems with Applications
    105, 1–10.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeong等（2018）Jeong, J., Yoon, T.S., Park, J.B., 2018. 基于多模态传感器的语义3D映射用于大规模环境。应用专家系统
    105, 1–10。
- en: 'Jiang et al. (2020a) Jiang, H., Yan, F., Cai, J., Zheng, J., Xiao, J., 2020a.
    End-to-end 3d point cloud instance segmentation without detection, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12796–12805.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2020a）Jiang, H., Yan, F., Cai, J., Zheng, J., Xiao, J., 2020a. 端到端3D点云实例分割无检测，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，页码：12796–12805。
- en: 'Jiang et al. (2017) Jiang, J., Zhang, Z., Huang, Y., Zheng, L., 2017. Incorporating
    depth into both cnn and crf for indoor semantic segmentation, in: Processing of
    the IEEE International Conference on Software Engineering and Service Science,
    IEEE. pp. 525–530.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等（2017）Jiang, J., Zhang, Z., Huang, Y., Zheng, L., 2017. 将深度信息融入CNN和CRF进行室内语义分割，发表于：IEEE国际软件工程与服务科学会议论文集，IEEE，页码：525–530。
- en: 'Jiang et al. (2020b) Jiang, L., Zhao, H., Shi, S., Liu, S., Fu, C., Jia, J.,
    2020b. Pointgroup: Dual-set point grouping for 3d instance segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4867–4876.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2020b）Jiang, L., Zhao, H., Shi, S., Liu, S., Fu, C., Jia, J., 2020b.
    Pointgroup：用于 3d 实例分割的双集点分组，见：IEEE/CVF 计算机视觉与模式识别大会论文集，页 4867–4876。
- en: 'Jiang et al. (2018) Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., 2018. Pointsift:
    A sift-like network module for 3d point cloud semantic segmentation. arXiv preprint
    arXiv:1807.00652 .'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2018）Jiang, M., Wu, Y., Zhao, T., Zhao, Z., Lu, C., 2018. Pointsift：一种类似于
    SIFT 的网络模块用于 3d 点云语义分割，arXiv 预印本 arXiv:1807.00652。
- en: 'Kalogerakis et al. (2017) Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri,
    S., 2017. 3d shape segmentation with projective convolutional networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3779–3788.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalogerakis 等（2017）Kalogerakis, E., Averkiou, M., Maji, S., Chaudhuri, S., 2017.
    使用投影卷积网络进行 3d 形状分割，见：IEEE/CVF 计算机视觉与模式识别大会论文集，页 3779–3788。
- en: Kirillov et al. (2023) Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland,
    C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., et al., 2023.
    Segment anything. arXiv preprint arXiv:2304.02643 .
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov 等（2023）Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson,
    L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., 等，2023. 任何东西都要分割。arXiv 预印本
    arXiv:2304.02643。
- en: 'Klokov and Lempitsky (2017) Klokov, R., Lempitsky, V., 2017. Escape from cells:
    Deep kd-networks for the recognition of 3d point cloud models, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 863–872.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Klokov 和 Lempitsky（2017）Klokov, R., Lempitsky, V., 2017. 摆脱网格：用于 3d 点云模型识别的深度
    kd 网络，见：IEEE/CVF 国际计算机视觉大会论文集，页 863–872。
- en: 'Kochanov et al. (2016) Kochanov, D., Ošep, A., Stückler, J., Leibe, B., 2016.
    Scene flow propagation for semantic mapping and object discovery in dynamic street
    scenes, in: Proc. IEEE Int. Conf. Intell. Rob. Syst., IEEE. pp. 1785–1792.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kochanov 等（2016）Kochanov, D., Ošep, A., Stückler, J., Leibe, B., 2016. 场景流传播用于语义映射和动态街景中的物体发现，见：IEEE
    智能机器人系统国际会议论文集，IEEE. 页 1785–1792。
- en: 'Komarichev et al. (2019) Komarichev, A., Zhong, Z., Hua, J., 2019. A-cnn: Annularly
    convolutional neural networks on point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 7421–7430.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komarichev 等（2019）Komarichev, A., Zhong, Z., Hua, J., 2019. A-cnn：用于点云的环状卷积神经网络，见：IEEE/CVF
    计算机视觉与模式识别大会论文集，页 7421–7430。
- en: Kong et al. (2023) Kong, L., Liu, Y., Chen, R., Ma, Y., Zhu, X., Li, Y., Hou,
    Y., Qiao, Y., Liu, Z., 2023. Rethinking range view representation for lidar segmentation.
    arXiv preprint arXiv:2303.05367 .
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等（2023）Kong, L., Liu, Y., Chen, R., Ma, Y., Zhu, X., Li, Y., Hou, Y., Qiao,
    Y., Liu, Z., 2023. 重新思考激光雷达分割的范围视图表示。arXiv 预印本 arXiv:2303.05367。
- en: 'Lahoud et al. (2019) Lahoud, J., Ghanem, B., Pollefeys, M., Oswald, M., 2019.
    3d instance segmentation via multi-task metric learning, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 9256–9266.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lahoud 等（2019）Lahoud, J., Ghanem, B., Pollefeys, M., Oswald, M., 2019. 通过多任务度量学习进行
    3d 实例分割，见：IEEE/CVF 国际计算机视觉大会论文集，页 9256–9266。
- en: 'Lai et al. (2023) Lai, X., Chen, Y., Lu, F., Liu, J., Jia, J., 2023. Spherical
    transformer for lidar-based 3d recognition, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 17545–17555.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等（2023）Lai, X., Chen, Y., Lu, F., Liu, J., Jia, J., 2023. 基于球形 Transformer
    的激光雷达 3d 识别，见：IEEE/CVF 计算机视觉与模式识别大会论文集，页 17545–17555。
- en: 'Lai et al. (2022) Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S.,
    Qi, X., Jia, J., 2022. Stratified transformer for 3d point cloud segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 8500–8509.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等（2022）Lai, X., Liu, J., Jiang, L., Wang, L., Zhao, H., Liu, S., Qi, X.,
    Jia, J., 2022. 用于 3d 点云分割的分层 Transformer，见：IEEE/CVF 计算机视觉与模式识别大会论文集，页 8500–8509。
- en: 'Landrieu and Simonovsky (2018) Landrieu, L., Simonovsky, M., 2018. Large-scale
    point cloud semantic segmentation with superpoint graphs, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4558–4567.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Landrieu 和 Simonovsky（2018）Landrieu, L., Simonovsky, M., 2018. 使用超点图进行大规模点云语义分割，见：IEEE/CVF
    计算机视觉与模式识别大会论文集，页 4558–4567。
- en: 'Lawin et al. (2017) Lawin, F., Danelljan, M., Tosteberg, P., Bhat, G., Khan,
    F., Felsberg, M., 2017. Deep projective 3d semantic segmentation, in: Computer
    Analysis of Images and Patterns, Springer. pp. 95–107.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lawin 等（2017）Lawin, F., Danelljan, M., Tosteberg, P., Bhat, G., Khan, F., Felsberg,
    M., 2017. 深度投影 3d 语义分割，见：计算机图像与模式分析，Springer. 页 95–107。
- en: 'Le and Duan (2018) Le, T., Duan, Y., 2018. Pointgrid: A deep network for 3d
    shape understanding, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 9204–9214.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Le 和 Duan (2018) Le, T., Duan, Y., 2018. Pointgrid: 用于 3D 形状理解的深度网络, 见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第9204–9214页。'
- en: 'Lei et al. (2019) Lei, H., Akhtar, N., Mian, A., 2019. Octree guided cnn with
    spherical kernels for 3d point clouds, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9631–9640.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 (2019) Lei, H., Akhtar, N., Mian, A., 2019. 带球形核的八叉树引导卷积神经网络用于 3D 点云,
    见：IEEE/CVF 计算机视觉与模式识别会议论文集，第9631–9640页。
- en: Lei et al. (2020) Lei, H., Akhtar, N., Mian, A., 2020. Spherical kernel for
    efficient graph convolution on 3d point clouds. IEEE Transactions on Pattern Analysis
    and Machine Intelligence .
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 (2020) Lei, H., Akhtar, N., Mian, A., 2020. 用于 3D 点云的高效图卷积的球形核。IEEE 模式分析与机器智能学报。
- en: Lei et al. (2023) Lei, H., Akhtar, N., Shah, M., Mian, A., 2023. Mesh convolution
    with continuous filters for 3-d surface parsing. IEEE Transactions on Neural Networks
    and Learning Systems .
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等人 (2023) Lei, H., Akhtar, N., Shah, M., Mian, A., 2023. 使用连续滤波器的网格卷积用于
    3D 表面解析。IEEE 神经网络与学习系统学报。
- en: 'Li et al. (2019a) Li, G., Muller, M., Thabet, A., Ghanem, B., 2019a. Deepgcns:
    Can gcns go as deep as cnns?, in: Processings of the IEEE/CVF International Conference
    on Computer Vision, pp. 9267–9276.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2019a) Li, G., Muller, M., Thabet, A., Ghanem, B., 2019a. Deepgcns:
    图卷积网络能否像卷积神经网络一样深入?, 见：IEEE/CVF 国际计算机视觉会议论文集，第9267–9276页。'
- en: 'Li et al. (2018a) Li, J., Chen, B.M., Hee Lee, G., 2018a. So-net: Self-organizing
    network for point cloud analysis, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pp. 9397–9406.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2018a) Li, J., Chen, B.M., Hee Lee, G., 2018a. So-net: 点云分析的自组织网络, 见：IEEE/CVF
    计算机视觉与模式识别会议论文集，第9397–9406页。'
- en: 'Li et al. (2018b) Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., 2018b.
    Pointcnn: Convolution on x-transformed points. Advances in Neural Information
    Processing Systemsms 31, 820–830.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2018b) Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B., 2018b. Pointcnn:
    在 X 变换点上的卷积。神经信息处理系统进展 31, 820–830。'
- en: 'Li et al. (2019b) Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. Tgnet:
    Geometric graph cnn on 3-d point cloud segmentation. IEEE Transactions on Geoscience
    and Remote Sensing 58, 3588–3600.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2019b) Li, Y., Ma, L., Zhong, Z., Cao, D., Li, J., 2019b. Tgnet: 用于
    3D 点云分割的几何图卷积神经网络。IEEE 地球科学与遥感学报 58, 3588–3600。'
- en: 'Li et al. (2016) Li, Z., Gan, Y., Liang, X., Yu, Y., Cheng, H., Lin, L., 2016.
    Lstm-cf: Unifying context modeling and fusion with lstms for rgb-d scene labeling,
    in: Proceedings of the European Conference on Computer Vision, Springer. pp. 541–557.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2016) Li, Z., Gan, Y., Liang, X., Yu, Y., Cheng, H., Lin, L., 2016.
    Lstm-cf: 通过 LSTMs 统一上下文建模与融合用于 RGB-D 场景标注, 见：欧洲计算机视觉会议论文集, Springer，第541–557页。'
- en: 'Lian et al. (2022) Lian, Q., Li, P., Chen, X., 2022. Monojsg: Joint semantic
    and geometric cost volume for monocular 3d object detection, in: Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1070–1079.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lian 等人 (2022) Lian, Q., Li, P., Chen, X., 2022. Monojsg: 单目 3D 物体检测的联合语义与几何成本体积,
    见：IEEE/CVF 计算机视觉与模式识别会议论文集, 第1070–1079页。'
- en: 'Liang et al. (2021) Liang, Z., Li, Z., Xu, S., Tan, M., Jia, K., 2021. Instance
    segmentation in 3d scenes using semantic superpoint tree networks, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pp. 2783–2792.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 (2021) Liang, Z., Li, Z., Xu, S., Tan, M., Jia, K., 2021. 使用语义超点树网络进行
    3D 场景中的实例分割, 见：IEEE/CVF 国际计算机视觉会议论文集，第2783–2792页。
- en: 'Liang et al. (2019a) Liang, Z., Yang, M., Deng, L., Wang, C., Wang, B., 2019a.
    Hierarchical depthwise graph convolutional neural network for 3d semantic segmentation
    of point clouds, in: Processing of the IEEE International Conference on Robotics
    and Automation, IEEE. pp. 8152–8158.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 (2019a) Liang, Z., Yang, M., Deng, L., Wang, C., Wang, B., 2019a. 用于点云的
    3D 语义分割的分层深度图卷积神经网络, 见：IEEE 国际机器人与自动化会议论文集, IEEE，第8152–8158页。
- en: Liang et al. (2019b) Liang, Z., Yang, M., Wang, C., 2019b. 3d graph embedding
    learning with a structure-aware loss function for point cloud semantic instance
    segmentation. arXiv preprint arXiv:1902.05247 .
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 (2019b) Liang, Z., Yang, M., Wang, C., 2019b. 使用结构感知损失函数的 3D 图嵌入学习用于点云语义实例分割。arXiv
    预印本 arXiv:1902.05247。
- en: 'Lin et al. (2017) Lin, D., Chen, G., Cohen-Or, D., Heng, P., Huang, H., 2017.
    Cascaded feature network for semantic segmentation of rgb-d images, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 1311–1319.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2017) Lin, D., Chen, G., Cohen-Or, D., Heng, P., Huang, H., 2017. 用于
    RGB-D 图像语义分割的级联特征网络, 见: Processings of the IEEE/CVF International Conference on
    Computer Vision, 第 1311–1319 页。'
- en: 'Liu and Furukawa (2019) Liu, C., Furukawa, Y., 2019. Masc: multi-scale affinity
    with sparse convolution for 3d instance segmentation. arXiv preprint arXiv:1902.04478
    .'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 和 Furukawa (2019) Liu, C., Furukawa, Y., 2019. MASC: 多尺度亲和性与稀疏卷积用于 3d 实例分割.
    arXiv 预印本 arXiv:1902.04478。'
- en: 'Liu et al. (2017) Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu,
    J., 2017. 3dcnn-dqn-rnn: A deep reinforcement learning framework for semantic
    parsing of large-scale 3d point clouds, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, pp. 5678–5687.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2017) Liu, F., Li, S., Zhang, L., Zhou, C., Ye, R., Wang, Y., Lu, J.,
    2017. 3dcnn-dqn-rnn: 一种用于大规模 3d 点云语义解析的深度强化学习框架, 见: Processings of the IEEE/CVF
    International Conference on Computer Vision, 第 5678–5687 页。'
- en: Liu et al. (2015) Liu, F., Shen, C., Lin, G., Reid, I., 2015. Learning depth
    from single monocular images using deep convolutional neural fields. IEEE Transactions
    on Pattern Analysis and Machine Intelligence 38, 2024–2039.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2015) Liu, F., Shen, C., Lin, G., Reid, I., 2015. 使用深度卷积神经场从单一单眼图像中学习深度.
    IEEE Transactions on Pattern Analysis and Machine Intelligence 38, 2024–2039。
- en: Liu et al. (2018a) Liu, H., Wu, W., Wang, X., Qian, Y., 2018a. Rgb-d joint modelling
    with scene geometric information for indoor semantic segmentation. Multimed. Tools.
    Appl. 77, 22475–22488.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2018a) Liu, H., Wu, W., Wang, X., Qian, Y., 2018a. 基于场景几何信息的 RGB-D 联合建模用于室内语义分割.
    Multimed. Tools. Appl. 77, 22475–22488。
- en: Liu et al. (2018b) Liu, J., Wang, Y., Li, Y., Fu, J., Li, J., Lu, H., 2018b.
    Collaborative deconvolutional neural networks for joint depth estimation and semantic
    segmentation. IEEE Trans. Neural Netw. Learn. Syst. 29, 5655–5666.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2018b) Liu, J., Wang, Y., Li, Y., Fu, J., Li, J., Lu, H., 2018b. 用于联合深度估计和语义分割的协同去卷积神经网络.
    IEEE Trans. Neural Netw. Learn. Syst. 29, 5655–5666。
- en: 'Liu et al. (2019a) Liu, W., Sun, J., Li, W., Hu, T., Wang, P., 2019a. Deep
    learning on point clouds and its application: A survey. Sensors 19, 4188.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019a) Liu, W., Sun, J., Li, W., Hu, T., Wang, P., 2019a. 点云上的深度学习及其应用：综述.
    Sensors 19, 4188。
- en: 'Liu et al. (2018c) Liu, Y., Yang, S., Li, B., Zhou, W., Xu, J., Li, H., Lu,
    Y., 2018c. Affinity derivation and graph merge for instance segmentation, in:
    Proceedings of the European Conference on Computer Vision, pp. 686–703.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2018c) Liu, Y., Yang, S., Li, B., Zhou, W., Xu, J., Li, H., Lu, Y.,
    2018c. 实例分割的亲和性推导和图合并, 见: Proceedings of the European Conference on Computer Vision,
    第 686–703 页。'
- en: 'Liu et al. (2019b) Liu, Z., Tang, H., Lin, Y., Han, S., 2019b. Point-voxel
    cnn for efficient 3d deep learning, in: Advances in Neural Information Processing
    Systemsms, pp. 965–975.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2019b) Liu, Z., Tang, H., Lin, Y., Han, S., 2019b. 用于高效 3d 深度学习的 Point-voxel
    cnn, 见: Advances in Neural Information Processing Systemsms, 第 965–975 页。'
- en: Lowe (2004) Lowe, D.G., 2004. Distinctive image features from scale-invariant
    keypoints. International journal of computer vision 60, 91–110.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe (2004) Lowe, D.G., 2004. 从尺度不变关键点中提取独特的图像特征. International journal of computer
    vision 60, 91–110。
- en: 'Ma et al. (2017) Ma, L., Stückler, J., Kerl, C., Cremers, D., 2017. Multi-view
    deep learning for consistent semantic mapping with rgb-d cameras, in: Proc. IEEE
    Int. Conf. Intell. Rob. Syst., IEEE. pp. 598–605.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 (2017) Ma, L., Stückler, J., Kerl, C., Cremers, D., 2017. 使用 RGB-D 相机进行一致语义映射的多视角深度学习,
    见: Proc. IEEE Int. Conf. Intell. Rob. Syst., IEEE. 第 598–605 页。'
- en: 'Ma et al. (2020) Ma, Y., Guo, Y., Liu, H., Lei, Y., Wen, G., 2020. Global context
    reasoning for semantic segmentation of 3d point clouds, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2931–2940.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等 (2020) Ma, Y., Guo, Y., Liu, H., Lei, Y., Wen, G., 2020. 用于 3d 点云语义分割的全局上下文推理,
    见: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
    第 2931–2940 页。'
- en: 'Maturana and Scherer (2015) Maturana, D., Scherer, S., 2015. Voxnet: A 3d convolutional
    neural network for real-time object recognition, in: Proc. IEEE Int. Conf. Intell.
    Rob. Syst., IEEE. pp. 922–928.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maturana 和 Scherer (2015) Maturana, D., Scherer, S., 2015. Voxnet: 一种用于实时物体识别的
    3d 卷积神经网络, 见: Proc. IEEE Int. Conf. Intell. Rob. Syst., IEEE. 第 922–928 页。'
- en: 'McCormac et al. (2017) McCormac, J., Handa, A., Davison, A., Leutenegger, S.,
    2017. Semanticfusion: Dense 3d semantic mapping with convolutional neural networks,
    in: Proceedings of the IEEE International Conference on Robotics and Automation,
    IEEE. pp. 4628–4635.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McCormac 等 (2017) McCormac, J., Handa, A., Davison, A., Leutenegger, S., 2017.
    Semanticfusion: 使用卷积神经网络进行密集 3d 语义映射, 见: Proceedings of the IEEE International
    Conference on Robotics and Automation, IEEE. 第 4628–4635 页。'
- en: 'Meng et al. (2019) Meng, H., Gao, L., Lai, Y., Manocha, D., 2019. Vv-net: Voxel
    vae net with group convolutions for point cloud segmentation, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 8500–8508.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Meng 等人（2019）Meng, H., Gao, L., Lai, Y., Manocha, D., 2019. Vv-net: 使用组卷积的体素
    VAE 网络进行点云分割，发表于：IEEE/CVF 国际计算机视觉会议论文集，第 8500–8508 页。'
- en: 'Meyer et al. (2019) Meyer, G.P., Charland, J., Hegde, D., Laddha, A., Vallespi-Gonzalez,
    C., 2019. Sensor fusion for joint 3d object detection and semantic segmentation,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pp. 0–0.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meyer 等人（2019）Meyer, G.P., Charland, J., Hegde, D., Laddha, A., Vallespi-Gonzalez,
    C., 2019. 传感器融合用于联合 3D 目标检测和语义分割，发表于：IEEE/CVF 计算机视觉与模式识别会议研讨会论文集，第 0–0 页。
- en: 'Milioto et al. (2019) Milioto, A., Vizzo, I., Behley, J., Stachniss, C., 2019.
    Rangenet++: Fast and accurate lidar semantic segmentation, in: Proc. IEEE Int.
    Conf. Intell. Rob. Syst., IEEE. pp. 4213–4220.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Milioto 等人（2019）Milioto, A., Vizzo, I., Behley, J., Stachniss, C., 2019. Rangenet++:
    快速且准确的激光雷达语义分割，发表于：IEEE 智能机器人系统国际会议论文集，IEEE，第 4213–4220 页。'
- en: Morton (1966) Morton, G.M., 1966. A computer oriented geodetic data base and
    a new technique in file sequencing .
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Morton（1966）Morton, G.M., 1966. 计算机导向的测地数据基和文件排序的新技术。
- en: 'Mousavian et al. (2016) Mousavian, A., Pirsiavash, H., Košecká, J., 2016. Joint
    semantic segmentation and depth estimation with deep convolutional networks, in:
    Processing of the International Conference on 3D Vision, IEEE. pp. 611–619.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mousavian 等人（2016）Mousavian, A., Pirsiavash, H., Košecká, J., 2016. 使用深度卷积网络的联合语义分割和深度估计，发表于：国际
    3D 视觉会议论文集，IEEE，第 611–619 页。
- en: 'Narita et al. (2019) Narita, G., Seno, T., Ishikawa, T., Kaji, Y., 2019. Panopticfusion:
    Online volumetric semantic mapping at the level of stuff and things. arXiv preprint
    arXiv:1903.01177 .'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Narita 等人（2019）Narita, G., Seno, T., Ishikawa, T., Kaji, Y., 2019. Panopticfusion:
    在线体积语义映射在物体和事物层面上的应用。arXiv 预印本 arXiv:1903.01177。'
- en: 'Naseer et al. (2018) Naseer, M., Khan, S., Porikli, F., 2018. Indoor scene
    understanding in 2.5/3d for autonomous agents: A survey. IEEE Access 7, 1859–1887.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naseer 等人（2018）Naseer, M., Khan, S., Porikli, F., 2018. 对于自主代理的 2.5/3D 室内场景理解：综述。《IEEE
    Access》7, 1859–1887。
- en: 'Ngo et al. (2023) Ngo, T.D., Hua, B.S., Nguyen, K., 2023. Isbnet: a 3d point
    cloud instance segmentation network with instance-aware sampling and box-aware
    dynamic convolution, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 13550–13559.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ngo 等人（2023）Ngo, T.D., Hua, B.S., Nguyen, K., 2023. Isbnet: 一种具有实例感知采样和框感知动态卷积的
    3D 点云实例分割网络，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，第 13550–13559 页。'
- en: 'Park et al. (2022) Park, C., Jeong, Y., Cho, M., Park, J., 2022. Fast point
    transformer, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 16949–16958.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2022）Park, C., Jeong, Y., Cho, M., Park, J., 2022. 快速点变换器，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 16949–16958 页。
- en: 'Park et al. (2023) Park, J., Kim, C., Kim, S., Jo, K., 2023. Pcscnet: Fast
    3d semantic segmentation of lidar point cloud for autonomous car using point convolution
    and sparse convolution network. Expert Systems with Applications 212, 118815.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等人（2023）Park, J., Kim, C., Kim, S., Jo, K., 2023. Pcscnet: 通过点卷积和稀疏卷积网络实现对激光雷达点云的快速
    3D 语义分割。《专家系统与应用》212, 118815。'
- en: 'Pham et al. (2019a) Pham, Q., Hua, B., Nguyen, T., Yeung, S., 2019a. Real-time
    progressive 3d semantic segmentation for indoor scenes, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, IEEE. pp. 1089–1098.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pham 等人（2019a）Pham, Q., Hua, B., Nguyen, T., Yeung, S., 2019a. 室内场景的实时渐进式 3D
    语义分割，发表于：IEEE/CVF 计算机视觉应用冬季会议论文集，IEEE，第 1089–1098 页。
- en: 'Pham et al. (2019b) Pham, Q.H., Nguyen, T., Hua, B.S., Roig, G., Yeung, S.K.,
    2019b. Jsis3d: joint semantic-instance segmentation of 3d point clouds with multi-task
    pointwise networks and multi-value conditional random fields, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8827–8836.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pham 等人（2019b）Pham, Q.H., Nguyen, T., Hua, B.S., Roig, G., Yeung, S.K., 2019b.
    Jsis3d: 具有多任务点级网络和多值条件随机场的 3D 点云联合语义-实例分割，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，第 8827–8836
    页。'
- en: 'Qi et al. (2017a) Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017a. Pointnet:
    Deep learning on point sets for 3d classification and segmentation, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 652–660.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等人（2017a）Qi, C.R., Su, H., Mo, K., Guibas, L.J., 2017a. Pointnet: 基于点集的深度学习用于
    3D 分类和分割，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，第 652–660 页。'
- en: 'Qi et al. (2017b) Qi, C.R., Yi, L., Su, H., Guibas, L.J., 2017b. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. Advances in
    Neural Information Processing Systemsms 30, 5099–5108.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等 (2017b) Qi, C.R., Yi, L., Su, H., Guibas, L.J., 2017b. Pointnet++：在度量空间中的点集深度层次特征学习。神经信息处理系统进展
    30，页码 5099–5108。
- en: 'Qi et al. (2017c) Qi, X., Liao, R., Jia, J., Fidler, S., Urtasun, R., 2017c.
    3d graph neural networks for rgbd semantic segmentation, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 5199–5208.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等 (2017c) Qi, X., Liao, R., Jia, J., Fidler, S., Urtasun, R., 2017c. 用于 RGBD
    语义分割的 3D 图神经网络，见于：IEEE/CVF 国际计算机视觉会议论文集，页码 5199–5208。
- en: 'Qian et al. (2022) Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H.A.A.K.,
    Elhoseiny, M., Ghanem, B., 2022. Pointnext: Revisiting pointnet++ with improved
    training and scaling strategies. arXiv preprint arXiv:2206.04670 .'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等 (2022) Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H.A.A.K., Elhoseiny,
    M., Ghanem, B., 2022. Pointnext：重新审视改进训练和扩展策略的 Pointnet++。arXiv 预印本 arXiv:2206.04670。
- en: Raj et al. (2015) Raj, A., Maturana, D., Scherer, S., 2015. Multi-scale convolutional
    architecture for semantic segmentation. Robotics Institute, Carnegie Mellon University,
    Tech. Rep. CMU-RITR-15-21 .
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raj 等 (2015) Raj, A., Maturana, D., Scherer, S., 2015. 用于语义分割的多尺度卷积架构。卡内基梅隆大学机器人研究所，技术报告
    CMU-RITR-15-21。
- en: 'Ran et al. (2022) Ran, H., Liu, J., Wang, C., 2022. Surface representation
    for point clouds, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pp. 18942–18952.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ran 等 (2022) Ran, H., Liu, J., Wang, C., 2022. 点云的表面表示，见于：IEEE/CVF 计算机视觉与模式识别会议论文集，页码
    18942–18952。
- en: 'Rethage et al. (2018) Rethage, D., Wald, J., Sturm, J., Navab, N., Tombari,
    F., 2018. Fully-convolutional point networks for large-scale point clouds, in:
    Proceedings of the European Conference on Computer Vision, pp. 596–611.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rethage 等 (2018) Rethage, D., Wald, J., Sturm, J., Navab, N., Tombari, F., 2018.
    用于大规模点云的全卷积点网络，见于：欧洲计算机视觉会议论文集，页码 596–611。
- en: 'Riegler et al. (2017) Riegler, G., Osman Ulusoy, A., Geiger, A., 2017. Octnet:
    Learning deep 3d representations at high resolutions, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 3577–3586.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riegler 等 (2017) Riegler, G., Osman Ulusoy, A., Geiger, A., 2017. Octnet：在高分辨率下学习深层
    3D 表示，见于：IEEE/CVF 计算机视觉与模式识别会议论文集，页码 3577–3586。
- en: 'Riemenschneider et al. (2014) Riemenschneider, H., Bódis-Szomorú, A., Weissenberg,
    J., Van Gool, L., 2014. Learning where to classify in multi-view semantic segmentation,
    in: Proceedings of the European Conference on Computer Vision, Springer. pp. 516–532.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riemenschneider 等 (2014) Riemenschneider, H., Bódis-Szomorú, A., Weissenberg,
    J., Van Gool, L., 2014. 在多视角语义分割中学习分类位置，见于：欧洲计算机视觉会议论文集，Springer，页码 516–532。
- en: 'Rosu et al. (2019) Rosu, R.A., Schütt, P., Quenzel, J., Behnke, S., 2019. Latticenet:
    Fast point cloud segmentation using permutohedral lattices. arXiv preprint arXiv:1912.05905
    .'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosu 等 (2019) Rosu, R.A., Schütt, P., Quenzel, J., Behnke, S., 2019. Latticenet：使用
    permutohedral 格点的快速点云分割。arXiv 预印本 arXiv:1912.05905。
- en: 'Roynard et al. (2018) Roynard, X., Deschaud, J., Goulette, F., 2018. Paris-lille-3d:
    A large and high-quality ground-truth urban point cloud dataset for automatic
    segmentation and classification. The International Journal of Robotics Research
    37, 545–557.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roynard 等 (2018) Roynard, X., Deschaud, J., Goulette, F., 2018. Paris-lille-3d：一个大型高质量的真实地面城市点云数据集，用于自动分割和分类。《国际机器人研究杂志》37，页码
    545–557。
- en: 'Shen et al. (2018) Shen, Y., Feng, C., Yang, Y., Tian, D., 2018. Mining point
    cloud local structures by kernel correlation and graph pooling, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4548–4557.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等 (2018) Shen, Y., Feng, C., Yang, Y., Tian, D., 2018. 通过核相关性和图池化挖掘点云局部结构，见于：IEEE/CVF
    计算机视觉与模式识别会议论文集，页码 4548–4557。
- en: 'Shi et al. (2020) Shi, H., Lin, G., Wang, H., Hung, T.Y., Wang, Z., 2020. Spsequencenet:
    Semantic segmentation network on 4d point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 4574–4583.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2020) Shi, H., Lin, G., Wang, H., Hung, T.Y., Wang, Z., 2020. Spsequencenet：4D
    点云的语义分割网络，见于：IEEE/CVF 计算机视觉与模式识别会议论文集，页码 4574–4583。
- en: 'Shi et al. (2022) Shi, H., Wei, J., Li, R., Liu, F., Lin, G., 2022. Weakly
    supervised segmentation on outdoor 4d point clouds with temporal matching and
    spatial graph propagation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 11840–11849.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等 (2022) Shi, H., Wei, J., Li, R., Liu, F., Lin, G., 2022. 在户外 4D 点云上进行弱监督分割，结合时间匹配和空间图传播，见于：IEEE/CVF
    计算机视觉与模式识别会议论文集，页码 11840–11849。
- en: 'Silberman and Fergus (2011) Silberman, N., Fergus, R., 2011. Indoor scene segmentation
    using a structured light sensor, in: Processings of the IEEE/CVF International
    Conference on Computer Vision Worksh., IEEE. pp. 601–608.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silberman and Fergus (2011) Silberman, N., Fergus, R., 2011. 使用结构光传感器进行室内场景分割,
    在：IEEE/CVF 国际计算机视觉会议研讨会论文集，IEEE，第 601–608 页。
- en: 'Silberman et al. (2012) Silberman, N., Hoiem, D., Kohli, P., Fergus, R., 2012.
    Indoor segmentation and support inference from rgbd images, in: Proceedings of
    the European Conference on Computer Vision, Springer. pp. 746–760.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silberman et al. (2012) Silberman, N., Hoiem, D., Kohli, P., Fergus, R., 2012.
    从 RGBD 图像进行室内分割和支持推断, 在：欧洲计算机视觉会议论文集，Springer，第 746–760 页。
- en: 'Simonovsky and Komodakis (2017) Simonovsky, M., Komodakis, N., 2017. Dynamic
    edge-conditioned filters in convolutional neural networks on graphs, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3693–3702.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonovsky and Komodakis (2017) Simonovsky, M., Komodakis, N., 2017. 图上卷积神经网络中的动态边缘条件滤波器,
    在：IEEE/CVF 计算机视觉与模式识别会议论文集，第 3693–3702 页。
- en: 'Song et al. (2015) Song, S., Lichtenberg, S.P., Xiao, J., 2015. Sun rgb-d:
    A rgb-d scene understanding benchmark suite, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 567–576.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2015) Song, S., Lichtenberg, S.P., Xiao, J., 2015. Sun rgb-d：一个
    RGB-D 场景理解基准套件, 在：IEEE/CVF 计算机视觉与模式识别会议论文集，第 567–576 页。
- en: 'Song et al. (2017) Song, Y., Chen, X., Li, J., Zhao, Q., 2017. Embedding 3d
    geometric features for rigid object part segmentation, in: Processings of the
    IEEE/CVF International Conference on Computer Vision, pp. 580–588.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2017) Song, Y., Chen, X., Li, J., Zhao, Q., 2017. 嵌入 3D 几何特征用于刚性物体部件分割,
    在：IEEE/CVF 国际计算机视觉会议论文集，第 580–588 页。
- en: 'Su et al. (2018) Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang,
    M.H., Kautz, J., 2018. Splatnet: Sparse lattice networks for point cloud processing,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2530–2539.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2018) Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang,
    M.H., Kautz, J., 2018. Splatnet：用于点云处理的稀疏晶格网络, 在：IEEE/CVF 计算机视觉与模式识别会议论文集，第 2530–2539
    页。
- en: 'Su et al. (2015) Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015.
    Multi-view convolutional neural networks for 3d shape recognition, in: Processings
    of the IEEE/CVF International Conference on Computer Vision, pp. 945–953.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2015) Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E., 2015.
    多视角卷积神经网络用于 3D 形状识别, 在：IEEE/CVF 国际计算机视觉会议论文集，第 945–953 页。
- en: Su et al. (2023) Su, Y., Xu, X., Jia, K., 2023. Weakly supervised 3d point cloud
    segmentation via multi-prototype learning. IEEE Transactions on Circuits and Systems
    for Video Technology .
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2023) Su, Y., Xu, X., Jia, K., 2023. 弱监督 3D 点云分割通过多原型学习. IEEE Transactions
    on Circuits and Systems for Video Technology.
- en: 'Tatarchenko et al. (2018) Tatarchenko, M., Park, J., Koltun, V., Zhou, Q.Y.,
    2018. Tangent convolutions for dense prediction in 3d, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3887–3896.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tatarchenko et al. (2018) Tatarchenko, M., Park, J., Koltun, V., Zhou, Q.Y.,
    2018. 用于密集预测的切线卷积, 在：IEEE/CVF 计算机视觉与模式识别会议论文集，第 3887–3896 页。
- en: 'Tchapmi et al. (2017) Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese,
    S., 2017. Segcloud: Semantic segmentation of 3d point clouds, in: Processing of
    the International Conference on 3D Vision, IEEE. pp. 537–547.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tchapmi et al. (2017) Tchapmi, L., Choy, C., Armeni, I., Gwak, J., Savarese,
    S., 2017. Segcloud：3D 点云的语义分割, 在：国际 3D 视觉会议论文集，IEEE，第 537–547 页。
- en: 'Thomas et al. (2019) Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B.,
    Goulette, F., Guibas, L.J., 2019. Kpconv: Flexible and deformable convolution
    for point clouds, in: Processings of the IEEE/CVF International Conference on
    Computer Vision, pp. 6411–6420.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomas et al. (2019) Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette,
    F., Guibas, L.J., 2019. Kpconv：灵活且可变形的点云卷积, 在：IEEE/CVF 国际计算机视觉会议论文集，第 6411–6420
    页。
- en: 'Valipour et al. (2017) Valipour, S., Siam, M., Jagersand, M., Ray, N., 2017.
    Recurrent fully convolutional networks for video segmentation, in: Proceedings
    of the IEEE/CVF Winter Conference on Applications of Computer Vision, IEEE. pp.
    29–36.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valipour et al. (2017) Valipour, S., Siam, M., Jagersand, M., Ray, N., 2017.
    用于视频分割的递归全卷积网络, 在：IEEE/CVF 冬季计算机视觉应用会议论文集，IEEE，第 29–36 页。
- en: 'Verma et al. (2018) Verma, N., Boyer, E., Verbeek, J., 2018. Feastnet: Feature-steered
    graph convolutions for 3d shape analysis, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 2598–2606.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Verma等（2018）Verma, N., Boyer, E., Verbeek, J., 2018。Feastnet：用于3d形状分析的特征引导图卷积，见：IEEE/CVF计算机视觉与模式识别会议论文集，第2598–2606页。
- en: 'Vu et al. (2022) Vu, T., Kim, K., Luu, T.M., Nguyen, T., Yoo, C.D., 2022. Softgroup
    for 3d instance segmentation on point clouds, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2708–2717.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vu等（2022）Vu, T., Kim, K., Luu, T.M., Nguyen, T., Yoo, C.D., 2022。Softgroup：针对点云的3d实例分割，见：IEEE/CVF计算机视觉与模式识别会议论文集，第2708–2717页。
- en: 'Wang et al. (2018a) Wang, C., Samari, B., Siddiqi, K., 2018a. Local spectral
    graph convolution for point set feature learning, in: Proceedings of the European
    Conference on Computer Vision, pp. 52–66.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018a）Wang, C., Samari, B., Siddiqi, K., 2018a。用于点集特征学习的局部谱图卷积，见：欧洲计算机视觉会议论文集，第52–66页。
- en: 'Wang et al. (2022) Wang, J., Li, X., Sullivan, A., Abbott, L., Chen, S., 2022.
    Pointmotionnet: Point-wise motion learning for large-scale lidar point clouds
    sequences, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 4419–4428.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2022）Wang, J., Li, X., Sullivan, A., Abbott, L., Chen, S., 2022。Pointmotionnet：大规模激光雷达点云序列的点级运动学习，见：IEEE/CVF计算机视觉与模式识别会议论文集，第4419–4428页。
- en: 'Wang et al. (2016) Wang, J., Wang, Z., Tao, D., See, S., Wang, G., 2016. Learning
    common and specific features for rgb-d semantic segmentation with deconvolutional
    networks, in: Proceedings of the European Conference on Computer Vision, Springer.
    pp. 664–679.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2016）Wang, J., Wang, Z., Tao, D., See, S., Wang, G., 2016。利用反卷积网络学习rgb-d语义分割中的共性和特征，见：欧洲计算机视觉会议论文集，Springer，第664–679页。
- en: Wang et al. (2018b) Wang, P., Gan, Y., Shui, P., Yu, F., Zhang, Y., Chen, S.,
    Sun, Z., 2018b. 3d shape segmentation via shape fully convolutional networks.
    Computers & Graphics 70, 128–139.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018b）Wang, P., Gan, Y., Shui, P., Yu, F., Zhang, Y., Chen, S., Sun, Z.,
    2018b。通过形状全卷积网络的3d形状分割。计算机与图形学 70, 128–139。
- en: 'Wang et al. (2015) Wang, P., Shen, X., Lin, Z., Cohen, S., Price, B., Yuille,
    A., 2015. Towards unified depth and semantic prediction from a single image, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2800–2809.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2015）Wang, P., Shen, X., Lin, Z., Cohen, S., Price, B., Yuille, A., 2015。针对单幅图像的统一深度和语义预测，见：IEEE/CVF计算机视觉与模式识别会议论文集，第2800–2809页。
- en: 'Wang et al. (2017) Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., 2017.
    O-cnn: Octree-based convolutional neural networks for 3d shape analysis. ACM Transactions
    on Graphics 36, 1–11.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2017）Wang, P.S., Liu, Y., Guo, Y.X., Sun, C.Y., Tong, X., 2017。O-cnn：基于八叉树的卷积神经网络用于3d形状分析。ACM图形学论文
    36, 1–11。
- en: 'Wang et al. (2018c) Wang, S., Suo, S., Ma, W.C., Pokrovsky, A., Urtasun, R.,
    2018c. Deep parametric continuous convolutional neural networks, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2589–2597.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018c）Wang, S., Suo, S., Ma, W.C., Pokrovsky, A., Urtasun, R., 2018c。深度参数连续卷积神经网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，第2589–2597页。
- en: 'Wang and Neumann (2018) Wang, W., Neumann, U., 2018. Depth-aware cnn for rgb-d
    segmentation, in: Proceedings of the European Conference on Computer Vision, pp.
    135–150.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang和Neumann（2018）Wang, W., Neumann, U., 2018。深度感知cnn用于rgb-d分割，见：欧洲计算机视觉会议论文集，第135–150页。
- en: 'Wang et al. (2018d) Wang, W., Yu, R., Huang, Q., Neumann, U., 2018d. Sgpn:
    Similarity group proposal network for 3d point cloud instance segmentation, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 2569–2578.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2018d）Wang, W., Yu, R., Huang, Q., Neumann, U., 2018d。Sgpn：用于3d点云实例分割的相似性组提议网络，见：IEEE/CVF计算机视觉与模式识别会议论文集，第2569–2578页。
- en: 'Wang et al. (2019a) Wang, X., Liu, S., Shen, X., Shen, C., Jia, J., 2019a.
    Associatively segmenting instances and semantics in point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4096–4105.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2019a）Wang, X., Liu, S., Shen, X., Shen, C., Jia, J., 2019a。在点云中关联性地分割实例和语义，见：IEEE/CVF计算机视觉与模式识别会议论文集，第4096–4105页。
- en: Wang et al. (2012) Wang, Y., Asafi, S., Van Kaick, O., Zhang, H., Cohen-Or,
    D., Chen, B., 2012. Active co-analysis of a set of shapes. ACM Transactions on
    Graphics 31, 1–10.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等（2012）Wang, Y., Asafi, S., Van Kaick, O., Zhang, H., Cohen-Or, D., Chen,
    B., 2012。主动共分析一组形状。ACM图形学论文 31, 1–10。
- en: 'Wang et al. (2018e) Wang, Y., Shi, T., Yun, P., Tai, L., Liu, M., 2018e. Pointseg:
    Real-time semantic segmentation based on 3d lidar point cloud. arXiv preprint
    arXiv:1807.06288 .'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2018e）王艺，石婷，云鹏，台亮，刘明，2018e。PointSeg：基于3D激光雷达点云的实时语义分割。arXiv预印本arXiv:1807.06288。
- en: Wang et al. (2019b) Wang, Y., Sun, Y., Liu, Z., Sarma, S.E., Bronstein, M.M.,
    Solomon, J.M., 2019b. Dynamic graph cnn for learning on point clouds. ACM Transactions
    on Graphics 38, 1–12.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等（2019b）王艺，孙阳，刘中，萨尔玛，S.E.，布朗斯坦，M.M.，所罗门，J.M.，2019b。用于点云学习的动态图CNN。《ACM图形学学报》38，1–12。
- en: 'Wang and Lu (2019) Wang, Z., Lu, F., 2019. Voxsegnet: Volumetric cnns for semantic
    part segmentation of 3d shapes. IEEE Transactions on Visualization and Computer
    Graphics .'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王和陆（2019）王振，陆飞，2019。Voxsegnet：用于3D形状语义部分分割的体积CNN。《IEEE可视化与计算机图形学学报》。
- en: Wei (2008) Wei, L.Y., 2008. Parallel poisson disk sampling. ACM Transactions
    on Graphics 27, 1–9.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏（2008）魏立阳，2008。并行泊松盘采样。《ACM图形学学报》27，1–9。
- en: 'Wei et al. (2022) Wei, Y., Liu, H., Xie, T., Ke, Q., Guo, Y., 2022. Spatial-temporal
    transformer for 3d point cloud sequences, in: Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision, pp. 1171–1180.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2022）魏勇，刘华，谢涛，柯庆，郭艳，2022。用于3D点云序列的时空变换器，发表于：IEEE/CVF冬季计算机视觉应用会议论文集，第1171–1180页。
- en: 'Wu et al. (2018a) Wu, B., Wan, A., Yue, X., Keutzer, K., 2018a. Squeezeseg:
    Convolutional neural nets with recurrent crf for real-time road-object segmentation
    from 3d lidar point cloud, in: Proceedings of the IEEE International Conference
    on Robotics and Automation, IEEE. pp. 1887–1893.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2018a）吴博，万安，岳翔，凯乌策，K.，2018a。Squeezeseg：带有递归CRF的卷积神经网络用于从3D激光雷达点云中实时道路对象分割，发表于：IEEE国际机器人与自动化会议论文集，IEEE。第1887–1893页。
- en: 'Wu et al. (2019a) Wu, B., Zhou, X., Zhao, S., Yue, X., Keutzer, K., 2019a.
    Squeezesegv2: Improved model structure and unsupervised domain adaptation for
    road-object segmentation from a lidar point cloud, in: Proceedings of the IEEE
    International Conference on Robotics and Automation, IEEE. pp. 4376–4382.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019a）吴博，周晓，赵思，岳翔，凯乌策，2019a。Squeezesegv2：改进的模型结构和无监督领域适应用于从激光雷达点云中进行道路对象分割，发表于：IEEE国际机器人与自动化会议论文集，IEEE。第4376–4382页。
- en: 'Wu et al. (2019b) Wu, W., Qi, Z., Fuxin, L., 2019b. Pointconv: Deep convolutional
    networks on 3d point clouds, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 9621–9630.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2019b）吴伟，齐泽，付鑫，2019b。PointConv：用于3D点云的深度卷积网络，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第9621–9630页。
- en: 'Wu et al. (2022a) Wu, X., Lao, Y., Jiang, L., Liu, X., Zhao, H., 2022a. Point
    transformer v2: Grouped vector attention and partition-based pooling. Advances
    in Neural Information Processing Systemsms 35, 33330–33342.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2022a）吴旭，劳扬，姜磊，刘轩，赵辉，2022a。点变换器v2：分组向量注意力与基于分区的池化。《神经信息处理系统进展》35，33330–33342。
- en: 'Wu et al. (2022b) Wu, Y., Shi, M., Du, S., Lu, H., Cao, Z., Zhong, W., 2022b.
    3d instances as 1d kernels, in: Proceedings of the European Conference on Computer
    Vision, Springer. pp. 235–252.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2022b）吴勇，石美，杜松，陆浩，曹志，钟伟，2022b。将3D实例作为1D卷积核，发表于：欧洲计算机视觉会议论文集，Springer。第235–252页。
- en: Wu et al. (2018b) Wu, Y., Wu, Y., Gkioxari, G., Tian, Y., 2018b. Building generalizable
    agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209
    .
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2018b）吴勇，吴云，Gkioxari，G.，田野，2018b。通过现实且丰富的3D环境构建可泛化的智能体。arXiv预印本arXiv:1801.02209。
- en: 'Wu et al. (2015) Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X.,
    Xiao, J., 2015. 3d shapenets: A deep representation for volumetric shapes, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 1912–1920.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2015）吴泽，宋帅，科斯拉，A.，余飞，张磊，唐晓，肖俊，2015。3D ShapeNets：体积形状的深度表示，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第1912–1920页。
- en: Wu et al. (2022c) Wu, Z., Zhou, Z., Allibert, G., Stolz, C., Demonceaux, C.,
    Ma, C., 2022c. Transformer fusion for indoor rgb-d semantic segmentation. Available
    at SSRN 4251286 .
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吴等（2022c）吴泽，周振，阿利贝尔，G.，斯托尔茨，C.，德蒙肖，C.，马超，2022c。室内RGB-D语义分割的变换器融合。可在SSRN 4251286处获取。
- en: 'Xia et al. (2023) Xia, Z., Liu, Y., Li, X., Zhu, X., Ma, Y., Li, Y., Hou, Y.,
    Qiao, Y., 2023. Scpnet: Semantic scene completion on point cloud, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17642–17651.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 夏等（2023）夏志，刘杨，李鑫，朱旭，马艳，李耀，侯勇，乔宇，2023。SCPNet：点云上的语义场景补全，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，第17642–17651页。
- en: 'Xiang and Fox (2017) Xiang, Y., Fox, D., 2017. Da-rnn: Semantic mapping with
    data associated recurrent neural networks. arXiv preprint arXiv:1703.03098 .'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang 和 Fox（2017）Xiang, Y., Fox, D., 2017. Da-rnn：使用数据关联递归神经网络进行语义映射。arXiv 预印本
    arXiv:1703.03098。
- en: 'Xiao et al. (2023) Xiao, A., Huang, J., Guan, D., Zhang, X., Lu, S., Shao,
    L., 2023. Unsupervised point cloud representation learning with deep neural networks:
    A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence .'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等（2023）Xiao, A., Huang, J., Guan, D., Zhang, X., Lu, S., Shao, L., 2023.
    无监督点云表示学习的深度神经网络：综述。IEEE 模式分析与机器智能学报。
- en: 'Xie et al. (2020a) Xie, Y., Jiaojiao, T., Zhu, X., 2020a. Linking points with
    labels in 3d: A review of point cloud semantic segmentation. IEEE Geoscience and
    Remote Sensing Magazine .'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2020a）Xie, Y., Jiaojiao, T., Zhu, X., 2020a. 在 3d 中链接带标签的点：点云语义分割的综述。IEEE
    地球科学与遥感杂志。
- en: Xie et al. (2020b) Xie, Z., Chen, J., Peng, B., 2020b. Point clouds learning
    with attention-based graph convolution networks. Neurocomputing .
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2020b）Xie, Z., Chen, J., Peng, B., 2020b. 基于注意力的图卷积网络进行点云学习。Neurocomputing。
- en: 'Xu et al. (2020) Xu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K.,
    Tomizuka, M., 2020. Squeezesegv3: Spatially-adaptive convolution for efficient
    point-cloud segmentation. arXiv preprint arXiv:2004.01803 .'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020）Xu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K., Tomizuka,
    M., 2020. Squeezesegv3：用于高效点云分割的空间自适应卷积。arXiv 预印本 arXiv:2004.01803。
- en: 'Xu et al. (2017) Xu, H., Dong, M., Zhong, Z., 2017. Directionally convolutional
    networks for 3d shape segmentation, in: Processings of the IEEE/CVF International
    Conference on Computer Vision, pp. 2698–2707.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2017）Xu, H., Dong, M., Zhong, Z., 2017. 用于 3d 形状分割的方向卷积网络，收录于：IEEE/CVF
    国际计算机视觉会议论文集，第 2698–2707 页。
- en: 'Xu et al. (2021) Xu, M., Ding, R., Zhao, H., Qi, X., 2021. Paconv: Position
    adaptive convolution with dynamic kernel assembling on point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3173–3182.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2021）Xu, M., Ding, R., Zhao, H., Qi, X., 2021. Paconv：点云上的位置自适应卷积与动态内核组装，收录于：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 3173–3182 页。
- en: 'Xu et al. (2018) Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., 2018. Spidercnn:
    Deep learning on point sets with parameterized convolutional filters, in: Proceedings
    of the European Conference on Computer Vision, pp. 87–102.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2018）Xu, Y., Fan, T., Xu, M., Zeng, L., Qiao, Y., 2018. Spidercnn：使用参数化卷积滤波器对点集进行深度学习，收录于：欧洲计算机视觉会议论文集，第
    87–102 页。
- en: 'Yan et al. (2020) Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., 2020. Pointasnl:
    Robust point clouds processing using nonlocal neural networks with adaptive sampling,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    pp. 5589–5598.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等（2020）Yan, X., Zheng, C., Li, Z., Wang, S., Cui, S., 2020. Pointasnl：使用自适应采样的非局部神经网络进行鲁棒的点云处理，收录于：IEEE/CVF
    计算机视觉与模式识别会议论文集，第 5589–5598 页。
- en: Yang et al. (2019) Yang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham,
    A., Trigoni, N., 2019. Learning object bounding boxes for 3d instance segmentation
    on point clouds. Advances in Neural Information Processing Systemsms 32, 6740–6749.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2019）Yang, B., Wang, J., Clark, R., Hu, Q., Wang, S., Markham, A., Trigoni,
    N., 2019. 学习 3d 实例分割的目标边界框。神经信息处理系统进展，第 32 卷，第 6740–6749 页。
- en: 'Yang et al. (2017) Yang, S., Huang, Y., Scherer, S., 2017. Semantic 3d occupancy
    mapping through efficient high order crfs, in: Proc. IEEE Int. Conf. Intell. Rob.
    Syst., IEEE. pp. 590–597.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2017）Yang, S., Huang, Y., Scherer, S., 2017. 通过高效高阶 CRFs 进行语义 3d 占用映射，收录于：IEEE
    智能机器人系统国际会议论文集，第 590–597 页。
- en: 'Yang et al. (2022) Yang, Y., Xu, Y., Zhang, C., Xu, Z., Huang, J., 2022. Hierarchical
    vision transformer with channel attention for rgb-d image segmentation, in: Proceedings
    of the 4th International Symposium on Signal Processing Systems, pp. 68–73.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2022）Yang, Y., Xu, Y., Zhang, C., Xu, Z., Huang, J., 2022. 具有通道注意力的层次化视觉变换器用于
    RGB-D 图像分割，收录于：第 4 届国际信号处理系统研讨会论文集，第 68–73 页。
- en: 'Ye et al. (2018) Ye, X., Li, J., Huang, H., Du, L., Zhang, X., 2018. 3d recurrent
    neural networks with context fusion for point cloud semantic segmentation, in:
    Proceedings of the European Conference on Computer Vision, pp. 403–417.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等（2018）Ye, X., Li, J., Huang, H., Du, L., Zhang, X., 2018. 具有上下文融合的 3d 循环神经网络用于点云语义分割，收录于：欧洲计算机视觉会议论文集，第
    403–417 页。
- en: Yi et al. (2016) Yi, L., Kim, V., Ceylan, D., Shen, I., Yan, M., Su, H., Lu,
    C., Huang, Q., Sheffer, A., Guibas, L., 2016. A scalable active framework for
    region annotation in 3d shape collections. ACM Transactions on Graphics 35, 1–12.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi 等人 (2016) Yi, L., Kim, V., Ceylan, D., Shen, I., Yan, M., Su, H., Lu, C.,
    Huang, Q., Sheffer, A., Guibas, L., 2016. 一种可扩展的区域标注框架用于 3d 形状集合. ACM Transactions
    on Graphics 35, 1–12。
- en: 'Yi et al. (2017) Yi, L., Su, H., Guo, X., Guibas, L.J., 2017. Syncspeccnn:
    Synchronized spectral cnn for 3d shape segmentation, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 2282–2290.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi 等人 (2017) Yi, L., Su, H., Guo, X., Guibas, L.J., 2017. Syncspeccnn: 同步光谱
    cnn 用于 3d 形状分割, 见: 计算机视觉与模式识别会议论文集, pp. 2282–2290。'
- en: 'Yi et al. (2019) Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., 2019.
    Gspn: Generative shape proposal network for 3d instance segmentation in point
    cloud, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 3947–3956.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi 等人 (2019) Yi, L., Zhao, W., Wang, H., Sung, M., Guibas, L.J., 2019. Gspn:
    生成形状提议网络用于点云中的 3d 实例分割, 见: 计算机视觉与模式识别会议论文集, pp. 3947–3956。'
- en: 'Ying and Chuah (2022) Ying, X., Chuah, M.C., 2022. Uctnet: Uncertainty-aware
    cross-modal transformer network for indoor rgb-d semantic segmentation, in: Proceedings
    of the European Conference on Computer Vision, Springer. pp. 20–37.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ying 和 Chuah (2022) Ying, X., Chuah, M.C., 2022. Uctnet: 具备不确定性感知的跨模态变换器网络用于室内
    rgb-d 语义分割, 见: 欧洲计算机视觉会议论文集, Springer. pp. 20–37。'
- en: 'Yu et al. (2019) Yu, F., Liu, K., Zhang, Y., Zhu, C., Xu, K., 2019. Partnet:
    A recursive part decomposition network for fine-grained and hierarchical shape
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 9491–9500.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 (2019) Yu, F., Liu, K., Zhang, Y., Zhu, C., Xu, K., 2019. Partnet: 一种递归部件分解网络用于精细化和层次化形状分割,
    见: 计算机视觉与模式识别会议论文集, pp. 9491–9500。'
- en: Yuan et al. (2021) Yuan, X., Shi, J., Gu, L., 2021. A review of deep learning
    methods for semantic segmentation of remote sensing imagery. Expert Systems with
    Applications 169, 114417.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 (2021) Yuan, X., Shi, J., Gu, L., 2021. 深度学习方法在遥感图像语义分割中的综述. Expert
    Systems with Applications 169, 114417。
- en: 'Yue et al. (2022) Yue, C., Wang, Y., Tang, X., Chen, Q., 2022. Drgcnn: Dynamic
    region graph convolutional neural network for point clouds. Expert Systems with
    Applications 205, 117663.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yue 等人 (2022) Yue, C., Wang, Y., Tang, X., Chen, Q., 2022. Drgcnn: 动态区域图卷积神经网络用于点云.
    Expert Systems with Applications 205, 117663。'
- en: 'Zeng and Gevers (2018) Zeng, W., Gevers, T., 2018. 3dcontextnet: Kd tree guided
    hierarchical learning of point clouds using local and global contextual cues,
    in: Proceedings of the European Conference on Computer Vision, pp. 0–0.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng 和 Gevers (2018) Zeng, W., Gevers, T., 2018. 3dcontextnet: kd 树引导的点云分层学习，使用局部和全局上下文线索,
    见: 欧洲计算机视觉会议论文集, pp. 0–0。'
- en: 'Zhang et al. (2022) Zhang, C., Wan, H., Shen, X., Wu, Z., 2022. Patchformer:
    An efficient point transformer with patch attention, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pp. 11799–11808.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2022) Zhang, C., Wan, H., Shen, X., Wu, Z., 2022. Patchformer: 一种高效的点变换器，具备补丁注意力,
    见: 计算机视觉与模式识别会议论文集, pp. 11799–11808。'
- en: 'Zhang et al. (2020) Zhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong,
    B., Foroosh, H., 2020. Polarnet: An improved grid representation for online lidar
    point clouds semantic segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pp. 9601–9610.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2020) Zhang, Y., Zhou, Z., David, P., Yue, X., Xi, Z., Gong, B.,
    Foroosh, H., 2020. Polarnet: 用于在线激光雷达点云语义分割的改进网格表示, 见: 计算机视觉与模式识别会议论文集, pp. 9601–9610。'
- en: Zhang et al. (2023) Zhang, Z., Han, X., Dong, B., Li, T., Yin, B., Yang, X.,
    2023. Point cloud scene completion with joint color and semantic estimation from
    single rgb-d image. IEEE Transactions on Pattern Analysis and Machine Intelligence
    .
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 (2023) Zhang, Z., Han, X., Dong, B., Li, T., Yin, B., Yang, X., 2023.
    点云场景补全，通过单个 rgb-d 图像联合颜色和语义估计. IEEE Transactions on Pattern Analysis and Machine
    Intelligence.
- en: Zhao et al. (2018) Zhao, C., Sun, L., Purkait, P., Duckett, T., Stolkin, R.,
    2018. Dense rgb-d semantic mapping with pixel-voxel neural network. Sensors 18,
    3099.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 (2018) Zhao, C., Sun, L., Purkait, P., Duckett, T., Stolkin, R., 2018.
    基于像素-体素神经网络的密集 rgb-d 语义映射. Sensors 18, 3099。
- en: 'Zhao et al. (2019a) Zhao, H., Jiang, L., Fu, C.W., Jia, J., 2019a. Pointweb:
    Enhancing local neighborhood features for point cloud processing, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5565–5573.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2019a）赵华，蒋亮，傅成巍，贾军，2019a。点网：增强点云处理的本地邻域特征。在：IEEE/CVF计算机视觉与模式识别大会论文集，pp. 5565–5573。
- en: 'Zhao et al. (2021) Zhao, H., Jiang, L., Jia, J., Torr, P.H., Koltun, V., 2021.
    Point transformer, in: Proceedings of the IEEE/CVF International Conference on
    Computer Vision, pp. 16259–16268.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2021）赵华，蒋亮，贾军，托尔，科尔通，瓦努，2021。点变换器。在：IEEE/CVF国际计算机视觉大会论文集，pp. 16259–16268。
- en: 'Zhao et al. (2019b) Zhao, Y., Birdal, T., Deng, H., Tombari, F., 2019b. 3d
    point capsule networks, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 1009–1018.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2019b）赵雨，比尔达尔，邓华，汤巴里，2019b。3D点胶囊网络。在：IEEE/CVF计算机视觉与模式识别大会论文集，pp. 1009–1018。
- en: )Yong He received the M.S. degree from China University of Mining and Technology,
    Xuzhou, Jiangsu, China, in 2018\. He is currently pursing the Ph.D. degree with
    Hunan University, Changsha, China, and he is also currently a Visiting Scholar
    with University of Western Australia, Perth, Australia. His research interests
    include computer vision, point clouds analysis, and deep learning.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 何勇于2018年从中国矿业大学获得硕士学位，并目前在中国湖南大学攻读博士学位，同时还是澳大利亚珀斯大学的访问学者。他的研究兴趣包括计算机视觉、点云分析和深度学习。
- en: )Hongshan Yu received the B.S., M.S. and Ph.D. degrees of Control Science and
    Technology from electrical and information engineering of Hunan University, Changsha,
    China, in 2001, 2004 and 2007 respectively. From 2011 to 2012, he worked as a
    postdoctoral researcher in Laboratory for Computational Neuroscience of University
    of Pittsburgh, USA. He is currently a professor of Hunan University and associate
    dean of National Engineering Laboratory for Robot Visual Perception and Control.
    His research interests include autonomous mobile robot and machine vision.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 余洪山分别于2001年、2004年和2007年从中国湖南大学电气与信息工程学院控制科学与技术专业获得学士、硕士和博士学位。2011年至2012年期间，他曾在美国匹兹堡大学计算神经科学实验室担任博士后研究员。他目前是湖南大学教授，国家机器人视觉感知与控制工程实验室副院长。他的研究兴趣包括自主移动机器人和机器视觉。
- en: )Xiaoyan Liu received her Ph.D. degree of Process and System Engineering in
    2005 from Otto-von-Guericke University Magdeburg, Germany. She is currently a
    professor of Hunan University. Her research interests include machine vision and
    pattern recognition.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 刘晓燕于2005年从德国马格德堡奥托-冯-居里克大学获得过程与系统工程博士学位。她目前是湖南大学教授。她的研究兴趣包括机器视觉和模式识别。
- en: )Zhengeng Yang received the B.S. and M.S. degrees from Central South University,
    Changsha, China, in 2009 and 2012, respectively. He received the Phd degree from
    Hunan University, Changsha, China, in 2020\. He is currently a post-doctor researcher
    at Hunan University, Changsha. He was a Visiting Scholar with the University of
    Pittsburgh, Pittsburgh, PA during 2018 -2020\. His research interests include
    computer vision, image analysis, and machine learning.
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 杨振庚分别于2009年和2012年从中国中南大学获得学士和硕士学位，于2020年从中国湖南大学获得博士学位。他目前是湖南大学博士后研究员。2018年至2020年间，他曾在美国匹兹堡大学担任访问学者。他的研究兴趣包括计算机视觉、图像分析和机器学习。
- en: )Wei Sun received the M.S. and Ph.D. degrees of Control Science and Technology
    from the Hunan University, Changsha, China, in 1999 and 2002, respectively. He
    is currently a Professor at Hunan University. His research interests include artificial
    intelligence, robot control, complex mechanical and electrical control systems,
    and automotive electronics.
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 孙维分别于1999年和2002年从中国湖南大学获得控制科学与技术硕士和博士学位。他目前是湖南大学教授。他的研究兴趣包括人工智能、机器人控制、复杂的机电控制系统和汽车电子。
- en: )Ajmal Mian is a Professor of computer science with The University of Western
    Australia. His research interests include 3D computer vision, machine learning,
    point cloud analysis, human action recognition, and video description. He is Fellow
    of the International Association for Pattern Recognition and has received several
    awards including the HBF Mid-Career Scientist of the Year Award, the West Australian
    Early Career Scientist of the Year Award, the Aspire Professional Development
    Award, the Vice-Chancellors Mid-Career Research Award, the Outstanding Young Investigator
    Award, the IAPR Best Scientific Paper Award, the EH Thompson Award, and excellence
    in Research Supervision Award. He has received three prestigious fellowships and
    several major research grants from the Australian Research Council, the National
    Health and Medical Research Council of Australia and the US Dept of Defense DARPA
    with a total funding of over $40 Million. He serves as a Senior Editor for the
    IEEE Transactions on Neural Networks and Learning Systems, and Associate Editor
    for the IEEE Transactions on Image Processing, and the Pattern Recognition Journal.
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: Ajmal Mian 是西澳大利亚大学的计算机科学教授。他的研究兴趣包括3D计算机视觉、机器学习、点云分析、人类动作识别和视频描述。他是国际模式识别协会的会员，并获得了包括HBF年度中期科学家奖、西澳大利亚年度早期职业科学家奖、Aspire职业发展奖、副校长中期研究奖、杰出青年研究员奖、IAPR最佳科学论文奖、EH
    Thompson奖以及研究监督卓越奖在内的多个奖项。他获得了三项著名的研究奖学金和来自澳大利亚研究委员会、澳大利亚国家健康与医学研究委员会以及美国国防部DARPA的若干重大研究资助，总资金超过4000万美元。他担任《IEEE神经网络与学习系统汇刊》的高级编辑，以及《IEEE图像处理汇刊》和《模式识别期刊》的副编辑。
