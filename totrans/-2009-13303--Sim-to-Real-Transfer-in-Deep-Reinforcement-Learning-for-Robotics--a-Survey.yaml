- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:59:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:59:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.13303] Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics:
    a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.13303] 深度强化学习在机器人领域中的仿真到现实转移：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.13303](https://ar5iv.labs.arxiv.org/html/2009.13303)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.13303](https://ar5iv.labs.arxiv.org/html/2009.13303)
- en: 'Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习在机器人领域中的仿真到现实转移：综述
- en: Wenshuai Zhao¹, Jorge Peña Queralta¹, Tomi Westerlund¹ ¹Turku Intelligent Embedded
    and Robotic Systems Lab, University of Turku, Finland
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 赵文帅¹，豪尔赫·佩尼亚·奎拉尔塔¹，托米·韦斯特伦德¹ ¹土耳其大学智能嵌入式与机器人系统实验室，芬兰
- en: 'Emails: ¹{wezhao, jopequ, tovewe}@utu.fi'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 邮件：¹{wezhao, jopequ, tovewe}@utu.fi
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Deep reinforcement learning has recently seen huge success across multiple
    areas in the robotics domain. Owing to the limitations of gathering real-world
    data, i.e., sample inefficiency and the cost of collecting it, simulation environments
    are utilized for training the different agents. This not only aids in providing
    a potentially infinite data source, but also alleviates safety concerns with real
    robots. Nonetheless, the gap between the simulated and real worlds degrades the
    performance of the policies once the models are transferred into real robots.
    Multiple research efforts are therefore now being directed towards closing this
    sim-to-real gap and accomplish more efficient policy transfer. Recent years have
    seen the emergence of multiple methods applicable to different domains, but there
    is a lack, to the best of our knowledge, of a comprehensive review summarizing
    and putting into context the different methods. In this survey paper, we cover
    the fundamental background behind sim-to-real transfer in deep reinforcement learning
    and overview the main methods being utilized at the moment: domain randomization,
    domain adaptation, imitation learning, meta-learning and knowledge distillation.
    We categorize some of the most relevant recent works, and outline the main application
    scenarios. Finally, we discuss the main opportunities and challenges of the different
    approaches and point to the most promising directions.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习最近在机器人领域的多个领域取得了巨大成功。由于获取真实数据的局限性，即样本效率低和收集成本高，仿真环境被用来训练不同的智能体。这不仅提供了一个潜在的无限数据源，还缓解了真实机器人带来的安全隐患。然而，模拟世界与真实世界之间的差距在将模型转移到真实机器人时会降低策略的性能。因此，当前的多个研究工作正在致力于缩小这一仿真到现实的差距，实现更高效的政策转移。近年来出现了适用于不同领域的多种方法，但据我们所知，尚缺乏一篇全面综述并将不同方法置于背景中的文献。在这篇综述论文中，我们涵盖了深度强化学习中仿真到现实转移的基本背景，并概述了当前主要使用的方法：领域随机化、领域适应、模仿学习、元学习和知识蒸馏。我们对一些最相关的近期工作进行了分类，并概述了主要的应用场景。最后，我们讨论了不同方法的主要机会和挑战，并指出了最有前景的方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Deep Reinforcement Learning; Robotics; Sim-to-Real; Transfer Learning; Meta
    Learning; Domain Randomization; Knowledge Distillation; Imitation Learning;
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习；机器人技术；仿真到现实；迁移学习；元学习；领域随机化；知识蒸馏；模仿学习；
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Reinforcement learning (RL) algorithms have been increasingly adopted by the
    robotics community over the past years to control complex robots or multi-robot
    systems [[1](#bib.bib1), [2](#bib.bib2)], or provide end-to-end policies from
    perception to control [[3](#bib.bib3)]. Inspired by the way we learn through trial-and-error
    processes, RL algorithms base their knowledge acquisition in the rewards that
    agents obtain when they act in certain manners given different experiences. This
    naturally requires a large number of episodes, and therefore the learning limitations
    in terms of time and experience variability in real-world scenarios is evident.
    Moreover, learning with real robots requires the consideration of potentially
    dangerous or unexpected behaviors in safety-critical applications [[4](#bib.bib4)].
    Deep reinforcement learning (DRL) algorithms have been successfully deployed in
    various types of simulation environments, yet their success beyond simulated worlds
    has been limited. An exception to this is, however, robotic tasks involving object
    manipulation [[5](#bib.bib5), [6](#bib.bib6)]. In this survey, we review the most
    relevant works that try to answer a key research question in this direction: how
    to exploit simulation-based training in real-world settings by transferring the
    knowledge and adapting the policies accordingly (Fig. [1](#S1.F1 "Figure 1 ‣ I
    Introduction ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics:
    a Survey")).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）算法在过去几年中被机器人社区越来越多地采用，用于控制复杂的机器人或多机器人系统[[1](#bib.bib1), [2](#bib.bib2)]，或提供从感知到控制的端到端策略[[3](#bib.bib3)]。受我们通过试错过程学习的方式启发，RL算法基于智能体在不同体验下以特定方式行动时获得的奖励来进行知识获取。这自然需要大量的试验，因此在现实世界场景中，时间和经验变异性的学习限制是显而易见的。此外，使用真实机器人进行学习需要考虑安全关键应用中可能出现的危险或意外行为[[4](#bib.bib4)]。深度强化学习（DRL）算法已成功部署在各种类型的模拟环境中，但在模拟世界之外的成功有限。然而，一个例外是涉及物体操作的机器人任务[[5](#bib.bib5),
    [6](#bib.bib6)]。在这项调查中，我们回顾了试图回答这一方向关键研究问题的最相关工作：如何通过知识转移和策略调整在现实世界设置中利用基于模拟的训练（见图[1](#S1.F1
    "图 1 ‣ I 引言 ‣ 机器人深度强化学习中的模拟到现实转移：调查")）。
- en: '![Refer to caption](img/27bf217b42294f29d5a1102402bf6c56.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/27bf217b42294f29d5a1102402bf6c56.png)'
- en: 'Figure 1: Conceptual view of a simulation-to-reality transfer process. One
    of the most common methods is domain randomization, through which different parameters
    of the simulator (e..g, colors, textures, dynamics) are randomized to produce
    more robust policies.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：模拟到现实转移过程的概念视图。最常见的方法之一是领域随机化，通过该方法可以将模拟器的不同参数（例如，颜色、纹理、动态）随机化，以生成更稳健的策略。
- en: Simulation-based training provides data at low-cost, but involves inherent mismatches
    with real-world settings. Bridging the gap between simulation and reality requires,
    first of all, methods that are able to account for mismatches in both sensing
    and actuation. The former aspect has been widely studied in recent years within
    the deep learning field, for instance with adversarial attacks on computer vision
    algorithms [[7](#bib.bib7)]. The latter risk can be minimized through more realistic
    simulation. In both of these cases, some of the current approaches include works
    that introduce perturbances in the environment [[8](#bib.bib8)] or focus on domain
    randomization [[9](#bib.bib9)]. Another key aspect to take into account is that
    an agent deployed in the real world will potentially be exposed to novel experiences
    that were not present in the simulations [[10](#bib.bib10)], as well as the potential
    need to adapt their policies to encompass wider sets of tasks. Some of the approaches
    to bridge the gap in this direction rely on meta learning [[11](#bib.bib11)] or
    continual learning [[12](#bib.bib12)], among others.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模拟的训练提供了低成本的数据，但涉及到与真实世界设置固有的不匹配。弥合模拟与现实之间的差距首先需要能够考虑传感和执行方面不匹配的方法。前者在近年来深度学习领域得到了广泛研究，例如针对计算机视觉算法的对抗攻击[[7](#bib.bib7)]。后者的风险可以通过更现实的模拟来最小化。在这两种情况下，当前的一些方法包括引入环境扰动[[8](#bib.bib8)]或专注于领域随机化[[9](#bib.bib9)]。另一个关键方面是，部署在现实世界中的智能体可能会遇到模拟中不存在的新体验[[10](#bib.bib10)]，以及需要调整其策略以涵盖更广泛的任务集。弥合这一差距的一些方法依赖于元学习[[11](#bib.bib11)]或持续学习[[12](#bib.bib12)]等。
- en: 'The methods described above focus on extracting knowledge from simulation-trained
    agents in order to deploy them in real-life scenarios. However, other approaches
    exist to the same end. In recent years, simulators have been progressing towards
    more realistic scenarios and physics engines: Airsim [[13](#bib.bib13)], CARLA [[14](#bib.bib14)],
    RotorS [[15](#bib.bib15), [16](#bib.bib16)], and others [[17](#bib.bib17)]. With
    some of these simulators, part of the aim is to be able to deploy the robotic
    agents directly into the real world by providing training data and experiences
    with minimal mismatches between real and simulated settings. Other research efforts
    have been directed towards increasing safety during training in real-settings.
    Safety is one of the main challenges towards achieving online training of complex
    agents in the real-world, from robot arms to self-driving cars [[4](#bib.bib4)].
    In this direction, recent works have shown promising results towards safe DRL
    that is able to ensure convergence even while reducing the exploration space [[3](#bib.bib3)].
    In this survey, we do not cover specific simulators or techniques for direct learning
    in real-world settings, but instead focus on describing the main methods for transferring
    knowledge learned in simulation towards their deployment in real robotic platforms.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法专注于从模拟训练的代理中提取知识，以便将其部署到现实生活场景中。然而，还有其他方法可以达到相同的目的。近年来，模拟器在更现实的场景和物理引擎方面取得了进展：Airsim [[13](#bib.bib13)]、CARLA [[14](#bib.bib14)]、RotorS [[15](#bib.bib15),
    [16](#bib.bib16)]等。对于这些模拟器中的一些，部分目标是通过提供训练数据和经验，减少现实和模拟设置之间的差异，从而能够将机器人代理直接部署到现实世界中。其他研究则专注于提高在实际设置中训练的安全性。安全性是实现复杂代理在现实世界中在线训练的主要挑战之一，从机器人手臂到自动驾驶汽车 [[4](#bib.bib4)]。在这一方向上，最近的工作在安全DRL方面显示了有希望的结果，即使在减少探索空间的同时，也能确保收敛 [[3](#bib.bib3)]。在这项调查中，我们不涉及具体的模拟器或技术在现实世界设置中的直接学习，而是专注于描述从模拟中学到的知识向真实机器人平台部署的主要方法。
- en: This is, to the best of our knowledge, the first survey that describes the different
    methods being utilized towards closing the simulation-to-reality gap in DRL for
    robotics. We also concentrate on describing the main application fields of current
    research efforts. We discuss recent works from a wider point of view by including
    related research directions in the areas of transfer learning and domain adaptation,
    knowledge distillation, and meta reinforcement learning. While other surveys have
    focused on transfer learning techniques [[18](#bib.bib18)] or safe reinforcement
    learning [[4](#bib.bib4)], we provide a different point of view with an emphasis
    on DRL policy transfer in the robotics domain. Finally, there is also a significant
    amount of publications deploying DRL policies on real robots. In this survey,
    nonetheless, we focus on those works that specifically tackle issues in sim-to-real
    transfer. The focus is mostly in end-to-end approaches, but we also describe relevant
    research where sim-to-real transfer techniques are applied to the sensing aspects
    of robotic operation, primarily the transfer of DL vision algorithms to real robots.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这是第一项描述用于缩小机器人领域DRL中模拟与现实之间差距的不同方法的调查。我们还集中描述当前研究工作的主要应用领域。我们从更广泛的角度讨论了最近的工作，包括转移学习和领域适应、知识蒸馏和元强化学习等相关研究方向。尽管其他调查侧重于转移学习技术 [[18](#bib.bib18)]或安全强化学习 [[4](#bib.bib4)]，我们提供了不同的视角，重点是机器人领域的DRL策略转移。最后，也有大量的出版物在真实机器人上部署DRL策略。然而，在这项调查中，我们专注于那些特别解决模拟到现实转移问题的工作。重点主要是在端到端方法上，但我们也描述了相关的研究，其中模拟到现实转移技术应用于机器人操作的感知方面，主要是将DL视觉算法转移到真实机器人。
- en: 'The rest of this paper is organized as follows. In Section [II](#S2 "II Background
    ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey"),
    we briefly introduce the main approaches to DRL, together with related research
    directions in knowledge distillation, transfer, adaptation and meta learning.
    Section [III](#S3 "III Methodologies for Sim-to-Real Transfer ‣ Sim-to-Real Transfer
    in Deep Reinforcement Learning for Robotics: a Survey") then delves into the different
    approaches being taken towards closing the simulation-to-reality gap, with Section [IV](#S4
    "IV Application Scenarios ‣ Sim-to-Real Transfer in Deep Reinforcement Learning
    for Robotics: a Survey") focusing on the most relevant application areas. Then,
    we discuss open challenges and promising research directions in Section [V](#S5
    "V Main Challenges and Future Directions ‣ Sim-to-Real Transfer in Deep Reinforcement
    Learning for Robotics: a Survey"). Finally, Section [VI](#S6 "VI Conclusion ‣
    Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey") concludes
    this survey.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。在第[II](#S2 "II Background ‣ Sim-to-Real Transfer in Deep Reinforcement
    Learning for Robotics: a Survey")节中，我们简要介绍了深度强化学习的主要方法，以及知识蒸馏、迁移、适应和元学习等相关研究方向。第[III](#S3
    "III Methodologies for Sim-to-Real Transfer ‣ Sim-to-Real Transfer in Deep Reinforcement
    Learning for Robotics: a Survey")节则深入探讨了缩小模拟与现实之间差距的不同方法，第[IV](#S4 "IV Application
    Scenarios ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics:
    a Survey")节则重点关注最相关的应用领域。接下来，我们在第[V](#S5 "V Main Challenges and Future Directions
    ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey")节讨论了开放挑战和有前途的研究方向。最后，第[VI](#S6
    "VI Conclusion ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics:
    a Survey")节总结了本次调查。'
- en: II Background
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: 'Sim-to-real is a very comprehensive concept and applied in many fields including
    robotics and classic machine vision tasks. Thereby quite a few methods and concepts
    intersect with this aim including transfer learning, robust RL, and meta learning.
    In this section, we briefly introduce the concepts of deep reinforcement learning,
    knowledge distillation, transfer learning and domain adaption, before going into
    more details about sim-to-real transfer methods for DRL. The relationship between
    there concepts is illustrated in Fig. [2](#S2.F2 "Figure 2 ‣ II Background ‣ Sim-to-Real
    Transfer in Deep Reinforcement Learning for Robotics: a Survey").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '模拟到现实是一个非常综合的概念，应用于许多领域，包括机器人技术和经典的机器视觉任务。因此，许多方法和概念与这一目标交叉，包括迁移学习、鲁棒强化学习和元学习。在这一节中，我们简要介绍了深度强化学习、知识蒸馏、迁移学习和领域适应的概念，然后详细探讨了深度强化学习中的模拟到现实转移方法。这些概念之间的关系在图[2](#S2.F2
    "Figure 2 ‣ II Background ‣ Sim-to-Real Transfer in Deep Reinforcement Learning
    for Robotics: a Survey")中进行了说明。'
- en: '![Refer to caption](img/970cc5f7d347a424f183a0d302686b99.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/970cc5f7d347a424f183a0d302686b99.png)'
- en: 'Figure 2: Illustration of the different methods related to sim-to-real transfer
    in deep reinforcement learning and their relationships.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同的深度强化学习模拟到现实转移方法及其关系的示意图。
- en: II-A Deep Reinforcement Learning
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度强化学习
- en: 'A standard reinforcement learning (RL) task can be regarded as a sequential
    decision making setup which consists of an agent interacting with an environment
    in discrete steps. The agent takes an action $a_{t}$ at each timestep t, causing
    the environment to change its state from $s_{t}$ to $s_{t+1}$ with a transition
    probability $p(s_{t+1}|s_{t},a_{t})$. This setup can be regarded as a Markov decision
    process (MDP) with a set of states $s\in\mathcal{S}$, actions $a\in\mathcal{A}$,
    transitions $p\in\mathcal{P}$ and rewards $r\in\mathcal{R}$. Therefore we can
    define this MDP as a tuple ([1](#S2.E1 "In II-A Deep Reinforcement Learning ‣
    II Background ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics:
    a Survey")).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '标准的强化学习（RL）任务可以被视为一个顺序决策设置，其中一个代理在离散的步骤中与环境交互。代理在每个时间步t采取一个动作$a_{t}$，使得环境的状态从$s_{t}$变为$s_{t+1}$，转移概率为$p(s_{t+1}|s_{t},a_{t})$。这种设置可以被视为一个马尔可夫决策过程（MDP），其状态集合为$s\in\mathcal{S}$，动作集合为$a\in\mathcal{A}$，转移概率集合为$p\in\mathcal{P}$，奖励集合为$r\in\mathcal{R}$。因此，我们可以将这个MDP定义为一个元组（[1](#S2.E1
    "In II-A Deep Reinforcement Learning ‣ II Background ‣ Sim-to-Real Transfer in
    Deep Reinforcement Learning for Robotics: a Survey")）。'
- en: '|  | $D\equiv(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R})$ |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $D\equiv(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R})$ |  | (1) |'
- en: The objective of reinforcement learning is to maximize the expected reward by
    choosing an optimal policy which will be represented via a deep neural network
    in DRL. Accelerated by modern computation capacity, DRL has shown significant
    success on various applications [[1](#bib.bib1), [19](#bib.bib19)], but particular
    in the simulated environment [[20](#bib.bib20)]. Therefore, how to transfer this
    success from simulation to reality is drawing more and more attention, which is
    also the motivation of this paper.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是通过选择一个最优策略来最大化预期奖励，该策略将在深度强化学习（DRL）中通过深度神经网络表示。得益于现代计算能力的加速，DRL在各种应用中取得了显著成功 [[1](#bib.bib1),
    [19](#bib.bib19)]，尤其是在模拟环境中 [[20](#bib.bib20)]。因此，如何将这种成功从模拟转移到现实中引起了越来越多的关注，这也是本文的动机。
- en: II-B Sim-to-Real Transfer
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 模拟到现实转移
- en: Transferring DRL policies form simulation environments to reality is a necessary
    step towards more complex robotic systems that have DL-defined controllers. This,
    however, is not a problem specific to DRL algorithms, but ML in general. While
    most DRL algorithms provide end-to-end policies, i.e., control mechanisms that
    take raw sensor data as inputs and produce direct actuation commands as outputs,
    these two dimensions of robotics can be separated. Closing the gap between simulation
    and reality gap in terms of actuation requires simulators to be more accurate,
    and to account for variability in agent dynamics. On the sensing part, however,
    the problem can be considered wider, as it also involves the more general ML problem
    of facing situations in the real world that have not appeared in simulation [[10](#bib.bib10)].
    In this paper, we focus mostly on end-to-end models, and overview both research
    directed towards system modeling and dynamics randomization, as well as research
    introducing randomization from the sensing point of view.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将DRL策略从模拟环境转移到现实是实现更复杂机器人系统（具有深度学习定义控制器）的必要步骤。然而，这不仅是DRL算法特有的问题，而是一般机器学习的问题。尽管大多数DRL算法提供端到端的策略，即将原始传感器数据作为输入并产生直接执行命令作为输出的控制机制，但这两个维度的机器人系统可以分开考虑。在执行方面缩小模拟与现实之间的差距需要模拟器更准确，并考虑代理动态的变异性。然而，在感知方面，问题可以被认为更广泛，因为它还涉及到面对现实世界中未在模拟中出现的情况的更一般的机器学习问题 [[10](#bib.bib10)]。本文主要集中在端到端模型上，概述了针对系统建模和动态随机化的研究，以及从感知角度引入随机化的研究。
- en: II-C Transfer Learning and Domain Adaptation
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 迁移学习与领域适应
- en: Transfer learning aims at improving the performance of target learners on target
    domains by transferring the knowledge contained in different but related source
    domains [[18](#bib.bib18)]. In this way, transfer learning can reduce the dependence
    of target domain data when constructing target learners.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的目标是通过将不同但相关源领域中的知识转移到目标领域，来提高目标学习者在目标领域的表现 [[18](#bib.bib18)]。通过这种方式，迁移学习可以减少在构建目标学习者时对目标领域数据的依赖。
- en: Domain adaptation is a subset of transfer learning methods. It specifies the
    situation when we have sufficient source domain labeled data and the same single
    task as the target task, but without or very few target domain data. In sim-to-real
    robotics, researchers tend to employ a simulator to train the RL model and then
    deploy it in the realistic environment, where we should take advantage of the
    domain adaptation techniques in order to transfer the simulation based model well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应是迁移学习方法的一个子集。它指定了当我们有足够的源领域标记数据和与目标任务相同的单一任务，但没有或仅有很少的目标领域数据时的情况。在模拟到现实的机器人中，研究人员倾向于使用模拟器来训练强化学习模型，然后在现实环境中部署它，在这里我们应利用领域适应技术，以便将基于模拟的模型有效转移。
- en: II-D Knowledge Distillation
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 知识蒸馏
- en: Large networks are typical in DRL with high-dimensional input data (e.g, complex
    visual tasks). Policy distillation is the process of extracting knowledge to train
    a new network that is able to maintain a similarly expert level while being significantly
    smaller and more efficient [[21](#bib.bib21)]. In these set-ups, the two networks
    are typically called teacher and student. The student is trained in a supervised
    manner with data generated by the teacher network. In [[12](#bib.bib12)], the
    authors presented DisCoRL, a modular, effective and scalable pipeline for continual
    DRL. DisCoRL has been succesfully applied to multiple tasks learned by different
    teachers, with their knowledge being distilled to a single student network.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有高维输入数据（例如复杂视觉任务）的DRL中，大型网络是典型的。策略蒸馏是提取知识以训练一个新网络的过程，该网络能够保持类似的专家水平，同时显著更小且更高效[[21](#bib.bib21)]。在这些设置中，这两个网络通常被称为教师网络和学生网络。学生网络以教师网络生成的数据进行监督训练。在[[12](#bib.bib12)]中，作者提出了DisCoRL，一个模块化、有效且可扩展的持续DRL管道。DisCoRL已经成功应用于多个任务，通过不同教师的知识被蒸馏到单个学生网络中。
- en: II-E Meta Reinforcement Learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 元强化学习
- en: Meta Learning, namely learning to learn, aims to learn the adaptation ability
    to unseen test tasks from multiple training tasks. A good meta learning model
    should be trained across a variety of learning tasks and optimized for the best
    performance over a distribution of tasks, including potentially unseen tasks when
    tested. This spirit can be applied on both supervised learning and reinforcement
    learning, and in the latter case it is called meta reinforcement learning (MetaRL) [[22](#bib.bib22)].
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习，即学习如何学习，旨在从多个训练任务中学习对未见测试任务的适应能力。一个好的元学习模型应该在各种学习任务上进行训练，并在任务分布上优化以获得最佳性能，包括在测试时可能未见过的任务。这一精神可以应用于监督学习和强化学习，在后者的情况下被称为元强化学习（MetaRL）[[22](#bib.bib22)]。
- en: The overall configuration of MetaRL is similar to an ordinary RL algorithm,
    except that MetaRL usually implements an LSTM policy and incorporates the last
    reward $r_{t-1}$ and last action $a_{t-1}$ into the current policy observation.
    In this case, the LSTM’s hidden states serve as a memory for tracking characteristics
    of the trajectories. Therefore, MetaRL draws knowledge from past training.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MetaRL的整体配置类似于普通RL算法，不同之处在于MetaRL通常实现一个LSTM策略，并将上一个奖励 $r_{t-1}$ 和上一个动作 $a_{t-1}$
    纳入当前策略观察中。在这种情况下，LSTM的隐藏状态作为跟踪轨迹特征的记忆。因此，MetaRL从过去的训练中汲取知识。
- en: II-F Robust RL and Imitation Learning
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-F 稳健的RL和模仿学习
- en: Robust RL [[23](#bib.bib23)] was proposed quite early as a new RL paradigm that
    explicitly takes into account input disturbances as well as modeling errors. It
    considers a bad, or even adversarial model and tries to maximize the reward as
    a optimization problem [[24](#bib.bib24), [25](#bib.bib25)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健的RL[[23](#bib.bib23)]早期就被提出作为一种新的RL范式，该范式明确考虑输入扰动以及建模误差。它考虑了一个不良或甚至对抗性的模型，并尝试将奖励最大化作为一个优化问题[[24](#bib.bib24),
    [25](#bib.bib25)]。
- en: Imitation learning proposes to employ expert demonstration or trajectories instead
    of manually constructing a fixed reward function to train RL agents. The methods
    of imitation learning can be broadly classified into two key areas: behaviour
    cloning where an agent learns a mapping from observations to actions given demonstrations [[26](#bib.bib26),
    [27](#bib.bib27)] and  inverse reinforcement learning where an agent attempts
    to estimate a reward function that describes the given demonstrations [[28](#bib.bib28)].
    Because it aims to give a robust reward for RL agents, sometimes imitation learning
    can be utilized to obtain robust RL or sim-to-real transfer [[29](#bib.bib29)].
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习提议使用专家演示或轨迹，而不是手动构建固定的奖励函数来训练RL智能体。模仿学习的方法大致可以分为两个关键领域：行为克隆，其中智能体学习从观察到动作的映射，给定演示[[26](#bib.bib26),
    [27](#bib.bib27)]；以及逆向强化学习，其中智能体尝试估计描述给定演示的奖励函数[[28](#bib.bib28)]。由于其旨在为RL智能体提供稳健的奖励，因此有时模仿学习可以用于获得稳健的RL或模拟到现实的转移[[29](#bib.bib29)]。
- en: 'TABLE I: Classification of the most relevant publications in Sim2Real Transfer.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: Sim2Real 转移中最相关出版物的分类。'
- en: '|  | Description | Sim-to-real transfer | Multi-agent | Simulator | Knowledge
    | Learning | Real | Application |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | 描述 | 模拟到现实转移 | 多智能体 | 模拟器 | 知识 | 学习 | 现实 | 应用 |'
- en: '|  | and learning details | learning | / Engine | Transfer | Algorithm | Robot/Platform
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | 和学习细节 | 学习 | / 引擎 | 转移 | 算法 | 机器人/平台 |'
- en: '| Balaji et al. [[30](#bib.bib30)] | DeepRacer: an educational autonomous racing
    platform. | Random colors and parallel domain randomization | ✓(sim only) Distr.
    rollout | Gazebo RoboMaker | ✗ | PPO | DeepRacer 4WD 1:18 Car | Autonomous racing
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| Balaji et al. [[30](#bib.bib30)] | DeepRacer：一个教育性的自主赛车平台。 | 随机颜色和并行领域随机化
    | ✓(仅模拟) 分布式回滚 | Gazebo RoboMaker | ✗ | PPO | DeepRacer 4WD 1:18车 | 自主赛车 |'
- en: '| Traore et al. [[12](#bib.bib12)] | Continual RL with policy distillation
    and sim-to-real transfer. | Continual learning with policy distillation. | ✗ |
    PyBullet | ✓Multi-task Distillation | PPO2 | Small mobile platform | Robotic navigation
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Traore et al. [[12](#bib.bib12)] | 带有策略蒸馏和Sim-to-real转移的持续RL。 | 通过策略蒸馏实现持续学习。
    | ✗ | PyBullet | ✓多任务蒸馏 | PPO2 | 小型移动平台 | 机器人导航 |'
- en: '| Kaspar et al. [[31](#bib.bib31)] | Sim-to-real transfer for RL without Dynamics
    Randomization. | System identification and a high-quality robot model. | ✗ | PyBullet
    | ✗ | SAC | KUKA LBR iiwa +WSG50 gripper | Peg-in-Hole manipulation |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Kaspar et al. [[31](#bib.bib31)] | 无动态随机化的Sim-to-real转移用于RL。 | 系统识别和高质量机器人模型。
    | ✗ | PyBullet | ✗ | SAC | KUKA LBR iiwa + WSG50夹具 | 钉孔操控 |'
- en: '| Matas et al. [[6](#bib.bib6)] | Sim-to-real RL for deformable object manipulation.
    | Stochastic grasping and domain radomization. | ✓(sim) | PyBullet | ✗ | DDPGfD
    | 7DOF Kinova Mico Arm | Dexterous manipulation |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Matas et al. [[6](#bib.bib6)] | 针对可变形物体操控的Sim-to-real RL。 | 随机抓取和领域随机化。 |
    ✓(模拟) | PyBullet | ✗ | DDPGfD | 7DOF Kinova Mico手臂 | 灵巧操控 |'
- en: '| Witman et al. [[32](#bib.bib32)] | Sim-to-real RL for thermal effects of
    an atmospheric pressure plasma jet. | Custom physics model and dynamics randomization
    | ✗ | Custom | ✗ | A3C | kHz-excited APPJ in He | Plasma jet control |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Witman et al. [[32](#bib.bib32)] | 针对大气压力等离子体喷射的热效应的Sim-to-real RL。 | 自定义物理模型和动态随机化
    | ✗ | 自定义 | ✗ | A3C | 高频激励的氦气APPJ | 等离子体喷射控制 |'
- en: '| Jeong et al. [[33](#bib.bib33)] | Modeling Generalized Forces with RL for
    Sim2Real Transfer | Modeling and learning state dependent generalized forces.
    | ✗ | MuJoCo | ✗ | MPO | Rethink Robotics Sawyer | Nonprehensile manipulation
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Jeong et al. [[33](#bib.bib33)] | 使用RL建模广义力以实现Sim2Real转移 | 建模和学习状态依赖的广义力。
    | ✗ | MuJoCo | ✗ | MPO | Rethink Robotics Sawyer | 非抓取操控 |'
- en: '| Arndt et al. [[11](#bib.bib11)] | Meta Reinforcement Learning for Sim2Real
    Domain Adaptation | Domain random. and model-agnostic meta-learning. | ✗ | MuJoCo
    | ✓Meta- training | PPO | Kuka LBR 4+ arm | Manipulation (hockey puck) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Arndt et al. [[11](#bib.bib11)] | 用于Sim2Real领域适配的元强化学习 | 领域随机化和模型无关的元学习。
    | ✗ | MuJoCo | ✓元训练 | PPO | Kuka LBR 4+手臂 | 操控（曲棍球） |'
- en: '| Breyer et al. [[34](#bib.bib34)] | Flexible robotic grasping with Sim2Real
    RL | Direct transfer. Elliptic mask to RGB-D images. | ✗ | PyBullet | ✗ | TRPO
    | ABB YuMi with parallel-jaw gripper | Robotic Grasping |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| Breyer et al. [[34](#bib.bib34)] | 使用Sim2Real RL的灵活机器人抓取 | 直接转移。椭圆掩膜到RGB-D图像。
    | ✗ | PyBullet | ✗ | TRPO | ABB YuMi与平行夹具 | 机器人抓取 |'
- en: '| Van Baar et al. [[35](#bib.bib35)] | Sim-to-real transfer with robustified
    policies for robot tasks. | Variation of appearance and/ or physics parameters.
    | ✓(sim) | MuJoCo +Ogre 3D | ✓ | A3C (sim) +Off-policy | Mitsubishi Melfa RV-6SL
    | Marble Maze Manipulation |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Van Baar et al. [[35](#bib.bib35)] | 用于机器人任务的Sim-to-real转移与稳健化策略。 | 外观和/或物理参数的变化。
    | ✓(模拟) | MuJoCo + Ogre 3D | ✓ | A3C（模拟） + 离策略 | 三菱Melfa RV-6SL | 大理石迷宫操控 |'
- en: '| Bassani et al. [[36](#bib.bib36)] | Sim2Real RL for robotic soccer competitions.
    | Domain adaptation and custom simulator for transfer. | ✗ | VSSS-RL | ✓ | DDPG
    /DQN | VSSS Robot | Robotic Navigation |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Bassani et al. [[36](#bib.bib36)] | 用于机器人足球比赛的Sim2Real RL。 | 领域适配和自定义模拟器用于转移。
    | ✗ | VSSS-RL | ✓ | DDPG /DQN | VSSS机器人 | 机器人导航 |'
- en: '| Qin et al. [[37](#bib.bib37)] | Sim2Real for six-legged robots with DRL and
    curriculum learning. | Curriculum learning with inverse kinematics. | ✗ | V-Rep
    | ✓ | PPO | Six-legged robot | Navigation and obstacle avoid. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Qin et al. [[37](#bib.bib37)] | 使用DRL和课程学习实现六足机器人Sim2Real。 | 课程学习与逆向运动学。
    | ✗ | V-Rep | ✓ | PPO | 六足机器人 | 导航与避障 |'
- en: '| Vacaro et al. [[38](#bib.bib38)] | Sim-to-real in reinforcement learning
    for everyone | Domain randomization (light + color + textures). | ✓(sim) | Unity3D
    | ✗ | IMPALA | Sainsmart robot arm | Low-cost robot arm |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Vacaro et al. [[38](#bib.bib38)] | 针对每个人的强化学习的Sim-to-real | 领域随机化（光 + 颜色
    + 纹理）。 | ✓(模拟) | Unity3D | ✗ | IMPALA | Sainsmart机器人手臂 | 低成本机器人手臂 |'
- en: '| Chaffre et al. [[39](#bib.bib39)] | Sim-to-Real Transfer with Incremental
    Environment Complexity | SAC training using incremental environment complexity.
    | ✗ | Gazebo | ✗ | DDPG /SAC | Wifibot Lab V4 | Mapless navigation |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Chaffre et al. [[39](#bib.bib39)] | 使用递增环境复杂度的Sim-to-Real转移 | 使用递增环境复杂度进行SAC训练。
    | ✗ | Gazebo | ✗ | DDPG /SAC | Wifibot Lab V4 | 无地图导航 |'
- en: '| Kaspar et al. [[40](#bib.bib40)] | Rl with Cartesian Commands for Peg in
    Hole Tasks. | Dynamics (CMA-ES) and environment randomization. | ✗ | PyBullet
    | ✗ | SAC | Kuka LBR iiwa | Peg-in-hole tasks |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Kaspar et al. [[40](#bib.bib40)] | 用于孔中插销任务的笛卡尔命令的强化学习。 | 动态（CMA-ES）和环境随机化。
    | ✗ | PyBullet | ✗ | SAC | Kuka LBR iiwa | 插销任务 |'
- en: '| Hundt et al. [[41](#bib.bib41)] | Efficient RL for Multi-Step Visual Tasks
    via Reward Shaping. | Direct transfer with custom simulation framework. | ✗ |
    SPOT Framework | ✗ | SPOT-Q +PER | Universal Robot UR5 | Long-term multi-step
    tasks |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Hundt et al. [[41](#bib.bib41)] | 通过奖励塑形进行高效的多步视觉任务强化学习。 | 使用自定义仿真框架的直接转移。
    | ✗ | SPOT框架 | ✗ | SPOT-Q + PER | Universal Robot UR5 | 长期多步任务 |'
- en: '| Pedersen et al. [[42](#bib.bib42)] | Sim-to-Real Transfer for Gripper Pose
    Estimation with GAN | CycleGANs for domain adaption and transfer. | ✗ | Unity
    | ✗ | PPO | Panda robot | Robotic Grippers |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| Pedersen et al. [[42](#bib.bib42)] | 基于GAN的抓取姿态估计的仿真到现实转移 | 用于领域适应和转移的CycleGAN。
    | ✗ | Unity | ✗ | PPO | Panda机器人 | 机器人抓手 |'
- en: '| Ding et al. [[43](#bib.bib43)] | Sim-to-Real Transfer for Optical Tactile
    Sensing | Analysis of different amounts of randomization. | ✗ | PyBullet | ✗ |
    CNN | Sawyer robot +TacTip sensor | Tactile sensing |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Ding et al. [[43](#bib.bib43)] | 光学触觉传感的仿真到现实转移 | 不同随机化量的分析。 | ✗ | PyBullet
    | ✗ | CNN | Sawyer机器人 + TacTip传感器 | 触觉传感 |'
- en: '| Muratore et al. [[9](#bib.bib9)] | Data-efficient Bayesian Domain Randomization
    for sim-to-real | Proposed bayesian randomization (BAYR). | ✗ | Custom/ BoTorch
    | ✗ | PPO / RF Classifier | Quanser Qube | swing-up/ balancing |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Muratore et al. [[9](#bib.bib9)] | 数据高效的贝叶斯领域随机化用于仿真到现实 | 提出的贝叶斯随机化（BAYR）。
    | ✗ | 自定义/ BoTorch | ✗ | PPO / RF分类器 | Quanser Qube | 摆动/平衡 |'
- en: '| Zhao et al. [[8](#bib.bib8)] | Towards closing the sim-to-real gap in collaborative
    DRL with perturbances | Domain randomization (custom perturbations) | ✓(sim) |
    Pybullet | ✗ | PPO | Kuka (sim-only) | Robot arm reacher |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[8](#bib.bib8)] | 通过扰动缩小协作DRL中的仿真到现实差距 | 领域随机化（自定义扰动） | ✓（仿真）
    | Pybullet | ✗ | PPO | Kuka（仅仿真） | 机器人臂 |'
- en: '| Nachum et al. [[44](#bib.bib44)] | Multi-agent manipulation via locomotion
    | Hierarchichal sim-to-real, model-free, zero-shot transfer. | ✓ | MuJoCo | ✗
    | Custom | D’Kitty robo (2x) | Multi-agent manipulation |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Nachum et al. [[44](#bib.bib44)] | 通过运动学实现的多智能体操作 | 分层仿真到现实、无模型、零样本转移。 |
    ✓ | MuJoCo | ✗ | Custom | D’Kitty机器人（2x） | 多智能体操作 |'
- en: '| Rajeswaran et al. [[5](#bib.bib5)] | Dexterous manipulation with DRL and
    demonstrators. | Imitation learning via demonstrators with VR. | ✗ | MuJoCo |
    ✗ | DAPG | ADROIT 24-DoF Hand | Multi-fingered robot hands |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Rajeswaran et al. [[5](#bib.bib5)] | 使用DRL和示范者进行灵巧操作。 | 通过示范者与VR进行模仿学习。 |
    ✗ | MuJoCo | ✗ | DAPG | ADROIT 24-DoF手 | 多指机器人手 |'
- en: III Methodologies for Sim-to-Real Transfer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 仿真到现实转移的方法论
- en: Research in sim-to-real transfer has resulted in an increase of several orders
    of magnitude in the number of publications over the past few years. Multiple research
    directions have been followed, and we summarize in this section the most representative
    methods for sim-to-real transfer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在仿真到现实转移（sim-to-real transfer）的研究中，过去几年里出版物的数量增加了几个数量级。研究方向多样，本节总结了仿真到现实转移中最具代表性的方法。
- en: 'Table [I](#S2.T1 "TABLE I ‣ II-F Robust RL and Imitation Learning ‣ II Background
    ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey")
    lists some of the most relevant and recent works in this field. The most widely
    used method for learning transfer is domain randomization, with other relevant
    examples including policy distillation, system identification, or meta-RL. The
    variability in terms of learning algorithms is higher, with DRL using proximal
    policy optimization (PPO) [[45](#bib.bib45)], trust region policy optimization
    (TRPO) [[46](#bib.bib46)], maximum a-posteriori policy optimization (MPO) [[47](#bib.bib47)],
    asynchronous actor critic (A3C) methods [[48](#bib.bib48)], soft actor critic
    (SAC) [[49](#bib.bib49)], or deep deterministic policy gradient (DDPG) [[50](#bib.bib50)],
    among others.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '表[I](#S2.T1 "TABLE I ‣ II-F Robust RL and Imitation Learning ‣ II Background
    ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey")列出了该领域一些最相关和最新的研究。学习转移中最广泛使用的方法是领域随机化，其他相关示例包括策略蒸馏、系统识别或元强化学习（meta-RL）。学习算法的变异性较高，DRL使用的算法包括邻近策略优化（PPO） [[45](#bib.bib45)]、信任区域策略优化（TRPO） [[46](#bib.bib46)]、最大后验策略优化（MPO） [[47](#bib.bib47)]、异步演员评论家（A3C）方法 [[48](#bib.bib48)]、软演员评论家（SAC） [[49](#bib.bib49)]，或深度确定性策略梯度（DDPG） [[50](#bib.bib50)]，等等。'
- en: III-A Zero-shot Transfer
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 零样本转移
- en: 'The most straightforward way of transferring knowledge from simulation to reality
    is to build a realistic simulator, or to have enough simulated experience, so
    that the model can be directly applied in real-world settings. This strategy is
    commonly referred to as zero-shot or direct transfer. System identification to
    build precise models of the real world and domain randomization are techniques
    that can be seen as one-shot transfer. We discuss both of these separately in
    Sections [III-B](#S3.SS2 "III-B System Identification ‣ III Methodologies for
    Sim-to-Real Transfer ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for
    Robotics: a Survey") and [III-C](#S3.SS3 "III-C Domain Randomization Methods ‣
    III Methodologies for Sim-to-Real Transfer ‣ Sim-to-Real Transfer in Deep Reinforcement
    Learning for Robotics: a Survey").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将知识从模拟转移到现实的最直接方法是构建一个逼真的模拟器，或拥有足够的模拟经验，以便模型可以直接应用于现实环境。这种策略通常被称为零样本或直接转移。系统识别用于构建精确的现实世界模型，而领域随机化则是可以视为一次性转移的技术。我们在第[III-B节](#S3.SS2
    "III-B 系统识别 ‣ III 从模拟到现实转移的方法 ‣ 深度强化学习中的从模拟到现实转移：综述")和第[III-C节](#S3.SS3 "III-C
    领域随机化方法 ‣ III 从模拟到现实转移的方法 ‣ 深度强化学习中的从模拟到现实转移：综述")中分别讨论了这两者。
- en: III-B System Identification
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 系统识别
- en: It is of note that simulators are not faithful representation of the real world.
    System identification [[51](#bib.bib51)] is exactly to build a precise mathematical
    model for a physical system and to make the simulator more realistic careful calibration
    is necessary. Nonetheless, challenges for obtaining a realistic enough simulator
    are still existing. For example, it is hard to build high-quality rendered image
    to simulate the real vision. Furthermore, many physical parameters of the same
    robot might vary significantly due to temperature, humidity, positioning or its
    wear-and-tear in time, which brings more difficulty for system identification.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，模拟器并不是现实世界的真实表示。系统识别[[51](#bib.bib51)]正是为了构建一个精确的物理系统数学模型，并使模拟器更现实，需要仔细的校准。然而，获得一个足够逼真的模拟器仍然面临挑战。例如，很难构建高质量的渲染图像来模拟真实视觉。此外，由于温度、湿度、定位或磨损等因素，相同机器人许多物理参数可能会显著变化，这给系统识别带来了更多困难。
- en: III-C Domain Randomization Methods
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 领域随机化方法
- en: 'Domain randomization is the idea that [[52](#bib.bib52)], instead of carefully
    modeling all the parameters of the real world, we could highly randomize the simulation
    in order to cover the real distribution of the real-world data despite the bias
    between the model and real world. Fig. [3(a)](#S3.F3.sf1 "In Figure 3 ‣ III-C
    Domain Randomization Methods ‣ III Methodologies for Sim-to-Real Transfer ‣ Sim-to-Real
    Transfer in Deep Reinforcement Learning for Robotics: a Survey") shows the paradigm
    of domain randomization.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 领域随机化的理念是[[52](#bib.bib52)]，与其仔细建模现实世界的所有参数，不如高度随机化模拟，以覆盖现实世界数据的实际分布，尽管模型和现实世界之间存在偏差。图[3(a)](#S3.F3.sf1
    "图3 ‣ III-C 领域随机化方法 ‣ III 从模拟到现实转移的方法 ‣ 深度强化学习中的从模拟到现实转移：综述")展示了领域随机化的范式。
- en: '![Refer to caption](img/ec1ea52f8af6b461980e7f9d1a95035c.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec1ea52f8af6b461980e7f9d1a95035c.png)'
- en: (a) Intuition behind the domain randomization paradigm.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 领域随机化范式的直观理解。
- en: '![Refer to caption](img/2b534a177ffa1d9684213cd17693cbdf.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b534a177ffa1d9684213cd17693cbdf.png)'
- en: (b) Intuition behind the domain adaptation paradigm.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 领域适应范式的直观理解。
- en: 'Figure 3: Illustration of two of the most widely used methods for sim-to-real
    transfer in DRL. Domain randomization and domain adaptation are often applied
    as separate techniques, but they can also be applied together.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：展示了深度强化学习中最广泛使用的两种从模拟到现实转移的方法。领域随机化和领域适应通常作为独立技术应用，但它们也可以结合使用。
- en: 'According to the components of the simulator randomized, we divide the methods
    of domain randomization into two kinds: visual randomization and dynamics randomization.
    In robotic vision tasks including object localization [[53](#bib.bib53)], object
    detection [[54](#bib.bib54)], pose estimation [[55](#bib.bib55)], and semantic
    segmentation [[56](#bib.bib56)], the training data from simulator always have
    different textures, lighting, and camera positions from the realistic environments.
    Therefore, visual domain randomization aims to provide enough simulated variability
    of the visual parameters at training time such that at test time the model is
    able to generalize to real-world data. In addition to adding randomization to
    the visual input, dynamics randomization could also help acquire a robust policy
    particularly where the controlling policy is needed. To learn dexterous in-hand
    manipulation policies for a physical five-fingered hand, [[57](#bib.bib57)] randomizes
    various physical parameters in the simulator, such as object dimensions, objects
    and robot link masses, surface friction coefficients, robot joint damping coefficients
    and actuator force gains. Their successful sim-to-real transfer experiments show
    the powerful effect of domain randomization.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模拟器组件的随机化，我们将领域随机化的方法分为两种：视觉随机化和动态随机化。在机器人视觉任务中，包括物体定位[[53](#bib.bib53)]、物体检测[[54](#bib.bib54)]、姿态估计[[55](#bib.bib55)]和语义分割[[56](#bib.bib56)]，模拟器生成的训练数据通常在纹理、光照和摄像机位置上与现实环境有所不同。因此，视觉领域随机化旨在提供足够的视觉参数模拟变异性，以便模型在测试时能够对真实世界数据进行泛化。除了在视觉输入中添加随机化外，动态随机化还可以帮助获取稳健的策略，特别是在需要控制策略的情况下。为了学习物理五指手的灵巧的手内操控策略，[[57](#bib.bib57)]
    在模拟器中随机化了各种物理参数，如物体尺寸、物体和机器人连杆质量、表面摩擦系数、机器人关节阻尼系数和驱动器力增益。他们成功的模拟到真实转移实验展示了领域随机化的强大效果。
- en: Besides usually making the simulated data randomized to cover the real-world
    data distribution, [[58](#bib.bib58)] provides another interesting angle to apply
    domain randomization. They proposes to translate the randomized simulated image
    and real-world into the canonical sim images and demonstrate the effectiveness
    of this sim-to-real approach by training a vision-based closed-loop grasping RL
    agent in simulation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通常使模拟数据随机化以覆盖真实世界的数据分布，[[58](#bib.bib58)] 提供了另一个有趣的角度来应用领域随机化。他们提出将随机化的模拟图像和真实世界图像转化为规范的模拟图像，并通过在模拟中训练基于视觉的闭环抓取强化学习代理来展示这种模拟到真实的方法的有效性。
- en: III-D Domain Adaptation Methods
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 领域适应方法
- en: 'Domain adaptation methods use data from source domain to improve the performance
    of a learned model on a different target domain where data is always less available.
    Since usually there are different feature spaces between the source domain and
    target domain, in order to better transfer the knowledge from source data, we
    should attempt to make these two feature space unified. This is the main spirit
    of domain adaptation, and can be described by the diagram in Fig. [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ III-C Domain Randomization Methods ‣ III Methodologies for Sim-to-Real
    Transfer ‣ Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a
    Survey").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 领域适应方法使用源领域的数据来提高学习模型在目标领域的性能，而目标领域的数据通常较少。由于源领域和目标领域之间通常存在不同的特征空间，为了更好地转移源数据中的知识，我们应尝试使这两个特征空间统一。这是领域适应的主要精神，可以通过图[3(b)](#S3.F3.sf2
    "在图3 ‣ III-C 领域随机化方法 ‣ III 模拟到真实转移的方法 ‣ 深度强化学习在机器人中的模拟到真实转移：综述")中的图示来描述。
- en: The research of domain adaptation is broadly conducted recently in vision-based
    tasks, such as image classification and semantic segmentation [[59](#bib.bib59),
    [60](#bib.bib60)]. However, in this paper we focus on the tasks related with reinforcement
    learning and the ones applied to robotics. In these scenarios, the pure vision
    related tasks employing domain adaptation play as priors to the succeeding building
    reinforcement learning agents or other controlling tasks [[58](#bib.bib58), [61](#bib.bib61),
    [29](#bib.bib29)]. There is also some image-to-policy work using domain adaptation
    to generalize the policy learned by synthetic data or speed up the learning on
    real-world robots [[61](#bib.bib61)]. Sometimes domain adaptation is used to directly
    transfer the policy between agents [[62](#bib.bib62)].
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，领域适应的研究在基于视觉的任务中广泛进行，例如图像分类和语义分割 [[59](#bib.bib59), [60](#bib.bib60)]。然而，在本文中，我们专注于与强化学习相关的任务及应用于机器人领域的任务。在这些场景中，采用领域适应的纯视觉任务作为后续构建强化学习代理或其他控制任务的先验知识 [[58](#bib.bib58),
    [61](#bib.bib61), [29](#bib.bib29)]。此外，还有一些使用领域适应进行图像到策略工作的研究，以将合成数据学习的策略进行推广，或加速在实际机器人上的学习 [[61](#bib.bib61)]。有时，领域适应被直接用于在代理之间转移策略 [[62](#bib.bib62)]。
- en: 'Specifically, we now formalize the domain adaptation scenarios in a reinforcement
    learning setting [[63](#bib.bib63)]. Based on the definition of MDP in equation ([1](#S2.E1
    "In II-A Deep Reinforcement Learning ‣ II Background ‣ Sim-to-Real Transfer in
    Deep Reinforcement Learning for Robotics: a Survey")), we denote the source domain
    as $D_{S}\equiv(\mathcal{S}_{S},\mathcal{A}_{S},\mathcal{P}_{S},\mathcal{R}_{S})$
    and target domain as $D_{T}\equiv(\mathcal{S}_{T},\mathcal{A}_{T},\mathcal{P}_{T},\mathcal{R}_{T})$,
    respectively. In reinforcement learning scenarios, the states $\mathcal{S}$ of
    the source and target domain can be quite different $(\mathcal{S}_{S}\neq\mathcal{S}_{T})$
    due to the perceptual-reality gap [[64](#bib.bib64)], while both domains share
    the action spaces and the transitions $\mathcal{P}$ $(\mathcal{A}_{S}\approx{\mathcal{A}_{T}},\mathcal{P}_{S}\approx{\mathcal{P}_{T}})$and
    their reward functions $\mathcal{R}$ have structural similarity $(\mathcal{R}_{S}\approx{\mathcal{R}_{T}})$.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们现在在强化学习设置中形式化领域适应场景 [[63](#bib.bib63)]。根据方程 ([1](#S2.E1 "In II-A Deep
    Reinforcement Learning ‣ II Background ‣ Sim-to-Real Transfer in Deep Reinforcement
    Learning for Robotics: a Survey"))中的MDP定义，我们将源领域表示为 $D_{S}\equiv(\mathcal{S}_{S},\mathcal{A}_{S},\mathcal{P}_{S},\mathcal{R}_{S})$，目标领域表示为
    $D_{T}\equiv(\mathcal{S}_{T},\mathcal{A}_{T},\mathcal{P}_{T},\mathcal{R}_{T})$。在强化学习场景中，由于感知现实差距 [[64](#bib.bib64)]，源领域和目标领域的状态
    $\mathcal{S}$ 可能会有很大不同 $(\mathcal{S}_{S}\neq\mathcal{S}_{T})$，而两个领域共享动作空间和转移 $\mathcal{P}$
    $(\mathcal{A}_{S}\approx{\mathcal{A}_{T}},\mathcal{P}_{S}\approx{\mathcal{P}_{T}})$，并且它们的奖励函数
    $\mathcal{R}$ 具有结构相似性 $(\mathcal{R}_{S}\approx{\mathcal{R}_{T}})$。'
- en: From the literature, we summarize three common methods for domain adaptation
    regardless of their tasks. They are discrepancy-based, adversarial-based, and
    reconstruction-based methods, which can be also used crossly. Discrepancy-based
    methods measure the feature distance between source and target domain by calculating
    pre-defined statistical metrics, in order to align their feature spaces [[65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67)]. Adversarial-based methods build a domain
    classifier to distinguish whether the features come from source domain or target
    domain. After being trained, the extractor could produce invariant feature from
    both source domain and target domain [[68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)].
    Reconstruction-based methods also aim to find the invariant or shared features
    between domains. However, they realize this goal by constructing one auxiliary
    reconstruction task and employ the shared feature to recover the original input [[71](#bib.bib71)].
    In this way, the shared feature should be invariant and independent with the domains.
    These three methods provide different angles to make the features from different
    domains unified, and can be utilized in both vision tasks and RL-based control
    tasks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从文献中，我们总结了三种常见的域适应方法，无论其任务如何。这些方法是基于**差异**、基于**对抗**和基于**重建**的方法，这些方法也可以交叉使用。基于差异的方法通过计算预定义的统计度量来衡量源域和目标域之间的特征距离，以对齐它们的特征空间[[65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67)]。基于对抗的方法建立一个域分类器来区分特征来自源域还是目标域。经过训练后，提取器可以从源域和目标域中产生不变的特征[[68](#bib.bib68),
    [69](#bib.bib69), [70](#bib.bib70)]。基于重建的方法也旨在找到域之间的不变或共享特征。然而，它们通过构建一个辅助重建任务，并使用共享特征来恢复原始输入[[71](#bib.bib71)]。这样，共享特征应在域之间保持不变且独立。这三种方法提供了不同的角度，使不同域的特征统一，并可用于视觉任务和基于RL的控制任务。
- en: III-E Learning with Disturbances
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 带扰动的学习
- en: Domain randomization and dynamics randomization methods focus on introducing
    perturbations in the simulation environments with the aim of making the agents
    less susceptible to the mismatches between simulation and reality [[30](#bib.bib30),
    [38](#bib.bib38), [40](#bib.bib40)]. The same conceptual idea has been extended
    in other works, where perturbances has been introduced to obtain more robust agents.
    For example, in [[72](#bib.bib72)], the authors consider noisy rewards. While
    not directly related to sim-to-real transfer, noisy rewards can better emulate
    real-world training of agents. Also, in some of our recent works [[8](#bib.bib8),
    [73](#bib.bib73)], we have considered environmental perturbations that affect
    differently different agents that are learning in parallel. This is an aspect
    that needs to be considered when multiple real agents are to be deployed or trained
    with a common policy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 域随机化和动态随机化方法专注于在模拟环境中引入扰动，旨在使智能体对模拟与现实之间的不匹配**不那么敏感**[[30](#bib.bib30), [38](#bib.bib38),
    [40](#bib.bib40)]。这种相同的概念在其他工作中得到了扩展，其中引入了扰动以获得更**强健**的智能体。例如，在[[72](#bib.bib72)]中，作者考虑了**噪声奖励**。虽然与模拟到现实的转移没有直接关系，噪声奖励可以更好地模拟智能体的现实世界训练。此外，在我们最近的一些工作中[[8](#bib.bib8),
    [73](#bib.bib73)]，我们考虑了环境扰动，这些扰动对并行学习的不同智能体有不同的影响。这是当多个真实智能体要部署或使用共同策略进行训练时需要考虑的一个方面。
- en: III-F Simulation Environments
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F 模拟环境
- en: A key aspect in sim-to-real transfer is the choice of simulation. Independently
    of the techniques utilized for efficiently transferring knowledge to real robots,
    the more realistic a simulation is the better results that can be expected. The
    most widely used simulators in the literature are Gazebo [[74](#bib.bib74)], Unity3D,
    and PyBullet [[75](#bib.bib75)] or MuJoCo [[17](#bib.bib17)]. Gazebo has the advantage
    of being widely integrated with the Robot Operating System (ROS) middleware, and
    therefore can be used together with part of the robotics stack that is present
    in real robots. PyBullet and MuJoCo, on the other hand, present wider integration
    with DL and RL libraries and gym environments. In general, Gazebo suits more complex
    scenarios while PyBullet and MuJoCo provide faster training.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟到实际转移的一个关键方面是模拟选择。无论用于有效转移知识到真实机器人的技术如何，模拟的现实性越高，预期的结果就越好。文献中最广泛使用的模拟器包括 Gazebo [[74](#bib.bib74)]、Unity3D
    和 PyBullet [[75](#bib.bib75)] 或 MuJoCo [[17](#bib.bib17)]。Gazebo 的优点是与机器人操作系统（ROS）中间件广泛集成，因此可以与真实机器人中的部分机器人堆栈一起使用。另一方面，PyBullet
    和 MuJoCo 与深度学习和强化学习库以及健身环境的集成更广泛。一般来说，Gazebo 更适合复杂场景，而 PyBullet 和 MuJoCo 提供更快的训练。
- en: In those cases where system identification for one-shot transfer is the objective,
    researchers have often built or customized specific simulations that meet problem-specific
    requirements and constraints [[32](#bib.bib32), [36](#bib.bib36), [41](#bib.bib41)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些以一次性转移为目标的系统识别案例中，研究人员通常会构建或定制满足问题特定需求和约束的模拟环境 [[32](#bib.bib32), [36](#bib.bib36),
    [41](#bib.bib41)]。
- en: IV Application Scenarios
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 应用场景
- en: Some of the most common applications for DRL in robotics are navigation and
    dexterous manipulation [[1](#bib.bib1), [76](#bib.bib76)]. Owing to the limited
    operational space in which most robotic arms operate, simulation environments
    for dexterous manipulation are relatively easier to generate than those for more
    complex robotic systems. For instance, the Open AI Gym [[77](#bib.bib77)], one
    of the most widely used frameworks for reinforcement learning, provides multiple
    environments for dexterous manipulation.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）在机器人领域的一些最常见应用是导航和灵巧操作 [[1](#bib.bib1), [76](#bib.bib76)]。由于大多数机器人臂的操作空间有限，灵巧操作的模拟环境相对容易生成，优于更复杂的机器人系统。例如，Open
    AI Gym [[77](#bib.bib77)]，这是最广泛使用的强化学习框架之一，提供了多个灵巧操作环境。
- en: IV-A Dexterous Robotic Manipulation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 灵巧机器人操作
- en: 'Robotic manipulation tasks that have been possible with DRL ranging from learning
    peg-in-hole tasks [[40](#bib.bib40)] to deformable object manipulation [[6](#bib.bib6)],
    and including more dexterous manipulation with multi-fingered hands [[5](#bib.bib5)],
    or learning force control policies [[78](#bib.bib78)]. The latter is particularly
    relevant for sim-to-real: applying excessive force to real objects might cause
    damage, while grasping can fail with a lack of force.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人操作任务已经能够通过深度强化学习（DRL）实现，从学习钉子插入孔任务 [[40](#bib.bib40)]到可变形物体的操作 [[6](#bib.bib6)]，以及更灵巧的多指操作 [[5](#bib.bib5)]，或者学习力控策略 [[78](#bib.bib78)]。后者对模拟到实际转移尤为相关：对真实物体施加过大的力量可能导致损坏，而力量不足则可能导致抓取失败。
- en: In [[6](#bib.bib6)], Matas et al. utilize domain randomization for learning
    manipulation of deformable objects. The authors identify as one of the main drawbacks
    of the simulation environment the inability to properly simulate the degree of
    deformability of the objects, with the real robot being unable to grasp stiffer
    objects. Moreover, a relevant conclusion from this work is that excessive domain
    randomization can be detrimental. Specifically, when the number of different colors
    that were being used for each texture was too large, the performance of the real
    robot was significantly worse.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[6](#bib.bib6)]中，Matas等人利用领域随机化来学习对可变形物体的操作。作者认为模拟环境的一个主要缺点是无法正确模拟物体的变形程度，导致真实机器人无法抓取更坚硬的物体。此外，研究的一个相关结论是过度的领域随机化可能有害。具体来说，当用于每种纹理的颜色数量过多时，真实机器人的表现显著变差。
- en: IV-B Robotic Navigation
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 机器人导航
- en: While learning navigation with reinforcement learning has been a topic of increasing
    research interest over the past years [[79](#bib.bib79), [80](#bib.bib80)], the
    literature focusing on sim-to-real transfer methods is sparse. The first difference
    with respect to more-established research in learning manipulation is perhaps
    the lack of standard simulation environments. Owing to the more specific environment
    and sensor suites that are required for different navigation tasks, custom simulators
    have often been used [[36](#bib.bib36), [37](#bib.bib37)], or simulation worlds
    have been created using Unity, Unreal Engine, or Gazebo [[39](#bib.bib39), [42](#bib.bib42)].
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用强化学习进行导航的研究兴趣在近年来持续增加[[79](#bib.bib79), [80](#bib.bib80)]，但专注于仿真到现实转移方法的文献仍然稀少。与学习操作中更成熟的研究相比，第一个不同之处可能在于缺乏标准的仿真环境。由于不同导航任务所需的环境和传感器设备更为具体，通常使用了定制的仿真器[[36](#bib.bib36),
    [37](#bib.bib37)]，或者使用 Unity、Unreal Engine 或 Gazebo[[39](#bib.bib39), [42](#bib.bib42)]
    创建了仿真世界。
- en: 'Sim-to-real transfer for DRL policies can be applied to complex navigation
    tasks: from six-legged robots [[37](#bib.bib37)] to depth-based mapless navigation [[39](#bib.bib39)],
    including robots for soccer competitions [[36](#bib.bib36)]. In order to achieve
    a successful transfer to the real world, different methods have been applied in
    the literature. Of particular interest due to their potential and novelty are
    the following methods: curriculum learning [[37](#bib.bib37)], incremental environment
    complexity [[39](#bib.bib39)], and continual learning and policy distillation
    for multiple tasks [[12](#bib.bib12)].'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: DRL 策略的仿真到现实转移可以应用于复杂的导航任务：从六足机器人[[37](#bib.bib37)] 到基于深度的无地图导航[[39](#bib.bib39)]，包括足球比赛的机器人[[36](#bib.bib36)]。为了实现成功的现实世界转移，文献中应用了不同的方法。由于其潜力和新颖性，特别感兴趣的方法包括：课程学习[[37](#bib.bib37)]、逐步环境复杂性[[39](#bib.bib39)]、以及针对多任务的持续学习和策略蒸馏[[12](#bib.bib12)]。
- en: IV-C Other Applications
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 其他应用
- en: Some other applications of DRL and sim-to-real transfer in robotics that have
    emerged over the past years are the control of a plasma jet [[32](#bib.bib32)],
    tactile sensing [[43](#bib.bib43)], or multi-agent manipulation [[44](#bib.bib44)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来出现的一些其他 DRL 和仿真到现实转移在机器人技术中的应用包括等离子体喷射[[32](#bib.bib32)]、触觉传感[[43](#bib.bib43)]
    或多智能体操作[[44](#bib.bib44)]。
- en: V Main Challenges and Future Directions
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 主要挑战和未来方向
- en: Albeit the progress presented in the papers we reviewed, sim-to-real remains
    challenging based on existing methods. For domain randomization, researchers tend
    to study empirically examining which randomization to add, but it is hard to explain
    formally how and why it works, which thereby brings the difficulty of designing
    efficiently simulations and randomization distributions. For domain adaptation,
    most existing algorithms focus on homogeneous deep domain adaptation, which assumes
    that the feature spaces between the source and target domains are the same. However,
    this assumption may not be true in many applications. Thus we expect more exploration
    to transfer knowledge without this limitation.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们审阅的论文中展示了进展，但基于现有方法，仿真到现实仍然具有挑战性。对于领域随机化，研究人员往往通过经验研究哪些随机化是必要的，但很难正式解释它如何以及为什么有效，这也带来了设计高效仿真和随机化分布的难度。对于领域适应，大多数现有算法集中于同质深度领域适应，假设源领域和目标领域之间的特征空间是相同的。然而，这一假设在许多应用中可能并不成立。因此，我们期望在没有这种限制的情况下进行更多的探索以迁移知识。
- en: 'Two of the most promising research directions are: (i) integration of different
    existing methods for more efficient transfer (e.g., domain randomization and domain
    adaptation); and (ii) incremental complexity learning, continual learning, and
    reward shaping for complex or multi-step tasks.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最有前景的研究方向是：(i) 将不同的现有方法整合以实现更高效的转移（例如，领域随机化和领域适应）；以及(ii) 逐步复杂性学习、持续学习和复杂或多步骤任务的奖励塑造。
- en: VI Conclusion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: Reinforcement learning algorithms often rely on simulated data to meet their
    need for vast amounts of labeled experiences. The mismatch between the simulation
    environments and real-world scenarios, however, requires further attention to
    be put to methods for sim-to-real transfer of the knowledge acquired in simulation.
    This is, to the best of our knowledge, the first survey that focuses on the different
    approaches being taken for sim-to-real transfer in DRL for robotics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法通常依赖于模拟数据来满足其对大量标记经验的需求。然而，模拟环境与现实世界场景之间的不匹配需要进一步关注用于将知识从模拟转移到现实的方法。据我们所知，这是首个聚焦于DRL在机器人领域中进行模拟到现实转移的不同方法的调查。
- en: Domain randomization has been identified as the most widely adopted method for
    increasing the realism of simulation and better prepare for the real world. However,
    we have discussed alternative research directions showing promising results. For
    instance, policy distillation is enabling multi-task learning and more efficient
    and smaller networks, while meta-learning methods allow for wider variability
    of tasks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 域随机化已被确定为提高仿真现实性和更好地为现实世界做准备的最广泛采用的方法。然而，我们讨论了显示出良好结果的替代研究方向。例如，策略蒸馏正在实现多任务学习以及更高效、更小的网络，而元学习方法则允许更广泛的任务变异。
- en: Multiple challenges remain in this field. While practical implementations show
    the efficiency of the different methods, wider theoretical and empirical studies
    are required to better understand the effect of these techniques in the learning
    process. Moreover, generalization of existing results with a more comprehensive
    analysis is also lacking in the literature.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域仍面临多个挑战。尽管实际应用展示了不同方法的效率，但需要更广泛的理论和实证研究以更好地理解这些技术在学习过程中的效果。此外，文献中也缺乏对现有结果的全面分析和泛化。
- en: Acknowledgements
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the Academy of Finland’s AutoSOS project with grant
    number 328755.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了芬兰学术院AutoSOS项目的资助，资助编号328755。
- en: References
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
    Bharath. A brief survey of deep reinforcement learning. arXiv:1708.05866, 2017.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage 和 Anil Anthony Bharath.
    深度强化学习的简要调查。arXiv:1708.05866，2017年。'
- en: '[2] Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement
    learning for multiagent systems: A review of challenges, solutions, and applications.
    IEEE transactions on cybernetics, 2020.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Thanh Thi Nguyen, Ngoc Duy Nguyen 和 Saeid Nahavandi. 多智能体系统的深度强化学习：挑战、解决方案和应用的回顾。《IEEE
    网络系统学报》，2020年。'
- en: '[3] Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. End-to-end
    safe reinforcement learning through barrier functions for safety-critical continuous
    control tasks. In AAAI Artificial Intelligence, volume 33, 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Richard Cheng, Gábor Orosz, Richard M Murray 和 Joel W Burdick. 通过障碍函数实现端到端安全强化学习，用于安全关键的连续控制任务。在AAAI人工智能，第33卷，2019年。'
- en: '[4] Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement
    learning. Journal of Machine Learning Research, 16(1), 2015.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Javier Garcıa 和 Fernando Fernández. 关于安全强化学习的全面调查。《机器学习研究期刊》，16(1)，2015年。'
- en: '[5] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John
    Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation
    with deep reinforcement learning and demonstrations. arXiv:1709.10087, 2017.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John
    Schulman, Emanuel Todorov 和 Sergey Levine. 通过深度强化学习和演示学习复杂的灵巧操作。arXiv:1709.10087，2017年。'
- en: '[6] Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforcement
    learning for deformable object manipulation. arXiv:1806.07851, 2018.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jan Matas, Stephen James 和 Andrew J Davison. 用于可变形物体操作的模拟到现实强化学习。arXiv:1806.07851，2018年。'
- en: '[7] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning
    in computer vision: A survey. IEEE Access, 6, 2018.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Naveed Akhtar 和 Ajmal Mian. 深度学习在计算机视觉中的对抗攻击威胁：一项调查。《IEEE Access》，6，2018年。'
- en: '[8] Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, and Tomi Westerlund. Towards
    closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning.
    In 5th ICRAE, 2020.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing 和 Tomi Westerlund. 迈向缩小协作多机器人深度强化学习中的仿真与现实之间的差距。在第五届ICRAE，2020年。'
- en: '[9] Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Peters. Bayesian
    domain randomization for sim-to-real transfer. arXiv:2003.02471, 2020.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Fabio Muratore, Christian Eilers, Michael Gienger 和 Jan Peters. 贝叶斯域随机化用于模拟到现实转移。arXiv:2003.02471，2020年。'
- en: '[10] Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, and Julie
    Shah. Blind spot detection for safe sim-to-real transfer. Journal of Artificial
    Intelligence Research, 67, 2020.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, 和 Julie Shah.
    用于安全 sim-to-real 转移的盲点检测。《人工智能研究期刊》，67，2020年。'
- en: '[11] Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta reinforcement
    learning for sim-to-real domain adaptation. arXiv:1909.12906, 2019.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, 和 Ville Kyrki. 用于 sim-to-real
    域适应的元强化学习。arXiv:1909.12906，2019年。'
- en: '[12] René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun, Natalia Díaz-Rodríguez,
    and David Filliat. Continual reinforcement learning deployed in real-life using
    policy distillation and sim2real transfer. arXiv:1906.04452, 2019.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun, Natalia Díaz-Rodríguez,
    和 David Filliat. 在实际应用中部署的持续强化学习，通过策略蒸馏和 sim2real 转移。arXiv:1906.04452，2019年。'
- en: '[13] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim:
    High-fidelity visual and physical simulation for autonomous vehicles. In Field
    and service robotics, 2018.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Shital Shah, Debadeepta Dey, Chris Lovett, 和 Ashish Kapoor. Airsim：用于自主车辆的高保真视觉和物理仿真。发表于《现场与服务机器人》，2018年。'
- en: '[14] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
    Koltun. Carla: An open urban driving simulator. arXiv:1711.03938, 2017.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, 和 Vladlen
    Koltun. Carla：一个开放的城市驾驶仿真器。arXiv:1711.03938，2017年。'
- en: '[15] Fadri Furrer, Michael Burri, Markus Achtelik, and Roland Siegwart. Rotors—a
    modular gazebo mav simulator framework. In Robot Operating System (ROS). 2016.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Fadri Furrer, Michael Burri, Markus Achtelik, 和 Roland Siegwart. Rotors——一个模块化的
    Gazebo MAV 仿真框架。发表于《机器人操作系统（ROS）》。2016年。'
- en: '[16] Cassandra McCord, Jorge Peña Queralta, Tuan Nguyen Gia, and Tomi Westerlund.
    Distributed progressive formation control for multi-agent systems: 2d and 3d deployment
    of uavs in ros/gazebo with rotors. In ECMR, 2019.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Cassandra McCord, Jorge Peña Queralta, Tuan Nguyen Gia, 和 Tomi Westerlund.
    多智能体系统的分布式渐进控制：在 ROS/Gazebo 中对 UAV 的 2D 和 3D 部署。发表于 ECMR，2019年。'
- en: '[17] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for
    model-based control. In IROS, 2012.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Emanuel Todorov, Tom Erez, 和 Yuval Tassa. Mujoco：一个用于基于模型控制的物理引擎。发表于 IROS，2012年。'
- en: '[18] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu
    Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings
    of the IEEE, 2020.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu
    Zhu, Hui Xiong, 和 Qing He. 关于迁移学习的全面调查。《IEEE 会议录》，2020年。'
- en: '[19] Jorge Peña Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan
    Sarker, Tuan Nguyen Gia, Hannu Tenhunen, Moncef Gabbouj, Jenni Raitoharju, and
    Tomi Westerlund. Collaborative multi-robot systems for search and rescue: Coordination
    and perception. arXiv preprint arXiv:2008.12610, 2020.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Jorge Peña Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan
    Sarker, Tuan Nguyen Gia, Hannu Tenhunen, Moncef Gabbouj, Jenni Raitoharju, 和 Tomi
    Westerlund. 用于搜索和救援的协作多机器人系统：协调与感知。arXiv 预印本 arXiv:2008.12610，2020年。'
- en: '[20] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław
    D ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse,
    et al. Dota 2 with large scale deep reinforcement learning. arXiv:1912.06680,
    2019.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław
    D ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse,
    等等. Dota 2 与大规模深度强化学习。arXiv:1912.06680，2019年。'
- en: '[21] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,
    James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia
    Hadsell. Policy distillation. arXiv:1511.06295, 2015.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guillaume Desjardins,
    James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, 和 Raia Hadsell.
    策略蒸馏。arXiv:1511.06295，2015年。'
- en: '[22] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo,
    Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to
    reinforcement learn. arXiv:1611.05763, 2016.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo,
    Remi Munos, Charles Blundell, Dharshan Kumaran, 和 Matt Botvinick. 学习强化学习。arXiv:1611.05763，2016年。'
- en: '[23] Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation,
    17(2), 2005.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Jun Morimoto 和 Kenji Doya. 鲁棒强化学习。《神经计算》，17(2)，2005年。'
- en: '[24] Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement
    learning and applications in continuous control. arXiv:1901.09184, 2019.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Chen Tessler, Yonathan Efroni, 和 Shie Mannor. 动作鲁棒强化学习及其在连续控制中的应用。arXiv:1901.09184，2019年。'
- en: '[25] Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas
    Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller.
    Robust reinforcement learning for continuous control with model misspecification.
    arXiv:1906.07516, 2019.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas
    Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester 和 Martin Riedmiller。具有模型误差的连续控制的稳健强化学习。arXiv:1906.07516，2019。'
- en: '[26] Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network.
    In Advances in neural information processing systems, 1989.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Dean A Pomerleau. Alvinn：一个在神经网络中的自主陆地车辆。见《神经信息处理系统进展》，1989。'
- en: '[27] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation
    learning and structured prediction to no-regret online learning. In AISTATS, 2011.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Stéphane Ross, Geoffrey Gordon 和 Drew Bagnell。将模仿学习和结构化预测简化为无悔在线学习。见AISTATS，2011。'
- en: '[28] Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforcement
    learning. In Icml, volume 1, 2000.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Andrew Y Ng, Stuart J Russell 等。逆向强化学习算法。见Icml，第1卷，2000。'
- en: '[29] Mengyuan Yan, Iuri Frosio, Stephen Tyree, and Jan Kautz. Sim-to-real transfer
    of accurate grasping with eye-in-hand observations and continuous control. arXiv:1712.03303,
    2017.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Mengyuan Yan, Iuri Frosio, Stephen Tyree 和 Jan Kautz。使用眼内观察和连续控制的准确抓取的模拟到现实转移。arXiv:1712.03303，2017。'
- en: '[30] Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac,
    Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, et al. Deepracer:
    Educational autonomous racing platform for experimentation with sim2real reinforcement
    learning. arXiv:1911.01562, 2019.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac,
    Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend等。Deepracer：用于模拟到现实强化学习实验的教育性自主赛车平台。arXiv:1911.01562，2019。'
- en: '[31] Manuel Kaspar, Juan David Munoz Osorio, and Jürgen Bock. Sim2real transfer
    for reinforcement learning without dynamics randomization. arXiv:2002.11635, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Manuel Kaspar, Juan David Munoz Osorio 和 Jürgen Bock。无需动态随机化的强化学习模拟到现实转移。arXiv:2002.11635，2020。'
- en: '[32] Matthew Witman, Dogan Gidon, David B Graves, Berend Smit, and Ali Mesbah.
    Sim-to-real transfer reinforcement learning for control of thermal effects of
    an atmospheric pressure plasma jet. Plasma Sources Science and Technology, 28(9),
    2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Matthew Witman, Dogan Gidon, David B Graves, Berend Smit 和 Ali Mesbah。用于控制大气压力等离子体喷射的热效应的模拟到现实转移强化学习。Plasma
    Sources Science and Technology, 28(9)，2019。'
- en: '[33] Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas
    Abdolmaleki, Tom Erez, Yuval Tassa, and Francesco Nori. Modelling generalized
    forces with reinforcement learning for sim-to-real transfer. arXiv:1910.09471,
    2019.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas
    Abdolmaleki, Tom Erez, Yuval Tassa 和 Francesco Nori。使用强化学习对模拟到现实转移进行建模的通用力。arXiv:1910.09471，2019。'
- en: '[34] Michel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, and Juan
    Nieto. Flexible robotic grasping with sim-to-real transfer based reinforcement
    learning. ArXiv e-prints, 2018.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Michel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart 和 Juan Nieto。基于模拟到现实转移的灵活机器人抓取。ArXiv
    e-prints，2018。'
- en: '[35] J van Baar, R Corcodel, A Sullivan, D Jha, D Romeres, and D Nikovski.
    Simulation to real transfer learning with robustified policies for robot tasks.
    2018.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J van Baar, R Corcodel, A Sullivan, D Jha, D Romeres 和 D Nikovski。带有稳健策略的模拟到现实转移学习用于机器人任务。2018。'
- en: '[36] Hansenclever F Bassani, Renie A Delgado, Jose Nilton de O Lima Junior,
    Heitor R Medeiros, Pedro HM Braga, and Alain Tapp. Learning to play soccer by
    reinforcement and applying sim-to-real to compete in the real world. arXiv:2003.11102,
    2020.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Hansenclever F Bassani, Renie A Delgado, Jose Nilton de O Lima Junior,
    Heitor R Medeiros, Pedro HM Braga 和 Alain Tapp。通过强化学习学习踢足球，并将模拟到现实应用于真实世界中的竞争。arXiv:2003.11102，2020。'
- en: '[37] Bangyu Qin, Yue Gao, and Yi Bai. Sim-to-real: Six-legged robot control
    with deep reinforcement learning and curriculum learning. In ICRAE, 2019.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Bangyu Qin, Yue Gao 和 Yi Bai。模拟到现实：使用深度强化学习和课程学习的六足机器人控制。见ICRAE，2019。'
- en: '[38] Juliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas
    Paula, Wagston Staehler, and David Murphy. Sim-to-real in reinforcement learning
    for everyone. In LARS-SBR-WRE, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Juliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas
    Paula, Wagston Staehler 和 David Murphy。针对所有人的模拟到现实强化学习。见LARS-SBR-WRE，2019。'
- en: '[39] Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, and Julien Marzat.
    Sim-to-real transfer with incremental environment complexity for reinforcement
    learning of depth-based robot navigation. arXiv:2004.14684, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong 和 Julien Marzat。用于基于深度的机器人导航的增量环境复杂度的模拟到现实转移。arXiv:2004.14684，2020。'
- en: '[40] Manuel Kaspar and Jürgen Bock. Reinforcement learning with cartesian commands
    and sim to real transfer for peg in hole tasks.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Manuel Kaspar 和 Jürgen Bock. 使用笛卡尔命令的强化学习和用于插销任务的仿真到现实转移。'
- en: '[41] Andrew Hundt, Benjamin Killeen, Heeyeon Kwon, Chris Paxton, and Gregory D
    Hager. ” good robot!”: Efficient reinforcement learning for multi-step visual
    tasks via reward shaping. arXiv:1909.11730, 2019.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Andrew Hundt, Benjamin Killeen, Heeyeon Kwon, Chris Paxton 和 Gregory D
    Hager. “好机器人！”：通过奖励塑形进行多步骤视觉任务的高效强化学习。arXiv:1909.11730，2019。'
- en: '[42] Ole-Magnus Pedersen. Sim-to-real transfer of robotic gripper pose estimation-using
    deep reinforcement learning, generative adversarial networks, and visual servoing.
    Master’s thesis, NTNU, 2019.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Ole-Magnus Pedersen. 机器人抓手位姿估计的仿真到现实转移——使用深度强化学习、生成对抗网络和视觉伺服。硕士论文，NTNU，2019。'
- en: '[43] Zihan Ding, Nathan F Lepora, and Edward Johns. Sim-to-real transfer for
    optical tactile sensing. arXiv:2004.00136, 2020.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Zihan Ding, Nathan F Lepora 和 Edward Johns. 光学触觉传感的仿真到现实转移。arXiv:2004.00136，2020。'
- en: '[44] Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent
    manipulation via locomotion using hierarchical sim2real. arXiv:1908.05224, 2019.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu 和 Vikash Kumar. 通过层次化仿真到现实的运动学进行多智能体操控。arXiv:1908.05224，2019。'
- en: '[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
    Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford 和 Oleg Klimov.
    近端策略优化算法。arXiv:1707.06347，2017。'
- en: '[46] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
    Moritz. Trust region policy optimization. In ICML, 2015.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan 和 Philipp
    Moritz. 信任域策略优化。在 ICML，2015。'
- en: '[47] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos,
    Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation.
    arXiv:1806.06920, 2018.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos,
    Nicolas Heess 和 Martin Riedmiller. 最大后验策略优化。arXiv:1806.06920，2018。'
- en: '[48] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In ICML, 2016.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver 和 Koray Kavukcuoglu. 深度强化学习的异步方法。在 ICML，2016。'
- en: '[49] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic:
    Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
    arXiv:1801.01290, 2018.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel 和 Sergey Levine. Soft actor-critic：具有随机演员的离线最大熵深度强化学习。arXiv:1801.01290，2018。'
- en: '[50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with
    deep reinforcement learning. arXiv:1509.02971, 2015.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess,
    Tom Erez, Yuval Tassa, David Silver 和 Daan Wierstra. 使用深度强化学习的连续控制。arXiv:1509.02971，2015。'
- en: '[51] Kristinn Kristinsson and Guy Albert Dumont. System identification and
    control using genetic algorithms. IEEE Transactions on Systems, Man, and Cybernetics,
    22(5), 1992.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Kristinn Kristinsson 和 Guy Albert Dumont. 使用遗传算法的系统辨识与控制。IEEE Transactions
    on Systems, Man, and Cybernetics, 22(5)，1992。'
- en: '[52] Joshua P Tobin. Real-World Robotic Perception and Control Using Synthetic
    Data. PhD thesis, UC Berkeley, 2019.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Joshua P Tobin. 使用合成数据的现实世界机器人感知与控制。博士论文，UC Berkeley，2019。'
- en: '[53] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba,
    and Pieter Abbeel. Domain randomization for transferring deep neural networks
    from simulation to the real world. In IROS, 2017.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba 和
    Pieter Abbeel. 通过领域随机化将深度神经网络从仿真转移到现实世界。在 IROS，2017。'
- en: '[54] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani,
    Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training
    deep networks with synthetic data: Bridging the reality gap by domain randomization.
    In CVPR Workshops, 2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani,
    Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon 和 Stan Birchfield. 使用合成数据训练深度网络：通过领域随机化弥合现实差距。在
    CVPR Workshops，2018。'
- en: '[55] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker,
    and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection
    from rgb images. In ECCV, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker
    和 Rudolph Triebel. 从 RGB 图像中进行 6D 物体检测的隐式 3D 方向学习。在 ECCV，2018。'
- en: '[56] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli,
    Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real
    generalization without accessing target domain data. In ICCV, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli,
    Kurt Keutzer 和 Boqing Gong。领域随机化和金字塔一致性：无须访问目标领域数据的模拟到现实泛化。在 ICCV，2019年。'
- en: '[57] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz,
    Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
    Ray, et al. Learning dexterous in-hand manipulation. The International Journal
    of Robotics Research, 39(1), 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz,
    Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex
    Ray 等人。学习灵巧的手部操控。《国际机器人研究杂志》，39(1)，2020年。'
- en: '[58] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov,
    Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis.
    Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical
    adaptation networks. In CVPR, 2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov,
    Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell 和 Konstantinos Bousmalis。通过模拟到模拟的方式实现从模拟到现实的数据高效机器人抓取：通过随机到规范的适应网络。在
    CVPR，2019年。'
- en: '[59] Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing,
    312, 2018.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Mei Wang 和 Weihong Deng。深度视觉领域适应：综述。《神经计算》，312，2018年。'
- en: '[60] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
    Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial
    domain adaptation. In ICML, 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate
    Saenko, Alexei Efros 和 Trevor Darrell。Cycada：循环一致对抗领域适应。在 ICML，2018年。'
- en: '[61] Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew
    Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige,
    et al. Using simulation and domain adaptation to improve efficiency of deep robotic
    grasping. In ICRA, 2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew
    Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige
    等人。利用模拟和领域适应提高深度机器人抓取的效率。在 ICRA，2018年。'
- en: '[62] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine.
    Learning invariant feature spaces to transfer skills with reinforcement learning.
    arXiv:1703.02949, 2017.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel 和 Sergey Levine。学习不变特征空间以通过强化学习转移技能。arXiv:1703.02949，2017年。'
- en: '[63] Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess,
    Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner.
    Darla: Improving zero-shot transfer in reinforcement learning. arXiv:1707.08475,
    2017.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess,
    Alexander Pritzel, Matthew Botvinick, Charles Blundell 和 Alexander Lerchner。Darla：改善强化学习中的零样本迁移。arXiv:1707.08475，2017年。'
- en: '[64] Andrei A Rusu, Matej Večerík, Thomas Rothörl, Nicolas Heess, Razvan Pascanu,
    and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets.
    In Conference on Robot Learning, 2017.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Andrei A Rusu, Matej Večerík, Thomas Rothörl, Nicolas Heess, Razvan Pascanu
    和 Raia Hadsell。通过渐进网络从像素到现实的机器人学习。在机器人学习大会，2017年。'
- en: '[65] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell.
    Deep domain confusion: Maximizing for domain invariance. arXiv:1412.3474, 2014.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko 和 Trevor Darrell。深度领域混淆：最大化领域不变性。arXiv:1412.3474，2014年。'
- en: '[66] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable
    features with deep adaptation networks. In ICML, 2015.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Mingsheng Long, Yue Cao, Jianmin Wang 和 Michael Jordan。使用深度适应网络学习可转移特征。在
    ICML，2015年。'
- en: '[67] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy
    domain adaptation. arXiv:1511.05547, 2015.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Baochen Sun, Jiashi Feng 和 Kate Saenko。令人沮丧的简单领域适应的回归。arXiv:1511.05547，2015年。'
- en: '[68] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle,
    François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial
    training of neural networks. The Journal of Machine Learning Research, 17(1),
    2016.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle,
    François Laviolette, Mario Marchand 和 Victor Lempitsky。神经网络的领域对抗训练。《机器学习研究杂志》，17(1)，2016年。'
- en: '[69] Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous
    deep transfer across domains and tasks. In ICCV, 2015.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Eric Tzeng, Judy Hoffman, Trevor Darrell 和 Kate Saenko。跨领域和任务的深度转移。在 ICCV，2015年。'
- en: '[70] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan,
    and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative
    adversarial networks. In CVPR, 2017.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan 和
    Dilip Krishnan。使用生成对抗网络进行无监督像素级领域适应。在 CVPR，2017年。'
- en: '[71] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan,
    and Dumitru Erhan. Domain separation networks. In Advances in neural information
    processing systems, 2016.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 康斯坦丁诺斯·布斯马利斯、乔治·特里戈吉斯、内森·席尔伯曼、迪利普·克里什南和杜米特鲁·厄尔汉。领域分离网络。发表于神经信息处理系统进展,
    2016。'
- en: '[72] Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed
    rewards. In AAAI, 2020.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 王晶康、刘洋和李博。带有扰动奖励的强化学习。发表于AAAI, 2020。'
- en: '[73] Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, and Tomi Westerlund.
    Ubiquitous distributed deep reinforcement learning at the edge: Analyzing byzantine
    agents in discrete action spaces. In The 11th International Conference on Emerging
    Ubiquitous Systems and Pervasive Networks (EUSPN 2020), 2020.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] 赵文帅、豪尔赫·佩尼亚·奎拉尔塔、李青青和托米·韦斯特伦。在边缘的普遍分布式深度强化学习：分析离散动作空间中的拜占庭代理。发表于第11届国际新兴普及系统与网络会议
    (EUSPN 2020), 2020。'
- en: '[74] Nathan Koenig and Andrew Howard. Design and use paradigms for gazebo,
    an open-source multi-robot simulator. In IROS, volume 3, 2004.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] 内森·科宁和安德鲁·霍华德。Gazebo的设计和使用范式，一个开源多机器人模拟器。发表于IROS, 第3卷, 2004。'
- en: '[75] Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation
    for games, robotics and machine learning. 2016.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] 埃尔温·考曼斯和尹飞·白。Pybullet，一个用于游戏、机器人和机器学习的物理仿真Python模块。2016。'
- en: '[76] J. Kober et al. Reinforcement learning in robotics: A survey. The International
    Journal of Robotics Research, 32(11), 2013.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] J. 科伯等。机器人中的强化学习：一项调查。国际机器人研究杂志, 32(11), 2013。'
- en: '[77] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John
    Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv:1606.01540, 2016.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] 格雷格·布洛克曼、维基·张、路德维希·佩特松、乔纳斯·施奈德、约翰·舒尔曼、杰·唐和沃伊切赫·扎伦巴。OpenAI Gym。arXiv:1606.01540,
    2016。'
- en: '[78] M. Kalakrishnan et al. Learning force control policies for compliant manipulation.
    In IROS, 2011.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. 卡拉克里什南等。学习柔性操作的力控制策略。发表于IROS, 2011。'
- en: '[79] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei,
    and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement
    learning. In ICRA, 2017.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] 朱玉克、鲁兹贝赫·莫塔吉、埃里克·科尔夫、约瑟夫·J·林、阿比纳夫·古普塔、李飞飞和阿里·法赫迪。使用深度强化学习在室内场景中进行目标驱动的视觉导航。发表于ICRA,
    2017。'
- en: '[80] Fanyu Zeng, Chen Wang, and Shuzhi Sam Ge. A survey on visual navigation
    for artificial agents with deep reinforcement learning. IEEE Access, 8, 2020.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] 曾凡宇、王晨和舒志·萨姆·戈。关于深度强化学习的人工智能视觉导航调查。IEEE Access, 8, 2020。'
