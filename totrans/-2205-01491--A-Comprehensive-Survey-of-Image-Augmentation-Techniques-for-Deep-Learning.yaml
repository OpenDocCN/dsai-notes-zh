- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:46:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2205.01491] A Comprehensive Survey of Image Augmentation Techniques for Deep
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01491](https://ar5iv.labs.arxiv.org/html/2205.01491)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey of Image Augmentation Techniques for Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mingle Xu Department of Electronics Engineering, Jeonbuk National University,
    Jeonbuk 54896, South Korea Sook Yoon Department of Computer Engineering, Mokpo
    National University, Jeonnam 58554, South Korea Alvaro Fuentes Core Research Institute
    of Intelligent Robots, Jeonbuk National University, Jeonbuk 54896, South Korea
    Dong Sun Park Core Research Institute of Intelligent Robots, Jeonbuk National
    University, Jeonbuk 54896, South Korea
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although deep learning has achieved satisfactory performance in computer vision,
    a large volume of images is required. However, collecting images is often expensive
    and challenging. Many image augmentation algorithms have been proposed to alleviate
    this issue. Understanding existing algorithms is, therefore, essential for finding
    suitable and developing novel methods for a given task. In this study, we perform
    a comprehensive survey of image augmentation for deep learning using a novel informative
    taxonomy. To examine the basic objective of image augmentation, we introduce challenges
    in computer vision tasks and vicinity distribution. The algorithms are then classified
    among three categories: model-free, model-based, and optimizing policy-based.
    The model-free category employs the methods from image processing, whereas the
    model-based approach leverages image generation models to synthesize images. In
    contrast, the optimizing policy-based approach aims to find an optimal combination
    of operations. Based on this analysis, we believe that our survey enhances the
    understanding necessary for choosing suitable methods and designing novel algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'image augmentation , deep learning , image variation , vicinity distribution
    , data augmentation , computer vision.^†^†journal: Journal of LaTeX Templates'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the recent years, deep learning has achieved significant improvements in
    computer vision based on three key elements, efficient computing devices, powerful
    algorithms, and large volumes of image. A main work over the last decade was designing
    a powerful model with numerous trainable parameters¹¹1https://spectrum.ieee.org/andrew-ng-data-centric-ai.
    The training of such a model requires a large volume of image to achieve competitive
    performance. However, collecting image is frequently an expensive and challenging
    process. Obtaining satisfactory performance with a limited dataset is particularly
    challenging in practical applications, such as medical [[1](#bib.bib1)] and agricultural
    images [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: To address this issue, image augmentation has been confirmed to be an effective
    and efficient strategy [[3](#bib.bib3), [4](#bib.bib4)]. As listed in Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ A Comprehensive Survey of Image Augmentation Techniques
    for Deep Learning"), many image augmentation methods have been utilized for image
    classification and object detection. Understanding existing image augmentation
    methods is, therefore, crucial in deploying suitable algorithms. Although similar
    surveys have been conducted previously [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)],
    our study is characterized by several essential differences. First, we do not
    confine ourselves to a specific type of image, such as facial images [[8](#bib.bib8)].
    Likewise, we consider many types of image augmentation algorithms, including generative
    adversarial networks [[9](#bib.bib9)] and image mixing [[10](#bib.bib10)]. Third,
    we do not focus on a specific application, such as object detection [[5](#bib.bib5)].
    Conversely, we consider image classification and object detection as two primary
    applications, along with other image and video applications such as segmentation
    and tracking. Finally, unlike two related studies [[6](#bib.bib6), [7](#bib.bib7)],
    our survey encompasses more recent yet effective image augmentation algorithms
    such as instance level multiple image mixing, as well as comprehensive analysis
    of model-based methods. Consequently, this paper encompasses a wider range of
    algorithms that yield a novel informative taxonomy.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we first explain why different image augmentation algorithms have
    been designed and leveraged across diverse applications. More specifically, challenges
    in computer vision and vicinity distribution are introduced to illustrate the
    necessity of image augmentation. By augmenting image data, the aforementioned
    challenges can be mitigated, and the vicinity distribution space can be dilated,
    thereby improving trained model’s generalizability. Based on this analysis, we
    argue that novel image augmentation methods are promising when new challenges
    are recognized. Simultaneously, once a challenge is observed in an application,
    it can be mitigated using an appropriate augmentation method.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our study makes the following contributions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We examine *challenges* and *vicinity distribution* to demonstrate the necessity
    of image augmentation for deep learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present a comprehensive survey on image augmentation with a novel *informative
    taxonomy* that encompasses a wider range of algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We discuss the current situation and future direction for image augmentation,
    along with three relevant topics: understanding image augmentation, new strategy
    to leverage image augmentation, and feature augmentation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The reminder of this paper is organized as follows. The second section introduces
    the research taxonomy. We then present two basic inspiration of image augmentation
    in the third section: the challenges of computer vision tasks and the vicinity
    distribution. Model-free image augmentation is covered in the fourth section,
    whereas the model-based methods are discussed in the fifth section. The process
    of determining an optimal image augmentation is introduced in the six section,
    followed by a discussion section. Concluding remarks are presented in the final
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Image augmentation method |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AlexNet [[11](#bib.bib11)] | Translate, Flip, Intensity Changing |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet [[12](#bib.bib12)] | Crop, Flip |'
  prefs: []
  type: TYPE_TB
- en: '| DenseNet [[13](#bib.bib13)] | Flip, Crop, Translate |'
  prefs: []
  type: TYPE_TB
- en: '| MobileNet [[14](#bib.bib14)] | Crop, Elastic distortion |'
  prefs: []
  type: TYPE_TB
- en: '| NasNet [[15](#bib.bib15)] | Cutout, Crop, Flip |'
  prefs: []
  type: TYPE_TB
- en: '| ResNeSt [[16](#bib.bib16)] | AutoAugment, Mixup, Crop |'
  prefs: []
  type: TYPE_TB
- en: '| DeiT [[17](#bib.bib17)] | AutoAugmentat, RandAugment, Random Erasing, Mixup,
    CutMix |'
  prefs: []
  type: TYPE_TB
- en: '| Swin Transformer [[18](#bib.bib18)] | RandAugment, Mixup, CutMix, Random
    Erasing |'
  prefs: []
  type: TYPE_TB
- en: '| Faster R-CNN [[19](#bib.bib19)] | Flip |'
  prefs: []
  type: TYPE_TB
- en: '| YOLO [[20](#bib.bib20)] | Scale, Translate, Color space |'
  prefs: []
  type: TYPE_TB
- en: '| SSD [[21](#bib.bib21)] | Crop, Resize, Flip, Color Space, Distortion |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv4 [[22](#bib.bib22)] | Mosaic, Distortion, Scale, Color space, Crop,
    Flip, Rotate, Random erase, Cutout, Hide-and-Seek, GridMask, Mixup, CutMix, StyleGAN
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Image augmentation algorithms used studies pertaining to image classification
    (up) and object detection (bottom).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in Table [2](#S2.T2 "Table 2 ‣ 2 Taxonomy ‣ A Comprehensive Survey
    of Image Augmentation Techniques for Deep Learning"), we classify the image augmentation
    algorithms among three main categories. A model-free approach does not utilize
    a pre-trained model to perform image augmentation, and may use single or multiple
    images. Conversely, model-based algorithms require the image augmentation algorithms
    to generate images using trained models. The augmentation process may unconditional,
    label-conditional, or image-conditional. Finally, Optimizing policy-based algorithms
    determine the optimal operations with suitable parameters from a large parameter
    space. These algorithms can further be sub-categorized into reinforcement learning-based
    and adversarial learning-based method. The former leverages a massive search space
    consisting of diverse operations and their magnitudes, along with an agent to
    find the optimal policy within the search space. In contrast, adversarial learning-based
    methods locate algorithms with the corresponding magnitude to allow the task model
    to have a sufficiently large loss.
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Relevant methods |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Model-free | Single-image | Geometrical transformation | translation, rotation,
    flip, scale, elastic distortion. |'
  prefs: []
  type: TYPE_TB
- en: '| Color image processing | jittering. |'
  prefs: []
  type: TYPE_TB
- en: '| Intensity transformation | blurring and adding noise, Hide-and-Seek [[23](#bib.bib23)],
    Cutout [[24](#bib.bib24)], Random Erasing [[25](#bib.bib25)], GridMask [[26](#bib.bib26)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multiple-image | Non-instance-level | SamplePairing [[27](#bib.bib27)], Mixup
    [[28](#bib.bib28)], BC Learning [[29](#bib.bib29)], CutMix [[30](#bib.bib30)],
    Mosaic [[22](#bib.bib22)], AugMix [[31](#bib.bib31)], PuzzleMix [[32](#bib.bib32)],
    Co-Mixup [[33](#bib.bib33)], SuperMix [[34](#bib.bib34)], GridMix [[35](#bib.bib35)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Instance-level | CutPas [[36](#bib.bib36)], Scale and Blend [[37](#bib.bib37)],
    Context DA [[38](#bib.bib38)], Simple CutPas [[39](#bib.bib39)], Continuous CutPas
    [[40](#bib.bib40)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Model-based | Unconditional | DCGAN [[41](#bib.bib41)], [[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)] |'
  prefs: []
  type: TYPE_TB
- en: '| Label-conditional | BDA [[45](#bib.bib45)], ImbCGAN [[46](#bib.bib46)], BAGAN
    [[47](#bib.bib47)], DAGAN [[48](#bib.bib48)], MFC-GAN [[49](#bib.bib49)], IDA-GAN
    [[50](#bib.bib50)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Image-conditional | Label-preserving | S+U Learning [[51](#bib.bib51)], AugGAN
    [[52](#bib.bib52)], Plant-CGAN [[53](#bib.bib53)], StyleAug [[54](#bib.bib54)],
    Shape bias [[55](#bib.bib55)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Label-changing | EmoGAN [[56](#bib.bib56)], $\delta$-encoder [[57](#bib.bib57)],
    Debiased NN [[58](#bib.bib58)], StyleMix [[59](#bib.bib59)], GAN-MBD [[60](#bib.bib60)],
    SCIT [[2](#bib.bib2)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizing policy-based | Reinforcement learning-based | AutoAugment [[61](#bib.bib61)],
    Fast AA [[62](#bib.bib62)], PBA [[63](#bib.bib63)], Faster AA [[64](#bib.bib64)],
    RandAugment [[65](#bib.bib65)], MADAO [[66](#bib.bib66)], LDA [[67](#bib.bib67)],
    LSSP [[68](#bib.bib68)]. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Adversarial learning-based | ADA [[69](#bib.bib69)], CDST-DA [[70](#bib.bib70)],
    AdaTransform [[71](#bib.bib71)], Adversarial AA [[72](#bib.bib72)], IF-DA [[73](#bib.bib73)],
    SPA [[74](#bib.bib74)]. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Taxonomy with relevant methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Motivation to perform image augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Challenges | Descriptions | Strategies and related studies |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Images variations | The following basic variations exist in many datasets
    and applications, including illumination, deformation, occlusion, background,
    viewpoint, and multiscale, as shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Challenges
    ‣ 3 Motivation to perform image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"). | Geometrical transformation and
    color image processing improve the majority of the variations. Occlusion: Hide-and-Seek
    [[23](#bib.bib23)], Cutout [[24](#bib.bib24)], Random Erasing [[25](#bib.bib25)],
    GridMask [[26](#bib.bib26)]. Background or context: CutMix [[30](#bib.bib30)],
    Mosaic [[22](#bib.bib22)], CutPas [[36](#bib.bib36)]. Multiscale: Scale and Blend
    [[37](#bib.bib37)], Simple CutPas [[39](#bib.bib39)]. |'
  prefs: []
  type: TYPE_TB
- en: '| Class imbalance and few images | Number of images vary between classes or
    some classes have only few images. | Reusing instance from minority class is one
    strategy by instance-level operation, Simple Copy-Paste [[39](#bib.bib39)]. Most
    studies attempt to generate images for the minority class: ImbCGAN [[46](#bib.bib46)],
    DAGAN [[48](#bib.bib48)], MFC-GAN [[49](#bib.bib49)], EmoGAN [[56](#bib.bib56)],
    $\delta$-encoder [[57](#bib.bib57)], GAN-MBD [[60](#bib.bib60)], SCIT [[2](#bib.bib2)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Domain shift | Training and testing datasets represent different domains,
    commonly referring to styles. | Changing styles for existing images is a main
    strategy, including S+U Learning [[51](#bib.bib51)], StyleAug [[54](#bib.bib54)],
    Shape bias [[55](#bib.bib55)], Debiased NN [[58](#bib.bib58)], StyleMix [[59](#bib.bib59)].
    |'
  prefs: []
  type: TYPE_TB
- en: '| Data remembering | Larger models with many learnable parameters tend to remember
    specific data points, which may result in overfitting. | The mechanism is increasing
    dataset size within or between vicinity distributions. Within version assumes
    label-preserving while between version changes labels, such as Mixup [[28](#bib.bib28)],
    AugMix [[31](#bib.bib31)], Co-Mixup [[33](#bib.bib33)]. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Challenges in computer vision tasks from the perspectives of datasets
    and deep learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/053ef7edc8840d6270b9df60bf38395d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example of image variations from Class CS231n.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [3](#S3.T3 "Table 3 ‣ 3.1 Challenges ‣ 3 Motivation to perform image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")
    describes the four types of challenges faced in computer vision tasks. The first
    challenge is *image variation*, resulting from effects such as illumination and
    deformation. Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Challenges ‣ 3 Motivation to perform
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning") illustrates image variations²²2http://cs231n.stanford.edu/. *Class
    imbalance* is another challenge, wherein different objects are observed with different
    frequencies. In medical imaging, abnormal cases often occur with a low probability,
    which is further exacerbated by privacy. When trained with an imbalanced dataset,
    a model assigns a higher probability to the normal case. Besides, class imbalance
    becomes few images from multiple classes to one class. Furthermore, *domain shift*
    represents a challenge where the training and testing datasets exhibit different
    distributions. This is exemplified by the night and day domains in the context
    of automatic driving. Because it is more convenient to collect images during the
    daytime, we may desire to train our model with a daytime dataset but evaluate
    it at the nighttime.
  prefs: []
  type: TYPE_NORMAL
- en: A new challenge introduced by deep learning is *data remembering*. In general,
    a larger set of learnable parameters requires more data for training, which is
    referred to as structural risk [[75](#bib.bib75)]. With an increase in parameters,
    a deep learning model may remember specific data points with an insufficient number
    of training images, which introduces a generalizability problem in the form of
    overfitting [[76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, image augmentation methods can mitigate these challenges and improve
    model generalizability by increasing the number and variance of images in the
    training dataset. To utilize an image augmentation algorithm efficiently, it is
    crucial to understand the challenges of application and apply suitable methods.
    This study was conducted to provide a survey that enhances the understanding of
    a wide range of image augmentation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Vicinity distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a supervised learning paradigm, we expect to find a function $f\in\mathcal{F}$
    that reflects the relationship between an input $x$ and target $y$ in a joint
    distribution $P(x,y)$. To learn $f$, a loss $l$ is defined to reduce the discrepancy
    between the prediction $f(x)$ and actual target $y$ for all examples in $P(x,y)$.
    We can then optimize $f$ by minimizing $l$ over $P(x,y)$, which is known as the
    expected risk [[75](#bib.bib75)] and can be formulated as $R(f)=\int{l(f(x),y)dP(x,y)}$.
    However, $P(x,y)$ is unknown in most applications [[77](#bib.bib77)]. Alternatively,
    we may use the empirical distribution $P_{e}(x,y)$ to approximate $P(x,y)$. In
    this case, the observed dataset $\mathcal{D}={(x_{i},y_{i})}_{i=1}^{n}$ is considered
    to be the empirical distribution, where $(x_{i},y_{i})$ is in $P_{e}(x,y)$ for
    a given $i$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{e}(x,y)=\frac{1}{n}\sum_{i=1}^{n}\delta((x=x_{i},y=y_{i})],$ |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\delta(x,y)$ is a Dirac mass function centered at point $(x_{i},y_{i})$,
    assuming that all masses in the probability distribution cluster around a single
    point [[78](#bib.bib78)]. Another natural notion for approximating $P(x,y)$ is
    the vicinity distribution $P_{v}(x,y)$, which replaces the Dirac mass function
    with an estimate of the density in the vicinity of point $(x_{i},y_{i})$ [[79](#bib.bib79)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P_{v}(x,y)=\frac{1}{n}\sum_{i=1}^{n}\delta_{v}(x=x_{i},y=y_{i}),$ |  |
    (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\delta_{v}$ is the vicinity point set of $(x_{i},y_{i})$ in $\mathcal{D}$.
    The vicinity distribution assumes that $P(x,y)$ is smooth around any point $(x_{i},y_{i})$
    [[77](#bib.bib77)]. In $P_{v}(x,y)$, models are less prone to memorizing all data
    points, and thus tend to yield higher performance in the testing process. One
    way to achieve vicinity distribution is to apply image augmentation, by which
    an original data point $(x_{i},y_{i})$ can be moved within its vicinity. For example,
    the Gaussian vicinity distribution is equivalent to the addition of Gaussian noise
    to an image [[79](#bib.bib79)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Model-free image augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image processing methods, such as geometric transformation and pixel-level manipulation,
    can be leveraged for augmentation purposes [[6](#bib.bib6), [7](#bib.bib7)]. In
    this study, we refer to model-free image augmentation as contrasting model-based
    image augmentation. The model-free approach consists of single- and multi-image
    branches. As suggested by the names, the former produces augmented images from
    a single image, whereas the latter generates output from multiple images.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Single-image augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From the vicinity distribution, single-image augmentation (SiA) aims to fluctuate
    the data points in the training dataset and increase distribution density. In
    general, SiA leverages traditional image processing, which is simple to understand
    and execute. SiA methods include geometric transformations, color image processing,
    and intensity transformations. Geometric transformation tries to modify the spatial
    relationship between pixels [[80](#bib.bib80)], including affine transformation
    and elastic deformation, while color image processing aims to vary the color of
    an input image. In contrast, the last one is advocated to change parts of the
    images and has recently received more attention.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Geometric transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Objects in naturally captured images can appear in many variations. Geometric
    transformations can be employed to increase this variability. For instance, *translation*
    provides a way to augment objects’ position. Furthermore, an image can be *rotated*,
    changing the perspectives of objects. The angle of rotation should be carefully
    considered to ensure the preservation of appropriate labels. Likewise, a *flip*
    can be executed horizontally or vertically, according to the characteristics of
    the training and testing datasets. For instance, the Cityscapes [[81](#bib.bib81)]
    dataset can be augmented horizontally but not vertically. In addition, objects
    can be magnified or shrunk via *scaling* to mimic multiscale variation. Finally,
    the *elastic distortion* can alter the shape or posture of an object. Among these
    methods, flips have been commonly utilized throughout many studies over the last
    decade for various computer vision tasks, such as image classification [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)], object detection [[82](#bib.bib82), [83](#bib.bib83)],
    and image translation [[84](#bib.bib84), [85](#bib.bib85)]. Two factors must be
    considered when using these methods: the magnitude of the operation to preserve
    label identity and variations in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Color image processing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike greyscale images, color images consist of three channels. Color image
    processing for augmentation assumes that the training and testing dataset distributions
    fluctuate in terms of colors, such as contrast. Although color image processing
    yields superior performance, it is rarely used because the color variations between
    the training and testing datasets are small. However, one interesting point is
    the use of robust features for contrast learning [[86](#bib.bib86)] via color
    image processing, which represents a case of task-agnostic learning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Intensity transformation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| Paper | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hide-and-Seek [[23](#bib.bib23)] | 2017 | Split an image into patches that
    are randomly blocked. |'
  prefs: []
  type: TYPE_TB
- en: '| Cutout [[24](#bib.bib24)] | 2017 | Apply a fixed-size mask to a random location
    for each image. |'
  prefs: []
  type: TYPE_TB
- en: '| Random Erasing [[25](#bib.bib25)] | 2020 | Randomly select a rectangular
    region and displace its pixels with random values. Figure [3](#S4.F3 "Figure 3
    ‣ 4.1.3 Intensity transformation ‣ 4.1 Single-image augmentation ‣ 4 Model-free
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| GridMask [[26](#bib.bib26)] | 2020 | Apply multiscale grid masks to an image
    to mimic occlusions. Figure [4](#S4.F4 "Figure 4 ‣ 4.1.3 Intensity transformation
    ‣ 4.1 Single-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Studies focusing upon intensity transformations. Each study is highlighted
    with its corresponding figure if available.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike geometric transformations and color image processing, intensity transformations
    entail changes at the pixel or patch levels. Random noise, such as Gaussian noise,
    is one of the simplest intensity transformation algorithms [[75](#bib.bib75)].
    The classical methods leverage random noise independently at the pixel level;
    however, the patch level has recently exhibited significant improvement for deep
    learning algorithms [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)].
    Studies pertaining to intensity transformations are listed in Table [4](#S4.T4
    "Table 4 ‣ 4.1.3 Intensity transformation ‣ 4.1 Single-image augmentation ‣ 4
    Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques
    for Deep Learning"). The underlying concept is that the changes push the model
    to learn robust features by avoiding trivial solutions [[76](#bib.bib76)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Cutout [[24](#bib.bib24)] randomly masks the most significant area with a finding
    mechanism to mimic occlusion. However, the most important aspect is cost. Hide-and-Seek
    [[23](#bib.bib23)] directly blocks part of the image with the objective of obscuring
    the most significant area through many iterations of a random process, which is
    simple and fast. Figure [2](#S4.F2 "Figure 2 ‣ 4.1.3 Intensity transformation
    ‣ 4.1 Single-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") shows that images
    are divided into $s\times s$ patches, and each patch is randomly blocked. One
    disadvantage is that the identical size of each patch yields the same level of
    occlusion. To address this issue, Random Erasing [[25](#bib.bib25)] has been employed
    with three random values: the size of the occluded area, height-to-width ratio,
    and top-left corner of the area. Figure [3](#S4.F3 "Figure 3 ‣ 4.1.3 Intensity
    transformation ‣ 4.1 Single-image augmentation ‣ 4 Model-free image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")
    demonstrates some examples of Random Erasing for three computer vision tasks.
    Additionally, this method can be leveraged in image- and object-aware conditions,
    thereby simplifying object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: GridMask aims to balance deleting and reservation, with the objective of blocking
    certain important areas of an object while preserving others to mimic real occlusion.
    To achieve this, GridMask uses a set of predefined masks, as opposed to a single
    mask [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]. As illustrated in
    Figure [4](#S4.F4 "Figure 4 ‣ 4.1.3 Intensity transformation ‣ 4.1 Single-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"), the generated mask is obtained from
    four values, denoting the width and height of every grid and the vertical and
    horizontal distance of the neighboring grid mask. By adjusting these four values,
    grid masks of different sizes and heigh-width ratios can be obtained. Under these
    conditions, GridMask achieves a better balance between deleting and reservation,
    and a preliminary experiment suggests that it has a lower chance of producing
    failure cases than Cutout [[24](#bib.bib24)] and Hide-and-See [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b3426a6750cb816dc7c8eb1dc14394b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Hide-and-Seek [[23](#bib.bib23)] carries out image augmentation where
    one image is split into several patches, and each patch is randomly blocked with
    a specified probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ebf46b9e1002960de28372cc9a04ae5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of Random Erasing [[25](#bib.bib25)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12d84e8f3b0c590a7ebc7903118656fe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: GridMask [[26](#bib.bib26)] and its setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Multiple-image augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multiple-image augmentation (MiA) algorithms are executed on more than one image.
    These methods can further be categorized as instance- and non-instance-level.
    Because one image may include more than one instance, we can mask instances and
    use them independently. Unlike SiA, MiA requires algorithms to merge multiple
    input instances.
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SamplePairing [[27](#bib.bib27)] | 2018 | Combine two images with a single
    label. |'
  prefs: []
  type: TYPE_TB
- en: '| Mixup [[28](#bib.bib28)] | 2018 | Linearly fuse images and their labels.
    Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2 Multiple-image augmentation
    ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| BC Learning [[29](#bib.bib29)] | 2018 | Combine two images and their labels.
    Treat the image as a waveform, and declare that image mixing makes sense for machines.
    |'
  prefs: []
  type: TYPE_TB
- en: '| CutMix [[30](#bib.bib30)] | 2019 | Spatially fuse two images and linearly
    fuse the labels. Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2
    Multiple-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| Mosaic [[22](#bib.bib22)] | 2020 | Spatially mix four images and their annotations,
    thereby enriching the context for each class. |'
  prefs: []
  type: TYPE_TB
- en: '| AugMix [[31](#bib.bib31)] | 2020 | One image undergoes several basic augmentations,
    and the results are fused with the original image. |'
  prefs: []
  type: TYPE_TB
- en: '| PuzzleMix [[32](#bib.bib32)] | 2020 | Optimize a mask for fusing two images
    to utilize the salient information and underlying statistics. |'
  prefs: []
  type: TYPE_TB
- en: '| Co-Mixup [[33](#bib.bib33)] | 2021 | Maximize the salient signal of input
    images and diversity among the augmented images. |'
  prefs: []
  type: TYPE_TB
- en: '| SuperMix [[34](#bib.bib34)] | 2021 | Optimize a mask for fusing two images
    to exploit the salient region with the Newton iterative method, 65x faster than
    gradient descent. |'
  prefs: []
  type: TYPE_TB
- en: '| GridMix [[35](#bib.bib35)] | 2021 | Split two images into patches, spatially
    fuse the patches, and linearly merge the annotation. |'
  prefs: []
  type: TYPE_TB
- en: '| Cut, Paste and Learn [[36](#bib.bib36)] | 2017 | Cut object instances and
    paste them onto random backgrounds. Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Instance-level
    ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| Scale and Blend [[37](#bib.bib37)] | 2017 | Cut and scale object instances,
    and blend them in meaningful locations. |'
  prefs: []
  type: TYPE_TB
- en: '| Context DA [[38](#bib.bib38)] | 2018 | Combine object instances using context
    guidance to obtain meaningful images. |'
  prefs: []
  type: TYPE_TB
- en: '| Simple Copy-Paste [[39](#bib.bib39)] | 2021 | Randomly paste object instances
    to images with large-scale jittering. |'
  prefs: []
  type: TYPE_TB
- en: '| Continuous Copy-Paste [[40](#bib.bib40)] | 2021 | Deploy Cut, Paste and Learn
    to videos. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Studies related to multiple-image augmentation, divided into non-instance-
    (up) and instance-level (bottom).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Non-instance-level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13beaeb3ddc6364b3b52518ef1d68bf4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of non-instance-level multiple-image algorithms [[30](#bib.bib30)].'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of MiA algorithms, the non-instance-level approach adopts and
    fuses the images. Studies pertaining to this concept are listed in Table [5](#S4.T5
    "Table 5 ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning"). One
    of the simplest methods is to compute the average value of each pixel. In Pairing
    Samples [[27](#bib.bib27)], two images are fused to produce an augmented image
    with a label from one source image. This assumption is generalized in Mixup [[28](#bib.bib28)],
    where the labels are also fused. Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level
    ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") illustrates the difference
    between Pairing Samples and Mixup. Mathematically, $\tilde{x}=\lambda x_{i}+(1-\lambda)x_{j}$
    and $\tilde{y}=\lambda y_{i}+(1-\lambda)y_{j}$, where $x_{i}$ and $x_{j}$ are
    two images, $y_{i}$ and $y_{j}$ are the corresponding one-hot labels, and $\tilde{x}$
    and $\tilde{y}$ denote the generated image and label, respectively. By adjusting
    $0\leq\lambda\leq 1$, many images with different labels can be created, thereby
    smoothing out the gap between the two labels in the augmented images. Although
    Pairing Samples and Mixup produce satisfactory results, the fused images are not
    reasonable for humans. Accordingly, these fused images have been declared to make
    sense for machines from the perspective of a waveform [[29](#bib.bib29)]. In addition,
    vicinity distribution can also be utilized to understand this situation. To be
    more specific, changing image variations yet maintaining the label can be regarded
    a deviation in the vicinity distribution space of a specific label, whereas image
    fusion can be considered as an interpolation between the vicinity distribution
    of two labels [[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to BC Learning [[29](#bib.bib29)], CutMix [[30](#bib.bib30)] spatially
    merges images to obtain results that are interpretable by humans. The last picture
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2 Multiple-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning") illustrates the method’s underlying
    strategy, wherein the merged image consists of two source images spatially, and
    its label is obtained from the ratio of certain pixels between two images. Although
    multiple-image augmentation generally utilizes two images, more images can be
    used. For example, Mosaic [[22](#bib.bib22)] employs four images wherein the number
    of objects in one image is increased, thus significantly reducing the need for
    a large mini-batch size for dense prediction. AugMix [[31](#bib.bib31)] randomly
    applies basic multiple methods of image augmentation, and the results are adopted
    to merge with the original image.
  prefs: []
  type: TYPE_NORMAL
- en: Non-instance-level image augmentation has extensions similar to those of intensity
    transformations. To account for the most important area, PuzzleMix [[32](#bib.bib32)]
    discriminates the foreground from the background, and mixes important information
    within the foreground. Further, salient areas from multiple input images are maximized
    to synthesize each augmented image [[33](#bib.bib33)], simultaneously maximizing
    the diversity among the augmented images. To quickly locate dominant regions,
    SuperMix [[34](#bib.bib34)] employs a variant of the Newton iterative method.
    As in Hide-and-Seek [[23](#bib.bib23)], GridMix [[35](#bib.bib35)] divides images
    into fixed-size grids, and each patch of the output image is randomly taken from
    the corresponding patches of two input images. Through this analysis, we believe
    that GridMask [[87](#bib.bib87)] can be adapted to fuse image pairs with changeable
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Instance-level
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whereas the non-instance-level approach employs images directly, the instance-level
    approach leverages instances masked from images. Related studies are listed in
    the second part of Table [5](#S4.T5 "Table 5 ‣ 4.2 Multiple-image augmentation
    ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning"). The instance-level approach comprises two main
    steps. As shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Instance-level ‣ 4.2 Multiple-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"), the first step involves cutting instances
    from source images given a semantic mask, and obtaining clean background senses.
    Next, the obtained instances and background are merged. Cut, Paste and Learn [[36](#bib.bib36)]
    is an early instance-level method, wherein local artifacts are noticed after pasting
    instances to the background. Because local region-based features are important
    for object detection, various blending modes are employed to reduce local artifacts.
    With the exception of boundaries, the instance scale and position are not trivial,
    as objects may be multiscale and recognizable with the help of their contexts,
    as addressed in [[37](#bib.bib37)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/945cf9db4c69ef9791c687032aa6dcd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Cut, Paste and Learn in training and testing process [[36](#bib.bib36)].'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, instance-level image augmentation can mitigate the challenges
    posed by class imbalance. By repurposing rare instances, the number of images
    in the corresponding class increases. Simple Copy-Paste [[39](#bib.bib39)] indicates
    that the instance level enables strong image augmentation methods, for instance,
    segmentation. While it is based on Copy, Paste and Learn, Simple Copy-Paste differs
    in two characteristics. First, the background image is randomly selected from
    the dataset, and subjected to random scale jittering and horizontal flipping.
    Second, large-scale jittering is leveraged to obtain more significant performance.
    The copy-paste concept has also been utilized for time-series tasks [[40](#bib.bib40)]
    such as tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Model-based image augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A model must be pre-trained in model-based image augmentation to generate augmented
    images. The present study classifies this process among three categories, according
    to the conditions to generate images: unconditional, label-conditional, and image-conditional.
    Table [6](#S5.T6 "Table 6 ‣ 5 Model-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") provides information
    regarding appropriate studies.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '| BDA [[45](#bib.bib45)] | 2017 | Use CGAN to generate images optimized by
    a Monte Carlo EM algorithm. Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Label-conditional
    image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of
    Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| ImbCGAN [[46](#bib.bib46)] | 2018 | Deploy CGAN as image augmentation for
    unbalanced classes. |'
  prefs: []
  type: TYPE_TB
- en: '| BAGAN [[47](#bib.bib47)] | 2018 | Train an auto-encoder to initialize generator.
    |'
  prefs: []
  type: TYPE_TB
- en: '| DAGAN [[48](#bib.bib48)] | 2018 | Image is taken as the class condition for
    generator and discriminator. Figure [8](#S5.F8 "Figure 8 ‣ 5.2 Label-conditional
    image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of
    Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| MFC-GAN [[49](#bib.bib49)] | 2019 | Use multiple fake classes to obtain a
    fine-grained image for minority class. |'
  prefs: []
  type: TYPE_TB
- en: '| IDA-GAN[[50](#bib.bib50)] | 2021 | Train a variational auto-encoder and CGAN
    simultaneously. |'
  prefs: []
  type: TYPE_TB
- en: '| S$+$U Learning [[51](#bib.bib51)] | 2017 | Translate synthetic images from
    a graphic model to realistic images using CGAN. |'
  prefs: []
  type: TYPE_TB
- en: '| AugGAN [[52](#bib.bib52)] | 2018 | Aim to semantically preserve object when
    changing its style. |'
  prefs: []
  type: TYPE_TB
- en: '| Plant-CGAN [[53](#bib.bib53)] | 2018 | Translate semantic instance layout
    to real images using CGAN. |'
  prefs: []
  type: TYPE_TB
- en: '| StyleAug [[54](#bib.bib54)] | 2019 | Change image style via style transfer.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Shape bias [[55](#bib.bib55)] | 2019 | Transfer image style from painted
    images to mitigate texture bias of CNN. |'
  prefs: []
  type: TYPE_TB
- en: '| EmoGAN [[56](#bib.bib56)] | 2018 | Translate a neutral face with another
    emotion. |'
  prefs: []
  type: TYPE_TB
- en: '| $\delta$-encoder [[57](#bib.bib57)] | 2018 | Image is taken as a class condition
    to generate images for new or infrequent class. |'
  prefs: []
  type: TYPE_TB
- en: '| Debiased NN [[58](#bib.bib58)] | 2021 | Merge style and content via style
    transfer and appropriate labels. Figure [10](#S5.F10 "Figure 10 ‣ 5.3.2 Label-changing
    image generation ‣ 5.3 Image-conditional image generation ‣ 5 Model-based image
    augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep
    Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| StyleMix [[59](#bib.bib59)] | 2021 | Merge two images with style, content,
    and labels. Figure [11](#S5.F11 "Figure 11 ‣ 5.3.2 Label-changing image generation
    ‣ 5.3 Image-conditional image generation ‣ 5 Model-based image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| GAN-MBD [[60](#bib.bib60)] | 2021 | Translate an image from one class to
    another while preserving semantics via multi-branch discriminator. Figure [9](#S5.F9
    "Figure 9 ‣ 5.3.2 Label-changing image generation ‣ 5.3 Image-conditional image
    generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| SCIT [[2](#bib.bib2)] | 2022 | Translate healthy leaves to abnormal one while
    retaining its style. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Studies relating to model-based image augmentation, label-conditional
    (top), label-preserving image-conditional (middle), and label-changing image-conditional
    (bottom).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Unconditional image generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An image synthesis model benefits image augmentation, which enables it to produce
    new images. Theoretically, the distribution of generated images is similar to
    that in the original dataset for a generative adversarial network (GAN) model
    after training [[88](#bib.bib88)]. However, the generated images are not the same
    as the original images and can be considered as points located in the vicinity
    distribution. In DCGAN [[41](#bib.bib41)], two random noises or latent vectors
    can be interpolated to generate intermediate images, which can be regarded as
    fluctuations between two original data points. Generally, a generative model with
    noise as input is deemed an unconditional model, and the corresponding image generation
    process is considered unconditional image generation. If the datasets encompass
    a single class, as in the case of medical images with one abnormal class [[42](#bib.bib42)],
    an unconditional image generation model can be directly applied to perform augmentation.
    Furthermore, a specific unconditional model can be leveraged for an individual
    class in the presence of multiple classes [[43](#bib.bib43)], [[44](#bib.bib44)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Label-conditional image generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although unconditional image generation has potential, the shared information
    of different classes cannot be utilized. In contrast, label-conditional image
    generation is expected to leverage the shared information and learn variations
    for minority classes using majority-class data. Label-conditional image generation
    requires one specific label as an extra input, and the generated image should
    align with the label condition.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc4717a6e8e6af1d8df4162e0a165596.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: GAN and variants of label-conditional GANs [[45](#bib.bib45)]. G:
    generator, A: authenticator, C: classifier, D: discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary issue in label-conditional image generation is the use of label
    conditions. CGAN [[89](#bib.bib89)] uses the label for a generator, whereas the
    authenticator does not use the label. Consequently, the generator tends to ignore
    label information, as the authenticator cannot provide feedback regarding the
    condition. ACGAN [[90](#bib.bib90)] introduces an auxiliary classifier in the
    discriminator, which encourages the generator to produce images aligned with label
    conditions. With a more complex classifier, BDA [[45](#bib.bib45)] separates the
    classifier from the discriminator. Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Label-conditional
    image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of
    Image Augmentation Techniques for Deep Learning") illustrates the differences
    between BDA and other label-conditional algorithms. In addition, MFC-GAN [[49](#bib.bib49)]
    adopts multiple fake classes in the classification loss to stabilize the training.
  prefs: []
  type: TYPE_NORMAL
- en: One of the main applications of label-conditional image generation is the class
    imbalance [[49](#bib.bib49)] [[46](#bib.bib46)] [[50](#bib.bib50)]. The generative
    model is expected to learn useful features from the majority class, and use them
    to generate images for the minority classes. The generated images are used to
    rebalance the original training dataset. However, it may be challenging to train
    a GAN model with an unbalanced dataset, as the majority class dominates the discriminator
    loss and the generator tends to produce images within the majority class. To address
    this challenge, a pretrained autoencoder with reconstruction loss has been employed
    to initialize a generator [[47](#bib.bib47)] [[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7ba8299378e3c40ec371e5519463eb88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Flowchart of DAGAN [[48](#bib.bib48)], where label information is
    obtained from an image via an encoder, rather than a label.'
  prefs: []
  type: TYPE_NORMAL
- en: Although various discriminators and classifiers may be employed, the aforementioned
    algorithms utilize the class condition on a one-hot label. One resulting limitation
    is that the trained model can generate only known-class images. To overcome this
    limitation, DAGAN [[48](#bib.bib48)] utilizes an image encoder to extract the
    class, so that the generated image is assumed to have the same class as the original
    image. Figure [8](#S5.F8 "Figure 8 ‣ 5.2 Label-conditional image generation ‣
    5 Model-based image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning") illustrates the DAGAN algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Image-conditional image generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In image generation, images can be employed as conditions, known as image translation.
    Generally, an image consists of content and style [[91](#bib.bib91), [92](#bib.bib92)].
    Content refers to class-dependent attributes, such as dogs and cats, whereas style
    denotes class-independent elements, such as color and illumination. Image-conditional
    image generation can be subcategorized into two types: label-preserving and label-changing.
    The former requires content to be retained, whereas the latter requires content
    to be changed.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Label-preserving image generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Label-preserving assumes that the label of a generated image is the same as
    that of the input image. One active field to deploy this approach is the domain
    shift, where the style of the source domain is different from that of the target
    domain. To address this challenge, original images can be translated from the
    source domain to the target domain. To preserve the object during image translation,
    AugGAN employs a segmentation module that extracts context-aware features to share
    parameters with a generator [[52](#bib.bib52)]. For practical applications, synthetic
    images generated by a graphical model are translated into natural images [[51](#bib.bib51)],
    and the leaf layout is translated as a real leaf image [[53](#bib.bib53)]. In
    addition, image translation can be utilized for semantic segmentation with a domain
    shift [[93](#bib.bib93)]. Furthermore, label-preserving can be leveraged to improve
    the robustness of a trained model. Inspired by the observation that CNNs exhibit
    bias on texture toward shape, original images are translated to have different
    textures, which allows the CNN to allocate more attention to shape [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: It is often challenging to obtain the desired style during the image generation
    process. Most algorithms utilize an encoder to extract style from an image, as
    in the case of DRIT++ [[94](#bib.bib94)] and SPADE [[95](#bib.bib95)]. This approach
    to image translation can be regarded as image fusion. In contrast, Jackson et
    al. [[54](#bib.bib54)] proposed style augmentation, where the style is generated
    from a multivariate normal distribution. Another challenge is that the *one* model
    can be adopted to generate images for *multiple* domains with fewer trained images.
    To address this, MetalGAN leverages domain loss and meta-learning strategies [[96](#bib.bib96)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Label-changing image generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to label-preserving, label-changing changes the label-dependent.
    For example, a neutral face can be transformed into a different emotion [[56](#bib.bib56)].
    Although the generated images have poor fidelity, the approach improves the classification
    of emotions. In addition to changing label dependence, the preservation of label
    independence has recently received attention as a way to improve variability within
    the target class, thereby mitigating class imbalance. To take variation from one
    to another class, a style loss is leveraged to retain the style when translating
    an image [[2](#bib.bib2)]. Similarly, a multi-branch discriminator with fewer
    channels is introduced to achieve semantic consistency such as the number of objects
    [[60](#bib.bib60)]. Figure [9](#S5.F9 "Figure 9 ‣ 5.3.2 Label-changing image generation
    ‣ 5.3 Image-conditional image generation ‣ 5 Model-based image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning") shows
    several satisfactory translated images. To address severe class imbalance, a $\delta$-encoder
    has been proposed to extract label-independent features from one label to another
    [[57](#bib.bib57)]. As in the case of DAGAN [[48](#bib.bib48)], class information
    is provided by an image. The $\delta$-encoder and decoder aim to reconstruct the
    given image in the training phase, whereas the decoder is provided a new label
    image and required to generate the same label in the testing phase.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd1655444c63b6eeb33f7ea6b539433c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Semantic level matching by GAN-MBD [[60](#bib.bib60)] for label-changing
    image augmentation, including position, number, and pose.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to label-preserving, label-changing yields more significant improvements
    in model robustness by changing the label and style simultaneously. As illustrated
    in Figure [10](#S5.F10 "Figure 10 ‣ 5.3.2 Label-changing image generation ‣ 5.3
    Image-conditional image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"), traditional image
    augmentation does not change the label after altering the color of the chimpanzee
    to that of a lemon, which incurs shape bias. By contrast, when a texture-biased
    model is trained, the translated image is labeled as a lemon. To balance the bias,
    the translated image by style transfer is taken with two labels [[58](#bib.bib58)]
    – chimpanzee and lemon – which eliminates bias. Inspired by Mixup [[28](#bib.bib28)],
    Hong et al. developed StyleMix [[59](#bib.bib59)], which merges the two inputs
    to obtain content and style labels, as shown in Figure [11](#S5.F11 "Figure 11
    ‣ 5.3.2 Label-changing image generation ‣ 5.3 Image-conditional image generation
    ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning"). These labels are then fused to obtain the final
    label for the generated images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09aa952cc7cb2bb6b4e7e1254a48697a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Label assignment for the biased and unbiased model with respect
    to shape and texture [[58](#bib.bib58)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a651b5f3a421dc498ad836e24e76d65e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Examples of label assignment with different algorithms [[59](#bib.bib59)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Optimizing policy-based image augmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All algorithms mentioned in the previous two sections represent specific schemes,
    wherein domain knowledge is required to achieve better performance. In general,
    individual operations with the desired magnitude are utilized to perform image
    augmentation for specific datasets according to their characteristics. However,
    hyperparameter optimization is challenging and time-consuming. One way to mitigate
    this is to design algorithms that determine optimal augmentation strategies. These
    algorithms, termed policy-based optimization, encompass two categories: reinforcement
    learning-based, and adversarial learning-based. The former category employs reinforcement
    learning (RL) to determine the optimal strategy, whereas the latter category adopts
    augmented operations and their magnitudes that generates a large training loss
    and small validation loss. As generative adversarial networks (GANs) can be utilized
    for both model-based and optimizing policy-based image augmentation, the objective
    to adopt GANs is the primary difference. Model-based category aims to *directly*
    generate images, instead of other goals such as finding optimal transformations
    [[69](#bib.bib69)]. Studies pertaining to policy-based optimization are listed
    in Table [7](#S6.T7 "Table 7 ‣ 6 Optimizing policy-based image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Year | Highlight |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AutoAugment [[61](#bib.bib61)] | 2019 | Use reinforcement learning to determine
    the optimal augmentation strategies. Figure [12](#S6.F12 "Figure 12 ‣ 6.1 Reinforcement
    learning-based ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| Fast AA [[62](#bib.bib62)] | 2019 | Use efficient density matching for augmentation
    policy search. |'
  prefs: []
  type: TYPE_TB
- en: '| PBA [[63](#bib.bib63)] | 2019 | Adopt non-stationary augmentation policy
    schedules via population-based training. |'
  prefs: []
  type: TYPE_TB
- en: '| Faster AA [[64](#bib.bib64)] | 2019 | Use a differentiable policy search
    pipeline via approximate gradients. |'
  prefs: []
  type: TYPE_TB
- en: '| RandAugment [[65](#bib.bib65)] | 2020 | Reduce the search space of AutoAug
    via probability adjustment. |'
  prefs: []
  type: TYPE_TB
- en: '| MADAO [[66](#bib.bib66)] | 2020 | Train task model and optimize the search
    space simultaneously by implicit gradient with Neumann series approximation. |'
  prefs: []
  type: TYPE_TB
- en: '| LDA [[67](#bib.bib67)] | 2020 | Take policy search as a discrete optimization
    for object detection. |'
  prefs: []
  type: TYPE_TB
- en: '| LSSP [[68](#bib.bib68)] | 2021 | Learn a sample-specific policy for sequential
    image augmentation. |'
  prefs: []
  type: TYPE_TB
- en: '| ADA [[69](#bib.bib69)] | 2016 | Seek a small transformation that yields maximal
    classification loss on the transformed sample. |'
  prefs: []
  type: TYPE_TB
- en: '| CDST-DA [[70](#bib.bib70)] | 2017 | Optimize a generative sequence using
    GAN in which the transformed image is pushed to be within the same class distribution.
    |'
  prefs: []
  type: TYPE_TB
- en: '| AdaTransform [[71](#bib.bib71)] | 2019 | Use a competitive task to obtain
    augmented images with a high task loss in the training stage, and a cooperative
    task to obtain augmented images with a low task loss in the testing stage. Figure
    [13](#S6.F13 "Figure 13 ‣ 6.2 Adversarial learning-based ‣ 6 Optimizing policy-based
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning"). |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial AA [[72](#bib.bib72)] | 2020 | Optimize a policy to increase
    task loss while allowing task model to minimize the loss. |'
  prefs: []
  type: TYPE_TB
- en: '| IF-DA [[73](#bib.bib73)] | 2020 | Use influence function to predict how validation
    loss is affected by image augmentation, and minimize the approximated validation
    loss. |'
  prefs: []
  type: TYPE_TB
- en: '| SPA [[74](#bib.bib74)] | 2021 | Select suitable samples to perform image
    augmentation. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Studies relating to optimizing policy of image augmentation. The upper
    and the bottom suggest reinforcement learning- and adversarial learning-based
    image augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Reinforcement learning-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AutoAugment [[61](#bib.bib61)] is a seminal approach that employs reinforcement
    learning. As shown in Figure [12](#S6.F12 "Figure 12 ‣ 6.1 Reinforcement learning-based
    ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"), iterative steps are used to find
    the optimal policy. The controller samples a strategy from a search space with
    the operation type and its corresponding probability and magnitude, and a task
    network subsequently obtains the validation accuracy as feedback to update the
    controller. Because the search space is very large, lighter child networks are
    leveraged. After training, the controller is used to train the original task model
    and can be finetuned in other datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/215549d727d9883fc00ee2a4f0628306.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Overview of AutoAugment [[61](#bib.bib61)], a reinforcement learning-based
    image augmentation method.'
  prefs: []
  type: TYPE_NORMAL
- en: Although AutoAugment achieves satisfactory classification performance across
    several datasets, it requires a long training time. To address this issue, several
    studies have been conducted from different perspectives. For instance, RandAugment
    [[65](#bib.bib65)] replaces several probabilities in AutoAugment with a uniform
    probability. Conversely, Fast AA [[62](#bib.bib62)] and Faster AA [[64](#bib.bib64)]
    leverage density matching, aligning the densities of the training and augmented
    training datasets, instead of Proximal Policy Optimization [[97](#bib.bib97)],
    to optimize the controller in AutoAugment. Furthermore, PBA [[63](#bib.bib63)]
    attempts to learn a policy schedule from population-based training, rather than
    a single policy.
  prefs: []
  type: TYPE_NORMAL
- en: Except for the long training phase, AutoAugment utilizes child models, by which
    the learned policy may not be optimal for the final task model. To address this
    issue, Hataya et al. [[66](#bib.bib66)] trained the target model and image augmentation
    policy simultaneously using the same differentiable image augmentation pipeline
    in Faster AA. In contrast, Adversarial AA [[72](#bib.bib72)] leverages adversarial
    loss simultaneously with reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of the algorithms mentioned above is that the learned image augmentation
    policy is at the dataset level. Conversely, class- and sample-level image augmentation
    methods were considered in [[98](#bib.bib98)] and [[68](#bib.bib68)], respectively,
    wherein each class or sample utilizes a specific policy. Furthermore, instance-level
    image augmentation was considered in [[67](#bib.bib67)] for object detection,
    where operations were performed only inside the bounding box.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Adversarial learning-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The primary objective of image augmentation is to train a task model with a
    training dataset to achieve sufficient generalizability on a testing dataset.
    One assumption is that hard samples are more useful, and the input images that
    cause a larger training loss are considered hard samples. Adversarial learning-based
    image augmentation aims to learn an image augmentation policy to generate hard
    samples based on the original training samples.
  prefs: []
  type: TYPE_NORMAL
- en: An early method [[69](#bib.bib69)] attempts to find a small transformation that
    maximizes training loss on the augmented samples, wherein learning optimization
    finds an optimal magnitude given an operation. One of the main limitations is
    the label-preserving assumption that the augmented image retains the same label
    as the original image. To meet this assumption, a common strategy is to design
    the type of operation and range of corresponding magnitude using human knowledge.
    To weaken this assumption, Ratner et al. [[70](#bib.bib70)] introduced generative
    adversarial loss to learn a transformation sequence in which the discriminator
    pushes the generated images to one of the original classes, instead of an unseen
    or null class.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, SPA [[74](#bib.bib74)] attempts to select suitable samples, and
    image augmentation is leveraged only on those samples in which the augmented image
    incurs a larger training loss than the original image. Although SPA trains the
    image augmentation policy and task model simultaneously at the sample level, the
    impact of the learned policy in the validation dataset is unknown. To address
    this challenge, an influence function was adopted for approximating the change
    in validation loss without actually comparing performance [[73](#bib.bib73)].
    Another interesting concept is the use of image augmentation in the *testing stage*.
    To achieve this, AdaTransform [[71](#bib.bib71)] learns two tasks – competitive
    and cooperative – as illustrated in Figure [13](#S6.F13 "Figure 13 ‣ 6.2 Adversarial
    learning-based ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). In a competitive
    task, the transformer learns to increase the input variance by increasing the
    loss of the target network, while the discriminator attempts to push the augmented
    image realistically. Conversely, the transformer learns to decrease the variance
    of the augmented image in the cooperative task by reducing the loss of the target
    network. After training, the transformer is utilized to reduce the variance of
    the input image, thereby simplifying the testing process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/295cf8f7f64afcf00e1f977e60b0948e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Overview of AdaTransform [[71](#bib.bib71)]. AdaTransform encompasses
    two tasks – competitive training and cooperative testing – and three components:
    transformer $T$, discriminator $D$, and target network $N$. The transformer increases
    the variance of training data by competing with both $D$ and $N$. It also cooperates
    with $N$ in the testing phase to reduce data variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, the usage of the mentioned strategies to perform image augmentation
    are first discussed. Several future directions are then illustrated. Furthermore,
    three related topics are discussed: understanding image augmentation from theory
    perspective, adopting image augmentation with other strategy, and augmenting features
    instead of images.'
  prefs: []
  type: TYPE_NORMAL
- en: Current situation. Datasets are assumed to be essential to obtain satisfactory
    performance. One way to generate an appropriate dataset is through image augmentation
    algorithms, which have demonstrated impressive results across multiple datasets
    and heterogeneous models. For instance, Mixup [[28](#bib.bib28)] increases the
    validation accuracy in ImageNet-2012 by 1.5 and 1.2 percent with ResNet-50 and
    ResNet-101\. Non-trivially, GAN-MBD [[60](#bib.bib60)] achieves 84.28 classification
    accuracy with an unbalance dataset setting in 102Flowers, 33.11, 31.44, and 14.05
    higher than non-image augmentation, geometrical transformation, and focal loss,
    respectively. Currently, mode-free and optimizing policies are widely leveraged,
    whereas the mode-based approach is an active research topic for specific challenges,
    such as class imbalance and domain adaptation. In addition, although most algorithms
    are label-preserving, label-changing algorithms have recently received attention.
  prefs: []
  type: TYPE_NORMAL
- en: Future direction. Although many image augmentation algorithms exist, developing
    novel algorithms remains crucial to improve the performance of deep learning.
    We argue that recognizing new challenges or variations may inspire novel methods
    if they can be mimicked using image augmentation. Further, most algorithms of
    image augmentation are designed for classification and hence extending them to
    other applications is one of the most applicable directions by incorporating application-based
    knowledge, such as time-series in video [[40](#bib.bib40)]. Another interesting
    direction is distinguishing specific applications from general computer vision
    tasks such as ImageNet [[99](#bib.bib99)] and COCO [[100](#bib.bib100)] and then
    finding new motivations to design image augmentation. For example, most variations
    in plant healthy and diseased leaves are shared and thus can be converted from
    one to another [[2](#bib.bib2)]. Finally, considering image augmentation from
    a systematic perspective is appealing. For example, the effects of image augmentation
    schedules on optimization such as learning rate and batch size, are analyzed in
    [[101](#bib.bib101)].
  prefs: []
  type: TYPE_NORMAL
- en: Understanding image augmentation. This study was conducted to understand the
    objectives of image augmentation in the context of deep learning, from the perspectives
    of challenges and vicinity distribution. Although it was also verified that image
    augmentation is similar to regularization [[79](#bib.bib79)], most of the evidences
    are empirically from experiments. Understanding them in theory is therefore appealing.
    Recently, kernel theory [[102](#bib.bib102)] and group theory [[103](#bib.bib103)]
    have been used to analyze the effects of image augmentation. In addition, the
    improvement yielded by image augmentation in the context of model generalizability
    has been quantified using affinity and diversity [[104](#bib.bib104)].
  prefs: []
  type: TYPE_NORMAL
- en: New strategy to leverage image augmentation. Although image augmentation is
    commonly used in a supervised manner, this must not necessarily be the case. First,
    a pretext task can be created via image augmentation, such as predicting the degrees
    of rotation [[105](#bib.bib105)] and relative positions of image patches [[106](#bib.bib106)].
    Second, image augmentation can be leveraged to generate positive samples for contrast
    learning under the assumption that an augmented image is similar to the corresponding
    original image [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)]. Furthermore,
    semi-supervised learning benefits from image augmentation [[79](#bib.bib79), [110](#bib.bib110),
    [111](#bib.bib111)].
  prefs: []
  type: TYPE_NORMAL
- en: Feature augmentation attempts to perform augmentation in feature space instead
    of image space in image augmentation, and thus reduces the computation cost but
    without visual evidences. A feature space generally has dense information in semantic
    level than an image space. Consequently, operation in feature space is more efficient
    [[112](#bib.bib112)], such as domain knowledge [[113](#bib.bib113)]. Simultaneously,
    we believe that most of the techniques in image augmentation can be extended to
    feature augmentation, such as Manifold Mixup [[114](#bib.bib114)] from Mixup [[28](#bib.bib28)]
    and occluded feature [[115](#bib.bib115)].
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This study surveyed a wide range of image augmentation algorithms with a novel
    taxonomy encompassing three categories: model-free, model-based, and optimizing
    policy-based. To understand the objectives of image augmentation, we analyzed
    the challenges of deploying a deep learning model for computer vision tasks, and
    adopted the concept of vicinity distribution. We found that image augmentation
    significantly improves task performance, and many algorithms have been designed
    for specific challenges, such as intensity transformations for occlusion, and
    model-based algorithms for class imbalance and domain shift. Based on this analysis,
    we argue that novel methods can be inspired by new challenges. Conversely, appropriate
    methods can be selected after recognizing the challenges posed by a dataset. Furthermore,
    we discussed the current situation and possible directions of image augmentation
    with three relevant interesting topics. We hope that our study will provide an
    enhanced understanding of image augmentation and encourage the community to prioritize
    dataset characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research was partly supported by the Basic Science Research Program through
    the National Research Foundation of Korea (NRF) funded by the Ministry of Education
    (No.2019R1A6A1A09031717), supported by the National Research Foundation of Korea
    (NRF) grant funded by the Ministry of Science and ICT (MSIT) (No. 2020R1A2C2013060),
    and supported by the Korea Institute of Planning and Evaluation for Technology
    in Food, Agriculture, and Forestry (IPET) and Korea Smart Farm R&D Foundation
    (KosFarm) through the Smart Farm Innovation Technology Development Program, funded
    by the Ministry of Agriculture, Food and Rural Affairs (MAFRA), Ministry of Science
    and ICT (MSIT), and Rural Development Administration (RDA) (No. 421027-04).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, A survey on deep learning
    in medical image analysis, Medical image analysis 42 (2017) 60–88.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Xu, S. Yoon, A. Fuentes, J. Yang, D. Park, Style-consistent image translation:
    A novel data augmentation paradigm to improve plant disease recognition, Front.
    Plant Sci. 12: 773142\. doi: 10.3389/fpls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] S. C. Wong, A. Gatt, V. Stamatescu, M. D. McDonnell, Understanding data
    augmentation for classification: when to warp?, in: 2016 international conference
    on digital image computing: techniques and applications (DICTA), IEEE, 2016, pp.
    1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] L. Taylor, G. Nitschke, Improving deep learning with generic data augmentation,
    in: 2018 IEEE Symposium Series on Computational Intelligence (SSCI), IEEE, 2018,
    pp. 1542–1547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] P. Kaur, B. S. Khehra, E. B. S. Mavi, Data augmentation for object detection:
    A review, in: 2021 IEEE International Midwest Symposium on Circuits and Systems
    (MWSCAS), IEEE, 2021, pp. 537–543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C. Shorten, T. M. Khoshgoftaar, A survey on image data augmentation for
    deep learning, Journal of big data 6 (1) (2019) 1–48.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] N. E. Khalifa, M. Loey, S. Mirjalili, A comprehensive survey of recent
    trends in deep learning for digital images augmentation, Artificial Intelligence
    Review (2021) 1–27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] X. Wang, K. Wang, S. Lian, A survey on face data augmentation for the training
    of deep neural networks, Neural computing and applications 32 (19) (2020) 15503–15531.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] F. H. K. d. S. Tanaka, C. Aranha, Data augmentation using gans, arXiv preprint
    arXiv:1904.09135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] H. Naveed, Survey: Image mixing and deleting for data augmentation, arXiv
    preprint arXiv:2106.07085.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
    deep convolutional neural networks, Advances in neural information processing
    systems 25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely connected
    convolutional networks, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 4700–4708.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transferable architectures
    for scalable image recognition, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 8697–8710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J. Mueller,
    R. Manmatha, et al., Resnest: Split-attention networks, arXiv preprint arXiv:2004.08955.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou, Training
    data-efficient image transformers & distillation through attention, in: International
    Conference on Machine Learning, PMLR, 2021, pp. 10347–10357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
    transformer: Hierarchical vision transformer using shifted windows, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10012–10022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object
    detection with region proposal networks, Advances in neural information processing
    systems 28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified,
    real-time object detection, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2016, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg,
    Ssd: Single shot multibox detector, in: European conference on computer vision,
    Springer, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: Optimal speed and accuracy
    of object detection, arXiv preprint arXiv:2004.10934.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] K. K. Singh, Y. J. Lee, Hide-and-seek: Forcing a network to be meticulous
    for weakly-supervised object and action localization, in: 2017 IEEE international
    conference on computer vision (ICCV), IEEE, 2017, pp. 3544–3553.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. DeVries, G. W. Taylor, Improved regularization of convolutional neural
    networks with cutout, arXiv preprint arXiv:1708.04552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang, Random erasing data augmentation,
    in: Proceedings of the AAAI conference on artificial intelligence, Vol. 34, 2020,
    pp. 13001–13008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] P. Chen, S. Liu, H. Zhao, J. Jia, Gridmask data augmentation, arXiv preprint
    arXiv:2001.04086.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] H. Inoue, Data augmentation by pairing samples for images classification,
    arXiv preprint arXiv:1801.02929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond empirical
    risk minimization, arXiv preprint arXiv:1710.09412.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Tokozume, Y. Ushiku, T. Harada, Between-class learning for image classification,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 5486–5494.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: Regularization
    strategy to train strong classifiers with localizable features, in: Proceedings
    of the IEEE/CVF international conference on computer vision, 2019, pp. 6023–6032.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, B. Lakshminarayanan,
    Augmix: A simple method to improve robustness and uncertainty under data shift,
    in: International conference on learning representations, Vol. 1, 2020, p. 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] J.-H. Kim, W. Choo, H. O. Song, Puzzle mix: Exploiting saliency and local
    statistics for optimal mixup, in: International Conference on Machine Learning,
    PMLR, 2020, pp. 5275–5285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] J. Kim, W. Choo, H. Jeong, H. O. Song, Co-mixup: Saliency guided joint
    mixup with supermodular diversity, in: International Conference on Learning Representations,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] A. Dabouei, S. Soleymani, F. Taherkhani, N. M. Nasrabadi, Supermix: Supervising
    the mixing data augmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 13794–13803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] K. Baek, D. Bang, H. Shim, Gridmix: Strong regularization through local
    context mapping, Pattern Recognition 109 (2021) 107594.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. Dwibedi, I. Misra, M. Hebert, Cut, paste and learn: Surprisingly easy
    synthesis for instance detection, in: Proceedings of the IEEE international conference
    on computer vision, 2017, pp. 1301–1310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] G. Georgakis, A. Mousavian, A. C. Berg, J. Kosecka, Synthesizing training
    data for object detection in indoor scenes, arXiv preprint arXiv:1702.07836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Dvornik, J. Mairal, C. Schmid, Modeling visual context is key to augmenting
    object detection datasets, in: Proceedings of the European Conference on Computer
    Vision (ECCV), 2018, pp. 364–380.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V.
    Le, B. Zoph, Simple copy-paste is a strong data augmentation method for instance
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2021, pp. 2918–2928.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Z. Xu, A. Meng, Z. Shi, W. Yang, Z. Chen, L. Huang, Continuous copy-paste
    for one-stage multi-object tracking and segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 15323–15332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Radford, L. Metz, S. Chintala, Unsupervised representation learning
    with deep convolutional generative adversarial networks, arXiv preprint arXiv:1511.06434.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Madani, M. Moradi, A. Karargyris, T. Syeda-Mahmood, Chest x-ray generation
    and data augmentation for cardiovascular abnormality classification, in: Medical
    Imaging 2018: Image Processing, Vol. 10574, International Society for Optics and
    Photonics, 2018, p. 105741M.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, Synthetic
    data augmentation using gan for improved liver lesion classification, in: 2018
    IEEE 15th international symposium on biomedical imaging (ISBI 2018), IEEE, 2018,
    pp. 289–293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan,
    Gan-based synthetic medical image augmentation for increased cnn performance in
    liver lesion classification, Neurocomputing 321 (2018) 321–331.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] T. Tran, T. Pham, G. Carneiro, L. Palmer, I. Reid, A bayesian data augmentation
    approach for learning deep models, Advances in neural information processing systems
    30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] G. Douzas, F. Bacao, Effective data generation for imbalanced learning
    using conditional generative adversarial networks, Expert Systems with applications
    91 (2018) 464–471.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, C. Malossi, Bagan: Data
    augmentation with balancing gan, arXiv preprint arXiv:1803.09655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Antoniou, A. Storkey, H. Edwards, Data augmentation generative adversarial
    networks, arXiv preprint arXiv:1711.04340.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Ali-Gombe, E. Elyan, Mfc-gan: class-imbalanced dataset classification
    using multiple fake class generative adversarial network, Neurocomputing 361 (2019)
    212–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] H. Yang, Y. Zhou, Ida-gan: A novel imbalanced data augmentation gan, in:
    2020 25th International Conference on Pattern Recognition (ICPR), IEEE, 2021,
    pp. 8299–8305.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, R. Webb, Learning
    from simulated and unsupervised images through adversarial training, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2107–2116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, S.-H. Lai, Auggan:
    Cross domain adaptation with gan-based data augmentation, in: Proceedings of the
    European Conference on Computer Vision (ECCV), 2018, pp. 718–731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Zhu, M. Aoun, M. Krijn, J. Vanschoren, H. T. Campus, Data augmentation
    using conditional generative adversarial networks for leaf counting in arabidopsis
    plants., in: BMVC, 2018, p. 324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] P. T. Jackson, A. A. Abarghouei, S. Bonner, T. P. Breckon, B. Obara, Style
    augmentation: data augmentation via style randomization., in: CVPR Workshops,
    Vol. 6, 2019, pp. 10–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, W. Brendel,
    Imagenet-trained cnns are biased towards texture; increasing shape bias improves
    accuracy and robustness, arXiv preprint arXiv:1811.12231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Zhu, Y. Liu, J. Li, T. Wan, Z. Qin, Emotion classification with data
    augmentation using generative adversarial networks, in: Pacific-Asia conference
    on knowledge discovery and data mining, Springer, 2018, pp. 349–360.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary, M. Marder, A. Kumar, R. Feris,
    R. Giryes, A. Bronstein, Delta-encoder: an effective sample synthesis method for
    few-shot object recognition, Advances in Neural Information Processing Systems
    31.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Li, Q. Yu, M. Tan, J. Mei, P. Tang, W. Shen, A. Yuille, et al., Shape-texture
    debiased neural network training, in: International Conference on Learning Representations,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] M. Hong, J. Choi, G. Kim, Stylemix: Separating content and style for enhanced
    data augmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2021, pp. 14862–14870.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Z. Zheng, Z. Yu, Y. Wu, H. Zheng, B. Zheng, M. Lee, Generative adversarial
    network with multi-branch discriminator for imbalanced cross-species image-to-image
    translation, Neural Networks 141 (2021) 355–371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le, Autoaugment: Learning
    augmentation strategies from data, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 113–123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Lim, I. Kim, T. Kim, C. Kim, S. Kim, Fast autoaugment, Advances in
    Neural Information Processing Systems 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] D. Ho, E. Liang, X. Chen, I. Stoica, P. Abbeel, Population based augmentation:
    Efficient learning of augmentation policy schedules, in: International Conference
    on Machine Learning, PMLR, 2019, pp. 2731–2741.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] R. Hataya, J. Zdenek, K. Yoshizoe, H. Nakayama, Faster autoaugment: Learning
    augmentation strategies using backpropagation, in: European Conference on Computer
    Vision, Springer, 2020, pp. 1–16.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] E. D. Cubuk, B. Zoph, J. Shlens, Q. V. Le, Randaugment: Practical automated
    data augmentation with a reduced search space, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 702–703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] R. Hataya, J. Zdenek, K. Yoshizoe, H. Nakayama, Meta approach to data
    augmentation optimization, in: Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision, 2022, pp. 2574–2583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] B. Zoph, E. D. Cubuk, G. Ghiasi, T.-Y. Lin, J. Shlens, Q. V. Le, Learning
    data augmentation strategies for object detection, in: European conference on
    computer vision, Springer, 2020, pp. 566–583.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] P. Li, X. Liu, X. Xie, Learning sample-specific policies for sequential
    image augmentation, in: Proceedings of the 29th ACM International Conference on
    Multimedia, 2021, pp. 4491–4500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. Fawzi, H. Samulowitz, D. Turaga, P. Frossard, Adaptive data augmentation
    for image classification, in: 2016 IEEE international conference on image processing
    (ICIP), Ieee, 2016, pp. 3688–3692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] A. J. Ratner, H. Ehrenberg, Z. Hussain, J. Dunnmon, C. Ré, Learning to
    compose domain-specific transformations for data augmentation, Advances in neural
    information processing systems 30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Z. Tang, X. Peng, T. Li, Y. Zhu, D. N. Metaxas, Adatransform: Adaptive
    data transformation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2019, pp. 2998–3006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] X. Zhang, Q. Wang, J. Zhang, Z. Zhong, Adversarial autoaugment, arXiv
    preprint arXiv:1912.11188.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. Lee, H. Park, T. Pham, C. D. Yoo, Learning augmentation network via
    influence functions, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2020, pp. 10961–10970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] T. Takase, R. Karakida, H. Asoh, Self-paced data augmentation for training
    neural networks, Neurocomputing 442 (2021) 296–306.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] V. Vapnik, Principles of risk minimization for learning theory, Advances
    in neural information processing systems 4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep
    learning requires rethinking generalization (2016), arXiv preprint arXiv:1611.03530.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] V. Vapnik, The nature of statistical learning theory, Springer science
    & business media, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] O. Chapelle, J. Weston, L. Bottou, V. Vapnik, Vicinal risk minimization,
    Advances in neural information processing systems 13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] M. P. Ekstrom, Digital image processing techniques, Vol. 2, Academic Press,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic urban scene
    understanding, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2016, pp. 3213–3223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C.-Y. Wang, A. Bochkovskiy, H.-Y. M. Liao, Scaled-yolov4: Scaling cross
    stage partial network, in: Proceedings of the IEEE/cvf conference on computer
    vision and pattern recognition, 2021, pp. 13029–13038.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] W. Ma, Y. Wu, F. Cen, G. Wang, Mdfn: Multi-scale deep feature learning
    network for object detection, Pattern Recognition 100 (2020) 107149.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation
    using cycle-consistent adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, 2017, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] W. Xu, K. Shawn, G. Wang, Toward learning a unified many-to-many mapping
    for diverse image translation, Pattern Recognition 93 (2019) 570–580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple framework for contrastive
    learning of visual representations, in: International conference on machine learning,
    PMLR, 2020, pp. 1597–1607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] J. Yang, Gridmask based data augmentation for bengali handwritten grapheme
    classification, in: Proceedings of the 2020 2nd International Conference on Intelligent
    Medicine and Image Processing, 2020, pp. 98–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, Y. Bengio, Generative adversarial nets, Advances in neural information
    processing systems 27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Mirza, S. Osindero, Conditional generative adversarial nets, arXiv
    preprint arXiv:1411.1784.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Odena, C. Olah, J. Shlens, Conditional image synthesis with auxiliary
    classifier gans, in: International conference on machine learning, PMLR, 2017,
    pp. 2642–2651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] X. Huang, S. Belongie, Arbitrary style transfer in real-time with adaptive
    instance normalization, in: Proceedings of the IEEE international conference on
    computer vision, 2017, pp. 1501–1510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] V. Dumoulin, J. Shlens, M. Kudlur, A learned representation for artistic
    style, arXiv preprint arXiv:1610.07629.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] R. Li, W. Cao, Q. Jiao, S. Wu, H.-S. Wong, Simplified unsupervised image
    translation for semantic segmentation adaptation, Pattern Recognition 105 (2020)
    107343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] H.-Y. Lee, H.-Y. Tseng, Q. Mao, J.-B. Huang, Y.-D. Lu, M. Singh, M.-H.
    Yang, Drit++: Diverse image-to-image translation via disentangled representations,
    International Journal of Computer Vision 128 (10) (2020) 2402–2417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, Semantic image synthesis with
    spatially-adaptive normalization, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2019, pp. 2337–2346.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] T. Fontanini, E. Iotti, L. Donati, A. Prati, Metalgan: Multi-domain label-less
    image synthesis using cgans and meta-learning, Neural Networks 131 (2020) 185–200.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy
    optimization algorithms, arXiv preprint arXiv:1707.06347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Lemley, S. Bazrafkan, P. Corcoran, Smart augmentation learning an optimal
    data augmentation strategy, Ieee Access 5 (2017) 5858–5869.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
    large-scale hierarchical image database, in: 2009 IEEE conference on computer
    vision and pattern recognition, Ieee, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    C. L. Zitnick, Microsoft coco: Common objects in context, in: European conference
    on computer vision, Springer, 2014, pp. 740–755.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] B. Hanin, Y. Sun, How data augmentation affects optimization for linear
    regression, Advances in Neural Information Processing Systems 34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, C. Ré, A kernel theory
    of modern data augmentation, in: International Conference on Machine Learning,
    PMLR, 2019, pp. 1528–1537.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] S. Chen, E. Dobriban, J. Lee, A group-theoretic framework for data augmentation,
    Advances in Neural Information Processing Systems 33 (2020) 21321–21333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. Gontijo-Lopes, S. Smullin, E. D. Cubuk, E. Dyer, Tradeoffs in data
    augmentation: An empirical study, in: International Conference on Learning Representations,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] N. Komodakis, S. Gidaris, Unsupervised representation learning by predicting
    image rotations, in: International Conference on Learning Representations (ICLR),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] C. Doersch, A. Gupta, A. A. Efros, Unsupervised visual representation
    learning by context prediction, in: Proceedings of the IEEE international conference
    on computer vision, 2015, pp. 1422–1430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M. Ye, X. Zhang, P. C. Yuen, S.-F. Chang, Unsupervised embedding learning
    via invariant and spreading instance feature, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2019, pp. 6210–6219.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
    C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al., Bootstrap your
    own latent-a new approach to self-supervised learning, Advances in Neural Information
    Processing Systems 33 (2020) 21271–21284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A. Joulin,
    Emerging properties in self-supervised vision transformers, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2021, pp. 9650–9660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang,
    C. Raffel, Remixmatch: Semi-supervised learning with distribution alignment and
    augmentation anchoring, arXiv preprint arXiv:1911.09785.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Q. Xie, Z. Dai, E. Hovy, T. Luong, Q. Le, Unsupervised data augmentation
    for consistency training, Advances in Neural Information Processing Systems 33
    (2020) 6256–6268.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Wang, G. Huang, S. Song, X. Pan, Y. Xia, C. Wu, Regularizing deep
    networks with semantic data augmentation, IEEE Transactions on Pattern Analysis
    and Machine Intelligence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Q. Wang, F. Meng, T. P. Breckon, Data augmentation with norm-vae for
    unsupervised domain adaptation, arXiv preprint arXiv:2012.00848.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    Y. Bengio, Manifold mixup: Better representations by interpolating hidden states,
    in: International Conference on Machine Learning, PMLR, 2019, pp. 6438–6447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] F. Cen, X. Zhao, W. Li, G. Wang, Deep feature augmentation for occluded
    image classification, Pattern Recognition 111 (2021) 107737.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
