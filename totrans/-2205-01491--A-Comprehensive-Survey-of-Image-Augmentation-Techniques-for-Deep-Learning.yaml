- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:46:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:46:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2205.01491] A Comprehensive Survey of Image Augmentation Techniques for Deep
    Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2205.01491] 深度学习图像增强技术的综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01491](https://ar5iv.labs.arxiv.org/html/2205.01491)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2205.01491](https://ar5iv.labs.arxiv.org/html/2205.01491)
- en: A Comprehensive Survey of Image Augmentation Techniques for Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习图像增强技术的综合调查
- en: Mingle Xu Department of Electronics Engineering, Jeonbuk National University,
    Jeonbuk 54896, South Korea Sook Yoon Department of Computer Engineering, Mokpo
    National University, Jeonnam 58554, South Korea Alvaro Fuentes Core Research Institute
    of Intelligent Robots, Jeonbuk National University, Jeonbuk 54896, South Korea
    Dong Sun Park Core Research Institute of Intelligent Robots, Jeonbuk National
    University, Jeonbuk 54896, South Korea
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Mingle Xu，电子工程系，全北国立大学，全北54896，韩国；Sook Yoon，计算机工程系，木浦国立大学，全南58554，韩国；Alvaro
    Fuentes，智能机器人核心研究所，全北国立大学，全北54896，韩国；Dong Sun Park，智能机器人核心研究所，全北国立大学，全北54896，韩国
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Although deep learning has achieved satisfactory performance in computer vision,
    a large volume of images is required. However, collecting images is often expensive
    and challenging. Many image augmentation algorithms have been proposed to alleviate
    this issue. Understanding existing algorithms is, therefore, essential for finding
    suitable and developing novel methods for a given task. In this study, we perform
    a comprehensive survey of image augmentation for deep learning using a novel informative
    taxonomy. To examine the basic objective of image augmentation, we introduce challenges
    in computer vision tasks and vicinity distribution. The algorithms are then classified
    among three categories: model-free, model-based, and optimizing policy-based.
    The model-free category employs the methods from image processing, whereas the
    model-based approach leverages image generation models to synthesize images. In
    contrast, the optimizing policy-based approach aims to find an optimal combination
    of operations. Based on this analysis, we believe that our survey enhances the
    understanding necessary for choosing suitable methods and designing novel algorithms.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在计算机视觉领域取得了令人满意的性能，但仍需要大量图像。然而，收集图像通常是昂贵且具有挑战性的。许多图像增强算法已被提出以缓解这一问题。因此，理解现有算法对于寻找适合的和开发新方法至关重要。在本研究中，我们使用一种新颖的信息分类法对深度学习的图像增强进行全面调查。为了考察图像增强的基本目标，我们介绍了计算机视觉任务中的挑战和附近分布。然后将算法分为三类：无模型、基于模型和优化策略。无模型类别采用图像处理方法，而基于模型的方法利用图像生成模型来合成图像。相比之下，优化策略方法旨在找到操作的最佳组合。基于这种分析，我们认为我们的调查增强了选择合适方法和设计新算法所需的理解。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'image augmentation , deep learning , image variation , vicinity distribution
    , data augmentation , computer vision.^†^†journal: Journal of LaTeX Templates'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图像增强，深度学习，图像变化，附近分布，数据增强，计算机视觉。^†^†期刊：LaTeX模板期刊
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Over the recent years, deep learning has achieved significant improvements in
    computer vision based on three key elements, efficient computing devices, powerful
    algorithms, and large volumes of image. A main work over the last decade was designing
    a powerful model with numerous trainable parameters¹¹1https://spectrum.ieee.org/andrew-ng-data-centric-ai.
    The training of such a model requires a large volume of image to achieve competitive
    performance. However, collecting image is frequently an expensive and challenging
    process. Obtaining satisfactory performance with a limited dataset is particularly
    challenging in practical applications, such as medical [[1](#bib.bib1)] and agricultural
    images [[2](#bib.bib2)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度学习在计算机视觉领域取得了显著的进展，这主要基于三个关键要素：高效的计算设备、强大的算法以及大量的图像。过去十年的主要工作之一是设计一个具有大量可训练参数的强大模型¹¹1https://spectrum.ieee.org/andrew-ng-data-centric-ai。训练这样一个模型需要大量的图像才能达到具有竞争力的性能。然而，收集图像通常是一个昂贵且具有挑战性的过程。在实际应用中，例如医学[[1](#bib.bib1)]和农业图像[[2](#bib.bib2)]，在有限数据集下获得令人满意的性能尤其具有挑战性。
- en: To address this issue, image augmentation has been confirmed to be an effective
    and efficient strategy [[3](#bib.bib3), [4](#bib.bib4)]. As listed in Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ A Comprehensive Survey of Image Augmentation Techniques
    for Deep Learning"), many image augmentation methods have been utilized for image
    classification and object detection. Understanding existing image augmentation
    methods is, therefore, crucial in deploying suitable algorithms. Although similar
    surveys have been conducted previously [[5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7)],
    our study is characterized by several essential differences. First, we do not
    confine ourselves to a specific type of image, such as facial images [[8](#bib.bib8)].
    Likewise, we consider many types of image augmentation algorithms, including generative
    adversarial networks [[9](#bib.bib9)] and image mixing [[10](#bib.bib10)]. Third,
    we do not focus on a specific application, such as object detection [[5](#bib.bib5)].
    Conversely, we consider image classification and object detection as two primary
    applications, along with other image and video applications such as segmentation
    and tracking. Finally, unlike two related studies [[6](#bib.bib6), [7](#bib.bib7)],
    our survey encompasses more recent yet effective image augmentation algorithms
    such as instance level multiple image mixing, as well as comprehensive analysis
    of model-based methods. Consequently, this paper encompasses a wider range of
    algorithms that yield a novel informative taxonomy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，图像增强已被证实是一种有效且高效的策略[[3](#bib.bib3), [4](#bib.bib4)]。如表[1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning")所列，许多图像增强方法已被用于图像分类和目标检测。因此，了解现有的图像增强方法对于部署合适的算法至关重要。虽然之前也有类似的调查[[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7)]，但我们的研究具有几个重要的不同点。首先，我们没有限制于特定类型的图像，如面部图像[[8](#bib.bib8)]。同样，我们考虑了许多类型的图像增强算法，包括生成对抗网络[[9](#bib.bib9)]和图像混合[[10](#bib.bib10)]。第三，我们不专注于特定的应用，如目标检测[[5](#bib.bib5)]。相反，我们将图像分类和目标检测视为两个主要应用，并考虑了其他图像和视频应用，如分割和跟踪。最后，与两个相关研究[[6](#bib.bib6),
    [7](#bib.bib7)]不同，我们的调查涵盖了更多近期有效的图像增强算法，如实例级多个图像混合，以及对基于模型的方法的综合分析。因此，本文涵盖了更广泛的算法，形成了一种新颖的信息分类法。
- en: Specifically, we first explain why different image augmentation algorithms have
    been designed and leveraged across diverse applications. More specifically, challenges
    in computer vision and vicinity distribution are introduced to illustrate the
    necessity of image augmentation. By augmenting image data, the aforementioned
    challenges can be mitigated, and the vicinity distribution space can be dilated,
    thereby improving trained model’s generalizability. Based on this analysis, we
    argue that novel image augmentation methods are promising when new challenges
    are recognized. Simultaneously, once a challenge is observed in an application,
    it can be mitigated using an appropriate augmentation method.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们首先解释了为什么不同的图像增强算法在各种应用中被设计和利用。更具体地说，介绍了计算机视觉中的挑战和邻域分布，以说明图像增强的必要性。通过增强图像数据，可以缓解上述挑战，并扩展邻域分布空间，从而提高训练模型的泛化能力。基于这一分析，我们认为当识别出新的挑战时，新颖的图像增强方法是有前景的。同时，一旦在应用中观察到挑战，可以通过适当的增强方法来缓解。
- en: In summary, our study makes the following contributions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的研究做出了以下贡献。
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: We examine *challenges* and *vicinity distribution* to demonstrate the necessity
    of image augmentation for deep learning.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们考察了*挑战*和*邻域分布*，以证明图像增强在深度学习中的必要性。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: We present a comprehensive survey on image augmentation with a novel *informative
    taxonomy* that encompasses a wider range of algorithms.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了一项关于图像增强的全面调查，并引入了一种新颖的*信息分类法*，涵盖了更广泛的算法。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We discuss the current situation and future direction for image augmentation,
    along with three relevant topics: understanding image augmentation, new strategy
    to leverage image augmentation, and feature augmentation.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了图像增强的现状和未来方向，以及三个相关主题：理解图像增强、利用图像增强的新策略和特征增强。
- en: 'The reminder of this paper is organized as follows. The second section introduces
    the research taxonomy. We then present two basic inspiration of image augmentation
    in the third section: the challenges of computer vision tasks and the vicinity
    distribution. Model-free image augmentation is covered in the fourth section,
    whereas the model-based methods are discussed in the fifth section. The process
    of determining an optimal image augmentation is introduced in the six section,
    followed by a discussion section. Concluding remarks are presented in the final
    section.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文的其余部分组织如下。第二部分介绍了研究分类法。第三部分介绍了图像增强的两个基本启示：计算机视觉任务的挑战和邻域分布。无模型图像增强在第四部分中介绍，而基于模型的方法在第五部分中讨论。第六部分介绍了确定最佳图像增强的过程，接着是讨论部分。最后一部分呈现了结论。
- en: '| Paper | Image augmentation method |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 图像增强方法 |'
- en: '| --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AlexNet [[11](#bib.bib11)] | Translate, Flip, Intensity Changing |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet [[11](#bib.bib11)] | 平移、翻转、强度变化 |'
- en: '| ResNet [[12](#bib.bib12)] | Crop, Flip |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| ResNet [[12](#bib.bib12)] | 裁剪、翻转 |'
- en: '| DenseNet [[13](#bib.bib13)] | Flip, Crop, Translate |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet [[13](#bib.bib13)] | 翻转、裁剪、平移 |'
- en: '| MobileNet [[14](#bib.bib14)] | Crop, Elastic distortion |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet [[14](#bib.bib14)] | 裁剪、弹性失真 |'
- en: '| NasNet [[15](#bib.bib15)] | Cutout, Crop, Flip |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| NasNet [[15](#bib.bib15)] | 剪裁、裁剪、翻转 |'
- en: '| ResNeSt [[16](#bib.bib16)] | AutoAugment, Mixup, Crop |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| ResNeSt [[16](#bib.bib16)] | AutoAugment、混合、裁剪 |'
- en: '| DeiT [[17](#bib.bib17)] | AutoAugmentat, RandAugment, Random Erasing, Mixup,
    CutMix |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| DeiT [[17](#bib.bib17)] | AutoAugmentat、RandAugment、随机擦除、混合、CutMix |'
- en: '| Swin Transformer [[18](#bib.bib18)] | RandAugment, Mixup, CutMix, Random
    Erasing |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Swin Transformer [[18](#bib.bib18)] | RandAugment、混合、CutMix、随机擦除 |'
- en: '| Faster R-CNN [[19](#bib.bib19)] | Flip |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN [[19](#bib.bib19)] | 翻转 |'
- en: '| YOLO [[20](#bib.bib20)] | Scale, Translate, Color space |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| YOLO [[20](#bib.bib20)] | 缩放、平移、色彩空间 |'
- en: '| SSD [[21](#bib.bib21)] | Crop, Resize, Flip, Color Space, Distortion |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| SSD [[21](#bib.bib21)] | 裁剪、调整大小、翻转、色彩空间、失真 |'
- en: '| YOLOv4 [[22](#bib.bib22)] | Mosaic, Distortion, Scale, Color space, Crop,
    Flip, Rotate, Random erase, Cutout, Hide-and-Seek, GridMask, Mixup, CutMix, StyleGAN
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv4 [[22](#bib.bib22)] | 马赛克、失真、缩放、色彩空间、裁剪、翻转、旋转、随机擦除、剪裁、藏猫猫、网格掩码、混合、CutMix、StyleGAN
    |'
- en: 'Table 1: Image augmentation algorithms used studies pertaining to image classification
    (up) and object detection (bottom).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 用于图像分类（上）和物体检测（下）的图像增强算法。'
- en: 2 Taxonomy
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 分类法
- en: As shown in Table [2](#S2.T2 "Table 2 ‣ 2 Taxonomy ‣ A Comprehensive Survey
    of Image Augmentation Techniques for Deep Learning"), we classify the image augmentation
    algorithms among three main categories. A model-free approach does not utilize
    a pre-trained model to perform image augmentation, and may use single or multiple
    images. Conversely, model-based algorithms require the image augmentation algorithms
    to generate images using trained models. The augmentation process may unconditional,
    label-conditional, or image-conditional. Finally, Optimizing policy-based algorithms
    determine the optimal operations with suitable parameters from a large parameter
    space. These algorithms can further be sub-categorized into reinforcement learning-based
    and adversarial learning-based method. The former leverages a massive search space
    consisting of diverse operations and their magnitudes, along with an agent to
    find the optimal policy within the search space. In contrast, adversarial learning-based
    methods locate algorithms with the corresponding magnitude to allow the task model
    to have a sufficiently large loss.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S2.T2 "表 2 ‣ 2 分类法 ‣ 深度学习图像增强技术的全面调查")所示，我们将图像增强算法分为三类。无模型的方法不利用预训练模型进行图像增强，可能使用单张或多张图像。相反，基于模型的算法需要图像增强算法使用训练好的模型生成图像。增强过程可以是无条件的、标签条件的或图像条件的。最后，优化策略算法从大参数空间中确定具有合适参数的最佳操作。这些算法进一步可以细分为基于强化学习和基于对抗学习的方法。前者利用由多样化操作及其幅度组成的大量搜索空间，并有一个代理在搜索空间内找到最优策略。相反，对抗学习方法则找到与任务模型具有足够大损失的相应幅度的算法。
- en: '| Categories | Relevant methods |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 相关方法 |'
- en: '| --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model-free | Single-image | Geometrical transformation | translation, rotation,
    flip, scale, elastic distortion. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 无模型 | 单图像 | 几何变换 | 平移、旋转、翻转、缩放、弹性失真。 |'
- en: '| Color image processing | jittering. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 彩色图像处理 | 抖动。 |'
- en: '| Intensity transformation | blurring and adding noise, Hide-and-Seek [[23](#bib.bib23)],
    Cutout [[24](#bib.bib24)], Random Erasing [[25](#bib.bib25)], GridMask [[26](#bib.bib26)].
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 强度变换 | 模糊和添加噪声，Hide-and-Seek [[23](#bib.bib23)], Cutout [[24](#bib.bib24)],
    Random Erasing [[25](#bib.bib25)], GridMask [[26](#bib.bib26)]。 |'
- en: '| Multiple-image | Non-instance-level | SamplePairing [[27](#bib.bib27)], Mixup
    [[28](#bib.bib28)], BC Learning [[29](#bib.bib29)], CutMix [[30](#bib.bib30)],
    Mosaic [[22](#bib.bib22)], AugMix [[31](#bib.bib31)], PuzzleMix [[32](#bib.bib32)],
    Co-Mixup [[33](#bib.bib33)], SuperMix [[34](#bib.bib34)], GridMix [[35](#bib.bib35)].
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 多图像 | 非实例级 | SamplePairing [[27](#bib.bib27)], Mixup [[28](#bib.bib28)],
    BC Learning [[29](#bib.bib29)], CutMix [[30](#bib.bib30)], Mosaic [[22](#bib.bib22)],
    AugMix [[31](#bib.bib31)], PuzzleMix [[32](#bib.bib32)], Co-Mixup [[33](#bib.bib33)],
    SuperMix [[34](#bib.bib34)], GridMix [[35](#bib.bib35)]。 |'
- en: '| Instance-level | CutPas [[36](#bib.bib36)], Scale and Blend [[37](#bib.bib37)],
    Context DA [[38](#bib.bib38)], Simple CutPas [[39](#bib.bib39)], Continuous CutPas
    [[40](#bib.bib40)]. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 实例级 | CutPas [[36](#bib.bib36)], Scale and Blend [[37](#bib.bib37)], Context
    DA [[38](#bib.bib38)], Simple CutPas [[39](#bib.bib39)], Continuous CutPas [[40](#bib.bib40)]。
    |'
- en: '| Model-based | Unconditional | DCGAN [[41](#bib.bib41)], [[42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)] |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 基于模型 | 无条件 | DCGAN [[41](#bib.bib41)], [[42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)] |'
- en: '| Label-conditional | BDA [[45](#bib.bib45)], ImbCGAN [[46](#bib.bib46)], BAGAN
    [[47](#bib.bib47)], DAGAN [[48](#bib.bib48)], MFC-GAN [[49](#bib.bib49)], IDA-GAN
    [[50](#bib.bib50)]. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 标签条件 | BDA [[45](#bib.bib45)], ImbCGAN [[46](#bib.bib46)], BAGAN [[47](#bib.bib47)],
    DAGAN [[48](#bib.bib48)], MFC-GAN [[49](#bib.bib49)], IDA-GAN [[50](#bib.bib50)]。
    |'
- en: '| Image-conditional | Label-preserving | S+U Learning [[51](#bib.bib51)], AugGAN
    [[52](#bib.bib52)], Plant-CGAN [[53](#bib.bib53)], StyleAug [[54](#bib.bib54)],
    Shape bias [[55](#bib.bib55)]. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 图像条件 | 保持标签 | S+U Learning [[51](#bib.bib51)], AugGAN [[52](#bib.bib52)],
    Plant-CGAN [[53](#bib.bib53)], StyleAug [[54](#bib.bib54)], Shape bias [[55](#bib.bib55)]。'
- en: '| Label-changing | EmoGAN [[56](#bib.bib56)], $\delta$-encoder [[57](#bib.bib57)],
    Debiased NN [[58](#bib.bib58)], StyleMix [[59](#bib.bib59)], GAN-MBD [[60](#bib.bib60)],
    SCIT [[2](#bib.bib2)]. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 标签变化 | EmoGAN [[56](#bib.bib56)], $\delta$-encoder [[57](#bib.bib57)], Debiased
    NN [[58](#bib.bib58)], StyleMix [[59](#bib.bib59)], GAN-MBD [[60](#bib.bib60)],
    SCIT [[2](#bib.bib2)]。 |'
- en: '| Optimizing policy-based | Reinforcement learning-based | AutoAugment [[61](#bib.bib61)],
    Fast AA [[62](#bib.bib62)], PBA [[63](#bib.bib63)], Faster AA [[64](#bib.bib64)],
    RandAugment [[65](#bib.bib65)], MADAO [[66](#bib.bib66)], LDA [[67](#bib.bib67)],
    LSSP [[68](#bib.bib68)]. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 基于优化策略 | 基于强化学习 | AutoAugment [[61](#bib.bib61)], Fast AA [[62](#bib.bib62)],
    PBA [[63](#bib.bib63)], Faster AA [[64](#bib.bib64)], RandAugment [[65](#bib.bib65)],
    MADAO [[66](#bib.bib66)], LDA [[67](#bib.bib67)], LSSP [[68](#bib.bib68)]。 |'
- en: '|  | Adversarial learning-based | ADA [[69](#bib.bib69)], CDST-DA [[70](#bib.bib70)],
    AdaTransform [[71](#bib.bib71)], Adversarial AA [[72](#bib.bib72)], IF-DA [[73](#bib.bib73)],
    SPA [[74](#bib.bib74)]. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | 对抗学习基础 | ADA [[69](#bib.bib69)], CDST-DA [[70](#bib.bib70)], AdaTransform
    [[71](#bib.bib71)], Adversarial AA [[72](#bib.bib72)], IF-DA [[73](#bib.bib73)],
    SPA [[74](#bib.bib74)]。 |'
- en: 'Table 2: Taxonomy with relevant methods.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：相关方法的分类。
- en: 3 Motivation to perform image augmentation
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 图像增强的动机
- en: 3.1 Challenges
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 挑战
- en: '| Challenges | Descriptions | Strategies and related studies |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | 描述 | 策略及相关研究 |'
- en: '| --- | --- | --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Images variations | The following basic variations exist in many datasets
    and applications, including illumination, deformation, occlusion, background,
    viewpoint, and multiscale, as shown in Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Challenges
    ‣ 3 Motivation to perform image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"). | Geometrical transformation and
    color image processing improve the majority of the variations. Occlusion: Hide-and-Seek
    [[23](#bib.bib23)], Cutout [[24](#bib.bib24)], Random Erasing [[25](#bib.bib25)],
    GridMask [[26](#bib.bib26)]. Background or context: CutMix [[30](#bib.bib30)],
    Mosaic [[22](#bib.bib22)], CutPas [[36](#bib.bib36)]. Multiscale: Scale and Blend
    [[37](#bib.bib37)], Simple CutPas [[39](#bib.bib39)]. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 图像变化 | 许多数据集和应用中存在以下基本变化，包括光照、变形、遮挡、背景、视角和多尺度，如图 [1](#S3.F1 "图 1 ‣ 3.1 挑战
    ‣ 3 动机以进行图像增强 ‣ 深度学习图像增强技术的全面调查") 所示。几何变换和颜色图像处理改善了大多数变化。遮挡：Hide-and-Seek [[23](#bib.bib23)],
    Cutout [[24](#bib.bib24)], Random Erasing [[25](#bib.bib25)], GridMask [[26](#bib.bib26)]。背景或上下文：CutMix
    [[30](#bib.bib30)], Mosaic [[22](#bib.bib22)], CutPas [[36](#bib.bib36)]。多尺度：Scale
    and Blend [[37](#bib.bib37)], Simple CutPas [[39](#bib.bib39)]。 |'
- en: '| Class imbalance and few images | Number of images vary between classes or
    some classes have only few images. | Reusing instance from minority class is one
    strategy by instance-level operation, Simple Copy-Paste [[39](#bib.bib39)]. Most
    studies attempt to generate images for the minority class: ImbCGAN [[46](#bib.bib46)],
    DAGAN [[48](#bib.bib48)], MFC-GAN [[49](#bib.bib49)], EmoGAN [[56](#bib.bib56)],
    $\delta$-encoder [[57](#bib.bib57)], GAN-MBD [[60](#bib.bib60)], SCIT [[2](#bib.bib2)].
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 类别不平衡和少量图像 | 不同类别之间的图像数量不同，或者某些类别只有少量图像。通过实例级操作重用少数类的实例是一种策略，简单的复制粘贴 [[39](#bib.bib39)]。大多数研究尝试为少数类生成图像：ImbCGAN
    [[46](#bib.bib46)]，DAGAN [[48](#bib.bib48)]，MFC-GAN [[49](#bib.bib49)]，EmoGAN
    [[56](#bib.bib56)]，$\delta$-encoder [[57](#bib.bib57)]，GAN-MBD [[60](#bib.bib60)]，SCIT
    [[2](#bib.bib2)]。 |'
- en: '| Domain shift | Training and testing datasets represent different domains,
    commonly referring to styles. | Changing styles for existing images is a main
    strategy, including S+U Learning [[51](#bib.bib51)], StyleAug [[54](#bib.bib54)],
    Shape bias [[55](#bib.bib55)], Debiased NN [[58](#bib.bib58)], StyleMix [[59](#bib.bib59)].
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 领域迁移 | 训练和测试数据集代表不同的领域，通常指风格。 | 改变现有图像的风格是主要策略，包括 S+U Learning [[51](#bib.bib51)]，StyleAug
    [[54](#bib.bib54)]，Shape bias [[55](#bib.bib55)]，Debiased NN [[58](#bib.bib58)]，StyleMix
    [[59](#bib.bib59)]。 |'
- en: '| Data remembering | Larger models with many learnable parameters tend to remember
    specific data points, which may result in overfitting. | The mechanism is increasing
    dataset size within or between vicinity distributions. Within version assumes
    label-preserving while between version changes labels, such as Mixup [[28](#bib.bib28)],
    AugMix [[31](#bib.bib31)], Co-Mixup [[33](#bib.bib33)]. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 数据记忆 | 较大的模型具有许多可学习参数，往往会记住特定的数据点，这可能导致过拟合。 | 机制是增加数据集大小，无论是在相邻分布内还是之间。内版本假定标签保持不变，而间版本更改标签，例如
    Mixup [[28](#bib.bib28)]，AugMix [[31](#bib.bib31)]，Co-Mixup [[33](#bib.bib33)]。
    |'
- en: 'Table 3: Challenges in computer vision tasks from the perspectives of datasets
    and deep learning models.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：从数据集和深度学习模型的角度来看，计算机视觉任务中的挑战。
- en: '![Refer to caption](img/053ef7edc8840d6270b9df60bf38395d.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/053ef7edc8840d6270b9df60bf38395d.png)'
- en: 'Figure 1: Example of image variations from Class CS231n.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：来自 Class CS231n 的图像变化示例。
- en: Table [3](#S3.T3 "Table 3 ‣ 3.1 Challenges ‣ 3 Motivation to perform image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")
    describes the four types of challenges faced in computer vision tasks. The first
    challenge is *image variation*, resulting from effects such as illumination and
    deformation. Figure [1](#S3.F1 "Figure 1 ‣ 3.1 Challenges ‣ 3 Motivation to perform
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning") illustrates image variations²²2http://cs231n.stanford.edu/. *Class
    imbalance* is another challenge, wherein different objects are observed with different
    frequencies. In medical imaging, abnormal cases often occur with a low probability,
    which is further exacerbated by privacy. When trained with an imbalanced dataset,
    a model assigns a higher probability to the normal case. Besides, class imbalance
    becomes few images from multiple classes to one class. Furthermore, *domain shift*
    represents a challenge where the training and testing datasets exhibit different
    distributions. This is exemplified by the night and day domains in the context
    of automatic driving. Because it is more convenient to collect images during the
    daytime, we may desire to train our model with a daytime dataset but evaluate
    it at the nighttime.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S3.T3 "表 3 ‣ 3.1 挑战 ‣ 3 进行图像增强的动机 ‣ 深度学习图像增强技术的综合调查") 描述了计算机视觉任务中面临的四种挑战。第一个挑战是*图像变化*，这是由照明和变形等效应造成的。图
    [1](#S3.F1 "图 1 ‣ 3.1 挑战 ‣ 3 进行图像增强的动机 ‣ 深度学习图像增强技术的综合调查") 说明了图像变化²²2http://cs231n.stanford.edu/。*类别不平衡*是另一个挑战，其中不同的物体以不同的频率被观察到。在医学成像中，异常情况往往发生的概率较低，这在隐私问题的影响下更加严重。当用不平衡的数据集进行训练时，模型对正常情况分配更高的概率。此外，类别不平衡变成了从多个类别到单一类别的少量图像。此外，*领域迁移*是一个挑战，其中训练和测试数据集展现了不同的分布。这在自动驾驶的昼夜领域中得到了体现。由于白天收集图像更为方便，我们可能希望用白天的数据集训练我们的模型，但在夜晚进行评估。
- en: A new challenge introduced by deep learning is *data remembering*. In general,
    a larger set of learnable parameters requires more data for training, which is
    referred to as structural risk [[75](#bib.bib75)]. With an increase in parameters,
    a deep learning model may remember specific data points with an insufficient number
    of training images, which introduces a generalizability problem in the form of
    overfitting [[76](#bib.bib76)].
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习带来了一个新的挑战，即 *数据记忆*。一般来说，更多的可学习参数需要更多的数据进行训练，这被称为结构风险 [[75](#bib.bib75)]。随着参数的增加，深度学习模型可能会记住特定的数据点，而训练图像数量不足，这会引入过拟合的问题，形成泛化能力的问题
    [[76](#bib.bib76)]。
- en: Fortunately, image augmentation methods can mitigate these challenges and improve
    model generalizability by increasing the number and variance of images in the
    training dataset. To utilize an image augmentation algorithm efficiently, it is
    crucial to understand the challenges of application and apply suitable methods.
    This study was conducted to provide a survey that enhances the understanding of
    a wide range of image augmentation algorithms.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，图像增强方法可以通过增加训练数据集中图像的数量和多样性来缓解这些挑战并提高模型的泛化能力。为了有效利用图像增强算法，关键是要理解应用的挑战并应用合适的方法。本研究旨在提供一项调查，增强对各种图像增强算法的理解。
- en: 3.2 Vicinity distribution
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 邻域分布
- en: 'In a supervised learning paradigm, we expect to find a function $f\in\mathcal{F}$
    that reflects the relationship between an input $x$ and target $y$ in a joint
    distribution $P(x,y)$. To learn $f$, a loss $l$ is defined to reduce the discrepancy
    between the prediction $f(x)$ and actual target $y$ for all examples in $P(x,y)$.
    We can then optimize $f$ by minimizing $l$ over $P(x,y)$, which is known as the
    expected risk [[75](#bib.bib75)] and can be formulated as $R(f)=\int{l(f(x),y)dP(x,y)}$.
    However, $P(x,y)$ is unknown in most applications [[77](#bib.bib77)]. Alternatively,
    we may use the empirical distribution $P_{e}(x,y)$ to approximate $P(x,y)$. In
    this case, the observed dataset $\mathcal{D}={(x_{i},y_{i})}_{i=1}^{n}$ is considered
    to be the empirical distribution, where $(x_{i},y_{i})$ is in $P_{e}(x,y)$ for
    a given $i$:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习范式中，我们期望找到一个函数 $f\in\mathcal{F}$，该函数反映了输入 $x$ 和目标 $y$ 在联合分布 $P(x,y)$ 中的关系。为了学习
    $f$，定义了一个损失函数 $l$，以减少预测 $f(x)$ 和实际目标 $y$ 之间的差异，适用于 $P(x,y)$ 中的所有示例。然后，我们可以通过最小化
    $P(x,y)$ 上的 $l$ 来优化 $f$，这被称为期望风险 [[75](#bib.bib75)]，可以表示为 $R(f)=\int{l(f(x),y)dP(x,y)}$。然而，大多数应用中
    $P(x,y)$ 是未知的 [[77](#bib.bib77)]。作为替代，我们可以使用经验分布 $P_{e}(x,y)$ 来近似 $P(x,y)$。在这种情况下，观察到的数据集
    $\mathcal{D}={(x_{i},y_{i})}_{i=1}^{n}$ 被认为是经验分布，其中 $(x_{i},y_{i})$ 对于给定的 $i$
    在 $P_{e}(x,y)$ 中：
- en: '|  | $P_{e}(x,y)=\frac{1}{n}\sum_{i=1}^{n}\delta((x=x_{i},y=y_{i})],$ |  |
    (1) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{e}(x,y)=\frac{1}{n}\sum_{i=1}^{n}\delta((x=x_{i},y=y_{i})],$ |  |
    (1) |'
- en: 'where $\delta(x,y)$ is a Dirac mass function centered at point $(x_{i},y_{i})$,
    assuming that all masses in the probability distribution cluster around a single
    point [[78](#bib.bib78)]. Another natural notion for approximating $P(x,y)$ is
    the vicinity distribution $P_{v}(x,y)$, which replaces the Dirac mass function
    with an estimate of the density in the vicinity of point $(x_{i},y_{i})$ [[79](#bib.bib79)]:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\delta(x,y)$ 是一个以 $(x_{i},y_{i})$ 为中心的狄拉克质量函数，假设概率分布中的所有质量集中在一个点 [[78](#bib.bib78)]。另一种自然的近似
    $P(x,y)$ 的方法是邻域分布 $P_{v}(x,y)$，它用点 $(x_{i},y_{i})$ 周围的密度估计来替代狄拉克质量函数 [[79](#bib.bib79)]：
- en: '|  | $P_{v}(x,y)=\frac{1}{n}\sum_{i=1}^{n}\delta_{v}(x=x_{i},y=y_{i}),$ |  |
    (2) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{v}(x,y)=\frac{1}{n}\sum_{i=1}^{n}\delta_{v}(x=x_{i},y=y_{i}),$ |  |
    (2) |'
- en: where $\delta_{v}$ is the vicinity point set of $(x_{i},y_{i})$ in $\mathcal{D}$.
    The vicinity distribution assumes that $P(x,y)$ is smooth around any point $(x_{i},y_{i})$
    [[77](#bib.bib77)]. In $P_{v}(x,y)$, models are less prone to memorizing all data
    points, and thus tend to yield higher performance in the testing process. One
    way to achieve vicinity distribution is to apply image augmentation, by which
    an original data point $(x_{i},y_{i})$ can be moved within its vicinity. For example,
    the Gaussian vicinity distribution is equivalent to the addition of Gaussian noise
    to an image [[79](#bib.bib79)].
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\delta_{v}$ 是 $\mathcal{D}$ 中 $(x_{i},y_{i})$ 的邻域点集。邻域分布假设 $P(x,y)$ 在任何点
    $(x_{i},y_{i})$ 周围是光滑的 [[77](#bib.bib77)]。在 $P_{v}(x,y)$ 中，模型不容易记住所有数据点，因此在测试过程中往往能获得更高的性能。实现邻域分布的一种方法是应用图像增强，通过这种方式，原始数据点
    $(x_{i},y_{i})$ 可以在其邻域内移动。例如，高斯邻域分布相当于在图像中添加高斯噪声 [[79](#bib.bib79)]。
- en: 4 Model-free image augmentation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 无模型图像增强
- en: Image processing methods, such as geometric transformation and pixel-level manipulation,
    can be leveraged for augmentation purposes [[6](#bib.bib6), [7](#bib.bib7)]. In
    this study, we refer to model-free image augmentation as contrasting model-based
    image augmentation. The model-free approach consists of single- and multi-image
    branches. As suggested by the names, the former produces augmented images from
    a single image, whereas the latter generates output from multiple images.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理方法，如几何变换和像素级操作，可以用于增强目的 [[6](#bib.bib6), [7](#bib.bib7)]。在本研究中，我们将无模型图像增强称为与基于模型的图像增强对比。无模型方法包括单图像和多图像分支。如名称所示，前者从单个图像生成增强图像，而后者则从多个图像生成输出。
- en: 4.1 Single-image augmentation
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 单图像增强
- en: From the vicinity distribution, single-image augmentation (SiA) aims to fluctuate
    the data points in the training dataset and increase distribution density. In
    general, SiA leverages traditional image processing, which is simple to understand
    and execute. SiA methods include geometric transformations, color image processing,
    and intensity transformations. Geometric transformation tries to modify the spatial
    relationship between pixels [[80](#bib.bib80)], including affine transformation
    and elastic deformation, while color image processing aims to vary the color of
    an input image. In contrast, the last one is advocated to change parts of the
    images and has recently received more attention.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从邻域分布来看，单图像增强（SiA）的目标是波动训练数据集中的数据点并增加分布密度。一般来说，SiA 利用传统的图像处理方法，这些方法易于理解和执行。SiA
    方法包括几何变换、彩色图像处理和强度变换。几何变换试图修改像素之间的空间关系 [[80](#bib.bib80)]，包括仿射变换和弹性变形，而彩色图像处理旨在改变输入图像的颜色。与之相比，最后一种方法倡导改变图像的部分内容，近年来受到了更多关注。
- en: 4.1.1 Geometric transformation
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 几何变换
- en: 'Objects in naturally captured images can appear in many variations. Geometric
    transformations can be employed to increase this variability. For instance, *translation*
    provides a way to augment objects’ position. Furthermore, an image can be *rotated*,
    changing the perspectives of objects. The angle of rotation should be carefully
    considered to ensure the preservation of appropriate labels. Likewise, a *flip*
    can be executed horizontally or vertically, according to the characteristics of
    the training and testing datasets. For instance, the Cityscapes [[81](#bib.bib81)]
    dataset can be augmented horizontally but not vertically. In addition, objects
    can be magnified or shrunk via *scaling* to mimic multiscale variation. Finally,
    the *elastic distortion* can alter the shape or posture of an object. Among these
    methods, flips have been commonly utilized throughout many studies over the last
    decade for various computer vision tasks, such as image classification [[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)], object detection [[82](#bib.bib82), [83](#bib.bib83)],
    and image translation [[84](#bib.bib84), [85](#bib.bib85)]. Two factors must be
    considered when using these methods: the magnitude of the operation to preserve
    label identity and variations in the dataset.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 自然捕捉的图像中的对象可能会出现许多变化。几何变换可以用来增加这种变化。例如，*平移* 提供了一种增强对象位置的方式。此外，图像可以被 *旋转*，改变对象的视角。旋转角度应仔细考虑，以确保适当标签的保留。同样，*翻转*
    可以根据训练和测试数据集的特征水平或垂直执行。例如，Cityscapes [[81](#bib.bib81)] 数据集可以进行水平增强但不能垂直增强。此外，通过
    *缩放* 可以放大或缩小对象，以模拟多尺度变化。最后，*弹性扭曲* 可以改变对象的形状或姿态。在这些方法中，翻转在过去十年中已被广泛应用于各种计算机视觉任务，如图像分类
    [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]、对象检测 [[82](#bib.bib82),
    [83](#bib.bib83)] 和图像翻译 [[84](#bib.bib84), [85](#bib.bib85)]。使用这些方法时必须考虑两个因素：操作的幅度以保留标签身份和数据集中的变化。
- en: 4.1.2 Color image processing
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 彩色图像处理
- en: Unlike greyscale images, color images consist of three channels. Color image
    processing for augmentation assumes that the training and testing dataset distributions
    fluctuate in terms of colors, such as contrast. Although color image processing
    yields superior performance, it is rarely used because the color variations between
    the training and testing datasets are small. However, one interesting point is
    the use of robust features for contrast learning [[86](#bib.bib86)] via color
    image processing, which represents a case of task-agnostic learning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 与灰度图像不同，彩色图像包含三个通道。彩色图像处理用于增强假设训练和测试数据集的颜色分布（如对比度）会波动。虽然彩色图像处理可以获得更好的性能，但由于训练和测试数据集之间的颜色变化较小，使用较少。然而，一个有趣的点是通过彩色图像处理用于对比度学习的鲁棒特征
    [[86](#bib.bib86)]，这代表了一种任务无关的学习情况。
- en: 4.1.3 Intensity transformation
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 强度变换
- en: '| Paper | Year | Highlight |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 年份 | 亮点 |'
- en: '| --- | --- | --- |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Hide-and-Seek [[23](#bib.bib23)] | 2017 | Split an image into patches that
    are randomly blocked. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 捉迷藏 [[23](#bib.bib23)] | 2017 | 将图像拆分为随机阻挡的补丁。 |'
- en: '| Cutout [[24](#bib.bib24)] | 2017 | Apply a fixed-size mask to a random location
    for each image. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Cutout [[24](#bib.bib24)] | 2017 | 对每张图像应用一个固定大小的掩码，位置随机。 |'
- en: '| Random Erasing [[25](#bib.bib25)] | 2020 | Randomly select a rectangular
    region and displace its pixels with random values. Figure [3](#S4.F3 "Figure 3
    ‣ 4.1.3 Intensity transformation ‣ 4.1 Single-image augmentation ‣ 4 Model-free
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning"). |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 随机擦除 [[25](#bib.bib25)] | 2020 | 随机选择一个矩形区域，并用随机值替换其像素。图 [3](#S4.F3 "图 3
    ‣ 4.1.3 强度变换 ‣ 4.1 单图像增强 ‣ 4 无模型图像增强 ‣ 深度学习图像增强技术的综合调查")。 |'
- en: '| GridMask [[26](#bib.bib26)] | 2020 | Apply multiscale grid masks to an image
    to mimic occlusions. Figure [4](#S4.F4 "Figure 4 ‣ 4.1.3 Intensity transformation
    ‣ 4.1 Single-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GridMask [[26](#bib.bib26)] | 2020 | 对图像应用多尺度网格掩码，以模拟遮挡。图 [4](#S4.F4 "图 4
    ‣ 4.1.3 强度变换 ‣ 4.1 单图像增强 ‣ 4 无模型图像增强 ‣ 深度学习图像增强技术的综合调查")。 |'
- en: 'Table 4: Studies focusing upon intensity transformations. Each study is highlighted
    with its corresponding figure if available.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：关注强度变换的研究。每项研究均附有对应的图例（如有）。
- en: Unlike geometric transformations and color image processing, intensity transformations
    entail changes at the pixel or patch levels. Random noise, such as Gaussian noise,
    is one of the simplest intensity transformation algorithms [[75](#bib.bib75)].
    The classical methods leverage random noise independently at the pixel level;
    however, the patch level has recently exhibited significant improvement for deep
    learning algorithms [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)].
    Studies pertaining to intensity transformations are listed in Table [4](#S4.T4
    "Table 4 ‣ 4.1.3 Intensity transformation ‣ 4.1 Single-image augmentation ‣ 4
    Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques
    for Deep Learning"). The underlying concept is that the changes push the model
    to learn robust features by avoiding trivial solutions [[76](#bib.bib76)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与几何变换和颜色图像处理不同，强度变换涉及像素或补丁级别的变化。随机噪声，如高斯噪声，是最简单的强度变换算法之一 [[75](#bib.bib75)]。经典方法在像素级别独立利用随机噪声；然而，最近补丁级别在深度学习算法中表现出显著改进
    [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]。关于强度变换的研究列在表
    [4](#S4.T4 "表 4 ‣ 4.1.3 强度变换 ‣ 4.1 单图像增强 ‣ 4 无模型图像增强 ‣ 深度学习图像增强技术的综合调查") 中。其基本概念是这些变化促使模型通过避免琐碎的解决方案来学习鲁棒特征
    [[76](#bib.bib76)]。
- en: 'Cutout [[24](#bib.bib24)] randomly masks the most significant area with a finding
    mechanism to mimic occlusion. However, the most important aspect is cost. Hide-and-Seek
    [[23](#bib.bib23)] directly blocks part of the image with the objective of obscuring
    the most significant area through many iterations of a random process, which is
    simple and fast. Figure [2](#S4.F2 "Figure 2 ‣ 4.1.3 Intensity transformation
    ‣ 4.1 Single-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") shows that images
    are divided into $s\times s$ patches, and each patch is randomly blocked. One
    disadvantage is that the identical size of each patch yields the same level of
    occlusion. To address this issue, Random Erasing [[25](#bib.bib25)] has been employed
    with three random values: the size of the occluded area, height-to-width ratio,
    and top-left corner of the area. Figure [3](#S4.F3 "Figure 3 ‣ 4.1.3 Intensity
    transformation ‣ 4.1 Single-image augmentation ‣ 4 Model-free image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")
    demonstrates some examples of Random Erasing for three computer vision tasks.
    Additionally, this method can be leveraged in image- and object-aware conditions,
    thereby simplifying object detection.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Cutout [[24](#bib.bib24)] 随机遮罩最重要的区域，使用发现机制模拟遮挡。然而，最重要的方面是成本。Hide-and-Seek [[23](#bib.bib23)]
    直接阻挡图像的一部分，旨在通过多次随机过程遮挡最重要的区域，这种方法简单且快速。图 [2](#S4.F2 "图2 ‣ 4.1.3 强度变换 ‣ 4.1 单图像增强
    ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术综合调查") 显示图像被分割成 $s\times s$ 的补丁，每个补丁被随机遮挡。一个缺点是每个补丁的相同大小导致相同程度的遮挡。为了解决这个问题，采用了
    Random Erasing [[25](#bib.bib25)]，使用三个随机值：遮挡区域的大小、高宽比和区域的左上角。图 [3](#S4.F3 "图3
    ‣ 4.1.3 强度变换 ‣ 4.1 单图像增强 ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术综合调查") 展示了 Random Erasing 在三项计算机视觉任务中的一些示例。此外，这种方法可以在图像和对象感知条件下使用，从而简化对象检测。
- en: GridMask aims to balance deleting and reservation, with the objective of blocking
    certain important areas of an object while preserving others to mimic real occlusion.
    To achieve this, GridMask uses a set of predefined masks, as opposed to a single
    mask [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]. As illustrated in
    Figure [4](#S4.F4 "Figure 4 ‣ 4.1.3 Intensity transformation ‣ 4.1 Single-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"), the generated mask is obtained from
    four values, denoting the width and height of every grid and the vertical and
    horizontal distance of the neighboring grid mask. By adjusting these four values,
    grid masks of different sizes and heigh-width ratios can be obtained. Under these
    conditions, GridMask achieves a better balance between deleting and reservation,
    and a preliminary experiment suggests that it has a lower chance of producing
    failure cases than Cutout [[24](#bib.bib24)] and Hide-and-See [[23](#bib.bib23)].
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: GridMask 旨在平衡删除和保留，目的是遮挡对象的某些重要区域，同时保留其他区域以模拟真实遮挡。为此，GridMask 使用一组预定义的遮罩，而不是单一遮罩
    [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。如图 [4](#S4.F4 "图4 ‣ 4.1.3
    强度变换 ‣ 4.1 单图像增强 ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术综合调查") 所示，生成的遮罩由四个值获得，表示每个网格的宽度和高度以及相邻网格遮罩的垂直和水平距离。通过调整这四个值，可以获得不同大小和高宽比的网格遮罩。在这些条件下，GridMask
    实现了删除和保留之间的更好平衡，并且初步实验表明它比 Cutout [[24](#bib.bib24)] 和 Hide-and-Seek [[23](#bib.bib23)]
    更少产生失败案例。
- en: '![Refer to caption](img/b3426a6750cb816dc7c8eb1dc14394b8.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b3426a6750cb816dc7c8eb1dc14394b8.png)'
- en: 'Figure 2: Hide-and-Seek [[23](#bib.bib23)] carries out image augmentation where
    one image is split into several patches, and each patch is randomly blocked with
    a specified probability.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Hide-and-Seek [[23](#bib.bib23)] 进行图像增强，将一张图像分割成若干个补丁，每个补丁都以指定的概率随机遮挡。
- en: '![Refer to caption](img/5ebf46b9e1002960de28372cc9a04ae5.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5ebf46b9e1002960de28372cc9a04ae5.png)'
- en: 'Figure 3: Examples of Random Erasing [[25](#bib.bib25)].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Random Erasing [[25](#bib.bib25)] 的示例。
- en: '![Refer to caption](img/12d84e8f3b0c590a7ebc7903118656fe.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/12d84e8f3b0c590a7ebc7903118656fe.png)'
- en: 'Figure 4: GridMask [[26](#bib.bib26)] and its setting.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：GridMask [[26](#bib.bib26)] 及其设置。
- en: 4.2 Multiple-image augmentation
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 多图像增强
- en: Multiple-image augmentation (MiA) algorithms are executed on more than one image.
    These methods can further be categorized as instance- and non-instance-level.
    Because one image may include more than one instance, we can mask instances and
    use them independently. Unlike SiA, MiA requires algorithms to merge multiple
    input instances.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 多图像增强（MiA）算法在多于一张图像上执行。这些方法可以进一步分类为实例级和非实例级。因为一张图像可能包含多个实例，我们可以对实例进行掩膜并独立使用。与
    SiA 不同，MiA 需要算法合并多个输入实例。 |
- en: '| Paper | Year | Highlight |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 年份 | 亮点 |'
- en: '| --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SamplePairing [[27](#bib.bib27)] | 2018 | Combine two images with a single
    label. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 样本配对 [[27](#bib.bib27)] | 2018 | 将两个图像与单一标签结合。 |'
- en: '| Mixup [[28](#bib.bib28)] | 2018 | Linearly fuse images and their labels.
    Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2 Multiple-image augmentation
    ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning"). |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 混合 [[28](#bib.bib28)] | 2018 | 线性融合图像及其标签。图 [5](#S4.F5 "图 5 ‣ 4.2.1 非实例级
    ‣ 4.2 多图像增强 ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术综述")。 |'
- en: '| BC Learning [[29](#bib.bib29)] | 2018 | Combine two images and their labels.
    Treat the image as a waveform, and declare that image mixing makes sense for machines.
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| BC 学习 [[29](#bib.bib29)] | 2018 | 结合两个图像及其标签。将图像视作波形，并声明图像混合对机器有意义。 |'
- en: '| CutMix [[30](#bib.bib30)] | 2019 | Spatially fuse two images and linearly
    fuse the labels. Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2
    Multiple-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CutMix [[30](#bib.bib30)] | 2019 | 空间上融合两个图像，并线性融合标签。图 [5](#S4.F5 "图 5 ‣
    4.2.1 非实例级 ‣ 4.2 多图像增强 ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术综述")。 |'
- en: '| Mosaic [[22](#bib.bib22)] | 2020 | Spatially mix four images and their annotations,
    thereby enriching the context for each class. |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 马赛克 [[22](#bib.bib22)] | 2020 | 空间上混合四个图像及其注释，从而丰富每个类别的上下文。 |'
- en: '| AugMix [[31](#bib.bib31)] | 2020 | One image undergoes several basic augmentations,
    and the results are fused with the original image. |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| AugMix [[31](#bib.bib31)] | 2020 | 一张图像经历几个基本增强，然后与原图像融合。 |'
- en: '| PuzzleMix [[32](#bib.bib32)] | 2020 | Optimize a mask for fusing two images
    to utilize the salient information and underlying statistics. |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 拼图混合 [[32](#bib.bib32)] | 2020 | 优化掩码以融合两个图像，从而利用显著信息和潜在统计。 |'
- en: '| Co-Mixup [[33](#bib.bib33)] | 2021 | Maximize the salient signal of input
    images and diversity among the augmented images. |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Co-Mixup [[33](#bib.bib33)] | 2021 | 最大化输入图像的显著信号和增强图像之间的多样性。 |'
- en: '| SuperMix [[34](#bib.bib34)] | 2021 | Optimize a mask for fusing two images
    to exploit the salient region with the Newton iterative method, 65x faster than
    gradient descent. |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 超级混合 [[34](#bib.bib34)] | 2021 | 使用牛顿迭代方法优化掩码以融合两个图像，从而利用显著区域，比梯度下降快65倍。
    |'
- en: '| GridMix [[35](#bib.bib35)] | 2021 | Split two images into patches, spatially
    fuse the patches, and linearly merge the annotation. |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 网格混合 [[35](#bib.bib35)] | 2021 | 将两个图像拆分为补丁，空间上融合这些补丁，并线性合并注释。 |'
- en: '| Cut, Paste and Learn [[36](#bib.bib36)] | 2017 | Cut object instances and
    paste them onto random backgrounds. Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Instance-level
    ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 剪切、粘贴和学习 [[36](#bib.bib36)] | 2017 | 剪切对象实例并将其粘贴到随机背景中。图 [6](#S4.F6 "图 6
    ‣ 4.2.2 实例级 ‣ 4.2 多图像增强 ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术综述")。 |'
- en: '| Scale and Blend [[37](#bib.bib37)] | 2017 | Cut and scale object instances,
    and blend them in meaningful locations. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 缩放和混合 [[37](#bib.bib37)] | 2017 | 剪切和缩放对象实例，并在有意义的位置混合它们。 |'
- en: '| Context DA [[38](#bib.bib38)] | 2018 | Combine object instances using context
    guidance to obtain meaningful images. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 上下文数据增强 [[38](#bib.bib38)] | 2018 | 使用上下文指导结合对象实例以获得有意义的图像。 |'
- en: '| Simple Copy-Paste [[39](#bib.bib39)] | 2021 | Randomly paste object instances
    to images with large-scale jittering. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 简单复制-粘贴 [[39](#bib.bib39)] | 2021 | 随机将对象实例粘贴到图像中，并进行大规模抖动。 |'
- en: '| Continuous Copy-Paste [[40](#bib.bib40)] | 2021 | Deploy Cut, Paste and Learn
    to videos. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 连续复制-粘贴 [[40](#bib.bib40)] | 2021 | 部署剪切、粘贴和学习到视频中。 |'
- en: 'Table 5: Studies related to multiple-image augmentation, divided into non-instance-
    (up) and instance-level (bottom).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 与多图像增强相关的研究，分为非实例级（上）和实例级（下）。 |'
- en: 4.2.1 Non-instance-level
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 非实例级 |
- en: '![Refer to caption](img/13beaeb3ddc6364b3b52518ef1d68bf4.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13beaeb3ddc6364b3b52518ef1d68bf4.png)'
- en: 'Figure 5: Comparison of non-instance-level multiple-image algorithms [[30](#bib.bib30)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 非实例级别的多图像算法比较 [[30](#bib.bib30)]。'
- en: In the context of MiA algorithms, the non-instance-level approach adopts and
    fuses the images. Studies pertaining to this concept are listed in Table [5](#S4.T5
    "Table 5 ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning"). One
    of the simplest methods is to compute the average value of each pixel. In Pairing
    Samples [[27](#bib.bib27)], two images are fused to produce an augmented image
    with a label from one source image. This assumption is generalized in Mixup [[28](#bib.bib28)],
    where the labels are also fused. Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level
    ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") illustrates the difference
    between Pairing Samples and Mixup. Mathematically, $\tilde{x}=\lambda x_{i}+(1-\lambda)x_{j}$
    and $\tilde{y}=\lambda y_{i}+(1-\lambda)y_{j}$, where $x_{i}$ and $x_{j}$ are
    two images, $y_{i}$ and $y_{j}$ are the corresponding one-hot labels, and $\tilde{x}$
    and $\tilde{y}$ denote the generated image and label, respectively. By adjusting
    $0\leq\lambda\leq 1$, many images with different labels can be created, thereby
    smoothing out the gap between the two labels in the augmented images. Although
    Pairing Samples and Mixup produce satisfactory results, the fused images are not
    reasonable for humans. Accordingly, these fused images have been declared to make
    sense for machines from the perspective of a waveform [[29](#bib.bib29)]. In addition,
    vicinity distribution can also be utilized to understand this situation. To be
    more specific, changing image variations yet maintaining the label can be regarded
    a deviation in the vicinity distribution space of a specific label, whereas image
    fusion can be considered as an interpolation between the vicinity distribution
    of two labels [[28](#bib.bib28)].
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MiA 算法的背景下，非实例级别的方法采用并融合了图像。相关的研究列在表 [5](#S4.T5 "表 5 ‣ 4.2 多图像增强 ‣ 4 模型无关图像增强
    ‣ 深度学习图像增强技术的综合调查") 中。最简单的方法之一是计算每个像素的平均值。在配对样本 [[27](#bib.bib27)] 中，将两张图像融合生成一个增强图像，并从其中一张源图像中获得标签。这个假设在
    Mixup [[28](#bib.bib28)] 中得到了推广，在那里标签也被融合。图 [5](#S4.F5 "图 5 ‣ 4.2.1 非实例级别 ‣ 4.2
    多图像增强 ‣ 4 模型无关图像增强 ‣ 深度学习图像增强技术的综合调查") 展示了配对样本和 Mixup 之间的差异。在数学上，$\tilde{x}=\lambda
    x_{i}+(1-\lambda)x_{j}$ 和 $\tilde{y}=\lambda y_{i}+(1-\lambda)y_{j}$，其中 $x_{i}$
    和 $x_{j}$ 是两张图像，$y_{i}$ 和 $y_{j}$ 是对应的一热标签，而 $\tilde{x}$ 和 $\tilde{y}$ 分别表示生成的图像和标签。通过调整
    $0\leq\lambda\leq 1$，可以创建许多具有不同标签的图像，从而平滑增强图像中两个标签之间的差距。尽管配对样本和 Mixup 产生了令人满意的结果，但这些融合图像对人类而言并不合理。因此，这些融合图像从波形的角度来看被认为对机器有意义
    [[29](#bib.bib29)]。此外，邻域分布也可以用于理解这种情况。更具体地说，改变图像变化但保持标签可以看作是在特定标签的邻域分布空间中的偏差，而图像融合可以被视为两个标签的邻域分布之间的插值
    [[28](#bib.bib28)]。
- en: In contrast to BC Learning [[29](#bib.bib29)], CutMix [[30](#bib.bib30)] spatially
    merges images to obtain results that are interpretable by humans. The last picture
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2 Multiple-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning") illustrates the method’s underlying
    strategy, wherein the merged image consists of two source images spatially, and
    its label is obtained from the ratio of certain pixels between two images. Although
    multiple-image augmentation generally utilizes two images, more images can be
    used. For example, Mosaic [[22](#bib.bib22)] employs four images wherein the number
    of objects in one image is increased, thus significantly reducing the need for
    a large mini-batch size for dense prediction. AugMix [[31](#bib.bib31)] randomly
    applies basic multiple methods of image augmentation, and the results are adopted
    to merge with the original image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 与 BC Learning [[29](#bib.bib29)] 相对，CutMix [[30](#bib.bib30)] 在空间上融合图像，以获得人类可解释的结果。图
    [5](#S4.F5 "Figure 5 ‣ 4.2.1 Non-instance-level ‣ 4.2 Multiple-image augmentation
    ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning") 中的最后一张图片展示了该方法的基本策略，其中融合图像由两个源图像在空间上组成，其标签是通过两个图像之间某些像素的比例获得的。尽管多图像增强通常利用两张图像，但也可以使用更多图像。例如，Mosaic
    [[22](#bib.bib22)] 使用四张图像，其中一张图像中的物体数量增加，从而显著减少了对大批量数据的需求以进行密集预测。AugMix [[31](#bib.bib31)]
    随机应用基础的多重图像增强方法，并将结果与原始图像融合。
- en: Non-instance-level image augmentation has extensions similar to those of intensity
    transformations. To account for the most important area, PuzzleMix [[32](#bib.bib32)]
    discriminates the foreground from the background, and mixes important information
    within the foreground. Further, salient areas from multiple input images are maximized
    to synthesize each augmented image [[33](#bib.bib33)], simultaneously maximizing
    the diversity among the augmented images. To quickly locate dominant regions,
    SuperMix [[34](#bib.bib34)] employs a variant of the Newton iterative method.
    As in Hide-and-Seek [[23](#bib.bib23)], GridMix [[35](#bib.bib35)] divides images
    into fixed-size grids, and each patch of the output image is randomly taken from
    the corresponding patches of two input images. Through this analysis, we believe
    that GridMask [[87](#bib.bib87)] can be adapted to fuse image pairs with changeable
    sizes.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 实例级别图像增强的扩展类似于强度变换的扩展。为了考虑最重要的区域，PuzzleMix [[32](#bib.bib32)] 区分前景和背景，并在前景中混合重要信息。此外，通过最大化来自多个输入图像的显著区域来合成每个增强图像
    [[33](#bib.bib33)]，同时最大化增强图像之间的多样性。为了快速定位主要区域，SuperMix [[34](#bib.bib34)] 使用了一种牛顿迭代方法的变体。与
    Hide-and-Seek [[23](#bib.bib23)] 类似，GridMix [[35](#bib.bib35)] 将图像划分为固定大小的网格，每个输出图像的补丁随机取自两个输入图像的对应补丁。通过这种分析，我们相信
    GridMask [[87](#bib.bib87)] 可以适应于融合大小可变的图像对。
- en: 4.2.2 Instance-level
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 实例级别
- en: Whereas the non-instance-level approach employs images directly, the instance-level
    approach leverages instances masked from images. Related studies are listed in
    the second part of Table [5](#S4.T5 "Table 5 ‣ 4.2 Multiple-image augmentation
    ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning"). The instance-level approach comprises two main
    steps. As shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.2.2 Instance-level ‣ 4.2 Multiple-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"), the first step involves cutting instances
    from source images given a semantic mask, and obtaining clean background senses.
    Next, the obtained instances and background are merged. Cut, Paste and Learn [[36](#bib.bib36)]
    is an early instance-level method, wherein local artifacts are noticed after pasting
    instances to the background. Because local region-based features are important
    for object detection, various blending modes are employed to reduce local artifacts.
    With the exception of boundaries, the instance scale and position are not trivial,
    as objects may be multiscale and recognizable with the help of their contexts,
    as addressed in [[37](#bib.bib37)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接使用图像的非实例级方法不同，实例级方法利用从图像中遮蔽的实例。相关研究列在表[5](#S4.T5 "Table 5 ‣ 4.2 Multiple-image
    augmentation ‣ 4 Model-free image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning")的第二部分中。实例级方法包括两个主要步骤。如图[6](#S4.F6 "Figure
    6 ‣ 4.2.2 Instance-level ‣ 4.2 Multiple-image augmentation ‣ 4 Model-free image
    augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep
    Learning")所示，第一个步骤是根据语义掩码从源图像中剪切实例，并获得干净的背景感知。接下来，将获得的实例和背景进行合并。Cut, Paste and
    Learn [[36](#bib.bib36)]是一种早期的实例级方法，其中实例粘贴到背景后会出现局部伪影。由于局部区域特征对目标检测非常重要，因此采用了各种混合模式来减少局部伪影。除了边界外，实例的尺度和位置并非微不足道，因为对象可能是多尺度的，并且可以借助其上下文进行识别，如[[37](#bib.bib37)]所述。
- en: '![Refer to caption](img/945cf9db4c69ef9791c687032aa6dcd8.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/945cf9db4c69ef9791c687032aa6dcd8.png)'
- en: 'Figure 6: Cut, Paste and Learn in training and testing process [[36](#bib.bib36)].'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：训练和测试过程中的Cut, Paste and Learn [[36](#bib.bib36)]。
- en: Interestingly, instance-level image augmentation can mitigate the challenges
    posed by class imbalance. By repurposing rare instances, the number of images
    in the corresponding class increases. Simple Copy-Paste [[39](#bib.bib39)] indicates
    that the instance level enables strong image augmentation methods, for instance,
    segmentation. While it is based on Copy, Paste and Learn, Simple Copy-Paste differs
    in two characteristics. First, the background image is randomly selected from
    the dataset, and subjected to random scale jittering and horizontal flipping.
    Second, large-scale jittering is leveraged to obtain more significant performance.
    The copy-paste concept has also been utilized for time-series tasks [[40](#bib.bib40)]
    such as tracking.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，实例级图像增强可以缓解类不平衡带来的挑战。通过重新利用稀有实例，相应类别的图像数量会增加。Simple Copy-Paste [[39](#bib.bib39)]
    表明实例级可以实现强大的图像增强方法，例如分割。虽然它基于Copy, Paste and Learn，但Simple Copy-Paste在两个特征上有所不同。首先，背景图像从数据集中随机选择，并进行随机尺度抖动和水平翻转。其次，利用大规模抖动以获得更显著的性能。复制粘贴概念也被应用于时间序列任务
    [[40](#bib.bib40)]，如跟踪。
- en: 5 Model-based image augmentation
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基于模型的图像增强
- en: 'A model must be pre-trained in model-based image augmentation to generate augmented
    images. The present study classifies this process among three categories, according
    to the conditions to generate images: unconditional, label-conditional, and image-conditional.
    Table [6](#S5.T6 "Table 6 ‣ 5 Model-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") provides information
    regarding appropriate studies.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 模型必须经过预训练，以进行基于模型的图像增强，从而生成增强后的图像。本研究将这一过程根据生成图像的条件分为三类：无条件、标签条件和图像条件。表[6](#S5.T6
    "Table 6 ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning")提供了有关适当研究的信息。
- en: '| Paper | Year | Highlight |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 年份 | 亮点 |'
- en: '| BDA [[45](#bib.bib45)] | 2017 | Use CGAN to generate images optimized by
    a Monte Carlo EM algorithm. Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Label-conditional
    image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of
    Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| BDA [[45](#bib.bib45)] | 2017 | 使用CGAN生成经过蒙特卡罗EM算法优化的图像。图[7](#S5.F7 "Figure
    7 ‣ 5.2 Label-conditional image generation ‣ 5 Model-based image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")。
    |'
- en: '| ImbCGAN [[46](#bib.bib46)] | 2018 | Deploy CGAN as image augmentation for
    unbalanced classes. |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| ImbCGAN [[46](#bib.bib46)] | 2018 | 将CGAN部署为不平衡类别的图像增强。'
- en: '| BAGAN [[47](#bib.bib47)] | 2018 | Train an auto-encoder to initialize generator.
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| BAGAN [[47](#bib.bib47)] | 2018 | 训练自编码器以初始化生成器。 |'
- en: '| DAGAN [[48](#bib.bib48)] | 2018 | Image is taken as the class condition for
    generator and discriminator. Figure [8](#S5.F8 "Figure 8 ‣ 5.2 Label-conditional
    image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of
    Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| DAGAN [[48](#bib.bib48)] | 2018 | 图像作为生成器和判别器的类别条件。图示[8](#S5.F8 "图 8 ‣ 5.2
    标签条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")。 |'
- en: '| MFC-GAN [[49](#bib.bib49)] | 2019 | Use multiple fake classes to obtain a
    fine-grained image for minority class. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| MFC-GAN [[49](#bib.bib49)] | 2019 | 使用多个假类获取少数类的细粒度图像。 |'
- en: '| IDA-GAN[[50](#bib.bib50)] | 2021 | Train a variational auto-encoder and CGAN
    simultaneously. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| IDA-GAN[[50](#bib.bib50)] | 2021 | 同时训练变分自编码器和CGAN。 |'
- en: '| S$+$U Learning [[51](#bib.bib51)] | 2017 | Translate synthetic images from
    a graphic model to realistic images using CGAN. |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| S$+$U Learning [[51](#bib.bib51)] | 2017 | 使用CGAN将合成图像从图形模型转换为真实图像。 |'
- en: '| AugGAN [[52](#bib.bib52)] | 2018 | Aim to semantically preserve object when
    changing its style. |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| AugGAN [[52](#bib.bib52)] | 2018 | 旨在更改风格时语义保留对象。 |'
- en: '| Plant-CGAN [[53](#bib.bib53)] | 2018 | Translate semantic instance layout
    to real images using CGAN. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Plant-CGAN [[53](#bib.bib53)] | 2018 | 使用CGAN将语义实例布局转换为真实图像。 |'
- en: '| StyleAug [[54](#bib.bib54)] | 2019 | Change image style via style transfer.
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| StyleAug [[54](#bib.bib54)] | 2019 | 通过风格迁移改变图像风格。 |'
- en: '| Shape bias [[55](#bib.bib55)] | 2019 | Transfer image style from painted
    images to mitigate texture bias of CNN. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Shape bias [[55](#bib.bib55)] | 2019 | 将图像风格从绘制图像迁移，以减轻CNN的纹理偏差。 |'
- en: '| EmoGAN [[56](#bib.bib56)] | 2018 | Translate a neutral face with another
    emotion. |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| EmoGAN [[56](#bib.bib56)] | 2018 | 将中性面孔转换为另一种情感。 |'
- en: '| $\delta$-encoder [[57](#bib.bib57)] | 2018 | Image is taken as a class condition
    to generate images for new or infrequent class. |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $\delta$-encoder [[57](#bib.bib57)] | 2018 | 将图像视为类别条件，以生成新类或不频繁出现类的图像。 |'
- en: '| Debiased NN [[58](#bib.bib58)] | 2021 | Merge style and content via style
    transfer and appropriate labels. Figure [10](#S5.F10 "Figure 10 ‣ 5.3.2 Label-changing
    image generation ‣ 5.3 Image-conditional image generation ‣ 5 Model-based image
    augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep
    Learning"). |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Debiased NN [[58](#bib.bib58)] | 2021 | 通过风格迁移和适当的标签合并风格和内容。图示[10](#S5.F10
    "图 10 ‣ 5.3.2 标签更改图像生成 ‣ 5.3 图像条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")。 |'
- en: '| StyleMix [[59](#bib.bib59)] | 2021 | Merge two images with style, content,
    and labels. Figure [11](#S5.F11 "Figure 11 ‣ 5.3.2 Label-changing image generation
    ‣ 5.3 Image-conditional image generation ‣ 5 Model-based image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| StyleMix [[59](#bib.bib59)] | 2021 | 合并两个具有风格、内容和标签的图像。图示[11](#S5.F11 "图
    11 ‣ 5.3.2 标签更改图像生成 ‣ 5.3 图像条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")。 |'
- en: '| GAN-MBD [[60](#bib.bib60)] | 2021 | Translate an image from one class to
    another while preserving semantics via multi-branch discriminator. Figure [9](#S5.F9
    "Figure 9 ‣ 5.3.2 Label-changing image generation ‣ 5.3 Image-conditional image
    generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"). |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| GAN-MBD [[60](#bib.bib60)] | 2021 | 通过多分支判别器将图像从一个类别转换为另一个类别，同时保持语义。图示[9](#S5.F9
    "图 9 ‣ 5.3.2 标签更改图像生成 ‣ 5.3 图像条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")。 |'
- en: '| SCIT [[2](#bib.bib2)] | 2022 | Translate healthy leaves to abnormal one while
    retaining its style. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| SCIT [[2](#bib.bib2)] | 2022 | 将健康叶片转换为异常叶片，同时保留其风格。 |'
- en: 'Table 6: Studies relating to model-based image augmentation, label-conditional
    (top), label-preserving image-conditional (middle), and label-changing image-conditional
    (bottom).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 相关于基于模型的图像增强、标签条件（顶部）、标签保留图像条件（中部）和标签更改图像条件（底部）的研究。 |'
- en: 5.1 Unconditional image generation
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 无条件图像生成 |
- en: An image synthesis model benefits image augmentation, which enables it to produce
    new images. Theoretically, the distribution of generated images is similar to
    that in the original dataset for a generative adversarial network (GAN) model
    after training [[88](#bib.bib88)]. However, the generated images are not the same
    as the original images and can be considered as points located in the vicinity
    distribution. In DCGAN [[41](#bib.bib41)], two random noises or latent vectors
    can be interpolated to generate intermediate images, which can be regarded as
    fluctuations between two original data points. Generally, a generative model with
    noise as input is deemed an unconditional model, and the corresponding image generation
    process is considered unconditional image generation. If the datasets encompass
    a single class, as in the case of medical images with one abnormal class [[42](#bib.bib42)],
    an unconditional image generation model can be directly applied to perform augmentation.
    Furthermore, a specific unconditional model can be leveraged for an individual
    class in the presence of multiple classes [[43](#bib.bib43)], [[44](#bib.bib44)].
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图像合成模型有助于图像增强，使其能够生成新图像。从理论上讲，对于生成对抗网络（GAN）模型，经过训练后，生成图像的分布与原始数据集的分布相似 [[88](#bib.bib88)]。然而，生成的图像与原始图像并不相同，可以被视为位于附近分布的点。在
    DCGAN [[41](#bib.bib41)] 中，可以对两个随机噪声或潜在向量进行插值，以生成中间图像，这可以看作是两个原始数据点之间的波动。一般来说，以噪声作为输入的生成模型被认为是无条件模型，相应的图像生成过程被认为是无条件图像生成。如果数据集涵盖了单一类别，例如具有一个异常类别的医学图像
    [[42](#bib.bib42)]，则可以直接应用无条件图像生成模型进行增强。此外，特定的无条件模型可以在存在多个类别的情况下用于单一类别 [[43](#bib.bib43)],
    [[44](#bib.bib44)]。
- en: 5.2 Label-conditional image generation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 标签条件图像生成
- en: Although unconditional image generation has potential, the shared information
    of different classes cannot be utilized. In contrast, label-conditional image
    generation is expected to leverage the shared information and learn variations
    for minority classes using majority-class data. Label-conditional image generation
    requires one specific label as an extra input, and the generated image should
    align with the label condition.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无条件图像生成具有潜力，但不同类别的共享信息无法被利用。相比之下，标签条件图像生成预计能够利用共享信息，并使用多数类数据学习少数类的变化。标签条件图像生成需要一个特定的标签作为额外输入，生成的图像应与标签条件一致。
- en: '![Refer to caption](img/bc4717a6e8e6af1d8df4162e0a165596.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bc4717a6e8e6af1d8df4162e0a165596.png)'
- en: 'Figure 7: GAN and variants of label-conditional GANs [[45](#bib.bib45)]. G:
    generator, A: authenticator, C: classifier, D: discriminator.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: GAN 和标签条件 GAN 的变体 [[45](#bib.bib45)]。G: 生成器，A: 鉴别器，C: 分类器，D: 鉴别器。'
- en: The primary issue in label-conditional image generation is the use of label
    conditions. CGAN [[89](#bib.bib89)] uses the label for a generator, whereas the
    authenticator does not use the label. Consequently, the generator tends to ignore
    label information, as the authenticator cannot provide feedback regarding the
    condition. ACGAN [[90](#bib.bib90)] introduces an auxiliary classifier in the
    discriminator, which encourages the generator to produce images aligned with label
    conditions. With a more complex classifier, BDA [[45](#bib.bib45)] separates the
    classifier from the discriminator. Figure [7](#S5.F7 "Figure 7 ‣ 5.2 Label-conditional
    image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of
    Image Augmentation Techniques for Deep Learning") illustrates the differences
    between BDA and other label-conditional algorithms. In addition, MFC-GAN [[49](#bib.bib49)]
    adopts multiple fake classes in the classification loss to stabilize the training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 标签条件图像生成的主要问题是标签条件的使用。CGAN [[89](#bib.bib89)] 使用标签来生成器，而鉴别器则不使用标签。因此，生成器往往忽略标签信息，因为鉴别器无法提供有关条件的反馈。ACGAN
    [[90](#bib.bib90)] 在鉴别器中引入了一个辅助分类器，鼓励生成器生成与标签条件一致的图像。BDA [[45](#bib.bib45)] 采用了更复杂的分类器，将分类器与鉴别器分开。图
    [7](#S5.F7 "Figure 7 ‣ 5.2 Label-conditional image generation ‣ 5 Model-based
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning") 说明了 BDA 与其他标签条件算法的区别。此外，MFC-GAN [[49](#bib.bib49)] 采用了多个假类的分类损失来稳定训练。
- en: One of the main applications of label-conditional image generation is the class
    imbalance [[49](#bib.bib49)] [[46](#bib.bib46)] [[50](#bib.bib50)]. The generative
    model is expected to learn useful features from the majority class, and use them
    to generate images for the minority classes. The generated images are used to
    rebalance the original training dataset. However, it may be challenging to train
    a GAN model with an unbalanced dataset, as the majority class dominates the discriminator
    loss and the generator tends to produce images within the majority class. To address
    this challenge, a pretrained autoencoder with reconstruction loss has been employed
    to initialize a generator [[47](#bib.bib47)] [[50](#bib.bib50)].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 标签条件图像生成的主要应用之一是类别不平衡 [[49](#bib.bib49)] [[46](#bib.bib46)] [[50](#bib.bib50)]。生成模型期望从多数类中学习有用的特征，并用它们生成少数类的图像。这些生成的图像用于重新平衡原始训练数据集。然而，用不平衡的数据集训练GAN模型可能具有挑战性，因为多数类主导了判别器损失，生成器倾向于生成多数类的图像。为了解决这一挑战，采用了具有重建损失的预训练自编码器来初始化生成器
    [[47](#bib.bib47)] [[50](#bib.bib50)]。
- en: '![Refer to caption](img/7ba8299378e3c40ec371e5519463eb88.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7ba8299378e3c40ec371e5519463eb88.png)'
- en: 'Figure 8: Flowchart of DAGAN [[48](#bib.bib48)], where label information is
    obtained from an image via an encoder, rather than a label.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：DAGAN的流程图 [[48](#bib.bib48)]，其中标签信息通过编码器从图像中获得，而不是通过标签。
- en: Although various discriminators and classifiers may be employed, the aforementioned
    algorithms utilize the class condition on a one-hot label. One resulting limitation
    is that the trained model can generate only known-class images. To overcome this
    limitation, DAGAN [[48](#bib.bib48)] utilizes an image encoder to extract the
    class, so that the generated image is assumed to have the same class as the original
    image. Figure [8](#S5.F8 "Figure 8 ‣ 5.2 Label-conditional image generation ‣
    5 Model-based image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning") illustrates the DAGAN algorithm.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以使用各种判别器和分类器，但上述算法利用的是一热标签的类别条件。一种结果限制是训练的模型只能生成已知类别的图像。为克服这一限制，DAGAN [[48](#bib.bib48)]
    利用图像编码器提取类别，以使生成的图像被认为具有与原始图像相同的类别。图 [8](#S5.F8 "图8 ‣ 5.2 标签条件图像生成 ‣ 5 基于模型的图像增强
    ‣ 深度学习图像增强技术的全面调查") 展示了DAGAN算法。
- en: 5.3 Image-conditional image generation
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 图像条件图像生成
- en: 'In image generation, images can be employed as conditions, known as image translation.
    Generally, an image consists of content and style [[91](#bib.bib91), [92](#bib.bib92)].
    Content refers to class-dependent attributes, such as dogs and cats, whereas style
    denotes class-independent elements, such as color and illumination. Image-conditional
    image generation can be subcategorized into two types: label-preserving and label-changing.
    The former requires content to be retained, whereas the latter requires content
    to be changed.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像生成中，图像可以作为条件使用，称为图像翻译。一般来说，图像由内容和风格组成 [[91](#bib.bib91), [92](#bib.bib92)]。内容指的是依赖类别的属性，如狗和猫，而风格则指的是独立于类别的元素，如颜色和光照。图像条件图像生成可以细分为两种类型：标签保留和标签变化。前者要求保留内容，而后者则要求改变内容。
- en: 5.3.1 Label-preserving image generation
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 标签保留图像生成
- en: Label-preserving assumes that the label of a generated image is the same as
    that of the input image. One active field to deploy this approach is the domain
    shift, where the style of the source domain is different from that of the target
    domain. To address this challenge, original images can be translated from the
    source domain to the target domain. To preserve the object during image translation,
    AugGAN employs a segmentation module that extracts context-aware features to share
    parameters with a generator [[52](#bib.bib52)]. For practical applications, synthetic
    images generated by a graphical model are translated into natural images [[51](#bib.bib51)],
    and the leaf layout is translated as a real leaf image [[53](#bib.bib53)]. In
    addition, image translation can be utilized for semantic segmentation with a domain
    shift [[93](#bib.bib93)]. Furthermore, label-preserving can be leveraged to improve
    the robustness of a trained model. Inspired by the observation that CNNs exhibit
    bias on texture toward shape, original images are translated to have different
    textures, which allows the CNN to allocate more attention to shape [[55](#bib.bib55)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 标签保持假设生成图像的标签与输入图像的标签相同。一个积极的应用领域是领域迁移，其中源域的风格与目标域不同。为了解决这个挑战，可以将原始图像从源域翻译到目标域。为了在图像翻译过程中保持对象，AugGAN
    使用了一个分割模块，该模块提取上下文感知特征，并与生成器共享参数 [[52](#bib.bib52)]。在实际应用中，通过图形模型生成的合成图像被翻译为自然图像
    [[51](#bib.bib51)]，叶片布局被翻译为真实叶片图像 [[53](#bib.bib53)]。此外，图像翻译可以用于具有领域迁移的语义分割 [[93](#bib.bib93)]。此外，标签保持可以被用来提高训练模型的鲁棒性。受
    CNN 在纹理上对形状的偏向的观察启发，原始图像被翻译为具有不同纹理的图像，这使得 CNN 可以将更多关注分配给形状 [[55](#bib.bib55)]。
- en: It is often challenging to obtain the desired style during the image generation
    process. Most algorithms utilize an encoder to extract style from an image, as
    in the case of DRIT++ [[94](#bib.bib94)] and SPADE [[95](#bib.bib95)]. This approach
    to image translation can be regarded as image fusion. In contrast, Jackson et
    al. [[54](#bib.bib54)] proposed style augmentation, where the style is generated
    from a multivariate normal distribution. Another challenge is that the *one* model
    can be adopted to generate images for *multiple* domains with fewer trained images.
    To address this, MetalGAN leverages domain loss and meta-learning strategies [[96](#bib.bib96)].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像生成过程中，获得期望风格常常是具有挑战性的。大多数算法使用编码器从图像中提取风格，如 DRIT++ [[94](#bib.bib94)] 和 SPADE
    [[95](#bib.bib95)]。这种图像翻译方法可以视为图像融合。相反，Jackson 等人 [[54](#bib.bib54)] 提出了风格增强，其中风格是从多元正态分布中生成的。另一个挑战是，*一个*
    模型可以用于生成 *多个* 域的图像，且训练图像较少。为了解决这个问题，MetalGAN 利用领域损失和元学习策略 [[96](#bib.bib96)]。
- en: 5.3.2 Label-changing image generation
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 标签变化图像生成
- en: In contrast to label-preserving, label-changing changes the label-dependent.
    For example, a neutral face can be transformed into a different emotion [[56](#bib.bib56)].
    Although the generated images have poor fidelity, the approach improves the classification
    of emotions. In addition to changing label dependence, the preservation of label
    independence has recently received attention as a way to improve variability within
    the target class, thereby mitigating class imbalance. To take variation from one
    to another class, a style loss is leveraged to retain the style when translating
    an image [[2](#bib.bib2)]. Similarly, a multi-branch discriminator with fewer
    channels is introduced to achieve semantic consistency such as the number of objects
    [[60](#bib.bib60)]. Figure [9](#S5.F9 "Figure 9 ‣ 5.3.2 Label-changing image generation
    ‣ 5.3 Image-conditional image generation ‣ 5 Model-based image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning") shows
    several satisfactory translated images. To address severe class imbalance, a $\delta$-encoder
    has been proposed to extract label-independent features from one label to another
    [[57](#bib.bib57)]. As in the case of DAGAN [[48](#bib.bib48)], class information
    is provided by an image. The $\delta$-encoder and decoder aim to reconstruct the
    given image in the training phase, whereas the decoder is provided a new label
    image and required to generate the same label in the testing phase.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签保留相比，标签变化改变了标签依赖。例如，中性面孔可以被转变为不同的情绪[[56](#bib.bib56)]。尽管生成的图像具有较差的保真度，但该方法改善了情绪分类。除了改变标签依赖性之外，标签独立性的保留最近也受到关注，作为提高目标类变异性的方式，从而缓解类别不平衡。为了从一个类转换到另一个类，利用风格损失在翻译图像时保持风格[[2](#bib.bib2)]。类似地，引入了具有较少通道的多分支判别器，以实现语义一致性，例如对象的数量[[60](#bib.bib60)]。图[9](#S5.F9
    "图9 ‣ 5.3.2 标签变化图像生成 ‣ 5.3 图像条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")展示了几个令人满意的翻译图像。为了解决严重的类别不平衡，提出了$\delta$-编码器，用于从一个标签提取独立特征到另一个标签[[57](#bib.bib57)]。如DAGAN
    [[48](#bib.bib48)]的情况，类别信息由图像提供。$\delta$-编码器和解码器旨在在训练阶段重建给定图像，而解码器在测试阶段提供新的标签图像，并要求生成相同的标签。
- en: '![Refer to caption](img/dd1655444c63b6eeb33f7ea6b539433c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd1655444c63b6eeb33f7ea6b539433c.png)'
- en: 'Figure 9: Semantic level matching by GAN-MBD [[60](#bib.bib60)] for label-changing
    image augmentation, including position, number, and pose.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：由GAN-MBD [[60](#bib.bib60)] 实现的语义层次匹配用于标签变化的图像增强，包括位置、数量和姿势。
- en: Compared to label-preserving, label-changing yields more significant improvements
    in model robustness by changing the label and style simultaneously. As illustrated
    in Figure [10](#S5.F10 "Figure 10 ‣ 5.3.2 Label-changing image generation ‣ 5.3
    Image-conditional image generation ‣ 5 Model-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"), traditional image
    augmentation does not change the label after altering the color of the chimpanzee
    to that of a lemon, which incurs shape bias. By contrast, when a texture-biased
    model is trained, the translated image is labeled as a lemon. To balance the bias,
    the translated image by style transfer is taken with two labels [[58](#bib.bib58)]
    – chimpanzee and lemon – which eliminates bias. Inspired by Mixup [[28](#bib.bib28)],
    Hong et al. developed StyleMix [[59](#bib.bib59)], which merges the two inputs
    to obtain content and style labels, as shown in Figure [11](#S5.F11 "Figure 11
    ‣ 5.3.2 Label-changing image generation ‣ 5.3 Image-conditional image generation
    ‣ 5 Model-based image augmentation ‣ A Comprehensive Survey of Image Augmentation
    Techniques for Deep Learning"). These labels are then fused to obtain the final
    label for the generated images.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与标签保留相比，标签变化通过同时改变标签和风格显著提高了模型的鲁棒性。如图[10](#S5.F10 "图10 ‣ 5.3.2 标签变化图像生成 ‣ 5.3
    图像条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")所示，传统的图像增强在将猩猩的颜色更改为柠檬的颜色后，并不会改变标签，这会导致形状偏差。相比之下，当训练一个纹理偏差模型时，翻译后的图像标记为柠檬。为了平衡偏差，通过风格迁移获得的翻译图像带有两个标签[[58](#bib.bib58)]——猩猩和柠檬——这消除了偏差。受到Mixup
    [[28](#bib.bib28)]的启发，Hong等人开发了StyleMix [[59](#bib.bib59)]，它将两个输入合并以获得内容和风格标签，如图[11](#S5.F11
    "图11 ‣ 5.3.2 标签变化图像生成 ‣ 5.3 图像条件图像生成 ‣ 5 基于模型的图像增强 ‣ 深度学习图像增强技术的综合调查")所示。这些标签随后被融合以获得生成图像的最终标签。
- en: '![Refer to caption](img/09aa952cc7cb2bb6b4e7e1254a48697a.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/09aa952cc7cb2bb6b4e7e1254a48697a.png)'
- en: 'Figure 10: Label assignment for the biased and unbiased model with respect
    to shape and texture [[58](#bib.bib58)].'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：带有形状和纹理的有偏模型和无偏模型的标签分配 [[58](#bib.bib58)]。
- en: '![Refer to caption](img/a651b5f3a421dc498ad836e24e76d65e.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a651b5f3a421dc498ad836e24e76d65e.png)'
- en: 'Figure 11: Examples of label assignment with different algorithms [[59](#bib.bib59)].'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：不同算法的标签分配示例 [[59](#bib.bib59)]。
- en: 6 Optimizing policy-based image augmentation
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 优化基于策略的图像增强
- en: 'All algorithms mentioned in the previous two sections represent specific schemes,
    wherein domain knowledge is required to achieve better performance. In general,
    individual operations with the desired magnitude are utilized to perform image
    augmentation for specific datasets according to their characteristics. However,
    hyperparameter optimization is challenging and time-consuming. One way to mitigate
    this is to design algorithms that determine optimal augmentation strategies. These
    algorithms, termed policy-based optimization, encompass two categories: reinforcement
    learning-based, and adversarial learning-based. The former category employs reinforcement
    learning (RL) to determine the optimal strategy, whereas the latter category adopts
    augmented operations and their magnitudes that generates a large training loss
    and small validation loss. As generative adversarial networks (GANs) can be utilized
    for both model-based and optimizing policy-based image augmentation, the objective
    to adopt GANs is the primary difference. Model-based category aims to *directly*
    generate images, instead of other goals such as finding optimal transformations
    [[69](#bib.bib69)]. Studies pertaining to policy-based optimization are listed
    in Table [7](#S6.T7 "Table 7 ‣ 6 Optimizing policy-based image augmentation ‣
    A Comprehensive Survey of Image Augmentation Techniques for Deep Learning").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 前两节中提到的所有算法代表了特定的方案，其中需要领域知识以实现更好的性能。通常，根据数据集的特征，利用具有所需幅度的单个操作进行图像增强。然而，超参数优化具有挑战性且耗时。缓解这一问题的一种方法是设计确定最佳增强策略的算法。这些算法称为基于策略的优化，涵盖了两类：基于强化学习的和基于对抗学习的。前一类采用强化学习（RL）来确定最佳策略，而后一类则采用生成大量训练损失和小验证损失的增强操作及其幅度。由于生成对抗网络（GANs）可以用于模型基础和优化策略基础的图像增强，因此采用
    GANs 是主要区别。基于模型的类别旨在*直接*生成图像，而不是其他目标，如寻找最佳变换 [[69](#bib.bib69)]。关于基于策略的优化的研究列在表
    [7](#S6.T7 "Table 7 ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning") 中。
- en: '| Paper | Year | Highlight |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 年份 | 亮点 |'
- en: '| --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| AutoAugment [[61](#bib.bib61)] | 2019 | Use reinforcement learning to determine
    the optimal augmentation strategies. Figure [12](#S6.F12 "Figure 12 ‣ 6.1 Reinforcement
    learning-based ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| AutoAugment [[61](#bib.bib61)] | 2019 | 使用强化学习来确定最佳增强策略。 图 [12](#S6.F12 "Figure
    12 ‣ 6.1 Reinforcement learning-based ‣ 6 Optimizing policy-based image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")。
    |'
- en: '| Fast AA [[62](#bib.bib62)] | 2019 | Use efficient density matching for augmentation
    policy search. |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Fast AA [[62](#bib.bib62)] | 2019 | 使用高效的密度匹配进行增强策略搜索。 |'
- en: '| PBA [[63](#bib.bib63)] | 2019 | Adopt non-stationary augmentation policy
    schedules via population-based training. |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| PBA [[63](#bib.bib63)] | 2019 | 通过基于群体的训练采用非平稳增强策略调度。 |'
- en: '| Faster AA [[64](#bib.bib64)] | 2019 | Use a differentiable policy search
    pipeline via approximate gradients. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 更快的 AA [[64](#bib.bib64)] | 2019 | 使用通过近似梯度的可微分策略搜索管道。 |'
- en: '| RandAugment [[65](#bib.bib65)] | 2020 | Reduce the search space of AutoAug
    via probability adjustment. |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| RandAugment [[65](#bib.bib65)] | 2020 | 通过概率调整减少 AutoAug 的搜索空间。 |'
- en: '| MADAO [[66](#bib.bib66)] | 2020 | Train task model and optimize the search
    space simultaneously by implicit gradient with Neumann series approximation. |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| MADAO [[66](#bib.bib66)] | 2020 | 通过 Neumann 系列逼近同时训练任务模型并优化搜索空间。 |'
- en: '| LDA [[67](#bib.bib67)] | 2020 | Take policy search as a discrete optimization
    for object detection. |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LDA [[67](#bib.bib67)] | 2020 | 将策略搜索视为目标检测的离散优化。 |'
- en: '| LSSP [[68](#bib.bib68)] | 2021 | Learn a sample-specific policy for sequential
    image augmentation. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LSSP [[68](#bib.bib68)] | 2021 | 学习用于序列图像增强的样本特定策略。 |'
- en: '| ADA [[69](#bib.bib69)] | 2016 | Seek a small transformation that yields maximal
    classification loss on the transformed sample. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ADA [[69](#bib.bib69)] | 2016 | 寻找一个小的变换，使变换后的样本产生最大的分类损失。 |'
- en: '| CDST-DA [[70](#bib.bib70)] | 2017 | Optimize a generative sequence using
    GAN in which the transformed image is pushed to be within the same class distribution.
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| CDST-DA [[70](#bib.bib70)] | 2017 | 使用生成对抗网络（GAN）优化生成序列，将变换后的图像推向相同类别分布内。
    |'
- en: '| AdaTransform [[71](#bib.bib71)] | 2019 | Use a competitive task to obtain
    augmented images with a high task loss in the training stage, and a cooperative
    task to obtain augmented images with a low task loss in the testing stage. Figure
    [13](#S6.F13 "Figure 13 ‣ 6.2 Adversarial learning-based ‣ 6 Optimizing policy-based
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning"). |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| AdaTransform [[71](#bib.bib71)] | 2019 | 使用竞争任务在训练阶段获得具有高任务损失的增强图像，并使用合作任务在测试阶段获得具有低任务损失的增强图像。图
    [13](#S6.F13 "Figure 13 ‣ 6.2 Adversarial learning-based ‣ 6 Optimizing policy-based
    image augmentation ‣ A Comprehensive Survey of Image Augmentation Techniques for
    Deep Learning")。'
- en: '| Adversarial AA [[72](#bib.bib72)] | 2020 | Optimize a policy to increase
    task loss while allowing task model to minimize the loss. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Adversarial AA [[72](#bib.bib72)] | 2020 | 优化一个策略以增加任务损失，同时允许任务模型最小化损失。 |'
- en: '| IF-DA [[73](#bib.bib73)] | 2020 | Use influence function to predict how validation
    loss is affected by image augmentation, and minimize the approximated validation
    loss. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| IF-DA [[73](#bib.bib73)] | 2020 | 使用影响函数预测图像增强如何影响验证损失，并最小化近似的验证损失。 |'
- en: '| SPA [[74](#bib.bib74)] | 2021 | Select suitable samples to perform image
    augmentation. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| SPA [[74](#bib.bib74)] | 2021 | 选择合适的样本进行图像增强。 |'
- en: 'Table 7: Studies relating to optimizing policy of image augmentation. The upper
    and the bottom suggest reinforcement learning- and adversarial learning-based
    image augmentation.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：与优化图像增强策略相关的研究。上半部分和下半部分分别提出了基于强化学习和对抗学习的图像增强。
- en: 6.1 Reinforcement learning-based
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 基于强化学习
- en: AutoAugment [[61](#bib.bib61)] is a seminal approach that employs reinforcement
    learning. As shown in Figure [12](#S6.F12 "Figure 12 ‣ 6.1 Reinforcement learning-based
    ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning"), iterative steps are used to find
    the optimal policy. The controller samples a strategy from a search space with
    the operation type and its corresponding probability and magnitude, and a task
    network subsequently obtains the validation accuracy as feedback to update the
    controller. Because the search space is very large, lighter child networks are
    leveraged. After training, the controller is used to train the original task model
    and can be finetuned in other datasets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: AutoAugment [[61](#bib.bib61)] 是一种开创性的方法，采用强化学习。正如图 [12](#S6.F12 "Figure 12
    ‣ 6.1 Reinforcement learning-based ‣ 6 Optimizing policy-based image augmentation
    ‣ A Comprehensive Survey of Image Augmentation Techniques for Deep Learning")
    所示，通过迭代步骤来寻找最佳策略。控制器从具有操作类型及其对应概率和幅度的搜索空间中采样策略，任务网络随后获取验证精度作为反馈来更新控制器。由于搜索空间非常大，因此利用了较轻的子网络。训练后，控制器用于训练原始任务模型，并可以在其他数据集中进行微调。
- en: '![Refer to caption](img/215549d727d9883fc00ee2a4f0628306.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/215549d727d9883fc00ee2a4f0628306.png)'
- en: 'Figure 12: Overview of AutoAugment [[61](#bib.bib61)], a reinforcement learning-based
    image augmentation method.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：AutoAugment [[61](#bib.bib61)] 的概述，这是一种基于强化学习的图像增强方法。
- en: Although AutoAugment achieves satisfactory classification performance across
    several datasets, it requires a long training time. To address this issue, several
    studies have been conducted from different perspectives. For instance, RandAugment
    [[65](#bib.bib65)] replaces several probabilities in AutoAugment with a uniform
    probability. Conversely, Fast AA [[62](#bib.bib62)] and Faster AA [[64](#bib.bib64)]
    leverage density matching, aligning the densities of the training and augmented
    training datasets, instead of Proximal Policy Optimization [[97](#bib.bib97)],
    to optimize the controller in AutoAugment. Furthermore, PBA [[63](#bib.bib63)]
    attempts to learn a policy schedule from population-based training, rather than
    a single policy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AutoAugment 在多个数据集上实现了令人满意的分类性能，但它需要较长的训练时间。为了解决这个问题，已经从不同的角度进行了多项研究。例如，RandAugment
    [[65](#bib.bib65)] 用均匀概率替代了 AutoAugment 中的多个概率。相对而言，Fast AA [[62](#bib.bib62)]
    和 Faster AA [[64](#bib.bib64)] 利用密度匹配，对训练和增强训练数据集的密度进行对齐，而不是使用 Proximal Policy
    Optimization [[97](#bib.bib97)] 来优化 AutoAugment 中的控制器。此外，PBA [[63](#bib.bib63)]
    尝试从基于人群的训练中学习一个策略调度，而不是单一策略。
- en: Except for the long training phase, AutoAugment utilizes child models, by which
    the learned policy may not be optimal for the final task model. To address this
    issue, Hataya et al. [[66](#bib.bib66)] trained the target model and image augmentation
    policy simultaneously using the same differentiable image augmentation pipeline
    in Faster AA. In contrast, Adversarial AA [[72](#bib.bib72)] leverages adversarial
    loss simultaneously with reinforcement learning.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 除了较长的训练阶段外，AutoAugment 还利用了子模型，这可能导致学习的策略对于最终任务模型并不最优。为了解决这个问题，Hataya 等人 [[66](#bib.bib66)]
    使用相同的可微图像增强管道同时训练目标模型和图像增强策略。而 Adversarial AA [[72](#bib.bib72)] 则同时利用对抗损失和强化学习。
- en: One limitation of the algorithms mentioned above is that the learned image augmentation
    policy is at the dataset level. Conversely, class- and sample-level image augmentation
    methods were considered in [[98](#bib.bib98)] and [[68](#bib.bib68)], respectively,
    wherein each class or sample utilizes a specific policy. Furthermore, instance-level
    image augmentation was considered in [[67](#bib.bib67)] for object detection,
    where operations were performed only inside the bounding box.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述算法的一个局限性是学习到的图像增强策略是在数据集级别的。相对而言，类级别和样本级别的图像增强方法在 [[98](#bib.bib98)] 和 [[68](#bib.bib68)]
    中被考虑，其中每个类或样本使用特定的策略。此外，[[67](#bib.bib67)] 中考虑了实例级别的图像增强，用于目标检测，其中操作仅在边界框内部进行。
- en: 6.2 Adversarial learning-based
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 基于对抗学习的
- en: The primary objective of image augmentation is to train a task model with a
    training dataset to achieve sufficient generalizability on a testing dataset.
    One assumption is that hard samples are more useful, and the input images that
    cause a larger training loss are considered hard samples. Adversarial learning-based
    image augmentation aims to learn an image augmentation policy to generate hard
    samples based on the original training samples.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图像增强的主要目标是通过训练数据集训练任务模型，以在测试数据集上实现足够的泛化能力。一个假设是困难样本更有用，而导致更大训练损失的输入图像被认为是困难样本。基于对抗学习的图像增强旨在学习一种图像增强策略，以基于原始训练样本生成困难样本。
- en: An early method [[69](#bib.bib69)] attempts to find a small transformation that
    maximizes training loss on the augmented samples, wherein learning optimization
    finds an optimal magnitude given an operation. One of the main limitations is
    the label-preserving assumption that the augmented image retains the same label
    as the original image. To meet this assumption, a common strategy is to design
    the type of operation and range of corresponding magnitude using human knowledge.
    To weaken this assumption, Ratner et al. [[70](#bib.bib70)] introduced generative
    adversarial loss to learn a transformation sequence in which the discriminator
    pushes the generated images to one of the original classes, instead of an unseen
    or null class.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一种早期的方法 [[69](#bib.bib69)] 尝试找到一个小的变换，以最大化增强样本上的训练损失，其中学习优化找到给定操作的最佳幅度。主要的限制之一是标签保留假设，即增强图像保留与原始图像相同的标签。为了满足这一假设，一个常见的策略是使用人工知识设计操作类型和相应幅度的范围。为了削弱这一假设，Ratner
    等人 [[70](#bib.bib70)] 引入了生成对抗损失，以学习一个变换序列，其中鉴别器将生成的图像推向原始类别之一，而不是未见过或空类别。
- en: Interestingly, SPA [[74](#bib.bib74)] attempts to select suitable samples, and
    image augmentation is leveraged only on those samples in which the augmented image
    incurs a larger training loss than the original image. Although SPA trains the
    image augmentation policy and task model simultaneously at the sample level, the
    impact of the learned policy in the validation dataset is unknown. To address
    this challenge, an influence function was adopted for approximating the change
    in validation loss without actually comparing performance [[73](#bib.bib73)].
    Another interesting concept is the use of image augmentation in the *testing stage*.
    To achieve this, AdaTransform [[71](#bib.bib71)] learns two tasks – competitive
    and cooperative – as illustrated in Figure [13](#S6.F13 "Figure 13 ‣ 6.2 Adversarial
    learning-based ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive
    Survey of Image Augmentation Techniques for Deep Learning"). In a competitive
    task, the transformer learns to increase the input variance by increasing the
    loss of the target network, while the discriminator attempts to push the augmented
    image realistically. Conversely, the transformer learns to decrease the variance
    of the augmented image in the cooperative task by reducing the loss of the target
    network. After training, the transformer is utilized to reduce the variance of
    the input image, thereby simplifying the testing process.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，SPA [[74](#bib.bib74)] 尝试选择合适的样本，并且仅对那些增强图像引发的训练损失大于原始图像的样本进行图像增强。尽管 SPA
    在样本层面上同时训练图像增强策略和任务模型，但所学策略在验证数据集上的影响尚不清楚。为了解决这个问题，采用了一种影响函数来近似验证损失的变化，而无需实际比较性能
    [[73](#bib.bib73)]。另一个有趣的概念是在*测试阶段*使用图像增强。为实现这一点，AdaTransform [[71](#bib.bib71)]
    学习了两个任务——竞争任务和合作任务——如图 [13](#S6.F13 "Figure 13 ‣ 6.2 Adversarial learning-based
    ‣ 6 Optimizing policy-based image augmentation ‣ A Comprehensive Survey of Image
    Augmentation Techniques for Deep Learning") 所示。在竞争任务中，变换器通过增加目标网络的损失来增加输入的方差，而鉴别器则试图使增强图像更真实。相反，在合作任务中，变换器通过减少目标网络的损失来学习降低增强图像的方差。训练完成后，变换器用于减少输入图像的方差，从而简化测试过程。
- en: '![Refer to caption](img/295cf8f7f64afcf00e1f977e60b0948e.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/295cf8f7f64afcf00e1f977e60b0948e.png)'
- en: 'Figure 13: Overview of AdaTransform [[71](#bib.bib71)]. AdaTransform encompasses
    two tasks – competitive training and cooperative testing – and three components:
    transformer $T$, discriminator $D$, and target network $N$. The transformer increases
    the variance of training data by competing with both $D$ and $N$. It also cooperates
    with $N$ in the testing phase to reduce data variance.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：AdaTransform [[71](#bib.bib71)] 的概述。AdaTransform 包含两个任务——竞争训练和合作测试——以及三个组件：变换器
    $T$、鉴别器 $D$ 和目标网络 $N$。变换器通过与 $D$ 和 $N$ 竞争来增加训练数据的方差。它还在测试阶段与 $N$ 合作以减少数据方差。
- en: 7 Discussions
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 'In this section, the usage of the mentioned strategies to perform image augmentation
    are first discussed. Several future directions are then illustrated. Furthermore,
    three related topics are discussed: understanding image augmentation from theory
    perspective, adopting image augmentation with other strategy, and augmenting features
    instead of images.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，首先讨论了提到的策略在图像增强中的应用。接着，展示了几个未来的发展方向。此外，还讨论了三个相关主题：从理论角度理解图像增强，结合其他策略采用图像增强，以及增强特征而非图像。
- en: Current situation. Datasets are assumed to be essential to obtain satisfactory
    performance. One way to generate an appropriate dataset is through image augmentation
    algorithms, which have demonstrated impressive results across multiple datasets
    and heterogeneous models. For instance, Mixup [[28](#bib.bib28)] increases the
    validation accuracy in ImageNet-2012 by 1.5 and 1.2 percent with ResNet-50 and
    ResNet-101\. Non-trivially, GAN-MBD [[60](#bib.bib60)] achieves 84.28 classification
    accuracy with an unbalance dataset setting in 102Flowers, 33.11, 31.44, and 14.05
    higher than non-image augmentation, geometrical transformation, and focal loss,
    respectively. Currently, mode-free and optimizing policies are widely leveraged,
    whereas the mode-based approach is an active research topic for specific challenges,
    such as class imbalance and domain adaptation. In addition, although most algorithms
    are label-preserving, label-changing algorithms have recently received attention.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 当前情况。数据集被认为是获得令人满意性能的关键。一种生成适当数据集的方法是通过图像增强算法，这些算法在多个数据集和异质模型上表现出色。例如，Mixup
    [[28](#bib.bib28)] 在 ImageNet-2012 上将 ResNet-50 和 ResNet-101 的验证准确率分别提高了 1.5%
    和 1.2%。非平凡地，GAN-MBD [[60](#bib.bib60)] 在 102Flowers 数据集中，在不平衡的数据集设置下达到了 84.28
    的分类准确率，比非图像增强、几何变换和焦点损失分别高出 33.11、31.44 和 14.05。目前，无模式和优化策略被广泛使用，而基于模式的方法则是针对特定挑战（如类别不平衡和领域适应）而活跃的研究主题。此外，尽管大多数算法保持标签不变，但标签变化的算法最近也受到了关注。
- en: Future direction. Although many image augmentation algorithms exist, developing
    novel algorithms remains crucial to improve the performance of deep learning.
    We argue that recognizing new challenges or variations may inspire novel methods
    if they can be mimicked using image augmentation. Further, most algorithms of
    image augmentation are designed for classification and hence extending them to
    other applications is one of the most applicable directions by incorporating application-based
    knowledge, such as time-series in video [[40](#bib.bib40)]. Another interesting
    direction is distinguishing specific applications from general computer vision
    tasks such as ImageNet [[99](#bib.bib99)] and COCO [[100](#bib.bib100)] and then
    finding new motivations to design image augmentation. For example, most variations
    in plant healthy and diseased leaves are shared and thus can be converted from
    one to another [[2](#bib.bib2)]. Finally, considering image augmentation from
    a systematic perspective is appealing. For example, the effects of image augmentation
    schedules on optimization such as learning rate and batch size, are analyzed in
    [[101](#bib.bib101)].
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 未来方向。尽管存在许多图像增强算法，开发新颖的算法仍然对提高深度学习的性能至关重要。我们认为，识别新的挑战或变体可能会激发新的方法，如果这些方法可以通过图像增强进行模拟的话。此外，大多数图像增强算法是针对分类设计的，因此，将其扩展到其他应用领域是一个应用广泛的方向，通过融入基于应用的知识，例如视频中的时间序列
    [[40](#bib.bib40)]。另一个有趣的方向是将特定应用与一般计算机视觉任务（如 ImageNet [[99](#bib.bib99)] 和 COCO
    [[100](#bib.bib100)]）区分开来，然后找到新的动机来设计图像增强。例如，植物健康和病变叶子的多数变化是共享的，因此可以相互转换 [[2](#bib.bib2)]。最后，从系统的角度考虑图像增强是很有吸引力的。例如，图像增强调度对优化的影响，如学习率和批量大小，在
    [[101](#bib.bib101)] 中进行了分析。
- en: Understanding image augmentation. This study was conducted to understand the
    objectives of image augmentation in the context of deep learning, from the perspectives
    of challenges and vicinity distribution. Although it was also verified that image
    augmentation is similar to regularization [[79](#bib.bib79)], most of the evidences
    are empirically from experiments. Understanding them in theory is therefore appealing.
    Recently, kernel theory [[102](#bib.bib102)] and group theory [[103](#bib.bib103)]
    have been used to analyze the effects of image augmentation. In addition, the
    improvement yielded by image augmentation in the context of model generalizability
    has been quantified using affinity and diversity [[104](#bib.bib104)].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 理解图像增强。本研究旨在从挑战和邻近分布的角度理解图像增强在深度学习中的目标。尽管也验证了图像增强类似于正则化 [[79](#bib.bib79)]，但大多数证据来自实验，因此从理论上理解它们是有吸引力的。最近，核理论
    [[102](#bib.bib102)] 和群体理论 [[103](#bib.bib103)] 被用于分析图像增强的效果。此外，在模型泛化的背景下，图像增强带来的改进已通过亲和性和多样性
    [[104](#bib.bib104)] 进行了量化。
- en: New strategy to leverage image augmentation. Although image augmentation is
    commonly used in a supervised manner, this must not necessarily be the case. First,
    a pretext task can be created via image augmentation, such as predicting the degrees
    of rotation [[105](#bib.bib105)] and relative positions of image patches [[106](#bib.bib106)].
    Second, image augmentation can be leveraged to generate positive samples for contrast
    learning under the assumption that an augmented image is similar to the corresponding
    original image [[107](#bib.bib107), [108](#bib.bib108), [109](#bib.bib109)]. Furthermore,
    semi-supervised learning benefits from image augmentation [[79](#bib.bib79), [110](#bib.bib110),
    [111](#bib.bib111)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 利用图像增强的新策略。尽管图像增强通常以监督方式使用，但这并非必须的。首先，可以通过图像增强创建前置任务，例如预测旋转角度[[105](#bib.bib105)]和图像块的相对位置[[106](#bib.bib106)]。其次，图像增强可以用来生成对比学习的正样本，前提是增强图像与对应的原始图像相似[[107](#bib.bib107),
    [108](#bib.bib108), [109](#bib.bib109)]。此外，半监督学习从图像增强中受益[[79](#bib.bib79), [110](#bib.bib110),
    [111](#bib.bib111)]。
- en: Feature augmentation attempts to perform augmentation in feature space instead
    of image space in image augmentation, and thus reduces the computation cost but
    without visual evidences. A feature space generally has dense information in semantic
    level than an image space. Consequently, operation in feature space is more efficient
    [[112](#bib.bib112)], such as domain knowledge [[113](#bib.bib113)]. Simultaneously,
    we believe that most of the techniques in image augmentation can be extended to
    feature augmentation, such as Manifold Mixup [[114](#bib.bib114)] from Mixup [[28](#bib.bib28)]
    and occluded feature [[115](#bib.bib115)].
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 特征增强尝试在特征空间而不是图像空间进行增强，从而降低计算成本，但没有视觉证据。特征空间通常在语义级别具有比图像空间更密集的信息。因此，特征空间中的操作更高效[[112](#bib.bib112)]，例如领域知识[[113](#bib.bib113)]。同时，我们认为图像增强中的大多数技术可以扩展到特征增强，例如从Mixup
    [[28](#bib.bib28)]到Manifold Mixup [[114](#bib.bib114)]以及遮挡特征[[115](#bib.bib115)]。
- en: 8 Conclusion
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: 'This study surveyed a wide range of image augmentation algorithms with a novel
    taxonomy encompassing three categories: model-free, model-based, and optimizing
    policy-based. To understand the objectives of image augmentation, we analyzed
    the challenges of deploying a deep learning model for computer vision tasks, and
    adopted the concept of vicinity distribution. We found that image augmentation
    significantly improves task performance, and many algorithms have been designed
    for specific challenges, such as intensity transformations for occlusion, and
    model-based algorithms for class imbalance and domain shift. Based on this analysis,
    we argue that novel methods can be inspired by new challenges. Conversely, appropriate
    methods can be selected after recognizing the challenges posed by a dataset. Furthermore,
    we discussed the current situation and possible directions of image augmentation
    with three relevant interesting topics. We hope that our study will provide an
    enhanced understanding of image augmentation and encourage the community to prioritize
    dataset characteristics.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究调查了广泛的图像增强算法，并提出了一种新颖的分类法，涵盖了三类：无模型、基于模型和优化策略基础。为了理解图像增强的目标，我们分析了在计算机视觉任务中部署深度学习模型的挑战，并采用了邻域分布的概念。我们发现图像增强显著提高了任务性能，许多算法针对特定挑战设计，例如用于遮挡的强度变换，以及用于类别不平衡和领域迁移的基于模型的算法。基于这一分析，我们认为新挑战可以激发新方法的灵感。相反，在识别数据集带来的挑战后，可以选择合适的方法。此外，我们讨论了图像增强的现状和可能的发展方向，并提出了三个相关的有趣话题。我们希望我们的研究能够增强对图像增强的理解，并鼓励社区优先考虑数据集的特征。
- en: Acknowledgment
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research was partly supported by the Basic Science Research Program through
    the National Research Foundation of Korea (NRF) funded by the Ministry of Education
    (No.2019R1A6A1A09031717), supported by the National Research Foundation of Korea
    (NRF) grant funded by the Ministry of Science and ICT (MSIT) (No. 2020R1A2C2013060),
    and supported by the Korea Institute of Planning and Evaluation for Technology
    in Food, Agriculture, and Forestry (IPET) and Korea Smart Farm R&D Foundation
    (KosFarm) through the Smart Farm Innovation Technology Development Program, funded
    by the Ministry of Agriculture, Food and Rural Affairs (MAFRA), Ministry of Science
    and ICT (MSIT), and Rural Development Administration (RDA) (No. 421027-04).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究部分得到了由韩国教育部资助的国家研究基金会（NRF）的基础科学研究项目（编号：2019R1A6A1A09031717）的支持，得到了韩国科学技术信息通信部（MSIT）资助的国家研究基金会（NRF）项目（编号：2020R1A2C2013060）的支持，以及由农业、食品和林业技术规划评估研究所（IPET）和韩国智能农场研发基金会（KosFarm）通过智能农场创新技术开发项目资助的支持，资金来源于农业部、食品和农村事务部（MAFRA）、科学技术信息通信部（MSIT）和农村发展管理局（RDA）（编号：421027-04）。
- en: References
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, A survey on deep learning
    in medical image analysis, Medical image analysis 42 (2017) 60–88.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian,
    J. A. Van Der Laak, B. Van Ginneken, C. I. Sánchez, 关于医学图像分析中的深度学习的调查，《医学图像分析》42
    (2017) 第60–88页。'
- en: '[2] M. Xu, S. Yoon, A. Fuentes, J. Yang, D. Park, Style-consistent image translation:
    A novel data augmentation paradigm to improve plant disease recognition, Front.
    Plant Sci. 12: 773142\. doi: 10.3389/fpls.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Xu, S. Yoon, A. Fuentes, J. Yang, D. Park, 风格一致的图像翻译：一种新颖的数据增强范式，用于提高植物疾病识别，《植物科学前沿》12:
    773142\. doi: 10.3389/fpls。'
- en: '[3] S. C. Wong, A. Gatt, V. Stamatescu, M. D. McDonnell, Understanding data
    augmentation for classification: when to warp?, in: 2016 international conference
    on digital image computing: techniques and applications (DICTA), IEEE, 2016, pp.
    1–6.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. C. Wong, A. Gatt, V. Stamatescu, M. D. McDonnell, 理解分类中的数据增强：何时进行变形？在：2016年国际数字图像计算技术与应用会议（DICTA），IEEE，2016，第1–6页。'
- en: '[4] L. Taylor, G. Nitschke, Improving deep learning with generic data augmentation,
    in: 2018 IEEE Symposium Series on Computational Intelligence (SSCI), IEEE, 2018,
    pp. 1542–1547.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Taylor, G. Nitschke, 通过通用数据增强改进深度学习，在：2018 IEEE计算智能研讨会系列（SSCI），IEEE，2018，第1542–1547页。'
- en: '[5] P. Kaur, B. S. Khehra, E. B. S. Mavi, Data augmentation for object detection:
    A review, in: 2021 IEEE International Midwest Symposium on Circuits and Systems
    (MWSCAS), IEEE, 2021, pp. 537–543.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] P. Kaur, B. S. Khehra, E. B. S. Mavi, 物体检测的数据增强：综述，在：2021 IEEE国际中西部电路与系统研讨会（MWSCAS），IEEE，2021，第537–543页。'
- en: '[6] C. Shorten, T. M. Khoshgoftaar, A survey on image data augmentation for
    deep learning, Journal of big data 6 (1) (2019) 1–48.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] C. Shorten, T. M. Khoshgoftaar, 关于深度学习图像数据增强的调查，《大数据期刊》6 (1) (2019) 第1–48页。'
- en: '[7] N. E. Khalifa, M. Loey, S. Mirjalili, A comprehensive survey of recent
    trends in deep learning for digital images augmentation, Artificial Intelligence
    Review (2021) 1–27.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] N. E. Khalifa, M. Loey, S. Mirjalili, 深度学习在数字图像增强中的最新趋势的综合调查，《人工智能评论》（2021）第1–27页。'
- en: '[8] X. Wang, K. Wang, S. Lian, A survey on face data augmentation for the training
    of deep neural networks, Neural computing and applications 32 (19) (2020) 15503–15531.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] X. Wang, K. Wang, S. Lian, 关于面部数据增强以训练深度神经网络的调查，《神经计算与应用》32 (19) (2020)
    第15503–15531页。'
- en: '[9] F. H. K. d. S. Tanaka, C. Aranha, Data augmentation using gans, arXiv preprint
    arXiv:1904.09135.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] F. H. K. d. S. Tanaka, C. Aranha, 使用生成对抗网络（GANs）的数据增强，arXiv预印本 arXiv:1904.09135。'
- en: '[10] H. Naveed, Survey: Image mixing and deleting for data augmentation, arXiv
    preprint arXiv:2106.07085.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] H. Naveed, 调查：图像混合和删除用于数据增强，arXiv预印本 arXiv:2106.07085。'
- en: '[11] A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with
    deep convolutional neural networks, Advances in neural information processing
    systems 25.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. Krizhevsky, I. Sutskever, G. E. Hinton, 使用深度卷积神经网络的Imagenet分类，《神经信息处理系统进展》25。'
- en: '[12] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2016, pp. 770–778.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] K. He, X. Zhang, S. Ren, J. Sun, 用于图像识别的深度残差学习，在：IEEE计算机视觉与模式识别会议论文集，2016，第770–778页。'
- en: '[13] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely connected
    convolutional networks, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 4700–4708.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger，密集连接卷积网络，见：IEEE
    计算机视觉与模式识别会议论文集，2017 年，第 4700–4708 页。'
- en: '[14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, H. Adam，Mobilenets: 用于移动视觉应用的高效卷积神经网络，arXiv 预印本 arXiv:1704.04861。'
- en: '[15] B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le, Learning transferable architectures
    for scalable image recognition, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 8697–8710.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] B. Zoph, V. Vasudevan, J. Shlens, Q. V. Le，学习可迁移架构用于可扩展图像识别，见：IEEE 计算机视觉与模式识别会议论文集，2018
    年，第 8697–8710 页。'
- en: '[16] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J. Mueller,
    R. Manmatha, et al., Resnest: Split-attention networks, arXiv preprint arXiv:2004.08955.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J.
    Mueller, R. Manmatha 等，Resnest: 分割注意力网络，arXiv 预印本 arXiv:2004.08955。'
- en: '[17] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou, Training
    data-efficient image transformers & distillation through attention, in: International
    Conference on Machine Learning, PMLR, 2021, pp. 10347–10357.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, H. Jégou，训练数据高效的图像变换器及通过注意力进行蒸馏，见：国际机器学习会议，PMLR，2021
    年，第 10347–10357 页。'
- en: '[18] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo, Swin
    transformer: Hierarchical vision transformer using shifted windows, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10012–10022.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo，Swin transformer:
    使用移位窗口的分层视觉变换器，见：IEEE/CVF 国际计算机视觉会议论文集，2021 年，第 10012–10022 页。'
- en: '[19] S. Ren, K. He, R. Girshick, J. Sun, Faster r-cnn: Towards real-time object
    detection with region proposal networks, Advances in neural information processing
    systems 28.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Ren, K. He, R. Girshick, J. Sun，Faster r-cnn: 面向实时对象检测的区域提议网络，神经信息处理系统进展
    28。'
- en: '[20] J. Redmon, S. Divvala, R. Girshick, A. Farhadi, You only look once: Unified,
    real-time object detection, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2016, pp. 779–788.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Redmon, S. Divvala, R. Girshick, A. Farhadi，你只看一次: 统一的实时对象检测，见：IEEE
    计算机视觉与模式识别会议论文集，2016 年，第 779–788 页。'
- en: '[21] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg,
    Ssd: Single shot multibox detector, in: European conference on computer vision,
    Springer, 2016, pp. 21–37.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, A. C. Berg，SSD:
    单次多框检测器，见：欧洲计算机视觉会议，Springer，2016 年，第 21–37 页。'
- en: '[22] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao, Yolov4: Optimal speed and accuracy
    of object detection, arXiv preprint arXiv:2004.10934.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Bochkovskiy, C.-Y. Wang, H.-Y. M. Liao，Yolov4: 对象检测的最佳速度和精度，arXiv 预印本
    arXiv:2004.10934。'
- en: '[23] K. K. Singh, Y. J. Lee, Hide-and-seek: Forcing a network to be meticulous
    for weakly-supervised object and action localization, in: 2017 IEEE international
    conference on computer vision (ICCV), IEEE, 2017, pp. 3544–3553.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] K. K. Singh, Y. J. Lee，藏匿与寻找: 迫使网络在弱监督对象和动作定位中细致，见：2017 IEEE 国际计算机视觉会议
    (ICCV)，IEEE，2017 年，第 3544–3553 页。'
- en: '[24] T. DeVries, G. W. Taylor, Improved regularization of convolutional neural
    networks with cutout, arXiv preprint arXiv:1708.04552.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. DeVries, G. W. Taylor，通过切除改进卷积神经网络的正则化，arXiv 预印本 arXiv:1708.04552。'
- en: '[25] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang, Random erasing data augmentation,
    in: Proceedings of the AAAI conference on artificial intelligence, Vol. 34, 2020,
    pp. 13001–13008.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Z. Zhong, L. Zheng, G. Kang, S. Li, Y. Yang，随机擦除数据增强，见：AAAI 人工智能会议论文集，第
    34 卷，2020 年，第 13001–13008 页。'
- en: '[26] P. Chen, S. Liu, H. Zhao, J. Jia, Gridmask data augmentation, arXiv preprint
    arXiv:2001.04086.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] P. Chen, S. Liu, H. Zhao, J. Jia，Gridmask 数据增强，arXiv 预印本 arXiv:2001.04086。'
- en: '[27] H. Inoue, Data augmentation by pairing samples for images classification,
    arXiv preprint arXiv:1801.02929.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. Inoue，通过配对样本进行数据增强用于图像分类，arXiv 预印本 arXiv:1801.02929。'
- en: '[28] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz, mixup: Beyond empirical
    risk minimization, arXiv preprint arXiv:1710.09412.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] H. Zhang, M. Cisse, Y. N. Dauphin, D. Lopez-Paz，Mixup: 超越经验风险最小化，arXiv
    预印本 arXiv:1710.09412。'
- en: '[29] Y. Tokozume, Y. Ushiku, T. Harada, Between-class learning for image classification,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    2018, pp. 5486–5494.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Y. Tokozume, Y. Ushiku, T. Harada, 类别间学习用于图像分类，发表于：IEEE 计算机视觉与模式识别会议论文集，2018年，第5486–5494页。'
- en: '[30] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix: Regularization
    strategy to train strong classifiers with localizable features, in: Proceedings
    of the IEEE/CVF international conference on computer vision, 2019, pp. 6023–6032.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, Y. Yoo, Cutmix：用于训练具有可定位特征的强分类器的正则化策略，发表于：IEEE/CVF
    国际计算机视觉会议论文集，2019年，第6023–6032页。'
- en: '[31] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, B. Lakshminarayanan,
    Augmix: A simple method to improve robustness and uncertainty under data shift,
    in: International conference on learning representations, Vol. 1, 2020, p. 6.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, B. Lakshminarayanan,
    Augmix：一种简单的方法来提高数据偏移下的鲁棒性和不确定性，发表于：国际学习表示会议，第1卷，2020年，第6页。'
- en: '[32] J.-H. Kim, W. Choo, H. O. Song, Puzzle mix: Exploiting saliency and local
    statistics for optimal mixup, in: International Conference on Machine Learning,
    PMLR, 2020, pp. 5275–5285.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J.-H. Kim, W. Choo, H. O. Song, Puzzle mix：利用显著性和局部统计进行最佳混合，发表于：国际机器学习会议，PMLR，2020年，第5275–5285页。'
- en: '[33] J. Kim, W. Choo, H. Jeong, H. O. Song, Co-mixup: Saliency guided joint
    mixup with supermodular diversity, in: International Conference on Learning Representations,
    2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Kim, W. Choo, H. Jeong, H. O. Song, Co-mixup：基于显著性引导的联合混合，发表于：国际学习表示会议，2021年。'
- en: '[34] A. Dabouei, S. Soleymani, F. Taherkhani, N. M. Nasrabadi, Supermix: Supervising
    the mixing data augmentation, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 13794–13803.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] A. Dabouei, S. Soleymani, F. Taherkhani, N. M. Nasrabadi, Supermix：监督混合数据增强，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021年，第13794–13803页。'
- en: '[35] K. Baek, D. Bang, H. Shim, Gridmix: Strong regularization through local
    context mapping, Pattern Recognition 109 (2021) 107594.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. Baek, D. Bang, H. Shim, Gridmix：通过局部上下文映射进行强正则化，《模式识别》109 (2021) 107594。'
- en: '[36] D. Dwibedi, I. Misra, M. Hebert, Cut, paste and learn: Surprisingly easy
    synthesis for instance detection, in: Proceedings of the IEEE international conference
    on computer vision, 2017, pp. 1301–1310.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] D. Dwibedi, I. Misra, M. Hebert, 剪切、粘贴和学习：意外简单的实例检测合成，发表于：IEEE 国际计算机视觉会议论文集，2017年，第1301–1310页。'
- en: '[37] G. Georgakis, A. Mousavian, A. C. Berg, J. Kosecka, Synthesizing training
    data for object detection in indoor scenes, arXiv preprint arXiv:1702.07836.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] G. Georgakis, A. Mousavian, A. C. Berg, J. Kosecka, 合成室内场景中的目标检测训练数据，arXiv
    预印本 arXiv:1702.07836。'
- en: '[38] N. Dvornik, J. Mairal, C. Schmid, Modeling visual context is key to augmenting
    object detection datasets, in: Proceedings of the European Conference on Computer
    Vision (ECCV), 2018, pp. 364–380.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] N. Dvornik, J. Mairal, C. Schmid, 建模视觉上下文是增强对象检测数据集的关键，发表于：欧洲计算机视觉会议（ECCV）论文集，2018年，第364–380页。'
- en: '[39] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V.
    Le, B. Zoph, Simple copy-paste is a strong data augmentation method for instance
    segmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, 2021, pp. 2918–2928.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] G. Ghiasi, Y. Cui, A. Srinivas, R. Qian, T.-Y. Lin, E. D. Cubuk, Q. V.
    Le, B. Zoph, 简单的复制-粘贴是一个强大的数据增强方法，适用于实例分割，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2021年，第2918–2928页。'
- en: '[40] Z. Xu, A. Meng, Z. Shi, W. Yang, Z. Chen, L. Huang, Continuous copy-paste
    for one-stage multi-object tracking and segmentation, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, 2021, pp. 15323–15332.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Z. Xu, A. Meng, Z. Shi, W. Yang, Z. Chen, L. Huang, 连续复制-粘贴用于单阶段多目标跟踪和分割，发表于：IEEE/CVF
    国际计算机视觉会议论文集，2021年，第15323–15332页。'
- en: '[41] A. Radford, L. Metz, S. Chintala, Unsupervised representation learning
    with deep convolutional generative adversarial networks, arXiv preprint arXiv:1511.06434.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Radford, L. Metz, S. Chintala, 使用深度卷积生成对抗网络的无监督表示学习，arXiv 预印本 arXiv:1511.06434。'
- en: '[42] A. Madani, M. Moradi, A. Karargyris, T. Syeda-Mahmood, Chest x-ray generation
    and data augmentation for cardiovascular abnormality classification, in: Medical
    Imaging 2018: Image Processing, Vol. 10574, International Society for Optics and
    Photonics, 2018, p. 105741M.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Madani, M. Moradi, A. Karargyris, T. Syeda-Mahmood, 胸部 X 射线生成与数据增强用于心血管异常分类，发表于：医学成像2018：图像处理，第10574卷，国际光学与光子学学会，2018年，第105741M页。'
- en: '[43] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, Synthetic
    data augmentation using gan for improved liver lesion classification, in: 2018
    IEEE 15th international symposium on biomedical imaging (ISBI 2018), IEEE, 2018,
    pp. 289–293.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, H. Greenspan, 使用gan进行合成数据增强以改进肝脏病变分类，见：2018
    IEEE第15届生物医学成像国际研讨会（ISBI 2018），IEEE，2018，第289–293页。'
- en: '[44] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan,
    Gan-based synthetic medical image augmentation for increased cnn performance in
    liver lesion classification, Neurocomputing 321 (2018) 321–331.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] M. Frid-Adar, I. Diamant, E. Klang, M. Amitai, J. Goldberger, H. Greenspan,
    基于gan的合成医学图像增强以提高cnn在肝脏病变分类中的表现，《Neurocomputing》321 (2018) 321–331。'
- en: '[45] T. Tran, T. Pham, G. Carneiro, L. Palmer, I. Reid, A bayesian data augmentation
    approach for learning deep models, Advances in neural information processing systems
    30.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] T. Tran, T. Pham, G. Carneiro, L. Palmer, I. Reid, 一种贝叶斯数据增强方法用于学习深度模型，《神经信息处理系统进展》30。'
- en: '[46] G. Douzas, F. Bacao, Effective data generation for imbalanced learning
    using conditional generative adversarial networks, Expert Systems with applications
    91 (2018) 464–471.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] G. Douzas, F. Bacao, 使用条件生成对抗网络进行有效的数据生成以应对不平衡学习，《专家系统应用》91 (2018) 464–471。'
- en: '[47] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, C. Malossi, Bagan: Data
    augmentation with balancing gan, arXiv preprint arXiv:1803.09655.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G. Mariani, F. Scheidegger, R. Istrate, C. Bekas, C. Malossi, Bagan: 使用平衡gan的数据增强，arXiv预印本arXiv:1803.09655。'
- en: '[48] A. Antoniou, A. Storkey, H. Edwards, Data augmentation generative adversarial
    networks, arXiv preprint arXiv:1711.04340.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Antoniou, A. Storkey, H. Edwards, 数据增强生成对抗网络，arXiv预印本arXiv:1711.04340。'
- en: '[49] A. Ali-Gombe, E. Elyan, Mfc-gan: class-imbalanced dataset classification
    using multiple fake class generative adversarial network, Neurocomputing 361 (2019)
    212–221.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. Ali-Gombe, E. Elyan, Mfc-gan: 使用多个虚假类别生成对抗网络进行类别不平衡数据集分类，《Neurocomputing》361
    (2019) 212–221。'
- en: '[50] H. Yang, Y. Zhou, Ida-gan: A novel imbalanced data augmentation gan, in:
    2020 25th International Conference on Pattern Recognition (ICPR), IEEE, 2021,
    pp. 8299–8305.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] H. Yang, Y. Zhou, Ida-gan: 一种新颖的不平衡数据增强gan，见：2020年第25届国际模式识别大会（ICPR），IEEE，2021，第8299–8305页。'
- en: '[51] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, R. Webb, Learning
    from simulated and unsupervised images through adversarial training, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2107–2116.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, R. Webb, 通过对抗训练从模拟和无监督图像中学习，见：IEEE计算机视觉与模式识别会议论文集，2017，第2107–2116页。'
- en: '[52] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, S.-H. Lai, Auggan:
    Cross domain adaptation with gan-based data augmentation, in: Proceedings of the
    European Conference on Computer Vision (ECCV), 2018, pp. 718–731.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, S.-H. Lai, Auggan:
    基于gan的数据增强的跨领域适应，见：欧洲计算机视觉会议（ECCV）论文集，2018，第718–731页。'
- en: '[53] Y. Zhu, M. Aoun, M. Krijn, J. Vanschoren, H. T. Campus, Data augmentation
    using conditional generative adversarial networks for leaf counting in arabidopsis
    plants., in: BMVC, 2018, p. 324.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Zhu, M. Aoun, M. Krijn, J. Vanschoren, H. T. Campus, 使用条件生成对抗网络进行叶子计数的数据增强，见：BMVC，2018，第324页。'
- en: '[54] P. T. Jackson, A. A. Abarghouei, S. Bonner, T. P. Breckon, B. Obara, Style
    augmentation: data augmentation via style randomization., in: CVPR Workshops,
    Vol. 6, 2019, pp. 10–11.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] P. T. Jackson, A. A. Abarghouei, S. Bonner, T. P. Breckon, B. Obara, 风格增强：通过风格随机化进行数据增强，见：CVPR研讨会，第6卷，2019，第10–11页。'
- en: '[55] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, W. Brendel,
    Imagenet-trained cnns are biased towards texture; increasing shape bias improves
    accuracy and robustness, arXiv preprint arXiv:1811.12231.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wichmann, W. Brendel,
    Imagenet训练的cnn对纹理存在偏见；增加形状偏见提高准确性和鲁棒性，arXiv预印本arXiv:1811.12231。'
- en: '[56] X. Zhu, Y. Liu, J. Li, T. Wan, Z. Qin, Emotion classification with data
    augmentation using generative adversarial networks, in: Pacific-Asia conference
    on knowledge discovery and data mining, Springer, 2018, pp. 349–360.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Zhu, Y. Liu, J. Li, T. Wan, Z. Qin, 使用生成对抗网络进行情感分类的数据增强，见：太平洋-亚洲知识发现与数据挖掘会议，Springer，2018，第349–360页。'
- en: '[57] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary, M. Marder, A. Kumar, R. Feris,
    R. Giryes, A. Bronstein, Delta-encoder: an effective sample synthesis method for
    few-shot object recognition, Advances in Neural Information Processing Systems
    31.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] E. Schwartz, L. Karlinsky, J. Shtok, S. Harary, M. Marder, A. Kumar, R.
    Feris, R. Giryes, A. Bronstein, Delta-encoder: 一种有效的样本合成方法用于少样本对象识别，《神经信息处理系统进展》31。'
- en: '[58] Y. Li, Q. Yu, M. Tan, J. Mei, P. Tang, W. Shen, A. Yuille, et al., Shape-texture
    debiased neural network training, in: International Conference on Learning Representations,
    2020.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Li, Q. Yu, M. Tan, J. Mei, P. Tang, W. Shen, A. Yuille, 等., 形状-纹理去偏神经网络训练,
    见：国际学习表示会议, 2020。'
- en: '[59] M. Hong, J. Choi, G. Kim, Stylemix: Separating content and style for enhanced
    data augmentation, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2021, pp. 14862–14870.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Hong, J. Choi, G. Kim, Stylemix: 分离内容与风格以增强数据增强, 见：IEEE/CVF计算机视觉与模式识别会议论文集,
    2021, 第14862–14870页。'
- en: '[60] Z. Zheng, Z. Yu, Y. Wu, H. Zheng, B. Zheng, M. Lee, Generative adversarial
    network with multi-branch discriminator for imbalanced cross-species image-to-image
    translation, Neural Networks 141 (2021) 355–371.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Z. Zheng, Z. Yu, Y. Wu, H. Zheng, B. Zheng, M. Lee, 基于多分支判别器的不平衡跨物种图像到图像翻译的生成对抗网络,
    神经网络141 (2021) 355–371。'
- en: '[61] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le, Autoaugment: Learning
    augmentation strategies from data, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2019, pp. 113–123.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, Q. V. Le, Autoaugment: 从数据中学习增强策略,
    见：IEEE/CVF计算机视觉与模式识别会议论文集, 2019, 第113–123页。'
- en: '[62] S. Lim, I. Kim, T. Kim, C. Kim, S. Kim, Fast autoaugment, Advances in
    Neural Information Processing Systems 32.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Lim, I. Kim, T. Kim, C. Kim, S. Kim, 快速自动增强, 神经信息处理系统32。'
- en: '[63] D. Ho, E. Liang, X. Chen, I. Stoica, P. Abbeel, Population based augmentation:
    Efficient learning of augmentation policy schedules, in: International Conference
    on Machine Learning, PMLR, 2019, pp. 2731–2741.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] D. Ho, E. Liang, X. Chen, I. Stoica, P. Abbeel, 基于人群的增强：高效学习增强策略时间表, 见：国际机器学习会议,
    PMLR, 2019, 第2731–2741页。'
- en: '[64] R. Hataya, J. Zdenek, K. Yoshizoe, H. Nakayama, Faster autoaugment: Learning
    augmentation strategies using backpropagation, in: European Conference on Computer
    Vision, Springer, 2020, pp. 1–16.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Hataya, J. Zdenek, K. Yoshizoe, H. Nakayama, Faster autoaugment: 使用反向传播学习增强策略,
    见：欧洲计算机视觉会议, Springer, 2020, 第1–16页。'
- en: '[65] E. D. Cubuk, B. Zoph, J. Shlens, Q. V. Le, Randaugment: Practical automated
    data augmentation with a reduced search space, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops, 2020, pp. 702–703.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] E. D. Cubuk, B. Zoph, J. Shlens, Q. V. Le, Randaugment: 实用的自动数据增强与简化的搜索空间,
    见：IEEE/CVF计算机视觉与模式识别会议研讨会论文集, 2020, 第702–703页。'
- en: '[66] R. Hataya, J. Zdenek, K. Yoshizoe, H. Nakayama, Meta approach to data
    augmentation optimization, in: Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision, 2022, pp. 2574–2583.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] R. Hataya, J. Zdenek, K. Yoshizoe, H. Nakayama, 数据增强优化的元方法, 见：IEEE/CVF计算机视觉应用冬季会议论文集,
    2022, 第2574–2583页。'
- en: '[67] B. Zoph, E. D. Cubuk, G. Ghiasi, T.-Y. Lin, J. Shlens, Q. V. Le, Learning
    data augmentation strategies for object detection, in: European conference on
    computer vision, Springer, 2020, pp. 566–583.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] B. Zoph, E. D. Cubuk, G. Ghiasi, T.-Y. Lin, J. Shlens, Q. V. Le, 学习物体检测的数据增强策略,
    见：欧洲计算机视觉会议, Springer, 2020, 第566–583页。'
- en: '[68] P. Li, X. Liu, X. Xie, Learning sample-specific policies for sequential
    image augmentation, in: Proceedings of the 29th ACM International Conference on
    Multimedia, 2021, pp. 4491–4500.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] P. Li, X. Liu, X. Xie, 学习特定样本的序列图像增强策略, 见：第29届ACM国际多媒体会议论文集, 2021, 第4491–4500页。'
- en: '[69] A. Fawzi, H. Samulowitz, D. Turaga, P. Frossard, Adaptive data augmentation
    for image classification, in: 2016 IEEE international conference on image processing
    (ICIP), Ieee, 2016, pp. 3688–3692.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Fawzi, H. Samulowitz, D. Turaga, P. Frossard, 图像分类的自适应数据增强, 见：2016
    IEEE国际图像处理会议（ICIP）, IEEE, 2016, 第3688–3692页。'
- en: '[70] A. J. Ratner, H. Ehrenberg, Z. Hussain, J. Dunnmon, C. Ré, Learning to
    compose domain-specific transformations for data augmentation, Advances in neural
    information processing systems 30.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] A. J. Ratner, H. Ehrenberg, Z. Hussain, J. Dunnmon, C. Ré, 学习组合领域特定变换用于数据增强,
    神经信息处理系统30。'
- en: '[71] Z. Tang, X. Peng, T. Li, Y. Zhu, D. N. Metaxas, Adatransform: Adaptive
    data transformation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2019, pp. 2998–3006.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Z. Tang, X. Peng, T. Li, Y. Zhu, D. N. Metaxas, Adatransform: 自适应数据变换,
    见：IEEE/CVF国际计算机视觉会议论文集, 2019, 第2998–3006页。'
- en: '[72] X. Zhang, Q. Wang, J. Zhang, Z. Zhong, Adversarial autoaugment, arXiv
    preprint arXiv:1912.11188.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X. Zhang, Q. Wang, J. Zhang, Z. Zhong, 对抗性自动增强, arXiv预印本 arXiv:1912.11188。'
- en: '[73] D. Lee, H. Park, T. Pham, C. D. Yoo, Learning augmentation network via
    influence functions, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2020, pp. 10961–10970.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] D. Lee, H. Park, T. Pham, C. D. Yoo，《通过影响函数学习增强网络》，发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，2020年，第10961–10970页。'
- en: '[74] T. Takase, R. Karakida, H. Asoh, Self-paced data augmentation for training
    neural networks, Neurocomputing 442 (2021) 296–306.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] T. Takase, R. Karakida, H. Asoh，《自适应数据增强用于训练神经网络》，《神经计算》442（2021）296–306。'
- en: '[75] V. Vapnik, Principles of risk minimization for learning theory, Advances
    in neural information processing systems 4.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] V. Vapnik，《学习理论的风险最小化原则》，《神经信息处理系统进展》4卷。'
- en: '[76] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding deep
    learning requires rethinking generalization (2016), arXiv preprint arXiv:1611.03530.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals，《理解深度学习需要重新思考泛化》（2016），arXiv预印本arXiv:1611.03530。'
- en: '[77] V. Vapnik, The nature of statistical learning theory, Springer science
    & business media, 1999.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] V. Vapnik，《统计学习理论的本质》，施普林格科学与商业媒体，1999年。'
- en: '[78] I. Goodfellow, Y. Bengio, A. Courville, Deep learning, MIT press, 2016.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] I. Goodfellow, Y. Bengio, A. Courville，《深度学习》，MIT出版社，2016年。'
- en: '[79] O. Chapelle, J. Weston, L. Bottou, V. Vapnik, Vicinal risk minimization,
    Advances in neural information processing systems 13.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] O. Chapelle, J. Weston, L. Bottou, V. Vapnik，《邻域风险最小化》，《神经信息处理系统进展》13卷。'
- en: '[80] M. P. Ekstrom, Digital image processing techniques, Vol. 2, Academic Press,
    2012.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] M. P. Ekstrom，《数字图像处理技术》，第2卷，学术出版社，2012年。'
- en: '[81] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, B. Schiele, The cityscapes dataset for semantic urban scene
    understanding, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2016, pp. 3213–3223.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, B. Schiele，《Cityscapes数据集用于语义城市场景理解》，发表于《IEEE计算机视觉与模式识别会议论文集》，2016年，第3213–3223页。'
- en: '[82] C.-Y. Wang, A. Bochkovskiy, H.-Y. M. Liao, Scaled-yolov4: Scaling cross
    stage partial network, in: Proceedings of the IEEE/cvf conference on computer
    vision and pattern recognition, 2021, pp. 13029–13038.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C.-Y. Wang, A. Bochkovskiy, H.-Y. M. Liao，《Scaled-YOLOv4：扩展跨阶段部分网络》，发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，2021年，第13029–13038页。'
- en: '[83] W. Ma, Y. Wu, F. Cen, G. Wang, Mdfn: Multi-scale deep feature learning
    network for object detection, Pattern Recognition 100 (2020) 107149.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] W. Ma, Y. Wu, F. Cen, G. Wang，《MDFN：用于目标检测的多尺度深度特征学习网络》，《模式识别》100（2020）107149。'
- en: '[84] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation
    using cycle-consistent adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, 2017, pp. 2223–2232.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros，《使用循环一致对抗网络进行未配对的图像到图像翻译》，发表于《IEEE国际计算机视觉会议论文集》，2017年，第2223–2232页。'
- en: '[85] W. Xu, K. Shawn, G. Wang, Toward learning a unified many-to-many mapping
    for diverse image translation, Pattern Recognition 93 (2019) 570–580.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] W. Xu, K. Shawn, G. Wang，《朝向学习统一的多对多映射以进行多样化图像翻译》，《模式识别》93（2019）570–580。'
- en: '[86] T. Chen, S. Kornblith, M. Norouzi, G. Hinton, A simple framework for contrastive
    learning of visual representations, in: International conference on machine learning,
    PMLR, 2020, pp. 1597–1607.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] T. Chen, S. Kornblith, M. Norouzi, G. Hinton，《视觉表征对比学习的简单框架》，发表于《国际机器学习会议》，PMLR，2020年，第1597–1607页。'
- en: '[87] J. Yang, Gridmask based data augmentation for bengali handwritten grapheme
    classification, in: Proceedings of the 2020 2nd International Conference on Intelligent
    Medicine and Image Processing, 2020, pp. 98–102.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Yang，《基于Gridmask的数据增强用于孟加拉手写字符分类》，发表于《2020年第二届国际智能医学与图像处理会议论文集》，2020年，第98–102页。'
- en: '[88] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, Y. Bengio, Generative adversarial nets, Advances in neural information
    processing systems 27.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, Y. Bengio，《生成对抗网络》，《神经信息处理系统进展》27卷。'
- en: '[89] M. Mirza, S. Osindero, Conditional generative adversarial nets, arXiv
    preprint arXiv:1411.1784.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Mirza, S. Osindero，《条件生成对抗网络》，arXiv预印本arXiv:1411.1784。'
- en: '[90] A. Odena, C. Olah, J. Shlens, Conditional image synthesis with auxiliary
    classifier gans, in: International conference on machine learning, PMLR, 2017,
    pp. 2642–2651.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Odena, C. Olah, J. Shlens，《使用辅助分类器GANs进行条件图像合成》，发表于《国际机器学习会议》，PMLR，2017年，第2642–2651页。'
- en: '[91] X. Huang, S. Belongie, Arbitrary style transfer in real-time with adaptive
    instance normalization, in: Proceedings of the IEEE international conference on
    computer vision, 2017, pp. 1501–1510.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] X. Huang, S. Belongie, 实时任意风格迁移与自适应实例归一化, 见: 《IEEE 国际计算机视觉会议论文集》, 2017,
    页 1501–1510.'
- en: '[92] V. Dumoulin, J. Shlens, M. Kudlur, A learned representation for artistic
    style, arXiv preprint arXiv:1610.07629.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] V. Dumoulin, J. Shlens, M. Kudlur, 一种用于艺术风格的学习表示, arXiv 预印本 arXiv:1610.07629.'
- en: '[93] R. Li, W. Cao, Q. Jiao, S. Wu, H.-S. Wong, Simplified unsupervised image
    translation for semantic segmentation adaptation, Pattern Recognition 105 (2020)
    107343.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] R. Li, W. Cao, Q. Jiao, S. Wu, H.-S. Wong, 简化的无监督图像翻译用于语义分割适应, 《模式识别》
    105 (2020) 107343.'
- en: '[94] H.-Y. Lee, H.-Y. Tseng, Q. Mao, J.-B. Huang, Y.-D. Lu, M. Singh, M.-H.
    Yang, Drit++: Diverse image-to-image translation via disentangled representations,
    International Journal of Computer Vision 128 (10) (2020) 2402–2417.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] H.-Y. Lee, H.-Y. Tseng, Q. Mao, J.-B. Huang, Y.-D. Lu, M. Singh, M.-H.
    Yang, Drit++: 通过解耦表示进行多样化图像到图像的转换, 《计算机视觉国际期刊》 128 (10) (2020) 2402–2417.'
- en: '[95] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, Semantic image synthesis with
    spatially-adaptive normalization, in: Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, 2019, pp. 2337–2346.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] T. Park, M.-Y. Liu, T.-C. Wang, J.-Y. Zhu, 具有空间自适应归一化的语义图像合成, 见: 《IEEE/CVF
    计算机视觉与模式识别会议论文集》, 2019, 页 2337–2346.'
- en: '[96] T. Fontanini, E. Iotti, L. Donati, A. Prati, Metalgan: Multi-domain label-less
    image synthesis using cgans and meta-learning, Neural Networks 131 (2020) 185–200.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T. Fontanini, E. Iotti, L. Donati, A. Prati, Metalgan: 使用cgan和元学习的多领域无标签图像合成,
    《神经网络》 131 (2020) 185–200.'
- en: '[97] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, Proximal policy
    optimization algorithms, arXiv preprint arXiv:1707.06347.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, O. Klimov, 近端策略优化算法,
    arXiv 预印本 arXiv:1707.06347.'
- en: '[98] J. Lemley, S. Bazrafkan, P. Corcoran, Smart augmentation learning an optimal
    data augmentation strategy, Ieee Access 5 (2017) 5858–5869.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Lemley, S. Bazrafkan, P. Corcoran, 智能增强学习最优数据增强策略, 《IEEE Access》 5
    (2017) 5858–5869.'
- en: '[99] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A
    large-scale hierarchical image database, in: 2009 IEEE conference on computer
    vision and pattern recognition, Ieee, 2009, pp. 248–255.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: 大规模分层图像数据库,
    见: 2009 IEEE 计算机视觉与模式识别会议, IEEE, 2009, 页 248–255.'
- en: '[100] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    C. L. Zitnick, Microsoft coco: Common objects in context, in: European conference
    on computer vision, Springer, 2014, pp. 740–755.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P.
    Dollár, C. L. Zitnick, Microsoft coco: 语境中的常见对象, 见: 《欧洲计算机视觉会议》, Springer, 2014,
    页 740–755.'
- en: '[101] B. Hanin, Y. Sun, How data augmentation affects optimization for linear
    regression, Advances in Neural Information Processing Systems 34.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] B. Hanin, Y. Sun, 数据增强如何影响线性回归的优化, 《神经信息处理系统进展》 34.'
- en: '[102] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, C. Ré, A kernel theory
    of modern data augmentation, in: International Conference on Machine Learning,
    PMLR, 2019, pp. 1528–1537.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] T. Dao, A. Gu, A. Ratner, V. Smith, C. De Sa, C. Ré, 现代数据增强的核理论, 见: 《国际机器学习会议》,
    PMLR, 2019, 页 1528–1537.'
- en: '[103] S. Chen, E. Dobriban, J. Lee, A group-theoretic framework for data augmentation,
    Advances in Neural Information Processing Systems 33 (2020) 21321–21333.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] S. Chen, E. Dobriban, J. Lee, 数据增强的群论框架, 《神经信息处理系统进展》 33 (2020) 21321–21333.'
- en: '[104] R. Gontijo-Lopes, S. Smullin, E. D. Cubuk, E. Dyer, Tradeoffs in data
    augmentation: An empirical study, in: International Conference on Learning Representations,
    2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] R. Gontijo-Lopes, S. Smullin, E. D. Cubuk, E. Dyer, 数据增强中的权衡: 实证研究, 见:
    《国际学习表示会议》, 2020.'
- en: '[105] N. Komodakis, S. Gidaris, Unsupervised representation learning by predicting
    image rotations, in: International Conference on Learning Representations (ICLR),
    2018.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] N. Komodakis, S. Gidaris, 通过预测图像旋转进行无监督表示学习, 见: 《国际学习表示会议 (ICLR)》, 2018.'
- en: '[106] C. Doersch, A. Gupta, A. A. Efros, Unsupervised visual representation
    learning by context prediction, in: Proceedings of the IEEE international conference
    on computer vision, 2015, pp. 1422–1430.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] C. Doersch, A. Gupta, A. A. Efros, 通过上下文预测进行无监督视觉表示学习, 见: 《IEEE 国际计算机视觉会议论文集》,
    2015, 页 1422–1430.'
- en: '[107] M. Ye, X. Zhang, P. C. Yuen, S.-F. Chang, Unsupervised embedding learning
    via invariant and spreading instance feature, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2019, pp. 6210–6219.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] M. Ye, X. Zhang, P. C. Yuen, S.-F. Chang，在：**通过不变和扩散实例特征进行无监督嵌入学习**，《IEEE/CVF计算机视觉与模式识别会议论文集》，2019，页码
    6210–6219。'
- en: '[108] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
    C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar, et al., Bootstrap your
    own latent-a new approach to self-supervised learning, Advances in Neural Information
    Processing Systems 33 (2020) 21271–21284.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya,
    C. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar 等，**Bootstrap your own
    latent**——自监督学习的新方法，《神经信息处理系统进展》33（2020）21271–21284。'
- en: '[109] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A. Joulin,
    Emerging properties in self-supervised vision transformers, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2021, pp. 9650–9660.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, A.
    Joulin，在：**自监督视觉变换器中的新兴属性**，《IEEE/CVF计算机视觉国际会议论文集》，2021，页码 9650–9660。'
- en: '[110] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang,
    C. Raffel, Remixmatch: Semi-supervised learning with distribution alignment and
    augmentation anchoring, arXiv preprint arXiv:1911.09785.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang,
    C. Raffel, **Remixmatch**：通过分布对齐和增强锚定进行半监督学习，arXiv 预印本 arXiv:1911.09785。'
- en: '[111] Q. Xie, Z. Dai, E. Hovy, T. Luong, Q. Le, Unsupervised data augmentation
    for consistency training, Advances in Neural Information Processing Systems 33
    (2020) 6256–6268.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Q. Xie, Z. Dai, E. Hovy, T. Luong, Q. Le，**无监督数据增强**用于一致性训练，《神经信息处理系统进展》33（2020）6256–6268。'
- en: '[112] Y. Wang, G. Huang, S. Song, X. Pan, Y. Xia, C. Wu, Regularizing deep
    networks with semantic data augmentation, IEEE Transactions on Pattern Analysis
    and Machine Intelligence.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Wang, G. Huang, S. Song, X. Pan, Y. Xia, C. Wu, 使用语义数据增强正则化深度网络，《IEEE模式分析与机器智能汇刊》。'
- en: '[113] Q. Wang, F. Meng, T. P. Breckon, Data augmentation with norm-vae for
    unsupervised domain adaptation, arXiv preprint arXiv:2012.00848.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Q. Wang, F. Meng, T. P. Breckon，**使用规范变分自编码器的**数据增强进行无监督领域适应，arXiv 预印本
    arXiv:2012.00848。'
- en: '[114] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    Y. Bengio, Manifold mixup: Better representations by interpolating hidden states,
    in: International Conference on Machine Learning, PMLR, 2019, pp. 6438–6447.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz,
    Y. Bengio，**Manifold mixup**：通过插值隐藏状态来获得更好的表示，在：国际机器学习会议，PMLR，2019，页码 6438–6447。'
- en: '[115] F. Cen, X. Zhao, W. Li, G. Wang, Deep feature augmentation for occluded
    image classification, Pattern Recognition 111 (2021) 107737.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] F. Cen, X. Zhao, W. Li, G. Wang，**深度特征增强**用于遮挡图像分类，《模式识别》111（2021）107737。'
