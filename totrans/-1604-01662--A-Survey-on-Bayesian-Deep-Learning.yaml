- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:09:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:09:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1604.01662] A Survey on Bayesian Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1604.01662] 关于贝叶斯深度学习的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1604.01662](https://ar5iv.labs.arxiv.org/html/1604.01662)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1604.01662](https://ar5iv.labs.arxiv.org/html/1604.01662)
- en: A Survey on Bayesian Deep Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于贝叶斯深度学习的调查
- en: Hao Wang [hoguewang@gmail.com](mailto:hoguewang@gmail.com) Massachusetts Institute
    of TechnologyUSA  and  Dit-Yan Yeung [dyyeung}@cse.ust.hk](mailto:dyyeung%7D@cse.ust.hk)
    Hong Kong University of Science and TechnologyP.O. Box 1212Hong Kong
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Wang [hoguewang@gmail.com](mailto:hoguewang@gmail.com) 麻省理工学院，美国  和  Dit-Yan
    Yeung [dyyeung}@cse.ust.hk](mailto:dyyeung%7D@cse.ust.hk) 香港科技大学，P.O. Box 1212，香港
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: A comprehensive artificial intelligence system needs to not only perceive the
    environment with different ‘senses’ (e.g., seeing and hearing) but also infer
    the world’s conditional (or even causal) relations and corresponding uncertainty.
    The past decade has seen major advances in many perception tasks such as visual
    object recognition and speech recognition using deep learning models. For higher-level
    inference, however, probabilistic graphical models with their Bayesian nature
    are still more powerful and flexible. In recent years, *Bayesian deep learning*
    has emerged as a unified probabilistic framework to tightly integrate deep learning
    and Bayesian models¹¹1See a curated and updating list of papers related to Bayesian
    deep learning at [https://github.com/js05212/BayesianDeepLearning-Survey](https://github.com/js05212/BayesianDeepLearning-Survey)..
    In this general framework, the perception of text or images using deep learning
    can boost the performance of higher-level inference and in turn, the feedback
    from the inference process is able to enhance the perception of text or images.
    This survey provides a comprehensive introduction to *Bayesian deep learning*
    and reviews its recent applications on recommender systems, topic models, control,
    etc. Besides, we also discuss the relationship and differences between Bayesian
    deep learning and other related topics such as Bayesian treatment of neural networks.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全面的人工智能系统不仅需要通过不同的“感官”（例如，视觉和听觉）来感知环境，还需要推断世界的条件（甚至因果）关系及相应的不确定性。过去十年，在视觉物体识别和语音识别等许多感知任务中，深度学习模型取得了重大进展。然而，对于更高层次的推断，具有贝叶斯特性的概率图模型仍然更为强大和灵活。近年来，*贝叶斯深度学习*作为一个统一的概率框架出现，以紧密结合深度学习和贝叶斯模型¹¹1请参见与贝叶斯深度学习相关的论文的策划和更新列表，[https://github.com/js05212/BayesianDeepLearning-Survey](https://github.com/js05212/BayesianDeepLearning-Survey)。在这一通用框架中，使用深度学习对文本或图像的感知可以提升更高层次推断的性能，反过来，推断过程中的反馈也能够增强对文本或图像的感知。本调查提供了对*贝叶斯深度学习*的全面介绍，并回顾了其在推荐系统、主题模型、控制等领域的近期应用。此外，我们还讨论了贝叶斯深度学习与其他相关主题（如贝叶斯神经网络处理）的关系和差异。
- en: 'Deep Learning, Bayesian Networks, Probabilistic Graphical Models, Generative
    Models^†^†copyright: acmlicensed^†^†journal: CSUR^†^†journalyear: 2020^†^†journalvolume:
    1^†^†journalnumber: 1^†^†article: 1^†^†publicationmonth: 1^†^†price: 15.00^†^†doi:
    10.1145/3409383^†^†conference: ACM Computing Surveys; March, 2020; New York, NY^†^†booktitle:
    ACM Computing Surveys^†^†price: 15.00^†^†isbn: xxx-x-xxxx-xxxx-x/xx/xx^†^†ccs:
    Mathematics of computing Probabilistic representations^†^†ccs: Information systems Data
    mining^†^†ccs: Computing methodologies Neural networks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，贝叶斯网络，概率图模型，生成模型^†^†版权：acmlicensed^†^†期刊：CSUR^†^†期刊年份：2020^†^†期刊卷号：1^†^†期刊期号：1^†^†文章号：1^†^†出版月份：1^†^†价格：15.00^†^†doi：10.1145/3409383^†^†会议：ACM
    Computing Surveys；2020年3月；纽约，NY^†^†书名：ACM Computing Surveys^†^†价格：15.00^†^†isbn：xxx-x-xxxx-xxxx-x/xx/xx^†^†ccs：计算数学 概率表示^†^†ccs：信息系统 数据挖掘^†^†ccs：计算方法论 神经网络
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Over the past decade, deep learning has achieved significant success in many
    popular perception tasks including visual object recognition, text understanding,
    and speech recognition. These tasks correspond to artificial intelligence (AI)
    systems’ ability to *see*, *read*, and *hear*, respectively, and they are undoubtedly
    indispensable for AI to effectively perceive the environment. However, in order
    to build a practical and comprehensive AI system, simply being able to perceive
    is far from sufficient. It should, above all, possess the ability of *thinking*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，深度学习在许多流行的感知任务中取得了显著成功，包括视觉物体识别、文本理解和语音识别。这些任务分别对应人工智能（AI）系统的*看*、*读*和*听*的能力，它们无疑是AI有效感知环境不可或缺的。然而，要建立一个实用且全面的AI系统，仅仅具备感知能力远远不够。它应当具备*思考*的能力。
- en: 'A typical example is medical diagnosis, which goes far beyond simple perception:
    besides *seeing* visible symptoms (or medical images from CT) and *hearing* descriptions
    from patients, a doctor also has to look for relations among all the symptoms
    and preferably infer their corresponding etiology. Only after that can the doctor
    provide medical advice for the patients. In this example, although the abilities
    of *seeing* and *hearing* allow the doctor to acquire information from the patients,
    it is the *thinking* part that defines a doctor. Specifically, the ability of
    *thinking* here could involve identifying conditional dependencies, causal inference,
    logic deduction, and dealing with uncertainty, which are apparently beyond the
    capability of conventional deep learning methods. Fortunately, another machine
    learning paradigm, probabilistic graphical models (PGM), excels at probabilistic
    or causal inference and at dealing with uncertainty. The problem is that PGM is
    not as good as deep learning models at perception tasks, which usually involve
    large-scale and high-dimensional signals (e.g., images and videos). To address
    this problem, it is therefore a natural choice to unify deep learning and PGM
    within a principled probabilistic framework, which we call *Bayesian deep learning*
    (BDL) in this paper.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的例子是医疗诊断，它远远超出了简单的感知：除了*看*可见症状（或来自CT的医学影像）和*听*患者的描述外，医生还必须寻找所有症状之间的关系，并尽可能推断其对应的病因。只有在此之后，医生才能为患者提供医疗建议。在这个例子中，虽然*看*和*听*的能力使医生能够从患者那里获取信息，但*思考*的部分定义了医生。具体来说，这里的*思考*能力可能涉及识别条件依赖、因果推断、逻辑推理和处理不确定性，这些显然超出了传统深度学习方法的能力。幸运的是，另一种机器学习范式——概率图模型（PGM）在概率或因果推断和处理不确定性方面表现出色。问题在于，PGM在感知任务方面不如深度学习模型好，这些任务通常涉及大规模和高维度的信号（例如图像和视频）。因此，在原则性的概率框架内统一深度学习和PGM是一个自然的选择，我们在本文中称之为*贝叶斯深度学习*（BDL）。
- en: In the example above, the *perception task* involves perceiving the patient’s
    symptoms (e.g., by *seeing* medical images), while the *inference task* involves
    handling conditional dependencies, causal inference, logic deduction, and uncertainty.
    With the principled integration in Bayesian deep learning, the perception task
    and inference task are regarded as a whole and can benefit from each other. Concretely,
    being able to see the medical image could help with the doctor’s diagnosis and
    inference. On the other hand, diagnosis and inference can, in turn, help understand
    the medical image. Suppose the doctor may not be sure about what a dark spot in
    a medical image is, but if she is able to *infer* the etiology of the symptoms
    and disease, it can help her better decide whether the dark spot is a tumor or
    not.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述例子中，*感知任务*涉及感知患者的症状（例如，通过*看*医学影像），而*推断任务*涉及处理条件依赖、因果推断、逻辑推理和不确定性。通过贝叶斯深度学习中的原则性整合，感知任务和推断任务被视为一个整体，并可以相互受益。具体来说，能够看医学影像可能有助于医生的诊断和推断。另一方面，诊断和推断也可以帮助理解医学影像。假设医生可能不确定医学影像中的黑点是什么，但如果她能够*推断*症状和疾病的病因，这可以帮助她更好地判断黑点是否为肿瘤。
- en: Take recommender systems ([CDL,](#bib.bib121) ; [DBLP:conf/aaai/LuDLX015,](#bib.bib71)
    ; [DBLP:journals/tkde/AdomaviciusK12,](#bib.bib1) ; [ricci2011introduction,](#bib.bib92)
    ; [DBLP:conf/recsys/LiuMLY11,](#bib.bib70) ) as another example. A highly accurate
    recommender system requires (1) thorough understanding of item content (e.g.,
    content in documents and movies) ([DBLP:journals/tkde/Park13,](#bib.bib85) ),
    (2) careful analysis of users’ profiles/preferences ([DBLP:journals/tkde/WeiMJ05,](#bib.bib126)
    ; [DBLP:journals/tkde/YapTP07,](#bib.bib130) ; [DBLP:conf/aaai/ZhengCZXY10,](#bib.bib134)
    ), and (3) proper evaluation of similarity among users ([DBLP:journals/tkde/CaiLLMTL14,](#bib.bib12)
    ; [DBLP:journals/tkde/TangQZX13,](#bib.bib109) ; [DBLP:journals/tkde/HornickT12,](#bib.bib46)
    ; [DBLP:journals/tkde/BartoliniZP11,](#bib.bib3) ). Deep learning with its ability
    to efficiently process dense high-dimensional data such as movie content is good
    at the first subtask, while PGM specializing in modeling conditional dependencies
    among users, items, and ratings (see Figure [7](#S5.F7 "Figure 7 ‣ 5.1.1\. Collaborative
    Deep Learning ‣ 5.1\. Supervised Bayesian Deep Learning for Recommender Systems
    ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning")
    as an example, where ${\bf u}$, ${\bf v}$, and ${\bf R}$ are user latent vectors,
    item latent vectors, and ratings, respectively) excels at the other two. Hence
    unifying them two in a single principled probabilistic framework gets us the best
    of both worlds. Such integration also comes with additional benefit that uncertainty
    in the recommendation process is handled elegantly. What’s more, one can also
    derive Bayesian treatments for concrete models, leading to more robust predictions ([CDL,](#bib.bib121)
    ; [ColVAE,](#bib.bib68) ).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以推荐系统（[CDL,](#bib.bib121)；[DBLP:conf/aaai/LuDLX015,](#bib.bib71)；[DBLP:journals/tkde/AdomaviciusK12,](#bib.bib1)；[ricci2011introduction,](#bib.bib92)；[DBLP:conf/recsys/LiuMLY11,](#bib.bib70)）为另一个例子。一个高度准确的推荐系统需要（1）对项目内容的透彻理解（例如，文档和电影中的内容）（[DBLP:journals/tkde/Park13,](#bib.bib85)），（2）对用户的档案/偏好的仔细分析（[DBLP:journals/tkde/WeiMJ05,](#bib.bib126)；[DBLP:journals/tkde/YapTP07,](#bib.bib130)；[DBLP:conf/aaai/ZhengCZXY10,](#bib.bib134)），以及（3）对用户之间相似性的适当评估（[DBLP:journals/tkde/CaiLLMTL14,](#bib.bib12)；[DBLP:journals/tkde/TangQZX13,](#bib.bib109)；[DBLP:journals/tkde/HornickT12,](#bib.bib46)；[DBLP:journals/tkde/BartoliniZP11,](#bib.bib3)）。深度学习凭借其高效处理密集高维数据（如电影内容）的能力擅长第一个子任务，而PGM专注于建模用户、项目和评分之间的条件依赖（参见图 [7](#S5.F7
    "Figure 7 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised Bayesian Deep
    Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣
    A Survey on Bayesian Deep Learning")作为例子，其中${\bf u}$、${\bf v}$和${\bf R}$分别为用户潜在向量、项目潜在向量和评分）在其他两个子任务中表现优异。因此，将它们统一到一个单一的原则性概率框架中，可以让我们兼得两者的优势。这种整合还带来了额外的好处，即优雅地处理推荐过程中的不确定性。此外，还可以为具体模型推导出贝叶斯处理方法，从而得到更稳健的预测（[CDL,](#bib.bib121)；[ColVAE,](#bib.bib68)）。
- en: As a third example, consider controlling a complex dynamical system according
    to the live video stream received from a camera. This problem can be transformed
    into iteratively performing two tasks, perception from raw images and control
    based on dynamic models. The perception task of processing raw images can be handled
    by deep learning while the control task usually needs more sophisticated models
    such as hidden Markov models and Kalman filters ([harrison1999bayesian,](#bib.bib35)
    ; [DBLP:conf/uai/MatsubaraGK14,](#bib.bib74) ). The feedback loop is then completed
    by the fact that actions chosen by the control model can affect the received video
    stream in turn. To enable an effective iterative process between the perception
    task and the control task, we need information to flow back and forth between
    them. The perception component would be the basis on which the control component
    estimates its states and the control component with a dynamic model built in would
    be able to predict the future trajectory (images). Therefore Bayesian deep learning
    is a suitable choice ([watter2015embed,](#bib.bib125) ) for this problem. Note
    that similar to the recommender system example, both noise from raw images and
    uncertainty in the control process can be naturally dealt with under such a probabilistic
    framework.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第三个例子，考虑根据从摄像头接收到的实时视频流来控制一个复杂的动态系统。这个问题可以转化为迭代地执行两个任务，即从原始图像中进行感知和基于动态模型进行控制。处理原始图像的感知任务可以通过深度学习来完成，而控制任务通常需要更复杂的模型，如隐马尔可夫模型和卡尔曼滤波器（[harrison1999bayesian,](#bib.bib35)
    ; [DBLP:conf/uai/MatsubaraGK14,](#bib.bib74) ）。反馈回路的完成是由于控制模型选择的动作可以反过来影响接收到的视频流。为了实现感知任务和控制任务之间的有效迭代过程，我们需要信息在它们之间来回流动。感知组件将是控制组件估计其状态的基础，而具备动态模型的控制组件则能够预测未来的轨迹（图像）。因此，贝叶斯深度学习（BDL）是这个问题的合适选择（[watter2015embed,](#bib.bib125)
    ）。需要注意的是，与推荐系统的例子类似，原始图像中的噪声和控制过程中的不确定性都可以在这样的概率框架下自然处理。
- en: 'The above examples demonstrate BDL’s major advantages as a principled way of
    unifying deep learning and PGM: information exchange between the *perception task*
    and the *inference task*, conditional dependencies on high-dimensional data, and
    effective modeling of uncertainty. In terms of uncertainty, it is worth noting
    that when BDL is applied to complex tasks, there are *three kinds of parameter
    uncertainty* that need to be taken into account:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上述例子展示了BDL作为一种将深度学习和概率图模型（PGM）统一的原则性方法的主要优势：感知任务与推断任务之间的信息交换、高维数据的条件依赖关系以及有效的不确定性建模。在不确定性方面，值得注意的是，当BDL应用于复杂任务时，有*三种参数不确定性*需要考虑：
- en: (1)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Uncertainty on the neural network parameters.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 神经网络参数的不确定性。
- en: (2)
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Uncertainty on the task-specific parameters.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务特定参数的不确定性。
- en: (3)
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Uncertainty of exchanging information between the perception component and the
    task-specific component.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感知组件与任务特定组件之间信息交换的不确定性。
- en: By representing the unknown parameters using distributions instead of point
    estimates, BDL offers a promising framework to handle these three kinds of uncertainty
    in a unified way. It is worth noting that the third uncertainty could only be
    handled under a unified framework like BDL; training the perception component
    and the task-specific component separately is equivalent to assuming no uncertainty
    when *exchanging information* between them two. Note that neural networks are
    usually over-parameterized and therefore pose additional challenges in efficiently
    handling the uncertainty in such a large parameter space. On the other hand, graphical
    models are often more concise and have smaller parameter space, providing better
    interpretability.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用分布而不是点估计来表示未知参数，BDL提供了一个有前景的框架来以统一的方式处理这三种不确定性。值得注意的是，第三种不确定性只有在像BDL这样的统一框架下才能处理；分别训练感知组件和任务特定组件等同于假设在它们之间*交换信息*时没有不确定性。需要注意的是，神经网络通常是过度参数化的，因此在处理如此大的参数空间的不确定性时会带来额外的挑战。另一方面，图形模型通常更简洁，参数空间更小，提供了更好的可解释性。
- en: Besides the advantages above, another benefit comes from the implicit regularization
    built in BDL. By imposing a prior on hidden units, parameters defining a neural
    network, or the model parameters specifying the conditional dependencies, BDL
    can to some degree avoid overfitting, especially when we have insufficient data.
    Usually, a BDL model consists of two components, a *perception component* that
    is a Bayesian formulation of a certain type of neural networks and a *task-specific
    component* that describes the relationship among different hidden or observed
    variables using PGM. Regularization is crucial for them both. Neural networks
    are usually heavily over-parameterized and therefore needs to be regularized properly.
    Regularization techniques such as weight decay and dropout ([srivastava2014dropout,](#bib.bib103)
    ) are shown to be effective in improving performance of neural networks and they
    both have Bayesian interpretations ([gal2015dropout,](#bib.bib22) ). In terms
    of the task-specific component, expert knowledge or prior information, as a kind
    of regularization, can be incorporated into the model through the prior we imposed
    to guide the model when data are scarce.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述优点外，BDL 还具有内在正则化的好处。通过对隐藏单元、定义神经网络的参数或指定条件依赖的模型参数施加先验，BDL 在一定程度上可以避免过拟合，特别是在数据不足时。通常，BDL
    模型由两个组件组成，一个是 *感知组件*，它是某种类型的神经网络的贝叶斯公式，另一个是 *任务特定组件*，它使用 PGM 描述不同隐藏或观察变量之间的关系。正则化对这两者都至关重要。神经网络通常具有过多的参数，因此需要适当的正则化。正则化技术，如权重衰减和
    dropout（[srivastava2014dropout](#bib.bib103)），已被证明对提高神经网络的性能有效，它们都有贝叶斯解释（[gal2015dropout](#bib.bib22)）。在任务特定组件方面，作为一种正则化手段，专家知识或先验信息可以通过我们施加的先验融入模型，以在数据稀缺时引导模型。
- en: There are also challenges when applying BDL to real-world tasks. (1) First,
    it is nontrivial to design an efficient Bayesian formulation of neural networks
    with reasonable time complexity. This line of work is pioneered by ([mackay1992practical,](#bib.bib72)
    ; [hinton1993keeping,](#bib.bib42) ; [neal1995bayesian,](#bib.bib80) ), but it
    has not been widely adopted due to its lack of scalability. Fortunately, some
    recent advances in this direction ([DBLP:conf/nips/Graves11,](#bib.bib31) ; [kingma2013auto,](#bib.bib58)
    ; [DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39) ; [DBLP:conf/icml/BlundellCKW15,](#bib.bib9)
    ; [balan2015bayesian,](#bib.bib2) ; [CDL,](#bib.bib121) ; [NPN,](#bib.bib119)
    ) seem to shed light²²2In summary, reduction in time complexity can be achieved
    via expectation propagation ([DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39)
    ), the reparameterization trick ([kingma2013auto,](#bib.bib58) ; [DBLP:conf/icml/BlundellCKW15,](#bib.bib9)
    ), probabilistic formulation of neural networks with maximum a posteriori estimates ([CDL,](#bib.bib121)
    ), approximate variational inference with natural-parameter networks ([NPN,](#bib.bib119)
    ), knowledge distillation ([balan2015bayesian,](#bib.bib2) ), etc. We refer readers
    to ([NPN,](#bib.bib119) ) for a detailed overview. on the practical adoption of
    Bayesian neural network³³3Here we refer to the Bayesian treatment of neural networks
    as Bayesian neural networks. The other term, Bayesian deep learning, is retained
    to refer to complex Bayesian models with both a perception component and a task-specific
    component. See Section [4.1](#S4.SS1 "4.1\. A Brief History of Bayesian Neural
    Networks and Bayesian Deep Learning ‣ 4\. Bayesian Deep Learning ‣ A Survey on
    Bayesian Deep Learning") for a detailed discussion.. (2) The second challenge
    is to ensure efficient and effective information exchange between the perception
    component and the task-specific component. Ideally both the first-order and second-order
    information (e.g., the mean and the variance) should be able to flow back and
    forth between the two components. A natural way is to represent the perception
    component as a PGM and seamlessly connect it to the task-specific PGM, as done
    in ([RSDAE,](#bib.bib118) ; [CDL,](#bib.bib121) ; [DPFA,](#bib.bib24) ).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将贝叶斯深度学习（BDL）应用于实际任务时也面临挑战。 (1) 首先，设计一个具有合理时间复杂度的高效贝叶斯神经网络模型并非易事。这个方向的工作由 ([mackay1992practical,](#bib.bib72)
    ; [hinton1993keeping,](#bib.bib42) ; [neal1995bayesian,](#bib.bib80) ) 开创，但由于缺乏可扩展性，尚未被广泛采用。幸运的是，最近在这一方向上的一些进展
    ([DBLP:conf/nips/Graves11,](#bib.bib31) ; [kingma2013auto,](#bib.bib58) ; [DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39)
    ; [DBLP:conf/icml/BlundellCKW15,](#bib.bib9) ; [balan2015bayesian,](#bib.bib2)
    ; [CDL,](#bib.bib121) ; [NPN,](#bib.bib119) ) 似乎揭示了希望。²²2 总之，可以通过期望传播 ([DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39)
    )、重参数化技巧 ([kingma2013auto,](#bib.bib58) ; [DBLP:conf/icml/BlundellCKW15,](#bib.bib9)
    )、具有最大后验估计的神经网络的概率模型 ([CDL,](#bib.bib121) )、具有自然参数网络的近似变分推断 ([NPN,](#bib.bib119)
    )、知识蒸馏 ([balan2015bayesian,](#bib.bib2) ) 等方法实现时间复杂度的减少。有关贝叶斯神经网络实际应用的详细概述，请参见
    ([NPN,](#bib.bib119) )。³³3 在这里我们将神经网络的贝叶斯处理称为贝叶斯神经网络。另一个术语“贝叶斯深度学习”保留用于指代既有感知组件又有任务特定组件的复杂贝叶斯模型。有关详细讨论，请参见第
    [4.1](#S4.SS1 "4.1\. 贝叶斯神经网络和贝叶斯深度学习的简要历史 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述") 节。 (2) 第二个挑战是确保感知组件和任务特定组件之间的信息交换既高效又有效。理想情况下，第一阶和第二阶信息（例如均值和方差）应该能够在两个组件之间来回流动。一个自然的方法是将感知组件表示为概率图模型（PGM），并将其无缝连接到任务特定的PGM，如在
    ([RSDAE,](#bib.bib118) ; [CDL,](#bib.bib121) ; [DPFA,](#bib.bib24) ) 中所做的那样。
- en: 'This survey provides a comprehensive overview of BDL with concrete models for
    various applications. The rest of the survey is organized as follows: In Section
    [2](#S2 "2\. Deep Learning ‣ A Survey on Bayesian Deep Learning"), we provide
    a review of some basic deep learning models. Section [3](#S3 "3\. Probabilistic
    Graphical Models ‣ A Survey on Bayesian Deep Learning") covers the main concepts
    and techniques for PGM. These two sections serve as the preliminaries for BDL,
    and the next section, Section [4](#S4 "4\. Bayesian Deep Learning ‣ A Survey on
    Bayesian Deep Learning"), demonstrates the rationale for the unified BDL framework
    and details various choices for implementing its *perception component* and *task-specific
    component*. Section [5](#S5 "5\. Concrete BDL Models and Applications ‣ A Survey
    on Bayesian Deep Learning") reviews the BDL models applied to various areas such
    as recommender systems, topic models, and control, showcasing how BDL works in
    supervised learning, unsupervised learning, and general representation learning,
    respectively. Section [6](#S6 "6\. Conclusions and Future Research ‣ A Survey
    on Bayesian Deep Learning") discusses some future research issues and concludes
    the paper.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查为贝叶斯深度学习（BDL）提供了多个应用的具体模型的全面概述。调查的其余部分组织如下：在[2](#S2 "2\. 深度学习 ‣ 关于贝叶斯深度学习的调查")部分，我们回顾了一些基本的深度学习模型。第[3](#S3
    "3\. 概率图模型 ‣ 关于贝叶斯深度学习的调查")部分涵盖了概率图模型的主要概念和技术。这两部分作为BDL的初步部分，接下来的部分，第[4](#S4 "4\.
    贝叶斯深度学习 ‣ 关于贝叶斯深度学习的调查")部分展示了统一的BDL框架的基本理念，并详细介绍了实现其*感知组件*和*任务特定组件*的各种选择。第[5](#S5
    "5\. 具体BDL模型和应用 ‣ 关于贝叶斯深度学习的调查")部分回顾了应用于各种领域的BDL模型，例如推荐系统、主题模型和控制，展示了BDL如何在监督学习、无监督学习和一般表示学习中发挥作用。第[6](#S6
    "6\. 结论和未来研究 ‣ 关于贝叶斯深度学习的调查")部分讨论了一些未来研究问题并总结了全文。
- en: 2\. Deep Learning
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 深度学习
- en: Deep learning normally refers to neural networks with more than two layers.
    To better understand deep learning, here we start with the simplest type of neural
    networks, multilayer perceptrons (MLP), as an example to show how conventional
    deep learning works. After that, we will review several other types of deep learning
    models based on MLP.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习通常指的是具有两个以上层的神经网络。为了更好地理解深度学习，我们从最简单类型的神经网络，即多层感知器（MLP）开始，作为一个例子来展示传统深度学习是如何工作的。之后，我们将回顾基于MLP的其他几种类型的深度学习模型。
- en: 2.1\. Multilayer Perceptrons
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 多层感知器
- en: 'Essentially a multilayer perceptron is a sequence of parametric nonlinear transformations.
    Suppose we want to train a multilayer perceptron to perform a regression task
    which maps a vector of $M$ dimensions to a vector of $D$ dimensions. We denote
    the input as a matrix ${\bf X}_{0}$ ($0$ means it is the $0$-th layer of the perceptron).
    The $j$-th row of ${\bf X}_{0}$, denoted as ${\bf X}_{0,j*}$, is an $M$-dimensional
    vector representing one data point. The target (the output we want to fit) is
    denoted as ${\bf Y}$. Similarly ${\bf Y}_{j*}$ denotes a $D$-dimensional row vector.
    The problem of learning an $L$-layer multilayer perceptron can be formulated as
    the following optimization problem:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，多层感知器（multilayer perceptron）是一系列参数化的非线性转换。假设我们想要训练一个多层感知器去执行将一个$M$维向量映射到一个$D$维向量的回归任务。我们将输入表示为矩阵${\bf
    X}_{0}$（$0$表示它是感知器的第$0$层）。${\bf X}_{0}$的第$j$行，表示为${\bf X}_{0,j*}$，是表示一个数据点的$M$维向量。目标（我们想要拟合的输出）表示为${\bf
    Y}$。同样，${\bf Y}_{j*}$表示一个$D$维行向量。学习$L$层多层感知器的问题可以被规划为以下优化问题：
- en: '|  | $\displaystyle\min\limits_{\{{\bf W}_{l}\},\{{\bf b}_{l}\}}~{}$ | $\displaystyle\&#124;{\bf
    X}_{L}-{\bf Y}\&#124;_{F}+\lambda\sum\limits_{l}\&#124;{\bf W}_{l}\&#124;_{F}^{2}$
    |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min\limits_{\{{\bf W}_{l}\},\{{\bf b}_{l}\}}~{}$ | $\displaystyle\&#124;{\bf
    X}_{L}-{\bf Y}\&#124;_{F}+\lambda\sum\limits_{l}\&#124;{\bf W}_{l}\&#124;_{F}^{2}$
    |  |'
- en: '|  | subject to | $\displaystyle{\bf X}_{l}=\sigma({\bf X}_{l-1}{\bf W}_{l}+{\bf
    b}_{l}),l=1,\dots,L-1$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | subject to | $\displaystyle{\bf X}_{l}=\sigma({\bf X}_{l-1}{\bf W}_{l}+{\bf
    b}_{l}),l=1,\dots,L-1$ |  |'
- en: '|  |  | $\displaystyle{\bf X}_{L}={\bf X}_{L-1}{\bf W}_{L}+{\bf b}_{L},$ |  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bf X}_{L}={\bf X}_{L-1}{\bf W}_{L}+{\bf b}_{L},$ |  |'
- en: where $\sigma(\cdot)$ is an element-wise sigmoid function for a matrix and $\sigma(x)=\frac{1}{1+\exp(-x)}$.
    $\|\cdot\|_{F}$ denotes the Frobenius norm. The purpose of imposing $\sigma(\cdot)$
    is to allow nonlinear transformation. Normally other transformations like $\tanh(x)$
    and $\max(0,x)$ can be used as alternatives of the sigmoid function.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma(\cdot)$ 是矩阵的逐元素 sigmoid 函数，$\sigma(x)=\frac{1}{1+\exp(-x)}$。$\|\cdot\|_{F}$
    表示 Frobenius 范数。施加 $\sigma(\cdot)$ 的目的是允许非线性变换。通常，其他变换如 $\tanh(x)$ 和 $\max(0,x)$
    可以作为 sigmoid 函数的替代。
- en: 'Here ${\bf X}_{l}$ ($l=1,2,\dots,L-1$) is the hidden units. As we can see,
    ${\bf X}_{L}$ can be easily computed once ${\bf X}_{0}$, ${\bf W}_{l}$, and ${\bf
    b}_{l}$ are given. Since ${\bf X}_{0}$ is given as input, one only needs to learn
    ${\bf W}_{l}$ and ${\bf b}_{l}$ here. Usually this is done using backpropagation
    and stochastic gradient descent (SGD). The key is to compute the gradients of
    the objective function with respect to ${\bf W}_{l}$ and ${\bf b}_{l}$. Denoting
    the value of the objective function as $E$, one can compute the gradients using
    the chain rule as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 ${\bf X}_{l}$ ($l=1,2,\dots,L-1$) 是隐藏单元。如我们所见，一旦给定了${\bf X}_{0}$、${\bf W}_{l}$和${\bf
    b}_{l}$，${\bf X}_{L}$可以很容易地计算出来。由于${\bf X}_{0}$作为输入给定，因此这里只需要学习${\bf W}_{l}$和${\bf
    b}_{l}$。通常，这通过反向传播和随机梯度下降（SGD）来完成。关键在于计算目标函数对${\bf W}_{l}$和${\bf b}_{l}$的梯度。将目标函数的值表示为$E$，可以使用链式法则计算梯度：
- en: '|  | $\displaystyle\frac{\partial E}{\partial{\bf X}_{L}}$ | $\displaystyle=2({\bf
    X}_{L}-{\bf Y}),\;\;\;\;\frac{\partial E}{\partial{\bf X}_{l}}=(\frac{\partial
    E}{\partial{\bf X}_{l+1}}\circ{\bf X}_{l+1}\circ(1-{\bf X}_{l+1})){\bf W}_{l+1},$
    |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial E}{\partial{\bf X}_{L}}$ | $\displaystyle=2({\bf
    X}_{L}-{\bf Y}),\;\;\;\;\frac{\partial E}{\partial{\bf X}_{l}}=(\frac{\partial
    E}{\partial{\bf X}_{l+1}}\circ{\bf X}_{l+1}\circ(1-{\bf X}_{l+1})){\bf W}_{l+1},$
    |  |'
- en: '|  | $\displaystyle\frac{\partial E}{\partial{\bf W}_{l}}$ | $\displaystyle={\bf
    X}_{l-1}^{T}(\frac{\partial E}{\partial{\bf X}_{l}}\circ{\bf X}_{l}\circ(1-{\bf
    X}_{l})),\;\;\;\;\frac{\partial E}{\partial{\bf b}_{l}}=mean(\frac{\partial E}{\partial{\bf
    X}_{l}}\circ{\bf X}_{l}\circ(1-{\bf X}_{l}),1),$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial E}{\partial{\bf W}_{l}}$ | $\displaystyle={\bf
    X}_{l-1}^{T}(\frac{\partial E}{\partial{\bf X}_{l}}\circ{\bf X}_{l}\circ(1-{\bf
    X}_{l})),\;\;\;\;\frac{\partial E}{\partial{\bf b}_{l}}=mean(\frac{\partial E}{\partial{\bf
    X}_{l}}\circ{\bf X}_{l}\circ(1-{\bf X}_{l}),1),$ |  |'
- en: where $l=1,\dots,L$ and the regularization terms are omitted. $\circ$ denotes
    the element-wise product and $mean(\cdot,1)$ is the matlab operation on matrices.
    In practice, we only use a small part of the data (e.g., $128$ data points) to
    compute the gradients for each update. This is called stochastic gradient descent.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l=1,\dots,L$，正则化项被省略。$\circ$ 表示逐元素乘积，$mean(\cdot,1)$ 是对矩阵的 Matlab 操作。在实际操作中，我们只使用少量数据（例如，$128$
    个数据点）来计算每次更新的梯度。这被称为随机梯度下降。
- en: As we can see, in conventional deep learning models, only ${\bf W}_{l}$ and
    ${\bf b}_{l}$ are free parameters, which we will update in each iteration of the
    optimization. ${\bf X}_{l}$ is not a free parameter since it can be computed exactly
    if ${\bf W}_{l}$ and ${\bf b}_{l}$ are given.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在传统的深度学习模型中，只有${\bf W}_{l}$和${\bf b}_{l}$是自由参数，我们将在优化的每次迭代中更新这些参数。${\bf
    X}_{l}$ 不是自由参数，因为如果给定了${\bf W}_{l}$和${\bf b}_{l}$，可以准确计算出${\bf X}_{l}$。
- en: '![Refer to caption](img/27df31e056f4222846c15db24ab5bd4a.png)![Refer to caption](img/841c5520d5a4d2f25c8dd3c39dae3dbf.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/27df31e056f4222846c15db24ab5bd4a.png)![参见标题](img/841c5520d5a4d2f25c8dd3c39dae3dbf.png)'
- en: 'Figure 1\. Left: A 2-layer SDAE with $L=4$. Right: A convolutional layer with
    $4$ input feature maps and $2$ output feature maps.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 左图：一个具有$L=4$的2层SDAE。右图：一个具有$4$个输入特征图和$2$个输出特征图的卷积层。
- en: 2.2\. Autoencoders
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 自编码器
- en: An autoencoder (AE) is a feedforward neural network to encode the input into
    a more compact representation and reconstruct the input with the learned representation.
    In its simplest form, an autoencoder is no more than a multilayer perceptron with
    a bottleneck layer (a layer with a small number of hidden units) in the middle.
    The idea of autoencoders has been around for decades ([lecun-87,](#bib.bib63)
    ; [bourlard1988auto,](#bib.bib10) ; [hinton1994autoencoders,](#bib.bib43) ; [dlbook,](#bib.bib29)
    ) and abundant variants of autoencoders have been proposed to enhance representation
    learning including sparse AE ([poultney2006efficient,](#bib.bib88) ), contrastive
    AE ([rifai2011contractive,](#bib.bib93) ), and denoising AE ([DBLP:journals/jmlr/VincentLLBM10,](#bib.bib111)
    ). For more details, please refer to a nice recent book on deep learning ([dlbook,](#bib.bib29)
    ). Here we introduce a kind of multilayer denoising AE, known as stacked denoising
    autoencoders (SDAE), both as an example of AE variants and as background for its
    applications on BDL-based recommender systems in Section [4](#S4 "4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning").
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器（AE）是一个前馈神经网络，用于将输入编码为更紧凑的表示，并用学习到的表示重建输入。在最简单的形式下，自编码器不过是一个具有瓶颈层（一个中间隐藏单元数量较少的层）的多层感知器。自编码器的思想已经存在了几十年
    ([lecun-87,](#bib.bib63); [bourlard1988auto,](#bib.bib10); [hinton1994autoencoders,](#bib.bib43);
    [dlbook,](#bib.bib29))，并且已经提出了大量自编码器变体，以增强表示学习，包括稀疏 AE ([poultney2006efficient,](#bib.bib88))，对比
    AE ([rifai2011contractive,](#bib.bib93))，以及去噪 AE ([DBLP:journals/jmlr/VincentLLBM10,](#bib.bib111))。有关更多细节，请参阅一本关于深度学习的优秀最近书籍
    ([dlbook,](#bib.bib29))。这里我们介绍一种多层去噪 AE，称为堆叠去噪自编码器（SDAE），作为 AE 变体的一个例子，以及作为第 [4](#S4
    "4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning") 节中基于 BDL 的推荐系统应用的背景。
- en: 'SDAE ([DBLP:journals/jmlr/VincentLLBM10,](#bib.bib111) ) is a feedforward neural
    network for learning representations (encoding) of the input data by learning
    to predict the clean input itself in the output, as shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1\. Multilayer Perceptrons ‣ 2\. Deep Learning ‣ A Survey on Bayesian
    Deep Learning")(left). The hidden layer in the middle, i.e., ${\bf X}_{2}$ in
    the figure, can be constrained to be a bottleneck to learn compact representations.
    The difference between traditional AE and SDAE is that the input layer ${\bf X}_{0}$
    is a *corrupted* version of the *clean* input data ${\bf X}_{c}$. Essentially
    an SDAE solves the following optimization problem:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SDAE ([DBLP:journals/jmlr/VincentLLBM10,](#bib.bib111)) 是一种前馈神经网络，用于通过学习在输出中预测干净的输入数据本身来学习输入数据的表示（编码），如图
    [1](#S2.F1 "Figure 1 ‣ 2.1\. Multilayer Perceptrons ‣ 2\. Deep Learning ‣ A Survey
    on Bayesian Deep Learning")(左)所示。中间的隐藏层，即图中的 ${\bf X}_{2}$，可以被约束为一个瓶颈，以学习紧凑的表示。传统
    AE 和 SDAE 之间的区别在于输入层 ${\bf X}_{0}$ 是 *受损的* 版本，而不是 *干净的* 输入数据 ${\bf X}_{c}$。本质上，SDAE
    解决了以下优化问题：
- en: '|  | $\displaystyle\min\limits_{\{{\bf W}_{l}\},\{{\bf b}_{l}\}}$ | $\displaystyle\&#124;{\bf
    X}_{c}-{\bf X}_{L}\&#124;_{F}^{2}+\lambda\sum\limits_{l}\&#124;{\bf W}_{l}\&#124;_{F}^{2}$
    |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min\limits_{\{{\bf W}_{l}\},\{{\bf b}_{l}\}}$ | $\displaystyle\&#124;{\bf
    X}_{c}-{\bf X}_{L}\&#124;_{F}^{2}+\lambda\sum\limits_{l}\&#124;{\bf W}_{l}\&#124;_{F}^{2}$
    |  |'
- en: '|  | subject to | $\displaystyle{\bf X}_{l}=\sigma({\bf X}_{l-1}{\bf W}_{l}+{\bf
    b}_{l}),l=1,\dots,L-1$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | subject to | $\displaystyle{\bf X}_{l}=\sigma({\bf X}_{l-1}{\bf W}_{l}+{\bf
    b}_{l}),l=1,\dots,L-1$ |  |'
- en: '|  |  | $\displaystyle{\bf X}_{L}={\bf X}_{L-1}{\bf W}_{L}+{\bf b}_{L},$ |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bf X}_{L}={\bf X}_{L-1}{\bf W}_{L}+{\bf b}_{L},$ |  |'
- en: where $\lambda$ is a regularization parameter. Here SDAE can be regarded as
    a multilayer perceptron for regression tasks described in the previous section.
    The input ${\bf X}_{0}$ of the MLP is the corrupted version of the data and the
    target ${\bf Y}$ is the clean version of the data ${\bf X}_{c}$. For example,
    ${\bf X}_{c}$ can be the raw data matrix, and we can randomly set $30\%$ of the
    entries in ${\bf X}_{c}$ to $0$ and get ${\bf X}_{0}$. In a nutshell, SDAE learns
    a neural network that takes the noisy data as input and recovers the clean data
    in the last layer. This is what ‘denoising’ in the name means. Normally, the output
    of the middle layer, i.e., ${\bf X}_{2}$ in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\.
    Multilayer Perceptrons ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep Learning")(left),
    would be used to compactly represent the data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是正则化参数。这里 SDAE 可以被视为上一节中描述的回归任务的多层感知机（MLP）。MLP 的输入 ${\bf X}_{0}$
    是数据的损坏版本，目标 ${\bf Y}$ 是数据 ${\bf X}_{c}$ 的干净版本。例如，${\bf X}_{c}$ 可以是原始数据矩阵，我们可以随机将
    ${\bf X}_{c}$ 中 $30\%$ 的条目设置为 $0$，从而得到 ${\bf X}_{0}$。总而言之，SDAE 学习一个神经网络，该网络将噪声数据作为输入，并在最后一层恢复干净的数据。这就是名称中“去噪”的含义。通常，中间层的输出，即图
    [1](#S2.F1 "Figure 1 ‣ 2.1\. Multilayer Perceptrons ‣ 2\. Deep Learning ‣ A Survey
    on Bayesian Deep Learning")(left) 中的 ${\bf X}_{2}$，会被用来紧凑地表示数据。
- en: 2.3\. Convolutional Neural Networks
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 卷积神经网络
- en: 'Convolutional neural networks (CNN) can be viewed as another variant of MLP.
    Different from AE, which is initially designed to perform dimensionality reduction,
    CNN is biologically inspired. According to ([hubel1968receptive,](#bib.bib53)
    ), two types of cells have been identified in the cat’s visual cortex. One is
    simple cells that respond maximally to specific patterns within their receptive
    field, and the other is complex cells with larger receptive field that are considered
    locally invariant to positions of patterns. Inspired by these findings, the two
    key concepts in CNN are then developed: convolution and max-pooling.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）可以被视为 MLP 的另一种变体。与最初设计用于执行降维的 AE 不同，CNN 是受生物启发的。根据 ([hubel1968receptive,](#bib.bib53)
    )，在猫的视觉皮层中已识别出两种类型的细胞。一种是简单细胞，对其感受野内的特定模式反应最强，另一种是复杂细胞，具有较大的感受野，被认为对模式的位置具有局部不变性。受到这些发现的启发，CNN
    中开发了两个关键概念：卷积和最大池化。
- en: 'Convolution: In CNN, a feature map is the result of the convolution of the
    input and a linear filter, followed by some element-wise nonlinear transformation.
    The *input* here can be the raw image or the feature map from the previous layer.
    Specifically, with input ${\bf X}$, weights ${\bf W}^{k}$, bias $b^{k}$, the $k$-th
    feature map ${\bf H}^{k}$ can be obtained as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积：在 CNN 中，特征图是输入与线性滤波器卷积的结果，之后进行一些逐元素的非线性变换。这里的 *input* 可以是原始图像或来自前一层的特征图。具体而言，对于输入
    ${\bf X}$，权重 ${\bf W}^{k}$，偏置 $b^{k}$，第 $k$ 个特征图 ${\bf H}^{k}$ 可以通过以下方式获得：
- en: '|  | $\displaystyle{\bf H}_{ij}^{k}=\tanh(({\bf W}^{k}*{\bf X})_{ij}+b^{k}).$
    |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf H}_{ij}^{k}=\tanh(({\bf W}^{k}*{\bf X})_{ij}+b^{k}).$
    |  |'
- en: Note that in the equation above we assume one single input feature map and multiple
    output feature maps. In practice, CNN often has multiple input feature maps as
    well due to its deep structure. A convolutional layer with $4$ input feature maps
    and $2$ output feature maps is shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1\. Multilayer
    Perceptrons ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep Learning")(right).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在上述方程中，我们假设只有一个输入特征图和多个输出特征图。在实际应用中，由于 CNN 的深层结构，通常也有多个输入特征图。图 [1](#S2.F1
    "Figure 1 ‣ 2.1\. Multilayer Perceptrons ‣ 2\. Deep Learning ‣ A Survey on Bayesian
    Deep Learning")(right) 显示了一个具有 $4$ 个输入特征图和 $2$ 个输出特征图的卷积层。
- en: 'Max-Pooling: Traditionally, a convolutional layer in CNN is followed by a max-pooling
    layer, which can be seen as a type of nonlinear downsampling. The operation of
    max-pooling is simple. For example, if we have a feature map of size $6\times
    9$, the result of max-pooling with a $3\times 3$ region would be a downsampled
    feature map of size $2\times 3$. Each entry of the downsampled feature map is
    the maximum value of the corresponding $3\times 3$ region in the $6\times 9$ feature
    map. Max-pooling layers can not only reduce computational cost by ignoring the
    non-maximal entries but also provide local translation invariance.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化：传统上，CNN 中的卷积层后面跟着一个最大池化层，这可以看作是一种非线性下采样。最大池化的操作很简单。例如，如果我们有一个大小为 $6\times
    9$ 的特征图，使用 $3\times 3$ 区域进行最大池化的结果将是一个大小为 $2\times 3$ 的下采样特征图。下采样特征图的每个条目是 $6\times
    9$ 特征图中对应 $3\times 3$ 区域的最大值。最大池化层不仅可以通过忽略非最大条目来降低计算成本，还提供了局部平移不变性。
- en: 'Putting it all together: Usually to form a complete and working CNN, the input
    would alternate between convolutional layers and max-pooling layers before going
    into an MLP for tasks such as classification or regression. One classic example
    is the LeNet-5 ([lecun1998gradient,](#bib.bib64) ), which alternates between $2$
    convolutional layers and $2$ max-pooling layers before going into a fully connected
    MLP for target tasks.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看：通常要形成一个完整且有效的 CNN，输入会在卷积层和最大池化层之间交替，然后进入 MLP 进行分类或回归等任务。一个经典的例子是 LeNet-5
    ([lecun1998gradient,](#bib.bib64) )，它在进入完全连接的 MLP 进行目标任务之前，交替经过 $2$ 个卷积层和 $2$
    个最大池化层。
- en: '![Refer to caption](img/bef2440df62ad04fce7fdf009b014f32.png)![Refer to caption](img/a769afe3160383435d126879a8dd09a2.png)![Refer
    to caption](img/7c95e4383a5caf46919576108bc4a487.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bef2440df62ad04fce7fdf009b014f32.png)![参见标题](img/a769afe3160383435d126879a8dd09a2.png)![参见标题](img/7c95e4383a5caf46919576108bc4a487.png)'
- en: 'Figure 2\. Left: A conventional feedforward neural network with one hidden
    layer, where ${\bf x}$ is the input, ${\bf z}$ is the hidden layer, and ${\bf
    o}$ is the output, ${\bf W}$ and ${\bf V}$ are the corresponding weights (biases
    are omitted here). Middle: A recurrent neural network with input $\{{\bf x}_{t}\}_{t=1}^{T}$,
    hidden states $\{{\bf h}_{t}\}_{t=1}^{T}$, and output $\{{\bf o}_{t}\}_{t=1}^{T}$.
    Right: An unrolled RNN which is equivalent to the one in Figure [2](#S2.F2 "Figure
    2 ‣ 2.3\. Convolutional Neural Networks ‣ 2\. Deep Learning ‣ A Survey on Bayesian
    Deep Learning")(middle). Here each node (e.g., ${\bf x}_{1}$, ${\bf h}_{1}$, or
    ${\bf o}_{1}$) is associated with one particular time step.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 左：一个具有一个隐藏层的传统前馈神经网络，其中 ${\bf x}$ 是输入，${\bf z}$ 是隐藏层，${\bf o}$ 是输出，${\bf
    W}$ 和 ${\bf V}$ 是对应的权重（这里省略了偏置）。中：一个具有输入 $\{{\bf x}_{t}\}_{t=1}^{T}$、隐藏状态 $\{{\bf
    h}_{t}\}_{t=1}^{T}$ 和输出 $\{{\bf o}_{t}\}_{t=1}^{T}$ 的循环神经网络。右：一个展开的 RNN，它等同于图
    [2](#S2.F2 "图 2 ‣ 2.3\. 卷积神经网络 ‣ 2\. 深度学习 ‣ 关于贝叶斯深度学习的调查")(中)。这里的每个节点（例如，${\bf
    x}_{1}$、${\bf h}_{1}$ 或 ${\bf o}_{1}$）与一个特定的时间步相关联。
- en: 2.4\. Recurrent Neural Network
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 循环神经网络
- en: When reading an article, one normally takes in one word at a time and try to
    understand the current word based on previous words. This is a recurrent process
    that needs short-term memory. Unfortunately conventional feedforward neural networks
    like the one shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.3\. Convolutional Neural
    Networks ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep Learning")(left) fail
    to do so. For example, imagine we want to constantly predict the next word as
    we read an article. Since the feedforward network only computes the output ${\bf
    o}$ as ${\bf V}q({\bf W}{\bf x})$, where the function $q(\cdot)$ denotes element-wise
    nonlinear transformation, it is unclear how the network could naturally model
    the sequence of words to predict the next word.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读文章时，人们通常一次接收一个单词，并尝试根据前面的单词理解当前单词。这是一个需要短期记忆的循环过程。不幸的是，传统的前馈神经网络（如图 [2](#S2.F2
    "图 2 ‣ 2.3\. 卷积神经网络 ‣ 2\. 深度学习 ‣ 关于贝叶斯深度学习的调查")(左) 中所示）无法做到这一点。例如，假设我们想在阅读文章时不断预测下一个单词。由于前馈网络仅将输出
    ${\bf o}$ 计算为 ${\bf V}q({\bf W}{\bf x})$，其中函数 $q(\cdot)$ 表示逐元素的非线性变换，因此不清楚网络如何自然地建模单词序列以预测下一个单词。
- en: 2.4.1\. Vanilla Recurrent Neural Network
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 原始循环神经网络
- en: 'To solve the problem, we need a recurrent neural network ([dlbook,](#bib.bib29)
    ) instead of a feedforward one. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.3\.
    Convolutional Neural Networks ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep
    Learning")(middle), the computation of the current hidden states ${\bf h}_{t}$
    depends on the current input ${\bf x}_{t}$ (e.g., the $t$-th word) and the previous
    hidden states ${\bf h}_{t-1}$. This is why there is a loop in the RNN. It is this
    loop that enables short-term memory in RNNs. The ${\bf h}_{t}$ in the RNN represents
    what the network knows so far at the $t$-th time step. To see the computation
    more clearly, we can unroll the loop and represent the RNN as in Figure [2](#S2.F2
    "Figure 2 ‣ 2.3\. Convolutional Neural Networks ‣ 2\. Deep Learning ‣ A Survey
    on Bayesian Deep Learning")(right). If we use hyperbolic tangent nonlinearity
    ($\tanh$), the computation of output ${\bf o}_{t}$ will be as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要使用循环神经网络（[dlbook,](#bib.bib29)），而不是前馈网络。如图[2](#S2.F2 "Figure 2
    ‣ 2.3\. Convolutional Neural Networks ‣ 2\. Deep Learning ‣ A Survey on Bayesian
    Deep Learning")（中）所示，当前隐藏状态${\bf h}_{t}$的计算依赖于当前输入${\bf x}_{t}$（例如，第$t$个词）和先前的隐藏状态${\bf
    h}_{t-1}$。这就是为什么RNN中存在一个循环。正是这个循环使得RNN具备短期记忆。RNN中的${\bf h}_{t}$表示网络在第$t$个时间步已知的内容。为了更清楚地看到计算过程，我们可以展开循环并将RNN表示为图[2](#S2.F2
    "Figure 2 ‣ 2.3\. Convolutional Neural Networks ‣ 2\. Deep Learning ‣ A Survey
    on Bayesian Deep Learning")（右）。如果我们使用双曲正切非线性函数（$\tanh$），则输出${\bf o}_{t}$的计算将如下：
- en: '|  | $\displaystyle{\bf a}_{t}={\bf W}{\bf h}_{t-1}+{\bf Y}{\bf x}_{t}+{\bf
    b},\;\;\;\;\;{\bf h}_{t}=\tanh({\bf a}_{t}),\;\;\;\;\;{\bf o}_{t}={\bf V}{\bf
    h}_{t}+{\bf c},$ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf a}_{t}={\bf W}{\bf h}_{t-1}+{\bf Y}{\bf x}_{t}+{\bf
    b},\;\;\;\;\;{\bf h}_{t}=\tanh({\bf a}_{t}),\;\;\;\;\;{\bf o}_{t}={\bf V}{\bf
    h}_{t}+{\bf c},$ |  |'
- en: where ${\bf Y}$, ${\bf W}$, and ${\bf V}$ denote the weight matrices for input-to-hidden,
    hidden-to-hidden, and hidden-to-output connections, respectively, and ${\bf b}$
    and ${\bf c}$ are the corresponding biases. If the task is to classify the input
    data at each time step, we can compute the classification probability as ${\bf
    p}_{t}=\mbox{softmax}({\bf o}_{t})$ where
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中${\bf Y}$、${\bf W}$和${\bf V}$分别表示输入到隐藏层、隐藏层到隐藏层和隐藏层到输出的权重矩阵，而${\bf b}$和${\bf
    c}$则是相应的偏置项。如果任务是在每个时间步对输入数据进行分类，我们可以计算分类概率为${\bf p}_{t}=\mbox{softmax}({\bf o}_{t})$，其中
- en: '|  | $\displaystyle\mbox{softmax}({\bf q})=\frac{\exp({\bf q})}{\sum\limits_{i}\exp({\bf
    q}_{i})}.$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mbox{softmax}({\bf q})=\frac{\exp({\bf q})}{\sum\limits_{i}\exp({\bf
    q}_{i})}.$ |  |'
- en: '![Refer to caption](img/526e3c6bc7f93bf0fc019df5beb2b997.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/526e3c6bc7f93bf0fc019df5beb2b997.png)'
- en: Figure 3\. The encoder-decoder architecture involving two LSTMs. The encoder
    LSTM (in the left rectangle) encodes the sequence ‘ABC’ into a representation
    and the decoder LSTM (in the right rectangle) recovers the sequence from the representation.
    ‘$’ marks the end of a sentence.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 包含两个 LSTM 的编码器-解码器架构。编码器 LSTM（在左侧矩形中）将序列‘ABC’编码成一个表示，解码器 LSTM（在右侧矩形中）从表示中恢复出序列。‘$’表示句子的结束。
- en: Similar to feedforward networks, an RNN is trained with a generalized back-propagation
    algorithm called *back-propagation through time* (BPTT) ([dlbook,](#bib.bib29)
    ). Essentially the gradients are computed through the unrolled network as shown
    in Figure [2](#S2.F2 "Figure 2 ‣ 2.3\. Convolutional Neural Networks ‣ 2\. Deep
    Learning ‣ A Survey on Bayesian Deep Learning")(right) with shared weights and
    biases for all time steps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络类似，RNN 使用一种称为*时间反向传播*（BPTT）的广义反向传播算法进行训练（[dlbook,](#bib.bib29)）。本质上，梯度是通过展开的网络进行计算，如图[2](#S2.F2
    "Figure 2 ‣ 2.3\. Convolutional Neural Networks ‣ 2\. Deep Learning ‣ A Survey
    on Bayesian Deep Learning")（右）所示，所有时间步共享相同的权重和偏置。
- en: 2.4.2\. Gated Recurrent Neural Network
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 门控循环神经网络
- en: 'The problem with the vanilla RNN above is that the gradients propagated over
    many time steps are prone to vanish or explode, making the optimization notoriously
    difficult. In addition, the signal passing through the RNN decays exponentially,
    making it impossible to model long-term dependencies in long sequences. Imagine
    we want to predict the last word in the paragraph ‘I have many books … I like
    *reading*’. In order to get the answer, we need ‘long-term memory’ to retrieve
    information (the word ‘books’) at the start of the text. To address this problem,
    the long short-term memory model (LSTM) is designed as a type of gated RNN to
    model and accumulate information over a relatively long duration. The intuition
    behind LSTM is that when processing a sequence consisting of several subsequences,
    it is sometimes useful for the neural network to summarize or forget the old states
    before moving on to process the next subsequence ([dlbook,](#bib.bib29) ). Using
    $t=1\dots T_{j}$ to index the words in the sequence, the formulation of LSTM is
    as follows (we drop the item index $j$ for notational simplicity):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 上述传统 RNN 的问题在于，梯度在许多时间步上传播时容易消失或爆炸，导致优化非常困难。此外，RNN 中信号的衰减呈指数级，使得无法对长序列中的长期依赖进行建模。想象一下我们要预测段落中的最后一个词‘I
    have many books … I like *reading*’。为了得到答案，我们需要‘长期记忆’来检索文本开头的信息（词汇‘books’）。为了解决这个问题，长短期记忆模型（LSTM）被设计为一种门控
    RNN，能够在相对较长的时间段内建模和积累信息。LSTM 的直觉是，当处理由几个子序列组成的序列时，神经网络有时需要总结或忘记旧状态，然后再处理下一个子序列
    ([dlbook,](#bib.bib29) )。使用 $t=1\dots T_{j}$ 对序列中的词进行索引，LSTM 的公式如下（为简化符号，我们省略了项目索引
    $j$）：
- en: '| (1) |  | $\displaystyle{\bf x}_{t}={\bf W}_{w}{\bf e}_{t},\;\;\;\;\;{\bf
    s}_{t}={\bf h}_{t-1}^{f}\odot{\bf s}_{t-1}+{\bf h}_{t-1}^{i}\odot\sigma({\bf Y}{\bf
    x}_{t-1}+{\bf W}{\bf h}_{t-1}+{\bf b}),$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\displaystyle{\bf x}_{t}={\bf W}_{w}{\bf e}_{t},\;\;\;\;\;{\bf
    s}_{t}={\bf h}_{t-1}^{f}\odot{\bf s}_{t-1}+{\bf h}_{t-1}^{i}\odot\sigma({\bf Y}{\bf
    x}_{t-1}+{\bf W}{\bf h}_{t-1}+{\bf b}),$ |  |'
- en: 'where ${\bf x}_{t}$ is the word embedding of the $t$-th word, ${\bf W}_{w}$
    is a $K_{W}$-by-$S$ word embedding matrix, and ${\bf e}_{t}$ is the $1$-of-$S$
    representation, $\odot$ stands for the element-wise product operation between
    two vectors, $\sigma(\cdot)$ denotes the sigmoid function, ${\bf s}_{t}$ is the
    cell state of the $t$-th word, and ${\bf b}$, ${\bf Y}$, and ${\bf W}$ denote
    the biases, input weights, and recurrent weights respectively. The forget gate
    units ${\bf h}_{t}^{f}$ and the input gate units ${\bf h}_{t}^{i}$ in Equation
    ([1](#S2.E1 "In 2.4.2\. Gated Recurrent Neural Network ‣ 2.4\. Recurrent Neural
    Network ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep Learning")) can be computed
    using their corresponding weights and biases ${\bf Y}^{f}$, ${\bf W}^{f}$, ${\bf
    Y}^{i}$, ${\bf W}^{i}$, ${\bf b}^{f}$, and ${\bf b}^{i}$:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf x}_{t}$ 是第 $t$ 个词的词嵌入，${\bf W}_{w}$ 是一个 $K_{W}$-by-$S$ 的词嵌入矩阵，${\bf
    e}_{t}$ 是 $1$-of-$S$ 的表示，$\odot$ 代表两个向量之间的逐元素乘积操作，$\sigma(\cdot)$ 表示 sigmoid 函数，${\bf
    s}_{t}$ 是第 $t$ 个词的单元状态，${\bf b}$、${\bf Y}$ 和 ${\bf W}$ 分别表示偏置、输入权重和递归权重。忘记门单元
    ${\bf h}_{t}^{f}$ 和输入门单元 ${\bf h}_{t}^{i}$ 可以使用其对应的权重和偏置 ${\bf Y}^{f}$、${\bf W}^{f}$、${\bf
    Y}^{i}$、${\bf W}^{i}$、${\bf b}^{f}$ 和 ${\bf b}^{i}$ 计算：
- en: '|  | $\displaystyle{\bf h}_{t}^{f}=\sigma({\bf Y}^{f}{\bf x}_{t}+{\bf W}^{f}{\bf
    h}_{t}+{\bf b}^{f}),\;\;\;\;\;{\bf h}_{t}^{i}=\sigma({\bf Y}^{i}{\bf x}_{t}+{\bf
    W}^{i}{\bf h}_{t}+{\bf b}^{i}).$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf h}_{t}^{f}=\sigma({\bf Y}^{f}{\bf x}_{t}+{\bf W}^{f}{\bf
    h}_{t}+{\bf b}^{f}),\;\;\;\;\;{\bf h}_{t}^{i}=\sigma({\bf Y}^{i}{\bf x}_{t}+{\bf
    W}^{i}{\bf h}_{t}+{\bf b}^{i}).$ |  |'
- en: 'The output depends on the output gate ${\bf h}_{t}^{o}$ which has its own weights
    and biases ${\bf Y}^{o}$, ${\bf W}^{o}$, and ${\bf b}^{o}$:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出依赖于输出门 ${\bf h}_{t}^{o}$，它有自己的权重和偏置 ${\bf Y}^{o}$、${\bf W}^{o}$ 和 ${\bf b}^{o}$：
- en: '|  | $\displaystyle{\bf h}_{t}=\tanh({\bf s}_{t})\odot{\bf h}_{t-1}^{o},\;\;\;\;\;{\bf
    h}_{t}^{o}=\sigma({\bf Y}^{o}{\bf x}_{t}+{\bf W}^{o}{\bf h}_{t}+{\bf b}^{o}).$
    |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf h}_{t}=\tanh({\bf s}_{t})\odot{\bf h}_{t-1}^{o},\;\;\;\;\;{\bf
    h}_{t}^{o}=\sigma({\bf Y}^{o}{\bf x}_{t}+{\bf W}^{o}{\bf h}_{t}+{\bf b}^{o}).$
    |  |'
- en: Note that in the LSTM, information of the processed sequence is contained in
    the cell states ${\bf s}_{t}$ and the output states ${\bf h}_{t}$, both of which
    are column vectors of length $K_{W}$.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在 LSTM 中，处理过的序列信息包含在单元状态 ${\bf s}_{t}$ 和输出状态 ${\bf h}_{t}$ 中，这两个都是长度为 $K_{W}$
    的列向量。
- en: Similar to ([sutskever2014sequence,](#bib.bib108) ; [DBLP:conf/emnlp/ChoMGBBSB14,](#bib.bib16)
    ), we can use the output state and cell state at the last time step (${\bf h}_{T_{j}}$
    and ${\bf s}_{T_{j}}$) of the first LSTM as the initial output state and cell
    state of the second LSTM. This way the two LSTMs can be concatenated to form an
    encoder-decoder architecture, as shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.4.1\.
    Vanilla Recurrent Neural Network ‣ 2.4\. Recurrent Neural Network ‣ 2\. Deep Learning
    ‣ A Survey on Bayesian Deep Learning").
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ([sutskever2014sequence,](#bib.bib108) ; [DBLP:conf/emnlp/ChoMGBBSB14,](#bib.bib16)
    )，我们可以将第一个 LSTM 在最后一个时间步的输出状态和单元状态（${\bf h}_{T_{j}}$ 和 ${\bf s}_{T_{j}}$）作为第二个
    LSTM 的初始输出状态和单元状态。这样，两个 LSTM 可以串联起来形成一个编码器-解码器架构，如图 [3](#S2.F3 "图 3 ‣ 2.4.1\.
    基本递归神经网络 ‣ 2.4\. 递归神经网络 ‣ 2\. 深度学习 ‣ 贝叶斯深度学习概述") 所示。
- en: Note that there is a vast literature on deep learning and neural networks. The
    introduction in this section intends to serve only as the background of Bayesian
    deep learning. Readers are referred to ([dlbook,](#bib.bib29) ) for a comprehensive
    survey and more details.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，关于深度学习和神经网络的文献非常丰富。本节的介绍仅作为贝叶斯深度学习的背景。有关综合调查和更多细节，请参阅 ([dlbook,](#bib.bib29)
    )。
- en: 3\. Probabilistic Graphical Models
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 概率图模型
- en: Probabilistic Graphical Models (PGM) use diagrammatic representations to describe
    random variables and relationships among them. Similar to a graph that contains
    nodes (vertices) and links (edges), PGM has nodes to represent random variables
    and links to indicate probabilistic relationships among them.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 概率图模型（PGM）使用图示表示来描述随机变量及其关系。类似于包含节点（顶点）和边（连线）的图，PGM 有节点来表示随机变量，并且通过连线表示它们之间的概率关系。
- en: 3.1\. Models
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 模型
- en: There are essentially two types of PGM, directed PGM (also known as Bayesian
    networks) and undirected PGM (also known as Markov random fields) ([PRML,](#bib.bib5)
    ). In this survey we mainly focus on directed PGM⁴⁴4For convenience, PGM stands
    for directed PGM in this survey unless specified otherwise.. For details on undirected
    PGM, readers are referred to ([PRML,](#bib.bib5) ).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: PGM 主要有两种类型：有向 PGM（也称为贝叶斯网络）和无向 PGM（也称为马尔可夫随机场） ([PRML,](#bib.bib5) )。在本调查中，我们主要关注有向
    PGM⁴⁴4 为了方便起见，本调查中 PGM 指的是有向 PGM，除非另有说明。有关无向 PGM 的详细信息，请参阅 ([PRML,](#bib.bib5)
    )。
- en: '![Refer to caption](img/ba60f9489eed5b62b8c54638158c12fb.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ba60f9489eed5b62b8c54638158c12fb.png)'
- en: Figure 4\. The probabilistic graphical model for LDA, $J$ is the number of documents,
    $D$ is the number of words in a document, and $K$ is the number of topics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. LDA 的概率图模型，$J$ 是文档的数量，$D$ 是每个文档中的词汇数量，$K$ 是主题数量。
- en: 'A classic example of PGM would be latent Dirichlet allocation (LDA), which
    is used as a topic model to analyze the generation of words and topics in documents ([LDA,](#bib.bib8)
    ). Usually PGM comes with a graphical representation of the model and a generative
    process to depict the story of how the random variables are generated step by
    step. Figure [4](#S3.F4 "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical
    Models ‣ A Survey on Bayesian Deep Learning") shows the graphical model for LDA
    and the corresponding generative process is as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PGM 的经典例子是潜在狄利克雷分配（LDA），它作为主题模型分析文档中词汇和主题的生成 ([LDA,](#bib.bib8) )。通常，PGM 附带模型的图形表示和生成过程，描述随机变量如何一步步生成。图
    [4](#S3.F4 "图 4 ‣ 3.1\. 模型 ‣ 3\. 概率图模型 ‣ 贝叶斯深度学习概述") 显示了 LDA 的图形模型，对应的生成过程如下：
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For each document $j$ ($j=1,2,\dots,J$),
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个文档 $j$ （$j=1,2,\dots,J$），
- en: (1)
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Draw topic proportions $\theta_{j}\sim\mbox{Dirichlet}(\alpha)$.
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制主题比例 $\theta_{j}\sim\mbox{Dirichlet}(\alpha)$。
- en: (2)
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: For each word $w_{jn}$ of item (document) ${\bf w}_{j}$,
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个词 $w_{jn}$，来自项目（文档） ${\bf w}_{j}$，
- en: (a)
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: Draw topic assignment $z_{jn}\sim\mbox{Mult}(\theta_{j})$.
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制主题分配 $z_{jn}\sim\mbox{Mult}(\theta_{j})$。
- en: (b)
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Draw word $w_{jn}\sim\mbox{Mult}(\beta_{z_{jn}})$.
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制词 $w_{jn}\sim\mbox{Mult}(\beta_{z_{jn}})$。
- en: The generative process above provides the story of how the random variables
    are generated. In the graphical model in Figure [4](#S3.F4 "Figure 4 ‣ 3.1\. Models
    ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian Deep Learning"), the
    shaded node denotes observed variables while the others are latent variables ($\theta$
    and ${\bf z}$) or parameters ($\alpha$ and $\beta$). Once the model is defined,
    learning algorithms can be applied to automatically learn the latent variables
    and parameters.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的生成过程提供了随机变量生成的故事。在图 [4](#S3.F4 "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic
    Graphical Models ‣ A Survey on Bayesian Deep Learning") 中的图形模型中，阴影节点表示观察变量，而其他节点是潜在变量（$\theta$和${\bf
    z}$）或参数（$\alpha$和$\beta$）。一旦定义了模型，学习算法可以应用于自动学习潜在变量和参数。
- en: Due to its Bayesian nature, PGM such as LDA is easy to extend to incorporate
    other information or to perform other tasks. For example, following LDA, different
    variants of topic models have been proposed. ([DTM,](#bib.bib7) ; [cDTM,](#bib.bib113)
    ) are proposed to incorporate temporal information, and ([CTM,](#bib.bib6) ) extends
    LDA by assuming correlations among topics. ([onlineLDA,](#bib.bib44) ) extends
    LDA from the batch mode to the online setting, making it possible to process large
    datasets. On recommender systems, collaborative topic regression (CTR) ([CTR,](#bib.bib112)
    ) extends LDA to incorporate rating information and make recommendations. This
    model is then further extended to incorporate social information ([CTR-SMF,](#bib.bib89)
    ; [CTRSR,](#bib.bib115) ; [RCTR,](#bib.bib116) ).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其贝叶斯性质，诸如LDA的PGM易于扩展，以包含其他信息或执行其他任务。例如，在LDA之后，提出了不同的主题模型变体。([DTM,](#bib.bib7)；[cDTM,](#bib.bib113))被提出以包含时间信息，而([CTM,](#bib.bib6))通过假设主题之间的相关性扩展了LDA。([onlineLDA,](#bib.bib44))将LDA从批处理模式扩展到在线设置，从而可以处理大型数据集。在推荐系统中，协作主题回归（CTR）（[CTR,](#bib.bib112)）将LDA扩展以包含评分信息并进行推荐。然后，这个模型进一步扩展以包含社交信息（[CTR-SMF,](#bib.bib89)；[CTRSR,](#bib.bib115)；[RCTR,](#bib.bib116)）。
- en: 'Table 1\. Summary of BDL Models with Different Learning Algorithms (MAP: Maximum
    a Posteriori, VI: Variational Inference, Hybrid MC: Hybrid Monte Carlo) and Different
    Variance Types (ZV: Zero-Variance, HV: Hyper-Variance, LV: Learnable-Variance).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '表1\. 不同学习算法下BDL模型的总结（MAP: 最大后验概率，VI: 变分推断，Hybrid MC: 混合蒙特卡罗），不同方差类型（ZV: 零方差，HV:
    超级方差，LV: 可学习方差）。'
- en: '| Applications | Models | Variance of $\mbox{\boldmath$\Omega$\unboldmath}_{h}$
    | MAP | VI | Gibbs Sampling | Hybrid MC |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: 应用 | 模型 | $\mbox{\boldmath$\Omega$\unboldmath}_{h}$的方差 | MAP | VI | Gibbs Sampling
    | Hybrid MC的变量 |
- en: '| Recommender Systems | Collaborative Deep Learning (CDL) ([CDL,](#bib.bib121)
    ) | HV | ✓ |  |  |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: 推荐系统 | 协作深度学习（CDL）（[CDL,](#bib.bib121)） | HV | ✓ |  |  |  |
- en: '| Bayesian CDL ([CDL,](#bib.bib121) ) | HV |  |  | ✓ |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: 贝叶斯CDL（[CDL,](#bib.bib121)） | HV |  |  | ✓ |  |
- en: '| Marginalized CDL ([li2015deep,](#bib.bib66) ) | LV | ✓ |  |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: 边际化CDL（[li2015deep,](#bib.bib66)） | LV | ✓ |  |  |  |
- en: '| Symmetric CDL ([li2015deep,](#bib.bib66) ) | LV | ✓ |  |  |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: 对称CDL（[li2015deep,](#bib.bib66)） | LV | ✓ |  |  |  |
- en: '| Collaborative Deep Ranking ([yingcollaborative,](#bib.bib131) ) | HV | ✓
    |  |  |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: 协作式深度排名（[yingcollaborative,](#bib.bib131)） | HV | ✓ |  |  |  |
- en: '| Collaborative Knowledge Base Embedding ([CKE,](#bib.bib132) ) | HV | ✓ |  |  |  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: 协作知识库嵌入（[CKE,](#bib.bib132)） | HV | ✓ |  |  |  |
- en: '| Collaborative Recurrent AE ([CRAE,](#bib.bib122) ) | HV | ✓ |  |  |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: 协作循环自编码器（[CRAE,](#bib.bib122)） | HV | ✓ |  |  |  |
- en: '| Collaborative Variational Autoencoders ([ColVAE,](#bib.bib68) ) | HV |  |
    ✓ |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: 协作变分自编码器（[ColVAE,](#bib.bib68)） | HV |  | ✓ |  |  |
- en: '| Topic Models | Relational SDAE | HV | ✓ |  |  |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: 主题模型 | 关系SDAE | HV | ✓ |  |  |  |
- en: '| Deep Poisson Factor Analysis with Sigmoid Belief Networks ([DPFA,](#bib.bib24)
    ) | ZV |  |  | ✓ | ✓ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: 具有Sigmoid信念网络的深泊松因子分析（[DPFA,](#bib.bib24)） | ZV |  |  | ✓ | ✓ |
- en: '| Deep Poisson Factor Analysis with Restricted Boltzmann Machine ([DPFA,](#bib.bib24)
    ) | ZV |  |  | ✓ | ✓ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: 具有受限玻尔兹曼机的深泊松因子分析（[DPFA,](#bib.bib24)） | ZV |  |  | ✓ | ✓ |
- en: '| Deep Latent Dirichlet Allocation ([DLDA,](#bib.bib18) ) | LV |  |  |  | ✓
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: 深层狄利克雷分配（[DLDA,](#bib.bib18)） | LV |  |  |  | ✓ |
- en: '|  | Dirichlet Belief Networks ([DirBN,](#bib.bib133) ) | LV |  |  | ✓ |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: 朴素贝叶斯网络（[DirBN,](#bib.bib133)） | LV |  |  |  | ✓ |
- en: '| Control | Embed to Control ([watter2015embed,](#bib.bib125) ) | LV |  | ✓
    |  |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: 控制 | 嵌入到控制（[watter2015embed,](#bib.bib125)） | LV |  | ✓ |  |  |
- en: '| Deep Variational Bayes Filters ([DVBF,](#bib.bib57) ) | LV |  | ✓ |  |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: 深度变分贝叶斯滤波器（[DVBF,](#bib.bib57)） | LV |  | ✓ |  |  |
- en: '| Probabilistic Recurrent State-Space Models ([PR-SSM,](#bib.bib19) ) | LV
    |  | ✓ |  |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: 概率循环状态空间模型（[PR-SSM,](#bib.bib19)） | LV |  | ✓ |  |  |
- en: '| Deep Planning Networks ([PlaNet,](#bib.bib34) ) | LV |  | ✓ |  |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 深度规划网络 ([PlaNet,](#bib.bib34) ) | LV |  | ✓ |  |  |'
- en: '| Link Prediction | Relational Deep Learning ([RDL,](#bib.bib120) ) | LV |
    ✓ | ✓ |  |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 链接预测 | 关系深度学习 ([RDL,](#bib.bib120) ) | LV | ✓ | ✓ |  |  |'
- en: '| Graphite ([Graphite,](#bib.bib32) ) | LV |  | ✓ |  |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Graphite ([Graphite,](#bib.bib32) ) | LV |  | ✓ |  |  |'
- en: '| Deep Generative Latent Feature Relational Model ([SBGNN,](#bib.bib75) ) |
    LV |  | ✓ |  |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 深度生成潜在特征关系模型 ([SBGNN,](#bib.bib75) ) | LV |  | ✓ |  |  |'
- en: '| NLP | Sequence to Better Sequence ([S2BS,](#bib.bib77) ) | LV |  | ✓ |  |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 自然语言处理 | 从序列到更好序列 ([S2BS,](#bib.bib77) ) | LV |  | ✓ |  |  |'
- en: '| Quantifiable Sequence Editing ([QuaSE,](#bib.bib69) ) | LV |  | ✓ |  |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 可量化序列编辑 ([QuaSE,](#bib.bib69) ) | LV |  | ✓ |  |  |'
- en: '| Computer Vision | Asynchronous Temporal Fields ([ATF,](#bib.bib102) ) | LV
    |  | ✓ |  |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 计算机视觉 | 异步时间场 ([ATF,](#bib.bib102) ) | LV |  | ✓ |  |  |'
- en: '| Attend, Infer, Repeat (AIR) ([AIR,](#bib.bib20) ) | LV |  | ✓ |  |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 注意、推理、重复 (AIR) ([AIR,](#bib.bib20) ) | LV |  | ✓ |  |  |'
- en: '| Fast AIR ([FastAIR,](#bib.bib105) ) | LV |  | ✓ |  |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 快速AIR ([FastAIR,](#bib.bib105) ) | LV |  | ✓ |  |  |'
- en: '| Sequential AIR ([SQAIR,](#bib.bib60) ) | LV |  | ✓ |  |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 序列AIR ([SQAIR,](#bib.bib60) ) | LV |  | ✓ |  |  |'
- en: '| Speech | Factorized Hierarchical VAE ([FHVAE,](#bib.bib48) ) | LV |  | ✓
    |  |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 语音 | 因子化分层VAE ([FHVAE,](#bib.bib48) ) | LV |  | ✓ |  |  |'
- en: '| Scalable Factorized Hierarchical VAE ([SFHVAE,](#bib.bib47) ) | LV |  | ✓
    |  |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展因子化分层VAE ([SFHVAE,](#bib.bib47) ) | LV |  | ✓ |  |  |'
- en: '| Gaussian Mixture Variational Autoencoders ([GMVAE,](#bib.bib49) ) | LV |  |
    ✓ |  |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 高斯混合变分自编码器 ([GMVAE,](#bib.bib49) ) | LV |  | ✓ |  |  |'
- en: '| Recurrent Poisson Process Units ([RPPU,](#bib.bib51) ) | LV | ✓ | ✓ |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 循环泊松过程单元 ([RPPU,](#bib.bib51) ) | LV | ✓ | ✓ |  |  |'
- en: '| Deep Graph Random Process ([DGP,](#bib.bib52) ) | LV | ✓ | ✓ |  |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 深度图随机过程 ([DGP,](#bib.bib52) ) | LV | ✓ | ✓ |  |  |'
- en: '| Time Series Forecasting | DeepAR ([DeepAR,](#bib.bib21) ) | LV |  | ✓ |  |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 时间序列预测 | DeepAR ([DeepAR,](#bib.bib21) ) | LV |  | ✓ |  |  |'
- en: '| DeepState ([DeepState,](#bib.bib90) ) | LV |  | ✓ |  |  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| DeepState ([DeepState,](#bib.bib90) ) | LV |  | ✓ |  |  |'
- en: '| Spline Quantile Function RNN ([SQF-RNN,](#bib.bib27) ) | LV |  | ✓ |  |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 样条分位函数RNN ([SQF-RNN,](#bib.bib27) ) | LV |  | ✓ |  |  |'
- en: '| DeepFactor ([DeepFactor,](#bib.bib124) ) | LV |  | ✓ |  |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| DeepFactor ([DeepFactor,](#bib.bib124) ) | LV |  | ✓ |  |  |'
- en: '| Health Care | Deep Poisson Factor Models ([DPFM,](#bib.bib38) ) | LV |  |  |  |
    ✓ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 健康护理 | 深度泊松因子模型 ([DPFM,](#bib.bib38) ) | LV |  |  |  | ✓ |'
- en: '| Deep Markov Models ([DeepMarkov,](#bib.bib61) ) | LV |  | ✓ |  |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 深度马尔科夫模型 ([DeepMarkov,](#bib.bib61) ) | LV |  | ✓ |  |  |'
- en: '| Black-Box False Discovery Rate ([BBFDR,](#bib.bib110) ) | LV |  | ✓ |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 黑箱假发现率 ([BBFDR,](#bib.bib110) ) | LV |  | ✓ |  |  |'
- en: '| Bidirectional Inference Networks ([BIN,](#bib.bib117) ) | LV | ✓ |  |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 双向推理网络 ([BIN,](#bib.bib117) ) | LV | ✓ |  |  |  |'
- en: 3.2\. Inference and Learning
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 推理与学习
- en: Strictly speaking, the process of finding the parameters (e.g., $\alpha$ and
    $\beta$ in Figure [4](#S3.F4 "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical
    Models ‣ A Survey on Bayesian Deep Learning")) is called learning and the process
    of finding the latent variables (e.g., $\theta$ and ${\bf z}$ in Figure [4](#S3.F4
    "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian
    Deep Learning")) given the parameters is called inference. However, given only
    the observed variables (e.g. ${\bf w}$ in Figure [4](#S3.F4 "Figure 4 ‣ 3.1\.
    Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian Deep Learning")),
    learning and inference are often intertwined. Usually the learning and inference
    of LDA would alternate between the updates of latent variables (which correspond
    to inference) and the updates of the parameters (which correspond to learning).
    Once the learning and inference of LDA is completed, one could obtain the learned
    parameters $\alpha$ and $\beta$. If a new document comes, one can now fix the
    learned $\alpha$ and $\beta$ and then perform inference alone to find the topic
    proportions $\theta_{j}$ of the new document.⁵⁵5For convenience, we use ‘learning’
    to represent both ‘learning and inference’ in the following text.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 严格来说，寻找参数（例如，图 [4](#S3.F4 "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical
    Models ‣ A Survey on Bayesian Deep Learning") 中的 $\alpha$ 和 $\beta$）的过程称为学习，而在给定参数的情况下寻找潜在变量（例如，图
    [4](#S3.F4 "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey
    on Bayesian Deep Learning") 中的 $\theta$ 和 ${\bf z}$）的过程称为推断。然而，考虑到仅有观察到的变量（例如，图
    [4](#S3.F4 "Figure 4 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey
    on Bayesian Deep Learning") 中的 ${\bf w}$），学习和推断通常是交织在一起的。通常，LDA 的学习和推断会在潜在变量的更新（对应于推断）和参数的更新（对应于学习）之间交替进行。一旦
    LDA 的学习和推断完成，就可以获得学习到的参数 $\alpha$ 和 $\beta$。如果有新的文档出现，可以固定已学习的 $\alpha$ 和 $\beta$，然后仅进行推断以找到新文档的主题比例
    $\theta_{j}$。⁵⁵ 为方便起见，以下文本中我们用‘学习’来表示‘学习和推断’。
- en: Similar to LDA, various learning and inference algorithms are available for
    each PGM. Among them, the most cost-effective one is probably maximum a posteriori
    (MAP), which amounts to maximizing the posterior probability of the latent variable.
    Using MAP, the learning process is equivalent to minimizing (or maximizing) an
    objective function with regularization. One famous example is the probabilistic
    matrix factorization (PMF) ([PMF,](#bib.bib96) ), where the learning of the graphical
    model is equivalent to factorizing a large matrix into two low-rank matrices with
    L2 regularization.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 LDA，每个 PGM 都有各种学习和推断算法。其中，最具性价比的可能是最大后验概率（MAP），这相当于最大化潜在变量的后验概率。使用 MAP，学习过程等同于最小化（或最大化）一个带有正则化的目标函数。一个著名的例子是概率矩阵分解（PMF）([PMF,](#bib.bib96)），其中图模型的学习等同于将一个大矩阵分解成两个低秩矩阵，并使用
    L2 正则化。
- en: MAP, as efficient as it is, gives us only *point estimates* of latent variables
    (and parameters). In order to take the uncertainty into account and harness the
    full power of Bayesian models, one would have to resort to Bayesian treatments
    such as variational inference and Markov chain Monte Carlo (MCMC). For example,
    the original LDA uses variational inference to approximate the true posterior
    with factorized variational distributions ([LDA,](#bib.bib8) ). Learning of the
    latent variables and parameters then boils down to minimizing the KL-divergence
    between the variational distributions and the true posterior distributions. Besides
    variational inference, another choice for a Bayesian treatment is MCMC. For example,
    MCMC algorithms such as ([porteous2008fast,](#bib.bib86) ) have been proposed
    to learn the posterior distributions of LDA.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 MAP 非常高效，但它仅提供潜在变量（和参数）的*点估计*。为了考虑不确定性并发挥贝叶斯模型的全部威力，必须使用贝叶斯处理方法，如变分推断和马尔可夫链蒙特卡罗（MCMC）。例如，原始的
    LDA 使用变分推断来用分解的变分分布来近似真实的后验分布 ([LDA,](#bib.bib8) )。潜在变量和参数的学习就归结为最小化变分分布和真实后验分布之间的
    KL 散度。除了变分推断，另一个贝叶斯处理的选择是 MCMC。例如，已经提出了像 ([porteous2008fast,](#bib.bib86) ) 这样的
    MCMC 算法来学习 LDA 的后验分布。
- en: 4\. Bayesian Deep Learning
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 贝叶斯深度学习
- en: With the preliminaries on deep learning and PGM, we are now ready to introduce
    the general framework and some concrete examples of BDL. Specifically, in this
    section we will list some recent BDL models with applications on recommender systems,
    topic models, control, etc. A summary of these models is shown in Table [1](#S3.T1
    "Table 1 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian
    Deep Learning").
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了深度学习和PGM的初步知识后，我们现在准备介绍BDL的一般框架和一些具体示例。具体而言，本节将列出一些最近的BDL模型，并介绍其在推荐系统、主题模型、控制等方面的应用。这些模型的总结见表[1](#S3.T1
    "Table 1 ‣ 3.1. Models ‣ 3. Probabilistic Graphical Models ‣ 贝叶斯深度学习调查")。
- en: 4.1\. A Brief History of Bayesian Neural Networks and Bayesian Deep Learning
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 贝叶斯神经网络和贝叶斯深度学习的简要历史
- en: One topic highly related to BDL is Bayesian neural networks (BNN) or Bayesian
    treatments of neural networks. Similar to any Bayesian treatment, BNN imposes
    a prior on the neural network’s parameters and aims to learn a posterior distribution
    of these parameters. During the inference phrase, such a distribution is then
    marginalized out to produce final predictions. In general such a process is called
    Bayesian model averaging ([PRML,](#bib.bib5) ) and can be seen as learning an
    infinite number of (or a distribution over) neural networks and then aggregating
    the results through ensembling.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 与BDL高度相关的一个主题是贝叶斯神经网络（BNN）或神经网络的贝叶斯处理。类似于任何贝叶斯处理，BNN对神经网络的参数施加了先验，并旨在学习这些参数的后验分布。在推断阶段，这种分布被边际化以生成最终预测。一般来说，这一过程被称为贝叶斯模型平均（[PRML,](#bib.bib5)），可以视为学习无限数量（或其分布）的神经网络，然后通过集成来汇总结果。
- en: The study of BNN dates back to 1990s with notable works from ([mackay1992practical,](#bib.bib72)
    ; [hinton1993keeping,](#bib.bib42) ; [neal1995bayesian,](#bib.bib80) ). Over the
    years, a large body of works ([DBLP:conf/nips/Graves11,](#bib.bib31) ; [kingma2013auto,](#bib.bib58)
    ; [DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39) ; [DBLP:conf/icml/BlundellCKW15,](#bib.bib9)
    ; [balan2015bayesian,](#bib.bib2) ; [MaxNPN,](#bib.bib100) ) have emerged to enable
    substantially better scalability and incorporate recent advancements of deep neural
    networks. Due to BNN’s long history, the term ‘Bayesian deep learning’ sometimes
    specifically refers to ‘Bayesian neural networks’ ([maddox2019simple,](#bib.bib73)
    ; [BDL-report,](#bib.bib128) ). In this survey, we instead use ‘Bayesian deep
    learning’ in a broader sense to refer to the probabilistic framework subsuming
    Bayesian neural networks. To see this, note that a BDL model with a *perception
    component* and an empty *task-specific component* is equivalent to a Bayesian
    neural network (details on these two components are discussed in Section [4.2](#S4.SS2
    "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning")).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: BNN的研究可以追溯到1990年代，著名的工作有（[mackay1992practical,](#bib.bib72) ; [hinton1993keeping,](#bib.bib42)
    ; [neal1995bayesian,](#bib.bib80)）。多年来，出现了大量的工作（[DBLP:conf/nips/Graves11,](#bib.bib31)
    ; [kingma2013auto,](#bib.bib58) ; [DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39)
    ; [DBLP:conf/icml/BlundellCKW15,](#bib.bib9) ; [balan2015bayesian,](#bib.bib2)
    ; [MaxNPN,](#bib.bib100)），以实现显著更好的可扩展性并融入深度神经网络的最新进展。由于BNN有着悠久的历史，“贝叶斯深度学习”一词有时专指“贝叶斯神经网络”（[maddox2019simple,](#bib.bib73)
    ; [BDL-report,](#bib.bib128)）。在本调查中，我们将“贝叶斯深度学习”用作广义上的术语，指代涵盖贝叶斯神经网络的概率框架。要理解这一点，请注意，具有*感知组件*和空的*任务特定组件*的BDL模型等同于贝叶斯神经网络（有关这两个组件的详细信息，请参见第[4.2节](#S4.SS2
    "4.2. General Framework ‣ 4. 贝叶斯深度学习 ‣ 贝叶斯深度学习调查")）。
- en: Interestingly, though BNN started in 1990s, the study of BDL in a broader sense
    started roughly in 2014 ([RSDAE,](#bib.bib118) ; [CDL,](#bib.bib121) ; [DPFM,](#bib.bib38)
    ; [BDL-thesis,](#bib.bib114) ), slightly after the deep learning breakthrough
    in the ImageNet LSVRC contest in 2012 ([krizhevsky2012imagenet,](#bib.bib62) ).
    As we will see in later sections, BNN is usually used as a perception component
    in BDL models.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，尽管BNN起步于1990年代，但广义上的BDL研究大致开始于2014年（[RSDAE,](#bib.bib118) ; [CDL,](#bib.bib121)
    ; [DPFM,](#bib.bib38) ; [BDL-thesis,](#bib.bib114)），略晚于2012年ImageNet LSVRC竞赛中的深度学习突破（[krizhevsky2012imagenet,](#bib.bib62)）。正如我们将在后续章节中看到的，BNN通常作为BDL模型中的感知组件使用。
- en: Today BDL is gaining more and more popularity, has found successful applications
    in areas such as recommender systems and computer vision, and appears as the theme
    of various conference workshops (e.g., the NeurIPS BDL workshop⁶⁶6[http://bayesiandeeplearning.org/](http://bayesiandeeplearning.org/)).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，BDL 正在获得越来越多的关注，已在推荐系统和计算机视觉等领域找到成功应用，并且作为各种会议研讨会的主题（例如，NeurIPS BDL 研讨会⁶⁶6[http://bayesiandeeplearning.org/](http://bayesiandeeplearning.org/)）。
- en: 4.2\. General Framework
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 一般框架
- en: 'As mentioned in Section [1](#S1 "1\. Introduction ‣ A Survey on Bayesian Deep
    Learning"), BDL is a principled probabilistic framework with two seamlessly integrated
    components: a *perception component* and a *task-specific component*.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[1](#S1 "1\. 引言 ‣ 贝叶斯深度学习调查")节中所述，BDL 是一个有原则的概率框架，包含两个无缝集成的组件：一个*感知组件*和一个*任务特定组件*。
- en: 'Two Components: Figure [5](#S4.F5 "Figure 5 ‣ 4.2\. General Framework ‣ 4\.
    Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning") shows the PGM of
    a simple BDL model as an example. The part inside the red rectangle on the left
    represents the perception component and the part inside the blue rectangle on
    the right is the task-specific component. Typically, the perception component
    would be a probabilistic formulation of a deep learning model with multiple nonlinear
    processing layers represented as a chain structure in the PGM. While the nodes
    and edges in the perception component are relatively simple, those in the task-specific
    component often describe more complex distributions and relationships among variables.
    Concretely, a task-specific component can take various forms. For example, it
    can be a typical Bayesian network (directed PGM) such as LDA, a deep Bayesian
    network ([BIN,](#bib.bib117) ), or a stochastic process ([ross1996stochastic,](#bib.bib94)
    ; [RPPU,](#bib.bib51) ), all of which can be represented in the form of PGM.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 两个组件：图[5](#S4.F5 "图 5 ‣ 4.2\. 一般框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习调查")显示了一个简单 BDL 模型的PGM作为示例。左侧红色矩形内的部分代表感知组件，右侧蓝色矩形内的部分代表任务特定组件。通常，感知组件会是一个深度学习模型的概率表述，具有多个非线性处理层，在PGM中表现为链式结构。虽然感知组件中的节点和边相对简单，但任务特定组件中的节点和边通常描述了变量之间更复杂的分布和关系。具体来说，任务特定组件可以采取各种形式。例如，它可以是典型的贝叶斯网络（有向PGM），如LDA、深度贝叶斯网络（[BIN,](#bib.bib117)），或随机过程（[ross1996stochastic,](#bib.bib94)；[RPPU,](#bib.bib51)），所有这些都可以用PGM的形式表示。
- en: '![Refer to caption](img/2cfab2827eb19b48e5b4aa756b994559.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cfab2827eb19b48e5b4aa756b994559.png)'
- en: Figure 5\. The PGM for an example BDL. The red rectangle on the left indicates
    the perception component, and the blue rectangle on the right indicates the task-specific
    component. The hinge variable $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    H}\}$.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 示例BDL的PGM。左侧的红色矩形表示感知组件，右侧的蓝色矩形表示任务特定组件。铰链变量 $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    H}\}$。
- en: 'Three Variable Sets: There are three sets of variables in a BDL model: perception
    variables, hinge variables, and task variables. In this paper, we use $\mbox{\boldmath$\Omega$\unboldmath}_{p}$
    to denote the set of perception variables (e.g., ${\bf X}_{0}$, ${\bf X}_{1}$,
    and ${\bf W}_{1}$ in Figure [5](#S4.F5 "Figure 5 ‣ 4.2\. General Framework ‣ 4\.
    Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning")), which are the
    variables in the perception component. Usually $\mbox{\boldmath$\Omega$\unboldmath}_{p}$
    would include the weights and neurons in the probabilistic formulation of a deep
    learning model. $\mbox{\boldmath$\Omega$\unboldmath}_{h}$ is used to denote the
    set of hinge variables (e.g. ${\bf H}$ in Figure [5](#S4.F5 "Figure 5 ‣ 4.2\.
    General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning")).
    These variables directly interact with the perception component from the task-specific
    component. The set of task variables (e.g. ${\bf A}$, ${\bf B}$, and ${\bf C}$
    in Figure [5](#S4.F5 "Figure 5 ‣ 4.2\. General Framework ‣ 4\. Bayesian Deep Learning
    ‣ A Survey on Bayesian Deep Learning")), i.e., variables in the task-specific
    component without direct relation to the perception component, is denoted as $\mbox{\boldmath$\Omega$\unboldmath}_{t}$.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 三个变量集合：BDL 模型中有三个变量集合：感知变量、铰链变量和任务变量。在本文中，我们用$\mbox{\boldmath$\Omega$\unboldmath}_{p}$表示感知变量集合（例如，图[5](#S4.F5
    "Figure 5 ‣ 4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on
    Bayesian Deep Learning")中的${\bf X}_{0}$、${\bf X}_{1}$和${\bf W}_{1}$），这些是感知组件中的变量。通常$\mbox{\boldmath$\Omega$\unboldmath}_{p}$会包括深度学习模型概率公式中的权重和神经元。$\mbox{\boldmath$\Omega$\unboldmath}_{h}$用来表示铰链变量集合（例如，图[5](#S4.F5
    "Figure 5 ‣ 4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on
    Bayesian Deep Learning")中的${\bf H}$）。这些变量直接与来自任务特定组件的感知组件互动。任务变量集合（例如，图[5](#S4.F5
    "Figure 5 ‣ 4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on
    Bayesian Deep Learning")中的${\bf A}$、${\bf B}$和${\bf C}$），即与感知组件没有直接关系的任务特定组件中的变量，用$\mbox{\boldmath$\Omega$\unboldmath}_{t}$表示。
- en: 'Generative Processes for Supervised and Unsupervised Learning: If the edges
    between the two components *point towards* $\mbox{\boldmath$\Omega$\unboldmath}_{h}$,
    the joint distribution of all variables can be written as:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习的生成过程：如果两个组件之间的边*指向*$\mbox{\boldmath$\Omega$\unboldmath}_{h}$，则所有变量的联合分布可以写作：
- en: '| (2) |  | $\displaystyle p(\mbox{\boldmath$\Omega$\unboldmath}_{p},\mbox{\boldmath$\Omega$\unboldmath}_{h},\mbox{\boldmath$\Omega$\unboldmath}_{t})=p(\mbox{\boldmath$\Omega$\unboldmath}_{p})p(\mbox{\boldmath$\Omega$\unboldmath}_{h}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{p})p(\mbox{\boldmath$\Omega$\unboldmath}_{t}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{h}).$
    |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\displaystyle p(\mbox{\boldmath$\Omega$\unboldmath}_{p},\mbox{\boldmath$\Omega$\unboldmath}_{h},\mbox{\boldmath$\Omega$\unboldmath}_{t})=p(\mbox{\boldmath$\Omega$\unboldmath}_{p})p(\mbox{\boldmath$\Omega$\unboldmath}_{h}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{p})p(\mbox{\boldmath$\Omega$\unboldmath}_{t}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{h}).$
    |  |'
- en: 'If the edges between the two components *originate from* $\mbox{\boldmath$\Omega$\unboldmath}_{h}$,
    the joint distribution of all variables can be written as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个组件之间的边*起始于*$\mbox{\boldmath$\Omega$\unboldmath}_{h}$，则所有变量的联合分布可以写作：
- en: '| (3) |  | $\displaystyle p(\mbox{\boldmath$\Omega$\unboldmath}_{p},\mbox{\boldmath$\Omega$\unboldmath}_{h},\mbox{\boldmath$\Omega$\unboldmath}_{t})=p(\mbox{\boldmath$\Omega$\unboldmath}_{t})p(\mbox{\boldmath$\Omega$\unboldmath}_{h}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{t})p(\mbox{\boldmath$\Omega$\unboldmath}_{p}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{h}).$
    |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle p(\mbox{\boldmath$\Omega$\unboldmath}_{p},\mbox{\boldmath$\Omega$\unboldmath}_{h},\mbox{\boldmath$\Omega$\unboldmath}_{t})=p(\mbox{\boldmath$\Omega$\unboldmath}_{t})p(\mbox{\boldmath$\Omega$\unboldmath}_{h}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{t})p(\mbox{\boldmath$\Omega$\unboldmath}_{p}&#124;\mbox{\boldmath$\Omega$\unboldmath}_{h}).$
    |  |'
- en: Equation ([2](#S4.E2 "In 4.2\. General Framework ‣ 4\. Bayesian Deep Learning
    ‣ A Survey on Bayesian Deep Learning")) and ([3](#S4.E3 "In 4.2\. General Framework
    ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning")) assume different
    generative processes for the data and correspond to different learning tasks.
    The former is usually used for supervised learning, where the perception component
    serves as a probabilistic (or Bayesian) representation learner to facilitate any
    downstream tasks (see Section [5.1](#S5.SS1 "5.1\. Supervised Bayesian Deep Learning
    for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey
    on Bayesian Deep Learning") for some examples). The latter is usually used for
    unsupervised learning, where the task-specific component provides structured constraints
    and domain knowledge to help the perception component learn stronger representations
    (see Section [5.2](#S5.SS2 "5.2\. Unsupervised Bayesian Deep Learning for Topic
    Models ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep
    Learning") for some examples).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 方程（[2](#S4.E2 "在4.2\. 一般框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述")）和（[3](#S4.E3 "在4.2\.
    一般框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述")）假设了不同的数据生成过程，并对应不同的学习任务。前者通常用于监督学习，其中感知组件作为概率（或贝叶斯）表示学习器，以促进任何下游任务（见第[5.1](#S5.SS1
    "5.1\. 监督贝叶斯深度学习用于推荐系统 ‣ 5\. 具体的BDL模型和应用 ‣ 贝叶斯深度学习综述")节中的一些示例）。后者通常用于无监督学习，其中任务特定组件提供结构约束和领域知识，以帮助感知组件学习更强的表示（见第[5.2](#S5.SS2
    "5.2\. 无监督贝叶斯深度学习用于主题模型 ‣ 5\. 具体的BDL模型和应用 ‣ 贝叶斯深度学习综述")节中的一些示例）。
- en: Note that besides these two vanilla cases, it is possible for BDL to simultaneously
    have some edges between the two components pointing towards $\mbox{\boldmath$\Omega$\unboldmath}_{h}$
    and some originating from $\mbox{\boldmath$\Omega$\unboldmath}_{h}$, in which
    case the decomposition of the joint distribution would be more complex.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了这两种普通情况外，BDL还可能在两个组件之间同时存在指向$\mbox{\boldmath$\Omega$\unboldmath}_{h}$的边缘和一些从$\mbox{\boldmath$\Omega$\unboldmath}_{h}$起源的边缘，在这种情况下，联合分布的分解将更加复杂。
- en: 'Independence Requirement: The introduction of hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}$
    and related conditional distributions simplifies the model (especially when $\mbox{\boldmath$\Omega$\unboldmath}_{h}$’s
    in-degree or out-degree is $1$), facilitate learning, and provides inductive bias
    to concentrate information inside $\mbox{\boldmath$\Omega$\unboldmath}_{h}$. Note
    that hinge variables are always in the task-specific component; the connections
    between hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}$ and the perception
    component (e.g., ${\bf X}_{4}\rightarrow{\bf H}$ in Figure [5](#S4.F5 "Figure
    5 ‣ 4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian
    Deep Learning")) should normally be independent for convenience of parallel computation
    in the perception component. For example, each row in ${\bf H}$ is related to
    only one corresponding row in ${\bf X}_{4}$. Although it is not mandatory in BDL
    models, meeting this requirement would significantly increase the efficiency of
    parallel computation in model training.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 独立性要求：引入铰链变量$\mbox{\boldmath$\Omega$\unboldmath}_{h}$及相关条件分布简化了模型（特别是当$\mbox{\boldmath$\Omega$\unboldmath}_{h}$的入度或出度为$1$时），促进学习，并提供归纳偏差，将信息集中在$\mbox{\boldmath$\Omega$\unboldmath}_{h}$内部。注意，铰链变量始终存在于任务特定组件中；铰链变量$\mbox{\boldmath$\Omega$\unboldmath}_{h}$与感知组件之间的连接（例如，图[5](#S4.F5
    "图5 ‣ 4.2\. 一般框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述")中的${\bf X}_{4}\rightarrow{\bf H}$）通常应独立，以方便感知组件中的并行计算。例如，${\bf
    H}$中的每一行仅与${\bf X}_{4}$中的一行对应。虽然在BDL模型中这不是强制要求，但满足这一要求将显著提高模型训练中的并行计算效率。
- en: 'Flexibility of Variance for $\mbox{\boldmath$\Omega$\unboldmath}_{h}$: As mentioned
    in Section [1](#S1 "1\. Introduction ‣ A Survey on Bayesian Deep Learning"), one
    of BDL’s motivations is to model the *uncertainty of exchanging information* between
    the perception component and the task-specific component, which boils down to
    modeling the uncertainty related to $\mbox{\boldmath$\Omega$\unboldmath}_{h}$.
    For example, such uncertainty is reflected in the variance of the conditional
    density $p(\mbox{\boldmath$\Omega$\unboldmath}_{h}|\mbox{\boldmath$\Omega$\unboldmath}_{p})$
    in Equation ([2](#S4.E2 "In 4.2\. General Framework ‣ 4\. Bayesian Deep Learning
    ‣ A Survey on Bayesian Deep Learning"))⁷⁷7For models with the joint likelihood
    decomposed as in Equation ([3](#S4.E3 "In 4.2\. General Framework ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")), the uncertainty is reflected
    in the variance of $p(\mbox{\boldmath$\Omega$\unboldmath}_{p}|\mbox{\boldmath$\Omega$\unboldmath}_{h})$..
    According to the degree of flexibility, there are three types of variance for
    $\mbox{\boldmath$\Omega$\unboldmath}_{h}$ (for simplicity we assume the joint
    likelihood of BDL is Equation ([2](#S4.E2 "In 4.2\. General Framework ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")), $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{p\}$,
    $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{h\}$, and $p(\mbox{\boldmath$\Omega$\unboldmath}_{h}|\mbox{\boldmath$\Omega$\unboldmath}_{p})={\mathcal{N}}(h|\mu_{p},\sigma_{p}^{2})$
    in our example):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: $\mbox{\boldmath$\Omega$\unboldmath}_{h}$ 方差的灵活性：如[1](#S1 "1\. Introduction
    ‣ A Survey on Bayesian Deep Learning")节中提到的，BDL的一个动机是建模*信息交换的不确定性*，这归结为建模与 $\mbox{\boldmath$\Omega$\unboldmath}_{h}$
    相关的不确定性。例如，这种不确定性反映在方程([2](#S4.E2 "In 4.2\. General Framework ‣ 4\. Bayesian Deep
    Learning ‣ A Survey on Bayesian Deep Learning"))中的条件密度 $p(\mbox{\boldmath$\Omega$\unboldmath}_{h}|\mbox{\boldmath$\Omega$\unboldmath}_{p})$
    的方差中⁷⁷7对于那些将联合似然分解如方程([3](#S4.E3 "In 4.2\. General Framework ‣ 4\. Bayesian Deep
    Learning ‣ A Survey on Bayesian Deep Learning"))的模型，这种不确定性反映在 $p(\mbox{\boldmath$\Omega$\unboldmath}_{p}|\mbox{\boldmath$\Omega$\unboldmath}_{h})$
    的方差中。根据灵活性的程度，对于 $\mbox{\boldmath$\Omega$\unboldmath}_{h}$ 有三种类型的方差（为了简便，我们假设BDL的联合似然为方程([2](#S4.E2
    "In 4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian
    Deep Learning"))，$\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{p\}$，$\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{h\}$，并且
    $p(\mbox{\boldmath$\Omega$\unboldmath}_{h}|\mbox{\boldmath$\Omega$\unboldmath}_{p})={\mathcal{N}}(h|\mu_{p},\sigma_{p}^{2})$
    在我们的例子中)：
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-Variance: Zero-Variance (ZV) assumes no uncertainty during the information
    exchange between the two components. In the example, zero-variance means directly
    setting $\sigma_{p}^{2}$ to $0$.'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零方差：零方差（ZV）假设在两个组件之间的信息交换过程中没有不确定性。在这个例子中，零方差意味着将 $\sigma_{p}^{2}$ 直接设置为 $0$。
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyper-Variance: Hyper-Variance (HV) assumes that uncertainty during the information
    exchange is defined through hyperparameters. In the example, HV means that $\sigma_{p}^{2}$
    is a manually tuned hyperparameter.'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超方差：超方差（HV）假设信息交换过程中的不确定性由超参数定义。在这个例子中，HV 意味着 $\sigma_{p}^{2}$ 是一个手动调整的超参数。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learnable Variance: Learnable Variance (LV) uses learnable parameters to represent
    uncertainty during the information exchange. In the example, $\sigma_{p}^{2}$
    is the learnable parameter.'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可学习方差：可学习方差（LV）使用可学习的参数来表示信息交换过程中的不确定性。在这个例子中，$\sigma_{p}^{2}$ 是可学习的参数。
- en: As shown above, we can see that in terms of model flexibility, $\text{LV}>\text{HV}>\text{ZV}$.
    Normally, if properly regularized, an LV model outperforms an HV model, which
    is superior to a ZV model. In Table [1](#S3.T1 "Table 1 ‣ 3.1\. Models ‣ 3\. Probabilistic
    Graphical Models ‣ A Survey on Bayesian Deep Learning"), we show the types of
    variance for $\mbox{\boldmath$\Omega$\unboldmath}_{h}$ in different BDL models.
    Note that although each model in the table has a specific type, one can always
    adjust the models to devise their counterparts of other types. For example, while
    CDL in the table is an HV model, we can easily adjust $p(\mbox{\boldmath$\Omega$\unboldmath}_{h}|\mbox{\boldmath$\Omega$\unboldmath}_{p})$
    in CDL to devise its ZV and LV counterparts. In ([CDL,](#bib.bib121) ), the authors
    compare the performance of an HV CDL and a ZV CDL and find that the former performs
    significantly better, meaning that sophisticatedly modeling uncertainty between
    two components is essential for performance.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，我们可以看到，就模型灵活性而言，$\text{LV}>\text{HV}>\text{ZV}$。通常，如果适当正则化，LV模型会优于HV模型，而HV模型又优于ZV模型。在表[1](#S3.T1
    "Table 1 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian
    Deep Learning")中，我们展示了不同BDL模型中$\mbox{\boldmath$\Omega$\unboldmath}_{h}$的方差类型。请注意，尽管表中的每个模型都有特定类型，但总是可以调整模型以设计出其他类型的对应模型。例如，虽然表中的CDL是HV模型，但我们可以轻松调整$
    p(\mbox{\boldmath$\Omega$\unboldmath}_{h}|\mbox{\boldmath$\Omega$\unboldmath}_{p})$以设计出它的ZV和LV对应模型。在([CDL,](#bib.bib121))中，作者比较了HV
    CDL和ZV CDL的性能，发现前者表现显著更好，这意味着在两个组件之间精细建模不确定性对性能至关重要。
- en: 'Learning Algorithms: Due to the nature of BDL, practical learning algorithms
    need to meet the following criteria:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法：由于BDL的性质，实际学习算法需要满足以下标准：
- en: (1)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: They should be online algorithms in order to scale well for large datasets.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它们应该是在线算法，以便对大数据集进行良好的扩展。
- en: (2)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: They should be efficient enough to scale linearly with the number of free parameters
    in the perception component.
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它们应该足够高效，以便随着感知组件中自由参数数量的增加而线性扩展。
- en: Criterion (1) implies that conventional variational inference or MCMC methods
    are not applicable. Usually an online version of them is needed ([onlineVB,](#bib.bib45)
    ). Most SGD-based methods do not work either unless only MAP inference (as opposed
    to Bayesian treatments) is performed. Criterion (2) is needed because there are
    typically a large number of free parameters in the perception component. This
    means methods based on Laplace approximation ([mackay1992practical,](#bib.bib72)
    ) are not realistic since they involve the computation of a Hessian matrix that
    scales quadratically with the number of free parameters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标准(1)意味着传统的变分推断或MCMC方法不适用。通常需要它们的在线版本([onlineVB,](#bib.bib45))。除非仅执行MAP推断（而非贝叶斯处理），否则大多数基于SGD的方法也无效。标准(2)是必要的，因为感知组件中通常有大量自由参数。这意味着基于拉普拉斯近似([mackay1992practical,](#bib.bib72))的方法不现实，因为它们涉及计算一个随着自由参数数量增加而二次扩展的Hessian矩阵。
- en: 4.3\. Perception Component
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 感知组件
- en: Ideally, the perception component should be a probabilistic or Bayesian neural
    network, in order to be compatible with the task-specific component, which is
    probabilistic in nature. This is to ensure the perception component’s built-in
    capability to handle uncertainty of parameters and its output.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，感知组件应该是一个概率或贝叶斯神经网络，以便与任务特定组件兼容，而任务特定组件本质上是概率性的。这是为了确保感知组件内置的处理参数和输出不确定性的能力。
- en: As mentioned in Section [4.1](#S4.SS1 "4.1\. A Brief History of Bayesian Neural
    Networks and Bayesian Deep Learning ‣ 4\. Bayesian Deep Learning ‣ A Survey on
    Bayesian Deep Learning"), the study of Bayesian neural networks dates back to
    1990s ([mackay1992practical,](#bib.bib72) ; [hinton1993keeping,](#bib.bib42) ;
    [neal1995bayesian,](#bib.bib80) ; [DBLP:conf/nips/Graves11,](#bib.bib31) ). However,
    pioneering work at that time was not widely adopted due to its lack of scalability.
    To address the this issue, there has been recent development such as restricted
    Boltzmann machine (RBM) ([DBN-fast,](#bib.bib41) ; [RBM,](#bib.bib40) ), probabilistic
    generalized stacked denoising autoencoders (pSDAE) ([RSDAE,](#bib.bib118) ; [CDL,](#bib.bib121)
    ), variational autoencoders (VAE) ([kingma2013auto,](#bib.bib58) ), probabilistic
    back-propagation (PBP) ([DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39) ), Bayes
    by Backprop (BBB) ([DBLP:conf/icml/BlundellCKW15,](#bib.bib9) ), Bayesian dark
    knowledge (BDK) ([balan2015bayesian,](#bib.bib2) ), and natural-parameter networks
    (NPN) ([NPN,](#bib.bib119) ).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如[4.1](#S4.SS1 "4.1\. 贝叶斯神经网络和贝叶斯深度学习的简要历史 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习调查")节中所述，贝叶斯神经网络的研究可以追溯到1990年代
    ([mackay1992practical,](#bib.bib72) ; [hinton1993keeping,](#bib.bib42) ; [neal1995bayesian,](#bib.bib80)
    ; [DBLP:conf/nips/Graves11,](#bib.bib31) )。然而，当时的开创性工作由于缺乏可扩展性而未被广泛采用。为了应对这一问题，最近发展出了限制玻尔兹曼机（RBM）
    ([DBN-fast,](#bib.bib41) ; [RBM,](#bib.bib40) ）、概率生成叠加去噪自编码器（pSDAE） ([RSDAE,](#bib.bib118)
    ; [CDL,](#bib.bib121) ）、变分自编码器（VAE） ([kingma2013auto,](#bib.bib58) ）、概率反向传播（PBP）
    ([DBLP:conf/icml/Hernandez-Lobato15b,](#bib.bib39) ）、贝叶斯反向传播（BBB） ([DBLP:conf/icml/BlundellCKW15,](#bib.bib9)
    ）、贝叶斯黑暗知识（BDK） ([balan2015bayesian,](#bib.bib2) ）和自然参数网络（NPN） ([NPN,](#bib.bib119)
    )。
- en: More recently, generative adversarial networks (GAN) ([GAN,](#bib.bib30) ) prevail
    as a new training scheme for training neural networks and have shown promise in
    generating photo-realistic images. Later on, Bayesian formulations (as well as
    related theoretical results) for GAN have also been proposed ([GAN,](#bib.bib30)
    ; [ProbGAN,](#bib.bib37) ). These models are also potential building blocks as
    the BDL framework’s perception component.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，生成对抗网络（GAN） ([GAN,](#bib.bib30) ) 作为一种新的神经网络训练方案越来越受到关注，并且在生成照片级真实图像方面表现出很大的潜力。后来，针对GAN的贝叶斯公式（以及相关理论结果）也被提出
    ([GAN,](#bib.bib30) ; [ProbGAN,](#bib.bib37) )。这些模型也有可能作为BDL框架的感知组件的基础构件。
- en: In this subsection, we mainly focus on the introduction of recent Baysian neural
    networks such as RBM, pSDAE, VAE, and NPN. We refer the readers to ([dlbook,](#bib.bib29)
    ) for earlier work in this direction.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们主要介绍了近期的贝叶斯神经网络，如RBM、pSDAE、VAE和NPN。有关早期工作的更多信息，请参见 ([dlbook,](#bib.bib29)
    )。
- en: 4.3.1\. Restricted Boltzmann Machine
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 限制玻尔兹曼机
- en: 'Restricted Boltzmann Machine (RBM) is a special kind of BNN in that (1) it
    is not trained with back-propagation (BP) and that (2) its hidden neurons are
    binary. Specifically, RBM defines the following energy:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机（RBM）是一种特殊的贝叶斯神经网络（BNN），因为（1）它不是通过反向传播（BP）进行训练的，（2）其隐藏神经元是二进制的。具体而言，RBM定义了以下能量：
- en: '|  | $\displaystyle E({\bf v},{\bf h})=-{\bf v}^{T}{\bf W}{\bf h}-{\bf v}^{T}{\bf
    b}-{\bf h}^{T}{\bf a},$ |  |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle E({\bf v},{\bf h})=-{\bf v}^{T}{\bf W}{\bf h}-{\bf v}^{T}{\bf
    b}-{\bf h}^{T}{\bf a},$ |  |'
- en: 'where ${\bf v}$ denotes visible (observed) neurons, and ${\bf h}$ denotes binary
    hidden neurons. ${\bf W}$, ${\bf a}$, and ${\bf b}$ are learnable weights. The
    energy function leads to the following conditional distributions:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf v}$ 表示可见（观察到的）神经元，${\bf h}$ 表示二进制隐藏神经元。${\bf W}$、${\bf a}$ 和 ${\bf
    b}$ 是可学习的权重。能量函数导致以下条件分布：
- en: '| (4) |  | $\displaystyle p({\bf v}&#124;{\bf h})=\frac{\exp(-E({\bf v},{\bf
    h}))}{\sum\limits_{{\bf v}}\exp(-E({\bf v},{\bf h}))},\;\;\;\;\;\;\;\;p({\bf h}&#124;{\bf
    v})=\frac{\exp(-E({\bf v},{\bf h}))}{\sum\limits_{{\bf h}}\exp(-E({\bf v},{\bf
    h}))}$ |  |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle p({\bf v}\mid{\bf h})=\frac{\exp(-E({\bf v},{\bf
    h}))}{\sum\limits_{{\bf v}}\exp(-E({\bf v},{\bf h}))},\;\;\;\;\;\;\;\;p({\bf h}\mid{\bf
    v})=\frac{\exp(-E({\bf v},{\bf h}))}{\sum\limits_{{\bf h}}\exp(-E({\bf v},{\bf
    h}))}$ |  |'
- en: RBM is trained using ‘Contrastive Divergence’ ([DBN-fast,](#bib.bib41) ) rather
    than BP. Once trained, RBM can infer ${\bf v}$ or ${\bf h}$ by marginalizing out
    other neurons. One can also stack layers of RBM to form a deep belief network
    (DBN) ([DBN-speech,](#bib.bib76) ), use multiple branches of deep RBN for multimodal
    learning ([MultRBM,](#bib.bib104) ), or combine DBN with convolutional layers
    to form a convolutional DBN ([ConvDBN,](#bib.bib65) ).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: RBM 使用‘对比散度’ ([DBN-fast,](#bib.bib41)) 而不是 BP 进行训练。训练完成后，RBM 可以通过边缘化其他神经元来推断
    ${\bf v}$ 或 ${\bf h}$。也可以将 RBM 层堆叠形成深度信念网络 (DBN) ([DBN-speech,](#bib.bib76))，使用多个深度
    RBN 分支进行多模态学习 ([MultRBM,](#bib.bib104))，或将 DBN 与卷积层结合形成卷积 DBN ([ConvDBN,](#bib.bib65))。
- en: 4.3.2\. Probabilistic Generalized SDAE
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 概率化广义 SDAE
- en: 'Following the introduction of SDAE in Section [2.2](#S2.SS2 "2.2\. Autoencoders
    ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep Learning"), if we assume that
    both the clean input ${\bf X}_{c}$ and the corrupted input ${\bf X}_{0}$ are observed,
    similar to ([PRML,](#bib.bib5) ; [mackay1992practical,](#bib.bib72) ; [DBLP:conf/nips/BengioYAV13,](#bib.bib4)
    ; [nmSDAE,](#bib.bib13) ), we can define the following generative process of the
    probabilistic SDAE:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 继第 [2.2](#S2.SS2 "2.2\. 自编码器 ‣ 2\. 深度学习 ‣ 贝叶斯深度学习概述") 节中 SDAE 的介绍后，如果我们假设干净输入
    ${\bf X}_{c}$ 和受损输入 ${\bf X}_{0}$ 都已观测到，类似于 ([PRML,](#bib.bib5) ; [mackay1992practical,](#bib.bib72)
    ; [DBLP:conf/nips/BengioYAV13,](#bib.bib4) ; [nmSDAE,](#bib.bib13))，我们可以定义以下概率
    SDAE 的生成过程：
- en: (1)
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: For each layer $l$ of the SDAE network,
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 SDAE 网络的每一层 $l$，
- en: (a)
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: For each column $n$ of the weight matrix ${\bf W}_{l}$, draw ${\bf W}_{l,*n}\sim{\mathcal{N}}({\bf
    0},\lambda_{w}^{-1}{\bf I}_{K_{l}}).$
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于权重矩阵 ${\bf W}_{l}$ 的每一列 $n$，绘制 ${\bf W}_{l,*n}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf
    I}_{K_{l}}).$
- en: (b)
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Draw the bias vector ${\bf b}_{l}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$.
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制偏置向量 ${\bf b}_{l}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf I}_{K_{l}})$。
- en: (c)
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: For each row $j$ of ${\bf X}_{l}$, draw
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每一行 $j$ 的 ${\bf X}_{l}$，绘制
- en: '| (5) |  | $\displaystyle{\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf X}_{l-1,j*}{\bf
    W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$ |  |'
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle{\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf X}_{l-1,j*}{\bf
    W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$ |  |'
- en: (2)
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: For each item $j$, draw a clean input⁸⁸8Note that while generation of the *clean*
    input ${\bf X}_{c}$ from ${\bf X}_{L}$ is part of the generative process of the
    Bayesian SDAE, generation of the *noise-corrupted* input ${\bf X}_{0}$ from ${\bf
    X}_{c}$ is an artificial noise injection process to help the SDAE learn a more
    robust feature representation. ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf
    I}_{B}).$
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个项目 $j$，绘制一个干净的输入⁸⁸8注意，虽然从 ${\bf X}_{L}$ 生成 *干净* 输入 ${\bf X}_{c}$ 是贝叶斯 SDAE
    的生成过程的一部分，但从 ${\bf X}_{c}$ 生成 *噪声干扰* 输入 ${\bf X}_{0}$ 是一种人为噪声注入过程，帮助 SDAE 学习更鲁棒的特征表示。
    ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf I}_{B}).$
- en: Note that if $\lambda_{s}$ goes to infinity, the Gaussian distribution in Equation
    ([5](#S4.E5 "In item 1c ‣ item 1 ‣ 4.3.2\. Probabilistic Generalized SDAE ‣ 4.3\.
    Perception Component ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning")) will become a Dirac delta distribution ([strichartz2003guide,](#bib.bib106)
    ) centered at $\sigma({\bf X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})$, where $\sigma(\cdot)$
    is the sigmoid function, and the model will degenerate into a Bayesian formulation
    of vanilla SDAE. This is why we call it ‘generalized’ SDAE.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果 $\lambda_{s}$ 趋于无穷大，方程中 Gaussian 分布 ([5](#S4.E5 "在条目 1c ‣ 条目 1 ‣ 4.3.2\.
    概率化广义 SDAE ‣ 4.3\. 感知组件 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习概述")) 将变成一个以 $\sigma({\bf X}_{l-1,j*}{\bf
    W}_{l}+{\bf b}_{l})$ 为中心的 Dirac delta 分布 ([strichartz2003guide,](#bib.bib106))，其中
    $\sigma(\cdot)$ 是 sigmoid 函数，模型将退化为 vanilla SDAE 的贝叶斯公式。这就是我们称之为‘广义’ SDAE 的原因。
- en: The first $L/2$ layers of the network act as an encoder and the last $L/2$ layers
    act as a decoder. Maximization of the posterior probability is equivalent to minimization
    of the reconstruction error with weight decay taken into consideration.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 网络的前 $L/2$ 层作为编码器，最后 $L/2$ 层作为解码器。后验概率的最大化等同于在考虑权重衰减的情况下最小化重构误差。
- en: Following pSDAE, both its convolutional version ([CKE,](#bib.bib132) ) and its
    recurrent version ([CRAE,](#bib.bib122) ) have been proposed with applications
    in knowledge base embedding and recommender systems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 继 pSDAE 之后，它的卷积版本 ([CKE,](#bib.bib132)) 和递归版本 ([CRAE,](#bib.bib122)) 已被提出，应用于知识库嵌入和推荐系统。
- en: 4.3.3\. Variational Autoencoders
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3\. 变分自编码器
- en: 'Variational Autoencoders (VAE) ([kingma2013auto,](#bib.bib58) ) essentially
    tries to learn parameters $\phi$ and $\theta$ that maximize the evidence lower
    bound (ELBO):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器（VAE） ([kingma2013auto,](#bib.bib58)) 本质上试图学习参数 $\phi$ 和 $\theta$，以最大化证据下界（ELBO）：
- en: '| (6) |  | $\displaystyle\mathcal{L}_{vae}=E_{q_{\phi}({\bf z}&#124;{\bf x})}[\log
    p_{\theta}({\bf x}&#124;{\bf z})]-KL(q_{\phi}({\bf z}&#124;{\bf x})\&#124;p({\bf
    z})),$ |  |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\mathcal{L}_{vae}=E_{q_{\phi}({\bf z}&#124;{\bf x})}[\log
    p_{\theta}({\bf x}&#124;{\bf z})]-KL(q_{\phi}({\bf z}&#124;{\bf x})\&#124;p({\bf
    z})),$ |  |'
- en: where $q_{\phi}({\bf z}|{\bf x})$ is the encoder parameterized by $\phi$ and
    $p_{\theta}({\bf x}|{\bf z})$ is the decoder parameterized by $\theta$. The negation
    of the first term is similar to the reconstrunction error in vanilla AE, while
    the KL divergence works as a regularization term for the encoder. During training
    $q_{\phi}({\bf z}|{\bf x})$ will output the mean and variance of a Gaussian distribution,
    from which ${\bf z}$ is sampled via the reparameterization trick. Usually $q_{\phi}({\bf
    z}|{\bf x})$ is parameterized by an MLP with two branches, one producing the mean
    and the other producing the variance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q_{\phi}({\bf z}|{\bf x})$ 是由 $\phi$ 参数化的编码器，而 $p_{\theta}({\bf x}|{\bf
    z})$ 是由 $\theta$ 参数化的解码器。第一个项的否定类似于普通自编码器中的重构误差，而 KL 散度作为编码器的正则化项。在训练过程中，$q_{\phi}({\bf
    z}|{\bf x})$ 将输出高斯分布的均值和方差，通过重参数化技巧从中采样 ${\bf z}$。通常，$q_{\phi}({\bf z}|{\bf x})$
    是由具有两个分支的多层感知机参数化的，一个分支产生均值，另一个分支产生方差。
- en: Similar to the case of pSDAE, various VAE variants have been proposed. For example,
    Importance weighted Autoencoders (IWAE) ([IWAE,](#bib.bib11) ) derived a tighter
    lower bound via importance weighting, ([LSTM-VAE-CNN,](#bib.bib129) ) combined
    LSTM, VAE, and dilated CNN for text modeling, ([VRNN,](#bib.bib17) ) proposed
    a recurrent version of VAE dubbed variational RNN (VRNN).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 与 pSDAE 的情况类似，已提出了各种 VAE 变体。例如，重要性加权自编码器（IWAE） ([IWAE,](#bib.bib11)) 通过重要性加权推导了更紧的下界，[LSTM-VAE-CNN,](#bib.bib129)
    结合了 LSTM、VAE 和扩张卷积神经网络进行文本建模，[VRNN,](#bib.bib17) 提出了一个称为变分 RNN（VRNN）的 VAE 递归版本。
- en: 4.3.4\. Natural-Parameter Networks
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4\. 自然参数网络
- en: Different from vanilla NN which usually takes deterministic input, NPN ([NPN,](#bib.bib119)
    ) is a probabilistic NN taking distributions as input. The input distributions
    go through layers of linear and nonlinear transformation to produce output distributions.
    In NPN, all hidden neurons and weights are also distributions expressed in closed
    form. Note that this is in contrast to VAE where only the middle layer output
    ${\bf z}$ is a distribution.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于通常接受确定性输入的普通神经网络，NPN ([NPN,](#bib.bib119)) 是一种将分布作为输入的概率神经网络。输入分布通过线性和非线性变换层生成输出分布。在
    NPN 中，所有隐藏神经元和权重也是以封闭形式表示的分布。请注意，这与 VAE 不同，后者只有中间层输出 ${\bf z}$ 是分布。
- en: 'As a simple example, in a vanilla linear NN $f_{w}(x)=wx$ takes a scalar $x$
    as input and computes the output based on a scalar parameter $w$; a corresponding
    Gaussian NPN would assume $w$ is drawn from a Gaussian distribution $\mathcal{N}(w_{m},w_{s})$
    and that $x$ is drawn from $\mathcal{N}(x_{m},x_{s})$ ($x_{s}$ is set to $0$ when
    the input is deterministic). With $\theta=(w_{m},w_{s})$ as a learnable parameter
    pair, NPN will then compute the mean and variance of the output Gaussian distribution
    $\mu_{\theta}(x_{m},x_{s})$ and $s_{\theta}(x_{m},x_{s})$ in closed form (bias
    terms are ignored for clarity) as:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个简单的例子，在一个普通线性神经网络中 $f_{w}(x)=wx$ 以标量 $x$ 作为输入，并根据标量参数 $w$ 计算输出；对应的高斯 NPN
    将假设 $w$ 从高斯分布 $\mathcal{N}(w_{m},w_{s})$ 中抽取，而 $x$ 从 $\mathcal{N}(x_{m},x_{s})$
    中抽取（当输入是确定性的时，$x_{s}$ 设为 $0$）。使用 $\theta=(w_{m},w_{s})$ 作为可学习的参数对，NPN 将计算输出高斯分布的均值和方差
    $\mu_{\theta}(x_{m},x_{s})$ 和 $s_{\theta}(x_{m},x_{s})$ 的封闭形式（为清晰起见，忽略了偏差项）如下：
- en: '| (7) |  | $\displaystyle\mu_{\theta}(x_{m},x_{s})$ | $\displaystyle=E[wx]=x_{m}w_{m},$
    |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\displaystyle\mu_{\theta}(x_{m},x_{s})$ | $\displaystyle=E[wx]=x_{m}w_{m},$
    |  |'
- en: '| (8) |  | $\displaystyle s_{\theta}(x_{m},x_{s})$ | $\displaystyle=D[wx]=x_{s}w_{s}+x_{s}w_{m}^{2}+x_{m}^{2}w_{s},$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\displaystyle s_{\theta}(x_{m},x_{s})$ | $\displaystyle=D[wx]=x_{s}w_{s}+x_{s}w_{m}^{2}+x_{m}^{2}w_{s},$
    |  |'
- en: Hence the output of this Gaussian NPN is a tuple $(\mu_{\theta}(x_{m},x_{s}),s_{\theta}(x_{m},x_{s}))$
    representing a Gaussian distribution instead of a single value. Input variance
    $x_{s}$ to NPN can be set to $0$ if not available. Note that since $s_{\theta}(x_{m},0)=x_{m}^{2}w_{s}$,
    $w_{m}$ and $w_{s}$ can still be learned even if $x_{s}=0$ for all data points.
    The derivation above is generalized to handle vectors and matrices in practice ([NPN,](#bib.bib119)
    ). Besides Gaussian distributions, NPN also support other exponential-family distributions
    such as Poisson distributions and gamma distributions ([NPN,](#bib.bib119) ).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个高斯 NPN 的输出是一个元组 $(\mu_{\theta}(x_{m},x_{s}),s_{\theta}(x_{m},x_{s}))$，表示一个高斯分布，而不是一个单一的值。如果没有可用的输入方差
    $x_{s}$，可以将其设置为 $0$。注意，由于 $s_{\theta}(x_{m},0)=x_{m}^{2}w_{s}$，即使所有数据点的 $x_{s}=0$，$w_{m}$
    和 $w_{s}$ 仍然可以被学习。上述推导被推广到实际中处理向量和矩阵 ([NPN,](#bib.bib119) )。除了高斯分布，NPN 还支持其他指数族分布，如泊松分布和伽玛分布 ([NPN,](#bib.bib119)
    )。
- en: Following NPN, a light-weight version ([LightNPN,](#bib.bib26) ) was proposed
    to speed up the training and inference process. Another variant, MaxNPN ([MaxNPN,](#bib.bib100)
    ), extended NPN to handle max-pooling and categorical layers. ConvNPN ([ConvNPN,](#bib.bib87)
    ) enables convolutional layers in NPN. In terms of model quantization and compression,
    BinaryNPN ([BinaryNPN,](#bib.bib107) ) was also proposed as NPN’s binary version
    to achieve better efficiency.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着 NPN，提出了一个轻量级版本 ([LightNPN,](#bib.bib26) ) 以加快训练和推理过程。另一种变体 MaxNPN ([MaxNPN,](#bib.bib100)
    ) 扩展了 NPN 以处理最大池化和分类层。ConvNPN ([ConvNPN,](#bib.bib87) ) 使 NPN 中的卷积层成为可能。在模型量化和压缩方面，还提出了
    BinaryNPN ([BinaryNPN,](#bib.bib107) ) 作为 NPN 的二进制版本，以实现更好的效率。
- en: 4.4\. Task-Specific Component
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 任务特定组件
- en: In this subsection, we introduce different forms of task-specific components.
    The purpose of a task-specific component is to incorporate probabilistic prior
    knowledge into the BDL model. Such knowledge can be naturally represented using
    PGM. Concretely, it can be a typical (or shallow) Bayesian network ([BayesNetBook,](#bib.bib54)
    ; [PRML,](#bib.bib5) ), a bidirectional inference network ([BIN,](#bib.bib117)
    ), or a stochastic process ([ross1996stochastic,](#bib.bib94) ).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们介绍了不同形式的任务特定组件。任务特定组件的目的是将概率先验知识纳入 BDL 模型。这种知识可以通过 PGM 自然地表示。具体来说，它可以是典型的（或浅层的）贝叶斯网络 ([BayesNetBook,](#bib.bib54)
    ; [PRML,](#bib.bib5) )、双向推理网络 ([BIN,](#bib.bib117) )，或随机过程 ([ross1996stochastic,](#bib.bib94)
    )。
- en: 4.4.1\. Bayesian Networks
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 贝叶斯网络
- en: 'Bayesian networks are the most common choice for a task-specific component.
    As mentioned in Section [3](#S3 "3\. Probabilistic Graphical Models ‣ A Survey
    on Bayesian Deep Learning"), Bayesian networks can naturally represent conditional
    dependencies and handle uncertainty. Besides LDA introduced above, a more straightforward
    example is probabilistic matrix factorization (PMF) ([PMF,](#bib.bib96) ), where
    one uses a Bayesian network to describe the conditional dependencies among users,
    items, and ratings. Specifically, PMF assumes the following generative process:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯网络是任务特定组件最常见的选择。如第 [3](#S3 "3\. Probabilistic Graphical Models ‣ A Survey
    on Bayesian Deep Learning") 节所述，贝叶斯网络可以自然地表示条件依赖关系并处理不确定性。除了上述介绍的 LDA，另一个更直接的例子是概率矩阵分解
    (PMF) ([PMF,](#bib.bib96) )，其中使用贝叶斯网络来描述用户、物品和评分之间的条件依赖关系。具体来说，PMF 假设以下生成过程：
- en: (1)
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'For each item $j$, draw a latent item vector: ${\bf v}_{i}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K}).$'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个物品 $j$，绘制一个潜在物品向量：${\bf v}_{i}\sim{\mathcal{N}}({\bf 0},\lambda_{v}^{-1}{\bf
    I}_{K})$。
- en: (2)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'For each user $i$, draw a latent user vector: ${\bf u}_{i}\sim{\mathcal{N}}({\bf
    0},\lambda_{u}^{-1}{\bf I}_{K}).$'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个用户 $i$，绘制一个潜在用户向量：${\bf u}_{i}\sim{\mathcal{N}}({\bf 0},\lambda_{u}^{-1}{\bf
    I}_{K})$。
- en: (3)
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'For each user-item pair $(i,j)$, draw a rating: ${\bf R}_{ij}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1}).$'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个用户-物品对 $(i,j)$，绘制一个评分：${\bf R}_{ij}\sim{\mathcal{N}}({\bf u}_{i}^{T}{\bf
    v}_{j},{\bf C}_{ij}^{-1})$。
- en: 'In the generative process above, ${\bf C}_{ij}^{-1}$ is the corresponding variance
    for the rating ${\bf R}_{ij}$. Using MAP estimates, learning PMF amounts to maximize
    the following log-likelihood of $p(\{{\bf u}_{i}\},\{{\bf v}_{j}\}|\{{\bf R}_{ij}\},\{{\bf
    C}_{ij}\},\lambda_{u},\lambda_{v})$:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述生成过程中，${\bf C}_{ij}^{-1}$ 是与评分 ${\bf R}_{ij}$ 相对应的方差。使用 MAP 估计，学习 PMF 就是最大化
    $p(\{{\bf u}_{i}\},\{{\bf v}_{j}\}|\{{\bf R}_{ij}\},\{{\bf C}_{ij}\},\lambda_{u},\lambda_{v})$
    的以下对数似然函数：
- en: '|  | $\displaystyle\mathscr{L}=-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf
    C}_{ij}}{2}({\bf R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2},$ |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathscr{L}=-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf
    C}_{ij}}{2}({\bf R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2},$ |  |'
- en: Note that one can also impose another layer of priors on the hyperparameters
    with a fully Bayesian treatment. For example, ([BPMF,](#bib.bib97) ) imposes priors
    on the precision matrix of latent factors and learn the Bayesian PMF with Gibbs
    sampling.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意可以对超参数施加另一层先验，通过完全的贝叶斯处理。例如，[BPMF](#bib.bib97) 对潜在因子的精度矩阵施加先验，并通过 Gibbs 采样学习贝叶斯
    PMF。
- en: In Section [5.1](#S5.SS1 "5.1\. Supervised Bayesian Deep Learning for Recommender
    Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep
    Learning"), we will show how PMF can be used as a task-specific component along
    with a perception component defined to significantly improve recommender systems’
    performance.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [5.1](#S5.SS1 "5.1\. 监督贝叶斯深度学习推荐系统 ‣ 5\. 具体的 BDL 模型和应用 ‣ 贝叶斯深度学习调查") 节中，我们将展示如何将
    PMF 作为任务特定组件与定义的感知组件一起使用，以显著提高推荐系统的性能。
- en: 4.4.2\. Bidirectional Inference Networks
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 双向推断网络
- en: Typical Bayesian networks assume ‘shallow’ conditional dependencies among random
    variables. In the generative process, one random variable (which can be either
    latent or observed) is usually drawn from a conditional distribution parameterized
    by the linear combination of its parent variables. For example, in PMF the rating
    ${\bf R}_{ij}$ is drawn from a Gaussian distribution mainly parameterized by the
    linear combination of ${\bf u}_{i}$ and ${\bf v}_{j}$, i.e., ${\bf R}_{ij}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1})$.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的贝叶斯网络假设随机变量之间存在“浅层”条件依赖。在生成过程中，随机变量（可以是潜在的或观察到的）通常从一个由其父变量的线性组合参数化的条件分布中抽取。例如，在
    PMF 中，评分 ${\bf R}_{ij}$ 从一个主要由 ${\bf u}_{i}$ 和 ${\bf v}_{j}$ 的线性组合参数化的高斯分布中抽取，即
    ${\bf R}_{ij}\sim{\mathcal{N}}({\bf u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1})$。
- en: '![Refer to caption](img/bceb1d41d2e5f6b07d3bb320a80b0414.png)![Refer to caption](img/66fcc7c07ec74be81f42e6a410b8e137.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bceb1d41d2e5f6b07d3bb320a80b0414.png)![参考标题](img/66fcc7c07ec74be81f42e6a410b8e137.png)'
- en: 'Figure 6\. Left: A simple example of BIN with each conditional distribution
    parameterized by a Bayesian neural networks (BNN) or simply a probabilistic neural
    network. Right: Another example BIN. Shaded and transparent nodes indicate observed
    and unobserved variables, respectively.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 左：一个简单的 BIN 示例，其中每个条件分布由贝叶斯神经网络（BNN）或仅由概率神经网络参数化。右：另一个 BIN 示例。阴影节点和透明节点分别表示观察到的和未观察到的变量。
- en: Such ‘shallow’ and linear structures can be replaced with nonlinear or even
    deep nonlinear structures to form a *deep Bayesian network*. As an example, bidirectional
    inference network (BIN) ([BIN,](#bib.bib117) ) is a class of deep Bayesian networks
    that enable deep nonlinear structures in each conditional distribution, while
    retaining the ability to incorporate prior knowledge as Bayesian networks.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这种“浅层”线性结构可以被非线性甚至深度非线性结构所替代，以形成*深度贝叶斯网络*。例如，双向推断网络（BIN）([BIN](#bib.bib117)）是一类深度贝叶斯网络，能够在每个条件分布中启用深度非线性结构，同时保留作为贝叶斯网络整合先验知识的能力。
- en: 'For example, Figure [6](#S4.F6 "Figure 6 ‣ 4.4.2\. Bidirectional Inference
    Networks ‣ 4.4\. Task-Specific Component ‣ 4\. Bayesian Deep Learning ‣ A Survey
    on Bayesian Deep Learning")(left) shows a BIN, where each conditional distribution
    is parameterized by a Bayesian neural network. Specifically, this example assumes
    the following factorization:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图 [6](#S4.F6 "图 6 ‣ 4.4.2\. 双向推断网络 ‣ 4.4\. 任务特定组件 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习调查")（左）展示了一个
    BIN，其中每个条件分布由贝叶斯神经网络参数化。具体而言，此示例假设以下因式分解：
- en: '|  | $\displaystyle p(v_{1},v_{2},v_{3}&#124;X)=p(v_{1}&#124;X)p(v_{2}&#124;X,v_{1})p(v_{3}&#124;X,v_{1},v_{2}).$
    |  |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p(v_{1},v_{2},v_{3}&#124;X)=p(v_{1}&#124;X)p(v_{2}&#124;X,v_{1})p(v_{3}&#124;X,v_{1},v_{2}).$
    |  |'
- en: A vanilla Bayesian network parameterizes each distribution with simple linear
    operations. For example, $p(v_{2}|X,v_{1})={\mathcal{N}}(v_{2}|Xw_{0}+v_{1}w_{1}+b,\sigma^{2})$).
    In contrast, BIN (as a deep Bayesian network) uses a BNN. For example, BIN has
    $p(v_{2}|X,v_{1})={\mathcal{N}}(v_{2}|\mu_{\theta}(X,v_{1}),s_{\theta}(X,v_{1}))$,
    where $\mu_{\theta}(X,v_{1})$ and $s_{\theta}(X,v_{1})$ are the output mean and
    variance of the BNN. The inference and learning of such a deep Bayesian network
    is done by performing BP across all BNNs (e.g., BNN 1, 2, and 3 in Figure [6](#S4.F6
    "Figure 6 ‣ 4.4.2\. Bidirectional Inference Networks ‣ 4.4\. Task-Specific Component
    ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning")(left)) ([BIN,](#bib.bib117)
    ).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 普通的贝叶斯网络通过简单的线性操作对每个分布进行参数化。例如，$p(v_{2}|X,v_{1})={\mathcal{N}}(v_{2}|Xw_{0}+v_{1}w_{1}+b,\sigma^{2})$）。相对而言，BIN（作为一种深度贝叶斯网络）使用了一个BNN。例如，BIN具有$p(v_{2}|X,v_{1})={\mathcal{N}}(v_{2}|\mu_{\theta}(X,v_{1}),s_{\theta}(X,v_{1}))$，其中$\mu_{\theta}(X,v_{1})$和$s_{\theta}(X,v_{1})$是BNN的输出均值和方差。这种深度贝叶斯网络的推断和学习通过在所有BNN（例如，图[6](#S4.F6
    "Figure 6 ‣ 4.4.2\. Bidirectional Inference Networks ‣ 4.4\. Task-Specific Component
    ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning")(left)中的BNN
    1、2和3）上执行BP来完成([BIN,](#bib.bib117) )。
- en: Compared to vanilla (shallow) Bayesian networks, deep Bayesian networks such
    as BIN make it possible to handle deep and nonlinear conditional dependencies
    effectively and efficiently. Besides, with BNN as building blocks, task-specific
    components based on deep Bayesian networks can better work with the perception
    component which is usually a BNN as well. Figure [6](#S4.F6 "Figure 6 ‣ 4.4.2\.
    Bidirectional Inference Networks ‣ 4.4\. Task-Specific Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")(right) shows a more complicated
    case with both observed (shaded nodes) and unobserved (transparent nodes) variables.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通（浅层）贝叶斯网络相比，像BIN这样的深度贝叶斯网络使得有效和高效地处理深层和非线性条件依赖关系成为可能。此外，通过BNN作为构建块，基于深度贝叶斯网络的任务特定组件可以更好地与感知组件配合，而感知组件通常也是BNN。图[6](#S4.F6
    "Figure 6 ‣ 4.4.2\. Bidirectional Inference Networks ‣ 4.4\. Task-Specific Component
    ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning")(right)展示了一个更复杂的情况，其中包括观察到的（阴影节点）和未观察到的（透明节点）变量。
- en: 4.4.3\. Stochastic Processes
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 随机过程
- en: Besides vanilla Bayesian networks and deep Bayesian networks, a task-specific
    component can also take the form of a stochastic process ([ross1996stochastic,](#bib.bib94)
    ). For example, a Wiener process can naturally describe a continuous-time Brownian
    motion model ${\bf x}_{t+u}|{\bf x}_{t}\sim{\mathcal{N}}({\bf x}_{t},\lambda u{\bf
    I}$), where ${\bf x}_{t+u}$ and ${\bf x}_{t}$ are the states at time $t$ and $t+u$,
    respectively. In the graphical model literature, such a process has been used
    to model the continuous-time topic evolution of articles over time ([cDTM,](#bib.bib113)
    ).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 除了普通贝叶斯网络和深度贝叶斯网络外，任务特定组件还可以采用随机过程的形式([ross1996stochastic,](#bib.bib94) )。例如，Wiener过程可以自然地描述一个连续时间布朗运动模型${\bf
    x}_{t+u}|{\bf x}_{t}\sim{\mathcal{N}}({\bf x}_{t},\lambda u{\bf I}$)，其中${\bf x}_{t+u}$和${\bf
    x}_{t}$分别是时间$t$和$t+u$的状态。在图形模型文献中，这种过程已经被用来建模文章的连续时间主题演变([cDTM,](#bib.bib113)
    )。
- en: Another example is to model phonemes’ boundary positions using a Poisson process
    in automatic speech recognition (ASR) ([RPPU,](#bib.bib51) ). Note that this is
    a fundamental problem in ASR since speech is no more than a sequence of phonemes.
    Specifically, a Poisson process defines the generative process $\Delta t_{i}=t_{i}-t_{i-1}\sim
    g(\lambda(t))$, with $\mathcal{T}=\{t_{1},t_{2},\dots,t_{N}\}$ as the set of boundary
    positions, and $g(\lambda(t))$ is a exponential distribution with the parameter
    $\lambda(t)$ (also known as the intensity). Such a stochastic process naturally
    models the occurrence of phoneme boundaries in continuous time. The parameter
    $\lambda(t)$ can be the output of a neural network taking raw speech signals as
    input ([RPPU,](#bib.bib51) ; [IFL,](#bib.bib99) ; [NN-SP,](#bib.bib83) ).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是在自动语音识别（ASR）中使用泊松过程来建模音素的边界位置([RPPU,](#bib.bib51) )。需要注意的是，这是ASR中的一个基本问题，因为语音不过是一系列音素的序列。具体来说，泊松过程定义了生成过程$\Delta
    t_{i}=t_{i}-t_{i-1}\sim g(\lambda(t))$，其中$\mathcal{T}=\{t_{1},t_{2},\dots,t_{N}\}$是边界位置的集合，$g(\lambda(t))$是参数为$\lambda(t)$的指数分布（也称为强度）。这种随机过程自然地建模了音素边界在连续时间中的出现。参数$\lambda(t)$可以是一个神经网络的输出，该网络以原始语音信号作为输入([RPPU,](#bib.bib51)
    ; [IFL,](#bib.bib99) ; [NN-SP,](#bib.bib83) )。
- en: Interestingly, stochastic processes can be seen as a type of dynamic Bayesian
    networks. To see this, we can rewrite the Poisson process above in an equivalent
    form, where given $t_{i-1}$, the probability that $t_{i}$ has not occurred at
    time $t$, $P(t_{i}>t)=\exp(\int_{t_{i-1}}^{t}-\lambda(t)dt)$. Obviously both the
    Wiener process and the Poisson process are Markovian and can be represented with
    a dynamic Bayesian network ([murphybook,](#bib.bib78) ).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，随机过程可以被视为一种动态贝叶斯网络。为此，我们可以将上面的泊松过程重写为等效形式，其中给定$t_{i-1}$时，$t_{i}$在时间$t$没有发生的概率为$P(t_{i}>t)=\exp(\int_{t_{i-1}}^{t}-\lambda(t)dt)$。显然，维纳过程和泊松过程都是马尔可夫的，并且可以用动态贝叶斯网络表示([murphybook,](#bib.bib78)
    )。
- en: For clarity, we focus on using vanilla Bayesian networks as task-specific components
    in Section [5](#S5 "5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian
    Deep Learning"); they can be naturally replaced with other types of task-specific
    components to represent different prior knowledge if necessary.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们在第[5](#S5 "5\. 具体BDL模型与应用 ‣ 贝叶斯深度学习综述")节中专注于使用基础贝叶斯网络作为特定任务的组件；如果必要，它们可以自然地被其他类型的特定任务组件替换，以表示不同的先验知识。
- en: 5\. Concrete BDL Models and Applications
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 具体BDL模型与应用
- en: In this section, we discuss how the BDL framework can facilitate supervised
    learning, unsupervised learning, and representation learning in general. Concretely
    we use examples in domains such as recommender systems, topic models, control,
    etc.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了BDL框架如何促进监督学习、无监督学习和一般表示学习。具体来说，我们使用了推荐系统、主题模型、控制等领域的示例。
- en: 5.1\. Supervised Bayesian Deep Learning for Recommender Systems
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 推荐系统的监督贝叶斯深度学习
- en: Despite the successful applications of deep learning on natural language processing
    and computer vision, very few attempts have been made to develop deep learning
    models for collaborative filtering (CF) before the emergence of BDL. ([DBLP:conf/icml/SalakhutdinovMH07,](#bib.bib98)
    ) uses restricted Boltzmann machines instead of the conventional matrix factorization
    formulation to perform CF and ([DBLP:conf/icml/GeorgievN13,](#bib.bib28) ) extends
    this work by incorporating user-user and item-item correlations. Although these
    methods involve both deep learning and CF, they actually belong to CF-based methods
    because they ignore users’ or items’ content information, which is crucial for
    accurate recommendation. ([DBLP:conf/icassp/SainathKSAR13,](#bib.bib95) ) uses
    low-rank matrix factorization in the last weight layer of a deep network to significantly
    reduce the number of model parameters and speed up training, but it is for classification
    instead of recommendation tasks. On music recommendation, ([DBLP:conf/nips/OordDS13,](#bib.bib84)
    ; [DBLP:conf/mm/WangW14,](#bib.bib123) ) directly use conventional CNN or deep
    belief networks (DBN) to assist representation learning for content information,
    but the deep learning components of their models are deterministic without modeling
    the noise and hence they are less robust. The models achieve performance boost
    mainly by loosely coupled methods without exploiting the interaction between content
    information and ratings. Besides, the CNN is linked directly to the rating matrix,
    which means the models will perform poorly due to serious overfitting when the
    ratings are sparse.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在自然语言处理和计算机视觉方面取得了成功应用，但在BDL出现之前，很少有人尝试开发用于协同过滤（CF）的深度学习模型。([DBLP:conf/icml/SalakhutdinovMH07,](#bib.bib98))
    使用限制玻尔兹曼机代替传统的矩阵分解方法来执行CF，([DBLP:conf/icml/GeorgievN13,](#bib.bib28)) 通过引入用户-用户和物品-物品相关性扩展了这项工作。虽然这些方法涉及深度学习和CF，但它们实际上属于基于CF的方法，因为它们忽略了用户或物品的内容信息，这对于准确推荐至关重要。([DBLP:conf/icassp/SainathKSAR13,](#bib.bib95))
    在深度网络的最后权重层使用低秩矩阵分解，显著减少模型参数的数量并加速训练，但它是用于分类而非推荐任务。在音乐推荐方面，([DBLP:conf/nips/OordDS13,](#bib.bib84)
    ; [DBLP:conf/mm/WangW14,](#bib.bib123)) 直接使用传统的CNN或深度信念网络（DBN）来辅助内容信息的表示学习，但它们模型的深度学习组件是确定性的，没有对噪声建模，因此鲁棒性较差。这些模型主要通过松散耦合的方法实现性能提升，而没有利用内容信息和评分之间的互动。此外，CNN直接链接到评分矩阵，这意味着当评分稀疏时，模型由于严重过拟合会表现不佳。
- en: 5.1.1\. Collaborative Deep Learning
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 协作深度学习
- en: To address the challenges above, a hierarchical Bayesian model called collaborative
    deep learning (CDL) as a novel tightly coupled method for recommender systems
    is introduced in ([CDL,](#bib.bib121) ). Based on a Bayesian formulation of SDAE,
    CDL tightly couples deep representation learning for the content information and
    collaborative filtering for the rating (feedback) matrix, allowing two-way interaction
    between the two. From BDL’s perspective, a probabilistic SDAE as the perception
    component is tightly coupled with a probabilistic graphical model as the task-specific
    component. Experiments show that CDL significantly improves upon the state of
    the art.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述挑战，介绍了一种称为协同深度学习（CDL）的分层贝叶斯模型，它是一种用于推荐系统的新型紧密耦合方法（[CDL,](#bib.bib121)）。基于SDAE的贝叶斯公式，CDL将内容信息的深度表示学习与评级（反馈）矩阵的协同过滤紧密耦合，实现两者之间的双向交互。从BDL的角度来看，作为感知组件的概率SDAE与作为任务特定组件的概率图模型紧密耦合。实验表明，CDL在技术水平上显著提升了现有的最佳水平。
- en: In the following text, we will start with the introduction of the notation used
    during our presentation of CDL. After that we will review the design and learning
    of CDL.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下文本中，我们将从介绍在展示CDL过程中使用的符号开始。之后，我们将回顾CDL的设计和学习。
- en: 'Notation and Problem Formulation: Similar to the work in ([CTR,](#bib.bib112)
    ), the recommendation task considered in CDL takes implicit feedback ([DBLP:conf/icdm/HuKV08,](#bib.bib50)
    ) as the training and test data. The entire collection of $J$ items (articles
    or movies) is represented by a $J$-by-$B$ matrix ${\bf X}_{c}$, where row $j$
    is the bag-of-words vector ${\bf X}_{c,j*}$ for item $j$ based on a vocabulary
    of size $B$. With $I$ users, we define an $I$-by-$J$ binary rating matrix ${\bf
    R}=[{\bf R}_{ij}]_{I\times J}$. For example, in the dataset *citeulike-a* ([CTR,](#bib.bib112)
    ; [CTRSR,](#bib.bib115) ; [CDL,](#bib.bib121) ) ${\bf R}_{ij}=1$ if user $i$ has
    article $j$ in his or her personal library and ${\bf R}_{ij}=0$ otherwise. Given
    part of the ratings in ${\bf R}$ and the content information ${\bf X}_{c}$, the
    problem is to predict the other ratings in ${\bf R}$. Note that although CDL in
    its current from focuses on movie recommendation (where plots of movies are considered
    as content information) and article recommendation like ([CTR,](#bib.bib112) )
    in this section, it is general enough to handle other recommendation tasks (e.g.,
    tag recommendation).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 符号和问题公式化：类似于（[CTR,](#bib.bib112)）中的工作，CDL中考虑的推荐任务使用隐式反馈（[DBLP:conf/icdm/HuKV08,](#bib.bib50)）作为训练和测试数据。整个$J$项（文章或电影）集合由一个$J$-by-$B$矩阵${\bf
    X}_{c}$表示，其中第$j$行是基于大小为$B$的词汇表的物品$j$的词袋向量${\bf X}_{c,j*}$。对于$I$个用户，我们定义一个$I$-by-$J$的二进制评分矩阵${\bf
    R}=[{\bf R}_{ij}]_{I\times J}$。例如，在数据集*citeulike-a*（[CTR,](#bib.bib112)；[CTRSR,](#bib.bib115)；[CDL,](#bib.bib121)）中，如果用户$i$在其个人库中拥有文章$j$，则${\bf
    R}_{ij}=1$，否则${\bf R}_{ij}=0$。给定${\bf R}$中的部分评分和内容信息${\bf X}_{c}$，问题是预测${\bf R}$中的其他评分。请注意，尽管当前版本的CDL集中于电影推荐（其中电影情节被视为内容信息）和文章推荐（如[CTR,](#bib.bib112)）本节中，但它足够通用，可以处理其他推荐任务（例如，标签推荐）。
- en: The matrix ${\bf X}_{c}$ plays the role of clean input to the SDAE while the
    noise-corrupted matrix, also a $J$-by-$B$ matrix, is denoted by ${\bf X}_{0}$.
    The output of layer $l$ of the SDAE is denoted by ${\bf X}_{l}$ which is a $J$-by-$K_{l}$
    matrix. Similar to ${\bf X}_{c}$, row $j$ of ${\bf X}_{l}$ is denoted by ${\bf
    X}_{l,j*}$. ${\bf W}_{l}$ and ${\bf b}_{l}$ are the weight matrix and bias vector,
    respectively, of layer $l$, ${\bf W}_{l,*n}$ denotes column $n$ of ${\bf W}_{l}$,
    and $L$ is the number of layers. For convenience, we use ${\bf W}^{+}$ to denote
    the collection of all layers of weight matrices and biases. Note that an $L/2$-layer
    SDAE corresponds to an $L$-layer network.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵${\bf X}_{c}$作为SDAE的干净输入，而噪声污染矩阵，也就是一个$J$-by-$B$的矩阵，用${\bf X}_{0}$表示。SDAE的第$l$层的输出用${\bf
    X}_{l}$表示，它是一个$J$-by-$K_{l}$的矩阵。类似于${\bf X}_{c}$，${\bf X}_{l}$的第$j$行用${\bf X}_{l,j*}$表示。${\bf
    W}_{l}$和${\bf b}_{l}$分别是第$l$层的权重矩阵和偏置向量，${\bf W}_{l,*n}$表示${\bf W}_{l}$的第$n$列，而$L$是层的数量。为了方便起见，我们用${\bf
    W}^{+}$表示所有层的权重矩阵和偏置的集合。请注意，一个$L/2$层的SDAE对应于一个$L$层的网络。
- en: '![Refer to caption](img/4827b82264d29eb9c941f21c9081035d.png)![Refer to caption](img/aae5d09904a009fc726ad18ed40e3f3e.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4827b82264d29eb9c941f21c9081035d.png)![参考说明](img/aae5d09904a009fc726ad18ed40e3f3e.png)'
- en: Figure 7\. On the left is the graphical model of CDL. The part inside the dashed
    rectangle represents an SDAE. An example SDAE with $L=2$ is shown. On the right
    is the graphical model of the degenerated CDL. The part inside the dashed rectangle
    represents the encoder of an SDAE. An example SDAE with $L=2$ is shown on its
    right. Note that although $L$ is still $2$, the decoder of the SDAE vanishes.
    To prevent clutter, we omit all variables ${\bf x}_{l}$ except ${\bf x}_{0}$ and
    ${\bf x}_{L/2}$ in the graphical models.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 左侧是 CDL 的图示模型。虚线矩形内部的部分表示 SDAE。显示了一个 $L=2$ 的 SDAE 示例。右侧是退化 CDL 的图示模型。虚线矩形内部的部分表示
    SDAE 的编码器。右侧显示了一个 $L=2$ 的 SDAE 示例。注意，尽管 $L$ 仍为 $2$，但 SDAE 的解码器消失了。为了避免杂乱，我们在图示模型中省略了所有变量
    ${\bf x}_{l}$，只保留了 ${\bf x}_{0}$ 和 ${\bf x}_{L/2}$。
- en: 'Collaborative Deep Learning: Using the probabilistic SDAE in Section [4.3.2](#S4.SS3.SSS2
    "4.3.2\. Probabilistic Generalized SDAE ‣ 4.3\. Perception Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning") as a component, the generative
    process of CDL is defined as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 协作深度学习：使用第 [4.3.2](#S4.SS3.SSS2 "4.3.2\. Probabilistic Generalized SDAE ‣ 4.3\.
    Perception Component ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning") 节中的概率性 SDAE 作为组件，CDL 的生成过程定义如下：
- en: (1)
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: For each layer $l$ of the SDAE network,
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 SDAE 网络的每一层 $l$，
- en: (a)
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: For each column $n$ of the weight matrix ${\bf W}_{l}$, draw ${\bf W}_{l,*n}\sim{\mathcal{N}}({\bf
    0},\lambda_{w}^{-1}{\bf I}_{K_{l}})$.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于权重矩阵 ${\bf W}_{l}$ 的每一列 $n$，绘制 ${\bf W}_{l,*n}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$。
- en: (b)
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Draw the bias vector ${\bf b}_{l}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制偏置向量 ${\bf b}_{l}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf I}_{K_{l}})$。
- en: (c)
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: For each row $j$ of ${\bf X}_{l}$, draw ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 ${\bf X}_{l}$ 的每一行 $j$，绘制 ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf X}_{l-1,j*}{\bf
    W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$
- en: (2)
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: For each item $j$,
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个项目 $j$，
- en: (a)
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: Draw a clean input ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf
    I}_{J}$).
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制干净输入 ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf
    I}_{J}$)。
- en: (b)
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Draw the latent item offset vector $\mbox{\boldmath$\epsilon$\unboldmath}_{j}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K})$ and then set the latent item vector: ${\bf v}_{j}=\mbox{\boldmath$\epsilon$\unboldmath}_{j}+{\bf
    X}_{\frac{L}{2},j*}^{T}.$'
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制潜在项目偏移向量 $\mbox{\boldmath$\epsilon$\unboldmath}_{j}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K})$，然后设定潜在项目向量：${\bf v}_{j}=\mbox{\boldmath$\epsilon$\unboldmath}_{j}+{\bf
    X}_{\frac{L}{2},j*}^{T}.$
- en: (3)
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Draw a latent user vector for each user $i$: ${\bf u}_{i}\sim{\mathcal{N}}({\bf
    0},\lambda_{u}^{-1}{\bf I}_{K}).$'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个用户 $i$ 绘制一个潜在用户向量：${\bf u}_{i}\sim{\mathcal{N}}({\bf 0},\lambda_{u}^{-1}{\bf
    I}_{K})$。
- en: (4)
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Draw a rating ${\bf R}_{ij}$ for each user-item pair $(i,j)$: <math id="S5.I1.i4.p1.3.m3.1"
    class="ltx_Math" alttext="{\bf R}_{ij}\sim{\mathcal{N}}({\bf u}_{i}^{T}{\bf v}_{j},{\bf
    C}_{ij}^{-1}).\\'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个用户-项目对 $(i,j)$ 绘制一个评分 ${\bf R}_{ij}$：<math id="S5.I1.i4.p1.3.m3.1" class="ltx_Math"
    alttext="{\bf R}_{ij}\sim{\mathcal{N}}({\bf u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1}).\\
- en: '" display="inline"><semantics id="S5.I1.i4.p1.3.m3.1a"><mrow id="S5.I1.i4.p1.3.m3.1.1.1"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.cmml"><mrow id="S5.I1.i4.p1.3.m3.1.1.1.1" xref="S5.I1.i4.p1.3.m3.1.1.1.1.cmml"><msub
    id="S5.I1.i4.p1.3.m3.1.1.1.1.4" xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.cmml"><mi id="S5.I1.i4.p1.3.m3.1.1.1.1.4.2"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.2.cmml">𝐑</mi><mrow id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.cmml"><mi id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.2"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.1" xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.1.cmml">​</mo><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.3.cmml">j</mi></mrow></msub><mo
    id="S5.I1.i4.p1.3.m3.1.1.1.1.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.3.cmml">∼</mo><mrow
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.4" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.4.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.3.cmml">​</mo><mrow
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.3.cmml"><mo
    stretchy="false" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.3.cmml">(</mo><mrow
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.cmml"><msubsup
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.cmml"><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.2.cmml">𝐮</mi><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.3.cmml">T</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.1" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.1.cmml">​</mo><msub
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.cmml"><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.2.cmml">𝐯</mi><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></msub></mrow><mo
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.4" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.3.cmml">,</mo><msubsup
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.cmml"><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.2.cmml">𝐂</mi><mrow
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.cmml"><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.1" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.3.cmml">j</mi></mrow><mrow
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.cmml"><mo
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3a" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.cmml">−</mo><mn
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.2.cmml">1</mn></mrow></msubsup><mo
    stretchy="false" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.5" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo
    lspace="0em" id="S5.I1.i4.p1.3.m3.1.1.1.2" xref="S5.I1.i4.p1.3.m3.1.1.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S5.I1.i4.p1.3.m3.1b"><apply id="S5.I1.i4.p1.3.m3.1.1.1.1.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1"><csymbol cd="latexml" id="S5.I1.i4.p1.3.m3.1.1.1.1.3.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.3">similar-to</csymbol><apply id="S5.I1.i4.p1.3.m3.1.1.1.1.4.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4"><csymbol cd="ambiguous" id="S5.I1.i4.p1.3.m3.1.1.1.1.4.1.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4">subscript</csymbol><ci id="S5.I1.i4.p1.3.m3.1.1.1.1.4.2.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.2">𝐑</ci><apply id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3"><ci id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.2.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.2">𝑖</ci><ci id="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.3.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.4.3.3">𝑗</ci></apply></apply><apply id="S5.I1.i4.p1.3.m3.1.1.1.1.2.cmml"
    xref="S5.I1.i4.p1.3.m3.1.1.1.1.2"><ci id="S5.I1.i4.p1.3.m3.1.1.1.1.2.4.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.4">𝒩</ci><interval
    closure="open" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2"><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1"><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2"><csymbol
    cd="ambiguous" id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2"><csymbol
    cd="ambiguous" id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.2">𝐮</ci><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.2.3">𝑖</ci></apply><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.2.3">𝑇</ci></apply><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3"><csymbol
    cd="ambiguous" id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.2">𝐯</ci><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2">superscript</csymbol><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.1.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2">subscript</csymbol><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.2">𝐂</ci><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3"><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.2">𝑖</ci><ci
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.2.3.3">𝑗</ci></apply></apply><apply
    id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3"><cn
    type="integer" id="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.2.cmml" xref="S5.I1.i4.p1.3.m3.1.1.1.1.2.2.2.2.3.2">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S5.I1.i4.p1.3.m3.1c">{\bf R}_{ij}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1}).\\</annotation></semantics></math>'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**R**_{ij} \sim **𝒩**(**u**_{i}^{T}**v**_{j}, **C**_{ij}^{-1}).'
- en: Here $\lambda_{w}$, $\lambda_{n}$, $\lambda_{u}$, $\lambda_{s}$, and $\lambda_{v}$
    are hyperparameters and ${\bf C}_{ij}$ is a confidence parameter similar to that
    for CTR ([CTR,](#bib.bib112) ) (${\bf C}_{ij}=a$ if ${\bf R}_{ij}=1$ and ${\bf
    C}_{ij}=b$ otherwise). Note that the middle layer ${\bf X}_{L/2}$ serves as a
    bridge between the ratings and content information. This middle layer, along with
    the latent offset $\mbox{\boldmath$\epsilon$\unboldmath}_{j}$, is the key that
    enables CDL to simultaneously learn an effective feature representation and capture
    the similarity and (implicit) relationship among items (and users). Similar to
    the generalized SDAE, we can also take $\lambda_{s}$ to infinity for computational
    efficiency.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$\lambda_{w}$、$\lambda_{n}$、$\lambda_{u}$、$\lambda_{s}$和$\lambda_{v}$是超参数，而${\bf
    C}_{ij}$是类似于CTR的置信度参数（[CTR](#bib.bib112)）（当${\bf R}_{ij}=1$时${\bf C}_{ij}=a$，否则${\bf
    C}_{ij}=b$）。注意，中间层${\bf X}_{L/2}$充当评分和内容信息之间的桥梁。这个中间层以及潜在偏差$\mbox{\boldmath$\epsilon$\unboldmath}_{j}$是使CDL能够同时学习有效特征表示并捕捉项目（和用户）之间的相似性和（隐含）关系的关键。类似于广义SDAE，我们也可以将$\lambda_{s}$取为无穷大以提高计算效率。
- en: The graphical model of CDL when $\lambda_{s}$ approaches positive infinity is
    shown in Figure [7](#S5.F7 "Figure 7 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\.
    Supervised Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models
    and Applications ‣ A Survey on Bayesian Deep Learning"), where, for notational
    simplicity, we use ${\bf x}_{0}$, ${\bf x}_{L/2}$, and ${\bf x}_{L}$ in place
    of ${\bf X}_{0,j*}^{T}$, ${\bf X}_{\frac{L}{2},j*}^{T}$, and ${\bf X}_{L,j*}^{T}$,
    respectively.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 当$\lambda_{s}$趋向于正无穷时，CDL的图形模型如图[7](#S5.F7 "Figure 7 ‣ 5.1.1\. Collaborative
    Deep Learning ‣ 5.1\. Supervised Bayesian Deep Learning for Recommender Systems
    ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning")所示，其中为简化符号，我们使用${\bf
    x}_{0}$、${\bf x}_{L/2}$和${\bf x}_{L}$代替${\bf X}_{0,j*}^{T}$、${\bf X}_{\frac{L}{2},j*}^{T}$和${\bf
    X}_{L,j*}^{T}$。
- en: Note that according the definition in Section [4.2](#S4.SS2 "4.2\. General Framework
    ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning"), here the
    perception variables $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf W}_{l}\},\{{\bf
    b}_{l}\},\{{\bf X}_{l}\},{\bf X}_{c}\}$, the hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    V}\}$, and the task variables $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf
    U},{\bf R}\}$.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，根据第[4.2](#S4.SS2 "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣
    A Survey on Bayesian Deep Learning")节的定义，这里感知变量为$\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf
    W}_{l}\},\{{\bf b}_{l}\},\{{\bf X}_{l}\},{\bf X}_{c}\}$，铰链变量为$\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    V}\}$，任务变量为$\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf U},{\bf R}\}$。
- en: 'Learning: Based on the CDL model above, all parameters could be treated as
    random variables so that fully Bayesian methods such as Markov chain Monte Carlo
    (MCMC) or variational inference ([DBLP:journals/ml/JordanGJS99,](#bib.bib55) )
    may be applied. However, such treatment typically incurs high computational cost.
    Therefore CDL uses an EM-style algorithm to obtain the MAP estimates, as in ([CTR,](#bib.bib112)
    ).'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 学习：基于上述CDL模型，所有参数都可以视为随机变量，因此可以应用完全贝叶斯方法，如马尔可夫链蒙特卡洛（MCMC）或变分推断（[DBLP:journals/ml/JordanGJS99](#bib.bib55)）。然而，这种处理通常会产生较高的计算成本。因此，CDL使用类似EM的算法来获得MAP估计，如（[CTR](#bib.bib112)）。
- en: 'Concretely, note that maximizing the posterior probability is equivalent to
    maximizing the joint log-likelihood of ${\bf U}$, ${\bf V}$, $\{{\bf X}_{l}\}$,
    ${\bf X}_{c}$, $\{{\bf W}_{l}\}$, $\{{\bf b}_{l}\}$, and ${\bf R}$ given $\lambda_{u}$,
    $\lambda_{v}$, $\lambda_{w}$, $\lambda_{s}$, and $\lambda_{n}$:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，最大化后验概率等同于最大化给定$\lambda_{u}$、$\lambda_{v}$、$\lambda_{w}$、$\lambda_{s}$和$\lambda_{n}$的${\bf
    U}$、${\bf V}$、$\{{\bf X}_{l}\}$、${\bf X}_{c}$、$\{{\bf W}_{l}\}$、$\{{\bf b}_{l}\}$和${\bf
    R}$的联合对数似然。
- en: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf W}_{l}\&#124;_{F}^{2}+\&#124;{\bf
    b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}-{\bf
    X}_{\frac{L}{2},j*}^{T}\&#124;_{2}^{2}-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;{\bf
    X}_{L,j*}-{\bf X}_{c,j*}\&#124;_{2}^{2}$ |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf W}_{l}\&#124;_{F}^{2}+\&#124;{\bf
    b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}-{\bf
    X}_{\frac{L}{2},j*}^{T}\&#124;_{2}^{2}-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;{\bf
    X}_{L,j*}-{\bf X}_{c,j*}\&#124;_{2}^{2}$ |  |'
- en: '|  |  | $\displaystyle-\frac{\lambda_{s}}{2}\sum\limits_{l}\sum\limits_{j}\&#124;\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})-{\bf X}_{l,j*}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf
    C}_{ij}}{2}({\bf R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2}.$ |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{\lambda_{s}}{2}\sum\limits_{l}\sum\limits_{j}\&#124;\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})-{\bf X}_{l,j*}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf
    C}_{ij}}{2}({\bf R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2}.$ |  |'
- en: 'If $\lambda_{s}$ goes to infinity, the likelihood becomes:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $\lambda_{s}$ 变为无穷大，则似然函数变为：
- en: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf W}_{l}\&#124;_{F}^{2}+\&#124;{\bf
    b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}-f_{e}({\bf
    X}_{0,j*},{\bf W}^{+})^{T}\&#124;_{2}^{2}$ |  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf W}_{l}\&#124;_{F}^{2}+\&#124;{\bf
    b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}-f_{e}({\bf
    X}_{0,j*},{\bf W}^{+})^{T}\&#124;_{2}^{2}$ |  |'
- en: '| (9) |  |  | $\displaystyle-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf
    C}_{ij}}{2}({\bf R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2},$ |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  |  | $\displaystyle-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf
    C}_{ij}}{2}({\bf R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2},$ |  |'
- en: where the encoder function $f_{e}(\cdot,{\bf W}^{+})$ takes the corrupted content
    vector ${\bf X}_{0,j*}$ of item $j$ as input and computes its encoding, and the
    function $f_{r}(\cdot,{\bf W}^{+})$ also takes ${\bf X}_{0,j*}$ as input, computes
    the encoding and then reconstructs item $j$’s content vector. For example, if
    the number of layers $L=6$, $f_{e}({\bf X}_{0,j*},{\bf W}^{+})$ is the output
    of the third layer while $f_{r}({\bf X}_{0,j*},{\bf W}^{+})$ is the output of
    the sixth layer.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其中编码函数 $f_{e}(\cdot,{\bf W}^{+})$ 以物品 $j$ 的损坏内容向量 ${\bf X}_{0,j*}$ 为输入并计算其编码，而函数
    $f_{r}(\cdot,{\bf W}^{+})$ 也以 ${\bf X}_{0,j*}$ 为输入，计算编码然后重构物品 $j$ 的内容向量。例如，如果层数
    $L=6$，则 $f_{e}({\bf X}_{0,j*},{\bf W}^{+})$ 是第三层的输出，而 $f_{r}({\bf X}_{0,j*},{\bf
    W}^{+})$ 是第六层的输出。
- en: '![Refer to caption](img/290a1884753821341acd8afeaea26d4d.png)![Refer to caption](img/2be2047032aae63cc3ed089eb8640a3f.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/290a1884753821341acd8afeaea26d4d.png)![参见说明](img/2be2047032aae63cc3ed089eb8640a3f.png)'
- en: 'Figure 8\. Left: NN representation for degenerated CDL. Right: Sampling as
    generalized BP in Bayesian CDL.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 左：退化的 CDL 的 NN 表示。右：在贝叶斯 CDL 中作为广义 BP 的采样。
- en: From the perspective of optimization, the third term in the objective function,
    i.e., Equation ([5.1.1](#S5.Ex20 "5.1.1\. Collaborative Deep Learning ‣ 5.1\.
    Supervised Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models
    and Applications ‣ A Survey on Bayesian Deep Learning")), above is equivalent
    to a multi-layer perceptron using the latent item vectors ${\bf v}_{j}$ as the
    target while the fourth term is equivalent to an SDAE minimizing the reconstruction
    error. Seeing from the view of neural networks (NN), when $\lambda_{s}$ approaches
    positive infinity, training of the probabilistic graphical model of CDL in Figure
    [7](#S5.F7 "Figure 7 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised
    Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning")(left) would degenerate to simultaneously
    training two neural networks overlaid together with a common input layer (the
    corrupted input) but different output layers, as shown in Figure [8](#S5.F8 "Figure
    8 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised Bayesian Deep Learning
    for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey
    on Bayesian Deep Learning")(left). Note that the second network is much more complex
    than typical neural networks due to the involvement of the rating matrix.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 从优化的角度来看，上述目标函数中的第三项，即方程 ([5.1.1](#S5.Ex20 "5.1.1\. Collaborative Deep Learning
    ‣ 5.1\. Supervised Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete
    BDL Models and Applications ‣ A Survey on Bayesian Deep Learning")) 等效于一个多层感知机，它使用潜在的物品向量
    ${\bf v}_{j}$ 作为目标，而第四项等效于一个 SDAE，它最小化重构误差。从神经网络（NN）的角度来看，当 $\lambda_{s}$ 接近正无穷大时，图
    [7](#S5.F7 "Figure 7 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised
    Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning")(左) 中 CDL 的概率图模型的训练将退化为同时训练两个叠加的神经网络，它们有一个公共的输入层（损坏的输入）但不同的输出层，如图
    [8](#S5.F8 "Figure 8 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised
    Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning")(左) 所示。注意，由于涉及评分矩阵，第二个网络比典型的神经网络复杂得多。
- en: When the ratio $\lambda_{n}/\lambda_{v}$ approaches positive infinity, it will
    degenerate to a two-step model in which the latent representation learned using
    SDAE is put directly into the CTR. Another extreme happens when $\lambda_{n}/\lambda_{v}$
    goes to zero where the decoder of the SDAE essentially vanishes. Figure [7](#S5.F7
    "Figure 7 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised Bayesian Deep
    Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣
    A Survey on Bayesian Deep Learning")(right) shows the graphical model of the degenerated
    CDL when $\lambda_{n}/\lambda_{v}$ goes to zero. As demonstrated in experiments,
    the predictive performance will suffer greatly for both extreme cases ([CDL,](#bib.bib121)
    ).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 当比率 $\lambda_{n}/\lambda_{v}$ 接近正无穷时，它会退化为一个两步模型，其中使用 SDAE 学习的潜在表示直接输入到 CTR
    中。另一种极端情况发生在 $\lambda_{n}/\lambda_{v}$ 接近零时，此时 SDAE 的解码器实际上消失。图 [7](#S5.F7 "Figure
    7 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised Bayesian Deep Learning
    for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey
    on Bayesian Deep Learning")(右) 显示了当 $\lambda_{n}/\lambda_{v}$ 接近零时退化的 CDL 的图示模型。如实验所示，两种极端情况的预测性能都会受到严重影响
    ([CDL,](#bib.bib121) )。
- en: 'For ${\bf u}_{i}$ and ${\bf v}_{j}$, block coordinate descent similar to ([CTR,](#bib.bib112)
    ; [DBLP:conf/icdm/HuKV08,](#bib.bib50) ) is used. Given the current ${\bf W}^{+}$,
    we compute the gradients of $\mathscr{L}$ with respect to ${\bf u}_{i}$ and ${\bf
    v}_{j}$ and then set them to zero, leading to the following update rules:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf u}_{i}$ 和 ${\bf v}_{j}$，使用类似于 ([CTR,](#bib.bib112) ; [DBLP:conf/icdm/HuKV08,](#bib.bib50)
    ) 的块坐标下降方法。在给定当前的 ${\bf W}^{+}$ 时，我们计算 $\mathscr{L}$ 关于 ${\bf u}_{i}$ 和 ${\bf
    v}_{j}$ 的梯度，然后将其设为零，从而得到以下更新规则：
- en: '|  | $\displaystyle{\bf u}_{i}$ | $\displaystyle\leftarrow({\bf V}{\bf C}_{i}{\bf
    V}^{T}+\lambda_{u}{\bf I}_{K})^{-1}{\bf V}{\bf C}_{i}{\bf R}_{i},\;\;\;\;{\bf
    v}_{j}\leftarrow({\bf U}{\bf C}_{i}{\bf U}^{T}+\lambda_{v}{\bf I}_{K})^{-1}({\bf
    U}{\bf C}_{j}{\bf R}_{j}+\lambda_{v}f_{e}({\bf X}_{0,j*},{\bf W}^{+})^{T}),$ |  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf u}_{i}$ | $\displaystyle\leftarrow({\bf V}{\bf C}_{i}{\bf
    V}^{T}+\lambda_{u}{\bf I}_{K})^{-1}{\bf V}{\bf C}_{i}{\bf R}_{i},\;\;\;\;{\bf
    v}_{j}\leftarrow({\bf U}{\bf C}_{i}{\bf U}^{T}+\lambda_{v}{\bf I}_{K})^{-1}({\bf
    U}{\bf C}_{j}{\bf R}_{j}+\lambda_{v}f_{e}({\bf X}_{0,j*},{\bf W}^{+})^{T}),$ |  |'
- en: where ${\bf U}=({\bf u}_{i})^{I}_{i=1}$, ${\bf V}=({\bf v}_{j})^{J}_{j=1}$,
    ${\bf C}_{i}=\mbox{diag}({\bf C}_{i1},\ldots,{\bf C}_{iJ})$ is a diagonal matrix,
    ${\bf R}_{i}=({\bf R}_{i1},\ldots,{\bf R}_{iJ})^{T}$ is a column vector containing
    all the ratings of user $i$, and ${\bf C}_{ij}$ reflects the confidence controlled
    by $a$ and $b$ as discussed in ([DBLP:conf/icdm/HuKV08,](#bib.bib50) ). ${\bf
    C}_{j}$ and ${\bf R}_{j}$ are defined similarly for item $j$.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf U}=({\bf u}_{i})^{I}_{i=1}$，${\bf V}=({\bf v}_{j})^{J}_{j=1}$，${\bf
    C}_{i}=\mbox{diag}({\bf C}_{i1},\ldots,{\bf C}_{iJ})$ 是一个对角矩阵，${\bf R}_{i}=({\bf
    R}_{i1},\ldots,{\bf R}_{iJ})^{T}$ 是一个包含用户 $i$ 所有评分的列向量，${\bf C}_{ij}$ 反映了由 $a$
    和 $b$ 控制的置信度，如 ([DBLP:conf/icdm/HuKV08,](#bib.bib50) ) 所讨论的。${\bf C}_{j}$ 和 ${\bf
    R}_{j}$ 对于项 $j$ 的定义类似。
- en: 'Given ${\bf U}$ and ${\bf V}$, we can learn the weights ${\bf W}_{l}$ and biases
    ${\bf b}_{l}$ for each layer using the back-propagation learning algorithm. The
    gradients of the likelihood with respect to ${\bf W}_{l}$ and ${\bf b}_{l}$ are
    as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定 ${\bf U}$ 和 ${\bf V}$ 的情况下，我们可以使用反向传播学习算法来学习每一层的权重 ${\bf W}_{l}$ 和偏置 ${\bf
    b}_{l}$。关于 ${\bf W}_{l}$ 和 ${\bf b}_{l}$ 的似然函数的梯度如下：
- en: '|  | $\displaystyle\nabla_{{\bf W}_{l}}\mathscr{L}=$ | $\displaystyle-\lambda_{w}{\bf
    W}_{l}-\lambda_{v}\sum\limits_{j}\nabla_{{\bf W}_{l}}f_{e}({\bf X}_{0,j*},{\bf
    W}^{+})^{T}(f_{e}({\bf X}_{0,j*},{\bf W}^{+})^{T}-{\bf v}_{j})$ |  |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{{\bf W}_{l}}\mathscr{L}=$ | $\displaystyle-\lambda_{w}{\bf
    W}_{l}-\lambda_{v}\sum\limits_{j}\nabla_{{\bf W}_{l}}f_{e}({\bf X}_{0,j*},{\bf
    W}^{+})^{T}(f_{e}({\bf X}_{0,j*},{\bf W}^{+})^{T}-{\bf v}_{j})$ |  |'
- en: '|  |  | $\displaystyle-\lambda_{n}\sum\limits_{j}\nabla_{{\bf W}_{l}}f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})(f_{r}({\bf X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*})$ |  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\lambda_{n}\sum\limits_{j}\nabla_{{\bf W}_{l}}f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})(f_{r}({\bf X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*})$ |  |'
- en: '|  | $\displaystyle\nabla_{{\bf b}_{l}}\mathscr{L}=$ | $\displaystyle-\lambda_{w}{\bf
    b}_{l}-\lambda_{v}\sum\limits_{j}\nabla_{{\bf b}_{l}}f_{e}({\bf X}_{0,j*},{\bf
    W}^{+})^{T}(f_{e}({\bf X}_{0,j*},{\bf W}^{+})^{T}-{\bf v}_{j})$ |  |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\nabla_{{\bf b}_{l}}\mathscr{L}=$ | $\displaystyle-\lambda_{w}{\bf
    b}_{l}-\lambda_{v}\sum\limits_{j}\nabla_{{\bf b}_{l}}f_{e}({\bf X}_{0,j*},{\bf
    W}^{+})^{T}(f_{e}({\bf X}_{0,j*},{\bf W}^{+})^{T}-{\bf v}_{j})$ |  |'
- en: '|  |  | $\displaystyle-\lambda_{n}\sum\limits_{j}\nabla_{{\bf b}_{l}}f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})(f_{r}({\bf X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*}).$ |  |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\lambda_{n}\sum\limits_{j}\nabla_{{\bf b}_{l}}f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})(f_{r}({\bf X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*}).$ |  |'
- en: By alternating the update of ${\bf U}$, ${\bf V}$, ${\bf W}_{l}$, and ${\bf
    b}_{l}$, we can find a local optimum for $\mathscr{L}$. Several commonly used
    techniques such as using a momentum term may be applied to alleviate the local
    optimum problem.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 通过交替更新 ${\bf U}$、${\bf V}$、${\bf W}_{l}$ 和 ${\bf b}_{l}$，我们可以找到 $\mathscr{L}$
    的局部最优解。许多常用技术，如使用动量项，可以用于缓解局部最优问题。
- en: 'Prediction: Let $D$ be the observed test data. Similar to ([CTR,](#bib.bib112)
    ), CDL uses the point estimates of ${\bf u}_{i}$, ${\bf W}^{+}$ and $\mbox{\boldmath$\epsilon$\unboldmath}_{j}$
    to calculate the predicted rating:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 预测：设 $D$ 为观察到的测试数据。类似于 ([CTR,](#bib.bib112) )，CDL 使用 ${\bf u}_{i}$、${\bf W}^{+}$
    和 $\mbox{\boldmath$\epsilon$\unboldmath}_{j}$ 的点估计来计算预测评分：
- en: '|  | $\displaystyle E[{\bf R}_{ij}&#124;D]\approx E[{\bf u}_{i}&#124;D]^{T}(E[f_{e}({\bf
    X}_{0,j*},{\bf W}^{+})^{T}&#124;D]+E[\mbox{\boldmath$\epsilon$\unboldmath}_{j}&#124;D]),$
    |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle E[{\bf R}_{ij}&#124;D]\approx E[{\bf u}_{i}&#124;D]^{T}(E[f_{e}({\bf
    X}_{0,j*},{\bf W}^{+})^{T}&#124;D]+E[\mbox{\boldmath$\epsilon$\unboldmath}_{j}&#124;D]),$
    |  |'
- en: 'where $E[\cdot]$ denotes the expectation operation. In other words, we approximate
    the predicted rating as:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E[\cdot]$ 表示期望操作。换句话说，我们将预测评分近似为：
- en: '|  | $\displaystyle{\bf R}^{*}_{ij}\approx({\bf u}^{*}_{j})^{T}(f_{e}({\bf
    X}_{0,j*},{{\bf W}^{+}}^{*})^{T}+\mbox{\boldmath$\epsilon$\unboldmath}^{*}_{j})=({\bf
    u}^{*}_{i})^{T}{\bf v}^{*}_{j}.$ |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf R}^{*}_{ij}\approx({\bf u}^{*}_{j})^{T}(f_{e}({\bf
    X}_{0,j*},{{\bf W}^{+}}^{*})^{T}+\mbox{\boldmath$\epsilon$\unboldmath}^{*}_{j})=({\bf
    u}^{*}_{i})^{T}{\bf v}^{*}_{j}.$ |  |'
- en: Note that for any new item $j$ with no rating in the training data, its offset
    $\mbox{\boldmath$\epsilon$\unboldmath}^{*}_{j}$ will be ${\bf 0}$.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于训练数据中没有评分的任何新项目 $j$，它的偏差 $\mbox{\boldmath$\epsilon$\unboldmath}^{*}_{j}$
    将为 ${\bf 0}$。
- en: Recall that in CDL, the probabilistic SDAE and PMF work as the perception and
    task-specific components. As mentioned in Section [4](#S4 "4\. Bayesian Deep Learning
    ‣ A Survey on Bayesian Deep Learning"), both components can take various forms,
    leading to different concrete models. For example, one can replace the probabilistic
    SDAE with a VAE or an NPN as the perception component ([ColVAE,](#bib.bib68) ).
    It is also possible to use Bayesian PMF ([BPMF,](#bib.bib97) ) rather than PMF ([PMF,](#bib.bib96)
    ) as the task-specific component and thereby produce more robust predictions.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 请回忆，在CDL中，概率性自编码器（SDAE）和概率矩阵因式分解（PMF）作为感知和特定任务组件工作。如第 [4](#S4 "4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning") 节所提到的，这两个组件可以采取各种形式，从而导致不同的具体模型。例如，可以用变分自编码器（VAE）或非参数朴素贝叶斯方法（NPN）替代概率性SDAE作为感知组件  ([ColVAE,](#bib.bib68)
    )。还可以使用贝叶斯PMF ([BPMF,](#bib.bib97) ) 而不是 PMF ([PMF,](#bib.bib96) ) 作为特定任务组件，从而产生更稳健的预测。
- en: In the following subsections, we provide several extensions of CDL from different
    perspectives.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们从不同角度提供几种CDL的扩展。
- en: 5.1.2\. Bayesian Collaborative Deep Learning
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 贝叶斯协同深度学习
- en: 'Besides the MAP estimates, a sampling-based algorithm for the Bayesian treatment
    of CDL is also proposed in ([CDL,](#bib.bib121) ). This algorithm turns out to
    be a Bayesian and generalized version of BP. We list the key conditional densities
    as follows:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 除了MAP估计外，还提出了一种采样算法用于CDL的贝叶斯处理，该算法被视为BP的贝叶斯和广义版本 ([CDL,](#bib.bib121) )。我们列出以下关键条件密度：
- en: 'For ${\bf W}^{+}$: We denote the concatenation of ${\bf W}_{l,*n}$ and ${\bf
    b}_{l}^{(n)}$ as ${\bf W}_{l,*n}^{+}$. Similarly, the concatenation of ${\bf X}_{l,j*}$
    and $1$ is denoted as ${\bf X}_{l,j*}^{+}$. The subscripts of ${\bf I}$ are ignored.
    Then'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf W}^{+}$：我们将 ${\bf W}_{l,*n}$ 和 ${\bf b}_{l}^{(n)}$ 的拼接表示为 ${\bf W}_{l,*n}^{+}$。同样，${\bf
    X}_{l,j*}$ 和 $1$ 的拼接表示为 ${\bf X}_{l,j*}^{+}$。忽略 ${\bf I}$ 的下标。然后
- en: '|  | $\displaystyle p({\bf W}_{l,*n}^{+}&#124;{\bf X}_{l-1,j*},{\bf X}_{l,j*},\lambda_{s})\propto\
    {\mathcal{N}}({\bf W}_{l,*n}^{+}&#124;0,\lambda_{w}^{-1}{\bf I})\cdot{\mathcal{N}}({\bf
    X}_{l,*n}&#124;\sigma({\bf X}_{l-1}^{+}{\bf W}_{l,*n}^{+}),\lambda_{s}^{-1}{\bf
    I}).$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p({\bf W}_{l,*n}^{+}&#124;{\bf X}_{l-1,j*},{\bf X}_{l,j*},\lambda_{s})\propto\
    {\mathcal{N}}({\bf W}_{l,*n}^{+}&#124;0,\lambda_{w}^{-1}{\bf I})\cdot{\mathcal{N}}({\bf
    X}_{l,*n}&#124;\sigma({\bf X}_{l-1}^{+}{\bf W}_{l,*n}^{+}),\lambda_{s}^{-1}{\bf
    I}).$ |  |'
- en: 'For ${\bf X}_{l,j*}$ ($l\neq L/2$): Similarly, we denote the concatenation
    of ${\bf W}_{l}$ and ${\bf b}_{l}$ as ${\bf W}_{l}^{+}$ and have'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf X}_{l,j*}$ ($l\neq L/2$)：同样，我们将 ${\bf W}_{l}$ 和 ${\bf b}_{l}$ 的拼接表示为
    ${\bf W}_{l}^{+}$，并且有
- en: '|  |  | $\displaystyle p({\bf X}_{l,j*}&#124;{\bf W}_{l}^{+},{\bf W}_{l+1}^{+},{\bf
    X}_{l-1,j*},{\bf X}_{l+1,j*}\lambda_{s})$ |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle p({\bf X}_{l,j*}&#124;{\bf W}_{l}^{+},{\bf W}_{l+1}^{+},{\bf
    X}_{l-1,j*},{\bf X}_{l+1,j*}\lambda_{s})$ |  |'
- en: '|  | $\displaystyle\propto\ $ | $\displaystyle{\mathcal{N}}({\bf X}_{l,j*}&#124;\sigma({\bf
    X}_{l-1,j*}^{+}{\bf W}_{l}^{+}),\lambda_{s}^{-1}{\bf I})\cdot{\mathcal{N}}({\bf
    X}_{l+1,j*}&#124;\sigma({\bf X}_{l,j*}^{+}{\bf W}_{l+1}^{+}),\lambda_{s}^{-1}{\bf
    I}),$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\propto\ $ | $\displaystyle{\mathcal{N}}({\bf X}_{l,j*}&#124;\sigma({\bf
    X}_{l-1,j*}^{+}{\bf W}_{l}^{+}),\lambda_{s}^{-1}{\bf I})\cdot{\mathcal{N}}({\bf
    X}_{l+1,j*}&#124;\sigma({\bf X}_{l,j*}^{+}{\bf W}_{l+1}^{+}),\lambda_{s}^{-1}{\bf
    I})$ |  |'
- en: where for the last layer ($l=L$) the second Gaussian would be ${\mathcal{N}}({\bf
    X}_{c,j*}|{\bf X}_{l,j*},\lambda_{s}^{-1}{\bf I})$ instead.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于最后一层 ($l=L$)，第二个高斯分布应改为 ${\mathcal{N}}({\bf X}_{c,j*}|{\bf X}_{l,j*},\lambda_{s}^{-1}{\bf
    I})$。
- en: 'For ${\bf X}_{l,j*}$ ($l=L/2$): Similarly, we have'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf X}_{l,j*}$ ($l=L/2$)：类似地，我们有
- en: '|  |  | $\displaystyle p({\bf X}_{l,j*}&#124;{\bf W}_{l}^{+},{\bf W}_{l+1}^{+},{\bf
    X}_{l-1,j*},{\bf X}_{l+1,j*},\lambda_{s},\lambda_{v},{\bf v}_{j})$ |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle p({\bf X}_{l,j*}&#124;{\bf W}_{l}^{+},{\bf W}_{l+1}^{+},{\bf
    X}_{l-1,j*},{\bf X}_{l+1,j*},\lambda_{s},\lambda_{v},{\bf v}_{j})$ |  |'
- en: '|  | $\displaystyle\propto\ $ | $\displaystyle{\mathcal{N}}({\bf X}_{l,j*}&#124;\sigma({\bf
    X}_{l-1,j*}^{+}{\bf W}_{l}^{+}),\lambda_{s}^{-1}{\bf I})\cdot{\mathcal{N}}({\bf
    X}_{l+1,j*}&#124;\sigma({\bf X}_{l,j*}^{+}{\bf W}_{l+1}^{+}),\lambda_{s}^{-1}{\bf
    I})\cdot{\mathcal{N}}({\bf v}_{j}&#124;{\bf X}_{l,j*},\lambda_{v}^{-1}{\bf I}).$
    |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\propto\ $ | $\displaystyle{\mathcal{N}}({\bf X}_{l,j*}&#124;\sigma({\bf
    X}_{l-1,j*}^{+}{\bf W}_{l}^{+}),\lambda_{s}^{-1}{\bf I})\cdot{\mathcal{N}}({\bf
    X}_{l+1,j*}&#124;\sigma({\bf X}_{l,j*}^{+}{\bf W}_{l+1}^{+}),\lambda_{s}^{-1}{\bf
    I})\cdot{\mathcal{N}}({\bf v}_{j}&#124;{\bf X}_{l,j*},\lambda_{v}^{-1}{\bf I})$
    |  |'
- en: 'For ${\bf v}_{j}$: The posterior $p({\bf v}_{j}|{\bf X}_{L/2,j*},{\bf R}_{*j},{\bf
    C}_{*j},\lambda_{v},{\bf U})\propto{\mathcal{N}}({\bf v}_{j}|{\bf X}_{L/2,j*}^{T},\lambda_{v}^{-1}{\bf
    I})\prod\limits_{i}{\mathcal{N}}({\bf R}_{ij}|{\bf u}_{i}^{T}{\bf v}_{j},{\bf
    C}_{ij}^{-1}).$'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf v}_{j}$：后验概率 $p({\bf v}_{j}|{\bf X}_{L/2,j*},{\bf R}_{*j},{\bf C}_{*j},\lambda_{v},{\bf
    U})\propto{\mathcal{N}}({\bf v}_{j}|{\bf X}_{L/2,j*}^{T},\lambda_{v}^{-1}{\bf
    I})\prod\limits_{i}{\mathcal{N}}({\bf R}_{ij}|{\bf u}_{i}^{T}{\bf v}_{j},{\bf
    C}_{ij}^{-1})$。
- en: 'For ${\bf u}_{i}$: The posterior $p({\bf u}_{i}|{\bf R}_{i*},{\bf V},\lambda_{u},{\bf
    C}_{i*})\propto{\mathcal{N}}({\bf u}_{i}|0,\lambda_{u}^{-1}{\bf I})\prod\limits_{j}{\mathcal{N}}({\bf
    R}_{ij}|{\bf u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1}).$'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf u}_{i}$：后验概率 $p({\bf u}_{i}|{\bf R}_{i*},{\bf V},\lambda_{u},{\bf C}_{i*})\propto{\mathcal{N}}({\bf
    u}_{i}|0,\lambda_{u}^{-1}{\bf I})\prod\limits_{j}{\mathcal{N}}({\bf R}_{ij}|{\bf
    u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1})$。
- en: Interestingly, if $\lambda_{s}$ goes to infinity and adaptive rejection Metropolis
    sampling (which involves using the gradients of the objective function to approximate
    the proposal distribution) is used, the sampling for ${\bf W}^{+}$ turns out to
    be a *Bayesian generalized* version of BP. Specifically, as Figure [8](#S5.F8
    "Figure 8 ‣ 5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised Bayesian Deep
    Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣
    A Survey on Bayesian Deep Learning")(right) shows, after getting the gradient
    of the loss function at one point (the red dashed line on the left), the next
    sample would be drawn in the region under that line, which is equivalent to a
    probabilistic version of BP. If a sample is above the curve of the loss function,
    a new tangent line (the black dashed line on the right) would be added to better
    approximate the distribution corresponding to the loss function. After that, samples
    would be drawn from the region under both lines. During the sampling, besides
    searching for local optima using the gradients (MAP), the algorithm also takes
    the variance into consideration. That is why it is called *Bayesian generalized
    back-propagation* in ([CDL,](#bib.bib121) ).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，如果 $\lambda_{s}$ 趋近于无穷大，并且使用了自适应拒绝 Metropolis 采样（涉及使用目标函数的梯度来近似提议分布），那么对
    ${\bf W}^{+}$ 的采样实际上变成了一个*贝叶斯广义*的 BP 版本。具体来说，如图 [8](#S5.F8 "图 8 ‣ 5.1.1\. 协作深度学习
    ‣ 5.1\. 监督式贝叶斯深度学习推荐系统 ‣ 5\. 具体的 BDL 模型与应用 ‣ 贝叶斯深度学习综述")(右) 所示，在某一点（左侧的红色虚线）获得损失函数的梯度后，下一个样本将会在该线下方的区域中绘制，这等同于
    BP 的一个概率版本。如果一个样本位于损失函数曲线的上方，则会添加一条新的切线（右侧的黑色虚线）以更好地近似与损失函数对应的分布。之后，样本将会从两条线下方的区域中绘制。在采样过程中，除了使用梯度（MAP）寻找局部最优解外，算法还考虑了方差。这就是为什么它在
    ([CDL,](#bib.bib121) ) 中被称为*贝叶斯广义反向传播*。
- en: 5.1.3\. Marginalized Collaborative Deep Learning
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 边际化协作深度学习
- en: In SDAE, corrupted input goes through the encoder and decoder to recover the
    clean input. Usually, different epochs of training use different corrupted versions
    as input. Hence generally, SDAE needs to go through enough epochs of training
    to see sufficient corrupted versions of the input. Marginalized SDAE (mSDAE) ([mSDAE,](#bib.bib14)
    ) seeks to avoid this by marginalizing out the corrupted input and obtaining closed-form
    solutions directly. In this sense, mSDAE is more computationally efficient than
    SDAE.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SDAE 中，被损坏的输入通过编码器和解码器以恢复干净的输入。通常，不同的训练时期使用不同的被损坏版本作为输入。因此，通常，SDAE 需要经过足够的训练时期以看到足够的被损坏版本。边际化
    SDAE (mSDAE) ([mSDAE,](#bib.bib14)) 试图通过边际化被损坏的输入并直接获得封闭形式的解来避免这一点。从这个意义上讲，mSDAE
    比 SDAE 更具计算效率。
- en: 'As mentioned in ([li2015deep,](#bib.bib66) ), using mSDAE instead of the Bayesian
    SDAE could lead to more efficient learning algorithms. For example, in ([li2015deep,](#bib.bib66)
    ), the objective when using a one-layer mSDAE can be written as follows:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如 ([li2015deep,](#bib.bib66)) 中提到的，使用 mSDAE 替代贝叶斯 SDAE 可能会导致更高效的学习算法。例如，在 ([li2015deep,](#bib.bib66))
    中，使用单层 mSDAE 时的目标可以写成如下：
- en: '|  | $\displaystyle\mathscr{L}=-\sum\limits_{j}\&#124;\widetilde{{\bf X}}_{0,j*}{\bf
    W}_{1}-\overline{{\bf X}}_{c,j*}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf C}_{ij}}{2}({\bf
    R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2}-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}^{T}{\bf
    P}_{1}-{\bf X}_{0,j*}{\bf W}_{1}\&#124;_{2}^{2},$ |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathscr{L}=-\sum\limits_{j}\&#124;\widetilde{{\bf X}}_{0,j*}{\bf
    W}_{1}-\overline{{\bf X}}_{c,j*}\&#124;_{2}^{2}-\sum\limits_{i,j}\frac{{\bf C}_{ij}}{2}({\bf
    R}_{ij}-{\bf u}_{i}^{T}{\bf v}_{j})^{2}-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}^{T}{\bf
    P}_{1}-{\bf X}_{0,j*}{\bf W}_{1}\&#124;_{2}^{2},$ |  |'
- en: where $\widetilde{{\bf X}}_{0,j*}$ is the collection of $k$ different corrupted
    versions of ${{\bf X}}_{0,j*}$ (a $k$-by-$B$ matrix) and $\overline{{\bf X}}_{c,j*}$
    is the $k$-time repeated version of ${{\bf X}}_{c,j*}$ (also a $k$-by-$B$ matrix).
    ${\bf P}_{1}$ is the transformation matrix for item latent factors.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{{\bf X}}_{0,j*}$ 是 $k$ 个不同的被损坏版本的集合 ($k$-by-$B$ 矩阵)，而 $\overline{{\bf
    X}}_{c,j*}$ 是 $k$ 次重复的 ${{\bf X}}_{c,j*}$ 的版本 (同样是 $k$-by-$B$ 矩阵)。${\bf P}_{1}$
    是用于项目潜在因素的变换矩阵。
- en: The solution for ${\bf W}_{1}$ would be ${\bf W}_{1}=E({\bf S}_{1})E({\bf Q}_{1})^{-1}$,
    where ${\bf S}_{1}=\overline{{\bf X}}_{c,j*}^{T}\widetilde{{\bf X}}_{0,j*}+\frac{\lambda_{v}}{2}{\bf
    P}_{1}^{T}{\bf V}{\bf X}_{c}$ and ${\bf Q}_{1}=\overline{{\bf X}}_{c,j*}^{T}\widetilde{{\bf
    X}}_{0,j*}+\frac{\lambda_{v}}{2}{\bf X}_{c}^{T}{\bf X}_{c}$. A solver for the
    expectation in the equation above is provided in ([mSDAE,](#bib.bib14) ). Note
    that this is a linear and one-layer case which can be generalized to the nonlinear
    and multi-layer case using the same techniques as in ([mSDAE,](#bib.bib14) ; [nmSDAE,](#bib.bib13)
    ).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ${\bf W}_{1}$ 的解将是 ${\bf W}_{1}=E({\bf S}_{1})E({\bf Q}_{1})^{-1}$，其中 ${\bf
    S}_{1}=\overline{{\bf X}}_{c,j*}^{T}\widetilde{{\bf X}}_{0,j*}+\frac{\lambda_{v}}{2}{\bf
    P}_{1}^{T}{\bf V}{\bf X}_{c}$ 和 ${\bf Q}_{1}=\overline{{\bf X}}_{c,j*}^{T}\widetilde{{\bf
    X}}_{0,j*}+\frac{\lambda_{v}}{2}{\bf X}_{c}^{T}{\bf X}_{c}$。上述方程中期望的解法在 ([mSDAE,](#bib.bib14))
    中提供。注意，这是一种线性单层情况，可以使用 ([mSDAE,](#bib.bib14); [nmSDAE,](#bib.bib13)) 中相同的技术推广到非线性和多层情况。
- en: Marginalized CDL’s perception variables $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{{\bf
    X}_{0},{\bf X}_{c},{\bf W}_{1}\}$, its hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    V}\}$, and its task variables $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf
    P}_{1},{\bf R},{\bf U}\}$.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 边际化 CDL 的感知变量 $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{{\bf X}_{0},{\bf X}_{c},{\bf
    W}_{1}\}$，其铰链变量 $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf V}\}$，以及其任务变量
    $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf P}_{1},{\bf R},{\bf U}\}$。
- en: 5.1.4\. Collaborative Deep Ranking
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4\. 协同深度排名
- en: 'CDL assumes a collaborative filtering setting to model the ratings directly.
    Naturally, one can design a similar model to focus more on the ranking among items
    rather than exact ratings ([yingcollaborative,](#bib.bib131) ). The corresponding
    generative process is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: CDL 假设一个协同过滤的设置以直接建模评分。自然地，可以设计一个类似的模型，更加关注项目之间的排名而非准确评分 ([yingcollaborative,](#bib.bib131))。对应的生成过程如下：
- en: (1)
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: For each layer $l$ of the SDAE network,
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 SDAE 网络的每一层 $l$，
- en: (a)
  id: totrans-331
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: For each column $n$ of the weight matrix ${\bf W}_{l}$, draw ${\bf W}_{l,*n}\sim{\mathcal{N}}({\bf
    0},\lambda_{w}^{-1}{\bf I}_{K_{l}})$.
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于权重矩阵 ${\bf W}_{l}$ 的每一列 $n$，画出 ${\bf W}_{l,*n}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$。
- en: (b)
  id: totrans-333
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Draw the bias vector ${\bf b}_{l}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$.
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 画出偏置向量 ${\bf b}_{l}\sim{\mathcal{N}}({\bf 0},\lambda_{w}^{-1}{\bf I}_{K_{l}})$。
- en: (c)
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: For each row $j$ of ${\bf X}_{l}$, draw ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每一行 $j$ 的 ${\bf X}_{l}$，抽取 ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf X}_{l-1,j*}{\bf
    W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$
- en: (2)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: For each item $j$,
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个项 $j$，
- en: (a)
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: Draw a clean input ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf
    I}_{J}$).
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 抽取一个干净的输入 ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf
    I}_{J}$)。
- en: (b)
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Draw a latent item offset vector $\mbox{\boldmath$\epsilon$\unboldmath}_{j}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K})$ and then set the latent item vector to be: ${\bf
    v}_{j}=\mbox{\boldmath$\epsilon$\unboldmath}_{j}+{\bf X}_{\frac{L}{2},j*}^{T}.$'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 抽取一个潜在项偏移向量 $\mbox{\boldmath$\epsilon$\unboldmath}_{j}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K})$，然后将潜在项向量设置为：${\bf v}_{j}=\mbox{\boldmath$\epsilon$\unboldmath}_{j}+{\bf
    X}_{\frac{L}{2},j*}^{T}.$
- en: (3)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: For each user $i$,
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个用户 $i$，
- en: (a)
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Draw a latent user vector for each user $i$: ${\bf u}_{i}\sim{\mathcal{N}}({\bf
    0},\lambda_{u}^{-1}{\bf I}_{K}).$'
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个用户 $i$ 抽取一个潜在的用户向量：${\bf u}_{i}\sim{\mathcal{N}}({\bf 0},\lambda_{u}^{-1}{\bf
    I}_{K}).$
- en: (b)
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'For each pair-wise preference $(j,k)\in\mathcal{P}_{i}$, where $\mathcal{P}_{i}=\{(j,k):{\bf
    R}_{ij}-{\bf R}_{ik}>0\}$, draw the preference: <math id="S5.I2.i3.I1.i2.p1.3.m3.2"
    class="ltx_Math" alttext="\mbox{\boldmath$\Delta$\unboldmath}_{ijk}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j}-{\bf u}_{i}^{T}{\bf v}_{k},{\bf C}_{ijk}^{-1}).\\'
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每对偏好 $(j,k)\in\mathcal{P}_{i}$，其中 $\mathcal{P}_{i}=\{(j,k):{\bf R}_{ij}-{\bf
    R}_{ik}>0\}$，抽取偏好：<math id="S5.I2.i3.I1.i2.p1.3.m3.2" class="ltx_Math" alttext="\mbox{\boldmath$\Delta$\unboldmath}_{ijk}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j}-{\bf u}_{i}^{T}{\bf v}_{k},{\bf C}_{ijk}^{-1}).\\
- en: '" display="inline"><semantics id="S5.I2.i3.I1.i2.p1.3.m3.2a"><mrow id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.cmml"><mrow id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.cmml"><msub id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.cmml"><mi id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.2"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.2.cmml">𝚫</mi><mrow id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.cmml"><mi id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.2"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1.cmml">​</mo><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.3.cmml">j</mi><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1a" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1.cmml">​</mo><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.4" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.4.cmml">k</mi></mrow></msub><mo
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.3.cmml">∼</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.4" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.4.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.3.cmml">​</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml"><mo
    stretchy="false" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml">(</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.cmml"><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.cmml"><msubsup
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.2.cmml">𝐮</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.3.cmml">i</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.3.cmml">T</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><msub
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.2.cmml">𝐯</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.3.cmml">j</mi></msub></mrow><mo
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.1.cmml">−</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.cmml"><msubsup
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.2.cmml">𝐮</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.3.cmml">i</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.3.cmml">T</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.1.cmml">​</mo><msub
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.2.cmml">𝐯</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.3.cmml">k</mi></msub></mrow></mrow><mo
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.4" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml">,</mo><msubsup
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.2.cmml">𝐂</mi><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.2.cmml">i</mi><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.3.cmml">j</mi><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.1a" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.1.cmml">​</mo><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.4" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.4.cmml">k</mi></mrow><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.cmml"><mo
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3a" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.cmml">−</mo><mn
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.2.cmml">1</mn></mrow></msubsup><mo
    stretchy="false" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.5" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo
    lspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S5.I2.i3.I1.i2.p1.3.m3.2b"><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1"><csymbol cd="latexml" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.3.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.3">similar-to</csymbol><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4"><csymbol cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.1.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4">subscript</csymbol><ci id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.2.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.2">𝚫</ci><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3"><ci id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.2.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.2">𝑖</ci><ci id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.3.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.3">𝑗</ci><ci id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.4.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.4">𝑘</ci></apply></apply><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2"><ci id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.4.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.4">𝒩</ci><interval closure="open" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2"><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1"><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2"><apply id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.1.cmml"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2">superscript</csymbol><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2">subscript</csymbol><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.2">𝐮</ci><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.3">𝑖</ci></apply><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.3">𝑇</ci></apply><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3">subscript</csymbol><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.2">𝐯</ci><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.3">𝑗</ci></apply></apply><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3"><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2">superscript</csymbol><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2">subscript</csymbol><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.2">𝐮</ci><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.2.3">𝑖</ci></apply><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.2.3">𝑇</ci></apply><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3">subscript</csymbol><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.2">𝐯</ci><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.3.3.3">𝑘</ci></apply></apply></apply><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2">superscript</csymbol><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2"><csymbol
    cd="ambiguous" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.1.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2">subscript</csymbol><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.2">𝐂</ci><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3"><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.2">𝑖</ci><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.3">𝑗</ci><ci
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.4.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.2.3.4">𝑘</ci></apply></apply><apply
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3"><cn
    type="integer" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.2.cmml" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.2.3.2">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S5.I2.i3.I1.i2.p1.3.m3.2c">\mbox{\boldmath$\Delta$\unboldmath}_{ijk}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j}-{\bf u}_{i}^{T}{\bf v}_{k},{\bf C}_{ijk}^{-1}).\\</annotation></semantics></math>'
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`display="inline"><semantics id="S5.I2.i3.I1.i2.p1.3.m3.2a"><mrow id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.cmml"><mrow id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.cmml"><msub id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.cmml"><mi id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.2"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.2.cmml">𝚫</mi><mrow id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.cmml"><mi id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.2"
    xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.2.cmml">i</mi><mo lspace="0em" rspace="0em"
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1.cmml">​</mo><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.3.cmml">j</mi><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1a" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.1.cmml">​</mo><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.4" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.4.3.4.cmml">k</mi></mrow></msub><mo
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.3.cmml">∼</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.cmml"><mi
    class="ltx_font_mathcaligraphic" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.4" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.4.cmml">𝒩</mi><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.3.cmml">​</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml"><mo
    stretchy="false" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.2.2.3.cmml">(</mo><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.cmml"><mrow
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.cmml"><msubsup
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.2.cmml">𝐮</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.2.3.cmml">i</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.2.3.cmml">T</mi></msubsup><mo
    lspace="0em" rspace="0em" id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.1" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><msub
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.cmml"><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.2" xref="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.2.cmml">𝐯</mi><mi
    id="S5.I2.i3.I1.i2.p1.3.m3.2.2.1.1.1.1.1.1.2.3.3" xref="S5.I2.i3.I1.i2.p1.3.m'
- en: 'Following the generative process above, the log-likelihood in Equation ([5.1.1](#S5.Ex20
    "5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised Bayesian Deep Learning
    for Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey
    on Bayesian Deep Learning")) becomes:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述生成过程，方程（[5.1.1](#S5.Ex20 "5.1.1\. Collaborative Deep Learning ‣ 5.1\. Supervised
    Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning")）中的对数似然变为：
- en: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf W}_{l}\&#124;_{F}^{2}+\&#124;{\bf
    b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}-f_{e}({\bf
    X}_{0,j*},{\bf W}^{+})^{T}\&#124;_{2}^{2}$ |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{u}}{2}\sum\limits_{i}\&#124;{\bf
    u}_{i}\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf W}_{l}\&#124;_{F}^{2}+\&#124;{\bf
    b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{v}}{2}\sum\limits_{j}\&#124;{\bf v}_{j}-f_{e}({\bf
    X}_{0,j*},{\bf W}^{+})^{T}\&#124;_{2}^{2}$ |  |'
- en: '|  |  | $\displaystyle-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*}\&#124;_{2}^{2}-\sum\limits_{i,j,k}\frac{{\bf
    C}_{ijk}}{2}(\mbox{\boldmath$\Delta$\unboldmath}_{ijk}-({\bf u}_{i}^{T}{\bf v}_{j}-{\bf
    u}_{i}^{T}{\bf v}_{k}))^{2}.$ |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;f_{r}({\bf
    X}_{0,j*},{\bf W}^{+})-{\bf X}_{c,j*}\&#124;_{2}^{2}-\sum\limits_{i,j,k}\frac{{\bf
    C}_{ijk}}{2}(\mbox{\boldmath$\Delta$\unboldmath}_{ijk}-({\bf u}_{i}^{T}{\bf v}_{j}-{\bf
    u}_{i}^{T}{\bf v}_{k}))^{2}.$ |  |'
- en: Similar algorithms can be used to learn the parameters in CDR. As reported in
    ([yingcollaborative,](#bib.bib131) ), using the ranking objective leads to significant
    improvement in the recommendation performance. Following the definition in Section
    [4.2](#S4.SS2 "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey
    on Bayesian Deep Learning"), CDR’s perception variables $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf
    W}_{l}\},\{{\bf b}_{l}\},\{{\bf X}_{l}\},{\bf X}_{c}\}$, the hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    V}\}$, and the task variables $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf
    U},\mbox{\boldmath$\Delta$\unboldmath}\}$.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 相似的算法可以用来学习CDR中的参数。如在（[yingcollaborative,](#bib.bib131)）中所述，使用排名目标会显著提高推荐性能。根据第[4.2](#S4.SS2
    "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning")节的定义，CDR的感知变量为$\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf W}_{l}\},\{{\bf
    b}_{l}\},\{{\bf X}_{l}\},{\bf X}_{c}\}$，铰链变量为$\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    V}\}$，任务变量为$\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf U},\mbox{\boldmath$\Delta$\unboldmath}\}$。
- en: 5.1.5\. Collaborative Variational Autoencoders
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.5\. 协作变分自编码器
- en: 'In CDL, the perception component takes the form of a probabilistic SDAE. Naturally,
    one can also replace the probabilistic SDAE in CDL with a VAE (introduced in Section [4.3.3](#S4.SS3.SSS3
    "4.3.3\. Variational Autoencoders ‣ 4.3\. Perception Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")), as is done in collaborative
    variational autoencoders (CVAE) ([ColVAE,](#bib.bib68) ). Specifically, CVAE with
    a inference network (encoder) denoted as $(f_{\mu}(\cdot),f_{s}(\cdot))$ and a
    generation network (decoder) denoted as $g(\cdot)$ assumes the following generative
    process:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在CDL中，感知组件采用概率性SDAE的形式。当然，还可以将CDL中的概率性SDAE替换为VAE（在第[4.3.3](#S4.SS3.SSS3 "4.3.3\.
    Variational Autoencoders ‣ 4.3\. Perception Component ‣ 4\. Bayesian Deep Learning
    ‣ A Survey on Bayesian Deep Learning")节中介绍），如在协作变分自编码器（CVAE）中（[ColVAE,](#bib.bib68)）。具体来说，CVAE具有一个推理网络（编码器），记为$(f_{\mu}(\cdot),f_{s}(\cdot))$，和一个生成网络（解码器），记为$g(\cdot)$，假设以下生成过程：
- en: (1)
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: For each item $j$,
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个项目$j$，
- en: (a)
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: 'Draw the latent item vector from the VAE inference network: ${\bf z}_{j}\sim{\mathcal{N}}(f_{\mu}({\bf
    X}_{0,j*}),f_{s}({\bf X}_{0,j*}))$'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从VAE推理网络中绘制潜在项目向量：${\bf z}_{j}\sim{\mathcal{N}}(f_{\mu}({\bf X}_{0,j*}),f_{s}({\bf
    X}_{0,j*}))$
- en: (b)
  id: totrans-360
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: 'Draw the latent item offset vector $\mbox{\boldmath$\epsilon$\unboldmath}_{j}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K})$ and then set the latent item vector: ${\bf v}_{j}=\mbox{\boldmath$\epsilon$\unboldmath}_{j}+{\bf
    z}_{j}.$'
  id: totrans-361
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从这里生成潜在项目偏移向量$\mbox{\boldmath$\epsilon$\unboldmath}_{j}\sim{\mathcal{N}}({\bf
    0},\lambda_{v}^{-1}{\bf I}_{K})$，然后设置潜在项目向量：${\bf v}_{j}=\mbox{\boldmath$\epsilon$\unboldmath}_{j}+{\bf
    z}_{j}.$
- en: (c)
  id: totrans-362
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: Draw the orignial input from the VAE generation network ${\bf X}_{0,j*}\sim{\mathcal{N}}(g({\bf
    z}_{j}),\lambda_{n}^{-1}{\bf I}_{B}$).
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从VAE生成网络中获取原始输入${\bf X}_{0,j*}\sim{\mathcal{N}}(g({\bf z}_{j}),\lambda_{n}^{-1}{\bf
    I}_{B}$)。
- en: (2)
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Draw a latent user vector for each user $i$: ${\bf u}_{i}\sim{\mathcal{N}}({\bf
    0},\lambda_{u}^{-1}{\bf I}_{K}).$'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个用户$i$绘制潜在用户向量：${\bf u}_{i}\sim{\mathcal{N}}({\bf 0},\lambda_{u}^{-1}{\bf
    I}_{K})$。
- en: (3)
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Draw a rating ${\bf R}_{ij}$ for each user-item pair $(i,j)$: ${\bf R}_{ij}\sim{\mathcal{N}}({\bf
    u}_{i}^{T}{\bf v}_{j},{\bf C}_{ij}^{-1}).$'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为每个用户-项目对$(i,j)$绘制评分${\bf R}_{ij}$：${\bf R}_{ij}\sim{\mathcal{N}}({\bf u}_{i}^{T}{\bf
    v}_{j},{\bf C}_{ij}^{-1})$。
- en: Similar to CDL, $\lambda_{n}$, $\lambda_{u}$, $\lambda_{s}$, and $\lambda_{v}$
    are hyperparameters and ${\bf C}_{ij}$ is a confidence parameter (${\bf C}_{ij}=a$
    if ${\bf R}_{ij}=1$ and ${\bf C}_{ij}=b$ otherwise). Following ([ColVAE,](#bib.bib68)
    ), the ELBO similar to Equation ([6](#S4.E6 "In 4.3.3\. Variational Autoencoders
    ‣ 4.3\. Perception Component ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian
    Deep Learning")) can be derived, using which one can train the model’s parameters
    using BP and the reparameterization trick.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于CDL，$\lambda_{n}$、$\lambda_{u}$、$\lambda_{s}$和$\lambda_{v}$是超参数，${\bf C}_{ij}$是置信度参数（如果${\bf
    R}_{ij}=1$则${\bf C}_{ij}=a$，否则${\bf C}_{ij}=b$）。按照（[ColVAE,](#bib.bib68)），可以推导出类似于方程（[6](#S4.E6
    "In 4.3.3\. Variational Autoencoders ‣ 4.3\. Perception Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")）的ELBO，利用该方程可以使用BP和重新参数化技巧训练模型的参数。
- en: The evolution from CDL to CVAE demonstrates the BDL framework’s flexibility
    in terms of its components’ specific forms. It is also worth noting that the perception
    component can be a recurrent version of probabilistic SDAE ([CRAE,](#bib.bib122)
    ) or VAE ([VRNN,](#bib.bib17) ; [ColVAE,](#bib.bib68) ) to handle raw sequential
    data, while the task-specific component can take more sophisticated forms to accommodate
    more complex recommendation scenarios (e.g., cross-domain recommendation).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 从CDL到CVAE的演变展示了BDL框架在其组件特定形式方面的灵活性。值得注意的是，感知组件可以是概率性SDAE的递归版本（[CRAE,](#bib.bib122)）或VAE（[VRNN,](#bib.bib17)；[ColVAE,](#bib.bib68)），以处理原始的序列数据，而任务特定组件可以采用更复杂的形式，以适应更复杂的推荐场景（例如，跨域推荐）。
- en: 5.1.6\. Discussion
  id: totrans-370
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.6. 讨论
- en: Recommender systems are a typical use case for BDL in that they often require
    both thorough understanding of high-dimensional signals (e.g., text and images)
    and principled reasoning on the conditional dependencies among users/items/ratings.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统是BDL的一个典型应用案例，因为它们通常需要对高维信号（例如，文本和图像）有深入的理解，并对用户/项目/评分之间的条件依赖进行有原则的推理。
- en: In this regard, CDL, as an instantiation of BDL, is the first hierarchical Bayesian
    model to bridge the gap between state-of-the-art deep learning models and recommender
    systems. By performing deep learning collaboratively, CDL and its variants can
    simultaneously extract an effective deep feature representation from high-dimensional
    content and capture the similarity and implicit relationship between items (and
    users). The learned representation may also be used for tasks other than recommendation.
    Unlike previous deep learning models which use a simple target like classification
    ([DBLP:journals/acl/KalchbrennerGB14,](#bib.bib56) ) and reconstruction ([DBLP:journals/jmlr/VincentLLBM10,](#bib.bib111)
    ), CDL-based models use CF as a more complex target in a probabilistic framework.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，作为BDL实例化的CDL是第一个将最先进的深度学习模型与推荐系统之间的差距弥合的层次贝叶斯模型。通过协作进行深度学习，CDL及其变体可以同时从高维内容中提取有效的深度特征表示，并捕捉项目（和用户）之间的相似性和隐含关系。学习到的表示还可以用于推荐之外的任务。与之前使用简单目标如分类（[DBLP:journals/acl/KalchbrennerGB14,](#bib.bib56)）和重建（[DBLP:journals/jmlr/VincentLLBM10,](#bib.bib111)）的深度学习模型不同，基于CDL的模型在概率框架中使用CF作为更复杂的目标。
- en: As mentioned in Section [1](#S1 "1\. Introduction ‣ A Survey on Bayesian Deep
    Learning"), *information exchange* between two components is crucial for the performance
    of BDL. In the CDL-based models above, the exchange is achieved by assuming Gaussian
    distributions that connect the hinge variables and the variables in the perception
    component (drawing the hinge variable ${\bf v}_{j}\sim{\mathcal{N}}({\bf X}_{\frac{L}{2},j*}^{T},\lambda_{v}^{-1}{\bf
    I}_{K})$ in the generative process of CDL, where ${\bf X}_{\frac{L}{2}}$ is a
    perception variable), which is simple but effective and efficient in computation.
    Among the eight CDL-based models in Table [1](#S3.T1 "Table 1 ‣ 3.1\. Models ‣
    3\. Probabilistic Graphical Models ‣ A Survey on Bayesian Deep Learning"), six
    of them are HV models and the others are LV models, according to the definition
    of Section [4.2](#S4.SS2 "4.2\. General Framework ‣ 4\. Bayesian Deep Learning
    ‣ A Survey on Bayesian Deep Learning"). Since it has been verified that the HV
    CDL significantly outperforms its ZV counterpart ([CDL,](#bib.bib121) ), we can
    expect additional performance boost from the LV counterparts of the six HV models.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[1节](#S1 "1\. Introduction ‣ A Survey on Bayesian Deep Learning")中提到的，两个组件之间的*信息交换*对于BDL的性能至关重要。在上述基于CDL的模型中，这种交换通过假设高斯分布来实现，这些分布连接了铰链变量和感知组件中的变量（在CDL的生成过程中绘制铰链变量${\bf
    v}_{j}\sim{\mathcal{N}}({\bf X}_{\frac{L}{2},j*}^{T},\lambda_{v}^{-1}{\bf I}_{K})$，其中${\bf
    X}_{\frac{L}{2}}$是感知变量），这种方法简单但在计算上有效且高效。在表[1](#S3.T1 "Table 1 ‣ 3.1\. Models
    ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian Deep Learning")中的八个基于CDL的模型中，六个是HV模型，其他是LV模型，按照第[4.2节](#S4.SS2
    "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning")的定义。由于已验证HV CDL显著优于其ZV对应物（[CDL,](#bib.bib121)），我们可以期待六个HV模型的LV对应物会带来额外的性能提升。
- en: Besides efficient *information exchange*, the model designs also meet the independence
    requirement on the distribution concerning hinge variables discussed in Section
    [4.2](#S4.SS2 "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey
    on Bayesian Deep Learning") and are hence easily parallelizable. In some models
    to be introduced later, we will see alternative designs to enable efficient and
    independent information exchange between the two components of BDL.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 除了高效的*信息交换*，模型设计还满足了第[4.2节](#S4.SS2 "4.2\. General Framework ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")中讨论的关于铰链变量分布的独立性要求，因此这些模型易于并行处理。在接下来介绍的一些模型中，我们将看到不同的设计以实现BDL两个组件之间高效且独立的信息交换。
- en: Note that BDL-based models above use typical static Bayesian networks as their
    task-specific components. Although these are often sufficient for most use cases,
    it is possible for the task-specific components to take the form of deep Bayesian
    networks such as BIN ([BIN,](#bib.bib117) ). This allows the models to handle
    highly nonlinear interactions between users and items if necessary. One can also
    use stochastic processes (or dynamic Bayesian networks in general) to explicitly
    model users purchase or clicking behaviors. For example, it is natural to model
    a user’s purchase of groceries as a Poisson process. In terms of perception components,
    one can also replace the pSDAE, mSDAE, or VAE above with their convolutional or
    recurrent counterparts (see Section [2.3](#S2.SS3 "2.3\. Convolutional Neural
    Networks ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep Learning") and Section [2.4](#S2.SS4
    "2.4\. Recurrent Neural Network ‣ 2\. Deep Learning ‣ A Survey on Bayesian Deep
    Learning")), as is done in Collaborative Knowledge Base Embedding (CKE) ([CKE,](#bib.bib132)
    ) or Collaborative Recurrent Autoencoders (CRAE) ([CRAE,](#bib.bib122) ), respectively.
    Note that for the convolutional or recurrent perception components to be compatible
    with the task-specific component (which is inherently probabilistic), ideally
    one would need to formulate probabilistic versions of CNN or RNN as well. Readers
    are referred to ([CKE,](#bib.bib132) ) and ([CRAE,](#bib.bib122) ) for more details.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，上述基于BDL的模型使用典型的静态贝叶斯网络作为其任务特定组件。虽然这些通常足以满足大多数用例，但任务特定组件也可以采取深度贝叶斯网络的形式，如BIN（[BIN,](#bib.bib117)）。这使得模型能够在必要时处理用户和项目之间的高度非线性交互。还可以使用随机过程（或一般的动态贝叶斯网络）来明确建模用户的购买或点击行为。例如，将用户购买食品建模为泊松过程是很自然的。在感知组件方面，也可以用其卷积或递归对等体（参见第[2.3](#S2.SS3
    "2.3\. 卷积神经网络 ‣ 2\. 深度学习 ‣ 贝叶斯深度学习综述")节和第[2.4](#S2.SS4 "2.4\. 递归神经网络 ‣ 2\. 深度学习
    ‣ 贝叶斯深度学习综述")节）替换上述的pSDAE、mSDAE或VAE，如在协同知识库嵌入（CKE）（[CKE,](#bib.bib132)）或协同递归自编码器（CRAE）（[CRAE,](#bib.bib122)）中所做的那样。请注意，为了使卷积或递归感知组件与任务特定组件（本质上是概率性的）兼容，理想情况下还需要制定CNN或RNN的概率版本。有关更多详细信息，请参考([CKE,](#bib.bib132)）和([CRAE,](#bib.bib122)）。
- en: In summary, this subsection discusses BDL’s applications on supervised leraning,
    using recommender systems as an example. Section [5.2](#S5.SS2 "5.2\. Unsupervised
    Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning") below will cover BDL’s applications on
    unsupervised learning.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本小节讨论了BDL在监督学习中的应用，以推荐系统为例。下面的第[5.2](#S5.SS2 "5.2\. 无监督贝叶斯深度学习用于主题模型 ‣
    5\. 具体BDL模型和应用 ‣ 贝叶斯深度学习综述")节将涵盖BDL在无监督学习中的应用。
- en: 5.2\. Unsupervised Bayesian Deep Learning for Topic Models
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 无监督贝叶斯深度学习用于主题模型
- en: To demonstrate how BDL can also be applied to unsupervised learning, we review
    some examples of BDL-based topic models in this section. These models combine
    the merits of PGM (which naturally incorporates the probabilistic relations among
    variables) and NN (which learns deep representations efficiently), leading to
    significant performance boost. In the case of unsupervised learning, the ‘task’
    for a task-specific component is to describe/characterize the conditional dependencies
    in the BDL model, thereby improving its interpretability and genearalizability.
    This is different from the supervised learning setting where the ‘task’ is simply
    to ‘match the target’.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示BDL如何应用于无监督学习，本节回顾了一些基于BDL的主题模型示例。这些模型结合了PGM（自然地包含变量之间的概率关系）和NN（高效地学习深度表示）的优点，从而显著提升性能。在无监督学习的情况下，任务特定组件的“任务”是描述/表征BDL模型中的条件依赖关系，从而提高其可解释性和泛化能力。这与监督学习中的“任务”不同，在监督学习中，“任务”只是“匹配目标”。
- en: 5.2.1\. Relational Stacked Denoising Autoencoders as Topic Models
  id: totrans-379
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 作为主题模型的关系堆叠去噪自编码器
- en: As a BDL-based topic model, relational stacked denoising autoencoders (RSDAE)
    essentially tries to learn a hierarchy of topics (or latent factors) while enforcing
    relational (graph) constraints under an unsupervised learning setting.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种基于BDL的主题模型，关系堆叠去噪自编码器（RSDAE）本质上试图在无监督学习环境下学习主题（或潜在因素）的层次结构，同时施加关系（图形）约束。
- en: 'Problem Statement and Notation: Assume we have a set of items (articles or
    movies) ${\bf X}_{c}$, with ${\bf X}_{c,j*}^{T}\in{\mathbb{R}}^{B}$ denoting the
    content (attributes) of item $j$. Besides, we use ${\bf I}_{K}$ to denote a $K$-dimensional
    identity matrix and ${\bf S}=[{\bf s}_{1},{\bf s}_{2},\cdots,{\bf s}_{J}]$ to
    denote the *relational latent matrix* with ${\bf s}_{j}$ representing the relational
    properties of item $j$.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 问题陈述与符号：假设我们有一组项目（文章或电影）${\bf X}_{c}$，其中 ${\bf X}_{c,j*}^{T}\in{\mathbb{R}}^{B}$
    表示项目 $j$ 的内容（属性）。此外，我们使用 ${\bf I}_{K}$ 来表示 $K$ 维单位矩阵，${\bf S}=[{\bf s}_{1},{\bf
    s}_{2},\cdots,{\bf s}_{J}]$ 表示 *关系潜在矩阵*，其中 ${\bf s}_{j}$ 代表项目 $j$ 的关系属性。
- en: From the perspective of SDAE, the $J$-by-$B$ matrix ${\bf X}_{c}$ represents
    the clean input to the SDAE and the noise-corrupted matrix of the same size is
    denoted by ${\bf X}_{0}$. Besides, we denote the output of layer $l$ of the SDAE,
    a $J$-by-$K_{l}$ matrix, by ${\bf X}_{l}$. Row $j$ of ${\bf X}_{l}$ is denoted
    by ${\bf X}_{l,j*}$, ${\bf W}_{l}$ and ${\bf b}_{l}$ are the weight matrix and
    bias vector of layer $l$, ${\bf W}_{l,*n}$ denotes column $n$ of ${\bf W}_{l}$,
    and $L$ is the number of layers. As a shorthand, we refer to the collection of
    weight matrices and biases in all layers as ${\bf W}^{+}$. Note that an $L/2$-layer
    SDAE corresponds to an $L$-layer network.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 从 SDAE 的角度来看，$J$-by-$B$ 矩阵 ${\bf X}_{c}$ 代表了 SDAE 的干净输入，而同样大小的噪声污染矩阵表示为 ${\bf
    X}_{0}$。此外，我们将 SDAE 第 $l$ 层的输出，一个 $J$-by-$K_{l}$ 矩阵，表示为 ${\bf X}_{l}$。${\bf X}_{l}$
    的第 $j$ 行表示为 ${\bf X}_{l,j*}$，${\bf W}_{l}$ 和 ${\bf b}_{l}$ 分别是第 $l$ 层的权重矩阵和偏置向量，${\bf
    W}_{l,*n}$ 表示 ${\bf W}_{l}$ 的第 $n$ 列，$L$ 是层数。作为简写，我们将所有层中的权重矩阵和偏置集合称为 ${\bf W}^{+}$。注意，$L/2$
    层的 SDAE 对应于 $L$ 层的网络。
- en: 'Model Formulation: In RSDAE, the perception component takes the form of a probabilistic
    SDAE (introduced in Section [4.3.2](#S4.SS3.SSS2 "4.3.2\. Probabilistic Generalized
    SDAE ‣ 4.3\. Perception Component ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian
    Deep Learning")) as a building block. At a higher level, RSDAE is formulated as
    a novel probabilistic model which seamlessly integrates a hierarchy of latent
    factors and the relational information available. This way, the model can simultaneously
    learn the feature representation from the content information and the relation
    among items ([RSDAE,](#bib.bib118) ). The graphical model for RSDAE is shown in
    Figure [9](#S5.F9 "Figure 9 ‣ 5.2.1\. Relational Stacked Denoising Autoencoders
    as Topic Models ‣ 5.2\. Unsupervised Bayesian Deep Learning for Topic Models ‣
    5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning"),
    and the generative process is listed as follows:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 模型表述：在 RSDAE 中，感知组件采用概率 SDAE（见第 [4.3.2](#S4.SS3.SSS2 "4.3.2\. 概率广义 SDAE ‣ 4.3\.
    感知组件 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述") 节介绍）作为构建块。在更高层次上，RSDAE 被表述为一种新颖的概率模型，它无缝地集成了潜在因子的层次结构和可用的关系信息。这样，模型可以同时从内容信息和项目之间的关系中学习特征表示（[RSDAE](#bib.bib118)）。RSDAE
    的图示模型如图 [9](#S5.F9 "图 9 ‣ 5.2.1\. 关系堆叠去噪自编码器作为主题模型 ‣ 5.2\. 无监督贝叶斯深度学习的主题模型 ‣ 5\.
    具体的 BDL 模型及应用 ‣ 贝叶斯深度学习综述") 所示，生成过程如下：
- en: '| ![Refer to caption](img/260ed05cf98cb20010f5e327706987d7.png) |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| ![参考说明](img/260ed05cf98cb20010f5e327706987d7.png) |'
- en: Figure 9\. Graphical model of RSDAE for $L=4$. $\lambda_{s}$ is omitted here
    to prevent clutter.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. RSDAE 的图示模型，$L=4$。为了避免混乱，省略了 $\lambda_{s}$。
- en: (1)
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Draw the relational latent matrix ${\bf S}$ from a *matrix-variate normal distribution* ([gupta2000matrix,](#bib.bib33)
    ):'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 *矩阵变异正态分布* 中绘制关系潜在矩阵 ${\bf S}$（[gupta2000matrix](#bib.bib33)）：
- en: '| (10) |  | $\displaystyle{\bf S}\sim{\mathcal{N}}_{K,J}(0,{\bf I}_{K}\otimes(\lambda_{l}\mathscr{L}_{a})^{-1}).$
    |  |'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (10) |  | $\displaystyle{\bf S}\sim{\mathcal{N}}_{K,J}(0,{\bf I}_{K}\otimes(\lambda_{l}\mathscr{L}_{a})^{-1})$
    |  |'
- en: (2)
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: For layer $l$ of the SDAE where $l=1,2,\dots,\frac{L}{2}-1$,
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 SDAE 的第 $l$ 层，其中 $l=1,2,\dots,\frac{L}{2}-1$，
- en: (a)
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: For each column $n$ of the weight matrix ${\bf W}_{l}$, draw ${\bf W}_{l,*n}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$.
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于权重矩阵 ${\bf W}_{l}$ 的每一列 $n$，绘制 ${\bf W}_{l,*n}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$。
- en: (b)
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Draw the bias vector ${\bf b}_{l}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf I}_{K_{l}})$.
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘制偏置向量 ${\bf b}_{l}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf I}_{K_{l}})$。
- en: (c)
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: For each row $j$ of ${\bf X}_{l}$, draw ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 ${\bf X}_{l}$ 的每一行 $j$，绘制 ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf X}_{l-1,j*}{\bf
    W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}})$。
- en: (3)
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'For layer $\frac{L}{2}$ of the SDAE, draw the representation vector for item
    $j$ from the product of two Gaussians (PoG) ([DBLP:journals/csl/GalesA06,](#bib.bib23)
    ):'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对于 SDAE 的第 $\frac{L}{2}$ 层，从两个高斯分布的乘积（PoG）中抽取项目 $j$ 的表示向量 ([DBLP:journals/csl/GalesA06](#bib.bib23)
    ):'
- en: '| (11) |  | $\displaystyle{\bf X}_{\frac{L}{2},j*}\sim\mbox{PoG}(\sigma({\bf
    X}_{\frac{L}{2}-1,j*}{\bf W}_{l}+{\bf b}_{l}),{\bf s}_{j}^{T},\lambda_{s}^{-1}{\bf
    I}_{K},\lambda_{r}^{-1}{\bf I}_{K}).$ |  |'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| (11) |  | $\displaystyle{\bf X}_{\frac{L}{2},j*}\sim\mbox{PoG}(\sigma({\bf
    X}_{\frac{L}{2}-1,j*}{\bf W}_{l}+{\bf b}_{l}),{\bf s}_{j}^{T},\lambda_{s}^{-1}{\bf
    I}_{K},\lambda_{r}^{-1}{\bf I}_{K}).$ |  |'
- en: (4)
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: For layer $l$ of the SDAE where $l=\frac{L}{2}+1,\frac{L}{2}+2,\dots,L$,
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 SDAE 的第 $l$ 层，其中 $l=\frac{L}{2}+1,\frac{L}{2}+2,\dots,L$，
- en: (a)
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: For each column $n$ of the weight matrix ${\bf W}_{l}$, draw ${\bf W}_{l,*n}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf
    I}_{K_{l}})$.
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于权重矩阵 ${\bf W}_{l}$ 的每一列 $n$，从 ${\mathcal{N}}(0,\lambda_{w}^{-1}{\bf I}_{K_{l}})$
    中抽样 ${\bf W}_{l,*n}\sim$。
- en: (b)
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: Draw the bias vector ${\bf b}_{l}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf I}_{K_{l}})$.
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 画偏置向量 ${\bf b}_{l}\sim{\mathcal{N}}(0,\lambda_{w}^{-1}{\bf I}_{K_{l}})$。
- en: (c)
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: For each row $j$ of ${\bf X}_{l}$, draw ${\bf X}_{l,j*}\sim{\mathcal{N}}(\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}}).$
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 ${\bf X}_{l}$ 的每一行 $j$，从 ${\mathcal{N}}(\sigma({\bf X}_{l-1,j*}{\bf W}_{l}+{\bf
    b}_{l}),\lambda_{s}^{-1}{\bf I}_{K_{l}})$ 中抽样 ${\bf X}_{l,j*}\sim$。
- en: (5)
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: For each item $j$, draw a clean input ${\bf X}_{c,j*}\sim{\mathcal{N}}({\bf
    X}_{L,j*},\lambda_{n}^{-1}{\bf I}_{B}).$
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于每个项目 $j$，从 ${\mathcal{N}}({\bf X}_{L,j*},\lambda_{n}^{-1}{\bf I}_{B})$ 中抽样一个干净输入
    ${\bf X}_{c,j*}\sim$。
- en: 'Here $K=K_{\frac{L}{2}}$ is the dimensionality of the learned representation
    vector for each item, ${\bf S}$ denotes the $K\times J$ relational latent matrix
    in which column $j$ is the *relational latent vector* ${\bf s}_{j}$ for item $j$.
    Note that ${\mathcal{N}}_{K,J}(0,{\bf I}_{K}\otimes(\lambda_{l}\mathscr{L}_{a})^{-1})$
    in Equation ([10](#S5.E10 "In item 1 ‣ 5.2.1\. Relational Stacked Denoising Autoencoders
    as Topic Models ‣ 5.2\. Unsupervised Bayesian Deep Learning for Topic Models ‣
    5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning"))
    is a matrix-variate normal distribution defined as in ([gupta2000matrix,](#bib.bib33)
    ):'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $K=K_{\frac{L}{2}}$ 是每个项目学习表示向量的维度，${\bf S}$ 表示 $K\times J$ 的关系潜在矩阵，其中第 $j$
    列是项目 $j$ 的*关系潜在向量* ${\bf s}_{j}$。请注意，Equation ([10](#S5.E10 "在项目 1 ‣ 5.2.1\. 关系堆叠去噪自编码器作为主题模型
    ‣ 5.2\. 无监督贝叶斯深度学习用于主题模型 ‣ 5\. 具体BDL模型及应用 ‣ 贝叶斯深度学习调查")) 中的 ${\mathcal{N}}_{K,J}(0,{\bf
    I}_{K}\otimes(\lambda_{l}\mathscr{L}_{a})^{-1})$ 是定义为如下的矩阵变量正态分布：
- en: '| (12) |  | $\displaystyle p({\bf S})={\mathcal{N}}_{K,J}(0,{\bf I}_{K}\otimes(\lambda_{l}\mathscr{L}_{a})^{-1})=\frac{\exp\{\mathrm{tr}[-\frac{\lambda_{l}}{2}{\bf
    S}\mathscr{L}_{a}{\bf S}^{T}]\}}{(2\pi)^{JK/2}&#124;{\bf I}_{K}&#124;^{J/2}&#124;\lambda_{l}\mathscr{L}_{a}&#124;^{-K/2}},$
    |  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\displaystyle p({\bf S})={\mathcal{N}}_{K,J}(0,{\bf I}_{K}\otimes(\lambda_{l}\mathscr{L}_{a})^{-1})=\frac{\exp\{\mathrm{tr}[-\frac{\lambda_{l}}{2}{\bf
    S}\mathscr{L}_{a}{\bf S}^{T}]\}}{(2\pi)^{JK/2}&#124;{\bf I}_{K}&#124;^{J/2}&#124;\lambda_{l}\mathscr{L}_{a}&#124;^{-K/2}},$
    |  |'
- en: where the operator $\otimes$ denotes the Kronecker product of two matrices ([gupta2000matrix,](#bib.bib33)
    ), $\mathrm{tr}(\cdot)$ denotes the trace of a matrix, and $\mathscr{L}_{a}$ is
    the Laplacian matrix incorporating the relational information. $\mathscr{L}_{a}={\bf
    D}-{\bf A}$, where ${\bf D}$ is a diagonal matrix whose diagonal elements ${\bf
    D}_{ii}=\sum_{j}{\bf A}_{ij}$ and ${\bf A}$ is the adjacency matrix representing
    the relational information with binary entries indicating the links (or relations)
    between items. ${\bf A}_{jj^{\prime}}=1$ indicates that there is a link between
    item $j$ and item $j^{\prime}$ and ${\bf A}_{jj^{\prime}}=0$ otherwise. $\mbox{PoG}(\sigma({\bf
    X}_{\frac{L}{2}-1,j*}{\bf W}_{l}+{\bf b}_{l}),{\bf s}_{j}^{T},\lambda_{s}^{-1}{\bf
    I}_{K},\lambda_{r}^{-1}{\bf I}_{K})$ denotes the product of the Gaussian ${\mathcal{N}}(\sigma({\bf
    X}_{\frac{L}{2}-1,j*}{\bf W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K})$ and
    the Gaussian ${\mathcal{N}}({\bf s}_{j}^{T},\lambda_{r}^{-1}{\bf I}_{K})$, which
    is also a Gaussian ([DBLP:journals/csl/GalesA06,](#bib.bib23) ).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 其中运算符$\otimes$表示两个矩阵的克罗内克积（[gupta2000matrix](https://example.org)），$\mathrm{tr}(\cdot)$表示矩阵的迹，$\mathscr{L}_{a}$是包含关系信息的拉普拉斯矩阵。$\mathscr{L}_{a}={\bf
    D}-{\bf A}$，其中${\bf D}$是对角矩阵，其对角元素为${\bf D}_{ii}=\sum_{j}{\bf A}_{ij}$，而${\bf
    A}$是表示关系信息的邻接矩阵，其二进制条目表示项之间的链接（或关系）。${\bf A}_{jj^{\prime}}=1$表示项$j$和项$j^{\prime}$之间有链接，否则${\bf
    A}_{jj^{\prime}}=0$。$\mbox{PoG}(\sigma({\bf X}_{\frac{L}{2}-1,j*}{\bf W}_{l}+{\bf
    b}_{l}),{\bf s}_{j}^{T},\lambda_{s}^{-1}{\bf I}_{K},\lambda_{r}^{-1}{\bf I}_{K})$表示高斯分布${\mathcal{N}}(\sigma({\bf
    X}_{\frac{L}{2}-1,j*}{\bf W}_{l}+{\bf b}_{l}),\lambda_{s}^{-1}{\bf I}_{K})$和高斯分布${\mathcal{N}}({\bf
    s}_{j}^{T},\lambda_{r}^{-1}{\bf I}_{K})$的乘积，这也是一个高斯分布（[DBLP:journals/csl/GalesA06](https://example.org)）。
- en: 'According to the generative process above, maximizing the posterior probability
    is equivalent to maximizing the joint log-likelihood of $\{{\bf X}_{l}\}$, ${\bf
    X}_{c}$, ${\bf S}$, $\{{\bf W}_{l}\}$, and $\{{\bf b}_{l}\}$ given $\lambda_{s}$,
    $\lambda_{w}$, $\lambda_{l}$, $\lambda_{r}$, and $\lambda_{n}$:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述生成过程，最大化后验概率等同于在给定$\lambda_{s}$、$\lambda_{w}$、$\lambda_{l}$、$\lambda_{r}$和$\lambda_{n}$的情况下，最大化$\{{\bf
    X}_{l}\}$、${\bf X}_{c}$、${\bf S}$、$\{{\bf W}_{l}\}$和$\{{\bf b}_{l}\}$的联合对数似然：
- en: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf
    S}\mathscr{L}_{a}{\bf S}^{T})-\frac{\lambda_{r}}{2}\sum\limits_{j}\&#124;({\bf
    s}_{j}^{T}-{\bf X}_{\frac{L}{2},j*})\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf
    W}_{l}\&#124;_{F}^{2}+\&#124;{\bf b}_{l}\&#124;_{2}^{2})$ |  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathscr{L}=$ | $\displaystyle-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf
    S}\mathscr{L}_{a}{\bf S}^{T})-\frac{\lambda_{r}}{2}\sum\limits_{j}\&#124;({\bf
    s}_{j}^{T}-{\bf X}_{\frac{L}{2},j*})\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf
    W}_{l}\&#124;_{F}^{2}+\&#124;{\bf b}_{l}\&#124;_{2}^{2})$ |  |'
- en: '|  |  | $\displaystyle-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;{\bf X}_{L,j*}-{\bf
    X}_{c,j*}\&#124;_{2}^{2}-\frac{\lambda_{s}}{2}\sum\limits_{l}\sum\limits_{j}\&#124;\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})-{\bf X}_{l,j*}\&#124;_{2}^{2}.$ |  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;{\bf X}_{L,j*}-{\bf
    X}_{c,j*}\&#124;_{2}^{2}-\frac{\lambda_{s}}{2}\sum\limits_{l}\sum\limits_{j}\&#124;\sigma({\bf
    X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})-{\bf X}_{l,j*}\&#124;_{2}^{2}.$ |  |'
- en: 'Similar to the pSDAE, taking $\lambda_{s}$ to infinity, the joint log-likelihood
    becomes:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于pSDAE，将$\lambda_{s}$趋于无穷大时，联合对数似然变为：
- en: '| (13) |  | $\displaystyle\mathscr{L}=-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf
    S}\mathscr{L}_{a}{\bf S}^{T})-\frac{\lambda_{r}}{2}\sum\limits_{j}\&#124;({\bf
    s}_{j}^{T}-{\bf X}_{\frac{L}{2},j*})\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf
    W}_{l}\&#124;_{F}^{2}+\&#124;{\bf b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;{\bf
    X}_{L,j*}-{\bf X}_{c,j*}\&#124;_{2}^{2},$ |  |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\displaystyle\mathscr{L}=-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf
    S}\mathscr{L}_{a}{\bf S}^{T})-\frac{\lambda_{r}}{2}\sum\limits_{j}\&#124;({\bf
    s}_{j}^{T}-{\bf X}_{\frac{L}{2},j*})\&#124;_{2}^{2}-\frac{\lambda_{w}}{2}\sum\limits_{l}(\&#124;{\bf
    W}_{l}\&#124;_{F}^{2}+\&#124;{\bf b}_{l}\&#124;_{2}^{2})-\frac{\lambda_{n}}{2}\sum\limits_{j}\&#124;{\bf
    X}_{L,j*}-{\bf X}_{c,j*}\&#124;_{2}^{2},$ |  |'
- en: where ${\bf X}_{l,j*}=\sigma({\bf X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})$. Note
    that the first term $-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf S}\mathscr{L}_{a}{\bf
    S}^{T})$ corresponds to $\log p({\bf S})$ in the matrix-variate distribution in
    Equation ([12](#S5.E12 "In 5.2.1\. Relational Stacked Denoising Autoencoders as
    Topic Models ‣ 5.2\. Unsupervised Bayesian Deep Learning for Topic Models ‣ 5\.
    Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning")).
    Besides, by simple manipulation, we have $\mathrm{tr}({\bf S}\mathscr{L}_{a}{\bf
    S}^{T})=\sum\limits_{k=1}^{K}{\bf S}_{k*}^{T}\mathscr{L}_{a}{\bf S}_{k*}$, where
    ${\bf S}_{k*}$ denotes the $k$-th row of ${\bf S}$. As we can see, maximizing
    $-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf S}^{T}\mathscr{L}_{a}{\bf S})$ is equivalent
    to making ${\bf s}_{j}$ closer to ${\bf s}_{j^{\prime}}$ if item $j$ and item
    $j^{\prime}$ are linked (namely ${\bf A}_{jj^{\prime}}=1$) ([CTRSR,](#bib.bib115)
    ).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf X}_{l,j*}=\sigma({\bf X}_{l-1,j*}{\bf W}_{l}+{\bf b}_{l})$。注意，第一个项
    $-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf S}\mathscr{L}_{a}{\bf S}^{T})$ 对应于方程 ([12](#S5.E12
    "In 5.2.1\. Relational Stacked Denoising Autoencoders as Topic Models ‣ 5.2\.
    Unsupervised Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models
    and Applications ‣ A Survey on Bayesian Deep Learning")) 中的 $\log p({\bf S})$。此外，通过简单的运算，我们有
    $\mathrm{tr}({\bf S}\mathscr{L}_{a}{\bf S}^{T})=\sum\limits_{k=1}^{K}{\bf S}_{k*}^{T}\mathscr{L}_{a}{\bf
    S}_{k*}$，其中 ${\bf S}_{k*}$ 表示 ${\bf S}$ 的第 $k$ 行。如我们所见，最大化 $-\frac{\lambda_{l}}{2}\mathrm{tr}({\bf
    S}^{T}\mathscr{L}_{a}{\bf S})$ 等同于使 ${\bf s}_{j}$ 更接近 ${\bf s}_{j^{\prime}}$，如果项
    $j$ 和项 $j^{\prime}$ 是链接的（即 ${\bf A}_{jj^{\prime}}=1$） ([CTRSR,](#bib.bib115) )。
- en: In RSDAE, the perception variables $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf
    X}_{l}\},{\bf X}_{c},\{{\bf W}_{l}\},\{{\bf b}_{l}\}\}$, the hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    S}\}$, and the task variables $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf
    A}\}$.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RSDAE 中，感知变量 $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf X}_{l}\},{\bf
    X}_{c},\{{\bf W}_{l}\},\{{\bf b}_{l}\}\}$，铰链变量 $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    S}\}$，和任务变量 $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf A}\}$。
- en: 'Learning and Inference: ([RSDAE,](#bib.bib118) ) provides an EM-style algorithm
    for MAP estimation. Below we review some of the key steps.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 学习和推断： ([RSDAE,](#bib.bib118) ) 提供了一个用于 MAP 估计的 EM 风格算法。下面我们回顾一些关键步骤。
- en: 'For the E step, the challenge lies in the inference of the relational latent
    matrix ${\bf S}$. We first fix all rows of ${\bf S}$ except the $k$-th one ${\bf
    S}_{k*}$ and then update ${\bf S}_{k*}$. Specifically, we take the gradient of
    $\mathscr{L}$ with respect to ${\bf S}_{k*}$, set it to 0, and get the following
    linear system:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 E 步骤，挑战在于推断关系潜在矩阵 ${\bf S}$。我们首先固定所有 ${\bf S}$ 的行，除了第 $k$ 行 ${\bf S}_{k*}$，然后更新
    ${\bf S}_{k*}$。具体来说，我们对 ${\bf S}_{k*}$ 求 $\mathscr{L}$ 的梯度，将其设置为 0，并得到以下线性系统：
- en: '| (14) |  | $\displaystyle(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf I}_{J}){\bf
    S}_{k*}=\lambda_{r}{\bf X}_{\frac{L}{2},*k}^{T}.$ |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $\displaystyle(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf I}_{J}){\bf
    S}_{k*}=\lambda_{r}{\bf X}_{\frac{L}{2},*k}^{T}.$ |  |'
- en: 'A naive approach is to solve the linear system by setting ${\bf S}_{k*}=\lambda_{r}(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf
    I}_{J})^{-1}{\bf X}_{\frac{L}{2},*k}^{T}$. Unfortunately, the complexity is $O(J^{3})$
    for one single update. Similar to ([DBLP:conf/ijcai/LiY09,](#bib.bib67) ), the
    steepest descent method ([techreport/Shewchuk94,](#bib.bib101) ) is used to iteratively
    update ${\bf S}_{k*}$:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的方法是通过设置 ${\bf S}_{k*}=\lambda_{r}(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf
    I}_{J})^{-1}{\bf X}_{\frac{L}{2},*k}^{T}$ 来求解线性系统。不幸的是，对于单次更新来说，其复杂度是 $O(J^{3})$。类似于
    ([DBLP:conf/ijcai/LiY09,](#bib.bib67) )，我们使用了最陡下降法 ([techreport/Shewchuk94,](#bib.bib101)
    ) 来迭代更新 ${\bf S}_{k*}$：
- en: '|  | $\displaystyle{\bf S}_{k*}(t+1)\leftarrow{\bf S}_{k*}(t)+\delta(t)r(t),\;\;\;r(t)\leftarrow\lambda_{r}{\bf
    X}_{\frac{L}{2},*k}^{T}-(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf I}_{J}){\bf
    S}_{k*}(t),\;\;\;\delta(t)\leftarrow\frac{r(t)^{T}r(t)}{r(t)^{T}(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf
    I}_{J})r(t)}.$ |  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf S}_{k*}(t+1)\leftarrow{\bf S}_{k*}(t)+\delta(t)r(t),\;\;\;r(t)\leftarrow\lambda_{r}{\bf
    X}_{\frac{L}{2},*k}^{T}-(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf I}_{J}){\bf
    S}_{k*}(t),\;\;\;\delta(t)\leftarrow\frac{r(t)^{T}r(t)}{r(t)^{T}(\lambda_{l}\mathscr{L}_{a}+\lambda_{r}{\bf
    I}_{J})r(t)}.$ |  |'
- en: As discussed in ([DBLP:conf/ijcai/LiY09,](#bib.bib67) ), the steepest descent
    method dramatically reduces the computation cost in each iteration from $O(J^{3})$
    to $O(J)$.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 ([DBLP:conf/ijcai/LiY09,](#bib.bib67) ) 中讨论的，最陡下降法将每次迭代的计算成本从 $O(J^{3})$
    大幅降低到 $O(J)$。
- en: The M step involves learning ${\bf W}_{l}$ and ${\bf b}_{l}$ for each layer
    using the back-propagation algorithm given ${\bf S}$. By alternating the update
    of ${\bf S}$, ${\bf W}_{l}$, and ${\bf b}_{l}$, a local optimum for $\mathscr{L}$
    can be found. Also, techniques such as including a momentum term may help to avoid
    being trapped in a local optimum.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: M 步涉及使用反向传播算法在给定 ${\bf S}$ 的情况下学习 ${\bf W}_{l}$ 和 ${\bf b}_{l}$。通过交替更新 ${\bf
    S}$、${\bf W}_{l}$ 和 ${\bf b}_{l}$，可以找到 $\mathscr{L}$ 的局部最优解。此外，包括动量项等技术可能有助于避免陷入局部最优解。
- en: 5.2.2\. Deep Poisson Factor Analysis with Sigmoid Belief Networks
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 使用 Sigmoid 信念网络的深度泊松因子分析
- en: The Poisson distribution with support over nonnegative integers is known as
    a natural choice to model counts. It is, therefore, desirable to use it as a building
    block for topic models, which are generally interested in word counts ([LDA,](#bib.bib8)
    ). With this motivation, ([PFA,](#bib.bib136) ) proposed a model, dubbed Poisson
    factor analysis (PFA), for latent nonnegative matrix factorization via Poisson
    distributions.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松分布在非负整数上的支持被认为是建模计数的自然选择。因此，使用它作为主题模型的构建块是有利的，因为主题模型通常关注于词汇计数（[LDA,](#bib.bib8)）。基于这个动机，（[PFA,](#bib.bib136)）提出了一种模型，称为泊松因子分析（PFA），用于通过泊松分布进行潜在非负矩阵分解。
- en: 'Poisson Factor Analysis: PFA assumes a discrete $P$-by-$N$ matrix ${\bf X}$
    containing word counts of $N$ documents with a vocabulary size of $P$ ([PFA,](#bib.bib136)
    ; [DPFA,](#bib.bib24) ). In a nutshell, PFA can be described using the equation
    ${\bf X}\sim\mbox{Pois}(\mbox{\boldmath$\Phi$\unboldmath}(\mbox{\boldmath$\Theta$\unboldmath}\circ{\bf
    H}))$, where $\Phi$ (of size $P$-by-$K$ where $K$ is the number of topics) denotes
    the factor loading matrix in factor analysis with the $k$-th column $\mbox{\boldmath$\phi$\unboldmath}_{k}$
    encoding the importance of each word in topic $k$. The $K$-by-$N$ matrix $\Theta$
    is the factor score matrix with the $n$-th column $\mbox{\boldmath$\theta$\unboldmath}_{n}$
    containing topic proportions for document $n$. The $K$-by-$N$ matrix ${\bf H}$
    is a latent binary matrix with the $n$-th column ${\bf h}_{n}$ defining a set
    of topics associated with document $n$.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 泊松因子分析：PFA 假设一个离散的 $P$-by-$N$ 矩阵 ${\bf X}$ 包含 $N$ 个文档的词汇计数，词汇表大小为 $P$（[PFA,](#bib.bib136)
    ; [DPFA,](#bib.bib24)）。简而言之，PFA 可以通过方程 ${\bf X}\sim\mbox{Pois}(\mbox{\boldmath$\Phi$\unboldmath}(\mbox{\boldmath$\Theta$\unboldmath}\circ{\bf
    H}))$ 来描述，其中 $\Phi$（大小为 $P$-by-$K$，$K$ 为主题数量）表示因子分析中的因子载荷矩阵，第 $k$ 列 $\mbox{\boldmath$\phi$\unboldmath}_{k}$
    编码了主题 $k$ 中每个词的重要性。$K$-by-$N$ 矩阵 $\Theta$ 是因子评分矩阵，第 $n$ 列 $\mbox{\boldmath$\theta$\unboldmath}_{n}$
    包含文档 $n$ 的主题比例。$K$-by-$N$ 矩阵 ${\bf H}$ 是一个潜在的二进制矩阵，第 $n$ 列 ${\bf h}_{n}$ 定义了一组与文档
    $n$ 相关联的主题。
- en: Different priors correspond to different models. For example, Dirichlet priors
    on $\mbox{\boldmath$\phi$\unboldmath}_{k}$ and $\mbox{\boldmath$\theta$\unboldmath}_{n}$
    with an all-one matrix ${\bf H}$ would recover LDA ([LDA,](#bib.bib8) ) while
    a beta-Bernoulli prior on ${\bf h}_{n}$ leads to the NB-FTM model in ([DBLP:journals/pami/ZhouC15,](#bib.bib135)
    ). In ([DPFA,](#bib.bib24) ), a deep-structured prior based on sigmoid belief
    networks (SBN) ([neal1992,](#bib.bib79) ) (an MLP variant with binary hidden units)
    is imposed on ${\bf h}_{n}$ to form a deep PFA model for topic modeling.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的先验对应不同的模型。例如，对 $\mbox{\boldmath$\phi$\unboldmath}_{k}$ 和 $\mbox{\boldmath$\theta$\unboldmath}_{n}$
    施加的狄利克雷先验以及全一矩阵 ${\bf H}$ 将恢复 LDA（[LDA,](#bib.bib8)），而对 ${\bf h}_{n}$ 施加的 beta-Bernoulli
    先验会导致 ([DBLP:journals/pami/ZhouC15,](#bib.bib135) ) 中的 NB-FTM 模型。在 ([DPFA,](#bib.bib24)
    ) 中，基于 sigmoid 信念网络（SBN）([neal1992,](#bib.bib79) )（一种具有二进制隐藏单元的 MLP 变体）的深层结构先验被施加在
    ${\bf h}_{n}$ 上，以形成用于主题建模的深度 PFA 模型。
- en: 'Deep Poisson Factor Analysis: In the deep PFA model ([DPFA,](#bib.bib24) ),
    the generative process can be summarized as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 深度泊松因子分析：在深度 PFA 模型中 ([DPFA,](#bib.bib24) )，生成过程可以总结如下：
- en: '|  | $\displaystyle\mbox{\boldmath$\phi$\unboldmath}_{k}$ | $\displaystyle\sim\mbox{Dir}(a_{\phi},\dots,a_{\phi}),\;\;\;\;\theta_{kn}\sim\mbox{Gamma}(r_{k},\frac{p_{n}}{1-p_{n}}),\;\;\;\;r_{k}\sim\mbox{Gamma}(\gamma_{0},\frac{1}{c_{0}}),\;\;\;\;\gamma_{0}\sim\mbox{Gamma}(e_{0},\frac{1}{f_{0}}),$
    |  |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mbox{\boldmath$\phi$\unboldmath}_{k}$ | $\displaystyle\sim\mbox{Dir}(a_{\phi},\dots,a_{\phi}),\;\;\;\;\theta_{kn}\sim\mbox{Gamma}(r_{k},\frac{p_{n}}{1-p_{n}}),\;\;\;\;r_{k}\sim\mbox{Gamma}(\gamma_{0},\frac{1}{c_{0}}),\;\;\;\;\gamma_{0}\sim\mbox{Gamma}(e_{0},\frac{1}{f_{0}}),$
    |  |'
- en: '| (15) |  | $\displaystyle h_{k_{L}n}^{(L)}$ | $\displaystyle\sim\mbox{Ber}(\sigma(b_{k_{L}}^{(L)})),\;\;\;\;h_{k_{l}n}^{(l)}\sim\mbox{Ber}(\sigma({{\bf
    w}_{k_{l}}^{(l)}}^{T}{\bf h}_{n}^{(l+1)}+b_{k_{l}}^{(l)})),\;\;\;\;x_{pnk}\sim\mbox{Pois}(\phi_{pk}\theta_{kn}h_{kn}^{(1)}),\;\;\;\;x_{pn}=\sum\limits_{k=1}^{K}x_{pnk},$
    |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $\displaystyle h_{k_{L}n}^{(L)}$ | $\displaystyle\sim\mbox{Ber}(\sigma(b_{k_{L}}^{(L)})),\;\;\;\;h_{k_{l}n}^{(l)}\sim\mbox{Ber}(\sigma({{\bf
    w}_{k_{l}}^{(l)}}^{T}{\bf h}_{n}^{(l+1)}+b_{k_{l}}^{(l)})),\;\;\;\;x_{pnk}\sim\mbox{Pois}(\phi_{pk}\theta_{kn}h_{kn}^{(1)}),\;\;\;\;x_{pn}=\sum\limits_{k=1}^{K}x_{pnk},$
    |  |'
- en: where $L$ is the number of layers in SBN, which corresponds to Equation ([15](#S5.E15
    "In 5.2.2\. Deep Poisson Factor Analysis with Sigmoid Belief Networks ‣ 5.2\.
    Unsupervised Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models
    and Applications ‣ A Survey on Bayesian Deep Learning")). $x_{pnk}$ is the count
    of word $p$ that comes from topic $k$ in document $n$.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是 SBN 中层数的数量，对应于方程 ([15](#S5.E15 "在 5.2.2\. 带有 Sigmoid 信念网络的深度 Poisson
    因子分析 ‣ 5.2\. 无监督贝叶斯深度学习用于主题模型 ‣ 5\. 具体的 BDL 模型和应用 ‣ 贝叶斯深度学习概述"))。 $x_{pnk}$ 是文档
    $n$ 中来自主题 $k$ 的词 $p$ 的计数。
- en: In this model, the perception variables $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf
    H}^{(l)}\},\{{\bf W}_{l}\},\{{\bf b}_{l}\}\}$, the hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    X}\}$, and the task variables $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{\{\mbox{\boldmath$\phi$\unboldmath}_{k}\},\{r_{k}\},\mbox{\boldmath$\Theta$\unboldmath},\gamma_{0}\}$.
    ${\bf W}_{l}$ is the weight matrix containing columns of ${\bf w}_{k_{l}}^{(l)}$
    and ${\bf b}_{l}$ is the bias vector containing entries of $b_{k_{l}}^{(l)}$ in
    Equation ([15](#S5.E15 "In 5.2.2\. Deep Poisson Factor Analysis with Sigmoid Belief
    Networks ‣ 5.2\. Unsupervised Bayesian Deep Learning for Topic Models ‣ 5\. Concrete
    BDL Models and Applications ‣ A Survey on Bayesian Deep Learning")).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，感知变量 $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{\{{\bf H}^{(l)}\},\{{\bf
    W}_{l}\},\{{\bf b}_{l}\}\}$，铰接变量 $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    X}\}$，以及任务变量 $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{\{\mbox{\boldmath$\phi$\unboldmath}_{k}\},\{r_{k}\},\mbox{\boldmath$\Theta$\unboldmath},\gamma_{0}\}$。
    ${\bf W}_{l}$ 是包含 ${\bf w}_{k_{l}}^{(l)}$ 列的权重矩阵，而 ${\bf b}_{l}$ 是包含方程 ([15](#S5.E15
    "在 5.2.2\. 带有 Sigmoid 信念网络的深度 Poisson 因子分析 ‣ 5.2\. 无监督贝叶斯深度学习用于主题模型 ‣ 5\. 具体的
    BDL 模型和应用 ‣ 贝叶斯深度学习概述")) 中 $b_{k_{l}}^{(l)}$ 条目的偏置向量。
- en: 'Learning Using Bayesian Conditional Density Filtering: Efficient learning algorithms
    are needed for Bayesian treatments of deep PFA. ([DPFA,](#bib.bib24) ) proposed
    to use an online version of MCMC called Bayesian conditional density filtering
    (BCDF) to learn both the global parameters $\mbox{\boldmath$\Psi$\unboldmath}_{g}=(\{\mbox{\boldmath$\phi$\unboldmath}_{k}\},\{r_{k}\},\gamma_{0},\{{\bf
    W}_{l}\},\{{\bf b}_{l}\})$ and the local variables $\mbox{\boldmath$\Psi$\unboldmath}_{l}=(\mbox{\boldmath$\Theta$\unboldmath},\{{\bf
    H}^{(l)}\})$. The key conditional densities used for the Gibbs updates are as
    follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贝叶斯条件密度过滤进行学习：对于深度 PFA 的贝叶斯处理，需要高效的学习算法。 ([DPFA,](#bib.bib24) ) 提出了使用一种称为贝叶斯条件密度过滤（BCDF）的
    MCMC 在线版本来学习全局参数 $\mbox{\boldmath$\Psi$\unboldmath}_{g}=(\{\mbox{\boldmath$\phi$\unboldmath}_{k}\},\{r_{k}\},\gamma_{0},\{{\bf
    W}_{l}\},\{{\bf b}_{l}\})$ 和局部变量 $\mbox{\boldmath$\Psi$\unboldmath}_{l}=(\mbox{\boldmath$\Theta$\unboldmath},\{{\bf
    H}^{(l)}\})$。用于 Gibbs 更新的关键条件密度如下：
- en: '|  | $\displaystyle x_{pnk}&#124;-$ | $\displaystyle\sim\mbox{Multi}(x_{pn};\zeta_{pn1},\dots,\zeta_{pnK}),\;\;\;\;\mbox{\boldmath$\phi$\unboldmath}_{k}&#124;-\sim\mbox{Dir}(a_{\phi}+x_{1\cdot
    k},\dots,a_{\phi}+x_{P\cdot k}),$ |  |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x_{pnk}&#124;-$ | $\displaystyle\sim\mbox{Multi}(x_{pn};\zeta_{pn1},\dots,\zeta_{pnK}),\;\;\;\;\mbox{\boldmath$\phi$\unboldmath}_{k}&#124;-\sim\mbox{Dir}(a_{\phi}+x_{1\cdot
    k},\dots,a_{\phi}+x_{P\cdot k}),$ |  |'
- en: '|  | $\displaystyle\theta_{kn}&#124;-$ | $\displaystyle\sim\mbox{Gamma}(r_{k}h_{kn}^{(1)}+x_{\cdot
    nk},p_{n}),\;\;\;\;h_{kn}^{(1)}&#124;-\sim\delta(x_{\cdot nk}=0)\mbox{Ber}(\frac{\widetilde{\pi}_{kn}}{\widetilde{\pi}_{kn}+(1-\pi_{kn})})+\delta(x_{\cdot
    nk}>0),$ |  |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{kn}&#124;-$ | $\displaystyle\sim\mbox{Gamma}(r_{k}h_{kn}^{(1)}+x_{\cdot
    nk},p_{n}),\;\;\;\;h_{kn}^{(1)}&#124;-\sim\delta(x_{\cdot nk}=0)\mbox{Ber}(\frac{\widetilde{\pi}_{kn}}{\widetilde{\pi}_{kn}+(1-\pi_{kn})})+\delta(x_{\cdot
    nk}>0),$ |  |'
- en: where $\widetilde{\pi}_{kn}=\pi_{kn}(1-p_{n})^{r_{k}}$, $\pi_{kn}=\sigma(({\bf
    w}_{k}^{(1)})^{T}{\bf h}_{n}^{(2)}+c_{k}^{(1)})$, $x_{\cdot nk}=\sum\limits_{p=1}^{P}x_{pnk}$,
    $x_{p\cdot k}=\sum\limits_{n=1}^{N}x_{pnk}$, and $\zeta_{pnk}\propto\phi_{pk}\theta_{kn}$.
    For the learning of $h_{kn}^{(l)}$ where $l>1$, the same techniques as in ([DBLP:conf/aistats/GanHCC15,](#bib.bib25)
    ) can be used.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{\pi}_{kn}=\pi_{kn}(1-p_{n})^{r_{k}}$，$\pi_{kn}=\sigma(({\bf w}_{k}^{(1)})^{T}{\bf
    h}_{n}^{(2)}+c_{k}^{(1)})$，$x_{\cdot nk}=\sum\limits_{p=1}^{P}x_{pnk}$，$x_{p\cdot
    k}=\sum\limits_{n=1}^{N}x_{pnk}$，并且 $\zeta_{pnk}\propto\phi_{pk}\theta_{kn}$。对于
    $h_{kn}^{(l)}$ 的学习，其中 $l>1$，可以使用与 ([DBLP:conf/aistats/GanHCC15,](#bib.bib25) )
    中相同的技术。
- en: 'Learning Using Stochastic Gradient Thermostats: An alternative way of learning
    deep PFA is through *stochastic gradient Nóse-Hoover thermostats* (SGNHT), which
    is more accurate and scalable. SGNHT is a generalization of the *stochastic gradient
    Langevin dynamics* (SGLD) ([SGLD,](#bib.bib127) ) and the *stochastic gradient
    Hamiltonian Monte Carlo* (SGHMC) ([SGHMC,](#bib.bib15) ). Compared with the previous
    two, SGNHT introduces momentum variables into the system, helping the system to
    jump out of local optima. Specifically, the following stochastic differential
    equations (SDE) can be used:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机梯度温控器进行学习：另一种学习深度 PFA 的方法是通过 *随机梯度 Nóse-Hoover 温控器*（SGNHT），这种方法更准确且可扩展。SGNHT
    是 *随机梯度 Langevin 动力学*（SGLD） ([SGLD,](#bib.bib127) ) 和 *随机梯度 Hamiltonian 蒙特卡罗*（SGHMC）
    ([SGHMC,](#bib.bib15) ) 的一种推广。与前两者相比，SGNHT 将动量变量引入系统，有助于系统跳出局部最优。具体地，可以使用以下随机微分方程（SDE）：
- en: '|  | $\displaystyle d\mbox{\boldmath$\Psi$\unboldmath}_{g}={\bf v}dt,\;\;\;\;d{\bf
    v}=\widetilde{f}(\mbox{\boldmath$\Psi$\unboldmath}_{g})dt-\xi{\bf v}dt+\sqrt{D}d\mathcal{W},\;\;\;\;d\xi=(\frac{1}{M}{\bf
    v}^{T}{\bf v}-1)dt,$ |  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d\mbox{\boldmath$\Psi$\unboldmath}_{g}={\bf v}dt,\;\;\;\;d{\bf
    v}=\widetilde{f}(\mbox{\boldmath$\Psi$\unboldmath}_{g})dt-\xi{\bf v}dt+\sqrt{D}d\mathcal{W},\;\;\;\;d\xi=(\frac{1}{M}{\bf
    v}^{T}{\bf v}-1)dt,$ |  |'
- en: 'where $\widetilde{f}(\mbox{\boldmath$\Psi$\unboldmath}_{g})=-\nabla_{\mbox{\boldmath$\Psi$\unboldmath}_{g}}\widetilde{U}(\mbox{\boldmath$\Psi$\unboldmath}_{g})$
    and $\widetilde{U}(\mbox{\boldmath$\Psi$\unboldmath}_{g})$ is the negative log-posterior
    of the model. $t$ indexes time and $\mathcal{W}$ denotes the standard Wiener process.
    $\xi$ is the thermostats variable to make sure the system has a constant temperature.
    $D$ is the injected variance which is a constant. To speed up convergence, the
    SDE is generalized to:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\widetilde{f}(\mbox{\boldmath$\Psi$\unboldmath}_{g})=-\nabla_{\mbox{\boldmath$\Psi$\unboldmath}_{g}}\widetilde{U}(\mbox{\boldmath$\Psi$\unboldmath}_{g})$
    且 $\widetilde{U}(\mbox{\boldmath$\Psi$\unboldmath}_{g})$ 是模型的负对数后验。$t$ 表示时间，$\mathcal{W}$
    表示标准 Wiener 过程。$\xi$ 是温控变量，用于确保系统具有恒定的温度。$D$ 是注入的方差，是一个常数。为了加快收敛速度，SDE 被推广为：
- en: '|  | $\displaystyle d\mbox{\boldmath$\Psi$\unboldmath}_{g}={\bf v}dt,\;\;\;\;d{\bf
    v}=\widetilde{f}(\mbox{\boldmath$\Psi$\unboldmath}_{g})dt-\mbox{\boldmath$\Xi$\unboldmath}{\bf
    v}dt+\sqrt{D}d\mathcal{W},\;\;\;\;d\mbox{\boldmath$\Xi$\unboldmath}=({\bf q}-{\bf
    I})dt,$ |  |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle d\mbox{\boldmath$\Psi$\unboldmath}_{g}={\bf v}dt,\;\;\;\;d{\bf
    v}=\widetilde{f}(\mbox{\boldmath$\Psi$\unboldmath}_{g})dt-\mbox{\boldmath$\Xi$\unboldmath}{\bf
    v}dt+\sqrt{D}d\mathcal{W},\;\;\;\;d\mbox{\boldmath$\Xi$\unboldmath}=({\bf q}-{\bf
    I})dt,$ |  |'
- en: where ${\bf I}$ is the identity matrix, $\mbox{\boldmath$\Xi$\unboldmath}=\mbox{diag}(\xi_{1},\dots,\xi_{M})$,
    ${\bf q}=\mbox{diag}(v_{1}^{2},\dots,v_{M}^{2})$, and $M$ is the dimensionality
    of the parameters.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf I}$ 是单位矩阵，$\mbox{\boldmath$\Xi$\unboldmath}=\mbox{diag}(\xi_{1},\dots,\xi_{M})$，${\bf
    q}=\mbox{diag}(v_{1}^{2},\dots,v_{M}^{2})$，$M$ 是参数的维度。
- en: SGNHT, SGLD, and SGHMC all belong to a larger class of sampling algorithms called
    hybrid Monte Carlo (HMC) ([PRML,](#bib.bib5) ). The idea is to leverage an analogy
    with physical systems to guide transitions of system states. Compared to the Metropolis
    algorithm, HMC can make much larger changes to system states while keeping a small
    rejection probability. For more details, we refer readers to ([PRML,](#bib.bib5)
    ; [neal2011mcmc,](#bib.bib81) ).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: SGNHT、SGLD 和 SGHMC 都属于更大类别的采样算法，称为混合蒙特卡罗（HMC） ([PRML,](#bib.bib5) )。其思想是利用与物理系统的类比来指导系统状态的过渡。与
    Metropolis 算法相比，HMC 可以对系统状态进行更大幅度的变化，同时保持较小的拒绝概率。有关更多详细信息，请参阅 ([PRML,](#bib.bib5)
    ; [neal2011mcmc,](#bib.bib81) )。
- en: 5.2.3\. Deep Poisson Factor Analysis with Restricted Boltzmann Machine
  id: totrans-446
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 限制玻尔兹曼机的深度泊松因子分析
- en: 'The deep PFA model above uses SBN as a perception component. Similarly, one
    can replace SBN with RBM ([RBM,](#bib.bib40) ) (discussed in Section [4.3.1](#S4.SS3.SSS1
    "4.3.1\. Restricted Boltzmann Machine ‣ 4.3\. Perception Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")) to achieve comparable performance.
    With RBM as the perception component, Equation ([15](#S5.E15 "In 5.2.2\. Deep
    Poisson Factor Analysis with Sigmoid Belief Networks ‣ 5.2\. Unsupervised Bayesian
    Deep Learning for Topic Models ‣ 5\. Concrete BDL Models and Applications ‣ A
    Survey on Bayesian Deep Learning")) becomes conditional distributions similar
    to Equation ([4](#S4.E4 "In 4.3.1\. Restricted Boltzmann Machine ‣ 4.3\. Perception
    Component ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning"))
    with the following energy ([RBM,](#bib.bib40) ):'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 上述深度PFA模型使用SBN作为感知组件。同样，可以用RBM ([RBM,](#bib.bib40))（在第[4.3.1](#S4.SS3.SSS1 "4.3.1\.
    Restricted Boltzmann Machine ‣ 4.3\. Perception Component ‣ 4\. Bayesian Deep
    Learning ‣ A Survey on Bayesian Deep Learning")节讨论）替代SBN，以获得类似的性能。以RBM作为感知组件时，方程
    ([15](#S5.E15 "In 5.2.2\. Deep Poisson Factor Analysis with Sigmoid Belief Networks
    ‣ 5.2\. Unsupervised Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL
    Models and Applications ‣ A Survey on Bayesian Deep Learning")) 变为类似于方程 ([4](#S4.E4
    "In 4.3.1\. Restricted Boltzmann Machine ‣ 4.3\. Perception Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")) 的条件分布，其能量为 ([RBM,](#bib.bib40))：
- en: '|  | $\displaystyle E({\bf h}_{n}^{(l)},{\bf h}_{n}^{(l+1)})=-({\bf h}_{n}^{(l)})^{T}{\bf
    c}^{(l)}-({\bf h}_{n}^{(l)})^{T}{\bf W}^{(l)}{\bf h}_{n}^{(l+1)}-({\bf h}_{n}^{(l+1)})^{T}{\bf
    c}^{(l+1)}.$ |  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle E({\bf h}_{n}^{(l)},{\bf h}_{n}^{(l+1)})=-({\bf h}_{n}^{(l)})^{T}{\bf
    c}^{(l)}-({\bf h}_{n}^{(l)})^{T}{\bf W}^{(l)}{\bf h}_{n}^{(l+1)}-({\bf h}_{n}^{(l+1)})^{T}{\bf
    c}^{(l+1)}.$ |  |'
- en: Similar learning algorithms as the deep PFA with SBN can be used. Specifically,
    the sampling process would alternate between $\{\{\mbox{\boldmath$\phi$\unboldmath}_{k}\},\{\gamma_{k}\},\gamma_{0}\}$
    and $\{\{{\bf W}^{(l)}\},\{{\bf c}^{(l)}\}\}$. The former involves similar conditional
    density as the SBN-based DPFA. The latter is RBM’s parameters and can be updated
    using the *contrastive divergence* algorithm.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似于深度PFA与SBN的学习算法。具体而言，采样过程将交替进行 $\{\{\mbox{\boldmath$\phi$\unboldmath}_{k}\},\{\gamma_{k}\},\gamma_{0}\}$
    和 $\{\{{\bf W}^{(l)}\},\{{\bf c}^{(l)}\}\}$。前者涉及与基于SBN的DPFA类似的条件密度。后者是RBM的参数，可以使用*对比散度*算法进行更新。
- en: 5.2.4\. Discussion
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 讨论
- en: Here we choose topic models as an example application to demonstrate how BDL
    can be applied in the unsupervised learning setting. In BDL-based topic models,
    the perception component is responsible for inferring the topic hierarchy from
    documents, while the task-specific component is in charge of modeling the word
    generation, topic generation, word-topic relation, or inter-document relation.
    The synergy between these two components comes from the bidirectional interaction
    between them. On one hand, knowledge on the topic hierarchy facilitates accurate
    modeling of words and topics, providing valuable information for learning inter-document
    relation. On the other hand, accurately modeling the words, topics, and inter-document
    relation can help discover the topic hierarchy and learn compact latent factors
    for documents.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择主题模型作为示例应用，以演示如何在无监督学习环境中应用BDL。在基于BDL的主题模型中，感知组件负责从文档中推断主题层级，而任务特定组件则负责建模单词生成、主题生成、单词-主题关系或文档间关系。这两个组件之间的协同作用来自它们之间的双向交互。一方面，关于主题层级的知识有助于准确建模单词和主题，为学习文档间关系提供有价值的信息。另一方面，准确建模单词、主题和文档间关系有助于发现主题层级并学习文档的紧凑潜在因素。
- en: It is worth noting that the *information exchange* mechanism in some BDL-based
    topic models is different from that in Section [5.1](#S5.SS1 "5.1\. Supervised
    Bayesian Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning"). For example, in the SBN-based DPFA model,
    the exchange is natural since the bottom layer of SBN, ${\bf H}^{(1)}$, and the
    relationship between ${\bf H}^{(1)}$ and $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    X}\}$ are both inherently probabilistic, as shown in Equation ([15](#S5.E15 "In
    5.2.2\. Deep Poisson Factor Analysis with Sigmoid Belief Networks ‣ 5.2\. Unsupervised
    Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning")), which means additional assumptions on
    the distribution are not necessary. The SBN-based DPFA model is equivalent to
    assuming that ${\bf H}$ in PFA is generated from a Dirac delta distribution (a
    Gaussian distribution with zero variance) centered at the bottom layer of the
    SBN, ${\bf H}^{(1)}$. Hence both DPFA models in Table [1](#S3.T1 "Table 1 ‣ 3.1\.
    Models ‣ 3\. Probabilistic Graphical Models ‣ A Survey on Bayesian Deep Learning")
    are ZV models, according to the definition in Section [4.2](#S4.SS2 "4.2\. General
    Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning").
    It is worth noting that RSDAE is an HV model (see Equation ([11](#S5.E11 "In item
    3 ‣ 5.2.1\. Relational Stacked Denoising Autoencoders as Topic Models ‣ 5.2\.
    Unsupervised Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models
    and Applications ‣ A Survey on Bayesian Deep Learning")), where ${\bf S}$ is the
    hinge variable and the others are perception variables), and naively modifying
    this model to be its ZV counterpart would violate the i.i.d. requirement in Section [4.2](#S4.SS2
    "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning").
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一些基于BDL的主题模型中的 *信息交换* 机制与章节 [5.1](#S5.SS1 "5.1\. 监督贝叶斯深度学习在推荐系统中的应用 ‣
    5\. 具体的贝叶斯深度学习模型与应用 ‣ 贝叶斯深度学习综述") 中的机制有所不同。例如，在基于SBN的DPFA模型中，交换是自然的，因为SBN的底层 ${\bf
    H}^{(1)}$ 和 ${\bf H}^{(1)}$ 与 $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    X}\}$ 之间的关系都是内在的概率性的，如方程 ([15](#S5.E15 "在 5.2.2\. 深度泊松因子分析与 sigmoid 信念网络 ‣ 5.2\.
    无监督贝叶斯深度学习在主题模型中的应用 ‣ 5\. 具体的贝叶斯深度学习模型与应用 ‣ 贝叶斯深度学习综述")) 所示，这意味着对分布的额外假设并非必要。基于SBN的DPFA模型等同于假设PFA中的
    ${\bf H}$ 是从一个集中于SBN底层 ${\bf H}^{(1)}$ 的狄拉克δ分布（零方差的高斯分布）生成的。因此，表 [1](#S3.T1 "表
    1 ‣ 3.1\. 模型 ‣ 3\. 概率图模型 ‣ 贝叶斯深度学习综述") 中的两个DPFA模型都是ZV模型，根据章节 [4.2](#S4.SS2 "4.2\.
    一般框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述") 的定义。值得注意的是，RSDAE是一个HV模型（见方程 ([11](#S5.E11 "在条目
    3 ‣ 5.2.1\. 关系堆叠去噪自编码器作为主题模型 ‣ 5.2\. 无监督贝叶斯深度学习在主题模型中的应用 ‣ 5\. 具体的贝叶斯深度学习模型与应用
    ‣ 贝叶斯深度学习综述")），其中 ${\bf S}$ 是铰链变量，而其他的是感知变量），而天真地将这个模型修改为其ZV对应物会违反章节 [4.2](#S4.SS2
    "4.2\. 一般框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习综述") 中的独立同分布要求。
- en: Similar to Section [5.1](#S5.SS1 "5.1\. Supervised Bayesian Deep Learning for
    Recommender Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian
    Deep Learning"), BDL-based topic models above use typical static Bayesian networks
    as task-specific components. Naturally, one can choose to use other forms of task-specific
    components. For example, it is straightforward to replace the relational prior
    of RSDAE in Section [5.2.1](#S5.SS2.SSS1 "5.2.1\. Relational Stacked Denoising
    Autoencoders as Topic Models ‣ 5.2\. Unsupervised Bayesian Deep Learning for Topic
    Models ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep
    Learning") with a stochastic process (e.g., a Wiener process as in ([cDTM,](#bib.bib113)
    )) to model the evolution of the topic hierarchy over time.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于章节 [5.1](#S5.SS1 "5.1\. 监督贝叶斯深度学习在推荐系统中的应用 ‣ 5\. 具体的贝叶斯深度学习模型与应用 ‣ 贝叶斯深度学习综述")，上述基于BDL的主题模型使用典型的静态贝叶斯网络作为任务特定组件。自然地，可以选择使用其他形式的任务特定组件。例如，将章节
    [5.2.1](#S5.SS2.SSS1 "5.2.1\. 关系堆叠去噪自编码器作为主题模型 ‣ 5.2\. 无监督贝叶斯深度学习在主题模型中的应用 ‣ 5\.
    具体的贝叶斯深度学习模型与应用 ‣ 贝叶斯深度学习综述")中RSDAE的关系先验替换为一个随机过程（例如，类似于 ([cDTM,](#bib.bib113)
    ) 的维纳过程）以模拟主题层级随时间的演变是直接的。
- en: 5.3\. Bayesian Deep Representation Learning for Control
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 贝叶斯深度表示学习在控制中的应用
- en: In Section [5.1](#S5.SS1 "5.1\. Supervised Bayesian Deep Learning for Recommender
    Systems ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep
    Learning") and Section [5.2](#S5.SS2 "5.2\. Unsupervised Bayesian Deep Learning
    for Topic Models ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian
    Deep Learning"), we covered how BDL can be applied in the supervised and unsupervised
    learning settings, respectively. In this section, we will discuss how BDL can
    help representation learning in general, using *control* as an example application.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5.1](#S5.SS1 "5.1\. 监督贝叶斯深度学习在推荐系统中的应用 ‣ 5\. 具体的BDL模型和应用 ‣ 贝叶斯深度学习综述")节和第[5.2](#S5.SS2
    "5.2\. 无监督贝叶斯深度学习在主题模型中的应用 ‣ 5\. 具体的BDL模型和应用 ‣ 贝叶斯深度学习综述")节中，我们讨论了BDL在监督和无监督学习中的应用。在本节中，我们将讨论BDL如何帮助一般的表示学习，以*控制*作为示例应用。
- en: As mentioned in Section [1](#S1 "1\. Introduction ‣ A Survey on Bayesian Deep
    Learning"), Bayesian deep learning can also be applied to the control of nonlinear
    dynamical systems from raw images. Consider controlling a complex dynamical system
    according to the live video stream received from a camera. One way of solving
    this control problem is by iteration between two tasks, perception from raw images
    and control based on dynamic models. The perception task can be taken care of
    using multiple layers of simple nonlinear transformation (deep learning) while
    the control task usually needs more sophisticated models like hidden Markov models
    and Kalman filters ([harrison1999bayesian,](#bib.bib35) ; [DBLP:conf/uai/MatsubaraGK14,](#bib.bib74)
    ). To enable an effective iterative process between the perception task and the
    control task, we need two-way information exchange between them. The perception
    component would be the basis on which the control component estimates its states
    and on the other hand, the control component with a dynamic model built in would
    be able to predict the future trajectory (images) by reversing the perception
    process ([watter2015embed,](#bib.bib125) ).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[1](#S1 "1\. 引言 ‣ 贝叶斯深度学习综述")节所述，贝叶斯深度学习也可以应用于从原始图像中控制非线性动态系统。考虑根据从摄像头接收到的实时视频流来控制复杂的动态系统。一种解决这个控制问题的方法是通过两个任务的迭代，即从原始图像中感知和基于动态模型的控制。感知任务可以通过多层简单的非线性变换（深度学习）来处理，而控制任务通常需要更复杂的模型，如隐马尔可夫模型和卡尔曼滤波器（[harrison1999bayesian,](#bib.bib35)；[DBLP:conf/uai/MatsubaraGK14,](#bib.bib74)）。为了实现感知任务和控制任务之间的有效迭代过程，我们需要它们之间的双向信息交换。感知组件将作为控制组件估计其状态的基础，另一方面，内置动态模型的控制组件将能够通过反向感知过程来预测未来的轨迹（图像）（[watter2015embed,](#bib.bib125)）。
- en: As one of the pioneering works in this direction, ([watter2015embed,](#bib.bib125)
    ) posed this task as a representation learning problem and proposed a model called
    *Embed to Control* to take into account the feedback loop mentioned above during
    representation learning. Essentially, the goal is to learn representations that
    (1) capture semantic information from raw images/videos and (2) preserve local
    linearity in the state space for convenient control. This is not possible without
    the BDL framework since the perception component guarantees the first sub-goal
    while the task-specific component guarantees the second. Below we start with some
    preliminaries on stochastic optimal control and then introduce the BDL-based model
    for representation learning.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 作为该方向的开创性工作之一，[（watter2015embed,](#bib.bib125)）将此任务提出为表示学习问题，并提出了一种名为*Embed
    to Control*的模型，以在表示学习过程中考虑上述反馈回路。实质上，目标是学习能够（1）从原始图像/视频中捕捉语义信息和（2）在状态空间中保留局部线性以便于控制的表示。这在没有BDL框架的情况下是不可能的，因为感知组件保证了第一个子目标，而任务特定组件保证了第二个。下面我们将从随机最优控制的一些基本概念开始，然后介绍基于BDL的表示学习模型。
- en: 5.3.1\. Stochastic Optimal Control
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 随机最优控制
- en: 'Following ([watter2015embed,](#bib.bib125) ), we consider the stochastic optimal
    control of an unknown dynamical system as follows:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 根据([watter2015embed,](#bib.bib125) )，我们考虑对未知动态系统的随机最优控制，如下所示：
- en: '| (16) |  | $\displaystyle{\bf z}_{t+1}=f({\bf z}_{t},{\bf u}_{t})+\mbox{\boldmath$\xi$\unboldmath},\
    \mbox{\boldmath$\xi$\unboldmath}\sim{\mathcal{N}}(0,\mbox{\boldmath$\Sigma$\unboldmath}_{\xi}),$
    |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\displaystyle{\bf z}_{t+1}=f({\bf z}_{t},{\bf u}_{t})+\mbox{\boldmath$\xi$\unboldmath},\
    \mbox{\boldmath$\xi$\unboldmath}\sim{\mathcal{N}}(0,\mbox{\boldmath$\Sigma$\unboldmath}_{\xi}),$
    |  |'
- en: 'where $t$ indexes the time steps and ${\bf z}_{t}\in\mathbb{R}^{n_{z}}$ is
    the latent states. ${\bf u}_{t}\in\mathbb{R}^{n_{u}}$ is the applied control at
    time $t$ and $\xi$ denotes the system noise. Equivalently, the equation above
    can be written as $P({\bf z}_{t+1}|{\bf z}_{t},{\bf u}_{t})={\mathcal{N}}({\bf
    z}_{t+1}|f({\bf z}_{t},{\bf u}_{t}),\mbox{\boldmath$\Sigma$\unboldmath}_{\xi})$.
    Hence we need a mapping function to map the corresponding raw image ${\bf x}_{t}$
    (observed input) into the latent space:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\bf z}_{t}=m({\bf x}_{t})+\mbox{\boldmath$\omega$\unboldmath},\
    \mbox{\boldmath$\omega$\unboldmath}\sim{\mathcal{N}}(0,\mbox{\boldmath$\Sigma$\unboldmath}_{\omega}),$
    |  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: 'where $\omega$ is the corresponding system noise. Similarly the equation above
    can be rewritten as ${\bf z}_{t}\sim{\mathcal{N}}(m({\bf x}_{t}),\mbox{\boldmath$\Sigma$\unboldmath}_{\omega})$.
    If the function $f$ is given, finding optimal control for a trajectory of length
    $T$ in a dynamical system amounts to minimizing the following cost:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $\displaystyle J({\bf z}_{1:T},{\bf u}_{1:T})=\mathbb{E}_{{\bf
    z}}(c_{T}({\bf z}_{T},{\bf u}_{T})+\sum\limits_{t_{0}}^{T-1}c({\bf z}_{t},{\bf
    u}_{t})),$ |  |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
- en: 'where $c_{T}({\bf z}_{T},{\bf u}_{T})$ is the terminal cost and $c({\bf z}_{t},{\bf
    u}_{t})$ is the instantaneous cost. ${\bf z}_{1:T}=\{{\bf z}_{1},\dots,{\bf z}_{T}\}$
    and ${\bf u}_{1:T}=\{{\bf u}_{1},\dots,{\bf u}_{T}\}$ are the state and action
    sequences, respectively. For simplicity we can let $c_{T}({\bf z}_{T},{\bf u}_{T})=c({\bf
    z}_{T},{\bf u}_{T})$ and use the following quadratic cost $c({\bf z}_{t},{\bf
    u}_{t})=({\bf z}_{t}-{\bf z}_{goal})^{T}{\bf R}_{z}({\bf z}_{t}-{\bf z}_{goal})+{\bf
    u}_{t}^{T}{\bf R}_{u}{\bf u}_{t},$ where ${\bf R}_{z}\in\mathbb{R}^{n_{z}\times
    n_{z}}$ and ${\bf R}_{u}\in\mathbb{R}^{n_{u}\times n_{u}}$ are the weighting matrices.
    ${\bf z}_{goal}$ is the target latent state that should be inferred from the raw
    images (observed input). Given the function $f$, $\overline{{\bf z}}_{1:T}$ (current
    estimates of the optimal trajectory), and $\overline{{\bf u}}_{1:T}$ (the corresponding
    controls), the dynamical system can be linearized as:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $\displaystyle{\bf z}_{t+1}={\bf A}(\overline{{\bf z}}_{t}){\bf
    z}_{t}+{\bf B}(\overline{{\bf z}}_{t}){\bf u}_{t}+{\bf o}(\overline{{\bf z}}_{t})+\mbox{\boldmath$\omega$\unboldmath},\
    \mbox{\boldmath$\omega$\unboldmath}\sim{\mathcal{N}}(0,\mbox{\boldmath$\Sigma$\unboldmath}_{\omega}),$
    |  |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
- en: where ${\bf A}(\overline{{\bf z}}_{t})=\frac{\partial f(\overline{{\bf z}}_{t},\overline{{\bf
    u}}_{t})}{\partial\overline{{\bf z}}_{t}}$ and ${\bf B}(\overline{{\bf z}}_{t})=\frac{\partial
    f(\overline{{\bf z}}_{t},\overline{{\bf u}}_{t})}{\partial\overline{{\bf u}}_{t}}$
    are local Jacobians. ${\bf o}(\overline{{\bf z}}_{t})$ is the offset.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2\. BDL-Based Representation Learning for Control
  id: totrans-468
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To minimize the function in Equation ([17](#S5.E17 "In 5.3.1\. Stochastic Optimal
    Control ‣ 5.3\. Bayesian Deep Representation Learning for Control ‣ 5\. Concrete
    BDL Models and Applications ‣ A Survey on Bayesian Deep Learning")), we need three
    key components: an encoding model to encode ${\bf x}_{t}$ into ${\bf z}_{t}$,
    a transition model to infer ${\bf z}_{t+1}$ given $({\bf z}_{t},{\bf u}_{t})$,
    and a reconstruction model to reconstruct ${\bf x}_{t+1}$ from the inferred ${\bf
    z}_{t+1}$.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化方程中的函数 ([17](#S5.E17 "在 5.3.1\. 随机最优控制 ‣ 5.3\. 贝叶斯深度表示学习用于控制 ‣ 5\. 具体的
    BDL 模型和应用 ‣ 贝叶斯深度学习综述"))，我们需要三个关键组件：一个编码模型将 ${\bf x}_{t}$ 编码为 ${\bf z}_{t}$，一个过渡模型用于推断
    ${\bf z}_{t+1}$，给定 $({\bf z}_{t},{\bf u}_{t})$，以及一个重建模型用于从推断的 ${\bf z}_{t+1}$
    重建 ${\bf x}_{t+1}$。
- en: 'Encoding Model: An encoding model $Q_{\phi}(Z|X)={\mathcal{N}}(\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{diag}(\mbox{\boldmath$\sigma$\unboldmath}_{t}^{2}))$,
    where the mean $\mbox{\boldmath$\mu$\unboldmath}_{t}\in\mathbb{R}^{n_{z}}$ and
    the diagonal covariance $\mbox{\boldmath$\Sigma$\unboldmath}_{t}=\mbox{diag}(\mbox{\boldmath$\sigma$\unboldmath}_{t}^{2})\in\mathbb{R}^{n_{z}\times
    n_{z}}$, encodes the raw images ${\bf x}_{t}$ into latent states ${\bf z}_{t}$.
    Here,'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 编码模型：一个编码模型 $Q_{\phi}(Z|X)={\mathcal{N}}(\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{diag}(\mbox{\boldmath$\sigma$\unboldmath}_{t}^{2}))$，其中均值
    $\mbox{\boldmath$\mu$\unboldmath}_{t}\in\mathbb{R}^{n_{z}}$ 和对角协方差 $\mbox{\boldmath$\Sigma$\unboldmath}_{t}=\mbox{diag}(\mbox{\boldmath$\sigma$\unboldmath}_{t}^{2})\in\mathbb{R}^{n_{z}\times
    n_{z}}$，将原始图像 ${\bf x}_{t}$ 编码为潜在状态 ${\bf z}_{t}$。
- en: '| (19) |  | $\displaystyle\mbox{\boldmath$\mu$\unboldmath}_{t}$ | $\displaystyle={\bf
    W}_{\mu}h_{\phi}^{\text{enc}}({\bf x}_{t})+{\bf b}_{\mu},\;\;\;\;\log\mbox{\boldmath$\sigma$\unboldmath}_{t}={\bf
    W}_{\sigma}h_{\phi}^{\text{enc}}({\bf x}_{t})+{\bf b}_{\sigma},$ |  |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $\displaystyle\mbox{\boldmath$\mu$\unboldmath}_{t}$ | $\displaystyle={\bf
    W}_{\mu}h_{\phi}^{\text{enc}}({\bf x}_{t})+{\bf b}_{\mu},\;\;\;\;\log\mbox{\boldmath$\sigma$\unboldmath}_{t}={\bf
    W}_{\sigma}h_{\phi}^{\text{enc}}({\bf x}_{t})+{\bf b}_{\sigma},$ |  |'
- en: where $h_{\phi}({\bf x}_{t})^{\text{enc}}$ is the output of the encoding network
    with ${\bf x}_{t}$ as its input.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h_{\phi}({\bf x}_{t})^{\text{enc}}$ 是以 ${\bf x}_{t}$ 为输入的编码网络的输出。
- en: 'Transition Model: A transition model like Equation ([18](#S5.E18 "In 5.3.1\.
    Stochastic Optimal Control ‣ 5.3\. Bayesian Deep Representation Learning for Control
    ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning"))
    infers ${\bf z}_{t+1}$ from $({\bf z}_{t},{\bf u}_{t})$. If we use $\widetilde{Q}_{\psi}(\widetilde{Z}|Z,{\bf
    u})$ to denote the approximate posterior distribution to generate ${\bf z}_{t+1}$,
    the generative process of the full model would be:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 过渡模型：一个类似于方程 ([18](#S5.E18 "在 5.3.1\. 随机最优控制 ‣ 5.3\. 贝叶斯深度表示学习用于控制 ‣ 5\. 具体的
    BDL 模型和应用 ‣ 贝叶斯深度学习综述")) 的过渡模型从 $({\bf z}_{t},{\bf u}_{t})$ 推断 ${\bf z}_{t+1}$。如果我们用
    $\widetilde{Q}_{\psi}(\widetilde{Z}|Z,{\bf u})$ 表示生成 ${\bf z}_{t+1}$ 的近似后验分布，则完整模型的生成过程为：
- en: '| (20) |  | $\displaystyle{\bf z}_{t}$ | $\displaystyle\sim Q_{\phi}(Z&#124;X)={\mathcal{N}}(\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{\boldmath$\Sigma$\unboldmath}_{t}),\;\;\widetilde{{\bf
    z}}_{t+1}\sim\widetilde{Q}_{\psi}(\widetilde{Z}&#124;Z,{\bf u})={\mathcal{N}}({\bf
    A}_{t}\mbox{\boldmath$\mu$\unboldmath}_{t}+{\bf B}_{t}{\bf u}_{t}+{\bf o}_{t},{\bf
    C}_{t}),\;\;\widetilde{{\bf x}}_{t},\widetilde{{\bf x}}_{t+1}\sim P_{\theta}(X&#124;Z)=Bern({\bf
    p}_{t}),$ |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\displaystyle{\bf z}_{t}$ | $\displaystyle\sim Q_{\phi}(Z&#124;X)={\mathcal{N}}(\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{\boldmath$\Sigma$\unboldmath}_{t}),\;\;\widetilde{{\bf
    z}}_{t+1}\sim\widetilde{Q}_{\psi}(\widetilde{Z}&#124;Z,{\bf u})={\mathcal{N}}({\bf
    A}_{t}\mbox{\boldmath$\mu$\unboldmath}_{t}+{\bf B}_{t}{\bf u}_{t}+{\bf o}_{t},{\bf
    C}_{t}),\;\;\widetilde{{\bf x}}_{t},\widetilde{{\bf x}}_{t+1}\sim P_{\theta}(X&#124;Z)=Bern({\bf
    p}_{t}),$ |  |'
- en: 'where the last equation is the reconstruction model to be discussed later,
    ${\bf C}_{t}={\bf A}_{t}\mbox{\boldmath$\Sigma$\unboldmath}_{t}{\bf A}_{t}^{T}+{\bf
    H}_{t}$, and ${\bf H}_{t}$ is the covariance matrix of the estimated system noise
    ($\mbox{\boldmath$\omega$\unboldmath}_{t}\sim{\mathcal{N}}({\bf 0},{\bf H}_{t})$).
    The key here is to learn ${\bf A}_{t}$, ${\bf B}_{t}$ and ${\bf o}_{t}$, which
    are parameterized as follows:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最后一个方程是稍后讨论的重建模型，${\bf C}_{t}={\bf A}_{t}\mbox{\boldmath$\Sigma$\unboldmath}_{t}{\bf
    A}_{t}^{T}+{\bf H}_{t}$，且 ${\bf H}_{t}$ 是估计系统噪声的协方差矩阵 ($\mbox{\boldmath$\omega$\unboldmath}_{t}\sim{\mathcal{N}}({\bf
    0},{\bf H}_{t})$)。关键在于学习 ${\bf A}_{t}$、${\bf B}_{t}$ 和 ${\bf o}_{t}$，它们的参数化形式如下：
- en: '|  | $\displaystyle\text{vec}({\bf A}_{t})={\bf W}_{A}h_{\psi}^{\text{trans}}({\bf
    z}_{t})+{\bf b}_{A},\;\;\;\;\text{vec}({\bf B}_{t})={\bf W}_{B}h_{\psi}^{\text{trans}}({\bf
    z}_{t})+{\bf b}_{B},\;\;\;\;{\bf o}_{t}={\bf W}_{o}h_{\psi}^{\text{trans}}({\bf
    z}_{t})+{\bf b}_{o},$ |  |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{vec}({\bf A}_{t})={\bf W}_{A}h_{\psi}^{\text{trans}}({\bf
    z}_{t})+{\bf b}_{A},\;\;\;\;\text{vec}({\bf B}_{t})={\bf W}_{B}h_{\psi}^{\text{trans}}({\bf
    z}_{t})+{\bf b}_{B},\;\;\;\;{\bf o}_{t}={\bf W}_{o}h_{\psi}^{\text{trans}}({\bf
    z}_{t})+{\bf b}_{o},$ |  |'
- en: where $h_{\psi}^{\text{trans}}({\bf z}_{t})$ is the output of the transition
    network.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h_{\psi}^{\text{trans}}({\bf z}_{t})$是转移网络的输出。
- en: 'Reconstruction Model: As mentioned in the last part of Equation ([20](#S5.E20
    "In 5.3.2\. BDL-Based Representation Learning for Control ‣ 5.3\. Bayesian Deep
    Representation Learning for Control ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning")), the posterior distribution $P_{\theta}(X|Z)$
    reconstructs the raw images ${\bf x}_{t}$ from the latent states ${\bf z}_{t}$.
    The parameters for the Bernoulli distribution ${\bf p}_{t}={\bf W}_{p}h_{\theta}^{\text{dec}}({\bf
    z}_{t})+{\bf b}_{p}$ where $h_{\theta}^{\text{dec}}({\bf z}_{t})$ is the output
    of a third network, called the decoding network or the reconstruction network.
    Putting it all together, Equation ([20](#S5.E20 "In 5.3.2\. BDL-Based Representation
    Learning for Control ‣ 5.3\. Bayesian Deep Representation Learning for Control
    ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian Deep Learning"))
    shows the generative process of the full model.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 重构模型：如方程([20](#S5.E20 "In 5.3.2\. BDL-Based Representation Learning for Control
    ‣ 5.3\. Bayesian Deep Representation Learning for Control ‣ 5\. Concrete BDL Models
    and Applications ‣ A Survey on Bayesian Deep Learning"))最后部分提到的，后验分布$P_{\theta}(X|Z)$从潜在状态${\bf
    z}_{t}$重构原始图像${\bf x}_{t}$。伯努利分布的参数为${\bf p}_{t}={\bf W}_{p}h_{\theta}^{\text{dec}}({\bf
    z}_{t})+{\bf b}_{p}$，其中$h_{\theta}^{\text{dec}}({\bf z}_{t})$是第三个网络的输出，称为解码网络或重构网络。综合来看，方程([20](#S5.E20
    "In 5.3.2\. BDL-Based Representation Learning for Control ‣ 5.3\. Bayesian Deep
    Representation Learning for Control ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning"))展示了完整模型的生成过程。
- en: 5.3.3\. Learning Using Stochastic Gradient Variational Bayes
  id: totrans-479
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 使用随机梯度变分贝叶斯进行学习
- en: 'With $\mathcal{D}=\{({\bf x}_{1},{\bf u}_{1},{\bf x}_{2}),\dots,({\bf x}_{T-1},{\bf
    u}_{T-1},{\bf x}_{T})\}$ as the training set, the loss function is as follows:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在$\mathcal{D}=\{({\bf x}_{1},{\bf u}_{1},{\bf x}_{2}),\dots,({\bf x}_{T-1},{\bf
    u}_{T-1},{\bf x}_{T})\}$作为训练集的情况下，损失函数如下：
- en: '|  | $\displaystyle\mathcal{L}=\sum\limits_{({\bf x}_{t},{\bf u}_{t},{\bf x}_{t+1})\in\mathcal{D}}\mathcal{L}^{\text{bound}}({\bf
    x}_{t},{\bf u}_{t},{\bf x}_{t+1})+\lambda~{}\mbox{KL}(\widetilde{Q}_{\psi}(\widetilde{Z}&#124;\mbox{\boldmath$\mu$\unboldmath}_{t},{\bf
    u}_{t})\&#124;Q_{\phi}(Z&#124;{\bf x}_{t+1})),$ |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}=\sum\limits_{({\bf x}_{t},{\bf u}_{t},{\bf x}_{t+1})\in\mathcal{D}}\mathcal{L}^{\text{bound}}({\bf
    x}_{t},{\bf u}_{t},{\bf x}_{t+1})+\lambda~{}\mbox{KL}(\widetilde{Q}_{\psi}(\widetilde{Z}&#124;\mbox{\boldmath$\mu$\unboldmath}_{t},{\bf
    u}_{t})\&#124;Q_{\phi}(Z&#124;{\bf x}_{t+1})),$ |  |'
- en: 'where the first term is the variational bound on the marginalized log-likelihood
    for each data point:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 其中第一个项是每个数据点的边际对数似然的变分下界：
- en: '|  | $\displaystyle\mathcal{L}^{\text{bound}}({\bf x}_{t},{\bf u}_{t},{\bf
    x}_{t+1})=\mathbb{E}_{\begin{subarray}{c}{\bf z}_{t}\sim Q_{\phi}\\ \widetilde{{\bf
    z}}_{t+1}\sim\widetilde{Q}_{\psi}\end{subarray}}(-\log P_{\theta}({\bf x}_{t}&#124;{\bf
    z}_{t})-\log P_{\theta}({\bf x}_{t+1}&#124;\widetilde{{\bf z}}_{t+1}))+\mbox{KL}(Q_{\phi}\&#124;P(Z)),$
    |  |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}^{\text{bound}}({\bf x}_{t},{\bf u}_{t},{\bf
    x}_{t+1})=\mathbb{E}_{\begin{subarray}{c}{\bf z}_{t}\sim Q_{\phi}\\ \widetilde{{\bf
    z}}_{t+1}\sim\widetilde{Q}_{\psi}\end{subarray}}(-\log P_{\theta}({\bf x}_{t}&#124;{\bf
    z}_{t})-\log P_{\theta}({\bf x}_{t+1}&#124;\widetilde{{\bf z}}_{t+1}))+\mbox{KL}(Q_{\phi}\&#124;P(Z)),$
    |  |'
- en: where $P(Z)$ is the prior distribution for $Z$. With the equations above, stochastic
    gradient variational Bayes can be used to learn the parameters.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P(Z)$是$Z$的先验分布。根据以上方程，可以使用随机梯度变分贝叶斯来学习参数。
- en: According to the generative process in Equation ([20](#S5.E20 "In 5.3.2\. BDL-Based
    Representation Learning for Control ‣ 5.3\. Bayesian Deep Representation Learning
    for Control ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on Bayesian
    Deep Learning")) and the definition in Section [4.2](#S4.SS2 "4.2\. General Framework
    ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep Learning"), the perception
    variables $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{h_{\phi}^{\text{enc}}(\cdot),{\bf
    W}_{p}^{+},{\bf x}_{t},\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{\boldmath$\sigma$\unboldmath}_{t},{\bf
    p}_{t},h_{\theta}^{\text{dec}}(\cdot)\}$, where ${\bf W}_{p}^{+}$ is shorthand
    for $\{{\bf W}_{\mu},{\bf b}_{\mu},{\bf W}_{\sigma},{\bf b}_{\sigma},{\bf W}_{p},{\bf
    b}_{p}\}$. The hinge variables $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf
    z}_{t},{\bf z}_{t+1}\}$ and the task variables $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf
    A}_{t},{\bf B}_{t},{\bf o}_{t},{\bf u}_{t},{\bf C}_{t},\mbox{\boldmath$\omega$\unboldmath}_{t},{\bf
    W}_{t}^{+},h_{\psi}^{\text{trans}}(\cdot)\}$, where ${\bf W}_{t}^{+}$ is shorthand
    for $\{{\bf W}_{A},{\bf b}_{A},{\bf W}_{B},{\bf b}_{B},{\bf W}_{o},{\bf b}_{o}\}$.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 根据方程（[20](#S5.E20 "在 5.3.2\. 基于BDL的控制表示学习 ‣ 5.3\. 贝叶斯深度表示学习 ‣ 5\. 具体BDL模型与应用
    ‣ 贝叶斯深度学习概述")）中的生成过程和第[4.2](#S4.SS2 "4.2\. 通用框架 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习概述")节中的定义，感知变量
    $\mbox{\boldmath$\Omega$\unboldmath}_{p}=\{h_{\phi}^{\text{enc}}(\cdot),{\bf W}_{p}^{+},{\bf
    x}_{t},\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{\boldmath$\sigma$\unboldmath}_{t},{\bf
    p}_{t},h_{\theta}^{\text{dec}}(\cdot)\}$，其中 ${\bf W}_{p}^{+}$ 是 $\{{\bf W}_{\mu},{\bf
    b}_{\mu},{\bf W}_{\sigma},{\bf b}_{\sigma},{\bf W}_{p},{\bf b}_{p}\}$ 的简写。铰链变量
    $\mbox{\boldmath$\Omega$\unboldmath}_{h}=\{{\bf z}_{t},{\bf z}_{t+1}\}$ 和任务变量
    $\mbox{\boldmath$\Omega$\unboldmath}_{t}=\{{\bf A}_{t},{\bf B}_{t},{\bf o}_{t},{\bf
    u}_{t},{\bf C}_{t},\mbox{\boldmath$\omega$\unboldmath}_{t},{\bf W}_{t}^{+},h_{\psi}^{\text{trans}}(\cdot)\}$，其中
    ${\bf W}_{t}^{+}$ 是 $\{{\bf W}_{A},{\bf b}_{A},{\bf W}_{B},{\bf b}_{B},{\bf W}_{o},{\bf
    b}_{o}\}$ 的简写。
- en: 5.3.4\. Discussion
  id: totrans-486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.4\. 讨论
- en: The example model above demonstrates BDL’s capability of learning representations
    that satisfy domain-specific requirements. In the case of *control*, we are interested
    in learning representations that can capture semantic information from raw input
    and preserve local linearity in the space of system states.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例模型展示了BDL学习满足特定领域需求的能力。在*控制*的情况下，我们感兴趣的是学习能够从原始输入中捕捉语义信息并在系统状态空间中保持局部线性的表示。
- en: To achieve this goal, the BDL-based model consists of two components, a perception
    component to *see* the live video and a control (task-specific) component to *infer*
    the states of the dynamical system. Inference of the system is based on the mapped
    states and the confidence of mapping from the perception component, and in turn,
    the control signals sent by the control component would affect the live video
    received by the perception component. Only when the two components work interactively
    within a unified probabilistic framework can the model reach its full potential
    and achieve the best control performance.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 为实现这一目标，基于BDL的模型包含两个组件，一个是感知组件，用于*观察*实时视频，另一个是控制（任务特定）组件，用于*推断*动态系统的状态。系统的推断基于感知组件映射的状态和映射的置信度，而控制组件发出的控制信号会影响感知组件接收到的实时视频。只有当两个组件在统一的概率框架下交互工作时，模型才能发挥其最大潜力并实现最佳控制性能。
- en: 'Note that the BDL-based control model discussed above uses a different *information
    exchange* mechanism from that in Section [5.1](#S5.SS1 "5.1\. Supervised Bayesian
    Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning") and Section [5.2](#S5.SS2 "5.2\. Unsupervised
    Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning"): it follows the VAE mechanism and uses
    neural networks to *separately* parameterize the mean and covariance of hinge
    variables (e.g., in the encoding model, the hinge variable ${\bf z}_{t}\sim{\mathcal{N}}(\mbox{\boldmath$\mu$\unboldmath}_{t},\mbox{diag}(\mbox{\boldmath$\sigma$\unboldmath}_{t}^{2}))$,
    where $\mbox{\boldmath$\mu$\unboldmath}_{t}$ and $\mbox{\boldmath$\sigma$\unboldmath}_{t}$
    are perception variables parameterized as in Equation ([19](#S5.E19 "In 5.3.2\.
    BDL-Based Representation Learning for Control ‣ 5.3\. Bayesian Deep Representation
    Learning for Control ‣ 5\. Concrete BDL Models and Applications ‣ A Survey on
    Bayesian Deep Learning"))), which is more flexible (with more free parameters)
    than models like CDL and CDR in Section [5.1](#S5.SS1 "5.1\. Supervised Bayesian
    Deep Learning for Recommender Systems ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning"), where Gaussian distributions with fixed
    variance are also used. Note that this BDL-based control model is an LV model
    as shown in Table [1](#S3.T1 "Table 1 ‣ 3.1\. Models ‣ 3\. Probabilistic Graphical
    Models ‣ A Survey on Bayesian Deep Learning"), and since the covariance is assumed
    to be diagonal, the model still meets the independence requirement in Section
    [4.2](#S4.SS2 "4.2\. General Framework ‣ 4\. Bayesian Deep Learning ‣ A Survey
    on Bayesian Deep Learning").'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Bayesian Deep Learning for Other Applications
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BDL has found wide applications such as recommender systems, topic models, and
    control in supervised learning, unsupervised learning, and representation learning
    in general. In this section, we briefly discuss a few more applications that could
    benefit from BDL.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1\. Link Prediction
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Link prediction has long been a core problem in network analysis and is recently
    attracting more interest with the new advancements brought by BDL and deep neural
    networks in general. ([RDL,](#bib.bib120) ) proposed the first BDL-based model,
    dubbed relational deep learning (RDL), for link prediction. Graphite ([Graphite,](#bib.bib32)
    ) extends RDL using a perception component based on graph convolutional networks
    (GCN) ([GCN,](#bib.bib59) ). ([SBGNN,](#bib.bib75) ) combines the classic stochastic
    blockmodel ([SBM,](#bib.bib82) ) (as a task-specific component) and GCN-based
    perception component to jointly model latent community structures and link generation
    in a graph with reported state-of-the-art performance in link prediction.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2\. Natural Language Processing
  id: totrans-494
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides topic modeling as discussed in Section [5.2](#S5.SS2 "5.2\. Unsupervised
    Bayesian Deep Learning for Topic Models ‣ 5\. Concrete BDL Models and Applications
    ‣ A Survey on Bayesian Deep Learning"), BDL is also useful for natural language
    processing in general. For example, ([S2BS,](#bib.bib77) ) and ([QuaSE,](#bib.bib69)
    ) build on top of the BDL principles to define a language revision process. These
    models typically involve RNN-based perception components and relatively simple
    task-specific components linking the input and output sequences.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在[5.2](#S5.SS2 "5.2\. 无监督贝叶斯深度学习用于主题模型 ‣ 5\. 具体的BDL模型及应用 ‣ 贝叶斯深度学习概述")中讨论的主题建模，BDL在自然语言处理方面也非常有用。例如，（[S2BS,](#bib.bib77)）和（[QuaSE,](#bib.bib69)）在BDL原则的基础上定义了一种语言修订过程。这些模型通常涉及基于RNN的感知组件和相对简单的任务特定组件，用于连接输入和输出序列。
- en: 5.4.3\. Computer Vision
  id: totrans-496
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3\. 计算机视觉
- en: BDL is particularly powerful for computer vision in the unsupervised learning
    setting. This is because in the BDL framework, one can clearly define a generative
    process of how objects in a scene are generated from various factors such as counts,
    positions, and the content ([AIR,](#bib.bib20) ). The perception component, usually
    taking the form of a probabilistic neural network, can focus on modeling the raw
    images’ visual features, while the task-specific component handles the conditional
    dependencies among objects’ various attributes in the images. One notable work
    in this direction is *Attend, Infer, Repeat* (AIR) ([AIR,](#bib.bib20) ), where
    the task-specific component involves latent variables on each object’ position,
    scale, appearance, and presence (which is related to counting of objects). Following
    AIR, variants such as Fast AIR ([FastAIR,](#bib.bib105) ) and Sequential AIR ([SQAIR,](#bib.bib60)
    ) are proposed to improve its computational efficiency and performance. Besides
    unsupervised learning, BDL can also be useful for supervised learning tasks such
    as action recognition in videos ([ATF,](#bib.bib102) ), where conditional dependencies
    among different actions are modeled using a task-specific component.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习环境中，BDL在计算机视觉领域特别强大。这是因为在BDL框架中，可以清楚地定义场景中对象如何从各种因素（如计数、位置和内容）生成的生成过程（[AIR,](#bib.bib20)）。感知组件通常采用概率神经网络的形式，专注于建模原始图像的视觉特征，而任务特定组件则处理图像中对象各种属性之间的条件依赖关系。在这一方向上的一项显著工作是*Attend,
    Infer, Repeat*（AIR）（[AIR,](#bib.bib20)），其中任务特定组件涉及每个对象的位置、尺度、外观和存在（与对象计数相关）的潜在变量。继AIR之后，提出了Fast
    AIR（[FastAIR,](#bib.bib105)）和Sequential AIR（[SQAIR,](#bib.bib60)）等变体，以提高其计算效率和性能。除了无监督学习，BDL还可以用于有监督学习任务，例如视频中的动作识别（[ATF,](#bib.bib102)），在这种任务中，使用任务特定组件对不同动作之间的条件依赖关系进行建模。
- en: 5.4.4\. Speech
  id: totrans-498
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.4\. 语音
- en: In the field of speech recognition and synthesis, researchers have also been
    adopting the BDL framework to improve both accuracy and interpretability. For
    example, factorized hierarchical VAE ([FHVAE,](#bib.bib48) ; [SFHVAE,](#bib.bib47)
    ) composes VAE with a factorized latent variable model (represented as a PGM)
    to learn different latent factors in speech data following an unsupervised setting.
    Similarly, Gaussian mixture VAE ([GMVAE,](#bib.bib49) ) uses a Gaussian mixture
    model as the task-specific component to achieve controllable speech synthesis
    from text. In terms of speech recognition, recurrent Poisson process units (RPPU) ([RPPU,](#bib.bib51)
    ) instead adopt a different form of task-specific component; they use a stochastic
    process (i.e., a Poisson process) as the task-specific component to model boundaries
    between phonemes and successfully achieve a significantly lower word error rate
    (WER) for speech recognition. Similarly, deep graph random process (DGP) ([DGP,](#bib.bib52)
    ) as another stochastic process operates on graphs to model the relational structure
    among utterances, further improving performance in speech recognition.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音识别和合成领域，研究人员也开始采用BDL框架来提高准确性和可解释性。例如，分解的层次VAE（[FHVAE,](#bib.bib48)；[SFHVAE,](#bib.bib47)）将VAE与分解的潜在变量模型（表示为PGM）结合，以在无监督设置下学习语音数据中的不同潜在因素。类似地，高斯混合VAE（[GMVAE,](#bib.bib49)）使用高斯混合模型作为任务特定组件，实现从文本到可控的语音合成。在语音识别方面，递归泊松过程单元（RPPU）（[RPPU,](#bib.bib51)）则采用了不同形式的任务特定组件；它们使用随机过程（即泊松过程）作为任务特定组件来建模音素之间的边界，并成功地实现了显著降低的词错误率（WER）。类似地，深度图随机过程（DGP）（[DGP,](#bib.bib52)）作为另一种随机过程在图上操作，以建模话语之间的关系结构，进一步提高了语音识别的性能。
- en: 5.4.5\. Time Series Forecasting
  id: totrans-500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.5\. 时间序列预测
- en: Time series forecasting is a long-standing core problem in economics, statistics,
    and machine learning ([harvey1990forecasting,](#bib.bib36) ). It has wide applications
    across multiple areas. For example, accurate forecasts of regional energy consumption
    can provide valuable guidance to optimize energy generation and allocation. In
    e-commerce, retails rely on demand forecasts to decide when and where to replenish
    their supplies, thereby avoiding items going out of stock and guaranteeing fastest
    deliveries for customers. During a pandemic such as COVID-19, it is crucial to
    obtain reasonable forecasts on hospital workload and medical supply demand in
    order to best allocate resources across the country. Needless to say, an ideal
    forecasting model requires both efficient processing of high-dimensional data
    and sophisticated modeling of different random variables, either observed or latent.
    BDL-based forecasting models ([DeepAR,](#bib.bib21) ; [SQF-RNN,](#bib.bib27) ;
    [DeepState,](#bib.bib90) ; [DeepFactor,](#bib.bib124) ) achieve these with an
    RNN-based perception component and a task-specific component handling the conditional
    dependencies among different variables, showing substantial improvement over previous
    non-BDL forecasting models.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测是经济学、统计学和机器学习中的一个长期核心问题（[harvey1990forecasting,](#bib.bib36)）。它在多个领域有广泛应用。例如，准确预测区域能源消耗可以为优化能源生成和分配提供宝贵的指导。在电子商务中，零售商依赖需求预测来决定何时何地补充库存，从而避免物品缺货并确保向客户提供最快的配送。在COVID-19这样的疫情期间，获得合理的医院工作负荷和医疗供应需求预测对于在全国范围内最佳分配资源至关重要。不用说，一个理想的预测模型需要高效处理高维数据和对不同随机变量（无论是观察到的还是潜在的）进行复杂建模。基于BDL的预测模型（[DeepAR,](#bib.bib21)；[SQF-RNN,](#bib.bib27)；[DeepState,](#bib.bib90)；[DeepFactor,](#bib.bib124)）通过RNN基础的感知组件和处理不同变量间条件依赖的任务特定组件实现了这些，相较于之前的非BDL预测模型显示出显著的改进。
- en: 5.4.6\. Health Care
  id: totrans-502
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.6\. 医疗保健
- en: 'In health-care-related applications ([ravi2016deep,](#bib.bib91) ), it is often
    desirable to incorporate human knowledge into models, either to boost performance
    or more importantly to improve interpretability. It is also crutial to ensure
    models’ robustness when they are used on under-represented data. BDL therefore
    provides a unified framework to meet all these requirements: (1) with its Bayesian
    nature, it can impose proper priors and perform Bayesian model averaging to improve
    robustness; (2) its task-specific component can naturally represents and incorporate
    human knowledge if necessary; (3) the model’s joint training provides interpretability
    for its both components. For example, ([DPFM,](#bib.bib38) ) proposed a deep Poisson
    factor model, which essentially stacks layers of Poisson factor models, to analyze
    electronic health records. ([BBFDR,](#bib.bib110) ) built a BDL model with experiment-specific
    priors (knowledge) to control the false discovery rate during study analysis with
    applications to cancer drug screening. ([DeepMarkov,](#bib.bib61) ) developed
    deep nonlinear state space models and demonstrated their effectiveness in processing
    electronic health records and performing counterfactual reasoning. Task-specific
    components in the BDL models above all take the form of a typical Bayesian network
    (as mentioned in Section [4.4.1](#S4.SS4.SSS1 "4.4.1\. Bayesian Networks ‣ 4.4\.
    Task-Specific Component ‣ 4\. Bayesian Deep Learning ‣ A Survey on Bayesian Deep
    Learning")). In contrast, ([BIN,](#bib.bib117) ) proposed to use bidirectional
    inference networks, which are essentially a class of deep Bayesian network, as
    the task-specific component (as mentioned in Section [4.4.2](#S4.SS4.SSS2 "4.4.2\.
    Bidirectional Inference Networks ‣ 4.4\. Task-Specific Component ‣ 4\. Bayesian
    Deep Learning ‣ A Survey on Bayesian Deep Learning")). This enables deep nonlinear
    structures in each conditional distribution of the Bayesian network and improves
    performance for applications such as health profiling.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在与健康护理相关的应用中 ([ravi2016deep,](#bib.bib91) )，通常希望将人类知识融入模型中，既可以提升性能，也更重要的是提高可解释性。确保模型在处理代表性不足的数据时的鲁棒性同样至关重要。因此，BDL
    提供了一个统一的框架来满足这些要求：（1）凭借其贝叶斯性质，它可以施加适当的先验并执行贝叶斯模型平均，以提高鲁棒性；（2）其任务特定组件可以自然地表示和融入人类知识（如有必要）；（3）模型的联合训练为其两个组件提供了解释性。例如，([DPFM,](#bib.bib38)
    ) 提出了一个深度泊松因子模型，该模型本质上堆叠了泊松因子模型的层，用于分析电子健康记录。 ([BBFDR,](#bib.bib110) ) 构建了一个具有实验特定先验（知识）的
    BDL 模型，以控制研究分析中的假发现率，应用于癌症药物筛选。 ([DeepMarkov,](#bib.bib61) ) 开发了深度非线性状态空间模型，并展示了其在处理电子健康记录和进行反事实推理中的有效性。上述
    BDL 模型中的任务特定组件均呈现为典型的贝叶斯网络（如第 [4.4.1](#S4.SS4.SSS1 "4.4.1\. 贝叶斯网络 ‣ 4.4\. 任务特定组件
    ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习调查") 节所述）。相比之下，([BIN,](#bib.bib117) ) 提议使用双向推理网络，该网络本质上是一类深度贝叶斯网络，作为任务特定组件（如第
    [4.4.2](#S4.SS4.SSS2 "4.4.2\. 双向推理网络 ‣ 4.4\. 任务特定组件 ‣ 4\. 贝叶斯深度学习 ‣ 贝叶斯深度学习调查")
    节所述）。这使得贝叶斯网络中每个条件分布的深度非线性结构成为可能，并提升了健康档案等应用的性能。
- en: 6\. Conclusions and Future Research
  id: totrans-504
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论与未来研究
- en: BDL strives to combine the merits of PGM and NN by organically integrating them
    in a single principled probabilistic framework. In this survey, we identified
    such a current trend and reviewed recent work. A BDL model consists of a perception
    component and a task-specific component; we therefore surveyed different instantiations
    of both components developed over the past few years respectively and discussed
    different variants in detail. To learn parameters in BDL, several types of algorithms
    have been proposed, ranging from block coordinate descent, Bayesian conditional
    density filtering, and stochastic gradient thermostats to stochastic gradient
    variational Bayes.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: BDL 努力通过在单一的有原则的概率框架中有机地整合 PGM 和 NN 的优点。在本次调查中，我们识别了这一当前趋势，并回顾了近期的工作。一个 BDL
    模型由感知组件和任务特定组件组成；因此，我们分别调查了过去几年中开发的这两个组件的不同实例，并详细讨论了不同的变体。为了学习 BDL 中的参数，提出了几种类型的算法，包括块坐标下降、贝叶斯条件密度过滤、随机梯度温控器和随机梯度变分贝叶斯。
- en: BDL draws inspiration and gain popularity both from the success of PGM and from
    recent promising advances on deep learning. Since many real-world tasks involve
    both efficient perception from high-dimensional signals (e.g., images and videos)
    and probabilistic inference on random variables, BDL emerges as a natural choice
    to harness the perception ability from NN and the (conditional and causal) inference
    ability from PGM. Over the past few years, BDL has found successful applications
    in various areas such as recommender systems, topic models, stochastic optimal
    control, computer vision, natural language processing, health care, etc. In the
    future, we can expect both more in-depth studies on existing applications and
    exploration on even more complex tasks. Besides, recent progress on efficient
    BNN (as the perception component of BDL) also lays down the foundation for further
    improving BDL’s scalability.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: BDL 从 PGM 的成功和最近在深度学习上的有前景的进展中汲取灵感并获得了人气。由于许多现实世界任务涉及从高维信号（如图像和视频）中进行高效感知和对随机变量进行概率推断，BDL
    成为利用 NN 的感知能力和 PGM 的（条件和因果）推断能力的自然选择。在过去几年中，BDL 在推荐系统、主题模型、随机最优控制、计算机视觉、自然语言处理、健康护理等多个领域找到了成功的应用。未来，我们可以期待对现有应用的更深入研究以及对更复杂任务的探索。此外，最近在高效
    BNN（作为 BDL 的感知组件）方面的进展也为进一步提高 BDL 的可扩展性奠定了基础。
- en: References
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Gediminas Adomavicius and YoungOk Kwon. Improving aggregate recommendation
    diversity using ranking-based techniques. TKDE, 24(5):896–911, 2012.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Gediminas Adomavicius 和 YoungOk Kwon。使用基于排名的技术提高聚合推荐多样性。TKDE，24(5):896–911，2012
    年。
- en: (2) Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy, and Max Welling.
    Bayesian dark knowledge. In NIPS, pages 3420–3428, 2015.
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Anoop Korattikara Balan、Vivek Rathod、Kevin P Murphy 和 Max Welling。贝叶斯暗知识。在
    NIPS，页码 3420–3428，2015 年。
- en: (3) Ilaria Bartolini, Zhenjie Zhang, and Dimitris Papadias. Collaborative filtering
    with personalized skylines. TKDE, 23(2):190–203, 2011.
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Ilaria Bartolini、Zhenjie Zhang 和 Dimitris Papadias。具有个性化天际线的协同过滤。TKDE，23(2):190–203，2011
    年。
- en: (4) Yoshua Bengio, Li Yao, Guillaume Alain, and Pascal Vincent. Generalized
    denoising auto-encoders as generative models. In NIPS, pages 899–907, 2013.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Yoshua Bengio、Li Yao、Guillaume Alain 和 Pascal Vincent。作为生成模型的广义去噪自编码器。在
    NIPS，页码 899–907，2013 年。
- en: (5) Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag
    New York, Inc., Secaucus, NJ, USA, 2006.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Christopher M. Bishop。模式识别与机器学习。Springer-Verlag New York, Inc.，Secaucus,
    NJ, USA，2006 年。
- en: (6) David Blei and John Lafferty. Correlated topic models. NIPS, 18:147, 2006.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) David Blei 和 John Lafferty。相关主题模型。NIPS，18:147，2006 年。
- en: (7) David M Blei and John D Lafferty. Dynamic topic models. In ICML, pages 113–120\.
    ACM, 2006.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) David M Blei 和 John D Lafferty。动态主题模型。在 ICML，页码 113–120。ACM，2006 年。
- en: (8) David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation.
    JMLR, 3:993–1022, 2003.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) David M Blei、Andrew Y Ng 和 Michael I Jordan。潜在狄利克雷分配。JMLR，3:993–1022，2003
    年。
- en: (9) Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
    Weight uncertainty in neural network. In ICML, pages 1613–1622, 2015.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Charles Blundell、Julien Cornebise、Koray Kavukcuoglu 和 Daan Wierstra。神经网络中的权重不确定性。在
    ICML，页码 1613–1622，2015 年。
- en: (10) Hervé Bourlard and Yves Kamp. Auto-association by multilayer perceptrons
    and singular value decomposition. Biological cybernetics, 59(4-5):291–294, 1988.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Hervé Bourlard 和 Yves Kamp。通过多层感知器和奇异值分解的自关联。生物物理学，59(4-5):291–294，1988
    年。
- en: (11) Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted
    autoencoders. In ICLR, 2016.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Yuri Burda、Roger B. Grosse 和 Ruslan Salakhutdinov。重要性加权自编码器。在 ICLR，2016
    年。
- en: (12) Yi Cai, Ho-fung Leung, Qing Li, Huaqing Min, Jie Tang, and Juanzi Li. Typicality-based
    collaborative filtering recommendation. TKDE, 26(3):766–779, 2014.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Yi Cai、Ho-fung Leung、Qing Li、Huaqing Min、Jie Tang 和 Juanzi Li。基于典型性的协同过滤推荐。TKDE，26(3):766–779，2014
    年。
- en: (13) Minmin Chen, Kilian Q Weinberger, Fei Sha, and Yoshua Bengio. Marginalized
    denoising auto-encoders for nonlinear representations. In ICML, pages 1476–1484,
    2014.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) 陈敏敏、Kilian Q Weinberger、Fei Sha 和 Yoshua Bengio。用于非线性表示的边缘去噪自编码器。在 ICML，页码
    1476–1484，2014 年。
- en: (14) Minmin Chen, Zhixiang Eddie Xu, Kilian Q. Weinberger, and Fei Sha. Marginalized
    denoising autoencoders for domain adaptation. In ICML, 2012.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) 陈敏敏、Zhixiang Eddie Xu、Kilian Q. Weinberger 和 Fei Sha。用于领域适应的边缘去噪自编码器。在
    ICML，2012 年。
- en: (15) Tianqi Chen, Emily B. Fox, and Carlos Guestrin. Stochastic gradient Hamiltonian
    Monte Carlo. In ICML, pages 1683–1691, 2014.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Tianqi Chen、Emily B. Fox 和 Carlos Guestrin。随机梯度哈密顿蒙特卡罗。在 ICML，页码 1683–1691，2014
    年。
- en: (16) Kyunghyun Cho, Bart van Merrienboer, cCaglar Gülccehre, Dzmitry Bahdanau,
    Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using RNN encoder-decoder for statistical machine translation. In EMNLP, pages
    1724–1734, 2014.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Kyunghyun Cho、Bart van Merrienboer、Caglar Gülccehre、Dzmitry Bahdanau、Fethi
    Bougares、Holger Schwenk 和 Yoshua Bengio. 使用 RNN 编码器-解码器学习短语表示，用于统计机器翻译。发表于 EMNLP，页码
    1724–1734，2014。
- en: (17) Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C. Courville,
    and Yoshua Bengio. A recurrent latent variable model for sequential data. In NIPS,
    pages 2980–2988, 2015.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Junyoung Chung、Kyle Kastner、Laurent Dinh、Kratarth Goel、Aaron C. Courville
    和 Yoshua Bengio. 一种用于序列数据的递归潜变量模型。发表于 NIPS，页码 2980–2988，2015。
- en: (18) Yulai Cong, Bo Chen, Hongwei Liu, and Mingyuan Zhou. Deep latent dirichlet
    allocation with topic-layer-adaptive stochastic gradient riemannian MCMC. In ICML,
    pages 864–873, 2017.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Yulai Cong、Bo Chen、Hongwei Liu 和 Mingyuan Zhou. 使用主题层自适应随机梯度黎曼 MCMC 的深度潜在狄利克雷分配。发表于
    ICML，页码 864–873，2017。
- en: (19) Andreas Doerr, Christian Daniel, Martin Schiegg, Duy Nguyen-Tuong, Stefan
    Schaal, Marc Toussaint, and Sebastian Trimpe. Probabilistic recurrent state-space
    models. In ICML, pages 1279–1288, 2018.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Andreas Doerr、Christian Daniel、Martin Schiegg、Duy Nguyen-Tuong、Stefan Schaal、Marc
    Toussaint 和 Sebastian Trimpe. 概率递归状态空间模型。发表于 ICML，页码 1279–1288，2018。
- en: '(20) S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari,
    Koray Kavukcuoglu, and Geoffrey E. Hinton. Attend, infer, repeat: Fast scene understanding
    with generative models. In NIPS, pages 3225–3233, 2016.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) S. M. Ali Eslami、Nicolas Heess、Theophane Weber、Yuval Tassa、David Szepesvari、Koray
    Kavukcuoglu 和 Geoffrey E. Hinton. 注意、推断、重复：使用生成模型的快速场景理解。发表于 NIPS，页码 3225–3233，2016。
- en: '(21) Valentin Flunkert, David Salinas, and Jan Gasthaus. Deepar: Probabilistic
    forecasting with autoregressive recurrent networks. CoRR, abs/1704.04110, 2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Valentin Flunkert、David Salinas 和 Jan Gasthaus. Deepar：使用自回归递归网络进行概率预测。CoRR，abs/1704.04110，2017。
- en: '(22) Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation:
    Insights and applications. In Deep Learning Workshop, ICML, 2015.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) Yarin Gal 和 Zoubin Ghahramani. Dropout 作为贝叶斯近似：见解与应用。发表于深度学习研讨会，ICML，2015。
- en: (23) M. J. F. Gales and S. S. Airey. Product of Gaussians for speech recognition.
    CSL, 20(1):22–40, 2006.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) M. J. F. Gales 和 S. S. Airey. 用于语音识别的高斯乘积。CSL，20(1):22–40，2006。
- en: (24) Zhe Gan, Changyou Chen, Ricardo Henao, David E. Carlson, and Lawrence Carin.
    Scalable deep Poisson factor analysis for topic modeling. In ICML, pages 1823–1832,
    2015.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) Zhe Gan、Changyou Chen、Ricardo Henao、David E. Carlson 和 Lawrence Carin.
    可扩展的深度泊松因子分析用于主题建模。发表于 ICML，页码 1823–1832，2015。
- en: (25) Zhe Gan, Ricardo Henao, David E. Carlson, and Lawrence Carin. Learning
    deep sigmoid belief networks with data augmentation. In AISTATS, 2015.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) Zhe Gan、Ricardo Henao、David E. Carlson 和 Lawrence Carin. 通过数据增强学习深度 sigmoid
    信念网络。发表于 AISTATS，2015。
- en: (26) Jochen Gast and Stefan Roth. Lightweight probabilistic deep networks. In
    CVPR, pages 3369–3378, 2018.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) Jochen Gast 和 Stefan Roth. 轻量级概率深度网络。发表于 CVPR，页码 3369–3378，2018。
- en: (27) Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram,
    David Salinas, Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting
    with spline quantile function rnns. In AISTATS, pages 1901–1910, 2019.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Jan Gasthaus、Konstantinos Benidis、Yuyang Wang、Syama Sundar Rangapuram、David
    Salinas、Valentin Flunkert 和 Tim Januschowski. 使用样条量化函数 RNN 进行概率预测。发表于 AISTATS，页码
    1901–1910，2019。
- en: (28) Kostadin Georgiev and Preslav Nakov. A non-iid framework for collaborative
    filtering with restricted Boltzmann machines. In ICML, pages 1148–1156, 2013.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Kostadin Georgiev 和 Preslav Nakov. 使用限制玻尔兹曼机的协同过滤非 IID 框架。发表于 ICML，页码 1148–1156，2013。
- en: (29) Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. Book
    in preparation for MIT Press, 2016.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) Ian Goodfellow、Yoshua Bengio 和 Aaron Courville. 深度学习。准备中，MIT Press，2016。
- en: (30) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In NIPS, pages 2672–2680, 2014.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David Warde-Farley、Sherjil
    Ozair、Aaron Courville 和 Yoshua Bengio. 生成对抗网络。发表于 NIPS，页码 2672–2680，2014。
- en: (31) Alex Graves. Practical variational inference for neural networks. In NIPS,
    pages 2348–2356, 2011.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) Alex Graves. 神经网络的实用变分推断。发表于 NIPS，页码 2348–2356，2011。
- en: '(32) Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative
    modeling of graphs. In ICML, pages 2434–2444, 2019.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) Aditya Grover、Aaron Zweig 和 Stefano Ermon. Graphite：图的迭代生成建模。发表于 ICML，页码
    2434–2444，2019。
- en: (33) A.K. Gupta and D.K. Nagar. Matrix Variate Distributions. Chapman & Hall/CRC
    Monographs and Surveys in Pure and Applied Mathematics. Chapman & Hall, 2000.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) A.K. Gupta 和 D.K. Nagar. 矩阵变量分布。Chapman & Hall/CRC 纯数学与应用数学系列丛书。Chapman
    & Hall，2000。
- en: (34) Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David
    Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from
    pixels. In ICML, pages 2555–2565, 2019.
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) Danijar Hafner, Timothy P. Lillicrap, Ian Fischer, Ruben Villegas, David
    Ha, Honglak Lee, 和 James Davidson. 从像素中学习潜在动态进行规划。发表于 ICML，第 2555–2565 页，2019
    年。
- en: (35) Jeff Harrison and Mike West. Bayesian Forecasting & Dynamic Models. Springer,
    1999.
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) Jeff Harrison 和 Mike West. 贝叶斯预测与动态模型。Springer, 1999 年。
- en: (36) Andrew C Harvey. Forecasting, structural time series models and the Kalman
    filter. Cambridge university press, 1990.
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) Andrew C Harvey. 预测、结构时间序列模型和卡尔曼滤波。剑桥大学出版社, 1990 年。
- en: '(37) Hao He, Hao Wang, Guang-He Lee, and Yonglong Tian. Probgan: Towards probabilistic
    GAN with theoretical guarantees. In ICLR, 2019.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(37) Hao He, Hao Wang, Guang-He Lee, 和 Yonglong Tian. Probgan: 朝向具有理论保证的概率
    GAN。发表于 ICLR, 2019 年。'
- en: (38) Ricardo Henao, James Lu, Joseph E. Lucas, Jeffrey M. Ferranti, and Lawrence
    Carin. Electronic health record analysis via deep poisson factor models. JMLR,
    17:186:1–186:32, 2016.
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) Ricardo Henao, James Lu, Joseph E. Lucas, Jeffrey M. Ferranti, 和 Lawrence
    Carin. 通过深度泊松因子模型分析电子健康记录。JMLR, 17:186:1–186:32, 2016 年。
- en: (39) José Miguel Hernández-Lobato and Ryan Adams. Probabilistic backpropagation
    for scalable learning of bayesian neural networks. In ICML, pages 1861–1869, 2015.
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) José Miguel Hernández-Lobato 和 Ryan Adams. 用于可扩展贝叶斯神经网络学习的概率反向传播。发表于 ICML，第
    1861–1869 页，2015 年。
- en: (40) Geoffrey E. Hinton. Training products of experts by minimizing contrastive
    divergence. Neural Computation, 14(8):1771–1800, 2002.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) Geoffrey E. Hinton. 通过最小化对比散度训练专家产品。神经计算, 14(8):1771–1800, 2002 年。
- en: (41) Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm
    for deep belief nets. Neural computation, 18(7):1527–1554, 2006.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (41) Geoffrey E Hinton, Simon Osindero, 和 Yee-Whye Teh. 用于深度信念网络的快速学习算法。神经计算,
    18(7):1527–1554, 2006 年。
- en: (42) Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple
    by minimizing the description length of the weights. In COLT, pages 5–13, 1993.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (42) Geoffrey E Hinton 和 Drew Van Camp. 通过最小化权重描述长度来保持神经网络简单。发表于 COLT，第 5–13
    页，1993 年。
- en: (43) Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum description
    length, and Helmholtz free energy. NIPS, pages 3–3, 1994.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (43) Geoffrey E Hinton 和 Richard S Zemel. 自编码器、最小描述长度和赫尔姆霍兹自由能。NIPS，第 3–3 页，1994
    年。
- en: (44) Matthew Hoffman, Francis R Bach, and David M Blei. Online learning for
    latent Dirichlet allocation. In NIPS, pages 856–864, 2010.
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (44) Matthew Hoffman, Francis R Bach, 和 David M Blei. 潜在狄利克雷分配的在线学习。发表于 NIPS，第
    856–864 页，2010 年。
- en: (45) Matthew D. Hoffman, David M. Blei, Chong Wang, and John William Paisley.
    Stochastic variational inference. JMLR, 14(1):1303–1347, 2013.
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (45) Matthew D. Hoffman, David M. Blei, Chong Wang, 和 John William Paisley.
    随机变分推断。JMLR, 14(1):1303–1347, 2013 年。
- en: '(46) Mark F. Hornick and Pablo Tamayo. Extending recommender systems for disjoint
    user/item sets: The conference recommendation problem. TKDE, 24(8):1478–1490,
    2012.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) Mark F. Hornick 和 Pablo Tamayo. 扩展推荐系统以处理不相交的用户/项目集合：会议推荐问题。TKDE, 24(8):1478–1490,
    2012 年。
- en: (47) Wei-Ning Hsu and James R. Glass. Scalable factorized hierarchical variational
    autoencoder training. In INTERSPEECH, pages 1462–1466, 2018.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Wei-Ning Hsu 和 James R. Glass. 可扩展的因子化层次变分自编码器训练。发表于 INTERSPEECH，第 1462–1466
    页，2018 年。
- en: (48) Wei-Ning Hsu, Yu Zhang, and James R. Glass. Unsupervised learning of disentangled
    and interpretable representations from sequential data. In NIPS, pages 1878–1889,
    2017.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) Wei-Ning Hsu, Yu Zhang, 和 James R. Glass. 从序列数据中无监督地学习解耦和可解释的表示。发表于 NIPS，第
    1878–1889 页，2017 年。
- en: (49) Wei-Ning Hsu, Yu Zhang, Ron J. Weiss, Heiga Zen, Yonghui Wu, Yuxuan Wang,
    Yuan Cao, Ye Jia, Zhifeng Chen, Jonathan Shen, Patrick Nguyen, and Ruoming Pang.
    Hierarchical generative modeling for controllable speech synthesis. In ICLR, 2019.
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (49) Wei-Ning Hsu, Yu Zhang, Ron J. Weiss, Heiga Zen, Yonghui Wu, Yuxuan Wang,
    Yuan Cao, Ye Jia, Zhifeng Chen, Jonathan Shen, Patrick Nguyen, 和 Ruoming Pang.
    用于可控语音合成的层次生成建模。发表于 ICLR, 2019 年。
- en: (50) Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for
    implicit feedback datasets. In ICDM, pages 263–272, 2008.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) Yifan Hu, Yehuda Koren, 和 Chris Volinsky. 隐式反馈数据集的协同过滤。发表于 ICDM，第 263–272
    页，2008 年。
- en: (51) Hengguan Huang, Hao Wang, and Brian Mak. Recurrent poisson process unit
    for speech recognition. In AAAI, pages 6538–6545, 2019.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) Hengguan Huang, Hao Wang, 和 Brian Mak. 用于语音识别的递归泊松过程单元。发表于 AAAI，第 6538–6545
    页，2019 年。
- en: (52) Hengguan Huang, Fuzhao Xue, Hao Wang, and Ye Wang. Deep graph random process
    for relational-thinking-based speech recognition. In ICML, 2020.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) Hengguan Huang, Fuzhao Xue, Hao Wang, 和 Ye Wang. 基于关系思维的语音识别的深度图随机过程。发表于
    ICML, 2020 年。
- en: (53) David H Hubel and Torsten N Wiesel. Receptive fields and functional architecture
    of monkey striate cortex. The Journal of physiology, 195(1):215–243, 1968.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) David H Hubel 和 Torsten N Wiesel。猴子初级皮层的感受野和功能结构。《生理学杂志》，195(1):215–243,
    1968。
- en: (54) Finn V Jensen et al. An introduction to Bayesian networks, volume 210.
    UCL press London, 1996.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) Finn V Jensen 等。《贝叶斯网络简介》，第210卷。UCL出版社伦敦, 1996。
- en: (55) Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola, and Lawrence K. Saul.
    An introduction to variational methods for graphical models. Machine Learning,
    37(2):183–233, 1999.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) Michael I. Jordan, Zoubin Ghahramani, Tommi Jaakkola 和 Lawrence K. Saul。《图形模型的变分方法简介》。机器学习,
    37(2):183–233, 1999。
- en: (56) Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional
    neural network for modelling sentences. ACL, pages 655–665, 2014.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Nal Kalchbrenner, Edward Grefenstette 和 Phil Blunsom。用于建模句子的卷积神经网络。ACL,
    页码 655–665, 2014。
- en: '(57) Maximilian Karl, Maximilian Sölch, Justin Bayer, and Patrick van der Smagt.
    Deep variational bayes filters: Unsupervised learning of state space models from
    raw data. In ICLR, 2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) Maximilian Karl, Maximilian Sölch, Justin Bayer 和 Patrick van der Smagt。深度变分贝叶斯滤波器：从原始数据中无监督学习状态空间模型。发表于
    ICLR, 2017。
- en: (58) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv
    preprint arXiv:1312.6114, 2013.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Diederik P Kingma 和 Max Welling。自编码变分贝叶斯。arXiv 预印本 arXiv:1312.6114, 2013。
- en: (59) Thomas N Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. arXiv preprint arXiv:1609.02907, 2016.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (59) Thomas N Kipf 和 Max Welling。图卷积网络的半监督分类。arXiv 预印本 arXiv:1609.02907, 2016。
- en: '(60) Adam R. Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential
    attend, infer, repeat: Generative modelling of moving objects. In NIPS, pages
    8615–8625, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (60) Adam R. Kosiorek, Hyunjik Kim, Yee Whye Teh 和 Ingmar Posner。顺序注意、推理、重复：移动物体的生成建模。发表于
    NIPS, 页码 8615–8625, 2018。
- en: (61) Rahul G. Krishnan, Uri Shalit, and David A. Sontag. Structured inference
    networks for nonlinear state space models. In AAAI, pages 2101–2109, 2017.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Rahul G. Krishnan, Uri Shalit 和 David A. Sontag。用于非线性状态空间模型的结构化推理网络。发表于
    AAAI, 页码 2101–2109, 2017。
- en: (62) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In NIPS, pages 1097–1105, 2012.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton。使用深度卷积神经网络的Imagenet分类。发表于
    NIPS, 页码 1097–1105, 2012。
- en: (63) Y. LeCun. Modeles connexionnistes de l’apprentissage (connectionist learning
    models). PhD thesis, Université P. et M. Curie (Paris 6), June 1987.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) Y. LeCun。学习的连接主义模型（Modeles connexionnistes de l’apprentissage）。博士论文，Université
    P. et M. Curie（巴黎6），1987年6月。
- en: (64) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,
    1998.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (64) Yann LeCun, Léon Bottou, Yoshua Bengio 和 Patrick Haffner。应用于文档识别的基于梯度的学习。《IEEE
    会议录》，86(11):2278–2324, 1998。
- en: (65) Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional
    deep belief networks for scalable unsupervised learning of hierarchical representations.
    In ICML, pages 609–616, 2009.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) Honglak Lee, Roger Grosse, Rajesh Ranganath 和 Andrew Y Ng。用于可扩展无监督学习的卷积深度置信网络。发表于
    ICML, 页码 609–616, 2009。
- en: (66) Sheng Li, Jaya Kawale, and Yun Fu. Deep collaborative filtering via marginalized
    denoising auto-encoder. In CIKM, pages 811–820, 2015.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) Sheng Li, Jaya Kawale 和 Yun Fu。通过边际化去噪自编码器的深度协同过滤。发表于 CIKM, 页码 811–820,
    2015。
- en: (67) Wu-Jun Li and Dit-Yan Yeung. Relation regularized matrix factorization.
    In IJCAI, 2009.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (67) Wu-Jun Li 和 Dit-Yan Yeung。关系正则化矩阵分解。发表于 IJCAI, 2009。
- en: (68) Xiaopeng Li and James She. Collaborative variational autoencoder for recommender
    systems. In KDD, pages 305–314, 2017.
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) Xiaopeng Li 和 James She。用于推荐系统的协同变分自编码器。发表于 KDD, 页码 305–314, 2017。
- en: '(69) Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam, and Tong Zhang. Quase:
    Sequence editing under quantifiable guidance. In EMNLP, pages 3855–3864, 2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (69) Yi Liao, Lidong Bing, Piji Li, Shuming Shi, Wai Lam 和 Tong Zhang。Quase：在可量化指导下的序列编辑。发表于
    EMNLP, 页码 3855–3864, 2018。
- en: '(70) Nathan Nan Liu, Xiangrui Meng, Chao Liu, and Qiang Yang. Wisdom of the
    better few: cold start recommendation via representative based rating elicitation.
    In RecSys, pages 37–44, 2011.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) Nathan Nan Liu, Xiangrui Meng, Chao Liu 和 Qiang Yang。更好的少数智慧：通过代表性评级引导的冷启动推荐。发表于
    RecSys, 页码 37–44, 2011。
- en: (71) Zhongqi Lu, Zhicheng Dou, Jianxun Lian, Xing Xie, and Qiang Yang. Content-based
    collaborative filtering for news topic recommendation. In AAAI, pages 217–223,
    2015.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) Zhongqi Lu, Zhicheng Dou, Jianxun Lian, Xing Xie 和 Qiang Yang。基于内容的协同过滤新闻主题推荐。发表于
    AAAI, 页码 217–223, 2015。
- en: (72) JC MacKay David. A practical Bayesian framework for backprop networks.
    Neural computation, 1992.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) JC MacKay David。用于反向传播网络的实用贝叶斯框架。神经计算，1992年。
- en: (73) Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon
    Wilson. A simple baseline for bayesian uncertainty in deep learning. In NIPS,
    pages 13132–13143, 2019.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) Wesley J Maddox、Pavel Izmailov、Timur Garipov、Dmitry P Vetrov 和 Andrew Gordon
    Wilson。深度学习中贝叶斯不确定性的简单基线。在 NIPS 会议中，页码13132–13143，2019年。
- en: (74) Takamitsu Matsubara, Vicencc Gómez, and Hilbert J. Kappen. Latent Kullback
    Leibler control for continuous-state systems using probabilistic graphical models.
    In UAI, pages 583–592, 2014.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) Takamitsu Matsubara、Vicencc Gómez 和 Hilbert J. Kappen。使用概率图模型的连续状态系统的潜在
    Kullback Leibler 控制。在 UAI 会议中，页码583–592，2014年。
- en: (75) Nikhil Mehta, Lawrence Carin, and Piyush Rai. Stochastic blockmodels meet
    graph neural networks. In ICML, pages 4466–4474, 2019.
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) Nikhil Mehta、Lawrence Carin 和 Piyush Rai。随机块模型与图神经网络的结合。在 ICML 会议中，页码4466–4474，2019年。
- en: (76) Abdel-rahman Mohamed, Tara N. Sainath, George E. Dahl, Bhuvana Ramabhadran,
    Geoffrey E. Hinton, and Michael A. Picheny. Deep belief networks using discriminative
    features for phone recognition. In ICASSP, pages 5060–5063, 2011.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (76) Abdel-rahman Mohamed、Tara N. Sainath、George E. Dahl、Bhuvana Ramabhadran、Geoffrey
    E. Hinton 和 Michael A. Picheny。使用区分特征的深度信念网络进行电话识别。在 ICASSP 会议中，页码5060–5063，2011年。
- en: '(77) Jonas Mueller, David K. Gifford, and Tommi S. Jaakkola. Sequence to better
    sequence: Continuous revision of combinatorial structures. In ICML, pages 2536–2544,
    2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (77) Jonas Mueller、David K. Gifford 和 Tommi S. Jaakkola。序列到更好序列：组合结构的连续修订。在
    ICML 会议中，页码2536–2544，2017年。
- en: '(78) Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press,
    2012.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (78) Kevin P Murphy。机器学习：概率视角。麻省理工学院出版社，2012年。
- en: (79) Radford M. Neal. Connectionist learning of belief networks. Artif. Intell.,
    56(1):71–113, 1992.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (79) Radford M. Neal。信念网络的连接主义学习。人工智能，第56卷第1期：71–113，1992年。
- en: (80) Radford M Neal. Bayesian learning for neural networks. PhD thesis, University
    of Toronto, 1995.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) Radford M Neal。神经网络的贝叶斯学习。博士论文，多伦多大学，1995年。
- en: (81) Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov
    chain monte carlo, 2(11):2, 2011.
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) Radford M Neal 等人。使用哈密顿动力学的 MCMC。马克ov链蒙特卡罗手册，第2卷第11期：2，2011年。
- en: (82) Krzysztof Nowicki and Tom A B Snijders. Estimation and prediction for stochastic
    blockstructures. JASA, 96(455):1077–1087, 2001.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (82) Krzysztof Nowicki 和 Tom A B Snijders。随机块结构的估计和预测。JASA，第96卷第455期：1077–1087，2001年。
- en: (83) Takahiro Omi, Naonori Ueda, and Kazuyuki Aihara. Fully neural network based
    model for general temporal point processes. In NIPS, pages 2120–2129, 2019.
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (83) Takahiro Omi、Naonori Ueda 和 Kazuyuki Aihara。用于一般时间点过程的全神经网络模型。在 NIPS 会议中，页码2120–2129，2019年。
- en: (84) Aäron Van Den Oord, Sander Dieleman, and Benjamin Schrauwen. Deep content-based
    music recommendation. In NIPS, pages 2643–2651, 2013.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (84) Aäron Van Den Oord、Sander Dieleman 和 Benjamin Schrauwen。基于内容的深度音乐推荐。在 NIPS
    会议中，页码2643–2651，2013年。
- en: (85) Yoon-Joo Park. The adaptive clustering method for the long tail problem
    of recommender systems. TKDE, 25(8):1904–1915, 2013.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (85) Yoon-Joo Park。用于推荐系统长尾问题的自适应聚类方法。TKDE，第25卷第8期：1904–1915，2013年。
- en: (86) Ian Porteous, David Newman, Alexander Ihler, Arthur Asuncion, Padhraic
    Smyth, and Max Welling. Fast collapsed gibbs sampling for latent Dirichlet allocation.
    In KDD, pages 569–577, 2008.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (86) Ian Porteous、David Newman、Alexander Ihler、Arthur Asuncion、Padhraic Smyth
    和 Max Welling。用于潜在狄利克雷分配的快速折叠吉布斯采样。在 KDD 会议中，页码569–577，2008年。
- en: (87) Janis Postels, Francesco Ferroni, Huseyin Coskun, Nassir Navab, and Federico
    Tombari. Sampling-free epistemic uncertainty estimation using approximated variance
    propagation. In ICCV, pages 2931–2940, 2019.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (87) Janis Postels、Francesco Ferroni、Huseyin Coskun、Nassir Navab 和 Federico
    Tombari。使用近似方差传播的无采样认识不确定性估计。在 ICCV 会议中，页码2931–2940，2019年。
- en: (88) Christopher Poultney, Sumit Chopra, Yann L Cun, et al. Efficient learning
    of sparse representations with an energy-based model. In NIPS, pages 1137–1144,
    2006.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (88) Christopher Poultney、Sumit Chopra、Yann L Cun 等人。使用基于能量的模型进行稀疏表示的高效学习。在
    NIPS 会议中，页码1137–1144，2006年。
- en: (89) Sanjay Purushotham, Yan Liu, and C.-C. Jay Kuo. Collaborative topic regression
    with social matrix factorization for recommendation systems. In ICML, pages 759–766,
    2012.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (89) Sanjay Purushotham、Yan Liu 和 C.-C. Jay Kuo。具有社会矩阵分解的协作主题回归推荐系统。在 ICML 会议中，页码759–766，2012年。
- en: (90) Syama Sundar Rangapuram, Matthias W. Seeger, Jan Gasthaus, Lorenzo Stella,
    Yuyang Wang, and Tim Januschowski. Deep state space models for time series forecasting.
    In NIPS, pages 7796–7805, 2018.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (90) Syama Sundar Rangapuram、Matthias W. Seeger、Jan Gasthaus、Lorenzo Stella、Yuyang
    Wang 和 Tim Januschowski。用于时间序列预测的深度状态空间模型。在 NIPS 会议中，页码7796–7805，2018年。
- en: (91) Daniele Ravì, Charence Wong, Fani Deligianni, Melissa Berthelot, Javier
    Andreu-Perez, Benny Lo, and Guang-Zhong Yang. Deep learning for health informatics.
    IEEE journal of biomedical and health informatics, 21(1):4–21, 2016.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (91) Daniele Ravì、Charence Wong、Fani Deligianni、Melissa Berthelot、Javier Andreu-Perez、Benny
    Lo 和 Guang-Zhong Yang。健康信息学中的深度学习。IEEE 生物医学与健康信息学期刊，21(1)：4–21，2016 年。
- en: (92) Francesco Ricci, Lior Rokach, and Bracha Shapira. Introduction to Recommender
    Systems Handbook. Springer, 2011.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (92) Francesco Ricci、Lior Rokach 和 Bracha Shapira。推荐系统手册简介。Springer，2011 年。
- en: '(93) Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua
    Bengio. Contractive auto-encoders: Explicit invariance during feature extraction.
    In ICML, pages 833–840, 2011.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (93) Salah Rifai、Pascal Vincent、Xavier Muller、Xavier Glorot 和 Yoshua Bengio。收缩自编码器：在特征提取过程中的显式不变性。在
    ICML 会议上，页码 833–840，2011 年。
- en: (94) Sheldon M Ross, John J Kelly, Roger J Sullivan, William James Perry, Donald
    Mercer, Ruth M Davis, Thomas Dell Washburn, Earl V Sager, Joseph B Boyce, and
    Vincent L Bristow. Stochastic processes, volume 2. Wiley New York, 1996.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) Sheldon M Ross、John J Kelly、Roger J Sullivan、William James Perry、Donald
    Mercer、Ruth M Davis、Thomas Dell Washburn、Earl V Sager、Joseph B Boyce 和 Vincent
    L Bristow。随机过程，第 2 卷。Wiley New York，1996 年。
- en: (95) Tara N. Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
    Ramabhadran. Low-rank matrix factorization for deep neural network training with
    high-dimensional output targets. In ICASSP, pages 6655–6659, 2013.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) Tara N. Sainath、Brian Kingsbury、Vikas Sindhwani、Ebru Arisoy 和 Bhuvana Ramabhadran。用于深度神经网络训练的低秩矩阵分解，具有高维输出目标。在
    ICASSP 会议上，页码 6655–6659，2013 年。
- en: (96) Ruslan Salakhutdinov and Andriy Mnih. Probabilistic matrix factorization.
    In NIPS, pages 1257–1264, 2007.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) Ruslan Salakhutdinov 和 Andriy Mnih。概率矩阵分解。在 NIPS 会议上，页码 1257–1264，2007
    年。
- en: (97) Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization
    using markov chain monte carlo. In ICML, pages 880–887, 2008.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) Ruslan Salakhutdinov 和 Andriy Mnih。使用马尔可夫链蒙特卡罗的贝叶斯概率矩阵分解。在 ICML 会议上，页码
    880–887，2008 年。
- en: (98) Ruslan Salakhutdinov, Andriy Mnih, and Geoffrey E. Hinton. Restricted Boltzmann
    machines for collaborative filtering. In ICML, pages 791–798, 2007.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (98) Ruslan Salakhutdinov、Andriy Mnih 和 Geoffrey E. Hinton。用于协同过滤的限制玻尔兹曼机。在
    ICML 会议上，页码 791–798，2007 年。
- en: (99) Oleksandr Shchur, Marin Bilos, and Stephan Günnemann. Intensity-free learning
    of temporal point processes. 2019.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) Oleksandr Shchur、Marin Bilos 和 Stephan Günnemann。无强度的时间点过程学习。2019 年。
- en: (100) Alexander Shekhovtsov and Boris Flach. Feed-forward propagation in probabilistic
    neural networks with categorical and max layers. In ICLR, 2019.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) Alexander Shekhovtsov 和 Boris Flach。在具有分类和最大层的概率神经网络中进行前馈传播。在 ICLR 会议上，2019
    年。
- en: (101) Jonathan R Shewchuk. An introduction to the conjugate gradient method
    without the agonizing pain. Technical report, Carnegie Mellon University, Pittsburgh,
    PA, USA, 1994.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Jonathan R Shewchuk。无痛的共轭梯度法简介。技术报告，卡内基梅隆大学，美国宾夕法尼亚州匹兹堡，1994 年。
- en: (102) Gunnar A. Sigurdsson, Santosh Kumar Divvala, Ali Farhadi, and Abhinav
    Gupta. Asynchronous temporal fields for action recognition. In CVPR, pages 5650–5659,
    2017.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) Gunnar A. Sigurdsson、Santosh Kumar Divvala、Ali Farhadi 和 Abhinav Gupta。用于动作识别的异步时间场。在
    CVPR 会议上，页码 5650–5659，2017 年。
- en: '(103) Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,
    and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from
    overfitting. JMLR, 15(1):1929–1958, 2014.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (103) Nitish Srivastava、Geoffrey Hinton、Alex Krizhevsky、Ilya Sutskever 和 Ruslan
    Salakhutdinov。Dropout：一种防止神经网络过拟合的简单方法。JMLR，15(1)：1929–1958，2014 年。
- en: (104) Nitish Srivastava and Russ R Salakhutdinov. Multimodal learning with deep
    boltzmann machines. In NIPS, pages 2222–2230, 2012.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) Nitish Srivastava 和 Russ R Salakhutdinov。使用深度玻尔兹曼机进行多模态学习。在 NIPS 会议上，页码
    2222–2230，2012 年。
- en: (105) Karl Stelzner, Robert Peharz, and Kristian Kersting. Faster attend-infer-repeat
    with tractable probabilistic models. In ICML, pages 5966–5975, 2019.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (105) Karl Stelzner、Robert Peharz 和 Kristian Kersting。使用可处理的概率模型进行更快的 attend-infer-repeat。在
    ICML 会议上，页码 5966–5975，2019 年。
- en: (106) Robert S Strichartz. A Guide to Distribution Theory and Fourier Transforms.
    World Scientific, 2003.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) Robert S Strichartz。《分布理论与傅里叶变换指南》。World Scientific，2003 年。
- en: (107) Jiahao Su, Milan Cvitkovic, and Furong Huang. Sampling-free learning of
    bayesian quantized neural networks. 2020.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) Jiahao Su、Milan Cvitkovic 和 Furong Huang。无采样的贝叶斯量化神经网络学习。2020 年。
- en: (108) Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning
    with neural networks. In NIPS, pages 3104–3112, 2014.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (108) Ilya Sutskever、Oriol Vinyals 和 Quoc VV Le。利用神经网络的序列到序列学习。在 NIPS 会议上，页码
    3104–3112，2014 年。
- en: (109) Jinhui Tang, Guo-Jun Qi, Liyan Zhang, and Changsheng Xu. Cross-space affinity
    learning with its application to movie recommendation. IEEE Transactions on Knowledge
    and Data Engineering, 25(7):1510–1519, 2013.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (110) Wesley Tansey, Yixin Wang, David M. Blei, and Raul Rabadan. Black box
    FDR. In ICML, pages 4874–4883, 2018.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(111) Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and
    Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations
    in a deep network with a local denoising criterion. JMLR, 11:3371–3408, 2010.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (112) Chong Wang and David M. Blei. Collaborative topic modeling for recommending
    scientific articles. In KDD, pages 448–456, 2011.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (113) Chong Wang, David M. Blei, and David Heckerman. Continuous time dynamic
    topic models. In UAI, pages 579–586, 2008.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(114) Hao Wang. Bayesian Deep Learning for Integrated Intelligence: Bridging
    the Gap between Perception and Inference. PhD thesis, Hong Kong University of
    Science and Technology, 2017.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (115) Hao Wang, Binyi Chen, and Wu-Jun Li. Collaborative topic regression with
    social regularization for tag recommendation. In IJCAI, pages 2719–2725, 2013.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (116) Hao Wang and Wu-Jun Li. Relational collaborative topic regression for
    recommender systems. TKDE, 27(5):1343–1355, 2015.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(117) Hao Wang, Chengzhi Mao, Hao He, Mingmin Zhao, Tommi S. Jaakkola, and
    Dina Katabi. Bidirectional inference networks: A class of deep bayesian networks
    for health profiling. In AAAI, pages 766–773, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (118) Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Relational stacked denoising
    autoencoder for tag recommendation. In AAAI, pages 3052–3058, 2015.
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(119) Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Natural-parameter networks:
    A class of probabilistic neural networks. In NIPS, pages 118–126, 2016.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(120) Hao Wang, Xingjian Shi, and Dit-Yan Yeung. Relational deep learning:
    A deep latent variable model for link prediction. In AAAI, pages 2688–2694, 2017.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (121) Hao Wang, Naiyan Wang, and Dit-Yan Yeung. Collaborative deep learning
    for recommender systems. In KDD, pages 1235–1244, 2015.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(122) Hao Wang, SHI Xingjian, and Dit-Yan Yeung. Collaborative recurrent autoencoder:
    Recommend while learning to fill in the blanks. In NIPS, pages 415–423, 2016.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (123) Xinxi Wang and Ye Wang. Improving content-based and hybrid music recommendation
    using deep learning. In ACM MM, pages 627–636, 2014.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (124) Yuyang Wang, Alex Smola, Danielle C. Maddix, Jan Gasthaus, Dean Foster,
    and Tim Januschowski. Deep factors for forecasting. In ICML, pages 6607–6617,
    2019.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(125) Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller.
    Embed to control: A locally linear latent dynamics model for control from raw
    images. In NIPS, pages 2728–2736, 2015.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (126) Yan Zheng Wei, Luc Moreau, and Nicholas R. Jennings. Learning users’ interests
    by quality classification in market-based recommender systems. TKDE, 17(12):1678–1688,
    2005.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (127) Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient
    langevin dynamics. In ICML, pages 681–688, 2011.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (127) Max Welling 和 Yee Whye Teh. 通过随机梯度朗之万动力学进行贝叶斯学习。见于 ICML, 页码 681–688, 2011。
- en: (128) Andrew Gordon Wilson. The case for bayesian deep learning. arXiv preprint
    arXiv:2001.10995, 2020.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (128) Andrew Gordon Wilson. 贝叶斯深度学习的案例。arXiv 预印本 arXiv:2001.10995, 2020。
- en: (129) Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick.
    Improved variational autoencoders for text modeling using dilated convolutions.
    In ICML, pages 3881–3890, 2017.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (129) Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, 和 Taylor Berg-Kirkpatrick.
    使用膨胀卷积改进的变分自编码器用于文本建模。见于 ICML, 页码 3881–3890, 2017。
- en: (130) Ghim-Eng Yap, Ah-Hwee Tan, and HweeHwa Pang. Discovering and exploiting
    causal dependencies for robust mobile context-aware recommenders. TKDE, 19(7):977–992,
    2007.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (130) Ghim-Eng Yap, Ah-Hwee Tan, 和 HweeHwa Pang. 发现和利用因果依赖关系以增强移动上下文感知推荐系统的鲁棒性。TKDE,
    19(7):977–992, 2007。
- en: '(131) Haochao Ying, Liang Chen, Yuwen Xiong, and Jian Wu. Collaborative deep
    ranking: a hybrid pair-wise recommendation algorithm with implicit feedback. In
    PAKDD, 2016.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (131) Haochao Ying, Liang Chen, Yuwen Xiong, 和 Jian Wu. 协作深度排序：一种具有隐式反馈的混合对偶推荐算法。见于
    PAKDD, 2016。
- en: (132) Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma.
    Collaborative knowledge base embedding for recommender systems. In Proceedings
    of the 22nd ACM SIGKDD international conference on knowledge discovery and data
    mining, pages 353–362\. ACM, 2016.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (132) Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, 和 Wei-Ying Ma.
    推荐系统的协作知识库嵌入。见于第22届 ACM SIGKDD 国际知识发现与数据挖掘大会论文集，页码 353–362\. ACM, 2016。
- en: (133) He Zhao, Lan Du, Wray L. Buntine, and Mingyuan Zhou. Dirichlet belief
    networks for topic structure learning. In NIPS, pages 7966–7977, 2018.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (133) He Zhao, Lan Du, Wray L. Buntine, 和 Mingyuan Zhou. 主题结构学习的狄利克雷信念网络。见于
    NIPS, 页码 7966–7977, 2018。
- en: '(134) Vincent Wenchen Zheng, Bin Cao, Yu Zheng, Xing Xie, and Qiang Yang. Collaborative
    filtering meets mobile recommendation: A user-centered approach. In AAAI, 2010.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (134) Vincent Wenchen Zheng, Bin Cao, Yu Zheng, Xing Xie, 和 Qiang Yang. 协同过滤遇上移动推荐：一种以用户为中心的方法。见于
    AAAI, 2010。
- en: (135) Mingyuan Zhou and Lawrence Carin. Negative binomial process count and
    mixture modeling. IEEE Trans. Pattern Anal. Mach. Intell., 37(2):307–320, 2015.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (135) Mingyuan Zhou 和 Lawrence Carin. 负二项过程计数与混合建模。IEEE Trans. Pattern Anal.
    Mach. Intell., 37(2):307–320, 2015。
- en: (136) Mingyuan Zhou, Lauren Hannah, David B. Dunson, and Lawrence Carin. Beta-negative
    binomial process and Poisson factor analysis. In AISTATS, pages 1462–1471, 2012.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (136) Mingyuan Zhou, Lauren Hannah, David B. Dunson, 和 Lawrence Carin. 贝塔负二项过程与泊松因子分析。见于
    AISTATS, 页码 1462–1471, 2012。
