- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:04:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1910.07738] A Survey of Deep Learning Techniques for Autonomous Driving'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1910.07738](https://ar5iv.labs.arxiv.org/html/1910.07738)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning Techniques for Autonomous Driving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sorin Grigorescu
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence,
  prefs: []
  type: TYPE_NORMAL
- en: Elektrobit Automotive.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics, Vision and Control Lab,
  prefs: []
  type: TYPE_NORMAL
- en: Transilvania University of Brasov.
  prefs: []
  type: TYPE_NORMAL
- en: Brasov, Romania
  prefs: []
  type: TYPE_NORMAL
- en: Sorin.Grigorescu@elektrobit.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Bogdan Trasnea'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence,
  prefs: []
  type: TYPE_NORMAL
- en: Elektrobit Automotive.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics, Vision and Control Lab,
  prefs: []
  type: TYPE_NORMAL
- en: Transilvania University of Brasov.
  prefs: []
  type: TYPE_NORMAL
- en: Brasov, Romania
  prefs: []
  type: TYPE_NORMAL
- en: Bogdan.Trasnea@elektrobit.com
  prefs: []
  type: TYPE_NORMAL
- en: \ANDTiberiu Cocias
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence,
  prefs: []
  type: TYPE_NORMAL
- en: Elektrobit Automotive.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics, Vision and Control Lab,
  prefs: []
  type: TYPE_NORMAL
- en: Transilvania University of Brasov.
  prefs: []
  type: TYPE_NORMAL
- en: Brasov, Romania
  prefs: []
  type: TYPE_NORMAL
- en: Tiberiu.Cocias@elektrobit.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Gigel Macesanu'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence,
  prefs: []
  type: TYPE_NORMAL
- en: Elektrobit Automotive.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics, Vision and Control Lab,
  prefs: []
  type: TYPE_NORMAL
- en: Transilvania University of Brasov.
  prefs: []
  type: TYPE_NORMAL
- en: Brasov, Romania
  prefs: []
  type: TYPE_NORMAL
- en: 'Gigel.Macesanu@elektrobit.com The authors are with Elektrobit Automotive and
    the Robotics, Vision and Control Laboratory (ROVIS Lab) at the Department of Automation
    and Information Technology, Transilvania University of Brasov, 500036 Romania.
    E-mail: (see [http://rovislab.com/sorin_grigorescu.html](http://rovislab.com/sorin_grigorescu.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The last decade witnessed increasingly rapid progress in self-driving vehicle
    technology, mainly backed up by advances in the area of deep learning and artificial
    intelligence. The objective of this paper is to survey the current state-of-the-art
    on deep learning technologies used in autonomous driving. We start by presenting
    AI-based self-driving architectures, convolutional and recurrent neural networks,
    as well as the deep reinforcement learning paradigm. These methodologies form
    a base for the surveyed driving scene perception, path planning, behavior arbitration
    and motion control algorithms. We investigate both the modular perception-planning-action
    pipeline, where each module is built using deep learning methods, as well as End2End
    systems, which directly map sensory information to steering commands. Additionally,
    we tackle current challenges encountered in designing AI architectures for autonomous
    driving, such as their safety, training data sources and computational hardware.
    The comparison presented in this survey helps to gain insight into the strengths
    and limitations of deep learning and AI approaches for autonomous driving and
    assist with design choices.¹¹1The articles referenced in this survey can be accessed
    at the web-page accompanying this paper, available at [http://rovislab.com/survey_DL_AD.html](http://rovislab.com/survey_DL_AD.html)
  prefs: []
  type: TYPE_NORMAL
- en: Contents
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[1 Introduction](#S1 "In A Survey of Deep Learning Techniques for Autonomous
    Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[2 Deep Learning based Decision-Making Architectures for Self-Driving Cars](#S2
    "In A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3 Overview of Deep Learning Technologies](#S3 "In A Survey of Deep Learning
    Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.1 Deep Convolutional Neural Networks](#S3.SS1 "In 3 Overview of Deep Learning
    Technologies ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.2 Recurrent Neural Networks](#S3.SS2 "In 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[3.3 Deep Reinforcement Learning](#S3.SS3 "In 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4 Deep Learning for Driving Scene Perception and Localization](#S4 "In A Survey
    of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.1 Sensing Hardware: Camera vs. LiDAR Debate](#S4.SS1 "In 4 Deep Learning
    for Driving Scene Perception and Localization ‣ A Survey of Deep Learning Techniques
    for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2 Driving Scene Understanding](#S4.SS2 "In 4 Deep Learning for Driving Scene
    Perception and Localization ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.1 Bounding-Box-Like Object Detectors](#S4.SS2.SSS1 "In 4.2 Driving Scene
    Understanding ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.2 Semantic and Instance Segmentation](#S4.SS2.SSS2 "In 4.2 Driving Scene
    Understanding ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.2.3 Localization](#S4.SS2.SSS3 "In 4.2 Driving Scene Understanding ‣ 4 Deep
    Learning for Driving Scene Perception and Localization ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[4.3 Perception using Occupancy Maps](#S4.SS3 "In 4 Deep Learning for Driving
    Scene Perception and Localization ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5 Deep Learning for Path Planning and Behavior Arbitration](#S5 "In A Survey
    of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6 Motion Controllers for AI-based Self-Driving Cars](#S6 "In A Survey of Deep
    Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.1 Learning Controllers](#S6.SS1 "In 6 Motion Controllers for AI-based Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[6.2 End2End Learning Control](#S6.SS2 "In 6 Motion Controllers for AI-based
    Self-Driving Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[7 Safety of Deep Learning in Autonomous Driving](#S7 "In A Survey of Deep
    Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8 Data Sources for Training Autonomous Driving Systems](#S8 "In A Survey of
    Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[9 Computational Hardware and Deployment](#S9 "In A Survey of Deep Learning
    Techniques for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[10 Discussion and Conclusions](#S10 "In A Survey of Deep Learning Techniques
    for Autonomous Driving")'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[10.1 Final Notes](#S10.SS1 "In 10 Discussion and Conclusions ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving")'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over the course of the last decade, Deep Learning and Artificial Intelligence
    (AI) became the main technologies behind many breakthroughs in computer vision [[1](#bib.bib1)],
    robotics [[2](#bib.bib2)] and Natural Language Processing (NLP) [[3](#bib.bib3)].
    They also have a major impact in the autonomous driving revolution seen today
    both in academia and industry. Autonomous Vehicles (AVs) and self-driving cars
    began to migrate from laboratory development and testing conditions to driving
    on public roads. Their deployment in our environmental landscape offers a decrease
    in road accidents and traffic congestions, as well as an improvement of our mobility
    in overcrowded cities. The title of ”self-driving” may seem self-evident, but
    there are actually five SAE Levels used to define autonomous driving. The SAE
    J3016 standard [[4](#bib.bib4)] introduces a scale from 0 to 5 for grading vehicle
    automation. Lower SAE Levels feature basic driver assistance, whilst higher SAE
    Levels move towards vehicles requiring no human interaction whatsoever. Cars in
    the level 5 category require no human input and typically will not even feature
    steering wheels or foot pedals.
  prefs: []
  type: TYPE_NORMAL
- en: Although most driving scenarios can be relatively simply solved with classical
    perception, path planning and motion control methods, the remaining unsolved scenarios
    are corner cases in which traditional methods fail.
  prefs: []
  type: TYPE_NORMAL
- en: One of the first autonomous cars was developed by Ernst Dickmanns [[5](#bib.bib5)]
    in the 1980s. This paved the way for new research projects, such as PROMETHEUS,
    which aimed to develop a fully functional autonomous car. In 1994, the VaMP driverless
    car managed to drive $1,600km$, out of which $95\%$ were driven autonomously.
    Similarly, in 1995, CMU NAVLAB demonstrated autonomous driving on $6,000km$, with
    $98\%$ driven autonomously. Another important milestone in autonomous driving
    were the DARPA Grand Challenges in 2004 and 2005, as well as the DARPA Urban Challenge
    in 2007\. The goal was for a driverless car to navigate an off-road course as
    fast as possible, without human intervention. In 2004, none of the 15 vehicles
    completed the race. Stanley, the winner of the 2005 race, leveraged Machine Learning
    techniques for navigating the unstructured environment. This was a turning point
    in self-driving cars development, acknowledging Machine Learning and AI as central
    components of autonomous driving. The turning point is also notable in this survey
    paper, since the majority of the surveyed work is dated after 2005.
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we review the different artificial intelligence and deep learning
    technologies used in autonomous driving, and provide a survey on state-of-the-art
    deep learning and AI methods applied to self-driving cars. We also dedicate complete
    sections on tackling safety aspects, the challenge of training data sources and
    the required computational hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Learning based Decision-Making Architectures for Self-Driving Cars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-driving cars are autonomous decision-making systems that process streams
    of observations coming from different on-board sources, such as cameras, radars,
    LiDARs, ultrasonic sensors, GPS units and/or inertial sensors. These observations
    are used by the car’s computer to make driving decisions. The basic block diagrams
    of an AI powered autonomous car are shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep
    Learning based Decision-Making Architectures for Self-Driving Cars ‣ A Survey
    of Deep Learning Techniques for Autonomous Driving"). The driving decisions are
    computed either in a modular perception-planning-action pipeline (Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures for Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(a)), or in
    an End2End learning fashion (Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based
    Decision-Making Architectures for Self-Driving Cars ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving")(b)), where sensory information is directly
    mapped to control outputs. The components of the modular pipeline can be designed
    either based on AI and deep learning methodologies, or using classical non-learning
    approaches. Various permutations of learning and non-learning based components
    are possible (e.g. a deep learning based object detector provides input to a classical
    A-star path planning algorithm). A safety monitor is designed to assure the safety
    of each module.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33a7e1b466baddacfdaef03824e9a545.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Deep Learning based self-driving car. The architecture can be implemented
    either as a sequential perception-planing-action pipeline (a), or as an End2End
    system (b). In the sequential pipeline case, the components can be designed either
    using AI and deep learning methodologies, or based on classical non-learning approaches.
    End2End learning systems are mainly based on deep learning methods. A safety monitor
    is usually designed to ensure the safety of each module.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The modular pipeline in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making
    Architectures for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for
    Autonomous Driving")(a) is hierarchically decomposed into four components which
    can be designed using either deep learning and AI approaches, or classical methods.
    These components are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perception and Localization,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-Level Path Planning,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Behavior Arbitration, or low-level path planning,
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motion Controllers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Based on these four high-level components, we have grouped together relevant
    deep learning papers describing methods developed for autonomous driving systems.
    Additional to the reviewed algorithms, we have also grouped relevant articles
    covering the safety, data sources and hardware aspects encountered when designing
    deep learning modules for self-driving cars.
  prefs: []
  type: TYPE_NORMAL
- en: Given a route planned through the road network, the first task of an autonomous
    car is to understand and localize itself in the surrounding environment. Based
    on this representation, a continuous path is planned and the future actions of
    the car are determined by the behavior arbitration system. Finally, a motion control
    system reactively corrects errors generated in the execution of the planned motion.
    A review of classical non-AI design methodologies for these four components can
    be found in [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: Following, we will give an introduction of deep learning and AI technologies
    used in autonomous driving, as well as surveying different methodologies used
    to design the hierarchical decision making process described above. Additionally,
    we provide an overview of End2End learning systems used to encode the hierarchical
    process into a single deep learning architecture which directly maps sensory observations
    to control outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Overview of Deep Learning Technologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the basis of deep learning technologies used in
    autonomous vehicles and comment on the capabilities of each paradigm. We focus
    on Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and Deep
    Reinforcement Learning (DRL), which are the most common deep learning methodologies
    applied to autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the survey, we use the following notations to describe time dependent
    sequences. The value of a variable is defined either for a single discrete time
    step $t$, written as superscript $<t>$, or as a discrete sequence defined in the
    $<t,t+k>$ time interval, where $k$ denotes the length of the sequence. For example,
    the value of a state variable $\mathbf{z}$ is defined either at discrete time
    $t$, as $\mathbf{z}^{<t>}$, or within a sequence interval $\mathbf{z}^{<t,t+k>}$.
    Vectors and matrices are indicated by bold symbols.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Deep Convolutional Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Convolutional Neural Networks (CNN) are mainly used for processing spatial information,
    such as images, and can be viewed as image features extractors and universal non-linear
    function approximators [[7](#bib.bib7)], [[8](#bib.bib8)]. Before the rise of
    deep learning, computer vision systems used to be implemented based on handcrafted
    features, such as HAAR [[9](#bib.bib9)], Local Binary Patterns (LBP) [[10](#bib.bib10)],
    or Histograms of Oriented Gradients (HoG) [[11](#bib.bib11)]. In comparison to
    these traditional handcrafted features, convolutional neural networks are able
    to automatically learn a representation of the feature space encoded in the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs can be loosely understood as very approximate analogies to different parts
    of the mammalian visual cortex [[12](#bib.bib12)]. An image formed on the retina
    is sent to the visual cortex through the thalamus. Each brain hemisphere has its
    own visual cortex. The visual information is received by the visual cortex in
    a crossed manner: the left visual cortex receives information from the right eye,
    while the right visual cortex is fed with visual data from the left eye. The information
    is processed according to the dual flux theory [[13](#bib.bib13)], which states
    that the visual flow follows two main fluxes: a ventral flux, responsible for
    visual identification and object recognition, and a dorsal flux used for establishing
    spatial relations between objects. A CNN mimics the functioning of the ventral
    flux, in which different areas of the brain are sensible to specific features
    in the visual field. The earlier brain cells in the visual cortex are activated
    by sharp transitions in the visual field of view, in the same way in which an
    edge detector highlights sharp transitions between the neighboring pixels in an
    image. These edges are further used in the brain to approximate object parts and
    finally to estimate abstract representations of objects.'
  prefs: []
  type: TYPE_NORMAL
- en: An CNN is parametrized by its weights vector $\theta=[\mathbf{W},\mathbf{b}]$,
    where $\mathbf{W}$ is the set of weights governing the inter-neural connections
    and $\mathbf{b}$ is the set of neuron bias values. The set of weights $\mathbf{W}$
    is organized as image filters, with coefficients learned during training. Convolutional
    layers within a CNN exploit local spatial correlations of image pixels to learn
    translation-invariant convolution filters, which capture discriminant image features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a multichannel signal representation $\mathbf{M}_{k}$ in layer $k$,
    which is a channel-wise integration of signal representations $\mathbf{M}_{k,c}$,
    where $c\in\mathbb{N}$. A signal representation can be generated in layer $k+1$
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{M}_{k+1,l}=\varphi(\mathbf{M}_{k}*\mathbf{w}_{k,l}+\mathbf{b}_{k,l}),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{w}_{k,l}\in\mathbf{W}$ is a convolutional filter with the same
    number of channels as $\mathbf{M}_{k}$, $\mathbf{b}_{k,l}\in\mathbf{b}$ represents
    the bias, $l$ is a channel index and $*$ denotes the convolution operation. $\varphi(\cdot)$
    is an activation function applied to each pixel in the input signal. Typically,
    the Rectified Linear Unit (ReLU) is the most commonly used activation function
    in computer vision applications [[1](#bib.bib1)]. The final layer of a CNN is
    usually a fully-connected layer which acts as an object discriminator on a high-level
    abstract representation of objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a supervised manner, the response $R(\cdot;\theta)$ of a CNN can be trained
    using a training database $\mathcal{D}=[(\mathbf{x}_{1},y_{1}),...,(\mathbf{x}_{m},y_{m})]$,
    where $\mathbf{x}_{i}$ is a data sample, $y_{i}$ is the corresponding label and
    $m$ is the number of training examples. The optimal network parameters can be
    calculated using Maximum Likelihood Estimation (MLE). For the clarity of explanation,
    we take as example the simple least-squares error function, which can be used
    to drive the MLE process when training regression estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\hat{\theta}}=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})=\arg\min_{\theta}\sum^{m}_{i=1}(R(\mathbf{x}_{i};\theta)-y_{i})^{2}.$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: For classification purposes, the least-squares error is usually replaced by
    the cross-entropy, or the negative log-likelihood loss functions. The optimization
    problem in Eq. [2](#S3.E2 "In 3.1 Deep Convolutional Neural Networks ‣ 3 Overview
    of Deep Learning Technologies ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving") is typically solved with Stochastic Gradient Descent (SGD) and the backpropagation
    algorithm for gradient estimation [[14](#bib.bib14)]. In practice, different variants
    of SGD are used, such as Adam [[15](#bib.bib15)] or AdaGrad [[16](#bib.bib16)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Recurrent Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Among deep learning techniques, Recurrent Neural Networks (RNN) are especially
    good in processing temporal sequence data, such as text, or video streams. Different
    from conventional neural networks, a RNN contains a time dependent feedback loop
    in its memory cell. Given a time dependent input sequence $[\mathbf{s}^{<t-\tau_{i}>},...,\mathbf{s}^{<t>}]$
    and an output sequence $[\mathbf{z}^{<t+1>},...,\mathbf{z}^{<t+\tau_{o}>}]$, a
    RNN can be ”unfolded” $\tau_{i}+\tau_{o}$ times to generate a loop-less network
    architecture matching the input length, as illustrated in Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2 Recurrent Neural Networks ‣ 3 Overview of Deep Learning Technologies ‣
    A Survey of Deep Learning Techniques for Autonomous Driving"). $t$ represents
    a temporal index, while $\tau_{i}$ and $\tau_{o}$ are the lengths of the input
    and output sequences, respectively. Such neural networks are also encountered
    under the name of sequence-to-sequence models. An unfolded network has $\tau_{i}+\tau_{o}+1$
    identical layers, that is, each layer shares the same learned weights. Once unfolded,
    a RNN can be trained using the backpropagation through time algorithm. When compared
    to a conventional neural network, the only difference is that the learned weights
    in each unfolded copy of the network are averaged, thus enabling the network to
    shared the same weights over time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fa54620f461cb53e71a774ee2d1e31c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A folded (a) and unfolded (b) over time, many-to-many Recurrent Neural
    Network. Over time $t$, both the input $\mathbf{s}^{<t-\tau_{i},t>}$ and output
    $\mathbf{z}^{<t+1,t+\tau_{o}>}$ sequences share the same weights $\mathbf{h}^{<\cdot>}$.
    The architecture is also referred to as a sequence-to-sequence model.'
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge in using basic RNNs is the vanishing gradient encountered
    during training. The gradient signal can end up being multiplied a large number
    of times, as many as the number of time steps. Hence, a traditional RNN is not
    suitable for capturing long-term dependencies in sequence data. If a network is
    very deep, or processes long sequences, the gradient of the network’s output would
    have a hard time in propagating back to affect the weights of the earlier layers.
    Under gradient vanishing, the weights of the network will not be effectively updated,
    ending up with very small weight values.
  prefs: []
  type: TYPE_NORMAL
- en: Long Short-Term Memory (LSTM) [[17](#bib.bib17)] networks are non-linear function
    approximators for estimating temporal dependencies in sequence data. As opposed
    to traditional recurrent neural networks, LSTMs solve the vanishing gradient problem
    by incorporating three gates, which control the input, output and memory state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent layers exploit temporal correlations of sequence data to learn time
    dependent neural structures. Consider the memory state $\mathbf{c}^{<t-1>}$ and
    the output state $\mathbf{h}^{<t-1>}$ in an LSTM network, sampled at time step
    $t-1$, as well as the input data $\mathbf{s}^{<t>}$ at time $t$. The opening or
    closing of a gate is controlled by a sigmoid function $\sigma(\cdot)$ of the current
    input signal $\mathbf{s}^{<t>}$ and the output signal of the last time point $\mathbf{h}^{<t-1>}$,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Gamma_{u}^{<t>}=\sigma(\mathbf{W}_{u}\mathbf{s}^{<t>}+\mathbf{U}_{u}\mathbf{h}^{<t-1>}+\mathbf{b}_{u}),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\Gamma_{f}^{<t>}=\sigma(\mathbf{W}_{f}\mathbf{s}^{<t>}+\mathbf{U}_{f}\mathbf{h}^{<t-1>}+\mathbf{b}_{f}),$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\Gamma_{o}^{<t>}=\sigma(\mathbf{W}_{o}\mathbf{s}^{<t>}+\mathbf{U}_{o}\mathbf{h}^{<t-1>}+\mathbf{b}_{o}),$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\Gamma_{u}^{<t>}$, $\Gamma_{f}^{<t>}$ and $\Gamma_{o}^{<t>}$ are gate
    functions of the input gate, forget gate and output gate, respectively. Given
    current observation, the memory state $\mathbf{c}^{<t>}$ will be updated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{c}^{<t>}=\Gamma_{u}^{<t>}*\tanh(\mathbf{W}_{c}\mathbf{s}^{<t>}+\mathbf{U}_{c}\mathbf{h}^{<t-1>}+\mathbf{b}_{c})+\Gamma_{f}*\mathbf{c}^{<t-1>},$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'The new network output $\mathbf{h}^{<t>}$ is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{h}^{<t>}=\Gamma_{o}^{<t>}*\tanh(\mathbf{c}^{<t>}).$ |  | (7)
    |'
  prefs: []
  type: TYPE_TB
- en: An LSTM network $Q$ is parametrized by $\theta=[\mathbf{W}_{i},\mathbf{U}_{i},\mathbf{b}_{i}]$,
    where $\mathbf{W}_{i}$ represents the weights of the network’s gates and memory
    cell multiplied with the input state, $\mathbf{U}_{i}$ are the weights governing
    the activations and $\mathbf{b}_{i}$ denotes the set of neuron bias values. $*$
    symbolizes element-wise multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a supervised learning setup, given a set of training sequences $\mathcal{D}=[(\mathbf{s}^{<t-\tau_{i},t>}_{1},\mathbf{z}^{<t+1,t+\tau_{o}>}_{1}),...,(\mathbf{s}^{<t-\tau_{i},t>}_{q},\mathbf{z}^{<t+1,t+\tau_{o}>}_{q})]$,
    that is, $q$ independent pairs of observed sequences with assignments $\mathbf{z}^{<t,t+\tau_{o}>}$,
    one can train the response of an LSTM network $Q(\cdot;\theta)$ using Maximum
    Likelihood Estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_math_unparsed" alttext="\begin{split}\hat{\theta}&amp;=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})\\
    &amp;=\arg\min_{\theta}\sum_{i=1}^{m}l_{i}(Q(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t+1,t+\tau_{o}>}_{i}),\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;=\arg\min_{\theta}\sum_{i=1}^{m}\sum_{t=1}^{\tau_{o}}l_{i}^{<t>}(Q^{<t>}(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t>}_{i}),\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd class="ltx_align_right"
    columnalign="right" ><mover accent="true" ><mi
    >θ</mi><mo >^</mo></mover></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    >=</mo><mrow ><mrow ><mi
    >arg</mi><mo lspace="0.167em" >⁡</mo><mrow
    ><munder ><mi
    >max</mi><mi >θ</mi></munder><mo
    lspace="0.167em" >⁡</mo><mi class="ltx_font_mathcaligraphic"
    >ℒ</mi></mrow></mrow><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mi
    >θ</mi><mo >;</mo><mi class="ltx_font_mathcaligraphic"
    >𝒟</mi><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mrow ><mo
    >=</mo><mrow ><mrow
    ><mi >arg</mi><mo
    lspace="0.167em" >⁡</mo><munder ><mi
    >min</mi><mi >θ</mi></munder></mrow><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><munderover ><mo
    movablelimits="false" >∑</mo><mrow ><mi
    >i</mi><mo >=</mo><mn
    >1</mn></mrow><mi >m</mi></munderover><mrow
    ><msub ><mi
    >l</mi><mi >i</mi></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mi >Q</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup
    ><mi >𝐬</mi><mi
    >i</mi><mrow ><mrow
    ><mo ><</mo><mrow
    ><mi >t</mi><mo
    >−</mo><msub ><mi
    >τ</mi><mi >i</mi></msub></mrow></mrow><mo
    >,</mo><mrow ><mi
    >t</mi><mo >></mo></mrow></mrow></msubsup><mo
    >;</mo><mi >θ</mi><mo
    stretchy="false" >)</mo></mrow></mrow><mo >,</mo><msubsup
    ><mi >𝐳</mi><mi
    >i</mi><mrow ><mrow
    ><mo ><</mo><mrow
    ><mi >t</mi><mo
    >+</mo><mn >1</mn></mrow></mrow><mo
    >,</mo><mrow ><mrow
    ><mi >t</mi><mo
    >+</mo><msub ><mi
    >τ</mi><mi >o</mi></msub></mrow><mo
    >></mo></mrow></mrow></msubsup><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mrow><mo >,</mo></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mrow ><mo
    >=</mo><mrow ><mrow
    ><mi >arg</mi><mo
    lspace="0.167em" >⁡</mo><munder ><mi
    >min</mi><mi >θ</mi></munder></mrow><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><munderover ><mo
    movablelimits="false" rspace="0em" >∑</mo><mrow ><mi
    >i</mi><mo >=</mo><mn
    >1</mn></mrow><mi >m</mi></munderover><mrow
    ><munderover ><mo
    movablelimits="false" >∑</mo><mrow ><mi
    >t</mi><mo >=</mo><mn
    >1</mn></mrow><msub ><mi
    >τ</mi><mi >o</mi></msub></munderover><mrow
    ><msubsup ><mi
    >l</mi><mi >i</mi><mrow
    ><mo fence="true" rspace="0em" ><</mo><mi
    >t</mi><mo fence="true" lspace="0em" >></mo></mrow></msubsup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><msup ><mi
    >Q</mi><mrow ><mo
    fence="true" rspace="0em" ><</mo><mi >t</mi><mo
    fence="true" lspace="0em" >></mo></mrow></msup><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup
    ><mi >𝐬</mi><mi
    >i</mi><mrow ><mrow
    ><mo ><</mo><mrow
    ><mi >t</mi><mo
    >−</mo><msub ><mi
    >τ</mi><mi >i</mi></msub></mrow></mrow><mo
    >,</mo><mrow ><mi
    >t</mi><mo >></mo></mrow></mrow></msubsup><mo
    >;</mo><mi >θ</mi><mo
    stretchy="false" >)</mo></mrow></mrow><mo >,</mo><msubsup
    ><mi >𝐳</mi><mi
    >i</mi><mrow ><mo
    fence="true" rspace="0em" ><</mo><mi >t</mi><mo
    fence="true" lspace="0em" >></mo></mrow></msubsup><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}\hat{\theta}&=\arg\max_{\theta}\mathcal{L}(\theta;\mathcal{D})\\
    &=\arg\min_{\theta}\sum_{i=1}^{m}l_{i}(Q(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t+1,t+\tau_{o}>}_{i}),\\
    &=\arg\min_{\theta}\sum_{i=1}^{m}\sum_{t=1}^{\tau_{o}}l_{i}^{<t>}(Q^{<t>}(\mathbf{s}^{<t-\tau_{i},t>}_{i};\theta),\mathbf{z}^{<t>}_{i}),\end{split}</annotation></semantics></math>
    |  | (8) |'
  prefs: []
  type: TYPE_NORMAL
- en: where an input sequence of observations $\mathbf{s}^{<t-\tau_{i},t>}=[\mathbf{s}^{<t-\tau_{i}>},...,\mathbf{s}^{<t-1>},\mathbf{s}^{<t>}]$
    is composed of $\tau_{i}$ consecutive data samples, $l(\cdot,\cdot)$ is the logistic
    regression loss function and $t$ represents a temporal index.
  prefs: []
  type: TYPE_NORMAL
- en: In recurrent neural networks terminology, the optimization procedure in Eq. [8](#S3.E8
    "In 3.2 Recurrent Neural Networks ‣ 3 Overview of Deep Learning Technologies ‣
    A Survey of Deep Learning Techniques for Autonomous Driving") is typically used
    for training ”many-to-many” RNN architectures, such as the one in Fig. [2](#S3.F2
    "Figure 2 ‣ 3.2 Recurrent Neural Networks ‣ 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving"), where the input
    and output states are represented by temporal sequences of $\tau_{i}$ and $\tau_{o}$
    data instances, respectively. This optimization problem is commonly solved using
    gradient based methods, like Stochastic Gradient Descent (SGD), together with
    the backpropagation through time algorithm for calculating the network’s gradients.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Deep Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following, we review the Deep Reinforcement Learning (DRL) concept as
    an autonomous driving task, using the Partially Observable Markov Decision Process
    (POMDP) formalism.
  prefs: []
  type: TYPE_NORMAL
- en: In a POMDP, an agent, which in our case is the self-driving car, senses the
    environment with observation $\mathbf{I}^{<t>}$, performs an action $a^{<t>}$
    in state $\mathbf{s}^{<t>}$, interacts with its environment through a received
    reward $R^{<t+1>}$, and transits to the next state $\mathbf{s}^{<t+1>}$ following
    a transition function $T_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$.
  prefs: []
  type: TYPE_NORMAL
- en: In RL based autonomous driving, the task is to learn an optimal driving policy
    for navigating from state $\mathbf{s}^{<t>}_{start}$ to a destination state $\mathbf{s}^{<t+k>}_{dest}$,
    given an observation $\mathbf{I}^{<t>}$ at time $t$ and the system’s state $\mathbf{s}^{<t>}$.
    $\mathbf{I}^{<t>}$ represents the observed environment, while $k$ is the number
    of time steps required for reaching the destination state $\mathbf{s}^{<t+k>}_{dest}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In reinforcement learning terminology, the above problem can be modeled as
    a POMDP $M:=(I,S,A,T,R,\gamma)$, where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $I$ is the set of observations, with $\mathbf{I}^{<t>}\in I$ defined as an observation
    of the environment at time $t$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $S$ represents a finite set of states, $\mathbf{s}^{<t>}\in S$ being the state
    of the agent at time $t$, commonly defined as the vehicle’s position, heading
    and velocity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $A$ represents a finite set of actions allowing the agent to navigate through
    the environment defined by $\mathbf{I}^{<t>}$, where $a^{<t>}\in A$ is the action
    performed by the agent at time $t$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $T:S\times A\times S\rightarrow[0,1]$ is a stochastic transition function, where
    $T_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$ describes the probability
    of arriving in state $\mathbf{s}^{<t+1>}$, after performing action $a^{<t>}$ in
    state $\mathbf{s}^{<t>}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $R:S\times A\times S\rightarrow\mathbb{R}$ is a scalar reward function which
    controls the estimation of $a$, where $R_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}\in\mathbb{R}$.
    For a state transition $\mathbf{s}^{<t>}\rightarrow\mathbf{s}^{<t+1>}$ at time
    $t$, we define a scalar reward function $R_{\mathbf{s}^{<t>},a^{<t>}}^{\mathbf{s}^{<t+1>}}$
    which quantifies how well did the agent perform in reaching the next state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\gamma$ is the discount factor controlling the importance of future versus
    immediate rewards.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Considering the proposed reward function and an arbitrary state trajectory
    $[\mathbf{s}^{<0>},\mathbf{s}^{<1>},...,\mathbf{s}^{<k>}]$ in observation space,
    at any time $\hat{t}\in[0,1,...,k]$, the associated cumulative future discounted
    reward is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R^{<\hat{t}>}=\sum^{k}_{t=\hat{t}}\gamma^{<t-\hat{t}>}r^{<t>},$ |  |
    (9) |'
  prefs: []
  type: TYPE_TB
- en: where the immediate reward at time $t$ is given by $r^{<t>}$. In RL theory,
    the statement in Eq. [9](#S3.E9 "In 3.3 Deep Reinforcement Learning ‣ 3 Overview
    of Deep Learning Technologies ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving") is known as a finite horizon learning episode of sequence length $k$ [[18](#bib.bib18)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective in RL is to find the desired trajectory policy that maximizes
    the associated cumulative future reward. We define the optimal action-value function
    $Q^{*}(\cdot,\cdot)$ which estimates the maximal future discounted reward when
    starting in state $\mathbf{s}^{<t>}$ and performing actions $[a^{<t>},...,a^{<t+k>}]$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q^{*}(\mathbf{s},a)=\underset{\pi}{\max}\mathbb{E}\text{ }[R^{<\hat{t}>}&#124;\mathbf{s}^{<\hat{t}>}=\mathbf{s},\text{
    }a^{<\hat{t}>}=a,\text{ }\pi],$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\pi$ is an action policy, viewed as a probability density function over
    a set of possible actions that can take place in a given state. The optimal action-value
    function $Q^{*}(\cdot,\cdot)$ maps a given state to the optimal action policy
    of the agent in any state:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\forall\mathbf{s}\in S:\pi^{*}(\mathbf{s})=\underset{a\in A}{\arg\max}Q^{*}(\mathbf{s},a).$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'The optimal action-value function $Q^{*}$ satisfies the Bellman optimality
    equation [[19](#bib.bib19)], which is a recursive formulation of Eq. [10](#S3.E10
    "In 3.3 Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Q^{*}(\mathbf{s},a)=\sum_{\mathbf{s}}T_{\mathbf{s},a}^{\mathbf{s}^{\prime}}\left(R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q^{*}(\mathbf{s}^{\prime},a^{\prime})\right)$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle=\mathbb{E}_{a^{\prime}}\left(R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q^{*}(\mathbf{s}^{\prime},a^{\prime})\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{s}^{\prime}$ represents a possible state visited after $\mathbf{s}=\mathbf{s}^{<t>}$
    and $a^{\prime}$ is the corresponding action policy. The model-based policy iteration
    algorithm was introduced in [[18](#bib.bib18)], based on the proof that the Bellman
    equation is a contraction mapping [[20](#bib.bib20)] when written as an operator
    $\nu$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\forall Q,\lim_{n\rightarrow\infty}\nu^{(n)}(Q)=Q^{*}.$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: 'However, the standard reinforcement learning method described above is not
    feasible in high dimensional state spaces. In autonomous driving applications,
    the observation space is mainly composed of sensory information made up of images,
    radar, LiDAR, etc. Instead of the traditional approach, a non-linear parametrization
    of $Q^{*}$ can be encoded in the layers of a deep neural network. In literature,
    such a non-linear approximator is called a Deep Q-Network (DQN) [[21](#bib.bib21)]
    and is used for estimating the approximate action-value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Q(\mathbf{s}^{<t>},a^{<t>};\Theta)\approx Q^{*}(\mathbf{s}^{<t>},a^{<t>}),$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: where $\Theta$ represents the parameters of the Deep Q-Network.
  prefs: []
  type: TYPE_NORMAL
- en: 'By taking into account the Bellman optimality equation [12](#S3.E12 "In 3.3
    Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies ‣ A Survey
    of Deep Learning Techniques for Autonomous Driving"), it is possible to train
    a deep Q-network in a reinforcement learning manner through the minimization of
    the mean squared error. The optimal expected Q value can be estimated within a
    training iteration $i$ based on a set of reference parameters $\bar{\Theta}_{i}$
    calculated in a previous iteration $i^{\prime}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y=R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}+\gamma\cdot\underset{a^{\prime}}{\max}Q(\mathbf{s}^{\prime},a^{\prime};\bar{\Theta}_{i}),$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\bar{\Theta}_{i}:=\Theta_{i^{\prime}}$. The new estimated network parameters
    at training step $i$ are evaluated using the following squared error function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla J_{\hat{\Theta}_{i}}=\underset{\Theta_{i}}{\min}\text{ }\mathbb{E}_{\mathbf{s},y,r,\mathbf{s}^{\prime}}\left[\left(y-Q(\mathbf{s},a;\Theta_{i})\right)^{2}\right],$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'where $r=R_{\mathbf{s},a}^{\mathbf{s}^{\prime}}$. Based on [16](#S3.E16 "In
    3.3 Deep Reinforcement Learning ‣ 3 Overview of Deep Learning Technologies ‣ A
    Survey of Deep Learning Techniques for Autonomous Driving"), the maximum likelihood
    estimation function from Eq. [8](#S3.E8 "In 3.2 Recurrent Neural Networks ‣ 3
    Overview of Deep Learning Technologies ‣ A Survey of Deep Learning Techniques
    for Autonomous Driving") can be applied for calculating the weights of the deep
    Q-network. The gradient is approximated with random samples and the backpropagation
    algorithm, which uses stochastic gradient descent for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\nabla_{\Theta_{i}}=\mathbb{E}_{\mathbf{s},a,r,\mathbf{s}^{\prime}}\left[\left(y-Q(\mathbf{s},a;\Theta_{i})\right)\nabla_{\Theta_{i}}\left(Q(\mathbf{s},a;\Theta_{i})\right)\right].$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'The deep reinforcement learning community has made several independent improvements
    to the original DQN algorithm [[21](#bib.bib21)]. A study on how to combine these
    improvements on deep reinforcement learning has been provided by DeepMind in [[22](#bib.bib22)],
    where the combined algorithm, entitled Rainbow, was able to outperform the independently
    competing methods. DeepMind [[22](#bib.bib22)] proposes six extensions to the
    base DQN, each addressing a distinct concern:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double Q Learning addresses the overestimation bias and decouples the selection
    of an action and its evaluation;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritized replay samples more frequently from the data in which there is information
    to learn;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dueling Networks aim at enhancing value based RL;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-step learning is used for training speed improvement;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional RL improves the target distribution in the Bellman equation;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy Nets improve the ability of the network to ignore noisy inputs and allows
    state-conditional exploration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: All of the above complementary improvements have been tested on the Atari 2600
    challenge. A good implementation of DQN regarding autonomous vehicles should start
    by combining the stated DQN extensions with respect to a desired performance.
    Given the advancements in deep reinforcement learning, the direct application
    of the algorithm still needs a training pipeline in which one should simulate
    and model the desired self-driving car’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The simulated environment state is not directly accessible to the agent. Instead,
    sensor readings provide clues about the true state of the environment. In order
    to decode the true environment state, it is not sufficient to map a single snapshot
    of sensors readings. The temporal information should also be included in the network’s
    input, since the environment’s state is modified over time. An example of DQN
    applied to autonomous vehicles in a simulator can be found in [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: DQN has been developed to operate in discrete action spaces. In the case of
    an autonomous car, the discrete actions would translate to discrete commands,
    such as turn left, turn right, accelerate, or break. The DQN approach described
    above has been extended to continuous action spaces based on policy gradient estimation [[24](#bib.bib24)].
    The method in [[24](#bib.bib24)] describes a model-free actor-critic algorithm
    able to learn different continuous control tasks directly from raw pixel inputs.
    A model-based solution for continuous Q-learning is proposed in [[25](#bib.bib25)].
  prefs: []
  type: TYPE_NORMAL
- en: Although continuous control with DRL is possible, the most common strategy for
    DRL in autonomous driving is based on discrete control [[26](#bib.bib26)]. The
    main challenge here is the training, since the agent has to explore its environment,
    usually through learning from collisions. Such systems, trained solely on simulated
    data, tend to learn a biased version of the driving environment. A solution here
    is to use Imitation Learning methods, such as Inverse Reinforcement Learning (IRL) [[27](#bib.bib27)],
    to learn from human driving demonstrations without needing to explore unsafe actions.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Learning for Driving Scene Perception and Localization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The self-driving technology enables a vehicle to operate autonomously by perceiving
    the environment and instrumenting a responsive answer. Following, we give an overview
    of the top methods used in driving scene understanding, considering camera based
    vs. LiDAR environment perception. We survey object detection and recognition,
    semantic segmentation and localization in autonomous driving, as well as scene
    understanding using occupancy maps. Surveys dedicated to autonomous vision and
    environment perception can be found in [[28](#bib.bib28)] and [[29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: '4.1 Sensing Hardware: Camera vs. LiDAR Debate'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning methods are particularly well suited for detecting and recognizing
    objects in 2D images and 3D point clouds acquired from video cameras and LiDAR
    (Light Detection and Ranging) devices, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the autonomous driving community, 3D perception is mainly based on LiDAR
    sensors, which provide a direct 3D representation of the surrounding environment
    in the form of 3D point clouds. The performance of a LiDAR is measured in terms
    of field of view, range, resolution and rotation/frame rate. 3D sensors, such
    as Velodyne^®, usually have a $360^{\circ}$ horizontal field of view. In order
    to operate at high speeds, an autonomous vehicle requires a minimum of $200m$
    range, allowing the vehicle to react to changes in road conditions in time. The
    3D object detection precision is dictated by the resolution of the sensor, with
    most advanced LiDARs being able to provide a $3cm$ accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7fd891e18f62c81967a53a5cbebd571d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of scene perception results. (a) 2D object detection in
    images. (b) 3D bounding box detector applied on LiDAR data. (c) Semantic segmentation
    results on images.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent debate sparked around camera vs. LiDAR (Light Detection and Ranging)
    sensing technologies. Tesla^® and Waymo^®, two of the companies leading the development
    of self-driving technology [[30](#bib.bib30)], have different philosophies with
    respect to their main perception sensor, as well as regarding the targeted SAE
    level [[4](#bib.bib4)]. Waymo^® is building their vehicles directly as Level 5
    systems, with currently more than 10 million miles driven autonomously²²2[https://arstechnica.com/cars/2018/10/waymo-has-driven-10-million-miles-on-public-roads/](https://arstechnica.com/cars/2018/10/waymo-has-driven-10-million-miles-on-public-roads/).
    On the other hand, Tesla^® deploys its AutoPilot as an ADAS (Advanced Driver Assistance
    System) component, which customers can turn on or off at their convenience. The
    advantage of Tesla^® resides in its large training database, consisting of more
    than 1 billion driven miles³³3[https://electrek.co/2018/11/28/tesla-autopilot-1-billion-miles/](https://electrek.co/2018/11/28/tesla-autopilot-1-billion-miles/).
    The database has been acquired by collecting data from customers-owned cars.
  prefs: []
  type: TYPE_NORMAL
- en: The main sensing technologies differ in both companies. Tesla^® tries to leverage
    on its camera systems, whereas Waymo’s driving technology relies more on Lidar
    sensors⁴⁴4[https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation](https://www.theverge.com/transportation/2018/4/19/17204044/tesla-waymo-self-driving-car-data-simulation).
    The sensing approaches have advantages and disadvantages. LiDARs have high resolution
    and precise perception even in the dark, but are vulnerable to bad weather conditions
    (e.g. heavy rain) [[31](#bib.bib31)] and involve moving parts. In contrast, cameras
    are cost efficient, but lack depth perception and cannot work in the dark. Cameras
    are also sensitive to bad weather, if the weather conditions are obstructing the
    field of view.
  prefs: []
  type: TYPE_NORMAL
- en: Researchers at Cornell University tried to replicate LiDAR-like point clouds
    from visual depth estimation [[32](#bib.bib32)]. An estimated depth map is reprojected
    into 3D space, with respect to the left sensor’s coordinate of a stereo camera.
    The resulting point cloud is referred to as pseudo-LiDAR. The pseudo-LiDAR data
    can be further fed to 3D deep learning processing methods, such as PointNet [[33](#bib.bib33)]
    or AVOD [[34](#bib.bib34)]. The success of image based 3D estimation is of high
    importance to the large scale deployment of autonomous cars, since the LiDAR is
    arguably one of the most expensive hardware component in a self-driving vehicle.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these sensing technologies, radar and ultrasonic sensors are used
    to enhance perception capabilities. For example, alongside three Lidar sensors,
    Waymo also makes use of five radars and eight cameras, while Tesla^® cars are
    equipped with eights cameras, 12 ultrasonic sensors and one forward-facing radar.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Driving Scene Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An autonomous car should be able to detect traffic participants and drivable
    areas, particularly in urban areas where a wide variety of object appearances
    and occlusions may appear. Deep learning based perception, in particular Convolutional
    Neural Networks (CNNs), became the de-facto standard in object detection and recognition,
    obtaining remarkable results in competitions such as the ImageNet Large Scale
    Visual Recognition Challenge [[35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Different neural networks architectures are used to detect objects as 2D regions
    of interest [[36](#bib.bib36)] [[37](#bib.bib37)] [[38](#bib.bib38)] [[39](#bib.bib39)] [[40](#bib.bib40)] [[41](#bib.bib41)]
    or pixel-wise segmented areas in images [[42](#bib.bib42)] [[43](#bib.bib43)] [[44](#bib.bib44)] [[45](#bib.bib45)],
    3D bounding boxes in LiDAR point clouds [[33](#bib.bib33)] [[46](#bib.bib46)] [[47](#bib.bib47)],
    as well as 3D representations of objects in combined camera-LiDAR data [[48](#bib.bib48)] [[49](#bib.bib49)] [[34](#bib.bib34)].
    Examples of scene perception results are illustrated in Fig. [3](#S4.F3 "Figure
    3 ‣ 4.1 Sensing Hardware: Camera vs. LiDAR Debate ‣ 4 Deep Learning for Driving
    Scene Perception and Localization ‣ A Survey of Deep Learning Techniques for Autonomous
    Driving"). Being richer in information, image data is more suited for the object
    recognition task. However, the real-world 3D positions of the detected objects
    have to be estimated, since depth information is lost in the projection of the
    imaged scene onto the imaging sensor.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Bounding-Box-Like Object Detectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most popular architectures for 2D object detection in images are single
    stage and double stage detectors. Popular single stage detectors are ”You Only
    Look Once” (Yolo) [[36](#bib.bib36)] [[50](#bib.bib50)] [[51](#bib.bib51)], the
    Single Shot multibox Detector (SSD) [[52](#bib.bib52)], CornerNet [[37](#bib.bib37)]
    and RefineNet [[38](#bib.bib38)]. Double stage detectors, such as RCNN [[53](#bib.bib53)],
    Faster-RCNN [[54](#bib.bib54)], or R-FCN [[41](#bib.bib41)], split the object
    detection process into two parts: region of interest candidates proposals and
    bounding boxes classification. In general, single stage detectors do not provide
    the same performances as double stage detectors, but are significantly faster.'
  prefs: []
  type: TYPE_NORMAL
- en: If in-vehicle computation resources are scarce, one can use detectors such as
    SqueezeNet [[40](#bib.bib40)] or [[55](#bib.bib55)], which are optimized to run
    on embedded hardware. These detectors usually have a smaller neural network architecture,
    making it possible to detect objects using a reduced number of operations, at
    the cost of detection accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: A comparison between the object detectors described above is given in Figure [4](#S4.F4
    "Figure 4 ‣ 4.2.2 Semantic and Instance Segmentation ‣ 4.2 Driving Scene Understanding
    ‣ 4 Deep Learning for Driving Scene Perception and Localization ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving"), based on the Pascal VOC 2012
    dataset and their measured mean Average Precision (mAP) with an Intersection over
    Union (IoU) value equal to $50$ and $75$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: A number of publications showcased object detection on raw 3D sensory data,
    as well as for combined video and LiDAR information. PointNet [[33](#bib.bib33)]
    and VoxelNet [[46](#bib.bib46)] are designed to detect objects solely from 3D
    data, providing also the 3D positions of the objects. However, point clouds alone
    do not contain the rich visual information available in images. In order to overcome
    this, combined camera-LiDAR architectures are used, such as Frustum PointNet [[48](#bib.bib48)],
    Multi-View 3D networks (MV3D) [[49](#bib.bib49)], or RoarNet [[56](#bib.bib56)].
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage in using a LiDAR in the sensory suite of a self-driving
    car is primarily its cost⁵⁵5[https://techcrunch.com/2019/03/06/waymo-to-start-selling-standalone-lidar-sensors/](https://techcrunch.com/2019/03/06/waymo-to-start-selling-standalone-lidar-sensors/).
    A solution here would be to use neural network architectures such as AVOD (Aggregate
    View Object Detection) [[34](#bib.bib34)], which leverage on LiDAR data only for
    training, while images are used during training and deployment. At deployment
    stage, AVOD is able to predict 3D bounding boxes of objects solely from image
    data. In such a system, a LiDAR sensor is necessary only for training data acquisition,
    much like the cars used today to gather road data for navigation maps.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Semantic and Instance Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Driving scene understanding can also be achieved using semantic segmentation,
    representing the categorical labeling of each pixel in an image. In the autonomous
    driving context, pixels can be marked with categorical labels representing drivable
    area, pedestrians, traffic participants, buildings, etc. It is one of the high-level
    tasks that paves the way towards complete scene understanding, being used in applications
    such as autonomous driving, indoor navigation, or virtual and augmented reality.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation networks like SegNet [[42](#bib.bib42)], ICNet [[43](#bib.bib43)],
    ENet [[57](#bib.bib57)], AdapNet [[58](#bib.bib58)], or Mask R-CNN [[45](#bib.bib45)]
    are mainly encoder-decoder architectures with a pixel-wise classification layer.
    These are based on building blocks from some common network topologies, such as
    AlexNet [[1](#bib.bib1)], VGG-16 [[59](#bib.bib59)], GoogLeNet [[60](#bib.bib60)],
    or ResNet [[61](#bib.bib61)].
  prefs: []
  type: TYPE_NORMAL
- en: As in the case of bounding-box detectors, efforts have been made to improve
    the computation time of these systems on embedded targets. In [[44](#bib.bib44)]
    and [[57](#bib.bib57)], the authors proposed approaches to speed up data processing
    and inference on embedded devices for autonomous driving. Both architectures are
    light networks providing similar results as SegNet, with a reduced computation
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness objective for semantic segmentation was tackled for optimization
    in AdapNet [[58](#bib.bib58)]. The model is capable of robust segmentation in
    various environments by adaptively learning features of expert networks based
    on scene conditions.
  prefs: []
  type: TYPE_NORMAL
- en: A combined bounding-box object detector and semantic segmentation result can
    be obtained using architectures such as Mask R-CNN [[45](#bib.bib45)]. The method
    extends the effectiveness of Faster-RCNN to instance segmentation by adding a
    branch for predicting an object mask in parallel with the existing branch for
    bounding box recognition.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [5](#S4.F5 "Figure 5 ‣ 4.2.3 Localization ‣ 4.2 Driving Scene Understanding
    ‣ 4 Deep Learning for Driving Scene Perception and Localization ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving") shows tests results performed
    on four key semantic segmentation networks, based on the CityScapes dataset. The
    per-class mean Intersection over Union (mIoU) refers to multi-class segmentation,
    where each pixel is labeled as belonging to a specific object class, while per-category
    mIoU refers to foreground (object) - background (non-object) segmentation. The
    input samples have a size of $480px\times 320px$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5902577748aa951168dccaf68f1d9e51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Object detection and recognition performance comparison. The evaluation
    has been performed on the Pascal VOC 2012 benchmarking database. The first four
    methods on the right represent single stage detectors, while the remaining six
    are double stage detectors. Due to their increased complexity, the runtime performance
    in Frames-per-Second (FPS) is lower for the case of double stage detectors.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Localization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Localization algorithms aim at calculating the pose (position and orientation)
    of the autonomous vehicle as it navigates. Although this can be achieved with
    systems such as GPS, in the followings we will focus on deep learning techniques
    for visual based localization.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Localization, also known as Visual Odometry (VO), is typically determined
    by matching keypoint landmarks in consecutive video frames. Given the current
    frame, these keypoints are used as input to a perspective-$n$-point mapping algorithm
    for computing the pose of the vehicle with respect to the previous frame. Deep
    learning can be used to improve the accuracy of VO by directly influencing the
    precision of the keypoints detector. In [[62](#bib.bib62)], a deep neural network
    has been trained for learning keypoints distractors in monocular VO. The so-called
    learned ephemerality mask, acts a a rejection scheme for keypoints outliers which
    might decrease the vehicle localization’s accuracy. The structure of the environment
    can be mapped incrementally with the computation of the camera pose. These methods
    belong to the area of Simultaneous Localization and Mapping (SLAM). For a survey
    on classical SLAM techniques, we refer the reader to [[63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks such as PoseNet [[64](#bib.bib64)], VLocNet++ [[65](#bib.bib65)],
    or the approaches introduced in [[66](#bib.bib66)], [[67](#bib.bib67)], [[68](#bib.bib68)], [[69](#bib.bib69)],
    or [[70](#bib.bib70)] are using image data to estimate the 3D pose of a camera
    in an End2End fashion. Scene semantics can be derived together with the estimated
    pose [[65](#bib.bib65)].
  prefs: []
  type: TYPE_NORMAL
- en: LiDAR intensity maps are also suited for learning a real-time, calibration-agnostic
    localization for autonomous cars [[71](#bib.bib71)]. The method uses a deep neural
    network to build a learned representation of the driving scene from LiDAR sweeps
    and intensity maps. The localization of the vehicle is obtained through convolutional
    matching. In [[72](#bib.bib72)], laser scans and a deep neural network are used
    to learn descriptors for localization in urban and natural environments.
  prefs: []
  type: TYPE_NORMAL
- en: In order to safely navigate the driving scene, an autonomous car should be able
    to estimate the motion of the surrounding environment, also known as scene flow.
    Previous LiDAR based scene flow estimation techniques mainly relied on manually
    designed features. In recent articles, we have noticed a tendency to replace these
    classical methods with deep learning architectures able to automatically learn
    the scene flow. In [[73](#bib.bib73)], an encoding deep network is trained on
    occupancy grids with the purpose of finding matching or non-matching locations
    between successive timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: Although much progress has been reported in the area of deep learning based
    localization, VO techniques are still dominated by classical keypoints matching
    algorithms, combined with acceleration data provided by inertial sensors. This
    is mainly due to the fact that keypoints detectors are computational efficient
    and can be easily deployed on embedded devices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c41e4ed19e7dc3a9f96326e93560fbd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Semantic segmentation performance comparison on the CityScapes dataset [[74](#bib.bib74)].
    The input samples are $480px\times 320px$ images of driving scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Perception using Occupancy Maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An occupancy map, also known as Occupancy Grid (OG), is a representation of
    the environment which divides the driving space into a set of cells and calculates
    the occupancy probability for each cell. Popular in robotics [[72](#bib.bib72)], [[75](#bib.bib75)],
    the OG representation became a suitable solution for self-driving vehicles. A
    couple of OG data samples are shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3 Perception
    using Occupancy Maps ‣ 4 Deep Learning for Driving Scene Perception and Localization
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving").
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is used in the context of occupancy maps either for dynamic objects
    detection and tracking [[76](#bib.bib76)], probabilistic estimation of the occupancy
    map surrounding the vehicle [[77](#bib.bib77)],[[78](#bib.bib78)], or for deriving
    the driving scene context [[79](#bib.bib79)], [[80](#bib.bib80)]. In the latter
    case, the OG is constructed by accumulating data over time, while a deep neural
    net is used to label the environment into driving context classes, such as highway
    driving, parking area, or inner-city driving.
  prefs: []
  type: TYPE_NORMAL
- en: Occupancy maps represent an in-vehicle virtual environment, integrating perceptual
    information in a form better suited for path planning and motion control. Deep
    learning plays an important role in the estimation of OG, since the information
    used to populate the grid cells is inferred from processing image and LiDAR data
    using scene perception methods, as the ones described in this chapter of the survey.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc80bb3ebfdd99845dd7539d6a6cd539.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Examples of Occupancy Grids (OG). The images show a snapshot of the
    driving environment together with its respective occupancy grid [[80](#bib.bib80)].'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Deep Learning for Path Planning and Behavior Arbitration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ability of an autonomous car to find a route between two points, that is,
    a start position and a desired location, represents path planning. According to
    the path planning process, a self-driving car should consider all possible obstacles
    that are present in the surrounding environment and calculate a trajectory along
    a collision-free route. As stated in [[81](#bib.bib81)], autonomous driving is
    a multi-agent setting where the host vehicle must apply sophisticated negotiation
    skills with other road users when overtaking, giving way, merging, taking left
    and right turns, all while navigating unstructured urban roadways. The literature
    findings point to a non trivial policy that should handle safety in driving. Considering
    a reward function $R(\bar{s})=-r$ for an accident event that should be avoided
    and $R(\bar{s})\in[-1,1]$ for the rest of the trajectories, the goal is to learn
    to perform difficult maneuvers smoothly and safe.
  prefs: []
  type: TYPE_NORMAL
- en: This emerging topic of optimal path planning for autonomous cars should operate
    at high computation speeds, in order to obtain short reaction times, while satisfying
    specific optimization criteria. The survey in [[82](#bib.bib82)] provides a general
    overview of path planning in the automotive context. It addresses the taxonomy
    aspects of path planning, namely the mission planner, behavior planner and motion
    planner. However, [[82](#bib.bib82)] does not include a review on deep learning
    technologies, although the state of the art literature has revealed an increased
    interest in using deep learning technologies for path planning and behavior arbitration.
    Following, we discuss two of the most representative deep learning paradigms for
    path planning, namely Imitation Learning (IL) [[83](#bib.bib83)], [[84](#bib.bib84)], [[85](#bib.bib85)]
    and Deep Reinforcement Learning (DRL) based planning [[86](#bib.bib86)] [[87](#bib.bib87)].
  prefs: []
  type: TYPE_NORMAL
- en: The goal in Imitation Learning [[83](#bib.bib83)], [[84](#bib.bib84)], [[85](#bib.bib85)]
    is to learn the behavior of a human driver from recorded driving experiences [[88](#bib.bib88)].
    The strategy implies a vehicle teaching process from human demonstration. Thus,
    the authors employ CNNs to learn planning from imitation. For example, NeuroTrajectory [[85](#bib.bib85)]
    is a perception-planning deep neural network that learns the desired state trajectory
    of the ego-vehicle over a finite prediction horizon. Imitation learning can also
    be framed as an Inverse Reinforcement Learning (IRL) problem, where the goal is
    to learn the reward function from a human driver [[89](#bib.bib89)], [[27](#bib.bib27)].
    Such methods use real drivers behaviors to learn reward-functions and to generate
    human-like driving trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: DRL for path planning deals mainly with learning driving trajectories in a simulator [[81](#bib.bib81)], [[90](#bib.bib90)], [[86](#bib.bib86)] [[87](#bib.bib87)].
    The real environmental model is abstracted and transformed into a virtual environment,
    based on a transfer model. In [[81](#bib.bib81)], it is stated that the objective
    function cannot ensure functional safety without causing a serious variance problem.
    The proposed solution for this issue is to construct a policy function composed
    of learnable and non-learnable parts. The learnable policy tries to maximize a
    reward function (which includes comfort, safety, overtake opportunity, etc.).
    At the same time, the non-learnable policy follows the hard constraints of functional
    safety, while maintaining an acceptable level of comfort.
  prefs: []
  type: TYPE_NORMAL
- en: Both IL and DRL for path planning have advantages and disadvantages. IL has
    the advantage that it can be trained with data collected from the real-world.
    Nevertheless, this data is scarce on corner cases (e.g. driving off-lanes, vehicle
    crashes, etc.), making the trained network’s response uncertain when confronted
    with unseen data. On the other hand, although DRL systems are able to explore
    different driving situations within a simulated world, these models tend to have
    a biased behavior when ported to the real-world.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Motion Controllers for AI-based Self-Driving Cars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The motion controller is responsible for computing the longitudinal and lateral
    steering commands of the vehicle. Learning algorithms are used either as part
    of Learning Controllers, within the motion control module from Fig. [1](#S2.F1
    "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures for Self-Driving
    Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(a), or as
    complete End2End Control Systems which directly map sensory data to steering commands,
    as shown in Fig. [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making
    Architectures for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for
    Autonomous Driving")(b).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Learning Controllers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional controllers make use of an a priori model composed of fixed parameters.
    When robots or other autonomous systems are used in complex environments, such
    as driving, traditional controllers cannot foresee every possible situation that
    the system has to cope with. Unlike controllers with fixed parameters, learning
    controllers make use of training information to learn their models over time.
    With every gathered batch of training data, the approximation of the true system
    model becomes more accurate, thus enabling model flexibility, consistent uncertainty
    estimates and anticipation of repeatable effects and disturbances that cannot
    be modeled prior to deployment [[91](#bib.bib91)]. Consider the following nonlinear,
    state-space system:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{z}^{<t+1>}=\mathbf{f}_{true}(\mathbf{z}^{<t>},\mathbf{u}^{<t>}),$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 'with observable state $\mathbf{z}^{<t>}\in\mathbb{R}^{n}$ and control input
    $\mathbf{u}^{<t>}\in\mathbb{R}^{m}$, at discrete time $t$. The true system $\mathbf{f}_{true}$
    is not known exactly and is approximated by the sum of an a-priori model and a
    learned dynamics model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{z}^{<t+1>}=\underset{\text{{a-priori} model}}{\mathbf{f}(\mathbf{z}^{<t>},\mathbf{u}^{<t>})}+\underset{\text{learned
    model}}{\mathbf{h}(\mathbf{z}^{<t>})}.$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: In previous works, learning controllers have been introduced based on simple
    function approximators, such as Gaussian Process (GP) modeling [[92](#bib.bib92)],
    [[93](#bib.bib93)], [[91](#bib.bib91)], [[94](#bib.bib94)], or Support Vector
    Regression [[95](#bib.bib95)].
  prefs: []
  type: TYPE_NORMAL
- en: Learning techniques are commonly used to learn a dynamics model which in turn
    improves an a priori system model in Iterative Learning Control (ILC) [[96](#bib.bib96)], [[97](#bib.bib97)], [[98](#bib.bib98)], [[99](#bib.bib99)]
    and Model Predictive Control (MPC) [[100](#bib.bib100)] [[101](#bib.bib101)], [[91](#bib.bib91)], [[94](#bib.bib94)], [[102](#bib.bib102)], [[103](#bib.bib103)], [[104](#bib.bib104)], [[105](#bib.bib105)], [[106](#bib.bib106)].
  prefs: []
  type: TYPE_NORMAL
- en: Iterative Learning Control (ILC) is a method for controlling systems which work
    in a repetitive mode, such as path tracking in self-driving cars. It has been
    successfully applied to navigation in off-road terrain [[96](#bib.bib96)], autonomous
    car parking [[97](#bib.bib97)] and modeling of steering dynamics in an autonomous
    race car [[98](#bib.bib98)]. Multiple benefits are highlighted, such as the usage
    of a simple and computationally light feedback controller, as well as a decreased
    controller design effort (achieved by predicting path disturbances and platform
    dynamics).
  prefs: []
  type: TYPE_NORMAL
- en: Model Predictive Control (MPC) [[107](#bib.bib107)] is a control strategy that
    computes control actions by solving an optimization problem. It received lots
    of attention in the last two decades due to its ability to handle complex nonlinear
    systems with state and input constraints. The central idea behind MPC is to calculate
    control actions at each sampling time by minimizing a cost function over a short
    time horizon, while considering observations, input-output constraints and the
    system’s dynamics given by a process model. A general review of MPC techniques
    for autonomous robots is given in [[108](#bib.bib108)].
  prefs: []
  type: TYPE_NORMAL
- en: Learning has been used in conjunction with MPC to learn driving models [[100](#bib.bib100)], [[101](#bib.bib101)],
    driving dynamics for race cars operating at their handling limits [[102](#bib.bib102)], [[103](#bib.bib103)], [[104](#bib.bib104)],
    as well as to improve path tracking accuracy [[109](#bib.bib109)], [[91](#bib.bib91)], [[94](#bib.bib94)].
    These methods use learning mechanisms to identify nonlinear dynamics that are
    used in the MPC’s trajectory cost function optimization. This enables one to better
    predict disturbances and the behavior of the vehicle, leading to optimal comfort
    and safety constraints applied to the control inputs. Training data is usually
    in the form of past vehicle states and observations. For example, CNNs can be
    used to compute a dense occupancy grid map in a local robot-centric coordinate
    system. The grid map is further passed to the MPC’s cost function for optimizing
    the trajectory of the vehicle over a finite prediction horizon.
  prefs: []
  type: TYPE_NORMAL
- en: A major advantage of learning controllers is that they optimally combine traditional
    model-based control theory with learning algorithms. This makes it possible to
    still use established methodologies for controller design and stability analysis,
    together with a robust learning component applied at system identification and
    prediction levels.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 End2End Learning Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of autonomous driving, End2End Learning Control is defined as
    a direct mapping from sensory data to control commands. The inputs are usually
    from a high-dimensional features space (e.g. images or point clouds). As illustrated
    in Fig [1](#S2.F1 "Figure 1 ‣ 2 Deep Learning based Decision-Making Architectures
    for Self-Driving Cars ‣ A Survey of Deep Learning Techniques for Autonomous Driving")(b),
    this is opposed to traditional processing pipelines, where at first objects are
    detected in the input image, after which a path is planned and finally the computed
    control values are executed. A summary of some of the most popular End2End learning
    systems is given in Table [1](#S6.T1 "Table 1 ‣ 6.2 End2End Learning Control ‣
    6 Motion Controllers for AI-based Self-Driving Cars ‣ A Survey of Deep Learning
    Techniques for Autonomous Driving").
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Problem Space |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Neural network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; architecture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sensor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Description |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ALVINN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[110](#bib.bib110)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Road following |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 3-layer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; back-prop. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Camera, laser &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; range finder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ALVINN stands for Autonomous Land Vehicle In a Neural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Network). Training has been conducted using simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; road images. Successful tests on the Carnegie Mellon &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; autonomous navigation test vehicle indicate that the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network can effectively follow real roads. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DAVE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[111](#bib.bib111)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| DARPA challenge |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 6-layer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Raw camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A vision-based obstacle avoidance system for off-road &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mobile robots. The robot is a 50cm off-road truck, with two &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; front color cameras. A remote computer processes the video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and controls the robot via radio. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NVIDIA PilotNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[112](#bib.bib112)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Autonomous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; driving in real &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; traffic situations &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Raw camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The system automatically learns internal representations of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the necessary processing steps such as detecting useful road &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; features with human steering angle as the training signal. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Novel FCN-LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ego-motion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prediction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| FCN-LSTM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Large scale &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; video data &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A generic vehicle motion model from large scale crowd- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; sourced video data is obtained, while developing an end-to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; -end trainable architecture (FCN-LSTM) for predicting a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; distribution of future vehicle ego-motion data. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Novel C-LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[114](#bib.bib114)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Steering angle control | C-LSTM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Camera frames, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; steering wheel &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; angle &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C-LSTM is end-to-end trainable, learning both visual and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dynamic temporal dependencies of driving. Additionally, the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; steering angle regression problem is considered classification &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; while imposing a spatial relationship between the output &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; layer neurons. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Drive360 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[115](#bib.bib115)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Steering angle and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; velocity control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN + Fully &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Connected + &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Surround-view &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras, CAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; bus reader &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The sensor setup provides data for a 360-degree view of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the area surrounding the vehicle. A new driving dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; is collected, covering diverse scenarios. A novel driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; model is developed by integrating the surround-view &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras with the route planner. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DNN policy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[116](#bib.bib116)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Steering angle control | CNN + FC | Camera images |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The trained neural net directly maps pixel data from a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; front-facing camera to steering commands and does not &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; require any other sensors. We compare the controller &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; performance with the steering behavior of a human driver. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DeepPicar &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[117](#bib.bib117)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Steering angle control | CNN | Camera images |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DeepPicar is a small scale replica of a real self-driving car &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; called DAVE-2 by NVIDIA. It uses the same network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; architecture and can drive itself in real-time using a web &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera and a Raspberry Pi 3. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TORCS DRL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[23](#bib.bib23)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lane keeping and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; obstacle avoidance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DQN + RNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; + CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TORCS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulator &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It incorporates Recurrent Neural Networks for information &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; integration, enabling the car to handle partially observable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; scenarios. It also reduces the computational complexity for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; deployment on embedded hardware. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TORCS E2E &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[118](#bib.bib118)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Steering angle control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in a simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; env. (TORCS) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TORCS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulator &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The image features are split into three categories (sky-related, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; roadside-related, and roadrelated features). Two experimental &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; frameworks are used to investigate the importance of each &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; single feature for training a CNN controller. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Agile Autonomous Driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[106](#bib.bib106)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Steering angle and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; velocity control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for aggressive driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CNN |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Raw camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; images &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; A CNN, refereed to as the learner, is trained with optimal &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; trajectory examples provided at training time by an MPC controller.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The MPC acts as an expert, encoding the scene dynamics &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; into the layers of the neural network. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WRC6 AD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[26](#bib.bib26)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Driving in a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; racing game &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN + LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Encoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; WRC6 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Racing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Game &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; An Asynchronous ActorCritic (A3C) framework is used to &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; learn the car control in a physically and graphically realistic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; rally game, with the agents evolving simultaneously on &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; different tracks. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of End2End learning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: End2End learning can also be formulated as a back-propagation algorithm scaled
    up to complex models. The paradigm was first introduced in the 1990s, when the
    Autonomous Land Vehicle in a Neural Network (ALVINN) system was built [[110](#bib.bib110)].
    ALVINN was designed to follow a pre-defined road, steering according to the observed
    road’s curvature. The next milestone in End2End driving is considered to be in
    the mid 2000s, when DAVE (Darpa Autonomous VEhicle) managed to drive through an
    obstacle-filled road, after it has been trained on hours of human driving acquired
    in similar, but not identical, driving scenarios [[111](#bib.bib111)]. Over the
    last couple of years, the technological advances in computing hardware have facilitated
    the usage of End2End learning models. The back-propagation algorithm for gradient
    estimation in deep networks is now efficiently implemented on parallel Graphic
    Processing Units (GPUs). This kind of processing allows the training of large
    and complex network architectures, which in turn require huge amounts of training
    samples (see Section [8](#S8 "8 Data Sources for Training Autonomous Driving Systems
    ‣ A Survey of Deep Learning Techniques for Autonomous Driving")).
  prefs: []
  type: TYPE_NORMAL
- en: End2End control papers mainly employ either deep neural networks trained offline
    on real-world and/or synthetic data [[119](#bib.bib119)], [[113](#bib.bib113)], [[114](#bib.bib114)], [[115](#bib.bib115)], [[120](#bib.bib120)], [[116](#bib.bib116)], [[117](#bib.bib117)], [[121](#bib.bib121)], [[118](#bib.bib118)],
    or Deep Reinforcement Learning (DRL) systems trained and evaluated in simulation [[23](#bib.bib23)] [[122](#bib.bib122)], [[26](#bib.bib26)].
    Methods for porting simulation trained DRL models to real-world driving have also
    been reported [[123](#bib.bib123)], as well as DRL systems trained directly on
    real-world image data [[105](#bib.bib105)], [[106](#bib.bib106)].
  prefs: []
  type: TYPE_NORMAL
- en: End2End methods have been popularized in the last couple of years by NVIDIA^®,
    as part of the PilotNet architecture. The approach is to train a CNN which maps
    raw pixels from a single front-facing camera directly to steering commands [[119](#bib.bib119)].
    The training data is composed of images and steering commands collected in driving
    scenarios performed in a diverse set of lighting and weather conditions, as well
    as on different road types. Prior to training, the data is enriched using augmentation,
    adding artificial shifts and rotations to the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'PilotNet has $250.000$ parameters and approx. $27mil.$ connections. The evaluation
    is performed in two stages: first in simulation and secondly in a test car. An
    autonomy performance metric represents the percentage of time when the neural
    network drives the car:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $autonomy=(1-\frac{(no.\;of\;interventions)*6\;sec}{elapsed\;time\;[sec]})*100.$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: An intervention is considered to take place when the simulated vehicle departs
    from the center line by more than one meter, assuming that $6$ seconds is the
    time needed by a human to retake control of the vehicle and bring it back to the
    desired state. An autonomy of $98\%$ was reached on a $20km$ drive from Holmdel
    to Atlantic Highlands in NJ, USA. Through training, PilotNet learns how the steering
    commands are computed by a human driver [[112](#bib.bib112)]. The focus is on
    determining which elements in the input traffic image have the most influence
    on the network’s steering decision. A method for finding the salient object regions
    in the input image is described, while reaching the conclusion that the low-level
    features learned by PilotNet are similar to the ones that are relevant to a human
    driver.
  prefs: []
  type: TYPE_NORMAL
- en: End2End architectures similar to PilotNet, which map visual data to steering
    commands, have been reported in [[116](#bib.bib116)], [[117](#bib.bib117)], [[121](#bib.bib121)].
    In [[113](#bib.bib113)], autonomous driving is formulated as a future ego-motion
    prediction problem. The introduced FCN-LSTM (Fully Convolutional Network - Long-Short
    Term Memory) method is designed to jointly train pixel-level supervised tasks
    using a fully convolutional encoder, together with motion prediction through a
    temporal encoder. The combination between visual temporal dependencies of the
    input data has also been considered in [[114](#bib.bib114)], where the C-LSTM
    (Convolutional Long Short Term Memory) network has been proposed for steering
    control. In [[115](#bib.bib115)], surround-view cameras were used for End2End
    learning. The claim is that human drivers also use rear and side-view mirrors
    for driving, thus all the information from around the vehicle needs to be gathered
    and integrated into the network model in order to output a suitable control command.
  prefs: []
  type: TYPE_NORMAL
- en: To carry out an evaluation of the Tesla^® Autopilot system, [[120](#bib.bib120)]
    proposed an End2End Convolutional Neural Network framework. It is designed to
    determine differences between Autopilot and its own output, taking into consideration
    edge cases. The network was trained using real data, collected from over $420$
    hours of real road driving. The comparison between Tesla^®’s Autopilot and the
    proposed framework was done in real-time on a Tesla^® car. The evaluation revealed
    an accuracy of $90.4\%$ in detecting differences between both systems and the
    control transfer of the car to a human driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach to design End2End driving systems is DRL. This is mainly performed
    in simulation, where an autonomous agent can safely explore different driving
    strategies. In [[23](#bib.bib23)], a DRL End2End system is used to compute steering
    command in the TORCS game simulation engine. Considering a more complex virtual
    environment, [[122](#bib.bib122)] proposed an asynchronous advantage Actor-Critic
    (A3C) method for training a CNN on images and vehicle velocity information. The
    same idea has been enhanced in [[26](#bib.bib26)], having a faster convergence
    and permissiveness for more generalization. Both articles rely on the following
    procedure: receiving the current state of the game, deciding on the next control
    commands and then getting a reward on the next iteration. The experimental setup
    benefited from a realistic car game, namely World Rally Championship 6, and also
    from other simulated environments, like TORCS.'
  prefs: []
  type: TYPE_NORMAL
- en: The next trend in DRL based control seems to be the inclusion of classical model-based
    control techniques, as the ones detailed in Section [6.1](#S6.SS1 "6.1 Learning
    Controllers ‣ 6 Motion Controllers for AI-based Self-Driving Cars ‣ A Survey of
    Deep Learning Techniques for Autonomous Driving"). The classical controller provides
    a stable and deterministic model on top of which the policy of the neural network
    is estimated. In this way, the hard constraints of the modeled system are transfered
    into the neural network policy [[124](#bib.bib124)]. A DRL policy trained on real-world
    image data has been proposed in [[105](#bib.bib105)] and [[106](#bib.bib106)]
    for the task of aggressive driving. In this case, a CNN, refereed to as the learner,
    is trained with optimal trajectory examples provided at training time by a model
    predictive controller.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Safety of Deep Learning in Autonomous Driving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Safety implies the absence of the conditions that cause a system to be dangerous [[125](#bib.bib125)].
    Demonstrating the safety of a system which is running deep learning techniques
    depends heavily on the type of technique and the application context. Thus, reasoning
    about the safety of deep learning techniques requires:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: understanding the impact of the possible failures;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: understanding the context within the wider system;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: defining the assumption regarding the system context and the environment in
    which it will likely be used;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: defining what a safe behavior means, including non-functional constraints.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In [[126](#bib.bib126)], an example is mapped on the above requirements with
    respect to a deep learning component. The problem space for the component is pedestrian
    detection with convolutional neural networks. The top level task of the system
    is to locate an object of class person from a distance of 100 meters, with a lateral
    accuracy of +/- 20 cm, a false negative rate of 1% and false positive rate of
    5%. The assumptions is that the braking distance and speed are sufficient to react
    when detecting persons which are 100 meters ahead of the planned trajectory of
    the vehicle. Alternative sensing methods can be used in order to reduce the overall
    false negative and false positive rates of the system to an acceptable level.
    The context information is that the distance and the accuracy shall be mapped
    to the dimensions of the image frames presented to the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: There is no commonly agreed definition for the term safety in the context of
    machine learning or deep learning. In [[127](#bib.bib127)], Varshney defines safety
    in terms of risk, epistemic uncertainty and the harm incurred by unwanted outcomes.
    He then analyses the choice of cost function and the appropriateness of minimizing
    the empirical average training cost.
  prefs: []
  type: TYPE_NORMAL
- en: '[[128](#bib.bib128)] takes into consideration the problem of accidents in machine
    learning systems. Such accidents are defined as unintended and harmful behaviors
    that may emerge from a poor AI system design. The authors present a list of five
    practical research problems related to accident risk, categorized according to
    whether the problem originates from having the wrong objective function (avoiding
    side effects and avoiding reward hacking), an objective function that is too expensive
    to evaluate frequently (scalable supervision), or undesirable behavior during
    the learning process (safe exploration and distributional shift).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enlarging the scope of safety, [[129](#bib.bib129)] propose a decision-theoretic
    definition of safety that applies to a broad set of domains and systems. They
    define safety to be the reduction or minimization of risk and epistemic uncertainty
    associated with unwanted outcomes that are severe enough to be seen as harmful.
    The key points in this definition are: i) the cost of unwanted outcomes has to
    be sufficiently high in some human sense for events to be harmful, and ii) safety
    involves reducing both the probability of expected harms, as well as the possibility
    of unexpected harms.'
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the above empirical definitions and possible interpretations of
    safety, the use of deep learning components in safety critical systems is still
    an open question. The ISO 26262 standard for functional safety of road vehicles
    provides a comprehensive set of requirements for assuring safety, but does not
    address the unique characteristics of deep learning-based software.
  prefs: []
  type: TYPE_NORMAL
- en: '[[130](#bib.bib130)] addresses this gap by analyzing the places where machine
    learning can impact the standard and provides recommendations on how to accommodate
    this impact. These recommendations are focused towards the direction of identifying
    the hazards, implementing tools and mechanism for fault and failure situations,
    but also ensuring complete training datasets and designing a multi-level architecture.
    The usage of specific techniques for various stages within the software development
    life-cycle is desired.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard ISO 26262 recommends the use of a Hazard Analysis and Risk Assessment
    (HARA) method to identify hazardous events in the system and to specify safety
    goals that mitigate the hazards. The standard has 10 parts. Our focus is on Part
    6: product development at the software level, the standard following the well-known
    V model for engineering. Automotive Safety Integrity Level (ASIL) refers to a
    risk classification scheme defined in ISO 26262 for an item (e.g. subsystem) in
    an automotive system.'
  prefs: []
  type: TYPE_NORMAL
- en: ASIL represents the degree of rigor required (e.g., testing techniques, types
    of documentation required, etc.) to reduce risk, where ASIL D represents the highest
    and ASIL A the lowest risk. If an element is assigned to QM (Quality Management),
    it does not require safety management. The ASIL assessed for a given hazard is
    at first assigned to the safety goal set to address the hazard and is then inherited
    by the safety requirements derived from that goal [[130](#bib.bib130)].
  prefs: []
  type: TYPE_NORMAL
- en: According to ISO26226, a hazard is defined as ”potential source of harm caused
    by a malfunctioning behavior, where harm is a physical injury or damage to the
    health of a person” [[131](#bib.bib131)]. Nevertheless, a deep learning component
    can create new types of hazards. An example of such a hazard is usually happening
    because humans think that the automated driver assistance (often developed using
    learning techniques) is more reliable than it actually is [[132](#bib.bib132)].
  prefs: []
  type: TYPE_NORMAL
- en: Due to its complexity, a deep learning component can fail in unique ways. For
    example, in Deep Reinforcement Learning systems, faults in the reward function
    can negatively affect the trained model [[128](#bib.bib128)]. In such a case,
    the automated vehicle figures out that it can avoid getting penalized for driving
    too close to other vehicles by exploiting certain sensor vulnerabilities so that
    it can’t see how close it is getting. Although hazards such as these may be unique
    to deep reinforcement learning components, they can be traced to faults, thus
    fitting within the existing guidelines of ISO 26262.
  prefs: []
  type: TYPE_NORMAL
- en: 'A key requirement for analyzing the safety of deep learning components is to
    examine whether immediate human costs of outcomes exceed some harm severity thresholds.
    Undesired outcomes are truly harmful in a human sense and their effect is felt
    in near real-time. These outcomes can be classified as safety issues. The cost
    of deep learning decisions is related to optimization formulations which explicitly
    include a loss function $L$. The loss function $L:X\times Y\times Y\textrightarrow\rightarrow
    R$ is defined as the measure of the error incurred by predicting the label of
    an observation $x$ as $f(x)$, instead of $y$. Statistical learning calls the risk
    of $f$ as the expected value of the loss of $f$ under $P$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $R(f)=\int L(x,f(x),y)dP(x,y),$ |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: 'where, $X\times Y$ is a random example space of observations $x$ and labels
    $y$, distributed according to a probability distribution $P(X,Y)$. The statistical
    learning problem consists of finding the function $f$ that optimizes (i.e. minimizes)
    the risk $R$ [[133](#bib.bib133)]. For an algorithm’s hypothesis $h$ and loss
    function $L$, the expected loss on the training set is called the empirical risk
    of $h$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{R}_{emp}(h)=\frac{1}{m}\sum\limits_{i=1}^{m}L(x^{(i)},h(x)^{(i)},y^{(i)}).$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: A machine learning algorithm then optimizes the empirical risk on the expectation
    that the risk decreases significantly. However, this standard formulation does
    not consider the issues related to the uncertainty that is relevant for safety.
    The distribution of the training samples ${(x_{1},y_{1}),...,(x_{m},y_{m})}$ is
    drawn from the true underlying probability distribution of $(X,Y)$, which may
    not always be the case. Usually the probability distribution is unknown, precluding
    the use of domain adaptation techniques [[134](#bib.bib134)] [[135](#bib.bib135)].
    This is one of the epistemic uncertainty that is relevant for safety because training
    on a dataset of different distribution can cause much harm through bias.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, a machine learning system only encounters a finite number of test
    samples and an actual operational risk is an empirical quantity on the test set.
    The operational risk may be much larger than the actual risk for small cardinality
    test sets, even if $h$ is risk-optimal. This uncertainty caused by the instantiation
    of the test set can have large safety implications on individual test samples [[136](#bib.bib136)].
  prefs: []
  type: TYPE_NORMAL
- en: Faults and failures of a programmed component (e.g. one using a formal algorithm
    to solve a problem) are totally different from the ones of a deep learning component.
    Specific faults of a deep learning component can be caused by unreliable or noisy
    sensor signals (video signal due to bad weather, radar signal due to absorbing
    construction materials, GPS data, etc.), neural network topology, learning algorithm,
    training set or unexpected changes in the environment (e.g. unknown driving scenes
    or accidents on the road). We must mention the first autonomous driving accident,
    produced by a Tesla^® car, where, due to object misclassification errors, the
    AutoPilot function collided the vehicle into a truck [[137](#bib.bib137)]. Despite
    the 130 million miles of testing and evaluation, the accident was caused under
    extremely rare circumstances, also known as Black Swans, given the height of the
    truck, its white color under bright sky, combined with the positioning of the
    vehicle across the road.
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving vehicles must have fail-safe mechanisms, usually encountered under
    the name of Safety Monitors. These must stop the autonomous control software once
    a failure is detected [[138](#bib.bib138)]. Specific fault types and failures
    have been cataloged for neural networks in [[139](#bib.bib139)], [[140](#bib.bib140)]
    and [[141](#bib.bib141)]. This led to the development of specific and focused
    tools and techniques to help finding faults. [[142](#bib.bib142)] describes a
    technique for debugging misclassifications due to bad training data, while an
    approach for troubleshooting faults due to complex interactions between linked
    machine learning components is proposed in [[143](#bib.bib143)]. In [[144](#bib.bib144)],
    a white box technique is used to inject faults onto a neural network by breaking
    the links or randomly changing the weights.
  prefs: []
  type: TYPE_NORMAL
- en: The training set plays a key role in the safety of the deep learning component.
    ISO 26262 standard states that the component behavior shall be fully specified
    and each refinement shall be verified with respect to its specification. This
    assumption is violated in the case of a deep learning system, where a training
    set is used instead of a specification. It is not clear how to ensure that the
    corresponding hazards are always mitigated. The training process is not a verification
    process since the trained model will be “correct by construction” with respect
    to the training set, up to the limits of the model and the learning algorithm [[130](#bib.bib130)].
    Effects of this considerations are visible in the commercial autonomous vehicle
    market, where Black Swan events caused by data not present in the training set
    may lead to fatalities [[141](#bib.bib141)].
  prefs: []
  type: TYPE_NORMAL
- en: Detailed requirements shall be formulated and traced to hazards. Such a requirement
    can specify how the training, validation and testing sets are obtained. Subsequently,
    the data gathered can be verified with respect to this specification. Furthermore,
    some specifications, for example the fact that a vehicle cannot be wider than
    3 meters, can be used to reject false positive detections. Such properties are
    used even directly during the training process to improve the accuracy of the
    model [[145](#bib.bib145)].
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning and deep learning techniques are starting to become effective
    and reliable even for safety critical systems, even if the complete safety assurance
    for this type of systems is still an open question. Current standards and regulation
    from the automotive industry cannot be fully mapped to such systems, requiring
    the development of new safety standards targeted for deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Data Sources for Training Autonomous Driving Systems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Undeniably, the usage of real world data is a key requirement for training and
    testing an autonomous driving component. The high amount of data needed in the
    development stage of such components made data collection on public roads a valuable
    activity. In order to obtain a comprehensive description of the driving scene,
    the vehicle used for data collection is equipped with a variety of sensors such
    as radar, LIDAR, GPS, cameras, Inertial Measurement Units (IMU) and ultrasonic
    sensors. The sensors setup differs from vehicle to vehicle, depending on how the
    data is planned to be used. A common sensor setup for an autonomous vehicle is
    presented in Fig. [7](#S8.F7 "Figure 7 ‣ 8 Data Sources for Training Autonomous
    Driving Systems ‣ A Survey of Deep Learning Techniques for Autonomous Driving").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/70619ce1a90cdcb899377e8dd73e5428.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Sensor suite of the nuTonomy^® self-driving car [[146](#bib.bib146)].'
  prefs: []
  type: TYPE_NORMAL
- en: In the last years, mainly due to the large and increasing research interest
    in autonomous vehicles, many driving datasets were made public and documented.
    They vary in size, sensor setup and data format. The researchers need only to
    identify the proper dataset which best fits their problem space. [[29](#bib.bib29)]
    published a survey on a broad spectrum of datasets. These datasets address the
    computer vision field in general, but there are few of them which fit to the autonomous
    driving topic.
  prefs: []
  type: TYPE_NORMAL
- en: A most comprehensive survey on publicly available datasets for self-driving
    vehicles algorithms can be found in [[147](#bib.bib147)]. The paper presents 27
    available datasets containing data recorded on public roads. The datasets are
    compared from different perspectives, such that the reader can select the one
    best suited for his task.
  prefs: []
  type: TYPE_NORMAL
- en: Despite our extensive search, we are yet to find a master dataset that combines
    at least parts of the ones available. The reason may be that there are no standard
    requirements for the data format and sensor setup. Each dataset heavily depends
    on the objective of the algorithm for which the data was collected. Recently,
    the companies Scale^® and nuTonomy^® started to create one of the largest and
    most detailed self-driving dataset on the market to date⁶⁶6[https://venturebeat.com/2018/09/14/scale-and-nutonomy-release-nuscenes-a-self-driving-dataset-with-over-1-4-million-images/](https://venturebeat.com/2018/09/14/scale-and-nutonomy-release-nuscenes-a-self-driving-dataset-with-over-1-4-million-images/).
    This includes Berkeley DeepDrive [[148](#bib.bib148)], a dataset developed by
    researchers at Berkeley University. More relevant datasets from the literature
    are pending for merging⁷⁷7[https://scale.com/open-datasets](https://scale.com/open-datasets).
  prefs: []
  type: TYPE_NORMAL
- en: In [[120](#bib.bib120)], the authors present a study that seeks to collect and
    analyze large scale naturalistic data of semi-autonomous driving in order to better
    characterize the state of the art of the current technology. The study involved
    $99$ participants, $29$ vehicles, $405,807$ miles and approximatively $5.5$ billion
    video frames. Unfortunately, the data collected in this study is not available
    for the public.
  prefs: []
  type: TYPE_NORMAL
- en: In the remaining of this section we will provide and highlight the distinctive
    characteristics of the most relevant datasets that are publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Problem Space | Sensor setup | Size | Location |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Traffic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; condition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| License |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; NuScenes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[146](#bib.bib146)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D tracking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Radar, Lidar, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EgoData, GPS, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IMU, Camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 345 GB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (1000 scenes, clips of 20s) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Boston, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Singapore &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Urban | CC BY-NC-SA 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AMUSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[149](#bib.bib149)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SLAM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Omnidirectional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera, IMU, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EgoData, GPS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1 TB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (7 clips) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Los Angeles | Urban | CC BY-NC-ND 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ford &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[150](#bib.bib150)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D tracking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D object detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Omnidirectional &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera, IMU, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lidar, GPS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 100 GB | Michigan | Urban | Not specified |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; KITTI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[151](#bib.bib151)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D tracking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D object detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SLAM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Monocular &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras, IMU &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lidar, GPS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 180 GB | Karlsruhe |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Urban &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CC BY-NC-SA 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Udacity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[152](#bib.bib152)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D tracking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D object detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Monocular &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras, IMU, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lidar, GPS, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EgoData &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 220 GB | Mountain View | Rural | MIT |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Cityscapes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[74](#bib.bib74)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semantic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; understanding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Color stereo &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 63 GB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (5 clips) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Darmstadt, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Zurich, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Strasbourg &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Urban | CC BY-NC-SA 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Oxford &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[153](#bib.bib153)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D tracking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3D object detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SLAM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Stereo and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; monocular &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras, GPS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lidar, IMU &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 23 TB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (133 clips) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Oxford |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Urban, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Highway &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| CC BY-NC-SA 3.0 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CamVid &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[154](#bib.bib154)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Monocular &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; color &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 8 GB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (4 clips) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cambridge | Urban | N/A |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Daimler &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pedestrian &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[155](#bib.bib155)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pedestrian detection, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Classification, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Path prediction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Stereo and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; monocular &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cameras &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91 GB &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (8 clips) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Amsterdam, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Beijing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Urban | N/A |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Caltech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[156](#bib.bib156)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Tracking, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Monocular &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; camera &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 11 GB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Los Angeles &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (USA) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Urban | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Summary of datasets for training autonomous driving systems'
  prefs: []
  type: TYPE_NORMAL
- en: KITTI Vision Benchmark dataset (KITTI) [[151](#bib.bib151)]. Provided by the
    Karlsruhe Institute of Technology (KIT) from Germany, this dataset fits the challenges
    of benchmarking stereo-vision, optical flow, 3D tracking, 3D object detection
    or SLAM algorithms. It is known as the most prestigious dataset in the self-driving
    vehicles domain. To this date it counts more than 2000 citations in the literature.
    The data collection vehicle is equipped with multiple high-resolution color and
    gray-scale stereo cameras, a Velodyne 3D LiDAR and high-precision GPS/IMU sensors.
    In total, it provides 6 hours of driving data collected in both rural and highway
    traffic scenarios around Karlsruhe. The dataset is provided under the Creative
    Commons Attribution-NonCommercial-ShareAlike 3.0 License.
  prefs: []
  type: TYPE_NORMAL
- en: NuScenes dataset [[146](#bib.bib146)]. Constructed by nuTonomy, this dataset
    contains 1000 driving scenes collected from Boston and Singapore, two known for
    their dense traffic and highly challenging driving situations. In order to facilitate
    common computer vision tasks, such as object detection and tracking, the providers
    annotated 25 object classes with accurate 3D bounding boxes at 2Hz over the entire
    dataset. Collection of vehicle data is still in progress. The final dataset will
    include approximately 1,4 million camera images, 400.000 Lidar sweeps, 1,3 million
    RADAR sweeps and 1,1 million object bounding boxes in 40.000 keyframes. The dataset
    is provided under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0
    License license.
  prefs: []
  type: TYPE_NORMAL
- en: Automotive multi-sensor dataset (AMUSE) [[149](#bib.bib149)]. Provided by Linköping
    University of Sweden, it consists of sequences recorded in various environments
    from a car equipped with an omnidirectional multi-camera, height sensors, an IMU,
    a velocity sensor and a GPS. The API for reading these data sets is provided to
    the public, together with a collection of long multi-sensor and multi-camera data
    streams stored in the given format. The dataset is provided under the Creative
    Commons Attribution-NonCommercial-NoDerivs 3.0 Unsupported License.
  prefs: []
  type: TYPE_NORMAL
- en: Ford campus vision and lidar dataset (Ford) [[150](#bib.bib150)]. Provided by
    University of Michigan, this dataset was collected using a Ford F250 pickup truck
    equipped with professional (Applanix POS-LV) and a consumer (Xsens MTi-G) inertial
    measurement units (IMU), a Velodyne Lidar scanner, two push-broom forward looking
    Riegl Lidars and a Point Grey Ladybug3 omnidirectional camera system. The approx.
    100 GB of data was recorded around the Ford Research campus and downtown Dearborn,
    Michigan in 2009\. The dataset is well suited to test various autonomous driving
    and simultaneous localization and mapping (SLAM) algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Udacity dataset [[152](#bib.bib152)]. The vehicle sensor setup contains monocular
    color cameras, GPS and IMU sensors, as well as a Velodyne 3D Lidar. The size of
    the dataset is 223GB. The data is labeled and the user is provided with the corresponding
    steering angle that was recorded during the test runs by the human driver.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cityscapes dataset[[74](#bib.bib74)]. Provided by Daimler AG R&D, Germany;
    Max Planck Institute for Informatics (MPI-IS), Germany, TU Darmstadt Visual Inference
    Group, Germany, the Cityscapes Dataset focuses on semantic understanding of urban
    street scenes, this being the reason for which it contains only stereo vision
    color images. The diversity of the images is very large: 50 cities, different
    seasons (spring, summer, fall), various weather conditions and different scene
    dynamics. There are 5000 images with fine annotations and 20000 images with coarse
    annotations. Two important challenges have used this dataset for benchmarking
    the development of algorithms for semantic segmentation [[157](#bib.bib157)] and
    instance segmentation [[158](#bib.bib158)].'
  prefs: []
  type: TYPE_NORMAL
- en: The Oxford dataset [[153](#bib.bib153)]. Provided by Oxford University, UK,
    the dataset collection spanned over 1 year, resulting in over 1000 km of recorded
    driving with almost 20 million images collected from 6 cameras mounted to the
    vehicle, along with LIDAR, GPS and INS ground truth. Data was collected in all
    weather conditions, including heavy rain, night, direct sunlight and snow. One
    of the particularities of this dataset is that the vehicle frequently drove the
    same route over the period of a year to enable researchers to investigate long-term
    localization and mapping for autonomous vehicles in real-world, dynamic urban
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: The Cambridge-driving Labeled Video Dataset (CamVid) [[154](#bib.bib154)]. Provided
    by the University of Cambridge, UK, it is one of the most cited dataset from the
    literature and the first released publicly, containing a collection of videos
    with object class semantic labels, along with metadata annotations. The database
    provides ground truth labels that associate each pixel with one of 32 semantic
    classes. The sensor setup is based on only one monocular camera mounted on the
    dashboard of the vehicle. The complexity of the scenes is quite low, the vehicle
    being driven only in urban areas with relatively low traffic and good weather
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: The Daimler pedestrian benchmark dataset [[155](#bib.bib155)]. Provided by Daimler
    AG R&D and University of Amsterdam, this dataset fits the topics of pedestrian
    detection, classification, segmentation and path prediction. Pedestrian data is
    observed from a traffic vehicle by using only on-board mono and stereo cameras.
    It is the first dataset with contains pedestrians. Recently, the dataset was extended
    with cyclist video samples captured with the same setup [[159](#bib.bib159)].
  prefs: []
  type: TYPE_NORMAL
- en: Caltech pedestrian detection dataset (Caltech) [[156](#bib.bib156)]. Provided
    by California Institute of Technology, US, the dataset contains richly annotated
    videos, recorded from a moving vehicle, with challenging images of low resolution
    and frequently occluded people. There are approx. 10 hours of driving scenarios
    cumulating about 250.000 frames with a total of 350 thousand bounding boxes and
    2.300 unique pedestrians annotations. The annotations include both temporal correspondences
    between bounding boxes and detailed occlusion labels.
  prefs: []
  type: TYPE_NORMAL
- en: Given the variety and complexity of the available databases, choosing one or
    more to develop and test an autonomous driving component may be difficult. As
    it can be observed, the sensor setup varies among all the available databases.
    For localization and vehicle motion, the Lidar and GPS/IMU sensors are necessary,
    with the most popular Lidar sensors used being Velodyne [[160](#bib.bib160)] and
    Sick [[161](#bib.bib161)]. Data recorded from a radar sensor is present only in
    the NuScenes dataset. The radar manufacturers adopt proprietary data formats which
    are not public. Almost all available datasets include images captured from a video
    camera, while there is a balance use of monocular and stereo cameras mainly configured
    to capture gray-scale images. AMUSE and Ford databases are the only ones that
    use omnidirectional cameras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides raw recorded data, the datasets usually contain miscellaneous files
    such as annotations, calibration files, labels, etc. In order to cope with this
    files, the dataset provider must offer tools and software that enable the user
    to read and post-process the data. Splitting of the datasets is also an important
    factor to consider, because some of the datasets (e.g. Caltech, Daimler, Cityscapes)
    already provide pre-processed data that is classified in different sets: training,
    testing and validation. This enables benchmarking of desired algorithms against
    similar approaches to be consistent.'
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect to consider is the license type. The most commonly used license
    is Creative Commons Attribution-NonCommercial-ShareAlike 3.0\. It allows the user
    to copy and redistribute in any medium or format and also to remix, transform,
    and build upon the material. KITTI and NuScenes databases are examples of such
    distribution license. The Oxford database uses a Creative Commons Attribution-Noncommercial
    4.0\. which, compared with the first license type, does not force the user to
    distribute his contributions under the same license as the database. Opposite
    to that, the AMUSE database is licensed under Creative Commons Attribution-Noncommercial-noDerivs
    3.0 which makes the database illegal to distribute if modification of the material
    are made.
  prefs: []
  type: TYPE_NORMAL
- en: With very few exceptions, the datasets are collected from a single city, which
    is usually around university campuses or company locations in Europe, the US,
    or Asia. Germany is the most active country for driving recording vehicles. Unfortunately,
    all available datasets together cover a very small portion of the world map. One
    reason for this is the memory size of the data which is in direct relation with
    the sensor setup and the quality. For example, the Ford dataset takes around 30
    GB for each driven kilometer, which means that covering an entire city will take
    hundreds of TeraBytes of driving data. The majority of the available datasets
    consider sunny, daylight and urban conditions, these being ideal operating conditions
    for autonomous driving systems.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Computational Hardware and Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deploying deep learning algorithms on target edge devices is not a trivial task.
    The main limitations when it comes to vehicles are the price, performance issues
    and power consumption. Therefore, embedded platforms are becoming essential for
    integration of AI algorithms inside vehicles due to their portability, versatility,
    and energy efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The market leader in providing hardware solutions for deploying deep learning
    algorithms inside autonomous cars is NVIDIA^®. DRIVE PX [[162](#bib.bib162)] is
    an AI car computer which was designed to enable the auto-makers to focus directly
    on the software for autonomous vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: The newest version of DrivePX architecture is based on two Tegra X2 [[163](#bib.bib163)]
    systems on a chip (SoCs). Each SoC contains two Denve [[164](#bib.bib164)] cores,
    4 ARM A57 cores and a graphical computeing unit (GPU) from the Pascal [[165](#bib.bib165)]
    generation. NVIDIA^® DRIVE PX is capable to perform real-time environment perception,
    path planning and localization. It combines deep learning, sensor fusion and surround
    vision to improve the driving experience.
  prefs: []
  type: TYPE_NORMAL
- en: Introduced in September 2018, NVIDIA^® DRIVE AGX developer kit platform was
    presented as the world’s most advanced self-driving car platform [[166](#bib.bib166)],
    being based on the Volta technology [[167](#bib.bib167)]. It is available in two
    different configurations, namely DRIVE AGX Xavier and DRIVE AGX Pegasus.
  prefs: []
  type: TYPE_NORMAL
- en: DRIVE AGX Xavier is a scalable open platform that can serve as an AI brain for
    self driving vehicles, and is an energy-efficient computing platform, with 30
    trillion operations per second, while meeting automotive standards like the ISO
    26262 functional safety specification. NVIDIA^® DRIVE AGX Pegasus improves the
    performance with an architecture which is built on two NVIDIA^® Xavier processors
    and two state of the art TensorCore GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: A hardware platform used by the car makers for Advanced Driver Assistance Systems
    (ADAS) is the R-Car V3H system-on-chip (SoC) platform from Renesas Autonomy [[168](#bib.bib168)].
    This SoC provides the possibility to implement high performance computer vision
    with low power consumption. R-Car V3H is optimized for applications that involve
    the usage of stereo cameras, containing dedicated hardware for convolutional neural
    networks, dense optical flow, stereo-vision, and object classification. The hardware
    features four 1.0 GHz Arm Cortex-A53 MPCore cores, which makes R-Car V3H a suitable
    hardware platform which can be used to deploy trained inference engines for solving
    specific deep learning tasks inside the automotive domain.
  prefs: []
  type: TYPE_NORMAL
- en: Renesas also provides a similar SoC, called R-Car H3 [[169](#bib.bib169)] which
    delivers improved computing capabilities and compliance with functional safety
    standards. Equipped with new CPU cores (Arm Cortex-A57), it can be used as an
    embedded platform for deploying various deep learning algorithms, compared with
    R-Car V3H, which is only optimized for CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Field-Programmable Gate Array (FPGA) is another viable solution, showing
    great improvements in both performance and power consumption in deep learning
    applications. The suitability of the FPGAs for running deep learning algorithms
    can be analyzed from four major perspectives: efficiency and power, raw computing
    power, flexibility and functional safety. Our study is based on the research published
    by Intel [[170](#bib.bib170)], Microsoft [[171](#bib.bib171)] and UCLA [[172](#bib.bib172)].'
  prefs: []
  type: TYPE_NORMAL
- en: By reducing the latency in deep learning applications, FPGAs provide additional
    raw computing power. The memory bottlenecks, associated with external memory accesses,
    are reduced or even eliminated by the high amount of chip cache memory. In addition,
    FPGAs have the advantages of supporting a full range of data types, together with
    custom user-defined types.
  prefs: []
  type: TYPE_NORMAL
- en: FPGAs are optimized when it comes to efficiency and power consumption. The studies
    presented by manufacturers like Microsoft and Xilinx show that GPUs can consume
    upon ten times more power than FPGAs when processing algorithms with the same
    computation complexity, demonstrating that FPGAs can be a much more suitable solution
    for deep learning applications in the automotive field.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of flexibility, FPGAs are built with multiple architectures, which
    are a mix of hardware programmable resources, digital signal processors and Processor
    Block RAM (BRAM) components. This architecture flexibility is suitable for deep
    and sparse neural networks, which are the state of the art for the current machine
    learning applications. Another advantage is the possibility of connecting to various
    input and output peripheral devices like sensors, network elements and storage
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: In the automotive field, functional safety is one of the most important challenges.
    FPGAs have been designed to meet the safety requirements for a wide range of applications,
    including ADAS. When compared to GPUs, which were originally built for graphics
    and high-performance computing systems, where functional safety is not necessary,
    FPGAs provide a significant advantage in developing driver assistance systems.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Discussion and Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have identified seven major areas that form open challenges in the field
    of autonomous driving. We believe that Deep Learning and Artificial Intelligence
    will play a key role in overcoming these challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Perception: In order for an autonomous car to safely navigate the driving scene,
    it must be able to understand its surroundings. Deep learning is the main technology
    behind a large number of perception systems. Although great progress has been
    reported with respect to accuracy in object detection and recognition [[173](#bib.bib173)],
    current systems are mainly designed to calculate 2D or 3D bounding boxes for a
    couple of trained object classes, or to provide a segmented image of the driving
    environment. Future methods for perception should focus on increasing the levels
    of recognized details, making it possible to perceive and track more objects in
    real-time. Furthermore, additional work is required for bridging the gap between
    image- and LiDAR-based 3D perception [[32](#bib.bib32)], enabling the computer
    vision community to close the current debate on camera vs. LiDAR as main perception
    sensors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Short- to middle-term reasoning: Additional to a robust and accurate perception
    system, an autonomous vehicle should be able to reason its driving behavior over
    a short (milliseconds) to middle (seconds to minutes) time horizon [[82](#bib.bib82)].
    AI and deep learning are promising tools that can be used for the high- and low-level
    path path planning required for navigating the miriad of driving scenarios. Currently,
    the largest portion of papers in deep learning for self-driving cars are focused
    mainly on perception and End2End learning [[81](#bib.bib81), [124](#bib.bib124)].
    Over the next period, we expect deep learning to play a significant role in the
    area of local trajectory estimation and planning. We consider long-term reasoning
    as solved, as provided by navigation systems. These are standard methods for selecting
    a route through the road network, from the car’s current position to destination [[82](#bib.bib82)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Availability of training data: ”Data is the new oil” became lately one of the
    most popular quote in the automotive industry. The effectiveness of deep learning
    systems is directly tied to the availability of training data. As a rule of thumb,
    current deep learning methods are also evaluated based on the quality of training
    data [[29](#bib.bib29)]. The better the quality of the data is, the higher the
    accuracy of the algorithm. The daily data recorded by an autonomous vehicle is
    on the order of petabytes. This poses challenges on the parallelization of the
    training procedure, as well as on the storage infrastructure. Simulation environments
    have been used in the last couple of years for bridging the gap between scarce
    data and the deep learning’s hunger for training examples. There is still a gap
    to be filled between the accuracy of a simulated world and real-world driving.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning corner cases: Most driving scenarios are considered solvable with
    classical methodologies. However, the remaining unsolved scenarios are corner
    cases which, until now, required the reasoning and intelligence of a human driver.
    In order to overcome corner cases, the generalization power of deep learning algorithms
    should be increased. Generalization in deep learning is of special importance
    in learning hazardous situations that can lead to accidents, especially due to
    the fact that training data for such corner cases is scarce. This implies also
    the design of one-shot and low-shot learning methods, that can be trained a reduced
    number of training examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning-based control methods: Classical controllers make use of an a-priori
    model composed of fixed parameters. In a complex case, such as autonomous driving,
    these controllers cannot anticipate all driving situations. The effectiveness
    of deep learning components to adapt based on past experiences can also be used
    to learn the parameters of the car’s control system, thus better approximating
    the underlaying true system model [[174](#bib.bib174), [94](#bib.bib94)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Functional safety: The usage of deep learning in safety-critical systems is
    still an open debate, efforts being made to bring the computational intelligence
    and functional safety communities closer to each other. Current safety standards,
    such as the ISO 26262, do not accommodate machine learning software [[130](#bib.bib130)].
    Although new data-driven design methodologies have been proposed, there are still
    opened issues on the explainability, stability, or classification robustness of
    deep neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-time computing and communication: Finally, real-time requirements have
    to be fulfilled for processing the large amounts of data gathered from the car’s
    sensors suite, as well as for updating the parameters of deep learning systems
    over high-speed communication lines [[170](#bib.bib170)]. These real-time constraints
    can be backed up by advances in semiconductor chips dedicated for self-driving
    cars, as well as by the rise of 5G communication networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Final Notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Autonomous vehicle technology has seen a rapid progress in the past decade,
    especially due to advances in the area of artificial intelligence and deep learning.
    Current AI methodologies are nowadays either used or taken into consideration
    when designing different components for a self-driving car. Deep learning approaches
    have influenced not only the design of traditional perception-planning-action
    pipelines, but have also enabled End2End learning systems, able do directly map
    sensory information to steering commands.
  prefs: []
  type: TYPE_NORMAL
- en: Driverless cars are complex systems which have to safely drive passengers or
    cargo from a starting location to destination. Several challenges are encountered
    with the advent of AI based autonomous vehicles deployment on public roads. A
    major challenge is the difficulty in proving the functional safety of these vehicle,
    given the current formalism and explainability of neural networks. On top of this,
    deep learning systems rely on large training databases and require extensive computational
    hardware.
  prefs: []
  type: TYPE_NORMAL
- en: This paper has provided a survey on deep learning technologies used in autonomous
    driving. The survey of performance and computational requirements serves as a
    reference for system level design of AI based self-driving vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The authors would like to thank Elektrobit Automotive for the infrastructure
    and research support.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classification
    with Deep Convolutional Neural Networks,” in *Advances in Neural Information Processing
    Systems 25*, F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, Eds.   Curran
    Associates, Inc., 2012, pp. 1097–1105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. McGrew, J. Pachocki,
    A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder,
    L. Weng, and W. Zaremba, “Learning Dexterous In-Hand Manipulation,” *CoRR*, vol.
    abs/1808.00177, August 2018\. [Online]. Available: [https://arxiv.org/abs/1808.00177](https://arxiv.org/abs/1808.00177)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Goldberg, *Neural Network Methods for Natural Language Processing*,
    ser. Synthesis Lectures on Human Language Technologies.   Morgan & Claypool, 2017,
    vol. 37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] SAE Committee, “Taxonomy and Definitions for Terms Related to On-road Motor
    Vehicle Automated Driving Systems,” 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] E. Dickmanns and V. Graefe, “Dynamic Monocular Machine Vision,” *Machine
    vision and applications*, vol. 1, pp. 223––240, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] B. Paden, M. Cáp, S. Z. Yong, D. S. Yershov, and E. Frazzoli, “A Survey
    of Motion Planning and Control Techniques for Self-Driving Urban Vehicles,” *IEEE
    Trans. Intelligent Vehicles*, vol. 1, no. 1, pp. 33–55, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based Learning
    Applied to Document Recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, Nov 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Y. Bengio, A. Courville, and P. Vincent, “Representation Learning: A Review
    and New Perspectives,” *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    vol. 35, no. 8, pp. 1798–1828, Aug 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] P. A. Viola and M. J. Jones, “Rapid Object Detection using a Boosted Cascade
    of Simple Features,” in *2001 IEEE Computer Society Conference on Computer Vision
    and Pattern Recognition (CVPR 2001), with CD-ROM, 8-14 December 2001, Kauai, HI,
    USA*, 2001, pp. 511–518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] T. Ojala, M. Pietikäinen, and D. Harwood, “A Comparative Study of Texture
    Measures with Classification Based on Featured Distributions,” *Pattern Recognition*,
    vol. 29, no. 1, pp. 51–59, Jan. 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] N. Dalal and B. Triggs, “Histograms of Oriented Gradients for Human Detection,”
    in *In CVPR*, 2005, pp. 886–893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] D. H. Hubel and T. N.Wiesel, “Shape and Arrangement of Columns in Cat’s
    Striate Cortex,” *The Journal of Physiology*, vol. 165, no. 3, p. 559–568, 1963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. A. Goodale and A. Milner, “Separate Visual Pathways for Perception
    and Action,” *Trends in Neurosciences*, vol. 15, no. 1, pp. 20 – 25, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. E. Rumelhart, J. L. McClelland, and C. PDP Research Group, Eds., *Parallel
    Distributed Processing: Explorations in the Microstructure of Cognition, Vol.
    1: Foundations*.   Cambridge, MA, USA: MIT Press, 1986.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”
    in *3rd Int. Conf. on Learning Representations, ICLR 2015*, San Diego, CA, USA,
    May 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] E. H. J. Duchi and Y. Singer, “Adaptive Subgradient Methods for Online
    Learning and Stochastic Optimization,” *Journal of Machine Learning Research*,
    vol. 12, pp. 2121–2159, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Hochreiter and J. Schmidhuber, “Long Short-term Memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] R. Sutton and A. Barto, *Introduction to Reinforcement Learning*.   MIT
    Press, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] R. Bellman, *Dynamic Programming*.   Princeton University Press, 1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] C. Watkins and P. Dayan, “Q-Learning,” *Machine Learning*, vol. 8, no. 3,
    p. 279–292, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,
    “Human-level Control Through Deep Reinforcement Learning,” *Nature*, vol. 518,
    no. 7540, pp. 529–533, Feb. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney,
    D. Horgan, B. Piot, M. Azar, and D. Silver, “Rainbow: Combining Improvements in
    Deep Reinforcement Learning,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep Reinforcement
    Learning framework for Autonomous Driving,” *CoRR*, vol. abs/1704.02532, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous Control with Deep Reinforcement Learning,”
    2-4 May 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Gu, T. Lillicrap, I. Sutskever, and S. Levine, “Continuous Deep Q-Learning
    with Model-based Acceleration,” in *Int. Conf. on Machine Learning ICML 2016*,
    vol. 48, Jun. 2016, pp. 2829–2838.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] M. Jaritz, R. de Charette, M. Toromanoff, E. Perot, and F. Nashashibi,
    “End-to-End Race Driving with Deep Reinforcement Learning,” *2018 IEEE Int. Conf.
    on Robotics and Automation (ICRA)*, pp. 2070–2075, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Wulfmeier, D. Z. Wang, and I. Posner, “Watch This: Scalable Cost-Function
    Learning for Path Planning in Urban Environments,” *2016 IEEE/RSJ Int. Conf. on
    Intelligent Robots and Systems (IROS)*, vol. abs/1607.02329, 2016. [Online]. Available:
    [http://arxiv.org/abs/1607.02329](http://arxiv.org/abs/1607.02329)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] H. Zhu, K.-V. Yuen, L. S. Mihaylova, and H. Leung, “Overview of Environment
    Perception for Intelligent Vehicles,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 18, pp. 2584–2601, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] J. Janai, F. Guney, A. Behl, and A. Geiger, “Computer Vision for Autonomous
    Vehicles: Problems, Datasets and State-of-the-Art,” 04 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. O’Kane, “How Tesla and Waymo are Tackling a Major Problem for Self-Driving
    Cars: Data,” *Transportation*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Hasirlioglu, A. Kamann, I. Doric, and T. Brandmeier, “Test Methodology
    for Rain Influence on Automotive Surround Sensors,” in *2016 IEEE 19th Int. Conf.
    on Intelligent Transportation Systems (ITSC)*, Nov 2016, pp. 2242–2247.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell, and K. Weinberger,
    “Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection
    for Autonomous Driving,” in *IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR) 2019*, June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep Learning on
    Point Sets for 3D Classification and Segmentation,” in *IEEE Conf. on Computer
    Vision and Pattern Recognition (CVPR) 2017*, July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Ku, M. Mozifian, J. Lee, A. Harakeh, and S. L. Waslander, “Joint 3D
    Proposal Generation and Object Detection from View Aggregation,” in *IEEE/RSJ
    Int. Conf. on Intelligent Robots and Systems (IROS) 2018*.   IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, “ImageNet Large
    Scale Visual Recognition Challenge,” *Int. Journal of Computer Vision (IJCV)*,
    vol. 115, no. 3, pp. 211–252, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You Only Look Once:
    Unified, Real-time Object Detection,” in *Proceedings of the IEEE Conf. on computer
    vision and pattern recognition*, 2016, pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Law and J. Deng, “Cornernet: Detecting Objects as Paired Keypoints,”
    in *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018, pp.
    734–750.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Single-shot Refinement
    Neural Network for Object Detection,” *IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Girshick, “Fast R-CNN,” in *Proceedings of the IEEE Int. Conf. on computer
    vision*, 2015, pp. 1440–1448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally, and K. Keutzer,
    “Squeezenet: Alexnet-level Accuracy with 50x Fewer Parameters and¡ 0.5 Mb Model
    Size,” *arXiv preprint arXiv:1602.07360*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object Detection via Region-based
    Fully Convolutional Networks,” in *Advances in neural information processing systems*,
    2016, pp. 379–387.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A Deep Convolutional
    Encoder-Decoder Architecture for Image Segmentation,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol. 39, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. Zhao, X. Qi, X. Shen, J. Shi, and J. Jia, “Icnet for Real-time Semantic
    Segmentation on High-resolution Images,” *European Conference on Computer Vision*,
    pp. 418–434, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] M. Treml, J. A. Arjona-Medina, T. Unterthiner, R. Durgesh, F. Friedmann,
    P. Schuberth, A. Mayr, M. Heusel, M. Hofmarcher, M. Widrich, B. Nessler, and S. Hochreiter,
    “Speeding up Semantic Segmentation for Autonomous Driving,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] K. He, G. Gkioxari, P. Dollar, and R. B. Girshick, “Mask R-CNN,” *2017
    IEEE Int. Conf. on Computer Vision (ICCV)*, pp. 2980–2988, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Zhou and O. Tuzel, “VoxelNet: End-to-End Learning for Point Cloud Based
    3D Object Detection,” *IEEE Conf. on Computer Vision and Pattern Recognition 2018*,
    pp. 4490–4499, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. Luo, B. Yang, and R. Urtasun, “Fast and Furious: Real Time End-to-End
    3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net,”
    in *IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) 2018*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum PointNets for
    3D Object Detection from RGB-D Data,” in *IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR) 2018*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-View 3D Object Detection
    Network for Autonomous Driving,” in *IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR) 2017*, July 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. Redmon and A. Farhadi, “YOLO9000: Better, Faster, Stronger,” *IEEE
    Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] ——, “Yolov3: An Incremental Improvement,” *arXiv preprint arXiv:1804.02767*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single Shot Multibox Detector,” in *European conference on computer
    vision*.   Springer, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich Feature Hierarchies
    for Accurate Object Detection and Semantic Segmentation,” in *Proceedings of the
    2014 IEEE Conf. on Computer Vision and Pattern Recognition*, ser. CVPR ’14.   Washington,
    DC, USA: IEEE Computer Society, 2014, pp. 580–587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-time
    Object Detection with Region Proposal Networks,” *IEEE Transactions on Pattern
    Analysis & Machine Intelligence*, no. 6, pp. 1137–1149, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Li, K. Peng, and C.-C. Chang, “An Efficient Object Detection Algorithm
    Based on Compressed Networks,” *Symmetry*, vol. 10, no. 7, p. 235, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] K. Shin, Y. P. Kwon, and M. Tomizuka, “RoarNet: A Robust 3D Object Detection
    based on RegiOn Approximation Refinement,” *CoRR*, vol. abs/1811.03818, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A Deep Neural
    Network Architecture for Real-time Semantic Segmentation,” *arXiv preprint arXiv:1606.02147*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] A. Valada, J. Vertens, A. Dhall, and W. Burgard, “AdapNet: Adaptive Semantic
    Segmentation in Adverse Environmental Conditions,” *2017 IEEE Int. Conf. on Robotics
    and Automation (ICRA)*, pp. 4644–4651, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. Simonyan and A. Zisserman, “Very Deep Convolutional Networks for Large-scale
    Image Recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going Deeper with Convolutions,” *IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning for Image
    Recognition,” in *Proceedings of the IEEE Conf. on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. Barnes, W. Maddern, G. Pascoe, and I. Posner, “Driven to Distraction:
    Self-Supervised Distractor Learning for Robust Monocular Visual Odometry in Urban
    Environments,” in *2018 IEEE Int. Conf. on Robotics and Automation (ICRA)*.   IEEE,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] G. Bresson, Z. Alsayed, L. Yu, and S. Glaser, “Simultaneous Localization
    and Mapping: A Survey of Current Trends in Autonomous Driving,” *IEEE Transactions
    on Intelligent Vehicles*, vol. 2, no. 3, pp. 194–220, Sep 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] A. Kendall, M. Grimes, and R. Cipolla, “PoseNet: A Convolutional Network
    for Real-Time 6-DOF Camera Relocalization,” in *Proceedings of the 2015 IEEE Int.
    Conf. on Computer Vision (ICCV)*.   Washington, DC, USA: IEEE Computer Society,
    2015, pp. 2938–2946.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] N. Radwan, A. Valada, and W. Burgard, “VLocNet++: Deep Multitask Learning
    for Semantic Visual Localization and Odometry,” *IEEE Robotics and Automation
    Letters*, Sep 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] F. Walch, C. Hazirbas, L. Leal-Taixé, T. Sattler, S. Hilsenbeck, and D. Cremers,
    “Image-Based Localization Using LSTMs for Structured Feature Correlation,” *2017
    IEEE Int. Conf. on Computer Vision (ICCV)*, pp. 627–637, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] I. Melekhov, J. Ylioinas, J. Kannala, and E. Rahtu, “Image-Based Localization
    Using Hourglass Networks,” *2017 IEEE Int. Conf. on Computer Vision Workshops
    (ICCVW)*, pp. 870–877, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Z. Laskar, I. Melekhov, S. Kalia, and J. Kannala, “Camera Relocalization
    by Computing Pairwise Relative Poses Using Convolutional Neural Network,” in *The
    IEEE Int. Conf. on Computer Vision (ICCV)*, Oct 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] E. Brachmann and C. Rother, “Learning Less is More – 6D Camera Localization
    via 3D Surface Regression,” in *IEEE Conf. on Computer Vision and Pattern Recognition
    (CVPR) 2018*, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] P. Sarlin, F. Debraine, M. Dymczyk, R. Siegwart, and C. Cadena, “Leveraging
    Deep Visual Descriptors for Hierarchical Efficient Localization,” in *Proc. of
    the 2nd Conf. on Robot Learning (CoRL)*, Oct 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] I. A. Barsan, S. Wang, A. Pokrovsky, and R. Urtasun, “Learning to Localize
    Using a LiDAR Intensity Map,” in *Proc. of the 2nd Conf. on Robot Learning (CoRL)*,
    Oct 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] O. Garcia-Favrot and M. Parent, “Laser Scanner Based SLAM in Real Road
    and Traffic Environment,” in *IEEE Int. Conf. Robotics and Automation (ICRA09).
    Workshop on Safe navigation in open and dynamic environments Application to autonomous
    vehicles*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] A. K. Ushani and R. M. Eustice, “Feature Learning for Scene Flow Estimation
    from LIDAR,” in *Proc. of the 2nd Conf. on Robot Learning (CoRL)*, vol. 87, Oct
    2018, pp. 283–292.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Cityscapes, “Cityscapes Data Collection,” [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] S. Thrun, W. Burgard, and D. Fox, “Probabilistic Robotics (Intelligent
    Robotics and Autonomous Agents),” in *Cambridge: The MIT Press*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] P. Ondruska, J. Dequaire, D. Z. Wang, and I. Posner, “End-to-End Tracking
    and Semantic Segmentation Using Recurrent Neural Networks,” *CoRR*, vol. abs/1604.05091,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Hoermann, M. Bach, and K. Dietmayer, “Dynamic Occupancy Grid Prediction
    for Urban Autonomous Driving: Deep Learning Approach with Fully Automatic Labeling,”
    *IEEE Int. Conf. on Robotics and Automation (ICRA)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] S. Ramos, S. K. Gehrig, P. Pinggera, U. Franke, and C. Rother, “Detecting
    Unexpected Obstacles for Self-Driving Cars: Fusing Deep Learning and Geometric
    Modeling,” *IEEE Intelligent Vehicles Symposium*, vol. 4, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Seeger, A. Müller, and L. Schwarz, “Towards Road Type Classification
    with Occupancy Grids,” in *Intelligent Vehicles Symposium - Workshop: DeepDriving
    - Learning Representations for Intelligent Vehicles, IEEE*, Gothenburg, Sweden,
    July 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] L. Marina, B. Trasnea, T. Cocias, A. Vasilcoi, F. Moldoveanu, and S. Grigorescu,
    “Deep Grid Net (DGN): A Deep Learning System for Real-Time Driving Context Understanding,”
    in *Int. Conf. on Robotic Computing IRC 2019*, Naples, Italy, 25-27 February 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Shalev-Shwartz, S. Shammah, and A. Shashua, “Safe, Multi-Agent, Reinforcement
    Learning for Autonomous Driving,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H. Eng,
    D. Rus, and M. H. Ang, “Perception, Planning, Control, and Coordination for Autonomous
    Vehicles,” *Machines*, vol. 5, no. 1, p. 6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] E. Rehder, J. Quehl, and C. Stiller, “Driving Like a Human: Imitation
    Learning for Path Planning using Convolutional Neural Networks,” in *Int. Conf.
    on Robotics and Automation Workshops*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, “A Fast Integrated Planning
    and Control Framework for Autonomous Driving via Imitation Learning,” *ASME 2018
    Dynamic Systems and Control Conference*, vol. 3, 2018. [Online]. Available: [https://arxiv.org/pdf/1707.02515.pdf](https://arxiv.org/pdf/1707.02515.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Grigorescu, B. Trasnea, L. Marina, A. Vasilcoi, and T. Cocias, “NeuroTrajectory:
    A Neuroevolutionary Approach to Local State Trajectory Learning for Autonomous
    Vehicles,” *IEEE Robotics and Automation Letters*, vol. 4, no. 4, pp. 3441–3448,
    October 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] L. Yu, X. Shao, Y. Wei, and K. Zhou, “Intelligent Land-Vehicle Model Transfer
    Trajectory Planning Method Based on Deep Reinforcement Learning,” *Sensors (Basel,
    Switzerland)*, vol. 18, 09 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] C. Paxton, V. Raman, G. D. Hager, and M. Kobilarov, “Combining Neural
    Networks and Tree Search for Task and Motion Planning in Challenging Environments,”
    *2017 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS)*, vol. abs/1703.07887,
    2017\. [Online]. Available: [http://arxiv.org/abs/1703.07887](http://arxiv.org/abs/1703.07887)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] W. Schwarting, J. Alonso-Mora, and D. Rus, “Planning and Decision-Making
    for Autonomous Vehicles,” *Annual Review of Control, Robotics, and Autonomous
    Systems*, vol. 1, 05 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] T. Gu, J. M. Dolan, and J. Lee, “Human-like Planning of Swerve Maneuvers
    for Autonomous Vehicles,” in *2016 IEEE Intelligent Vehicles Symposium (IV)*,
    June 2016, pp. 716–721.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. I. Panov, K. S. Yakovlev, and R. Suvorov, “Grid Path Planning with
    Deep Reinforcement Learning: Preliminary Results,” *Procedia Computer Science*,
    vol. 123, pp. 347 – 353, 2018, 8th Annual Int. Conf. on Biologically Inspired
    Cognitive Architectures, BICA 2017\. [Online]. Available: [http://www.sciencedirect.com/science/article/pii/S1877050918300553](http://www.sciencedirect.com/science/article/pii/S1877050918300553)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] C. J. Ostafew, J. Collier, A. P. Schoellig, and T. D. Barfoot, “Learning-based
    Nonlinear Model Predictive Control to Improve Vision-based Mobile Robot Path Tracking,”
    *Journal of Field Robotics*, vol. 33, no. 1, pp. 133–152, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] P. J. Nguyen-Tuong D and S. M, “Local Gaussian Process Regression for
    Real Time Online Model Learning,” in *Proceedings of the neural information processing
    systems Conference*, 2008, pp. 1193––1200.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. P. Meier F and S. S, “Efficient Bayesian Local Model Learning for Control,”
    in *IEEE/RSJ Int. Conf. on Intelligent Robots and Systems (IROS) 2016*.   IEEE,
    2014, pp. 2244––2249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] C. J. Ostafew, A. P. Schoellig, and T. D. Barfoot, “Robust Constrained
    Learning-Based NMPC Enabling Reliable Mobile Robot Path Tracking,” *Int. Journal
    of Robotics Research*, vol. 35, no. 13, pp. 1547–1563, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] O. Sigaud, C. Salaün, and V. Padois, “On-line Regression Algorithms for
    Learning Mechanical Models of Robots: A Survey,” *Robotics and Autonomous Systems*,
    vol. 59, no. 12, pp. 1115–1129, Dec. 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. Ostafew, A. Schoellig, and T. D. Barfoot, “Visual Teach and Repeat,
    Repeat, Repeat: Iterative Learning Control to Improve Mobile Robot Path Tracking
    in Challenging Outdoor Environments,” 11 2013, pp. 176–181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] B. Panomruttanarug, “Application of Iterative Learning Control in Tracking
    a Dubin’s Path in Parallel Parking,” *Int. Journal of Automotive Technology*,
    vol. 18, no. 6, pp. 1099–1107, Dec 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] N. R. Kapania and J. C. Gerdes, “Path Tracking of Highly Dynamic Autonomous
    Vehicle Trajectories via Iterative Learning Control,” in *2015 American Control
    Conference (ACC)*, July 2015, pp. 2753–2758.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Z. Yang, F. Zhou, Y. Li, and Y. Wang, “A Novel Iterative Learning Path-tracking
    Control for Nonholonomic Mobile Robots Against Initial Shifts,” *Int. Journal
    of Advanced Robotic Systems*, vol. 14, p. 172988141771063, 05 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Lefèvre, A. Carvalho, and F. Borrelli, “A Learning-Based Framework
    for Velocity Control in Autonomous Driving,” *IEEE Transactions on Automation
    Science and Engineering*, vol. 13, no. 1, pp. 32–42, Jan 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] S. Lefevre, A. Carvalho, and F. Borrelli, “Autonomous Car Following:
    A Learning-based Approach,” in *2015 IEEE Intelligent Vehicles Symposium (IV)*,
    June 2015, pp. 920–926.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] P. Drews, G. Williams, B. Goldfain, E. A Theodorou, and J. M Rehg, “Aggressive
    Deep Driving: Combining Convolutional Neural Networks and Model Predictive Control,”
    01 2017, pp. 133–142.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] P. Drews, G. Williams, B. Goldfain, E. A. Theodorou, and J. M. Rehg,
    “Aggressive Deep Driving: Model Predictive Control with a CNN Cost Model,” *CoRR*,
    vol. abs/1707.05303, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] U. Rosolia, A. Carvalho, and F. Borrelli, “Autonomous Racing using Learning
    Model Predictive Control,” in *2017 American Control Conference (ACC)*, May 2017,
    pp. 5115–5120.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. A. Theodorou, and
    B. Boots, “Learning Deep Neural Network Control Policies for Agile Off-Road Autonomous
    Driving,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Y. Pan, C. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and B. Boots,
    “Agile Off-Road Autonomous Driving Using End-to-End Deep Imitation Learning,”
    *Robotics: Science and Systems 2018*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] J. Rawlings and D. Mayne, *Model Predictive Control: Theory and Design*.   Nob
    Hill Pub., 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. Kamel, A. Hafez, and X. Yu, “A Review on Motion Control of Unmanned
    Ground and Aerial Vehicles Based on Model Predictive Control Techniques,” *Engineering
    Science and Military Technologies*, vol. 2, pp. 10–23, 03 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Brunner, U. Rosolia, J. Gonzales, and F. Borrelli, “Repetitive Learning
    Model Predictive Control: An Autonomous Racing Example,” in *2017 IEEE 56th Annual
    Conference on Decision and Control (CDC)*, Dec 2017, pp. 2545–2550.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] D. A. Pomerleau, “Alvinn: An autonomous Land Vehicle in a Neural Network,”
    in *Advances in neural information processing systems*, 1989, pp. 305–313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. L. Cun, “Off-road Obstacle
    Avoidance through End-to-End Learning,” in *Advances in neural information processing
    systems*, 2006, pp. 739–746.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski, B. Firner, L. Jackel,
    and U. Muller, “Explaining How a Deep Neural Network Trained with End-to-End Learning
    Steers a Car,” *arXiv preprint arXiv:1704.07911*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] H. Xu, Y. Gao, F. Yu, and T. Darrell, “End-to-End Learning of Driving
    Models from Large-scale Video Datasets,” *IEEE Conf. on Computer Vision and Pattern
    Recognition (CVPR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. M. Eraqi, M. N. Moustafa, and J. Honer, “End-to-end Deep Learning
    for Steering Autonomous Vehicles Considering Temporal Dependencies,” *arXiv preprint
    arXiv:1710.03804*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Hecker, D. Dai, and L. Van Gool, “End-to-End Learning of Driving Models
    with Surround-view Cameras and Route Planners,” in *European Conference on Computer
    Vision (ECCV)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] V. Rausch, A. Hansen, E. Solowjow, C. Liu, E. Kreuzer, and J. K. Hedrick,
    “Learning a Deep Neural Net Policy for End-to-End Control of Autonomous Vehicles,”
    in *2017 American Control Conference (ACC)*, May 2017, pp. 4914–4919.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M. G. Bechtel, E. McEllhiney, and H. Yun, “DeepPicar: A Low-cost Deep
    Neural Network-based Autonomous Car,” in *The 24th IEEE Inter. Conf. on Embedded
    and Real-Time Computing Systems and Applications (RTCSA)*, August 2018, pp. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] S. Yang, W. Wang, C. Liu, K. Deng, and J. K. Hedrick, “Feature Analysis
    and Selection for Training an End-to-End Autonomous Vehicle Controller Using the
    Deep Learning Approach,” *2017 IEEE Intelligent Vehicles Symposium*, vol. 1, 2017\.
    [Online]. Available: [http://arxiv.org/abs/1703.09744](http://arxiv.org/abs/1703.09744)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal,
    L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba,
    “End to End Learning for Self-Driving Cars,” *CoRR*, vol. abs/1604.07316, 2016\.
    [Online]. Available: [http://arxiv.org/abs/1604.07316](http://arxiv.org/abs/1604.07316)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] L. Fridman, D. E. Brown, M. Glazer, W. Angell, S. Dodd, B. Jenik, J. Terwilliger,
    J. Kindelsberger, L. Ding, S. Seaman, H. Abraham, A. Mehler, A. Sipperley, A. Pettinato,
    L. Angell, B. Mehler, and B. Reimer, “MIT Autonomous Vehicle Technology Study:
    Large-Scale Deep Learning Based Analysis of Driver Behavior and Interaction with
    Automation,” *IEEE Access 2017*, 2017\. [Online]. Available: [https://arxiv.org/abs/1711.06976](https://arxiv.org/abs/1711.06976)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] C. Chen, A. Seff, A. L. Kornhauser, and J. Xiao, “DeepDriving: Learning
    Affordance for Direct Perception in Autonomous Driving,” *2015 IEEE Int. Conf.
    on Computer Vision (ICCV)*, pp. 2722–2730, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] E. Perot, M. Jaritz, M. Toromanoff, and R. D. Charette, “End-to-End Driving
    in a Realistic Racing Game with Deep Reinforcement Learning,” in *2017 IEEE Conf.
    on Computer Vision and Pattern Recognition Workshops (CVPRW)*, July 2017, pp.
    474–475.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Wayve. (2018) Learning to Drive in a Day. [Online]. Available: [https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning](https://wayve.ai/blog/learning-to-drive-in-a-day-with-reinforcement-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] T. Zhang, G. Kahn, S. Levine, and P. Abbeel, “Learning Deep Control Policies
    for Autonomous Aerial Vehicles with MPC-guided Policy Search,” *2016 IEEE Int.
    Conf. on Robotics and Automation (ICRA)*, May 2016\. [Online]. Available: [http://dx.doi.org/10.1109/ICRA.2016.7487175](http://dx.doi.org/10.1109/ICRA.2016.7487175)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] T. Ferrel, “Engineering Safety-critical Systems in the 21st Century,”
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] H. C. Burton S., Gauerhof L., “Making the Case for Safety of Machine
    Learning in Highly Automated Driving,” *Lecture Notes in Computer Science*, vol.
    10489, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] K. R. Varshney, “Engineering Safety in Machine Learning,” in *2016 Information
    Theory and Applications Workshop (ITA)*, Jan 2016, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D. Amodei, C. Olah, J. Steinhardt, P. F. Christiano, J. Schulman, and
    D. Mané, “Concrete Problems in AI Safety,” *CoRR*, vol. abs/1606.06565, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] N. Möller, *The Concepts of Risk and Safety*.   Springer Netherlands,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] R. Salay, R. Queiroz, and K. Czarnecki, “An Analysis of ISO 26262: Using
    Machine Learning Safely in Automotive Software,” *CoRR*, vol. abs/1709.02435,
    2017\. [Online]. Available: [http://arxiv.org/abs/1709.02435](http://arxiv.org/abs/1709.02435)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S. Bernd, R. Detlev, E. Susanne, W. Ulf, B. Wolfgang, Patz, and Carsten,
    “Challenges in Applying the ISO 26262 for Driver Assistance Systems,” in *Schwerpunkt
    Vernetzung, 5\. Tagung Fahrerassistenz*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] R. Parasuraman and V. Riley, “Humans and Automation: Use, Misuse, Disuse,
    Abuse,” *Human Factors*, vol. 39, no. 2, pp. 230–253, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] F. Jose, *Safety-Critical Systems*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] H. Daumé, III and D. Marcu, “Domain Adaptation for Statistical Classifiers,”
    *J. Artif. Int. Res.*, vol. 26, no. 1, pp. 101–126, May 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad, “Intelligible
    Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission,”
    in *Proceedings of the 21th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data
    Mining*, 2015, pp. 1721–1730.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. R. Varshney and H. Alemzadeh, “On the Safety of Machine Learning:
    Cyber-Physical Systems, Decision Sciences, and Data Products,” *Big data*, vol. 5,
    10 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. Levin, “Tesla Fatal Crash: ’Autopilot’ Mode Sped up Car Before Driver
    Killed, Report Finds,” *The Guardian*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] P. Koopman, “Challenges in Autonomous Vehicle Validation: Keynote Presentation
    Abstract,” in *Proceedings of the 1st Int. Workshop on Safe Control of Connected
    and Autonomous Vehicles*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Z. Kurd, T. Kelly, and J. Austin, “Developing Artificial Neural Networks
    for Safety Critical Systems,” *Neural Computing and Applications*, vol. 16, no. 1,
    pp. 11–19, Jan 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] M. Harris, “Google Reports Self-driving Car Mistakes: 272 Failures and
    13 Near Misses,” *The Guardian*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. McPherson, “How Uber’s Self-Driving Technology Could Have Failed In
    The Fatal Tempe Crash,” *Forbes*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. Chakarov, A. Nori, S. Rajamani, S. Sen, and D. Vijaykeerthy, “Debugging
    Machine Learning Tasks,” *arXiv preprint arXiv:1603.07292*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] B. Nushi, E. Kamar, E. Horvitz, and D. Kossmann, “On Human Intellect
    and Machine Failures: Troubleshooting Integrative Machine Learning Systems,” in
    *AAAI*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] I. Takanami, M. Sato, and Y. P. Yang, “A Fault-value Injection Approach
    for Multiple-weight-fault Tolerance of MNNs,” in *Proceedings of the IEEE-INNS-ENNS*,
    2000, pp. 515–520 vol.3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer,
    “Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks,” in *CAV*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuScenes: A multimodal Dataset for Autonomous
    Driving,” *arXiv preprint arXiv:1903.11027*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] H. Yin and C. Berger, “When to Use what Data Set for Your Self-driving
    Car Algorithm: An Overview of Publicly Available Driving Datasets,” in *2017 IEEE
    20th Int. Conf. on Intelligent Transportation Systems (ITSC)*, Oct 2017, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and T. Darrell,
    “BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling,”
    *CoRR*, vol. abs/1805.04687, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] P. Koschorrek, T. Piccini, P. Öberg, M. Felsberg, L. Nielsen, and R. Mester,
    “A Multi-sensor Traffic Scene Dataset with Omnidirectional Video,” in *Ground
    Truth - What is a good dataset? CVPR Workshop 2013*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] G. Pandey, J. R. McBride, and R. M. Eustice, “Ford Campus Vision and
    Lidar Data Set ,” *Int. Journal of Robotics Research*, vol. 30, no. 13, pp. 1543–1552,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision Meets Robotics:
    The KITTI Dataset,” *The Int. Journal of Robotics Research*, vol. 32, no. 11,
    pp. 1231–1237, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Udacity, “Udacity Data Collection,” [http://academictorrents.com/collection/self-driving-cars](http://academictorrents.com/collection/self-driving-cars),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] W. Maddern, G. Pascoe, C. Linegar, and P. Newman, “1 Year, 1000km: The
    Oxford RobotCar Dataset,” *The Int. Journal of Robotics Research (IJRR)*, vol. 36,
    no. 1, pp. 3–15, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] G. J. Brostow, J. Fauqueur, and R. Cipolla, “Semantic Object Classes
    in Video: A High-definition Ground Truth Database,” *Pattern Recognition Letters*,
    vol. 30, pp. 88–97, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] F. Flohr and D. M. Gavrila, “Daimler Pedestrian Segmentation Benchmark
    Dataset,” in *Proc. of the British Machine Vision Conference*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] P. Dollar, C. Wojek, B. Schiele, and P. Perona, “Pedestrian Detection:
    A Benchmark,” in *2009 IEEE Conf. on Computer Vision and Pattern Recognition*,
    2009, pp. 304–311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid Scene Parsing Network,”
    in *2017 IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)*, 2017,
    pp. 6230–6239.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] S. Liu, J. Jia, S. Fidler, and R. Urtasun, “SGN: Sequential Grouping
    Networks for Instance Segmentation,” pp. 3516–3524, 10 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] X. Li, F. Flohr, Y. Yang, H. Xiong, M. Braun, S. Pan, K. Li, and D. M.
    Gavrila, “A New Benchmark for Vision-based Cyclist Detection,” in *2016 IEEE Intelligent
    Vehicles Symposium (IV)*, 2016, pp. 1028–1033.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Velodyne, “Velodyne LiDAR for Data Collection,” [https://velodynelidar.com/](https://velodynelidar.com/),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Sick, “Sick LiDAR for Data Collection,” [https://www.sick.com/](https://www.sick.com/),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] NVIDIA, “NVIDIA AI Car Computer Drive PX,” https://www.nvidia.com/en-au/self-driving-cars/drive-px/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] ——, “Tegra X2,” https://devblogs.nvidia.com/jetson-tx2-delivers-twice-intelligence-edge/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] ——, “Denver Core,” https://en.wikichip.org/wiki/nvidia/microarchitectures/denver.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] ——, “Pascal Microarchitecture,” https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] ——, “NVIDIA Drive AGX,” https://www.nvidia.com/en-us/self-driving-cars/drive-platform/hardware/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] ——, “NVIDIA Volta,” https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Renesas, “R-Car V3H,” https://www.renesas.com/eu/en/solutions/automotive/soc/r-car-v3h.html/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] ——, “R-Car H3,” https://www.renesas.com/sg/en/solutions/automotive/soc/r-car-h3.html/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] E. Nurvitadhi, G. Venkatesh, J. Sim, D. Marr, R. Huang, J. Ong Gee Hock,
    Y. T. Liew, K. Srivatsan, D. Moss, S. Subhaschandra, and G. Boudoukh, “Can FPGAs
    Beat GPUs in Accelerating Next-Generation Deep Neural Networks?” in *Proceedings
    of the 2017 ACM/SIGDA Int. Symposium on Field-Programmable Gate Arrays*, ser.
    FPGA ’17.   New York, NY, USA: ACM, 2017, pp. 5–14\. [Online]. Available: [http://doi.acm.org/10.1145/3020078.3021740](http://doi.acm.org/10.1145/3020078.3021740)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers, K. Strauss, and E. Chung,
    “Accelerating Deep Convolutional Neural Networks Using Specialized Hardware,”
    February 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J. Cong, Z. Fang, M. Lo, H. Wang, J. Xu, and S. Zhang, “Understanding
    Performance Differences of FPGAs and GPUs: (Abtract Only),” in *Proceedings of
    the 2018 ACM/SIGDA Int. Symposium on Field-Programmable Gate Arrays*, ser. FPGA
    ’18.   New York, NY, USA: ACM, 2018, pp. 288–288\. [Online]. Available: [http://doi.acm.org/10.1145/3174243.3174970](http://doi.acm.org/10.1145/3174243.3174970)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object Detection with Deep
    Learning: A Review,” *IEEE transactions on neural networks and learning systems*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] C. J. Ostafew, “Learning-based Control for Autonomous Mobile Robots,”
    Ph.D. dissertation, University of Toronto, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
