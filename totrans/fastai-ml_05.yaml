- en: 'Machine Learning 1: Lesson 5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-5-df45f0c99618](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-5-df45f0c99618)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Video](https://youtu.be/3jl2h9hSRvc)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Review
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Test sets , training sets, validation sets and OOB
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have a dataset with bunch of rows in it and we’ve got some dependent variable.
    What is the difference between machine learning and any other kind of work? The
    difference is that in machine learning, the thing we care about is the generalization
    accuracy or the generalization error where else, in pretty much in everything
    else, all we care about is how well we could map to the observations. So this
    thing about generalization is the key unique piece of machine learning. And if
    we want to know whether we are doing a good job of machine learning, we need to
    know whether we are doing a good job of generalizing. If we don’t know that, we
    know nothing.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: By generalizing, do you mean scaling? Being able to scale larger?
    [[1:26](https://youtu.be/3jl2h9hSRvc?t=1m26s)] No, I don’t mean scaling at all.
    Scaling is an important thing in many areas. It’s like okay we’ve got something
    that works on my computer with 10,000 items, I now need to make it work on 10,000
    items per second. So scaling is important but not just for machine learning but
    for just about everything we put in production. Generalization is where I say
    okay, here is a model that can predict cats from dogs. I’ve looked at five pictures
    of cats and five pictures of dogs, and I’ve built a model that is perfect. Then
    I look at a different set of five cats and dogs, and it gets them all wrong. So
    in that case, what it learned was not the difference between a cat and a dog,
    but it learnt what those five exact cats looked like and what those five exact
    dogs looked like. Or I built a model of predicting grocery sales for a particular
    product, so for toilet rolls in New Jersey last month, and then I go and put it
    into production and it scales great (in other words, a great latency, no high
    CPU load) but it fails to predicting anything other than toilet rolls in New Jersey.
    It also turns out it only did it well for last month, not the next month. So these
    are all generalization failures.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The most common way that people check for the ability to generalize is to create
    a random sample. So they will grab a few rows at random and pull it out into a
    test set. Then they will build all of their models on the rest of the rows and
    then when they are finished, they will check that the accuracy they got on the
    test set (the rest of the rows are called the training set). So at the end of
    their modeling process, on the training set, they got an accuracy of 99% of predicting
    cats from dogs, at the very end, they check it against a test set to make sure
    that the model really does generalize.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Now the problem is, what if it doesn’t? Well, I could go back and change some
    hyper parameters, do some data augmentation, whatever else trying to create a
    more generalizable model. And then I’ll go back again after doing all that, and
    check and it’s still no good. I’ll keep doing this again and again until eventually,
    after fifty attempts, it does generalize. But does it really generalize? Because
    maybe all I’ve done is accidentally found this one which happens to work just
    for that test set because I’ve tried fifty different things. So if I’ve got something
    which is right coincidentally 5% of the time, they are not very likely to accidentally
    get a good result. So what we generally do is we put aside a second dataset (validation
    set). Then everything that’s not in the validation or test is now training. What
    we do is we train a model, check it against the validation to see if it generalizes,
    do that a few times. Then when we finally got something we think will generalize
    successfully based on the validation set (at the end of the project), we check
    it against the test set.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，如果不行怎么办？嗯，我可以回去改变一些超参数，做一些数据增强，或者尝试创建一个更具泛化性的模型。然后我再次回去，做了所有这些之后，检查结果仍然不好。我会一遍又一遍地这样做，直到最终，在尝试了五十次之后，它泛化了。但它真的泛化了吗？因为也许我所做的一切只是偶然找到了这个恰好适用于那个测试集的模型，因为我尝试了五十种不同的方法。所以如果我有一个东西，巧合地5%的时间是正确的，那么很可能不会偶然得到一个好的结果。所以我们通常会将第二个数据集（验证集）放在一边。然后，不在验证集或测试集中的所有内容现在都是训练集。我们训练一个模型，对其进行验证以查看其是否泛化，重复几次。然后当我们最终得到了我们认为会根据验证集成功泛化的东西（在项目结束时），我们会对其进行测试。
- en: '**Question**: So basically by making this two layer test set validation set,
    if it gets one right the other wrong, you are kind of double checking your errors?
    [[5:19](https://youtu.be/3jl2h9hSRvc?t=5m19s)] It’s checking that we haven’t overfit
    to the validation set. So if we are using the validation set again and again,
    then we could end up not coming up with a generalizable sort of hyper parameters
    but a set of hyper parameters that just so happened to work on the training set
    and the validation set. So if we try 50 different models against the validation
    set and then at the end of all that, we then check that against the test set and
    it’s still generalized as well, then we are going to say okay that’s good we’ve
    actually come up with generalizable model. If it doesn’t, then that’s going to
    say we’ve actually now overfit to the validation set. At that point, you are kind
    of in trouble. Because you don’t have anything left behind. So the idea is to
    use effective techniques during the modeling so that doesn’t happen. But if it’s
    going to happen, you want to find out about it — you need that test set to be
    there because otherwise when you put it in production and then it turns out that
    it doesn’t generalize, that would be a really bad outcome. You’ll end up with
    less people clicking on your ads or selling less of your products, or providing
    car insurance to very risky vehicles.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以基本上通过制作这两层测试集和验证集，如果一个对了另一个错了，你就是在双重检查你的错误？它检查我们是否过度拟合验证集。所以如果我们一遍又一遍地使用验证集，那么我们最终可能得不到一组适用于训练集和验证集的可泛化的超参数，而只是一组恰好适用于训练集和验证集的超参数。所以如果我们对验证集尝试了50种不同的模型，然后在所有这些之后，我们再对测试集进行检查，结果仍然是泛化的，那么我们会说好的，我们实际上已经得到了一个可泛化的模型。如果不是，那么就会说我们实际上现在过度拟合了验证集。在那一点上，你会陷入麻烦。因为你没有留下任何东西。所以想法是在建模过程中使用有效的技术，以防止这种情况发生。但如果它确实发生了，你希望找出原因——你需要那个测试集，否则当你投入生产时，结果却不能泛化，那将是一个非常糟糕的结果。你最终会发现点击你的广告的人会减少，或者销售你的产品会减少，或者为高风险车辆提供汽车保险的人会减少。
- en: '**Question**: So just to make sure, do we need to ever check if the validation
    set and the test set are coherent or you just keep test set?[[6:43](https://youtu.be/3jl2h9hSRvc?t=6m43s)]
    So if you’ve done what I’ve just done here which is to randomly sample, there
    is no particular reason to check as long as they are big enough. But we will come
    back to your question in a different context in just moment.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以只是为了确保，我们需要检查验证集和测试集是否一致，还是只保留测试集？如果你像我刚刚做的那样随机抽样，没有特定的原因需要检查，只要它们足够大。但我们将在稍后的不同情境中回答你的问题。
- en: Another trick we’ve learnt for random forest is a way of not needing a validation
    set [[7:10](https://youtu.be/3jl2h9hSRvc?t=7m10s)]. And the way we learnt was
    to use, instead, the OOB score. This idea was to say every time we train a tree
    in a random forest, there is a bunch of observations that are held out anyway
    because that’s how we get some of the randomness. So let’s calculate our score
    for each tree based on those held out samples and therefore the forest by averaging
    the trees that each row was not part of training. So the OOB score gives us something
    which is pretty similar to the validation score, but on average it’s a little
    less good. Why? Because every row is going to be using a subset of the trees to
    make its prediction, and with less trees, we know we get a less accurate prediction.
    So that’s a subtle one and if you didn’t get it, have a think during the week
    until you understand why this is because it’s a really interesting test of your
    understanding of random forests. Why is OOB score on average less good than your
    validation score? They are both using random held-out subsets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学到的另一个随机森林的技巧是一种不需要验证集的方法。我们学到的方法是使用OOB分数。这个想法是，每次我们在随机森林中训练一棵树时，都会有一些观察结果被保留，因为这就是我们获得一些随机性的方式。因此，让我们基于这些保留样本计算每棵树的分数，然后通过对每行不参与训练的树进行平均，得到整个森林的分数。因此，OOB分数给我们提供了与验证分数非常相似的东西，但平均来看，它稍微差一些。为什么？因为每一行都将使用一部分树来进行预测，而树越少，我们知道预测就越不准确。这是一个微妙的问题，如果你没有理解，那就在这一周内考虑一下，直到你明白为什么，因为这是对你对随机森林理解的一个非常有趣的测试。为什么OOB分数平均来看比你的验证分数差一些？它们都使用随机保留的子集。
- en: Anyway, it’s generally close enough [[11:06](https://youtu.be/3jl2h9hSRvc?t=11m6s)].
    So why have a validation set at all when you are using random forests? If it’s
    a randomly chosen validation set, it’s not strictly speaking necessary but you’ve
    got like four levels of things to test — so you could test on the OOB, when that’s
    working well, you can test on the validation set, and hopefully by the time you
    check against the test set, there’s going to be no surprises so that’ll be one
    good reason.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这通常足够了。那么在使用随机森林时为什么还要有一个验证集呢？如果是一个随机选择的验证集，严格来说并不是必需的，但你有四个层次的测试——所以你可以在OOB上测试，当那个工作良好时，你可以在验证集上测试，希望到检查测试集时不会有什么意外，这将是一个很好的理由。
- en: 'What Kaggle do, the way they do this, is kind of clever. What Kaggle do is
    they split the test set into two pieces: a public and a private. And they don’t
    tell you which is which. So you submit your predictions to Kaggle and then a random
    30% of those are used to tell you the leaderboard score. But then at the end of
    the competition, that gets thrown away and they use the other 70% to calculate
    your real score. So what that’s doing is that you are making sure that you are
    not continuously using that feedback from the leaderboard to figure out some set
    of hyper parameters that happens to do well on the public that actually doesn’t
    generalize. So it’s a great test. This is one of the reasons why it’s good practice
    to use Kaggle because at the end of a competition, at some point this will happen
    to you, and you’ll drop a hundred places on the leaderboard the last day of the
    competition when they use the private test set and say oh okay, that’s what it
    feels like to overfit and it’s much better to practice and get that sense there
    than it is to do it in a company where there’s hundreds of millions of dollars
    on the line.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle的做法是相当聪明的。Kaggle的做法是将测试集分成两部分：一个公共部分和一个私有部分。他们不告诉你哪个是哪个。所以你将你的预测提交给Kaggle，然后随机选择其中30%用来告诉你排行榜分数。但是在比赛结束时，这部分将被丢弃，他们将使用另外70%来计算你的真实分数。这样做的目的是确保你不会持续使用来自排行榜的反馈来找出一组在公共部分表现良好但实际上不具有泛化性的超参数。这是一个很好的测试。这也是为什么在比赛结束时使用Kaggle是一个很好的做法的原因之一，因为在比赛的最后一天，当他们使用私有测试集时，你的排名会下降一百名，然后你会明白，这就是过拟合的感觉，练习和获得这种感觉要比在公司中有数亿美元的风险要好得多。
- en: This is like the easiest possible situation where you are able to use a random
    sample for your validation set [[12:55](https://youtu.be/3jl2h9hSRvc?t=12m55s)].
    Why might I not be able to use a random sample from my validation or possibly
    fail? My claim is that by using a random validation set, we could get totally
    the wrong idea about our model. important thing to remember is when you build
    a model, you always have a systematic error which is that you’re going to use
    the model at a later time than the time you built it. You’re going to put it into
    production by which time the world is different to the world that you are in now
    and even when you’re building the model, you’re using data which is older than
    today anyway. So there’s some lag between the data that you are building it on
    and the data that it’s going to actually be used on in real life. And a lot of
    the time, if not most of the time, that matters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是可能的最简单情况，你可以使用一个随机样本作为你的验证集。为什么我可能无法使用一个随机样本作为我的验证集，或者可能失败呢？我的观点是，通过使用一个随机验证集，我们可能会对我们的模型完全产生错误的看法。要记住的重要事情是，当你构建一个模型时，你总是会有一个系统误差，即你将在比构建模型时晚的时间使用该模型。你将在生产中使用它，到那时，世界已经不同于你现在所处的世界，即使在构建模型时，你使用的数据也比今天的数据旧。因此，在你构建模型的数据和实际使用的数据之间存在一些滞后。大部分时间，如果不是大部分时间，这是很重要的。
- en: 'So if we are predicting who is going to buy toilet paper in New Jersey and
    it takes us two weeks to put it in production and we did it using data from the
    last couple of years and by that time, things may look very different. And particularly
    our validation set, if we randomly sampled it, and it was from a four year period,
    then the vast majority of the data is going to be over a year old. And it may
    be that the toilet paper buying habits of folks in New Jersey may have dramatically
    shifted. Maybe they’ve got a terrible recession there now and they can’t afford
    a high-quality toilet paper anymore. Or maybe they know their paper making industry
    has gone through the roof and suddenly they are buying a lot more toilet paper
    because it’s so cheap. So the world changes and therefore if you use a random
    sample for your validation set, then you are actually checking how good are you
    at predicting things that are totally obsolete now? How good are you at predicting
    things that happened four years ago? That’s not interesting. So what we want to
    do in practice, anytime there is some temporal piece is to instead say assuming
    that we’ve ordered it by time, we’ll use the latest portion as our validation
    set. I suppose, actually do it properly:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c4f153983c2108eddb42bd5a032ebdf6.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: That’s our validation set, and that’s our test set. So the rest is our training
    set and we use that and we try and be able to model that still works on stuff
    that’s later in time than anything the model was built on. So we are not just
    testing generalization in some kind of abstract sense but in a very specific time
    sense which is it generalizes to the future.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: As you said, there is some temporal ordering in the data, so
    in that case, is it wise to take the entire data for training or only a few recent
    dataset for training [[19:07](https://youtu.be/3jl2h9hSRvc?t=19m7s)]? Yeah, that
    is a whole other question. So how do you get the validation set to be good? So
    I build a random forest on all the training data. It looks good on the training
    data, it looks good on OOB. And this is actually a really good reason to have
    OOB. If it looks good on the OOB then it means you are not overfitting in a statistical
    sense. It’s working well on a random sample. But then it looks bad on the validation
    set. So what happened? Well, what happened was that you somehow failed to predict
    the future. You only predicted the past so Suraj had an idea about how we could
    fix that. Okay, well, maybe we should just train so maybe we shouldn’t use the
    whole training set. We should try a recent period only. Now on the downside, we
    are now using less data so we can create less rich models, on the upside, it’s
    more up-to-date data. And this is something you have to play around with. Most
    machine learning functions have the ability to provide a weight that is given
    to each row. So for example, with a random forest, rather than bootstrapping at
    random, you could have a weight on every row and randomly pick that row with some
    probability. So we could put probability such that the most recent rows have a
    higher probability of being selected. That can work really well. It’s something
    that you have to try and if you don’t have a validation set that represents the
    future compared to what you are training on, you have no way to know which of
    your techniques are working.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: How do you make the compromise between amount of data vs. recency of data? What
    I tend to do is when I have this kind of temporal issue, which is probably most
    of the time, once I have something that’s working well on the validation set,
    I wouldn’t then go and just use that model on the test set because the test set
    is much farther in the future compared to the training set. So I would then replicate
    building that model again but this time I would combine the training and validation
    sets together and retrain the model. At that point, you’ve got no way to test
    against a validation set, so you have to make sure you have a reproducible script
    or notebook that does exactly the same steps in exactly the same ways because
    if you get something wrong then you’re going to find on the test set that you’ve
    got a problem. So what I do in practice is I need to know if my validation set
    is a truly representative of the test set. So what I do is I build five models
    on the training set, and I try to have them vary in how good I think they are.
    Then I score my five models on the validation set and then I also score them on
    the test set. So I’m not cheating since I’m not using any feedback from the test
    set to change my hyper parameters — I’m only using it for this one thing which
    is to check my validation set. So I get my five scores from the validation set
    and test set and then I check that they fall in a line. If they don’t, then you’re
    not going to get good enough feedback from the validation set. So keep doing that
    process until you’re getting a line and that can be quite tricky . Trying to create
    something that’s as similar to the real-world outcome as possible is difficult.
    And in the real world, the same is true of creating the test set — the test set
    has to be as close to production as possible. So what’s the actual mix of customers
    that are going to be using this, how much time is there actually going to be between
    when you build the model and when you put it in production? How often you are
    going to be able to refresh the model? These are all the things to think about
    when you build that test set.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何在数据量和数据新旧之间做出妥协？我倾向于这样做，当我遇到这种时间问题时，也许大部分时间都是这样，一旦我在验证集上找到了一个表现良好的模型，我就不会直接在测试集上使用那个模型，因为测试集比训练集要远得多。所以我会重新构建那个模型，但这次我会将训练和验证集合并起来重新训练模型。在那一点上，你没有办法对验证集进行测试，所以你必须确保你有一个可重现的脚本或笔记本，以确保完全相同的步骤，因为如果你出错了，你会发现在测试集上出现了问题。所以我在实践中所做的是，我需要知道我的验证集是否真正代表了测试集。所以我在训练集上建立了五个模型，并尝试让它们在我认为它们有多好的方面有所不同。然后我在验证集上对我的五个模型进行评分，然后我也在测试集上对它们进行评分。所以我没有作弊，因为我没有使用来自测试集的任何反馈来改变我的超参数——我只是用它来检查我的验证集。所以我从验证集和测试集中得到了五个分数，然后我检查它们是否在一条线上。如果不是，那么你将无法从验证集中获得足够好的反馈。所以继续这个过程，直到你得到一条线，这可能会很棘手。试图创建尽可能接近真实结果的东西是困难的。在现实世界中，创建测试集也是如此——测试集必须尽可能接近生产。那么实际使用这个产品的客户组合是什么样的，你构建模型和投入生产之间实际会有多少时间？你能够多频繁地更新模型？这些都是在构建测试集时需要考虑的事情。
- en: '**Question**: So first you build five models on the training data and if you
    didn’t get a straight-line relationship, change your validation and test set [[24:01](https://youtu.be/3jl2h9hSRvc?t=24m1s)]?
    You can’t really change the test set generally, so this is assuming that the test
    set is given and you change the validation set. So if you start with a random
    sample validation set and then it’s all over the place and you realize oh I should
    have picked the last two months. Then you pick the last two months and it’s still
    going all over the place and you realize oh I should have picked it so that it’s
    also from the first of the month to the fifteenth of the month, and keep changing
    the validation set until you found a set which is indicative of your test set
    results.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：所以首先你在训练数据上建立了五个模型，如果没有得到直线关系，就改变你的验证和测试集？通常你不能真正改变测试集，所以这是假设测试集已经给定，你改变验证集。所以如果你开始用一个随机样本验证集，然后结果千奇百怪，你意识到哦，我应该选择最近的两个月。然后你选择了最近的两个月，结果还是千奇百怪，你意识到哦，我应该选择从每个月的第一天到第十五天，然后不断改变验证集，直到找到一个能够反映你的测试集结果的集合。'
- en: '**Question**: For five models, you start with maybe just random data, average,
    etc [[24:45](https://youtu.be/3jl2h9hSRvc?t=24m45s)]? Maybe five not terrible
    ones but you want some variety and you also particularly want some variety in
    how well they might generalize through time. So one that was trained on the whole
    training set, one was trained on the last two weeks, one that was trained on the
    last six week, one which used lots and lots of columns and might over fit a bit
    more. So you want to get a sense of if my validation set fails to generalize temporarily,
    I’d want to see that, if it fails to generalize statistically, I’d want to see
    that.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于五个模型，你可能从随机数据、平均值等开始？也许不是五个糟糕的模型，但你想要一些变化，尤其是你想要一些在时间上可能泛化得更好的模型。一个是在整个训练集上训练的，一个是在最后两周训练的，一个是在最后六周训练的，一个使用了很多列可能会过拟合一些。所以你想要知道如果我的验证集在时间上无法泛化，我想看到这一点，如果在统计上无法泛化，我也想看到这一点。'
- en: '**Question**: Can you explain a bit more in detail what you mean by change
    your validation set so it indicates the test set? What does that look like [[25:28](https://youtu.be/3jl2h9hSRvc?t=25m28s)]?
    Let’s take the groceries competitions where we are trying to predict the next
    two weeks of grocery sales. The possible validation set that Terrance and I played
    with was:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：你能详细解释一下你所说的改变验证集以使其表示测试集是什么意思吗？看起来是什么样子？让我们以杂货竞赛为例，我们试图预测接下来两周的杂货销售额。Terrance和我尝试过的可能的验证集是：'
- en: Random sample (4 years)
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机样本（4年）
- en: Last month of data (July 15–August 15)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近一个月的数据（7月15日至8月15日）
- en: Last 2 weeks (August 1–15)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过去的两周（8月1日至15日）
- en: Same day range one month earlier (July 15–30)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个月前的同一天范围（7月15日至30日）
- en: The test set in this competition was the 15th to the 30th of the August. So
    above were four different validation sets we tried. With random, our results were
    all over the place. With last month, they were not bad but not great. The last
    two weeks, there was a couple that didn’t look good but on the whole they were
    good. The same day range a month earlier, they’ve got a basically perfect line.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比赛中的测试集是8月15日至30日。所以上面是我们尝试的四个不同的验证集。随机的结果是完全不稳定的。上个月的结果不错但也不是很好。过去的两周，有一些看起来不好，但总体上还不错。一个月前的同一天范围内，他们有一个基本完美的线。
- en: '**Question**: What exactly are we comparing it to from the test set [[26:58](https://youtu.be/3jl2h9hSRvc?t=26m58s)]?
    I build 5 models, so there might be 1\. just predict the average, 2\. do some
    kind of simple group mean of the whole data set, 3\. do some group mean over the
    last month of the data, 4\. build a random forests of the whole thing, 5, build
    random forest from the last three weeks. On each of those, I calculate the validation
    score. Then I retrain the model on the whole training set and calculate the same
    thing on the test set. So each of these points now tells me how well did it go
    on the validation set and how well did it go in the test set. If the validation
    set is useful, we would say every time the validation score set improves, the
    test set score should also improve.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我们到底是在与测试集中的什么进行比较？我建立了5个模型，可能是1.只是预测平均值，2.对整个数据集进行某种简单的组平均，3.对过去一个月的数据进行某种组平均，4.构建整个数据集的随机森林，5.从过去三周构建随机森林。在每一个上，我计算验证分数。然后我在整个训练集上重新训练模型，并在测试集上进行相同的计算。所以现在每个点告诉我它在验证集上表现如何，它在测试集上表现如何。如果验证集有用，我们会说每次验证分数提高，测试集分数也应该提高。'
- en: '**Question** : When you say “re-train” do you mean re-train the model on training
    and validation set [[27:50](https://youtu.be/3jl2h9hSRvc?t=27m50s)]? Yes, so once
    I’ve got the validation score based on just the training set, then retrain it
    on the train and validation and check against the test set.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：当你说“重新训练”时，你是指在训练和验证集上重新训练模型吗？是的，所以一旦我得到了基于训练集的验证分数，然后在训练和验证集上重新训练，并与测试集进行对比。'
- en: '**Question**: By test set, do you mean submitting it to Kaggle and check the
    score? If it’s Kaggle, then your test set is Kaggle’s leader board. In the real
    world, the test set is this third data set you put aside. It’s that third dataset
    that having it reflect real world production differences is the most important
    step in a machine learning project. Why is it the most important step? Because
    if you screw up everything else but you don’t screw up that, you’ll know you screwed
    up. If you’ve got a good test set, then you’ll know you screwed up because you
    screwed up something else and you tested it and it didn’t work out, it’s okay.
    You’re not going to destroy the company. If you screwed up creating the test set,
    that would be awful. Because then you don’t know if you’ve made a mistake. You
    try to build a model, you test it on the test set and it looks good. But the test
    set was not indicative of real-world environment. So you don’t actually know if
    you are going to destroy the company. Hopefully you’ve got ways to put things
    into production gradually so you won’t actually destroy the company but you’ll
    at least destroy your reputation at work. Oh, Jeremy tried to put this thing into
    production and in the first week the cohort we tried it on, their sales halved
    and we’re never gonna give Jeremy a machine learning job again. But if Jeremy
    had used a proper test set then he would have known, uh-oh this is half as good
    as my validation set said it would be, I’ll keep trying. Now I’m not going to
    get in any trouble. I was actually like oh, Jeremy is awesome — he identifies
    ahead of time when there’s going to be a generalization problem.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：通过测试集，你是指将其提交到Kaggle并检查分数吗？如果是Kaggle，那么你的测试集就是Kaggle的排行榜。在现实世界中，测试集是你放在一边的第三个数据集。这第三个数据集反映真实世界生产差异是机器学习项目中最重要的一步。为什么这是最重要的一步？因为如果你搞砸了其他一切但没有搞砸这个，你会知道你搞砸了。如果你有一个好的测试集，那么你会知道你搞砸了，因为你搞砸了其他东西并测试了它，结果不尽人意，没关系。你不会毁掉公司。如果你搞砸了创建测试集，那将是可怕的。因为那样你就不知道自己是否犯了错误。你尝试构建一个模型，你在测试集上测试它，看起来不错。但测试集并不代表真实世界环境。所以你实际上不知道你是否会毁掉公司。希望你有逐渐将事物投入生产的方式，这样你就不会真的毁掉公司，但至少会毁掉你在工作中的声誉。哦，Jeremy试图将这个东西投入生产，在第一周我们尝试的队伍中，他们的销售额减半了，我们再也不会让Jeremy做机器学习工作了。但如果Jeremy使用了适当的测试集，那么他会知道，哦，这只有我的验证集说的一半好，我会继续尝试。现在我不会惹麻烦了。我实际上很喜欢Jeremy
    - 他能提前识别出将会出现泛化问题的情况。'
- en: This is something everybody talks about a little bit in machine learning classes
    but often it stops at the point where you learned that there is a thing in sklearn
    called make `train_test_split` and it returns these things and off you go, or
    here is the cross-validation function [[30:10](https://youtu.be/3jl2h9hSRvc?t=30m10s)].
    The fact that these things always give you random samples tells you that much
    if not most of the time, you shouldn’t be using them. The fact that random forest
    gives you an OOB for free, it’s useful but only tells you that this generalizes
    in a statistical sense, not in a practical sense.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是每个人在机器学习课程中都会谈论一点的事情，但通常停在你学到了sklearn中有一个叫做make `train_test_split`的东西，它返回这些东西，然后你就可以继续了，或者这里是交叉验证函数。这些东西总是给你随机样本的事实告诉你，如果不是大部分时间，你不应该使用它们。随机森林免费提供OOB，这很有用，但只告诉你这在统计意义上是泛化的，而不是在实际意义上。
- en: Cross validation [[30:54](https://youtu.be/3jl2h9hSRvc?t=30m54s)]
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交叉验证
- en: Outside of class, you guys have been talking about a lot which makes me feel
    somebody’s been over emphasizing the value of this technique. So I’ll explain
    what cross-validation is and then I explain why you probably shouldn’t be using
    it most of the time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation says let’s not just pull out one validation set, but let’s
    pull out five, for example. So let’s assume that we’re going to randomly shuffle
    the data first of all. This is critical.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Randomly shuffle the data.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split it into five groups
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For model №1, we will call the first one the validation set, and the bottom
    four the training set.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will train and we will check against the validation and we get some RMSE,
    R², etc.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will repeat that five times, and we will take the average of RMSE, R², etc,
    and that is a cross-validation average accuracy.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the benefit of using cross-validation over a standard validation set
    I talked about before? You can use all of the data. You don’t have to put anything
    aside. And you get a little benefit as well in that you’ve now got five models
    that you could ensemble together, each one used 80% of the data. So sometimes
    that ensemble can be helpful.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: What could be some reasons that you wouldn’t use cross-validation? For large
    dataset, it will take a long time. We have to fit five models rather than one,
    so time is a key downside. If we are doing deep learning and it takes a day to
    run, suddenly it takes five days or we need 5 GPUs. What about my earlier issues
    about validation sets? Our earlier concerns about why random validation sets are
    a problem are entirely relevant here. These validation sets are random, so if
    a random validation set is not appropriate for your problem, most likely because,
    for example, of temporal issues then none of these five validation sets are any
    good. They are all random. So if you have temporal data like we did before, there’s
    no way to do cross validation or no good way to do cross validation. You want
    to have your validation set to be as close to the test set as possible, and you
    can’t do that by randomly sampling different things. You may well not need to
    do cross validation because most of the time in the real world, we don’t really
    have that little data — unless your data is based on some very very expensive
    labeling process or some experiments that cost a lot to run, etc. But nowadays,
    data scientists are not very often doing that kind of work. Some are, in which
    case this is an issue, but most of us aren’t. So we probably don’t need to. If
    we do do it, it’s going to take a whole a lot of time, then even if we did do
    it and we took up all that time, it might give us totally the wrong answer because
    random validation sets are inappropriate for our probblem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: I’m not going to be spending much time on cross validation because I think it’s
    an interesting tool to have, it’s easy to use (sklearn has a cross validation
    thing you can use), but it’s not that often that it’s going to be an important
    part of your toolbox in my opinion. It’ll come up sometimes. So that is validation
    sets.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Tree interpretation [[38:02](https://youtu.be/3jl2h9hSRvc?t=38m2s)]
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What does tree interpreter do and how does it do it? Let’s start with the output
    of tree interpreter [[38:51](https://youtu.be/3jl2h9hSRvc?t=38m51s)]. Here is
    a single tree:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/765db675f664a2bce60cbc7fe6307f92.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: The root of the tree is before there’s been any split at all. So 10.189 is the
    average log price of all of the options in our training set. Then if I go `Coupler_System
    ≤ 0.5`, then we get an average of 10.345 (the subset of 16815). Off the people
    with `Coupler_System ≤0.5`, we then take the subset where `Enclosure ≤ 2.0` and
    average log price there is 9.955\. Then the final step is `ModelID ≤ 4573.0` and
    that gives us 10.226.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f6cadf323e5c2862cd75b5bdc4b03bc0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: We can then calculate the change in average log price by each additional criteria.
    We can draw that as what’s called a waterfall plot. Waterfall plots are one of
    the most useful plots I know about and weirdly enough, there’s nothing in Python
    to do them. This is one of these things where there’s this disconnect between
    the world of management consulting and business where everybody uses waterfall
    plots all the time and academia who have no idea what these things are. Every
    time you have a starting point and a number of changes and a finishing point,
    waterfall charts are pretty much always the best way to show it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4ba7c2df232997d01f2f2f798998bbc7.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: With Excel 2016, it’s build-in. You just click insert waterfall chart and there
    it is. If you want to be a hero, create a waterfall chart package for matplotlib,
    put it on pip, and everybody will love you for it. These are actually super easy
    to build. You basically do a stacked column plot where the bottom of this is all
    white. You can kind of do it but if you can wrap that up, put the points in the
    right spots and color them nicely, that would be totally awesome. I think you’ve
    all got the skills to do it, and would be a terrific thing for your portfolio.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: In general, they are going from all, then going through each change, then the
    sum of all of those is going to be equal to the final prediction [[43:38](https://youtu.be/3jl2h9hSRvc?t=43m38s)].
    So if we were just doing a decision tree and someone asks “how come this particular
    auction’s prediction was this particular price?”, this is how you can answer “because
    these three things had these three impacts”.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: For a random forest, we could do that across all of the trees. So every time
    we see coupler, we add up that change. Every time we see enclosure, we add up
    that change, and so on. Then we combine them all together, we get what tree interpreter
    does. So you could go into the source code for tree interpreter and it’s not at
    all complex logic. Or you could build it yourself and you can see how it does
    exactly this.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'So when you go `treeinterpreter.predict` with a random forest model for some
    specific auction (in this case it’s zero index row), it tells you:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '`prediction`: the same as the random forest prediction'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bias`: this is going to be always the same — it’s the average sale price for
    everybody for each of the random samples in the tree'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`contributions`: the total of all the contributions for each time we see that
    specific column appear in a tree.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The last time I made a mistake of not sorting this correctly. so this time `np.argsort`
    is a super handy function. It doesn’t actually sort `contributions[0]`, it just
    tells you where each item would move to if it were sorted. So now by passing `idxs`
    to each one of the column, the level, and contribution , I can then print out
    all those in the right order.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So small piece of industrial equipment meant that it was less expensive. If
    it was made pretty recently meant that was more expensive, etc. So this is not
    going to really help you much at all with Kaggle where you just need predictions.
    But it’s going to help you a lot in a production environment or even pre production.
    So something which any good manager should do if you say here is a machine learning
    model I think we should use is they should go away and grab a few example of actual
    customers or actual auctions and check whether your model looks intuitive. If
    it says my prediction is that lots of people are going to really enjoy this crappy
    movie, and it is like “wow, that was a really crappy movie” then they’re going
    to come back to you and say “explain why your model is telling me that I’m going
    to like this movie because I hate that movie”. Then you can go back and say well,
    it’s because you like this movie and because you’re this age range and you’re
    this gender, and on average actually people like you did like that movie.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What’s the second element of each tuple [[47:25](https://youtu.be/3jl2h9hSRvc?t=47m25s)]?
    This is saying for this particular row, ‘ProductSize’ was ‘Mini’, and it was 11
    years old, etc. So it’s just feeding back and telling you. Because this is actually
    what it was:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It was these numbers. So I just went back to the original data to actually pull
    out the descriptive versions of each one.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: So if we sum up all the contributions together, and then add them to the bias,
    then that would give us the final prediction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This is an almost totally unknown technique and this particular library is almost
    totally unknown as well. So it’s a great opportunity to show something that a
    lot of people don’t know. It’s totally critical in my opinion but rarely done.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: So this is kind of the end of the random forest interpretation piece and hopefully
    you’ve now seen enough that when somebody says we can’t use modern machine learning
    techniques because they are black boxes that aren’t interpretable, you have enough
    information to say you are full of crap. They are extremely interpretable and
    the stuff that we’ve just done — trying to do that with a linear model, good luck
    to you. Even where you can do something similar with a linear model, trying to
    do it so that is not giving you totally the wrong answer and you had no idea it’s
    a wrong answer is going to be a real challenge.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Extrapolation [[49:23](https://youtu.be/3jl2h9hSRvc?t=49m23s)]
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last step we are going to do before we try and build our own random forest
    is to deal with this tricky issue of extrapolation. So in this case, if we look
    at the accuracy of our recent trees, we still have a big difference between our
    validation score and our training score.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb86638bde04fb34da0a4ac67e16e973.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Actually, in this case, the difference between the OOB (0.89420) and the validation
    (0.89319) is actually pretty close. So if there was a big difference, I’d be very
    worried about whether we’ve dealt with the temporal side of things correctly.
    Here is the most recent model:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9a0ab597e234a6d7f66adce0d8ca03e3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: On Kaggle, you need that last decimal place. In real world, I probably stopped
    here. But quite often you’ll see there’s a big difference between your validation
    score and your OOB score, and I want to show you how you would deal with that
    particularly because we know that the OOB score should be a little worse because
    it’s using less trees so it gives me a sense that we should get to do a little
    bit better. The way we should be able to do a little bit better is by handling
    the time component a little bit better.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Here is the problem with random forests when it comes to extrapolation. When
    you’ve got a dataset that got four years of sales data in it, and you create your
    tree and it says if it’s in some particular store and some particular item and
    it is on special, here is the average price. And it actually tells us the average
    price over the whole training set which could be pretty old. So when you then
    want to step forward to what is going to be the price next month, it’s never seen
    next month. Where else with a linear model, it can find a relationship between
    time and price where even though we only had this much data, when you then go
    and predict something in the future, it can extrapolate that. But a random forest
    can’t do that. If you think about it, there is no way for a tree to be able to
    say well next month, it would be higher still. So there is a few ways to deal
    with this and we’ll talk about it over the next couple of lessons, but one simple
    way is just to try to avoid using time variables as predictors if there’s something
    else we could use that’s going to give us a better or stronger relationship that’s
    actually going to work in the future [[52:19](https://youtu.be/3jl2h9hSRvc?t=52m19s)].
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: So in this case, what I wanted to do was to first of all figure out what’s the
    difference between our validation set and our training set. If I understand the
    difference between our validation set and our training set, then that tells me
    what are the predictors which have a strong temporal component and therefore they
    may be irrelevant by the time I get to the future time period. So I do something
    really interesting which is I create a random forest where my dependent variable
    is “is it in the validation set” (`is_valid`). I’ve gone back and I’ve got my
    whole data frame with the training and validation all together and I’ve created
    a new column called `is_valid` which I’ve set to one and then for all of the stuff
    in the training set, I set it to zero. So I’ve got a new column which is just
    is this in the validation set or not and then I’m going to use that as my dependent
    variable and build a random forest. This is a random forest not to predict price
    but predict is this in the validation set or not. So if your variable were not
    time dependent, then it shouldn’t be possible to figure out if something is in
    the validation set or not.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is a great trick in Kaggle because they often won’t tell you whether the
    test set is a random sample or not. So you could put the test set and training
    set together, create a new column called `is_test` and see if you can predict
    it. If you can, you don’t have a random sample which means you have to figure
    out how to create a validation set from it.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this case, I can see I don’t have a random sample because my validation set
    can be predicted with a .9999 R².
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: So then if I look at feature importance, the top thing is `SalesID` [[54:36](https://youtu.be/3jl2h9hSRvc?t=54m36s)].
    So this is really interesting. It tells us very clearly `SalesID` is not a random
    identifier but probably it’s something that’s just set consecutively as time goes
    on — we just increase the `SalesID`. `saleElapsed` was the number of days since
    the first date in our dataset so not surprisingly that also is a good predictor.
    Interestingly `MachineID` — clearly each machine is being labeled with some consecutive
    identifier as well and then there’s a big drop in importance, so we’ll stop here.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](../Images/a692ee826e44d034c4c389ddb819a6f7.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: Let’s next grab those top three and we can then have a look at their values
    both from the training set and in the validation set. [[55:22](https://youtu.be/3jl2h9hSRvc?t=55m22s)]
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../Images/927ade0892b7e4ac5ed43e9df0334faf.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](../Images/5550478d44046e76f333581c005dbce5.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: We can see for example, SalesID on average is 1.8 million in the training set
    and 5.8 million in the validation set (notice that the value is divided by 1000).
    So you can confirm they are very different.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: So let’s drop them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So after I drop them, let’s now see if I can predict whether something is in
    the validation set. I still can with 0.98 R².
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../Images/9836d21c99139fb67f800888ef76da83.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Once you remove some things, then other things can come to the front, and it
    now turns out that not surprisingly age — things that are old are more likely
    to be in the validation set because earlier on in the training set, they can’t
    be that old yet. YearMade for the same reason. So then we can try removing those
    as well — `SalesID`, `saleElapsed`, `MachineID` from the first one, `age`, `YearMade`,
    and `saleDayofyear` from the second one. They are all time dependent features.
    I still want them in my random forest if they are important. But if they are not
    important, then taking them out if there are some other none-time dependent variables
    that work just as well — that would be better. Because now I am going to have
    a model that generalizes over time better.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So here, I’m just going to go through each one of those features and drop each
    one, one at a time, retrain a new random forest, and print out the score [[57:19](https://youtu.be/3jl2h9hSRvc?t=57m19s)].
    Before we do any of that, our score was 0.88 for validation, 0.89 for OOB. And
    you can see below, when I remove SalesID, my score goes up. This is what we were
    hoping for. We’ve removed a time dependent variable, there were other variables
    that could find similar relationships without the time dependency. So removing
    it caused our validation to go up. Now OOB didn’t go up, because this is genuinely
    statistically a useful predictor, but it’s a time dependent one and we have a
    time dependent validation set. So this is really subtle but it can be really important.
    It’s trying to find the things that gives you a generalizable-across-time prediction
    and here is how you can see it.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We should remove `SalesID` for sure, but `saleElapsed` didn’t get better, so
    we don’t want. `MachineID` did get better — 0.888 to 0.893 so it’s actually quite
    a bit better. `age` got a bit better. `YearMade` got worse, `saleDayofyear` got
    a bit better.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So now we can say, let’s get rid of the three where we know that getting rid
    of it actually made it better. And as a result, we are now up to .915! So we got
    rid of three time dependent things and now as expected our validation is better
    than our OOB.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: So that was a super successful approach there, and now we can check the feature
    importance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/474f880db65e91ac289aa57aab5deebb.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s go ahead and say alright, that was pretty darn good. Let’s now leave it
    for a while so give it 160 trees, let it chew on it, and see how that goes.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Our final model!
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, we did all of our interpretation, all of our fine-tuning basically
    with smaller models/subsets and at the end, we run the whole thing. And it actually
    still only took 16 seconds and so we’ve now got an RMSE of 0.21\. Now we can check
    that against Kaggle. Unfortunately, this is an older competition and we are not
    allowed to enter anymore to see how we would have gone. So the best we can do
    is check whether it looks like we could have done well based on their validation
    set so it should be in the right area. Based on that, we would have come first.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: I think this is an interesting series of steps. So you can go through the same
    series of steps in your Kaggle projects and more importantly, your real-world
    projects. One of the challenges is once you leave this learning environment, suddenly
    you are surrounded by people who never have enough time, they always want you
    to be in a hurry, they’re always telling you do this and then do that. You need
    to find the time to step away and go back because this is a genuine real-world
    modeling process you can use. And it gives, when I said gives world-class results,
    I mean it. The guy who won this, Leustagos, sadly he passed away but he is the
    top Kaggle competitor of all time. He won I believe dozens of competitions so
    if we can get a score even within cooee of him, then we are doing really well.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '**Clarification** [[1:01:31](https://youtu.be/3jl2h9hSRvc?t=1h1m31s)]: The
    change in R² between these two is not just due to the fact that we removed these
    three predictors. We also went `reset_rf_samples()` . So to actually see the impact
    of just removing, we need to compare it to the final step earlier.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/bb918a228059a483c993a32b6f8c7b3e.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: So it’s actually compared to 0.907 validation. So removing those three things
    took us from 0.907 to 0.915\. In the end, of course, what matters is our final
    model but just to clarify.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Writing Random Forest from scratch! [[1:02:31](https://youtu.be/3jl2h9hSRvc?t=1h2m31s)]
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson3-rf_foundations.ipynb)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: My original plan here was to do it in real time and then as I started to do
    it, I realized that would have been boring, so instead, we might do more of a
    walk through the code together.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Implementing random forest algorithm is actually quite tricky not because the
    code is tricky [[1:05:03](https://youtu.be/3jl2h9hSRvc?t=1h5m3s)]. Generally speaking,
    most random forest algorithms are pretty conceptually easy. Generally speaking,
    academic papers and books have a knack of making them look difficult, but they
    are not difficult conceptually. What’s difficult is getting all the details right
    and knowing when you’re right. In other words, we need a good way of doing testing.
    So if we are going to reimplement something that already exists, like say we want
    to create a random forest in some different framework, different language, different
    operating system, I would always start with something that does exist. So in this
    case, we’re just going to do it as learning exercise, writing a random forest
    in Python, so for testing, I’m going to compare it to an existing random forest
    implementation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 实现随机森林算法实际上是相当棘手的，不是因为代码很棘手。一般来说，大多数随机森林算法在概念上都很容易。一般来说，学术论文和书籍往往让它们看起来很困难，但从概念上讲并不困难。困难的是把所有细节搞对，知道什么时候是对的。换句话说，我们需要一种好的测试方法。因此，如果我们要重新实现已经存在的东西，比如说我们想在一些不同的框架、不同的语言、不同的操作系统中创建一个随机森林，我总是从已经存在的东西开始。因此，在这种情况下，我们只是把它作为学习练习，用Python编写一个随机森林，因此为了测试，我将把它与现有的随机森林实现进行比较。
- en: That’s critical. Anytime you are doing anything involving non-trivial amounts
    of code in machine learning, knowing whether you’ve got it right or wrong is the
    hardest bit. I always assume that I’ve screwed everything up at every step, and
    so I’m thinking okay assuming that I screwed it up, how do I figure out that I
    screwed it up. Then much to my surprise from time to time, I actually get something
    right and then I can move on. But most of the time, I get it wrong, so unfortunately
    with machine learning, there’s a lot of ways you can get things wrong that don’t
    give you an error. They just make your result slightly less good and so that’s
    what you you want to pick up.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是至关重要的。每当你在涉及机器学习中的非平凡量的代码时，知道你是对还是错是最困难的部分。我总是假设在每一步都搞砸了一切，所以我在想，好吧，假设我搞砸了，我怎么知道我搞砸了。然后令我惊讶的是，有时候我实际上做对了，然后我可以继续。但大多数时候，我做错了，所以不幸的是，对于机器学习来说，有很多方法可以让你出错，而不会给你错误。它们只会让你的结果稍微不那么好，这就是你想要发现的。
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: So given that I want to compare it to an existing implementation, I’m going
    to use our existing dataset, our existing validation set, and then to simplify
    things, I’m just going to use two columns to start with [[1:06:44](https://youtu.be/3jl2h9hSRvc?t=1h6m44s)].
    So let’s go ahead and start writing a random forest.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，考虑到我想要将其与现有实现进行比较，我将使用我们现有的数据集，我们现有的验证集，然后为了简化事情，我只会从两列开始。因此，让我们继续开始编写一个随机森林。
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'My way of writing nearly all code is top-down just like my teaching. So by
    top-down, I start by assuming that everything I want already exists. In other
    words, the first thing I want to do, I’m going to call this a tree ensemble. To
    create a random forest, the first question I have is what do I need to pass in.
    What do I need to initialize my random forest. I’m going to need:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我写代码的方式几乎都是自顶向下的，就像我的教学一样。因此，从顶部开始，我假设我想要的一切都已经存在。换句话说，我想要做的第一件事是，我将称之为树集合。要创建一个随机森林，我首先要问的问题是我需要传入什么。我需要初始化我的随机森林。我将需要：
- en: '`x`: some independent variables'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`：一些自变量'
- en: '`y`: some dependent variable'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`：一些因变量'
- en: '`n_trees`: pick how many trees I want'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_trees`：选择我想要的树的数量'
- en: '`sample_sz`: I’m going to use the sample size parameter from the start here,
    so how big you want each sample to be'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_sz`：我将从一开始使用样本大小参数，因此您希望每个样本有多大'
- en: '`min_leaf`: then maybe some optional parameter of what’s the smallest leaf
    size.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_leaf`：然后可能是一些可选参数，表示最小叶子大小。'
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'For testing, it’s nice to use a constant random seed, so we’ll get the same
    result each time. So `np.random.seed(42)` is how you set a random seed. Maybe
    it’s worth mentioning for those of you who aren’t familiar with it, random number
    generators on computers aren’t random at all. They are actually called pseudo
    random number generators and what they do is given some initial starting point
    (in this case 42), a pseudo random number generator is a mathematical function
    that generates a deterministic (always the same) sequence of numbers such that
    those numbers are designed to be:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试，最好使用一个固定的随机种子，这样每次都会得到相同的结果。`np.random.seed(42)`是设置随机种子的方法。也许值得一提的是，对于那些不熟悉的人来说，计算机上的随机数生成器根本不是随机的。它们实际上被称为伪随机数生成器，它们的作用是在给定一些初始起点（在这种情况下是42）的情况下，生成一系列确定性（始终相同）的数字，这些数字被设计为：
- en: as uncorrelated with the pervious number as possible
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能与前一个数字不相关
- en: as unpredictable as possible
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能不可预测
- en: as uncorrelated as possible with something with a different random seed (so
    the second number in the sequence starting with 42 should be very different to
    the second number starting with 41)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽可能与具有不同随机种子的东西不相关（因此以42开头的序列中的第二个数字应该与以41开头的序列中的第二个数字非常不同）
- en: And generally, they involve using big prime numbers, taking mods, and stuff
    like that. It’s an interesting area of math. If you want real random numbers,
    the only way to do that is you can actually buy hardware called a hardware random
    number generator that’ll have inside them like a little bit of some radioactive
    substance and something that detects how many things it’s spitting out or there’ll
    be some hardware thing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，它们涉及使用大素数，取模等等。这是一个有趣的数学领域。如果你想要真正的随机数，唯一的方法就是你可以购买一种叫做硬件随机数生成器的硬件，里面会有一点放射性物质，以及一些检测它输出了多少东西的东西，或者会有一些硬件设备。
- en: '**Question**: Is current system time a valid random number generation [[1:09:25](https://youtu.be/3jl2h9hSRvc?t=1h9m25s)]?
    So that would be maybe for a random seed (the thing we start the function with).
    One of the really interesting area is in your computer, if you don’t set the random
    seed, what is it set to. Quite often, people use the current time for security
    — obviously we use a lot of random number stuff for security, like if you are
    generating an SSH key, it needs to be random. It turns out people can figure out
    roughly when you created a key. They could look at oh, `id_rsa` has a timestamp
    and they could try all the different nanoseconds starting points for a random
    number generator around that timestamp and figure out your key. So in practice,
    a lot of high randomness requiring applications actually have a step that say
    “please move your mouse and type random stuff at the keyboard for a while” and
    so it gets you to be a source of “entropy”. Other approach is they’ll look at
    the hash of some of your log files or stuff like that. It’s a really really fun
    area.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: In our case, our purpose actually is to remove randomness [[1:10:48](https://youtu.be/3jl2h9hSRvc?t=1h10m48s)].
    So we are saying okay, generate a series of pseudo random numbers starting with
    42, so it always should be the same.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: If you haven’t done much stuff in Python OO, this is basically standard idiom
    at least I write it this way, most people don’t, but if you pass in five things
    that you are going to want to keep inside this object, then you basically have
    to say `self.x = x`, etc. We can assign to a tuple from a tuple.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b50c5cfec00ef768509952835719b1ac.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: This is my way of coding. Most people thing this is horrible, but I prefer to
    be able to see everything at once and so I know in my code anytime I see something
    that looks like this, it’s always all of the stuff in the method being set. If
    I did it a different way, then half of the code now come off the bottom of the
    page and you can’t see it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: So that was the first thing I thought about — to create a random forest what
    information do you need. Then I’m going to need to store that information inside
    my object, and then I need to create some trees. A random forest is something
    that has some trees. So I basically figured to use list comprehension to create
    a list of trees. How many trees do we have? We got `n_trees` trees. That’s what
    we asked for. `range(n_trees)` gives me the numbers from 0 up to `n_trees — 1`.
    So if I create a list comprehension that loops through that range calling `create_tree`
    each time, I now have `n_trees` trees.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: To write that, I didn’t have to think at all. That’s all obvious. So I’ve delayed
    the thinking to the point where it’s like well wait, we don’t have something to
    create a tree. Okay, no worries. But let’s pretend we did. If we did, we’ve now
    created a random forest. we’d still need to do a few things on top of that. For
    example, once we have it we need a predict function. Okay, let’s write a predict
    function. How do you predict in a random forest? For a particular row (or rows),
    go through each tree, calculate its prediction. So here is a list comprehension
    that is calculating the prediction for every tree for `x`. I don’t know if `x`
    is one row or multiple rows, it doesn’t matter as long as `tree.predict` works
    on it. And once you’ve got a list of things, a cool thing to know is you can pass
    `numpy.mean` a regular non numpy list and it will take the mean — you just need
    to tell it `axis=0` means average across the lists. So this is going to return
    the average of `.predict()` for each tree.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: I find list comprehensions allow me to write the code in the way the brain works
    [[1:14:24](https://youtu.be/3jl2h9hSRvc?t=1h14m24s)]. You could take the words
    and translate them into this code, or you could take this code and translate them
    into words. So when I write code, I want it to be as much like that as possible.
    I want it to be readable and so hopefully you’ll find when you look at the fast.ai
    code trying to understand how Jeremy did x, I try to write things in a way that
    you can read it and turn it into English in your head.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We’ve nearly finished writing our random forest, haven’t we [[1:15:29](https://youtu.be/3jl2h9hSRvc?t=1h15m29s)]?
    All we need to do now is write `create_tree`. We will construct a decision tree
    (i.e. non-random tree) from a random sample of the data. So again, we’ve delayed
    any actual thought process here. We’ve basically said ok, we could pick some random
    IDs. This is a good trick to know. If you call `np.random.permutation` passing
    in an `int`, it’ll give you back a randomly shuffled sequence from zero to that
    `int`. So if you grab the first `:n` items of that, that’s now a random subsample.
    So this is not doing bootstrapping (i.e. we are not doing sampling with replacement)
    here which I think is fine. For my random forest, I’m deciding that it’s going
    to be something where we do subsampling not bootstrapping.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a14a2ce4e8535f02402de4c95e880637.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: '`np.random.permutation(len(self.y))[:self.sample_sz]`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: So here is a good line of code to know how to write because it comes up all
    the time. I find in machine learning, most algorithms I use are somewhat random
    and so often I need some kind of random sample.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Personally I prefer this over bootstrapping because I feel like most of the
    time, we have more data than we want to put in a tree at once [[1:18:54](https://youtu.be/3jl2h9hSRvc?t=1h18m54s)].
    Back when Breiman created random forest, it was 1999 and was a very different
    world. We now have too much data. So what people tend to do is fire-up a spark
    cluster and they will run it on hundreds of machines when it makes no sense because
    if they had just used a subsample each time, they could have done it on one machine.
    The overhead of spark is a huge amount of I/O overhead. If you do something on
    a single machine, it can often be hundreds of times faster because you don’t have
    this I/O overhead and it also tends to be easier to write the algorithms, easier
    to visualize, cheaper and so forth. So I almost always avoid distributed computing
    and I have my whole life. Even 25 years ago when I was starting in machine learning,
    I still didn’t use clusters because I always feel like whatever I could do with
    a cluster now, I could do with a single machine in five years time. So why not
    focus on always being as good as possible with a single machine. That would be
    more interactive and iterative.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: So again, we delayed thinking to the point where we have to write decision tree
    [[1:20:26](https://youtu.be/3jl2h9hSRvc?t=1h20m26s)]. So hopefully you get an
    idea that this top-down approach, the goal is going to be that we’re going to
    keep delaying thinking so long that eventually we’ve somehow written the whole
    thing without actually having to think. Notice that you never have to design anything.
    You just say, what if somebody already gave me the exact API I needed, how would
    I use it? Then to implement the next stage, what would be the exact API I would
    need to implement that? You keep going down until eventually you notice oh, that
    already exists.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: This assumes we’ve got a class called `DecisionTree` , so we’re going to have
    to create that [[1:21:13](https://youtu.be/3jl2h9hSRvc?t=1h21m13s)]. We know what
    we’re going to have to pass it because we just passed it. So we are passing in
    random sample of `x`’s and `y`’s. We know that a decision tree is going to contain
    decision trees which themselves contain decision trees. So as we go down the decision
    tree, there’s going to be some subset of the original data that we’ve kind of
    got so I’m going to pass in the indexes of the data that we’re actually going
    to use here. So initially, it’s the entire random sample. And we also pass down
    the `min_leaf` size. So everything that we got for constructing the random forest,
    we’ll pass down to the decision tree except, of course, `num_tree` which is irrelevant
    for the decision tree.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`self.n`: how many rows we have in this tree (the number of indexes we’ve given)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.c`: how many columns we have (how ever many columns there are in the
    independent variables)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.val`: For this tree, what’s its prediction. Prediction of this tree is
    the mean of our dependent variable for those indexes. When we talk about indexes,
    we are not talking about the random sampling to create the tree. We’re assuming
    this tree now has some random sample. Inside decision tree, the whole random sampling
    thing is gone. That was done by the random forest. So at this point, we are building
    something that is just a plain old decision tree. It’s not in any way a random
    sampling anything. So indexes is literally which subset of the data have we got
    to so far in this tree.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick Object Oriented Programming primer[[1:24:50](https://youtu.be/3jl2h9hSRvc?t=1h24m50s)]
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I’ll skip this but here is the funny bit about `self`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: You can call it anything you like. If you call it anything other than “self”,
    everybody will hate you and you’re a bad person. [[1:29:24](https://youtu.be/3jl2h9hSRvc?t=1h29m24s)]
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
