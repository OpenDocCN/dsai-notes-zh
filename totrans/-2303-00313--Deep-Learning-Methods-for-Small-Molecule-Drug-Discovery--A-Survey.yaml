- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:41:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2303.00313] Deep Learning Methods for Small Molecule Drug Discovery: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2303.00313](https://ar5iv.labs.arxiv.org/html/2303.00313)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning Methods for Small Molecule Drug Discovery: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wenhao Hu^∗    Yingying Liu^∗    Xuanyu Chen    Wenhao Chai    Hangyue Chen
       Hongwei Wang^†    \IEEEmembershipMember, IEEE and Gaoang Wang^†    \IEEEmembershipMember, IEEE
    ^∗ Equal contribution.^† Corresponding author: Hongwei Wang, and Gaoang Wang.Wenhao
    Hu, Yingying Liu, Xuanyu Chen, and Wenhao Chai are with the Zhejiang University-University
    of Illinois at Urbana-Champaign Institute, Zhejiang University, China (e-mail:
    wenhao.21@intl.zju.edu.cn, yingying.19@intl.zju.edu.cn, xuanyu.19@intl.zju.edu.cn,
    wenhaochai.19@intl.zju.edu.cn).Hongwei Wang, and Gaoang Wang are with the Zhejiang
    University-University of Illinois at Urbana-Champaign Institute, and College of
    Computer Science and Technology, Zhejiang University, China (e-mail: hongweiwang@intl.zju.edu.cn,
    gaoangwang@intl.zju.edu.cn).Hangyue Chen is with the Hangzhou Dianzi University,
    China (e-mail: chy@hdu.edu.cn).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the development of computer-assisted techniques, research communities including
    biochemistry and deep learning have been devoted into the drug discovery field
    for over a decade. Various applications of deep learning have drawn great attention
    in drug discovery, such as molecule generation, molecular property prediction,
    retrosynthesis prediction, and reaction prediction. While most existing surveys
    only focus on one of the applications, limiting the view of researchers in the
    community. In this paper, we present a comprehensive review on the aforementioned
    four aspects, and discuss the relationships among different applications. The
    latest literature and classical benchmarks are presented for better understanding
    the development of variety of approaches.
  prefs: []
  type: TYPE_NORMAL
- en: We commence by summarizing the molecule representation format in these works,
    followed by an introduction of recent proposed approaches for each of the four
    tasks. Furthermore, we review a variety of commonly used datasets and evaluation
    metrics and compare the performance of deep learning-based models. Finally, we
    conclude by identifying remaining challenges and discussing the future trend for
    deep learning methods in drug discovery.
  prefs: []
  type: TYPE_NORMAL
- en: '{IEEEImpStatement}'
  prefs: []
  type: TYPE_NORMAL
- en: As artificial intelligence continues to evolve, an increasing number of deep
    learning algorithms are proposed to utilize extensive experimental data to accelerate
    the time-consuming and costly procedure for drug discovery. Unlike previous surveys
    that tend to focus on a single aspect of drug discovery, we offer a more comprehensive
    survey that encompasses four areas of drug discovery utilizing deep learning techniques,
    including molecule generation, molecular property prediction, retrosynthesis,
    and reaction prediction. Before introducing the main methods, we start with molecular
    data representation, which is also crucial for deep learning algorithms. Moreover,
    we present the recent benchmarks for deep learning-based drug discovery, which
    cover 9 widely used databases.
  prefs: []
  type: TYPE_NORMAL
- en: '{IEEEkeywords}'
  prefs: []
  type: TYPE_NORMAL
- en: deep learning, drug discovery.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/178371c3a75109202d1712882bce7b2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Framework of deep learning methods for drug discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \IEEEPARstart
  prefs: []
  type: TYPE_NORMAL
- en: 'Drug discovery is a very long process that involves extensive experiments and
    different stages. It may take more than ten years and cost around 2.6 billion
    dollars to discover a new drug [[1](#bib.bib1)]. The number of the existing molecules
    has reached more than 130 million, which leads to the fact that the searching
    space for effective drug molecules is pretty significant [[2](#bib.bib2)]. With
    the advent of artificial intelligence, more and more algorithms and methods are
    proposed to utilize the extensive experimental data from chemical labs to accelerate
    the time-consuming and costly procedure for drug discovery. Deep Learning can
    be applied to drug discovery in different stages. Among them, there are several
    vital aspects where the applications hugely help improve performance. In this
    survey, we focus on the following applications: 1) molecule generation, 2) molecular
    property prediction, 3) retrosynthesis, and 4) reaction prediction, as shown in
    Fig. [1](#S0.F1 "Figure 1 ‣ Deep Learning Methods for Small Molecule Drug Discovery:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Molecule generation is the basis of rational drug design and the starting point
    for obtaining new drug molecules. The core problem is to obtain candidate molecules
    that satisfy specific properties and druggability from the huge chemical space.
    Mathematically, molecule generation is usually defined as an optimization problem
    in which the desired molecular properties are predicted by oracle functions. First,
    molecular characteristics are learned from a large set of molecules. Then, novel
    candidates are obtained from the learned distribution with the restriction of
    predefined oracle functions. In the early stage of molecule generation, molecules
    with novel structures can be constructed by combining fragments of existing compounds
    [[3](#bib.bib3), [4](#bib.bib4)]. This requires assistance of chemical experiment
    which is relatively inefficient. Optimization algorithms such as genetic algorithms
    [[5](#bib.bib5)] are also considered for solving the molecule generation problem.
    Based on evolution principles, genetic algorithms obtain molecules with specific
    properties by changing parameters of a population. While the early approaches
    rely on stochastic steps that struggle to capture the constraints of chemical
    design [[6](#bib.bib6)]. Deep learning brings the ability to learn from vast amounts
    of data and the potential for drug design beyond chemical intuition, narrowing
    the $10^{60}$ size chemical space to dozens of valid candidate compounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Molecular property prediction is a crucial part of drug development, and its
    primary purpose is to predict the drug activity, absorption, toxicity, and other
    properties of candidate compounds. Molecular property prediction has two main
    applications: quantitative structure-property relationship (QSPR) and virtual
    screening. QSPR models are usually defined as regression or classification models
    which predict property by establishing relationships between molecule structure
    and property. Early machine learning methods such as random forest [[7](#bib.bib7)],
    k nearest-neighbor [[8](#bib.bib8)], support vector machine [[9](#bib.bib9)] have
    been used for QSPR problems. These methods still perform better than modern deep
    learning methods for some specific problem instances [[10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]. For example, algorithms
    using extremely randomized trees achieve better performance than deep learning
    methods on small datasets for organic molecular energy prediction problems [[15](#bib.bib15)].'
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of virtual screening is to search the molecule library in order
    to identify those drug ligand structures which are most likely to bind to a target
    protein [[16](#bib.bib16), [17](#bib.bib17)]. Traditional virtual screening methods
    need to synthesize many compounds for biological experiments, and the whole process
    has a high cost, long cycle, and low success rate. The virtual screening of drugs
    through deep learning is expected to replace traditional activity screening methods,
    speed up intermediate steps, and significantly reduce costs. For instance, a particularly
    successful application is deep learning-based score function that can predict
    the affinities of protein-bound ligands with high accuracy [[18](#bib.bib18)].
    More specifically, AtomNet incorporates the 3D structural features of the protein–ligand
    complexes to the CNNs [[19](#bib.bib19)]. Feng et al. [[20](#bib.bib20)] combine
    the fringerprint and graph representations as ligand features. This model showes
    better performance on the Davis [[21](#bib.bib21)], Metz [[22](#bib.bib22)], and
    KIBA [[23](#bib.bib23)] benchmark datasets. Gao et al. [[24](#bib.bib24)] use
    the LSTM recurrent neural networks for the protein sequences and the GCN for ligand
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: Retrosynthesis analysis and reaction prediction are the critical links in drug
    synthesis. Reaction prediction explores possible product molecules synthesized
    by given reactants [[25](#bib.bib25)]. Contrary to reaction prediction, retrosynthesis
    is a reverse extrapolation from the product molecule to accessible starting materials
    [[26](#bib.bib26)]. Early computer-aided synthesis planning primarily relied on
    human-encoded reaction rules, which can be only applied to specific reactions
    [[27](#bib.bib27)]. With neural networks, the reaction patterns of molecules and
    the breaking rules of chemical bonds are learned from the existing massive chemical
    reaction data. These rules are applied to the retrosynthesis analysis and reaction
    prediction of drug molecules, which can significantly speed up the synthesis of
    new drugs.
  prefs: []
  type: TYPE_NORMAL
- en: Many existing surveys focus on drug discovery in the AI era. Lavecchia [[28](#bib.bib28)]
    analyzes the application of machine learning techniques in ligand-based virtual
    screening. Gawehn et al. [[29](#bib.bib29)] use several classic deep neural networks
    as examples to introduce the potential application fields. Some present how machine
    learning takes advantage of big data to improve the traditional method and the
    applications in various stages of drug discovery [[30](#bib.bib30)]. While those
    works focus on general aspects of drug discovery, other surveys emphasize one
    specific task and analyze state-of-the-art (SOTA) models for the particular application.
    For property prediction, some papers review the main methods, introduce several
    fundamental machine learning techniques, and analyze the performance from the
    data side [[31](#bib.bib31), [32](#bib.bib32)]. For molecular design, Tong et
    al. [[33](#bib.bib33)] summarize generative models and list several tools which
    use those models, while Elton et al. [[34](#bib.bib34)] focus on the generation
    of lead molecules, classifying the models and comparing their performances. For
    the synthesis procedure, some surveys discuss the dataset, models, and tools for
    retrosynthesis and reaction prediction and point out the potential development
    directions [[26](#bib.bib26), [35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, our survey first integrates the four applications
    mentioned above. The main contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize the latest literature and classical benchmarks in four applications
    and categorize them according to molecule representations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We enumerate commonly used datasets and evaluation metrics and compare the results
    of existing models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose the problems of existing methods and analyze the future development
    direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Following the three main components of deep learning: representation, model,
    and data [[36](#bib.bib36)], our organization for the paper will be as follows:
    we first briefly introduce the formats used in deep learning to represent the
    molecular structure in Section [2](#S2 "2 Molecule Representations ‣ Deep Learning
    Methods for Small Molecule Drug Discovery: A Survey"), including fingerprints,
    simplified molecular input line entry system (SMILES) and molecular graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, molecule generation, molecular property prediction, retrosynthesis
    and reaction prediction are introduced in Section [3](#S3 "3 Molecule Generation
    ‣ Deep Learning Methods for Small Molecule Drug Discovery: A Survey"), Section [4](#S4
    "4 Molecular Property Prediction ‣ Deep Learning Methods for Small Molecule Drug
    Discovery: A Survey"), Section [5](#S5 "5 Retrosynthesis ‣ Deep Learning Methods
    for Small Molecule Drug Discovery: A Survey"), and Section [6](#S6 "6 Reaction
    Prediction ‣ Deep Learning Methods for Small Molecule Drug Discovery: A Survey"),
    respectively. We also provide standard datasets, existing benchmark platforms,
    and usual evaluation metrics for all four tasks in Section [7](#S7 "7 Datasets
    and Benchmarks ‣ Deep Learning Methods for Small Molecule Drug Discovery: A Survey").
    Finally, we discuss the challenges and possible development directions in Section [8](#S8
    "8 Challenges and Future trend ‣ Deep Learning Methods for Small Molecule Drug
    Discovery: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Molecule Representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3da122ae03f638779493ef2a478ee93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Taxonomy for deep neural network methods for drug discovery.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to convert the molecular structure to computer-readable information,
    there are several ways to represent the molecules. This section focuses on three
    common representations: fingerprints, SMILES string, and molecular graphs. After
    the introduction of the data representations, we show the taxonomy of the deep
    learning methods for drug discovery according to different representations in
    Fig. [2](#S2.F2 "Figure 2 ‣ 2 Molecule Representations ‣ Deep Learning Methods
    for Small Molecule Drug Discovery: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Fingerprints
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Molecular fingerprint is a bit string that encodes the structural information
    of a molecule. Two types of fingerprints are widely used for molecule representation,
    i.e., key-based fingerprints and hash fingerprints.
  prefs: []
  type: TYPE_NORMAL
- en: Key-based fingerprints, including molecular ACCess system (MACCS) [[37](#bib.bib37)],
    and PubChem fingerprint [[38](#bib.bib38)], have a predefined fragment library
    so that each molecule can be encoded into a binary bit stream according to its
    substructure. The MACCS contains 166 predefined fragments and the fragment keys
    are implemented in many cheminformatics software packages, including RDKit [[39](#bib.bib39)],
    OpenBabel [[40](#bib.bib40)], CDK [[41](#bib.bib41)]. The PubChem fingerprint
    has 881 bits representing element count, type of ring system, atom pairing, nearest
    neighbors, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Hash fingerprints, such as Morgan fingerprints and functional-class fingerprints,
    do not have predefined substructures. Instead, they extract fragments from the
    given dataset and convert them into numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: Due to extraction of molecular substructures and matching process, obtaining
    molecular fingerprints is quite complicated. Meanwhile, the representation ability
    of binary molecular fingerprints is limited. Though molecular fingerprints are
    still used in SMILES [[42](#bib.bib42)] and molecular graphs [[43](#bib.bib43)],
    there are less works that take molecular fingerprints as the representation approach
    in recent progress.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 SMILES
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simplified molecular input line entry system (SMILES) [[44](#bib.bib44)] is
    the method that uses strings to represent the molecular structure. Complex molecular
    configurations are transformed into sequences in vector form, which is easy to
    obtain. Meanwhile, with the development of natural language processing techniques,
    SMILES string can be regarded as a sentence for molecule representations [[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)]. However, the SMILES representation also has
    certain limitations. The SMILES representation length varies with the molecule
    size, which poses significant challenges in devising generic models. Also, the
    property of sentences makes it challenging to represent three-dimensional information
    about molecules.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Molecular Graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Graphs are realistic representations of molecules, preserving the rigorous details
    of the topology and geometry structures. The nodes in the graph can represent
    the attribute, and valence state of atoms, while the edges in the graph contain
    information such as the type and length of chemical bonds. Deep learning methods
    for graphs, especially for graph neural networks (GNNs), have promising performance
    in many tasks, such as property prediction and reaction prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to SMILES and manually designed fingerprints in sequence, the graph-formed
    representation alleviates the order ambiguity represented in sequence form. It
    preserves rich structural information, such as the distances between atoms and
    the angles between bonds. Chen et al. [[48](#bib.bib48)] emphasize the great importance
    of edge features, inspiring more to be explored by preserving atom and bond information
    via the propagation of GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Molecule Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/254a4d83bebd59c753560b4236954518.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustrations of (A) Recurrent Neural Networks, (B) Variational Autoencoders
    and (C) Graph Neural Networks in molecule generation.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 String-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on this molecular representation method, some standard deep learning models
    for processing sequence information, such as recurrent neural networks (RNNs),
    variational auto-encoders (VAEs), and genetic algorithm, are applied to the molecular
    generation model. These methods have been shown to perform well for many domains
    in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent Neural Networks (RNN). RNNs generate output symbols based on input
    from previous steps, then these symbols are gathered to form generated molecule
    (Fig. [3](#S3.F3 "Figure 3 ‣ 3 Molecule Generation ‣ Deep Learning Methods for
    Small Molecule Drug Discovery: A Survey")A). Olivecrona et al. [[49](#bib.bib49)]
    develop a policy based RL approach to fine-tune a pre-trained RNN network. Segler
    et al. [[50](#bib.bib50)] generate large sets of diverse molecules for virtual
    screening campaigns and then generate smaller, focused libraries enriched with
    possibly active molecules for a specific target. Stacked RNN layers are used to
    generate new molecular SMILES representations. This work demonstrates the robustness
    and transferability of RNNs in molecular generative models. Gupta et al. [[51](#bib.bib51)]
    also follow the idea of pre-training on large-scale molecular datasets and fine-tuning
    on small-scale datasets to generate molecules of specific properties. This work
    also highlights the potential of RNN-based generative models to complete molecules
    with particular properties from molecular fragments. Grisoni et al. [[52](#bib.bib52)]
    propose a method for generative and data augmentation from both directions of
    the sequence. Since molecular sequences are not as directional as NLP-like sequences,
    this attempt has been shown to perform better than unidirectional RNNs in experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: Many works also employ long-short term memory (LSTM) networks. Bjerrum et al.
    [[53](#bib.bib53)] train LSTM-based molecular generative models on two datasets
    consisting of fragment-like and drug-like molecules. Ertl et al. [[54](#bib.bib54)]
    use an LSTM-based molecule generation model to generate a large number of new
    molecules in a short period, of which structural features and functional groups
    remain closed within the drug-like space defined by the bioactive molecules from
    ChEMBL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variational Auto-Encoders (VAE). VAEs encode high-dimensional, discrete chemical
    space into a low-dimensional, continuous latent space, which can be used to sample
    molecules with given property (Fig. [3](#S3.F3 "Figure 3 ‣ 3 Molecule Generation
    ‣ Deep Learning Methods for Small Molecule Drug Discovery: A Survey")B). Schiff
    et al. [[55](#bib.bib55)] only use SMILES representations instead of graph-based
    molecular representations. The method is well-performed by incorporating 3D geometric
    information into the VAE. Alperstein et al. [[56](#bib.bib56)] consider the non-uniqueness
    of molecular SMILES expression and propose encoding multiple SMILES strings of
    a single molecule. VAE effectively learns the hidden information between different
    SMILES representations and performs well in molecule generation and optimization
    in fully supervised and semi-supervised situations.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the commonly used RNN and VAE models, Nigam et al. [[57](#bib.bib57)]
    combine the traditional genetic algorithm with deep neural network. The DNN-based
    discriminator improves the diversity of generated molecules and increases the
    interpretability of genetic algorithm. Moss et al. [[58](#bib.bib58)] first use
    string kernels and genetic algorithms in bayesian optimization loops. Most bayesian
    optimization approaches need to convert the original string inputs to fixed-size
    vectors, while this architecture can directly work on raw strings.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, string-based molecule generation considers the input and target
    molecules as the sequence data. Deep learning models like RNN, VAE, and genetic
    algorithm achieve great success in the generation task. However, there is still
    a limitation for the string-based method since it may lose the complex geometry
    information of molecules. Thus, most current works focus on the graph-based model.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Graph-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motif, also referred to as scaffold or rationale, is defined as the molecule
    fragment. Motif corresponds to the concept of functional groups in chemistry,
    closely related to the molecule’s chemical properties. Thus, motif can provide
    more prior knowledge when generating molecules with given properties. Meanwhile,
    motif already has a specific valid structure. With the involvement of motif, generating
    invalid molecules can be avoided to some extent. Motif-based molecular generation
    treats molecules as a collection of fragments. Substructures are then spliced
    to generate new molecules. Non-motif molecular generation generates molecules
    atom by atom (Fig. [3](#S3.F3 "Figure 3 ‣ 3 Molecule Generation ‣ Deep Learning
    Methods for Small Molecule Drug Discovery: A Survey")C).'
  prefs: []
  type: TYPE_NORMAL
- en: Motif-based Molecular Generation. Jin et al. [[43](#bib.bib43)] generate molecular
    graphs using a variational auto-encoder. They first extract valid chemical substructures
    from the training set to generate a tree-structured scaffold, then use the subgraphs
    as building blocks and combine them to form a molecular graph. Compared with the
    node-by-node approach, this structure-by-structure approach maintains chemical
    validity at each step. Jin et al. [[59](#bib.bib59)] further combine variational
    auto-encoder with an adversarial training method to obtain various and valid output
    distributions. An adversarial regularization is included to align the distribution
    of generated graphs with the distribution of valid targets. Jin et al. [[60](#bib.bib60)]
    propose a hierarchical graph encoder-decoder that can generate large molecules
    such as polymers. Each molecule has a fine-to-coarse representation from atom
    level to motif level. Similar to [[43](#bib.bib43)], Maziarz et al. [[61](#bib.bib61)]
    use extracted motif to generate molecules. Meanwhile, they integrate this method
    with atom-by-atom generation. You et al. [[62](#bib.bib62)] develop a graph convolutional
    network through reinforcement learning. The generation procedure is regarded as
    Markov Decision Process, and domain-specific rewards are incorporated into the
    model. Yang et al. [[63](#bib.bib63)] also propose a reinforcement learning framework
    that generates pharmacochemically acceptable molecules with significant docking
    scores. This method chooses a chemically realistic and pharmacochemically acceptable
    fragment based on the given state of the molecule. To generate molecules with
    multiple property constraints, Jin et al. [[64](#bib.bib64)] extract rationales
    for each property and combine them as multi-property rationales. A graph completion
    model is then implemented to generate complete molecules from rationales. Based
    on [[64](#bib.bib64)], Chen et al. [[65](#bib.bib65)] propose an Expectation-Maximization
    like algorithm. In the E-step, an explainable model extracts rationales from candidate
    molecules. While in the M-step, rationales are completed to generate molecules
    with a higher property score. Xie et al. [[66](#bib.bib66)] employ the annealed
    Markov chain Monte Carlo sampling method [[67](#bib.bib67)] to search the vase
    chemical space. They further train a graph neural network based on the sampling
    result to choose proper candidate edits. Fu et al. [[68](#bib.bib68)] propose
    differentiable scaffolding tree which is a gradient-based optimization method.
    Graph convolutional network transforms the discrete chemical space into a locally
    differentiable space. Further, the molecule is optimized by gradient back-propagation
    from the target.
  prefs: []
  type: TYPE_NORMAL
- en: Non-motif Molecular Generation. Zhou et al. [[69](#bib.bib69)] generate molecule
    atom by atom with the help of the double Q-learning technique. This method does
    not require pre-training, so that the training set does not limit exploration
    capability. Combining autoregressive and flow-based methods, Shi et al. [[70](#bib.bib70)]
    develop a flow-based autoregressive graph generation model, allowing parallel
    computation in the training process. The model generates edges and nodes concerning
    the current graph. In each generation step, chemical knowledge like the valence
    rule could be utilized to check the molecular validity. For molecule optimization
    problem, Korovina et al. [[71](#bib.bib71)] develop a bayesian optimization model
    that focuses on small organic molecules. Chen et al. [[72](#bib.bib72)] propose
    a cost-effective evolution strategy in latent space. An evolutionary algorithm
    is introduced into the latent space to search for the desired molecules. Wu et
    al. [[73](#bib.bib73)] propose distilled graph attention policy network, which
    is aimed to optimize molecule structure based on user-defined objectives. A spatial
    graph attention mechanism is introduced, considering self-attention over the node
    and edge attributes.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to exploring 2D molecule generation, many works have been done on
    3D molecule generation tasks. Simm et al. [[74](#bib.bib74)] combine a conditional
    variational autoencoder with a euclidean distance geometry algorithm. This method
    uses a set of pairwise distances between atoms to describe molecule conformation
    instead of cartesian coordinates. Following [[74](#bib.bib74)], Xu et al. [[75](#bib.bib75)]
    first propose an end-to-end conditional variational autoencoder framework for
    molecular conformation prediction. In this work, the distance prediction problem
    and the distance geometry problem are simultaneously optimized via bilevel programming.
    Shi et al. [[76](#bib.bib76)] directly estimate the gradient fields of the log
    density of atomic coordinates. The estimated gradient fields can generate conformations
    directly via Langevin dynamics, thus reducing the computation error. Zhu et al.
    [[77](#bib.bib77)] design a loss function invariant to the roto-translation of
    coordinates of conformations and permutation of symmetric atoms in molecules.
    Xu et al. [[78](#bib.bib78)] analogize conformation generation to diffusion process
    in thermodynamics. Atoms are first stabilized in specific conformations and then
    diffuse into a noise distribution. The reverse process is regarded as Markov chain
    solving for conformation generation problems. Luo et al. [[79](#bib.bib79)] propose
    an autoregressive flow model which can generate 3D molecules. This method directly
    generates distances, angles, and torsion angles of atoms rather than 3D coordinates.
    Once a molecule is generated, the relative distances and angles between atoms
    are determined, thus ensuring invariance and equivariance. Mansimov et al. [[80](#bib.bib80)]
    propose a conditional deep generative graph neural network to learn the energy
    function of molecule conformations. Compared with conventional molecular force
    field methods, this model can directly generate energy-supported molecular conformations
    without requiring multiple iterations. Guan et al. [[81](#bib.bib81)] also propose
    an energy-inspired neural optimization formulation. The molecule conformation
    is optimized by gradient descent for 3D coordinates through the energy surface.
    Xu et al. [[82](#bib.bib82)] propose a method that combines flow-based and energy-based
    models. Flow-based model is used to generate a distance matrix from a Gaussian
    prior. Then the energy-based model searches and optimizes the 3D coordinates of
    the molecule.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Molecular Property Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12ea22a42fd64546adb4d74a8c909a5c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustrations of (A) Recurrent Neural Networks, (B) Transformer and
    (C) Graph Neural Networks in molecular property prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 SMILES-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SMILES-based methods utilize various architectures, such as RNN, VAE, and
    transformers, which are demonstrated as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent Neural Networks (RNN). RNNs are shown to be effective designs for
    sequential input such as text data. Thus, RNN and its two variants, LSTMs [[83](#bib.bib83)]
    and GRUs [[84](#bib.bib84)], are applied to learn chemical properties from SMILES
    (Fig. [4](#S4.F4 "Figure 4 ‣ 4 Molecular Property Prediction ‣ Deep Learning Methods
    for Small Molecule Drug Discovery: A Survey")A).'
  prefs: []
  type: TYPE_NORMAL
- en: Mayr et al. [[85](#bib.bib85)] employe long short-term memory networks to construct
    the SmilesLSTM architecture, which utilizes SMILES strings as inputs. Inspired
    by the success of RNNs in sequence-to-sequence language translation, Goh et al.
    propose a sequence-to-vector model SMILES2vec [[86](#bib.bib86)], where the sequence
    is SMILES, and the vector is measured property. Instead of explicitly encoding
    the molecular information from element compositions in SMILES, they explore four
    CNN-based and RNNs-based architectures to learn the desired features directly.
    Meanwhile, an explanation mask is developed to localize the most important characters,
    improving the model’s interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: While Goh et al. [[86](#bib.bib86)] stick to the commonly-used canonical SMILES,
    which preserves a one-to-one mapping from molecule to string, Li et al. [[87](#bib.bib87)]
    make use of multiple SMILES strings of a single molecule as data augmentations,
    which helps to learn better grammatical features. The proposed model generates
    the molecular representation from multiple SMILES sequences that first convert
    to a one-hot vector and then follow the message-passing process in the stacked
    CNN and RNN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Considering that chemical information is able to canonicalize SMILES strings,
    Peng et al. [[88](#bib.bib88)] exploit a framework called TOP, combining the usage
    of SMILES and predefined properties, including root atom positions and isomeric
    features. A bidirectional gated recurrent unit-based RNN (BiGRU) is employed for
    the SMILES strings to capture local and global context information. At the same
    time, a fully connected neural network (FCN) is used to process the physicochemical
    feature vector. When performing on balanced datasets, TOP significantly outperforms
    other methods, including SMILES2vec [[86](#bib.bib86)], CheMixNet [[89](#bib.bib89)],
    and Chemception [[90](#bib.bib90)], which uses only the images of 2D molecular
    drawings decoded from SMILES.
  prefs: []
  type: TYPE_NORMAL
- en: Variational Auto-Encoder (VAE). VAE models share a similar structure to the
    encoder-decoder structure mentioned in the RNN-based architecture. However, it
    is different in formulating the assumption that the embedded space follows Gaussian
    distributions. Among the existing architectures, VAEs focus more on molecular-based
    latent representations instead of specifying prediction tasks. Adapting the SSVAE
    devised in [[91](#bib.bib91)], Kang and Cho feed SMILES as the input variables
    and its continuous-valued property vectors as outputs. The encoder and decoder
    networks perform the recovery task of SMILES with the learned latent molecular
    representation as an intermediate between the two stages. Once the SSVAE is trained,
    the remaining RNN acts as the predictor network. ALL SMILES VAE [[56](#bib.bib56)]
    ensures the learned latent molecular representation to capture the molecular feature
    instead of its SMILES realization by using multiple SMILES and implicitly performing
    message passing among spanning trees of the molecular graph via recurrent structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformer. Transformer architecture follows a similar encoder-decoder process,
    but it does not employ recurrent connections like RNNs and is thus prone to be
    more stable to converge and superior to RNNs in featurization on longer and larger
    corpus (Fig. [4](#S4.F4 "Figure 4 ‣ 4 Molecular Property Prediction ‣ Deep Learning
    Methods for Small Molecule Drug Discovery: A Survey")B) [[45](#bib.bib45), [46](#bib.bib46)].
    To guarantee the learned molecular representations are able to successfully transfer
    in numerous downstream tasks, pre-training strategies on a large SMILES corpus
    stand out. SMILES-GPT [[46](#bib.bib46)], prolonging the pre-training approach,
    applies a lightweight adapter network after each attention block on the design
    of GPT-2 [[47](#bib.bib47)] to learn specific task patterns. In this way, it alleviates
    the loss of domain knowledge when transferring the pre-trained model to the separate
    downstream task.'
  prefs: []
  type: TYPE_NORMAL
- en: The language representation model BERT [[92](#bib.bib92)] introduces masked
    language model (MLM) pre-training techniques that alleviate the constraints of
    incorporating context from uni-direction. Benefiting from the similarities between
    MLM and atom masking in molecules, the BERT-style architecture has emerged as
    a robust technique in molecular representation learning. The efficacy of BERT-liked
    architectures, such as RoBERTa [[93](#bib.bib93)], is proved to achieve comparatively
    high classification accuracy when training through an extensive database [[94](#bib.bib94)].
    With a careful exploration of pre-training dataset size, tokenizer, and string
    representation, ChemBERTa [[95](#bib.bib95)] adapted from RoBERTa [[93](#bib.bib93)]
    again emphasizes the efficiency of large-scale pre-training in molecular property
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Fingerprint-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fingerprints, as another class of molecular representations, also benefits molecular
    predictive tasks. Unterthiner et al. [[96](#bib.bib96)] employ multi-task learning
    in molecular property prediction, leveraging the power of deep neural networks
    to generate hierarchical representations of compounds. Meanwhile, Mayr et al. [[97](#bib.bib97)]
    introduce the DeepTox pipeline. DeepTox employs normalization techniques to generate
    standardized chemical representations of compounds, and subsequently computes
    an extensive array of chemical descriptors that serve as input to machine learning
    algorithms. Traditionally hand-crafted fingerprints are also introduced as the
    complement of SMILES strings for molecular feature information. In this way, the
    model performance no longer fluctuates with the length of SMILES strings drastically.
    Paul et al. [[89](#bib.bib89)] harness two successful architectures for property
    prediction, including SMILES2vec and MLP, into a multi-input-single-output (MISO)
    architecture CheMixNet. The model takes both SMILES strings and MACCS fingerprints
    as inputs. Leveraging both types of molecular structural representations as parallel
    inputs increase generalizability. Schimunek et al. [[98](#bib.bib98)] present
    a novel few-shot method that enhances the representation of a molecule by incorporating
    information from known context or reference molecules.
  prefs: []
  type: TYPE_NORMAL
- en: However, the performance of traditional fingerprints is not good enough in prediction
    tasks due to the restrictions of the manually designed methods. For example, the
    hash-based fingerprint brings limited task-specific information due to its irreversibility.
    With the advancement of deep-learning techniques, novel methods have been invented
    to generate deep-learning-based fingerprints for property prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks (RNN). RNNs-based unsupervised fingerprint methods
    are proposed to tackle the problem of scarce labeled data. Deep neural networks
    are applied to utilize a large pool of unlabeled data. Motivated by the breakthrough
    success of the model applied in seq-to-seq language translations, encoder-decoder
    structured neural networks have been popular in generating fingerprints based
    on sequence-like SMILES strings. Xu et al. [[42](#bib.bib42)] extract the intermediate
    fixed-size molecular feature vectors as seq2seq fingerprints during the process
    of mapping a SMILES string to the desired vector and then translating back to
    the original string. The extracted fingerprints, combined with SMILES labels,
    are trained for prediction tasks in supervised methods with classifiers or regressors,
    such as multi-layer perceptron. Based on the design, Zhang et al. [[99](#bib.bib99)]
    modify it to the first end-to-end framework coupling recovery and inference tasks
    in a model named seq3seq fingerprint.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer. Inspired by the excellent performance of the self-attention mechanism
    in language tasks, Transformer is believed to have a good performance on abundant
    unlabeled molecular data. By adopting the pre-training approach that shows promising
    results in the NLP field, SMILES transformer [[45](#bib.bib45)] is presented to
    learn molecular fingerprints through large-scaled pre-training on canonical SMILES.
    The data-driven extracted fingerprints work well with a simple multi-layer perceptron,
    which shows the great potential of exploiting unsupervised pre-training in property
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: BERT, similar to the encoder of the Transformer, is also a promising structure.
    SMILES-BERT [[100](#bib.bib100)], following the BERT approach, randomly masks
    selected tokens in an input SMILE to the mask token, any other token in the dictionary,
    or unchanged according to an 85/10/5 split ratio. The fingerprints generated from
    the MLM strategy are experimentally proved to have higher predictive accuracy
    on a rather large dataset than some representative fingerprints, including circular
    fingerprint [[101](#bib.bib101)], neural fingerprint [[102](#bib.bib102)] and
    seq2seq fingerprint [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Graph-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Graph Convolutional Network (GCN). Graph convolutional networks (GCN) [[103](#bib.bib103)]
    are always considered a common baseline choice for applications. The model focuses
    on learning node states and aggregates the neighboring messages (Fig. [4](#S4.F4
    "Figure 4 ‣ 4 Molecular Property Prediction ‣ Deep Learning Methods for Small
    Molecule Drug Discovery: A Survey")C). A wide range of applications in molecule
    representation learning is based on GCNs due to its lightweight calculation and
    scalability to large graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Labeled datasets, for instance, the QM9 dataset [[104](#bib.bib104)] is composed
    of molecular properties approximated by costly methods such as density functional
    theory. The neighborhood aggregation scheme of GCNs is thus frequently used to
    exploit an overall high-order representation learning for molecules. A multilevel
    graph convolutional neural network proposed in [[105](#bib.bib105)] learns the
    node representations by preserving the conformation information with hierarchical
    modeling (atom-wise, pair-wise, etc), and its spatial information by introducing
    a radial basis function layer for robust distance tensors. The interaction layers
    that effectively combine self features with collected messages from neighbors
    are applied for high-order atom representations.
  prefs: []
  type: TYPE_NORMAL
- en: It is often challenging that annotated labels are so expensive that datasets
    are commonly seen to be partially labeled in real-world applications. Training
    on such limited labeled data easily leads to over-fitting and poor performance
    of dissimilar molecules from the training data. To address the problem of scarce
    task-specific labels and out-of-distribution predictions, researchers have made
    great efforts in pre-training to leverage the unlabeled data and improve the generalization
    power of GNNs. The key to pre-training is finding a suitable and effective task
    to leverage many unlabeled structures [[106](#bib.bib106)], yet choosing effective
    pre-training strategies is challenging since the selected properties must be aligned
    with the interests of specific downstream tasks. Otherwise, as mentioned above,
    the “negative transfer” effect can seriously harm the generalization when transferring
    the pre-learned knowledge to a new downstream task [[107](#bib.bib107)].
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training is a common and effective strategy for CNNs, ImageMol[[108](#bib.bib108)]
    though the robust scheme has not been explored for GNNs until recent years. Hu
    et al. [[107](#bib.bib107)] conduct the first systematic large-scale investigation
    of pre-training strategies for GNNs. They propose self-supervised context prediction
    and attribute masking methods at the node level and supervised task predictions
    for domain-specific information decoding. Along with the foremost introduction
    of pre-training strategies for property prediction models, Hu et al. [[107](#bib.bib107)]
    apply context prediction and attribute masking with various GNN structures pre-trained
    on eight datasets from MoluculeNet, proving the efficacy of node-level strategies,.
    These methods open new ways for GNN pre-training schemes yet are far from satisfactory.
    Rong et al. [[109](#bib.bib109)] argue that the graph-level tasks are impeded
    with limited labels and introduce more risks of negative transfer in downstream
    tasks. Also, isolating the context and node type predictions as labels makes it
    hard to preserve local structure knowledge or address highly frequent atoms. To
    achieve better generalizability, some have made great efforts to investigate pre-training
    with data augmentation. The proposed model GraphCL [[110](#bib.bib110)] performs
    pre-training through maximizing MI between two augmented representations in the
    latent space generated by different data augmentation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by these generalized techniques, Wang et al. [[111](#bib.bib111)] introduce
    the specific molecular-designed model MolCLR that addresses GNNs pre-training
    with graph augmentations by contrastive learning. It follows a similar manner
    as GraphCL [[110](#bib.bib110)] by maximizing distance between two latent augmented
    vectors resulting from two stochastic augmentations. However, augmentation methods
    are more carefully designed and more specified for molecules. Despite atom masking
    and subgraph removal, the model utilizes bond deletions instead of random perturbations
    that can mimic the reactions of chemical bonds to learn more valuable features.
    However, researchers are still doubting the effectiveness of data augmentation
    in pre-training. Li et al. [[112](#bib.bib112)] argue that random modification
    of atoms and edges can harmfully destroy the natural structure. Liu et al. [[113](#bib.bib113)]
    propose the N-gram graph, which is a unsupervised representation for molecules.
    The approach commences by embedding the vertices in the molecule graph. It then
    creates a concise representation for the graph by assembling the vertex embeddings
    in short walks within the graph. Altae et al. [[114](#bib.bib114)] develop a architecture
    that combines iterative refinement long short-term memory with graph convolutional
    neural networks. This one-shot learning approach allows for significant reductions
    in the amount of data needed to make accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Messsage Passing Neural Network (MPNN). When inferring molecular properties,
    the impact of bond types and distances of atoms can not be neglected. GCNs, though
    considering neighborhood aggregation, are not able to distinguish different edge
    types. To better handle edge features, Gilmer et al. [[115](#bib.bib115)] formally
    propose a message passing neural network (MPNN) to learn a graph level embedding
    with node features as well as the weighted edge messages. The framework involves
    two phases, a message-passing phase and a readout phase. By modeling atoms as
    nodes and bonds as edges, feature vectors of nodes and edges are input to the
    model as initialization. Further, message are aggregated from neighboring nodes
    through multiple layers of message passing. The readout phase then summarizes
    the feature vectors of the whole graph to target the downstream molecular property
    prediction task. Variations based on MPNN [[115](#bib.bib115)] have made great
    progress, such as message passing with attention mechanism in [[109](#bib.bib109)],
    spherical message passing for 3D molecular graphs [[116](#bib.bib116)] and so
    on. Despite the focus on node states, preservation of important bond (edge) information
    is also taken into account for better performance [[48](#bib.bib48)].
  prefs: []
  type: TYPE_NORMAL
- en: The message passing scheme has shown its superior performance in molecular property
    prediction. Hao et al. [[117](#bib.bib117)] make the first attempt to employ the
    powerful framework for molecular property prediction in a semi-supervised manner.
    The novel strategy, by finding the most diversified subset in the unlabeled set
    and continuously adding them to the labeled dataset for training, alleviates the
    over-fitting and imbalance between labeled and unlabeled molecules.
  prefs: []
  type: TYPE_NORMAL
- en: Under the assumption of message passing neural networks, two atoms in the same
    neighborhood should have similar representations, neglecting that their positions
    in the molecule are distinct. Therefore, MPNNs are usually armed with positional
    encoding to alleviate the limitations. Instead of applying higher-order representations,
    Dwivedi et al. [[118](#bib.bib118)] introduce positional encoding (PE) for nodes
    and leverage the information as input information to learn both positional and
    structural feature representations at the same time. Combining PE with MPNNs contributes
    to more robust node embedding since it fills the gap in the canonical positioning
    of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: While 2D molecule graph provides rich information on topology, 3D geometric
    views also play a vital role in predicting molecular characteristics. Based upon
    the original message passing network [[115](#bib.bib115)], Liu et al. [[116](#bib.bib116)]
    propose the spherical message passing and SphereNet for 3D molecular learning.
    The model performs message passing in the spherical coordinate system and uses
    the relative 3D information and torsion computation to generalize invariant predictions
    of rotation and translation. Furthermore, it is shown that 3D geometric views
    significantly contribute to robust representation learning. In GeomGCL [[112](#bib.bib112)],
    the efficiency of 3D geometry is illustrated to be superior to other methods that
    confine to topology structures. Li et al. [[112](#bib.bib112)] devise a method
    to leverage 2D and 3D pair information to improve generalization ability. The
    dual-channel geometric message passing architecture allows collaborative supervision
    of 2D and 3D views between each other. To further leverage the local spatial correlations,
    a regularizer on angle domains is applied for generalization on similar property
    prediction. These strategies help achieve the goal of capturing both chemical
    semantic information and geometric information.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Isomorphism Network (GIN). Capturing statistical dependencies is essential
    in molecular presentation, and the contrastive method is among the most effective
    approaches to obtaining the features [[119](#bib.bib119)]. Borrowed the idea of
    mutual information neural estimator (MINE) from mutual information (MI), deep
    InfoMax (DIM) [[120](#bib.bib120)] is introduced to maximize information content
    between input and output. Inspired by DIM, deep graph InfoMax (DGI) [[119](#bib.bib119)]
    trains a contrastive node encoder that uses GCN to maximize local mutual information
    to capture global information. However, the GCN convolutional encoding fails to
    distinguish some graph structures. Instead of mean and max aggregation, a more
    careful design of neighbor aggregation is needed for more substantial discriminative
    power for graph representations.
  prefs: []
  type: TYPE_NORMAL
- en: Graph isomorphism network (GIN) [[121](#bib.bib121)] is then proposed and shown
    to be as powerful as Weisfeiler-Lehman graph isomorphism test that determines
    whether two graphs are topologically identical. The structure using sum aggregation
    has better performance in discriminating graph instances. Sun et al. [[122](#bib.bib122)]
    replace the graph convolution encoders with GIN and expands the mutual information
    idea in semi-supervised learning scheme using a ”student-teacher” model to train
    the whole graph level embedding.
  prefs: []
  type: TYPE_NORMAL
- en: GINs are frequently used for backbone models in recent 2D GNN designs for molecular
    applications. Liu et al. [[106](#bib.bib106)] consider 3D geometric views for
    pre-training with message passing, aiming to train the 2D molecular GIN encoder
    to recover its 3D counterparts to some extent. By maximizing MI between 2D topological
    structure and 3D spatial and geometric information, the pre-trained model can
    benefit from implicit 3D geometric prior information, even if no 3D structure
    is provided in the downstream task. Meanwhile, a comparisons on two pre-training
    models of GraphCL [[110](#bib.bib110)] and GraphMVP are also illustrated in [[106](#bib.bib106)],
    showing that incorporate both 2D topology and 3D geometry is more competitive.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Graph Neural Networks. Recurrent neural networks, such as RNNs, GRUs,
    and LSTMs, are powerful tools for processing vary-sized sequences. Converting
    graphs to sequences is a potential solution to get rid of the fixed-size matrix
    problem for molecule graphs. Popular recurrent units, GRUs and LSTMs, have been
    widely used in molecule generation. Furthermore, recurrent units play a significant
    role in message aggregation and conformation preserving.
  prefs: []
  type: TYPE_NORMAL
- en: The gated recurrent units are responsible for updating hidden representations
    of nodes in message passing. While the edge network aggregates messages from 1-hop
    neighbors of the nodes, the GRU updates the node states with the most recent node
    state resulting from aggregated message based on the previous states [[115](#bib.bib115)],
    therefore, allowing information to travel through connected bonds. The architecture
    also allows preserving information from long ago. Li et al. [[123](#bib.bib123)]
    propose a conformation-aware architecture with GRUs, namely HamNet. With modifications
    in message calculations, the HamNet incorporates relative positions and momentum
    in atom representations generated from GRU.
  prefs: []
  type: TYPE_NORMAL
- en: The message passing scheme of the GCNs follows that adjacent atoms are in an
    identical chemical environment, i.e., neighborhood should have similar representation.
    Although the deep neural network can capture the local and global structures,
    it may fail to distinguish atoms performing different roles yet having the same
    neighborhoods. Therefore, the topological order usually needs to be introduced
    to the design by leveraging positional encoding techniques [[118](#bib.bib118)],
    or ordered SMILES representation. With an LSTM conducted over the GCN outputs,
    the unique positions for atoms can be determined by controlling the order of atoms
    in the LSTM that coincides with the canonical SMILES [[123](#bib.bib123), [124](#bib.bib124)].
  prefs: []
  type: TYPE_NORMAL
- en: Transformer. Multiple prior works have applied transformer-style architecture
    in molecular applications. The expressive power of transformer trained on sequence
    representations, SMILES, and fingerprints has significantly been explored [[45](#bib.bib45),
    [93](#bib.bib93), [95](#bib.bib95)], and the architecture is also adapted to molecular
    graph structures to enhance the representational power. Maziarka et al. [[125](#bib.bib125)]
    introduce a novel pre-trained molecule attention transformer that learns domain-knowledge
    from the pre-text task and then utilizes the knowledge to augment self-attention.
    The key innovation of the proposed model is the replacement of the molecular multi-head
    self-attention layers that integrate the molecule graph and inter-atomic distances
    with the self-attention. Both sources of information extracted from graph structures
    further improve the performance. Rong et al. [[109](#bib.bib109)] follow a similar
    pre-train idea and pay more attention to bi-level information extraction from
    molecule graphs to better preserve the domain knowledge and avoid a negative transfer.
    Dynamic message passing networks are applied to extract local subgraph information
    from node embedding, while transformer modules are introduced respectively for
    nodes and edges to extract global relations of nodes. Transformer-style architecture
    with a dynamic message passing mechanism has an excellent performance in capturing
    structural information and enhances the expressive power of GNN. Future work may
    explore better pre-training tasks for the transformer-style architecture to help
    with various downstream molecular tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Retrosynthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fa4e57e3591e7d0bd513e46c805816a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Illustrations of (A) Template-based and (B) Template-free Methods
    in Retrosynthesis and Reaction Prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of retrosynthesis is to decompose a complicated target product to find
    a set of reactant molecules with relatively simple structures. It is a fundamental
    task in synthesis planning and drug manufacturing. For example, a common prescription
    drug—aspirin, can be chopped up into two synthons, easily obtained from a nucleophilic
    addition-elimination reaction. However, the enormous potential searching space
    for all possible combinations results in the high cost of computational complexity.
    With the significant development of artificial intelligence, scientists have devoted
    themselves to finding efficient and effective computer-aid methods in retrosynthesis
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: This section analyzes works that apply machine learning methods to solve the
    retrosynthesis prediction. We will introduce some early methods based on reaction
    templates and then analyze the template-free models for each task.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Template-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Template-based methods rely on encoding reaction templates or rules, which
    are generated either manually or automatically from the currently existing databases.
    Those templates are used to derive the retrosynthetic precursors. As there are
    different possible reaction templates for one target molecule, people developed
    various algorithms which consider chemical context, possible trade-offs, and reactivity
    to decide the most appropriate rule (Fig. [5](#S5.F5 "Figure 5 ‣ 5 Retrosynthesis
    ‣ Deep Learning Methods for Small Molecule Drug Discovery: A Survey")A).'
  prefs: []
  type: TYPE_NORMAL
- en: String-based. String-based methods convert molecules to fingerprints or SMILES
    and then put them into the network. Segler and Waller [[126](#bib.bib126)] propose
    a neural symbolic model which uses a deep neural network to predict the possible
    reaction template. As the encoding reaction templates are finite, they view retrosynthesis
    analysis as a multi-class classification task. They represent the input target
    molecule as Morgan fingerprint, and the output transformation will be applied
    to the target molecule to get the retrosynthetic precursors. Because similar molecules
    may have similar reactions, Coley et al. [[127](#bib.bib127)] introduce a similarity-based
    approach. This method uses similarity as the metric to determine the precedent
    reactions, which are used to find the appropriate reaction site for the target
    molecule. It then applies the corresponding templates to yield candidate precursors,
    which convert to reaction molecules by further applying similarity calculation.
    Baylon et al. [[128](#bib.bib128)] present a multi-scale approach with a deep
    highway network, which groups reaction rules based on chemical similarities without
    supervision. The model determines the reaction rule in two stages. It first decides
    which group the target molecule belongs to and then predicts the specified transformation
    rules. Segler et al. [[129](#bib.bib129)] view retrosynthesis as a Markov decision
    process. They combine Monte Carlo tree search with different neural networks,
    which guide the search and select the proper retrosynthesis direction. Seidl et
    al. [[130](#bib.bib130)] present a novel approach to single-step retrosynthesis
    modeling using Modern Hopfield Networks. Their template-based model utilizes encoded
    representations of both molecules and reaction templates to accurately predict
    the relevance of templates for a given molecule.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based. Graphical models utilize the expressiveness and scalability of
    neural networks to extract useful information. Dai et al. [[131](#bib.bib131)]
    apply conditional graph logic network, which views chemical templates as logical
    rules. In logical rules, graphical structures of molecules and subgraphs are considered
    as logical variables. The predicted outcome is then formulated as the joint probability
    of the reactants and the rules. Instead of predicting reactants globally, Chen
    et al. [[132](#bib.bib132)] propose LocalRetro, which focuses on local reaction
    templates. Each atom and bond uses a message passing neural network to learn the
    local reactivity and applies several particular layers to learn the remote chemical
    knowledge. Based on the predicted atoms and bonds, local reaction templates are
    then applied to get the final reactants.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Template-free
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As deep learning has developed significantly in recent years, some template-free
    methods based on deep learning strategies have become increasingly popular. Template-free
    methods transform chemical structures from one to others without any reaction
    rules (Fig. [5](#S5.F5 "Figure 5 ‣ 5 Retrosynthesis ‣ Deep Learning Methods for
    Small Molecule Drug Discovery: A Survey")B).'
  prefs: []
  type: TYPE_NORMAL
- en: String-based. As machine translation and retrosynthesis transform the source
    to target languages or molecules, researchers propose various models based on
    such an analogy. Those models utilize SMILES notation to represent molecules as
    strings. Liu et al. [[133](#bib.bib133)] build a LSTM-based sequence-to-sequence
    model, where the inputs are the target molecule and the specific reaction type,
    and the output is the most probable corresponding reactants. The advent of the
    transformer offers exciting opportunities for studying its various applications.
    Karpov et al. [[134](#bib.bib134)] utilize the transformer model and applies weights
    averaging and snapshot learning to complete the task. In order to make the prediction
    more general and diverse, Chen et al. [[135](#bib.bib135)] propose several pre-training
    approaches which generate new examples by randomly disconnecting the bonds or
    following the templates. They also use a mixture model with many latent variables
    to increase the diversity of the possible outcome reactants. Ishiguro et al. [[136](#bib.bib136)]
    further improve seq2seq models by applying the idea of data transfer to retrosynthesis.
    They consider different methods, including joint training, self-training, pre-training,
    and fine-tuning, and evaluate their performance using augmented datasets. Zheng
    et al. [[137](#bib.bib137)] develop a self-corrected retrosynthesis predictor
    which applies the reaction predictor to get raw candidate reactants. It uses another
    transformer that mimics the grammar corrector to fix molecular syntax errors.
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et al. [[138](#bib.bib138)] utilize reaction-aware substructures–which
    will not change during the chemical interactions to predict retrosynthesis transformation.
    Similar to previous works, they utilize a dual transformer encoder structure to
    get a product to candidate reactants mapping. They further isolate the unchanged
    substructure from those candidate reactants based on their fingerprints and use
    a sequence-to-sequence model to predict the output.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based. Besides the popularity of sequence models, various graphical models
    are proposed with their unique advantages in the expressiveness of subgraph structures
    and the rapid development of graph neural networks. Sacha et al. [[139](#bib.bib139)]
    present Molecule Edit Graph Attention Network, which views the retrosynthesis
    as a sequence of the graph editing process. The encoder-decoder structure model
    outputs a set of graph actions, including editing and adding atoms, bonds, or
    rings. Those actions are then applied to the target molecule to generate the desired
    reactants. Instead of performing retrosynthesis as a single-step task, G2Gs, proposed
    by Shi et al. [[140](#bib.bib140)], dissembles it into two parts. Their model
    first finds the reaction center by ranking the reactivity scores of atom pairs
    and then chops up the input target molecular graph into several sub-graphs. By
    applying a series of variation graph translations with a latent space for increasing
    diversity, it generates reactant graphs from those synthons. GraphRetro, built
    by Somnath et al. [[141](#bib.bib141)], also utilizes the two-step framework.
    Instead of scoring the atom pairs, GraphRetro ranks both bonds and atoms by a
    message passing network to predict the possible synthons. Unlike G2Gs that directly
    translates graphs to graphs, this model views the process of producing reactants
    from synthons as a classification problem and selects leaving groups from precomputed
    semi-templates to complete synthons. Another model, Semi-Retro [[142](#bib.bib142)],
    further adds local structures of synthons into semi-templates and proposes a directed
    relational graph attention layer for identifying the reaction center.
  prefs: []
  type: TYPE_NORMAL
- en: Combination of Graph and String. In order to incorporate the knowledge from
    both graph representation and sequence representation, some researchers propose
    methods that balance both properties. Like G2Gs, Yan et al. [[143](#bib.bib143)]
    propose RetroXpert, which splits retrosynthesis into two stages. RetroXpert uses
    Edge-enhanced Graph Attention Network to find the reaction centers among bonds,
    and it has an auxiliary task to predict the number of disconnects. In the second
    stage, it converts synthon graph into SMILE notation to apply the sequence model
    to generate the final reactants. Sun et al. [[144](#bib.bib144)] propose an energy-based
    framework for both graphical and sequence models by designing different energy
    functions. Based on the duality of forwarding reaction and retrosynthesis, they
    also develop a dual EBM variant model. By researching the intrinsic connection
    between graph neural network and transformer, Graph Truncated Attention, proposed
    by Seo et al. [[145](#bib.bib145)], adds molecular graph information to the attention
    layers of the transformer, which combines two representations smoothly without
    introducing additional parameters. Alternatively, Retroformer, presented by Wan
    et al. [[146](#bib.bib146)], uses a local attention head, which integrates graphical
    and sequence information, to learn local reactivity and global context simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Reaction Prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the inverse processes of reaction generate massive convenience for the
    chemical industry, the forward synthesis of the organic reaction also raises tremendous
    interest. As another fundamental task in synthesis planning, reaction prediction
    is a task to predict the possible compounds for a given set of reactants, reagents,
    and solvents. Reaction prediction not only assists the evaluation for retrosynthesis
    but also helps with expensive and time-consuming experiments, especially those
    potentially harmful to laboratory technicians.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Template-based
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like retrosynthesis analysis, template-based models for reaction prediction
    are built on reaction templates. As the search spaces for the given reactants
    to generate compounds are pretty big, with substantial computational costs, reaction
    templates can solve this problem by restricting the space. Prediction models use
    different ways to match the appropriate templates and various procedures to decide
    the outcome molecules.
  prefs: []
  type: TYPE_NORMAL
- en: String-based. Some template-based models represent molecules as strings and
    then use string-based models to predict results. Using fingerprints representation
    of reactants and reagents as input, Wei et al. [[147](#bib.bib147)] present a
    model that first explores the application of machine learning in this task. The
    model predicts the possible reaction type as output, but only 16 kinds of reactions
    are considered in their work. By obtaining larger sets of reaction templates,
    more works are proposed. As described in the previous section, the model presented
    by Segler and Waller [[126](#bib.bib126)] can apply to both retrosynthesis analysis
    and reaction prediction. Specifically, their model utilizes neural network to
    process the Morgan fingerprint and uses probability as the criterion to decide
    the most probable reaction rules to apply to the input molecules. Coley et al.
    [[148](#bib.bib148)] also present a synthesis framework that applies all possible
    reaction templates to reactants, in order to generate all possible candidate reactions
    and use softmax layer to score and rank all the candidates.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based. Unlike Segler and Waller [[126](#bib.bib126)] who use fingerprints
    to constitute the reaction template, Coley et al. [[148](#bib.bib148)] focus on
    the reaction cores as the representation. By formalizing reaction prediction as
    finding missing links between reactant molecules, reaction prediction can also
    be interpreted as predicting the missing nodes and edges for a given chemical
    knowledge graph. Based on such understanding, Segler and Waller [[149](#bib.bib149)]
    build a knowledge graph using millions of existing reactions and molecules from
    the database. Instead of simply constructing molecules as nodes and reactions
    as edges, they construct both molecules and reactions as nodes and assign different
    roles to the edges. The corresponding graph matching algorithms can perform product
    prediction and conditional prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Template-free
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the other hand, the models without using reaction templates also become prevalent
    in recent years. When researchers perform the task of reaction prediction, they
    often first find the reactivity site and then make the comparison to get the final
    products. Some template-free models mimic this process.
  prefs: []
  type: TYPE_NORMAL
- en: Graph-based Jin et al. [[150](#bib.bib150)] propose a model based on Weisfeiler-Lehman
    Network. It takes a two-step strategy similar to the experts with two separate
    models. The first model is used to find the reaction center by predicting the
    reactivity scores of pairwise atoms. Based on chemical constraints, the bonds
    will be generated for those pairs of atoms, possibly the reaction site. Finally,
    the second model will be applied to rank those candidate products. Later, they
    improve their work [[151](#bib.bib151)] by replacing Weisfeiler-Lehman Network
    with a graph convolution network, then combing reaction center prediction and
    scoring candidate production as one single task. Qian et al. [[152](#bib.bib152)]
    also propose a two-stage model with a graph convolution network but consider things
    like hydrogen counts and formal charges when identifying the reaction site. Specifically,
    they utilize a message passing neural network to get the molecular embedding in
    each atom and use a convolution-based co-estimation network to predict the reaction
    site. Finally, they use integer linear programming to search the production space.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the above models design in a “human manner” with multi-step, there are
    also end-to-end models. A graph convolution network architecture proposed by Sacha
    et al. [[139](#bib.bib139)] for both reaction prediction and retrosynthesis generates
    the reaction by a sequence of graph edit, which is already discussed in the previous
    section. Focusing on predicting reaction mechanisms instead of the reaction products
    directly, Bradshaw et al. [[153](#bib.bib153)] propose an end-to-end generative
    model called ELECTRO. The reaction with the linear electron flow mechanism is
    then represented as the sequence of electron steps. Combining reinforcement learning
    with graphical models, Do et al. [[154](#bib.bib154)] propose a graph transformation
    policy network, which devotes to tackling the problem using less chemical knowledge.
    There are three parts in this model: a graph neural network used to represent
    the input molecules, including reactants and reagents, a node pair prediction
    network used to output the reaction triples, and a policy network used to generate
    the intermediate molecules. Bi et al.[[155](#bib.bib155)] proposed Non-autoregressive
    Electron Redistribution Framework, which views the change of edge as flowing electrons
    in molecules. Like others, they use graph neural networks to represent the molecule,
    but instead of predicting the generation or breaking of the molecular bonds, they
    predict the flow of electrons.'
  prefs: []
  type: TYPE_NORMAL
- en: String-based. Besides the various graphical models, those entirely data-driven
    sequence-based models are also well-proposed. Schwaller et al. [[156](#bib.bib156)]
    present a model which views reaction prediction as a translation problem and thus
    applies neural machine translation models. The model consists of two recurrent
    neural networks. The first is used to encode the sequence representation of the
    molecule, and the second is a decoder used to produce the outcome products and
    the corresponding probability. Later, after the advent of the transformer, they
    further developed a multi-head attention molecular transformer model [[2](#bib.bib2)],
    which takes advantage of the attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Template-based methods have their own unique advantages. The process of finding
    possible existing reaction rules is similar to the way chemists complete this
    task. These methods inherently enhance the interpretability of models. The most
    important parts of template-based models are the generation of templates because
    the quality and numbers of reaction templates directly influence the model performance.
    The template generation leads to some problems. Firstly, because those models
    cannot apply the reaction rules or transformations not covered in the template
    library in the test stage, the generalization becomes a huge problem. Secondly,
    some hand-encoding rules and transforms have bias because they are generated from
    a small part of chemists [[147](#bib.bib147)] and thus not that accurate. Besides,
    some early models which use fingerprints as inputs [[126](#bib.bib126), [127](#bib.bib127),
    [128](#bib.bib128), [129](#bib.bib129)] have limitation from the data side. Fingerprints
    do not include much information, especially on the substructures, connectivity
    [[132](#bib.bib132)], and local contexts. In the later stage, the fully data-driven
    template generating algorithms tackle the second problem, but the computational
    source limitation problem still exists. It leads to the restriction on the amounts
    of templates, which further negatively influences the model’s coverage, scalability,
    and diversity. As Schwaller et al. [[2](#bib.bib2)] point out in their article,
    those auto-generating algorithms have flaws logically. Those template extracting
    algorithms are based on atom mapping, which comes from a template library. This
    creates a logical ring and is thus not that credible and accurate.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, template-free models overcome the drawbacks mentioned above.
    Graphical models use graph representation, and it has excellent advantages in
    interpretability. As the atoms and bonds can be directly converted to nodes and
    edges, those graphs and subgraphs are explainable chemically. By using graph neural
    network, the local information can be aggregated to each node and edge. As graph
    neural network learns task-specific embeddings, for both retrosynthesis and reaction
    prediction, many graph-based models take two-stage strategies. By modifying the
    layers and focusing on latent spaces, some models like Chen et al. [[136](#bib.bib136)]
    also enhance the diversity of the models. Although graphical models have promising
    outcomes, wonderful interpretability, and scalability, they require atom-mapped
    datasets in the training stage [[147](#bib.bib147)]. Similar to the problems for
    template-based models, they implicitly use pre-defined reaction templates and
    thus maybe not be as credible as ground truth. Sequence-based models utilize the
    advantages of transformer and perform well on those tasks; however, it lacks interpretability
    because the string can hardly expose the local information for the three-dimensional
    structured molecules. The integration of sequence-based models containing graphical
    structures as input information [[141](#bib.bib141), [142](#bib.bib142), [143](#bib.bib143),
    [144](#bib.bib144)] take advantage of both sides, but it still can not bypass
    the drawbacks of atom-mapped datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Datasets and Benchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PubChem. PubChem [[157](#bib.bib157)] is an open chemistry database at the National
    Institutes of Health (NIH). Most samples in the database are small molecules,
    but large molecules such as nucleotides, carbohydrates, lipids, peptides, and
    chemically-modified macromolecules also exist. The information and annotations
    of molecules include chemical structures, identifiers, chemical and physical properties,
    biological activities, patents, health, safety, toxicity data, and many others.
    PubChem contains 93.9 million entries for compounds (54 million in September 2014),
    including pure and characterized compounds; 236 million entries for molecules
    (163 million in September 2014), and also mixtures, extracts, complexes, and uncharacterized
    substances.
  prefs: []
  type: TYPE_NORMAL
- en: ChEMBL. ChEMBL [[158](#bib.bib158)] is a manually curated database of bioactive
    molecules with drug-like properties. It brings together chemical, bioactivity
    and genomic data to aid the translation of genomic information into effective
    new drugs. It contains 14,885 targets and 2,157,379 distinct compounds by February
    2022\. The annotation of ChEMBL mainly includes the positive and negative effects
    of the compounds on the corresponding targets.
  prefs: []
  type: TYPE_NORMAL
- en: ZINC. ZINC [[159](#bib.bib159)] is a free database of commercially-available
    compounds for virtual screening. ZINC contains over 230 million purchasable compounds
    in ready-to-dock, 3D formats and over 750 million purchasable compounds.
  prefs: []
  type: TYPE_NORMAL
- en: GDB. GDB [[160](#bib.bib160)] is the largest publicly available small organic
    molecule database. GDB dataset is a collection of SMILES strings, GDB-11 [[161](#bib.bib161)]
    has 100 million molecules, GDB-13 [[162](#bib.bib162)] has 1 billion molecules,
    and GDB-17 [[163](#bib.bib163)] has 160 billion molecules. The number in the postscript
    indicates the maximum number of atoms in the molecule in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'QM9. QM9 [[104](#bib.bib104)] is a benchmark dataset that contains 134K stable
    organic molecules. QM9 records geometric, energetic, electronic, and thermodynamic
    properties. GEOM-QM9 is a subset of QM9, including 3D geometry structures with
    small molecular mass and few rotatable bonds. All molecules are modeled based
    on DFT methods, and same for the calculated quantum mechanical properties. Descriptions
    of recorded properties are described in TABLE [1](#S7.T1 "Table 1 ‣ 7.1 Datasets
    ‣ 7 Datasets and Benchmarks ‣ Deep Learning Methods for Small Molecule Drug Discovery:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Description of properties in QM9 dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '| Properties | Description |'
  prefs: []
  type: TYPE_TB
- en: '| $U_{0}$ | Internal energy at 0K |'
  prefs: []
  type: TYPE_TB
- en: '| $U$ | Internal energy at 298.15K |'
  prefs: []
  type: TYPE_TB
- en: '| $G$ | Free energy at 298.15K |'
  prefs: []
  type: TYPE_TB
- en: '| $H$ | Enthalpy at 298.15K |'
  prefs: []
  type: TYPE_TB
- en: '| $C_{v}$ | Heat capacity at 298.15K |'
  prefs: []
  type: TYPE_TB
- en: '| $\epsilon_{HOMO}(HOMO)$ | Energy of HOMO |'
  prefs: []
  type: TYPE_TB
- en: '| $\epsilon_{LUMO}(LUMO)$ | Energy of LUMO |'
  prefs: []
  type: TYPE_TB
- en: '| $\epsilon_{gap}(gap)$ | Gap ($\epsilon_{LUMO}-\epsilon_{HOMO}$) |'
  prefs: []
  type: TYPE_TB
- en: '| ZPVE | Zero point vibrational energy |'
  prefs: []
  type: TYPE_TB
- en: '| $<R^{2}>$ | Electronic spatial extent |'
  prefs: []
  type: TYPE_TB
- en: '| $\mu$ | Dipole moment |'
  prefs: []
  type: TYPE_TB
- en: '| $\alpha$ | Isotropic polarizability |'
  prefs: []
  type: TYPE_TB
- en: GEOM-Drugs. GEOM-Drugs [[164](#bib.bib164)] is a dataset that provides 3D molecular
    geometries for larger drug molecules, up to a maximum of 181 atoms (91 heavy atoms).
    It also contains multiple conformations for each molecule, with a larger structure
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: USPTO. USPTO [[165](#bib.bib165)] is an open-source chemical reaction database
    and contains about 3.7 million reactions. The most popular datasets for reaction
    and retrosynthesis prediction are from USPTO. The frequently used dataset includes
    USPTO-50K (contains 50k reactions), USPTO-MIT, USPTO-15k (contains 50k reactions),
    and USPTO-FULL (contains 1M reactions). Among all of these datasets, the most
    commonly used one is USPTO-50K. It randomly picks 50000 reactions from US patents,
    and all reactions are assigned to a specific reaction type. For single-step retrosynthesis,
    USPTO-50K often serves as a benchmark. However, the distribution of the reaction
    classes is not that uniform, and some patented syntheses are not validated by
    experiments and thus may have flaws.
  prefs: []
  type: TYPE_NORMAL
- en: Reaxys. Reaxys [[165](#bib.bib165)] is a database used by scholars who focus
    on reaction prediction. It has the world’s largest physical and chemical properties,
    factual reaction database and pharmaceutical chemistry database. This database
    has 253M substances, 58M reactions, 96M documents, 32M patents, and 42M bioactivities.
    Despite the rich contents, large scale, and long history of Reaxys, in order to
    extract useful information from the database and then apply it to the models,
    researchers need additional efforts in pre-processing (filter) the data.
  prefs: []
  type: TYPE_NORMAL
- en: MoleculeNet. MoleculeNet [[166](#bib.bib166)] is a data collection specifically
    designed for evaluating molecular prediction models. The dataset collection includes
    over 700,000 compounds of various properties, mainly in four categories, quantum
    mechanics, physical chemistry, biophysics, and physiology. The first two collections
    are used to test as regression tasks, while the latter two are for classification
    ones. Quantum mechanics are information on electronic properties determined by
    DFT-based methods. ESOL, FreeSolv, and Lipophilicity in the physical chemistry
    category respectively include data of solubility, hydration free energy, and distribution
    coefficient in water. Physiology collection, including drug-related and toxicology
    data, with Biophysics collection composed of data of biological properties such
    as binding affinities and measured biological activities, are the most frequently
    used datasets for model evaluation in property prediction in the articles above.
  prefs: []
  type: TYPE_NORMAL
- en: ExcapeDB. ExcapeDB [[167](#bib.bib167)] is a comprehensive dataset that integrates
    active and inactive compounds from PubChem and ChEMBL. It serves as a data hub,
    providing researchers worldwide with convenient access to a standardized chemogenomics
    dataset, with the data and accompanying software available under open licenses.
  prefs: []
  type: TYPE_NORMAL
- en: LSC. LSC [[85](#bib.bib85)] is a large benchmark dataset assembled from the
    ChEMBL database, enabling accurate evaluation of machine learning methods for
    compound target prediction. With over 500,000 compounds and more than 1,000 assays,
    the dataset represents a diverse array of target classes, including enzymes, ion
    channels, and receptors.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are multiple evaluation metrics for molecule generation. Novelty illustrates
    the percentage of generated molecules dissimilar to molecules in the training
    set [[168](#bib.bib168)]. Diversity is calculated based on pairwise Tanimoto similarity,
    which measures the diversity of generated molecules. Property score is the average
    score of the top 100 molecules. The success rate is the percentage of generated
    molecules that satisfy all objective functions [[49](#bib.bib49)]. For the 3D
    molecule generation problem, generated and reference molecule sets are used for
    evaluation. Coverage (COV) score [[82](#bib.bib82)] measures the percentage of
    molecule conformations in one set covered by another. Matching (MAT) score measures
    the distance of the closest neighbor between two sets. “-P” and “-R” means precision
    metric and recall metric, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Mean-absolute error (MAE), root-mean-square error (RMSE), and ROC-AUC scores
    are the most commonly used evaluation metrics for molecular property prediction.
    Performance is measured on either classification or regression tasks, in which
    QM datasets and datasets from MoleculeNet [[166](#bib.bib166)] are commonly used
    for evaluation. If property prediction is regarded as a classification task, which
    is often the case, models are evaluated on a set of given labels after training.
    Regression tasks for molecular property prediction are more demanding than the
    classification ones, requiring a competitive model to predict the exact numeric
    values of molecular properties [[111](#bib.bib111)]. Supervised models are commonly
    introduced in the evaluation process as baselines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Datasets and Evaluation Metrics for Selected Models'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task and Dataset | Model | Year | Evaluation Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Molecule Generation |  |  | Diversity ↑ | Property Score ↑ | Success Rate
    ↑ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ZINC | GCPN [[62](#bib.bib62)] | 2018 | 0.596 | 0.450 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | MolDQN [[69](#bib.bib69)] | 2019 | 0.597 | 0.365 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | LSTM [[50](#bib.bib50)] | 2019 | 0.706 | 0.672 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | BOSS [[58](#bib.bib58)] | 2020 | 0.561 | 0.504 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ChemBO [[71](#bib.bib71)] | 2020 | 0.701 | 0.648 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | DST [[68](#bib.bib68)] | 2022 | 0.755 | 0.752 | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ChEMBL | REINVENT [[49](#bib.bib49)] | 2017 | 0.666 | - | 46.6% |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | JT-VAE [[43](#bib.bib43)] | 2018 | 0.277 | - | 5.4% |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | GA+D [[57](#bib.bib57)] | 2019 | 0.363 | - | 85.7% |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | RationaleRL [[64](#bib.bib64)] | 2020 | 0.706 | - | 75.0% |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | MARS [[66](#bib.bib66)] | 2021 | 0.719 | - | 92.3% |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | MolEvol [[65](#bib.bib65)] | 2021 | 0.681 | - | 93.0% |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 3D Molecule Generation |  |  | COV-R (%) ↑ | MAT-R (Å) ↓ | COV-P (%) ↑ |
    MAT-P (%) ↓ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Mean | Median | Mean | Median | Mean | Median | Mean | Median |'
  prefs: []
  type: TYPE_TB
- en: '| GEOM-Drugs | CVGAE [[80](#bib.bib80)] | 2019 | 0.00 | 0.00 | 3.0702 | 2.9937
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | GRAPHDG [[74](#bib.bib74)] | 2020 | 8.27 | 0.00 | 1.9722 | 1.9845 | 2.08
    | 0.00 | 2.4340 | 2.4100 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CGCF [[82](#bib.bib82)] | 2021 | 53.96 | 57.06 | 1.2487 | 1.2247 | 21.68
    | 13.72 | 1.8571 | 1.8066 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CONFVAE [[75](#bib.bib75)] | 2021 | 55.20 | 59.43 | 1.2380 | 1.1417 |
    22.96 | 14.05 | 1.8287 | 1.8159 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CONFGF [[76](#bib.bib76)] | 2021 | 62.15 | 70.93 | 1.1629 | 1.1596 | 23.42
    | 15.52 | 1.7219 | 1.6863 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GEOMOL [[77](#bib.bib77)] | 2022 | 67.16 | 71.71 | 1.0875 | 1.0586 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | GEODIFF [[78](#bib.bib78)] | 2022 | 89.13 | 97.88 | 0.8629 | 0.8529 |
    61.47 | 64.55 | 1.1712 | 1.1232 |'
  prefs: []
  type: TYPE_TB
- en: '| GEOM-QM9 | CVGAE [[80](#bib.bib80)] | 2019 | 0.09 | 0.00 | 1.6713 | 1.6088
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | GRAPHDG [[74](#bib.bib74)] | 2020 | 3.33 | 84.21 | 0.4245 | 0.3973 | 43.90
    | 35.33 | 0.5809 | 0.5823 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CGCF [[82](#bib.bib82)] | 2021 | 78.05 | 82.48 | 0.4219 | 0.3900 | 36.49
    | 33.57 | 0.6615 | 0.6427 |'
  prefs: []
  type: TYPE_TB
- en: '|  | CONFVAE [[75](#bib.bib75)] | 2021 | 77.84 | 88.20 | 0.4154 | 0.3739 |
    38.02 | 34.67 | 0.6215 | 0.6091 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GEOMOL [[77](#bib.bib77)] | 2022 | 71.26 | 72.00 | 0.3731 | 0.3731 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | CONFGF [[76](#bib.bib76)] | 2021 | 88.49 | 94.31 | 0.2673 | 0.2685 | 46.43
    | 43.41 | 0.5224 | 0.5124 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GEODIFF [[78](#bib.bib78)] | 2022 | 90.07 | 93.39 | 0.2090 | 0.1988 |
    52.79 | 50.29 | 0.4448 | 0.4267 |'
  prefs: []
  type: TYPE_TB
- en: '| Retrosynthesis Prediction |  |  | Accuracies (%) ↑ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | Top-1 | Top-3 | Top-5 | Top-10 |'
  prefs: []
  type: TYPE_TB
- en: '| USPTO-50k | SCROP [[137](#bib.bib137)] | 2019 | 59.0 | 74.8 | 78.1 | 81.1
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GLN [[131](#bib.bib131)] | 2019 | 63.2 | 77.5 | 83.4 | 89.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | G2Gs [[140](#bib.bib140)] | 2020 | 61.0 | 81.3 | 86.0 | 88.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RetroXpert [[143](#bib.bib143)] | 2020 | 70.4 | 83.4 | 85.3 | 86.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dual [[144](#bib.bib144)] | 2020 | 65.7 | 81.9 | 84.7 | 85.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MEGAN [[139](#bib.bib139)] | 2021 | 60.7 | 82.0 | 87.5 | 91.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GraphRetro [[141](#bib.bib141)] | 2021 | 63.9 | 81.5 | 85.2 | 88.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | LocalRetro [[132](#bib.bib132)] | 2021 | 63.9 | 86.8 | 92.4 | 96.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Retroformer [[146](#bib.bib146)] | 2022 | 64.0 | 82.5 | 86.7 | 90.2 |'
  prefs: []
  type: TYPE_TB
- en: '| USPTO-full | RetroSim [[127](#bib.bib127)] | 2017 | 32.8 | - | - | 56.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MEGAN [[139](#bib.bib139)] | 2021 | 33.6 | - | - | 63.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GLN [[131](#bib.bib131)] | 2019 | 39.3 | - | - | 63.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GTA [[145](#bib.bib145)] | 2021 | 46.6 | - | - | 70.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reaction-Aware [[138](#bib.bib138)] | 2022 | 51.6 | - | - | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Reaction Prediction |  |  | Coverages (%) ↑ |  | Accuracies (%) ↑ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | C@6 | C@8 | C@10 |  | Top-1 | Top-2 | Top-3 | Top-5 |'
  prefs: []
  type: TYPE_TB
- en: '| USPTO-15k | WLDN [[150](#bib.bib150)] | 2017 | 81.6 | 86.1 | 89.1 |  | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | GTPN [[154](#bib.bib154)] | 2019 | 88.9 | 92.0 | 93.6 |  | - | - | - |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| USPTO-MIT | WLDN [[150](#bib.bib150)] | 2017 | - | - | - |  | 79.6 | - |
    87.7 | 89.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GTPN [[154](#bib.bib154)] | 2019 | - | - | - |  | 83.2 | - | 86.0 | 86.5
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Molecular Transformer[[2](#bib.bib2)] | 2019 | - | - | - |  | 90.4 | 93.7
    | 94.6 | 95.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MEGAN [[139](#bib.bib139)] | 2021 | - | - | - |  | 89.3 | 92.7 | 94.4
    | 95.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | NERF [[155](#bib.bib155)] | 2021 | - | - | - |  | 90.7 | 92.3 | 93.3 |
    93.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Datasets and Evaluation Metrics for Selected Models'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task and Dataset | Model | Year | Evaluation Metrics |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Property Prediction |  |  | MAE ↓ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $U_{0}$(eV) | $U$(eV) | $G$(eV) | $H$(eV) | $C_{v}$($\frac{cal}{molK}$)
    | $HOMO$(eV) |'
  prefs: []
  type: TYPE_TB
- en: '| QM9 | InfoGraph [[122](#bib.bib122)] | 2019 | 0.1410 | 0.1702 | 0.1592 |
    0.1552 | 0.1965 | 0.1605 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MGCN [[105](#bib.bib105)] | 2019 | 0.0129 | 0.0144 | 0.0146 | 0.0162 |
    0.0380 | 0.0421 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASGN [[117](#bib.bib117)] | 2020 | 0.0562 | 0.0594 | 0.0560 | 0.0583 |
    0.0984 | 0.1190 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SphereNet [[116](#bib.bib116)] | 2021 | 0.0062 | 0.0064 | 0.0078 | 0.0063
    | 0.0215 | 0.0228 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | MAE ↓ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $LUMO$(eV) | $gap$(eV) | $ZPVE$(eV) | $R^{2}$ ($a_{0}^{2}$) | $\mu$(D)
    | $\alpha$ ($a_{0}^{3}$) |'
  prefs: []
  type: TYPE_TB
- en: '| QM9 | InfoGraph [[122](#bib.bib122)] | 2019 | 0.1659 | 0.2421 | 0.00036 |
    4.92 | 0.3168 | 0.5444 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MGCN [[105](#bib.bib105)] | 2019 | 0.0574 | 0.0642 | 0.00112 | 0.11 |
    0.0560 | 0.0300 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ASGN [[117](#bib.bib117)] | 2020 | 0.1061 | 0.2012 | 0.00017 | 1.38 |
    0.1947 | 0.2818 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SphereNet [[116](#bib.bib116)] | 2021 | 0.0189 | 0.0311 | 0.00110 | 0.27
    | 0.0245 | 0.0449 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | MAE ↓ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | FreeSolv | ESOL | Lipo | QM7 | QM8 |  |'
  prefs: []
  type: TYPE_TB
- en: '| MoleculeNet | MPNN [[115](#bib.bib115)] | 2017 | 2.185 | 1.167 | 0.672 |
    113.0 | 0.0150 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | MGCN [[105](#bib.bib105)] | 2019 | 3.349 | 1.266 | 1.113 | 77.6 | 0.0220
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | GROVER [[109](#bib.bib109)] | 2020 | 1.544 | 0.831 | 0.560 | 72.6 | 0.0125
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | multiple SMILES[[87](#bib.bib87)] | 2022 | 0.786 | 0.445 | 0.548 | - |
    - |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ChemNet [[124](#bib.bib124)] | 2021 | - | - | - | 60.1 | 0.0100 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | PhysChem [[124](#bib.bib124)] | 2021 | - | - | - | 59.6 | 0.0101 |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | RMSE ↓ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | FreeSolv | ESOL | Lipo |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MoleculeNet | N-Gram [[113](#bib.bib113)] | 2019 | 2.51 | 1.10 | 0.88 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | GROVER [[109](#bib.bib109)] | 2020 | 2.48 | 0.99 | 0.66 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | MolCLR [[111](#bib.bib111)] | 2022 | 2.20 | 1.11 | 0.65 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | ROC-AUC ↑ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | BBBP | SIDER | ClinTox | BACE | Tox21 | ToxCast |'
  prefs: []
  type: TYPE_TB
- en: '| MoleculeNet | AttrMasking [[107](#bib.bib107)] | 2019 | 0.702 | 0.604 | 0.686
    | 0.772 | 0.742 | 0.625 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ContextPred [[107](#bib.bib107)] | 2019 | 0.712 | 0.593 | 0.737 | 0.786
    | 0.733 | 0.628 |'
  prefs: []
  type: TYPE_TB
- en: '|  | InfoGraph [[122](#bib.bib122)] | 2019 | 0.692 | 0.592 | 0.751 | 0.739
    | 0.730 | 0.620 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GraphCL [[110](#bib.bib110)] | 2020 | 0.675 | 0.601 | 0.789 | 0.687 |
    0.750 | 0.628 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GROVER [[109](#bib.bib109)] | 2020 | 0.718 | 0.637 | 0.843 | 0.822 | 0.765
    | 0.635 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ChemBERTa [[46](#bib.bib46)] | 2021 | 0.643 | - | 0.733 | - | 0.728 |
    - |'
  prefs: []
  type: TYPE_TB
- en: '|  | GraphMVP [[106](#bib.bib106)] | 2021 | 0.724 | 0.639 | 0.791 | 0.812 |
    0.759 | 0.631 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | AUC ↑ |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | ClinTox | Tox21 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| MoleculeNet | Chemception [[90](#bib.bib90)] | 2017 | 0.745 | 0.766 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | SMILES2vec [[86](#bib.bib86)] | 2017 | 0.693 | 0.810 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | CheMixNet [[89](#bib.bib89)] | 2018 | 0.944 | 0.920 |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | TOP [[88](#bib.bib88)] | 2019 | 0.946 | - |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: The standard evaluation metric for retrosynthesis and reaction prediction is
    the top-k accuracy, a concept from probability. This metric will compute the percentage
    of times the correct answer is in the highest k ranking scores. Referring to the
    application in retrosynthesis prediction and reaction prediction, the match of
    the ground-truth reactants (or products) with the predicted one will be computed.
    Based on such an exact match, they will then apply top-k accuracy. The choice
    of k varies from 1 to 50, depending on different works. Another evaluation metric
    used in reaction prediction is Coverage@k, which is the proportion of reactions
    in all the actual atom pairs founded in the set of the predicted atom pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Benchmark Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'According to evaluation data of 2D molecule generation models in table [2](#S7.T2
    "Table 2 ‣ 7.2 Evaluation Metrics ‣ 7 Datasets and Benchmarks ‣ Deep Learning
    Methods for Small Molecule Drug Discovery: A Survey"), we could conclude that
    graph models like DST [[68](#bib.bib68)], MARS [[66](#bib.bib66)] and MolEvol
    [[65](#bib.bib65)] have better performance than SMILES models like GA+D [[57](#bib.bib57)]
    and LSTM [[169](#bib.bib169)]. This may be because SMILES strings fail to represent
    distances between atoms. For example, atoms close in the SMILES string may be
    far apart in the actual structure. Meanwhile, models using the notion of motif
    like MolEvol [[65](#bib.bib65)], MARS [[66](#bib.bib66)] and RationaleRL [[64](#bib.bib64)]
    have shown better success rate than atom by atom generation models. However, novelty
    of these models is limited since the motifs are extracted from the training set.
    For 3D molecule generation, variational auto-encoder and flow are two commonly
    used techniques in previous works. GEODIFF [[78](#bib.bib78)] combines the diffusion
    model with the molecular generation task and achieves the state-of-the-art result.
    Thus, finding models that better match 3D molecular invariant features is challenging
    for the 3D molecule generation task.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the model performances on the QM9 datasets, we find out that utilizing
    unlabeled data appears to be an effective solution for predicting a large number
    of molecules [[117](#bib.bib117), [105](#bib.bib105)]. ASGN improves the transferability
    by weight transfer, while MGCN transfers the knowledge of small molecules to large
    ones to resolve the structural shortage of the dataset. These robust models achieve
    a more than 50$\%$ reduction on properties, such as, $U_{0}$, $C_{v}$, $\alpha$
    compared with baselines that rely on limited labeled data, showing a great necessity
    to improve generalizability and transferability. Incorporating multilevel information,
    especially the spatial position [[105](#bib.bib105), [116](#bib.bib116)], are
    proved to contribute to better performance. Instead of simply putting 3D information,
    such as angle, into use, spherical message passing (SMP) [[116](#bib.bib116)]
    produces accurate, complete, and physically meaningful data representations on
    3D graphs. The advancement has shown a significant improvement in almost all properties.
  prefs: []
  type: TYPE_NORMAL
- en: As discovered in the molecule generation task, the overall performances on the
    MoleculeNet dataset reveal the overwhelming advantages of graph models over the
    string ones, especially for those involving multilevel structural details [[109](#bib.bib109)].
    The superb performance of TOP [[88](#bib.bib88)] on the ClinTox dataset demonstrates
    the strengths of incorporating multiple kinds of fingerprints compared to SMILES2vec
    [[86](#bib.bib86)]. The specially-designed mixed representation with SMILES strings
    and physiochemical properties demonstrate its superior capability for toxicity
    prediction on the ClinTox dataset. The innovation of combining properties with
    learned structural features provides insights to overcome shortcomings of existing
    explorations.
  prefs: []
  type: TYPE_NORMAL
- en: The overall model performance on the MoleculeNet dataset also indicates that
    self-supervised pre-training tasks result in a performance boost. While supervised
    pre-training tasks introduce the risk of negative transfer [[107](#bib.bib107)],
    well-designed self-supervised pre-trained models show rival performance in learning
    implicit domain knowledge [[109](#bib.bib109), [110](#bib.bib110), [111](#bib.bib111)].
    However, some tasks that involve alteration of the molecule structure, such as
    subgraph removal [[111](#bib.bib111)], result in poor performance in datasets
    sensitive to topology changes, such as the BBBP dataset. Moreover, the composition
    of different augmentation strategies is not bound to perform better since removing
    various substructures might easily lead to the loss of important structural information.
    Furthermore, models involving 3D information have superior performance on datasets
    of quantum mechanical properties. MGCN [[105](#bib.bib105)], though not showing
    distinct advantages on other datasets, shows a competitive result as those pre-training
    techniques on QM7 and QM8 datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For retrosynthesis prediction, RetroXpert [[143](#bib.bib143)] and Reaction-Aware
    [[138](#bib.bib138)] combine the information from both graph representation and
    sequence representation and perform well. Generally speaking, template-free models
    have better performance. However, LocalRetro [[132](#bib.bib132)], a template-based
    model, outperforms others. This may be due to the model design, which not only
    strictly constrains the templates but also considers global structure. For reaction
    prediction, models such as GTPN [[154](#bib.bib154)], Molecular Transformer [[2](#bib.bib2)]
    and NERF [[155](#bib.bib155)] which utilize the state-of-art models in other applications
    of machine learning also get good results in this task.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Challenges and Future trend
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 8.1 Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data. Despite all the machine learning models discussed above, it should be
    emphasized that data is the limiting factor in drug discovery. Data must be of
    sufficient quantity and high quality to make the model useful and practical. Although
    existing databases contain a large number of molecules, the number of data for
    a certain task can be very sparse [[170](#bib.bib170), [114](#bib.bib114), [171](#bib.bib171)].
    For instance, the training data is particularly limited for generating polymers
    with given properties [[172](#bib.bib172)]. Meanwhile, the quality of datasets
    still needs to be improved. The USPTO datasets are the most popular datasets for
    retrosynthesis due to their integrity and accessibility. However, the USPTO dataset
    still suffers from atom-mapping inaccuracy, and noisy stereochemical data [[173](#bib.bib173)].
    Data inconsistency is also a crucial problem in drug design [[174](#bib.bib174)].
    For example, data inconsistencies from different centres lead to substantial inconsistencies
    in drug responses in large pharmacogenomic studies [[175](#bib.bib175)].
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics. The currently used evaluation metrics need to be improved.
    Renz et al. [[176](#bib.bib176)] have shown that it is easy to maximize “validity”
    by inserting a carbon atom into the training SMILES string. Current evaluation
    metrics for retrosynthesis also have misleading meanings [[177](#bib.bib177),
    [178](#bib.bib178), [179](#bib.bib179)]. Therefore, although a large number of
    evaluation metrics have been provided, these metrics are simplified due to computational
    complexity and differ from the real definition to some extent. How to balance
    computational complexity with real definitions is still an unsolved problem, which
    requires continuous efforts to refine the evaluation methods better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretability. Interpretability issue is another challenge in drug discovery.
    Although existing models perform well, the reasons behind them are still unclear.
    For instance, retrosynthesis can predict the initial products. However, if the
    mechanism behind retrosynthesis could also be provided, it would be helpful for
    the research of chemical reactions. More specifically, there are four aspects
    to cover [[180](#bib.bib180)]: 1) Transparency, which is knowing how the system
    reaches a particular answer; 2) Justification, which is elucidating why the answer
    provided by the model is acceptable; 3) Informativeness, which is providing new
    information to human decision-makers; and 4) Uncertainty estimation, which is
    quantifying how reliable a prediction is. An interpretable model has many advantages.
    It can help the expert to analyze the causes when the performance of model is
    poor and provide directions for further improvement. When the interpretation of
    the model is consistent with the prior knowledge, the confidence of the model
    can be improved [[181](#bib.bib181)]. Although there are some explorations to
    the interpretability via case studies [[68](#bib.bib68), [182](#bib.bib182), [183](#bib.bib183)],
    how to quantitatively evaluate interpretability remains a challenge [[184](#bib.bib184)].'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty Quantification. Most work has focused on improving model accuracy,
    while quantifying uncertainty requires more attention [[185](#bib.bib185)]. Model
    accuracy can only represent the reliability of the entire model. However, uncertainty
    quantification predicts the property interval of a given molecule at a certain
    confidence level, estimating reliability at the instance level [[186](#bib.bib186)].
    By combining the prediction results of the model with the quantification of uncertainty,
    it is possible to screen for reliable molecules with good properties, thus improving
    the efficiency of drug design. In addition, the algorithm using uncertainty quantization
    is robust when exploring the domain outside the distribution of training data,
    which is important for the model to explore in the huge and diverse chemical space [[187](#bib.bib187)].
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Future Trend
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For molecule generation tasks, the molecular representation framework has gradually
    shifted from SMILES sequences to graph neural networks. Compared with SMILES sequences,
    graph neural networks can better represent molecular structures, especially cyclic
    ones. However, graph neural networks can only assign atom relationships to nodes
    and edges on the graph. Combining the constraints between atoms with neural networks
    remains to be explored. Meanwhile, this survey focuses on structured small molecule
    generation. Other compounds such as protein, gene and crystal with more complex
    data structures may also benefit from the methods discussed here.
  prefs: []
  type: TYPE_NORMAL
- en: For molecular property prediction tasks, the current trend of leveraging 2D
    topology and 3D geometry will be sustained. It appears that mutual information
    maximization and the MPNN framework will be continued to act as the basic theme.
    While most architectures focus on molecular representations, transferability to
    downstream tasks requires more attention to avoid the “negative transfer” effect
    since molecular property prediction is often viewed as a downstream task. Novel
    pre-training strategies will continue to be a powerful trend since these strategies
    address the limited labeled data and out-of-distribution prediction problems to
    some extent. Novel strategies for combining molecular properties, such as chemical
    reactions and molecular dynamics, will be explored with a specific focus on multi-dimensional
    structures.
  prefs: []
  type: TYPE_NORMAL
- en: For retrosynthesis and reaction prediction tasks, the methods for both tasks
    can be classified into two categories, i.e., template-based and template-free.
    The development space is limited for the multi-step template-based models due
    to the weak generalization ability. Meanwhile, more and more template-free models
    are proposed because of their excellent coverage, scalability, and diversity.
  prefs: []
  type: TYPE_NORMAL
- en: However, most of the proposed methods lack information about conditions, such
    as reagents, catalysts, solvents, temperature. Although recent developed algorithms
    have made a preliminary attempt on this aspect [[177](#bib.bib177), [188](#bib.bib188),
    [189](#bib.bib189)], the conversion of algorithms to experimental procedures should
    receive more attention in the future [[188](#bib.bib188)].
  prefs: []
  type: TYPE_NORMAL
- en: Although models and algorithms are proposed separately for different tasks,
    those tasks have intrinsic relationships. For example, after a molecule is produced
    by generation models, experts need to know some basic properties of this molecule
    (e.g., whether it is toxic, whether it can be absorbed by human body) as well
    as how to synthesize this molecule. This is where the property prediction and
    retrosynthesis algorithms come into play. Thus, making connections between different
    tasks and integrating them might be a trend in the future. Furthermore, most of
    the existing models are based on theory and lack experimental verification. The
    results of the model have a guiding effect on the experiment, and the data obtained
    from the experiment can also be fed back to improve the model. The potential for
    closing the loop for theoretical model and practical experiments is quite profound
    [[190](#bib.bib190)].
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we comprehensively review molecule generation, molecular property
    prediction, retrosynthesis, and reaction prediction tasks under the setting of
    deep learning. We categorize the existing models based on the molecule representation
    they employ. Additionally, we delve into the datasets and evaluation metrics and
    analyze benchmark models in terms of performance. Finally, we highlight the challenges
    and outline the future trend of drug discovery utilizing deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by National Natural Science Foundation of China (62106219)
    and Natural Science Foundation of Zhejiang Province (QY19E050003).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Deng, Z. Yang, I. Ojima, D. Samaras, and F. Wang, “Artificial intelligence
    in drug discovery: applications and techniques,” *Briefings in Bioinformatics*,
    vol. 23, no. 1, p. bbab430, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas, and
    A. A. Lee, “Molecular transformer: a model for uncertainty-calibrated chemical
    reaction prediction,” *ACS central science*, vol. 5, no. 9, pp. 1572–1583, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Kumar, A. Voet, and K. Zhang, “Fragment based drug design: from experimental
    to computational approaches,” *Current medicinal chemistry*, vol. 19, no. 30,
    pp. 5128–5147, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Sheng and W. Zhang, “Fragment informatics and computational fragment-based
    drug design: an overview and update,” *Medicinal Research Reviews*, vol. 33, no. 3,
    pp. 554–598, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] R. P. Sheridan and S. K. Kearsley, “Using a genetic algorithm to suggest
    combinatorial libraries,” *Journal of Chemical Information and Computer Sciences*,
    vol. 35, no. 2, pp. 310–320, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. Schwalbe-Koda and R. Gómez-Bombarelli, “Generative models for automatic
    chemical design,” in *Machine Learning Meets Quantum Physics*.   Springer, 2020,
    pp. 445–467.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] V. Svetnik, A. Liaw, C. Tong, J. C. Culberson, R. P. Sheridan, and B. P.
    Feuston, “Random forest: a classification and regression tool for compound classification
    and qsar modeling,” *Journal of chemical information and computer sciences*, vol. 43,
    no. 6, pp. 1947–1958, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] L. Zhang, H. Zhu, T. I. Oprea, A. Golbraikh, and A. Tropsha, “Qsar modeling
    of the blood–brain barrier permeability for diverse organic compounds,” *Pharmaceutical
    research*, vol. 25, no. 8, pp. 1902–1914, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Fröhlich, J. K. Wegner, F. Sieker, and A. Zell, “Kernel functions for
    attributed molecular graphs–a new similarity-based approach to adme prediction
    in classification and regression,” *QSAR & Combinatorial Science*, vol. 25, no. 4,
    pp. 317–326, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. Chen, S. Liu, P. Kingsbury, S. Sohn, C. B. Storlie, E. B. Habermann,
    J. M. Naessens, D. W. Larson, and H. Liu, “Deep learning and alternative learning
    strategies for retrospective real-world clinical data,” *NPJ digital medicine*,
    vol. 2, no. 1, pp. 1–5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] V. Borisov, T. Leemann, K. Seßler, J. Haug, M. Pawelczyk, and G. Kasneci,
    “Deep neural networks and tabular data: A survey,” *arXiv preprint arXiv:2110.01889*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] H. Li, K.-H. Sze, G. Lu, and P. J. Ballester, “Machine-learning scoring
    functions for structure-based virtual screening,” *Wiley Interdisciplinary Reviews:
    Computational Molecular Science*, vol. 11, no. 1, p. e1478, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Bomane, A. Gonçalves, and P. J. Ballester, “Paclitaxel response can
    be predicted with interpretable multi-variate classifiers exploiting dna-methylation
    and mirna data,” *Frontiers in genetics*, vol. 10, p. 1041, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] L. Grinsztajn, E. Oyallon, and G. Varoquaux, “Why do tree-based models
    still outperform deep learning on tabular data?” *arXiv preprint arXiv:2207.08815*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. Paul, A. Furmanchuk, W.-k. Liao, A. Choudhary, and A. Agrawal, “Property
    prediction of organic donor molecules for photovoltaic applications using extremely
    randomized trees,” *Molecular informatics*, vol. 38, no. 11-12, p. 1900038, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] U. Rester, “From virtuality to reality-virtual screening in lead discovery
    and lead optimization: a medicinal chemistry perspective.” *Current opinion in
    drug discovery & development*, vol. 11, no. 4, pp. 559–568, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. M. Rollinger, H. Stuppner, and T. Langer, “Virtual screening for the
    discovery of bioactive natural products,” *Natural compounds as drugs Volume I*,
    pp. 211–249, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] G. Ghislat, T. Rahman, and P. J. Ballester, “Recent progress on the prospective
    application of machine learning to structure-based virtual screening,” *Current
    opinion in chemical biology*, vol. 65, pp. 28–34, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] I. Wallach, M. Dzamba, and A. Heifets, “Atomnet: a deep convolutional
    neural network for bioactivity prediction in structure-based drug discovery,”
    *arXiv preprint arXiv:1510.02855*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Q. Feng, E. Dueva, A. Cherkasov, and M. Ester, “Padme: A deep learning-based
    framework for drug-target interaction prediction,” *arXiv preprint arXiv:1807.09741*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. I. Davis, J. P. Hunt, S. Herrgard, P. Ciceri, L. M. Wodicka, G. Pallares,
    M. Hocker, D. K. Treiber, and P. P. Zarrinkar, “Comprehensive analysis of kinase
    inhibitor selectivity,” *Nature biotechnology*, vol. 29, no. 11, pp. 1046–1051,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. T. Metz, E. F. Johnson, N. B. Soni, P. J. Merta, L. Kifle, and P. J.
    Hajduk, “Navigating the kinome,” *Nature chemical biology*, vol. 7, no. 4, pp.
    200–202, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Tang, A. Szwajda, S. Shakyawar, T. Xu, P. Hintsanen, K. Wennerberg,
    and T. Aittokallio, “Making sense of large-scale kinase inhibitor bioactivity
    data sets: a comparative and integrative analysis,” *Journal of Chemical Information
    and Modeling*, vol. 54, no. 3, pp. 735–743, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] K. Y. Gao, A. Fokoue, H. Luo, A. Iyengar, S. Dey, P. Zhang *et al.*, “Interpretable
    drug target prediction using deep neural representation.” in *IJCAI*, vol. 2018,
    2018, pp. 3371–3377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Q. Yang, V. Sresht, P. Bolgar, X. Hou, J. L. Klug-McLeod, C. R. Butler
    *et al.*, “Molecular transformer unifies reaction prediction and retrosynthesis
    across pharma chemical space,” *Chemical communications*, vol. 55, no. 81, pp.
    12 152–12 155, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Dong, M. Zhao, Y. Liu, Y. Su, and X. Zeng, “Deep learning in retrosynthesis
    planning: datasets, models and tools,” *Briefings in Bioinformatics*, vol. 23,
    no. 1, p. bbab391, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Cook, A. P. Johnson, J. Law, M. Mirzazadeh, O. Ravitz, and A. Simon,
    “Computer-aided synthesis design: 40 years on,” *Wiley Interdisciplinary Reviews:
    Computational Molecular Science*, vol. 2, no. 1, pp. 79–107, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. Lavecchia, “Machine-learning approaches in drug discovery: methods
    and applications,” *Drug discovery today*, vol. 20, no. 3, pp. 318–331, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] E. Gawehn, J. A. Hiss, and G. Schneider, “Deep learning in drug discovery,”
    *Molecular informatics*, vol. 35, no. 1, pp. 3–14, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] R. Gupta, D. Srivastava, M. Sahu, S. Tiwari, R. K. Ambasta, and P. Kumar,
    “Artificial intelligence to deep learning: machine intelligence approach for drug
    discovery,” *Molecular Diversity*, vol. 25, no. 3, pp. 1315–1360, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A. Nigam, R. Pollice, M. F. Hurley, R. J. Hickman, M. Aldeghi, N. Yoshikawa,
    S. Chithrananda, V. A. Voelz, and A. Aspuru-Guzik, “Assigning confidence to molecular
    property prediction,” *Expert opinion on drug discovery*, vol. 16, no. 9, pp.
    1009–1023, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] O. Wieder, S. Kohlbacher, M. Kuenemann, A. Garon, P. Ducrot, T. Seidel,
    and T. Langer, “A compact review of molecular property prediction with graph neural
    networks,” *Drug Discovery Today: Technologies*, vol. 37, pp. 1–12, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] X. Tong, X. Liu, X. Tan, X. Li, J. Jiang, Z. Xiong, T. Xu, H. Jiang, N. Qiao,
    and M. Zheng, “Generative models for de novo drug design,” *Journal of Medicinal
    Chemistry*, vol. 64, no. 19, pp. 14 011–14 027, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D. C. Elton, Z. Boukouvalas, M. D. Fuge, and P. W. Chung, “Deep learning
    for molecular design—a review of the state of the art,” *Molecular Systems Design
    & Engineering*, vol. 4, no. 4, pp. 828–849, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] T. J. Struble, J. C. Alvarez, S. P. Brown, M. Chytil, J. Cisar, R. L.
    DesJarlais, O. Engkvist, S. A. Frank, D. R. Greve, D. J. Griffin *et al.*, “Current
    and future roles of artificial intelligence in medicinal chemistry synthesis,”
    *Journal of medicinal chemistry*, vol. 63, no. 16, pp. 8667–8682, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] M. R. Dobbelaere, P. P. Plehiers, R. Van de Vijver, C. V. Stevens, and
    K. M. Van Geem, “Machine learning in chemical engineering: strengths, weaknesses,
    opportunities, and threats,” *Engineering*, vol. 7, no. 9, pp. 1201–1211, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. L. Durant, B. A. Leland, D. R. Henry, and J. G. Nourse, “Reoptimization
    of mdl keys for use in drug discovery,” *Journal of chemical information and computer
    sciences*, vol. 42, no. 6, pp. 1273–1280, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] “Pubchem substructure fingerprint,” [https://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.pdf](https://ftp.ncbi.nlm.nih.gov/pubchem/specifications/pubchem_fingerprints.pdf),
    accessed September 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] “Rdkit,” [https://www.rdkit.org/](https://www.rdkit.org/), accessed September
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] N. M. O’Boyle, M. Banck, C. A. James, C. Morley, T. Vandermeersch, and
    G. R. Hutchison, “Open babel: An open chemical toolbox,” *Journal of cheminformatics*,
    vol. 3, no. 1, pp. 1–14, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] “Chemistry development kit (cdk),” [https://cdk.github.io/](https://cdk.github.io/),
    accessed September 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Xu, S. Wang, F. Zhu, and J. Huang, “Seq2seq fingerprint: An unsupervised
    deep molecular embedding for drug discovery,” in *Proceedings of the 8th ACM international
    conference on bioinformatics, computational biology, and health informatics*,
    2017, pp. 285–294.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] W. Jin, R. Barzilay, and T. Jaakkola, “Junction tree variational autoencoder
    for molecular graph generation,” in *ICML*.   PMLR, 2018, pp. 2323–2332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] D. Weininger, “Smiles, a chemical language and information system. 1.
    introduction to methodology and encoding rules,” *Journal of chemical information
    and computer sciences*, vol. 28, no. 1, pp. 31–36, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Honda, S. Shi, and H. R. Ueda, “Smiles transformer: Pre-trained molecular
    fingerprint for low data drug discovery,” *arXiv preprint arXiv:1911.04738*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Adilov, “Generative pre-training from molecules,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] P. Chen, W. Liu, C.-Y. Hsieh, G. Chen, and S. Zhang, “Utilizing edge features
    in graph neural networks via variational information maximization,” *arXiv preprint
    arXiv:1906.05488*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Olivecrona, T. Blaschke, O. Engkvist, and H. Chen, “Molecular de-novo
    design through deep reinforcement learning,” *Journal of cheminformatics*, vol. 9,
    no. 1, pp. 1–14, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, “Generating focused
    molecule libraries for drug discovery with recurrent neural networks,” *ACS central
    science*, vol. 4, no. 1, pp. 120–131, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Gupta, A. T. Müller, B. J. Huisman, J. A. Fuchs, P. Schneider, and
    G. Schneider, “Generative recurrent networks for de novo drug design,” *Molecular
    informatics*, vol. 37, no. 1-2, p. 1700111, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] F. Grisoni, M. Moret, R. Lingwood, and G. Schneider, “Bidirectional molecule
    generation with recurrent neural networks,” *Journal of chemical information and
    modeling*, vol. 60, no. 3, pp. 1175–1183, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] E. J. Bjerrum and R. Threlfall, “Molecular generation with recurrent neural
    networks (rnns),” *arXiv preprint arXiv:1705.04612*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] P. Ertl, R. Lewis, E. Martin, and V. Polyakov, “In silico generation of
    novel, drug-like chemical matter using the lstm neural network,” *arXiv preprint
    arXiv:1712.07449*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Schiff, V. Chenthamarakshan, S. C. Hoffman, K. N. Ramamurthy, and P. Das,
    “Augmenting molecular deep generative models with topological data analysis representations,”
    in *ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*.   IEEE, 2022, pp. 3783–3787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z. Alperstein, A. Cherkasov, and J. T. Rolfe, “All smiles variational
    autoencoder,” *arXiv preprint arXiv:1905.13343*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Nigam, P. Friederich, M. Krenn, and A. Aspuru-Guzik, “Augmenting genetic
    algorithms with deep neural networks for exploring the chemical space,” *arXiv
    preprint arXiv:1909.11655*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Moss, D. Leslie, D. Beck, J. Gonzalez, and P. Rayson, “Boss: Bayesian
    optimization over string spaces,” *Advances in neural information processing systems*,
    vol. 33, pp. 15 476–15 486, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] W. Jin, K. Yang, R. Barzilay, and T. Jaakkola, “Learning multimodal graph-to-graph
    translation for molecular optimization,” *arXiv preprint arXiv:1812.01070*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] W. Jin, R. Barzilay, and T. Jaakkola, “Hierarchical generation of molecular
    graphs using structural motifs,” in *ICML*.   PMLR, 2020, pp. 4839–4848.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] K. Maziarz, H. Jackson-Flux, P. Cameron, F. Sirockin, N. Schneider, N. Stiefl,
    M. Segler, and M. Brockschmidt, “Learning to extend molecular scaffolds with structural
    motifs,” *arXiv preprint arXiv:2103.03864*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. You, B. Liu, Z. Ying, V. Pande, and J. Leskovec, “Graph convolutional
    policy network for goal-directed molecular graph generation,” *Advances in neural
    information processing systems*, vol. 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Yang, D. Hwang, S. Lee, S. Ryu, and S. J. Hwang, “Hit and lead discovery
    with explorative rl and fragment-based molecule generation,” *Advances in Neural
    Information Processing Systems*, vol. 34, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] W. Jin, R. Barzilay, and T. Jaakkola, “Multi-objective molecule generation
    using interpretable substructures,” in *ICML*.   PMLR, 2020, pp. 4849–4859.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] B. Chen, T. Wang, C. Li, H. Dai, and L. Song, “Molecule optimization by
    explainable evolution,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Y. Xie, C. Shi, H. Zhou, Y. Yang, W. Zhang, Y. Yu, and L. Li, “Mars: Markov
    molecular sampling for multi-objective drug discovery,” *arXiv preprint arXiv:2103.10432*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan, “An introduction
    to mcmc for machine learning,” *Machine learning*, vol. 50, no. 1, pp. 5–43, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. Fu, W. Gao, C. Xiao, J. Yasonik, C. W. Coley, and J. Sun, “Differentiable
    scaffolding tree for molecular optimization,” *arXiv preprint arXiv:2109.10469*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Z. Zhou, S. Kearnes, L. Li, R. N. Zare, and P. Riley, “Optimization of
    molecules via deep reinforcement learning,” *Scientific reports*, vol. 9, no. 1,
    pp. 1–10, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] C. Shi, M. Xu, Z. Zhu, W. Zhang, M. Zhang, and J. Tang, “Graphaf: a flow-based
    autoregressive model for molecular graph generation,” *arXiv preprint arXiv:2001.09382*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] K. Korovina, S. Xu, K. Kandasamy, W. Neiswanger, B. Poczos, J. Schneider,
    and E. Xing, “Chembo: Bayesian optimization of small organic molecules with synthesizable
    recommendations,” in *International Conference on Artificial Intelligence and
    Statistics*.   PMLR, 2020, pp. 3393–3403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Z. Chen, X. Fang, F. Wang, X. Fan, H. Wu, and H. Wang, “Cells: Cost-effective
    evolution in latent space for goal-directed molecular generation,” *arXiv preprint
    arXiv:2112.00905*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Wu, N. Choma, A. Chen, M. Cashman, É. T. Prates, M. Shah, V. G. M.
    Vergara, A. Clyde, T. S. Brettin, W. A. de Jong *et al.*, “Spatial graph attention
    and curiosity-driven policy for antiviral drug discovery,” *arXiv preprint arXiv:2106.02190*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] G. N. Simm and J. M. Hernández-Lobato, “A generative model for molecular
    distance geometry,” *arXiv preprint arXiv:1909.11459*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Xu, W. Wang, S. Luo, C. Shi, Y. Bengio, R. Gomez-Bombarelli, and J. Tang,
    “An end-to-end framework for molecular conformation generation via bilevel programming,”
    in *ICML*.   PMLR, 2021, pp. 11 537–11 547.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. Shi, S. Luo, M. Xu, and J. Tang, “Learning gradient fields for molecular
    conformation generation,” in *ICML*.   PMLR, 2021, pp. 9558–9568.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] J. Zhu, Y. Xia, C. Liu, L. Wu, S. Xie, T. Wang, Y. Wang, W. Zhou, T. Qin,
    H. Li *et al.*, “Direct molecular conformation generation,” *arXiv preprint arXiv:2202.01356*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, “Geodiff: A geometric
    diffusion model for molecular conformation generation,” *arXiv preprint arXiv:2203.02923*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Y. Luo and S. Ji, “An autoregressive flow model for 3d molecular geometry
    generation from scratch,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] E. Mansimov, O. Mahmood, S. Kang, and K. Cho, “Molecular geometry prediction
    using a deep generative graph neural network,” *Scientific reports*, vol. 9, no. 1,
    pp. 1–13, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] J. Guan, W. W. Qian, W.-Y. Ma, J. Ma, J. Peng *et al.*, “Energy-inspired
    molecular conformation optimization,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] M. Xu, S. Luo, Y. Bengio, J. Peng, and J. Tang, “Learning neural generative
    dynamics for molecular conformation generation,” *arXiv preprint arXiv:2102.10240*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “On the properties
    of neural machine translation: Encoder-decoder approaches,” *arXiv preprint arXiv:1409.1259*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] A. Mayr, G. Klambauer, T. Unterthiner, M. Steijaert, J. K. Wegner, H. Ceulemans,
    D.-A. Clevert, and S. Hochreiter, “Large-scale comparison of machine learning
    methods for drug target prediction on chembl,” *Chemical science*, vol. 9, no. 24,
    pp. 5441–5451, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] G. B. Goh, N. O. Hodas, C. Siegel, and A. Vishnu, “Smiles2vec: An interpretable
    general-purpose deep neural network for predicting chemical properties,” *arXiv
    preprint arXiv:1712.02034*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] C. Li, J. Feng, S. Liu, and J. Yao, “A novel molecular representation
    learning for molecular property prediction with a multiple smiles-based augmentation,”
    *Computational Intelligence and Neuroscience*, vol. 2022, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Peng, Z. Zhang, Q. Jiang, J. Guan, and S. Zhou, “Top: Towards better
    toxicity prediction by deep molecular representation learning,” in *2019 IEEE
    International Conference on Bioinformatics and Biomedicine*.   IEEE, 2019, pp.
    318–325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Paul, D. Jha, R. Al-Bahrani, W.-k. Liao, A. Choudhary, and A. Agrawal,
    “Chemixnet: Mixed dnn architectures for predicting chemical properties using multiple
    molecular representations,” *arXiv preprint arXiv:1811.08283*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] G. B. Goh, C. Siegel, A. Vishnu, N. O. Hodas, and N. Baker, “Chemception:
    a deep neural network with minimal chemistry knowledge matches the performance
    of expert-developed qsar/qspr models,” *arXiv preprint arXiv:1706.06689*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] D. P. Kingma, S. Mohamed, D. Jimenez Rezende, and M. Welling, “Semi-supervised
    learning with deep generative models,” *Advances in neural information processing
    systems*, vol. 27, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining
    approach,” *arXiv preprint arXiv:1907.11692*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] D. R. Rahimovich, S. R. Abdiqayum o’g, A. S. Qaxramon o’g’li *et al.*,
    “Predicting the activity and properties of chemicals based on roberta,” in *2021
    International Conference on Information Science and Communications Technologies
    (ICISCT)*.   IEEE, 2021, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Chithrananda, G. Grand, and B. Ramsundar, “Chemberta: large-scale self-supervised
    pretraining for molecular property prediction,” *arXiv preprint arXiv:2010.09885*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] T. Unterthiner, A. Mayr, G. Klambauer, M. Steijaert, J. K. Wegner, H. Ceulemans,
    and S. Hochreiter, “Deep learning as an opportunity in virtual screening,” in
    *Proceedings of the deep learning workshop at NIPS*, vol. 27, 2014, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Mayr, G. Klambauer, T. Unterthiner, and S. Hochreiter, “Deeptox: toxicity
    prediction using deep learning,” *Frontiers in Environmental Science*, vol. 3,
    p. 80, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Schimunek, P. Seidl, L. Friedrich, D. Kuhn, F. Rippmann, S. Hochreiter,
    and G. Klambauer, “Context-enriched molecule representations improve few-shot
    drug discovery,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Zhang, S. Wang, F. Zhu, Z. Xu, Y. Wang, and J. Huang, “Seq3seq fingerprint:
    towards end-to-end semi-supervised deep drug discovery,” in *Proceedings of the
    2018 ACM International Conference on Bioinformatics, Computational Biology, and
    Health Informatics*, 2018, pp. 404–413.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Wang, Y. Guo, Y. Wang, H. Sun, and J. Huang, “Smiles-bert: large scale
    unsupervised pre-training for molecular property prediction,” in *Proceedings
    of the 10th ACM international conference on bioinformatics, computational biology
    and health informatics*, 2019, pp. 429–436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] R. C. Glen, A. Bender, C. H. Arnby, L. Carlsson, S. Boyer, and J. Smith,
    “Circular fingerprints: flexible molecular descriptors with applications from
    physical chemistry to adme,” *IDrugs*, vol. 9, no. 3, p. 199, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv preprint arXiv:1409.0473*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” *arXiv preprint arXiv:1609.02907*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. Von Lilienfeld, “Quantum
    chemistry structures and properties of 134 kilo molecules,” *Scientific data*,
    vol. 1, no. 1, pp. 1–7, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] C. Lu, Q. Liu, C. Wang, Z. Huang, P. Lin, and L. He, “Molecular property
    prediction: A multilevel quantum interactions modeling perspective,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 33, no. 01, 2019, pp.
    1052–1060.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] S. Liu, H. Wang, W. Liu, J. Lasenby, H. Guo, and J. Tang, “Pre-training
    molecular graph representation with 3d geometry,” *arXiv preprint arXiv:2110.07728*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec,
    “Strategies for pre-training graph neural networks,” *arXiv preprint arXiv:1905.12265*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] X. Zeng, H. Xiang, L. Yu, J. Wang, K. Li, R. Nussinov, and F. Cheng,
    “Accurate prediction of molecular properties and drug targets using a self-supervised
    image representation learning framework,” *Nature Machine Intelligence*, pp. 1–13,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang, “Self-supervised
    graph transformer on large-scale molecular data,” *Advances in Neural Information
    Processing Systems*, vol. 33, pp. 12 559–12 571, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, “Graph contrastive
    learning with augmentations,” *Advances in Neural Information Processing Systems*,
    vol. 33, pp. 5812–5823, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Wang, J. Wang, Z. Cao, and A. Barati Farimani, “Molecular contrastive
    learning of representations via graph neural networks,” *Nature Machine Intelligence*,
    vol. 4, no. 3, pp. 279–287, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] S. Li, J. Zhou, T. Xu, D. Dou, and H. Xiong, “Geomgcl: Geometric graph
    contrastive learning for molecular property prediction,” *arXiv preprint arXiv:2109.11730*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] S. Liu, M. F. Demirel, and Y. Liang, “N-gram graph: Simple unsupervised
    representation for graphs, with applications to molecules,” *Advances in neural
    information processing systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Altae-Tran, B. Ramsundar, A. S. Pappu, and V. Pande, “Low data drug
    discovery with one-shot learning,” *ACS central science*, vol. 3, no. 4, pp. 283–293,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,
    “Neural message passing for quantum chemistry,” in *ICML*.   PMLR, 2017, pp. 1263–1272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Y. Liu, L. Wang, M. Liu, Y. Lin, X. Zhang, B. Oztekin, and S. Ji, “Spherical
    message passing for 3d molecular graphs,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Z. Hao, C. Lu, Z. Huang, H. Wang, Z. Hu, Q. Liu, E. Chen, and C. Lee,
    “Asgn: An active semi-supervised graph neural network for molecular property prediction,”
    in *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    & Data Mining*, 2020, pp. 731–752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bresson, “Graph
    neural networks with learnable structural and positional representations,” *arXiv
    preprint arXiv:2110.07875*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] P. Velickovic, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D.
    Hjelm, “Deep graph infomax.” *ICLR (Poster)*, vol. 2, no. 3, p. 4, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman,
    A. Trischler, and Y. Bengio, “Learning deep representations by mutual information
    estimation and maximization,” *arXiv preprint arXiv:1808.06670*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural
    networks?” *arXiv preprint arXiv:1810.00826*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] F.-Y. Sun, J. Hoffmann, V. Verma, and J. Tang, “Infograph: Unsupervised
    and semi-supervised graph-level representation learning via mutual information
    maximization,” *arXiv preprint arXiv:1908.01000*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Z. Li, S. Yang, G. Song, and L. Cai, “Conformation-guided molecular representation
    with hamiltonian neural networks,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] S. Yang, Z. Li, G. Song, and L. Cai, “Deep molecular representation learning
    via fusing physical and chemical information,” *Advances in Neural Information
    Processing Systems*, vol. 34, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Ł. Maziarka, T. Danel, S. Mucha, K. Rataj, J. Tabor, and S. Jastrzębski,
    “Molecule attention transformer,” *arXiv preprint arXiv:2002.08264*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. H. Segler and M. P. Waller, “Neural-symbolic machine learning for
    retrosynthesis and reaction prediction,” *Chemistry–A European Journal*, vol. 23,
    no. 25, pp. 5966–5971, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] C. W. Coley, L. Rogers, W. H. Green, and K. F. Jensen, “Computer-assisted
    retrosynthesis based on molecular similarity,” *ACS central science*, vol. 3,
    no. 12, pp. 1237–1245, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] J. L. Baylon, N. A. Cilfone, J. R. Gulcher, and T. W. Chittenden, “Enhancing
    retrosynthetic reaction prediction with deep learning using multiscale reaction
    classification,” *Journal of chemical information and modeling*, vol. 59, no. 2,
    pp. 673–688, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] M. H. Segler, M. Preuss, and M. P. Waller, “Planning chemical syntheses
    with deep neural networks and symbolic ai,” *Nature*, vol. 555, no. 7698, pp.
    604–610, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] P. Seidl, P. Renz, N. Dyubankova, P. Neves, J. Verhoeven, J. K. Wegner,
    M. Segler, S. Hochreiter, and G. Klambauer, “Improving few-and zero-shot reaction
    template prediction using modern hopfield networks,” *Journal of chemical information
    and modeling*, vol. 62, no. 9, pp. 2111–2120, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] H. Dai, C. Li, C. Coley, B. Dai, and L. Song, “Retrosynthesis prediction
    with conditional graph logic network,” *Advances in Neural Information Processing
    Systems*, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] S. Chen and Y. Jung, “Deep retrosynthetic reaction prediction using local
    reactivity and global attention,” *JACS Au*, vol. 1, no. 10, pp. 1612–1620, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu Nguyen,
    S. Ho, J. Sloane, P. Wender, and V. Pande, “Retrosynthetic reaction prediction
    using neural sequence-to-sequence models,” *ACS central science*, vol. 3, no. 10,
    pp. 1103–1113, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] P. Karpov, G. Godin, and I. V. Tetko, “A transformer model for retrosynthesis,”
    in *International Conference on Artificial Neural Networks*.   Springer, 2019,
    pp. 817–830.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] B. Chen, T. Shen, T. S. Jaakkola, and R. Barzilay, “Learning to make
    generalizable and diverse predictions for retrosynthesis,” *arXiv preprint arXiv:1910.09688*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. Ishiguro, K. Ujihara, R. Sawada, H. Akita, and M. Kotera, “Data transfer
    approaches to improve seq-to-seq retrosynthesis,” *arXiv preprint arXiv:2010.00792*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. Zheng, J. Rao, Z. Zhang, J. Xu, and Y. Yang, “Predicting retrosynthetic
    reactions using self-corrected transformer neural networks,” *Journal of chemical
    information and modeling*, vol. 60, no. 1, pp. 47–55, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] M. Zhao, L. Fang, L. Tan, J.-G. Lou, and Y. Lepage, “Leveraging reaction-aware
    substructures for retrosynthesis and reaction prediction,” *arXiv preprint arXiv:2204.05919*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. Sacha, M. Błaz, P. Byrski, P. Dabrowski-Tumanski, M. Chrominski, R. Loska,
    P. Włodarczyk-Pruszynski, and S. Jastrzebski, “Molecule edit graph attention network:
    modeling chemical reactions as sequences of graph edits,” *Journal of Chemical
    Information and Modeling*, vol. 61, no. 7, pp. 3273–3284, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] C. Shi, M. Xu, H. Guo, M. Zhang, and J. Tang, “A graph to graphs framework
    for retrosynthesis prediction,” in *ICML*.   PMLR, 2020, pp. 8818–8827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] V. R. Somnath, C. Bunne, C. Coley, A. Krause, and R. Barzilay, “Learning
    graph models for retrosynthesis prediction,” *Advances in Neural Information Processing
    Systems*, vol. 34, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Z. Gao, C. Tan, L. Wu, and S. Z. Li, “Semiretro: Semi-template framework
    boosts deep retrosynthesis prediction,” *arXiv preprint arXiv:2202.08205*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] C. Yan, Q. Ding, P. Zhao, S. Zheng, J. Yang, Y. Yu, and J. Huang, “Retroxpert:
    Decompose retrosynthesis prediction like a chemist,” *Advances in Neural Information
    Processing Systems*, vol. 33, pp. 11 248–11 258, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] R. Sun, H. Dai, L. Li, S. Kearnes, and B. Dai, “Energy-based view of
    retrosynthesis,” *arXiv preprint arXiv:2007.13437*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] S.-W. Seo, Y. Y. Song, J. Y. Yang, S. Bae, H. Lee, J. Shin, S. J. Hwang,
    and E. Yang, “Gta: Graph truncated attention for retrosynthesis,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 35, no. 1, 2021, pp.
    531–539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Y. Wan, B. Liao, C.-Y. Hsieh, and S. Zhang, “Retroformer: Pushing the
    limits of interpretable end-to-end retrosynthesis transformer,” *arXiv preprint
    arXiv:2201.12475*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] J. N. Wei, D. Duvenaud, and A. Aspuru-Guzik, “Neural networks for the
    prediction of organic chemistry reactions,” *ACS central science*, vol. 2, no. 10,
    pp. 725–732, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] C. W. Coley, R. Barzilay, T. S. Jaakkola, W. H. Green, and K. F. Jensen,
    “Prediction of organic reaction outcomes using machine learning,” *ACS central
    science*, vol. 3, no. 5, pp. 434–443, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. H. Segler and M. P. Waller, “Modelling chemical reasoning to predict
    and invent reactions,” *Chemistry–A European Journal*, vol. 23, no. 25, pp. 6118–6128,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] W. Jin, C. Coley, R. Barzilay, and T. Jaakkola, “Predicting organic reaction
    outcomes with weisfeiler-lehman network,” *Advances in neural information processing
    systems*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] C. W. Coley, W. Jin, L. Rogers, T. F. Jamison, T. S. Jaakkola, W. H.
    Green, R. Barzilay, and K. F. Jensen, “A graph-convolutional neural network model
    for the prediction of chemical reactivity,” *Chemical science*, vol. 10, no. 2,
    pp. 370–377, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] W. W. Qian, N. T. Russell, C. L. Simons, Y. Luo, M. D. Burke, and J. Peng,
    “Integrating deep neural networks and symbolic inference for organic reactivity
    prediction,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] J. Bradshaw, M. J. Kusner, B. Paige, M. H. Segler, and J. M. Hernández-Lobato,
    “A generative model for electron paths,” *arXiv preprint arXiv:1805.10970*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] K. Do, T. Tran, and S. Venkatesh, “Graph transformation policy network
    for chemical reaction prediction,” in *Proceedings of the 25th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining*, 2019, pp. 750–760.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Bi, H. Wang, C. Shi, C. Coley, J. Tang, and H. Guo, “Non-autoregressive
    electron redistribution modeling for reaction prediction,” in *ICML*.   PMLR,
    2021, pp. 904–913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] P. Schwaller, T. Gaudin, D. Lanyi, C. Bekas, and T. Laino, ““found in
    translation”: predicting outcomes of complex organic chemistry reactions using
    neural sequence-to-sequence models,” *Chemical science*, vol. 9, no. 28, pp. 6091–6098,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li, B. A. Shoemaker,
    P. A. Thiessen, B. Yu *et al.*, “Pubchem in 2021: new data content and improved
    web interfaces,” *Nucleic acids research*, vol. 49, no. D1, pp. D1388–D1395, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] A. Gaulton, L. J. Bellis, A. P. Bento, J. Chambers, M. Davies, A. Hersey,
    Y. Light, S. McGlinchey, D. Michalovich, B. Al-Lazikani *et al.*, “Chembl: a large-scale
    bioactivity database for drug discovery,” *Nucleic acids research*, vol. 40, no. D1,
    pp. D1100–D1107, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] T. Sterling and J. J. Irwin, “Zinc 15–ligand discovery for everyone,”
    *Journal of chemical information and modeling*, vol. 55, no. 11, pp. 2324–2337,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Fink, H. Bruggesser, and J.-L. Reymond, “Virtual exploration of the
    small-molecule chemical universe below 160 daltons,” *Angewandte Chemie International
    Edition*, vol. 44, no. 10, pp. 1504–1508, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] T. Fink and J.-L. Reymond, “Virtual exploration of the chemical universe
    up to 11 atoms of c, n, o, f: assembly of 26.4 million structures (110.9 million
    stereoisomers) and analysis for new ring systems, stereochemistry, physicochemical
    properties, compound classes, and drug discovery,” *Journal of chemical information
    and modeling*, vol. 47, no. 2, pp. 342–353, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] L. C. Blum and J.-L. Reymond, “970 million druglike small molecules for
    virtual screening in the chemical universe database gdb-13,” *Journal of the American
    Chemical Society*, vol. 131, no. 25, pp. 8732–8733, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] L. Ruddigkeit, R. Van Deursen, L. C. Blum, and J.-L. Reymond, “Enumeration
    of 166 billion organic small molecules in the chemical universe database gdb-17,”
    *Journal of chemical information and modeling*, vol. 52, no. 11, pp. 2864–2875,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] S. Axelrod and R. Gomez-Bombarelli, “Geom, energy-annotated molecular
    conformations for property prediction and molecular generation,” *Scientific Data*,
    vol. 9, no. 1, pp. 1–14, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] N. Schneider, N. Stiefl, and G. A. Landrum, “What’s what: The (nearly)
    definitive guide to reaction role assignment,” *Journal of chemical information
    and modeling*, vol. 56, no. 12, pp. 2336–2346, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu,
    K. Leswing, and V. Pande, “Moleculenet: a benchmark for molecular machine learning,”
    *Chemical science*, vol. 9, no. 2, pp. 513–530, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Sun, N. Jeliazkova, V. Chupakhin, J.-F. Golib-Dzib, O. Engkvist, L. Carlsson,
    J. Wegner, H. Ceulemans, I. Georgiev, V. Jeliazkov *et al.*, “Excape-db: an integrated
    large scale dataset facilitating big data analysis in chemogenomics,” *Journal
    of cheminformatics*, vol. 9, pp. 1–9, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] W. P. Walters and M. Murcko, “Assessing the impact of generative ai on
    medicinal chemistry,” *Nature biotechnology*, vol. 38, no. 2, pp. 143–145, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher, “Guacamol: benchmarking
    models for de novo molecular design,” *Journal of chemical information and modeling*,
    vol. 59, no. 3, pp. 1096–1108, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] M. Stanley, J. F. Bronskill, K. Maziarz, H. Misztela, J. Lanini, M. Segler,
    N. Schneider, and M. Brockschmidt, “Fs-mol: A few-shot learning dataset of molecules,”
    in *Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track (Round 2)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] G. Subramanian, B. Ramsundar, V. Pande, and R. A. Denny, “Computational
    modeling of $\beta$-secretase 1 (bace-1) inhibitors using ligand based approaches,”
    *Journal of chemical information and modeling*, vol. 56, no. 10, pp. 1936–1949,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Guo, V. Thost, B. Li, P. Das, J. Chen, and W. Matusik, “Data-efficient
    graph grammar learning for molecular generation,” *arXiv preprint arXiv:2203.08031*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] B. Chen, C. Li, H. Dai, and L. Song, “Retro*: learning retrosynthetic
    planning with neural guided a* search,” in *ICML*.   PMLR, 2020, pp. 1608–1616.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] P. Sidorov, S. Naulaerts, J. Ariey-Bonnet, E. Pasquier, and P. J. Ballester,
    “Predicting synergism of cancer drug combinations using nci-almanac data,” *Frontiers
    in chemistry*, vol. 7, p. 509, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Z. Safikhani, P. Smirnov, M. Freeman, N. El-Hachem, A. She, Q. Rene,
    A. Goldenberg, N. J. Birkbak, C. Hatzis, L. Shi *et al.*, “Revisiting inconsistency
    in large pharmacogenomic studies,” *F1000Research*, vol. 5, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] P. Renz, D. Van Rompaey, J. K. Wegner, S. Hochreiter, and G. Klambauer,
    “On failure modes in molecule generation and optimization,” *Drug Discovery Today:
    Technologies*, vol. 32, pp. 55–63, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] P. Schwaller, R. Petraglia, V. Zullo, V. H. Nair, R. A. Haeuselmann,
    R. Pisoni, C. Bekas, A. Iuliano, and T. Laino, “Predicting retrosynthetic pathways
    using transformer-based models and a hyper-graph exploration strategy,” *Chemical
    science*, vol. 11, no. 12, pp. 3316–3325, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] A. Thakkar, T. Kogej, J.-L. Reymond, O. Engkvist, and E. J. Bjerrum,
    “Datasets and their influence on the development of computer assisted synthesis
    planning tools in the pharmaceutical domain,” *Chemical science*, vol. 11, no. 1,
    pp. 154–168, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] M. E. Fortunato, C. W. Coley, B. C. Barnes, and K. F. Jensen, “Data augmentation
    and pretraining for template-based retrosynthetic prediction in computer-aided
    synthesis planning,” *Journal of chemical information and modeling*, vol. 60,
    no. 7, pp. 3398–3407, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] J. Jiménez-Luna, F. Grisoni, and G. Schneider, “Drug discovery with explainable
    artificial intelligence,” *Nature Machine Intelligence*, vol. 2, no. 10, pp. 573–584,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran, G. Lee,
    B. Li, A. Madabhushi, P. Shah, M. Spitzer *et al.*, “Applications of machine learning
    in drug discovery and development,” *Nature reviews Drug discovery*, vol. 18,
    no. 6, pp. 463–477, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] K. Preuer, G. Klambauer, F. Rippmann, S. Hochreiter, and T. Unterthiner,
    “Interpretable deep learning in drug discovery,” *Explainable AI: interpreting,
    explaining and visualizing deep learning*, pp. 331–345, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Y. Du, X. Guo, A. Shehu, and L. Zhao, “Interpretable molecular graph
    generation via monotonic constraints,” in *Proceedings of the 2022 SIAM International
    Conference on Data Mining (SDM)*.   SIAM, 2022, pp. 73–81.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Y. Du, T. Fu, J. Sun, and S. Liu, “Molgensurvey: A systematic survey
    in machine learning models for molecule design,” *arXiv preprint arXiv:2203.14500*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] L. H. Mervin, S. Johansson, E. Semenova, K. A. Giblin, and O. Engkvist,
    “Uncertainty quantification in drug design,” *Drug discovery today*, vol. 26,
    no. 2, pp. 474–489, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] S. Hernández-Hernández, S. Vishwakarma, and P. Ballester, “Conformal
    prediction of small-molecule drug resistance in cancer cell lines,” in *Conformal
    and Probabilistic Prediction with Applications*.   PMLR, 2022, pp. 92–108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] B. Hie, B. D. Bryson, and B. Berger, “Leveraging uncertainty in machine
    learning accelerates biological discovery and design,” *Cell systems*, vol. 11,
    no. 5, pp. 461–477, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] A. C. Vaucher, P. Schwaller, J. Geluykens, V. H. Nair, A. Iuliano, and
    T. Laino, “Inferring experimental procedures from text-based representations of
    chemical reactions,” *Nature communications*, vol. 12, no. 1, pp. 1–11, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] X. Wang, Y. Qian, H. Gao, C. W. Coley, Y. Mo, R. Barzilay, and K. F.
    Jensen, “Towards efficient discovery of green synthetic pathways with monte carlo
    tree search and reinforcement learning,” *Chemical science*, vol. 11, no. 40,
    pp. 10 959–10 972, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] A. B. Henson, P. S. Gromski, and L. Cronin, “Designing algorithms to
    aid discovery by chemical robots,” *ACS central science*, vol. 4, no. 7, pp. 793–804,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
