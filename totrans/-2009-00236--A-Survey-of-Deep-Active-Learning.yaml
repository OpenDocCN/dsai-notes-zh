- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:59:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:59:45'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2009.00236] A Survey of Deep Active Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2009.00236] 深度主动学习的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2009.00236](https://ar5iv.labs.arxiv.org/html/2009.00236)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2009.00236](https://ar5iv.labs.arxiv.org/html/2009.00236)
- en: A Survey of Deep Active Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度主动学习的调查
- en: Pengzhen Ren [pzhren@foxmail.com](mailto:pzhren@foxmail.com) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier") ,  Yun Xiao [yxiao@nwu.edu.cn](mailto:yxiao@nwu.edu.cn) Northwest
    University ,  Xiaojun Chang [cxj273@gmail.com](mailto:cxj273@gmail.com) RMIT University
    ,  Po-Yao Huang Carnegie Mellon University ,  Zhihui Li Qilu University of Technology
    (Shandong Academy of Sciences) ,  Brij B. Gupta National Institute of Technology
    Kurukshetra, India ,  Xiaojiang Chen  and  Xin Wang Northwest University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 彭振仁 [pzhren@foxmail.com](mailto:pzhren@foxmail.com) [1234-5678-9012](https://orcid.org/1234-5678-9012
    "ORCID identifier") ，晓云 [yxiao@nwu.edu.cn](mailto:yxiao@nwu.edu.cn) 西北大学 ，常晓军
    [cxj273@gmail.com](mailto:cxj273@gmail.com) RMIT大学 ，黄泊尧 卡内基梅隆大学 ，李智辉 齐鲁工业大学（山东科学院）
    ，布里吉·B·古普塔 印度国家技术学院库鲁克谢特尔分校 ，陈晓江 和 王欣 西北大学
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Active learning (AL) attempts to maximize a model’s performance gain while annotating
    the fewest samples possible. Deep learning (DL) is greedy for data and requires
    a large amount of data supply to optimize a massive number of parameters if the
    model is to learn how to extract high-quality features. In recent years, due to
    the rapid development of internet technology, we have entered an era of information
    abundance characterized by massive amounts of available data. As a result, DL
    has attracted significant attention from researchers and has been rapidly developed.
    Compared with DL, however, researchers have a relatively low interest in AL. This
    is mainly because before the rise of DL, traditional machine learning requires
    relatively few labeled samples, meaning that early AL is rarely according the
    value it deserves. Although DL has made breakthroughs in various fields, most
    of this success is due to a large number of publicly available annotated datasets.
    However, the acquisition of a large number of high-quality annotated datasets
    consumes a lot of manpower, making it unfeasible in fields that require high levels
    of expertise (such as speech recognition, information extraction, medical images,
    etc.). Therefore, AL is gradually coming to receive the attention it is due.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习（AL）试图在标注尽可能少的样本的同时，最大化模型的性能提升。深度学习（DL）对数据有极大的需求，需要大量的数据来优化大量的参数，以便模型能够学习如何提取高质量的特征。近年来，由于互联网技术的快速发展，我们进入了一个信息丰富的时代，海量数据触手可及。因此，DL受到了研究人员的广泛关注，并得到了迅速发展。然而，与DL相比，研究人员对AL的兴趣相对较低。这主要是因为在DL兴起之前，传统机器学习需要相对较少的标注样本，早期的AL并未得到应有的重视。尽管DL在各个领域取得了突破，但这些成功大多归功于大量公开的标注数据集。然而，获得大量高质量标注数据集需要耗费大量人力，这在需要高度专业知识的领域（如语音识别、信息提取、医学图像等）中是不可行的。因此，AL逐渐开始得到应有的关注。
- en: It is therefore natural to investigate whether AL can be used to reduce the
    cost of sample annotation while retaining the powerful learning capabilities of
    DL. As a result of such investigations, deep active learning (DeepAL) has emerged.
    Although research on this topic is quite abundant, there has not yet been a comprehensive
    survey of DeepAL-related works; accordingly, this article aims to fill this gap.
    We provide a formal classification method for the existing work, along with a
    comprehensive and systematic overview. In addition, we also analyze and summarize
    the development of DeepAL from an application perspective. Finally, we discuss
    the confusion and problems associated with DeepAL and provide some possible development
    directions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，探讨AL是否可以在保留DL强大学习能力的同时减少样本标注成本是很自然的。经过这样的研究，深度主动学习（DeepAL）应运而生。尽管这一主题的研究相当丰富，但尚未对DeepAL相关工作进行全面调查；因此，本文旨在填补这一空白。我们为现有工作提供了一种正式的分类方法，并进行了全面系统的概述。此外，我们还从应用角度分析和总结了DeepAL的发展。最后，我们讨论了与DeepAL相关的困惑和问题，并提供了一些可能的发展方向。
- en: 'Deep Learning, Active Learning, Deep Active Learning.^†^†ccs: Computing methodologies
     Machine learning algorithms'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习、主动学习、深度主动学习。^†^†ccs: 计算方法 学习算法'
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Both deep learning (DL) and active learning (AL) are a subfield of machine
    learning. DL is also called representation learning (Deng, [2014](#bib.bib51)).
    It originates from the study of artificial neural networks and realizes the automatic
    extraction of data features. DL has strong learning capabilities due to its complex
    structure, but this also means that DL requires a large number of labeled samples
    to complete the corresponding training. With the release of a large number of
    large-scale data sets with annotations and the continuous improvement of computer
    computing power, DL-related research has ushered in large development opportunities.
    Compared with traditional machine learning algorithms, DL has an absolute advantage
    in performance in most application areas. AL focuses on the study of data sets,
    and it is also known as query learning (Settles, [2009](#bib.bib194)). AL assumes
    that different samples in the same data set have different values for the update
    of the current model, and tries to select the samples with the highest value to
    construct the training set. Then, the corresponding learning task is completed
    with the smallest annotation cost. Both DL and AL have important applications
    in the machine learning community. Due to their excellent characteristics, they
    have attracted widespread research interest in recent years. More specifically,
    DL has achieved unprecedented breakthroughs in various challenging tasks; however,
    this is largely due to the publication of massive labeled datasets (Bengio et al.,
    [2006](#bib.bib22); Krizhevsky et al., [2012](#bib.bib121)). Therefore, DL is
    limited by the high cost of sample labeling in some professional fields that require
    rich knowledge. In comparison, an effective AL algorithm can theoretically achieve
    exponential acceleration in labeling efficiency (Balcan et al., [2009](#bib.bib18)).
    This large potential saving in labeling costs is a fascinating development. However,
    the classic AL algorithm also finds it difficult to handle high-dimensional data
    (Tong, [2001](#bib.bib222)). Therefore, the combination of DL and AL, referred
    to as DeepAL, is expected to achieve superior results. DeepAL has been widely
    utilized in various fields, including image recognition (Du et al., [2019](#bib.bib57);
    Gal et al., [2017](#bib.bib73); Gudovskiy et al., [2020](#bib.bib83); Huang et al.,
    [2020](#bib.bib99)), text classification (Zhang et al., [2017b](#bib.bib252);
    Schröder and Niekler, [2020](#bib.bib191)), visual question answering (Lin and
    Parikh, [2017](#bib.bib135)) and object detection (Qu et al., [2020](#bib.bib171);
    Aghdam et al., [2019](#bib.bib5); Feng et al., [2019](#bib.bib64)), etc. Although
    a rich variety of related work has been published, DeepAL still lacks a unified
    classification framework. To fill this gap, in this article, we will provide a
    comprehensive overview of the existing DeepAL related work ¹¹1We search about
    270 related papers on [DBLP](https://dblp.uni-trier.de/) using ”deep active learning”
    as the keyword. We review the relevance of these papers to DeepAL one by one,
    eliminate irrelevant (just containing a few keywords) or information missing papers,
    and manually add some papers that do not contain these keywords but use DeepAL-related
    methods or relate to our current discussion. Finally, the survey references are
    constructed. The latest paper is updated to November 2020\. The references include
    103 conference papers, 153 journal papers, 3 books (Sanderson, [2008](#bib.bib184);
    Tong, [2001](#bib.bib222); Farahani and Hekmatfar, [2009](#bib.bib63)), 1 research
    report (Settles, [2009](#bib.bib194)), and 1 dissertation (Zhu et al., [2005](#bib.bib261)).
    There are 28 unpublished papers., along with a formal classification method. The
    contributions of this survey are summarized as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）和主动学习（AL）都是机器学习的一个子领域。DL 也被称为表示学习（Deng, [2014](#bib.bib51)）。它起源于对人工神经网络的研究，实现了数据特征的自动提取。由于其复杂的结构，DL
    具有强大的学习能力，但这也意味着 DL 需要大量标记样本来完成相应的训练。随着大量标注的大规模数据集的发布和计算机计算能力的持续提高，DL 相关的研究迎来了巨大的发展机会。与传统机器学习算法相比，DL
    在大多数应用领域的性能上具有绝对优势。AL 关注数据集的研究，也称为查询学习（Settles, [2009](#bib.bib194)）。AL 假设同一数据集中不同的样本对当前模型的更新具有不同的价值，并尝试选择具有最高价值的样本来构建训练集。然后，以最小的标注成本完成相应的学习任务。DL
    和 AL 在机器学习领域都具有重要应用。由于它们优越的特性，近年来引起了广泛的研究兴趣。更具体地说，DL 在各种具有挑战性的任务中取得了前所未有的突破；然而，这在很大程度上是由于大规模标记数据集的发布（Bengio
    et al., [2006](#bib.bib22); Krizhevsky et al., [2012](#bib.bib121)）。因此，DL 在一些需要丰富知识的专业领域受到标记样本高成本的限制。相比之下，有效的
    AL 算法在标记效率上理论上可以实现指数级加速（Balcan et al., [2009](#bib.bib18)）。这种标记成本的巨大潜在节省是一个令人着迷的发展。然而，经典的
    AL 算法也发现很难处理高维数据（Tong, [2001](#bib.bib222)）。因此，被称为 DeepAL 的 DL 和 AL 的结合，预计能实现优异的结果。DeepAL
    已广泛应用于各种领域，包括图像识别（Du et al., [2019](#bib.bib57); Gal et al., [2017](#bib.bib73);
    Gudovskiy et al., [2020](#bib.bib83); Huang et al., [2020](#bib.bib99)）、文本分类（Zhang
    et al., [2017b](#bib.bib252); Schröder 和 Niekler, [2020](#bib.bib191)）、视觉问答（Lin
    和 Parikh, [2017](#bib.bib135)）以及目标检测（Qu et al., [2020](#bib.bib171); Aghdam et
    al., [2019](#bib.bib5); Feng et al., [2019](#bib.bib64)）等。虽然已有大量相关工作发表，但 DeepAL
    仍缺乏统一的分类框架。为填补这一空白，本文将对现有的 DeepAL 相关工作进行全面概述¹¹1我们在 [DBLP](https://dblp.uni-trier.de/)
    上使用“deep active learning”作为关键词搜索了大约 270 篇相关论文。我们逐一审查这些论文与 DeepAL 的相关性，剔除不相关（仅包含少量关键词）或信息缺失的论文，并手动添加一些未包含这些关键词但使用
    DeepAL 相关方法或与我们当前讨论相关的论文。最后，构建了调查参考文献。最新的论文更新到 2020 年 11 月。参考文献包括 103 篇会议论文、153
    篇期刊论文、3 本书（Sanderson, [2008](#bib.bib184); Tong, [2001](#bib.bib222); Farahani
    和 Hekmatfar, [2009](#bib.bib63)）、1 篇研究报告（Settles, [2009](#bib.bib194)）和 1 篇学位论文（Zhu
    et al., [2005](#bib.bib261)）。还有 28 篇未发表的论文，以及一个正式的分类方法。本调查的贡献总结如下：
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As far as we know, this is the first comprehensive review work in the field
    of deep active learning.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这是深度主动学习领域的首个全面综述工作。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We analyze the challenges of combining active learning and deep learning, and
    systematically summarize and categorize existing DeepAL-related work for these
    challenges.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们分析了结合主动学习和深度学习的挑战，并对这些挑战的DeepAL相关工作进行了系统总结和分类。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct a comprehensive and detailed analysis of DeepAL-related applications
    in various fields and future directions.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对DeepAL相关应用在各个领域的情况以及未来方向进行了全面而详细的分析。
- en: Next, we first briefly review the development status of DL and AL in their respective
    fields. Subsequently, in Section [2](#S2 "2\. The necessity and challenge of combining
    DL and AL ‣ A Survey of Deep Active Learning"), the necessity and challenges of
    combining DL and AL are explicated. In Section [3](#S3 "3\. Deep Active Learning
    ‣ A Survey of Deep Active Learning"), we conduct a comprehensive and systematic
    summary and discussion of the various strategies used in DeepAL. In Section [4](#S4
    "4\. Application of DeepAL in fields such as vision and NLP ‣ A Survey of Deep
    Active Learning"), we review various applications of DeepAL in detail. In Section
    [5](#S5 "5\. Discussion and future directions ‣ A Survey of Deep Active Learning"),
    we conduct a comprehensive discussion on the future direction of DeepAL. Finally,
    in Section [6](#S6 "6\. Summary and conclusions ‣ A Survey of Deep Active Learning"),
    we make a summary and conclusion of this survey.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们首先简要回顾深度学习（DL）和主动学习（AL）在各自领域的发展现状。随后，在第[2](#S2 "2\. 结合DL和AL的必要性和挑战 ‣ 深度主动学习的调查")节中，我们阐述了结合DL和AL的必要性和挑战。在第[3](#S3
    "3\. 深度主动学习 ‣ 深度主动学习的调查")节中，我们对深度主动学习（DeepAL）中使用的各种策略进行了全面和系统的总结和讨论。在第[4](#S4
    "4\. DeepAL在视觉和NLP等领域的应用 ‣ 深度主动学习的调查")节中，我们详细回顾了DeepAL的各种应用。在第[5](#S5 "5\. 讨论和未来方向
    ‣ 深度主动学习的调查")节中，我们对DeepAL的未来方向进行了全面讨论。最后，在第[6](#S6 "6\. 总结与结论 ‣ 深度主动学习的调查")节中，我们对本次调查进行了总结和结论。
- en: 1.1\. Deep Learning
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 深度学习
- en: 'DL attempts to build appropriate models by simulating the structure of the
    human brain. The McCulloch-Pitts (MCP) model proposed in 1943 by (Fitch, [1944](#bib.bib66))
    is regarded as the beginning of modern DL. Subsequently, in 1986, (Rumelhart et al.,
    [1986b](#bib.bib181)) introduced backpropagation into the optimization of neural
    networks, which laid the foundation for the subsequent rapid development of DL.
    In the same year, Recurrent Neural Networks (RNNs) (Jordan, [1986](#bib.bib106))
    were first proposed. In 1998, the LeNet (LeCun et al., [1998](#bib.bib129)) network
    made its first appearance, representing one of the earliest uses of deep neural
    networks (DNN). However, these pioneering early works were limited by the computing
    resources available at the time and did not receive as much attention and investigation
    as they should have (Lecun et al., [2015](#bib.bib127)). In 2006, Deep Belief
    Networks (DBNs) (Hinton et al., [2006](#bib.bib92)) were proposed and used to
    explore a deeper range of networks, which prompted the name of neural networks
    as DL. AlexNet (Krizhevsky et al., [2012](#bib.bib121)) is considered the first
    CNN deep learning model, which greatly improves the image classification results
    on large-scale data sets (such as ImageNet). In the ImageNet Large-Scale Visual
    Recognition Challenge (ILSVRC)-2012 competition (Deng et al., [2012](#bib.bib50)),
    the AlexNet (Krizhevsky et al., [2012](#bib.bib121)) won the championship in the
    top-5 test error rate by nearly 10% ahead of the second place. AlexNet uses the
    ReLUs (Rectified Linear Units) (Nair and Hinton, [2010](#bib.bib151)) activation
    function to effectively suppress the gradient disappearance problem, while the
    use of multiple GPUs greatly improves the training speed of the model. Subsequently,
    DL began to win championships in various competitions and demonstrated very competitive
    results in many fields, such as visual data processing, natural language processing,
    speech processing, and many other well-known applications (Yan et al., [2017](#bib.bib241),
    [2015](#bib.bib242)). From the perspective of automation, the emergence of DL
    has transformed the manual design of features (Dalal and Triggs, [2005](#bib.bib43);
    Lowe, [1999](#bib.bib140)) in machine learning to facilitate automatic extraction
    (Simonyan and Zisserman, [2015](#bib.bib204); He et al., [2016](#bib.bib88)).
    It is precisely because of this powerful automatic feature extraction capability
    that DL has demonstrated such unprecedented advantages in many fields. After decades
    of development, the research work related to DL is very rich. In Fig.[1a](#S1.F1.sf1
    "In Figure 1 ‣ 1.1\. Deep Learning ‣ 1\. Introduction ‣ A Survey of Deep Active
    Learning"), we present a standard deep learning model example: convolutional neural
    network (CNN) (Rumelhart et al., [1986a](#bib.bib180); LeCun et al., [1989](#bib.bib128)).
    Based on this approach, similar CNNs are applied to various image processing tasks.
    In addition, RNNs and GANs (Generative Adversarial Networks) (Salvaris et al.,
    [2018](#bib.bib183)) are also widely utilized. Beginning in 2017, DL gradually
    shifted from the initial feature extraction automation to the automation of model
    architecture design (Zoph and Le, [2017](#bib.bib263); Ren et al., [2020](#bib.bib175);
    Baker et al., [2017](#bib.bib17)); however, this still has a long way to go.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）试图通过模拟人脑的结构来构建适当的模型。1943年，由(Fitch, [1944](#bib.bib66))提出的McCulloch-Pitts
    (MCP) 模型被视为现代深度学习的起点。随后，在1986年，(Rumelhart et al., [1986b](#bib.bib181))将反向传播引入神经网络优化，为深度学习的快速发展奠定了基础。同年，递归神经网络（RNNs）（Jordan,
    [1986](#bib.bib106)）首次被提出。1998年，LeNet (LeCun et al., [1998](#bib.bib129)) 网络首次出现，代表了深度神经网络（DNN）的早期应用之一。然而，这些开创性的早期工作受到当时计算资源的限制，未能获得应有的关注和研究（Lecun
    et al., [2015](#bib.bib127)）。2006年，深度置信网络（DBNs）（Hinton et al., [2006](#bib.bib92)）被提出，用于探索更深层次的网络，这促使了神经网络被称为深度学习。AlexNet
    (Krizhevsky et al., [2012](#bib.bib121)) 被认为是第一个CNN深度学习模型，它在大规模数据集（如ImageNet）上的图像分类结果得到了极大的提升。在ImageNet大规模视觉识别挑战赛（ILSVRC）-2012年比赛（Deng
    et al., [2012](#bib.bib50)）中，AlexNet (Krizhevsky et al., [2012](#bib.bib121))
    凭借近10%的优势赢得了前五名测试误差率的冠军。AlexNet利用ReLUs（Rectified Linear Units）（Nair and Hinton,
    [2010](#bib.bib151)）激活函数有效抑制了梯度消失问题，而使用多GPU则大大提高了模型的训练速度。随后，深度学习开始在各种竞赛中获胜，并在视觉数据处理、自然语言处理、语音处理等许多领域展示了非常有竞争力的结果（Yan
    et al., [2017](#bib.bib241), [2015](#bib.bib242)）。从自动化的角度来看，深度学习的出现将机器学习中的特征手动设计（Dalal
    and Triggs, [2005](#bib.bib43); Lowe, [1999](#bib.bib140)）转变为自动提取（Simonyan and
    Zisserman, [2015](#bib.bib204); He et al., [2016](#bib.bib88)）。正是因为这种强大的自动特征提取能力，深度学习在许多领域展示了前所未有的优势。经过几十年的发展，关于深度学习的研究工作非常丰富。在图[1a](#S1.F1.sf1
    "In Figure 1 ‣ 1.1\. Deep Learning ‣ 1\. Introduction ‣ A Survey of Deep Active
    Learning")中，我们展示了一个标准的深度学习模型示例：卷积神经网络（CNN）（Rumelhart et al., [1986a](#bib.bib180);
    LeCun et al., [1989](#bib.bib128)）。基于这种方法，类似的CNN被应用于各种图像处理任务。此外，RNNs和GANs（生成对抗网络）（Salvaris
    et al., [2018](#bib.bib183)）也被广泛利用。从2017年开始，深度学习逐渐从最初的特征提取自动化转向模型架构设计的自动化（Zoph
    and Le, [2017](#bib.bib263); Ren et al., [2020](#bib.bib175); Baker et al., [2017](#bib.bib17)）；然而，这仍然还有很长的路要走。
- en: '![Refer to caption](img/3287949765a8d45a027e5b6c56e59c64.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3287949765a8d45a027e5b6c56e59c64.png)'
- en: (a) Structure diagram of convolutional neural network.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 卷积神经网络的结构图。
- en: '![Refer to caption](img/0ac6209814d2e314894fe8f640a139fb.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0ac6209814d2e314894fe8f640a139fb.png)'
- en: (b) The pool-based active learning cycle.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基于池的主动学习循环。
- en: '![Refer to caption](img/6e8d0011f81438d627102eebe339f40b.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6e8d0011f81438d627102eebe339f40b.png)'
- en: (c) A typical example of deep active learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 深度主动学习的一个典型示例。
- en: 'Figure 1\. Comparison of typical architectures of DL, AL, and DeepAL. (a) A
    common DL model: Convolutional Neural Network. (b)The pool-based AL cycle: Use
    the query strategy to query the sample in the unlabeled pool $U$ and hand it over
    to the oracle for labeling, then add the queried sample to the labeled training
    dataset $L$ and train, and then use the newly learned knowledge for the next round
    of querying. Repeat this process until the label budget is exhausted or the pre-defined
    termination conditions are reached. (c) A typical example of DeepAL: The parameters
    $\theta$ of the DL model are initialized or pre-trained on the labeled training
    set $L_{0}$, and the samples of the unlabeled pool $U$ are used to extract features
    through the DL model. Select samples based on the corresponding query strategy,
    and query the label in querying to form a new label training set $L$, then train
    the DL model on $L$, and update $U$ at the same time. Repeat this process until
    the label budget is exhausted or the pre-defined termination conditions are reached
    (see Section [3.4](#S3.SS4 "3.4\. DeepAL Stopping Strategy ‣ 3\. Deep Active Learning
    ‣ A Survey of Deep Active Learning") for stopping strategy details).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 对比典型的 DL、AL 和 DeepAL 架构。 (a) 一个常见的 DL 模型：卷积神经网络。 (b) 基于池的 AL 循环：使用查询策略从未标记池
    $U$ 中查询样本，并将其交给 Oracle 进行标记，然后将查询的样本添加到标记训练数据集 $L$ 中进行训练，再用新学到的知识进行下一轮查询。重复这个过程直到标记预算耗尽或达到预定义的终止条件。
    (c) DeepAL 的一个典型示例：DL 模型的参数 $\theta$ 在标记训练集 $L_{0}$ 上初始化或预训练，利用未标记池 $U$ 的样本通过
    DL 模型提取特征。根据相应的查询策略选择样本，在查询中查询标签以形成新的标记训练集 $L$，然后在 $L$ 上训练 DL 模型，同时更新 $U$。重复这个过程直到标记预算耗尽或达到预定义的终止条件（有关停止策略的详细信息，请参见第
    [3.4](#S3.SS4 "3.4\. DeepAL Stopping Strategy ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning") 节）。
- en: Thanks to the publication of a large number of existing annotation datasets
    (Bengio et al., [2006](#bib.bib22); Krizhevsky et al., [2012](#bib.bib121)), in
    recent years, DL has made breakthroughs in various fields including machine translation
    (Aharoni et al., [2019](#bib.bib6); Tan et al., [2019a](#bib.bib218); Wang et al.,
    [2019b](#bib.bib232); Bau et al., [2019](#bib.bib19)), speech recognition (Park
    et al., [2019](#bib.bib160); Nassif et al., [2019](#bib.bib153); Qin et al., [2019](#bib.bib170);
    Schneider et al., [2019](#bib.bib190)), and image classification (He et al., [2019b](#bib.bib90);
    Mateen et al., [2019](#bib.bib145); Paoletti et al., [2019](#bib.bib159); Yalniz
    et al., [2019](#bib.bib240)). However, this comes at the cost of a large number
    of manually labeled datasets, and DL has a strong greedy attribute to the data.
    While, in the real world, obtaining a large number of unlabeled datasets is relatively
    simple, the manual labeling of datasets comes at a high cost; this is particularly
    true for those fields where labeling requires a high degree of professional knowledge
    (Hoi et al., [2006](#bib.bib95); Smith et al., [2018](#bib.bib208)). For example,
    the labeling and description of lung lesion images of COVID-19 patients requires
    experienced clinicians to complete, and it is clearly impractical to demand that
    such professionals complete a large amount of medical image labeling. Similar
    fields also include speech recognition (Zhu et al., [2005](#bib.bib261); Abdel-Wahab
    and Busso, [2019](#bib.bib2)), medical imaging (Hoi et al., [2006](#bib.bib95);
    Nam et al., [2019](#bib.bib152); Lee and Paeng, [2018](#bib.bib130); Yang et al.,
    [2017](#bib.bib244)), recommender systems (Adomavicius and Tuzhilin, [2005](#bib.bib3);
    Cheng et al., [2019](#bib.bib38)), information extraction (Bhattacharjee et al.,
    [2017](#bib.bib23)), satellite remote sensing (Liu et al., [2017](#bib.bib136))
    and robotics (Calinon et al., [2007](#bib.bib33); Andersson et al., [2017](#bib.bib9);
    Takahashi et al., [2017](#bib.bib217); Zhou and Schoellig, [2019](#bib.bib259);
    Burka and Kuchenbecker, [2017](#bib.bib32)), machine translation (Bloodgood and
    Callison-Burch, [2014](#bib.bib26); Platanios et al., [2019](#bib.bib165)) and
    text classification (Zhang et al., [2017b](#bib.bib252); Schröder and Niekler,
    [2020](#bib.bib191)), etc. Therefore, a way of maximizing the performance gain
    of the model when annotating a small number of samples is urgently required.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大量现有注释数据集的发布（Bengio等，[2006](#bib.bib22)；Krizhevsky等，[2012](#bib.bib121)），近年来，深度学习（DL）在包括机器翻译（Aharoni等，[2019](#bib.bib6)；Tan等，[2019a](#bib.bib218)；Wang等，[2019b](#bib.bib232)；Bau等，[2019](#bib.bib19)），语音识别（Park等，[2019](#bib.bib160)；Nassif等，[2019](#bib.bib153)；Qin等，[2019](#bib.bib170)；Schneider等，[2019](#bib.bib190)）和图像分类（He等，[2019b](#bib.bib90)；Mateen等，[2019](#bib.bib145)；Paoletti等，[2019](#bib.bib159)；Yalniz等，[2019](#bib.bib240)）等领域取得了突破。然而，这需要大量手动标注的数据集，而深度学习对数据有很强的贪婪属性。尽管在现实世界中，获取大量未标记的数据集相对简单，但手动标注数据集的成本很高；尤其是在那些标注需要高度专业知识的领域（Hoi等，[2006](#bib.bib95)；Smith等，[2018](#bib.bib208)）中尤为如此。例如，COVID-19患者肺部病变图像的标注和描述需要经验丰富的临床医生完成，要求这些专业人员完成大量医学图像标注显然不切实际。类似的领域还包括语音识别（Zhu等，[2005](#bib.bib261)；Abdel-Wahab和Busso，[2019](#bib.bib2)），医学成像（Hoi等，[2006](#bib.bib95)；Nam等，[2019](#bib.bib152)；Lee和Paeng，[2018](#bib.bib130)；Yang等，[2017](#bib.bib244)），推荐系统（Adomavicius和Tuzhilin，[2005](#bib.bib3)；Cheng等，[2019](#bib.bib38)），信息提取（Bhattacharjee等，[2017](#bib.bib23)），卫星遥感（Liu等，[2017](#bib.bib136)）和机器人技术（Calinon等，[2007](#bib.bib33)；Andersson等，[2017](#bib.bib9)；Takahashi等，[2017](#bib.bib217)；Zhou和Schoellig，[2019](#bib.bib259)；Burka和Kuchenbecker，[2017](#bib.bib32)），机器翻译（Bloodgood和Callison-Burch，[2014](#bib.bib26)；Platanios等，[2019](#bib.bib165)）以及文本分类（Zhang等，[2017b](#bib.bib252)；Schröder和Niekler，[2020](#bib.bib191)）等。因此，急需一种方法来最大化模型在标注少量样本时的性能提升。
- en: 1.2\. Active Learning
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 主动学习
- en: AL is just such a method dedicated to studying how to obtain as many performance
    gains as possible by labeling as few samples as possible. More specifically, it
    aims to select the most useful samples from the unlabeled dataset and hand it
    over to the oracle (e.g., human annotator) for labeling, to reduce the cost of
    labeling as much as possible while still maintaining performance. AL approaches
    can be divided into membership query synthesis (Angluin, [1988](#bib.bib11); King
    et al., [2004](#bib.bib114)), stream-based selective sampling (Dagan and Engelson,
    [1995](#bib.bib42); Krishnamurthy, [2002](#bib.bib119)) and pool-based (Lewis
    and Gale, [1994](#bib.bib132)) AL from application scenarios (Settles, [2009](#bib.bib194)).
    Membership query synthesis means that the learner can request to query the label
    of any unlabeled sample in the input space, including the sample generated by
    the learner. Moreover, the key difference between stream-based selective sampling
    and pool-based sampling is that the former makes an independent judgment on whether
    each sample in the data stream needs to query the labels of unlabeled samples,
    while the latter chooses the best query sample based on the evaluation and ranking
    of the entire dataset. Related research on stream-based selective sampling is
    mainly aimed at the application scenarios of small mobile devices that require
    timeliness, because these small devices often have limited storage and computing
    capabilities. The more common pool-based sampling strategy in the paper related
    to AL research is more suitable for large devices with sufficient computing and
    storage resources. In Fig.[1b](#S1.F1.sf2 "In Figure 1 ‣ 1.1\. Deep Learning ‣
    1\. Introduction ‣ A Survey of Deep Active Learning"), we illustrate the framework
    diagram of the pool-based active learning cycle. In the initial state, we can
    randomly select one or more samples from the unlabeled pool $U$, give this sample
    to the oracle query label to get the labeled dataset $L$, and then train the model
    on $L$ using supervised learning. Next, we use this new knowledge to select the
    next sample to be queried, add the newly queried sample to $L$, and then conduct
    training. This process is repeated until the label budget is exhausted or the
    pre-defined termination conditions are reached (see Section [3.4](#S3.SS4 "3.4\.
    DeepAL Stopping Strategy ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active
    Learning") for stopping strategy details).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: AL 就是这样一种方法，专注于通过标记尽可能少的样本来获得尽可能多的性能提升。更具体地说，它旨在从未标记的数据集中选择最有用的样本，并交给 oracle（例如，人类标注员）进行标记，以尽可能降低标记成本，同时保持性能。AL
    方法可以分为成员查询合成（Angluin，[1988](#bib.bib11)；King 等，[2004](#bib.bib114)），基于流的选择性采样（Dagan
    和 Engelson，[1995](#bib.bib42)；Krishnamurthy，[2002](#bib.bib119)）和基于池的（Lewis 和
    Gale，[1994](#bib.bib132)）AL 根据应用场景（Settles，[2009](#bib.bib194)）。成员查询合成意味着学习者可以请求查询输入空间中任何未标记样本的标签，包括学习者生成的样本。此外，基于流的选择性采样与基于池的采样之间的关键区别在于，前者对数据流中每个样本是否需要查询未标记样本的标签进行独立判断，而后者则根据对整个数据集的评估和排名选择最佳查询样本。与基于流的选择性采样相关的研究主要针对需要及时性的较小移动设备，因为这些小型设备通常具有有限的存储和计算能力。论文中与
    AL 研究相关的更常见的基于池的采样策略更适用于具有足够计算和存储资源的大型设备。在图[1b](#S1.F1.sf2 "在图 1 ‣ 1.1. 深度学习 ‣
    1. 引言 ‣ 深度主动学习的调查")中，我们展示了基于池的主动学习循环的框架图。在初始状态下，我们可以从未标记的池 $U$ 中随机选择一个或多个样本，将这些样本交给
    oracle 查询标签，以获得标记数据集 $L$，然后在 $L$ 上使用监督学习训练模型。接下来，我们利用这些新知识选择下一个待查询的样本，将新查询的样本添加到
    $L$ 中，然后进行训练。这个过程会重复进行，直到标签预算用尽或达到预定义的终止条件（有关停止策略的详细信息，请参见第 [3.4](#S3.SS4 "3.4.
    DeepAL 停止策略 ‣ 3. 深度主动学习 ‣ 深度主动学习的调查") 节）。
- en: It is different from DL by using manual or automatic methods to design models
    with high-performance feature extraction capabilities. AL starts with datasets,
    primarily through the design of elaborate query rules to select the best samples
    from unlabeled datasets and query their labels, in an attempt to reduce the labeling
    cost to the greatest extent possible. Therefore, the design of query rules is
    crucial to the performance of AL methods. Related research is also quite rich.
    For example, in a given set of unlabeled datasets, the main query strategies include
    the uncertainty-based approach (Lewis and Gale, [1994](#bib.bib132); Joshi et al.,
    [2009](#bib.bib107); Ranganathan et al., [2017](#bib.bib174); Tong and Koller,
    [2002](#bib.bib223); Seung et al., [1992](#bib.bib197); Beluch et al., [2018](#bib.bib20)),
    diversity-based approach (Bilgic and Getoor, [2009](#bib.bib24); Guo, [2010](#bib.bib85);
    Nguyen and Smeulders, [2004](#bib.bib154); Gal et al., [2017](#bib.bib73)) and
    expected model change (Freytag et al., [2014](#bib.bib70); Roy and McCallum, [2001](#bib.bib178);
    Settles et al., [2007](#bib.bib196)). In addition, many works have also studied
    hybrid query strategies (Shui et al., [2020](#bib.bib201); Yin et al., [2017](#bib.bib245);
    Ash et al., [2020](#bib.bib15); Zhdanov, [2019](#bib.bib256)), taking into account
    the uncertainty and diversity of query samples, and attempting to find a balance
    between these two strategies. Because separate sampling based on uncertainty often
    results in sampling bias (Dasgupta, [2011](#bib.bib45); Bloodgood and Vijay-Shanker,
    [2009](#bib.bib27)), the currently selected sample is not representative of the
    distribution of unlabeled datasets. On the other hand, considering only strategies
    that promote diversity in sampling may lead to increased labeling costs, as may
    be a considerable number of samples with low information content will consequently
    be selected. More classic query strategies are examined in (Settles, [2012](#bib.bib195)).
    Although there is a substantial body of existing AL-related research, AL still
    faces the problem of expanding to high-dimensional data (e.g., images, text, and
    video, etc.) (Tong, [2001](#bib.bib222)); thus, most AL works tend to concentrate
    on low-dimensional problems (Tong, [2001](#bib.bib222); Hernández-Lobato and Adams,
    [2015](#bib.bib91)). In addition, AL often queries high-value samples based on
    features extracted in advance and does not have the ability to extract features.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与DL不同，AL通过手动或自动方法设计具有高性能特征提取能力的模型。AL从数据集开始，主要通过设计精细的查询规则来从未标记的数据集中选择最佳样本并查询其标签，以尽可能减少标记成本。因此，查询规则的设计对AL方法的性能至关重要。相关研究也相当丰富。例如，在给定的未标记数据集上，主要的查询策略包括不确定性基础方法（Lewis和Gale，[1994](#bib.bib132)；Joshi等，[2009](#bib.bib107)；Ranganathan等，[2017](#bib.bib174)；Tong和Koller，[2002](#bib.bib223)；Seung等，[1992](#bib.bib197)；Beluch等，[2018](#bib.bib20)），多样性基础方法（Bilgic和Getoor，[2009](#bib.bib24)；Guo，[2010](#bib.bib85)；Nguyen和Smeulders，[2004](#bib.bib154)；Gal等，[2017](#bib.bib73)）和期望模型变化（Freytag等，[2014](#bib.bib70)；Roy和McCallum，[2001](#bib.bib178)；Settles等，[2007](#bib.bib196)）。此外，许多工作也研究了混合查询策略（Shui等，[2020](#bib.bib201)；Yin等，[2017](#bib.bib245)；Ash等，[2020](#bib.bib15)；Zhdanov，[2019](#bib.bib256)），考虑了查询样本的不确定性和多样性，并试图在这两种策略之间找到平衡。由于基于不确定性的独立采样通常会导致采样偏差（Dasgupta，[2011](#bib.bib45)；Bloodgood和Vijay-Shanker，[2009](#bib.bib27)），当前选择的样本可能不具有代表性。另一方面，仅考虑促进采样多样性的策略可能会导致标记成本增加，因为可能会选择到大量信息内容较低的样本。更多经典查询策略在（Settles，[2012](#bib.bib195)）中进行了审查。尽管现有的AL相关研究已经相当丰富，但AL仍面临扩展到高维数据（如图像、文本和视频等）的难题（Tong，[2001](#bib.bib222)）；因此，大多数AL工作倾向于集中在低维问题上（Tong，[2001](#bib.bib222)；Hernández-Lobato和Adams，[2015](#bib.bib91)）。此外，AL通常基于提前提取的特征查询高价值样本，但不具备提取特征的能力。
- en: 2\. The necessity and challenge of combining DL and AL
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 结合DL和AL的必要性与挑战
- en: 'DL has a strong learning capability in the context of high-dimensional data
    processing and automatic feature extraction, while AL has significant potential
    to effectively reduce labeling costs. Therefore, an obvious approach is to combine
    DL and AL, as this will greatly expand their application potential. This combined
    approach, referred to as DeepAL, was proposed by considering the complementary
    advantages of the two methods, and researchers have high expectations for the
    results of studies in this field. However, although AL-related research on query
    strategy is quite rich, it is still quite difficult to apply this strategy directly
    to DL. This is mainly due to:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在高维数据处理和自动特征提取方面具有强大的学习能力，而主动学习在有效降低标注成本方面具有显著潜力。因此，显而易见的方法是将深度学习和主动学习结合起来，这将大大扩展它们的应用潜力。这种结合方法，称为DeepAL，是在考虑两种方法的互补优势的基础上提出的，研究人员对这一领域的研究结果寄予厚望。然而，尽管主动学习相关的查询策略研究相当丰富，但将这一策略直接应用于深度学习仍然相当困难。这主要是因为：
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model uncertainty in Deep Learning. The query strategy based on uncertainty
    is an important direction of AL research. In classification tasks, although DL
    can use the softmax layer to obtain the probability distribution of the label,
    the facts show that they are too confident. The SR (Softmax Response) (Wang et al.,
    [2017](#bib.bib230)) of the final output is unreliable as a measure of confidence,
    and the performance of this method will thus be even worse than that of random
    sampling (Wang and Shang, [2014](#bib.bib228)).
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在深度学习中建模不确定性。基于不确定性的查询策略是主动学习（AL）研究的一个重要方向。在分类任务中，尽管深度学习（DL）可以通过softmax层获得标签的概率分布，但事实证明它们过于自信。最终输出的SR（Softmax
    Response）（Wang et al., [2017](#bib.bib230)）作为信心度量是不可靠的，因此这种方法的性能甚至会比随机采样（Wang
    and Shang, [2014](#bib.bib228)）更差。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Insufficient data for labeled samples. AL often relies on a small amount of
    labeled sample data to learn and update the model, while DL is often very greedy
    for data (Hinton et al., [2012](#bib.bib93)). The labeled training samples provided
    by the classic AL method thus insufficient to support the training of traditional
    DL. In addition, the one-by-one sample query method commonly used in AL is also
    not applicable in the DL context (Zhdanov, [2019](#bib.bib256)).
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标记样本的数据不足。主动学习（AL）通常依赖少量标记样本数据来学习和更新模型，而深度学习（DL）则往往对数据非常贪婪（Hinton et al., [2012](#bib.bib93)）。经典主动学习方法提供的标记训练样本因此不足以支持传统深度学习的训练。此外，主动学习中常用的逐个样本查询方法在深度学习背景下也不适用（Zhdanov,
    [2019](#bib.bib256)）。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Processing pipeline inconsistency. The processing pipelines of AL and DL are
    inconsistent. Most AL algorithms focus primarily on the training of classifiers,
    and the various query strategies utilized are largely based on fixed feature representations.
    In DL, however, feature learning and classifier training are jointly optimized.
    Only fine-tuning the DL models in the AL framework, or treating them as two separate
    problems, may thus cause divergent issues (Wang et al., [2017](#bib.bib230)).
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 处理流程不一致。主动学习和深度学习的处理流程不一致。大多数主动学习算法主要集中于分类器的训练，使用的各种查询策略大多基于固定的特征表示。而在深度学习中，特征学习和分类器训练是联合优化的。因此，仅仅在主动学习框架中微调深度学习模型，或将它们视为两个独立的问题，可能会导致问题的分歧（Wang
    et al., [2017](#bib.bib230)）。
- en: To address the first problem, some researchers have applied Bayesian deep learning
    (Gal and Ghahramani, [2015](#bib.bib71)) to deal with the high-dimensional mini-batch
    samples with fewer queries in the AL context (Gal et al., [2017](#bib.bib73);
    Pop and Fulop, [2018](#bib.bib166); Kirsch et al., [2019](#bib.bib116); Tran et al.,
    [2019](#bib.bib224)), thereby effectively alleviating the problem of the DL model
    being too confident about the output results. To solve the problem of insufficient
    labelled sample data, researchers have considered using generative networks for
    data augmentation (Tran et al., [2019](#bib.bib224)) or assigning pseudo-labels
    to high-confidence samples to expand the labeled training set (Wang et al., [2017](#bib.bib230)).
    Some researchers have also used labeled and unlabeled datasets to combine supervised
    and semisupervised training across AL cycles (Simeoni et al., [2019](#bib.bib203);
    Hossain and Roy, [2019](#bib.bib97)). In addition, the empirical research in (Sener
    and Savarese, [2018](#bib.bib193)) shows that the previous heuristic-based AL
    (Settles, [2009](#bib.bib194)) query strategy is invalid when it is applied to
    DL in batch settings; therefore, for the one-by-one query strategy in classic
    AL, many researchers focus on the improvement of the batch sample query strategy
    (Kirsch et al., [2019](#bib.bib116); Zhdanov, [2019](#bib.bib256); Gissin and
    Shalev-Shwartz, [2019](#bib.bib80); Ash et al., [2020](#bib.bib15)), taking both
    the amount of information and the diversity of batch samples into account. Furthermore,
    to deal with the pipeline inconsistency problem, researchers have considered modifying
    the combined framework of AL and DL to make the proposed DeepAL model as general
    as possible, an approach that can be extended to various application fields. This
    is of great significance to the promotion of DeepAL. For example, (Yoo and Kweon,
    [2019](#bib.bib246)) embeds the idea of AL into DL and consequently proposes a
    task-independent architecture design.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个问题，一些研究人员将贝叶斯深度学习（Gal 和 Ghahramani，[2015](#bib.bib71)）应用于处理主动学习背景下的高维小批量样本，并且查询次数较少的情况（Gal
    等，[2017](#bib.bib73)；Pop 和 Fulop，[2018](#bib.bib166)；Kirsch 等，[2019](#bib.bib116)；Tran
    等，[2019](#bib.bib224)），从而有效缓解了深度学习模型对输出结果过于自信的问题。为了解决标签样本数据不足的问题，研究人员考虑使用生成对抗网络进行数据增强（Tran
    等，[2019](#bib.bib224)）或为高置信度样本分配伪标签以扩展标记训练集（Wang 等，[2017](#bib.bib230)）。一些研究人员还利用标记和未标记的数据集在主动学习周期中结合监督学习和半监督学习（Simeoni
    等，[2019](#bib.bib203)；Hossain 和 Roy，[2019](#bib.bib97)）。此外，(Sener 和 Savarese，[2018](#bib.bib193))
    的实证研究表明，以前基于启发式的主动学习（Settles，[2009](#bib.bib194)）查询策略在批量设置中应用于深度学习时无效；因此，对于经典主动学习中的逐一查询策略，许多研究人员将重点放在批量样本查询策略的改进上（Kirsch
    等，[2019](#bib.bib116)；Zhdanov，[2019](#bib.bib256)；Gissin 和 Shalev-Shwartz，[2019](#bib.bib80)；Ash
    等，[2020](#bib.bib15)），考虑了信息量和批量样本的多样性。此外，为了解决管道不一致问题，研究人员考虑修改主动学习和深度学习的联合框架，使提出的
    DeepAL 模型尽可能通用，这一方法可以扩展到各种应用领域。这对推动 DeepAL 的应用具有重要意义。例如，(Yoo 和 Kweon，[2019](#bib.bib246))
    将主动学习的理念嵌入到深度学习中，从而提出了一种任务无关的架构设计。
- en: 3\. Deep Active Learning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 深度主动学习
- en: 'In this section, we will provide a comprehensive and systematic overview of
    DeepAL-related works. Fig.[1c](#S1.F1.sf3 "In Figure 1 ‣ 1.1\. Deep Learning ‣
    1\. Introduction ‣ A Survey of Deep Active Learning") illustrates a typical example
    of DeepAL model architecture. The parameters $\theta$ of the deep learning model
    are initialized or pre-trained on the labeled training set $L_{0}$, while the
    samples of the unlabeled pool $U$ are used to extract features through the deep
    learning model. The next steps are to select samples based on the corresponding
    query strategy, and query the label in the oracle to form a new label training
    set $L$, then train the deep learning model on $L$ and update $U$ at the same
    time. This process is repeated until the label budget is exhausted or the predefined
    termination conditions are reached (see Section [3.4](#S3.SS4 "3.4\. DeepAL Stopping
    Strategy ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning") for stopping
    strategy details). From the DeepAL framework example in Fig.[1c](#S1.F1.sf3 "In
    Figure 1 ‣ 1.1\. Deep Learning ‣ 1\. Introduction ‣ A Survey of Deep Active Learning"),
    we can roughly divide the DeepAL framework into two parts: namely, the AL query
    strategy on the unlabeled dataset and the DL model training method. These will
    be discussed and summarized in the following Section [3.1](#S3.SS1 "3.1\. Query
    Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep
    Active Learning") and [3.2](#S3.SS2 "3.2\. Data Expansion of Labeled Samples in
    DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning") respectively.
    Next, we will discuss the efforts made by DeepAL on the generalization of the
    model in Section [3.3](#S3.SS3 "3.3\. DeepAL Generic Framework ‣ 3\. Deep Active
    Learning ‣ A Survey of Deep Active Learning"). Finally, we briefly discuss the
    stopping strategy in DeepAL in Section [3.4](#S3.SS4 "3.4\. DeepAL Stopping Strategy
    ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将提供关于DeepAL相关工作的全面系统的概述。图[1c](#S1.F1.sf3 "在图1 ‣ 1.1\. 深度学习 ‣ 1\. 介绍 ‣
    深度主动学习概述")展示了DeepAL模型架构的一个典型示例。深度学习模型的参数$\theta$在标记训练集$L_{0}$上初始化或预训练，同时未标记池$U$的样本用于通过深度学习模型提取特征。接下来的步骤是基于相应的查询策略选择样本，并在oracle中查询标签以形成新的标签训练集$L$，然后在$L$上训练深度学习模型，并同时更新$U$。这个过程会重复进行，直到标签预算耗尽或达到预定义的终止条件（有关终止策略的详细信息，请参见第[3.4节](#S3.SS4
    "3.4\. DeepAL终止策略 ‣ 3\. 深度主动学习 ‣ 深度主动学习概述")）。从图[1c](#S1.F1.sf3 "在图1 ‣ 1.1\. 深度学习
    ‣ 1\. 介绍 ‣ 深度主动学习概述")中的DeepAL框架示例，我们可以大致将DeepAL框架分为两个部分：即未标记数据集上的AL查询策略和DL模型训练方法。这些将在接下来的第[3.1节](#S3.SS1
    "3.1\. DeepAL中的查询策略优化 ‣ 3\. 深度主动学习 ‣ 深度主动学习概述")和第[3.2节](#S3.SS2 "3.2\. DeepAL中的标记样本数据扩展
    ‣ 3\. 深度主动学习 ‣ 深度主动学习概述")中讨论和总结。接下来，我们将在第[3.3节](#S3.SS3 "3.3\. DeepAL通用框架 ‣ 3\.
    深度主动学习 ‣ 深度主动学习概述")中讨论DeepAL在模型泛化方面的努力。最后，我们将在第[3.4节](#S3.SS4 "3.4\. DeepAL终止策略
    ‣ 3\. 深度主动学习 ‣ 深度主动学习概述")中简要讨论DeepAL中的终止策略。
- en: 3.1\. Query Strategy Optimization in DeepAL
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. DeepAL中的查询策略优化
- en: 'In the pool-based method, we define $U^{n}=\{\mathcal{X},\mathcal{Y}\}$ as
    an unlabeled dataset with $n$ samples; here, $\mathcal{X}$ is the sample space,
    $\mathcal{Y}$ is the label space, and $P(x,y)$ is a potential distribution, where
    $x\in\mathcal{X},y\in\mathcal{Y}$. $L^{m}=\{X,Y\}$ is the current labeled training
    set with $m$ samples, where $\mathrm{x}\in X,\mathrm{y}\in Y$. Under the standard
    supervision environment of DeepAL, our main goal is to design a query strategy
    $Q$, $U^{n}\stackrel{{\scriptstyle Q}}{{\longrightarrow}}L^{m}$, using the deep
    model $f\in\mathcal{F},f:\mathcal{X}\rightarrow\mathcal{Y}$. The optimization
    problem of DeepAL in a supervised environment can be expressed as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于池的方法中，我们将$U^{n}=\{\mathcal{X},\mathcal{Y}\}$定义为一个具有$n$个样本的未标记数据集；其中，$\mathcal{X}$是样本空间，$\mathcal{Y}$是标签空间，$P(x,y)$是潜在分布，其中$x\in\mathcal{X},y\in\mathcal{Y}$。$L^{m}=\{X,Y\}$是当前带标签的训练集，具有$m$个样本，其中$\mathrm{x}\in
    X,\mathrm{y}\in Y$。在DeepAL的标准监督环境下，我们的主要目标是设计一个查询策略$Q$，使得$U^{n}\stackrel{{\scriptstyle
    Q}}{{\longrightarrow}}L^{m}$，利用深度模型$f\in\mathcal{F},f:\mathcal{X}\rightarrow\mathcal{Y}$。DeepAL在监督环境中的优化问题可以表示为：
- en: '| (1) |  | $\mathop{\arg\min}_{L^{m}\subseteq U^{n},(\mathrm{x,y})\in L^{m},(x,y)\in
    U^{n}}\mathbb{E}_{(x,y)}[\ell(f(\mathrm{x}),\mathrm{y})],$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\mathop{\arg\min}_{L^{m}\subseteq U^{n},(\mathrm{x,y})\in L^{m},(x,y)\in
    U^{n}}\mathbb{E}_{(x,y)}[\ell(f(\mathrm{x}),\mathrm{y})],$ |  |'
- en: where $\ell(\cdot)\in\mathcal{R}^{+}$ is the given loss equation, and we expect
    that $m\ll n$. Our goal is to make $m$ as small as possible while ensuring a predetermined
    level of accuracy. Therefore, the query strategy $Q$ in DeepAL is crucial to reduce
    the labeling cost. Next, we will conduct a comprehensive and systematic review
    of DeepAL’s query strategy from the following five aspects.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\ell(\cdot)\in\mathcal{R}^{+}$ 是给定的损失函数，我们期望 $m\ll n$。我们的目标是在确保预定的准确度水平的同时，将
    $m$ 尽可能地缩小。因此，DeepAL 中的查询策略 $Q$ 对于减少标注成本至关重要。接下来，我们将从以下五个方面对 DeepAL 的查询策略进行全面和系统的回顾。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Batch Mode DeepAL (BMDAL). The batch-based query strategy is the foundation
    of DeepAL. The one-by-one sample query strategy in traditional AL is inefficient
    and not applicable to DeepAL, so it is replaced by batch-based query strategy.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 批量模式深度主动学习（BMDAL）。基于批量的查询策略是深度主动学习（DeepAL）的基础。在传统的主动学习（AL）中，逐个样本查询策略效率低下，不适用于深度主动学习，因此被批量查询策略所取代。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uncertainty-based and Hybrid Query Strategies. Uncertainty-based query strategy
    refers to the model based on sample uncertainty ranking to select the sample to
    be queried. The greater the uncertainty of the sample, the easier it is to be
    selected. However, this is likely to ignore the relationship between samples.
    Therefore, the method that considers multiple sample attributes is called the
    hybrid query strategy.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于不确定性和混合查询策略。基于不确定性的查询策略是指基于样本不确定性排名来选择需要查询的样本。样本的不确定性越大，越容易被选择。然而，这可能忽略样本之间的关系。因此，考虑多种样本属性的方法被称为混合查询策略。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Deep Bayesian Active Learning (DBAL). Active learning based on Bayesian convolutional
    neural network (Gal and Ghahramani, [2015](#bib.bib71)) is called deep Bayesian
    active learning.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度贝叶斯主动学习（DBAL）。基于贝叶斯卷积神经网络的主动学习（Gal 和 Ghahramani，[2015](#bib.bib71)）被称为深度贝叶斯主动学习。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Density-based Methods. The density-based method is a query strategy that attempts
    to find a core subset (Phillips, [2016](#bib.bib162)) representing the distribution
    of the entire dataset from the perspective of the dataset to reduce the cost of
    annotation.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于密度的方法。基于密度的方法是一种查询策略，旨在从数据集的角度寻找一个核心子集（Phillips，[2016](#bib.bib162)），以表示整个数据集的分布，从而减少标注成本。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Automated Design of DeepAL. Automated design of DeepAL refers to a method that
    uses automated methods to design AL query strategies or DL models that have an
    important impact on DeepAL performance.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度主动学习的自动化设计。深度主动学习的自动化设计指的是使用自动化方法来设计对 DeepAL 性能有重要影响的 AL 查询策略或 DL 模型。
- en: 3.1.1\. Batch Mode DeepAL (BMDAL)
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 批量模式深度主动学习（BMDAL）
- en: 'The main difference between DeepAL and classical AL is that DeepAL uses batch-based
    sample querying. In traditional AL, most algorithms use a one-by-one query method,
    which leads to frequent training of the learning model but little change in the
    training data. The training set obtained by this query method is not only inefficient
    in the training of the DL model, but can also easily lead to overfitting. Therefore,
    it is necessary to investigate BMDAL in more depth. In the context of BMDAL, at
    each acquisition step, we score a batch of candidate unlabeled data samples $\mathcal{B}=\{x_{1},x_{2},...,x_{b}\}\subseteq
    U$ based on the acquisition function $a$ used and the deep model $f_{\theta}(L)$
    trained on $L$, to select a new batch of data samples $\mathcal{B}^{*}=\{x_{1}^{*},x_{2}^{*},...,x_{b}^{*}\}$.
    This problem can be formulated as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAL 与经典 AL 的主要区别在于 DeepAL 使用基于批量的样本查询。在传统的 AL 中，大多数算法使用逐个查询的方法，这导致学习模型的频繁训练，但训练数据变化很小。这种查询方法获得的训练集不仅在
    DL 模型训练中效率低下，而且容易导致过拟合。因此，有必要深入研究 BMDAL。在 BMDAL 的背景下，在每个获取步骤中，我们根据所使用的获取函数 $a$
    和在 $L$ 上训练的深度模型 $f_{\theta}(L)$ 对一批候选未标记数据样本 $\mathcal{B}=\{x_{1},x_{2},...,x_{b}\}\subseteq
    U$ 进行评分，以选择一批新的数据样本 $\mathcal{B}^{*}=\{x_{1}^{*},x_{2}^{*},...,x_{b}^{*}\}$。这个问题可以表述为：
- en: '| (2) |  | $\mathcal{B}^{*}=\mathop{\arg\max}_{\mathcal{B}\subseteq U}a_{batch}(\mathcal{B},f_{\theta}(L)),$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\mathcal{B}^{*}=\mathop{\arg\max}_{\mathcal{B}\subseteq U}a_{batch}(\mathcal{B},f_{\theta}(L)),$
    |  |'
- en: where $L$ is labeled training set. In order to facilitate understanding, we
    also use $D_{train}$ to represent the labeled training set.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 是标注的训练集。为了便于理解，我们也使用 $D_{train}$ 来表示标注的训练集。
- en: '![Refer to caption](img/2a718b69c9a728e34140073df67d3882.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2a718b69c9a728e34140073df67d3882.png)'
- en: (a) Batch query strategy considering only the amount of information.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: （a）仅考虑信息量的批量查询策略。
- en: '![Refer to caption](img/43866388683c57250c99e0a9aef034fe.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43866388683c57250c99e0a9aef034fe.png)'
- en: (b) Batch query strategy considering both information volume and diversity.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 同时考虑信息量和多样性的批量查询策略。
- en: Figure 2\. A comparison diagram of two batch query strategies, one that only
    considers the amount of information and one that considers both the amount and
    diversity of information. The size of the dots indicates the amount of information
    in the samples, while the distance between the dots represents the similarity
    between the samples. The points shaded in gray indicate the sample points to be
    queried in a batch.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 两种批量查询策略的对比图，一种只考虑信息量，另一种同时考虑信息量和信息的多样性。点的大小表示样本中的信息量，而点之间的距离表示样本之间的相似性。灰色阴影的点表示批量查询中的样本点。
- en: 'A naive approach would be to continuously query a batch of samples based on
    the one-by-one strategy. For example, (Gal and Ghahramani, [2016](#bib.bib72);
    Janz et al., [2017](#bib.bib104)) adopts the method of batch acquisition and chooses
    BALD (Bayesian Active Learning by Disagreement) (Houlsby et al., [2011](#bib.bib98))
    to query top-$K$ samples with the highest scores. The acquisition function $a_{BALD}$
    of this idea is expressed as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是基于逐一策略不断查询一批样本。例如，(Gal 和 Ghahramani, [2016](#bib.bib72); Janz 等, [2017](#bib.bib104))
    采用批量获取的方法，并选择 BALD（贝叶斯主动学习的不一致性）（Houlsby 等, [2011](#bib.bib98)）查询得分最高的前 $K$ 个样本。该想法的获取函数
    $a_{BALD}$ 表达如下：
- en: '| (3) |  |  | $\displaystyle a_{\mathrm{BALD}}\left(\left\{x_{1},\ldots,x_{b}\right\},\mathcal{P}\left(\omega\mid
    D_{train}\right)\right)=\sum_{i=1}^{b}\mathbb{I}\left(y_{i};\omega\mid x_{i},D_{train}\right),$
    |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  |  | $\displaystyle a_{\mathrm{BALD}}\left(\left\{x_{1},\ldots,x_{b}\right\},\mathcal{P}\left(\omega\mid
    D_{train}\right)\right)=\sum_{i=1}^{b}\mathbb{I}\left(y_{i};\omega\mid x_{i},D_{train}\right),$
    |  |'
- en: '|  |  | $\displaystyle\mathbb{I}\left(y;\boldsymbol{\omega}\mid x,D_{train}\right)=\mathbb{H}\left(y\mid
    x,D_{train}\right)-\mathbb{E}_{\mathcal{P}\left(\boldsymbol{\omega}\mid D_{train}\right)}\left[\mathbb{H}\left(y\mid
    x,\boldsymbol{\omega},D_{train}\right)\right],$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{I}\left(y;\boldsymbol{\omega}\mid x,D_{train}\right)=\mathbb{H}\left(y\mid
    x,D_{train}\right)-\mathbb{E}_{\mathcal{P}\left(\boldsymbol{\omega}\mid D_{train}\right)}\left[\mathbb{H}\left(y\mid
    x,\boldsymbol{\omega},D_{train}\right)\right],$ |  |'
- en: where $\mathbb{I}\left(y;\boldsymbol{\omega}\mid x,D_{train}\right)$ used in
    BALD is to estimate the mutual information between model parameters and model
    predictions. The larger the mutual information value $\mathbb{I}(*)$, the higher
    the uncertainty of the sample. The condition of $\boldsymbol{\omega}$ on $D_{train}$
    indicates that the model has been trained with $D_{train}$. And $\omega\sim\mathcal{P}\left(\omega\mid
    D_{train}\right)$ represents the model parameters of the current Bayesian model.
    $\mathbb{H(*)}$ represents the entropy of the model prediction. $\mathbb{E}[H(*)]$
    is the expectation of the entropy of the model prediction over the posterior of
    the model parameters. Equation ([3](#S3.E3 "In 3.1.1\. Batch Mode DeepAL (BMDAL)
    ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning")) considers each sample independently and selects samples
    to construct a batch query dataset in a one-by-one way.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbb{I}\left(y;\boldsymbol{\omega}\mid x,D_{train}\right)$ 在BALD中用于估计模型参数与模型预测之间的互信息。互信息值
    $\mathbb{I}(*)$ 越大，样本的不确定性越高。$\boldsymbol{\omega}$ 在 $D_{train}$ 上的条件表示模型已经用 $D_{train}$
    进行过训练。而 $\omega\sim\mathcal{P}\left(\omega\mid D_{train}\right)$ 代表当前贝叶斯模型的模型参数。$\mathbb{H(*)}$
    代表模型预测的熵。$\mathbb{E}[H(*)]$ 是对模型参数后验分布下模型预测熵的期望。方程 ([3](#S3.E3 "In 3.1.1\. Batch
    Mode DeepAL (BMDAL) ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active
    Learning ‣ A Survey of Deep Active Learning")) 独立考虑每个样本，并以逐一方式选择样本以构建批量查询数据集。
- en: 'Clearly, however, this method is not feasible, as it is very likely to choose
    a set of information-rich but similar samples. The information provided to the
    model by such similar samples is essentially the same, which not only wastes labeling
    resources, but also makes it difficult for the model to learn genuinely useful
    information. In addition, this query method that considers each sample independently
    also ignores the correlation between samples. This is likely to lead to local
    decisions that make the batch sample set of queries insufficiently optimized.
    Therefore, how to simultaneously consider the correlation between different query
    samples is the primary problem for BMDAL. To solve the above problems, BatchBALD
    (Kirsch et al., [2019](#bib.bib116)) expands BALD, which considers the correlation
    between data points by estimating the joint mutual information between multiple
    data points and model parameters. The acquisition function of BatchBALD can be
    expressed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，显然这种方法不可行，因为它很可能选择一组信息丰富但相似的样本。此类相似样本提供给模型的信息本质上是相同的，这不仅浪费了标注资源，还使得模型难以学习真正有用的信息。此外，这种独立考虑每个样本的查询方法也忽略了样本之间的相关性。这很可能导致局部决策，使得批次样本集的查询未得到充分优化。因此，如何同时考虑不同查询样本之间的相关性是BMDAL的主要问题。为了解决上述问题，BatchBALD（Kirsch等，
    [2019](#bib.bib116)）扩展了BALD，通过估计多个数据点和模型参数之间的联合互信息来考虑数据点之间的相关性。BatchBALD的获取函数可以表示如下：
- en: '| (4) |  |  | $\displaystyle a_{\text{BatchBALD }}\left(\left\{x_{1},\ldots,x_{b}\right\},\mathcal{P}\left(\omega\mid
    D_{train}\right)\right)=\mathbb{I}\left(y_{1},\ldots,y_{b};\omega\mid x_{1},\ldots,x_{b},D_{train}\right),$
    |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  |  | $\displaystyle a_{\text{BatchBALD }}\left(\left\{x_{1},\ldots,x_{b}\right\},\mathcal{P}\left(\omega\mid
    D_{train}\right)\right)=\mathbb{I}\left(y_{1},\ldots,y_{b};\omega\mid x_{1},\ldots,x_{b},D_{train}\right),$
    |  |'
- en: '|  |  | $\displaystyle\mathbb{I}\left(y_{1:b};\boldsymbol{\omega}\mid x_{1:b},D_{train}\right)=\mathbb{H}\left(y_{1:b}\mid
    x_{1:b},D_{train}\right)-\mathbb{E}_{\mathcal{P}\left(\boldsymbol{\omega}\mid
    D_{train}\right)}\mathbb{H}\left(y_{1:b}\mid x_{1:b},\boldsymbol{\omega},D_{train}\right),$
    |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{I}\left(y_{1:b};\boldsymbol{\omega}\mid x_{1:b},D_{train}\right)=\mathbb{H}\left(y_{1:b}\mid
    x_{1:b},D_{train}\right)-\mathbb{E}_{\mathcal{P}\left(\boldsymbol{\omega}\mid
    D_{train}\right)}\mathbb{H}\left(y_{1:b}\mid x_{1:b},\boldsymbol{\omega},D_{train}\right),$
    |  |'
- en: where $x_{1},\ldots,x_{b}$ and $y_{1},\ldots,y_{b}$ are represented by joint
    random variables $x_{1:b}$ and $y_{1:b}$ in a product probability space, and $\mathbb{I}\left(y_{1:b};\boldsymbol{\omega}\mid
    x_{1:b},D_{train}\right)$ denotes the mutual information between these two random
    variables. BatchBALD considers the correlation between different query samples
    by designing an explicit joint mutual information mechanism to obtain a better
    query batch sample set.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{1},\ldots,x_{b}$ 和 $y_{1},\ldots,y_{b}$ 在乘积概率空间中由联合随机变量 $x_{1:b}$ 和 $y_{1:b}$
    表示，$\mathbb{I}\left(y_{1:b};\boldsymbol{\omega}\mid x_{1:b},D_{train}\right)$
    表示这两个随机变量之间的互信息。BatchBALD 通过设计一个明确的联合互信息机制来考虑不同查询样本之间的相关性，从而获得更好的查询批次样本集。
- en: The batch-based query strategy forms the basis of the combination of AL and
    DL, and related research on this topic is also very rich. We will provide a detailed
    overview and discussion of BMDAL query strategies in the following sections.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于批次的查询策略构成了主动学习（AL）和深度学习（DL）结合的基础，相关的研究也非常丰富。我们将在以下章节中提供BMDAL查询策略的详细概述和讨论。
- en: 3.1.2\. Uncertainty-based and Hybrid Query Strategies
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 基于不确定性和混合查询策略
- en: 'Because the uncertainty-based approach is simple in form and has low computational
    complexity, it is a very popular query strategy in AL. This query strategy is
    mainly used in certain shallow models (eg, SVM (Tong and Koller, [2002](#bib.bib223))
    or KNN (Jain and Kapoor, [2009](#bib.bib103))). This is mainly because the uncertainty
    of these models can be accurately obtained by traditional uncertainty sampling
    methods. In uncertainty-based sampling, learners try to select the most uncertain
    samples to form a batch query set. For example, in the margin sampling (Scheffer
    et al., [2001](#bib.bib189)), margin $M$ is defined as the difference between
    the predicted highest probability and the predicted second highest probability
    of an sample as follows: $M=P\left(y_{1}\mid x\right)-P\left(y_{2}\mid x\right),$
    where $y_{1}$ and $y_{2}$ are the first and second most probable labels predicted
    for the sample $x$ under the current model. The smaller the margin $M$, the greater
    the uncertainty of the sample $x$. The AL algorithm selects the top-$K$ samples
    with the smallest margin $M$ as the batch query set by calculating the margin
    $M$ of all unlabeled samples. Information entropy (Settles, [2009](#bib.bib194))
    is also a commonly used uncertainty measurement standard. For a $k$-class task,
    the information entropy $\mathbb{E}(x)$ of sample $x$ can be defined as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基于不确定性的方法形式简单且计算复杂度低，它在主动学习中是一种非常受欢迎的查询策略。这种查询策略主要用于某些浅层模型（例如，SVM（Tong and
    Koller，[2002](#bib.bib223)）或KNN（Jain and Kapoor，[2009](#bib.bib103)））。这主要是因为这些模型的不确定性可以通过传统的不确定性采样方法准确获得。在基于不确定性的采样中，学习者尝试选择最不确定的样本以形成批量查询集。例如，在边界采样（Scheffer
    et al.，[2001](#bib.bib189)）中，边界$M$定义为样本的预测最高概率与预测第二高概率之间的差值，如下所示：$M=P\left(y_{1}\mid
    x\right)-P\left(y_{2}\mid x\right),$ 其中$y_{1}$和$y_{2}$是当前模型下对样本$x$预测的第一和第二高概率标签。边界$M$越小，样本$x$的不确定性越大。AL算法通过计算所有未标记样本的边界$M$，选择边界$M$最小的前$K$个样本作为批量查询集。信息熵（Settles，[2009](#bib.bib194)）也是一种常用的不确定性测量标准。对于一个$k$-类任务，样本$x$的信息熵$\mathbb{E}(x)$可以定义如下：
- en: '| (5) |  | $\mathbb{E}(x)=-\sum_{i=1}^{k}P(y_{i}\mid x)\cdot\log\left(P(y_{i}\mid
    x)\right),$ |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\mathbb{E}(x)=-\sum_{i=1}^{k}P(y_{i}\mid x)\cdot\log\left(P(y_{i}\mid
    x)\right),$ |  |'
- en: where $P(y_{i}\mid x)$ is the probability that the current sample $x$ is predicted
    to be class $y_{i}$. The greater the entropy of the sample, the greater its uncertainty.
    Therefore, the top-$K$ samples with the largest information entropy should be
    selected. More query strategies based on uncertainty can be found in (Aggarwal
    et al., [2014](#bib.bib4)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$P(y_{i}\mid x)$是当前样本$x$被预测为类别$y_{i}$的概率。样本的熵越大，其不确定性也越大。因此，应该选择信息熵最大的前$K$个样本。基于不确定性的更多查询策略可以在（Aggarwal
    et al.，[2014](#bib.bib4)）中找到。
- en: There are many DeepAL (Ranganathan et al., [2017](#bib.bib174); Asghar et al.,
    [2017](#bib.bib14); He et al., [2019a](#bib.bib89); Ostapuk et al., [2019](#bib.bib157))
    methods that directly utilize an uncertainty-based sampling strategy. However,
    DFAL (DeepFool Active Learning) (Ducoffe and Precioso, [2018](#bib.bib58)) contends
    that these methods are easily fooled by adversarial examples; thus, it focuses
    on the study of examples near the decision boundary, and actively uses the information
    provided by these adversarial examples on the input spatial distribution in order
    to approximate their distance to the decision boundary. This adversarial query
    strategy can effectively improve the convergence speed of CNN training. Nevertheless,
    as analyzed in Section [3.1.1](#S3.SS1.SSS1 "3.1.1\. Batch Mode DeepAL (BMDAL)
    ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning"), this can easily lead to insufficient diversity of batch
    query samples (such that relevant knowledge regarding the data distribution is
    not fully utilized), which in turn leads to low or even invalid DL model training
    performance. A feasible strategy would thus be to use a hybrid query strategy
    in a batch query, taking into account both the information volume and diversity
    of samples in either an explicit or implicit manner.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 DeepAL（Ranganathan 等，[2017](#bib.bib174)；Asghar 等，[2017](#bib.bib14)；He 等，[2019a](#bib.bib89)；Ostapuk
    等，[2019](#bib.bib157)）方法直接利用基于不确定性的采样策略。然而，DFAL（DeepFool Active Learning）（Ducoffe
    和 Precioso，[2018](#bib.bib58)）认为这些方法容易被对抗样本欺骗；因此，它集中于研究接近决策边界的样本，并主动利用这些对抗样本提供的信息，以近似它们到决策边界的距离。这种对抗查询策略可以有效提高
    CNN 训练的收敛速度。然而，如第 [3.1.1](#S3.SS1.SSS1 "3.1.1\. Batch Mode DeepAL (BMDAL) ‣ 3.1\.
    Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of
    Deep Active Learning") 节分析，这可能容易导致批次查询样本的多样性不足（从而相关的数据分布知识未被充分利用），这反过来导致 DL 模型训练性能低下或甚至无效。因此，一种可行的策略是在批次查询中使用混合查询策略，以显式或隐式方式考虑样本的信息量和多样性。
- en: 'The performance of early Batch Mode Active Learning (BMAL) (Wang et al., [2016](#bib.bib236);
    Joshi et al., [2010](#bib.bib108); Brinker, [2003](#bib.bib30); Xia et al., [2016](#bib.bib239);
    Nguyen and Smeulders, [2004](#bib.bib154); Tan et al., [2019b](#bib.bib219)) algorithms
    are often excessively reliant on the measurement of similarity between samples.
    In addition, these algorithms are often only good at exploitation (learners tend
    to focus only on samples near the current decision boundary, corresponding to
    high-information query strategies), meaning that the samples in the query batch
    sample set cannot represent the true data distribution of the feature space (due
    to the insufficient diversity of batch sample sets). To address this issue, Exploration-P
    (Yin et al., [2017](#bib.bib245)) uses a deep neural network to learn the feature
    representation of the samples, then explicitly calculates the similarity between
    the samples. At the same time, the processes of exploitation and exploration (in
    the early days of model training, learners used random sampling strategies for
    exploration purposes) are balanced to enable more accurate measurement of the
    similarity between samples. More specifically, Exploration-P uses the information
    entropy in Equation ([5](#S3.E5 "In 3.1.2\. Uncertainty-based and Hybrid Query
    Strategies ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning
    ‣ A Survey of Deep Active Learning")) to estimate the uncertainty of sample $x$
    under the current model. The uncertainty of the selected sample set $S$ can be
    expressed as $E(S)=\sum_{x_{i}\in S}\mathbb{E}(x_{i})$. Furthermore, to measure
    the redundancy between samples in the selected sample set $S$, Exploration-P uses
    $R(S)$ to represent the redundancy of selected sample set $S$:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的 Batch Mode Active Learning (BMAL) (Wang et al., [2016](#bib.bib236); Joshi
    et al., [2010](#bib.bib108); Brinker, [2003](#bib.bib30); Xia et al., [2016](#bib.bib239);
    Nguyen and Smeulders, [2004](#bib.bib154); Tan et al., [2019b](#bib.bib219)) 算法的性能往往过度依赖于样本之间的相似性度量。此外，这些算法通常只擅长**利用**（学习者往往只关注当前决策边界附近的样本，对应于高信息量的查询策略），这意味着查询批次样本集中的样本不能代表特征空间的真实数据分布（由于批次样本集的多样性不足）。为了解决这个问题，Exploration-P
    (Yin et al., [2017](#bib.bib245)) 使用深度神经网络来学习样本的特征表示，然后明确计算样本之间的相似性。同时，**利用**和**探索**的过程（在模型训练的早期，学习者使用随机采样策略进行探索）得到了平衡，从而实现了更准确的样本相似性度量。更具体地，Exploration-P
    使用方程 ([5](#S3.E5 "In 3.1.2\. Uncertainty-based and Hybrid Query Strategies ‣ 3.1\.
    Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of
    Deep Active Learning")) 中的信息熵来估计当前模型下样本 $x$ 的不确定性。所选择的样本集 $S$ 的不确定性可以表示为 $E(S)=\sum_{x_{i}\in
    S}\mathbb{E}(x_{i})$。此外，为了度量所选样本集 $S$ 中样本之间的冗余，Exploration-P 使用 $R(S)$ 来表示选定样本集
    $S$ 的冗余：
- en: '| (6) |  | $R(S)=\sum_{x_{i}\in S}\sum_{x_{j}\in S}Sim(x_{i},x_{j}),\quad Sim(x_{i},x_{j})=f(x_{i})\mathcal{M}f(x_{j}),$
    |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $R(S)=\sum_{x_{i}\in S}\sum_{x_{j}\in S}Sim(x_{i},x_{j}),\quad Sim(x_{i},x_{j})=f(x_{i})\mathcal{M}f(x_{j}),$
    |  |'
- en: 'where $f(x)$ represents the feature of sample $x$ extracted by deep learning
    model $f$, $Sim(x_{i},x_{j})$ measures the similarity between two samples, and
    $\mathcal{M}$ is a similarity matrix (when $\mathcal{M}$ is the identity matrix,
    the similarity of two samples is the product of their feature vectors. In addition,
    $\mathcal{M}$ can also be learned as a parameter of $f$). Therefore, the selected
    sample set $S$ is expected to have the largest uncertainty and the smallest redundancy.
    For this reason, Exploration-P considers these two strategies, and the final goal
    equation is defined as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$f(x)$ 表示由深度学习模型 $f$ 提取的样本 $x$ 的特征，$Sim(x_{i},x_{j})$ 测量两个样本之间的相似性，$\mathcal{M}$
    是相似性矩阵（当 $\mathcal{M}$ 是单位矩阵时，两个样本的相似性是其特征向量的乘积。此外，$\mathcal{M}$ 也可以作为 $f$ 的一个参数来学习）。因此，所选择的样本集
    $S$ 期望具有最大的**不确定性**和最小的**冗余**。为此，Exploration-P 考虑了这两种策略，最终目标方程定义为：
- en: '| (7) |  | $\mathrm{I}(S)=E(S)-\frac{\alpha}{&#124;S&#124;}R(S),$ |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\mathrm{I}(S)=E(S)-\frac{\alpha}{&#124;S&#124;}R(S),$ |  |'
- en: where, $\alpha$ is used to balance the weight of the hybrid query strategies,
    uncertainty and redundancy.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\alpha$ 用于平衡混合查询策略中的不确定性和冗余的权重。
- en: Moreover, DMBAL (Diverse Mini-Batch Active Learning) (Zhdanov, [2019](#bib.bib256))
    adds informativeness to the optimization goal of K-means by weight, and further
    presents an in-depth study of a hybrid query strategy that considers the sample
    information volume and diversity under the mini-batch sample query setting. DMBAL
    (Zhdanov, [2019](#bib.bib256)) can easily achieve expansion from the generalized
    linear model to DL; this not only increases the scalability of DMBAL (Zhdanov,
    [2019](#bib.bib256)) but also increases the diversity of active query samples
    in the mini-batch. Fig.[2](#S3.F2 "Figure 2 ‣ 3.1.1\. Batch Mode DeepAL (BMDAL)
    ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning") illustrates a schematic diagram of this idea. This hybrid
    query strategy is quite popular. For example, WI-DL (Weighted Incremental Dictionary
    Learning) (Liu et al., [2017](#bib.bib136)) mainly considers the two stages of
    DBN. In the unsupervised feature learning stage, the key consideration is the
    representativeness of the data, while in the supervised fine-tuning stage, the
    uncertainty of the data is considered; these two indicators are then integrated,
    and finally optimized using the proposed weighted incremental dictionary learning
    algorithm.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DMBAL（多样化小批量主动学习）（Zhdanov，[2019](#bib.bib256)）通过权重将信息量加入K-means的优化目标，并进一步深入研究了一种混合查询策略，该策略在小批量样本查询设置下考虑样本信息量和多样性。DMBAL（Zhdanov，[2019](#bib.bib256)）可以轻松实现从广义线性模型到深度学习的扩展；这不仅增加了DMBAL（Zhdanov，[2019](#bib.bib256)）的可扩展性，还增加了小批量中主动查询样本的多样性。图[2](#S3.F2
    "图 2 ‣ 3.1.1. 批处理模式深度主动学习（BMDAL） ‣ 3.1. 查询策略优化 ‣ 3. 深度主动学习 ‣ 深度主动学习概述")展示了这一思想的示意图。这种混合查询策略相当流行。例如，WI-DL（加权增量字典学习）（Liu等，[2017](#bib.bib136)）主要考虑了DBN的两个阶段。在无监督特征学习阶段，关键考虑因素是数据的代表性，而在有监督微调阶段，则考虑数据的不确定性；这两个指标随后被整合，并最终使用提出的加权增量字典学习算法进行优化。
- en: 'Although the above improvements have resulted in a good performance, there
    is still a hidden danger that must be addressed: namely, that, diversity-based
    strategies are not appropriate for all datasets. More specifically, the richer
    the category content of the dataset, the larger the batch size, and the better
    the effect of diversity-based methods; by contrast, an uncertainty-based query
    strategy will perform better with smaller batch sizes and less rich content. These
    characteristics depend on the statistical characteristics of the dataset. The
    BMAL context, whether the data are unfamiliar and potentially unstructured, makes
    it impossible to determine which AL query strategy is more appropriate. In light
    of this, BADGE (Batch Active learning by Diverse Gradient Embeddings) (Ash et al.,
    [2020](#bib.bib15)) samples point groups that are disparate and high magnitude
    when represented in a hallucinated gradient space, meaning that both the prediction
    uncertainty of the model and the diversity of the samples in a batch are considered
    simultaneously. Most importantly, BADGE can achieve an automatic balance between
    forecast uncertainty and sample diversity without the need for manual hyperparameter
    adjustments. Moreover, while BADGE (Ash et al., [2020](#bib.bib15)) considers
    this hybrid query strategy in an implicit way, WAAL (Wasserstein Adversarial Active
    Learning) (Shui et al., [2020](#bib.bib201)) proposes a hybrid query strategy
    that explicitly balances uncertainty and diversity. In addition, WAAL (Shui et al.,
    [2020](#bib.bib201)) uses Wasserstein distance to model the interactive procedure
    in AL as a distribution matching problem, derives losses from it, and then decomposes
    WAAL (Shui et al., [2020](#bib.bib201)) into two stages: DNN parameter optimization
    and query batch selection. TA-VAAL (Task-Aware Variational Adversarial Active
    Learning) (Kim et al., [2020](#bib.bib113)) also explores the balance of this
    hybrid query strategy. The assumption underpinning TA-VAAL is that the uncertainty-based
    method does not make good use of the overall data distribution, while the data
    distribution-based method often ignores the structure of the task. Consequently,
    TA-VAAL proposes to integrate the loss prediction module (Yoo and Kweon, [2019](#bib.bib246))
    and the concept of RankCGAN (Saquil et al., [2018](#bib.bib186)) into VAAL (Variational
    Adversarial Active Learning) (Sinha et al., [2019](#bib.bib205)), enabling both
    the data distribution and the model uncertainty to be considered. TA-VAAL has
    achieved good performance on various balanced and unbalanced benchmark datasets.
    The structure diagram of TA-VAAL and VAAL is presented in Fig.[3](#S3.F3 "Figure
    3 ‣ 3.1.2\. Uncertainty-based and Hybrid Query Strategies ‣ 3.1\. Query Strategy
    Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述改进已取得了良好的效果，但仍存在一个必须解决的潜在风险：即，基于多样性的策略并不适用于所有数据集。更具体地说，数据集的类别内容越丰富，批量大小越大，基于多样性的方法效果越好；相比之下，基于不确定性的查询策略在批量较小和内容较少时表现更佳。这些特征依赖于数据集的统计特性。BMAL
    环境下，无论数据是否陌生且潜在结构化，都无法确定哪个 AL 查询策略更为合适。鉴于此，BADGE（Batch Active learning by Diverse
    Gradient Embeddings）（Ash 等， [2020](#bib.bib15)）在幻觉梯度空间中对不相似且高幅度的点组进行采样，这意味着模型的预测不确定性和批次样本的多样性同时被考虑。最重要的是，BADGE
    可以在不需要手动调整超参数的情况下自动实现预测不确定性和样本多样性之间的平衡。此外，虽然 BADGE（Ash 等， [2020](#bib.bib15)）以隐式方式考虑这种混合查询策略，WAAL（Wasserstein
    Adversarial Active Learning）（Shui 等， [2020](#bib.bib201)）提出了一种显式平衡不确定性和多样性的混合查询策略。此外，WAAL（Shui
    等， [2020](#bib.bib201)）使用 Wasserstein 距离将 AL 中的交互过程建模为分布匹配问题，从中推导损失，然后将 WAAL（Shui
    等， [2020](#bib.bib201)）分解为两个阶段：DNN 参数优化和查询批次选择。TA-VAAL（Task-Aware Variational
    Adversarial Active Learning）（Kim 等， [2020](#bib.bib113)）也探索了这种混合查询策略的平衡。TA-VAAL
    的假设是，基于不确定性的方法未能充分利用整体数据分布，而基于数据分布的方法常常忽视任务的结构。因此，TA-VAAL 提出了将损失预测模块（Yoo 和 Kweon，
    [2019](#bib.bib246)）和 RankCGAN（Saquil 等， [2018](#bib.bib186)）的概念集成到 VAAL（Variational
    Adversarial Active Learning）（Sinha 等， [2019](#bib.bib205)）中，从而同时考虑数据分布和模型不确定性。TA-VAAL
    在各种平衡和不平衡的基准数据集上取得了良好的表现。TA-VAAL 和 VAAL 的结构图见图。[3](#S3.F3 "图 3 ‣ 3.1.2\. 基于不确定性和混合查询策略
    ‣ 3.1\. 深度活跃学习中的查询策略优化 ‣ 3\. 深度活跃学习 ‣ 深度活跃学习综述")
- en: '![Refer to caption](img/5e1f6bf2044c1f0d17e942b851dcc328.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5e1f6bf2044c1f0d17e942b851dcc328.png)'
- en: Figure 3\. Structure comparison chart of VAAL (Sinha et al., [2019](#bib.bib205))
    and TA-VAAL (Kim et al., [2020](#bib.bib113)). 1) VAAL uses labeled data and unlabeled
    data in a semi-supervised way to learn the latent representation space of the
    data, then selects the unlabeled data with the largest amount of information according
    to the latent space for labeling. 2) TA-VAAL expands VAAL and integrates the loss
    prediction module (Yoo and Kweon, [2019](#bib.bib246)) and RankCGAN (Saquil et al.,
    [2018](#bib.bib186)) into VAAL in order to consider data distribution and model
    uncertainty simultaneously.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. VAAL（Sinha 等， [2019](#bib.bib205)）和 TA-VAAL（Kim 等，[2020](#bib.bib113)）的结构比较图。1）VAAL
    以半监督的方式使用标记数据和未标记数据来学习数据的潜在表示空间，然后根据潜在空间选择信息量最大的未标记数据进行标记。2）TA-VAAL 扩展了 VAAL，并将损失预测模块（Yoo
    和 Kweon，[2019](#bib.bib246)）和 RankCGAN（Saquil 等，[2018](#bib.bib186)）整合到 VAAL 中，以同时考虑数据分布和模型不确定性。
- en: Notably, although the hybrid query strategy achieves superior performance, the
    uncertainty-based AL query strategy is more convenient to combine with the output
    of the softmax layer of DL. Thus, the query strategy based on uncertainty is still
    widely used.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管混合查询策略表现优越，但基于不确定性的主动学习查询策略更方便与深度学习的softmax层输出结合。因此，基于不确定性的查询策略仍然被广泛使用。
- en: 3.1.3\. Deep Bayesian Active Learning (DBAL)
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 深度贝叶斯主动学习（DBAL）
- en: As noted in Section [2](#S2 "2\. The necessity and challenge of combining DL
    and AL ‣ A Survey of Deep Active Learning"), which analyzes the challenge of combining
    DL and AL, the acquisition function based on uncertainty is an important research
    direction of many classic AL algorithms. Moreover, traditional DL methods rarely
    represent such model uncertainty.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2](#S2 "2\. The necessity and challenge of combining DL and AL ‣ A Survey
    of Deep Active Learning")节所述，分析了结合深度学习和主动学习的挑战，基于不确定性的获取函数是许多经典主动学习算法的重要研究方向。此外，传统的深度学习方法很少表示这种模型不确定性。
- en: 'To solve the above problems, Deep Bayesian Active Learning appears. In the
    given input set $X$ and the output $Y$ belonging to class $c$, the probabilistic
    neural network model can be defined as $f(\mathrm{x};\theta)$, $p(\theta)$ is
    a prior on the parameter space $\theta$ (usually Gaussian), and the likelihood
    $p(\mathrm{y}=c|\mathrm{x},\theta)$ is usually given by $softma\mathrm{x}(f(\mathrm{x};\theta))$.
    Our goal is to obtain the posterior distribution over $\theta$, as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，出现了深度贝叶斯主动学习。在给定的输入集 $X$ 和属于类 $c$ 的输出 $Y$ 中，概率神经网络模型可以定义为 $f(\mathrm{x};\theta)$，$p(\theta)$
    是参数空间 $\theta$ 上的先验（通常是高斯分布），而似然函数 $p(\mathrm{y}=c|\mathrm{x},\theta)$ 通常由 $softma\mathrm{x}(f(\mathrm{x};\theta))$
    给出。我们的目标是获取 $\theta$ 的后验分布，如下所示：
- en: '| (8) |  | $p(\theta&#124;X,Y)=\frac{p(Y&#124;X,\theta)p(\theta)}{p(Y&#124;X)}.$
    |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $p(\theta \mid X,Y)=\frac{p(Y \mid X,\theta)p(\theta)}{p(Y \mid
    X)}.$ |  |'
- en: 'For a given new data point $\mathrm{x}^{*}$, $\hat{\mathrm{y}}$ is predicted
    by:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的新数据点 $\mathrm{x}^{*}$，$\hat{\mathrm{y}}$ 的预测方式如下：
- en: '| (9) |  | $p\left(\hat{\mathrm{y}}&#124;\mathrm{x}^{*},X,Y\right)=\int p\left(\hat{\mathrm{y}}&#124;\mathrm{x},\theta\right)p(\theta&#124;X,Y)d\theta=\mathbb{E}_{\theta\sim
    p(\theta&#124;X,Y)}[f(\mathrm{x};\theta)].$ |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $p\left(\hat{\mathrm{y}} \mid \mathrm{x}^{*},X,Y\right)=\int p\left(\hat{\mathrm{y}}
    \mid \mathrm{x},\theta\right)p(\theta \mid X,Y)d\theta=\mathbb{E}_{\theta \sim
    p(\theta \mid X,Y)}[f(\mathrm{x};\theta)].$ |  |'
- en: DBAL (Gal et al., [2017](#bib.bib73)) combines BCNNs (Bayesian Convolutional
    Neural Networks) (Gal and Ghahramani, [2015](#bib.bib71)) with AL methods to adapt
    BALD (Houlsby et al., [2011](#bib.bib98)) to the deep learning environment, thereby
    developing a new AL framework for high-dimensional data. This approach adopts
    the above method to first perform Gaussian prior modeling on the weights of a
    CNN, and then uses variational inference to obtain the posterior distribution
    of network prediction. In addition, in practice, researchers often also use a
    powerful and low-cost MC-dropout (Monte-Carlo dropout) (Srivastava et al., [2014](#bib.bib213))
    stochastic regularization technique to obtain posterior samples, consequently
    attaining good performance on real-world datasets (Leibig et al., [2017](#bib.bib131);
    Kendall et al., [2017](#bib.bib112)). Moreover, this regularization technique
    has been proven to be equivalent to variational inference (Gal and Ghahramani,
    [2016](#bib.bib72)). However, a core-set approach (Sener and Savarese, [2018](#bib.bib193))
    points out that DBAL (Gal et al., [2017](#bib.bib73)) is unsuitable for large
    datasets due to the need for batch sampling. It should be noted here that while
    DBAL (Gal et al., [2017](#bib.bib73)) allows the use of dropout in testing for
    better confidence estimation, the analysis presented in (Gissin and Shalev-Shwartz,
    [2019](#bib.bib80)) contends that the performance of this method is similar to
    the performance of using neural network SR (Wang et al., [2017](#bib.bib230))
    as uncertainty sampling, which requires vigilance. In addition, DEBAL (Deep Ensemble
    Bayesian Active Learning) (Pop and Fulop, [2018](#bib.bib166)) argues that the
    pattern collapse phenomenon (Srivastava et al., [2017](#bib.bib212)) in the variational
    inference method leads to the overconfident prediction characteristic of the DBAL
    method. For this reason, DEBAL combines the expressive power of ensemble methods
    with MC-dropout to obtain better uncertainty in the absence of trading representativeness.
    For its part, BatchBALD (Kirsch et al., [2019](#bib.bib116)) opts to expand BALD
    (Houlsby et al., [2011](#bib.bib98)) to the batch query context; this approach
    no longer calculates the mutual information between a single sample and model
    parameters but rather recalculates the mutual information between the batch samples
    and the model parameters to jointly score the batch of samples. This enables BatchBALD
    to more accurately evaluate the joint mutual information. Inspired by the latest
    research on Bayesian core sets (Huggins et al., [2016](#bib.bib100); Campbell
    and Broderick, [2019](#bib.bib34)), ACS-FW (Active Bayesian CoreSets with Frank-Wolfe
    optimization) (Pinsler et al., [2019](#bib.bib163)) reconstructed the batch structure
    to optimize the sparse subset approximation of the log-posterior induced by the
    entire dataset. Using this similarity, ACS-FW then employs the Frank-Wolfe (Frank
    et al., [1956](#bib.bib69)) algorithm to enable effective Bayesian AL at scale,
    while its use of random projection has made it still more popular. Compared with
    other query strategies (e.g., maximizing the predictive entropy (MAXENT) (Sener
    and Savarese, [2018](#bib.bib193); Gal et al., [2017](#bib.bib73)) and BALD (Houlsby
    et al., [2011](#bib.bib98))), ACS-FW achieves better coverage across the entire
    data manifold. DPEs (Deep Probabilistic Ensembles) (Chitta et al., [2018](#bib.bib40))
    introduces an expandable DPEs technology, which uses a regularized ensemble to
    approximate the deep BNN, and then evaluates the classification effect of these
    DPEs in a series of large-scale visual AL experiments.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DBAL (Gal et al., [2017](#bib.bib73)) 结合了 BCNNs（贝叶斯卷积神经网络）(Gal and Ghahramani,
    [2015](#bib.bib71)) 和 AL 方法，将 BALD (Houlsby et al., [2011](#bib.bib98)) 适配到深度学习环境中，从而开发了一个用于高维数据的新
    AL 框架。该方法采用上述方法首先对 CNN 的权重进行高斯先验建模，然后使用变分推断获得网络预测的后验分布。此外，在实践中，研究人员还经常使用一种强大且低成本的
    MC-dropout（蒙特卡洛丢弃）(Srivastava et al., [2014](#bib.bib213)) 随机正则化技术来获得后验样本，从而在实际数据集上取得了良好的表现
    (Leibig et al., [2017](#bib.bib131); Kendall et al., [2017](#bib.bib112))。此外，这种正则化技术已被证明等同于变分推断
    (Gal and Ghahramani, [2016](#bib.bib72))。然而，核心集方法 (Sener and Savarese, [2018](#bib.bib193))
    指出，DBAL (Gal et al., [2017](#bib.bib73)) 由于需要批量采样，不适用于大型数据集。这里需要注意的是，虽然 DBAL (Gal
    et al., [2017](#bib.bib73)) 允许在测试中使用丢弃以获得更好的置信度估计，但 (Gissin and Shalev-Shwartz,
    [2019](#bib.bib80)) 中的分析认为，这种方法的表现与使用神经网络 SR (Wang et al., [2017](#bib.bib230))
    作为不确定性采样的方法相似，这需要谨慎。此外，DEBAL（深度集成贝叶斯主动学习）(Pop and Fulop, [2018](#bib.bib166))
    认为，变分推断方法中的模式崩溃现象 (Srivastava et al., [2017](#bib.bib212)) 导致了 DBAL 方法的过度自信预测特征。因此，DEBAL
    将集成方法的表现力与 MC-dropout 结合，以在不牺牲代表性的情况下获得更好的不确定性。至于 BatchBALD (Kirsch et al., [2019](#bib.bib116))，它选择将
    BALD (Houlsby et al., [2011](#bib.bib98)) 扩展到批量查询上下文；这种方法不再计算单个样本与模型参数之间的互信息，而是重新计算批量样本与模型参数之间的互信息，以联合评分样本批次。这使得
    BatchBALD 能够更准确地评估联合互信息。受到最新贝叶斯核心集研究 (Huggins et al., [2016](#bib.bib100); Campbell
    and Broderick, [2019](#bib.bib34)) 的启发，ACS-FW（具有弗兰克-沃尔夫优化的主动贝叶斯核心集）(Pinsler et
    al., [2019](#bib.bib163)) 重建了批量结构，以优化由整个数据集引起的对数后验的稀疏子集近似。利用这种相似性，ACS-FW 然后使用弗兰克-沃尔夫
    (Frank et al., [1956](#bib.bib69)) 算法实现了大规模有效的贝叶斯 AL，同时其随机投影的使用使其更加流行。与其他查询策略（例如，最大化预测熵
    (MAXENT) (Sener and Savarese, [2018](#bib.bib193); Gal et al., [2017](#bib.bib73))
    和 BALD (Houlsby et al., [2011](#bib.bib98))) 相比，ACS-FW 实现了对整个数据流形的更好覆盖。DPEs（深度概率集成）(Chitta
    et al., [2018](#bib.bib40)) 引入了一种可扩展的 DPEs 技术，该技术使用正则化集成来近似深度 BNN，然后在一系列大规模视觉
    AL 实验中评估这些 DPEs 的分类效果。
- en: ActiveLink (Deep Active Learning for Link Prediction in Knowledge Graphs) (Ostapuk
    et al., [2019](#bib.bib157)) is inspired by the latest advances in Bayesian deep
    learning (Gal and Ghahramani, [2016](#bib.bib72); Welling and Teh, [2011](#bib.bib237)).
    Adopting the Bayesian view of the existing neural link predictors, it expands
    the uncertainty sampling method by using the basic structure of the knowledge
    graph, thereby creating a novel DeepAL method. ActiveLink further noted that although
    AL can sample efficiently, the model needs to be retrained from scratch for each
    iteration in the AL process, which is unacceptable in the DL model training context.
    A simple solution would be to use newly selected data to train the model incrementally,
    or to combine it with existing training data (Shen et al., [2017](#bib.bib200));
    however, this would cause the model to be biased either towards a small amount
    of newly selected data or towards data selected early in the process. In order
    to solve this bias problem, ActiveLink adopts a principled and unbiased incremental
    training method based on meta-learning. More specifically, in each AL iteration,
    ActiveLink uses the newly selected samples to update the model parameters, then
    approximates the meta-objective of the model’s future prediction by generalizing
    the model based on the samples selected in the previous iteration. This enables
    ActiveLink to strike a balance between the importance of the newly and previously
    selected data, and thereby to achieve an unbiased estimation of the model parameters.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ActiveLink（知识图谱中链接预测的深度主动学习）（Ostapuk等人，[2019](#bib.bib157)）灵感源于贝叶斯深度学习的最新进展（Gal和Ghahramani，[2016](#bib.bib72)；Welling和Teh，[2011](#bib.bib237)）。
    通过采用现有神经链接预测器的贝叶斯视图，扩展了不确定性采样方法，并利用知识图谱的基本结构，从而创建了全新的DeepAL方法。 ActiveLink进一步指出，尽管AL可以有效地进行采样，但在AL过程中模型需要从头开始重新训练，这在DL模型训练环境中是不可接受的。一个简单的解决方案是使用新选定的数据来增量训练模型，或者将其与现有训练数据结合使用（Shen等人，[2017](#bib.bib200)）；然而，这将导致模型偏向于新选定的少量数据或者过早选定的数据。为了解决这个偏向问题，ActiveLink采用了一种基于元学习的原则性和无偏的增量训练方法。
    更具体地说，在每个AL迭代中，ActiveLink使用新选定的样本来更新模型参数，然后通过在前一次迭代中选择的样本概括模型来估计模型未来预测的元目标。 这使得ActiveLink能够在新选定的数据和先前选定数据之间取得平衡，从而实现对模型参数的无偏估计。
- en: In addition to the above-mentioned DBAL work, due to the lesser parameter of
    BNN and the uncertainty sampling strategy being similar to traditional AL, the
    research on DBAL is quite extensive, and there are many works related to this
    topic (Siddhant and Lipton, [2018](#bib.bib202); Rottmann et al., [2018](#bib.bib177);
    Yang et al., [2018](#bib.bib243); Zeng et al., [2018](#bib.bib248); Gudur et al.,
    [2019](#bib.bib84); Martínez-Arellano and Ratchev, [2019](#bib.bib144)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述的DBAL工作之外，由于BNN的参数较少，且不确定性采样策略类似于传统的AL，对DBAL的研究非常广泛，有许多与该主题相关的工作（Siddhant和Lipton，[2018](#bib.bib202)；Rottmann等人，[2018](#bib.bib177)；Yang等人，[2018](#bib.bib243)；Zeng等人，[2018](#bib.bib248)；Gudur等人，[2019](#bib.bib84)；Martínez-Arellano和Ratchev，[2019](#bib.bib144)）。
- en: 3.1.4\. Density-based Methods
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 基于密度的方法
- en: The term, density-based method, mainly refers to the selection of samples from
    the perspective of the set (core set (Phillips, [2016](#bib.bib162))). The construction
    of the core set is a representative query strategy. This idea is mainly inspired
    by the compression idea of the core set dataset and attempts to use the core set
    to represent the distribution of the feature space of the entire original dataset,
    thereby reducing the labeling cost of AL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 密度方法一词主要指从集合的角度选择样本（核心集（Phillips，[2016](#bib.bib162)））。核心集的构建是一种代表性的查询策略。 这个想法主要受到核心集数据集的压缩想法的启发，并尝试使用核心集代表整个原始数据集的特征空间的分布，从而降低AL的标记成本。
- en: FF-Active (Farthest First Active Learning) (Geifman and El-Yaniv, [2017](#bib.bib76))
    is based on this idea and uses the farthest-first traversal in the space of neural
    activation over a representation layer to query consecutive points from the pool.
    It is worth noting here that FF-Active (Geifman and El-Yaniv, [2017](#bib.bib76))
    and Exploration-P (Yin et al., [2017](#bib.bib245)) resemble the way in which
    random queries are used in the early stages of AL to enhance AL’s exploration
    ability, which prevents AL from falling into the trap of insufficient sample diversity.
    Similarly, to solve the sampling bias problem in batch querying, the diversity
    of batch query samples is increased. The Core-set approach (Sener and Savarese,
    [2018](#bib.bib193)) attempts to solve this problem by constructing a core subset.
    A further attempt was made to solve the k-Center problem (Farahani and Hekmatfar,
    [2009](#bib.bib63)) by building a core subset so that the model learned on the
    selected core set will be more competitive than the rest of the data. However,
    the Core-set approach requires a large distance matrix to be built on the unlabeled
    dataset, meaning that this search process is computationally expensive; this disadvantage
    will become more apparent on large-scale unlabeled datasets (Ash et al., [2020](#bib.bib15)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: FF-Active（最远优先主动学习）（Geifman 和 El-Yaniv，[2017](#bib.bib76)）基于这一思想，利用最远优先遍历神经激活空间中的表示层，从池中查询连续的点。在这里需要注意的是，FF-Active（Geifman
    和 El-Yaniv，[2017](#bib.bib76)）和 Exploration-P（Yin 等，[2017](#bib.bib245)）类似于在主动学习（AL）早期阶段使用随机查询的方式，以增强
    AL 的探索能力，从而防止 AL 陷入样本多样性不足的陷阱。同样，为了解决批量查询中的采样偏差问题，增加了批量查询样本的多样性。Core-set 方法（Sener
    和 Savarese，[2018](#bib.bib193)）尝试通过构建核心子集来解决这一问题。进一步的尝试是通过构建核心子集来解决 k-Center 问题（Farahani
    和 Hekmatfar，[2009](#bib.bib63)），使得在选定核心集上学习的模型比其他数据更具竞争力。然而，Core-set 方法需要在未标记数据集上构建一个大型距离矩阵，这意味着这一搜索过程在计算上非常昂贵；这一缺点在大规模未标记数据集上会变得更加明显（Ash
    等，[2020](#bib.bib15)）。
- en: Active Palmprint Recognition (Du et al., [2019](#bib.bib57)) applies DeepAL
    to high-dimensional and complex palmprint recognition data. Similar to the core
    set concept, (Du et al., [2019](#bib.bib57)) regards AL as a binary classification
    task. It is expected that the labeled and unlabeled sample sets will have the
    same data distribution, making the two difficult to distinguish; that is, the
    goal is to find a labeled core subset with the same distribution as the original
    dataset. More specifically, due to the heuristic generative model simulation data
    distribution being difficult to train and unsuitable for high-dimensional and
    complex data such as palm prints, the author considers whether the sample can
    be positively distinguished from the unlabeled or labeled dataset with a high
    degree of confidence. Those samples that can be clearly distinguished are obviously
    different from the data distribution of the core annotation subset. These samples
    will then be added to the annotation dataset for the next round of training. Previous
    core-set-based methods (Geifman and El-Yaniv, [2017](#bib.bib76); Sener and Savarese,
    [2018](#bib.bib193)) often simply try to query data points as far as possible
    to cover all points of the data manifold without considering the density, which
    results in the queried data points overly representing sample points from manifold
    sparse areas. Similar to (Du et al., [2019](#bib.bib57)), DAL (Discriminative
    Active Learning) (Gissin and Shalev-Shwartz, [2019](#bib.bib80)) also regards
    AL as a binary classification task and further aims to make the queried labeled
    dataset indistinguishable from the unlabeled dataset. The key advantage of DAL
    (Gissin and Shalev-Shwartz, [2019](#bib.bib80)) is that it can sample from the
    unlabeled dataset in proportion to the data density, without biasing the sample
    points in the sparse popular domain. Moreover, the method proposed by DAL (Gissin
    and Shalev-Shwartz, [2019](#bib.bib80)) is not limited to classification tasks,
    which are conceptually easy to transfer to other new tasks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 主动掌纹识别（Du 等， [2019](#bib.bib57)）将 DeepAL 应用于高维复杂的掌纹识别数据。类似于核心集合概念，（Du 等，[2019](#bib.bib57)）将
    AL 视为一个二分类任务。预计标记和未标记样本集将具有相同的数据分布，使得这两者难以区分；即目标是找到一个标记的核心子集，其分布与原始数据集相同。更具体地，由于启发式生成模型模拟数据分布难以训练且不适合像掌纹这样的高维复杂数据，作者考虑样本是否能以高度信心从未标记或已标记数据集中明确区分。那些能够明显区分的样本显然与核心标注子集的数据分布不同。这些样本将被添加到标注数据集中，进行下一轮训练。以往基于核心集合的方法（Geifman
    和 El-Yaniv，[2017](#bib.bib76)；Sener 和 Savarese，[2018](#bib.bib193)）通常只是尽可能查询数据点以覆盖数据流形的所有点，而不考虑密度，这导致查询的数据点过度代表来自流形稀疏区域的样本点。类似于（Du
    等，[2019](#bib.bib57)），DAL（区分性主动学习）（Gissin 和 Shalev-Shwartz，[2019](#bib.bib80)）也将
    AL 视为一个二分类任务，并进一步旨在使查询的标记数据集与未标记数据集无法区分。DAL（Gissin 和 Shalev-Shwartz，[2019](#bib.bib80)）的关键优势在于，它可以按数据密度从未标记数据集中抽样，而不会偏向稀疏热门领域的样本点。此外，DAL（Gissin
    和 Shalev-Shwartz，[2019](#bib.bib80)）提出的方法不限于分类任务，这在概念上容易转移到其他新任务。
- en: In addition to the corresponding query strategy, some researchers have also
    considered the impact of batch query size on query performance. For example, (Kirsch
    et al., [2019](#bib.bib116); Zhdanov, [2019](#bib.bib256); Ash et al., [2020](#bib.bib15);
    Pinsler et al., [2019](#bib.bib163)) focus primarily on the optimization of query
    strategies in smaller batches, while (Chitta et al., [2019](#bib.bib39)) recommended
    expanding the query scale of AL for large-scale sampling (10k or 500k samples
    at a time). Moreover, by integrating hundreds of models and reusing intermediate
    checkpoints, the distributed searching of training data on large-scale labeled
    datasets can be efficiently realized with a small computational cost. (Chitta
    et al., [2019](#bib.bib39)) also proved that the performance of using the entire
    dataset for training is not the upper limit of performance, as well as that AL
    based on subsets specifically may yield better performance.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了相应的查询策略，一些研究人员还考虑了批量查询大小对查询性能的影响。例如，（Kirsch 等， [2019](#bib.bib116)；Zhdanov，
    [2019](#bib.bib256)；Ash 等， [2020](#bib.bib15)；Pinsler 等， [2019](#bib.bib163)）主要关注较小批量的查询策略优化，而（Chitta
    等， [2019](#bib.bib39)）建议扩大 AL 的查询规模以进行大规模采样（每次 10k 或 500k 样本）。此外，通过整合数百个模型并重用中间检查点，可以在大规模标记数据集上高效地进行分布式训练数据搜索，计算成本较低。（Chitta
    等， [2019](#bib.bib39)）还证明，使用整个数据集进行训练的性能并不是性能的上限，基于子集的 AL 可能会带来更好的性能。
- en: Furthermore, the attributes of the dataset itself also have an important impact
    on the performance of DeepAL. With this in mind, GA (Gradient Analysis) (Vodrahalli
    et al., [2018](#bib.bib226)) assesses the relative importance of image data in
    common datasets and proposes a general data analysis tool design to facilitate
    a better understanding of the diversity of training examples in the dataset. GA
    (Vodrahalli et al., [2018](#bib.bib226)) finds that not all datasets can be trained
    on a small sub-sample set because the relative difference of sample importance
    in some datasets is almost negligible; therefore, it is not advisable to blindly
    use smaller sub-datasets in the AL context. In addition, (Beluch et al., [2018](#bib.bib20))
    finds that compared with the Bayesian deep learning approach (Monte-Carlo dropout
    (Gal et al., [2017](#bib.bib73))) and density-based (Sener and Savarese, [2017](#bib.bib192))
    methods, ensemble-based AL can effectively offset the imbalance of categories
    in the dataset during the acquisition process, resulting in more calibration prediction
    uncertainty, and thus better performance.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，数据集本身的属性对 DeepAL 的性能也有重要影响。考虑到这一点，GA（梯度分析）（Vodrahalli 等， [2018](#bib.bib226)）评估了常见数据集中图像数据的相对重要性，并提出了一种通用数据分析工具设计，以促进对数据集中训练示例多样性的更好理解。GA（Vodrahalli
    等， [2018](#bib.bib226)）发现，并非所有数据集都可以在小子样本集上进行训练，因为某些数据集中样本重要性的相对差异几乎可以忽略；因此，在
    AL 背景下盲目使用较小的子数据集是不明智的。此外，（Beluch 等， [2018](#bib.bib20)）发现，与贝叶斯深度学习方法（Monte-Carlo
    dropout（Gal 等， [2017](#bib.bib73)））和基于密度的方法（Sener 和 Savarese，[2017](#bib.bib192)）相比，基于集成的
    AL 能够有效抵消数据集中类别的不平衡，从而产生更多的校准预测不确定性，并因此表现更好。
- en: In general, density-based methods primarily consider the selection of core subsets
    from the perspective of data distribution. There are relatively few related research
    methods, which suggests a new possible direction for sample querying.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，基于密度的方法主要从数据分布的角度考虑核心子集的选择。相关的研究方法相对较少，这为样本查询提供了一个新的可能方向。
- en: '![Refer to caption](img/678d92bedddcdd441c70d4c12b579365.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/678d92bedddcdd441c70d4c12b579365.png)'
- en: (a) Active learning pipeline.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 主动学习管道。
- en: '![Refer to caption](img/c9f2e154885bc2b054af2cf9514e8f5f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c9f2e154885bc2b054af2cf9514e8f5f.png)'
- en: (b) Reinforced Active Learning (RAL) (Haußmann et al., [2019](#bib.bib87)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 强化主动学习（RAL）（Haußmann 等， [2019](#bib.bib87)）。
- en: '![Refer to caption](img/26b666c441683602301671bc1fc742e2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/26b666c441683602301671bc1fc742e2.png)'
- en: (c) Deep Reinforcement Active Learning (DRAL) (Liu et al., [2019](#bib.bib137)).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 深度强化主动学习（DRAL）（Liu 等， [2019](#bib.bib137)）。
- en: Figure 4\. Comparison of standard AL, RAL (Haußmann et al., [2019](#bib.bib87))
    and DRAL (Liu et al., [2019](#bib.bib137)) pipelines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 标准 AL、RAL（Haußmann 等， [2019](#bib.bib87)）和 DRAL（Liu 等， [2019](#bib.bib137)）管道的比较。
- en: 3.1.5\. Automated Design of DeepAL
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5\. DeepAL 的自动化设计
- en: 'DeepAL is composed of two parts: deep learning and active learning. Manually
    designing these two parts requires a lot of energy and their performance is severely
    limited by the experience of researchers. Therefore, it has important significance
    to consider how to automate the design of deep learning models and active learning
    query strategies in DeepAL.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAL 由两个部分组成：深度学习和主动学习。手动设计这两个部分需要大量的精力，并且它们的性能受限于研究人员的经验。因此，考虑如何自动化设计深度学习模型和主动学习查询策略在
    DeepAL 中具有重要意义。
- en: To this end, (Fang et al., [2017](#bib.bib62)) redefines the heuristic AL algorithm
    as a reinforcement learning problem and introduces a new description through a
    clear selection strategy. In addition, some researchers have also noted that,
    in traditional AL workflows, the acquisition function is often regarded as a fixed
    known prior, and that it will not be known whether this acquisition function is
    appropriate until the label budget is exhausted. This makes it impossible to flexibly
    and quickly tune the acquisition function. Accordingly, one good option may be
    to use reinforcement learning to dynamically tune the acquisition function. RAL
    (Reinforced Active Learning) (Haußmann et al., [2019](#bib.bib87)) proposes to
    use BNN as a learning predictor for acquisition functions. As such, all probability
    information provided by the BNN predictor will be combined to obtain a comprehensive
    probability distribution; subsequently, the probability distribution is sent to
    a BNN probabilistic policy network, which performs reinforcement learning in each
    labeling round based on the oracle feedback. This feedback will fine-tune the
    acquisition function, thereby continuously improving its quality. DRAL (Deep Reinforcement
    Active Learning) (Liu et al., [2019](#bib.bib137)) adopts a similar idea and designs
    a deep reinforcement active learning framework for the person Re-ID task. This
    approach uses the idea of reinforcement learning to dynamically adjust the acquisition
    function so as to obtain high-quality query samples. Fig.[4](#S3.F4 "Figure 4
    ‣ 3.1.4\. Density-based Methods ‣ 3.1\. Query Strategy Optimization in DeepAL
    ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning") presents a comparison
    between traditional AL, RAL and DRAL pipelines. The pipeline of AL is shown in
    Fig.[4a](#S3.F4.sf1 "In Figure 4 ‣ 3.1.4\. Density-based Methods ‣ 3.1\. Query
    Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep
    Active Learning"). The standard AL pipeline usually consists of three parts. The
    oracle provides a set of labeled data; the predictor (here, BNN) is used to learn
    these data and provides predictable uncertainty for the guide. The guide is usually
    a fixed, hard-coded acquisition function that picks the next sample for the oracle
    to restart the cycle. The pipeline of RAL (Reinforced Active Learning) (Haußmann
    et al., [2019](#bib.bib87)) is shown in Fig.[4b](#S3.F4.sf2 "In Figure 4 ‣ 3.1.4\.
    Density-based Methods ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep
    Active Learning ‣ A Survey of Deep Active Learning"). RAL replaces the fixed acquisition
    function with the policy BNN. The policy BNN learns in a probabilistic manner,
    obtains feedback from the oracle, and learns how to select the next optimal sample
    point (new parts in red) in a reinforcement learning-based manner. Therefore,
    RAL can adjust the acquisition function more flexibly to adapt to the existing
    dataset. The pipeline of DRAL (Deep Reinforcement Active Learning) (Liu et al.,
    [2019](#bib.bib137)) is shown in Fig.[4c](#S3.F4.sf3 "In Figure 4 ‣ 3.1.4\. Density-based
    Methods ‣ 3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning
    ‣ A Survey of Deep Active Learning"). DRAL utilizes a deep reinforcement active
    learning framework for the person Re-ID task. For each query anchor (probe), the
    agent (reinforcement active learner) will select sequential instances from the
    gallery pool during the active learning process and hand it to the oracle to obtain
    manual annotation with binary feedback (positive/negative). The state evaluates
    the similarity relationships between all instances and calculates rewards based
    on oracle feedback to adjust agent queries.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，（Fang等，[2017](#bib.bib62)）将启发式AL算法重新定义为强化学习问题，并通过清晰的选择策略引入了新的描述。此外，一些研究人员还指出，在传统的AL工作流中，获取函数通常被视为固定的已知先验，并且直到标签预算用尽之前无法确定此获取函数是否合适。这使得灵活和快速地调整获取函数变得不可能。因此，使用强化学习动态调整获取函数可能是一个不错的选择。RAL（Reinforced
    Active Learning）（Haußmann等，[2019](#bib.bib87)）提出使用BNN作为获取函数的学习预测器。因此，BNN预测器提供的所有概率信息将被组合以获得综合概率分布；随后，该概率分布被发送到BNN概率策略网络，基于oracle反馈在每次标注轮次中进行强化学习。这个反馈将微调获取函数，从而不断提高其质量。DRAL（Deep
    Reinforcement Active Learning）（Liu等，[2019](#bib.bib137)）采用了类似的思想，并为人员Re-ID任务设计了一个深度强化主动学习框架。这种方法利用强化学习的思想动态调整获取函数，以获取高质量的查询样本。图[4](#S3.F4
    "Figure 4 ‣ 3.1.4\. Density-based Methods ‣ 3.1\. Query Strategy Optimization
    in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning")展示了传统AL、RAL和DRAL管道的比较。AL的管道见图[4a](#S3.F4.sf1
    "In Figure 4 ‣ 3.1.4\. Density-based Methods ‣ 3.1\. Query Strategy Optimization
    in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning")。标准AL管道通常由三部分组成。oracle提供一组标记数据；预测器（在这里是BNN）用于学习这些数据，并为引导提供可预测的不确定性。引导通常是一个固定的硬编码获取函数，用于选择下一个样本供oracle重新开始循环。RAL（Reinforced
    Active Learning）（Haußmann等，[2019](#bib.bib87)）的管道见图[4b](#S3.F4.sf2 "In Figure
    4 ‣ 3.1.4\. Density-based Methods ‣ 3.1\. Query Strategy Optimization in DeepAL
    ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning")。RAL用策略BNN替代了固定获取函数。策略BNN以概率方式学习，从oracle获取反馈，并以基于强化学习的方式学习如何选择下一个最佳样本点（新部分以红色标出）。因此，RAL可以更灵活地调整获取函数以适应现有数据集。DRAL（Deep
    Reinforcement Active Learning）（Liu等，[2019](#bib.bib137)）的管道见图[4c](#S3.F4.sf3 "In
    Figure 4 ‣ 3.1.4\. Density-based Methods ‣ 3.1\. Query Strategy Optimization in
    DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning")。DRAL利用深度强化主动学习框架进行人员Re-ID任务。对于每个查询锚点（probe），代理（强化主动学习者）将在主动学习过程中从画廊池中选择顺序实例，并将其交给oracle以获得带有二元反馈（正/负）的手动注释。状态评估所有实例之间的相似性关系，并根据oracle反馈计算奖励以调整代理查询。
- en: On the other hand, Active-iNAS (Active Learning with incremental Neural Architecture
    Search) (Geifman and El-Yaniv, [2019](#bib.bib77)) notices that most previous
    DeepAL methods (Aghdam et al., [2019](#bib.bib5); Alahmari et al., [2019](#bib.bib7);
    Kwolek et al., [2019](#bib.bib124)) assume that a suitable DL model has been designed
    for the current task, meaning that their primary focus is on how to design an
    effective query mechanism; however, the existing DL model is not necessarily optimal
    for the current DeepAL task. Active-iNAS (Geifman and El-Yaniv, [2019](#bib.bib77))
    accordingly challenges this assumption and uses NAS (neural architecture search)
    (Ren et al., [2020](#bib.bib175)) technology to dynamically search for the most
    effective model architectures while conducting active learning. There is also
    some work devoted to providing a convenient performance comparison platform for
    DeepAL; for example, (Munjal et al., [2020](#bib.bib149)) discusses and studies
    the robustness and reproducibility of the DeepAL method in detail, and presents
    many useful suggestions.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Active-iNAS（增量神经架构搜索的主动学习）（Geifman和El-Yaniv，[2019](#bib.bib77)）注意到，大多数先前的DeepAL方法（Aghdam
    et al., [2019](#bib.bib5); Alahmari et al., [2019](#bib.bib7); Kwolek et al.,
    [2019](#bib.bib124)）假设已经为当前任务设计了一个合适的DL模型，这意味着它们主要关注如何设计有效的查询机制；然而，现有的DL模型不一定对当前的DeepAL任务最优。Active-iNAS（Geifman和El-Yaniv，[2019](#bib.bib77)）因此挑战了这一假设，并使用NAS（神经架构搜索）（Ren
    et al., [2020](#bib.bib175)）技术在进行主动学习的同时动态搜索最有效的模型架构。还有一些工作致力于为DeepAL提供一个方便的性能比较平台；例如，（Munjal
    et al., [2020](#bib.bib149)）详细讨论和研究了DeepAL方法的鲁棒性和可重复性，并提出了许多有用的建议。
- en: 'In general, these query strategies are not independent of each other but are
    rather interrelated. Batch-based BMDAL provides the basis for the update training
    of AL query samples on the DL model. Although the query strategies in DeepAL are
    rich and complex, they are largely designed to take the diversity and uncertainty
    of query batches in BMDAL into account. Previous uncertainty-based methods often
    ignore the diversity in the batch and can thus be roughly divided into two categories:
    those that design a mechanism that explicitly encourages batch diversity in the
    input or learning representation space, and those that directly measure the mutual
    information (MI) of the entire batch.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，这些查询策略并不是彼此独立的，而是相互关联的。基于批处理的BMDAL为AL查询样本在DL模型上的更新训练提供了基础。尽管DeepAL中的查询策略丰富而复杂，但它们主要设计用于考虑BMDAL中查询批次的多样性和不确定性。以前基于不确定性的方法通常忽略了批次中的多样性，因此可以大致分为两类：一类是设计一个明确鼓励批次多样性的机制，无论是在输入还是学习表示空间中；另一类是直接测量整个批次的互信息（MI）。
- en: 3.2\. Data Expansion of Labeled Samples in DeepAL
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. DeepAL中的标记样本数据扩展
- en: AL often requires only a small amount of labeled sample data to realize learning
    and model updating, while DL requires a large amount of labeled data for effective
    training. Therefore, the combination of AL and DL requires as much as possible
    to use the data strategy without consuming too much human resources to achieve
    DeepAL model training. Most previous DeepAL methods (Zhao et al., [2017](#bib.bib254))
    often only train on the labeled sample set sampled by the query strategy. However,
    this ignores the existence of existing unlabeled datasets, meaning that the corresponding
    data expansion and training strategies are not fully utilized. These strategies
    help to improve the problem of insufficient labeled data in DeepAL training without
    adding to the manual labeling costs. Therefore, the study of these strategies
    is also quite meaningful.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习（AL）通常只需要少量标记样本数据即可实现学习和模型更新，而深度学习（DL）则需要大量的标记数据来进行有效的训练。因此，AL和DL的结合需要尽可能利用数据策略，而不消耗过多的人力资源来实现DeepAL模型训练。大多数先前的DeepAL方法（Zhao
    et al., [2017](#bib.bib254)）通常只在通过查询策略采样得到的标记样本集上进行训练。然而，这忽略了现有未标记数据集的存在，意味着相应的数据扩展和训练策略没有得到充分利用。这些策略有助于改善DeepAL训练中标记数据不足的问题，而无需增加人工标记成本。因此，这些策略的研究也具有相当大的意义。
- en: '![Refer to caption](img/90803881ff4387bbc2004f80776167f8.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/90803881ff4387bbc2004f80776167f8.png)'
- en: 'Figure 5\. In CEAL (Wang et al., [2017](#bib.bib230)), the overall framework
    of DeepAL is utilized. CEAL (Wang et al., [2017](#bib.bib230)) gradually feeds
    the samples from the unlabeled dataset to the initialized CNN, after which the
    CNN classifier outputs two types of samples: a small number of uncertain samples
    and a large number of samples with high prediction confidence. A small number
    of uncertain samples are labeled through the oracle, and the CNN classifier is
    used to automatically assign pseudo-labels to a large number of high-prediction
    confidence samples. These two types of samples are then used to fine-tune the
    CNN, and the updated process is repeated.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 在 CEAL（Wang 等，[2017](#bib.bib230)）中，利用了 DeepAL 的整体框架。CEAL（Wang 等，[2017](#bib.bib230)）逐渐将未标记数据集中的样本送入初始化的
    CNN，然后 CNN 分类器输出两种类型的样本：少量不确定样本和大量高预测置信度样本。少量的不确定样本通过 oracle 进行标记，CNN 分类器用于自动为大量高预测置信度样本分配伪标签。然后，这两种类型的样本被用来微调
    CNN，更新过程不断重复。
- en: For example, CEAL (Cost-Effective Active Learning) (Wang et al., [2017](#bib.bib230))
    enriches the training set by assigning pseudo-labels to samples with high confidence
    in model prediction in addition to the labeled dataset sampled by the query strategy.
    This expanded training set is then also used in the training of the DL model.
    This strategy is shown in Fig.[5](#S3.F5 "Figure 5 ‣ 3.2\. Data Expansion of Labeled
    Samples in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning").
    Another very popular strategy involves performing unsupervised training on labeled
    and unlabeled datasets and incorporating other strategies to train the entire
    network structure. For example, WI-DL (Liu et al., [2017](#bib.bib136)) notes
    that full DBN training requires a large number of training samples, and it is
    impractical to apply DBN to a limited training set in an AL context. Therefore,
    in order to improve the training efficiency of DBN, WI-DL employs a combination
    of unsupervised feature learning on all datasets and supervised fine-tuning on
    labeled datasets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CEAL（成本有效主动学习）（Wang 等，[2017](#bib.bib230)）通过为模型预测高置信度的样本分配伪标签来丰富训练集，除了由查询策略采样的标记数据集之外。然后，这个扩展的训练集也被用于DL模型的训练。该策略如图[5](#S3.F5
    "图 5 ‣ 3.2\. 深度主动学习中的标记样本数据扩展 ‣ 3\. 深度主动学习 ‣ 深度主动学习调查")所示。另一个非常流行的策略涉及对标记和未标记数据集进行无监督训练，并结合其他策略来训练整个网络结构。例如，WI-DL（Liu
    等，[2017](#bib.bib136)）指出，完整的DBN训练需要大量训练样本，而在AL背景下将DBN应用于有限的训练集是不切实际的。因此，为了提高DBN的训练效率，WI-DL采用了在所有数据集上进行无监督特征学习和在标记数据集上进行监督微调的组合。
- en: At the same time, some researchers have considered using GAN (Generative Adversarial
    Networks) for data augmentation. For example, GAAL (Generative Adversarial Active
    Learning) (Zhu and Bento, [2017](#bib.bib260)) introduced the GAN to the AL query
    method for the first time. GAAL aims to use generative learning to generate samples
    with more information than the original dataset. However, random data augmentation
    does not guarantee that the generated samples will have more information than
    those contained in the original data, and could thus represent a waste of computing
    resources. Accordingly, BGADL (Bayesian Generative Active Deep Learning) (Tran
    et al., [2019](#bib.bib224)) expands the idea of GAAL (Zhu and Bento, [2017](#bib.bib260))
    and proposes a Bayesian generative active deep learning method. More specifically,
    BGADL combines the generative adversarial active learning (Zhu and Bento, [2017](#bib.bib260)),
    Bayesian data augmentation (Tran et al., [2017](#bib.bib225)), ACGAN (Auxiliary-Classifier
    Generative Adversarial Networks) (Odena et al., [2017](#bib.bib156)) and VAE (Variational
    Autoencoder) (Kingma and Welling, [2014](#bib.bib115)) methods, with the aim of
    generating samples of disagreement regions (Settles, [2012](#bib.bib195)) belonging
    to different categories. Structure comparison between GAAL and BGADL is presented
    in Fig.[6](#S3.F6 "Figure 6 ‣ 3.2\. Data Expansion of Labeled Samples in DeepAL
    ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning").
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，一些研究人员考虑使用 GAN（生成对抗网络）进行数据增强。例如，GAAL（生成对抗主动学习）（Zhu 和 Bento, [2017](#bib.bib260)）首次将
    GAN 引入主动学习查询方法。GAAL 旨在利用生成学习生成比原始数据集包含更多信息的样本。然而，随机数据增强不能保证生成的样本会比原始数据中包含的样本有更多信息，因此可能会浪费计算资源。因此，BGADL（贝叶斯生成主动深度学习）（Tran
    等人, [2019](#bib.bib224)）扩展了 GAAL（Zhu 和 Bento, [2017](#bib.bib260)）的思想，并提出了一种贝叶斯生成主动深度学习方法。更具体地说，BGADL
    结合了生成对抗主动学习（Zhu 和 Bento, [2017](#bib.bib260)）、贝叶斯数据增强（Tran 等人, [2017](#bib.bib225)）、ACGAN（辅助分类生成对抗网络）（Odena
    等人, [2017](#bib.bib156)）和 VAE（变分自编码器）（Kingma 和 Welling, [2014](#bib.bib115)）方法，旨在生成属于不同类别的异议区域样本（Settles,
    [2012](#bib.bib195)）。GAAL 和 BGADL 的结构比较见图 [6](#S3.F6 "图6 ‣ 3.2\. 深度主动学习中的标记样本数据扩展
    ‣ 3\. 深度主动学习 ‣ 深度主动学习综述")。
- en: '![Refer to caption](img/b19cbe40f94e85537437955b9201f412.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b19cbe40f94e85537437955b9201f412.png)'
- en: (a) Generative adversarial active learning (GAAL).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 生成对抗主动学习（GAAL）。
- en: '![Refer to caption](img/702d2df4b7daa6153be625818f78624a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/702d2df4b7daa6153be625818f78624a.png)'
- en: (b) Bayesian generative active deep learning (BGADL).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 贝叶斯生成主动深度学习（BGADL）。
- en: Figure 6\. Structure comparison chart of GAAL (Zhu and Bento, [2017](#bib.bib260))
    and BGADL (Tran et al., [2019](#bib.bib224)). For more details, please see (Tran
    et al., [2019](#bib.bib224)).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6\. GAAL（Zhu 和 Bento, [2017](#bib.bib260)）和 BGADL（Tran 等人, [2019](#bib.bib224)）的结构比较图。更多细节，请参见（Tran
    等人, [2019](#bib.bib224)）。
- en: Subsequently, VAAL (Sinha et al., [2019](#bib.bib205)) and ARAL (Adversarial
    Representation Active Learning) (Mottaghi and Yeung, [2019](#bib.bib147)) borrowed
    from several previous methods (Liu et al., [2017](#bib.bib136); Zhu and Bento,
    [2017](#bib.bib260); Tran et al., [2019](#bib.bib224)) not only to train the network
    using labeled and unlabeled datasets but also to introduce generative adversarial
    learning into the network architecture for data augmentation purposes, thereby
    further improving the learning ability of the network. In more detail, VAAL (Sinha
    et al., [2019](#bib.bib205)) noticed that the batch-based query strategy based
    on uncertainty not only readily leads to insufficient sample diversity, but is
    also highly susceptible to interference from outliers. In addition, density-based
    methods (Sener and Savarese, [2018](#bib.bib193)) are susceptible to $p$-norm
    limitations when applied to high-dimensional data, resulting in calculation distances
    that are too concentrated (Donoho et al., [2000](#bib.bib55)). To this end, VAAL
    (Sinha et al., [2019](#bib.bib205)) proposes to use the adversarial learning representation
    method to distinguish between the potential spatial coding features of labeled
    and unlabeled data, thus reducing interference from outliers. VAAL (Sinha et al.,
    [2019](#bib.bib205)) also uses labeled and unlabeled data to jointly train a VAE
    (Kingma and Welling, [2014](#bib.bib115); Sohn et al., [2015](#bib.bib209)) in
    a semi-supervised manner; the goal here is to deceive the adversarial network
    (Goodfellow et al., [2014](#bib.bib81)) into predicting that all data points come
    from the labeled pool, in order to solve the problem of distance concentration.
    VAAL (Sinha et al., [2019](#bib.bib205)) can learn an effective low-dimensional
    latent representation on a large-scale dataset, and further provides an effective
    sampling method by jointly learning the representation form and uncertainty.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，VAAL（Sinha等，[2019](#bib.bib205)）和ARAL（对抗表示主动学习）（Mottaghi和Yeung，[2019](#bib.bib147)）借鉴了几种先前的方法（Liu等，[2017](#bib.bib136)；Zhu和Bento，[2017](#bib.bib260)；Tran等，[2019](#bib.bib224)），不仅用于训练网络，使用标记和未标记的数据集，还将生成对抗学习引入网络结构中以进行数据增强，从而进一步提高了网络的学习能力。更详细地说，VAAL（Sinha等，[2019](#bib.bib205)）注意到，基于不确定性的批处理查询策略不仅容易导致样本多样性不足，还极易受到离群点的干扰。此外，基于密度的方法（Sener和Savarese，[2018](#bib.bib193)）在应用于高维数据时容易受到$p$-范数限制，导致计算的距离过于集中（Donoho等，[2000](#bib.bib55)）。为此，VAAL（Sinha等，[2019](#bib.bib205)）提出使用对抗学习表示方法来区分标记和未标记数据的潜在空间编码特征，从而减少离群点的干扰。VAAL（Sinha等，[2019](#bib.bib205)）还使用标记和未标记的数据以半监督的方式共同训练一个VAE（Kingma和Welling，[2014](#bib.bib115)；Sohn等，[2015](#bib.bib209)）；这里的目标是欺骗对抗网络（Goodfellow等，[2014](#bib.bib81)），使其预测所有数据点来自标记池，从而解决距离集中问题。VAAL（Sinha等，[2019](#bib.bib205)）可以在大规模数据集上学习到有效的低维潜在表示，并通过共同学习表示形式和不确定性，进一步提供了一种有效的采样方法。
- en: Subsequently, ARAL (Mottaghi and Yeung, [2019](#bib.bib147)) expanded VAAL (Sinha
    et al., [2019](#bib.bib205)), aiming to use as few manual annotation samples as
    possible while still making full use of the existing or generated data information
    in order to improve the model’s learning ability. In addition to using labeled
    and unlabeled datasets, ARAL (Mottaghi and Yeung, [2019](#bib.bib147)) also uses
    samples produced by deep production networks to jointly train the entire model.
    ARAL (Mottaghi and Yeung, [2019](#bib.bib147)) comprises both VAAL (Sinha et al.,
    [2019](#bib.bib205)) and adversarial representation learning (Donahue and Simonyan,
    [2019](#bib.bib54)). By using VAAL (Sinha et al., [2019](#bib.bib205)) to learn
    the potential feature representation space of the labeled and unlabeled data,
    the unlabeled samples with the largest amount of information can be selected accordingly.
    At the same time, both real and generated data are used to enhance the model’s
    learning ability through confrontational representation learning (Donahue and
    Simonyan, [2019](#bib.bib54)). Similarly, TA-VAAL (Kim et al., [2020](#bib.bib113))
    also extends VAAL by using the global data structure from VAAL and local task-related
    information from the learning loss for sample querying purposes. We present the
    framework of ARAL (Mottaghi and Yeung, [2019](#bib.bib147)) in Fig.[7](#S3.F7
    "Figure 7 ‣ 3.2\. Data Expansion of Labeled Samples in DeepAL ‣ 3\. Deep Active
    Learning ‣ A Survey of Deep Active Learning").
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，ARAL（Mottaghi 和 Yeung，[2019](#bib.bib147)）扩展了 VAAL（Sinha 等，[2019](#bib.bib205)），旨在尽可能少地使用人工标注样本，同时充分利用现有或生成的数据，以提高模型的学习能力。除了使用标记和未标记的数据集外，ARAL（Mottaghi
    和 Yeung，[2019](#bib.bib147)）还利用深度生成网络生成的样本来联合训练整个模型。ARAL（Mottaghi 和 Yeung，[2019](#bib.bib147)）包含了
    VAAL（Sinha 等，[2019](#bib.bib205)）和对抗性表示学习（Donahue 和 Simonyan，[2019](#bib.bib54)）。通过使用
    VAAL（Sinha 等，[2019](#bib.bib205)）学习标记和未标记数据的潜在特征表示空间，可以相应地选择信息量最大的未标记样本。同时，使用真实数据和生成的数据，通过对抗性表示学习（Donahue
    和 Simonyan，[2019](#bib.bib54)）来增强模型的学习能力。类似地，TA-VAAL（Kim 等，[2020](#bib.bib113)）通过使用
    VAAL 的全局数据结构和学习损失中的局部任务相关信息来扩展 VAAL，以用于样本查询。我们在图 [7](#S3.F7 "图 7 ‣ 3.2\. 深度主动学习中的标记样本的数据扩展
    ‣ 3\. 深度主动学习 ‣ 深度主动学习综述") 中展示了 ARAL（Mottaghi 和 Yeung，[2019](#bib.bib147)）的框架。
- en: '![Refer to caption](img/37f8a98fc64bd18ed5218999752cd0a8.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/37f8a98fc64bd18ed5218999752cd0a8.png)'
- en: Figure 7\. The overall structure of ARAL (Mottaghi and Yeung, [2019](#bib.bib147)).
    ARAL uses not only real datasets (both labeled and unlabeled), but also generated
    datasets to jointly train the network. The whole network consists of an encoder
    ($E$), generator ($G$), discriminator ($D$), classifier ($C$) and sampler ($S$),
    and all parts of the model are trained together.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. ARAL 的整体结构（Mottaghi 和 Yeung，[2019](#bib.bib147)）。ARAL 不仅使用真实数据集（包括标记和未标记数据），还使用生成的数据集来联合训练网络。整个网络包括一个编码器（`$E$`）、生成器（`$G$`）、鉴别器（`$D$`）、分类器（`$C$`）和采样器（`$S$`），模型的所有部分一起训练。
- en: 'Unlike ARAL (Mottaghi and Yeung, [2019](#bib.bib147)) and VAAL (Sinha et al.,
    [2019](#bib.bib205)), which use labeled and unlabeled datasets for adversarial
    representation learning, SSAL (Semi-Supervised Active Learning) (Simeoni et al.,
    [2019](#bib.bib203)) implements a new training method. More specifically, SSAL
    (Simeoni et al., [2019](#bib.bib203)) uses unsupervised, supervised, and semi-supervised
    learning methods across AL cycles, and makes full use of existing information
    for training without increasing the cost of labeling as much as possible. In more
    detail, the process is as follows: before the AL starts, first use labeled and
    unlabeled data for unsupervised pretraining. In each AL learning cycle, first,
    perform supervised training on the labeled dataset, then perform semi-supervised
    training on all datasets. This represents an attempt to devise a wholly new training
    method. The author finds that, compared with the difference between the sampling
    strategies, this model training method yields a surprising performance improvement.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用标注和未标注数据集进行对抗性表示学习的ARAL（Mottaghi 和 Yeung，[2019](#bib.bib147)）和VAAL（Sinha
    等，[2019](#bib.bib205)）不同，SSAL（半监督主动学习）（Simeoni 等，[2019](#bib.bib203)）实现了一种新的训练方法。更具体地说，SSAL（Simeoni
    等，[2019](#bib.bib203)）在AL循环中使用无监督、监督和半监督学习方法，并充分利用现有信息进行训练，同时尽可能不增加标注成本。更详细地说，过程如下：在AL开始之前，首先使用标注和未标注数据进行无监督预训练。在每个AL学习周期中，首先对标注数据集进行监督训练，然后对所有数据集进行半监督训练。这代表了一种尝试制定全新训练方法的努力。作者发现，与采样策略之间的差异相比，这种模型训练方法带来了令人惊讶的性能提升。
- en: As analyzed above, this kind of exploration of training methods and data utilization
    skills is also essential; in fact, the resultant performance gains may even exceed
    those generated by changing the query strategy. Applying these techniques enables
    the full use of existing data without any associated increase in labeling costs,
    which helps in resolving the issue of the number of AL query samples being insufficient
    to support the updating of the DL model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，这种训练方法和数据利用技巧的探索同样至关重要；实际上，由此产生的性能提升甚至可能超过改变查询策略所带来的效果。应用这些技术可以充分利用现有数据，而无需增加标注成本，这有助于解决AL查询样本数量不足以支持DL模型更新的问题。
- en: 3.3\. DeepAL Generic Framework
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. DeepAL 通用框架
- en: As mentioned in Section [2](#S2 "2\. The necessity and challenge of combining
    DL and AL ‣ A Survey of Deep Active Learning"), a processing pipeline inconsistency
    exists between AL and DL; thus, only fine-tuning the DL model in the AL framework,
    or simply combining AL and DL to treat them as two separate problems, may cause
    divergence. For example, (Asghar et al., [2017](#bib.bib14)) first conducts offline
    supervised training of the DL model on two different types of session datasets
    to grant basic conversational capabilities to the backbone network, then enables
    the online AL stage to interact with human users, enabling the model to be improved
    in an open way based on user feedback. AL-DL (Wang and Shang, [2014](#bib.bib228))
    proposes an AL method for DL models with DBNs, while ADN (Zhou et al., [2010](#bib.bib258))
    further proposes an active deep network architecture for sentiment classification.
    (Stark et al., [2015](#bib.bib214)) proposes an AL algorithm using CNN for captcha
    recognition. However, generally speaking, the above methods first perform routine
    supervised training on this depth model on the labeled dataset, then actively
    sample based on the output of the depth model. There are many similar related
    works (Shelmanov et al., [2019](#bib.bib199); Feng et al., [2019](#bib.bib64))
    that adopt this split-and-splitting approach that treats the training of AL and
    deep models as two independent problems and consequently increases the possibility,
    which the two problems will diverge. Although this method achieved some success
    at the time, a general framework that closely combines the two tasks of DL and
    AL would play a vital role in the performance improvement and promotion of DeepAL.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如第[2](#S2 "2\. 将DL与AL结合的必要性和挑战 ‣ 深度主动学习调查")节中提到，AL和DL之间存在处理管道的不一致性；因此，仅在AL框架中对DL模型进行微调，或将AL和DL简单地结合并将其视为两个独立的问题，可能会导致分歧。例如，（Asghar等，[2017](#bib.bib14)）首先在两种不同类型的会话数据集上对DL模型进行离线监督训练，以赋予基础网络基本的对话能力，然后启用在线AL阶段与人类用户交互，从而使模型能够根据用户反馈以开放的方式进行改进。AL-DL（Wang和Shang，[2014](#bib.bib228)）提出了一种用于DL模型的DBNs的AL方法，而ADN（Zhou等，[2010](#bib.bib258)）进一步提出了一种用于情感分类的主动深度网络架构。（Stark等，[2015](#bib.bib214)）提出了一种使用CNN进行验证码识别的AL算法。然而，总体而言，上述方法首先对深度模型在标记数据集上进行常规监督训练，然后根据深度模型的输出进行主动采样。有许多类似的相关工作（Shelmanov等，[2019](#bib.bib199)；Feng等，[2019](#bib.bib64)）采用这种将AL和深度模型的训练视为两个独立问题的拆分方法，从而增加了这两个问题可能会分歧的可能性。尽管这种方法当时取得了一些成功，但一个将DL和AL两个任务紧密结合的通用框架将在DeepAL的性能提升和推广中发挥至关重要的作用。
- en: 'CEAL (Wang et al., [2017](#bib.bib230)) is one of the first works to combine
    AL and DL in order to solve the problem of depth image classification. CEAL (Wang
    et al., [2017](#bib.bib230)) merges deep convolutional neural networks into AL,
    and consequently proposes a novel DeepAL framework. It sends samples from the
    unlabeled dataset to the CNN step by step, after which the CNN classifier outputs
    two types of samples: a small number of uncertain samples and a large number of
    samples with high prediction confidence. A small number of uncertain samples are
    labeled by the oracle, and the CNN classifier is used to automatically assign
    pseudo-labels to a large number of high-prediction-confidence samples. Then, these
    two types of samples are used to fine-tune the CNN and the update process is repeated.
    In Fig.[5](#S3.F5 "Figure 5 ‣ 3.2\. Data Expansion of Labeled Samples in DeepAL
    ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning"), we present the
    overall framework of CEAL. Moreover, HDAL (Heuristic Deep Active Learning) (Li
    et al., [2017](#bib.bib133)) uses a similar framework for face recognition tasks:
    it combines AL with a deep CNN model to integrate feature learning and AL query
    model training.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: CEAL（Wang等，[2017](#bib.bib230)）是首批将AL和DL结合以解决深度图像分类问题的工作之一。CEAL（Wang等，[2017](#bib.bib230)）将深度卷积神经网络融入AL，并因此提出了一个新颖的DeepAL框架。它将来自未标记数据集的样本逐步发送到CNN中，之后CNN分类器输出两种类型的样本：少量不确定样本和大量高预测置信度样本。少量不确定样本由oracle标记，CNN分类器用于自动为大量高预测置信度样本分配伪标签。然后，这两种类型的样本用于微调CNN，并重复更新过程。在图[5](#S3.F5
    "图 5 ‣ 3.2\. DeepAL中的标记样本数据扩展 ‣ 3\. 深度主动学习 ‣ 深度主动学习调查")中，我们展示了CEAL的总体框架。此外，HDAL（启发式深度主动学习）（Li等，[2017](#bib.bib133)）使用类似的框架进行人脸识别任务：它将AL与深度CNN模型结合，以集成特征学习和AL查询模型训练。
- en: 'In addition, Fig.[1c](#S1.F1.sf3 "In Figure 1 ‣ 1.1\. Deep Learning ‣ 1\. Introduction
    ‣ A Survey of Deep Active Learning") illustrates a widespread general framework
    for DeepAL tasks. Related works include (Yang et al., [2017](#bib.bib244); He
    et al., [2019a](#bib.bib89); Du et al., [2019](#bib.bib57); Zhao et al., [2020](#bib.bib255);
    Lv et al., [2020](#bib.bib141)) , among others. More specifically, (Yang et al.,
    [2017](#bib.bib244)) proposes a framework that uses an FCN (Fully Convolutional
    Network) (Long et al., [2015](#bib.bib138)) and AL to solve the medical image
    segmentation problem using a small number of annotations. It first trains FCN
    on a small number of labeled datasets, then extracts the features of the unlabeled
    datasets through FCN, using these features to estimate the uncertainty and similarity
    of unlabeled samples. This strategy, which is similar to that described in Section
    [3.1.2](#S3.SS1.SSS2 "3.1.2\. Uncertainty-based and Hybrid Query Strategies ‣
    3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning"), helps to select highly uncertain and diverse samples
    to be added to the labeled dataset in order to start the next stage of training.
    Active Palmprint Recognition (Du et al., [2019](#bib.bib57)) proposes a similar
    DeepAL framework as that for the palmprint recognition task. The difference is
    that inspired by domain adaptation (Bendavid et al., [2010](#bib.bib21)), Active
    Palmprint Recognition (Du et al., [2019](#bib.bib57)) regards AL as a binary classification
    task: it is expected that the labeled and unlabeled sample sets have the same
    data distribution, making the two difficult to distinguish. Supervision training
    can be performed directly on a small number of labeled datasets, which reduces
    the burden associated with labeling. (Lv et al., [2020](#bib.bib141)) proposes
    a DeepAL framework for defect detection. This approach performs uncertainty sampling
    based on the output features of the detection model to generate a list of candidate
    samples for annotation. In order to further take the diversity of defect categories
    in the samples into account, (Lv et al., [2020](#bib.bib141)) designs an average
    margin method to control the sampling ratio of each defect category.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图[1c](#S1.F1.sf3 "在图1 ‣ 1.1\. 深度学习 ‣ 1\. 引言 ‣ 深度主动学习的调查")展示了一个广泛应用于 DeepAL
    任务的通用框架。相关工作包括（杨等，[2017](#bib.bib244)；何等，[2019a](#bib.bib89)；杜等，[2019](#bib.bib57)；赵等，[2020](#bib.bib255)；吕等，[2020](#bib.bib141)），等等。更具体地说，（杨等，[2017](#bib.bib244)）提出了一个框架，利用
    FCN（全卷积网络）（龙等，[2015](#bib.bib138)）和 AL 来解决使用少量注释的医学图像分割问题。它首先在少量标注数据集上训练 FCN，然后通过
    FCN 提取未标注数据集的特征，利用这些特征估计未标注样本的不同程度和相似性。这种策略类似于在第 [3.1.2](#S3.SS1.SSS2 "3.1.2\.
    基于不确定性和混合查询策略 ‣ 3.1\. DeepAL 中的查询策略优化 ‣ 3\. 深度主动学习 ‣ 深度主动学习的调查")节中描述的策略，有助于选择不确定性高且多样化的样本，添加到标注数据集中以开始下一阶段的训练。主动掌纹识别（杜等，[2019](#bib.bib57)）提出了一个类似的
    DeepAL 框架，用于掌纹识别任务。不同之处在于，受领域自适应（本达维德等，[2010](#bib.bib21)）的启发，主动掌纹识别（杜等，[2019](#bib.bib57)）将
    AL 视为一个二分类任务：期望标注和未标注样本集具有相同的数据分布，使得这两者难以区分。监督训练可以直接在少量标注数据集上进行，从而减少标注负担。（吕等，[2020](#bib.bib141)）提出了一个用于缺陷检测的
    DeepAL 框架。这种方法基于检测模型的输出特征进行不确定性采样，以生成候选样本列表进行注释。为了进一步考虑样本中缺陷类别的多样性，（吕等，[2020](#bib.bib141)）设计了一种平均边距方法来控制每个缺陷类别的采样比例。
- en: '![Refer to caption](img/e3cdfb57984f500d7c100355b592f62a.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e3cdfb57984f500d7c100355b592f62a.png)'
- en: Figure 8\. Taking a common CNN as an example, this figure presents a comparison
    between the traditional uncertainty measurement method (Yang et al., [2017](#bib.bib244);
    Du et al., [2019](#bib.bib57); Lv et al., [2020](#bib.bib141)) and the uncertainty
    measurement method of synthesizing information in two stages (He et al., [2019a](#bib.bib89);
    Yoo and Kweon, [2019](#bib.bib246); Zhao et al., [2020](#bib.bib255)) (i.e., the
    feature extraction stage and task learning stage).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 以常见的 CNN 为例，图中展示了传统不确定性测量方法（杨等，[2017](#bib.bib244)；杜等，[2019](#bib.bib57)；吕等，[2020](#bib.bib141)）与两阶段信息合成的不确定性测量方法（何等，[2019a](#bib.bib89)；
    Yoo 和 Kweon，[2019](#bib.bib246)；赵等，[2020](#bib.bib255)）的比较（即特征提取阶段和任务学习阶段）。
- en: Different from the above methods, it is common for the final output of the DL
    model to be used as the basis for determining the uncertainty or diversity of
    the sample (Active Palmprint Recognition (Du et al., [2019](#bib.bib57)) uses
    the output of the first fully connected layer). (He et al., [2019a](#bib.bib89);
    Yoo and Kweon, [2019](#bib.bib246); Zhao et al., [2020](#bib.bib255)) also used
    the output of the DL model’s middle hidden layer. As analyzed in Section [3.1.2](#S3.SS1.SSS2
    "3.1.2\. Uncertainty-based and Hybrid Query Strategies ‣ 3.1\. Query Strategy
    Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active Learning")
    and Section [2](#S2 "2\. The necessity and challenge of combining DL and AL ‣
    A Survey of Deep Active Learning"), due to the difference in learning paradigms
    between the deep and shallow models, the traditional uncertainty-based query strategy
    cannot be directly applied to the DL model. In addition, unlike the shallow model,
    the deep model can be regarded as composed of two stages, namely the feature extraction
    stage and the task learning stage. It is inaccurate to use only the output of
    the last layer of the DL model as the basis for evaluating the sample prediction
    uncertainty; this is because the uncertainty of the DL model is in fact composed
    of the uncertainty of these two stages. A schematic diagram of this concept is
    presented in Fig.[8](#S3.F8 "Figure 8 ‣ 3.3\. DeepAL Generic Framework ‣ 3\. Deep
    Active Learning ‣ A Survey of Deep Active Learning"). To this end, AL-MV (Active
    Learning with Multiple Views) (He et al., [2019a](#bib.bib89)) treats the features
    from different hidden layers in the middle of CNN as multiview data, taking the
    uncertainty of both stages into account, and the AL-MV algorithm is designed to
    implement adaptive weighting of the uncertainty of each layer, to enable more
    accurate measurement of the sampling uncertainty. LLAL (Learning Loss for Active
    Learning) (Yoo and Kweon, [2019](#bib.bib246)) also used a similar idea. More
    specifically, LLAL designs a small parameter module of the loss prediction module
    to attach to the target network, using the output of multiple hidden layers of
    the target network as the input of the loss prediction module. The loss prediction
    module is learned to predict the target loss of the unlabeled dataset, while the
    top-$K$ strategy is used to select the query samples. LLAL achieves task-agnostic
    AL framework design at a small parameter cost and further achieves competitive
    performance on a variety of mainstream visual tasks (namely, image classification,
    target detection, and human pose estimation). Similarly, (Zhao et al., [2020](#bib.bib255))
    uses a similar strategy to implement a DeepAL framework for finger bone segmentation
    tasks. (Zhao et al., [2020](#bib.bib255)) uses Deeply Supervised U-Net (Ronneberger
    et al., [2015](#bib.bib176)) as the segmentation network, then subsequently uses
    the output of the multilevel segmentation hidden layer and the output of the last
    layer as the input of AL; this input information is then integrated to form the
    basis for the evaluation of the sample information size. We take LLAL (Yoo and
    Kweon, [2019](#bib.bib246)) as an example to explicate the overall network structure
    of this idea in Fig.[9](#S3.F9 "Figure 9 ‣ 3.3\. DeepAL Generic Framework ‣ 3\.
    Deep Active Learning ‣ A Survey of Deep Active Learning").
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与上述方法不同，深度学习（DL）模型的最终输出通常被用作确定样本不确定性或多样性的基础（例如，主动掌纹识别（Du et al., [2019](#bib.bib57)）使用了第一层全连接层的输出）。(He
    et al., [2019a](#bib.bib89)；Yoo 和 Kweon，[2019](#bib.bib246)；Zhao et al., [2020](#bib.bib255))
    也使用了DL模型中间隐藏层的输出。如在第[3.1.2节](#S3.SS1.SSS2 "3.1.2\. 基于不确定性和混合查询策略 ‣ 3.1\. 深度主动学习中的查询策略优化
    ‣ 3\. 深度主动学习 ‣ 深度主动学习综述")和第[2节](#S2 "2\. 结合DL和AL的必要性和挑战 ‣ 深度主动学习综述")分析的，由于深度模型和浅层模型之间的学习范式差异，传统的基于不确定性的查询策略无法直接应用于DL模型。此外，与浅层模型不同，深度模型可以看作是由两个阶段组成，即特征提取阶段和任务学习阶段。仅使用DL模型最后一层的输出作为评估样本预测不确定性的依据是不准确的，因为DL模型的不确定性实际上是由这两个阶段的不确定性组成的。该概念的示意图见图[8](#S3.F8
    "图 8 ‣ 3.3\. 深度主动学习通用框架 ‣ 3\. 深度主动学习 ‣ 深度主动学习综述")。为此，AL-MV（具有多个视图的主动学习）（He et
    al., [2019a](#bib.bib89)）将CNN中不同隐藏层的特征视为多视图数据，考虑了两个阶段的不确定性，AL-MV算法被设计为对每一层的不确定性进行自适应加权，以实现更准确的采样不确定性测量。LLAL（主动学习的学习损失）（Yoo
    和 Kweon，[2019](#bib.bib246)）也采用了类似的思想。更具体地，LLAL设计了一个小型参数模块，该模块附加到目标网络上，使用目标网络多个隐藏层的输出作为损失预测模块的输入。损失预测模块学习预测未标记数据集的目标损失，同时使用top-$K$策略选择查询样本。LLAL以较小的参数成本实现了任务无关的主动学习框架设计，并在多种主流视觉任务（即图像分类、目标检测和人体姿态估计）上取得了竞争性的性能。同样，（Zhao
    et al., [2020](#bib.bib255)）使用类似策略实现了用于手指骨分割任务的DeepAL框架。（Zhao et al., [2020](#bib.bib255)）使用深度监督U-Net（Ronneberger
    et al., [2015](#bib.bib176)）作为分割网络，然后将多层分割隐藏层的输出和最后一层的输出作为AL的输入；这些输入信息随后被整合，以形成样本信息量评估的基础。我们以LLAL（Yoo
    和 Kweon，[2019](#bib.bib246)）为例，阐明该思想的整体网络结构，见图[9](#S3.F9 "图 9 ‣ 3.3\. 深度主动学习通用框架
    ‣ 3\. 深度主动学习 ‣ 深度主动学习综述")。
- en: '![Refer to caption](img/577f8bbfb38eb8a7d1367b579534ec19.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/577f8bbfb38eb8a7d1367b579534ec19.png)'
- en: Figure 9\. The overall framework of LLAL (Yoo and Kweon, [2019](#bib.bib246)).
    The black line represents the stage of training model parameters, optimizing the
    overall loss composed of target loss and loss-prediction loss. The red line represents
    the sample query phase of AL. The output of the multiple hidden layers of the
    DL model is used as the input of the loss prediction module, while the top-$K$
    unlabeled data points are selected according to the predicted losses and assigned
    labels by the oracle.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. LLAL的整体框架（Yoo and Kweon, [2019](#bib.bib246)）。黑线表示训练模型参数的阶段，优化由目标损失和损失预测损失组成的整体损失。红线表示AL的样本查询阶段。DL模型的多个隐藏层的输出用作损失预测模块的输入，而根据预测的损失选择前$K$个未标记数据点，并由oracle分配标签。
- en: The research on the general framework is highly beneficial to the development
    and promotion of DeepAL, as this task-independent framework can be conveniently
    transplanted to other fields. In the current fusion of DL and AL, DL is primarily
    responsible for feature extraction, while AL is mainly responsible for sample
    querying; thus, a deeper and tighter fusion will help DeepAL achieve better performance.
    Of course, this will require additional exploration and effort on the part of
    researchers. Finally, the challenges of combining DL and AL and related work on
    the corresponding solutions are summarized in Table [1](#S3.T1 "Table 1 ‣ 3.4\.
    DeepAL Stopping Strategy ‣ 3\. Deep Active Learning ‣ A Survey of Deep Active
    Learning").
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对通用框架的研究对DeepAL的发展和推广具有很大的好处，因为这种与任务无关的框架可以方便地移植到其他领域。在当前DL和AL的融合中，DL主要负责特征提取，而AL主要负责样本查询；因此，更深层次和更紧密的融合将有助于DeepAL实现更好的性能。当然，这将需要研究人员额外的探索和努力。最后，DL和AL的结合挑战及其对应解决方案的相关工作在表[1](#S3.T1
    "Table 1 ‣ 3.4\. DeepAL Stopping Strategy ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning")中进行了总结。
- en: 3.4\. DeepAL Stopping Strategy
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. DeepAL停止策略
- en: In addition to querying strategies and training methods, an appropriate stopping
    strategy has an important impact on DeepAL performance. At present, most DeepALs
    (Liu et al., [2017](#bib.bib136); Maldonado and Harabagiu, [2019](#bib.bib142);
    Folmsbee et al., [2018](#bib.bib67); Budd et al., [2019](#bib.bib31); Schröder
    and Niekler, [2020](#bib.bib191)) often use the predefined stopping criterion,
    and when the criterion is satisfied, they stop querying labels from the oracle.
    These predefined stopping criteria include the maximum number of iterations, the
    minimum threshold for changing classification accuracy, the minimum number of
    labeled samples, and the expected accuracy value, etc.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 除了查询策略和训练方法之外，适当的停止策略对DeepAL的性能有重要影响。目前，大多数DeepAL（Liu et al., [2017](#bib.bib136);
    Maldonado and Harabagiu, [2019](#bib.bib142); Folmsbee et al., [2018](#bib.bib67);
    Budd et al., [2019](#bib.bib31); Schröder and Niekler, [2020](#bib.bib191)）通常使用预定义的停止标准，当满足该标准时，他们会停止从oracle处查询标签。这些预定义的停止标准包括最大迭代次数、分类准确性变化的最小阈值、标记样本的最小数量以及期望的准确度值等。
- en: Although these stopping criteria are simple, these predefined stopping criteria
    are likely to cause DeepAL to fail to achieve optimal performance. This is because
    the premature termination of AL annotation querying leads to large performance
    losses in the model, and excessive annotation behavior wastes a lot of annotation
    budget. Therefore, Stabilizing Predictions (SP) (Bloodgood and Vijay-Shanker,
    [2014](#bib.bib28)) makes a comprehensive review of AL stopping strategies and
    proposes an AL stopping strategy based on stability prediction. Specifically,
    the SP predivides a part of the samples from the unlabeled dataset to form a stop
    set (the stop set does not need to be labeled), and the SP checks the prediction
    stability on the stop set in each iteration. When the prediction performance of
    the model on the stop set stabilizes, the iteration is stopped. A well-trained
    model often has a stable predictive ability, and SP takes advantage of this feature.
    The predivided stop set does not require specific labeling information, which
    avoids additional labeling costs contrary to the purpose of AL. Although SP is
    a stopping strategy proposed mainly for AL, it also is relevant for DeepAL.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些停止标准很简单，但这些预定义的停止标准可能会导致深度主动学习无法实现最佳性能。这是因为主动学习（AL）注释查询的过早终止会导致模型的性能损失，而过度的注释行为则浪费了大量的注释预算。因此，稳定预测
    (SP) (Bloodgood 和 Vijay-Shanker，[2014](#bib.bib28)) 对 AL 停止策略进行了全面回顾，并提出了一种基于稳定性预测的
    AL 停止策略。具体而言，SP 会从未标记的数据集中预先划分出一部分样本以形成停止集（停止集不需要标记），并且 SP 在每次迭代中检查停止集上的预测稳定性。当模型在停止集上的预测性能稳定时，迭代过程将被停止。一个训练良好的模型通常具有稳定的预测能力，而
    SP 利用这一特性。预先划分的停止集不需要特定的标记信息，这避免了与 AL 目的相悖的额外标记成本。虽然 SP 主要是为 AL 提出的停止策略，但它也与深度主动学习相关。
- en: Table 1\. The challenges of combining DL and AL, as well as a summary of related
    work on the corresponding solutions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 结合深度学习 (DL) 和主动学习 (AL) 的挑战，以及对相应解决方案的相关工作的总结。
- en: '| Challenges | Solutions | Foundation | Category | Publications |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | 解决方案 | 基础 | 类别 | 发表文献 |'
- en: '| Model uncertainty in Deep Learning | Query strategy optimization | Batch
    Mode DeepAL (BMDAL) | Uncertainty-based and Hybrid Query Strategies | (Ranganathan
    et al., [2017](#bib.bib174); Asghar et al., [2017](#bib.bib14); He et al., [2019a](#bib.bib89);
    Ostapuk et al., [2019](#bib.bib157); Yin et al., [2017](#bib.bib245); Zhdanov,
    [2019](#bib.bib256); Liu et al., [2017](#bib.bib136); Ash et al., [2020](#bib.bib15);
    Shui et al., [2020](#bib.bib201); Kim et al., [2020](#bib.bib113); Ducoffe and
    Precioso, [2018](#bib.bib58)) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 深度学习中的模型不确定性 | 查询策略优化 | 批量模式深度主动学习 (BMDAL) | 基于不确定性和混合查询策略 | (Ranganathan
    等，[2017](#bib.bib174)；Asghar 等，[2017](#bib.bib14)；He 等，[2019a](#bib.bib89)；Ostapuk
    等，[2019](#bib.bib157)；Yin 等，[2017](#bib.bib245)；Zhdanov，[2019](#bib.bib256)；Liu
    等，[2017](#bib.bib136)；Ash 等，[2020](#bib.bib15)；Shui 等，[2020](#bib.bib201)；Kim
    等，[2020](#bib.bib113)；Ducoffe 和 Precioso，[2018](#bib.bib58)) |'
- en: '| Deep Bayesian Active Learning (DBAL) | (Gal et al., [2017](#bib.bib73); Sener
    and Savarese, [2018](#bib.bib193); Pop and Fulop, [2018](#bib.bib166); Kirsch
    et al., [2019](#bib.bib116); Pinsler et al., [2019](#bib.bib163); Chitta et al.,
    [2018](#bib.bib40); Ostapuk et al., [2019](#bib.bib157); Siddhant and Lipton,
    [2018](#bib.bib202); Rottmann et al., [2018](#bib.bib177); Yang et al., [2018](#bib.bib243))
    (Zeng et al., [2018](#bib.bib248); Gudur et al., [2019](#bib.bib84); Martínez-Arellano
    and Ratchev, [2019](#bib.bib144)) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 深度贝叶斯主动学习 (DBAL) | (Gal 等，[2017](#bib.bib73)；Sener 和 Savarese，[2018](#bib.bib193)；Pop
    和 Fulop，[2018](#bib.bib166)；Kirsch 等，[2019](#bib.bib116)；Pinsler 等，[2019](#bib.bib163)；Chitta
    等，[2018](#bib.bib40)；Ostapuk 等，[2019](#bib.bib157)；Siddhant 和 Lipton，[2018](#bib.bib202)；Rottmann
    等，[2018](#bib.bib177)；Yang 等，[2018](#bib.bib243)) (Zeng 等，[2018](#bib.bib248)；Gudur
    等，[2019](#bib.bib84)；Martínez-Arellano 和 Ratchev，[2019](#bib.bib144)) |'
- en: '| Density-based Methods | (Geifman and El-Yaniv, [2017](#bib.bib76); Yin et al.,
    [2017](#bib.bib245); Sener and Savarese, [2018](#bib.bib193); Du et al., [2019](#bib.bib57);
    Gissin and Shalev-Shwartz, [2019](#bib.bib80); Chitta et al., [2019](#bib.bib39))
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 基于密度的方法 | (Geifman 和 El-Yaniv，[2017](#bib.bib76)；Yin 等，[2017](#bib.bib245)；Sener
    和 Savarese，[2018](#bib.bib193)；Du 等，[2019](#bib.bib57)；Gissin 和 Shalev-Shwartz，[2019](#bib.bib80)；Chitta
    等，[2019](#bib.bib39)) |'
- en: '| Automated Design of DeepAL | (Fang et al., [2017](#bib.bib62); Haußmann et al.,
    [2019](#bib.bib87); Liu et al., [2019](#bib.bib137); Geifman and El-Yaniv, [2019](#bib.bib77))
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 深度主动学习的自动化设计 | (Fang 等，[2017](#bib.bib62)；Haußmann 等，[2019](#bib.bib87)；Liu
    等，[2019](#bib.bib137)；Geifman 和 El-Yaniv，[2019](#bib.bib77)) |'
- en: '| Insufficient data for labeled samples | Data expansion of labeled samples
    | - | (Wang et al., [2017](#bib.bib230); Liu et al., [2017](#bib.bib136); Zhu
    and Bento, [2017](#bib.bib260); Tran et al., [2019](#bib.bib224); Sinha et al.,
    [2019](#bib.bib205); Mottaghi and Yeung, [2019](#bib.bib147); Kim et al., [2020](#bib.bib113);
    Simeoni et al., [2019](#bib.bib203)) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 标签样本数据不足 | 标签样本数据扩展 | - | (Wang et al., [2017](#bib.bib230); Liu et al.,
    [2017](#bib.bib136); Zhu and Bento, [2017](#bib.bib260); Tran et al., [2019](#bib.bib224);
    Sinha et al., [2019](#bib.bib205); Mottaghi and Yeung, [2019](#bib.bib147); Kim
    et al., [2020](#bib.bib113); Simeoni et al., [2019](#bib.bib203)) |'
- en: '| Processing pipeline inconsistency | Common framework DeepAL | - | (Asghar
    et al., [2017](#bib.bib14); Wang and Shang, [2014](#bib.bib228); Zhou et al.,
    [2010](#bib.bib258); Stark et al., [2015](#bib.bib214); Shelmanov et al., [2019](#bib.bib199);
    Feng et al., [2019](#bib.bib64); Wang et al., [2017](#bib.bib230); Li et al.,
    [2017](#bib.bib133); Yang et al., [2017](#bib.bib244); He et al., [2019a](#bib.bib89))
    (Du et al., [2019](#bib.bib57); Zhao et al., [2020](#bib.bib255); Lv et al., [2020](#bib.bib141);
    Yoo and Kweon, [2019](#bib.bib246); Vodrahalli et al., [2018](#bib.bib226)) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 处理流程不一致 | 常见框架 DeepAL | - | (Asghar et al., [2017](#bib.bib14); Wang and
    Shang, [2014](#bib.bib228); Zhou et al., [2010](#bib.bib258); Stark et al., [2015](#bib.bib214);
    Shelmanov et al., [2019](#bib.bib199); Feng et al., [2019](#bib.bib64); Wang et
    al., [2017](#bib.bib230); Li et al., [2017](#bib.bib133); Yang et al., [2017](#bib.bib244);
    He et al., [2019a](#bib.bib89)) (Du et al., [2019](#bib.bib57); Zhao et al., [2020](#bib.bib255);
    Lv et al., [2020](#bib.bib141); Yoo and Kweon, [2019](#bib.bib246); Vodrahalli
    et al., [2018](#bib.bib226)) |'
- en: 4\. Application of DeepAL in fields such as vision and NLP
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. DeepAL 在视觉和 NLP 等领域的应用
- en: Today, DeepAL has been applied to areas including but not limited to visual
    data processing (such as object detection, semantic segmentation, etc.), NLP (such
    as machine translation, text classification, semantic analysis, etc.), speech
    and audio processing, social network analysis, medical image processing, wildlife
    protection, industrial robotics, and disaster analysis, among other fields. In
    this section, we provide a systematic and detailed overview of existing DeepAL-related
    work from an application perspective.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，DeepAL 已应用于包括但不限于视觉数据处理（如目标检测、语义分割等）、自然语言处理（如机器翻译、文本分类、语义分析等）、语音和音频处理、社交网络分析、医学图像处理、野生动物保护、工业机器人和灾难分析等领域。在这一部分，我们从应用的角度对现有的
    DeepAL 相关工作进行了系统而详细的概述。
- en: 4.1\. Visual Data Processing
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 视觉数据处理
- en: Just as DL is widely used in the computer vision field, the first field in which
    DeepAL is expected to reach its potential is that of computer vision. In this
    section, we mainly discuss DeepAL-related research in the field of visual data
    processing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 就像深度学习（DL）广泛应用于计算机视觉领域一样，DeepAL 最有可能发挥其潜力的第一个领域是计算机视觉。在这一部分，我们主要讨论 DeepAL 在视觉数据处理领域的相关研究。
- en: 4.1.1\. Image classification and recognition
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 图像分类和识别
- en: As with DL, the classification and recognition of images in DeepAL form the
    basis for research into other vision tasks. One of the most important problems
    that DeepAL faces in the field of image vision tasks is that of how to efficiently
    query samples of high-dimensional data (an area in which traditional AL performs
    poorly) and obtain satisfactory performance at the smallest possible labeling
    cost.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度学习（DL）一样，DeepAL 中的图像分类和识别为其他视觉任务的研究奠定了基础。DeepAL 在图像视觉任务领域面临的一个重要问题是如何高效地查询高维数据样本（这是传统的主动学习（AL）表现不佳的领域），并在尽可能少的标注成本下获得令人满意的性能。
- en: To solve this problem, CEAL (Wang et al., [2017](#bib.bib230)) assigns pseudo-labels
    to samples with high confidence and adds them to the highly uncertain sample set
    queried using the uncertainty-based AL method, then uses the expanded training
    set to train the DeepAL model image classifier. (Ranganathan et al., [2017](#bib.bib174))
    first integrated the criteria of AL into the deep belief network and subsequently
    conducted extensive research on classification tasks on a variety of real uni-modal
    and multi-modal datasets. WI-DL (Liu et al., [2017](#bib.bib136)) uses the DeepAL
    method to simultaneously consider the two selection criteria of maximizing representativeness
    and uncertainty on hyperspectral image (HSI) datasets for remote sensing classification
    tasks. Similarly, (Lin et al., [2018](#bib.bib134); Deng et al., [2019](#bib.bib49))
    also studied the classification of HSI. (Lin et al., [2018](#bib.bib134)) introduces
    AL to initialize HSI and then performs transfer learning. This work also recommends
    constructing and connecting higher-level features to source and target HSI data
    in order to further overcome the cross-domain disparity. (Deng et al., [2019](#bib.bib49))
    proposes a unified deep network combined with active transfer learning, thereby
    training the HSI classification well while using less labeled training data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，CEAL (Wang et al., [2017](#bib.bib230)) 为高置信度的样本分配伪标签，并将其添加到通过基于不确定性的AL方法查询的高不确定性样本集中，然后使用扩展的训练集训练DeepAL模型图像分类器。(Ranganathan
    et al., [2017](#bib.bib174)) 首先将AL的标准整合到深度信念网络中，随后对各种真实的单模态和多模态数据集上的分类任务进行了广泛研究。WI-DL
    (Liu et al., [2017](#bib.bib136)) 使用DeepAL方法同时考虑高光谱图像（HSI）数据集的最大代表性和不确定性这两个选择标准，用于遥感分类任务。同样，(Lin
    et al., [2018](#bib.bib134); Deng et al., [2019](#bib.bib49)) 也研究了HSI的分类。(Lin
    et al., [2018](#bib.bib134)) 引入AL以初始化HSI，然后进行迁移学习。这项工作还建议构建和连接更高层次的特征到源和目标HSI数据中，以进一步克服跨领域差异。(Deng
    et al., [2019](#bib.bib49)) 提出了一个统一的深度网络，结合了主动迁移学习，从而在使用较少标记训练数据的情况下，良好地训练了HSI分类器。
- en: Medical image analysis is also an important application. For example, (Folmsbee
    et al., [2018](#bib.bib67)) explores the use of AL rather than random learning
    to train convolutional neural networks for tissue (e.g., stroma, lymphocytes,
    tumor, mucosa, keratin pearls, blood, and background/adipose) classification tasks.
    (Budd et al., [2019](#bib.bib31)) conducted a comprehensive review of DeepAL-related
    methods in the field of medical image analysis. As discussed above, since the
    annotation of medical images requires strong professional knowledge, it is usually
    both very difficult and very expensive to find well-trained experts willing to
    perform annotations. In addition, DL has achieved impressive performance on various
    image feature tasks. Therefore, a large number of works continue to focus on combining
    DL and AL in order to apply DeepAL to the field of medical image analysis (Du
    et al., [2018](#bib.bib56); Sayantan et al., [2018](#bib.bib187); Chen et al.,
    [2018](#bib.bib37); Smailagic et al., [2018](#bib.bib207); Kwolek et al., [2019](#bib.bib124);
    Scandalea et al., [2019](#bib.bib188); Smailagic et al., [2020](#bib.bib206);
    Sadafi et al., [2019](#bib.bib182)). The DeepAL method is also used to classify
    in situ plankton (Bochinski et al., [2018](#bib.bib29)) and perform the automatic
    counting of cells (Alahmari et al., [2019](#bib.bib7)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像分析也是一个重要的应用。例如，(Folmsbee et al., [2018](#bib.bib67)) 探讨了使用主动学习（AL）而非随机学习来训练卷积神经网络进行组织（如基质、淋巴细胞、肿瘤、黏膜、角化珠、血液和背景/脂肪）分类任务。(Budd
    et al., [2019](#bib.bib31)) 对医学图像分析领域中的DeepAL相关方法进行了全面的综述。如前所述，由于医学图像的标注需要强大的专业知识，通常很难且非常昂贵地找到愿意进行标注的高素质专家。此外，深度学习（DL）在各种图像特征任务上取得了令人瞩目的成绩。因此，大量研究继续集中于将DL和AL相结合，以便将DeepAL应用于医学图像分析领域（Du
    et al., [2018](#bib.bib56); Sayantan et al., [2018](#bib.bib187); Chen et al.,
    [2018](#bib.bib37); Smailagic et al., [2018](#bib.bib207); Kwolek et al., [2019](#bib.bib124);
    Scandalea et al., [2019](#bib.bib188); Smailagic et al., [2020](#bib.bib206);
    Sadafi et al., [2019](#bib.bib182))。DeepAL方法还用于分类原位浮游生物（Bochinski et al., [2018](#bib.bib29)）以及进行细胞的自动计数（Alahmari
    et al., [2019](#bib.bib7)）。
- en: In addition, DeepAL also has a wide range of applications in our daily life.
    For example, (Stark et al., [2015](#bib.bib214)) proposes an AL algorithm that
    uses CNN for verification code recognition. It can use the ability to obtain labeled
    data for free to avoid human intervention and greatly improve the recognition
    accuracy when less labeled data is used. HDAL (Li et al., [2017](#bib.bib133))
    combines the excellent feature extraction capabilities of deep CNN and the ability
    to save on AL labeling costs to design a heuristic deep active learning framework
    for face recognition tasks.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DeepAL 在我们日常生活中也有广泛的应用。例如，(Stark et al., [2015](#bib.bib214)) 提出了一个使用 CNN
    进行验证码识别的 AL 算法。它可以利用获取标注数据的能力来避免人工干预，并在使用较少标注数据时大幅提高识别准确性。HDAL (Li et al., [2017](#bib.bib133))
    结合了深度 CNN 的优异特征提取能力和节省 AL 标注成本的能力，设计了一个用于人脸识别任务的启发式深度主动学习框架。
- en: 4.1.2\. Object detection and semantic segmentation
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 目标检测与语义分割
- en: Object detection and semantic segmentation have important applications in various
    fields, including autonomous driving, medical image processing, and wildlife protection.
    However, these fields are also limited by the higher sample labeling cost. Thus,
    the lower labeling cost of DeepAL is expected to accelerate the application of
    the corresponding DL models in certain real-world areas where labeling is more
    difficult.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测与语义分割在各种领域中具有重要应用，包括自动驾驶、医学图像处理和野生动物保护。然而，这些领域也受到较高样本标注成本的限制。因此，DeepAL 的较低标注成本预计将加速相应深度学习模型在某些标注更为困难的实际领域中的应用。
- en: (Roy et al., [2018](#bib.bib179)) designs a DeepAL framework for object detection,
    which uses the layered architecture where labeling is more difficult as an example
    of ”query by committee” to select the image set to be queried, while at the same
    time introducing a similar exploration/exploitation trade-off strategy to (Yin
    et al., [2017](#bib.bib245)). DeepAL is also widely used in natural biological
    fields and industrial applications. For example, (Norouzzadeh et al., [2019](#bib.bib155))
    uses deep neural networks to quickly transferable and automatically extract information,
    and further combines transfer learning and AL to design a DeepAL framework for
    species identification and counting in camera trap images. (Kellenberger et al.,
    [2019](#bib.bib111)) uses unmanned aerial vehicles (UAV) to obtain images for
    wildlife detection purposes; moreover, to enable this wildlife detector to be
    reused, (Kellenberger et al., [2019](#bib.bib111)) uses AL and introduces transfer
    sampling (TS) to find the corresponding area between the source and target datasets,
    thereby facilitating the transfer of data to the target domain. (Feng et al.,
    [2019](#bib.bib64)) proposes a DeepAL framework for deep object detection in autonomous
    driving to train LiDAR 3D object detectors. (Lv et al., [2020](#bib.bib141)) proposes
    the adaptation of a widespread DeepAL framework to defect detection in real industries,
    along with an uncertainty sampling method for use in generating candidate label
    categories. This work uses the average margin method to set the sampling scale
    of each defect category and is thus able to obtain the required performance with
    less labeled data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (Roy et al., [2018](#bib.bib179)) 设计了一种用于目标检测的 DeepAL 框架，该框架采用分层架构，其中标注更为困难的层作为“委员会查询”的示例来选择要查询的图像集，同时引入了类似于
    (Yin et al., [2017](#bib.bib245)) 的探索/利用权衡策略。DeepAL 也广泛应用于自然生物领域和工业应用。例如，(Norouzzadeh
    et al., [2019](#bib.bib155)) 使用深度神经网络快速迁移和自动提取信息，进一步结合迁移学习和 AL 设计了一个用于相机陷阱图像中物种识别和计数的
    DeepAL 框架。 (Kellenberger et al., [2019](#bib.bib111)) 使用无人机 (UAV) 获取用于野生动物检测的图像；此外，为了使该野生动物检测器能够重复使用，(Kellenberger
    et al., [2019](#bib.bib111)) 使用 AL 并引入了迁移采样 (TS) 以在源数据集和目标数据集之间找到相应的区域，从而促进数据向目标领域的转移。
    (Feng et al., [2019](#bib.bib64)) 提出了一个用于自动驾驶中深度物体检测的 DeepAL 框架，以训练 LiDAR 3D 物体检测器。
    (Lv et al., [2020](#bib.bib141)) 提出了将广泛应用的 DeepAL 框架适应于实际工业中的缺陷检测，并提出了一种用于生成候选标签类别的置信度采样方法。这项工作使用了平均边际方法来设定每个缺陷类别的采样规模，因此能够以较少的标注数据获得所需的性能。
- en: In addition, DeepAL also has important applications in the area of medical image
    segmentation. For example, (Gaur et al., [2016](#bib.bib75)) proposes an AL-based
    transfer learning mechanism for medical image segmentation, which can effectively
    improve the image segmentation performance on a limited labeled dataset. (Yang
    et al., [2017](#bib.bib244)) combines FCN and AL to create a DeepAL framework
    for biological-image segmentation. This work uses the uncertainty and similarity
    information provided by the FCN to extend the maximum set cover problem, significantly
    reducing the required labeling workload by pointing out the most effective labeling
    areas. DASL (Deep Active Self-paced Learning) (Wang et al., [2018b](#bib.bib234))
    proposes a deep region-based network, Nodules R-CNN, for pulmonary nodule segmentation
    tasks. This work generates segmentation masks for use as examples, and at the
    same time, combines AL and SPL (Self-Paced Learning) (Kumar et al., [2010](#bib.bib122))
    to propose a new deep active self-paced learning strategy that reduces the labeling
    workload. (Wang et al., [2019a](#bib.bib233)) proposes a Nodule-plus Region-based
    CNN for pulmonary nodule detection and segmentation in 3D thoracic Computed Tomography
    (CT). This work combines AL and SPL strategies to create a new deep self-paced
    active learning (DSAL) strategy, which reduces the annotation workload and makes
    effective use of unannotated data. (Zhao et al., [2020](#bib.bib255)) further
    proposes a new deep-supervised active learning method for finger bone segmentation
    tasks. This model can be fine-tuned in an iterative and incremental learning manner
    and uses the output of the intermediate hidden layer as the basis for sample selection.
    Compared with the complete markup, (Zhao et al., [2020](#bib.bib255)) achieved
    comparable segmentation results using fewer samples.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，DeepAL在医学图像分割领域也有重要应用。例如，（Gaur et al., [2016](#bib.bib75)）提出了一种基于AL的迁移学习机制，用于医学图像分割，能够在有限标记数据集上有效提升图像分割性能。（Yang
    et al., [2017](#bib.bib244)）结合了FCN和AL，创建了一个用于生物图像分割的DeepAL框架。该工作利用FCN提供的不确定性和相似性信息来扩展最大集合覆盖问题，通过指出最有效的标注区域，显著减少了所需的标注工作量。DASL（深度主动自适应学习）（Wang
    et al., [2018b](#bib.bib234)）提出了一种基于深度区域的网络Nodules R-CNN，用于肺结节分割任务。该工作生成用于示例的分割掩码，并同时结合AL和SPL（自适应学习）（Kumar
    et al., [2010](#bib.bib122)）提出了一种新的深度主动自适应学习策略，从而减少了标注工作量。（Wang et al., [2019a](#bib.bib233)）提出了一种Nodule-plus区域基CNN，用于3D胸部CT的肺结节检测和分割。该工作结合了AL和SPL策略，创建了一种新的深度自适应主动学习（DSAL）策略，减少了注释工作量，并有效利用了未标注的数据。（Zhao
    et al., [2020](#bib.bib255)）进一步提出了一种新的深度监督主动学习方法，用于指骨分割任务。该模型可以通过迭代和增量学习方式进行微调，并利用中间隐藏层的输出作为样本选择的依据。与完整标注相比，（Zhao
    et al., [2020](#bib.bib255)）使用更少的样本实现了可比的分割结果。
- en: 4.1.3\. Video processing
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3. 视频处理
- en: Compared with the image task that only needs to process information in the spatial
    dimension, the video task also needs to process the information in the temporal
    dimension. This makes the task of annotating the video more expensive, which also
    means that the need to introduce AL has become more urgent. DeepAL also has broader
    application scenarios in this field.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 与仅需处理空间维度信息的图像任务相比，视频任务还需要处理时间维度的信息。这使得视频标注任务更加昂贵，也意味着引入AL的需求变得更加紧迫。DeepAL在这一领域也有更广泛的应用场景。
- en: For example, (Hussein et al., [2016](#bib.bib101)) proposes to use imitation
    learning to perform navigation tasks. The visual environment and actions taken
    by the teacher viewed from a first-person perspective are used as the training
    set. Through training, it is hoped that students will become able to predict and
    execute corresponding actions in their own environment. When performing tasks,
    students use deep convolutional neural networks for feature extraction, learn
    imitation strategies, and further use the AL method to select samples with insufficient
    confidence, which are added to the training set to update the action strategy.
    (Hussein et al., [2016](#bib.bib101)) significantly improves the initial strategy
    using fewer samples. DeActive (Hossain et al., [2018](#bib.bib96)) proposes a
    DeepAL activity recognition model. Compared with the traditional DL activity recognition
    model, DeActive requires fewer labeled samples, consumes fewer resources, and
    achieves high recognition accuracy. (Wang et al., [2018a](#bib.bib231)) minimizes
    the annotation cost of the video-based person Re-ID dataset by integrating AL
    into the DL framework. Similarly, (Liu et al., [2019](#bib.bib137)) proposes a
    deep reinforcement active learning method for person Re-ID, using oracle feedback
    to guide the agent (i.e. the model in the reinforcement learning process) in selecting
    the next uncertainty sample. The agent selection mechanism is continuously optimized
    through alternately refined reinforcement learning strategies. (Aghdam et al.,
    [2019](#bib.bib5)) further proposes an active learning object detection method
    based on convolutional neural networks for pedestrian target detection in video
    and static images.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，（Hussein 等，[2016](#bib.bib101)）建议使用模仿学习来执行导航任务。教师从第一人称视角看到的视觉环境和所采取的行动被用作训练集。通过训练，希望学生能够在自己的环境中预测和执行相应的动作。在执行任务时，学生使用深度卷积神经网络进行特征提取，学习模仿策略，并进一步使用
    AL 方法选择置信度不足的样本，将其添加到训练集中以更新动作策略。（Hussein 等，[2016](#bib.bib101)）使用更少的样本显著改善了初始策略。DeActive（Hossain
    等，[2018](#bib.bib96)）提出了一个 DeepAL 活动识别模型。与传统的 DL 活动识别模型相比，DeActive 需要更少的标注样本，消耗更少的资源，并且实现了高识别准确率。（Wang
    等，[2018a](#bib.bib231)）通过将 AL 集成到 DL 框架中，最小化了基于视频的人 Re-ID 数据集的标注成本。同样，（Liu 等，[2019](#bib.bib137)）提出了一种用于人
    Re-ID 的深度强化主动学习方法，使用 oracle 反馈来指导代理（即强化学习过程中的模型）选择下一个不确定样本。通过交替优化强化学习策略，代理选择机制不断得到优化。（Aghdam
    等，[2019](#bib.bib5)）进一步提出了一种基于卷积神经网络的主动学习目标检测方法，用于视频和静态图像中的行人目标检测。
- en: 4.2\. Natural Language Processing (NLP)
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 自然语言处理（NLP）
- en: NLP has always been a very challenging task. The goals of NLP are to make computers
    understand complex human language and to help humans deal with various natural
    language-related tasks. Insufficient data labeling is also a key challenge in
    the NLP context. Below, we introduce some of the most famous DeepAL methods in
    the NLP field.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 一直以来都是一个非常具有挑战性的任务。NLP 的目标是使计算机理解复杂的人类语言，并帮助人类处理各种自然语言相关的任务。数据标注不足也是 NLP
    领域的一个关键挑战。下面，我们介绍一些在 NLP 领域最著名的 DeepAL 方法。
- en: 4.2.1\. Machine translation
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 机器翻译
- en: Machine translation has very important application value, but it usually requires
    a large number of parallel corpora as a training set. For many low-resource language
    pairs, building such a corpus requires a very high cost.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译具有非常重要的应用价值，但通常需要大量的平行语料作为训练集。对于许多低资源语言对，构建这样的语料库需要非常高的成本。
- en: 'For this reason, (Zhang et al., [2018](#bib.bib249)) proposes to use the AL
    framework to select information source sentences to construct a parallel corpus.
    It proposes two effective sentence selection methods for AL: selection based on
    semantic similarity and decoder probability. Compared with traditional methods,
    the two proposed sentence selection methods show considerable advantages. (Platanios
    et al., [2019](#bib.bib165)) proposes a curriculum learning framework related
    to AL for machine translation tasks. It can decide which training samples to show
    to the model during different periods of training based on the estimated difficulty
    of a sample and the current competence of the model. This method not only effectively
    improves the training efficiency but also obtains a good accuracy improvement.
    This kind of thinking is also very valuable for DeepAL’s sample selection strategy.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这个原因，（Zhang 等人，[2018](#bib.bib249)）提出使用 AL 框架来选择信息源句子以构建平行语料库。它提出了两种有效的 AL
    句子选择方法：基于语义相似度的选择和解码器概率。与传统方法相比，这两种提出的句子选择方法显示了相当大的优势。（Platanios 等人，[2019](#bib.bib165)）提出了一个与
    AL 相关的课程学习框架，用于机器翻译任务。它可以根据样本的估计难度和模型的当前能力来决定在训练的不同阶段向模型展示哪些训练样本。这种方法不仅有效提高了训练效率，还获得了良好的准确性提升。这种思路对
    DeepAL 的样本选择策略也非常有价值。
- en: 4.2.2\. Text classification
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 文本分类
- en: Text classification tasks also face the challenge of excessive labeling costs,
    such as patent classification (Larkey, [1999](#bib.bib126); Fall et al., [2003](#bib.bib61))
    and clinical text classification (Pestian et al., [2007](#bib.bib161); Figueroa
    et al., [2012](#bib.bib65); Garla et al., [2013](#bib.bib74)). These labeling
    tasks often need to be completed by experts, and the number of datasets and texts
    in each document is often very large, which makes it difficult for human experts
    to complete the corresponding labeling tasks.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类任务也面临着标签成本过高的挑战，例如专利分类（Larkey，[1999](#bib.bib126)；Fall 等人，[2003](#bib.bib61)）和临床文本分类（Pestian
    等人，[2007](#bib.bib161)；Figueroa 等人，[2012](#bib.bib65)；Garla 等人，[2013](#bib.bib74)）。这些标注任务通常需要专家完成，而且每个文档中的数据集和文本数量往往非常庞大，这使得人工专家难以完成相应的标注任务。
- en: (Zhang et al., [2017b](#bib.bib252)) claims to be the first AL method for text
    classification with CNNs. (Zhang et al., [2017b](#bib.bib252)) focuses on selecting
    those samples that have the greatest impact on the embedding space. It proposes
    a method for sentence classification that selects instances containing words whose
    embeddings are likely to be updated with the greatest magnitude, thereby rapidly
    learning discriminative, task-specific embeddings. They also extend this method
    to text classification tasks, which outperformed the baseline AL method in sentence
    and text classification tasks. (An et al., [2018](#bib.bib8)) also proposes a
    new DeepAL framework for text classification tasks. It uses RNN as the acquisition
    function in AL. The method proposed by (An et al., [2018](#bib.bib8)) can effectively
    reduce the number of label instances required for deep learning while saving training
    time without reducing model accuracy. (Prabhu et al., [2019](#bib.bib167)) focuses
    on the problem of sampling bias in deep active classification and apply active
    text classification on the large-scale text corpora of (Zhang et al., [2015](#bib.bib251)).
    These methods generally show better performance than that of the traditional AL-based
    baseline methods, and more relevant DeepAL-based text classification applications
    can be found in (Schröder and Niekler, [2020](#bib.bib191)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: （Zhang 等人，[2017b](#bib.bib252)）声称是第一个用于文本分类的 CNNs AL 方法。（Zhang 等人，[2017b](#bib.bib252)）专注于选择那些对嵌入空间影响最大的样本。它提出了一种句子分类方法，选择包含嵌入可能被最大幅度更新的词的实例，从而快速学习区分性的、任务特定的嵌入。他们还将这种方法扩展到文本分类任务中，这在句子和文本分类任务中超越了基线
    AL 方法。（An 等人，[2018](#bib.bib8)）还提出了一种新的 DeepAL 框架，用于文本分类任务。它使用 RNN 作为 AL 中的获取函数。（An
    等人，[2018](#bib.bib8)）提出的方法可以有效减少深度学习所需的标签实例数量，同时节省训练时间而不降低模型准确性。（Prabhu 等人，[2019](#bib.bib167)）关注于深度主动分类中的采样偏差问题，并在（Zhang
    等人，[2015](#bib.bib251)）的大规模文本语料库上应用了主动文本分类。这些方法通常比传统的基于 AL 的基线方法表现更好，更多相关的基于 DeepAL
    的文本分类应用可以在（Schröder 和 Niekler，[2020](#bib.bib191)）中找到。
- en: 4.2.3\. Semantic analysis
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 语义分析
- en: In this typical NLP task, the aim is to make the computer understand a natural
    language description. The relevant application scenarios are numerous and varied,
    including but not limited to sentiment classification, news identification, etc.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个典型的NLP任务中，目标是让计算机理解自然语言描述。相关的应用场景众多且多样，包括但不限于情感分类、新闻识别等。
- en: More specifically, for example, (Zhou et al., [2010](#bib.bib258)) uses restricted
    Boltzmann machines (RBM) to construct an active deep network (ADN), then conduct
    unsupervised training on the labeled and unlabeled datasets. ADN uses a large
    number of unlabeled datasets to improve the model’s generalization ability, and
    further employs AL in a semi-supervised learning framework, unifying the selection
    of labeled data and classifiers in a semi-supervised classification framework;
    this approach obtains competitive results on sentiment classification tasks. (Bhattacharjee
    et al., [2017](#bib.bib23)) proposes a human-computer collaborative learning system
    for news accuracy detection tasks (that is, identifying misleading and false information
    in news) that utilizes only a limited number of annotation samples. This system
    is a deep AL-based model that uses 1-2 orders of magnitude fewer annotation samples
    than fully supervised learning. Such a reduction in the number of samples greatly
    accelerates the convergence speed of the model and results in an astonishing 25%
    average performance gains in detection performance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体来说，例如，（Zhou et al., [2010](#bib.bib258)）使用限制玻尔兹曼机（RBM）构建了一个主动深度网络（ADN），然后对标注和未标注的数据集进行无监督训练。ADN利用大量未标注的数据集来提高模型的泛化能力，并进一步在半监督学习框架中应用AL，将标注数据和分类器的选择统一在半监督分类框架中；这种方法在情感分类任务上取得了竞争性的结果。（Bhattacharjee
    et al., [2017](#bib.bib23)）提出了一种用于新闻准确性检测任务的人机协作学习系统（即识别新闻中的误导和虚假信息），该系统仅利用有限数量的标注样本。该系统是一个基于深度AL的模型，使用的标注样本数量比完全监督学习少1-2个数量级。这种样本数量的减少大大加快了模型的收敛速度，并在检测性能上取得了令人惊讶的25%平均性能提升。
- en: 4.2.4\. Information extraction
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 信息提取
- en: Information extraction aims to extract and simplify the most important information
    from large texts, which is an important basis for correlation analysis between
    different concepts.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 信息提取旨在从大量文本中提取和简化最重要的信息，这是不同概念之间关联分析的重要基础。
- en: (Priya et al., [2019](#bib.bib169)) uses relevant tweets from disaster-stricken
    areas to extract information that facilitates the identification of infrastructure
    damage during earthquakes. For this reason, (Priya et al., [2019](#bib.bib169))
    combines RNN and GRU-based models with AL, using AL-based methods to pre-train
    the model so that it will retrieve tweets featuring infrastructure damage in different
    regions, thereby significantly reducing the manual labeling workload. In addition,
    entity resolution (ER) is the task of recognizing the same real entities with
    different representations across databases and represents a key step in knowledge
    base creation and text mining. (Shen et al., [2017](#bib.bib200); Shardlow et al.,
    [2019](#bib.bib198); Chang et al., [2020](#bib.bib35)) uses the combination of
    DL and AL to determine how the technical level of NER (Named Entity Recognition)
    can be improved in the case of a small training set. (Kasai et al., [2019](#bib.bib110))
    developed a DL-based ER method that combines transfer learning and AL to design
    an architecture that allows for the learning of a model that is transferable from
    high-resource environments to low-resource environments. (Maldonado and Harabagiu,
    [2019](#bib.bib142)) proposes a novel ALPNN (Active Learning Policy Neural Network)
    design to recognize the concepts and relationships in large EEG (electroencephalogram)
    reports; this approach can help humans extract available clinical knowledge from
    a large number of such reports.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: (Priya et al., [2019](#bib.bib169)) 使用来自灾区的相关推文提取信息，以帮助识别地震中的基础设施损坏。因此，(Priya
    et al., [2019](#bib.bib169)) 将基于 RNN 和 GRU 的模型与 AL 结合，使用基于 AL 的方法对模型进行预训练，以便检索到不同地区出现基础设施损坏的推文，从而显著减少手动标注的工作量。此外，实体解析
    (ER) 是识别不同数据库中具有不同表示的相同真实实体的任务，是知识库创建和文本挖掘的关键步骤。(Shen et al., [2017](#bib.bib200);
    Shardlow et al., [2019](#bib.bib198); Chang et al., [2020](#bib.bib35)) 结合了 DL
    和 AL 来确定在小规模训练集的情况下，如何提高 NER (命名实体识别) 的技术水平。(Kasai et al., [2019](#bib.bib110))
    开发了一种结合迁移学习和 AL 的 DL 基础 ER 方法，设计了一种架构，使得模型能够从高资源环境迁移到低资源环境。(Maldonado and Harabagiu,
    [2019](#bib.bib142)) 提出了一个新颖的 ALPNN (主动学习策略神经网络) 设计，用于识别大型 EEG (脑电图) 报告中的概念和关系；这种方法可以帮助人们从大量此类报告中提取有用的临床知识。
- en: 4.2.5\. Question-answering
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5. 问答系统
- en: Intelligent question-answering is also a common processing task in the NLP context,
    and DL has achieved impressive results in these areas. However, the performance
    of these applications still relies on the availability of massive labeled datasets;
    AL is expected to bring new hope to this challenge.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 智能问答也是 NLP 领域中的常见处理任务，DL 在这些领域取得了令人印象深刻的成果。然而，这些应用的性能仍然依赖于大量标注数据集的可用性；AL 有望为这一挑战带来新的希望。
- en: The automatic question-answering system has a very wide range of applications
    in the industry, and DeepAL is also highly valuable in this field. For example,
    (Asghar et al., [2017](#bib.bib14)) uses the online AL strategy combined with
    the DL model to achieve an open domain dialogue by interacting with real users
    and learning incrementally from user feedback in each round of dialogue. (Jedoui
    et al., [2019](#bib.bib105)) finds that AL strategies designed for specific tasks
    (e.g., classification) often have only one correct answer and that these uncertainty-based
    measurements are often calculated based on the output of the model. Many real-world
    vision tasks often have multiple correct answers, which leads to the overestimation
    of uncertainty measures and sometimes even worse performance than random sampling
    baselines. For this reason, (Jedoui et al., [2019](#bib.bib105)) proposes to estimate
    the uncertainty in the hidden space within the model rather than the uncertainty
    in the output space of the model in the Visual Question Answer (VQA) generation,
    thus overcoming the paraphrasing nature of language.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 自动问答系统在工业领域应用范围非常广泛，DeepAL 在这个领域也具有很高的价值。例如，(Asghar et al., [2017](#bib.bib14))
    结合了在线 AL 策略和 DL 模型，通过与真实用户互动并从每轮对话中的用户反馈中逐步学习，实现了开放领域对话。(Jedoui et al., [2019](#bib.bib105))
    发现，专门为特定任务（例如分类）设计的 AL 策略通常只有一个正确答案，而这些基于不确定性的测量通常是基于模型的输出来计算的。许多实际视觉任务常常有多个正确答案，这会导致对不确定性测量的过度估计，有时甚至表现比随机抽样基线还要差。因此，(Jedoui
    et al., [2019](#bib.bib105)) 提出了在模型的隐藏空间中估计不确定性，而不是在模型的输出空间中，从而克服语言的同义性。
- en: 4.3\. Other Applications
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 其他应用
- en: The emergence of DeepAL is exciting, as it is expected to reduce the annotation
    costs by orders of magnitude while maintaining performance levels. For this reason,
    DeepAL is also widely used in other fields.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAL 的出现令人兴奋，因为它预计将大幅降低标注成本，同时保持性能水平。因此，DeepAL 在其他领域也得到了广泛应用。
- en: These applications include, but are not limited to, gene expression, robotics,
    wearable device data analysis, social networking, ECG signal analysis, etc. For
    some more specific examples, MLFS (Multi-Level Feature Selection) (Ibrahim et al.,
    [2014](#bib.bib102)) combines DL and AL to select genes/miRNAs based on expression
    profiles and proposes a novel multi-level feature selection method. MLFS also
    considers the biological relationship between miRNAs and genes and applies this
    method to miRNA expansion tasks. Moreover, the failure risk of real-world robots
    is expensive. (Andersson et al., [2017](#bib.bib9)) proposes a risk-aware resampling
    technique; this approach uses AL together with existing solvers and DL to optimize
    the robot’s trajectory, enabling it to effectively deal with the collision problem
    in scenes with moving obstacles, and verify the effectiveness of the DeepAL method
    on a real nano-quadcopter. (Zhou and Schoellig, [2019](#bib.bib259)) further proposes
    an active trajectory generation framework for the inverse dynamics model of the
    robot control algorithm, which enables the systematic design of the information
    trajectory used to train the DNN inverse dynamics module.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这些应用包括但不限于基因表达、机器人技术、可穿戴设备数据分析、社交网络、心电图信号分析等。举一些更具体的例子，MLFS（多级特征选择）（Ibrahim
    等，[2014](#bib.bib102)）结合了 DL 和 AL，根据表达谱选择基因/miRNA，并提出了一种新颖的多级特征选择方法。MLFS 还考虑了
    miRNAs 和基因之间的生物学关系，并将此方法应用于 miRNA 扩展任务。此外，现实世界机器人失败的风险成本高昂。（Andersson 等，[2017](#bib.bib9)）提出了一种风险感知的重采样技术；该方法结合
    AL 与现有求解器和 DL 来优化机器人的轨迹，使其能够有效处理移动障碍物场景中的碰撞问题，并验证了 DeepAL 方法在真实纳米四旋翼上的有效性。（Zhou
    和 Schoellig，[2019](#bib.bib259)）进一步提出了一种用于机器人控制算法逆动力学模型的主动轨迹生成框架，使得可以系统地设计用于训练
    DNN 逆动力学模块的信息轨迹。
- en: In addition, (Hossain and Roy, [2019](#bib.bib97); Gudur et al., [2019](#bib.bib84))
    uses sensors installed in wearable devices or mobile terminals to collect user
    movement information for human activity recognition purposes. (Hossain and Roy,
    [2019](#bib.bib97)) proposes a DeepAL framework for activity recognition with
    context-aware annotator selection. ActiveHARNet (Active Learning for Human Activity
    Recognition) (Gudur et al., [2019](#bib.bib84)) proposes a resource-efficient
    deep ensembled model that supports incremental learning and inference on the device,
    utilizes the approximation in the BNN to represent the uncertainty of the model,
    and further proves the feasibility of ActiveHARNet deployment and incremental
    learning on two public datasets. For its part, DALAUP (Deep Active Learning for
    Anchor User Prediction) (Cheng et al., [2019](#bib.bib38)) designs a DeepAL framework
    for anchor user prediction in social networks that reduces the cost of annotating
    anchor users and improves the prediction accuracy. DeepAL is also using in the
    classification of electrocardiogram (ECG) signals. For example, (Rahhal et al.,
    [2016](#bib.bib172)) proposes an active DL-based ECG signal classification method.
    (Hanbay, [2019](#bib.bib86)) proposed an AL-based ECG classification method using
    eigenvalues and DL. The use of the AL method enables the cost of marking ECG signals
    by medical experts to be effectively reduced. Furthermore, the cost of label annotation
    in the speech and audio fields is also relatively high. (Abdel-Wahab and Busso,
    [2019](#bib.bib2)) finds that a model trained on a corpus composed of thousands
    of recordings collected by a small number of speakers is unable to be generalized
    to new domains; therefore, (Abdel-Wahab and Busso, [2019](#bib.bib2)) developed
    a practical scheme that involves using AL to train deep neural networks for speech
    emotion recognition tasks when label resources are limited.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，（Hossain 和 Roy，[2019](#bib.bib97)；Gudur 等，[2019](#bib.bib84)）使用安装在可穿戴设备或移动终端上的传感器来收集用户运动信息，以用于人体活动识别。（Hossain
    和 Roy，[2019](#bib.bib97)）提出了一种具有上下文感知标注选择的 DeepAL 框架用于活动识别。ActiveHARNet（用于人体活动识别的主动学习网络）（Gudur
    等，[2019](#bib.bib84)）提出了一种资源高效的深度集成模型，支持设备上的增量学习和推断，利用 BNN 中的近似来表示模型的不确定性，并进一步证明了
    ActiveHARNet 在两个公开数据集上的部署和增量学习的可行性。另一方面，DALAUP（用于锚定用户预测的深度主动学习）（Cheng 等，[2019](#bib.bib38)）设计了一种用于社交网络中锚定用户预测的
    DeepAL 框架，减少了标注锚定用户的成本并提高了预测准确性。DeepAL 还被用于心电图（ECG）信号的分类。例如，（Rahhal 等，[2016](#bib.bib172)）提出了一种基于主动深度学习的
    ECG 信号分类方法。（Hanbay，[2019](#bib.bib86)）提出了一种基于主动学习的 ECG 分类方法，结合了特征值和深度学习。使用 AL
    方法有效降低了医学专家标记 ECG 信号的成本。此外，语音和音频领域的标签注释成本也相对较高。（Abdel-Wahab 和 Busso，[2019](#bib.bib2)）发现，基于少数发言者收集的数千条录音组成的语料库训练的模型无法泛化到新领域；因此，（Abdel-Wahab
    和 Busso，[2019](#bib.bib2)）开发了一种实用方案，涉及使用 AL 训练深度神经网络用于语音情感识别任务，当标签资源有限时。
- en: In general, the current applications of DeepAL are mainly focused on visual
    image processing tasks, although there are also applications in NLP and other
    fields. Compared with DL and AL, DeepAL is still in the preliminary stage of research,
    meaning that the corresponding classic works are relatively few; however, it still
    has the same broad application scenarios and practical value as DL. In addition,
    in order to facilitate readers’ access to specific applications of DeepAL in related
    fields, we have classified and summarized all application scenarios and datasets
    used by survey-related work in Section [4](#S4 "4\. Application of DeepAL in fields
    such as vision and NLP ‣ A Survey of Deep Active Learning") in detail. The specific
    information is shown in Table [2](#S4.T2 "Table 2 ‣ 4.3\. Other Applications ‣
    4\. Application of DeepAL in fields such as vision and NLP ‣ A Survey of Deep
    Active Learning").
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，目前深度主动学习（DeepAL）的应用主要集中在视觉图像处理任务上，尽管在自然语言处理（NLP）和其他领域也有应用。与深度学习（DL）和主动学习（AL）相比，DeepAL
    仍处于研究的初级阶段，这意味着相关的经典工作相对较少；然而，它仍然具有与 DL 相同的广泛应用场景和实际价值。此外，为了方便读者了解 DeepAL 在相关领域的具体应用，我们在第[4](#S4
    "4\. DeepAL在视觉和NLP等领域的应用 ‣ 深度主动学习调查")节中详细分类和总结了调查相关工作使用的所有应用场景和数据集。具体信息见表[2](#S4.T2
    "表2 ‣ 4.3\. 其他应用 ‣ 4\. DeepAL在视觉和NLP等领域的应用 ‣ 深度主动学习调查")。
- en: Table 2\. DeepAL’s research examples in Vision, NLP and other fields.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. DeepAL 在视觉、NLP 和其他领域的研究示例。
- en: '| Field | Task | Publications | Datasets | Scenes |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 任务 | 发表文献 | 数据集 | 场景 |'
- en: '| Vision | Image classification and recognition | (Wang et al., [2017](#bib.bib230);
    Ranganathan et al., [2017](#bib.bib174); Stark et al., [2015](#bib.bib214)) |
    CACD (Chen et al., [2014](#bib.bib36)), Caltech-256 (Griffin et al., [2007](#bib.bib82)),
    VidTIMIT (Sanderson, [2008](#bib.bib184)), CK (Kanade et al., [2000](#bib.bib109)),
    MNIST (LeCun et al., [1998](#bib.bib129)), CIFAR 10 (Krizhevsky et al., [2009](#bib.bib120)),
    emoFBVP (Ranganathan et al., [2016](#bib.bib173)), MindReading (El Kaliouby and
    Robinson, [2004](#bib.bib59)) Cool PHP CAPTCHA (Stark et al., [2015](#bib.bib214))
    | Handwritten numbers, face, CAPTCHA recognition, etc. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| (Liu et al., [2017](#bib.bib136); Deng et al., [2019](#bib.bib49); Lin et al.,
    [2018](#bib.bib134)) | PaviaC, PaviaU, Botswana (Liu et al., [2017](#bib.bib136)),
    Salinas Valley, Indian Pines (Deng et al., [2019](#bib.bib49)), Washington DC
    Mall, Urban (Lin et al., [2018](#bib.bib134)) | Hyperspectral image |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| (Folmsbee et al., [2018](#bib.bib67); Budd et al., [2019](#bib.bib31); Du
    et al., [2018](#bib.bib56); Sayantan et al., [2018](#bib.bib187)) (Smailagic et al.,
    [2018](#bib.bib207); Kwolek et al., [2019](#bib.bib124); Scandalea et al., [2019](#bib.bib188))
    (Smailagic et al., [2020](#bib.bib206); Sadafi et al., [2019](#bib.bib182); Chen
    et al., [2018](#bib.bib37)) | Erie County (Folmsbee et al., [2018](#bib.bib67)),
    EEG (Andrzejak et al., [2001](#bib.bib10)), BreaKHis (Spanhol et al., [2016](#bib.bib211)),
    SVEB, SVDB (Sayantan et al., [2018](#bib.bib187)) | Biomedical |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| Object detection | (Roy et al., [2018](#bib.bib179)) | VOC (Everingham et al.,
    [2010](#bib.bib60)), Kitti (Geiger et al., [[n.d.]](#bib.bib78)) | – |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| (Norouzzadeh et al., [2019](#bib.bib155); Kellenberger et al., [2019](#bib.bib111))
    | SS (Swanson et al., [2015](#bib.bib216)), eMML (Forrester et al., [2013](#bib.bib68)),
    NACTI¹, CCT², UAV³ | Biodiversity survey |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| (Feng et al., [2019](#bib.bib64)) | KITTI (Geiger et al., [2012](#bib.bib79))
    | Autonomous driving |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| (Lv et al., [2020](#bib.bib141)) | NEU-DET (Song and Yan, [2013](#bib.bib210))
    | Defect detection |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| Semantic segmentation | (Gaur et al., [2016](#bib.bib75); Yang et al., [2017](#bib.bib244);
    Wang et al., [2018b](#bib.bib234); Wang et al., [2019a](#bib.bib233)) | SPIM (Delibaltov
    et al., [2016](#bib.bib47)), Confocal(Delibaltov et al., [2013](#bib.bib48)),
    LIDC-IDRI (Armato III et al., [2011](#bib.bib13)), MICCAI, Lymph node (Zhang et al.,
    [2016](#bib.bib253)) | Bio-medical image |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| Video processing | (Hussein et al., [2016](#bib.bib101)) | Mash-simulator⁴
    | Autonomous navigation |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| (Hossain et al., [2018](#bib.bib96)) | OPPORTUNITY (Hossain et al., [2018](#bib.bib96)),
    WISDM (Kwapisz et al., [2010](#bib.bib123)), SenseBox (Taylor et al., [2017](#bib.bib220)),
    Skoda Daphnet (Bachlin et al., [2009](#bib.bib16)),CASAS (Cook and Schmitter-Edgecombe,
    [2009](#bib.bib41)) | Smart home |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| (Wang et al., [2018a](#bib.bib231); Aghdam et al., [2019](#bib.bib5)) | PRID
    (Hirzer et al., [2011](#bib.bib94)), MARS (Zheng et al., [2016](#bib.bib257)),
    BDD100K (Yu et al., [2018](#bib.bib247)), DukeMTMC-VideoReID (Wu et al., [2018](#bib.bib238)),
    CityPersons (Zhang et al., [2017a](#bib.bib250)), Caltech Pedestrian(Dollár et al.,
    [2012](#bib.bib53)) | Person Re-id |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| NLP | Machine translation | (Zhang et al., [2018](#bib.bib249); Platanios
    et al., [2019](#bib.bib165)) | OPUS (Tiedemann, [2012](#bib.bib221)), UNPC (Ziemski
    et al., [2016](#bib.bib262)), IWSLT, WMT (Platanios et al., [2018](#bib.bib164))
    | Ind-En, Ch-En, En-Vi, Fr-En, En-De, etc. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Text classification | (Zhang et al., [2017b](#bib.bib252); An et al., [2018](#bib.bib8);
    Schröder and Niekler, [2020](#bib.bib191); Prabhu et al., [2019](#bib.bib167))
    | CR⁵, Subj, MR⁶, MuR⁷, DR (Wallace et al., [2014](#bib.bib227)) AGN, DBP, AMZP,
    AMZF, YRF (Zhang et al., [2015](#bib.bib251)) | – |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Semantic analysis | (Zhou et al., [2010](#bib.bib258)) | MOV (Pang et al.,
    [2002](#bib.bib158)), BOO, DVDs, ELE, KIT (Blitzer et al., [2007](#bib.bib25);
    Dasgupta and Ng, [2009](#bib.bib46)) | Sentiment classification |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| (Bhattacharjee et al., [2017](#bib.bib23)) | KDnugget’s Fake News⁸, Harvard
    Dataverse (Kwon et al., [2017](#bib.bib125)), Liar (Wang, [2017](#bib.bib235))
    | News veracity detection |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| Information extraction | (Priya et al., [2019](#bib.bib169)) | Italy, Iran-Iraq,
    Mexico earthquake dataset | Disaster assessment |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| (Maldonado and Harabagiu, [2019](#bib.bib142)) | Temple University Hospital^(10)
    | Electroencephalography (EEG) reports |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| (Shen et al., [2017](#bib.bib200); Shardlow et al., [2019](#bib.bib198);
    Chang et al., [2020](#bib.bib35); Kasai et al., [2019](#bib.bib110)) | CoNLL (Sang
    and Meulder, [2003](#bib.bib185)), NCBI (Dogan et al., [2014](#bib.bib52)), MedMentions
    (Murty et al., [2018](#bib.bib150)), OntoNotes (Pradhan et al., [2013](#bib.bib168)),
    DBLP, FZ, AG (Mudgal et al., [2018](#bib.bib148)), Cora (Wang et al., [2011](#bib.bib229))
    | Named entity recognition (NER) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Question answering | (Asghar et al., [2017](#bib.bib14)) | CMDC (Danescu-Niculescu-Mizil
    and Lee, [2011](#bib.bib44)), JabberWacky’s chatlogs⁹ | Dialogue generation |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| (Jedoui et al., [2019](#bib.bib105)) | Visual Genome (Krishna et al., [2017](#bib.bib118)),
    VQA (Antol et al., [2015](#bib.bib12)) | Visual question answer (VQA) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Other | – | (Ibrahim et al., [2014](#bib.bib102)) | BC, HCC, Lung | Gene
    expression |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| (Andersson et al., [2017](#bib.bib9); Zhou and Schoellig, [2019](#bib.bib259))
    | EATG (Zhou and Schoellig, [2019](#bib.bib259)), Crazyflie 2.0^(11) | Robotics
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| (Hossain and Roy, [2019](#bib.bib97); Gudur et al., [2019](#bib.bib84)) |
    HHAR (Stisen et al., [2015](#bib.bib215)), NWFD (Mauldin et al., [2018](#bib.bib146))
    | Smart device |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| (Cheng et al., [2019](#bib.bib38)) | Foursquare, Twitter (Kong et al., [2013](#bib.bib117))
    | Social network |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| (Rahhal et al., [2016](#bib.bib172); Hanbay, [2019](#bib.bib86)) | MIT-BIH
    (Mark et al., [1982](#bib.bib143)), INCART, SVDB (Rahhal et al., [2016](#bib.bib172))
    | Electrocardiogram (ECG) signal classification |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| (Rahhal et al., [2016](#bib.bib172); Hanbay, [2019](#bib.bib86)) | MIT-BIH
    (Mark et al., [1982](#bib.bib143)), INCART, SVDB (Rahhal et al., [2016](#bib.bib172))
    | 心电图（ECG）信号分类 |'
- en: '| (Abdel-Wahab and Busso, [2019](#bib.bib2)) | MSP-Podcast (Lotfian and Busso,
    [2017](#bib.bib139)) | Speech emotion recognition |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| (Abdel-Wahab and Busso, [2019](#bib.bib2)) | MSP-Podcast (Lotfian and Busso,
    [2017](#bib.bib139)) | 语音情感识别 |'
- en: '1'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: http://lila.science/datasets/nacti
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: http://lila.science/datasets/nacti
- en: '2'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: http://lila.science/datasets/caltech-camera-traps
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: http://lila.science/datasets/caltech-camera-traps
- en: '3'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3'
- en: http://kuzikus-namibia.de/xe_index.html
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: http://kuzikus-namibia.de/xe_index.html
- en: '4'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4'
- en: https://github.com/idiap/mash-simulator
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: https://github.com/idiap/mash-simulator
- en: '5'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5'
- en: www.cs.uic.edu/liub/FBS/sentiment-analysis.html
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: www.cs.uic.edu/liub/FBS/sentiment-analysis.html
- en: '6'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6'
- en: 'Subj and MR datasets are available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Subj 和 MR 数据集可在： http://www.cs.cornell.edu/people/pabo/movie-review-data/
- en: '7'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7'
- en: http://www.cs.jhu.edu/˜mdredze/datasets/sentiment/
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: http://www.cs.jhu.edu/˜mdredze/datasets/sentiment/
- en: '8'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8'
- en: https://github.com/GeorgeMcIntire/fake_real_news_dataset
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: https://github.com/GeorgeMcIntire/fake_real_news_dataset
- en: '9'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '9'
- en: http://www.jabberwacky.com/j2conversations. JabberWacky is an in-browser, open-domain,
    retrieval-based bot.
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: http://www.jabberwacky.com/j2conversations。JabberWacky 是一个浏览器内的开放域检索型机器人。
- en: '10'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '10'
- en: https://www.isip.piconepress.com/projects/tuh_eeg/
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: https://www.isip.piconepress.com/projects/tuh_eeg/
- en: '11'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '11'
- en: https://www.bitcraze.io/
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: https://www.bitcraze.io/
- en: –
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Non-specific application scenarios
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非特定应用场景
- en: 5\. Discussion and future directions
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 讨论与未来方向
- en: 'DeepAL combines the common advantages of DL and AL: it inherits not only DL’s
    ability to process high-dimensional image data and conduct automatic feature extraction
    but also AL’s potential to effectively reduce annotation costs. DeepAL, therefore,
    has fascinating potential especially in areas where labels require high levels
    of expertise and are difficult to obtain.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: DeepAL结合了DL和AL的共同优势：它不仅继承了DL处理高维图像数据和进行自动特征提取的能力，还具备AL有效降低注释成本的潜力。因此，DeepAL在需要高水平专业知识且难以获得标签的领域具有令人兴奋的潜力。
- en: Most recent work reveals that DeepAL has been successful in many common tasks.
    DeepAL has attracted the interest of a large number of researchers by reducing
    the cost of annotation and its ability to implement the powerful feature extraction
    capabilities of DL; consequently, the related research work is also extremely
    rich. However, there are still a large number of unanswered questions on this
    subject. As (Munjal et al., [2020](#bib.bib149)) discovered, the results reported
    on the random sampling baseline (RSB) differ significantly between different studies.
    For example, under the same settings, using 20% of the label data of CIFAR 10,
    the RSB performance reported by (Yoo and Kweon, [2019](#bib.bib246)) is 13% higher
    than that in (Tran et al., [2019](#bib.bib224)). Secondly, the same DeepAL method
    may yield different results in different studies. For example, using 40% of the
    label data of CIFAR 100 (Krizhevsky et al., [2009](#bib.bib120)) and VGG16 (Simonyan
    and Zisserman, [2015](#bib.bib204)) as the extraction network, the reported results
    of (Sener and Savarese, [2018](#bib.bib193)) and (Sinha et al., [2019](#bib.bib205))
    differ by 8%. Furthermore, the latest DeepAL research also exhibits some inconsistencies.
    For example, (Sener and Savarese, [2018](#bib.bib193)) and (Ducoffe and Precioso,
    [2018](#bib.bib58)) point out that diversity-based methods have always been better
    than uncertainty-based methods, and that uncertainty-based methods perform worse
    than RSB; however, the latest research of (Yoo and Kweon, [2019](#bib.bib246))
    shows that this is not the case.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，DeepAL 在许多常见任务中取得了成功。DeepAL 通过降低标注成本以及实现深度学习（DL）的强大特征提取能力，吸引了大量研究人员的关注；因此，相关的研究工作也极为丰富。然而，这个领域仍然存在许多未解答的问题。正如
    (Munjal et al., [2020](#bib.bib149)) 发现的那样，不同研究中报告的随机采样基线（RSB）结果存在显著差异。例如，在相同设置下，使用
    CIFAR 10 的 20% 标注数据，(Yoo and Kweon, [2019](#bib.bib246)) 报告的 RSB 性能比 (Tran et
    al., [2019](#bib.bib224)) 高出 13%。其次，同一 DeepAL 方法在不同研究中可能会得到不同的结果。例如，使用 CIFAR 100
    (Krizhevsky et al., [2009](#bib.bib120)) 和 VGG16 (Simonyan and Zisserman, [2015](#bib.bib204))
    作为提取网络时，(Sener and Savarese, [2018](#bib.bib193)) 和 (Sinha et al., [2019](#bib.bib205))
    报告的结果相差 8%。此外，最新的 DeepAL 研究也显示出一些不一致。例如，(Sener and Savarese, [2018](#bib.bib193))
    和 (Ducoffe and Precioso, [2018](#bib.bib58)) 指出，基于多样性的方法始终优于基于不确定性的方法，并且基于不确定性的方法表现不如
    RSB；然而，(Yoo and Kweon, [2019](#bib.bib246)) 的最新研究显示情况并非如此。
- en: Compared with AL’s strategic selection of high-value samples, RSB has been regarded
    as a strong baseline (Yoo and Kweon, [2019](#bib.bib246); Sener and Savarese,
    [2018](#bib.bib193)). However, the above problems reveal an urgent need to design
    a general performance evaluation platform for DeepAL work, as well as to determine
    a unified high-performance RSB. Secondly, the reproducibility of different DeepAL
    methods is also an important issue. The highly reproducible DeepAL method helps
    to evaluate the performance of different DALs. A common evaluation platform should
    be used for experiments under consistent settings, and snapshots of experimental
    settings should be shared. In addition, multiple repetitive experiments with different
    initializations under the same experimental conditions should be implemented,
    as this could effectively avoid misleading conclusions caused by experimental
    setup problems. Researchers should pay sufficient attention to these inconsistent
    studies to enable them to clarify the principles involved. On the other hand,
    adequate ablation experiments and transfer experiments are also necessary. The
    former will make it easier for us to determine which improvements bring about
    performance gains, while the latter can help to ensure that the AL selection strategy
    does indeed enable the indiscriminate selection of high-value samples for the
    dataset.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AL 策略性选择高价值样本相比，RSB 被视为一个强有力的基线 (Yoo and Kweon, [2019](#bib.bib246); Sener
    and Savarese, [2018](#bib.bib193))。然而，上述问题揭示了设计一个通用的 DeepAL 性能评估平台的迫切需求，以及确定一个统一的高性能
    RSB 的必要性。其次，不同 DeepAL 方法的可重复性也是一个重要问题。高度可重复的 DeepAL 方法有助于评估不同 DAL 的性能。应该在一致的设置下使用通用评估平台进行实验，并共享实验设置的快照。此外，在相同实验条件下进行多次不同初始化的重复实验也应实施，因为这可以有效避免因实验设置问题导致的误导性结论。研究人员应充分关注这些不一致的研究，以便澄清涉及的原则。另一方面，适当的消融实验和迁移实验也是必要的。前者将使我们更容易确定哪些改进带来了性能提升，而后者可以帮助确保
    AL 选择策略确实能使数据集中的高价值样本得到无差别的选择。
- en: The current research directions regarding DeepAL methods focus primarily on
    the improvement of AL selection strategies, the optimization of training methods,
    and the improvement of task-independent models. As noted in Section [3.1](#S3.SS1
    "3.1\. Query Strategy Optimization in DeepAL ‣ 3\. Deep Active Learning ‣ A Survey
    of Deep Active Learning"), the improvement of AL selection strategy is currently
    centered around taking into account the query strategy based on uncertainty and
    diversity explicitly or implicitly. Moreover, hybrid selection strategies are
    increasingly favored by researchers. Moreover, the optimization of training methods
    mainly focuses on labeled datasets, unlabeled datasets, or the use of methods
    such as GAN to expand data, as well as the hybrid training method of unsupervised,
    semi-supervised, and supervised learning across the AL cycle. This training method
    promises to deliver even more performance improvements than are thought to be
    achievable through changes to the selection strategy. In fact, this makes up for
    the issues of the DL model requiring a large number of labeled training samples
    and the AL selecting a limited number of labeled samples. In addition, the use
    of unlabeled or generated datasets is also conducive to making full use of existing
    information without adding to the annotation costs. Furthermore, the incremental
    training method is also an important research direction. From a computing resource
    perspective, it is unacceptable to train a deep model from scratch in each cycle.
    While simple incremental training will cause the deviation of model parameters,
    the huge potential savings on resources are quite attractive. Although related
    research remains quite scarce, this is still a very promising research direction.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Task independence is also an important research direction, as it helps to make
    DeepAL models more directly and widely extensible to other tasks. However, the
    related research remains insufficient, and the corresponding DeepAL methods tend
    to focus only on the uncertainty-based selection method. Because DL itself is
    easier to integrate with the uncertainty-based AL selection strategy, we believe
    that uncertainty-based methods will continue to dominate research directions not
    related to these tasks in the future. On the other hand, it may also be advisable
    to explicitly take the diversity-based selection strategy into account; of course,
    this will also give rise to great challenges. In addition, it should be pointed
    out that blindly pursuing the idea of training models on smaller subsets would
    be unwise, as the relative difference in sample importance in some datasets with
    a large variety of content and a large number of samples can almost be ignored.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: There is no conflict between the above-mentioned improvement directions; thus,
    a mixed improvement strategy is an important development direction for the future.
    In general, DeepAL research has significant practical application value in terms
    of both labeling costs and application scenarios; however, DeepAL research remains
    in its infancy at present, and there is still a long way to go in the future.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Summary and conclusions
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the first time, the necessity and challenges of combining traditional active
    learning and deep learning have been comprehensively analyzed and summarized.
    In response to these challenges, we analyze and compare existing work from three
    perspectives: query strategy optimization, labeled sample data expansion, and
    model generality. In addition, we also summarize the stopping strategy of DeepAL.
    Then, we review the related work of DeepAL from the perspective of the application.
    Finally, we conduct a comprehensive discussion on the future direction of DeepAL.
    As far as we know, this is the first comprehensive and systematic review in the
    field of deep active learning.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work was partially supported by the NSFC under Grant (No.61972315 and No.62072372)
    and the Shaanxi Science and Technology Innovation Team Support Project under grant
    agreement (No.2018TD-026) and the Australian Research Council Discovery Early
    Career Researcher Award (No.DE190100626).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdel-Wahab and Busso (2019) Mohammed Abdel-Wahab and Carlos Busso. 2019. Active
    Learning for Speech Emotion Recognition Using Deep Neural Network. In *8th International
    Conference on Affective Computing and Intelligent Interaction, ACII 2019, Cambridge,
    United Kingdom, September 3-6, 2019*. IEEE, 1–7.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adomavicius and Tuzhilin (2005) Gediminas Adomavicius and Alexander Tuzhilin.
    2005. Toward the next generation of recommender systems: a survey of the state-of-the-art
    and possible extensions. *IEEE Transactions on Knowledge and Data Engineering*
    17, 6 (2005), 734–749.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aggarwal et al. (2014) Charu C. Aggarwal, Xiangnan Kong, Quanquan Gu, Jiawei
    Han, and Philip S. Yu. 2014. Active Learning: A Survey. In *Data Classification:
    Algorithms and Applications*. CRC Press, 571–606.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aghdam et al. (2019) Hamed Habibi Aghdam, Abel Gonzalez-Garcia, Antonio M. López,
    and Joost van de Weijer. 2019. Active Learning for Deep Detection Neural Networks.
    In *2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
    Korea (South), October 27 - November 2, 2019*. IEEE, 3671–3679.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aharoni et al. (2019) Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.
    Massively Multilingual Neural Machine Translation. In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
    June 2-7, 2019, Volume 1 (Long and Short Papers)*. Association for Computational
    Linguistics, 3874–3884.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alahmari et al. (2019) Saeed S. Alahmari, Dmitry B. Goldgof, Lawrence O. Hall,
    and Peter R. Mouton. 2019. Automatic Cell Counting using Active Deep Learning
    and Unbiased Stereology. In *2019 IEEE International Conference on Systems, Man
    and Cybernetics, SMC 2019, Bari, Italy, October 6-9, 2019*. IEEE, 1708–1713.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An et al. (2018) Bang An, Wenjun Wu, and Huimin Han. 2018. Deep Active Learning
    for Text Classification. In *Proceedings of the 2nd International Conference on
    Vision, Image and Signal Processing, ICVISP 2018, Las Vegas, NV, USA, August 27-29,
    2018*. ACM, 22:1–22:6.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andersson et al. (2017) Olov Andersson, Mariusz Wzorek, and Patrick Doherty.
    2017. Deep Learning Quadcopter Control via Risk-Aware Active Learning. In *Proceedings
    of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9,
    2017, San Francisco, California, USA*. AAAI Press, 3812–3818.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Andrzejak et al. (2001) Ralph G Andrzejak, Klaus Lehnertz, Florian Mormann,
    Christoph Rieke, Peter David, and Christian E Elger. 2001. Indications of nonlinear
    deterministic and finite-dimensional structures in time series of brain electrical
    activity: Dependence on recording region and brain state. *Physical Review E*
    64, 6 (2001), 061907.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Angluin (1988) Dana Angluin. 1988. Queries and Concept Learning. *Machine Learning*
    2, 4 (1988), 319–342.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Antol et al. (2015) Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret
    Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual
    Question Answering. In *2015 IEEE International Conference on Computer Vision,
    ICCV 2015, Santiago, Chile, December 7-13, 2015*. IEEE Computer Society, 2425–2433.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Armato III et al. (2011) Samuel G Armato III, Geoffrey McLennan, Luc Bidaut,
    Michael F McNitt-Gray, Charles R Meyer, Anthony P Reeves, Binsheng Zhao, Denise R
    Aberle, Claudia I Henschke, Eric A Hoffman, et al. 2011. The lung image database
    consortium (LIDC) and image database resource initiative (IDRI): a completed reference
    database of lung nodules on CT scans. *Medical physics* 38, 2 (2011), 915–931.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asghar et al. (2017) Nabiha Asghar, Pascal Poupart, Xin Jiang, and Hang Li.
    2017. Deep Active Learning for Dialogue Generation. In *Proceedings of the 6th
    Joint Conference on Lexical and Computational Semantics, *SEM @ACM 2017, Vancouver,
    Canada, August 3-4, 2017*. Association for Computational Linguistics, 78–83.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ash et al. (2020) Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John
    Langford, and Alekh Agarwal. 2020. Deep Batch Active Learning by Diverse, Uncertain
    Gradient Lower Bounds. In *8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bachlin et al. (2009) Marc Bachlin, Meir Plotnik, Daniel Roggen, Inbal Maidan,
    Jeffrey M Hausdorff, Nir Giladi, and Gerhard Troster. 2009. Wearable assistant
    for Parkinson’s disease patients with the freezing of gait symptom. *IEEE Transactions
    on Information Technology in Biomedicine* 14, 2 (2009), 436–446.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baker et al. (2017) Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar.
    2017. Designing Neural Network Architectures using Reinforcement Learning. In
    *5th International Conference on Learning Representations, ICLR 2017, Toulon,
    France, April 24-26, 2017, Conference Track Proceedings*. OpenReview.net.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balcan et al. (2009) Mariaflorina Balcan, Alina Beygelzimer, and John Langford.
    2009. Agnostic active learning. *J. Comput. System Sci.* 75, 1 (2009), 78–89.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bau et al. (2019) Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani,
    Fahim Dalvi, and James R. Glass. 2019. Identifying and Controlling Important Neurons
    in Neural Machine Translation. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beluch et al. (2018) William H. Beluch, Tim Genewein, Andreas Nürnberger, and
    Jan M. Köhler. 2018. The Power of Ensembles for Active Learning in Image Classification.
    In *2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018,
    Salt Lake City, UT, USA, June 18-22, 2018*. IEEE Computer Society, 9368–9377.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bendavid et al. (2010) Shai Bendavid, John Blitzer, Koby Crammer, Alex Kulesza,
    Fernando Pereira, and Jennifer Wortman Vaughan. 2010. A theory of learning from
    different domains. *Machine Learning* 79, 1 (2010), 151–175.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio et al. (2006) Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle.
    2006. Greedy Layer-Wise Training of Deep Networks. (2006), 153–160.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bhattacharjee et al. (2017) Sreyasee Das Bhattacharjee, Ashit Talukder, and
    Bala Venkatram Balantrapu. 2017. Active learning based news veracity detection
    with feature weighting and deep-shallow fusion. (2017), 556–565.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilgic and Getoor (2009) Mustafa Bilgic and Lise Getoor. 2009. Link-based active
    learning. In *NIPS Workshop on Analyzing Networks and Learning with Graphs*.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blitzer et al. (2007) John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
    Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment
    Classification. In *ACL 2007, Proceedings of the 45th Annual Meeting of the Association
    for Computational Linguistics, June 23-30, 2007, Prague, Czech Republic*. The
    Association for Computational Linguistics.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bloodgood and Callison-Burch (2014) Michael Bloodgood and Chris Callison-Burch.
    2014. Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical
    Machine Translation. *CoRR* abs/1410.5877 (2014).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bloodgood and Vijay-Shanker (2009) Michael Bloodgood and K. Vijay-Shanker.
    2009. Taking into Account the Differences between Actively and Passively Acquired
    Data: The Case of Active Learning with Support Vector Machines for Imbalanced
    Datasets. In *Human Language Technologies: Conference of the North American Chapter
    of the Association of Computational Linguistics, Proceedings, May 31 - June 5,
    2009, Boulder, Colorado, USA, Short Papers*. The Association for Computational
    Linguistics, 137–140.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bloodgood and Vijay-Shanker (2014) Michael Bloodgood and K. Vijay-Shanker. 2014.
    A Method for Stopping Active Learning Based on Stabilizing Predictions and the
    Need for User-Adjustable Stopping. *CoRR* abs/1409.5165 (2014).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bochinski et al. (2018) Erik Bochinski, Ghassen Bacha, Volker Eiselein, Tim
    J. W. Walles, Jens C. Nejstgaard, and Thomas Sikora. 2018. Deep Active Learning
    for In Situ Plankton Classification. In *Pattern Recognition and Information Forensics
    - ICPR 2018 International Workshops, CVAUI, IWCF, and MIPPSNA, Beijing, China,
    August 20-24, 2018, Revised Selected Papers* *(Lecture Notes in Computer Science,
    Vol. 11188)*. Springer, 5–15.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brinker (2003) Klaus Brinker. 2003. Incorporating Diversity in Active Learning
    with Support Vector Machines. In *Machine Learning, Proceedings of the Twentieth
    International Conference (ICML 2003), August 21-24, 2003, Washington, DC, USA*.
    AAAI Press, 59–66.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budd et al. (2019) Samuel Budd, Emma C. Robinson, and Bernhard Kainz. 2019.
    A Survey on Active Learning and Human-in-the-Loop Deep Learning for Medical Image
    Analysis. *CoRR* abs/1910.02923 (2019).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burka and Kuchenbecker (2017) Alex Burka and Katherine J. Kuchenbecker. 2017.
    How Much Haptic Surface Data Is Enough?. In *2017 AAAI Spring Symposia, Stanford
    University, Palo Alto, California, USA, March 27-29, 2017*. AAAI Press.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calinon et al. (2007) Sylvain Calinon, Florent Guenter, and Aude Billard. 2007.
    On Learning, Representing, and Generalizing a Task in a Humanoid Robot. *IEEE
    Trans. Syst. Man Cybern. Part B* 37, 2 (2007), 286–298.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campbell and Broderick (2019) Trevor Campbell and Tamara Broderick. 2019. Automated
    Scalable Bayesian Inference via Hilbert Coresets. *Journal of Machine Learning
    Research* 20, 15 (2019), 1–38.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang et al. (2020) Haw-Shiuan Chang, Shankar Vembu, Sunil Mohan, Rheeya Uppaal,
    and Andrew McCallum. 2020. Using error decay prediction to overcome practical
    issues of deep active learning for named entity recognition. *Mach. Learn.* 109,
    9-10 (2020), 1749–1778.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2014) Bor-Chun Chen, Chu-Song Chen, and Winston H. Hsu. 2014. Cross-Age
    Reference Coding for Age-Invariant Face Recognition and Retrieval. In *Computer
    Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September
    6-12, 2014, Proceedings, Part VI* *(Lecture Notes in Computer Science, Vol. 8694)*.
    Springer, 768–783.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2018) Xuhui Chen, Jinlong Ji, Tianxi Ji, and Pan Li. 2018. Cost-Sensitive
    Deep Active Learning for Epileptic Seizure Detection. In *Proceedings of the 2018
    ACM International Conference on Bioinformatics, Computational Biology, and Health
    Informatics, BCB 2018, Washington, DC, USA, August 29 - September 01, 2018*. ACM,
    226–235.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2019) Anfeng Cheng, Chuan Zhou, Hong Yang, Jia Wu, Lei Li, Jianlong
    Tan, and Li Guo. 2019. Deep Active Learning for Anchor User Prediction. In *Proceedings
    of the Twenty-Eighth International Joint Conference on Artificial Intelligence,
    IJCAI 2019, Macao, China, August 10-16, 2019*. ijcai.org, 2151–2157.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chitta et al. (2019) Kashyap Chitta, Jose M Alvarez, Elmar Haussmann, and Clement
    Farabet. 2019. Training Data Distribution Search with Ensemble Active Learning.
    *arXiv preprint arXiv:1905.12737* (2019).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chitta et al. (2018) Kashyap Chitta, Jose M. Alvarez, and Adam Lesnikowski.
    2018. Large-Scale Visual Active Learning with Deep Probabilistic Ensembles. *CoRR*
    abs/1811.03575 (2018).
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cook and Schmitter-Edgecombe (2009) Diane J Cook and Maureen Schmitter-Edgecombe.
    2009. Assessing the quality of activities in a smart environment. *Methods of
    information in medicine* 48, 5 (2009), 480.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dagan and Engelson (1995) Ido Dagan and Sean P. Engelson. 1995. Committee-Based
    Sampling For Training Probabilistic Classifiers. In *Machine Learning, Proceedings
    of the Twelfth International Conference on Machine Learning, Tahoe City, California,
    USA, July 9-12, 1995*. Morgan Kaufmann, 150–157.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalal and Triggs (2005) Navneet Dalal and Bill Triggs. 2005. Histograms of Oriented
    Gradients for Human Detection. In *2005 IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition (CVPR 2005), 20-26 June 2005, San Diego, CA, USA*.
    IEEE Computer Society, 886–893.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Danescu-Niculescu-Mizil and Lee (2011) Cristian Danescu-Niculescu-Mizil and
    Lillian Lee. 2011. Chameleons in Imagined Conversations: A New Approach to Understanding
    Coordination of Linguistic Style in Dialogs. In *Proceedings of the 2nd Workshop
    on Cognitive Modeling and Computational Linguistics, CMCL@ACL 2011, Portland,
    Oregon, USA, June 23, 2011*. Association for Computational Linguistics, 76–87.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasgupta (2011) Sanjoy Dasgupta. 2011. Two faces of active learning. *Theoretical
    Computer Science* 412, 19 (2011), 1767–1781.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dasgupta and Ng (2009) Sajib Dasgupta and Vincent Ng. 2009. Mine the Easy,
    Classify the Hard: A Semi-Supervised Approach to Automatic Sentiment Classification.
    In *ACL 2009, Proceedings of the 47th Annual Meeting of the Association for Computational
    Linguistics and the 4th International Joint Conference on Natural Language Processing
    of the AFNLP, 2-7 August 2009, Singapore*, Keh-Yih Su, Jian Su, and Janyce Wiebe
    (Eds.). The Association for Computer Linguistics, 701–709. [https://www.aclweb.org/anthology/P09-1079/](https://www.aclweb.org/anthology/P09-1079/)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delibaltov et al. (2016) Diana L. Delibaltov, Utkarsh Gaur, Jennifer Kim, Matthew
    Kourakis, Erin Newman-Smith, William Smith, Samuel A. Belteton, Daniel Szymanski,
    and B. S. Manjunath. 2016. CellECT: cell evolution capturing tool. *BMC Bioinform.*
    17 (2016), 88.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delibaltov et al. (2013) Diana L. Delibaltov, Pratim Ghosh, Volkan Rodoplu,
    Michael Veeman, William Smith, and B. S. Manjunath. 2013. A Linear Program Formulation
    for the Segmentation of Ciona Membrane Volumes. In *Medical Image Computing and
    Computer-Assisted Intervention - MICCAI 2013 - 16th International Conference,
    Nagoya, Japan, September 22-26, 2013, Proceedings, Part I* *(Lecture Notes in
    Computer Science, Vol. 8149)*. Springer, 444–451.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2019) Cheng Deng, Yumeng Xue, Xianglong Liu, Chao Li, and Dacheng
    Tao. 2019. Active Transfer Learning Network: A Unified Deep Joint Spectral–Spatial
    Feature Learning Model for Hyperspectral Image Classification. *IEEE Transactions
    on Geoscience and Remote Sensing* 57, 3 (2019), 1741–1754.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2012) Jia Deng, Alex Berg, Sanjeev Satheesh, Hao Su, Aditya Khosla,
    and Fei-Fei Li. 2012. Large scale visual recognition challenge. *www. image-net.
    org/challenges/LSVRC/2012* 1 (2012).
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng (2014) Li Deng. 2014. A tutorial survey of architectures, algorithms, and
    applications for deep learning. *APSIPA Transactions on Signal and Information
    Processing* 3 (2014).
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dogan et al. (2014) Rezarta Islamaj Dogan, Robert Leaman, and Zhiyong Lu. 2014.
    NCBI disease corpus: A resource for disease name recognition and concept normalization.
    *J. Biomed. Informatics* 47 (2014), 1–10.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dollár et al. (2012) Piotr Dollár, Christian Wojek, Bernt Schiele, and Pietro
    Perona. 2012. Pedestrian Detection: An Evaluation of the State of the Art. *IEEE
    Trans. Pattern Anal. Mach. Intell.* 34, 4 (2012), 743–761.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Donahue and Simonyan (2019) Jeff Donahue and Karen Simonyan. 2019. Large Scale
    Adversarial Representation Learning. In *Advances in Neural Information Processing
    Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS
    2019, December 8-14, 2019, Vancouver, BC, Canada*. 10541–10551.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Donoho et al. (2000) David L Donoho et al. 2000. High-dimensional data analysis:
    The curses and blessings of dimensionality. *AMS math challenges lecture* 1, 2000
    (2000), 32.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2018) Baolin Du, Qi Qi, Han Zheng, Yue Huang, and Xinghao Ding. 2018.
    Breast Cancer Histopathological Image Classification via Deep Active Learning
    and Confidence Boosting. In *Artificial Neural Networks and Machine Learning -
    ICANN 2018 - 27th International Conference on Artificial Neural Networks, Rhodes,
    Greece, October 4-7, 2018, Proceedings, Part II* *(Lecture Notes in Computer Science,
    Vol. 11140)*. Springer, 109–116.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. (2019) Xuefeng Du, Dexing Zhong, and Huikai Shao. 2019. Building an
    Active Palmprint Recognition System. In *2019 IEEE International Conference on
    Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019*. IEEE, 1685–1689.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ducoffe and Precioso (2018) Melanie Ducoffe and Frédéric Precioso. 2018. Adversarial
    Active Learning for Deep Networks: a Margin Based Approach. *CoRR* abs/1802.09841
    (2018).'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'El Kaliouby and Robinson (2004) Rana El Kaliouby and Peter Robinson. 2004.
    Mind reading machines: Automated inference of cognitive mental states from video.
    In *2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat.
    No. 04CH37583)*, Vol. 1\. IEEE, 682–688.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. (2010) Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, and Andrew Zisserman. 2010. The pascal visual object classes (voc)
    challenge. *International journal of computer vision* 88, 2 (2010), 303–338.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fall et al. (2003) Caspar J. Fall, A. Törcsvári, K. Benzineb, and G. Karetka.
    2003. Automated categorization in the international patent classification. *SIGIR
    Forum* 37, 1 (2003), 10–25.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2017) Meng Fang, Yuan Li, and Trevor Cohn. 2017. Learning how
    to Active Learn: A Deep Reinforcement Learning Approach. In *Proceedings of the
    2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017,
    Copenhagen, Denmark, September 9-11, 2017*. Association for Computational Linguistics,
    595–605.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Farahani and Hekmatfar (2009) Reza Zanjirani Farahani and Masoud Hekmatfar.
    2009. *Facility location: concepts, models, algorithms and case studies*. Springer.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2019) Di Feng, Xiao Wei, Lars Rosenbaum, Atsuto Maki, and Klaus
    Dietmayer. 2019. Deep Active Learning for Efficient Training of a LiDAR 3D Object
    Detector. In *2019 IEEE Intelligent Vehicles Symposium, IV 2019, Paris, France,
    June 9-12, 2019*. IEEE, 667–674.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Figueroa et al. (2012) Rosa L. Figueroa, Qing Zeng-Treitler, Long H. Ngo, Sergey
    Goryachev, and Eduardo P. Wiechmann. 2012. Active learning for clinical text classification:
    is it better than random sampling? *J. Am. Medical Informatics Assoc.* 19, 5 (2012),
    809–816.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitch (1944) Frederic Brenton Fitch. 1944. McCulloch Warren S. and Pitts Walter.
    A logical calculus of the ideas immanent in nervous activity. Bulletin of mathematical
    biophysics , vol. 5 (1943), pp. 115–133. *Journal of Symbolic Logic* 9 (1944),
    49–50.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Folmsbee et al. (2018) Jonathan Folmsbee, Xulei Liu, Margaret Brandwein-Weber,
    and Scott Doyle. 2018. Active deep learning: Improved training efficiency of convolutional
    neural networks for tissue classification in oral cavity cancer. In *2018 IEEE
    15th International Symposium on Biomedical Imaging (ISBI 2018)*. IEEE, 770–773.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forrester et al. (2013) Tavis Forrester, William J McShea, RW Keys, Robert
    Costello, Megan Baker, and Arielle Parsons. 2013. eMammal–citizen science camera
    trapping as a solution for broad-scale, long-term monitoring of wildlife populations.
    *Sustainable Pathways: Learning from the Past and Shaping the Future* (2013).'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frank et al. (1956) Marguerite Frank, Philip Wolfe, et al. 1956. An algorithm
    for quadratic programming. *Naval research logistics quarterly* 3, 1-2 (1956),
    95–110.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Freytag et al. (2014) Alexander Freytag, Erik Rodner, and Joachim Denzler.
    2014. Selecting Influential Examples: Active Learning with Expected Model Output
    Changes. In *Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland,
    September 6-12, 2014, Proceedings, Part IV* *(Lecture Notes in Computer Science,
    Vol. 8692)*. Springer, 562–577.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal and Ghahramani (2015) Yarin Gal and Zoubin Ghahramani. 2015. Bayesian Convolutional
    Neural Networks with Bernoulli Approximate Variational Inference. *CoRR* abs/1506.02158
    (2015).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal and Ghahramani (2016) Yarin Gal and Zoubin Ghahramani. 2016. Dropout as
    a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In
    *Proceedings of the 33nd International Conference on Machine Learning, ICML 2016,
    New York City, NY, USA, June 19-24, 2016* *(JMLR Workshop and Conference Proceedings,
    Vol. 48)*. JMLR.org, 1050–1059.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gal et al. (2017) Yarin Gal, Riashat Islam, and Zoubin Ghahramani. 2017. Deep
    Bayesian Active Learning with Image Data. In *Proceedings of the 34th International
    Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
    2017* *(Proceedings of Machine Learning Research, Vol. 70)*. PMLR, 1183–1192.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Garla et al. (2013) Vijay Garla, Caroline Taylor, and Cynthia Brandt. 2013.
    Semi-supervised clinical text classification with Laplacian SVMs: An application
    to cancer case management. *J. Biomed. Informatics* 46, 5 (2013), 869–875.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaur et al. (2016) Utkarsh Gaur, Matthew Kourakis, Erin Newman-Smith, William
    Smith, and B. S. Manjunath. 2016. Membrane segmentation via active learning with
    deep networks. In *2016 IEEE International Conference on Image Processing, ICIP
    2016, Phoenix, AZ, USA, September 25-28, 2016*. IEEE, 1943–1947.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geifman and El-Yaniv (2017) Yonatan Geifman and Ran El-Yaniv. 2017. Deep Active
    Learning over the Long Tail. *CoRR* abs/1711.00941 (2017).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geifman and El-Yaniv (2019) Yonatan Geifman and Ran El-Yaniv. 2019. Deep Active
    Learning with a Neural Architecture Search. In *Advances in Neural Information
    Processing Systems 32: Annual Conference on Neural Information Processing Systems
    2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada*. 5974–5984.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geiger et al. ([n.d.]) Andreas Geiger, Philip Lenz, and Raquel Urtasun. [n.d.].
    Are we ready for autonomous driving. In *Proc. CVPR*. 3354–3361.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geiger et al. (2012) Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012.
    Are we ready for autonomous driving? The KITTI vision benchmark suite. In *2012
    IEEE Conference on Computer Vision and Pattern Recognition, Providence, RI, USA,
    June 16-21, 2012*. IEEE Computer Society, 3354–3361.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gissin and Shalev-Shwartz (2019) Daniel Gissin and Shai Shalev-Shwartz. 2019.
    Discriminative Active Learning. *CoRR* abs/1907.06347 (2019).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative Adversarial Nets. In *Advances in Neural Information Processing
    Systems 27: Annual Conference on Neural Information Processing Systems 2014, December
    8-13 2014, Montreal, Quebec, Canada*. 2672–2680.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Griffin et al. (2007) Gregory Griffin, Alex Holub, and Pietro Perona. 2007.
    Caltech-256 object category dataset. (2007).
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gudovskiy et al. (2020) Denis A. Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi,
    and Sotaro Tsukizawa. 2020. Deep Active Learning for Biased Datasets via Fisher
    Kernel Self-Supervision. In *2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020*. IEEE, 9038–9046.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gudur et al. (2019) Gautham Krishna Gudur, Prahalathan Sundaramoorthy, and
    Venkatesh Umaashankar. 2019. ActiveHARNet: Towards On-Device Deep Bayesian Active
    Learning for Human Activity Recognition. *CoRR* abs/1906.00108.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo (2010) Yuhong Guo. 2010. Active Instance Sampling via Matrix Partition.
    In *Advances in Neural Information Processing Systems 23: 24th Annual Conference
    on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9
    December 2010, Vancouver, British Columbia, Canada*. Curran Associates, Inc.,
    802–810.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanbay (2019) Kazim Hanbay. 2019. Deep Neural Network Based Approach for ECG
    Classification Using Hybrid Differential Features and Active Learning. *Iet Signal
    Processing* 13, 2 (2019), 165–175.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haußmann et al. (2019) Manuel Haußmann, Fred A. Hamprecht, and Melih Kandemir.
    2019. Deep Active Learning with Adaptive Acquisition. In *Proceedings of the Twenty-Eighth
    International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao,
    China, August 10-16, 2019*. ijcai.org, 2470–2476.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep Residual Learning for Image Recognition. In *2016 IEEE Conference on Computer
    Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016*.
    IEEE Computer Society, 770–778.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2019a) Tao He, Xiaoming Jin, Guiguang Ding, Lan Yi, and Chenggang
    Yan. 2019a. Towards Better Uncertainty Sampling: Active Learning with Multiple
    Views for Deep Convolutional Neural Network. In *IEEE International Conference
    on Multimedia and Expo, ICME 2019, Shanghai, China, July 8-12, 2019*. IEEE, 1360–1365.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2019b) Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie,
    and Mu Li. 2019b. Bag of Tricks for Image Classification with Convolutional Neural
    Networks. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR
    2019, Long Beach, CA, USA, June 16-20, 2019*. Computer Vision Foundation / IEEE,
    558–567.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hernández-Lobato and Adams (2015) José Miguel Hernández-Lobato and Ryan P. Adams.
    2015. Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks.
    In *Proceedings of the 32nd International Conference on Machine Learning, ICML
    2015, Lille, France, 6-11 July 2015* *(JMLR Workshop and Conference Proceedings,
    Vol. 37)*. JMLR.org, 1861–1869.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2006) Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. 2006.
    A Fast Learning Algorithm for Deep Belief Nets. *Neural Comput.* 18, 7 (2006),
    1527–1554.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2012) Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing
    co-adaptation of feature detectors. *CoRR* abs/1207.0580 (2012).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hirzer et al. (2011) Martin Hirzer, Csaba Beleznai, Peter M Roth, and Horst
    Bischof. 2011. Person re-identification by descriptive and discriminative classification.
    In *Scandinavian conference on Image analysis*. Springer, 91–102.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hoi et al. (2006) Steven C. H. Hoi, Rong Jin, Jianke Zhu, and Michael R. Lyu.
    2006. Batch mode active learning and its application to medical image classification.
    In *Machine Learning, Proceedings of the Twenty-Third International Conference
    (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006* *(ACM International
    Conference Proceeding Series, Vol. 148)*. ACM, 417–424.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hossain et al. (2018) H. M. Sajjad Hossain, M. D. Abdullah Al Haiz Khan, and
    Nirmalya Roy. 2018. DeActive: Scaling Activity Recognition with Active Deep Learning.
    *Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.* 2, 2 (2018), 66:1–66:23.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hossain and Roy (2019) H. M. Sajjad Hossain and Nirmalya Roy. 2019. Active Deep
    Learning for Activity Recognition with Context Aware Annotator Selection. In *Proceedings
    of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data
    Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019*. ACM, 1862–1870.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. (2011) Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and Máté
    Lengyel. 2011. Bayesian Active Learning for Classification and Preference Learning.
    *CoRR* abs/1112.5745 (2011).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2020) Yue Huang, Zhenwei Liu, Minghui Jiang, Xian Yu, and Xinghao
    Ding. 2020. Cost-Effective Vehicle Type Recognition in Surveillance Images With
    Deep Active Learning and Web Data. *IEEE Transactions on Intelligent Transportation
    Systems* 21, 1 (2020), 79–86.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huggins et al. (2016) Jonathan H Huggins, Trevor Campbell, and Tamara Broderick.
    2016. Coresets for Scalable Bayesian Logistic Regression. (2016), 4080–4088.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hussein et al. (2016) Ahmed Hussein, Mohamed Medhat Gaber, and Eyad Elyan. 2016.
    Deep Active Learning for Autonomous Navigation. In *Engineering Applications of
    Neural Networks - 17th International Conference, EANN 2016, Aberdeen, UK, September
    2-5, 2016, Proceedings* *(Communications in Computer and Information Science,
    Vol. 629)*. Springer, 3–17.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ibrahim et al. (2014) Rania Ibrahim, Noha A Yousri, Mohamed A Ismail, and Nagwa M
    El-Makky. 2014. Multi-level gene/MiRNA feature selection using deep belief nets
    and active learning. In *2014 36th Annual International Conference of the IEEE
    Engineering in Medicine and Biology Society*. IEEE, 3957–3960.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain and Kapoor (2009) Prateek Jain and Ashish Kapoor. 2009. Active learning
    for large multi-class problems. (2009), 762–769.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Janz et al. (2017) David Janz, Jos van der Westhuizen, and José Miguel Hernández-Lobato.
    2017. Actively Learning what makes a Discrete Sequence Valid. *CoRR* abs/1708.04465
    (2017).
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jedoui et al. (2019) Khaled Jedoui, Ranjay Krishna, Michael Bernstein, and Fei-Fei
    Li. 2019. Deep Bayesian Active Learning for Multiple Correct Outputs. *CoRR* abs/1912.01119
    (2019).
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jordan (1986) Michael I. Jordan. 1986. Serial Order: A Parallel Distributed
    Processing Approach. *Advances in psychology* 121 (1986), 471–495.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. (2009) Ajay Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.
    2009. Multi-class active learning for image classification. (2009), 2372–2379.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joshi et al. (2010) J. Ajay Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos.
    2010. Multi-class batch-mode active learning for image classification. *Robotics
    and Automation* (2010), 1873–1878.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanade et al. (2000) Takeo Kanade, Jeffrey F Cohn, and Yingli Tian. 2000. Comprehensive
    database for facial expression analysis. In *Proceedings Fourth IEEE International
    Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)*. IEEE,
    46–53.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasai et al. (2019) Jungo Kasai, Kun Qian, Sairam Gurajada, Yunyao Li, and Lucian
    Popa. 2019. Low-resource Deep Entity Resolution with Transfer and Active Learning.
    (2019), 5851–5861.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kellenberger et al. (2019) Benjamin Kellenberger, Diego Marcos, Sylvain Lobry,
    and Devis Tuia. 2019. Half a Percent of Labels is Enough: Efficient Animal Detection
    in UAV Imagery Using Deep CNNs and Active Learning. *IEEE Transactions on Geoscience
    and Remote Sensing* 57, 12 (2019), 9524–9533.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kendall et al. (2017) Alex Kendall, Vijay Badrinarayanan, and Roberto Cipolla.
    2017. Bayesian SegNet: Model Uncertainty in Deep Convolutional Encoder-Decoder
    Architectures for Scene Understanding. In *British Machine Vision Conference 2017,
    BMVC 2017, London, UK, September 4-7, 2017*. BMVA Press.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2020) Kwanyoung Kim, Dongwon Park, Kwang In Kim, and Se Young Chun.
    2020. Task-Aware Variational Adversarial Active Learning. *arXiv: Learning* (2020).'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: King et al. (2004) Ross D King, Kenneth E Whelan, Ffion M Jones, Philip G K
    Reiser, Christopher H Bryant, Stephen Muggleton, Douglas B Kell, and Stephen G
    Oliver. 2004. Functional genomic hypothesis generation and experimentation by
    a robot scientist. *Nature* 427, 6971 (2004), 247–252.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2014) Diederik P. Kingma and Max Welling. 2014. Auto-Encoding
    Variational Bayes. In *2nd International Conference on Learning Representations,
    ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings*.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kirsch et al. (2019) Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. 2019.
    BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning.
    In *Advances in Neural Information Processing Systems 32: Annual Conference on
    Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada*. 7024–7035.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong et al. (2013) Xiangnan Kong, Jiawei Zhang, and Philip S. Yu. 2013. Inferring
    anchor links across multiple heterogeneous social networks. In *22nd ACM International
    Conference on Information and Knowledge Management, CIKM’13, San Francisco, CA,
    USA, October 27 - November 1, 2013*. ACM, 179–188.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishna et al. (2017) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,
    Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A.
    Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting
    Language and Vision Using Crowdsourced Dense Image Annotations. *Int. J. Comput.
    Vis.* 123, 1 (2017), 32–73.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krishnamurthy (2002) Vikram Krishnamurthy. 2002. Algorithms for optimal scheduling
    and management of hidden Markov model sensors. *IEEE Transactions on Signal Processing*
    50, 6 (2002), 1382–1397.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    multiple layers of features from tiny images. Citeseer.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
    2012. ImageNet Classification with Deep Convolutional Neural Networks. (2012),
    1097–1105.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2010) M P Kumar, Benjamin Packer, and Daphne Koller. 2010. Self-Paced
    Learning for Latent Variable Models. (2010), 1189–1197.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwapisz et al. (2010) Jennifer R. Kwapisz, Gary M. Weiss, and Samuel Moore.
    2010. Activity recognition using cell phone accelerometers. *SIGKDD Explor.* 12,
    2 (2010), 74–82.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwolek et al. (2019) Bogdan Kwolek, Michal Koziarski, Andrzej Bukala, Zbigniew
    Antosz, Boguslaw Olborski, Pawel Wąsowicz, Jakub Swadźba, and Boguslaw Cyganek.
    2019. Breast Cancer Classification on Histopathological Images Affected by Data
    Imbalance Using Active Learning and Deep Convolutional Neural Network. (2019),
    299–312.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2017) Sejeong Kwon, Meeyoung Cha, and Kyomin Jung. 2017. Rumor
    detection over varying time windows. *PloS one* 12, 1 (2017), e0168344.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larkey (1999) Leah S. Larkey. 1999. A Patent Search and Classification System.
    In *Proceedings of the Fourth ACM conference on Digital Libraries, August 11-14,
    1999, Berkeley, CA, USA*. ACM, 179–187.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecun et al. (2015) Yann Lecun, Yoshua Bengio, and Geoffrey E Hinton. 2015.
    Deep learning. *Nature* 521, 7553 (2015), 436–444.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Y. LeCun, B. Boser, S. J. Denker, D. Henderson, E. R. Howard,
    W. Hubbard, and D. L. Jackel. 1989. Backpropagation Applied to Handwritten Zip
    Code Recognition. *Neural Computation* (1989), 541–551.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee and Paeng (2018) Byungjae Lee and Kyunghyun Paeng. 2018. A Robust and Effective
    Approach Towards Accurate Metastasis Detection and pN-stage Classification in
    Breast Cancer. (2018), 841–850.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leibig et al. (2017) Christian Leibig, Vaneeda Allken, Murat Seckin Ayhan, Philipp
    Berens, and Siegfried Wahl. 2017. Leveraging uncertainty information from deep
    neural networks for disease detection. *Scientific Reports* 7, 1 (2017), 17816–17816.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis and Gale (1994) David D Lewis and William A Gale. 1994. A sequential algorithm
    for training text classifiers. (1994), 3–12.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2017) Ya Li, Keze Wang, Lin Nie, and Qing Wang. 2017. Face Recognition
    via Heuristic Deep Active Learning. (2017), 97–107.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2018) Jianzhe Lin, Liang Zhao, Shuying Li, Rabab K Ward, and Z Jane
    Wang. 2018. Active-Learning-Incorporated Deep Transfer Learning for Hyperspectral
    Image Classification. *IEEE Journal of Selected Topics in Applied Earth Observations
    and Remote Sensing* 11, 11 (2018), 4048–4062.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin and Parikh (2017) Xiao Lin and Devi Parikh. 2017. Active Learning for Visual
    Question Answering: An Empirical Study. *CoRR* abs/1711.01732 (2017).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017) Peng Liu, Hui Zhang, and Kie B Eom. 2017. Active Deep Learning
    for Classification of Hyperspectral Images. *IEEE Journal of Selected Topics in
    Applied Earth Observations and Remote Sensing* 10, 2 (2017), 712–724.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Zimo Liu, Jingya Wang, Shaogang Gong, Huchuan Lu, and Dacheng
    Tao. 2019. Deep Reinforcement Active Learning for Human-in-the-Loop Person Re-Identification.
    (2019), 6122–6131.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2015) Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015.
    Fully convolutional networks for semantic segmentation. (2015), 3431–3440.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lotfian and Busso (2017) Reza Lotfian and Carlos Busso. 2017. Building naturalistic
    emotionally balanced speech corpus by retrieving emotional speech from existing
    podcast recordings. *IEEE Transactions on Affective Computing* (2017).
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lowe (1999) David G. Lowe. 1999. Object Recognition from Local Scale-Invariant
    Features. In *Proceedings of the International Conference on Computer Vision,
    Kerkyra, Corfu, Greece, September 20-25, 1999*. 1150–1157.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lv et al. (2020) Xiaoming Lv, Fajie Duan, Jiajia Jiang, Xiao Fu, and Lin Gan.
    2020. Deep Active Learning for Surface Defect Detection. *Sensors* 20, 6 (2020),
    1650.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maldonado and Harabagiu (2019) Ramon Maldonado and Sanda M Harabagiu. 2019.
    Active deep learning for the identification of concepts and relations in electroencephalography
    reports. *Journal of Biomedical Informatics* 98 (2019), 103265.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mark et al. (1982) RG Mark, PS Schluter, G Moody, P Devlin, and D Chernoff.
    1982. An annotated ECG database for evaluating arrhythmia detectors. In *IEEE
    Transactions on Biomedical Engineering*, Vol. 29\. IEEE-INST ELECTRICAL ELECTRONICS
    ENGINEERS INC 345 E 47TH ST, NEW YORK, NY …, 600–600.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martínez-Arellano and Ratchev (2019) Giovanna Martínez-Arellano and Svetan M.
    Ratchev. 2019. Towards An Active Learning Approach To Tool Condition Monitoring
    With Bayesian Deep Learning. In *Proceedings of the 33rd International ECMS Conference
    on Modelling and Simulation, ECMS 2019 Caserta, Italy, June 11-14, 2019*. European
    Council for Modeling and Simulation, 223–229.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mateen et al. (2019) Muhammad Mateen, Junhao Wen, Nasrullah, Sun Song, and Zhouping
    Huang. 2019. Fundus Image Classification Using VGG-19 Architecture with PCA and
    SVD. *Symmetry* 11 (2019), 1.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mauldin et al. (2018) Taylor R. Mauldin, Marc E. Canby, Vangelis Metsis, Anne
    H. H. Ngu, and Coralys Cubero Rivera. 2018. SmartFall: A Smartwatch-Based Fall
    Detection System Using Deep Learning. *Sensors* 18, 10 (2018), 3363.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mottaghi and Yeung (2019) Ali Mottaghi and Serena Yeung. 2019. Adversarial Representation
    Active Learning. *CoRR* abs/1912.09720 (2019).
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mudgal et al. (2018) Sidharth Mudgal, Han Li, Theodoros Rekatsinas, AnHai Doan,
    Youngchoon Park, Ganesh Krishnan, Rohit Deep, Esteban Arcaute, and Vijay Raghavendra.
    2018. Deep Learning for Entity Matching: A Design Space Exploration. In *Proceedings
    of the 2018 International Conference on Management of Data, SIGMOD Conference
    2018, Houston, TX, USA, June 10-15, 2018*. ACM, 19–34.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munjal et al. (2020) Prateek Munjal, Nasir Hayat, Munawar Hayat, Jamshid Sourati,
    and Shadab Khan. 2020. Towards Robust and Reproducible Active Learning Using Neural
    Networks. *arXiv* (2020), arXiv–2002.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Murty et al. (2018) Shikhar Murty, Patrick Verga, Luke Vilnis, Irena Radovanovic,
    and Andrew McCallum. 2018. Hierarchical Losses and New Resources for Fine-grained
    Entity Typing and Linking. In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,
    Volume 1: Long Papers*. Association for Computational Linguistics, 97–109.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair and Hinton (2010) Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear
    units improve restricted boltzmann machines. In *ICML*.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nam et al. (2019) Ju Gang Nam, Sunggyun Park, Eui Jin Hwang, Jong Hyuk Lee,
    Kwang Nam Jin, Kun Young Lim, Thienkai Huy Vu, Jae Ho Sohn, Sangheum Hwang, Jin Mo
    Goo, et al. 2019. Development and Validation of Deep Learning–based Automatic
    Detection Algorithm for Malignant Pulmonary Nodules on Chest Radiographs. *Radiology*
    290, 1 (2019), 218–228.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nassif et al. (2019) Ali Bou Nassif, Ismail Shahin, Imtinan B. Attili, Mohammad
    Azzeh, and Khaled Shaalan. 2019. Speech Recognition Using Deep Neural Networks:
    A Systematic Review. *IEEE Access* 7 (2019), 19143–19165.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen and Smeulders (2004) T. Hieu Nguyen and Arnold Smeulders. 2004. Active
    learning using pre-clustering. *ICML* (2004), 79–79.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Norouzzadeh et al. (2019) Mohammad Sadegh Norouzzadeh, Dan Morris, Sara Beery,
    Neel Joshi, Nebojsa Jojic, and Jeff Clune. 2019. A deep active learning system
    for species identification and counting in camera trap images. *CoRR* abs/1910.09716
    (2019).
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Odena et al. (2017) Augustus Odena, Christopher Olah, and Jonathon Shlens. 2017.
    Conditional Image Synthesis With Auxiliary Classifier GANs. (2017), 2642–2651.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ostapuk et al. (2019) Natalia Ostapuk, Jie Yang, and Philippe Cudre-Mauroux.
    2019. ActiveLink: Deep Active Learning for Link Prediction in Knowledge Graphs.
    *WWW ’19: The Web Conference on The World Wide Web Conference WWW 2019* (2019),
    1398–1408.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pang et al. (2002) Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
    Thumbs up? Sentiment Classification using Machine Learning Techniques. In *Proceedings
    of the 2002 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2002, Philadelphia, PA, USA, July 6-7, 2002*. 79–86.
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paoletti et al. (2019) Mercedes Eugenia Paoletti, Juan Mario Haut, Rubén Fernández-Beltran,
    Javier Plaza, Antonio J. Plaza, Jun Yu Li, and Filiberto Pla. 2019. Capsule Networks
    for Hyperspectral Image Classification. *IEEE Transactions on Geoscience and Remote
    Sensing* 57 (2019), 2145–2160.
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2019) Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu,
    Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. SpecAugment: A Simple Data Augmentation
    Method for Automatic Speech Recognition. In *Interspeech 2019, 20th Annual Conference
    of the International Speech Communication Association, Graz, Austria, 15-19 September
    2019*. ISCA, 2613–2617.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pestian et al. (2007) John P. Pestian, Chris Brew, Pawel Matykiewicz, D. J.
    Hovermale, Neil Johnson, K. Bretonnel Cohen, and Wlodzislaw Duch. 2007. A shared
    task involving multi-label classification of clinical free text. In *Biological,
    translational, and clinical language processing, BioNLP@ACL 2007, Prague, Czech
    Republic, June 29, 2007*. Association for Computational Linguistics, 97–104.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phillips (2016) Jeff M Phillips. 2016. Coresets and sketches. *arXiv preprint
    arXiv:1601.00617* (2016).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pinsler et al. (2019) Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel
    Hernandezlobato. 2019. Bayesian Batch Active Learning as Sparse Subset Approximation.
    (2019), 6356–6367.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platanios et al. (2018) Emmanouil Antonios Platanios, Mrinmaya Sachan, Graham
    Neubig, and Tom M. Mitchell. 2018. Contextual Parameter Generation for Universal
    Neural Machine Translation. In *Proceedings of the 2018 Conference on Empirical
    Methods in Natural Language Processing, Brussels, Belgium, October 31 - November
    4, 2018*. Association for Computational Linguistics, 425–435.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Platanios et al. (2019) Emmanouil Antonios Platanios, Otilia Stretcu, Graham
    Neubig, Barnabas Poczos, and Tom M Mitchell. 2019. Competence-based curriculum
    learning for neural machine translation. *arXiv preprint arXiv:1903.09848* (2019).
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pop and Fulop (2018) Remus Pop and Patric Fulop. 2018. Deep Ensemble Bayesian
    Active Learning : Addressing the Mode Collapse issue in Monte Carlo dropout via
    Ensembles. *CoRR* abs/1811.03897 (2018).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prabhu et al. (2019) Ameya Prabhu, Charles Dognin, and Maneesh Singh. 2019.
    Sampling Bias in Deep Active Classification: An Empirical Study. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP
    2019, Hong Kong, China, November 3-7, 2019*. Association for Computational Linguistics,
    4056–4066.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pradhan et al. (2013) Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Hwee Tou
    Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards
    Robust Linguistic Analysis using OntoNotes. In *Proceedings of the Seventeenth
    Conference on Computational Natural Language Learning, CoNLL 2013, Sofia, Bulgaria,
    August 8-9, 2013*. ACL, 143–152.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Priya et al. (2019) Shalini Priya, Saharsh Singh, Sourav Kumar Dandapat, Kripabandhu
    Ghosh, and Joydeep Chandra. 2019. Identifying infrastructure damage during earthquake
    using deep active learning. (2019), 551–552.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2019) Yao Qin, Nicholas Carlini, Ian J. Goodfellow, Garrison W.
    Cottrell, and Colin Raffel. 2019. Imperceptible, Robust, and Targeted Adversarial
    Examples for Automatic Speech Recognition. *ArXiv* abs/1903.10346 (2019).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qu et al. (2020) Zhenshen Qu, Jingda Du, Yong Cao, Qiuyu Guan, and Pengbo Zhao.
    2020. Deep Active Learning for Remote Sensing Object Detection. *CoRR* abs/2003.08793
    (2020).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahhal et al. (2016) M M Al Rahhal, Yakoub Bazi, Haikel Alhichri, Naif Alajlan,
    Farid Melgani, and Ronald R Yager. 2016. Deep learning approach for active classification
    of electrocardiogram signals. *Information Sciences* 345, 345 (2016), 340–354.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranganathan et al. (2016) Hiranmayi Ranganathan, Shayok Chakraborty, and Sethuraman
    Panchanathan. 2016. Multimodal emotion recognition using deep learning architectures.
    In *2016 IEEE Winter Conference on Applications of Computer Vision (WACV)*. IEEE,
    1–9.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ranganathan et al. (2017) Hiranmayi Ranganathan, Hemanth Venkateswara, Shayok
    Chakraborty, and Sethuraman Panchanathan. 2017. Deep active learning for image
    classification. (2017), 3934–3938.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. (2020) Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui
    Li, Xiaojiang Chen, and Xin Wang. 2020. A Comprehensive Survey of Neural Architecture
    Search: Challenges and Solutions. *CoRR* abs/2006.02903 (2020).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. (2015),
    234–241.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rottmann et al. (2018) Matthias Rottmann, Karsten Kahl, and Hanno Gottschalk.
    2018. Deep Bayesian Active Semi-Supervised Learning. In *17th IEEE International
    Conference on Machine Learning and Applications, ICMLA 2018, Orlando, FL, USA,
    December 17-20, 2018*. IEEE, 158–164.
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy and McCallum (2001) Nicholas Roy and Andrew McCallum. 2001. Toward optimal
    active learning through monte carlo estimation of error reduction. *ICML, Williamstown*
    (2001), 441–448.
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. (2018) Soumya Roy, Asim Unmesh, and Vinay P. Namboodiri. 2018. Deep
    active learning for object detection. In *British Machine Vision Conference 2018,
    BMVC 2018, Newcastle, UK, September 3-6, 2018*. BMVA Press, 91.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rumelhart et al. (1986a) David E Rumelhart, Geoffrey E Hinton, and Ronald J
    Williams. 1986a. Learning internal representations by error propagation. *Parallel
    distributed processing: explorations in the microstructure of cognition, vol.
    1* (1986), 318–362.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986b) David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
    Williams. 1986b. Learning representations by back-propagating errors. *Nature*
    323 (1986), 533–536.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sadafi et al. (2019) Ario Sadafi, Niklas Koehler, Asya Makhro, Anna Bogdanova,
    Nassir Navab, Carsten Marr, and Tingying Peng. 2019. Multiclass Deep Active Learning
    for Detecting Red Blood Cell Subtypes in Brightfield Microscopy. (2019), 685–693.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salvaris et al. (2018) Mathew Salvaris, Danielle Dean, and Wee Hyong Tok. 2018.
    Generative Adversarial Networks. *arXiv: Machine Learning* (2018), 187–208.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanderson (2008) Conrad Sanderson. 2008. *Biometric person recognition: Face,
    speech and fusion*. Vol. 4. VDM Publishing.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sang and Meulder (2003) Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction
    to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition.
    In *Proceedings of the Seventh Conference on Natural Language Learning, CoNLL
    2003, Held in cooperation with HLT-NAACL 2003, Edmonton, Canada, May 31 - June
    1, 2003*. ACL, 142–147.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saquil et al. (2018) Yassir Saquil, Kwang In Kim, and Peter Hall. 2018. Ranking
    CGANs: Subjective Control over Semantic Image Attributes. (2018), 131.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sayantan et al. (2018) G Sayantan, P T Kien, and K V Kadambari. 2018. Classification
    of ECG beats using deep belief network and active learning. *Medical & Biological
    Engineering & Computing* 56, 10 (2018), 1887–1898.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scandalea et al. (2019) Melanie Lubrano Di Scandalea, Christian S Perone, Mathieu
    Boudreau, and Julien Cohenadad. 2019. Deep Active Learning for Axon-Myelin Segmentation
    on Histology Data. *arXiv: Computer Vision and Pattern Recognition* (2019).'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheffer et al. (2001) Tobias Scheffer, Christian Decomain, and Stefan Wrobel.
    2001. Active Hidden Markov Models for Information Extraction. In *Advances in
    Intelligent Data Analysis, 4th International Conference, IDA 2001, Cascais, Portugal,
    September 13-15, 2001, Proceedings* *(Lecture Notes in Computer Science, Vol. 2189)*.
    Springer, 309–318.
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schneider et al. (2019) Steffen Schneider, Alexei Baevski, Ronan Collobert,
    and Michael Auli. 2019. wav2vec: Unsupervised Pre-training for Speech Recognition.
    *ArXiv* abs/1904.05862 (2019).'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schröder and Niekler (2020) Christopher Schröder and Andreas Niekler. 2020.
    A Survey of Active Learning for Text Classification using Deep Neural Networks.
    *arXiv preprint arXiv:2008.07267* (2020).
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sener and Savarese (2017) Ozan Sener and Silvio Savarese. 2017. A geometric
    approach to active learning for convolutional neural networks. *arXiv preprint
    arXiv:1708.00489* 7 (2017).
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sener and Savarese (2018) Ozan Sener and Silvio Savarese. 2018. Active Learning
    for Convolutional Neural Networks: A Core-Set Approach. *international conference
    on learning representations* (2018).'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles (2009) Burr Settles. 2009. *Active learning literature survey*. Technical
    Report. University of Wisconsin-Madison Department of Computer Sciences.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles (2012) Burr Settles. 2012. Active Learning, volume 6 of Synthesis Lectures
    on Artificial Intelligence and Machine Learning. *Morgan & Claypool* (2012).
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Settles et al. (2007) Burr Settles, Mark Craven, and Soumya Ray. 2007. Multiple-Instance
    Active Learning. (2007), 1289–1296.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seung et al. (1992) H S Seung, M Opper, and H Sompolinsky. 1992. Query by committee.
    (1992), 287–294.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shardlow et al. (2019) Matthew Shardlow, Meizhi Ju, Maolin Li, Christian O’Reilly,
    Elisabetta Iavarone, John McNaught, and Sophia Ananiadou. 2019. A text mining
    pipeline using active and deep learning aimed at curating information in computational
    neuroscience. *Neuroinformatics* 17, 3 (2019), 391–406.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shelmanov et al. (2019) Artem Shelmanov, Vadim Liventsev, Danil Kireev, Nikita
    Khromov, Alexander Panchenko, Irina Fedulova, and Dmitry V. Dylov. 2019. Active
    Learning with Deep Pre-trained Models for Sequence Tagging of Clinical and Biomedical
    Texts. In *2019 IEEE International Conference on Bioinformatics and Biomedicine,
    BIBM 2019, San Diego, CA, USA, November 18-21, 2019*. IEEE, 482–489.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2017) Yanyao Shen, Hyokun Yun, Zachary C Lipton, Yakov Kronrod,
    and Animashree Anandkumar. 2017. Deep active learning for named entity recognition.
    *arXiv preprint arXiv:1707.05928* (2017).
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shui et al. (2020) Changjian Shui, Fan Zhou, Christian Gagné, and Boyu Wang.
    2020. Deep active learning: Unified and principled method for query and training.
    In *International Conference on Artificial Intelligence and Statistics*. PMLR,
    1308–1318.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siddhant and Lipton (2018) Aditya Siddhant and Zachary C. Lipton. 2018. Deep
    Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale
    Empirical Study. In *Proceedings of the 2018 Conference on Empirical Methods in
    Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018*.
    Association for Computational Linguistics, 2904–2909.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simeoni et al. (2019) Oriane Simeoni, Mateusz Budnik, Yannis Avrithis, and
    Guillaume Gravier. 2019. Rethinking deep active learning: Using unlabeled data
    at model training. *CoRR* abs/1911.08177 (2019).'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. 2015. Very
    Deep Convolutional Networks for Large-Scale Image Recognition. In *3rd International
    Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,
    2015, Conference Track Proceedings*.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sinha et al. (2019) Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. 2019.
    Variational Adversarial Active Learning. In *2019 IEEE/CVF International Conference
    on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2,
    2019*. IEEE, 5971–5980.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smailagic et al. (2020) Asim Smailagic, Pedro Costa, Alex Gaudio, Kartik Khandelwal,
    Mostafa Mirshekari, Jonathon Fagert, Devesh Walawalkar, Susu Xu, Adrian Galdran,
    Pei Zhang, et al. 2020. O-MedAL: Online active deep learning for medical image
    analysis. *Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery*
    10, 4 (2020), e1353.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smailagic et al. (2018) Asim Smailagic, Pedro Costa, Hae Young Noh, Devesh
    Walawalkar, Kartik Khandelwal, Adrian Galdran, Mostafa Mirshekari, Jonathon Fagert,
    Susu Xu, Pei Zhang, et al. 2018. MedAL: Accurate and Robust Deep Active Learning
    for Medical Image Analysis. (2018), 481–488.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith et al. (2018) Justin S Smith, Benjamin Nebgen, Nicholas Lubbers, Olexandr
    Isayev, and Adrian E Roitberg. 2018. Less is more: Sampling chemical space with
    active learning. *Journal of Chemical Physics* 148, 24 (2018), 241733.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohn et al. (2015) Kihyuk Sohn, Xinchen Yan, and Honglak Lee. 2015. Learning
    structured output representation using deep conditional generative models. (2015),
    3483–3491.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song and Yan (2013) Kechen Song and Yunhui Yan. 2013. A noise robust method
    based on completed local binary patterns for hot-rolled steel strip surface defects.
    *Applied Surface Science* 285 (2013), 858–864.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spanhol et al. (2016) Fabio A. Spanhol, Luiz S. Oliveira, Caroline Petitjean,
    and Laurent Heutte. 2016. A Dataset for Breast Cancer Histopathological Image
    Classification. *IEEE Trans. Biomed. Eng.* 63, 7 (2016), 1455–1462.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2017) Akash Srivastava, Lazar Valkoz, Chris Russell, U. Michael
    Gutmann, and A. Charles Sutton. 2017. VEEGAN: Reducing Mode Collapse in GANs using
    Implicit Variational Learning. *neural information processing systems* (2017),
    3310–3320.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2014) Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky,
    Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent
    neural networks from overfitting. *Journal of Machine Learning Research* 15, 1
    (2014), 1929–1958.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stark et al. (2015) Fabian Stark, Caner Hazırbas, Rudolph Triebel, and Daniel
    Cremers. 2015. Captcha recognition with active deep learning. In *Workshop new
    challenges in neural computation*, Vol. 2015\. Citeseer, 94.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stisen et al. (2015) Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger
    Prentow, Mikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen.
    2015. Smart devices are different: Assessing and mitigatingmobile sensing heterogeneities
    for activity recognition. In *Proceedings of the 13th ACM conference on embedded
    networked sensor systems*. 127–140.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swanson et al. (2015) Alexandra Swanson, Margaret Kosmala, Chris Lintott, Robert
    Simpson, Arfon Smith, and Craig Packer. 2015. Snapshot Serengeti, high-frequency
    annotated camera trap images of 40 mammalian species in an African savanna. *Scientific
    data* 2, 1 (2015), 1–14.
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Takahashi et al. (2017) Kuniyuki Takahashi, Tetsuya Ogata, Jun Nakanishi, Gordon
    Cheng, and Shigeki Sugano. 2017. Dynamic motion learning for multi-DOF flexible-joint
    robots using active–passive motor babbling through deep learning. *Advanced Robotics*
    31, 18 (2017), 1002–1015.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019a) Xu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and Tie-Yan Liu.
    2019a. Multilingual Neural Machine Translation with Knowledge Distillation. *ArXiv*
    abs/1902.10461 (2019).
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tan et al. (2019b) Yao Tan, Liu Yang, Qinghua Hu, and Zhibin Du. 2019b. Batch
    Mode Active Learning for Semantic Segmentation Based on Multi-Clue Sample Selection.
    In *Proceedings of the 28th ACM International Conference on Information and Knowledge
    Management, CIKM 2019, Beijing, China, November 3-7, 2019*. ACM, 831–840.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taylor et al. (2017) Joseph Taylor, H. M. Sajjad Hossain, Mohammad Arif Ul
    Alam, Md Abdullah Al Hafiz Khan, Nirmalya Roy, Elizabeth Galik, and Aryya Gangopadhyay.
    2017. SenseBox: A low-cost smart home system. In *2017 IEEE International Conference
    on Pervasive Computing and Communications Workshops, PerCom Workshops 2017, Kona,
    Big Island, HI, USA, March 13-17, 2017*. IEEE, 60–62.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tiedemann (2012) Jörg Tiedemann. 2012. Parallel Data, Tools and Interfaces in
    OPUS. In *Proceedings of the Eighth International Conference on Language Resources
    and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012*. European Language
    Resources Association (ELRA), 2214–2218.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tong (2001) Simon Tong. 2001. *Active learning: theory and applications*. Vol. 1.
    Stanford University USA.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tong and Koller (2002) Simon Tong and Daphne Koller. 2002. Support vector machine
    active learning with applications to text classification. *Journal of Machine
    Learning Research* 2, 1 (2002), 45–66.
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2019) Toan Tran, Thanh-Toan Do, Ian D. Reid, and Gustavo Carneiro.
    2019. Bayesian Generative Active Deep Learning. In *Proceedings of the 36th International
    Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,
    USA* *(Proceedings of Machine Learning Research, Vol. 97)*. PMLR, 6295–6304.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tran et al. (2017) Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer, and
    Ian Reid. 2017. A bayesian data augmentation approach for learning deep models.
    In *Advances in neural information processing systems*. 2797–2806.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vodrahalli et al. (2018) Kailas Vodrahalli, Ke Li, and Jitendra Malik. 2018.
    Are All Training Examples Created Equal? An Empirical Study. *CoRR* abs/1811.12569
    (2018).
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wallace et al. (2014) Byron C. Wallace, Michael J. Paul, Urmimala Sarkar, Thomas A.
    Trikalinos, and Mark Dredze. 2014. A large-scale quantitative analysis of latent
    factors and sentiment in online doctor reviews. *J. Am. Medical Informatics Assoc.*
    21, 6 (2014), 1098–1103.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang and Shang (2014) Dan Wang and Yi Shang. 2014. A new active labeling method
    for deep learning. (2014), 112–119.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2011) Jiannan Wang, Guoliang Li, Jeffrey Xu Yu, and Jianhua Feng.
    2011. Entity Matching: How Similar Is Similar. *Proc. VLDB Endow.* 4, 10 (2011),
    622–633.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) K. Wang, D. Zhang, Y. Li, R. Zhang, and L. Lin. 2017. Cost-Effective
    Active Learning for Deep Image Classification. *IEEE Transactions on Circuits
    and Systems for Video Technology* 27, 12 (2017), 2591–2600.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) Menglin Wang, Baisheng Lai, Zhongming Jin, Xiaojin Gong,
    Jianqiang Huang, and Xiansheng Hua. 2018a. Deep active learning for video-based
    person re-identification. *arXiv preprint arXiv:1812.05785* (2018).
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li,
    Derek F. Wong, and Lidia S. Chao. 2019b. Learning Deep Transformer Models for
    Machine Translation. In *Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
    Volume 1: Long Papers*. Association for Computational Linguistics, 1810–1822.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019a) Wenzhe Wang, Ruiwei Feng, Jintai Chen, Yifei Lu, Tingting
    Chen, Hongyun Yu, Danny Z Chen, and Jian Wu. 2019a. Nodule-Plus R-CNN and Deep
    Self-Paced Active Learning for 3D Instance Segmentation of Pulmonary Nodules.
    *IEEE Access* 7 (2019), 128796–128805.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) Wenzhe Wang, Yifei Lu, Bian Wu, Tingting Chen, Danny Z Chen,
    and Jian Wu. 2018b. Deep Active Self-paced Learning for Accurate Pulmonary Nodule
    Segmentation. (2018), 723–731.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang (2017) William Yang Wang. 2017. ”Liar, Liar Pants on Fire”: A New Benchmark
    Dataset for Fake News Detection. In *Proceedings of the 55th Annual Meeting of
    the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July
    30 - August 4, Volume 2: Short Papers*, Regina Barzilay and Min-Yen Kan (Eds.).
    Association for Computational Linguistics, 422–426.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2016) Zengmao Wang, Bo Du, Lefei Zhang, and Liangpei Zhang. 2016.
    A batch-mode active learning framework by querying discriminative and representative
    samples for hyperspectral image classification. *Neurocomputing* 179 (2016), 88–100.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welling and Teh (2011) Max Welling and Whye Yee Teh. 2011. Bayesian Learning
    via Stochastic Gradient Langevin Dynamics. *ICML* (2011), 681–688.
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2018) Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wanli Ouyang, and
    Yi Yang. 2018. Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification
    by Stepwise Learning. In *2018 IEEE Conference on Computer Vision and Pattern
    Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018*. IEEE Computer
    Society, 5177–5186.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xia et al. (2016) Xide Xia, Pavlos Protopapas, and Finale Doshivelez. 2016.
    Cost-Sensitive Batch Mode Active Learning: Designing Astronomical Observation
    by Optimizing Telescope Time and Telescope Choice. (2016), 477–485.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yalniz et al. (2019) Ismet Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri,
    and Dhruv Kumar Mahajan. 2019. Billion-scale semi-supervised learning for image
    classification. *ArXiv* abs/1905.00546 (2019).
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2017) Yilin Yan, Min Chen, Saad Sadiq, and Mei-Ling Shyu. 2017.
    Efficient imbalanced multimedia concept retrieval by deep learning on spark clusters.
    *International Journal of Multimedia Data Engineering and Management (IJMDEM)*
    8, 1 (2017), 1–20.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2015) Yilin Yan, Min Chen, Mei-Ling Shyu, and Shu-Ching Chen. 2015.
    Deep learning for imbalanced multimedia data classification. In *2015 IEEE international
    symposium on multimedia (ISM)*. IEEE, 483–488.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2018) Jie Yang, Thomas Drake, Andreas Damianou, and Yoelle Maarek.
    2018. Leveraging Crowdsourcing Data For Deep Active Learning – An Application:
    Learning Intents in Alexa. (2018), 23–32.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2017) Lin Yang, Yizhe Zhang, Jianxu Chen, Siyuan Zhang, and Danny Z
    Chen. 2017. Suggestive Annotation: A Deep Active Learning Framework for Biomedical
    Image Segmentation. (2017), 399–407.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2017) C. Yin, B. Qian, S. Cao, X. Li, J. Wei, Q. Zheng, and I. Davidson.
    2017. Deep Similarity-Based Batch Mode Active Learning with Exploration-Exploitation.
    In *2017 IEEE International Conference on Data Mining (ICDM)*. 575–584.
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yoo and Kweon (2019) Donggeun Yoo and In So Kweon. 2019. Learning Loss for Active
    Learning. In *IEEE Conference on Computer Vision and Pattern Recognition, CVPR
    2019, Long Beach, CA, USA, June 16-20, 2019*. Computer Vision Foundation / IEEE,
    93–102.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao,
    Vashisht Madhavan, and Trevor Darrell. 2018. BDD100K: A Diverse Driving Video
    Database with Scalable Annotation Tooling. *CoRR* abs/1805.04687 (2018).'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeng et al. (2018) Jiaming Zeng, Adam Lesnikowski, and Jose M. Alvarez. 2018.
    The Relevance of Bayesian Layer Positioning to Model Uncertainty in Deep Bayesian
    Active Learning. *CoRR* abs/1811.12535 (2018).
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Pei Zhang, Xueying Xu, and Deyi Xiong. 2018. Active Learning
    for Neural Machine Translation. In *2018 International Conference on Asian Language
    Processing, IALP 2018, Bandung, Indonesia, November 15-17, 2018*. IEEE, 153–158.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017a) Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. 2017a.
    CityPersons: A Diverse Dataset for Pedestrian Detection. In *2017 IEEE Conference
    on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July
    21-26, 2017*. IEEE Computer Society, 4457–4465.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2015) Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015. Character-level
    Convolutional Networks for Text Classification. In *Advances in Neural Information
    Processing Systems 28: Annual Conference on Neural Information Processing Systems
    2015, December 7-12, 2015, Montreal, Quebec, Canada*. 649–657.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017b) Ye Zhang, Matthew Lease, and Byron C. Wallace. 2017b. Active
    Discriminative Text Representation Learning. (2017), 3386–3392.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2016) Yizhe Zhang, Michael T. C. Ying, Lin Yang, Anil T. Ahuja,
    and Danny Z. Chen. 2016. Coarse-to-Fine Stacked Fully Convolutional Nets for lymph
    node segmentation in ultrasound images. In *IEEE International Conference on Bioinformatics
    and Biomedicine, BIBM 2016, Shenzhen, China, December 15-18, 2016*. IEEE Computer
    Society, 443–448. [https://doi.org/10.1109/BIBM.2016.7822557](https://doi.org/10.1109/BIBM.2016.7822557)
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2017) Wencang Zhao, Yu Kong, Zhengming Ding, and Yun Fu. 2017.
    Deep Active Learning Through Cognitive Information Parcels. (2017), 952–960.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020) Ziyuan Zhao, Xiaoyan Yang, Bharadwaj Veeravalli, and Zeng
    Zeng. 2020. Deeply Supervised Active Learning for Finger Bones Segmentation. *arXiv
    preprint arXiv:2005.03225* (2020).
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhdanov (2019) Fedor Zhdanov. 2019. Diverse mini-batch Active Learning. *CoRR*
    abs/1901.05954 (2019).
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2016) Liang Zheng, Zhi Bie, Yifan Sun, Jingdong Wang, Chi Su,
    Shengjin Wang, and Qi Tian. 2016. MARS: A Video Benchmark for Large-Scale Person
    Re-Identification. In *Computer Vision - ECCV 2016 - 14th European Conference,
    Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI* *(Lecture
    Notes in Computer Science, Vol. 9910)*. Springer, 868–884.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2010) Shusen Zhou, Qingcai Chen, and Xiaolong Wang. 2010. Active
    Deep Networks for Semi-Supervised Sentiment Classification. (2010), 1515–1523.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou and Schoellig (2019) Siqi Zhou and Angela P Schoellig. 2019. Active Training
    Trajectory Generation for Inverse Dynamics Model Learning with Deep Neural Networks.
    In *2019 IEEE 58th Conference on Decision and Control (CDC)*. IEEE, 1784–1790.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu and Bento (2017) Jia-Jie Zhu and José Bento. 2017. Generative adversarial
    active learning. *arXiv preprint arXiv:1702.07956* (2017).
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2005) Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. 2005. *Semi-supervised
    learning with graphs*. Ph.D. Dissertation. Carnegie Mellon University, language
    technologies institute, school of ….
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziemski et al. (2016) Michal Ziemski, Marcin Junczys-Dowmunt, and Bruno Pouliquen.
    2016. The United Nations Parallel Corpus v1.0\. In *Proceedings of the Tenth International
    Conference on Language Resources and Evaluation LREC 2016, Portorož, Slovenia,
    May 23-28, 2016*. European Language Resources Association (ELRA).
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph and Le (2017) Barret Zoph and Quoc V. Le. 2017. Neural Architecture Search
    with Reinforcement Learning. In *5th International Conference on Learning Representations,
    ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings*.
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
