- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:02:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2001.05566] Image Segmentation Using Deep Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2001.05566] 使用深度学习的图像分割：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2001.05566](https://ar5iv.labs.arxiv.org/html/2001.05566)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2001.05566](https://ar5iv.labs.arxiv.org/html/2001.05566)
- en: 'Image Segmentation Using Deep Learning:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习的图像分割：
- en: A Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一项综述
- en: Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz,
    and Demetri Terzopoulos
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Shervin Minaee、Yuri Boykov、Fatih Porikli、Antonio Plaza、Nasser Kehtarnavaz 和
    Demetri Terzopoulos
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Image segmentation is a key topic in image processing and computer vision with
    applications such as scene understanding, medical image analysis, robotic perception,
    video surveillance, augmented reality, and image compression, among many others.
    Various algorithms for image segmentation have been developed in the literature.
    Recently, due to the success of deep learning models in a wide range of vision
    applications, there has been a substantial amount of works aimed at developing
    image segmentation approaches using deep learning models. In this survey, we provide
    a comprehensive review of the literature at the time of this writing, covering
    a broad spectrum of pioneering works for semantic and instance-level segmentation,
    including fully convolutional pixel-labeling networks, encoder-decoder architectures,
    multi-scale and pyramid based approaches, recurrent networks, visual attention
    models, and generative models in adversarial settings. We investigate the similarity,
    strengths and challenges of these deep learning models, examine the most widely
    used datasets, report performances, and discuss promising future research directions
    in this area.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是图像处理和计算机视觉中的关键主题，应用包括场景理解、医学图像分析、机器人感知、视频监控、增强现实和图像压缩等。文献中已经开发了各种图像分割算法。最近，由于深度学习模型在广泛的视觉应用中的成功，已经有大量的研究致力于使用深度学习模型开发图像分割方法。在这项综述中，我们提供了截至本文撰写时的文献综合评述，涵盖了语义分割和实例级分割的广泛开创性工作，包括完全卷积像素标记网络、编码器-解码器架构、多尺度和金字塔方法、递归网络、视觉注意模型和对抗设置中的生成模型。我们调查了这些深度学习模型的相似性、优势和挑战，检查了最广泛使用的数据集，报告了性能，并讨论了该领域的有前景的未来研究方向。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Image segmentation, deep learning, convolutional neural networks, encoder-decoder
    models, recurrent models, generative models, semantic segmentation, instance segmentation,
    medical image segmentation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割、深度学习、卷积神经网络、编码器-解码器模型、递归模型、生成模型、语义分割、实例分割、医学图像分割。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Image segmentation is an essential component in many visual understanding systems.
    It involves partitioning images (or video frames) into multiple segments or objects
    [[1](#bib.bib1)]. Segmentation plays a central role in a broad range of applications
    [[2](#bib.bib2)], including medical image analysis (e.g., tumor boundary extraction
    and measurement of tissue volumes), autonomous vehicles (e.g., navigable surface
    and pedestrian detection), video surveillance, and augmented reality to count
    a few. Numerous image segmentation algorithms have been developed in the literature,
    from the earliest methods, such as thresholding [[3](#bib.bib3)], histogram-based
    bundling, region-growing [[4](#bib.bib4)], k-means clustering [[5](#bib.bib5)],
    watersheds [[6](#bib.bib6)], to more advanced algorithms such as active contours
    [[7](#bib.bib7)], graph cuts [[8](#bib.bib8)], conditional and Markov random fields
    [[9](#bib.bib9)], and sparsity-based [[10](#bib.bib10)]-[[11](#bib.bib11)] methods.
    Over the past few years, however, deep learning (DL) models have yielded a new
    generation of image segmentation models with remarkable performance improvements
    —often achieving the highest accuracy rates on popular benchmarks— resulting in
    a paradigm shift in the field. For example, Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Image Segmentation Using Deep Learning: A Survey") presents image segmentation
    outputs of a popular deep learning model, DeepLabv3 [[12](#bib.bib12)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割是许多视觉理解系统中的一个重要组成部分。它涉及将图像（或视频帧）划分为多个分段或对象[[1](#bib.bib1)]。分割在广泛的应用中扮演着核心角色[[2](#bib.bib2)]，包括医学图像分析（例如，肿瘤边界提取和组织体积测量）、自动驾驶车辆（例如，可导航表面和行人检测）、视频监控和增强现实等。文献中已经开发了许多图像分割算法，从最早的方法，如阈值法[[3](#bib.bib3)]、基于直方图的捆绑、区域生长[[4](#bib.bib4)]、k均值聚类[[5](#bib.bib5)]、流域法[[6](#bib.bib6)]，到更先进的算法，如主动轮廓[[7](#bib.bib7)]、图割[[8](#bib.bib8)]、条件随机场和马尔可夫随机场[[9](#bib.bib9)]、以及基于稀疏性的方法[[10](#bib.bib10)]-[[11](#bib.bib11)]。然而，近年来，深度学习（DL）模型产生了一代新的图像分割模型，表现出显著的性能提升——通常在流行基准测试中达到最高准确率——从而导致了该领域的范式转变。例如，图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 使用深度学习进行图像分割：综述")展示了一个流行深度学习模型DeepLabv3 [[12](#bib.bib12)]的图像分割结果。
- en: '![Refer to caption](img/78fabde039d2af7682f483b038927d73.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/78fabde039d2af7682f483b038927d73.png)'
- en: 'Figure 1: Segmentation results of DeepLabV3 [[12](#bib.bib12)] on sample images.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：DeepLabV3 [[12](#bib.bib12)] 在样本图像上的分割结果。
- en: Image segmentation can be formulated as a classification problem of pixels with
    semantic labels (semantic segmentation) or partitioning of individual objects
    (instance segmentation). Semantic segmentation performs pixel-level labeling with
    a set of object categories (e.g., human, car, tree, sky) for all image pixels,
    thus it is generally a harder undertaking than image classification, which predicts
    a single label for the entire image. Instance segmentation extends semantic segmentation
    scope further by detecting and delineating each object of interest in the image
    (e.g., partitioning of individual persons).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分割可以被表述为带有语义标签的像素分类问题（语义分割）或个体对象的划分（实例分割）。语义分割对所有图像像素进行像素级标注，并提供一组对象类别（例如，人、车、树、天空），因此通常比图像分类更具挑战性，后者对整张图像预测单一标签。实例分割通过检测和勾画图像中每个感兴趣的对象（例如，划分个体）进一步扩展了语义分割的范围。
- en: Our survey covers the most recent literature in image segmentation and discusses
    more than a hundred deep learning-based segmentation methods proposed until 2019.
    We provide a comprehensive review and insights on different aspects of these methods,
    including the training data, the choice of network architectures, loss functions,
    training strategies, and their key contributions. We present a comparative summary
    of the performance of the reviewed methods and discuss several challenges and
    potential future directions for deep learning-based image segmentation models.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的综述涵盖了图像分割领域最新的文献，并讨论了截至2019年的100多种基于深度学习的分割方法。我们对这些方法的不同方面进行了全面的回顾和见解，包括训练数据、网络架构选择、损失函数、训练策略及其主要贡献。我们提供了所审阅方法性能的比较总结，并讨论了几个挑战和深度学习图像分割模型的潜在未来方向。
- en: 'We group deep learning-based works into the following categories based on their
    main technical contributions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据深度学习技术的主要贡献将相关工作分为以下几类：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Fully convolutional networks
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全卷积网络
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Convolutional models with graphical models
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卷积模型与图形模型
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Encoder-decoder based models
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编码器-解码器基模型
- en: '4.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Multi-scale and pyramid network based models
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多尺度和金字塔网络模型
- en: '5.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: R-CNN based models (for instance segmentation)
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于R-CNN的模型（例如实例分割）
- en: '6.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Dilated convolutional models and DeepLab family
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 膨胀卷积模型和DeepLab家族
- en: '7.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Recurrent neural network based models
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于递归神经网络的模型
- en: '8.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: Attention-based models
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于注意力的模型
- en: '9.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: Generative models and adversarial training
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成模型与对抗训练
- en: '10.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: Convolutional models with active contour models
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与主动轮廓模型结合的卷积模型
- en: '11.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: Other models
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他模型
- en: 'Some the key contributions of this survey paper can be summarized as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查论文的一些主要贡献可以总结如下：
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This survey covers the contemporary literature with respect to segmentation
    problem, and overviews more than 100 segmentation algorithms proposed till 2019,
    grouped into 10 categories.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本调查涵盖了关于分割问题的现代文献，并综述了截至2019年提出的100多种分割算法，这些算法被分为10个类别。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comprehensive review and an insightful analysis of different aspects
    of segmentation algorithms using deep learning, including the training data, the
    choice of network architectures, loss functions, training strategies, and their
    key contributions.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对使用深度学习的分割算法各个方面的全面回顾和深刻分析，包括训练数据、网络架构选择、损失函数、训练策略及其关键贡献。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide an overview of around 20 popular image segmentation datasets, grouped
    into 2D, 2.5D (RGB-D), and 3D images.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了大约20种流行的图像分割数据集的概述，这些数据集被分为2D、2.5D（RGB-D）和3D图像。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide a comparative summary of the properties and performance of the reviewed
    methods for segmentation purposes, on popular benchmarks.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了对所评审方法在流行基准上的属性和性能的比较总结。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide several challenges and potential future directions for deep learning-based
    image segmentation.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了深度学习图像分割的若干挑战和潜在未来方向。
- en: 'The remainder of this survey is organized as follows: Section [2](#S2 "2 Overview
    of Deep Neural Networks ‣ Image Segmentation Using Deep Learning: A Survey") provides
    an overview of popular deep neural network architectures that serve as the backbone
    of many modern segmentation algorithms. Section [3](#S3 "3 DL-Based Image Segmentation
    Models ‣ Image Segmentation Using Deep Learning: A Survey") provides a comprehensive
    overview of the most significant state-of-the-art deep learning based segmentation
    models, more than 100 till 2020. We also discuss their strengths and contributions
    over previous works here. Section [4](#S4 "4 Image Segmentation Datasets ‣ Image
    Segmentation Using Deep Learning: A Survey") reviews some of the most popular
    image segmentation datasets and their characteristics. Section [5.1](#S5.SS1 "5.1
    Metrics For Segmentation Models ‣ 5 Performance Review ‣ Image Segmentation Using
    Deep Learning: A Survey") reviews popular metrics for evaluating deep-learning-based
    segmentation models. In Section [5.2](#S5.SS2 "5.2 Quantitative Performance of
    DL-Based Models ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning:
    A Survey"), we report the quantitative results and experimental performance of
    these models. In Section [6](#S6 "6 Challenges and Opportunities ‣ Image Segmentation
    Using Deep Learning: A Survey"), we discuss the main challenges and future directions
    for deep learning-based segmentation methods. Finally, we present our conclusions
    in Section [7](#S7 "7 Conclusions ‣ Image Segmentation Using Deep Learning: A
    Survey").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查的其余部分组织如下：第[2](#S2 "2 Overview of Deep Neural Networks ‣ Image Segmentation
    Using Deep Learning: A Survey")节提供了作为许多现代分割算法支柱的流行深度神经网络架构概述。第[3](#S3 "3 DL-Based
    Image Segmentation Models ‣ Image Segmentation Using Deep Learning: A Survey")节提供了对最重要的前沿深度学习分割模型的全面概述，到2020年有100多种。我们还在这里讨论了它们的优点和对之前工作的贡献。第[4](#S4
    "4 Image Segmentation Datasets ‣ Image Segmentation Using Deep Learning: A Survey")节回顾了一些最受欢迎的图像分割数据集及其特点。第[5.1](#S5.SS1
    "5.1 Metrics For Segmentation Models ‣ 5 Performance Review ‣ Image Segmentation
    Using Deep Learning: A Survey")节回顾了评估深度学习分割模型的流行指标。在第[5.2](#S5.SS2 "5.2 Quantitative
    Performance of DL-Based Models ‣ 5 Performance Review ‣ Image Segmentation Using
    Deep Learning: A Survey")节，我们报告了这些模型的定量结果和实验性能。在第[6](#S6 "6 Challenges and Opportunities
    ‣ Image Segmentation Using Deep Learning: A Survey")节，我们讨论了基于深度学习的分割方法的主要挑战和未来方向。最后，我们在第[7](#S7
    "7 Conclusions ‣ Image Segmentation Using Deep Learning: A Survey")节中总结了我们的结论。'
- en: 2 Overview of Deep Neural Networks
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络概述
- en: This section provides an overview of some of the most prominent deep learning
    architectures used by the computer vision community, including convolutional neural
    networks (CNNs) [[13](#bib.bib13)], recurrent neural networks (RNNs) and long
    short term memory (LSTM) [[14](#bib.bib14)], encoder-decoders [[15](#bib.bib15)],
    and generative adversarial networks (GANs) [[16](#bib.bib16)]. With the popularity
    of deep learning in recent years, several other deep neural architectures have
    been proposed, such as transformers, capsule networks, gated recurrent units,
    spatial transformer networks, etc., which will not be covered here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了计算机视觉社区使用的一些最著名的深度学习架构，包括卷积神经网络（CNNs）[[13](#bib.bib13)]、递归神经网络（RNNs）和长短期记忆网络（LSTM）[[14](#bib.bib14)]、编码器-解码器[[15](#bib.bib15)]以及生成对抗网络（GANs）[[16](#bib.bib16)]。随着近年来深度学习的普及，提出了若干其他深度神经网络架构，如transformers、胶囊网络、门控递归单元、空间变换网络等，这些内容将在这里不作讨论。
- en: It is worth mentioning that in some cases the DL-models can be trained from
    scratch on new applications/datasets (assuming a sufficient quantity of labeled
    training data), but in many cases there are not enough labeled data available
    to train a model from scratch and one can use transfer learning to tackle this
    problem. In transfer learning, a model trained on one task is re-purposed on another
    (related) task, usually by some adaptation process toward the new task. For example,
    one can imagine adapting an image classification model trained on ImageNet to
    a different task, such as texture classification, or face recognition. In image
    segmentation case, many people use a model trained on ImageNet (a larger dataset
    than most of image segmentation datasets), as the encoder part of the network,
    and re-train their model from those initial weights. The assumption here is that
    those pre-trained models should be able to capture the semantic information of
    the image required for segmentation, and therefore enabling them to train the
    model with less labeled samples.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，在某些情况下，深度学习模型可以在新的应用/数据集上从头开始训练（假设有足够数量的标记训练数据），但在许多情况下，可能没有足够的标记数据来从头开始训练模型，这时可以使用迁移学习来解决这个问题。在迁移学习中，一个在某个任务上训练好的模型被重新用于另一个（相关的）任务，通常通过某种适应过程来完成。例如，可以考虑将一个在ImageNet上训练的图像分类模型调整到不同的任务，如纹理分类或人脸识别。在图像分割的情况下，许多人使用在ImageNet（一个比大多数图像分割数据集更大的数据集）上训练的模型作为网络的编码器部分，并从这些初始权重重新训练他们的模型。这里的假设是，这些预训练的模型应该能够捕捉到图像分割所需的语义信息，从而使它们能够用更少的标记样本来训练模型。
- en: 2.1 Convolutional Neural Networks (CNNs)
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 卷积神经网络（CNNs）
- en: 'CNNs are among the most successful and widely used architectures in the deep
    learning community, especially for computer vision tasks. CNNs were initially
    proposed by Fukushima in his seminal paper on the “Neocognitron” [[17](#bib.bib17)],
    based on the hierarchical receptive field model of the visual cortex proposed
    by Hubel and Wiesel. Subsequently, Waibel et al. [[18](#bib.bib18)] introduced
    CNNs with weights shared among temporal receptive fields and backpropagation training
    for phoneme recognition, and LeCun et al. [[13](#bib.bib13)] developed a CNN architecture
    for document recognition (Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Convolutional Neural
    Networks (CNNs) ‣ 2 Overview of Deep Neural Networks ‣ Image Segmentation Using
    Deep Learning: A Survey")).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 是深度学习社区中最成功且广泛使用的架构之一，特别是在计算机视觉任务中。CNNs 最初由福岛在其关于“Neocognitron”的开创性论文中提出[[17](#bib.bib17)]，其基础是由Hubel和Wiesel提出的视觉皮层的层次性接收场模型。随后，Waibel
    等人[[18](#bib.bib18)] 引入了在时间接收场之间共享权重的CNNs，并利用反向传播训练进行音素识别，LeCun 等人[[13](#bib.bib13)]
    开发了用于文档识别的CNN架构（见图[2](#S2.F2 "图 2 ‣ 2.1 卷积神经网络（CNNs） ‣ 2 深度神经网络概述 ‣ 使用深度学习的图像分割：综述")）。
- en: '![Refer to caption](img/9ad14d6844826233f85cb8df11c9c04c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9ad14d6844826233f85cb8df11c9c04c.png)'
- en: 'Figure 2: Architecture of convolutional neural networks. From [[13](#bib.bib13)].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：卷积神经网络的架构。来源 [[13](#bib.bib13)]。
- en: 'CNNs mainly consist of three type of layers: i) convolutional layers, where
    a kernel (or filter) of weights is convolved in order to extract features; ii)
    nonlinear layers, which apply an activation function on feature maps (usually
    element-wise) in order to enable the modeling of non-linear functions by the network;
    and iii) pooling layers, which replace a small neighborhood of a feature map with
    some statistical information (mean, max, etc.) about the neighborhood and reduce
    spatial resolution. The units in layers are locally connected; that is, each unit
    receives weighted inputs from a small neighborhood, known as the receptive field,
    of units in the previous layer. By stacking layers to form multi-resolution pyramids,
    the higher-level layers learn features from increasingly wider receptive fields.
    The main computational advantage of CNNs is that all the receptive fields in a
    layer share weights, resulting in a significantly smaller number of parameters
    than fully-connected neural networks. Some of the most well-known CNN architectures
    include: AlexNet [[19](#bib.bib19)], VGGNet [[20](#bib.bib20)], ResNet [[21](#bib.bib21)],
    GoogLeNet [[22](#bib.bib22)], MobileNet [[23](#bib.bib23)], and DenseNet [[24](#bib.bib24)].'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 主要由三种类型的层组成：i) 卷积层，其中的核（或滤波器）通过卷积操作以提取特征；ii) 非线性层，这些层对特征图（通常是逐元素）的激活函数进行应用，以便网络能够建模非线性函数；iii)
    池化层，这些层用一些关于邻域的统计信息（均值、最大值等）替换特征图的小邻域，从而减少空间分辨率。层中的单元是局部连接的；也就是说，每个单元接收来自前一层小邻域的加权输入，该邻域称为感受野。通过堆叠层形成多分辨率金字塔，高层学习来自越来越宽的感受野的特征。CNN
    的主要计算优势在于同一层中的所有感受野共享权重，从而使参数数量比全连接神经网络显著减少。一些最著名的 CNN 架构包括：AlexNet [[19](#bib.bib19)]、VGGNet
    [[20](#bib.bib20)]、ResNet [[21](#bib.bib21)]、GoogLeNet [[22](#bib.bib22)]、MobileNet
    [[23](#bib.bib23)] 和 DenseNet [[24](#bib.bib24)]。
- en: 2.2 Recurrent Neural Networks (RNNs) and the LSTM
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 循环神经网络（RNNs）与长短期记忆（LSTM）
- en: 'RNNs [[25](#bib.bib25)] are widely used to process sequential data, such as
    speech, text, videos, and time-series, where data at any given time/position depends
    on previously encountered data. At each time-stamp the model collects the input
    from the current time $X_{i}$ and the hidden state from the previous step $h_{i-1}$,
    and outputs a target value and a new hidden state (Figure [3](#S2.F3 "Figure 3
    ‣ 2.2 Recurrent Neural Networks (RNNs) and the LSTM ‣ 2 Overview of Deep Neural
    Networks ‣ Image Segmentation Using Deep Learning: A Survey")).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs [[25](#bib.bib25)] 广泛用于处理序列数据，如语音、文本、视频和时间序列，其中任何给定时间/位置的数据依赖于之前遇到的数据。在每个时间戳，模型收集来自当前时间的输入
    $X_{i}$ 和来自上一步的隐藏状态 $h_{i-1}$，并输出目标值和新的隐藏状态（图 [3](#S2.F3 "图 3 ‣ 2.2 循环神经网络（RNNs）与长短期记忆（LSTM）
    ‣ 2 深度神经网络概述 ‣ 基于深度学习的图像分割：综述")）。
- en: '![Refer to caption](img/92f7564bd8245e07171b146942f15fba.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/92f7564bd8245e07171b146942f15fba.png)'
- en: 'Figure 3: Architecture of a simple recurrent neural network.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：简单循环神经网络的结构。
- en: 'RNNs are typically problematic with long sequences as they cannot capture long-term
    dependencies in many real-world applications (although they exhibit no theoretical
    limitations in this regard) and often suffer from gradient vanishing or exploding
    problems. However, a type of RNNs called Long Short Term Memory (LSTM) [[14](#bib.bib14)]
    is designed to avoid these issues. The LSTM architecture (Figure [4](#S2.F4 "Figure
    4 ‣ 2.2 Recurrent Neural Networks (RNNs) and the LSTM ‣ 2 Overview of Deep Neural
    Networks ‣ Image Segmentation Using Deep Learning: A Survey")) includes three
    gates (input gate, output gate, forget gate), which regulate the flow of information
    into and out from a memory cell, which stores values over arbitrary time intervals.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 通常在处理长序列时存在问题，因为它们无法捕捉许多实际应用中的长期依赖关系（尽管在理论上没有此类限制），并且经常出现梯度消失或爆炸问题。然而，一种称为长短期记忆（LSTM）的
    RNN 类型 [[14](#bib.bib14)] 被设计用来避免这些问题。LSTM 架构（图 [4](#S2.F4 "图 4 ‣ 2.2 循环神经网络（RNNs）与长短期记忆（LSTM）
    ‣ 2 深度神经网络概述 ‣ 基于深度学习的图像分割：综述")）包括三个门（输入门、输出门、遗忘门），这些门调节信息进出一个记忆单元，记忆单元可以在任意时间间隔内存储值。
- en: '![Refer to caption](img/ef6ae8d7f5e0346899e41565f0c14c61.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ef6ae8d7f5e0346899e41565f0c14c61.png)'
- en: 'Figure 4: Architecture of a standard LSTM module. Courtesy of Karpathy.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：标准 LSTM 模块的结构。由 Karpathy 提供。
- en: 2.3 Encoder-Decoder and Auto-Encoder Models
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 编码器-解码器和自动编码器模型
- en: 'Encoder-Decoder models are a family of models which learn to map data-points
    from an input domain to an output domain via a two-stage network: The encoder,
    represented by an encoding function $z=f(x)$, compresses the input into a latent-space
    representation; the decoder, $y=g(z)$, aims to predict the output from the latent
    space representation [[26](#bib.bib26), [15](#bib.bib15)]. The latent representation
    here essentially refers to a feature (vector) representation, which is able to
    capture the underlying semantic information of the input that is useful for predicting
    the output. These models are extremely popular in image-to-image translation problems,
    as well as for sequence-to-sequence models in NLP. Figure [5](#S2.F5 "Figure 5
    ‣ 2.3 Encoder-Decoder and Auto-Encoder Models ‣ 2 Overview of Deep Neural Networks
    ‣ Image Segmentation Using Deep Learning: A Survey") illustrates the block-diagram
    of a simple encoder-decoder model. These models are usually trained by minimizing
    the reconstruction loss $L(y,\hat{y})$, which measures the differences between
    the ground-truth output $y$ and the subsequent reconstruction $\hat{y}$. The output
    here could be an enhanced version of the image (such as in image de-blurring,
    or super-resolution), or a segmentation map. Auto-encoders are special case of
    encoder-decoder models in which the input and output are the same.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模型是一类通过两阶段网络将数据点从输入域映射到输出域的模型：编码器，由编码函数$z=f(x)$表示，将输入压缩为潜在空间表示；解码器$y=g(z)$，旨在从潜在空间表示中预测输出[[26](#bib.bib26),
    [15](#bib.bib15)]。这里的潜在表示本质上指的是特征（向量）表示，能够捕捉输入的基本语义信息，这些信息对于预测输出是有用的。这些模型在图像到图像翻译问题中非常流行，也用于自然语言处理中的序列到序列模型。图 [5](#S2.F5
    "图5 ‣ 2.3 编码器-解码器和自编码器模型 ‣ 2 深度神经网络概述 ‣ 使用深度学习的图像分割：综述")展示了一个简单的编码器-解码器模型的框图。这些模型通常通过最小化重建损失$L(y,\hat{y})$来训练，该损失衡量了真实输出$y$与后续重建$\hat{y}$之间的差异。这里的输出可以是图像的增强版本（例如图像去模糊或超分辨率），也可以是分割图。自编码器是编码器-解码器模型的特例，其中输入和输出是相同的。
- en: '![Refer to caption](img/f589eedfcfee747bf748cae135fcd3de.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f589eedfcfee747bf748cae135fcd3de.png)'
- en: 'Figure 5: The architecture of a simple encoder-decoder model.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：一个简单的编码器-解码器模型的架构。
- en: 2.4 Generative Adversarial Networks (GANs)
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 生成对抗网络（GANs）
- en: 'GANs are a newer family of deep learning models [[16](#bib.bib16)]. They consist
    of two networks—a generator and a discriminator (Figure [6](#S2.F6 "Figure 6 ‣
    2.4 Generative Adversarial Networks (GANs) ‣ 2 Overview of Deep Neural Networks
    ‣ Image Segmentation Using Deep Learning: A Survey")). The generator network $G=z\rightarrow
    y$ in the conventional GAN learns a mapping from noise $z$ (with a prior distribution)
    to a target distribution $y$, which is similar to the “real” samples. The discriminator
    network $D$ attempts to distinguish the generated samples (“fakes”) from the “real”
    ones. The GAN loss function may be written as $\mathcal{L}_{\text{GAN}}=\mathbb{E}_{x\sim
    p_{\text{data}}(x)}[\log D(x)]+\mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G(z)))]$.
    We can regard the GAN as a minimax game between $G$ and $D$, where $D$ is trying
    to minimize its classification error in distinguishing fake samples from real
    ones, hence maximizing the loss function, and $G$ is trying to maximize the discriminator
    network’s error, hence minimizing the loss function. After training the model,
    the trained generator model would be $G^{*}=\text{arg}\ \min_{G}\max_{D}\ \mathcal{L}_{\text{GAN}}$
    In practice, this function may not provide enough gradient for effectively training
    $G$, specially initially (when $D$ can easily discriminate fake samples from real
    ones). Instead of minimizing $\mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G(z)))]$, a
    possible solution is to train it to maximize $\mathbb{E}_{z\sim p_{z}(z)}[\log(D(G(z)))]$.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'GANs 是一类较新的深度学习模型[[16](#bib.bib16)]。它们由两个网络组成——生成器和鉴别器（图[6](#S2.F6 "Figure
    6 ‣ 2.4 Generative Adversarial Networks (GANs) ‣ 2 Overview of Deep Neural Networks
    ‣ Image Segmentation Using Deep Learning: A Survey")）。传统GAN中的生成器网络 $G=z\rightarrow
    y$ 学习将噪声 $z$（具有先验分布）映射到目标分布 $y$，该分布类似于“真实”样本。鉴别器网络 $D$ 试图将生成的样本（“假样本”）与“真实”样本区分开。GAN损失函数可以写成
    $\mathcal{L}_{\text{GAN}}=\mathbb{E}_{x\sim p_{\text{data}}(x)}[\log D(x)]+\mathbb{E}_{z\sim
    p_{z}(z)}[\log(1-D(G(z)))]$。我们可以将GAN视为 $G$ 和 $D$ 之间的一个极小极大博弈，其中 $D$ 试图最小化其在区分假样本与真实样本中的分类错误，从而最大化损失函数，而
    $G$ 试图最大化鉴别器网络的错误，从而最小化损失函数。在训练模型后，训练后的生成器模型将是 $G^{*}=\text{arg}\ \min_{G}\max_{D}\
    \mathcal{L}_{\text{GAN}}$。在实践中，这个函数可能无法为有效训练 $G$ 提供足够的梯度，特别是在初期（当 $D$ 可以轻易区分假样本和真实样本时）。一个可能的解决方案是训练它以最大化
    $\mathbb{E}_{z\sim p_{z}(z)}[\log(D(G(z)))]$，而不是最小化 $\mathbb{E}_{z\sim p_{z}(z)}[\log(1-D(G(z)))]$。'
- en: '![Refer to caption](img/9294aa533cb02e798fb54592446a9c2a.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9294aa533cb02e798fb54592446a9c2a.png)'
- en: 'Figure 6: Architecture of a generative adversarial network.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：生成对抗网络的结构。
- en: Since the invention of GANs, researchers have endeavored to improve/modify GANs
    several ways. For example, Radford et al. [[27](#bib.bib27)] proposed a convolutional
    GAN model, which works better than fully-connected networks when used for image
    generation. Mirza [[28](#bib.bib28)] proposed a conditional GAN model that can
    generate images conditioned on class labels, which enables one to generate samples
    with specified labels. Arjovsky et al. [[29](#bib.bib29)] proposed a new loss
    function based on the Wasserstein (a.k.a. earth mover’s distance) to better estimate
    the distance for cases in which the distribution of real and generated samples
    are non-overlapping (hence the Kullback–Leiber divergence is not a good measure
    of the distance). For additional works, we refer the reader to [[30](#bib.bib30)].
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 自从GANs（生成对抗网络）被发明以来，研究人员已经在多种方式上致力于改进/修改GANs。例如，Radford等人[[27](#bib.bib27)]
    提出了一个卷积GAN模型，该模型在用于图像生成时比全连接网络表现更好。Mirza[[28](#bib.bib28)] 提出了一个条件GAN模型，可以根据类别标签生成图像，从而生成具有指定标签的样本。Arjovsky等人[[29](#bib.bib29)]
    提出了一个基于Wasserstein（也称为地球移动者距离）的新损失函数，以更好地估计当真实样本和生成样本的分布不重叠时的距离（因此Kullback–Leiber散度不是一个好的距离度量）。有关更多工作，请参阅[[30](#bib.bib30)]。
- en: 3 DL-Based Image Segmentation Models
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基于深度学习的图像分割模型
- en: 'This section provides a detailed review of more than a hundred deep learning-based
    segmentation methods proposed until 2019, grouped into 10 categories (based on
    their model architecture). It is worth mentioning that there are some pieces that
    are common among many of these works, such as having encoder and decoder parts,
    skip-connections, multi-scale analysis, and more recently the use of dilated convolution.
    Because of this, it is difficult to mention the unique contributions of each work,
    but easier to group them based on their underlying architectural contribution
    over previous works. Besides the architectural categorization of these models,
    one can also group them based on the segmentation goal into: semantic, instance,
    panoptic, and depth segmentation categories. But due to the big difference in
    terms of volume of work in those tasks, we decided to follow the architectural
    grouping.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对截至 2019 年提出的超过一百种基于深度学习的分割方法的详细回顾，这些方法被分为 10 个类别（基于其模型架构）。值得一提的是，这些研究中有一些共同的部分，如具有编码器和解码器部分、跳跃连接、多尺度分析，以及最近使用扩张卷积。因此，虽然难以提及每个工作的独特贡献，但更容易根据其对先前工作的架构贡献进行分类。除了这些模型的架构分类，还可以根据分割目标将其分为：语义分割、实例分割、全景分割和深度分割类别。但由于这些任务的工作量差异较大，我们决定遵循架构分组。
- en: 3.1 Fully Convolutional Networks
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 完全卷积网络
- en: 'Long et al. [[31](#bib.bib31)] proposed one of the first deep learning works
    for semantic image segmentation, using a fully convolutional network (FCN). An
    FCN (Figure [7](#S3.F7 "Figure 7 ‣ 3.1 Fully Convolutional Networks ‣ 3 DL-Based
    Image Segmentation Models ‣ Image Segmentation Using Deep Learning: A Survey"))
    includes only convolutional layers, which enables it to take an image of arbitrary
    size and produce a segmentation map of the same size. The authors modified existing
    CNN architectures, such as VGG16 and GoogLeNet, to manage non-fixed sized input
    and output, by replacing all fully-connected layers with the fully-convolutional
    layers. As a result, the model outputs a spatial segmentation map instead of classification
    scores.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Long 等人 [[31](#bib.bib31)] 提出了第一个深度学习语义图像分割的研究之一，使用了完全卷积网络（FCN）。一个 FCN（图 [7](#S3.F7
    "图 7 ‣ 3.1 完全卷积网络 ‣ 3 基于 DL 的图像分割模型 ‣ 使用深度学习的图像分割：综述")）仅包括卷积层，使其能够接受任意大小的图像，并生成相同大小的分割图。作者修改了现有的
    CNN 架构，如 VGG16 和 GoogLeNet，通过将所有全连接层替换为完全卷积层，以处理非固定大小的输入和输出。因此，模型输出的是空间分割图，而不是分类分数。
- en: '![Refer to caption](img/5a7cd9f069e9cad1d3fb42915a25d90f.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/5a7cd9f069e9cad1d3fb42915a25d90f.png)'
- en: 'Figure 7: A fully convolutional image segmentation network. The FCN learns
    to make dense, pixel-wise predictions. From [[31](#bib.bib31)].'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：一个完全卷积的图像分割网络。FCN 学习进行密集的像素级预测。来自 [[31](#bib.bib31)]。
- en: 'Through the use of skip connections in which feature maps from the final layers
    of the model are up-sampled and fused with feature maps of earlier layers (Figure [8](#S3.F8
    "Figure 8 ‣ 3.1 Fully Convolutional Networks ‣ 3 DL-Based Image Segmentation Models
    ‣ Image Segmentation Using Deep Learning: A Survey")), the model combines semantic
    information (from deep, coarse layers) and appearance information (from shallow,
    fine layers) in order to produce accurate and detailed segmentations. The model
    was tested on PASCAL VOC, NYUDv2, and SIFT Flow, and achieved state-of-the-art
    segmentation performance.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用跳跃连接，将模型最后几层的特征图上采样并与早期层的特征图融合（图 [8](#S3.F8 "图 8 ‣ 3.1 完全卷积网络 ‣ 3 基于 DL
    的图像分割模型 ‣ 使用深度学习的图像分割：综述")），模型结合了语义信息（来自深层粗糙层）和外观信息（来自浅层精细层），以生成准确且详细的分割。该模型在
    PASCAL VOC、NYUDv2 和 SIFT Flow 上进行了测试，并达到了最先进的分割性能。
- en: '![Refer to caption](img/f3c1fe0344ff501eb1fdd10cc8c7dea6.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f3c1fe0344ff501eb1fdd10cc8c7dea6.png)'
- en: 'Figure 8: Skip connections combine coarse, high-level information and fine,
    low-level information. From [[31](#bib.bib31)].'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：跳跃连接结合了粗略的高层信息和精细的低层信息。来自 [[31](#bib.bib31)]。
- en: This work is considered a milestone in image segmentation, demonstrating that
    deep networks can be trained for semantic segmentation in an end-to-end manner
    on variable-sized images. However, despite its popularity and effectiveness, the
    conventional FCN model has some limitations—it is not fast enough for real-time
    inference, it does not take into account the global context information in an
    efficient way, and it is not easily transferable to 3D images. Several efforts
    have attempted to overcome some of the limitations of the FCN.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作被认为是图像分割领域的一个里程碑，展示了深度网络可以在可变尺寸的图像上以端到端的方式进行语义分割。然而，尽管它很受欢迎并且有效，但传统的FCN模型存在一些局限性——它的实时推理速度不够快，未能有效考虑全局上下文信息，并且不容易转移到3D图像。为了克服FCN的一些局限性，已有几项努力进行尝试。
- en: 'For instance, Liu et al. [[32](#bib.bib32)] proposed a model called ParseNet,
    to address an issue with FCN—ignoring global context information. ParseNet adds
    global context to FCNs by using the average feature for a layer to augment the
    features at each location. The feature map for a layer is pooled over the whole
    image resulting in a context vector. This context vector is normalized and unpooled
    to produce new feature maps of the same size as the initial ones. These feature
    maps are then concatenated. In a nutshell, ParseNet is an FCN with the described
    module replacing the convolutional layers (Figure [9](#S3.F9 "Figure 9 ‣ 3.1 Fully
    Convolutional Networks ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation
    Using Deep Learning: A Survey")).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，刘等人[[32](#bib.bib32)] 提出了一个叫做ParseNet的模型，以解决FCN忽略全局上下文信息的问题。ParseNet通过使用每层的平均特征来增强每个位置的特征，从而将全局上下文添加到FCN中。一个层的特征图在整个图像上进行池化，生成一个上下文向量。这个上下文向量被归一化并反池化，以生成与初始特征图相同大小的新特征图。这些特征图随后被连接在一起。简而言之，ParseNet是一个FCN，其中描述的模块替代了卷积层（图 [9](#S3.F9
    "图 9 ‣ 3.1 完全卷积网络 ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")）。
- en: '![Refer to caption](img/11713ab06a25242f0470e386f6a41163.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/11713ab06a25242f0470e386f6a41163.png)'
- en: 'Figure 9: ParseNet, showing the use of extra global context to produce smoother
    segmentation (d) than an FCN (c). From [[32](#bib.bib32)].'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: ParseNet，展示了使用额外的全局上下文来生成比FCN（c）更平滑的分割（d）。来自[[32](#bib.bib32)]。'
- en: FCNs have been applied to a variety of segmentation problems, such as brain
    tumor segmentation [[33](#bib.bib33)], instance-aware semantic segmentation [[34](#bib.bib34)],
    skin lesion segmentation [[35](#bib.bib35)], and iris segmentation [[36](#bib.bib36)].
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: FCN已应用于多种分割问题，如脑肿瘤分割[[33](#bib.bib33)]、实例感知语义分割[[34](#bib.bib34)]、皮肤病变分割[[35](#bib.bib35)]和虹膜分割[[36](#bib.bib36)]。
- en: 3.2 Convolutional Models With Graphical Models
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 卷积模型与图形模型
- en: As discussed, FCN ignores potentially useful scene-level semantic context. To
    integrate more context, several approaches incorporate probabilistic graphical
    models, such as Conditional Random Fields (CRFs) and Markov Random Field (MRFs),
    into DL architectures.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，FCN忽略了潜在的有用场景级语义上下文。为了整合更多上下文，几种方法将概率图模型，如条件随机场（CRF）和马尔可夫随机场（MRF），纳入深度学习架构中。
- en: 'Chen et al. [[37](#bib.bib37)] proposed a semantic segmentation algorithm based
    on the combination of CNNs and fully connected CRFs (Figure [10](#S3.F10 "Figure
    10 ‣ 3.2 Convolutional Models With Graphical Models ‣ 3 DL-Based Image Segmentation
    Models ‣ Image Segmentation Using Deep Learning: A Survey")). They showed that
    responses from the final layer of deep CNNs are not sufficiently localized for
    accurate object segmentation (due to the invariance properties that make CNNs
    good for high level tasks such as classification). To overcome the poor localization
    property of deep CNNs, they combined the responses at the final CNN layer with
    a fully-connected CRF. They showed that their model is able to localize segment
    boundaries at a higher accuracy rate than it was possible with previous methods.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[37](#bib.bib37)] 提出了基于CNN和全连接CRF组合的语义分割算法（图 [10](#S3.F10 "图 10 ‣ 3.2 卷积模型与图形模型
    ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")）。他们展示了深度CNN的最终层响应在对象分割时定位不够准确（由于CNN的不可变性特性使其在分类等高层任务中表现良好）。为了克服深度CNN的较差定位特性，他们将最终CNN层的响应与全连接CRF结合起来。他们的模型能够以比以前的方法更高的准确率定位分割边界。
- en: '![Refer to caption](img/6f4c1e26cab0882697c0e12ad28f6b12.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f4c1e26cab0882697c0e12ad28f6b12.png)'
- en: 'Figure 10: A CNN+CRF model. The coarse score map of a CNN is up-sampled via
    interpolated interpolation, and fed to a fully-connected CRF to refine the segmentation
    result. From [[37](#bib.bib37)].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：一个 CNN+CRF 模型。CNN 的粗略得分图通过插值上采样，并送入全连接 CRF 以细化分割结果。来源于 [[37](#bib.bib37)]。
- en: Schwing and Urtasun [[38](#bib.bib38)] proposed a fully-connected deep structured
    network for image segmentation. They presented a method that jointly trains CNNs
    and fully-connected CRFs for semantic image segmentation, and achieved encouraging
    results on the challenging PASCAL VOC 2012 dataset. In [[39](#bib.bib39)], Zheng
    et al. proposed a similar semantic segmentation approach integrating CRF with
    CNN.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Schwing 和 Urtasun [[38](#bib.bib38)] 提出了一个用于图像分割的全连接深度结构网络。他们提出了一种方法，联合训练 CNN
    和全连接 CRF 进行语义图像分割，并在具有挑战性的 PASCAL VOC 2012 数据集上取得了令人鼓舞的结果。在 [[39](#bib.bib39)]
    中，Zheng 等人提出了一种将 CRF 与 CNN 集成的类似语义分割方法。
- en: In another relevant work, Lin et al. [[40](#bib.bib40)] proposed an efficient
    algorithm for semantic segmentation based on contextual deep CRFs. They explored
    “patch-patch” context (between image regions) and “patch-background” context to
    improve semantic segmentation through the use of contextual information.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项相关工作中，Lin 等人 [[40](#bib.bib40)] 提出了基于上下文深度 CRFs 的高效语义分割算法。他们探索了“补丁-补丁”上下文（图像区域之间）和“补丁-背景”上下文，通过利用上下文信息来改进语义分割。
- en: Liu et al. [[41](#bib.bib41)] proposed a semantic segmentation algorithm that
    incorporates rich information into MRFs, including high-order relations and mixture
    of label contexts. Unlike previous works that optimized MRFs using iterative algorithms,
    they proposed a CNN model, namely a Parsing Network, which enables deterministic
    end-to-end computation in a single forward pass.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人 [[41](#bib.bib41)] 提出了一个语义分割算法，将丰富的信息纳入 MRFs，包括高阶关系和标签上下文的混合。与之前使用迭代算法优化
    MRFs 的工作不同，他们提出了一种 CNN 模型，即 Parsing Network，它可以在单次前向传递中进行确定性端到端计算。
- en: 3.3 Encoder-Decoder Based Models
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 编码器-解码器基于模型
- en: Another popular family of deep models for image segmentation is based on the
    convolutional encoder-decoder architecture. Most of the DL-based segmentation
    works use some kind of encoder-decoder models. We group these works into two categories,
    encoder-decoder models for general segmentation, and for medical image segmentation
    (to better distinguish between applications).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的深度模型家族用于图像分割，是基于卷积编码器-解码器架构。大多数基于深度学习的分割工作使用某种编码器-解码器模型。我们将这些工作分为两类，一类是用于一般分割的编码器-解码器模型，另一类是用于医学图像分割的模型（以更好地区分应用）。
- en: 3.3.1 Encoder-Decoder Models for General Segmentation
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 一般分割的编码器-解码器模型
- en: 'Noh et al. [[42](#bib.bib42)] published an early paper on semantic segmentation
    based on deconvolution (a.k.a. transposed convolution). Their model (Figure [11](#S3.F11
    "Figure 11 ‣ 3.3.1 Encoder-Decoder Models for General Segmentation ‣ 3.3 Encoder-Decoder
    Based Models ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using
    Deep Learning: A Survey")) consists of two parts, an encoder using convolutional
    layers adopted from the VGG 16-layer network and a deconvolutional network that
    takes the feature vector as input and generates a map of pixel-wise class probabilities.
    The deconvolution network is composed of deconvolution and unpooling layers, which
    identify pixel-wise class labels and predict segmentation masks.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Noh 等人 [[42](#bib.bib42)] 发布了一篇早期关于基于反卷积（也称为转置卷积）的语义分割论文。他们的模型（图 [11](#S3.F11
    "图 11 ‣ 3.3.1 一般分割的编码器-解码器模型 ‣ 3.3 编码器-解码器基于模型 ‣ 3 深度学习基于的图像分割模型 ‣ 图像分割使用深度学习：综述")）由两部分组成，一部分是使用
    VGG 16 层网络的卷积层的编码器，另一部分是反卷积网络，它以特征向量为输入并生成像素级类别概率图。反卷积网络由反卷积和反池化层组成，这些层识别像素级类别标签并预测分割掩码。
- en: '![Refer to caption](img/1772153fe0f77d0134425e9273d02872.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1772153fe0f77d0134425e9273d02872.png)'
- en: 'Figure 11: Deconvolutional semantic segmentation. Following a convolution network
    based on the VGG 16-layer net, is a multi-layer deconvolution network to generate
    the accurate segmentation map. From [[42](#bib.bib42)].'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：反卷积语义分割。基于 VGG 16 层网络的卷积网络之后，是一个多层反卷积网络，用于生成准确的分割图。来源于 [[42](#bib.bib42)]。
- en: This network achieved promising performance on the PASCAL VOC 2012 dataset,
    and obtained the best accuracy (72.5%) among the methods trained with no external
    data at the time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络在 PASCAL VOC 2012 数据集上取得了令人满意的性能，并且在当时获得了在没有外部数据的情况下训练的方法中最佳的准确率（72.5%）。
- en: 'In another promising work known as SegNet, Badrinarayanan et al. [[15](#bib.bib15)]
    proposed a convolutional encoder-decoder architecture for image segmentation (Figure [12](#S3.F12
    "Figure 12 ‣ 3.3.1 Encoder-Decoder Models for General Segmentation ‣ 3.3 Encoder-Decoder
    Based Models ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using
    Deep Learning: A Survey")). Similar to the deconvolution network, the core trainable
    segmentation engine of SegNet consists of an encoder network, which is topologically
    identical to the 13 convolutional layers in the VGG16 network, and a corresponding
    decoder network followed by a pixel-wise classification layer. The main novelty
    of SegNet is in the way the decoder upsamples its lower resolution input feature
    map(s); specifically, it uses pooling indices computed in the max-pooling step
    of the corresponding encoder to perform non-linear up-sampling. This eliminates
    the need for learning to up-sample. The (sparse) up-sampled maps are then convolved
    with trainable filters to produce dense feature maps. SegNet is also significantly
    smaller in the number of trainable parameters than other competing architectures.
    A Bayesian version of SegNet was also proposed by the same authors to model the
    uncertainty inherent to the convolutional encoder-decoder network for scene segmentation
    [[43](#bib.bib43)].'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个有前景的工作中，即 SegNet，Badrinarayanan 等人 [[15](#bib.bib15)] 提出了一个用于图像分割的卷积编码器-解码器架构（图[12](#S3.F12
    "图 12 ‣ 3.3.1 通用分割的编码器-解码器模型 ‣ 3.3 编码器-解码器基础模型 ‣ 3 DL 基于图像分割模型 ‣ 基于深度学习的图像分割：综述")）。与反卷积网络类似，SegNet
    的核心可训练分割引擎由一个编码器网络组成，该网络在拓扑上与 VGG16 网络中的 13 个卷积层相同，以及一个相应的解码器网络，后跟一个像素级分类层。SegNet
    的主要创新在于解码器如何上采样其较低分辨率的输入特征图；具体而言，它利用在相应编码器的最大池化步骤中计算的池化索引来进行非线性上采样。这消除了学习上采样的需求。然后，（稀疏）上采样图与可训练的滤波器卷积，以生成密集的特征图。与其他竞争架构相比，SegNet
    的可训练参数数量也显著较少。相同作者还提出了 SegNet 的贝叶斯版本，以建模卷积编码器-解码器网络在场景分割中的固有不确定性 [[43](#bib.bib43)]。
- en: '![Refer to caption](img/edb7844d381a734a80d06c4b0e474e71.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/edb7844d381a734a80d06c4b0e474e71.png)'
- en: 'Figure 12: SegNet has no fully-connected layers; hence, the model is fully
    convolutional. A decoder up-samples its input using the transferred pool indices
    from its encoder to produce a sparse feature map(s). From [[15](#bib.bib15)].'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：SegNet 没有全连接层，因此该模型是完全卷积的。解码器使用从编码器转移来的池化索引对输入进行上采样，以生成稀疏特征图。从 [[15](#bib.bib15)]。
- en: 'Another popular model in this category is the recently-developed segmentation
    network, high-resolution network (HRNet) [[44](#bib.bib44)] Figure [13](#S3.F13
    "Figure 13 ‣ 3.3.1 Encoder-Decoder Models for General Segmentation ‣ 3.3 Encoder-Decoder
    Based Models ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using
    Deep Learning: A Survey"). Other than recovering high-resolution representations
    as done in DeConvNet, SegNet, U-Net and V-Net, HRNet maintains high-resolution
    representations through the encoding process by connecting the high-to-low resolution
    convolution streams in parallel, and repeatedly exchanging the information across
    resolutions. Many of the more recent works on semantic segmentation use HRNet
    as the backbone by exploiting contextual models, such as self-attention and its
    extensions.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类别中另一个流行的模型是最近开发的分割网络——高分辨率网络（HRNet）[[44](#bib.bib44)] 图[13](#S3.F13 "图 13
    ‣ 3.3.1 通用分割的编码器-解码器模型 ‣ 3.3 编码器-解码器基础模型 ‣ 3 DL 基于图像分割模型 ‣ 基于深度学习的图像分割：综述")。与
    DeConvNet、SegNet、U-Net 和 V-Net 中恢复高分辨率表示不同，HRNet 通过将高分辨率到低分辨率的卷积流并行连接，并在分辨率之间反复交换信息，来保持高分辨率表示。许多最新的语义分割工作通过利用上下文模型，如自注意力及其扩展，使用
    HRNet 作为骨干网络。
- en: '![Refer to caption](img/6acfbc1c59f8e23953d627d095cf7832.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6acfbc1c59f8e23953d627d095cf7832.png)'
- en: 'Figure 13: Illustrating the HRNet architecture. It consists of parallel high-to-low
    resolution convolution streams with repeated information exchange across multi-resolution
    steams. There are four stages. The 1st stage consists of high-resolution convolutions.
    The 2nd (3rd, 4th) stage repeats two-resolution (three-resolution, four-resolution)
    blocks. From [[44](#bib.bib44)].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：说明 HRNet 架构。它由平行的高到低分辨率卷积流组成，并在多分辨率流之间反复交换信息。共有四个阶段。第 1 阶段包括高分辨率卷积。第 2（第
    3、第 4）阶段重复两个分辨率（三个分辨率、四个分辨率）块。从 [[44](#bib.bib44)]。
- en: Several other works adopt transposed convolutions, or encoder-decoders for image
    segmentation, such as Stacked Deconvolutional Network (SDN) [[45](#bib.bib45)],
    Linknet [[46](#bib.bib46)], W-Net [[47](#bib.bib47)], and locality-sensitive deconvolution
    networks for RGB-D segmentation [[48](#bib.bib48)]. One limitation of Encoder-Decoder
    based models is the loss of fine-grained information of the image, due to the
    loss of high-resolution representations through the encoding process. This issue
    is however addressed in some of the recent architectures such as HR-Net.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些工作采用了转置卷积或编码器-解码器用于图像分割，例如堆叠反卷积网络（SDN） [[45](#bib.bib45)]，Linknet [[46](#bib.bib46)]，W-Net
    [[47](#bib.bib47)]，以及用于 RGB-D 分割的局部敏感反卷积网络 [[48](#bib.bib48)]。编码器-解码器模型的一个限制是图像的细粒度信息丧失，因为通过编码过程丢失了高分辨率表示。然而，这个问题在一些最新架构中得到了改善，例如
    HR-Net。
- en: 3.3.2 Encoder-Decoder Models for Medical and Biomedical Image Segmentation
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 医学和生物医学图像分割的编码器-解码器模型
- en: There are several models initially developed for medical/biomedical image segmentation,
    which are inspired by FCNs and encoder-decoder models. U-Net [[49](#bib.bib49)],
    and V-Net [[50](#bib.bib50)], are two well-known such architectures, which are
    now also being used outside the medical domain.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种最初为医学/生物医学图像分割开发的模型，它们的灵感来自 FCNs 和编码器-解码器模型。U-Net [[49](#bib.bib49)] 和 V-Net
    [[50](#bib.bib50)] 是两个著名的此类架构，现在也被用于医学领域之外的应用。
- en: 'Ronneberger et al. [[49](#bib.bib49)] proposed the U-Net for segmenting biological
    microscopy images. Their network and training strategy relies on the use of data
    augmentation to learn from the very few annotated images effectively. The U-Net
    architecture (Figure [14](#S3.F14 "Figure 14 ‣ 3.3.2 Encoder-Decoder Models for
    Medical and Biomedical Image Segmentation ‣ 3.3 Encoder-Decoder Based Models ‣
    3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep Learning:
    A Survey")) comprises two parts, a contracting path to capture context, and a
    symmetric expanding path that enables precise localization. The down-sampling
    or contracting part has a FCN-like architecture that extracts features with $3\times
    3$ convolutions. The up-sampling or expanding part uses up-convolution (or deconvolution),
    reducing the number of feature maps while increasing their dimensions. Feature
    maps from the down-sampling part of the network are copied to the up-sampling
    part to avoid losing pattern information. Finally, a $1\times 1$ convolution processes
    the feature maps to generate a segmentation map that categorizes each pixel of
    the input image. U-Net was trained on 30 transmitted light microscopy images,
    and it won the ISBI cell tracking challenge 2015 by a large margin.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Ronneberger 等人 [[49](#bib.bib49)] 提出了用于分割生物显微镜图像的 U-Net。他们的网络和训练策略依赖于数据增强来有效地从极少量的标注图像中学习。U-Net
    架构（图 [14](#S3.F14 "图 14 ‣ 3.3.2 医学和生物医学图像分割的编码器-解码器模型 ‣ 3.3 基于编码器-解码器的模型 ‣ 3 DL
    基于图像分割模型 ‣ 使用深度学习进行图像分割：综述")）由两部分组成，一部分是收缩路径以捕获上下文，另一部分是对称的扩展路径以实现精确定位。收缩或下采样部分具有类似于
    FCN 的架构，通过 $3\times 3$ 卷积提取特征。扩展或上采样部分使用上卷积（或反卷积），在减少特征图数量的同时增加其维度。网络下采样部分的特征图被复制到上采样部分，以避免丢失模式信息。最后，$1\times
    1$ 卷积处理特征图以生成分类每个输入图像像素的分割图。U-Net 在 30 张透射光显微镜图像上进行训练，并在 ISBI 细胞追踪挑战赛 2015 中大幅获胜。
- en: '![Refer to caption](img/f0fb1539271bea11355ec360d53eed16.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f0fb1539271bea11355ec360d53eed16.png)'
- en: 'Figure 14: The U-net model. The blue boxes denote feature map blocks with their
    indicated shapes. From [[49](#bib.bib49)].'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：U-net 模型。蓝色方框表示特征图块及其指示的形状。摘自 [[49](#bib.bib49)]。
- en: Various extensions of U-Net have been developed for different kinds of images.
    For example, Cicek [[51](#bib.bib51)] proposed a U-Net architecture for 3D images.
    Zhou et al. [[52](#bib.bib52)] developed a nested U-Net architecture. U-Net has
    also been applied to various other problems. For example, Zhang et al. [[53](#bib.bib53)]
    developed a road segmentation/extraction algorithm based on U-Net.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 各种 U-Net 的扩展已经被开发用于不同类型的图像。例如，Cicek [[51](#bib.bib51)] 提出了用于 3D 图像的 U-Net 架构。Zhou
    等人 [[52](#bib.bib52)] 开发了一种嵌套的 U-Net 架构。U-Net 也被应用于各种其他问题。例如，Zhang 等人 [[53](#bib.bib53)]
    开发了一种基于 U-Net 的道路分割/提取算法。
- en: V-Net is another well-known, FCN-based model, which was proposed by Milletari
    et al. [[50](#bib.bib50)] for 3D medical image segmentation. For model training,
    they introduced a new objective function based on Dice coefficient, enabling the
    model to deal with situations in which there is a strong imbalance between the
    number of voxels in the foreground and background. The network was trained end-to-end
    on MRI volumes of prostate, and learns to predict segmentation for the whole volume
    at once. Some of the other relevant works on medical image segmentation includes
    Progressive Dense V-net (PDV-Net) et al. for fast and automatic segmentation of
    pulmonary lobes from chest CT images, and the 3D-CNN encoder for lesion segmentation
    [[54](#bib.bib54)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: V-Net是另一个著名的基于全卷积网络（FCN）的模型，由Milletari等人[[50](#bib.bib50)]提出，用于3D医学图像分割。在模型训练中，他们引入了一种基于Dice系数的新目标函数，使模型能够处理前景和背景中体素数量严重不平衡的情况。该网络在前列腺的MRI体积上进行了端到端训练，并学习一次性预测整个体积的分割。其他相关的医学图像分割工作包括用于从胸部CT图像中快速自动分割肺叶的渐进式密集V-net（PDV-Net）等，以及用于病变分割的3D-CNN编码器[[54](#bib.bib54)]。
- en: 3.4 Multi-Scale and Pyramid Network Based Models
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 多尺度和金字塔网络模型
- en: Multi-scale analysis, a rather old idea in image processing, has been deployed
    in various neural network architectures. One of the most prominent models of this
    sort is the Feature Pyramid Network (FPN) proposed by Lin et al. [[55](#bib.bib55)],
    which was developed mainly for object detection but was then also applied to segmentation.
    The inherent multi-scale, pyramidal hierarchy of deep CNNs was used to construct
    feature pyramids with marginal extra cost. To merge low and high resolution features,
    the FPN is composed of a bottom-up pathway, a top-down pathway and lateral connections.
    The concatenated feature maps are then processed by a $3\times 3$ convolution
    to produce the output of each stage. Finally, each stage of the top-down pathway
    generates a prediction to detect an object. For image segmentation, the authors
    use two multi-layer perceptrons (MLPs) to generate the masks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 多尺度分析是图像处理中的一个相当古老的概念，已被应用于各种神经网络架构中。其中最著名的模型之一是由Lin等人提出的特征金字塔网络（Feature Pyramid
    Network, FPN）[[55](#bib.bib55)]，该模型主要用于目标检测，但后来也被应用于图像分割。深度卷积神经网络（CNN）固有的多尺度金字塔层次结构被用来构建特征金字塔，额外的成本很少。为了合并低分辨率和高分辨率的特征，FPN由一个自下而上的路径、一个自上而下的路径和横向连接组成。连接的特征图经过$3\times
    3$卷积处理，生成每个阶段的输出。最后，每个自上而下路径的阶段都会生成一个预测来检测目标。对于图像分割，作者使用了两个多层感知机（MLP）来生成掩模。
- en: 'Zhao et al. [[56](#bib.bib56)] developed the Pyramid Scene Parsing Network
    (PSPN), a multi-scale network to better learn the global context representation
    of a scene (Figure [15](#S3.F15 "Figure 15 ‣ 3.4 Multi-Scale and Pyramid Network
    Based Models ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using
    Deep Learning: A Survey")). Different patterns are extracted from the input image
    using a residual network (ResNet) as a feature extractor, with a dilated network.
    These feature maps are then fed into a pyramid pooling module to distinguish patterns
    of different scales. They are pooled at four different scales, each one corresponding
    to a pyramid level and processed by a $1\times 1$ convolutional layer to reduce
    their dimensions. The outputs of the pyramid levels are up-sampled and concatenated
    with the initial feature maps to capture both local and global context information.
    Finally, a convolutional layer is used to generate the pixel-wise predictions.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao等人[[56](#bib.bib56)]开发了金字塔场景解析网络（Pyramid Scene Parsing Network, PSPN），这是一个多尺度网络，用于更好地学习场景的全局上下文表示（图 [15](#S3.F15
    "图 15 ‣ 3.4 多尺度和金字塔网络模型 ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")）。不同的模式从输入图像中提取，使用残差网络（ResNet）作为特征提取器，以及膨胀网络。这些特征图随后被输入到金字塔池化模块中，以区分不同尺度的模式。它们在四个不同的尺度下进行池化，每个尺度对应一个金字塔层，并通过$1\times
    1$卷积层来减少其维度。金字塔层的输出被上采样，并与初始特征图连接，以捕捉局部和全局上下文信息。最后，使用卷积层生成像素级预测。
- en: '![Refer to caption](img/59fe96558d8f5a84d0dfeacc61c007cf.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59fe96558d8f5a84d0dfeacc61c007cf.png)'
- en: 'Figure 15: The PSPN architecture. A CNN produces the feature map and a pyramid
    pooling module aggregates the different sub-region representations. Up-sampling
    and concatenation are used to form the final feature representation from which,
    the final pixel-wise prediction is obtained through convolution. From [[56](#bib.bib56)].'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：PSPN 架构。一个 CNN 生成特征图，金字塔池化模块聚合不同的子区域表示。通过上采样和拼接形成最终特征表示，然后通过卷积获得最终的逐像素预测。来源
    [[56](#bib.bib56)]。
- en: Ghiasi and Fowlkes [[57](#bib.bib57)] developed a multi-resolution reconstruction
    architecture based on a Laplacian pyramid that uses skip connections from higher
    resolution feature maps and multiplicative gating to successively refine segment
    boundaries reconstructed from lower-resolution maps. They showed that, while the
    apparent spatial resolution of convolutional feature maps is low, the high-dimensional
    feature representation contains significant sub-pixel localization information.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Ghiasi 和 Fowlkes [[57](#bib.bib57)] 开发了一种基于拉普拉斯金字塔的多分辨率重建架构，该架构利用来自高分辨率特征图的跳跃连接和乘法门控来逐步细化从低分辨率图重建的分割边界。他们展示了尽管卷积特征图的表观空间分辨率较低，但高维特征表示包含了显著的亚像素定位信息。
- en: There are other models using multi-scale analysis for segmentation, such as
    DM-Net (Dynamic Multi-scale Filters Network) [[58](#bib.bib58)], Context contrasted
    network and gated multi-scale aggregation (CCN) [[59](#bib.bib59)], Adaptive Pyramid
    Context Network (APC-Net) [[60](#bib.bib60)], Multi-scale context intertwining
    (MSCI) [[61](#bib.bib61)], and salient object segmentation [[62](#bib.bib62)].
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其他使用多尺度分析进行分割的模型包括 DM-Net（动态多尺度滤波网络）[[58](#bib.bib58)]、上下文对比网络和门控多尺度聚合（CCN）[[59](#bib.bib59)]、自适应金字塔上下文网络（APC-Net）[[60](#bib.bib60)]、多尺度上下文交织（MSCI）[[61](#bib.bib61)]和显著性物体分割[[62](#bib.bib62)]。
- en: 3.5 R-CNN Based Models (for Instance Segmentation)
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 基于 R-CNN 的模型（用于实例分割）
- en: 'The regional convolutional network (R-CNN) and its extensions (Fast R-CNN,
    Faster R-CNN, Maksed-RCNN) have proven successful in object detection applications.
    In particular, the Faster R-CNN [[63](#bib.bib63)] architecture (Figure [16](#S3.F16
    "Figure 16 ‣ 3.5 R-CNN Based Models (for Instance Segmentation) ‣ 3 DL-Based Image
    Segmentation Models ‣ Image Segmentation Using Deep Learning: A Survey")) developed
    for object detection uses a region proposal network (RPN) to propose bounding
    box candidates. The RPN extracts a Region of Interest (RoI), and a RoIPool layer
    computes features from these proposals in order to infer the bounding box coordinates
    and the class of the object. Some of the extensions of R-CNN have been heavily
    used to address the instance segmentation problem; i.e., the task of simultaneously
    performing object detection and semantic segmentation.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 区域卷积网络（R-CNN）及其扩展（Fast R-CNN、Faster R-CNN、Masked-RCNN）在物体检测应用中取得了成功。特别是，Faster
    R-CNN [[63](#bib.bib63)] 架构（图 [16](#S3.F16 "图 16 ‣ 3.5 基于 R-CNN 的模型（用于实例分割） ‣
    3 基于深度学习的图像分割模型 ‣ 使用深度学习进行图像分割：调查")）用于物体检测的区域提议网络（RPN）提出边界框候选。RPN 提取感兴趣区域（RoI），RoIPool
    层从这些提议中计算特征，以推断边界框坐标和物体类别。R-CNN 的一些扩展被广泛用于解决实例分割问题，即同时执行物体检测和语义分割的任务。
- en: '![Refer to caption](img/0c3c9075242492ac2ac704a1f38abbb1.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0c3c9075242492ac2ac704a1f38abbb1.png)'
- en: 'Figure 16: Faster R-CNN architecture. Courtesy of [[63](#bib.bib63)].'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：Faster R-CNN 架构。图片来源 [[63](#bib.bib63)]。
- en: 'In one extension of this model, He et al. [[64](#bib.bib64)] proposed a Mask
    R-CNN for object instance segmentation, which beat all previous benchmarks on
    many COCO challenges. This model efficiently detects objects in an image while
    simultaneously generating a high-quality segmentation mask for each instance.
    Mask R-CNN is essentially a Faster R-CNN with 3 output branches (Figure [17](#S3.F17
    "Figure 17 ‣ 3.5 R-CNN Based Models (for Instance Segmentation) ‣ 3 DL-Based Image
    Segmentation Models ‣ Image Segmentation Using Deep Learning: A Survey"))—the
    first computes the bounding box coordinates, the second computes the associated
    classes, and the third computes the binary mask to segment the object. The Mask
    R-CNN loss function combines the losses of the bounding box coordinates, the predicted
    class, and the segmentation mask, and trains all of them jointly. Figure [18](#S3.F18
    "Figure 18 ‣ 3.5 R-CNN Based Models (for Instance Segmentation) ‣ 3 DL-Based Image
    Segmentation Models ‣ Image Segmentation Using Deep Learning: A Survey") shows
    the Mask-RCNN result on some sample images.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在该模型的一个扩展中，何等人 [[64](#bib.bib64)] 提出了一个用于对象实例分割的 Mask R-CNN，该模型在许多 COCO 挑战中超越了所有先前的基准。该模型有效地检测图像中的对象，同时为每个实例生成高质量的分割掩码。Mask
    R-CNN 本质上是一个带有 3 个输出分支的 Faster R-CNN（图 [17](#S3.F17 "图 17 ‣ 3.5 R-CNN 基于模型（用于实例分割）
    ‣ 3 DL 基于图像分割模型 ‣ 使用深度学习的图像分割：综述")）——第一个计算边界框坐标，第二个计算相关类别，第三个计算二进制掩码以分割对象。Mask
    R-CNN 损失函数将边界框坐标、预测类别和分割掩码的损失结合在一起，并共同训练它们。图 [18](#S3.F18 "图 18 ‣ 3.5 R-CNN 基于模型（用于实例分割）
    ‣ 3 DL 基于图像分割模型 ‣ 使用深度学习的图像分割：综述") 显示了 Mask-RCNN 在一些样本图像上的结果。
- en: '![Refer to caption](img/017f09fd85bccce79246a895a45b62f4.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/017f09fd85bccce79246a895a45b62f4.png)'
- en: 'Figure 17: Mask R-CNN architecture for instance segmentation. From [[64](#bib.bib64)].'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：用于实例分割的 Mask R-CNN 架构。摘自 [[64](#bib.bib64)]。
- en: '![Refer to caption](img/872e73570ba8d5c7765742343f212952.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/872e73570ba8d5c7765742343f212952.png)'
- en: 'Figure 18: Mask R-CNN results on sample images from the COCO test set. From
    [[64](#bib.bib64)].'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：Mask R-CNN 在 COCO 测试集样本图像上的结果。摘自 [[64](#bib.bib64)]。
- en: 'The Path Aggregation Network (PANet) proposed by Liu et al. [[65](#bib.bib65)]
    is based on the Mask R-CNN and FPN models (Figure [19](#S3.F19 "Figure 19 ‣ 3.5
    R-CNN Based Models (for Instance Segmentation) ‣ 3 DL-Based Image Segmentation
    Models ‣ Image Segmentation Using Deep Learning: A Survey")). The feature extractor
    of the network uses an FPN architecture with a new augmented bottom-up pathway
    improving the propagation of low-layer features. Each stage of this third pathway
    takes as input the feature maps of the previous stage and processes them with
    a $3\times 3$ convolutional layer. The output is added to the same stage feature
    maps of the top-down pathway using a lateral connection and these feature maps
    feed the next stage. As in the Mask R-CNN, the output of the adaptive feature
    pooling layer feeds three branches. The first two use a fully connected layer
    to generate the predictions of the bounding box coordinates and the associated
    object class. The third processes the RoI with an FCN to predict the object mask.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人提出的路径聚合网络（PANet）[[65](#bib.bib65)] 基于 Mask R-CNN 和 FPN 模型（图 [19](#S3.F19
    "图 19 ‣ 3.5 R-CNN 基于模型（用于实例分割） ‣ 3 DL 基于图像分割模型 ‣ 使用深度学习的图像分割：综述")）。该网络的特征提取器使用了具有增强的自下而上的路径的新
    FPN 架构，改进了低层特征的传播。第三路径的每个阶段将上一阶段的特征图作为输入，并通过 $3\times 3$ 卷积层处理。输出通过横向连接与自上而下路径的相同阶段特征图相加，这些特征图输入到下一阶段。与
    Mask R-CNN 相同，适应性特征池化层的输出连接到三个分支。前两个使用全连接层生成边界框坐标预测和相关的对象类别。第三个使用 FCN 处理 RoI 以预测对象掩码。
- en: '![Refer to caption](img/be15ec990088a232974805cf90d0f57b.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be15ec990088a232974805cf90d0f57b.png)'
- en: 'Figure 19: The Path Aggregation Network. (a) FPN backbone. (b) Bottom-up path
    augmentation. (c) Adaptive feature pooling. (d) Box branch. (e) Fully-connected
    fusion. Courtesy of [[65](#bib.bib65)].'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：路径聚合网络。 (a) FPN 主干。 (b) 自下而上的路径增强。 (c) 适应性特征池化。 (d) 边框分支。 (e) 全连接融合。 由 [[65](#bib.bib65)]
    提供。
- en: Dai et al. [[66](#bib.bib66)] developed a multi-task network for instance-aware
    semantic segmentation, that consists of three networks, respectively differentiating
    instances, estimating masks, and categorizing objects. These networks form a cascaded
    structure, and are designed to share their convolutional features. Hu et al. [[67](#bib.bib67)]
    proposed a new partially-supervised training paradigm, together with a novel weight
    transfer function, that enables training instance segmentation models on a large
    set of categories, all of which have box annotations, but only a small fraction
    of which have mask annotations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Dai 等人 [[66](#bib.bib66)] 开发了一个多任务网络，用于实例感知语义分割，该网络由三个分别进行实例区分、掩码估计和对象分类的网络组成。这些网络形成了一个级联结构，设计上共享卷积特征。Hu
    等人 [[67](#bib.bib67)] 提出了一个新的部分监督训练范式，并结合了一个新型的权重转移函数，使得可以在一个大类别集合上训练实例分割模型，其中所有类别都有框注释，但只有一小部分有掩码注释。
- en: 'Chen et al. [[68](#bib.bib68)] developed an instance segmentation model, MaskLab
    (Figure [20](#S3.F20 "Figure 20 ‣ 3.5 R-CNN Based Models (for Instance Segmentation)
    ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep Learning:
    A Survey")), by refining object detection with semantic and direction features
    based on Faster R-CNN. This model produces three outputs, box detection, semantic
    segmentation, and direction prediction. Building on the Faster-RCNN object detector,
    the predicted boxes provide accurate localization of object instances. Within
    each region of interest, MaskLab performs foreground/background segmentation by
    combining semantic and direction prediction.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chen 等人 [[68](#bib.bib68)] 开发了一个实例分割模型 MaskLab（图 [20](#S3.F20 "Figure 20 ‣
    3.5 R-CNN Based Models (for Instance Segmentation) ‣ 3 DL-Based Image Segmentation
    Models ‣ Image Segmentation Using Deep Learning: A Survey")），通过基于 Faster R-CNN
    的语义和方向特征来改进对象检测。该模型产生三个输出：框检测、语义分割和方向预测。在 Faster-RCNN 对象检测器的基础上，预测的框提供了对象实例的准确定位。在每个感兴趣区域内，MaskLab
    通过结合语义和方向预测来执行前景/背景分割。'
- en: '![Refer to caption](img/6597fea5e23f6950082bbd9bd65f84d2.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/6597fea5e23f6950082bbd9bd65f84d2.png)'
- en: 'Figure 20: The MaskLab model. MaskLab generates three outputs—refined box predictions
    (from Faster R-CNN), semantic segmentation logits for pixel-wise classification,
    and direction prediction logits for predicting each pixel’s direction toward its
    instance center. From [[68](#bib.bib68)].'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：MaskLab 模型。MaskLab 生成三个输出——改进的框预测（来自 Faster R-CNN）、用于像素级分类的语义分割对数值和用于预测每个像素朝向其实例中心方向的方向预测对数值。来源于
    [[68](#bib.bib68)]。
- en: Another interesting model is Tensormask, proposed by Chen et al. [[69](#bib.bib69)],
    which is based on dense sliding window instance segmentation. They treat dense
    instance segmentation as a prediction task over 4D tensors and present a general
    framework that enables novel operators on 4D tensors. They demonstrate that the
    tensor view leads to large gains over baselines and yields results comparable
    to Mask R-CNN. TensorMask achieves promising results on dense object segmentation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的模型是 Tensormask，由 Chen 等人 [[69](#bib.bib69)] 提出，基于密集滑动窗口实例分割。他们将密集实例分割视为
    4D 张量上的预测任务，并提出了一个通用框架，允许在 4D 张量上进行新型操作。他们展示了张量视图相对于基线的大幅提升，并且结果与 Mask R-CNN 相当。TensorMask
    在密集对象分割上取得了有希望的结果。
- en: Many other instance segmentation models have been developed based on R-CNN,
    such as those developed for mask proposals, including R-FCN [[70](#bib.bib70)],
    DeepMask [[71](#bib.bib71)], PolarMask [[72](#bib.bib72)], boundary-aware instance
    segmentation [[73](#bib.bib73)], and CenterMask [[74](#bib.bib74)]. It is worth
    noting that there is another promising research direction that attempts to solve
    the instance segmentation problem by learning grouping cues for bottom-up segmentation,
    such as Deep Watershed Transform [[75](#bib.bib75)], real-time instance segmentation
    [[76](#bib.bib76)], and Semantic Instance Segmentation via Deep Metric Learning
    [[77](#bib.bib77)].
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 R-CNN 开发了许多其他实例分割模型，例如用于掩码提议的模型，包括 R-FCN [[70](#bib.bib70)]、DeepMask [[71](#bib.bib71)]、PolarMask
    [[72](#bib.bib72)]、边界感知实例分割 [[73](#bib.bib73)] 和 CenterMask [[74](#bib.bib74)]。值得注意的是，还有一个有前景的研究方向试图通过学习底层分割的分组线索来解决实例分割问题，例如
    Deep Watershed Transform [[75](#bib.bib75)]、实时实例分割 [[76](#bib.bib76)] 和通过深度度量学习的语义实例分割
    [[77](#bib.bib77)]。
- en: 3.6 Dilated Convolutional Models and DeepLab Family
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 膨胀卷积模型及 DeepLab 系列
- en: 'Dilated convolution (a.k.a. “atrous” convolution) introduces another parameter
    to convolutional layers, the dilation rate. The dilated convolution (Figure [21](#S3.F21
    "Figure 21 ‣ 3.6 Dilated Convolutional Models and DeepLab Family ‣ 3 DL-Based
    Image Segmentation Models ‣ Image Segmentation Using Deep Learning: A Survey"))
    of a signal $x(i)$ is defined as $y_{i}=\sum_{k=1}^{K}x[i+rk]w[k]$, where $r$
    is the dilation rate that defines a spacing between the weights of the kernel
    $w$. For example, a $3\times 3$ kernel with a dilation rate of 2 will have the
    same size receptive field as a $5\times 5$ kernel while using only 9 parameters,
    thus enlarging the receptive field with no increase in computational cost. Dilated
    convolutions have been popular in the field of real-time segmentation, and many
    recent publications report the use of this technique. Some of most important include
    the DeepLab family [[78](#bib.bib78)], multi-scale context aggregation [[79](#bib.bib79)],
    dense upsampling convolution and hybrid dilatedconvolution (DUC-HDC) [[80](#bib.bib80)],
    densely connected Atrous Spatial Pyramid Pooling (DenseASPP) [[81](#bib.bib81)],
    and the efficient neural network (ENet) [[82](#bib.bib82)].'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积（即“atrous”卷积）引入了一个额外的参数，即膨胀率。信号 $x(i)$ 的膨胀卷积（图 [21](#S3.F21 "图 21 ‣ 3.6
    膨胀卷积模型和DeepLab家族 ‣ 3 基于DL的图像分割模型 ‣ 使用深度学习的图像分割：综述")）定义为 $y_{i}=\sum_{k=1}^{K}x[i+rk]w[k]$，其中
    $r$ 是膨胀率，它定义了卷积核 $w$ 的权重之间的间距。例如，一个膨胀率为2的 $3\times 3$ 卷积核将具有与 $5\times 5$ 卷积核相同大小的接收场，同时只使用9个参数，从而在不增加计算成本的情况下扩大了接收场。膨胀卷积在实时分割领域中非常流行，许多最近的出版物报告了这种技术的使用。一些最重要的包括DeepLab家族
    [[78](#bib.bib78)]，多尺度上下文聚合 [[79](#bib.bib79)]，密集上采样卷积和混合膨胀卷积（DUC-HDC） [[80](#bib.bib80)]，密集连接的Atrous
    Spatial Pyramid Pooling（DenseASPP） [[81](#bib.bib81)]，以及高效神经网络（ENet） [[82](#bib.bib82)]。
- en: '![Refer to caption](img/b980f82e4fb83238e266429261f40cc5.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b980f82e4fb83238e266429261f40cc5.png)'
- en: 'Figure 21: Dilated convolution. A $3\times 3$ kernel at different dilation
    rates.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：膨胀卷积。不同膨胀率下的 $3\times 3$ 卷积核。
- en: 'DeepLabv1 [[37](#bib.bib37)] and DeepLabv2 [[78](#bib.bib78)] are among some
    of the most popular image segmentation approaches, developed by Chen et al.. The
    latter has three key features. First is the use of dilated convolution to address
    the decreasing resolution in the network (caused by max-pooling and striding).
    Second is Atrous Spatial Pyramid Pooling (ASPP), which probes an incoming convolutional
    feature layer with filters at multiple sampling rates, thus capturing objects
    as well as image context at multiple scales to robustly segment objects at multiple
    scales. Third is improved localization of object boundaries by combining methods
    from deep CNNs and probabilistic graphical models. The best DeepLab (using a ResNet-101
    as backbone) has reached a 79.7% mIoU score on the 2012 PASCAL VOC challenge,
    a 45.7% mIoU score on the PASCAL-Context challenge and a 70.4% mIoU score on the
    Cityscapes challenge. Figure [22](#S3.F22 "Figure 22 ‣ 3.6 Dilated Convolutional
    Models and DeepLab Family ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation
    Using Deep Learning: A Survey") illustrates the Deeplab model, which is similar
    to [[37](#bib.bib37)], the main difference being the use of dilated convolution
    and ASPP.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: DeepLabv1 [[37](#bib.bib37)] 和 DeepLabv2 [[78](#bib.bib78)] 是一些最受欢迎的图像分割方法，由Chen等人开发。后者有三个关键特性。第一是使用膨胀卷积来解决网络中分辨率降低的问题（由最大池化和步幅引起）。第二是Atrous
    Spatial Pyramid Pooling (ASPP)，它通过在多个采样率下使用滤波器探测输入的卷积特征层，从而捕获多尺度的对象以及图像上下文，以稳健地分割多尺度的对象。第三是通过结合深度CNN和概率图模型的方法来改进对象边界的定位。最佳的DeepLab（使用ResNet-101作为骨干网络）在2012年PASCAL
    VOC挑战中达到了79.7%的mIoU分数，在PASCAL-Context挑战中达到了45.7%的mIoU分数，在Cityscapes挑战中达到了70.4%的mIoU分数。图 [22](#S3.F22
    "图 22 ‣ 3.6 膨胀卷积模型和DeepLab家族 ‣ 3 基于DL的图像分割模型 ‣ 使用深度学习的图像分割：综述") 说明了Deeplab模型，其类似于
    [[37](#bib.bib37)]，主要区别在于使用了膨胀卷积和ASPP。
- en: '![Refer to caption](img/4e6ad6ddfa3b79b69080f1356e254e77.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e6ad6ddfa3b79b69080f1356e254e77.png)'
- en: 'Figure 22: The DeepLab model. A CNN model such as VGG-16 or ResNet-101 is employed
    in fully convolutional fashion, using dilated convolution. A bilinear interpolation
    stage enlarges the feature maps to the original image resolution. Finally, a fully
    connected CRF refines the segmentation result to better capture the object boundaries.
    From [[78](#bib.bib78)]'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：DeepLab模型。采用如VGG-16或ResNet-101的CNN模型，以全卷积方式使用膨胀卷积。双线性插值阶段将特征图放大至原始图像分辨率。最后，完全连接的CRF对分割结果进行优化，以更好地捕捉物体边界。来源
    [[78](#bib.bib78)]
- en: Subsequently, Chen et al. [[12](#bib.bib12)] proposed DeepLabv3, which combines
    cascaded and parallel modules of dilated convolutions. The parallel convolution
    modules are grouped in the ASPP. A $1\times 1$ convolution and batch normalisation
    are added in the ASPP. All the outputs are concatenated and processed by another
    $1\times 1$ convolution to create the final output with logits for each pixel.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，Chen等人 [[12](#bib.bib12)] 提出了DeepLabv3，结合了级联和并行的膨胀卷积模块。并行卷积模块在ASPP中分组。ASPP中添加了$1\times
    1$卷积和批量归一化。所有输出都被串联并由另一个$1\times 1$卷积处理，以创建每个像素的最终输出logits。
- en: 'In 2018, Chen et al. [[83](#bib.bib83)] released Deeplabv3+, which uses an
    encoder-decoder architecture (Figure [23](#S3.F23 "Figure 23 ‣ 3.6 Dilated Convolutional
    Models and DeepLab Family ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation
    Using Deep Learning: A Survey")), including atrous separable convolution, composed
    of a depthwise convolution (spatial convolution for each channel of the input)
    and pointwise convolution ($1\times 1$ convolution with the depthwise convolution
    as input). They used the DeepLabv3 framework as encoder. The most relevant model
    has a modified Xception backbone with more layers, dilated depthwise separable
    convolutions instead of max pooling and batch normalization. The best DeepLabv3+
    pretrained on the COCO and the JFT datasets has obtained a 89.0% mIoU score on
    the 2012 PASCAL VOC challenge.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，Chen等人 [[83](#bib.bib83)] 发布了Deeplabv3+，采用编码器-解码器架构（图 [23](#S3.F23 "图
    23 ‣ 3.6 膨胀卷积模型和DeepLab家族 ‣ 3 基于DL的图像分割模型 ‣ 使用深度学习的图像分割：调查")），包括可分离的膨胀卷积，由深度卷积（对输入的每个通道进行空间卷积）和逐点卷积（$1\times
    1$卷积，深度卷积作为输入）组成。他们使用DeepLabv3框架作为编码器。最相关的模型具有修改版的Xception骨干网，增加了更多层，采用膨胀深度可分离卷积代替最大池化和批量归一化。在COCO和JFT数据集上预训练的最佳DeepLabv3+在2012年PASCAL
    VOC挑战赛中获得了89.0%的mIoU得分。
- en: '![Refer to caption](img/0419977402677ae5551c905a8979e3a2.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0419977402677ae5551c905a8979e3a2.png)'
- en: 'Figure 23: The DeepLabv3+ model. From [[83](#bib.bib83)].'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：DeepLabv3+模型。来源 [[83](#bib.bib83)]。
- en: 3.7 Recurrent Neural Network Based Models
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 基于递归神经网络的模型
- en: While CNNs are a natural fit for computer vision problems, they are not the
    only possibility. RNNs are useful in modeling the short/long term dependencies
    among pixels to (potentially) improve the estimation of the segmentation map.
    Using RNNs, pixels may be linked together and processed sequentially to model
    global contexts and improve semantic segmentation. One challenge, though, is the
    natural 2D structure of images.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然CNN非常适合计算机视觉问题，但它们并不是唯一的选择。RNN在建模像素间的短期/长期依赖关系方面很有用，可以（潜在地）改善分割图的估计。通过使用RNN，像素可以被连接在一起并按顺序处理，以建模全局上下文并改善语义分割。然而，一个挑战是图像的自然二维结构。
- en: 'Visin et al. [[84](#bib.bib84)] proposed an RNN-based model for semantic segmentation
    called ReSeg. This model is mainly based on another work, ReNet [[85](#bib.bib85)],
    which was developed for image classification. Each ReNet layer is composed of
    four RNNs that sweep the image horizontally and vertically in both directions,
    encoding patches/activations, and providing relevant global information. To perform
    image segmentation with the ReSeg model (Figure [24](#S3.F24 "Figure 24 ‣ 3.7
    Recurrent Neural Network Based Models ‣ 3 DL-Based Image Segmentation Models ‣
    Image Segmentation Using Deep Learning: A Survey")), ReNet layers are stacked
    on top of pre-trained VGG-16 convolutional layers that extract generic local features.
    ReNet layers are then followed by up-sampling layers to recover the original image
    resolution in the final predictions. Gated Recurrent Units (GRUs) are used because
    they provide a good balance between memory usage and computational power.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Visin 等人 [[84](#bib.bib84)] 提出了一个基于 RNN 的语义分割模型，称为 ReSeg。该模型主要基于另一项工作 ReNet
    [[85](#bib.bib85)]，后者是为图像分类开发的。每个 ReNet 层由四个 RNN 组成，它们在水平和垂直方向上扫掠图像，编码补丁/激活，并提供相关的全局信息。为了使用
    ReSeg 模型进行图像分割（图 [24](#S3.F24 "图 24 ‣ 3.7 基于递归神经网络的模型 ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")），ReNet
    层堆叠在预训练的 VGG-16 卷积层上，这些层提取通用局部特征。随后是上采样层，以在最终预测中恢复原始图像分辨率。使用门控递归单元（GRU），因为它们在内存使用和计算能力之间提供了良好的平衡。
- en: '![Refer to caption](img/770bbe1d1adb5a6b2344eb13b6179d9d.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/770bbe1d1adb5a6b2344eb13b6179d9d.png)'
- en: 'Figure 24: The ReSeg model. The pre-trained VGG-16 feature extractor network
    is not shown. From [[84](#bib.bib84)].'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24：ReSeg 模型。未显示预训练的 VGG-16 特征提取网络。来自 [[84](#bib.bib84)]。
- en: In another work, Byeon et al. [[86](#bib.bib86)] developed a pixel-level segmentation
    and classification of scene images using long-short-term-memory (LSTM) network.
    They investigated two-dimensional (2D) LSTM networks for images of natural scenes,
    taking into account the complex spatial dependencies of labels. In this work,
    classification, segmentation, and context integration are all carried out by 2D
    LSTM networks, allowing texture and spatial model parameters to be learned within
    a single model.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项研究中，Byeon 等人 [[86](#bib.bib86)] 使用长短期记忆（LSTM）网络开发了场景图像的像素级分割和分类。他们研究了用于自然场景图像的二维（2D）LSTM
    网络，考虑了标签的复杂空间依赖性。在这项工作中，分类、分割和上下文集成都由 2D LSTM 网络完成，从而使纹理和空间模型参数可以在单个模型中进行学习。
- en: 'Liang et al. [[87](#bib.bib87)] proposed a semantic segmentation model based
    on the Graph Long Short-Term Memory (Graph LSTM) network, a generalization of
    LSTM from sequential data or multidimensional data to general graph-structured
    data. Instead of evenly dividing an image to pixels or patches in existing multi-dimensional
    LSTM structures (e.g., row, grid and diagonal LSTMs), they take each arbitrary-shaped
    superpixel as a semantically consistent node, and adaptively construct an undirected
    graph for the image, where the spatial relations of the superpixels are naturally
    used as edges. Figure [25](#S3.F25 "Figure 25 ‣ 3.7 Recurrent Neural Network Based
    Models ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep
    Learning: A Survey") presents a visual comparison of the traditional pixel-wise
    RNN model and graph-LSTM model. To adapt the Graph LSTM model to semantic segmentation
    (Figure [26](#S3.F26 "Figure 26 ‣ 3.7 Recurrent Neural Network Based Models ‣
    3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep Learning:
    A Survey")) , LSTM layers built on a super-pixel map are appended on the convolutional
    layers to enhance visual features with global structure context. The convolutional
    features pass through $1\times 1$ convolutional filters to generate the initial
    confidence maps for all labels. The node updating sequence for the subsequent
    Graph LSTM layers is determined by the confidence-drive scheme based on the initial
    confidence maps, and then the Graph LSTM layers can sequentially update the hidden
    states of all superpixel nodes.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '梁等人[[87](#bib.bib87)]提出了一种基于图长短期记忆（Graph LSTM）网络的语义分割模型，这是LSTM从顺序数据或多维数据推广到一般图结构数据的一个扩展。与现有多维LSTM结构（例如行、网格和对角LSTM）将图像均匀划分为像素或补丁不同，他们将每个任意形状的超像素视为语义一致的节点，并自适应地为图像构建无向图，其中超像素的空间关系自然地用作边。图[25](#S3.F25
    "Figure 25 ‣ 3.7 Recurrent Neural Network Based Models ‣ 3 DL-Based Image Segmentation
    Models ‣ Image Segmentation Using Deep Learning: A Survey")展示了传统像素级RNN模型和图-LSTM模型的视觉对比。为了将Graph
    LSTM模型适应于语义分割（图[26](#S3.F26 "Figure 26 ‣ 3.7 Recurrent Neural Network Based Models
    ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep Learning:
    A Survey")），在卷积层上附加了基于超像素图的LSTM层，以增强具有全局结构上下文的视觉特征。卷积特征通过$1\times 1$卷积滤波器生成所有标签的初始置信度图。后续Graph
    LSTM层的节点更新序列由基于初始置信度图的置信度驱动方案确定，然后Graph LSTM层可以顺序更新所有超像素节点的隐藏状态。'
- en: '![Refer to caption](img/d0ff72d6d17dfa810a865b22b547c266.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d0ff72d6d17dfa810a865b22b547c266.png)'
- en: 'Figure 25: Comparison between the graph-LSTM model and traditional pixel-wise
    RNN models. From [[87](#bib.bib87)].'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：图-LSTM模型与传统像素级RNN模型的比较。来源于[[87](#bib.bib87)]。
- en: '![Refer to caption](img/f4622535c230f0f8d283c73db94b0e62.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f4622535c230f0f8d283c73db94b0e62.png)'
- en: 'Figure 26: The graph-LSTM model for semantic segmentation. From [[87](#bib.bib87)].'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图26：用于语义分割的图-LSTM模型。来源于[[87](#bib.bib87)]。
- en: Xiang and Fox [[88](#bib.bib88)] proposed Data Associated Recurrent Neural Networks
    (DA-RNNs), for joint 3D scene mapping and semantic labeling. DA-RNNs use a new
    recurrent neural network architecture for semantic labeling on RGB-D videos. The
    output of the network is integrated with mapping techniques such as Kinect-Fusion
    in order to inject semantic information into the reconstructed 3D scene.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Xiang和Fox[[88](#bib.bib88)]提出了数据关联递归神经网络（DA-RNNs），用于联合3D场景映射和语义标注。DA-RNNs使用一种新的递归神经网络架构在RGB-D视频上进行语义标注。网络的输出与如Kinect-Fusion等映射技术集成，以将语义信息注入重建的3D场景中。
- en: 'Hu et al. [[89](#bib.bib89)] developed a semantic segmentation algorithm based
    on natural language expression, using a combination of CNN to encode the image
    and LSTM to encode its natural language description. This is different from traditional
    semantic segmentation over a predefined set of semantic classes, as, e.g., the
    phrase “two men sitting on the right bench” requires segmenting only the two people
    on the right bench and no one standing or sitting on another bench. To produce
    pixel-wise segmentation for language expression, they propose an end-to-end trainable
    recurrent and convolutional model that jointly learns to process visual and linguistic
    information (Figure [27](#S3.F27 "Figure 27 ‣ 3.7 Recurrent Neural Network Based
    Models ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep
    Learning: A Survey")). In the considered model, a recurrent LSTM network is used
    to encode the referential expression into a vector representation, and an FCN
    is used to extract a spatial feature map from the image and output a spatial response
    map for the target object. An example segmentation result of this model (for the
    query “people in blue coat”) is shown in Figure [28](#S3.F28 "Figure 28 ‣ 3.7
    Recurrent Neural Network Based Models ‣ 3 DL-Based Image Segmentation Models ‣
    Image Segmentation Using Deep Learning: A Survey").'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 胡等人[[89](#bib.bib89)]开发了一种基于自然语言表达的语义分割算法，使用 CNN 编码图像和 LSTM 编码自然语言描述的组合。这不同于在预定义的语义类别集合上进行的传统语义分割，例如，短语“两个男人坐在右边的长椅上”只需要分割右边长椅上的两个个人，而不涉及站立或坐在其他长椅上的人。为了生成像素级分割以匹配语言表达，他们提出了一种端到端可训练的递归和卷积模型，该模型共同学习处理视觉和语言信息（图[27](#S3.F27
    "图 27 ‣ 3.7 基于递归神经网络的模型 ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")）。在考虑的模型中，使用递归 LSTM
    网络将指称表达编码为向量表示，使用 FCN 从图像中提取空间特征图，并输出目标对象的空间响应图。该模型的一个示例分割结果（对于查询“穿蓝色外套的人”）如图[28](#S3.F28
    "图 28 ‣ 3.7 基于递归神经网络的模型 ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")所示。
- en: '![Refer to caption](img/8727973dfef1251fc5730d69fb2849d7.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8727973dfef1251fc5730d69fb2849d7.png)'
- en: 'Figure 27: The CNN+LSTM architecture for segmentation from natural language
    expressions. From [[89](#bib.bib89)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图27：自然语言表达的 CNN+LSTM 架构。来自[[89](#bib.bib89)]。
- en: '![Refer to caption](img/9f1138c8c919b42df564a0b7082b3c0c.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f1138c8c919b42df564a0b7082b3c0c.png)'
- en: 'Figure 28: Segmentation masks generated for the query “people in blue coat”.
    From [[89](#bib.bib89)].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图28：为查询“穿蓝色外套的人”生成的分割掩模。来自[[89](#bib.bib89)]。
- en: One limitation of RNN based models is that, due to the sequential nature these
    models, they will be slower than their CNN counterpart, since this sequential
    calculation cannot be parallelized easily.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 基于模型的一个限制是，由于这些模型的顺序性质，它们的速度比 CNN 对应模型慢，因为这种顺序计算不容易并行化。
- en: 3.8 Attention-Based Models
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 基于注意力的模型
- en: Attention mechanisms have been persistently explored in computer vision over
    the years, and it is therefore not surprising to find publications that apply
    such mechanisms to semantic segmentation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，注意力机制在计算机视觉领域得到了持续探索，因此发现应用这些机制于语义分割的出版物也就不足为奇了。
- en: 'Chen et al. [[90](#bib.bib90)] proposed an attention mechanism that learns
    to softly weight multi-scale features at each pixel location. They adapt a powerful
    semantic segmentation model and jointly train it with multi-scale images and the
    attention model (Figure [29](#S3.F29 "Figure 29 ‣ 3.8 Attention-Based Models ‣
    3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep Learning:
    A Survey")). The attention mechanism outperforms average and max pooling, and
    it enables the model to assess the importance of features at different positions
    and scales.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[90](#bib.bib90)]提出了一种注意力机制，能够在每个像素位置上学习柔性加权多尺度特征。他们调整了一种强大的语义分割模型，并与多尺度图像和注意力模型共同训练（图[29](#S3.F29
    "图 29 ‣ 3.8 基于注意力的模型 ‣ 3 基于深度学习的图像分割模型 ‣ 使用深度学习的图像分割：综述")）。注意力机制优于平均和最大池化，并使模型能够评估不同位置和尺度下特征的重要性。
- en: '![Refer to caption](img/8a51828d51fd6869cfb3308b1cc3a49e.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a51828d51fd6869cfb3308b1cc3a49e.png)'
- en: 'Figure 29: Attention-based semantic segmentation model. The attention model
    learns to assign different weights to objects of different scales; e.g., the model
    assigns large weights on the small person (green dashed circle) for features from
    scale 1.0, and large weights on the large child (magenta dashed circle) for features
    from scale 0.5\. From [[90](#bib.bib90)].'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 29：基于注意力的语义分割模型。注意力模型学习为不同尺度的对象分配不同的权重；例如，模型为来自尺度 1.0 的特征分配较大的权重给小型人（绿色虚线圆圈），为来自尺度
    0.5 的特征分配较大的权重给大型儿童（洋红色虚线圆圈）。来自 [[90](#bib.bib90)]。
- en: 'In contrast to other works in which convolutional classifiers are trained to
    learn the representative semantic features of labeled objects, Huang et al. [[91](#bib.bib91)]
    proposed a semantic segmentation approach using reverse attention mechanisms.
    Their Reverse Attention Network (RAN) architecture (Figure [30](#S3.F30 "Figure
    30 ‣ 3.8 Attention-Based Models ‣ 3 DL-Based Image Segmentation Models ‣ Image
    Segmentation Using Deep Learning: A Survey")) trains the model to capture the
    opposite concept (i.e., features that are not associated with a target class)
    as well. The RAN is a three-branch network that performs the direct, and reverse-attention
    learning processes simultaneously.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他工作中训练卷积分类器以学习标记对象的代表性语义特征不同，Huang 等人 [[91](#bib.bib91)] 提出了使用反向注意力机制的语义分割方法。他们的反向注意力网络（RAN）架构（图 [30](#S3.F30
    "图 30 ‣ 3.8 基于注意力的模型 ‣ 3 DL 基于图像分割模型 ‣ 使用深度学习的图像分割：一项调查")）训练模型捕捉相反的概念（即，与目标类别无关的特征）。RAN
    是一个三分支网络，能够同时执行直接和反向注意力学习过程。
- en: '![Refer to caption](img/e27d0be7188c555b9e9770313391fc77.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/e27d0be7188c555b9e9770313391fc77.png)'
- en: 'Figure 30: The reverse attention network for segmentation. From [[91](#bib.bib91)].'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 30：用于分割的反向注意力网络。来自 [[91](#bib.bib91)]。
- en: Li et al. [[92](#bib.bib92)] developed a Pyramid Attention Network for semantic
    segmentation. This model exploits the impact of global contextual information
    in semantic segmentation. They combined attention mechanisms and spatial pyramids
    to extract precise dense features for pixel labeling, instead of complicated dilated
    convolutions and artificially designed decoder networks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 [[92](#bib.bib92)] 开发了一个用于语义分割的金字塔注意力网络。该模型利用了全局上下文信息在语义分割中的影响。他们结合了注意力机制和空间金字塔，以提取用于像素标记的精确密集特征，而不是复杂的扩张卷积和人工设计的解码器网络。
- en: More recently, Fu et al. [[93](#bib.bib93)] proposed a dual attention network
    for scene segmentation, which can capture rich contextual dependencies based on
    the self-attention mechanism. Specifically, they append two types of attention
    modules on top of a dilated FCN which models the semantic inter-dependencies in
    spatial and channel dimensions, respectively. The position attention module selectively
    aggregates the feature at each position by a weighted sum of the features at all
    positions.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Fu 等人 [[93](#bib.bib93)] 提出了一个用于场景分割的双重注意力网络，该网络可以基于自注意力机制捕捉丰富的上下文依赖关系。具体来说，他们在扩张的全卷积网络（FCN）上附加了两种类型的注意力模块，分别建模空间和通道维度中的语义相互依赖关系。位置注意力模块通过对所有位置的特征加权求和，选择性地聚合每个位置的特征。
- en: 'Various other works explore attention mechanisms for semantic segmentation,
    such as OCNet [[94](#bib.bib94)] which proposed an object context pooling inspired
    by self-attention mechanism, Expectation-Maximization Attention (EMANet) [[95](#bib.bib95)],
    Criss-Cross Attention Network (CCNet) [[96](#bib.bib96)], end-to-end instance
    segmentation with recurrent attention [[97](#bib.bib97)], a point-wise spatial
    attention network for scene parsing [[98](#bib.bib98)], and a discriminative feature
    network (DFN) [[99](#bib.bib99)], which comprises two sub-networks: a Smooth Network
    (that contains a Channel Attention Block and global average pooling to select
    the more discriminative features) and a Border Network (to make the bilateral
    features of the boundary distinguishable).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 其他各种工作探索了用于语义分割的注意力机制，例如 OCNet [[94](#bib.bib94)] 提出了受自注意力机制启发的对象上下文池化，Expectation-Maximization
    Attention (EMANet) [[95](#bib.bib95)]，交叉注意力网络 (CCNet) [[96](#bib.bib96)]，具有递归注意力的端到端实例分割
    [[97](#bib.bib97)]，用于场景解析的点对点空间注意力网络 [[98](#bib.bib98)]，以及判别特征网络 (DFN) [[99](#bib.bib99)]，该网络包括两个子网络：一个平滑网络（包含一个通道注意力块和全局平均池化，用于选择更具判别性的特征）和一个边界网络（使边界的双边特征可区分）。
- en: 3.9 Generative Models and Adversarial Training
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9 生成模型和对抗训练
- en: Since their introduction, GANs have been applied to a wide range tasks in computer
    vision, and have been adopted for image segmentation too.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 自从它们被引入以来，GAN 已被应用于计算机视觉的广泛任务，也被用于图像分割。
- en: 'Luc et al. [[100](#bib.bib100)] proposed an adversarial training approach for
    semantic segmentation. They trained a convolutional semantic segmentation network
    (Figure [31](#S3.F31 "Figure 31 ‣ 3.9 Generative Models and Adversarial Training
    ‣ 3 DL-Based Image Segmentation Models ‣ Image Segmentation Using Deep Learning:
    A Survey")), along with an adversarial network that discriminates ground-truth
    segmentation maps from those generated by the segmentation network. They showed
    that the adversarial training approach leads to improved accuracy on the Stanford
    Background and PASCAL VOC 2012 datasets.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Luc 等人 [[100](#bib.bib100)] 提出了用于语义分割的对抗训练方法。他们训练了一个卷积语义分割网络（图 [31](#S3.F31
    "图 31 ‣ 3.9 生成模型与对抗训练 ‣ 3 基于 DL 的图像分割模型 ‣ 基于深度学习的图像分割：综述")），以及一个对抗网络，该网络用于区分真实分割图和由分割网络生成的图。他们展示了对抗训练方法在斯坦福背景和
    PASCAL VOC 2012 数据集上的准确性有所提高。
- en: '![Refer to caption](img/91eb6aa4763feb9c3e5a3d799dd631ec.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/91eb6aa4763feb9c3e5a3d799dd631ec.png)'
- en: 'Figure 31: The GAN for semantic segmentation. From [[100](#bib.bib100)].'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '图 31: 用于语义分割的 GAN。来自 [[100](#bib.bib100)]。'
- en: Souly et al. [[101](#bib.bib101)] proposed semi-weakly supervised semantic segmentation
    using GANs. It consists of a generator network providing extra training examples
    to a multi-class classifier, acting as discriminator in the GAN framework, that
    assigns sample a label y from the $K$ possible classes or marks it as a fake sample
    (extra class).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Souly 等人 [[101](#bib.bib101)] 提出了使用 GAN 的半弱监督语义分割方法。该方法包括一个生成器网络，为多类别分类器提供额外的训练样本，该分类器在
    GAN 框架中充当判别器，给样本分配 $K$ 个可能类别中的一个标签，或将其标记为伪样本（额外类别）。
- en: 'In another work, Hung et al. [[102](#bib.bib102)] developed a framework for
    semi-supervised semantic segmentation using an adversarial network. They designed
    an FCN discriminator to differentiate the predicted probability maps from the
    ground truth segmentation distribution, considering the spatial resolution. The
    considered loss function of this model contains three terms: cross-entropy loss
    on the segmentation ground truth, adversarial loss of the discriminator network,
    and semi-supervised loss based on the confidence map; i.e., the output of the
    discriminator.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作中，Hung 等人 [[102](#bib.bib102)] 开发了一个使用对抗网络的半监督语义分割框架。他们设计了一个 FCN 判别器，以区分预测概率图和真实分割分布，同时考虑空间分辨率。该模型考虑的损失函数包含三个部分：分割真实标签的交叉熵损失、判别器网络的对抗损失，以及基于置信度图的半监督损失；即判别器的输出。
- en: Xue et al. [[103](#bib.bib103)] proposed an adversarial network with multi-scale
    L1 Loss for medical image segmentation. They used an FCN as the segmentor to generate
    segmentation label maps, and proposed a novel adversarial critic network with
    a multi-scale L1 loss function to force the critic and segmentor to learn both
    global and local features that capture long and short range spatial relationships
    between pixels.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Xue 等人 [[103](#bib.bib103)] 提出了一个具有多尺度 L1 损失的对抗网络，用于医学图像分割。他们使用 FCN 作为分割器生成分割标签图，并提出了一种新颖的对抗评论网络，该网络具有多尺度
    L1 损失函数，迫使评论器和分割器学习捕捉像素之间长短距离空间关系的全局和局部特征。
- en: Various other publications report on segmentation models based on adversarial
    training, such as Cell Image Segmentation Using GANs [[104](#bib.bib104)], and
    segmentation and generation of the invisible parts of objects [[105](#bib.bib105)].
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其他各种出版物报告了基于对抗训练的分割模型，例如使用 GAN 的细胞图像分割 [[104](#bib.bib104)]，以及对象不可见部分的分割和生成
    [[105](#bib.bib105)]。
- en: 3.10 CNN Models With Active Contour Models
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.10 活动轮廓模型的 CNN 模型
- en: The exploration of synergies between FCNs and Active Contour Models (ACMs) [[7](#bib.bib7)]
    has recently attracted research interest. One approach is to formulate new loss
    functions that are inspired by ACM principles. For example, inspired by the global
    energy formulation of [[106](#bib.bib106)], Chen et al. [[107](#bib.bib107)] proposed
    a supervised loss layer that incorporated area and size information of the predicted
    masks during training of an FCN and tackled the problem of ventricle segmentation
    in cardiac MRI.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: FCNs 与主动轮廓模型（ACMs）之间协同效应的探索[[7](#bib.bib7)]最近引起了研究兴趣。一种方法是制定新的损失函数，灵感来自ACM原理。例如，受到[[106](#bib.bib106)]全球能量公式的启发，Chen
    等人[[107](#bib.bib107)]提出了一种监督式损失层，在FCN训练期间结合了预测掩膜的面积和大小信息，并解决了心脏MRI中的心室分割问题。
- en: A different approach initially sought to utilize the ACM merely as a post-processor
    of the output of an FCN and several efforts attempted modest co-learning by pre-training
    the FCN. One example of an ACM post-processor for the task of semantic segmentation
    of natural images is the work by Le et al. [[108](#bib.bib108)] in which level-set
    ACMs are implemented as RNNs. Deep Active Contours by Rupprecht et al. [[109](#bib.bib109)],
    is another example. For medical image segmentation, Hatamizadeh et al. [[110](#bib.bib110)]
    proposed an integrated Deep Active Lesion Segmentation (DALS) model that trains
    the FCN backbone to predict the parameter functions of a novel, locally-parameterized
    level-set energy functional. In another relevant effort, Marcos et al. [[111](#bib.bib111)]
    proposed Deep Structured Active Contours (DSAC), which combines ACMs and pre-trained
    FCNs in a structured prediction framework for building instance segmentation (albeit
    with manual initialization) in aerial images. For the same application, Cheng
    et al. [[112](#bib.bib112)] proposed the Deep Active Ray Network (DarNet), which
    is similar to DSAC, but with a different explicit ACM formulation based on polar
    coordinates to prevent contour self-intersection. A truly end-to-end backpropagation
    trainable, fully-integrated FCN-ACM combination was recently introduced by Hatamizadeh
    et al. [[113](#bib.bib113)], dubbed Deep Convolutional Active Contours (DCAC).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法最初旨在仅将ACM用作FCN输出的后处理器，并且有几个尝试通过预训练FCN来进行适度的共同学习。例如，Le 等人[[108](#bib.bib108)]在自然图像语义分割任务中提出了ACM后处理器的工作，其中级别集ACMs被实现为RNNs。Rupprecht
    等人的深度主动轮廓[[109](#bib.bib109)]是另一个例子。对于医学图像分割，Hatamizadeh 等人[[110](#bib.bib110)]提出了一种集成的深度主动病变分割（DALS）模型，该模型训练FCN主干以预测新型局部参数化级别集能量泛函的参数函数。在另一个相关的努力中，Marcos
    等人[[111](#bib.bib111)]提出了深度结构化主动轮廓（DSAC），该方法结合了ACMs和预训练的FCNs，在结构化预测框架中进行实例分割（尽管需要手动初始化）。对于相同的应用，Cheng
    等人[[112](#bib.bib112)]提出了深度主动射线网络（DarNet），该方法类似于DSAC，但使用基于极坐标的不同显式ACM公式来防止轮廓自交。Hatamizadeh
    等人最近引入了一种真正的端到端反向传播可训练的全集成FCN-ACM组合，称为深度卷积主动轮廓（DCAC）[[113](#bib.bib113)]。
- en: 3.11 Other Models
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.11 其他模型
- en: 'In addition to the above models, there are several other popular DL architectures
    for segmentation, such as the following: Context Encoding Network (EncNet) that
    uses a basic feature extractor and feeds the feature maps into a Context Encoding
    Module [[114](#bib.bib114)]. RefineNet [[115](#bib.bib115)], which is a multi-path
    refinement network that explicitly exploits all the information available along
    the down-sampling process to enable high-resolution prediction using long-range
    residual connections. Seednet [[116](#bib.bib116)], which introduced an automatic
    seed generation technique with deep reinforcement learning that learns to solve
    the interactive segmentation problem. ”Object-Contextual Representations” (OCR)
    [[44](#bib.bib44)], which learns object regions under the supervision of the ground-truth,
    and computes the object region representation, and the relation between each pixel
    and each object region, and augment the representation pixels with the object-contextual
    representation. Yet additional models include BoxSup [[117](#bib.bib117)], Graph
    convolutional networks [[118](#bib.bib118)], Wide ResNet [[119](#bib.bib119)],
    Exfuse (enhancing low-level and high-level features fusion) [[120](#bib.bib120)],
    Feedforward-Net [[121](#bib.bib121)], saliency-aware models for geodesic video
    segmentation [[122](#bib.bib122)], dual image segmentation (DIS) [[123](#bib.bib123)],
    FoveaNet (Perspective-aware scene parsing) [[124](#bib.bib124)], Ladder DenseNet
    [[125](#bib.bib125)], Bilateral segmentation network (BiSeNet) [[126](#bib.bib126)],
    Semantic Prediction Guidance for Scene Parsing (SPGNet) [[127](#bib.bib127)],
    Gated shape CNNs [[128](#bib.bib128)], Adaptive context network (AC-Net) [[129](#bib.bib129)],
    Dynamic-structured semantic propagation network (DSSPN) [[130](#bib.bib130)],
    symbolic graph reasoning (SGR) [[131](#bib.bib131)], CascadeNet [[132](#bib.bib132)],
    Scale-adaptive convolutions (SAC) [[133](#bib.bib133)], Unified perceptual parsing
    (UperNet) [[134](#bib.bib134)], segmentation by re-training and self-training
    [[135](#bib.bib135)], densely connected neural architecture search [[136](#bib.bib136)],
    hierarchical multi-scale attention [[137](#bib.bib137)].'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述模型，还有一些其他流行的深度学习架构用于分割，如：使用基础特征提取器并将特征图输入到上下文编码模块的上下文编码网络（EncNet） [[114](#bib.bib114)]。RefineNet
    [[115](#bib.bib115)]，这是一个多路径细化网络，明确利用下采样过程中的所有可用信息，通过长距离残差连接实现高分辨率预测。Seednet [[116](#bib.bib116)]，引入了一种自动种子生成技术，结合深度强化学习，学习解决交互式分割问题。
    “对象上下文表示” (OCR) [[44](#bib.bib44)]，在真实标签的监督下学习对象区域，计算对象区域表示，以及每个像素与每个对象区域之间的关系，并用对象上下文表示增强表示像素。其他模型还包括
    BoxSup [[117](#bib.bib117)]、图卷积网络 [[118](#bib.bib118)]、Wide ResNet [[119](#bib.bib119)]、Exfuse（增强低级和高级特征融合）
    [[120](#bib.bib120)]、Feedforward-Net [[121](#bib.bib121)]、用于测地视频分割的显著性感知模型 [[122](#bib.bib122)]、双图像分割（DIS）
    [[123](#bib.bib123)]、FoveaNet（透视感知场景解析） [[124](#bib.bib124)]、Ladder DenseNet [[125](#bib.bib125)]、双边分割网络（BiSeNet）
    [[126](#bib.bib126)]、用于场景解析的语义预测指导（SPGNet） [[127](#bib.bib127)]、门控形状 CNNs [[128](#bib.bib128)]、自适应上下文网络（AC-Net）
    [[129](#bib.bib129)]、动态结构语义传播网络（DSSPN） [[130](#bib.bib130)]、符号图推理（SGR） [[131](#bib.bib131)]、CascadeNet
    [[132](#bib.bib132)]、尺度自适应卷积（SAC） [[133](#bib.bib133)]、统一感知解析（UperNet） [[134](#bib.bib134)]、通过重新训练和自我训练进行分割
    [[135](#bib.bib135)]、密集连接神经架构搜索 [[136](#bib.bib136)] 和层次化多尺度注意力 [[137](#bib.bib137)]。
- en: Panoptic segmentation [[138](#bib.bib138)] is also another interesting segmentation
    problem with rising popularity, and there are already several interesting works
    on this direction, including Panoptic Feature Pyramid Network [[139](#bib.bib139)],
    attention-guided network for Panoptic segmentation [[140](#bib.bib140)], Seamless
    Scene Segmentation [[141](#bib.bib141)], panoptic deeplab [[142](#bib.bib142)],
    unified panoptic segmentation network [[143](#bib.bib143)], efficient panoptic
    segmentation [[144](#bib.bib144)].
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 全景分割 [[138](#bib.bib138)] 也是一个引人注目的分割问题，近年来逐渐受到关注，目前已有若干有趣的工作在这一方向上，包括全景特征金字塔网络
    [[139](#bib.bib139)]、用于全景分割的注意力引导网络 [[140](#bib.bib140)]、无缝场景分割 [[141](#bib.bib141)]、全景深度实验室
    [[142](#bib.bib142)]、统一全景分割网络 [[143](#bib.bib143)] 和高效全景分割 [[144](#bib.bib144)]。
- en: 'Figure [32](#S3.F32 "Figure 32 ‣ 3.11 Other Models ‣ 3 DL-Based Image Segmentation
    Models ‣ Image Segmentation Using Deep Learning: A Survey") illustrates the timeline
    of popular DL-based works for semantic segmentation, as well as instance segmentation
    since 2014. Given the large number of works developed in the last few years, we
    only show some of the most representative ones.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [32](#S3.F32 "图 32 ‣ 3.11 其他模型 ‣ 3 基于DL的图像分割模型 ‣ 基于深度学习的图像分割：一项调查") 说明了自2014年以来流行的基于DL的语义分割和实例分割的时间线。鉴于近年来开发了大量工作，我们仅展示了一些最具代表性的工作。
- en: '![Refer to caption](img/724e3edf34501d42e290c569cf2c8721.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/724e3edf34501d42e290c569cf2c8721.png)'
- en: 'Figure 32: The timeline of DL-based segmentation algorithms for 2D images,
    from 2014 to 2020\. Orange, green, andn yellow blocks refer to semantic, instance,
    and panoptic segmentation algorithms respectively.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '图 32: 从2014年到2020年的2D图像的DL-based分割算法时间线。橙色、绿色和黄色块分别表示语义分割、实例分割和全景分割算法。'
- en: 4 Image Segmentation Datasets
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 图像分割数据集
- en: In this section we provide a summary of some of the most widely used image segmentation
    datasets. We group these datasets into 3 categories—2D images, 2.5D RGB-D (color+depth)
    images, and 3D images—and provide details about the characteristics of each dataset.
    The listed datasets have pixel-wise labels, which can be used for evaluating model
    performance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本节总结了一些最广泛使用的图像分割数据集。我们将这些数据集分为3类——2D图像、2.5D RGB-D（彩色+深度）图像和3D图像，并提供有关每个数据集特征的详细信息。列出的数据集具有像素级标签，可用于评估模型性能。
- en: It is worth mentioning that some of these works, use data augmentation to increase
    the number of labeled samples, specially the ones which deal with small datasets
    (such as in medical domain). Data augmentation serves to increase the number of
    training samples by applying a set of transformation (either in the data space,
    or feature space, or sometimes both) to the images (i.e., both the input image
    and the segmentation map). Some typical transformations include translation, reflection,
    rotation, warping, scaling, color space shifting, cropping, and projections onto
    principal components. Data augmentation has proven to improve the performance
    of the models, especially when learning from limited datasets, such as those in
    medical image analysis. It can also be beneficial in yielding faster convergence,
    decreasing the chance of over-fitting, and enhancing generalization. For some
    small datasets, data augmentation has been shown to boost model performance more
    than 20%.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，其中一些工作使用数据增强来增加标记样本的数量，特别是处理小型数据集的工作（例如医学领域）。数据增强通过对图像（即输入图像和分割图）应用一系列变换（无论是在数据空间、特征空间，还是有时两者兼而有之）来增加训练样本的数量。一些典型的变换包括平移、反射、旋转、变形、缩放、色彩空间转换、裁剪和主成分投影。数据增强已被证明可以提高模型的性能，特别是在从有限的数据集中学习时，例如医学图像分析中的数据集。它还可以在实现更快的收敛、减少过拟合的可能性和增强泛化能力方面发挥作用。对于一些小型数据集，数据增强已被证明可以将模型性能提升超过20%。
- en: 4.1 2D Datasets
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 2D 数据集
- en: 'The majority of image segmentation research has focused on 2D images; therefore,
    many 2D image segmentation datasets are available. The following are some of the
    most popular:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数图像分割研究集中于2D图像，因此有许多2D图像分割数据集可用。以下是一些最受欢迎的数据集：
- en: 'PASCAL Visual Object Classes (VOC) [[145](#bib.bib145)] is one of most popular
    datasets in computer vision, with annotated images available for 5 tasks—classification,
    segmentation, detection, action recognition, and person layout. Nearly all popular
    segmentation algorithms reported in the literature have been evaluated on this
    dataset. For the segmentation task, there are 21 classes of object labels—vehicles,
    household, animals, aeroplane, bicycle, boat, bus, car, motorbike, train, bottle,
    chair, dining table, potted plant, sofa, TV/monitor, bird, cat, cow, dog, horse,
    sheep, and person (pixel are labeled as background if they do not belong to any
    of these classes). This dataset is divided into two sets, training and validation,
    with 1,464 and 1,449 images, respectively. There is a private test set for the
    actual challenge. Figure [33](#S4.F33 "Figure 33 ‣ 4.1 2D Datasets ‣ 4 Image Segmentation
    Datasets ‣ Image Segmentation Using Deep Learning: A Survey") shows an example
    image and its pixel-wise label.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 'PASCAL Visual Object Classes (VOC) [[145](#bib.bib145)] 是计算机视觉领域中最受欢迎的数据集之一，提供了5种任务的标注图像——分类、分割、检测、动作识别和人员布局。几乎所有文献中报告的流行分割算法都在该数据集上进行了评估。对于分割任务，有21类物体标签——车辆、家庭用品、动物、飞机、自行车、船、公共汽车、汽车、摩托车、火车、瓶子、椅子、餐桌、盆栽植物、沙发、电视/显示器、鸟、猫、牛、狗、马、羊和人（如果像素不属于这些类别中的任何一种，则标记为背景）。该数据集分为两个子集，训练集和验证集，分别包含1,464张和1,449张图像。实际挑战中还有一个私人测试集。图 [33](#S4.F33
    "Figure 33 ‣ 4.1 2D Datasets ‣ 4 Image Segmentation Datasets ‣ Image Segmentation
    Using Deep Learning: A Survey") 显示了一个示例图像及其像素级标签。'
- en: '![Refer to caption](img/fbd7ee801b7aab257330ad8f214eea3c.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fbd7ee801b7aab257330ad8f214eea3c.png)'
- en: 'Figure 33: An example image from the PASCAL VOC dataset. From [[146](#bib.bib146)]'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 33：来自 PASCAL VOC 数据集的示例图像。来源 [[146](#bib.bib146)]
- en: PASCAL Context [[147](#bib.bib147)] is an extension of the PASCAL VOC 2010 detection
    challenge, and it contains pixel-wise labels for all training images. It contains
    more than 400 classes (including the original 20 classes plus backgrounds from
    PASCAL VOC segmentation), divided into three categories (objects, stuff, and hybrids).
    Many of the object categories of this dataset are too sparse and; therefore, a
    subset of 59 frequent classes are usually selected for use.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL Context [[147](#bib.bib147)] 是对 PASCAL VOC 2010 检测挑战的扩展，包含所有训练图像的像素级标签。它包含超过400个类别（包括PASCAL
    VOC 分割中的原始20个类别加上背景），分为三类（物体、材料和混合体）。该数据集的许多物体类别过于稀疏，因此通常选择59个常见类别的子集进行使用。
- en: 'Microsoft Common Objects in Context (MS COCO) [[148](#bib.bib148)] is another
    large-scale object detection, segmentation, and captioning dataset. COCO includes
    images of complex everyday scenes, containing common objects in their natural
    contexts. This dataset contains photos of 91 objects types, with a total of 2.5
    million labeled instances in 328k images. Figure [34](#S4.F34 "Figure 34 ‣ 4.1
    2D Datasets ‣ 4 Image Segmentation Datasets ‣ Image Segmentation Using Deep Learning:
    A Survey") shows the difference between MS-COCO labels and the previous datasets
    for a given sample image. The detection challenge includes more than 80 classes,
    providing more than 82k images for training, 40.5k images for validation, and
    more than 80k images for its test set.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 'Microsoft Common Objects in Context (MS COCO) [[148](#bib.bib148)] 是另一个大规模的目标检测、分割和标注数据集。COCO
    包含复杂日常场景的图像，展示了常见物体在自然背景中的样子。该数据集包含91种物体类型的照片，共有250万标注实例，分布在328,000张图像中。图 [34](#S4.F34
    "Figure 34 ‣ 4.1 2D Datasets ‣ 4 Image Segmentation Datasets ‣ Image Segmentation
    Using Deep Learning: A Survey") 显示了 MS-COCO 标签与之前数据集在给定样本图像中的区别。检测挑战包括80多个类别，为训练提供了超过82,000张图像，为验证提供了40,500张图像，并为测试集提供了超过80,000张图像。'
- en: '![Refer to caption](img/3ace14d7a9556d361dcab420a5163386.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3ace14d7a9556d361dcab420a5163386.png)'
- en: 'Figure 34: A sample image and its segmentation map in COCO, and its comparison
    with previous datasets. From [[148](#bib.bib148)].'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 34：COCO中的一个示例图像及其分割图，并与之前的数据集进行比较。来源 [[148](#bib.bib148)]。
- en: 'Cityscapes [[149](#bib.bib149)] is a large-scale database with a focus on semantic
    understanding of urban street scenes. It contains a diverse set of stereo video
    sequences recorded in street scenes from 50 cities, with high quality pixel-level
    annotation of 5k frames, in addition to a set of 20k weakly annotated frames.
    It includes semantic and dense pixel annotations of 30 classes, grouped into 8
    categories—flat surfaces, humans, vehicles, constructions, objects, nature, sky,
    and void. Figure [35](#S4.F35 "Figure 35 ‣ 4.1 2D Datasets ‣ 4 Image Segmentation
    Datasets ‣ Image Segmentation Using Deep Learning: A Survey") shows four sample
    segmentation maps from this dataset.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cityscapes [[149](#bib.bib149)] 是一个大型数据库，专注于城市街道场景的语义理解。它包含在 50 个城市街道场景中录制的多样化立体视频序列，提供了
    5k 帧的高质量逐像素注释，以及 20k 帧的弱标注。它包括 30 类的语义和密集像素注释，分为 8 个类别——平面表面、人类、车辆、建筑、物体、自然、天空和空洞。图 [35](#S4.F35
    "Figure 35 ‣ 4.1 2D Datasets ‣ 4 Image Segmentation Datasets ‣ Image Segmentation
    Using Deep Learning: A Survey") 显示了来自该数据集的四个样本分割图。'
- en: '![Refer to caption](img/9af351c421fad0a0d2b8f185fe05323f.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9af351c421fad0a0d2b8f185fe05323f.png)'
- en: 'Figure 35: Three sample images with their corresponding segmentation maps from
    the Cityscapes dataset. From [[149](#bib.bib149)].'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 35：来自 Cityscapes 数据集的三个样本图像及其对应的分割图。来源 [[149](#bib.bib149)]。
- en: ADE20K / MIT Scene Parsing (SceneParse150) offers a standard training and evaluation
    platform for scene parsing algorithms. The data for this benchmark comes from
    the ADE20K dataset [[132](#bib.bib132)], which contains more than 20K scene-centric
    images exhaustively annotated with objects and object parts. The benchmark is
    divided into 20K images for training, 2K images for validation, and another batch
    of images for testing. There are 150 semantic categories in this dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ADE20K / MIT Scene Parsing (SceneParse150) 提供了一个标准的训练和评估平台，用于场景解析算法。该基准的数据来自
    ADE20K 数据集 [[132](#bib.bib132)]，该数据集包含超过 20K 张以场景为中心的图像，逐一标注了对象及其部件。基准分为 20K 张用于训练，2K
    张用于验证，以及另一批用于测试的图像。该数据集有 150 个语义类别。
- en: SiftFlow [[150](#bib.bib150)] includes 2,688 annotated images from a subset
    of the LabelMe database. The $256\times 256$ pixel images are based on 8 different
    outdoor scenes, among them streets, mountains, fields, beaches, and buildings.
    All images belong to one of 33 semantic classes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: SiftFlow [[150](#bib.bib150)] 包括来自 LabelMe 数据库子集的 2,688 张注释图像。这些 $256\times
    256$ 像素图像基于 8 种不同的户外场景，包括街道、山脉、田野、海滩和建筑。所有图像属于 33 个语义类别中的一种。
- en: Stanford background [[151](#bib.bib151)] contains outdoor images of scenes from
    existing datasets, such as LabelMe, MSRC, and PASCAL VOC. It contains 715 images
    with at least one foreground object. The dataset is pixel-wise annotated, and
    can be used for semantic scene understanding. Semantic and geometric labels for
    this dataset were obtained using Amazon’s Mechanical Turk (AMT).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Stanford background [[151](#bib.bib151)] 包含来自现有数据集（如 LabelMe、MSRC 和 PASCAL VOC）的户外场景图像。该数据集包含
    715 张至少有一个前景对象的图像。数据集是逐像素注释的，可用于语义场景理解。该数据集的语义和几何标签是通过亚马逊的 Mechanical Turk (AMT)
    获得的。
- en: Berkeley Segmentation Dataset (BSD) [[152](#bib.bib152)] contains 12,000 hand-labeled
    segmentations of 1,000 Corel dataset images from 30 human subjects. It aims to
    provide an empirical basis for research on image segmentation and boundary detection.
    Half of the segmentations were obtained from presenting the subject a color image
    and the other half from presenting a grayscale image.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Berkeley Segmentation Dataset (BSD) [[152](#bib.bib152)] 包含 12,000 个由 30 名人类受试者对
    1,000 张 Corel 数据集图像进行的手工标注分割。它旨在为图像分割和边界检测研究提供实证基础。半数的分割是通过向受试者呈现彩色图像获得的，另一半则通过呈现灰度图像获得。
- en: Youtube-Objects [[153](#bib.bib153)] contains videos collected from YouTube,
    which include objects from ten PASCAL VOC classes (aeroplane, bird, boat, car,
    cat, cow, dog, horse, motorbike, and train). The original dataset did not contain
    pixel-wise annotations (as it was originally developed for object detection, with
    weak annotations). However, Jain et al. [[154](#bib.bib154)] manually annotated
    a subset of 126 sequences, and then extracted a subset of frames to further generate
    semantic labels. In total, there are about 10,167 annotated 480x360 pixel frames
    available in this dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Youtube-Objects [[153](#bib.bib153)] 包含了从YouTube收集的视频，这些视频包括了来自十个PASCAL VOC类别的物体（飞机、鸟类、船、汽车、猫、牛、狗、马、摩托车和火车）。原始数据集没有包含逐像素的注释（因为它最初是为物体检测开发的，注释较弱）。然而，Jain等人[[154](#bib.bib154)]
    手动注释了126个序列的一个子集，然后提取了一部分帧以进一步生成语义标签。总共有约10,167帧480x360像素的注释图像在这个数据集中可用。
- en: KITTI [[155](#bib.bib155)] is one of the most popular datasets for mobile robotics
    and autonomous driving. It contains hours of videos of traffic scenarios, recorded
    with a variety of sensor modalities (including high-resolution RGB, grayscale
    stereo cameras, and a 3D laser scanners). The original dataset does not contain
    ground truth for semantic segmentation, but researchers have manually annotated
    parts of the dataset for research purposes. For example, Alvarez et al. [[156](#bib.bib156)]
    generated ground truth for 323 images from the road detection challenge with 3
    classes, road, vertical, and sky.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI [[155](#bib.bib155)] 是最受欢迎的移动机器人和自动驾驶数据集之一。它包含了数小时的交通场景视频，这些视频是通过各种传感器（包括高分辨率RGB、灰度立体相机和3D激光扫描仪）录制的。原始数据集没有提供语义分割的地面真相，但研究人员已经手动注释了部分数据集以供研究使用。例如，Alvarez等人[[156](#bib.bib156)]
    为道路检测挑战生成了323张图像的地面真相，分为道路、垂直和天空3类。
- en: Other Datasets are available for image segmentation purposes too, such as Semantic
    Boundaries Dataset (SBD) [[157](#bib.bib157)], PASCAL Part [[158](#bib.bib158)],
    SYNTHIA [[159](#bib.bib159)], and Adobe’s Portrait Segmentation [[160](#bib.bib160)].
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据集也可用于图像分割目的，例如语义边界数据集（SBD）[[157](#bib.bib157)]、PASCAL Part [[158](#bib.bib158)]、SYNTHIA
    [[159](#bib.bib159)] 和Adobe的肖像分割数据集[[160](#bib.bib160)]。
- en: 4.2 2.5D Datasets
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 2.5D数据集
- en: 'With the availability of affordable range scanners, RGB-D images have became
    popular in both research and industrial applications. The following RGB-D datasets
    are some of the most popular:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 随着廉价测距扫描仪的出现，RGB-D图像在研究和工业应用中变得越来越流行。以下RGB-D数据集是一些最受欢迎的数据集：
- en: 'NYU-D V2 [[161](#bib.bib161)] consists of video sequences from a variety of
    indoor scenes, recorded by the RGB and depth cameras of the Microsoft Kinect.
    It includes 1,449 densely labeled pairs of aligned RGB and depth images from more
    than 450 scenes taken from 3 cities. Each object is labeled with a class and an
    instance number (e.g., cup1, cup2, cup3, etc.). It also contains 407,024 unlabeled
    frames. This dataset is relatively small compared to other existing datasets.
    Figure [36](#S4.F36 "Figure 36 ‣ 4.2 2.5D Datasets ‣ 4 Image Segmentation Datasets
    ‣ Image Segmentation Using Deep Learning: A Survey") shows a sample image and
    its segmentation map.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 'NYU-D V2 [[161](#bib.bib161)] 包含了来自各种室内场景的视频序列，这些序列是由微软Kinect的RGB和深度相机录制的。数据集中包括了来自3个城市的超过450个场景的1,449对对齐的RGB和深度图像，每个物体都被标注了类别和实例编号（例如，cup1,
    cup2, cup3等）。它还包含了407,024帧未标注的图像。与其他现有的数据集相比，该数据集相对较小。图[36](#S4.F36 "Figure 36
    ‣ 4.2 2.5D Datasets ‣ 4 Image Segmentation Datasets ‣ Image Segmentation Using
    Deep Learning: A Survey")展示了一个样本图像及其分割图。'
- en: '![Refer to caption](img/03681523bce988e9439b08c112b38f38.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/03681523bce988e9439b08c112b38f38.png)'
- en: 'Figure 36: A sample from the NYU V2 dataset. From left: the RGB image, pre-processed
    depth, and set of labels. From [[161](#bib.bib161)].'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图36：来自NYU V2数据集的样本。从左到右：RGB图像、预处理的深度图和标签集。来自[[161](#bib.bib161)]。
- en: SUN-3D [[162](#bib.bib162)] is a large-scale RGB-D video dataset that contains
    415 sequences captured for 254 different spaces in 41 different buildings; 8 sequences
    are annotated and more will be annotated in the future. Each annotated frame comes
    with the semantic segmentation of the objects in the scene, as well as information
    about the camera pose.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: SUN-3D [[162](#bib.bib162)] 是一个大规模的RGB-D视频数据集，包含了在41栋不同建筑中的254个不同空间捕获的415个序列；其中8个序列已经被注释，未来会有更多序列进行注释。每个注释的帧都附有场景中物体的语义分割信息，以及相机姿态的信息。
- en: SUN RGB-D [[163](#bib.bib163)] provides an RGB-D benchmark for the goal of advancing
    the state-of-the-art in all major scene understanding tasks. It is captured by
    four different sensors and contains 10,000 RGB-D images at a scale similar to
    PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons
    and 58,657 3D bounding boxes with accurate object orientations, as well as the
    3D room category and layout for scenes.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: SUN RGB-D [[163](#bib.bib163)] 提供了一个 RGB-D 基准，用于推动所有主要场景理解任务的最新技术。它由四种不同的传感器捕获，包含
    10,000 张 RGB-D 图像，规模类似于 PASCAL VOC。整个数据集密集注释，包括 146,617 个 2D 多边形和 58,657 个 3D
    边界框，具有准确的对象方向，以及场景的 3D 房间类别和布局。
- en: UW RGB-D Object Dataset [[164](#bib.bib164)] contains 300 common household objects
    recorded using a Kinect style 3D camera. The objects are organized into 51 categories,
    arranged using WordNet hypernym-hyponym relationships (similar to ImageNet). This
    dataset was recorded using a Kinect style 3D camera that records synchronized
    and aligned $640\times 480$ pixel RGB and depth images at 30 Hz. This dataset
    also includes 8 annotated video sequences of natural scenes, containing objects
    from the dataset (the UW RGB-D Scenes Dataset).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: UW RGB-D Object Dataset [[164](#bib.bib164)] 包含 300 个常见的家用物体，使用 Kinect 风格的 3D
    相机记录。这些物体被组织成 51 个类别，按 WordNet 上下位关系排列（类似于 ImageNet）。该数据集使用 Kinect 风格的 3D 相机记录，能够同步并对齐
    $640\times 480$ 像素的 RGB 和深度图像，采样率为 30 Hz。该数据集还包括 8 个注释的自然场景视频序列，包含数据集中对象（UW RGB-D
    Scenes Dataset）。
- en: ScanNet [[165](#bib.bib165)] is an RGB-D video dataset containing 2.5 million
    views in more than 1,500 scans, annotated with 3D camera poses, surface reconstructions,
    and instance-level semantic segmentations. To collect these data, an easy-to-use
    and scalable RGB-D capture system was designed that includes automated surface
    reconstruction, and the semantic annotation was crowd-sourced. Using this data
    helped achieve state-of-the-art performance on several 3D scene understanding
    tasks, including 3D object classification, semantic voxel labeling, and CAD model
    retrieval.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ScanNet [[165](#bib.bib165)] 是一个 RGB-D 视频数据集，包含 250 万个视图，来自 1,500 多个扫描，注释有 3D
    相机姿态、表面重建和实例级语义分割。为了收集这些数据，设计了一个易于使用和可扩展的 RGB-D 捕获系统，包括自动表面重建，语义注释则通过众包方式完成。利用这些数据在多个
    3D 场景理解任务中实现了最新的性能，包括 3D 对象分类、语义体素标记和 CAD 模型检索。
- en: 4.3 3D Datasets
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 3D 数据集
- en: 3D image datasets are popular in robotic, medical image analysis, 3D scene analysis,
    and construction applications. Three dimensional images are usually provided via
    meshes or other volumetric representations, such as point clouds. Here, we mention
    some of the popular 3D datasets.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 图像数据集在机器人、医学图像分析、3D 场景分析和建筑应用中非常受欢迎。三维图像通常通过网格或其他体积表示（如点云）提供。这里，我们提到一些流行的
    3D 数据集。
- en: 'Stanford 2D-3D: This dataset provides a variety of mutually registered modalities
    from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations
    [[166](#bib.bib166)], and is collected in 6 indoor areas. It contains over 70,000
    RGB images, along with the corresponding depths, surface normals, semantic annotations,
    global XYZ images as well as camera information.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 'Stanford 2D-3D: 该数据集提供了来自 2D、2.5D 和 3D 领域的多种互相注册的模式，具有实例级的语义和几何注释[[166](#bib.bib166)]，并且在
    6 个室内区域中收集。它包含超过 70,000 张 RGB 图像，以及相应的深度、表面法线、语义注释、全球 XYZ 图像以及相机信息。'
- en: 'ShapeNet Core: ShapeNetCore is a subset of the full ShapeNet dataset [[167](#bib.bib167)]
    with single clean 3D models and manually verified category and alignment annotations
    [[168](#bib.bib168)]. It covers 55 common object categories with about 51,300
    unique 3D models.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 'ShapeNet Core: ShapeNetCore 是完整 ShapeNet 数据集的一个子集[[167](#bib.bib167)]，包含单一干净的
    3D 模型和手动验证的类别及对齐注释[[168](#bib.bib168]]。它覆盖了 55 个常见对象类别，共约 51,300 个独特的 3D 模型。'
- en: 'Sydney Urban Objects Dataset: This dataset contains a variety of common urban
    road objects, collected in the central business district of Sydney, Australia.
    There are 631 individual scans of objects across classes of vehicles, pedestrians,
    signs and trees [[169](#bib.bib169)].'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sydney Urban Objects Dataset: 该数据集包含了多种常见的城市道路对象，采集于澳大利亚悉尼中央商务区。共有 631 个不同类别的对象扫描，包括车辆、行人、标志和树木[[169](#bib.bib169)]。'
- en: 5 Performance Review
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 性能评估
- en: In this section, we first provide a summary of some of the popular metrics used
    in evaluating the performance of segmentation models, and then we provide the
    quantitative performance of the promising DL-based segmentation models on popular
    datasets.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先总结了一些用于评估分割模型性能的流行指标，然后提供了有前景的基于 DL 的分割模型在流行数据集上的定量性能。
- en: 5.1 Metrics For Segmentation Models
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 分割模型指标
- en: Ideally, a model should be evaluated in multiple respects, such as quantitative
    accuracy, speed (inference time), and storage requirements (memory footprint).
    However, most of the research works so far, focus on the metrics for evaluating
    the model accuracy. Below we summarize the most popular metrics for assessing
    the accuracy of segmentation algorithms. Although quantitative metrics are used
    to compare different models on benchmarks, the visual quality of model outputs
    is also important in deciding which model is best (as human is the final consumer
    of many of the models developed for computer vision applications).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，模型应从多个方面进行评估，如定量准确性、速度（推断时间）和存储需求（内存占用）。然而，到目前为止，大多数研究工作主要关注评估模型准确性的指标。下面我们总结了评估分割算法准确性的最流行指标。虽然定量指标用于在基准测试中比较不同模型，但模型输出的视觉质量在决定哪个模型最佳时也很重要（因为人类是许多计算机视觉应用开发模型的最终用户）。
- en: 'Pixel accuracy simply finds the ratio of pixels properly classified, divided
    by the total number of pixels. For $K+1$ classes ($K$ foreground classes and the
    background) pixel accuracy is defined as Eq [1](#S5.E1 "In 5.1 Metrics For Segmentation
    Models ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning: A Survey"):'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '像素准确性简单地计算正确分类像素的比率，除以总像素数。对于 $K+1$ 类（$K$ 前景类别和背景），像素准确性定义如公式 [1](#S5.E1 "In
    5.1 Metrics For Segmentation Models ‣ 5 Performance Review ‣ Image Segmentation
    Using Deep Learning: A Survey") 所示：'
- en: '|  | $\text{PA}=\frac{\sum_{i=0}^{K}p_{ii}}{\sum_{i=0}^{K}\sum_{j=0}^{K}p_{ij}},$
    |  | (1) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{PA}=\frac{\sum_{i=0}^{K}p_{ii}}{\sum_{i=0}^{K}\sum_{j=0}^{K}p_{ij}},$
    |  | (1) |'
- en: where $p_{ij}$ is the number of pixels of class $i$ predicted as belonging to
    class $j$.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{ij}$ 是预测为属于类别 $j$ 的类别 $i$ 的像素数。
- en: 'Mean Pixel Accuracy (MPA) is the extended version of PA, in which the ratio
    of correct pixels is computed in a per-class manner and then averaged over the
    total number of classes, as in Eq [2](#S5.E2 "In 5.1 Metrics For Segmentation
    Models ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning: A Survey"):'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'Mean Pixel Accuracy (MPA) 是 PA 的扩展版本，其中正确像素的比率是按类别计算的，然后对类别总数取平均，如公式 [2](#S5.E2
    "In 5.1 Metrics For Segmentation Models ‣ 5 Performance Review ‣ Image Segmentation
    Using Deep Learning: A Survey") 所示：'
- en: '|  | $\text{MPA}=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}.$
    |  | (2) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MPA}=\frac{1}{K+1}\sum_{i=0}^{K}\frac{p_{ii}}{\sum_{j=0}^{K}p_{ij}}.$
    |  | (2) |'
- en: 'Intersection over Union (IoU) or the Jaccard Index is one of the most commonly
    used metrics in semantic segmentation. It is defined as the area of intersection
    between the predicted segmentation map and the ground truth, divided by the area
    of union between the predicted segmentation map and the ground truth:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Intersection over Union (IoU) 或 Jaccard 指数是语义分割中最常用的指标之一。它被定义为预测分割图与真实情况之间的交集面积，除以预测分割图与真实情况之间的并集面积：
- en: '|  | $\text{IoU}=J(A,B)=\frac{&#124;A\cap B&#124;}{&#124;A\cup B&#124;},$ |  |
    (3) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{IoU}=J(A,B)=\frac{&#124;A\cap B&#124;}{&#124;A\cup B&#124;},$ |  |
    (3) |'
- en: where $A$ and $B$ denote the ground truth and the predicted segmentation maps,
    respectively. It ranges between 0 and 1.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 和 $B$ 分别表示真实情况和预测分割图。其范围在 0 和 1 之间。
- en: Mean-IoU is another popular metric, which is defined as the average IoU over
    all classes. It is widely used in reporting the performance of modern segmentation
    algorithms.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Mean-IoU 是另一个流行的指标，定义为所有类别的平均 IoU。它在报告现代分割算法的性能时被广泛使用。
- en: 'Precision / Recall / F1 score are popular metrics for reporting the accuracy
    of many of the classical image segmentation models. Precision and recall can be
    defined for each class, as well as at the aggregate level, as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Precision / Recall / F1 score 是用于报告许多经典图像分割模型准确性的流行指标。Precision 和 recall 可以为每个类别定义，也可以在总体水平上定义，如下所示：
- en: '|  | Precision | $\displaystyle=\frac{\text{TP}}{\text{TP}+\text{FP}},\ \ \text{Recall}$
    | $\displaystyle=\frac{\text{TP}}{\text{TP}+\text{FN}},$ |  | (4) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | Precision | $\displaystyle=\frac{\text{TP}}{\text{TP}+\text{FP}},\ \ \text{Recall}$
    | $\displaystyle=\frac{\text{TP}}{\text{TP}+\text{FN}},$ |  | (4) |'
- en: 'where TP refers to the true positive fraction, FP refers to the false positive
    fraction, and FN refers to the false negative fraction. Usually we are interested
    into a combined version of precision and recall rates. A popular such a metric
    is called the F1 score, which is defined as the harmonic mean of precision and
    recall:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 TP 指的是真正例率，FP 指的是假正例率，而 FN 指的是假负例率。通常我们关注的是精确度和召回率的综合版本。一种常用的指标是 F1 分数，它被定义为精确度和召回率的调和均值：
- en: '|  | $\text{F1-score}=\frac{2\ \text{Prec}\ \text{Rec}}{\text{Prec}+\text{Rec}}.$
    |  | (5) |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{F1-score}=\frac{2\ \text{Prec}\ \text{Rec}}{\text{Prec}+\text{Rec}}.$
    |  | (5) |'
- en: 'Dice coefficient is another popular metric for image segmentation (and is more
    commonly used in medical image analysis), which can be defined as twice the overlap
    area of predicted and ground-truth maps, divided by the total number of pixels
    in both images. The Dice coefficient is very similar to the IoU:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Dice 系数是另一种用于图像分割的流行指标（在医学图像分析中更为常用），它可以定义为预测图和真实图的重叠区域的两倍，除以两个图像的总像素数。Dice
    系数与 IoU 非常相似：
- en: '|  | $\text{Dice}=\frac{2&#124;A\cap B&#124;}{&#124;A&#124;+&#124;B&#124;}.$
    |  | (6) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Dice}=\frac{2\lvert A\cap B\rvert}{\lvert A\rvert+\lvert B\rvert}.$
    |  | (6) |'
- en: 'When applied to boolean data (e.g., binary segmentation maps), and referring
    to the foreground as a positive class, the Dice coefficient is essentially identical
    to the F1 score, defined as Eq [7](#S5.E7 "In 5.1 Metrics For Segmentation Models
    ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning: A Survey"):'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于布尔数据（例如，二值分割图），并将前景视为正类时，Dice 系数本质上等同于 F1 分数，定义见公式 [7](#S5.E7 "在 5.1 分割模型指标
    ‣ 5 性能回顾 ‣ 使用深度学习的图像分割：综述")：
- en: '|  | $\text{Dice}=\frac{2\text{TP}}{2\text{TP}+\text{FP}+\text{FN}}=\text{F1}.$
    |  | (7) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Dice}=\frac{2\text{TP}}{2\text{TP}+\text{FP}+\text{FN}}=\text{F1}.$
    |  | (7) |'
- en: 5.2 Quantitative Performance of DL-Based Models
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于深度学习的模型的定量性能
- en: In this section we tabulate the performance of several of the previously discussed
    algorithms on popular segmentation benchmarks. It is worth mentioning that although
    most models report their performance on standard datasets and use standard metrics,
    some of them fail to do so, making across-the-board comparisons difficult. Furthermore,
    only a small percentage of publications provide additional information, such as
    execution time and memory footprint, in a reproducible way, which is important
    to industrial applications of segmentation models (such as drones, self-driving
    cars, robotics, etc.) that may run on embedded consumer devices with limited computational
    power and storage, making fast, light-weight models crucial.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们列出了之前讨论的几个算法在流行的分割基准测试中的表现。值得提到的是，尽管大多数模型报告了它们在标准数据集上的表现并使用标准指标，但其中一些未能做到这一点，这使得全面比较变得困难。此外，只有一小部分出版物以可重复的方式提供了额外的信息，如执行时间和内存占用，这对工业应用中的分割模型（如无人机、自动驾驶汽车、机器人等）非常重要，这些应用可能在具有有限计算能力和存储的嵌入式消费设备上运行，因此快速、轻量级的模型至关重要。
- en: 'TABLE I: Accuracies of segmentation models on the PASCAL VOC test set. (* Refers
    to the model pre-trained on another dataset (such as MS-COCO, ImageNet, or JFT-300M).)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：PASCAL VOC 测试集上的分割模型准确率。（* 指模型在另一个数据集（如 MS-COCO、ImageNet 或 JFT-300M）上预训练。）
- en: '| Method | Backbone | mIoU |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | mIoU |'
- en: '| --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FCN [[31](#bib.bib31)] | VGG-16 | 62.2 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| FCN [[31](#bib.bib31)] | VGG-16 | 62.2 |'
- en: '| CRF-RNN [[39](#bib.bib39)] | - | 72.0 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CRF-RNN [[39](#bib.bib39)] | - | 72.0 |'
- en: '| CRF-RNN^∗ [[39](#bib.bib39)] | - | 74.7 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| CRF-RNN^∗ [[39](#bib.bib39)] | - | 74.7 |'
- en: '| BoxSup* [[117](#bib.bib117)] | - | 75.1 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| BoxSup* [[117](#bib.bib117)] | - | 75.1 |'
- en: '| Piecewise^∗ [[40](#bib.bib40)] | - | 78.0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Piecewise^∗ [[40](#bib.bib40)] | - | 78.0 |'
- en: '| DPN^∗ [[41](#bib.bib41)] | - | 77.5 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| DPN^∗ [[41](#bib.bib41)] | - | 77.5 |'
- en: '| DeepLab-CRF [[78](#bib.bib78)] | ResNet-101 | 79.7 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| DeepLab-CRF [[78](#bib.bib78)] | ResNet-101 | 79.7 |'
- en: '| GCN^∗ [[118](#bib.bib118)] | ResNet-152 | 82.2 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| GCN^∗ [[118](#bib.bib118)] | ResNet-152 | 82.2 |'
- en: '| RefineNet [[115](#bib.bib115)] | ResNet-152 | 84.2 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| RefineNet [[115](#bib.bib115)] | ResNet-152 | 84.2 |'
- en: '| Wide ResNet [[119](#bib.bib119)] | WideResNet-38 | 84.9 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Wide ResNet [[119](#bib.bib119)] | WideResNet-38 | 84.9 |'
- en: '| PSPNet [[56](#bib.bib56)] | ResNet-101 | 85.4 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| PSPNet [[56](#bib.bib56)] | ResNet-101 | 85.4 |'
- en: '| DeeplabV3 [[12](#bib.bib12)] | ResNet-101 | 85.7 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| DeeplabV3 [[12](#bib.bib12)] | ResNet-101 | 85.7 |'
- en: '| PSANet [[98](#bib.bib98)] | ResNet-101 | 85.7 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| PSANet [[98](#bib.bib98)] | ResNet-101 | 85.7 |'
- en: '| EncNet [[114](#bib.bib114)] | ResNet-101 | 85.9 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| EncNet [[114](#bib.bib114)] | ResNet-101 | 85.9 |'
- en: '| DFN^∗ [[99](#bib.bib99)] | ResNet-101 | 86.2 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| DFN^∗ [[99](#bib.bib99)] | ResNet-101 | 86.2 |'
- en: '| Exfuse [[120](#bib.bib120)] | ResNet-101 | 86.2 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Exfuse [[120](#bib.bib120)] | ResNet-101 | 86.2 |'
- en: '| SDN* [[45](#bib.bib45)] | DenseNet-161 | 86.6 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| SDN* [[45](#bib.bib45)] | DenseNet-161 | 86.6 |'
- en: '| DIS [[123](#bib.bib123)] | ResNet-101 | 86.8 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| DIS [[123](#bib.bib123)] | ResNet-101 | 86.8 |'
- en: '| DM-Net^∗ [[58](#bib.bib58)] | ResNet-101 | 87.06 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| DM-Net^∗ [[58](#bib.bib58)] | ResNet-101 | 87.06 |'
- en: '| APC-Net^∗ [[60](#bib.bib60)] | ResNet-101 | 87.1 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| APC-Net^∗ [[60](#bib.bib60)] | ResNet-101 | 87.1 |'
- en: '| EMANet [[95](#bib.bib95)] | ResNet-101 | 87.7 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| EMANet [[95](#bib.bib95)] | ResNet-101 | 87.7 |'
- en: '| DeeplabV3+ [[83](#bib.bib83)] | Xception-71 | 87.8 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DeeplabV3+ [[83](#bib.bib83)] | Xception-71 | 87.8 |'
- en: '| Exfuse [[120](#bib.bib120)] | ResNeXt-131 | 87.9 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Exfuse [[120](#bib.bib120)] | ResNeXt-131 | 87.9 |'
- en: '| MSCI [[61](#bib.bib61)] | ResNet-152 | 88.0 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| MSCI [[61](#bib.bib61)] | ResNet-152 | 88.0 |'
- en: '| EMANet [[95](#bib.bib95)] | ResNet-152 | 88.2 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| EMANet [[95](#bib.bib95)] | ResNet-152 | 88.2 |'
- en: '| DeeplabV3+^∗ [[83](#bib.bib83)] | Xception-71 | 89.0 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| DeeplabV3+^∗ [[83](#bib.bib83)] | Xception-71 | 89.0 |'
- en: '| EfficientNet+NAS-FPN [[135](#bib.bib135)] | - | 90.5 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet+NAS-FPN [[135](#bib.bib135)] | - | 90.5 |'
- en: 'The following tables summarize the performances of several of the prominent
    DL-based segmentation models on different datasets. Table [I](#S5.T1 "TABLE I
    ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review ‣ Image
    Segmentation Using Deep Learning: A Survey") focuses on the PASCAL VOC test set.
    Clearly, there has been much improvement in the accuracy of the models since the
    introduction of the FCN, the first DL-based image segmentation model. Table [II](#S5.T2
    "TABLE II ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review
    ‣ Image Segmentation Using Deep Learning: A Survey") focuses on the Cityscape
    test dataset. The latest models feature about 23% relative gain over the initial
    FCN model on this dataset. Table [III](#S5.T3 "TABLE III ‣ 5.2 Quantitative Performance
    of DL-Based Models ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning:
    A Survey") focuses on the MS COCO stuff test set. This dataset is more challenging
    than PASCAL VOC, and Cityescapes, as the highest mIoU is approximately 40%. Table [IV](#S5.T4
    "TABLE IV ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review
    ‣ Image Segmentation Using Deep Learning: A Survey") focuses on the ADE20k validation
    set. This dataset is also more challenging than the PASCAL VOC and Cityescapes
    datasets.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '以下表格总结了几种著名的基于DL的分割模型在不同数据集上的性能。表[I](#S5.T1 "TABLE I ‣ 5.2 Quantitative Performance
    of DL-Based Models ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning:
    A Survey")侧重于PASCAL VOC测试集。显然，自FCN首次推出以来，模型的准确性有了很大的提升，它是第一个基于DL的图像分割模型。表[II](#S5.T2
    "TABLE II ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review
    ‣ Image Segmentation Using Deep Learning: A Survey")关注Cityscape测试数据集。最新的模型在该数据集上相对初始FCN模型提高了约23%。表[III](#S5.T3
    "TABLE III ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review
    ‣ Image Segmentation Using Deep Learning: A Survey")关注了MS COCO stuff测试集。该数据集比PASCAL
    VOC和Cityescapes更具挑战性，最高的mIoU约为40%。表[IV](#S5.T4 "TABLE IV ‣ 5.2 Quantitative Performance
    of DL-Based Models ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning:
    A Survey")侧重于ADE20k验证集。该数据集也比PASCAL VOC和Cityescapes数据集更具挑战性。'
- en: 'Table [V](#S5.T5 "TABLE V ‣ 5.2 Quantitative Performance of DL-Based Models
    ‣ 5 Performance Review ‣ Image Segmentation Using Deep Learning: A Survey") provides
    the performance of prominent instance segmentation algorithms on COCO test-dev
    2017 dataset, in terms of average precision, and their speed. Table [VI](#S5.T6
    "TABLE VI ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review
    ‣ Image Segmentation Using Deep Learning: A Survey") provides the performance
    of prominent panoptic segmentation algorithms on MS-COCO val dataset, in terms
    of panoptic quality [[138](#bib.bib138)]. Finally, Table [VII](#S5.T7 "TABLE VII
    ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review ‣ Image
    Segmentation Using Deep Learning: A Survey") summarizes the performance of several
    prominent models for RGB-D segmentation on the NYUD-v2 and SUN-RGBD datasets.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '表[V](#S5.T5 "TABLE V ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5
    Performance Review ‣ Image Segmentation Using Deep Learning: A Survey")提供了COCO
    test-dev 2017数据集中突出的实例分割算法的性能，以平均精度和速度为指标。表[VI](#S5.T6 "TABLE VI ‣ 5.2 Quantitative
    Performance of DL-Based Models ‣ 5 Performance Review ‣ Image Segmentation Using
    Deep Learning: A Survey")提供了MS-COCO val数据集中突出的全景分割算法的性能，以全景质量为指标[[138](#bib.bib138)]。最后，表[VII](#S5.T7
    "TABLE VII ‣ 5.2 Quantitative Performance of DL-Based Models ‣ 5 Performance Review
    ‣ Image Segmentation Using Deep Learning: A Survey")总结了NYUD-v2和SUN-RGBD数据集上几个突出模型的RGB-D分割性能。'
- en: 'TABLE II: Accuracies of segmentation models on the Cityescapes dataset.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：Cityescapes数据集上分割模型的准确性。
- en: '| Method | Backbone | mIoU |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干 | mIoU |'
- en: '| --- | --- | --- |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FCN-8s [[31](#bib.bib31)] | - | 65.3 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| FCN-8s [[31](#bib.bib31)] | - | 65.3 |'
- en: '| DPN [[41](#bib.bib41)] | - | 66.8 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| DPN [[41](#bib.bib41)] | - | 66.8 |'
- en: '| Dilation10 [[79](#bib.bib79)] | - | 67.1 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| Dilation10 [[79](#bib.bib79)] | - | 67.1 |'
- en: '| DeeplabV2 [[78](#bib.bib78)] | ResNet-101 | 70.4 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| DeeplabV2 [[78](#bib.bib78)] | ResNet-101 | 70.4 |'
- en: '| RefineNet [[115](#bib.bib115)] | ResNet-101 | 73.6 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| RefineNet [[115](#bib.bib115)] | ResNet-101 | 73.6 |'
- en: '| FoveaNet [[124](#bib.bib124)] | ResNet-101 | 74.1 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| FoveaNet [[124](#bib.bib124)] | ResNet-101 | 74.1 |'
- en: '| Ladder DenseNet [[125](#bib.bib125)] | Ladder DenseNet-169 | 73.7 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 梯度密集网络 [[125](#bib.bib125)] | 梯度密集网络-169 | 73.7 |'
- en: '| GCN [[118](#bib.bib118)] | ResNet-101 | 76.9 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GCN [[118](#bib.bib118)] | ResNet-101 | 76.9 |'
- en: '| DUC-HDC [[80](#bib.bib80)] | ResNet-101 | 77.6 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| DUC-HDC [[80](#bib.bib80)] | ResNet-101 | 77.6 |'
- en: '| Wide ResNet [[119](#bib.bib119)] | WideResNet-38 | 78.4 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Wide ResNet [[119](#bib.bib119)] | WideResNet-38 | 78.4 |'
- en: '| PSPNet [[56](#bib.bib56)] | ResNet-101 | 85.4 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| PSPNet [[56](#bib.bib56)] | ResNet-101 | 85.4 |'
- en: '| BiSeNet [[126](#bib.bib126)] | ResNet-101 | 78.9 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| BiSeNet [[126](#bib.bib126)] | ResNet-101 | 78.9 |'
- en: '| DFN [[99](#bib.bib99)] | ResNet-101 | 79.3 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| DFN [[99](#bib.bib99)] | ResNet-101 | 79.3 |'
- en: '| PSANet [[98](#bib.bib98)] | ResNet-101 | 80.1 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| PSANet [[98](#bib.bib98)] | ResNet-101 | 80.1 |'
- en: '| DenseASPP [[81](#bib.bib81)] | DenseNet-161 | 80.6 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| DenseASPP [[81](#bib.bib81)] | DenseNet-161 | 80.6 |'
- en: '| SPGNet [[127](#bib.bib127)] | 2xResNet-50 | 81.1 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| SPGNet [[127](#bib.bib127)] | 2xResNet-50 | 81.1 |'
- en: '| DANet [[93](#bib.bib93)] | ResNet-101 | 81.5 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| DANet [[93](#bib.bib93)] | ResNet-101 | 81.5 |'
- en: '| CCNet [[96](#bib.bib96)] | ResNet-101 | 81.4 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| CCNet [[96](#bib.bib96)] | ResNet-101 | 81.4 |'
- en: '| DeeplabV3 [[12](#bib.bib12)] | ResNet-101 | 81.3 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| DeeplabV3 [[12](#bib.bib12)] | ResNet-101 | 81.3 |'
- en: '| AC-Net [[129](#bib.bib129)] | ResNet-101 | 82.3 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| AC-Net [[129](#bib.bib129)] | ResNet-101 | 82.3 |'
- en: '| OCR [[44](#bib.bib44)] | ResNet-101 | 82.4 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| OCR [[44](#bib.bib44)] | ResNet-101 | 82.4 |'
- en: '| GS-CNN [[128](#bib.bib128)] | WideResNet | 82.8 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| GS-CNN [[128](#bib.bib128)] | WideResNet | 82.8 |'
- en: '| HRNetV2+OCR (w/ASPP) [[44](#bib.bib44)] | HRNetV2-W48 | 83.7 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| HRNetV2+OCR (w/ASPP) [[44](#bib.bib44)] | HRNetV2-W48 | 83.7 |'
- en: '| Hierarchical MSA [[137](#bib.bib137)] | HRNet-OCR | 85.1 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Hierarchical MSA [[137](#bib.bib137)] | HRNet-OCR | 85.1 |'
- en: 'TABLE III: Accuracies of segmentation models on the MS COCO stuff dataset.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：MS COCO stuff 数据集上的分割模型准确率
- en: '| Method | Backbone | mIoU |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | mIoU |'
- en: '| --- | --- | --- |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RefineNet [[115](#bib.bib115)] | ResNet-101 | 33.6 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| RefineNet [[115](#bib.bib115)] | ResNet-101 | 33.6 |'
- en: '| CCN [[59](#bib.bib59)] | Ladder DenseNet-101 | 35.7 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| CCN [[59](#bib.bib59)] | 梯度密集网络-101 | 35.7 |'
- en: '| DANet [[93](#bib.bib93)] | ResNet-50 | 37.9 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| DANet [[93](#bib.bib93)] | ResNet-50 | 37.9 |'
- en: '| DSSPN [[130](#bib.bib130)] | ResNet-101 | 37.3 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| DSSPN [[130](#bib.bib130)] | ResNet-101 | 37.3 |'
- en: '| EMA-Net [[95](#bib.bib95)] | ResNet-50 | 37.5 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| EMA-Net [[95](#bib.bib95)] | ResNet-50 | 37.5 |'
- en: '| SGR [[131](#bib.bib131)] | ResNet-101 | 39.1 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| SGR [[131](#bib.bib131)] | ResNet-101 | 39.1 |'
- en: '| OCR [[44](#bib.bib44)] | ResNet-101 | 39.5 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| OCR [[44](#bib.bib44)] | ResNet-101 | 39.5 |'
- en: '| DANet [[93](#bib.bib93)] | ResNet-101 | 39.7 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| DANet [[93](#bib.bib93)] | ResNet-101 | 39.7 |'
- en: '| EMA-Net [[95](#bib.bib95)] | ResNet-50 | 39.9 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| EMA-Net [[95](#bib.bib95)] | ResNet-50 | 39.9 |'
- en: '| AC-Net [[129](#bib.bib129)] | ResNet-101 | 40.1 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| AC-Net [[129](#bib.bib129)] | ResNet-101 | 40.1 |'
- en: '| OCR [[44](#bib.bib44)] | HRNetV2-W48 | 40.5 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| OCR [[44](#bib.bib44)] | HRNetV2-W48 | 40.5 |'
- en: 'TABLE IV: Accuracies of segmentation models on the ADE20k validation dataset.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：ADE20k 验证数据集上的分割模型准确率
- en: '| Method | Backbone | mIoU |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | mIoU |'
- en: '| --- | --- | --- |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| FCN [[31](#bib.bib31)] | - | 29.39 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| FCN [[31](#bib.bib31)] | - | 29.39 |'
- en: '| DilatedNet [[79](#bib.bib79)] | - | 32.31 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| DilatedNet [[79](#bib.bib79)] | - | 32.31 |'
- en: '| CascadeNet [[132](#bib.bib132)] | - | 34.9 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| CascadeNet [[132](#bib.bib132)] | - | 34.9 |'
- en: '| RefineNet [[115](#bib.bib115)] | ResNet-152 | 40.7 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| RefineNet [[115](#bib.bib115)] | ResNet-152 | 40.7 |'
- en: '| PSPNet [[56](#bib.bib56)] | ResNet-101 | 43.29 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| PSPNet [[56](#bib.bib56)] | ResNet-101 | 43.29 |'
- en: '| PSPNet [[56](#bib.bib56)] | ResNet-269 | 44.94 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| PSPNet [[56](#bib.bib56)] | ResNet-269 | 44.94 |'
- en: '| EncNet [[114](#bib.bib114)] | ResNet-101 | 44.64 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| EncNet [[114](#bib.bib114)] | ResNet-101 | 44.64 |'
- en: '| SAC [[133](#bib.bib133)] | ResNet-101 | 44.3 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| SAC [[133](#bib.bib133)] | ResNet-101 | 44.3 |'
- en: '| PSANet [[98](#bib.bib98)] | ResNet-101 | 43.7 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| PSANet [[98](#bib.bib98)] | ResNet-101 | 43.7 |'
- en: '| UperNet [[134](#bib.bib134)] | ResNet-101 | 42.66 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| UperNet [[134](#bib.bib134)] | ResNet-101 | 42.66 |'
- en: '| DSSPN [[130](#bib.bib130)] | ResNet-101 | 43.68 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| DSSPN [[130](#bib.bib130)] | ResNet-101 | 43.68 |'
- en: '| DM-Net [[58](#bib.bib58)] | ResNet-101 | 45.5 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| DM-Net [[58](#bib.bib58)] | ResNet-101 | 45.5 |'
- en: '| AC-Net [[129](#bib.bib129)] | ResNet-101 | 45.9 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| AC-Net [[129](#bib.bib129)] | ResNet-101 | 45.9 |'
- en: 'TABLE V: Instance Segmentation Models Performance on COCO test-dev 2017'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：COCO test-dev 2017 实例分割模型性能
- en: '| Method | Backbone | FPS | AP |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | FPS | AP |'
- en: '| --- | --- | --- | --- |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| YOLACT-550 [[76](#bib.bib76)] | R-101-FPN | 33.5 | 29.8 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| YOLACT-550 [[76](#bib.bib76)] | R-101-FPN | 33.5 | 29.8 |'
- en: '| YOLACT-700 [[76](#bib.bib76)] | R-101-FPN | 23.8 | 31.2 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| YOLACT-700 [[76](#bib.bib76)] | R-101-FPN | 23.8 | 31.2 |'
- en: '| RetinaMask [[170](#bib.bib170)] | R-101-FPN | 10.2 | 34.7 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| RetinaMask [[170](#bib.bib170)] | R-101-FPN | 10.2 | 34.7 |'
- en: '| TensorMask [[69](#bib.bib69)] | R-101-FPN | 2.6 | 37.1 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| TensorMask [[69](#bib.bib69)] | R-101-FPN | 2.6 | 37.1 |'
- en: '| SharpMask [[171](#bib.bib171)] | R-101-FPN | 8.0 | 37.4 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| SharpMask [[171](#bib.bib171)] | R-101-FPN | 8.0 | 37.4 |'
- en: '| Mask-RCNN [[64](#bib.bib64)] | R-101-FPN | 10.6 | 37.9 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| Mask-RCNN [[64](#bib.bib64)] | R-101-FPN | 10.6 | 37.9 |'
- en: '| CenterMask [[74](#bib.bib74)] | R-101-FPN | 13.2 | 38.3 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| CenterMask [[74](#bib.bib74)] | R-101-FPN | 13.2 | 38.3 |'
- en: 'TABLE VI: Panoptic Segmentation Models Performance on the MS-COCO val dataset.
    $*$ denotes use of deformable convolution.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：MS-COCO val 数据集上的全景分割模型性能。$*$ 表示使用了可变形卷积。
- en: '| Method | Backbone | PQ |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | PQ |'
- en: '| --- | --- | --- |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Panoptic FPN [[139](#bib.bib139)] | ResNet-50 | 39.0 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Panoptic FPN [[139](#bib.bib139)] | ResNet-50 | 39.0 |'
- en: '| Panoptic FPN [[139](#bib.bib139)] | ResNet-101 | 40.3 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| Panoptic FPN [[139](#bib.bib139)] | ResNet-101 | 40.3 |'
- en: '| AU-Net [[140](#bib.bib140)] | ResNet-50 | 39.6 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| AU-Net [[140](#bib.bib140)] | ResNet-50 | 39.6 |'
- en: '| Panoptic-DeepLab [[142](#bib.bib142)] | Xception-71 | 39.7 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| Panoptic-DeepLab [[142](#bib.bib142)] | Xception-71 | 39.7 |'
- en: '| OANet [[172](#bib.bib172)] | ResNet-50 | 39.0 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| OANet [[172](#bib.bib172)] | ResNet-50 | 39.0 |'
- en: '| OANet [[172](#bib.bib172)] | ResNet-101 | 40.7 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| OANet [[172](#bib.bib172)] | ResNet-101 | 40.7 |'
- en: '| AdaptIS [[173](#bib.bib173)] | ResNet-50 | 35.9 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| AdaptIS [[173](#bib.bib173)] | ResNet-50 | 35.9 |'
- en: '| AdaptIS [[173](#bib.bib173)] | ResNet-101 | 37.0 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| AdaptIS [[173](#bib.bib173)] | ResNet-101 | 37.0 |'
- en: '| UPSNet^∗ [[143](#bib.bib143)] | ResNet-50 | 42.5 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| UPSNet^∗ [[143](#bib.bib143)] | ResNet-50 | 42.5 |'
- en: '| OCFusion^∗ [[174](#bib.bib174)] | ResNet-50 | 41.3 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| OCFusion^∗ [[174](#bib.bib174)] | ResNet-50 | 41.3 |'
- en: '| OCFusion^∗ [[174](#bib.bib174)] | ResNet-101 | 43.0 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| OCFusion^∗ [[174](#bib.bib174)] | ResNet-101 | 43.0 |'
- en: '| OCFusion^∗ [[174](#bib.bib174)] | ResNeXt-101 | 45.7 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| OCFusion^∗ [[174](#bib.bib174)] | ResNeXt-101 | 45.7 |'
- en: 'TABLE VII: Performance of segmentation models on the NYUD-v2, and SUN-RGBD
    datasets, in terms of mIoU, and mean Accuracy (mAcc).'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：NYUD-v2 和 SUN-RGBD 数据集上的分割模型性能，按 mIoU 和平均准确度 (mAcc) 进行评估。
- en: '|  | NYUD-v2 | SUN-RGBD |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | NYUD-v2 | SUN-RGBD |'
- en: '| --- | --- | --- |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | m-Acc | m-IoU | m-Acc | m-IoU |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | m-Acc | m-IoU | m-Acc | m-IoU |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Mutex [[175](#bib.bib175)] | - | 31.5 | - | - |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Mutex [[175](#bib.bib175)] | - | 31.5 | - | - |'
- en: '| MS-CNN [[176](#bib.bib176)] | 45.1 | 34.1 | - | - |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| MS-CNN [[176](#bib.bib176)] | 45.1 | 34.1 | - | - |'
- en: '| FCN [[31](#bib.bib31)] | 46.1 | 34.0 | - | - |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| FCN [[31](#bib.bib31)] | 46.1 | 34.0 | - | - |'
- en: '| Joint-Seg [[177](#bib.bib177)] | 52.3 | 39.2 | - | - |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Joint-Seg [[177](#bib.bib177)] | 52.3 | 39.2 | - | - |'
- en: '| SegNet [[15](#bib.bib15)] | - | - | 44.76 | 31.84 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| SegNet [[15](#bib.bib15)] | - | - | 44.76 | 31.84 |'
- en: '| Structured Net [[40](#bib.bib40)] | 53.6 | 40.6 | 53.4 | 42.3 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Structured Net [[40](#bib.bib40)] | 53.6 | 40.6 | 53.4 | 42.3 |'
- en: '| B-SegNet [[43](#bib.bib43)] | - | - | 45.9 | 30.7 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| B-SegNet [[43](#bib.bib43)] | - | - | 45.9 | 30.7 |'
- en: '| 3D-GNN [[178](#bib.bib178)] | 55.7 | 43.1 | 57.0 | 45.9 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 3D-GNN [[178](#bib.bib178)] | 55.7 | 43.1 | 57.0 | 45.9 |'
- en: '| LSD-Net [[48](#bib.bib48)] | 60.7 | 45.9 | 58.0 | - |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| LSD-Net [[48](#bib.bib48)] | 60.7 | 45.9 | 58.0 | - |'
- en: '| RefineNet [[115](#bib.bib115)] | 58.9 | 46.5 | 58.5 | 45.9 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| RefineNet [[115](#bib.bib115)] | 58.9 | 46.5 | 58.5 | 45.9 |'
- en: '| D-aware CNN [[179](#bib.bib179)] | 61.1 | 48.4 | 53.5 | 42.0 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| D-aware CNN [[179](#bib.bib179)] | 61.1 | 48.4 | 53.5 | 42.0 |'
- en: '| RDFNet [[180](#bib.bib180)] | 62.8 | 50.1 | 60.1 | 47.7 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| RDFNet [[180](#bib.bib180)] | 62.8 | 50.1 | 60.1 | 47.7 |'
- en: '| G-Aware Net [[181](#bib.bib181)] | 68.7 | 59.6 | 74.9 | 54.5 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| G-Aware Net [[181](#bib.bib181)] | 68.7 | 59.6 | 74.9 | 54.5 |'
- en: '| MTI-Net [[181](#bib.bib181)] | 68.7 | 59.6 | 74.9 | 54.5 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| MTI-Net [[181](#bib.bib181)] | 68.7 | 59.6 | 74.9 | 54.5 |'
- en: To summarize the tabulated data, there has been significant progress in the
    performance of deep segmentation models over the past 5–6 years, with a relative
    improvement of 25%-42% in mIoU on different datasets. However, some publications
    suffer from lack of reproducibility for multiple reasons—they report performance
    on non-standard benchmarks/databases, or they report performance only on arbitrary
    subsets of the test set from a popular benchmark, or they do not adequately describe
    the experimental setup and sometimes evaluate the model performance only on a
    subset of object classes. Most importantly, many publications do not provide the
    source-code for their model implementations. However, with the increasing popularity
    of deep learning models, the trend has been positive and many research groups
    are moving toward reproducible frameworks and open-sourcing their implementations.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 总结表格数据，过去 5-6 年深度分割模型的性能有了显著进展，在不同数据集上的 mIoU 相对提高了 25%-42%。然而，一些出版物因多种原因缺乏可重复性——它们报告了非标准基准/数据库上的性能，或仅报告了流行基准测试集的任意子集上的性能，或未充分描述实验设置，有时仅在部分目标类别上评估模型性能。最重要的是，许多出版物没有提供其模型实现的源代码。然而，随着深度学习模型的日益流行，趋势已变得积极，许多研究小组正在转向可重复的框架并开源他们的实现。
- en: 6 Challenges and Opportunities
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个挑战和机遇
- en: There is not doubt that image segmentation has benefited greatly from deep learning,
    but several challenges lie ahead. We will next introduce some of the promising
    research directions that we believe will help in further advancing image segmentation
    algorithms.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，图像分割从深度学习中受益匪浅，但仍面临若干挑战。我们将接下来介绍一些有前景的研究方向，我们相信这些方向将有助于进一步推动图像分割算法的发展。
- en: 6.1 More Challenging Datasets
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 更具挑战性的数据集
- en: Several large-scale image datasets have been created for semantic segmentation
    and instance segmentation. However, there remains a need for more challenging
    datasets, as well as datasets for different kinds of images. For still images,
    datasets with a large number of objects and overlapping objects would be very
    valuable. This can enable training models that are better at handling dense object
    scenarios, as well as large overlaps among objects as is common in real-world
    scenarios.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 已经创建了多个大规模图像数据集用于语义分割和实例分割。然而，仍然需要更具挑战性的数据集，以及用于不同类型图像的数据集。对于静态图像，具有大量对象和重叠对象的数据集将非常有价值。这可以使训练模型更好地处理密集对象场景以及现实世界中常见的大面积重叠。
- en: With the rising popularity of 3D image segmentation, especially in medical image
    analysis, there is also a strong need for large-scale 3D images datasets. These
    datasets are more difficult to create than their lower dimensional counterparts.
    Existing datasets for 3D image segmentation available are typically not large
    enough, and some are synthetic, and therefore larger and more challenging 3D image
    datasets can be very valuable.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 3D 图像分割的日益流行，特别是在医学图像分析中，对大规模 3D 图像数据集的需求也越来越强烈。这些数据集的创建比其低维对应物更为困难。现有的 3D
    图像分割数据集通常不够大，有些是合成的，因此更大、更具挑战性的 3D 图像数据集可能非常宝贵。
- en: 6.2 Interpretable Deep Models
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 可解释的深度模型
- en: While DL-based models have achieved promising performance on challenging benchmarks,
    there remain open questions about these models. For example, what exactly are
    deep models learning? How should we interpret the features learned by these models?
    What is a minimal neural architecture that can achieve a certain segmentation
    accuracy on a given dataset? Although some techniques are available to visualize
    the learned convolutional kernels of these models, a concrete study of the underlying
    behavior/dynamics of these models is lacking. A better understanding of the theoretical
    aspects of these models can enable the development of better models curated toward
    various segmentation scenarios.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于深度学习的模型在挑战性基准测试上取得了有希望的表现，但这些模型仍存在开放性问题。例如，深度模型到底在学习什么？我们应如何解释这些模型学到的特征？在给定数据集上实现某种分割准确性的最小神经网络架构是什么？尽管有一些技术可以可视化这些模型学到的卷积核，但对这些模型的基本行为/动态的具体研究仍然不足。更好地理解这些模型的理论方面可以促使更好的模型发展，针对各种分割场景进行优化。
- en: 6.3 Weakly-Supervised and Unsupervised Learning
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 弱监督和无监督学习
- en: Weakly-supervised (a.k.a. few shot learning) [[182](#bib.bib182)] and unsupervised
    learning [[183](#bib.bib183)] are becoming very active research areas. These techniques
    promise to be specially valuable for image segmentation, as collecting labeled
    samples for segmentation problem is problematic in many application domains, particularly
    so in medical image analysis. The transfer learning approach is to train a generic
    image segmentation model on a large set of labeled samples (perhaps from a public
    benchmark), and then fine-tune that model on a few samples from some specific
    target application. Self-supervised learning is another promising direction that
    is attracting much attraction in various fields. There are many details in images
    that that can be captured to train a segmentation models with far fewer training
    samples, with the help of self-supervised learning. Models based on reinforcement
    learning could also be another potential future direction, as they have scarcely
    received attention for image segmentation. For example, MOREL [[184](#bib.bib184)]
    introduced a deep reinforcement learning approach for moving object segmentation
    in videos.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督（也称为少样本学习）[[182](#bib.bib182)]和无监督学习[[183](#bib.bib183)]正成为非常活跃的研究领域。这些技术对图像分割特别有价值，因为在许多应用领域中，收集用于分割问题的标记样本是有问题的，特别是在医学图像分析中。迁移学习的方法是先在大量标记样本（可能来自公共基准）上训练一个通用图像分割模型，然后在来自某些特定目标应用的少量样本上对该模型进行微调。自监督学习是另一个前景广阔的方向，在各个领域都受到广泛关注。通过自监督学习，可以利用图像中的许多细节来训练分割模型，从而使用更少的训练样本。基于强化学习的模型也可能是未来的潜在方向，因为它们在图像分割中几乎没有受到关注。例如，MOREL
    [[184](#bib.bib184)] 引入了一种用于视频中移动物体分割的深度强化学习方法。
- en: 6.4 Real-time Models for Various Applications
  id: totrans-406
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 各种应用的实时模型
- en: In many applications, accuracy is the most important factor; however, there
    are applications in which it is also critical to have segmentation models that
    can run in near real-time, or at least near common camera frame rates (at least
    25 frames per second). This is useful for computer vision systems that are, for
    example, deployed in autonomous vehicles. Most of the current models are far from
    this frame-rate; e.g., FCN-8 takes roughly 100 ms to process a low-resolution
    image. Models based on dilated convolution help to increase the speed of segmentation
    models to some extent, but there is still plenty of room for improvement.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，准确性是最重要的因素；然而，有些应用中，拥有能够接近实时运行的分割模型或至少接近常见相机帧率（至少每秒25帧）也是至关重要的。这对例如部署在自动驾驶汽车中的计算机视觉系统非常有用。目前的大多数模型远未达到这一帧率；例如，FCN-8处理低分辨率图像大约需要100毫秒。基于膨胀卷积的模型在一定程度上有助于提高分割模型的速度，但仍有很大的改进空间。
- en: 6.5 Memory Efficient Models
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 内存高效模型
- en: Many modern segmentation models require a significant amount of memory even
    during the inference stage. So far, much effort has been directed towards improving
    the accuracy of such models, but in order to fit them into specific devices, such
    as mobile phones, the networks must be simplified. This can be done either by
    using simpler models, or by using model compression techniques, or even training
    a complex model and then using knowledge distillation techniques to compress it
    into a smaller, memory efficient network that mimics the complex model.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现代分割模型即使在推理阶段也需要大量内存。迄今为止，大部分努力都集中在提高这些模型的准确性上，但为了将它们适配到特定设备，如手机，网络必须简化。这可以通过使用更简单的模型、模型压缩技术，甚至训练一个复杂的模型然后使用知识蒸馏技术将其压缩成一个更小的、内存高效的网络来模仿复杂模型。
- en: 6.6 3D Point-Cloud Segmentation
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 3D 点云分割
- en: Numerous works have focused on 2D image segmentation, but much fewer have addressed
    3D point-cloud segmentation. However, there has been an increasing interest in
    point-cloud segmentation, which has a wide range of applications, in 3D modeling,
    self-driving cars, robotics, building modeling, etc. Dealing with 3D unordered
    and unstructured data such as point clouds poses several challenges. For example,
    the best way to apply CNNs and other classical deep learning architectures on
    point clouds is unclear. Graph-based deep models can be a potential area of exploration
    for point-cloud segmentation, enabling additional industrial applications of these
    data.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 众多研究集中于2D图像分割，但3D点云分割关注较少。然而，对点云分割的兴趣日益增加，它在3D建模、自动驾驶汽车、机器人技术、建筑建模等领域有广泛的应用。处理3D无序和非结构化数据（如点云）面临诸多挑战。例如，如何将CNN及其他经典深度学习架构应用于点云尚不明确。基于图的深度模型可能成为点云分割的潜在探索领域，促进这些数据的额外工业应用。
- en: 6.7 Application Scenarios
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 应用场景
- en: In this section, we briefly investigate some application scenarios of recent
    DL-based segmentation methods, and some challenges ahead. Most notably, these
    methods have been successfully applied to segment satellite images in the field
    of remote sensing [[185](#bib.bib185)], including techniques for urban planning
    [[186](#bib.bib186)] or precision agriculture [[187](#bib.bib187)]. Remote sensing
    images collected by airborne platforms [[188](#bib.bib188)] and drones [[189](#bib.bib189)]
    have also been segmented using DL-based techniques, offering the opportunity to
    address important environmental problems such as those involving climate change.
    The main challenges of segmenting this kind of images are related to the very
    large dimensionality of the data (often collected by imaging spectrometers with
    hundreds or even thousands of spectral bands) and the limited ground-truth information
    to evaluate the accuracy of the results obtained by segmentation algorithms. Another
    very important application field for DL-based segmentation has been medical imaging
    [[190](#bib.bib190)]. Here, an opportunity is to design standardized image databases
    that can be used to evaluate fast spreading new diseases and pandemics. Last but
    not least, we should also mention DL-based segmentation techniques in biology
    [[191](#bib.bib191)] and evaluation of construction materials [[192](#bib.bib192)],
    which offer the opportunity to address highly relevant application domains but
    are subject to challenges related to the massive volume of the related image data
    and the limited reference information for validation purposes.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们简要探讨了近期基于深度学习的分割方法的一些应用场景及面临的一些挑战。尤其是，这些方法已成功应用于遥感领域的卫星图像分割[[185](#bib.bib185)]，包括城市规划[[186](#bib.bib186)]或精准农业[[187](#bib.bib187)]的技术。通过航空平台[[188](#bib.bib188)]和无人机[[189](#bib.bib189)]收集的遥感图像也已采用深度学习技术进行分割，提供了解决气候变化等重要环境问题的机会。分割这类图像的主要挑战与数据的极大维度（通常由成像光谱仪收集，具有数百甚至数千个光谱波段）以及用于评估分割算法结果准确性的有限真实信息有关。另一个非常重要的深度学习分割应用领域是医学影像[[190](#bib.bib190)]。在这里，一个机会是设计标准化的图像数据库，以评估新兴传染病和大流行病的快速传播。最后但同样重要的是，我们还应提到深度学习分割技术在生物学[[191](#bib.bib191)]和建筑材料评估[[192](#bib.bib192)]中的应用，它们提供了解决高度相关应用领域的机会，但也面临与相关图像数据的大量体积和验证目的的有限参考信息有关的挑战。
- en: 7 Conclusions
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'We have surveyed more than 100 recent image segmentation algorithms based on
    deep learning models, which have achieved impressive performance in various image
    segmentation tasks and benchmarks, grouped into ten categories such as: CNN and
    FCN, RNN, R-CNN, dilated CNN, attention-based models, generative and adversarial
    models, among others. We summarized quantitative performance analyses of these
    models on some popular benchmarks, such as the PASCAL VOC, MS COCO, Cityscapes,
    and ADE20k datasets. Finally, we discussed some of the open challenges and potential
    research directions for image segmentation that could be pursued in the coming
    years.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了100多种基于深度学习模型的最新图像分割算法，这些算法在各种图像分割任务和基准测试中表现出色，分为十个类别，如：CNN和FCN、RNN、R-CNN、膨胀CNN、基于注意力的模型、生成对抗模型等。我们总结了这些模型在一些流行基准测试上的定量性能分析，如PASCAL
    VOC、MS COCO、Cityscapes和ADE20k数据集。最后，我们讨论了一些开放挑战和未来几年可以研究的图像分割方向。
- en: Acknowledgments
  id: totrans-416
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank Tsung-Yi Lin from Google Brain, and Jingdong
    Wang and Yuhui Yuan from Microsoft Research Asia, for reviewing this work, and
    providing very helpful comments and suggestions.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢 Google Brain 的 Tsung-Yi Lin，以及 Microsoft Research Asia 的 Jingdong Wang
    和 Yuhui Yuan，感谢他们审阅此工作并提供了非常有用的评论和建议。
- en: References
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] R. Szeliski, *Computer vision: algorithms and applications*.   Springer
    Science & Business Media, 2010.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] R. Szeliski，*计算机视觉：算法与应用*，Springer 科学与商业媒体，2010 年。'
- en: '[2] D. Forsyth and J. Ponce, *Computer vision: a modern approach*.   Prentice
    Hall Professional Technical Reference, 2002.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] D. Forsyth 和 J. Ponce，*计算机视觉：一种现代方法*，Prentice Hall 专业技术参考，2002 年。'
- en: '[3] N. Otsu, “A threshold selection method from gray-level histograms,” *IEEE
    transactions on systems, man, and cybernetics*, vol. 9, no. 1, pp. 62–66, 1979.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] N. Otsu，“一种基于灰度级直方图的阈值选择方法”，*IEEE 系统、男人与控制论学报*，第 9 卷，第 1 期，页 62–66，1979
    年。'
- en: '[4] R. Nock and F. Nielsen, “Statistical region merging,” *IEEE Transactions
    on pattern analysis and machine intelligence*, vol. 26, no. 11, pp. 1452–1458,
    2004.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Nock 和 F. Nielsen，“统计区域合并”，*IEEE 模式分析与机器智能学报*，第 26 卷，第 11 期，页 1452–1458，2004
    年。'
- en: '[5] N. Dhanachandra, K. Manglem, and Y. J. Chanu, “Image segmentation using
    k-means clustering algorithm and subtractive clustering algorithm,” *Procedia
    Computer Science*, vol. 54, pp. 764–771, 2015.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] N. Dhanachandra, K. Manglem 和 Y. J. Chanu，“使用 k-means 聚类算法和减法聚类算法的图像分割”，*Procedia
    计算机科学*，第 54 卷，页 764–771，2015 年。'
- en: '[6] L. Najman and M. Schmitt, “Watershed of a continuous function,” *Signal
    Processing*, vol. 38, no. 1, pp. 99–112, 1994.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Najman 和 M. Schmitt，“连续函数的分水岭”，*信号处理*，第 38 卷，第 1 期，页 99–112，1994 年。'
- en: '[7] M. Kass, A. Witkin, and D. Terzopoulos, “Snakes: Active contour models,”
    *International journal of computer vision*, vol. 1, no. 4, pp. 321–331, 1988.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Kass, A. Witkin 和 D. Terzopoulos，“蛇形：活动轮廓模型”，*国际计算机视觉杂志*，第 1 卷，第 4 期，页
    321–331，1988 年。'
- en: '[8] Y. Boykov, O. Veksler, and R. Zabih, “Fast approximate energy minimization
    via graph cuts,” *IEEE Transactions on pattern analysis and machine intelligence*,
    vol. 23, no. 11, pp. 1222–1239, 2001.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Boykov, O. Veksler 和 R. Zabih，“通过图割进行快速近似能量最小化”，*IEEE 模式分析与机器智能学报*，第
    23 卷，第 11 期，页 1222–1239，2001 年。'
- en: '[9] N. Plath, M. Toussaint, and S. Nakajima, “Multi-class image segmentation
    using conditional random fields and global classification,” in *Proceedings of
    the 26th Annual International Conference on Machine Learning*.   ACM, 2009, pp.
    817–824.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] N. Plath, M. Toussaint 和 S. Nakajima，“利用条件随机场和全局分类的多类图像分割”，见 *第 26 届年度国际机器学习大会论文集*，ACM，2009
    年，页 817–824。'
- en: '[10] J.-L. Starck, M. Elad, and D. L. Donoho, “Image decomposition via the
    combination of sparse representations and a variational approach,” *IEEE transactions
    on image processing*, vol. 14, no. 10, pp. 1570–1582, 2005.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J.-L. Starck, M. Elad 和 D. L. Donoho，“通过稀疏表示与变分方法结合的图像分解”，*IEEE 图像处理学报*，第
    14 卷，第 10 期，页 1570–1582，2005 年。'
- en: '[11] S. Minaee and Y. Wang, “An admm approach to masked signal decomposition
    using subspace representation,” *IEEE Transactions on Image Processing*, vol. 28,
    no. 7, pp. 3192–3204, 2019.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Minaee 和 Y. Wang，“一种基于子空间表示的掩蔽信号分解的 ADMM 方法”，*IEEE 图像处理学报*，第 28 卷，第
    7 期，页 3192–3204，2019 年。'
- en: '[12] L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous
    convolution for semantic image segmentation,” *arXiv preprint arXiv:1706.05587*,
    2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] L.-C. Chen, G. Papandreou, F. Schroff 和 H. Adam，“重新思考用于语义图像分割的空洞卷积”，*arXiv
    预印本 arXiv:1706.05587*，2017 年。'
- en: '[13] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner *et al.*, “Gradient-based learning
    applied to document recognition,” *Proceedings of the IEEE*, vol. 86, no. 11,
    pp. 2278–2324, 1998.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. LeCun, L. Bottou, Y. Bengio, P. Haffner *等*，“应用梯度学习于文档识别”，*IEEE 学报*，第
    86 卷，第 11 期，页 2278–2324，1998 年。'
- en: '[14] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. Hochreiter 和 J. Schmidhuber，“长短期记忆”，*神经计算*，第 9 卷，第 8 期，页 1735–1780，1997
    年。'
- en: '[15] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] V. Badrinarayanan, A. Kendall 和 R. Cipolla，“Segnet: 一种用于图像分割的深度卷积编码-解码器架构”，*IEEE
    模式分析与机器智能学报*，第 39 卷，第 12 期，页 2481–2495，2017 年。'
- en: '[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio, “生成对抗网络，” 见于 *神经信息处理系统进展*，2014年，第2672–2680页。'
- en: '[17] K. Fukushima, “Neocognitron: A self-organizing neural network model for
    a mechanism of pattern recognition unaffected by shift in position,” *Biological
    cybernetics*, vol. 36, no. 4, pp. 193–202, 1980.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] K. Fukushima, “Neocognitron: 一种自组织神经网络模型，用于不受位置偏移影响的模式识别机制，” *生物控制论*，第36卷，第4期，第193–202页，1980年。'
- en: '[18] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang, “Phoneme
    recognition using time-delay neural networks,” *IEEE transactions on acoustics,
    speech, and signal processing*, vol. 37, no. 3, pp. 328–339, 1989.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano 和 K. J. Lang, “使用时间延迟神经网络的音素识别，”
    *IEEE声学、语音与信号处理学报*，第37卷，第3期，第328–339页，1989年。'
- en: '[19] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in neural information processing
    systems*, 2012, pp. 1097–1105.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Krizhevsky, I. Sutskever 和 G. E. Hinton, “使用深度卷积神经网络的ImageNet分类，” 见于
    *神经信息处理系统进展*，2012年，第1097–1105页。'
- en: '[20] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深卷积网络，” *arXiv预印本 arXiv:1409.1556*，2014年。'
- en: '[21] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] K. He, X. Zhang, S. Ren 和 J. Sun, “用于图像识别的深度残差学习，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2016年，第770–778页。'
- en: '[22] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1–9.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke 和 A. Rabinovich, “更深的卷积网络，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第1–9页。'
- en: '[23] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” *arXiv preprint arXiv:1704.04861*, 2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto 和 H. Adam, “Mobilenets: 用于移动视觉应用的高效卷积神经网络，” *arXiv预印本 arXiv:1704.04861*，2017年。'
- en: '[24] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2017, pp. 4700–4708.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] G. Huang, Z. Liu, L. Van Der Maaten 和 K. Q. Weinberger, “密集连接卷积网络，” 见于
    *IEEE计算机视觉与模式识别会议论文集*，2017年，第4700–4708页。'
- en: '[25] D. E. Rumelhart, G. E. Hinton, R. J. Williams *et al.*, “Learning representations
    by back-propagating errors,” *Cognitive modeling*, vol. 5, no. 3, p. 1, 1988.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] D. E. Rumelhart, G. E. Hinton, R. J. Williams *等*，“通过反向传播错误学习表示，” *认知建模*，第5卷，第3期，第1页，1988年。'
- en: '[26] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*.   MIT press,
    2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] I. Goodfellow, Y. Bengio 和 A. Courville, *深度学习*。 MIT出版社，2016年。'
- en: '[27] A. Radford, L. Metz, and S. Chintala, “Unsupervised representation learning
    with deep convolutional generative adversarial networks,” *arXiv preprint arXiv:1511.06434*,
    2015.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Radford, L. Metz 和 S. Chintala, “使用深度卷积生成对抗网络的无监督表示学习，” *arXiv预印本 arXiv:1511.06434*，2015年。'
- en: '[28] M. Mirza and S. Osindero, “Conditional generative adversarial nets,” *arXiv
    preprint arXiv:1411.1784*, 2014.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. Mirza 和 S. Osindero, “条件生成对抗网络，” *arXiv预印本 arXiv:1411.1784*，2014年。'
- en: '[29] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” *arXiv preprint
    arXiv:1701.07875*, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Arjovsky, S. Chintala 和 L. Bottou, “Wasserstein gan，” *arXiv预印本 arXiv:1701.07875*，2017年。'
- en: '[30] [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo).'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)。'
- en: '[31] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for
    semantic segmentation,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2015, pp. 3431–3440.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Long, E. Shelhamer 和 T. Darrell, “用于语义分割的全卷积网络，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第3431–3440页。'
- en: '[32] W. Liu, A. Rabinovich, and A. C. Berg, “Parsenet: Looking wider to see
    better,” *arXiv preprint arXiv:1506.04579*, 2015.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] W. Liu, A. Rabinovich 和 A. C. Berg, “Parsenet: 通过更宽视角来提升效果，” *arXiv预印本
    arXiv:1506.04579*，2015年。'
- en: '[33] G. Wang, W. Li, S. Ourselin, and T. Vercauteren, “Automatic brain tumor
    segmentation using cascaded anisotropic convolutional neural networks,” in *International
    MICCAI Brainlesion Workshop*.   Springer, 2017, pp. 178–190.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] G. Wang, W. Li, S. Ourselin, 和 T. Vercauteren, “使用级联各向异性卷积神经网络进行自动脑肿瘤分割”，发表于
    *国际MICCAI脑病变研讨会*。Springer，2017年，第178–190页。'
- en: '[34] Y. Li, H. Qi, J. Dai, X. Ji, and Y. Wei, “Fully convolutional instance-aware
    semantic segmentation,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2017, pp. 2359–2367.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. Li, H. Qi, J. Dai, X. Ji, 和 Y. Wei, “完全卷积的实例感知语义分割”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2017年，第2359–2367页。'
- en: '[35] Y. Yuan, M. Chao, and Y.-C. Lo, “Automatic skin lesion segmentation using
    deep fully convolutional networks with jaccard distance,” *IEEE transactions on
    medical imaging*, vol. 36, no. 9, pp. 1876–1886, 2017.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Yuan, M. Chao, 和 Y.-C. Lo, “使用深度完全卷积网络和Jaccard距离进行自动皮肤病变分割”，*IEEE医学成像交易*，第36卷，第9期，第1876–1886页，2017年。'
- en: '[36] N. Liu, H. Li, M. Zhang, J. Liu, Z. Sun, and T. Tan, “Accurate iris segmentation
    in non-cooperative environments using fully convolutional networks,” in *2016
    International Conference on Biometrics (ICB)*.   IEEE, 2016, pp. 1–8.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] N. Liu, H. Li, M. Zhang, J. Liu, Z. Sun, 和 T. Tan, “在非合作环境中使用完全卷积网络进行准确的虹膜分割”，发表于
    *2016年国际生物识别会议（ICB）*。IEEE，2016年，第1–8页。'
- en: '[37] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Semantic
    image segmentation with deep convolutional nets and fully connected crfs,” *arXiv
    preprint arXiv:1412.7062*, 2014.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille, “使用深度卷积网络和完全连接的条件随机场进行语义图像分割”，*arXiv预印本arXiv:1412.7062*，2014年。'
- en: '[38] A. G. Schwing and R. Urtasun, “Fully connected deep structured networks,”
    *arXiv preprint arXiv:1503.02351*, 2015.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] A. G. Schwing 和 R. Urtasun, “完全连接的深度结构网络”，*arXiv预印本arXiv:1503.02351*，2015年。'
- en: '[39] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang,
    and P. H. Torr, “Conditional random fields as recurrent neural networks,” in *Proceedings
    of the IEEE international conference on computer vision*, 2015, pp. 1529–1537.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C.
    Huang, 和 P. H. Torr, “将条件随机场视作递归神经网络”，发表于 *IEEE国际计算机视觉会议论文集*，2015年，第1529–1537页。'
- en: '[40] G. Lin, C. Shen, A. Van Den Hengel, and I. Reid, “Efficient piecewise
    training of deep structured models for semantic segmentation,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2016, pp.
    3194–3203.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] G. Lin, C. Shen, A. Van Den Hengel, 和 I. Reid, “高效的深度结构模型的逐片训练用于语义分割”，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2016年，第3194–3203页。'
- en: '[41] Z. Liu, X. Li, P. Luo, C.-C. Loy, and X. Tang, “Semantic image segmentation
    via deep parsing network,” in *Proceedings of the IEEE international conference
    on computer vision*, 2015, pp. 1377–1385.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Z. Liu, X. Li, P. Luo, C.-C. Loy, 和 X. Tang, “通过深度解析网络进行语义图像分割”，发表于 *IEEE国际计算机视觉会议论文集*，2015年，第1377–1385页。'
- en: '[42] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for semantic
    segmentation,” in *Proceedings of the IEEE international conference on computer
    vision*, 2015, pp. 1520–1528.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] H. Noh, S. Hong, 和 B. Han, “为语义分割学习去卷积网络”，发表于 *IEEE国际计算机视觉会议论文集*，2015年，第1520–1528页。'
- en: '[43] A. Kendall, V. Badrinarayanan, and R. Cipolla, “Bayesian segnet: Model
    uncertainty in deep convolutional encoder-decoder architectures for scene understanding,”
    *arXiv preprint arXiv:1511.02680*, 2015.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] A. Kendall, V. Badrinarayanan, 和 R. Cipolla, “贝叶斯SegNet：用于场景理解的深度卷积编码器-解码器架构中的模型不确定性”，*arXiv预印本arXiv:1511.02680*，2015年。'
- en: '[44] Y. Yuan, X. Chen, and J. Wang, “Object-contextual representations for
    semantic segmentation,” *arXiv preprint arXiv:1909.11065*, 2019.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Yuan, X. Chen, 和 J. Wang, “用于语义分割的对象上下文表示”，*arXiv预印本arXiv:1909.11065*，2019年。'
- en: '[45] J. Fu, J. Liu, Y. Wang, J. Zhou, C. Wang, and H. Lu, “Stacked deconvolutional
    network for semantic segmentation,” *IEEE Transactions on Image Processing*, 2019.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Fu, J. Liu, Y. Wang, J. Zhou, C. Wang, 和 H. Lu, “堆叠去卷积网络用于语义分割”，*IEEE图像处理交易*，2019年。'
- en: '[46] A. Chaurasia and E. Culurciello, “Linknet: Exploiting encoder representations
    for efficient semantic segmentation,” in *2017 IEEE Visual Communications and
    Image Processing (VCIP)*.   IEEE, 2017, pp. 1–4.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Chaurasia 和 E. Culurciello, “Linknet：利用编码器表示进行高效的语义分割”，发表于 *2017年IEEE视觉通信与图像处理会议（VCIP）*。IEEE，2017年，第1–4页。'
- en: '[47] X. Xia and B. Kulis, “W-net: A deep model for fully unsupervised image
    segmentation,” *arXiv preprint arXiv:1711.08506*, 2017.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] X. Xia 和 B. Kulis, “W-net：一种完全无监督的图像分割深度模型”，*arXiv预印本arXiv:1711.08506*，2017年。'
- en: '[48] Y. Cheng, R. Cai, Z. Li, X. Zhao, and K. Huang, “Locality-sensitive deconvolution
    networks with gated fusion for rgb-d indoor semantic segmentation,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2017, pp.
    3029–3037.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Cheng, R. Cai, Z. Li, X. Zhao, 和 K. Huang，“具有门控融合的局部敏感去卷积网络用于rgb-d室内语义分割”，在*IEEE计算机视觉与模式识别会议论文集*，2017年，第3029–3037页。'
- en: '[49] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] O. Ronneberger, P. Fischer, 和 T. Brox，“U-net：用于生物医学图像分割的卷积网络”，在*医学图像计算与计算机辅助干预国际会议*，Springer，2015年，第234–241页。'
- en: '[50] F. Milletari, N. Navab, and S.-A. Ahmadi, “V-net: Fully convolutional
    neural networks for volumetric medical image segmentation,” in *2016 Fourth International
    Conference on 3D Vision (3DV)*.   IEEE, 2016, pp. 565–571.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] F. Milletari, N. Navab, 和 S.-A. Ahmadi，“V-net：用于体积医学图像分割的全卷积神经网络”，在*2016第四届国际3D视觉会议（3DV）*，IEEE，2016年，第565–571页。'
- en: '[51] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger,
    “3d u-net: learning dense volumetric segmentation from sparse annotation,” in
    *International conference on medical image computing and computer-assisted intervention*.   Springer,
    2016, pp. 424–432.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, 和 O. Ronneberger，“3d
    u-net：从稀疏标注学习密集体积分割”，在*医学图像计算与计算机辅助干预国际会议*，Springer，2016年，第424–432页。'
- en: '[52] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, and J. Liang, “Unet++: A nested
    u-net architecture for medical image segmentation,” in *Deep Learning in Medical
    Image Analysis and Multimodal Learning for Clinical Decision Support*.   Springer,
    2018, pp. 3–11.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Z. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, 和 J. Liang，“Unet++：一种用于医学图像分割的嵌套u-net架构”，在*医学图像分析中的深度学习与临床决策支持的多模态学习*，Springer，2018年，第3–11页。'
- en: '[53] Z. Zhang, Q. Liu, and Y. Wang, “Road extraction by deep residual u-net,”
    *IEEE Geoscience and Remote Sensing Letters*, vol. 15, no. 5, pp. 749–753, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Z. Zhang, Q. Liu, 和 Y. Wang，“深度残差u-net进行道路提取”，*IEEE地球科学与遥感快报*，第15卷，第5期，第749–753页，2018年。'
- en: '[54] T. Brosch, L. Y. Tang, Y. Yoo, D. K. Li, A. Traboulsee, and R. Tam, “Deep
    3d convolutional encoder networks with shortcuts for multiscale feature integration
    applied to multiple sclerosis lesion segmentation,” *IEEE transactions on medical
    imaging*, vol. 35, no. 5, pp. 1229–1239, 2016.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] T. Brosch, L. Y. Tang, Y. Yoo, D. K. Li, A. Traboulsee, 和 R. Tam，“具有快捷连接的深度3d卷积编码器网络用于多尺度特征集成，应用于多发性硬化病变分割”，*IEEE医学影像学事务*，第35卷，第5期，第1229–1239页，2016年。'
- en: '[55] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 2117–2125.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, 和 S. Belongie，“用于物体检测的特征金字塔网络”，在*IEEE计算机视觉与模式识别会议论文集*，2017年，第2117–2125页。'
- en: '[56] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2017, pp. 2881–2890.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] H. Zhao, J. Shi, X. Qi, X. Wang, 和 J. Jia，“金字塔场景解析网络”，在*IEEE计算机视觉与模式识别会议论文集*，2017年，第2881–2890页。'
- en: '[57] G. Ghiasi and C. C. Fowlkes, “Laplacian pyramid reconstruction and refinement
    for semantic segmentation,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 519–534.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] G. Ghiasi 和 C. C. Fowlkes，“用于语义分割的拉普拉斯金字塔重建与精细化”，在*欧洲计算机视觉会议*，Springer，2016年，第519–534页。'
- en: '[58] J. He, Z. Deng, and Y. Qiao, “Dynamic multi-scale filters for semantic
    segmentation,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2019, pp. 3562–3572.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] J. He, Z. Deng, 和 Y. Qiao，“用于语义分割的动态多尺度滤波器”，在*IEEE国际计算机视觉会议论文集*，2019年，第3562–3572页。'
- en: '[59] H. Ding, X. Jiang, B. Shuai, A. Qun Liu, and G. Wang, “Context contrasted
    feature and gated multi-scale aggregation for scene segmentation,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    2393–2402.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Ding, X. Jiang, B. Shuai, A. Qun Liu, 和 G. Wang，“用于场景分割的上下文对比特征和门控多尺度聚合”，在*IEEE计算机视觉与模式识别会议论文集*，2018年，第2393–2402页。'
- en: '[60] J. He, Z. Deng, L. Zhou, Y. Wang, and Y. Qiao, “Adaptive pyramid context
    network for semantic segmentation,” in *Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 7519–7528.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. He, Z. Deng, L. Zhou, Y. Wang, 和 Y. Qiao，“用于语义分割的自适应金字塔上下文网络”，在*计算机视觉与模式识别会议*，2019年，第7519–7528页。'
- en: '[61] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, and H. Huang, “Multi-scale
    context intertwining for semantic segmentation,” in *Proceedings of the European
    Conference on Computer Vision (ECCV)*, 2018, pp. 603–619.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] D. Lin, Y. Ji, D. Lischinski, D. Cohen-Or, 和 H. Huang, “语义分割中的多尺度上下文交织,”
    在 *欧洲计算机视觉会议论文集 (ECCV)*, 2018, 第603–619页。'
- en: '[62] G. Li, Y. Xie, L. Lin, and Y. Yu, “Instance-level salient object segmentation,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 2386–2395.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] G. Li, Y. Xie, L. Lin, 和 Y. Yu, “实例级显著物体分割,” 在 *IEEE计算机视觉与模式识别会议论文集*,
    2017, 第2386–2395页。'
- en: '[63] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *Advances in neural information
    processing systems*, 2015, pp. 91–99.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] S. Ren, K. He, R. Girshick, 和 J. Sun, “Faster r-cnn: 朝着实时物体检测与区域建议网络,”
    在 *神经信息处理系统进展*, 2015, 第91–99页。'
- en: '[64] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *Proceedings
    of the IEEE international conference on computer vision*, 2017, pp. 2961–2969.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. He, G. Gkioxari, P. Dollár, 和 R. Girshick, “Mask r-cnn,” 在 *IEEE国际计算机视觉会议论文集*,
    2017, 第2961–2969页。'
- en: '[65] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia, “Path aggregation network for
    instance segmentation,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 8759–8768.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. Liu, L. Qi, H. Qin, J. Shi, 和 J. Jia, “用于实例分割的路径聚合网络,” 在 *IEEE计算机视觉与模式识别会议论文集*,
    2018, 第8759–8768页。'
- en: '[66] J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via multi-task
    network cascades,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2016, pp. 3150–3158.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Dai, K. He, 和 J. Sun, “通过多任务网络级联的实例感知语义分割,” 在 *IEEE计算机视觉与模式识别会议论文集*,
    2016, 第3150–3158页。'
- en: '[67] R. Hu, P. Dollár, K. He, T. Darrell, and R. Girshick, “Learning to segment
    every thing,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2018, pp. 4233–4241.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Hu, P. Dollár, K. He, T. Darrell, 和 R. Girshick, “学习分割所有物体,” 在 *IEEE计算机视觉与模式识别会议论文集*,
    2018, 第4233–4241页。'
- en: '[68] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff, P. Wang, and H. Adam,
    “Masklab: Instance segmentation by refining object detection with semantic and
    direction features,” in *Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition*, 2018, pp. 4013–4022.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L.-C. Chen, A. Hermans, G. Papandreou, F. Schroff, P. Wang, 和 H. Adam,
    “Masklab: 通过语义和方向特征细化物体检测的实例分割,” 在 *IEEE计算机视觉与模式识别会议论文集*, 2018, 第4013–4022页。'
- en: '[69] X. Chen, R. Girshick, K. He, and P. Dollár, “Tensormask: A foundation
    for dense object segmentation,” *arXiv preprint arXiv:1903.12174*, 2019.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] X. Chen, R. Girshick, K. He, 和 P. Dollár, “Tensormask: 密集物体分割的基础,” *arXiv
    预印本 arXiv:1903.12174*, 2019。'
- en: '[70] J. Dai, Y. Li, K. He, and J. Sun, “R-fcn: Object detection via region-based
    fully convolutional networks,” in *Advances in neural information processing systems*,
    2016, pp. 379–387.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Dai, Y. Li, K. He, 和 J. Sun, “R-fcn: 通过基于区域的全卷积网络进行物体检测,” 在 *神经信息处理系统进展*,
    2016, 第379–387页。'
- en: '[71] P. O. Pinheiro, R. Collobert, and P. Dollár, “Learning to segment object
    candidates,” in *Advances in Neural Information Processing Systems*, 2015, pp.
    1990–1998.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] P. O. Pinheiro, R. Collobert, 和 P. Dollár, “学习分割物体候选,” 在 *神经信息处理系统进展*,
    2015, 第1990–1998页。'
- en: '[72] E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, and P. Luo,
    “Polarmask: Single shot instance segmentation with polar representation,” *arXiv
    preprint arXiv:1909.13226*, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] E. Xie, P. Sun, X. Song, W. Wang, X. Liu, D. Liang, C. Shen, 和 P. Luo,
    “Polarmask: 单次实例分割与极坐标表示,” *arXiv 预印本 arXiv:1909.13226*, 2019。'
- en: '[73] Z. Hayder, X. He, and M. Salzmann, “Boundary-aware instance segmentation,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 5696–5704.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Z. Hayder, X. He, 和 M. Salzmann, “边界感知实例分割,” 在 *IEEE计算机视觉与模式识别会议论文集*,
    2017, 第5696–5704页。'
- en: '[74] Y. Lee and J. Park, “Centermask: Real-time anchor-free instance segmentation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2020, pp. 13 906–13 915.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y. Lee 和 J. Park, “Centermask: 实时无锚点实例分割,” 在 *IEEE/CVF计算机视觉与模式识别会议论文集*,
    2020, 第13,906–13,915页。'
- en: '[75] M. Bai and R. Urtasun, “Deep watershed transform for instance segmentation,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 5221–5229.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] M. Bai 和 R. Urtasun, “深度分水岭变换用于实例分割,” 在 *IEEE计算机视觉与模式识别会议论文集*, 2017, 第5221–5229页。'
- en: '[76] D. Bolya, C. Zhou, F. Xiao, and Y. J. Lee, “Yolact: Real-time instance
    segmentation,” in *Proceedings of the IEEE international conference on computer
    vision*, 2019, pp. 9157–9166.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] D. Bolya, C. Zhou, F. Xiao, 和 Y. J. Lee, “Yolact: 实时实例分割，” 见于 *IEEE国际计算机视觉会议论文集*，2019年，第9157–9166页。'
- en: '[77] A. Fathi, Z. Wojna, V. Rathod, P. Wang, H. O. Song, S. Guadarrama, and
    K. P. Murphy, “Semantic instance segmentation via deep metric learning,” *arXiv
    preprint arXiv:1703.10277*, 2017.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] A. Fathi, Z. Wojna, V. Rathod, P. Wang, H. O. Song, S. Guadarrama, 和 K.
    P. Murphy, “通过深度度量学习进行语义实例分割，” *arXiv预印本 arXiv:1703.10277*，2017年。'
- en: '[78] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 40, no. 4, pp. 834–848, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, 和 A. L. Yuille, “DeepLab:
    使用深度卷积网络、空洞卷积和全连接CRF进行语义图像分割，” *IEEE模式分析与机器智能学报*，第40卷，第4期，第834–848页，2017年。'
- en: '[79] F. Yu and V. Koltun, “Multi-scale context aggregation by dilated convolutions,”
    *arXiv preprint arXiv:1511.07122*, 2015.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] F. Yu 和 V. Koltun, “通过扩张卷积进行多尺度上下文聚合，” *arXiv预印本 arXiv:1511.07122*，2015年。'
- en: '[80] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, and G. Cottrell,
    “Understanding convolution for semantic segmentation,” in *winter conference on
    applications of computer vision*.   IEEE, 2018, pp. 1451–1460.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] P. Wang, P. Chen, Y. Yuan, D. Liu, Z. Huang, X. Hou, 和 G. Cottrell, “理解卷积在语义分割中的作用，”
    见于 *冬季计算机视觉应用会议*，IEEE，2018年，第1451–1460页。'
- en: '[81] M. Yang, K. Yu, C. Zhang, Z. Li, and K. Yang, “Denseaspp for semantic
    segmentation in street scenes,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2018, pp. 3684–3692.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Yang, K. Yu, C. Zhang, Z. Li, 和 K. Yang, “用于街景语义分割的DenseASPP，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，第3684–3692页。'
- en: '[82] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep neural
    network architecture for real-time semantic segmentation,” *arXiv preprint arXiv:1606.02147*,
    2016.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. Paszke, A. Chaurasia, S. Kim, 和 E. Culurciello, “Enet: 用于实时语义分割的深度神经网络架构，”
    *arXiv预印本 arXiv:1606.02147*，2016年。'
- en: '[83] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
    with atrous separable convolution for semantic image segmentation,” in *Proceedings
    of the European conference on computer vision (ECCV)*, 2018, pp. 801–818.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, 和 H. Adam, “具有空洞可分离卷积的编码器-解码器用于语义图像分割，”
    见于 *欧洲计算机视觉会议（ECCV）论文集*，2018年，第801–818页。'
- en: '[84] F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho, Y. Bengio, M. Matteucci,
    and A. Courville, “Reseg: A recurrent neural network-based model for semantic
    segmentation,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition Workshops*, 2016, pp. 41–48.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] F. Visin, M. Ciccone, A. Romero, K. Kastner, K. Cho, Y. Bengio, M. Matteucci,
    和 A. Courville, “Reseg: 基于递归神经网络的语义分割模型，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2016年，第41–48页。'
- en: '[85] F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville, and Y. Bengio,
    “Renet: A recurrent neural network based alternative to convolutional networks,”
    *arXiv preprint arXiv:1505.00393*, 2015.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] F. Visin, K. Kastner, K. Cho, M. Matteucci, A. Courville, 和 Y. Bengio,
    “Renet: 递归神经网络作为卷积网络的替代方案，” *arXiv预印本 arXiv:1505.00393*，2015年。'
- en: '[86] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki, “Scene labeling with
    lstm recurrent neural networks,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2015, pp. 3547–3555.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] W. Byeon, T. M. Breuel, F. Raue, 和 M. Liwicki, “使用LSTM递归神经网络进行场景标注，” 见于
    *IEEE计算机视觉与模式识别会议*，2015年，第3547–3555页。'
- en: '[87] X. Liang, X. Shen, J. Feng, L. Lin, and S. Yan, “Semantic object parsing
    with graph lstm,” in *European Conference on Computer Vision*.   Springer, 2016,
    pp. 125–143.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] X. Liang, X. Shen, J. Feng, L. Lin, 和 S. Yan, “使用图形LSTM进行语义对象解析，” 见于 *欧洲计算机视觉会议*，Springer，2016年，第125–143页。'
- en: '[88] Y. Xiang and D. Fox, “Da-rnn: Semantic mapping with data associated recurrent
    neural networks,” *arXiv:1703.03098*, 2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Y. Xiang 和 D. Fox, “Da-rnn: 使用数据关联递归神经网络进行语义映射，” *arXiv:1703.03098*，2017年。'
- en: '[89] R. Hu, M. Rohrbach, and T. Darrell, “Segmentation from natural language
    expressions,” in *European Conference on Computer Vision*.   Springer, 2016, pp.
    108–124.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] R. Hu, M. Rohrbach, 和 T. Darrell, “基于自然语言表达的分割，” 见于 *欧洲计算机视觉会议*，Springer，2016年，第108–124页。'
- en: '[90] L.-C. Chen, Y. Yang, J. Wang, W. Xu, and A. L. Yuille, “Attention to scale:
    Scale-aware semantic image segmentation,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2016, pp. 3640–3649.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] L.-C. Chen, Y. Yang, J. Wang, W. Xu 和 A. L. Yuille，"注意尺度: 规模感知的语义图像分割"，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2016年，页码3640–3649。'
- en: '[91] Q. Huang, C. Xia, C. Wu, S. Li, Y. Wang, Y. Song, and C.-C. J. Kuo, “Semantic
    segmentation with reverse attention,” *arXiv preprint arXiv:1707.06426*, 2017.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Q. Huang, C. Xia, C. Wu, S. Li, Y. Wang, Y. Song 和 C.-C. J. Kuo，"具有反向注意力的语义分割"，*arXiv预印本
    arXiv:1707.06426*，2017年。'
- en: '[92] H. Li, P. Xiong, J. An, and L. Wang, “Pyramid attention network for semantic
    segmentation,” *arXiv preprint arXiv:1805.10180*, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] H. Li, P. Xiong, J. An 和 L. Wang，"用于语义分割的金字塔注意力网络"，*arXiv预印本 arXiv:1805.10180*，2018年。'
- en: '[93] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, “Dual attention
    network for scene segmentation,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 3146–3154.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang 和 H. Lu，"用于场景分割的双重注意力网络"，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2019年，页码3146–3154。'
- en: '[94] Y. Yuan and J. Wang, “Ocnet: Object context network for scene parsing,”
    *arXiv preprint arXiv:1809.00916*, 2018.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Y. Yuan 和 J. Wang，"Ocnet: 用于场景解析的对象上下文网络"，*arXiv预印本 arXiv:1809.00916*，2018年。'
- en: '[95] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, “Expectation-maximization
    attention networks for semantic segmentation,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 9167–9176.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin 和 H. Liu，"期望最大化注意力网络用于语义分割"，发表于
    *IEEE国际计算机视觉会议论文集*，2019年，页码9167–9176。'
- en: '[96] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei, and W. Liu, “Ccnet: Criss-cross
    attention for semantic segmentation,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2019, pp. 603–612.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Z. Huang, X. Wang, L. Huang, C. Huang, Y. Wei 和 W. Liu，"Ccnet: 用于语义分割的交叉注意力"，发表于
    *IEEE国际计算机视觉会议论文集*，2019年，页码603–612。'
- en: '[97] M. Ren and R. S. Zemel, “End-to-end instance segmentation with recurrent
    attention,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2017, pp. 6656–6664.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] M. Ren 和 R. S. Zemel，"端到端实例分割与递归注意力"，发表于 *IEEE计算机视觉与模式识别会议论文集*，2017年，页码6656–6664。'
- en: '[98] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. Change Loy, D. Lin, and J. Jia,
    “Psanet: Point-wise spatial attention network for scene parsing,” in *Proceedings
    of the European Conference on Computer Vision (ECCV)*, 2018, pp. 267–283.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. Change Loy, D. Lin 和 J. Jia，"Psanet:
    点对点空间注意力网络用于场景解析"，发表于 *欧洲计算机视觉会议（ECCV）论文集*，2018年，页码267–283。'
- en: '[99] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, “Learning a discriminative
    feature network for semantic segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 1857–1866.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu 和 N. Sang，"学习用于语义分割的区分特征网络"，发表于
    *IEEE计算机视觉与模式识别会议论文集*，2018年，页码1857–1866。'
- en: '[100] P. Luc, C. Couprie, S. Chintala, and J. Verbeek, “Semantic segmentation
    using adversarial networks,” *arXiv preprint arXiv:1611.08408*, 2016.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] P. Luc, C. Couprie, S. Chintala 和 J. Verbeek，"使用对抗网络进行语义分割"，*arXiv预印本
    arXiv:1611.08408*，2016年。'
- en: '[101] N. Souly, C. Spampinato, and M. Shah, “Semi supervised semantic segmentation
    using generative adversarial network,” in *Proceedings of the IEEE International
    Conference on Computer Vision*, 2017, pp. 5688–5696.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] N. Souly, C. Spampinato 和 M. Shah，"使用生成对抗网络的半监督语义分割"，发表于 *IEEE国际计算机视觉会议论文集*，2017年，页码5688–5696。'
- en: '[102] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H. Yang, “Adversarial
    learning for semi-supervised semantic segmentation,” *arXiv preprint arXiv:1802.07934*,
    2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin 和 M.-H. Yang，"用于半监督语义分割的对抗学习"，*arXiv预印本
    arXiv:1802.07934*，2018年。'
- en: '[103] Y. Xue, T. Xu, H. Zhang, L. R. Long, and X. Huang, “Segan: Adversarial
    network with multi-scale l 1 loss for medical image segmentation,” *Neuroinformatics*,
    vol. 16, no. 3-4, pp. 383–392, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Xue, T. Xu, H. Zhang, L. R. Long 和 X. Huang，"Segan: 具有多尺度 l 1 损失的对抗网络用于医学图像分割"，*Neuroinformatics*，第16卷，第3-4期，页码383–392，2018年。'
- en: '[104] M. Majurski, P. Manescu, S. Padi, N. Schaub, N. Hotaling, C. Simon Jr,
    and P. Bajcsy, “Cell image segmentation using generative adversarial networks,
    transfer learning, and augmentations,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops*, 2019, pp. 0–0.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] M. Majurski、P. Manescu、S. Padi、N. Schaub、N. Hotaling、C. Simon Jr 和 P.
    Bajcsy，“使用生成对抗网络、迁移学习和数据增强进行细胞图像分割”，见于 *IEEE计算机视觉与模式识别大会研讨会论文集*，2019年，第0–0页。'
- en: '[105] K. Ehsani, R. Mottaghi, and A. Farhadi, “Segan: Segmenting and generating
    the invisible,” in *Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition*, 2018, pp. 6144–6153.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] K. Ehsani、R. Mottaghi 和 A. Farhadi，“Segan: 分割和生成不可见的”，见于 *IEEE计算机视觉与模式识别大会论文集*，2018年，第6144–6153页。'
- en: '[106] T. F. Chan and L. A. Vese, “Active contours without edges,” *IEEE Transactions
    on Image Processing*, vol. 10, no. 2, pp. 266–277, 2001.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] T. F. Chan 和 L. A. Vese，“无边缘主动轮廓”，*IEEE图像处理汇刊*，第10卷，第2期，第266–277页，2001年。'
- en: '[107] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner, R. Williams,
    and Y. Zheng, “Learning active contour models for medical image segmentation,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 11 632–11 640.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] X. Chen、B. M. Williams、S. R. Vallabhaneni、G. Czanner、R. Williams 和 Y.
    Zheng，“学习医学图像分割的主动轮廓模型”，见于 *IEEE计算机视觉与模式识别大会论文集*，2019年，第11632–11640页。'
- en: '[108] T. H. N. Le, K. G. Quach, K. Luu, C. N. Duong, and M. Savvides, “Reformulating
    level sets as deep recurrent neural network approach to semantic segmentation,”
    *IEEE Transactions on Image Processing*, vol. 27, no. 5, pp. 2393–2407, 2018.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] T. H. N. Le、K. G. Quach、K. Luu、C. N. Duong 和 M. Savvides，“将水平集重新表述为深度递归神经网络方法用于语义分割”，*IEEE图像处理汇刊*，第27卷，第5期，第2393–2407页，2018年。'
- en: '[109] C. Rupprecht, E. Huaroc, M. Baust, and N. Navab, “Deep active contours,”
    *arXiv preprint arXiv:1607.05074*, 2016.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] C. Rupprecht、E. Huaroc、M. Baust 和 N. Navab，“深度主动轮廓”，*arXiv预印本arXiv:1607.05074*，2016年。'
- en: '[110] A. Hatamizadeh, A. Hoogi, D. Sengupta, W. Lu, B. Wilcox, D. Rubin, and
    D. Terzopoulos, “Deep active lesion segmentation,” in *Proc. International Workshop
    on Machine Learning in Medical Imaging*, ser. Lecture Notes in Computer Science,
    vol. 11861.   Springer, 2019, pp. 98–105.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. Hatamizadeh、A. Hoogi、D. Sengupta、W. Lu、B. Wilcox、D. Rubin 和 D. Terzopoulos，“深度主动病变分割”，见于
    *国际医学影像机器学习研讨会论文集*，讲义系列：计算机科学讲义，第 11861 卷。Springer，2019年，第98–105页。'
- en: '[111] D. Marcos, D. Tuia, B. Kellenberger, L. Zhang, M. Bai, R. Liao, and R. Urtasun,
    “Learning deep structured active contours end-to-end,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018, pp.
    8877–8885.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] D. Marcos、D. Tuia、B. Kellenberger、L. Zhang、M. Bai、R. Liao 和 R. Urtasun，“端到端学习深度结构化主动轮廓”，见于
    *IEEE计算机视觉与模式识别大会论文集（CVPR）*，2018年，第8877–8885页。'
- en: '[112] D. Cheng, R. Liao, S. Fidler, and R. Urtasun, “Darnet: Deep active ray
    network for building segmentation,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 7431–7439.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] D. Cheng、R. Liao、S. Fidler 和 R. Urtasun，“Darnet: 用于建筑分割的深度主动射线网络”，见于
    *IEEE计算机视觉与模式识别大会论文集*，2019年，第7431–7439页。'
- en: '[113] A. Hatamizadeh, D. Sengupta, and D. Terzopoulos, “End-to-end deep convolutional
    active contours for image segmentation,” *arXiv preprint arXiv:1909.13359*, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Hatamizadeh、D. Sengupta 和 D. Terzopoulos，“端到端深度卷积主动轮廓图像分割”，*arXiv预印本arXiv:1909.13359*，2019年。'
- en: '[114] H. Zhang, K. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi, and A. Agrawal,
    “Context encoding for semantic segmentation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 7151–7160.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] H. Zhang、K. Dana、J. Shi、Z. Zhang、X. Wang、A. Tyagi 和 A. Agrawal，“语义分割的上下文编码”，见于
    *IEEE计算机视觉与模式识别大会论文集*，2018年，第7151–7160页。'
- en: '[115] G. Lin, A. Milan, C. Shen, and I. Reid, “Refinenet: Multi-path refinement
    networks for high-resolution semantic segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 1925–1934.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] G. Lin、A. Milan、C. Shen 和 I. Reid，“Refinenet: 多路径细化网络用于高分辨率语义分割”，见于 *IEEE计算机视觉与模式识别大会论文集*，2017年，第1925–1934页。'
- en: '[116] G. Song, H. Myeong, and K. Mu Lee, “Seednet: Automatic seed generation
    with deep reinforcement learning for robust interactive segmentation,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    1760–1768.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] G. Song、H. Myeong 和 K. Mu Lee，“Seednet: 基于深度强化学习的自动种子生成用于鲁棒交互式分割”，见于
    *IEEE计算机视觉与模式识别大会论文集*，2018年，第1760–1768页。'
- en: '[117] J. Dai, K. He, and J. Sun, “Boxsup: Exploiting bounding boxes to supervise
    convolutional networks for semantic segmentation,” in *Proceedings of the IEEE
    International Conference on Computer Vision*, 2015, pp. 1635–1643.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] J. Dai, K. He, 和 J. Sun，“Boxsup: 利用边界框来监督卷积网络进行语义分割，”发表在*IEEE 国际计算机视觉会议论文集*，2015年，页1635–1643。'
- en: '[118] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, “Large kernel matters–improve
    semantic segmentation by global convolutional network,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2017, pp. 4353–4361.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] C. Peng, X. Zhang, G. Yu, G. Luo, 和 J. Sun，“大卷积核的重要性——通过全局卷积网络改进语义分割，”发表在*IEEE
    计算机视觉与模式识别会议论文集*，2017年，页4353–4361。'
- en: '[119] Z. Wu, C. Shen, and A. Van Den Hengel, “Wider or deeper: Revisiting the
    resnet model for visual recognition,” *Pattern Recognition*, vol. 90, pp. 119–133,
    2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Z. Wu, C. Shen, 和 A. Van Den Hengel，“更宽还是更深: 重新审视用于视觉识别的 ResNet 模型，”*模式识别*，第90卷，页119–133，2019年。'
- en: '[120] Z. Zhang, X. Zhang, C. Peng, X. Xue, and J. Sun, “Exfuse: Enhancing feature
    fusion for semantic segmentation,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 269–284.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Z. Zhang, X. Zhang, C. Peng, X. Xue, 和 J. Sun，“Exfuse: 增强特征融合以进行语义分割，”发表在*欧洲计算机视觉会议论文集
    (ECCV)*，2018年，页269–284。'
- en: '[121] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich, “Feedforward semantic
    segmentation with zoom-out features,” in *Proceedings of the IEEE conference on
    computer vision and pattern recognition*, 2015, pp. 3376–3385.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Mostajabi, P. Yadollahpour, 和 G. Shakhnarovich，“使用放大特征的前馈语义分割，”发表在*IEEE
    计算机视觉与模式识别会议论文集*，2015年，页3376–3385。'
- en: '[122] W. Wang, J. Shen, and F. Porikli, “Saliency-aware geodesic video object
    segmentation,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2015, pp. 3395–3402.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] W. Wang, J. Shen, 和 F. Porikli，“显著性感知地理视频目标分割，”发表在*IEEE 计算机视觉与模式识别会议论文集*，2015年，页3395–3402。'
- en: '[123] P. Luo, G. Wang, L. Lin, and X. Wang, “Deep dual learning for semantic
    image segmentation,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2017, pp. 2718–2726.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] P. Luo, G. Wang, L. Lin, 和 X. Wang，“用于语义图像分割的深度双重学习，”发表在*IEEE 国际计算机视觉会议论文集*，2017年，页2718–2726。'
- en: '[124] X. Li, Z. Jie, W. Wang, C. Liu, J. Yang, X. Shen, Z. Lin, Q. Chen, S. Yan,
    and J. Feng, “Foveanet: Perspective-aware urban scene parsing,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2017, pp. 784–792.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] X. Li, Z. Jie, W. Wang, C. Liu, J. Yang, X. Shen, Z. Lin, Q. Chen, S.
    Yan, 和 J. Feng，“Foveanet: 透视感知的城市场景解析，”发表在*IEEE 国际计算机视觉会议论文集*，2017年，页784–792。'
- en: '[125] I. Kreso, S. Segvic, and J. Krapac, “Ladder-style densenets for semantic
    segmentation of large natural images,” in *IEEE International Conference on Computer
    Vision*, 2017, pp. 238–245.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] I. Kreso, S. Segvic, 和 J. Krapac，“用于大自然图像语义分割的阶梯式密集网络，”发表在*IEEE 国际计算机视觉会议*，2017年，页238–245。'
- en: '[126] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang, “Bisenet: Bilateral
    segmentation network for real-time semantic segmentation,” in *European Conference
    on Computer Vision*, 2018, pp. 325–341.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, 和 N. Sang，“Bisenet: 实时语义分割的双边分割网络，”发表在*欧洲计算机视觉会议*，2018年，页325–341。'
- en: '[127] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, and H. Shi, “Spgnet: Semantic prediction guidance for scene parsing,”
    in *Proceedings of the IEEE International Conference on Computer Vision*, 2019,
    pp. 5218–5228.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] B. Cheng, L.-C. Chen, Y. Wei, Y. Zhu, Z. Huang, J. Xiong, T. S. Huang,
    W.-M. Hwu, 和 H. Shi，“Spgnet: 用于场景解析的语义预测引导，”发表在*IEEE 国际计算机视觉会议论文集*，2019年，页5218–5228。'
- en: '[128] T. Takikawa, D. Acuna, V. Jampani, and S. Fidler, “Gated-scnn: Gated
    shape cnns for semantic segmentation,” in *IEEE International Conference on Computer
    Vision*, 2019, pp. 5229–5238.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] T. Takikawa, D. Acuna, V. Jampani, 和 S. Fidler，“Gated-scnn: 用于语义分割的门控形状卷积网络，”发表在*IEEE
    国际计算机视觉会议*，2019年，页5229–5238。'
- en: '[129] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, and H. Lu, “Adaptive
    context network for scene parsing,” in *Proceedings of the IEEE international
    conference on computer vision*, 2019, pp. 6748–6757.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Fu, J. Liu, Y. Wang, Y. Li, Y. Bao, J. Tang, 和 H. Lu，“用于场景解析的自适应上下文网络，”发表在*IEEE
    国际计算机视觉会议论文集*，2019年，页6748–6757。'
- en: '[130] X. Liang, H. Zhou, and E. Xing, “Dynamic-structured semantic propagation
    network,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2018, pp. 752–761.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] X. Liang, H. Zhou, 和 E. Xing，“动态结构语义传播网络，”发表在*IEEE 计算机视觉与模式识别会议论文集*，2018年，页752–761。'
- en: '[131] X. Liang, Z. Hu, H. Zhang, L. Lin, and E. P. Xing, “Symbolic graph reasoning
    meets convolutions,” in *Advances in Neural Information Processing Systems*, 2018,
    pp. 1853–1863.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] X. Liang, Z. Hu, H. Zhang, L. Lin, 和 E. P. Xing，“符号图推理与卷积相结合”，见于 *神经信息处理系统进展*，2018，第1853–1863页。'
- en: '[132] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, “Scene
    parsing through ade20k dataset,” in *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition*, 2017.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, 和 A. Torralba，“通过ADE20K数据集进行场景解析”，见于
    *IEEE计算机视觉与模式识别会议论文集*，2017。'
- en: '[133] R. Zhang, S. Tang, Y. Zhang, J. Li, and S. Yan, “Scale-adaptive convolutions
    for scene parsing,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2017, pp. 2031–2039.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] R. Zhang, S. Tang, Y. Zhang, J. Li, 和 S. Yan，“用于场景解析的尺度自适应卷积”，见于 *IEEE国际计算机视觉会议论文集*，2017，第2031–2039页。'
- en: '[134] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Unified perceptual parsing
    for scene understanding,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 418–434.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, 和 J. Sun，“用于场景理解的统一感知解析”，见于 *欧洲计算机视觉会议论文集（ECCV）*，2018，第418–434页。'
- en: '[135] B. Zoph, G. Ghiasi, T.-Y. Lin, Y. Cui, H. Liu, E. D. Cubuk, and Q. V.
    Le, “Rethinking pre-training and self-training,” *arXiv preprint arXiv:2006.06882*,
    2020.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] B. Zoph, G. Ghiasi, T.-Y. Lin, Y. Cui, H. Liu, E. D. Cubuk, 和 Q. V. Le，“重新思考预训练和自我训练”，*arXiv
    预印本 arXiv:2006.06882*，2020。'
- en: '[136] X. Zhang, H. Xu, H. Mo, J. Tan, C. Yang, and W. Ren, “Dcnas: Densely
    connected neural architecture search for semantic image segmentation,” *arXiv
    preprint arXiv:2003.11883*, 2020.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] X. Zhang, H. Xu, H. Mo, J. Tan, C. Yang, 和 W. Ren，“DCNAS: 密集连接的神经架构搜索用于语义图像分割”，*arXiv
    预印本 arXiv:2003.11883*，2020。'
- en: '[137] A. Tao, K. Sapra, and B. Catanzaro, “Hierarchical multi-scale attention
    for semantic segmentation,” *arXiv preprint arXiv:2005.10821*, 2020.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] A. Tao, K. Sapra, 和 B. Catanzaro，“用于语义分割的层次多尺度注意力”，*arXiv 预印本 arXiv:2005.10821*，2020。'
- en: '[138] A. Kirillov, K. He, R. Girshick, C. Rother, and P. Dollár, “Panoptic
    segmentation,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 9404–9413.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] A. Kirillov, K. He, R. Girshick, C. Rother, 和 P. Dollár，“全景分割”，见于 *IEEE计算机视觉与模式识别会议论文集*，2019，第9404–9413页。'
- en: '[139] A. Kirillov, R. Girshick, K. He, and P. Dollar, “Panoptic feature pyramid
    networks,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 6399–6408.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. Kirillov, R. Girshick, K. He, 和 P. Dollar，“全景特征金字塔网络”，见于 *IEEE计算机视觉与模式识别会议论文集*，2019，第6399–6408页。'
- en: '[140] Y. Li, X. Chen, Z. Zhu, L. Xie, G. Huang, D. Du, and X. Wang, “Attention-guided
    unified network for panoptic segmentation,” in *IEEE Conference on Computer Vision
    and Pattern Recognition*, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Li, X. Chen, Z. Zhu, L. Xie, G. Huang, D. Du, 和 X. Wang，“用于全景分割的注意力引导统一网络”，见于
    *IEEE计算机视觉与模式识别会议*，2019。'
- en: '[141] L. Porzi, S. R. Bulo, A. Colovic, and P. Kontschieder, “Seamless scene
    segmentation,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 8277–8286.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] L. Porzi, S. R. Bulo, A. Colovic, 和 P. Kontschieder，“无缝场景分割”，见于 *IEEE计算机视觉与模式识别会议论文集*，2019，第8277–8286页。'
- en: '[142] B. Cheng, M. D. Collins, Y. Zhu, T. Liu, T. S. Huang, H. Adam, and L.-C.
    Chen, “Panoptic-deeplab,” *arXiv preprint arXiv:1910.04751*, 2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] B. Cheng, M. D. Collins, Y. Zhu, T. Liu, T. S. Huang, H. Adam, 和 L.-C.
    Chen，“全景DeepLab”，*arXiv 预印本 arXiv:1910.04751*，2019。'
- en: '[143] Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, and R. Urtasun,
    “Upsnet: A unified panoptic segmentation network,” in *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, 2019, pp. 8818–8826.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Y. Xiong, R. Liao, H. Zhao, R. Hu, M. Bai, E. Yumer, 和 R. Urtasun，“Upsnet:
    一种统一的全景分割网络”，见于 *IEEE计算机视觉与模式识别会议论文集*，2019，第8818–8826页。'
- en: '[144] R. Mohan and A. Valada, “Efficientps: Efficient panoptic segmentation,”
    *arXiv preprint arXiv:2004.02307*, 2020.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] R. Mohan 和 A. Valada，“EfficientPS: 高效全景分割”，*arXiv 预印本 arXiv:2004.02307*，2020。'
- en: '[145] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *International journal of
    computer vision*, vol. 88, pp. 303–338, 2010.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, 和 A. Zisserman，“PASCAL视觉目标类别（VOC）挑战”，*计算机视觉国际杂志*，第88卷，第303–338页，2010。'
- en: '[146] [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] [http://host.robots.ox.ac.uk/pascal/VOC/voc2012/](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)。'
- en: '[147] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun,
    and A. Yuille, “The role of context for object detection and semantic segmentation
    in the wild,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2014, pp. 891–898.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] R. Mottaghi、X. Chen、X. Liu、N.-G. Cho、S.-W. Lee、S. Fidler、R. Urtasun 和
    A. Yuille，“背景在野外对象检测和语义分割中的作用”，发表于 *IEEE计算机视觉与模式识别会议*，2014年，第891–898页。'
- en: '[148] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *European conference
    on computer vision*.   Springer, 2014.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] T.-Y. Lin、M. Maire、S. Belongie、J. Hays、P. Perona、D. Ramanan、P. Dollár
    和 C. L. Zitnick，“Microsoft coco：上下文中的常见对象”，发表于 *欧洲计算机视觉会议*。Springer，2014年。'
- en: '[149] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for semantic urban
    scene understanding,” in *IEEE Conference on Computer Vision and Pattern Recognition*,
    2016, pp. 3213–3223.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. Cordts、M. Omran、S. Ramos、T. Rehfeld、M. Enzweiler、R. Benenson、U. Franke、S.
    Roth 和 B. Schiele，“用于语义城市场景理解的Cityscapes数据集”，发表于 *IEEE计算机视觉与模式识别会议*，2016年，第3213–3223页。'
- en: '[150] C. Liu, J. Yuen, and A. Torralba, “Nonparametric scene parsing: Label
    transfer via dense scene alignment,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2009.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] C. Liu、J. Yuen 和 A. Torralba，“非参数场景解析：通过密集场景对齐进行标签转移”，发表于 *IEEE计算机视觉与模式识别会议论文集*，2009年。'
- en: '[151] S. Gould, R. Fulton, and D. Koller, “Decomposing a scene into geometric
    and semantically consistent regions,” in *2009 IEEE 12th international conference
    on computer vision*.   IEEE, 2009, pp. 1–8.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] S. Gould、R. Fulton 和 D. Koller，“将场景分解为几何和语义一致的区域”，发表于 *2009 IEEE第十二届国际计算机视觉会议*。IEEE，2009年，第1–8页。'
- en: '[152] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented
    natural images and its application to evaluating segmentation algorithms and measuring
    ecological statistics,” in *Proc. 8th Int’l Conf. Computer Vision*, vol. 2, July
    2001, pp. 416–423.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] D. Martin、C. Fowlkes、D. Tal 和 J. Malik，“一个人类分割自然图像的数据库及其在评估分割算法和测量生态统计中的应用”，发表于
    *第八届国际计算机视觉会议论文集*，第2卷，2001年7月，第416–423页。'
- en: '[153] A. Prest, C. Leistner, J. Civera, C. Schmid, and V. Ferrari, “Learning
    object class detectors from weakly annotated video,” in *2012 IEEE Conference
    on Computer Vision and Pattern Recognition*.   IEEE, 2012, pp. 3282–3289.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] A. Prest、C. Leistner、J. Civera、C. Schmid 和 V. Ferrari，“从弱标注视频中学习对象类别检测器”，发表于
    *2012 IEEE计算机视觉与模式识别会议*。IEEE，2012年，第3282–3289页。'
- en: '[154] S. D. Jain and K. Grauman, “Supervoxel-consistent foreground propagation
    in video,” in *European conference on computer vision*.   Springer, 2014, pp.
    656–671.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] S. D. Jain 和 K. Grauman，“视频中的超像素一致前景传播”，发表于 *欧洲计算机视觉会议*。Springer，2014年，第656–671页。'
- en: '[155] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:
    The kitti dataset,” *The International Journal of Robotics Research*, vol. 32,
    no. 11, pp. 1231–1237, 2013.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] A. Geiger、P. Lenz、C. Stiller 和 R. Urtasun，“视觉与机器人技术的融合：KITTI数据集”，*国际机器人研究杂志*，第32卷，第11期，第1231–1237页，2013年。'
- en: '[156] J. M. Alvarez, T. Gevers, Y. LeCun, and A. M. Lopez, “Road scene segmentation
    from a single image,” in *European Conference on Computer Vision*.   Springer,
    2012, pp. 376–389.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. M. Alvarez、T. Gevers、Y. LeCun 和 A. M. Lopez，“从单张图像中进行道路场景分割”，发表于 *欧洲计算机视觉会议*。Springer，2012年，第376–389页。'
- en: '[157] B. Hariharan, P. Arbeláez, L. Bourdev, S. Maji, and J. Malik, “Semantic
    contours from inverse detectors,” in *2011 International Conference on Computer
    Vision*.   IEEE, 2011, pp. 991–998.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] B. Hariharan、P. Arbeláez、L. Bourdev、S. Maji 和 J. Malik，“从逆检测器中提取语义轮廓”，发表于
    *2011国际计算机视觉会议*。IEEE，2011年，第991–998页。'
- en: '[158] X. Chen, R. Mottaghi, X. Liu, S. Fidler, R. Urtasun, and A. Yuille, “Detect
    what you can: Detecting and representing objects using holistic models and body
    parts,” in *IEEE Conference on Computer Vision and Pattern Recognition*, 2014,
    pp. 1971–1978.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] X. Chen、R. Mottaghi、X. Liu、S. Fidler、R. Urtasun 和 A. Yuille，“尽可能检测：使用整体模型和身体部位检测和表示对象”，发表于
    *IEEE计算机视觉与模式识别会议*，2014年，第1971–1978页。'
- en: '[159] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez, “The
    synthia dataset: A large collection of synthetic images for semantic segmentation
    of urban scenes,” in *IEEE conference on computer vision and pattern recognition*,
    2016, pp. 3234–3243.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] G. Ros、L. Sellart、J. Materzynska、D. Vazquez 和 A. M. Lopez，“Synthia数据集：用于城市场景语义分割的大量合成图像”，发表于
    *IEEE计算机视觉与模式识别会议*，2016年，第3234–3243页。'
- en: '[160] X. Shen, A. Hertzmann, J. Jia, S. Paris, B. Price, E. Shechtman, and
    I. Sachs, “Automatic portrait segmentation for image stylization,” in *Computer
    Graphics Forum*, vol. 35, no. 2.   Wiley Online Library, 2016, pp. 93–102.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] X. Shen, A. Hertzmann, J. Jia, S. Paris, B. Price, E. Shechtman, 和 I.
    Sachs，“用于图像风格化的自动肖像分割”，发表于*计算机图形学论坛*，第35卷，第2期。Wiley在线图书馆，2016年，第93–102页。'
- en: '[161] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, “Indoor segmentation
    and support inference from rgbd images,” in *European Conference on Computer Vision*.   Springer,
    2012, pp. 746–760.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] N. Silberman, D. Hoiem, P. Kohli, 和 R. Fergus，“从RGBD图像中进行室内分割和支撑推断”，发表于*欧洲计算机视觉会议*。Springer，2012年，第746–760页。'
- en: '[162] J. Xiao, A. Owens, and A. Torralba, “Sun3d: A database of big spaces
    reconstructed using sfm and object labels,” in *IEEE International Conference
    on Computer Vision*, 2013, pp. 1625–1632.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] J. Xiao, A. Owens, 和 A. Torralba，“Sun3d：一个使用SFM和对象标签重建的大空间数据库”，发表于*IEEE国际计算机视觉会议*，2013年，第1625–1632页。'
- en: '[163] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene understanding
    benchmark suite,” in *IEEE conference on computer vision and pattern recognition*,
    2015, pp. 567–576.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] S. Song, S. P. Lichtenberg, 和 J. Xiao，“Sun rgb-d：RGB-D场景理解基准套件”，发表于*IEEE计算机视觉与模式识别会议*，2015年，第567–576页。'
- en: '[164] K. Lai, L. Bo, X. Ren, and D. Fox, “A large-scale hierarchical multi-view
    rgb-d object dataset,” in *2011 IEEE international conference on robotics and
    automation*.   IEEE, 2011, pp. 1817–1824.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] K. Lai, L. Bo, X. Ren, 和 D. Fox，“一个大规模分层多视角RGB-D对象数据集”，发表于*2011年IEEE国际机器人与自动化会议*。IEEE，2011年，第1817–1824页。'
- en: '[165] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
    “Scannet: Richly-annotated 3d reconstructions of indoor scenes,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2017, pp.
    5828–5839.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, 和 M. Nießner，“Scannet：丰富注释的室内场景3D重建”，发表于*IEEE计算机视觉与模式识别会议论文集*，2017年，第5828–5839页。'
- en: '[166] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese, “Joint 2D-3D-Semantic
    Data for Indoor Scene Understanding,” *ArXiv e-prints*, Feb. 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] I. Armeni, A. Sax, A. R. Zamir, 和 S. Savarese，“用于室内场景理解的联合2D-3D-语义数据”，*ArXiv电子预印本*，2017年2月。'
- en: '[167] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su *et al.*, “Shapenet: An information-rich
    3d model repository,” *arXiv preprint arXiv:1512.03012*, 2015.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su *等*，“Shapenet：一个信息丰富的3D模型库”，*arXiv预印本 arXiv:1512.03012*，2015年。'
- en: '[168] L. Yi, L. Shao, M. Savva, H. Huang, Y. Zhou, Q. Wang, B. Graham, M. Engelcke,
    R. Klokov, V. Lempitsky *et al.*, “Large-scale 3d shape reconstruction and segmentation
    from shapenet core55,” *arXiv preprint arXiv:1710.06104*, 2017.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] L. Yi, L. Shao, M. Savva, H. Huang, Y. Zhou, Q. Wang, B. Graham, M. Engelcke,
    R. Klokov, V. Lempitsky *等*，“从shapenet core55进行大规模3D形状重建和分割”，*arXiv预印本 arXiv:1710.06104*，2017年。'
- en: '[169] M. De Deuge, A. Quadros, C. Hung, and B. Douillard, “Unsupervised feature
    learning for classification of outdoor 3d scans,” in *Australasian Conference
    on Robitics and Automation*, vol. 2, 2013, p. 1.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] M. De Deuge, A. Quadros, C. Hung, 和 B. Douillard，“用于户外3D扫描分类的无监督特征学习”，发表于*澳大利亚机器人与自动化会议*，第2卷，2013年，第1页。'
- en: '[170] C.-Y. Fu, M. Shvets, and A. C. Berg, “Retinamask: Learning to predict
    masks improves state-of-the-art single-shot detection for free,” *arXiv preprint
    arXiv:1901.03353*, 2019.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C.-Y. Fu, M. Shvets, 和 A. C. Berg，“Retinamask：学习预测掩膜提升现有单次检测技术”，*arXiv预印本
    arXiv:1901.03353*，2019年。'
- en: '[171] P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Dollár, “Learning to
    refine object segments,” in *European Conference on Computer Vision*.   Springer,
    2016, pp. 75–91.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] P. O. Pinheiro, T.-Y. Lin, R. Collobert, 和 P. Dollár，“学习优化对象分割”，发表于*欧洲计算机视觉会议*。Springer，2016年，第75–91页。'
- en: '[172] H. Liu, C. Peng, C. Yu, J. Wang, X. Liu, G. Yu, and W. Jiang, “An end-to-end
    network for panoptic segmentation,” in *Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition*, 2019, pp. 6172–6181.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] H. Liu, C. Peng, C. Yu, J. Wang, X. Liu, G. Yu, 和 W. Jiang，“一种用于全景分割的端到端网络”，发表于*IEEE计算机视觉与模式识别会议论文集*，2019年，第6172–6181页。'
- en: '[173] K. Sofiiuk, O. Barinova, and A. Konushin, “Adaptis: Adaptive instance
    selection network,” in *Proceedings of the IEEE International Conference on Computer
    Vision*, 2019, pp. 7355–7363.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] K. Sofiiuk, O. Barinova, 和 A. Konushin，“Adaptis：自适应实例选择网络”，发表于*IEEE国际计算机视觉会议论文集*，2019年，第7355–7363页。'
- en: '[174] J. Lazarow, K. Lee, K. Shi, and Z. Tu, “Learning instance occlusion for
    panoptic segmentation,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 10 720–10 729.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] J. Lazarow, K. Lee, K. Shi 和 Z. Tu，“面向全景分割的实例遮挡学习”，发表于 *IEEE/CVF 计算机视觉与模式识别大会论文集*，2020年，第10 720–10 729页。'
- en: '[175] Z. Deng, S. Todorovic, and L. Jan Latecki, “Semantic segmentation of
    rgbd images with mutex constraints,” in *Proceedings of the IEEE international
    conference on computer vision*, 2015, pp. 1733–1741.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Deng, S. Todorovic 和 L. Jan Latecki，“带有互斥约束的 rgbd 图像语义分割”，发表于 *IEEE
    国际计算机视觉大会论文集*，2015年，第1733–1741页。'
- en: '[176] D. Eigen and R. Fergus, “Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture,” in *IEEE international
    conference on computer vision*, 2015, pp. 2650–2658.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] D. Eigen 和 R. Fergus，“使用通用多尺度卷积架构预测深度、表面法线和语义标签”，发表于 *IEEE 国际计算机视觉大会*，2015年，第2650–2658页。'
- en: '[177] A. Mousavian, H. Pirsiavash, and J. Kosecka, “Joint semantic segmentation
    and depth estimation with deep convolutional networks,” in *International Conference
    on 3D Vision*.   IEEE, 2016.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] A. Mousavian, H. Pirsiavash 和 J. Kosecka，“使用深度卷积网络的联合语义分割和深度估计”，发表于 *3D
    视觉国际会议*。IEEE，2016年。'
- en: '[178] X. Qi, R. Liao, J. Jia, S. Fidler, and R. Urtasun, “3d graph neural networks
    for rgbd semantic segmentation,” in *IEEE International Conference on Computer
    Vision*, 2017, pp. 5199–5208.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] X. Qi, R. Liao, J. Jia, S. Fidler 和 R. Urtasun，“用于 rgbd 语义分割的 3d 图神经网络”，发表于
    *IEEE 国际计算机视觉大会*，2017年，第5199–5208页。'
- en: '[179] W. Wang and U. Neumann, “Depth-aware cnn for rgb-d segmentation,” in
    *Proceedings of the European Conference on Computer Vision (ECCV)*, 2018, pp.
    135–150.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] W. Wang 和 U. Neumann，“基于深度的 cnn 用于 rgb-d 分割”，发表于 *欧洲计算机视觉会议 (ECCV) 论文集*，2018年，第135–150页。'
- en: '[180] S.-J. Park, K.-S. Hong, and S. Lee, “Rdfnet: Rgb-d multi-level residual
    feature fusion for indoor semantic segmentation,” in *IEEE International Conference
    on Computer Vision*, 2017, pp. 4980–4989.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] S.-J. Park, K.-S. Hong 和 S. Lee，“Rdfnet: 用于室内语义分割的 rgb-d 多层残差特征融合”，发表于
    *IEEE 国际计算机视觉大会*，2017年，第4980–4989页。'
- en: '[181] J. Jiao, Y. Wei, Z. Jie, H. Shi, R. W. Lau, and T. S. Huang, “Geometry-aware
    distillation for indoor semantic segmentation,” in *IEEE Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 2869–2878.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] J. Jiao, Y. Wei, Z. Jie, H. Shi, R. W. Lau 和 T. S. Huang，“面向室内语义分割的几何感知蒸馏”，发表于
    *IEEE 计算机视觉与模式识别大会*，2019年，第2869–2878页。'
- en: '[182] Z.-H. Zhou, “A brief introduction to weakly supervised learning,” *National
    Science Review*, vol. 5, no. 1, pp. 44–53, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Z.-H. Zhou，“弱监督学习简要介绍”，*国家科学评论*，第5卷，第1期，第44–53页，2018年。'
- en: '[183] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2020.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] L. Jing 和 Y. Tian，“使用深度神经网络的自监督视觉特征学习：综述”，*IEEE 模式分析与机器智能学报*，2020年。'
- en: '[184] V. Goel, J. Weng, and P. Poupart, “Unsupervised video object segmentation
    for deep reinforcement learning,” in *Advances in Neural Information Processing
    Systems*, 2018, pp. 5683–5694.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] V. Goel, J. Weng 和 P. Poupart，“用于深度强化学习的无监督视频对象分割”，发表于 *神经信息处理系统进展*，2018年，第5683–5694页。'
- en: '[185] L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin, and B. A. Johnson, “Deep learning
    in remote sensing applications: A meta-analysis and review,” *ISPRS Journal of
    Photogrammetry and Remote Sensing*, vol. 152, pp. 166 – 177, 2019.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] L. Ma, Y. Liu, X. Zhang, Y. Ye, G. Yin 和 B. A. Johnson，“深度学习在遥感应用中的应用：元分析和综述”，*ISPRS
    摄影测量与遥感杂志*，第152卷，第166–177页，2019年。'
- en: '[186] L. Gao, Y. Zhang, F. Zou, J. Shao, and J. Lai, “Unsupervised urban scene
    segmentation via domain adaptation,” *Neurocomputing*, vol. 406, pp. 295 – 301,
    2020.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] L. Gao, Y. Zhang, F. Zou, J. Shao 和 J. Lai，“通过领域适配进行无监督城市场景分割”，*神经计算*，第406卷，第295–301页，2020年。'
- en: '[187] M. Paoletti, J. Haut, J. Plaza, and A. Plaza, “Deep learning classifiers
    for hyperspectral imaging: A review,” *ISPRS Journal of Photogrammetry and Remote
    Sensing*, vol. 158, pp. 279 – 317, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] M. Paoletti, J. Haut, J. Plaza 和 A. Plaza，“高光谱成像的深度学习分类器：综述”，*ISPRS 摄影测量与遥感杂志*，第158卷，第279–317页，2019年。'
- en: '[188] J. F. Abrams, A. Vashishtha, S. T. Wong, A. Nguyen, A. Mohamed, S. Wieser,
    A. Kuijper, A. Wilting, and A. Mukhopadhyay, “Habitat-net: Segmentation of habitat
    images using deep learning,” *Ecological Informatics*, vol. 51, pp. 121 – 128,
    2019.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] J. F. Abrams, A. Vashishtha, S. T. Wong, A. Nguyen, A. Mohamed, S. Wieser,
    A. Kuijper, A. Wilting 和 A. Mukhopadhyay，“Habitat-net: 使用深度学习进行栖息地图像分割”，*生态信息学*，第51卷，第121–128页，2019年。'
- en: '[189] M. Kerkech, A. Hafiane, and R. Canals, “Vine disease detection in uav
    multispectral images using optimized image registration and deep learning segmentation
    approach,” *Computers and Electronics in Agriculture*, vol. 174, p. 105446, 2020.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] M. Kerkech, A. Hafiane 和 R. Canals，《利用优化图像配准和深度学习分割方法在无人机多光谱图像中检测葡萄藤病害》，*农业中的计算机与电子学*，第174卷，第105446页，2020年。'
- en: '[190] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding,
    “Embracing imperfect datasets: A review of deep learning solutions for medical
    image segmentation,” *Medical Image Analysis*, vol. 63, p. 101693, 2020.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu 和 X. Ding，《接受不完美的数据集：深度学习解决方案在医学图像分割中的综述》，*医学图像分析*，第63卷，第101693页，2020年。'
- en: '[191] A. Amyar, R. Modzelewski, H. Li, and S. Ruan, “Multi-task deep learning
    based ct imaging analysis for covid-19 pneumonia: Classification and segmentation,”
    *Computers in Biology and Medicine*, vol. 126, p. 104037, 2020.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] A. Amyar, R. Modzelewski, H. Li 和 S. Ruan，《基于多任务深度学习的 COVID-19 肺炎 CT
    成像分析：分类与分割》，*生物医学计算机*，第126卷，第104037页，2020年。'
- en: '[192] Y. Song, Z. Huang, C. Shen, H. Shi, and D. A. Lange, “Deep learning-based
    automated image segmentation for concrete petrographic analysis,” *Cement and
    Concrete Research*, vol. 135, p. 106118, 2020.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Song, Z. Huang, C. Shen, H. Shi 和 D. A. Lange，《基于深度学习的混凝土岩相分析自动图像分割》，*水泥与混凝土研究*，第135卷，第106118页，2020年。'
- en: '| ![[Uncaptioned image]](img/17fbdce8d4f5ec5144b2cbdeda059774.png) | Shervin
    Minaee is a machine learning lead at Snapchat computer vision team. He received
    his PhD in Electrical Engineering and Computer Science from NYU, in 2018. His
    research interest includes computer vision, image segmentation, biometrics recognition,
    and applied deep learning. He has published more than 40 papers and patents during
    his PhD. He has previously worked as a research scientist at Samsung Research,
    AT&T Labs, Huawei Labs, and as a data scientist at Expedia group. He is a reviewer
    for more than 20 computer vision related journals from IEEE, ACM, and Elsevier.
    |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/17fbdce8d4f5ec5144b2cbdeda059774.png) | Shervin Minaee 是
    Snapchat 计算机视觉团队的机器学习负责人。他于2018年获得纽约大学电气工程与计算机科学博士学位。他的研究兴趣包括计算机视觉、图像分割、生物识别和应用深度学习。他在博士期间发表了40多篇论文和专利。他曾在三星研究所、AT&T
    实验室、华为实验室担任研究科学家，并在 Expedia 集团担任数据科学家。他是 IEEE、ACM 和 Elsevier 等20多本计算机视觉相关期刊的审稿人。'
- en: '| ![[Uncaptioned image]](img/1f387ae29cd870358e87ae691328b33a.png) | Yuri Boykov
    is a Professor at Cheriton School of Computer Science at the University of Waterloo.
    His research is concentrated in the area of computer vision and biomedical image
    analysis with focus on modeling and optimization for structured segmentation,
    restoration, registration, stereo, motion, model fitting, recognition, photo-video
    editing and other data analysis problems. He is an editor for the International
    Journal of Computer Vision (IJCV). His work was listed among 10 most influential
    papers in IEEE TPAMI (Top Picks for 30 years). In 2017 Google Scholar listed his
    work on segmentation as a ”classic paper in computer vision and pattern recognition”
    (from 2006). In 2011 he received Helmholtz Prize from IEEE and Test of Time Award
    by the International Conference on Computer Vision. |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/1f387ae29cd870358e87ae691328b33a.png) | Yuri Boykov 是滑铁卢大学
    Cheriton 计算机科学学院的教授。他的研究集中在计算机视觉和生物医学图像分析领域，重点是结构化分割、恢复、配准、立体、运动、模型拟合、识别、照片视频编辑和其他数据分析问题的建模与优化。他是《国际计算机视觉期刊》（IJCV）的编辑。他的工作曾被列为
    IEEE TPAMI（30年来的精选）中10篇最具影响力的论文之一。2017年，Google Scholar 将他的分割工作列为“计算机视觉和模式识别领域的经典论文”（自2006年起）。2011年，他获得了
    IEEE 赫尔姆霍兹奖和国际计算机视觉大会的时效测试奖。'
- en: '|  | Fatih Porikli is an IEEE Fellow and a Senior Director at Qualcomm, San
    Diego. He was a full Professor in the Research School of Engineering, Australian
    National University and a Vice President at Huawei CBG Device; Hardware, San Diego
    until recently. He led the Computer Vision Research Group at NICTA, Australia
    and research projects as a Distinguished Research Scientist at Mitsubishi Electric
    Research Laboratories, Cambridge. He received his Ph.D. from New York University
    in 2002\. He was the recipient of the R&D 100 Scientist of the Year Award in 2006\.
    He won six best paper awards, authored more than 250 papers, co-edited two books,
    and invented over 100 patents. He served as the General Chair and Technical Program
    Chair of many IEEE conferences and an Associate Editor of premier IEEE and Springer
    journals for the past 15 years. |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '|  | **法提赫·波尔克利**是IEEE院士，现任高通公司圣地亚哥的高级董事。他曾在澳大利亚国立大学工程研究学院担任全职教授，并在华为CBG设备业务部担任副总裁，直到最近。他曾领导NICTA澳大利亚的计算机视觉研究组，并在三菱电机研究实验室剑桥担任杰出研究科学家，负责研究项目。他于2002年从纽约大学获得博士学位。他在2006年获得了R&D
    100年度科学家奖。他获得了六项最佳论文奖，发表了250多篇论文，主编了两本书籍，并获得了100多项专利。他在过去15年中担任了多个IEEE会议的总主席和技术程序主席，并担任了IEEE和Springer主要期刊的副编辑。
    |'
- en: '| ![[Uncaptioned image]](img/4490ecbf5dde3580c0c20246212c1b15.png) | Prof.
    Antonio Plaza is an IEEE Fellow and a professor at the Department of Technology
    of Computers and Communications, University of Extremadura, where he received
    the M.Sc. degree in 1999 and the PhD degree in 2002, both in Computer Engineering.
    He has authored more than 600 publications, including 300 JCR journal papers (more
    than 170 in IEEE journals), 24 book chapters, and over 300 peer-reviewed conference
    proceeding papers. He is a recipient of the Best Column Award of the IEEE Signal
    Processing Magazine in 2015, the 2013 Best Paper Award of the JSTARS journal,
    and the most highly cited paper (2005-2010) in the Journal of Parallel and Distributed
    Computing. He is included in the 2018 and 2019 Highly Cited Researchers List.
    |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/4490ecbf5dde3580c0c20246212c1b15.png) | **安东尼奥·普拉萨**教授是IEEE院士，现任极端大区大学计算机与通信技术系教授，1999年获得硕士学位，2002年获得计算机工程博士学位。他已发表了600多篇论文，其中包括300篇JCR期刊论文（其中170篇以上在IEEE期刊上）、24章书籍章节以及300多篇同行评审的会议论文。他获得了2015年IEEE信号处理杂志最佳专栏奖、2013年JSTARS期刊最佳论文奖，以及2005-2010年《并行与分布计算期刊》最被引用论文奖。他被列入2018年和2019年高被引研究人员名单。
    |'
- en: '|  | Nasser Kehtarnavaz is a Distinguished Professor at the Department of Electrical
    and Computer Engineering at the University of Texas at Dallas, Richardson, TX.
    His research interests include signal and image processing, machine learning,
    and real-time implementation on embedded processors. He has authored or co-authored
    ten books and more than 390 journal papers, conference papers, patents, manuals,
    and editorials in these areas. He is a Fellow of SPIE, a licensed Professional
    Engineer, and Editor-in-Chief of Journal of Real-Time Image Processing. |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '|  | **纳赛尔·克赫塔纳瓦兹**是德克萨斯大学达拉斯分校电气与计算机工程系的杰出教授。他的研究兴趣包括信号和图像处理、机器学习以及嵌入式处理器上的实时实现。他已在这些领域撰写或合著了十本书籍和390多篇期刊论文、会议论文、专利、手册及社论。他是SPIE院士、注册专业工程师，并担任《实时图像处理期刊》的主编。
    |'
- en: '| ![[Uncaptioned image]](img/37f22226b29a6044e88fa3984efb0385.png) | Demetri
    Terzopoulos is a Distinguished Professor of Computer Science at the University
    of California, Los Angeles, where he directs the UCLA Computer Graphics & Vision
    Laboratory. He is also Co-Founder and Chief Scientist of VoxelCloud, Inc. He received
    his PhD degree from Massachusetts Institute of Technology (MIT) in 1984. He is
    or was a Guggenheim Fellow, a Fellow of the ACM, IEEE, Royal Society of Canada,
    and Royal Society of London, and a Member of the European Academy of Sciences,
    the New York Academy of Sciences, and Sigma Xi. Among his many awards are an Academy
    Award from the Academy of Motion Picture Arts and Sciences for his pioneering
    work on physics-based computer animation, and the inaugural Computer Vision Distinguished
    Researcher Award from the IEEE for his pioneering and sustained research on deformable
    models and their applications. ISI and other indexes have listed him among the
    most highly-cited authors in engineering and computer science, with more than
    400 published research papers and several volumes.Previously, he was Professor
    of Computer Science and Professor of Electrical and Computer Engineering at the
    University of Toronto. Before becoming an academic in 1989, he was a Program Leader
    at Schlumberger corporate research centers in California and Texas. |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图像]](img/37f22226b29a6044e88fa3984efb0385.png) | Demetri Terzopoulos
    是加州大学洛杉矶分校计算机科学的杰出教授，负责领导 UCLA 计算机图形与视觉实验室。他还是 VoxelCloud, Inc. 的联合创始人兼首席科学家。他于1984年获得麻省理工学院（MIT）的博士学位。他曾是古根海姆研究员、ACM、IEEE、加拿大皇家学会和伦敦皇家学会的会员，并且是欧洲科学院、纽约科学院和Sigma
    Xi的成员。他获得的许多奖项包括由美国电影艺术与科学学院颁发的学院奖，以表彰他在基于物理的计算机动画方面的开创性工作，以及 IEEE 颁发的首届计算机视觉杰出研究者奖，以表彰他在可变形模型及其应用方面的开创性和持续性研究。ISI
    和其他索引将他列为工程和计算机科学领域被引用最多的作者之一，发表了超过400篇研究论文和几部专著。此前，他曾是多伦多大学的计算机科学教授和电气与计算机工程教授。在1989年成为学术界人士之前，他在加州和德克萨斯州的施耐德公司研究中心担任项目负责人。'
