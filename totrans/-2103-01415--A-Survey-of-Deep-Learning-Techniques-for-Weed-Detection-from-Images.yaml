- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:56:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2103.01415] A Survey of Deep Learning Techniques for Weed Detection from Images'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.01415](https://ar5iv.labs.arxiv.org/html/2103.01415)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \usesmartdiagramlibrary
  prefs: []
  type: TYPE_NORMAL
- en: additions
  prefs: []
  type: TYPE_NORMAL
- en: A Survey of Deep Learning Techniques for Weed Detection from Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A S M Mahmudul Hasan¹¹1Corresponding author email: 33916214@student.murdoch.edu.au
    Information Technology, Murdoch University, Murdoch, WA 6150, Australia Centre
    for Crop and Food Innovation, Food Futures Institute, Murdoch University, Murdoch,
    WA 6150, Australia Ferdous Sohel Information Technology, Murdoch University, Murdoch,
    WA 6150, Australia Centre for Crop and Food Innovation, Food Futures Institute,
    Murdoch University, Murdoch, WA 6150, Australia Dean Diepeveen Centre for Crop
    and Food Innovation, Food Futures Institute, Murdoch University, Murdoch, WA 6150,
    Australia Department of Primary Industries and Regional Development, Western Australia,
    South Perth, WA, 6151, Australia Centre for Sustainable Farming Systems, Murdoch
    University, Murdoch, WA 6150, Australia Hamid Laga Information Technology, Murdoch
    University, Murdoch, WA 6150, Australia Centre of Biosecurity and One Health,
    Harry Butler Institute, Murdoch University, Murdoch University, Murdoch, WA 6150,
    Australia Michael G.K. Jones Centre for Crop and Food Innovation, Food Futures
    Institute, Murdoch University, Murdoch, WA 6150, Australia'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rapid advances in Deep Learning (DL) techniques have enabled rapid detection,
    localisation, and recognition of objects from images or videos. DL techniques
    are now being used in many applications related to agriculture and farming. Automatic
    detection and classification of weeds can play an important role in weed management
    and so contribute to higher yields. Weed detection in crops from imagery is inherently
    a challenging problem because both weeds and crops have similar colours (‘green-on-green’),
    and their shapes and texture can be very similar at the growth phase. Also, a
    crop in one setting can be considered a weed in another. In addition to their
    detection, the recognition of specific weed species is essential so that targeted
    controlling mechanisms (e.g. appropriate herbicides and correct doses) can be
    applied. In this paper, we review existing deep learning-based weed detection
    and classification techniques. We cover the detailed literature on four main procedures,
    i.e., data acquisition, dataset preparation, DL techniques employed for detection,
    location and classification of weeds in crops, and evaluation metrics approaches.
    We found that most studies applied supervised learning techniques, they achieved
    high classification accuracy by fine-tuning pre-trained models on any plant dataset,
    and past experiments have already achieved high accuracy when a large amount of
    labelled data is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords: Deep learning, Weed detection, Weed classification, Machine Learning,
    Digital agriculture.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The world population has been increasing rapidly, and it is expected to reach
    nine billion by 2050\. Agricultural production needs to increase by about 70%
    to meet the anticipated demands [[125](#bib.bibx125)]. However, the agricultural
    sector will face many challenges during this time, including a reduction of cultivatable
    land and the need for more intensive production. Other issues, such as climate
    change and water scarcity, will also affect productivity. Precision agriculture
    or digital agriculture can provide strategies to mitigate these issues [[85](#bib.bibx85),
    [139](#bib.bibx139), [125](#bib.bibx125)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Weeds are plants that can spread quickly and undesirably, and can impact on
    crop yields and quality [[118](#bib.bibx118)]. Weeds compete with crops for nutrition,
    water, sunlight, and growing space [[67](#bib.bibx67)]. Therefore, farmers have
    to deploy resources to reduce weeds. The management strategies used to reduce
    the impact of weeds depend on many factors. These strategies can be categorised
    into five main types [[137](#bib.bibx137)]: ‘preventative’ (prevent weeds from
    becoming established), ‘cultural’ (by maintaining field hygiene – low weed seed
    bank), ‘mechanical’ (e.g., mowing, mulching and tilling), ‘biological’ (using
    natural enemies of weeds such as insects, grazing animals or disease), and ‘chemical’
    (application of herbicides). These approaches all have drawbacks. In general,
    there is a financial burden and they require time and extra work. In addition,
    control treatments may impact the health of people, plants, soil, animals, or
    the environment [[112](#bib.bibx112), [137](#bib.bibx137), [59](#bib.bibx59)].'
  prefs: []
  type: TYPE_NORMAL
- en: As the costs of labour has increased, and people have become more concerned
    about health and environmental issues, automation of weed control has become desirable
    [[99](#bib.bibx99)]. Automated weed control systems can be beneficial both economically
    and environmentally. Such systems can reduce labour costs by using a machine to
    remove weeds and, selective spraying techniques can minimise the use of the herbicides
    [[87](#bib.bibx87)].
  prefs: []
  type: TYPE_NORMAL
- en: To develop an automatic weed management system, an essential first step is to
    be able to detect and recognise weeds correctly [[99](#bib.bibx99)]. Detection
    of weeds in crops is challenging as weeds and crop plants often have similar colours,
    textures, and shapes. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") shows crop plants
    with weeds growing amongst them. Common challenges in detection and classification
    of crops and weeds are occlusion (Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), similarity
    in colour and texture (Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), plants
    shadowed in natural light (Figure [1(c)](#S1.F1.sf3 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), colour
    and texture variations due to lighting conditions and illumination (Figure [1(d)](#S1.F1.sf4
    "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images")) and different species of weeds which appear similar (Figure
    [1(e)](#S1.F1.sf5 "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images")). Same crop plants or weeds may show dissimilarities
    during growth phases (Figure [1(f)](#S1.F1.sf6 "In Figure 1 ‣ 1 Introduction ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")). Motion
    blur and noise in the image also increase the difficulty in classifying plants
    (Figure [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")). In addition, depending on the geographical
    location (Figure [1(h)](#S1.F1.sf8 "In Figure 1 ‣ 1 Introduction ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images")) and the variety of
    the crop, weather and soil conditions, the species of weeds can vary [[70](#bib.bibx70)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/085f5ae511ca127e91de98360bd959bb.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Occlusion of crop and weed [[54](#bib.bibx54)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/194c24f66d6e6842dc66b9e2bead8edc.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Colour and texture similarities between crop and weed plants [[12](#bib.bibx12)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80d2e04c1bf666c4f24d5fcd99bc664a.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Shadow effects in natural weed image [[123](#bib.bibx123)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/38d77e08f38de3efafe3b3497077cc88.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Effects of illumination conditions [[33](#bib.bibx33)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/407cfe3028f66e91448d813473ceb626.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Four different species of weeds that share similarities (inter-class similarity)
    [[113](#bib.bibx113)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d7fead47e3714cfd6f0d890bdac7200.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Sugar beet crop at different growth stages (intra-class variations) [[49](#bib.bibx49)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ab4e4116cd982f20ef29eb9676a7a5f.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) Effects of motion blur and noise [[49](#bib.bibx49), [3](#bib.bibx3)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42779da4bd4433db68ea7ca56ec8c650.png)'
  prefs: []
  type: TYPE_IMG
- en: '(h) Weeds can vary at different geographic/weather locations: weed in carrot
    crop collected from Germany(left) [[54](#bib.bibx54)] and Macedonia (Right) [[88](#bib.bibx88)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Weeds in different crops (green boxes indicate crops and red boxes
    indicate weeds).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical weed detection system follows four key steps: image acquisition,
    pre-processing of images, extraction of features and detection and classification
    of weeds [[140](#bib.bibx140)]. Different emerging technologies have been used
    to accomplish these steps. The most crucial part of these steps is weed detection
    and classification. In recent years, with advances in computer technologies, particularly
    in graphical processing units (GPU), embedded processors coupled with the use
    of Machine Learning (ML) techniques have become more widely used for automatic
    detection of weed species [[91](#bib.bibx91), [50](#bib.bibx50), [171](#bib.bibx171)].'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning (DL) is an important branch of ML. For image classification, object
    detection, and recognition, DL algorithms have many advantages over traditional
    ML approaches (in this paper, the term machine learning, we mean traditional machine
    learning approaches). Extracting and selecting discriminating features with ML
    methods is difficult because crops and weeds can be similar. This problem can
    be addressed efficiently by using DL approaches based on their strong feature
    learning capabilities. Recently, many research articles have been published on
    DL-based weed recognition, yet few review articles have been published on this
    topic. Su [[149](#bib.bibx149)] recently published a review paper in which the
    main focus was on the use of point spectroscopy, RGB, and hyperspectral imaging
    to classify weeds in crops automatically. However, most of the articles covered
    in this review have applied traditional machine learning approaches, with few
    citations of recent papers. [[99](#bib.bibx99)] analysed a number of publications
    on weed detection, but from the perspective of selective spraying.
  prefs: []
  type: TYPE_NORMAL
- en: We provide this comprehensive literature survey to highlight the great potential
    now presented by different DL techniques for detecting, localising, and classifying
    weeds in crops. We present a taxonomy of the DL techniques for weed detection
    and recognition, and classify major publications based on that taxonomy. We also
    cover data collection, data preparation, and data representation approaches. We
    provide an overview of different evaluation metrics used to benchmark the performance
    of the techniques surveyed in this article.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the paper is organised as follows. Existing review papers in this
    area are discussed briefly in Section [2](#S2 "2 Related Surveys ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images"). Advantages of DL-based
    weed detection approaches over traditional ML methods are discussed in Section
    [3](#S3 "3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images"). In Section [4](#S4 "4 Paper
    Selection Criteria in this Survey ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images"), we describe how the papers for review were selected.
    A taxonomy and an overview of DL-based weed detection techniques are provided
    in Section [5](#S5 "5 An Overview and Taxonomy of Deep Learning-based Weed Detection
    Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection from Images").
    We describe four major steps of DL-based approaches, i.e. data acquisition (Section
    [6](#S6 "6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images")), dataset preparation (Section [7](#S7 "7 Dataset Preparation ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")), detection
    and classification methods (Section [10](#S10 "10 Deep Learning Architecture ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")) and evaluation
    metrics (Section [11](#S11 "11 Performance Evaluation Metrics ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")). In Section [8](#S8 "8 Detection
    Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")
    we have highlighted the approaches to detection of weeds in crop plants adopted
    in the related work. The learning methods applied the relevant studies are explained
    in Section [9](#S9 "9 Learning Methods ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images"). We summarise the current state in this field
    and provide future directions in Section [12](#S12 "12 Discussion ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images") with conclusions are
    provided in Section [13](#S13 "13 Conclusion ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Surveys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML and DL techniques have been used for weed detection, recognition and thus
    for weed management. In 2018, [[73](#bib.bibx73)] published a survey of 40 research
    papers that applied DL-techniques to address various agricultural problems, including
    weed detection. The study reported that DL-techniques outperformed more than traditional
    image processing methods.
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, [[106](#bib.bibx106)] discussed ten components that are essential and
    possible obstructions to develop a fully autonomous mechanical weed management
    system. With the advance in DL, it seems that the problems raised can now be addressed.
    [[6](#bib.bibx6)] articulated that DL-based plant classification modules can be
    deployed not only in weed management systems but also for fertilisation, irrigation,
    and phenotyping. Their study explained how “Deepfield Robotics” systems could
    reduce labour required for weed control in agriculture and horticulture.
  prefs: []
  type: TYPE_NORMAL
- en: '[[165](#bib.bibx165)] highlighted that the most challenging part of a weed
    detection techniques is to distinguish between weed and crop species. They focused
    on different machine vision and image processing techniques used for ground-based
    weed detection. [[21](#bib.bibx21)] made a similar observation. They reviewed
    remote sensing for weed mapping and ground-based detection techniques. They also
    reported the limitations of using either spectral or spatial features to identify
    weeds in crops. According to their study, it is preferable to use both features.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[45](#bib.bibx45)] reviewed technologies that can be used to monitor weeds
    in crops. They explored different remotely sensed and ground-based weed monitoring
    systems in agricultural fields. They reported that weed monitoring is essential
    for weed management. They foresaw that the data collected using different sensors
    could be stored in cloud systems for timely use in relevant contexts. In another
    study, [[109](#bib.bibx109)] evaluated a small number of DL approaches used for
    detecting weeds in crops. They identified research gaps, e.g., the lack of large
    crop-weed datasets, acceptable classification accuracy and lack of generalised
    models for detecting different crop plants and weed species. However, the article
    only covered a handful of publications and as such the paper was not thorough
    and did not adequately cover the breadth and depth of the literature.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Traditional ML- vs DL-based Weed Detection Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A typical ML-based weed classification technique follows five key steps: image
    acquisition, pre-processing such as image enhancement, feature extraction or with
    feature selection, applying an ML-based classifier and evaluation of the performance
    [[99](#bib.bibx99), [23](#bib.bibx23), [17](#bib.bibx17), [96](#bib.bibx96)].'
  prefs: []
  type: TYPE_NORMAL
- en: Different image processing methods have been applied for crop and weed classification
    [[167](#bib.bibx167), [57](#bib.bibx57), [158](#bib.bibx158)]. By extracting shape
    features, many researchers identify weeds and crops using discriminate analysis
    [[24](#bib.bibx24), [107](#bib.bibx107)]. In some other research, different colour
    [[175](#bib.bibx175), [69](#bib.bibx69), [52](#bib.bibx52), [75](#bib.bibx75)]
    and texture [[13](#bib.bibx13)] features were used.
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge in weed detection and classification is that both weeds and
    crops can have very similar colours or textures. Machine learning approaches learn
    the features from the training data that are available [[12](#bib.bibx12)]. Understandably,
    for traditional ML-approaches, the combination of multiple modalities of data
    e.g. the shape, texture and colour or a combination of multiple sensor data is
    expected to generate superior results to a single modality of data. [[80](#bib.bibx80)]
    argued that colour or texture features of an image alone are not adequate to classify
    wheat from weed species Bidens pilosa. They used Near-Infrared (NIR) image cues
    with those features. [[136](#bib.bibx136)] extracted eight texture features based
    on the grey level co-occurrence matrix (GLCM), two spectral descriptors of texture,
    thirteen different colour features, five moment-invariant features, and eight
    shape features. They compared the performance of several algorithms, such as the
    ant colony algorithm, simulated annealing method, and genetic algorithm for selecting
    more discriminative features. The performance of the Cultural Algorithm, Linear
    Discriminant Analysis (LDA), Support Vector Machine (SVM), and Random Forest classifiers
    were also evaluated to distinguish between crops and weeds.
  prefs: []
  type: TYPE_NORMAL
- en: '[[74](#bib.bibx74)] applied SVM for detecting weeds in corn from hyperspectral
    images. In other research, [[166](#bib.bibx166)] used SVM and LDA for classifying
    plants. They proposed a self-supervised approach for discrimination. Before training
    the models, they applied vegetation separation techniques to remove background
    and different spectral pre-processing to extract features using Principal Component
    Analysis (PCA). [[68](#bib.bibx68)] extracted different shape features and the
    feature vectors were evaluated using a single-layer perceptron classifier to distinguish
    narrow and broad-leafed weeds.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S3.F2.pic1" class="ltx_picture ltx_centering" height="465.68" overflow="visible"
    version="1.1" width="619.13"><g transform="translate(0,465.68) matrix(1 0 0 -1
    0 0) translate(55.72,0) translate(0,373.3)"><g stroke="#000000" fill="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -34.59 1.6)" fill="#000000"
    stroke="#000000"><foreignobject width="69.19" height="22.56" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Collect Data <g fill="#CCFFCC"><path d="M 177.84
    0 C 177.84 13.78 153.01 24.95 122.39 24.95 C 91.77 24.95 66.95 13.78 66.95 0 C
    66.95 -13.78 91.77 -24.95 122.39 -24.95 C 153.01 -24.95 177.84 -13.78 177.84 0
    Z M 122.39 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 87.8 3.34)" fill="#000000"
    stroke="#000000"><foreignobject width="69.19" height="26.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Use Public Data</foreignobject></g> <g fill="#FFCCCC"><path
    d="M 123.61 -47.36 L -12.91 -47.36 C -15.97 -47.36 -18.45 -49.84 -18.45 -52.89
    L -18.45 -81.19 C -18.45 -84.25 -15.97 -86.73 -12.91 -86.73 L 123.61 -86.73 C
    126.67 -86.73 129.15 -84.25 129.15 -81.19 L 129.15 -52.89 C 129.15 -49.84 126.67
    -47.36 123.61 -47.36 Z M -18.45 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 -13.84 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data Acquisition</foreignobject></g>
    <path d="M 17.8 -21.56 L 36.76 -44.52" style="fill:none"><g transform="matrix(0.63672
    -0.77109 0.77109 0.63672 36.76 -44.52)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 99.41 -22.98 L 77.65 -44.73" style="fill:none"><g
    transform="matrix(-0.70717 -0.70705 0.70705 -0.70717 77.65 -44.73)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path
    d="M 301.35 -47.36 L 164.83 -47.36 C 161.77 -47.36 159.29 -49.84 159.29 -52.89
    L 159.29 -81.19 C 159.29 -84.25 161.77 -86.73 164.83 -86.73 L 301.35 -86.73 C
    304.41 -86.73 306.89 -84.25 306.89 -81.19 L 306.89 -52.89 C 306.89 -49.84 304.41
    -47.36 301.35 -47.36 Z M 159.29 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 163.9 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dataset
    Preparation</foreignobject></g> <g stroke-width="0.6pt" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0
    1.0 151.17 -67.04)"><path d="M 0 4.05 L 7.01 0 L 0 -4.05 Z" style="fill:none"></path></g><g
    stroke-width="3.0pt"><path d="M 129.42 -67.04 L 150.71 -67.04" style="fill:none"></path></g><g
    stroke-width="1.4pt" stroke="#FFFFFF"><path d="M 129.42 -67.04 L 150.71 -67.04"
    style="fill:none"></path></g><g stroke-width="2.0pt" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><path d="M 129.42 -67.04 L 152.79 -67.04" style="fill:none"></path></g><g
    fill="#CCFFCC"><path d="M 150.16 -134.09 C 150.16 -115.19 125.34 -99.87 94.72
    -99.87 C 64.1 -99.87 39.27 -115.19 39.27 -134.09 C 39.27 -152.99 64.1 -168.31
    94.72 -168.31 C 125.34 -168.31 150.16 -152.99 150.16 -134.09 Z M 94.72 -134.09"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 60.13 -124.19)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Colour
    Model Conversion</foreignobject></g> <g fill="#CCFFCC"><path d="M 164 -214.97
    C 164 -200.22 139.18 -188.27 108.56 -188.27 C 77.93 -188.27 53.11 -200.22 53.11
    -214.97 C 53.11 -229.72 77.93 -241.67 108.56 -241.67 C 139.18 -241.67 164 -229.72
    164 -214.97 Z M 108.56 -214.97"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    73.96 -210.39)" fill="#000000" stroke="#000000"><foreignobject width="69.19" height="28.54"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image Resizing</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 286.39 -214.97 C 286.39 -201.27 261.57 -190.17 230.95
    -190.17 C 200.33 -190.17 175.5 -201.27 175.5 -214.97 C 175.5 -228.66 200.33 -239.77
    230.95 -239.77 C 261.57 -239.77 286.39 -228.66 286.39 -214.97 Z M 230.95 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 196.35 -211.73)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="25.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image
    Augmentation</foreignobject></g> <g fill="#CCFFCC"><path d="M 426.9 -134.09 C
    426.9 -119.34 402.08 -107.39 371.46 -107.39 C 340.84 -107.39 316.02 -119.34 316.02
    -134.09 C 316.02 -148.83 340.84 -160.79 371.46 -160.79 C 402.08 -160.79 426.9
    -148.83 426.9 -134.09 Z M 371.46 -134.09"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 336.87 -129.51)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="28.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data Labelling</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 408.78 -214.97 C 408.78 -196.07 383.96 -180.75 353.34
    -180.75 C 322.72 -180.75 297.9 -196.07 297.9 -214.97 C 297.9 -233.87 322.72 -249.19
    353.34 -249.19 C 383.96 -249.19 408.78 -233.87 408.78 -214.97 Z M 353.34 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 318.75 -205.07)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Generate
    Synthetic Data</foreignobject></g> <path d="M 138.6 -112.83 L 188.92 -88.45" style="fill:none"><g
    transform="matrix(0.89996 0.43597 -0.43597 0.89996 188.92 -88.45)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path d="M 129.59
    -189.99 L 214.15 -89.55" style="fill:none"><g transform="matrix(0.64403 0.765
    -0.765 0.64403 214.15 -89.55)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 231.31 -189.9 L 232.75 -90.33" style="fill:none"><g transform="matrix(0.01447
    0.9999 -0.9999 0.01447 232.75 -90.33)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 332.07 -115.01 L 277.26 -88.45"
    style="fill:none"><g transform="matrix(-0.89996 0.43597 -0.43597 -0.89996 277.26
    -88.45)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83
    -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 328.29 -184.16 L 251.41 -89.58" style="fill:none"><g transform="matrix(-0.63078
    0.77596 -0.77596 -0.63078 251.41 -89.58)"><path d="M 3.32 0 C 1.94 0.28 -0.55
    0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28
    3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path d="M 506.76 -47.36
    L 370.24 -47.36 C 367.18 -47.36 364.71 -49.84 364.71 -52.89 L 364.71 -81.19 C
    364.71 -84.25 367.18 -86.73 370.24 -86.73 L 506.76 -86.73 C 509.82 -86.73 512.3
    -84.25 512.3 -81.19 L 512.3 -52.89 C 512.3 -49.84 509.82 -47.36 506.76 -47.36
    Z M 364.71 -86.73"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32 -70.66)"
    fill="#000000" stroke="#000000"><foreignobject width="138.37" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Pre-processing</foreignobject></g> <g
    fill="#CCFFCC"><path d="M 369.41 0 C 369.41 13.84 344.59 25.06 313.97 25.06 C
    283.35 25.06 258.53 13.84 258.53 0 C 258.53 -13.84 283.35 -25.06 313.97 -25.06
    C 344.59 -25.06 369.41 -13.84 369.41 0 Z M 313.97 0"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 279.38 3.42)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Background
    Removal</foreignobject></g> <g fill="#CCFFCC"><path d="M 397.09 67.04 C 397.09
    80.88 372.26 92.1 341.64 92.1 C 311.02 92.1 286.2 80.88 286.2 67.04 C 286.2 53.2
    311.02 41.99 341.64 41.99 C 372.26 41.99 397.09 53.2 397.09 67.04 Z M 341.64 67.04"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 307.05 70.46)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Removing
    Motion Blur</foreignobject></g> <g fill="#CCFFCC"><path d="M 519.48 67.04 C 519.48
    80.45 494.66 91.33 464.04 91.33 C 433.41 91.33 408.59 80.45 408.59 67.04 C 408.59
    53.63 433.41 42.76 464.04 42.76 C 494.66 42.76 519.48 53.63 519.48 67.04 Z M 464.04
    67.04"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 429.44 69.92)" fill="#000000"
    stroke="#000000"><foreignobject width="69.19" height="25.12" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Enhancement</foreignobject></g> <g fill="#CCFFCC"><path
    d="M 563.13 0 C 563.13 8.35 538.31 15.11 507.69 15.11 C 477.07 15.11 452.24 8.35
    452.24 0 C 452.24 -8.35 477.07 -15.11 507.69 -15.11 C 538.31 -15.11 563.13 -8.35
    563.13 0 Z M 507.69 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 473.1 -3.61)"
    fill="#000000" stroke="#000000"><foreignobject width="69.19" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Denoising</foreignobject></g> <path d="M 349.93
    -19.35 L 398.52 -45.51" style="fill:none"><g transform="matrix(0.88054 -0.47397
    0.47397 0.88054 398.52 -45.51)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 359.03 42.97 L 422.14 -44.39" style="fill:none"><g transform="matrix(0.58559
    -0.81061 0.81061 0.58559 422.14 -44.39)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 492.42 -14.8 L 461.48 -44.77" style="fill:none"><g
    transform="matrix(-0.7182 -0.69585 0.69585 -0.7182 461.48 -44.77)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path d="M 459.38
    42.57 L 442.92 -43.82" style="fill:none"><g transform="matrix(-0.18707 -0.98235
    0.98235 -0.18707 442.92 -43.82)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g
    stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(1.0 0.0 0.0 1.0 356.59 -67.04)"><path d="M 0 4.05 L 7.01 0 L
    0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 307.16
    -67.04 L 356.13 -67.04" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 307.16 -67.04 L 356.13 -67.04" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 307.16 -67.04 L 358.2
    -67.04" style="fill:none"></path></g><g fill="#FFCCCC"><path d="M 506.76 -252.77
    L 370.24 -252.77 C 367.18 -252.77 364.71 -255.25 364.71 -258.31 L 364.71 -286.61
    C 364.71 -289.67 367.18 -292.14 370.24 -292.14 L 506.76 -292.14 C 509.82 -292.14
    512.3 -289.67 512.3 -286.61 L 512.3 -258.31 C 512.3 -255.25 509.82 -252.77 506.76
    -252.77 Z M 364.71 -292.14"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32
    -269.04)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Apply Deep Learning based
    Classifiers</foreignobject></g> <g fill="#FFCCCC"><path d="M 506.76 -333.65 L
    370.24 -333.65 C 367.18 -333.65 364.71 -336.13 364.71 -339.19 L 364.71 -367.49
    C 364.71 -370.55 367.18 -373.02 370.24 -373.02 L 506.76 -373.02 C 509.82 -373.02
    512.3 -370.55 512.3 -367.49 L 512.3 -339.19 C 512.3 -336.13 509.82 -333.65 506.76
    -333.65 Z M 364.71 -373.02"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32
    -349.92)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Evaluation of the Model</foreignobject></g><g
    stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(0.0 -1.0 1.0 0.0 438.5 -244.66)"><path d="M 0 4.05 L 7.01 0
    L 0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 438.5
    -87.01 L 438.5 -244.19" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 438.5 -87.01 L 438.5 -244.19" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 438.5 -87.01 L 438.5
    -246.27" style="fill:none"></path></g><g stroke-width="0.6pt" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 -1.0 1.0
    0.0 438.5 -325.54)"><path d="M 0 4.05 L 7.01 0 L 0 -4.05 Z" style="fill:none"></path></g><g
    stroke-width="3.0pt"><path d="M 438.5 -292.42 L 438.5 -325.08" style="fill:none"></path></g><g
    stroke-width="1.4pt" stroke="#FFFFFF"><path d="M 438.5 -292.42 L 438.5 -325.08"
    style="fill:none"></path></g><g stroke-width="2.0pt" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><path d="M 438.5 -292.42 L 438.5 -327.15" style="fill:none"></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: A summary workflow of weed detection techniques using deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Conventional ML techniques require substantial domain expertise to construct
    a feature extractor from raw data. On the other hand, the DL approach uses a representation-learning
    method where a machine can automatically discover the discriminative features
    from raw data for classification or object detection problems [[91](#bib.bibx91)].
    A machine can learn to classify directly from images, text and sounds [[120](#bib.bibx120)].
    The ability to extract the features that best suit the task automatically is also
    known as feature learning. As deep learning is a hierarchical architecture of
    learning, the features of the higher levels of the hierarchy are composed of lower-level
    features [[110](#bib.bibx110), [58](#bib.bibx58)].
  prefs: []
  type: TYPE_NORMAL
- en: Several popular and high performing network architectures are available in deep
    learning. Two of the frequently used architectures are Convolutional Neural Networks
    (CNNs) and Recurrent Neural Networks (RNNs) [[91](#bib.bibx91), [60](#bib.bibx60)].
    Although CNNs are used for other types of data, the most widespread use of CNNs
    is to analyse and classify images. The word convolution refers to the filtering
    process. A stack of convolutional layers is the basis of CNN. Each layer receives
    the input data, transform, or convolve them and output to the next layer. This
    convolutional operation eventually simplifies the data so that it can be better
    processed and understood. RNNs have a built-in feedback loop, which allows them
    to act as a forecasting engine. Feed-forward or CNN take a fixed size input and
    produces a fixed size output. The signal flow of the feed-forward network is unidirectional,
    i.e., from input to output. They cannot even capture the sequence or time-series
    information. RNNs overcome the limitation. In RNN, the current inputs and outputs
    of the network are influenced by prior input. Long Short-Term Memory (LSTM) is
    a type of RNN [[91](#bib.bibx91)], which has a memory cell to remember important
    prior information, thus can help improving the performance. Depending on the network
    architecture, DL has several components like convolutional layers, pooling layers,
    activation functions, dense/fully connected layers, encoder/decoder schemes, memory
    cells, gates etc. [[120](#bib.bibx120)].
  prefs: []
  type: TYPE_NORMAL
- en: For image classification, object detection, and localisation, DL algorithms
    have many advantages over traditional ML approaches. Because of the strong feature
    learning capabilities, DL methods can effectively extract discriminative features
    of crops and weeds. Also, with increasing data, the performance of traditional
    ML approaches has become saturated. Using large dataset, DL techniques show superior
    performance compared to traditional ML techniques [[5](#bib.bibx5)]. This characteristic
    is leading to the increasing application of DL approaches. Many of the research
    reports in Section [10](#S10 "10 Deep Learning Architecture ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images") show comparisons between
    DL and other ML approaches to detect weeds in crops. Figure [2](#S3.F2 "Figure
    2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images") gives an overview of DL-based weed
    detection and recognition techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Not all the steps outlined in Figure [2](#S3.F2 "Figure 2 ‣ 3 Traditional ML-
    vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning Techniques for
    Weed Detection from Images") need to be present in every method. Four major steps
    are followed in this process. They are Data Acquisition, Dataset Preparation/Image
    Pre-processing, Classification and Evaluation. In this paper, we describe the
    steps used in different research work to discriminate between weeds and crops
    using DL techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Paper Selection Criteria in this Survey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To overview the state of research, we have undertaken a comprehensive literature
    review. The process involved two major steps: (i) searching and selecting related
    studies and (ii) detailed analysis of these studies. The main research question
    is: What is the role of deep learning techniques for detecting, localising and
    classifying weeds in crops? For collecting the related work based on this research
    question, we applied a keyword-based search in Google Scholar, Web of Science,
    IEEE Xplore, Scopus, ScienceDirect, Multidisciplinary Digital Publishing Institute
    (MDPI), Springer and Murdoch University Library databases for journal articles
    and conference papers. We have applied a keyword search from 2010 to 30 August
    2020\. Table [1](#S4.T1 "Table 1 ‣ 4 Paper Selection Criteria in this Survey ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images") shows the
    number of search results for the search query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Number of documents resulted for the queries indicated'
  prefs: []
  type: TYPE_NORMAL
- en: '| No. | Academic Research Databases | Search Query | Number of Retrieved Documents
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1. | Google Scholar | [“Weed Detection” OR “Weed management” OR “Weed Classification”]
    AND [“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”] | 998
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2. | Web of Science | (Weed Detection OR Weed management OR Weed Classification)
    AND (Deep Learning OR Deep Machine Learning OR Deep Neural Network) | 124 |'
  prefs: []
  type: TYPE_TB
- en: '| 3. | IEEE Xplore | (((“All Metadata”:“Deep Learning”) OR “All Metadata”:“Deep
    Machine Learning”) OR “All Metadata”:“Deep Neural Network”) AND Weed detection
    | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| 4. | ScienceDirect | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 87
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5. | Scopus | ((Weed AND detection) OR (Weed AND Management) OR (Weed AND
    Classification)) AND ((Deep AND Learning) OR (Deep AND Machine AND Learning) OR
    (Deep AND Neural AND Network)) | 118 |'
  prefs: []
  type: TYPE_TB
- en: '| 6. | MDPI | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 76
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7. | SpringerLink | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 46
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8. | Murdoch University Library | (“Weed Detection” OR “Weed management”
    OR “Weed Classification”) AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep
    Neural Network”) | 179 |'
  prefs: []
  type: TYPE_TB
- en: 'After searching the above databases, duplicated documents were removed: that
    provided 988 documents. We further identified and counted those using DL-based
    methodology. In Figure [3](#S4.F3 "Figure 3 ‣ 4 Paper Selection Criteria in this
    Survey ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    we show the total number of papers which used DL between 2010 to 30 August 2020\.
    This shows that before 2016, the number of publications in this area was very
    small, but that there is an upward trend in the number of papers from 2016\. For
    this reason, articles published from 2016 and onward were used in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F3.pic1" class="ltx_picture ltx_centering" height="208.1" overflow="visible"
    version="1.1" width="467.38"><g transform="translate(0,208.1) matrix(1 0 0 -1
    0 0) translate(49.37,0) translate(0,24.32) matrix(1.0 0.0 0.0 1.0 -49.37 -24.32)"
    fill="#000000" stroke="#000000" stroke-width="0.4pt"><g class="ltx_nestedsvg"
    transform="matrix(1 0 0 1 0 0) translate(84.18,0) translate(0,24.32)" fill="#000000"
    stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -13.84
    -19.71)" fill="#000000" stroke="#000000"><foreignobject width="27.67" height="8.92"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2010</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 20.97 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2011</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.78 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2012</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 90.6 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2013</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.41 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2014</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.22 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 195.03 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 229.84 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 264.65 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 299.46 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 334.27 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.62 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 44.36)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$100$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 93.17)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$200$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 141.99)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$300$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -14.21 10.26)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$11$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 24.06 6.35)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$3$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 58.87 6.84)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$4$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 93.68 8.79)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$8$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.04 10.75)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$12$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 159.85 11.23)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$13$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 194.66 27.83)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$47$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 229.47 49.31)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$91$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 260.82 99.1)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="20.76" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$193$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 295.63 166.46)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$331$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 330.44 139.13)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="20.76" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$275$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 108.26 5.38)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 143.07 5.38)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 177.88 5.86)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 212.69 7.33)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 244.05 12.7)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$16$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 278.86 19.53)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$30$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 313.67 27.34)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$46$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 348.48 23.44)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$38$</foreignobject></g><g transform="matrix(0.0
    1.0 -1.0 0.0 -69.96 19.57)" fill="#000000" stroke="#000000"><foreignobject width="138.99"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Number of
    Publication</foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 51.43 155.96)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1
    0 9.365)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 10.12)"><g
    class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1
    8.43 0) translate(61.4,0) matrix(1.0 0.0 0.0 1.0 -58.63 -3.77)" fill="#000000"
    stroke="#000000"><foreignobject width="117.27" height="9.61" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Searched document</foreignobject></g><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 139.66 0) translate(52.79,0)
    matrix(1.0 0.0 0.0 1.0 -50.02 -3.77)" fill="#000000" stroke="#000000"><foreignobject
    width="100.05" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DL
    based article</foreignobject></g></g></g></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The number of selected publications on DL-based weed detection approach
    from 2010 to 30 August 2020'
  prefs: []
  type: TYPE_NORMAL
- en: 5 An Overview and Taxonomy of Deep Learning-based Weed Detection Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An overall taxonomy of DL-based weed detection techniques is shown in Figure
    [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based Weed
    Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images"). The papers covered in this survey are categorised using this taxonomy
    and listed in Table LABEL:tab:different_DL_approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a10571ca5061433f93182d580e47d020.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An overall taxonomy of deep learning-based weed detection techniques'
  prefs: []
  type: TYPE_NORMAL
- en: The related publications have been analysed based on the taxonomy in Figure
    [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based Weed
    Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images"). Here, the data acquisition process, sensors and mounting vehicles
    are highlighted. Moreover, an overview of the dataset preparation approaches,
    i.e., image pre-processing, data generation and annotation are also given. While
    analysing these publications, it has been found that the related works either
    generate a weed map for the target site or a classification for each of the plants
    (crops/weeds). For developing the classifiers, the researchers applied supervised,
    unsupervised or semi-supervised learning approaches. Depending on the learning
    approaches and the research goal, different DL architectures were used. An overview
    of the related research is provided in Table LABEL:tab:different_DL_approach.
    It shows the crop and weed species selected for experimental work, the steps taken
    to collect and prepare the datasets, and the DL methods applied in the research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: An overview of different DL approaches used in weed detection'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | Crop | Weed Species | DL Architectures Applied | Operations Performed
    (based on Figure [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based
    Weed Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images")) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[40](#bib.bibx40)] | Tomato, Cotton | Black nightshade, velvetleaf | Modified
    Xception, Inception-ResNet, VGGNet, MobileNet, DenseNet | DC; (IP, IA, ILA); PBC
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[164](#bib.bibx164)] | Sugar beet, Oilseed | Not specified | FCN | (DC,
    FR); (IP, IA, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bibx89)] | Canola, corn, radish | Not specified | Filtered Local
    Binary Pattern with Contour Mask and Coefficient k (k-FLBPCM), VGG-16, VGG-19,
    ResNet-50, Inception-v3 | (ATV, MC); (IP, IA, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[61](#bib.bibx61)] | Not specified | Chinee apple, Lantana, Parkinsonia,
    Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed | Inception-v3,
    ResNet-50, DenseNet-202, Inception-ResNet-v2, GCN | PD; IP; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[161](#bib.bibx161)] | Carrot | Not specified | SegNet-512, SegNet-256 |
    PD; IA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[65](#bib.bibx65)] | Rice | Leptochloa chinensis, Cyperus iria, Digitaria
    sanguinalis (L). Scop, Barnyard Grass | FCN | (DC, UAV); (IP, PLA); WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[47](#bib.bibx47)] | Sugar beet | Convolvulus sepium (hedge bindweed) |
    YOLO-v3, tiny YOLO-v3 | DC; (IA, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[146](#bib.bibx146)] | Soybean | Waterhemp, Palmer amaranthus, common lambsquarters,
    velvetleaf, foxtail species | Single-Shot Detector (SSD), Faster R-CNN | (DC,
    UAV); (IP, IA, BBA); WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bibx71)] | Corn, lettuce, radish | Cirsium setosum, Chenopodium
    album, bluegrass, sedge, other unspecified weed | GCN | PD; (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[18](#bib.bibx18)] | Sugar Beets, Carrots, Onions | Not specified | SegNet
    | PD; PLA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[168](#bib.bibx168)] | Paddy | Alternanthera philoxeroides, Eclipta prostrata,
    Ludwigia adscendens, Sagittaria trifolia, Echinochloa crus-galli, Leptochloa chinensis
    | AlexNet | DC; ILA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[172](#bib.bibx172)] | Wheat | Cirsium Setosum, Descurainia Sophia, Euphorbia
    Helioscopia, Veronica Didyma, Avena Fatu | YOLO-v3, Tiny YOLO-v3 | (DC, UAV);
    (IP, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bibx102)] | Sugar beet | Dicot weeds, grass weeds | FCN | MC;
    (IP, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[159](#bib.bibx159)] | Not specifies | 12 species of “Plant Seedlings dataset”,
    21 species of “CNU weeds dataset” | NASNet, ResNet, Inception–ResNet, MobileNet,
    VGGNet | DC; ILA, PD |'
  prefs: []
  type: TYPE_TB
- en: '| [[119](#bib.bibx119)] | Not specified | Scentless Mayweed, Chickweed, Cranesbill,
    Shepherd’s Purse, Cleavers, Charlock, Fat Hen, Maise, Sugar beet, Common wheat,
    Black-grass, Loose Silky-bent | Mask R-CNN | PD; PLA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[127](#bib.bibx127)] | Sugar beet | Not specified | DeepLab-v3, SegNet,
    U-Net | (MC, UAV); (IP, PLA) |'
  prefs: []
  type: TYPE_TB
- en: '| [[114](#bib.bibx114)] | Lettuce | Not specified | YOLO-v3, Mask R-CNN, SVM
    | (MC, UAV); (IP, PLA) |'
  prefs: []
  type: TYPE_TB
- en: '| [[86](#bib.bibx86)] | Grasslands | Rumex obtusifolius | VGG-16 | (DC, UAV);
    (IP, PLA) |'
  prefs: []
  type: TYPE_TB
- en: '| [[142](#bib.bibx142)] | Strawberry, Tomato | Goosegrass | Tiny YOLO-v3 |
    DC; (IP, BBA) |'
  prefs: []
  type: TYPE_TB
- en: '| [[122](#bib.bibx122)] | Not specified | Colchicum autumnale | U-Net | (DC,
    UAV); (IP, IA, BBA) |'
  prefs: []
  type: TYPE_TB
- en: '| [[31](#bib.bibx31)] | Carrot | Not specified | Faster YOLO-v3, tiny YOLO-v3
    | (DC, FR); ILA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bibx117)] | Blueberry | Not specified | Faster R-CNN, YOLO-v3,
    ResNet-50, ResNet-101, Darknet-53 | (DC, ATV); (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[116](#bib.bibx116)] | Pepper | Portulaca weeds | Tiny YOLO-v3, YOLO-v3
    | (DC, ATV); (IP, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[113](#bib.bibx113)] | Not specified | Chinee apple, Lantana, Parkinsonia,
    Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed | Inception-v3,
    ResNet-50 | (DC, FR); (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[82](#bib.bibx82)] | Clover, grass | Broad-leaved dock | AlexNet, VGG-F,
    VGG-VD-16, Inception-v1, ResNet-50, ResNet-101 | (DC, FR); PLA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[128](#bib.bibx128)] | Mache salad | Not specified | Scatter Transform,
    Local Binary Pattern (LBP), GLCM, Gabor filter, CNN | (DC, FR); (IP, SDG, BBA);
    PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[138](#bib.bibx138)] | Chrysanthemum | Para grass, Nutsedge | SVM, Artificial
    Neural Network (ANN), CNN | DC; (IP, IA, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[104](#bib.bibx104)] | Rice | Sagittaria trifolia | SegNet, FCN, U-Net |
    DC; (IP, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[9](#bib.bibx9)] | Canola | Not specified | U-Net, SegNet | (DC, ATV); (IP,
    IA, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[171](#bib.bibx171)] | Bermudagrass | Hydrocotyle spp., Hedyotis cormybosa,
    Richardia scabra | VGGNet, GoogLeNet, DetectNet | DC; (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[1](#bib.bibx1)] | Oilseed | Not specified | FCN | DC; (IA, PLA); WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[170](#bib.bibx170)] | Perennial ryegrass | dandelion, ground ivy, spotted
    spurge | AlexNet, VGGNet, GoogLeNet, DetectNet | DC; (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[97](#bib.bibx97)] | Not specified | Not specified | CNN, Histogram of oriented
    Gradients (HoG), LBP | (DC, UAV); (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[141](#bib.bibx141)] | Strawberry | Carolina geranium | VGGNet, GoogLeNet,
    DetectNet | DC; (IP, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[44](#bib.bibx44)] | Sunflower, carrots, sugar beets | Not specified | SegNet,
    U-Net, BonNet, FCN8 | (DC, FR, PD); PLA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[162](#bib.bibx162)] | Grassland | Rumex obtusifolius | AlexNet | (DC, UAV);
    (IP, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bibx29)] | Beet, cauliflower, cabbage, strawberry | Not specified
    | Hybrid Network | (DC, ATV); (IP, IA, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[20](#bib.bibx20)] | Carrot | Not specified | U-Net | PD; (IA, PLA); PBC
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[16](#bib.bibx16)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | ResNet-101 | PD, (IP,
    IA, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[72](#bib.bibx72)] | Cotton | Not specified | Faster R-CNN | DC, (IP, BBA);
    PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[2](#bib.bibx2)] | Paddy | Wild millet | ESNet, U-Net, FCN-8s, and DeepLab-v3,
    Faster R-CNN, EDNet | DC; (IP, IA, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[43](#bib.bibx43)] | Sugar beet | Alli, hyme, hyac, azol, other unspecified
    weeds | CNN, FCN, LBP, superpixel based LBP, FCN-SPLBP | HC; (IP, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[79](#bib.bibx79)] | Carrot | Not specified | CNN | DC; (IP, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[35](#bib.bibx35)] | Soybean | grass, broadleaf weeds, Chinee apple, Lantana,
    Parkinsonia, Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed |
    Joint Unsupervised LEarning (JULE), DeepCluster | PD; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[132](#bib.bibx132)] | Not specified | Gamba grass | U-Net | SI; (IP, PLA)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[147](#bib.bibx147)] | Clover | Grass | FCN-8s | DC, (IP, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[173](#bib.bibx173)] | Pasture | Not specified | CNN, SVM | (DC, ATV); (IP,
    IA, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bibx81)] | Grasslands | Broad-leaved dock | AlexNet, VGG-F, GoogLeNet
    | (DC, FR); BBA; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bibx64)] | Rice | Not specified | FCN, SVM | (DC, UAV); BBA; WM
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[157](#bib.bibx157)] | Not specified | Common field speedwell, field pansy,
    common chickweed, fat-hen, fine grasses (annual meadow-grass, loose silky-bent),
    blackgrass, hemp-nettle, shepherd’s purse, common fumitory, scentless mayweed,
    cereal, brassicaceae, maise, polygonum, oat (volunteers), cranesbill, dead-nettle,
    common poppy | Inception-v3 | DC; (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[160](#bib.bibx160)] | Carrot | Not specified | GoogleNet | PD, (IA, BBA);
    PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[151](#bib.bibx151)] | Sugar beets | Volunteer potato | AlexNet, VGG-19,
    GoogLeNet, ResNet-50, ResNet-101, Inception-v3 | (DC, ATV); (IP, IA, ILA); PBC
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[41](#bib.bibx41)] | Not specified | Hyme, Alli, Azol, Hyac | CNN | HC,
    (IP, IA, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[11](#bib.bibx11)] | Spinach, bean | Not specified | ResNet-18 | (DC, UAV);
    (IP, IA, BBA); WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[42](#bib.bibx42)] | Not specified | Hyme, Alli, Azol, Hyac | CNN, HoG |
    HC, (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[103](#bib.bibx103)] | Sugar beet | Not specified | FCN | (MC, FR); (IP,
    PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[135](#bib.bibx135)] | Sugar beet | Galinsoga spec., Amaranthus retroflexus,
    Atriplex spec., Polygonum spec., Gramineae (Echinochloa crus-galli, agropyron,
    others.), Convolvulus arvensis, Stellaria media, Taraxacum spec. | SegNet | (MC,
    UAV); PLA; WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[63](#bib.bibx63)] | Rice | Not specified | CNN, FCN | (DC, UAV); (IP, PLA);
    WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[62](#bib.bibx62)] | Rice | Not specified | FCN-8s, FCN-4s, DeepLab | (DC,
    UAV) (IP, PLA); WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bibx26)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | AlexNet, VGGNet, Hybrid
    Network | PD; PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[111](#bib.bibx111)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | KNN, SVM, CNN | PD; (IP,
    BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[134](#bib.bibx134)] | Sugar beet | Not specified | SegNet | (MC, UAV),
    (IP, BBA), WM |'
  prefs: []
  type: TYPE_TB
- en: '| [[7](#bib.bibx7)] | Maise | Not specified | LeNET, AlexNet, cNET, sNET |
    DC; (IP, IA, PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[37](#bib.bibx37)] | Winter wheat | Not specified | FCN | (DC, ATV); (IP,
    BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[34](#bib.bibx34)] | Soybean | Grass, broadleaf weeds | AlexNet, SVM, Adaboost
    – C4.5, Random Forest | (DC, UAV); (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[155](#bib.bibx155)] | Soybean | Cephalanoplos, digitaria, bindweed | Back
    propagation neural network, SVM, CNN | DC; (IP, ILA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[108](#bib.bibx108)] | Sugar beet | Not specified | CNN | (DC, UAV); (IP,
    PLA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[121](#bib.bibx121)] | Lawn grass | Not specified | CNN | (DC, FR); (IP,
    SDG, BBA); PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[33](#bib.bibx33)] | Sugar beet | Capsella bursa-pastoris, galium aparine
    | SegNet | SDG, PBC |'
  prefs: []
  type: TYPE_TB
- en: '| [[38](#bib.bibx38)] | Tobacco, thale cress, cleavers, common Poppy, cornflower,
    wheat, maise, sugar beet, cabbage, barley | Sherpherd’s-Purse , chamomile, knotweed
    family, cranesbill, chickweed, veronica, fat-hen, narrow-leaved grasses, field
    pancy, broad-leaved grasses, annual nettle, black nightshade | CNN | PD; (IP,
    IA); PBC |'
  prefs: []
  type: TYPE_TB
- en: 6 Data Acquisition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL based weed detection and classification techniques require an adequate amount
    of labelled data. Different modalities of data are collected using various types
    of sensors that are mounted on a variety of platforms. Below we discuss the popular
    ways of weed data collection.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Sensors and Camera Mounting Vehicle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1 Unmanned Aerial Vehicles (UAVs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unmanned Aerial Vehicles are often used for data acquisition in agricultural
    research. Generally, UAVs are used for mapping weed density across a field by
    collecting RGB images [[65](#bib.bibx65), [63](#bib.bibx63), [64](#bib.bibx64),
    [122](#bib.bibx122)] or multispectral images [[135](#bib.bibx135), [134](#bib.bibx134),
    [119](#bib.bibx119), [114](#bib.bibx114), [127](#bib.bibx127)]. In addition, UAVs
    can be used to identify crop rows and map weeds within crop rows by collecting
    RGB (Red, Green and Blue color) images [[11](#bib.bibx11)]. [[162](#bib.bibx162)]
    used a small quad-rotor UAV for recording images from grassland to detect broad-leaved
    dock (Rumex obtusifolius). As UAVs fly over the field at a certain height, the
    images captured by them cover a large area. Some of the studies split the images
    into smaller patches and use the patches to distinguish between weeds and crop
    plants [[108](#bib.bibx108), [34](#bib.bibx34), [146](#bib.bibx146)]. However,
    the flight altitude can be maintained at a low height, e.g. 2 meters, so that
    each plant can be labelled as either a weed or crop [[172](#bib.bibx172), [114](#bib.bibx114)].
    [[97](#bib.bibx97)] collected image data using a drone by maintaining an altitude
    of 2.5 meters. [[62](#bib.bibx62)] collected images with a resolution of 3000$\times$4000
    pixels using a sequence of forward-overlaps and side-overlaps to cover the entire
    field. [[86](#bib.bibx86)] flew DJI Phantom 3 and 4 Pro drones with a RGB camera
    at three different heights (10, 15 and 20 m) to determine the optimal height for
    weed detection.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Field Robots (FRs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Various types of field robot can also be used to collect images. A robotic vehicle
    can carry one or more cameras. As previously discussed, robotic vehicles are used
    to collect RGB images by mounted digital cameras [[31](#bib.bibx31), [113](#bib.bibx113),
    [82](#bib.bibx82), [128](#bib.bibx128), [44](#bib.bibx44)]. Mobile phone in-built
    cameras have also been used for such data collection. For example, an iPhone 6
    was used to collect video data by mounting it on a Robotic Rover [[121](#bib.bibx121)].
    A robotic platform called “BoniRob” has been used to collect multi-spectral images
    from the field [[103](#bib.bibx103), [102](#bib.bibx102)]. [[81](#bib.bibx81)]
    used three monochrome cameras mounted on a robot to take images. They argued that,
    in most cases, weeds are green, and so are the crops. There is no need to use
    colour features to distinguish them.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 All-Terrain Vehicles (ATVs)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To collect images from the field, all-terrain vehicles have also been used.
    ATVs can be mounted with different types of camera [[37](#bib.bibx37), [173](#bib.bibx173),
    [9](#bib.bibx9), [117](#bib.bibx117), [29](#bib.bibx29)]. [[89](#bib.bibx89)]
    used a combination of multi-spectral and spatial sensors to capture data. Even
    multiple low-resolution webcams have been used on an ATV [[116](#bib.bibx116)].
    To maintain specific height with external lighting conditions, and illumination,
    custom made mobile platforms have been used to carry the cameras for capturing
    RGB images [[151](#bib.bibx151), [147](#bib.bibx147)]. When it is not possible
    to use any vehicle to collect images at a certain height, tripods can be used
    as an alternative [[1](#bib.bibx1)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Collect Data without Camera Mounting Devices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On a few occasions, weed data have been collected by cameras without being mounted
    on a vehicle. As such, video data are collected using handheld cameras [[171](#bib.bibx171),
    [138](#bib.bibx138), [104](#bib.bibx104), [170](#bib.bibx170), [157](#bib.bibx157),
    [155](#bib.bibx155), [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [79](#bib.bibx79),
    [168](#bib.bibx168), [72](#bib.bibx72), [142](#bib.bibx142)]. [[141](#bib.bibx141)]
    collected their data by maintaining a certain height (130 cm) from the soil surface.
    Brimrose VA210 filter and JAI BM-141 cameras have been used to collect hyperspectral
    images of weeds and crops without using any platform [[41](#bib.bibx41), [43](#bib.bibx43),
    [42](#bib.bibx42)]. [[7](#bib.bibx7)] manually focused a camera on the target
    plants in such a way that it could capture images, including all the features
    of these plants. In [[159](#bib.bibx159)], they focus the camera on many parts
    of weeds, such as flowers, leaf, fruits, or the full weeds structure.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Satellite Imagery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[132](#bib.bibx132)] use the Pleiades-HR 1A to collect high-resolution 4-band
    (RGB+NIR) imagery over the area of interest. They made use of high-resolution
    satellite images and applied masking to indicate the presence of weeds.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Public Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several publicly available crop and weed datasets that can be used
    to train the DL models. [[28](#bib.bibx28)] developed a dataset containing weeds
    in sugar beet crops. Another annotated dataset containing images of crops and
    weeds collected from fields has been made available by [[54](#bib.bibx54)]. A
    dataset of annotated (7853 annotations) crops and weed images was developed by
    [[150](#bib.bibx150)], which comprises 1118 images of six food crops and eight
    weed species. [[95](#bib.bibx95)] developed a dataset containing 7,590 RGB images
    with 315,038 plant objects, representing 64,292 individual plants from 47 different
    species. These data were collected in Denmark and made available for further use.
    A summary of the publicly available datasets related to weed detection and plant
    classification is listed in Table [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣
    6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection from
    Images").
  prefs: []
  type: TYPE_NORMAL
- en: We have listed nineteen datasets in Table [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets
    ‣ 6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") which are available in this area, and can be used by researchers.
    Amongst these datasets, researchers will need to send a request to the owner of
    “Perennial ryegrass and weed”, “CNU Weed Dataset” and “Sugar beet and hedge bindweed”
    dataset to obtain the data. Other datasets can be downloaded directly on-line.
    Most of the datasets contain RGB images of food crops and weeds from different
    parts of the world. The RGB data have generally been collected using high-resolution
    digital cameras. However, [[157](#bib.bibx157)] used a point grey industrial camera.
    While acquiring data for the “DeepWeeds” dataset, the researchers added a “Fujinon
    CF25HA-1” lens with their “FLIR Blackfly 23S6C” camera and mounted the camera
    on a weed control robot (“AutoWeed”). [[28](#bib.bibx28)] and [[55](#bib.bibx55)]
    employed “Bonirob” (an autonomous field robot) to mount the multi-spectral cameras.
    “Carrots 2017” and “Onions 2017” datasets were also acquired using a multi-spectral
    camera, namely the “Teledyne DALSA Genie Nano”. These researchers used a manually
    pulled cart to carry the camera. The “CNU Weed Dataset” has 208,477 images of
    weeds collect from farms and fields in the Republic of Korea, which is the highest
    number among the datasets. Though this dataset exhibits a class imbalance, it
    contains twenty-one species of weeds from five families. [[147](#bib.bibx147)]
    developed a dataset of red clover, white clover and other associated weeds. The
    dataset contains 31,600 unlabelled data together with 8000 synthetic data. Their
    goal was to generate labels for the data using unsupervised or self-supervised
    approaches. All the other datasets were manually labelled using image level, pixel-wise
    or bounding box annotation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: List of publicly available crop and weed datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset and Reference | Type/Number of Crop | Type/Number of Weed Species
    | Data Type | Sensor and Mounting Vehicle | Number of Images | Data Annotation
    | Data Location | Class imbalance? | Source |'
  prefs: []
  type: TYPE_TB
- en: '| Crop/Weed Field Image Dataset [[55](#bib.bibx55)] | Carrot | Not specified
    | Multi-spectral image | MC and FR | 60 | PLA | Germany | Yes | [https://github.com/cwfid/dataset](https://github.com/cwfid/dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dataset of food crops and weed [[150](#bib.bibx150)] | Six crop | Eight weed
    species | RGB | DC | 1118 | BBA | Latvia | Yes | [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DeepWeeds [[113](#bib.bibx113)] | Not specified | Eight weed species | RGB
    | DC and FR | 17,509 | ILA | Australia | No | [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Early crop weed dataset [[40](#bib.bibx40)] | Tomato, cotton | Black nightshad,
    velvetleaf | RGB | 508 | DC | ILA | Greece | Yes | [https://github.com/AUAgroup/early-crop-weed](https://github.com/AUAgroup/early-crop-weed)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Perennial ryegrass and weed [[170](#bib.bibx170)] | Perennial ryegrass |
    dandelion, ground ivy, and spotted spurge | RGB | DC | 33086 | ILA | USA | No
    | [https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full](https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Soybean and weed dataset [[34](#bib.bibx34)] | Soybean | Grass and broadleaf
    weeds | RGB | DC and UAV | 400 | ILA | Brazil | Yes | [https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops](https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Open Plant Phenotype Database [[95](#bib.bibx95)] | Not specified | 46 most
    common monocotyledon (grass) and dicotyledon (broadleaved) weeds | RGB | DC |
    7,590 | BBA | Denmark | No | [https://gitlab.au.dk/AUENG-Vision/OPPD](https://gitlab.au.dk/AUENG-Vision/OPPD)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sugar beet and hedge bindweed dataset [[47](#bib.bibx47)] | Sugar beet |
    Convolvulus sepium (hedge bindweed) | RGB | DC | 652 | BBA | Belgium | Yes | [https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sugar beet fields dataset [[28](#bib.bibx28)] | Sugar beet | Not specified
    | Multi-spectral image | MC and FR | 12340 | PLA | Germany | No | [https://www.ipb.uni-bonn.de/2018/10/](https://www.ipb.uni-bonn.de/2018/10/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| UAV Sugarbeets 2015-16 Datasets [[27](#bib.bibx27)] | Sugarbeets | Not specified
    | RGB | DC and UAV | 675 | PLA | Switzerland | No | [https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/](https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Corn, lettuce and weed dataset [[71](#bib.bibx71)] | Corn and lettuce | Cirsium
    setosum, Chenopodium album, bluegrass and sedge | RGB | DC | 6800 | ILA | China
    | No | [https://github.com/zhangchuanyin/weed-datasets](https://github.com/zhangchuanyin/weed-datasets)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Carrot-Weed dataset [[88](#bib.bibx88)] | Carrot | Not specified | RGB |
    DC | 39 | PLA | Republic of Macedonia | Yes | [https://github.com/lameski/rgbweeddetection](https://github.com/lameski/rgbweeddetection)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bccr-segset dataset [[90](#bib.bibx90)] | Canola, corn, radish | Not specified
    | RGB | DC | 30,000 | ILA | Australia | No | [https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497](https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Carrots 2017 dataset [[18](#bib.bibx18)] | Carrots | Not specified | Multi-spectral
    image | MC and manually pulled cart | 20 | PLA | UK | No | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Onions 2017 dataset [[18](#bib.bibx18)] | Onions | Not specified | Multi-spectral
    image | MC and manually pulled cart | 20 | PLA | UK | No | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GrassClover image dataset [[147](#bib.bibx147)] | Red clover and white clover
    | Not specified | RGB | DC and manually operated platform | 31,600 real and 8000
    synthetic images | PLA | Denmark | Yes | [https://vision.eng.au.dk/grass-clover-dataset/](https://vision.eng.au.dk/grass-clover-dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Leaf counting dataset [[157](#bib.bibx157)] | Not specified | Eighteen weed
    species | RGB | DC | 9372 | ILA | Denmark | Yes | [https://vision.eng.au.dk/leaf-counting-dataset/](https://vision.eng.au.dk/leaf-counting-dataset/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CNU Weed Dataset [[159](#bib.bibx159)] | Not specified | Twenty one species
    of weed | RGB | DC | 208,477 | ILA | Republic of Korea | Yes | [https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025](https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Plant Seedlings Dataset [[49](#bib.bibx49)] | Three crop | Nine weed species
    | RGB | DC | 5539 | ILA | Denmark | Yes | [https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset)
    |'
  prefs: []
  type: TYPE_TB
- en: '[[38](#bib.bibx38)] use six publicly available datasets containing 22 different
    plant species to classify using deep learning methods. Several studies proposed
    an encoder-decoder architecture to distinguish crops and weeds using the Crop
    Weed Field Image Dataset [[161](#bib.bibx161), [20](#bib.bibx20), [160](#bib.bibx160)].
    The DeepWeeds dataset [[113](#bib.bibx113)] was used by [[61](#bib.bibx61)] to
    evaluate their proposed method. In the study of [[71](#bib.bibx71)], the “Carrot-Weed
    dataset” [[88](#bib.bibx88)] was used with their own dataset the “Corn, lettuce
    and weed dataset”. [[44](#bib.bibx44)] collected data from a sunflower farm in
    Italy. To demonstrate the proposed method’s generalising ability, they also used
    two publicly available datasets containing images of carrots, sugar beets and
    associated weeds. [[18](#bib.bibx18)] also used those datasets along with the
    Carrot 2017 and Onion 2017 datasets. The “Plant Seedlings” dataset is a publicly
    available dataset containing 12 different plant species. Several studies used
    this dataset to develop a crop-weed classification model [[26](#bib.bibx26), [111](#bib.bibx111),
    [16](#bib.bibx16), [119](#bib.bibx119)]. [[35](#bib.bibx35)] used DeepWeeds [[113](#bib.bibx113)]
    and “Soybean and weed” datasets, which are publicly available.'
  prefs: []
  type: TYPE_NORMAL
- en: While several datasets are publicly available, they are somewhat site/crop-specific.
    As such there is no so-called benchmark weed dataset like ImageNet [[32](#bib.bibx32)]
    and MS COCO [[98](#bib.bibx98)] in this research field, that is widely used in
    the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Dataset Preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After acquiring data from different sources, it is necessary to prepare data
    for training, testing, and to validate models. Raw data is not always suitable
    for the DL model. The dataset preparation approaches include applying different
    image processing techniques, data labelling, using image augmentation techniques
    to increase the number of input data, or impose variations in the data and generating
    synthetic data for training. Commonly used image processing techniques are - background
    removal, resizing the collected image, green component segmentation, removing
    motion blur, de-noising, image enhancement, extraction of colour vegetation indices,
    and changing the colour model. [[121](#bib.bibx121)] decoded video into a sequence
    of RGB images and then converted them into grayscale images. In further research,
    the camera was set to auto-capture mode to collect images in the TIFF format and
    then these were converted into the RGB colour model [[151](#bib.bibx151)]. Using
    three webcams on an ATV, [[116](#bib.bibx116)] took videos and then converted
    them into different frames of images. In some occasions, it was necessary to change
    the image format to accurately train the model, especially when using public datasets.
    For instance, [[16](#bib.bibx16)] converted the “Plant Seedlings Dataset” [[49](#bib.bibx49)]
    from PNG to JPEG format, as a number of studies have show that the JPEG format
    is better for training Residual Networks architectures [[39](#bib.bibx39)].
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Image Pre-processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of relevant studies undertook some level of image processing before
    providing the data as an input to the DL model. It helps the DL architecture to
    extract features more accurately. Here we discuss image pre-processing operations
    used in the related studies.
  prefs: []
  type: TYPE_NORMAL
- en: Image Resizing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[[41](#bib.bibx41)] investigate the performance of Deep Convolutional Neural
    Networks based on spatial resolution. They used three different special resolutions
    30$\times$30, 45$\times$45, and 60$\times$60 pixels. The lower patch size achieved
    good accuracy and required less time to train the model. To make the processing
    faster and reduce the computational complexity, most of the studies performed
    image resizing operations on the dataset before inputting into the DL model. After
    collecting images from the field, the resolution of the images is reduced based
    on the DL network requirement. [[171](#bib.bibx171)] used 1280$\times$720 pixel-sized
    images to train DetectNet [[156](#bib.bibx156)] architecture and 640$\times$360
    pixels for GoogLeNet [[152](#bib.bibx152)] and VGGNet [[145](#bib.bibx145)] neural
    networks. The commonly used image sizes (in pixel) are- 64$\times$64 [[11](#bib.bibx11),
    [108](#bib.bibx108), [173](#bib.bibx173), [7](#bib.bibx7)], 128$\times$128 [[40](#bib.bibx40),
    [38](#bib.bibx38), [16](#bib.bibx16)], 224$\times$224 [[113](#bib.bibx113), [71](#bib.bibx71),
    [16](#bib.bibx16)], 227$\times$227 [[162](#bib.bibx162), [151](#bib.bibx151)],
    228$\times$228 [[89](#bib.bibx89)], 256$\times$256 [[34](#bib.bibx34), [155](#bib.bibx155),
    [121](#bib.bibx121), [61](#bib.bibx61), [122](#bib.bibx122)], 320$\times$240 [[29](#bib.bibx29)],
    288$\times$288 [[2](#bib.bibx2)], 360$\times$360 [[16](#bib.bibx16)].'
  prefs: []
  type: TYPE_NORMAL
- en: Images with high resolution are sometimes split into a number of patches to
    reduce the computational complexity. For instance, in the work of [[128](#bib.bibx128)],
    the images were split with a resolution of 5120$\times$3840 into 56 patches. Similar
    operations were performed by [[63](#bib.bibx63), [9](#bib.bibx9), [104](#bib.bibx104)]
    where they divided the original images into tiles of size 912$\times$1024, 1440$\times$960
    and 1000$\times$1000 pixels. [[127](#bib.bibx127)] captured only five images at
    high resolution using a drone which was then split into small patches of size
    480$\times$360 without overlapping and 512$\times$512 with 30% overlap. [[117](#bib.bibx117)]
    collected images using three cameras simultaneously of resolution 640$\times$480
    pixels. They then merged those into a single image of 1920$\times$480 pixels which
    was resized to 1024$\times$256 pixels. [[170](#bib.bibx170)] scaled down the images
    of their dataset to 1224$\times$1024 pixels, so that the training did not run
    low on memory. [[62](#bib.bibx62)] used orthomosaic imagery, which is usually
    quite large. They split the images into small patches of 1000$\times$1000 pixels.
    In the study of [[141](#bib.bibx141)], the images were resized to 1280$\times$720
    pixels and then cropped into four sub-images. [[114](#bib.bibx114)] used 1280$\times$960
    pixel size image with four spectral bands. By applying union operation on the
    red, green, and near infrared bands, they generated a false green image in order
    to highlight the vegetation. [[142](#bib.bibx142)] resized the collected image
    to 1280$\times$853 pixels and then cropped it to 1280$\times$720 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Background Removal
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[[65](#bib.bibx65)] collected images using a UAV and applied image mosaicing
    to generate an orthophoto. [[11](#bib.bibx11)] applied Hough-transform to highlight
    the aligned pixels and used Otsu-adaptive-thresholding method to differentiate
    the background and green crops or weeds. On the other hand, for removing the background
    soil image, [[108](#bib.bibx108)] applied the Normalised Difference Vegetation
    Index (NDVI). They also used morphological opening and closing operations to remove
    the noise and fill tiny gaps among vegetation pixels. To annotate the images manually
    into respective classes, [[34](#bib.bibx34)] applied the Simple Linear Iterative
    Clustering algorithm. This algorithm helps to segment weeds, crops, and background
    from images. Image pre-processing techniques were also involved in [[134](#bib.bibx134)]
    for having a bounding box around crop plants or weeds and removed the background.
    They first used image correlation and cropping for alignment and then applied
    Gaussian blur, followed by a sharpening operation to remove shadows, small debris,
    etc. Finally, for executing the blob detection process on connected pixels, Otsu’s
    method was employed. [[102](#bib.bibx102)] applied the pre-processing operation
    on red, green, blue, and NIR channels separately. They also performed the Gaussian
    blur operation to remove noise using a [5$\times$5] kernel. To standardise the
    channels, the values were subtracted by the mean of all channel values and divided
    by their standard deviation. After that, they normalised and zero-centred the
    channel values. [[72](#bib.bibx72)] applied a Contrast Limited Adaptive Histogram
    Equalisation algorithm to enhance the image contrast and reduce the image variation
    due to ambient illumination changes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the work of [[89](#bib.bibx89)] and [[13](#bib.bibx13)], all images were
    segmented using the Excess Green minus Excess Red Indices (ExG-ExR) method, which
    effectively removed the background. They also applied opening and closing morphological
    operations of images and generated contour masks to extract features. On the other
    hand, [[9](#bib.bibx9)] argued that the Maximum Likelihood Classification technique
    performed better than thresholding techniques for segmenting the background soil
    and green plants. According to [[4](#bib.bibx4)], images captured from the field
    had many problems (e.g. lack of brightness). It was necessary to apply image pre-processing
    operations to prepare the data for training. They performed several morphological
    operations to remove motion blur and light illumination. They also removed the
    noisy region before applying segmentation operations for separating the background.
    Threshold-based segmentation techniques had been used to separate the soil and
    green plants in an image. In the reports of [[40](#bib.bibx40)] and [[7](#bib.bibx7)],
    the RGB channels of the images were normalised to avoid differences in lighting
    conditions before removing the background. For vegetation segmentation, Otsu’s
    thresholding was applied, followed by the ExG (Excess Green) vegetation indexing
    operation. However, [[38](#bib.bibx38)] used a simple excessive green segmentation
    technique for removing the background and detecting the green pixels. [[79](#bib.bibx79)]
    converted the RGB image to HSV colour space, applied thresholding method and band-pass
    filtering, and then used binary masking to extract the image’s green component.
  prefs: []
  type: TYPE_NORMAL
- en: Image Enhancement and Denoising
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[[111](#bib.bibx111)] investigated the importance of image pre-processing operation
    by training the CNN model with raw data and processed data. They found that without
    image pre-processing the model performance decreased. They used Gaussian Blur
    for smoothing the images and removed the high-frequency content. They then converted
    the colour of the image to HSV space. Using a morphological erosion with an 11$\times$11
    structuring kernel, they subtracted the background soil and produced foreground
    seedling images. [[103](#bib.bibx103)] reported that image pre-processing improved
    the generalisation capabilities of a classification system. They applied [5$\times$5]
    Gaussian Kernel to remove noise and to normalise the data. They also zero-centred
    the pixel values of the image. The study of [[138](#bib.bibx138)] used the Gaussian
    and median filter to remove Gaussian noise and Salt and Pepper noise respectively.
    [[155](#bib.bibx155)] also normalised the data to maintain zero-mean and unit
    variance. Besides, they applied Principal Component Analysis and Zero-phase Component
    Analysis data whitening for eliminating the correlation among the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[164](#bib.bibx164)] evaluated the performance of the DL model based on the
    input representation of images. They applied many image pre-processing operations,
    such as histogram equalisation, automatic adjustment of the contrast of images
    and deep photo enhancement. They also used several vegetation indices including
    ExG, Excess Red, ExG-ExR, NDVI, Normalised Difference Index, Colour Index of Vegetation,
    Vegetative Index, and Modified Excess Green Index and Combined Indices. [[97](#bib.bibx97)]
    split the collected data into blocks which contained multiple plants. The blocks
    were then divided into sub-images with a single plant in them. After that, the
    histogram equalisation operation was performed to enhance the contrast of the
    sub-images.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[132](#bib.bibx132)] applied orthorectification and radiometric corrections
    operation to process the satellite image. They then normalised the pixel values
    of each band. After that, the large satellite image was split into 2138 samples
    of pixel size 128$\times$128.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Training Data Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To enlarge the size of the training data, in several related studies data augmentation
    was applied. It is a very useful technique when the dataset is not large enough
    [[161](#bib.bibx161)]. If there is a little variation [[138](#bib.bibx138)] or
    class imbalance [[11](#bib.bibx11)] among the images of the dataset, the image
    augmentation techniques are helpful. [[164](#bib.bibx164)] applied an augmentation
    to the dataset to determine the generalisation capability of their proposed approach.
    Table [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation ‣ 7 Dataset Preparation
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images") shows
    different types of data augmentation used in the relevant studies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Different types of data augmentation techniques used in the relevant
    studies'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Augmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Technique &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| Rotation | Rotate the image to the right or left on an axis between 1^∘ and
    359^∘  [[144](#bib.bibx144)] | [[11](#bib.bibx11), [89](#bib.bibx89), [138](#bib.bibx138),
    [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [41](#bib.bibx41), [7](#bib.bibx7),
    [38](#bib.bibx38), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scaling | Use zooming in/out to resize the image [[84](#bib.bibx84)]. | [[9](#bib.bibx9),
    [47](#bib.bibx47), [2](#bib.bibx2), [20](#bib.bibx20), [16](#bib.bibx16)] |'
  prefs: []
  type: TYPE_TB
- en: '| Shearing | Shift one part of the image to a direction and the other part
    to the opposite direction [[144](#bib.bibx144)]. | [[9](#bib.bibx9), [89](#bib.bibx89),
    [47](#bib.bibx47), [20](#bib.bibx20), [173](#bib.bibx173)] |'
  prefs: []
  type: TYPE_TB
- en: '| Flipping | Flip the image horizontally or vertically [[84](#bib.bibx84)].
    | [[9](#bib.bibx9), [1](#bib.bibx1), [138](#bib.bibx138), [47](#bib.bibx47), [2](#bib.bibx2),
    [38](#bib.bibx38), [29](#bib.bibx29), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173),
    [122](#bib.bibx122)] |'
  prefs: []
  type: TYPE_TB
- en: '| Gamma Correction | Encode and decode the luminance values of an image [[19](#bib.bibx19)].
    | [[164](#bib.bibx164)] |'
  prefs: []
  type: TYPE_TB
- en: '| Colour Space | Isolating a single colour channel, increase or decrease the
    brightness of the image, changing the intensity values in the histograms [[144](#bib.bibx144)].
    | [[11](#bib.bibx11), [164](#bib.bibx164), [9](#bib.bibx9), [40](#bib.bibx40),
    [2](#bib.bibx2), [29](#bib.bibx29), [122](#bib.bibx122)] |'
  prefs: []
  type: TYPE_TB
- en: '| Colour Space Transformations | Increase or decrease the pixel values by a
    constant value and restricting pixel values to a certain min or max value [[144](#bib.bibx144)].
    | [[89](#bib.bibx89), [138](#bib.bibx138), [16](#bib.bibx16), [122](#bib.bibx122)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Noise Injection | Injecting a matrix of random values to the image matrix.
    For example: Salt-Pepper noise, Gaussian noise etc [[144](#bib.bibx144)]. | [[138](#bib.bibx138),
    [40](#bib.bibx40), [122](#bib.bibx122)] |'
  prefs: []
  type: TYPE_TB
- en: '| Kernel filtering | Sharpening or blurring the image [[144](#bib.bibx144)].
    | [[11](#bib.bibx11), [9](#bib.bibx9), [40](#bib.bibx40), [122](#bib.bibx122)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cropping | Remove a certain portion of an image [[154](#bib.bibx154)]. Usually
    this is done at random in case of data augmentation [[144](#bib.bibx144)]. | [[9](#bib.bibx9),
    [2](#bib.bibx2), [41](#bib.bibx41), [122](#bib.bibx122)] |'
  prefs: []
  type: TYPE_TB
- en: '| Translation | Shift the position of all the image pixels [[66](#bib.bibx66)].
    | [[9](#bib.bibx9), [1](#bib.bibx1), [20](#bib.bibx20)] |'
  prefs: []
  type: TYPE_TB
- en: As shown in Table [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation ‣ 7 Dataset
    Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    it is observed that in most of the studies, different geometric transformation
    operation were applied to the data. Use of colour augmentation can be helpful
    to train a model for developing a real-time classification system. This is because
    the colour of the object varies depending on the lighting condition and motion
    of the sensors.
  prefs: []
  type: TYPE_NORMAL
- en: Image data that are not collected from the real environments and created artificially
    or programmatically are known as synthetic data or images [[163](#bib.bibx163)].
    It is not always possible to manage a large amount of labelled data to train a
    model. In these cases, the use of synthetic data is an excellent alternative to
    use together with the real data. Several research studies show that artificial
    data might have a significant change in classifying images [[8](#bib.bibx8)].
    In weed detection using DL approaches, synthetic data generation technique is
    not applied very often. [[128](#bib.bibx128)] used synthetically generated images
    to train the model and achieved a good classification accuracy while testing on
    a real dataset.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, [[121](#bib.bibx121)] created complex occlusion of crops
    and weeds and generated variation in leaf size, colour, and orientation by producing
    synthetic data. To minimise human effort for annotating data, [[33](#bib.bibx33)]
    generated synthetic data to train the model. For that purpose, they used a generic
    kinematic model of a leaf prototype to generate a single leaf of different plant
    species and then meshed that leaf to the artificial plant. Finally, they placed
    the plant in a virtual crop field for collecting the data without any extra effort
    for annotation.
  prefs: []
  type: TYPE_NORMAL
- en: '[[147](#bib.bibx147)] generated a 8000 synthetic dataset for labelling a real
    dataset. To create artificial data, they cropped out different parts of the plant,
    randomly selected any background from the real data, applied image processing
    (e.g. rotation, scaling, etc.), and added an artificial shadow using a Gaussian
    filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Data Labelling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of the reviewed publications used manually annotated data labelled
    by experts for training the deep learning model in a supervised manner. The researchers
    applied different annotations, such as bounding boxes annotation, pixel-level
    annotation, image-level annotation, polygon annotation, and synthetic labelling,
    based on the research need. Table [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7
    Dataset Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") shows different image annotation approaches used for weed detection.
    However, [[71](#bib.bibx71)] applied a semi-supervised method to label the images;
    they used a few labelled images to annotate the unlabelled data. On the other
    hand, [[35](#bib.bibx35)] proposed a semi-automatic labelling approach. Unlike
    semi-supervised data annotation, they did not use any manually labelled data,
    but applied the clustering method to label the data. First, they divided the data
    into different clusters according to their features and then labelled the clusters.
    Similar techniques were used by [[51](#bib.bibx51)]. [[170](#bib.bibx170)] separated
    the collected images into two parts; one with positive images that contained weeds,
    and the other of negative images without weeds. [[86](#bib.bibx86)] proposed an
    object-based approach to generate labelled data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Different image annotation techniques used for weed detection using
    deep learning'
  prefs: []
  type: TYPE_NORMAL
- en: '| Type of Image Annotation | Description | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| Pixel Level Annotation | Label each pixel whether it belongs to crop or weed
    in the image. | [[68](#bib.bibx68), [17](#bib.bibx17), [96](#bib.bibx96), [65](#bib.bibx65),
    [135](#bib.bibx135), [172](#bib.bibx172), [82](#bib.bibx82), [51](#bib.bibx51),
    [1](#bib.bibx1), [7](#bib.bibx7), [44](#bib.bibx44), [29](#bib.bibx29), [20](#bib.bibx20),
    [2](#bib.bibx2), [43](#bib.bibx43), [18](#bib.bibx18), [9](#bib.bibx9), [79](#bib.bibx79),
    [161](#bib.bibx161), [103](#bib.bibx103), [63](#bib.bibx63), [62](#bib.bibx62),
    [108](#bib.bibx108), [102](#bib.bibx102), [132](#bib.bibx132), [119](#bib.bibx119),
    [114](#bib.bibx114), [127](#bib.bibx127), [86](#bib.bibx86), [147](#bib.bibx147)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Region Level Annotation | Bounding Boxes Annotation | There may be a mixture
    of weeds and crops in a single image. Using a bounding box the crops and weeds
    are labelled in the image. | [[64](#bib.bibx64), [134](#bib.bibx134), [11](#bib.bibx11),
    [81](#bib.bibx81), [104](#bib.bibx104), [111](#bib.bibx111), [41](#bib.bibx41),
    [141](#bib.bibx141), [72](#bib.bibx72), [16](#bib.bibx16), [47](#bib.bibx47),
    [146](#bib.bibx146), [116](#bib.bibx116), [128](#bib.bibx128), [162](#bib.bibx162),
    [37](#bib.bibx37), [122](#bib.bibx122), [142](#bib.bibx142)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Polygon Annotation | This is used for semantic segmentation to detect
    irregular shaped object. It outlines the region of interest with arbitrary number
    of sides. | [[119](#bib.bibx119)] |'
  prefs: []
  type: TYPE_TB
- en: '| Image Level Annotation | Uses separate image for weeds and crops to train
    the model. | [[171](#bib.bibx171), [31](#bib.bibx31), [164](#bib.bibx164), [173](#bib.bibx173),
    [117](#bib.bibx117), [89](#bib.bibx89), [151](#bib.bibx151), [4](#bib.bibx4),
    [138](#bib.bibx138), [170](#bib.bibx170), [157](#bib.bibx157), [155](#bib.bibx155),
    [40](#bib.bibx40), [42](#bib.bibx42), [168](#bib.bibx168), [97](#bib.bibx97),
    [71](#bib.bibx71), [113](#bib.bibx113), [34](#bib.bibx34), [159](#bib.bibx159)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Synthetic Labelling | For training the model use generated and labelled data.
    | [[121](#bib.bibx121), [33](#bib.bibx33)] |'
  prefs: []
  type: TYPE_TB
- en: 'As summarised in Table [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7 Dataset
    Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    commonly used annotation techniques are bounding boxes, pixel-wise labelling and
    image level annotation. However, plants are irregular in shape: by using polygon
    annotation, the images of crops and weeds can be separated accurately. Synthetic
    labelling approaches can minimise labelling costs and help to generate large annotated
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Detection Approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Studies in this area apply two broad approaches for detecting, localising,
    and classifying weeds in crops: i) localise every plant in an image and classify
    that image either as a crop or as a weed; ii) map the density of weeds in the
    field. To detect weeds in crops, the concept of “row planting” has been used.
    In some of these studies, there are further classification steps of the weed species.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Plant-based Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To develop a weed management system, a major step is to classify every plant
    as weed or crop plant [[101](#bib.bibx101)]. The first problem is to detect weeds,
    followed by localisation and finally, classification. This approach is useful
    for real-time weed management techniques. For instance, [[126](#bib.bibx126)]
    developed a real-time weeding system where a robotic machine detected the weeds
    and used a knife to remove them. In this case, it was necessary to label individual
    plants, whether as a weed or crop plant. In traditional farming approaches, farmers
    usually apply a uniform amount of herbicide over the whole crop in a field. A
    machine needs to identify individual crop plants and weeds to apply automatic
    selective spraying techniques. Besides, identifying the weed species is also important
    to apply specific treatments [[102](#bib.bibx102)]. We have found that this approach
    has been used in most of the studies reported.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Weed Mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mapping weed density can also be helpful for site-specific weed management and
    can lead to a reduction in the use of herbicides. [[65](#bib.bibx65)] used the
    DL technique to map the density of weeds in a rice field. An appropriate amount
    of herbicides can be applied to a specific site based on the density map. The
    work in [[1](#bib.bibx1)] segmented the images and detected the weed presence
    in the region of that image. Using a deep learning approach, [[64](#bib.bibx64)]
    generated a weed distribution map of the field. In addition, some researchers
    argued that weed mapping helps to monitor the conditions of the field automatically
    [[134](#bib.bibx134), [135](#bib.bibx135)]. Farmers can monitor the distribution
    and spread of weeds, and can take action accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Learning Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.1 Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning occurs when the datasets for training and validation are
    labelled. The dataset passed in the DL model as input contains the image along
    with the corresponding labels. That means, in supervised training, the model learns
    how to create a map from a given input to a particular output based on the labelled
    dataset. Supervised learning is popular to solve classification and regression
    problems [[22](#bib.bibx22)]. In most of the related research the supervised learning
    approach was used to train the DL models. Section [10](#S10 "10 Deep Learning
    Architecture ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")
    presents a detail description of those DL architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Unsupervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning occurs when the training set is not labelled. The dataset
    passed as input in the unsupervised model has no corresponding annotation. The
    models attempt to learn the structure of the data and extract distinguishable
    information or features from data. Using this process, the model becomes able
    to map the input to the particular output. From this, the objects in the whole
    dataset will be divided into separate groups or clusters. The features of the
    objects in a cluster are similar and differ from other clusters. This is how unsupervised
    learning can classify objects of a dataset into separate categories. Clustering
    is one of the applications of unsupervised learning [[14](#bib.bibx14)].
  prefs: []
  type: TYPE_NORMAL
- en: Most of the relevant studies used a supervised learning approach to detect and
    classify weeds in crops automatically. However, [[35](#bib.bibx35)] proposed unsupervised
    clustering algorithms with a semi-automatic data labelling approach in their research.
    They applied two clustering methods- Joint Unsupervised Learning (JULE) and Deep
    Clustering for Unsupervised Learning of Visual Features algorithms (DeepCluster).
    They developed the models using AlexNet [[83](#bib.bibx83)] and VGG-16 [[145](#bib.bibx145)]
    architecture and initialised with pre-trained weights. They achieved a promising
    result (accuracy 97%) in classifying weeds in crops and reduce the cost of manual
    data labelling.
  prefs: []
  type: TYPE_NORMAL
- en: '[[155](#bib.bibx155)] applied an unsupervised K-means clustering algorithm
    as a pre-training process and generate a feature dictionary. They then used those
    features to initialise the weights of the CNN model. They claimed that it can
    improve generalisation ability in feature extraction and resolve the unstable
    identification problem. The proposed approach shows better accuracy than SVM,
    Back Propagation neural network, and even CNN with randomly initialised weights.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Semi-supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semi-supervised learning takes the middle ground between supervised and unsupervised
    learning [[94](#bib.bibx94)]. A few researchers used Graph Convolutional Network
    (GCN) [[78](#bib.bibx78)] in their research, which is a semi-supervised model.
    The major difference between CNN and GCN is the structure of input data. CNN is
    for regular structured data, whereas GCN uses graph data structure [[105](#bib.bibx105)].
    We discuss the use of GCN in the related work in Section [10.4](#S10.SS4 "10.4
    Graph Convolutional Network (GCN) ‣ 10 Deep Learning Architecture ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images").
  prefs: []
  type: TYPE_NORMAL
- en: 10 Deep Learning Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our analysis shows that the related studies apply different DL architectures
    to classify the weeds in crop plants based on the dataset and research goal. Most
    researchers compared their proposed models either with other DL architecture or
    with traditional machine learning approaches. Table LABEL:tab:different_DL_approach
    shows an overview of different DL approach used in weed detection. A CNN model
    generally consists of two basic parts- feature extraction and classification [[77](#bib.bibx77)].
    In related research, some researchers applied CNN models using various permutation
    of feature extraction and classification layers. However, in most cases, they
    preferred to use state-of-art CNN models like VGGNet [[145](#bib.bibx145)], ResNet
    (deep Residual Network) [[56](#bib.bibx56)], AlexNet [[83](#bib.bibx83)], InceptionNet
    [[152](#bib.bibx152)], and many more. Fully Convolutional Networks (FCNs) like
    SegNet [[10](#bib.bibx10)] and U-Net [[133](#bib.bibx133)] were also used in several
    studies.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Convolutional Neural Network (CNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 10.1.1 Pre-trained Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[151](#bib.bibx151)] applied six well known CNN models namely AlexNet, VGG-19,
    GoogLeNet, ResNet-50, ResNet-101 and Inception-v3\. They evaluated the network
    performance based on the transfer learning approach and found that pre-trained
    weights had a significant influence on training the model. They obtained the highest
    classification accuracy (98.7%) using the VGG-19 model, but it took the longest
    classification time. Considering that, the AlexNet model worked best for detecting
    volunteer potato plants in sugar beet according to their experimental setup. Even
    under varying light conditions, the model could classify plants with an accuracy
    of about 97%. The study of [[34](#bib.bibx34)] also supported that. They compared
    the classification accuracy of AlexNet with SVM, Adaboost – C4.5, and the Random
    Forest model. The AlexNet architecture performed better than other models in discriminating
    soybean crop, soil, grass, and broadleaf weeds. Similarly, [[162](#bib.bibx162)]
    reported that the AlexNet model with pre-trained weights showed excellent performance
    for detecting Rumex in grasslands. They also showed that by increasing heterogeneous
    characteristics of the input image might improve the model accuracy (90%). However,
    [[86](#bib.bibx86)] argued that to detect Rumex in grassland the VGG-16 model
    performs well with an accuracy of 92.1%.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[157](#bib.bibx157)] demonstrated that, although ImageNet dataset does not
    contain the images of different plant species, the pre-trained weights of the
    dataset could still help to reduce the number of training iterations. They fine-tuned
    Inception-v3 architecture for classifying eighteen weed species and determining
    growth stages based on the number of leaves. The model achieved the classification
    accuracy of 46% to 78% and showed an average accuracy of 70% while counting the
    leaves. However, [[113](#bib.bibx113)] differed from them. They developed a multi-class
    weed image dataset consisting of eight nationally significant weed species. The
    dataset contains 17,509 annotated images collected from different locations of
    northern Australia. They also applied the pre-trained Inception-v3 model along
    with ResNet-50 to classify the weed species (source code is available here: [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)).
    The average classification accuracy of ResNet-50 (95.7%) was a little higher than
    Inception-v3 (95.1%). [[11](#bib.bibx11)] also used ResNet with pre-trained weights
    as they found it more useful to detect weeds.'
  prefs: []
  type: TYPE_NORMAL
- en: According to [[171](#bib.bibx171)], Deep Convolutional Neural Network (DCNN)
    can perform well in detecting different species of weeds in bermudagrass. They
    used three pre-trained (from ImageNet dataset and KITTI dataset [[48](#bib.bibx48)])
    models including VGGNet, GoogLeNet and DetectNet. In another study, they added
    AlexNet architecture with the previous models for detecting weeds in perennial
    ryegrass [[170](#bib.bibx170)]. Though all the models performed well, DetectNet
    exhibited a bit higher F1 score of $\geq$0.99\. On the other hand, [[141](#bib.bibx141)]
    evaluated the performance of VGGNet, GoogLeNet, and DetectNet architecture using
    two variations of images (i.e., whole and cropped images). They also agreed that
    the DetectNet model could detect and classify weed in strawberry plants more accurately
    using cropped sub-images. They suggested that the most visible and prevalent part
    of the plant should be annotated rather than labelling the whole plant in the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[[89](#bib.bibx89)] proposed a model namely Filtered LBP (Local Binary Patterns)
    with Contour Mask and coefficient k (k-FLBPCM). They compared the model with VGG-16,
    VGG-19, ResNet-50, and Inception-v3 architecture. The k-FLBPCM method effectively
    classified barley, canola and wild radish with an accuracy of approximately 99%,
    which was better than other CNN models (source code is available here: [https://github.com/vinguyenle/k-FLBPCM-method](https://github.com/vinguyenle/k-FLBPCM-method)).
    The network was trained using pre-trained weights from ImageNet dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[7](#bib.bibx7)] compared the performance of LeNET [[92](#bib.bibx92)], AlexNet,
    cNET [[46](#bib.bibx46)], and sNET [[124](#bib.bibx124)] in their research. They
    found that cNET was better in classifying maize crop plants and their weeds. They
    further compared the performance of the original cNET architecture with the reduced
    number of filter layers (16 filter layers). The result reported that with pre-processed
    images, 16 filter layers were adequate to classify the crops and weeds. Besides,
    it made the model 2.5 times faster than its typical architecture and helped to
    detect weeds in real-time.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[117](#bib.bibx117)] analysed the performance of Faster R-CNN [[131](#bib.bibx131)],
    YOLO-v3 [[130](#bib.bibx130)], ResNet-50, ResNet-101, and Darknet-53 [[129](#bib.bibx129)]
    models to develop a smart sprayer for controlling weed in real-time. Based on
    precision and recall value, ResNet-50 model performed better than others. In contrast,
    [[16](#bib.bibx16)] applied the ResNet-101 model. They demonstrated that the size
    of the input image could affect the performance of ResNet-101 architecture. They
    used three different pixel sizes (i.e., 128px, 224px, and 360px) for their experiment
    and reported that model accuracy gets better by increasing the pixel size of the
    input image.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[159](#bib.bibx159)] proposed a multi-modal DL approach for classifying species
    of weeds. In this approach, they trained five pre-trained DL models, including
    NASNet, ResNet, Inception–ResNet, MobileNet, and VGGNet independently. The Bayesian
    conditional probability-based technique and priority weight scoring method were
    used to calculate the score vector of models. The model with better scores has
    a higher priority on determining the classes of species. To classify weed species,
    they summed up the probability vectors generated by the softmax layer of each
    model and the species with the highest probability value was determined. According
    to the experimental results, they argued that the performance of this approach
    was better than a single DL model.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Training from Scratch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[38](#bib.bibx38)] argued that, a CNN model initialised with pre-trained weights
    which was not trained on any plant images would not work well. They therefore
    built a new architecture using a combination of convolutional layers, batch normalisation,
    activation functions, max-pooling layers, fully connected layers, and residual
    layers according to their need. The model was used to classify twenty-two plant
    species, and they achieved a classification accuracy ranging from 33% to 98%.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[108](#bib.bibx108)] built a CNN model for blob wise discrimination of crops
    and weeds. They used multi-spectral images to train the model. They investigated
    different combinations of convolutional layers and fully connected layers to explore
    an optimised, light-weight and over-fitting problem-free model. Finally, using
    three convolutional layers and two fully connected layers, they obtained a better
    result. They stated that this approach did not have any geometric priors like
    planting the crops in rows. [[41](#bib.bibx41)] claimed in their research that
    the classification accuracy of the CNN model depended on the number of the hyperspectral
    band and the resolution of the image patch. They also built a CNN model using
    a combination of convolutional, nonlinear transformation, pooling and dropout
    layers. In further research, they proved that a CNN model trained with a higher
    number of bands could classify images more accurately than HoG (Histogram of oriented
    Gradients) based method [[42](#bib.bibx42)].'
  prefs: []
  type: TYPE_NORMAL
- en: '[[111](#bib.bibx111)] compared CNN’s performance with SVM (61.47%) and K-Nearest
    Neighbour (KNN) algorithm (56.84%) and found that CNN could distinguish crop plants
    from weeds better. They used six convolutional layers and three fully connected
    layers in the CNN architecture to achieve the accuracy of 92.6%. They also evaluated
    the accuracy of CNN using the original images and the pre-processed images. The
    experimental results suggested that classification accuracy improved by using
    pre-processed images. [[138](#bib.bibx138)] agreed that CNN offers better accuracy
    than SVM and ANN in detecting weeds in crop plants because of its deep learning
    ability. [[97](#bib.bibx97)] employed a CNN architecture that consists of three
    convolutional, three pooling, four Dropout layers, and a fully connected layer
    for developing a low-cost weed recognition system. Their experiment also proved
    that the performance of the CNN model in classification was better than the HoG
    and LBP methods. [[173](#bib.bibx173)] also demonstrated that the CNN model was
    better than SVM for detecting broad-leaf weeds in pastures. They used a CNN model
    with six convolutional layers and three fully connected classification layers.
    The model could recognise weeds with an accuracy of 96.88%, where SVM achieved
    maximum accuracy of 89.4%.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[121](#bib.bibx121)] used synthetic data to train their CNN model and evaluated
    it on real data. They built a CNN model with five convolutional layers and two
    fully connected layers. The results showed that CNN could classify crop plants
    and weeds very well from natural images and with multiple occlusion. Although
    [[128](#bib.bibx128)] applied the same architecture in their research, they argued
    that the Scatter Transform method achieved better accuracy with a small dataset
    than the CNN architecture. They compared several machine learning approaches like
    Scatter Transform, LBP, GLCM, Gabor filter with the CNN model. They also used
    synthetic data for training and evaluated the models’ performance on real field
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Region Proposal Networks (RPN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the tiny YOLO-v3 [[169](#bib.bibx169)] framework, [[47](#bib.bibx47)]
    proposed a DL model which speeds up the inference time of classification (source
    code is available here: [https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing](https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing)).
    They added two extra convolutional layers to the original model for better feature
    fusion and also reduced the number of detection scales to two. They trained the
    model with both synthetic data and real data. Although YOLO-v3 archived better
    classification accuracy in the experiments, they recommended the tiny YOLO-v3
    model for real-time application. [[142](#bib.bibx142)] also used tiny YOLO-v3
    model to detect goosegrass in strawberry and tomato plants.'
  prefs: []
  type: TYPE_NORMAL
- en: YOLO-v3 and tiny YOLO-v3 models were also employed in a research by [[116](#bib.bibx116)].
    The aim was to find a low-cost, smart weed management system. They applied the
    models on two machines with different hardware configurations. Their paper reported
    that YOLO-v3 showed good performance when tested on powerful and expensive computers,
    but the processing speed decreased if executed on a lower power computer. From
    their experiments, they came to the conclusion that to save the hardware cost,
    the tiny YOLO-v3 model was better. [[173](#bib.bibx173)] also preferred to use
    tiny YOLO-v3 instead of YOLO-v3, because it was a lightweight method and took
    less time and resources to classify objects. In contrast, [[31](#bib.bibx31)]
    proposed to use YOLO-v3 with a relatively larger input image size (832 $\times$
    832 pixels). They argued that the model performed better in their research with
    a small dataset. They agreed that tiny YOLO-v3 or Fast YOLO-v3 could improve the
    detection speed, but there was a need to compromise with the model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[[146](#bib.bibx146)] trained and evaluated the performance of a pre-trained
    Faster R-CNN and SSD (Single Shot Detector) [[100](#bib.bibx100)] object detection
    models to detect late-season weed in soybean fields. Moreover, they compared these
    object detection models with patch-based CNN model. The result showed that Faster
    R-CNN performed better in terms of weed detection accuracy and inference speed.
    [[72](#bib.bibx72)] proposed the Faster R-CNN model to detect the weeds and crop
    plants and to count the number of seedlings from the video frames. They used Inception-ResNet-v2
    architecture as the feature extractor. On the other hand, by applying the Mask
    R-CNN model on “Plant Seedlings Dataset” [[119](#bib.bibx119)] achieved more than
    98% classification accuracy. They argued that Mask R-CNN detected plant species
    more accurately with less training time than FCN.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[114](#bib.bibx114)] compared two RPN models, namely YOLO-v3 and Mask R-CNN
    with SVM. The classification accuracy of RPN architectures was 94%, whereas SVM
    achieved 88%. However, they reported that as SVM required less processing capacity,
    it could be used for IoT based solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Fully Convolutional Networks (FCN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike CNN, FCN replaces all the fully connected layers with convolutional layers
    and uses a transposed convolution layer to reconstruct the image with the same
    size as the input. It helps to predict the output by making a one-to-one correspondence
    with the input image in the spatial dimension [[143](#bib.bibx143), [65](#bib.bibx65)].
  prefs: []
  type: TYPE_NORMAL
- en: '[[63](#bib.bibx63)] compared the performance of AlexNet, VGGNet, and GoogLeNet
    as the base model for FCN architecture. VGGNet achieved the best accuracy among
    those. They further compared the model with patch-based CNN and pixel-based CNN
    architectures. The result showed that the VGG-16 based FCN model achieved the
    highest classification accuracy. On the other hand, [[64](#bib.bibx64)] applied
    ResNet-101 and VGG-16 as a baseline model of FCN for segmentation. They also compared
    the performance of the FCN models with a pixel-based SVM model. In their case,
    ResNet-101 based FCN architecture performed better. [[9](#bib.bibx9)] compared
    two FCN architecture for detecting weeds in canola fields, i.e., SegNet and U-Net.
    They used VGG-16 and ResNet-50 as the encoder block in both the models. The SegNet
    with ResNet-50 as the base model achieved the highest accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to [[104](#bib.bibx104)], SegNet (accuracy 92.7%) architecture was
    better than traditional FCN (accuracy 89.5%) and U-Net (accuracy 70.8%) for weed
    image segmentation when classifying rice plants and weeds in the paddy field.
    The study of [[1](#bib.bibx1)] reported that the accuracy of image segmentation
    depended on the size of the dataset. That is why it is difficult to train a model
    from scratch. To address this problem, they applied transfer learning and real-time
    data augmentation to train the model. In their experiment, they used VGG-16 based
    SegNet architecture. They applied three different transfer learning approaches
    for VGG-16\. Moreover, the performance of the model was compared with the VGG-19
    based architecture. The VGG-16 based SegNet achieved the highest accuracy of 96%
    when they used pre-trained weights only for feature extraction and the shallow
    machine learning classifier (i.e., SVM) for segmentation. [[134](#bib.bibx134)]
    also applied SegNet with the pre-trained VGG-16 as the base model (source code
    is available here: [https://github.com/inkyusa/weedNet](https://github.com/inkyusa/weedNet)).
    They trained the model by varying the number of channels in the input images.
    They then compared the inference speed and accuracy of different arrangements
    by deploying the model on an embedded GPU system, which was carried out by a small
    micro aerial vehicle (MAV). [[161](#bib.bibx161)] compared the performance of
    SegNet-512 and SegNet-256 encoder-decoder architectures for semantic segmentation
    of weeds in crop plants. The experiment proved that SegNet-512 was better for
    classification. In the study of [[33](#bib.bibx33)], the SegNet model was trained
    using synthetic data, and the performance was evaluated on a real crop and weed
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[44](#bib.bibx44)] proposed U-Net architecture using VGG-16 as an encoder
    for semantic segmentation. They also applied a VGG-16 model for classifying the
    crop plants and weeds. They also trained the model with one dataset containing
    sunflower crop and evaluated it with two different datasets with carrots and sugar
    beets crops. In the work of [[132](#bib.bibx132)], a ResNet based U-Net model
    was employed to map the presence of gamba grass in the satellite image. However,
    [[127](#bib.bibx127)] compared the performance of DeepLab-v3 [[30](#bib.bibx30)]
    with SegNet and U-Net model in their research. The results demonstrated that DeepLab-v3
    architecture achieved better classification accuracy using class balanced data
    that has greater spatial context.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[103](#bib.bibx103)] also proposed FCN architecture using DenseNet as a baseline
    model. Their novel approach provided a pixel-wise semantic segmentation of crop
    plants and weeds. The work of [[102](#bib.bibx102)] proposed a task-specific decoder
    network. As the plants were sown at a regular distance, they trained the model
    in a way so that the model could learn the spatial plant arrangement from the
    image sequence. They then fused this sequential feature with the visual features
    to localise and classify weeds in crop plants. [[37](#bib.bibx37)] used FCN architecture
    not only for segmentation but also for generating bounding boxes around the plants.
    They applied pre-trained GoogLeNet architecture as the base model.'
  prefs: []
  type: TYPE_NORMAL
- en: According to [[164](#bib.bibx164)], changes in the input representation could
    make a difference in classification performance. They employed the encoder-decoder
    deep learning network for semantic segmentation of crop and weed plants by initialising
    the input layers with pre-trained weights. They evaluated the model with different
    input representation by including NIR information with colour space transformation
    on the input, which improved crop-weed segmentation and classification accuracy
    (96%). [[135](#bib.bibx135)] also evaluated different input representation to
    train the network. They applied VGG-16 based SegNet architecture for detecting
    background, crop plants and weeds. The model was evaluated by varying the number
    of spectral bands and changing the hyper-parameters. The experimental results
    showed that the model achieved far better accuracy by using nine spectral channels
    of an image rather than the RGB image.
  prefs: []
  type: TYPE_NORMAL
- en: '[[62](#bib.bibx62)] stated that the original FCN-4s architecture was designed
    for PASCAL VOC 2011 dataset, which had 1000 classes of objects. However, their
    dataset had only three categories (i.e., rice, weeds, and others). As a result
    they reduced the feature maps of the intermediate layers to 2048\. They then compared
    the accuracy and efficiency of the model with original FCN-8s and DeepLab architecture
    and proved that the modified FCN-4s model performed better. For the same reason,
    [[18](#bib.bibx18)] simplified the original architecture of SegNet and named it
    as SegNet‐Basic. They decreased the number of convolutional layers from 13 to
    4.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems with the basic architecture of FCN is that the spatial features
    can not be recovered properly. The prediction accuracy can be decreased due to
    this issue. To address this problem, [[65](#bib.bibx65)] improved the model by
    adding skip architecture (SA), fully connected conditional random fields and partially
    connected conditional random fields. They fine-tuned AlexNet, VGGNet, GoogLeNet,
    and ResNet based FCN. They then compared the performance of different FCNs and
    Object-based image analysis (OBIA) method. Experimental results reported that
    the VGGNet-based FCN with proposed improvements achieved the highest accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '[[20](#bib.bibx20)] modified the original U-Net architecture for pixel-level
    classification of crop plants and weeds. They added a convolutional layer with
    a kernel size of 1$\times$1\. For that change, they adjusted the input size of
    the network. Besides, replacing the ReLU activation functions with the Exponential
    Linear Unit (ELU), they used adadelta optimiser algorithm instead of the stochastic
    gradient descent and included dropout layers in between convolutional layers.
    [[122](#bib.bibx122)] also modified the U-Net model to detect one species of weed
    in grasslands.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Graph Convolutional Network (GCN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[61](#bib.bibx61)] proposed Graph Weeds Net (GWN). GWN is a graph-based deep
    learning architecture to classify weed species. [[61](#bib.bibx61)] used ResNet-50
    and DenseNet-202 model to learn vertex features with graph convolution layers,
    vertex-wise dense layers, and the multi-level graph pooling mechanisms included
    in GWN architecture. Here, an RGB image was represented as a multi-scale graph.
    The graph-based model with DenseNet-202 architecture achieved the classification
    accuracy of 98.1%.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[71](#bib.bibx71)] proposed ResNet-101 based graph convolutional network in
    their research. They chose GCN, because it was a semi-supervised learning approach.
    Moreover, the feature relationships were captured using a graph structure. In
    this model, the label information was shared by neighbouring vertices of the graph,
    which make the learning more accurate with limited annotated data. They compared
    the proposed model with AlexNet, VGG-16, and ResNet-101 architecture on four different
    datasets. The GCN approach achieved 97.80%, 99.37%, 98.93% and 96.51% classification
    accuracy for each dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Hybrid Networks (HN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hybrid architectures are those where the researchers combine the characteristics
    of two or more DL models. For instance, [[26](#bib.bibx26)] proposed the AgroAVNET
    model, which was a hybrid of AlexNet and VGGNet architecture. They chose VGGNet
    for setting the depth of filters and used the normalisation concept of AlexNet.
    They then compared the performance of the AgroAVNET network with the original
    AlexNet and VGGNet and their different variants. All the parameters were initialised
    using pre-trained weights except for the third layer of the fully connected layers.
    They initialised that randomly. The AgroAVNET model outperformed others with a
    classification accuracy of 98.21%. However, [[43](#bib.bibx43)] adopted the feature
    concatenation approach in their research. They combined a super pixel-based LBP
    (SPLBP) method to extract local texture features, CNN for learning the spatial
    features and SVM for classification. They compared their proposed FCN-SPLBP model
    with CNN, LBP, FCN, and SPLBP architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '[[148](#bib.bibx148)] proposed OverFeat-GoogLeNet architecture by combining
    the features from LSTM and GoogLeNet model. The model was used to develop a “Parallelised
    Weed Detection System” by [[160](#bib.bibx160)]. They claimed that this system
    was robust, scalable and could be applied for real-time weed detection. The classification
    accuracy of the system was 91.1%.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[82](#bib.bibx82)] fine-tuned AlexNet, VGG-F, VGG-16, Inception-v1, ResNet-50,
    and ResNet-101 model to extract features from the images. They replaced the CNNs’
    default classifiers with linear classifiers, i.e., SVM and logistic regression.
    They compared the performance of various SVM and logistic regression classifiers
    by combining them with CNN models for detecting weeds. They achieved the most
    balanced result in terms of accuracy and false positive rate by using “L2-regularised
    with L2-loss logistic regression model using primal computation” classifier. This
    classifier performed better while being used with GoogLeNet architecture for detecting
    weeds in grasslands [[81](#bib.bibx81)].'
  prefs: []
  type: TYPE_NORMAL
- en: '[[40](#bib.bibx40)] also replaced CNN’s default classifier with traditional
    ML classifiers including SVM, XGBoost, and Logistic Regression. They initialised
    Xception, Inception-ResNet, VGGNets, MobileNet, and DenseNet model with pre-trained
    weights. The experimental result showed that the best performing network was DenseNet
    model with the SVM classifier. The micro F1 score for the architecture was 99.29%.
    This research also reported that with a small dataset, network performance could
    be enhanced using this approach.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2](#bib.bibx2)] proposed a fully convolutional encoder-decoder network named
    as Enhanced Skip Network. The model had multiple VGGNet-like blocks in the encoder
    and decoder. However, the decoder part had fewer future maps to reduce the computational
    complexity and memory requirement. Besides, the skip layers, larger convolutional
    kernels and a multi-scale filter bank were incorporated in the proposed model.
    The weights were initialised using the transfer learning method. The model performed
    better than U-Net, FCN8, and DeepLab-v3, Faster R-CNN, and EDNet in identifying
    weeds in the paddy field. [[29](#bib.bibx29)] combined U-Net architecture, MobileNet-v2
    and DenseNet architectures and replaced transposed convolution layers with activation
    map scaling.'
  prefs: []
  type: TYPE_NORMAL
- en: 11 Performance Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, evaluation metrics is the measurement tool to quantify the performance
    of a classifier. Different metrics are used to evaluate various characteristics
    of a classifier [[53](#bib.bibx53)]. The evaluation metrics can be used either
    to measure the quality of a classification model [[53](#bib.bibx53)] or to compare
    the performance of the different trained models for selecting the best one [[115](#bib.bibx115)].
    Various metrics were used in related studies based on the research need. The most
    commonly used metric is classification accuracy (CA) to evaluate the DL model.
    Many of the authors used multiple metrics to assess the model before drawing any
    conclusion. Table [6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A
    Survey of Deep Learning Techniques for Weed Detection from Images") lists the
    evaluation metrics applied in the relevant studies.
  prefs: []
  type: TYPE_NORMAL
- en: As Table [6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") shows, it is not
    easy to compare the related works as different types of evaluation metrics are
    employed depending on the DL model, the goal of classification, dataset and detection
    approach. However, the most frequently used evaluation metrics are CA, F1 score
    and mIoU. In the case of classifying plant species, researchers prefer to use
    confusion metrics to evaluate the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The evaluation metrics applied by different researchers of the related
    works'
  prefs: []
  type: TYPE_NORMAL
- en: '| No. | Performance Metric | Meaning |'
  prefs: []
  type: TYPE_TB
- en: '| 1. | Classification Accuracy (CA) | The percentage of correct prediction
    among the input. A model is judged based on how high the value is |'
  prefs: []
  type: TYPE_TB
- en: '| 2. | True Positive (TP) | How many times the model correctly predict the
    actual classes of the object. |'
  prefs: []
  type: TYPE_TB
- en: '| 3. | False Positive Rate (FPR) | It is the proportion of negative cases incorrectly
    identified as positive cases in the data. |'
  prefs: []
  type: TYPE_TB
- en: '| 4. | False Negative Rate (FNR) | The ratio of positive samples that were
    incorrectly classified. |'
  prefs: []
  type: TYPE_TB
- en: '| 5. | Specificity (S) | The fraction of True Negative from the sum of False
    Positive and True Negative. |'
  prefs: []
  type: TYPE_TB
- en: '| 6. | Mean Pixel Accuracy (MPA) | It is the average of ration of the correctly
    classified pixels among all pixels of the images in the dataset. It is used to
    evaluate the model for semantic segmentation. |'
  prefs: []
  type: TYPE_TB
- en: '| 7. | Precision (P) | The fraction of correct prediction (True Positive) from
    the total number of relevant result (Sum of True Positive and False Positive).
    It helps when the value of False Positives are high. |'
  prefs: []
  type: TYPE_TB
- en: '| 8. | Mean Average Precision (mAP) | It is the mean of average precision over
    all the classes of an object in the data. |'
  prefs: []
  type: TYPE_TB
- en: '| 9. | Recall | The fraction of True Positive from the sum of True Positive
    and False Negative. It helps when the value of False Negatives are high. |'
  prefs: []
  type: TYPE_TB
- en: '| 10. | F1 Score (F1) | The harmonic mean of precision and recall. |'
  prefs: []
  type: TYPE_TB
- en: '| 11. | Confusion Matrix (CM) | It is the summary of the number of correct
    and incorrect prediction made by a model. It helps to visualise not only the errors
    made by the model but also the types of error in predicting the class of object.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 12. | Intersection over Union (IoU) | It is the ratio of the overlapping
    area of ground truth (the hand labelled bounding boxes from the testing dataset)
    and predicted area (predicted bounding boxes from the model) to the total area.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 13. | Mean Intersection over Union (mIoU) | It is average IoU over all the
    classes of an object in the dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| 14. | Frequency Weighted Intersection over Union (FWIoU) | It is the weighted
    average of IoUs based on pixel classes. |'
  prefs: []
  type: TYPE_TB
- en: '| 15. | Mean Square Error (MSE) | It is the mean of all the squared errors
    between the predicted and actual target class |'
  prefs: []
  type: TYPE_TB
- en: '| 16. | Root Mean Square Error (RMSE) | It is the standard deviation of the
    difference between the predicted value and observed values. |'
  prefs: []
  type: TYPE_TB
- en: '| 17. | Mean Absolute Error (MAE) | It is the mean of the absolute values of
    each prediction error on all instances of the test dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| 18. | R2 | It is the squared correlation between the observed and the predicted
    outcome by the model. |'
  prefs: []
  type: TYPE_TB
- en: '| 19. | K-fold Cross Validation | The dataset is divided into K number of parts
    and each of the parts is used as testing dataset. |'
  prefs: []
  type: TYPE_TB
- en: '| 20. | Receiver Operating Characteristic (ROC) curve | The true positive rate
    is plotted in function of the false positive rate for different cut-off points
    of a parameter. |'
  prefs: []
  type: TYPE_TB
- en: '| 21. | Kappa Coefficient | Measures the degree of agreement between the true
    values and the predicted values |'
  prefs: []
  type: TYPE_TB
- en: '| 22. | Matthews correlation coefficient (MCC) | A correlation coefficient
    between the observed and predicted binary classifications |'
  prefs: []
  type: TYPE_TB
- en: '| 23. | Dice Similarity Coefficient (DSC) | It is a measure of spatial overlap
    between two sets of pixels. |'
  prefs: []
  type: TYPE_TB
- en: In addition to the evaluation metrics provided in Table [6](#S11.T6 "Table 6
    ‣ 11 Performance Evaluation Metrics ‣ A Survey of Deep Learning Techniques for
    Weed Detection from Images"), [[108](#bib.bibx108)] justified their model based
    on run-time. This was because, to develop a real-time weeds and crop plants classifier,
    it is important to identify the class of a plant as quickly as possible. They
    showed how quickly their model could detect a plant in an image. Similarly, [[151](#bib.bibx151)]
    calculated the classification accuracy of their model along with the time required
    to train and identify classes of plants, as they intended to develop a real-time
    classifier. [[104](#bib.bibx104)] also used run-time for justifying the model
    performance. They found that, by increasing the patch size of the input images,
    it was possible to reduce the time required to train the model. Another research
    method used inference time to compare different DL architecture [[63](#bib.bibx63)].
    [[34](#bib.bibx34)] evaluated the CNN model not only based on time but also in
    terms of the memory consumed by the model during training. They argued that though
    the CNN architecture achieved higher accuracy than other machine learning model,
    it required more time and memory to train the model. [[7](#bib.bibx7)] showed
    that reducing the number of layers of the DL model could make it faster in detecting
    and identifying the crop and weed plants. They also used processing time as an
    evaluation criterion while choosing the CNN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 12 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is evident that the DL model offers high performance in the area of weed
    detection and classification in crops. In this paper, we have provided an overview
    of the current status of the area of the automatic weed detection technique. In
    most relevant studies, the preferred method to acquire data was using a digital
    camera mounted on a ground vehicle to collect RGB images. A few research studies
    collected multi-spectral or hyper-spectral data. To prepare the dataset for training,
    different image processing techniques were used to resize the images, background
    and noise removing and image enhancement. The datasets were generally annotated
    using bounding boxes, pixel-wise and image level annotation approaches. For training
    the model, supervised learning approaches are applied by the researchers. They
    employ different DL techniques to find a better weed detection model. Detection
    accuracy is given as the most important parameter to evaluate the performance
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, there is still room for improvements in this area. Use of emerging
    technologies can help to improve the accuracy and speed of automatic weed detection
    systems. As crop and weed plants have many similarities, the use of other spectral
    indices can improve the performance.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a lack of large datasets for crops and weeds. It is necessary
    to construct a large benchmark dataset by capturing a variety of crops/weeds from
    different geographical locations, weather conditions and at various growth stages
    of crops and weeds. At the same time, it will be expensive to annotate these large
    datasets. Semi-supervised [[25](#bib.bibx25), [174](#bib.bibx174)] or weakly supervised
    [[176](#bib.bibx176), [36](#bib.bibx36)] approaches could be employed to address
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Generative Adversarial Network (GAN) [[93](#bib.bibx93)] or other
    synthetic data generation techniques can contribute to creating a large dataset.
    Random point generation and polygon labelling can further improve the precision
    of automatic weed detection systems. DL is evolving very fast, and new state-of-art
    techniques are being proposed. In addition to developing new solutions, researchers
    can enhance and apply those methods in the area of weed detection. They can also
    consider using weakly supervised, self-supervised or unsupervised approaches like
    multiple instance learning, few-shot or zero-shot learning as a means for synthetic
    data generation.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, most datasets mentioned in this paper exhibit class imbalance,
    which may create biases and lead to over-fitting of the model. Future research
    needs to address the problem. This can be achieved via the use of appropriate
    data redistribution approaches, cost-sensitive learning approaches [[76](#bib.bibx76)],
    or class balancing classifiers [[153](#bib.bibx153), [15](#bib.bibx15)].
  prefs: []
  type: TYPE_NORMAL
- en: To summarise, the primary objective of developing automatic weed detection system
    is to provide a weed management technique that will minimise cost and maximise
    crop yields. To do so, researchers need to come up with a system that can be deployed
    on devices with a lower computational requirement and can detect weeds accurately
    in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: 13 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study provides a comprehensive survey of the deep learning-based research
    in detecting and classifying weed species in value crops. A total of 70 relevant
    papers have been examined based on data acquisition, dataset preparation, detection
    and classification methods and model evaluation process. Publicly available datasets
    in the related field are also highlighted for prospective researchers. In this
    article, we provide a taxonomy of the research studies in this area and summarise
    the approaches of detecting weeds (Table LABEL:tab:different_DL_approach). It
    was found that most of the studies applied supervised learning techniques using
    state-of-art deep learning models and they can achieve better performance and
    classification accuracy by fine-tuning pre-trained models on any plant dataset.
    The results also show that the experiments already have achieved very high accuracy
    when a sufficient amount of labelled data of each class is available for training
    the models. However, the existing research only achieved high accuracy in a limited
    experiment setup, e.g., on small datasets of a select number of crops and weeds
    species. Computational speed in the recognition process is another limiting factor
    for deployment on real-time fast-moving herbicide spraying vehicles. An important
    future direction would be to investigate highly efficient detection techniques
    using very large datasets with a variety of crop and weed species so that one
    single model can be used across any weed-crop setting as needed. Other potential
    future research directions include the need for large generalised datasets, tailored
    machine learning models in weed-crop settings, addressing the class imbalance
    problems, identifying the growth stage of the weeds, as well as thorough field
    trials for commercial deployments.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Alwaseela Abdalla et al. “Fine-tuning convolutional neural network with
    transfer learning for semantic segmentation of ground-level oilseed rape images
    in a field with high weed pressure” In *Computers and Electronics in Agriculture*
    167 Elsevier, 2019, pp. 105091'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Shyam Prasad Adhikari, Heechan Yang and Hyongsuk Kim “Learning semantic
    graphics using convolutional encoder-decoder network for autonomous weeding in
    paddy field” In *Frontiers in plant science* 10 Frontiers, 2019, pp. 1404'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Jamil Ahmad et al. “Visual features based boosted classification of weeds
    for real-time selective herbicide sprayer systems” In *Computers in Industry*
    98 Elsevier, 2018, pp. 23–33'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Mansoor Alam et al. “Real-Time Machine-Learning Based Crop/Weed Detection
    and Classification for Variable-Rate Spraying in Precision Agriculture” In *2020
    7th International Conference on Electrical and Electronics Engineering (ICEEE)*,
    2020, pp. 273–280 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Md Zahangir Alom et al. “A state-of-the-art survey on deep learning theory
    and architectures” In *Electronics* 8.3 Multidisciplinary Digital Publishing Institute,
    2019, pp. 292'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Sandra Amend et al. “Weed Management of the Future” In *KI-Künstliche Intelligenz*
    33.4 Springer, 2019, pp. 411–415'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Córdova-Cruzatty Andrea, Barreno Barreno Mauricio Daniel and Jácome Barrionuevo
    José Misael “Precise weed and maize classification through convolutional neuronal
    networks” In *2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)*, 2017,
    pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Paolo Andreini et al. “Image generation by gan and style transfer for agar
    plate image segmentation” In *Computer Methods and Programs in Biomedicine* 184
    Elsevier, 2020, pp. 105268'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Muhammad Hamza Asad and Abdul Bais “Weed detection in canola fields using
    maximum likelihood classification and deep convolutional neural network” In *Information
    Processing in Agriculture*, 2019 DOI: [10.1016/j.inpa.2019.12.002](https://dx.doi.org/10.1016/j.inpa.2019.12.002)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla “Segnet: A deep
    convolutional encoder-decoder architecture for image segmentation” In *IEEE transactions
    on pattern analysis and machine intelligence* 39.12 IEEE, 2017, pp. 2481–2495'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M Dian Bah, Adel Hafiane and Raphael Canals “Deep learning with unsupervised
    data labeling for weed detection in line crops in UAV images” In *Remote sensing*
    10.11 Multidisciplinary Digital Publishing Institute, 2018, pp. 1690'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Adel Bakhshipour and Abdolabbas Jafari “Evaluation of support vector machine
    and artificial neural networks in weed detection using shape features” In *Computers
    and Electronics in Agriculture* 145 Elsevier, 2018, pp. 153–160'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Adel Bakhshipour, Abdolabbas Jafari, Seyed Mehdi Nassiri and Dariush Zare
    “Weed segmentation using texture features extracted from wavelet sub-images” In
    *Biosystems Engineering* 157 Elsevier, 2017, pp. 1–12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Horace B Barlow “Unsupervised learning” In *Neural computation* 1.3 MIT
    Press, 1989, pp. 295–311'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Jingjun Bi and Chongsheng Zhang “An empirical comparison on state-of-the-art
    multi-class imbalance learning algorithms and a new diversified ensemble learning
    scheme” In *Knowledge-Based Systems* 158 Elsevier, 2018, pp. 81–93'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Abdel-Aziz Binguitcha-Fare and Prince Sharma “Crops and weeds classification
    using Convolutional Neural Networks via optimization of transfer learning parameters”
    In *International Journal of Engineering and Advanced Technology (IJEAT)* 8.5
    Blue Eyes Intelligence Engineering & Sciences Publication, 2019, pp. 2284–2294'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] D Bini, D Pamela and Shajin Prince “Machine Vision and Machine Learning
    for Intelligent Agrobots: A review” In *2020 5th International Conference on Devices,
    Circuits and Systems (ICDCS)*, 2020, pp. 12–16 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Petra Bosilj, Erchan Aptoula, Tom Duckett and Grzegorz Cielniak “Transfer
    learning between crop types for semantic segmentation of crops versus weeds in
    precision agriculture” In *Journal of Field Robotics* 37.1 Wiley Online Library,
    2020, pp. 7–19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] E. Brasseur In *ericbrasseur.org* URL: [http://www.ericbrasseur.org/gamma.html?i=1](http://www.ericbrasseur.org/gamma.html?i=1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Anderson Brilhador et al. “Classification of Weeds and Crops at the Pixel-Level
    Using Convolutional Neural Networks and Data Augmentation” In *2019 IEEE Latin
    American Conference on Computational Intelligence (LA-CCI)*, 2019, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Ralph B Brown and Scott D Noble “Site-specific weed management: sensing
    requirements—what do we need to see?” In *Weed Science* 53.2 BioOne, 2005, pp.
    252–258'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Rich Caruana and Alexandru Niculescu-Mizil “An empirical comparison of
    supervised learning algorithms” In *Proceedings of the 23rd international conference
    on Machine learning*, 2006, pp. 161–168'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Paulo César Pereira Júnior et al. “Comparison of Supervised Classifiers
    and Image Features for Crop Rows Segmentation on Aerial Images” In *Applied Artificial
    Intelligence* 34.4 Taylor & Francis, 2020, pp. 271–291'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N Zhang C Chaisattapagon “Effective criteria for weed identification in
    wheat fields using machine vision” In *Transactions of the ASAE* 38.3 American
    Society of AgriculturalBiological Engineers, 1995, pp. 965–974'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Olivier Chapelle, Bernhard Scholkopf and Alexander Zien “Semi-supervised
    learning (chapelle, o. et al., eds.; 2006)[book reviews]” In *IEEE Transactions
    on Neural Networks* 20.3 IEEE, 2009, pp. 542–542'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Trupti R Chavan and Abhijeet V Nandedkar “AgroAVNET for crops and weeds
    classification: A step forward in automatic farming” In *Computers and Electronics
    in Agriculture* 154 Elsevier, 2018, pp. 361–372'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Nived Chebrolu, Thomas Läbe and Cyrill Stachniss “Robust long-term registration
    of UAV images of crop fields for precision agriculture” In *IEEE Robotics and
    Automation Letters* 3.4 IEEE, 2018, pp. 3097–3104'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Nived Chebrolu et al. “Agricultural robot dataset for plant classification,
    localization and mapping on sugar beet fields” In *The International Journal of
    Robotics Research* 36.10 SAGE Publications Sage UK: London, England, 2017, pp.
    1045–1052'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Lukasz Chechlinski, Barbara Siemikatkowska and Michal Majewski “A System
    for Weeds and Crops Identification—Reaching over 10 FPS on Raspberry Pi with the
    Usage of MobileNets, DenseNet and Custom Modifications” In *Sensors* 19.17 Multidisciplinary
    Digital Publishing Institute, 2019, pp. 3787'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Liang-Chieh Chen, George Papandreou, Florian Schroff and Hartwig Adam
    “Rethinking atrous convolution for semantic image segmentation” In *arXiv preprint
    arXiv:1706.05587*, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Vitali Czymmek, Leif O Harders, Florian J Knoll and Stephan Hussmann “Vision-Based
    Deep Learning Approach for Real-Time Detection of Weeds in Organic Farming” In
    *2019 IEEE International Instrumentation and Measurement Technology Conference
    (I2MTC)*, 2019, pp. 1–5 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Jia Deng et al. “Imagenet: A large-scale hierarchical image database”
    In *2009 IEEE conference on computer vision and pattern recognition*, 2009, pp.
    248–255 Ieee'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti and Alberto Pretto “Automatic
    model based dataset generation for fast and accurate crop and weeds detection”
    In *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2017, pp. 5188–5195 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Alessandro Santos Ferreira et al. “Weed detection in soybean crops using
    ConvNets” In *Computers and Electronics in Agriculture* 143 Elsevier, 2017, pp.
    314–324'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Alessandro Santos Ferreira et al. “Unsupervised deep learning and semi-automatic
    data labeling in weed discrimination” In *Computers and Electronics in Agriculture*
    165 Elsevier, 2019, pp. 104963'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Thibaut Durand, Taylor Mordan, Nicolas Thome and Matthieu Cord “Wildcat:
    Weakly supervised learning of deep convnets for image classification, pointwise
    localization and segmentation” In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 642–651'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Mads Dyrmann, Rasmus Nyholm Jørgensen and Henrik Skov Midtiby “RoboWeedSupport-Detection
    of weed locations in leaf occluded cereal crops using a fully convolutional neural
    network” In *Adv. Anim. Biosci* 8.2, 2017, pp. 842–847'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Mads Dyrmann, Henrik Karstoft and Henrik Skov Midtiby “Plant species classification
    using deep convolutional neural network” In *Biosystems Engineering* 151 Elsevier,
    2016, pp. 72–80'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Max Ehrlich and Larry S Davis “Deep residual learning in the jpeg transform
    domain” In *Proceedings of the IEEE International Conference on Computer Vision*,
    2019, pp. 3484–3493'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Borja Espejo-Garcia et al. “Towards weeds identification assistance through
    transfer learning” In *Computers and Electronics in Agriculture* 171 Elsevier,
    2020, pp. 105306'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Adnan Farooq, Jiankun Hu and Xiuping Jia “Analysis of spectral bands and
    spatial resolutions for weed classification via deep convolutional neural network”
    In *IEEE Geoscience and Remote Sensing Letters* 16.2 IEEE, 2018, pp. 183–187'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Adnan Farooq, Jiankun Hu and Xiuping Jia “Weed classification in hyperspectral
    remote sensing images via deep convolutional neural network” In *IGARSS 2018-2018
    IEEE International Geoscience and Remote Sensing Symposium*, 2018, pp. 3816–3819
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Adnan Farooq, Xiuping Jia, Jiankun Hu and Jun Zhou “Multi-resolution weed
    classification via convolutional neural network and superpixel based local binary
    pattern using remote sensing images” In *Remote Sensing* 11.14 Multidisciplinary
    Digital Publishing Institute, 2019, pp. 1692'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Mulham Fawakherji et al. “Crop and weeds classification for precision
    agriculture using context-independent pixel-wise segmentation” In *2019 Third
    IEEE International Conference on Robotic Computing (IRC)*, 2019, pp. 146–152 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C Fernández-Quintanilla et al. “Is the current state of the art of weed
    monitoring suitable for site-specific weed management in arable crops?” In *Weed
    research* 58.4 Wiley Online Library, 2018, pp. 259–272'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] AJ Gabor, RR Leach and FU Dowla “Automated seizure detection using a self-organizing
    neural network” In *Electroencephalography and clinical Neurophysiology* 99.3
    Elsevier, 1996, pp. 257–266'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Junfeng Gao et al. “Deep convolutional neural networks for image-based
    Convolvulus sepium detection in sugar beet fields” In *Plant Methods* 16.1 BioMed
    Central, 2020, pp. 1–12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Andreas Geiger, Philip Lenz, Christoph Stiller and Raquel Urtasun “Vision
    meets robotics: The kitti dataset” In *The International Journal of Robotics Research*
    32.11 Sage Publications Sage UK: London, England, 2013, pp. 1231–1237'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Thomas Mosgaard Giselsson et al. “A public image database for benchmark
    of plant seedling classification algorithms” In *arXiv preprint arXiv:1711.05458*,
    2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Jiuxiang Gu et al. “Recent advances in convolutional neural networks”
    In *Pattern Recognition* 77 Elsevier, 2018, pp. 354–377'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] David Hall, Feras Dayoub, Tristan Perez and Chris Mccool “A rapidly deployable
    classification system using visual data for the application of precision weed
    management” In *Computers and Electronics in Agriculture* 148, 2018, pp. 107–120
    DOI: [10.1016/j.compag.2018.02.023](https://dx.doi.org/10.1016/j.compag.2018.02.023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Esmael Hamuda, Brian Mc Ginley, Martin Glavin and Edward Jones “Automatic
    crop detection under field conditions using the HSV colour space and morphological
    operations” In *Computers and electronics in agriculture* 133 Elsevier, 2017,
    pp. 97–107'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] David J Hand “Measuring classifier performance: a coherent alternative
    to the area under the ROC curve” In *Machine learning* 77.1 Springer, 2009, pp.
    103–123'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Sebastian Haug and Jörn Ostermann “A crop/weed field image dataset for
    the evaluation of computer vision based precision agriculture tasks” In *European
    Conference on Computer Vision*, 2014, pp. 105–116 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Sebastian Haug and Jörn Ostermann “A Crop/Weed Field Image Dataset for
    the Evaluation of Computer Vision Based Precision Agriculture Tasks” In *Computer
    Vision - ECCV 2014 Workshops*, 2015, pp. 105–116 DOI: [10.1007/978-3-319-16220-1˙8](https://dx.doi.org/10.1007/978-3-319-16220-1_8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun “Deep residual learning
    for image recognition” In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2016, pp. 770–778'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J Hemming and T Rath “Image processing for plant determination using the
    Hough transform and clustering methods” In *Gartenbauwissenschaft* 67.1 Stuttgart:
    Eugen Ulmer GmbH & Co., 1928-2002., 2002, pp. 1–10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Geoffrey E Hinton, Simon Osindero and Yee-Whye Teh “A fast learning algorithm
    for deep belief nets” In *Neural computation* 18.7 MIT Press, 2006, pp. 1527–1554'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Jodie S Holt “Principles of Weed Management in Agroecosystems and Wildlands1”
    In *Weed Technology* 18.sp1 BioOne, 2004, pp. 1559–1562'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Mohammad-Parsa Hosseini et al. “Deep learning architectures” In *Deep
    learning: concepts and architectures* Springer, 2020, pp. 1–24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Kun Hu et al. “Graph weeds net: A graph-based deep learning method for
    weed recognition” In *Computers and Electronics in Agriculture* 174 Elsevier,
    2020, pp. 105520'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Huasheng Huang et al. “Accurate weed mapping and prescription map generation
    based on fully convolutional networks using UAV imagery” In *Sensors* 18.10 Multidisciplinary
    Digital Publishing Institute, 2018, pp. 3299'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Huasheng Huang et al. “A fully convolutional network for weed mapping
    of unmanned aerial vehicle (UAV) imagery” In *PloS one* 13.4 Public Library of
    Science San Francisco, CA USA, 2018, pp. e0196302'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Huasheng Huang et al. “A semantic labeling approach for accurate weed
    mapping of high resolution UAV imagery” In *Sensors* 18.7 Multidisciplinary Digital
    Publishing Institute, 2018, pp. 2113'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Huasheng Huang et al. “Deep learning versus Object-based Image Analysis
    (OBIA) in weed mapping of UAV imagery” In *International Journal of Remote Sensing*
    41.9 Taylor & Francis, 2020, pp. 3446–3479'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Sheng-Wei Huang et al. “Auggan: Cross domain adaptation with gan-based
    data augmentation” In *Proceedings of the European Conference on Computer Vision
    (ECCV)*, 2018, pp. 718–731'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Nadeem Iqbal, Sudheesh Manalil, Bhagirath S Chauhan and Steve W Adkins
    “Investigation of alternate herbicides for effective weed management in glyphosate-tolerant
    cotton” In *Archives of Agronomy and Soil Science* 65.13 Taylor & Francis, 2019,
    pp. 1885–1899'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Asnor Juraiza Ishak, Siti Salasiah Mokri, Mohd Marzuki Mustafa and Aini
    Hussain “Weed detection utilizing quadratic polynomial and ROI techniques” In
    *2007 5th Student Conference on Research and Development*, 2007, pp. 1–5 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Abdolabbas Jafari, Seyed Saeid Mohtasebi, H Eghbali Jahromi and Mahmoud
    Omid “Weed detection in sugar beet fields using machine vision” In *Int. J. Agric.
    Biol* 8.5, 2006, pp. 602–605'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Troy Arnold Jensen, Bruen Smith and Livia Faria Defeo “An automated site-specific
    fallow weed management system using unmanned aerial vehicles”, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Honghua Jiang et al. “CNN feature based graph convolutional network for
    weed and crop recognition in smart farming” In *Computers and Electronics in Agriculture*
    174 Elsevier, 2020, pp. 105450'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Yu Jiang, Changying Li, Andrew H Paterson and Jon S Robertson “DeepSeedling:
    deep convolutional network and Kalman filter for plant seedling detection and
    counting in the field” In *Plant methods* 15.1 Springer, 2019, pp. 141'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Andreas Kamilaris and Francesc X Prenafeta-Boldú “Deep learning in agriculture:
    A survey” In *Computers and electronics in agriculture* 147 Elsevier, 2018, pp.
    70–90'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Y Karimi, SO Prasher, RM Patel and SH Kim “Application of support vector
    machine technology for weed and nitrogen stress detection in corn” In *Computers
    and electronics in agriculture* 51.1-2 Elsevier, 2006, pp. 99–109'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Wajahat Kazmi et al. “Detecting creeping thistle in sugar beet fields
    using vegetation indices” In *Computers and Electronics in Agriculture* 112 Elsevier,
    2015, pp. 10–19'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Salman H Khan et al. “Cost-sensitive learning of deep feature representations
    from imbalanced data” In *IEEE transactions on neural networks and learning systems*
    29.8 IEEE, 2017, pp. 3573–3587'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Mina Khoshdeli, Richard Cong and Bahram Parvin “Detection of nuclei in
    H&E stained sections using convolutional neural networks” In *2017 IEEE EMBS International
    Conference on Biomedical & Health Informatics (BHI)*, 2017, pp. 105–108 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Thomas N Kipf and Max Welling “Semi-supervised classification with graph
    convolutional networks” In *arXiv preprint arXiv:1609.02907*, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Florian J Knoll, Vitali Czymmek, Leif O Harders and Stephan Hussmann “Real-time
    classification of weeds in organic carrot production using deep learning algorithms”
    In *Computers and Electronics in Agriculture* 167 Elsevier, 2019, pp. 105097'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S Kodagoda, Z Zhang, D Ruiz and G Dissanayake “Weed detection and classification
    for autonomous farming” In *Intelligent Production Machines and Systems* I* PROMS,
    2008'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Tsampikos Kounalakis et al. “A robotic system employing deep learning
    for visual recognition and detection of weeds in Grasslands” In *2018 IEEE International
    Conference on Imaging Systems and Techniques (IST)*, 2018, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Tsampikos Kounalakis, Georgios A Triantafyllidis and Lazaros Nalpantidis
    “Deep learning-based visual recognition of rumex for robotic precision farming”
    In *Computers and Electronics in Agriculture* 165 Elsevier, 2019, pp. 104973'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Alex Krizhevsky, Ilya Sutskever and Geoffrey E Hinton “Imagenet classification
    with deep convolutional neural networks” In *Advances in neural information processing
    systems* 25, 2012, pp. 1097–1105'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Harshit Kumar “Data augmentation Techniques” In *OpenGenus IQ: Learn Computer
    Science* OpenGenus IQ: Learn Computer Science, 2019 URL: [https://iq.opengenus.org/data-augmentation/](https://iq.opengenus.org/data-augmentation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Rattan Lal “Soil structure and sustainability” In *Journal of sustainable
    agriculture* 1.4 Taylor & Francis, 1991, pp. 67–92'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Olee Hoi Ying Lam et al. “An open source workflow for weed mapping in
    native grassland using unmanned aerial vehicle: using Rumex obtusifolius as a
    case study” In *European Journal of Remote Sensing* Taylor & Francis, 2020, pp.
    1–18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Petre Lameski, Eftim Zdravevski and Andrea Kulakov “Review of automated
    weed control approaches: an environmental impact perspective” In *International
    Conference on Telecommunications*, 2018, pp. 132–147 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Petre Lameski, Eftim Zdravevski, Vladimir Trajkovik and Andrea Kulakov
    “Weed detection dataset with RGB images taken under variable light conditions”
    In *International Conference on ICT Innovations*, 2017, pp. 112–119 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Vi Nguyen Thanh Le, Selam Ahderom and Kamal Alameh “Performances of the
    LBP Based Algorithm over CNN Models for Detecting Crops and Weeds with Similar
    Morphologies” In *Sensors* 20.8 Multidisciplinary Digital Publishing Institute,
    2020, pp. 2193'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Vi Nguyen Thanh Le, Selam Ahderom, Beniamin Apopei and Kamal Alameh “A
    novel method for detecting morphologically similar crops and weeds based on the
    combination of contour masks and filtered Local Binary Pattern operators” In *GigaScience*
    9.3 Oxford University Press, 2020, pp. giaa017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Yann LeCun, Yoshua Bengio and Geoffrey Hinton “Deep learning” In *nature*
    521.7553 Nature Publishing Group, 2015, pp. 436–444'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Yann LeCun et al. “Backpropagation applied to handwritten zip code recognition”
    In *Neural computation* 1.4 MIT Press, 1989, pp. 541–551'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Christian Ledig et al. “Photo-realistic single image super-resolution
    using a generative adversarial network” In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 4681–4690'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Dong-Hyun Lee “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks” In *Workshop on challenges in representation
    learning, ICML* 3.2, 2013'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Simon Leminen Madsen et al. “Open Plant Phenotype Database of Common Weeds
    in Denmark” In *Remote Sensing* 12.8 Multidisciplinary Digital Publishing Institute,
    2020, pp. 1246'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Konstantinos G Liakos et al. “Machine learning in agriculture: A review”
    In *Sensors* 18.8 Multidisciplinary Digital Publishing Institute, 2018, pp. 2674'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Wei-Che Liang, You-Jei Yang and Chih-Min Chao “Low-Cost Weed Identification
    System Using Drones” In *2019 Seventh International Symposium on Computing and
    Networking Workshops (CANDARW)*, 2019, pp. 260–263 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Tsung-Yi Lin et al. “Microsoft coco: Common objects in context” In *European
    conference on computer vision*, 2014, pp. 740–755 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Bo Liu and Ryan Bruch “Weed Detection for Selective Spraying: a Review”
    In *Current Robotics Reports* 1.1 Springer, 2020, pp. 19–26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Wei Liu et al. “Ssd: Single shot multibox detector” In *European conference
    on computer vision*, 2016, pp. 21–37 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Philipp Lottes et al. “Joint stem detection and crop-weed classification
    for plant-specific treatment in precision farming” In *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2018, pp. 8233–8238 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Philipp Lottes et al. “Robust joint stem detection and crop-weed classification
    using image sequences for plant-specific treatment in precision farming” In *Journal
    of Field Robotics* 37.1 Wiley Online Library, 2020, pp. 20–34'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Philipp Lottes, Jens Behley, Andres Milioto and Cyrill Stachniss “Fully
    convolutional networks with sequential information for robust crop and weed detection
    in precision farming” In *IEEE Robotics and Automation Letters* 3.4 IEEE, 2018,
    pp. 2870–2877'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Xu Ma et al. “Fully convolutional network for rice seedling and weed
    image segmentation at the seedling stage in paddy fields” In *PloS one* 14.4 Public
    Library of Science San Francisco, CA USA, 2019, pp. e0215676'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Inneke Mayachita “Understanding Graph Convolutional Networks for Node
    Classification” In *Medium* Towards Data Science, 2020 URL: [https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] C.. Merfield “Robotic weeding’s false dawn? Ten requirements for fully
    autonomous mechanical weed management” In *Weed Research* 56.5, 2016, pp. 340–344
    DOI: [10.1111/wre.12217](https://dx.doi.org/10.1111/wre.12217)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] GE Meyer et al. “Textural imaging and discriminant analysis for distinguishingweeds
    for spot spraying” In *Transactions of the ASAE* 41.4 American Society of AgriculturalBiological
    Engineers, 1998, pp. 1189'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Andres Milioto, Philipp Lottes and Cyrill Stachniss “Real-time blob-wise
    sugar beets vs weeds classification for monitoring fields using convolutional
    neural networks” In *ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
    Information Sciences* 4 Copernicus GmbH, 2017, pp. 41'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Syed I Moazzam et al. “A Review of Application of Deep Learning for Weeds
    and Crops Classification in Agriculture” In *2019 International Conference on
    Robotics and Automation in Industry (ICRAI)*, 2019, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Maryam M Najafabadi et al. “Deep learning applications and challenges
    in big data analytics” In *Journal of Big Data* 2.1 Springer, 2015, pp. 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Daniel K Nkemelu, Daniel Omeiza and Nancy Lubalo “Deep convolutional
    neural network for plant seedlings classification” In *arXiv preprint arXiv:1811.08404*,
    2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] K. Okese, Theresah Kankam, Jennifer Boamah and Owusu Mensah Evans “Basic
    Principles of Weeds Control and Management” In *MyFarm Blog*, 2020 URL: [https://blog.agrihomegh.com/principles-weeds-control-management/](https://blog.agrihomegh.com/principles-weeds-control-management/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Alex Olsen et al. “DeepWeeds: A multiclass weed species image dataset
    for deep learning” In *Scientific reports* 9.1 Nature Publishing Group, 2019,
    pp. 1–12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Kavir Osorio et al. “A Deep Learning Approach for Weed Detection in Lettuce
    Crops Using Multispectral Images” In *AgriEngineering* 2.3 Multidisciplinary Digital
    Publishing Institute, 2020, pp. 471–488'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Akin Ozcift and Arif Gulten “Classifier ensemble construction with rotation
    forest to improve medical diagnosis performance of machine learning algorithms”
    In *Computer methods and programs in biomedicine* 104.3 Elsevier, 2011, pp. 443–451'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Victor Partel, Sri Charan Kakarla and Yiannis Ampatzidis “Development
    and evaluation of a low-cost and smart technology for precision weed management
    utilizing artificial intelligence” In *Computers and electronics in agriculture*
    157 Elsevier, 2019, pp. 339–350'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Victor Partel et al. “Smart Sprayer for Precision Weed Control Using
    Artificial Intelligence: Comparison of Deep Learning Frameworks” In *Association
    for the Advancement of Artificial Intelligence*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] DD Patel and BA Kumbhar “Weed and its management: A major threats to
    crop economy” In *J. Pharm. Sci. Bioscientific Res* 6.6, 2016, pp. 453–758'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Sanjay Patidar, Utkarsh Singh and Sumit Kumar Sharma “Weed Seedling Detection
    Using Mask Regional Convolutional Neural Network” In *2020 International Conference
    on Electronics and Sustainable Communication Systems (ICESC)*, 2020, pp. 311–316
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Josh Patterson and Adam Gibson “Deep learning: A practitioner’s approach”
    ” O’Reilly Media, Inc.”, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Larry Pearlstein, Mun Kim and Warren Seto “Convolutional neural network
    application to plant detection, based on synthetic imagery” In *2016 IEEE Applied
    Imagery Pattern Recognition Workshop (AIPR)*, 2016, pp. 1–4 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Lukas Petrich et al. “Detection of Colchicum autumnale in drone images,
    using a machine-learning approach” Springer, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] PyTorch “AI for AG: Production machine learning for agriculture” In *Medium*
    PyTorch, 2020 URL: [https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Zheng Qin et al. “Thundernet: Towards real-time generic object detection
    on mobile devices” In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 6718–6727'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Thomas Lagkas
    and Ioannis Moscholios “A compilation of UAV applications for precision agriculture”
    In *Computer Networks* 172 Elsevier, 2020, pp. 107148'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Rekha Raja, Thuy T Nguyen, David C Slaughter and Steven A Fennimore “Real-time
    robotic weed knife control system for tomato and lettuce based on geometric appearance
    of plant labels” In *Biosystems Engineering* 194 Elsevier, 2020, pp. 152–164'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] W Ramirez, P Achanccaray, LF Mendoza and MAC Pacheco “Deep Convolutional
    Neural Networks for Weed Detection in Agricultural Crops Using Optical Aerial
    Images” In *2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)*,
    2020, pp. 133–137 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Pejman Rasti et al. “Supervised Image Classification by Scattering Transform
    with Application to Weed Detection in Culture Crops of High Density” In *Remote
    Sensing* 11.3, 2019, pp. 249 DOI: [10.3390/rs11030249](https://dx.doi.org/10.3390/rs11030249)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Joseph Redmon In *Darknet: Open Source Neural Networks in C* URL: [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Joseph Redmon and Ali Farhadi “Yolov3: An incremental improvement” In
    *arXiv preprint arXiv:1804.02767*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun “Faster r-cnn: Towards
    real-time object detection with region proposal networks” In *arXiv preprint arXiv:1506.01497*,
    2015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Yannik Rist, Iurii Shendryk, Foivos Diakogiannis and Shaun Levick “Weed
    Mapping Using Very High Resolution Satellite Imagery and Fully Convolutional Neural
    Network” In *IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing
    Symposium*, 2019, pp. 9784–9787 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Olaf Ronneberger, Philipp Fischer and Thomas Brox “U-net: Convolutional
    networks for biomedical image segmentation” In *International Conference on Medical
    image computing and computer-assisted intervention*, 2015, pp. 234–241 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Inkyu Sa et al. “weednet: Dense semantic weed classification using multispectral
    images and mav for smart farming” In *IEEE Robotics and Automation Letters* 3.1
    IEEE, 2017, pp. 588–595'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Inkyu Sa et al. “Weedmap: a large-scale semantic weed mapping framework
    using aerial multispectral imaging and deep neural network for precision farming”
    In *Remote Sensing* 10.9 Multidisciplinary Digital Publishing Institute, 2018,
    pp. 1423'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Sajad Sabzi, Yousef Abbaspour-Gilandeh and Juan Ignacio Arribas “An automatic
    visible-range video weed detection, segmentation and classification prototype
    in potato field” In *Heliyon* 6.5 Elsevier, 2020, pp. e03685'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Linda Sakyi “Linda Sakyi” In *Greenroot Limited*, 2019 URL: [https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/](https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] T Sarvini et al. “Performance Comparison of Weed Detection Algorithms”
    In *2019 International Conference on Communication and Signal Processing (ICCSP)*,
    2019, pp. 0843–0847 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Santhosh K Seelan, Soizik Laguette, Grant M Casady and George A Seielstad
    “Remote sensing applications for precision agriculture: A learning community approach”
    In *Remote sensing of environment* 88.1-2 Elsevier, 2003, pp. 157–169'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Saraswathi Shanmugam et al. “Automated Weed Detection Systems: A Review”
    In *KnE Engineering*, 2020, pp. 271–284'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Detection of Carolina
    geranium (Geranium carolinianum) growing in competition with strawberry using
    convolutional neural networks” In *Weed Science* 67.2 BioOne, 2019, pp. 239–245'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Goosegrass Detection
    in Strawberry and Tomato Using a Convolutional Neural Network” In *Scientific
    Reports* 10.1 Nature Publishing Group, 2020, pp. 1–8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Evan Shelhamer, Jonathan Long and Trevor Darrell “Fully convolutional
    networks for semantic segmentation” In *IEEE transactions on pattern analysis
    and machine intelligence* 39.4, 2017, pp. 640–651'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Connor Shorten and Taghi M Khoshgoftaar “A survey on image data augmentation
    for deep learning” In *Journal of Big Data* 6.1 Springer, 2019, pp. 60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Karen Simonyan and Andrew Zisserman “Very deep convolutional networks
    for large-scale image recognition” In *arXiv preprint arXiv:1409.1556*, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Arun Narenthiran Veeranampalayam Sivakumar et al. “Comparison of Object
    Detection and Patch-Based Classification Deep Learning Models on Mid-to Late-Season
    Weed Detection in UAV Imagery” In *Remote Sensing* 12.13 Multidisciplinary Digital
    Publishing Institute, 2020, pp. 2136'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Soren Skovsen et al. “The GrassClover Image Dataset for Semantic and
    Hierarchical Species Understanding in Agriculture” In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition Workshops*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Russell Stewart, Mykhaylo Andriluka and Andrew Y Ng “End-to-end people
    detection in crowded scenes” In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2016, pp. 2325–2333'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Wen-Hao Su “Advanced Machine Learning in Point Spectroscopy, RGB-and
    Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review”
    In *Smart Cities* 3.3 Multidisciplinary Digital Publishing Institute, 2020, pp.
    767–792'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Kaspars Sudars et al. “Dataset of annotated food crops and weed images
    for robotic computer vision control” In *Data in Brief* Elsevier, 2020, pp. 105833'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Hyun K Suh, Joris Ijsselmuiden, Jan Willem Hofstee and Eldert J Henten
    “Transfer learning for the classification of sugar beet and volunteer potato under
    field conditions” In *Biosystems engineering* 174 Elsevier, 2018, pp. 50–65'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Christian Szegedy et al. “Going deeper with convolutions” In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1–9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Aboozar Taherkhani, Georgina Cosma and T Martin McGinnity “AdaBoost-CNN:
    An adaptive boosting algorithm for convolutional neural networks to classify multi-class
    imbalanced datasets using transfer learning” In *Neurocomputing* 404 Elsevier,
    2020, pp. 351–366'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Ryo Takahashi, Takashi Matsubara and Kuniaki Uehara “Ricap: Random image
    cropping and patching data augmentation for deep cnns” In *Asian Conference on
    Machine Learning*, 2018, pp. 786–798'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] JingLei Tang et al. “Weed identification based on K-means feature learning
    combined with convolutional neural network” In *Computers and electronics in agriculture*
    135 Elsevier, 2017, pp. 63–70'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Andrew Tao, Jon Barker and Sriya Sarathy “Detectnet: Deep neural network
    for object detection in digits” In *Parallel Forall* 4, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Nima Teimouri et al. “Weed growth stage estimator using deep convolutional
    neural networks” In *Sensors* 18.5 Multidisciplinary Digital Publishing Institute,
    2018, pp. 1580'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] L Tian, DC Slaughter and RF Norris “Machine vision identification of
    tomato seedlings for automated weed control” In *Transactions of ASAE* 40.6 Citeseer,
    2000, pp. 1761–1768'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Vo Hoang Trong, Yu Gwang-hyun, Dang Thanh Vu and Kim Jin-young “Late
    fusion of multimodal deep neural networks for weeds classification” In *Computers
    and Electronics in Agriculture* 175 Elsevier, 2020, pp. 105506'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] S Umamaheswari, R Arjun and D Meganathan “Weed Detection in Farm Crops
    using Parallel Image Processing” In *2018 Conference on Information and Communication
    Technology (CICT)*, 2018, pp. 1–4 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S Umamaheswari and Ashvini V Jain “Encoder–Decoder Architecture for Crop-Weed
    Classification Using Pixel-Wise Labelling” In *2020 International Conference on
    Artificial Intelligence and Signal Processing (AISP)*, 2020, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J Valente, M Doldersum, C Roers and L Kooistra “DETECTING RUMEX OBTUSIFOLIUS
    WEED PLANTS IN GRASSLANDS FROM UAV RGB IMAGERY USING DEEP LEARNING.” In *ISPRS
    Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences* 4, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Viraf “Create A Synthetic Image Dataset - The ”What”, The ”Why” and The
    ”How”” In *Medium* Towards Data Science, 2020 URL: [https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718](https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Aichen Wang, Yifei Xu, Xinhua Wei and Bingbo Cui “Semantic Segmentation
    of Crop and Weed using an Encoder-Decoder Network and Image Enhancement Method
    under Uncontrolled Outdoor Illumination” In *IEEE Access* 8 IEEE, 2020, pp. 81724–81734'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Aichen Wang, Wen Zhang and Xinhua Wei “A review on weed detection using
    ground-based machine vision and image processing techniques” In *Computers and
    electronics in agriculture* 158 Elsevier, 2019, pp. 226–240'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Alexander Wendel and James Underwood “Self-supervised weed detection
    in vegetable crops using ground based hyperspectral imaging” In *2016 IEEE international
    conference on robotics and automation (ICRA)*, 2016, pp. 5128–5135 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] DM Woebbecke, GE Meyer, K Von Bargen and DA Mortensen “Shape features
    for identifying young weeds using image analysis” In *Transactions of the ASAE*
    38.1 American Society of AgriculturalBiological Engineers, 1995, pp. 271–281'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Xue Yan, Xiangwu Deng and Jing Jin “Classification of weed species in
    the paddy field with DCNN-Learned features” In *2020 IEEE 5th Information Technology
    and Mechatronics Engineering Conference (ITOEC)*, 2020, pp. 336–340 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Zhang Yi, Shen Yongliang and Zhang Jun “An improved tiny-yolov3 pedestrian
    detection algorithm” In *Optik* 183 Elsevier, 2019, pp. 17–23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Jialin Yu et al. “Weed Detection in Perennial Ryegrass With Deep Learning
    Convolutional Neural Network” In *Frontiers in plant science* 10 Frontiers Media
    SA, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Jialin Yu, Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Deep
    learning for image-based weed detection in turfgrass” In *European journal of
    agronomy* 104 Elsevier, 2019, pp. 78–84'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Rufei Zhang et al. “Weed location and recognition based on UAV imaging
    and deep learning” In *International Journal of Precision Agricultural Aviation*
    3.1, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Wenhao Zhang et al. “Broad-leaf weed detection in pasture” In *2018 IEEE
    3rd International Conference on Image, Vision and Computing (ICIVC)*, 2018, pp.
    101–105 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Xiao-Yu Zhang, Haichao Shi, Xiaobin Zhu and Peng Li “Active semi-supervised
    learning based on self-expressive correlation with generative adversarial networks”
    In *Neurocomputing* 345 Elsevier, 2019, pp. 103–113'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Yang Zheng et al. “Maize and weed classification using color indices
    with support vector data description in outdoor fields” In *Computers and Electronics
    in Agriculture* 141 Elsevier, 2017, pp. 215–222'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Zhi-Hua Zhou “A brief introduction to weakly supervised learning” In
    *National Science Review* 5.1 Oxford University Press, 2018, pp. 44–53'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
