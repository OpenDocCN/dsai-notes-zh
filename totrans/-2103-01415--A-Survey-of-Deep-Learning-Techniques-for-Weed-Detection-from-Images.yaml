- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:56:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.01415] A Survey of Deep Learning Techniques for Weed Detection from Images'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.01415] 深度学习技术在图像中杂草检测的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.01415](https://ar5iv.labs.arxiv.org/html/2103.01415)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.01415](https://ar5iv.labs.arxiv.org/html/2103.01415)
- en: \usesmartdiagramlibrary
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \usesmartdiagramlibrary
- en: additions
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: additions
- en: A Survey of Deep Learning Techniques for Weed Detection from Images
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术在图像中杂草检测的调查
- en: 'A S M Mahmudul Hasan¹¹1Corresponding author email: 33916214@student.murdoch.edu.au
    Information Technology, Murdoch University, Murdoch, WA 6150, Australia Centre
    for Crop and Food Innovation, Food Futures Institute, Murdoch University, Murdoch,
    WA 6150, Australia Ferdous Sohel Information Technology, Murdoch University, Murdoch,
    WA 6150, Australia Centre for Crop and Food Innovation, Food Futures Institute,
    Murdoch University, Murdoch, WA 6150, Australia Dean Diepeveen Centre for Crop
    and Food Innovation, Food Futures Institute, Murdoch University, Murdoch, WA 6150,
    Australia Department of Primary Industries and Regional Development, Western Australia,
    South Perth, WA, 6151, Australia Centre for Sustainable Farming Systems, Murdoch
    University, Murdoch, WA 6150, Australia Hamid Laga Information Technology, Murdoch
    University, Murdoch, WA 6150, Australia Centre of Biosecurity and One Health,
    Harry Butler Institute, Murdoch University, Murdoch University, Murdoch, WA 6150,
    Australia Michael G.K. Jones Centre for Crop and Food Innovation, Food Futures
    Institute, Murdoch University, Murdoch, WA 6150, Australia'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: A S M Mahmudul Hasan¹¹1通讯作者电子邮件：33916214@student.murdoch.edu.au 信息技术，默多克大学，澳大利亚，WA
    6150，默多克，中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA 6150，默多克，Ferdous Sohel 信息技术，默多克大学，澳大利亚，WA
    6150，默多克，中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA 6150，默多克，Dean Diepeveen 中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA
    6150，默多克，初级产业与区域发展部，西澳大利亚州，南珀斯，WA 6151，澳大利亚，中心可持续农业系统，默多克大学，澳大利亚，WA 6150，默多克，Hamid
    Laga 信息技术，默多克大学，澳大利亚，WA 6150，默多克，生物安全与一体化健康中心，哈里·巴特勒研究所，默多克大学，澳大利亚，WA 6150，默多克，Michael
    G.K. Jones 中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA 6150，默多克
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advances in Deep Learning (DL) techniques have enabled rapid detection,
    localisation, and recognition of objects from images or videos. DL techniques
    are now being used in many applications related to agriculture and farming. Automatic
    detection and classification of weeds can play an important role in weed management
    and so contribute to higher yields. Weed detection in crops from imagery is inherently
    a challenging problem because both weeds and crops have similar colours (‘green-on-green’),
    and their shapes and texture can be very similar at the growth phase. Also, a
    crop in one setting can be considered a weed in another. In addition to their
    detection, the recognition of specific weed species is essential so that targeted
    controlling mechanisms (e.g. appropriate herbicides and correct doses) can be
    applied. In this paper, we review existing deep learning-based weed detection
    and classification techniques. We cover the detailed literature on four main procedures,
    i.e., data acquisition, dataset preparation, DL techniques employed for detection,
    location and classification of weeds in crops, and evaluation metrics approaches.
    We found that most studies applied supervised learning techniques, they achieved
    high classification accuracy by fine-tuning pre-trained models on any plant dataset,
    and past experiments have already achieved high accuracy when a large amount of
    labelled data is available.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）技术的快速进步使得从图像或视频中迅速检测、定位和识别对象成为可能。DL 技术现在在许多与农业和种植相关的应用中得到了应用。自动检测和分类杂草可以在杂草管理中发挥重要作用，从而提高产量。由于杂草和作物的颜色（‘绿对绿’）相似，它们的形状和纹理在生长阶段可能非常相似，因此从图像中检测作物中的杂草本质上是一个具有挑战性的问题。此外，一个环境中的作物在另一个环境中可能被视为杂草。除了检测外，识别特定的杂草种类也至关重要，以便能够施加针对性的控制机制（例如，适当的除草剂和正确的剂量）。在本文中，我们回顾了现有的基于深度学习的杂草检测和分类技术。我们涵盖了关于四个主要过程的详细文献，即数据采集、数据集准备、用于检测、定位和分类作物中的杂草的
    DL 技术，以及评估指标方法。我们发现大多数研究应用了监督学习技术，通过对任何植物数据集微调预训练模型实现了高分类准确率，并且过去的实验已经在大量标记数据可用时达到了高准确率。
- en: 'Keywords: Deep learning, Weed detection, Weed classification, Machine Learning,
    Digital agriculture.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：深度学习，杂草检测，杂草分类，机器学习，数字农业。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The world population has been increasing rapidly, and it is expected to reach
    nine billion by 2050\. Agricultural production needs to increase by about 70%
    to meet the anticipated demands [[125](#bib.bibx125)]. However, the agricultural
    sector will face many challenges during this time, including a reduction of cultivatable
    land and the need for more intensive production. Other issues, such as climate
    change and water scarcity, will also affect productivity. Precision agriculture
    or digital agriculture can provide strategies to mitigate these issues [[85](#bib.bibx85),
    [139](#bib.bibx139), [125](#bib.bibx125)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 世界人口正在迅速增长，预计到2050年将达到90亿。为了满足预期需求，农业生产需要增加约70%[[125](#bib.bibx125)]。然而，农业部门在此期间将面临许多挑战，包括可耕作土地的减少和对更高生产强度的需求。其他问题，如气候变化和水资源短缺，也将影响生产力。精准农业或数字农业可以提供缓解这些问题的策略[[85](#bib.bibx85),
    [139](#bib.bibx139), [125](#bib.bibx125)]。
- en: 'Weeds are plants that can spread quickly and undesirably, and can impact on
    crop yields and quality [[118](#bib.bibx118)]. Weeds compete with crops for nutrition,
    water, sunlight, and growing space [[67](#bib.bibx67)]. Therefore, farmers have
    to deploy resources to reduce weeds. The management strategies used to reduce
    the impact of weeds depend on many factors. These strategies can be categorised
    into five main types [[137](#bib.bibx137)]: ‘preventative’ (prevent weeds from
    becoming established), ‘cultural’ (by maintaining field hygiene – low weed seed
    bank), ‘mechanical’ (e.g., mowing, mulching and tilling), ‘biological’ (using
    natural enemies of weeds such as insects, grazing animals or disease), and ‘chemical’
    (application of herbicides). These approaches all have drawbacks. In general,
    there is a financial burden and they require time and extra work. In addition,
    control treatments may impact the health of people, plants, soil, animals, or
    the environment [[112](#bib.bibx112), [137](#bib.bibx137), [59](#bib.bibx59)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 杂草是能够迅速且不受欢迎地传播的植物，它们可能影响作物的产量和质量[[118](#bib.bibx118)]。杂草与作物争夺营养、水分、阳光和生长空间[[67](#bib.bibx67)]。因此，农民必须投入资源以减少杂草。用于减少杂草影响的管理策略取决于许多因素。这些策略可以分为五种主要类型[[137](#bib.bibx137)]：‘预防性’（防止杂草建立），‘文化性’（通过保持田间卫生——低杂草种子库），‘机械性’（例如，割草、覆盖和耕作），‘生物性’（利用自然敌人如昆虫、放牧动物或疾病），以及‘化学性’（施用除草剂）。这些方法都有其缺点。一般来说，它们带来财务负担，需要时间和额外的工作。此外，控制措施可能影响人类、植物、土壤、动物或环境的健康[[112](#bib.bibx112),
    [137](#bib.bibx137), [59](#bib.bibx59)]。
- en: As the costs of labour has increased, and people have become more concerned
    about health and environmental issues, automation of weed control has become desirable
    [[99](#bib.bibx99)]. Automated weed control systems can be beneficial both economically
    and environmentally. Such systems can reduce labour costs by using a machine to
    remove weeds and, selective spraying techniques can minimise the use of the herbicides
    [[87](#bib.bibx87)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着劳动力成本的增加，人们对健康和环境问题的关注也在上升，自动化杂草控制变得越来越受到欢迎[[99](#bib.bibx99)]。自动化杂草控制系统在经济和环境方面都具有好处。这些系统可以通过使用机器去除杂草来减少劳动力成本，并且选择性喷洒技术可以最小化除草剂的使用[[87](#bib.bibx87)]。
- en: To develop an automatic weed management system, an essential first step is to
    be able to detect and recognise weeds correctly [[99](#bib.bibx99)]. Detection
    of weeds in crops is challenging as weeds and crop plants often have similar colours,
    textures, and shapes. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") shows crop plants
    with weeds growing amongst them. Common challenges in detection and classification
    of crops and weeds are occlusion (Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), similarity
    in colour and texture (Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), plants
    shadowed in natural light (Figure [1(c)](#S1.F1.sf3 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), colour
    and texture variations due to lighting conditions and illumination (Figure [1(d)](#S1.F1.sf4
    "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images")) and different species of weeds which appear similar (Figure
    [1(e)](#S1.F1.sf5 "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images")). Same crop plants or weeds may show dissimilarities
    during growth phases (Figure [1(f)](#S1.F1.sf6 "In Figure 1 ‣ 1 Introduction ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")). Motion
    blur and noise in the image also increase the difficulty in classifying plants
    (Figure [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")). In addition, depending on the geographical
    location (Figure [1(h)](#S1.F1.sf8 "In Figure 1 ‣ 1 Introduction ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images")) and the variety of
    the crop, weather and soil conditions, the species of weeds can vary [[70](#bib.bibx70)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个自动化杂草管理系统，首要步骤是能够正确检测和识别杂草[[99](#bib.bibx99)]。由于杂草和作物植物通常具有相似的颜色、纹理和形状，检测作物中的杂草具有挑战性。图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述") 显示了杂草与作物植物共生的场景。检测和分类作物与杂草的常见挑战包括遮挡（图
    [1(a)](#S1.F1.sf1 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、颜色和纹理的相似性（图 [1(b)](#S1.F1.sf2
    "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、自然光下植物的阴影（图 [1(c)](#S1.F1.sf3 "在图 1 ‣ 1
    引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、由于光照条件和光源造成的颜色和纹理变化（图 [1(d)](#S1.F1.sf4 "在图 1 ‣ 1
    引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）以及外观相似的不同杂草种类（图 [1(e)](#S1.F1.sf5 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）。相同的作物植物或杂草在不同生长阶段可能会显示出差异（图
    [1(f)](#S1.F1.sf6 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）。图像中的运动模糊和噪声也增加了植物分类的难度（图
    [1(g)](#S1.F1.sf7 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）。此外，根据地理位置（图 [1(h)](#S1.F1.sf8
    "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、作物品种、天气和土壤条件，杂草种类可能会有所不同[[70](#bib.bibx70)]。
- en: '![Refer to caption](img/085f5ae511ca127e91de98360bd959bb.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/085f5ae511ca127e91de98360bd959bb.png)'
- en: (a) Occlusion of crop and weed [[54](#bib.bibx54)]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 作物与杂草的遮挡 [[54](#bib.bibx54)]
- en: '![Refer to caption](img/194c24f66d6e6842dc66b9e2bead8edc.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/194c24f66d6e6842dc66b9e2bead8edc.png)'
- en: (b) Colour and texture similarities between crop and weed plants [[12](#bib.bibx12)]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 作物和杂草植物之间的颜色和纹理相似性 [[12](#bib.bibx12)]
- en: '![Refer to caption](img/80d2e04c1bf666c4f24d5fcd99bc664a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/80d2e04c1bf666c4f24d5fcd99bc664a.png)'
- en: (c) Shadow effects in natural weed image [[123](#bib.bibx123)]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 自然杂草图像中的阴影效果 [[123](#bib.bibx123)]
- en: '![Refer to caption](img/38d77e08f38de3efafe3b3497077cc88.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38d77e08f38de3efafe3b3497077cc88.png)'
- en: (d) Effects of illumination conditions [[33](#bib.bibx33)]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 光照条件的影响 [[33](#bib.bibx33)]
- en: '![Refer to caption](img/407cfe3028f66e91448d813473ceb626.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/407cfe3028f66e91448d813473ceb626.png)'
- en: (e) Four different species of weeds that share similarities (inter-class similarity)
    [[113](#bib.bibx113)]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 四种具有相似性的不同杂草种类（类别间相似性） [[113](#bib.bibx113)]
- en: '![Refer to caption](img/0d7fead47e3714cfd6f0d890bdac7200.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d7fead47e3714cfd6f0d890bdac7200.png)'
- en: (f) Sugar beet crop at different growth stages (intra-class variations) [[49](#bib.bibx49)]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 不同生长阶段的甜菜作物（类别内部变化）[[49](#bib.bibx49)]
- en: '![Refer to caption](img/9ab4e4116cd982f20ef29eb9676a7a5f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ab4e4116cd982f20ef29eb9676a7a5f.png)'
- en: (g) Effects of motion blur and noise [[49](#bib.bibx49), [3](#bib.bibx3)]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 运动模糊和噪声的影响 [[49](#bib.bibx49), [3](#bib.bibx3)]
- en: '![Refer to caption](img/42779da4bd4433db68ea7ca56ec8c650.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/42779da4bd4433db68ea7ca56ec8c650.png)'
- en: '(h) Weeds can vary at different geographic/weather locations: weed in carrot
    crop collected from Germany(left) [[54](#bib.bibx54)] and Macedonia (Right) [[88](#bib.bibx88)]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 杂草在不同的地理/天气位置可能有所不同：来自德国（左） [[54](#bib.bibx54)] 和马其顿（右） [[88](#bib.bibx88)]
    的胡萝卜作物中的杂草
- en: 'Figure 1: Weeds in different crops (green boxes indicate crops and red boxes
    indicate weeds).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同作物中的杂草（绿色框表示作物，红色框表示杂草）。
- en: 'A typical weed detection system follows four key steps: image acquisition,
    pre-processing of images, extraction of features and detection and classification
    of weeds [[140](#bib.bibx140)]. Different emerging technologies have been used
    to accomplish these steps. The most crucial part of these steps is weed detection
    and classification. In recent years, with advances in computer technologies, particularly
    in graphical processing units (GPU), embedded processors coupled with the use
    of Machine Learning (ML) techniques have become more widely used for automatic
    detection of weed species [[91](#bib.bibx91), [50](#bib.bibx50), [171](#bib.bibx171)].'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的杂草检测系统包括四个关键步骤：图像采集、图像预处理、特征提取以及杂草的检测和分类 [[140](#bib.bibx140)]。不同的新兴技术已被用于完成这些步骤。这些步骤中最关键的是杂草的检测和分类。近年来，随着计算机技术的进步，特别是在图形处理单元（GPU）方面，结合使用机器学习（ML）技术的嵌入式处理器已被更广泛地用于自动检测杂草种类
    [[91](#bib.bibx91), [50](#bib.bibx50), [171](#bib.bibx171)]。
- en: Deep learning (DL) is an important branch of ML. For image classification, object
    detection, and recognition, DL algorithms have many advantages over traditional
    ML approaches (in this paper, the term machine learning, we mean traditional machine
    learning approaches). Extracting and selecting discriminating features with ML
    methods is difficult because crops and weeds can be similar. This problem can
    be addressed efficiently by using DL approaches based on their strong feature
    learning capabilities. Recently, many research articles have been published on
    DL-based weed recognition, yet few review articles have been published on this
    topic. Su [[149](#bib.bibx149)] recently published a review paper in which the
    main focus was on the use of point spectroscopy, RGB, and hyperspectral imaging
    to classify weeds in crops automatically. However, most of the articles covered
    in this review have applied traditional machine learning approaches, with few
    citations of recent papers. [[99](#bib.bibx99)] analysed a number of publications
    on weed detection, but from the perspective of selective spraying.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习（ML）的一个重要分支。在图像分类、物体检测和识别方面，DL算法相较于传统机器学习方法具有许多优势（在本文中，术语“机器学习”指的是传统机器学习方法）。使用机器学习方法提取和选择辨别特征是困难的，因为作物和杂草可能相似。这个问题可以通过利用DL方法的强大特征学习能力有效解决。最近，已经有许多研究文章发表了基于DL的杂草识别，但关于这一主题的综述文章却很少。Su
    [[149](#bib.bibx149)] 最近发表了一篇综述文章，主要关注使用点谱学、RGB和高光谱成像来自动分类作物中的杂草。然而，大多数涵盖的文章应用了传统机器学习方法，且对近期论文的引用较少。
    [[99](#bib.bibx99)] 从选择性喷洒的角度分析了多篇关于杂草检测的出版物。
- en: We provide this comprehensive literature survey to highlight the great potential
    now presented by different DL techniques for detecting, localising, and classifying
    weeds in crops. We present a taxonomy of the DL techniques for weed detection
    and recognition, and classify major publications based on that taxonomy. We also
    cover data collection, data preparation, and data representation approaches. We
    provide an overview of different evaluation metrics used to benchmark the performance
    of the techniques surveyed in this article.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供这份综合文献调查，以突显不同深度学习（DL）技术在检测、定位和分类作物中的杂草方面所展现的巨大潜力。我们展示了用于杂草检测和识别的深度学习技术的分类法，并根据该分类法对主要出版物进行了分类。我们还涵盖了数据收集、数据准备和数据表示方法。我们概述了用于评估本文中调查技术性能的不同评估指标。
- en: The rest of the paper is organised as follows. Existing review papers in this
    area are discussed briefly in Section [2](#S2 "2 Related Surveys ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images"). Advantages of DL-based
    weed detection approaches over traditional ML methods are discussed in Section
    [3](#S3 "3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images"). In Section [4](#S4 "4 Paper
    Selection Criteria in this Survey ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images"), we describe how the papers for review were selected.
    A taxonomy and an overview of DL-based weed detection techniques are provided
    in Section [5](#S5 "5 An Overview and Taxonomy of Deep Learning-based Weed Detection
    Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection from Images").
    We describe four major steps of DL-based approaches, i.e. data acquisition (Section
    [6](#S6 "6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images")), dataset preparation (Section [7](#S7 "7 Dataset Preparation ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")), detection
    and classification methods (Section [10](#S10 "10 Deep Learning Architecture ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")) and evaluation
    metrics (Section [11](#S11 "11 Performance Evaluation Metrics ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")). In Section [8](#S8 "8 Detection
    Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")
    we have highlighted the approaches to detection of weeds in crop plants adopted
    in the related work. The learning methods applied the relevant studies are explained
    in Section [9](#S9 "9 Learning Methods ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images"). We summarise the current state in this field
    and provide future directions in Section [12](#S12 "12 Discussion ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images") with conclusions are
    provided in Section [13](#S13 "13 Conclusion ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images").
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。现有的综述论文在本节[2](#S2 "2 相关调查 ‣ 基于深度学习的杂草检测技术综述")中进行了简要讨论。深度学习方法在杂草检测中的优势与传统机器学习方法的对比在本节[3](#S3
    "3 传统机器学习与深度学习方法的杂草检测 ‣ 基于深度学习的杂草检测技术综述")中进行了讨论。在本节[4](#S4 "4 本综述中的论文选择标准 ‣ 基于深度学习的杂草检测技术综述")中，我们描述了如何选择用于综述的论文。关于深度学习的杂草检测技术的分类和概述在本节[5](#S5
    "5 基于深度学习的杂草检测方法概述与分类 ‣ 基于深度学习的杂草检测技术综述")中提供。我们描述了深度学习方法的四个主要步骤，即数据获取（第[6](#S6
    "6 数据获取 ‣ 基于深度学习的杂草检测技术综述")节）、数据集准备（第[7](#S7 "7 数据集准备 ‣ 基于深度学习的杂草检测技术综述")节）、检测和分类方法（第[10](#S10
    "10 深度学习架构 ‣ 基于深度学习的杂草检测技术综述")节）以及评估指标（第[11](#S11 "11 性能评估指标 ‣ 基于深度学习的杂草检测技术综述")节）。在第[8](#S8
    "8 检测方法 ‣ 基于深度学习的杂草检测技术综述")节中，我们突出介绍了相关工作中采用的作物植物杂草检测方法。相关研究中应用的学习方法在第[9](#S9
    "9 学习方法 ‣ 基于深度学习的杂草检测技术综述")节中进行了说明。我们在第[12](#S12 "12 讨论 ‣ 基于深度学习的杂草检测技术综述")节中总结了该领域的当前状态并提供了未来方向，结论在第[13](#S13
    "13 结论 ‣ 基于深度学习的杂草检测技术综述")节中提供。
- en: 2 Related Surveys
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关调查
- en: ML and DL techniques have been used for weed detection, recognition and thus
    for weed management. In 2018, [[73](#bib.bibx73)] published a survey of 40 research
    papers that applied DL-techniques to address various agricultural problems, including
    weed detection. The study reported that DL-techniques outperformed more than traditional
    image processing methods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）和深度学习（DL）技术已被用于杂草检测、识别以及杂草管理。2018年，[[73](#bib.bibx73)] 发表了一项综述，回顾了40篇应用深度学习技术解决各种农业问题的研究论文，包括杂草检测。研究报告显示，深度学习技术在性能上优于传统的图像处理方法。
- en: In 2016, [[106](#bib.bibx106)] discussed ten components that are essential and
    possible obstructions to develop a fully autonomous mechanical weed management
    system. With the advance in DL, it seems that the problems raised can now be addressed.
    [[6](#bib.bibx6)] articulated that DL-based plant classification modules can be
    deployed not only in weed management systems but also for fertilisation, irrigation,
    and phenotyping. Their study explained how “Deepfield Robotics” systems could
    reduce labour required for weed control in agriculture and horticulture.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，[[106](#bib.bibx106)]讨论了十项发展全自动机械除草系统所必需的组成部分和可能的障碍。随着深度学习的进展，这些问题似乎可以得到解决。[[6](#bib.bibx6)]指出，基于深度学习的植物分类模块不仅可用于除草管理系统，还可用于肥料、灌溉和表型分析。他们的研究解释了“深度机器人技术”系统如何减少农业和园艺中除草控制所需的劳动力。
- en: '[[165](#bib.bibx165)] highlighted that the most challenging part of a weed
    detection techniques is to distinguish between weed and crop species. They focused
    on different machine vision and image processing techniques used for ground-based
    weed detection. [[21](#bib.bibx21)] made a similar observation. They reviewed
    remote sensing for weed mapping and ground-based detection techniques. They also
    reported the limitations of using either spectral or spatial features to identify
    weeds in crops. According to their study, it is preferable to use both features.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[[165](#bib.bibx165)]强调杂草检测技术中最具挑战性的部分是区分杂草和作物品种。他们关注地面杂草检测所使用的不同机器视觉和图像处理技术。[[21](#bib.bibx21)]发表了类似的观察。他们对杂草制图和地面检测技术进行了回顾。他们还报告了使用光谱特征或空间特征来识别作物中的杂草的局限性。根据他们的研究，最好同时使用这两种特征。'
- en: '[[45](#bib.bibx45)] reviewed technologies that can be used to monitor weeds
    in crops. They explored different remotely sensed and ground-based weed monitoring
    systems in agricultural fields. They reported that weed monitoring is essential
    for weed management. They foresaw that the data collected using different sensors
    could be stored in cloud systems for timely use in relevant contexts. In another
    study, [[109](#bib.bibx109)] evaluated a small number of DL approaches used for
    detecting weeds in crops. They identified research gaps, e.g., the lack of large
    crop-weed datasets, acceptable classification accuracy and lack of generalised
    models for detecting different crop plants and weed species. However, the article
    only covered a handful of publications and as such the paper was not thorough
    and did not adequately cover the breadth and depth of the literature.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[[45](#bib.bibx45)]回顾了可用于监测作物中杂草的技术。他们探索了农田中的不同遥感和地面监测系统。他们报告了杂草监测对于杂草管理的重要性。他们预测，使用不同传感器收集的数据可以存储在云系统中，以适时在相关情境中使用。在另一项研究中，[[109](#bib.bibx109)]评估了用于检测作物中杂草的少数深度学习方法。他们确定了研究的空白，例如缺乏大规模的作物杂草数据集，可接受的分类准确性以及缺乏适用于检测不同作物植物和杂草品种的广义模型。然而，该文章只涵盖了少数几篇论文，因此并不全面，无法充分覆盖文献的广度和深度。'
- en: 3 Traditional ML- vs DL-based Weed Detection Methods
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统机器学习与深度学习在杂草检测方法上的区别
- en: 'A typical ML-based weed classification technique follows five key steps: image
    acquisition, pre-processing such as image enhancement, feature extraction or with
    feature selection, applying an ML-based classifier and evaluation of the performance
    [[99](#bib.bibx99), [23](#bib.bibx23), [17](#bib.bibx17), [96](#bib.bibx96)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的基于机器学习的杂草分类技术包括五个关键步骤：图像获取、预处理（如图像增强）、特征提取或特征选择、应用基于机器学习的分类器和性能评估[[99](#bib.bibx99)，[23](#bib.bibx23)，[17](#bib.bibx17)，[96](#bib.bibx96)]。
- en: Different image processing methods have been applied for crop and weed classification
    [[167](#bib.bibx167), [57](#bib.bibx57), [158](#bib.bibx158)]. By extracting shape
    features, many researchers identify weeds and crops using discriminate analysis
    [[24](#bib.bibx24), [107](#bib.bibx107)]. In some other research, different colour
    [[175](#bib.bibx175), [69](#bib.bibx69), [52](#bib.bibx52), [75](#bib.bibx75)]
    and texture [[13](#bib.bibx13)] features were used.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的图像处理方法已被应用于作物和杂草的分类[[167](#bib.bibx167)，[57](#bib.bibx57)，[158](#bib.bibx158)]。通过提取形状特征，许多研究人员使用判别分析来识别杂草和作物[[24](#bib.bibx24),
    [107](#bib.bibx107)]。在其他一些研究中，使用不同的颜色[[175](#bib.bibx175)，[69](#bib.bibx69)，[52](#bib.bibx52)，[75](#bib.bibx75)]和纹理[[13](#bib.bibx13)]特征。
- en: The main challenge in weed detection and classification is that both weeds and
    crops can have very similar colours or textures. Machine learning approaches learn
    the features from the training data that are available [[12](#bib.bibx12)]. Understandably,
    for traditional ML-approaches, the combination of multiple modalities of data
    e.g. the shape, texture and colour or a combination of multiple sensor data is
    expected to generate superior results to a single modality of data. [[80](#bib.bibx80)]
    argued that colour or texture features of an image alone are not adequate to classify
    wheat from weed species Bidens pilosa. They used Near-Infrared (NIR) image cues
    with those features. [[136](#bib.bibx136)] extracted eight texture features based
    on the grey level co-occurrence matrix (GLCM), two spectral descriptors of texture,
    thirteen different colour features, five moment-invariant features, and eight
    shape features. They compared the performance of several algorithms, such as the
    ant colony algorithm, simulated annealing method, and genetic algorithm for selecting
    more discriminative features. The performance of the Cultural Algorithm, Linear
    Discriminant Analysis (LDA), Support Vector Machine (SVM), and Random Forest classifiers
    were also evaluated to distinguish between crops and weeds.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在杂草检测和分类中，主要挑战在于杂草和作物可能具有非常相似的颜色或纹理。机器学习方法从可用的训练数据中学习特征[[12](#bib.bibx12)]。可以理解，对于传统的机器学习方法，数据的多模态组合（例如形状、纹理和颜色，或多个传感器数据的组合）被期望能产生比单一数据模态更优的结果。[[80](#bib.bibx80)]
    认为，仅依靠图像的颜色或纹理特征不足以将小麦与杂草物种*悲伤蓟*区分开来。他们使用了近红外（NIR）图像线索与这些特征结合。[[136](#bib.bibx136)]
    基于灰度共生矩阵（GLCM）提取了八种纹理特征、两种纹理光谱描述符、十三种不同的颜色特征、五种不变矩特征和八种形状特征。他们比较了多种算法的性能，如蚁群算法、模拟退火法和遗传算法，以选择更具判别力的特征。还评估了文化算法、线性判别分析（LDA）、支持向量机（SVM）和随机森林分类器的性能，以区分作物和杂草。
- en: '[[74](#bib.bibx74)] applied SVM for detecting weeds in corn from hyperspectral
    images. In other research, [[166](#bib.bibx166)] used SVM and LDA for classifying
    plants. They proposed a self-supervised approach for discrimination. Before training
    the models, they applied vegetation separation techniques to remove background
    and different spectral pre-processing to extract features using Principal Component
    Analysis (PCA). [[68](#bib.bibx68)] extracted different shape features and the
    feature vectors were evaluated using a single-layer perceptron classifier to distinguish
    narrow and broad-leafed weeds.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[[74](#bib.bibx74)] 应用SVM从高光谱图像中检测玉米中的杂草。在其他研究中，[[166](#bib.bibx166)] 使用SVM和LDA进行植物分类。他们提出了一种自监督的方法用于区分。在训练模型之前，他们应用了植被分离技术去除背景，并使用主成分分析（PCA）进行不同的光谱预处理以提取特征。[[68](#bib.bibx68)]
    提取了不同的形状特征，使用单层感知器分类器评估特征向量，以区分狭叶和宽叶杂草。'
- en: <svg   height="465.68" overflow="visible" version="1.1" width="619.13"><g transform="translate(0,465.68)
    matrix(1 0 0 -1 0 0) translate(55.72,0) translate(0,373.3)"><g stroke="#000000"
    fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -34.59
    1.6)" fill="#000000" stroke="#000000"><foreignobject width="69.19" height="22.56"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Collect Data <g fill="#CCFFCC"><path
    d="M 177.84 0 C 177.84 13.78 153.01 24.95 122.39 24.95 C 91.77 24.95 66.95 13.78
    66.95 0 C 66.95 -13.78 91.77 -24.95 122.39 -24.95 C 153.01 -24.95 177.84 -13.78
    177.84 0 Z M 122.39 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 87.8 3.34)"
    fill="#000000" stroke="#000000"><foreignobject width="69.19" height="26.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Use Public Data</foreignobject></g> <g fill="#FFCCCC"><path
    d="M 123.61 -47.36 L -12.91 -47.36 C -15.97 -47.36 -18.45 -49.84 -18.45 -52.89
    L -18.45 -81.19 C -18.45 -84.25 -15.97 -86.73 -12.91 -86.73 L 123.61 -86.73 C
    126.67 -86.73 129.15 -84.25 129.15 -81.19 L 129.15 -52.89 C 129.15 -49.84 126.67
    -47.36 123.61 -47.36 Z M -18.45 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 -13.84 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data Acquisition</foreignobject></g>
    <path d="M 17.8 -21.56 L 36.76 -44.52" style="fill:none"><g transform="matrix(0.63672
    -0.77109 0.77109 0.63672 36.76 -44.52)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 99.41 -22.98 L 77.65 -44.73" style="fill:none"><g
    transform="matrix(-0.70717 -0.70705 0.70705 -0.70717 77.65 -44.73)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path
    d="M 301.35 -47.36 L 164.83 -47.36 C 161.77 -47.36 159.29 -49.84 159.29 -52.89
    L 159.29 -81.19 C 159.29 -84.25 161.77 -86.73 164.83 -86.73 L 301.35 -86.73 C
    304.41 -86.73 306.89 -84.25 306.89 -81.19 L 306.89 -52.89 C 306.89 -49.84 304.41
    -47.36 301.35 -47.36 Z M 159.29 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 163.9 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dataset
    Preparation</foreignobject></g> <g stroke-width="0.6pt" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0
    1.0 151.17 -67.04)"><path d="M 0 4.05 L 7.01 0 L 0 -4.05 Z" style="fill:none"></path></g><g
    stroke-width="3.0pt"><path d="M 129.42 -67.04 L 150.71 -67.04" style="fill:none"></path></g><g
    stroke-width="1.4pt" stroke="#FFFFFF"><path d="M 129.42 -67.04 L 150.71 -67.04"
    style="fill:none"></path></g><g stroke-width="2.0pt" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><path d="M 129.42 -67.04 L 152.79 -67.04" style="fill:none"></path></g><g
    fill="#CCFFCC"><path d="M 150.16 -134.09 C 150.16 -115.19 125.34 -99.87 94.72
    -99.87 C 64.1 -99.87 39.27 -115.19 39.27 -134.09 C 39.27 -152.99 64.1 -168.31
    94.72 -168.31 C 125.34 -168.31 150.16 -152.99 150.16 -134.09 Z M 94.72 -134.09"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 60.13 -124.19)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Colour
    Model Conversion</foreignobject></g> <g fill="#CCFFCC"><path d="M 164 -214.97
    C 164 -200.22 139.18 -188.27 108.56 -188.27 C 77.93 -188.27 53.11 -200.22 53.11
    -214.97 C 53.11 -229.72 77.93 -241.67 108.56 -241.67 C 139.18 -241.67 164 -229.72
    164 -214.97 Z M 108.56 -214.97"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    73.96 -210.39)" fill="#000000" stroke="#000000"><foreignobject width="69.19" height="28.54"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image Resizing</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 286.39 -214.97 C 286.39 -201.27 261.57 -190.17 230.95
    -190.17 C 200.33 -190.17 175.5 -201.27 175.5 -214.97 C 175.5 -228.66 200.33 -239.77
    230.95 -239.77 C 261.57 -239.77 286.39 -228.66 286.39 -214.97 Z M 230.95 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 196.35 -211.73)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="25.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image
    Augmentation</foreignobject></g> <g fill="#CCFFCC"><path d="M 426.9 -134.09 C
    426.9 -119.34 402.08 -107.39 371.46 -107.39 C 340.84 -107.39 316.02 -119.34 316.02
    -134.09 C 316.02 -148.83 340.84 -160.79 371.46 -160.79 C 402.08 -160.79 426.9
    -148.83 426.9 -134.09 Z M 371.46 -134.09"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 336.87 -129.51)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="28.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data Labelling</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 408.78 -214.97 C 408.78 -196.07 383.96 -180.75 353.34
    -180.75 C 322.72 -180.75 297.9 -196.07 297.9 -214.97 C 297.9 -233.87 322.72 -249.19
    353.34 -249.19 C 383.96 -249.19 408.78 -233.87 408.78 -214.97 Z M 353.34 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 318.75 -205.07)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Generate
    Synthetic Data</foreignobject></g> <path d="M 138.6 -112.83 L 188.92 -88.45" style="fill:none"><g
    transform="matrix(0.89996 0.43597 -0.43597 0.89996 188.92 -88.45)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path d="M 129.59
    -189.99 L 214.15 -89.55" style="fill:none"><g transform="matrix(0.64403 0.765
    -0.765 0.64403 214.15 -89.55)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 231.31 -189.9 L 232.75 -90.33" style="fill:none"><g transform="matrix(0.01447
    0.9999 -0.9999 0.01447 232.75 -90.33)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 332.07 -115.01 L 277.26 -88.45"
    style="fill:none"><g transform="matrix(-0.89996 0.43597 -0.43597 -0.89996 277.26
    -88.45)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83
    -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 328.29 -184.16 L 251.41 -89.58" style="fill:none"><g transform="matrix(-0.63078
    0.77596 -0.77596 -0.63078 251.41 -89.58)"><path d="M 3.32 0 C 1.94 0.28 -0.55
    0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28
    3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path d="M 506.76 -47.36
    L 370.24 -47.36 C 367.18 -47.36 364.71 -49.84 364.71 -52.89 L 364.71 -81.19 C
    364.71 -84.25 367.18 -86.73 370.24 -86.73 L 506.76 -86.73 C 509.82 -86.73 512.3
    -84.25 512.3 -81.19 L 512.3 -52.89 C 512.3 -49.84 509.82 -47.36 506.76 -47.36
    Z M 364.71 -86.73"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32 -70.66)"
    fill="#000000" stroke="#000000"><foreignobject width="138.37" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Pre-processing</foreignobject></g> <g
    fill="#CCFFCC"><path d="M 369.41 0 C 369.41 13.84 344.59 25.06 313.97 25.06 C
    283.35 25.06 258.53 13.84 258.53 0 C 258.53 -13.84 283.35 -25.06 313.97 -25.06
    C 344.59 -25.06 369.41 -13.84 369.41 0 Z M 313.97 0"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 279.38 3.42)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Background
    Removal</foreignobject></g> <g fill="#CCFFCC"><path d="M 397.09 67.04 C 397.09
    80.88 372.26 92.1 341.64 92.1 C 311.02 92.1 286.2 80.88 286.2 67.04 C 286.2 53.2
    311.02 41.99 341.64 41.99 C 372.26 41.99 397.09 53.2 397.09 67.04 Z M 341.64 67.04"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 307.05 70.46)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Removing
    Motion Blur</foreignobject></g> <g fill="#CCFFCC"><path d="M 519.48 67.04 C 519.48
    80.45 494.66 91.33 464.04 91.33 C 433.41 91.33 408.59 80.45 408.59 67.04 C 408.59
    53.63 433.41 42.76 464.04 42.76 C 494.66 42.76 519.48 53.63 519.48 67.04 Z M 464.04
    67.04"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 429.44 69.92)" fill="#000000"
    stroke="#000000"><foreignobject width="69.19" height="25.12" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Enhancement</foreignobject></g> <g fill="#CCFFCC"><path
    d="M 563.13 0 C 563.13 8.35 538.31 15.11 507.69 15.11 C 477.07 15.11 452.24 8.35
    452.24 0 C 452.24 -8.35 477.07 -15.11 507.69 -15.11 C 538.31 -15.11 563.13 -8.35
    563.13 0 Z M 507.69 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 473.1 -3.61)"
    fill="#000000" stroke="#000000"><foreignobject width="69.19" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Denoising</foreignobject></g> <path d="M 349.93
    -19.35 L 398.52 -45.51" style="fill:none"><g transform="matrix(0.88054 -0.47397
    0.47397 0.88054 398.52 -45.51)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 359.03 42.97 L 422.14 -44.39" style="fill:none"><g transform="matrix(0.58559
    -0.81061 0.81061 0.58559 422.14 -44.39)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 492.42 -14.8 L 461.48 -44.77" style="fill:none"><g
    transform="matrix(-0.7182 -0.69585 0.69585 -0.7182 461.48 -44.77)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path d="M 459.38
    42.57 L 442.92 -43.82" style="fill:none"><g transform="matrix(-0.18707 -0.98235
    0.98235 -0.18707 442.92 -43.82)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g
    stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(1.0 0.0 0.0 1.0 356.59 -67.04)"><path d="M 0 4.05 L 7.01 0 L
    0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 307.16
    -67.04 L 356.13 -67.04" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 307.16 -67.04 L 356.13 -67.04" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 307.16 -67.04 L 358.2
    -67.04" style="fill:none"></path></g><g fill="#FFCCCC"><path d="M 506.76 -252.77
    L 370.24 -252.77 C 367.18 -252.77 364.71 -255.25 364.71 -258.31 L 364.71 -286.61
    C 364.71 -289.67 367.18 -292.14 370.24 -292.14 L 506.76 -292.14 C 509.82 -292.14
    512.3 -289.67 512.3 -286.61 L 512.3 -258.31 C 512.3 -255.25 509.82 -252.77 506.76
    -252.77 Z M 364.71 -292.14"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32
    -269.04)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Apply Deep Learning based
    Classifiers</foreignobject></g> <g fill="#FFCCCC"><path d="M 506.76 -333.65 L
    370.24 -333.65 C 367.18 -333.65 364.71 -336.13 364.71 -339.19 L 364.71 -367.49
    C 364.71 -370.55 367.18 -373.02 370.24 -373.02 L 506.76 -373.02 C 509.82 -373.02
    512.3 -370.55 512.3 -367.49 L 512.3 -339.19 C 512.3 -336.13 509.82 -333.65 506.76
    -333.65 Z M 364.71 -373.02"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32
    -349.92)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Evaluation of the Model</foreignobject></g><g
    stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(0.0 -1.0 1.0 0.0 438.5 -244.66)"><path d="M 0 4.05 L 7.01 0
    L 0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 438.5
    -87.01 L 438.5 -244.19" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 438.5 -87.01 L 438.5 -244.19" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 438.5 -87.01 L 438.5
    -246.27" style="fill:none"></path></g><g stroke-width="0.6pt" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 -1.0 1.0
    0.0 438.5 -325.54)"><path d="M 0 4.05 L 7.01 0 L 0 -4.05 Z" style="fill:none"></path></g><g
    stroke-width="3.0pt"><path d="M 438.5 -292.42 L 438.5 -325.08" style="fill:none"></path></g><g
    stroke-width="1.4pt" stroke="#FFFFFF"><path d="M 438.5 -292.42 L 438.5 -325.08"
    style="fill:none"></path></g><g stroke-width="2.0pt" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><path d="M 438.5 -292.42 L 438.5 -327.15" style="fill:none"></path></g>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: A summary workflow of weed detection techniques using deep learning.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的杂草检测技术的工作流程总结。
- en: Conventional ML techniques require substantial domain expertise to construct
    a feature extractor from raw data. On the other hand, the DL approach uses a representation-learning
    method where a machine can automatically discover the discriminative features
    from raw data for classification or object detection problems [[91](#bib.bibx91)].
    A machine can learn to classify directly from images, text and sounds [[120](#bib.bibx120)].
    The ability to extract the features that best suit the task automatically is also
    known as feature learning. As deep learning is a hierarchical architecture of
    learning, the features of the higher levels of the hierarchy are composed of lower-level
    features [[110](#bib.bibx110), [58](#bib.bibx58)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习技术需要大量的领域专业知识来从原始数据中构建特征提取器。另一方面，深度学习方法使用表示学习的方法，机器可以自动从原始数据中发现用于分类或物体检测问题的区分特征[[91](#bib.bibx91)]。机器可以直接从图像、文本和声音中学习进行分类[[120](#bib.bibx120)]。自动提取最适合任务的特征的能力也被称为特征学习。由于深度学习是一个层次化的学习架构，因此层次较高的特征由层次较低的特征组成[[110](#bib.bibx110),
    [58](#bib.bibx58)]。
- en: Several popular and high performing network architectures are available in deep
    learning. Two of the frequently used architectures are Convolutional Neural Networks
    (CNNs) and Recurrent Neural Networks (RNNs) [[91](#bib.bibx91), [60](#bib.bibx60)].
    Although CNNs are used for other types of data, the most widespread use of CNNs
    is to analyse and classify images. The word convolution refers to the filtering
    process. A stack of convolutional layers is the basis of CNN. Each layer receives
    the input data, transform, or convolve them and output to the next layer. This
    convolutional operation eventually simplifies the data so that it can be better
    processed and understood. RNNs have a built-in feedback loop, which allows them
    to act as a forecasting engine. Feed-forward or CNN take a fixed size input and
    produces a fixed size output. The signal flow of the feed-forward network is unidirectional,
    i.e., from input to output. They cannot even capture the sequence or time-series
    information. RNNs overcome the limitation. In RNN, the current inputs and outputs
    of the network are influenced by prior input. Long Short-Term Memory (LSTM) is
    a type of RNN [[91](#bib.bibx91)], which has a memory cell to remember important
    prior information, thus can help improving the performance. Depending on the network
    architecture, DL has several components like convolutional layers, pooling layers,
    activation functions, dense/fully connected layers, encoder/decoder schemes, memory
    cells, gates etc. [[120](#bib.bibx120)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中有几种流行且高效的网络架构。两个常用的架构是卷积神经网络（CNNs）和递归神经网络（RNNs）[[91](#bib.bibx91), [60](#bib.bibx60)]。虽然CNNs也用于其他类型的数据，但CNNs最广泛的应用是分析和分类图像。卷积这个词指的是过滤过程。卷积层的堆叠是CNN的基础。每一层接收输入数据，进行变换或卷积，并输出到下一层。这种卷积操作最终简化了数据，使其能够更好地处理和理解。RNNs具有内置的反馈循环，使它们可以作为预测引擎。前馈网络或CNN接收固定大小的输入并产生固定大小的输出。前馈网络的信号流是单向的，即从输入到输出。它们甚至无法捕捉序列或时间序列信息。RNNs克服了这一限制。在RNN中，网络的当前输入和输出受到先前输入的影响。长短期记忆（LSTM）是一种RNN[[91](#bib.bibx91)]，它具有记忆单元来记住重要的先前信息，从而可以帮助提高性能。根据网络架构，深度学习有多个组件，如卷积层、池化层、激活函数、全连接层、编码器/解码器方案、记忆单元、门控等[[120](#bib.bibx120)]。
- en: For image classification, object detection, and localisation, DL algorithms
    have many advantages over traditional ML approaches. Because of the strong feature
    learning capabilities, DL methods can effectively extract discriminative features
    of crops and weeds. Also, with increasing data, the performance of traditional
    ML approaches has become saturated. Using large dataset, DL techniques show superior
    performance compared to traditional ML techniques [[5](#bib.bibx5)]. This characteristic
    is leading to the increasing application of DL approaches. Many of the research
    reports in Section [10](#S10 "10 Deep Learning Architecture ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images") show comparisons between
    DL and other ML approaches to detect weeds in crops. Figure [2](#S3.F2 "Figure
    2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images") gives an overview of DL-based weed
    detection and recognition techniques.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类、目标检测和定位，深度学习算法相比传统机器学习方法有很多优势。由于强大的特征学习能力，深度学习方法可以有效提取作物和杂草的辨别特征。此外，随着数据量的增加，传统机器学习方法的性能已经趋于饱和。使用大数据集时，深度学习技术相比传统机器学习技术表现出更优的性能[[5](#bib.bibx5)]。这一特性导致了深度学习方法应用的增加。许多在[10](#S10
    "10 Deep Learning Architecture ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images")节中的研究报告展示了深度学习与其他机器学习方法在作物杂草检测方面的比较。图[2](#S3.F2 "Figure
    2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")概述了基于深度学习的杂草检测和识别技术。
- en: Not all the steps outlined in Figure [2](#S3.F2 "Figure 2 ‣ 3 Traditional ML-
    vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning Techniques for
    Weed Detection from Images") need to be present in every method. Four major steps
    are followed in this process. They are Data Acquisition, Dataset Preparation/Image
    Pre-processing, Classification and Evaluation. In this paper, we describe the
    steps used in different research work to discriminate between weeds and crops
    using DL techniques.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S3.F2 "Figure 2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")中概述的所有步骤并不需要在每种方法中都出现。该过程遵循四个主要步骤：数据采集、数据集准备/图像预处理、分类和评估。在本文中，我们描述了在不同研究工作中使用的步骤，以便利用深度学习技术区分杂草和作物。
- en: 4 Paper Selection Criteria in this Survey
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 本调查中的论文选择标准
- en: 'To overview the state of research, we have undertaken a comprehensive literature
    review. The process involved two major steps: (i) searching and selecting related
    studies and (ii) detailed analysis of these studies. The main research question
    is: What is the role of deep learning techniques for detecting, localising and
    classifying weeds in crops? For collecting the related work based on this research
    question, we applied a keyword-based search in Google Scholar, Web of Science,
    IEEE Xplore, Scopus, ScienceDirect, Multidisciplinary Digital Publishing Institute
    (MDPI), Springer and Murdoch University Library databases for journal articles
    and conference papers. We have applied a keyword search from 2010 to 30 August
    2020\. Table [1](#S4.T1 "Table 1 ‣ 4 Paper Selection Criteria in this Survey ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images") shows the
    number of search results for the search query.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了概述研究状态，我们进行了全面的文献综述。该过程包括两个主要步骤：（i）搜索和选择相关研究，以及（ii）对这些研究的详细分析。主要研究问题是：深度学习技术在检测、定位和分类作物中的杂草方面的作用是什么？为了收集与这一研究问题相关的工作，我们在Google
    Scholar、Web of Science、IEEE Xplore、Scopus、ScienceDirect、多学科数字出版研究所（MDPI）、Springer和Murdoch大学图书馆数据库中应用了基于关键词的搜索，以查找期刊文章和会议论文。我们从2010年到2020年8月30日进行了关键词搜索。表[1](#S4.T1
    "Table 1 ‣ 4 Paper Selection Criteria in this Survey ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")显示了搜索查询的结果数量。
- en: 'Table 1: Number of documents resulted for the queries indicated'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：指定查询的文献数量
- en: '| No. | Academic Research Databases | Search Query | Number of Retrieved Documents
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| No. | 学术研究数据库 | 搜索查询 | 检索到的文献数量 |'
- en: '| 1. | Google Scholar | [“Weed Detection” OR “Weed management” OR “Weed Classification”]
    AND [“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”] | 998
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 1. | Google Scholar | [“杂草检测” OR “杂草管理” OR “杂草分类”] AND [“深度学习” OR “深度机器学习”
    OR “深度神经网络”] | 998 |'
- en: '| 2. | Web of Science | (Weed Detection OR Weed management OR Weed Classification)
    AND (Deep Learning OR Deep Machine Learning OR Deep Neural Network) | 124 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2. | Web of Science | (杂草检测 OR 杂草管理 OR 杂草分类) AND (深度学习 OR 深度机器学习 OR 深度神经网络)
    | 124 |'
- en: '| 3. | IEEE Xplore | (((“All Metadata”:“Deep Learning”) OR “All Metadata”:“Deep
    Machine Learning”) OR “All Metadata”:“Deep Neural Network”) AND Weed detection
    | 22 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3. | IEEE Xplore | （（（“所有元数据”:“深度学习”） OR “所有元数据”:“深度机器学习”） OR “所有元数据”:“深度神经网络”）
    AND 杂草检测 | 22 |'
- en: '| 4. | ScienceDirect | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 87
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 4. | ScienceDirect | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习” OR “深度机器学习”
    OR “深度神经网络”） | 87 |'
- en: '| 5. | Scopus | ((Weed AND detection) OR (Weed AND Management) OR (Weed AND
    Classification)) AND ((Deep AND Learning) OR (Deep AND Machine AND Learning) OR
    (Deep AND Neural AND Network)) | 118 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 5. | Scopus | （（杂草 AND 检测） OR （杂草 AND 管理） OR （杂草 AND 分类）） AND （（深度 AND 学习）
    OR （深度 AND 机器 AND 学习） OR （深度 AND 神经 AND 网络）） | 118 |'
- en: '| 6. | MDPI | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 76
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 6. | MDPI | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习” OR “深度机器学习” OR “深度神经网络”）
    | 76 |'
- en: '| 7. | SpringerLink | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 46
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 7. | SpringerLink | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习” OR “深度机器学习”
    OR “深度神经网络”） | 46 |'
- en: '| 8. | Murdoch University Library | (“Weed Detection” OR “Weed management”
    OR “Weed Classification”) AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep
    Neural Network”) | 179 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 8. | Murdoch University Library | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习”
    OR “深度机器学习” OR “深度神经网络”） | 179 |'
- en: 'After searching the above databases, duplicated documents were removed: that
    provided 988 documents. We further identified and counted those using DL-based
    methodology. In Figure [3](#S4.F3 "Figure 3 ‣ 4 Paper Selection Criteria in this
    Survey ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    we show the total number of papers which used DL between 2010 to 30 August 2020\.
    This shows that before 2016, the number of publications in this area was very
    small, but that there is an upward trend in the number of papers from 2016\. For
    this reason, articles published from 2016 and onward were used in this survey.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索了上述数据库后，重复的文献被移除：这样得到了988篇文献。我们进一步确定并统计了使用深度学习（DL）方法的文献。在图 [3](#S4.F3 "图
    3 ‣ 本调查中的4篇论文选择标准 ‣ 一项关于基于深度学习的图像杂草检测技术的调查") 中，我们展示了2010年到2020年8月30日之间使用深度学习的论文总数。这显示在2016年之前，该领域的出版物数量非常少，但从2016年开始，论文数量呈上升趋势。因此，本调查中使用了2016年及之后发表的文章。
- en: <svg   height="208.1" overflow="visible" version="1.1" width="467.38"><g transform="translate(0,208.1)
    matrix(1 0 0 -1 0 0) translate(49.37,0) translate(0,24.32) matrix(1.0 0.0 0.0
    1.0 -49.37 -24.32)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(84.18,0) translate(0,24.32)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -13.84 -19.71)" fill="#000000"
    stroke="#000000"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2010</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 20.97 -19.71)" fill="#000000" stroke="#000000"><foreignobject width="27.67"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2011</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.78 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2012</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 90.6 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2013</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.41 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2014</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.22 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 195.03 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 229.84 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 264.65 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 299.46 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 334.27 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.62 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 44.36)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$100$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 93.17)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$200$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 141.99)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$300$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -14.21 10.26)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$11$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 24.06 6.35)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$3$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 58.87 6.84)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$4$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 93.68 8.79)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$8$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.04 10.75)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$12$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 159.85 11.23)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$13$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 194.66 27.83)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$47$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 229.47 49.31)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$91$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 260.82 99.1)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="20.76" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$193$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 295.63 166.46)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$331$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 330.44 139.13)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="20.76" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$275$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 108.26 5.38)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 143.07 5.38)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 177.88 5.86)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 212.69 7.33)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 244.05 12.7)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$16$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 278.86 19.53)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$30$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 313.67 27.34)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$46$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 348.48 23.44)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$38$</foreignobject></g><g transform="matrix(0.0
    1.0 -1.0 0.0 -69.96 19.57)" fill="#000000" stroke="#000000"><foreignobject width="138.99"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Number of
    Publication</foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 51.43 155.96)"><g  transform="matrix(1 0 0 -1 0 9.365)"><g  transform="matrix(1
    0 0 1 0 10.12)"><g transform="matrix(1 0 0 -1 8.43 0) translate(61.4,0) matrix(1.0
    0.0 0.0 1.0 -58.63 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="117.27"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Searched
    document</foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 139.66 0) translate(52.79,0) matrix(1.0 0.0 0.0 1.0
    -50.02 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="100.05" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DL based article</foreignobject></g></g></g></g></g></g></svg>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The number of selected publications on DL-based weed detection approach
    from 2010 to 30 August 2020'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：2010年至2020年8月30日关于深度学习基础的杂草检测方法的所选出版物数量
- en: 5 An Overview and Taxonomy of Deep Learning-based Weed Detection Approaches
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度学习基础的杂草检测方法概述及分类
- en: An overall taxonomy of DL-based weed detection techniques is shown in Figure
    [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based Weed
    Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images"). The papers covered in this survey are categorised using this taxonomy
    and listed in Table LABEL:tab:different_DL_approach.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习基础的杂草检测技术的整体分类见图 [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep
    Learning-based Weed Detection Approaches ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images")。本文综述中涉及的论文按照此分类法进行分类，并列在表格 LABEL:tab:different_DL_approach
    中。
- en: '![Refer to caption](img/a10571ca5061433f93182d580e47d020.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a10571ca5061433f93182d580e47d020.png)'
- en: 'Figure 4: An overall taxonomy of deep learning-based weed detection techniques'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：深度学习基础的杂草检测技术的整体分类
- en: The related publications have been analysed based on the taxonomy in Figure
    [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based Weed
    Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images"). Here, the data acquisition process, sensors and mounting vehicles
    are highlighted. Moreover, an overview of the dataset preparation approaches,
    i.e., image pre-processing, data generation and annotation are also given. While
    analysing these publications, it has been found that the related works either
    generate a weed map for the target site or a classification for each of the plants
    (crops/weeds). For developing the classifiers, the researchers applied supervised,
    unsupervised or semi-supervised learning approaches. Depending on the learning
    approaches and the research goal, different DL architectures were used. An overview
    of the related research is provided in Table LABEL:tab:different_DL_approach.
    It shows the crop and weed species selected for experimental work, the steps taken
    to collect and prepare the datasets, and the DL methods applied in the research.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 相关文献已经基于图 [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based
    Weed Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") 中的分类法进行了分析。此处重点介绍了数据获取过程、传感器和安装车辆。此外，还概述了数据集准备方法，即图像预处理、数据生成和注释。在分析这些出版物时发现，相关工作要么为目标地点生成杂草地图，要么对每种植物（作物/杂草）进行分类。在开发分类器时，研究人员应用了监督学习、无监督学习或半监督学习方法。根据学习方法和研究目标，使用了不同的深度学习架构。相关研究的概述见表格
    LABEL:tab:different_DL_approach。该表显示了用于实验的作物和杂草种类、收集和准备数据集的步骤以及研究中应用的深度学习方法。
- en: 'Table 2: An overview of different DL approaches used in weed detection'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同深度学习方法在杂草检测中的概述
- en: '|  |  |  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Reference | Crop | Weed Species | DL Architectures Applied | Operations Performed
    (based on Figure [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based
    Weed Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images")) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 作物 | 杂草种类 | 应用的DL架构 | 执行的操作（基于图 [4](#S5.F4 "Figure 4 ‣ 5 An Overview
    and Taxonomy of Deep Learning-based Weed Detection Approaches ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| [[40](#bib.bibx40)] | Tomato, Cotton | Black nightshade, velvetleaf | Modified
    Xception, Inception-ResNet, VGGNet, MobileNet, DenseNet | DC; (IP, IA, ILA); PBC
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bibx40)] | 西红柿、棉花 | 黑夜shade、天鹅绒叶 | 修改版Xception、Inception-ResNet、VGGNet、MobileNet、DenseNet
    | DC；(IP, IA, ILA)；PBC |'
- en: '| [[164](#bib.bibx164)] | Sugar beet, Oilseed | Not specified | FCN | (DC,
    FR); (IP, IA, ILA); PBC |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [[164](#bib.bibx164)] | 甜菜、油菜 | 未指定 | FCN | (DC, FR)；(IP, IA, ILA)；PBC |'
- en: '| [[89](#bib.bibx89)] | Canola, corn, radish | Not specified | Filtered Local
    Binary Pattern with Contour Mask and Coefficient k (k-FLBPCM), VGG-16, VGG-19,
    ResNet-50, Inception-v3 | (ATV, MC); (IP, IA, ILA); PBC |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bibx89)] | 油菜、玉米、萝卜 | 未指定 | 带轮廓掩膜和系数 k (k-FLBPCM) 的滤波局部二值模式、VGG-16、VGG-19、ResNet-50、Inception-v3
    | (ATV, MC)；(IP, IA, ILA)；PBC |'
- en: '| [[61](#bib.bibx61)] | Not specified | Chinee apple, Lantana, Parkinsonia,
    Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed | Inception-v3,
    ResNet-50, DenseNet-202, Inception-ResNet-v2, GCN | PD; IP; PBC |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bibx61)] | 未指定 | 中国苹果、兰塔那、帕金森树、帕尔滕草、刺槐、橡胶藤、暹罗草、蛇草 | Inception-v3、ResNet-50、DenseNet-202、Inception-ResNet-v2、GCN
    | PD；IP；PBC |'
- en: '| [[161](#bib.bibx161)] | Carrot | Not specified | SegNet-512, SegNet-256 |
    PD; IA; PBC |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [[161](#bib.bibx161)] | 胡萝卜 | 未指定 | SegNet-512, SegNet-256 | PD; IA; PBC
    |'
- en: '| [[65](#bib.bibx65)] | Rice | Leptochloa chinensis, Cyperus iria, Digitaria
    sanguinalis (L). Scop, Barnyard Grass | FCN | (DC, UAV); (IP, PLA); WM |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [[65](#bib.bibx65)] | 水稻 | 狗尾草、稗草、马唐、雀稗 | FCN | (DC, UAV); (IP, PLA); WM
    |'
- en: '| [[47](#bib.bibx47)] | Sugar beet | Convolvulus sepium (hedge bindweed) |
    YOLO-v3, tiny YOLO-v3 | DC; (IA, BBA); PBC |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [[47](#bib.bibx47)] | 甜菜 | 牵牛花 | YOLO-v3, tiny YOLO-v3 | DC; (IA, BBA); PBC
    |'
- en: '| [[146](#bib.bibx146)] | Soybean | Waterhemp, Palmer amaranthus, common lambsquarters,
    velvetleaf, foxtail species | Single-Shot Detector (SSD), Faster R-CNN | (DC,
    UAV); (IP, IA, BBA); WM |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [[146](#bib.bibx146)] | 大豆 | 水藜、苋科、田间蕹菜、马唐、狗尾草 | Single-Shot Detector (SSD),
    Faster R-CNN | (DC, UAV); (IP, IA, BBA); WM |'
- en: '| [[71](#bib.bibx71)] | Corn, lettuce, radish | Cirsium setosum, Chenopodium
    album, bluegrass, sedge, other unspecified weed | GCN | PD; (IP, ILA); PBC |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [[71](#bib.bibx71)] | 玉米、生菜、萝卜 | 刺毛蓟、苋属、蓝草、莎草、其他未指定杂草 | GCN | PD; (IP, ILA);
    PBC |'
- en: '| [[18](#bib.bibx18)] | Sugar Beets, Carrots, Onions | Not specified | SegNet
    | PD; PLA; PBC |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [[18](#bib.bibx18)] | 甜菜、胡萝卜、洋葱 | 未指定 | SegNet | PD; PLA; PBC |'
- en: '| [[168](#bib.bibx168)] | Paddy | Alternanthera philoxeroides, Eclipta prostrata,
    Ludwigia adscendens, Sagittaria trifolia, Echinochloa crus-galli, Leptochloa chinensis
    | AlexNet | DC; ILA; PBC |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [[168](#bib.bibx168)] | 水稻田 | 水莲、秋水仙、鼓油菜、三叶箭、稗草、马唐 | AlexNet | DC; ILA; PBC
    |'
- en: '| [[172](#bib.bibx172)] | Wheat | Cirsium Setosum, Descurainia Sophia, Euphorbia
    Helioscopia, Veronica Didyma, Avena Fatu | YOLO-v3, Tiny YOLO-v3 | (DC, UAV);
    (IP, PLA); PBC |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [[172](#bib.bibx172)] | 小麦 | 刺毛蓟、风毛菊、旋覆花、马唐、稗草 | YOLO-v3, Tiny YOLO-v3 |
    (DC, UAV); (IP, PLA); PBC |'
- en: '| [[102](#bib.bibx102)] | Sugar beet | Dicot weeds, grass weeds | FCN | MC;
    (IP, PLA); PBC |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bibx102)] | 甜菜 | 双子叶杂草、禾本科杂草 | FCN | MC; (IP, PLA); PBC |'
- en: '| [[159](#bib.bibx159)] | Not specifies | 12 species of “Plant Seedlings dataset”,
    21 species of “CNU weeds dataset” | NASNet, ResNet, Inception–ResNet, MobileNet,
    VGGNet | DC; ILA, PD |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [[159](#bib.bibx159)] | 未指定 | “植物幼苗数据集”12种、“CNU杂草数据集”21种 | NASNet, ResNet,
    Inception–ResNet, MobileNet, VGGNet | DC; ILA, PD |'
- en: '| [[119](#bib.bibx119)] | Not specified | Scentless Mayweed, Chickweed, Cranesbill,
    Shepherd’s Purse, Cleavers, Charlock, Fat Hen, Maise, Sugar beet, Common wheat,
    Black-grass, Loose Silky-bent | Mask R-CNN | PD; PLA; PBC |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bibx119)] | 未指定 | 无香菊、鸭跖草、老鹳草、牧草繁缕、贪夜蛾、婆罗门参、麻疯树、蛇草 | Mask R-CNN
    | PD; PLA; PBC |'
- en: '| [[127](#bib.bibx127)] | Sugar beet | Not specified | DeepLab-v3, SegNet,
    U-Net | (MC, UAV); (IP, PLA) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [[127](#bib.bibx127)] | 甜菜 | 未指定 | DeepLab-v3, SegNet, U-Net | (MC, UAV);
    (IP, PLA) |'
- en: '| [[114](#bib.bibx114)] | Lettuce | Not specified | YOLO-v3, Mask R-CNN, SVM
    | (MC, UAV); (IP, PLA) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bibx114)] | 生菜 | 未指定 | YOLO-v3, Mask R-CNN, SVM | (MC, UAV);
    (IP, PLA) |'
- en: '| [[86](#bib.bibx86)] | Grasslands | Rumex obtusifolius | VGG-16 | (DC, UAV);
    (IP, PLA) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [[86](#bib.bibx86)] | 草地 | 钝叶酸模草 | VGG-16 | (DC, UAV); (IP, PLA) |'
- en: '| [[142](#bib.bibx142)] | Strawberry, Tomato | Goosegrass | Tiny YOLO-v3 |
    DC; (IP, BBA) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bibx142)] | 草莓、番茄 | 鹅草 | Tiny YOLO-v3 | DC; (IP, BBA) |'
- en: '| [[122](#bib.bibx122)] | Not specified | Colchicum autumnale | U-Net | (DC,
    UAV); (IP, IA, BBA) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [[122](#bib.bibx122)] | 未指定 | 秋水仙素 | U-Net | (DC, UAV); (IP, IA, BBA) |'
- en: '| [[31](#bib.bibx31)] | Carrot | Not specified | Faster YOLO-v3, tiny YOLO-v3
    | (DC, FR); ILA; PBC |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [[31](#bib.bibx31)] | 胡萝卜 | 未指定 | Faster YOLO-v3, tiny YOLO-v3 | (DC, FR);
    ILA; PBC |'
- en: '| [[117](#bib.bibx117)] | Blueberry | Not specified | Faster R-CNN, YOLO-v3,
    ResNet-50, ResNet-101, Darknet-53 | (DC, ATV); (IP, ILA); PBC |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bibx117)] | 蓝莓 | 未指定 | Faster R-CNN, YOLO-v3, ResNet-50, ResNet-101,
    Darknet-53 | (DC, ATV); (IP, ILA); PBC |'
- en: '| [[116](#bib.bibx116)] | Pepper | Portulaca weeds | Tiny YOLO-v3, YOLO-v3
    | (DC, ATV); (IP, BBA); PBC |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bibx116)] | 辣椒 | 马齿苋类杂草 | Tiny YOLO-v3, YOLO-v3 | (DC, ATV);
    (IP, BBA); PBC |'
- en: '| [[113](#bib.bibx113)] | Not specified | Chinee apple, Lantana, Parkinsonia,
    Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed | Inception-v3,
    ResNet-50 | (DC, FR); (IP, ILA); PBC |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bibx113)] | 未指定 | 中华苹果、马缨丹、洋木芙蓉、羽叶合欢、刺槐、胶乳藤、大飞蓬、玉米、甜菜、普通小麦、黑草、柔滑假禾
    | Inception-v3, ResNet-50 | (DC, FR); (IP, ILA); PBC |'
- en: '| [[82](#bib.bibx82)] | Clover, grass | Broad-leaved dock | AlexNet, VGG-F,
    VGG-VD-16, Inception-v1, ResNet-50, ResNet-101 | (DC, FR); PLA; PBC |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [[82](#bib.bibx82)] | 三叶草、草类 | 阔叶酸模草 | AlexNet, VGG-F, VGG-VD-16, Inception-v1,
    ResNet-50, ResNet-101 | (DC, FR); PLA; PBC |'
- en: '| [[128](#bib.bibx128)] | Mache salad | Not specified | Scatter Transform,
    Local Binary Pattern (LBP), GLCM, Gabor filter, CNN | (DC, FR); (IP, SDG, BBA);
    PBC |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bibx128)] | 沙拉菜 | 未指定 | 散射变换、局部二值模式（LBP）、GLCM、Gabor滤波器、CNN |
    （DC，FR）；（IP，SDG，BBA）；PBC |'
- en: '| [[138](#bib.bibx138)] | Chrysanthemum | Para grass, Nutsedge | SVM, Artificial
    Neural Network (ANN), CNN | DC; (IP, IA, ILA); PBC |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bibx138)] | 菊花 | 无香臭蔷薇、狗牙根 | SVM、人工神经网络（ANN）、CNN | DC；（IP，IA，ILA）；PBC
    |'
- en: '| [[104](#bib.bibx104)] | Rice | Sagittaria trifolia | SegNet, FCN, U-Net |
    DC; (IP, BBA); PBC |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [[104](#bib.bibx104)] | 水稻 | 三叶箭 | SegNet、FCN、U-Net | DC；（IP，BBA）；PBC |'
- en: '| [[9](#bib.bibx9)] | Canola | Not specified | U-Net, SegNet | (DC, ATV); (IP,
    IA, PLA); PBC |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [[9](#bib.bibx9)] | 油菜 | 未指定 | U-Net、SegNet | （DC，ATV）；（IP，IA，PLA）；PBC |'
- en: '| [[171](#bib.bibx171)] | Bermudagrass | Hydrocotyle spp., Hedyotis cormybosa,
    Richardia scabra | VGGNet, GoogLeNet, DetectNet | DC; (IP, ILA); PBC |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bibx171)] | 百慕大草 | 水蓼属、长蕊青枯草、黄檗草 | VGGNet、GoogLeNet、DetectNet
    | DC；（IP，ILA）；PBC |'
- en: '| [[1](#bib.bibx1)] | Oilseed | Not specified | FCN | DC; (IA, PLA); WM |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [[1](#bib.bibx1)] | 油料作物 | 未指定 | FCN | DC；（IA，PLA）；WM |'
- en: '| [[170](#bib.bibx170)] | Perennial ryegrass | dandelion, ground ivy, spotted
    spurge | AlexNet, VGGNet, GoogLeNet, DetectNet | DC; (IP, ILA); PBC |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [[170](#bib.bibx170)] | 常年生黑麦草 | 蒲公英、地钱、斑点麻疯树 | AlexNet、VGGNet、GoogLeNet、DetectNet
    | DC；（IP，ILA）；PBC |'
- en: '| [[97](#bib.bibx97)] | Not specified | Not specified | CNN, Histogram of oriented
    Gradients (HoG), LBP | (DC, UAV); (IP, ILA); PBC |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [[97](#bib.bibx97)] | 未指定 | 未指定 | CNN、方向梯度直方图（HoG）、LBP | （DC，UAV）；（IP，ILA）；PBC
    |'
- en: '| [[141](#bib.bibx141)] | Strawberry | Carolina geranium | VGGNet, GoogLeNet,
    DetectNet | DC; (IP, BBA); PBC |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bibx141)] | 草莓 | 卡罗莱纳天竺葵 | VGGNet、GoogLeNet、DetectNet | DC；（IP，BBA）；PBC
    |'
- en: '| [[44](#bib.bibx44)] | Sunflower, carrots, sugar beets | Not specified | SegNet,
    U-Net, BonNet, FCN8 | (DC, FR, PD); PLA; PBC |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| [[44](#bib.bibx44)] | 向日葵、胡萝卜、甜菜 | 未指定 | SegNet、U-Net、BonNet、FCN8 | （DC，FR，PD）；PLA；PBC
    |'
- en: '| [[162](#bib.bibx162)] | Grassland | Rumex obtusifolius | AlexNet | (DC, UAV);
    (IP, BBA); PBC |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [[162](#bib.bibx162)] | 草地 | 大黄 | AlexNet | （DC，UAV）；（IP，BBA）；PBC |'
- en: '| [[29](#bib.bibx29)] | Beet, cauliflower, cabbage, strawberry | Not specified
    | Hybrid Network | (DC, ATV); (IP, IA, PLA); PBC |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bibx29)] | 甜菜、花椰菜、卷心菜、草莓 | 未指定 | 混合网络 | （DC，ATV）；（IP，IA，PLA）；PBC
    |'
- en: '| [[20](#bib.bibx20)] | Carrot | Not specified | U-Net | PD; (IA, PLA); PBC
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| [[20](#bib.bibx20)] | 胡萝卜 | 未指定 | U-Net | PD；（IA，PLA）；PBC |'
- en: '| [[16](#bib.bibx16)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | ResNet-101 | PD, (IP,
    IA, BBA); PBC |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [[16](#bib.bibx16)] | 玉米、普通小麦、甜菜 | 无香臭母草、普通雀麦、荠菜、粘附草、红舌草、芥菜、肥白菜、小花苍耳、田园堇、黑草、松散丝状草
    | ResNet-101 | PD，（IP，IA，BBA）；PBC |'
- en: '| [[72](#bib.bibx72)] | Cotton | Not specified | Faster R-CNN | DC, (IP, BBA);
    PBC |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bibx72)] | 棉花 | 未指定 | Faster R-CNN | DC，（IP，BBA）；PBC |'
- en: '| [[2](#bib.bibx2)] | Paddy | Wild millet | ESNet, U-Net, FCN-8s, and DeepLab-v3,
    Faster R-CNN, EDNet | DC; (IP, IA, PLA); PBC |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [[2](#bib.bibx2)] | 稻田 | 野生小米 | ESNet、U-Net、FCN-8s、DeepLab-v3、Faster R-CNN、EDNet
    | DC；（IP，IA，PLA）；PBC |'
- en: '| [[43](#bib.bibx43)] | Sugar beet | Alli, hyme, hyac, azol, other unspecified
    weeds | CNN, FCN, LBP, superpixel based LBP, FCN-SPLBP | HC; (IP, PLA); PBC |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bibx43)] | 甜菜 | Alli、hyme、hyac、azol、其他未指定杂草 | CNN、FCN、LBP、基于超像素的LBP、FCN-SPLBP
    | HC；（IP，PLA）；PBC |'
- en: '| [[79](#bib.bibx79)] | Carrot | Not specified | CNN | DC; (IP, PLA); PBC |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [[79](#bib.bibx79)] | 胡萝卜 | 未指定 | CNN | DC；（IP，PLA）；PBC |'
- en: '| [[35](#bib.bibx35)] | Soybean | grass, broadleaf weeds, Chinee apple, Lantana,
    Parkinsonia, Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed |
    Joint Unsupervised LEarning (JULE), DeepCluster | PD; PBC |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bibx35)] | 大豆 | 草、阔叶杂草、中华苹果、藤黄、帕金森树、帕特尼草、刺槐、橡胶藤、暹罗草、蛇麻草 | 联合无监督学习（JULE）、DeepCluster
    | PD；PBC |'
- en: '| [[132](#bib.bibx132)] | Not specified | Gamba grass | U-Net | SI; (IP, PLA)
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bibx132)] | 未指定 | Gamba草 | U-Net | SI；（IP，PLA） |'
- en: '| [[147](#bib.bibx147)] | Clover | Grass | FCN-8s | DC, (IP, PLA); PBC |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [[147](#bib.bibx147)] | 苜蓿 | 草 | FCN-8s | DC，（IP，PLA）；PBC |'
- en: '| [[173](#bib.bibx173)] | Pasture | Not specified | CNN, SVM | (DC, ATV); (IP,
    IA, ILA); PBC |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [[173](#bib.bibx173)] | 牧草 | 未指定 | CNN、SVM | （DC，ATV）；（IP，IA，ILA）；PBC |'
- en: '| [[81](#bib.bibx81)] | Grasslands | Broad-leaved dock | AlexNet, VGG-F, GoogLeNet
    | (DC, FR); BBA; PBC |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bibx81)] | 草原 | 广叶大黄 | AlexNet、VGG-F、GoogLeNet | （DC，FR）；BBA；PBC
    |'
- en: '| [[64](#bib.bibx64)] | Rice | Not specified | FCN, SVM | (DC, UAV); BBA; WM
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bibx64)] | 水稻 | 未指定 | FCN、SVM | （DC，UAV）；BBA；WM |'
- en: '| [[157](#bib.bibx157)] | Not specified | Common field speedwell, field pansy,
    common chickweed, fat-hen, fine grasses (annual meadow-grass, loose silky-bent),
    blackgrass, hemp-nettle, shepherd’s purse, common fumitory, scentless mayweed,
    cereal, brassicaceae, maise, polygonum, oat (volunteers), cranesbill, dead-nettle,
    common poppy | Inception-v3 | DC; (IP, ILA); PBC |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bibx157)] | 未指定 | 普通田间速生草、田间堇菜、常见凤仙花、蓼、细草（年度草、细丝草）、黑麦草、麻疯草、雀草、常见痢疾草、无香菊、谷物、十字花科、玉米、蓼科、燕麦（志愿者）、千屈菜、荨麻、罂粟
    | Inception-v3 | DC; (IP, ILA); PBC |'
- en: '| [[160](#bib.bibx160)] | Carrot | Not specified | GoogleNet | PD, (IA, BBA);
    PBC |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [[160](#bib.bibx160)] | 胡萝卜 | 未指定 | GoogleNet | PD, (IA, BBA); PBC |'
- en: '| [[151](#bib.bibx151)] | Sugar beets | Volunteer potato | AlexNet, VGG-19,
    GoogLeNet, ResNet-50, ResNet-101, Inception-v3 | (DC, ATV); (IP, IA, ILA); PBC
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[151](#bib.bibx151)] | 甜菜 | 志愿者土豆 | AlexNet, VGG-19, GoogLeNet, ResNet-50,
    ResNet-101, Inception-v3 | (DC, ATV); (IP, IA, ILA); PBC |'
- en: '| [[41](#bib.bibx41)] | Not specified | Hyme, Alli, Azol, Hyac | CNN | HC,
    (IP, IA, BBA); PBC |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [[41](#bib.bibx41)] | 未指定 | Hyme、Alli、Azol、Hyac | CNN | HC, (IP, IA, BBA);
    PBC |'
- en: '| [[11](#bib.bibx11)] | Spinach, bean | Not specified | ResNet-18 | (DC, UAV);
    (IP, IA, BBA); WM |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [[11](#bib.bibx11)] | 菠菜、豆类 | 未指定 | ResNet-18 | (DC, UAV); (IP, IA, BBA);
    WM |'
- en: '| [[42](#bib.bibx42)] | Not specified | Hyme, Alli, Azol, Hyac | CNN, HoG |
    HC, (IP, ILA); PBC |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bibx42)] | 未指定 | Hyme、Alli、Azol、Hyac | CNN, HoG | HC, (IP, ILA);
    PBC |'
- en: '| [[103](#bib.bibx103)] | Sugar beet | Not specified | FCN | (MC, FR); (IP,
    PLA); PBC |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bibx103)] | 甜菜 | 未指定 | FCN | (MC, FR); (IP, PLA); PBC |'
- en: '| [[135](#bib.bibx135)] | Sugar beet | Galinsoga spec., Amaranthus retroflexus,
    Atriplex spec., Polygonum spec., Gramineae (Echinochloa crus-galli, agropyron,
    others.), Convolvulus arvensis, Stellaria media, Taraxacum spec. | SegNet | (MC,
    UAV); PLA; WM |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [[135](#bib.bibx135)] | 甜菜 | 细叶菊、红缨草、蓼属植物、蓼科植物、禾本科植物（狗牙根、早熟禾等）、旋花科植物、星星草、蒲公英
    | SegNet | (MC, UAV); PLA; WM |'
- en: '| [[63](#bib.bibx63)] | Rice | Not specified | CNN, FCN | (DC, UAV); (IP, PLA);
    WM |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bibx63)] | 水稻 | 未指定 | CNN, FCN | (DC, UAV); (IP, PLA); WM |'
- en: '| [[62](#bib.bibx62)] | Rice | Not specified | FCN-8s, FCN-4s, DeepLab | (DC,
    UAV) (IP, PLA); WM |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bibx62)] | 水稻 | 未指定 | FCN-8s, FCN-4s, DeepLab | (DC, UAV) (IP,
    PLA); WM |'
- en: '| [[26](#bib.bibx26)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | AlexNet, VGGNet, Hybrid
    Network | PD; PBC |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bibx26)] | 玉米、普通小麦、甜菜 | 无香菊、常见凤仙花、雀草、荨麻、红脐、芥菜、肥儿草、小花千屈菜、田间堇菜、黑麦草、细丝草
    | AlexNet, VGGNet, 混合网络 | PD; PBC |'
- en: '| [[111](#bib.bibx111)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | KNN, SVM, CNN | PD; (IP,
    BBA); PBC |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[111](#bib.bibx111)] | 玉米、普通小麦、甜菜 | 无香菊、常见凤仙花、雀草、荨麻、红脐、芥菜、肥儿草、小花千屈菜、田间堇菜、黑麦草、细丝草
    | KNN, SVM, CNN | PD; (IP, BBA); PBC |'
- en: '| [[134](#bib.bibx134)] | Sugar beet | Not specified | SegNet | (MC, UAV),
    (IP, BBA), WM |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bibx134)] | 甜菜 | 未指定 | SegNet | (MC, UAV), (IP, BBA), WM |'
- en: '| [[7](#bib.bibx7)] | Maise | Not specified | LeNET, AlexNet, cNET, sNET |
    DC; (IP, IA, PLA); PBC |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[7](#bib.bibx7)] | 玉米 | 未指定 | LeNET, AlexNet, cNET, sNET | DC; (IP, IA,
    PLA); PBC |'
- en: '| [[37](#bib.bibx37)] | Winter wheat | Not specified | FCN | (DC, ATV); (IP,
    BBA); PBC |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bibx37)] | 冬小麦 | 未指定 | FCN | (DC, ATV); (IP, BBA); PBC |'
- en: '| [[34](#bib.bibx34)] | Soybean | Grass, broadleaf weeds | AlexNet, SVM, Adaboost
    – C4.5, Random Forest | (DC, UAV); (IP, ILA); PBC |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| [[34](#bib.bibx34)] | 大豆 | 草本、阔叶杂草 | AlexNet, SVM, Adaboost – C4.5, 随机森林
    | (DC, UAV); (IP, ILA); PBC |'
- en: '| [[155](#bib.bibx155)] | Soybean | Cephalanoplos, digitaria, bindweed | Back
    propagation neural network, SVM, CNN | DC; (IP, ILA); PBC |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bibx155)] | 大豆 | 头状植物、狗尾草、贯叶藤 | 反向传播神经网络、SVM、CNN | DC; (IP, ILA);
    PBC |'
- en: '| [[108](#bib.bibx108)] | Sugar beet | Not specified | CNN | (DC, UAV); (IP,
    PLA); PBC |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bibx108)] | 甜菜 | 未指定 | CNN | (DC, UAV); (IP, PLA); PBC |'
- en: '| [[121](#bib.bibx121)] | Lawn grass | Not specified | CNN | (DC, FR); (IP,
    SDG, BBA); PBC |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bibx121)] | 草坪草 | 未指定 | CNN | (DC, FR); (IP, SDG, BBA); PBC |'
- en: '| [[33](#bib.bibx33)] | Sugar beet | Capsella bursa-pastoris, galium aparine
    | SegNet | SDG, PBC |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| [[33](#bib.bibx33)] | 甜菜 | 菊苣、荨麻 | SegNet | SDG, PBC |'
- en: '| [[38](#bib.bibx38)] | Tobacco, thale cress, cleavers, common Poppy, cornflower,
    wheat, maise, sugar beet, cabbage, barley | Sherpherd’s-Purse , chamomile, knotweed
    family, cranesbill, chickweed, veronica, fat-hen, narrow-leaved grasses, field
    pancy, broad-leaved grasses, annual nettle, black nightshade | CNN | PD; (IP,
    IA); PBC |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: 6 Data Acquisition
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL based weed detection and classification techniques require an adequate amount
    of labelled data. Different modalities of data are collected using various types
    of sensors that are mounted on a variety of platforms. Below we discuss the popular
    ways of weed data collection.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Sensors and Camera Mounting Vehicle
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1 Unmanned Aerial Vehicles (UAVs)
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unmanned Aerial Vehicles are often used for data acquisition in agricultural
    research. Generally, UAVs are used for mapping weed density across a field by
    collecting RGB images [[65](#bib.bibx65), [63](#bib.bibx63), [64](#bib.bibx64),
    [122](#bib.bibx122)] or multispectral images [[135](#bib.bibx135), [134](#bib.bibx134),
    [119](#bib.bibx119), [114](#bib.bibx114), [127](#bib.bibx127)]. In addition, UAVs
    can be used to identify crop rows and map weeds within crop rows by collecting
    RGB (Red, Green and Blue color) images [[11](#bib.bibx11)]. [[162](#bib.bibx162)]
    used a small quad-rotor UAV for recording images from grassland to detect broad-leaved
    dock (Rumex obtusifolius). As UAVs fly over the field at a certain height, the
    images captured by them cover a large area. Some of the studies split the images
    into smaller patches and use the patches to distinguish between weeds and crop
    plants [[108](#bib.bibx108), [34](#bib.bibx34), [146](#bib.bibx146)]. However,
    the flight altitude can be maintained at a low height, e.g. 2 meters, so that
    each plant can be labelled as either a weed or crop [[172](#bib.bibx172), [114](#bib.bibx114)].
    [[97](#bib.bibx97)] collected image data using a drone by maintaining an altitude
    of 2.5 meters. [[62](#bib.bibx62)] collected images with a resolution of 3000$\times$4000
    pixels using a sequence of forward-overlaps and side-overlaps to cover the entire
    field. [[86](#bib.bibx86)] flew DJI Phantom 3 and 4 Pro drones with a RGB camera
    at three different heights (10, 15 and 20 m) to determine the optimal height for
    weed detection.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Field Robots (FRs)
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Various types of field robot can also be used to collect images. A robotic vehicle
    can carry one or more cameras. As previously discussed, robotic vehicles are used
    to collect RGB images by mounted digital cameras [[31](#bib.bibx31), [113](#bib.bibx113),
    [82](#bib.bibx82), [128](#bib.bibx128), [44](#bib.bibx44)]. Mobile phone in-built
    cameras have also been used for such data collection. For example, an iPhone 6
    was used to collect video data by mounting it on a Robotic Rover [[121](#bib.bibx121)].
    A robotic platform called “BoniRob” has been used to collect multi-spectral images
    from the field [[103](#bib.bibx103), [102](#bib.bibx102)]. [[81](#bib.bibx81)]
    used three monochrome cameras mounted on a robot to take images. They argued that,
    in most cases, weeds are green, and so are the crops. There is no need to use
    colour features to distinguish them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 All-Terrain Vehicles (ATVs)
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To collect images from the field, all-terrain vehicles have also been used.
    ATVs can be mounted with different types of camera [[37](#bib.bibx37), [173](#bib.bibx173),
    [9](#bib.bibx9), [117](#bib.bibx117), [29](#bib.bibx29)]. [[89](#bib.bibx89)]
    used a combination of multi-spectral and spatial sensors to capture data. Even
    multiple low-resolution webcams have been used on an ATV [[116](#bib.bibx116)].
    To maintain specific height with external lighting conditions, and illumination,
    custom made mobile platforms have been used to carry the cameras for capturing
    RGB images [[151](#bib.bibx151), [147](#bib.bibx147)]. When it is not possible
    to use any vehicle to collect images at a certain height, tripods can be used
    as an alternative [[1](#bib.bibx1)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Collect Data without Camera Mounting Devices
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On a few occasions, weed data have been collected by cameras without being mounted
    on a vehicle. As such, video data are collected using handheld cameras [[171](#bib.bibx171),
    [138](#bib.bibx138), [104](#bib.bibx104), [170](#bib.bibx170), [157](#bib.bibx157),
    [155](#bib.bibx155), [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [79](#bib.bibx79),
    [168](#bib.bibx168), [72](#bib.bibx72), [142](#bib.bibx142)]. [[141](#bib.bibx141)]
    collected their data by maintaining a certain height (130 cm) from the soil surface.
    Brimrose VA210 filter and JAI BM-141 cameras have been used to collect hyperspectral
    images of weeds and crops without using any platform [[41](#bib.bibx41), [43](#bib.bibx43),
    [42](#bib.bibx42)]. [[7](#bib.bibx7)] manually focused a camera on the target
    plants in such a way that it could capture images, including all the features
    of these plants. In [[159](#bib.bibx159)], they focus the camera on many parts
    of weeds, such as flowers, leaf, fruits, or the full weeds structure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Satellite Imagery
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[132](#bib.bibx132)] use the Pleiades-HR 1A to collect high-resolution 4-band
    (RGB+NIR) imagery over the area of interest. They made use of high-resolution
    satellite images and applied masking to indicate the presence of weeds.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Public Datasets
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several publicly available crop and weed datasets that can be used
    to train the DL models. [[28](#bib.bibx28)] developed a dataset containing weeds
    in sugar beet crops. Another annotated dataset containing images of crops and
    weeds collected from fields has been made available by [[54](#bib.bibx54)]. A
    dataset of annotated (7853 annotations) crops and weed images was developed by
    [[150](#bib.bibx150)], which comprises 1118 images of six food crops and eight
    weed species. [[95](#bib.bibx95)] developed a dataset containing 7,590 RGB images
    with 315,038 plant objects, representing 64,292 individual plants from 47 different
    species. These data were collected in Denmark and made available for further use.
    A summary of the publicly available datasets related to weed detection and plant
    classification is listed in Table [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣
    6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection from
    Images").
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个公开的作物和杂草数据集可以用于训练深度学习模型。[[28](#bib.bibx28)] 开发了一个包含甜菜作物中杂草的数据集。另一个标注的数据集，包含从田间收集的作物和杂草的图像，由
    [[54](#bib.bibx54)] 提供。[[150](#bib.bibx150)] 开发了一个包含 7853 个标注的作物和杂草图像的数据集，其中包括
    1118 张图像，涵盖了六种食品作物和八种杂草物种。[[95](#bib.bibx95)] 开发了一个包含 7,590 张 RGB 图像的数据集，其中包括
    315,038 个植物对象，代表 47 个不同物种的 64,292 株植物。这些数据是在丹麦收集的，并且已开放供进一步使用。有关杂草检测和植物分类的公开数据集的汇总列在表格
    [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣ 6 Data Acquisition ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images") 中。
- en: We have listed nineteen datasets in Table [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets
    ‣ 6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") which are available in this area, and can be used by researchers.
    Amongst these datasets, researchers will need to send a request to the owner of
    “Perennial ryegrass and weed”, “CNU Weed Dataset” and “Sugar beet and hedge bindweed”
    dataset to obtain the data. Other datasets can be downloaded directly on-line.
    Most of the datasets contain RGB images of food crops and weeds from different
    parts of the world. The RGB data have generally been collected using high-resolution
    digital cameras. However, [[157](#bib.bibx157)] used a point grey industrial camera.
    While acquiring data for the “DeepWeeds” dataset, the researchers added a “Fujinon
    CF25HA-1” lens with their “FLIR Blackfly 23S6C” camera and mounted the camera
    on a weed control robot (“AutoWeed”). [[28](#bib.bibx28)] and [[55](#bib.bibx55)]
    employed “Bonirob” (an autonomous field robot) to mount the multi-spectral cameras.
    “Carrots 2017” and “Onions 2017” datasets were also acquired using a multi-spectral
    camera, namely the “Teledyne DALSA Genie Nano”. These researchers used a manually
    pulled cart to carry the camera. The “CNU Weed Dataset” has 208,477 images of
    weeds collect from farms and fields in the Republic of Korea, which is the highest
    number among the datasets. Though this dataset exhibits a class imbalance, it
    contains twenty-one species of weeds from five families. [[147](#bib.bibx147)]
    developed a dataset of red clover, white clover and other associated weeds. The
    dataset contains 31,600 unlabelled data together with 8000 synthetic data. Their
    goal was to generate labels for the data using unsupervised or self-supervised
    approaches. All the other datasets were manually labelled using image level, pixel-wise
    or bounding box annotation techniques.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣ 6 Data Acquisition ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") 中列出了十九个该领域可用的数据集，研究人员可以使用这些数据集。在这些数据集中，研究人员需要向“多年生黑麦草和杂草”、“CNU
    杂草数据集”和“甜菜和刺篱草”数据集的拥有者申请数据。其他数据集可以直接在线下载。大多数数据集包含来自不同地区的食品作物和杂草的 RGB 图像。这些 RGB
    数据通常使用高分辨率数字相机收集。然而，[[157](#bib.bibx157)] 使用了一台点灰工业相机。在收集“DeepWeeds”数据集的过程中，研究人员将“Fujinon
    CF25HA-1”镜头添加到他们的“FLIR Blackfly 23S6C”相机上，并将相机安装在一个杂草控制机器人（“AutoWeed”）上。[[28](#bib.bibx28)]
    和 [[55](#bib.bibx55)] 使用了“Bonirob”（一款自主田间机器人）来安装多光谱相机。“Carrots 2017”和“Onions 2017”数据集也是使用多光谱相机，即“Teledyne
    DALSA Genie Nano”获得的。这些研究人员使用了一辆手动推车来运输相机。“CNU 杂草数据集”包含了从韩国农场和田间收集的 208,477 张杂草图像，是数据集中数量最多的。虽然该数据集存在类别不平衡的问题，但它包含了来自五个科的二十一种杂草。[[147](#bib.bibx147)]
    开发了一个红三叶草、白三叶草及其他相关杂草的数据集。该数据集包含 31,600 个未标注的数据以及 8000 个合成数据。他们的目标是使用无监督或自监督方法生成数据的标签。所有其他数据集都使用图像级、像素级或边界框标注技术进行手动标注。
- en: 'Table 3: List of publicly available crop and weed datasets'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：公开可用的作物和杂草数据集列表
- en: '| Dataset and Reference | Type/Number of Crop | Type/Number of Weed Species
    | Data Type | Sensor and Mounting Vehicle | Number of Images | Data Annotation
    | Data Location | Class imbalance? | Source |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| Crop/Weed Field Image Dataset [[55](#bib.bibx55)] | Carrot | Not specified
    | Multi-spectral image | MC and FR | 60 | PLA | Germany | Yes | [https://github.com/cwfid/dataset](https://github.com/cwfid/dataset)
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| Dataset of food crops and weed [[150](#bib.bibx150)] | Six crop | Eight weed
    species | RGB | DC | 1118 | BBA | Latvia | Yes | [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/)
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| DeepWeeds [[113](#bib.bibx113)] | Not specified | Eight weed species | RGB
    | DC and FR | 17,509 | ILA | Australia | No | [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| Early crop weed dataset [[40](#bib.bibx40)] | Tomato, cotton | Black nightshad,
    velvetleaf | RGB | 508 | DC | ILA | Greece | Yes | [https://github.com/AUAgroup/early-crop-weed](https://github.com/AUAgroup/early-crop-weed)
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| Perennial ryegrass and weed [[170](#bib.bibx170)] | Perennial ryegrass |
    dandelion, ground ivy, and spotted spurge | RGB | DC | 33086 | ILA | USA | No
    | [https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full](https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full)
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Soybean and weed dataset [[34](#bib.bibx34)] | Soybean | Grass and broadleaf
    weeds | RGB | DC and UAV | 400 | ILA | Brazil | Yes | [https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops](https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops)
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Open Plant Phenotype Database [[95](#bib.bibx95)] | Not specified | 46 most
    common monocotyledon (grass) and dicotyledon (broadleaved) weeds | RGB | DC |
    7,590 | BBA | Denmark | No | [https://gitlab.au.dk/AUENG-Vision/OPPD](https://gitlab.au.dk/AUENG-Vision/OPPD)
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| Sugar beet and hedge bindweed dataset [[47](#bib.bibx47)] | Sugar beet |
    Convolvulus sepium (hedge bindweed) | RGB | DC | 652 | BBA | Belgium | Yes | [https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| Sugar beet fields dataset [[28](#bib.bibx28)] | Sugar beet | Not specified
    | Multi-spectral image | MC and FR | 12340 | PLA | Germany | No | [https://www.ipb.uni-bonn.de/2018/10/](https://www.ipb.uni-bonn.de/2018/10/)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| UAV Sugarbeets 2015-16 Datasets [[27](#bib.bibx27)] | Sugarbeets | Not specified
    | RGB | DC and UAV | 675 | PLA | Switzerland | No | [https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/](https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/)
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| Corn, lettuce and weed dataset [[71](#bib.bibx71)] | Corn and lettuce | Cirsium
    setosum, Chenopodium album, bluegrass and sedge | RGB | DC | 6800 | ILA | China
    | No | [https://github.com/zhangchuanyin/weed-datasets](https://github.com/zhangchuanyin/weed-datasets)
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| Carrot-Weed dataset [[88](#bib.bibx88)] | Carrot | Not specified | RGB |
    DC | 39 | PLA | Republic of Macedonia | Yes | [https://github.com/lameski/rgbweeddetection](https://github.com/lameski/rgbweeddetection)
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| Bccr-segset dataset [[90](#bib.bibx90)] | Canola, corn, radish | Not specified
    | RGB | DC | 30,000 | ILA | Australia | No | [https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497](https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497)
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| Carrots 2017 dataset [[18](#bib.bibx18)] | Carrots | Not specified | Multi-spectral
    image | MC and manually pulled cart | 20 | PLA | UK | No | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR)
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| Onions 2017 dataset [[18](#bib.bibx18)] | Onions | Not specified | Multi-spectral
    image | MC and manually pulled cart | 20 | PLA | UK | No | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| GrassClover image dataset [[147](#bib.bibx147)] | Red clover and white clover
    | Not specified | RGB | DC and manually operated platform | 31,600 real and 8000
    synthetic images | PLA | Denmark | Yes | [https://vision.eng.au.dk/grass-clover-dataset/](https://vision.eng.au.dk/grass-clover-dataset/)
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| Leaf counting dataset [[157](#bib.bibx157)] | Not specified | Eighteen weed
    species | RGB | DC | 9372 | ILA | Denmark | Yes | [https://vision.eng.au.dk/leaf-counting-dataset/](https://vision.eng.au.dk/leaf-counting-dataset/)
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| CNU Weed Dataset [[159](#bib.bibx159)] | Not specified | Twenty one species
    of weed | RGB | DC | 208,477 | ILA | Republic of Korea | Yes | [https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025](https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| Plant Seedlings Dataset [[49](#bib.bibx49)] | Three crop | Nine weed species
    | RGB | DC | 5539 | ILA | Denmark | Yes | [https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '[[38](#bib.bibx38)] use six publicly available datasets containing 22 different
    plant species to classify using deep learning methods. Several studies proposed
    an encoder-decoder architecture to distinguish crops and weeds using the Crop
    Weed Field Image Dataset [[161](#bib.bibx161), [20](#bib.bibx20), [160](#bib.bibx160)].
    The DeepWeeds dataset [[113](#bib.bibx113)] was used by [[61](#bib.bibx61)] to
    evaluate their proposed method. In the study of [[71](#bib.bibx71)], the “Carrot-Weed
    dataset” [[88](#bib.bibx88)] was used with their own dataset the “Corn, lettuce
    and weed dataset”. [[44](#bib.bibx44)] collected data from a sunflower farm in
    Italy. To demonstrate the proposed method’s generalising ability, they also used
    two publicly available datasets containing images of carrots, sugar beets and
    associated weeds. [[18](#bib.bibx18)] also used those datasets along with the
    Carrot 2017 and Onion 2017 datasets. The “Plant Seedlings” dataset is a publicly
    available dataset containing 12 different plant species. Several studies used
    this dataset to develop a crop-weed classification model [[26](#bib.bibx26), [111](#bib.bibx111),
    [16](#bib.bibx16), [119](#bib.bibx119)]. [[35](#bib.bibx35)] used DeepWeeds [[113](#bib.bibx113)]
    and “Soybean and weed” datasets, which are publicly available.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[[38](#bib.bibx38)] 使用了六个公开的数据集，这些数据集包含22种不同的植物物种，通过深度学习方法进行分类。一些研究提出了一种编码器-解码器架构，利用
    Crop Weed Field Image Dataset [[161](#bib.bibx161), [20](#bib.bibx20), [160](#bib.bibx160)]
    来区分作物和杂草。DeepWeeds 数据集 [[113](#bib.bibx113)] 被 [[61](#bib.bibx61)] 用于评估他们提出的方法。在
    [[71](#bib.bibx71)] 的研究中，使用了“胡萝卜-杂草数据集” [[88](#bib.bibx88)] 和他们自己的数据集“玉米、生菜和杂草数据集”。[[44](#bib.bibx44)]
    收集了来自意大利一个向日葵农场的数据。为了展示所提方法的泛化能力，他们还使用了两个公开的数据集，这些数据集包含胡萝卜、甜菜及相关杂草的图像。[[18](#bib.bibx18)]
    也使用了这些数据集以及 Carrot 2017 和 Onion 2017 数据集。“植物幼苗”数据集是一个公开的数据集，包含12种不同的植物物种。一些研究使用该数据集开发了作物-杂草分类模型
    [[26](#bib.bibx26), [111](#bib.bibx111), [16](#bib.bibx16), [119](#bib.bibx119)]。[[35](#bib.bibx35)]
    使用了 DeepWeeds [[113](#bib.bibx113)] 和“豆类和杂草”数据集，这些数据集都是公开的。'
- en: While several datasets are publicly available, they are somewhat site/crop-specific.
    As such there is no so-called benchmark weed dataset like ImageNet [[32](#bib.bibx32)]
    and MS COCO [[98](#bib.bibx98)] in this research field, that is widely used in
    the evaluation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有几个数据集是公开的，但它们在某种程度上具有特定的站点/作物特点。因此，在这个研究领域内，并没有像 ImageNet [[32](#bib.bibx32)]
    和 MS COCO [[98](#bib.bibx98)] 那样广泛用于评估的所谓基准杂草数据集。
- en: 7 Dataset Preparation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 数据集准备
- en: After acquiring data from different sources, it is necessary to prepare data
    for training, testing, and to validate models. Raw data is not always suitable
    for the DL model. The dataset preparation approaches include applying different
    image processing techniques, data labelling, using image augmentation techniques
    to increase the number of input data, or impose variations in the data and generating
    synthetic data for training. Commonly used image processing techniques are - background
    removal, resizing the collected image, green component segmentation, removing
    motion blur, de-noising, image enhancement, extraction of colour vegetation indices,
    and changing the colour model. [[121](#bib.bibx121)] decoded video into a sequence
    of RGB images and then converted them into grayscale images. In further research,
    the camera was set to auto-capture mode to collect images in the TIFF format and
    then these were converted into the RGB colour model [[151](#bib.bibx151)]. Using
    three webcams on an ATV, [[116](#bib.bibx116)] took videos and then converted
    them into different frames of images. In some occasions, it was necessary to change
    the image format to accurately train the model, especially when using public datasets.
    For instance, [[16](#bib.bibx16)] converted the “Plant Seedlings Dataset” [[49](#bib.bibx49)]
    from PNG to JPEG format, as a number of studies have show that the JPEG format
    is better for training Residual Networks architectures [[39](#bib.bibx39)].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在从不同来源获取数据后，有必要为训练、测试和验证模型准备数据。原始数据并不总是适合深度学习模型。数据集准备方法包括应用不同的图像处理技术、数据标记、使用图像增强技术来增加输入数据的数量，或对数据施加变化并生成合成数据用于训练。常用的图像处理技术包括
    - 背景去除、调整收集图像的大小、绿色成分分割、去除运动模糊、去噪、图像增强、提取颜色植被指数和更改颜色模型。[[121](#bib.bibx121)] 将视频解码成一系列RGB图像，然后将其转换为灰度图像。在进一步的研究中，摄像头设置为自动捕获模式以收集TIFF格式的图像，然后将这些图像转换为RGB颜色模型[[151](#bib.bibx151)]。使用三台网络摄像头在全地形车上，[[116](#bib.bibx116)]
    拍摄视频，然后将其转换为不同的图像帧。在某些情况下，必须更改图像格式以准确训练模型，尤其是在使用公共数据集时。例如，[[16](#bib.bibx16)]
    将“植物幼苗数据集”[[49](#bib.bibx49)] 从PNG格式转换为JPEG格式，因为许多研究表明，JPEG格式更适合训练残差网络架构[[39](#bib.bibx39)]。
- en: 7.1 Image Pre-processing
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 图像预处理
- en: The majority of relevant studies undertook some level of image processing before
    providing the data as an input to the DL model. It helps the DL architecture to
    extract features more accurately. Here we discuss image pre-processing operations
    used in the related studies.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数相关研究在将数据输入到深度学习模型之前进行了某种程度的图像处理。这有助于深度学习架构更准确地提取特征。这里我们讨论了相关研究中使用的图像预处理操作。
- en: Image Resizing
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像大小调整
- en: '[[41](#bib.bibx41)] investigate the performance of Deep Convolutional Neural
    Networks based on spatial resolution. They used three different special resolutions
    30$\times$30, 45$\times$45, and 60$\times$60 pixels. The lower patch size achieved
    good accuracy and required less time to train the model. To make the processing
    faster and reduce the computational complexity, most of the studies performed
    image resizing operations on the dataset before inputting into the DL model. After
    collecting images from the field, the resolution of the images is reduced based
    on the DL network requirement. [[171](#bib.bibx171)] used 1280$\times$720 pixel-sized
    images to train DetectNet [[156](#bib.bibx156)] architecture and 640$\times$360
    pixels for GoogLeNet [[152](#bib.bibx152)] and VGGNet [[145](#bib.bibx145)] neural
    networks. The commonly used image sizes (in pixel) are- 64$\times$64 [[11](#bib.bibx11),
    [108](#bib.bibx108), [173](#bib.bibx173), [7](#bib.bibx7)], 128$\times$128 [[40](#bib.bibx40),
    [38](#bib.bibx38), [16](#bib.bibx16)], 224$\times$224 [[113](#bib.bibx113), [71](#bib.bibx71),
    [16](#bib.bibx16)], 227$\times$227 [[162](#bib.bibx162), [151](#bib.bibx151)],
    228$\times$228 [[89](#bib.bibx89)], 256$\times$256 [[34](#bib.bibx34), [155](#bib.bibx155),
    [121](#bib.bibx121), [61](#bib.bibx61), [122](#bib.bibx122)], 320$\times$240 [[29](#bib.bibx29)],
    288$\times$288 [[2](#bib.bibx2)], 360$\times$360 [[16](#bib.bibx16)].'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Images with high resolution are sometimes split into a number of patches to
    reduce the computational complexity. For instance, in the work of [[128](#bib.bibx128)],
    the images were split with a resolution of 5120$\times$3840 into 56 patches. Similar
    operations were performed by [[63](#bib.bibx63), [9](#bib.bibx9), [104](#bib.bibx104)]
    where they divided the original images into tiles of size 912$\times$1024, 1440$\times$960
    and 1000$\times$1000 pixels. [[127](#bib.bibx127)] captured only five images at
    high resolution using a drone which was then split into small patches of size
    480$\times$360 without overlapping and 512$\times$512 with 30% overlap. [[117](#bib.bibx117)]
    collected images using three cameras simultaneously of resolution 640$\times$480
    pixels. They then merged those into a single image of 1920$\times$480 pixels which
    was resized to 1024$\times$256 pixels. [[170](#bib.bibx170)] scaled down the images
    of their dataset to 1224$\times$1024 pixels, so that the training did not run
    low on memory. [[62](#bib.bibx62)] used orthomosaic imagery, which is usually
    quite large. They split the images into small patches of 1000$\times$1000 pixels.
    In the study of [[141](#bib.bibx141)], the images were resized to 1280$\times$720
    pixels and then cropped into four sub-images. [[114](#bib.bibx114)] used 1280$\times$960
    pixel size image with four spectral bands. By applying union operation on the
    red, green, and near infrared bands, they generated a false green image in order
    to highlight the vegetation. [[142](#bib.bibx142)] resized the collected image
    to 1280$\times$853 pixels and then cropped it to 1280$\times$720 pixels.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Background Removal
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[[65](#bib.bibx65)] collected images using a UAV and applied image mosaicing
    to generate an orthophoto. [[11](#bib.bibx11)] applied Hough-transform to highlight
    the aligned pixels and used Otsu-adaptive-thresholding method to differentiate
    the background and green crops or weeds. On the other hand, for removing the background
    soil image, [[108](#bib.bibx108)] applied the Normalised Difference Vegetation
    Index (NDVI). They also used morphological opening and closing operations to remove
    the noise and fill tiny gaps among vegetation pixels. To annotate the images manually
    into respective classes, [[34](#bib.bibx34)] applied the Simple Linear Iterative
    Clustering algorithm. This algorithm helps to segment weeds, crops, and background
    from images. Image pre-processing techniques were also involved in [[134](#bib.bibx134)]
    for having a bounding box around crop plants or weeds and removed the background.
    They first used image correlation and cropping for alignment and then applied
    Gaussian blur, followed by a sharpening operation to remove shadows, small debris,
    etc. Finally, for executing the blob detection process on connected pixels, Otsu’s
    method was employed. [[102](#bib.bibx102)] applied the pre-processing operation
    on red, green, blue, and NIR channels separately. They also performed the Gaussian
    blur operation to remove noise using a [5$\times$5] kernel. To standardise the
    channels, the values were subtracted by the mean of all channel values and divided
    by their standard deviation. After that, they normalised and zero-centred the
    channel values. [[72](#bib.bibx72)] applied a Contrast Limited Adaptive Histogram
    Equalisation algorithm to enhance the image contrast and reduce the image variation
    due to ambient illumination changes.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[[65](#bib.bibx65)] 使用无人机收集了图像，并应用图像镶嵌生成了正射影像图。[[11](#bib.bibx11)] 使用霍夫变换突出对齐的像素，并使用大津自适应阈值方法区分背景与绿色作物或杂草。另一方面，为了去除背景土壤图像，[[108](#bib.bibx108)]
    应用了归一化差异植被指数（NDVI）。他们还使用了形态学开运算和闭运算来去除噪声并填补植物像素间的小空隙。为了手动注释图像到各自的类别，[[34](#bib.bibx34)]
    应用了简单线性迭代聚类算法。该算法有助于将杂草、作物和背景从图像中分割出来。[[134](#bib.bibx134)] 也涉及了图像预处理技术，以便在作物植物或杂草周围生成边界框，并去除了背景。他们首先使用图像相关性和裁剪进行对齐，然后应用高斯模糊，接着进行锐化操作以去除阴影、小碎片等。最后，为了在连接的像素上执行斑点检测过程，采用了大津方法。[[102](#bib.bibx102)]
    对红色、绿色、蓝色和NIR通道分别进行了预处理操作。他们还使用[5$\times$5]内核进行高斯模糊操作以去除噪声。为了标准化通道，值被减去所有通道值的均值，并除以其标准差。之后，他们将通道值归一化并零中心化。[[72](#bib.bibx72)]
    应用了对比度限制自适应直方图均衡算法来增强图像对比度并减少由于环境光照变化导致的图像变化。'
- en: In the work of [[89](#bib.bibx89)] and [[13](#bib.bibx13)], all images were
    segmented using the Excess Green minus Excess Red Indices (ExG-ExR) method, which
    effectively removed the background. They also applied opening and closing morphological
    operations of images and generated contour masks to extract features. On the other
    hand, [[9](#bib.bibx9)] argued that the Maximum Likelihood Classification technique
    performed better than thresholding techniques for segmenting the background soil
    and green plants. According to [[4](#bib.bibx4)], images captured from the field
    had many problems (e.g. lack of brightness). It was necessary to apply image pre-processing
    operations to prepare the data for training. They performed several morphological
    operations to remove motion blur and light illumination. They also removed the
    noisy region before applying segmentation operations for separating the background.
    Threshold-based segmentation techniques had been used to separate the soil and
    green plants in an image. In the reports of [[40](#bib.bibx40)] and [[7](#bib.bibx7)],
    the RGB channels of the images were normalised to avoid differences in lighting
    conditions before removing the background. For vegetation segmentation, Otsu’s
    thresholding was applied, followed by the ExG (Excess Green) vegetation indexing
    operation. However, [[38](#bib.bibx38)] used a simple excessive green segmentation
    technique for removing the background and detecting the green pixels. [[79](#bib.bibx79)]
    converted the RGB image to HSV colour space, applied thresholding method and band-pass
    filtering, and then used binary masking to extract the image’s green component.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Image Enhancement and Denoising
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[[111](#bib.bibx111)] investigated the importance of image pre-processing operation
    by training the CNN model with raw data and processed data. They found that without
    image pre-processing the model performance decreased. They used Gaussian Blur
    for smoothing the images and removed the high-frequency content. They then converted
    the colour of the image to HSV space. Using a morphological erosion with an 11$\times$11
    structuring kernel, they subtracted the background soil and produced foreground
    seedling images. [[103](#bib.bibx103)] reported that image pre-processing improved
    the generalisation capabilities of a classification system. They applied [5$\times$5]
    Gaussian Kernel to remove noise and to normalise the data. They also zero-centred
    the pixel values of the image. The study of [[138](#bib.bibx138)] used the Gaussian
    and median filter to remove Gaussian noise and Salt and Pepper noise respectively.
    [[155](#bib.bibx155)] also normalised the data to maintain zero-mean and unit
    variance. Besides, they applied Principal Component Analysis and Zero-phase Component
    Analysis data whitening for eliminating the correlation among the data.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[[164](#bib.bibx164)] evaluated the performance of the DL model based on the
    input representation of images. They applied many image pre-processing operations,
    such as histogram equalisation, automatic adjustment of the contrast of images
    and deep photo enhancement. They also used several vegetation indices including
    ExG, Excess Red, ExG-ExR, NDVI, Normalised Difference Index, Colour Index of Vegetation,
    Vegetative Index, and Modified Excess Green Index and Combined Indices. [[97](#bib.bibx97)]
    split the collected data into blocks which contained multiple plants. The blocks
    were then divided into sub-images with a single plant in them. After that, the
    histogram equalisation operation was performed to enhance the contrast of the
    sub-images.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[[132](#bib.bibx132)] applied orthorectification and radiometric corrections
    operation to process the satellite image. They then normalised the pixel values
    of each band. After that, the large satellite image was split into 2138 samples
    of pixel size 128$\times$128.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Training Data Generation
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To enlarge the size of the training data, in several related studies data augmentation
    was applied. It is a very useful technique when the dataset is not large enough
    [[161](#bib.bibx161)]. If there is a little variation [[138](#bib.bibx138)] or
    class imbalance [[11](#bib.bibx11)] among the images of the dataset, the image
    augmentation techniques are helpful. [[164](#bib.bibx164)] applied an augmentation
    to the dataset to determine the generalisation capability of their proposed approach.
    Table [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation ‣ 7 Dataset Preparation
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images") shows
    different types of data augmentation used in the relevant studies.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Different types of data augmentation techniques used in the relevant
    studies'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Augmentation &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Technique &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Reference |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Rotation | Rotate the image to the right or left on an axis between 1^∘ and
    359^∘  [[144](#bib.bibx144)] | [[11](#bib.bibx11), [89](#bib.bibx89), [138](#bib.bibx138),
    [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [41](#bib.bibx41), [7](#bib.bibx7),
    [38](#bib.bibx38), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173)]
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| Scaling | Use zooming in/out to resize the image [[84](#bib.bibx84)]. | [[9](#bib.bibx9),
    [47](#bib.bibx47), [2](#bib.bibx2), [20](#bib.bibx20), [16](#bib.bibx16)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| Shearing | Shift one part of the image to a direction and the other part
    to the opposite direction [[144](#bib.bibx144)]. | [[9](#bib.bibx9), [89](#bib.bibx89),
    [47](#bib.bibx47), [20](#bib.bibx20), [173](#bib.bibx173)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| Flipping | Flip the image horizontally or vertically [[84](#bib.bibx84)].
    | [[9](#bib.bibx9), [1](#bib.bibx1), [138](#bib.bibx138), [47](#bib.bibx47), [2](#bib.bibx2),
    [38](#bib.bibx38), [29](#bib.bibx29), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173),
    [122](#bib.bibx122)] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| Gamma Correction | Encode and decode the luminance values of an image [[19](#bib.bibx19)].
    | [[164](#bib.bibx164)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Colour Space | Isolating a single colour channel, increase or decrease the
    brightness of the image, changing the intensity values in the histograms [[144](#bib.bibx144)].
    | [[11](#bib.bibx11), [164](#bib.bibx164), [9](#bib.bibx9), [40](#bib.bibx40),
    [2](#bib.bibx2), [29](#bib.bibx29), [122](#bib.bibx122)] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| Colour Space Transformations | Increase or decrease the pixel values by a
    constant value and restricting pixel values to a certain min or max value [[144](#bib.bibx144)].
    | [[89](#bib.bibx89), [138](#bib.bibx138), [16](#bib.bibx16), [122](#bib.bibx122)]
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Noise Injection | Injecting a matrix of random values to the image matrix.
    For example: Salt-Pepper noise, Gaussian noise etc [[144](#bib.bibx144)]. | [[138](#bib.bibx138),
    [40](#bib.bibx40), [122](#bib.bibx122)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Kernel filtering | Sharpening or blurring the image [[144](#bib.bibx144)].
    | [[11](#bib.bibx11), [9](#bib.bibx9), [40](#bib.bibx40), [122](#bib.bibx122)]
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| Cropping | Remove a certain portion of an image [[154](#bib.bibx154)]. Usually
    this is done at random in case of data augmentation [[144](#bib.bibx144)]. | [[9](#bib.bibx9),
    [2](#bib.bibx2), [41](#bib.bibx41), [122](#bib.bibx122)] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| Translation | Shift the position of all the image pixels [[66](#bib.bibx66)].
    | [[9](#bib.bibx9), [1](#bib.bibx1), [20](#bib.bibx20)] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: As shown in Table [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation ‣ 7 Dataset
    Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    it is observed that in most of the studies, different geometric transformation
    operation were applied to the data. Use of colour augmentation can be helpful
    to train a model for developing a real-time classification system. This is because
    the colour of the object varies depending on the lighting condition and motion
    of the sensors.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Image data that are not collected from the real environments and created artificially
    or programmatically are known as synthetic data or images [[163](#bib.bibx163)].
    It is not always possible to manage a large amount of labelled data to train a
    model. In these cases, the use of synthetic data is an excellent alternative to
    use together with the real data. Several research studies show that artificial
    data might have a significant change in classifying images [[8](#bib.bibx8)].
    In weed detection using DL approaches, synthetic data generation technique is
    not applied very often. [[128](#bib.bibx128)] used synthetically generated images
    to train the model and achieved a good classification accuracy while testing on
    a real dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, [[121](#bib.bibx121)] created complex occlusion of crops
    and weeds and generated variation in leaf size, colour, and orientation by producing
    synthetic data. To minimise human effort for annotating data, [[33](#bib.bibx33)]
    generated synthetic data to train the model. For that purpose, they used a generic
    kinematic model of a leaf prototype to generate a single leaf of different plant
    species and then meshed that leaf to the artificial plant. Finally, they placed
    the plant in a virtual crop field for collecting the data without any extra effort
    for annotation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[[147](#bib.bibx147)] generated a 8000 synthetic dataset for labelling a real
    dataset. To create artificial data, they cropped out different parts of the plant,
    randomly selected any background from the real data, applied image processing
    (e.g. rotation, scaling, etc.), and added an artificial shadow using a Gaussian
    filter.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Data Labelling
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of the reviewed publications used manually annotated data labelled
    by experts for training the deep learning model in a supervised manner. The researchers
    applied different annotations, such as bounding boxes annotation, pixel-level
    annotation, image-level annotation, polygon annotation, and synthetic labelling,
    based on the research need. Table [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7
    Dataset Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") shows different image annotation approaches used for weed detection.
    However, [[71](#bib.bibx71)] applied a semi-supervised method to label the images;
    they used a few labelled images to annotate the unlabelled data. On the other
    hand, [[35](#bib.bibx35)] proposed a semi-automatic labelling approach. Unlike
    semi-supervised data annotation, they did not use any manually labelled data,
    but applied the clustering method to label the data. First, they divided the data
    into different clusters according to their features and then labelled the clusters.
    Similar techniques were used by [[51](#bib.bibx51)]. [[170](#bib.bibx170)] separated
    the collected images into two parts; one with positive images that contained weeds,
    and the other of negative images without weeds. [[86](#bib.bibx86)] proposed an
    object-based approach to generate labelled data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Different image annotation techniques used for weed detection using
    deep learning'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '| Type of Image Annotation | Description | Reference |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: '| Pixel Level Annotation | Label each pixel whether it belongs to crop or weed
    in the image. | [[68](#bib.bibx68), [17](#bib.bibx17), [96](#bib.bibx96), [65](#bib.bibx65),
    [135](#bib.bibx135), [172](#bib.bibx172), [82](#bib.bibx82), [51](#bib.bibx51),
    [1](#bib.bibx1), [7](#bib.bibx7), [44](#bib.bibx44), [29](#bib.bibx29), [20](#bib.bibx20),
    [2](#bib.bibx2), [43](#bib.bibx43), [18](#bib.bibx18), [9](#bib.bibx9), [79](#bib.bibx79),
    [161](#bib.bibx161), [103](#bib.bibx103), [63](#bib.bibx63), [62](#bib.bibx62),
    [108](#bib.bibx108), [102](#bib.bibx102), [132](#bib.bibx132), [119](#bib.bibx119),
    [114](#bib.bibx114), [127](#bib.bibx127), [86](#bib.bibx86), [147](#bib.bibx147)]
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
- en: '| Region Level Annotation | Bounding Boxes Annotation | There may be a mixture
    of weeds and crops in a single image. Using a bounding box the crops and weeds
    are labelled in the image. | [[64](#bib.bibx64), [134](#bib.bibx134), [11](#bib.bibx11),
    [81](#bib.bibx81), [104](#bib.bibx104), [111](#bib.bibx111), [41](#bib.bibx41),
    [141](#bib.bibx141), [72](#bib.bibx72), [16](#bib.bibx16), [47](#bib.bibx47),
    [146](#bib.bibx146), [116](#bib.bibx116), [128](#bib.bibx128), [162](#bib.bibx162),
    [37](#bib.bibx37), [122](#bib.bibx122), [142](#bib.bibx142)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
- en: '|  | Polygon Annotation | This is used for semantic segmentation to detect
    irregular shaped object. It outlines the region of interest with arbitrary number
    of sides. | [[119](#bib.bibx119)] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| Image Level Annotation | Uses separate image for weeds and crops to train
    the model. | [[171](#bib.bibx171), [31](#bib.bibx31), [164](#bib.bibx164), [173](#bib.bibx173),
    [117](#bib.bibx117), [89](#bib.bibx89), [151](#bib.bibx151), [4](#bib.bibx4),
    [138](#bib.bibx138), [170](#bib.bibx170), [157](#bib.bibx157), [155](#bib.bibx155),
    [40](#bib.bibx40), [42](#bib.bibx42), [168](#bib.bibx168), [97](#bib.bibx97),
    [71](#bib.bibx71), [113](#bib.bibx113), [34](#bib.bibx34), [159](#bib.bibx159)]
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| Synthetic Labelling | For training the model use generated and labelled data.
    | [[121](#bib.bibx121), [33](#bib.bibx33)] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: 'As summarised in Table [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7 Dataset
    Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    commonly used annotation techniques are bounding boxes, pixel-wise labelling and
    image level annotation. However, plants are irregular in shape: by using polygon
    annotation, the images of crops and weeds can be separated accurately. Synthetic
    labelling approaches can minimise labelling costs and help to generate large annotated
    datasets.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 8 Detection Approaches
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Studies in this area apply two broad approaches for detecting, localising,
    and classifying weeds in crops: i) localise every plant in an image and classify
    that image either as a crop or as a weed; ii) map the density of weeds in the
    field. To detect weeds in crops, the concept of “row planting” has been used.
    In some of these studies, there are further classification steps of the weed species.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 8.1 Plant-based Classification
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To develop a weed management system, a major step is to classify every plant
    as weed or crop plant [[101](#bib.bibx101)]. The first problem is to detect weeds,
    followed by localisation and finally, classification. This approach is useful
    for real-time weed management techniques. For instance, [[126](#bib.bibx126)]
    developed a real-time weeding system where a robotic machine detected the weeds
    and used a knife to remove them. In this case, it was necessary to label individual
    plants, whether as a weed or crop plant. In traditional farming approaches, farmers
    usually apply a uniform amount of herbicide over the whole crop in a field. A
    machine needs to identify individual crop plants and weeds to apply automatic
    selective spraying techniques. Besides, identifying the weed species is also important
    to apply specific treatments [[102](#bib.bibx102)]. We have found that this approach
    has been used in most of the studies reported.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 8.2 Weed Mapping
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mapping weed density can also be helpful for site-specific weed management and
    can lead to a reduction in the use of herbicides. [[65](#bib.bibx65)] used the
    DL technique to map the density of weeds in a rice field. An appropriate amount
    of herbicides can be applied to a specific site based on the density map. The
    work in [[1](#bib.bibx1)] segmented the images and detected the weed presence
    in the region of that image. Using a deep learning approach, [[64](#bib.bibx64)]
    generated a weed distribution map of the field. In addition, some researchers
    argued that weed mapping helps to monitor the conditions of the field automatically
    [[134](#bib.bibx134), [135](#bib.bibx135)]. Farmers can monitor the distribution
    and spread of weeds, and can take action accordingly.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 9 Learning Methods
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 9.1 Supervised Learning
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised learning occurs when the datasets for training and validation are
    labelled. The dataset passed in the DL model as input contains the image along
    with the corresponding labels. That means, in supervised training, the model learns
    how to create a map from a given input to a particular output based on the labelled
    dataset. Supervised learning is popular to solve classification and regression
    problems [[22](#bib.bibx22)]. In most of the related research the supervised learning
    approach was used to train the DL models. Section [10](#S10 "10 Deep Learning
    Architecture ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")
    presents a detail description of those DL architectures.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Unsupervised Learning
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unsupervised learning occurs when the training set is not labelled. The dataset
    passed as input in the unsupervised model has no corresponding annotation. The
    models attempt to learn the structure of the data and extract distinguishable
    information or features from data. Using this process, the model becomes able
    to map the input to the particular output. From this, the objects in the whole
    dataset will be divided into separate groups or clusters. The features of the
    objects in a cluster are similar and differ from other clusters. This is how unsupervised
    learning can classify objects of a dataset into separate categories. Clustering
    is one of the applications of unsupervised learning [[14](#bib.bibx14)].
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Most of the relevant studies used a supervised learning approach to detect and
    classify weeds in crops automatically. However, [[35](#bib.bibx35)] proposed unsupervised
    clustering algorithms with a semi-automatic data labelling approach in their research.
    They applied two clustering methods- Joint Unsupervised Learning (JULE) and Deep
    Clustering for Unsupervised Learning of Visual Features algorithms (DeepCluster).
    They developed the models using AlexNet [[83](#bib.bibx83)] and VGG-16 [[145](#bib.bibx145)]
    architecture and initialised with pre-trained weights. They achieved a promising
    result (accuracy 97%) in classifying weeds in crops and reduce the cost of manual
    data labelling.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[[155](#bib.bibx155)] applied an unsupervised K-means clustering algorithm
    as a pre-training process and generate a feature dictionary. They then used those
    features to initialise the weights of the CNN model. They claimed that it can
    improve generalisation ability in feature extraction and resolve the unstable
    identification problem. The proposed approach shows better accuracy than SVM,
    Back Propagation neural network, and even CNN with randomly initialised weights.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Semi-supervised Learning
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semi-supervised learning takes the middle ground between supervised and unsupervised
    learning [[94](#bib.bibx94)]. A few researchers used Graph Convolutional Network
    (GCN) [[78](#bib.bibx78)] in their research, which is a semi-supervised model.
    The major difference between CNN and GCN is the structure of input data. CNN is
    for regular structured data, whereas GCN uses graph data structure [[105](#bib.bibx105)].
    We discuss the use of GCN in the related work in Section [10.4](#S10.SS4 "10.4
    Graph Convolutional Network (GCN) ‣ 10 Deep Learning Architecture ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images").
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 10 Deep Learning Architecture
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our analysis shows that the related studies apply different DL architectures
    to classify the weeds in crop plants based on the dataset and research goal. Most
    researchers compared their proposed models either with other DL architecture or
    with traditional machine learning approaches. Table LABEL:tab:different_DL_approach
    shows an overview of different DL approach used in weed detection. A CNN model
    generally consists of two basic parts- feature extraction and classification [[77](#bib.bibx77)].
    In related research, some researchers applied CNN models using various permutation
    of feature extraction and classification layers. However, in most cases, they
    preferred to use state-of-art CNN models like VGGNet [[145](#bib.bibx145)], ResNet
    (deep Residual Network) [[56](#bib.bibx56)], AlexNet [[83](#bib.bibx83)], InceptionNet
    [[152](#bib.bibx152)], and many more. Fully Convolutional Networks (FCNs) like
    SegNet [[10](#bib.bibx10)] and U-Net [[133](#bib.bibx133)] were also used in several
    studies.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Convolutional Neural Network (CNN)
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 10.1.1 Pre-trained Network
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[151](#bib.bibx151)] applied six well known CNN models namely AlexNet, VGG-19,
    GoogLeNet, ResNet-50, ResNet-101 and Inception-v3\. They evaluated the network
    performance based on the transfer learning approach and found that pre-trained
    weights had a significant influence on training the model. They obtained the highest
    classification accuracy (98.7%) using the VGG-19 model, but it took the longest
    classification time. Considering that, the AlexNet model worked best for detecting
    volunteer potato plants in sugar beet according to their experimental setup. Even
    under varying light conditions, the model could classify plants with an accuracy
    of about 97%. The study of [[34](#bib.bibx34)] also supported that. They compared
    the classification accuracy of AlexNet with SVM, Adaboost – C4.5, and the Random
    Forest model. The AlexNet architecture performed better than other models in discriminating
    soybean crop, soil, grass, and broadleaf weeds. Similarly, [[162](#bib.bibx162)]
    reported that the AlexNet model with pre-trained weights showed excellent performance
    for detecting Rumex in grasslands. They also showed that by increasing heterogeneous
    characteristics of the input image might improve the model accuracy (90%). However,
    [[86](#bib.bibx86)] argued that to detect Rumex in grassland the VGG-16 model
    performs well with an accuracy of 92.1%.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[[157](#bib.bibx157)] demonstrated that, although ImageNet dataset does not
    contain the images of different plant species, the pre-trained weights of the
    dataset could still help to reduce the number of training iterations. They fine-tuned
    Inception-v3 architecture for classifying eighteen weed species and determining
    growth stages based on the number of leaves. The model achieved the classification
    accuracy of 46% to 78% and showed an average accuracy of 70% while counting the
    leaves. However, [[113](#bib.bibx113)] differed from them. They developed a multi-class
    weed image dataset consisting of eight nationally significant weed species. The
    dataset contains 17,509 annotated images collected from different locations of
    northern Australia. They also applied the pre-trained Inception-v3 model along
    with ResNet-50 to classify the weed species (source code is available here: [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)).
    The average classification accuracy of ResNet-50 (95.7%) was a little higher than
    Inception-v3 (95.1%). [[11](#bib.bibx11)] also used ResNet with pre-trained weights
    as they found it more useful to detect weeds.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: According to [[171](#bib.bibx171)], Deep Convolutional Neural Network (DCNN)
    can perform well in detecting different species of weeds in bermudagrass. They
    used three pre-trained (from ImageNet dataset and KITTI dataset [[48](#bib.bibx48)])
    models including VGGNet, GoogLeNet and DetectNet. In another study, they added
    AlexNet architecture with the previous models for detecting weeds in perennial
    ryegrass [[170](#bib.bibx170)]. Though all the models performed well, DetectNet
    exhibited a bit higher F1 score of $\geq$0.99\. On the other hand, [[141](#bib.bibx141)]
    evaluated the performance of VGGNet, GoogLeNet, and DetectNet architecture using
    two variations of images (i.e., whole and cropped images). They also agreed that
    the DetectNet model could detect and classify weed in strawberry plants more accurately
    using cropped sub-images. They suggested that the most visible and prevalent part
    of the plant should be annotated rather than labelling the whole plant in the
    image.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[[89](#bib.bibx89)] proposed a model namely Filtered LBP (Local Binary Patterns)
    with Contour Mask and coefficient k (k-FLBPCM). They compared the model with VGG-16,
    VGG-19, ResNet-50, and Inception-v3 architecture. The k-FLBPCM method effectively
    classified barley, canola and wild radish with an accuracy of approximately 99%,
    which was better than other CNN models (source code is available here: [https://github.com/vinguyenle/k-FLBPCM-method](https://github.com/vinguyenle/k-FLBPCM-method)).
    The network was trained using pre-trained weights from ImageNet dataset.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[[7](#bib.bibx7)] compared the performance of LeNET [[92](#bib.bibx92)], AlexNet,
    cNET [[46](#bib.bibx46)], and sNET [[124](#bib.bibx124)] in their research. They
    found that cNET was better in classifying maize crop plants and their weeds. They
    further compared the performance of the original cNET architecture with the reduced
    number of filter layers (16 filter layers). The result reported that with pre-processed
    images, 16 filter layers were adequate to classify the crops and weeds. Besides,
    it made the model 2.5 times faster than its typical architecture and helped to
    detect weeds in real-time.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[[117](#bib.bibx117)] analysed the performance of Faster R-CNN [[131](#bib.bibx131)],
    YOLO-v3 [[130](#bib.bibx130)], ResNet-50, ResNet-101, and Darknet-53 [[129](#bib.bibx129)]
    models to develop a smart sprayer for controlling weed in real-time. Based on
    precision and recall value, ResNet-50 model performed better than others. In contrast,
    [[16](#bib.bibx16)] applied the ResNet-101 model. They demonstrated that the size
    of the input image could affect the performance of ResNet-101 architecture. They
    used three different pixel sizes (i.e., 128px, 224px, and 360px) for their experiment
    and reported that model accuracy gets better by increasing the pixel size of the
    input image.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[[159](#bib.bibx159)] proposed a multi-modal DL approach for classifying species
    of weeds. In this approach, they trained five pre-trained DL models, including
    NASNet, ResNet, Inception–ResNet, MobileNet, and VGGNet independently. The Bayesian
    conditional probability-based technique and priority weight scoring method were
    used to calculate the score vector of models. The model with better scores has
    a higher priority on determining the classes of species. To classify weed species,
    they summed up the probability vectors generated by the softmax layer of each
    model and the species with the highest probability value was determined. According
    to the experimental results, they argued that the performance of this approach
    was better than a single DL model.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Training from Scratch
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[[38](#bib.bibx38)] argued that, a CNN model initialised with pre-trained weights
    which was not trained on any plant images would not work well. They therefore
    built a new architecture using a combination of convolutional layers, batch normalisation,
    activation functions, max-pooling layers, fully connected layers, and residual
    layers according to their need. The model was used to classify twenty-two plant
    species, and they achieved a classification accuracy ranging from 33% to 98%.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[[108](#bib.bibx108)] built a CNN model for blob wise discrimination of crops
    and weeds. They used multi-spectral images to train the model. They investigated
    different combinations of convolutional layers and fully connected layers to explore
    an optimised, light-weight and over-fitting problem-free model. Finally, using
    three convolutional layers and two fully connected layers, they obtained a better
    result. They stated that this approach did not have any geometric priors like
    planting the crops in rows. [[41](#bib.bibx41)] claimed in their research that
    the classification accuracy of the CNN model depended on the number of the hyperspectral
    band and the resolution of the image patch. They also built a CNN model using
    a combination of convolutional, nonlinear transformation, pooling and dropout
    layers. In further research, they proved that a CNN model trained with a higher
    number of bands could classify images more accurately than HoG (Histogram of oriented
    Gradients) based method [[42](#bib.bibx42)].'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[[111](#bib.bibx111)] compared CNN’s performance with SVM (61.47%) and K-Nearest
    Neighbour (KNN) algorithm (56.84%) and found that CNN could distinguish crop plants
    from weeds better. They used six convolutional layers and three fully connected
    layers in the CNN architecture to achieve the accuracy of 92.6%. They also evaluated
    the accuracy of CNN using the original images and the pre-processed images. The
    experimental results suggested that classification accuracy improved by using
    pre-processed images. [[138](#bib.bibx138)] agreed that CNN offers better accuracy
    than SVM and ANN in detecting weeds in crop plants because of its deep learning
    ability. [[97](#bib.bibx97)] employed a CNN architecture that consists of three
    convolutional, three pooling, four Dropout layers, and a fully connected layer
    for developing a low-cost weed recognition system. Their experiment also proved
    that the performance of the CNN model in classification was better than the HoG
    and LBP methods. [[173](#bib.bibx173)] also demonstrated that the CNN model was
    better than SVM for detecting broad-leaf weeds in pastures. They used a CNN model
    with six convolutional layers and three fully connected classification layers.
    The model could recognise weeds with an accuracy of 96.88%, where SVM achieved
    maximum accuracy of 89.4%.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[[121](#bib.bibx121)] used synthetic data to train their CNN model and evaluated
    it on real data. They built a CNN model with five convolutional layers and two
    fully connected layers. The results showed that CNN could classify crop plants
    and weeds very well from natural images and with multiple occlusion. Although
    [[128](#bib.bibx128)] applied the same architecture in their research, they argued
    that the Scatter Transform method achieved better accuracy with a small dataset
    than the CNN architecture. They compared several machine learning approaches like
    Scatter Transform, LBP, GLCM, Gabor filter with the CNN model. They also used
    synthetic data for training and evaluated the models’ performance on real field
    images.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Region Proposal Networks (RPN)
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the tiny YOLO-v3 [[169](#bib.bibx169)] framework, [[47](#bib.bibx47)]
    proposed a DL model which speeds up the inference time of classification (source
    code is available here: [https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing](https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing)).
    They added two extra convolutional layers to the original model for better feature
    fusion and also reduced the number of detection scales to two. They trained the
    model with both synthetic data and real data. Although YOLO-v3 archived better
    classification accuracy in the experiments, they recommended the tiny YOLO-v3
    model for real-time application. [[142](#bib.bibx142)] also used tiny YOLO-v3
    model to detect goosegrass in strawberry and tomato plants.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: YOLO-v3 and tiny YOLO-v3 models were also employed in a research by [[116](#bib.bibx116)].
    The aim was to find a low-cost, smart weed management system. They applied the
    models on two machines with different hardware configurations. Their paper reported
    that YOLO-v3 showed good performance when tested on powerful and expensive computers,
    but the processing speed decreased if executed on a lower power computer. From
    their experiments, they came to the conclusion that to save the hardware cost,
    the tiny YOLO-v3 model was better. [[173](#bib.bibx173)] also preferred to use
    tiny YOLO-v3 instead of YOLO-v3, because it was a lightweight method and took
    less time and resources to classify objects. In contrast, [[31](#bib.bibx31)]
    proposed to use YOLO-v3 with a relatively larger input image size (832 $\times$
    832 pixels). They argued that the model performed better in their research with
    a small dataset. They agreed that tiny YOLO-v3 or Fast YOLO-v3 could improve the
    detection speed, but there was a need to compromise with the model accuracy.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[[146](#bib.bibx146)] trained and evaluated the performance of a pre-trained
    Faster R-CNN and SSD (Single Shot Detector) [[100](#bib.bibx100)] object detection
    models to detect late-season weed in soybean fields. Moreover, they compared these
    object detection models with patch-based CNN model. The result showed that Faster
    R-CNN performed better in terms of weed detection accuracy and inference speed.
    [[72](#bib.bibx72)] proposed the Faster R-CNN model to detect the weeds and crop
    plants and to count the number of seedlings from the video frames. They used Inception-ResNet-v2
    architecture as the feature extractor. On the other hand, by applying the Mask
    R-CNN model on “Plant Seedlings Dataset” [[119](#bib.bibx119)] achieved more than
    98% classification accuracy. They argued that Mask R-CNN detected plant species
    more accurately with less training time than FCN.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[[114](#bib.bibx114)] compared two RPN models, namely YOLO-v3 and Mask R-CNN
    with SVM. The classification accuracy of RPN architectures was 94%, whereas SVM
    achieved 88%. However, they reported that as SVM required less processing capacity,
    it could be used for IoT based solution.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Fully Convolutional Networks (FCN)
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike CNN, FCN replaces all the fully connected layers with convolutional layers
    and uses a transposed convolution layer to reconstruct the image with the same
    size as the input. It helps to predict the output by making a one-to-one correspondence
    with the input image in the spatial dimension [[143](#bib.bibx143), [65](#bib.bibx65)].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[[63](#bib.bibx63)] compared the performance of AlexNet, VGGNet, and GoogLeNet
    as the base model for FCN architecture. VGGNet achieved the best accuracy among
    those. They further compared the model with patch-based CNN and pixel-based CNN
    architectures. The result showed that the VGG-16 based FCN model achieved the
    highest classification accuracy. On the other hand, [[64](#bib.bibx64)] applied
    ResNet-101 and VGG-16 as a baseline model of FCN for segmentation. They also compared
    the performance of the FCN models with a pixel-based SVM model. In their case,
    ResNet-101 based FCN architecture performed better. [[9](#bib.bibx9)] compared
    two FCN architecture for detecting weeds in canola fields, i.e., SegNet and U-Net.
    They used VGG-16 and ResNet-50 as the encoder block in both the models. The SegNet
    with ResNet-50 as the base model achieved the highest accuracy.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'According to [[104](#bib.bibx104)], SegNet (accuracy 92.7%) architecture was
    better than traditional FCN (accuracy 89.5%) and U-Net (accuracy 70.8%) for weed
    image segmentation when classifying rice plants and weeds in the paddy field.
    The study of [[1](#bib.bibx1)] reported that the accuracy of image segmentation
    depended on the size of the dataset. That is why it is difficult to train a model
    from scratch. To address this problem, they applied transfer learning and real-time
    data augmentation to train the model. In their experiment, they used VGG-16 based
    SegNet architecture. They applied three different transfer learning approaches
    for VGG-16\. Moreover, the performance of the model was compared with the VGG-19
    based architecture. The VGG-16 based SegNet achieved the highest accuracy of 96%
    when they used pre-trained weights only for feature extraction and the shallow
    machine learning classifier (i.e., SVM) for segmentation. [[134](#bib.bibx134)]
    also applied SegNet with the pre-trained VGG-16 as the base model (source code
    is available here: [https://github.com/inkyusa/weedNet](https://github.com/inkyusa/weedNet)).
    They trained the model by varying the number of channels in the input images.
    They then compared the inference speed and accuracy of different arrangements
    by deploying the model on an embedded GPU system, which was carried out by a small
    micro aerial vehicle (MAV). [[161](#bib.bibx161)] compared the performance of
    SegNet-512 and SegNet-256 encoder-decoder architectures for semantic segmentation
    of weeds in crop plants. The experiment proved that SegNet-512 was better for
    classification. In the study of [[33](#bib.bibx33)], the SegNet model was trained
    using synthetic data, and the performance was evaluated on a real crop and weed
    dataset.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[[44](#bib.bibx44)] proposed U-Net architecture using VGG-16 as an encoder
    for semantic segmentation. They also applied a VGG-16 model for classifying the
    crop plants and weeds. They also trained the model with one dataset containing
    sunflower crop and evaluated it with two different datasets with carrots and sugar
    beets crops. In the work of [[132](#bib.bibx132)], a ResNet based U-Net model
    was employed to map the presence of gamba grass in the satellite image. However,
    [[127](#bib.bibx127)] compared the performance of DeepLab-v3 [[30](#bib.bibx30)]
    with SegNet and U-Net model in their research. The results demonstrated that DeepLab-v3
    architecture achieved better classification accuracy using class balanced data
    that has greater spatial context.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[[103](#bib.bibx103)] also proposed FCN architecture using DenseNet as a baseline
    model. Their novel approach provided a pixel-wise semantic segmentation of crop
    plants and weeds. The work of [[102](#bib.bibx102)] proposed a task-specific decoder
    network. As the plants were sown at a regular distance, they trained the model
    in a way so that the model could learn the spatial plant arrangement from the
    image sequence. They then fused this sequential feature with the visual features
    to localise and classify weeds in crop plants. [[37](#bib.bibx37)] used FCN architecture
    not only for segmentation but also for generating bounding boxes around the plants.
    They applied pre-trained GoogLeNet architecture as the base model.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: According to [[164](#bib.bibx164)], changes in the input representation could
    make a difference in classification performance. They employed the encoder-decoder
    deep learning network for semantic segmentation of crop and weed plants by initialising
    the input layers with pre-trained weights. They evaluated the model with different
    input representation by including NIR information with colour space transformation
    on the input, which improved crop-weed segmentation and classification accuracy
    (96%). [[135](#bib.bibx135)] also evaluated different input representation to
    train the network. They applied VGG-16 based SegNet architecture for detecting
    background, crop plants and weeds. The model was evaluated by varying the number
    of spectral bands and changing the hyper-parameters. The experimental results
    showed that the model achieved far better accuracy by using nine spectral channels
    of an image rather than the RGB image.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[[62](#bib.bibx62)] stated that the original FCN-4s architecture was designed
    for PASCAL VOC 2011 dataset, which had 1000 classes of objects. However, their
    dataset had only three categories (i.e., rice, weeds, and others). As a result
    they reduced the feature maps of the intermediate layers to 2048\. They then compared
    the accuracy and efficiency of the model with original FCN-8s and DeepLab architecture
    and proved that the modified FCN-4s model performed better. For the same reason,
    [[18](#bib.bibx18)] simplified the original architecture of SegNet and named it
    as SegNet‐Basic. They decreased the number of convolutional layers from 13 to
    4.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: One of the problems with the basic architecture of FCN is that the spatial features
    can not be recovered properly. The prediction accuracy can be decreased due to
    this issue. To address this problem, [[65](#bib.bibx65)] improved the model by
    adding skip architecture (SA), fully connected conditional random fields and partially
    connected conditional random fields. They fine-tuned AlexNet, VGGNet, GoogLeNet,
    and ResNet based FCN. They then compared the performance of different FCNs and
    Object-based image analysis (OBIA) method. Experimental results reported that
    the VGGNet-based FCN with proposed improvements achieved the highest accuracy.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[[20](#bib.bibx20)] modified the original U-Net architecture for pixel-level
    classification of crop plants and weeds. They added a convolutional layer with
    a kernel size of 1$\times$1\. For that change, they adjusted the input size of
    the network. Besides, replacing the ReLU activation functions with the Exponential
    Linear Unit (ELU), they used adadelta optimiser algorithm instead of the stochastic
    gradient descent and included dropout layers in between convolutional layers.
    [[122](#bib.bibx122)] also modified the U-Net model to detect one species of weed
    in grasslands.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Graph Convolutional Network (GCN)
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[[61](#bib.bibx61)] proposed Graph Weeds Net (GWN). GWN is a graph-based deep
    learning architecture to classify weed species. [[61](#bib.bibx61)] used ResNet-50
    and DenseNet-202 model to learn vertex features with graph convolution layers,
    vertex-wise dense layers, and the multi-level graph pooling mechanisms included
    in GWN architecture. Here, an RGB image was represented as a multi-scale graph.
    The graph-based model with DenseNet-202 architecture achieved the classification
    accuracy of 98.1%.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[[71](#bib.bibx71)] proposed ResNet-101 based graph convolutional network in
    their research. They chose GCN, because it was a semi-supervised learning approach.
    Moreover, the feature relationships were captured using a graph structure. In
    this model, the label information was shared by neighbouring vertices of the graph,
    which make the learning more accurate with limited annotated data. They compared
    the proposed model with AlexNet, VGG-16, and ResNet-101 architecture on four different
    datasets. The GCN approach achieved 97.80%, 99.37%, 98.93% and 96.51% classification
    accuracy for each dataset.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 10.5 Hybrid Networks (HN)
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hybrid architectures are those where the researchers combine the characteristics
    of two or more DL models. For instance, [[26](#bib.bibx26)] proposed the AgroAVNET
    model, which was a hybrid of AlexNet and VGGNet architecture. They chose VGGNet
    for setting the depth of filters and used the normalisation concept of AlexNet.
    They then compared the performance of the AgroAVNET network with the original
    AlexNet and VGGNet and their different variants. All the parameters were initialised
    using pre-trained weights except for the third layer of the fully connected layers.
    They initialised that randomly. The AgroAVNET model outperformed others with a
    classification accuracy of 98.21%. However, [[43](#bib.bibx43)] adopted the feature
    concatenation approach in their research. They combined a super pixel-based LBP
    (SPLBP) method to extract local texture features, CNN for learning the spatial
    features and SVM for classification. They compared their proposed FCN-SPLBP model
    with CNN, LBP, FCN, and SPLBP architectures.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[[148](#bib.bibx148)] proposed OverFeat-GoogLeNet architecture by combining
    the features from LSTM and GoogLeNet model. The model was used to develop a “Parallelised
    Weed Detection System” by [[160](#bib.bibx160)]. They claimed that this system
    was robust, scalable and could be applied for real-time weed detection. The classification
    accuracy of the system was 91.1%.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[[82](#bib.bibx82)] fine-tuned AlexNet, VGG-F, VGG-16, Inception-v1, ResNet-50,
    and ResNet-101 model to extract features from the images. They replaced the CNNs’
    default classifiers with linear classifiers, i.e., SVM and logistic regression.
    They compared the performance of various SVM and logistic regression classifiers
    by combining them with CNN models for detecting weeds. They achieved the most
    balanced result in terms of accuracy and false positive rate by using “L2-regularised
    with L2-loss logistic regression model using primal computation” classifier. This
    classifier performed better while being used with GoogLeNet architecture for detecting
    weeds in grasslands [[81](#bib.bibx81)].'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[[40](#bib.bibx40)] also replaced CNN’s default classifier with traditional
    ML classifiers including SVM, XGBoost, and Logistic Regression. They initialised
    Xception, Inception-ResNet, VGGNets, MobileNet, and DenseNet model with pre-trained
    weights. The experimental result showed that the best performing network was DenseNet
    model with the SVM classifier. The micro F1 score for the architecture was 99.29%.
    This research also reported that with a small dataset, network performance could
    be enhanced using this approach.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[[2](#bib.bibx2)] proposed a fully convolutional encoder-decoder network named
    as Enhanced Skip Network. The model had multiple VGGNet-like blocks in the encoder
    and decoder. However, the decoder part had fewer future maps to reduce the computational
    complexity and memory requirement. Besides, the skip layers, larger convolutional
    kernels and a multi-scale filter bank were incorporated in the proposed model.
    The weights were initialised using the transfer learning method. The model performed
    better than U-Net, FCN8, and DeepLab-v3, Faster R-CNN, and EDNet in identifying
    weeds in the paddy field. [[29](#bib.bibx29)] combined U-Net architecture, MobileNet-v2
    and DenseNet architectures and replaced transposed convolution layers with activation
    map scaling.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: 11 Performance Evaluation Metrics
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, evaluation metrics is the measurement tool to quantify the performance
    of a classifier. Different metrics are used to evaluate various characteristics
    of a classifier [[53](#bib.bibx53)]. The evaluation metrics can be used either
    to measure the quality of a classification model [[53](#bib.bibx53)] or to compare
    the performance of the different trained models for selecting the best one [[115](#bib.bibx115)].
    Various metrics were used in related studies based on the research need. The most
    commonly used metric is classification accuracy (CA) to evaluate the DL model.
    Many of the authors used multiple metrics to assess the model before drawing any
    conclusion. Table [6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A
    Survey of Deep Learning Techniques for Weed Detection from Images") lists the
    evaluation metrics applied in the relevant studies.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: As Table [6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") shows, it is not
    easy to compare the related works as different types of evaluation metrics are
    employed depending on the DL model, the goal of classification, dataset and detection
    approach. However, the most frequently used evaluation metrics are CA, F1 score
    and mIoU. In the case of classifying plant species, researchers prefer to use
    confusion metrics to evaluate the model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: The evaluation metrics applied by different researchers of the related
    works'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '| No. | Performance Metric | Meaning |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| 1. | Classification Accuracy (CA) | The percentage of correct prediction
    among the input. A model is judged based on how high the value is |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| 2. | True Positive (TP) | How many times the model correctly predict the
    actual classes of the object. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| 3. | False Positive Rate (FPR) | It is the proportion of negative cases incorrectly
    identified as positive cases in the data. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| 4. | False Negative Rate (FNR) | The ratio of positive samples that were
    incorrectly classified. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| 5. | Specificity (S) | The fraction of True Negative from the sum of False
    Positive and True Negative. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| 6. | Mean Pixel Accuracy (MPA) | It is the average of ration of the correctly
    classified pixels among all pixels of the images in the dataset. It is used to
    evaluate the model for semantic segmentation. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| 7. | Precision (P) | The fraction of correct prediction (True Positive) from
    the total number of relevant result (Sum of True Positive and False Positive).
    It helps when the value of False Positives are high. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| 8. | Mean Average Precision (mAP) | It is the mean of average precision over
    all the classes of an object in the data. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| 9. | Recall | The fraction of True Positive from the sum of True Positive
    and False Negative. It helps when the value of False Negatives are high. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| 10. | F1 Score (F1) | The harmonic mean of precision and recall. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| 11. | Confusion Matrix (CM) | It is the summary of the number of correct
    and incorrect prediction made by a model. It helps to visualise not only the errors
    made by the model but also the types of error in predicting the class of object.
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| 12. | Intersection over Union (IoU) | It is the ratio of the overlapping
    area of ground truth (the hand labelled bounding boxes from the testing dataset)
    and predicted area (predicted bounding boxes from the model) to the total area.
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| 13. | Mean Intersection over Union (mIoU) | It is average IoU over all the
    classes of an object in the dataset. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| 14. | Frequency Weighted Intersection over Union (FWIoU) | It is the weighted
    average of IoUs based on pixel classes. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| 15. | Mean Square Error (MSE) | It is the mean of all the squared errors
    between the predicted and actual target class |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| 16. | Root Mean Square Error (RMSE) | It is the standard deviation of the
    difference between the predicted value and observed values. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| 17. | Mean Absolute Error (MAE) | It is the mean of the absolute values of
    each prediction error on all instances of the test dataset. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| 18. | R2 | It is the squared correlation between the observed and the predicted
    outcome by the model. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| 19. | K-fold Cross Validation | The dataset is divided into K number of parts
    and each of the parts is used as testing dataset. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| 20. | Receiver Operating Characteristic (ROC) curve | The true positive rate
    is plotted in function of the false positive rate for different cut-off points
    of a parameter. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| 21. | Kappa Coefficient | Measures the degree of agreement between the true
    values and the predicted values |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| 22. | Matthews correlation coefficient (MCC) | A correlation coefficient
    between the observed and predicted binary classifications |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| 23. | Dice Similarity Coefficient (DSC) | It is a measure of spatial overlap
    between two sets of pixels. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: In addition to the evaluation metrics provided in Table [6](#S11.T6 "Table 6
    ‣ 11 Performance Evaluation Metrics ‣ A Survey of Deep Learning Techniques for
    Weed Detection from Images"), [[108](#bib.bibx108)] justified their model based
    on run-time. This was because, to develop a real-time weeds and crop plants classifier,
    it is important to identify the class of a plant as quickly as possible. They
    showed how quickly their model could detect a plant in an image. Similarly, [[151](#bib.bibx151)]
    calculated the classification accuracy of their model along with the time required
    to train and identify classes of plants, as they intended to develop a real-time
    classifier. [[104](#bib.bibx104)] also used run-time for justifying the model
    performance. They found that, by increasing the patch size of the input images,
    it was possible to reduce the time required to train the model. Another research
    method used inference time to compare different DL architecture [[63](#bib.bibx63)].
    [[34](#bib.bibx34)] evaluated the CNN model not only based on time but also in
    terms of the memory consumed by the model during training. They argued that though
    the CNN architecture achieved higher accuracy than other machine learning model,
    it required more time and memory to train the model. [[7](#bib.bibx7)] showed
    that reducing the number of layers of the DL model could make it faster in detecting
    and identifying the crop and weed plants. They also used processing time as an
    evaluation criterion while choosing the CNN architecture.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 12 Discussion
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is evident that the DL model offers high performance in the area of weed
    detection and classification in crops. In this paper, we have provided an overview
    of the current status of the area of the automatic weed detection technique. In
    most relevant studies, the preferred method to acquire data was using a digital
    camera mounted on a ground vehicle to collect RGB images. A few research studies
    collected multi-spectral or hyper-spectral data. To prepare the dataset for training,
    different image processing techniques were used to resize the images, background
    and noise removing and image enhancement. The datasets were generally annotated
    using bounding boxes, pixel-wise and image level annotation approaches. For training
    the model, supervised learning approaches are applied by the researchers. They
    employ different DL techniques to find a better weed detection model. Detection
    accuracy is given as the most important parameter to evaluate the performance
    of the model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, there is still room for improvements in this area. Use of emerging
    technologies can help to improve the accuracy and speed of automatic weed detection
    systems. As crop and weed plants have many similarities, the use of other spectral
    indices can improve the performance.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a lack of large datasets for crops and weeds. It is necessary
    to construct a large benchmark dataset by capturing a variety of crops/weeds from
    different geographical locations, weather conditions and at various growth stages
    of crops and weeds. At the same time, it will be expensive to annotate these large
    datasets. Semi-supervised [[25](#bib.bibx25), [174](#bib.bibx174)] or weakly supervised
    [[176](#bib.bibx176), [36](#bib.bibx36)] approaches could be employed to address
    this problem.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, Generative Adversarial Network (GAN) [[93](#bib.bibx93)] or other
    synthetic data generation techniques can contribute to creating a large dataset.
    Random point generation and polygon labelling can further improve the precision
    of automatic weed detection systems. DL is evolving very fast, and new state-of-art
    techniques are being proposed. In addition to developing new solutions, researchers
    can enhance and apply those methods in the area of weed detection. They can also
    consider using weakly supervised, self-supervised or unsupervised approaches like
    multiple instance learning, few-shot or zero-shot learning as a means for synthetic
    data generation.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, most datasets mentioned in this paper exhibit class imbalance,
    which may create biases and lead to over-fitting of the model. Future research
    needs to address the problem. This can be achieved via the use of appropriate
    data redistribution approaches, cost-sensitive learning approaches [[76](#bib.bibx76)],
    or class balancing classifiers [[153](#bib.bibx153), [15](#bib.bibx15)].
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: To summarise, the primary objective of developing automatic weed detection system
    is to provide a weed management technique that will minimise cost and maximise
    crop yields. To do so, researchers need to come up with a system that can be deployed
    on devices with a lower computational requirement and can detect weeds accurately
    in real-time.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 13 Conclusion
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This study provides a comprehensive survey of the deep learning-based research
    in detecting and classifying weed species in value crops. A total of 70 relevant
    papers have been examined based on data acquisition, dataset preparation, detection
    and classification methods and model evaluation process. Publicly available datasets
    in the related field are also highlighted for prospective researchers. In this
    article, we provide a taxonomy of the research studies in this area and summarise
    the approaches of detecting weeds (Table LABEL:tab:different_DL_approach). It
    was found that most of the studies applied supervised learning techniques using
    state-of-art deep learning models and they can achieve better performance and
    classification accuracy by fine-tuning pre-trained models on any plant dataset.
    The results also show that the experiments already have achieved very high accuracy
    when a sufficient amount of labelled data of each class is available for training
    the models. However, the existing research only achieved high accuracy in a limited
    experiment setup, e.g., on small datasets of a select number of crops and weeds
    species. Computational speed in the recognition process is another limiting factor
    for deployment on real-time fast-moving herbicide spraying vehicles. An important
    future direction would be to investigate highly efficient detection techniques
    using very large datasets with a variety of crop and weed species so that one
    single model can be used across any weed-crop setting as needed. Other potential
    future research directions include the need for large generalised datasets, tailored
    machine learning models in weed-crop settings, addressing the class imbalance
    problems, identifying the growth stage of the weeds, as well as thorough field
    trials for commercial deployments.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Alwaseela Abdalla et al. “Fine-tuning convolutional neural network with
    transfer learning for semantic segmentation of ground-level oilseed rape images
    in a field with high weed pressure” In *Computers and Electronics in Agriculture*
    167 Elsevier, 2019, pp. 105091'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Shyam Prasad Adhikari, Heechan Yang and Hyongsuk Kim “Learning semantic
    graphics using convolutional encoder-decoder network for autonomous weeding in
    paddy field” In *Frontiers in plant science* 10 Frontiers, 2019, pp. 1404'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Jamil Ahmad et al. “Visual features based boosted classification of weeds
    for real-time selective herbicide sprayer systems” In *Computers in Industry*
    98 Elsevier, 2018, pp. 23–33'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Mansoor Alam et al. “Real-Time Machine-Learning Based Crop/Weed Detection
    and Classification for Variable-Rate Spraying in Precision Agriculture” In *2020
    7th International Conference on Electrical and Electronics Engineering (ICEEE)*,
    2020, pp. 273–280 IEEE'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Md Zahangir Alom et al. “A state-of-the-art survey on deep learning theory
    and architectures” In *Electronics* 8.3 Multidisciplinary Digital Publishing Institute,
    2019, pp. 292'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Sandra Amend et al. “Weed Management of the Future” In *KI-Künstliche Intelligenz*
    33.4 Springer, 2019, pp. 411–415'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Córdova-Cruzatty Andrea, Barreno Barreno Mauricio Daniel and Jácome Barrionuevo
    José Misael “Precise weed and maize classification through convolutional neuronal
    networks” In *2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)*, 2017,
    pp. 1–6 IEEE'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Paolo Andreini et al. “Image generation by gan and style transfer for agar
    plate image segmentation” In *Computer Methods and Programs in Biomedicine* 184
    Elsevier, 2020, pp. 105268'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Muhammad Hamza Asad and Abdul Bais “Weed detection in canola fields using
    maximum likelihood classification and deep convolutional neural network” In *Information
    Processing in Agriculture*, 2019 DOI: [10.1016/j.inpa.2019.12.002](https://dx.doi.org/10.1016/j.inpa.2019.12.002)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla “Segnet: A deep
    convolutional encoder-decoder architecture for image segmentation” In *IEEE transactions
    on pattern analysis and machine intelligence* 39.12 IEEE, 2017, pp. 2481–2495'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] M Dian Bah, Adel Hafiane and Raphael Canals “Deep learning with unsupervised
    data labeling for weed detection in line crops in UAV images” In *Remote sensing*
    10.11 Multidisciplinary Digital Publishing Institute, 2018, pp. 1690'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Adel Bakhshipour and Abdolabbas Jafari “Evaluation of support vector machine
    and artificial neural networks in weed detection using shape features” In *Computers
    and Electronics in Agriculture* 145 Elsevier, 2018, pp. 153–160'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Adel Bakhshipour, Abdolabbas Jafari, Seyed Mehdi Nassiri and Dariush Zare
    “Weed segmentation using texture features extracted from wavelet sub-images” In
    *Biosystems Engineering* 157 Elsevier, 2017, pp. 1–12'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Horace B Barlow “Unsupervised learning” In *Neural computation* 1.3 MIT
    Press, 1989, pp. 295–311'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Jingjun Bi and Chongsheng Zhang “An empirical comparison on state-of-the-art
    multi-class imbalance learning algorithms and a new diversified ensemble learning
    scheme” In *Knowledge-Based Systems* 158 Elsevier, 2018, pp. 81–93'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Abdel-Aziz Binguitcha-Fare and Prince Sharma “Crops and weeds classification
    using Convolutional Neural Networks via optimization of transfer learning parameters”
    In *International Journal of Engineering and Advanced Technology (IJEAT)* 8.5
    Blue Eyes Intelligence Engineering & Sciences Publication, 2019, pp. 2284–2294'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] D Bini, D Pamela and Shajin Prince “Machine Vision and Machine Learning
    for Intelligent Agrobots: A review” In *2020 5th International Conference on Devices,
    Circuits and Systems (ICDCS)*, 2020, pp. 12–16 IEEE'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Petra Bosilj, Erchan Aptoula, Tom Duckett and Grzegorz Cielniak “Transfer
    learning between crop types for semantic segmentation of crops versus weeds in
    precision agriculture” In *Journal of Field Robotics* 37.1 Wiley Online Library,
    2020, pp. 7–19'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] E. Brasseur In *ericbrasseur.org* URL: [http://www.ericbrasseur.org/gamma.html?i=1](http://www.ericbrasseur.org/gamma.html?i=1)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Anderson Brilhador et al. “Classification of Weeds and Crops at the Pixel-Level
    Using Convolutional Neural Networks and Data Augmentation” In *2019 IEEE Latin
    American Conference on Computational Intelligence (LA-CCI)*, 2019, pp. 1–6 IEEE'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Ralph B Brown and Scott D Noble “Site-specific weed management: sensing
    requirements—what do we need to see?” In *Weed Science* 53.2 BioOne, 2005, pp.
    252–258'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Rich Caruana and Alexandru Niculescu-Mizil “An empirical comparison of
    supervised learning algorithms” In *Proceedings of the 23rd international conference
    on Machine learning*, 2006, pp. 161–168'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Paulo César Pereira Júnior et al. “Comparison of Supervised Classifiers
    and Image Features for Crop Rows Segmentation on Aerial Images” In *Applied Artificial
    Intelligence* 34.4 Taylor & Francis, 2020, pp. 271–291'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] N Zhang C Chaisattapagon “Effective criteria for weed identification in
    wheat fields using machine vision” In *Transactions of the ASAE* 38.3 American
    Society of AgriculturalBiological Engineers, 1995, pp. 965–974'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Olivier Chapelle, Bernhard Scholkopf and Alexander Zien “Semi-supervised
    learning (chapelle, o. et al., eds.; 2006)[book reviews]” In *IEEE Transactions
    on Neural Networks* 20.3 IEEE, 2009, pp. 542–542'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Trupti R Chavan and Abhijeet V Nandedkar “AgroAVNET for crops and weeds
    classification: A step forward in automatic farming” In *Computers and Electronics
    in Agriculture* 154 Elsevier, 2018, pp. 361–372'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Nived Chebrolu, Thomas Läbe and Cyrill Stachniss “Robust long-term registration
    of UAV images of crop fields for precision agriculture” In *IEEE Robotics and
    Automation Letters* 3.4 IEEE, 2018, pp. 3097–3104'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Nived Chebrolu et al. “Agricultural robot dataset for plant classification,
    localization and mapping on sugar beet fields” In *The International Journal of
    Robotics Research* 36.10 SAGE Publications Sage UK: London, England, 2017, pp.
    1045–1052'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Lukasz Chechlinski, Barbara Siemikatkowska and Michal Majewski “A System
    for Weeds and Crops Identification—Reaching over 10 FPS on Raspberry Pi with the
    Usage of MobileNets, DenseNet and Custom Modifications” In *Sensors* 19.17 Multidisciplinary
    Digital Publishing Institute, 2019, pp. 3787'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Liang-Chieh Chen, George Papandreou, Florian Schroff and Hartwig Adam
    “Rethinking atrous convolution for semantic image segmentation” In *arXiv preprint
    arXiv:1706.05587*, 2017'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Vitali Czymmek, Leif O Harders, Florian J Knoll and Stephan Hussmann “Vision-Based
    Deep Learning Approach for Real-Time Detection of Weeds in Organic Farming” In
    *2019 IEEE International Instrumentation and Measurement Technology Conference
    (I2MTC)*, 2019, pp. 1–5 IEEE'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Jia Deng et al. “Imagenet: A large-scale hierarchical image database”
    In *2009 IEEE conference on computer vision and pattern recognition*, 2009, pp.
    248–255 Ieee'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti and Alberto Pretto “Automatic
    model based dataset generation for fast and accurate crop and weeds detection”
    In *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2017, pp. 5188–5195 IEEE'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Alessandro Santos Ferreira et al. “Weed detection in soybean crops using
    ConvNets” In *Computers and Electronics in Agriculture* 143 Elsevier, 2017, pp.
    314–324'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Alessandro Santos Ferreira et al. “Unsupervised deep learning and semi-automatic
    data labeling in weed discrimination” In *Computers and Electronics in Agriculture*
    165 Elsevier, 2019, pp. 104963'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Thibaut Durand, Taylor Mordan, Nicolas Thome and Matthieu Cord “Wildcat:
    Weakly supervised learning of deep convnets for image classification, pointwise
    localization and segmentation” In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 642–651'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Mads Dyrmann, Rasmus Nyholm Jørgensen and Henrik Skov Midtiby “RoboWeedSupport-Detection
    of weed locations in leaf occluded cereal crops using a fully convolutional neural
    network” In *Adv. Anim. Biosci* 8.2, 2017, pp. 842–847'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Mads Dyrmann, Henrik Karstoft and Henrik Skov Midtiby “Plant species classification
    using deep convolutional neural network” In *Biosystems Engineering* 151 Elsevier,
    2016, pp. 72–80'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Max Ehrlich and Larry S Davis “Deep residual learning in the jpeg transform
    domain” In *Proceedings of the IEEE International Conference on Computer Vision*,
    2019, pp. 3484–3493'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Borja Espejo-Garcia et al. “Towards weeds identification assistance through
    transfer learning” In *Computers and Electronics in Agriculture* 171 Elsevier,
    2020, pp. 105306'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Adnan Farooq, Jiankun Hu and Xiuping Jia “Analysis of spectral bands and
    spatial resolutions for weed classification via deep convolutional neural network”
    In *IEEE Geoscience and Remote Sensing Letters* 16.2 IEEE, 2018, pp. 183–187'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Adnan Farooq, Jiankun Hu and Xiuping Jia “Weed classification in hyperspectral
    remote sensing images via deep convolutional neural network” In *IGARSS 2018-2018
    IEEE International Geoscience and Remote Sensing Symposium*, 2018, pp. 3816–3819
    IEEE'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Adnan Farooq, Xiuping Jia, Jiankun Hu and Jun Zhou “Multi-resolution weed
    classification via convolutional neural network and superpixel based local binary
    pattern using remote sensing images” In *Remote Sensing* 11.14 Multidisciplinary
    Digital Publishing Institute, 2019, pp. 1692'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Mulham Fawakherji et al. “Crop and weeds classification for precision
    agriculture using context-independent pixel-wise segmentation” In *2019 Third
    IEEE International Conference on Robotic Computing (IRC)*, 2019, pp. 146–152 IEEE'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] C Fernández-Quintanilla et al. “Is the current state of the art of weed
    monitoring suitable for site-specific weed management in arable crops?” In *Weed
    research* 58.4 Wiley Online Library, 2018, pp. 259–272'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] AJ Gabor, RR Leach and FU Dowla “Automated seizure detection using a self-organizing
    neural network” In *Electroencephalography and clinical Neurophysiology* 99.3
    Elsevier, 1996, pp. 257–266'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Junfeng Gao et al. “Deep convolutional neural networks for image-based
    Convolvulus sepium detection in sugar beet fields” In *Plant Methods* 16.1 BioMed
    Central, 2020, pp. 1–12'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Andreas Geiger, Philip Lenz, Christoph Stiller and Raquel Urtasun “Vision
    meets robotics: The kitti dataset” In *The International Journal of Robotics Research*
    32.11 Sage Publications Sage UK: London, England, 2013, pp. 1231–1237'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Thomas Mosgaard Giselsson et al. “A public image database for benchmark
    of plant seedling classification algorithms” In *arXiv preprint arXiv:1711.05458*,
    2017'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Jiuxiang Gu et al. “Recent advances in convolutional neural networks”
    In *Pattern Recognition* 77 Elsevier, 2018, pp. 354–377'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] David Hall, Feras Dayoub, Tristan Perez and Chris Mccool “A rapidly deployable
    classification system using visual data for the application of precision weed
    management” In *Computers and Electronics in Agriculture* 148, 2018, pp. 107–120
    DOI: [10.1016/j.compag.2018.02.023](https://dx.doi.org/10.1016/j.compag.2018.02.023)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Esmael Hamuda, Brian Mc Ginley, Martin Glavin and Edward Jones “Automatic
    crop detection under field conditions using the HSV colour space and morphological
    operations” In *Computers and electronics in agriculture* 133 Elsevier, 2017,
    pp. 97–107'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] David J Hand “Measuring classifier performance: a coherent alternative
    to the area under the ROC curve” In *Machine learning* 77.1 Springer, 2009, pp.
    103–123'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Sebastian Haug and Jörn Ostermann “A crop/weed field image dataset for
    the evaluation of computer vision based precision agriculture tasks” In *European
    Conference on Computer Vision*, 2014, pp. 105–116 Springer'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Sebastian Haug and Jörn Ostermann “A Crop/Weed Field Image Dataset for
    the Evaluation of Computer Vision Based Precision Agriculture Tasks” In *Computer
    Vision - ECCV 2014 Workshops*, 2015, pp. 105–116 DOI: [10.1007/978-3-319-16220-1˙8](https://dx.doi.org/10.1007/978-3-319-16220-1_8)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun “Deep residual learning
    for image recognition” In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2016, pp. 770–778'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J Hemming and T Rath “Image processing for plant determination using the
    Hough transform and clustering methods” In *Gartenbauwissenschaft* 67.1 Stuttgart:
    Eugen Ulmer GmbH & Co., 1928-2002., 2002, pp. 1–10'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Geoffrey E Hinton, Simon Osindero and Yee-Whye Teh “A fast learning algorithm
    for deep belief nets” In *Neural computation* 18.7 MIT Press, 2006, pp. 1527–1554'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Jodie S Holt “Principles of Weed Management in Agroecosystems and Wildlands1”
    In *Weed Technology* 18.sp1 BioOne, 2004, pp. 1559–1562'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Mohammad-Parsa Hosseini et al. “Deep learning architectures” In *Deep
    learning: concepts and architectures* Springer, 2020, pp. 1–24'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Kun Hu et al. “Graph weeds net: A graph-based deep learning method for
    weed recognition” In *Computers and Electronics in Agriculture* 174 Elsevier,
    2020, pp. 105520'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Huasheng Huang et al. “Accurate weed mapping and prescription map generation
    based on fully convolutional networks using UAV imagery” In *Sensors* 18.10 Multidisciplinary
    Digital Publishing Institute, 2018, pp. 3299'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Huasheng Huang et al. “A fully convolutional network for weed mapping
    of unmanned aerial vehicle (UAV) imagery” In *PloS one* 13.4 Public Library of
    Science San Francisco, CA USA, 2018, pp. e0196302'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Huasheng Huang et al. “A semantic labeling approach for accurate weed
    mapping of high resolution UAV imagery” In *Sensors* 18.7 Multidisciplinary Digital
    Publishing Institute, 2018, pp. 2113'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Huasheng Huang et al. “Deep learning versus Object-based Image Analysis
    (OBIA) in weed mapping of UAV imagery” In *International Journal of Remote Sensing*
    41.9 Taylor & Francis, 2020, pp. 3446–3479'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Sheng-Wei Huang et al. “Auggan: Cross domain adaptation with gan-based
    data augmentation” In *Proceedings of the European Conference on Computer Vision
    (ECCV)*, 2018, pp. 718–731'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Nadeem Iqbal, Sudheesh Manalil, Bhagirath S Chauhan and Steve W Adkins
    “Investigation of alternate herbicides for effective weed management in glyphosate-tolerant
    cotton” In *Archives of Agronomy and Soil Science* 65.13 Taylor & Francis, 2019,
    pp. 1885–1899'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Asnor Juraiza Ishak, Siti Salasiah Mokri, Mohd Marzuki Mustafa and Aini
    Hussain “Weed detection utilizing quadratic polynomial and ROI techniques” In
    *2007 5th Student Conference on Research and Development*, 2007, pp. 1–5 IEEE'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Abdolabbas Jafari, Seyed Saeid Mohtasebi, H Eghbali Jahromi and Mahmoud
    Omid “Weed detection in sugar beet fields using machine vision” In *Int. J. Agric.
    Biol* 8.5, 2006, pp. 602–605'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Troy Arnold Jensen, Bruen Smith and Livia Faria Defeo “An automated site-specific
    fallow weed management system using unmanned aerial vehicles”, 2020'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Honghua Jiang et al. “CNN feature based graph convolutional network for
    weed and crop recognition in smart farming” In *Computers and Electronics in Agriculture*
    174 Elsevier, 2020, pp. 105450'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Yu Jiang, Changying Li, Andrew H Paterson and Jon S Robertson “DeepSeedling:
    deep convolutional network and Kalman filter for plant seedling detection and
    counting in the field” In *Plant methods* 15.1 Springer, 2019, pp. 141'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Andreas Kamilaris and Francesc X Prenafeta-Boldú “Deep learning in agriculture:
    A survey” In *Computers and electronics in agriculture* 147 Elsevier, 2018, pp.
    70–90'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Y Karimi, SO Prasher, RM Patel and SH Kim “Application of support vector
    machine technology for weed and nitrogen stress detection in corn” In *Computers
    and electronics in agriculture* 51.1-2 Elsevier, 2006, pp. 99–109'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Wajahat Kazmi et al. “Detecting creeping thistle in sugar beet fields
    using vegetation indices” In *Computers and Electronics in Agriculture* 112 Elsevier,
    2015, pp. 10–19'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Salman H Khan et al. “Cost-sensitive learning of deep feature representations
    from imbalanced data” In *IEEE transactions on neural networks and learning systems*
    29.8 IEEE, 2017, pp. 3573–3587'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Mina Khoshdeli, Richard Cong and Bahram Parvin “Detection of nuclei in
    H&E stained sections using convolutional neural networks” In *2017 IEEE EMBS International
    Conference on Biomedical & Health Informatics (BHI)*, 2017, pp. 105–108 IEEE'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Thomas N Kipf and Max Welling “Semi-supervised classification with graph
    convolutional networks” In *arXiv preprint arXiv:1609.02907*, 2016'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Florian J Knoll, Vitali Czymmek, Leif O Harders and Stephan Hussmann “Real-time
    classification of weeds in organic carrot production using deep learning algorithms”
    In *Computers and Electronics in Agriculture* 167 Elsevier, 2019, pp. 105097'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S Kodagoda, Z Zhang, D Ruiz and G Dissanayake “Weed detection and classification
    for autonomous farming” In *Intelligent Production Machines and Systems* I* PROMS,
    2008'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Tsampikos Kounalakis et al. “A robotic system employing deep learning
    for visual recognition and detection of weeds in Grasslands” In *2018 IEEE International
    Conference on Imaging Systems and Techniques (IST)*, 2018, pp. 1–6 IEEE'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Tsampikos Kounalakis, Georgios A Triantafyllidis and Lazaros Nalpantidis
    “Deep learning-based visual recognition of rumex for robotic precision farming”
    In *Computers and Electronics in Agriculture* 165 Elsevier, 2019, pp. 104973'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Alex Krizhevsky, Ilya Sutskever and Geoffrey E Hinton “Imagenet classification
    with deep convolutional neural networks” In *Advances in neural information processing
    systems* 25, 2012, pp. 1097–1105'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Harshit Kumar “Data augmentation Techniques” In *OpenGenus IQ: Learn Computer
    Science* OpenGenus IQ: Learn Computer Science, 2019 URL: [https://iq.opengenus.org/data-augmentation/](https://iq.opengenus.org/data-augmentation/)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Rattan Lal “Soil structure and sustainability” In *Journal of sustainable
    agriculture* 1.4 Taylor & Francis, 1991, pp. 67–92'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Olee Hoi Ying Lam et al. “An open source workflow for weed mapping in
    native grassland using unmanned aerial vehicle: using Rumex obtusifolius as a
    case study” In *European Journal of Remote Sensing* Taylor & Francis, 2020, pp.
    1–18'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Petre Lameski, Eftim Zdravevski and Andrea Kulakov “Review of automated
    weed control approaches: an environmental impact perspective” In *International
    Conference on Telecommunications*, 2018, pp. 132–147 Springer'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Petre Lameski, Eftim Zdravevski, Vladimir Trajkovik and Andrea Kulakov
    “Weed detection dataset with RGB images taken under variable light conditions”
    In *International Conference on ICT Innovations*, 2017, pp. 112–119 Springer'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Vi Nguyen Thanh Le, Selam Ahderom and Kamal Alameh “Performances of the
    LBP Based Algorithm over CNN Models for Detecting Crops and Weeds with Similar
    Morphologies” In *Sensors* 20.8 Multidisciplinary Digital Publishing Institute,
    2020, pp. 2193'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Vi Nguyen Thanh Le, Selam Ahderom, Beniamin Apopei and Kamal Alameh “A
    novel method for detecting morphologically similar crops and weeds based on the
    combination of contour masks and filtered Local Binary Pattern operators” In *GigaScience*
    9.3 Oxford University Press, 2020, pp. giaa017'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Yann LeCun, Yoshua Bengio and Geoffrey Hinton “Deep learning” In *nature*
    521.7553 Nature Publishing Group, 2015, pp. 436–444'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Yann LeCun et al. “Backpropagation applied to handwritten zip code recognition”
    In *Neural computation* 1.4 MIT Press, 1989, pp. 541–551'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Christian Ledig et al. “Photo-realistic single image super-resolution
    using a generative adversarial network” In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 4681–4690'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Dong-Hyun Lee “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks” In *Workshop on challenges in representation
    learning, ICML* 3.2, 2013'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Simon Leminen Madsen et al. “Open Plant Phenotype Database of Common Weeds
    in Denmark” In *Remote Sensing* 12.8 Multidisciplinary Digital Publishing Institute,
    2020, pp. 1246'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Konstantinos G Liakos et al. “Machine learning in agriculture: A review”
    In *Sensors* 18.8 Multidisciplinary Digital Publishing Institute, 2018, pp. 2674'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Wei-Che Liang, You-Jei Yang and Chih-Min Chao “Low-Cost Weed Identification
    System Using Drones” In *2019 Seventh International Symposium on Computing and
    Networking Workshops (CANDARW)*, 2019, pp. 260–263 IEEE'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Tsung-Yi Lin et al. “Microsoft coco: Common objects in context” In *European
    conference on computer vision*, 2014, pp. 740–755 Springer'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Bo Liu and Ryan Bruch “Weed Detection for Selective Spraying: a Review”
    In *Current Robotics Reports* 1.1 Springer, 2020, pp. 19–26'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Wei Liu et al. “Ssd: Single shot multibox detector” In *European conference
    on computer vision*, 2016, pp. 21–37 Springer'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Philipp Lottes et al. “Joint stem detection and crop-weed classification
    for plant-specific treatment in precision farming” In *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2018, pp. 8233–8238 IEEE'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Philipp Lottes et al. “Robust joint stem detection and crop-weed classification
    using image sequences for plant-specific treatment in precision farming” In *Journal
    of Field Robotics* 37.1 Wiley Online Library, 2020, pp. 20–34'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Philipp Lottes, Jens Behley, Andres Milioto and Cyrill Stachniss “Fully
    convolutional networks with sequential information for robust crop and weed detection
    in precision farming” In *IEEE Robotics and Automation Letters* 3.4 IEEE, 2018,
    pp. 2870–2877'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Xu Ma et al. “Fully convolutional network for rice seedling and weed
    image segmentation at the seedling stage in paddy fields” In *PloS one* 14.4 Public
    Library of Science San Francisco, CA USA, 2019, pp. e0215676'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Inneke Mayachita “Understanding Graph Convolutional Networks for Node
    Classification” In *Medium* Towards Data Science, 2020 URL: [https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] C.. Merfield “Robotic weeding’s false dawn? Ten requirements for fully
    autonomous mechanical weed management” In *Weed Research* 56.5, 2016, pp. 340–344
    DOI: [10.1111/wre.12217](https://dx.doi.org/10.1111/wre.12217)'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] GE Meyer et al. “Textural imaging and discriminant analysis for distinguishingweeds
    for spot spraying” In *Transactions of the ASAE* 41.4 American Society of AgriculturalBiological
    Engineers, 1998, pp. 1189'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Andres Milioto, Philipp Lottes and Cyrill Stachniss “Real-time blob-wise
    sugar beets vs weeds classification for monitoring fields using convolutional
    neural networks” In *ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
    Information Sciences* 4 Copernicus GmbH, 2017, pp. 41'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Syed I Moazzam et al. “A Review of Application of Deep Learning for Weeds
    and Crops Classification in Agriculture” In *2019 International Conference on
    Robotics and Automation in Industry (ICRAI)*, 2019, pp. 1–6 IEEE'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Maryam M Najafabadi et al. “Deep learning applications and challenges
    in big data analytics” In *Journal of Big Data* 2.1 Springer, 2015, pp. 1'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Daniel K Nkemelu, Daniel Omeiza and Nancy Lubalo “Deep convolutional
    neural network for plant seedlings classification” In *arXiv preprint arXiv:1811.08404*,
    2018'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] K. Okese, Theresah Kankam, Jennifer Boamah and Owusu Mensah Evans “Basic
    Principles of Weeds Control and Management” In *MyFarm Blog*, 2020 URL: [https://blog.agrihomegh.com/principles-weeds-control-management/](https://blog.agrihomegh.com/principles-weeds-control-management/)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Alex Olsen et al. “DeepWeeds: A multiclass weed species image dataset
    for deep learning” In *Scientific reports* 9.1 Nature Publishing Group, 2019,
    pp. 1–12'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Kavir Osorio et al. “A Deep Learning Approach for Weed Detection in Lettuce
    Crops Using Multispectral Images” In *AgriEngineering* 2.3 Multidisciplinary Digital
    Publishing Institute, 2020, pp. 471–488'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Akin Ozcift and Arif Gulten “Classifier ensemble construction with rotation
    forest to improve medical diagnosis performance of machine learning algorithms”
    In *Computer methods and programs in biomedicine* 104.3 Elsevier, 2011, pp. 443–451'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Victor Partel, Sri Charan Kakarla and Yiannis Ampatzidis “Development
    and evaluation of a low-cost and smart technology for precision weed management
    utilizing artificial intelligence” In *Computers and electronics in agriculture*
    157 Elsevier, 2019, pp. 339–350'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Victor Partel et al. “Smart Sprayer for Precision Weed Control Using
    Artificial Intelligence: Comparison of Deep Learning Frameworks” In *Association
    for the Advancement of Artificial Intelligence*, 2019'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] DD Patel and BA Kumbhar “Weed and its management: A major threats to
    crop economy” In *J. Pharm. Sci. Bioscientific Res* 6.6, 2016, pp. 453–758'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Sanjay Patidar, Utkarsh Singh and Sumit Kumar Sharma “Weed Seedling Detection
    Using Mask Regional Convolutional Neural Network” In *2020 International Conference
    on Electronics and Sustainable Communication Systems (ICESC)*, 2020, pp. 311–316
    IEEE'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Josh Patterson and Adam Gibson “Deep learning: A practitioner’s approach”
    ” O’Reilly Media, Inc.”, 2017'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Larry Pearlstein, Mun Kim and Warren Seto “Convolutional neural network
    application to plant detection, based on synthetic imagery” In *2016 IEEE Applied
    Imagery Pattern Recognition Workshop (AIPR)*, 2016, pp. 1–4 IEEE'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Lukas Petrich et al. “Detection of Colchicum autumnale in drone images,
    using a machine-learning approach” Springer, 2019'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] PyTorch “AI for AG: Production machine learning for agriculture” In *Medium*
    PyTorch, 2020 URL: [https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Zheng Qin et al. “Thundernet: Towards real-time generic object detection
    on mobile devices” In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 6718–6727'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Thomas Lagkas
    and Ioannis Moscholios “A compilation of UAV applications for precision agriculture”
    In *Computer Networks* 172 Elsevier, 2020, pp. 107148'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Rekha Raja, Thuy T Nguyen, David C Slaughter and Steven A Fennimore “Real-time
    robotic weed knife control system for tomato and lettuce based on geometric appearance
    of plant labels” In *Biosystems Engineering* 194 Elsevier, 2020, pp. 152–164'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] W Ramirez, P Achanccaray, LF Mendoza and MAC Pacheco “Deep Convolutional
    Neural Networks for Weed Detection in Agricultural Crops Using Optical Aerial
    Images” In *2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)*,
    2020, pp. 133–137 IEEE'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Pejman Rasti et al. “Supervised Image Classification by Scattering Transform
    with Application to Weed Detection in Culture Crops of High Density” In *Remote
    Sensing* 11.3, 2019, pp. 249 DOI: [10.3390/rs11030249](https://dx.doi.org/10.3390/rs11030249)'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Joseph Redmon In *Darknet: Open Source Neural Networks in C* URL: [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Joseph Redmon and Ali Farhadi “Yolov3: An incremental improvement” In
    *arXiv preprint arXiv:1804.02767*, 2018'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun “Faster r-cnn: Towards
    real-time object detection with region proposal networks” In *arXiv preprint arXiv:1506.01497*,
    2015'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Yannik Rist, Iurii Shendryk, Foivos Diakogiannis and Shaun Levick “Weed
    Mapping Using Very High Resolution Satellite Imagery and Fully Convolutional Neural
    Network” In *IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing
    Symposium*, 2019, pp. 9784–9787 IEEE'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Olaf Ronneberger, Philipp Fischer and Thomas Brox “U-net: Convolutional
    networks for biomedical image segmentation” In *International Conference on Medical
    image computing and computer-assisted intervention*, 2015, pp. 234–241 Springer'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Inkyu Sa et al. “weednet: Dense semantic weed classification using multispectral
    images and mav for smart farming” In *IEEE Robotics and Automation Letters* 3.1
    IEEE, 2017, pp. 588–595'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Inkyu Sa et al. “Weedmap: a large-scale semantic weed mapping framework
    using aerial multispectral imaging and deep neural network for precision farming”
    In *Remote Sensing* 10.9 Multidisciplinary Digital Publishing Institute, 2018,
    pp. 1423'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Sajad Sabzi, Yousef Abbaspour-Gilandeh and Juan Ignacio Arribas “An automatic
    visible-range video weed detection, segmentation and classification prototype
    in potato field” In *Heliyon* 6.5 Elsevier, 2020, pp. e03685'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Linda Sakyi “Linda Sakyi” In *Greenroot Limited*, 2019 URL: [https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/](https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] T Sarvini et al. “Performance Comparison of Weed Detection Algorithms”
    In *2019 International Conference on Communication and Signal Processing (ICCSP)*,
    2019, pp. 0843–0847 IEEE'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Santhosh K Seelan, Soizik Laguette, Grant M Casady and George A Seielstad
    “Remote sensing applications for precision agriculture: A learning community approach”
    In *Remote sensing of environment* 88.1-2 Elsevier, 2003, pp. 157–169'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Saraswathi Shanmugam et al. “Automated Weed Detection Systems: A Review”
    In *KnE Engineering*, 2020, pp. 271–284'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Detection of Carolina
    geranium (Geranium carolinianum) growing in competition with strawberry using
    convolutional neural networks” In *Weed Science* 67.2 BioOne, 2019, pp. 239–245'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Goosegrass Detection
    in Strawberry and Tomato Using a Convolutional Neural Network” In *Scientific
    Reports* 10.1 Nature Publishing Group, 2020, pp. 1–8'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Evan Shelhamer, Jonathan Long and Trevor Darrell “Fully convolutional
    networks for semantic segmentation” In *IEEE transactions on pattern analysis
    and machine intelligence* 39.4, 2017, pp. 640–651'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Connor Shorten and Taghi M Khoshgoftaar “A survey on image data augmentation
    for deep learning” In *Journal of Big Data* 6.1 Springer, 2019, pp. 60'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Karen Simonyan and Andrew Zisserman “Very deep convolutional networks
    for large-scale image recognition” In *arXiv preprint arXiv:1409.1556*, 2014'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Arun Narenthiran Veeranampalayam Sivakumar et al. “Comparison of Object
    Detection and Patch-Based Classification Deep Learning Models on Mid-to Late-Season
    Weed Detection in UAV Imagery” In *Remote Sensing* 12.13 Multidisciplinary Digital
    Publishing Institute, 2020, pp. 2136'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Soren Skovsen et al. “The GrassClover Image Dataset for Semantic and
    Hierarchical Species Understanding in Agriculture” In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition Workshops*, 2019'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Russell Stewart, Mykhaylo Andriluka and Andrew Y Ng “End-to-end people
    detection in crowded scenes” In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2016, pp. 2325–2333'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Wen-Hao Su “Advanced Machine Learning in Point Spectroscopy, RGB-and
    Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review”
    In *Smart Cities* 3.3 Multidisciplinary Digital Publishing Institute, 2020, pp.
    767–792'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Kaspars Sudars et al. “Dataset of annotated food crops and weed images
    for robotic computer vision control” In *Data in Brief* Elsevier, 2020, pp. 105833'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Hyun K Suh, Joris Ijsselmuiden, Jan Willem Hofstee and Eldert J Henten
    “Transfer learning for the classification of sugar beet and volunteer potato under
    field conditions” In *Biosystems engineering* 174 Elsevier, 2018, pp. 50–65'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Christian Szegedy et al. “Going deeper with convolutions” In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1–9'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Aboozar Taherkhani, Georgina Cosma and T Martin McGinnity “AdaBoost-CNN:
    An adaptive boosting algorithm for convolutional neural networks to classify multi-class
    imbalanced datasets using transfer learning” In *Neurocomputing* 404 Elsevier,
    2020, pp. 351–366'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Ryo Takahashi, Takashi Matsubara and Kuniaki Uehara “Ricap: Random image
    cropping and patching data augmentation for deep cnns” In *Asian Conference on
    Machine Learning*, 2018, pp. 786–798'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] JingLei Tang et al. “Weed identification based on K-means feature learning
    combined with convolutional neural network” In *Computers and electronics in agriculture*
    135 Elsevier, 2017, pp. 63–70'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Andrew Tao, Jon Barker and Sriya Sarathy “Detectnet: Deep neural network
    for object detection in digits” In *Parallel Forall* 4, 2016'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Nima Teimouri et al. “Weed growth stage estimator using deep convolutional
    neural networks” In *Sensors* 18.5 Multidisciplinary Digital Publishing Institute,
    2018, pp. 1580'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] L Tian, DC Slaughter and RF Norris “Machine vision identification of
    tomato seedlings for automated weed control” In *Transactions of ASAE* 40.6 Citeseer,
    2000, pp. 1761–1768'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Vo Hoang Trong, Yu Gwang-hyun, Dang Thanh Vu and Kim Jin-young “Late
    fusion of multimodal deep neural networks for weeds classification” In *Computers
    and Electronics in Agriculture* 175 Elsevier, 2020, pp. 105506'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] S Umamaheswari, R Arjun and D Meganathan “Weed Detection in Farm Crops
    using Parallel Image Processing” In *2018 Conference on Information and Communication
    Technology (CICT)*, 2018, pp. 1–4 IEEE'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S Umamaheswari and Ashvini V Jain “Encoder–Decoder Architecture for Crop-Weed
    Classification Using Pixel-Wise Labelling” In *2020 International Conference on
    Artificial Intelligence and Signal Processing (AISP)*, 2020, pp. 1–6 IEEE'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J Valente, M Doldersum, C Roers and L Kooistra “DETECTING RUMEX OBTUSIFOLIUS
    WEED PLANTS IN GRASSLANDS FROM UAV RGB IMAGERY USING DEEP LEARNING.” In *ISPRS
    Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences* 4, 2019'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Viraf “Create A Synthetic Image Dataset - The ”What”, The ”Why” and The
    ”How”” In *Medium* Towards Data Science, 2020 URL: [https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718](https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718)'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Aichen Wang, Yifei Xu, Xinhua Wei and Bingbo Cui “Semantic Segmentation
    of Crop and Weed using an Encoder-Decoder Network and Image Enhancement Method
    under Uncontrolled Outdoor Illumination” In *IEEE Access* 8 IEEE, 2020, pp. 81724–81734'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Aichen Wang, Wen Zhang and Xinhua Wei “A review on weed detection using
    ground-based machine vision and image processing techniques” In *Computers and
    electronics in agriculture* 158 Elsevier, 2019, pp. 226–240'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Alexander Wendel and James Underwood “Self-supervised weed detection
    in vegetable crops using ground based hyperspectral imaging” In *2016 IEEE international
    conference on robotics and automation (ICRA)*, 2016, pp. 5128–5135 IEEE'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] DM Woebbecke, GE Meyer, K Von Bargen and DA Mortensen “Shape features
    for identifying young weeds using image analysis” In *Transactions of the ASAE*
    38.1 American Society of AgriculturalBiological Engineers, 1995, pp. 271–281'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Xue Yan, Xiangwu Deng and Jing Jin “Classification of weed species in
    the paddy field with DCNN-Learned features” In *2020 IEEE 5th Information Technology
    and Mechatronics Engineering Conference (ITOEC)*, 2020, pp. 336–340 IEEE'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Zhang Yi, Shen Yongliang and Zhang Jun “An improved tiny-yolov3 pedestrian
    detection algorithm” In *Optik* 183 Elsevier, 2019, pp. 17–23'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Jialin Yu et al. “Weed Detection in Perennial Ryegrass With Deep Learning
    Convolutional Neural Network” In *Frontiers in plant science* 10 Frontiers Media
    SA, 2019'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Jialin Yu, Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Deep
    learning for image-based weed detection in turfgrass” In *European journal of
    agronomy* 104 Elsevier, 2019, pp. 78–84'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Rufei Zhang et al. “Weed location and recognition based on UAV imaging
    and deep learning” In *International Journal of Precision Agricultural Aviation*
    3.1, 2020'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Wenhao Zhang et al. “Broad-leaf weed detection in pasture” In *2018 IEEE
    3rd International Conference on Image, Vision and Computing (ICIVC)*, 2018, pp.
    101–105 IEEE'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Xiao-Yu Zhang, Haichao Shi, Xiaobin Zhu and Peng Li “Active semi-supervised
    learning based on self-expressive correlation with generative adversarial networks”
    In *Neurocomputing* 345 Elsevier, 2019, pp. 103–113'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Yang Zheng et al. “Maize and weed classification using color indices
    with support vector data description in outdoor fields” In *Computers and Electronics
    in Agriculture* 141 Elsevier, 2017, pp. 215–222'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Zhi-Hua Zhou “A brief introduction to weakly supervised learning” In
    *National Science Review* 5.1 Oxford University Press, 2018, pp. 44–53'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
