- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:56:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:56:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2103.01415] A Survey of Deep Learning Techniques for Weed Detection from Images'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2103.01415] 深度学习技术在图像中杂草检测的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2103.01415](https://ar5iv.labs.arxiv.org/html/2103.01415)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2103.01415](https://ar5iv.labs.arxiv.org/html/2103.01415)
- en: \usesmartdiagramlibrary
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \usesmartdiagramlibrary
- en: additions
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: additions
- en: A Survey of Deep Learning Techniques for Weed Detection from Images
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习技术在图像中杂草检测的调查
- en: 'A S M Mahmudul Hasan¹¹1Corresponding author email: 33916214@student.murdoch.edu.au
    Information Technology, Murdoch University, Murdoch, WA 6150, Australia Centre
    for Crop and Food Innovation, Food Futures Institute, Murdoch University, Murdoch,
    WA 6150, Australia Ferdous Sohel Information Technology, Murdoch University, Murdoch,
    WA 6150, Australia Centre for Crop and Food Innovation, Food Futures Institute,
    Murdoch University, Murdoch, WA 6150, Australia Dean Diepeveen Centre for Crop
    and Food Innovation, Food Futures Institute, Murdoch University, Murdoch, WA 6150,
    Australia Department of Primary Industries and Regional Development, Western Australia,
    South Perth, WA, 6151, Australia Centre for Sustainable Farming Systems, Murdoch
    University, Murdoch, WA 6150, Australia Hamid Laga Information Technology, Murdoch
    University, Murdoch, WA 6150, Australia Centre of Biosecurity and One Health,
    Harry Butler Institute, Murdoch University, Murdoch University, Murdoch, WA 6150,
    Australia Michael G.K. Jones Centre for Crop and Food Innovation, Food Futures
    Institute, Murdoch University, Murdoch, WA 6150, Australia'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: A S M Mahmudul Hasan¹¹1通讯作者电子邮件：33916214@student.murdoch.edu.au 信息技术，默多克大学，澳大利亚，WA
    6150，默多克，中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA 6150，默多克，Ferdous Sohel 信息技术，默多克大学，澳大利亚，WA
    6150，默多克，中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA 6150，默多克，Dean Diepeveen 中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA
    6150，默多克，初级产业与区域发展部，西澳大利亚州，南珀斯，WA 6151，澳大利亚，中心可持续农业系统，默多克大学，澳大利亚，WA 6150，默多克，Hamid
    Laga 信息技术，默多克大学，澳大利亚，WA 6150，默多克，生物安全与一体化健康中心，哈里·巴特勒研究所，默多克大学，澳大利亚，WA 6150，默多克，Michael
    G.K. Jones 中心作物与食品创新，食品未来研究所，默多克大学，澳大利亚，WA 6150，默多克
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advances in Deep Learning (DL) techniques have enabled rapid detection,
    localisation, and recognition of objects from images or videos. DL techniques
    are now being used in many applications related to agriculture and farming. Automatic
    detection and classification of weeds can play an important role in weed management
    and so contribute to higher yields. Weed detection in crops from imagery is inherently
    a challenging problem because both weeds and crops have similar colours (‘green-on-green’),
    and their shapes and texture can be very similar at the growth phase. Also, a
    crop in one setting can be considered a weed in another. In addition to their
    detection, the recognition of specific weed species is essential so that targeted
    controlling mechanisms (e.g. appropriate herbicides and correct doses) can be
    applied. In this paper, we review existing deep learning-based weed detection
    and classification techniques. We cover the detailed literature on four main procedures,
    i.e., data acquisition, dataset preparation, DL techniques employed for detection,
    location and classification of weeds in crops, and evaluation metrics approaches.
    We found that most studies applied supervised learning techniques, they achieved
    high classification accuracy by fine-tuning pre-trained models on any plant dataset,
    and past experiments have already achieved high accuracy when a large amount of
    labelled data is available.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）技术的快速进步使得从图像或视频中迅速检测、定位和识别对象成为可能。DL 技术现在在许多与农业和种植相关的应用中得到了应用。自动检测和分类杂草可以在杂草管理中发挥重要作用，从而提高产量。由于杂草和作物的颜色（‘绿对绿’）相似，它们的形状和纹理在生长阶段可能非常相似，因此从图像中检测作物中的杂草本质上是一个具有挑战性的问题。此外，一个环境中的作物在另一个环境中可能被视为杂草。除了检测外，识别特定的杂草种类也至关重要，以便能够施加针对性的控制机制（例如，适当的除草剂和正确的剂量）。在本文中，我们回顾了现有的基于深度学习的杂草检测和分类技术。我们涵盖了关于四个主要过程的详细文献，即数据采集、数据集准备、用于检测、定位和分类作物中的杂草的
    DL 技术，以及评估指标方法。我们发现大多数研究应用了监督学习技术，通过对任何植物数据集微调预训练模型实现了高分类准确率，并且过去的实验已经在大量标记数据可用时达到了高准确率。
- en: 'Keywords: Deep learning, Weed detection, Weed classification, Machine Learning,
    Digital agriculture.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：深度学习，杂草检测，杂草分类，机器学习，数字农业。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The world population has been increasing rapidly, and it is expected to reach
    nine billion by 2050\. Agricultural production needs to increase by about 70%
    to meet the anticipated demands [[125](#bib.bibx125)]. However, the agricultural
    sector will face many challenges during this time, including a reduction of cultivatable
    land and the need for more intensive production. Other issues, such as climate
    change and water scarcity, will also affect productivity. Precision agriculture
    or digital agriculture can provide strategies to mitigate these issues [[85](#bib.bibx85),
    [139](#bib.bibx139), [125](#bib.bibx125)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 世界人口正在迅速增长，预计到2050年将达到90亿。为了满足预期需求，农业生产需要增加约70%[[125](#bib.bibx125)]。然而，农业部门在此期间将面临许多挑战，包括可耕作土地的减少和对更高生产强度的需求。其他问题，如气候变化和水资源短缺，也将影响生产力。精准农业或数字农业可以提供缓解这些问题的策略[[85](#bib.bibx85),
    [139](#bib.bibx139), [125](#bib.bibx125)]。
- en: 'Weeds are plants that can spread quickly and undesirably, and can impact on
    crop yields and quality [[118](#bib.bibx118)]. Weeds compete with crops for nutrition,
    water, sunlight, and growing space [[67](#bib.bibx67)]. Therefore, farmers have
    to deploy resources to reduce weeds. The management strategies used to reduce
    the impact of weeds depend on many factors. These strategies can be categorised
    into five main types [[137](#bib.bibx137)]: ‘preventative’ (prevent weeds from
    becoming established), ‘cultural’ (by maintaining field hygiene – low weed seed
    bank), ‘mechanical’ (e.g., mowing, mulching and tilling), ‘biological’ (using
    natural enemies of weeds such as insects, grazing animals or disease), and ‘chemical’
    (application of herbicides). These approaches all have drawbacks. In general,
    there is a financial burden and they require time and extra work. In addition,
    control treatments may impact the health of people, plants, soil, animals, or
    the environment [[112](#bib.bibx112), [137](#bib.bibx137), [59](#bib.bibx59)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 杂草是能够迅速且不受欢迎地传播的植物，它们可能影响作物的产量和质量[[118](#bib.bibx118)]。杂草与作物争夺营养、水分、阳光和生长空间[[67](#bib.bibx67)]。因此，农民必须投入资源以减少杂草。用于减少杂草影响的管理策略取决于许多因素。这些策略可以分为五种主要类型[[137](#bib.bibx137)]：‘预防性’（防止杂草建立），‘文化性’（通过保持田间卫生——低杂草种子库），‘机械性’（例如，割草、覆盖和耕作），‘生物性’（利用自然敌人如昆虫、放牧动物或疾病），以及‘化学性’（施用除草剂）。这些方法都有其缺点。一般来说，它们带来财务负担，需要时间和额外的工作。此外，控制措施可能影响人类、植物、土壤、动物或环境的健康[[112](#bib.bibx112),
    [137](#bib.bibx137), [59](#bib.bibx59)]。
- en: As the costs of labour has increased, and people have become more concerned
    about health and environmental issues, automation of weed control has become desirable
    [[99](#bib.bibx99)]. Automated weed control systems can be beneficial both economically
    and environmentally. Such systems can reduce labour costs by using a machine to
    remove weeds and, selective spraying techniques can minimise the use of the herbicides
    [[87](#bib.bibx87)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着劳动力成本的增加，人们对健康和环境问题的关注也在上升，自动化杂草控制变得越来越受到欢迎[[99](#bib.bibx99)]。自动化杂草控制系统在经济和环境方面都具有好处。这些系统可以通过使用机器去除杂草来减少劳动力成本，并且选择性喷洒技术可以最小化除草剂的使用[[87](#bib.bibx87)]。
- en: To develop an automatic weed management system, an essential first step is to
    be able to detect and recognise weeds correctly [[99](#bib.bibx99)]. Detection
    of weeds in crops is challenging as weeds and crop plants often have similar colours,
    textures, and shapes. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") shows crop plants
    with weeds growing amongst them. Common challenges in detection and classification
    of crops and weeds are occlusion (Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), similarity
    in colour and texture (Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), plants
    shadowed in natural light (Figure [1(c)](#S1.F1.sf3 "In Figure 1 ‣ 1 Introduction
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")), colour
    and texture variations due to lighting conditions and illumination (Figure [1(d)](#S1.F1.sf4
    "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images")) and different species of weeds which appear similar (Figure
    [1(e)](#S1.F1.sf5 "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images")). Same crop plants or weeds may show dissimilarities
    during growth phases (Figure [1(f)](#S1.F1.sf6 "In Figure 1 ‣ 1 Introduction ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")). Motion
    blur and noise in the image also increase the difficulty in classifying plants
    (Figure [1(g)](#S1.F1.sf7 "In Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")). In addition, depending on the geographical
    location (Figure [1(h)](#S1.F1.sf8 "In Figure 1 ‣ 1 Introduction ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images")) and the variety of
    the crop, weather and soil conditions, the species of weeds can vary [[70](#bib.bibx70)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个自动化杂草管理系统，首要步骤是能够正确检测和识别杂草[[99](#bib.bibx99)]。由于杂草和作物植物通常具有相似的颜色、纹理和形状，检测作物中的杂草具有挑战性。图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述") 显示了杂草与作物植物共生的场景。检测和分类作物与杂草的常见挑战包括遮挡（图
    [1(a)](#S1.F1.sf1 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、颜色和纹理的相似性（图 [1(b)](#S1.F1.sf2
    "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、自然光下植物的阴影（图 [1(c)](#S1.F1.sf3 "在图 1 ‣ 1
    引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、由于光照条件和光源造成的颜色和纹理变化（图 [1(d)](#S1.F1.sf4 "在图 1 ‣ 1
    引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）以及外观相似的不同杂草种类（图 [1(e)](#S1.F1.sf5 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）。相同的作物植物或杂草在不同生长阶段可能会显示出差异（图
    [1(f)](#S1.F1.sf6 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）。图像中的运动模糊和噪声也增加了植物分类的难度（图
    [1(g)](#S1.F1.sf7 "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）。此外，根据地理位置（图 [1(h)](#S1.F1.sf8
    "在图 1 ‣ 1 引言 ‣ 关于从图像中检测杂草的深度学习技术的综述")）、作物品种、天气和土壤条件，杂草种类可能会有所不同[[70](#bib.bibx70)]。
- en: '![Refer to caption](img/085f5ae511ca127e91de98360bd959bb.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/085f5ae511ca127e91de98360bd959bb.png)'
- en: (a) Occlusion of crop and weed [[54](#bib.bibx54)]
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 作物与杂草的遮挡 [[54](#bib.bibx54)]
- en: '![Refer to caption](img/194c24f66d6e6842dc66b9e2bead8edc.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/194c24f66d6e6842dc66b9e2bead8edc.png)'
- en: (b) Colour and texture similarities between crop and weed plants [[12](#bib.bibx12)]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 作物和杂草植物之间的颜色和纹理相似性 [[12](#bib.bibx12)]
- en: '![Refer to caption](img/80d2e04c1bf666c4f24d5fcd99bc664a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/80d2e04c1bf666c4f24d5fcd99bc664a.png)'
- en: (c) Shadow effects in natural weed image [[123](#bib.bibx123)]
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 自然杂草图像中的阴影效果 [[123](#bib.bibx123)]
- en: '![Refer to caption](img/38d77e08f38de3efafe3b3497077cc88.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/38d77e08f38de3efafe3b3497077cc88.png)'
- en: (d) Effects of illumination conditions [[33](#bib.bibx33)]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 光照条件的影响 [[33](#bib.bibx33)]
- en: '![Refer to caption](img/407cfe3028f66e91448d813473ceb626.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/407cfe3028f66e91448d813473ceb626.png)'
- en: (e) Four different species of weeds that share similarities (inter-class similarity)
    [[113](#bib.bibx113)]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 四种具有相似性的不同杂草种类（类别间相似性） [[113](#bib.bibx113)]
- en: '![Refer to caption](img/0d7fead47e3714cfd6f0d890bdac7200.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d7fead47e3714cfd6f0d890bdac7200.png)'
- en: (f) Sugar beet crop at different growth stages (intra-class variations) [[49](#bib.bibx49)]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 不同生长阶段的甜菜作物（类别内部变化）[[49](#bib.bibx49)]
- en: '![Refer to caption](img/9ab4e4116cd982f20ef29eb9676a7a5f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ab4e4116cd982f20ef29eb9676a7a5f.png)'
- en: (g) Effects of motion blur and noise [[49](#bib.bibx49), [3](#bib.bibx3)]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 运动模糊和噪声的影响 [[49](#bib.bibx49), [3](#bib.bibx3)]
- en: '![Refer to caption](img/42779da4bd4433db68ea7ca56ec8c650.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/42779da4bd4433db68ea7ca56ec8c650.png)'
- en: '(h) Weeds can vary at different geographic/weather locations: weed in carrot
    crop collected from Germany(left) [[54](#bib.bibx54)] and Macedonia (Right) [[88](#bib.bibx88)]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 杂草在不同的地理/天气位置可能有所不同：来自德国（左） [[54](#bib.bibx54)] 和马其顿（右） [[88](#bib.bibx88)]
    的胡萝卜作物中的杂草
- en: 'Figure 1: Weeds in different crops (green boxes indicate crops and red boxes
    indicate weeds).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同作物中的杂草（绿色框表示作物，红色框表示杂草）。
- en: 'A typical weed detection system follows four key steps: image acquisition,
    pre-processing of images, extraction of features and detection and classification
    of weeds [[140](#bib.bibx140)]. Different emerging technologies have been used
    to accomplish these steps. The most crucial part of these steps is weed detection
    and classification. In recent years, with advances in computer technologies, particularly
    in graphical processing units (GPU), embedded processors coupled with the use
    of Machine Learning (ML) techniques have become more widely used for automatic
    detection of weed species [[91](#bib.bibx91), [50](#bib.bibx50), [171](#bib.bibx171)].'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的杂草检测系统包括四个关键步骤：图像采集、图像预处理、特征提取以及杂草的检测和分类 [[140](#bib.bibx140)]。不同的新兴技术已被用于完成这些步骤。这些步骤中最关键的是杂草的检测和分类。近年来，随着计算机技术的进步，特别是在图形处理单元（GPU）方面，结合使用机器学习（ML）技术的嵌入式处理器已被更广泛地用于自动检测杂草种类
    [[91](#bib.bibx91), [50](#bib.bibx50), [171](#bib.bibx171)]。
- en: Deep learning (DL) is an important branch of ML. For image classification, object
    detection, and recognition, DL algorithms have many advantages over traditional
    ML approaches (in this paper, the term machine learning, we mean traditional machine
    learning approaches). Extracting and selecting discriminating features with ML
    methods is difficult because crops and weeds can be similar. This problem can
    be addressed efficiently by using DL approaches based on their strong feature
    learning capabilities. Recently, many research articles have been published on
    DL-based weed recognition, yet few review articles have been published on this
    topic. Su [[149](#bib.bibx149)] recently published a review paper in which the
    main focus was on the use of point spectroscopy, RGB, and hyperspectral imaging
    to classify weeds in crops automatically. However, most of the articles covered
    in this review have applied traditional machine learning approaches, with few
    citations of recent papers. [[99](#bib.bibx99)] analysed a number of publications
    on weed detection, but from the perspective of selective spraying.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习（ML）的一个重要分支。在图像分类、物体检测和识别方面，DL算法相较于传统机器学习方法具有许多优势（在本文中，术语“机器学习”指的是传统机器学习方法）。使用机器学习方法提取和选择辨别特征是困难的，因为作物和杂草可能相似。这个问题可以通过利用DL方法的强大特征学习能力有效解决。最近，已经有许多研究文章发表了基于DL的杂草识别，但关于这一主题的综述文章却很少。Su
    [[149](#bib.bibx149)] 最近发表了一篇综述文章，主要关注使用点谱学、RGB和高光谱成像来自动分类作物中的杂草。然而，大多数涵盖的文章应用了传统机器学习方法，且对近期论文的引用较少。
    [[99](#bib.bibx99)] 从选择性喷洒的角度分析了多篇关于杂草检测的出版物。
- en: We provide this comprehensive literature survey to highlight the great potential
    now presented by different DL techniques for detecting, localising, and classifying
    weeds in crops. We present a taxonomy of the DL techniques for weed detection
    and recognition, and classify major publications based on that taxonomy. We also
    cover data collection, data preparation, and data representation approaches. We
    provide an overview of different evaluation metrics used to benchmark the performance
    of the techniques surveyed in this article.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供这份综合文献调查，以突显不同深度学习（DL）技术在检测、定位和分类作物中的杂草方面所展现的巨大潜力。我们展示了用于杂草检测和识别的深度学习技术的分类法，并根据该分类法对主要出版物进行了分类。我们还涵盖了数据收集、数据准备和数据表示方法。我们概述了用于评估本文中调查技术性能的不同评估指标。
- en: The rest of the paper is organised as follows. Existing review papers in this
    area are discussed briefly in Section [2](#S2 "2 Related Surveys ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images"). Advantages of DL-based
    weed detection approaches over traditional ML methods are discussed in Section
    [3](#S3 "3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images"). In Section [4](#S4 "4 Paper
    Selection Criteria in this Survey ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images"), we describe how the papers for review were selected.
    A taxonomy and an overview of DL-based weed detection techniques are provided
    in Section [5](#S5 "5 An Overview and Taxonomy of Deep Learning-based Weed Detection
    Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection from Images").
    We describe four major steps of DL-based approaches, i.e. data acquisition (Section
    [6](#S6 "6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images")), dataset preparation (Section [7](#S7 "7 Dataset Preparation ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")), detection
    and classification methods (Section [10](#S10 "10 Deep Learning Architecture ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images")) and evaluation
    metrics (Section [11](#S11 "11 Performance Evaluation Metrics ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")). In Section [8](#S8 "8 Detection
    Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")
    we have highlighted the approaches to detection of weeds in crop plants adopted
    in the related work. The learning methods applied the relevant studies are explained
    in Section [9](#S9 "9 Learning Methods ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images"). We summarise the current state in this field
    and provide future directions in Section [12](#S12 "12 Discussion ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images") with conclusions are
    provided in Section [13](#S13 "13 Conclusion ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images").
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。现有的综述论文在本节[2](#S2 "2 相关调查 ‣ 基于深度学习的杂草检测技术综述")中进行了简要讨论。深度学习方法在杂草检测中的优势与传统机器学习方法的对比在本节[3](#S3
    "3 传统机器学习与深度学习方法的杂草检测 ‣ 基于深度学习的杂草检测技术综述")中进行了讨论。在本节[4](#S4 "4 本综述中的论文选择标准 ‣ 基于深度学习的杂草检测技术综述")中，我们描述了如何选择用于综述的论文。关于深度学习的杂草检测技术的分类和概述在本节[5](#S5
    "5 基于深度学习的杂草检测方法概述与分类 ‣ 基于深度学习的杂草检测技术综述")中提供。我们描述了深度学习方法的四个主要步骤，即数据获取（第[6](#S6
    "6 数据获取 ‣ 基于深度学习的杂草检测技术综述")节）、数据集准备（第[7](#S7 "7 数据集准备 ‣ 基于深度学习的杂草检测技术综述")节）、检测和分类方法（第[10](#S10
    "10 深度学习架构 ‣ 基于深度学习的杂草检测技术综述")节）以及评估指标（第[11](#S11 "11 性能评估指标 ‣ 基于深度学习的杂草检测技术综述")节）。在第[8](#S8
    "8 检测方法 ‣ 基于深度学习的杂草检测技术综述")节中，我们突出介绍了相关工作中采用的作物植物杂草检测方法。相关研究中应用的学习方法在第[9](#S9
    "9 学习方法 ‣ 基于深度学习的杂草检测技术综述")节中进行了说明。我们在第[12](#S12 "12 讨论 ‣ 基于深度学习的杂草检测技术综述")节中总结了该领域的当前状态并提供了未来方向，结论在第[13](#S13
    "13 结论 ‣ 基于深度学习的杂草检测技术综述")节中提供。
- en: 2 Related Surveys
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关调查
- en: ML and DL techniques have been used for weed detection, recognition and thus
    for weed management. In 2018, [[73](#bib.bibx73)] published a survey of 40 research
    papers that applied DL-techniques to address various agricultural problems, including
    weed detection. The study reported that DL-techniques outperformed more than traditional
    image processing methods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）和深度学习（DL）技术已被用于杂草检测、识别以及杂草管理。2018年，[[73](#bib.bibx73)] 发表了一项综述，回顾了40篇应用深度学习技术解决各种农业问题的研究论文，包括杂草检测。研究报告显示，深度学习技术在性能上优于传统的图像处理方法。
- en: In 2016, [[106](#bib.bibx106)] discussed ten components that are essential and
    possible obstructions to develop a fully autonomous mechanical weed management
    system. With the advance in DL, it seems that the problems raised can now be addressed.
    [[6](#bib.bibx6)] articulated that DL-based plant classification modules can be
    deployed not only in weed management systems but also for fertilisation, irrigation,
    and phenotyping. Their study explained how “Deepfield Robotics” systems could
    reduce labour required for weed control in agriculture and horticulture.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，[[106](#bib.bibx106)]讨论了十项发展全自动机械除草系统所必需的组成部分和可能的障碍。随着深度学习的进展，这些问题似乎可以得到解决。[[6](#bib.bibx6)]指出，基于深度学习的植物分类模块不仅可用于除草管理系统，还可用于肥料、灌溉和表型分析。他们的研究解释了“深度机器人技术”系统如何减少农业和园艺中除草控制所需的劳动力。
- en: '[[165](#bib.bibx165)] highlighted that the most challenging part of a weed
    detection techniques is to distinguish between weed and crop species. They focused
    on different machine vision and image processing techniques used for ground-based
    weed detection. [[21](#bib.bibx21)] made a similar observation. They reviewed
    remote sensing for weed mapping and ground-based detection techniques. They also
    reported the limitations of using either spectral or spatial features to identify
    weeds in crops. According to their study, it is preferable to use both features.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[[165](#bib.bibx165)]强调杂草检测技术中最具挑战性的部分是区分杂草和作物品种。他们关注地面杂草检测所使用的不同机器视觉和图像处理技术。[[21](#bib.bibx21)]发表了类似的观察。他们对杂草制图和地面检测技术进行了回顾。他们还报告了使用光谱特征或空间特征来识别作物中的杂草的局限性。根据他们的研究，最好同时使用这两种特征。'
- en: '[[45](#bib.bibx45)] reviewed technologies that can be used to monitor weeds
    in crops. They explored different remotely sensed and ground-based weed monitoring
    systems in agricultural fields. They reported that weed monitoring is essential
    for weed management. They foresaw that the data collected using different sensors
    could be stored in cloud systems for timely use in relevant contexts. In another
    study, [[109](#bib.bibx109)] evaluated a small number of DL approaches used for
    detecting weeds in crops. They identified research gaps, e.g., the lack of large
    crop-weed datasets, acceptable classification accuracy and lack of generalised
    models for detecting different crop plants and weed species. However, the article
    only covered a handful of publications and as such the paper was not thorough
    and did not adequately cover the breadth and depth of the literature.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[[45](#bib.bibx45)]回顾了可用于监测作物中杂草的技术。他们探索了农田中的不同遥感和地面监测系统。他们报告了杂草监测对于杂草管理的重要性。他们预测，使用不同传感器收集的数据可以存储在云系统中，以适时在相关情境中使用。在另一项研究中，[[109](#bib.bibx109)]评估了用于检测作物中杂草的少数深度学习方法。他们确定了研究的空白，例如缺乏大规模的作物杂草数据集，可接受的分类准确性以及缺乏适用于检测不同作物植物和杂草品种的广义模型。然而，该文章只涵盖了少数几篇论文，因此并不全面，无法充分覆盖文献的广度和深度。'
- en: 3 Traditional ML- vs DL-based Weed Detection Methods
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统机器学习与深度学习在杂草检测方法上的区别
- en: 'A typical ML-based weed classification technique follows five key steps: image
    acquisition, pre-processing such as image enhancement, feature extraction or with
    feature selection, applying an ML-based classifier and evaluation of the performance
    [[99](#bib.bibx99), [23](#bib.bibx23), [17](#bib.bibx17), [96](#bib.bibx96)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的基于机器学习的杂草分类技术包括五个关键步骤：图像获取、预处理（如图像增强）、特征提取或特征选择、应用基于机器学习的分类器和性能评估[[99](#bib.bibx99)，[23](#bib.bibx23)，[17](#bib.bibx17)，[96](#bib.bibx96)]。
- en: Different image processing methods have been applied for crop and weed classification
    [[167](#bib.bibx167), [57](#bib.bibx57), [158](#bib.bibx158)]. By extracting shape
    features, many researchers identify weeds and crops using discriminate analysis
    [[24](#bib.bibx24), [107](#bib.bibx107)]. In some other research, different colour
    [[175](#bib.bibx175), [69](#bib.bibx69), [52](#bib.bibx52), [75](#bib.bibx75)]
    and texture [[13](#bib.bibx13)] features were used.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的图像处理方法已被应用于作物和杂草的分类[[167](#bib.bibx167)，[57](#bib.bibx57)，[158](#bib.bibx158)]。通过提取形状特征，许多研究人员使用判别分析来识别杂草和作物[[24](#bib.bibx24),
    [107](#bib.bibx107)]。在其他一些研究中，使用不同的颜色[[175](#bib.bibx175)，[69](#bib.bibx69)，[52](#bib.bibx52)，[75](#bib.bibx75)]和纹理[[13](#bib.bibx13)]特征。
- en: The main challenge in weed detection and classification is that both weeds and
    crops can have very similar colours or textures. Machine learning approaches learn
    the features from the training data that are available [[12](#bib.bibx12)]. Understandably,
    for traditional ML-approaches, the combination of multiple modalities of data
    e.g. the shape, texture and colour or a combination of multiple sensor data is
    expected to generate superior results to a single modality of data. [[80](#bib.bibx80)]
    argued that colour or texture features of an image alone are not adequate to classify
    wheat from weed species Bidens pilosa. They used Near-Infrared (NIR) image cues
    with those features. [[136](#bib.bibx136)] extracted eight texture features based
    on the grey level co-occurrence matrix (GLCM), two spectral descriptors of texture,
    thirteen different colour features, five moment-invariant features, and eight
    shape features. They compared the performance of several algorithms, such as the
    ant colony algorithm, simulated annealing method, and genetic algorithm for selecting
    more discriminative features. The performance of the Cultural Algorithm, Linear
    Discriminant Analysis (LDA), Support Vector Machine (SVM), and Random Forest classifiers
    were also evaluated to distinguish between crops and weeds.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在杂草检测和分类中，主要挑战在于杂草和作物可能具有非常相似的颜色或纹理。机器学习方法从可用的训练数据中学习特征[[12](#bib.bibx12)]。可以理解，对于传统的机器学习方法，数据的多模态组合（例如形状、纹理和颜色，或多个传感器数据的组合）被期望能产生比单一数据模态更优的结果。[[80](#bib.bibx80)]
    认为，仅依靠图像的颜色或纹理特征不足以将小麦与杂草物种*悲伤蓟*区分开来。他们使用了近红外（NIR）图像线索与这些特征结合。[[136](#bib.bibx136)]
    基于灰度共生矩阵（GLCM）提取了八种纹理特征、两种纹理光谱描述符、十三种不同的颜色特征、五种不变矩特征和八种形状特征。他们比较了多种算法的性能，如蚁群算法、模拟退火法和遗传算法，以选择更具判别力的特征。还评估了文化算法、线性判别分析（LDA）、支持向量机（SVM）和随机森林分类器的性能，以区分作物和杂草。
- en: '[[74](#bib.bibx74)] applied SVM for detecting weeds in corn from hyperspectral
    images. In other research, [[166](#bib.bibx166)] used SVM and LDA for classifying
    plants. They proposed a self-supervised approach for discrimination. Before training
    the models, they applied vegetation separation techniques to remove background
    and different spectral pre-processing to extract features using Principal Component
    Analysis (PCA). [[68](#bib.bibx68)] extracted different shape features and the
    feature vectors were evaluated using a single-layer perceptron classifier to distinguish
    narrow and broad-leafed weeds.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[[74](#bib.bibx74)] 应用SVM从高光谱图像中检测玉米中的杂草。在其他研究中，[[166](#bib.bibx166)] 使用SVM和LDA进行植物分类。他们提出了一种自监督的方法用于区分。在训练模型之前，他们应用了植被分离技术去除背景，并使用主成分分析（PCA）进行不同的光谱预处理以提取特征。[[68](#bib.bibx68)]
    提取了不同的形状特征，使用单层感知器分类器评估特征向量，以区分狭叶和宽叶杂草。'
- en: <svg   height="465.68" overflow="visible" version="1.1" width="619.13"><g transform="translate(0,465.68)
    matrix(1 0 0 -1 0 0) translate(55.72,0) translate(0,373.3)"><g stroke="#000000"
    fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -34.59
    1.6)" fill="#000000" stroke="#000000"><foreignobject width="69.19" height="22.56"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Collect Data <g fill="#CCFFCC"><path
    d="M 177.84 0 C 177.84 13.78 153.01 24.95 122.39 24.95 C 91.77 24.95 66.95 13.78
    66.95 0 C 66.95 -13.78 91.77 -24.95 122.39 -24.95 C 153.01 -24.95 177.84 -13.78
    177.84 0 Z M 122.39 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 87.8 3.34)"
    fill="#000000" stroke="#000000"><foreignobject width="69.19" height="26.06" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Use Public Data</foreignobject></g> <g fill="#FFCCCC"><path
    d="M 123.61 -47.36 L -12.91 -47.36 C -15.97 -47.36 -18.45 -49.84 -18.45 -52.89
    L -18.45 -81.19 C -18.45 -84.25 -15.97 -86.73 -12.91 -86.73 L 123.61 -86.73 C
    126.67 -86.73 129.15 -84.25 129.15 -81.19 L 129.15 -52.89 C 129.15 -49.84 126.67
    -47.36 123.61 -47.36 Z M -18.45 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 -13.84 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data Acquisition</foreignobject></g>
    <path d="M 17.8 -21.56 L 36.76 -44.52" style="fill:none"><g transform="matrix(0.63672
    -0.77109 0.77109 0.63672 36.76 -44.52)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 99.41 -22.98 L 77.65 -44.73" style="fill:none"><g
    transform="matrix(-0.70717 -0.70705 0.70705 -0.70717 77.65 -44.73)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path
    d="M 301.35 -47.36 L 164.83 -47.36 C 161.77 -47.36 159.29 -49.84 159.29 -52.89
    L 159.29 -81.19 C 159.29 -84.25 161.77 -86.73 164.83 -86.73 L 301.35 -86.73 C
    304.41 -86.73 306.89 -84.25 306.89 -81.19 L 306.89 -52.89 C 306.89 -49.84 304.41
    -47.36 301.35 -47.36 Z M 159.29 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 163.9 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Dataset
    Preparation</foreignobject></g> <g stroke-width="0.6pt" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(1.0 0.0 0.0
    1.0 151.17 -67.04)"><path d="M 0 4.05 L 7.01 0 L 0 -4.05 Z" style="fill:none"></path></g><g
    stroke-width="3.0pt"><path d="M 129.42 -67.04 L 150.71 -67.04" style="fill:none"></path></g><g
    stroke-width="1.4pt" stroke="#FFFFFF"><path d="M 129.42 -67.04 L 150.71 -67.04"
    style="fill:none"></path></g><g stroke-width="2.0pt" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><path d="M 129.42 -67.04 L 152.79 -67.04" style="fill:none"></path></g><g
    fill="#CCFFCC"><path d="M 150.16 -134.09 C 150.16 -115.19 125.34 -99.87 94.72
    -99.87 C 64.1 -99.87 39.27 -115.19 39.27 -134.09 C 39.27 -152.99 64.1 -168.31
    94.72 -168.31 C 125.34 -168.31 150.16 -152.99 150.16 -134.09 Z M 94.72 -134.09"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 60.13 -124.19)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Colour
    Model Conversion</foreignobject></g> <g fill="#CCFFCC"><path d="M 164 -214.97
    C 164 -200.22 139.18 -188.27 108.56 -188.27 C 77.93 -188.27 53.11 -200.22 53.11
    -214.97 C 53.11 -229.72 77.93 -241.67 108.56 -241.67 C 139.18 -241.67 164 -229.72
    164 -214.97 Z M 108.56 -214.97"></path></g><g transform="matrix(1.0 0.0 0.0 1.0
    73.96 -210.39)" fill="#000000" stroke="#000000"><foreignobject width="69.19" height="28.54"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image Resizing</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 286.39 -214.97 C 286.39 -201.27 261.57 -190.17 230.95
    -190.17 C 200.33 -190.17 175.5 -201.27 175.5 -214.97 C 175.5 -228.66 200.33 -239.77
    230.95 -239.77 C 261.57 -239.77 286.39 -228.66 286.39 -214.97 Z M 230.95 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 196.35 -211.73)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="25.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Image
    Augmentation</foreignobject></g> <g fill="#CCFFCC"><path d="M 426.9 -134.09 C
    426.9 -119.34 402.08 -107.39 371.46 -107.39 C 340.84 -107.39 316.02 -119.34 316.02
    -134.09 C 316.02 -148.83 340.84 -160.79 371.46 -160.79 C 402.08 -160.79 426.9
    -148.83 426.9 -134.09 Z M 371.46 -134.09"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 336.87 -129.51)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="28.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Data Labelling</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 408.78 -214.97 C 408.78 -196.07 383.96 -180.75 353.34
    -180.75 C 322.72 -180.75 297.9 -196.07 297.9 -214.97 C 297.9 -233.87 322.72 -249.19
    353.34 -249.19 C 383.96 -249.19 408.78 -233.87 408.78 -214.97 Z M 353.34 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 318.75 -205.07)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Generate
    Synthetic Data</foreignobject></g> <path d="M 138.6 -112.83 L 188.92 -88.45" style="fill:none"><g
    transform="matrix(0.89996 0.43597 -0.43597 0.89996 188.92 -88.45)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path d="M 129.59
    -189.99 L 214.15 -89.55" style="fill:none"><g transform="matrix(0.64403 0.765
    -0.765 0.64403 214.15 -89.55)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 231.31 -189.9 L 232.75 -90.33" style="fill:none"><g transform="matrix(0.01447
    0.9999 -0.9999 0.01447 232.75 -90.33)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 332.07 -115.01 L 277.26 -88.45"
    style="fill:none"><g transform="matrix(-0.89996 0.43597 -0.43597 -0.89996 277.26
    -88.45)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83
    -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 328.29 -184.16 L 251.41 -89.58" style="fill:none"><g transform="matrix(-0.63078
    0.77596 -0.77596 -0.63078 251.41 -89.58)"><path d="M 3.32 0 C 1.94 0.28 -0.55
    0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28
    3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path d="M 506.76 -47.36
    L 370.24 -47.36 C 367.18 -47.36 364.71 -49.84 364.71 -52.89 L 364.71 -81.19 C
    364.71 -84.25 367.18 -86.73 370.24 -86.73 L 506.76 -86.73 C 509.82 -86.73 512.3
    -84.25 512.3 -81.19 L 512.3 -52.89 C 512.3 -49.84 509.82 -47.36 506.76 -47.36
    Z M 364.71 -86.73"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32 -70.66)"
    fill="#000000" stroke="#000000"><foreignobject width="138.37" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Pre-processing</foreignobject></g> <g
    fill="#CCFFCC"><path d="M 369.41 0 C 369.41 13.84 344.59 25.06 313.97 25.06 C
    283.35 25.06 258.53 13.84 258.53 0 C 258.53 -13.84 283.35 -25.06 313.97 -25.06
    C 344.59 -25.06 369.41 -13.84 369.41 0 Z M 313.97 0"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 279.38 3.42)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Background
    Removal</foreignobject></g> <g fill="#CCFFCC"><path d="M 397.09 67.04 C 397.09
    80.88 372.26 92.1 341.64 92.1 C 311.02 92.1 286.2 80.88 286.2 67.04 C 286.2 53.2
    311.02 41.99 341.64 41.99 C 372.26 41.99 397.09 53.2 397.09 67.04 Z M 341.64 67.04"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 307.05 70.46)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="26.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Removing
    Motion Blur</foreignobject></g> <g fill="#CCFFCC"><path d="M 519.48 67.04 C 519.48
    80.45 494.66 91.33 464.04 91.33 C 433.41 91.33 408.59 80.45 408.59 67.04 C 408.59
    53.63 433.41 42.76 464.04 42.76 C 494.66 42.76 519.48 53.63 519.48 67.04 Z M 464.04
    67.04"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 429.44 69.92)" fill="#000000"
    stroke="#000000"><foreignobject width="69.19" height="25.12" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Image Enhancement</foreignobject></g> <g fill="#CCFFCC"><path
    d="M 563.13 0 C 563.13 8.35 538.31 15.11 507.69 15.11 C 477.07 15.11 452.24 8.35
    452.24 0 C 452.24 -8.35 477.07 -15.11 507.69 -15.11 C 538.31 -15.11 563.13 -8.35
    563.13 0 Z M 507.69 0"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 473.1 -3.61)"
    fill="#000000" stroke="#000000"><foreignobject width="69.19" height="12.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Denoising</foreignobject></g> <path d="M 349.93
    -19.35 L 398.52 -45.51" style="fill:none"><g transform="matrix(0.88054 -0.47397
    0.47397 0.88054 398.52 -45.51)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path
    d="M 359.03 42.97 L 422.14 -44.39" style="fill:none"><g transform="matrix(0.58559
    -0.81061 0.81061 0.58559 422.14 -44.39)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 492.42 -14.8 L 461.48 -44.77" style="fill:none"><g
    transform="matrix(-0.7182 -0.69585 0.69585 -0.7182 461.48 -44.77)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><path d="M 459.38
    42.57 L 442.92 -43.82" style="fill:none"><g transform="matrix(-0.18707 -0.98235
    0.98235 -0.18707 442.92 -43.82)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83 -2.21
    2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g
    stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(1.0 0.0 0.0 1.0 356.59 -67.04)"><path d="M 0 4.05 L 7.01 0 L
    0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 307.16
    -67.04 L 356.13 -67.04" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 307.16 -67.04 L 356.13 -67.04" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 307.16 -67.04 L 358.2
    -67.04" style="fill:none"></path></g><g fill="#FFCCCC"><path d="M 506.76 -252.77
    L 370.24 -252.77 C 367.18 -252.77 364.71 -255.25 364.71 -258.31 L 364.71 -286.61
    C 364.71 -289.67 367.18 -292.14 370.24 -292.14 L 506.76 -292.14 C 509.82 -292.14
    512.3 -289.67 512.3 -286.61 L 512.3 -258.31 C 512.3 -255.25 509.82 -252.77 506.76
    -252.77 Z M 364.71 -292.14"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32
    -269.04)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Apply Deep Learning based
    Classifiers</foreignobject></g> <g fill="#FFCCCC"><path d="M 506.76 -333.65 L
    370.24 -333.65 C 367.18 -333.65 364.71 -336.13 364.71 -339.19 L 364.71 -367.49
    C 364.71 -370.55 367.18 -373.02 370.24 -373.02 L 506.76 -373.02 C 509.82 -373.02
    512.3 -370.55 512.3 -367.49 L 512.3 -339.19 C 512.3 -336.13 509.82 -333.65 506.76
    -333.65 Z M 364.71 -373.02"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 369.32
    -349.92)" fill="#000000" stroke="#000000"><foreignobject width="138.37" height="26.21"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Evaluation of the Model</foreignobject></g><g
    stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(0.0 -1.0 1.0 0.0 438.5 -244.66)"><path d="M 0 4.05 L 7.01 0
    L 0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 438.5
    -87.01 L 438.5 -244.19" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 438.5 -87.01 L 438.5 -244.19" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 438.5 -87.01 L 438.5
    -246.27" style="fill:none"></path></g><g stroke-width="0.6pt" stroke-dasharray="none"
    stroke-dashoffset="0.0pt" stroke-linejoin="miter" transform="matrix(0.0 -1.0 1.0
    0.0 438.5 -325.54)"><path d="M 0 4.05 L 7.01 0 L 0 -4.05 Z" style="fill:none"></path></g><g
    stroke-width="3.0pt"><path d="M 438.5 -292.42 L 438.5 -325.08" style="fill:none"></path></g><g
    stroke-width="1.4pt" stroke="#FFFFFF"><path d="M 438.5 -292.42 L 438.5 -325.08"
    style="fill:none"></path></g><g stroke-width="2.0pt" fill="#FFFFFF" stroke="#FFFFFF"
    color="#FFFFFF"><path d="M 438.5 -292.42 L 438.5 -327.15" style="fill:none"></path></g>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`<svg   height="465.68" overflow="visible" version="1.1" width="619.13"><g
    transform="translate(0,465.68) matrix(1 0 0 -1 0 0) translate(55.72,0) translate(0,373.3)"><g
    stroke="#000000" fill="#000000"><g stroke-width="0.4pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -34.59 1.6)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">收集数据</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 87.8 3.34)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="26.06" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">使用公共数据</foreignobject></g>
    <g fill="#FFCCCC"><path d="M 123.61 -47.36 L -12.91 -47.36 C -15.97 -47.36 -18.45
    -49.84 -18.45 -52.89 L -18.45 -81.19 C -18.45 -84.25 -15.97 -86.73 -12.91 -86.73
    L 123.61 -86.73 C 126.67 -86.73 129.15 -84.25 129.15 -81.19 L 129.15 -52.89 C
    129.15 -49.84 126.67 -47.36 123.61 -47.36 Z M -18.45 -86.73"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -13.84 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">数据采集</foreignobject></g>
    <path d="M 17.8 -21.56 L 36.76 -44.52" style="fill:none"><g transform="matrix(0.63672
    -0.77109 0.77109 0.63672 36.76 -44.52)"><path d="M 3.32 0 C 1.94 0.28 -0.55 0.83
    -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08 C -0.55 -0.83 1.94 -0.28 3.32
    0 Z" style="stroke:none"></path></g><path d="M 99.41 -22.98 L 77.65 -44.73" style="fill:none"><g
    transform="matrix(-0.70717 -0.70705 0.70705 -0.70717 77.65 -44.73)"><path d="M
    3.32 0 C 1.94 0.28 -0.55 0.83 -2.21 2.08 C -0.83 0.55 -0.83 -0.55 -2.21 -2.08
    C -0.55 -0.83 1.94 -0.28 3.32 0 Z" style="stroke:none"></path></g><g fill="#FFCCCC"><path
    d="M 301.35 -47.36 L 164.83 -47.36 C 161.77 -47.36 159.29 -49.84 159.29 -52.89
    L 159.29 -81.19 C 159.29 -84.25 161.77 -86.73 164.83 -86.73 L 301.35 -86.73 C
    304.41 -86.73 306.89 -84.25 306.89 -81.19 L 306.89 -52.89 C 306.89 -49.84 304.41
    -47.36 301.35 -47.36 Z M 159.29 -86.73"></path></g><g transform="matrix(1.0 0.0
    0.0 1.0 163.9 -70.66)" fill="#000000" stroke="#000000"><foreignobject width="138.37"
    height="12.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">数据集准备</foreignobject></g>
    <g stroke-width="0.6pt" stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linejoin="miter"
    transform="matrix(1.0 0.0 0.0 1.0 151.17 -67.04)"><path d="M 0 4.05 L 7.01 0 L
    0 -4.05 Z" style="fill:none"></path></g><g stroke-width="3.0pt"><path d="M 129.42
    -67.04 L 150.71 -67.04" style="fill:none"></path></g><g stroke-width="1.4pt" stroke="#FFFFFF"><path
    d="M 129.42 -67.04 L 150.71 -67.04" style="fill:none"></path></g><g stroke-width="2.0pt"
    fill="#FFFFFF" stroke="#FFFFFF" color="#FFFFFF"><path d="M 129.42 -67.04 L 152.79
    -67.04" style="fill:none"></path></g><g fill="#CCFFCC"><path d="M 150.16 -134.09
    C 150.16 -115.19 125.34 -99.87 94.72 -99.87 C 64.1 -99.87 39.27 -115.19 39.27
    -134.09 C 39.27 -152.99 64.1 -168.31 94.72 -168.31 C 125.34 -168.31 150.16 -152.99
    150.16 -134.09 Z M 94.72 -134.09"></path></g><g transform="matrix(1.0 0.0 0.0
    1.0 60.13 -124.19)" fill="#000000" stroke="#000000"><foreignobject width="69.19"
    height="39.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">颜色模型转换</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 164 -214.97 C 164 -200.22 139.18 -188.27 108.56 -188.27
    C 77.93 -188.27 53.11 -200.22 53.11 -214.97 C 53.11 -229.72 77.93 -241.67 108.56
    -241.67 C 139.18 -241.67 164 -229.72 164 -214.97 Z M 108.56 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 73.96 -210.39)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="28.54" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">图像缩放</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 286.39 -214.97 C 286.39 -201.27 261.57 -190.17 230.95
    -190.17 C 200.33 -190.17 175.5 -201.27 175.5 -214.97 C 175.5 -228.66 200.33 -239.77
    230.95 -239.77 C 261.57 -239.77 286.39 -228.66 286.39 -214.97 Z M 230.95 -214.97"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 196.35 -211.73)" fill="#000000" stroke="#000000"><foreignobject
    width="69.19" height="25.85" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">图像增强</foreignobject></g>
    <g fill="#CCFFCC"><path d="M 426.9 -134.09 C 426.9 -119.34 402.08 -107.39 371.46
    -107.39 C 340.84 -107.39 316.02 -119.34 316.02 -134.09 C 316.02 -148.83 340.84
    -160.79 371'
- en: 'Figure 2: A summary workflow of weed detection techniques using deep learning.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的杂草检测技术的工作流程总结。
- en: Conventional ML techniques require substantial domain expertise to construct
    a feature extractor from raw data. On the other hand, the DL approach uses a representation-learning
    method where a machine can automatically discover the discriminative features
    from raw data for classification or object detection problems [[91](#bib.bibx91)].
    A machine can learn to classify directly from images, text and sounds [[120](#bib.bibx120)].
    The ability to extract the features that best suit the task automatically is also
    known as feature learning. As deep learning is a hierarchical architecture of
    learning, the features of the higher levels of the hierarchy are composed of lower-level
    features [[110](#bib.bibx110), [58](#bib.bibx58)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的机器学习技术需要大量的领域专业知识来从原始数据中构建特征提取器。另一方面，深度学习方法使用表示学习的方法，机器可以自动从原始数据中发现用于分类或物体检测问题的区分特征[[91](#bib.bibx91)]。机器可以直接从图像、文本和声音中学习进行分类[[120](#bib.bibx120)]。自动提取最适合任务的特征的能力也被称为特征学习。由于深度学习是一个层次化的学习架构，因此层次较高的特征由层次较低的特征组成[[110](#bib.bibx110),
    [58](#bib.bibx58)]。
- en: Several popular and high performing network architectures are available in deep
    learning. Two of the frequently used architectures are Convolutional Neural Networks
    (CNNs) and Recurrent Neural Networks (RNNs) [[91](#bib.bibx91), [60](#bib.bibx60)].
    Although CNNs are used for other types of data, the most widespread use of CNNs
    is to analyse and classify images. The word convolution refers to the filtering
    process. A stack of convolutional layers is the basis of CNN. Each layer receives
    the input data, transform, or convolve them and output to the next layer. This
    convolutional operation eventually simplifies the data so that it can be better
    processed and understood. RNNs have a built-in feedback loop, which allows them
    to act as a forecasting engine. Feed-forward or CNN take a fixed size input and
    produces a fixed size output. The signal flow of the feed-forward network is unidirectional,
    i.e., from input to output. They cannot even capture the sequence or time-series
    information. RNNs overcome the limitation. In RNN, the current inputs and outputs
    of the network are influenced by prior input. Long Short-Term Memory (LSTM) is
    a type of RNN [[91](#bib.bibx91)], which has a memory cell to remember important
    prior information, thus can help improving the performance. Depending on the network
    architecture, DL has several components like convolutional layers, pooling layers,
    activation functions, dense/fully connected layers, encoder/decoder schemes, memory
    cells, gates etc. [[120](#bib.bibx120)].
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中有几种流行且高效的网络架构。两个常用的架构是卷积神经网络（CNNs）和递归神经网络（RNNs）[[91](#bib.bibx91), [60](#bib.bibx60)]。虽然CNNs也用于其他类型的数据，但CNNs最广泛的应用是分析和分类图像。卷积这个词指的是过滤过程。卷积层的堆叠是CNN的基础。每一层接收输入数据，进行变换或卷积，并输出到下一层。这种卷积操作最终简化了数据，使其能够更好地处理和理解。RNNs具有内置的反馈循环，使它们可以作为预测引擎。前馈网络或CNN接收固定大小的输入并产生固定大小的输出。前馈网络的信号流是单向的，即从输入到输出。它们甚至无法捕捉序列或时间序列信息。RNNs克服了这一限制。在RNN中，网络的当前输入和输出受到先前输入的影响。长短期记忆（LSTM）是一种RNN[[91](#bib.bibx91)]，它具有记忆单元来记住重要的先前信息，从而可以帮助提高性能。根据网络架构，深度学习有多个组件，如卷积层、池化层、激活函数、全连接层、编码器/解码器方案、记忆单元、门控等[[120](#bib.bibx120)]。
- en: For image classification, object detection, and localisation, DL algorithms
    have many advantages over traditional ML approaches. Because of the strong feature
    learning capabilities, DL methods can effectively extract discriminative features
    of crops and weeds. Also, with increasing data, the performance of traditional
    ML approaches has become saturated. Using large dataset, DL techniques show superior
    performance compared to traditional ML techniques [[5](#bib.bibx5)]. This characteristic
    is leading to the increasing application of DL approaches. Many of the research
    reports in Section [10](#S10 "10 Deep Learning Architecture ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images") show comparisons between
    DL and other ML approaches to detect weeds in crops. Figure [2](#S3.F2 "Figure
    2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images") gives an overview of DL-based weed
    detection and recognition techniques.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像分类、目标检测和定位，深度学习算法相比传统机器学习方法有很多优势。由于强大的特征学习能力，深度学习方法可以有效提取作物和杂草的辨别特征。此外，随着数据量的增加，传统机器学习方法的性能已经趋于饱和。使用大数据集时，深度学习技术相比传统机器学习技术表现出更优的性能[[5](#bib.bibx5)]。这一特性导致了深度学习方法应用的增加。许多在[10](#S10
    "10 Deep Learning Architecture ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images")节中的研究报告展示了深度学习与其他机器学习方法在作物杂草检测方面的比较。图[2](#S3.F2 "Figure
    2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")概述了基于深度学习的杂草检测和识别技术。
- en: Not all the steps outlined in Figure [2](#S3.F2 "Figure 2 ‣ 3 Traditional ML-
    vs DL-based Weed Detection Methods ‣ A Survey of Deep Learning Techniques for
    Weed Detection from Images") need to be present in every method. Four major steps
    are followed in this process. They are Data Acquisition, Dataset Preparation/Image
    Pre-processing, Classification and Evaluation. In this paper, we describe the
    steps used in different research work to discriminate between weeds and crops
    using DL techniques.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S3.F2 "Figure 2 ‣ 3 Traditional ML- vs DL-based Weed Detection Methods
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")中概述的所有步骤并不需要在每种方法中都出现。该过程遵循四个主要步骤：数据采集、数据集准备/图像预处理、分类和评估。在本文中，我们描述了在不同研究工作中使用的步骤，以便利用深度学习技术区分杂草和作物。
- en: 4 Paper Selection Criteria in this Survey
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 本调查中的论文选择标准
- en: 'To overview the state of research, we have undertaken a comprehensive literature
    review. The process involved two major steps: (i) searching and selecting related
    studies and (ii) detailed analysis of these studies. The main research question
    is: What is the role of deep learning techniques for detecting, localising and
    classifying weeds in crops? For collecting the related work based on this research
    question, we applied a keyword-based search in Google Scholar, Web of Science,
    IEEE Xplore, Scopus, ScienceDirect, Multidisciplinary Digital Publishing Institute
    (MDPI), Springer and Murdoch University Library databases for journal articles
    and conference papers. We have applied a keyword search from 2010 to 30 August
    2020\. Table [1](#S4.T1 "Table 1 ‣ 4 Paper Selection Criteria in this Survey ‣
    A Survey of Deep Learning Techniques for Weed Detection from Images") shows the
    number of search results for the search query.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了概述研究状态，我们进行了全面的文献综述。该过程包括两个主要步骤：（i）搜索和选择相关研究，以及（ii）对这些研究的详细分析。主要研究问题是：深度学习技术在检测、定位和分类作物中的杂草方面的作用是什么？为了收集与这一研究问题相关的工作，我们在Google
    Scholar、Web of Science、IEEE Xplore、Scopus、ScienceDirect、多学科数字出版研究所（MDPI）、Springer和Murdoch大学图书馆数据库中应用了基于关键词的搜索，以查找期刊文章和会议论文。我们从2010年到2020年8月30日进行了关键词搜索。表[1](#S4.T1
    "Table 1 ‣ 4 Paper Selection Criteria in this Survey ‣ A Survey of Deep Learning
    Techniques for Weed Detection from Images")显示了搜索查询的结果数量。
- en: 'Table 1: Number of documents resulted for the queries indicated'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：指定查询的文献数量
- en: '| No. | Academic Research Databases | Search Query | Number of Retrieved Documents
    |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| No. | 学术研究数据库 | 搜索查询 | 检索到的文献数量 |'
- en: '| 1. | Google Scholar | [“Weed Detection” OR “Weed management” OR “Weed Classification”]
    AND [“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”] | 998
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 1. | Google Scholar | [“杂草检测” OR “杂草管理” OR “杂草分类”] AND [“深度学习” OR “深度机器学习”
    OR “深度神经网络”] | 998 |'
- en: '| 2. | Web of Science | (Weed Detection OR Weed management OR Weed Classification)
    AND (Deep Learning OR Deep Machine Learning OR Deep Neural Network) | 124 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 2. | Web of Science | (杂草检测 OR 杂草管理 OR 杂草分类) AND (深度学习 OR 深度机器学习 OR 深度神经网络)
    | 124 |'
- en: '| 3. | IEEE Xplore | (((“All Metadata”:“Deep Learning”) OR “All Metadata”:“Deep
    Machine Learning”) OR “All Metadata”:“Deep Neural Network”) AND Weed detection
    | 22 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3. | IEEE Xplore | （（（“所有元数据”:“深度学习”） OR “所有元数据”:“深度机器学习”） OR “所有元数据”:“深度神经网络”）
    AND 杂草检测 | 22 |'
- en: '| 4. | ScienceDirect | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 87
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 4. | ScienceDirect | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习” OR “深度机器学习”
    OR “深度神经网络”） | 87 |'
- en: '| 5. | Scopus | ((Weed AND detection) OR (Weed AND Management) OR (Weed AND
    Classification)) AND ((Deep AND Learning) OR (Deep AND Machine AND Learning) OR
    (Deep AND Neural AND Network)) | 118 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 5. | Scopus | （（杂草 AND 检测） OR （杂草 AND 管理） OR （杂草 AND 分类）） AND （（深度 AND 学习）
    OR （深度 AND 机器 AND 学习） OR （深度 AND 神经 AND 网络）） | 118 |'
- en: '| 6. | MDPI | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 76
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 6. | MDPI | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习” OR “深度机器学习” OR “深度神经网络”）
    | 76 |'
- en: '| 7. | SpringerLink | (“Weed Detection” OR “Weed management” OR “Weed Classification”)
    AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep Neural Network”) | 46
    |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 7. | SpringerLink | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习” OR “深度机器学习”
    OR “深度神经网络”） | 46 |'
- en: '| 8. | Murdoch University Library | (“Weed Detection” OR “Weed management”
    OR “Weed Classification”) AND (“Deep Learning” OR “Deep Machine Learning” OR “Deep
    Neural Network”) | 179 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 8. | Murdoch University Library | （“杂草检测” OR “杂草管理” OR “杂草分类”） AND （“深度学习”
    OR “深度机器学习” OR “深度神经网络”） | 179 |'
- en: 'After searching the above databases, duplicated documents were removed: that
    provided 988 documents. We further identified and counted those using DL-based
    methodology. In Figure [3](#S4.F3 "Figure 3 ‣ 4 Paper Selection Criteria in this
    Survey ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    we show the total number of papers which used DL between 2010 to 30 August 2020\.
    This shows that before 2016, the number of publications in this area was very
    small, but that there is an upward trend in the number of papers from 2016\. For
    this reason, articles published from 2016 and onward were used in this survey.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索了上述数据库后，重复的文献被移除：这样得到了988篇文献。我们进一步确定并统计了使用深度学习（DL）方法的文献。在图 [3](#S4.F3 "图
    3 ‣ 本调查中的4篇论文选择标准 ‣ 一项关于基于深度学习的图像杂草检测技术的调查") 中，我们展示了2010年到2020年8月30日之间使用深度学习的论文总数。这显示在2016年之前，该领域的出版物数量非常少，但从2016年开始，论文数量呈上升趋势。因此，本调查中使用了2016年及之后发表的文章。
- en: <svg   height="208.1" overflow="visible" version="1.1" width="467.38"><g transform="translate(0,208.1)
    matrix(1 0 0 -1 0 0) translate(49.37,0) translate(0,24.32) matrix(1.0 0.0 0.0
    1.0 -49.37 -24.32)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(84.18,0) translate(0,24.32)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -13.84 -19.71)" fill="#000000"
    stroke="#000000"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2010</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 20.97 -19.71)" fill="#000000" stroke="#000000"><foreignobject width="27.67"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2011</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.78 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2012</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 90.6 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2013</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.41 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2014</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.22 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 195.03 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 229.84 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 264.65 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 299.46 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 334.27 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.62 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 44.36)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$100$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 93.17)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$200$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 141.99)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$300$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -14.21 10.26)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$11$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 24.06 6.35)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$3$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 58.87 6.84)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$4$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 93.68 8.79)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$8$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.04 10.75)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$12$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 159.85 11.23)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$13$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 194.66 27.83)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$47$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 229.47 49.31)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$91$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 260.82 99.1)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="20.76" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$193$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 295.63 166.46)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$331$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 330.44 139.13)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="20.76" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$275$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 108.26 5.38)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 143.07 5.38)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 177.88 5.86)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 212.69 7.33)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$5$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 244.05 12.7)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$16$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 278.86 19.53)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$30$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 313.67 27.34)" fill="#FF0000" stroke="#FF0000" color="#FF0000"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$46$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 348.48 23.44)" fill="#FF0000" stroke="#FF0000"
    color="#FF0000"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$38$</foreignobject></g><g transform="matrix(0.0
    1.0 -1.0 0.0 -69.96 19.57)" fill="#000000" stroke="#000000"><foreignobject width="138.99"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Number of
    Publication</foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0
    0.0 0.0 1.0 51.43 155.96)"><g  transform="matrix(1 0 0 -1 0 9.365)"><g  transform="matrix(1
    0 0 1 0 10.12)"><g transform="matrix(1 0 0 -1 8.43 0) translate(61.4,0) matrix(1.0
    0.0 0.0 1.0 -58.63 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="117.27"
    height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Searched
    document</foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 139.66 0) translate(52.79,0) matrix(1.0 0.0 0.0 1.0
    -50.02 -3.77)" fill="#000000" stroke="#000000"><foreignobject width="100.05" height="9.61"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">DL based article</foreignobject></g></g></g></g></g></g></svg>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="208.1" overflow="visible" version="1.1" width="467.38"><g transform="translate(0,208.1)
    matrix(1 0 0 -1 0 0) translate(49.37,0) translate(0,24.32) matrix(1.0 0.0 0.0
    1.0 -49.37 -24.32)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1
    0 0 1 0 0) translate(84.18,0) translate(0,24.32)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -13.84 -19.71)" fill="#000000"
    stroke="#000000"><foreignobject width="27.67" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">2010</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 20.97 -19.71)" fill="#000000" stroke="#000000"><foreignobject width="27.67"
    height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2011</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 55.78 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2012</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 90.6 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2013</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.41 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2014</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 160.22 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2015</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 195.03 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2016</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 229.84 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2017</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 264.65 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2018</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 299.46 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2019</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 334.27 -19.71)" fill="#000000" stroke="#000000"><foreignobject
    width="27.67" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">2020</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -46.62 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 44.36)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$100$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 93.17)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$200$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -60.46 141.99)" fill="#000000" stroke="#000000"><foreignobject
    width="20.76" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$300$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -14.21 10.26)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$11$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 24.06 6.35)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$3$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 58.87 6.84)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="6.92" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$4$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 93.68 8.79)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$8$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 125.04 10.75)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$12$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 159.85 11.23)" fill="#0000FF" stroke="#0000FF" color="#0000FF"><foreignobject
    width="13.84" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$13$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 194.66 27.83)" fill="#0000FF" stroke="#0000FF"
    color="#0000FF"><foreignobject width="13.84" height="8.92" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$47$</foreignobject></g><g transform
- en: 'Figure 3: The number of selected publications on DL-based weed detection approach
    from 2010 to 30 August 2020'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：2010年至2020年8月30日关于深度学习基础的杂草检测方法的所选出版物数量
- en: 5 An Overview and Taxonomy of Deep Learning-based Weed Detection Approaches
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 深度学习基础的杂草检测方法概述及分类
- en: An overall taxonomy of DL-based weed detection techniques is shown in Figure
    [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based Weed
    Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images"). The papers covered in this survey are categorised using this taxonomy
    and listed in Table LABEL:tab:different_DL_approach.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习基础的杂草检测技术的整体分类见图 [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep
    Learning-based Weed Detection Approaches ‣ A Survey of Deep Learning Techniques
    for Weed Detection from Images")。本文综述中涉及的论文按照此分类法进行分类，并列在表格 LABEL:tab:different_DL_approach
    中。
- en: '![Refer to caption](img/a10571ca5061433f93182d580e47d020.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a10571ca5061433f93182d580e47d020.png)'
- en: 'Figure 4: An overall taxonomy of deep learning-based weed detection techniques'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：深度学习基础的杂草检测技术的整体分类
- en: The related publications have been analysed based on the taxonomy in Figure
    [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based Weed
    Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images"). Here, the data acquisition process, sensors and mounting vehicles
    are highlighted. Moreover, an overview of the dataset preparation approaches,
    i.e., image pre-processing, data generation and annotation are also given. While
    analysing these publications, it has been found that the related works either
    generate a weed map for the target site or a classification for each of the plants
    (crops/weeds). For developing the classifiers, the researchers applied supervised,
    unsupervised or semi-supervised learning approaches. Depending on the learning
    approaches and the research goal, different DL architectures were used. An overview
    of the related research is provided in Table LABEL:tab:different_DL_approach.
    It shows the crop and weed species selected for experimental work, the steps taken
    to collect and prepare the datasets, and the DL methods applied in the research.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 相关文献已经基于图 [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based
    Weed Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") 中的分类法进行了分析。此处重点介绍了数据获取过程、传感器和安装车辆。此外，还概述了数据集准备方法，即图像预处理、数据生成和注释。在分析这些出版物时发现，相关工作要么为目标地点生成杂草地图，要么对每种植物（作物/杂草）进行分类。在开发分类器时，研究人员应用了监督学习、无监督学习或半监督学习方法。根据学习方法和研究目标，使用了不同的深度学习架构。相关研究的概述见表格
    LABEL:tab:different_DL_approach。该表显示了用于实验的作物和杂草种类、收集和准备数据集的步骤以及研究中应用的深度学习方法。
- en: 'Table 2: An overview of different DL approaches used in weed detection'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同深度学习方法在杂草检测中的概述
- en: '|  |  |  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Reference | Crop | Weed Species | DL Architectures Applied | Operations Performed
    (based on Figure [4](#S5.F4 "Figure 4 ‣ 5 An Overview and Taxonomy of Deep Learning-based
    Weed Detection Approaches ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images")) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 作物 | 杂草种类 | 应用的DL架构 | 执行的操作（基于图 [4](#S5.F4 "Figure 4 ‣ 5 An Overview
    and Taxonomy of Deep Learning-based Weed Detection Approaches ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| [[40](#bib.bibx40)] | Tomato, Cotton | Black nightshade, velvetleaf | Modified
    Xception, Inception-ResNet, VGGNet, MobileNet, DenseNet | DC; (IP, IA, ILA); PBC
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| [[40](#bib.bibx40)] | 西红柿、棉花 | 黑夜shade、天鹅绒叶 | 修改版Xception、Inception-ResNet、VGGNet、MobileNet、DenseNet
    | DC；(IP, IA, ILA)；PBC |'
- en: '| [[164](#bib.bibx164)] | Sugar beet, Oilseed | Not specified | FCN | (DC,
    FR); (IP, IA, ILA); PBC |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| [[164](#bib.bibx164)] | 甜菜、油菜 | 未指定 | FCN | (DC, FR)；(IP, IA, ILA)；PBC |'
- en: '| [[89](#bib.bibx89)] | Canola, corn, radish | Not specified | Filtered Local
    Binary Pattern with Contour Mask and Coefficient k (k-FLBPCM), VGG-16, VGG-19,
    ResNet-50, Inception-v3 | (ATV, MC); (IP, IA, ILA); PBC |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bibx89)] | 油菜、玉米、萝卜 | 未指定 | 带轮廓掩膜和系数 k (k-FLBPCM) 的滤波局部二值模式、VGG-16、VGG-19、ResNet-50、Inception-v3
    | (ATV, MC)；(IP, IA, ILA)；PBC |'
- en: '| [[61](#bib.bibx61)] | Not specified | Chinee apple, Lantana, Parkinsonia,
    Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed | Inception-v3,
    ResNet-50, DenseNet-202, Inception-ResNet-v2, GCN | PD; IP; PBC |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bibx61)] | 未指定 | 中国苹果、兰塔那、帕金森树、帕尔滕草、刺槐、橡胶藤、暹罗草、蛇草 | Inception-v3、ResNet-50、DenseNet-202、Inception-ResNet-v2、GCN
    | PD；IP；PBC |'
- en: '| [[161](#bib.bibx161)] | Carrot | Not specified | SegNet-512, SegNet-256 |
    PD; IA; PBC |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [[161](#bib.bibx161)] | 胡萝卜 | 未指定 | SegNet-512, SegNet-256 | PD; IA; PBC
    |'
- en: '| [[65](#bib.bibx65)] | Rice | Leptochloa chinensis, Cyperus iria, Digitaria
    sanguinalis (L). Scop, Barnyard Grass | FCN | (DC, UAV); (IP, PLA); WM |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [[65](#bib.bibx65)] | 水稻 | 狗尾草、稗草、马唐、雀稗 | FCN | (DC, UAV); (IP, PLA); WM
    |'
- en: '| [[47](#bib.bibx47)] | Sugar beet | Convolvulus sepium (hedge bindweed) |
    YOLO-v3, tiny YOLO-v3 | DC; (IA, BBA); PBC |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [[47](#bib.bibx47)] | 甜菜 | 牵牛花 | YOLO-v3, tiny YOLO-v3 | DC; (IA, BBA); PBC
    |'
- en: '| [[146](#bib.bibx146)] | Soybean | Waterhemp, Palmer amaranthus, common lambsquarters,
    velvetleaf, foxtail species | Single-Shot Detector (SSD), Faster R-CNN | (DC,
    UAV); (IP, IA, BBA); WM |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| [[146](#bib.bibx146)] | 大豆 | 水藜、苋科、田间蕹菜、马唐、狗尾草 | Single-Shot Detector (SSD),
    Faster R-CNN | (DC, UAV); (IP, IA, BBA); WM |'
- en: '| [[71](#bib.bibx71)] | Corn, lettuce, radish | Cirsium setosum, Chenopodium
    album, bluegrass, sedge, other unspecified weed | GCN | PD; (IP, ILA); PBC |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| [[71](#bib.bibx71)] | 玉米、生菜、萝卜 | 刺毛蓟、苋属、蓝草、莎草、其他未指定杂草 | GCN | PD; (IP, ILA);
    PBC |'
- en: '| [[18](#bib.bibx18)] | Sugar Beets, Carrots, Onions | Not specified | SegNet
    | PD; PLA; PBC |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| [[18](#bib.bibx18)] | 甜菜、胡萝卜、洋葱 | 未指定 | SegNet | PD; PLA; PBC |'
- en: '| [[168](#bib.bibx168)] | Paddy | Alternanthera philoxeroides, Eclipta prostrata,
    Ludwigia adscendens, Sagittaria trifolia, Echinochloa crus-galli, Leptochloa chinensis
    | AlexNet | DC; ILA; PBC |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| [[168](#bib.bibx168)] | 水稻田 | 水莲、秋水仙、鼓油菜、三叶箭、稗草、马唐 | AlexNet | DC; ILA; PBC
    |'
- en: '| [[172](#bib.bibx172)] | Wheat | Cirsium Setosum, Descurainia Sophia, Euphorbia
    Helioscopia, Veronica Didyma, Avena Fatu | YOLO-v3, Tiny YOLO-v3 | (DC, UAV);
    (IP, PLA); PBC |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| [[172](#bib.bibx172)] | 小麦 | 刺毛蓟、风毛菊、旋覆花、马唐、稗草 | YOLO-v3, Tiny YOLO-v3 |
    (DC, UAV); (IP, PLA); PBC |'
- en: '| [[102](#bib.bibx102)] | Sugar beet | Dicot weeds, grass weeds | FCN | MC;
    (IP, PLA); PBC |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bibx102)] | 甜菜 | 双子叶杂草、禾本科杂草 | FCN | MC; (IP, PLA); PBC |'
- en: '| [[159](#bib.bibx159)] | Not specifies | 12 species of “Plant Seedlings dataset”,
    21 species of “CNU weeds dataset” | NASNet, ResNet, Inception–ResNet, MobileNet,
    VGGNet | DC; ILA, PD |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| [[159](#bib.bibx159)] | 未指定 | “植物幼苗数据集”12种、“CNU杂草数据集”21种 | NASNet, ResNet,
    Inception–ResNet, MobileNet, VGGNet | DC; ILA, PD |'
- en: '| [[119](#bib.bibx119)] | Not specified | Scentless Mayweed, Chickweed, Cranesbill,
    Shepherd’s Purse, Cleavers, Charlock, Fat Hen, Maise, Sugar beet, Common wheat,
    Black-grass, Loose Silky-bent | Mask R-CNN | PD; PLA; PBC |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bibx119)] | 未指定 | 无香菊、鸭跖草、老鹳草、牧草繁缕、贪夜蛾、婆罗门参、麻疯树、蛇草 | Mask R-CNN
    | PD; PLA; PBC |'
- en: '| [[127](#bib.bibx127)] | Sugar beet | Not specified | DeepLab-v3, SegNet,
    U-Net | (MC, UAV); (IP, PLA) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| [[127](#bib.bibx127)] | 甜菜 | 未指定 | DeepLab-v3, SegNet, U-Net | (MC, UAV);
    (IP, PLA) |'
- en: '| [[114](#bib.bibx114)] | Lettuce | Not specified | YOLO-v3, Mask R-CNN, SVM
    | (MC, UAV); (IP, PLA) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| [[114](#bib.bibx114)] | 生菜 | 未指定 | YOLO-v3, Mask R-CNN, SVM | (MC, UAV);
    (IP, PLA) |'
- en: '| [[86](#bib.bibx86)] | Grasslands | Rumex obtusifolius | VGG-16 | (DC, UAV);
    (IP, PLA) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| [[86](#bib.bibx86)] | 草地 | 钝叶酸模草 | VGG-16 | (DC, UAV); (IP, PLA) |'
- en: '| [[142](#bib.bibx142)] | Strawberry, Tomato | Goosegrass | Tiny YOLO-v3 |
    DC; (IP, BBA) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| [[142](#bib.bibx142)] | 草莓、番茄 | 鹅草 | Tiny YOLO-v3 | DC; (IP, BBA) |'
- en: '| [[122](#bib.bibx122)] | Not specified | Colchicum autumnale | U-Net | (DC,
    UAV); (IP, IA, BBA) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| [[122](#bib.bibx122)] | 未指定 | 秋水仙素 | U-Net | (DC, UAV); (IP, IA, BBA) |'
- en: '| [[31](#bib.bibx31)] | Carrot | Not specified | Faster YOLO-v3, tiny YOLO-v3
    | (DC, FR); ILA; PBC |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| [[31](#bib.bibx31)] | 胡萝卜 | 未指定 | Faster YOLO-v3, tiny YOLO-v3 | (DC, FR);
    ILA; PBC |'
- en: '| [[117](#bib.bibx117)] | Blueberry | Not specified | Faster R-CNN, YOLO-v3,
    ResNet-50, ResNet-101, Darknet-53 | (DC, ATV); (IP, ILA); PBC |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bibx117)] | 蓝莓 | 未指定 | Faster R-CNN, YOLO-v3, ResNet-50, ResNet-101,
    Darknet-53 | (DC, ATV); (IP, ILA); PBC |'
- en: '| [[116](#bib.bibx116)] | Pepper | Portulaca weeds | Tiny YOLO-v3, YOLO-v3
    | (DC, ATV); (IP, BBA); PBC |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bibx116)] | 辣椒 | 马齿苋类杂草 | Tiny YOLO-v3, YOLO-v3 | (DC, ATV);
    (IP, BBA); PBC |'
- en: '| [[113](#bib.bibx113)] | Not specified | Chinee apple, Lantana, Parkinsonia,
    Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed | Inception-v3,
    ResNet-50 | (DC, FR); (IP, ILA); PBC |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bibx113)] | 未指定 | 中华苹果、马缨丹、洋木芙蓉、羽叶合欢、刺槐、胶乳藤、大飞蓬、玉米、甜菜、普通小麦、黑草、柔滑假禾
    | Inception-v3, ResNet-50 | (DC, FR); (IP, ILA); PBC |'
- en: '| [[82](#bib.bibx82)] | Clover, grass | Broad-leaved dock | AlexNet, VGG-F,
    VGG-VD-16, Inception-v1, ResNet-50, ResNet-101 | (DC, FR); PLA; PBC |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| [[82](#bib.bibx82)] | 三叶草、草类 | 阔叶酸模草 | AlexNet, VGG-F, VGG-VD-16, Inception-v1,
    ResNet-50, ResNet-101 | (DC, FR); PLA; PBC |'
- en: '| [[128](#bib.bibx128)] | Mache salad | Not specified | Scatter Transform,
    Local Binary Pattern (LBP), GLCM, Gabor filter, CNN | (DC, FR); (IP, SDG, BBA);
    PBC |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bibx128)] | 沙拉菜 | 未指定 | 散射变换、局部二值模式（LBP）、GLCM、Gabor滤波器、CNN |
    （DC，FR）；（IP，SDG，BBA）；PBC |'
- en: '| [[138](#bib.bibx138)] | Chrysanthemum | Para grass, Nutsedge | SVM, Artificial
    Neural Network (ANN), CNN | DC; (IP, IA, ILA); PBC |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bibx138)] | 菊花 | 无香臭蔷薇、狗牙根 | SVM、人工神经网络（ANN）、CNN | DC；（IP，IA，ILA）；PBC
    |'
- en: '| [[104](#bib.bibx104)] | Rice | Sagittaria trifolia | SegNet, FCN, U-Net |
    DC; (IP, BBA); PBC |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| [[104](#bib.bibx104)] | 水稻 | 三叶箭 | SegNet、FCN、U-Net | DC；（IP，BBA）；PBC |'
- en: '| [[9](#bib.bibx9)] | Canola | Not specified | U-Net, SegNet | (DC, ATV); (IP,
    IA, PLA); PBC |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| [[9](#bib.bibx9)] | 油菜 | 未指定 | U-Net、SegNet | （DC，ATV）；（IP，IA，PLA）；PBC |'
- en: '| [[171](#bib.bibx171)] | Bermudagrass | Hydrocotyle spp., Hedyotis cormybosa,
    Richardia scabra | VGGNet, GoogLeNet, DetectNet | DC; (IP, ILA); PBC |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bibx171)] | 百慕大草 | 水蓼属、长蕊青枯草、黄檗草 | VGGNet、GoogLeNet、DetectNet
    | DC；（IP，ILA）；PBC |'
- en: '| [[1](#bib.bibx1)] | Oilseed | Not specified | FCN | DC; (IA, PLA); WM |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| [[1](#bib.bibx1)] | 油料作物 | 未指定 | FCN | DC；（IA，PLA）；WM |'
- en: '| [[170](#bib.bibx170)] | Perennial ryegrass | dandelion, ground ivy, spotted
    spurge | AlexNet, VGGNet, GoogLeNet, DetectNet | DC; (IP, ILA); PBC |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| [[170](#bib.bibx170)] | 常年生黑麦草 | 蒲公英、地钱、斑点麻疯树 | AlexNet、VGGNet、GoogLeNet、DetectNet
    | DC；（IP，ILA）；PBC |'
- en: '| [[97](#bib.bibx97)] | Not specified | Not specified | CNN, Histogram of oriented
    Gradients (HoG), LBP | (DC, UAV); (IP, ILA); PBC |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| [[97](#bib.bibx97)] | 未指定 | 未指定 | CNN、方向梯度直方图（HoG）、LBP | （DC，UAV）；（IP，ILA）；PBC
    |'
- en: '| [[141](#bib.bibx141)] | Strawberry | Carolina geranium | VGGNet, GoogLeNet,
    DetectNet | DC; (IP, BBA); PBC |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| [[141](#bib.bibx141)] | 草莓 | 卡罗莱纳天竺葵 | VGGNet、GoogLeNet、DetectNet | DC；（IP，BBA）；PBC
    |'
- en: '| [[44](#bib.bibx44)] | Sunflower, carrots, sugar beets | Not specified | SegNet,
    U-Net, BonNet, FCN8 | (DC, FR, PD); PLA; PBC |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| [[44](#bib.bibx44)] | 向日葵、胡萝卜、甜菜 | 未指定 | SegNet、U-Net、BonNet、FCN8 | （DC，FR，PD）；PLA；PBC
    |'
- en: '| [[162](#bib.bibx162)] | Grassland | Rumex obtusifolius | AlexNet | (DC, UAV);
    (IP, BBA); PBC |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| [[162](#bib.bibx162)] | 草地 | 大黄 | AlexNet | （DC，UAV）；（IP，BBA）；PBC |'
- en: '| [[29](#bib.bibx29)] | Beet, cauliflower, cabbage, strawberry | Not specified
    | Hybrid Network | (DC, ATV); (IP, IA, PLA); PBC |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bibx29)] | 甜菜、花椰菜、卷心菜、草莓 | 未指定 | 混合网络 | （DC，ATV）；（IP，IA，PLA）；PBC
    |'
- en: '| [[20](#bib.bibx20)] | Carrot | Not specified | U-Net | PD; (IA, PLA); PBC
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| [[20](#bib.bibx20)] | 胡萝卜 | 未指定 | U-Net | PD；（IA，PLA）；PBC |'
- en: '| [[16](#bib.bibx16)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | ResNet-101 | PD, (IP,
    IA, BBA); PBC |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| [[16](#bib.bibx16)] | 玉米、普通小麦、甜菜 | 无香臭母草、普通雀麦、荠菜、粘附草、红舌草、芥菜、肥白菜、小花苍耳、田园堇、黑草、松散丝状草
    | ResNet-101 | PD，（IP，IA，BBA）；PBC |'
- en: '| [[72](#bib.bibx72)] | Cotton | Not specified | Faster R-CNN | DC, (IP, BBA);
    PBC |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bibx72)] | 棉花 | 未指定 | Faster R-CNN | DC，（IP，BBA）；PBC |'
- en: '| [[2](#bib.bibx2)] | Paddy | Wild millet | ESNet, U-Net, FCN-8s, and DeepLab-v3,
    Faster R-CNN, EDNet | DC; (IP, IA, PLA); PBC |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| [[2](#bib.bibx2)] | 稻田 | 野生小米 | ESNet、U-Net、FCN-8s、DeepLab-v3、Faster R-CNN、EDNet
    | DC；（IP，IA，PLA）；PBC |'
- en: '| [[43](#bib.bibx43)] | Sugar beet | Alli, hyme, hyac, azol, other unspecified
    weeds | CNN, FCN, LBP, superpixel based LBP, FCN-SPLBP | HC; (IP, PLA); PBC |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bibx43)] | 甜菜 | Alli、hyme、hyac、azol、其他未指定杂草 | CNN、FCN、LBP、基于超像素的LBP、FCN-SPLBP
    | HC；（IP，PLA）；PBC |'
- en: '| [[79](#bib.bibx79)] | Carrot | Not specified | CNN | DC; (IP, PLA); PBC |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| [[79](#bib.bibx79)] | 胡萝卜 | 未指定 | CNN | DC；（IP，PLA）；PBC |'
- en: '| [[35](#bib.bibx35)] | Soybean | grass, broadleaf weeds, Chinee apple, Lantana,
    Parkinsonia, Parthenium, Prickly acacia, Rubber vine, Siam weed, Snake weed |
    Joint Unsupervised LEarning (JULE), DeepCluster | PD; PBC |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bibx35)] | 大豆 | 草、阔叶杂草、中华苹果、藤黄、帕金森树、帕特尼草、刺槐、橡胶藤、暹罗草、蛇麻草 | 联合无监督学习（JULE）、DeepCluster
    | PD；PBC |'
- en: '| [[132](#bib.bibx132)] | Not specified | Gamba grass | U-Net | SI; (IP, PLA)
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bibx132)] | 未指定 | Gamba草 | U-Net | SI；（IP，PLA） |'
- en: '| [[147](#bib.bibx147)] | Clover | Grass | FCN-8s | DC, (IP, PLA); PBC |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| [[147](#bib.bibx147)] | 苜蓿 | 草 | FCN-8s | DC，（IP，PLA）；PBC |'
- en: '| [[173](#bib.bibx173)] | Pasture | Not specified | CNN, SVM | (DC, ATV); (IP,
    IA, ILA); PBC |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| [[173](#bib.bibx173)] | 牧草 | 未指定 | CNN、SVM | （DC，ATV）；（IP，IA，ILA）；PBC |'
- en: '| [[81](#bib.bibx81)] | Grasslands | Broad-leaved dock | AlexNet, VGG-F, GoogLeNet
    | (DC, FR); BBA; PBC |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bibx81)] | 草原 | 广叶大黄 | AlexNet、VGG-F、GoogLeNet | （DC，FR）；BBA；PBC
    |'
- en: '| [[64](#bib.bibx64)] | Rice | Not specified | FCN, SVM | (DC, UAV); BBA; WM
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bibx64)] | 水稻 | 未指定 | FCN、SVM | （DC，UAV）；BBA；WM |'
- en: '| [[157](#bib.bibx157)] | Not specified | Common field speedwell, field pansy,
    common chickweed, fat-hen, fine grasses (annual meadow-grass, loose silky-bent),
    blackgrass, hemp-nettle, shepherd’s purse, common fumitory, scentless mayweed,
    cereal, brassicaceae, maise, polygonum, oat (volunteers), cranesbill, dead-nettle,
    common poppy | Inception-v3 | DC; (IP, ILA); PBC |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bibx157)] | 未指定 | 普通田间速生草、田间堇菜、常见凤仙花、蓼、细草（年度草、细丝草）、黑麦草、麻疯草、雀草、常见痢疾草、无香菊、谷物、十字花科、玉米、蓼科、燕麦（志愿者）、千屈菜、荨麻、罂粟
    | Inception-v3 | DC; (IP, ILA); PBC |'
- en: '| [[160](#bib.bibx160)] | Carrot | Not specified | GoogleNet | PD, (IA, BBA);
    PBC |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| [[160](#bib.bibx160)] | 胡萝卜 | 未指定 | GoogleNet | PD, (IA, BBA); PBC |'
- en: '| [[151](#bib.bibx151)] | Sugar beets | Volunteer potato | AlexNet, VGG-19,
    GoogLeNet, ResNet-50, ResNet-101, Inception-v3 | (DC, ATV); (IP, IA, ILA); PBC
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| [[151](#bib.bibx151)] | 甜菜 | 志愿者土豆 | AlexNet, VGG-19, GoogLeNet, ResNet-50,
    ResNet-101, Inception-v3 | (DC, ATV); (IP, IA, ILA); PBC |'
- en: '| [[41](#bib.bibx41)] | Not specified | Hyme, Alli, Azol, Hyac | CNN | HC,
    (IP, IA, BBA); PBC |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| [[41](#bib.bibx41)] | 未指定 | Hyme、Alli、Azol、Hyac | CNN | HC, (IP, IA, BBA);
    PBC |'
- en: '| [[11](#bib.bibx11)] | Spinach, bean | Not specified | ResNet-18 | (DC, UAV);
    (IP, IA, BBA); WM |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| [[11](#bib.bibx11)] | 菠菜、豆类 | 未指定 | ResNet-18 | (DC, UAV); (IP, IA, BBA);
    WM |'
- en: '| [[42](#bib.bibx42)] | Not specified | Hyme, Alli, Azol, Hyac | CNN, HoG |
    HC, (IP, ILA); PBC |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bibx42)] | 未指定 | Hyme、Alli、Azol、Hyac | CNN, HoG | HC, (IP, ILA);
    PBC |'
- en: '| [[103](#bib.bibx103)] | Sugar beet | Not specified | FCN | (MC, FR); (IP,
    PLA); PBC |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bibx103)] | 甜菜 | 未指定 | FCN | (MC, FR); (IP, PLA); PBC |'
- en: '| [[135](#bib.bibx135)] | Sugar beet | Galinsoga spec., Amaranthus retroflexus,
    Atriplex spec., Polygonum spec., Gramineae (Echinochloa crus-galli, agropyron,
    others.), Convolvulus arvensis, Stellaria media, Taraxacum spec. | SegNet | (MC,
    UAV); PLA; WM |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| [[135](#bib.bibx135)] | 甜菜 | 细叶菊、红缨草、蓼属植物、蓼科植物、禾本科植物（狗牙根、早熟禾等）、旋花科植物、星星草、蒲公英
    | SegNet | (MC, UAV); PLA; WM |'
- en: '| [[63](#bib.bibx63)] | Rice | Not specified | CNN, FCN | (DC, UAV); (IP, PLA);
    WM |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| [[63](#bib.bibx63)] | 水稻 | 未指定 | CNN, FCN | (DC, UAV); (IP, PLA); WM |'
- en: '| [[62](#bib.bibx62)] | Rice | Not specified | FCN-8s, FCN-4s, DeepLab | (DC,
    UAV) (IP, PLA); WM |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| [[62](#bib.bibx62)] | 水稻 | 未指定 | FCN-8s, FCN-4s, DeepLab | (DC, UAV) (IP,
    PLA); WM |'
- en: '| [[26](#bib.bibx26)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | AlexNet, VGGNet, Hybrid
    Network | PD; PBC |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bibx26)] | 玉米、普通小麦、甜菜 | 无香菊、常见凤仙花、雀草、荨麻、红脐、芥菜、肥儿草、小花千屈菜、田间堇菜、黑麦草、细丝草
    | AlexNet, VGGNet, 混合网络 | PD; PBC |'
- en: '| [[111](#bib.bibx111)] | Maise, common wheat, sugar beet | Scentless Mayweed,
    common chickweed, shepherd’s purse, cleavers, Redshank, charlock, fat hen, small-flowered
    Cranesbill, field pansy, black-grass, loose silky-bent | KNN, SVM, CNN | PD; (IP,
    BBA); PBC |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| [[111](#bib.bibx111)] | 玉米、普通小麦、甜菜 | 无香菊、常见凤仙花、雀草、荨麻、红脐、芥菜、肥儿草、小花千屈菜、田间堇菜、黑麦草、细丝草
    | KNN, SVM, CNN | PD; (IP, BBA); PBC |'
- en: '| [[134](#bib.bibx134)] | Sugar beet | Not specified | SegNet | (MC, UAV),
    (IP, BBA), WM |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| [[134](#bib.bibx134)] | 甜菜 | 未指定 | SegNet | (MC, UAV), (IP, BBA), WM |'
- en: '| [[7](#bib.bibx7)] | Maise | Not specified | LeNET, AlexNet, cNET, sNET |
    DC; (IP, IA, PLA); PBC |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| [[7](#bib.bibx7)] | 玉米 | 未指定 | LeNET, AlexNet, cNET, sNET | DC; (IP, IA,
    PLA); PBC |'
- en: '| [[37](#bib.bibx37)] | Winter wheat | Not specified | FCN | (DC, ATV); (IP,
    BBA); PBC |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| [[37](#bib.bibx37)] | 冬小麦 | 未指定 | FCN | (DC, ATV); (IP, BBA); PBC |'
- en: '| [[34](#bib.bibx34)] | Soybean | Grass, broadleaf weeds | AlexNet, SVM, Adaboost
    – C4.5, Random Forest | (DC, UAV); (IP, ILA); PBC |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| [[34](#bib.bibx34)] | 大豆 | 草本、阔叶杂草 | AlexNet, SVM, Adaboost – C4.5, 随机森林
    | (DC, UAV); (IP, ILA); PBC |'
- en: '| [[155](#bib.bibx155)] | Soybean | Cephalanoplos, digitaria, bindweed | Back
    propagation neural network, SVM, CNN | DC; (IP, ILA); PBC |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bibx155)] | 大豆 | 头状植物、狗尾草、贯叶藤 | 反向传播神经网络、SVM、CNN | DC; (IP, ILA);
    PBC |'
- en: '| [[108](#bib.bibx108)] | Sugar beet | Not specified | CNN | (DC, UAV); (IP,
    PLA); PBC |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bibx108)] | 甜菜 | 未指定 | CNN | (DC, UAV); (IP, PLA); PBC |'
- en: '| [[121](#bib.bibx121)] | Lawn grass | Not specified | CNN | (DC, FR); (IP,
    SDG, BBA); PBC |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bibx121)] | 草坪草 | 未指定 | CNN | (DC, FR); (IP, SDG, BBA); PBC |'
- en: '| [[33](#bib.bibx33)] | Sugar beet | Capsella bursa-pastoris, galium aparine
    | SegNet | SDG, PBC |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| [[33](#bib.bibx33)] | 甜菜 | 菊苣、荨麻 | SegNet | SDG, PBC |'
- en: '| [[38](#bib.bibx38)] | Tobacco, thale cress, cleavers, common Poppy, cornflower,
    wheat, maise, sugar beet, cabbage, barley | Sherpherd’s-Purse , chamomile, knotweed
    family, cranesbill, chickweed, veronica, fat-hen, narrow-leaved grasses, field
    pancy, broad-leaved grasses, annual nettle, black nightshade | CNN | PD; (IP,
    IA); PBC |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bibx38)] | 烟草、发菜、荨麻、罂粟、矢车菊、小麦、玉米、甜菜、卷心菜、大麦 | 口袋草、洋甘菊、蓼科、天竺葵、雀麦、波斯菊、脂麻、狭叶草、田野堇菜、阔叶草、一年生荨麻、黑夜shade
    | CNN | PD；（IP，IA）；PBC |'
- en: 6 Data Acquisition
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 数据采集
- en: DL based weed detection and classification techniques require an adequate amount
    of labelled data. Different modalities of data are collected using various types
    of sensors that are mounted on a variety of platforms. Below we discuss the popular
    ways of weed data collection.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的杂草检测和分类技术需要足够数量的标记数据。不同的数据模态通过安装在各种平台上的不同类型的传感器进行收集。以下我们讨论了流行的杂草数据采集方式。
- en: 6.1 Sensors and Camera Mounting Vehicle
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 传感器和相机安装车辆
- en: 6.1.1 Unmanned Aerial Vehicles (UAVs)
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 无人机（UAVs）
- en: Unmanned Aerial Vehicles are often used for data acquisition in agricultural
    research. Generally, UAVs are used for mapping weed density across a field by
    collecting RGB images [[65](#bib.bibx65), [63](#bib.bibx63), [64](#bib.bibx64),
    [122](#bib.bibx122)] or multispectral images [[135](#bib.bibx135), [134](#bib.bibx134),
    [119](#bib.bibx119), [114](#bib.bibx114), [127](#bib.bibx127)]. In addition, UAVs
    can be used to identify crop rows and map weeds within crop rows by collecting
    RGB (Red, Green and Blue color) images [[11](#bib.bibx11)]. [[162](#bib.bibx162)]
    used a small quad-rotor UAV for recording images from grassland to detect broad-leaved
    dock (Rumex obtusifolius). As UAVs fly over the field at a certain height, the
    images captured by them cover a large area. Some of the studies split the images
    into smaller patches and use the patches to distinguish between weeds and crop
    plants [[108](#bib.bibx108), [34](#bib.bibx34), [146](#bib.bibx146)]. However,
    the flight altitude can be maintained at a low height, e.g. 2 meters, so that
    each plant can be labelled as either a weed or crop [[172](#bib.bibx172), [114](#bib.bibx114)].
    [[97](#bib.bibx97)] collected image data using a drone by maintaining an altitude
    of 2.5 meters. [[62](#bib.bibx62)] collected images with a resolution of 3000$\times$4000
    pixels using a sequence of forward-overlaps and side-overlaps to cover the entire
    field. [[86](#bib.bibx86)] flew DJI Phantom 3 and 4 Pro drones with a RGB camera
    at three different heights (10, 15 and 20 m) to determine the optimal height for
    weed detection.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机（UAV）常用于农业研究中的数据采集。通常，无人机通过收集RGB图像[[65](#bib.bibx65)、[63](#bib.bibx63)、[64](#bib.bibx64)、[122](#bib.bibx122)]或多光谱图像[[135](#bib.bibx135)、[134](#bib.bibx134)、[119](#bib.bibx119)、[114](#bib.bibx114)、[127](#bib.bibx127)]，用于绘制田间杂草密度图。此外，无人机还可以通过收集RGB（红色、绿色和蓝色）图像[[11](#bib.bibx11)]来识别作物行和绘制作物行内的杂草分布。[[162](#bib.bibx162)]使用一架小型四旋翼无人机记录草地上的图像，以检测阔叶蓟（Rumex
    obtusifolius）。由于无人机在一定高度飞越田地，因此它们捕捉的图像覆盖了较大的区域。一些研究将图像分割成较小的块，并利用这些块来区分杂草和作物植物[[108](#bib.bibx108)、[34](#bib.bibx34)、[146](#bib.bibx146)]。然而，飞行高度可以保持在较低高度，例如2米，以便将每个植物标记为杂草或作物[[172](#bib.bibx172)、[114](#bib.bibx114)]。[[97](#bib.bibx97)]通过保持2.5米的高度使用无人机收集图像数据。[[62](#bib.bibx62)]使用前后重叠和侧面重叠的序列收集了3000$\times$4000像素分辨率的图像，以覆盖整个田地。[[86](#bib.bibx86)]用RGB相机在三种不同的高度（10、15和20米）飞行DJI
    Phantom 3和4 Pro无人机，以确定杂草检测的最佳高度。
- en: 6.1.2 Field Robots (FRs)
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 田间机器人（FRs）
- en: Various types of field robot can also be used to collect images. A robotic vehicle
    can carry one or more cameras. As previously discussed, robotic vehicles are used
    to collect RGB images by mounted digital cameras [[31](#bib.bibx31), [113](#bib.bibx113),
    [82](#bib.bibx82), [128](#bib.bibx128), [44](#bib.bibx44)]. Mobile phone in-built
    cameras have also been used for such data collection. For example, an iPhone 6
    was used to collect video data by mounting it on a Robotic Rover [[121](#bib.bibx121)].
    A robotic platform called “BoniRob” has been used to collect multi-spectral images
    from the field [[103](#bib.bibx103), [102](#bib.bibx102)]. [[81](#bib.bibx81)]
    used three monochrome cameras mounted on a robot to take images. They argued that,
    in most cases, weeds are green, and so are the crops. There is no need to use
    colour features to distinguish them.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 各种类型的田间机器人也可以用来收集图像。机器人车辆可以携带一个或多个相机。如前所述，机器人车辆通过安装的数字相机来收集 RGB 图像 [[31](#bib.bibx31),
    [113](#bib.bibx113), [82](#bib.bibx82), [128](#bib.bibx128), [44](#bib.bibx44)]。内置相机的手机也被用于这种数据收集。例如，一部
    iPhone 6 被用于通过将其安装在机器人漫游车上来收集视频数据 [[121](#bib.bibx121)]。一种名为“BoniRob”的机器人平台被用于从田间收集多光谱图像
    [[103](#bib.bibx103), [102](#bib.bibx102)]。[[81](#bib.bibx81)] 使用了三台单色相机安装在机器人上拍摄图像。他们认为，在大多数情况下，杂草和作物都是绿色的，因此无需使用颜色特征来区分它们。
- en: 6.1.3 All-Terrain Vehicles (ATVs)
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 全地形车辆（ATV）
- en: To collect images from the field, all-terrain vehicles have also been used.
    ATVs can be mounted with different types of camera [[37](#bib.bibx37), [173](#bib.bibx173),
    [9](#bib.bibx9), [117](#bib.bibx117), [29](#bib.bibx29)]. [[89](#bib.bibx89)]
    used a combination of multi-spectral and spatial sensors to capture data. Even
    multiple low-resolution webcams have been used on an ATV [[116](#bib.bibx116)].
    To maintain specific height with external lighting conditions, and illumination,
    custom made mobile platforms have been used to carry the cameras for capturing
    RGB images [[151](#bib.bibx151), [147](#bib.bibx147)]. When it is not possible
    to use any vehicle to collect images at a certain height, tripods can be used
    as an alternative [[1](#bib.bibx1)].
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从田间收集图像，也使用了全地形车辆。ATV 可以安装不同类型的相机 [[37](#bib.bibx37), [173](#bib.bibx173),
    [9](#bib.bibx9), [117](#bib.bibx117), [29](#bib.bibx29)]。[[89](#bib.bibx89)] 结合了多光谱和空间传感器来捕捉数据。甚至有多个低分辨率的网络摄像头被安装在
    ATV 上 [[116](#bib.bibx116)]。为了在特定高度下维持外部光照条件和照明，定制的移动平台被用于携带相机以捕捉 RGB 图像 [[151](#bib.bibx151),
    [147](#bib.bibx147)]。当无法使用任何车辆在特定高度下收集图像时，可以使用三脚架作为替代 [[1](#bib.bibx1)]。
- en: 6.1.4 Collect Data without Camera Mounting Devices
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 在不安装相机设备的情况下收集数据
- en: On a few occasions, weed data have been collected by cameras without being mounted
    on a vehicle. As such, video data are collected using handheld cameras [[171](#bib.bibx171),
    [138](#bib.bibx138), [104](#bib.bibx104), [170](#bib.bibx170), [157](#bib.bibx157),
    [155](#bib.bibx155), [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [79](#bib.bibx79),
    [168](#bib.bibx168), [72](#bib.bibx72), [142](#bib.bibx142)]. [[141](#bib.bibx141)]
    collected their data by maintaining a certain height (130 cm) from the soil surface.
    Brimrose VA210 filter and JAI BM-141 cameras have been used to collect hyperspectral
    images of weeds and crops without using any platform [[41](#bib.bibx41), [43](#bib.bibx43),
    [42](#bib.bibx42)]. [[7](#bib.bibx7)] manually focused a camera on the target
    plants in such a way that it could capture images, including all the features
    of these plants. In [[159](#bib.bibx159)], they focus the camera on many parts
    of weeds, such as flowers, leaf, fruits, or the full weeds structure.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些情况下，杂草数据是通过未安装在车辆上的相机收集的。因此，视频数据是使用手持相机收集的 [[171](#bib.bibx171), [138](#bib.bibx138),
    [104](#bib.bibx104), [170](#bib.bibx170), [157](#bib.bibx157), [155](#bib.bibx155),
    [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [79](#bib.bibx79), [168](#bib.bibx168),
    [72](#bib.bibx72), [142](#bib.bibx142)]。[[141](#bib.bibx141)] 通过保持离土壤表面一定高度（130
    cm）来收集数据。Brimrose VA210 滤光器和 JAI BM-141 相机用于在不使用任何平台的情况下收集杂草和作物的高光谱图像 [[41](#bib.bibx41),
    [43](#bib.bibx43), [42](#bib.bibx42)]。[[7](#bib.bibx7)] 手动对焦相机于目标植物，以便捕捉到包括这些植物所有特征的图像。在
    [[159](#bib.bibx159)] 中，他们将相机对准杂草的多个部分，例如花朵、叶子、果实或完整的杂草结构。
- en: 6.2 Satellite Imagery
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 卫星影像
- en: '[[132](#bib.bibx132)] use the Pleiades-HR 1A to collect high-resolution 4-band
    (RGB+NIR) imagery over the area of interest. They made use of high-resolution
    satellite images and applied masking to indicate the presence of weeds.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[[132](#bib.bibx132)] 使用了 Pleiades-HR 1A 以获取感兴趣区域的高分辨率四波段（RGB+NIR）影像。他们利用了高分辨率卫星图像，并应用了遮罩以指示杂草的存在。'
- en: 6.3 Public Datasets
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 公共数据集
- en: There are several publicly available crop and weed datasets that can be used
    to train the DL models. [[28](#bib.bibx28)] developed a dataset containing weeds
    in sugar beet crops. Another annotated dataset containing images of crops and
    weeds collected from fields has been made available by [[54](#bib.bibx54)]. A
    dataset of annotated (7853 annotations) crops and weed images was developed by
    [[150](#bib.bibx150)], which comprises 1118 images of six food crops and eight
    weed species. [[95](#bib.bibx95)] developed a dataset containing 7,590 RGB images
    with 315,038 plant objects, representing 64,292 individual plants from 47 different
    species. These data were collected in Denmark and made available for further use.
    A summary of the publicly available datasets related to weed detection and plant
    classification is listed in Table [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣
    6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection from
    Images").
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个公开的作物和杂草数据集可以用于训练深度学习模型。[[28](#bib.bibx28)] 开发了一个包含甜菜作物中杂草的数据集。另一个标注的数据集，包含从田间收集的作物和杂草的图像，由
    [[54](#bib.bibx54)] 提供。[[150](#bib.bibx150)] 开发了一个包含 7853 个标注的作物和杂草图像的数据集，其中包括
    1118 张图像，涵盖了六种食品作物和八种杂草物种。[[95](#bib.bibx95)] 开发了一个包含 7,590 张 RGB 图像的数据集，其中包括
    315,038 个植物对象，代表 47 个不同物种的 64,292 株植物。这些数据是在丹麦收集的，并且已开放供进一步使用。有关杂草检测和植物分类的公开数据集的汇总列在表格
    [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣ 6 Data Acquisition ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images") 中。
- en: We have listed nineteen datasets in Table [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets
    ‣ 6 Data Acquisition ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") which are available in this area, and can be used by researchers.
    Amongst these datasets, researchers will need to send a request to the owner of
    “Perennial ryegrass and weed”, “CNU Weed Dataset” and “Sugar beet and hedge bindweed”
    dataset to obtain the data. Other datasets can be downloaded directly on-line.
    Most of the datasets contain RGB images of food crops and weeds from different
    parts of the world. The RGB data have generally been collected using high-resolution
    digital cameras. However, [[157](#bib.bibx157)] used a point grey industrial camera.
    While acquiring data for the “DeepWeeds” dataset, the researchers added a “Fujinon
    CF25HA-1” lens with their “FLIR Blackfly 23S6C” camera and mounted the camera
    on a weed control robot (“AutoWeed”). [[28](#bib.bibx28)] and [[55](#bib.bibx55)]
    employed “Bonirob” (an autonomous field robot) to mount the multi-spectral cameras.
    “Carrots 2017” and “Onions 2017” datasets were also acquired using a multi-spectral
    camera, namely the “Teledyne DALSA Genie Nano”. These researchers used a manually
    pulled cart to carry the camera. The “CNU Weed Dataset” has 208,477 images of
    weeds collect from farms and fields in the Republic of Korea, which is the highest
    number among the datasets. Though this dataset exhibits a class imbalance, it
    contains twenty-one species of weeds from five families. [[147](#bib.bibx147)]
    developed a dataset of red clover, white clover and other associated weeds. The
    dataset contains 31,600 unlabelled data together with 8000 synthetic data. Their
    goal was to generate labels for the data using unsupervised or self-supervised
    approaches. All the other datasets were manually labelled using image level, pixel-wise
    or bounding box annotation techniques.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格 [3](#S6.T3 "Table 3 ‣ 6.3 Public Datasets ‣ 6 Data Acquisition ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") 中列出了十九个该领域可用的数据集，研究人员可以使用这些数据集。在这些数据集中，研究人员需要向“多年生黑麦草和杂草”、“CNU
    杂草数据集”和“甜菜和刺篱草”数据集的拥有者申请数据。其他数据集可以直接在线下载。大多数数据集包含来自不同地区的食品作物和杂草的 RGB 图像。这些 RGB
    数据通常使用高分辨率数字相机收集。然而，[[157](#bib.bibx157)] 使用了一台点灰工业相机。在收集“DeepWeeds”数据集的过程中，研究人员将“Fujinon
    CF25HA-1”镜头添加到他们的“FLIR Blackfly 23S6C”相机上，并将相机安装在一个杂草控制机器人（“AutoWeed”）上。[[28](#bib.bibx28)]
    和 [[55](#bib.bibx55)] 使用了“Bonirob”（一款自主田间机器人）来安装多光谱相机。“Carrots 2017”和“Onions 2017”数据集也是使用多光谱相机，即“Teledyne
    DALSA Genie Nano”获得的。这些研究人员使用了一辆手动推车来运输相机。“CNU 杂草数据集”包含了从韩国农场和田间收集的 208,477 张杂草图像，是数据集中数量最多的。虽然该数据集存在类别不平衡的问题，但它包含了来自五个科的二十一种杂草。[[147](#bib.bibx147)]
    开发了一个红三叶草、白三叶草及其他相关杂草的数据集。该数据集包含 31,600 个未标注的数据以及 8000 个合成数据。他们的目标是使用无监督或自监督方法生成数据的标签。所有其他数据集都使用图像级、像素级或边界框标注技术进行手动标注。
- en: 'Table 3: List of publicly available crop and weed datasets'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：公开可用的作物和杂草数据集列表
- en: '| Dataset and Reference | Type/Number of Crop | Type/Number of Weed Species
    | Data Type | Sensor and Mounting Vehicle | Number of Images | Data Annotation
    | Data Location | Class imbalance? | Source |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 数据集和参考 | 作物类型/数量 | 杂草物种类型/数量 | 数据类型 | 传感器和安装车辆 | 图像数量 | 数据注释 | 数据位置 | 类别不平衡？
    | 来源 |'
- en: '| Crop/Weed Field Image Dataset [[55](#bib.bibx55)] | Carrot | Not specified
    | Multi-spectral image | MC and FR | 60 | PLA | Germany | Yes | [https://github.com/cwfid/dataset](https://github.com/cwfid/dataset)
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Crop/Weed Field 图像数据集 [[55](#bib.bibx55)] | 胡萝卜 | 未指定 | 多光谱图像 | MC 和 FR |
    60 | PLA | 德国 | 是 | [https://github.com/cwfid/dataset](https://github.com/cwfid/dataset)
    |'
- en: '| Dataset of food crops and weed [[150](#bib.bibx150)] | Six crop | Eight weed
    species | RGB | DC | 1118 | BBA | Latvia | Yes | [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/)
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 食用作物和杂草数据集 [[150](#bib.bibx150)] | 六种作物 | 八种杂草物种 | RGB | DC | 1118 | BBA
    | 拉脱维亚 | 是 | [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7305380/)
    |'
- en: '| DeepWeeds [[113](#bib.bibx113)] | Not specified | Eight weed species | RGB
    | DC and FR | 17,509 | ILA | Australia | No | [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DeepWeeds [[113](#bib.bibx113)] | 未指定 | 八种杂草物种 | RGB | DC 和 FR | 17,509 |
    ILA | 澳大利亚 | 否 | [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)
    |'
- en: '| Early crop weed dataset [[40](#bib.bibx40)] | Tomato, cotton | Black nightshad,
    velvetleaf | RGB | 508 | DC | ILA | Greece | Yes | [https://github.com/AUAgroup/early-crop-weed](https://github.com/AUAgroup/early-crop-weed)
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 早期作物杂草数据集 [[40](#bib.bibx40)] | 番茄、棉花 | 黑夜影、丝绸叶 | RGB | 508 | DC | ILA |
    希腊 | 是 | [https://github.com/AUAgroup/early-crop-weed](https://github.com/AUAgroup/early-crop-weed)
    |'
- en: '| Perennial ryegrass and weed [[170](#bib.bibx170)] | Perennial ryegrass |
    dandelion, ground ivy, and spotted spurge | RGB | DC | 33086 | ILA | USA | No
    | [https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full](https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full)
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 多年生黑麦草和杂草 [[170](#bib.bibx170)] | 多年生黑麦草 | 蒲公英、地锦草和斑点雀稗 | RGB | DC | 33086
    | ILA | 美国 | 否 | [https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full](https://www.frontiersin.org/articles/10.3389/fpls.2019.01422/full)
    |'
- en: '| Soybean and weed dataset [[34](#bib.bibx34)] | Soybean | Grass and broadleaf
    weeds | RGB | DC and UAV | 400 | ILA | Brazil | Yes | [https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops](https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops)
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 大豆和杂草数据集 [[34](#bib.bibx34)] | 大豆 | 草类和阔叶杂草 | RGB | DC 和 UAV | 400 | ILA
    | 巴西 | 是 | [https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops](https://www.kaggle.com/fpeccia/weed-detection-in-soybean-crops)
    |'
- en: '| Open Plant Phenotype Database [[95](#bib.bibx95)] | Not specified | 46 most
    common monocotyledon (grass) and dicotyledon (broadleaved) weeds | RGB | DC |
    7,590 | BBA | Denmark | No | [https://gitlab.au.dk/AUENG-Vision/OPPD](https://gitlab.au.dk/AUENG-Vision/OPPD)
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 开放植物表型数据库 [[95](#bib.bibx95)] | 未指定 | 46 种最常见的单子叶植物（草类）和双子叶植物（阔叶植物）杂草 | RGB
    | DC | 7,590 | BBA | 丹麦 | 否 | [https://gitlab.au.dk/AUENG-Vision/OPPD](https://gitlab.au.dk/AUENG-Vision/OPPD)
    |'
- en: '| Sugar beet and hedge bindweed dataset [[47](#bib.bibx47)] | Sugar beet |
    Convolvulus sepium (hedge bindweed) | RGB | DC | 652 | BBA | Belgium | Yes | [https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z)
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 糖用甜菜和篱笆马唐数据集 [[47](#bib.bibx47)] | 糖用甜菜 | 篱笆马唐（Convolvulus sepium） | RGB
    | DC | 652 | BBA | 比利时 | 是 | [https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-020-00570-z)
    |'
- en: '| Sugar beet fields dataset [[28](#bib.bibx28)] | Sugar beet | Not specified
    | Multi-spectral image | MC and FR | 12340 | PLA | Germany | No | [https://www.ipb.uni-bonn.de/2018/10/](https://www.ipb.uni-bonn.de/2018/10/)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 糖用甜菜田数据集 [[28](#bib.bibx28)] | 糖用甜菜 | 未指定 | 多光谱图像 | MC 和 FR | 12340 | PLA
    | 德国 | 否 | [https://www.ipb.uni-bonn.de/2018/10/](https://www.ipb.uni-bonn.de/2018/10/)
    |'
- en: '| UAV Sugarbeets 2015-16 Datasets [[27](#bib.bibx27)] | Sugarbeets | Not specified
    | RGB | DC and UAV | 675 | PLA | Switzerland | No | [https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/](https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/)
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| UAV Sugarbeets 2015-16 数据集 [[27](#bib.bibx27)] | 糖用甜菜 | 未指定 | RGB | DC 和
    UAV | 675 | PLA | 瑞士 | 否 | [https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/](https://www.ipb.uni-bonn.de/data/uav-sugarbeets-2015-16/)
    |'
- en: '| Corn, lettuce and weed dataset [[71](#bib.bibx71)] | Corn and lettuce | Cirsium
    setosum, Chenopodium album, bluegrass and sedge | RGB | DC | 6800 | ILA | China
    | No | [https://github.com/zhangchuanyin/weed-datasets](https://github.com/zhangchuanyin/weed-datasets)
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 玉米、生菜和杂草数据集 [[71](#bib.bibx71)] | 玉米和生菜 | 蒲公英、陈腐酸草、蓝草和莎草 | RGB | DC | 6800
    | ILA | 中国 | 否 | [https://github.com/zhangchuanyin/weed-datasets](https://github.com/zhangchuanyin/weed-datasets)
    |'
- en: '| Carrot-Weed dataset [[88](#bib.bibx88)] | Carrot | Not specified | RGB |
    DC | 39 | PLA | Republic of Macedonia | Yes | [https://github.com/lameski/rgbweeddetection](https://github.com/lameski/rgbweeddetection)
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 胡萝卜-杂草数据集 [[88](#bib.bibx88)] | 胡萝卜 | 未指定 | RGB | DC | 39 | PLA | 北马其顿共和国
    | 是 | [https://github.com/lameski/rgbweeddetection](https://github.com/lameski/rgbweeddetection)
    |'
- en: '| Bccr-segset dataset [[90](#bib.bibx90)] | Canola, corn, radish | Not specified
    | RGB | DC | 30,000 | ILA | Australia | No | [https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497](https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497)
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Bccr-segset 数据集 [[90](#bib.bibx90)] | 油菜、玉米、萝卜 | 未指定 | RGB | DC | 30,000
    | ILA | 澳大利亚 | 否 | [https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497](https://academic.oup.com/gigascience/article/9/3/giaa017/5780256#200419497)
    |'
- en: '| Carrots 2017 dataset [[18](#bib.bibx18)] | Carrots | Not specified | Multi-spectral
    image | MC and manually pulled cart | 20 | PLA | UK | No | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR)
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 胡萝卜 2017 数据集 [[18](#bib.bibx18)] | 胡萝卜 | 未指定 | 多光谱图像 | MC 和手动拖车 | 20 | PLA
    | 英国 | 否 | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/RYni5xngnEZEFkR)
    |'
- en: '| Onions 2017 dataset [[18](#bib.bibx18)] | Onions | Not specified | Multi-spectral
    image | MC and manually pulled cart | 20 | PLA | UK | No | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN)
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 洋葱 2017 数据集 [[18](#bib.bibx18)] | 洋葱 | 未指定 | 多光谱图像 | MC 和手动拖车 | 20 | PLA
    | 英国 | 否 | [https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN](https://lcas.lincoln.ac.uk/nextcloud/index.php/s/e8uiyrogObAPtcN)
    |'
- en: '| GrassClover image dataset [[147](#bib.bibx147)] | Red clover and white clover
    | Not specified | RGB | DC and manually operated platform | 31,600 real and 8000
    synthetic images | PLA | Denmark | Yes | [https://vision.eng.au.dk/grass-clover-dataset/](https://vision.eng.au.dk/grass-clover-dataset/)
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| GrassClover 图像数据集 [[147](#bib.bibx147)] | 红三叶草和白三叶草 | 未指定 | RGB | DC 和手动操作平台
    | 31,600 张真实图像和 8,000 张合成图像 | PLA | 丹麦 | 是 | [https://vision.eng.au.dk/grass-clover-dataset/](https://vision.eng.au.dk/grass-clover-dataset/)
    |'
- en: '| Leaf counting dataset [[157](#bib.bibx157)] | Not specified | Eighteen weed
    species | RGB | DC | 9372 | ILA | Denmark | Yes | [https://vision.eng.au.dk/leaf-counting-dataset/](https://vision.eng.au.dk/leaf-counting-dataset/)
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 叶片计数数据集 [[157](#bib.bibx157)] | 未指定 | 十八种杂草 | RGB | DC | 9,372 | ILA | 丹麦
    | 是 | [https://vision.eng.au.dk/leaf-counting-dataset/](https://vision.eng.au.dk/leaf-counting-dataset/)
    |'
- en: '| CNU Weed Dataset [[159](#bib.bibx159)] | Not specified | Twenty one species
    of weed | RGB | DC | 208,477 | ILA | Republic of Korea | Yes | [https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025](https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025)
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CNU 杂草数据集 [[159](#bib.bibx159)] | 未指定 | 二十一种杂草 | RGB | DC | 208,477 | ILA
    | 大韩民国 | 是 | [https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025](https://www.sciencedirect.com/science/article/pii/S0168169919319799#s0025)
    |'
- en: '| Plant Seedlings Dataset [[49](#bib.bibx49)] | Three crop | Nine weed species
    | RGB | DC | 5539 | ILA | Denmark | Yes | [https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset)
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 植物幼苗数据集 [[49](#bib.bibx49)] | 三种作物 | 九种杂草 | RGB | DC | 5,539 | ILA | 丹麦 |
    是 | [https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset](https://www.kaggle.com/vbookshelf/v2-plant-seedlings-dataset)
    |'
- en: '[[38](#bib.bibx38)] use six publicly available datasets containing 22 different
    plant species to classify using deep learning methods. Several studies proposed
    an encoder-decoder architecture to distinguish crops and weeds using the Crop
    Weed Field Image Dataset [[161](#bib.bibx161), [20](#bib.bibx20), [160](#bib.bibx160)].
    The DeepWeeds dataset [[113](#bib.bibx113)] was used by [[61](#bib.bibx61)] to
    evaluate their proposed method. In the study of [[71](#bib.bibx71)], the “Carrot-Weed
    dataset” [[88](#bib.bibx88)] was used with their own dataset the “Corn, lettuce
    and weed dataset”. [[44](#bib.bibx44)] collected data from a sunflower farm in
    Italy. To demonstrate the proposed method’s generalising ability, they also used
    two publicly available datasets containing images of carrots, sugar beets and
    associated weeds. [[18](#bib.bibx18)] also used those datasets along with the
    Carrot 2017 and Onion 2017 datasets. The “Plant Seedlings” dataset is a publicly
    available dataset containing 12 different plant species. Several studies used
    this dataset to develop a crop-weed classification model [[26](#bib.bibx26), [111](#bib.bibx111),
    [16](#bib.bibx16), [119](#bib.bibx119)]. [[35](#bib.bibx35)] used DeepWeeds [[113](#bib.bibx113)]
    and “Soybean and weed” datasets, which are publicly available.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[[38](#bib.bibx38)] 使用了六个公开的数据集，这些数据集包含22种不同的植物物种，通过深度学习方法进行分类。一些研究提出了一种编码器-解码器架构，利用
    Crop Weed Field Image Dataset [[161](#bib.bibx161), [20](#bib.bibx20), [160](#bib.bibx160)]
    来区分作物和杂草。DeepWeeds 数据集 [[113](#bib.bibx113)] 被 [[61](#bib.bibx61)] 用于评估他们提出的方法。在
    [[71](#bib.bibx71)] 的研究中，使用了“胡萝卜-杂草数据集” [[88](#bib.bibx88)] 和他们自己的数据集“玉米、生菜和杂草数据集”。[[44](#bib.bibx44)]
    收集了来自意大利一个向日葵农场的数据。为了展示所提方法的泛化能力，他们还使用了两个公开的数据集，这些数据集包含胡萝卜、甜菜及相关杂草的图像。[[18](#bib.bibx18)]
    也使用了这些数据集以及 Carrot 2017 和 Onion 2017 数据集。“植物幼苗”数据集是一个公开的数据集，包含12种不同的植物物种。一些研究使用该数据集开发了作物-杂草分类模型
    [[26](#bib.bibx26), [111](#bib.bibx111), [16](#bib.bibx16), [119](#bib.bibx119)]。[[35](#bib.bibx35)]
    使用了 DeepWeeds [[113](#bib.bibx113)] 和“豆类和杂草”数据集，这些数据集都是公开的。'
- en: While several datasets are publicly available, they are somewhat site/crop-specific.
    As such there is no so-called benchmark weed dataset like ImageNet [[32](#bib.bibx32)]
    and MS COCO [[98](#bib.bibx98)] in this research field, that is widely used in
    the evaluation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有几个数据集是公开的，但它们在某种程度上具有特定的站点/作物特点。因此，在这个研究领域内，并没有像 ImageNet [[32](#bib.bibx32)]
    和 MS COCO [[98](#bib.bibx98)] 那样广泛用于评估的所谓基准杂草数据集。
- en: 7 Dataset Preparation
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 数据集准备
- en: After acquiring data from different sources, it is necessary to prepare data
    for training, testing, and to validate models. Raw data is not always suitable
    for the DL model. The dataset preparation approaches include applying different
    image processing techniques, data labelling, using image augmentation techniques
    to increase the number of input data, or impose variations in the data and generating
    synthetic data for training. Commonly used image processing techniques are - background
    removal, resizing the collected image, green component segmentation, removing
    motion blur, de-noising, image enhancement, extraction of colour vegetation indices,
    and changing the colour model. [[121](#bib.bibx121)] decoded video into a sequence
    of RGB images and then converted them into grayscale images. In further research,
    the camera was set to auto-capture mode to collect images in the TIFF format and
    then these were converted into the RGB colour model [[151](#bib.bibx151)]. Using
    three webcams on an ATV, [[116](#bib.bibx116)] took videos and then converted
    them into different frames of images. In some occasions, it was necessary to change
    the image format to accurately train the model, especially when using public datasets.
    For instance, [[16](#bib.bibx16)] converted the “Plant Seedlings Dataset” [[49](#bib.bibx49)]
    from PNG to JPEG format, as a number of studies have show that the JPEG format
    is better for training Residual Networks architectures [[39](#bib.bibx39)].
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在从不同来源获取数据后，有必要为训练、测试和验证模型准备数据。原始数据并不总是适合深度学习模型。数据集准备方法包括应用不同的图像处理技术、数据标记、使用图像增强技术来增加输入数据的数量，或对数据施加变化并生成合成数据用于训练。常用的图像处理技术包括
    - 背景去除、调整收集图像的大小、绿色成分分割、去除运动模糊、去噪、图像增强、提取颜色植被指数和更改颜色模型。[[121](#bib.bibx121)] 将视频解码成一系列RGB图像，然后将其转换为灰度图像。在进一步的研究中，摄像头设置为自动捕获模式以收集TIFF格式的图像，然后将这些图像转换为RGB颜色模型[[151](#bib.bibx151)]。使用三台网络摄像头在全地形车上，[[116](#bib.bibx116)]
    拍摄视频，然后将其转换为不同的图像帧。在某些情况下，必须更改图像格式以准确训练模型，尤其是在使用公共数据集时。例如，[[16](#bib.bibx16)]
    将“植物幼苗数据集”[[49](#bib.bibx49)] 从PNG格式转换为JPEG格式，因为许多研究表明，JPEG格式更适合训练残差网络架构[[39](#bib.bibx39)]。
- en: 7.1 Image Pre-processing
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 图像预处理
- en: The majority of relevant studies undertook some level of image processing before
    providing the data as an input to the DL model. It helps the DL architecture to
    extract features more accurately. Here we discuss image pre-processing operations
    used in the related studies.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数相关研究在将数据输入到深度学习模型之前进行了某种程度的图像处理。这有助于深度学习架构更准确地提取特征。这里我们讨论了相关研究中使用的图像预处理操作。
- en: Image Resizing
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像大小调整
- en: '[[41](#bib.bibx41)] investigate the performance of Deep Convolutional Neural
    Networks based on spatial resolution. They used three different special resolutions
    30$\times$30, 45$\times$45, and 60$\times$60 pixels. The lower patch size achieved
    good accuracy and required less time to train the model. To make the processing
    faster and reduce the computational complexity, most of the studies performed
    image resizing operations on the dataset before inputting into the DL model. After
    collecting images from the field, the resolution of the images is reduced based
    on the DL network requirement. [[171](#bib.bibx171)] used 1280$\times$720 pixel-sized
    images to train DetectNet [[156](#bib.bibx156)] architecture and 640$\times$360
    pixels for GoogLeNet [[152](#bib.bibx152)] and VGGNet [[145](#bib.bibx145)] neural
    networks. The commonly used image sizes (in pixel) are- 64$\times$64 [[11](#bib.bibx11),
    [108](#bib.bibx108), [173](#bib.bibx173), [7](#bib.bibx7)], 128$\times$128 [[40](#bib.bibx40),
    [38](#bib.bibx38), [16](#bib.bibx16)], 224$\times$224 [[113](#bib.bibx113), [71](#bib.bibx71),
    [16](#bib.bibx16)], 227$\times$227 [[162](#bib.bibx162), [151](#bib.bibx151)],
    228$\times$228 [[89](#bib.bibx89)], 256$\times$256 [[34](#bib.bibx34), [155](#bib.bibx155),
    [121](#bib.bibx121), [61](#bib.bibx61), [122](#bib.bibx122)], 320$\times$240 [[29](#bib.bibx29)],
    288$\times$288 [[2](#bib.bibx2)], 360$\times$360 [[16](#bib.bibx16)].'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[[41](#bib.bibx41)] 研究了基于空间分辨率的深度卷积神经网络的性能。他们使用了三种不同的特殊分辨率：30$\times$30、45$\times$45
    和 60$\times$60 像素。较小的补丁尺寸实现了较好的准确性，并且训练模型所需的时间更少。为了加快处理速度并减少计算复杂度，大多数研究在将数据输入到
    DL 模型之前对数据集进行了图像缩放操作。在从现场收集图像后，根据 DL 网络的要求降低图像的分辨率。[[171](#bib.bibx171)] 使用了1280$\times$720
    像素大小的图像来训练 DetectNet [[156](#bib.bibx156)] 架构，并使用了640$\times$360 像素的图像来训练 GoogLeNet
    [[152](#bib.bibx152)] 和 VGGNet [[145](#bib.bibx145)] 神经网络。常用的图像尺寸（以像素为单位）包括：64$\times$64
    [[11](#bib.bibx11), [108](#bib.bibx108), [173](#bib.bibx173), [7](#bib.bibx7)]，128$\times$128
    [[40](#bib.bibx40), [38](#bib.bibx38), [16](#bib.bibx16)]，224$\times$224 [[113](#bib.bibx113),
    [71](#bib.bibx71), [16](#bib.bibx16)]，227$\times$227 [[162](#bib.bibx162), [151](#bib.bibx151)]，228$\times$228
    [[89](#bib.bibx89)]，256$\times$256 [[34](#bib.bibx34), [155](#bib.bibx155), [121](#bib.bibx121),
    [61](#bib.bibx61), [122](#bib.bibx122)]，320$\times$240 [[29](#bib.bibx29)]，288$\times$288
    [[2](#bib.bibx2)]，360$\times$360 [[16](#bib.bibx16)]。'
- en: Images with high resolution are sometimes split into a number of patches to
    reduce the computational complexity. For instance, in the work of [[128](#bib.bibx128)],
    the images were split with a resolution of 5120$\times$3840 into 56 patches. Similar
    operations were performed by [[63](#bib.bibx63), [9](#bib.bibx9), [104](#bib.bibx104)]
    where they divided the original images into tiles of size 912$\times$1024, 1440$\times$960
    and 1000$\times$1000 pixels. [[127](#bib.bibx127)] captured only five images at
    high resolution using a drone which was then split into small patches of size
    480$\times$360 without overlapping and 512$\times$512 with 30% overlap. [[117](#bib.bibx117)]
    collected images using three cameras simultaneously of resolution 640$\times$480
    pixels. They then merged those into a single image of 1920$\times$480 pixels which
    was resized to 1024$\times$256 pixels. [[170](#bib.bibx170)] scaled down the images
    of their dataset to 1224$\times$1024 pixels, so that the training did not run
    low on memory. [[62](#bib.bibx62)] used orthomosaic imagery, which is usually
    quite large. They split the images into small patches of 1000$\times$1000 pixels.
    In the study of [[141](#bib.bibx141)], the images were resized to 1280$\times$720
    pixels and then cropped into four sub-images. [[114](#bib.bibx114)] used 1280$\times$960
    pixel size image with four spectral bands. By applying union operation on the
    red, green, and near infrared bands, they generated a false green image in order
    to highlight the vegetation. [[142](#bib.bibx142)] resized the collected image
    to 1280$\times$853 pixels and then cropped it to 1280$\times$720 pixels.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率的图像有时会被拆分成多个小块以减少计算复杂性。例如，在[[128](#bib.bibx128)]的研究中，分辨率为5120$\times$3840的图像被拆分为56个小块。类似的操作在[[63](#bib.bibx63)、[9](#bib.bibx9)、[104](#bib.bibx104)]中也有进行，它们将原始图像分割成912$\times$1024、1440$\times$960和1000$\times$1000像素的瓦片。[[127](#bib.bibx127)]
    使用无人机仅拍摄了五张高分辨率图像，这些图像随后被拆分成尺寸为480$\times$360（无重叠）和512$\times$512（30%重叠）的小块。[[117](#bib.bibx117)]
    使用三台相机同时拍摄了640$\times$480像素的图像，然后将这些图像合并成一张1920$\times$480像素的单张图像，并将其调整为1024$\times$256像素。[[170](#bib.bibx170)]
    将其数据集中的图像缩小到1224$\times$1024像素，以避免训练时内存不足。[[62](#bib.bibx62)] 使用的是通常相当大的正射影像。他们将图像拆分成1000$\times$1000像素的小块。在[[141](#bib.bibx141)]的研究中，图像被调整为1280$\times$720像素，然后裁剪成四个子图像。[[114](#bib.bibx114)]
    使用了尺寸为1280$\times$960像素的图像，并具有四个光谱波段。通过对红色、绿色和近红外波段进行联合操作，他们生成了一张虚假的绿色图像以突出显示植被。[[142](#bib.bibx142)]
    将收集到的图像调整为1280$\times$853像素，然后裁剪为1280$\times$720像素。
- en: Background Removal
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 背景去除
- en: '[[65](#bib.bibx65)] collected images using a UAV and applied image mosaicing
    to generate an orthophoto. [[11](#bib.bibx11)] applied Hough-transform to highlight
    the aligned pixels and used Otsu-adaptive-thresholding method to differentiate
    the background and green crops or weeds. On the other hand, for removing the background
    soil image, [[108](#bib.bibx108)] applied the Normalised Difference Vegetation
    Index (NDVI). They also used morphological opening and closing operations to remove
    the noise and fill tiny gaps among vegetation pixels. To annotate the images manually
    into respective classes, [[34](#bib.bibx34)] applied the Simple Linear Iterative
    Clustering algorithm. This algorithm helps to segment weeds, crops, and background
    from images. Image pre-processing techniques were also involved in [[134](#bib.bibx134)]
    for having a bounding box around crop plants or weeds and removed the background.
    They first used image correlation and cropping for alignment and then applied
    Gaussian blur, followed by a sharpening operation to remove shadows, small debris,
    etc. Finally, for executing the blob detection process on connected pixels, Otsu’s
    method was employed. [[102](#bib.bibx102)] applied the pre-processing operation
    on red, green, blue, and NIR channels separately. They also performed the Gaussian
    blur operation to remove noise using a [5$\times$5] kernel. To standardise the
    channels, the values were subtracted by the mean of all channel values and divided
    by their standard deviation. After that, they normalised and zero-centred the
    channel values. [[72](#bib.bibx72)] applied a Contrast Limited Adaptive Histogram
    Equalisation algorithm to enhance the image contrast and reduce the image variation
    due to ambient illumination changes.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[[65](#bib.bibx65)] 使用无人机收集了图像，并应用图像镶嵌生成了正射影像图。[[11](#bib.bibx11)] 使用霍夫变换突出对齐的像素，并使用大津自适应阈值方法区分背景与绿色作物或杂草。另一方面，为了去除背景土壤图像，[[108](#bib.bibx108)]
    应用了归一化差异植被指数（NDVI）。他们还使用了形态学开运算和闭运算来去除噪声并填补植物像素间的小空隙。为了手动注释图像到各自的类别，[[34](#bib.bibx34)]
    应用了简单线性迭代聚类算法。该算法有助于将杂草、作物和背景从图像中分割出来。[[134](#bib.bibx134)] 也涉及了图像预处理技术，以便在作物植物或杂草周围生成边界框，并去除了背景。他们首先使用图像相关性和裁剪进行对齐，然后应用高斯模糊，接着进行锐化操作以去除阴影、小碎片等。最后，为了在连接的像素上执行斑点检测过程，采用了大津方法。[[102](#bib.bibx102)]
    对红色、绿色、蓝色和NIR通道分别进行了预处理操作。他们还使用[5$\times$5]内核进行高斯模糊操作以去除噪声。为了标准化通道，值被减去所有通道值的均值，并除以其标准差。之后，他们将通道值归一化并零中心化。[[72](#bib.bibx72)]
    应用了对比度限制自适应直方图均衡算法来增强图像对比度并减少由于环境光照变化导致的图像变化。'
- en: In the work of [[89](#bib.bibx89)] and [[13](#bib.bibx13)], all images were
    segmented using the Excess Green minus Excess Red Indices (ExG-ExR) method, which
    effectively removed the background. They also applied opening and closing morphological
    operations of images and generated contour masks to extract features. On the other
    hand, [[9](#bib.bibx9)] argued that the Maximum Likelihood Classification technique
    performed better than thresholding techniques for segmenting the background soil
    and green plants. According to [[4](#bib.bibx4)], images captured from the field
    had many problems (e.g. lack of brightness). It was necessary to apply image pre-processing
    operations to prepare the data for training. They performed several morphological
    operations to remove motion blur and light illumination. They also removed the
    noisy region before applying segmentation operations for separating the background.
    Threshold-based segmentation techniques had been used to separate the soil and
    green plants in an image. In the reports of [[40](#bib.bibx40)] and [[7](#bib.bibx7)],
    the RGB channels of the images were normalised to avoid differences in lighting
    conditions before removing the background. For vegetation segmentation, Otsu’s
    thresholding was applied, followed by the ExG (Excess Green) vegetation indexing
    operation. However, [[38](#bib.bibx38)] used a simple excessive green segmentation
    technique for removing the background and detecting the green pixels. [[79](#bib.bibx79)]
    converted the RGB image to HSV colour space, applied thresholding method and band-pass
    filtering, and then used binary masking to extract the image’s green component.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[89](#bib.bibx89)]和[[13](#bib.bibx13)]的工作中，所有图像都使用了过量绿色减去过量红色指数（ExG-ExR）方法进行分割，这有效地去除了背景。他们还应用了图像的开闭运算，并生成了轮廓掩膜以提取特征。另一方面，[[9](#bib.bibx9)]
    认为最大似然分类技术在分割背景土壤和绿色植物方面优于阈值技术。根据[[4](#bib.bibx4)]，从田间拍摄的图像存在许多问题（例如亮度不足）。因此，必须应用图像预处理操作来准备数据进行训练。他们进行了几种形态学操作以去除运动模糊和光照。还在应用分割操作以分离背景之前去除了噪声区域。基于阈值的分割技术已被用于分离图像中的土壤和绿色植物。在[[40](#bib.bibx40)]和[[7](#bib.bibx7)]的报告中，图像的RGB通道在去除背景之前被归一化，以避免光照条件的差异。对于植被分割，应用了Otsu的阈值分割，随后进行了ExG（过量绿色）植被指数操作。然而，[[38](#bib.bibx38)]
    使用了一种简单的过量绿色分割技术来去除背景和检测绿色像素。[[79](#bib.bibx79)] 将RGB图像转换为HSV颜色空间，应用了阈值方法和带通滤波，然后使用二进制掩膜提取图像的绿色组件。
- en: Image Enhancement and Denoising
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图像增强与去噪
- en: '[[111](#bib.bibx111)] investigated the importance of image pre-processing operation
    by training the CNN model with raw data and processed data. They found that without
    image pre-processing the model performance decreased. They used Gaussian Blur
    for smoothing the images and removed the high-frequency content. They then converted
    the colour of the image to HSV space. Using a morphological erosion with an 11$\times$11
    structuring kernel, they subtracted the background soil and produced foreground
    seedling images. [[103](#bib.bibx103)] reported that image pre-processing improved
    the generalisation capabilities of a classification system. They applied [5$\times$5]
    Gaussian Kernel to remove noise and to normalise the data. They also zero-centred
    the pixel values of the image. The study of [[138](#bib.bibx138)] used the Gaussian
    and median filter to remove Gaussian noise and Salt and Pepper noise respectively.
    [[155](#bib.bibx155)] also normalised the data to maintain zero-mean and unit
    variance. Besides, they applied Principal Component Analysis and Zero-phase Component
    Analysis data whitening for eliminating the correlation among the data.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[[111](#bib.bibx111)] 研究了图像预处理操作的重要性，通过用原始数据和处理后的数据训练CNN模型。他们发现没有图像预处理时，模型性能会下降。他们使用了高斯模糊来平滑图像并去除高频内容。随后，他们将图像的颜色转换为HSV空间。使用11$\times$11的形态学腐蚀结构核，他们去除了背景土壤并生成了前景幼苗图像。[[103](#bib.bibx103)]
    报告称，图像预处理改善了分类系统的泛化能力。他们应用了[5$\times$5]高斯核去除噪声并对数据进行归一化。他们还对图像的像素值进行了零中心化。[[138](#bib.bibx138)]的研究使用了高斯滤波器和中值滤波器分别去除高斯噪声和椒盐噪声。[[155](#bib.bibx155)]
    也对数据进行了归一化，以保持零均值和单位方差。此外，他们应用了主成分分析和零相位分量分析数据白化，以消除数据间的相关性。'
- en: '[[164](#bib.bibx164)] evaluated the performance of the DL model based on the
    input representation of images. They applied many image pre-processing operations,
    such as histogram equalisation, automatic adjustment of the contrast of images
    and deep photo enhancement. They also used several vegetation indices including
    ExG, Excess Red, ExG-ExR, NDVI, Normalised Difference Index, Colour Index of Vegetation,
    Vegetative Index, and Modified Excess Green Index and Combined Indices. [[97](#bib.bibx97)]
    split the collected data into blocks which contained multiple plants. The blocks
    were then divided into sub-images with a single plant in them. After that, the
    histogram equalisation operation was performed to enhance the contrast of the
    sub-images.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[[164](#bib.bibx164)] 评估了基于图像输入表示的深度学习模型的性能。他们应用了许多图像预处理操作，如直方图均衡、图像对比度的自动调整和深度照片增强。他们还使用了几种植被指数，包括
    ExG、Excess Red、ExG-ExR、NDVI、标准化差异指数、植被颜色指数、植被指数、修改的过剩绿色指数和组合指数。[[97](#bib.bibx97)]
    将收集的数据分成包含多株植物的块，然后将这些块分割成包含单一植物的子图像。之后，执行了直方图均衡操作以增强子图像的对比度。'
- en: '[[132](#bib.bibx132)] applied orthorectification and radiometric corrections
    operation to process the satellite image. They then normalised the pixel values
    of each band. After that, the large satellite image was split into 2138 samples
    of pixel size 128$\times$128.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[[132](#bib.bibx132)] 对卫星图像进行了正射纠正和辐射校正操作。他们然后对每个波段的像素值进行了标准化。之后，将大的卫星图像拆分为2138个大小为128$\times$128的样本。'
- en: 7.2 Training Data Generation
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 训练数据生成
- en: To enlarge the size of the training data, in several related studies data augmentation
    was applied. It is a very useful technique when the dataset is not large enough
    [[161](#bib.bibx161)]. If there is a little variation [[138](#bib.bibx138)] or
    class imbalance [[11](#bib.bibx11)] among the images of the dataset, the image
    augmentation techniques are helpful. [[164](#bib.bibx164)] applied an augmentation
    to the dataset to determine the generalisation capability of their proposed approach.
    Table [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation ‣ 7 Dataset Preparation
    ‣ A Survey of Deep Learning Techniques for Weed Detection from Images") shows
    different types of data augmentation used in the relevant studies.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩大训练数据的规模，在几项相关研究中应用了数据增强。这是一种非常有用的技术，特别是当数据集不够大时 [[161](#bib.bibx161)]。如果数据集中的图像存在少量变异
    [[138](#bib.bibx138)] 或类别不平衡 [[11](#bib.bibx11)]，图像增强技术是有帮助的。[[164](#bib.bibx164)]
    对数据集应用了增强，以确定其提出的方法的泛化能力。表 [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation
    ‣ 7 Dataset Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") 显示了相关研究中使用的不同类型的数据增强。
- en: 'Table 4: Different types of data augmentation techniques used in the relevant
    studies'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 相关研究中使用的不同类型的数据增强技术'
- en: '|'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Augmentation &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '&#124; Technique &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 技术 &#124;'
- en: '| Description | Reference |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 参考 |'
- en: '| Rotation | Rotate the image to the right or left on an axis between 1^∘ and
    359^∘  [[144](#bib.bibx144)] | [[11](#bib.bibx11), [89](#bib.bibx89), [138](#bib.bibx138),
    [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [41](#bib.bibx41), [7](#bib.bibx7),
    [38](#bib.bibx38), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173)]
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 旋转 | 将图像绕轴旋转1^∘到359^∘ [[144](#bib.bibx144)] | [[11](#bib.bibx11), [89](#bib.bibx89),
    [138](#bib.bibx138), [40](#bib.bibx40), [47](#bib.bibx47), [2](#bib.bibx2), [41](#bib.bibx41),
    [7](#bib.bibx7), [38](#bib.bibx38), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173)]
    |'
- en: '| Scaling | Use zooming in/out to resize the image [[84](#bib.bibx84)]. | [[9](#bib.bibx9),
    [47](#bib.bibx47), [2](#bib.bibx2), [20](#bib.bibx20), [16](#bib.bibx16)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 缩放 | 使用放大/缩小来调整图像的大小 [[84](#bib.bibx84)]。 | [[9](#bib.bibx9), [47](#bib.bibx47),
    [2](#bib.bibx2), [20](#bib.bibx20), [16](#bib.bibx16)] |'
- en: '| Shearing | Shift one part of the image to a direction and the other part
    to the opposite direction [[144](#bib.bibx144)]. | [[9](#bib.bibx9), [89](#bib.bibx89),
    [47](#bib.bibx47), [20](#bib.bibx20), [173](#bib.bibx173)] |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 剪切 | 将图像的一部分移动到一个方向，而另一部分移动到相反的方向 [[144](#bib.bibx144)]。 | [[9](#bib.bibx9),
    [89](#bib.bibx89), [47](#bib.bibx47), [20](#bib.bibx20), [173](#bib.bibx173)]
    |'
- en: '| Flipping | Flip the image horizontally or vertically [[84](#bib.bibx84)].
    | [[9](#bib.bibx9), [1](#bib.bibx1), [138](#bib.bibx138), [47](#bib.bibx47), [2](#bib.bibx2),
    [38](#bib.bibx38), [29](#bib.bibx29), [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173),
    [122](#bib.bibx122)] |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 翻转 | 水平或垂直翻转图像 [[84](#bib.bibx84)]。 | [[9](#bib.bibx9), [1](#bib.bibx1),
    [138](#bib.bibx138), [47](#bib.bibx47), [2](#bib.bibx2), [38](#bib.bibx38), [29](#bib.bibx29),
    [20](#bib.bibx20), [16](#bib.bibx16), [173](#bib.bibx173), [122](#bib.bibx122)]
    |'
- en: '| Gamma Correction | Encode and decode the luminance values of an image [[19](#bib.bibx19)].
    | [[164](#bib.bibx164)] |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 伽马校正 | 编码和解码图像的亮度值 [[19](#bib.bibx19)]。 | [[164](#bib.bibx164)] |'
- en: '| Colour Space | Isolating a single colour channel, increase or decrease the
    brightness of the image, changing the intensity values in the histograms [[144](#bib.bibx144)].
    | [[11](#bib.bibx11), [164](#bib.bibx164), [9](#bib.bibx9), [40](#bib.bibx40),
    [2](#bib.bibx2), [29](#bib.bibx29), [122](#bib.bibx122)] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 颜色空间 | 隔离单一颜色通道，增加或减少图像的亮度，更改直方图中的强度值 [[144](#bib.bibx144)]。 | [[11](#bib.bibx11),
    [164](#bib.bibx164), [9](#bib.bibx9), [40](#bib.bibx40), [2](#bib.bibx2), [29](#bib.bibx29),
    [122](#bib.bibx122)] |'
- en: '| Colour Space Transformations | Increase or decrease the pixel values by a
    constant value and restricting pixel values to a certain min or max value [[144](#bib.bibx144)].
    | [[89](#bib.bibx89), [138](#bib.bibx138), [16](#bib.bibx16), [122](#bib.bibx122)]
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 颜色空间变换 | 通过常数值增加或减少像素值，并将像素值限制在某个最小或最大值 [[144](#bib.bibx144)]。 | [[89](#bib.bibx89),
    [138](#bib.bibx138), [16](#bib.bibx16), [122](#bib.bibx122)] |'
- en: '| Noise Injection | Injecting a matrix of random values to the image matrix.
    For example: Salt-Pepper noise, Gaussian noise etc [[144](#bib.bibx144)]. | [[138](#bib.bibx138),
    [40](#bib.bibx40), [122](#bib.bibx122)] |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 噪声注入 | 向图像矩阵注入随机值矩阵。例如：盐-胡椒噪声、高斯噪声等 [[144](#bib.bibx144)]。 | [[138](#bib.bibx138),
    [40](#bib.bibx40), [122](#bib.bibx122)] |'
- en: '| Kernel filtering | Sharpening or blurring the image [[144](#bib.bibx144)].
    | [[11](#bib.bibx11), [9](#bib.bibx9), [40](#bib.bibx40), [122](#bib.bibx122)]
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 核过滤 | 锐化或模糊图像 [[144](#bib.bibx144)]。 | [[11](#bib.bibx11), [9](#bib.bibx9),
    [40](#bib.bibx40), [122](#bib.bibx122)] |'
- en: '| Cropping | Remove a certain portion of an image [[154](#bib.bibx154)]. Usually
    this is done at random in case of data augmentation [[144](#bib.bibx144)]. | [[9](#bib.bibx9),
    [2](#bib.bibx2), [41](#bib.bibx41), [122](#bib.bibx122)] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 裁剪 | 去除图像的某一部分 [[154](#bib.bibx154)]。通常，在数据增强的情况下，这种操作是随机进行的 [[144](#bib.bibx144)]。
    | [[9](#bib.bibx9), [2](#bib.bibx2), [41](#bib.bibx41), [122](#bib.bibx122)] |'
- en: '| Translation | Shift the position of all the image pixels [[66](#bib.bibx66)].
    | [[9](#bib.bibx9), [1](#bib.bibx1), [20](#bib.bibx20)] |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 | 移动所有图像像素的位置 [[66](#bib.bibx66)]。 | [[9](#bib.bibx9), [1](#bib.bibx1),
    [20](#bib.bibx20)] |'
- en: As shown in Table [4](#S7.T4 "Table 4 ‣ 7.2 Training Data Generation ‣ 7 Dataset
    Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    it is observed that in most of the studies, different geometric transformation
    operation were applied to the data. Use of colour augmentation can be helpful
    to train a model for developing a real-time classification system. This is because
    the colour of the object varies depending on the lighting condition and motion
    of the sensors.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [4](#S7.T4 "表 4 ‣ 7.2 训练数据生成 ‣ 7 数据集准备 ‣ 深度学习技术在图像中的杂草检测调查") 所示，大多数研究中都对数据应用了不同的几何变换操作。使用颜色增强有助于训练模型以开发实时分类系统。这是因为对象的颜色会根据光照条件和传感器的运动而变化。
- en: Image data that are not collected from the real environments and created artificially
    or programmatically are known as synthetic data or images [[163](#bib.bibx163)].
    It is not always possible to manage a large amount of labelled data to train a
    model. In these cases, the use of synthetic data is an excellent alternative to
    use together with the real data. Several research studies show that artificial
    data might have a significant change in classifying images [[8](#bib.bibx8)].
    In weed detection using DL approaches, synthetic data generation technique is
    not applied very often. [[128](#bib.bibx128)] used synthetically generated images
    to train the model and achieved a good classification accuracy while testing on
    a real dataset.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 未从真实环境中收集而是人工或程序生成的图像数据被称为合成数据或图像 [[163](#bib.bibx163)]。管理大量标记数据以训练模型并不总是可行。在这些情况下，使用合成数据是与真实数据一起使用的绝佳替代方案。多个研究表明，人工数据在图像分类中可能具有显著变化
    [[8](#bib.bibx8)]。在使用深度学习方法进行杂草检测时，合成数据生成技术并不常用。[[128](#bib.bibx128)] 使用合成生成的图像来训练模型，并在测试真实数据集时获得了良好的分类准确率。
- en: On the other hand, [[121](#bib.bibx121)] created complex occlusion of crops
    and weeds and generated variation in leaf size, colour, and orientation by producing
    synthetic data. To minimise human effort for annotating data, [[33](#bib.bibx33)]
    generated synthetic data to train the model. For that purpose, they used a generic
    kinematic model of a leaf prototype to generate a single leaf of different plant
    species and then meshed that leaf to the artificial plant. Finally, they placed
    the plant in a virtual crop field for collecting the data without any extra effort
    for annotation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，[[121](#bib.bibx121)] 通过生成合成数据来创建作物和杂草的复杂遮挡，并在叶片的大小、颜色和方向上产生变化。为了减少人工标注数据的工作量，[[33](#bib.bibx33)]
    生成了合成数据来训练模型。为此，他们使用了一个通用的叶片原型运动模型，生成不同植物物种的单个叶片，然后将该叶片与人工植物进行网格化。最后，他们将植物放置在虚拟作物田中，以便在无需额外标注工作的情况下收集数据。
- en: '[[147](#bib.bibx147)] generated a 8000 synthetic dataset for labelling a real
    dataset. To create artificial data, they cropped out different parts of the plant,
    randomly selected any background from the real data, applied image processing
    (e.g. rotation, scaling, etc.), and added an artificial shadow using a Gaussian
    filter.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '[[147](#bib.bibx147)] 生成了一个8000个合成数据集，用于标注真实数据集。为了创建人工数据，他们裁剪了植物的不同部分，随机选择了真实数据中的任何背景，应用了图像处理（如旋转、缩放等），并使用高斯滤波器添加了人工阴影。'
- en: 7.3 Data Labelling
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 数据标注
- en: The majority of the reviewed publications used manually annotated data labelled
    by experts for training the deep learning model in a supervised manner. The researchers
    applied different annotations, such as bounding boxes annotation, pixel-level
    annotation, image-level annotation, polygon annotation, and synthetic labelling,
    based on the research need. Table [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7
    Dataset Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection
    from Images") shows different image annotation approaches used for weed detection.
    However, [[71](#bib.bibx71)] applied a semi-supervised method to label the images;
    they used a few labelled images to annotate the unlabelled data. On the other
    hand, [[35](#bib.bibx35)] proposed a semi-automatic labelling approach. Unlike
    semi-supervised data annotation, they did not use any manually labelled data,
    but applied the clustering method to label the data. First, they divided the data
    into different clusters according to their features and then labelled the clusters.
    Similar techniques were used by [[51](#bib.bibx51)]. [[170](#bib.bibx170)] separated
    the collected images into two parts; one with positive images that contained weeds,
    and the other of negative images without weeds. [[86](#bib.bibx86)] proposed an
    object-based approach to generate labelled data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数被审查的出版物使用了专家手工标注的数据以监督方式训练深度学习模型。研究人员根据研究需求应用了不同的标注方式，例如边界框标注、像素级标注、图像级标注、多边形标注和合成标注。表格
    [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7 Dataset Preparation ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images") 显示了用于杂草检测的不同图像标注方法。然而，[[71](#bib.bibx71)]
    应用了半监督方法来标注图像；他们使用了一些标注过的图像来标注未标注的数据。另一方面，[[35](#bib.bibx35)] 提出了半自动标注方法。与半监督数据标注不同，他们没有使用任何手工标注的数据，而是应用了聚类方法来标注数据。首先，他们根据特征将数据划分为不同的簇，然后标注这些簇。类似的技术也被[[51](#bib.bibx51)]
    使用。[[170](#bib.bibx170)] 将收集的图像分为两部分；一部分是包含杂草的正样本图像，另一部分是没有杂草的负样本图像。[[86](#bib.bibx86)]
    提出了基于对象的方法来生成标注数据。
- en: 'Table 5: Different image annotation techniques used for weed detection using
    deep learning'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：用于杂草检测的不同图像标注技术，采用深度学习方法
- en: '| Type of Image Annotation | Description | Reference |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 图像标注类型 | 描述 | 参考文献 |'
- en: '| Pixel Level Annotation | Label each pixel whether it belongs to crop or weed
    in the image. | [[68](#bib.bibx68), [17](#bib.bibx17), [96](#bib.bibx96), [65](#bib.bibx65),
    [135](#bib.bibx135), [172](#bib.bibx172), [82](#bib.bibx82), [51](#bib.bibx51),
    [1](#bib.bibx1), [7](#bib.bibx7), [44](#bib.bibx44), [29](#bib.bibx29), [20](#bib.bibx20),
    [2](#bib.bibx2), [43](#bib.bibx43), [18](#bib.bibx18), [9](#bib.bibx9), [79](#bib.bibx79),
    [161](#bib.bibx161), [103](#bib.bibx103), [63](#bib.bibx63), [62](#bib.bibx62),
    [108](#bib.bibx108), [102](#bib.bibx102), [132](#bib.bibx132), [119](#bib.bibx119),
    [114](#bib.bibx114), [127](#bib.bibx127), [86](#bib.bibx86), [147](#bib.bibx147)]
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 像素级注释 | 标记每个像素是否属于作物或杂草。 | [[68](#bib.bibx68), [17](#bib.bibx17), [96](#bib.bibx96),
    [65](#bib.bibx65), [135](#bib.bibx135), [172](#bib.bibx172), [82](#bib.bibx82),
    [51](#bib.bibx51), [1](#bib.bibx1), [7](#bib.bibx7), [44](#bib.bibx44), [29](#bib.bibx29),
    [20](#bib.bibx20), [2](#bib.bibx2), [43](#bib.bibx43), [18](#bib.bibx18), [9](#bib.bibx9),
    [79](#bib.bibx79), [161](#bib.bibx161), [103](#bib.bibx103), [63](#bib.bibx63),
    [62](#bib.bibx62), [108](#bib.bibx108), [102](#bib.bibx102), [132](#bib.bibx132),
    [119](#bib.bibx119), [114](#bib.bibx114), [127](#bib.bibx127), [86](#bib.bibx86),
    [147](#bib.bibx147)] |'
- en: '| Region Level Annotation | Bounding Boxes Annotation | There may be a mixture
    of weeds and crops in a single image. Using a bounding box the crops and weeds
    are labelled in the image. | [[64](#bib.bibx64), [134](#bib.bibx134), [11](#bib.bibx11),
    [81](#bib.bibx81), [104](#bib.bibx104), [111](#bib.bibx111), [41](#bib.bibx41),
    [141](#bib.bibx141), [72](#bib.bibx72), [16](#bib.bibx16), [47](#bib.bibx47),
    [146](#bib.bibx146), [116](#bib.bibx116), [128](#bib.bibx128), [162](#bib.bibx162),
    [37](#bib.bibx37), [122](#bib.bibx122), [142](#bib.bibx142)] |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 区域级注释 | 边界框注释 | 在单张图像中可能会混合出现杂草和作物。使用边界框标记图像中的作物和杂草。 | [[64](#bib.bibx64),
    [134](#bib.bibx134), [11](#bib.bibx11), [81](#bib.bibx81), [104](#bib.bibx104),
    [111](#bib.bibx111), [41](#bib.bibx41), [141](#bib.bibx141), [72](#bib.bibx72),
    [16](#bib.bibx16), [47](#bib.bibx47), [146](#bib.bibx146), [116](#bib.bibx116),
    [128](#bib.bibx128), [162](#bib.bibx162), [37](#bib.bibx37), [122](#bib.bibx122),
    [142](#bib.bibx142)] |'
- en: '|  | Polygon Annotation | This is used for semantic segmentation to detect
    irregular shaped object. It outlines the region of interest with arbitrary number
    of sides. | [[119](#bib.bibx119)] |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | 多边形注释 | 这用于语义分割，以检测不规则形状的对象。它用任意数量的边界轮廓出感兴趣区域。 | [[119](#bib.bibx119)]
    |'
- en: '| Image Level Annotation | Uses separate image for weeds and crops to train
    the model. | [[171](#bib.bibx171), [31](#bib.bibx31), [164](#bib.bibx164), [173](#bib.bibx173),
    [117](#bib.bibx117), [89](#bib.bibx89), [151](#bib.bibx151), [4](#bib.bibx4),
    [138](#bib.bibx138), [170](#bib.bibx170), [157](#bib.bibx157), [155](#bib.bibx155),
    [40](#bib.bibx40), [42](#bib.bibx42), [168](#bib.bibx168), [97](#bib.bibx97),
    [71](#bib.bibx71), [113](#bib.bibx113), [34](#bib.bibx34), [159](#bib.bibx159)]
    |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 图像级注释 | 使用不同的图像进行杂草和作物的模型训练。 | [[171](#bib.bibx171), [31](#bib.bibx31), [164](#bib.bibx164),
    [173](#bib.bibx173), [117](#bib.bibx117), [89](#bib.bibx89), [151](#bib.bibx151),
    [4](#bib.bibx4), [138](#bib.bibx138), [170](#bib.bibx170), [157](#bib.bibx157),
    [155](#bib.bibx155), [40](#bib.bibx40), [42](#bib.bibx42), [168](#bib.bibx168),
    [97](#bib.bibx97), [71](#bib.bibx71), [113](#bib.bibx113), [34](#bib.bibx34),
    [159](#bib.bibx159)] |'
- en: '| Synthetic Labelling | For training the model use generated and labelled data.
    | [[121](#bib.bibx121), [33](#bib.bibx33)] |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 合成标注 | 使用生成的标注数据进行模型训练。 | [[121](#bib.bibx121), [33](#bib.bibx33)] |'
- en: 'As summarised in Table [5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7 Dataset
    Preparation ‣ A Survey of Deep Learning Techniques for Weed Detection from Images"),
    commonly used annotation techniques are bounding boxes, pixel-wise labelling and
    image level annotation. However, plants are irregular in shape: by using polygon
    annotation, the images of crops and weeds can be separated accurately. Synthetic
    labelling approaches can minimise labelling costs and help to generate large annotated
    datasets.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[5](#S7.T5 "Table 5 ‣ 7.3 Data Labelling ‣ 7 Dataset Preparation ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images")所总结，常用的注释技术包括边界框、像素级标注和图像级注释。然而，植物形状不规则：通过使用多边形注释，可以准确区分作物和杂草的图像。合成标注方法可以最小化标注成本，并帮助生成大型标注数据集。
- en: 8 Detection Approaches
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 检测方法
- en: 'Studies in this area apply two broad approaches for detecting, localising,
    and classifying weeds in crops: i) localise every plant in an image and classify
    that image either as a crop or as a weed; ii) map the density of weeds in the
    field. To detect weeds in crops, the concept of “row planting” has been used.
    In some of these studies, there are further classification steps of the weed species.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的研究采用了两种广泛的方法来检测、定位和分类作物中的杂草：i）定位图像中的每个植物，并将该图像分类为作物或杂草；ii）绘制田间杂草的密度。为了检测作物中的杂草，使用了“行栽种”的概念。在一些研究中，还有进一步分类杂草种类的步骤。
- en: 8.1 Plant-based Classification
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 基于植物的分类
- en: To develop a weed management system, a major step is to classify every plant
    as weed or crop plant [[101](#bib.bibx101)]. The first problem is to detect weeds,
    followed by localisation and finally, classification. This approach is useful
    for real-time weed management techniques. For instance, [[126](#bib.bibx126)]
    developed a real-time weeding system where a robotic machine detected the weeds
    and used a knife to remove them. In this case, it was necessary to label individual
    plants, whether as a weed or crop plant. In traditional farming approaches, farmers
    usually apply a uniform amount of herbicide over the whole crop in a field. A
    machine needs to identify individual crop plants and weeds to apply automatic
    selective spraying techniques. Besides, identifying the weed species is also important
    to apply specific treatments [[102](#bib.bibx102)]. We have found that this approach
    has been used in most of the studies reported.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要开发一种杂草管理系统，一个主要步骤是将每一株植物分类为杂草或作物植物[[101](#bib.bibx101)]。第一个问题是检测杂草，接着是定位，最后是分类。这种方法对于实时杂草管理技术很有用。例如，[[126](#bib.bibx126)]开发了一个实时除草系统，其中一台机器人机器检测到杂草并使用刀具将其去除。在这种情况下，有必要对单独的植物进行标记，无论是作为杂草还是作物植物。在传统的耕作方法中，农民通常在整个田地上均匀喷洒除草剂。机器需要识别单独的作物植物和杂草，以应用自动选择性喷洒技术。此外，识别杂草种类对于施用特定处理也很重要[[102](#bib.bibx102)]。我们发现这种方法在大多数已报告的研究中都有使用。
- en: 8.2 Weed Mapping
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 杂草绘图
- en: Mapping weed density can also be helpful for site-specific weed management and
    can lead to a reduction in the use of herbicides. [[65](#bib.bibx65)] used the
    DL technique to map the density of weeds in a rice field. An appropriate amount
    of herbicides can be applied to a specific site based on the density map. The
    work in [[1](#bib.bibx1)] segmented the images and detected the weed presence
    in the region of that image. Using a deep learning approach, [[64](#bib.bibx64)]
    generated a weed distribution map of the field. In addition, some researchers
    argued that weed mapping helps to monitor the conditions of the field automatically
    [[134](#bib.bibx134), [135](#bib.bibx135)]. Farmers can monitor the distribution
    and spread of weeds, and can take action accordingly.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制杂草密度图对于特定场地的杂草管理也很有帮助，并且可以减少除草剂的使用。[[65](#bib.bibx65)]使用深度学习技术绘制了稻田中杂草的密度图。可以根据密度图向特定地点施用适量的除草剂。[[1](#bib.bibx1)]对图像进行了分割，并检测了图像区域中的杂草存在。使用深度学习方法，[[64](#bib.bibx64)]生成了田地的杂草分布图。此外，一些研究人员认为，杂草绘图有助于自动监控田地条件[[134](#bib.bibx134),
    [135](#bib.bibx135)]。农民可以监控杂草的分布和扩散，并据此采取行动。
- en: 9 Learning Methods
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 学习方法
- en: 9.1 Supervised Learning
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 监督学习
- en: Supervised learning occurs when the datasets for training and validation are
    labelled. The dataset passed in the DL model as input contains the image along
    with the corresponding labels. That means, in supervised training, the model learns
    how to create a map from a given input to a particular output based on the labelled
    dataset. Supervised learning is popular to solve classification and regression
    problems [[22](#bib.bibx22)]. In most of the related research the supervised learning
    approach was used to train the DL models. Section [10](#S10 "10 Deep Learning
    Architecture ‣ A Survey of Deep Learning Techniques for Weed Detection from Images")
    presents a detail description of those DL architectures.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习发生在训练和验证的数据集已标记的情况下。传递到深度学习（DL）模型的输入数据集包含图像以及相应的标签。这意味着，在监督训练中，模型学习如何根据标记的数据集从给定的输入创建到特定输出的映射。监督学习在解决分类和回归问题中很受欢迎[[22](#bib.bibx22)]。在大多数相关研究中，使用了监督学习方法来训练深度学习模型。第[10](#S10
    "10 Deep Learning Architecture ‣ A Survey of Deep Learning Techniques for Weed
    Detection from Images")节详细描述了这些深度学习架构。
- en: 9.2 Unsupervised Learning
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2 无监督学习
- en: Unsupervised learning occurs when the training set is not labelled. The dataset
    passed as input in the unsupervised model has no corresponding annotation. The
    models attempt to learn the structure of the data and extract distinguishable
    information or features from data. Using this process, the model becomes able
    to map the input to the particular output. From this, the objects in the whole
    dataset will be divided into separate groups or clusters. The features of the
    objects in a cluster are similar and differ from other clusters. This is how unsupervised
    learning can classify objects of a dataset into separate categories. Clustering
    is one of the applications of unsupervised learning [[14](#bib.bibx14)].
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习发生在训练集没有标签的情况下。无监督模型输入的数据集没有对应的注释。模型试图学习数据的结构并从数据中提取可区分的信息或特征。通过这一过程，模型能够将输入映射到特定的输出。由此，整个数据集中的对象将被分成不同的组或簇。簇中的对象特征相似，而与其他簇有所不同。这就是无监督学习如何将数据集中的对象分类到不同类别中的方式。聚类是无监督学习的一个应用
    [[14](#bib.bibx14)]。
- en: Most of the relevant studies used a supervised learning approach to detect and
    classify weeds in crops automatically. However, [[35](#bib.bibx35)] proposed unsupervised
    clustering algorithms with a semi-automatic data labelling approach in their research.
    They applied two clustering methods- Joint Unsupervised Learning (JULE) and Deep
    Clustering for Unsupervised Learning of Visual Features algorithms (DeepCluster).
    They developed the models using AlexNet [[83](#bib.bibx83)] and VGG-16 [[145](#bib.bibx145)]
    architecture and initialised with pre-trained weights. They achieved a promising
    result (accuracy 97%) in classifying weeds in crops and reduce the cost of manual
    data labelling.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数相关研究使用了监督学习方法来自动检测和分类作物中的杂草。然而，[[35](#bib.bibx35)] 在他们的研究中提出了带有半自动数据标注方法的无监督聚类算法。他们应用了两种聚类方法——联合无监督学习（JULE）和用于无监督学习视觉特征的深度聚类算法（DeepCluster）。他们使用AlexNet
    [[83](#bib.bibx83)] 和VGG-16 [[145](#bib.bibx145)] 架构进行模型开发，并用预训练权重进行初始化。他们在作物杂草分类中取得了令人满意的结果（准确率97%），并减少了手动数据标注的成本。
- en: '[[155](#bib.bibx155)] applied an unsupervised K-means clustering algorithm
    as a pre-training process and generate a feature dictionary. They then used those
    features to initialise the weights of the CNN model. They claimed that it can
    improve generalisation ability in feature extraction and resolve the unstable
    identification problem. The proposed approach shows better accuracy than SVM,
    Back Propagation neural network, and even CNN with randomly initialised weights.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[[155](#bib.bibx155)] 应用了无监督的K-means聚类算法作为预训练过程，并生成了特征词典。然后，他们利用这些特征来初始化CNN模型的权重。他们声称，这可以提高特征提取的泛化能力，并解决不稳定的识别问题。所提出的方法比SVM、反向传播神经网络甚至随机初始化权重的CNN表现出更好的准确性。'
- en: 9.3 Semi-supervised Learning
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3 半监督学习
- en: Semi-supervised learning takes the middle ground between supervised and unsupervised
    learning [[94](#bib.bibx94)]. A few researchers used Graph Convolutional Network
    (GCN) [[78](#bib.bibx78)] in their research, which is a semi-supervised model.
    The major difference between CNN and GCN is the structure of input data. CNN is
    for regular structured data, whereas GCN uses graph data structure [[105](#bib.bibx105)].
    We discuss the use of GCN in the related work in Section [10.4](#S10.SS4 "10.4
    Graph Convolutional Network (GCN) ‣ 10 Deep Learning Architecture ‣ A Survey of
    Deep Learning Techniques for Weed Detection from Images").
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习在监督学习和无监督学习之间找到了一种中间路径 [[94](#bib.bibx94)]。一些研究者在他们的研究中使用了图卷积网络（GCN） [[78](#bib.bibx78)]，这是一种半监督模型。CNN和GCN之间的主要区别在于输入数据的结构。CNN用于规则结构的数据，而GCN使用图数据结构
    [[105](#bib.bibx105)]。我们在第[10.4](#S10.SS4 "10.4 图卷积网络（GCN） ‣ 10 深度学习架构 ‣ 深度学习技术在图像杂草检测中的应用")节中讨论了GCN在相关工作中的应用。
- en: 10 Deep Learning Architecture
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10 深度学习架构
- en: Our analysis shows that the related studies apply different DL architectures
    to classify the weeds in crop plants based on the dataset and research goal. Most
    researchers compared their proposed models either with other DL architecture or
    with traditional machine learning approaches. Table LABEL:tab:different_DL_approach
    shows an overview of different DL approach used in weed detection. A CNN model
    generally consists of two basic parts- feature extraction and classification [[77](#bib.bibx77)].
    In related research, some researchers applied CNN models using various permutation
    of feature extraction and classification layers. However, in most cases, they
    preferred to use state-of-art CNN models like VGGNet [[145](#bib.bibx145)], ResNet
    (deep Residual Network) [[56](#bib.bibx56)], AlexNet [[83](#bib.bibx83)], InceptionNet
    [[152](#bib.bibx152)], and many more. Fully Convolutional Networks (FCNs) like
    SegNet [[10](#bib.bibx10)] and U-Net [[133](#bib.bibx133)] were also used in several
    studies.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析显示，相关研究根据数据集和研究目标应用了不同的深度学习（DL）架构来分类作物植物中的杂草。大多数研究人员将他们提出的模型与其他深度学习架构或传统机器学习方法进行了比较。表
    LABEL:tab:different_DL_approach 显示了用于杂草检测的不同深度学习方法的概述。一个CNN模型通常包括两个基本部分——特征提取和分类
    [[77](#bib.bibx77)]。在相关研究中，一些研究人员使用不同的特征提取和分类层的排列组合应用了CNN模型。然而，在大多数情况下，他们更倾向于使用先进的CNN模型，如VGGNet
    [[145](#bib.bibx145)]、ResNet（深度残差网络） [[56](#bib.bibx56)]、AlexNet [[83](#bib.bibx83)]、InceptionNet
    [[152](#bib.bibx152)]等。全卷积网络（FCNs）如SegNet [[10](#bib.bibx10)] 和 U-Net [[133](#bib.bibx133)]
    也在多个研究中被使用。
- en: 10.1 Convolutional Neural Network (CNN)
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1 卷积神经网络 (CNN)
- en: 10.1.1 Pre-trained Network
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1 预训练网络
- en: '[[151](#bib.bibx151)] applied six well known CNN models namely AlexNet, VGG-19,
    GoogLeNet, ResNet-50, ResNet-101 and Inception-v3\. They evaluated the network
    performance based on the transfer learning approach and found that pre-trained
    weights had a significant influence on training the model. They obtained the highest
    classification accuracy (98.7%) using the VGG-19 model, but it took the longest
    classification time. Considering that, the AlexNet model worked best for detecting
    volunteer potato plants in sugar beet according to their experimental setup. Even
    under varying light conditions, the model could classify plants with an accuracy
    of about 97%. The study of [[34](#bib.bibx34)] also supported that. They compared
    the classification accuracy of AlexNet with SVM, Adaboost – C4.5, and the Random
    Forest model. The AlexNet architecture performed better than other models in discriminating
    soybean crop, soil, grass, and broadleaf weeds. Similarly, [[162](#bib.bibx162)]
    reported that the AlexNet model with pre-trained weights showed excellent performance
    for detecting Rumex in grasslands. They also showed that by increasing heterogeneous
    characteristics of the input image might improve the model accuracy (90%). However,
    [[86](#bib.bibx86)] argued that to detect Rumex in grassland the VGG-16 model
    performs well with an accuracy of 92.1%.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[[151](#bib.bibx151)] 应用了六种著名的CNN模型，即AlexNet、VGG-19、GoogLeNet、ResNet-50、ResNet-101和Inception-v3。他们基于迁移学习方法评估了网络性能，发现预训练的权重对模型训练有显著影响。他们使用VGG-19模型获得了最高的分类准确率（98.7%），但分类时间也最长。考虑到这一点，根据他们的实验设置，AlexNet模型在检测志愿者土豆植物方面表现最佳。即使在光照条件变化的情况下，该模型也能以约97%的准确率分类植物。[[34](#bib.bibx34)]
    的研究也支持了这一点。他们比较了AlexNet与SVM、Adaboost – C4.5和随机森林模型的分类准确率。AlexNet架构在区分大豆作物、土壤、草和阔叶杂草方面优于其他模型。同样，[[162](#bib.bibx162)]
    报告称，带有预训练权重的AlexNet模型在检测草地中的酸模表现出色。他们还表明，通过增加输入图像的异质特征可能会提高模型的准确率（90%）。然而，[[86](#bib.bibx86)]
    认为，为了在草地中检测酸模，VGG-16模型以92.1%的准确率表现良好。'
- en: '[[157](#bib.bibx157)] demonstrated that, although ImageNet dataset does not
    contain the images of different plant species, the pre-trained weights of the
    dataset could still help to reduce the number of training iterations. They fine-tuned
    Inception-v3 architecture for classifying eighteen weed species and determining
    growth stages based on the number of leaves. The model achieved the classification
    accuracy of 46% to 78% and showed an average accuracy of 70% while counting the
    leaves. However, [[113](#bib.bibx113)] differed from them. They developed a multi-class
    weed image dataset consisting of eight nationally significant weed species. The
    dataset contains 17,509 annotated images collected from different locations of
    northern Australia. They also applied the pre-trained Inception-v3 model along
    with ResNet-50 to classify the weed species (source code is available here: [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)).
    The average classification accuracy of ResNet-50 (95.7%) was a little higher than
    Inception-v3 (95.1%). [[11](#bib.bibx11)] also used ResNet with pre-trained weights
    as they found it more useful to detect weeds.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[[157](#bib.bibx157)] 展示了尽管 ImageNet 数据集不包含不同植物物种的图像，但该数据集的预训练权重仍然可以帮助减少训练迭代次数。他们微调了
    Inception-v3 架构，用于分类十八种杂草物种，并基于叶片数量确定生长阶段。该模型达到了46%到78%的分类准确率，在计数叶片时显示了70%的平均准确率。然而，[[113](#bib.bibx113)]
    与他们的研究有所不同。他们开发了一个由八种国家重要杂草物种组成的多类杂草图像数据集。该数据集包含了17,509张从澳大利亚北部不同地点收集的标注图像。他们还应用了预训练的
    Inception-v3 模型和 ResNet-50 来分类杂草物种（源代码可在此处获取: [https://github.com/AlexOlsen/DeepWeeds](https://github.com/AlexOlsen/DeepWeeds)）。ResNet-50
    的平均分类准确率（95.7%）略高于 Inception-v3（95.1%）。[[11](#bib.bibx11)] 也使用了带预训练权重的 ResNet，因为他们发现它在杂草检测中更为有用。'
- en: According to [[171](#bib.bibx171)], Deep Convolutional Neural Network (DCNN)
    can perform well in detecting different species of weeds in bermudagrass. They
    used three pre-trained (from ImageNet dataset and KITTI dataset [[48](#bib.bibx48)])
    models including VGGNet, GoogLeNet and DetectNet. In another study, they added
    AlexNet architecture with the previous models for detecting weeds in perennial
    ryegrass [[170](#bib.bibx170)]. Though all the models performed well, DetectNet
    exhibited a bit higher F1 score of $\geq$0.99\. On the other hand, [[141](#bib.bibx141)]
    evaluated the performance of VGGNet, GoogLeNet, and DetectNet architecture using
    two variations of images (i.e., whole and cropped images). They also agreed that
    the DetectNet model could detect and classify weed in strawberry plants more accurately
    using cropped sub-images. They suggested that the most visible and prevalent part
    of the plant should be annotated rather than labelling the whole plant in the
    image.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [[171](#bib.bibx171)]，深度卷积神经网络（DCNN）在检测百慕大草中的不同杂草物种方面表现良好。他们使用了三个预训练模型（来自
    ImageNet 数据集和 KITTI 数据集 [[48](#bib.bibx48)]），包括 VGGNet、GoogLeNet 和 DetectNet。在另一项研究中，他们将
    AlexNet 架构添加到之前的模型中，用于检测多年生黑麦草中的杂草 [[170](#bib.bibx170)]。尽管所有模型表现良好，DetectNet
    展现出了稍高的 F1 分数 $\geq$0.99。另一方面，[[141](#bib.bibx141)] 评估了 VGGNet、GoogLeNet 和 DetectNet
    架构在使用两种图像变体（即完整图像和裁剪图像）下的性能。他们还同意 DetectNet 模型在使用裁剪子图像检测和分类草莓植物中的杂草时更加准确。他们建议标注植物的最明显和最常见部分，而不是标记图像中的整个植物。
- en: '[[89](#bib.bibx89)] proposed a model namely Filtered LBP (Local Binary Patterns)
    with Contour Mask and coefficient k (k-FLBPCM). They compared the model with VGG-16,
    VGG-19, ResNet-50, and Inception-v3 architecture. The k-FLBPCM method effectively
    classified barley, canola and wild radish with an accuracy of approximately 99%,
    which was better than other CNN models (source code is available here: [https://github.com/vinguyenle/k-FLBPCM-method](https://github.com/vinguyenle/k-FLBPCM-method)).
    The network was trained using pre-trained weights from ImageNet dataset.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[[89](#bib.bibx89)] 提出了一个名为 Filtered LBP（局部二值模式）与轮廓掩码和系数 k（k-FLBPCM）的模型。他们将该模型与
    VGG-16、VGG-19、ResNet-50 和 Inception-v3 架构进行了比较。k-FLBPCM 方法有效地以约99%的准确率分类大麦、油菜和野生芥菜，这优于其他
    CNN 模型（源代码可在此处获取: [https://github.com/vinguyenle/k-FLBPCM-method](https://github.com/vinguyenle/k-FLBPCM-method)）。该网络使用了来自
    ImageNet 数据集的预训练权重进行训练。'
- en: '[[7](#bib.bibx7)] compared the performance of LeNET [[92](#bib.bibx92)], AlexNet,
    cNET [[46](#bib.bibx46)], and sNET [[124](#bib.bibx124)] in their research. They
    found that cNET was better in classifying maize crop plants and their weeds. They
    further compared the performance of the original cNET architecture with the reduced
    number of filter layers (16 filter layers). The result reported that with pre-processed
    images, 16 filter layers were adequate to classify the crops and weeds. Besides,
    it made the model 2.5 times faster than its typical architecture and helped to
    detect weeds in real-time.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[[7](#bib.bibx7)] 在他们的研究中比较了 LeNET [[92](#bib.bibx92)]、AlexNet、cNET [[46](#bib.bibx46)]
    和 sNET [[124](#bib.bibx124)] 的性能。他们发现 cNET 在分类玉米作物和杂草方面表现更好。他们进一步比较了原始 cNET 架构与减少滤波器层数（16
    个滤波器层）的性能。结果报告显示，在处理过的图像下，16 个滤波器层足以分类作物和杂草。此外，这使得模型比其典型架构快 2.5 倍，并有助于实时检测杂草。'
- en: '[[117](#bib.bibx117)] analysed the performance of Faster R-CNN [[131](#bib.bibx131)],
    YOLO-v3 [[130](#bib.bibx130)], ResNet-50, ResNet-101, and Darknet-53 [[129](#bib.bibx129)]
    models to develop a smart sprayer for controlling weed in real-time. Based on
    precision and recall value, ResNet-50 model performed better than others. In contrast,
    [[16](#bib.bibx16)] applied the ResNet-101 model. They demonstrated that the size
    of the input image could affect the performance of ResNet-101 architecture. They
    used three different pixel sizes (i.e., 128px, 224px, and 360px) for their experiment
    and reported that model accuracy gets better by increasing the pixel size of the
    input image.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[[117](#bib.bibx117)] 分析了 Faster R-CNN [[131](#bib.bibx131)]、YOLO-v3 [[130](#bib.bibx130)]、ResNet-50、ResNet-101
    和 Darknet-53 [[129](#bib.bibx129)] 模型的性能，以开发用于实时控制杂草的智能喷雾器。根据精度和召回值，ResNet-50
    模型的表现优于其他模型。相比之下，[[16](#bib.bibx16)] 应用了 ResNet-101 模型。他们展示了输入图像的大小可能会影响 ResNet-101
    架构的性能。他们使用了三种不同的像素大小（即 128px、224px 和 360px）进行实验，并报告称，通过增加输入图像的像素大小，模型的准确性得到了改善。'
- en: '[[159](#bib.bibx159)] proposed a multi-modal DL approach for classifying species
    of weeds. In this approach, they trained five pre-trained DL models, including
    NASNet, ResNet, Inception–ResNet, MobileNet, and VGGNet independently. The Bayesian
    conditional probability-based technique and priority weight scoring method were
    used to calculate the score vector of models. The model with better scores has
    a higher priority on determining the classes of species. To classify weed species,
    they summed up the probability vectors generated by the softmax layer of each
    model and the species with the highest probability value was determined. According
    to the experimental results, they argued that the performance of this approach
    was better than a single DL model.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[[159](#bib.bibx159)] 提出了一个多模态 DL 方法用于分类杂草物种。在这种方法中，他们独立训练了五个预训练的 DL 模型，包括
    NASNet、ResNet、Inception–ResNet、MobileNet 和 VGGNet。采用了基于贝叶斯条件概率的方法和优先权重评分方法来计算模型的评分向量。得分较高的模型在确定物种类别时具有更高的优先级。为了分类杂草物种，他们将每个模型的
    softmax 层生成的概率向量进行汇总，并确定概率值最高的物种。根据实验结果，他们认为这种方法的性能优于单一的 DL 模型。'
- en: 10.1.2 Training from Scratch
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.2 从头训练
- en: '[[38](#bib.bibx38)] argued that, a CNN model initialised with pre-trained weights
    which was not trained on any plant images would not work well. They therefore
    built a new architecture using a combination of convolutional layers, batch normalisation,
    activation functions, max-pooling layers, fully connected layers, and residual
    layers according to their need. The model was used to classify twenty-two plant
    species, and they achieved a classification accuracy ranging from 33% to 98%.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[[38](#bib.bibx38)] 认为，使用预训练权重初始化的 CNN 模型，如果没有在任何植物图像上训练，效果会很差。因此，他们根据需求构建了一个新的架构，结合了卷积层、批量归一化、激活函数、最大池化层、全连接层和残差层。该模型用于分类二十二种植物物种，取得了
    33% 到 98% 的分类准确率。'
- en: '[[108](#bib.bibx108)] built a CNN model for blob wise discrimination of crops
    and weeds. They used multi-spectral images to train the model. They investigated
    different combinations of convolutional layers and fully connected layers to explore
    an optimised, light-weight and over-fitting problem-free model. Finally, using
    three convolutional layers and two fully connected layers, they obtained a better
    result. They stated that this approach did not have any geometric priors like
    planting the crops in rows. [[41](#bib.bibx41)] claimed in their research that
    the classification accuracy of the CNN model depended on the number of the hyperspectral
    band and the resolution of the image patch. They also built a CNN model using
    a combination of convolutional, nonlinear transformation, pooling and dropout
    layers. In further research, they proved that a CNN model trained with a higher
    number of bands could classify images more accurately than HoG (Histogram of oriented
    Gradients) based method [[42](#bib.bibx42)].'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[[108](#bib.bibx108)] 构建了一个用于作物和杂草区分的CNN模型。他们使用多光谱图像来训练模型。通过研究不同的卷积层和全连接层的组合，探索了一个优化的、轻量级的且不易过拟合的模型。最终，使用了三层卷积层和两层全连接层，获得了更好的结果。他们表示，这种方法没有任何几何先验，例如作物按行种植的模式。[[41](#bib.bibx41)]
    在他们的研究中声称，CNN模型的分类准确性依赖于高光谱波段的数量和图像块的分辨率。他们还使用了卷积、非线性变换、池化和丢弃层的组合来构建CNN模型。在进一步的研究中，他们证明了用更多波段训练的CNN模型比基于HoG（方向梯度直方图）的方法能更准确地分类图像[[42](#bib.bibx42)]。'
- en: '[[111](#bib.bibx111)] compared CNN’s performance with SVM (61.47%) and K-Nearest
    Neighbour (KNN) algorithm (56.84%) and found that CNN could distinguish crop plants
    from weeds better. They used six convolutional layers and three fully connected
    layers in the CNN architecture to achieve the accuracy of 92.6%. They also evaluated
    the accuracy of CNN using the original images and the pre-processed images. The
    experimental results suggested that classification accuracy improved by using
    pre-processed images. [[138](#bib.bibx138)] agreed that CNN offers better accuracy
    than SVM and ANN in detecting weeds in crop plants because of its deep learning
    ability. [[97](#bib.bibx97)] employed a CNN architecture that consists of three
    convolutional, three pooling, four Dropout layers, and a fully connected layer
    for developing a low-cost weed recognition system. Their experiment also proved
    that the performance of the CNN model in classification was better than the HoG
    and LBP methods. [[173](#bib.bibx173)] also demonstrated that the CNN model was
    better than SVM for detecting broad-leaf weeds in pastures. They used a CNN model
    with six convolutional layers and three fully connected classification layers.
    The model could recognise weeds with an accuracy of 96.88%, where SVM achieved
    maximum accuracy of 89.4%.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[[111](#bib.bibx111)] 将CNN的性能与SVM（61.47%）和K-Nearest Neighbour（KNN）算法（56.84%）进行了比较，发现CNN能够更好地区分作物植物和杂草。他们在CNN架构中使用了六层卷积层和三层全连接层，达到了92.6%的准确率。他们还评估了使用原始图像和预处理图像的CNN准确性。实验结果表明，使用预处理图像可以提高分类准确性。[[138](#bib.bibx138)]
    认为，由于深度学习能力，CNN在检测作物植物中的杂草时比SVM和ANN提供了更好的准确性。[[97](#bib.bibx97)] 采用了一种CNN架构，包括三层卷积、三层池化、四层丢弃和一层全连接层，用于开发低成本的杂草识别系统。他们的实验还证明了CNN模型在分类中的表现优于HoG和LBP方法。[[173](#bib.bibx173)]
    还展示了CNN模型在检测牧场中的阔叶杂草方面优于SVM。他们使用了一个具有六层卷积层和三层全连接分类层的CNN模型。该模型能够以96.88%的准确率识别杂草，而SVM的最高准确率为89.4%。'
- en: '[[121](#bib.bibx121)] used synthetic data to train their CNN model and evaluated
    it on real data. They built a CNN model with five convolutional layers and two
    fully connected layers. The results showed that CNN could classify crop plants
    and weeds very well from natural images and with multiple occlusion. Although
    [[128](#bib.bibx128)] applied the same architecture in their research, they argued
    that the Scatter Transform method achieved better accuracy with a small dataset
    than the CNN architecture. They compared several machine learning approaches like
    Scatter Transform, LBP, GLCM, Gabor filter with the CNN model. They also used
    synthetic data for training and evaluated the models’ performance on real field
    images.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '[[121](#bib.bibx121)] 使用合成数据训练了他们的CNN模型，并在真实数据上进行了评估。他们构建了一个具有五个卷积层和两个全连接层的CNN模型。结果表明，CNN可以很好地从自然图像中分类作物植物和杂草，并且能够处理多重遮挡。尽管[[128](#bib.bibx128)]
    在他们的研究中应用了相同的架构，但他们认为，Scatter Transform方法在小数据集上比CNN架构取得了更好的准确性。他们将几种机器学习方法如Scatter
    Transform、LBP、GLCM、Gabor滤波器与CNN模型进行了比较。他们还使用合成数据进行训练，并在真实田间图像上评估了模型的性能。'
- en: 10.2 Region Proposal Networks (RPN)
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2 区域提议网络（RPN）
- en: 'Based on the tiny YOLO-v3 [[169](#bib.bibx169)] framework, [[47](#bib.bibx47)]
    proposed a DL model which speeds up the inference time of classification (source
    code is available here: [https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing](https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing)).
    They added two extra convolutional layers to the original model for better feature
    fusion and also reduced the number of detection scales to two. They trained the
    model with both synthetic data and real data. Although YOLO-v3 archived better
    classification accuracy in the experiments, they recommended the tiny YOLO-v3
    model for real-time application. [[142](#bib.bibx142)] also used tiny YOLO-v3
    model to detect goosegrass in strawberry and tomato plants.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 基于微型YOLO-v3 [[169](#bib.bibx169)] 框架，[[47](#bib.bibx47)] 提出了一个深度学习模型，该模型加快了分类的推理时间（源代码可在这里获取：[https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing](https://drive.google.com/file/d/1-E_b_5oqQgAK2IkzpTf6E3X1OPm0pjqy/view?usp=sharing)）。他们在原始模型中添加了两个额外的卷积层，以实现更好的特征融合，并将检测尺度数量减少到两个。他们使用合成数据和真实数据训练了该模型。尽管YOLO-v3在实验中实现了更好的分类准确性，但他们推荐将微型YOLO-v3模型用于实时应用。[[142](#bib.bibx142)]
    还使用微型YOLO-v3模型检测草莓和番茄植物中的狗尾草。
- en: YOLO-v3 and tiny YOLO-v3 models were also employed in a research by [[116](#bib.bibx116)].
    The aim was to find a low-cost, smart weed management system. They applied the
    models on two machines with different hardware configurations. Their paper reported
    that YOLO-v3 showed good performance when tested on powerful and expensive computers,
    but the processing speed decreased if executed on a lower power computer. From
    their experiments, they came to the conclusion that to save the hardware cost,
    the tiny YOLO-v3 model was better. [[173](#bib.bibx173)] also preferred to use
    tiny YOLO-v3 instead of YOLO-v3, because it was a lightweight method and took
    less time and resources to classify objects. In contrast, [[31](#bib.bibx31)]
    proposed to use YOLO-v3 with a relatively larger input image size (832 $\times$
    832 pixels). They argued that the model performed better in their research with
    a small dataset. They agreed that tiny YOLO-v3 or Fast YOLO-v3 could improve the
    detection speed, but there was a need to compromise with the model accuracy.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO-v3和微型YOLO-v3模型也被[[116](#bib.bibx116)]的研究采用。目标是寻找一种低成本、智能的杂草管理系统。他们将这些模型应用于两台具有不同硬件配置的机器上。他们的论文报告称，YOLO-v3在强大且昂贵的计算机上测试时表现良好，但如果在低功耗计算机上执行，则处理速度会下降。通过他们的实验，他们得出的结论是，为了节省硬件成本，微型YOLO-v3模型更好。[[173](#bib.bibx173)]
    也更倾向于使用微型YOLO-v3而不是YOLO-v3，因为它是一种轻量级的方法，分类对象所需的时间和资源更少。相比之下，[[31](#bib.bibx31)]
    提议使用YOLO-v3并将输入图像尺寸设置得相对较大（832 $\times$ 832 像素）。他们认为，模型在他们的研究中在小数据集上表现更好。他们一致认为，微型YOLO-v3或Fast
    YOLO-v3可以提高检测速度，但需要在模型准确性上做出妥协。
- en: '[[146](#bib.bibx146)] trained and evaluated the performance of a pre-trained
    Faster R-CNN and SSD (Single Shot Detector) [[100](#bib.bibx100)] object detection
    models to detect late-season weed in soybean fields. Moreover, they compared these
    object detection models with patch-based CNN model. The result showed that Faster
    R-CNN performed better in terms of weed detection accuracy and inference speed.
    [[72](#bib.bibx72)] proposed the Faster R-CNN model to detect the weeds and crop
    plants and to count the number of seedlings from the video frames. They used Inception-ResNet-v2
    architecture as the feature extractor. On the other hand, by applying the Mask
    R-CNN model on “Plant Seedlings Dataset” [[119](#bib.bibx119)] achieved more than
    98% classification accuracy. They argued that Mask R-CNN detected plant species
    more accurately with less training time than FCN.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '[[146](#bib.bibx146)] 训练并评估了预训练的 Faster R-CNN 和 SSD (Single Shot Detector)
    [[100](#bib.bibx100)] 物体检测模型，以检测大豆田中的晚季杂草。此外，他们将这些物体检测模型与基于补丁的 CNN 模型进行了比较。结果显示，Faster
    R-CNN 在杂草检测准确率和推理速度方面表现更佳。[[72](#bib.bibx72)] 提出了 Faster R-CNN 模型来检测杂草和作物植物，并从视频帧中统计幼苗数量。他们使用了
    Inception-ResNet-v2 架构作为特征提取器。另一方面，通过在“Plant Seedlings Dataset” [[119](#bib.bibx119)]
    上应用 Mask R-CNN 模型，达到了超过 98% 的分类准确率。他们认为 Mask R-CNN 比 FCN 更准确地检测植物种类，并且训练时间更短。'
- en: '[[114](#bib.bibx114)] compared two RPN models, namely YOLO-v3 and Mask R-CNN
    with SVM. The classification accuracy of RPN architectures was 94%, whereas SVM
    achieved 88%. However, they reported that as SVM required less processing capacity,
    it could be used for IoT based solution.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[[114](#bib.bibx114)] 比较了两种 RPN 模型，即 YOLO-v3 和带 SVM 的 Mask R-CNN。RPN 架构的分类准确率为
    94%，而 SVM 达到了 88%。然而，他们报告说，由于 SVM 需要更少的处理能力，它可以用于基于 IoT 的解决方案。'
- en: 10.3 Fully Convolutional Networks (FCN)
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3 全卷积网络（FCN）
- en: Unlike CNN, FCN replaces all the fully connected layers with convolutional layers
    and uses a transposed convolution layer to reconstruct the image with the same
    size as the input. It helps to predict the output by making a one-to-one correspondence
    with the input image in the spatial dimension [[143](#bib.bibx143), [65](#bib.bibx65)].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CNN 不同，FCN 用卷积层替代了所有的全连接层，并使用转置卷积层来重建与输入图像大小相同的图像。这有助于通过在空间维度上与输入图像进行一对一的对应来预测输出
    [[143](#bib.bibx143), [65](#bib.bibx65)]。
- en: '[[63](#bib.bibx63)] compared the performance of AlexNet, VGGNet, and GoogLeNet
    as the base model for FCN architecture. VGGNet achieved the best accuracy among
    those. They further compared the model with patch-based CNN and pixel-based CNN
    architectures. The result showed that the VGG-16 based FCN model achieved the
    highest classification accuracy. On the other hand, [[64](#bib.bibx64)] applied
    ResNet-101 and VGG-16 as a baseline model of FCN for segmentation. They also compared
    the performance of the FCN models with a pixel-based SVM model. In their case,
    ResNet-101 based FCN architecture performed better. [[9](#bib.bibx9)] compared
    two FCN architecture for detecting weeds in canola fields, i.e., SegNet and U-Net.
    They used VGG-16 and ResNet-50 as the encoder block in both the models. The SegNet
    with ResNet-50 as the base model achieved the highest accuracy.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[[63](#bib.bibx63)] 比较了 AlexNet、VGGNet 和 GoogLeNet 作为 FCN 架构的基础模型。这些模型中，VGGNet
    实现了最佳准确率。他们进一步将模型与基于补丁的 CNN 和基于像素的 CNN 架构进行了比较。结果显示，基于 VGG-16 的 FCN 模型达到了最高的分类准确率。另一方面，[[64](#bib.bibx64)]
    采用 ResNet-101 和 VGG-16 作为 FCN 的基线模型进行分割。他们还将 FCN 模型的性能与基于像素的 SVM 模型进行了比较。在他们的案例中，基于
    ResNet-101 的 FCN 架构表现更佳。[[9](#bib.bibx9)] 比较了两种用于检测油菜田杂草的 FCN 架构，即 SegNet 和 U-Net。他们在两个模型中使用了
    VGG-16 和 ResNet-50 作为编码块。以 ResNet-50 为基础模型的 SegNet 实现了最高的准确率。'
- en: 'According to [[104](#bib.bibx104)], SegNet (accuracy 92.7%) architecture was
    better than traditional FCN (accuracy 89.5%) and U-Net (accuracy 70.8%) for weed
    image segmentation when classifying rice plants and weeds in the paddy field.
    The study of [[1](#bib.bibx1)] reported that the accuracy of image segmentation
    depended on the size of the dataset. That is why it is difficult to train a model
    from scratch. To address this problem, they applied transfer learning and real-time
    data augmentation to train the model. In their experiment, they used VGG-16 based
    SegNet architecture. They applied three different transfer learning approaches
    for VGG-16\. Moreover, the performance of the model was compared with the VGG-19
    based architecture. The VGG-16 based SegNet achieved the highest accuracy of 96%
    when they used pre-trained weights only for feature extraction and the shallow
    machine learning classifier (i.e., SVM) for segmentation. [[134](#bib.bibx134)]
    also applied SegNet with the pre-trained VGG-16 as the base model (source code
    is available here: [https://github.com/inkyusa/weedNet](https://github.com/inkyusa/weedNet)).
    They trained the model by varying the number of channels in the input images.
    They then compared the inference speed and accuracy of different arrangements
    by deploying the model on an embedded GPU system, which was carried out by a small
    micro aerial vehicle (MAV). [[161](#bib.bibx161)] compared the performance of
    SegNet-512 and SegNet-256 encoder-decoder architectures for semantic segmentation
    of weeds in crop plants. The experiment proved that SegNet-512 was better for
    classification. In the study of [[33](#bib.bibx33)], the SegNet model was trained
    using synthetic data, and the performance was evaluated on a real crop and weed
    dataset.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[104](#bib.bibx104)]，在对稻田中水稻和杂草进行分类时，SegNet（准确率92.7%）架构比传统的FCN（准确率89.5%）和U-Net（准确率70.8%）在杂草图像分割上表现更好。[[1](#bib.bibx1)]的研究报告指出，图像分割的准确性取决于数据集的大小。这也是从头开始训练模型的困难所在。为了解决这个问题，他们应用了迁移学习和实时数据增强来训练模型。在他们的实验中，他们使用了基于VGG-16的SegNet架构。他们为VGG-16应用了三种不同的迁移学习方法。此外，他们还将模型的性能与基于VGG-19的架构进行了比较。当仅使用预训练权重进行特征提取，并使用浅层机器学习分类器（即SVM）进行分割时，基于VGG-16的SegNet达到了96%的最高准确率。[[134](#bib.bibx134)]还应用了以预训练VGG-16为基础模型的SegNet（源代码可在此处获取：[https://github.com/inkyusa/weedNet](https://github.com/inkyusa/weedNet)）。他们通过改变输入图像中的通道数量来训练模型。然后，他们通过在嵌入式GPU系统上部署模型，比较了不同安排的推理速度和准确性，这项工作由一台小型微型航空器（MAV）执行。[[161](#bib.bibx161)]比较了SegNet-512和SegNet-256编码器-解码器架构在作物植物中杂草语义分割的性能。实验证明，SegNet-512在分类上表现更好。在[[33](#bib.bibx33)]的研究中，SegNet模型使用合成数据进行训练，性能在真实作物和杂草数据集上进行评估。
- en: '[[44](#bib.bibx44)] proposed U-Net architecture using VGG-16 as an encoder
    for semantic segmentation. They also applied a VGG-16 model for classifying the
    crop plants and weeds. They also trained the model with one dataset containing
    sunflower crop and evaluated it with two different datasets with carrots and sugar
    beets crops. In the work of [[132](#bib.bibx132)], a ResNet based U-Net model
    was employed to map the presence of gamba grass in the satellite image. However,
    [[127](#bib.bibx127)] compared the performance of DeepLab-v3 [[30](#bib.bibx30)]
    with SegNet and U-Net model in their research. The results demonstrated that DeepLab-v3
    architecture achieved better classification accuracy using class balanced data
    that has greater spatial context.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[[44](#bib.bibx44)]提出了使用VGG-16作为编码器的U-Net架构用于语义分割。他们还应用了VGG-16模型来分类作物植物和杂草。他们还使用包含向日葵作物的数据集训练模型，并用包含胡萝卜和甜菜作物的两个不同数据集进行评估。在[[132](#bib.bibx132)]的工作中，使用了基于ResNet的U-Net模型来映射卫星图像中的甘蔗草存在。然而，[[127](#bib.bibx127)]在其研究中比较了DeepLab-v3
    [[30](#bib.bibx30)]与SegNet和U-Net模型的性能。结果表明，DeepLab-v3架构在使用类平衡数据时，具有更大的空间上下文，达到了更好的分类准确率。'
- en: '[[103](#bib.bibx103)] also proposed FCN architecture using DenseNet as a baseline
    model. Their novel approach provided a pixel-wise semantic segmentation of crop
    plants and weeds. The work of [[102](#bib.bibx102)] proposed a task-specific decoder
    network. As the plants were sown at a regular distance, they trained the model
    in a way so that the model could learn the spatial plant arrangement from the
    image sequence. They then fused this sequential feature with the visual features
    to localise and classify weeds in crop plants. [[37](#bib.bibx37)] used FCN architecture
    not only for segmentation but also for generating bounding boxes around the plants.
    They applied pre-trained GoogLeNet architecture as the base model.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[[103](#bib.bibx103)]还提出了使用DenseNet作为基线模型的FCN架构。他们的新方法提供了作物植物和杂草的像素级语义分割。[[102](#bib.bibx102)]的工作提出了一种任务特定的解码器网络。由于植物以规则的间距播种，他们训练模型以便模型能够从图像序列中学习空间植物排列。然后，他们将这种序列特征与视觉特征融合，以定位和分类作物植物中的杂草。[[37](#bib.bibx37)]使用FCN架构不仅进行分割，还生成植物周围的边界框。他们应用了预训练的GoogLeNet架构作为基础模型。'
- en: According to [[164](#bib.bibx164)], changes in the input representation could
    make a difference in classification performance. They employed the encoder-decoder
    deep learning network for semantic segmentation of crop and weed plants by initialising
    the input layers with pre-trained weights. They evaluated the model with different
    input representation by including NIR information with colour space transformation
    on the input, which improved crop-weed segmentation and classification accuracy
    (96%). [[135](#bib.bibx135)] also evaluated different input representation to
    train the network. They applied VGG-16 based SegNet architecture for detecting
    background, crop plants and weeds. The model was evaluated by varying the number
    of spectral bands and changing the hyper-parameters. The experimental results
    showed that the model achieved far better accuracy by using nine spectral channels
    of an image rather than the RGB image.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[164](#bib.bibx164)]，输入表示的变化可能会影响分类性能。他们采用了编码器-解码器深度学习网络，通过用预训练权重初始化输入层来进行作物和杂草植物的语义分割。他们通过在输入中加入NIR信息和颜色空间转换来评估模型，这提高了作物-杂草分割和分类准确率（96%）。[[135](#bib.bibx135)]也评估了不同的输入表示来训练网络。他们应用了基于VGG-16的SegNet架构来检测背景、作物植物和杂草。模型通过变化光谱带数量和调整超参数进行评估。实验结果表明，使用九个光谱通道的图像比RGB图像取得了更好的准确率。
- en: '[[62](#bib.bibx62)] stated that the original FCN-4s architecture was designed
    for PASCAL VOC 2011 dataset, which had 1000 classes of objects. However, their
    dataset had only three categories (i.e., rice, weeds, and others). As a result
    they reduced the feature maps of the intermediate layers to 2048\. They then compared
    the accuracy and efficiency of the model with original FCN-8s and DeepLab architecture
    and proved that the modified FCN-4s model performed better. For the same reason,
    [[18](#bib.bibx18)] simplified the original architecture of SegNet and named it
    as SegNet‐Basic. They decreased the number of convolutional layers from 13 to
    4.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[[62](#bib.bibx62)]指出，原始的FCN-4s架构是为PASCAL VOC 2011数据集设计的，该数据集有1000个物体类别。然而，他们的数据集只有三个类别（即稻米、杂草和其他）。因此，他们将中间层的特征图减少到2048。他们比较了模型与原始FCN-8s和DeepLab架构的准确性和效率，并证明修改后的FCN-4s模型表现更好。出于同样的原因，[[18](#bib.bibx18)]简化了原始的SegNet架构，并将其命名为SegNet‐Basic。他们将卷积层的数量从13减少到4。'
- en: One of the problems with the basic architecture of FCN is that the spatial features
    can not be recovered properly. The prediction accuracy can be decreased due to
    this issue. To address this problem, [[65](#bib.bibx65)] improved the model by
    adding skip architecture (SA), fully connected conditional random fields and partially
    connected conditional random fields. They fine-tuned AlexNet, VGGNet, GoogLeNet,
    and ResNet based FCN. They then compared the performance of different FCNs and
    Object-based image analysis (OBIA) method. Experimental results reported that
    the VGGNet-based FCN with proposed improvements achieved the highest accuracy.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: FCN基本架构的一个问题是空间特征无法正确恢复。由于这个问题，预测准确性可能会降低。为了解决这个问题，[[65](#bib.bibx65)]通过添加跳跃架构（SA）、全连接条件随机场和部分连接条件随机场来改进模型。他们微调了基于AlexNet、VGGNet、GoogLeNet和ResNet的FCN。然后，他们比较了不同FCN和基于对象的图像分析（OBIA）方法的性能。实验结果报告说，基于VGGNet的FCN在提出的改进下达到了最高的准确率。
- en: '[[20](#bib.bibx20)] modified the original U-Net architecture for pixel-level
    classification of crop plants and weeds. They added a convolutional layer with
    a kernel size of 1$\times$1\. For that change, they adjusted the input size of
    the network. Besides, replacing the ReLU activation functions with the Exponential
    Linear Unit (ELU), they used adadelta optimiser algorithm instead of the stochastic
    gradient descent and included dropout layers in between convolutional layers.
    [[122](#bib.bibx122)] also modified the U-Net model to detect one species of weed
    in grasslands.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '[[20](#bib.bibx20)] 修改了原始的 U-Net 架构，以实现对作物植物和杂草的像素级分类。他们添加了一个核大小为 1$\times$1
    的卷积层。由于这一改变，他们调整了网络的输入大小。此外，他们将 ReLU 激活函数替换为指数线性单元（ELU），使用了 adadelta 优化算法替代了随机梯度下降，并在卷积层之间加入了
    dropout 层。[[122](#bib.bibx122)] 也修改了 U-Net 模型，以检测草地中的一种杂草。'
- en: 10.4 Graph Convolutional Network (GCN)
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4 图卷积网络（GCN）
- en: '[[61](#bib.bibx61)] proposed Graph Weeds Net (GWN). GWN is a graph-based deep
    learning architecture to classify weed species. [[61](#bib.bibx61)] used ResNet-50
    and DenseNet-202 model to learn vertex features with graph convolution layers,
    vertex-wise dense layers, and the multi-level graph pooling mechanisms included
    in GWN architecture. Here, an RGB image was represented as a multi-scale graph.
    The graph-based model with DenseNet-202 architecture achieved the classification
    accuracy of 98.1%.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[[61](#bib.bibx61)] 提出了图杂草网络（GWN）。GWN 是一种基于图的深度学习架构，用于分类杂草物种。[[61](#bib.bibx61)]
    使用了 ResNet-50 和 DenseNet-202 模型，通过图卷积层、逐顶点密集层以及 GWN 架构中包含的多级图池化机制来学习顶点特征。在这里，RGB
    图像被表示为多尺度图。基于 DenseNet-202 架构的图模型达到了 98.1% 的分类准确率。'
- en: '[[71](#bib.bibx71)] proposed ResNet-101 based graph convolutional network in
    their research. They chose GCN, because it was a semi-supervised learning approach.
    Moreover, the feature relationships were captured using a graph structure. In
    this model, the label information was shared by neighbouring vertices of the graph,
    which make the learning more accurate with limited annotated data. They compared
    the proposed model with AlexNet, VGG-16, and ResNet-101 architecture on four different
    datasets. The GCN approach achieved 97.80%, 99.37%, 98.93% and 96.51% classification
    accuracy for each dataset.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[[71](#bib.bibx71)] 在他们的研究中提出了基于 ResNet-101 的图卷积网络。他们选择了 GCN，因为它是一种半监督学习方法。此外，特征关系通过图结构被捕捉。在该模型中，标签信息由图的邻近顶点共享，这使得在有限标注数据下学习更为准确。他们将所提出的模型与
    AlexNet、VGG-16 和 ResNet-101 架构在四个不同的数据集上进行了比较。GCN 方法在每个数据集上分别达到了 97.80%、99.37%、98.93%
    和 96.51% 的分类准确率。'
- en: 10.5 Hybrid Networks (HN)
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5 混合网络（HN）
- en: Hybrid architectures are those where the researchers combine the characteristics
    of two or more DL models. For instance, [[26](#bib.bibx26)] proposed the AgroAVNET
    model, which was a hybrid of AlexNet and VGGNet architecture. They chose VGGNet
    for setting the depth of filters and used the normalisation concept of AlexNet.
    They then compared the performance of the AgroAVNET network with the original
    AlexNet and VGGNet and their different variants. All the parameters were initialised
    using pre-trained weights except for the third layer of the fully connected layers.
    They initialised that randomly. The AgroAVNET model outperformed others with a
    classification accuracy of 98.21%. However, [[43](#bib.bibx43)] adopted the feature
    concatenation approach in their research. They combined a super pixel-based LBP
    (SPLBP) method to extract local texture features, CNN for learning the spatial
    features and SVM for classification. They compared their proposed FCN-SPLBP model
    with CNN, LBP, FCN, and SPLBP architectures.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 混合架构是指研究人员结合了两种或更多深度学习模型的特征。例如，[[26](#bib.bibx26)] 提出了 AgroAVNET 模型，这是 AlexNet
    和 VGGNet 架构的混合体。他们选择了 VGGNet 来设置滤波器的深度，并使用了 AlexNet 的标准化概念。然后，他们将 AgroAVNET 网络的性能与原始的
    AlexNet 和 VGGNet 以及它们的不同变体进行了比较。除了全连接层的第三层外，所有参数都使用了预训练权重。该层被随机初始化。AgroAVNET 模型以
    98.21% 的分类准确率超越了其他模型。然而，[[43](#bib.bibx43)] 在他们的研究中采用了特征连接方法。他们结合了基于超像素的 LBP（SPLBP）方法来提取局部纹理特征，CNN
    用于学习空间特征，SVM 用于分类。他们将提出的 FCN-SPLBP 模型与 CNN、LBP、FCN 和 SPLBP 架构进行了比较。
- en: '[[148](#bib.bibx148)] proposed OverFeat-GoogLeNet architecture by combining
    the features from LSTM and GoogLeNet model. The model was used to develop a “Parallelised
    Weed Detection System” by [[160](#bib.bibx160)]. They claimed that this system
    was robust, scalable and could be applied for real-time weed detection. The classification
    accuracy of the system was 91.1%.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '[[148](#bib.bibx148)] 通过结合 LSTM 和 GoogLeNet 模型的特征，提出了 OverFeat-GoogLeNet 架构。该模型被用来开发一个由
    [[160](#bib.bibx160)] 提出的“并行化杂草检测系统”。他们声称该系统具有鲁棒性、可扩展性，并可应用于实时杂草检测。系统的分类准确率为 91.1%。'
- en: '[[82](#bib.bibx82)] fine-tuned AlexNet, VGG-F, VGG-16, Inception-v1, ResNet-50,
    and ResNet-101 model to extract features from the images. They replaced the CNNs’
    default classifiers with linear classifiers, i.e., SVM and logistic regression.
    They compared the performance of various SVM and logistic regression classifiers
    by combining them with CNN models for detecting weeds. They achieved the most
    balanced result in terms of accuracy and false positive rate by using “L2-regularised
    with L2-loss logistic regression model using primal computation” classifier. This
    classifier performed better while being used with GoogLeNet architecture for detecting
    weeds in grasslands [[81](#bib.bibx81)].'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[[82](#bib.bibx82)] 对 AlexNet、VGG-F、VGG-16、Inception-v1、ResNet-50 和 ResNet-101
    模型进行了微调，以从图像中提取特征。他们用线性分类器（即 SVM 和逻辑回归）替换了 CNN 的默认分类器。他们通过将各种 SVM 和逻辑回归分类器与 CNN
    模型结合，比较了它们在检测杂草方面的性能。他们通过使用“带 L2 正则化和 L2 损失的逻辑回归模型（使用原始计算）”分类器，在准确性和假阳性率方面取得了最平衡的结果。这个分类器在与
    GoogLeNet 架构一起用于检测草地中的杂草时表现更佳 [[81](#bib.bibx81)]。'
- en: '[[40](#bib.bibx40)] also replaced CNN’s default classifier with traditional
    ML classifiers including SVM, XGBoost, and Logistic Regression. They initialised
    Xception, Inception-ResNet, VGGNets, MobileNet, and DenseNet model with pre-trained
    weights. The experimental result showed that the best performing network was DenseNet
    model with the SVM classifier. The micro F1 score for the architecture was 99.29%.
    This research also reported that with a small dataset, network performance could
    be enhanced using this approach.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[[40](#bib.bibx40)] 还用传统的机器学习分类器（包括SVM、XGBoost 和逻辑回归）替换了 CNN 的默认分类器。他们用预训练的权重初始化了
    Xception、Inception-ResNet、VGGNets、MobileNet 和 DenseNet 模型。实验结果显示，性能最好的网络是结合 SVM
    分类器的 DenseNet 模型。该架构的微平均 F1 分数为 99.29%。这项研究还报告了在小数据集下，使用这种方法可以提高网络性能。'
- en: '[[2](#bib.bibx2)] proposed a fully convolutional encoder-decoder network named
    as Enhanced Skip Network. The model had multiple VGGNet-like blocks in the encoder
    and decoder. However, the decoder part had fewer future maps to reduce the computational
    complexity and memory requirement. Besides, the skip layers, larger convolutional
    kernels and a multi-scale filter bank were incorporated in the proposed model.
    The weights were initialised using the transfer learning method. The model performed
    better than U-Net, FCN8, and DeepLab-v3, Faster R-CNN, and EDNet in identifying
    weeds in the paddy field. [[29](#bib.bibx29)] combined U-Net architecture, MobileNet-v2
    and DenseNet architectures and replaced transposed convolution layers with activation
    map scaling.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2](#bib.bibx2)] 提出了一个名为增强跳跃网络（Enhanced Skip Network）的全卷积编码器-解码器网络。该模型在编码器和解码器中具有多个类似
    VGGNet 的块。然而，为了降低计算复杂度和内存需求，解码器部分的特征图较少。此外，模型中还加入了跳跃层、更大的卷积核和多尺度滤波器组。权重使用迁移学习方法进行了初始化。该模型在识别稻田中的杂草方面表现优于
    U-Net、FCN8、DeepLab-v3、Faster R-CNN 和 EDNet。[[29](#bib.bibx29)] 结合了 U-Net 架构、MobileNet-v2
    和 DenseNet 架构，并用激活图缩放替换了转置卷积层。'
- en: 11 Performance Evaluation Metrics
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11 性能评估指标
- en: In general, evaluation metrics is the measurement tool to quantify the performance
    of a classifier. Different metrics are used to evaluate various characteristics
    of a classifier [[53](#bib.bibx53)]. The evaluation metrics can be used either
    to measure the quality of a classification model [[53](#bib.bibx53)] or to compare
    the performance of the different trained models for selecting the best one [[115](#bib.bibx115)].
    Various metrics were used in related studies based on the research need. The most
    commonly used metric is classification accuracy (CA) to evaluate the DL model.
    Many of the authors used multiple metrics to assess the model before drawing any
    conclusion. Table [6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A
    Survey of Deep Learning Techniques for Weed Detection from Images") lists the
    evaluation metrics applied in the relevant studies.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，评估指标是量化分类器性能的测量工具。不同的指标用于评估分类器的各种特性[[53](#bib.bibx53)]。评估指标可以用来测量分类模型的质量[[53](#bib.bibx53)]，也可以用来比较不同训练模型的性能，以选择最佳模型[[115](#bib.bibx115)]。相关研究中使用了各种指标，依据研究需要而定。最常用的指标是分类准确率
    (CA) 来评估深度学习模型。许多作者使用了多种指标来评估模型，然后才得出结论。表[6](#S11.T6 "Table 6 ‣ 11 Performance
    Evaluation Metrics ‣ A Survey of Deep Learning Techniques for Weed Detection from
    Images")列出了相关研究中应用的评估指标。
- en: As Table [6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A Survey
    of Deep Learning Techniques for Weed Detection from Images") shows, it is not
    easy to compare the related works as different types of evaluation metrics are
    employed depending on the DL model, the goal of classification, dataset and detection
    approach. However, the most frequently used evaluation metrics are CA, F1 score
    and mIoU. In the case of classifying plant species, researchers prefer to use
    confusion metrics to evaluate the model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")所示，由于不同的深度学习模型、分类目标、数据集和检测方法使用了不同类型的评估指标，比较相关工作并不容易。然而，最常用的评估指标是CA、F1分数和mIoU。在植物物种分类的情况下，研究人员更倾向于使用混淆矩阵来评估模型。
- en: 'Table 6: The evaluation metrics applied by different researchers of the related
    works'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：不同研究者在相关工作中应用的评估指标
- en: '| No. | Performance Metric | Meaning |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 编号 | 性能指标 | 含义 |'
- en: '| 1. | Classification Accuracy (CA) | The percentage of correct prediction
    among the input. A model is judged based on how high the value is |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 1. | 分类准确率 (CA) | 输入中正确预测的百分比。一个模型的评判依据是这一值的高低。 |'
- en: '| 2. | True Positive (TP) | How many times the model correctly predict the
    actual classes of the object. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 2. | 真正例 (TP) | 模型正确预测实际类别的次数。'
- en: '| 3. | False Positive Rate (FPR) | It is the proportion of negative cases incorrectly
    identified as positive cases in the data. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 3. | 假阳性率 (FPR) | 在数据中，被错误识别为正例的负例比例。 |'
- en: '| 4. | False Negative Rate (FNR) | The ratio of positive samples that were
    incorrectly classified. |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 4. | 假负例率 (FNR) | 错误分类的正样本比例。 |'
- en: '| 5. | Specificity (S) | The fraction of True Negative from the sum of False
    Positive and True Negative. |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 5. | 特异性 (S) | 从假阳性和真正例之和中计算真正例的比例。 |'
- en: '| 6. | Mean Pixel Accuracy (MPA) | It is the average of ration of the correctly
    classified pixels among all pixels of the images in the dataset. It is used to
    evaluate the model for semantic segmentation. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 6. | 平均像素准确率 (MPA) | 图像数据集中所有像素的正确分类像素比例的平均值。用于评估模型在语义分割中的表现。 |'
- en: '| 7. | Precision (P) | The fraction of correct prediction (True Positive) from
    the total number of relevant result (Sum of True Positive and False Positive).
    It helps when the value of False Positives are high. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 7. | 精确率 (P) | 从相关结果的总数（真正例和假阳例之和）中计算正确预测（真正例）的比例。当假阳例的值很高时，这一指标很有帮助。 |'
- en: '| 8. | Mean Average Precision (mAP) | It is the mean of average precision over
    all the classes of an object in the data. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 8. | 平均平均精度 (mAP) | 数据中所有类别的平均精度的均值。 |'
- en: '| 9. | Recall | The fraction of True Positive from the sum of True Positive
    and False Negative. It helps when the value of False Negatives are high. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 9. | 召回率 | 从真正例和假负例之和中计算真正例的比例。当假负例的值很高时，这一指标很有帮助。 |'
- en: '| 10. | F1 Score (F1) | The harmonic mean of precision and recall. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 10. | F1分数 (F1) | 精确率和召回率的调和平均数。 |'
- en: '| 11. | Confusion Matrix (CM) | It is the summary of the number of correct
    and incorrect prediction made by a model. It helps to visualise not only the errors
    made by the model but also the types of error in predicting the class of object.
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 11. | 混淆矩阵（CM） | 这是模型所做的正确和错误预测的总结。它有助于可视化模型不仅的错误，还包括预测对象类别的错误类型。 |'
- en: '| 12. | Intersection over Union (IoU) | It is the ratio of the overlapping
    area of ground truth (the hand labelled bounding boxes from the testing dataset)
    and predicted area (predicted bounding boxes from the model) to the total area.
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 12. | 交并比（IoU） | 这是地面真实（测试数据集中手动标注的边界框）和预测区域（模型预测的边界框）重叠区域与总区域的比率。 |'
- en: '| 13. | Mean Intersection over Union (mIoU) | It is average IoU over all the
    classes of an object in the dataset. |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 13. | 平均交并比（mIoU） | 这是数据集中所有对象类别的平均IoU。 |'
- en: '| 14. | Frequency Weighted Intersection over Union (FWIoU) | It is the weighted
    average of IoUs based on pixel classes. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 14. | 频率加权交并比（FWIoU） | 这是基于像素类别的IoU的加权平均。 |'
- en: '| 15. | Mean Square Error (MSE) | It is the mean of all the squared errors
    between the predicted and actual target class |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 15. | 均方误差（MSE） | 这是预测值与实际目标类别之间所有平方误差的平均值 |'
- en: '| 16. | Root Mean Square Error (RMSE) | It is the standard deviation of the
    difference between the predicted value and observed values. |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 16. | 均方根误差（RMSE） | 这是预测值和观察值之间差异的标准差。 |'
- en: '| 17. | Mean Absolute Error (MAE) | It is the mean of the absolute values of
    each prediction error on all instances of the test dataset. |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 17. | 平均绝对误差（MAE） | 这是测试数据集中所有实例的每个预测误差的绝对值的平均值。 |'
- en: '| 18. | R2 | It is the squared correlation between the observed and the predicted
    outcome by the model. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 18. | R2 | 这是观察值和模型预测结果之间的平方相关性。 |'
- en: '| 19. | K-fold Cross Validation | The dataset is divided into K number of parts
    and each of the parts is used as testing dataset. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 19. | K折交叉验证 | 数据集被分成K个部分，每个部分都用作测试数据集。 |'
- en: '| 20. | Receiver Operating Characteristic (ROC) curve | The true positive rate
    is plotted in function of the false positive rate for different cut-off points
    of a parameter. |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 20. | 接收者操作特征（ROC）曲线 | 绘制不同参数的截断点下的真正率与假正率的函数关系。 |'
- en: '| 21. | Kappa Coefficient | Measures the degree of agreement between the true
    values and the predicted values |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 21. | Kappa系数 | 衡量真实值和预测值之间一致性的程度 |'
- en: '| 22. | Matthews correlation coefficient (MCC) | A correlation coefficient
    between the observed and predicted binary classifications |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 22. | Matthews相关系数（MCC） | 观察到的和预测的二分类结果之间的相关系数 |'
- en: '| 23. | Dice Similarity Coefficient (DSC) | It is a measure of spatial overlap
    between two sets of pixels. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 23. | Dice相似度系数（DSC） | 这是两个像素集合之间空间重叠的度量。 |'
- en: In addition to the evaluation metrics provided in Table [6](#S11.T6 "Table 6
    ‣ 11 Performance Evaluation Metrics ‣ A Survey of Deep Learning Techniques for
    Weed Detection from Images"), [[108](#bib.bibx108)] justified their model based
    on run-time. This was because, to develop a real-time weeds and crop plants classifier,
    it is important to identify the class of a plant as quickly as possible. They
    showed how quickly their model could detect a plant in an image. Similarly, [[151](#bib.bibx151)]
    calculated the classification accuracy of their model along with the time required
    to train and identify classes of plants, as they intended to develop a real-time
    classifier. [[104](#bib.bibx104)] also used run-time for justifying the model
    performance. They found that, by increasing the patch size of the input images,
    it was possible to reduce the time required to train the model. Another research
    method used inference time to compare different DL architecture [[63](#bib.bibx63)].
    [[34](#bib.bibx34)] evaluated the CNN model not only based on time but also in
    terms of the memory consumed by the model during training. They argued that though
    the CNN architecture achieved higher accuracy than other machine learning model,
    it required more time and memory to train the model. [[7](#bib.bibx7)] showed
    that reducing the number of layers of the DL model could make it faster in detecting
    and identifying the crop and weed plants. They also used processing time as an
    evaluation criterion while choosing the CNN architecture.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表[6](#S11.T6 "Table 6 ‣ 11 Performance Evaluation Metrics ‣ A Survey of Deep
    Learning Techniques for Weed Detection from Images")中提供的评估指标外，[[108](#bib.bibx108)]
    还基于运行时间对其模型进行了验证。这是因为，为了开发实时杂草和作物植物分类器，快速识别植物类别至关重要。他们展示了他们的模型在图像中检测植物的速度。同样，[[151](#bib.bibx151)]
    计算了其模型的分类准确率以及训练和识别植物类别所需的时间，因为他们打算开发一个实时分类器。[[104](#bib.bibx104)] 也使用运行时间来证明模型的性能。他们发现，通过增加输入图像的补丁大小，可以减少训练模型所需的时间。另一种研究方法使用推断时间来比较不同的深度学习架构[[63](#bib.bibx63)]。[[34](#bib.bibx34)]
    在评估CNN模型时不仅基于时间，还考虑了模型在训练过程中消耗的内存。他们认为，尽管CNN架构在准确性方面优于其他机器学习模型，但训练模型所需的时间和内存更多。[[7](#bib.bibx7)]
    显示，通过减少深度学习模型的层数，可以加快检测和识别作物及杂草植物的速度。他们在选择CNN架构时也将处理时间作为评估标准。
- en: 12 Discussion
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12 讨论
- en: It is evident that the DL model offers high performance in the area of weed
    detection and classification in crops. In this paper, we have provided an overview
    of the current status of the area of the automatic weed detection technique. In
    most relevant studies, the preferred method to acquire data was using a digital
    camera mounted on a ground vehicle to collect RGB images. A few research studies
    collected multi-spectral or hyper-spectral data. To prepare the dataset for training,
    different image processing techniques were used to resize the images, background
    and noise removing and image enhancement. The datasets were generally annotated
    using bounding boxes, pixel-wise and image level annotation approaches. For training
    the model, supervised learning approaches are applied by the researchers. They
    employ different DL techniques to find a better weed detection model. Detection
    accuracy is given as the most important parameter to evaluate the performance
    of the model.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，深度学习（DL）模型在作物中的杂草检测和分类方面表现出色。在本文中，我们概述了自动杂草检测技术领域的当前状态。在大多数相关研究中，获取数据的首选方法是使用安装在地面车辆上的数字相机来收集RGB图像。一些研究收集了多光谱或高光谱数据。为了准备训练数据集，使用了不同的图像处理技术，包括图像大小调整、背景和噪声去除以及图像增强。数据集通常采用边界框、像素级和图像级标注方法进行注释。为了训练模型，研究人员采用了监督学习方法。他们使用不同的深度学习技术来寻找更好的杂草检测模型。检测准确率被认为是评估模型性能的最重要参数。
- en: Nevertheless, there is still room for improvements in this area. Use of emerging
    technologies can help to improve the accuracy and speed of automatic weed detection
    systems. As crop and weed plants have many similarities, the use of other spectral
    indices can improve the performance.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，该领域仍有改进的空间。采用新兴技术可以帮助提高自动杂草检测系统的准确性和速度。由于作物和杂草植物有许多相似之处，使用其他光谱指数可以提高性能。
- en: However, there is a lack of large datasets for crops and weeds. It is necessary
    to construct a large benchmark dataset by capturing a variety of crops/weeds from
    different geographical locations, weather conditions and at various growth stages
    of crops and weeds. At the same time, it will be expensive to annotate these large
    datasets. Semi-supervised [[25](#bib.bibx25), [174](#bib.bibx174)] or weakly supervised
    [[176](#bib.bibx176), [36](#bib.bibx36)] approaches could be employed to address
    this problem.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前缺乏大规模的作物和杂草数据集。需要通过捕捉来自不同地理位置、天气条件以及作物和杂草不同生长阶段的各种作物/杂草来构建一个大型基准数据集。同时，标注这些大型数据集将是昂贵的。可以采用半监督[[25](#bib.bibx25),
    [174](#bib.bibx174)]或弱监督[[176](#bib.bibx176), [36](#bib.bibx36)]方法来解决这个问题。
- en: Moreover, Generative Adversarial Network (GAN) [[93](#bib.bibx93)] or other
    synthetic data generation techniques can contribute to creating a large dataset.
    Random point generation and polygon labelling can further improve the precision
    of automatic weed detection systems. DL is evolving very fast, and new state-of-art
    techniques are being proposed. In addition to developing new solutions, researchers
    can enhance and apply those methods in the area of weed detection. They can also
    consider using weakly supervised, self-supervised or unsupervised approaches like
    multiple instance learning, few-shot or zero-shot learning as a means for synthetic
    data generation.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，生成对抗网络（GAN）[[93](#bib.bibx93)]或其他合成数据生成技术可以帮助创建大型数据集。随机点生成和多边形标注可以进一步提高自动除草系统的精度。深度学习（DL）发展迅速，新的最先进技术不断提出。除了开发新解决方案外，研究人员还可以在除草检测领域增强和应用这些方法。他们还可以考虑使用弱监督、自监督或无监督的方法，如多实例学习、少样本或零样本学习作为合成数据生成的手段。
- en: Furthermore, most datasets mentioned in this paper exhibit class imbalance,
    which may create biases and lead to over-fitting of the model. Future research
    needs to address the problem. This can be achieved via the use of appropriate
    data redistribution approaches, cost-sensitive learning approaches [[76](#bib.bibx76)],
    or class balancing classifiers [[153](#bib.bibx153), [15](#bib.bibx15)].
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本文提到的大多数数据集存在类别不平衡的问题，这可能会导致偏差并导致模型过拟合。未来的研究需要解决这个问题。可以通过使用适当的数据重分配方法、成本敏感学习方法[[76](#bib.bibx76)]或类别平衡分类器[[153](#bib.bibx153),
    [15](#bib.bibx15)]来实现。
- en: To summarise, the primary objective of developing automatic weed detection system
    is to provide a weed management technique that will minimise cost and maximise
    crop yields. To do so, researchers need to come up with a system that can be deployed
    on devices with a lower computational requirement and can detect weeds accurately
    in real-time.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，开发自动除草系统的主要目标是提供一种能最小化成本和最大化作物产量的除草管理技术。为此，研究人员需要提出一个可以在计算需求较低的设备上部署并且能够实时准确检测杂草的系统。
- en: 13 Conclusion
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13 结论
- en: This study provides a comprehensive survey of the deep learning-based research
    in detecting and classifying weed species in value crops. A total of 70 relevant
    papers have been examined based on data acquisition, dataset preparation, detection
    and classification methods and model evaluation process. Publicly available datasets
    in the related field are also highlighted for prospective researchers. In this
    article, we provide a taxonomy of the research studies in this area and summarise
    the approaches of detecting weeds (Table LABEL:tab:different_DL_approach). It
    was found that most of the studies applied supervised learning techniques using
    state-of-art deep learning models and they can achieve better performance and
    classification accuracy by fine-tuning pre-trained models on any plant dataset.
    The results also show that the experiments already have achieved very high accuracy
    when a sufficient amount of labelled data of each class is available for training
    the models. However, the existing research only achieved high accuracy in a limited
    experiment setup, e.g., on small datasets of a select number of crops and weeds
    species. Computational speed in the recognition process is another limiting factor
    for deployment on real-time fast-moving herbicide spraying vehicles. An important
    future direction would be to investigate highly efficient detection techniques
    using very large datasets with a variety of crop and weed species so that one
    single model can be used across any weed-crop setting as needed. Other potential
    future research directions include the need for large generalised datasets, tailored
    machine learning models in weed-crop settings, addressing the class imbalance
    problems, identifying the growth stage of the weeds, as well as thorough field
    trials for commercial deployments.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提供了基于深度学习的杂草物种检测与分类的全面综述。共审查了70篇相关论文，涵盖了数据采集、数据集准备、检测与分类方法以及模型评估过程。文中还突出了相关领域的公共数据集，以供未来研究者参考。我们提供了该领域研究的分类，并总结了杂草检测的方法（见表
    LABEL:tab:different_DL_approach）。研究发现，大多数研究使用了最先进的深度学习模型进行监督学习，通过对预训练模型进行微调，可以在任何植物数据集上获得更好的性能和分类准确度。结果还显示，当每个类别有足够的标记数据用于训练模型时，实验已经达到了非常高的准确度。然而，现有研究仅在有限的实验设置中取得了高准确度，例如在少量作物和杂草物种的小数据集上。识别过程中的计算速度是实时快速移动除草剂喷洒车辆部署的另一个限制因素。一个重要的未来方向是研究使用非常大数据集的高效检测技术，以便在任何杂草-作物设置中使用单一模型。其他潜在的未来研究方向包括需要大规模通用数据集、针对杂草-作物设置的量身定制的机器学习模型、解决类别不平衡问题、识别杂草的生长阶段以及进行商业化部署的全面实地试验。
- en: References
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Alwaseela Abdalla et al. “Fine-tuning convolutional neural network with
    transfer learning for semantic segmentation of ground-level oilseed rape images
    in a field with high weed pressure” In *Computers and Electronics in Agriculture*
    167 Elsevier, 2019, pp. 105091'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Alwaseela Abdalla 等 “利用迁移学习微调卷积神经网络，对高杂草压力田间的地面油菜图像进行语义分割” *发表在《农业计算机与电子学》*
    167 Elsevier, 2019, 页码 105091'
- en: '[2] Shyam Prasad Adhikari, Heechan Yang and Hyongsuk Kim “Learning semantic
    graphics using convolutional encoder-decoder network for autonomous weeding in
    paddy field” In *Frontiers in plant science* 10 Frontiers, 2019, pp. 1404'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Shyam Prasad Adhikari, Heechan Yang 和 Hyongsuk Kim “利用卷积编码器-解码器网络学习语义图形，用于稻田中的自主除草”
    *发表在《植物科学前沿》* 10 Frontiers, 2019, 页码 1404'
- en: '[3] Jamil Ahmad et al. “Visual features based boosted classification of weeds
    for real-time selective herbicide sprayer systems” In *Computers in Industry*
    98 Elsevier, 2018, pp. 23–33'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Jamil Ahmad 等 “基于视觉特征的增强分类方法用于实时选择性除草剂喷洒系统” *发表在《工业计算机》* 98 Elsevier, 2018,
    页码 23–33'
- en: '[4] Mansoor Alam et al. “Real-Time Machine-Learning Based Crop/Weed Detection
    and Classification for Variable-Rate Spraying in Precision Agriculture” In *2020
    7th International Conference on Electrical and Electronics Engineering (ICEEE)*,
    2020, pp. 273–280 IEEE'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Mansoor Alam 等 “基于实时机器学习的作物/杂草检测与分类，用于精确农业中的变速喷洒” *发表在《2020年第七届电气与电子工程国际会议（ICEEE）》*
    2020, 页码 273–280 IEEE'
- en: '[5] Md Zahangir Alom et al. “A state-of-the-art survey on deep learning theory
    and architectures” In *Electronics* 8.3 Multidisciplinary Digital Publishing Institute,
    2019, pp. 292'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Md Zahangir Alom 等 “深度学习理论和架构的最先进综述” *发表在《电子学》* 8.3 Multidisciplinary Digital
    Publishing Institute, 2019, 页码 292'
- en: '[6] Sandra Amend et al. “Weed Management of the Future” In *KI-Künstliche Intelligenz*
    33.4 Springer, 2019, pp. 411–415'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Sandra Amend 等. “未来的杂草管理” 在 *KI-人工智能* 33.4 Springer，2019年，第411–415页'
- en: '[7] Córdova-Cruzatty Andrea, Barreno Barreno Mauricio Daniel and Jácome Barrionuevo
    José Misael “Precise weed and maize classification through convolutional neuronal
    networks” In *2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)*, 2017,
    pp. 1–6 IEEE'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Córdova-Cruzatty Andrea, Barreno Barreno Mauricio Daniel 和 Jácome Barrionuevo
    José Misael “通过卷积神经网络精确分类杂草和玉米” 在 *2017年IEEE第二届厄瓜多技术分会会议（ETCM）*，2017年，第1–6页 IEEE'
- en: '[8] Paolo Andreini et al. “Image generation by gan and style transfer for agar
    plate image segmentation” In *Computer Methods and Programs in Biomedicine* 184
    Elsevier, 2020, pp. 105268'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Paolo Andreini 等. “通过 GAN 和风格迁移生成图像以进行琼脂平板图像分割” 在 *计算机方法与生物医学程序* 184 Elsevier，2020年，第105268页'
- en: '[9] Muhammad Hamza Asad and Abdul Bais “Weed detection in canola fields using
    maximum likelihood classification and deep convolutional neural network” In *Information
    Processing in Agriculture*, 2019 DOI: [10.1016/j.inpa.2019.12.002](https://dx.doi.org/10.1016/j.inpa.2019.12.002)'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Muhammad Hamza Asad 和 Abdul Bais “使用最大似然分类和深度卷积神经网络检测油菜田中的杂草” 在 *农业信息处理*，2019年
    DOI: [10.1016/j.inpa.2019.12.002](https://dx.doi.org/10.1016/j.inpa.2019.12.002)'
- en: '[10] Vijay Badrinarayanan, Alex Kendall and Roberto Cipolla “Segnet: A deep
    convolutional encoder-decoder architecture for image segmentation” In *IEEE transactions
    on pattern analysis and machine intelligence* 39.12 IEEE, 2017, pp. 2481–2495'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Vijay Badrinarayanan, Alex Kendall 和 Roberto Cipolla “Segnet：一种深度卷积编码器-解码器架构用于图像分割”
    在 *IEEE模式分析与机器智能学报* 39.12 IEEE，2017年，第2481–2495页'
- en: '[11] M Dian Bah, Adel Hafiane and Raphael Canals “Deep learning with unsupervised
    data labeling for weed detection in line crops in UAV images” In *Remote sensing*
    10.11 Multidisciplinary Digital Publishing Institute, 2018, pp. 1690'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M Dian Bah, Adel Hafiane 和 Raphael Canals “使用无监督数据标注的深度学习进行 UAV 图像中的行作物杂草检测”
    在 *遥感* 10.11 多学科数字出版学院，2018年，第1690页'
- en: '[12] Adel Bakhshipour and Abdolabbas Jafari “Evaluation of support vector machine
    and artificial neural networks in weed detection using shape features” In *Computers
    and Electronics in Agriculture* 145 Elsevier, 2018, pp. 153–160'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Adel Bakhshipour 和 Abdolabbas Jafari “使用形状特征在杂草检测中对支持向量机和人工神经网络的评估” 在
    *农业计算机与电子学* 145 Elsevier，2018年，第153–160页'
- en: '[13] Adel Bakhshipour, Abdolabbas Jafari, Seyed Mehdi Nassiri and Dariush Zare
    “Weed segmentation using texture features extracted from wavelet sub-images” In
    *Biosystems Engineering* 157 Elsevier, 2017, pp. 1–12'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Adel Bakhshipour, Abdolabbas Jafari, Seyed Mehdi Nassiri 和 Dariush Zare
    “使用从小波子图像提取的纹理特征进行杂草分割” 在 *生物系统工程* 157 Elsevier，2017年，第1–12页'
- en: '[14] Horace B Barlow “Unsupervised learning” In *Neural computation* 1.3 MIT
    Press, 1989, pp. 295–311'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Horace B Barlow “无监督学习” 在 *神经计算* 1.3 MIT出版社，1989年，第295–311页'
- en: '[15] Jingjun Bi and Chongsheng Zhang “An empirical comparison on state-of-the-art
    multi-class imbalance learning algorithms and a new diversified ensemble learning
    scheme” In *Knowledge-Based Systems* 158 Elsevier, 2018, pp. 81–93'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jingjun Bi 和 Chongsheng Zhang “对最先进的多类不平衡学习算法及一种新的多样化集成学习方案的实证比较” 在 *知识基础系统*
    158 Elsevier，2018年，第81–93页'
- en: '[16] Abdel-Aziz Binguitcha-Fare and Prince Sharma “Crops and weeds classification
    using Convolutional Neural Networks via optimization of transfer learning parameters”
    In *International Journal of Engineering and Advanced Technology (IJEAT)* 8.5
    Blue Eyes Intelligence Engineering & Sciences Publication, 2019, pp. 2284–2294'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Abdel-Aziz Binguitcha-Fare 和 Prince Sharma “利用卷积神经网络通过优化迁移学习参数对作物和杂草进行分类”
    在 *国际工程与先进技术期刊（IJEAT）* 8.5 Blue Eyes Intelligence Engineering & Sciences出版，2019年，第2284–2294页'
- en: '[17] D Bini, D Pamela and Shajin Prince “Machine Vision and Machine Learning
    for Intelligent Agrobots: A review” In *2020 5th International Conference on Devices,
    Circuits and Systems (ICDCS)*, 2020, pp. 12–16 IEEE'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] D Bini, D Pamela 和 Shajin Prince “智能农业机器人中的机器视觉和机器学习：综述” 在 *2020年第五届国际设备、电路和系统会议（ICDCS）*，2020年，第12–16页
    IEEE'
- en: '[18] Petra Bosilj, Erchan Aptoula, Tom Duckett and Grzegorz Cielniak “Transfer
    learning between crop types for semantic segmentation of crops versus weeds in
    precision agriculture” In *Journal of Field Robotics* 37.1 Wiley Online Library,
    2020, pp. 7–19'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Petra Bosilj, Erchan Aptoula, Tom Duckett 和 Grzegorz Cielniak “在精准农业中进行作物与杂草的语义分割的作物类型迁移学习”
    在 *田间机器人学报* 37.1 Wiley在线图书馆，2020年，第7–19页'
- en: '[19] E. Brasseur In *ericbrasseur.org* URL: [http://www.ericbrasseur.org/gamma.html?i=1](http://www.ericbrasseur.org/gamma.html?i=1)'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] E. Brasseur 见于*ericbrasseur.org* 网址: [http://www.ericbrasseur.org/gamma.html?i=1](http://www.ericbrasseur.org/gamma.html?i=1)'
- en: '[20] Anderson Brilhador et al. “Classification of Weeds and Crops at the Pixel-Level
    Using Convolutional Neural Networks and Data Augmentation” In *2019 IEEE Latin
    American Conference on Computational Intelligence (LA-CCI)*, 2019, pp. 1–6 IEEE'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Anderson Brilhador 等 “使用卷积神经网络和数据增强的像素级杂草和作物分类” 见于*2019 IEEE拉丁美洲计算智能会议
    (LA-CCI)*, 2019, 页 1–6 IEEE'
- en: '[21] Ralph B Brown and Scott D Noble “Site-specific weed management: sensing
    requirements—what do we need to see?” In *Weed Science* 53.2 BioOne, 2005, pp.
    252–258'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Ralph B Brown 和 Scott D Noble “场地特定杂草管理：感测需求—我们需要看到什么？” 见于*杂草科学* 53.2
    BioOne, 2005, 页 252–258'
- en: '[22] Rich Caruana and Alexandru Niculescu-Mizil “An empirical comparison of
    supervised learning algorithms” In *Proceedings of the 23rd international conference
    on Machine learning*, 2006, pp. 161–168'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Rich Caruana 和 Alexandru Niculescu-Mizil “有监督学习算法的经验比较” 见于*第23届国际机器学习大会论文集*,
    2006, 页 161–168'
- en: '[23] Paulo César Pereira Júnior et al. “Comparison of Supervised Classifiers
    and Image Features for Crop Rows Segmentation on Aerial Images” In *Applied Artificial
    Intelligence* 34.4 Taylor & Francis, 2020, pp. 271–291'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Paulo César Pereira Júnior 等 “用于空中图像上作物行分割的监督分类器和图像特征比较” 见于*应用人工智能* 34.4
    Taylor & Francis, 2020, 页 271–291'
- en: '[24] N Zhang C Chaisattapagon “Effective criteria for weed identification in
    wheat fields using machine vision” In *Transactions of the ASAE* 38.3 American
    Society of AgriculturalBiological Engineers, 1995, pp. 965–974'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] N Zhang C Chaisattapagon “基于机器视觉的小麦田杂草识别有效标准” 见于*ASAE学报* 38.3 美国农业生物工程师学会,
    1995, 页 965–974'
- en: '[25] Olivier Chapelle, Bernhard Scholkopf and Alexander Zien “Semi-supervised
    learning (chapelle, o. et al., eds.; 2006)[book reviews]” In *IEEE Transactions
    on Neural Networks* 20.3 IEEE, 2009, pp. 542–542'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Olivier Chapelle, Bernhard Scholkopf 和 Alexander Zien “半监督学习（chapelle,
    o. 等，编著；2006)[书评]” 见于*IEEE神经网络汇刊* 20.3 IEEE, 2009, 页 542–542'
- en: '[26] Trupti R Chavan and Abhijeet V Nandedkar “AgroAVNET for crops and weeds
    classification: A step forward in automatic farming” In *Computers and Electronics
    in Agriculture* 154 Elsevier, 2018, pp. 361–372'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Trupti R Chavan 和 Abhijeet V Nandedkar “AgroAVNET 用于作物和杂草分类：自动化农业的一步前进”
    见于*农业计算机与电子学* 154 Elsevier, 2018, 页 361–372'
- en: '[27] Nived Chebrolu, Thomas Läbe and Cyrill Stachniss “Robust long-term registration
    of UAV images of crop fields for precision agriculture” In *IEEE Robotics and
    Automation Letters* 3.4 IEEE, 2018, pp. 3097–3104'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Nived Chebrolu, Thomas Läbe 和 Cyrill Stachniss “UAV 图像的长期稳健配准用于精准农业” 见于*IEEE机器人与自动化快报*
    3.4 IEEE, 2018, 页 3097–3104'
- en: '[28] Nived Chebrolu et al. “Agricultural robot dataset for plant classification,
    localization and mapping on sugar beet fields” In *The International Journal of
    Robotics Research* 36.10 SAGE Publications Sage UK: London, England, 2017, pp.
    1045–1052'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Nived Chebrolu 等 “用于植物分类、定位和糖用甜菜田地映射的农业机器人数据集” 见于*国际机器人研究杂志* 36.10 SAGE出版公司
    Sage UK: 伦敦, 英国, 2017, 页 1045–1052'
- en: '[29] Lukasz Chechlinski, Barbara Siemikatkowska and Michal Majewski “A System
    for Weeds and Crops Identification—Reaching over 10 FPS on Raspberry Pi with the
    Usage of MobileNets, DenseNet and Custom Modifications” In *Sensors* 19.17 Multidisciplinary
    Digital Publishing Institute, 2019, pp. 3787'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Lukasz Chechlinski, Barbara Siemikatkowska 和 Michal Majewski “一种杂草和作物识别系统—通过使用
    MobileNets, DenseNet 和自定义修改在 Raspberry Pi 上达到超过 10 FPS” 见于*传感器* 19.17 多学科数字出版学院,
    2019, 页 3787'
- en: '[30] Liang-Chieh Chen, George Papandreou, Florian Schroff and Hartwig Adam
    “Rethinking atrous convolution for semantic image segmentation” In *arXiv preprint
    arXiv:1706.05587*, 2017'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Liang-Chieh Chen, George Papandreou, Florian Schroff 和 Hartwig Adam “重新思考扩张卷积在语义图像分割中的应用”
    见于*arXiv 预印本 arXiv:1706.05587*, 2017'
- en: '[31] Vitali Czymmek, Leif O Harders, Florian J Knoll and Stephan Hussmann “Vision-Based
    Deep Learning Approach for Real-Time Detection of Weeds in Organic Farming” In
    *2019 IEEE International Instrumentation and Measurement Technology Conference
    (I2MTC)*, 2019, pp. 1–5 IEEE'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Vitali Czymmek, Leif O Harders, Florian J Knoll 和 Stephan Hussmann “基于视觉的深度学习方法用于有机农业中杂草的实时检测”
    见于*2019 IEEE国际仪器与测量技术会议 (I2MTC)*, 2019, 页 1–5 IEEE'
- en: '[32] Jia Deng et al. “Imagenet: A large-scale hierarchical image database”
    In *2009 IEEE conference on computer vision and pattern recognition*, 2009, pp.
    248–255 Ieee'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Jia Deng 等 “Imagenet：一个大规模的层次化图像数据库” 见于*2009 IEEE计算机视觉与模式识别会议*, 2009,
    页 248–255 Ieee'
- en: '[33] Maurilio Di Cicco, Ciro Potena, Giorgio Grisetti and Alberto Pretto “Automatic
    model based dataset generation for fast and accurate crop and weeds detection”
    In *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*,
    2017, pp. 5188–5195 IEEE'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Maurilio Di Cicco、Ciro Potena、Giorgio Grisetti 和 Alberto Pretto “基于模型的自动数据集生成用于快速准确的作物和杂草检测”
    见 *2017 IEEE/RSJ国际智能机器人与系统会议（IROS）*，2017年，第5188–5195页 IEEE'
- en: '[34] Alessandro Santos Ferreira et al. “Weed detection in soybean crops using
    ConvNets” In *Computers and Electronics in Agriculture* 143 Elsevier, 2017, pp.
    314–324'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Alessandro Santos Ferreira 等 “使用卷积网络在大豆作物中检测杂草” 见 *计算机与农业电子学* 143 Elsevier，2017年，第314–324页'
- en: '[35] Alessandro Santos Ferreira et al. “Unsupervised deep learning and semi-automatic
    data labeling in weed discrimination” In *Computers and Electronics in Agriculture*
    165 Elsevier, 2019, pp. 104963'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Alessandro Santos Ferreira 等 “无监督深度学习和半自动数据标注在杂草辨别中的应用” 见 *计算机与农业电子学*
    165 Elsevier，2019年，第104963页'
- en: '[36] Thibaut Durand, Taylor Mordan, Nicolas Thome and Matthieu Cord “Wildcat:
    Weakly supervised learning of deep convnets for image classification, pointwise
    localization and segmentation” In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 642–651'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Thibaut Durand、Taylor Mordan、Nicolas Thome 和 Matthieu Cord “Wildcat：弱监督学习深度卷积网络用于图像分类、逐点定位和分割”
    见 *IEEE计算机视觉与模式识别会议论文集*，2017年，第642–651页'
- en: '[37] Mads Dyrmann, Rasmus Nyholm Jørgensen and Henrik Skov Midtiby “RoboWeedSupport-Detection
    of weed locations in leaf occluded cereal crops using a fully convolutional neural
    network” In *Adv. Anim. Biosci* 8.2, 2017, pp. 842–847'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Mads Dyrmann、Rasmus Nyholm Jørgensen 和 Henrik Skov Midtiby “RoboWeedSupport-使用完全卷积神经网络检测叶片遮挡的谷物作物中的杂草位置”
    见 *Adv. Anim. Biosci* 8.2，2017年，第842–847页'
- en: '[38] Mads Dyrmann, Henrik Karstoft and Henrik Skov Midtiby “Plant species classification
    using deep convolutional neural network” In *Biosystems Engineering* 151 Elsevier,
    2016, pp. 72–80'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Mads Dyrmann、Henrik Karstoft 和 Henrik Skov Midtiby “使用深度卷积神经网络进行植物种类分类”
    见 *生物系统工程* 151 Elsevier，2016年，第72–80页'
- en: '[39] Max Ehrlich and Larry S Davis “Deep residual learning in the jpeg transform
    domain” In *Proceedings of the IEEE International Conference on Computer Vision*,
    2019, pp. 3484–3493'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Max Ehrlich 和 Larry S Davis “在 JPEG 变换域中的深度残差学习” 见 *IEEE国际计算机视觉会议论文集*，2019年，第3484–3493页'
- en: '[40] Borja Espejo-Garcia et al. “Towards weeds identification assistance through
    transfer learning” In *Computers and Electronics in Agriculture* 171 Elsevier,
    2020, pp. 105306'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Borja Espejo-Garcia 等 “通过迁移学习推动杂草识别辅助” 见 *计算机与农业电子学* 171 Elsevier，2020年，第105306页'
- en: '[41] Adnan Farooq, Jiankun Hu and Xiuping Jia “Analysis of spectral bands and
    spatial resolutions for weed classification via deep convolutional neural network”
    In *IEEE Geoscience and Remote Sensing Letters* 16.2 IEEE, 2018, pp. 183–187'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Adnan Farooq、Jiankun Hu 和 Xiuping Jia “通过深度卷积神经网络分析光谱带和空间分辨率以进行杂草分类” 见
    *IEEE地球科学与遥感快报* 16.2 IEEE，2018年，第183–187页'
- en: '[42] Adnan Farooq, Jiankun Hu and Xiuping Jia “Weed classification in hyperspectral
    remote sensing images via deep convolutional neural network” In *IGARSS 2018-2018
    IEEE International Geoscience and Remote Sensing Symposium*, 2018, pp. 3816–3819
    IEEE'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Adnan Farooq、Jiankun Hu 和 Xiuping Jia “通过深度卷积神经网络在高光谱遥感图像中进行杂草分类” 见 *IGARSS
    2018-2018 IEEE国际地球科学与遥感会议*，2018年，第3816–3819页 IEEE'
- en: '[43] Adnan Farooq, Xiuping Jia, Jiankun Hu and Jun Zhou “Multi-resolution weed
    classification via convolutional neural network and superpixel based local binary
    pattern using remote sensing images” In *Remote Sensing* 11.14 Multidisciplinary
    Digital Publishing Institute, 2019, pp. 1692'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Adnan Farooq、Xiuping Jia、Jiankun Hu 和 Jun Zhou “通过卷积神经网络和基于超像素的局部二值模式在遥感图像中进行多分辨率杂草分类”
    见 *遥感* 11.14 多学科数字出版机构，2019年，第1692页'
- en: '[44] Mulham Fawakherji et al. “Crop and weeds classification for precision
    agriculture using context-independent pixel-wise segmentation” In *2019 Third
    IEEE International Conference on Robotic Computing (IRC)*, 2019, pp. 146–152 IEEE'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Mulham Fawakherji 等 “利用无上下文的逐像素分割进行精确农业中的作物和杂草分类” 见 *2019第三届IEEE国际机器人计算会议（IRC）*，2019年，第146–152页
    IEEE'
- en: '[45] C Fernández-Quintanilla et al. “Is the current state of the art of weed
    monitoring suitable for site-specific weed management in arable crops?” In *Weed
    research* 58.4 Wiley Online Library, 2018, pp. 259–272'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C Fernández-Quintanilla 等 “当前的杂草监测技术是否适用于可作物的现场特定杂草管理？” 见 *杂草研究* 58.4
    Wiley Online Library，2018年，第259–272页'
- en: '[46] AJ Gabor, RR Leach and FU Dowla “Automated seizure detection using a self-organizing
    neural network” In *Electroencephalography and clinical Neurophysiology* 99.3
    Elsevier, 1996, pp. 257–266'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] AJ Gabor, RR Leach 和 FU Dowla “使用自组织神经网络的自动癫痫检测” 刊登于*Electroencephalography
    and clinical Neurophysiology* 99.3 Elsevier, 1996, 第257–266页'
- en: '[47] Junfeng Gao et al. “Deep convolutional neural networks for image-based
    Convolvulus sepium detection in sugar beet fields” In *Plant Methods* 16.1 BioMed
    Central, 2020, pp. 1–12'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Junfeng Gao 等 “基于深度卷积神经网络的甜菜田中卷曲朝鲜蓟的检测” 刊登于*Plant Methods* 16.1 BioMed
    Central, 2020, 第1–12页'
- en: '[48] Andreas Geiger, Philip Lenz, Christoph Stiller and Raquel Urtasun “Vision
    meets robotics: The kitti dataset” In *The International Journal of Robotics Research*
    32.11 Sage Publications Sage UK: London, England, 2013, pp. 1231–1237'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Andreas Geiger, Philip Lenz, Christoph Stiller 和 Raquel Urtasun “视觉与机器人技术的结合：Kitti
    数据集” 刊登于*The International Journal of Robotics Research* 32.11 Sage Publications
    Sage UK: 伦敦, 英国, 2013, 第1231–1237页'
- en: '[49] Thomas Mosgaard Giselsson et al. “A public image database for benchmark
    of plant seedling classification algorithms” In *arXiv preprint arXiv:1711.05458*,
    2017'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Thomas Mosgaard Giselsson 等 “用于植物幼苗分类算法基准测试的公共图像数据库” 刊登于*arXiv preprint
    arXiv:1711.05458*, 2017'
- en: '[50] Jiuxiang Gu et al. “Recent advances in convolutional neural networks”
    In *Pattern Recognition* 77 Elsevier, 2018, pp. 354–377'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Jiuxiang Gu 等 “卷积神经网络的最新进展” 刊登于*Pattern Recognition* 77 Elsevier, 2018,
    第354–377页'
- en: '[51] David Hall, Feras Dayoub, Tristan Perez and Chris Mccool “A rapidly deployable
    classification system using visual data for the application of precision weed
    management” In *Computers and Electronics in Agriculture* 148, 2018, pp. 107–120
    DOI: [10.1016/j.compag.2018.02.023](https://dx.doi.org/10.1016/j.compag.2018.02.023)'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] David Hall, Feras Dayoub, Tristan Perez 和 Chris Mccool “使用视觉数据的快速部署分类系统，用于精确杂草管理应用”
    刊登于*Computers and Electronics in Agriculture* 148, 2018, 第107–120页 DOI: [10.1016/j.compag.2018.02.023](https://dx.doi.org/10.1016/j.compag.2018.02.023)'
- en: '[52] Esmael Hamuda, Brian Mc Ginley, Martin Glavin and Edward Jones “Automatic
    crop detection under field conditions using the HSV colour space and morphological
    operations” In *Computers and electronics in agriculture* 133 Elsevier, 2017,
    pp. 97–107'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Esmael Hamuda, Brian Mc Ginley, Martin Glavin 和 Edward Jones “使用 HSV 色彩空间和形态学操作在田间条件下自动检测作物”
    刊登于*Computers and electronics in agriculture* 133 Elsevier, 2017, 第97–107页'
- en: '[53] David J Hand “Measuring classifier performance: a coherent alternative
    to the area under the ROC curve” In *Machine learning* 77.1 Springer, 2009, pp.
    103–123'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] David J Hand “测量分类器性能：ROC 曲线下面积的连贯替代方案” 刊登于*Machine learning* 77.1 Springer,
    2009, 第103–123页'
- en: '[54] Sebastian Haug and Jörn Ostermann “A crop/weed field image dataset for
    the evaluation of computer vision based precision agriculture tasks” In *European
    Conference on Computer Vision*, 2014, pp. 105–116 Springer'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Sebastian Haug 和 Jörn Ostermann “用于评估计算机视觉精确农业任务的作物/杂草田地图像数据集” 刊登于*European
    Conference on Computer Vision*, 2014, 第105–116页 Springer'
- en: '[55] Sebastian Haug and Jörn Ostermann “A Crop/Weed Field Image Dataset for
    the Evaluation of Computer Vision Based Precision Agriculture Tasks” In *Computer
    Vision - ECCV 2014 Workshops*, 2015, pp. 105–116 DOI: [10.1007/978-3-319-16220-1˙8](https://dx.doi.org/10.1007/978-3-319-16220-1_8)'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Sebastian Haug 和 Jörn Ostermann “用于评估计算机视觉精确农业任务的作物/杂草田地图像数据集” 刊登于*Computer
    Vision - ECCV 2014 Workshops*, 2015, 第105–116页 DOI: [10.1007/978-3-319-16220-1˙8](https://dx.doi.org/10.1007/978-3-319-16220-1_8)'
- en: '[56] Kaiming He, Xiangyu Zhang, Shaoqing Ren and Jian Sun “Deep residual learning
    for image recognition” In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2016, pp. 770–778'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun “用于图像识别的深度残差学习” 刊登于*Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2016, 第770–778页'
- en: '[57] J Hemming and T Rath “Image processing for plant determination using the
    Hough transform and clustering methods” In *Gartenbauwissenschaft* 67.1 Stuttgart:
    Eugen Ulmer GmbH & Co., 1928-2002., 2002, pp. 1–10'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J Hemming 和 T Rath “使用霍夫变换和聚类方法进行植物识别的图像处理” 刊登于*Gartenbauwissenschaft*
    67.1 斯图加特: Eugen Ulmer GmbH & Co., 1928-2002., 2002, 第1–10页'
- en: '[58] Geoffrey E Hinton, Simon Osindero and Yee-Whye Teh “A fast learning algorithm
    for deep belief nets” In *Neural computation* 18.7 MIT Press, 2006, pp. 1527–1554'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Geoffrey E Hinton, Simon Osindero 和 Yee-Whye Teh “深度信念网络的快速学习算法” 刊登于*Neural
    computation* 18.7 MIT Press, 2006, 第1527–1554页'
- en: '[59] Jodie S Holt “Principles of Weed Management in Agroecosystems and Wildlands1”
    In *Weed Technology* 18.sp1 BioOne, 2004, pp. 1559–1562'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Jodie S Holt “农业生态系统和野生环境中的杂草管理原则” 刊登于*Weed Technology* 18.sp1 BioOne,
    2004, 第1559–1562页'
- en: '[60] Mohammad-Parsa Hosseini et al. “Deep learning architectures” In *Deep
    learning: concepts and architectures* Springer, 2020, pp. 1–24'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 穆罕默德·帕尔萨·霍赛尼等。“深度学习架构” 见 *深度学习：概念与架构* 施普林格，2020年，第1–24页'
- en: '[61] Kun Hu et al. “Graph weeds net: A graph-based deep learning method for
    weed recognition” In *Computers and Electronics in Agriculture* 174 Elsevier,
    2020, pp. 105520'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 胡坤等。“图形杂草网：一种基于图的深度学习方法用于杂草识别” 见 *农业中的计算机与电子学* 174 爱思唯尔，2020年，第105520页'
- en: '[62] Huasheng Huang et al. “Accurate weed mapping and prescription map generation
    based on fully convolutional networks using UAV imagery” In *Sensors* 18.10 Multidisciplinary
    Digital Publishing Institute, 2018, pp. 3299'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 黄华生等。“基于全卷积网络和无人机图像的准确杂草映射与处方图生成” 见 *传感器* 18.10 多学科数字出版研究所，2018年，第3299页'
- en: '[63] Huasheng Huang et al. “A fully convolutional network for weed mapping
    of unmanned aerial vehicle (UAV) imagery” In *PloS one* 13.4 Public Library of
    Science San Francisco, CA USA, 2018, pp. e0196302'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 黄华生等。“一种用于无人机（UAV）图像杂草映射的全卷积网络” 见 *PloS one* 13.4 公共科学图书馆旧金山，加州，美国，2018年，第e0196302页'
- en: '[64] Huasheng Huang et al. “A semantic labeling approach for accurate weed
    mapping of high resolution UAV imagery” In *Sensors* 18.7 Multidisciplinary Digital
    Publishing Institute, 2018, pp. 2113'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 黄华生等。“一种用于高分辨率无人机图像准确杂草映射的语义标记方法” 见 *传感器* 18.7 多学科数字出版研究所，2018年，第2113页'
- en: '[65] Huasheng Huang et al. “Deep learning versus Object-based Image Analysis
    (OBIA) in weed mapping of UAV imagery” In *International Journal of Remote Sensing*
    41.9 Taylor & Francis, 2020, pp. 3446–3479'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 黄华生等。“深度学习与基于对象的图像分析（OBIA）在无人机图像杂草映射中的比较” 见 *国际遥感杂志* 41.9 泰勒与弗朗西斯，2020年，第3446–3479页'
- en: '[66] Sheng-Wei Huang et al. “Auggan: Cross domain adaptation with gan-based
    data augmentation” In *Proceedings of the European Conference on Computer Vision
    (ECCV)*, 2018, pp. 718–731'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 黄生伟等。“Auggan: 基于GAN的数据增强的跨领域适应” 见 *欧洲计算机视觉会议（ECCV）论文集*，2018年，第718–731页'
- en: '[67] Nadeem Iqbal, Sudheesh Manalil, Bhagirath S Chauhan and Steve W Adkins
    “Investigation of alternate herbicides for effective weed management in glyphosate-tolerant
    cotton” In *Archives of Agronomy and Soil Science* 65.13 Taylor & Francis, 2019,
    pp. 1885–1899'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 纳迪姆·伊克巴尔、苏德希什·马纳利、巴吉拉特·S·肖汉和史蒂夫·W·阿德金斯 “替代除草剂在耐草甘膦棉花中有效杂草管理的研究” 见 *农业与土壤科学档案*
    65.13 泰勒与弗朗西斯，2019年，第1885–1899页'
- en: '[68] Asnor Juraiza Ishak, Siti Salasiah Mokri, Mohd Marzuki Mustafa and Aini
    Hussain “Weed detection utilizing quadratic polynomial and ROI techniques” In
    *2007 5th Student Conference on Research and Development*, 2007, pp. 1–5 IEEE'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 阿斯诺尔·朱拉伊扎·伊沙克、席蒂·萨拉西亚赫·莫克里、莫哈德·马尔祖基·穆斯塔法和艾尼·侯赛因 “利用二次多项式和ROI技术进行杂草检测”
    见 *2007年第五届学生研究与发展会议*，2007年，第1–5页 IEEE'
- en: '[69] Abdolabbas Jafari, Seyed Saeid Mohtasebi, H Eghbali Jahromi and Mahmoud
    Omid “Weed detection in sugar beet fields using machine vision” In *Int. J. Agric.
    Biol* 8.5, 2006, pp. 602–605'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 阿卜杜拉巴斯·贾法里、赛义德·赛义德·莫赫塔塞比、H·埃赫巴利·贾赫罗米和马赫穆德·奥米德 “利用机器视觉在甜菜田中检测杂草” 见 *国际农业与生物学杂志*
    8.5，2006年，第602–605页'
- en: '[70] Troy Arnold Jensen, Bruen Smith and Livia Faria Defeo “An automated site-specific
    fallow weed management system using unmanned aerial vehicles”, 2020'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] 特洛伊·阿诺德·詹森、布伦·史密斯和利维亚·法里亚·德费奥 “使用无人机的自动化特定地点休闲杂草管理系统”，2020年'
- en: '[71] Honghua Jiang et al. “CNN feature based graph convolutional network for
    weed and crop recognition in smart farming” In *Computers and Electronics in Agriculture*
    174 Elsevier, 2020, pp. 105450'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 姜红华等。“基于CNN特征的图卷积网络用于智能农业中的杂草与作物识别” 见 *农业中的计算机与电子学* 174 爱思唯尔，2020年，第105450页'
- en: '[72] Yu Jiang, Changying Li, Andrew H Paterson and Jon S Robertson “DeepSeedling:
    deep convolutional network and Kalman filter for plant seedling detection and
    counting in the field” In *Plant methods* 15.1 Springer, 2019, pp. 141'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 龚玉、李昌英、安德鲁·H·帕特森和乔恩·S·罗伯特森 “DeepSeedling：用于植物幼苗检测和计数的深度卷积网络与卡尔曼滤波器” 见
    *植物方法* 15.1 施普林格，2019年，第141页'
- en: '[73] Andreas Kamilaris and Francesc X Prenafeta-Boldú “Deep learning in agriculture:
    A survey” In *Computers and electronics in agriculture* 147 Elsevier, 2018, pp.
    70–90'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] 安德烈亚斯·卡米拉里斯和弗朗西斯克·X·普雷纳费塔-博尔杜 “农业中的深度学习：综述” 见 *农业中的计算机与电子学* 147 爱思唯尔，2018年，第70–90页'
- en: '[74] Y Karimi, SO Prasher, RM Patel and SH Kim “Application of support vector
    machine technology for weed and nitrogen stress detection in corn” In *Computers
    and electronics in agriculture* 51.1-2 Elsevier, 2006, pp. 99–109'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Y·卡里米、S·O·普拉舍尔、R·M·帕特尔和S·H·金 “支持向量机技术在玉米中检测杂草与氮素胁迫的应用” 见 *农业中的计算机与电子学*
    51.1-2 爱思唯尔，2006年，第99–109页'
- en: '[75] Wajahat Kazmi et al. “Detecting creeping thistle in sugar beet fields
    using vegetation indices” In *Computers and Electronics in Agriculture* 112 Elsevier,
    2015, pp. 10–19'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Wajahat Kazmi 等 “使用植被指数检测糖用甜菜田中的爬行蓟” 发表在 *农业中的计算机与电子* 112 Elsevier，2015年，页10–19'
- en: '[76] Salman H Khan et al. “Cost-sensitive learning of deep feature representations
    from imbalanced data” In *IEEE transactions on neural networks and learning systems*
    29.8 IEEE, 2017, pp. 3573–3587'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Salman H Khan 等 “从不平衡数据中深度特征表示的成本敏感学习” 发表在 *IEEE神经网络与学习系统期刊* 29.8 IEEE，2017年，页3573–3587'
- en: '[77] Mina Khoshdeli, Richard Cong and Bahram Parvin “Detection of nuclei in
    H&E stained sections using convolutional neural networks” In *2017 IEEE EMBS International
    Conference on Biomedical & Health Informatics (BHI)*, 2017, pp. 105–108 IEEE'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Mina Khoshdeli, Richard Cong 和 Bahram Parvin “使用卷积神经网络检测H&E染色切片中的细胞核”
    发表在 *2017 IEEE EMBS国际生物医学与健康信息学会议 (BHI)*，2017年，页105–108 IEEE'
- en: '[78] Thomas N Kipf and Max Welling “Semi-supervised classification with graph
    convolutional networks” In *arXiv preprint arXiv:1609.02907*, 2016'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Thomas N Kipf 和 Max Welling “图卷积网络的半监督分类” 发表在 *arXiv预印本 arXiv:1609.02907*，2016年'
- en: '[79] Florian J Knoll, Vitali Czymmek, Leif O Harders and Stephan Hussmann “Real-time
    classification of weeds in organic carrot production using deep learning algorithms”
    In *Computers and Electronics in Agriculture* 167 Elsevier, 2019, pp. 105097'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Florian J Knoll, Vitali Czymmek, Leif O Harders 和 Stephan Hussmann “使用深度学习算法实时分类有机胡萝卜生产中的杂草”
    发表在 *农业中的计算机与电子* 167 Elsevier，2019年，页105097'
- en: '[80] S Kodagoda, Z Zhang, D Ruiz and G Dissanayake “Weed detection and classification
    for autonomous farming” In *Intelligent Production Machines and Systems* I* PROMS,
    2008'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S Kodagoda, Z Zhang, D Ruiz 和 G Dissanayake “自主农业中的杂草检测与分类” 发表在 *智能生产机器与系统*
    I* PROMS，2008年'
- en: '[81] Tsampikos Kounalakis et al. “A robotic system employing deep learning
    for visual recognition and detection of weeds in Grasslands” In *2018 IEEE International
    Conference on Imaging Systems and Techniques (IST)*, 2018, pp. 1–6 IEEE'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Tsampikos Kounalakis 等 “采用深度学习的机器人系统在草地中进行杂草的视觉识别和检测” 发表在 *2018 IEEE国际成像系统与技术会议
    (IST)*，2018年，页1–6 IEEE'
- en: '[82] Tsampikos Kounalakis, Georgios A Triantafyllidis and Lazaros Nalpantidis
    “Deep learning-based visual recognition of rumex for robotic precision farming”
    In *Computers and Electronics in Agriculture* 165 Elsevier, 2019, pp. 104973'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Tsampikos Kounalakis, Georgios A Triantafyllidis 和 Lazaros Nalpantidis
    “基于深度学习的 Rumex 视觉识别用于机器人精密农业” 发表在 *农业中的计算机与电子* 165 Elsevier，2019年，页104973'
- en: '[83] Alex Krizhevsky, Ilya Sutskever and Geoffrey E Hinton “Imagenet classification
    with deep convolutional neural networks” In *Advances in neural information processing
    systems* 25, 2012, pp. 1097–1105'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Alex Krizhevsky, Ilya Sutskever 和 Geoffrey E Hinton “使用深度卷积神经网络进行Imagenet分类”
    发表在 *神经信息处理系统进展* 25，2012年，页1097–1105'
- en: '[84] Harshit Kumar “Data augmentation Techniques” In *OpenGenus IQ: Learn Computer
    Science* OpenGenus IQ: Learn Computer Science, 2019 URL: [https://iq.opengenus.org/data-augmentation/](https://iq.opengenus.org/data-augmentation/)'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Harshit Kumar “数据增强技术” 发表在 *OpenGenus IQ: 学习计算机科学* OpenGenus IQ: 学习计算机科学，2019年
    URL: [https://iq.opengenus.org/data-augmentation/](https://iq.opengenus.org/data-augmentation/)'
- en: '[85] Rattan Lal “Soil structure and sustainability” In *Journal of sustainable
    agriculture* 1.4 Taylor & Francis, 1991, pp. 67–92'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Rattan Lal “土壤结构与可持续性” 发表在 *可持续农业期刊* 1.4 Taylor & Francis，1991年，页67–92'
- en: '[86] Olee Hoi Ying Lam et al. “An open source workflow for weed mapping in
    native grassland using unmanned aerial vehicle: using Rumex obtusifolius as a
    case study” In *European Journal of Remote Sensing* Taylor & Francis, 2020, pp.
    1–18'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Olee Hoi Ying Lam 等 “基于无人机的本地草地杂草绘图开源工作流程：以 Rumex obtusifolius 为案例研究”
    发表在 *欧洲遥感期刊* Taylor & Francis，2020年，页1–18'
- en: '[87] Petre Lameski, Eftim Zdravevski and Andrea Kulakov “Review of automated
    weed control approaches: an environmental impact perspective” In *International
    Conference on Telecommunications*, 2018, pp. 132–147 Springer'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Petre Lameski, Eftim Zdravevski 和 Andrea Kulakov “自动化除草方法综述：环境影响视角” 发表在
    *国际电信会议*，2018年，页132–147 Springer'
- en: '[88] Petre Lameski, Eftim Zdravevski, Vladimir Trajkovik and Andrea Kulakov
    “Weed detection dataset with RGB images taken under variable light conditions”
    In *International Conference on ICT Innovations*, 2017, pp. 112–119 Springer'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Petre Lameski, Eftim Zdravevski, Vladimir Trajkovik 和 Andrea Kulakov “在不同光照条件下拍摄的RGB图像的杂草检测数据集”
    发表在 *国际信息通信技术创新会议*，2017年，页112–119 Springer'
- en: '[89] Vi Nguyen Thanh Le, Selam Ahderom and Kamal Alameh “Performances of the
    LBP Based Algorithm over CNN Models for Detecting Crops and Weeds with Similar
    Morphologies” In *Sensors* 20.8 Multidisciplinary Digital Publishing Institute,
    2020, pp. 2193'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] 莱维阮斯安＝托勒古，塞拉姆·阿德罗姆和卡迈勒·阿拉米：“基于CNN模型的LBP算法在检测具有相似形态的农作物和杂草方面的表现” 发表于*传感器*
    20.8 Multidisciplinary Digital Publishing Institute，2020年，第2193页'
- en: '[90] Vi Nguyen Thanh Le, Selam Ahderom, Beniamin Apopei and Kamal Alameh “A
    novel method for detecting morphologically similar crops and weeds based on the
    combination of contour masks and filtered Local Binary Pattern operators” In *GigaScience*
    9.3 Oxford University Press, 2020, pp. giaa017'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] 老维西阮斯安，塞拉姆·阿德罗姆，贝尼阿明·阿波皮和卡迈勒·阿拉米：“基于轮廓掩模和滤波局部二值模式运算符组合的检测形态相似作物和杂草的新方法”
    发表于*GigaScience* 9.3 牛津大学出版社，2020年，第giaa017页'
- en: '[91] Yann LeCun, Yoshua Bengio and Geoffrey Hinton “Deep learning” In *nature*
    521.7553 Nature Publishing Group, 2015, pp. 436–444'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] 杨立康，杨子洋和李振宇：“深度学习” 发表于*nature* 521.7553 Nature Publishing Group，2015年，第436-444页'
- en: '[92] Yann LeCun et al. “Backpropagation applied to handwritten zip code recognition”
    In *Neural computation* 1.4 MIT Press, 1989, pp. 541–551'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] 杨立康等人：“反向传播应用于手写邮政编码识别” 发表于*神经计算* 1.4 MIT Press，1989年，第541-551页'
- en: '[93] Christian Ledig et al. “Photo-realistic single image super-resolution
    using a generative adversarial network” In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2017, pp. 4681–4690'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] 克里斯蒂安·勒迪格等人：“使用生成对抗网络进行逼真的单幅图像超分辨率” 发表于*IEEE计算机视觉与模式识别大会论文集*，2017年，第4681-4690页'
- en: '[94] Dong-Hyun Lee “Pseudo-label: The simple and efficient semi-supervised
    learning method for deep neural networks” In *Workshop on challenges in representation
    learning, ICML* 3.2, 2013'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] 李东炫：“伪标签：深度神经网络的简单高效半监督学习方法” 发表于*ICML中的表现学习挑战研讨会* 3.2，2013年'
- en: '[95] Simon Leminen Madsen et al. “Open Plant Phenotype Database of Common Weeds
    in Denmark” In *Remote Sensing* 12.8 Multidisciplinary Digital Publishing Institute,
    2020, pp. 1246'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] 西蒙·勒米宁·马德森等人：“丹麦常见杂草的开放式植物表型数据库” 发表于*遥感* 12.8 Multidisciplinary Digital
    Publishing Institute，2020年，第1246页'
- en: '[96] Konstantinos G Liakos et al. “Machine learning in agriculture: A review”
    In *Sensors* 18.8 Multidisciplinary Digital Publishing Institute, 2018, pp. 2674'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Konstantinos G Liakos等人：“农业中的机器学习：一项综述” 发表于*传感器* 18.8 Multidisciplinary
    Digital Publishing Institute，2018年，第2674页'
- en: '[97] Wei-Che Liang, You-Jei Yang and Chih-Min Chao “Low-Cost Weed Identification
    System Using Drones” In *2019 Seventh International Symposium on Computing and
    Networking Workshops (CANDARW)*, 2019, pp. 260–263 IEEE'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] 梁维哲，杨佑杰和赵志敏：“使用无人机的低成本杂草识别系统” 发表于*2019年第七届计算与网络研讨会 (CANDARW)*，2019年，第260-263页
    IEEE'
- en: '[98] Tsung-Yi Lin et al. “Microsoft coco: Common objects in context” In *European
    conference on computer vision*, 2014, pp. 740–755 Springer'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] 林宗毅等人：“微软COCO：上下文中的常见对象” 发表于*欧洲计算机视觉大会*，2014年，第740-755页 Springer'
- en: '[99] Bo Liu and Ryan Bruch “Weed Detection for Selective Spraying: a Review”
    In *Current Robotics Reports* 1.1 Springer, 2020, pp. 19–26'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] 刘波和莱恩·布鲁赫：“用于选择性喷洒的杂草检测：一项综述” 发表于*当前机器人报告* 1.1 Springer，2020年，第19-26页'
- en: '[100] Wei Liu et al. “Ssd: Single shot multibox detector” In *European conference
    on computer vision*, 2016, pp. 21–37 Springer'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] 刘伟等人：“SSD：单次多盒子检测器” 发表于*欧洲计算机视觉大会*，2016年，第21-37页 Springer'
- en: '[101] Philipp Lottes et al. “Joint stem detection and crop-weed classification
    for plant-specific treatment in precision farming” In *2018 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2018, pp. 8233–8238 IEEE'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Philipp Lottes等人：“植物特定处理的联合茎检测和作物-杂草分类在精准农业中的应用” 发表于*2018年IEEE/RSJ国际智能机器人与系统大会
    (IROS)*，2018年，第8233-8238页 IEEE'
- en: '[102] Philipp Lottes et al. “Robust joint stem detection and crop-weed classification
    using image sequences for plant-specific treatment in precision farming” In *Journal
    of Field Robotics* 37.1 Wiley Online Library, 2020, pp. 20–34'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Philipp Lottes等人：“使用图像序列进行植物特定处理的强大的联合茎检测和作物-杂草分类” 发表于*田野机器人学杂志* 37.1
    Wiley Online Library，2020年，第20-34页'
- en: '[103] Philipp Lottes, Jens Behley, Andres Milioto and Cyrill Stachniss “Fully
    convolutional networks with sequential information for robust crop and weed detection
    in precision farming” In *IEEE Robotics and Automation Letters* 3.4 IEEE, 2018,
    pp. 2870–2877'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Philipp Lottes, Jens Behley, Andres Milioto和Cyrill Stachniss：“具有顺序信息的完全卷积网络用于精准农业中的强大作物和杂草检测”
    发表于*IEEE机器人与自动化通讯* 3.4 IEEE，2018年，第2870-2877页'
- en: '[104] Xu Ma et al. “Fully convolutional network for rice seedling and weed
    image segmentation at the seedling stage in paddy fields” In *PloS one* 14.4 Public
    Library of Science San Francisco, CA USA, 2019, pp. e0215676'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Xu Ma 等 “基于全卷积网络的稻苗与杂草图像分割：以稻田苗期为例” 发表于 *PloS One* 14.4 公共科学图书馆，旧金山，加州，美国，2019年，第e0215676页'
- en: '[105] Inneke Mayachita “Understanding Graph Convolutional Networks for Node
    Classification” In *Medium* Towards Data Science, 2020 URL: [https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b)'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Inneke Mayachita “理解图卷积网络在节点分类中的应用” 发表于 *Medium* 《数据科学前沿》，2020年 URL:
    [https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b](https://towardsdatascience.com/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b)'
- en: '[106] C.. Merfield “Robotic weeding’s false dawn? Ten requirements for fully
    autonomous mechanical weed management” In *Weed Research* 56.5, 2016, pp. 340–344
    DOI: [10.1111/wre.12217](https://dx.doi.org/10.1111/wre.12217)'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] C. Merfield “机器人除草的虚假黎明？完全自主机械杂草管理的十项要求” 发表于 *杂草研究* 56.5，2016年，第340–344页
    DOI: [10.1111/wre.12217](https://dx.doi.org/10.1111/wre.12217)'
- en: '[107] GE Meyer et al. “Textural imaging and discriminant analysis for distinguishingweeds
    for spot spraying” In *Transactions of the ASAE* 41.4 American Society of AgriculturalBiological
    Engineers, 1998, pp. 1189'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] GE Meyer 等 “用于区分杂草的纹理成像与判别分析” 发表于 *ASAE学报* 41.4 美国农业生物工程师学会，1998年，第1189页'
- en: '[108] Andres Milioto, Philipp Lottes and Cyrill Stachniss “Real-time blob-wise
    sugar beets vs weeds classification for monitoring fields using convolutional
    neural networks” In *ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial
    Information Sciences* 4 Copernicus GmbH, 2017, pp. 41'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Andres Milioto, Philipp Lottes 和 Cyrill Stachniss “使用卷积神经网络实时分类糖用甜菜与杂草：田间监测研究”
    发表于 *ISPRS 摄影测量、遥感与空间信息科学年刊* 4 Copernicus GmbH，2017年，第41页'
- en: '[109] Syed I Moazzam et al. “A Review of Application of Deep Learning for Weeds
    and Crops Classification in Agriculture” In *2019 International Conference on
    Robotics and Automation in Industry (ICRAI)*, 2019, pp. 1–6 IEEE'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Syed I Moazzam 等 “深度学习在农业中杂草与作物分类应用的综述” 发表于 *2019国际机器人与自动化工业会议 (ICRAI)*，2019年，第1–6页
    IEEE'
- en: '[110] Maryam M Najafabadi et al. “Deep learning applications and challenges
    in big data analytics” In *Journal of Big Data* 2.1 Springer, 2015, pp. 1'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Maryam M Najafabadi 等 “大数据分析中的深度学习应用与挑战” 发表于 *大数据期刊* 2.1 Springer，2015年，第1页'
- en: '[111] Daniel K Nkemelu, Daniel Omeiza and Nancy Lubalo “Deep convolutional
    neural network for plant seedlings classification” In *arXiv preprint arXiv:1811.08404*,
    2018'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Daniel K Nkemelu, Daniel Omeiza 和 Nancy Lubalo “用于植物苗木分类的深度卷积神经网络” 发表于
    *arXiv 预印本 arXiv:1811.08404*，2018年'
- en: '[112] K. Okese, Theresah Kankam, Jennifer Boamah and Owusu Mensah Evans “Basic
    Principles of Weeds Control and Management” In *MyFarm Blog*, 2020 URL: [https://blog.agrihomegh.com/principles-weeds-control-management/](https://blog.agrihomegh.com/principles-weeds-control-management/)'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] K. Okese, Theresah Kankam, Jennifer Boamah 和 Owusu Mensah Evans “杂草控制与管理的基本原理”
    发表于 *MyFarm博客*，2020年 URL: [https://blog.agrihomegh.com/principles-weeds-control-management/](https://blog.agrihomegh.com/principles-weeds-control-management/)'
- en: '[113] Alex Olsen et al. “DeepWeeds: A multiclass weed species image dataset
    for deep learning” In *Scientific reports* 9.1 Nature Publishing Group, 2019,
    pp. 1–12'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Alex Olsen 等 “DeepWeeds：用于深度学习的多类别杂草物种图像数据集” 发表于 *科学报告* 9.1 自然出版集团，2019年，第1–12页'
- en: '[114] Kavir Osorio et al. “A Deep Learning Approach for Weed Detection in Lettuce
    Crops Using Multispectral Images” In *AgriEngineering* 2.3 Multidisciplinary Digital
    Publishing Institute, 2020, pp. 471–488'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Kavir Osorio 等 “一种基于深度学习的生菜作物杂草检测方法：利用多光谱图像” 发表于 *农业工程* 2.3 多学科数字出版机构，2020年，第471–488页'
- en: '[115] Akin Ozcift and Arif Gulten “Classifier ensemble construction with rotation
    forest to improve medical diagnosis performance of machine learning algorithms”
    In *Computer methods and programs in biomedicine* 104.3 Elsevier, 2011, pp. 443–451'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Akin Ozcift 和 Arif Gulten “通过旋转森林构建分类器集成以提高机器学习算法的医学诊断性能” 发表于 *生物医学计算方法与程序*
    104.3 Elsevier，2011年，第443–451页'
- en: '[116] Victor Partel, Sri Charan Kakarla and Yiannis Ampatzidis “Development
    and evaluation of a low-cost and smart technology for precision weed management
    utilizing artificial intelligence” In *Computers and electronics in agriculture*
    157 Elsevier, 2019, pp. 339–350'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Victor Partel, Sri Charan Kakarla 和 Yiannis Ampatzidis “开发和评估用于精准杂草管理的低成本智能技术，利用人工智能”
    见 *农业计算机与电子学* 157 Elsevier, 2019, 页码 339–350'
- en: '[117] Victor Partel et al. “Smart Sprayer for Precision Weed Control Using
    Artificial Intelligence: Comparison of Deep Learning Frameworks” In *Association
    for the Advancement of Artificial Intelligence*, 2019'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Victor Partel 等 “利用人工智能的精准喷雾器进行杂草控制：深度学习框架的比较” 见 *人工智能促进协会*, 2019'
- en: '[118] DD Patel and BA Kumbhar “Weed and its management: A major threats to
    crop economy” In *J. Pharm. Sci. Bioscientific Res* 6.6, 2016, pp. 453–758'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] DD Patel 和 BA Kumbhar “杂草及其管理：对作物经济的主要威胁” 见 *药学科学与生物科学研究* 6.6, 2016,
    页码 453–758'
- en: '[119] Sanjay Patidar, Utkarsh Singh and Sumit Kumar Sharma “Weed Seedling Detection
    Using Mask Regional Convolutional Neural Network” In *2020 International Conference
    on Electronics and Sustainable Communication Systems (ICESC)*, 2020, pp. 311–316
    IEEE'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Sanjay Patidar, Utkarsh Singh 和 Sumit Kumar Sharma “使用掩模区域卷积神经网络的杂草幼苗检测”
    见 *2020 国际电子与可持续通信系统会议 (ICESC)*, 2020, 页码 311–316 IEEE'
- en: '[120] Josh Patterson and Adam Gibson “Deep learning: A practitioner’s approach”
    ” O’Reilly Media, Inc.”, 2017'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Josh Patterson 和 Adam Gibson “深度学习：从业者的视角” “O''Reilly Media, Inc.”, 2017'
- en: '[121] Larry Pearlstein, Mun Kim and Warren Seto “Convolutional neural network
    application to plant detection, based on synthetic imagery” In *2016 IEEE Applied
    Imagery Pattern Recognition Workshop (AIPR)*, 2016, pp. 1–4 IEEE'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Larry Pearlstein, Mun Kim 和 Warren Seto “基于合成图像的植物检测卷积神经网络应用” 见 *2016
    IEEE 应用图像模式识别研讨会 (AIPR)*, 2016, 页码 1–4 IEEE'
- en: '[122] Lukas Petrich et al. “Detection of Colchicum autumnale in drone images,
    using a machine-learning approach” Springer, 2019'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Lukas Petrich 等 “使用机器学习方法检测无人机图像中的秋水仙” Springer, 2019'
- en: '[123] PyTorch “AI for AG: Production machine learning for agriculture” In *Medium*
    PyTorch, 2020 URL: [https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] PyTorch “AI for AG: 农业生产机器学习” 见 *Medium* PyTorch, 2020 网址: [https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)'
- en: '[124] Zheng Qin et al. “Thundernet: Towards real-time generic object detection
    on mobile devices” In *Proceedings of the IEEE/CVF International Conference on
    Computer Vision*, 2019, pp. 6718–6727'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Zheng Qin 等 “Thundernet: 面向移动设备的实时通用对象检测” 见 *IEEE/CVF 国际计算机视觉会议论文集*,
    2019, 页码 6718–6727'
- en: '[125] Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Thomas Lagkas
    and Ioannis Moscholios “A compilation of UAV applications for precision agriculture”
    In *Computer Networks* 172 Elsevier, 2020, pp. 107148'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Panagiotis Radoglou-Grammatikis, Panagiotis Sarigiannidis, Thomas Lagkas
    和 Ioannis Moscholios “针对精准农业的无人机应用汇编” 见 *计算机网络* 172 Elsevier, 2020, 页码 107148'
- en: '[126] Rekha Raja, Thuy T Nguyen, David C Slaughter and Steven A Fennimore “Real-time
    robotic weed knife control system for tomato and lettuce based on geometric appearance
    of plant labels” In *Biosystems Engineering* 194 Elsevier, 2020, pp. 152–164'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Rekha Raja, Thuy T Nguyen, David C Slaughter 和 Steven A Fennimore “基于植物标签几何外观的实时机器人除草刀控制系统”
    见 *生物系统工程* 194 Elsevier, 2020, 页码 152–164'
- en: '[127] W Ramirez, P Achanccaray, LF Mendoza and MAC Pacheco “Deep Convolutional
    Neural Networks for Weed Detection in Agricultural Crops Using Optical Aerial
    Images” In *2020 IEEE Latin American GRSS & ISPRS Remote Sensing Conference (LAGIRS)*,
    2020, pp. 133–137 IEEE'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] W Ramirez, P Achanccaray, LF Mendoza 和 MAC Pacheco “利用光学航空图像的深度卷积神经网络进行农业作物杂草检测”
    见 *2020 IEEE 拉丁美洲 GRSS & ISPRS 遥感会议 (LAGIRS)*, 2020, 页码 133–137 IEEE'
- en: '[128] Pejman Rasti et al. “Supervised Image Classification by Scattering Transform
    with Application to Weed Detection in Culture Crops of High Density” In *Remote
    Sensing* 11.3, 2019, pp. 249 DOI: [10.3390/rs11030249](https://dx.doi.org/10.3390/rs11030249)'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Pejman Rasti 等 “基于散射变换的监督图像分类及其在高密度文化作物杂草检测中的应用” 见 *遥感* 11.3, 2019, 页码
    249 DOI: [10.3390/rs11030249](https://dx.doi.org/10.3390/rs11030249)'
- en: '[129] Joseph Redmon In *Darknet: Open Source Neural Networks in C* URL: [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Joseph Redmon 见 *Darknet: 用C语言编写的开源神经网络* 网址: [https://pjreddie.com/darknet/](https://pjreddie.com/darknet/)'
- en: '[130] Joseph Redmon and Ali Farhadi “Yolov3: An incremental improvement” In
    *arXiv preprint arXiv:1804.02767*, 2018'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Joseph Redmon 和 Ali Farhadi “Yolov3: 一项增量改进” 在 *arXiv 预印本 arXiv:1804.02767*，2018年'
- en: '[131] Shaoqing Ren, Kaiming He, Ross Girshick and Jian Sun “Faster r-cnn: Towards
    real-time object detection with region proposal networks” In *arXiv preprint arXiv:1506.01497*,
    2015'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Shaoqing Ren, Kaiming He, Ross Girshick 和 Jian Sun “Faster R-CNN: 基于区域提议网络的实时目标检测”
    在 *arXiv 预印本 arXiv:1506.01497*，2015年'
- en: '[132] Yannik Rist, Iurii Shendryk, Foivos Diakogiannis and Shaun Levick “Weed
    Mapping Using Very High Resolution Satellite Imagery and Fully Convolutional Neural
    Network” In *IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing
    Symposium*, 2019, pp. 9784–9787 IEEE'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Yannik Rist, Iurii Shendryk, Foivos Diakogiannis 和 Shaun Levick “使用超高分辨率卫星图像和全卷积神经网络的杂草映射”
    在 *IGARSS 2019-2019 IEEE 国际地球科学与遥感会议*，2019年，第 9784–9787 页 IEEE'
- en: '[133] Olaf Ronneberger, Philipp Fischer and Thomas Brox “U-net: Convolutional
    networks for biomedical image segmentation” In *International Conference on Medical
    image computing and computer-assisted intervention*, 2015, pp. 234–241 Springer'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Olaf Ronneberger, Philipp Fischer 和 Thomas Brox “U-net: 用于生物医学图像分割的卷积网络”
    在 *国际医学图像计算与计算机辅助干预会议*，2015年，第 234–241 页 Springer'
- en: '[134] Inkyu Sa et al. “weednet: Dense semantic weed classification using multispectral
    images and mav for smart farming” In *IEEE Robotics and Automation Letters* 3.1
    IEEE, 2017, pp. 588–595'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Inkyu Sa 等 “weednet: 使用多光谱图像和 MAV 进行智能农业的密集语义杂草分类” 在 *IEEE Robotics and
    Automation Letters* 3.1 IEEE，2017年，第 588–595 页'
- en: '[135] Inkyu Sa et al. “Weedmap: a large-scale semantic weed mapping framework
    using aerial multispectral imaging and deep neural network for precision farming”
    In *Remote Sensing* 10.9 Multidisciplinary Digital Publishing Institute, 2018,
    pp. 1423'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Inkyu Sa 等 “Weedmap: 使用空中多光谱成像和深度神经网络进行精准农业的大规模语义杂草映射框架” 在 *Remote Sensing*
    10.9 多学科数字出版机构，2018年，第 1423 页'
- en: '[136] Sajad Sabzi, Yousef Abbaspour-Gilandeh and Juan Ignacio Arribas “An automatic
    visible-range video weed detection, segmentation and classification prototype
    in potato field” In *Heliyon* 6.5 Elsevier, 2020, pp. e03685'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Sajad Sabzi, Yousef Abbaspour-Gilandeh 和 Juan Ignacio Arribas “在土豆田中自动可见范围视频杂草检测、分割和分类原型”
    在 *Heliyon* 6.5 Elsevier，2020年，第 e03685 页'
- en: '[137] Linda Sakyi “Linda Sakyi” In *Greenroot Limited*, 2019 URL: [https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/](https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Linda Sakyi “Linda Sakyi” 在 *Greenroot Limited*，2019 网址: [https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/](https://greenrootltd.com/2019/02/19/five-general-categories-of-weed-control-methods/)'
- en: '[138] T Sarvini et al. “Performance Comparison of Weed Detection Algorithms”
    In *2019 International Conference on Communication and Signal Processing (ICCSP)*,
    2019, pp. 0843–0847 IEEE'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] T Sarvini 等 “杂草检测算法性能比较” 在 *2019 国际通信与信号处理会议 (ICCSP)*，2019年，第 0843–0847
    页 IEEE'
- en: '[139] Santhosh K Seelan, Soizik Laguette, Grant M Casady and George A Seielstad
    “Remote sensing applications for precision agriculture: A learning community approach”
    In *Remote sensing of environment* 88.1-2 Elsevier, 2003, pp. 157–169'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Santhosh K Seelan, Soizik Laguette, Grant M Casady 和 George A Seielstad
    “精准农业的遥感应用：学习社区方法” 在 *Remote Sensing of Environment* 88.1-2 Elsevier，2003年，第 157–169
    页'
- en: '[140] Saraswathi Shanmugam et al. “Automated Weed Detection Systems: A Review”
    In *KnE Engineering*, 2020, pp. 271–284'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Saraswathi Shanmugam 等 “自动化杂草检测系统：综述” 在 *KnE Engineering*，2020年，第 271–284
    页'
- en: '[141] Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Detection of Carolina
    geranium (Geranium carolinianum) growing in competition with strawberry using
    convolutional neural networks” In *Weed Science* 67.2 BioOne, 2019, pp. 239–245'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Shaun M Sharpe, Arnold W Schumann 和 Nathan S Boyd “使用卷积神经网络检测与草莓竞争生长的卡罗莱纳天竺葵
    (Geranium carolinianum)” 在 *Weed Science* 67.2 BioOne，2019年，第 239–245 页'
- en: '[142] Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Goosegrass Detection
    in Strawberry and Tomato Using a Convolutional Neural Network” In *Scientific
    Reports* 10.1 Nature Publishing Group, 2020, pp. 1–8'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Shaun M Sharpe, Arnold W Schumann 和 Nathan S Boyd “使用卷积神经网络检测草莓和番茄中的鹅草”
    在 *Scientific Reports* 10.1 Nature Publishing Group，2020年，第 1–8 页'
- en: '[143] Evan Shelhamer, Jonathan Long and Trevor Darrell “Fully convolutional
    networks for semantic segmentation” In *IEEE transactions on pattern analysis
    and machine intelligence* 39.4, 2017, pp. 640–651'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Evan Shelhamer, Jonathan Long 和 Trevor Darrell “用于语义分割的全卷积网络” 在 *IEEE
    计算机学会模式分析与机器智能汇刊* 39.4，2017年，第 640–651 页'
- en: '[144] Connor Shorten and Taghi M Khoshgoftaar “A survey on image data augmentation
    for deep learning” In *Journal of Big Data* 6.1 Springer, 2019, pp. 60'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Connor Shorten 和 Taghi M Khoshgoftaar “深度学习的图像数据增强综述” 发表在 *大数据期刊* 6.1
    Springer, 2019, 页码 60'
- en: '[145] Karen Simonyan and Andrew Zisserman “Very deep convolutional networks
    for large-scale image recognition” In *arXiv preprint arXiv:1409.1556*, 2014'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Karen Simonyan 和 Andrew Zisserman “用于大规模图像识别的非常深卷积网络” 发表在 *arXiv 预印本
    arXiv:1409.1556*, 2014'
- en: '[146] Arun Narenthiran Veeranampalayam Sivakumar et al. “Comparison of Object
    Detection and Patch-Based Classification Deep Learning Models on Mid-to Late-Season
    Weed Detection in UAV Imagery” In *Remote Sensing* 12.13 Multidisciplinary Digital
    Publishing Institute, 2020, pp. 2136'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Arun Narenthiran Veeranampalayam Sivakumar 等. “中后季杂草检测中的目标检测与基于补丁的分类深度学习模型比较”
    发表在 *遥感* 12.13 Multidisciplinary Digital Publishing Institute, 2020, 页码 2136'
- en: '[147] Soren Skovsen et al. “The GrassClover Image Dataset for Semantic and
    Hierarchical Species Understanding in Agriculture” In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition Workshops*, 2019'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Soren Skovsen 等. “GrassClover 图像数据集：农业中语义和层次物种理解” 发表在 *IEEE 计算机视觉与模式识别会议研讨会论文集*,
    2019'
- en: '[148] Russell Stewart, Mykhaylo Andriluka and Andrew Y Ng “End-to-end people
    detection in crowded scenes” In *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2016, pp. 2325–2333'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Russell Stewart, Mykhaylo Andriluka 和 Andrew Y Ng “拥挤场景中的端到端人员检测” 发表在
    *IEEE 计算机视觉与模式识别会议论文集*, 2016, 页码 2325–2333'
- en: '[149] Wen-Hao Su “Advanced Machine Learning in Point Spectroscopy, RGB-and
    Hyperspectral-Imaging for Automatic Discriminations of Crops and Weeds: A Review”
    In *Smart Cities* 3.3 Multidisciplinary Digital Publishing Institute, 2020, pp.
    767–792'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Wen-Hao Su “点光谱、RGB 和高光谱成像中的高级机器学习用于作物和杂草的自动区分：综述” 发表在 *智慧城市* 3.3 Multidisciplinary
    Digital Publishing Institute, 2020, 页码 767–792'
- en: '[150] Kaspars Sudars et al. “Dataset of annotated food crops and weed images
    for robotic computer vision control” In *Data in Brief* Elsevier, 2020, pp. 105833'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Kaspars Sudars 等. “用于机器人计算机视觉控制的注释食品作物和杂草图像数据集” 发表在 *数据简报* Elsevier,
    2020, 页码 105833'
- en: '[151] Hyun K Suh, Joris Ijsselmuiden, Jan Willem Hofstee and Eldert J Henten
    “Transfer learning for the classification of sugar beet and volunteer potato under
    field conditions” In *Biosystems engineering* 174 Elsevier, 2018, pp. 50–65'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Hyun K Suh, Joris Ijsselmuiden, Jan Willem Hofstee 和 Eldert J Henten
    “在田间条件下的糖用甜菜和自生马铃薯分类的迁移学习” 发表在 *生物系统工程* 174 Elsevier, 2018, 页码 50–65'
- en: '[152] Christian Szegedy et al. “Going deeper with convolutions” In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2015, pp.
    1–9'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Christian Szegedy 等. “深入卷积” 发表在 *IEEE 计算机视觉与模式识别会议论文集*, 2015, 页码 1–9'
- en: '[153] Aboozar Taherkhani, Georgina Cosma and T Martin McGinnity “AdaBoost-CNN:
    An adaptive boosting algorithm for convolutional neural networks to classify multi-class
    imbalanced datasets using transfer learning” In *Neurocomputing* 404 Elsevier,
    2020, pp. 351–366'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Aboozar Taherkhani, Georgina Cosma 和 T Martin McGinnity “AdaBoost-CNN：一种用于分类多类别不平衡数据集的卷积神经网络自适应提升算法”
    发表在 *神经计算* 404 Elsevier, 2020, 页码 351–366'
- en: '[154] Ryo Takahashi, Takashi Matsubara and Kuniaki Uehara “Ricap: Random image
    cropping and patching data augmentation for deep cnns” In *Asian Conference on
    Machine Learning*, 2018, pp. 786–798'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Ryo Takahashi, Takashi Matsubara 和 Kuniaki Uehara “Ricap：随机图像裁剪和补丁数据增强用于深度卷积神经网络”
    发表在 *亚洲机器学习会议*, 2018, 页码 786–798'
- en: '[155] JingLei Tang et al. “Weed identification based on K-means feature learning
    combined with convolutional neural network” In *Computers and electronics in agriculture*
    135 Elsevier, 2017, pp. 63–70'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] JingLei Tang 等. “基于 K-means 特征学习结合卷积神经网络的杂草识别” 发表在 *农业计算机与电子学* 135 Elsevier,
    2017, 页码 63–70'
- en: '[156] Andrew Tao, Jon Barker and Sriya Sarathy “Detectnet: Deep neural network
    for object detection in digits” In *Parallel Forall* 4, 2016'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Andrew Tao, Jon Barker 和 Sriya Sarathy “Detectnet：用于数字对象检测的深度神经网络” 发表在
    *Parallel Forall* 4, 2016'
- en: '[157] Nima Teimouri et al. “Weed growth stage estimator using deep convolutional
    neural networks” In *Sensors* 18.5 Multidisciplinary Digital Publishing Institute,
    2018, pp. 1580'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Nima Teimouri 等. “利用深度卷积神经网络的杂草生长阶段估计器” 发表在 *传感器* 18.5 Multidisciplinary
    Digital Publishing Institute, 2018, 页码 1580'
- en: '[158] L Tian, DC Slaughter and RF Norris “Machine vision identification of
    tomato seedlings for automated weed control” In *Transactions of ASAE* 40.6 Citeseer,
    2000, pp. 1761–1768'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] L Tian, DC Slaughter 和 RF Norris “机器视觉识别番茄幼苗用于自动化杂草控制” 发表在 *ASAE 交易*
    40.6 Citeseer, 2000, 页码 1761–1768'
- en: '[159] Vo Hoang Trong, Yu Gwang-hyun, Dang Thanh Vu and Kim Jin-young “Late
    fusion of multimodal deep neural networks for weeds classification” In *Computers
    and Electronics in Agriculture* 175 Elsevier, 2020, pp. 105506'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Vo Hoang Trong, Yu Gwang-hyun, Dang Thanh Vu 和 Kim Jin-young “多模态深度神经网络的晚期融合用于杂草分类”
    发表在*农业计算机与电子学* 175 Elsevier，2020年，第105506页'
- en: '[160] S Umamaheswari, R Arjun and D Meganathan “Weed Detection in Farm Crops
    using Parallel Image Processing” In *2018 Conference on Information and Communication
    Technology (CICT)*, 2018, pp. 1–4 IEEE'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] S Umamaheswari, R Arjun 和 D Meganathan “使用并行图像处理进行农作物中的杂草检测” 发表在*2018
    信息与通信技术会议（CICT）*，2018年，第1–4页 IEEE'
- en: '[161] S Umamaheswari and Ashvini V Jain “Encoder–Decoder Architecture for Crop-Weed
    Classification Using Pixel-Wise Labelling” In *2020 International Conference on
    Artificial Intelligence and Signal Processing (AISP)*, 2020, pp. 1–6 IEEE'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S Umamaheswari 和 Ashvini V Jain “使用像素级标记的编码器-解码器架构进行作物-杂草分类” 发表在*2020
    国际人工智能与信号处理会议（AISP）*，2020年，第1–6页 IEEE'
- en: '[162] J Valente, M Doldersum, C Roers and L Kooistra “DETECTING RUMEX OBTUSIFOLIUS
    WEED PLANTS IN GRASSLANDS FROM UAV RGB IMAGERY USING DEEP LEARNING.” In *ISPRS
    Annals of Photogrammetry, Remote Sensing & Spatial Information Sciences* 4, 2019'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] J Valente, M Doldersum, C Roers 和 L Kooistra “利用深度学习从无人机RGB图像中检测草地上的阔叶蓼杂草”
    发表在*ISPRS 摄影测量、遥感与空间信息科学年刊* 4，2019年'
- en: '[163] Viraf “Create A Synthetic Image Dataset - The ”What”, The ”Why” and The
    ”How”” In *Medium* Towards Data Science, 2020 URL: [https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718](https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718)'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Viraf “创建合成图像数据集——‘什么’，‘为什么’以及‘如何’” 发表在*Medium* 数据科学之路，2020年 URL: [https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718](https://towardsdatascience.com/create-a-synthetic-image-dataset-the-what-the-why-and-the-how-f820e6b6f718)'
- en: '[164] Aichen Wang, Yifei Xu, Xinhua Wei and Bingbo Cui “Semantic Segmentation
    of Crop and Weed using an Encoder-Decoder Network and Image Enhancement Method
    under Uncontrolled Outdoor Illumination” In *IEEE Access* 8 IEEE, 2020, pp. 81724–81734'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Aichen Wang, Yifei Xu, Xinhua Wei 和 Bingbo Cui “使用编码器-解码器网络和图像增强方法在不受控户外光照下对作物和杂草进行语义分割”
    发表在*IEEE Access* 8 IEEE，2020年，第81724–81734页'
- en: '[165] Aichen Wang, Wen Zhang and Xinhua Wei “A review on weed detection using
    ground-based machine vision and image processing techniques” In *Computers and
    electronics in agriculture* 158 Elsevier, 2019, pp. 226–240'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Aichen Wang, Wen Zhang 和 Xinhua Wei “基于地面机器视觉和图像处理技术的杂草检测综述” 发表在*农业计算机与电子学*
    158 Elsevier，2019年，第226–240页'
- en: '[166] Alexander Wendel and James Underwood “Self-supervised weed detection
    in vegetable crops using ground based hyperspectral imaging” In *2016 IEEE international
    conference on robotics and automation (ICRA)*, 2016, pp. 5128–5135 IEEE'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Alexander Wendel 和 James Underwood “利用地面高光谱成像进行蔬菜作物的自监督杂草检测” 发表在*2016
    IEEE 国际机器人与自动化会议（ICRA）*，2016年，第5128–5135页 IEEE'
- en: '[167] DM Woebbecke, GE Meyer, K Von Bargen and DA Mortensen “Shape features
    for identifying young weeds using image analysis” In *Transactions of the ASAE*
    38.1 American Society of AgriculturalBiological Engineers, 1995, pp. 271–281'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] DM Woebbecke, GE Meyer, K Von Bargen 和 DA Mortensen “使用图像分析识别幼苗杂草的形状特征”
    发表在*ASAE会刊* 38.1 美国农业生物工程学会，1995年，第271–281页'
- en: '[168] Xue Yan, Xiangwu Deng and Jing Jin “Classification of weed species in
    the paddy field with DCNN-Learned features” In *2020 IEEE 5th Information Technology
    and Mechatronics Engineering Conference (ITOEC)*, 2020, pp. 336–340 IEEE'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Xue Yan, Xiangwu Deng 和 Jing Jin “使用DCNN学习特征对稻田杂草种类进行分类” 发表在*2020 IEEE
    第五届信息技术与机电工程会议（ITOEC）*，2020年，第336–340页 IEEE'
- en: '[169] Zhang Yi, Shen Yongliang and Zhang Jun “An improved tiny-yolov3 pedestrian
    detection algorithm” In *Optik* 183 Elsevier, 2019, pp. 17–23'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Zhang Yi, Shen Yongliang 和 Zhang Jun “一种改进的tiny-yolov3行人检测算法” 发表在*Optik*
    183 Elsevier，2019年，第17–23页'
- en: '[170] Jialin Yu et al. “Weed Detection in Perennial Ryegrass With Deep Learning
    Convolutional Neural Network” In *Frontiers in plant science* 10 Frontiers Media
    SA, 2019'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Jialin Yu 等 “利用深度学习卷积神经网络对多年生黑麦草中的杂草进行检测” 发表在*植物科学前沿* 10 Frontiers Media
    SA，2019年'
- en: '[171] Jialin Yu, Shaun M Sharpe, Arnold W Schumann and Nathan S Boyd “Deep
    learning for image-based weed detection in turfgrass” In *European journal of
    agronomy* 104 Elsevier, 2019, pp. 78–84'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Jialin Yu, Shaun M Sharpe, Arnold W Schumann 和 Nathan S Boyd “基于图像的草坪杂草深度学习检测”
    发表在*欧洲农业学杂志* 104 Elsevier，2019年，第78–84页'
- en: '[172] Rufei Zhang et al. “Weed location and recognition based on UAV imaging
    and deep learning” In *International Journal of Precision Agricultural Aviation*
    3.1, 2020'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] 张如飞等人。“基于无人机成像和深度学习的杂草位置与识别” 见于 *International Journal of Precision Agricultural
    Aviation* 3.1, 2020'
- en: '[173] Wenhao Zhang et al. “Broad-leaf weed detection in pasture” In *2018 IEEE
    3rd International Conference on Image, Vision and Computing (ICIVC)*, 2018, pp.
    101–105 IEEE'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] 张文浩等人。“牧场中宽叶杂草的检测” 见于 *2018 IEEE第3届国际图像、视觉与计算会议 (ICIVC)*, 2018, 页101–105
    IEEE'
- en: '[174] Xiao-Yu Zhang, Haichao Shi, Xiaobin Zhu and Peng Li “Active semi-supervised
    learning based on self-expressive correlation with generative adversarial networks”
    In *Neurocomputing* 345 Elsevier, 2019, pp. 103–113'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] 张晓宇、石海超、朱小斌和李鹏 “基于自我表达相关性的主动半监督学习与生成对抗网络” 见于 *Neurocomputing* 345 Elsevier,
    2019, 页103–113'
- en: '[175] Yang Zheng et al. “Maize and weed classification using color indices
    with support vector data description in outdoor fields” In *Computers and Electronics
    in Agriculture* 141 Elsevier, 2017, pp. 215–222'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] 杨征等人。“使用色彩指数与支持向量数据描述进行的玉米与杂草分类” 见于 *Computers and Electronics in Agriculture*
    141 Elsevier, 2017, 页215–222'
- en: '[176] Zhi-Hua Zhou “A brief introduction to weakly supervised learning” In
    *National Science Review* 5.1 Oxford University Press, 2018, pp. 44–53'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] 周志华 “弱监督学习简要介绍” 见于 *National Science Review* 5.1 Oxford University Press,
    2018, 页44–53'
