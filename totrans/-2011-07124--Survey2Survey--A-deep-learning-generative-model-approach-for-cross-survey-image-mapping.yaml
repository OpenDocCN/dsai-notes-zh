- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:58:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2011.07124] Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2011.07124](https://ar5iv.labs.arxiv.org/html/2011.07124)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Brandon Buncher¹, Awshesh Nath Sharma², and Matias Carrasco Kind^(3,4,5)
  prefs: []
  type: TYPE_NORMAL
- en: ¹Department of Physics, University of Illinois, Champaign, IL 61820 USA
  prefs: []
  type: TYPE_NORMAL
- en: ²Department of Earth Sciences, Indian Institute of Technology Roorkee, Roorkee,
    Uttarakhand 247667 India
  prefs: []
  type: TYPE_NORMAL
- en: ³Department of Astronomy, University of Illinois, Urbana, IL 61801 USA
  prefs: []
  type: TYPE_NORMAL
- en: ⁴National Center for Supercomputing Applications, Urbana, IL 61801 USA
  prefs: []
  type: TYPE_NORMAL
- en: ⁵Center for Astrophysical Surveys, Urbana, IL 61801 USA buncher2@illinois.edu(Accepted
    XXX. Received YYY; in original form ZZZ)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During the last decade, there has been an explosive growth in survey data and
    deep learning techniques, both of which have enabled great advances for astronomy.
    The amount of data from various surveys from multiple epochs with a wide range
    of wavelengths, albeit with varying brightness and quality, is overwhelming, and
    leveraging information from overlapping observations from different surveys has
    limitless potential in understanding galaxy formation and evolution. Synthetic
    galaxy image generation using physical models has been an important tool for survey
    data analysis, while deep learning generative models show great promise. In this
    paper, we present a novel approach for robustly expanding and improving survey
    data through cross survey feature translation. We trained two types of neural
    networks to map images from the Sloan Digital Sky Survey (SDSS) to corresponding
    images from the Dark Energy Survey (DES). This map was used to generate false
    DES representations of SDSS images, increasing the brightness and S/N while retaining
    important morphological information. We substantiate the robustness of our method
    by generating DES representations of SDSS images from outside the overlapping
    region, showing that the brightness and quality are improved even when the source
    images are of lower quality than the training images. Finally, we highlight several
    images in which the reconstruction process appears to have removed large artifacts
    from SDSS images. While only an initial application, our method shows promise
    as a method for robustly expanding and improving the quality of optical survey
    data and provides a potential avenue for cross-band reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'galaxies: formation – galaxies: evolution – techniques: image processing –
    surveys – virtual observatory tools^†^†pubyear: 2020^†^†pagerange: Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping–[A](#A1
    "Appendix A Additional Image Samples ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analysis of optical data at a wide frequency range collected by various
    astronomical surveys is a critical component used to study the origin and evolution
    of galaxies. Data on galaxy shape (Wang et al., [2019](#bib.bib57)) and luminosity
    (Padmanabhan & Loeb, [2020](#bib.bib36); Cortese et al., [2017](#bib.bib12)) in
    various bands provide information about the evolution of galaxies at different
    cosmic times. As each band provides information about different characteristics
    of each object, stronger conclusions may be drawn from studies that incorporate
    data from a wide range of wavelengths. While a large range of optical wavelengths
    is covered by most modern surveys, such as the Dark Energy Survey (DES; Abbott
    et al., [2018](#bib.bib3)) and the Sloan Digital Sky Survey (SDSS; Abazajian et al.,
    [2009](#bib.bib2); Jiang et al., [2014a](#bib.bib23)), the depth, the footprint,
    and signal-to-noise ratio (S/N) varies from survey to survey. Inparticular, these
    will be vastly improved with future surveys like the Legacy Survey of Space and
    Time (LSST) (Ivezić et al., [2019](#bib.bib21)). As a result, feature extraction
    in a particular band may be difficult in certain regions due to incomplete field
    coverage by surveys with high-quality data within that band.
  prefs: []
  type: TYPE_NORMAL
- en: Prior Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to understand the underlying galaxy formation model and physics behind
    galaxy properties, simulations are required to mimic observations; however, their
    systematics are computationally expensive. Synthetic image generation of individual
    objects via deep learning is an alternative method for synthetic sky catalog generation
    that avoids the time and computational expense of other physically driven simulations.
    Various neural network architectures have been used for this purpose, including
    variational autoencoders (Regier et al., [2015a](#bib.bib42), [b](#bib.bib43);
    Lanusse et al., [2020](#bib.bib26); Spindler et al., [2020](#bib.bib48)) and generative
    adversarial networks (GANs) (Smith & Geach, [2019](#bib.bib47); Ullmo et al.,
    [2020](#bib.bib50)). While these methods efficiently generate mock galaxy images,
    the accuracy of the output images depends on that of the input images. As a result,
    their physical information is fundamentally limited by the quality of the survey
    data they are trained with.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to image generation, autoencoders of various types have been used
    for a number of purposes in astronomy, including anomaly detection (Villar et al.,
    [2020a](#bib.bib53)) and object classification (Ralph et al., [2019](#bib.bib41);
    Spindler et al., [2020](#bib.bib48); Villar et al., [2020b](#bib.bib54)). GANs
    have also been utilized for feature extraction (Shirasaki et al., [2019](#bib.bib46))
    and anomaly detection (Storey-Fisher et al., [2020](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: Two autoencoder architectures that are of particular importance for this work
    are convolutional autoencoders (CAEs) (Masci et al., [2011](#bib.bib31)) and denoising
    autoencoders (DAEs) (Vincent et al., [2008](#bib.bib55)). CAEs have been utilized
    in astronomy for purposes including classification/feature extraction (Cai et al.,
    [2020](#bib.bib9); Cheng et al., [2020](#bib.bib10); Patel & Upla, [2019](#bib.bib37))
    and anomaly detection (Storey-Fisher et al., [2020](#bib.bib49)). DAEs take as
    input an artificially corrupted input and are trained to reconstruct a distortion-free
    representation of that input. DAEs are primarily used to eliminate noise from
    images (Graff et al., [2014](#bib.bib16)) and data (Shen et al., [2017](#bib.bib45)),
    as well as for feature extraction (Frontera-Pons et al., [2017](#bib.bib15); Wang
    et al., [2020](#bib.bib58)).
  prefs: []
  type: TYPE_NORMAL
- en: One little explored alternative for improving the size and quality of survey
    datasets is through the use of feature transfer techniques across survey data.
    A feature transfer model is trained to recognize differences between features
    in corresponding image pairs $\mathcal{X}$ and $\mathcal{Y}$ from datasets $\mathbb{X}$
    and $\mathbb{Y}$. Using an image from $\mathcal{X}^{\prime}\in\mathbb{X}$ as input,
    the trained neural network can then be used to construct a representation of this
    image with the features characteristic of images in $\mathbb{Y}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of astronomy and astrophysics, feature transfer learning using
    conditional GANs has recently found application for data analysis and feature
    extraction. Moriwaki et al. ([2020](#bib.bib33)) developed a method to extract/reconstruct
    H$\alpha$ line intensity maps from noisy hydrodynamic simulation data. In addition,
    Shirasaki et al. ([2019](#bib.bib46)) used feature transfer techniques to extract
    information from weak lensing maps. However, other modified GAN architectures
    can be used for feature transfer learning; in particular, cycle-consistent generative
    adversarial networks (CycleGANs; these are described in Section [3](#S3 "3 Methodology
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")) are particularly suited for image analysis and generation. Developed
    by Zhu et al. ([2017](#bib.bib62)) and Isola et al. ([2016](#bib.bib20)), CycleGANs
    have been used for image-to-image translation (feature transfer between paired
    or unpaired sets of images) (Jia et al., [2020](#bib.bib22); Liu et al., [2020](#bib.bib28);
    Luo et al., [2021](#bib.bib29); Osakabe et al., [2020](#bib.bib35); Maziarka et al.,
    [2019](#bib.bib32)). However, there has been minimal exploration of generative
    models using feature transfer learning in astronomy and astrophysics.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Lin et al. ([2021](#bib.bib27)) used image-to-image translation to
    reconstruct high-frequency noise patterns characteristic to different astronomical
    surveys. The authors used several modified CycleGAN architectures with a semi-supervised
    training scheme using unpaired images to separate the signal and noise in images
    from two distinct surveys. A noise emulator is then used to reconstruct the noise
    patterns from each survey. The noise emulator can then be used to reconstruct
    images from a target dataset with said characteristic noise patterns. While several
    of their models were successful at emulating noise patterns, training using unpaired
    images hindered the reconstruction of small-scale features of the signal. Save
    for this work, the authors have been unable to identify any other use of CycleGANs
    in astronomy and astrophysics.
  prefs: []
  type: TYPE_NORMAL
- en: Methods other than feature transfer ones can hypothetically be used to generate
    representations of galaxies with altered parameters. In particular, fader networks
    (Lample et al., [2017](#bib.bib25); Perarnau et al., [2016](#bib.bib38)) have
    been used by Schawinski et al. ([2018](#bib.bib44)) for the purpose of testing
    hypotheses about mechanisms that drive galaxy formation. While this could be used
    as a method to transfer individual physical parameters of galaxies from one dataset
    to another, image reconstruction would not be feasible using this method because
    a large number of parameters must be known ab initio to generate faithful representations
    of images in the target dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We propose a novel method of feature transfer between galaxy surveys using CAEs
    and CycleGANs that can be used to expand galaxy image catalogs and can be adopted
    to multiple wavelengths and resolutions. By training these architectures with
    images from DES DR1 (Abbott et al., [2018](#bib.bib3)) paired with corresponding
    images from SDSS DR16 (Ahumada et al., [2020](#bib.bib5)), we demonstrate that
    information from DES images may be transferred to SDSS images, improving their
    S/N, contrast, and brightness. We show that the synthetic DES images reconstructed
    from SDSS images share the same characteristics as the true DES images, and that
    this consistency is retained when performing reconstructions using images from
    a separate set of lower quality SDSS images which do not have a counterpart in
    the DES catalog.
  prefs: []
  type: TYPE_NORMAL
- en: 'While other works have demonstrated that variational autoencoders (Regier et al.,
    [2015a](#bib.bib42), [b](#bib.bib43); Lanusse et al., [2020](#bib.bib26); Spindler
    et al., [2020](#bib.bib48)) and GANs (Smith & Geach, [2019](#bib.bib47); Ullmo
    et al., [2020](#bib.bib50)) are effective at generating realistic synthetic galaxy
    images and improving the S/N, these models both train and validate using images
    from the same dataset. Our method utilizes techniques that are generally similar
    to these; however, by using different data to train (SDSS) and validate (DES),
    we are able to generate false images that share the same morphological features
    of the SDSS images, but with a brightness and S/N more characteristic of the DES
    images. Like other generative models, this can be used to increase the size of
    survey datasets; however, this method generates false representations of real
    observed galaxies. This provides benefit when studying the properties of galaxies
    in a specific region that has not yet be covered by high quality surveys. More
    importantly, transfer learning may allow for cross-band reconstruction: all surveys
    cover a limited range of wavelengths at sufficiently high quality for effective
    analysis, making feature extraction from particular bands impossible in certain
    regions of the sky. By training using images with fewer bands than the validation
    data, a feature transfer-based generative model may be able to generate synthetic
    representations of galaxies with a greater range of wavelength bands than the
    input image. This provides a method to allow more thorough analysis of galaxies
    in regions that lack sufficient band coverage.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we demonstrate the creation of Survey2Survey, a neural network
    architecture used to transfer features between SDSS and DES galaxy images that
    can be easily generalized to other optical surveys or even across multiple wavelengths.
    The parameters of the SDSS and DES datasets used for training and validation are
    described in Section [2](#S2 "2 Data ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). In Section [3](#S3 "3 Methodology
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we detail the CAE and CycleGAN architectures used. In Section [4](#S4
    "4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), we present qualitative and quantitative metrics
    of the accuracy of the reconstructed image, then summarize our findings in Section
    [5](#S5 "5 Conclusions ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we describe the datasets used to carry out this study. We focused
    on optical data from the SDSS and DES surveys and the overlapping region in the
    Stripe82 (Jiang et al., [2014b](#bib.bib24)). All of the data used in this paper
    is publicly accessible via their respective websites.
  prefs: []
  type: TYPE_NORMAL
- en: All images consisted of three layers (one layer for each RGB channel), where
    the brightness of each pixel $P_{i}$ was represented by an 32-bit float, $0\leq
    P_{i}\leq 1$. Each SDSS image was 150 $\times$ 150 $\mathrm{\,pix}$, and each
    DES image was 228 $\times$ 228 $\mathrm{\,pix}$; the DES images were downscaled
    to match the dimensions of the SDSS images prior to training and reconstruction.
    After the reconstruction and prior to the analysis, each three-layer 150 $\times$
    150 $\mathrm{\,pix}$ image was reduced to a single layer by averaging over the
    RGB channels.
  prefs: []
  type: TYPE_NORMAL
- en: SDSS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SDSS images were captured by the Ritchey-Chrètien altitude-azimuth telescope
    (Gunn et al., [2006](#bib.bib17)), the Irènèe du Pont Telescope (Bowen & Vaughan,
    [1973](#bib.bib7)), and the NMSU 1-Meter Telescope (Holtzman et al., [2010](#bib.bib18)).
    We selected a sample of galaxies in Stripe82 that overlapped with the DES footprint,
    and randomly sampled data from outside that region and within the northern cap
    for a total of 25,000 galaxies. We chose galaxies with band Petrosian magnitude
    limits $14<R<17.77$, $z<0.25$ and a resolution of $0.396\mathrm{\,arcsec}/\!\mathrm{\,pix}$,
    using the galaxy flag produced by SDSS to select high confidence galaxy images.
    Images of these galaxies were obtained from the SDSS cutout server¹¹1http://casjobs.sdss.org/ImgCutoutDR7.
  prefs: []
  type: TYPE_NORMAL
- en: DES and Overlap Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DES uses the Dark Energy Camera (DECam; Flaugher et al., [2015](#bib.bib14))
    mounted at the Blanco 4m telescope at the Cerro Tololo Inter-American Observatory
    (CTIO) in Chile to observe ${\sim}\,5000\mathrm{\,deg}^{2}$ of the southern sky
    in the $g$, $r$, $i$, $z$, and $Y$ broadband filters ranging from ${\sim}\,400\mathrm{\,nm}$
    to ${\sim}\,1000\mathrm{\,nm}$ in wavelength.
  prefs: []
  type: TYPE_NORMAL
- en: We used images from the Dark Energy Survey DR1 release (Abbott et al., [2018](#bib.bib3)),
    which is comprised of over 10,000 co-added tiles of $0.534\mathrm{\,deg}^{2}$
    with a resolution of $0.263\mathrm{\,arcsec}/\mathrm{\,pix}$ and a depth reaching
    S/N ${\sim}\,10$ for extended objects up to $i_{AB}\,{\sim}\,23.1$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We selected DES galaxies using a combination of filtered criteria in terms
    of the concentration and error in the magnitude model as recommended²²2https://des.ncsa.illinois.edu/releases/dr1/dr1-faq
    with $g<17$ located in the Stripe 82 region (Jiang et al., [2014b](#bib.bib24))
    corresponding to roughly $300\mathrm{\,deg}^{2}$ near the celestial equator. We
    selected all images from Stripe 82 that have an SDSS counterpart (Abazajian et al.,
    [2009](#bib.bib2); Jiang et al., [2014a](#bib.bib23)). These images were obtained
    using the public DES cutout service³³3https://des.ncsa.illinois.edu/desaccess.
    We removed images with incomplete coverage and cleaned the images of anomalies
    and contaminants such as stars using visual inspection. Each DES image was scaled
    to 150 $\times$ 150 $\mathrm{\,pix}$ to match the resolution of the SDSS images.
    We aligned the orientation and central pixels of each DES/SDSS image pair, and
    the final RGB composite was generated using the Lupton et al. ([2004](#bib.bib30))
    prescription in order to closely match the SDSS colors. Figure [1](#S2.F1 "Figure
    1 ‣ DES and Overlap Region ‣ 2 Data ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") shows examples of the galaxies
    selected, where we can see that the DES images appear brighter and more detailed
    than the SDSS images.'
  prefs: []
  type: TYPE_NORMAL
- en: The overlap region was used for training and validation; each SDSS image in
    the overlap region had a DES counterpart. In total, there were 5,538 RGB images
    in the overlap region. 5,000 SDSS/DES image pairs were used for training the models,
    while the remaining 538 were used as the validation dataset. Because of the large
    variation in the brightness and spatial extent of objects in the SDSS and DES
    datasets, we chose to use a training dataset that was $\approx 5\times$ larger
    than those used by both (Isola et al., [2016](#bib.bib20)) and (Zhu et al., [2017](#bib.bib62)).
  prefs: []
  type: TYPE_NORMAL
- en: The external dataset, which consisted of 25,076 from outside of the Stripe 82
    region, was used to provide evidence for the robustness of our methodology. These
    images were fainter and of lower S/N than the training and validation datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26c4ee7e60106311e8a5c89ab0f5856f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Sample images used from the Dark Energy Survey (DES) (top row) and
    Sloan Digital Sky Survey (SDSS) (bottom row) datasets. More examples can be seen
    throughout the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Convolutional Autoencoders (CAE) (Masci et al., [2011](#bib.bib31)) and Cycle-Consistent
    Generative Adversarial Networks (CycleGAN) (Durugkar et al., [2016](#bib.bib13))
    were used to generate synthetic galaxy images from the SDSS input images. Since
    the images were scaled, rotated, and centered so that each pair of pixels in a
    given image pair corresponded with one another, minimizing the loss function used
    for both models corresponded with minimizing the pixel-to-pixel differences between
    the reconstructed image and the DES target image. These two types of models differ
    in their implementation and objective function as described below. We did not
    perform any methods that have traditionally been used to reduce overfitting and
    provide data augmentation, such as image rotations and translations, for either
    the CAE or CycleGAN. Spatial transformations would have likely led to failure:
    as we intend to perform pixel-to-pixel translations, any misalignment of pixels
    would lead to the creation of an invalid mapping function. While this may not
    cause an issue in many other cases, spatial transformations on SDSS and DES images
    could drastically reduce the accuracy of the mapping function given the small
    spatial extent of the signal region relative to the background in many images.
    While this may have led to overfitting, the analysis of the external reconstructions
    provides evidence of the robustness of our method. As an initial application of
    image-to-image translation for false image generation, we chose to minimize the
    number of factors that could affect the pixel-to-pixel map; future research should
    be dedicated to establishing methods to ensure that overfitting does not occur.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Convolutional Autoencoders (CAE)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An autoencoder is a neural network architecture typically used for classification
    that is comprised of an encoder/decoder pair. The encoder compresses data from
    an input image using one or more hidden layers to isolate important features from
    that image, generating a latent space representation of that image with lower
    dimensionality. The decoder uses the information in the latent space to reconstruct
    a representation of the input image. The autoencoder is trained to optimize a
    loss function to minimize the difference between the source and reconstructed
    image. A convolutional autoencoder performs encoding and decoding using convolution
    filters: during the encoding stage, convolution filters are used to extract information
    from and decrease the dimensionality of the input image. Additional convolution
    filters are used to map the latent space representation to a reconstruction of
    the input image. Training is performed by iteratively modifying the weights of
    the convolution filters to minimize the differences between the source image and
    its reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our CAE was implemented using Keras (Chollet et al., [2015](#bib.bib11)) using
    a Tensorflow (Abadi et al., [2015](#bib.bib1)) backend, and was run on a 32 GB
    Tesla V1000-PCIE GPU. Training over the course of 100 epochs (a value chosen via
    early stopping) took ${\sim}\,30$ minutes. Details about our architecture are
    shown in Table [1](#S3.T1 "Table 1 ‣ 3.1 Convolutional Autoencoders (CAE) ‣ 3
    Methodology ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"). We intentionally did not substantially decrease the dimensionality
    of the latent space of each layer because of the complexity of the images we aimed
    to reproduce. The SDSS images were generally less bright and noisier, and objects
    from the DES dataset often had a greater number of pixels distinguishable from
    the background noise (i.e. the signal in DES images had a larger spatial extent)
    than the SDSS images, so it is unlikely that a low-dimensionality latent space
    would be capable of producing sufficiently detailed false images.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stage | Output Shape | Activation | $\bm{N_{\textnormal{{P}}}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Input | 150 $\times$ 150 $\times$ 3 | N/A |'
  prefs: []
  type: TYPE_TB
- en: '| Encoder | 150 $\times$ 150 $\times$ 128 | ReLU | 3584 |'
  prefs: []
  type: TYPE_TB
- en: '| 150 $\times$ 150 $\times$ 64 | ReLU | 73792 |'
  prefs: []
  type: TYPE_TB
- en: '| 150 $\times$ 150 $\times$ 32 | ReLU | 18464 |'
  prefs: []
  type: TYPE_TB
- en: '| Decoder | 150 $\times$ 150 $\times$ 32 | ReLU | 9248 |'
  prefs: []
  type: TYPE_TB
- en: '| 150 $\times$ 150 $\times$ 64 | ReLU | 18496 |'
  prefs: []
  type: TYPE_TB
- en: '| 150 $\times$ 150 $\times$ 128 | ReLU | 73856 |'
  prefs: []
  type: TYPE_TB
- en: '| 150 $\times$ 150 $\times$ 3 | Sigmoid | 3459 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: CAE architecture used for image reconstruction. The initial input
    and final output images were 150 $\times$ 150 $\mathrm{\,pix}$ with 3 color channels;
    the output shape of each image in the encoder and decoder stages is length $\times$ width
    $\times$ no. filters. Each row in the encoder and decoder stages represents a
    single convolution layer with the specified activation function; convolution was
    performed using 3 $\times$ 3 kernel with a stride of 1 and zero padding. The image
    passed to the subsequent convolution layer had dimensions corresponding to that
    row’s output shape. $N_{\textnormal{P}}$ is the number of training features in
    that layer. The number of filters and activation functions used were chosen through
    manual tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: The RGB data from each image was separated into three layers, each of which
    were used to generate a unique set of filters. The encoder and decoder both consisted
    of three hidden layers, each of which filtered the image data from the previous
    layer using 150 3 $\times$ 3 $\mathrm{\,pix}$ convolution filters. These filters
    were initialized using randomly generated weights. Rectified Linear Unit (ReLU)
    activation functions were used for each layer of the encoder and decoder, and
    a sigmoid activation function was used during the final reconstruction phase.
    For each epoch, the input image $\textbf{{x}}_{0}$ was an image from the SDSS
    catalog, while the target image $\textbf{{x}}_{T}$ was the same object taken from
    the DES catalog. The difference between the reconstructed image $\textbf{{x}}^{\prime}_{0}$
    and the target image was calculated using the mean squared error loss function
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}\left(\textbf{{x}}^{\prime}_{0},\textbf{{x}}_{T}\right)$
    | $\displaystyle=\left\lVert\textbf{{x}}_{T}-\textbf{{x}}^{\prime}_{0}\right\rVert$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: The Adadelta (Zeiler, [2012](#bib.bib60)) optimizer was used to determine filter
    weights. At the conclusion of 100 training epochs, the trained algorithm was used
    to reconstruct the DES validation images from their corresponding SDSS image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Cycle-Consistent Generative Adversarial Networks (CycleGAN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4afbfe47a6065e497c4acad885773cc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A representation of the architecture of a CycleGAN. 1. A false image
    representation $\hat{x}$ is generated from $y$, a member of the target dataset,
    via the mapping function $F$. 2. $\hat{x}$ is mapped to a false image $\hat{y}$
    via the mapping function $G$. 3. The GAN loss function $\mathcal{L}_{\textnormal{GAN}}^{X}$
    for discriminator $D_{\mathcal{X}}$ is calculated by comparing $x$ and $\hat{x}$.
    4. The backward cycle-consistency loss function $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{b}}$
    is calculated by comparing $\hat{y}$ to the true target image $y$ (the forward
    cycle-consistency loss function $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    is calculated similarly using $\hat{x}$ and $x$). In our case, to calculate $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$,
    we generate a DES representation of an SDSS image $x$, then use $F$ to generate
    a false SDSS representation of that image. $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    quantifies the differences between the source SDSS image and the false SDSS image;
    its combination with $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{b}}$ quantifies
    the error accumulated when the SDSS image completes a full “cycle” between the
    SDSS and DES image spaces ($\mathbb{X}\to\mathbb{Y}\to\mathbb{X}$). 5. These loss
    functions are combined with $\mathcal{L}_{\textnormal{GAN}}^{Y}$ and $\mathcal{L}_{\textnormal{cyc}}^{\textnormal{f}}$
    to calculate the total loss function $\mathcal{L}$. $F$ and $G$ are then updated
    to minimize $\mathcal{L}$. This process is repeated to optimize the neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: A Generative Adversarial Network (GAN) (Durugkar et al., [2016](#bib.bib13))
    is an unsupervised or semi-supervised generative model consisting of a generator
    $G$ and discriminator $D$. $D$ is trained to distinguish between images from a
    training dataset of “true” images ($\mathcal{Y}$) and those generated by sampling
    from the latent space of $G$ ($\mathcal{X}$). Backpropagation of error from $D$
    is used to generate a map $g:\mathcal{X}\to\mathcal{Y}$ from the latent space
    of $G$ to the “true” image dataset by minimizing a loss function $\mathcal{L}(G,D,\mathcal{X},\mathcal{Y})$.
    After training, the GAN may be used to generate false images that replicate the
    features of $\mathcal{Y}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CycleGAN (Zhu et al., [2017](#bib.bib62); Isola et al., [2016](#bib.bib20))
    is a variation of a traditional GAN that minimizes cycle-consistency loss through
    the additional of a second generator/discriminator pair; a diagram of this architecture
    is shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Cycle-Consistent Generative Adversarial
    Networks (CycleGAN) ‣ 3 Methodology ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). Images from $\mathcal{X}$ ($\mathcal{Y}$)
    are used to train discriminators $D_{\mathcal{X}}$ ($D_{\mathcal{Y}}$). The generators
    $F:\mathcal{X}\to\mathcal{Y}$ and $G:\mathcal{Y}\to\mathcal{X}$ are trained to
    extremize the adversarial loss function $\mathcal{L}(H,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})$
    for generator $G$, discriminator $D_{\mathcal{Y}}$, and datasets $\mathcal{X}$
    and $\mathcal{Y}$. For the purposes of this project, we chose to use the loss
    function used by Zhu et al. ([2017](#bib.bib62)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})\kern
    2.0pt=\kern 2.0pt$ | $\displaystyle\mathbb{E}_{\kern 0.5pty\sim p_{\textnormal{data}}(y)}\left[\kern
    0.5pt\log D_{\mathcal{Y}}(y)\kern 0.5pt\right]+$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbb{E}_{\kern 0.5ptx\sim p_{\textnormal{data}}(X)}\left[\kern
    0.5pt\log(1-D_{\mathcal{Y}}(G(x))\kern 0.5pt\right]$ |  |'
  prefs: []
  type: TYPE_TB
- en: for images $x\in\mathcal{X}$ and $y\in\mathcal{Y}$, where $p_{\textnormal{data}}$
    is the true data distribution. $G$ was trained to maximize $\mathcal{L}_{\textnormal{GAN}}$
    ($\max_{G}\max_{D_{\mathcal{Y}}}\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})$),
    while $F$ was trained to minimize it ($\min_{F}\max_{D_{\mathcal{X}}}\mathcal{L}_{\textnormal{GAN}}(F,D_{\mathcal{X}},\mathcal{Y},\mathcal{X})$).
  prefs: []
  type: TYPE_NORMAL
- en: To constrain the space of possible mapping functions, a CycleGAN optimizes $F$
    and $G$ by minimizing the forward and backward cycle consistency error. For images
    $x\in\mathcal{X}$ and $y\in\mathcal{Y}$, let $x^{\prime}=F(G(x))$ and $y^{\prime}=G(F(y))$.
    Forward cycle consistency is achieved when the difference between $x^{\prime}$
    and $x$ is minimized (i.e. $F=G^{-1}+\varepsilon_{x}$ for some small error $\varepsilon_{x}$),
    indicating that the full translation cycle beginning in $\mathcal{X}$ reproduces
    a close approximation of $x$; backward cycle consistency is defined identically
    for images $y\in\mathcal{Y}$ and $G=F^{-1}+\varepsilon_{y}$ for some small error
    $\varepsilon_{y}$. An optimized CycleGAN will simultaneously minimize the forward
    and backward cycle-consistency error; this is equivalent to ensuring that $F$
    and $G$ are bijective inverses of one another, limiting the size of the set of
    possible mapping functions. This improves the robustness of the neural network
    and decreases the amount of training required relative to many other GAN variations.
  prefs: []
  type: TYPE_NORMAL
- en: We note that for our particular case we used the architecture described in Isola
    et al. ([2016](#bib.bib20)) which is adapted from the unsupervised representation
    learning GAN architecture introduced in Radford et al. ([2016](#bib.bib40)). In
    particular, we highlight the use of a generator with skips and a Markovian discriminator.
    These additions helped with the translation process and limited the GAN discriminator
    to high-frequency structures, reducing the potential for artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: The cycle-consistency loss function $\mathcal{L}_{\textnormal{cyc}}(G,F)$ we
    used is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\textnormal{cyc}}(G,F)=\kern 3.0pt$ | $\displaystyle\mathbb{E}_{x\sim
    p_{\textnormal{data}}(x)}\left[\kern 0.5pt\left\lvert F(G(x))-x\right\rvert_{1}\right]+$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbb{E}_{y\sim p_{\textnormal{data}}(y)}\left[\kern
    0.5pt\left\lvert G(F(y))-y\right\rvert_{1}\right],$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\left\lvert A-B\right\rvert_{1}=\sum\limits_{i}\hskip 2.0pt\left\lvert
    A_{i}-B_{i}\right\rvert$ is the pixel-to-pixel $L^{1}$-norm between images $A$
    (SDSS) and $B$ (DES).
  prefs: []
  type: TYPE_NORMAL
- en: The full loss function used for training $F$ and $G$ was
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}(G,F,D_{\mathcal{X}},D_{\mathcal{Y}})\kern 2.0pt=\kern
    2.0pt$ | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(G,D_{\mathcal{Y}},\mathcal{X},\mathcal{Y})\kern
    2.0pt+$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathcal{L}_{\textnormal{GAN}}(F,D_{\mathcal{X}},\mathcal{Y},\mathcal{X})\kern
    2.0pt+$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\lambda\kern 0.5pt\mathcal{L}_{\textnormal{cyc}}(G,F)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: for some parameter $\lambda$, which describes the relative importance of the
    optimization of the adversarial and cycle consistency errors. For this work, we
    set $\lambda=0.2$.
  prefs: []
  type: TYPE_NORMAL
- en: Image translation using a CycleGAN architecture provides benefit over a traditional
    GAN by constraining the allowed mapping functions by ensuring that the discriminator
    pair $F$ and $G$ are inverses. This benefits the translation between noisy images
    by making sure that the differences in noise patterns in $\mathcal{X}$ and $\mathcal{Y}$
    is taken into account, helping distinguish between the signal and noise more easily
    after training on many images.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we demonstrate that we can transfer information from DES images to their
    SDSS counterparts, generating synthetic images that are brighter, of higher quality,
    and have less noise, yet retain the morphological information contained within
    the source image. We begin with a qualitative analysis to understand properties
    of the reconstructed images, then quantify the brightness and noise level of the
    image datasets. We then use correlations between the light profiles of the source
    and reconstructed objects to establish the small-scale differences between the
    datasets. Finally, we combine this information with comparative quality assessments
    to establish that the image reconstruction process improves the image quality,
    brightens objects, and reduces background noise. We provide evidence for the robustness
    of the reconstruction process by comparing the statistics of the validation and
    external datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Qualitative Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6050b54db1b0d18498f75da7caaccaca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of galaxy images from the validation dataset (from the Stripe82
    region). Each column shows an SDSS galaxy (row A), its DES counterpart (row B),
    and the DES image reconstruction by the CAE (row C) and CycleGAN (row D) methods.
    CAE and CycleGAN residuals (reconstruction - DES) are shown in rows E and F respectively,
    wile the CAE and CycleGAN pixel-to-pixel brightness increases (reconstruction
    - SDSS) are shown in rows G and H, respectively. Note that to increase visibility,
    images in rows E, F, G and H were artificially enhanced with a power law transform
    ($P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$ for each pixel $P_{i}$). In rows
    E and F, $\gamma=0.3$, while in rows G and H, $\gamma=0.5$. Additional galaxy
    samples can be found in Appendix [A](#A1 "Appendix A Additional Image Samples
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e88870814ca1b03dbd0887e0dc20ea83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Examples of galaxy images from the external dataset (from outside
    of the Stripe82 overlap region). Each column shows an SDSS galaxy (row A) and
    its DES image reconstruction by the CAE (row B) and CycleGAN (row C) methods.
    The CAE and CycleGAN pixel-to-pixel brightness increases (reconstruction - SDSS)
    are shown in rows D and E, respectively. Note that to increase visibility, images
    in rows D and E were artificially enhanced with a power law transform ($P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$
    for each pixel $P_{i}$, where $\gamma=0.5$). Additional galaxy samples can be
    found in Appendix [A](#A1 "Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") shows several examples of false images generated by the neural networks
    paired with their corresponding SDSS and DES images from the overlap region. These
    images were selected to demonstrate the wide variety of galaxy types and structures
    included in the validation sample which were not including during the training.
    Row A contains images from the SDSS catalog; the corresponding DES images are
    located in row B. Rows C and D contain the reconstructed CAE and CycleGAN images,
    respectively. We can observe that the DES images and the synthetic images in rows
    C and D are remarkably similar, where the small differences come from the the
    lack of structure resolution of the reconstructed objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitatively, the reconstructed images are generally blurrier than the corresponding
    DES images. However, images reconstructed by both the models are generally brighter
    than their SDSS counterparts. In addition, the false images, particularly the
    those generated by the CAE, are often less noisy than their SDSS and/or DES counterparts.
    Image residuals for the CAE and CycleGAN reconstructions are shown in rows E and
    F, respectively. These show the pixel-to-pixel brightness differences between
    the reconstructed and DES images; note that these images were artificially enhanced
    using a power law transform ($P_{i}\,\to\,{P_{i}}^{\prime}=P_{i}^{\gamma}$ for
    each pixel $P_{i}$; in rows E and F, $\gamma=0.3$, while in rows G and H, $\gamma=0.5$).
    This was done so that the residual structure was visible. It appears that both
    neural networks isolated and enhanced the galaxy signal while affecting the background
    minimally or try to reduce the noise. Both networks were also able to distinguish
    between separate structures in each image; this is particularly evident in the
    second column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rows G and H show the pixel-to-pixel brightness increase provided by the CAE
    and CycleGAN reconstructions relative to the corresponding SDSS galaxies, respectively.
    Qualitatively, the CAE reconstructions are brighter than the CycleGAN images,
    and provided greater amplification to the internal structure of each galaxy. Interestingly,
    both networks consistently amplified the galaxy center more than other regions.
    This amplification was not exclusive to the central galaxy; rather, it was present
    in most regions the network identified as a signal region. Other example galaxies
    are included in Appendix [A](#A1 "Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") shows examples of images from the external dataset (from outside of
    the Stripe82 region). These images are generally of lower quality; however, both
    reconstruction models succeeded in selecting and amplifying the objects of interest
    with little effect on the background, even maintaining much of the small-scale
    detail of the images (particularly in the fourth and seventh columns). As in Fig.
    [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"), the
    reconstructions generally increased the spatial extent of objects in the image.
    Notably, the CAE reconstruction appears to have removed an artifact from the SDSS
    image in the final column; this phenomenon is discussed in greater detail in Section
    [4.4](#S4.SS4 "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Dataset Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we quantify the brightness and quality of images from each dataset to
    use as baseline comparison metrics between the original input SDSS images, the
    DES target images, and the reconstructions.
  prefs: []
  type: TYPE_NORMAL
- en: Pseudo-Flux Magnitude
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this work we have used the RGB images from SDSS and DES to test our architectures.
    Image brightness was quantified using the average pseudo-flux magnitude $F$ of
    each image. We refer to $F$ as the “pseudo-flux magnitude” because, while $F$
    does not represent the physical flux magnitude (our images consisted solely of
    (r, g, b) channel pixel values), it acts as a proxy for this quantity due to the
    similarities between the two measurements. The pseudo-flux magnitude $F$ was defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle F$ | $\displaystyle=30-2.5\log\left(\sum_{{\hskip 6.0ptr_{i}\,<\,r_{\textnormal{max}}}}\beta_{i}\hskip
    2.0pt\right)$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=30-2.5\log\,\beta^{\circ}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, the pixel brightness $\beta_{i}$ describes the average of the red, green,
    and blue channel values in $P_{i}$ and $\beta^{\circ}$ is the total pixel brightness
    contained within an aperture of radius $r_{\textnormal{max}}=75\mathrm{\,pix}$.
    A constant factor (zero point) of 30 was added to approximate the appearance of
    a physical magnitude distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaussian kernel density estimates (KDEs) for histograms of the pseudo-flux
    magnitudes are shown in Fig. [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2
    Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). The first, second, and third
    quartiles were used as a conservative estimate of the spread of the distribution
    data; this was chosen due to the heavy skew of the distributions of the external
    data. However, they cannot be used to determine whether there was a significant
    difference between the S/N of different datasets. This is because the differences
    in pseudo-flux magnitude must be evaluated on an image-to-image basis, not by
    the relative frequency of each S/N value. The pseudo-flux magnitude values for
    the reconstructions in the validation datasets were comparable to those of the
    DES dataset, showing an improvement in the brightness relative to the SDSS dataset.
    In the external dataset, the pseudo-flux magnitude distributions for both reconstructions
    were shifted to the left of the SDSS distribution, indicating that the reconstruction
    process successfully increased the brightness of images from the external dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: To quantify the image-to-image brightnesses and provide an error estimate, define
    the mean relative flux difference $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$ between datasets $i$ and $j$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}=\frac{1}{N_{\textnormal{img}}}\sum_{m}^{N_{\textnormal{img}}}\frac{F_{m}^{\kern
    1.0ptj}-F_{m}^{\kern 1.0pti}}{F_{m}^{\kern 1.0pti}},$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $F_{m}^{\kern 1.0pti}$ and $F_{m}^{\kern 1.0ptj}$ are the pseudo-flux
    magnitudes of corresponding images from the $N_{\textnormal{img}}$ image in datasets
    $\mathbb{X}_{i}$ and $\mathbb{X}_{j}$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The values of $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$for the external and validation datasets are shown in Table [2](#S4.T2
    "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") (scaled by a factor of $10^{3}$ to enhance readability); the error was
    estimated using the standard deviation of $\frac{F_{m}^{\kern 1.0ptj}-F_{m}^{\kern
    1.0pti}}{F_{m}^{\kern 1.0pti}}$. Quartiles were used to estimate the error of
    the pseudo-flux magnitude plot (Figure [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping")) due to the clear skew
    of the data, so the variance would not provide an adequate representation of the
    spread of the data. However, the distribution of $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$was more symmetric than those of $F$, allowing the use of the standard
    deviation as an estimate of error.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The only dataset pair in which there was not a significant difference between
    the fluxes was for CycleGAN vs. DES. This implies that the CycleGAN reconstructions
    decreased the flux of the SDSS images to match that of the DES images. It should
    be noted that the results from this table seem to be in conflict with those from
    Figure [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣
    4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"). This is not unexpected: Figure [5](#S4.F5 "Figure
    5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") show the distribution of the relative frequency of individual pseudo-flux
    magnitudes, while the values in Table [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping") show an image-to-image
    comparison of the pseudo-flux magnitudes. Hence, the values of Table [2](#S4.T2
    "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") provide an appropriate measure of the differences in the pseudo-flux
    magnitudes of the reconstructions relative to their source images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notably, there was not a statistically significant difference between the values
    of $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CAE}$ in the validation
    and external datasets; the same is also true for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS,
    CycleGAN}$. Hence, the decrease in flux provided by the reconstructions were similar
    for both the validation and external datasets. This provides evidence for the
    robustness of our method: the increase in the brightnesses of the false images
    was the same regardless of the brightnesses of the input images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa3ecd8ea6c17321b76892e62fc07187.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Top: Pseudo-flux magnitudes (defined in Eqn. ([5](#S4.E5 "In Pseudo-Flux
    Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping"))) for
    the validation and external data. Bottom: The first, second, and third quartiles
    of the corresponding datasets, providing a measure of spread. SDSS images tended
    to be fainter than the DES and reconstructed images. Note that the quartiles cannot
    be used as a measure of error/statistical significance because this plot does
    not provide a representation of the image-to-image differences in $F$; this is
    discussed in greater detail in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}\times 10^{3}$ | External | Validation |'
  prefs: []
  type: TYPE_TB
- en: '| <svg version="1.1" height="25.53" width="150" overflow="visible"><g transform="translate(0,25.53)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,13.52)
    scale(1, -1)"><foreignobject width="24.16" height="13.52" overflow="visible">$\mathbb{X}_{j}$</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(124.04,13.52)"><g transform="translate(0,12.01)
    scale(1, -1)"><foreignobject width="25.96" height="12.01" overflow="visible">$\mathbb{X}_{i}$</foreignobject></g></g></g></svg>
    | SDSS | SDSS | DES |'
  prefs: []
  type: TYPE_TB
- en: '| CAE | $-40.65\pm 5.59$ | $-41.90\pm 5.54$ | $-9.36\pm 6.72$ |'
  prefs: []
  type: TYPE_TB
- en: '| CycleGAN | $-30.44\pm 7.28$ | $-29.33\pm 6.28$ | $\mathit{3.65\pm 8.22}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| DES | N/A | $-32.80\pm 9.13$ | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The mean proportional difference $\Delta\hbox to0.0pt{\hskip 0.76389pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{F}$}}^{\kern
    1.0ptij}$(scaled by a factor of $10^{3}$) in the pseudo-flux magnitudes (defined
    in Eq. ([6](#S4.E6 "In Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"))) between each of the image sets; the standard deviation was used
    to estimate the error. The only dataset pair that does not show a significant
    difference in $F$ is CycleGAN vs. DES.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7db6a19f80ed6e83161246cdf695d17a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Top: KDEs of the histograms of the mean S/N (defined in Eqn. ([7](#S4.E7
    "In Signal-to-Noise Ratio ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"))) for the validation and external data. Bottom: The first, second, and
    third quartiles of the distributions. Both reconstruction models were effective
    at increasing the S/N. Note that, as described in Figure [5](#S4.F5 "Figure 5
    ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"), the
    quartile bars cannot be used to determine statistical significance; see the text
    for further explanation.'
  prefs: []
  type: TYPE_NORMAL
- en: Signal-to-Noise Ratio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As metric for image quality, we measured the average signal-to-noise ratio (S/N)
    of images in each dataset. We define the mean S/N as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | S/N | $\displaystyle=\frac{\mu^{\circ}_{\beta}}{\sigma^{\circ}_{\beta}},$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mu^{\circ}_{\beta}$ $\left(\sigma^{\circ}_{\beta}\right)$ is the mean
    (standard deviation) of the pixel brightness $\beta$ for pixels within a radius
    of $r_{\textnormal{max}}=75\mathrm{\,pix}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Fig. [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), we show KDEs for histograms of the mean S/N,
    along with the first, second, and third quartiles, which are used as a measure
    of spread; however, as discussed in the analysis of Table [2](#S4.T2 "Table 2
    ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"), they
    cannot be used as a measure of error. On average, both reconstruction models were
    effective at boosting the S/N relative to the SDSS images, and the S/N for the
    CycleGAN reconstructions nearly matched that of the DES images. Denoising autoencoders
    have been used to reduce the amount of noise in images (Vincent et al., [2008](#bib.bib55)),
    so it is not surprising that the S/N in CAE images was greater than that of the
    target images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [3](#S4.T3 "Table 3 ‣ Signal-to-Noise Ratio ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), we list the mean proportional differences in
    the signal-to-noise ratios between image sets $i$ and $j$ to summarize the results
    from Figure [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"). As in Eq. ([6](#S4.E6 "In Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping")), we define the mean
    proportional difference between image set $i$ and $j$ as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}=\frac{1}{N_{\textnormal{img}}}\sum_{m}^{N_{\textnormal{img}}}\frac{\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0ptj}-\left[\textnormal{S/N}\right]_{m}^{\kern 1.0pti}}{\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0pti}}$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\left[\textnormal{S/N}\right]_{m}^{\kern 1.0pti}$ and $\left[\textnormal{S/N}\right]_{m}^{\kern
    1.0ptj}$ are the signal-to-noise ratios (as defined in Eqn. ([7](#S4.E7 "In Signal-to-Noise
    Ratio ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) of corresponding
    image pairs from among the $N_{\textnormal{img}}$ images in datasets $\mathbb{X}_{i}$
    and $\mathbb{X}_{j}$, respectively. In Table [3](#S4.T3 "Table 3 ‣ Signal-to-Noise
    Ratio ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"), we can see
    that, for the validation dataset, the CycleGAN reconstructions did not provide
    a significant increase in the S/N relative to the DES images ($\Delta\hbox to0.0pt{\hskip
    0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}-\sigma<0<\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$ for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CycleGAN}$, where
    $\sigma$ is the standard deviation of $\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$). However, the CAE reconstructions did provide a significant increase
    over the DES images. The second and third columns from the left ($\mathbb{X}_{i}=\textnormal{SDSS}$
    for the external and validation data, respectively) indicate that the reconstructions
    provided a significant increase in the S/N of their corresponding SDSS images.
    Moreover, there was not a significant difference between the value of $\Delta\hbox
    to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS, CAE}$ in the validation
    and external datasets; this relationship is the same for $\mathbb{X}_{i},\mathbb{X}_{j}=\textnormal{SDSS,
    CycleGAN}$. This implies that image reconstruction via feature translation using
    our architectures provides a robust method to generate false galaxy images that
    share the same S/N as DES galaxies in this study.'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\Delta\hbox to0.0pt{\hskip 0.27777pt\leavevmode\hbox{\set@color$\overline{\hbox{}}$}\hss}{\leavevmode\hbox{\set@color$\mathcal{S}$}}^{\kern
    1.0ptij}$ | External | Validation |'
  prefs: []
  type: TYPE_TB
- en: '| <svg version="1.1" height="25.53" width="150" overflow="visible"><g transform="translate(0,25.53)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,13.52)
    scale(1, -1)"><foreignobject width="24.16" height="13.52" overflow="visible">$\mathbb{X}_{j}$</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(124.04,13.52)"><g transform="translate(0,12.01)
    scale(1, -1)"><foreignobject width="25.96" height="12.01" overflow="visible">$\mathbb{X}_{i}$</foreignobject></g></g></g></svg>
    | SDSS | SDSS | DES |'
  prefs: []
  type: TYPE_TB
- en: '| CAE | $5.138\pm 2.719$ | $2.893\pm 1.819$ | $0.710\pm 0.317$ |'
  prefs: []
  type: TYPE_TB
- en: '| CycleGAN | $2.067\pm 1.001$ | $1.334\pm 0.716$ | $\mathit{0.069\pm 0.125}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| DES | N/A | $1.224\pm 0.779$ | N/A |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: The mean proportional difference in the signal-to-noise ratios (see
    Eqn. ([8](#S4.E8 "In Signal-to-Noise Ratio ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"))) between each of the image sets; the standard deviation was used
    to estimate the error. As in Table [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping"), the only dataset pair
    that does not show a significant difference in S/N is CycleGAN vs. DES.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Pseudo-Luminosity Profile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/827d79fd9a6d229751fb897377e94e40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Pseudo-luminosity profiles $\frac{dF}{dS}$ for the validation (left)
    and external (right) data; $\frac{dF}{dS}$ is defined in Eqn. ([9](#S4.E9 "In
    4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping")). The solid
    line represents $\frac{dF}{dS}$, while the dotted lines show $\frac{dF}{dS}\pm\hat{\sigma}$,
    where $\hat{\sigma}$ is the sample standard deviation. There was no statistically
    significant difference between the pseudo-luminosity profiles for any dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we showed that the image reconstructions provided a significant increase
    in the pseudo-flux magnitude and S/N of their corresponding SDSS images that matched
    that of the DES images. We also demonstrate that the improvement in $F$ and S/N
    were not heavily dependent on the dataset from which the source image was taken,
    providing evidence for the robustness of our method. Now, we will compare the
    pseudo-luminosity profiles of the objects in these images to characterize the
    structure of the objects themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: The pseudo-luminosity profile $\frac{dF}{dS}$, which is analogous to the luminosity
    profile in observed data, is defined by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{dF(r)}{dS}$ | $\displaystyle=\frac{1}{2\pi r{\Delta
    r}}\sum_{\hskip 4.0ptr_{i}\in\textnormal{Ann}(r)}F_{i}$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{F_{\textnormal{Ann}(r)}}{2\pi r{\Delta r}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $F_{\textnormal{Ann}(r)}$ is the total flux contained within an annulus-shaped
    aperture $\textnormal{Ann}(r)$ with central radius $r$ and area $S=2\pi r{\Delta
    r}$, where ${\Delta r}=1\mathrm{\,pix}$
  prefs: []
  type: TYPE_NORMAL
- en: 'Plots of the pseudo-luminosity profile for the validation and external datasets
    are shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.3 Pseudo-Luminosity Profile ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"). Bootstrapping was used to estimate the sample variance $\hat{\sigma}^{2}$;
    the dotted lines represent $\frac{dF}{dS}\pm\hat{\sigma}$. $\hat{\sigma}^{2}$
    was estimated by resampling each dataset 1000 times; for the validation dataset,
    the sample size was 50, while for the external dataset, the sample size was 2500\.
    In both the external and validation datasets, there was no significant difference
    between the pseudo-luminosity profiles of any image datasets, and from the pseudo-flux
    magnitude results (Figure [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")), we know that the reconstructions
    were generally brighter than their SDSS counterparts. This implies that the reconstructions
    improved the brightness quality of the SDSS images without losing information
    about the object’s brightness profile distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Image Quality Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef20c93e639fb70d1dd1296ed867b2f3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Top: Mean luminance index $\bar{\ell}$ (defined in Eqn. ([4.4](#S4.Ex7
    "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. $\bar{\ell}$ describes the similarities in brightness between
    two images at small scales ($\sim 10\mathrm{\,pix}$). The robustness of the method
    is indicated by the similarities in the validation and external distributions
    in the left-hand plot. In the right-hand plot, both reconstruction models increased
    $\bar{\ell}$ by a similar amount, indicating that, at small scales, the brightness
    increase provided by the two models were similar. This supports the conclusions
    drawn from the pseudo-flux magnitude in Fig. [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux
    Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping") and Table
    [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"). Note that unlike in Figures [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux
    Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping") and [6](#S4.F6
    "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), the quartiles can be used to determine statistical significance because
    $\ell$ calculations provide direct image-to-image comparisons.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/474df107226942783921e50f55420156.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Top: Mean contrast index $\bar{c}$ (defined in Eqn. ([4.4](#S4.Ex7
    "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. $\bar{c}$ describes the relative sharpness of two images at
    small scales ($\sim 10\mathrm{\,pix}$). The robustness of the method is indicated
    by the similarities in the validation and external distributions in the left-hand
    plot. In the right-hand plot, $\bar{c}$ was generally lower for the CAE reconstructions
    than for the CycleGAN reconstructions, implying that the CAE images were generally
    blurrier than the CycleGAN images. This confirms the qualitative observations
    about the images described in Section [4.1](#S4.SS1 "4.1 Qualitative Analysis
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping") (see Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")). Note that unlike in Figures
    [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping") and [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"), the quartiles can be used to
    determine statistical significance because $c$ calculations provide direct image-to-image
    comparisons.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad45a2cb30553c8ec3db5849829568b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Top: Mean cross-correlation index $\bar{s}$ (defined in Eqn. ([4.4](#S4.Ex7
    "4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. $\bar{s}$ describes the similarities between the structure
    of two images at small scales ($\sim 10\mathrm{\,pix}$), providing a measure of
    the faithfulness of the reconstruction. The robustness of the method is indicated
    by the similarities in the validation and external distributions in the left-hand
    plot. In the right-hand plot, $\bar{s}$ was generally lower for the CycleGAN reconstructions
    than for the CAE reconstructions, implying that the CAE architecture more accurately
    recreated small-scale details of the DES images, providing a more accurate reconstruction
    of the morphological properties of the image. Note that unlike in Figures [5](#S4.F5
    "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") and [6](#S4.F6 "Figure 6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), the quartiles can be used to determine statistical
    significance because $s$ calculations provide direct image-to-image comparisons.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d36018af3c0d40382433e6ee08cc5e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Top: Mean structural similarity index (defined in Eqn. ([12](#S4.E12
    "In 4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"))) for the
    validation and external data. Bottom: The first, second, and third quartiles of
    each distribution. The MSSIM, which is the mean of the product of $\ell$, $c$,
    and $s$, provides a metric for the overall relative image quality. The robustness
    of the method is indicated by the similarities in the validation and external
    distributions in the left-hand plot. From the right-hand plot, we can see that
    the overall quality of the CAE images was similar to that of the DES images, while
    the quality of the CycleGAN reconstructions was further removed from that of the
    DES images. Note that unlike in Figures [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude
    ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping") and [6](#S4.F6 "Figure
    6 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results and Analysis ‣
    Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), the quartiles can be used to determine statistical significance because
    MSSIM calculations provide direct image-to-image comparisons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), the brightnesses and S/N of the reconstructions were greater than or
    not significantly different from those of the DES images, and in [4.3](#S4.SS3
    "4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis ‣ Survey2Survey: A deep
    learning generative model approach for cross-survey image mapping"), we show that
    the brightness increase provided by the reconstruction has little effect on the
    radial profiles of the objects. Now, we will characterize how effective each reconstruction
    model is at amplifying the image signal, reducing background noise, improving
    image quality, and retaining the morphological information contained within the
    original image. We also highlight several notable images from the external dataset
    that show that CAE reconstructions may help remove image artifact.'
  prefs: []
  type: TYPE_NORMAL
- en: The mean structural similarity index (MSSIM) (Zhou Wang et al., [2004](#bib.bib61))
    is a method used to compare image quality that takes into account differences
    in brightness, sharpness, and small-scale features. The MSSIM is defined by the
    product of the luminance index $\ell$, contrast index $c$, and cross-correlation
    index $s$. For a pair of images X and Y, where each respective entry $\textbf{{X}}_{ij}$
    and $\textbf{{Y}}_{ij}$ is the pixel brightness $\beta_{ij}$ of pixel $P_{ij}$,
    let $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$ be an 11 $\times$ 11
    window centered around pixel $x_{ij}$ $\left(y_{ij}\right)$. After smoothing $\textbf{{x}}_{ij}$
    $\left(\textbf{{y}}_{ij}\right)$ by an 11-tap Gaussian filter, define $\ell$,
    $c$, and $s$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\ell\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ |
    $\displaystyle=\frac{2\mu_{x}\mu_{y}+C_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+C_{1}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle c\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ | $\displaystyle=\frac{2\sigma_{x}\sigma_{y}+C_{2}}{\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}},$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$ | $\displaystyle=\frac{2\sigma_{xy}+C_{2}}{2\sigma_{x}\sigma_{y}+C_{2}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then structural similarity index SSIM can be calculated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textnormal{SSIM}\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$
    | $\displaystyle=\ell\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)c\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)s\left(\textbf{{x}}_{ij},\textbf{{y}}_{ij}\right)$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle=\frac{\left(2\mu_{x}\mu_{y}+C_{1}\right)\left(2\sigma_{xy}+C_{2}\right)}{\left(\mu_{x}^{2}+\mu_{y}^{2}+C_{1}\right)\left(\sigma_{x}^{2}+\sigma_{y}^{2}+C_{2}\right)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Here, $\mu_{x}$ ($\mu_{y}$) is the mean of $\textbf{{x}}_{ij}$ $\left(\textbf{{y}}_{ij}\right)$,
    $\sigma_{x}^{2}$ $\left(\sigma_{y}^{2}\right)$ is the variance of $\textbf{{x}}_{ij}$
    $\left(\textbf{{y}}_{ij}\right)$, $\sigma_{xy}^{2}$ is the covariance, $C_{1}=(0.01R_{D})^{2}$,
    and $C_{2}=(0.03R_{D})^{2}$ are stabilization constants for which $R_{D}$ is the
    dynamic range of the image (in our case, $R_{D}=1$). Then the MSSIM is defined
    by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\textnormal{MSSIM}=\frac{1}{N_{P}^{2}}\sum_{i,j}^{N_{P}}\textnormal{SSIM}\left(\textbf{{x}}_{i,j},\textbf{{y}}_{i,j}\right)$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: and the mean luminance, contrast, and cross-correlation indices ($\bar{\ell}$,
    $\bar{c}$, and $\bar{s}$, respectively) are defined similarly.
  prefs: []
  type: TYPE_NORMAL
- en: 'KDEs of histograms for $\bar{\ell}$, $\bar{c}$, $\bar{s}$, and MSSIM for the
    overlap and external data are shown in Figures [8](#S4.F8 "Figure 8 ‣ 4.4 Image
    Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"), [9](#S4.F9 "Figure 9 ‣ 4.4 Image
    Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"), [10](#S4.F10 "Figure 10 ‣ 4.4
    Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping"), and [11](#S4.F11 "Figure
    11 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A
    deep learning generative model approach for cross-survey image mapping"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: As the SDSS galaxies are substantially different in brightness and radii, it
    is not valid to use $\bar{\ell}$, $\bar{c}$, $\bar{s}$, and MSSIM as image quality
    metrics for DES/SDSS and reconstruction/SDSS image pairs. However, if the reconstruction
    process is robust, the distributions for DES/SDSS pairs and reconstruction/SDSS
    pairs should be consistent in the validation and external datasets. Hence, we
    will use reconstruction/DES and reconstruction/reconstruction measurements to
    quantify the reconstruction quality and reconstruction/SDSS measurements as metrics
    for robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean luminance index $\bar{\ell}$ is a measure of the differences in the
    pixel-to-pixel brightness of two (smoothed) images. The reconstruction/DES distributions
    for $\bar{\ell}$ were similar in shape, and there was not a significant difference
    between their medians, indicating that they had similar brightness qualities to
    one another; this is consistent with the pseudo-flux magnitude results (Figure
    [5](#S4.F5 "Figure 5 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset Properties ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping") and Table [2](#S4.T2 "Table 2 ‣ Pseudo-Flux Magnitude ‣ 4.2 Dataset
    Properties ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")). The brightness quality of the
    reconstructions relative to their SDSS counterparts were extremely similar to
    one another in both the validation and external distributions, implying that both
    were equally effective at increasing the image brightnesses.'
  prefs: []
  type: TYPE_NORMAL
- en: While the $\bar{\ell}$ values for the external dataset cannot be interpreted
    as measures of the image brightness qualities, they can be used to support the
    robustness of the reconstruction process. The shapes of the CAE/SDSS $\bar{\ell}$
    distributions for the validation and external datasets were similar to one another,
    and there was not a significant difference between their medians; the same is
    true for CycleGAN/SDSS. This implies that the brightness quality improvement was
    consistent for both the validation and external SDSS datasets, providing evidence
    for the robustness of this method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mean contrast index $\bar{c}$ describes the average difference in smoothness
    between small cut-outs of image pairs. For the validation data, the CycleGAN reconstructions
    had a significantly higher contrast index than the CAE reconstructions, implying
    that the sharpness of the CycleGAN images was more consistent with that of the
    DES images. This confirms that the CAE reconstructions tended to smooth the images,
    leading to the blurriness seen in Figures [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") and [4](#S4.F4 "Figure 4 ‣ 4.1
    Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping"). The robustness of
    the reconstructions can again be seen by the lack of a significant difference
    between the test and external distributions for the reconstruction/SDSS $\bar{c}$
    distributions. Note that the differences between the S/N for each image set likely
    contributed to the value of $\bar{c}$; however, the qualitative sharpness of the
    reconstructions are consistent with the conclusions drawn from $\bar{c}$.'
  prefs: []
  type: TYPE_NORMAL
- en: The mean cross-correlation index $\bar{s}$ is a measure of the deviations in
    the small-scale structure between two images; large values of $\bar{s}$ indicate
    that, after normalizing for the brightness and sharpness, the morphological features
    of the images at small scales are similar to (strongly correlated with) one another.
    The $\bar{s}$ distributions for the validation data indicates that the CAE reconstructions
    are significantly more closely correlated with their DES counterparts at small
    scales than the CycleGAN reconstructions. This implies that CAE reconstruction
    preserves more information at small scales than CycleGAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The combination of these quantities yields the MSSIM distributions seen in
    Fig. [11](#S4.F11 "Figure 11 ‣ 4.4 Image Quality Comparison ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"). This metric indicates that the overall quality of the CAE images was
    comparable to that of the CycleGAN reconstructions; however, the breakdown in
    terms of $\bar{\ell}$, $\bar{c}$, and $\bar{s}$ suggests that the reconstruction
    methods provide differing benefits. Specifically, CycleGAN reconstructions are
    generally sharper than their CAE counterparts, while CAE reconstructions preserve
    more information at small scales in the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/016eed935e10f08987035c24784c45ca.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: A selection of several notable objects from the external dataset.
    In each of these images, it appears that the CAE reconstructions may have removed
    artifacts from the image. The reconstructed objects may have been generated through
    inpainting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30091a24c2501684c7b05093ac318fc3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: A selection of several notable objects from the validation dataset.
    In each of these images, it appears that the CAE reconstructions may have removed
    large artifacts from the image. Note that both reconstructions may have used inpainting
    to generate the images in column 1, while the CAE reconstruction of the central
    object in column 2 appears to have removed the corrupted region of that object.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we would like to highlight several unique images from the external
    dataset; these are shown in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality
    Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). These images were found through
    visual inspection of images with the lowest reconstruction/SDSS $\bar{\ell}$,
    $\bar{c}$, $\bar{s}$, and/or MSSIM values in the external dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each image in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping") is heavily corrupted by artifacts; however, the
    CAE reconstructions appear to have removed these artifacts at the cost of blurring
    the objects in the image. These results are consistent with studies of denoising
    autoencoders (Vincent et al., [2008](#bib.bib55)), which proven effective at smoothing
    brightness/color variations, removing artifacts, and restoring corrupted images.
    As the base architecture of a denoising autoencoder is similar to that of our
    encoder/decoder pair, it is not surprising that the image reconstructions were
    effective at removing these artifacts. The CycleGAN reconstructions, however,
    fail to consistently remove these artifacts, though do succeed in amplifying the
    brightness of these objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [13](#S4.F13 "Figure 13 ‣ 4.4 Image Quality Comparison ‣ 4 Results and
    Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping") shows several images from the validation dataset that contain
    artifacts. The images in row 1 were found due to their extreme reconstruction/SDSS
    $\bar{\ell}$, $\bar{c}$, $\bar{s}$, and MSSIM values; however, those in row 2
    were found via manual inspection of the validation dataset. This was to be expected
    because the Stripe82 dataset is generally of higher quality than the external
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In row 1, it appears that the CAE reconstruction removed the artifact, albeit
    at the cost of blurring the central object. The artifact in row 2 consists of
    a blue streak passing through the upper-left edge of the central object. Like
    column 4 in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison ‣ 4
    Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), there is no signal in this region of the CAE
    reconstruction, implying that little or no inpainting was performed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the validation data and training data were taken from the same population,
    it is likely that the training data had a similar incidence of corrupted images
    as the validation data. As a result, it is unlikely that either neural network
    was trained sufficiently to accurately extract the signal from the heavily corrupted
    images in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison ‣ 4 Results
    and Analysis ‣ Survey2Survey: A deep learning generative model approach for cross-survey
    image mapping"), implying that objects recovered in these images likely resulted
    from inpainting. While outside the scope of this work, the improvement in the
    quality of the images in Figure [12](#S4.F12 "Figure 12 ‣ 4.4 Image Quality Comparison
    ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative model approach
    for cross-survey image mapping"), especially given the lack of training on corrupted
    images, warrants a more thorough analysis of the effectiveness of corrupted image
    reconstruction using our CAE architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, we demonstrated the viability of robust cross-survey galaxy image
    translation using neural networks and generative models. Using the pseudo-flux
    magnitude (Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")) and mean luminance index $\bar{\ell}$ (Section [4.4](#S4.SS4 "4.4 Image
    Quality Comparison ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping")), we show that the average brightnesses
    of the reconstructions more closely match DES images than their SDSS source images
    while preserving the structural information contained within the source galaxy
    (Section [4.3](#S4.SS3 "4.3 Pseudo-Luminosity Profile ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping")). In Section [4.2](#S4.SS2 "4.2 Dataset Properties ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we also demonstrated that the reconstruction process improved the signal-to-noise
    ratio of the source images. The signal-to-noise ratio of the CycleGAN images closely
    correlated with that of the DES images, while the CAE images improved this quantity
    relative to the DES images; this behavior is expected because autoencoders have
    been shown to be effective at reducing the amount of noise in images (Vincent
    et al., [2008](#bib.bib55)). Together, these imply that our method can be used
    to improve image brightness and signal strength using image-to-image translation.
    In Section [4.4](#S4.SS4 "4.4 Image Quality Comparison ‣ 4 Results and Analysis
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), we discuss the pros and cons of each reconstruction method using the
    mean contrast index $\bar{c}$ and cross-correlation index $\bar{s}$. We found
    that CycleGAN reconstructions were sharper, while CAE reconstructions more accurately
    reproduced the structure of DES galaxies at length scales on the order of several
    pixels at the cost of being slightly blurrier. Finally, we highlighted several
    instances in which the reconstructions appear to have removed large artifacts.
    We find evidence for the robustness of our method by performing reconstructions
    on images from the SDSS catalog in the external region, which contains objects
    without a DES counterpart. Though these images were fainter and had lower S/N
    than images from the overlap region (Stripe82), the large- and small-scale statistics
    of these image reconstructions were similar to those in the overlap region, implying
    that the reconstruction process accurately created DES representation of these
    objects. However, there is the possibility that our model was overfitted due to
    our choice to avoid factors that may impact the accuracy of the map between SDSS
    and DES images in the Stripe82 region.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While this only constitutes an initial application, our results show that feature
    transfer learning shows promise as a method for false galaxy image generation.
    This has great implications for the analysis of astronomical survey data: assuming
    that there is a sufficiently large sample of corresponding SDSS and DES image
    pairs, one could improve the brightness and S/N of many images from the SDSS catalog,
    decreasing the amount of error and improving the statistical power of analyses.
    Additionally, this provides an important advantage over other generative models
    used supplement survey data: while other methods generate false images that share
    the properties of the images in the data set of interest, feature-to-feature translation
    provides representations of observed galaxies, providing a way to extend both
    the size and the sky coverage of galaxy surveys.'
  prefs: []
  type: TYPE_NORMAL
- en: The reconstruction pipeline we developed solely constitutes a initial exploration,
    but the efficiency and robustness of the reconstruction process shows promise
    as a method for generating or improving survey data. While SDSS and DES data were
    used in this work, we expect that this may be applicable to other surveys, particularly
    for deeper surveys such as LSST (Ivezić et al., [2019](#bib.bib21)). All quantities
    calculated were derived solely from the mean of the (r, g, b) channel pixel values
    of survey images; however, we anticipate that similar methods could be used for
    the generation of false images with physical observables consistent with those
    of survey images. In addition, our methodology could be expanded to enable cross-wavelength
    or band-to-band translation. A neural network could be trained with a feature
    set containing fewer bands than the target dataset, generating a map between each
    pair of bands in the training and target data. The trained network could be used
    to supplement survey data by generating realistic reconstructions of image data
    in frequency bands not probed by that survey. We intend to explore these applications
    in future work using DES DR2 data, which contains more images and has a greater
    field depth than DES DR1 (Abbott et al., [2021](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This material is based upon work supported by the National Science Foundation
    Graduate Research Fellowship Program under Grant No. DGE — 1746047\. M. Carrasco
    Kind has been supported by NSF Grant AST-1536171.
  prefs: []
  type: TYPE_NORMAL
- en: Author contribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'B. Buncher: Data analysis, figure creation, writing, and editing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A. N. Sharma: AI model creation, data collection, figure creation, writing,
    and editing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'M. Carrasco Kind: Oversight, data collection, writing, and editing.'
  prefs: []
  type: TYPE_NORMAL
- en: Softwares Used
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This research made us of matplotlib (Hunter, [2007](#bib.bib19)), numpy (Oliphant,
    [2006](#bib.bib34); Van Der Walt et al., [2011](#bib.bib51)), scikit-image (Van der
    Walt et al., [2014](#bib.bib52)), SciPY (Virtanen et al., [2019](#bib.bib56)),
    and seaborn (Waskom et al., [2017](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: This research made use of Astropy,⁴⁴4http://www.astropy.org a community-developed
    core Python package for Astronomy (Astropy Collaboration et al., [2013](#bib.bib6);
    Price-Whelan et al., [2018](#bib.bib39)).
  prefs: []
  type: TYPE_NORMAL
- en: This research made use of Photutils, an Astropy package for detection and photometry
    of astronomical sources (Bradley et al., [2019](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: Data Availability Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data underlying this article will be shared on reasonable request to the
    corresponding author.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abadi et al. (2015) Abadi M., et al., 2015, TensorFlow: Large-Scale Machine
    Learning on Heterogeneous Systems, [http://tensorflow.org/](http://tensorflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abazajian et al. (2009) Abazajian K. N., et al., 2009, [ApJS](http://dx.doi.org/10.1088/0067-0049/182/2/543),
    [182, 543](https://ui.adsabs.harvard.edu/abs/2009ApJS..182..543A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abbott et al. (2018) Abbott T. M. C., et al., 2018, [ApJS](http://dx.doi.org/10.3847/1538-4365/aae9f0),
    [239, 18](https://ui.adsabs.harvard.edu/abs/2018ApJS..239...18A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abbott et al. (2021) Abbott T. M. C., et al., 2021, arXiv e-prints, [p. arXiv:2101.05765](https://ui.adsabs.harvard.edu/abs/2021arXiv210105765A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahumada et al. (2020) Ahumada R., et al., 2020, [ApJS](http://dx.doi.org/10.3847/1538-4365/ab929e),
    [249, 3](https://ui.adsabs.harvard.edu/abs/2020ApJS..249....3A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astropy Collaboration et al. (2013) Astropy Collaboration et al., 2013, [A&A](http://dx.doi.org/10.1051/0004-6361/201322068),
    [558, A33](http://adsabs.harvard.edu/abs/2013A%26A...558A..33A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bowen & Vaughan (1973) Bowen I. S., Vaughan A. H., 1973, [Appl. Opt.](http://dx.doi.org/10.1364/AO.12.001430),
    12, 1430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bradley et al. (2019) Bradley L., et al., 2019, astropy/photutils: v0.6, [doi:10.5281/zenodo.2533376](http://dx.doi.org/10.5281/zenodo.2533376),
    [https://doi.org/10.5281/zenodo.2533376](https://doi.org/10.5281/zenodo.2533376)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cai et al. (2020) Cai M. X., Bédorf J., Saletore V. A., Codreanu V., Podareanu
    D., Chaibi A., Qian P. X., 2020, arXiv e-prints, [p. arXiv:2010.11630](https://ui.adsabs.harvard.edu/abs/2020arXiv201011630C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2020) Cheng T.-Y., Li N., Conselice C. J., Aragón-Salamanca A.,
    Dye S., Metcalf R. B., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/staa1015),
    [494, 3750](https://ui.adsabs.harvard.edu/abs/2020MNRAS.494.3750C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet et al. (2015) Chollet F., et al., 2015, Keras, [https://keras.io](https://keras.io)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cortese et al. (2017) Cortese L., Catinella B., Janowiecki S., 2017, [ApJL](http://dx.doi.org/10.3847/2041-8213/aa8cc3),
    [848, L7](https://ui.adsabs.harvard.edu/abs/2017ApJ...848L...7C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Durugkar et al. (2016) Durugkar I. P., Gemp I., Mahadevan S., 2016, CoRR, abs/1611.01673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flaugher et al. (2015) Flaugher B., Diehl H. T., Honscheid K., et al., 2015,
    [AJ](http://dx.doi.org/10.1088/0004-6256/150/5/150), [150, 150](http://adsabs.harvard.edu/abs/2015AJ....150..150F)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frontera-Pons et al. (2017) Frontera-Pons J., Sureau F., Bobin J., Le Floc’h
    E., 2017, [A&A](http://dx.doi.org/10.1051/0004-6361/201630240), [603, A60](https://ui.adsabs.harvard.edu/abs/2017A&A...603A..60F)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graff et al. (2014) Graff P., Feroz F., Hobson M. P., Lasenby A., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu642),
    [441, 1741](https://ui.adsabs.harvard.edu/abs/2014MNRAS.441.1741G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gunn et al. (2006) Gunn J. E., et al., 2006, [Astron. J.](http://dx.doi.org/10.1086/500975),
    131, 2332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holtzman et al. (2010) Holtzman J., Harrison T., Coughlin J., 2010, [Advances
    in Astronomy](http://dx.doi.org/10.1155/2010/193086), 2010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hunter (2007) Hunter J. D., 2007, Computing in Science & Engineering, 9, 90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola et al. (2016) Isola P., Zhu J., Zhou T., Efros A. A., 2016, CoRR, abs/1611.07004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. (2020) Jia Z., Yuan B., Wang K., Wu H., Clifford D., Yuan Z., Su
    H., 2020, arXiv e-prints, [p. arXiv:2012.04932](https://ui.adsabs.harvard.edu/abs/2020arXiv201204932J)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2014a) Jiang L., et al., 2014a, [Astrophys. J. Suppl.](http://dx.doi.org/10.1088/0067-0049/213/1/12),
    213, 12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2014b) Jiang L., et al., 2014b, [ApJS](http://dx.doi.org/10.1088/0067-0049/213/1/12),
    [213, 12](https://ui.adsabs.harvard.edu/abs/2014ApJS..213...12J)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample et al. (2017) Lample G., Zeghidour N., Usunier N., Bordes A., Denoyer
    L., Ranzato M., 2017, arXiv e-prints, [p. arXiv:1706.00409](https://ui.adsabs.harvard.edu/abs/2017arXiv170600409L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lanusse et al. (2020) Lanusse F., Mandelbaum R., Ravanbakhsh S., Li C.-L., Freeman
    P., Poczos B., 2020, arXiv e-prints, [p. arXiv:2008.03833](https://ui.adsabs.harvard.edu/abs/2020arXiv200803833L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) Lin Q., Fouchez D., Pasquet J., 2021, arXiv e-prints, [p.
    arXiv:2101.07389](https://ui.adsabs.harvard.edu/abs/2021arXiv210107389L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) Liu H., Liu J., Tao T., Hou S., Han J., 2020, arXiv e-prints,
    [p. arXiv:2012.14142](https://ui.adsabs.harvard.edu/abs/2020arXiv201214142L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2021) Luo H., Liao G., Hou X., Liu B., Zhou F., Qiu G., 2021, arXiv
    e-prints, [p. arXiv:2101.02384](https://ui.adsabs.harvard.edu/abs/2021arXiv210102384L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lupton et al. (2004) Lupton R., Blanton M. R., Fekete G., Hogg D. W., O’Mullane
    W., Szalay A., Wherry N., 2004, [PASP](http://dx.doi.org/10.1086/382245), [116,
    133](https://ui.adsabs.harvard.edu/abs/2004PASP..116..133L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masci et al. (2011) Masci J., Meier U., Cireşan D., Schmidhuber J., 2011, in
    Honkela T., Duch W., Girolami M., Kaski S., eds, Artificial Neural Networks and
    Machine Learning – ICANN 2011\. Springer Berlin Heidelberg, Berlin, Heidelberg,
    pp 52–59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maziarka et al. (2019) Maziarka Ł., Pocha A., Kaczmarczyk J., Rataj K., Warchoł
    M., 2019, arXiv e-prints, [p. arXiv:1902.02119](https://ui.adsabs.harvard.edu/abs/2019arXiv190202119M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moriwaki et al. (2020) Moriwaki K., Shirasaki M., Yoshida N., 2020, arXiv e-prints,
    [p. arXiv:2010.00809](https://ui.adsabs.harvard.edu/abs/2020arXiv201000809M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Oliphant (2006) Oliphant T., 2006, NumPy: A guide to NumPy, USA: Trelgol Publishing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Osakabe et al. (2020) Osakabe T., Tanaka M., Kinoshita Y., Kiya H., 2020, arXiv
    e-prints, [p. arXiv:2012.00287](https://ui.adsabs.harvard.edu/abs/2020arXiv201200287O)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padmanabhan & Loeb (2020) Padmanabhan H., Loeb A., 2020, arXiv e-prints, [p.
    arXiv:2002.01489](https://ui.adsabs.harvard.edu/abs/2020arXiv200201489P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patel & Upla (2019) Patel H., Upla K. P., 2019, in Arora C., Mitra K., eds,
    Computer Vision Applications. Springer Singapore, Singapore, pp 115–128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perarnau et al. (2016) Perarnau G., van de Weijer J., Raducanu B., Álvarez J. M.,
    2016, arXiv e-prints, [p. arXiv:1611.06355](https://ui.adsabs.harvard.edu/abs/2016arXiv161106355P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Price-Whelan et al. (2018) Price-Whelan A. M., et al., 2018, [AJ](http://dx.doi.org/10.3847/1538-3881/aabc4f),
    [156, 123](https://ui.adsabs.harvard.edu/#abs/2018AJ....156..123T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2016) Radford A., Metz L., Chintala S., 2016, Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks ([arXiv:1511.06434](http://arxiv.org/abs/1511.06434))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ralph et al. (2019) Ralph N. O., et al., 2019, [Publications of the Astronomical
    Society of the Pacific](http://dx.doi.org/10.1088/1538-3873/ab213d), 131, 108011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regier et al. (2015a) Regier J., McAuliffe J., Prabhat M., 2015a, in NIPS Workshop:
    Advances in Approximate Bayesian Inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regier et al. (2015b) Regier J., Miller A., McAuliffe J., Adams R., Hoffman
    M., Lang D., Schlegel D., Prabhat 2015b, arXiv e-prints, [p. arXiv:1506.01351](https://ui.adsabs.harvard.edu/abs/2015arXiv150601351R)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schawinski et al. (2018) Schawinski K., Turp M. D., Zhang C., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833800),
    [616, L16](https://ui.adsabs.harvard.edu/abs/2018A&A...616L..16S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2017) Shen H., George D., Huerta E. A., Zhao Z., 2017, arXiv e-prints,
    [p. arXiv:1711.09919](https://ui.adsabs.harvard.edu/abs/2017arXiv171109919S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shirasaki et al. (2019) Shirasaki M., Yoshida N., Ikeda S., Oogi T., Nishimichi
    T., 2019, arXiv e-prints, [p. arXiv:1911.12890](https://ui.adsabs.harvard.edu/abs/2019arXiv191112890S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith & Geach (2019) Smith M. J., Geach J. E., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2886),
    [490, 4985](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.4985S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spindler et al. (2020) Spindler A., Geach J. E., Smith M. J., 2020, [MNRAS](http://dx.doi.org/10.1093/mnras/staa3670),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storey-Fisher et al. (2020) Storey-Fisher K., Huertas-Company M., Ramachandra
    N., Lanusse F., Leauthaud A., Luo Y., Huang S., 2020, arXiv e-prints, [p. arXiv:2012.08082](https://ui.adsabs.harvard.edu/abs/2020arXiv201208082S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ullmo et al. (2020) Ullmo M., Decelle A., Aghanim N., 2020, arXiv e-prints,
    [p. arXiv:2011.05244](https://ui.adsabs.harvard.edu/abs/2020arXiv201105244U)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Der Walt et al. (2011) Van Der Walt S., Colbert S. C., Varoquaux G., 2011,
    Computing in Science & Engineering, 13, 22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van der Walt et al. (2014) Van der Walt S., Schönberger J. L., Nunez-Iglesias
    J., Boulogne F., Warner J. D., Yager N., Gouillart E., Yu T., 2014, PeerJ, 2,
    e453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villar et al. (2020a) Villar V. A., Cranmer M., Contardo G., Ho S., Yao-Yu Lin
    J., 2020a, arXiv e-prints, [p. arXiv:2010.11194](https://ui.adsabs.harvard.edu/abs/2020arXiv201011194V)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villar et al. (2020b) Villar V. A., et al., 2020b, [ApJ](http://dx.doi.org/10.3847/1538-4357/abc6fd),
    [905, 94](https://ui.adsabs.harvard.edu/abs/2020ApJ...905...94V)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vincent et al. (2008) Vincent P., Larochelle H., Bengio Y., Manzagol P.-A.,
    2008, in Proceedings of the 25th International Conference on Machine Learning.
    ICML ’08. Association for Computing Machinery, New York, NY, USA, p. 1096–1103,
    [doi:10.1145/1390156.1390294](http://dx.doi.org/10.1145/1390156.1390294), [https://doi.org/10.1145/1390156.1390294](https://doi.org/10.1145/1390156.1390294)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtanen et al. (2019) Virtanen P., et al., 2019, arXiv e-prints, [p. arXiv:1907.10121](https://ui.adsabs.harvard.edu/abs/2019arXiv190710121V)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019) Wang Y., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz2907),
    [490, 5722](https://ui.adsabs.harvard.edu/abs/2019MNRAS.490.5722W)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Wang Y.-C., Xie Y.-B., Zhang T.-J., Huang H.-C., Zhang T.,
    Liu K., 2020, arXiv e-prints, [p. arXiv:2005.10628](https://ui.adsabs.harvard.edu/abs/2020arXiv200510628W)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waskom et al. (2017) Waskom M., et al., 2017, mwaskom/seaborn: v0.8.1 (September
    2017), [doi:10.5281/zenodo.883859](http://dx.doi.org/10.5281/zenodo.883859), [https://doi.org/10.5281/zenodo.883859](https://doi.org/10.5281/zenodo.883859)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler (2012) Zeiler M. D., 2012, arXiv e-prints, [p. arXiv:1212.5701](https://ui.adsabs.harvard.edu/abs/2012arXiv1212.5701Z)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou Wang et al. (2004) Zhou Wang Bovik A. C., Sheikh H. R., Simoncelli E. P.,
    2004, IEEE Transactions on Image Processing, 13, 600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) Zhu J.-Y., Park T., Isola P., Efros A. A., 2017, arXiv e-prints,
    [p. arXiv:1703.10593](https://ui.adsabs.harvard.edu/abs/2017arXiv170310593Z)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Additional Image Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we show additional examples of SDSS, DES, and reconstructed images from
    the validation dataset similar to Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping"). They were randomly selected to
    provide examples of objects with a variety of types, brightnesses, and extents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rows in Figs. [14](#A1.F14 "Figure 14 ‣ Appendix A Additional Image Samples
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping") - [18](#A1.F18 "Figure 18 ‣ Appendix A Additional Image Samples ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping") represent
    the following quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: (A)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SDSS representation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (B)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DES representation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (C)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CAE reconstruction
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (D)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CycleGAN reconstruction
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (E)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CAE residuals (CAE - DES)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (F)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CycleGAN residuals (CycleGAN - DES)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (G)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CAE gain (CAE - SDSS)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (H)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CycleGAN gain (CycleGAN - SDSS),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'while in Fig. [19](#A1.F19 "Figure 19 ‣ Appendix A Additional Image Samples
    ‣ Survey2Survey: A deep learning generative model approach for cross-survey image
    mapping"), they represent the following quantities:'
  prefs: []
  type: TYPE_NORMAL
- en: (A)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SDSS representation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (B)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CAE reconstruction
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (C)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CycleGAN reconstruction
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (D)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CAE gain (CAE - SDSS)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (E)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: CycleGAN gain (CycleGAN - SDSS),
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note that to increase visibility, the residual and gain images were artificially
    enhanced with a power law transform (see Figs. [3](#S4.F3 "Figure 3 ‣ 4.1 Qualitative
    Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning generative
    model approach for cross-survey image mapping") and [4](#S4.F4 "Figure 4 ‣ 4.1
    Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey: A deep learning
    generative model approach for cross-survey image mapping") for details).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ba6c01e0eaedf3c04019db15b936099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ef12532696ee42dec60ea4030bbc2b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fec0db5fed2038ef8433542dd24f962.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d24e20651b57b75c8155b2288c3f39f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4dadfcaa733131eab6fce6820784f1d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Additional examples of source, target, and reconstructed images
    from the validation dataset; the formatting is the same as in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows E, F, G and H were enhanced for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48def07828a454ec4fb41f2edfe6b51f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Additional examples of source, target, and reconstructed images
    from the external dataset; the formatting is the same as in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Qualitative Analysis ‣ 4 Results and Analysis ‣ Survey2Survey:
    A deep learning generative model approach for cross-survey image mapping"). Note
    that the images in rows D and E were enhanced for clarity.'
  prefs: []
  type: TYPE_NORMAL
