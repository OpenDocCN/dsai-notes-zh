- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:05:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:05:34
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.12740] Deep Learning in Video Multi-Object Tracking: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.12740] 视频多目标跟踪中的深度学习：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.12740](https://ar5iv.labs.arxiv.org/html/1907.12740)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.12740](https://ar5iv.labs.arxiv.org/html/1907.12740)
- en: 'Deep Learning in Video Multi-Object Tracking: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频多目标跟踪中的深度学习：综述
- en: Gioele Ciaparrone Department of Management and Innovation Systems, University
    of Salerno, 84084 Fisciano (SA), Italy Andalusian Research Institute in Data Science
    and Computational Intelligence, University of Granada, 18071 Granada, Spain Francisco
    Luque Sánchez Andalusian Research Institute in Data Science and Computational
    Intelligence, University of Granada, 18071 Granada, Spain Siham Tabik Andalusian
    Research Institute in Data Science and Computational Intelligence, University
    of Granada, 18071 Granada, Spain Luigi Troiano Department of Engineering, University
    of Sannio, 82100 Benevento, Italy Roberto Tagliaferri Department of Management
    and Innovation Systems, University of Salerno, 84084 Fisciano (SA), Italy Francisco
    Herrera Andalusian Research Institute in Data Science and Computational Intelligence,
    University of Granada, 18071 Granada, Spain
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Gioele Ciaparrone 管理与创新系统系，萨莱诺大学，84084 Fisciano (SA)，意大利 安达卢西亚数据科学与计算智能研究所，格拉纳达大学，18071
    格拉纳达，西班牙 Francisco Luque Sánchez 安达卢西亚数据科学与计算智能研究所，格拉纳达大学，18071 格拉纳达，西班牙 Siham
    Tabik 安达卢西亚数据科学与计算智能研究所，格拉纳达大学，18071 格拉纳达，西班牙 Luigi Troiano 工程系，萨尼奥大学，82100 贝内文托，意大利
    Roberto Tagliaferri 管理与创新系统系，萨莱诺大学，84084 Fisciano (SA)，意大利 Francisco Herrera 安达卢西亚数据科学与计算智能研究所，格拉纳达大学，18071
    格拉纳达，西班牙
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The problem of Multiple Object Tracking (MOT) consists in following the trajectory
    of different objects in a sequence, usually a video. In recent years, with the
    rise of Deep Learning, the algorithms that provide a solution to this problem
    have benefited from the representational power of deep models. This paper provides
    a comprehensive survey on works that employ Deep Learning models to solve the
    task of MOT on single-camera videos. Four main steps in MOT algorithms are identified,
    and an in-depth review of how Deep Learning was employed in each one of these
    stages is presented. A complete experimental comparison of the presented works
    on the three MOTChallenge datasets is also provided, identifying a number of similarities
    among the top-performing methods and presenting some possible future research
    directions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标跟踪（MOT）问题在于跟踪序列中不同物体的轨迹，通常是视频。近年来，随着深度学习的兴起，解决该问题的算法受益于深度模型的表示能力。本文对采用深度学习模型解决单摄像头视频上的MOT任务的工作进行了全面综述。识别了MOT算法中的四个主要步骤，并对每个阶段中如何运用深度学习进行了深入审查。还提供了对三个MOTChallenge数据集上呈现工作的完整实验比较，识别了顶级方法中的一些相似性，并提出了一些可能的未来研究方向。
- en: '*K*eywords Multiple Object Tracking  $\cdot$ Deep Learning  $\cdot$ Video Tracking
     $\cdot$ Computer Vision  $\cdot$ Convolutional Neural Networks  $\cdot$ LSTM
     $\cdot$ Reinforcement Learning'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*eywords 多目标跟踪  $\cdot$ 深度学习  $\cdot$ 视频跟踪  $\cdot$ 计算机视觉  $\cdot$ 卷积神经网络
     $\cdot$ LSTM  $\cdot$ 强化学习'
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Multiple Object Tracking (MOT), also called Multi-Target Tracking (MTT), is
    a computer vision task that aims to analyze videos in order to identify and track
    objects belonging to one or more categories, such as pedestrians, cars, animals
    and inanimate objects, without any prior knowledge about the appearance and number
    of targets. Differently from object detection algorithms, whose output is a collection
    of rectangular bounding boxes identified by their coordinates, height and width,
    MOT algorithms also associate a target ID to each box (known as a detection),
    in order to distinguish among intra-class objects. An example of the output of
    a MOT algorithm is illustrated in figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey"). The MOT task plays
    an important role in computer vision: from video surveillance to autonomous cars,
    from action recognition to crowd behaviour analysis, many of these problems would
    benefit from a high-quality tracking algorithm.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 多目标跟踪（MOT），也称为多目标跟踪（MTT），是一种计算机视觉任务，旨在分析视频，以识别和跟踪属于一个或多个类别的对象，例如行人、汽车、动物和无生命物体，而无需任何关于目标外观和数量的先验知识。与目标检测算法不同，目标检测算法的输出是由其坐标、长度和宽度确定的一系列矩形边界框，而MOT算法还会为每个框（称为检测）分配一个目标ID，以区分同类对象。图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 视频多目标跟踪中的深度学习：综述")展示了MOT算法的输出示例。MOT任务在计算机视觉中扮演着重要角色：从视频监控到自动驾驶汽车，从动作识别到人群行为分析，许多这些问题都将受益于高质量的跟踪算法。
- en: '![Refer to caption](img/6ce0cec186bd66543d221fa63344ec15.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6ce0cec186bd66543d221fa63344ec15.png)'
- en: 'Figure 1: An illustration of the output of a MOT algorithm. Each output bounding
    box has a number that identifies a specific person in the video.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MOT算法输出的示意图。每个输出边界框都有一个编号，用于识别视频中的特定人物。
- en: While in Single Object Tracking (SOT) the appearance of the target is known
    a priori, in MOT a detection step is necessary to identify the targets, that can
    leave or enter the scene. The main difficulty in tracking multiple targets simultaneously
    stems from the various occlusions and interactions between objects, that can sometimes
    also have similar appearance. Thus, simply applying SOT models directly to solve
    MOT leads to poor results, often incurring in target drift and numerous ID switch
    errors, as such models usually struggle in distinguishing between similar looking
    intra-class objects. A series of algorithms specifically tuned to multi-target
    tracking have then been developed in recent years to address these issues, together
    with a number of benchmark datasets and competitions to ease the comparisons between
    the different methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在单目标跟踪（SOT）中目标的外观是已知的，但在MOT中，需要进行检测步骤以识别目标，这些目标可能会离开或进入场景。与此同时，同时跟踪多个目标的主要难点在于对象之间的各种遮挡和相互作用，有时它们的外观也可能相似。因此，简单地将SOT模型直接应用于解决MOT问题会导致较差的结果，通常会出现目标漂移和大量ID切换错误，因为这些模型通常难以区分外观相似的同类对象。近年来，开发了一系列专门针对多目标跟踪的算法，以解决这些问题，并且有多个基准数据集和竞赛以简化不同方法之间的比较。
- en: Recently, more and more of such algorithms have started exploiting the representational
    power of deep learning (DL). The strength of Deep Neural Networks (DNN) resides
    in their ability to learn rich representations and to extract complex and abstract
    features from their input. Convolutional neural networks (CNN) currently constitute
    the state-of-the-art in spatial pattern extraction, and are employed in tasks
    such as image classification [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]
    or object detection [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)], while recurrent
    neural networks (RNN) like the Long Short-Term Memory (LSTM) are used to process
    sequential data, like audio signals, temporal series and text [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]. Since DL methods have been
    able to reach top performance in many of those tasks, we are now progressively
    seeing them used in most of the top performing MOT algorithms, aiding to solve
    some of the subtasks in which the problem is divided.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，越来越多的算法开始利用深度学习（DL）的表现力。深度神经网络（DNN）的优势在于其学习丰富表示和从输入中提取复杂抽象特征的能力。卷积神经网络（CNN）目前在空间模式提取方面处于最先进水平，并被用于图像分类
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)] 或目标检测 [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6)] 等任务，而循环神经网络（RNN），如长短期记忆（LSTM），则用于处理序列数据，如音频信号、时间序列和文本 [[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]。由于DL方法在许多任务中已能达到顶尖性能，我们现在逐渐看到它们被用于大多数顶级MOT算法中，帮助解决问题被划分成的一些子任务。
- en: This work presents a survey of algorithms that make use of the capabilities
    of deep learning models to perform Multiple Object Tracking, focusing on the different
    approaches used for the various components of a MOT algorithm and putting them
    in the context of each of the proposed methods. While the MOT task can be applied
    to both 2D and 3D data, and to both single-camera and multi-camera scenarios,
    in this survey we focus on 2D data extracted from videos recorded by a single
    camera.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对利用深度学习模型能力进行多目标跟踪（MOT）的算法进行了综述，重点介绍了MOT算法各个组件使用的不同方法，并将它们放在每种提议方法的背景中。虽然MOT任务可以应用于2D和3D数据，以及单摄像头和多摄像头场景，但本综述专注于从单一摄像头录制的视频中提取的2D数据。
- en: 'Some reviews and surveys have been published on the subject of MOT. Their main
    contributions and limitations are the following:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 关于MOT主题，已经发表了一些综述和调查。它们的主要贡献和局限性如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Luo et al. [[11](#bib.bib11)] presented the first comprehensive review to focus
    specifically on MOT, in particular on pedestrian tracking. They provided a unified
    formulation of the MOT problem and described the main techniques used in the key
    steps of a MOT system. They presented deep learning as one of the future research
    directions, since at the time it had only been employed by very few algorithms.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Luo 等人 [[11](#bib.bib11)] 提出了首个全面专注于MOT，特别是行人跟踪的综述。他们提供了MOT问题的统一表述，并描述了MOT系统关键步骤中使用的主要技术。他们将深度学习作为未来研究方向之一，因为当时只有少数算法应用了这种方法。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Camplani et al. [[12](#bib.bib12)] presented a survey on Multiple Pedestrian
    Tracking, but they focused on RGB-D data, while our focus is on 2D RGB images,
    without additional inputs. Moreover, their review does not cover deep learning
    based algorithms.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Camplani 等人 [[12](#bib.bib12)] 对多行人跟踪进行了综述，但他们专注于RGB-D数据，而我们的关注点是没有额外输入的2D RGB图像。此外，他们的综述未涵盖基于深度学习的算法。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Emami et al. [[13](#bib.bib13)] proposed a formulation of single and multi-sensor
    tracking tasks as a Multidimensional Assignment Problem (MDAP). They also presented
    a few approaches that employed deep learning in tracking problems, but it wasn’t
    the focus of their paper and they didn’t provide any experimental comparison among
    such methods.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Emami 等人 [[13](#bib.bib13)] 提出了将单传感器和多传感器跟踪任务表述为多维分配问题（MDAP）的公式。他们还介绍了一些在跟踪问题中应用深度学习的方法，但这不是他们论文的重点，他们也没有提供这些方法之间的实验比较。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Leal-Taixé et al. [[14](#bib.bib14)] presented an analysis of the results obtained
    by algorithms on the MOT15 [[15](#bib.bib15)] and MOT16 [[16](#bib.bib16)] datasets,
    providing a summary of the trending lines of research and statistics about the
    results. They found that after 2015, methods have been shifting from trying to
    find better optimization algorithms for the association problem to focusing on
    improving the affinity models, and they predict that many more approaches would
    tackle this issue by using deep learning. However, this work also did not focus
    on deep learning, and it does not cover more recent MOT algorithms, published
    in the last years.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Leal-Taixé等人[[14](#bib.bib14)]对算法在MOT15[[15](#bib.bib15)]和MOT16[[16](#bib.bib16)]数据集上的结果进行了分析，提供了研究趋势的总结和结果的统计数据。他们发现，自2015年以来，方法已从寻找更好的优化算法以解决关联问题转向改善亲和力模型，并预测许多更多的方法将通过使用深度学习来解决这个问题。然而，这项工作也没有集中于深度学习，并且没有涵盖近年来发布的更新的MOT算法。
- en: 'In this paper, based on the discussed limitations, our aim is to provide a
    survey with the following main contributions:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，基于讨论的局限性，我们的目标是提供一项调查，具有以下主要贡献：
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide the first comprehensive survey on the use of Deep Learning in Multiple
    Object Tracking, focusing on 2D data extracted from single-camera videos, including
    recent works that have not been covered by past surveys and reviews. The use of
    DL in MOT is in fact recent, and many approaches have been published in the last
    three years.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了有关深度学习在多目标跟踪中的应用的首次综合调查，重点关注从单摄像机视频中提取的2D数据，包括过去的调查和评审中未覆盖的最新工作。实际上，深度学习在MOT中的应用是最近的，过去三年中发布了许多方法。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We identify four common steps in MOT algorithms and describe the different DL
    models and approaches employed in each of those steps, including the algorithmic
    context in which they are used. The techniques utilized by each analyzed work
    are also summarized in a table, together with links to the available source code,
    to serve as a quick reference for future research.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们识别了MOT算法中的四个常见步骤，并描述了在这些步骤中使用的不同深度学习模型和方法，包括它们所使用的算法背景。每项分析工作的技术也在表格中总结，并附有可用源代码的链接，以便作为未来研究的快速参考。
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We collect experimental results on the most commonly used MOT datasets to perform
    a numerical comparison among them, also identifying the main trends in the best
    performing algorithms.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们收集了最常用的MOT数据集上的实验结果，以进行数值比较，同时识别表现最佳的算法中的主要趋势。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As final point, we discuss the possible future directions of research.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们讨论了可能的未来研究方向。
- en: 'The survey is further organized in this manner. We first describe the general
    structure of MOT algorithms and the most commonly used metrics and datasets in
    section [2](#S2 "2 MOT: algorithms, metrics and datasets ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey"). Section [3](#S3 "3 Deep learning in MOT ‣ Deep
    Learning in Video Multi-Object Tracking: A Survey") explores the various DL-based
    models and algorithms in each of the four identified steps of a MOT algorithm.
    Section [4](#S4 "4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") presents a numerical comparison among the presented algorithms
    and identifies common trends and patterns in current approaches, as well as some
    limitations and possible future research directions. Finally, section [5](#S5
    "5 Conclusion and future directions ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey") summarizes the findings of the previous sections and presents some
    final remarks.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '调查进一步以以下方式组织。我们首先在第[2](#S2 "2 MOT: algorithms, metrics and datasets ‣ Deep
    Learning in Video Multi-Object Tracking: A Survey")节中描述MOT算法的一般结构以及最常用的度量标准和数据集。第[3](#S3
    "3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")节探讨了每个MOT算法四个识别步骤中的各种基于深度学习的模型和算法。第[4](#S4
    "4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object Tracking: A
    Survey")节提供了所展示算法的数值比较，识别了当前方法中的共同趋势和模式，以及一些局限性和可能的未来研究方向。最后，第[5](#S5 "5 Conclusion
    and future directions ‣ Deep Learning in Video Multi-Object Tracking: A Survey")节总结了前几节的发现，并提出了一些最终的评论。'
- en: '2 MOT: algorithms, metrics and datasets'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '2 MOT: 算法、度量标准和数据集'
- en: 'In this section, a general description about the problem of MOT is provided.
    The main characteristics and common steps of MOT algorithms are identified and
    described in section [2.1](#S2.SS1 "2.1 Introduction to MOT algorithms ‣ 2 MOT:
    algorithms, metrics and datasets ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey"). The metrics that are usually employed to evaluate the performance
    of the models are discussed in section [2.2](#S2.SS2 "2.2 Metrics ‣ 2 MOT: algorithms,
    metrics and datasets ‣ Deep Learning in Video Multi-Object Tracking: A Survey"),
    while the most important benchmark datasets are presented in section [2.3](#S2.SS3
    "2.3 Benchmark datasets ‣ 2 MOT: algorithms, metrics and datasets ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '本节提供了关于MOT问题的一般描述。MOT算法的主要特征和常见步骤在[2.1](#S2.SS1 "2.1 Introduction to MOT algorithms
    ‣ 2 MOT: algorithms, metrics and datasets ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey")节中被识别和描述。通常用于评估模型性能的指标在[2.2](#S2.SS2 "2.2 Metrics ‣ 2 MOT:
    algorithms, metrics and datasets ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey")节中讨论，而最重要的基准数据集在[2.3](#S2.SS3 "2.3 Benchmark datasets ‣ 2 MOT: algorithms,
    metrics and datasets ‣ Deep Learning in Video Multi-Object Tracking: A Survey")节中呈现。'
- en: 2.1 Introduction to MOT algorithms
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 MOT算法简介
- en: 'The standard approach employed in MOT algorithms is tracking-by-detection:
    a set of detections (i.e. bounding boxes identifying the targets in the image)
    are extracted from the video frames and are used to guide the tracking process,
    usually by associating them together in order to assign the same ID to bounding
    boxes that contain the same target. For this reason, many MOT algorithms formulate
    the task as an assignment problem. Modern detection frameworks [[4](#bib.bib4),
    [17](#bib.bib17), [18](#bib.bib18), [5](#bib.bib5), [6](#bib.bib6)] ensure a good
    detection quality, and the majority of MOT methods (with some exceptions, as we
    will see) have been focusing on improving the association; indeed, many MOT datasets
    provide a standard set of detections that can be used by the algorithms (that
    can thus skip the detection stage) in order to exclusively compare their performances
    on the quality of the association algorithm, since the detector performance can
    heavily affect the tracking results.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: MOT算法中使用的标准方法是基于检测的跟踪：从视频帧中提取一组检测（即识别图像中目标的边界框），并用它们来指导跟踪过程，通常通过将它们关联在一起，以便为包含相同目标的边界框分配相同的ID。因此，许多MOT算法将任务表述为一个分配问题。现代检测框架[[4](#bib.bib4),
    [17](#bib.bib17), [18](#bib.bib18), [5](#bib.bib5), [6](#bib.bib6)]确保了良好的检测质量，而大多数MOT方法（有一些例外，如我们将看到的）专注于改进关联；实际上，许多MOT数据集提供了一套标准的检测，供算法使用（因此可以跳过检测阶段），以便专门比较它们在关联算法质量上的表现，因为检测器的性能会严重影响跟踪结果。
- en: MOT algorithms can also be divided into batch and online methods. Batch tracking
    algorithms are allowed to use future information (i.e. from future frames) when
    trying to determine the object identities in a certain frame. They often exploit
    global information and thus result in better tracking quality. Online tracking
    algorithms, on the contrary, can only use present and past information to make
    predictions about the current frame. This is a requirement in some scenarios,
    like autonomous driving and robot navigation. Compared to batch methods, online
    methods tend to perform worse, since they cannot fix past errors using future
    information. It is important to note that while a real-time algorithm is required
    to run in an online fashion, not every online method necessarily runs in real-time;
    quite often, in fact, with very few exceptions, online algorithms are still too
    slow to be employed in a real-time environment, especially when exploiting deep
    learning algorithms, that are often computationally intensive.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MOT 算法还可以分为批处理和在线方法。批处理跟踪算法在确定某一帧中的对象身份时允许使用未来的信息（即来自未来帧的信息）。它们通常利用全局信息，因此跟踪质量较好。相反，在线跟踪算法只能使用当前和过去的信息来对当前帧进行预测。这在某些场景下是必要的，如自动驾驶和机器人导航。与批处理方法相比，在线方法的表现往往较差，因为它们无法利用未来的信息来修正过去的错误。值得注意的是，虽然实时算法要求以在线方式运行，但并非所有在线方法都能实时运行；实际上，在线算法通常仍然太慢，无法在实时环境中使用，特别是当利用计算量大的深度学习算法时。
- en: 'Despite the huge variety of approaches presented in the literature, the vast
    majority of MOT algorithms share part or all of the following steps (summarized
    in figure [2](#S2.F2 "Figure 2 ‣ 2.1 Introduction to MOT algorithms ‣ 2 MOT: algorithms,
    metrics and datasets ‣ Deep Learning in Video Multi-Object Tracking: A Survey")):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管文献中介绍了大量不同的方法，但绝大多数MOT算法共享部分或所有以下步骤（见图[2](#S2.F2 "图2 ‣ 2.1 MOT算法介绍 ‣ 2 MOT：算法、度量和数据集
    ‣ 视频多目标跟踪中的深度学习：综述")）：
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Detection stage: an object detection algorithm analyzes each input frame to
    identify objects belonging to the target class(es) using bounding boxes, also
    known as ‘detections’ in the context of MOT;'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测阶段：一个目标检测算法分析每个输入帧，以识别属于目标类别的对象，使用边界框，也称为在MOT上下文中的‘检测’；
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Feature extraction/motion prediction stage: one or more feature extraction
    algorithms analyze the detections and/or the tracklets to extract appearance,
    motion and/or interaction features. Optionally, a motion predictor predicts the
    next position of each tracked target;'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 特征提取/运动预测阶段：一个或多个特征提取算法分析检测和/或跟踪，以提取外观、运动和/或交互特征。可选地，运动预测器预测每个跟踪目标的下一个位置；
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Affinity stage: features and motion predictions are used to compute a similarity/distance
    score between pairs of detections and/or tracklets;'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 亲和度阶段：使用特征和运动预测来计算检测和/或跟踪对之间的相似度/距离分数；
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Association stage: the similarity/distance measures are used to associate detections
    and tracklets belonging to the same target by assigning the same ID to detections
    that identify the same target.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关联阶段：使用相似度/距离度量来关联属于同一目标的检测和跟踪，方法是为识别相同目标的检测分配相同的ID。
- en: '![Refer to caption](img/30133b80e416f14dcbda1d1dcfa494fa.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/30133b80e416f14dcbda1d1dcfa494fa.png)'
- en: 'Figure 2: Usual workflow of a MOT algorithm: given the raw frames of a video
    (1), an object detector is run to obtain the bounding boxes of the objects (2).
    Then, for every detected object, different features are computed, usually visual
    and motion ones (3). After that, an affinity computation step calculates the probability
    of two objects belonging to the same target (4), and finally an association step
    assigns a numerical ID to each object (5).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：MOT算法的常见工作流程：给定视频的原始帧（1），运行一个目标检测器以获得对象的边界框（2）。然后，对于每个检测到的对象，计算不同的特征，通常是视觉和运动特征（3）。之后，亲和度计算步骤计算两个对象属于同一目标的概率（4），最后，关联步骤为每个对象分配一个数字ID（5）。
- en: While these stages can be performed sequentially in the order presented here
    (often once per frame for online methods and once for the whole video for batch
    methods), there are many algorithms that merge some of these steps together, or
    intertwine them, or even perform them multiple times using different techniques
    (e.g. in algorithms that work in two phases). Moreover, some methods do not directly
    associate detections together, but use them to refine trajectory predictions and
    to manage initialization and termination of new tracks; nonetheless, many of the
    presented steps can often still be identified even in such cases, as we will see.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些阶段可以按这里所示的顺序依次执行（通常对在线方法每帧执行一次，对批处理方法整个视频执行一次），但许多算法将这些步骤中的一些合并在一起，或交织在一起，甚至使用不同的技术多次执行它们（例如，在两个阶段工作的算法）。此外，一些方法不会直接将检测关联在一起，而是使用它们来精炼轨迹预测，并管理新跟踪的初始化和终止；尽管如此，许多呈现的步骤即使在这种情况下也通常可以被识别，如我们将看到的。
- en: 2.2 Metrics
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 度量
- en: 'In order to provide a common experimental setup where algorithms can be fairly
    tested and compared, a group of metrics have been de facto established as standard,
    and they are used in almost every work. The most relevant ones are metrics defined
    by Wu and Nevatia [[19](#bib.bib19)], the so-called CLEAR MOT metrics [[20](#bib.bib20)],
    and recently the ID metrics [[21](#bib.bib21)]. These sets of metrics aim to reflect
    the overall performance of the tested models, and point out the possible drawbacks
    of each one. Therefore, those metrics are defined as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个公平测试和比较算法的共同实验设置，实际上已经建立了一组标准度量，并且它们在几乎每项工作中都被使用。最相关的度量包括吴和Nevatia定义的度量[[19](#bib.bib19)]，所谓的CLEAR
    MOT度量[[20](#bib.bib20)]，以及最近的ID度量[[21](#bib.bib21)]。这些度量集旨在反映测试模型的整体性能，并指出每个模型的可能缺陷。因此，这些度量的定义如下：
- en: Classical metrics
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经典度量
- en: 'These metrics, defined by Wu and Nevatia [[19](#bib.bib19)], highlight the
    different types of errors a MOT algorithm can make. In order to show those problems,
    the following values are computed:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标由吴和Nevatia定义[[19](#bib.bib19)]，突出了MOT算法可能出现的不同类型的错误。为了展示这些问题，计算了以下值：
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Mostly Tracked (MT) trajectories: number of ground-truth trajectories that
    are correctly tracked in at least 80% of the frames.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数跟踪（MT）轨迹：在至少80%的帧中被正确跟踪的地面真实轨迹的数量。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fragments: trajectory hypotheses which cover at most 80% of a ground truth
    trajectory. Observe that a true trajectory can be covered by more than one fragment.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 片段：覆盖最多80%的地面真实轨迹的轨迹假设。请注意，一个真实轨迹可能会被多个片段覆盖。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Mostly Lost (ML) trajectories: number of ground-truth trajectories that are
    correctly tracked in less than 20% of the frames.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数丢失（ML）轨迹：在不到20%的帧中被正确跟踪的地面真实轨迹的数量。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'False trajectories: predicted trajectories which do not correspond to a real
    object (i.e. to a ground truth trajectory).'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 错误轨迹：预测的轨迹与真实对象（即地面真实轨迹）不对应的情况。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ID switches: number of times when the object is correctly tracked, but the
    associated ID for the object is mistakenly changed.'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ID切换：对象被正确跟踪的次数，但对象的关联ID被错误地更改。
- en: CLEAR MOT metrics
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CLEAR MOT指标
- en: 'The CLEAR MOT metrics were developed for the Classification of Events, Activities
    and Relationships (CLEAR) workshops held in 2006 [[22](#bib.bib22)] and 2007 [[23](#bib.bib23)].
    The workshops were jointly organized by the the European CHIL project, the U.S.
    VACE project, and the National Institute of Standards and Technology (NIST). Those
    metrics are MOTA (Multiple Object Tracking Accuracy) and MOTP (Multiple Object
    Tracking Precision). They serve as a summary of other simpler metrics which compose
    them. We will explain the simpler metrics at first and build the complex ones
    over them. A detailed description on how to match the real objects (ground truth)
    with the tracker hypothesis can be found in [[20](#bib.bib20)], as it is not trivial
    how to consider when a hypothesis is related to an object, and it depends on the
    precise tracking task to be evaluated. In our case, as we are focusing on 2D tracking
    with single camera, the most used metric to decide whether an object and a prediction
    are related or not is Intersection over Union (IoU) of bounding boxes, as it was
    the measure established in the presentation paper of MOT15 dataset [[15](#bib.bib15)].
    Specifically, the mapping between ground truth and hypotheses is established as
    follows: if the ground truth object $o_{i}$ and the hypothesis $h_{j}$ are matched
    in frame $t-1$, and in frame $t$ the $IoU(o_{i},h_{j})\geq 0.5$, then $o_{i}$
    and $h_{j}$ are matched in that frame, even if there exists another hypothesis
    $h_{k}$ such that $IoU(o_{i},h_{j})<IoU(o_{i},h_{k})$, considering the continuity
    constraint. After the matching from previous frames has been performed, the remaining
    objects are tried to be matched with the remaining hypotheses, still using a 0.5
    IoU threshold. The ground truth bounding boxes that cannot be associated with
    a hypothesis are counted as false negatives (FN), and the hypotheses that cannot
    be associated with a real bounding box are marked as false positives (FP). Also,
    every time a ground truth object tracking is interrupted and later resumed is
    counted as a fragmentation, while every time a tracked ground truth object ID
    is incorrectly changed during the tracking duration is counted as an ID switch.
    Then, the simple metrics computed are the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: CLEAR MOT 指标是在 2006 [[22](#bib.bib22)] 和 2007 [[23](#bib.bib23)] 年举行的事件、活动和关系分类
    (CLEAR) 研讨会上开发的。这些研讨会由欧洲 CHIL 项目、美国 VACE 项目和国家标准与技术研究院 (NIST) 共同组织。这些指标包括 MOTA（多目标跟踪准确度）和
    MOTP（多目标跟踪精度）。它们作为其他简单指标的总结。我们将首先解释这些简单指标，并在其基础上构建复杂指标。关于如何将真实对象（真实框）与跟踪器假设匹配的详细描述可以在
    [[20](#bib.bib20)] 中找到，因为考虑一个假设是否与对象相关并不是简单的，这取决于要评估的精确跟踪任务。在我们的案例中，由于我们专注于使用单摄像头进行
    2D 跟踪，最常用的度量标准是边界框的交并比 (IoU)，这也是 MOT15 数据集 [[15](#bib.bib15)] 介绍论文中确立的度量标准。具体而言，真实框与假设之间的映射如下：如果真实框对象
    $o_{i}$ 和假设 $h_{j}$ 在帧 $t-1$ 中匹配，并且在帧 $t$ 中 $IoU(o_{i},h_{j})\geq 0.5$，则 $o_{i}$
    和 $h_{j}$ 在该帧中匹配，即使存在另一个假设 $h_{k}$，使得 $IoU(o_{i},h_{j})<IoU(o_{i},h_{k})$，考虑到连续性约束。在完成前几帧的匹配后，剩余的对象会尝试与剩余的假设匹配，仍使用
    0.5 的 IoU 阈值。无法与假设关联的真实框被计为假阴性 (FN)，而无法与真实框关联的假设被标记为假阳性 (FP)。此外，每次真实框对象跟踪中断并稍后恢复的情况被计为碎片化，每次跟踪过程中真实框对象
    ID 被错误更改的情况被计为 ID 切换。然后，计算出的简单指标如下：
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FP: the number of false positives in the whole video;'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FP：整个视频中的假阳性数量；
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FN: the number of false negatives in the whole video;'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FN：整个视频中的假阴性数量；
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fragm: the total number of fragmentations;'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Fragm：碎片化的总数量；
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'IDSW: the total number of ID switches.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: IDSW：ID 切换的总数量。
- en: 'The MOTA score is then defined as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: MOTA 分数定义如下：
- en: '|  | $\mathit{MOTA}=1-\frac{(\mathit{FN}+\mathit{FP}+\mathit{IDSW})}{\mathit{GT}}\quad\in(-\infty,1]$
    |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathit{MOTA}=1-\frac{(\mathit{FN}+\mathit{FP}+\mathit{IDSW})}{\mathit{GT}}\quad\in(-\infty,1]$
    |  |'
- en: 'where $GT$ is the number of ground truth boxes. It is important to note that
    the score can be negative, as the algorithm can commit a number of errors greater
    than the number of ground truth boxes. Usually, instead of reporting MOTA, it
    is common to report the percentage MOTA, which is just the previous expression
    expressed as a percentage. On the other hand, MOTP is computed as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $GT$ 是真实框的数量。重要的是要注意，分数可能为负，因为算法可能会犯比真实框数量更多的错误。通常，不报告 MOTA，而是报告百分比 MOTA，它只是前述表达式的百分比形式。另一方面，MOTP
    的计算公式为：
- en: '|  | $\mathit{MOTP}=\frac{\sum_{t,i}d_{t,i}}{\sum_{t}c_{t}}$ |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathit{MOTP}=\frac{\sum_{t,i}d_{t,i}}{\sum_{t}c_{t}}$ |  |'
- en: where $c_{t}$ denotes the number of matches in frame $t$, and $d_{t,i}$ is the
    bounding box overlap between the hypothesis $i$ with its assigned ground truth
    object. It is important to note that this metric takes few information about tracking
    into account, and rather focuses on the quality of the detections.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$c_{t}$表示帧$t$中的匹配数，$d_{t,i}$是假设$i$与其分配的真实目标之间的边界框重叠。需要注意的是，这一指标考虑了很少的追踪信息，而是专注于检测的质量。
- en: ID scores
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ID 评分
- en: 'The main problem of MOTA score is that it takes into account the number of
    times a tracker makes an incorrect decision, such as an ID switch, but in some
    scenarios (e.g. airport security) one could be more interested in rewarding a
    tracker that can follow an object for the longest time possible, in order to not
    lose its position. Because of that, in [[21](#bib.bib21)] a couple of alternative
    new metrics are defined, that are supposed to complement the information given
    by the CLEAR MOT metrics. Instead of matching ground truth and detections frame
    by frame, the mapping is performed globally, and the trajectory hypothesis assigned
    to a given ground truth trajectory is the one that maximizes the number of frames
    correctly classified for the ground truth. In order to solve that problem, a bipartite
    graph is constructed, and the minimum cost solution for that problem is taken
    as the problem solution. For the bipartite graph, the sets of vertices are defined
    as follows: the first set of vertices, $V_{T}$, has a so-called regular node for
    each true trajectory, and a false positive node for each computed trajectory.
    The second set, $V_{C}$, has a regular node for each computed trajectory and a
    false negative for each true one. The costs of the edges are set in order to count
    the number of false negative and false positive frames in case that edge were
    chosen (more information can be found in [[21](#bib.bib21)]). After the association
    is performed, there are four different possible pairs, attending to the nature
    of the involved nodes. If a regular node from $V_{T}$ is matched with a regular
    node of $V_{C}$ (i.e. a true trajectory is matched with a computed trajectory),
    a true positive ID is counted. Every false positive from $V_{T}$ matched with
    a regular node from $V_{C}$ counts as a false positive ID. Every regular node
    from $V_{T}$ matched with a false negative from $V_{C}$ counts as a false negative
    ID, and finally, every false positive matched with a false negative counts as
    a true negative ID. Afterwards, three scores are calculated. IDTP is the sum of
    the weights of the edges selected as true positive ID matches (it can be seen
    as the percentage of detections correctly assigned in the whole video). IDFN is
    the sum of weights from the selected false negative ID edges, and IDFP is the
    sum of weights from the selected false positive ID edges. With these three basic
    measures, another three measures are computed:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: MOTA 评分的主要问题在于，它考虑了追踪器做出错误决策的次数，例如 ID 切换，但在一些场景中（例如机场安检），人们可能更关注奖励那些能够尽可能长时间跟踪一个目标的追踪器，以免丢失目标的位置。因此，在[[21](#bib.bib21)]中定义了几个新的替代指标，这些指标旨在补充
    CLEAR MOT 指标提供的信息。不同于逐帧匹配真实情况和检测，映射是全球性进行的，分配给特定真实轨迹的轨迹假设是最大化正确分类的帧数的轨迹。为了解决这个问题，构建了一个二分图，并以该问题的最小成本解作为问题解决方案。对于二分图，顶点集合定义如下：第一个顶点集合$V_{T}$为每个真实轨迹设有一个所谓的常规节点，为每个计算轨迹设有一个假阳性节点。第二个集合$V_{C}$为每个计算轨迹设有一个常规节点，为每个真实轨迹设有一个假阴性节点。边的成本设置为计算假阴性和假阳性帧的数量，以便选择该边（更多信息见[[21](#bib.bib21)]）。完成关联后，根据涉及节点的性质，可能有四种不同的配对。如果$V_{T}$中的常规节点与$V_{C}$中的常规节点匹配（即真实轨迹与计算轨迹匹配），则计为真正正
    ID。每个$V_{T}$中的假阳性与$V_{C}$中的常规节点匹配计为假阳性 ID。每个$V_{T}$中的常规节点与$V_{C}$中的假阴性匹配计为假阴性
    ID，最后，每个假阳性与假阴性匹配计为真正负 ID。之后，计算三个分数。IDTP 是作为真正正 ID 匹配的边的权重之和（可以视为整个视频中正确分配的检测百分比）。IDFN
    是选定假阴性 ID 边的权重之和，IDFP 是选定假阳性 ID 边的权重之和。通过这三种基本度量，计算出另外三种度量：
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Identification precision: $\mathit{IDP}=\frac{\mathit{IDTP}}{\mathit{IDTP}+\mathit{IDFP}}$'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别精度：$\mathit{IDP}=\frac{\mathit{IDTP}}{\mathit{IDTP}+\mathit{IDFP}}$
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Identification recall: $\mathit{IDR}=\frac{\mathit{IDTP}}{\mathit{IDTP}+\mathit{IDFN}}$'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别召回率：$\mathit{IDR}=\frac{\mathit{IDTP}}{\mathit{IDTP}+\mathit{IDFN}}$
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Identification F1: $\mathit{IDF1}=\frac{2}{\frac{1}{\mathit{IDP}}+\frac{1}{\mathit{IDR}}}=\frac{2\mathit{IDTP}}{2\mathit{IDTP}+\mathit{IDFP}+\mathit{IDFN}}$'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别F1：$\mathit{IDF1}=\frac{2}{\frac{1}{\mathit{IDP}}+\frac{1}{\mathit{IDR}}}=\frac{2\mathit{IDTP}}{2\mathit{IDTP}+\mathit{IDFP}+\mathit{IDFN}}$
- en: 'Usually, the reported metrics in almost every piece of work are the CLEAR MOT
    metrics, mostly tracked trajectories (MT), mostly lost trajectories (ML) and IDF1,
    since this metrics are the ones shown in MOTChallenge leaderboards (see section
    [2.3](#S2.SS3 "2.3 Benchmark datasets ‣ 2 MOT: algorithms, metrics and datasets
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey") for details). Additionally,
    the number of frames per second (FPS) the tracker can process is often reported,
    and is also included in the leaderboards. However, we find this metric difficult
    to compare among different algorithms, since some of the methods include the detection
    phase while others skip that computation. Also, the dependency on the hardware
    employed is relevant in terms of speed.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，几乎所有工作的报告指标都是CLEAR MOT指标，包括主要跟踪轨迹（MT）、主要丢失轨迹（ML）和IDF1，因为这些指标是MOTChallenge排行榜中显示的（详细信息请参见[2.3](#S2.SS3
    "2.3 Benchmark datasets ‣ 2 MOT: algorithms, metrics and datasets ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey")）。此外，跟踪器可以处理的每秒帧数（FPS）也经常报告，并且也包括在排行榜中。然而，我们发现这个指标在不同算法之间难以比较，因为一些方法包括了检测阶段，而其他方法则跳过了这一步骤。同时，硬件的依赖在速度方面也很重要。'
- en: 2.3 Benchmark datasets
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 基准数据集
- en: In the past few years, a number of datasets for MOT have been published. In
    this section we are going to describe the most important ones, starting from a
    general description of the MOTChallenge benchmark, then focusing on its datasets,
    and finally describing KITTI and other less commonly used MOT datasets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，已经发布了许多MOT数据集。在这一部分，我们将描述最重要的数据集，从对MOTChallenge基准的总体描述开始，然后专注于其数据集，最后描述KITTI及其他使用较少的MOT数据集。
- en: 'MOTChallenge. MOTChallenge¹¹1[https://motchallenge.net/](https://motchallenge.net/)
    is the most commonly used benchmark for multiple object tracking. It provides,
    among others, some of largest datasets for pedestrian tracking that are currently
    publicly available. For each dataset, the ground truth for the training split,
    and detections for both training and test splits are provided. The reason why
    MOTChallenge datasets frequently provide detections (often referred to as public
    detections, as opposed to the private detections, that are obtained by the algorithm
    authors by using a detector of their own) is that the detection quality has a
    big impact on the final performance of the tracker, but the detection part of
    the algorithms is often independent from the tracking part and usually uses already
    existing models; providing public detections that every model can use makes the
    comparison of the tracking algorithms easier, since the detection quality is factored
    out from the performance computation and trackers start on a common ground. The
    evaluation of an algorithm on the test dataset is done by submitting the results
    to a test server. The MOTChallenge website contains a leaderboard for each of
    the datasets, showing in separate pages models using the publicly provided detections
    and the ones using private detections. Online methods are also marked as so. MOTA
    is the primary evaluation score for the MOTChallenge, but many other metrics are
    shown, including all the ones presented in section [2.2](#S2.SS2 "2.2 Metrics
    ‣ 2 MOT: algorithms, metrics and datasets ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey"). As we will see, since the vast majority of MOT algorithms
    that use deep learning focus on pedestrians, the MOTChallenge datasets are the
    most widely used, as they are the most comprehensive ones currently available,
    providing more data to train deep models.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 'MOTChallenge。MOTChallenge¹¹1[https://motchallenge.net/](https://motchallenge.net/)
    是最常用的多目标跟踪基准测试。它提供了当前公开的、一些最大的行人跟踪数据集。每个数据集都提供了训练集的真实标签和训练及测试集的检测结果。MOTChallenge
    数据集频繁提供检测结果（通常称为公开检测，与算法作者使用自己检测器获得的私人检测相对）是因为检测质量对跟踪器的最终性能有很大影响，而算法的检测部分通常独立于跟踪部分，通常使用现有模型；提供所有模型都可以使用的公共检测结果可以使跟踪算法的比较更容易，因为检测质量被从性能计算中剔除，跟踪器从共同的基础开始。对测试数据集进行算法评估是通过将结果提交到测试服务器来完成的。MOTChallenge
    网站包含每个数据集的排行榜，显示使用公开检测结果和使用私人检测结果的模型，在线方法也会标记出来。MOTA 是 MOTChallenge 的主要评估分数，但还显示了许多其他指标，包括第
    [2.2](#S2.SS2 "2.2 Metrics ‣ 2 MOT: algorithms, metrics and datasets ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey") 节中介绍的所有指标。如我们所见，由于绝大多数使用深度学习的 MOT 算法都专注于行人，MOTChallenge
    数据集是使用最广泛的，因为它们是当前最全面的数据集，提供了更多数据用于训练深度模型。'
- en: 'MOT15. The first MOTChallenge dataset is 2D MOT 2015²²2Dataset: [https://motchallenge.net/data/2D_MOT_2015/](https://motchallenge.net/data/2D_MOT_2015/),
    leaderboard: [https://motchallenge.net/results/2D_MOT_2015/](https://motchallenge.net/results/2D_MOT_2015/).
    [[15](#bib.bib15)] (often just called MOT15). It contains a series of 22 videos
    (11 for training and 11 for testing), collected from older datasets, with a variety
    of characteristics (fixed and moving cameras, different environments and lighting
    conditions, and so on) so that the models would need to generalize better in order
    to obtain good results on it. In total, it contains 11283 frames at various resolutions,
    with 1221 different identities and 101345 boxes. The provided detections were
    obtained using the ACF detector [[24](#bib.bib24)].'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MOT15。第一个 MOTChallenge 数据集是 2D MOT 2015²²2 数据集：[https://motchallenge.net/data/2D_MOT_2015/](https://motchallenge.net/data/2D_MOT_2015/)，排行榜：[https://motchallenge.net/results/2D_MOT_2015/](https://motchallenge.net/results/2D_MOT_2015/)。[[15](#bib.bib15)]（通常称为
    MOT15）。它包含一系列 22 个视频（11 个用于训练，11 个用于测试），这些视频来自较早的数据集，具有各种特征（固定和移动摄像机、不同环境和光照条件等），以便模型需要更好地进行泛化以获得良好的结果。总共包含
    11283 帧，具有 1221 个不同的身份和 101345 个框。提供的检测结果是使用 ACF 检测器 [[24](#bib.bib24)] 获得的。
- en: 'MOT16/17. A new version of the dataset was presented in 2016, called MOT16³³3Dataset:
    [https://motchallenge.net/data/MOT16/](https://motchallenge.net/data/MOT16/),
    leaderboard: [https://motchallenge.net/results/MOT16/](https://motchallenge.net/results/MOT16/).
    [[16](#bib.bib16)]. This time, the ground truth was made from scratch, so that
    it was consistent throughout the dataset. The videos are also more challenging,
    since they have a higher pedestrian density. A total of 14 videos are included
    in the set (7 for training and 7 for testing), with public detections obtained
    using the Deformable Part-based Model (DPM) v5 [[25](#bib.bib25), [26](#bib.bib26)],
    that they found to obtain better performance in detecting pedestrians on the dataset
    when compared to other models. This time the dataset includes 11235 frames with
    1342 identities and 292733 boxes in total. The MOT17 dataset⁴⁴4Dataset: [https://motchallenge.net/data/MOT17/](https://motchallenge.net/data/MOT17/),
    leaderboard: [https://motchallenge.net/results/MOT17/](https://motchallenge.net/results/MOT17/).
    includes the same videos as MOT16, but with more accurate ground truth and with
    three sets of detections for each video: one from Faster R-CNN [[4](#bib.bib4)],
    one from DPM and one from the Scale-Dependent Pooling detector (SDP) [[27](#bib.bib27)].
    The trackers would then have to prove to be versatile and robust enough to get
    a good performance using different detection qualities.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: MOT16/17。该数据集的新版本于2016年发布，称为MOT16³³3数据集：[https://motchallenge.net/data/MOT16/](https://motchallenge.net/data/MOT16/)，排行榜：[https://motchallenge.net/results/MOT16/](https://motchallenge.net/results/MOT16/)。[[16](#bib.bib16)]。这次，地面真实数据从零开始制作，因此在整个数据集中保持一致。视频也更加具有挑战性，因为它们的行人密度更高。该数据集总共包含14个视频（7个用于训练，7个用于测试），其中公共检测是使用Deformable
    Part-based Model (DPM) v5 [[25](#bib.bib25)，[26](#bib.bib26)] 获得的，发现该模型在检测数据集中的行人时比其他模型表现更好。这次的数据集包含11235帧，总共有1342个身份和292733个框。MOT17数据集⁴⁴4数据集：[https://motchallenge.net/data/MOT17/](https://motchallenge.net/data/MOT17/)，排行榜：[https://motchallenge.net/results/MOT17/](https://motchallenge.net/results/MOT17/)，包含了与MOT16相同的视频，但地面真实数据更加准确，并且每个视频有三组检测结果：一组来自Faster
    R-CNN [[4](#bib.bib4)]，一组来自DPM，另一组来自Scale-Dependent Pooling探测器 (SDP) [[27](#bib.bib27)]。跟踪器需要证明自己具有足够的多样性和鲁棒性，以便在不同的检测质量下获得良好的性能。
- en: MOT19. Very recently, a new version of the dataset for the CVPR 2019 Tracking
    Challenge⁵⁵5[https://motchallenge.net/workshops/bmtt2019/tracking.html](https://motchallenge.net/workshops/bmtt2019/tracking.html)
    has been released, containing 8 videos (4 for training, 4 for testing) with extremely
    high pedestrian density, reaching up to 245 pedestrians per frame on average in
    the most crowded video. The dataset contains 13410 frames with 6869 tracks and
    a total of 2259143 boxes, much more than the previous datasets. While submissions
    for this dataset have only been allowed for a limited amount of time, this data
    will be the basis for the release of MOT19 in late 2019 [[28](#bib.bib28)].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: MOT19。最近，为CVPR 2019 Tracking Challenge⁵⁵5[https://motchallenge.net/workshops/bmtt2019/tracking.html](https://motchallenge.net/workshops/bmtt2019/tracking.html)发布了一个新版本的数据集，其中包含8个视频（4个用于训练，4个用于测试），具有极高的行人密度，在最拥挤的视频中，每帧平均达到245个行人。该数据集包含13410帧，6869个轨迹，总共有2259143个框，比以前的数据集多得多。虽然对该数据集的提交仅允许有限的时间，但这些数据将成为2019年末MOT19发布的基础[[28](#bib.bib28)]。
- en: KITTI. While the MOTChallenge datasets focus on pedestrian tracking, the KITTI
    tracking benchmark⁶⁶6[http://www.cvlibs.net/datasets/kitti/eval_tracking.php](http://www.cvlibs.net/datasets/kitti/eval_tracking.php)
    [[29](#bib.bib29), [30](#bib.bib30)] allows for tracking of both people and vehicles.
    The dataset was collected by driving a car around a city and it was released in
    2012\. It consists of 21 training videos and 29 test ones, with a total of about
    19000 frames (32 minutes). It includes detections obtained using the DPM⁷⁷7The
    website says the detections were obtained using a model based on a latent SVM,
    or L-SVM. That model is now known as Deformable Parts Model (DPM). and RegionLets⁸⁸8[http://www.xiaoyumu.com/project/detection](http://www.xiaoyumu.com/project/detection)
    [[31](#bib.bib31)] detectors, as well as stereo and laser information; however,
    as explained, in this survey we are only going to focus on models using 2D images.
    The CLEAR MOT metrics, MT, ML, ID switches and fragmentations are used to evaluate
    the methods. It is possible to submit results only for pedestrians or only for
    cars, and two different leaderboards are maintained for the two classes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: KITTI. 虽然MOTChallenge数据集侧重于行人跟踪，但KITTI跟踪基准⁶⁶6[http://www.cvlibs.net/datasets/kitti/eval_tracking.php](http://www.cvlibs.net/datasets/kitti/eval_tracking.php)
    [[29](#bib.bib29), [30](#bib.bib30)] 可以进行人和车辆的跟踪。该数据集是通过驾驶汽车在城市中收集的，并于2012年发布。它包括21个训练视频和29个测试视频，总共约有19000帧（32分钟）。它包含了使用DPM⁷⁷7网站上称这些检测结果是通过基于潜在SVM或L-SVM的模型获得的。这个模型现在被称为可变形零件模型（DPM）。和RegionLets⁸⁸8[http://www.xiaoyumu.com/project/detection](http://www.xiaoyumu.com/project/detection)
    [[31](#bib.bib31)]检测器获得的检测结果，以及立体和激光信息。然而，正如解释的那样，在本调研中我们只关注使用2D图像的模型。使用CLEAR
    MOT指标进行评估的方法包括MT、ML、ID切换和分段。可以只提交关于行人或车辆的结果，两个不同的排行榜分别针对两个分类。
- en: Other datasets. Besides the previously described datasets, there is a number
    of older, and now less frequently used, ones. Among those we can find the UA-DETRAC
    tracking benchmark⁹⁹9[https://detrac-db.rit.albany.edu/Tracking](https://detrac-db.rit.albany.edu/Tracking)
    [[32](#bib.bib32)], that focuses on vehicles tracked from traffic cameras, and
    the TUD^(10)^(10)10[https://www.d2.mpi-inf.mpg.de/node/428](https://www.d2.mpi-inf.mpg.de/node/428)
    [[33](#bib.bib33)] and PETS2009^(11)^(11)11[http://www.cvg.reading.ac.uk/PETS2009/a.html](http://www.cvg.reading.ac.uk/PETS2009/a.html)
    [[34](#bib.bib34)] datasets, that both focus on pedestrians. Many of their videos
    are now part of the MOTChallenge datasets.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其他数据集。除了之前描述的数据集外，还有一些较旧且现在使用较少的数据集。其中包括UA-DETRAC跟踪基准⁹⁹9[https://detrac-db.rit.albany.edu/Tracking](https://detrac-db.rit.albany.edu/Tracking)
    [[32](#bib.bib32)]，侧重于从交通摄像头跟踪的车辆，以及TUD^(10)^(10)10[https://www.d2.mpi-inf.mpg.de/node/428](https://www.d2.mpi-inf.mpg.de/node/428)
    [[33](#bib.bib33)]和PETS2009^(11)^(11)11[http://www.cvg.reading.ac.uk/PETS2009/a.html](http://www.cvg.reading.ac.uk/PETS2009/a.html)
    [[34](#bib.bib34)]数据集，这两个数据集都侧重于行人。其中许多视频现在已成为MOTChallenge数据集的一部分。
- en: 3 Deep learning in MOT
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 MOT中的深度学习
- en: 'As this survey focuses on the use of deep learning in the MOT task, we organize
    this section into five subsections. Each of the first four subsections provides
    a review on how deep learning is exploited in each one of the four MOT stages
    defined previously^(12)^(12)12Note that the classification of the models should
    not be considered as a strict categorization, since it’s not rare that one of
    them has been used for multiple purposes and drawing a line is sometimes difficult.
    For example, some deep learning models, Siamese networks in particular, are often
    trained to output an affinity score, but at inference time they are only used
    to extract ‘association features’, and a simple hardcoded distance measure is
    then used instead to compute the affinities. In those cases, we decided to consider
    the network as performing feature extraction, since the similarity measure is
    not directly learned. However, those models could have also been considered to
    use deep learning for affinity computation.. Subsection [3.4](#S3.SS4 "3.4 DL
    in Association/Tracking step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey"), besides presenting the use of deep learning
    in the association process, will also include its use in the overall track management
    process (e.g. initialization/termination of tracks), since it is strictly linked
    to the association step. Subsection [3.5](#S3.SS5 "3.5 Other uses of DL in MOT
    ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")
    will finally describe uses of deep learning in MOT that do not fit into the four-step
    scheme.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '由于本调查关注于深度学习在多目标跟踪（MOT）任务中的应用，我们将本节组织为五个子节。前四个子节分别回顾了深度学习在之前定义的四个MOT阶段中的应用^(12)^(12)12请注意，模型的分类不应被视为严格的分类，因为一个模型经常用于多个目的，因此划分界限有时是困难的。例如，一些深度学习模型，特别是孪生网络，通常被训练以输出一个亲和力评分，但在推理时它们仅用于提取“关联特征”，然后使用简单的硬编码距离度量来计算亲和力。在这些情况下，我们决定将网络视为执行特征提取，因为相似度度量并没有直接学习。然而，这些模型也可以被认为是用于亲和力计算的深度学习。子节
    [3.4](#S3.SS4 "3.4 DL in Association/Tracking step ‣ 3 Deep learning in MOT ‣
    Deep Learning in Video Multi-Object Tracking: A Survey") 除了介绍深度学习在关联过程中的应用外，还将包括其在整体跟踪管理过程中的应用（例如，跟踪的初始化/终止），因为它与关联步骤密切相关。子节
    [3.5](#S3.SS5 "3.5 Other uses of DL in MOT ‣ 3 Deep learning in MOT ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey") 将最终描述不符合四步骤方案的深度学习在MOT中的其他应用。'
- en: 'We have included a summary table in [A](#A1 "Appendix A Appendix ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey") that shows the main techniques used
    in each of the four steps in each paper presented in this survey. The mode of
    operation (batch vs. online) is indicated and a link to the source code or to
    other provided material is also included (when available).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 [A](#A1 "Appendix A Appendix ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey") 中包含了一个总结表，展示了本调查中每篇论文在四个步骤中使用的主要技术。操作模式（批处理与在线）已被标明，并且包含了源代码或其他提供材料的链接（如有）。'
- en: 3.1 DL in detection step
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 深度学习在检测步骤中的应用
- en: While many works have used as input to their algorithms dataset-provided detections
    generated by various detectors (for example Aggregated Channel Features [[24](#bib.bib24)]
    for MOT15 [[15](#bib.bib15)] or Deformable Parts Model [[25](#bib.bib25)] for
    MOT16 [[16](#bib.bib16)]), there have also been algorithms that integrated a custom
    detection step, that often contributed to improve the overall tracking performance
    by enhancing the detection quality.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多工作将各种检测器生成的数据集提供的检测结果作为其算法的输入（例如 Aggregated Channel Features [[24](#bib.bib24)]
    用于 MOT15 [[15](#bib.bib15)] 或 Deformable Parts Model [[25](#bib.bib25)] 用于 MOT16
    [[16](#bib.bib16)]），也有一些算法集成了自定义检测步骤，这通常通过提高检测质量来改善整体跟踪性能。
- en: 'As we will see, most of the algorithms that employed custom detections made
    use of Faster R-CNN and its variants (section [3.1.1](#S3.SS1.SSS1 "3.1.1 Faster
    R-CNN ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey")) or SSD (section [3.1.2](#S3.SS1.SSS2 "3.1.2
    SSD ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey")), but approaches that used different models
    also exist (section [3.1.3](#S3.SS1.SSS3 "3.1.3 Other detectors ‣ 3.1 DL in detection
    step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey")). Despite the vast majority of algorithms utilized deep learning models
    to extract rectangular bounding boxes, a few works made a different use of deep
    networks in the detection step: these works are the focus of section [3.1.4](#S3.SS1.SSS4
    "3.1.4 Other uses of CNNs in the detection step ‣ 3.1 DL in detection step ‣ 3
    Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '正如我们所见，大多数采用自定义检测的算法使用了 Faster R-CNN 及其变体（第 [3.1.1](#S3.SS1.SSS1 "3.1.1 Faster
    R-CNN ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey") 节）或 SSD（第 [3.1.2](#S3.SS1.SSS2 "3.1.2 SSD ‣
    3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") 节），但也存在使用不同模型的方法（第 [3.1.3](#S3.SS1.SSS3 "3.1.3 Other detectors
    ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") 节）。尽管绝大多数算法利用深度学习模型来提取矩形边界框，但少数研究在检测步骤中对深度网络进行了不同的使用：这些研究是第
    [3.1.4](#S3.SS1.SSS4 "3.1.4 Other uses of CNNs in the detection step ‣ 3.1 DL
    in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") 节的重点。'
- en: '![Refer to caption](img/1a82b20fd99699edb989c72323a4c53c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1a82b20fd99699edb989c72323a4c53c.png)'
- en: 'Figure 3: Example of a deep learning based detector (Faster R-CNN architecture
    [[4](#bib.bib4)])'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于深度学习的检测器示例（Faster R-CNN 架构 [[4](#bib.bib4)]）
- en: 3.1.1 Faster R-CNN
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 Faster R-CNN
- en: 'The Simple Online and Realtime Tracking (SORT) algorithm [[35](#bib.bib35)]
    has been one of the first MOT pipelines to leverage convolutional neural networks
    for the detection of pedestrians. Bewley et al. showed that replacing detections
    obtained using Aggregated Channel Features (ACF) [[24](#bib.bib24)] with detections
    computed by Faster R-CNN [[4](#bib.bib4)] (illustrated in figure [3](#S3.F3 "Figure
    3 ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey")) could improve the MOTA score by 18.9% (absolute
    change) on the MOT15 dataset [[15](#bib.bib15)]. They used a relatively simple
    approach that consisted in predicting object motion using the Kalman filter [[36](#bib.bib36)]
    and then associating the detections together with the help of the Hungarian algorithm
    [[37](#bib.bib37)], using intersection-over-union (IoU) distances to compute the
    cost matrix. At the time of publishing, SORT was ranked as the best-performing
    open source algorithm on the MOT15 dataset.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 'Simple Online and Realtime Tracking (SORT) 算法 [[35](#bib.bib35)] 是最早利用卷积神经网络进行行人检测的
    MOT 流水线之一。Bewley 等人展示了用 Faster R-CNN [[4](#bib.bib4)]（如图 [3](#S3.F3 "Figure 3
    ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") 所示）替换使用 Aggregated Channel Features (ACF) [[24](#bib.bib24)]
    获得的检测结果，可以使 MOT15 数据集 [[15](#bib.bib15)] 的 MOTA 得分提高 18.9%（绝对变化）。他们使用了一种相对简单的方法，包括利用
    Kalman 滤波器 [[36](#bib.bib36)] 预测物体运动，然后借助匈牙利算法 [[37](#bib.bib37)] 将检测结果关联起来，使用交并比（IoU）距离来计算成本矩阵。在发布时，SORT
    被评为 MOT15 数据集上表现最好的开源算法。'
- en: 'Yu et al. reached the same conclusions in [[38](#bib.bib38)] using a modified
    Faster R-CNN, that included skip-pooling [[39](#bib.bib39)] and multi-region features
    [[40](#bib.bib40)] and that was fine-tuned on multiple pedestrian detection datasets.
    With this architecture they were able to improve the performance of the algorithm
    they proposed (see section [3.2.2](#S3.SS2.SSS2 "3.2.2 CNNs as visual feature
    extractors ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")) by more than
    30% (absolute change, measured in MOTA), reaching state-of-the-art performance
    on the MOT16 dataset [[16](#bib.bib16)]. They also showed that having higher-quality
    detections reduces the need of complex tracking algorithms while still obtaining
    similar results: this is because the MOTA score is heavily influenced by the amount
    of false positives and false negatives, and using accurate detections is an effective
    way of reducing both. The detections computed by [[38](#bib.bib38)] on the MOT16
    dataset have also been made available to the public^(13)^(13)13[https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view](https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view)
    and many MOT algorithms have since exploited them [[41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)].'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '于等人在[[38](#bib.bib38)]中得出了相同的结论，使用了经过修改的Faster R-CNN，该版本包含了skip-pooling [[39](#bib.bib39)]和多区域特征[[40](#bib.bib40)]，并在多个行人检测数据集上进行了微调。通过这种架构，他们能够将他们提出的算法的性能（见章节[3.2.2](#S3.SS2.SSS2
    "3.2.2 CNNs as visual feature extractors ‣ 3.2 DL in feature extraction and motion
    prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey")）提升超过30%（绝对变化，以MOTA为测量标准），在MOT16数据集上达到了最先进的性能[[16](#bib.bib16)]。他们还表明，提高检测质量可以减少对复杂跟踪算法的需求，同时仍然获得类似的结果：这是因为MOTA得分受虚假正例和虚假负例的数量影响很大，使用准确的检测是减少这两者的有效方法。[[38](#bib.bib38)]在MOT16数据集上计算的检测结果也已公开^(13)^(13)13[https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view](https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view)，许多MOT算法随后利用了这些结果[[41](#bib.bib41)、[42](#bib.bib42)、[43](#bib.bib43)、[44](#bib.bib44)、[45](#bib.bib45)、[46](#bib.bib46)、[47](#bib.bib47)、[48](#bib.bib48)、[49](#bib.bib49)、[50](#bib.bib50)、[51](#bib.bib51)]。'
- en: In the following years, other works have taken advantage of the detection accuracy
    of Faster R-CNN, that has since been applied as part of MOT algorithms to detect
    athletes [[52](#bib.bib52)], cells [[53](#bib.bib53)] and pigs [[54](#bib.bib54)].
    Moreover, an adaptation of Faster R-CNN that adds a segmentation branch, Mask
    R-CNN [[17](#bib.bib17)], has been used for example by Zhou et al. [[55](#bib.bib55)]
    both to detect and to track pedestrians,
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的几年里，其他研究利用了Faster R-CNN的检测精度，该技术随后被应用于多目标跟踪算法中，用于检测运动员[[52](#bib.bib52)]、细胞[[53](#bib.bib53)]和猪[[54](#bib.bib54)]。此外，Faster
    R-CNN的一个改编版本，即Mask R-CNN [[17](#bib.bib17)]，例如被周等人[[55](#bib.bib55)]用来同时检测和跟踪行人，
- en: 3.1.2 SSD
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 SSD
- en: 'The SSD [[5](#bib.bib5)] detector is another commonly used network in the detection
    step. In particular, Zhang et al. [[54](#bib.bib54)] compared it with Faster R-CNN
    and R-FCN [[18](#bib.bib18)] in their pig tracking pipeline, showing that it worked
    better on their dataset. They employed a Discriminative Correlation Filters (DCF)
    based online tracking method [[56](#bib.bib56)] with the use of HOG [[57](#bib.bib57)]
    and Colour Names [[58](#bib.bib58)] features to predict the position of so-called
    tag-boxes, small regions around the center of each animal. The Hungarian algorithm
    was used for the association between tracked tag-boxes and detections, and in
    the case of tracking failure the output of the DCF tracker was used to refine
    the bounding boxes. Lu et al. [[59](#bib.bib59)] also used SSD, but in this case
    to detect a variety of object classes to track (people, animals, cars, etc., see
    section [3.2.4](#S3.SS2.SSS4 "3.2.4 More complex approaches for visual feature
    extraction ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: SSD [[5](#bib.bib5)]检测器是检测步骤中另一种常用的网络。特别是，Zhang等人[[54](#bib.bib54)]在他们的猪跟踪管道中将其与Faster
    R-CNN和R-FCN [[18](#bib.bib18)]进行了比较，表明它在他们的数据集上表现更好。他们采用了基于判别相关滤波器（DCF）的在线跟踪方法[[56](#bib.bib56)]，使用HOG
    [[57](#bib.bib57)]和颜色名称[[58](#bib.bib58)]特征预测所谓的标签框的位置，即每个动物中心周围的小区域。匈牙利算法用于跟踪标签框和检测之间的关联，在跟踪失败的情况下，使用DCF跟踪器的输出细化边界框。Lu等人[[59](#bib.bib59)]也使用了SSD，但在这种情况下是为了检测各种物体类别进行跟踪（如人、动物、汽车等，见[3.2.4](#S3.SS2.SSS4
    "3.2.4 更复杂的视觉特征提取方法 ‣ 3.2 深度学习在特征提取和运动预测中的应用 ‣ 3 深度学习在多目标跟踪中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")）。
- en: Some works have tried to refine the detections obtained with SSD by taking into
    account the information obtained in other steps of the tracking algorithm. Kieritz
    et al. [[60](#bib.bib60)], in their joint detection and tracking framework, used
    the affinity scores computed between tracks and detections to replace the standard
    Non-Maximum Suppression (NMS) step included in the SSD network with a version
    that refines detection confidence scores based on their correspondence to tracked
    targets.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究尝试通过考虑跟踪算法其他步骤中获得的信息来改进SSD获得的检测结果。Kieritz等人[[60](#bib.bib60)]在他们的联合检测和跟踪框架中，使用了在轨迹和检测之间计算的亲和度分数，来替换SSD网络中包含的标准非极大值抑制（NMS）步骤，改用一种基于与跟踪目标的对应关系来改进检测置信度分数的版本。
- en: Zhao et al. [[61](#bib.bib61)] instead employed the SSD detector to search for
    pedestrians and vehicles in a scene, but they used a CNN-based Correlation Filter
    (CCF) to allow SSD to generate more accurate bounding boxes. The CCF exploited
    PCA-compressed [[62](#bib.bib62)] CNN features to predict the position of a target
    in the subsequent frame; the predicted position was then used to crop a ROI (Region
    Of Interest) around it, that was given as input to SSD. In that way, the network
    was able to compute small detections using deeper layers, that extract more valuable
    semantic information and that are thus known to produce more accurate bounding
    boxes and less false negatives. The algorithm then combined these detections with
    the ones obtained on the full image with a NMS step and then association between
    tracks and detections was performed using the Hungarian algorithm, with a cost
    matrix that took into account geometric (IoU) and appearance (Average Peak-to-Correlation
    Energy - APCE [[63](#bib.bib63)]) cues. APCE was also used for an object re-identification
    (ReID) step, to recover from occlusions. The authors showed that training a detector
    with multi-scale augmentation could lead to much better performance in tracking
    and the algorithm reached accuracy comparable to state-of-the-art online algorithms
    on KITTI and MOT15.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao等人[[61](#bib.bib61)]则采用了SSD检测器来搜索场景中的行人和车辆，但他们使用了基于CNN的相关滤波器（CCF）来使SSD生成更精确的边界框。CCF利用PCA压缩的[[62](#bib.bib62)]CNN特征预测目标在后续帧中的位置；然后将预测的位置用于裁剪一个感兴趣区域（ROI），该区域作为SSD的输入。这样，网络能够使用更深层的特征进行小检测，这些层提取了更有价值的语义信息，从而已知能够产生更准确的边界框并减少假阴性。该算法随后将这些检测结果与在全图像上获得的结果结合，通过NMS步骤进行组合，然后使用匈牙利算法进行轨迹和检测之间的关联，成本矩阵考虑了几何（IoU）和外观（平均峰值相关能量
    - APCE [[63](#bib.bib63)])线索。APCE还用于物体重新识别（ReID）步骤，以从遮挡中恢复。作者展示了使用多尺度增强训练检测器可以显著提高跟踪性能，该算法在KITTI和MOT15上达到了与最先进的在线算法相当的准确度。
- en: 3.1.3 Other detectors
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 其他检测器
- en: 'Among the other CNN models used as detectors in MOT, we can mention the YOLO
    series of detectors [[64](#bib.bib64), [6](#bib.bib6), [65](#bib.bib65)]; in particular,
    YOLOv2 has been used by Kim et al. [[66](#bib.bib66)] also to detect pedestrians.
    Sharma et al. [[67](#bib.bib67)] used instead a Recurrent Rolling Convolution
    (RRC) CNN [[68](#bib.bib68)] and a SubCNN [[69](#bib.bib69)] to detect vehicles
    in videos recorded on a moving camera in the context of autonomous driving (see
    section [3.2.4](#S3.SS2.SSS4 "3.2.4 More complex approaches for visual feature
    extraction ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")). Pernici et
    al. [[70](#bib.bib70)] used the Tiny CNN detector [[71](#bib.bib71)] in their
    face tracking algorithm, obtaining a better performance when compared to the Deformable
    Parts Model detector (DPM) [[25](#bib.bib25)], that does not use deep learning
    techniques.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MOT 中使用的其他 CNN 模型中，我们可以提到 YOLO 系列检测器 [[64](#bib.bib64), [6](#bib.bib6), [65](#bib.bib65)]；特别是，YOLOv2
    被 Kim 等人 [[66](#bib.bib66)] 用于检测行人。Sharma 等人 [[67](#bib.bib67)] 则使用了递归卷积（RRC）CNN
    [[68](#bib.bib68)] 和 SubCNN [[69](#bib.bib69)] 来检测在移动相机上录制的视频中的车辆，应用于自动驾驶的背景下（见第
    [3.2.4 节](#S3.SS2.SSS4 "3.2.4 更复杂的视觉特征提取方法 ‣ 3.2 深度学习在特征提取和运动预测中的应用 ‣ 3 MOT 中的深度学习
    ‣ 深度学习在视频多目标跟踪中的应用：综述")）。Pernici 等人 [[70](#bib.bib70)] 在他们的面部跟踪算法中使用了 Tiny CNN
    检测器 [[71](#bib.bib71)]，与不使用深度学习技术的可变形部件模型检测器（DPM） [[25](#bib.bib25)] 相比，取得了更好的性能。
- en: 3.1.4 Other uses of CNNs in the detection step
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 CNN 在检测步骤中的其他用途
- en: Sometimes CNNs have been employed in the MOT detection step for uses other than
    directly computing object bounding boxes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，CNN 被用于 MOT 检测步骤中的其他用途，而不仅仅是直接计算物体的边界框。
- en: For example, CNNs have been exploited to reduce false positives in [[72](#bib.bib72)],
    where vehicle detections were obtained with a modified version of the ViBe algorithm
    [[73](#bib.bib73)] that performed background subtraction on the input. These detections
    were first given as input to a SVM [[74](#bib.bib74)] and, in case the SVM was
    not confident enough to either discard or confirm them, a Faster-CNN based network
    [[75](#bib.bib75)] would then be used to decide whether to keep or discard each
    of them. In this way, only a few objects would have to be analyzed by the CNN,
    making the detection step faster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，CNN 被用于减少假阳性 [[72](#bib.bib72)]，其中车辆检测是通过对输入进行背景减除的 ViBe 算法的修改版本 [[73](#bib.bib73)]
    获得的。这些检测结果首先作为输入提供给 SVM [[74](#bib.bib74)]，如果 SVM 对是否丢弃或确认这些检测结果不够自信，则使用基于 Faster-CNN
    的网络 [[75](#bib.bib75)] 来决定是否保留或丢弃每一个检测结果。这样，只有少量物体需要由 CNN 分析，从而使检测步骤变得更快。
- en: Bullinger et al. explored a different approach in [[76](#bib.bib76)], where
    instead of computing classical bounding boxes in the detection step, a Multi-task
    Network Cascade [[77](#bib.bib77)] was instead employed to obtain instance-aware
    semantic segmentation maps. The authors argue that since the 2D shape of instances,
    differently from rectangular bounding boxes, do not contain background structures
    or parts of other objects, optical flow based tracking algorithms would perform
    better, especially when the target position in the image is also subject to camera
    motion in addition to the object’s own motion. After obtaining the segmentation
    maps for the various instances present in the current frame, an optical flow method
    ([[78](#bib.bib78), [79](#bib.bib79), [80](#bib.bib80)]) was applied to predict
    the position and shape of each instance in the next frame. An affinity matrix
    between predicted and detected instances was then computed and given as input
    to the Hungarian algorithm for association. While the method obtained slightly
    lower MOTA score on the whole MOT15 dataset when compared to SORT, the authors
    showed that it performed better on videos with moving camera.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Bullinger 等人在 [[76](#bib.bib76)] 中探索了另一种方法，该方法不是在检测步骤中计算经典的边界框，而是采用了多任务网络级联
    [[77](#bib.bib77)] 来获取实例感知的语义分割图。作者认为，由于实例的 2D 形状不同于矩形边界框，不包含背景结构或其他物体的部分，因此基于光流的跟踪算法会表现得更好，特别是当目标在图像中的位置也受到相机运动和物体自身运动的影响时。在获得当前帧中各种实例的分割图后，应用了一种光流方法（[[78](#bib.bib78),
    [79](#bib.bib79), [80](#bib.bib80)]）来预测下一帧中每个实例的位置和形状。然后计算预测和检测实例之间的亲和矩阵，并将其作为输入提供给匈牙利算法进行关联。尽管该方法在整个
    MOT15 数据集上的 MOTA 得分略低于 SORT，但作者表明它在移动相机的视频上表现更好。
- en: 3.2 DL in feature extraction and motion prediction
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 特征提取和运动预测中的深度学习
- en: 'The feature extraction phase is the preferred one for the employment of deep
    learning models, due to their strong representational power that makes them good
    at extracting meaningful high-level features. The most typical approach in this
    area is the use of CNNs to extract visual features, as it is commented in section
    [3.2.2](#S3.SS2.SSS2 "3.2.2 CNNs as visual feature extractors ‣ 3.2 DL in feature
    extraction and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey"). Instead of using classical CNN models, another
    recurrent idea consists in training them as Siamese CNNs, using contrastive loss
    functions, in order to find the set of features that best distinguish between
    subjects. Those approaches are explained in section [3.2.3](#S3.SS2.SSS3 "3.2.3
    Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey"). Furthermore,
    some authors explored the capabilities of CNNs to predict object motion inside
    correlation filter based algorithms: these are commented in section [3.2.5](#S3.SS2.SSS5
    "3.2.5 CNNs for motion prediction: correlation filters ‣ 3.2 DL in feature extraction
    and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey"). Finally, other types of deep learning models have been employed,
    usually including them in more complex systems, combining deep features with classical
    ones. They are explained in sections [3.2.4](#S3.SS2.SSS4 "3.2.4 More complex
    approaches for visual feature extraction ‣ 3.2 DL in feature extraction and motion
    prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey") (specifically for visual features) and [3.2.6](#S3.SS2.SSS6 "3.2.6
    Other approaches ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey") (for
    approaches that don’t fit in the other categories).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取阶段是采用深度学习模型的首选，因为它们强大的表示能力使其擅长提取有意义的高级特征。在这一领域中，最典型的方法是使用CNNs来提取视觉特征，如在[3.2.2](#S3.SS2.SSS2
    "3.2.2 CNNs作为视觉特征提取器 ‣ 3.2 特征提取和运动预测 ‣ 3 深度学习在MOT中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")部分所述。除了使用经典的CNN模型，另一种反复出现的想法是将其训练为Siamese
    CNNs，使用对比损失函数，以找到最能区分目标的特征集。这些方法在[3.2.3](#S3.SS2.SSS3 "3.2.3 Siamese网络 ‣ 3.2 特征提取和运动预测中的深度学习
    ‣ 3 深度学习在MOT中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")部分中进行了解释。此外，一些作者探索了CNNs在基于相关滤波器的算法中预测物体运动的能力，这些内容在[3.2.5](#S3.SS2.SSS5
    "3.2.5 CNNs在运动预测中的应用：相关滤波器 ‣ 3.2 特征提取和运动预测中的深度学习 ‣ 3 深度学习在MOT中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")部分中进行了评论。最后，其他类型的深度学习模型也被应用，通常将其包含在更复杂的系统中，将深度特征与经典特征结合起来。这些在[3.2.4](#S3.SS2.SSS4
    "3.2.4 视觉特征提取的更复杂方法 ‣ 3.2 特征提取和运动预测中的深度学习 ‣ 3 深度学习在MOT中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")（专门针对视觉特征）和[3.2.6](#S3.SS2.SSS6
    "3.2.6 其他方法 ‣ 3.2 特征提取和运动预测中的深度学习 ‣ 3 深度学习在MOT中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")（针对不符合其他类别的方法）部分中进行了说明。
- en: '3.2.1 Autoencoders: first usage of DL in a MOT pipeline'
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 自编码器：深度学习在MOT管道中的首次应用
- en: To the best of our knowledge, the first approach using deep learning in MOT
    was presented by Wang et al. [[81](#bib.bib81)] in 2014\. They proposed a network
    of autoencoders stacked in two layers that were used to refine visual features
    extracted from natural scenes [[82](#bib.bib82)]. After the extraction step, affinity
    computation was performed using a SVM, and the association task was formulated
    as a minimum spanning tree problem. They showed that feature refinement greatly
    improved the model performance. However, the dataset on which the algorithm was
    tested is not commonly used and results are hardly comparable to other methods.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，使用深度学习在MOT中的首个方法由Wang等人[[81](#bib.bib81)]于2014年提出。他们提出了一个由两层堆叠的自编码器组成的网络，用于优化从自然场景中提取的视觉特征[[82](#bib.bib82)]。在提取步骤之后，使用SVM进行亲和度计算，并将关联任务公式化为最小生成树问题。他们展示了特征优化显著提高了模型性能。然而，算法测试所用的数据集并不常见，因此结果难以与其他方法进行比较。
- en: 3.2.2 CNNs as visual feature extractors
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 CNNs作为视觉特征提取器
- en: The most widely used methods for feature extraction are based on subtle modifications
    of convolutional neural networks. One of the first uses of these models can be
    found in [[83](#bib.bib83)]. Here, Kim et al. incorporated visual features into
    a classical algorithm, called Multiple Hypothesis Tracking, using a pretrained
    CNN that extracted 4096 visual features from the detections, that were later reduced
    to 256 using PCA. This modification improved the MOTA score on MOT15 by more than
    3 points. By the time that paper was submitted, it was the top ranked algorithm
    on that dataset. Yu el al. [[38](#bib.bib38)] used a modified version of GoogLeNet
    [[2](#bib.bib2)], pretrained on a custom re-identification dataset, built by combining
    classical person identification datasets (PRW [[84](#bib.bib84)], Market-1501
    [[85](#bib.bib85)], VIPeR [[86](#bib.bib86)], CUHK03 [[87](#bib.bib87)]). Visual
    features were combined with spatial ones, extracted with a Kalman filter, and
    then an affinity matrix was computed.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的特征提取方法基于卷积神经网络的微妙修改。这些模型的早期应用之一可以在 [[83](#bib.bib83)] 中找到。在这里，Kim 等人将视觉特征融入到一种经典算法中，称为多假设跟踪，使用预训练的
    CNN 从检测中提取了 4096 个视觉特征，后来通过 PCA 将其减少到 256 个。这一修改使得 MOT15 的 MOTA 得分提高了 3 分以上。在该论文提交时，它是该数据集上的最高排名算法。Yu
    等人 [[38](#bib.bib38)] 使用了 GoogLeNet [[2](#bib.bib2)] 的修改版，预训练于一个由经典人员识别数据集（PRW
    [[84](#bib.bib84)]，Market-1501 [[85](#bib.bib85)]，VIPeR [[86](#bib.bib86)]，CUHK03
    [[87](#bib.bib87)]) 组合而成的定制重新识别数据集上。视觉特征与由卡尔曼滤波器提取的空间特征结合，然后计算了亲和力矩阵。
- en: Other examples of the use of CNNs for feature extraction can be found in [[88](#bib.bib88)],
    where a custom CNN was used to extract appearance features in a Multiple Hypothesis
    Tracking framework, in [[89](#bib.bib89)], whose tracker employed a pretrained
    region-based CNN [[90](#bib.bib90)], or in [[91](#bib.bib91)], where a CNN extracted
    visual features from fish heads, later combined with motion prediction from a
    Kalman Filter.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 用于特征提取的其他示例可以在 [[88](#bib.bib88)] 中找到，其中在多个假设跟踪框架中使用了定制 CNN 提取外观特征，在 [[89](#bib.bib89)]
    中的跟踪器使用了预训练的基于区域的 CNN [[90](#bib.bib90)]，或者在 [[91](#bib.bib91)] 中，CNN 从鱼头中提取视觉特征，随后与来自卡尔曼滤波器的运动预测结合。
- en: 'The SORT algorithm [[35](#bib.bib35)], presented in section [3.1.1](#S3.SS1.SSS1
    "3.1.1 Faster R-CNN ‣ 3.1 DL in detection step ‣ 3 Deep learning in MOT ‣ Deep
    Learning in Video Multi-Object Tracking: A Survey"), was later refined with deep
    features, and this new version was called DeepSORT [[41](#bib.bib41)]. This model
    incorporated visual information extracted by a custom residual CNN [[92](#bib.bib92)].
    The CNN provided a normalized vector with 128 features as output, and the cosine
    distance between those vectors was added to the affinity scores used in SORT.
    A diagram of the network structure can be found in figure [4](#S3.F4 "Figure 4
    ‣ 3.2.2 CNNs as visual feature extractors ‣ 3.2 DL in feature extraction and motion
    prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey"). The experiments showed that this modification overcame the main drawback
    of the SORT algorithm, which was a high number of ID switches.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 'SORT 算法 [[35](#bib.bib35)]，在 [3.1.1](#S3.SS1.SSS1 "3.1.1 Faster R-CNN ‣ 3.1
    DL in detection step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") 节中介绍，后来通过深度特征进行了改进，新版本被称为 DeepSORT [[41](#bib.bib41)]。该模型结合了由定制残差
    CNN [[92](#bib.bib92)] 提取的视觉信息。CNN 提供了一个带有 128 个特征的归一化向量，余弦距离被添加到 SORT 中使用的亲和力评分中。网络结构的图示见图
    [4](#S3.F4 "Figure 4 ‣ 3.2.2 CNNs as visual feature extractors ‣ 3.2 DL in feature
    extraction and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey")。实验表明，这一修改克服了 SORT 算法的主要缺点，即 ID 切换过多。'
- en: '![Refer to caption](img/44dbf71cb8fcfaedbf2b8cac86459ad8.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/44dbf71cb8fcfaedbf2b8cac86459ad8.png)'
- en: 'Figure 4: Diagram of DeepSORT [[41](#bib.bib41)] CNN-based feature extractor.
    The red blocks are simple convolutional layers, the yellow block is a max pooling
    layer, and the blue blocks are residual blocks, that are composed of three convolutional
    layers each [[3](#bib.bib3)]. The final green block represents a fully-connected
    layer with batch normalization and L2 normalization. The output size of each block
    is indicated in parentheses.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: DeepSORT [[41](#bib.bib41)] CNN 基于特征提取器的图示。红色块是简单的卷积层，黄色块是最大池化层，蓝色块是残差块，每个残差块由三个卷积层组成
    [[3](#bib.bib3)]。最终的绿色块表示具有批量归一化和 L2 归一化的全连接层。每个块的输出大小在括号中指示。'
- en: Mahmoudi et al. [[42](#bib.bib42)] also incorporated CNN extracted visual features
    along with dynamic and position features, and then solved the association problem
    via Hungarian algorithm. In [[93](#bib.bib93)], a ResNet-50 [[3](#bib.bib3)] pretrained
    on ImageNet was used as visual feature extractor. An extensive explanation of
    how a CNN can be used to distinguish pedestrians can be found in [[94](#bib.bib94)].
    In their model, Bae et al. combined the output of the CNN with shape and motion
    models, and computed an aggregated affinity score for each pair of detections;
    the association problem was then solved by the Hungarian algorithm. Again, Ullah
    et al. [[95](#bib.bib95)] applied an off-the-shelf version of GoogLeNet [[2](#bib.bib2)]
    for feature extraction. Fang et al. [[96](#bib.bib96)] selected as visual features
    the output of a hidden convolutional layer of an Inception CNN [[97](#bib.bib97)].
    Fu et al. [[98](#bib.bib98)] employed the DeepSORT feature extractor, and measured
    the correlation of features using a discriminative correlation filter. Afterwards,
    the matching score was combined with a spatio-temporal relation score, and the
    final score was used as a likelihood in a Gaussian Mixture Probability Hypothesis
    Density filter [[99](#bib.bib99)]. The authors in [[100](#bib.bib100)] used a
    fine-tuned GoogLeNet on the ILSVRC CLS-LOC [[101](#bib.bib101)] dataset for pedestrians
    recognition. In [[70](#bib.bib70)], the authors reused the visual features extracted
    by the CNN-based detector, and the association was performed using a Reverse Nearest
    Neighbor technique [[102](#bib.bib102)]. Sheng et al. [[103](#bib.bib103)] employed
    the convolutional part of GoogLeNet to extract appearance features, using the
    cosine distance between them to compute an affinity score between pairs of detections,
    and merging that information with motion prediction in order to compute an overall
    affinity which serves as edge cost in a graph problem. Chen et al. [[104](#bib.bib104)]
    utilized the convolutional part of ResNet to build a custom model, stacking a
    LSTM cell on top of the convolutions, in order to compute simultaneously a similarity
    score and a bounding box regression.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Mahmoudi 等人 [[42](#bib.bib42)] 还结合了 CNN 提取的视觉特征以及动态和位置特征，然后通过匈牙利算法解决了关联问题。在
    [[93](#bib.bib93)] 中，使用了在 ImageNet 上预训练的 ResNet-50 [[3](#bib.bib3)] 作为视觉特征提取器。有关如何使用
    CNN 区分行人的详细说明可以在 [[94](#bib.bib94)] 中找到。在他们的模型中，Bae 等人将 CNN 的输出与形状和运动模型结合，并计算了每对检测的综合亲和度分数；然后通过匈牙利算法解决了关联问题。再次，Ullah
    等人 [[95](#bib.bib95)] 使用了现成的 GoogLeNet [[2](#bib.bib2)] 进行特征提取。Fang 等人 [[96](#bib.bib96)]
    选择了 Inception CNN [[97](#bib.bib97)] 隐藏卷积层的输出作为视觉特征。Fu 等人 [[98](#bib.bib98)] 采用了
    DeepSORT 特征提取器，并使用判别相关滤波器测量特征的相关性。随后，将匹配分数与时空关系分数结合，最终分数作为高斯混合概率假设密度滤波器 [[99](#bib.bib99)]
    的可能性。[[100](#bib.bib100)] 的作者使用了在 ILSVRC CLS-LOC [[101](#bib.bib101)] 数据集上微调的
    GoogLeNet 进行行人识别。在 [[70](#bib.bib70)] 中，作者重用了 CNN 基于检测器提取的视觉特征，并使用反向最近邻技术 [[102](#bib.bib102)]
    进行关联。Sheng 等人 [[103](#bib.bib103)] 使用 GoogLeNet 的卷积部分提取外观特征，利用它们之间的余弦距离计算检测对之间的亲和度分数，并将该信息与运动预测合并，以计算作为图问题中的边缘成本的整体亲和度。Chen
    等人 [[104](#bib.bib104)] 利用 ResNet 的卷积部分构建了一个自定义模型，将 LSTM 单元堆叠在卷积层上，以同时计算相似度分数和边界框回归。
- en: In [[53](#bib.bib53)], the model learned to distinguish fast moving cells from
    slow moving cells. After the classification was computed, slow cells were associated
    using only motion features, since they were almost still, while fast cells were
    associated using both motion features and visual features extracted by a Fast
    R-CNN based on VGG-16 [[1](#bib.bib1)], specifically fine-tuned for the cell classification
    task. Moreover, the proposed model included a tracking optimization step, where
    false negatives and false positives were reduced by combining possible tracklets
    that were mistakenly interrupted.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[53](#bib.bib53)] 中，该模型学会了区分快速移动细胞和慢速移动细胞。在计算出分类后，慢速细胞仅使用运动特征进行关联，因为它们几乎静止，而快速细胞则同时使用运动特征和由基于
    VGG-16 [[1](#bib.bib1)] 的 Fast R-CNN 提取的视觉特征，特别是为细胞分类任务微调。此外，提出的模型包括一个跟踪优化步骤，通过组合可能的被错误中断的轨迹来减少假阴性和假阳性。
- en: 'Ran et al. [[52](#bib.bib52)] proposed a combination of a classical CNN for
    visual features extraction and AlphaPose CNN for pose estimation. The output of
    these two networks was then fed into a LSTM model together with the tracklet information
    history to compute a similarity, as it is explained in section [3.3.1](#S3.SS3.SSS1
    "3.3.1 Recurrent neural networks and LSTMs ‣ 3.3 DL in affinity computation ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ran 等人 [[52](#bib.bib52)] 提出了将经典 CNN 用于视觉特征提取，并将 AlphaPose CNN 用于姿态估计的组合。这两个网络的输出被输入到
    LSTM 模型中，与轨迹信息历史一起计算相似度，详见第 [3.3.1](#S3.SS3.SSS1 "3.3.1 Recurrent neural networks
    and LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey") 节。'
- en: 'An interesting employment of CNNs in feature extraction can be found in [[51](#bib.bib51)].
    The authors used a pose detector, called DeepCut [[105](#bib.bib105)], that was
    a modification of Fast R-CNN; its output consisted in score maps predicting the
    presence of fourteen body parts. These were combined with the cropped images of
    detected pedestrians and fed into a CNN. A more detailed explanation of the algorithm
    is available in section [3.3.6](#S3.SS3.SSS6 "3.3.6 CNNs for affinity computation
    ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey").'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '在特征提取中，CNN 的一个有趣应用可以在 [[51](#bib.bib51)] 中找到。作者使用了一种姿态检测器，称为 DeepCut [[105](#bib.bib105)]，这是
    Fast R-CNN 的修改版；其输出为预测十四个身体部位的得分图。这些得分图与检测到的行人裁剪图像结合，并输入到 CNN 中。算法的更详细解释见第 [3.3.6](#S3.SS3.SSS6
    "3.3.6 CNNs for affinity computation ‣ 3.3 DL in affinity computation ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey") 节。'
- en: 3.2.3 Siamese networks
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 孪生网络
- en: '![Refer to caption](img/45784d78feb0c1ecd2fa4486095975db.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/45784d78feb0c1ecd2fa4486095975db.png)'
- en: 'Figure 5: Example of a Siamese CNN architecture. For feature extraction, the
    network is trained as a Siamese CNN, but at inference time the output probability
    is discarded, and the last fully connected layer is used as feature vector for
    a single candidate. When the network is used for affinity computation, the whole
    structure is preserved during inference.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：孪生 CNN 架构的示例。为了特征提取，网络被训练为孪生 CNN，但在推断时，输出概率被丢弃，最后的全连接层作为单个候选者的特征向量。当网络用于亲和度计算时，整个结构在推断过程中保持不变。
- en: 'Another recurrent idea is training CNNs with loss functions that combine information
    from different images, in order to learn the set of features that best differentiates
    examples of different objects. These networks are usually called Siamese networks
    (an example of the architecture is shown in figure [5](#S3.F5 "Figure 5 ‣ 3.2.3
    Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")). Kim
    et al. [[106](#bib.bib106)] proposed a Siamese network [[107](#bib.bib107)] which
    was trained using a contrastive loss. The network took two images, their IoU score
    and their area ratio as input, and produced a contrastive loss as output. After
    the net was trained, the layer that computed the contrastive loss was removed,
    and the last layer was used as a feature vector for the input image. The similarity
    score was later computed by combining the Euclidean distance between feature vectors,
    the IoU score and the area ratio between bounding boxes. The association step
    was solved using a custom greedy algorithm. Wang et al. [[108](#bib.bib108)] also
    proposed a Siamese network which took two image patches and computed a similarity
    score between them. The score at test time was computed comparing the visual features
    extracted by the network for the two images, and including temporally constrained
    information. The distance employed as similarity score was a Mahalanobis distance
    with a weight matrix, also learned by the model.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个反复出现的想法是使用结合不同图像信息的损失函数来训练卷积神经网络（CNN），以学习能够最佳区分不同物体示例的特征集。这些网络通常被称为Siamese网络（架构示例见图[5](#S3.F5
    "Figure 5 ‣ 3.2.3 Siamese networks ‣ 3.2 DL in feature extraction and motion prediction
    ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")）。Kim等人[[106](#bib.bib106)]提出了一种使用对比损失训练的Siamese网络[[107](#bib.bib107)]。该网络接受两幅图像、它们的IoU评分和面积比作为输入，输出对比损失。训练后，计算对比损失的层被移除，最后一层被用作输入图像的特征向量。相似度评分通过结合特征向量之间的欧几里得距离、IoU评分和边界框之间的面积比来计算。关联步骤使用自定义贪婪算法解决。Wang等人[[108](#bib.bib108)]还提出了一种Siamese网络，该网络接受两个图像块并计算它们之间的相似度评分。测试时的评分是通过比较网络提取的两幅图像的视觉特征，并包括时间约束信息来计算的。作为相似度评分使用的距离是带有权重矩阵的马氏距离，该权重矩阵也由模型学习。'
- en: Zhang et al. [[109](#bib.bib109)] proposed a loss function called SymTriplet
    loss. According to their explanation, during the training phase three CNNs with
    shared weights were used, and the loss function combined the information extracted
    from two images belonging to the same object (positive pair) and from an image
    of a different one (two negative pairs). The SymTriplet loss decreased when the
    distance between the feature vectors of the positive pair was small, and increased
    when the negative pairs’ features were close. Optimizing that function resulted
    in very similar feature vectors for images of the same object, while producing
    different vectors for different objects, with a larger distance between them.
    The dataset on which the tracking algorithm was tested was made of chapters from
    TV series and music videos from YouTube. Since the videos included different shots,
    the problem was divided into two stages. First, data association between frames
    in the same shot were performed. The affinity score in that case was a combination
    between the Euclidean distance of the feature vectors from the detections, temporal
    and kinematic information. Afterwards, tracklets were linked across shots, using
    a Hierarchical Agglomerative Clustering algorithm working over the appearance
    features.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang等人[[109](#bib.bib109)]提出了一种称为SymTriplet损失的损失函数。根据他们的解释，在训练阶段使用了三种具有共享权重的CNN，损失函数结合了来自同一物体的两幅图像（正样本对）和来自不同物体的图像（两个负样本对）提取的信息。当正样本对的特征向量之间的距离较小时，SymTriplet损失减少；当负样本对的特征接近时，损失增加。优化该函数会使同一物体的图像产生非常相似的特征向量，同时使不同物体的特征向量不同，相互之间距离更大。跟踪算法测试所用的数据集由电视系列剧和YouTube音乐视频的章节组成。由于视频包含不同的镜头，问题被分为两个阶段。首先，在同一镜头中的帧之间进行数据关联。此情况下的亲和度评分是检测到的特征向量的欧几里得距离、时间信息和运动信息的组合。随后，使用层次聚类算法在镜头之间链接跟踪片段，该算法在外观特征上进行工作。
- en: Leal-Taixé et al. [[110](#bib.bib110)] proposed a Siamese CNN which received
    two stacked images as an input, and output the probability of both images belonging
    to the same person. They trained the network with this output so that it learned
    the most representative features to distinguish subjects. Afterwards, the output
    layer was removed and the features extracted by the last hidden layer were used
    as input for a Gradient Boosting model, together with contextual information,
    in order to get an affinity score between detections. Then, the association step
    was solved using Linear Programming [[111](#bib.bib111)].
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Leal-Taixé 等人[[110](#bib.bib110)] 提出了一个 Siamese CNN，它接收两个堆叠的图像作为输入，并输出这两个图像属于同一人的概率。他们使用这个输出训练网络，使其学习到区分主体的最具代表性的特征。随后，移除了输出层，使用最后一个隐藏层提取的特征作为
    Gradient Boosting 模型的输入，同时结合上下文信息，以获得检测之间的亲和得分。然后，使用线性规划[[111](#bib.bib111)]解决关联步骤。
- en: Son et al. [[112](#bib.bib112)] proposed a new CNN architecture, called Quad-CNN.
    This model received as input four image patches, where the first three of them
    were from the same person, but in increasing time order, and the last one from
    another person. The network was trained using a custom loss, combining information
    about temporal distances between detections, extracted visual features, and bounding
    box positions. During the test phase, the network took two detections, and predicted
    the probability that both detections belonged to the same person, using the learned
    embedding.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: Son 等人[[112](#bib.bib112)] 提出了一个新的 CNN 架构，称为 Quad-CNN。该模型接收四个图像补丁作为输入，其中前三个来自同一人，但时间顺序递增，最后一个来自另一人。网络使用自定义损失进行训练，结合了检测之间的时间距离、提取的视觉特征和边界框位置的信息。在测试阶段，网络接收两个检测，并预测这两个检测属于同一人的概率，使用所学习到的嵌入。
- en: In [[55](#bib.bib55)] a Siamese network based on Mask R-CNN [[17](#bib.bib17)]
    was built. After the Mask R-CNN had produced the mask for each detection, three
    examples were fed into the shallow Siamese net, two from the same object (positive
    pair) and one from another object (negative pair), again, and a triplet loss was
    used for training. After the training phase, the output layer was removed, and
    a 128-d vector was extracted from the last hidden layer. The appearance similarity
    was then computed using the cosine distance. That similarity was further combined
    with a motion consistency, which consisted on a score based on the predicted position
    of the object, assuming linear motion, and with a spatial potential, which was
    a more complex motion model. The association problem was then solved with a power
    iteration over a 3-d tensor of computed similarities.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[55](#bib.bib55)]中，构建了一个基于 Mask R-CNN [[17](#bib.bib17)] 的 Siamese 网络。在 Mask
    R-CNN 为每个检测生成了掩码后，将三个样本输入到浅层 Siamese 网络中，其中两个来自同一对象（正对），一个来自其他对象（负对），并使用三元组损失进行训练。训练阶段后，移除了输出层，从最后一个隐藏层提取了一个
    128-d 向量。然后，使用余弦距离计算外观相似性。该相似性进一步与运动一致性结合，运动一致性基于假设线性运动的对象预测位置的得分，以及空间潜力，空间潜力是一个更复杂的运动模型。随后，通过对计算出的相似性
    3-d 张量进行幂迭代来解决关联问题。
- en: 'Maksai et al. [[113](#bib.bib113)] directly used the 128-d feature vector extracted
    by the ReID triplet CNN proposed in [[114](#bib.bib114)], and combined it with
    other appearance-based features (as an alternative to an appearance-less version
    of the algorithm). Those features were further processed by a bidirectional LSTM.
    In [[115](#bib.bib115)] a similar approach was followed, with a so-called Spatial
    Attention Network (SAN). The SAN was a Siamese CNN, which used a pretrained ResNet-50
    as base model. That net was truncated so that only the convolutional layers were
    employed. Then, a Spatial Attention Map was extracted from the last convolutional
    layers of the model: it represented a measure of the importance of different parts
    in the bounding box, in order to exclude background and other targets from the
    extracted features. The features were in fact weighted by this map, acting as
    a mask. The masked features from both detections were then merged into a fully
    connected layer which computed the similarity between them. During training, the
    network was also set to output a classification score, because the authors observed
    that jointly optimizing classification and affinity computation tasks resulted
    in a better performance in the latter. The affinity information was further fed
    into a bidirectional LSTM, as in the previous example. Both will be further discussed
    in section [3.3](#S3.SS3 "3.3 DL in affinity computation ‣ 3 Deep learning in
    MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey"). Ma et al. [[116](#bib.bib116)]
    also trained a Siamese CNN in order to extract visual features from tracked pedestrians
    in their model, which is explained in detail in section [3.4.1](#S3.SS4.SSS1 "3.4.1
    Recurrent neural networks ‣ 3.4 DL in Association/Tracking step ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'Maksai等人[[113](#bib.bib113)]直接使用了ReID triplet CNN提取的128维特征向量，该CNN是在[[114](#bib.bib114)]中提出的，并将其与其他基于外貌的特征结合在一起（作为算法无外貌版本的替代方法）。这些特征进一步通过双向LSTM进行处理。在[[115](#bib.bib115)]中，采用了类似的方法，使用了所谓的空间注意力网络（SAN）。SAN是一个孪生CNN，使用预训练的ResNet-50作为基础模型。该网络被截断，只使用卷积层。然后，从模型的最后一个卷积层中提取出空间注意力图：它表示边界框中不同部分的重要性的度量，以便从提取的特征中排除背景和其他目标。实际上，这些特征经过了该地图的加权处理，作为一种掩码。然后，来自两个检测的掩码特征被合并到一个完全连接的层中，该层计算它们之间的相似度。在训练过程中，网络也被设置为输出一个分类分数，因为作者观察到联合优化分类和相似度计算任务会导致后者的更好性能。相似度信息进一步被输入到双向LSTM中，就像前面的例子一样。后面的部分[3.3](#S3.SS3
    "3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey")将进一步讨论这两个方法。Ma等人[[116](#bib.bib116)]还训练了一个孪生CNN，以从其模型中跟踪的行人中提取视觉特征，该模型的详细介绍在[3.4.1](#S3.SS4.SSS1
    "3.4.1 Recurrent neural networks ‣ 3.4 DL in Association/Tracking step ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")部分说明。'
- en: 'In [[117](#bib.bib117)], Zhou et al. proposed a visual displacement CNN, which
    learned to predict the next position of an object depending on previous positions
    of the objects, and the influence that an object had over other objects in the
    scene. That CNN was then used to predict the location of objects in the next frame,
    taking as input their past trajectories. The network was also capable of extracting
    visual information from the predicted positions and the actual detections, in
    order to compute a similarity score, as it is explained in section [3.3.6](#S3.SS3.SSS6
    "3.3.6 CNNs for affinity computation ‣ 3.3 DL in affinity computation ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '在[[117](#bib.bib117)]中，Zhou等人提出了一个视觉位移CNN，该CNN通过学习对象的先前位置以及对象对场景中其他对象的影响，来预测对象的下一个位置。然后使用该CNN来预测对象在下一帧中的位置，以其过去的轨迹为输入。该网络还能够从预测位置和实际检测中提取视觉信息，以计算相似度得分，详细说明请参见[3.3.6](#S3.SS3.SSS6
    "3.3.6 CNNs for affinity computation ‣ 3.3 DL in affinity computation ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")部分。'
- en: Chen et al. [[118](#bib.bib118)] proposed a two-steps algorithm which employed
    GoogLeNet trained with triplet loss for feature extraction. In the first step,
    the model used a R-FCN to predict possible detection candidates using information
    from the existing tracklets. Then, those detections were combined with the actual
    detections and NMS was performed. Afterwards, using the customly trained GoogLeNet
    model, they extracted visual features from the detections, and solved the association
    problem with a hierarchical association algorithm. When their paper was published,
    the algorithm was ranked on top among online methods in the MOT16 dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人 [[118](#bib.bib118)] 提出了一个两步算法，该算法采用了通过三元组损失训练的 GoogLeNet 进行特征提取。在第一步中，模型使用
    R-FCN 预测可能的检测候选区域，利用现有轨迹的信息。然后，这些检测结果与实际检测结果结合，并进行了 NMS（非极大值抑制）。之后，利用自定义训练的 GoogLeNet
    模型，从检测结果中提取视觉特征，并通过分层关联算法解决关联问题。当他们的论文发表时，该算法在 MOT16 数据集中的在线方法中排名靠前。
- en: 'Lee et al. [[119](#bib.bib119)] recently explored an interesting approach,
    combining pyramid and Siamese networks together. Their model, called Feature Pyramid
    Siamese Network, employed a backbone network (they studied the performance using
    SqueezeNet [[120](#bib.bib120)] and GoogLeNet [[2](#bib.bib2)], but the backbone
    network can be changed), which extracted visual features from two different images
    using the same parameters. Afterwards, some of the hidden feature maps from the
    network were extracted and given to the Feature Pyramid Siamese Network. The network
    then employed an upsampling and merging strategy to create a feature vector for
    every stage of the pyramid. Deeper layers were merged with shallower ones in order
    to enrich the simpler features with more complex ones. Afterwards, affinity score
    computation was performed, as explained in section [3.3.7](#S3.SS3.SSS7 "3.3.7
    Siamese CNNs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep
    Learning in Video Multi-Object Tracking: A Survey").'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '李等人 [[119](#bib.bib119)] 最近探索了一种有趣的方法，将金字塔网络和孪生网络结合起来。他们的模型称为特征金字塔孪生网络，采用了一个主干网络（他们研究了
    SqueezeNet [[120](#bib.bib120)] 和 GoogLeNet [[2](#bib.bib2)] 的性能，但主干网络可以更改），该网络使用相同的参数从两张不同的图像中提取视觉特征。之后，从网络中提取了一些隐藏的特征图，并输入到特征金字塔孪生网络中。该网络采用上采样和合并策略，为金字塔的每个阶段创建特征向量。将更深层的特征与较浅的特征合并，以用更复杂的特征丰富简单的特征。之后，进行了亲和度得分计算，如
    [3.3.7](#S3.SS3.SSS7 "3.3.7 Siamese CNNs ‣ 3.3 DL in affinity computation ‣ 3
    Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")
    节中所述。'
- en: 3.2.4 More complex approaches for visual feature extraction
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 更复杂的视觉特征提取方法
- en: '![Refer to caption](img/2e0a6695a6a10f40441218e94d528214.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e0a6695a6a10f40441218e94d528214.png)'
- en: 'Figure 6: Typical usage of LSTM for motion prediction. A group of bounding
    boxes are fed into the network, and the produced output is the predicted bounding
    box in the next frame'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：LSTM 在运动预测中的典型使用。将一组边界框输入网络，产生的输出是下一帧中预测的边界框。
- en: More complex approaches have also been proposed. Lu et al. [[59](#bib.bib59)]
    employed the class predicted by the SSD in the detection step as a feature, and
    combined it with an image descriptor extracted with RoI pooling for each detection.
    Afterwards, the extracted features were used as input for a LSTM network, which
    learned to compute association features for the detections. Those features were
    later used for affinity computation, using the cosine distance between them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 也提出了更复杂的方法。陆等人 [[59](#bib.bib59)] 将 SSD 在检测步骤中预测的类别作为特征，并与每个检测的 RoI 池化提取的图像描述符结合。之后，将提取的特征作为
    LSTM 网络的输入，LSTM 网络学习计算检测的关联特征。这些特征随后用于亲和度计算，使用它们之间的余弦距离。
- en: In [[121](#bib.bib121)], the shallower layers of GoogLeNet were employed to
    learn a dictionary of features of the tracked objects. In order to learn the dictionary,
    the algorithm randomly selected objects in the first 100 frames of the video.
    The model extracted the feature maps in the first seven layers of the network.
    Then a dimensionality reduction was performed using Orthogonal Matching Pursuit
    (OPM) [[122](#bib.bib122)] over the features extracted from the objects, and the
    learned representation was used as a dictionary. During the test phase, the OPM
    representation was computed for every detected object in the scene, and compared
    with the dictionary in order to construct a cost matrix, combining visual and
    motion information extracted by a Kalman filter. Finally, the association was
    performed using the Hungarian algorithm.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[121](#bib.bib121)]中，使用了 GoogLeNet 的较浅层来学习跟踪对象的特征字典。为了学习字典，算法在视频的前 100 帧中随机选择对象。模型提取了网络前七层中的特征图。然后，使用正交匹配追踪
    (OPM) [[122](#bib.bib122)] 对提取的特征进行降维，学习到的表示被用作字典。在测试阶段，计算了场景中每个检测对象的 OPM 表示，并与字典进行比较，以构建成本矩阵，结合了由卡尔曼滤波器提取的视觉和运动信息。最后，使用匈牙利算法进行关联。
- en: 'LSTMs are sometimes employed for motion prediction, in order to learn more
    complex, non-linear motion models from the data. In figure [6](#S3.F6 "Figure
    6 ‣ 3.2.4 More complex approaches for visual feature extraction ‣ 3.2 DL in feature
    extraction and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey") a scheme of the typical use of LSTMs for motion
    prediction is shown. An example of this use of recurrent networks is shown by
    Sadeghian et al. [[123](#bib.bib123)], who proposed a model that employed three
    different RNNs to compute various types of features, not only motion ones, for
    each detection. The first RNN was employed to extract appearance features. The
    input of this RNN was a visual features vector extracted by a VGG CNN [[1](#bib.bib1)],
    pretrained specifically for person re-identification. The second RNN was a LSTM
    trained to predict the motion model for every tracked object. In this case, the
    output of the LSTM was the velocity vector of each object. The last RNN was trained
    to learn the interactions between different objects on the scene, since the position
    of some objects could be influenced by the behavior of surrounding items. The
    affinity computation was then performed by another LSTM, taking the information
    of the other RNNs as input.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'LSTM 有时被用于运动预测，以便从数据中学习更复杂的非线性运动模型。图 [6](#S3.F6 "Figure 6 ‣ 3.2.4 More complex
    approaches for visual feature extraction ‣ 3.2 DL in feature extraction and motion
    prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey") 显示了 LSTM 在运动预测中的典型使用方案。Sadeghian 等人 [[123](#bib.bib123)] 展示了递归网络的这种使用方式，他们提出了一个模型，采用了三种不同的
    RNN 来计算每个检测的各种特征，不仅仅是运动特征。第一个 RNN 用于提取外观特征。这个 RNN 的输入是由 VGG CNN [[1](#bib.bib1)]
    提取的视觉特征向量，该 CNN 专门为人员再识别进行预训练。第二个 RNN 是一个 LSTM，训练用于预测每个跟踪对象的运动模型。在这种情况下，LSTM 的输出是每个对象的速度向量。最后一个
    RNN 训练用于学习场景中不同对象之间的交互，因为某些对象的位置可能会受到周围物体行为的影响。然后，通过另一个 LSTM 执行亲和力计算，以其他 RNN 的信息作为输入。'
- en: In [[124](#bib.bib124)], a model of stacked CNNs was proposed. The first section
    of the model consisted of a pretrained shared CNN which extracted common features
    for every object in the scene. That CNN was not updated online. Then, a RoI pooling
    was applied and the RoI features for every candidate were extracted. Afterwards,
    for every tracked candidate a new specific CNN was instantiated and trained online.
    Those CNNs extracted both the visibility map and the spatial attention map for
    its candidate. Finally, after the refined features were extracted, the probability
    of each new image belonging to every already tracked object was computed, and
    the association step was finally performed using a greedy algorithm.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[124](#bib.bib124)]中，提出了一种堆叠 CNN 的模型。该模型的第一部分由一个预训练的共享 CNN 组成，用于提取场景中每个对象的共同特征。这个
    CNN 不会在线更新。接着，应用了 RoI 池化，提取了每个候选区域的 RoI 特征。随后，为每个跟踪的候选区域实例化并在线训练了一个新的特定 CNN。这些
    CNN 提取了每个候选区域的可见性图和空间注意力图。最后，在提取了精细特征后，计算了每个新图像属于每个已跟踪对象的概率，并使用贪婪算法进行了关联步骤。
- en: Sharma et al. [[67](#bib.bib67)] designed a set of cost functions to compute
    similarity between detections of vehicles. Those costs combined appearance features,
    extracted by a CNN, with 3D shape and position features assuming an environment
    with a moving camera. The defined costs were a 3D-2D cost, were the estimated
    3D projection of the bounding box on the previous frame was compared with the
    2D bounding box on the new frame, a 3D-3D cost, were the 3D projection of the
    previous bounding box was overlapped with the 3D projection of the current bounding
    box, an appearance cost, were the euclidean distance of the extracted visual features
    was computed, and a shape and pose cost, were the rough shape and position of
    the object in the bounding boxes were computed and compared. Note that while 3D
    projections were inferred, the input was still 2D images. After every cost was
    computed, the final pairwise cost between detections in two subsequent frames
    was a linear combination of the former costs. The final association problem was
    solved using the Hungarian algorithm.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Sharma等人[[67](#bib.bib67)]设计了一组成本函数来计算车辆检测之间的相似性。这些成本将通过CNN提取的外观特征与假设为移动摄像机环境中的3D形状和位置特征相结合。定义的成本包括3D-2D成本，其中将前一帧的边界框的估计3D投影与新帧的2D边界框进行比较；3D-3D成本，其中将前一边界框的3D投影与当前边界框的3D投影重叠；外观成本，其中计算了提取的视觉特征的欧几里得距离；以及形状和姿态成本，其中计算并比较了边界框中物体的大致形状和位置。注意，尽管推断了3D投影，输入仍然是2D图像。计算每个成本后，两个后续帧之间的最终成对成本是以前成本的线性组合。最终的关联问题使用匈牙利算法解决。
- en: Kim et al. [[66](#bib.bib66)] employed the information extracted by the YOLOv2
    CNN object detector to build a random ferns classifier [[125](#bib.bib125)]. The
    algorithm worked in two steps. In the first step, a so-called teacher-RF was trained
    in order to differentiate pedestrians from non-pedestrians. After the teacher-RF
    was trained, for every tracked object, a random ferns classifier was constructed.
    Those classifiers were called student-RF, and they were smaller than the teacher-RF.
    They were specialized in distinguishing their tracked object from the rest of
    the objects in the scene. The decision of having a small random ferns classifier
    for every object was taken in order to reduce the computational complexity of
    the overall model, so that it could work in real time.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Kim等人[[66](#bib.bib66)]利用YOLOv2 CNN对象检测器提取的信息构建了一个随机森林分类器[[125](#bib.bib125)]。该算法分为两个步骤。第一步，训练了一个所谓的teacher-RF，以区分行人与非行人。在teacher-RF训练完成后，为每个跟踪的对象构建了一个随机森林分类器。这些分类器被称为student-RF，它们比teacher-RF小。它们专门用于区分其跟踪的对象与场景中的其他对象。决定为每个对象使用小型随机森林分类器的目的是为了降低整体模型的计算复杂度，从而使其能够实时工作。
- en: In [[126](#bib.bib126)] the number of affinity computations that the model must
    compute was reduced by estimating first the position of objects in subsequent
    frames, using a Hidden Markov Model [[127](#bib.bib127)]. Then, the feature extraction
    was performed using a pretrained CNN. After the visual features were extracted,
    the affinity computation was only computed between feasible pairs, that is, between
    detections close enough to the HMM prediction to be considered as the same object.
    The affinity score was obtained using a mutual information function between visual
    features. When the affinity scores were computed, a dynamic programming algorithm
    was used to associate detections.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[126](#bib.bib126)]中，通过首先估计物体在后续帧中的位置，使用隐马尔可夫模型[[127](#bib.bib127)]，减少了模型必须计算的亲和度计算次数。然后，使用预训练的CNN进行特征提取。在提取了视觉特征后，仅在可行的对之间计算亲和度，即在与HMM预测足够接近以被视为同一物体的检测之间计算亲和度。亲和度得分是通过视觉特征之间的互信息函数获得的。当计算出亲和度得分时，使用动态规划算法来关联检测结果。
- en: '3.2.5 CNNs for motion prediction: correlation filters'
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 CNNs用于运动预测：相关滤波器
- en: Wang et al. [[128](#bib.bib128)] studied the employment of a correlation filter
    [[129](#bib.bib129)], whose output is a response map for the tracked object. That
    map was an estimation of the new position of the object in the next frame. Such
    affinity was further combined with an optical flow affinity, computed using the
    Lucas-Kanade algorithm [[130](#bib.bib130)], a motion affinity calculated with
    a Kalman filter, and a scale affinity, represented by a ratio involving height
    and width of the bounding boxes. The affinity between two detections was computed
    as a linear combination of the previous scores. There was also a step of false
    detections removal, using a SVM classifier, and missing detections handling, using
    for that task the response map calculated in the previous steps. If an object
    was mistakenly lost and then re-identified, that step could fix the mistake and
    reconnect the broken tracklet.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[128](#bib.bib128)] 研究了相关滤波器的使用 [[129](#bib.bib129)]，其输出是被跟踪物体的响应图。该图是物体在下一帧中的新位置的估计。这种关联进一步与使用
    Lucas-Kanade 算法 [[130](#bib.bib130)] 计算的光流关联、使用 Kalman 滤波器计算的运动关联以及由边界框的高度和宽度比表示的尺度关联进行组合。两个检测之间的关联计算为前一评分的线性组合。此外，还有一个虚假检测去除步骤，使用
    SVM 分类器，以及使用前面步骤计算的响应图处理缺失检测。如果一个物体被错误地丢失然后重新识别，该步骤可以修正错误并重新连接断开的跟踪。
- en: In [[61](#bib.bib61)], a correlation filter was also employed to predict the
    position of the object in subsequent frames. The filter received as input the
    appearance features extracted by a CNN, previously reduced using PCA, and produced
    a response map of the predicted position for the object in the next frame as output.
    The predicted position was later used to compute a similarity score, combining
    the IoU between prediction and detections, and the APCE score of the response
    map. After the cost matrix was constructed, computing said score for every pair
    of detections between frames, the assignment problem was solved using the Hungarian
    algorithm.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[61](#bib.bib61)] 中，也使用了相关滤波器来预测物体在后续帧中的位置。该滤波器接收由 CNN 提取的外观特征作为输入，这些特征经过
    PCA 降维，并产生预测位置的响应图作为输出。预测位置随后用于计算相似度得分，结合了预测与检测之间的 IoU 和响应图的 APCE 得分。在构建成本矩阵后，通过计算每对帧之间的检测得分，使用匈牙利算法解决了分配问题。
- en: 3.2.6 Other approaches
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6 其他方法
- en: Rosello et al. [[131](#bib.bib131)] explored a completely different approach,
    using a reinforcement learning framework to train a set of agents that helped
    in the feature extraction step. The algorithm was based solely on motion features,
    without any visual information employed. The motion model was learned using a
    Kalman filter, whose behavior was managed by an agent, using one agent for each
    tracked object. The agent learned to decide which action should the Kalman filter
    take, between a set of actions that included ignoring the prediction, ignoring
    the new measure, using both information pieces, starting or stopping a track.
    The authors claimed that their algorithm could solve the tracking task even in
    non-visual scenarios, in contrast with classical algorithms whose performance
    was deeply influenced by visual features. However, the experimental results on
    MOT15 are not reliable and cannot be compared with other models because the model
    was tested on the training set.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Rosello 等人 [[131](#bib.bib131)] 探索了一种完全不同的方法，使用强化学习框架来训练一组代理，以帮助完成特征提取步骤。该算法完全基于运动特征，没有使用任何视觉信息。运动模型是通过
    Kalman 滤波器学习的，其行为由一个代理管理，每个被跟踪的物体使用一个代理。代理学习决定 Kalman 滤波器在一组操作中采取哪种行动，这些操作包括忽略预测、忽略新测量、同时使用两条信息、开始或停止跟踪。作者声称，他们的算法即使在非视觉场景中也能解决跟踪任务，而与传统算法相比，传统算法的性能深受视觉特征的影响。然而，由于模型是在训练集上测试的，因此对
    MOT15 的实验结果不可靠，无法与其他模型进行比较。
- en: Another algorithm that relied solely on motion features was the one proposed
    in [[132](#bib.bib132)]. Babaee et al. presented a LSTM which learned to predict
    the new position and size of the bounding box for every object in the scene, using
    information about position and velocity in previous frames. Using the IoU between
    the predicted bounding box and the real detection, an affinity measure was computed,
    and the tracks were associated using a custom greedy algorithm. The pipeline was
    applied on tracking results obtained by other algorithms, in order to handle occlusions,
    and the authors showed that their method could effectively reduce the number of
    ID switches.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个完全依赖于运动特征的算法是[[132](#bib.bib132)]中提出的。Babaee等人提出了一种LSTM，它学习预测场景中每个对象的边界框的新位置和大小，利用先前帧中的位置和速度信息。通过预测的边界框与实际检测之间的IoU计算相似度度量，并使用自定义贪婪算法关联轨迹。该管道应用于其他算法获得的跟踪结果，以处理遮挡问题，作者表明他们的方法能够有效减少ID切换的数量。
- en: 3.3 DL in affinity computation
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度学习在相似度计算中的应用
- en: While many works compute affinity between tracklets and detections (or tracklets
    and other tracklets) by using some distance measure over features extracted by
    a CNN, there are also algorithms that use deep learning models to directly output
    an affinity score, without having to specify an explicit distance metric between
    the features. This section focuses on such works.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多工作通过使用CNN提取的特征上的某些距离度量来计算跟踪片段与检测（或跟踪片段与其他跟踪片段）之间的相似度，但也有一些算法使用深度学习模型直接输出相似度分数，而无需为特征之间指定明确的距离度量。本节重点介绍这些工作。
- en: 'In particular, we are first going to describe algorithms that used recurrent
    neural networks, starting from standard LSTMs (section [3.3.1](#S3.SS3.SSS1 "3.3.1
    Recurrent neural networks and LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")) and
    then describing uses of Siamese LSTMs (section [3.3.2](#S3.SS3.SSS2 "3.3.2 Siamese
    LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey")) and Bidirectional LSTMs (section [3.3.3](#S3.SS3.SSS3
    "3.3.3 Bidirectional LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")). A particular
    use of LSTM-computed affinities in the context of Multiple Hypothesis Tracking
    (MHT) frameworks is presented in section [3.3.4](#S3.SS3.SSS4 "3.3.4 Uses of LSTMs
    in MHT frameworks ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣
    Deep Learning in Video Multi-Object Tracking: A Survey"); finally, a few works
    that employed different kinds of recurrent network for affinity computations are
    presented in section [3.3.5](#S3.SS3.SSS5 "3.3.5 Other recurrent networks ‣ 3.3
    DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey").'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们将首先描述使用递归神经网络的算法，从标准的LSTM（见[3.3.1](#S3.SS3.SSS1 "3.3.1 Recurrent neural
    networks and LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣
    Deep Learning in Video Multi-Object Tracking: A Survey")）开始，然后描述Siamese LSTM的应用（见[3.3.2](#S3.SS3.SSS2
    "3.3.2 Siamese LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey")）和双向LSTM（见[3.3.3](#S3.SS3.SSS3
    "3.3.3 Bidirectional LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")）。在[3.3.4](#S3.SS3.SSS4
    "3.3.4 Uses of LSTMs in MHT frameworks ‣ 3.3 DL in affinity computation ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")节中展示了在多假设跟踪（MHT）框架中使用LSTM计算相似度的具体应用；最后，在[3.3.5](#S3.SS3.SSS5
    "3.3.5 Other recurrent networks ‣ 3.3 DL in affinity computation ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")节中展示了一些使用不同类型递归网络进行相似度计算的工作。'
- en: 'In the second part of this section we are going to explore instead the uses
    of CNNs in affinity computation (section [3.3.6](#S3.SS3.SSS6 "3.3.6 CNNs for
    affinity computation ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey")), including the algorithms
    that used the output of Siamese CNNs directly as an affinity score (section [3.3.7](#S3.SS3.SSS7
    "3.3.7 Siamese CNNs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey")), instead of relying
    on distance measures over feature vectors like in section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节的第二部分，我们将探讨 CNN 在亲和力计算中的应用（见[3.3.6](#S3.SS3.SSS6 "3.3.6 CNNs for affinity
    computation ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey")），包括那些直接使用 Siamese CNN 输出作为亲和力分数的算法（见[3.3.7](#S3.SS3.SSS7
    "3.3.7 Siamese CNNs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey")），而不是像在[3.2.3](#S3.SS2.SSS3
    "3.2.3 Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")中那样依赖于特征向量上的距离度量。'
- en: 3.3.1 Recurrent neural networks and LSTMs
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 循环神经网络和 LSTM
- en: 'One of the first works to use a deep network to directly compute an affinity
    is [[133](#bib.bib133)], where Milan et al. proposed an end-to-end learning approach
    for online MOT, summarized in figure [7](#S3.F7 "Figure 7 ‣ 3.3.1 Recurrent neural
    networks and LSTMs ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣
    Deep Learning in Video Multi-Object Tracking: A Survey"). A recurrent neural network
    (RNN) based model was used as the main tracker, mimicking a Bayesian filter algorithm,
    consisting of three blocks: the first was a motion prediction block, that learned
    a motion model that took as input the state of the target in the past frames (i.e.
    the old bounding box locations and sizes) and predicted the target state in the
    next frame without accounting for the detections; the second block refined the
    state prediction using the detections in the new frame and an association vector
    containing the probability of associating the target with all such detections
    (it is evident how this can be considered an affinity score); the third block
    managed the birth and death of tracks, as it used the previous collected information
    to predict the probability of existence of the track in the new frame^(14)^(14)14To
    smooth the existence probability predictions and avoid deleting tracks of temporarily
    occluded objects, the difference between the new and the old existence probability
    was also output so that it could be minimized during training.. The association
    vector was computed using a LSTM-based network, that used the Euclidean distance
    between the predicted state of the target and the states of the detections in
    the new frame as input features (besides the hidden state and the cell state,
    as any standard LSTM). The networks were trained separately using 100K 20-frame
    long synthetically generated sequences. While the algorithm performed favorably
    to other techniques, like the combination of a Kalman filter with the Hungarian
    algorithm, the results on the MOT15 test set did not quite reach top accuracy;
    however, the algorithm was able to run much faster than other algorithms ($\sim
    165$ FPS) and did not use any kind of appearance features, leaving room for future
    improvements.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '使用深度网络直接计算相似度的首批工作之一是[[133](#bib.bib133)]，其中Milan等人提出了一种用于在线MOT的端到端学习方法，见图[7](#S3.F7
    "Figure 7 ‣ 3.3.1 Recurrent neural networks and LSTMs ‣ 3.3 DL in affinity computation
    ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")。一个基于递归神经网络（RNN）的模型被用作主要跟踪器，模仿了贝叶斯滤波算法，包含三个模块：第一个是运动预测模块，学习了一个运动模型，该模型以目标在过去帧中的状态（即旧的边界框位置和尺寸）作为输入，并预测下一帧中的目标状态，而不考虑检测；第二个模块使用新帧中的检测和包含将目标与所有这些检测关联的概率的关联向量来细化状态预测（显然这可以被视为一种相似度分数）；第三个模块管理轨迹的出生和死亡，因为它使用之前收集的信息来预测新帧中轨迹的存在概率^(14)^(14)14为了平滑存在概率预测并避免删除暂时遮挡的目标轨迹，输出了新旧存在概率之间的差异，以便在训练过程中最小化。关联向量是使用基于LSTM的网络计算的，该网络使用目标预测状态和新帧中检测状态之间的欧几里得距离作为输入特征（除了隐藏状态和单元状态外，与任何标准LSTM相同）。这些网络分别使用100K
    20帧长的合成生成序列进行训练。虽然该算法相较于其他技术（如卡尔曼滤波器与匈牙利算法的组合）表现较好，但在MOT15测试集上的结果未能达到顶级准确度；然而，该算法能够比其他算法运行得更快（$\sim
    165$ FPS），且未使用任何外观特征，留下了未来改进的空间。'
- en: '![Refer to caption](img/c43a6edc81714469b478592039bc7ef6.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c43a6edc81714469b478592039bc7ef6.png)'
- en: 'Figure 7: Diagram of the MOT algorithm proposed by Milan et al. [[133](#bib.bib133)]
    employing a LSTM to predict detection associations. The algorithm used two different
    RNNs to solve them problem, each one specialized in one subtask. The LSTM (left)
    learned to associate detections with tracks given predicted positions. It received
    the pairwise-distance matrix between detections and predictions ($C_{t+1}$), the
    cell state ($c_{i}$) and the hidden state ($h_{i}$) as input, and output the vector
    $A^{i}_{t+1}$ representing the probability of associating target $i$ with the
    detections in the frame. The RNN (right) was trained to predict the position of
    the targets in the new frame and the possible birth and death of new ones. It
    received as input the hidden state ($h_{t}$) and the current position of the target
    ($x_{t}$), outputting the predicted position and the new hidden state (blue box).
    After the associations from the LSTM were computed, using the detections $z_{t+1}$,
    the positions of targets were updated (green box), and the existence probability
    $\varepsilon$ was computed to predict death and birth of trajectories (red box).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：Milan等人提出的MOT算法示意图[[133](#bib.bib133)]，采用LSTM预测检测关联。该算法使用了两个不同的RNN来解决问题，每个RNN专注于一个子任务。LSTM（左侧）学习根据预测位置将检测与轨迹关联。它接收检测与预测之间的成对距离矩阵（$C_{t+1}$）、单元状态（$c_{i}$）和隐藏状态（$h_{i}$）作为输入，输出表示将目标$i$与帧中的检测关联的概率的向量$A^{i}_{t+1}$。RNN（右侧）被训练来预测新帧中的目标位置以及可能的新目标的出生和死亡。它接收隐藏状态（$h_{t}$）和当前目标位置（$x_{t}$）作为输入，输出预测位置和新的隐藏状态（蓝色框）。在计算了LSTM的关联后，使用检测$z_{t+1}$更新目标位置（绿色框），并计算存在概率$\varepsilon$以预测轨迹的死亡和出生（红色框）。
- en: 'Among the other works that later used LSTMs there is [[123](#bib.bib123)],
    that used a LSTM with a fully-connected (FC) layer to fuse features extracted
    by 3 other LSTMs (as already explained in section [3.2.4](#S3.SS2.SSS4 "3.2.4
    More complex approaches for visual feature extraction ‣ 3.2 DL in feature extraction
    and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey")) and output an affinity score^(15)^(15)15The paper seems
    to imply that while the LSTM is trained to predict an affinity score, only the
    affinity features are extracted and are then used to replace the handcrafted features
    used in the MDP paper. The algorithm presented in the MDP paper, though, adds
    on top of those features another FC layer, trained with reinforcement learning
    to classify the track/detection pair as belonging to the same identity or not.
    Thus we can consider the overall affinity computation as performed by a deep learning
    model.. The overall algorithm is similar to the Markov Decision Processes (MDP)
    based framework presented in [[134](#bib.bib134)]: a single object tracker (SOT)
    is used to track targets independently; when a target gets occluded, the SOT is
    stopped and a bipartite graph is built that uses the affinities computed by the
    LSTM as edge costs and the association problem is then solved with the help of
    the Hungarian algorithm. The authors showed that using both a combination of all
    the 3 feature extractors and an LSTM rather than a plain FC layer led to consistently
    better performance on a MOT15 validation set. The algorithm also reached state-of-the-art
    MOTA scores on both MOT15 and MOT16 test sets at the time of publication, confirming
    the validity of the approach.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在后来使用LSTM的其他工作中，有[[123](#bib.bib123)]，它使用了一个具有全连接（FC）层的LSTM来融合由另外3个LSTM提取的特征（如第[3.2.4](#S3.SS2.SSS4
    "3.2.4 更复杂的视觉特征提取方法 ‣ 3.2 DL在特征提取和运动预测中的应用 ‣ 3 深度学习在多目标跟踪中的应用 ‣ 深度学习在视频多目标跟踪中的应用：综述")节中已解释）并输出一个亲和力评分^(15)^(15)15
    论文似乎暗示，虽然LSTM被训练以预测亲和力评分，但仅提取了亲和力特征，并用这些特征替代了MDP论文中使用的手工特征。然而，MDP论文中提出的算法在这些特征上添加了另一层FC，通过强化学习训练以将轨迹/检测对分类为同一身份或不同身份。因此，我们可以将整体亲和力计算视为由深度学习模型执行的。整体算法类似于[[134](#bib.bib134)]中提出的基于马尔可夫决策过程（MDP）的框架：使用单目标跟踪器（SOT）独立跟踪目标；当目标被遮挡时，停止SOT并构建一个双分图，使用LSTM计算的亲和力作为边成本，然后借助匈牙利算法解决关联问题。作者展示了使用所有3个特征提取器的组合和LSTM，而不是简单的FC层，在MOT15验证集上带来了持续更好的性能。该算法在发布时还在MOT15和MOT16测试集上达到了最先进的MOTA评分，确认了该方法的有效性。
- en: 'Another approach using multiple LSTMs is [[52](#bib.bib52)], where Ran et al.
    proposed a Pose-based Triple Stream Network, that computed an affinity combining
    3 other affinities output by 3 LSTMs: one for appearance similarity, using CNN
    features and pose information extracted with AlphaPose [[135](#bib.bib135)], one
    for motion similarity, using pose joints velocity, and one for interaction similarity,
    using an interaction grid. A custom tracking algorithm is then used to associate
    the detections. The comparison with other state-of-the-art MOT algorithms on their
    proprietary Volleyball dataset for athlete tracking was favourable.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用多个 LSTM 的方法是 [[52](#bib.bib52)]，其中 Ran 等人提出了一种基于姿态的三流网络，该网络计算了结合 3 个 LSTM
    输出的其他亲和度的亲和度：一个用于外观相似度，使用 CNN 特征和用 AlphaPose [[135](#bib.bib135)] 提取的姿态信息，一个用于运动相似度，使用姿态关节速度，另一个用于交互相似度，使用交互网格。然后使用自定义跟踪算法来关联检测。与其他最先进的
    MOT 算法在其专有的排球数据集上的比较结果是有利的。
- en: 3.3.2 Siamese LSTMs
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 Siamese LSTM
- en: Liang et al. [[136](#bib.bib136)] also used multiple LSTMs to model various
    features, but they proceeded in a different way. Since extracting appearance features
    with CNNs is computationally expensive, they proceeded with a so-called pre-association
    step, that used a SVM to predict the association probability between tracklets
    and detections. The SVM took as input position and velocity similarity scores,
    computed using two LSTMs for position and velocity prediction. The pre-association
    step then consisted in discarding the detections with low SVM affinity scores.
    After this step, an actual association step was performed by using VGG-16 features
    given as input to a Siamese LSTM, that predicted an affinity score between the
    tracklet and the detections. Association was performed in a greedy manner, associating
    the detection with the highest score to the tracklet. Testing was done on the
    MOT17 datasets, and the results were in line with the top-performing algorithms.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 梁等人 [[136](#bib.bib136)] 也使用了多个 LSTM 来建模各种特征，但他们采用了不同的方法。由于使用 CNN 提取外观特征的计算开销很大，他们进行了所谓的预关联步骤，该步骤使用
    SVM 预测轨迹段和检测之间的关联概率。SVM 以位置和速度相似度评分作为输入，这些评分通过两个 LSTM 进行位置和速度预测来计算。预关联步骤随后包括丢弃低
    SVM 亲和分数的检测。在这一步骤之后，实际的关联步骤通过将 VGG-16 特征作为输入提供给 Siamese LSTM 来执行，Siamese LSTM
    预测轨迹段和检测之间的亲和分数。关联以贪婪的方式进行，将具有最高分数的检测与轨迹段关联。测试在 MOT17 数据集上进行，结果与顶级算法相符。
- en: Wan et al. [[43](#bib.bib43)] also used a Siamese LSTM in their algorithm, that
    was also composed of two steps. In the first step, short reliable tracklets were
    built by using Hungarian algorithm with affinity measures computed using the IoU
    between detections and the predicted target positions (obtained with Kalman filter
    or Lucas-Kanade optical flow). The second step also used the Hungarian algorithm
    to join the tracklets, but this time the affinity was computed using a Siamese
    LSTM framework that used motion features concatenated to appearance features extracted
    by a CNN (like in [[137](#bib.bib137)]), pre-trained on the CUHK03 Re-ID dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Wan 等人 [[43](#bib.bib43)] 的算法中也使用了 Siamese LSTM，算法由两个步骤组成。在第一步中，通过使用匈牙利算法构建短期可靠的轨迹段，亲和度度量是通过检测与预测目标位置之间的
    IoU（由 Kalman 滤波器或 Lucas-Kanade 光流获得）计算的。第二步也使用匈牙利算法来连接轨迹段，但这次亲和度是使用 Siamese LSTM
    框架计算的，该框架使用与 CNN 提取的外观特征拼接的运动特征（如 [[137](#bib.bib137)]），并在 CUHK03 Re-ID 数据集上进行预训练。
- en: 3.3.3 Bidirectional LSTMs
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 双向 LSTM
- en: 'A different usage of LSTMs in the affinity computation phase was presented
    by Zhu et al. [[115](#bib.bib115)]. They used a so-called Temporal Attention Network
    (TAN) to compute attention coefficients to weigh the features extracted by the
    Spatial Attention Network (SAN) (see section [3.2.3](#S3.SS2.SSS3 "3.2.3 Siamese
    networks ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")) to give less
    importance to noisy observations. A bidirectional LSTM was employed to this end.
    The whole network (called Dual Matching Attention Network) was used to recover
    from occlusions when a modified version of the Efficient Convolution Operators
    tracker (ECO) [[56](#bib.bib56)], trained exploiting hard example mining, failed
    to detect a target. The algorithm obtained results comparable to online state-of-the-art
    methods on MOT16 and MOT17 according to various metrics (MOTA, IDF1, number of
    ID switches).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhu 等人 [[115](#bib.bib115)] 提出了在相似度计算阶段的另一种 LSTM 使用方法。他们使用了所谓的时间注意力网络（TAN）来计算注意力系数，以加权由空间注意力网络（SAN）提取的特征（见[3.2.3](#S3.SS2.SSS3
    "3.2.3 Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")），从而降低噪声观察的权重。为此使用了双向
    LSTM。整个网络（称为双重匹配注意力网络）用于在改进版的高效卷积操作符跟踪器（ECO）[[56](#bib.bib56)] 训练中利用困难样本挖掘失败检测目标时进行遮挡恢复。根据各种指标（MOTA、IDF1、ID
    切换次数），该算法在 MOT16 和 MOT17 上获得了与在线最先进方法相当的结果。'
- en: Yoon et al. [[138](#bib.bib138)] also used a Bidirectional LSTM to compute affinities,
    on top of some FC layers that encoded non-appearance features (only bounding box
    coordinates and detection confidences). The association was solved using the Hungarian
    algorithm. They trained the network on the Stanford Drone Dataset (SDD) [[139](#bib.bib139)]
    and evaluated it on both SDD and MOT15\. They reached comparable results with
    top algorithms that did not use visual cues, but the performance was still worse
    than appearance-based methods.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Yoon 等人 [[138](#bib.bib138)] 还使用了双向 LSTM 来计算相似度，该方法建立在一些 FC 层之上，这些 FC 层编码了非外观特征（仅边界框坐标和检测置信度）。通过使用匈牙利算法解决关联问题。他们在斯坦福无人机数据集（SDD）[[139](#bib.bib139)]
    上训练了网络，并在 SDD 和 MOT15 上进行了评估。他们取得了与未使用视觉提示的顶级算法相当的结果，但性能仍低于基于外观的方法。
- en: 3.3.4 Uses of LSTMs in MHT frameworks
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 LSTM 在 MHT 框架中的应用
- en: In Multiple Hypothesis Tracking approaches, a tree of potential track hypotheses
    for each candidate target is first built. Then the likelihood of each track is
    computed and the combination of tracks that has the highest likelihood is chosen
    as a solution. Various deep learning algorithms have also been employed to enhance
    MHT based approaches.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在多假设跟踪方法中，首先为每个候选目标构建潜在跟踪假设的树。然后计算每个跟踪的可能性，并选择具有最高可能性的跟踪组合作为解决方案。各种深度学习算法也已被用来增强基于
    MHT 的方法。
- en: Kim et al. [[93](#bib.bib93)] proposed the use of a so-called Bilinear LSTM
    network as the gating step of the MHT-DAM [[83](#bib.bib83)] algorithm, that is,
    the affinity score computed by the LSTM was used to decide whether to prune or
    not a certain branch of the hypotheses tree. The LSTM cell had a modified forward
    pass (inspired by the online recursive least squares estimator proposed in [[83](#bib.bib83)])
    and took as input the appearance features of a tracklet in the past frames, extracted
    with a ResNet-50 CNN. The output of the LSTM cell was a feature matrix representing
    the historical appearance of the tracklet, and such matrix was then multiplied
    by the vector with the appearance features of the detection that needed to be
    compared with the tracklet. FC layers on top of that finally computed the affinity
    score between the tracklet and the detection. The authors claimed that such a
    modified LSTM is able to store longer-term appearance models than classical LSTMs.
    They also proposed to add a motion modeling classical LSTM to compute historical
    motion features (using the normalized bounding box coordinates and sizes), that
    were then concatenated to the appearance features before proceeding with the FC
    layers and the final softmax that output the affinity score. The two LSTMs were
    first trained separately and then fine-tuned jointly. The training data was also
    augmented including localization errors and missing detections, to resemble real-world
    data more closely. They used MOT15, MOT17, ETH, KITTI and other minor datasets
    for training and they evaluated the model on MOT16 and MOT17\. They showed that
    their model is sensitive to the quality of detections, as they improved on the
    MOTA performance of MHT-DAM when using the public Faster R-CNN and SDP detections,
    while they performed worse than it on the public DPM detections. Anyway, they
    seemed to get a higher IDF1 score regardless of the detections used, and their
    overall results reflected that, since they got the highest IDF1 of all the methods
    using MHT-based algorithms. However, the tracking quality, as measured both in
    MOTA and in IDF1, was still lower than other state-of-the-art algorithms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Kim 等人 [[93](#bib.bib93)] 提出了将一种所谓的双线性 LSTM 网络用作 MHT-DAM [[83](#bib.bib83)]
    算法的门控步骤，即利用 LSTM 计算的相似度分数来决定是否修剪假设树的某个分支。LSTM 单元具有经过修改的前向传播（灵感来源于 [[83](#bib.bib83)]
    中提出的在线递归最小二乘估计器），输入为过去帧中用 ResNet-50 CNN 提取的轨迹的外观特征。LSTM 单元的输出是一个表示轨迹历史外观的特征矩阵，该矩阵随后与需要与轨迹比较的检测的外观特征向量相乘。其上的全连接层最终计算了轨迹与检测之间的相似度分数。作者声称，这种修改后的
    LSTM 能够存储比经典 LSTM 更长期的外观模型。他们还提出添加一个运动建模经典 LSTM 来计算历史运动特征（使用归一化的边界框坐标和大小），然后将这些特征与外观特征连接在一起，之后才进行全连接层和最终的
    softmax 以输出相似度分数。这两个 LSTM 首先分别训练，然后联合微调。训练数据还包括了定位误差和丢失检测，以更贴近真实世界的数据。他们使用了 MOT15、MOT17、ETH、KITTI
    以及其他小型数据集进行训练，并在 MOT16 和 MOT17 上评估了模型。他们展示了他们的模型对检测质量敏感，因为在使用公开的 Faster R-CNN
    和 SDP 检测时，MHT-DAM 的 MOTA 性能有所提升，而在使用公开的 DPM 检测时表现则较差。无论使用何种检测，他们似乎都获得了更高的 IDF1
    分数，而他们的整体结果也反映了这一点，因为他们在所有使用 MHT 基于算法的方法中获得了最高的 IDF1 分数。然而，无论是 MOTA 还是 IDF1 测量的跟踪质量，仍然低于其他最先进的算法。
- en: 'A similar use of a RNN has been recently presented by Maksai et al. [[113](#bib.bib113)],
    who also employed a LSTM to compute tracklet scores in a variation of the MHT
    algorithm, that grows and prunes tracklets iteratively and then tries to select
    the set of tracklets that maximizes said score.^(16)^(16)16While it is not an
    explicit affinity measure, it can still be seen as an evaluation of the effect
    of merging two tracklets and thus plays a similar role (i.e. taking decisions
    about associating tracklets) as other affinities presented in this section. The
    goal of their work was to solve two frequent problems in training recurrent networks
    for multiple object tracking: the loss-evaluation mismatch, that arises when a
    network is trained by optimizing a loss that is not well-aligned to the evaluation
    metric used at inference time (e.g. classification score vs. MOTA); the exposure
    bias, that is present when the model is not exposed to its own errors during the
    training process. In order to solve the first problem, they introduced a novel
    way to score tracklets (using a RNN) that is a direct proxy of the IDF1 metric
    and does not use the ground truth; the network can then be trained to optimize
    such metric. The second problem was solved instead by adding to the training set
    of the network the tracklets computed using the current version of the network,
    together with hard example mining and random tracklet merging during training;
    in this way, the training set distribution should be more similar to the inference-time
    input data distribution. The network used was a Bidirectional LSTM, on top of
    an embedding layer that took as input various features. The authors presented
    a version of the algorithm using only geometric features and a version that instead
    used appearance-based features, that performed better. Lots of ablation studies
    were run, and various alternative approaches were tested. The final algorithm
    was able to reach top-performance on various MOT datasets (MOT15, MOT17, DukeMTMC
    [[21](#bib.bib21)]) when considering the IDF1 metric, even though it didn’t excel
    in MOTA.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Maksai 等人最近提出了类似的 RNN 使用方法 [[113](#bib.bib113)]，他们还使用了 LSTM 来计算 MHT 算法变体中的轨迹得分，该变体通过迭代增长和修剪轨迹，然后尝试选择最大化该得分的轨迹集。^(16)^(16)16
    虽然这不是一个明确的亲和度度量，但它仍然可以被视为合并两个轨迹的效果评估，因此在决定轨迹关联时发挥了类似的作用（即与本节中介绍的其他亲和度相似）。他们工作的目标是解决训练递归网络用于多目标跟踪中的两个常见问题：损失评估不匹配，即当网络通过优化一个与推理时使用的评估指标（例如分类得分与
    MOTA）不匹配的损失进行训练时出现的情况；暴露偏差，即模型在训练过程中没有接触到自身错误时存在的偏差。为了解决第一个问题，他们引入了一种新的轨迹评分方法（使用
    RNN），这种方法是 IDF1 指标的直接代理，并且不使用真实数据；网络可以通过优化这种指标进行训练。第二个问题则通过将使用当前版本网络计算的轨迹、困难样本挖掘以及训练期间随机轨迹合并添加到网络的训练集中来解决；这样，训练集的分布应该更类似于推理时输入数据的分布。所使用的网络是一个双向
    LSTM，位于一个嵌入层之上，该层以各种特征作为输入。作者展示了一个仅使用几何特征的算法版本和一个使用基于外观特征的版本，后者表现更好。进行了大量的消融研究，并测试了各种替代方法。最终算法能够在多个
    MOT 数据集（MOT15、MOT17、DukeMTMC [[21](#bib.bib21)]）上在 IDF1 指标上达到顶级性能，尽管在 MOTA 上没有表现出色。
- en: Among the other approaches in the MHT family using RNNs we can also find [[104](#bib.bib104)],
    where Chen et al. used a so-called Recurrent Metric Network (RMNet) to compute
    appearance affinity between tracklet hypotheses and detections (together with
    a motion-based affinity) in their Batch Multi-Hypothesis Tracking strategy. The
    RMNet is an LSTM that takes as input appearance features of the detection sequence
    under consideration, extracted with a ResNet CNN, and outputs a similarity score
    together with bounding box regression parameters. A dual-threshold approach in
    gating and forming hypothesis was used, and a re-find reward was employed to encourage
    recovery from occlusions. The hypotheses were selected by casting the problem
    as a binary linear programming one, solved using lpsolve. Kalman filter was finally
    used to smooth the trajectories. Evaluation was performed on MOT15, PETS2009 [[34](#bib.bib34)],
    TUD [[140](#bib.bib140)] and KITTI, obtaining better results on the IDF1 metric,
    that gives more weight to people re-identification, than on MOTA.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用RNN的MHT家族中的其他方法中，我们还可以找到[[104](#bib.bib104)]，其中Chen等人使用了所谓的递归度量网络（RMNet）来计算轨迹假设和检测之间的外观亲和力（以及基于运动的亲和力），并将其应用于他们的批量多假设跟踪策略。RMNet是一个LSTM，它以考虑中的检测序列的外观特征作为输入，这些特征由ResNet
    CNN提取，并输出一个相似性分数以及边界框回归参数。使用了双阈值方法进行门控和假设形成，并采用了再发现奖励来鼓励从遮挡中恢复。通过将问题转化为二进制线性规划问题来选择假设，使用lpsolve进行求解。最后，使用卡尔曼滤波器平滑轨迹。评估在MOT15、PETS2009
    [[34](#bib.bib34)]、TUD [[140](#bib.bib140)]和KITTI上进行，结果在IDF1指标上优于MOTA，IDF1对人员再识别给予了更多权重。
- en: 3.3.5 Other recurrent networks
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 其他递归网络
- en: Fang et al. [[96](#bib.bib96)] used instead Gated Recurrent Units (GRUs) [[141](#bib.bib141)]
    inside their Recurrent Autoregressive Network (RAN) framework for pedestrian tracking.
    The GRUs were used to estimate the parameters of autoregressive models, one for
    motion and the other for appearance for each tracked target, that computed the
    probability of observing a given detection motion/appearance based on the tracklet’s
    past motion/appearance features. The two probabilities, that can be easily seen
    as a kind of affinity measure, were then multiplied together to obtain a final
    association probability, used to solve a bipartite matching problem for association
    between tracklets and detections following the algorithm in [[134](#bib.bib134)].
    The RAN training step was formulated as a maximum likelihood estimation problem.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Fang等人[[96](#bib.bib96)]在其递归自回归网络（RAN）框架中使用了门控递归单元（GRUs）[[141](#bib.bib141)]用于行人跟踪。GRUs用于估计自回归模型的参数，一个用于运动，另一个用于每个跟踪目标的外观，这些模型计算了基于轨迹的过去运动/外观特征观察到给定检测运动/外观的概率。然后将这两个概率相乘以获得最终的关联概率，用于解决轨迹和检测之间的二分匹配问题，按照[[134](#bib.bib134)]中的算法进行。RAN训练步骤被表述为最大似然估计问题。
- en: Kieritz et al. [[60](#bib.bib60)] used a recurrent 2-hidden-layer multi-layer
    perceptron (MLP) to compute an appearance affinity score between a detection and
    a tracklet. Such affinity was then given as input to another MLP, together with
    track and detection confidence scores, to predict an aggregate affinity score
    (called association metric). Such score was finally used by the Hungarian algorithm
    to perform association. The method reached top performance on the UA-DETRAC dataset
    [[32](#bib.bib32)], but the performance on MOT16 was not very good when compared
    with other algorithms using private detections.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Kieritz等人[[60](#bib.bib60)]使用了递归的2隐藏层多层感知机（MLP）来计算检测与轨迹之间的外观亲和力分数。该亲和力随后作为输入提供给另一个MLP，与轨迹和检测的置信度分数一起，用于预测综合亲和力分数（称为关联指标）。该分数最终由匈牙利算法用于执行关联。该方法在UA-DETRAC数据集[[32](#bib.bib32)]上达到了顶尖性能，但与使用私人检测的其他算法相比，MOT16上的性能并不理想。
- en: 3.3.6 CNNs for affinity computation
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.6 用于亲和力计算的卷积神经网络（CNNs）
- en: 'Other algorithms used instead CNNs to compute some kind of similarity score.
    Tang et al. [[51](#bib.bib51)] tested the use of 4 different CNNs to compute an
    affinity score between nodes in a graph, with the association task being formulated
    as a minimum cost lifted multicut problem [[142](#bib.bib142)]: it can be seen
    as a graph clustering problem, where each output cluster represents a single tracked
    object. The costs associated to the edges accounted for the similarity between
    two detections. Such similarity was a combination of person re-identification
    confidence, deep correspondence matching and spatio-temporal relations. To compute
    the person re-identification affinity, various architectures were tested (after
    being trained on a dataset of 2511 identities extracted from MOT15, MOT16, CUHK03,
    Market-1501 datasets), but the best performing one was the novel StackNetPose.
    It incorporated body part information extracted using the DeepCut body part detector
    [[105](#bib.bib105)] (see section [3.2.2](#S3.SS2.SSS2 "3.2.2 CNNs as visual feature
    extractors ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep learning
    in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")). The 14 score
    maps for the body parts of two images were stacked together with the two images
    themselves to produce a 20-channel input. The network followed the VGG-16 architecture
    and output an affinity score between the two input identities. Differently from
    Siamese CNNs, the pair of images were able to ‘communicate’ in the early stages
    of the network. The authors showed that the StackNetPose network performed better
    in the person re-identification task, and thus they used it to compute the ReID
    affinity. The combined affinity score was computed by multiplying a weight vector
    (learned with logistic regression, and dependent on the time interval between
    the two detections) with a 14-d vector containing ReID affinity, DeepMatching-based
    affinity [[143](#bib.bib143)], a spatio-temporal affinity score, the minimum of
    the two detection confidences and quadratic terms with all the pairwise combinations
    of the previously mentioned terms. The authors showed that combining all these
    features produced better results, and together with the improvement in framing
    the problem as a minimum cost lifted multicut problem (solved heuristically using
    the algorithm proposed in [[144](#bib.bib144)]), they managed to reach state-of-the-art
    performance (measured in MOTA score) on the MOT16 dataset at the time of publishing.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 其他算法使用了除CNN之外的方法来计算某种相似性得分。Tang等人[[51](#bib.bib51)]测试了4种不同的CNN来计算图中节点之间的亲和度得分，其中关联任务被形式化为最小成本的提升多切割问题[[142](#bib.bib142)]：这可以视作一个图聚类问题，其中每个输出聚类代表一个单独跟踪的对象。与边相关的成本考虑了两个检测之间的相似性。这种相似性是个人再识别信心、深度对应匹配和时空关系的结合。为了计算个人再识别的亲和度，测试了各种架构（在MOT15、MOT16、CUHK03和Market-1501数据集中提取的2511个身份上进行训练），但表现最佳的是新型的StackNetPose。它结合了使用DeepCut身体部位检测器[[105](#bib.bib105)]提取的身体部位信息（参见[3.2.2](#S3.SS2.SSS2
    "3.2.2 CNNs作为视觉特征提取器 ‣ 3.2 深度学习在特征提取和运动预测中的应用 ‣ 3 深度学习在多目标跟踪中的应用 ‣ 视频多目标跟踪中的深度学习：综述")）。将两张图像的14个身体部位得分图与这两张图像本身叠加在一起，生成了20通道输入。网络遵循了VGG-16架构，并输出了两个输入身份之间的亲和度得分。与Siamese
    CNN不同，图像对在网络的早期阶段能够‘交流’。作者展示了StackNetPose网络在个人再识别任务中的表现更好，因此他们使用它来计算ReID亲和度。结合的亲和度得分是通过将一个权重向量（使用逻辑回归学习，并且依赖于两个检测之间的时间间隔）与一个包含ReID亲和度、基于DeepMatching的亲和度[[143](#bib.bib143)]、时空亲和度得分、两个检测置信度的最小值以及与之前提到的所有成对组合的二次项的14维向量相乘来计算的。作者表明，结合所有这些特征产生了更好的结果，加上将问题框定为最小成本提升多切割问题（使用[[144](#bib.bib144)]中提出的算法启发式地解决），他们在发布时在MOT16数据集上达到了最先进的性能（以MOTA得分衡量）。
- en: Another approach using CNNs was presented in [[145](#bib.bib145)], where Chen
    et al. used a Particle Filter [[146](#bib.bib146)] to predict target motion, weighting
    the importance of each particle using a modified Faster R-CNN network. Such model
    was trained to predict the probability that the bounding box contains an object,
    but it was also augmented with a target-specific branch, that took as input features
    from lower layers of the CNN and merged them with the target historical features
    to predict the probability of the two objects being the same. The difference with
    the previous approaches is that here the affinity is computed between the sampled
    particles and the tracked target, instead of being computed between targets and
    detections. The detections that did not overlap with the tracked objects were
    instead used to initialize new tracks or retrieve missing objects. Despite being
    an online tracking algorithm, it was able to reach top performance on MOT15 at
    the time of publishing, both when using public detections and when using private
    ones (obtained from [[147](#bib.bib147)]).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用 CNN 的方法在 [[145](#bib.bib145)] 中提出，其中 Chen 等人使用了粒子滤波器 [[146](#bib.bib146)]
    来预测目标运动，通过修改的 Faster R-CNN 网络加权每个粒子的权重。该模型被训练以预测边界框包含对象的概率，同时还增加了一个特定目标的分支，该分支以
    CNN 的低层特征作为输入，并将其与目标历史特征合并，以预测两个对象是否相同。与之前的方法的不同之处在于，这里亲和力是计算样本粒子与跟踪目标之间的，而不是计算目标与检测之间的。未与跟踪对象重叠的检测结果则用于初始化新的轨迹或检索丢失的对象。尽管是一种在线跟踪算法，但在发布时能够在
    MOT15 上达到顶级性能，无论是使用公共检测还是使用私人检测（从 [[147](#bib.bib147)] 获得）。
- en: 'Zhou et al. [[117](#bib.bib117)] used a visual-similarity CNN, similar to the
    ResNet-101 based visual-displacement CNN presented in section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey"),
    that outputs affinity scores between the detections and the tracklet boxes predicted
    by the Deep Continuous Conditional Random Fields. This visual affinity score was
    merged with a spatial similarity using IoU, and then the detection with the highest
    score was associated to each tracklet; in case of conflicts, the Hungarian algorithm
    was employed. The method reached results comparable to state-of-the-art online
    MOT algorithms on MOT15 and MOT16 in terms of MOTA score.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhou 等人 [[117](#bib.bib117)] 使用了一种视觉相似性 CNN，类似于第 [3.2.3](#S3.SS2.SSS3 "3.2.3
    Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey") 节中提出的基于
    ResNet-101 的视觉位移 CNN，该 CNN 输出检测结果和由深度连续条件随机场预测的轨迹框之间的亲和力分数。该视觉亲和力分数与使用 IoU 的空间相似性进行了合并，然后将具有最高分数的检测结果与每个轨迹关联；如有冲突，则使用匈牙利算法。该方法在
    MOTA 评分方面达到了与最先进的在线 MOT 算法相当的结果，适用于 MOT15 和 MOT16。'
- en: 3.3.7 Siamese CNNs
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.7 孪生 CNNs
- en: 'Siamese CNNs are also a common approach used in affinity computation. An example
    of Siamese CNN is shown in figure [5](#S3.F5 "Figure 5 ‣ 3.2.3 Siamese networks
    ‣ 3.2 DL in feature extraction and motion prediction ‣ 3 Deep learning in MOT
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey"). The approaches presented
    here decided to directly use the output of the Siamese CNN as an affinity, instead
    of employing classical distances between feature vectors extracted from the penultimate
    layer of the network, like the algorithms presented in section [3.2.3](#S3.SS2.SSS3
    "3.2.3 Siamese networks ‣ 3.2 DL in feature extraction and motion prediction ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").
    For example, Ma et al. [[148](#bib.bib148)] used one to compute affinities between
    tracklets in a two-step algorithm. They chose to apply hierarchical correlation
    clustering, solving two successive lifted multicut problems: local data association
    and global data association. In the local data association step temporally-close
    detections were joined together by using the robust similarity measure presented
    in [[149](#bib.bib149)], that uses DeepMatching and detection confidences to compute
    an affinity score between detections. In this step, only edges between close detections
    were inserted into the graph. The multicut problem was solved with the heuristic
    algorithm proposed in [[144](#bib.bib144)]. In the global data association step,
    local tracks that were split by long-term occlusion needed to be joined together,
    and a fully-connected graph with all the tracklets was then built. The Siamese
    CNN was used to compute the affinities that would serve as edge costs in the graph.
    The architecture was based on GoogLeNet [[2](#bib.bib2)] and it was pretrained
    on ImageNet. The net was then trained on the Market-1501 ReID dataset and then
    fine-tuned on the MOT15 and MOT16 training sequences. Besides the verification
    layer, that output a similarity score between the two images, two classification
    layers were added to the network only during training to classify the identity
    of each training image; this was shown to improve the network performance in computing
    the affinity score. This so-called ‘generic’ ReID net was also fine-tuned on each
    test sequence in an unsupervised manner, without using any ground truth information,
    to adapt the net to the illumination conditions, resolution, camera angle, etc.
    of each particular sequence. This was done by sampling positive and negative detection
    pairs by looking at the tracklets built in the local data association step. The
    effectiveness of the algorithm was proven by the results obtained on MOT16, where
    it is at the time of writing the best performing method with a published paper,
    with a 49.3 MOTA score.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生卷积神经网络（Siamese CNNs）也是一种常用于亲和力计算的方法。图 [5](#S3.F5 "图 5 ‣ 3.2.3 孪生网络 ‣ 3.2 深度学习在特征提取和运动预测中的应用
    ‣ 3 深度学习在多目标跟踪中的应用 ‣ 视频多目标跟踪中的深度学习综述") 展示了一个孪生卷积神经网络的示例。这里提出的方法决定直接使用孪生卷积神经网络的输出作为亲和力，而不是采用经典的特征向量距离，这些特征向量是从网络倒数第二层提取的，如[3.2.3](#S3.SS2.SSS3
    "3.2.3 孪生网络 ‣ 3.2 深度学习在特征提取和运动预测中的应用 ‣ 3 深度学习在多目标跟踪中的应用 ‣ 视频多目标跟踪中的深度学习综述")节中介绍的算法。例如，Ma等人[[148](#bib.bib148)]使用一种算法在两步中计算轨迹片段之间的亲和力。他们选择应用层次相关聚类，解决两个连续的多割问题：局部数据关联和全局数据关联。在局部数据关联步骤中，通过使用[[149](#bib.bib149)]中提出的鲁棒相似度度量，将时间上接近的检测结果结合在一起，该度量使用DeepMatching和检测置信度来计算检测之间的亲和力分数。在这一阶段，仅在图中插入了接近检测之间的边。多割问题通过[[144](#bib.bib144)]中提出的启发式算法解决。在全局数据关联步骤中，需要将被长期遮挡分开的局部轨迹结合在一起，然后建立一个全连接图，包含所有的轨迹片段。孪生卷积神经网络被用来计算作为图中边的代价的亲和力。这一架构基于GoogLeNet
    [[2](#bib.bib2)]，并在ImageNet上进行了预训练。然后，网络在Market-1501 ReID数据集上进行了训练，之后在MOT15和MOT16训练序列上进行了微调。除了输出两张图像之间相似度分数的验证层外，还在训练过程中向网络中添加了两个分类层，以分类每张训练图像的身份；这被证明提高了网络在计算亲和力分数方面的性能。这种所谓的“通用”ReID网络还在每个测试序列上进行了无监督微调，无需使用任何地面真实信息，以使网络适应每个特定序列的光照条件、分辨率、摄像机角度等。这是通过观察局部数据关联步骤中构建的轨迹片段，采样正负检测对来完成的。该算法的有效性通过在MOT16上获得的结果得到证明，目前在写作时，它是最有效的方法，拥有49.3的MOTA得分。
- en: 'As explained in section [3.2.3](#S3.SS2.SSS3 "3.2.3 Siamese networks ‣ 3.2
    DL in feature extraction and motion prediction ‣ 3 Deep learning in MOT ‣ Deep
    Learning in Video Multi-Object Tracking: A Survey"), Lee et al. [[119](#bib.bib119)]
    used a Feature Pyramid Siamese Network to extract appearance features. When employing
    this kind of network in the MOT problem, a vector of motion features was concatenated
    to the appearance features and 3 fully-connected layers were then added on top
    to predict an affinity score between a track and a detection; the network was
    trained end-to-end. Detections were then associated iteratively, starting from
    the pairs with highest affinity scores and stopping when the score got below a
    threshold. The method obtained top performance results among the online algorithms
    on the MOT17 dataset at the time of publishing.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '如在章节[3.2.3](#S3.SS2.SSS3 "3.2.3 Siamese networks ‣ 3.2 DL in feature extraction
    and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey")中解释的那样，Lee等人[[119](#bib.bib119)]使用了特征金字塔Siamese网络来提取外观特征。在MOT问题中应用这种网络时，将运动特征的向量与外观特征连接在一起，然后在其上添加了3个全连接层，以预测轨迹与检测之间的亲和分数；该网络进行了端到端训练。然后，检测结果进行了迭代关联，从具有最高亲和分数的对开始，并在分数低于阈值时停止。该方法在发布时在MOT17数据集上获得了在线算法中的最佳性能结果。'
- en: 3.4 DL in Association/Tracking step
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 深度学习在关联/跟踪步骤中的应用
- en: 'Some works, albeit not as many as for the other steps in the pipeline, have
    used deep learning models to improve the association process performed by classical
    algorithms, like the Hungarian algorithm, or to manage the track status (e.g.
    by deciding to start or terminate a track). We are going to present them in this
    section, including the use of RNNs (section [3.4.1](#S3.SS4.SSS1 "3.4.1 Recurrent
    neural networks ‣ 3.4 DL in Association/Tracking step ‣ 3 Deep learning in MOT
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey")), deep multi-layer
    perceptrons (section [3.4.2](#S3.SS4.SSS2 "3.4.2 Deep Multi-Layer Perceptron ‣
    3.4 DL in Association/Tracking step ‣ 3 Deep learning in MOT ‣ Deep Learning in
    Video Multi-Object Tracking: A Survey")) and deep reinforcement learning agents
    (section [3.4.3](#S3.SS4.SSS3 "3.4.3 Deep Reinforcement Learning agents ‣ 3.4
    DL in Association/Tracking step ‣ 3 Deep learning in MOT ‣ Deep Learning in Video
    Multi-Object Tracking: A Survey")).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '一些研究工作，尽管不如管道中的其他步骤多，但已经使用了深度学习模型来改进经典算法（如匈牙利算法）执行的关联过程，或管理轨迹状态（例如，通过决定何时启动或终止轨迹）。我们将在本节中介绍这些工作，包括RNN的使用（章节[3.4.1](#S3.SS4.SSS1
    "3.4.1 Recurrent neural networks ‣ 3.4 DL in Association/Tracking step ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")）、深度多层感知器（章节[3.4.2](#S3.SS4.SSS2
    "3.4.2 Deep Multi-Layer Perceptron ‣ 3.4 DL in Association/Tracking step ‣ 3 Deep
    learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")）和深度强化学习智能体（章节[3.4.3](#S3.SS4.SSS3
    "3.4.3 Deep Reinforcement Learning agents ‣ 3.4 DL in Association/Tracking step
    ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")）。'
- en: 3.4.1 Recurrent neural networks
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 循环神经网络
- en: 'A first example of algorithms employing DL to manage the track status is the
    one presented by Milan et al. in [[133](#bib.bib133)], already described in section
    [3.3.1](#S3.SS3.SSS1 "3.3.1 Recurrent neural networks and LSTMs ‣ 3.3 DL in affinity
    computation ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey"), that used a RNN to predict the probability of existence of a track
    in each frame, thus helping with the decision of when to initiate or terminate
    the tracks.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '使用深度学习来管理轨迹状态的第一个算法示例是Milan等人在[[133](#bib.bib133)]中提出的，该算法已在章节[3.3.1](#S3.SS3.SSS1
    "3.3.1 Recurrent neural networks and LSTMs ‣ 3.3 DL in affinity computation ‣
    3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey")中描述，该算法使用了RNN来预测每帧中轨迹的存在概率，从而帮助决定何时启动或终止轨迹。'
- en: 'Ma et al. [[116](#bib.bib116)] used a bidirectional GRU RNN to decide where
    to split tracklets. The algorithm proceeded in three main stages: a tracklet generation
    step, that included a NMS step to remove redundant detections and then employed
    the Hungarian algorithm with appearance and motion affinity together to form high-confidence
    tracklets; then, a tracklet cleaving step was performed: since a tracklet might
    contain an ID switch error due to occlusions, this step aimed to split the tracklets
    at the point where the ID switch happened, in order to obtain two separate tracklets
    that contained the same identity; finally, a tracklet reconnection step was employed,
    using a customized association algorithm that made use of features extracted by
    a Siamese bidirectional GRU. The gaps within the newly-formed tracklets were then
    filled with polynomial curve fitting. The cleaving step was performed with a bidirectional
    GRU RNN, that used features extracted by a Wide Residual Network CNN [[92](#bib.bib92)].
    The GRU output a pair of feature vectors for each frame (one for each direction
    of the GRU); then the distance between pairs of such feature vectors was computed
    and a distance vector was obtained. The highest value in this vector indicated
    where to split the tracklet, provided that the score was higher than a threshold.
    The reconnection GRU was similar, but it had an additional FC layer on top of
    the GRU and a temporal pooling layer to extract a feature vector representing
    the whole tracklet; the distance between the features of the two tracklets was
    then used to decide which tracklets to reconnect. The algorithm reached results
    comparable to state-of-the-art on the MOT16 dataset.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Ma 等人 [[116](#bib.bib116)] 使用了一个双向 GRU RNN 来决定轨迹片段的分割位置。算法分为三个主要阶段：首先是轨迹片段生成步骤，包括一个
    NMS 步骤以去除冗余检测，然后结合外观和运动亲和力使用匈牙利算法形成高置信度的轨迹片段；接着进行轨迹片段切割步骤：由于轨迹片段可能由于遮挡而包含 ID 切换错误，该步骤旨在在
    ID 切换发生的点切割轨迹片段，以获得两个包含相同身份的独立轨迹片段；最后，进行轨迹片段重连步骤，使用自定义的关联算法，该算法利用由双向 GRU 提取的特征。然后，通过多项式曲线拟合填补新形成的轨迹片段中的间隙。切割步骤使用了一个双向
    GRU RNN，该 RNN 使用由 Wide Residual Network CNN [[92](#bib.bib92)] 提取的特征。GRU 为每一帧输出一对特征向量（每个
    GRU 方向一个）；然后计算这些特征向量对之间的距离，并得到一个距离向量。该向量中的最高值指示轨迹片段的切割位置，前提是得分高于阈值。重连 GRU 类似，但其上有一个额外的
    FC 层以及一个时间池化层，用于提取表示整个轨迹片段的特征向量；然后用这两个轨迹片段的特征之间的距离来决定连接哪些轨迹片段。该算法在 MOT16 数据集上达到了与最先进技术相当的结果。
- en: 3.4.2 Deep Multi-Layer Perceptron
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 深度多层感知机
- en: 'Despite not being a very common approach, deep multi-layer perceptrons (MLP)
    have also been employed to guide the tracking process. For example, Kieritz et
    al. [[60](#bib.bib60)] used a MLP with two hidden layers to compute track confidence
    scores, taking as input the track score at the previous step and various information
    about the last associated detection (like association score and detection confidence).
    This confidence score was then used to manage the termination of tracks: they
    decided in fact to keep a fixed number of targets through time, replacing with
    new tracks the older ones that had the lowest confidence scores. The rest of the
    algorithm has been explained in section [3.3.5](#S3.SS3.SSS5 "3.3.5 Other recurrent
    networks ‣ 3.3 DL in affinity computation ‣ 3 Deep learning in MOT ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey").'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这不是一种很常见的方法，深度多层感知机（MLP）也被用于指导跟踪过程。例如，Kieritz 等人 [[60](#bib.bib60)] 使用了一个具有两层隐藏层的
    MLP 来计算轨迹置信度分数，输入为前一步的轨迹分数以及关于最后一次关联检测的各种信息（如关联分数和检测置信度）。然后使用这个置信度分数来管理轨迹的终止：他们决定在时间中保持固定数量的目标，替换掉置信度分数最低的旧轨迹。算法的其余部分已在章节
    [3.3.5](#S3.SS3.SSS5 "3.3.5 其他递归网络 ‣ 3.3 亲和计算中的深度学习 ‣ 3 深度学习在多目标跟踪中的应用") 中解释。
- en: 3.4.3 Deep Reinforcement Learning agents
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 深度强化学习代理
- en: 'Some works have used Deep Reinforcement Learning (RL) agents to take decisions
    in the tracking process. Rosello et al. [[131](#bib.bib131)], as explained in
    section [3.2.6](#S3.SS2.SSS6 "3.2.6 Other approaches ‣ 3.2 DL in feature extraction
    and motion prediction ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey"), used multiple deep RL agents to manage the various tracked
    targets, deciding when to start and stop tracks and influencing the operation
    of the Kalman filter. The agent was modeled with a MLP with 3 hidden layers.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作已经使用深度强化学习（RL）代理来在跟踪过程中做出决策。如在第[3.2.6](#S3.SS2.SSS6 "3.2.6 其他方法 ‣ 3.2 深度学习在特征提取和运动预测中的应用
    ‣ 3 深度学习在MOT中的应用 ‣ 视频多目标跟踪中的深度学习：综述")节中解释，Rosello等人[[131](#bib.bib131)]使用了多个深度RL代理来管理各种跟踪目标，决定何时开始和停止跟踪，并影响卡尔曼滤波器的操作。该代理使用了一个具有3个隐藏层的多层感知机（MLP）进行建模。
- en: 'Ren et al. [[150](#bib.bib150)] also used multiple deep RL agents in a collaborative
    environment to manage the association task. The algorithm was mainly composed
    of two parts: a prediction network and a decision network. The prediction network
    was a CNN that was learned to predict the movement of the target in the new frame
    looking at the target and at the new image, and also using the recent tracklet
    trajectory. The decision network was instead a collaborative system that consisted
    of multiple agents (one for each tracked target) and the environment. Each agent
    took decisions based on the information about themselves, the neighbours and the
    environment; the interactions between the agents and the environment were exploited
    by maximizing a shared utility function: the agents thus did not operate independently
    from each other. Every agent/object was represented by a trajectory, its appearance
    features (extracted using MDNet [[151](#bib.bib151)]) and its current position.
    The environment was represented by the detections in the new frame. The detection
    network took as input, for each target, its predicted location in the new frame
    (output by the prediction network), the nearest target and the nearest detection,
    and based on various factors, such as the detection reliability and the target
    occlusion status, took one among various actions: updating the track and its appearance
    features using both the prediction and the detection, ignoring the detection and
    only using the prediction to update the track, detecting an occlusion of the tracked
    target, deleting the track. The agents were modelled using 3 FC layers on top
    of the feature extraction part of the MDNet. Various ablation studies showed the
    effectiveness of using the prediction and detection networks instead of linear
    motion models and Hungarian algorithm, respectively, and the method obtained very
    good results on the MOT15 and MOT16 datasets, reaching state-of-the-art performance
    among online methods, despite suffering from a relatively high number of ID switches.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Ren等人[[150](#bib.bib150)]也在协作环境中使用了多个深度强化学习代理来管理关联任务。该算法主要由两个部分组成：预测网络和决策网络。预测网络是一个卷积神经网络（CNN），通过观察目标及新图像以及最近的轨迹来学习预测目标在新帧中的运动。决策网络则是一个由多个代理（每个跟踪目标一个）和环境组成的协作系统。每个代理根据关于自身、邻近对象和环境的信息做出决策；代理和环境之间的互动通过最大化共享效用函数来利用：因此代理不会相互独立操作。每个代理/对象通过轨迹、外观特征（使用MDNet[[151](#bib.bib151)]提取）和当前位置表示。环境由新帧中的检测表示。检测网络以每个目标在新帧中的预测位置（由预测网络输出）、最近目标和最近检测作为输入，根据检测可靠性和目标遮挡状态等各种因素，采取如下动作之一：使用预测和检测更新轨迹及其外观特征、忽略检测仅使用预测更新轨迹、检测跟踪目标的遮挡、删除轨迹。代理使用MDNet的特征提取部分之上的3个全连接层进行建模。各种消融研究表明，使用预测和检测网络而非线性运动模型和匈牙利算法的有效性，该方法在MOT15和MOT16数据集上获得了非常好的结果，在在线方法中达到了最先进的性能，尽管存在较高的ID切换数量。
- en: 3.5 Other uses of DL in MOT
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 深度学习在MOT中的其他应用
- en: 'In this section we will present other interesting uses of deep learning models
    that don’t neatly fit into one of the four common steps of a multiple object tracking
    algorithm. For this reason, such works have not been included in table LABEL:tab:summary_table,
    but are summarized instead in table [1](#S3.T1 "Table 1 ‣ 3.5 Other uses of DL
    in MOT ‣ 3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking:
    A Survey").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍一些不完全符合多目标跟踪算法四个常见步骤的深度学习模型的有趣用途。因此，这些工作没有包含在表 LABEL:tab:summary_table
    中，而是总结在表 [1](#S3.T1 "Table 1 ‣ 3.5 Other uses of DL in MOT ‣ 3 Deep learning in
    MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey") 中。'
- en: '|  | Detection | Description | Mode | Source and data |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | 检测 | 描述 | 模式 | 来源与数据 |'
- en: '| [[152](#bib.bib152)] | N/A | They integrate a bounding box regression step
    in various existing MOT algorithms. The regression is done using Deep Reinforcement
    Learning using CNN features. | N/A |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| [[152](#bib.bib152)] | 不适用 | 他们将边界框回归步骤集成到各种现有的 MOT 算法中。回归通过使用 CNN 特征的深度强化学习完成。
    | 不适用 |  |'
- en: '| [[153](#bib.bib153)] | Public | An ensemble of 2 CNNs, color histograms and
    a KLT motion detector are used to compute likelihoods for a Markov Chain Monte
    Carlo sampling; the position sampling was used to form short tracklets. A Changing
    Point Detection algorithm was employed to merge and delete tracklets. | Online
    |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [[153](#bib.bib153)] | 公开 | 一个包含 2 个 CNN、颜色直方图和 KLT 运动检测器的集成用于计算马尔可夫链蒙特卡洛采样的可能性；位置采样用于形成短轨迹。使用了变化点检测算法来合并和删除轨迹。
    | 在线 |  |'
- en: '| [[154](#bib.bib154)] | CNN | Multi-Bernoulli Filter with a novel Interactive
    Likelihood, computed using a CNN. | Online |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| [[154](#bib.bib154)] | CNN | 具有新颖交互似然的多伯努利滤波器，通过 CNN 计算。 | 在线 |  |'
- en: '| [[155](#bib.bib155)] | Public | Body detections are refined using head detections
    obtained with a CNN [[156](#bib.bib156)]. A modified version of the Frank-Wolfe
    algorithm is used to solve a correlation clustering problem for association, using
    spatial and temporal costs. | Batch |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] | 公开 | 使用从 CNN [[156](#bib.bib156)] 获得的头部检测来细化身体检测。使用修改版
    Frank-Wolfe 算法解决关联的相关聚类问题，利用空间和时间成本。 | 批量 |  |'
- en: '| [[157](#bib.bib157)] | Public | Modified MDNet CNN with target-specific branches
    to compute affinities between targets and candidates extracted with Gaussian sampling.
    Combination of appearance and motion features to reduce ID Switches. | Online
    |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bib157)] | 公开 | 使用带有目标特定分支的改进版 MDNet CNN 计算目标与通过高斯采样提取的候选之间的亲和度。结合外观和运动特征以减少身份切换。
    | 在线 |  |'
- en: '| [[158](#bib.bib158)] | Public | CNN to extract app features and LSTM to extract
    motion features. The LSTM is part of a BF-Net, that performs Bayesian filtering
    and uses the output from Hungarian algorithm for track refinement. | Online |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | 公开 | CNN 用于提取应用特征，LSTM 用于提取运动特征。LSTM 是 BF-Net 的一部分，该网络执行贝叶斯滤波并使用匈牙利算法的输出进行轨迹精化。
    | 在线 |  |'
- en: '| [[159](#bib.bib159)] | Public | PafNet and PartNet CNNs to distinguish targets
    from background and among themselves. KCF SOT tracker is used. SVM+Hungarian algorithm
    for error recovering. CNN trained with RL for model updating. | Online |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| [[159](#bib.bib159)] | 公开 | 使用 PafNet 和 PartNet CNN 来区分目标与背景及其相互之间。使用 KCF
    SOT 跟踪器。SVM+匈牙利算法用于错误恢复。使用 RL 训练的 CNN 用于模型更新。 | 在线 |  |'
- en: 'Table 1: Information summary about methods using DL that don’t fit the 4-step
    scheme.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 不符合 4 步骤方案的使用深度学习方法的信息总结。'
- en: One example is [[152](#bib.bib152)], where Jiang et al. use a Deep RL agent
    to perform bounding box regression after the use of one of many MOT algorithms.
    The procedure is in fact completely independent from the tracking algorithm employed,
    and can be used a posteriori to increase the accuracy of the model. A VGG-16 CNN
    was used to extract appearance features from the region enclosed by the bounding
    box, then those features were concatenated to a vector representing the history
    of the last 10 actions taken by the agent. Finally a Q-network [[160](#bib.bib160)]
    made of 3 fully-connected layers was used to predict one among 13 possible actions,
    that included motion and scaling of the bounding box and a termination action,
    to signal the completion of the regression. The use of this bounding box regression
    technique on various state-of-the-art MOT algorithms allowed an improvement between
    2 and 7 absolute MOTA points on the MOT15 dataset, reaching top score among public
    detections methods. The authors also showed that their regression approach had
    better results than using conventional methods, such as the bounding box regression
    computed by a Faster R-CNN model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是[[152](#bib.bib152)]，在这个例子中，Jiang等人使用了一个深度强化学习（Deep RL）代理在应用多种MOT算法后进行边界框回归。这个过程实际上完全独立于所使用的跟踪算法，可以在事后使用以提高模型的准确性。使用了VGG-16
    CNN来提取边界框所包围区域的外观特征，然后将这些特征与表示代理最近10次动作历史的向量进行拼接。最后，使用一个由3个全连接层组成的Q网络[[160](#bib.bib160)]来预测13种可能动作之一，包括边界框的运动和缩放，以及一个终止动作，以信号回归的完成。将这种边界框回归技术应用于各种最先进的MOT算法上，使得MOT15数据集的绝对MOTA得分提高了2到7点，在公共检测方法中达到了最高分。作者还展示了他们的回归方法比使用传统方法，如Faster
    R-CNN模型计算的边界框回归，具有更好的结果。
- en: Lee et al. [[153](#bib.bib153)] proposed a multi-class multi-object tracker
    that used an ensemble of detectors, including CNN models like VGG-16 and ResNet,
    to compute the likelihood of each target being at a certain location in the next
    frame. A Markov Chain Monte Carlo sampling from a distribution that was influenced
    by said likelihoods was used to predict the next position for each target, and
    together with an estimation of track birth and death probabilities, short track
    segments were built. Finally, a changing point detection [[161](#bib.bib161)]
    algorithm was employed to detect abrupt changes in stationary time series representing
    track segments; this was done in order to detect track drift, to remove unstable
    track segments and to combine the segments together. The algorithm reached results
    comparable to state-of-the-art MOT methods using private detections.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Lee等人[[153](#bib.bib153)] 提出了一个多类多目标跟踪器，使用了包括VGG-16和ResNet等CNN模型在内的检测器集成，以计算每个目标在下一个帧中的位置可能性。使用了从受上述可能性影响的分布中进行的马尔可夫链蒙特卡洛采样来预测每个目标的下一个位置，并结合轨迹出生和死亡概率的估计，构建了短轨迹段。最后，使用了一个变化点检测[[161](#bib.bib161)]
    算法来检测表示轨迹段的静态时间序列中的突然变化；这是为了检测轨迹漂移，去除不稳定的轨迹段并将这些段合并。该算法在使用私人检测方法时达到了与最先进的MOT方法相当的结果。
- en: Hoak et al. [[154](#bib.bib154)] proposed a 5-layer custom CNN network, trained
    on the Caltech pedestrian detection dataset [[162](#bib.bib162)], to compute the
    likelihood of a target being at a certain location in the image. They used a multi-Bernoulli
    filter (implemented using the particle filter algorithm presented in [[163](#bib.bib163)]),
    and a novel Interactive Likelihood (ILH) was computed for each particle, in order
    to weigh them based on their distance from particles belonging to other targets;
    this was done to prevent the algorithm from sampling from areas that belong to
    different objects. The algorithm obtained good results on the VSPETS 2003 INMOVE
    soccer dataset^(17)^(17)17[ftp://ftp.cs.rdg.ac.uk/pub/VS-PETS/](ftp://ftp.cs.rdg.ac.uk/pub/VS-PETS/)
    and the AFL dataset [[164](#bib.bib164)].
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Hoak等人[[154](#bib.bib154)] 提出了一个5层自定义CNN网络，该网络在Caltech行人检测数据集[[162](#bib.bib162)]上进行训练，以计算目标在图像中特定位置的可能性。他们使用了一个多伯努利滤波器（使用[[163](#bib.bib163)]中提出的粒子滤波器算法实现），并为每个粒子计算了一个新颖的交互式可能性（ILH），以根据它们与其他目标粒子的距离加权；这是为了防止算法从属于不同物体的区域进行采样。该算法在VSPETS
    2003 INMOVE足球数据集^(17)^(17)17[ftp://ftp.cs.rdg.ac.uk/pub/VS-PETS/](ftp://ftp.cs.rdg.ac.uk/pub/VS-PETS/)和AFL数据集[[164](#bib.bib164)]上取得了良好的结果。
- en: 'Henschel et al. [[155](#bib.bib155)] used head detections, extracted with a
    CNN [[156](#bib.bib156)], in addition to the usual body detections to perform
    pedestrian tracking. The presence/absence of a head and its position relative
    to the bounding box can help determine if a bounding box is a true or a false
    positive. The association problem was modelled as a correlation clustering problem
    on graphs, that the authors solved with a modified version of the Frank-Wolfe
    algorithm [[165](#bib.bib165)]; the association costs were computed as a combination
    of spatial and temporal costs: the spatial costs were the distance and the angle
    between the detected and the predicted head positions; the temporal costs were
    computed using the correspondences between pixels between the two frames, obtained
    using DeepMatching [[79](#bib.bib79)]. The algorithm reached top MOTA score on
    MOT17 and second-best score on MOT16 at the time of publication.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Henschel等人[[155](#bib.bib155)]除了使用常规的身体检测，还使用了通过CNN [[156](#bib.bib156)]提取的头部检测来进行行人跟踪。头部的有无及其相对于边界框的位置可以帮助判断边界框是否为真实或虚假的正样本。关联问题被建模为图上的相关性聚类问题，作者通过修改版的Frank-Wolfe算法[[165](#bib.bib165)]解决了这一问题；关联成本被计算为空间和时间成本的组合：空间成本是检测到的和预测的头部位置之间的距离和角度；时间成本则使用通过DeepMatching
    [[79](#bib.bib79)]获得的两帧之间的像素对应关系进行计算。该算法在MOT17上取得了最高的MOTA评分，在MOT16上取得了第二好的评分。
- en: Gan et al. [[157](#bib.bib157)] employed a modified MDNet [[151](#bib.bib151)]
    in their online pedestrian tracking framework. Besides 3 shared convolutional
    layers, common to all the targets, each target also had 3 specific FC layers,
    that were updated online to capture the appearance change of the target. A set
    of box candidates, including detections intersecting the last bounding box of
    the target and a set of boxes sampled from a Gaussian distribution with parameters
    estimated using a linear motion model, were given as input to the network, that
    output a confidence score for each of them. The candidate with the highest score
    was considered the optimal estimated target location. To reduce the number of
    ID switch errors, the algorithms tried to find the past trajectory that was most
    similar to the estimated box, using another affinity measure between the pairs;
    such affinity was computed using appearance and motion cues, together with the
    tracklet confidence score and a collision factor. Detections were also used to
    initialize new tracklets and to fix the motion prediction errors when occlusions
    happened.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Gan等人[[157](#bib.bib157)]在他们的在线行人跟踪框架中采用了修改版的MDNet [[151](#bib.bib151)]。除了所有目标共享的3层卷积层外，每个目标还具有3层特定的全连接层，这些层在线更新以捕捉目标外观的变化。一组框候选，包括与目标的最后边界框相交的检测结果以及从高斯分布中抽样的框（使用线性运动模型估计的参数），作为输入传递给网络，网络对每个候选框输出置信度评分。置信度最高的候选框被认为是最优的目标位置。为了减少ID切换错误，算法尝试找到与估计框最相似的过去轨迹，使用另一种亲和性度量来计算这种亲和性，包括外观和运动线索、轨迹置信度评分和碰撞因素。检测结果也用于初始化新的轨迹片段，并在发生遮挡时修正运动预测错误。
- en: 'Xiang et al. [[158](#bib.bib158)] used a MetricNet to track pedestrians. The
    model unified an affinity model with trajectory estimation, done with a Bayesian
    filter. An appearance model, made of a VGG-16 CNN trained for person re-identification
    on various datasets, extracted features and performed bounding box regression;
    the motion model instead consisted of two parts: an LSTM-based feature extractor,
    that took as input the trajectory’s past coordinates, and a so-called BF-Net on
    top, made of various FC layers, that combined the features extracted by the LSTM
    and a detection box (chosen by the Hungarian algorithm) to perform the Bayesian
    filtering step and output the new position of the target. The MetricNet was trained
    using a triplet loss, similar to other models presented in the previous sections.
    The algorithm obtained the best and second-best results among online methods on
    MOT16 and MOT15, respectively.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Xiang等人[[158](#bib.bib158)]使用MetricNet进行行人跟踪。该模型将亲和性模型与通过贝叶斯滤波进行的轨迹估计统一在一起。一个由VGG-16
    CNN构成的外观模型在各种数据集上进行人物重识别训练，提取特征并进行边界框回归；而运动模型则由两部分组成：一个基于LSTM的特征提取器，输入为轨迹的过去坐标，以及一个称为BF-Net的网络，由各种全连接层组成，结合了LSTM提取的特征和通过匈牙利算法选择的检测框，以执行贝叶斯滤波步骤并输出目标的新位置。MetricNet使用类似于前面章节中其他模型的三元组损失进行训练。该算法在MOT16和MOT15的在线方法中分别获得了最佳和第二好的结果。
- en: 'Finally, Chu et al. [[159](#bib.bib159)] used three different CNNs in their
    algorithm. The first one, called PafNet [[166](#bib.bib166)], was used to distinguish
    the background from the tracked objects. The second one, called PartNet [[167](#bib.bib167)],
    was employed to distinguish among the different targets. The third CNN, made of
    one convolutional layer and one FC layer, was instead used to decide whether to
    refresh the tracking model or not. The overall algorithm worked as follows: for
    every tracked target in the past frame, two score maps were computed in the current
    one, using PafNet and PartNet. Then, using the Kernel Correlation Filter tracker
    [[168](#bib.bib168)], a new position for the object was predicted. Moreover, after
    a certain number of frames, a so-called detection verification step was performed:
    the detections output by a detector (in their experiments, they chose to use the
    public detections provided with the dataset) were assigned to the tracked targets
    by solving a graph multicut problem. Targets that were not associated to a detection
    for a certain number of frames were terminated. Then, the third CNN was employed
    to check if the associated detection box was better than the predicted one. If
    so, the KCF model parameters were updated to reflect the change in the object
    characteristics. Such CNN used the maps extracted by PafNet, and was trained using
    reinforcement learning. Unassociated detections were then employed to recover
    from target occlusion, using a SVM classifier and the Hungarian algorithm. Finally,
    the remaining unassociated detections were used to initialize new targets. The
    algorithm was evaluated both on MOT15 and MOT16 datasets, reaching top performance
    overall on the first one, and top performance among online methods on the second.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Chu等人[[159](#bib.bib159)]在其算法中使用了三种不同的CNN。第一个叫做PafNet [[166](#bib.bib166)]，用于区分背景与跟踪对象。第二个叫做PartNet
    [[167](#bib.bib167)]，用于区分不同的目标。第三个CNN，由一个卷积层和一个全连接层组成，用于决定是否刷新跟踪模型。整体算法的工作流程如下：对每个过去帧中的跟踪目标，在当前帧中使用PafNet和PartNet计算两个得分图。然后，使用Kernel
    Correlation Filter跟踪器[[168](#bib.bib168)]预测对象的新位置。此外，在一定数量的帧之后，执行所谓的检测验证步骤：由检测器输出的检测（在他们的实验中，他们选择使用数据集提供的公共检测）通过解决图的多割问题分配给跟踪目标。那些在一定数量的帧内没有与检测关联的目标将被终止。然后，第三个CNN用于检查关联的检测框是否比预测的更好。如果是这样，KCF模型参数将更新以反映对象特征的变化。该CNN使用PafNet提取的图，并通过强化学习进行训练。未关联的检测随后用于恢复目标遮挡，使用SVM分类器和匈牙利算法。最后，剩余的未关联检测用于初始化新目标。该算法在MOT15和MOT16数据集上进行了评估，在第一个数据集上总体表现最佳，在第二个数据集上的在线方法中表现最佳。
- en: 4 Analysis and comparisons
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 分析与比较
- en: This section presents a comparison between all the works that have tested their
    algorithm on one of the MOTChallenge datasets. We will only focus on the MOTChallenge
    datasets since for other datasets there aren’t enough relevant papers using deep
    learning to perform a meaningful analysis.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 本节呈现了所有在MOTChallenge数据集上测试其算法的工作的比较。我们将仅关注MOTChallenge数据集，因为对于其他数据集，尚无足够相关的论文使用深度学习进行有意义的分析。
- en: 'We first describe the setup of the experimental analysis, including the considered
    metrics and the organization of the tables in section [4.1](#S4.SS1 "4.1 Setup
    and organization ‣ 4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey"). Section [4.2](#S4.SS2 "4.2 Discussion of the results ‣ 4
    Analysis and comparisons ‣ Deep Learning in Video Multi-Object Tracking: A Survey")
    will then present the actual results and considerations derived from the analysis.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先描述实验分析的设置，包括所考虑的指标和第[4.1](#S4.SS1 "4.1 Setup and organization ‣ 4 Analysis
    and comparisons ‣ Deep Learning in Video Multi-Object Tracking: A Survey")节中表格的组织。然后第[4.2](#S4.SS2
    "4.2 Discussion of the results ‣ 4 Analysis and comparisons ‣ Deep Learning in
    Video Multi-Object Tracking: A Survey")节将呈现实际结果和从分析中得出的考虑。'
- en: 4.1 Setup and organization
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置和组织
- en: For a fair comparison, we only show results reported on the whole test sets.
    Some of the discussed papers report their results using subsets of the test set,
    or validation datasets extracted from the training splits of the MOTChallenge
    datasets. These results are discarded as they are not comparable with the others.
    Moreover, the reported results are divided into algorithms that use public detections
    and algorithms that use private detections, since the different quality of the
    detections has a big impact on performance. The results are further split into
    online and batch methods, since the online methods are at a disadvantage, being
    only able to access present and past information to assign IDs in each frame.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行公平比较，我们仅展示在整个测试集上报告的结果。一些讨论中的论文使用测试集的子集或从MOTChallenge数据集中提取的训练分割验证数据集来报告结果。这些结果被丢弃，因为它们与其他结果不可比较。此外，报告的结果分为使用公共检测的算法和使用私人检测的算法，因为检测的不同质量对性能有很大影响。结果进一步分为在线方法和批处理方法，因为在线方法处于劣势，只能访问当前和过去的信息来为每一帧分配ID。
- en: For each algorithm we indicate the year of the referenced published paper, their
    mode of operation (batch vs. online); the MOTA, MOTP, IDF1, Mostly Tracked (MT)
    and Mostly Lost (ML) metrics, expressed in percentages; the absolute number of
    false positives (FP), false negatives (FN), ID switches (IDS) and fragmentations
    (Frag); the speed of the algorithm expressed in frames per second (Hz). For each
    metric, an arrow pointing up ($\uparrow$) indicates that a higher score is better,
    while an arrow pointing down ($\downarrow$) indicates the opposite. The metrics
    shown here are the same that can be found on the public leaderboards on the MOTChallenge
    website. The numerical results presented in the referenced works have been integrated
    with data from the MOTChallenge leaderboards.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个算法，我们标明了参考文献发布论文的年份、其操作模式（批处理 vs. 在线）；MOTA、MOTP、IDF1、主要跟踪（MT）和主要丢失（ML）指标，以百分比表示；假阳性（FP）、假阴性（FN）、ID切换（IDS）和片段化（Frag）的绝对数量；算法的速度，以每秒帧数（Hz）表示。对于每个指标，向上箭头（$\uparrow$）表示更高的分数更好，而向下箭头（$\downarrow$）表示相反。这里显示的指标与MOTChallenge网站上的公共排行榜上可以找到的指标相同。参考文献中展示的数值结果已与MOTChallenge排行榜的数据整合。
- en: 'Attending to the classification presented before, a table for each of the combinations
    dataset/detection source is shown. Tables [2](#S4.T2 "Table 2 ‣ 4.1 Setup and
    organization ‣ 4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") and [3](#S4.T3 "Table 3 ‣ 4.1 Setup and organization ‣ 4
    Analysis and comparisons ‣ Deep Learning in Video Multi-Object Tracking: A Survey")
    show results on MOT15 using public and private detections respectively; tables
    [4](#S4.T4 "Table 4 ‣ 4.1 Setup and organization ‣ 4 Analysis and comparisons
    ‣ Deep Learning in Video Multi-Object Tracking: A Survey") and [5](#S4.T5 "Table
    5 ‣ 4.1 Setup and organization ‣ 4 Analysis and comparisons ‣ Deep Learning in
    Video Multi-Object Tracking: A Survey") do the same on MOT16; finally, table [6](#S4.T6
    "Table 6 ‣ 4.1 Setup and organization ‣ 4 Analysis and comparisons ‣ Deep Learning
    in Video Multi-Object Tracking: A Survey") shows results on MOT17, who currently
    only has published algorithms that use public detections. Each table groups online
    and batch methods separately, and for each group the papers are sorted by year,
    and then by ascending MOTA score if the papers are from the same year, since it
    is the main metric considered in the MOTChallenge datasets^(18)^(18)18If not differently
    specified, when we use in this section expressions such as ”best performing” or
    similar, we are always referring to a higher MOTA score, since it’s the main evaluation
    metric used in the MOTChallenge benchmark.. If a work presents multiple results
    on the same dataset, using the same set of detections and the same mode of operation,
    we only show the result with the highest MOTA. The best performance for each metric
    is highlighted in bold, while the best performance among papers operating in the
    same mode (batch/online) is underlined. It is important to note though that comparisons
    on the Hz metric may not be reliable since the performance is usually reported
    only for the tracking part of the algorithms, without the detection step and sometimes
    without including the runtime of deep learning models, that are usually the most
    computational intensive part of the algorithms presented in this survey; moreover,
    the algorithms were run on widely different hardware.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前提出的分类，展示了每个组合数据集/检测源的表格。表格[2](#S4.T2 "表 2 ‣ 4.1 设置与组织 ‣ 4 分析与比较 ‣ 视频中的深度学习多目标跟踪：综述")和[3](#S4.T3
    "表 3 ‣ 4.1 设置与组织 ‣ 4 分析与比较 ‣ 视频中的深度学习多目标跟踪：综述")分别展示了使用公共和私有检测的MOT15结果；表格[4](#S4.T4
    "表 4 ‣ 4.1 设置与组织 ‣ 4 分析与比较 ‣ 视频中的深度学习多目标跟踪：综述")和[5](#S4.T5 "表 5 ‣ 4.1 设置与组织 ‣
    4 分析与比较 ‣ 视频中的深度学习多目标跟踪：综述")展示了MOT16的相同结果；最后，表格[6](#S4.T6 "表 6 ‣ 4.1 设置与组织 ‣ 4
    分析与比较 ‣ 视频中的深度学习多目标跟踪：综述")展示了MOT17的结果，该数据集目前仅有使用公共检测的已发布算法。每个表格分别对在线和批处理方法进行分组，并且每组中的论文按年份排序，如果论文来自同一年，则按升序MOTA分数排序，因为这是MOTChallenge数据集中主要考虑的指标^(18)^(18)18如果没有其他特别说明，当我们在本节中使用“最佳性能”等表达时，我们始终指的是更高的MOTA分数，因为这是MOTChallenge基准测试中使用的主要评估指标..
    如果一个工作在同一数据集上呈现了多个结果，使用相同的检测集和操作模式，我们只展示MOTA最高的结果。每个指标的最佳性能用粗体突出显示，而在相同模式（批处理/在线）下的论文中的最佳性能则用下划线标出。然而，值得注意的是，Hz指标的比较可能不可靠，因为性能通常仅报告算法的跟踪部分，而不包括检测步骤，有时也不包括深度学习模型的运行时间，而深度学习模型通常是本文综述中呈现的算法中计算最密集的部分；此外，算法运行在差异较大的硬件上。
- en: '|  | Year | Mode | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT
    $\uparrow$ | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$
    | Frag $\downarrow$ | Hz $\uparrow$ |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 模式 | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT $\uparrow$
    | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$ | Frag
    $\downarrow$ | Hz $\uparrow$ |'
- en: '| [[83](#bib.bib83)] | 2015 | Online | $32.4$ | $71.8$ | $45.3$ | $16.0$ |
    $43.8$ | $9064$ | $32\,060$ | $435$ | $826$826 | $0.7$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| [[83](#bib.bib83)] | 2015 | 在线 | $32.4$ | $71.8$ | $45.3$ | $16.0$ | $43.8$
    | $9064$ | $32\,060$ | $435$ | $826$826 | $0.7$ |'
- en: '| [[133](#bib.bib133)] | 2017 | $19.0$ | $71.0$ | $17.1$ | $5.5$ | $45.6$ |
    $11\,578$ | $36\,706$ | $1490$ | $2081$ | $165.2$ |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | 2017 | $19.0$ | $71.0$ | $17.1$ | $5.5$ | $45.6$ |
    $11\,578$ | $36\,706$ | $1490$ | $2081$ | $165.2$ |'
- en: '| [[128](#bib.bib128)] | 2017 | $31.6$ | $71.8$ |  | $10.1$ | $46.3$ |  |  |
    $491$ | $994$ |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)] | 2017 | $31.6$ | $71.8$ |  | $10.1$ | $46.3$ |  |  |
    $491$ | $994$ |  |'
- en: '| [[94](#bib.bib94)] | 2017 | $32.8$ | $70.7$ | $38.8$ | $9.7$ | $42.2$ | $4983$
    | $35\,690$ | $614$ | $1583$ | $2.3$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | 2017 | $32.8$ | $70.7$ | $38.8$ | $9.7$ | $42.2$ | $4983$
    | $35\,690$ | $614$ | $1583$ | $2.3$ |'
- en: '| [[124](#bib.bib124)] | 2017 | $34.3$ | $70.5$ | $48.3$ | $11.4$ | $43.4$
    | $5154$ | $34\,848$ | $348$ | $1463$ | $0.5$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| [[124](#bib.bib124)] | 2017 | $34.3$ | $70.5$ | $48.3$ | $11.4$ | $43.4$
    | $5154$ | $34\,848$ | $348$ | $1463$ | $0.5$ |'
- en: '| [[89](#bib.bib89)] | 2017 | $35.0$ | $72.6$72.6 | $47.7$ | $11.4$ | $42.2$
    | $8455$ | $31\,140$ | $358$ | $1267$ | $4.6$ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] | 2017 | $35.0$ | $72.6$ | $47.7$ | $11.4$ | $42.2$ |
    $8455$ | $31\,140$ | $358$ | $1267$ | $4.6$ |'
- en: '| [[123](#bib.bib123)] | 2017 | $37.6$ | $71.7$ | $46.0$ | $15.8$ | $26.8$
    | $7933$ | $29\,397$ | $1026$ | $2024$ | $1.0$ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [[123](#bib.bib123)] | 2017 | $37.6$ | $71.7$ | $46.0$ | $15.8$ | $26.8$
    | $7933$ | $29\,397$ | $1026$ | $2024$ | $1.0$ |'
- en: '| [[145](#bib.bib145)] | 2017 | $38.5$ | $72.6$72.6 | $47.1$ | $8.7$ | $37.4$
    | $4005$ | $33\,204$ | $586$ | $1263$ | $6.7$ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | 2017 | $38.5$ | $72.6$ | $47.1$ | $8.7$ | $37.4$ |
    $4005$ | $33\,204$ | $586$ | $1263$ | $6.7$ |'
- en: '| [[117](#bib.bib117)] | 2018 | $33.6$ | $70.9$ | $39.1$ | $10.4$ | $37.6$
    | $5917$ | $34\,002$ | $866$ | $1566$ | $0.1$ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | 2018 | $33.6$ | $70.9$ | $39.1$ | $10.4$ | $37.6$
    | $5917$ | $34\,002$ | $866$ | $1566$ | $0.1$ |'
- en: '| [[96](#bib.bib96)] | 2018 | $35.1$ | $70.9$ | $45.4$ | $13.0$ | $42.3$ |
    $6771$ | $32\,717$ | $381$ | $1523$ | $5.4$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 2018 | $35.1$ | $70.9$ | $45.4$ | $13.0$ | $42.3$ |
    $6771$ | $32\,717$ | $381$ | $1523$ | $5.4$ |'
- en: '| [[150](#bib.bib150)] | 2018 | $37.1$ | $71.0$ |  | $14.0$ | $31.3$ | $7036$
    | $30\,440$ |  |  |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | 2018 | $37.1$ | $71.0$ |  | $14.0$ | $31.3$ | $7036$
    | $30\,440$ |  |  |  |'
- en: '| [[152](#bib.bib152)] | 2018 | $42.3$ |  | $47.7$ | $13.6$ | $39.7$ |  |  |  |  |
    $3.1$ |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| [[152](#bib.bib152)] | 2018 | $42.3$ |  | $47.7$ | $13.6$ | $39.7$ |  |  |  |  |
    $3.1$ |'
- en: '| [[138](#bib.bib138)] | 2019 | $22.5$ | $70.9$ | $25.9$ | $6.4$ | $61.9$ |
    $7346$ | $39\,092$ | $1159$ | $1538$ | $172.8$ |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | 2019 | $22.5$ | $70.9$ | $25.9$ | $6.4$ | $61.9$ |
    $7346$ | $39\,092$ | $1159$ | $1538$ | $172.8$ |'
- en: '| [[158](#bib.bib158)] | 2019 | $37.1$ | $72.5$ | $48.4$ | $12.6$ | $39.7$
    | $8305$ | $29\,732$ | $580$ | $1193$ | $1.0$ |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | 2019 | $37.1$ | $72.5$ | $48.4$ | $12.6$ | $39.7$
    | $8305$ | $29\,732$ | $580$ | $1193$ | $1.0$ |'
- en: '| [[159](#bib.bib159)] | 2019 | $38.9$ | $70.6$ | $44.5$ | $16.6$ | $31.5$
    | $7321$ | $29\,501$ | $720$ | $1440$ | $0.3$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| [[159](#bib.bib159)] | 2019 | $38.9$ | $70.6$ | $44.5$ | $16.6$ | $31.5$
    | $7321$ | $29\,501$ | $720$ | $1440$ | $0.3$ |'
- en: '| [[110](#bib.bib110)] | 2016 | Batch | $29.0$ | $71.2$ | $34.3$ | $8.5$ |
    $48.4$ | $5160$5160 | $37\,798$ | $639$ | $1316$ | $52.8$52.8 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | 2016 | 批次 | $29.0$ | $71.2$ | $34.3$ | $8.5$ | $48.4$
    | $5160$ | $37\,798$ | $639$ | $1316$ | $52.8$ |'
- en: '| [[108](#bib.bib108)] | 2016 | $29.6$ | $71.8$ | $36.8$ | $11.2$ | $44.0$
    | $7786$ | $34\,733$ | $712$ | $943$ | $1.7$ |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bib108)] | 2016 | $29.6$ | $71.8$ | $36.8$ | $11.2$ | $44.0$
    | $7786$ | $34\,733$ | $712$ | $943$ | $1.7$ |'
- en: '| [[112](#bib.bib112)] | 2017 | $33.8$33.8 | $73.4$ | $40.4$40.4 | $12.9$12.9
    | $36.9$36.9 | $7898$ | $32\,061$32061 | $703$ | $1430$ | $3.7$ |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | 2017 | $33.8$ | $73.4$ | $40.4$ | $12.9$ | $36.9$
    | $7898$ | $32\,061$ | $703$ | $1430$ | $3.7$ |'
- en: '| [[113](#bib.bib113)] | 2018 | $22.2$ | $71.1$ | $27.2$ | $3.1$ | $61.6$ |
    $5591$ | $41\,531$ | $700$ | $1240$ | $8.9$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | 2018 | $22.2$ | $71.1$ | $27.2$ | $3.1$ | $61.6$ |
    $5591$ | $41\,531$ | $700$ | $1240$ | $8.9$ |'
- en: '| [[104](#bib.bib104)] | 2019 | $28.1$ | $74.3$ | $38.7$ |  |  | $6733$ | $36\,952$
    | $477$477 | $790$ | $16.9$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| [[104](#bib.bib104)] | 2019 | $28.1$ | $74.3$ | $38.7$ |  |  | $6733$ | $36\,952$
    | $477$ | $790$ | $16.9$ |'
- en: 'Table 2: Experimental results of MOT algorithms using deep learning and public
    detections on MOT15 dataset.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：使用深度学习和公共检测在MOT15数据集上的MOT算法实验结果。
- en: '|  | Year | Mode | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT
    $\uparrow$ | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$
    | Frag $\downarrow$ | Hz $\uparrow$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | 年份 | 模式 | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT $\uparrow$
    | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$ | Frag
    $\downarrow$ | Hz $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| [[35](#bib.bib35)] | 2016 | Online | $33.4$ | $72.1$ | $40.4$ | $11.7$ |
    $30.9$ | $7318$ | $32\,615$ | $1001$ | $1764$ | $260.0$ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bib35)] | 2016 | 在线 | $33.4$ | $72.1$ | $40.4$ | $11.7$ | $30.9$
    | $7318$ | $32\,615$ | $1001$ | $1764$ | $260.0$ |'
- en: '| [[76](#bib.bib76)] | 2017 | $32.1$ | $70.9$ |  | $13.2$ | $30.1$ | $6551$
    | $33\,473$ | $1687$ | $2471$ |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| [[76](#bib.bib76)] | 2017 | $32.1$ | $70.9$ |  | $13.2$ | $30.1$ | $6551$
    | $33\,473$ | $1687$ | $2471$ |  |'
- en: '| [[94](#bib.bib94)] | 2017 | $51.3$ | $74.2$ | $54.1$ | $36.3$ | $22.2$ |
    $7110$ | $22\,271$ | $544$ | $1335$ | $1.3$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | 2017 | $51.3$ | $74.2$ | $54.1$ | $36.3$ | $22.2$ |
    $7110$ | $22\,271$ | $544$ | $1335$ | $1.3$ |'
- en: '| [[145](#bib.bib145)] | 2017 | $53.0$ | $75.5$ | $52.2$ | $29.1$ | $20.2$
    | $5159$ | $22\,984$ | $708$ | $1476$ | $6.7$ |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | 2017 | $53.0$ | $75.5$ | $52.2$ | $29.1$ | $20.2$
    | $5159$ | $22\,984$ | $708$ | $1476$ | $6.7$ |'
- en: '| [[61](#bib.bib61)] | 2018 | $32.7$ |  | $38.9$ | $26.2$ | $19.6$ |  |  |  |  |
    $11.1$ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | 2018 | $32.7$ |  | $38.9$ | $26.2$ | $19.6$ |  |  |  |  |
    $11.1$ |'
- en: '| [[96](#bib.bib96)] | 2018 | $56.5$ | $73.0$ | $61.3$ | $45.1$ | $14.6$ |
    $9386$ | $16\,921$ | $428$ | $1364$ | $5.1$ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 2018 | $56.5$ | $73.0$ | $61.3$ | $45.1$ | $14.6$ |
    $9386$ | $16\,921$ | $428$ | $1364$ | $5.1$ |'
- en: 'Table 3: Experimental results of MOT algorithms using deep learning and private
    detections on MOT15 dataset.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：使用深度学习和私有检测在 MOT15 数据集上测试的 MOT 算法实验结果。
- en: '|  | Year | Mode | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT
    $\uparrow$ | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$
    | Frag $\downarrow$ | Hz $\uparrow$ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | 年 | 模式 | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT $\uparrow$
    | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$ | Frag
    $\downarrow$ | Hz $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| [[106](#bib.bib106)] | 2016 | Online | $35.3$ | $75.2$ |  | $7.4$ | $51.1$
    | $5592$ | $110\,778$ | $1598$ | $5153$ | $7.9$ |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| [[106](#bib.bib106)] | 2016 | 在线 | $35.3$ | $75.2$ |  | $7.4$ | $51.1$ |
    $5592$ | $110\,778$ | $1598$ | $5153$ | $7.9$ |'
- en: '| [[147](#bib.bib147)] | 2016 | $38.8$ | $75.1$ |  | $7.9$ | $49.1$ | $8114$
    | $102\,452$ | $965$ | $1657$ | $11.8$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| [[147](#bib.bib147)] | 2016 | $38.8$ | $75.1$ |  | $7.9$ | $49.1$ | $8114$
    | $102\,452$ | $965$ | $1657$ | $11.8$ |'
- en: '| [[94](#bib.bib94)] | 2017 | $43.9$ | $74.7$ | $45.1$ | $10.7$ | $44.4$ |
    $6450$ | $95\,175$ | $676$ | $1795$ | $0.5$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | 2017 | $43.9$ | $74.7$ | $45.1$ | $10.7$ | $44.4$ |
    $6450$ | $95\,175$ | $676$ | $1795$ | $0.5$ |'
- en: '| [[124](#bib.bib124)] | 2017 | $46.0$ | $74.9$ | $50.0$ | $14.6$ | $43.6$
    | $6895$ | $91\,117$ | $473$473 | $1422$ | $0.2$ |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| [[124](#bib.bib124)] | 2017 | $46.0$ | $74.9$ | $50.0$ | $14.6$ | $43.6$
    | $6895$ | $91\,117$ | $473$473 | $1422$ | $0.2$ |'
- en: '| [[123](#bib.bib123)] | 2017 | $47.2$ | $75.8$ | $46.3$ | $14.0$ | $41.6$
    | $2681$ | $92\,856$ | $774$ | $1675$ | $1.0$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| [[123](#bib.bib123)] | 2017 | $47.2$ | $75.8$ | $46.3$ | $14.0$ | $41.6$
    | $2681$ | $92\,856$ | $774$ | $1675$ | $1.0$ |'
- en: '| [[157](#bib.bib157)] | 2018 | $44.2$ | $78.3$78.3 |  | $15.2$ | $45.7$ |
    $7912$ | $93\,215$ | $560$ | $1212$ |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bib157)] | 2018 | $44.2$ | $78.3$78.3 |  | $15.2$ | $45.7$ |
    $7912$ | $93\,215$ | $560$ | $1212$ |  |'
- en: '| [[117](#bib.bib117)] | 2018 | $44.8$ | $75.6$ | $39.7$ | $14.1$ | $42.3$
    | $5613$ | $94\,125$ | $968$ | $1378$ | $0.1$ |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | 2018 | $44.8$ | $75.6$ | $39.7$ | $14.1$ | $42.3$
    | $5613$ | $94\,125$ | $968$ | $1378$ | $0.1$ |'
- en: '| [[96](#bib.bib96)] | 2018 | $45.9$ | $74.8$ | $48.8$ | $13.2$ | $41.9$ |
    $6871$ | $91\,173$ | $648$ | $1992$ | $0.9$ |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 2018 | $45.9$ | $74.8$ | $48.8$ | $13.2$ | $41.9$ |
    $6871$ | $91\,173$ | $648$ | $1992$ | $0.9$ |'
- en: '| [[115](#bib.bib115)] | 2018 | $46.1$ | $73.8$ | $54.8$ | $17.4$17.4 | $42.7$
    | $7909$ | $89\,874$ | $532$ | $1616$ | $0.3$ |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | 2018 | $46.1$ | $73.8$ | $54.8$ | $17.4$17.4 | $42.7$
    | $7909$ | $89\,874$ | $532$ | $1616$ | $0.3$ |'
- en: '| [[150](#bib.bib150)] | 2018 | $47.3$ | $74.6$ |  | $17.4$17.4 | $39.9$ |
    $6375$ | $88\,543$ |  |  |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | 2018 | $47.3$ | $74.6$ |  | $17.4$17.4 | $39.9$ |
    $6375$ | $88\,543$ |  |  |  |'
- en: '| [[118](#bib.bib118)] | 2018 | $47.6$ | $74.8$ | $50.9$ | $15.2$ | $38.3$
    | $9253$ | $85\,431$85431 | $792$ | $1858$ | $20.6$ |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | 2018 | $47.6$ | $74.8$ | $50.9$ | $15.2$ | $38.3$
    | $9253$ | $85\,431$85431 | $792$ | $1858$ | $20.6$ |'
- en: '| [[158](#bib.bib158)] | 2019 | $48.3$ | $76.7$ | $50.9$ | $15.4$ | $40.1$
    | $2706$ | $91\,047$ | $543$ | $896$896 | $0.5$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | 2019 | $48.3$ | $76.7$ | $50.9$ | $15.4$ | $40.1$
    | $2706$ | $91\,047$ | $543$ | $896$896 | $0.5$ |'
- en: '| [[159](#bib.bib159)] | 2019 | $48.8$48.8 | $75.7$ | $47.2$ | $15.8$ | $38.1$38.1
    | $5875$ | $86\,567$ | $906$ | $1116$ | $0.1$ |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| [[159](#bib.bib159)] | 2019 | $48.8$48.8 | $75.7$ | $47.2$ | $15.8$ | $38.1$38.1
    | $5875$ | $86\,567$ | $906$ | $1116$ | $0.1$ |'
- en: '| [[112](#bib.bib112)] | 2017 | Batch | $44.1$ | $76.4$ | $38.3$ | $14.6$ |
    $44.9$ | $6388$ | $94\,775$ | $745$ | $1096$ | $1.8$ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | 2017 | 批处理 | $44.1$ | $76.4$ | $38.3$ | $14.6$ | $44.9$
    | $6388$ | $94\,775$ | $745$ | $1096$ | $1.8$ |'
- en: '| [[88](#bib.bib88)] | 2017 | $45.3$ | $75.9$ | $47.9$ | $17.0$ | $39.9$ |
    $11\,122$ | $87\,890$ | $639$ | $946$ | $1.8$ |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | 2017 | $45.3$ | $75.9$ | $47.9$ | $17.0$ | $39.9$ |
    $11\,122$ | $87\,890$ | $639$ | $946$ | $1.8$ |'
- en: '| [[51](#bib.bib51)] | 2017 | $48.8$ | $79.0$ |  | $18.2$ | $40.1$ | $6654$
    | $86\,245$ | $481$ | $595$ | $0.5$ |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | 2017 | $48.8$ | $79.0$ |  | $18.2$ | $40.1$ | $6654$
    | $86\,245$ | $481$ | $595$ | $0.5$ |'
- en: '| [[93](#bib.bib93)] | 2018 | $42.1$ |  | $47.8$ | $14.9$ | $44.4$ | $11\,637$
    | $93\,172$ | $753$ | $1156$ | $1.8$ |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | 2018 | $42.1$ |  | $47.8$ | $14.9$ | $44.4$ | $11\,637$
    | $93\,172$ | $753$ | $1156$ | $1.8$ |'
- en: '| [[132](#bib.bib132)] | 2018 | $46.9$ | $76.4$ | $46.8$ | $16.1$ | $43.2$
    | $6257$ | $91\,669$ | $549$ | $757$ |  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | 2018 | $46.9$ | $76.4$ | $46.8$ | $16.1$ | $43.2$
    | $6257$ | $91\,669$ | $549$ | $757$ |  |'
- en: '| [[103](#bib.bib103)] | 2018 | $47.2$ | $75.7$ | $52.4$52.4 | $18.6$ | $42.8$
    | $12\,586$ | $83\,107$ | $542$ | $787$ | $0.5$ |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | 2018 | $47.2$ | $75.7$ | $52.4$52.4 | $18.6$ | $42.8$
    | $12\,586$ | $83\,107$ | $542$ | $787$ | $0.5$ |'
- en: '| [[100](#bib.bib100)] | 2018 | $47.5$ |  | $43.6$ | $19.4$ | $36.9$ | $13\,002$
    | $81\,762$ | $1035$ | $1408$ | $0.8$ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| [[100](#bib.bib100)] | 2018 | $47.5$ |  | $43.6$ | $19.4$ | $36.9$ | $13\,002$
    | $81\,762$ | $1035$ | $1408$ | $0.8$ |'
- en: '| [[155](#bib.bib155)] | 2018 | $47.8$ | $75.5$ | $44.3$ | $19.1$ | $38.2$
    | $8886$ | $85\,487$ | $852$ | $1534$ | $0.6$ |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] | 2018 | $47.8$ | $75.5$ | $44.3$ | $19.1$ | $38.2$
    | $8886$ | $85\,487$ | $852$ | $1534$ | $0.6$ |'
- en: '| [[116](#bib.bib116)] | 2018 | $48.2$ | $77.5$ | $48.6$ | $12.9$ | $41.1$
    | $5104$5104 | $88\,586$ | $821$ | $1117$ | $2.8$2.8 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bib116)] | 2018 | $48.2$ | $77.5$ | $48.6$ | $12.9$ | $41.1$
    | $5104$ | $88\,586$ | $821$ | $1117$ | $2.8$ |'
- en: '| [[148](#bib.bib148)] | 2018 | $49.3$ | $79.0$ | $50.7$ | $17.8$ | $39.9$
    | $5333$ | $86\,795$ | $391$ | $535$ | $0.8$ |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] | 2018 | $49.3$ | $79.0$ | $50.7$ | $17.8$ | $39.9$
    | $5333$ | $86\,795$ | $391$ | $535$ | $0.8$ |'
- en: 'Table 4: Experimental results of MOT algorithms using deep learning and public
    detections on MOT16 dataset.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：使用深度学习和公共检测在MOT16数据集上的MOT算法实验结果。
- en: '|  | Year | Mode | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT
    $\uparrow$ | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$
    | Frag $\downarrow$ | Hz $\uparrow$ |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  | 年份 | 模式 | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT $\uparrow$
    | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$ | Frag
    $\downarrow$ | Hz $\uparrow$ |'
- en: '| [[38](#bib.bib38)] | 2016 | Online | $66.1$66.1 | $79.5$79.5 | $65.1$ | $34.0$
    | $20.8$ | $5061$5061 | $55\,914$ | $805$ | $3093$ | $9.9$ |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] | 2016 | 在线 | $66.1$ | $79.5$ | $65.1$ | $34.0$ | $20.8$
    | $5061$ | $55\,914$ | $805$ | $3093$ | $9.9$ |'
- en: '| [[41](#bib.bib41)] | 2017 | $61.4$ | $79.1$ | $62.2$ | $32.8$ | $18.2$ |
    $12\,852$ | $56\,668$ | $781$ | $2008$ | $17.4$ |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| [[41](#bib.bib41)] | 2017 | $61.4$ | $79.1$ | $62.2$ | $32.8$ | $18.2$ |
    $12\,852$ | $56\,668$ | $781$ | $2008$ | $17.4$ |'
- en: '| [[60](#bib.bib60)] | 2018 | $39.1$ |  |  | $11.1$ | $41.1$ | $9411$ | $99\,727$
    | $1906$ |  | $4.5$ |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] | 2018 | $39.1$ |  |  | $11.1$ | $41.1$ | $9411$ | $99\,727$
    | $1906$ |  | $4.5$ |'
- en: '| [[44](#bib.bib44)] | 2018 | $55.0$ | $76.7$ |  | $20.4$ | $24.5$ | $15\,766$
    | $65\,297$ | $1024$ | $1594$ | $16.9$ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| [[44](#bib.bib44)] | 2018 | $55.0$ | $76.7$ |  | $20.4$ | $24.5$ | $15\,766$
    | $65\,297$ | $1024$ | $1594$ | $16.9$ |'
- en: '| [[43](#bib.bib43)] | 2018 | $62.6$ | $78.3$ |  | $32.7$ | $21.1$ | $10\,604$
    | $56\,182$ | $1389$ | $1534$ |  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43)] | 2018 | $62.6$ | $78.3$ |  | $32.7$ | $21.1$ | $10\,604$
    | $56\,182$ | $1389$ | $1534$ |  |'
- en: '| [[96](#bib.bib96)] | 2018 | $63.0$ | $78.8$ | $63.8$ | $39.9$ | $22.1$ |
    $13\,663$ | $53\,248$ | $482$482 | $1251$ | $1.6$ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 2018 | $63.0$ | $78.8$ | $63.8$ | $39.9$ | $22.1$ |
    $13\,663$ | $53\,248$ | $482$ | $1251$ | $1.6$ |'
- en: '| [[55](#bib.bib55)] | 2018 | $64.8$ | $78.6$ | $73.5$ | $40.6$40.6 | $22.0$
    | $13\,470$ | $49\,927$49927 | $794$ | $1050$1050 | $39.4$ |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | 2018 | $64.8$ | $78.6$ | $73.5$ | $40.6$ | $22.0$ |
    $13\,470$ | $49\,927$ | $794$ | $1050$ | $39.4$ |'
- en: '| [[42](#bib.bib42)] | 2019 | $65.2$ | $78.4$ | $62.2$ | $32.4$ | $21.3$ |
    $6578$ | $55\,896$ | $946$ | $2283$ | $11.2$ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bib42)] | 2019 | $65.2$ | $78.4$ | $62.2$ | $32.4$ | $21.3$ |
    $6578$ | $55\,896$ | $946$ | $2283$ | $11.2$ |'
- en: '| [[153](#bib.bib153)] | 2016 | Batch | $62.4$ | $78.3$ | $51.6$ | $31.5$ |
    $24.2$ | $9855$ | $57\,257$ | $1394$ | $1318$ | $34.9$34.9 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| [[153](#bib.bib153)] | 2016 | 批处理 | $62.4$ | $78.3$ | $51.6$ | $31.5$ | $24.2$
    | $9855$ | $57\,257$ | $1394$ | $1318$ | $34.9$ |'
- en: '| [[38](#bib.bib38)] | 2016 | $68.2$ | $79.4$ | $60.0$ | $41.0$ | $19.0$19.0
    | $11\,479$ | $45\,605$ | $933$ | $1093$ | $0.7$ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] | 2016 | $68.2$ | $79.4$ | $60.0$ | $41.0$ | $19.0$ |
    $11\,479$ | $45\,605$ | $933$ | $1093$ | $0.7$ |'
- en: '| [[51](#bib.bib51)] | 2017 | $71.0$ | $80.2$ | $70.1$70.1 | $46.9$ | $21.9$
    | $7880$ | $44\,564$ | $434$ | $587$ | $0.5$ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | 2017 | $71.0$ | $80.2$ | $70.1$ | $46.9$ | $21.9$ |
    $7880$ | $44\,564$ | $434$ | $587$ | $0.5$ |'
- en: '| [[132](#bib.bib132)] | 2018 | $58.1$ | $77.2$ | $47.4$ | $23.1$ | $33.3$
    | $4883$ | $70\,207$ | $1624$ | $2539$ |  |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | 2018 | $58.1$ | $77.2$ | $47.4$ | $23.1$ | $33.3$
    | $4883$ | $70\,207$ | $1624$ | $2539$ |  |'
- en: 'Table 5: Experimental results of MOT algorithms using deep learning and private
    detections on MOT16 dataset.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用深度学习和私有检测在MOT16数据集上的MOT算法实验结果。
- en: '|  | Year | Mode | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT
    $\uparrow$ | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$
    | Frag $\downarrow$ | Hz $\uparrow$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | 年份 | 模式 | MOTA $\uparrow$ | MOTP $\uparrow$ | IDF1 $\uparrow$ | MT $\uparrow$
    | ML $\downarrow$ | FP $\downarrow$ | FN $\downarrow$ | IDS $\downarrow$ | Frag
    $\downarrow$ | Hz $\uparrow$ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    |'
- en: '| [[157](#bib.bib157)] | 2018 | Online | $44.9$ | $78.9$ |  | $13.8$ | $44.2$
    | $22\,085$ | $287\,267$ | $1537$1537 | $3295$3295 |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| [[157](#bib.bib157)] | 2018 | 在线 | $44.9$ | $78.9$ |  | $13.8$ | $44.2$ |
    $22\,085$ | $287\,267$ | $1537$ | $3295$ |  |'
- en: '| [[98](#bib.bib98)] | 2018 | $46.5$ | $77.2$ |  | $16.9$ | $37.2$ | $23\,859$
    | $272\,430$ | $5649$ | $9298$ | $1.6$ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| [[98](#bib.bib98)] | 2018 | $46.5$ | $77.2$ |  | $16.9$ | $37.2$ | $23\,859$
    | $272\,430$ | $5649$ | $9298$ | $1.6$ |'
- en: '| [[115](#bib.bib115)] | 2018 | $48.2$ | $75.7$ | $55.7$55.7 | $19.3$19.3 |
    $38.3$ | $26\,218$ | $263\,608$ | $2194$ | $5378$ | $0.3$ |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | 2018 | $48.2$ | $75.7$ | $55.7$ | $19.3$ | $38.3$
    | $26\,218$ | $263\,608$ | $2194$ | $5378$ | $0.3$ |'
- en: '| [[118](#bib.bib118)] | 2018 | $50.9$50.9 | $76.6$ | $52.7$ | $17.5$ | $35.7$35.7
    | $24\,069$ | $250\,768$250768 | $2474$ | $5317$ | $18.3$ |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | 2018 | $50.9$50.9 | $76.6$ | $52.7$ | $17.5$ | $35.7$35.7
    | $24\,069$ | $250\,768$250768 | $2474$ | $5317$ | $18.3$ |'
- en: '| [[119](#bib.bib119)] | 2019 | $44.9$ | $76.6$ | $48.4$ | $16.5$ | $35.8$
    | $33\,757$ | $269\,952$ | $7136$ | $14\,491$ | $10.1$ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119)] | 2019 | $44.9$ | $76.6$ | $48.4$ | $16.5$ | $35.8$
    | $33\,757$ | $269\,952$ | $7136$ | $14\,491$ | $10.1$ |'
- en: '| [[113](#bib.bib113)] | 2018 | Batch | $44.2$ | $76.4$ | $57.2$ | $16.1$ |
    $44.3$ | $29\,473$ | $283\,611$ | $1529$ | $2644$ | $4.8$4.8 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | 2018 | 批处理 | $44.2$ | $76.4$ | $57.2$ | $16.1$ | $44.3$
    | $29\,473$ | $283\,611$ | $1529$ | $2644$ | $4.8$4.8 |'
- en: '| [[93](#bib.bib93)] | 2018 | $47.5$ |  | $51.9$ | $18.2$ | $41.7$ | $25\,981$
    | $268\,042$ | $2069$ | $3124$ | $1.9$ |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | 2018 | $47.5$ |  | $51.9$ | $18.2$ | $41.7$ | $25\,981$
    | $268\,042$ | $2069$ | $3124$ | $1.9$ |'
- en: '| [[136](#bib.bib136)] | 2018 | $50.3$ |  | $47.9$ | $21.8$ | $36.2$ | $22\,204$22204
    | $249\,342$ | $3243$ | $3155$ | $1.9$ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | 2018 | $50.3$ |  | $47.9$ | $21.8$ | $36.2$ | $22\,204$22204
    | $249\,342$ | $3243$ | $3155$ | $1.9$ |'
- en: '| [[155](#bib.bib155)] | 2018 | $51.3$ | $77.0$77.0 | $47.6$ | $21.4$ | $35.2$
    | $24\,101$ | $247\,921$ | $2648$ | $4279$ | $0.2$ |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] | 2018 | $51.3$ | $77.0$77.0 | $47.6$ | $21.4$ | $35.2$
    | $24\,101$ | $247\,921$ | $2648$ | $4279$ | $0.2$ |'
- en: '| [[103](#bib.bib103)] | 2018 | $51.8$ | $77.0$77.0 | $54.7$ | $23.4$ | $37.9$
    | $33\,212$ | $236\,772$ | $1834$ | $2739$ | $0.7$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | 2018 | $51.8$ | $77.0$77.0 | $54.7$ | $23.4$ | $37.9$
    | $33\,212$ | $236\,772$ | $1834$ | $2739$ | $0.7$ |'
- en: 'Table 6: Experimental results of MOT algorithms using deep learning and public
    detections on MOT17 dataset.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在MOT17数据集上使用深度学习和公共检测的MOT算法的实验结果。
- en: 4.2 Discussion of the results
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果讨论
- en: General observations
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一般观察
- en: 'As expected, the best performing algorithms on each dataset use private detections,
    confirming the fact that the detection quality dominates the overall performance
    of the tracker: 56.5% MOTA vs. 42.3% for MOT15 and 71.0% vs. 49.3% for MOT16\.
    Moreover, on MOT16 and MOT17 the batch algorithms slightly outperform the online
    ones, even though the online methods are progressively getting closer to the performance
    of the batch ones. In fact, on MOT15 the best reported algorithm that uses deep
    learning runs in an online fashion. However, this can be an effect of a greater
    focus on developing online methods, which is a trend in the MOT deep learning
    research community. A common problem among online methods, that is not reflected
    in the MOTA score, is the higher number of fragmentations, as we can see in table
    [7](#S4.T7 "Table 7 ‣ General observations ‣ 4.2 Discussion of the results ‣ 4
    Analysis and comparisons ‣ Deep Learning in Video Multi-Object Tracking: A Survey").
    This happens because, when occlusions occur or detections are missing, online
    algorithms cannot look ahead in the video, re-identify the lost targets and interpolate
    the missing part of the trajectories [[124](#bib.bib124), [89](#bib.bib89), [94](#bib.bib94)].
    We can see in figure [8](#S4.F8 "Figure 8 ‣ General observations ‣ 4.2 Discussion
    of the results ‣ 4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") an example of trajectory that is fragmented by an online
    method, MOTDT [[118](#bib.bib118)], while it is correctly tracked by a batch method,
    eHAF16 [[103](#bib.bib103)].'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '正如预期的那样，每个数据集上表现最好的算法都使用了私有检测，这证实了检测质量主导了跟踪器的整体性能：MOT15上的56.5% MOTA对比42.3%，MOT16上的71.0%对比49.3%。此外，在MOT16和MOT17上，批处理算法略微优于在线算法，尽管在线方法逐渐接近批处理方法的性能。实际上，在MOT15上，使用深度学习的最佳算法是以在线方式运行的。然而，这可能是因为在线方法的开发受到更大关注，这在MOT深度学习研究社区中是一个趋势。在线方法的一个常见问题是其碎片化的数量更高，这在MOTA评分中没有体现出来，如表[7](#S4.T7
    "Table 7 ‣ General observations ‣ 4.2 Discussion of the results ‣ 4 Analysis and
    comparisons ‣ Deep Learning in Video Multi-Object Tracking: A Survey")所示。这是因为当发生遮挡或检测丢失时，在线算法无法在视频中向前查看，重新识别丢失的目标并插补丢失的轨迹部分[[124](#bib.bib124),
    [89](#bib.bib89), [94](#bib.bib94)]。我们可以在图[8](#S4.F8 "Figure 8 ‣ General observations
    ‣ 4.2 Discussion of the results ‣ 4 Analysis and comparisons ‣ Deep Learning in
    Video Multi-Object Tracking: A Survey")中看到一个被在线方法MOTDT [[118](#bib.bib118)] 处理而碎片化的轨迹示例，而该轨迹被批处理方法eHAF16
    [[103](#bib.bib103)] 正确跟踪。'
- en: '![Refer to caption](img/04341286035bbbba3f26f4698c556296.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/04341286035bbbba3f26f4698c556296.png)'
- en: (a) MOTDT output before occlusion
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MOTDT在遮挡前的输出
- en: '![Refer to caption](img/e2dfce9756848ba13820f46e2d1afb9e.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e2dfce9756848ba13820f46e2d1afb9e.png)'
- en: (b) MOTDT output during occlusion
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MOTDT在遮挡期间的输出
- en: '![Refer to caption](img/c494cca5c0407803e699e1deac75b304.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c494cca5c0407803e699e1deac75b304.png)'
- en: (c) MOTDT output after occlusion
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: (c) MOTDT在遮挡后的输出
- en: '![Refer to caption](img/e1522aaf5aa877e70bd7e26dd4027b0b.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e1522aaf5aa877e70bd7e26dd4027b0b.png)'
- en: (d) eHAF16 output before occlusion
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: (d) eHAF16 在遮挡前的输出
- en: '![Refer to caption](img/cba70b3fda7eb2f67dc83ed60454b22e.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cba70b3fda7eb2f67dc83ed60454b22e.png)'
- en: (e) eHAF16 output during occlusion
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: (e) eHAF16 在遮挡期间的输出
- en: '![Refer to caption](img/6713e355aca725afea9245efc84aad82.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6713e355aca725afea9245efc84aad82.png)'
- en: (f) eHAF16 output after occlusion
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: (f) eHAF16 在遮挡后的输出
- en: 'Figure 8: Example of fragmentation produced by an online method during occlusion.
    Above: tracking results for MOTDT [[118](#bib.bib118)], online algorithm. Below:
    tracking results for eHAF16 [[103](#bib.bib103)], batch algorithm. From left to
    right, frames 50, 60 and 70 of the MOT16-08 video are shown for both methods.
    Only the relevant boxes are shown to avoid clutter. As we can see in the image,
    while some online algorithms are able to re-identify lost targets after occlusions,
    they are usually unable to track them while the target is not visible, and this
    results in a fragmentation. Batch methods, on the other hand, are capable of reconstructing
    a fragmented trajectory by inferring the position of the target given past and
    future information.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：在线方法在遮挡期间产生的碎片化示例。上图：MOTDT [[118](#bib.bib118)] 的跟踪结果，在线算法。下图：eHAF16 [[103](#bib.bib103)]
    的跟踪结果，批处理算法。左右分别显示了 MOT16-08 视频的第 50 帧、第 60 帧和第 70 帧。为了避免混乱，仅显示了相关的框。如图所示，虽然一些在线算法能够在遮挡后重新识别丢失的目标，但它们通常无法在目标不可见时进行跟踪，这会导致碎片化。另一方面，批处理方法能够通过利用过去和未来的信息来重建碎片化的轨迹。
- en: 'Another interesting thing to notice is that since the MOTA score is basically
    a normalized sum of FPs, FNs and ID switches, and since the number of FNs is usually
    at least an order of magnitude higher than the FPs and two order of magnitudes
    higher than the ID switches, the methods that manage to strongly reduce the number
    of FNs are the ones that obtain the best performances. We can in fact observe
    a strong correlation between MOTA and number of FNs, in accordance to what was
    found in [[14](#bib.bib14)]: MOTA and FN values are linked by a Pearson correlation
    coefficient of $-0.95$ on MOT15, $-0.98$ on MOT16 and $-0.95$ on MOT17\. So, while
    there have been limited improvements in the reduction of FNs using public detections,
    the most effective way is still building and training a custom detector; the halving
    of the number of FNs is in fact the main reason why private detectors have lead
    to better tracking performances, being able to identify previously uncovered targets.
    In figure [9](#S4.F9 "Figure 9 ‣ Best approaches in the four MOT steps ‣ 4.2 Discussion
    of the results ‣ 4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") we can see how the SORT algorithm, that is particularly sensitive
    to missing detections, is not able to detect a target as soon as the relative
    detection is missing.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个值得注意的有趣现象是，由于 MOTA 分数基本上是 FPs、FNs 和 ID 切换的标准化总和，而 FNs 的数量通常至少比 FPs 多一个数量级，比
    ID 切换多两个数量级，因此那些能够显著减少 FNs 数量的方法往往会获得最佳表现。我们实际上可以观察到 MOTA 和 FNs 数量之间有很强的相关性，这与[[14](#bib.bib14)]中的发现一致：MOTA
    和 FN 值在 MOT15 上的 Pearson 相关系数为 $-0.95$，在 MOT16 上为 $-0.98$，在 MOT17 上为 $-0.95$。因此，虽然使用公共检测方法在减少
    FNs 方面的改进有限，但最有效的方法仍然是构建和训练自定义检测器；FNs 数量减少一半实际上是私有检测器导致更好跟踪性能的主要原因，因为它们能够识别之前未覆盖的目标。在图
    [9](#S4.F9 "Figure 9 ‣ Best approaches in the four MOT steps ‣ 4.2 Discussion
    of the results ‣ 4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey") 中，我们可以看到特别对缺失检测敏感的 SORT 算法在相对检测丢失后无法检测目标。'
- en: To avoid this issue, many new algorithms are including new strategies to tackle
    this problem. In fact, while basic approaches that perform interpolation are able
    to recover missing boxes during occlusions, this is still insufficient to detect
    targets that are not covered by even a single detection, that have been shown
    to be 18% of the total on MOT15 and MOT16 [[14](#bib.bib14)]. For example, the
    eHAF16 algorithm presented by Sheng et al. [[103](#bib.bib103)] employed a superpixel
    extraction algorithm to complement the publicly provided detections and was in
    fact able to significantly reduce the number of false negatives on MOT17, reaching
    top MOTA score on the dataset. The MOTDT algorithm [[118](#bib.bib118)] instead
    used a R-FCN to integrate the missing detections with new candidates, and was
    able to reach best MOTA and lowest number of false negatives among online algorithms
    on MOT17\. The AP-RCNN algorithm [[145](#bib.bib145)] was also able to avoid the
    problems caused by missing detections by employing a Particle Filter algorithm
    and relying on detections only to initialize new targets and to recover lost ones.
    The algorithm presented in [[150](#bib.bib150)] also reduces FNs by designing
    a deep prediction network, whose aim is to learn the motion model of the objects.
    At test time, the network is capable to predict the position of them in subsequent
    frames, and thus reducing the amount of false negatives produced by missing detections.
    In fact, it is the second best among online methods in MOT16 regarding this metric.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，许多新算法正在引入新的策略来解决这个问题。实际上，虽然执行插值的基本方法能够在遮挡期间恢复丢失的目标框，但这仍然不足以检测那些连一个检测都没有覆盖的目标，这些目标在MOT15和MOT16中占总数的18%
    [[14](#bib.bib14)]。例如，Sheng等人提出的eHAF16算法 [[103](#bib.bib103)] 使用了超像素提取算法来补充公开提供的检测，并且实际上能够显著减少MOT17中的假阴性数量，达到了数据集上的最高MOTA得分。MOTDT算法
    [[118](#bib.bib118)] 则使用R-FCN来将丢失的检测与新候选者整合，并能够在MOT17中达到在线算法中的最佳MOTA和最低假阴性数量。AP-RCNN算法
    [[145](#bib.bib145)] 也能够通过使用粒子滤波算法避免丢失检测造成的问题，仅依靠检测来初始化新目标和恢复丢失目标。[[150](#bib.bib150)]中提出的算法也通过设计深度预测网络来减少假阴性，其目标是学习对象的运动模型。在测试时，该网络能够预测对象在后续帧中的位置，从而减少因丢失检测而产生的假阴性。事实上，在MOT16中，它在这一指标上是在线方法中的第二佳。
- en: '| Mode | MOT15 | MOT16 | MOT17 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | MOT15 | MOT16 | MOT17 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Batch | 1143.8 | 1104.9 | 3188.2 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 批次 | 1143.8 | 1104.9 | 3188.2 |'
- en: '| Online | 1509.5 | 1820.2 | 7555.8 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 在线 | 1509.5 | 1820.2 | 7555.8 |'
- en: 'Table 7: Average number of fragmentations for online and batch methods in the
    three considered datasets.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：三种考虑的数据集中在线和批处理方法的平均碎片数。
- en: An important observation must be made regarding the training strategy for affinity
    networks. As noted by [[93](#bib.bib93)], training a network using ground truth
    trajectories to predict affinities might produce suboptimal results, as at test
    time those networks would be exposed to a different data distribution, made of
    noisy trajectories that can include missing/wrong detections. Many algorithms
    in fact have chosen to train networks using either actual detections [[96](#bib.bib96)]
    or by manually adding noise and errors to the ground truth trajectories [[93](#bib.bib93),
    [115](#bib.bib115)], although this may slow the training procedure sometimes and
    not always be feasible [[60](#bib.bib60)].
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 关于亲和网络的训练策略，必须做出一个重要的观察。正如[[93](#bib.bib93)]所指出的，使用真实轨迹来训练网络以预测亲和度可能会产生次优结果，因为在测试时，这些网络将面对不同的数据分布，由噪声轨迹组成，这些轨迹可能包括丢失/错误的检测。事实上，许多算法选择使用实际检测
    [[96](#bib.bib96)] 或通过手动添加噪声和错误到真实轨迹 [[93](#bib.bib93), [115](#bib.bib115)] 来训练网络，尽管这有时可能会减慢训练过程，并且并不总是可行
    [[60](#bib.bib60)]。
- en: Best approaches in the four MOT steps
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 四个MOT步骤中的最佳方法
- en: 'Speaking of private detections, the tables show that the best performing detectors
    are currently Faster R-CNN and its variants. In fact, the algorithm presented
    in [[38](#bib.bib38)], that uses a modified Faster R-CNN, has held its top ranking
    position among online methods on MOT16 for 3 years, and many of the other top-performing
    MOT16 algorithms have employed the same detections. In contrast, algorithms that
    employed the SSD detector, such as the ones presented in [[60](#bib.bib60)] and
    [[61](#bib.bib61)], tend to perform worse. A big advantage of SSD, though, is
    its faster speed: thanks to that the algorithm by Kieritz et al. [[60](#bib.bib60)]
    was able to reach near real-time performance (4.5 FPS) including the detection
    step^(19)^(19)19We remind the reader that the FPS reported by many algorithms
    tend to exclude the detection step, that can easily be the most computationally
    expensive part of the algorithm.. Despite the great number of online methods,
    a major issue in using deep learning techniques in a MOT pipeline is still the
    difficulty in obtaining real-time predictions, making such algorithms not usable
    in most practical online scenarios.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 说到私有检测，表格显示目前表现最佳的检测器是 Faster R-CNN 及其变体。事实上，[[38](#bib.bib38)] 中提出的使用改进版 Faster
    R-CNN 的算法，在 MOT16 上的在线方法中已经保持了 3 年的领先地位，许多其他表现优异的 MOT16 算法也采用了相同的检测方法。相比之下，使用
    SSD 检测器的算法，例如 [[60](#bib.bib60)] 和 [[61](#bib.bib61)] 中提出的算法，往往表现较差。然而，SSD 的一个大优点是其更快的速度：由于这一点，Kieritz
    等人 [[60](#bib.bib60)] 的算法能够达到接近实时的性能（4.5 FPS），包括检测步骤^(19)^(19)19我们提醒读者，许多算法报告的
    FPS 值往往不包括检测步骤，而检测步骤通常是算法中计算最密集的部分.. 尽管在线方法数量众多，但在 MOT 流程中使用深度学习技术的主要问题仍然是难以获得实时预测，使得这些算法在大多数实际在线场景中无法使用。
- en: '![Refer to caption](img/c87cd5bdf8e84918771d0e6f1d21b56e.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c87cd5bdf8e84918771d0e6f1d21b56e.png)'
- en: (a) Public detections
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 公共检测
- en: '![Refer to caption](img/71a1ac84cebdd62e5c80a0f7f72a0ea4.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/71a1ac84cebdd62e5c80a0f7f72a0ea4.png)'
- en: (b) Detections from [[38](#bib.bib38)]
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 来自 [[38](#bib.bib38)] 的检测
- en: '![Refer to caption](img/806dcc2215ffc4eaef32c7e3c5e685d2.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/806dcc2215ffc4eaef32c7e3c5e685d2.png)'
- en: (c) Output tracks with public detections
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 公共检测的输出轨迹
- en: '![Refer to caption](img/c7313e65f196c003ca72313d5e63409b.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7313e65f196c003ca72313d5e63409b.png)'
- en: (d) Output tracks with detections from [[38](#bib.bib38)]
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 使用 [[38](#bib.bib38)] 检测结果的输出轨迹
- en: 'Figure 9: Above: Public detections (generated using DPM v5 [[26](#bib.bib26)])
    and private detections (obtained by [[38](#bib.bib38)] with a customized Faster
    R-CNN trained on multiple datasets) for frame 70 of MOT16-08 sequence. It can
    be observed that the man in the foreground is correctly detected by the custom
    Faster R-CNN detector (b), while it is ignored by DPM (a). Below: Results of tracking
    for the two detection sets using the SORT [[35](#bib.bib35)] algorithm, whose
    performance is heavily dependent on detection errors. We can indeed see that the
    mentioned missing detection produces a corresponding false negative in the tracking
    output (c), while in (d) the man is correctly tracked.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：上图：公共检测（使用 DPM v5 [[26](#bib.bib26)] 生成）和私有检测（通过 [[38](#bib.bib38)] 获取，使用在多个数据集上训练的自定义
    Faster R-CNN）对于 MOT16-08 序列第 70 帧。可以观察到，前景中的人被自定义 Faster R-CNN 检测器（b）正确检测到，而被
    DPM（a）忽略。下图：使用 SORT [[35](#bib.bib35)] 算法对两个检测集进行跟踪的结果，该算法的性能严重依赖于检测错误。我们确实可以看到，提到的漏检在跟踪输出（c）中产生了相应的假阴性，而在（d）中，这个人被正确跟踪。
- en: 'Regarding feature extraction, all the top performing methods on the three considered
    datasets have used a CNN to extract appearance features, where GoogLeNet is the
    most common one. Methods that do not exploit appearance (either extracted with
    deep or classical methods) tend to perform worse. However, visual features are
    not enough: many of the best algorithms also employ other types of features to
    compute affinity, especially motion ones. In fact, algorithms like LSTMs and Kalman
    Filters are often employed to predict the position of the target in the next frame
    and this often helps in improving the quality of the association. Various Bayesian
    filter algorithms, such as particle filter and hypotheses density filter, are
    also used to predict target motion, and they benefit from the use of deep models
    [[158](#bib.bib158), [145](#bib.bib145), [98](#bib.bib98)]. Nonetheless, even
    when employed together with non-visual features, appearance still plays a major
    role in improving the overall performance of the algorithm [[123](#bib.bib123),
    [158](#bib.bib158)], especially in avoiding ID switches [[83](#bib.bib83)] or
    to re-identify targets after long occlusions [[41](#bib.bib41)]. In the latter
    case, simple motion predictors do not work since the linear motion assumption
    is easily broken, as noted by Zhou et al. [[55](#bib.bib55)].'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 关于特征提取，所有在考虑的三个数据集上表现最好的方法都使用了CNN来提取外观特征，其中GoogLeNet是最常见的。未利用外观特征（无论是通过深度方法还是经典方法提取的）的方法表现往往较差。然而，视觉特征并不足够：许多最佳算法还使用其他类型的特征来计算相似度，特别是运动特征。实际上，LSTMs和卡尔曼滤波器等算法通常被用来预测目标在下一帧中的位置，这通常有助于提高关联的质量。各种贝叶斯滤波算法，如粒子滤波器和假设密度滤波器，也用于预测目标运动，并从使用深度模型中受益[[158](#bib.bib158)，[145](#bib.bib145)，[98](#bib.bib98)]。尽管如此，即使与非视觉特征一起使用，外观特征在提高算法的整体表现上仍然发挥了主要作用[[123](#bib.bib123)，[158](#bib.bib158)]，尤其是在避免ID切换[[83](#bib.bib83)]或在长时间遮挡后重新识别目标[[41](#bib.bib41)]。在后者的情况下，简单的运动预测器不起作用，因为线性运动假设很容易被打破，正如Zhou等人所指出的[[55](#bib.bib55)]。
- en: 'While deep learning plays an important role in detection and feature extraction,
    the use of deep networks to learn affinity functions is less ubiquitous and has
    not yet been proven to be essential for a good MOT algorithm. Many algorithms
    in fact rely on a combination of hand-crafted distance metrics on a variety of
    deep and non-deep features. However, some works have already demonstrated how
    using affinity networks can produce top-performing algorithms [[148](#bib.bib148),
    [145](#bib.bib145), [51](#bib.bib51), [96](#bib.bib96)], with approaches ranging
    from the use of Siamese CNNs to recurrent neural networks. In particular, the
    adapted Siamese network proposed by Ma et al. [[148](#bib.bib148)] was able to
    produce reliable similarity measures that helped with the person re-identification
    after occlusions and allowed the algorithm to reach the highest MOTA score on
    MOT16\. The integration of body part information was also crucial for the StackNetPose
    CNN proposed by Tang et al. [[51](#bib.bib51)]: it served as an attention mechanism
    that allowed the network to focus on the relevant parts of the input images, thus
    producing more accurate similarity measures. The algorithm was able to reach top
    performance on MOT16 using private detections.'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在检测和特征提取中扮演了重要角色，但使用深度网络来学习相似度函数的普及程度较低，目前尚未证明这对于一个好的MOT算法是必不可少的。实际上，许多算法依赖于对各种深度和非深度特征的手工制作距离度量的组合。然而，一些研究已经展示了如何使用相似度网络来产生顶级表现的算法[[148](#bib.bib148)，[145](#bib.bib145)，[51](#bib.bib51)，[96](#bib.bib96)]，这些方法从使用Siamese
    CNNs到递归神经网络都有涉及。特别是Ma等人提出的改进版Siamese网络[[148](#bib.bib148)]能够产生可靠的相似度度量，这有助于在遮挡后进行人员重新识别，并使算法在MOT16中达到最高的MOTA评分。Tang等人提出的StackNetPose
    CNN[[51](#bib.bib51)]中，身体部位信息的整合也是至关重要的：它作为一种注意机制，使网络能够集中关注输入图像的相关部分，从而产生更准确的相似度度量。该算法使用私有检测在MOT16中达到了顶级表现。
- en: Very few works have instead explored the use deep learning models to guide the
    association process, and this could be a interesting research direction for future
    approaches.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有研究探索使用深度学习模型来指导关联过程，这可能是未来研究的一种有趣方向。
- en: Other trends in top-performing algorithms
  id: totrans-352
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 顶级表现算法的其他趋势
- en: 'Some other trends can be identified among current top ranked methods. For example,
    a successful approach in online methods is the use of Single Object Tracking algorithms,
    properly modified in order to solve the MOT task. Some of the top performing online
    algorithms on the 3 datasets have in fact employed a SOT tracker augmented with
    deep learning techniques to recover from occlusions or to refresh the target models
    [[159](#bib.bib159), [115](#bib.bib115), [123](#bib.bib123)]. Interestingly, to
    the best of our knowledge, no adapted SOT algorithm has been used to perform tracking
    with private detections. As we have already observed, the use of private detections
    reduces the number of completely uncovered targets; since SOT trackers don’t usually
    need detections to keep following a target once it’s been identified, the reduction
    in uncovered targets might translate in a much lower number of lost tracks, that
    in turn would enhance the overall performance of tracker. The application of a
    SOT tracker on private detections could thus be a good research direction to try
    to further improve the results on the MOTChallenge datasets. A batch method could
    also exploit a SOT tracker to look at past frames in order to recover missed detections
    before the target was first identified by the detector. However, SOT-based MOT
    trackers can sometimes still be prone to tracking drift and produce a higher number
    of ID switches. For example, the KCF16 algorithm [[159](#bib.bib159)], while reaching
    top MOTA score among online methods on MOT16 on public detections, it still produces
    a relatively high number of switches due to tracker drift, as can be seen in figure
    [10](#S4.F10 "Figure 10 ‣ Other trends in top-performing algorithms ‣ 4.2 Discussion
    of the results ‣ 4 Analysis and comparisons ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey"). Moreover, SOT-based MOT algorithms must be careful not to
    keep tracking spurious trajectories, caused by the inevitably higher number of
    false positive detections predicted by higher-quality detectors, for too many
    frames, as this might offset the reduction in the number of FNs. Current approaches
    [[159](#bib.bib159), [115](#bib.bib115)] still tend to use detection overlap (e.g.
    in how many recent frames the trajectory is covered by a detection) to understand
    if a trajectory is a true or a false positive in the long run, but better solutions
    should be investigated to avoid exclusive reliance on detections.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 当前排名前列的方法中可以识别出一些其他趋势。例如，在在线方法中，一个成功的策略是使用单目标跟踪算法，经过适当修改以解决多目标跟踪任务。实际上，一些在3个数据集上表现最好的在线算法采用了增强了深度学习技术的SOT跟踪器，以应对遮挡或刷新目标模型[[159](#bib.bib159),
    [115](#bib.bib115), [123](#bib.bib123)]。有趣的是，据我们所知，还没有适应的SOT算法用于利用私有检测进行跟踪。正如我们已经观察到的那样，使用私有检测减少了完全未覆盖目标的数量；由于SOT跟踪器通常不需要检测就能继续跟踪一个已识别的目标，未覆盖目标的减少可能会转化为丢失轨迹的显著降低，这反过来会提升跟踪器的总体性能。因此，将SOT跟踪器应用于私有检测可能是进一步提高MOTChallenge数据集结果的一个良好研究方向。批量方法也可以利用SOT跟踪器查看过去的帧，以恢复在目标首次被检测器识别之前错过的检测。然而，基于SOT的MOT跟踪器有时仍可能面临跟踪漂移的问题，并产生较高的ID切换。例如，KCF16算法[[159](#bib.bib159)]在公共检测的MOT16中虽然在在线方法中取得了顶级MOTA分数，但由于跟踪器漂移，仍然产生了相对较高的切换次数，如图[10](#S4.F10
    "图 10 ‣ 表现最好的算法中的其他趋势 ‣ 4.2 结果讨论 ‣ 4 分析与比较 ‣ 深度学习在视频多目标跟踪中的应用：综述")所示。此外，基于SOT的MOT算法必须小心不要跟踪由高质量检测器预测的不可避免的较高假阳性检测产生的虚假轨迹过多帧，因为这可能会抵消假阴性数量的减少。目前的方法[[159](#bib.bib159),
    [115](#bib.bib115)]仍倾向于使用检测重叠（例如，轨迹在多少最近帧中被检测到）来理解一个轨迹在长期中是正样本还是假阳性，但应研究更好的解决方案以避免完全依赖检测。
- en: '![Refer to caption](img/09057d2919cd411fa759b91216cc385c.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/09057d2919cd411fa759b91216cc385c.png)'
- en: (a) Frame 14
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 帧 14
- en: '![Refer to caption](img/2c1adcc74c7f16bebe4f1d27deca808e.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2c1adcc74c7f16bebe4f1d27deca808e.png)'
- en: (b) Frame 22
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 帧 22
- en: '![Refer to caption](img/0520676026c25bdf6e28962e50d58254.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0520676026c25bdf6e28962e50d58254.png)'
- en: (c) Frame 28
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 帧 28
- en: '![Refer to caption](img/ca1eaac10da9860cdc78ba3d49d91b0d.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ca1eaac10da9860cdc78ba3d49d91b0d.png)'
- en: (d) Frame 34
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 帧 34
- en: 'Figure 10: Example of SOT drift in the context of a MOT algorithm (KFC16 [[159](#bib.bib159)]).
    The four images are cropped from the MOT16-07 video, and are best viewed in color,
    since each color represents a different target ID. At first (a) the three persons
    are tracked. After a few frames (b) the red box starts drifting towards the occluded
    man, while the light blue box starts drifting towards the foreground woman. In
    (c) the white track is interrupted and the first two ID switches are completed.
    In (d) a new identity is assigned to the woman in the background, causing a third
    ID switch.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：MOT算法（KFC16 [[159](#bib.bib159)]）中的SOT漂移示例。四张图片是从MOT16-07视频中裁剪出来的，最佳观赏效果为彩色图像，因为每种颜色代表不同的目标ID。最初（a）跟踪了三个人。经过几帧（b），红色框开始漂移向被遮挡的人，而浅蓝色框开始漂移向前景中的女人。在（c）中，白色轨迹被中断，前两个ID切换完成。在（d）中，背景中的女人被分配了一个新身份，导致了第三次ID切换。
- en: While many methods perform association by formulating the task as a graph optimization
    problem, batch methods benefit in particular from this, since they can perform
    global optimization on them. For example, the minimum cost lifted multicut problem
    has reached top performance on MOT16, helped by CNN-computed affinities [[148](#bib.bib148),
    [51](#bib.bib51)], while heterogeneous association graph fusion and correlation
    clustering are used on the two top MOT17 methods [[103](#bib.bib103), [155](#bib.bib155)].
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多方法通过将任务表述为图优化问题来执行关联，但批处理方法特别从中受益，因为它们可以在全局范围内进行优化。例如，最小成本提升多割问题在MOT16中表现最佳，得到了CNN计算的亲和度
    [[148](#bib.bib148), [51](#bib.bib51)] 的帮助，而异质关联图融合和相关聚类则用于两个顶级MOT17方法 [[103](#bib.bib103),
    [155](#bib.bib155)]。
- en: Finally, it can be noticed that the accuracy of bounding boxes radically affects
    the final performance of the algorithms. In fact, the top ranked tracker on MOT15
    [[89](#bib.bib89)] obtained a relatively high MOTA score by just performing a
    bounding box regression step on the output of a previous state-of-the-art algorithm
    [[89](#bib.bib89)] using a deep RL agent. Developing an effective bounding box
    regressor to be incorporated in future MOT algorithms could be an interesting
    research direction that has not yet been explored thoroughly. Moreover, instead
    of relying on a single frame to fix the boxes, that could make them enclose the
    wrong target in case of an occlusion, batch methods could also try to exploit
    future and past target appearance to more accurately regress the bounding boxes
    around the right target.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以注意到边界框的准确性对算法的最终性能有着根本性的影响。事实上，在MOT15 [[89](#bib.bib89)] 中排名最高的跟踪器通过对之前的最先进算法的输出进行边界框回归，获得了相对较高的MOTA得分
    [[89](#bib.bib89)]，使用了一个深度RL代理。开发一个有效的边界框回归器以纳入未来的MOT算法可能是一个尚未彻底探索的有趣研究方向。此外，与依赖单帧来修正框（在遮挡情况下可能会将错误的目标包含在内）相比，批处理方法也可以尝试利用未来和过去的目标外观来更准确地回归到正确目标周围的边界框。
- en: 5 Conclusion and future directions
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来方向
- en: 'We have presented a comprehensive description of all MOT algorithms employing
    deep learning techniques, focusing on single-camera videos and 2D data. Four main
    steps have been shown to characterize a generic MOT pipeline: detection, feature
    extraction, affinity computation, association. The use of deep learning in each
    of these four steps has been explored. While most of the approaches have focused
    on the first two, some applications of deep learning to learn affinity functions
    are also present, but only very few approaches use deep learning to directly guide
    the association algorithm. A numerical comparison of the results on the MOTChallenge
    datasets has also been provided, showing that, despite the wide variety of approaches,
    some commonalities can be found among the presented methods:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对所有采用深度学习技术的MOT算法进行了全面描述，重点关注单摄像头视频和二维数据。展示了四个主要步骤来描述一个通用的MOT流程：检测、特征提取、亲和计算、关联。探讨了深度学习在这四个步骤中的应用。虽然大多数方法集中在前两步，但也有一些深度学习应用于学习亲和函数的案例，但只有很少的方法直接使用深度学习来指导关联算法。还提供了对MOTChallenge数据集结果的数值比较，显示尽管方法种类繁多，但呈现的方法之间可以找到一些共同点：
- en: •
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'detection quality is important: the amount of false negatives still dominates
    the MOTA score. While deep learning has allowed for some improvement in this regard
    for algorithms employing public detections, the use of higher quality detections
    is still the most effective way to reduce false negatives. Thus, a careful use
    of deep learning in the detection step can considerably improve the performance
    of a tracking algorithm;'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测质量很重要：假阴性的数量仍然主导了MOTA评分。虽然深度学习在使用公共检测算法方面有所改善，但使用更高质量的检测仍然是减少假阴性的最有效方法。因此，深度学习在检测步骤中的谨慎使用可以显著提高跟踪算法的性能；
- en: •
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CNNs are essential in feature extraction: the use of appearance features is
    also fundamental for a good tracker and CNNs are particularly effective at extracting
    them. Moreover, strong trackers tend to use them in conjunction with motion features,
    that can be computed using LSTMs, Kalman filter or other Bayesian filters;'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNN）在特征提取中至关重要：外观特征的使用对一个优秀的跟踪器也至关重要，而CNN在提取这些特征方面特别有效。此外，强大的跟踪器往往将这些特征与运动特征结合使用，运动特征可以通过长短期记忆网络（LSTM）、卡尔曼滤波器或其他贝叶斯滤波器来计算；
- en: •
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SOT trackers and global graph optimization work: the adaptation of SOT trackers
    to the MOT task, with the help of deep learning, has recently produced good-performing
    online trackers; batch methods have instead benefited from the integration of
    deep models in global graph optimization algorithms.'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单目标跟踪（SOT）跟踪器和全局图优化工作：SOT跟踪器在深度学习的帮助下，最近在多目标跟踪任务中产生了良好的在线跟踪器；而批处理方法则得益于在全局图优化算法中集成深度模型。
- en: 'As deep learning has been introduced only recently in the field of MOT, a number
    of promising future research directions have also been identified:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习在多目标跟踪领域仅最近被引入，因此还识别出了一些有前景的未来研究方向：
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'researching more strategies to mitigate detection errors: although modern detectors
    are constantly reaching better and better performances, they are still prone to
    produce a significant number of false negatives and false positives in complex
    scenarios such as dense pedestrian tracking. Some algorithms have provided solutions
    to reduce the exclusive reliance on detections by integrating them with information
    extracted from other sources (e.g. superpixels [[103](#bib.bib103)], R-FCN [[118](#bib.bib118)],
    Particle Filter [[145](#bib.bib145)], etc.), but further strategies should be
    investigated;'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究更多策略以减轻检测错误：尽管现代检测器不断提高性能，但在复杂场景（如密集行人跟踪）中，它们仍然容易产生大量的假阴性和假阳性。一些算法提供了解决方案，通过将检测与其他来源提取的信息（例如超级像素[[103](#bib.bib103)]、R-FCN
    [[118](#bib.bib118)]、粒子滤波器 [[145](#bib.bib145)]等）结合使用，减少对检测的过度依赖，但仍需研究进一步的策略；
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'applying DL to track different targets: most of DL-based MOT algorithms have
    focused on pedestrian tracking. Since different types of targets pose different
    challenges, possible improvements in tracking vehicles, animals, or other objects
    with the use of deep networks should be investigated;'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将深度学习应用于跟踪不同目标：大多数基于深度学习的多目标跟踪（MOT）算法主要关注行人跟踪。由于不同类型的目标带来了不同的挑战，应该研究使用深度网络在跟踪车辆、动物或其他物体方面可能的改进；
- en: •
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'investigating the robustness of current algorithms: how do current methods
    perform under different camera conditions? How do a varying contrast, illumination,
    the presence of noisy/missing frames affect the result of current algorithms?
    Are existing DL networks able to generalize to different tracking contexts? For
    example, the vast majority of people tracking frameworks are trained to follow
    pedestrians or athletes, but tracking could be useful in other scenarios. A possible
    new application could be helping with scene understanding in different contexts:
    inside movies, in order to generate textual descriptions to provide a coarse way
    of searching for a scene in a movie; or on social networks, in order to generate
    descriptions for blind users or to detect inappropriate videos that should be
    removed from the platform. These different scenarios would probably require changes
    to the current detection and tracking algorithms, since the people could appear
    in unusual poses and behaviors that are not present in the existing datasets for
    MOT;'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调查当前算法的鲁棒性：当前方法在不同摄像头条件下的表现如何？变化的对比度、光照、噪声/丢失帧的存在如何影响当前算法的结果？现有的DL网络能否适应不同的跟踪环境？例如，绝大多数人跟踪框架是训练来跟踪行人或运动员，但跟踪在其他场景中可能也有用。一个可能的新应用是帮助理解不同背景下的场景：在电影中生成文本描述，以提供粗略的搜索方式；或在社交网络上生成盲人用户的描述，或检测应从平台上移除的不当视频。这些不同的场景可能需要对当前检测和跟踪算法进行修改，因为人们可能会以不寻常的姿势和行为出现，这些在现有的MOT数据集中并不存在。
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'applying DL to guide association: the use of deep learning to guide the association
    algorithm and to directly perform tracking is still in its infancy: more research
    is needed in this direction to understand if deep algorithms can be useful in
    this step too;'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用DL来指导关联：使用深度学习来指导关联算法并直接进行跟踪仍处于起步阶段：需要在这一方向上进行更多研究，以了解深度算法是否在这一步也能发挥作用；
- en: •
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'combining SOT trackers with private detections: a possible way to reduce the
    number of lost tracks, and thus reduce the false negatives, could be the combination
    of SOT trackers with private detections, especially in a batch setting, where
    it would be possible to recover past detections that were previously missed;'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将SOT跟踪器与私人检测结合：减少丢失轨迹的可能方法之一，从而减少假阴性，是将SOT跟踪器与私人检测结合，尤其是在批处理设置中，在那里可以恢复之前遗漏的检测；
- en: •
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'investigating bounding box regression: the use of bounding box regression has
    been shown to be a promising step in obtaining a higher MOTA score, but this has
    not yet been explored in detail and further improvements should be investigated,
    e.g. the use of past and future information to guide the regression;'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调查边界框回归：使用边界框回归已被证明是获得更高MOTA分数的有前途的一步，但这一方法尚未被详细探讨，进一步改进应予以调查，例如使用过去和未来的信息来引导回归；
- en: •
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'investigating post-tracking processing: in batch contexts, it is possible to
    apply correction algorithms on the output of a tracker to increase its performance.
    This has already been shown by Babaee et al. [[132](#bib.bib132)], that have applied
    occlusion handling on top of existing algorithms, and by Jiang et al. [[152](#bib.bib152)]
    with the aforementioned bounding box regression step. More complex processing
    could be applied on the results from a tracker to further improve the results.'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 调查后跟踪处理：在批处理环境中，可以在跟踪器的输出上应用修正算法以提高其性能。Babaee等人已经展示了这一点[[132](#bib.bib132)]，他们在现有算法上应用了遮挡处理，而Jiang等人[[152](#bib.bib152)]则进行了前述的边界框回归步骤。可以在跟踪器的结果上应用更复杂的处理，以进一步提高结果。
- en: Finally, as very few of the presented algorithms have provided public access
    to their source code, we would like to encourage future researchers to publish
    their code in order to allow for better reproducibility of their results and benefit
    the whole research community.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，由于很少有已发布的算法提供了源代码公开访问，我们希望鼓励未来的研究人员发布他们的代码，以便更好地复现他们的结果并惠及整个研究社区。
- en: Acknowledgements
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research work is partially supported by the Spanish Ministry of Science
    and Technology under the project TIN2017-89517-P and the project DeepSCOP-Ayudas
    Fundación BBVA a Equipos de Investigación Científica en Big Data 2018\. Siham
    Tabik was supported by the Ramon y Cajal Programme (RYC-2015-18136).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究部分由西班牙科学与技术部项目 TIN2017-89517-P 和项目 DeepSCOP-Ayudas Fundación BBVA a Equipos
    de Investigación Científica en Big Data 2018 资助。Siham Tabik 还获得了 Ramon y Cajal
    计划（RYC-2015-18136）的支持。
- en: References
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for
    large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Karen Simonyan 和 Andrew Zisserman. 用于大规模图像识别的非常深卷积网络。arXiv 预印本 arXiv:1409.1556，2014。'
- en: '[2] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 1–9, 2015.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, 和 Andrew Rabinovich. 通过卷积进一步深入。见
    IEEE 计算机视觉与模式识别会议论文集，页码 1–9，2015。'
- en: '[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 用于图像识别的深度残差学习。见 IEEE
    计算机视觉与模式识别会议论文集，页码 770–778，2016。'
- en: '[4] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
    real-time object detection with region proposal networks. In Advances in neural
    information processing systems, pages 91–99, 2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Shaoqing Ren, Kaiming He, Ross Girshick, 和 Jian Sun. Faster R-CNN：利用区域提议网络实现实时目标检测。见神经信息处理系统进展，页码
    91–99，2015。'
- en: '[5] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European
    conference on computer vision, pages 21–37. Springer, 2016.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, 和 Alexander C Berg. SSD：单次多框检测器。见欧洲计算机视觉会议论文集，页码 21–37。Springer，2016。'
- en: '[6] Joseph Redmon and Ali Farhadi. Yolo9000: better, faster, stronger. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 7263–7271,
    2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Joseph Redmon 和 Ali Farhadi. Yolo9000：更好、更快、更强。见 IEEE 计算机视觉与模式识别会议论文集，页码
    7263–7271，2017。'
- en: '[7] Haşim Sak, Andrew Senior, and Françoise Beaufays. Long short-term memory
    recurrent neural network architectures for large scale acoustic modeling. In Fifteenth
    annual conference of the international speech communication association, 2014.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Haşim Sak, Andrew Senior, 和 Françoise Beaufays. 用于大规模声学建模的长短期记忆递归神经网络架构。见国际语音通信协会第十五届年会，2014。'
- en: '[8] Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. Lstm neural networks
    for language modeling. In Thirteenth annual conference of the international speech
    communication association, 2012.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Martin Sundermeyer, Ralf Schlüter, 和 Hermann Ney. Lstm 神经网络在语言建模中的应用。见国际语音通信协会第十三届年会，2012。'
- en: '[9] Yuchen Fan, Yao Qian, Feng-Long Xie, and Frank K Soong. Tts synthesis with
    bidirectional lstm based recurrent neural networks. In Fifteenth Annual Conference
    of the International Speech Communication Association, 2014.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Yuchen Fan, Yao Qian, Feng-Long Xie, 和 Frank K Soong. 基于双向 LSTM 的 TTS 合成。见国际语音通信协会第十五届年会，2014。'
- en: '[10] Erik Marchi, Giacomo Ferroni, Florian Eyben, Leonardo Gabrielli, Stefano
    Squartini, and Björn Schuller. Multi-resolution linear prediction based features
    for audio onset detection with bidirectional lstm neural networks. In 2014 IEEE
    international conference on acoustics, speech and signal processing (ICASSP),
    pages 2164–2168\. IEEE, 2014.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Erik Marchi, Giacomo Ferroni, Florian Eyben, Leonardo Gabrielli, Stefano
    Squartini, 和 Björn Schuller. 基于多分辨率线性预测的音频起始检测特征与双向 LSTM 神经网络。见 2014 IEEE 国际声学、语音与信号处理会议（ICASSP），页码
    2164–2168。IEEE，2014。'
- en: '[11] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Xiaowei
    Zhao, and Tae-Kyun Kim. Multiple object tracking: A literature review. arXiv preprint
    arXiv:1409.7618, 2014.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, Xiaowei
    Zhao, 和 Tae-Kyun Kim. 多目标跟踪：文献综述。arXiv 预印本 arXiv:1409.7618，2014。'
- en: '[12] Massimo Camplani, Adeline Paiement, Majid Mirmehdi, Dima Damen, Sion Hannuna,
    Tilo Burghardt, and Lili Tao. Multiple human tracking in rgb-depth data: a survey.
    IET computer vision, 11(4):265–285, 2016.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Massimo Camplani, Adeline Paiement, Majid Mirmehdi, Dima Damen, Sion Hannuna,
    Tilo Burghardt, 和 Lili Tao. 基于 RGB-深度数据的人体多目标跟踪：综述。IET 计算机视觉，11(4)：265–285，2016。'
- en: '[13] Patrick Emami, Panos M Pardalos, Lily Elefteriadou, and Sanjay Ranka.
    Machine learning methods for solving assignment problems in multi-target tracking.
    arXiv preprint arXiv:1802.06897, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Patrick Emami、Panos M Pardalos、Lily Elefteriadou 和 Sanjay Ranka。解决多目标跟踪中的分配问题的机器学习方法。arXiv预印本arXiv:1802.06897，2018年。'
- en: '[14] Laura Leal-Taixé, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid,
    and Stefan Roth. Tracking the trackers: An analysis of the state of the art in
    multiple object tracking. arXiv preprint arXiv:1704.02781, 2017.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Laura Leal-Taixé、Anton Milan、Konrad Schindler、Daniel Cremers、Ian Reid
    和 Stefan Roth。追踪跟踪器：多目标跟踪技术的现状分析。arXiv预印本arXiv:1704.02781，2017年。'
- en: '[15] Laura Leal-Taixé, Anton Milan, Ian Reid, Stefan Roth, and Konrad Schindler.
    Motchallenge 2015: Towards a benchmark for multi-target tracking. arXiv preprint
    arXiv:1504.01942, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Laura Leal-Taixé、Anton Milan、Ian Reid、Stefan Roth 和 Konrad Schindler。Motchallenge
    2015：多目标跟踪基准的进展。arXiv预印本arXiv:1504.01942，2015年。'
- en: '[16] Anton Milan, Laura Leal-Taixé, Ian Reid, Stefan Roth, and Konrad Schindler.
    Mot16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831,
    2016.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Anton Milan、Laura Leal-Taixé、Ian Reid、Stefan Roth 和 Konrad Schindler。Mot16：多目标跟踪基准。arXiv预印本arXiv:1603.00831，2016年。'
- en: '[17] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE international conference on computer vision, pages
    2961–2969, 2017.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Kaiming He、Georgia Gkioxari、Piotr Dollár 和 Ross Girshick。Mask R-CNN。发表于IEEE国际计算机视觉会议，页2961–2969，2017年。'
- en: '[18] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via
    region-based fully convolutional networks. In Advances in neural information processing
    systems, pages 379–387, 2016.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Jifeng Dai、Yi Li、Kaiming He 和 Jian Sun。R-fcn：通过基于区域的全卷积网络进行物体检测。发表于神经信息处理系统进展，页379–387，2016年。'
- en: '[19] Bo Wu and Ram Nevatia. Tracking of multiple, partially occluded humans
    based on static body part detection. In 2006 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition (CVPR’06), volume 1, pages 951–958\.
    IEEE, 2006.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Bo Wu 和 Ram Nevatia。基于静态身体部位检测的多目标、部分遮挡人的跟踪。发表于2006年IEEE计算机学会计算机视觉与模式识别会议（CVPR’06），第1卷，页951–958。IEEE，2006年。'
- en: '[20] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking
    performance: the clear mot metrics. Journal on Image and Video Processing, 2008:1,
    2008.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Keni Bernardin 和 Rainer Stiefelhagen。评估多目标跟踪性能：CLEAR MOT指标。《图像与视频处理期刊》，2008:1，2008年。'
- en: '[21] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo
    Tomasi. Performance measures and a data set for multi-target, multi-camera tracking.
    In European Conference on Computer Vision, pages 17–35. Springer, 2016.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Ergys Ristani、Francesco Solera、Roger Zou、Rita Cucchiara 和 Carlo Tomasi。多目标、多摄像头跟踪的性能度量和数据集。发表于欧洲计算机视觉会议，页17–35。Springer，2016年。'
- en: '[22] Rainer Stiefelhagen and John Garofolo. Multimodal Technologies for Perception
    of Humans: First International Evaluation Workshop on Classification of Events,
    Activities and Relationships, CLEAR 2006, Southampton, UK, April 6-7, 2006, Revised
    Selected Papers, volume 4122. Springer, 2007.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Rainer Stiefelhagen 和 John Garofolo。人类感知的多模态技术：第一次国际评估研讨会关于事件、活动和关系分类，CLEAR
    2006，南安普顿，英国，2006年4月6-7日，修订版精选论文，第4122卷。Springer，2007年。'
- en: '[23] Rainer Stiefelhagen, Rachel Bowers, and Jonathan Fiscus. Multimodal Technologies
    for Perception of Humans: International Evaluation Workshops CLEAR 2007 and RT
    2007, Baltimore, MD, USA, May 8-11, 2007, Revised Selected Papers, volume 4625.
    Springer, 2008.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Rainer Stiefelhagen、Rachel Bowers 和 Jonathan Fiscus。人类感知的多模态技术：国际评估研讨会CLEAR
    2007和RT 2007，巴尔的摩，美国，2007年5月8-11日，修订版精选论文，第4625卷。Springer，2008年。'
- en: '[24] Piotr Dollár, Ron Appel, Serge Belongie, and Pietro Perona. Fast feature
    pyramids for object detection. IEEE transactions on pattern analysis and machine
    intelligence, 36(8):1532–1545, 2014.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Piotr Dollár、Ron Appel、Serge Belongie 和 Pietro Perona。用于物体检测的快速特征金字塔。《模式分析与机器智能IEEE事务》，36(8)：1532–1545，2014年。'
- en: '[25] Pedro F. Felzenszwalb, Ross B. Girshick, David McAllester, and Deva Ramanan.
    Object detection with discriminatively trained part-based models. IEEE transactions
    on pattern analysis and machine intelligence, 32(9):1627–1645, 2009.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Pedro F. Felzenszwalb、Ross B. Girshick、David McAllester 和 Deva Ramanan。基于判别训练的部件模型的物体检测。《模式分析与机器智能IEEE事务》，32(9)：1627–1645，2009年。'
- en: '[26] Ross B. Girshick, Pedro F. Felzenszwalb, and David McAllester. Discriminatively
    trained deformable part models, release 5. [http://people.cs.uchicago.edu/~rbg/latent-release5/](http://people.cs.uchicago.edu/~rbg/latent-release5/),
    2012.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ross B. Girshick, Pedro F. Felzenszwalb 和 David McAllester. 区分训练的可变形部件模型，第
    5 版。[http://people.cs.uchicago.edu/~rbg/latent-release5/](http://people.cs.uchicago.edu/~rbg/latent-release5/)，2012
    年。'
- en: '[27] Fan Yang, Wongun Choi, and Yuanqing Lin. Exploit all the layers: Fast
    and accurate cnn object detector with scale dependent pooling and cascaded rejection
    classifiers. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 2129–2137, 2016.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Fan Yang, Wongun Choi 和 Yuanqing Lin. 利用所有层级：具有尺度依赖池化和级联拒绝分类器的快速而准确的 CNN
    目标检测器。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 2129–2137，2016 年。'
- en: '[28] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers,
    Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixe. Cvpr19 tracking
    and detection challenge: How crowded can it get?, 2019.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers,
    Ian Reid, Stefan Roth, Konrad Schindler 和 Laura Leal-Taixe. CVPR19 跟踪与检测挑战：可以拥挤到什么程度？，2019
    年。'
- en: '[29] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous
    driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer
    Vision and Pattern Recognition, pages 3354–3361\. IEEE, 2012.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Andreas Geiger, Philip Lenz 和 Raquel Urtasun. 我们为自动驾驶做好准备了吗？KITTI 视觉基准套件。发表于
    2012 年 IEEE 计算机视觉与模式识别会议，页码 3354–3361。IEEE，2012 年。'
- en: '[30] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision
    meets robotics: The kitti dataset. The International Journal of Robotics Research,
    32(11):1231–1237, 2013.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Andreas Geiger, Philip Lenz, Christoph Stiller 和 Raquel Urtasun. 视觉遇见机器人：KITTI
    数据集。《国际机器人研究杂志》，32(11):1231–1237，2013 年。'
- en: '[31] Xiaoyu Wang, Ming Yang, Shenghuo Zhu, and Yuanqing Lin. Regionlets for
    generic object detection. In Proceedings of the IEEE international conference
    on computer vision, pages 17–24, 2013.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Xiaoyu Wang, Ming Yang, Shenghuo Zhu 和 Yuanqing Lin. 用于通用目标检测的 Regionlets。发表于
    IEEE 国际计算机视觉会议论文集，页码 17–24，2013 年。'
- en: '[32] Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang
    Qi, Jongwoo Lim, Ming-Hsuan Yang, and Siwei Lyu. Ua-detrac: A new benchmark and
    protocol for multi-object detection and tracking. arXiv preprint arXiv:1511.04136,
    2015.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Longyin Wen, Dawei Du, Zhaowei Cai, Zhen Lei, Ming-Ching Chang, Honggang
    Qi, Jongwoo Lim, Ming-Hsuan Yang 和 Siwei Lyu. Ua-detrac：多目标检测与跟踪的新基准和协议。arXiv
    预印本 arXiv:1511.04136，2015 年。'
- en: '[33] Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele. Monocular 3d pose
    estimation and tracking by detection. In 2010 IEEE Computer Society Conference
    on Computer Vision and Pattern Recognition, pages 623–630\. IEEE, 2010.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Mykhaylo Andriluka, Stefan Roth 和 Bernt Schiele. 通过检测进行单目 3D 姿态估计和跟踪。发表于
    2010 年 IEEE 计算机协会计算机视觉与模式识别会议，页码 623–630。IEEE，2010 年。'
- en: '[34] James Ferryman and Ali Shahrokni. Pets2009: Dataset and challenge. In
    2009 Twelfth IEEE International Workshop on Performance Evaluation of Tracking
    and Surveillance, pages 1–6\. IEEE, 2009.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] James Ferryman 和 Ali Shahrokni. Pets2009：数据集和挑战。发表于 2009 年第十二届 IEEE 国际跟踪与监控性能评估研讨会，页码
    1–6。IEEE，2009 年。'
- en: '[35] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple
    online and realtime tracking. In 2016 IEEE International Conference on Image Processing
    (ICIP), pages 3464–3468\. IEEE, 2016.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos 和 Ben Upcroft. 简单的在线和实时跟踪。发表于
    2016 年 IEEE 国际图像处理会议 (ICIP)，页码 3464–3468。IEEE，2016 年。'
- en: '[36] Rudolph Emil Kalman. A new approach to linear filtering and prediction
    problems. Journal of basic Engineering, 82(1):35–45, 1960.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Rudolph Emil Kalman. 线性滤波和预测问题的新方法。《基础工程杂志》，82(1):35–45，1960 年。'
- en: '[37] Harold W Kuhn. The hungarian method for the assignment problem. Naval
    research logistics quarterly, 2(1-2):83–97, 1955.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Harold W Kuhn. 匈牙利算法用于分配问题。《海军研究物流季刊》，2(1-2):83–97，1955 年。'
- en: '[38] Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi, and Junjie Yan.
    Poi: Multiple object tracking with high performance detection and appearance feature.
    In European Conference on Computer Vision, pages 36–42. Springer, 2016.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Fengwei Yu, Wenbo Li, Quanquan Li, Yu Liu, Xiaohua Shi 和 Junjie Yan. Poi：具有高性能检测和外观特征的多目标跟踪。发表于欧洲计算机视觉会议，页码
    36–42。Springer，2016 年。'
- en: '[39] Sean Bell, C Lawrence Zitnick, Kavita Bala, and Ross Girshick. Inside-outside
    net: Detecting objects in context with skip pooling and recurrent neural networks.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 2874–2883, 2016.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Sean Bell, C Lawrence Zitnick, Kavita Bala 和 Ross Girshick. Inside-outside
    net：使用跳跃池化和递归神经网络在上下文中检测对象。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 2874–2883，2016 年。'
- en: '[40] Spyros Gidaris and Nikos Komodakis. Object detection via a multi-region
    and semantic segmentation-aware cnn model. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 1134–1142, 2015.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Spyros Gidaris 和 Nikos Komodakis. 通过多区域和语义分割感知的 CNN 模型进行物体检测。发表于 IEEE
    国际计算机视觉会议论文集，页码 1134–1142, 2015。'
- en: '[41] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime
    tracking with a deep association metric. In 2017 IEEE International Conference
    on Image Processing (ICIP), pages 3645–3649\. IEEE, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Nicolai Wojke, Alex Bewley, 和 Dietrich Paulus. 使用深度关联度量的简单在线和实时追踪。发表于
    2017 IEEE 国际图像处理会议 (ICIP) 论文集，页码 3645–3649\. IEEE, 2017。'
- en: '[42] Nima Mahmoudi, Seyed Mohammad Ahadi, and Mohammad Rahmati. Multi-target
    tracking using cnn-based features: Cnnmtt. Multimedia Tools and Applications,
    78(6):7077–7096, 2019.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Nima Mahmoudi, Seyed Mohammad Ahadi, 和 Mohammad Rahmati. 使用基于 CNN 的特征进行多目标追踪：CNNMTT。Multimedia
    Tools and Applications, 78(6):7077–7096, 2019。'
- en: '[43] Xingyu Wan, Jinjun Wang, and Sanping Zhou. An online and flexible multi-object
    tracking framework using long short-term memory. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pages 1230–1238, 2018.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Xingyu Wan, Jinjun Wang, 和 Sanping Zhou. 使用长短期记忆的在线灵活多目标追踪框架。发表于 IEEE
    计算机视觉与模式识别会议研讨会论文集，页码 1230–1238, 2018。'
- en: '[44] Takayuki Ujiie, Masayuki Hiromoto, and Takashi Sato. Interpolation-based
    object detection using motion vectors for embedded real-time tracking systems.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    Workshops, pages 616–624, 2018.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Takayuki Ujiie, Masayuki Hiromoto, 和 Takashi Sato. 基于插值的物体检测，使用运动矢量进行嵌入式实时追踪系统。发表于
    IEEE 计算机视觉与模式识别会议研讨会论文集，页码 616–624, 2018。'
- en: '[45] Qizheng He, Jianan Wu, Gang Yu, and Chi Zhang. Sot for mot. arXiv preprint
    arXiv:1712.01059, 2017.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Qizheng He, Jianan Wu, Gang Yu, 和 Chi Zhang. 针对多目标追踪的 SOT。arXiv 预印本 arXiv:1712.01059,
    2017。'
- en: '[46] Minghua Li, Zhengxi Liu, Yunyu Xiong, and Zheng Li. Multi-person tracking
    by discriminative affinity model and hierarchical association. In 2017 3rd IEEE
    International Conference on Computer and Communications (ICCC), pages 1741–1745\.
    IEEE, 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Minghua Li, Zhengxi Liu, Yunyu Xiong, 和 Zheng Li. 通过区分亲和模型和分层关联进行多人追踪。发表于
    2017 第三届 IEEE 国际计算机与通信会议 (ICCC) 论文集，页码 1741–1745\. IEEE, 2017。'
- en: '[47] Wenbo Li, Ming-Ching Chang, and Siwei Lyu. Who did what at where and when:
    Simultaneous multi-person tracking and activity recognition. arXiv preprint arXiv:1807.01253,
    2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Wenbo Li, Ming-Ching Chang, 和 Siwei Lyu. 谁在何处何时做了什么：同时进行多人物追踪和活动识别。arXiv
    预印本 arXiv:1807.01253, 2018。'
- en: '[48] Felipe Jorquera, Sergio Hernández, and Diego Vergara. Probability hypothesis
    density filter using determinantal point processes for multi object tracking.
    Computer Vision and Image Understanding, 2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Felipe Jorquera, Sergio Hernández, 和 Diego Vergara. 使用行列式点过程的概率假设密度滤波器进行多目标追踪。计算机视觉与图像理解,
    2019。'
- en: '[49] Zhao Zhong, Zichen Yang, Weitao Feng, Wei Wu, Yangyang Hu, and Cheng-lin
    Liu. Decision controller for object tracking with deep reinforcement learning.
    IEEE Access, 2019.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Zhao Zhong, Zichen Yang, Weitao Feng, Wei Wu, Yangyang Hu, 和 Cheng-lin
    Liu. 基于深度强化学习的目标追踪决策控制器。IEEE Access, 2019。'
- en: '[50] Weigang Lu, Zhiping Zhou, Lijuan Zhang, and Guoqiang Zheng. Multi-target
    tracking by non-linear motion patterns based on hierarchical network flows. Multimedia
    Systems, pages 1–12, 2019.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Weigang Lu, Zhiping Zhou, Lijuan Zhang, 和 Guoqiang Zheng. 基于层次网络流的非线性运动模式的多目标追踪。多媒体系统,
    页码 1–12, 2019。'
- en: '[51] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, and Bernt Schiele. Multiple
    people tracking by lifted multicut and person re-identification. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3539–3548,
    2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Siyu Tang, Mykhaylo Andriluka, Bjoern Andres, 和 Bernt Schiele. 通过提升的多割集和人物再识别进行多人物追踪。发表于
    IEEE 计算机视觉与模式识别会议论文集，页码 3539–3548, 2017。'
- en: '[52] Nan Ran, Longteng Kong, Yunhong Wang, and Qingjie Liu. A robust multi-athlete
    tracking algorithm by exploiting discriminant features and long-term dependencies.
    In International Conference on Multimedia Modeling, pages 411–423\. Springer,
    2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Nan Ran, Longteng Kong, Yunhong Wang, 和 Qingjie Liu. 通过利用区分特征和长期依赖的鲁棒多运动员追踪算法。发表于国际多媒体建模会议论文集，页码
    411–423\. Springer, 2019。'
- en: '[53] Haigen Hu, Lili Zhou, Qiu Guan, Qianwei Zhou, and Shengyong Chen. An automatic
    tracking method for multiple cells based on multi-feature fusion. IEEE Access,
    6:69782–69793, 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Haigen Hu, Lili Zhou, Qiu Guan, Qianwei Zhou, 和 Shengyong Chen. 一种基于多特征融合的多细胞自动追踪方法。IEEE
    Access, 6:69782–69793, 2018。'
- en: '[54] Lei Zhang, Helen Gray, Xujiong Ye, Lisa Collins, and Nigel Allinson. Automatic
    individual pig detection and tracking in pig farms. Sensors, 19(5):1188, 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Lei Zhang, Helen Gray, Xujiong Ye, Lisa Collins, 和 Nigel Allinson。在猪场自动检测和跟踪个体猪。《传感器》，19(5):1188,
    2019年。'
- en: '[55] Zongwei Zhou, Junliang Xing, Mengdan Zhang, and Weiming Hu. Online multi-target
    tracking with tensor-based high-order graph matching. In 2018 24th International
    Conference on Pattern Recognition (ICPR), pages 1809–1814\. IEEE, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Zongwei Zhou, Junliang Xing, Mengdan Zhang, 和 Weiming Hu。基于张量的高阶图匹配的在线多目标跟踪。在2018年第24届国际模式识别大会（ICPR），页面1809–1814\.
    IEEE, 2018年。'
- en: '[56] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg.
    Eco: efficient convolution operators for tracking. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 6638–6646, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, 和 Michael Felsberg。Eco:
    用于跟踪的高效卷积操作符。在IEEE计算机视觉与模式识别会议论文集，页面6638–6646, 2017年。'
- en: '[57] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human
    detection. In international Conference on computer vision & Pattern Recognition
    (CVPR’05), volume 1, pages 886–893\. IEEE Computer Society, 2005.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Navneet Dalal 和 Bill Triggs。面向人类检测的方向梯度直方图。在计算机视觉与模式识别国际会议（CVPR’05），第1卷，页面886–893\.
    IEEE计算机学会，2005年。'
- en: '[58] Joost Van De Weijer, Cordelia Schmid, Jakob Verbeek, and Diane Larlus.
    Learning color names for real-world applications. IEEE Transactions on Image Processing,
    18(7):1512–1523, 2009.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Joost Van De Weijer, Cordelia Schmid, Jakob Verbeek, 和 Diane Larlus。为实际应用学习颜色名称。《IEEE图像处理汇刊》，18(7):1512–1523,
    2009年。'
- en: '[59] Yongyi Lu, Cewu Lu, and Chi-Keung Tang. Online video object detection
    using association lstm. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 2344–2352, 2017.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Yongyi Lu, Cewu Lu, 和 Chi-Keung Tang。使用关联LSTM的在线视频目标检测。在IEEE计算机视觉国际会议论文集，页面2344–2352,
    2017年。'
- en: '[60] Hilke Kieritz, Wolfgang Hubner, and Michael Arens. Joint detection and
    online multi-object tracking. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition Workshops, pages 1459–1467, 2018.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Hilke Kieritz, Wolfgang Hubner, 和 Michael Arens。联合检测和在线多目标跟踪。在IEEE计算机视觉与模式识别会议论文集，页面1459–1467,
    2018年。'
- en: '[61] Dawei Zhao, Hao Fu, Liang Xiao, Tao Wu, and Bin Dai. Multi-object tracking
    with correlation filter for autonomous vehicle. Sensors, 18(7):2004, 2018.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Dawei Zhao, Hao Fu, Liang Xiao, Tao Wu, 和 Bin Dai。用于自动驾驶车辆的相关滤波多目标跟踪。《传感器》，18(7):2004,
    2018年。'
- en: '[62] Karl Pearson. Liii. on lines and planes of closest fit to systems of points
    in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal
    of Science, 2(11):559–572, 1901.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 卡尔·皮尔逊。Liii. 论空间点系统的最佳拟合直线和平面。《伦敦、爱丁堡和都柏林哲学杂志与科学期刊》，2(11):559–572, 1901年。'
- en: '[63] Mengmeng Wang, Yong Liu, and Zeyi Huang. Large margin object tracking
    with circulant feature maps. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 4021–4029, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Mengmeng Wang, Yong Liu, 和 Zeyi Huang。具有循环特征图的大边距目标跟踪。在IEEE计算机视觉与模式识别会议论文集，页面4021–4029,
    2017年。'
- en: '[64] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only
    look once: Unified, real-time object detection. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 779–788, 2016.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Joseph Redmon, Santosh Divvala, Ross Girshick, 和 Ali Farhadi。你只看一次: 统一的实时目标检测。在IEEE计算机视觉与模式识别会议论文集，页面779–788,
    2016年。'
- en: '[65] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Joseph Redmon 和 Ali Farhadi。Yolov3: 一次增量改进。arXiv预印本 arXiv:1804.02767,
    2018年。'
- en: '[66] Sang Jun Kim, Jae-Yeal Nam, and Byoung Chul Ko. Online tracker optimization
    for multi-pedestrian tracking using a moving vehicle camera. IEEE Access, 6:48675–48687,
    2018.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Sang Jun Kim, Jae-Yeal Nam, 和 Byoung Chul Ko。使用移动车辆摄像头的多行人跟踪在线跟踪优化。《IEEE
    Access》，6:48675–48687, 2018年。'
- en: '[67] Sarthak Sharma, Junaid Ahmed Ansari, J Krishna Murthy, and K Madhava Krishna.
    Beyond pixels: Leveraging geometry and shape cues for online multi-object tracking.
    In 2018 IEEE International Conference on Robotics and Automation (ICRA), pages
    3508–3515\. IEEE, 2018.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Sarthak Sharma, Junaid Ahmed Ansari, J Krishna Murthy, 和 K Madhava Krishna。超越像素:
    利用几何和形状线索进行在线多目标跟踪。在2018年IEEE国际机器人与自动化会议（ICRA），页面3508–3515\. IEEE, 2018年。'
- en: '[68] Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan,
    Yu-Wing Tai, and Li Xu. Accurate single stage detector using recurrent rolling
    convolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition, pages 5420–5428, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Jimmy Ren, Xiaohao Chen, Jianbo Liu, Wenxiu Sun, Jiahao Pang, Qiong Yan,
    Yu-Wing Tai 和 Li Xu. 使用递归卷积的精确单阶段检测器。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 5420–5428，2017
    年。'
- en: '[69] Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Subcategory-aware
    convolutional neural networks for object proposals and detection. In 2017 IEEE
    winter conference on applications of computer vision (WACV), pages 924–933\. IEEE,
    2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Yu Xiang, Wongun Choi, Yuanqing Lin 和 Silvio Savarese. 面向子类别的卷积神经网络用于目标提议和检测。发表于
    2017 IEEE 冬季计算机视觉应用会议（WACV），页码 924–933。IEEE，2017 年。'
- en: '[70] Federico Pernici, Federico Bartoli, Matteo Bruni, and Alberto Del Bimbo.
    Memory based online learning of deep representations from video streams. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2324–2334,
    2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Federico Pernici, Federico Bartoli, Matteo Bruni 和 Alberto Del Bimbo.
    基于记忆的在线学习深度表示从视频流中提取。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 2324–2334，2018 年。'
- en: '[71] Peiyun Hu and Deva Ramanan. Finding tiny faces. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 951–959, 2017.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Peiyun Hu 和 Deva Ramanan. 发现微小的面孔。发表于 IEEE 计算机视觉与模式识别会议论文集，页码 951–959，2017
    年。'
- en: '[72] Weidong Min, Mengdan Fan, Xiaoguang Guo, and Qing Han. A new approach
    to track multiple vehicles with the combination of robust detection and two classifiers.
    IEEE Transactions on Intelligent Transportation Systems, 19(1):174–186, 2018.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Weidong Min, Mengdan Fan, Xiaoguang Guo 和 Qing Han. 一种通过结合鲁棒检测和两个分类器跟踪多辆车辆的新方法。IEEE
    智能交通系统汇刊，19(1):174–186，2018 年。'
- en: '[73] Olivier Barnich and Marc Van Droogenbroeck. Vibe: A universal background
    subtraction algorithm for video sequences. IEEE Transactions on Image processing,
    20(6):1709–1724, 2011.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Olivier Barnich 和 Marc Van Droogenbroeck. Vibe: 一种用于视频序列的通用背景减除算法。IEEE
    图像处理汇刊，20(6):1709–1724，2011 年。'
- en: '[74] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning,
    20(3):273–297, 1995.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Corinna Cortes 和 Vladimir Vapnik. 支持向量网络。机器学习，20(3):273–297，1995 年。'
- en: '[75] Shaoyong Yu, Yun Wu, Wei Li, Zhijun Song, and Wenhua Zeng. A model for
    fine-grained vehicle classification based on deep learning. Neurocomputing, 257:97–103,
    2017.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Shaoyong Yu, Yun Wu, Wei Li, Zhijun Song 和 Wenhua Zeng. 基于深度学习的细粒度车辆分类模型。Neurocomputing，257:97–103，2017
    年。'
- en: '[76] Sebastian Bullinger, Christoph Bodensteiner, and Michael Arens. Instance
    flow based online multiple object tracking. In 2017 IEEE International Conference
    on Image Processing (ICIP), pages 785–789\. IEEE, 2017.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Sebastian Bullinger, Christoph Bodensteiner 和 Michael Arens. 基于实例流的在线多目标跟踪。发表于
    2017 IEEE 国际图像处理会议（ICIP），页码 785–789。IEEE，2017 年。'
- en: '[77] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic segmentation
    via multi-task network cascades. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 3150–3158, 2016.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Jifeng Dai, Kaiming He 和 Jian Sun. 基于多任务网络级联的实例感知语义分割。发表于 IEEE 计算机视觉与模式识别会议论文集，页码
    3150–3158，2016 年。'
- en: '[78] Gunnar Farnebäck. Two-frame motion estimation based on polynomial expansion.
    In Scandinavian conference on Image analysis, pages 363–370. Springer, 2003.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Gunnar Farnebäck. 基于多项式展开的双帧运动估计。发表于斯堪的纳维亚图像分析会议，页码 363–370。Springer，2003
    年。'
- en: '[79] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid.
    Deepmatching: Hierarchical deformable dense matching. International Journal of
    Computer Vision, 120(3):300–323, 2016.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Jerome Revaud, Philippe Weinzaepfel, Zaid Harchaoui 和 Cordelia Schmid.
    Deepmatching: 分层可变形稠密匹配。国际计算机视觉期刊，120(3):300–323，2016 年。'
- en: '[80] Yinlin Hu, Rui Song, and Yunsong Li. Efficient coarse-to-fine patchmatch
    for large displacement optical flow. In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pages 5704–5712, 2016.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Yinlin Hu, Rui Song 和 Yunsong Li. 高效的粗到细 Patchmatch 用于大位移光流。发表于 IEEE 计算机视觉与模式识别会议论文集，页码
    5704–5712，2016 年。'
- en: '[81] Li Wang, Nam Trung Pham, Tian-Tsong Ng, Gang Wang, Kap Luk Chan, and Karianto
    Leman. Learning deep features for multiple object tracking by using a multi-task
    learning strategy. In 2014 IEEE International Conference on Image Processing (ICIP),
    pages 838–842\. IEEE, 2014.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Li Wang, Nam Trung Pham, Tian-Tsong Ng, Gang Wang, Kap Luk Chan 和 Karianto
    Leman. 通过多任务学习策略学习深度特征以进行多目标跟踪。发表于 2014 IEEE 国际图像处理会议（ICIP），页码 838–842。IEEE，2014
    年。'
- en: '[82] Charles Cadieu and Bruno A Olshausen. Learning transformational invariants
    from natural movies. In Advances in neural information processing systems, pages
    209–216, 2009.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Charles Cadieu 和 Bruno A Olshausen. 从自然电影中学习变换不变性。在神经信息处理系统进展，页码209–216，2009年。'
- en: '[83] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M Rehg. Multiple hypothesis
    tracking revisited. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4696–4704, 2015.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Chanho Kim, Fuxin Li, Arridhana Ciptadi 和 James M Rehg. 多假设跟踪的回顾。在IEEE国际计算机视觉大会论文集中，页码4696–4704，2015年。'
- en: '[84] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang,
    and Qi Tian. Person re-identification in the wild. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 1367–1376, 2017.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang
    和 Qi Tian. 野外环境中的行人再识别。在IEEE计算机视觉与模式识别会议论文集中，页码1367–1376，2017年。'
- en: '[85] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.
    Scalable person re-identification: A benchmark. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 1116–1124, 2015.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang 和 Qi Tian.
    可扩展的行人再识别：一个基准。在IEEE国际计算机视觉大会论文集中，页码1116–1124，2015年。'
- en: '[86] Douglas Gray and Hai Tao. Viewpoint invariant pedestrian recognition with
    an ensemble of localized features. In European conference on computer vision,
    pages 262–275. Springer, 2008.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Douglas Gray 和 Hai Tao. 使用局部特征集成的视角不变行人识别。在欧洲计算机视觉大会，页码262–275。Springer，2008年。'
- en: '[87] Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deepreid: Deep filter
    pairing neural network for person re-identification. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pages 152–159, 2014.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Wei Li, Rui Zhao, Tong Xiao 和 Xiaogang Wang. Deepreid: 用于行人再识别的深度滤波配对神经网络。在IEEE计算机视觉与模式识别会议论文集中，页码152–159，2014年。'
- en: '[88] Jiahui Chen, Hao Sheng, Yang Zhang, and Zhang Xiong. Enhancing detection
    model for multiple hypothesis tracking. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pages 18–27, 2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Jiahui Chen, Hao Sheng, Yang Zhang 和 Zhang Xiong. 增强检测模型用于多假设跟踪。在IEEE计算机视觉与模式识别会议论文集中，页码18–27，2017年。'
- en: '[89] Min Yang, Yuwei Wu, and Yunde Jia. A hybrid data association framework
    for robust online multi-object tracking. IEEE Transactions on Image Processing,
    26(12):5667–5679, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Min Yang, Yuwei Wu 和 Yunde Jia. 一种用于鲁棒在线多目标跟踪的混合数据关联框架。IEEE图像处理汇刊，26(12):5667–5679，2017年。'
- en: '[90] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Region-based
    convolutional networks for accurate object detection and segmentation. IEEE transactions
    on pattern analysis and machine intelligence, 38(1):142–158, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Ross Girshick, Jeff Donahue, Trevor Darrell 和 Jitendra Malik. 基于区域的卷积网络用于准确的物体检测和分割。IEEE模式分析与机器智能汇刊，38(1):142–158，2015年。'
- en: '[91] Shuo Hong Wang, Jing Wen Zhao, and Yan Qiu Chen. Robust tracking of fish
    schools using cnn for head identification. Multimedia Tools and Applications,
    76(22):23679–23697, 2017.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Shuo Hong Wang, Jing Wen Zhao 和 Yan Qiu Chen. 使用CNN进行鱼群头部识别的鲁棒跟踪。多媒体工具与应用，76(22):23679–23697，2017年。'
- en: '[92] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
    arXiv:1605.07146, 2016.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Sergey Zagoruyko 和 Nikos Komodakis. 宽残差网络。arXiv预印本 arXiv:1605.07146，2016年。'
- en: '[93] Chanho Kim, Fuxin Li, and James M Rehg. Multi-object tracking with neural
    gating using bilinear lstm. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 200–215, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Chanho Kim, Fuxin Li 和 James M Rehg. 使用双线性LSTM的多目标跟踪与神经门控。在欧洲计算机视觉会议（ECCV）论文集中，页码200–215，2018年。'
- en: '[94] Seung-Hwan Bae and Kuk-Jin Yoon. Confidence-based data association and
    discriminative deep appearance learning for robust online multi-object tracking.
    IEEE transactions on pattern analysis and machine intelligence, 40(3):595–610,
    2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Seung-Hwan Bae 和 Kuk-Jin Yoon. 基于置信度的数据关联与判别性深度外观学习用于鲁棒在线多目标跟踪。IEEE模式分析与机器智能汇刊，40(3):595–610，2017年。'
- en: '[95] Mohib Ullah and Faouzi Alaya Cheikh. Deep feature based end-to-end transportation
    network for multi-target tracking. In 2018 25th IEEE International Conference
    on Image Processing (ICIP), pages 3738–3742\. IEEE, 2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Mohib Ullah 和 Faouzi Alaya Cheikh. 基于深度特征的端到端运输网络用于多目标跟踪。在2018年第25届IEEE国际图像处理大会（ICIP），页码3738–3742。IEEE，2018年。'
- en: '[96] Kuan Fang, Yu Xiang, Xiaocheng Li, and Silvio Savarese. Recurrent autoregressive
    networks for online multi-object tracking. In 2018 IEEE Winter Conference on Applications
    of Computer Vision (WACV), pages 466–475\. IEEE, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Kuan Fang、Yu Xiang、Xiaocheng Li 和 Silvio Savarese。用于在线多目标跟踪的递归自回归网络。在2018年IEEE冬季计算机视觉应用会议（WACV），第466–475页。IEEE，2018年。'
- en: '[97] Tong Xiao, Hongsheng Li, Wanli Ouyang, and Xiaogang Wang. Learning deep
    feature representations with domain guided dropout for person re-identification.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 1249–1258, 2016.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Tong Xiao、Hongsheng Li、Wanli Ouyang 和 Xiaogang Wang。通过领域引导的丢弃学习深度特征表示进行行人重识别。在《IEEE计算机视觉与模式识别会议论文集》，第1249–1258页，2016年。'
- en: '[98] Zeyu Fu, Federico Angelini, Syed Mohsen Naqvi, and Jonathon A Chambers.
    Gm-phd filter based online multiple human tracking using deep discriminative correlation
    matching. In 2018 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP), pages 4299–4303\. IEEE, 2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Zeyu Fu、Federico Angelini、Syed Mohsen Naqvi 和 Jonathon A Chambers。基于Gm-phd滤波器的深度判别相关匹配在线多人人体跟踪。在2018年IEEE国际声学、语音与信号处理会议（ICASSP），第4299–4303页。IEEE，2018年。'
- en: '[99] B-N Vo and W-K Ma. The gaussian mixture probability hypothesis density
    filter. IEEE Transactions on signal processing, 54(11):4091–4104, 2006.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] B-N Vo 和 W-K Ma。高斯混合概率假设密度滤波器。IEEE信号处理汇刊，54(11): 4091–4104，2006年。'
- en: '[100] Longyin Wen, Dawei Du, Shengkun Li, Xiao Bian, and Siwei Lyu. Learning
    non-uniform hypergraph for multi-object tracking. Thirty-Third AAAI Conference
    on Artificial Intelligence, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Longyin Wen、Dawei Du、Shengkun Li、Xiao Bian 和 Siwei Lyu。学习非均匀超图用于多目标跟踪。第三十三届AAAI人工智能会议，2019年。'
- en: '[101] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
    Imagenet large scale visual recognition challenge. International journal of computer
    vision, 115(3):211–252, 2015.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Olga Russakovsky、Jia Deng、Hao Su、Jonathan Krause、Sanjeev Satheesh、Sean
    Ma、Zhiheng Huang、Andrej Karpathy、Aditya Khosla、Michael Bernstein 等。Imagenet大规模视觉识别挑战。国际计算机视觉期刊，115(3):
    211–252，2015年。'
- en: '[102] Flip Korn and Suresh Muthukrishnan. Influence sets based on reverse nearest
    neighbor queries. ACM Sigmod Record, 29(2):201–212, 2000.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Flip Korn 和 Suresh Muthukrishnan。基于逆最近邻查询的影响集合。ACM Sigmod Record，29(2):
    201–212，2000年。'
- en: '[103] Hao Sheng, Yang Zhang, Jiahui Chen, Zhang Xiong, and Jun Zhang. Heterogeneous
    association graph fusion for target association in multiple object tracking. IEEE
    Transactions on Circuits and Systems for Video Technology, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Hao Sheng、Yang Zhang、Jiahui Chen、Zhang Xiong 和 Jun Zhang。用于多目标跟踪的异质关联图融合。IEEE视频技术电路与系统汇刊，2018年。'
- en: '[104] Longtao Chen, Xiaojiang Peng, and Mingwu Ren. Recurrent metric networks
    and batch multiple hypothesis for multi-object tracking. IEEE Access, 7:3093–3105,
    2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Longtao Chen、Xiaojiang Peng 和 Mingwu Ren。递归度量网络和批量多假设用于多目标跟踪。IEEE Access，7:
    3093–3105，2019年。'
- en: '[105] Leonid Pishchulin, Eldar Insafutdinov, Siyu Tang, Bjoern Andres, Mykhaylo
    Andriluka, Peter V Gehler, and Bernt Schiele. Deepcut: Joint subset partition
    and labeling for multi person pose estimation. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 4929–4937, 2016.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Leonid Pishchulin、Eldar Insafutdinov、Siyu Tang、Bjoern Andres、Mykhaylo
    Andriluka、Peter V Gehler 和 Bernt Schiele。Deepcut: 结合子集划分和标记的多人姿态估计。在《IEEE计算机视觉与模式识别会议论文集》，第4929–4937页，2016年。'
- en: '[106] Minyoung Kim, Stefano Alletto, and Luca Rigazio. Similarity mapping with
    enhanced siamese network for multi-object tracking. In Machine Learning for Intelligent
    Transportation Systems (MLITS), 2016 NIPS Workshop, 2016.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Minyoung Kim、Stefano Alletto 和 Luca Rigazio。使用增强的孪生网络进行多目标跟踪的相似性映射。在智能交通系统的机器学习（MLITS），2016年NIPS研讨会，2016年。'
- en: '[107] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak
    Shah. Signature verification using a" siamese" time delay neural network. In Advances
    in neural information processing systems, pages 737–744, 1994.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Jane Bromley、Isabelle Guyon、Yann LeCun、Eduard Säckinger 和 Roopak Shah。使用“孪生”时延神经网络进行签名验证。在《神经信息处理系统进展》，第737–744页，1994年。'
- en: '[108] Bing Wang, Li Wang, Bing Shuai, Zhen Zuo, Ting Liu, Kap Luk Chan, and
    Gang Wang. Joint learning of convolutional neural networks and temporally constrained
    metrics for tracklet association. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition Workshops, pages 1–8, 2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] 冯·王、李 王、冰·帅、镇·左、婷·刘、Kap Luk Chan和刚·王。卷积神经网络与时间约束度量的联合学习用于轨迹关联。IEEE计算机视觉与模式识别会议研讨会论文集,
    页1–8, 2016。'
- en: '[109] Shun Zhang, Yihong Gong, Jia-Bin Huang, Jongwoo Lim, Jinjun Wang, Narendra
    Ahuja, and Ming-Hsuan Yang. Tracking persons-of-interest via adaptive discriminative
    features. In European conference on computer vision, pages 415–433. Springer,
    2016.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] 申·张、易宏·龚、贾-宾·黄、钟武·林、金俊·王、纳伦德拉·阿胡贾和明轩·杨。通过自适应判别特征跟踪感兴趣的人。在欧洲计算机视觉会议，页415–433。Springer,
    2016。'
- en: '[110] Laura Leal-Taixé, Cristian Canton-Ferrer, and Konrad Schindler. Learning
    by tracking: Siamese cnn for robust target association. In Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 33–40,
    2016.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] 劳拉·李尔-泰克、克里斯蒂安·坎顿-费雷尔和孔拉德·辛德勒。通过跟踪学习：用于鲁棒目标关联的Siamese CNN。IEEE计算机视觉与模式识别会议研讨会论文集,
    页33–40, 2016。'
- en: '[111] Laura Leal-Taixé, Gerard Pons-Moll, and Bodo Rosenhahn. Everybody needs
    somebody: Modeling social and grouping behavior on a linear programming multiple
    people tracker. In 2011 IEEE international conference on computer vision workshops
    (ICCV workshops), pages 120–127\. IEEE, 2011.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] 劳拉·李尔-泰克、杰拉德·庞斯-莫尔和博多·罗森汉。每个人都需要有人：在线性规划多人物跟踪器上建模社会和分组行为。在2011年IEEE国际计算机视觉研讨会（ICCV研讨会），页120–127。IEEE,
    2011。'
- en: '[112] Jeany Son, Mooyeol Baek, Minsu Cho, and Bohyung Han. Multi-object tracking
    with quadruplet convolutional neural networks. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 5620–5629, 2017.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Jeany Son、Mooyeol Baek、Minsu Cho和Bohyung Han。多目标跟踪与四元组卷积神经网络。IEEE计算机视觉与模式识别会议论文集,
    页5620–5629, 2017。'
- en: '[113] Andrii Maksai and Pascal Fua. Eliminating exposure bias and loss-evaluation
    mismatch in multiple object tracking. Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] 安德烈·马克赛和帕斯卡尔·富亚。消除多目标跟踪中的曝光偏差和损失评估不匹配。IEEE计算机视觉与模式识别会议论文集, 2019。'
- en: '[114] Alexander Hermans, Lucas Beyer, and Bastian Leibe. In defense of the
    triplet loss for person re-identification. arXiv preprint arXiv:1703.07737, 2017.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] 亚历山大·赫尔曼斯、卢卡斯·贝耶尔和巴斯蒂安·莱贝。为行人再识别辩护三元组损失。arXiv 预印本 arXiv:1703.07737, 2017。'
- en: '[115] Ji Zhu, Hua Yang, Nian Liu, Minyoung Kim, Wenjun Zhang, and Ming-Hsuan
    Yang. Online multi-object tracking with dual matching attention networks. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 366–382, 2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] 吉 朱、华 杨、年 刘、敏永·金、温俊·张和明轩·杨。具有双重匹配注意力网络的在线多目标跟踪。欧洲计算机视觉会议（ECCV）论文集, 页366–382,
    2018。'
- en: '[116] Cong Ma, Changshui Yang, Fan Yang, Yueqing Zhuang, Ziwei Zhang, Huizhu
    Jia, and Xiaodong Xie. Trajectory factory: Tracklet cleaving and re-connection
    by deep siamese bi-gru for multiple object tracking. In 2018 IEEE International
    Conference on Multimedia and Expo (ICME), pages 1–6\. IEEE, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] 孔 马、常水·杨、范·杨、岳青·庄、子维·张、惠珠·贾和肖冬·谢。轨迹工厂：通过深度Siamese Bi-GRU进行轨迹分裂和重连接以实现多目标跟踪。在2018年IEEE国际多媒体与博览会（ICME），页1–6。IEEE,
    2018。'
- en: '[117] Hui Zhou, Wanli Ouyang, Jian Cheng, Xiaogang Wang, and Hongsheng Li.
    Deep continuous conditional random fields with asymmetric inter-object constraints
    for online multi-object tracking. IEEE Transactions on Circuits and Systems for
    Video Technology, 2018.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] 惠 周、万里·欧阳、简·程、肖刚·王和洪生·李。具有非对称对象间约束的深度连续条件随机场用于在线多目标跟踪。IEEE 视频技术电路与系统汇刊,
    2018。'
- en: '[118] Chen Long, Ai Haizhou, Zhuang Zijie, and Shang Chong. Real-time multiple
    people tracking with deeply learned candidate selection and person re-identification.
    In ICME, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] 陈·龙、艾·海洲、庄·紫杰和尚·冲。通过深度学习的候选选择和行人再识别实现实时多人物跟踪。在ICME, 2018。'
- en: '[119] Sangyun Lee and Euntai Kim. Multiple object tracking via feature pyramid
    siamese networks. IEEE Access, 7:8181–8194, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] 常勋·李和恩泰·金。通过特征金字塔Siamese网络实现多目标跟踪。IEEE Access, 7:8181–8194, 2019。'
- en: '[120] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J
    Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters
    and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William
    J Dally, 和 Kurt Keutzer. SqueezeNet：与 AlexNet 相当的准确度，参数减少 50 倍，模型大小 < 0.5 MB.
    arXiv 预印本 arXiv:1602.07360, 2016.'
- en: '[121] Mohib Ullah, Ahmed Kedir Mohammed, Faouzi Alaya Cheikh, and Zhaohui Wang.
    A hierarchical feature model for multi-target tracking. In 2017 IEEE International
    Conference on Image Processing (ICIP), pages 2612–2616\. IEEE, 2017.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Mohib Ullah, Ahmed Kedir Mohammed, Faouzi Alaya Cheikh, 和 Zhaohui Wang.
    一种用于多目标跟踪的层次特征模型. 在 2017 IEEE 国际图像处理会议 (ICIP), 页码 2612–2616\. IEEE, 2017.'
- en: '[122] Stéphane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency
    dictionaries. IEEE Transactions on signal processing, 41(12):3397–3415, 1993.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Stéphane G Mallat 和 Zhifeng Zhang. 时间频率字典的匹配追踪. IEEE 信号处理汇刊, 41(12):3397–3415,
    1993.'
- en: '[123] Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Tracking the untrackable:
    Learning to track multiple cues with long-term dependencies. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 300–311, 2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Amir Sadeghian, Alexandre Alahi, 和 Silvio Savarese. 跟踪不可跟踪的目标：学习跟踪具有长期依赖性的多个线索.
    发表在 IEEE 国际计算机视觉大会论文集, 页码 300–311, 2017.'
- en: '[124] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai
    Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal
    attention mechanism. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 4836–4845, 2017.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, 和 Nenghai
    Yu. 基于 CNN 的单目标跟踪器与时空注意力机制的在线多目标跟踪. 发表在 IEEE 国际计算机视觉大会论文集, 页码 4836–4845, 2017.'
- en: '[125] Mustafa Ozuysal, Michael Calonder, Vincent Lepetit, and Pascal Fua. Fast
    keypoint recognition using random ferns. IEEE transactions on pattern analysis
    and machine intelligence, 32(3):448–461, 2009.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Mustafa Ozuysal, Michael Calonder, Vincent Lepetit, 和 Pascal Fua. 使用随机蕨类植物进行快速关键点识别.
    IEEE 模式分析与机器智能汇刊, 32(3):448–461, 2009.'
- en: '[126] Mohib Ullah and Faouzi Alaya Cheikh. A directed sparse graphical model
    for multi-target tracking. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition Workshops, pages 1816–1823, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Mohib Ullah 和 Faouzi Alaya Cheikh. 一种用于多目标跟踪的定向稀疏图模型. 发表在 IEEE 计算机视觉与模式识别会议研讨会论文集,
    页码 1816–1823, 2018.'
- en: '[127] Lawrence R Rabiner and Biing-Hwang Juang. An introduction to hidden markov
    models. ieee assp magazine, 3(1):4–16, 1986.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Lawrence R Rabiner 和 Biing-Hwang Juang. 隐马尔可夫模型简介. IEEE ASSP 杂志, 3(1):4–16,
    1986.'
- en: '[128] Lu Wang, Lisheng Xu, Min Young Kim, Luca Rigazico, and Ming-Hsuan Yang.
    Online multiple object tracking via flow and convolutional features. In 2017 IEEE
    International Conference on Image Processing (ICIP), pages 3630–3634\. IEEE, 2017.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Lu Wang, Lisheng Xu, Min Young Kim, Luca Rigazico, 和 Ming-Hsuan Yang.
    通过流和卷积特征进行在线多目标跟踪. 在 2017 IEEE 国际图像处理会议 (ICIP), 页码 3630–3634\. IEEE, 2017.'
- en: '[129] Chao Ma, Jia-Bin Huang, Xiaokang Yang, and Ming-Hsuan Yang. Hierarchical
    convolutional features for visual tracking. In Proceedings of the IEEE international
    conference on computer vision, pages 3074–3082, 2015.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Chao Ma, Jia-Bin Huang, Xiaokang Yang, 和 Ming-Hsuan Yang. 视觉跟踪的层次卷积特征.
    发表在 IEEE 国际计算机视觉会议论文集, 页码 3074–3082, 2015.'
- en: '[130] Bruce D. Lucas and Takeo Kanade. An iterative image registration technique
    with an application to stereo vision. In Proceedings of Imaging Understanding
    Workshop, pages 121–130\. Vancouver, British Columbia, 1981.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Bruce D. Lucas 和 Takeo Kanade. 一种迭代图像配准技术及其在立体视觉中的应用. 发表在成像理解研讨会论文集,
    页码 121–130\. 温哥华, 不列颠哥伦比亚省, 1981.'
- en: '[131] Pol Rosello and Mykel J Kochenderfer. Multi-agent reinforcement learning
    for multi-object tracking. In Proceedings of the 17th International Conference
    on Autonomous Agents and MultiAgent Systems, pages 1397–1404\. International Foundation
    for Autonomous Agents and Multiagent Systems, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Pol Rosello 和 Mykel J Kochenderfer. 多智能体强化学习用于多目标跟踪. 发表在第 17 届国际自主代理与多智能体系统会议论文集,
    页码 1397–1404\. 国际自主代理与多智能体系统基金会, 2018.'
- en: '[132] Maryam Babaee, Zimu Li, and Gerhard Rigoll. Occlusion handling in tracking
    multiple people using rnn. In 2018 25th IEEE International Conference on Image
    Processing (ICIP), pages 2715–2719\. IEEE, 2018.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Maryam Babaee, Zimu Li, 和 Gerhard Rigoll. 使用 RNN 处理多人物跟踪中的遮挡. 在 2018
    第 25 届 IEEE 国际图像处理会议 (ICIP), 页码 2715–2719\. IEEE, 2018.'
- en: '[133] Anton Milan, S Hamid Rezatofighi, Anthony Dick, Ian Reid, and Konrad
    Schindler. Online multi-target tracking using recurrent neural networks. In Thirty-First
    AAAI Conference on Artificial Intelligence, 2017.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Anton Milan, S Hamid Rezatofighi, Anthony Dick, Ian Reid 和 Konrad Schindler。使用递归神经网络进行在线多目标跟踪。在第三十一届AAAI人工智能会议，2017年。'
- en: '[134] Yu Xiang, Alexandre Alahi, and Silvio Savarese. Learning to track: Online
    multi-object tracking by decision making. In Proceedings of the IEEE international
    conference on computer vision, pages 4705–4713, 2015.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Yu Xiang, Alexandre Alahi 和 Silvio Savarese。学习跟踪：通过决策进行在线多目标跟踪。在IEEE国际计算机视觉会议论文集，页码4705–4713，2015年。'
- en: '[135] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. Rmpe: Regional multi-person
    pose estimation. In Proceedings of the IEEE International Conference on Computer
    Vision, pages 2334–2343, 2017.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai 和 Cewu Lu。Rmpe：区域性多人姿态估计。在IEEE国际计算机视觉会议论文集，页码2334–2343，2017年。'
- en: '[136] Yiming Liang and Yue Zhou. Lstm multiple object tracker combining multiple
    cues. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages
    2351–2355\. IEEE, 2018.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Yiming Liang 和 Yue Zhou。结合多种线索的LSTM多目标跟踪器。在2018年第25届IEEE国际图像处理会议（ICIP），页码2351–2355。IEEE，2018年。'
- en: '[137] Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin, Yubing Li, Yihong
    Gong, and Nanning Zheng. Deep self-paced learning for person re-identification.
    Pattern Recognition, 76:739–751, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin, Yubing Li, Yihong
    Gong 和 Nanning Zheng。用于人员重新识别的深度自适应学习。《模式识别》，76:739–751，2018年。'
- en: '[138] Kwangjin Yoon, Du Yong Kim, Young-Chul Yoon, and Moongu Jeon. Data association
    for multi-object tracking via deep neural networks. Sensors, 19(3):559, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Kwangjin Yoon, Du Yong Kim, Young-Chul Yoon 和 Moongu Jeon。通过深度神经网络进行多目标跟踪的数据关联。《传感器》，19(3):559，2019年。'
- en: '[139] Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese.
    Learning social etiquette: Human trajectory understanding in crowded scenes. In
    European conference on computer vision, pages 549–565. Springer, 2016.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi 和 Silvio Savarese。学习社交礼仪：在人群拥挤场景中的人类轨迹理解。在欧洲计算机视觉会议，页码549–565。施普林格，2016年。'
- en: '[140] Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele. People-tracking-by-detection
    and people-detection-by-tracking. In 2008 IEEE Conference on computer vision and
    pattern recognition, pages 1–8\. IEEE, 2008.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Mykhaylo Andriluka, Stefan Roth 和 Bernt Schiele。基于检测的人员跟踪和基于跟踪的人员检测。在2008年IEEE计算机视觉与模式识别会议，页码1–8。IEEE，2008年。'
- en: '[141] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio.
    On the properties of neural machine translation: Encoder-decoder approaches. arXiv
    preprint arXiv:1409.1259, 2014.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau 和 Yoshua Bengio。神经机器翻译的属性：编码器-解码器方法。arXiv预印本
    arXiv:1409.1259，2014年。'
- en: '[142] Bjoern Andres, Andrea Fuksová, and Jan-Hendrik Lange. Lifting of multicuts.
    CoRR, abs/1503.03791, 3, 2015.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Bjoern Andres, Andrea Fuksová 和 Jan-Hendrik Lange。多割的提升。CoRR，abs/1503.03791，3，2015年。'
- en: '[143] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui, and Cordelia Schmid.
    Deepflow: Large displacement optical flow with deep matching. In Proceedings of
    the IEEE International Conference on Computer Vision, pages 1385–1392, 2013.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Philippe Weinzaepfel, Jerome Revaud, Zaid Harchaoui 和 Cordelia Schmid。Deepflow：具有深度匹配的大位移光流。在IEEE国际计算机视觉会议论文集，页码1385–1392，2013年。'
- en: '[144] Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas
    Brox, and Bjorn Andres. Efficient decomposition of image and mesh graphs by lifted
    multicuts. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 1751–1759, 2015.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Margret Keuper, Evgeny Levinkov, Nicolas Bonneel, Guillaume Lavoué, Thomas
    Brox 和 Bjorn Andres。通过提升的多割高效分解图像和网格图。在IEEE国际计算机视觉会议论文集，页码1751–1759，2015年。'
- en: '[145] Long Chen, Haizhou Ai, Chong Shang, Zijie Zhuang, and Bo Bai. Online
    multi-object tracking with convolutional neural networks. In 2017 IEEE International
    Conference on Image Processing (ICIP), pages 645–649\. IEEE, 2017.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Long Chen, Haizhou Ai, Chong Shang, Zijie Zhuang 和 Bo Bai。使用卷积神经网络进行在线多目标跟踪。在2017年IEEE国际图像处理会议（ICIP），页码645–649。IEEE，2017年。'
- en: '[146] M Sanjeev Arulampalam, Simon Maskell, Neil Gordon, and Tim Clapp. A tutorial
    on particle filters for online nonlinear/non-gaussian bayesian tracking. IEEE
    Transactions on signal processing, 50(2):174–188, 2002.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] M Sanjeev Arulampalam, Simon Maskell, Neil Gordon 和 Tim Clapp。关于在线非线性/非高斯贝叶斯跟踪的粒子滤波教程。《IEEE信号处理学报》，50(2):174–188，2002年。'
- en: '[147] Ricardo Sanchez-Matilla, Fabio Poiesi, and Andrea Cavallaro. Online multi-target
    tracking with strong and weak detections. In European Conference on Computer Vision,
    pages 84–99. Springer, 2016.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Ricardo Sanchez-Matilla、Fabio Poiesi 和 Andrea Cavallaro. 在线多目标跟踪中的强检测和弱检测。在欧洲计算机视觉会议，第84–99页。Springer，2016年。'
- en: '[148] Liqian Ma, Siyu Tang, Michael J. Black, and Luc Van Gool. Customized
    multi-person tracker. In Computer Vision – ACCV 2018. Springer International Publishing,
    December 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Liqian Ma、Siyu Tang、Michael J. Black 和 Luc Van Gool. 定制化的多人员跟踪器。在计算机视觉
    - ACCV 2018。Springer国际出版，2018年12月。'
- en: '[149] Siyu Tang, Bjoern Andres, Mykhaylo Andriluka, and Bernt Schiele. Multi-person
    tracking by multicut and deep matching. In European Conference on Computer Vision,
    pages 100–111. Springer, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Siyu Tang、Bjoern Andres、Mykhaylo Andriluka 和 Bernt Schiele. 通过多切割和深度匹配进行多人的跟踪。在欧洲计算机视觉会议，第100–111页。Springer，2016年。'
- en: '[150] Liangliang Ren, Jiwen Lu, Zifeng Wang, Qi Tian, and Jie Zhou. Collaborative
    deep reinforcement learning for multi-object tracking. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 586–602, 2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Liangliang Ren、Jiwen Lu、Zifeng Wang、Qi Tian 和 Jie Zhou. 协作深度强化学习用于多目标跟踪。在欧洲计算机视觉会议（ECCV）论文集，第586–602页，2018年。'
- en: '[151] Hyeonseob Nam and Bohyung Han. Learning multi-domain convolutional neural
    networks for visual tracking. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition, pages 4293–4302, 2016.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Hyeonseob Nam 和 Bohyung Han. 学习多域卷积神经网络用于视觉跟踪。在IEEE计算机视觉与模式识别会议论文集，第4293–4302页，2016年。'
- en: '[152] Yifan Jiang, Hyunhak Shin, and Hanseok Ko. Precise regression for bounding
    box correction for improved tracking based on deep reinforcement learning. In
    2018 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), pages 1643–1647\. IEEE, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Yifan Jiang、Hyunhak Shin 和 Hanseok Ko. 基于深度强化学习的精确回归用于边界框校正以改进跟踪。2018
    IEEE国际声学、语音和信号处理会议（ICASSP），第1643–1647页。IEEE，2018年。'
- en: '[153] Byungjae Lee, Enkhbayar Erdenee, Songguo Jin, Mi Young Nam, Young Giu
    Jung, and Phill Kyu Rhee. Multi-class multi-object tracking using changing point
    detection. In European Conference on Computer Vision, pages 68–83. Springer, 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Byungjae Lee、Enkhbayar Erdenee、Songguo Jin、Mi Young Nam、Young Giu Jung
    和 Phill Kyu Rhee. 使用变化点检测的多类多目标跟踪。在欧洲计算机视觉会议，第68–83页。Springer，2016年。'
- en: '[154] Anthony Hoak, Henry Medeiros, and Richard Povinelli. Image-based multi-target
    tracking through multi-bernoulli filtering with interactive likelihoods. Sensors,
    17(3):501, 2017.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Anthony Hoak、Henry Medeiros 和 Richard Povinelli. 通过多伯努利滤波和交互式似然进行基于图像的多目标跟踪。传感器，17(3):501，2017年。'
- en: '[155] Roberto Henschel, Laura Leal-Taixé, Daniel Cremers, and Bodo Rosenhahn.
    Fusion of head and full-body detectors for multi-object tracking. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages
    1428–1437, 2018.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Roberto Henschel、Laura Leal-Taixé、Daniel Cremers 和 Bodo Rosenhahn. 头部和全身检测器融合用于多目标跟踪。在IEEE计算机视觉与模式识别会议研讨会论文集，第1428–1437页，2018年。'
- en: '[156] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people
    detection in crowded scenes. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2325–2333, 2016.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Russell Stewart、Mykhaylo Andriluka 和 Andrew Y Ng. 在拥挤场景中的端到端人检测。在IEEE计算机视觉与模式识别会议论文集，第2325–2333页，2016年。'
- en: '[157] Weihao Gan, Shuo Wang, Xuejing Lei, Ming-Sui Lee, and C-C Jay Kuo. Online
    cnn-based multiple object tracking with enhanced model updates and identity association.
    Signal Processing: Image Communication, 66:95–102, 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Weihao Gan、Shuo Wang、Xuejing Lei、Ming-Sui Lee 和 C-C Jay Kuo. 基于CNN的在线多目标跟踪，增强模型更新和身份关联。信号处理：图像通信，66:95–102，2018年。'
- en: '[158] Jun Xiang, Guoshuai Zhang, and Jianhua Hou. Online multi-object tracking
    based on feature representation and bayesian filtering within a deep learning
    architecture. IEEE Access, 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Jun Xiang、Guoshuai Zhang 和 Jianhua Hou. 基于特征表示和贝叶斯滤波的在线多目标跟踪，在深度学习架构中实现。IEEE
    Access，2019年。'
- en: '[159] Peng Chu, Heng Fan, Chiu C Tan, and Haibin Ling. Online multi-object
    tracking with instance-aware tracker and dynamic model refreshment. In 2019 IEEE
    Winter Conference on Applications of Computer Vision (WACV), pages 161–170\. IEEE,
    2019.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Peng Chu、Heng Fan、Chiu C Tan 和 Haibin Ling. 带实例感知跟踪器和动态模型刷新功能的在线多目标跟踪。在2019
    IEEE冬季计算机视觉应用会议（WACV），第161–170页。IEEE，2019年。'
- en: '[160] Christopher John Cornish Hellaby Watkins. Learning from delayed rewards.
    PhD thesis, King’s College, Cambridge, 1989.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Christopher John Cornish Hellaby Watkins. 从延迟奖励中学习。博士论文，剑桥大学国王学院，1989年。'
- en: '[161] Jun-ichi Takeuchi and Kenji Yamanishi. A unifying framework for detecting
    outliers and change points from time series. IEEE transactions on Knowledge and
    Data Engineering, 18(4):482–492, 2006.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Jun-ichi Takeuchi 和 Kenji Yamanishi. 一个统一的框架用于从时间序列中检测异常值和变化点。IEEE 知识与数据工程学报，18(4):482–492，2006
    年。'
- en: '[162] Piotr Dollar, Christian Wojek, Bernt Schiele, and Pietro Perona. Pedestrian
    detection: A benchmark. In 2009 IEEE Conference on Computer Vision and Pattern
    Recognition, pages 304–311\. IEEE, 2009.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Piotr Dollar, Christian Wojek, Bernt Schiele, 和 Pietro Perona. 行人检测：一个基准。发表于
    2009 IEEE 计算机视觉与模式识别会议论文集，第 304–311 页。IEEE，2009 年。'
- en: '[163] Reza Hoseinnezhad, Ba-Ngu Vo, Ba-Tuong Vo, and David Suter. Visual tracking
    of numerous targets via multi-bernoulli filtering of image data. Pattern Recognition,
    45(10):3625–3635, 2012.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Reza Hoseinnezhad, Ba-Ngu Vo, Ba-Tuong Vo, 和 David Suter. 通过多伯努利滤波图像数据进行大量目标的视觉跟踪。《模式识别》，45(10):3625–3635，2012
    年。'
- en: '[164] Anton Milan, Rikke Gade, Anthony Dick, Thomas B Moeslund, and Ian Reid.
    Improving global multi-target tracking with local updates. In European Conference
    on Computer Vision, pages 174–190. Springer, 2014.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Anton Milan, Rikke Gade, Anthony Dick, Thomas B Moeslund, 和 Ian Reid.
    通过局部更新改善全局多目标跟踪。发表于欧洲计算机视觉会议论文集，第 174–190 页。Springer，2014 年。'
- en: '[165] Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming.
    Naval research logistics quarterly, 3(1-2):95–110, 1956.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Marguerite Frank 和 Philip Wolfe. 二次规划算法。《海军研究物流季刊》，3(1-2):95–110，1956
    年。'
- en: '[166] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person
    2d pose estimation using part affinity fields. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 7291–7299, 2017.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Zhe Cao, Tomas Simon, Shih-En Wei, 和 Yaser Sheikh. 使用部分亲和场的实时多人人体 2D
    姿态估计。发表于 IEEE 计算机视觉与模式识别会议论文集，第 7291–7299 页，2017 年。'
- en: '[167] Liming Zhao, Xi Li, Yueting Zhuang, and Jingdong Wang. Deeply-learned
    part-aligned representations for person re-identification. In Proceedings of the
    IEEE International Conference on Computer Vision, pages 3219–3228, 2017.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Liming Zhao, Xi Li, Yueting Zhuang, 和 Jingdong Wang. 深度学习的部分对齐表示用于行人重新识别。发表于
    IEEE 国际计算机视觉会议论文集，第 3219–3228 页，2017 年。'
- en: '[168] João F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed
    tracking with kernelized correlation filters. IEEE transactions on pattern analysis
    and machine intelligence, 37(3):583–596, 2014.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] João F Henriques, Rui Caseiro, Pedro Martins, 和 Jorge Batista. 使用内核化相关滤波器进行高速跟踪。IEEE
    模式分析与机器智能学报，37(3):583–596，2014 年。'
- en: '[169] Longyin Wen, Wenbo Li, Junjie Yan, Zhen Lei, Dong Yi, and Stan Z Li.
    Multiple target tracking based on undirected hierarchical relation hypergraph.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 1282–1289, 2014.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Longyin Wen, Wenbo Li, Junjie Yan, Zhen Lei, Dong Yi, 和 Stan Z Li. 基于无向分层关系超图的多目标跟踪。发表于
    IEEE 计算机视觉与模式识别会议论文集，第 1282–1289 页，2014 年。'
- en: '[170] Zhi-Ming Qian, Xi En Cheng, and Yan Qiu Chen. Automatically detect and
    track multiple fish swimming in shallow water with frequent occlusion. PloS one,
    9(9):e106506, 2014.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Zhi-Ming Qian, Xi En Cheng, 和 Yan Qiu Chen. 自动检测和跟踪在浅水中游动的多条鱼并处理频繁遮挡。《PloS
    one》，9(9):e106506，2014 年。'
- en: '[171] Hamed Pirsiavash, Deva Ramanan, and Charless C Fowlkes. Globally-optimal
    greedy algorithms for tracking a variable number of objects. In CVPR 2011, pages
    1201–1208\. IEEE, 2011.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Hamed Pirsiavash, Deva Ramanan, 和 Charless C Fowlkes. 针对变数量目标的全局最优贪婪算法。发表于
    CVPR 2011 会议论文集，第 1201–1208 页。IEEE，2011 年。'
- en: '[172] Steven Gold, Anand Rangarajan, et al. Softmax to softassign: Neural network
    algorithms for combinatorial optimization. Journal of Artificial Neural Networks,
    2(4):381–399, 1996.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Steven Gold, Anand Rangarajan 等. 从 Softmax 到 Softassign：用于组合优化的神经网络算法。《人工神经网络杂志》，2(4):381–399，1996
    年。'
- en: '[173] Chang Huang, Bo Wu, and Ramakant Nevatia. Robust object tracking by hierarchical
    association of detection responses. In European Conference on Computer Vision,
    pages 788–801. Springer, 2008.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Chang Huang, Bo Wu, 和 Ramakant Nevatia. 通过检测响应的分层关联进行稳健的目标跟踪。发表于欧洲计算机视觉会议论文集，第
    788–801 页。Springer，2008 年。'
- en: '[174] Rodrigo Benenson, Markus Mathias, Radu Timofte, and Luc Van Gool. Pedestrian
    detection at 100 frames per second. In 2012 IEEE Conference on Computer Vision
    and Pattern Recognition, pages 2903–2910\. IEEE, 2012.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Rodrigo Benenson, Markus Mathias, Radu Timofte, 和 Luc Van Gool. 每秒 100
    帧的行人检测。发表于 2012 IEEE 计算机视觉与模式识别会议论文集，第 2903–2910 页。IEEE，2012 年。'
- en: Appendix A Appendix
  id: totrans-566
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: Here we present a table containing a summary of the techniques used by each
    algorithm presented in this paper. The table follows the order of presentation
    of the papers. Since we think that the publication of open source code can greatly
    help the research community, we have also provided links to the source codes for
    the papers that provide them.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在这里，我们展示了一个包含每种算法使用的技术总结的表格。该表格按照论文的展示顺序排列。由于我们认为开源代码的发布可以极大地帮助研究社区，因此我们还提供了提供源代码的论文的链接。'
- en: 'Table 8: Information summary about the methods commented in section [3](#S3
    "3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object Tracking: A Survey").
    In each column, the approach for each paper in that step is shown. app. means
    appearance, mot. means motion, feat. means features, pred. means prediction; O
    and B in the Mode column indicate Online and Batch methods respectively. Text
    in the last column is clickable and contains links to the specified data.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '-   表8：有关第[3](#S3 "3 Deep learning in MOT ‣ Deep Learning in Video Multi-Object
    Tracking: A Survey")节中评论方法的信息摘要。在每一列中，显示了该步骤中每篇论文的方法。app. 表示外观，mot. 表示运动，feat.
    表示特征，pred. 表示预测；模式列中的O和B分别表示在线和批处理方法。最后一列的文本是可点击的，包含指向指定数据的链接。'
- en: '|  | Detection | Feature extr. / mot. pred. | Affinity / cost computation |
    Association / Tracking | Mode | Source and data |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|  | 检测 | 特征提取/运动预测 | 亲和性/成本计算 | 关联/跟踪 | 模式 | 来源和数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| [[35](#bib.bib35)] | Faster R-CNN | Kalman filter | IoU | Hungarian algorithm
    | O | [Source](https://github.com/abewley/sort) |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| [[35](#bib.bib35)] | Faster R-CNN | 卡尔曼滤波器 | IoU | 匈牙利算法 | O | [源代码](https://github.com/abewley/sort)
    |'
- en: '| [[38](#bib.bib38)] | Modified Faster R-CNN | Modified GoogLeNet, Kalman filter
    | Cosine distance + IoU | Hungarian algorithm (online), modified H2T [[169](#bib.bib169)]
    (batch) | O+B | [Detections and appearance features](https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view)
    |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| [[38](#bib.bib38)] | 修改版Faster R-CNN | 修改版GoogLeNet，卡尔曼滤波器 | 余弦距离 + IoU |
    匈牙利算法（在线），修改版H2T [[169](#bib.bib169)]（批处理） | O+B | [检测和外观特征](https://drive.google.com/file/d/0B5ACiy41McAHMjczS2p0dFg3emM/view)
    |'
- en: '| [[52](#bib.bib52)] | Faster R-CNN | CNN (app.), AlphaPose CNN, pose joints
    velocities, interaction grid | Pose-based Triple Stream Network (LSTM-based) |
    Custom algorithm | O |  |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| [[52](#bib.bib52)] | Faster R-CNN | CNN（应用），AlphaPose CNN，姿态关节速度，交互网格 | 基于姿态的三重流网络（基于LSTM）
    | 自定义算法 | O |  |'
- en: '| [[53](#bib.bib53)] | Faster R-CNN | CNN | Euclidean distance, cosine distance
    | Multifeature fusion re-tracking algorithm | B |  |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| [[53](#bib.bib53)] | Faster R-CNN | CNN | 欧几里得距离，余弦距离 | 多特征融合重新跟踪算法 | B |  |'
- en: '| [[54](#bib.bib54)] | CNN | HOG + Colour Names | Variation of Discriminative
    Correlation Filter | Custom algorithm + Hungarian algorithm | O |  |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| [[54](#bib.bib54)] | CNN | HOG + 颜色名称 | 判别相关滤波器的变体 | 自定义算法 + 匈牙利算法 | O |  |'
- en: '| [[59](#bib.bib59)] | SSD | SSD, LSTM | Cosine similarity | Hungarian algorithm
    | O |  |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] | SSD | SSD，LSTM | 余弦相似度 | 匈牙利算法 | O |  |'
- en: '| [[60](#bib.bib60)] | SSD | SSD | RNN | Hungarian algorithm, MLP (track scores)
    | O |  |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] | SSD | SSD | RNN | 匈牙利算法，MLP（跟踪分数） | O |  |'
- en: '| [[61](#bib.bib61)] | SSD | SSD + Correlation Filter | IoU + APCE | Hungarian
    algorithm | O |  |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| [[61](#bib.bib61)] | SSD | SSD + 相关滤波器 | IoU + APCE | 匈牙利算法 | O |  |'
- en: '| [[55](#bib.bib55)] | Public / Mask R-CNN | Siamese Mask R-CNN | App. affinity,
    mot. consistency, spatial structural potential | Tensor-based high-order graph
    matching | O | Code will be released |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| [[55](#bib.bib55)] | Public / Mask R-CNN | Siamese Mask R-CNN | 应用亲和性，运动一致性，空间结构潜力
    | 基于张量的高阶图匹配 | O | 代码将被发布 |'
- en: '| [[66](#bib.bib66)] | YOLOv2 | Tiny Yolo, Particle filter, Random Ferns, KLT
    | Pairwise overlap ratio, student Random Ferns, Euclidean distance | Greedy bipartite
    assignment | O |  |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] | YOLOv2 | Tiny Yolo，粒子滤波器，随机蕨，KLT | 成对重叠比例，学生随机蕨，欧几里得距离
    | 贪婪二分分配 | O |  |'
- en: '| [[67](#bib.bib67)] | RRC or SubCNN | Feature-based odometry, Pose Adjustment
    CNN, stacked-hourglass CNN | 3D-2D cost + 3D-3D cost + appearance, shape and pose
    costs | Hungarian algorithm | O | [Source](https://github.com/JunaidCS032/MOTBeyondPixels)
    |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| [[67](#bib.bib67)] | RRC或SubCNN | 基于特征的里程计，姿态调整CNN，堆叠小时玻璃CNN | 3D-2D成本 +
    3D-3D成本 + 外观、形状和姿态成本 | 匈牙利算法 | O | [源代码](https://github.com/JunaidCS032/MOTBeyondPixels)
    |'
- en: '| [[70](#bib.bib70)] | DPM or Tiny (CNN) | DPM or Tiny (CNN) | Implicit in
    Reverse Nearest Neighbour | Reverse Nearest Neighbour Matching | O | Code will
    be released |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| [[70](#bib.bib70)] | DPM或Tiny（CNN） | DPM或Tiny（CNN） | 反向最近邻隐式 | 反向最近邻匹配 |
    O | 代码将被发布 |'
- en: '| [[72](#bib.bib72)] | ViBe + SVM + CNN |  | IoU | Region Matching algorithm
    | O |  |'
  id: totrans-583
  prefs: []
  type: TYPE_TB
  zh: '| [[72](#bib.bib72)] | ViBe + SVM + CNN |  | IoU | 区域匹配算法 | O |  |'
- en: '| [[76](#bib.bib76)] | Multi-task Network Cascades (CNN) | Optical flow | Overlap
    of segmentation instances | Hungarian algorithm | O |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '| [[76](#bib.bib76)] | 多任务网络级联（CNN） | 光流 | 分割实例的重叠 | 匈牙利算法 | O |  |'
- en: '| [[81](#bib.bib81)] | Dalal-Triggs detector | Autoencoders | SVM | Minimum
    spanning tree | O |  |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '| [[81](#bib.bib81)] | Dalal-Triggs 检测器 | 自编码器 | SVM | 最小生成树 | O |  |'
- en: '| [[83](#bib.bib83)] | Public | CNN + PCA | Multi-Output Regularized Least
    Squares | Variation of Multiple Hypothesis Tracking | O | [Source](http://rehg.org/mht/)
    |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| [[83](#bib.bib83)] | 公共 | CNN + PCA | 多输出正则化最小二乘 | 多重假设跟踪的变体 | O | [源](http://rehg.org/mht/)
    |'
- en: '| [[88](#bib.bib88)] | Public | CNN, Kalman Filter | Multi-Output Regularized
    Least Squares + Kalman Filter + detection-scene score | Maximum Weighted Independent
    Set | B |  |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| [[88](#bib.bib88)] | 公共 | CNN, 卡尔曼滤波器 | 多输出正则化最小二乘 + 卡尔曼滤波器 + 检测场景分数 | 最大加权独立集
    | B |  |'
- en: '| [[89](#bib.bib89)] | Public | R-CNN | Observation cost + transition cost
    + birth-death cost | Min-cost multi commodity flow problem, solved with Dantzig-Wolfe
    decomposition | O |  |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] | 公共 | R-CNN | 观察成本 + 转换成本 + 出生-死亡成本 | 最小成本多商品流问题，使用 Dantzig-Wolfe
    分解解决 | O |  |'
- en: '| [[91](#bib.bib91)] | DoH [[170](#bib.bib170)] | CNN | CNN + Kalman filter
    | Custom algorithm, SVM | B |  |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] | DoH [[170](#bib.bib170)] | CNN | CNN + 卡尔曼滤波器 | 自定义算法,
    SVM | B |  |'
- en: '| [[41](#bib.bib41)] | From [[38](#bib.bib38)] | Kalman filter, Wide Residual
    Net | Mahalanobis dist. (mot.) + cosine distance (app.), IoU | Hungarian algorithm
    | O | [Source](https://github.com/nwojke/deep_sort) |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| [[41](#bib.bib41)] | 来源于 [[38](#bib.bib38)] | 卡尔曼滤波器, 宽残差网络 | 马哈拉诺比斯距离（运动）+
    余弦距离（应用），IoU | 匈牙利算法 | O | [源](https://github.com/nwojke/deep_sort) |'
- en: '| [[42](#bib.bib42)] | From [[38](#bib.bib38)] | CNN | appearance + motion
    + dynamic affinity | Hungarian algorithm | O |  |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| [[42](#bib.bib42)] | 来源于 [[38](#bib.bib38)] | CNN | 外观 + 运动 + 动态亲和力 | 匈牙利算法
    | O |  |'
- en: '| [[93](#bib.bib93)] | Public | CNN | Bilinear LSTM | Variant of MHT-DAM [[83](#bib.bib83)]
    | B |  |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| [[93](#bib.bib93)] | 公共 | CNN | 双线性 LSTM | MHT-DAM [[83](#bib.bib83)] 的变体
    | B |  |'
- en: '| [[94](#bib.bib94)] | Public / SDP+RPN | CNN | Appearance + motion + shape
    affinities | Hungarian algorithm | O | [Source](https://cvl.gist.ac.kr/project/cmot.html)
    |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] | 公共 / SDP+RPN | CNN | 外观 + 运动 + 形状亲和力 | 匈牙利算法 | O | [源](https://cvl.gist.ac.kr/project/cmot.html)
    |'
- en: '| [[95](#bib.bib95)] | Public | GoogLeNet CNN | App. similarity | Bayesian
    inference using [[171](#bib.bib171)] | B |  |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| [[95](#bib.bib95)] | 公共 | GoogLeNet CNN | 应用相似性 | 使用 [[171](#bib.bib171)]
    的贝叶斯推断 | B |  |'
- en: '| [[96](#bib.bib96)] | Public / Faster R-CNN | GoogLeNet CNN | Recurrent Autoregressive
    Networks (GRU-based) | Bipartite graph matching | O |  |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| [[96](#bib.bib96)] | 公共 / Faster R-CNN | GoogLeNet CNN | 循环自回归网络（基于 GRU）
    | 二分图匹配 | O |  |'
- en: '| [[98](#bib.bib98)] | Public | CNN | Hybrid Likelihood Function (Discriminative
    Correlation Filter + Gaussian Mixture Probability Hypothesis Density) | Hungarian
    algorithm | O |  |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| [[98](#bib.bib98)] | 公共 | CNN | 混合似然函数（判别相关滤波器 + 高斯混合概率假设密度） | 匈牙利算法 | O
    |  |'
- en: '| [[100](#bib.bib100)] | Public | CNN | app. + HSV histogram + motion similarities
    | Pairwise update algorithm + SSVM | B | Will be available at [this link](https://github.com/longyin880815)
    |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| [[100](#bib.bib100)] | 公共 | CNN | 应用 + HSV 直方图 + 运动相似性 | 成对更新算法 + SSVM |
    B | 将在 [此链接](https://github.com/longyin880815) 提供 |'
- en: '| [[103](#bib.bib103)] | Public | GoogLeNet CNN, Optical flow | Distance between
    app. features, common superpixels, optical flow predictions | Multiple Hypotheses
    Tracking | B |  |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| [[103](#bib.bib103)] | 公共 | GoogLeNet CNN, 光流 | 应用特征之间的距离, 共同超像素, 光流预测 |
    多重假设跟踪 | B |  |'
- en: '| [[104](#bib.bib104)] | Public | CNN | LSTM (app.) + motion affinity | Batch
    Multi-Hypothesis | B |  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| [[104](#bib.bib104)] | 公共 | CNN | LSTM（应用） + 运动亲和力 | 批量多重假设 | B |  |'
- en: '| [[51](#bib.bib51)] | Public / From [[38](#bib.bib38)] | DeepCut CNN [[105](#bib.bib105)],
    StackNetPose CNN | StackNetPose CNN | Lifted multicut problem, solved as in [[144](#bib.bib144)]
    | B | [Source](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/people-detection-pose-estimation-and-tracking/multiple-people-tracking-with-lifted-multicut-and-person-re-identification/)
    |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| [[51](#bib.bib51)] | 公共 / 来源于 [[38](#bib.bib38)] | DeepCut CNN [[105](#bib.bib105)],
    StackNetPose CNN | StackNetPose CNN | 提升的多切割问题，按 [[144](#bib.bib144)] 中的方法解决 |
    B | [源](https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/people-detection-pose-estimation-and-tracking/multiple-people-tracking-with-lifted-multicut-and-person-re-identification/)
    |'
- en: '| [[106](#bib.bib106)] | Public | Siamese CNN | Euclidean distance (app. feat.)
    + IoU + box area ratio | Custom greedy algorithm | O |  |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| [[106](#bib.bib106)] | 公共 | 孪生CNN | 欧几里得距离（应用特征）+ IoU + 边界框面积比 | 自定义贪婪算法
    | O |  |'
- en: '| [[108](#bib.bib108)] | DPM | Siamese CNN with temporal constraints | Mahalanobis
    distance (app. feat.) + motion affinity | Generalized Linear Assignment solved
    with Softassign [[172](#bib.bib172)], Dual-threshold strategy [[173](#bib.bib173)]
    | B |  |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| [[108](#bib.bib108)] | DPM | 带时间约束的孪生CNN | 马哈拉诺比斯距离（应用特征）+ 运动亲和性 | 使用Softassign
    [[172](#bib.bib172)] 解决的广义线性分配问题，双阈值策略 [[173](#bib.bib173)] | B |  |'
- en: '| [[109](#bib.bib109)] | HeadHunter [[174](#bib.bib174)] | CNN | Euclidean
    distance (app. feat.), temporal and kinematic affinities | Hungarian algorithm,
    Agglomerative clustering | B | [Source](https://github.com/shunzhang876/AdaptiveFeatureLearning)
    |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] | HeadHunter [[174](#bib.bib174)] | CNN | 欧几里得距离（应用特征），时间和运动亲和性
    | 匈牙利算法，凝聚性聚类 | B | [来源](https://github.com/shunzhang876/AdaptiveFeatureLearning)
    |'
- en: '| [[110](#bib.bib110)] | Public | Siamese CNN, contextual features | Gradient
    Boosting | Linear programming | B |  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] | 公共 | 孪生CNN，上下文特征 | 梯度提升 | 线性规划 | B |  |'
- en: '| [[112](#bib.bib112)] | Public | CNN, sequence-specific statistics, optical
    flow, FC layers | FC layer combining app. and mot. distances | Minimax label propagation
    | B |  |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| [[112](#bib.bib112)] | 公共 | CNN，序列特定统计，光流，FC层 | FC层结合应用和运动距离 | 最小最大标签传播 |
    B |  |'
- en: '| [[113](#bib.bib113)] | Public | CNN + various app. and non-app. feat. | embedding
    layer + bidirectional LSTM | Variation of Multiple Hypothesis Tracking | B |  |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| [[113](#bib.bib113)] | 公共 | CNN + 各种应用和非应用特征 | 嵌入层 + 双向LSTM | 多假设跟踪的变体 |
    B |  |'
- en: '| [[115](#bib.bib115)] | Public | Linear motion model, Spatial Attention Network
    CNN | Temporal Attention Network (bidirectional LSTM) | Custom greedy algorithm,
    ECO (SOT tracker) | O | [Source](https://github.com/jizhu1023/DMAN_MOT) |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| [[115](#bib.bib115)] | 公共 | 线性运动模型，空间注意力网络CNN | 时间注意力网络（双向LSTM） | 自定义贪婪算法，ECO（SOT跟踪器）
    | O | [来源](https://github.com/jizhu1023/DMAN_MOT) |'
- en: '| [[116](#bib.bib116)] | Public | Siamese CNN, LSTM, WRN CNN, Siamese Bi-GRU
    + CNN | Euclidean dist. (app. feat.), spatial distance, GRU feature matching |
    Hungarian algorithm, bi-GRU RNN (track split), custom algorithm | B |  |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| [[116](#bib.bib116)] | 公共 | 孪生CNN，LSTM，WRN CNN，孪生Bi-GRU + CNN | 欧几里得距离（应用特征），空间距离，GRU特征匹配
    | 匈牙利算法，bi-GRU RNN（跟踪分裂），自定义算法 | B |  |'
- en: '| [[117](#bib.bib117)] | Public | DCCRF, visual-displacement CNN | Visual-similarity
    CNN, IoU | Hungarian algorithm | O |  |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| [[117](#bib.bib117)] | 公共 | DCCRF，视觉位移CNN | 视觉相似性CNN，IoU | 匈牙利算法 | O |  |'
- en: '| [[118](#bib.bib118)] | Public | R-FCN + Kalman Filter, GoogleNet | Eucl.
    dist. (app. feat.), IoU | Hierarchical Data Association | O | [Source](https://github.com/longcw/MOTDT)
    |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| [[118](#bib.bib118)] | 公共 | R-FCN + 卡尔曼滤波器，GoogleNet | 欧几里得距离（应用特征），IoU |
    层次数据关联 | O | [来源](https://github.com/longcw/MOTDT) |'
- en: '| [[119](#bib.bib119)] | Public | Feature Pyramid Siamese Network, motion features
    | Feature Pyramid Siamese Network | Custom greedy algorithm | O |  |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| [[119](#bib.bib119)] | 公共 | 特征金字塔孪生网络，运动特征 | 特征金字塔孪生网络 | 自定义贪婪算法 | O |  |'
- en: '| [[121](#bib.bib121)] | Public | Kalman Filter, GoogLeNet | Distance between
    sparse coding of features using a learned dictionary | Hungarian algorithm | B
    |  |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| [[121](#bib.bib121)] | 公共 | 卡尔曼滤波器，GoogLeNet | 使用学习字典的特征稀疏编码之间的距离 | 匈牙利算法
    | B |  |'
- en: '| [[123](#bib.bib123)] | Public | 3 LSTMs (app., mot., interaction features)
    using CNN, bb velocity, occupancy map | LSTM | Hungarian algorithm, SOT tracker
    [[134](#bib.bib134)] | O |  |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| [[123](#bib.bib123)] | 公共 | 使用CNN的3个LSTMs（应用、运动、互动特征），边界框速度，占用图 | LSTM |
    匈牙利算法，SOT跟踪器 [[134](#bib.bib134)] | O |  |'
- en: '| [[124](#bib.bib124)] | Public | Linear motion model, CNN | CNN | Association
    to highest classification score | O |  |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| [[124](#bib.bib124)] | 公共 | 线性运动模型，CNN | CNN | 关联到最高分类得分 | O |  |'
- en: '| [[126](#bib.bib126)] | Manually generated | Hidden Markov Models, CNN | Mutual
    information (app. feat.) | Dynamic programming algorithm from [[171](#bib.bib171)]
    | B |  |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| [[126](#bib.bib126)] | 手动生成 | 隐马尔可夫模型，CNN | 互信息（应用特征） | 来源于[[171](#bib.bib171)]的动态规划算法
    | B |  |'
- en: '| [[128](#bib.bib128)] | Public | LK optical flow, Convolutional Correlation
    Filter CNN, Kalman filter | Optical flow aff., app. feat. aff., IoU, scale affinity,
    distance between detections | Custom algorithm (with Hungarian alg.) | O | [Source](http://faculty.neu.edu.cn/ise/wanglu/CCF_MOT.htm)
    |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| [[128](#bib.bib128)] | 公共 | LK光流，卷积相关滤波器CNN，卡尔曼滤波器 | 光流附加，应用特征附加，IoU，尺度亲和性，检测之间的距离
    | 自定义算法（带匈牙利算法） | O | [来源](http://faculty.neu.edu.cn/ise/wanglu/CCF_MOT.htm) |'
- en: '| [[131](#bib.bib131)] | Public | Kalman filter + Deep RL agent | IoU | Hungarian
    algorithm + Deep RL agent | O |  |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| [[131](#bib.bib131)] | 公共 | 卡尔曼滤波器 + 深度 RL 代理 | IoU | 匈牙利算法 + 深度 RL 代理 |
    O |  |'
- en: '| [[132](#bib.bib132)] | N/A | LSTM (mot.) | Stitching score using IoU | Custom
    iterative tracklet-stitching algorithm | B |  |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| [[132](#bib.bib132)] | N/A | LSTM（运动） | 使用 IoU 的拼接得分 | 自定义迭代轨迹拼接算法 | B |  |'
- en: '| [[133](#bib.bib133)] | Public | RNN (mot.) | LSTM | RNN | O | [Source](https://bitbucket.org/amilan/rnntracking)
    |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| [[133](#bib.bib133)] | 公共 | RNN（运动） | LSTM | RNN | O | [来源](https://bitbucket.org/amilan/rnntracking)
    |'
- en: '| [[136](#bib.bib136)] | Public | 2 LSTMs, VGG16 CNN | SVM, Siamese LSTM |
    Greedy association | B |  |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] | 公共 | 2 LSTMs, VGG16 CNN | SVM, Siamese LSTM | 贪婪关联
    | B |  |'
- en: '| [[43](#bib.bib43)] | From [[38](#bib.bib38)] | Kalman filter or LK optical
    flow, CNN + motion features | IoU, Siamese LSTM | Hungarian algorithm | B |  |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| [[43](#bib.bib43)] | 来自[[38](#bib.bib38)] | 卡尔曼滤波器或 LK 光流，CNN + 运动特征 | IoU，Siamese
    LSTM | 匈牙利算法 | B |  |'
- en: '| [[138](#bib.bib138)] | Public |  | FC layers + Bi-directional LSTM | Hungarian
    algorithm | O |  |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| [[138](#bib.bib138)] | 公共 |  | FC 层 + 双向 LSTM | 匈牙利算法 | O |  |'
- en: '| [[145](#bib.bib145)] | Public / from [[147](#bib.bib147)] (combines DPM,
    SDP and ACF) | Modified Faster R-CNN | Modified Faster R-CNN | Particle filter
    | O |  |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] | 公共 / 来自[[147](#bib.bib147)]（结合 DPM, SDP 和 ACF） | 修改版
    Faster R-CNN | 修改版 Faster R-CNN | 粒子滤波器 | O |  |'
- en: '| [[148](#bib.bib148)] | Public | DeepMatching, Siamese CNN | Edge potential
    as in [[149](#bib.bib149)], Siamese CNN | Lifted multicut | B |  |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] | 公共 | DeepMatching, Siamese CNN | 边缘潜力如[[149](#bib.bib149)]，Siamese
    CNN | 提升的多割 | B |  |'
- en: '| [[150](#bib.bib150)] | Public | CNN (motion pred.), part of MDNet (CNN) |
    N/A | Deep RL agents | O |  |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| [[150](#bib.bib150)] | 公共 | CNN（运动预测），MDNet（CNN）的一个部分 | N/A | 深度 RL 代理 |
    O |  |'
