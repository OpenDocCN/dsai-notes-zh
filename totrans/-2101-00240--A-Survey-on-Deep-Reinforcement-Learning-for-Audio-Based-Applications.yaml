- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:57:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:57:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2101.00240] A Survey on Deep Reinforcement Learning for Audio-Based Applications'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2101.00240] 在音频基础应用中的深度强化学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.00240](https://ar5iv.labs.arxiv.org/html/2101.00240)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2101.00240](https://ar5iv.labs.arxiv.org/html/2101.00240)
- en: A Survey on Deep Reinforcement Learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习
- en: for Audio-Based Applications
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频基础应用中的调查
- en: 'Siddique Latif Email: siddique.latif@usq.edu.au University of Southern Queensland,
    Australia Heriberto Cuayáhuitl University of Lincoln, United Kingdom Farrukh Pervez
    National University of Science and Technology, Pakistan Fahad Shamshad Information
    Technology University, Pakistan Hafiz Shehbaz Ali EmulationAI Erik Cambria Nanyang
    Technological University, Singapore'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Siddique Latif 邮箱：siddique.latif@usq.edu.au 昆士兰大学，澳大利亚 Heriberto Cuayáhuitl
    林肯大学，英国 Farrukh Pervez 国家科学与技术大学，巴基斯坦 Fahad Shamshad 信息技术大学，巴基斯坦 Hafiz Shehbaz
    Ali EmulationAI Erik Cambria 南洋理工大学，新加坡
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial
    intelligence (AI) by endowing autonomous systems with high levels of understanding
    of the real world. Currently, deep learning (DL) is enabling DRL to effectively
    solve various intractable problems in various fields. Most importantly, DRL algorithms
    are also being employed in audio signal processing to learn directly from speech,
    music and other sound signals in order to create audio-based autonomous systems
    that have many promising application in the real world. In this article, we conduct
    a comprehensive survey on the progress of DRL in the audio domain by bringing
    together the research studies across different speech and music related areas.
    We begin with an introduction to the general field of DL and reinforcement learning
    (RL), then progress to the main DRL methods and their applications in the audio
    domain. We conclude by presenting challenges faced by audio-based DRL agents and
    highlighting open areas for future research and investigation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）有望通过赋予自主系统对现实世界的高水平理解，彻底改变人工智能（AI）领域。目前，深度学习（DL）使得DRL能够有效解决各个领域中的各种棘手问题。最重要的是，DRL算法也被应用于音频信号处理，以便直接从语音、音乐和其他声音信号中学习，从而创建具有许多实际应用前景的音频基础自主系统。在本文中，我们通过汇集不同语音和音乐相关领域的研究，全面调查DRL在音频领域的进展。我们首先介绍DL和强化学习（RL）的一般领域，然后探讨主要的DRL方法及其在音频领域的应用。最后，我们介绍了音频基础DRL代理面临的挑战，并突出了未来研究和调查的开放领域。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep learning, reinforcement learning, speech recognition, emotion recognition,
    (embodied) dialogue systems
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，强化学习，语音识别，情感识别，（具身的）对话系统
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Artificial intelligence (AI) has gained widespread attention in many areas of
    life, especially in audio signal processing. Audio processing covers many diverse
    fields including speech, music and environmental sound processing. In all these
    areas, AI techniques are playing crucial roles in the design of audio-based intelligent
    systems [[1](#bib.bib1)]. One of the prime goals of AI is to create fully autonomous
    audio-based intelligent systems or agents that can learn optimal behaviours by
    listening or interacting with their environments and improving their behaviour
    over time through trial and error. Designing of such autonomous systems has been
    a long-standing problem, ranging from robots that can react to the changes in
    their environment, to purely software-based agents that can interact with humans
    using natural language and multimedia. Reinforcement learning (RL) [[2](#bib.bib2)]
    represents a principled mathematical framework of such experience-driven learning.
    Although RL had some successes in the past [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)],
    however, previous methods were inherently limited to low-dimensional problems
    due to lack of scalability. Moreover, RL also has issues of memory, computational
    and sample complexity—in the case of learning algorithms [[6](#bib.bib6)]. Recently,
    deep learning (DL) models have risen as new tools with powerful function approximation
    and representation learning properties to solve these issues.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）在生活的许多领域获得了广泛关注，尤其是在音频信号处理方面。音频处理涵盖了许多不同的领域，包括语音、音乐和环境声音处理。在所有这些领域中，AI技术在音频智能系统的设计中发挥了关键作用[[1](#bib.bib1)]。AI的主要目标之一是创建完全自主的基于音频的智能系统或代理，这些系统可以通过听取或与环境互动来学习最佳行为，并通过试错过程随着时间的推移改善其行为。设计这样的自主系统一直是一个长期存在的问题，从能够对环境变化做出反应的机器人，到能够使用自然语言和多媒体与人类互动的纯软件代理。强化学习（RL）[[2](#bib.bib2)]代表了这种经验驱动学习的原则性数学框架。尽管RL过去有一些成功[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]，但由于缺乏可扩展性，先前的方法本质上限于低维问题。此外，RL在学习算法的情况下也面临记忆、计算和样本复杂性的问题[[6](#bib.bib6)]。最近，深度学习（DL）模型作为具有强大函数逼近和表示学习特性的工具，崛起以解决这些问题。
- en: The advent of DL has had a significant impact on many areas of machine learning
    (ML) by dramatically improving the state-of-the-art in image processing tasks
    such as object detection and image classification. Deep models such as deep neural
    networks (DNNs) [[7](#bib.bib7), [8](#bib.bib8)], convolutional neural networks
    (CNNs) [[9](#bib.bib9)], and long short-term memory (LSTM) networks [[10](#bib.bib10)]
    have also enabled many practical applications by outperforming traditional methods
    in audio signal processing. Given that DL has also accelerated RL’s progress with
    the use of DL algorithms within RL, it has given rise to the field of deep reinforcement
    learning (DRL).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）的出现对机器学习（ML）许多领域产生了重大影响，通过显著提升图像处理任务中的最新技术，如物体检测和图像分类。深度模型，如深度神经网络（DNNs）[[7](#bib.bib7),
    [8](#bib.bib8)]、卷积神经网络（CNNs）[[9](#bib.bib9)] 和长短期记忆（LSTM）网络[[10](#bib.bib10)]，也通过在音频信号处理中的优于传统方法，推动了许多实际应用。考虑到DL还通过在强化学习（RL）中使用DL算法加速了RL的发展，它催生了深度强化学习（DRL）领域。
- en: DRL embraces the advancements in DL to establish the learning processes, performance
    and speed of RL algorithms. This enables RL to operate in high-dimensional state
    and action spaces to solve complex problems that were previously difficult to
    solve. As a result, DRL has been adopted to solve many problems. Inspired by previous
    works such as [[11](#bib.bib11)], two outstanding works kick-started the revolution
    in DRL. The first was the development of an algorithm that could learn to play
    Atari 2600 video games directly from image pixels at a superhuman level [[12](#bib.bib12)].
    The second success was design of the hybrid DRL system, AlphaGo, which defeated
    a human world champion in the game of Go [[13](#bib.bib13)]. In addition to playing
    games, DRL has also been applied to a wide range of problems including robotics
    to control policies [[14](#bib.bib14)]; generalisable agents in complex environments
    with meta-learning [[15](#bib.bib15), [16](#bib.bib16)]; indoor navigation [[17](#bib.bib17)],
    and many more [[18](#bib.bib18)]. In particular, DRL is also gaining increased
    interest in audio signal processing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）利用深度学习（DL）的进步来建立学习过程、性能和强化学习（RL）算法的速度。这使得 RL 能够在高维状态和动作空间中操作，解决以前难以解决的复杂问题。因此，DRL
    已被应用于许多问题。受到之前工作的启发 [[11](#bib.bib11)]，两项杰出的工作开启了 DRL 的革命。第一个是开发了一种可以直接从图像像素中学习玩
    Atari 2600 视频游戏的算法，达到了超人类水平 [[12](#bib.bib12)]。第二项成功是设计了混合 DRL 系统 AlphaGo，击败了围棋世界冠军
    [[13](#bib.bib13)]。除了游戏外，DRL 还被应用于广泛的问题，包括控制政策的机器人 [[14](#bib.bib14)]；在具有元学习的复杂环境中可泛化的智能体
    [[15](#bib.bib15), [16](#bib.bib16)]；室内导航 [[17](#bib.bib17)]，以及更多 [[18](#bib.bib18)]。特别是，DRL
    在音频信号处理方面也越来越受到关注。
- en: 'TABLE I: Comparison of our paper with that of the existing surveys.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 我们的论文与现有调查的比较。'
- en: '|  | Focus |  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | 重点 |  |'
- en: '| --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Reference |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 参考 |'
- en: '&#124; Deep &#124;'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度 &#124;'
- en: '&#124; Reinforcement &#124;'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 强化学习 &#124;'
- en: '&#124; Learning &#124;'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习 &#124;'
- en: '|'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Audio &#124;'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 音频 &#124;'
- en: '&#124; Applications &#124;'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '|'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Other &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 其他 &#124;'
- en: '&#124; Applications &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 应用 &#124;'
- en: '| Details |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 详细信息 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Arulkumaran et al. [[18](#bib.bib18)] &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Arulkumaran 等人 [[18](#bib.bib18)] &#124;'
- en: '&#124; 2017 &#124;'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2017 &#124;'
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; This paper presents a brief overview of recent developments in &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文简要概述了近期 DRL 的发展 &#124;'
- en: '&#124; DRL algorithms and highlights the benefits of DRL and several &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DRL 算法，并突出了 DRL 的优势及多个 &#124;'
- en: '&#124; current areas of research. &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 当前的研究领域。 &#124;'
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Yuxi Li [[19](#bib.bib19)] &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Yuxi Li [[19](#bib.bib19)] &#124;'
- en: '&#124; 2017 &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2017 &#124;'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; This paper presents a generalised overview of recent exciting &#124;'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文提供了对近期令人兴奋的 &#124;'
- en: '&#124; achievements of DRL and discus core elements and mechanisms. &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 讨论了深度强化学习（DRL）的成就及其核心要素和机制。 &#124;'
- en: '&#124; It also discusses various fields where DRL can be applied. &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 还讨论了 DRL 可以应用的各个领域。 &#124;'
- en: '|'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Luong et al. [[20](#bib.bib20)] &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Luong 等人 [[20](#bib.bib20)] &#124;'
- en: '&#124; 2019 &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2019 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; communications &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通信 &#124;'
- en: '&#124; and networking &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和网络 &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; This paper presents a comprehensive literature review on the &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文对 &#124;'
- en: '&#124; applications of DRL in communications and networking, highlights &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DRL 在通信和网络中的应用，突出显示 &#124;'
- en: '&#124; challenges, and discusses open issues and future directions. &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 挑战，并讨论了开放问题和未来的方向。 &#124;'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Kiran et al. [[21](#bib.bib21)] &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Kiran 等人 [[21](#bib.bib21)] &#124;'
- en: '&#124; 2020 &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2020 &#124;'
- en: '|'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; autonomous &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化 &#124;'
- en: '&#124; driving &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 驾驶 &#124;'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; This paper summarises DRL algorithms and autonomous driving, &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文总结了 DRL 算法和自动驾驶， &#124;'
- en: '&#124; where (D)RL methods have been employed.It also highlights key &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (D)RL 方法的应用情况。还强调了关键的 &#124;'
- en: '&#124; challenges towards real-world deployment of autonomous cars. &#124;'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 针对自动驾驶汽车在现实世界部署中的挑战。 &#124;'
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Haydari et al. [[22](#bib.bib22)] &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Haydari 等人 [[22](#bib.bib22)] &#124;'
- en: '&#124; 2020 &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2020 &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; transportation &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 交通 &#124;'
- en: '&#124; systems &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统 &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; This paper summarises existing works in the field of transportation,
    &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文总结了交通领域的现有工作， &#124;'
- en: '&#124; and discusses the challenges and open questions regarding DRL &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 并讨论了关于DRL的挑战和未解问题 &#124;'
- en: '&#124; in transportation systems. &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在交通系统中。 &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ours (2020) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (2020) |'
- en: '&#124; ✓ &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✓ &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✓ &#124;'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ✗ &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ✗ &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; We present a comprehensive review focused on DRL applications &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们提供了一份综合性综述，专注于DRL应用 &#124;'
- en: '&#124; in the audio domain, highlight existing challenges that hinder the &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在音频领域，强调了现有挑战，这些挑战阻碍了 &#124;'
- en: '&#124; progress of DRL in audio, and discuss pointers for future research.
    &#124;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DRL在音频领域的进展，并讨论了未来研究的指引。 &#124;'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In audio processing, DRL has been recently used as an emerging tool to address
    various problems and challenges in automatic speech recognition (ASR), spoken
    dialogue systems (SDSs), speech emotion recognition (SER), audio enhancement,
    music generation, and audio-driven controlled robotics. In this work, we therefore
    focus on covering the advancements in audio processing by DRL. Although there
    are multiple survey articles on DRL. For instance, Arulkumaran et al. [[18](#bib.bib18)]
    presented a brief survey on DRL by covering seminal and recent developments in
    DRL—including innovative ways in which DNNs can be used to develop autonomous
    agents. Similarly, in [[19](#bib.bib19)], authors attempted to provide comprehensive
    details on DRL and cover its applications in various areas to highlight advances
    and challenges. Other relevant works include applications of DRL in communications
    and networking [[20](#bib.bib20)], human-level agents [[23](#bib.bib23)], and
    autonomous driving [[24](#bib.bib24)]. None of these articles has focused on DRL
    applications in audio processing as highlighted in Table [I](#S1.T1 "TABLE I ‣
    I Introduction ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    This paper aims to fill this gap by presenting an up-to-date literature review
    on DRL studies in the audio domain, discussing challenges that hinder the progress
    of DRL in audio, and pointing out future research areas. We hope this paper will
    help researchers and scientists interested in DRL for audio-driven applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频处理领域，DRL最近被作为一种新兴工具，用于解决自动语音识别（ASR）、语音对话系统（SDSs）、语音情感识别（SER）、音频增强、音乐生成和音频驱动的控制机器人等问题和挑战。因此，在这项工作中，我们专注于覆盖DRL在音频处理方面的进展。尽管已有多篇关于DRL的综述文章。例如，Arulkumaran
    等人[[18](#bib.bib18)] 提供了关于DRL的简要综述，涵盖了DRL的开创性和近期发展，包括DNNs在开发自主智能体中的创新方式。类似地，在[[19](#bib.bib19)]中，作者试图提供关于DRL的全面细节，并涵盖其在各个领域的应用，以突出进展和挑战。其他相关工作包括DRL在通信和网络[[20](#bib.bib20)]、人类水平智能体[[23](#bib.bib23)]以及自主驾驶[[24](#bib.bib24)]中的应用。这些文章中没有一篇专注于DRL在音频处理中的应用，如表[I](#S1.T1
    "TABLE I ‣ I Introduction ‣ A Survey on Deep Reinforcement Learning for Audio-Based
    Applications")所示。本文旨在填补这一空白，通过提供最新的DRL音频领域文献综述，讨论阻碍DRL在音频领域进展的挑战，并指出未来的研究方向。我们希望这篇文章能帮助对音频驱动应用的DRL感兴趣的研究人员和科学家。
- en: This paper is organised as follows. A concise background of DL and RL is provided
    in Section [II](#S2 "II Background ‣ A Survey on Deep Reinforcement Learning for
    Audio-Based Applications"), followed by an overview of recent DRL algorithms in
    Section [III](#S3 "III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications"). With those foundations, Section [IV](#S4
    "IV Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based
    Applications") covers recent DRL works in domains such as speech, music, and environmental
    sound processing; and their challenges are discussed in Section [V](#S5 "V Challenges
    in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    Section [VI](#S6 "VI Summary and Future Pointers ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications") summaries this review and highlights the
    future pointers for audio-based DRL research and Section [VII](#S7 "VII Conclusions
    ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") concludes
    the paper.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的组织结构如下。第二节[II](#S2 "II Background ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications")提供了深度学习（DL）和强化学习（RL）的简要背景，接着在第三节[III](#S3 "III Deep
    Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning for Audio-Based
    Applications")概述了近期的深度强化学习（DRL）算法。以这些基础为基础，第四节[IV](#S4 "IV Audio-Based DRL ‣ A
    Survey on Deep Reinforcement Learning for Audio-Based Applications")涵盖了在语音、音乐和环境声音处理等领域中的近期DRL工作，并在第五节[V](#S5
    "V Challenges in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for
    Audio-Based Applications")讨论了其挑战。第六节[VI](#S6 "VI Summary and Future Pointers ‣
    A Survey on Deep Reinforcement Learning for Audio-Based Applications")总结了这篇综述并突出音频基础的DRL研究的未来方向，第七节[VII](#S7
    "VII Conclusions ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications")对本文进行了总结。
- en: II Background
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A Deep Learning (DL)
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 深度学习（DL）
- en: DNNs have been shown to produce state-of-the-art results in audio and speech
    processing due to their ability to distil compact and robust representations from
    large amounts of data. The first major milestone was significantly increasing
    the accuracy of large-scale automatic speech recognition based on the use of fully
    connected DNNs and deep autoencoders around 2010 [[7](#bib.bib7)]. It focuses
    on the use of artificial neural networks, which consists of multiple nonlinear
    modules arranged hierarchically in layers to automatically discover suitable representations
    or features from raw data for specific tasks. These non-linearities allow DNNs
    to learn complicated manifolds in speech and audio datasets. Below we discuss
    different DL architectures, which are illustrated in Figure [1](#S2.F1 "Figure
    1 ‣ II-A Deep Learning (DL) ‣ II Background ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications").
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）由于其从大量数据中提炼出紧凑且稳健的表示的能力，已在音频和语音处理领域展示了最先进的结果。第一个重要的里程碑是在2010年左右显著提高了大规模自动语音识别的准确性，这主要归功于使用了全连接的深度神经网络和深度自编码器[[7](#bib.bib7)]。它专注于使用人工神经网络，这些网络由多个非线性模块按层次结构排列，能够自动从原始数据中发现适用于特定任务的表示或特征。这些非线性特性使得深度神经网络能够学习语音和音频数据集中的复杂流形。下面我们将讨论不同的深度学习（DL）架构，如图[1](#S2.F1
    "Figure 1 ‣ II-A Deep Learning (DL) ‣ II Background ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications")所示。
- en: 'Convolutional neural networks (CNNs) are a kind of feedforward neural networks
    that have been specifically designed for processing data having grid-like topologies
    such as images [[25](#bib.bib25)]. Recently, CNN’s have shown state-of-the-art
    performance in various image processing tasks, including segmentation, detection,
    and classification, among others [[26](#bib.bib26)]. In contrast to DNNs, CNNs
    limit the number of parameters and memory requirements dramatically by leveraging
    on two key concepts: local receptive fields and shared weights. They often consist
    of a series of convolutional layers interleaved with pooling layers, followed
    by one or more dense layers. For sequence labelling, the dense layers can be omitted
    to obtain a fully-convolutional network (FCN). FCNs have been extended with domain
    adaptation for increased robustness [[27](#bib.bib27)]. Recently, CNN models have
    been extensively studied for a variety of audio processing tasks including music
    onset detection [[28](#bib.bib28)], speech enhancement [[29](#bib.bib29)], ASR [[30](#bib.bib30)],
    etc. However, raw audio waveform with high sample rates might have problems with
    limited receptive fields of CNNs, which can result in deteriorated performance.
    To handle this performance issue, dilated convolution layers can be used in order
    to extend the receptive field by inserting zeros between their filter coefficients [[31](#bib.bib31),
    [32](#bib.bib32)].'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）是一种前馈神经网络，专门设计用于处理具有网格状拓扑的数据，如图像[[25](#bib.bib25)]。最近，CNN在各种图像处理任务中表现出了最先进的性能，包括分割、检测和分类等[[26](#bib.bib26)]。与深度神经网络（DNNs）相比，CNN通过利用两个关键概念：局部感受野和共享权重，显著限制了参数和内存需求。它们通常由一系列卷积层和池化层交替组成，最后跟随一个或多个全连接层。对于序列标注任务，可以省略全连接层，从而得到一个全卷积网络（FCN）。FCN已经通过领域适应扩展，以增加鲁棒性[[27](#bib.bib27)]。最近，CNN模型在各种音频处理任务中得到了广泛研究，包括音乐起始检测[[28](#bib.bib28)]、语音增强[[29](#bib.bib29)]、自动语音识别（ASR）[[30](#bib.bib30)]等。然而，高采样率的原始音频波形可能由于CNN的感受野有限而出现性能下降的问题。为了解决这个性能问题，可以使用扩张卷积层，通过在滤波器系数之间插入零来扩展感受野[[31](#bib.bib31),
    [32](#bib.bib32)]。
- en: '![Refer to caption](img/1a6af4d997dafac7484cdeff8a4ed23c.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1a6af4d997dafac7484cdeff8a4ed23c.png)'
- en: 'Figure 1: Graphical illustration of different DL architectures.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同深度学习架构的图示。
- en: Recurrent neural networks (RNNs) follow a different approach for modelling sequential
    data [[33](#bib.bib33)]. They introduce recurrent connections to enable parameters
    to be shared across time, which make them very powerful in learning temporal structures
    from the input sequences (e.g., audio, video). They have demonstrated their superiority
    over traditional HMM-based systems in a variety of speech and audio processing
    tasks [[34](#bib.bib34)]. Due to these abilities, RNNs, especially long-short
    term memory (LSTM) [[10](#bib.bib10)] and gated recurrent unit (GRU) [[35](#bib.bib35)]
    networks, have had an enormous impact in the speech community, and they are incorporated
    in state-of-the-art audio-based systems. Recently, these RNN models have been
    extended to include information in the frequency domain besides temporal information
    in the form of Frequency-LSTMs [[36](#bib.bib36)] and Time-Frequency LSTMs [[37](#bib.bib37)].
    In order to benefit from both neural architectures, CNNs and RNNs can be combined
    into a single network with convolutional layers followed by recurrent layers,
    often referred to as convolutional recurrent neural networks (CRNN). Related works
    combining CNNs and RNNs have been presented in ASR [[38](#bib.bib38)], SER [[39](#bib.bib39)],
    music classification [[40](#bib.bib40)], and other audio related applications [[34](#bib.bib34)].
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）采用不同的方法来建模序列数据[[33](#bib.bib33)]。它们引入了递归连接，使参数可以跨时间共享，这使得它们在从输入序列（例如音频、视频）中学习时间结构方面非常强大。它们在各种语音和音频处理任务中表现出了优于传统HMM系统的优势[[34](#bib.bib34)]。由于这些能力，RNN，尤其是长短期记忆（LSTM）[[10](#bib.bib10)]和门控循环单元（GRU）[[35](#bib.bib35)]网络，在语音领域产生了巨大影响，并且被纳入了最先进的音频系统中。最近，这些RNN模型已经扩展到除了时间信息之外，还包括频域信息，以形式如频率LSTM
    [[36](#bib.bib36)] 和时频LSTM [[37](#bib.bib37)]。为了同时利用这两种神经网络架构，CNN和RNN可以结合成一个单一网络，其中卷积层后跟循环层，通常称为卷积递归神经网络（CRNN）。结合CNN和RNN的相关工作已经在ASR
    [[38](#bib.bib38)]、情感识别（SER）[[39](#bib.bib39)]、音乐分类[[40](#bib.bib40)]以及其他音频相关应用[[34](#bib.bib34)]中得到了展示。
- en: Sequence-to-sequence (Seq2Seq) models were motivated due to problems requiring
    sequences of unknown lengths [[41](#bib.bib41)]. Although they were initially
    applied to machine translation, they can be applied to many different applications
    involving sequence modelling. In a Seq2Seq model, while one RNN reads the inputs
    in order to generate a vector representation (the encoder), another RNN inherits
    those learnt features to generate the outputs (the decoder). The neural architectures
    can be single or multilayer, unidirectional or bidirectional [[42](#bib.bib42)],
    and they can combine multiple architectures [[33](#bib.bib33), [43](#bib.bib43)]
    using end-to-end learning by optimising a joint objective instead of independent
    ones. Seq2Seq models have been gaining much popularity in the speech community
    due to their capability of transducing input to output sequences. DL frameworks
    are particularly suitable for this direct translation task due to their large
    model capacity and their capability to train in an end-to-end manner—to directly
    map the input signals to the target sequences [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46)]. Various Seq2Seq models have been explored in the speech, audio
    and language processing literature including Recurrent Neural Network Transducer
    (RNNT) [[47](#bib.bib47)], Monotonic Alignments [[48](#bib.bib48)], Listen, Attend
    and Spell (LAS) [[49](#bib.bib49)], Neural Transducer [[50](#bib.bib50)], Recurrent
    Neural Aligner (RNA) [[48](#bib.bib48)], and Transformer Networks [[51](#bib.bib51)],
    among others.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列（Seq2Seq）模型的动机来源于需要处理未知长度序列的问题 [[41](#bib.bib41)]。虽然最初它们被应用于机器翻译，但它们也可以应用于许多涉及序列建模的不同应用。在
    Seq2Seq 模型中，一个 RNN 读取输入以生成一个向量表示（编码器），另一个 RNN 继承这些学习到的特征以生成输出（解码器）。神经网络架构可以是单层或多层、单向或双向 [[42](#bib.bib42)]，并且它们可以通过优化一个联合目标而不是独立目标来结合多种架构
    [[33](#bib.bib33), [43](#bib.bib43)]。由于其将输入转换为输出序列的能力，Seq2Seq 模型在语音社区中越来越受欢迎。深度学习框架特别适合这种直接翻译任务，因为它们具有大模型容量和端到端训练能力——直接将输入信号映射到目标序列 [[44](#bib.bib44),
    [45](#bib.bib45), [46](#bib.bib46)]。各种 Seq2Seq 模型已经在语音、音频和语言处理文献中得到探索，包括递归神经网络转导器（RNNT） [[47](#bib.bib47)]、单调对齐 [[48](#bib.bib48)]、听、关注和拼写（LAS） [[49](#bib.bib49)]、神经转导器 [[50](#bib.bib50)]、递归神经对齐器（RNA） [[48](#bib.bib48)]
    和 Transformer 网络 [[51](#bib.bib51)]，等等。
- en: 'Generative Models have been attaining much interest in the three types of generative
    models: Generative Adversarial Networks (GANs) [[52](#bib.bib52)], Variational
    Autoencoders (VAEs) [[53](#bib.bib53)], and autoregressive models [[54](#bib.bib54)].
    This type of models are powerful enough to learn the underlying distribution of
    speech datasets and have been extensively investigated in the speech and audio
    processing scientific community. Specifically, in the case of GANs and VAEs, audio
    signal is often synthesised from a low-dimensional representation, from which
    it needs to by upsampled (e.g., through linear interpolation or the nearest neighbour)
    to the high-resolution signal [[55](#bib.bib55), [56](#bib.bib56)]. Therefore,
    VAEs and GANs have been extensively explored for synthesising speech or to augment
    the training material by generating features [[57](#bib.bib57)] or speech itself.
    In the autoregressive approach, the new samples are synthesised iteratively—based
    on an infinitely long context of previous samples via RNNs (for example, using
    LSTM or GRU networks)—but at the cost of expensive computation during training [[58](#bib.bib58)].'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型在三种生成模型中引起了广泛关注：生成对抗网络（GANs） [[52](#bib.bib52)]、变分自编码器（VAEs） [[53](#bib.bib53)]
    和自回归模型 [[54](#bib.bib54)]。这些模型足够强大，能够学习语音数据集的潜在分布，并在语音和音频处理科学社区中得到了广泛研究。具体来说，在
    GANs 和 VAEs 的情况下，音频信号通常从低维表示中合成，需要通过上采样（例如，通过线性插值或最近邻）到高分辨率信号 [[55](#bib.bib55),
    [56](#bib.bib56)]。因此，VAEs 和 GANs 已被广泛探索用于合成语音或通过生成特征 [[57](#bib.bib57)] 或语音本身来增强训练材料。在自回归方法中，新样本通过
    RNN（例如，使用 LSTM 或 GRU 网络）在先前样本的无限长上下文中迭代合成——但这在训练期间代价昂贵 [[58](#bib.bib58)]。
- en: II-B Reinforcement learning
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 强化学习
- en: Reinforcement learning (RL) is a popular paradigm of ML, which involves agents
    to learn their behaviour by trial and error [[2](#bib.bib2)]. RL agents aim to
    learn sequential decision-making by successfully interacting with the environment
    where they operate. At time $t$ (0 at the beginning of the interaction, $T$ at
    the end of an episodic interaction, or $\infty$ in the case of non-episodic tasks),
    an RL agent in state $s_{t}$ takes an action $a\in A$, transits to a new state
    $s_{t+1}$, and receives reward $r_{t+1}$ for having chosen action $a$. This process—repeated
    iteratively—is illustrated in Figure [2](#S2.F2 "Figure 2 ‣ II-B Reinforcement
    learning ‣ II Background ‣ A Survey on Deep Reinforcement Learning for Audio-Based
    Applications").
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习 (RL) 是一种流行的机器学习范式，它涉及代理通过试错来学习其行为 [[2](#bib.bib2)]。RL 代理旨在通过成功与其操作的环境互动来学习序列决策。在时间
    $t$（交互开始时为 0， episodic 交互结束时为 $T$，或者在非 episodic 任务中为 $\infty$），RL 代理在状态 $s_{t}$
    下采取一个动作 $a\in A$，转移到新状态 $s_{t+1}$，并因选择动作 $a$ 而获得奖励 $r_{t+1}$。这一过程—反复迭代—如图 [2](#S2.F2
    "图 2 ‣ II-B 强化学习 ‣ II 背景 ‣ 深度强化学习在音频应用中的综述") 所示。
- en: '![Refer to caption](img/4ba421221eb5185f9858834d9391bd71.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4ba421221eb5185f9858834d9391bd71.png)'
- en: 'Figure 2: Basic RL setting.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 基本的 RL 设置。'
- en: 'An RL agent aims to learn the best sequence of actions, known as policy, to
    obtain the highest overall cumulative reward in the task (or set of tasks) that
    is being trained on. While it can choose any action from a set of available actions,
    the set of actions that an agent takes from start to finish is called an episode.
    A Markov decision process (MDP) [[59](#bib.bib59)] can be used to capture the
    episodic dynamics of an RL problem. An MDP can be represented using the tuple
    ($S$, $A$, $\gamma$, $P$, $R$). The decision-maker or agent chooses an action
    $a\in A$ in state $s\in S$ at time $t$ according to its policy $\pi(a_{t}|s_{t})$—which
    determines the agent’s way of behaving. The probability of moving to the next
    state $s_{t+1}\in S$ is given by the state transition function $P(s_{t+1}|s_{t},a_{t})$.
    The environment produces a reward $R(s_{t},a_{t},s_{t+1})$ based on the action
    taken by the agent at time $t$. This process continues until the maximum time
    step or the agent reaches a terminal state. The objective is to maximise the expected
    discounted cumulative reward, which is given by:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: RL 代理旨在学习最佳的动作序列，即策略，以在所训练的任务（或任务集）中获得最高的总体累积奖励。尽管它可以从可用动作集中选择任何动作，但代理从开始到结束所采取的动作集称为一个
    episode。马尔可夫决策过程 (MDP) [[59](#bib.bib59)] 可用于捕捉 RL 问题的 episodic 动态。MDP 可以使用元组
    ($S$, $A$, $\gamma$, $P$, $R$) 来表示。决策者或代理在时间 $t$ 的状态 $s\in S$ 下根据其策略 $\pi(a_{t}|s_{t})$
    选择一个动作 $a\in A$，该策略决定了代理的行为方式。转移到下一个状态 $s_{t+1}\in S$ 的概率由状态转移函数 $P(s_{t+1}|s_{t},a_{t})$
    给出。环境根据代理在时间 $t$ 采取的动作产生奖励 $R(s_{t},a_{t},s_{t+1})$。这一过程持续直到达到最大时间步或代理到达终止状态。目标是最大化期望折扣累积奖励，其公式为：
- en: '|  | $E_{\pi}[R_{t}]=E_{\pi}\Big{[}\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}\Big{]}$
    |  | (1) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{\pi}[R_{t}]=E_{\pi}\Big{[}\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}\Big{]}$
    |  | (1) |'
- en: where $\gamma$ $\in$ [0,1] is a discount factor used to specify that rewards
    in the distant future are less valuable than in the nearer future. While an RL
    agent may only learn its policy, it may also learn (online or offline) the transition
    and reward functions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ $\in$ [0,1] 是一个折扣因子，用于指定远期奖励比近期奖励的价值更低。虽然 RL 代理可能只学习其策略，但它也可能学习（在线或离线）转移和奖励函数。
- en: III Deep Reinforcement Learning
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度强化学习
- en: 'Deep reinforcement learning (DRL) combines conventional RL with DL to overcome
    the limitations of RL in complex environments with large state spaces or high
    computation requirements. DRL employs DNNs to estimate value, policy or model
    that are learnt through the storage of state-action pairs in conventional RL [[19](#bib.bib19)].
    Deep RL algorithms can be classified along several dimensions. For instance, on-policy
    vs off-policy, model-free vs model-based, value-based vs policy-based DRL algorithms,
    among others. The salient features of various key categories of DRL algorithms
    are presented and depicted in Figure [3](#S3.F3 "Figure 3 ‣ III Deep Reinforcement
    Learning ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    Interested readers are referred to [[19](#bib.bib19)] for more details on these
    algorithms. This section focuses on popular DRL algorithms employed in audio-based
    applications in three main categories: (i) value-based DRL, (ii) policy gradient-based
    DRL and (iii) model-based DRL.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习（DRL）将传统的强化学习（RL）与深度学习（DL）结合，以克服 RL 在具有大状态空间或高计算需求的复杂环境中的限制。DRL 利用深度神经网络（DNNs）来估计价值、策略或模型，这些都是通过存储状态-动作对在传统
    RL 中学习的 [[19](#bib.bib19)]。深度 RL 算法可以从多个维度进行分类。例如，按策略（on-policy vs off-policy）、模型（model-free
    vs model-based）、价值（value-based vs policy-based）等进行分类。各种关键类别的 DRL 算法的显著特征在图 [3](#S3.F3
    "图 3 ‣ III 深度强化学习 ‣ 深度强化学习在基于音频的应用中的调查") 中进行了展示和描述。感兴趣的读者可参考 [[19](#bib.bib19)]
    了解这些算法的更多细节。本节重点介绍了在音频应用中使用的流行 DRL 算法，分为三大类：（i）基于价值的 DRL，（ii）基于策略梯度的 DRL，以及（iii）基于模型的
    DRL。
- en: '![Refer to caption](img/8cae1e7c500b08e3e0278c7e48339ca2.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8cae1e7c500b08e3e0278c7e48339ca2.png)'
- en: 'Figure 3: Classification of DRL algorithms.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：DRL 算法分类。
- en: III-A Value-Based DRL
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 基于价值的 DRL
- en: One of the most famous value-based DRL algorithms is Deep Q-network (DQN), introduced
    by Mnih et al. [[12](#bib.bib12)], that learns directly from high-dimensional
    inputs. It employs convolution neural networks (CNNs) to estimate a value function
    $Q(s,a)$, which is subsequently used to define a policy. DQN enhances the stability
    of the learning process using the concept of target Q-network along with experience
    replay. The loss function computed by DQN at $i^{th}$ iteration is given by
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的基于价值的深度强化学习（DRL）算法之一是 Deep Q-network（DQN），由 Mnih 等人 [[12](#bib.bib12)] 提出，该算法直接从高维输入中学习。它采用卷积神经网络（CNNs）来估计价值函数
    $Q(s,a)$，然后用以定义策略。DQN 通过使用目标 Q 网络和经验重放的概念，增强了学习过程的稳定性。DQN 在第 $i^{th}$ 次迭代中计算的损失函数为
- en: '|  | $\displaystyle L_{i}(\theta_{i})=\mathbb{E}_{s,a\sim p(.)}[(y_{i}-Q(s,a;\theta_{i}))^{2}],$
    |  | (2) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{i}(\theta_{i})=\mathbb{E}_{s,a\sim p(.)}[(y_{i}-Q(s,a;\theta_{i}))^{2}],$
    |  | (2) |'
- en: '|  | $\displaystyle\text{where}\quad y_{i}=\mathbb{E}_{s\prime\sim s}[r+\gamma\underset{a\prime}{\text{max}}Q(s\prime,a\prime;\theta_{i-1}&#124;{s,a}].$
    |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{其中}\quad y_{i}=\mathbb{E}_{s\prime\sim s}[r+\gamma\underset{a\prime}{\text{max}}Q(s\prime,a\prime;\theta_{i-1}&#124;{s,a}].$
    |  |'
- en: 'Although DQN, since inception, has rendered super-human performance in Atari
    games, it is based on a single max operator, given in (2), for selection as well
    evaluation of an action. Thus, the selection of an overestimated action may lead
    to over-optimistic action value estimates that induces an upward bias. Double
    DQN (DDQN) [[60](#bib.bib60)] eliminates this positive bias by introducing two
    decoupled estimators: one for the selection of an action, and one for the evaluation
    of an action. Schaul et al. in [[61](#bib.bib61)] show that the performance of
    DQN and DDQN is enhanced considerably if significant experience transitions are
    prioritised and replayed more frequently. Wang et al. [[62](#bib.bib62)] present
    a duelling network architecture (DNA) to estimate a value function $V(s)$ and
    associated advantage function $A(s,a)$ separately, and then combine them to get
    action-value function $Q(s,a)$. Results prove that DQN and DDQN having DNA and
    prioritised experience replay can lead to improved performance.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自诞生以来，DQN 在 Atari 游戏中已展现出超人的性能，但它基于单一的最大化运算符，如公式（2）所示，用于选择和评估动作。因此，选择一个被高估的动作可能会导致过于乐观的动作价值估计，从而引入向上的偏差。双重
    DQN（DDQN）[[60](#bib.bib60)] 通过引入两个解耦的估计器来消除这种正向偏差：一个用于动作选择，另一个用于动作评估。Schaul 等人
    [[61](#bib.bib61)] 表明，如果优先考虑重要的经验过渡并更频繁地重放，DQN 和 DDQN 的性能会显著提升。Wang 等人 [[62](#bib.bib62)]
    提出了一个对抗网络架构（DNA），分别估计价值函数 $V(s)$ 和相关的优势函数 $A(s,a)$，然后将它们结合起来得到动作-价值函数 $Q(s,a)$。结果证明，DQN
    和 DDQN 结合 DNA 以及优先经验重放可以提升性能。
- en: Unlike the aforementioned DQN algorithms that focus on the expected return,
    distributional DQN [[63](#bib.bib63)] aims to learn the full distribution of the
    value in order to have additional information about rewards. Despite both DQN
    and distributional DQN focusing on maximising the expected return, the latter
    comparatively results in performant learning. Will et al. [[64](#bib.bib64)] propose
    distributional DQN with quantile regression (QR-DQN) to explicitly model the distribution
    of the value function. Results prove that QR-DQN successfully bridges the gap
    between theoretic and algorithmic results. Implicit Quantile Networks (IQN) [[65](#bib.bib65)],
    an extension to QR-DQN, estimate quantile regression by learning the full quantile
    function instead of focusing on a discrete number of quantiles. IQN also provides
    flexibility regarding its training with the required number of samples per update,
    ranging from one sample to a maximum computationally allowed. IQN has shown to
    outperform QR-DQN comprehensively in the Atari domain.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与前述专注于期望回报的DQN算法不同，分布式DQN [[63](#bib.bib63)]旨在学习价值的完整分布，以获得关于奖励的额外信息。尽管DQN和分布式DQN都专注于最大化期望回报，但后者在学习性能上相对更优。Will等人[[64](#bib.bib64)]提出了带有分位回归的分布式DQN（QR-DQN），以显式建模价值函数的分布。结果证明QR-DQN成功地弥合了理论和算法结果之间的差距。隐式分位网络（IQN）[[65](#bib.bib65)]是QR-DQN的扩展，通过学习完整的分位函数来估计分位回归，而不是专注于离散数量的分位点。IQN还提供了关于每次更新所需样本数量的灵活性，样本数量从一个样本到最大计算允许的数量。IQN在Atari领域已证明在性能上全面优于QR-DQN。
- en: The astounding success of DQN to learn rich representations is highly attributed
    to DNNs, while batch algorithms prove to have better stability and data efficiency
    (requiring less tuning of hyperparameters). Authors in [[66](#bib.bib66)] propose
    a hybrid approach named as Least Squares DQN (LS-DQN) that exploits the advantages
    of both DQN and batch algorithms. Deep Q-learning from demonstrations (DQfD) [[67](#bib.bib67)]
    leverages human demonstrations to learn at an accelerated rate from the start.
    Deep Quality-Value (DQV) [[68](#bib.bib68)] is a novel temporal-difference-based
    algorithm that trains the Value network initially, and subsequently uses it to
    train a Quality-value neural network for estimating a value function. Results
    in the Atari domain indicate that DQV outperforms DQN as well as DDQN. Authors
    in [[69](#bib.bib69)] propose RUDDER (Return Decomposition for Delayed Rewards),
    which encompasses reward redistribution and return decomposition for Markov decision
    processes (MDPs) with delayed rewards. Pohlen et al. [[70](#bib.bib70)] employ
    a transformed Bellman operator along with human demonstrations in the proposed
    algorithm Ape-X DQfD to attain human-level performance over a wide range of games.
    Results prove that the proposed algorithm achieves average-human performance in
    40 out of 42 Atari games with the same set of hyperparameters. Schulman et al.
    in [[71](#bib.bib71)] study the connection between Q-learning and policy gradient
    methods. They show that soft Q-learning (an entropy-regularised version of Q-learning)
    is equivalent to policy gradient methods and that they perform as well (if not
    better) than standard variants.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: DQN在学习丰富表示方面的惊人成功主要归功于深度神经网络（DNNs），而批量算法则证明了具有更好的稳定性和数据效率（需要较少的超参数调整）。[[66](#bib.bib66)]中的作者提出了一种名为最小二乘DQN（LS-DQN）的混合方法，利用DQN和批量算法的优点。基于示例的深度Q学习（DQfD）[[67](#bib.bib67)]利用人类示例以加速从一开始的学习。深度质量-价值（DQV）[[68](#bib.bib68)]是一种新型的基于时序差分的算法，最初训练价值网络，然后使用它来训练质量-价值神经网络以估计价值函数。在Atari领域的结果表明，DQV在性能上优于DQN和DDQN。[[69](#bib.bib69)]中的作者提出了RUDDER（延迟奖励的回报分解），涵盖了奖励重分配和回报分解，用于具有延迟奖励的马尔可夫决策过程（MDPs）。Pohlen等人[[70](#bib.bib70)]在所提议的算法Ape-X
    DQfD中结合了变换的Bellman算子和人类示例，在广泛的游戏中达到了人类水平的表现。结果证明，该算法在42款Atari游戏中的40款中实现了与人类相当的表现，使用的是相同的超参数。[[71](#bib.bib71)]中的Schulman等人研究了Q学习与策略梯度方法之间的联系。他们展示了软Q学习（Q学习的熵正则化版本）等同于策略梯度方法，并且它们的表现（如果不是更好）与标准变体一样。
- en: Previous studies have also attempted to incorporate the memory element into
    DRL algorithms. For instance, the deep recurrent Q-network (DRQN) approach introduced
    by [[72](#bib.bib72)] was able to successfully integrate information through time,
    which performed well on standard Atari games. A further improvement was made by
    introducing an attention mechanism to DQN, resulting in a deep recurrent Q-network
    (DARQN) [[73](#bib.bib73)]. This allows DARQN to focus on a specific part of the
    input and achieve better performance compared to DQN and DRQN on games. Some other
    studies [[74](#bib.bib74), [75](#bib.bib75)] have also proposed methods to incorporate
    memory into DRL, but this area remains to be investigated further.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究也尝试将记忆元素融入到深度强化学习算法中。例如，[[72](#bib.bib72)]提出的深度递归Q网络（DRQN）方法能够成功地整合时间上的信息，在标准的Atari游戏中表现良好。通过引入注意力机制到DQN中，进一步改进了这一方法，形成了深度递归Q网络（DARQN）[[73](#bib.bib73)]。这使得DARQN能够专注于输入的特定部分，并在游戏中相较于DQN和DRQN实现更好的性能。其他一些研究[[74](#bib.bib74),
    [75](#bib.bib75)]也提出了将记忆融入深度强化学习的方法，但这一领域仍需进一步研究。
- en: III-B Policy Gradient-Based DRL
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 基于策略梯度的深度强化学习（DRL）
- en: Policy gradient-based DRL algorithms aim to learn an optimal policy that maximises
    performance objectives, such as expected cumulative reward. This class of algorithms
    make use of gradient theorems to reach optimal policy parameters. Policy gradient
    typically requires the estimation of a value function based on the current policy.
    This may be accomplished using the actor-critic architecture, where the actor
    represents the policy and the critic refers to value function estimate [[76](#bib.bib76)].
    Mnih et al. [[77](#bib.bib77)] show that asynchronous execution of multiple parallel
    agents on standard CPU-based hardware leads to time-efficient and resource-efficient
    learning. The proposed asynchronous version of actor-critic, asynchronous advantage
    actor-critic (A3C) exhibit remarkable learning in both 2D and 3D games with action
    spaces in discrete as well as continuous domains. Authors in [[78](#bib.bib78)]
    propose a hybrid CPU/GPU-based A3C —named as GA3C — showing significantly higher
    speeds as compared to its CPU-based counterpart.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略梯度的深度强化学习算法旨在学习一种最优策略，以最大化性能目标，例如期望累计奖励。这类算法利用梯度定理来达到最优策略参数。策略梯度通常需要基于当前策略估计价值函数。这可以通过演员-评论家架构来实现，其中演员表示策略，评论家则指代价值函数估计[[76](#bib.bib76)]。Mnih等人[[77](#bib.bib77)]
    证明了在标准CPU硬件上异步执行多个并行代理可以实现时间和资源的高效学习。提出的异步演员-评论家版本，即异步优势演员-评论家（A3C），在具有离散和连续动作空间的2D和3D游戏中表现出卓越的学习效果。[[78](#bib.bib78)]中的作者提出了一种混合CPU/GPU的A3C——称为GA3C——与其基于CPU的对应物相比，显示出了显著更高的速度。
- en: 'Asynchronous actor-critic algorithms, including A3C and GA3C, may suffer from
    inconsistent and asynchronous parameter updates. A novel framework for asynchronous
    algorithms is proposed in [[79](#bib.bib79)] to leverage parallelisation while
    providing synchronous parameters updates. Authors show that the proposed parallel
    advantage actor-critic (PAAC) algorithm enables true on-policy learning in addition
    to faster convergence. Authors in [[80](#bib.bib80)] propose a hybrid policy-gradient-and-Q-learning
    (PGQL) algorithm that combines on-policy policy gradient with off-policy Q-learning.
    Results demonstrate PGQL’s superior performance on Atari games as compared to
    both A3C and Q-learning. Munos et al. [[81](#bib.bib81)] propose a novel algorithm
    by bringing together three off-policy algorithms: Instance Sampling (IS), Q($\lambda$),
    and Tree-Backup TB($\lambda$). This algorithm — called Retrace($\lambda$) — alleviates
    the weaknesses of all three algorithms (IS has low variance, Q($\lambda$) is not
    safe, and TB($\lambda$) is inefficient) and promises safety, efficiency and guaranteed
    convergence. Reactor (Retrace-Actor) [[82](#bib.bib82)] is a Retrace-based actor-critic
    agent architecture that combines time efficiency of asynchronous algorithms with
    sample efficiency of off-policy experience replay-based algorithms. Results in
    the Atari domain indicate that the proposed algorithm performs comparably with
    state-of-the-art algorithms while yielding substantial gains in terms of training
    time. The importance of weighted actor-learner architecture (IMPALA) [[83](#bib.bib83)]
    is a scalable distributed agent that is capable of handling multiple tasks with
    a single set of parameters. Results show that IMPALA outperforms A3C baselines
    in a diverse multi-task environment.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 异步演员-评论家算法，包括A3C和GA3C，可能会面临不一致和异步的参数更新问题。提出了一种新的异步算法框架[[79](#bib.bib79)]，以利用并行化的同时提供同步的参数更新。作者表明，提出的并行优势演员-评论家（PAAC）算法不仅实现了真正的在线策略学习，还加快了收敛速度。[[80](#bib.bib80)]中的作者提出了一种混合策略梯度和Q学习（PGQL）算法，结合了在线策略梯度和离策略Q学习。结果表明，与A3C和Q学习相比，PGQL在Atari游戏中表现优越。Munos等人[[81](#bib.bib81)]通过将三种离策略算法结合在一起：实例采样（IS）、Q($\lambda$)和树备份TB($\lambda$)，提出了一种新算法——Retrace($\lambda$)。该算法缓解了所有三种算法的弱点（IS具有低方差，Q($\lambda$)不安全，TB($\lambda$)效率低），并承诺安全、高效且保证收敛。Reactor（Retrace-Actor）[[82](#bib.bib82)]是一种基于Retrace的演员-评论家代理架构，结合了异步算法的时间效率和基于离策略经验回放算法的样本效率。Atari领域的结果表明，提出的算法在训练时间方面表现出显著的提升，与最先进的算法相当。加权演员-学习者架构的重要性（IMPALA）[[83](#bib.bib83)]是一种可扩展的分布式代理，能够用一组参数处理多个任务。结果表明，IMPALA在多任务环境中优于A3C基线。
- en: Schulman et al. [[84](#bib.bib84)] propose a robust and scalable trust region
    policy optimisation (TRPO) algorithm for optimising stochastic control policies.
    TRPO promises guaranteed monotonic improvement regarding the optimisation of nonlinear
    and complex policies having an inundated number of parameters. This learning algorithm
    makes use of a fixed KL divergence constraint rather than a fixed penalty coefficient,
    and outperforms a number of gradient-free and policy-gradient methods over a wide
    variety of tasks. [[85](#bib.bib85)] introduce proximal policy optimisation (PPO),
    which aims to be as reliable and stable as TRPO but relatively better in terms
    of implementation and sample complexity.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Schulman等人[[84](#bib.bib84)]提出了一种稳健且可扩展的信任区域策略优化（TRPO）算法，用于优化随机控制策略。TRPO承诺在优化具有大量参数的非线性和复杂策略时保证单调改进。该学习算法使用固定的KL散度约束，而不是固定的惩罚系数，并在各种任务中优于许多无梯度和策略梯度方法。[[85](#bib.bib85)]介绍了近端策略优化（PPO），旨在实现像TRPO一样可靠和稳定，但在实现和样本复杂性方面相对更好。
- en: 'TABLE II: Summary of DRL algorithms.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：深度强化学习算法总结。
- en: '| DRL algorithms | Approach | Details |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 深度强化学习算法 | 方法 | 详情 |'
- en: '&#124; off-policy/ &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 离策略/ &#124;'
- en: '&#124; on policy &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在线策略 &#124;'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Value-based DRL |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 基于价值的深度强化学习 |'
- en: '| DQN [2] | Target Q-network, experience replay |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| DQN [2] | 目标Q网络，经验回放 |'
- en: '&#124; •  Learns directly from high dimensional visual inputs &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 直接从高维视觉输入中学习 &#124;'
- en: '&#124; •  Stabilises learning process with target Q-network &#124;'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 通过目标Q网络稳定学习过程 &#124;'
- en: '&#124; •  Experience replay to avoid divergence in parameters &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 经验回放以避免参数发散 &#124;'
- en: '| off-policy |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 离策略 |'
- en: '| DDQN [3] | Double Q-learning |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| DDQN [3] | 双重Q学习 |'
- en: '&#124; Decoupled estimators for the selection and evaluation of an action &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 为动作选择和评估提供解耦的估计器 &#124;'
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Prioritised DQN [4] | Prioritised experience replay |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 优先DQN [4] | 优先经验重放 |'
- en: '&#124; Significant experience transitions are prioritised and replayed &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重要的经验过渡被优先考虑并重放 &#124;'
- en: '&#124; frequently thus leading to efficient learning &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 经常这样做，从而实现高效学习 &#124;'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DNA [5] | Duelling neural network architecture |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| DNA [5] | 对抗神经网络架构 |'
- en: '&#124; Estimates a value function and associated advantage function and combine
    &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计一个值函数及相关的优势函数并结合 &#124;'
- en: '&#124; them to get a value function with faster convergence than Q-learning
    &#124;'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 它们能够得到一个比Q学习更快收敛的值函数 &#124;'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Distributional DQN [6] |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Distributional DQN [6] |'
- en: '&#124; Learns distribution of cumulative returns &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习累积回报的分布 &#124;'
- en: '&#124; using distributional Bellman equation &#124;'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用分布式Bellman方程 &#124;'
- en: '|'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Leads to performant learning than DQN &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  比DQN更高效的学习 &#124;'
- en: '&#124; •  Possibility to implement risk-aware behaviour &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  有可能实现风险意识行为 &#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| QR-DQN [7] |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| QR-DQN [7] |'
- en: '&#124; Distributional DQN with quantile regression &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具有分布式DQN的分位回归 &#124;'
- en: '| Bridges gap between theoretical and algorithmic results |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 缩小理论结果与算法结果之间的差距 |'
- en: '| IQN [8] |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| IQN [8] |'
- en: '&#124; Extends QR-DQN with a full quantile function &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 扩展了QR-DQN，具有完整的分位函数 &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Provides flexibility regarding number of samples required for training
    &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提供关于训练所需样本数量的灵活性 &#124;'
- en: '|'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LS-DQN [9] |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LS-DQN [9] |'
- en: '&#124; A hybrid approach combining DQN with &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一种将DQN与 &#124;'
- en: '&#124; least-squares method &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最小二乘法 &#124;'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Exploits advantages of both DQN, ability to learn rich representations,
    &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用DQN的优势，能够学习丰富的表示， &#124;'
- en: '&#124; and batch algorithms, stability and data efficiency &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以及批量算法，稳定性和数据效率 &#124;'
- en: '|'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DQfD [10] | Learns from demonstrations | Learns at an accelerated rate from
    the start. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| DQfD [10] | 从示范中学习 | 从一开始就加速学习。 |'
- en: '| DQV [11] |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| DQV [11] |'
- en: '&#124; Uses temporal difference to train a Value network &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用时序差分训练一个值网络 &#124;'
- en: '&#124; and subsequently uses it for training a Quality-Value &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 随后用于训练一个质量-值模型 &#124;'
- en: '&#124; network that estimates state-action values &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计状态-动作值的网络 &#124;'
- en: '| Learns significantly better and faster than DQN and DDQN |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 学习效果显著优于DQN和DDQN |'
- en: '| RUDDER [12] | Reward redistribution and return decomposition |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| RUDDER [12] | 奖励重新分配和回报分解 |'
- en: '&#124; Provides prominent improvement on games having long &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在长时间游戏中提供显著改进 &#124;'
- en: '&#124; delayed rewards &#124;'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 延迟奖励 &#124;'
- en: '|'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Ape-X DQfD [13] |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| Ape-X DQfD [13] |'
- en: '&#124; Employs transformed Bellman operator &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用变换的Bellman算子 &#124;'
- en: '&#124; together with temporal consistency loss &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以及时序一致性损失 &#124;'
- en: '|'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Surpasses average human performance on 40 out &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在42个Atari 2600游戏中超越了平均人类表现 &#124;'
- en: '&#124; of 42 Atari 2600 games &#124;'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 42款游戏中的表现 &#124;'
- en: '|'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Soft DQN [14] |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Soft DQN [14] |'
- en: '&#124; Incorporation of soft KL penalty and entropy bonus &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融合了软KL惩罚和熵奖励 &#124;'
- en: '|'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Establishes equivalence between Soft DQN and policy gradient &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 建立了Soft DQN与策略梯度之间的等价关系 &#124;'
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| DRQN, DARQN [14-15] |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| DRQN, DARQN [14-15] |'
- en: '&#124; Memory, attention &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 记忆，注意力 &#124;'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DQN policies modelled by attention-based recurrent networks &#124;'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DQN策略通过基于注意力的递归网络建模 &#124;'
- en: '|  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '| Policy Gradient-based DRL |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 基于策略梯度的DRL |'
- en: '| A3C [16] | Asynchronous gradient descent | Consumes less resources; able
    to run on a standard multi-core CPU | on-policy |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| A3C [16] | 异步梯度下降 | 消耗较少资源；能够在标准多核CPU上运行 | 在政策 |'
- en: '| GA3C [17] | Hybrid CPU/GPU-based A3C | Achieves speed significantly higher
    than its CPU-based counterpart |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| GA3C [17] | 混合CPU/GPU的A3C | 实现了显著高于其CPU版本的速度 |'
- en: '| PAAC [18] |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| PAAC [18] |'
- en: '&#124; Novel framework for asynchronous algorithms &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 针对异步算法的创新框架 &#124;'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Computationally efficient & enables faster convergence to optimal policies
    &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算效率高，能够更快地收敛到最优策略 &#124;'
- en: '|'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PGQL [19] |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| PGQL [19] |'
- en: '&#124; Combines on-policy policy gradient &#124;'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合了政策梯度的在策略学习 &#124;'
- en: '&#124; with off-policy Q-learning &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结合了离线策略Q学习 &#124;'
- en: '| Enhanced stability and data efficiency | off-policy |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 增强了稳定性和数据效率 | 离线策略 |'
- en: '| Retrace($\lambda$) [20] |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Retrace($\lambda$) [20] |'
- en: '&#124; Expresses three off-policy algorithms—IS, &#124;'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 表达了三种离策略算法—IS，&#124;'
- en: '&#124; Q($\lambda$) and TB($\lambda$)— in a common form &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Q($\lambda$) 和 TB($\lambda$)— 以常见形式&#124;'
- en: '| Safe, sample efficient and has low variance |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 安全、样本效率高且方差低 |'
- en: '| Reactor [21] | Retrace-based actor-critic agent architecture | Yields substantial
    gains in terms of training time. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Reactor [21] | 基于Retrace的演员-评论员代理架构 | 在训练时间上取得显著提升。 |'
- en: '| IMPALA [22] |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| IMPALA [22] |'
- en: '&#124; Scalable distributed agent capable &#124;'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可扩展的分布式代理&#124;'
- en: '&#124; of handling multiple tasks with &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 能处理多个任务&#124;'
- en: '&#124; a single set of parameters &#124;'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一组参数&#124;'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; outperforms state-of-the-art agents in a &#124;'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在&#124; Atari游戏中超越最先进的代理'
- en: '&#124; diverse multi-task environment &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多样化的多任务环境&#124;'
- en: '| on-policy |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 在策略 |'
- en: '| TRPO [23] |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| TRPO [23] |'
- en: '&#124; Employs fixed KL divergence constraint &#124;'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 采用固定的KL散度约束&#124;'
- en: '&#124; for optimising stochastic control policies &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 用于优化随机控制策略&#124;'
- en: '|'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Performs well over a wide variety of large-scale tasks &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在各种大规模任务中表现良好&#124;'
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PPO [24] |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| PPO [24] |'
- en: '&#124; Makes use of adaptive KL penalty coefficient &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 利用自适应KL惩罚系数&#124;'
- en: '|'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; As reliable and stable as TRPO but relatively better &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与TRPO一样可靠和稳定，但相对更优&#124;'
- en: '&#124; in terms of implementation and sample complexity &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在实现和样本复杂性方面&#124;'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Model-based DRL |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 基于模型的DRL |'
- en: '| SimPLe [26] |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| SimPLe [26] |'
- en: '&#124; Video prediction-based model-based algorithm that &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于视频预测的模型算法&#124;'
- en: '&#124; requires much fewer agent-environment &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要更少的代理-环境&#124;'
- en: '&#124; interactions than model-free algorithms &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 相比无模型算法的互动更丰富&#124;'
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Outperforms state-of-the-art model-free algorithms in Atari games &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在Atari游戏中超越最先进的无模型算法&#124;'
- en: '| on-policy |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 在策略 |'
- en: '| TreeQN [27] |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| TreeQN [27] |'
- en: '&#124; Estimates Q-values based on a dynamic &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于动态&#124;估计Q值'
- en: '&#124; tree constructed recursively through &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过&#124;递归构建的树结构'
- en: '&#124; an implicit transition model &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一个隐式的过渡模型&#124;'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Outperforms n-step DQN and value prediction networks &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 超越n步DQN和价值预测网络&#124;'
- en: '&#124; in multiple Atari games &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在多个Atari游戏中&#124;'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| STRAW [29] |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| STRAW [29] |'
- en: '&#124; Capable of natural decision making &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 具备自然决策能力&#124;'
- en: '&#124; by learning macro actions &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过学习宏观动作&#124;'
- en: '| Improves performance significantly in Atari games |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 在Atari游戏中显著提升性能 |'
- en: '| VProp [30] |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| VProp [30] |'
- en: '&#124; A set of Value Iteration-based planning &#124;'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一组基于值迭代的规划&#124;'
- en: '&#124; modules that is trained using RL &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过RL训练的模块&#124;'
- en: '|'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Able to solve an unseen task and navigate in complex environments
    &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  能够解决未知任务并在复杂环境中导航&#124;'
- en: '&#124; •  Able to generalise in dynamic and noisy environment &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  能够在动态和嘈杂的环境中泛化&#124;'
- en: '| - |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| - |'
- en: '| MuZero [31] |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| MuZero [31] |'
- en: '&#124; Combines tree-based search with learned &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将基于树的搜索与学习的&#124;'
- en: '&#124; model to render superhuman performance &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 渲染超人类性能的模型&#124;'
- en: '&#124; in challenging environments &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在挑战性环境中&#124;'
- en: '|'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Delivers state-of-the-art performance on 57 diverse Atari games &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在57种不同的Atari游戏中提供最先进的性能&#124;'
- en: '| off-policy |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 离策略 |'
- en: '![Refer to caption](img/85cebe8dc02a10d4f8adcf57a15ff2e5.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/85cebe8dc02a10d4f8adcf57a15ff2e5.png)'
- en: 'Figure 4: Schematic diagram of DRL agents for audio-based applications, where
    the DL model (via DNNs, CNN, RNNs, etc.) generates audio features from raw waveforms
    or other audio representations for taking actions that change the environment
    from state $s_{t}$ to a next state $s_{t+1}$.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于音频应用的DRL代理示意图，其中DL模型（通过DNN、CNN、RNN等）从原始波形或其他音频表示中生成音频特征，以采取使环境从状态$s_{t}$转变为下一个状态$s_{t+1}$的动作。
- en: III-C Model-Based DRL
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 基于模型的DRL
- en: Model-based DRL algorithms rely on models of the environment (i.e. underlying
    dynamics and reward functions) in conjunction with a planning algorithm. Unlike
    model-free DRL methods that typically entail a large number of samples to render
    adequate performance, model-based algorithms generally lead to improved sample
    and time efficiency [[86](#bib.bib86)].
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模型的DRL算法依赖于环境模型（即基础动态和奖励函数）以及规划算法。与通常需要大量样本以实现充分性能的无模型DRL方法不同，基于模型的算法通常能提高样本和时间效率[[86](#bib.bib86)]。
- en: Kaiser et al. [[87](#bib.bib87)] propose simulated policy learning (SimPLe),
    a video prediction-based model-based DRL algorithm that requires much fewer agent-environment
    interactions than model-free algorithms. Experimental results indicate that SimPLe
    outperforms state-of-the-art model-free algorithms in Atari games. Farquhar et
    al. [[88](#bib.bib88)] propose TreeQN for complex environments, where the transition
    model is not explicitly given. The proposed algorithm combines model-free and
    model-based approaches in order to estimate Q-values based on a dynamic tree constructed
    recursively through an implicit transition model. Authors of [[88](#bib.bib88)]
    also propose an actor-critic variant named ATreeC that augments TreeQN with a
    softmax layer to form a stochastic policy network. They show that both algorithms
    yield superior performance than n-step DQN and value prediction networks [[89](#bib.bib89)]
    on multiple Atari games. Authors in [[90](#bib.bib90)] introduce a Strategic Attentive
    Writer (STRAW), which is capable of making natural decisions by learning macro-actions.
    Unlike state-of-the-art DRL algorithms that yield only one action after every
    observation, STRAW generates a sequence of actions, thus leading to structured
    exploration. Experimental results indicate a significant improvement in Atari
    games with STRAW. Value Propagation (VProp) [[91](#bib.bib91)] is a set of Value
    Iteration-based planning modules trained using RL and capable of solving unseen
    tasks and navigating in complex environments. It is also demonstrated that VProp
    is able to generalise in a dynamic and noisy environment. Authors in [[92](#bib.bib92)]
    present a model-based algorithm named MuZero that combines tree-based search with
    a learned model to render superhuman performance in challenging environments.
    Experimental results demonstrate that MuZero delivers state-of-the-art performance
    on 57 diverse Atari games. Table  [II](#S3.T2 "TABLE II ‣ III-B Policy Gradient-Based
    DRL ‣ III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications") presents an overview of DRL algorithms for a reader’s
    glance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Kaiser 等人[[87](#bib.bib87)] 提出了模拟策略学习（SimPLe），这是一种基于视频预测的模型化深度强化学习算法，比起无模型算法，它需要的代理-环境交互次数要少得多。实验结果表明，SimPLe
    在 Atari 游戏中的表现优于最先进的无模型算法。Farquhar 等人[[88](#bib.bib88)] 提出了 TreeQN 用于复杂环境，其中转移模型并没有明确给出。所提算法结合了无模型和模型化方法，以基于通过隐式转移模型递归构建的动态树来估计
    Q 值。[[88](#bib.bib88)] 的作者还提出了一种名为 ATreeC 的演员-评论家变体，它在 TreeQN 的基础上增加了一个 softmax
    层，形成一个随机策略网络。他们展示了这两种算法在多个 Atari 游戏中比 n 步 DQN 和价值预测网络[[89](#bib.bib89)] 表现更为优越。[[90](#bib.bib90)]
    的作者介绍了一个战略性注意力写手（STRAW），它通过学习宏观动作能够做出自然的决策。与每次观察后仅产生一个动作的最先进深度强化学习算法不同，STRAW 生成一系列动作，从而导致结构化探索。实验结果表明，STRAW
    在 Atari 游戏中的表现有显著提升。价值传播（VProp）[[91](#bib.bib91)] 是一套基于价值迭代的规划模块，通过强化学习进行训练，能够解决未见过的任务并在复杂环境中导航。还证明了
    VProp 在动态和嘈杂的环境中能够进行泛化。[[92](#bib.bib92)] 的作者提出了一种名为 MuZero 的模型化算法，它将基于树的搜索与学习模型相结合，在挑战性环境中表现出超人类水平的性能。实验结果表明，MuZero
    在 57 款多样化的 Atari 游戏中提供了最先进的表现。表 [II](#S3.T2 "TABLE II ‣ III-B Policy Gradient-Based
    DRL ‣ III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications") 提供了深度强化学习算法的概述，供读者浏览。
- en: IV Audio-Based DRL
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 基于音频的深度强化学习
- en: 'This section surveys related works where audio is a key element in the learning
    environments of DRL agents. An example scenario is a human speaking to a machine
    trained via DRL as in Figure [4](#S3.F4 "Figure 4 ‣ III-B Policy Gradient-Based
    DRL ‣ III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications"), where the machine has to act based on features
    derived from audio signals. Table [III](#S4.T3 "TABLE III ‣ IV Audio-Based DRL
    ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") summarises
    the characterisation of DRL agents for six audio-related areas:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 本节调查了音频在深度强化学习代理的学习环境中作为关键元素的相关工作。一个示例场景是人类与通过深度强化学习训练的机器进行对话，如图 [4](#S3.F4
    "Figure 4 ‣ III-B Policy Gradient-Based DRL ‣ III Deep Reinforcement Learning
    ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") 所示，其中机器必须根据从音频信号中提取的特征进行操作。表
    [III](#S4.T3 "TABLE III ‣ IV Audio-Based DRL ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications") 总结了六个与音频相关的领域中深度强化学习代理的特征：
- en: '1.'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: automatic speech recognition;
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动语音识别；
- en: '2.'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: spoken dialogue systems;
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语音对话系统；
- en: '3.'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: emotions modelling;
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 情感建模；
- en: '4.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: audio enhancement;
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 音频增强；
- en: '5.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: music listening and generation; and
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 音乐听觉与生成；以及
- en: '6.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: robotics, control and interaction.
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器人技术、控制与交互。
- en: 'TABLE III: Summary of audio related fields, characterisation of DRL agents,
    and related datasets.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 音频相关领域的总结、DRL 代理的特征描述以及相关数据集。'
- en: '| Appl. Area | State Representations $\mathcal{S}$, Actions $\mathcal{A}$,
    and Reward Functions $\mathcal{R}$ | Popular Datasets |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 应用领域 | 状态表示 $\mathcal{S}$、动作 $\mathcal{A}$ 和奖励函数 $\mathcal{R}$ | 常用数据集 |'
- en: '| --- | --- | --- |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Automatic &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动 &#124;'
- en: '&#124; Speech &#124;'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音 &#124;'
- en: '&#124; Recognition &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别 &#124;'
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{S}$: States are learnt representations from input speech features
    (e.g. fMLLR or MFCC vectors [[93](#bib.bib93)]). &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{S}$: 状态是从输入语音特征（例如 fMLLR 或 MFCC 向量 [[93](#bib.bib93)]）中学习到的表示。
    &#124;'
- en: '&#124; $\mathcal{A}$: Actions include phonemes, graphemes, commands, or candidates
    from the ASR N-best list. &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{A}$: 动作包括音素、字形、命令或来自 ASR N-best 列表的候选项。 &#124;'
- en: '&#124; $\mathcal{R}$: They have included binary rewards (positive for selecting
    the correct choice, 0 otherwise), &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{R}$: 他们包含了二进制奖励（选择正确的选项时为正，其他情况为0），&#124;'
- en: '&#124; and non-binary sentence/token rewards based on the Levenshtein distance
    algorithm. &#124;'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和基于 Levenshtein 距离算法的非二进制句子/标记奖励。 &#124;'
- en: '|'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  LibriSpeech [[94](#bib.bib94)] &#124;'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  LibriSpeech [[94](#bib.bib94)] &#124;'
- en: '&#124; •  TED-LIUM [[95](#bib.bib95)] &#124;'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  TED-LIUM [[95](#bib.bib95)] &#124;'
- en: '&#124; •  Wall Street Journal [[96](#bib.bib96)] &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  华尔街日报 [[96](#bib.bib96)] &#124;'
- en: '&#124; •  SWITCHBOARD [[97](#bib.bib97)] &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  SWITCHBOARD [[97](#bib.bib97)] &#124;'
- en: '&#124; •  TIMIT [[98](#bib.bib98)] &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  TIMIT [[98](#bib.bib98)] &#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Spoken &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 口语 &#124;'
- en: '&#124; Dialogue &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对话 &#124;'
- en: '&#124; Systems &#124;'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统 &#124;'
- en: '|'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{S}$: They encode the uttered words by the system and recognised
    user words into a dialogue history &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{S}$: 他们将系统发出的词语和识别的用户词语编码成对话历史 &#124;'
- en: '&#124; and some additional information from classifiers such as user goals,
    user intents, speech recognition &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以及来自分类器的额外信息，如用户目标、用户意图、语音识别 &#124;'
- en: '&#124; confidence scores, and visual information (in the case of multimodal
    systems), among others. &#124;'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 置信度分数和视觉信息（在多模态系统中）等。 &#124;'
- en: '&#124; $\mathcal{A}$: While actions in task-oriented systems include slot requests/confirmations/apologies,
    slot-value &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{A}$: 在任务导向系统中，动作包括槽请求/确认/道歉、槽值 &#124;'
- en: '&#124; selection, ask question, data retrieval, information presentation, among
    others; actions in open-ended &#124;'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择、提问、数据检索、信息呈现等；开放式系统中的动作 &#124;'
- en: '&#124; systems include either all possible sentences (infinite) or clusters
    of sentences (finite). &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统包括所有可能的句子（无限）或句子集群（有限）。 &#124;'
- en: '&#124; $\mathcal{R}$: They vary depending on the company/project requirements
    and tend to include sparse and &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{R}$: 奖励函数根据公司/项目需求有所不同，倾向于包括稀疏和 &#124;'
- en: '&#124; non-sparse numerical rewards such as dialogue length, task success,
    dialogue similarity, dialogue &#124;'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非稀疏的数值奖励，如对话长度、任务成功、对话相似度、对话 &#124;'
- en: '&#124; coherence, dialogue repetitiveness, game scores (in the case of game-based
    systems), among others. &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一致性、对话重复性、游戏得分（在基于游戏的系统中）等。 &#124;'
- en: '|'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  SGD [[99](#bib.bib99)] &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  SGD [[99](#bib.bib99)] &#124;'
- en: '&#124; •  DSTC [[100](#bib.bib100)] &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  DSTC [[100](#bib.bib100)] &#124;'
- en: '&#124; •  Frames [[101](#bib.bib101)] &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  Frames [[101](#bib.bib101)] &#124;'
- en: '&#124; •  MultiWOZ [[102](#bib.bib102)] &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  MultiWOZ [[102](#bib.bib102)] &#124;'
- en: '&#124; •  SubTle Corpus [[103](#bib.bib103)] &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  SubTle 语料库 [[103](#bib.bib103)] &#124;'
- en: '&#124; •  Simulations [[104](#bib.bib104)] &#124;'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  模拟 [[104](#bib.bib104)] &#124;'
- en: '&#124; •  Other datasets [[105](#bib.bib105)] &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  其他数据集 [[105](#bib.bib105)] &#124;'
- en: '|'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Speech &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音 &#124;'
- en: '&#124; Emotion &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 情感 &#124;'
- en: '&#124; Recognition &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 识别 &#124;'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{S}$: Speech features (e.g., MFCC) are considered as input
    features. &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{S}$: 语音特征（例如，MFCC）被视为输入特征。 &#124;'
- en: '&#124; $\mathcal{A}$: Actions include speech emotion labels (e.g. unhappy,
    neutral, happy), sentiment detection &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{A}$: 动作包括语音情感标签（例如，不高兴、中性、高兴）、情感检测 &#124;'
- en: '&#124; (e.g. negative, neutral, positive), and termination from utterance listening.
    &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （例如，负面、中性、积极），以及从语音听取中终止。 &#124;'
- en: '&#124; $\mathcal{R}$: Binary reward functions have been used (positive for
    choosing the correct choice, 0 otherwise). &#124;'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{R}$: 使用了二进制奖励函数（选择正确选项时为正，其他情况为0）。 &#124;'
- en: '|'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  EMODB [[106](#bib.bib106)] &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  EMODB [[106](#bib.bib106)] &#124;'
- en: '&#124; •  IEMOCAP [[107](#bib.bib107)] &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  IEMOCAP [[107](#bib.bib107)] &#124;'
- en: '&#124; •  MSP-IMPROV [[108](#bib.bib108)] &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  MSP-IMPROV [[108](#bib.bib108)] &#124;'
- en: '&#124; •  SEMAINE [[109](#bib.bib109)] &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  SEMAINE [[109](#bib.bib109)] &#124;'
- en: '&#124; •  MELD [[110](#bib.bib110)] &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  MELD [[110](#bib.bib110)] &#124;'
- en: '|'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Audio &#124;'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 音频 &#124;'
- en: '&#124; Enhancement &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 增强 &#124;'
- en: '|'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{S}$: States are learnt from clean and noisy acoustic features.
    &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{S}$: 状态从干净和嘈杂的声学特征中学习。 &#124;'
- en: '&#124; $\mathcal{A}$: Finding closest cluster and its index, time-frequency
    mask estimation, and increasing or decreasing &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{A}$: 寻找最近的簇及其索引，时间频率掩膜估计，以及增加或减少 &#124;'
- en: '&#124; the parameter values of the speech-enhancement algorithm. &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语音增强算法的参数值。 &#124;'
- en: '&#124; $\mathcal{R}$: Positive rewards for correct choice, negative otherwise.
    &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{R}$: 正确选择的正奖励，其他情况为负奖励。 &#124;'
- en: '|'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  DEMAND [[111](#bib.bib111)] &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  DEMAND [[111](#bib.bib111)] &#124;'
- en: '&#124; •  CHiME-3 [[112](#bib.bib112)] &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  CHiME-3 [[112](#bib.bib112)] &#124;'
- en: '&#124; •  WHAMR [[113](#bib.bib113)] &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  WHAMR [[113](#bib.bib113)] &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Music &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐 &#124;
- en: '&#124; Generation &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 生成 &#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{S}$: State representations are learned from Musical notes.
    &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{S}$: 状态表示从音乐音符中学习。 &#124;'
- en: '&#124; $\mathcal{A}$: Musical generation and next note selection are considered
    as actions. &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{A}$: 音乐生成和下一个音符选择被视为行为。 &#124;'
- en: '&#124; $\mathcal{R}$: Binary reward functions based on hard-coded musical theory
    rules, including the likelihood of actions. &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{R}$: 基于硬编码音乐理论规则的二元奖励函数，包括行为的可能性。 &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  Classical piano MIDI &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  古典钢琴 MIDI &#124;'
- en: '&#124; database [[114](#bib.bib114)] &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据库 [[114](#bib.bib114)] &#124;'
- en: '&#124; •  MusicNet dataset [[115](#bib.bib115)] &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  MusicNet 数据集 [[115](#bib.bib115)] &#124;'
- en: '&#124; •  JSB Chorales &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  JSB 复调曲 &#124;'
- en: '&#124; dataset [[116](#bib.bib116)] &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 [[116](#bib.bib116)] &#124;'
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Robotics, &#124;'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 机器人学, &#124;'
- en: '&#124; Control and &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 控制和 &#124;'
- en: '&#124; Interaction &#124;'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动 &#124;'
- en: '|'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\mathcal{S}$: They encode visual and verbal representations derived
    from image embeddings, speech features, &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{S}$: 它们编码从图像嵌入、语音特征中得出的视觉和语言表征， &#124;'
- en: '&#124; and word or sentence embeddings. Additional information include user
    intents, speech recognition &#124;'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 词或句子的嵌入。附加信息包括用户意图、语音识别 &#124;'
- en: '&#124; scores, human activities, postures, emotions, and body joint angles,
    among others. &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分数、人体活动、姿势、情感和身体关节角度等。 &#124;'
- en: '&#124; $\mathcal{A}$: They include motor commands (e.g. gestures, locomotion,
    navigation, manipulation, gaze) and &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{A}$: 包括运动命令（例如手势、运动、导航、操作、注视）和 &#124;'
- en: '&#124; verbalizations such as dialogue acts and backchannels (e.g. laughs,
    smiles, noddings, head-shakes). &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 语言化，如对话行为和回话（例如笑声、微笑、点头、摇头）。 &#124;'
- en: '&#124; $\mathcal{R}$: They are based on task success (positive rewards for
    achieving the goal, negative rewards for &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\mathcal{R}$: 基于任务成功（达到目标的正奖励，未达到的负奖励）。 &#124;'
- en: '&#124; failing the task, and zero/shaped rewards otherwise) and user engagement.
    &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完成任务失败，其他情况为零/形状奖励) 和用户参与。 &#124;'
- en: '|'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; •  AVDIAR [[117](#bib.bib117)] &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  AVDIAR [[117](#bib.bib117)] &#124;'
- en: '&#124; •  NLI Corpus [[118](#bib.bib118)] &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  NLI 语料库 [[118](#bib.bib118)] &#124;'
- en: '&#124; •  VEIL dataset [[119](#bib.bib119)] &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  VEIL 数据集 [[119](#bib.bib119)] &#124;'
- en: '&#124; •  Simulations [[120](#bib.bib120)] &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; •  模拟 [[120](#bib.bib120)] &#124;'
- en: '&#124; •  Real-world &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; • 现实世界 &#124;'
- en: '&#124; interactions [[121](#bib.bib121)] &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 互动 [[121](#bib.bib121)] &#124;'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: IV-A Automatic Speech Recognition (ASR)
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 自动语音识别 (ASR)
- en: Automatic speech recognition (ASR) is the process of converting a speech signal
    into its corresponding text by algorithms. Contemporary ASR technology has reached
    great levels of performance due to advancements in DL models. The performance
    of ASR systems, however, relies heavily on supervised training of deep models
    with large amounts of transcribed data. Even for resource-rich languages, additional
    transcription costs required for new tasks hinders the applications of ASR. To
    broaden the scope of ASR, different studies have attempted RL based models with
    the ability to learn from feedback. This form of learning aims to reduce transcription
    costs and time by humans providing positive or negative rewards instead of detailed
    transcriptions. For instance, Kala et al. [[122](#bib.bib122)] proposed an RL
    framework for ASR based on the policy gradient method that provides a new view
    of existing training and adaptation methods. They achieved improved recognition
    performance and reduced Word Error Rate (WER) compared to unsupervised adaptation.
    In ASR, sequence-to-sequence models have shown great success; however, these models
    fail to approximate real-world speech during inference. Tjandra et al. [[123](#bib.bib123)]
    solved this issue by training a sequence-to-sequence model with a policy gradient
    algorithm. Their results showed a significant improvement using an RL-based objective
    and a maximum likelihood estimation (MLE) objective compared to the model trained
    with only the MLE objective. In another study, [[124](#bib.bib124)] extended their
    own work by providing more details on their model and experimentation. They found
    that using token-level rewards (intermediate rewards are given after each time
    step) provide improved performance compared to sentence-level rewards and baseline
    systems. In order solve the issues of semi-supervised training of sequence-to-sequence
    ASR models, Chung et al. [[125](#bib.bib125)] investigated the REINFORCE algorithm
    by rewarding the ASR to output more correct sentences for both unpaired and paired
    speech input data. Experimental evaluations showed that the DRL-based method was
    able to effectively reduce character error rates from 10.4% to 8.7%.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 自动语音识别（ASR）是将语音信号转换为相应文本的过程，依靠算法实现。现代ASR技术因深度学习模型的进步而达到很高的性能水平。然而，ASR系统的性能严重依赖于大量转录数据的深度模型的监督训练。即使对于资源丰富的语言，新的任务所需的额外转录成本仍然阻碍了ASR的应用。为了扩大ASR的范围，不同的研究尝试了基于RL的模型，这些模型能够从反馈中学习。这种学习形式旨在通过人类提供正面或负面的奖励来减少转录成本和时间，而不是详细的转录。比如，Kala等人[[122](#bib.bib122)]
    提出了一个基于策略梯度方法的ASR RL框架，为现有的训练和适应方法提供了新的视角。他们在识别性能和Word Error Rate（WER）方面取得了改进，相较于无监督适应。ASR中，序列到序列模型表现出很大的成功；然而，这些模型在推断过程中未能逼近真实世界的语音。Tjandra等人[[123](#bib.bib123)]通过用策略梯度算法训练序列到序列模型解决了这个问题。他们的结果显示，与仅用MLE目标训练的模型相比，使用基于RL的目标和最大似然估计（MLE）目标的结果显著改善。在另一项研究中，[[124](#bib.bib124)]
    通过提供更多关于其模型和实验的细节，扩展了他们自己的工作。他们发现，使用令牌级奖励（在每个时间步后给予中间奖励）相较于句子级奖励和基线系统，能够提供更好的性能。为了解决序列到序列ASR模型的半监督训练问题，Chung等人[[125](#bib.bib125)]
    研究了REINFORCE算法，通过奖励ASR输出更多正确的句子来处理未配对和配对的语音输入数据。实验评估显示，基于DRL的方法能够有效将字符错误率从10.4%降低到8.7%。
- en: Karita et al. [[126](#bib.bib126)] propose to train an encoder-decoder ASR system
    using a sequence-level evaluation metric based on the policy gradient objective
    function. This enables the minimisation of the expected WER of the model predictions.
    In this way, the authors found that the proposed method improves recognition performance.
    The ASR system of [[127](#bib.bib127)] was jointly trained with maximum likelihood
    and policy gradient to improve via end-to-end learning. They were able to optimise
    the performance metric directly and achieve 4% to 13% relative performance improvement.
    In [[128](#bib.bib128)], the authors attempted to solve sequence-to-sequence problems
    by proposing a model based on supervised backpropagation and a policy gradient
    method, which can directly maximise the log probability of the correct answer.
    They achieved very encouraging results on a small scale and a medium scale ASR.
    Radzikowski et al. [[129](#bib.bib129)] proposed a dual supervised model based
    on a policy gradient methodology for non-native speech recognition. They were
    able to achieve promising results for the English language pronounced by Japanese
    and Polish speakers.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: Karita 等人[[126](#bib.bib126)] 提出了使用基于策略梯度目标函数的序列级评估指标来训练编码器-解码器 ASR 系统。这使得模型预测的期望
    WER 最小化。通过这种方式，作者发现所提方法提高了识别性能。[[127](#bib.bib127)] 的 ASR 系统通过最大似然和策略梯度联合训练，以通过端到端学习进行改进。他们能够直接优化性能指标，并实现了
    4% 到 13% 的相对性能提升。在 [[128](#bib.bib128)] 中，作者通过提出一种基于监督反向传播和策略梯度方法的模型来解决序列到序列问题，该模型可以直接最大化正确答案的对数概率。他们在小规模和中等规模的
    ASR 上取得了非常令人鼓舞的结果。Radzikowski 等人[[129](#bib.bib129)] 提出了一个基于策略梯度方法的双重监督模型用于非母语语音识别。他们能够在由日本和波兰说话者发音的英语中取得有希望的结果。
- en: To achieve the best possible accuracy, end-to-end ASR systems are becoming increasingly
    large and complex. DRL methods can also be leveraged to provide model compression [[130](#bib.bib130)].
    In [[131](#bib.bib131)], RL-based ShrinkML is proposed to optimise the per-layer
    compression ratios in a state-of-the-art LSTM-based ASR model with attention.
    For time-efficient ASR, [[132](#bib.bib132)] evaluated the pre-training of an
    RL-based policy gradient network. They found that pre-training in DRL offers faster
    convergence compared to non-pre-trained networks, and also achieve improved recognition
    in lesser time. To tackle the slow convergence time of the REINFORCE algorithm [[133](#bib.bib133)],
    Lawson et al. [[134](#bib.bib134)], evaluated Variational Inference for Monte
    Carlo Objectives (VIMCO) and Neural Variational Inference (NVIL) for phoneme recognition
    tasks in clean and noisy environments. The authors found that the proposed method
    (using VIMCO and NVIL) outperforms REINFORCE and other methods at training online
    sequence-to-sequence models.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳的准确度，端到端 ASR 系统变得越来越大和复杂。DRL 方法还可以用于模型压缩[[130](#bib.bib130)]。在[[131](#bib.bib131)]中，提出了基于
    RL 的 ShrinkML 来优化具有注意力机制的最先进的 LSTM 基于 ASR 模型中的每层压缩比。为了提高时间效率的 ASR，[[132](#bib.bib132)]
    评估了基于 RL 的策略梯度网络的预训练。他们发现，DRL 的预训练相比未预训练网络提供了更快的收敛，并且在更短时间内实现了更好的识别。为了应对 REINFORCE
    算法的缓慢收敛时间[[133](#bib.bib133)]，Lawson 等人[[134](#bib.bib134)] 评估了变分推断 Monte Carlo
    目标（VIMCO）和神经变分推断（NVIL）在干净和嘈杂环境中的音素识别任务。作者发现，所提方法（使用 VIMCO 和 NVIL）在训练在线序列到序列模型方面优于
    REINFORCE 和其他方法。
- en: All of the above-mentioned studies highlight several benefits of using DRL for
    ASR. Despite these promising results, further research is required on DRL algorithms
    towards building autonomous ASR systems that can work in complex real-life settings.
    REINFORCE algorithm is very popular in ASR, therefore, research is also required
    to explore other DRL algorithms to highlights suitability for ASR.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 上述研究突出了使用 DRL 在 ASR 中的几个好处。尽管这些结果很有前景，但仍需进一步研究 DRL 算法，以建立能够在复杂现实环境中工作的自主 ASR
    系统。REINFORCE 算法在 ASR 中非常流行，因此还需要研究其他 DRL 算法，以突显其在 ASR 中的适用性。
- en: IV-B Spoken Dialogue Systems (SDSs)
  id: totrans-421
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 口语对话系统 (SDSs)
- en: Spoken dialogue systems are gaining interest due to many applications in customer
    services and goal-oriented human-computer-interaction. Typical SDSs integrate
    several key components including speech recogniser, intent recogniser, knowledge
    base and/or database backend, dialogue manager, language generator, and speech
    synthesis, among others [[135](#bib.bib135)]. The task of a dialogue manager in
    SDSs is to select actions based on observed events [[136](#bib.bib136), [137](#bib.bib137)].
    Researchers have shown that the action selection process can be effectively optimised
    using RL to model the dynamics of spoken dialogue as a fully or partially observable
    Markov Decision Process [[138](#bib.bib138)]. Numerous studies have utilised RL-based
    algorithms in spoken dialogue systems. In contrast to text-based dialogue systems
    that can be trained directly using large amounts of text data [[139](#bib.bib139)],
    most SDSs have been trained using user simulations [[104](#bib.bib104)]. The justification
    for that is mainly due to insufficient amounts of training dialogues to train
    or test from real data [[105](#bib.bib105)].
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 语音对话系统因其在客户服务和目标导向的人机交互中的众多应用而引起了关注。典型的语音对话系统集成了多个关键组件，包括语音识别器、意图识别器、知识库和/或数据库后端、对话管理器、语言生成器和语音合成等[[135](#bib.bib135)]。对话管理器在语音对话系统中的任务是根据观察到的事件选择行动[[136](#bib.bib136),
    [137](#bib.bib137)]。研究人员已经证明，可以通过使用强化学习（RL）来优化行动选择过程，从而将语音对话的动态建模为完全或部分可观测的马尔可夫决策过程[[138](#bib.bib138)]。大量研究在语音对话系统中利用了基于强化学习的算法。与可以直接利用大量文本数据进行训练的基于文本的对话系统相比，大多数语音对话系统都是使用用户模拟进行训练的[[104](#bib.bib104)]。这样做的理由主要是由于用于训练或测试的实际对话数据量不足[[105](#bib.bib105)]。
- en: SDSs involve policy optimisation to respond to humans by taking the current
    state of the dialogue, selecting an action, and returning the verbal response
    of the system. For instance, Chen et al. [[140](#bib.bib140)] presented an online
    DRL-based dialogue state tracking framework in order to improve the performance
    of a dialogue manager. They achieved promising results for online dialogue state
    tracking in the second and third dialog state tracking challenges ( [[141](#bib.bib141),
    [142](#bib.bib142)]). Weisz et al. [[143](#bib.bib143)] utilised DRL approaches,
    including actor-critic methods and off-policy RL. They also evaluated actor-critic
    with experience replay (ACER) [[144](#bib.bib144), [81](#bib.bib81)], which has
    shown promising results on simple gaming tasks. They showed that the proposed
    method is sample efficient and that performed better than some state-of-the-art
    DL approaches for spoken dialogue. A task-oriented end-to-end DRL-based dialogue
    system is proposed in [[145](#bib.bib145)]. They showed that DRL-based optimisation
    produced significant improvement in task success rate and also caused a reduction
    in dialogue length compared to supervised training. Zhao et al. [[146](#bib.bib146)]
    utilised deep recurrent Q-networks (DRQN) for dialogue state tracking and management.
    Experimental results showed that the proposed model can exploit the strengths
    of DRL and supervised learning to achieve faster learning speed and better results
    than the modular-based baseline system. To present baseline results, a benchmark
    study [[147](#bib.bib147)] is performed using DRL algorithms including DQN, A2C
    and natural actor-critic [[148](#bib.bib148)] and their performance is compared
    against GP-SARSA [[149](#bib.bib149)]. Based on experimental results on the PyDial
    toolkit [[150](#bib.bib150)], the authors conclude that substantial improvements
    are still needed for DRL methods to match the performance of carefully designed
    handcrafted policies. In addition to SDSs optimised via flat DRL, hierarchical
    RL/DRL methods have been proposed for policy learning using dialogue states with
    different levels of abstraction and dialogue actions at different levels of granularity
    (via primitive and composite actions) [[151](#bib.bib151), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156)].
    The benefits of this form of learning include faster training and policy reuse.
    A deep Q-network based multi-domain dialogue system is proposed in [[157](#bib.bib157)].
    They train the proposed SDS using a network of DQN agents, which is similar to
    hierarchical DRL but with more flexibility for transitioning across dialogues
    domains. Another work related to faster training is proposed by [[158](#bib.bib158)],
    where the behaviour of RL agents is guided by expert demonstrations.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: SDSs 涉及策略优化，通过获取对话的当前状态，选择一个动作，并返回系统的语言响应。例如，Chen 等人[[140](#bib.bib140)] 提出了一个基于在线
    DRL 的对话状态跟踪框架，以提高对话管理器的性能。他们在第二和第三次对话状态跟踪挑战中取得了有希望的结果（[[141](#bib.bib141), [142](#bib.bib142)]）。Weisz
    等人[[143](#bib.bib143)] 使用了 DRL 方法，包括演员-评论家方法和离线策略 RL。他们还评估了带有经验回放的演员-评论家方法（ACER）[[144](#bib.bib144),
    [81](#bib.bib81)]，在简单的游戏任务中表现出良好的结果。他们展示了所提出的方法在样本效率方面的优势，并且在口语对话的某些最先进 DL 方法中表现更好。在[[145](#bib.bib145)]中提出了一个基于任务导向的端到端
    DRL 对话系统。他们展示了基于 DRL 的优化在任务成功率方面取得了显著改进，并且相比于监督训练还减少了对话长度。Zhao 等人[[146](#bib.bib146)]
    利用深度递归 Q 网络（DRQN）进行对话状态跟踪和管理。实验结果表明，所提出的模型可以利用 DRL 和监督学习的优势，实现更快的学习速度和比模块化基准系统更好的结果。为了展示基准结果，[[147](#bib.bib147)]
    进行了基于 DRL 算法（包括 DQN、A2C 和自然演员-评论家[[148](#bib.bib148)]）的基准研究，并将其性能与 GP-SARSA[[149](#bib.bib149)]
    进行了比较。基于 PyDial 工具包[[150](#bib.bib150)] 的实验结果，作者总结出 DRL 方法仍需大量改进，以匹配精心设计的手工制作策略的性能。除了通过平面
    DRL 优化的 SDSs，已经提出了层次 RL/DRL 方法用于策略学习，这些方法利用具有不同抽象层次的对话状态和不同粒度的对话动作（通过原始动作和复合动作）[[151](#bib.bib151),
    [152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155),
    [156](#bib.bib156)]。这种学习形式的好处包括更快的训练和策略重用。[[157](#bib.bib157)] 提出了一个基于深度 Q 网络的多领域对话系统。他们使用
    DQN 代理网络训练所提出的 SDS，这类似于层次 DRL，但在对话领域之间的过渡上提供了更多灵活性。与更快训练相关的另一项工作由[[158](#bib.bib158)]
    提出，其中 RL 代理的行为由专家演示指导。
- en: 'The optimisation of dialogue policies requires a reward function that unfortunately
    is not easy to specify. This often requires annotated data for training a reward
    predictor instead of a hand-crafted one. In real-world applications, such annotations
    are either scarce or not available. Therefore, some researchers have turner their
    attention to methods for online active reward learning. In [[159](#bib.bib159)],
    the authors presented an online learning framework for a spoken dialogue system.
    They jointly trained the dialogue policy alongside the reward model via active
    learning. Based on the results, the authors showed that the proposed framework
    can significantly reduce data annotation costs and can also mitigate noisy user
    feedback in dialogue policy learning. Su et al. [[148](#bib.bib148)] introduced
    two approaches: trust region actor-critic with experience replay (TRACER) and
    episodic natural actor-critic with experience replay (eNACER) for dialogue policy
    optimisation. From these two algorithms, they achieved the best performance using
    TRACER.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 对话策略的优化需要一个奖励函数，而这个奖励函数不幸的是不易指定。这通常需要带有标注的数据来训练奖励预测器，而不是手工制作的奖励函数。在实际应用中，这些标注数据要么稀缺，要么不可用。因此，一些研究人员将注意力转向了在线主动奖励学习的方法。在[[159](#bib.bib159)]中，作者提出了一个用于口语对话系统的在线学习框架。他们通过主动学习联合训练对话策略和奖励模型。根据结果，作者展示了该框架可以显著减少数据标注成本，并且可以缓解对话策略学习中的嘈杂用户反馈。Su
    等人[[148](#bib.bib148)]介绍了两种方法：带有经验回放的信任域演员-评论员 (TRACER) 和带有经验回放的情节自然演员-评论员 (eNACER)
    用于对话策略优化。在这两种算法中，他们使用 TRACER 获得了最佳性能。
- en: In [[160](#bib.bib160)], the authors propose to learn a domain-independent reward
    function based on user satisfaction for dialogue policy learning. The authors
    showed that the proposed framework yields good performance for both task success
    rate and user satisfaction. Researchers have also used DRL to learn dialogue policies
    in noisy environments, and some have shown that their proposed models can generate
    dialogues indistinguishable from human ones [[161](#bib.bib161)]. Carrara et al. [[162](#bib.bib162)]
    propose online learning and transfer for user adaptation in RL-based dialogue
    systems. Experiments were carried out on a negotiation dialogue task, which showed
    significant improvements over baselines. In another study [[163](#bib.bib163)],
    authors proposed $\epsilon$-safe, a Q-learning algorithm, for safe transfer learning
    for dialogue applications. A DRL-based chatbot called MILABOT was designed in [[164](#bib.bib164)],
    which can converse with humans on popular topics through both speech and text—performing
    significantly better than many competing systems. The text-based chatbot in [[165](#bib.bib165)]
    used an ensemble of DRL agents, and showed that training multiple dialogue agents
    performs better than a single agent.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[160](#bib.bib160)]中，作者提议基于用户满意度学习一个领域无关的奖励函数，用于对话策略学习。作者展示了该框架在任务成功率和用户满意度方面均表现良好。研究人员还利用
    DRL 在嘈杂环境中学习对话策略，有些人展示了他们提出的模型能够生成与人类对话不可区分的对话[[161](#bib.bib161)]。Carrara 等人[[162](#bib.bib162)]提出了基于
    RL 的对话系统的在线学习和转移以适应用户。实验在谈判对话任务上进行，显示出比基准显著的改进。在另一项研究中[[163](#bib.bib163)]，作者提出了
    $\epsilon$-safe，一种用于对话应用的安全转移学习的 Q-learning 算法。在[[164](#bib.bib164)]中设计了一种名为 MILABOT
    的基于 DRL 的聊天机器人，可以通过语音和文本与人类进行流行话题的对话——表现明显优于许多竞争系统。在[[165](#bib.bib165)]中的文本聊天机器人使用了一组
    DRL 代理，并显示出训练多个对话代理比训练单一代理效果更佳。
- en: 'TABLE IV: Summary of research papers on dialogue systems trained with DRL algorithms
    (?=information not available)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 基于 DRL 算法训练的对话系统研究论文总结（?=信息不可用）'
- en: '| Refe- | Application | DRL | User Si- | Transfer | Training | Human | Reward
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 参考- | 应用 | DRL | 用户满意度- | 转移 | 训练 | 人类 | 奖励 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| rence | Domain(s) | Algorithm | mulations | Learning | (Test) Data | Evaluation
    | Function |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 领域 | 算法 | 模拟 | 学习 | (测试) 数据 | 评估 | 函数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| [[166](#bib.bib166)] | Games: | KG-DQN | No | Yes | 40 (10) games | No |
    +1 for getting closer to the finish, -1 for |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| [[166](#bib.bib166)] | 游戏: | KG-DQN | 否 | 是 | 40 (10) 游戏 | 否 | +1 代表接近终点，-1
    代表 |'
- en: '| Slice of Life, Horror | extending the minimum steps, 0 otherwise |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 生活片段，恐怖 | 扩展最小步骤，其他情况为0 |'
- en: '| [[167](#bib.bib167)] | Restaurants, laptops | FDQN, | Yes | No | 4K ($0.5$K)
    dialogues | No | +20 if successful dialogue or 0 otherwise, |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] | 餐馆，笔记本电脑 | FDQN, | 是 | 否 | 4K ($0.5$K) 对话 | 否 | 成功对话时+20，否则为0，
    |'
- en: '| GP-Sarsa | minus dialogue length |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| GP-Sarsa | 减去对话长度 |'
- en: '| [[168](#bib.bib168)] | Restaurants | MADQN | Yes | Yes | 15K ($?$) dialogues
    | No | +1 if successful dialogue, |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| [[168](#bib.bib168)] | 餐馆 | MADQN | 是 | 是 | 15K ($?$) 对话 | 否 | 成功对话时+1， |'
- en: '| -0.05 at each dialogue turn |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 每轮对话-0.05 |'
- en: '| [[120](#bib.bib120)] | Chitchat | Ensemble | No | No | $\leq$64K (1k) dialogues
    | Yes | +1 for a human-like response, |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| [[120](#bib.bib120)] | 闲聊 | Ensemble | 否 | 否 | $\leq$64K (1k) 对话 | 是 | 人类般的响应+1，
    |'
- en: '| DQN | -1 for a randomly chosen response |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| DQN | 随机选择的响应-1 |'
- en: '| [[165](#bib.bib165)] | Robot playing | Competitive | Yes | No | 20K (3K)
    games | Yes | +5 for a game win, |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| [[165](#bib.bib165)] | 机器人玩耍 | 竞争性 | 是 | 否 | 20K (3K) 游戏 | 是 | 游戏获胜时+5， |'
- en: '| noughts & crosses | DQN | +1 for a draw, -5 for a loss |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 井字游戏 | DQN | 平局+1，失利-5 |'
- en: '| [[169](#bib.bib169)] | Restaurants, hotels | NDQN | Yes | No | 8.7K (1K)
    dialogues | No | $Pr$(TaskSuccess) plus $Pr$(Data-Like) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| [[169](#bib.bib169)] | 餐馆，酒店 | NDQN | 是 | 否 | 8.7K (1K) 对话 | 否 | $Pr$(任务成功)
    加上 $Pr$(数据类似) |'
- en: '| minus number of turns $\times-0.1$ |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 减去轮数 $\times-0.1$ |'
- en: '| [[170](#bib.bib170)] | Visual | Reinforce | No | No | 68K (9.5K) images |
    Yes | Euclidean distances between predicted and |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| [[170](#bib.bib170)] | 视觉 | Reinforce | 否 | 否 | 68K (9.5K) 图像 | 是 | 预测值与
    |'
- en: '| Question-Answering | target descriptions of the last 2 time steps |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 问答 | 目标描述的最后两个时间步 |'
- en: '| [[171](#bib.bib171)] | Restaurants | DA2C | Yes | No | 15K (0.5K) dialogues
    | No | +1 if successful dialogue, -0.03 at each turn, |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bib171)] | 餐馆 | DA2C | 是 | 否 | 15K (0.5K) 对话 | 否 | 成功对话时+1，每轮-0.03，
    |'
- en: '| -1 if unsuccessful dialogue or hangup |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 成功对话时-1，否则挂断 |'
- en: '| [[172](#bib.bib172)] | Movie chat | Dueling | Yes | No | 150K ($?$) sentences
    | No | +10 for correct recognition, -12 for incorrect |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| [[172](#bib.bib172)] | 电影聊天 | Dueling | 是 | 否 | 150K ($?$) 句子 | 否 | 正确识别时+10，错误时-12
    |'
- en: '| DDQN | recognition, smaller rewards for confirm/elicit |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| DDQN | 识别，确认/引导的奖励较少 |'
- en: '| [[158](#bib.bib158)] | MultiWoz | NDfQ | Yes | No | 11.4K (1K) dialogues
    | No | +100 for successfully completing the task, |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | MultiWoz | NDfQ | 是 | 否 | 11.4K (1K) 对话 | 否 | 成功完成任务时+100，
    |'
- en: '| -1 at each turn |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 每轮-1 |'
- en: '| [[173](#bib.bib173)] | Chitchat | DBCQ | No | No | 14.2$\times$2K ($?$) sentences
    | Yes | Weighted scores combining sentiment, asking, |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| [[173](#bib.bib173)] | 闲聊 | DBCQ | 否 | 否 | 14.2$\times$2K ($?$) 句子 | 是 |
    加权分数结合情感、提问， |'
- en: '|  | laughter, long dialogues, & sentence similarity |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  | 笑声，长对话，& 句子相似性 |'
- en: '| [[174](#bib.bib174)] | OpenSubtitles | Reinforce | No | No | 1̃0M (1K) sentences
    | Yes | Weighted scores combining ease of answering, |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| [[174](#bib.bib174)] | OpenSubtitles | Reinforce | 否 | 否 | 1̃0M (1K) 句子 |
    是 | 加权分数结合回答的难易程度， |'
- en: '| information flow, and semantic coherence |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 信息流和语义一致性 |'
- en: '| [[175](#bib.bib175)] | OpenSubtitles | Adversarial | No | No | $?$ ($?$)
    dialogues | Yes | Learnt rewards (binary classifier determining |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| [[175](#bib.bib175)] | OpenSubtitles | 对抗性 | 否 | 否 | $?$ ($?$) 对话 | 是 | 学习奖励（二分类器确定
    |'
- en: '| Reinforce | a machine- or human-generated dialogue) |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| Reinforce | 机器生成或人工生成的对话） |'
- en: '| [[176](#bib.bib176)] | Movie booking | BBQN | Yes | No | 20K (10K) dialogues
    | Yes | +40 if successful dialogue, -1 at each turn, |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| [[176](#bib.bib176)] | 电影预订 | BBQN | 是 | 否 | 20K (10K) 对话 | 是 | 成功对话时+40，每轮-1，
    |'
- en: '| -10 for a failed dialogue |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| 失败对话-10 |'
- en: '| [[177](#bib.bib177)] | Freeway, Bomberman, | Text-DQN, | No | Yes | 10M-15M
    (50K) steps | No | Learnt rewards (CNN network trained from |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| [[177](#bib.bib177)] | 高速公路，炸弹人， | 文本-DQN, | 否 | 是 | 10M-15M (50K) 步骤 | 否
    | 学习奖励（CNN网络从 |'
- en: '| Bourderchase, F&E | Text-VI | crowdsourced text descriptions of gameplays)
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Bourderchase, F&E | 文本-VI | 众包游戏玩法文本描述） |'
- en: '| [[155](#bib.bib155)] | Flights and hotels | Hierarchical | Yes | No | 20K
    (2K) dialogues | Yes | +120 if successful dialogue, -1 at each turn, |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| [[155](#bib.bib155)] | 航班和酒店 | 分层 | 是 | 否 | 20K (2K) 对话 | 是 | 成功对话时+120，每轮-1，
    |'
- en: '| DQN | -60 for a failed dialogue |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| DQN | 失败对话-60 |'
- en: '| [[178](#bib.bib178)] | Movie-ticket booking | Adversarial | Yes | No | 100K
    (5K) dialogues | No | Learnt rewards (MLP network comparing |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| [[178](#bib.bib178)] | 电影票预订 | 对抗性 | 是 | 否 | 100K (5K) 对话 | 否 | 学习奖励（MLP网络比较
    |'
- en: '| A2C | state-action pairs with human dialogues) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| A2C | 状态-动作对与人类对话） |'
- en: '| [[179](#bib.bib179)] | Chitchat | Hierarchical | No | No | 109K (10K) dialogues
    | Yes | Predefined scores combining question, |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| [[179](#bib.bib179)] | 闲聊 | 层次化 | 否 | 否 | 109K (10K) 对话 | 是 | 预定义分数组合问题，
    |'
- en: '| Reinforce | repetition, semantic similarity, and toxicity |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 强化 | 重复、语义相似性和毒性 |'
- en: '| [[180](#bib.bib180)] | Chitchat | Reinforce | No | No | $\sim$2M ($?$) dialogues
    | Yes | Positive reward from ease of answering - |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| [[180](#bib.bib180)] | 闲聊 | 强化 | 否 | 否 | $\sim$2M ($?$) 对话 | 是 | 从回答的简易性获得正奖励
    - |'
- en: '| negative reward for manual dull utterances |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 对手动枯燥话语的负奖励 |'
- en: '| [[181](#bib.bib181)] | Chitchat | Reinforce | No | No | $\sim$5K (0.1) dialogues
    | Yes | Learnt rewards (linear regressor predicting |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| [[181](#bib.bib181)] | 闲聊 | 强化 | 否 | 否 | $\sim$5K (0.1) 对话 | 是 | 学习奖励（线性回归预测
    |'
- en: '| user scores at the end of the dialogue) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 对话结束时的用户评分） |'
- en: '| [[182](#bib.bib182)] | Restaurants | TRACER, | Yes | No | $\leq$3.5K (0.6K)
    dialogues | No | +20 if successful dialogue (0 otherwise) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| [[182](#bib.bib182)] | 餐馆 | TRACER | 是 | 否 | $\leq$3.5K (0.6K) 对话 | 否 | 成功对话
    +20（否则为 0） |'
- en: '| eNACER | minus 0.05 $\times$ number of dialogue turns |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| eNACER | 减去 0.05 $\times$ 对话轮次数量 |'
- en: '| [[183](#bib.bib183)] | Buses, restaurants, | GP-Sarsa | Yes | No | 1K (0.1K)
    dialogues | No | Learnt rewards (Support Vector Machine |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| [[183](#bib.bib183)] | 公交车、餐馆 | GP-Sarsa | 是 | 否 | 1K (0.1K) 对话 | 否 | 学习奖励（支持向量机
    |'
- en: '| hotels, laptops | predicting user dialogue ratings) |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 酒店、笔记本电脑 | 预测用户对话评分） |'
- en: '| [[184](#bib.bib184)] | Medical diagnosis | KR-DQN | Yes | No | 423 (104)
    dialogues | Yes | +44 for successful diagnoses, -22 for failed |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| [[184](#bib.bib184)] | 医学诊断 | KR-DQN | 是 | 否 | 423 (104) 对话 | 是 | 成功诊断 +44，失败诊断
    -22 |'
- en: '| (4 diseases) | diagnoses, -1 for failing symptom requests |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| (4 种疾病) | 诊断，未满足症状请求 -1 |'
- en: '| [[185](#bib.bib185)] | Restaurants | ACER | Yes | No | 4K (4K) dialogues
    | Yes | +20 for a successful dialogue minus number |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| [[185](#bib.bib185)] | 餐馆 | ACER | 是 | 否 | 4K (4K) 对话 | 是 | 成功对话 +20 减去数量
    |'
- en: '| of turns in the dialogue |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 对话中的轮次 |'
- en: '| [[186](#bib.bib186)] | Dialling domain | Reinforce | Yes | No | 5K (0.5K)
    dialogues | No | +1 for successfully completing the dialogue, |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| [[186](#bib.bib186)] | 拨号领域 | 强化 | 是 | 否 | 5K (0.5K) 对话 | 否 | 成功完成对话 +1，
    |'
- en: '| 0 otherwise |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 其他为 0 |'
- en: '| [[187](#bib.bib187)] | 20-question game | DRQN | Yes | No | 120K (5K) sentences
    | No | +30 for a game win, -30 for a lost game, |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| [[187](#bib.bib187)] | 20 问题游戏 | DRQN | 是 | 否 | 120K (5K) 句子 | 否 | 游戏胜利 +30，游戏失败
    -30， |'
- en: '| -5 for a wrong guess |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 错误猜测 -5 |'
- en: '| [[188](#bib.bib188)] | DealOrNotDeal, | Reinforce | Yes;No | No | $\leq$8.4K
    ($\leq$1K) dialogues | No | +$\leq$10 for a negotiation, 0 for no agreement; |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| [[188](#bib.bib188)] | DealOrNotDeal | 强化 | 是;否 | 否 | $\leq$8.4K ($\leq$1K)
    对话 | 否 | 谈判 +$\leq$10，未达成协议 0； |'
- en: '| MultiWoz | language constrained reward curve |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| MultiWoz | 语言限制奖励曲线 |'
- en: '| [[156](#bib.bib156)] | 20 images | DRRN+ | Yes | No | 20K (1K) games | No
    | +10 for a game win, -10 for a lost game, |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| [[156](#bib.bib156)] | 20 张图像 | DRRN+ | 是 | 否 | 20K (1K) 游戏 | 否 | 游戏胜利 +10，游戏失败
    -10， |'
- en: '| guessing game | DQN | a pseudo reward for question selection |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| 猜测游戏 | DQN | 问题选择的伪奖励 |'
- en: '| [[189](#bib.bib189)] | MultiWoz | GP${}_{\mbox{mbcm}}$, PPO | Yes | No |
    10.5K (1K) dialogues | Yes | Learnt rewards (MLP network comparing |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| [[189](#bib.bib189)] | MultiWoz | GP${}_{\mbox{mbcm}}$, PPO | 是 | 否 | 10.5K
    (1K) 对话 | 是 | 学习奖励（MLP 网络比较 |'
- en: '| ACER, ALDM | state-action pairs with human dialogues) |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| ACER, ALDM | 与人类对话的状态-动作对） |'
- en: Table [IV](#S4.T4 "TABLE IV ‣ IV-B Spoken Dialogue Systems (SDSs) ‣ IV Audio-Based
    DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") shows
    a summary of DRL-based dialogue systems. While not all involve spoken interactions,
    they can be applied to speech-based systems by for example using the outputs from
    a speech recogniser instead of typed interactions. In terms of application, we
    can observe that most systems focus on one or a few domains—systems trained with
    a large amount of domains is usually not attempted, presumably due to the high
    requirements of data and compute involved. Regarding algorithms, the most popular
    are DQN-based or REINFORCE among other more recent algorithms—when to use one
    over another algorithm still needs to be understood better. We can also observe
    that user simulations are mostly used for training task-oriented dialogue systems,
    while real data is the preferred choice for open-ended dialogue systems. We can
    note that while transfer learning is an important component in a trained SDS,
    it is not common-place yet. Given that learning from scratch every time a system
    is trained is neither scalable nor practical, it looks like transfer learning
    will naturally be adopted more and more in the future as more domains are taken
    into account. In terms of datasets, most of them they are still in the small size.
    It is rare to see SDSs trained with millions of training dialogues or sentences.
    As datasets grow, the need for more efficient training methods will take more
    relevance in future systems. Regarding human evaluations, we can observe that
    about half of research works involve human evaluations. While human evaluations
    may not always be required to answer a research question, they certainly should
    be used whenever learnt conversational skills are being assessed or judged. We
    can also note that there is no standard for specifying reward functions due to
    the wide variety of functions used in previous works—almost every paper uses a
    different reward function. Even when some works use learnt reward functions (e.g.
    based on adversarial learning), they focus on learning to discriminate between
    machine-generated and human generated dialogues without taking other dimensions
    into account such as task success or additional penalties. Although there is advancement
    in the specification of reward functions by learning them instead of hand-crafting
    them, this area requires better understanding for optimising different types of
    dialogues including information-seeking, chitchat, game-based, negotiation-based,
    etc.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 表[IV](#S4.T4 "TABLE IV ‣ IV-B Spoken Dialogue Systems (SDSs) ‣ IV Audio-Based
    DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications")展示了基于DRL的对话系统的总结。虽然并非所有系统都涉及口语交互，但它们可以应用于语音系统，例如使用语音识别器的输出而非输入的文本交互。在应用方面，我们可以观察到，大多数系统集中在一个或少数几个领域——通常不会尝试用大量领域进行训练，可能是因为数据和计算的高需求。关于算法，最流行的是基于DQN或REINFORCE的算法以及其他一些较新的算法——何时使用一种算法而非另一种算法仍需更好地理解。我们还可以观察到，用户模拟主要用于训练任务导向的对话系统，而实际数据是开放式对话系统的首选。我们可以注意到，尽管迁移学习是训练SDS中的一个重要组成部分，但它尚未普及。由于每次系统训练时从头开始学习既不可扩展也不实用，因此迁移学习在未来将会越来越自然地被采用，因为将考虑更多领域。在数据集方面，大多数数据集仍然较小。看到用数百万训练对话或句子的SDS很少见。随着数据集的增长，对更高效训练方法的需求在未来系统中将变得更加重要。关于人工评估，我们可以观察到，大约一半的研究工作涉及人工评估。尽管人工评估可能并不总是回答研究问题所必需的，但在评估或判断学到的对话技能时肯定应该使用。我们还可以注意到，由于以前工作中使用的奖励函数种类繁多，尚无标准来指定奖励函数——几乎每篇论文使用不同的奖励函数。即使一些工作使用了学习到的奖励函数（例如基于对抗学习），它们也专注于学习区分机器生成的对话和人类生成的对话，而没有考虑其他维度，如任务成功或额外的惩罚。尽管通过学习而不是手工制作的奖励函数规格有所进展，但这一领域仍需更好地理解，以优化不同类型的对话，包括信息检索、闲聊、基于游戏的、基于谈判的等。
- en: IV-C Emotions Modelling
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 情感建模
- en: 'Emotions are essential in vocal human communication, and they have recently
    received growing interest by the research community [[190](#bib.bib190), [191](#bib.bib191),
    [39](#bib.bib39)]. Arguably, human-robot interaction can be significantly enhanced
    if dialogue agents can perceive the emotional state of a user and its dynamics [[192](#bib.bib192),
    [193](#bib.bib193)]. This line of research is categorised into two areas: emotion
    recognition in conversations [[194](#bib.bib194)], and affective dialogue generation [[195](#bib.bib195),
    [196](#bib.bib196)]. Speech emotion recognition (SER) can be used as a reward
    for RL based dialogue systems [[197](#bib.bib197)]. This would allow the system
    to adjust the behaviour based on the emotional states of the dialogue partner.
    Lack of labelled emotional corpora and low accuracy in SER are two major challenges
    in the field. To achieve the best possible accuracy, various DL-based methods
    have been applied to SER, however, performance improvement is still needed for
    real-time deployments. DRL offers different advantages to SER, as highlighted
    in different studies. In order to improve audio-visual SER performance, Ouyang
    et al. [[198](#bib.bib198)] presented a model-based RL framework that utilised
    feedback of testing results as rewards from environment to update the fusion weights.
    They evaluated the proposed model on the Multimodal Emotion Recognition Challenge
    (MEC 2017) dataset and achieved top 2 at the MEC 2017 Audio-Visual Challenge.
    To minimise the latency in SER, Lakomin et al. [[199](#bib.bib199)] proposed EmoRL
    for predicting the emotional state of a speaker as soon as it gains enough confidence
    while listening. In this way, EmoRL was able to achieve lower latency and minimise
    the need for audio segmentation required in DL-based approaches for SER. In [[200](#bib.bib200)],
    authors used RL with an adaptive fractional deep Belief network (AFDBN) for SER
    to enhance human-computer interaction. They showed that the combination of RL
    with AFDBN is efficient in terms of processing time and SER performance. Another
    study [[201](#bib.bib201)] utilised an LSTM-based gated multimodal embedding with
    temporal attention for sentiment analysis. They exploited the policy gradient
    method REINFORCE to balance exploration and optimisation by random sampling. They
    empirically show that the proposed model was able to deal with various challenges
    of understanding communication dynamics.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 情感在人际交流中至关重要，近年来在研究界获得了越来越多的关注[[190](#bib.bib190), [191](#bib.bib191), [39](#bib.bib39)]。可以说，如果对话代理能够感知用户的情感状态及其动态，人机交互可以显著提升[[192](#bib.bib192),
    [193](#bib.bib193)]。这一研究方向分为两个领域：对话中的情感识别[[194](#bib.bib194)]和情感对话生成[[195](#bib.bib195),
    [196](#bib.bib196)]。语音情感识别（SER）可以作为基于强化学习（RL）的对话系统的奖励[[197](#bib.bib197)]。这将允许系统根据对话伙伴的情感状态调整行为。缺乏标注的情感语料库和SER的低准确率是该领域的两个主要挑战。为了实现最佳准确率，各种基于深度学习（DL）的方法已被应用于SER，但实时部署的性能改进仍然是必要的。强化学习（DRL）在SER中提供了不同的优势，如不同研究所强调的那样。为了提高视听SER性能，Ouyang等人[[198](#bib.bib198)]提出了一种基于模型的RL框架，该框架利用测试结果的反馈作为来自环境的奖励来更新融合权重。他们在多模态情感识别挑战赛（MEC
    2017）数据集上评估了该模型，并在MEC 2017视听挑战赛中获得了前两名。为了最小化SER的延迟，Lakomin等人[[199](#bib.bib199)]提出了EmoRL，用于在获得足够的置信度时立即预测说话者的情感状态。通过这种方式，EmoRL能够实现更低的延迟，并最小化DL方法在SER中所需的音频分割。在[[200](#bib.bib200)]中，作者使用了带有自适应分数深度置信网络（AFDBN）的RL来提高SER，以增强人机交互。他们展示了RL与AFDBN的结合在处理时间和SER性能方面的高效性。另一项研究[[201](#bib.bib201)]利用基于LSTM的门控多模态嵌入和时间注意力进行情感分析。他们利用策略梯度方法REINFORCE通过随机采样来平衡探索和优化。他们通过实证研究表明，所提出的模型能够应对理解沟通动态的各种挑战。
- en: DRL is less popular in SER compared ASR and SDSs. The above mentioned studies
    attempted to helps solving different SER challenges using DRL, however, there
    is still a need for developing adaptive SER agents that can perform SER in cross-lingual
    settings.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于自动语音识别（ASR）和对话系统（SDS），DRL在SER中较少受到关注。上述研究尝试通过DRL解决不同的SER挑战，但仍需要开发能够在跨语言环境中执行SER的自适应SER代理。
- en: IV-D Audio Enhancement
  id: totrans-493
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 音频增强
- en: The performance of audio-based intelligent systems is critically vulnerable
    to noisy conditions and degrades according to the noise levels in the environment [[202](#bib.bib202)].
    Several approaches have been proposed [[203](#bib.bib203)] to address problems
    caused by environmental noise. One popular approach is audio enhancement, which
    aims to generate an enhanced audio signal from its noisy or corrupted version [[204](#bib.bib204)].
    DL-based speech enhancement has attained increased attention due to its superior
    performance compared to traditional methods [[205](#bib.bib205), [206](#bib.bib206)].
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 基于音频的智能系统的性能在噪声条件下极易受到影响，并且会随着环境中的噪声水平而下降[[202](#bib.bib202)]。已经提出了几种方法[[203](#bib.bib203)]
    来解决环境噪声造成的问题。一种流行的方法是音频增强，旨在从嘈杂或损坏的版本中生成增强的音频信号[[204](#bib.bib204)]。由于其相较于传统方法的优越性能，基于
    DL 的语音增强已获得越来越多的关注[[205](#bib.bib205), [206](#bib.bib206)]。
- en: In DL-based systems, the audio enhancement module is generally optimised separately
    from the main task such as minimisation of WER. Besides the speech enhancement
    module, there are different other units in speech-based systems which increase
    their complexity and make them non-differentiable. In such situations, DRL can
    achieve complex goals in an iterative manner, which makes it suitable for such
    applications. Such DRL-based approaches have been proposed in [[207](#bib.bib207)]
    to optimise the speech enhancement module based on the speech recognition results.
    Experimental results have shown that DRL-based methods can effectively improve
    the system’s performance by 12.4% and 19.2% error rate reductions for the signal
    to noise ratio at 0 dB and 5 dB, respectively. In [[208](#bib.bib208)], authors
    attempted to optimise DNN-based source enhancement using RL with numerical rewards
    calculated from conventional perceptual scores such as perceptual evaluation of
    speech quality (PESQ) [[209](#bib.bib209)] and perceptual evaluation methods for
    audio source separation (PEASS) [[210](#bib.bib210)]. They showed empirically
    that the proposed method can improve the quality of the output speech signals
    by using RL-based optimisation. Fakoor et al. [[211](#bib.bib211)] performed a
    study in an attempt to improve the adaptivity of speech enhancement methods via
    RL. They propose to model the noise-suppression module as a black box, requiring
    no knowledge of the algorithmic mechanics. Using an LSTM-based agent, they showed
    that their method improves system performance compared to methods with no adaptivity.
    In [[212](#bib.bib212)], the authors presented a DRL-based method to achieve personalised
    compression from noisy speech for a specific user in a hearing aid application.
    To deal with non-linearities of human hearing via the reward/punishment mechanism,
    they used a DRL agent that receives preference feedback from the target user.
    Experimental results showed that the developed approach achieved preferred hearing
    outcomes.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于 DL 的系统中，音频增强模块通常与主任务（如 WER 的最小化）分开优化。除了语音增强模块，语音系统中还有其他不同的单元，这些单元增加了系统的复杂性并使其不可微分。在这种情况下，DRL
    可以以迭代的方式实现复杂目标，这使其适用于此类应用。这样的基于 DRL 的方法已在[[207](#bib.bib207)]中提出，用于根据语音识别结果优化语音增强模块。实验结果表明，基于
    DRL 的方法可以有效提高系统性能，分别在信噪比为 0 dB 和 5 dB 时减少 12.4% 和 19.2% 的错误率。在[[208](#bib.bib208)]中，作者尝试使用
    RL 和从传统感知评分（如语音质量感知评估（PESQ）[[209](#bib.bib209)] 和音频源分离的感知评估方法（PEASS）[[210](#bib.bib210)]
    计算的数值奖励来优化基于 DNN 的源增强。他们通过实验证明，所提出的方法可以通过使用基于 RL 的优化来提高输出语音信号的质量。Fakoor 等人[[211](#bib.bib211)]
    进行了一项研究，试图通过 RL 改善语音增强方法的适应性。他们提出将噪声抑制模块建模为一个黑箱，无需了解算法的机制。通过使用基于 LSTM 的代理，他们展示了他们的方法在与没有适应性的方法相比时提高了系统性能。在[[212](#bib.bib212)]中，作者提出了一种基于
    DRL 的方法，以在助听器应用中实现针对特定用户的个性化压缩。为了通过奖励/惩罚机制处理人类听力的非线性，他们使用了一个接收目标用户偏好反馈的 DRL 代理。实验结果表明，该方法实现了用户期望的听力效果。
- en: Similar to SER, very few studies explored DRL for audio enhancement. Most of
    these studies evaluated DRL-based methods to achieve a certain level of signal
    enhancement in a controlled environment. Further research efforts are needed to
    develop DRL agents that can perform their tasks in real and complex noisy environments.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SER 类似，关于 DRL 在音频增强中的应用研究非常有限。这些研究大多数评估了基于 DRL 的方法在受控环境中实现某种程度的信号增强。还需要进一步的研究来开发能够在真实且复杂的噪声环境中执行任务的
    DRL 代理。
- en: IV-E Music Listening and Generation
  id: totrans-497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 音乐听觉与生成
- en: DL models are widely used for generating content including images, text, and
    music. The motivation for using DL for music generation lies in its generality
    since it can learn from arbitrary corpora of music and be able to generate various
    musical genres compared to classical methods [[213](#bib.bib213), [214](#bib.bib214)].
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）模型被广泛用于生成内容，包括图像、文本和音乐。使用深度学习进行音乐生成的动机在于其通用性，因为它可以从任意音乐语料库中学习，并且能够生成各种音乐类型，相比于经典方法[[213](#bib.bib213),
    [214](#bib.bib214)]。
- en: Here, DRL offers opportunities to impose rules of music theory for the generation
    of more real musical structures [[215](#bib.bib215)]. Various researchers have
    explored such opportunities of DRL for music generation. For instance, in [[216](#bib.bib216)]
    authors achieved better quantitative and qualitative results using an LSTM-based
    architecture in an RL setting generating polyphonic music aligned with musical
    rules. Jiang et al. [[217](#bib.bib217)] presented an interactive RL-Duet framework
    for real-time human-machine duet improvisation. The actor-critic with generalised
    advantage estimator (GAE) [[218](#bib.bib218)] based music generation agent was
    able to learn a policy to generate musical note based on the previous context.
    They trained the model on monophonic and polyphonic data and were able to generate
    high-quality musical pieces compared to a baseline method. Jaques et al. [[215](#bib.bib215)]
    utilised a deep Q-learning agent with a reward function based on rules of music
    theory and probabilistic outputs of an RNN. They showed that the proposed model
    can learn composition rules while maintaining the important information of data
    learned from supervised training. For audio-based generative models, it is often
    important to tune the generated samples towards some domain-specific metrics.
    To achieve this, Guimaraes et al. [[219](#bib.bib219)] proposed a method that
    combines adversarial training with RL. Specifically, they extend the training
    process of a GAN framework to include the domain-specific objectives in addition
    to the discriminator reward. Experimental results show that the proposed model
    can generate music while maintaining the information originally learned from data,
    and attained improvement in the desired metrics. In [[220](#bib.bib220)], they
    also used a GAN-based model for music generation and explored optimisation via
    RL. RaveForce [[221](#bib.bib221)] is a DRL-based environment for music generation,
    which can be used to search new synthesis parameters for a specific timbre of
    an electronic musical note or loop.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，深度强化学习（DRL）为音乐理论规则的施加提供了机会，以生成更真实的音乐结构[[215](#bib.bib215)]。各种研究者探讨了DRL在音乐生成中的应用机会。例如，在[[216](#bib.bib216)]中，作者在强化学习环境下使用基于LSTM的架构生成符合音乐规则的复调音乐，取得了更好的定量和定性结果。江等人[[217](#bib.bib217)]提出了一个实时人机二重奏即兴演奏的互动强化学习（RL-Duet）框架。基于带有广义优势估计器（GAE）[[218](#bib.bib218)]的演员-评论家音乐生成代理能够根据之前的上下文学习生成音乐音符的策略。他们在单声部和复调数据上训练模型，并能生成高质量的音乐作品，相比于基线方法。Jaques等人[[215](#bib.bib215)]利用基于音乐理论规则和RNN概率输出的奖励函数的深度Q学习代理。他们展示了该模型在保持从监督训练中学到的重要数据的同时，可以学习作曲规则。对于基于音频的生成模型，通常重要的是将生成的样本调整到一些特定领域的指标。为此，Guimaraes等人[[219](#bib.bib219)]提出了一种将对抗训练与强化学习相结合的方法。他们特别扩展了GAN框架的训练过程，以包括领域特定的目标以及判别器奖励。实验结果表明，所提出的模型可以在保持原始数据学习的信息的同时生成音乐，并在期望的指标上取得了改进。在[[220](#bib.bib220)]中，他们还使用基于GAN的模型进行音乐生成，并探索了通过强化学习进行优化。RaveForce[[221](#bib.bib221)]是一个基于DRL的音乐生成环境，可用于为特定音色的电子音乐音符或循环搜索新的合成参数。
- en: Score following is the process of tracking a musical performance for a known
    symbolic representation (a score). In [[222](#bib.bib222)], the authors modelled
    the score following task with DRL algorithms such as synchronous advantage actor-critic
    (A2C). They designed a multi-modal RL agent that listens to music, reads the score
    from an image, and follows the audio in an end-to-end fashion. Experiments on
    monophonic and polyphonic piano music showed promising results compared to state-of-the-art
    methods. The score following task is studied in [[223](#bib.bib223)] using the
    A2C and proximal policy optimisation (PPO). This study showed that the proposed
    approach could be applied to track real piano recordings of human performances.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 乐谱跟踪是指追踪音乐表现的过程，用已知的符号表示（乐谱）。在[[222](#bib.bib222)]中，作者使用如同步优势演员-评论家（A2C）等深度强化学习（DRL）算法建模了乐谱跟踪任务。他们设计了一个多模态强化学习代理，该代理能够听音乐、从图像中读取乐谱，并以端到端的方式跟随音频。在单音和复音钢琴音乐上的实验结果显示，相比于最先进的方法，取得了令人鼓舞的结果。乐谱跟踪任务在[[223](#bib.bib223)]中使用了A2C和近端策略优化（PPO）进行研究。该研究表明，所提出的方法可以用于跟踪真实的钢琴演奏录音。
- en: '![Refer to caption](img/6e458c45c415ddf57f1966ae2c5b2876.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6e458c45c415ddf57f1966ae2c5b2876.png)'
- en: 'Figure 5: A summary of audio-based DRL connecting the application areas and
    algorithms described in the previous two sections – the coloured circles correspond
    to the three groups of algorithms (from left to right: value-based, policy-based,
    model-based)'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：音频基础DRL的总结，连接了前两节中描述的应用领域和算法——彩色圆圈对应于三组算法（从左到右：基于价值的、基于策略的、基于模型的）
- en: IV-F Robotics, Control, and Interaction
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 机器人技术、控制与互动
- en: There is a recent growing research interest in robotics to enable robots with
    abilities such as recognition of users’ gestures and intentions [[224](#bib.bib224)],
    and generation of socially appropriate speech-based behaviours [[225](#bib.bib225)].
    In such applications, RL is suitable because robots are required to learn from
    rewards obtained from their actions. Different studies have explored different
    DRL-based approaches for audio and speech processing in robotics. Gao et al. [[226](#bib.bib226)]
    simulated an experiment for the acquisition of spoken-language to provide a proof-of-concept
    of Skinner’s idea [[227](#bib.bib227)], which states that children acquire language
    based on behaviourist reinforcement principles by associating words with meanings.
    Based on their results, the authors were able to show that acquiring spoken language
    is a combination of observing the environment, processing the observation, and
    grounding the observed inputs with their true meaning through a series of reinforcement
    attempts. In [[228](#bib.bib228)], authors build a virtual agent for language
    learning in a maze-like world. It interactively acquires the teacher’s language
    from question answering sentence-directed navigation. Some other studies [[229](#bib.bib229),
    [230](#bib.bib230), [231](#bib.bib231)] in this direction have also explored RL-based
    methods for spoken language learning.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 最近对机器人技术的研究兴趣不断增长，旨在使机器人具备诸如识别用户手势和意图[[224](#bib.bib224)]、生成社会适当的基于语言的行为[[225](#bib.bib225)]等能力。在这些应用中，强化学习（RL）非常适合，因为机器人需要从其行动获得的奖励中学习。不同的研究探索了不同的基于DRL的音频和语音处理方法。高等人[[226](#bib.bib226)]模拟了一个语言学习实验，以证明斯金纳的理论[[227](#bib.bib227)]，即儿童通过将词语与意义联系起来，根据行为主义强化原则获得语言。基于他们的结果，作者们展示了获取语言的过程是观察环境、处理观察结果，并通过一系列强化尝试将观察到的输入与其真实意义对接。在[[228](#bib.bib228)]中，作者在迷宫般的世界中构建了一个用于语言学习的虚拟代理。该代理通过问题回答的句子导向导航互动地获取教师的语言。一些其他的研究[[229](#bib.bib229),
    [230](#bib.bib230), [231](#bib.bib231)]也探讨了基于RL的语言学习方法。
- en: In human-robot interaction, researchers have used audio-driven DRL for robot
    gaze control and dialogue management. In [[232](#bib.bib232)], the authors used
    Q-learning with DNNs for audio-visual gaze control with the specific goal of finding
    good policies to control the orientation of a robot head towards groups of people
    using audio-visual information. Similarly, authors of [[233](#bib.bib233)] used
    a deep Q-network taking into account visual and acoustic observations to direct
    the robot’s head towards targets of interest. Based on the results, the authors
    showed that the proposed framework generates state-of-the-art results. Clark et
    al. [[234](#bib.bib234)] proposed an end-to-end learning framework that can induce
    generalised and high-level rules of human interactions from structured demonstrations.
    They empirically show that the proposed model was able to identify both auditory
    and gestural responses correctly. Another interesting work [[235](#bib.bib235)]
    utilised a deep Q-network for speech-driven backchannels like laugh generation
    to enhance engagement in human-robot interaction. Based on their experiments,
    they found that the proposed method has the potential of training a robot for
    engaging behaviours. Similarly, [[236](#bib.bib236)] utilised recurrent Q-learning
    for backchannel generation to engage agents during human-robot interaction. They
    showed that an agent trained using off-policy RL produces more engagement than
    an agent trained from imitation learning. In a similar strand, [[237](#bib.bib237)]
    have applied a deep Q-network to control the speech volume of a humanoid robot
    in environments with different amounts of noise. In a trial with human subjects,
    participants rated the proposed DRL-based solution better than fixed-volume robots.
    DRL has also been applied to spoken language understanding [[238](#bib.bib238)],
    where a deep Q-network receives symbolic representations from an intent recogniser
    and outputs actions such as (keep mug on sink). In [[121](#bib.bib121)], the authors
    trained a humanoid robot to acquire social skills for tracking and greeting people.
    In their experiments, the robot learnt its human-like behaviour from experiences
    in a real uncontrolled environment. In [[120](#bib.bib120)], they propose an approach
    for efficiently training the behaviour of a robot playing games using a very limited
    amount of demonstration dialogues. Although the learnt multimodal behaviours are
    not always perfect (due to noisy perceptions), they were reasonable while the
    trained robot interacted with real human players. Efficient training has also
    been explored using interactive feedback from human demonstrators as in [[239](#bib.bib239)],
    who show that DRL with interactive feedback leads to faster learning and with
    fewer mistakes than autonomous DRL (without interactive feedback).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在人机互动中，研究人员已使用音频驱动的深度强化学习（DRL）来控制机器人视线和对话管理。在[[232](#bib.bib232)]中，作者使用了结合深度神经网络（DNN）的Q学习，用于音视频视线控制，具体目标是找到有效的策略来控制机器人头部朝向一组人的方向，利用音视频信息。类似地，[[233](#bib.bib233)]的作者使用了深度Q网络，结合视觉和听觉观察，来引导机器人的头部朝向感兴趣的目标。基于结果，作者表明所提出的框架生成了最先进的结果。Clark等人[[234](#bib.bib234)]提出了一个端到端学习框架，可以从结构化演示中引导出一般化和高级别的人际互动规则。他们通过实验证明，该模型能够正确识别听觉和手势反应。另一项有趣的工作[[235](#bib.bib235)]利用深度Q网络生成由语音驱动的反馈通道，如笑声生成，以增强人机互动中的参与感。根据他们的实验，他们发现所提出的方法具有训练机器人进行互动行为的潜力。类似地，[[236](#bib.bib236)]利用递归Q学习生成反馈通道，以在机器人互动过程中吸引参与者。他们表明，使用脱离策略的强化学习训练的代理比使用模仿学习训练的代理能产生更多的互动。在类似的研究中，[[237](#bib.bib237)]应用深度Q网络来控制类人机器人在不同噪音环境中的语音音量。在与人类受试者的试验中，参与者评价所提出的基于DRL的解决方案优于固定音量的机器人。DRL也被应用于口语理解[[238](#bib.bib238)]，其中深度Q网络从意图识别器接收符号表示，并输出诸如（将杯子放在水槽上）等动作。在[[121](#bib.bib121)]中，作者训练了一个类人机器人，以获取跟踪和迎接人的社交技能。在他们的实验中，机器人从现实的非受控环境中学习了类似人类的行为。在[[120](#bib.bib120)]中，他们提出了一种高效训练机器人玩游戏行为的方法，使用非常有限的示范对话。尽管学习到的多模态行为并不总是完美（由于感知噪声），但在训练的机器人与真实人类玩家互动时，这些行为是合理的。高效训练也通过人类示范者的互动反馈得到了探索，如[[239](#bib.bib239)]所示，他们表明，带有互动反馈的DRL学习比无互动反馈的自主DRL（没有互动反馈）能够更快地学习且错误更少。
- en: Robotics plays an interesting role in bringing audio-based DRL applications
    together including all or some of the above. For example, a robot recognising
    speech and understanding language [[238](#bib.bib238)], aware of emotions [[199](#bib.bib199)],
    carryout activities such as playing games [[120](#bib.bib120)], greeting people [[121](#bib.bib121)],
    or playing music [[240](#bib.bib240)], among others. Such a collection of DRL
    agents are currently trained independently, but we should expect more connectedness
    between them in the future work.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人在将音频基础深度强化学习应用结合在一起方面发挥了有趣的作用，包括以上所有或部分内容。例如，能够识别语音和理解语言的机器人[[238](#bib.bib238)]，能够感知情绪[[199](#bib.bib199)]，执行如玩游戏[[120](#bib.bib120)]、问候他人[[121](#bib.bib121)]或播放音乐[[240](#bib.bib240)]等活动。这样的深度强化学习代理集目前是独立训练的，但我们应该期待未来它们之间有更多的联系。
- en: V Challenges in Audio-Based DRL
  id: totrans-507
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 音频基础深度强化学习中的挑战
- en: The research works in the previous section have focused on a narrow set of DRL
    algorithms and have ignored the existence of many other algorithms, as can be
    noted in Figure [5](#S4.F5 "Figure 5 ‣ IV-E Music Listening and Generation ‣ IV
    Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    This suggests the need for a stronger collaboration between core DRL and audio-based
    DRL, which may be already happening. In Figure [6](#S5.F6 "Figure 6 ‣ V-A Real-World
    Audio-Based Systems ‣ V Challenges in Audio-Based DRL ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications"), we note an increased interest in the
    communities of core and applied DRL. While core DRL grew from 3 to 4 orders of
    magnitude from 2015 to 2020, applied DRL grew from 2 to 3 orders of magnitude
    in the same period.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节的研究工作集中在一小部分深度强化学习算法上，并忽略了许多其他算法的存在，如图[5](#S4.F5 "Figure 5 ‣ IV-E 音乐听赏与生成
    ‣ IV 音频基础深度强化学习 ‣ 音频基础应用的深度强化学习调查")所示。这表明了核心深度强化学习与音频基础深度强化学习之间需要更强的合作，这种合作可能已经在发生。在图[6](#S5.F6
    "Figure 6 ‣ V-A 现实世界音频基础系统 ‣ V 音频基础深度强化学习中的挑战 ‣ 音频基础应用的深度强化学习调查")中，我们注意到核心和应用深度强化学习社区的兴趣增加。核心深度强化学习从2015年到2020年增长了3到4个数量级，而应用深度强化学习在同一期间增长了2到3个数量级。
- en: Figure [7](#S5.F7 "Figure 7 ‣ V-C Multi-Agent and Truly Autonomous Systems ‣
    V Challenges in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for
    Audio-Based Applications") help us to illustrate that previous works have only
    explored a modest space of what is possible. Based on the related works above,
    we have identified three main challenges that need to be addressed by future systems.
    Those dimensions converge in what we call ‘very advanced systems’.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#S5.F7 "Figure 7 ‣ V-C 多智能体与真正自主系统 ‣ V 音频基础深度强化学习中的挑战 ‣ 音频基础应用的深度强化学习调查")帮助我们说明了之前的研究仅探索了可能性的一小部分。基于上述相关工作，我们已经确定了未来系统需要解决的三大主要挑战。这些维度汇聚在我们所称的“非常先进的系统”中。
- en: V-A Real-World Audio-Based Systems
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 现实世界音频基础系统
- en: Most of the DRL algorithms described in Section [III](#S3 "III Deep Reinforcement
    Learning ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications")
    carry out experiments on the Atari benchmark [[241](#bib.bib241)], where there
    is no difference between training and test environments. This is an important
    limitation in the literature, and it should be taken into account in the development
    of future DRL algorithms. In contrast, audio-based DRL applications tend to make
    use of a more explicit separation between training and test environments. While
    audio-based DRL agents may be trained from offline interactions or simulations,
    their performance requires to be assessed using a separate set of offline data
    or real interactions. The latter (often referred to as human evaluations) is very
    important for analysing and evidencing the quality of learnt behaviours. In almost
    all (if not all) audio-based systems, the creation of data is difficult and expensive.
    This highlights the need for more data-efficient algorithms—specially if DRL agents
    are expected to learn from real data instead of synthetic data. In high-frequency
    audio-based control tasks, DRL agents have the requirements of learning fast and
    avoiding repeating the same mistake. Real-world audio-based systems require algorithms
    that are sample efficient and performant in their operations. This makes the application
    of DRL algorithms in real systems very challenging. Some studies such as [[242](#bib.bib242),
    [243](#bib.bib243), [244](#bib.bib244)], have presented approaches to improve
    the sample efficiency of DRL systems. These approaches, however, have not been
    applied to audio-based systems. This suggests that much more research is required
    to make DRL more practical and successful for its application in real audio-based
    systems.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 本节[III](#S3 "III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications")中描述的大多数DRL算法在Atari基准上进行实验[[241](#bib.bib241)]，其中训练环境和测试环境之间没有区别。这是文献中的一个重要限制，应该在未来DRL算法的发展中加以考虑。相比之下，基于音频的DRL应用倾向于在训练环境和测试环境之间进行更明确的分离。虽然基于音频的DRL代理可以从离线交互或模拟中进行训练，但其性能需要使用独立的离线数据集或真实交互来评估。后者（通常称为人工评估）对于分析和证实学习到的行为的质量非常重要。在几乎所有（如果不是所有）基于音频的系统中，数据的创建都是困难且昂贵的。这突显了对更高效的数据算法的需求——特别是当DRL代理被期望从真实数据中学习而不是合成数据时。在高频音频控制任务中，DRL代理需要快速学习并避免重复相同的错误。真实世界的基于音频的系统要求算法在样本效率和操作性能方面表现优异。这使得DRL算法在真实系统中的应用非常具有挑战性。一些研究如[[242](#bib.bib242),
    [243](#bib.bib243), [244](#bib.bib244)]提出了提高DRL系统样本效率的方法。然而，这些方法尚未应用于基于音频的系统。这表明，需要更多的研究以使DRL在实际的基于音频的系统中的应用更加实用和成功。
- en: '![Refer to caption](img/79395a5d5d5b8d336be6ba48e800e8bb.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/79395a5d5d5b8d336be6ba48e800e8bb.png)'
- en: 'Figure 6: Cumulative distribution of publications per year (data gathered from
    2015 to 2020) – from https://www.scopus.com'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：每年出版物的累计分布（数据收集时间为2015年至2020年） – 来源于 [https://www.scopus.com](https://www.scopus.com)
- en: V-B Knowledge Transfer and Generalisation
  id: totrans-514
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 知识转移与泛化
- en: Learning behaviours from complex signals like speech and audio with DRL requires
    processing high-dimensional inputs and performing extensive training on a large
    number of samples to achieve improved performance. The unavailability of large
    labelled datasets is indeed one of the major obstacles in the area of audio-driven
    DRL [[1](#bib.bib1)]. Moreover, it is computationally expensive to train a single
    DRL agent, and there is a need for training multiple DRL agents in order to equip
    audio-based systems with a variety of learnt skills. Therefore, some researchers
    have turned their attention to studying different schemes such as policy distillation [[245](#bib.bib245)],
    progressive neural networks [[246](#bib.bib246)], multi-domain/multi-task learning [[169](#bib.bib169),
    [150](#bib.bib150), [247](#bib.bib247), [248](#bib.bib248)] and others [[249](#bib.bib249),
    [250](#bib.bib250), [251](#bib.bib251)] to promote transfer learning and generalisation
    in DRL to improve system performance and reduce computational costs. Only a few
    studies in dialogue systems have started to explore transfer learning in DRL for
    the speech, audio and dialogue domains [[252](#bib.bib252), [163](#bib.bib163),
    [168](#bib.bib168), [177](#bib.bib177), [166](#bib.bib166)], and more research
    is needed in this area. DRL agents are often trained from scratch instead of inheriting
    useful behaviours from other agents. Research efforts in these directions would
    contribute towards a more practical, cost-effective, and robust application of
    audio-based DRL agents. On the one hand, to train agents less data-intensively,
    and on the other to achieve reasonable performance in the real world.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 从复杂信号如语音和音频中学习行为使用深度强化学习（DRL）需要处理高维输入，并在大量样本上进行广泛训练，以实现性能提升。大型标注数据集的缺乏确实是音频驱动的DRL领域的主要障碍之一[[1](#bib.bib1)]。此外，训练单个DRL智能体的计算成本很高，而为了使基于音频的系统具备多种学习技能，需要训练多个DRL智能体。因此，一些研究人员将注意力转向研究不同的方案，如策略蒸馏[[245](#bib.bib245)]、渐进神经网络[[246](#bib.bib246)]、多领域/多任务学习[[169](#bib.bib169),
    [150](#bib.bib150), [247](#bib.bib247), [248](#bib.bib248)]及其他[[249](#bib.bib249),
    [250](#bib.bib250), [251](#bib.bib251)]，以促进DRL中的迁移学习和泛化，从而提高系统性能并降低计算成本。在对话系统中，只有少数研究开始探索DRL在语音、音频和对话领域的迁移学习[[252](#bib.bib252),
    [163](#bib.bib163), [168](#bib.bib168), [177](#bib.bib177), [166](#bib.bib166)]，这一领域还需要更多研究。DRL智能体通常从头开始训练，而不是从其他智能体那里继承有用的行为。这些研究方向的努力将有助于实现更实用、成本效益高且更强健的音频驱动DRL智能体应用。一方面，以减少数据密集型训练，另一方面，以在现实世界中实现合理的性能。
- en: V-C Multi-Agent and Truly Autonomous Systems
  id: totrans-516
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 多智能体与真正自主系统
- en: Audio-based DRL has achieved impressive performance in single-agent domains,
    where the environment stays mostly stationary. But in the case of audio-based
    systems operating in real-world scenarios, the environments are typically challenging
    and dynamic. For instance, multi-lingual ASR and spoken dialogue systems need
    to learn policies for different languages and domains. These tasks not only involve
    a high degree of uncertainty and complicated dynamics but are also characterised
    by the fact that they are situated in the real physical world, thus have an inherently
    distributed nature. The problem, thus, falls naturally into the realm of multi-agent
    RL (MARL), an area of knowledge with a relatively long history, and has recently
    re-emerged due to advances in single-agent RL techniques [[253](#bib.bib253),
    [254](#bib.bib254)]. Coupled with recent advances in DNNs, MARL has been in the
    limelight for many recent breakthroughs in various domains including control systems,
    communication networks, economics, etc. However, applications in the audio processing
    domain are relatively limited due to various challenges. The learning goals in
    MARL are multidimensional—because the objectives of all agents are not necessarily
    aligned. This situation can arise for example in simultaneous emotion and speaker
    voice recognition, where the goal of one agent is to identify emotions and the
    goal of the other agent is to recognise the speaker. As a consequence, these agents
    can independently perceive the environment, and act according to their individual
    objectives (rewards) thus modifying the environment. This can bring up the challenge
    of dealing with equilibrium points, as well as some additional performance criteria
    beyond return-optimisation, such as the robustness against potential adversarial
    agents. As all agents try to improve their policies according to their interests
    concurrently, therefore the action executed by one agent affects the goals and
    objectives of the other agents (e.g. speaker, gender, and emotion identification
    from speech at the same time), and vice-versa.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 基于音频的DRL在单一智能体领域中取得了显著的成绩，这些领域中的环境大多保持稳定。然而，在实际场景中操作的基于音频的系统面临的环境通常具有挑战性和动态性。例如，多语言自动语音识别（ASR）和口语对话系统需要为不同语言和领域学习策略。这些任务不仅涉及高度的不确定性和复杂的动态，还具有它们存在于真实物理世界中的特点，因此具有固有的分布式特性。因此，这个问题自然地落入了多智能体强化学习（MARL）的范畴，这是一个相对有着较长历史的知识领域，近年来由于单一智能体强化学习技术的进展而重新受到关注[[253](#bib.bib253),
    [254](#bib.bib254)]。结合最近在深度神经网络（DNN）方面的进展，MARL在控制系统、通信网络、经济学等各个领域的多个突破中都备受瞩目。然而，由于各种挑战，音频处理领域的应用相对有限。MARL中的学习目标是多维的——因为所有智能体的目标不一定是对齐的。例如，在同时进行情感和说话者声音识别时，某个智能体的目标可能是识别情感，而另一个智能体的目标可能是识别说话者。因此，这些智能体可以独立感知环境，并根据各自的目标（奖励）进行行动，从而改变环境。这会带来处理均衡点的挑战，以及一些额外的性能标准，如对潜在对抗智能体的鲁棒性。由于所有智能体同时试图根据各自的兴趣改善其策略，因此一个智能体执行的动作会影响其他智能体的目标和目的（例如，同时从语音中识别说话者、性别和情感），反之亦然。
- en: '![Refer to caption](img/eef5431f4e0d4b23dcfdf60716f9f7ff.png)'
  id: totrans-518
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eef5431f4e0d4b23dcfdf60716f9f7ff.png)'
- en: 'Figure 7: A pictorial view of previous works on audio-based DRL and potential
    dimensions to explore in future systems.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：先前在基于音频的深度强化学习（DRL）上的研究及未来系统中潜在的探索维度的图示。
- en: One remaining challenging aspect is that of autonomous skill acquisition. Most,
    if not all, DRL agents currently require a substantial amount of pre-programming
    as opposed to acquiring skills autonomously to enable personalised/extensible
    behaviour. Such pre-programming includes explicit implementations of states, actions,
    rewards, and policies. Although substantial progress in different areas has been
    made, the idea of creating audio-driven DRL agents that autonomously learn their
    states, actions, and rewards in order to induce useful skills remains to be investigated
    further. Such kind of agents would have to know when and how to observe their
    environments, identify a task and input features, induce a set of actions, induce
    a reward function (from audio, images, or both), and use all of that to train
    policies. Such agents have the potential to show advanced levels of intelligence,
    and they would be very useful for applications such as personal assistants or
    interactive robots.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 一项仍然具有挑战性的方面是自主技能获取。目前大多数DRL代理需要大量的预编程，而不是自主获取技能以实现个性化/可扩展的行为。这种预编程包括状态、动作、奖励和策略的明确实现。尽管在不同领域取得了重大进展，但创建音频驱动的DRL代理自主学习状态、动作和奖励以引导有用技能的想法仍需进一步研究。这类代理需要知道何时以及如何观察环境，识别任务和输入特征，产生一组动作，生成奖励函数（来自音频、图像或两者），并使用这些来训练策略。这类代理有可能展现出高级智能水平，并且对个人助手或互动机器人等应用非常有用。
- en: VI Summary and Future Pointers
  id: totrans-521
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 总结与未来指引
- en: 'This literature review shows that DRL is becoming popular in audio processing
    and related applications. We collected DRL research papers in six different but
    related areas: automatic speech recognition (ASR), speech emotion recognition
    (SER), spoken dialogue systems (SDSs), audio enhancement, audio-driven robotic
    control, and music generation.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇文献综述表明，DRL在音频处理及相关应用中越来越受欢迎。我们收集了在六个不同但相关领域的DRL研究论文：自动语音识别（ASR）、语音情感识别（SER）、口语对话系统（SDSs）、音频增强、音频驱动的机器人控制以及音乐生成。
- en: '1.'
  id: totrans-523
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: In ASR, most of the studies have used policy gradient-based DRL, as it allows
    learning an optimal policy that maximises the performance objective. We found
    studies aiming to solve the complexity of ASR models [[131](#bib.bib131)], tackle
    slow convergence issues [[133](#bib.bib133)], and speed up the convergence in
    DRL [[132](#bib.bib132)].
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在ASR中，大多数研究使用了基于策略梯度的DRL，因为它允许学习一个优化性能目标的最优策略。我们发现一些研究旨在解决ASR模型的复杂性[[131](#bib.bib131)]，应对慢收敛问题[[133](#bib.bib133)]，以及加快DRL中的收敛速度[[132](#bib.bib132)]。
- en: '2.'
  id: totrans-525
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The development of SDSs with DRL is gaining interest and different studies have
    shown very interesting results that have outperformed current state-of-the-art
    DL approaches [[143](#bib.bib143)]. However, there is still room for improvement
    regarding the effective and practical training of DRL-based spoken dialogue systems.
  id: totrans-526
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用DRL发展的SDSs正受到越来越多的关注，不同的研究显示了非常有趣的结果，超越了当前最先进的DL方法[[143](#bib.bib143)]。然而，关于DRL基础的口语对话系统的有效和实用训练仍有改进空间。
- en: '3.'
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Several studies have also applied DRL to emotion recognition and empirically
    showed that DRL can (i) lower latency while making predictions [[199](#bib.bib199)],
    (ii) understand emotional dynamics in communication [[200](#bib.bib200)], and
    (iii) enhance human-computer interaction [[201](#bib.bib201)].
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些研究还将DRL应用于情感识别，并实证显示DRL可以 (i) 在预测时降低延迟[[199](#bib.bib199)]，(ii) 理解交流中的情感动态[[200](#bib.bib200)]，以及
    (iii) 增强人机互动[[201](#bib.bib201)]。
- en: '4.'
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: In the case of audio enhancement, studies have shown the potential of DRL. While
    these studies have focused their attention on the speech signals, DRL can be used
    to optimise the audio enhancement module along with performance objectives such
    as those in ASR [[207](#bib.bib207)].
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在音频增强的情况下，研究已经显示了DRL的潜力。虽然这些研究将注意力集中在语音信号上，但DRL可以用于优化音频增强模块以及其他性能目标，如ASR中的目标[[207](#bib.bib207)]。
- en: '5.'
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: In music generation, DRL can optimise rules of music theory as validated in
    different studies [[215](#bib.bib215), [219](#bib.bib219)]. It can also be used
    to search for new tone synthesis parameters [[221](#bib.bib221)]. Moreover, DRL
    can be used to perform score following to track a musical performance [[222](#bib.bib222)],
    and it is even suitable for tracking real piano recordings [[223](#bib.bib223)],
    among other possible tasks.
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在音乐生成中，DRL 可以优化音乐理论规则，这已在不同研究中得到验证 [[215](#bib.bib215), [219](#bib.bib219)]。它还可以用于寻找新的音调合成参数
    [[221](#bib.bib221)]。此外，DRL 可用于进行乐谱跟踪以追踪音乐表演 [[222](#bib.bib222)]，甚至适用于跟踪真实的钢琴录音
    [[223](#bib.bib223)]，以及其他可能的任务。
- en: '6.'
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: In robotics, audio-based DRL agents are in their infancy. Previous studies have
    trained DRL-based agents using simulations, which have shown that reinforcement
    principles help agents in the acquisition of spoken language. Some recent works [[235](#bib.bib235),
    [236](#bib.bib236)] have shown that DRL can be utilised to train gaze controllers
    and speech-driven backchannels like laughs in human-robot interaction.
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在机器人技术中，基于音频的 DRL 代理仍处于起步阶段。以往的研究通过模拟训练了基于 DRL 的代理，结果显示强化原则有助于代理在获取语言方面的能力。一些近期的工作
    [[235](#bib.bib235), [236](#bib.bib236)] 显示 DRL 可以用于训练注视控制器和基于语音的回馈，如人机交互中的笑声。
- en: The related works reviewed above highlight several benefits of using DRL for
    audio processing and applications. Challenges remain before such advancements
    will succeed in the real world, including endowing agents with commonsense knowledge,
    knowledge transfer, generalisation, and autonomous learning, among others. Such
    advances need to be demonstrated not only in simulated and stationary environments,
    but in real and non-stationary one as in real world scenarios. Steady progress,
    however, is being made in the right direction for designing more adaptive audio-based
    systems that can be better suited for real-world settings. If such scientific
    progress keeps growing rapidly, perhaps we are not too far away from AI-based
    autonomous systems that can listen, process, and understand audio and act in more
    human-like ways in increasingly complex environments.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 上述相关工作突显了使用 DRL 进行音频处理和应用的若干好处。然而，在这些进展能够在现实世界中取得成功之前，还存在一些挑战，包括赋予代理常识知识、知识迁移、泛化和自主学习等。这些进展需要在真实且非静态的环境中得到验证，而不仅仅是在模拟和静态环境中。然而，设计出更适合现实世界环境的更具适应性的音频系统的稳步进展正在朝着正确的方向发展。如果这种科学进展持续快速增长，也许我们离能够在越来越复杂的环境中以更类似人类的方式听、处理和理解音频并做出反应的基于
    AI 的自主系统并不太远。
- en: VII Conclusions
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: In this work, we have focused on presenting a comprehensive review of deep reinforcement
    learning (DRL) techniques for audio based applications. We reviewed DRL research
    works in six different audio-related areas including automatic speech recognition
    (ASR), speech emotion recognition (SER), spoken dialogue systems (SDSs), audio
    enhancement, audio-driven robotic control, and music generation. In all of these
    areas, the use of DRL techniques is becoming increasingly popular, and ongoing
    research on this topic has explored many DRL algorithms with encouraging results
    for audio-related applications. Apart from providing a detailed review, we have
    also highlighted (i) various challenges that hinder DRL research in audio applications
    and (ii) various avenues for future research. We hope that this paper will help
    researchers and practitioners interested in exploring and solving problems in
    the audio domain using DRL techniques.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于呈现深度强化学习（DRL）技术在音频应用中的全面综述。我们回顾了在六个不同音频相关领域的 DRL 研究工作，包括自动语音识别（ASR）、语音情感识别（SER）、口语对话系统（SDSs）、音频增强、音频驱动的机器人控制和音乐生成。在所有这些领域，DRL
    技术的使用变得越来越受欢迎，关于该主题的持续研究探讨了许多 DRL 算法，并取得了对音频相关应用的鼓舞人心的结果。除了提供详细的综述外，我们还突出了 (i)
    阻碍 DRL 在音频应用中研究的各种挑战，以及 (ii) 未来研究的各种途径。我们希望本文能够帮助那些有兴趣利用 DRL 技术探索和解决音频领域问题的研究人员和从业者。
- en: VIII Acknowledgements
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 致谢
- en: We would like to thank Kai Arulkumaran (Imperial College London, United Kingdom)
    and Dr Soujanya Poria (Singapore University of Technology and Design) for proving
    feedback on the paper. We also thank Waleed Iqbal (Queen Mary University of London,
    United Kingdom) for helping with the extraction of DRL related data from Scopus
    (Figure [6](#S5.F6 "Figure 6 ‣ V-A Real-World Audio-Based Systems ‣ V Challenges
    in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications")).
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢 Kai Arulkumaran（伦敦帝国学院，英国）和 Dr. Soujanya Poria（新加坡科技设计大学）对论文的反馈。 我们还要感谢
    Waleed Iqbal（伦敦玛丽女王大学，英国）在从 Scopus 提取 DRL 相关数据方面的帮助（见图 [6](#S5.F6 "Figure 6 ‣
    V-A Real-World Audio-Based Systems ‣ V Challenges in Audio-Based DRL ‣ A Survey
    on Deep Reinforcement Learning for Audio-Based Applications")）。
- en: References
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Purwins, B. Li, T. Virtanen, J. Schlüter, S.-Y. Chang, and T. Sainath,
    “Deep learning for audio signal processing,” *IEEE Journal of Selected Topics
    in Signal Processing*, vol. 13, no. 2, 2019.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Purwins, B. Li, T. Virtanen, J. Schlüter, S.-Y. Chang, 和 T. Sainath，“用于音频信号处理的深度学习，”
    *IEEE 选择信号处理期刊*，第13卷，第2期，2019年。'
- en: '[2] R. S. Sutton, A. G. Barto *et al.*, *Introduction to reinforcement learning*.   MIT
    press Cambridge, 1998, vol. 135.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] R. S. Sutton, A. G. Barto *等*，*强化学习导论*。 麻省理工学院出版社剑桥，1998年，第135卷。'
- en: '[3] N. Kohl and P. Stone, “Policy gradient reinforcement learning for fast
    quadrupedal locomotion,” in *IEEE International Conference on Robotics and Automation
    (ICRA)*, vol. 3, 2004.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] N. Kohl 和 P. Stone，“用于快速四足运动的策略梯度强化学习，” 在 *IEEE 国际机器人与自动化会议（ICRA）*，第3卷，2004年。'
- en: '[4] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger,
    and E. Liang, “Autonomous inverted helicopter flight via reinforcement learning,”
    in *Experimental robotics IX*.   Springer, 2006.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger,
    和 E. Liang，“通过强化学习实现自主倒立直升机飞行，” 在 *实验机器人学 IX*。 施普林格，2006年。'
- en: '[5] S. Singh, D. Litman, M. Kearns, and M. Walker, “Optimizing dialogue management
    with reinforcement learning: Experiments with the njfun system,” *Journal of Artificial
    Intelligence Research*, vol. 16, 2002.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Singh, D. Litman, M. Kearns, 和 M. Walker，“通过强化学习优化对话管理：对 njfun 系统的实验，”
    *人工智能研究期刊*，第16卷，2002年。'
- en: '[6] A. L. Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Littman, “Pac
    model-free reinforcement learning,” in *International Conference on Machine Learning
    (ICML)*, 2006.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. L. Strehl, L. Li, E. Wiewiora, J. Langford, 和 M. L. Littman，“Pac模型无关强化学习，”
    在 *国际机器学习会议（ICML）*，2006年。'
- en: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal processing magazine*, vol. 29, no. 6, 2012.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *等*，“用于语音识别的深度神经网络：四个研究组的共享观点，” *IEEE 信号处理杂志*，第29卷，第6期，2012年。'
- en: '[8] A.-r. Mohamed, G. Dahl, and G. Hinton, “Deep belief networks for phone
    recognition,” in *NIPS workshop on deep learning for speech recognition and related
    applications*, 2009.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A.-r. Mohamed, G. Dahl, 和 G. Hinton，“用于电话识别的深度信念网络，” 在 *NIPS 深度学习语音识别与相关应用研讨会*，2009年。'
- en: '[9] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural computation*, vol. 1, no. 4, 1989.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    和 L. D. Jackel，“反向传播应用于手写邮政编码识别，” *神经计算*，第1卷，第4期，1989年。'
- en: '[10] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, 1997.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” *神经计算*，第9卷，第8期，1997年。'
- en: '[11] S. Lange, M. A. Riedmiller, and A. Voigtländer, “Autonomous reinforcement
    learning on raw visual input data in a real world application,” in *International
    Joint Conference on Neural Networks (IJCNN), Brisbane, Australia, June 10-15,
    2012*.   IEEE, 2012.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. Lange, M. A. Riedmiller, 和 A. Voigtländer，“在实际应用中对原始视觉输入数据进行自主强化学习，”
    在 *国际神经网络联合会议（IJCNN），澳大利亚布里斯班，2012年6月10-15日*。 IEEE，2012年。'
- en: '[12] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, 2015.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *等*，“通过深度强化学习实现人类水平的控制，”
    *Nature*，第518卷，第7540期，2015年。'
- en: '[13] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *nature*, vol. 529,
    no. 7587, 2016.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *等*, “通过深度神经网络和树搜索掌握围棋游戏，”
    *自然*, vol. 529, no. 7587, 2016。'
- en: '[14] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, vol. 17,
    no. 1, 2016.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. Levine, C. Finn, T. Darrell, 和 P. Abbeel, “深度视觉运动策略的端到端训练，” *机器学习研究杂志*,
    vol. 17, no. 1, 2016。'
- en: '[15] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel,
    “Rl²: Fast reinforcement learning via slow reinforcement learning,” *arXiv preprint
    arXiv:1611.02779*, 2016.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, 和 P. Abbeel,
    “Rl²：通过慢强化学习实现快速强化学习，” *arXiv 预印本 arXiv:1611.02779*, 2016。'
- en: '[16] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos,
    C. Blundell, D. Kumaran, and M. Botvinick, “Learning to reinforcement learn,”
    *arXiv preprint arXiv:1611.05763*, 2016.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos,
    C. Blundell, D. Kumaran, 和 M. Botvinick, “学习强化学习，” *arXiv 预印本 arXiv:1611.05763*,
    2016。'
- en: '[17] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *IEEE international conference on robotics and automation (ICRA)*, 2017.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, 和 A. Farhadi,
    “使用深度强化学习在室内场景中进行目标驱动的视觉导航，” 见 *IEEE国际机器人与自动化会议（ICRA）*, 2017。'
- en: '[18] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, 2017.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] K. Arulkumaran, M. P. Deisenroth, M. Brundage, 和 A. A. Bharath, “深度强化学习：简要综述，”
    *IEEE信号处理杂志*, vol. 34, no. 6, 2017。'
- en: '[19] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Y. Li, “深度强化学习：概述，” *arXiv 预印本 arXiv:1701.07274*, 2017。'
- en: '[20] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim, “Applications of deep reinforcement learning in communications and
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 4,
    2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, 和
    D. I. Kim, “深度强化学习在通信和网络中的应用：一项综述，” *IEEE通信调查与教程*, vol. 21, no. 4, 2019。'
- en: '[21] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani,
    and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,”
    *arXiv preprint arXiv:2002.00444*, 2020.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani,
    和 P. Pérez, “深度强化学习在自主驾驶中的应用：一项综述，” *arXiv 预印本 arXiv:2002.00444*, 2020。'
- en: '[22] A. Haydari and Y. Yilmaz, “Deep reinforcement learning for intelligent
    transportation systems: A survey,” *arXiv preprint arXiv:2005.00935*, 2020.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Haydari 和 Y. Yilmaz, “智能交通系统中的深度强化学习：一项综述，” *arXiv 预印本 arXiv:2005.00935*,
    2020。'
- en: '[23] N. D. Nguyen, T. Nguyen, and S. Nahavandi, “System design perspective
    for human-level agents using deep reinforcement learning: A survey,” *IEEE Access*,
    vol. 5, 2017.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] N. D. Nguyen, T. Nguyen, 和 S. Nahavandi, “使用深度强化学习的人类级代理系统设计视角：一项综述，”
    *IEEE Access*, vol. 5, 2017。'
- en: '[24] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement
    learning framework for autonomous driving,” *Electronic Imaging*, vol. 2017, no. 19,
    2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. E. Sallab, M. Abdou, E. Perot, 和 S. Yogamani, “用于自主驾驶的深度强化学习框架，” *电子成像*,
    vol. 2017, no. 19, 2017。'
- en: '[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2012.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行Imagenet分类，”
    见 *神经信息处理系统进展（NIPS）*, 2012。'
- en: '[26] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, “A survey of the recent
    architectures of deep convolutional neural networks,” *Artif. Intell. Rev.*, vol. 53,
    no. 8, 2020.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Khan, A. Sohail, U. Zahoora, 和 A. S. Qureshi, “深度卷积神经网络的最新架构综述，” *人工智能评论*,
    vol. 53, no. 8, 2020。'
- en: '[27] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *2017 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2017.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] E. Tzeng, J. Hoffman, K. Saenko, 和 T. Darrell, “对抗性判别域自适应，” 见 *2017 IEEE计算机视觉与模式识别会议（CVPR）*,
    2017。'
- en: '[28] J. Schlüter and S. Böck, “Improved musical onset detection with convolutional
    neural networks,” in *International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2014.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Schlüter 和 S. Böck，"使用卷积神经网络改进音乐开始检测"，发表于 *国际声学、语音与信号处理会议 (ICASSP)*，2014年。'
- en: '[29] N. Mamun, S. Khorram, and J. H. Hansen, “Convolutional neural network-based
    speech enhancement for cochlear implant recipients,” in *Interspeech*, 2019.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] N. Mamun, S. Khorram, 和 J. H. Hansen，"基于卷积神经网络的听力植入接收者语音增强"，发表于 *Interspeech*，2019年。'
- en: '[30] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu,
    “Convolutional neural networks for speech recognition,” *IEEE/ACM Transactions
    on audio, speech, and language processing*, vol. 22, no. 10, 2014.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, 和 D. Yu，"用于语音识别的卷积神经网络"，*IEEE/ACM音频、语音与语言处理交易*，第22卷，第10期，2014年。'
- en: '[31] S.-Y. Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi, A. van den Oord,
    and O. Vinyals, “Temporal modeling using dilated convolution and gating for voice-activity-detection,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2018.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S.-Y. Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi, A. van den Oord,
    和 O. Vinyals，"使用膨胀卷积和门控的时间建模用于语音活动检测"，发表于 *国际声学、语音与信号处理会议 (ICASSP)*，2018年。'
- en: '[32] Y. Chen, Q. Guo, X. Liang, J. Wang, and Y. Qian, “Environmental sound
    classification with dilated convolutions,” *Applied Acoustics*, vol. 148, 2019.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Chen, Q. Guo, X. Liang, J. Wang, 和 Y. Qian，"使用膨胀卷积的环境声音分类"，*应用声学*，第148卷，2019年。'
- en: '[33] Z. C. Lipton, “A critical review of recurrent neural networks for sequence
    learning,” *CoRR*, vol. abs/1506.00019, 2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Z. C. Lipton，"递归神经网络在序列学习中的关键评审"，*CoRR*，第abs/1506.00019卷，2015年。'
- en: '[34] S. Latif, J. Qadir, A. Qayyum, M. Usama, and S. Younis, “Speech technology
    for healthcare: Opportunities, challenges, and state of the art,” *IEEE Reviews
    in Biomedical Engineering*, 2020.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Latif, J. Qadir, A. Qayyum, M. Usama, 和 S. Younis，"医疗保健的语音技术：机会、挑战和最新进展"，*IEEE生物医学工程评论*，2020年。'
- en: '[35] K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Conference on Empirical Methods in Natural
    Language Processing (EMNLP)*, 2014.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    和 Y. Bengio，"使用 rnn 编码器-解码器进行短语表示学习以用于统计机器翻译"，发表于 *自然语言处理实证方法会议 (EMNLP)*，2014年。'
- en: '[36] J. Li, A. Mohamed, G. Zweig, and Y. Gong, “LSTM time and frequency recurrence
    for automatic speech recognition,” in *IEEE workshop on automatic speech recognition
    and understanding (ASRU)*, 2015.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Li, A. Mohamed, G. Zweig, 和 Y. Gong，"用于自动语音识别的 LSTM 时间和频率递归"，发表于 *IEEE
    自动语音识别与理解研讨会 (ASRU)*，2015年。'
- en: '[37] T. N. Sainath and B. Li, “Modeling time-frequency patterns with lstm vs.
    convolutional architectures for lvcsr tasks,” in *Interspeech*, 2016.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. N. Sainath 和 B. Li，"使用 LSTM 与卷积架构建模时间-频率模式用于大词汇连续语音识别任务"，发表于 *Interspeech*，2016年。'
- en: '[38] Y. Qian, M. Bi, T. Tan, and K. Yu, “Very deep convolutional neural networks
    for noise robust speech recognition,” *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, vol. 24, no. 12, 2016.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Qian, M. Bi, T. Tan, 和 K. Yu，"用于噪声鲁棒语音识别的非常深的卷积神经网络"，*IEEE/ACM音频、语音与语言处理交易*，第24卷，第12期，2016年。'
- en: '[39] S. Latif, “Deep representation learning for improving speech emotion recognition,”
    2020.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Latif，"用于提高语音情感识别的深度表示学习"，2020年。'
- en: '[40] D. Ghosal and M. H. Kolekar, “Music genre recognition using deep neural
    networks and transfer learning.” in *Interspeech*, vol. 2018, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] D. Ghosal 和 M. H. Kolekar，"使用深度神经网络和迁移学习的音乐风格识别"，发表于 *Interspeech*，第2018卷，2018年。'
- en: '[41] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Advances in Neural Information Processing Systems (NIPS)*,
    2014.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] I. Sutskever, O. Vinyals, 和 Q. V. Le，"使用神经网络进行序列到序列学习"，发表于 *神经信息处理系统进展
    (NIPS)*，2014年。'
- en: '[42] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”
    *IEEE Transactions Signal Process.*, vol. 45, no. 11, 1997.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Schuster 和 K. K. Paliwal，"双向递归神经网络"，*IEEE信号处理学报*，第45卷，第11期，1997年。'
- en: '[43] H. Salehinejad, J. Baarbe, S. Sankar, J. Barfett, E. Colak, and S. Valaee,
    “Recent advances in recurrent neural networks,” *CoRR*, vol. abs/1801.01078, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] H. Salehinejad, J. Baarbe, S. Sankar, J. Barfett, E. Colak, 和 S. Valaee，"递归神经网络的最新进展"，*CoRR*，第abs/1801.01078卷，2018年。'
- en: '[44] Y. Zhang, W. Chan, and N. Jaitly, “Very deep convolutional networks for
    end-to-end speech recognition,” in *International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*, 2017.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Zhang, W. Chan 和 N. Jaitly，“用于端到端语音识别的超深卷积网络，” 见于 *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*，2017年。'
- en: '[45] L. Lu, X. Zhang, and S. Renals, “On training the recurrent neural network
    encoder-decoder for large vocabulary end-to-end speech recognition,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2016.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] L. Lu, X. Zhang 和 S. Renals，“训练用于大词汇量端到端语音识别的递归神经网络编码解码器，” 见于 *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*，2016年。'
- en: '[46] R. Liu, J. Yang, and M. Liu, “A new end-to-end long-time speech synthesis
    system based on tacotron2,” in *International Symposium on Signal Processing Systems*,
    2019.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. Liu, J. Yang 和 M. Liu，“基于Tacotron2的新型端到端长时间语音合成系统，” 见于 *International
    Symposium on Signal Processing Systems*，2019年。'
- en: '[47] A. Graves, “Sequence transduction with recurrent neural networks,” *Workshop
    on Representation Learning, International Conference of Machine Learning (ICML)
    2012*, 2012.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. Graves，“使用递归神经网络进行序列转导，” *Workshop on Representation Learning, International
    Conference of Machine Learning (ICML) 2012*，2012年。'
- en: '[48] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, “Online and
    linear-time attention by enforcing monotonic alignments,” in *International Conference
    on Machine Learning (ICML)*.   JMLR. org, 2017.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss 和 D. Eck，“通过强制单调对齐实现在线线性时间注意力，”
    见于 *International Conference on Machine Learning (ICML)*，JMLR. org，2017年。'
- en: '[49] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell:
    A neural network for large vocabulary conversational speech recognition,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] W. Chan, N. Jaitly, Q. Le 和 O. Vinyals，“听、注意和拼写：用于大词汇量对话语音识别的神经网络，” 见于
    *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*，2016年。'
- en: '[50] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,
    “An online sequence-to-sequence model using partial conditioning,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2016.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo 和 S. Bengio，“使用部分条件的在线序列到序列模型，”
    见于 *Advances in Neural Information Processing Systems (NIPS)*，2016年。'
- en: '[51] N. Pham, T. Nguyen, J. Niehues, M. Müller, and A. Waibel, “Very deep self-attention
    networks for end-to-end speech recognition,” in *Interspeech*, G. Kubin and Z. Kacic,
    Eds.   ISCA, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] N. Pham, T. Nguyen, J. Niehues, M. Müller 和 A. Waibel，“用于端到端语音识别的超深自注意力网络，”
    见于 *Interspeech*，G. Kubin 和 Z. Kacic 编，ISCA，2019年。'
- en: '[52] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems (NIPS)*, 2014.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville 和 Y. Bengio，“生成对抗网络，” 见于 *Advances in Neural Information Processing
    Systems (NIPS)*，2014年。'
- en: '[53] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] D. P. Kingma 和 M. Welling，“自编码变分贝叶斯，” *arXiv preprint arXiv:1312.6114*，2013年。'
- en: '[54] M. Shannon, H. Zen, and W. Byrne, “Autoregressive models for statistical
    parametric speech synthesis,” *IEEE transactions on audio, speech, and language
    processing*, vol. 21, no. 3, 2012.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Shannon, H. Zen 和 W. Byrne，“用于统计参数语音合成的自回归模型，” *IEEE transactions on
    audio, speech, and language processing*，第21卷，第3期，2012年。'
- en: '[55] W.-N. Hsu, Y. Zhang, and J. Glass, “Learning latent representations for
    speech generation and transformation,” in *Interspeech*, 2017.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] W.-N. Hsu, Y. Zhang 和 J. Glass，“学习用于语音生成和转换的潜在表示，” 见于 *Interspeech*，2017年。'
- en: '[56] S. Ma, D. McDuff, and Y. Song, “M3D-GAN: Multi-modal multi-domain translation
    with universal attention,” *arXiv preprint arXiv:1907.04378*, 2019.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Ma, D. McDuff 和 Y. Song，“M3D-GAN：具有通用注意力的多模态多领域翻译，” *arXiv preprint
    arXiv:1907.04378*，2019年。'
- en: '[57] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir, and B. W. Schuller,
    “Deep representation learning in speech processing: Challenges, recent advances,
    and future trends,” *arXiv preprint arXiv:2001.00378*, 2020.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir 和 B. W. Schuller，“语音处理中的深度表示学习：挑战、近期进展与未来趋势，”
    *arXiv preprint arXiv:2001.00378*，2020年。'
- en: '[58] X. Wang, S. Takaki, and J. Yamagishi, “Autoregressive neural f0 model
    for statistical parametric speech synthesis,” *IEEE/ACM Transactions on Audio,
    Speech, and Language Processing*, vol. 26, no. 8, 2018.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] X. Wang, S. Takaki 和 J. Yamagishi，“用于统计参数语音合成的自回归神经f0模型，” *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing*，第26卷，第8期，2018年。'
- en: '[59] R. Bellman, “Dynamic programming,” *Science*, vol. 153, no. 3731, 1966.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] R. Bellman，“动态规划，” *Science*，第153卷，第3731期，1966年。'
- en: '[60] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double Q-learning,” in *AAAI Conference*, 2016.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. Van Hasselt, A. Guez 和 D. Silver，“使用双重 Q 学习的深度强化学习，”在 *AAAI 会议*，2016年。'
- en: '[61] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *International Conference on Learning Representations (ICLR)*, 2016.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] T. Schaul, J. Quan, I. Antonoglou 和 D. Silver，“优先经验重放，”*国际学习表示会议（ICLR）*，2016年。'
- en: '[62] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    Conference on Machine Learning (ICML)*, 2016.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot 和 N. Freitas，“用于深度强化学习的对抗网络架构，”在
    *国际机器学习会议（ICML）*，2016年。'
- en: '[63] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International Conference on Machine Learning (ICML)*.   JMLR.
    org, 2017.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. G. Bellemare, W. Dabney 和 R. Munos，“关于强化学习的分布视角，”在 *国际机器学习会议（ICML）*。
    JMLR.org，2017年。'
- en: '[64] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos, “Distributional
    reinforcement learning with quantile regression,” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] W. Dabney, M. Rowland, M. G. Bellemare 和 R. Munos，“带有分位回归的分布式强化学习，”在 *人工智能
    AAAI 会议*，2018年。'
- en: '[65] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile networks
    for distributional reinforcement learning,” in *International Conference on Machine
    Learning*, 2018.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] W. Dabney, G. Ostrovski, D. Silver 和 R. Munos，“用于分布式强化学习的隐式分位网络，”在 *国际机器学习会议*，2018年。'
- en: '[66] N. Levine, T. Zahavy, D. J. Mankowitz, A. Tamar, and S. Mannor, “Shallow
    updates for deep reinforcement learning,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2017.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] N. Levine, T. Zahavy, D. J. Mankowitz, A. Tamar 和 S. Mannor，“深度强化学习的浅层更新，”在
    *神经信息处理系统进展（NIPS）*，2017年。'
- en: '[67] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan,
    J. Quan, A. Sendonaris, I. Osband *et al.*, “Deep Q-learning from demonstrations,”
    in *AAAI Conference*, 2018.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D.
    Horgan, J. Quan, A. Sendonaris, I. Osband *等*，“通过演示进行深度 Q 学习，”在 *AAAI 会议*，2018年。'
- en: '[68] M. Sabatelli, G. Louppe, P. Geurts, and M. Wiering, “Deep quality value
    (dqv) learning,” *Advances in Neural Information Processing Systems (NIPS)*, 2018.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] M. Sabatelli, G. Louppe, P. Geurts 和 M. Wiering，“深度质量价值（dqv）学习，”*神经信息处理系统进展（NIPS）*，2018年。'
- en: '[69] J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter,
    and S. Hochreiter, “Rudder: Return decomposition for delayed rewards,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2019.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter
    和 S. Hochreiter，“Rudder：延迟奖励的回报分解，”在 *神经信息处理系统进展（NIPS）*，2019年。'
- en: '[70] T. Pohlen, B. Piot, T. Hester, M. G. Azar, D. Horgan, D. Budden, G. Barth-Maron,
    H. Van Hasselt, J. Quan, M. Večerík *et al.*, “Observe and look further: Achieving
    consistent performance on Atari,” *arXiv preprint arXiv:1805.11593*, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] T. Pohlen, B. Piot, T. Hester, M. G. Azar, D. Horgan, D. Budden, G. Barth-Maron,
    H. Van Hasselt, J. Quan, M. Večerík *等*，“观察并展望：在 Atari 上实现一致表现，”*arXiv 预印本 arXiv:1805.11593*，2018年。'
- en: '[71] J. Schulman, X. Chen, and P. Abbeel, “Equivalence between policy gradients
    and soft q-learning,” *arXiv preprint arXiv:1704.06440*, 2017.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Schulman, X. Chen 和 P. Abbeel，“策略梯度与软 Q 学习之间的等效性，”*arXiv 预印本 arXiv:1704.06440*，2017年。'
- en: '[72] M. Hausknecht and P. Stone, “Deep recurrent Q-learning for partially observable
    MDPs,” in *AAAI Fall Symposium Series*, 2015.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Hausknecht 和 P. Stone，“针对部分可观测 MDP 的深度递归 Q 学习，”在 *AAAI 秋季研讨会系列*，2015年。'
- en: '[73] I. Sorokin, A. Seleznev, M. Pavlov, A. Fedorov, and A. Ignateva, “Deep
    attention recurrent Q-network,” *Deep Reinforcement Learning Workshop, NIPS*,
    2015.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] I. Sorokin, A. Seleznev, M. Pavlov, A. Fedorov 和 A. Ignateva，“深度注意力递归
    Q 网络，”*深度强化学习研讨会，NIPS*，2015年。'
- en: '[74] J. Oh, V. Chockalingam, H. Lee *et al.*, “Control of memory, active perception,
    and action in minecraft,” in *International Conference on Machine Learning*, 2016.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. Oh, V. Chockalingam, H. Lee *等*，“在 Minecraft 中控制记忆、主动感知和行动，”在 *国际机器学习会议*，2016年。'
- en: '[75] E. Parisotto and R. Salakhutdinov, “Neural map: Structured memory for
    deep reinforcement learning,” in *International Conference on Learning Representations*,
    2018.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] E. Parisotto 和 R. Salakhutdinov，“神经地图：深度强化学习的结构化记忆，”在 *国际学习表示会议*，2018年。'
- en: '[76] V. R. Konda and J. N. Tsitsiklis, “Actor-Critic agorithms,” in *Neural
    Information Processing Systems (NIPS)*, 1999.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] V. R. Konda 和 J. N. Tsitsiklis，“Actor-Critic 算法，”在 *神经信息处理系统（NIPS）*，1999年。'
- en: '[77] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning (ICML)*, 2016.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D.
    Silver 和 K. Kavukcuoglu, “深度强化学习的异步方法，” 见*国际机器学习会议（ICML）*，2016年。'
- en: '[78] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, “Reinforcement
    learning through asynchronous advantage actor-critic on a gpu,” in *Learning Representations*.   ICLR,
    2017.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons 和 J. Kautz, “通过异步优势演员-评论家在GPU上的强化学习，”
    见*学习表示*。ICLR, 2017年。'
- en: '[79] C. Alfredo, C. Humberto, and C. Arjun, “Efficient parallel methods for
    deep reinforcement learning,” in *The Multi-disciplinary Conference on Reinforcement
    Learning and Decision Making (RLDM)*, 2017.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] C. Alfredo, C. Humberto 和 C. Arjun, “深度强化学习的高效并行方法，” 见*多学科强化学习与决策会议（RLDM）*，2017年。'
- en: '[80] B. O’Donoghue, R. Munos, K. Kavukcuoglu, and V. Mnih, “PGQ: Combining
    policy gradient and Q-learning,” *arXiv preprint arXiv:1611.01626*, 2016.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] B. O’Donoghue, R. Munos, K. Kavukcuoglu 和 V. Mnih, “PGQ：结合策略梯度和Q学习，” *arXiv预印本arXiv:1611.01626*，2016年。'
- en: '[81] R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare, “Safe and efficient
    off-policy reinforcement learning,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2016.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. Munos, T. Stepleton, A. Harutyunyan 和 M. Bellemare, “安全高效的离策略强化学习，”
    见*神经信息处理系统进展（NIPS）*，2016年。'
- en: '[82] A. Gruslys, M. G. Azar, M. G. Bellemare, and R. Munos, “The reactor: A
    sample-efficient actor-critic architecture,” *arXiv preprint arXiv:1704.04651*,
    2017.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] A. Gruslys, M. G. Azar, M. G. Bellemare 和 R. Munos, “反应器：一种样本高效的演员-评论家架构，”
    *arXiv预印本arXiv:1704.04651*，2017年。'
- en: '[83] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    V. Firoiu, T. Harley, I. Dunning *et al.*, “IMPALA: Scalable distributed deep-RL
    with importance weighted actor-learner architectures,” in *International Conference
    on Machine Learning (ICML)*, 2018.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    V. Firoiu, T. Harley, I. Dunning *等*，“IMPALA：具有重要性加权演员-学习者架构的可扩展分布式深度强化学习，” 见*国际机器学习会议（ICML）*，2018年。'
- en: '[84] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning (ICML)*,
    2015.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J. Schulman, S. Levine, P. Abbeel, M. Jordan 和 P. Moritz, “信任区域策略优化，”
    见*国际机器学习会议（ICML）*，2015年。'
- en: '[85] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] J. Schulman, F. Wolski, P. Dhariwal, A. Radford 和 O. Klimov, “邻近策略优化算法，”
    *arXiv预印本arXiv:1707.06347*，2017年。'
- en: '[86] B. Ravindran, “Introduction to deep reinforcement learning,” 2019.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] B. Ravindran, “深度强化学习简介，” 2019年。'
- en: '[87] Ł. Kaiser, M. Babaeizadeh, P. Miłos, B. Osiński, R. H. Campbell, K. Czechowski,
    D. Erhan, C. Finn, P. Kozakowski, S. Levine *et al.*, “Model based reinforcement
    learning for atari,” in *International Conference on Learning Representations*,
    2019.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Ł. Kaiser, M. Babaeizadeh, P. Miłos, B. Osiński, R. H. Campbell, K. Czechowski,
    D. Erhan, C. Finn, P. Kozakowski, S. Levine *等*，“基于模型的强化学习用于Atari，” 见*国际学习表示会议*，2019年。'
- en: '[88] S. Whiteson, “TreeQN and ATreeC: Differentiable tree planning for deep
    reinforcement learning,” 2018.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] S. Whiteson, “TreeQN和ATreeC：用于深度强化学习的可微树规划，” 2018年。'
- en: '[89] J. Oh, S. Singh, and H. Lee, “Value prediction network,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2017.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] J. Oh, S. Singh 和 H. Lee, “价值预测网络，” 见*神经信息处理系统进展（NIPS）*，2017年。'
- en: '[90] A. Vezhnevets, V. Mnih, S. Osindero, A. Graves, O. Vinyals, J. Agapiou
    *et al.*, “Strategic attentive writer for learning macro-actions,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2016.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Vezhnevets, V. Mnih, S. Osindero, A. Graves, O. Vinyals, J. Agapiou
    *等*，“用于学习宏动作的战略注意力编写器，” 见*神经信息处理系统进展（NIPS）*，2016年。'
- en: '[91] N. Nardelli, G. Synnaeve, Z. Lin, P. Kohli, P. H. Torr, and N. Usunier,
    “Value propagation networks,” in *International Conference on Learning Representations*,
    2018.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] N. Nardelli, G. Synnaeve, Z. Lin, P. Kohli, P. H. Torr 和 N. Usunier, “价值传播网络，”
    见*国际学习表示会议*，2018年。'
- en: '[92] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt,
    A. Guez, E. Lockhart, D. Hassabis, T. Graepel *et al.*, “Mastering Atari, Go,
    Chess and Shogi by planning with a learned model,” *arXiv preprint arXiv:1911.08265*,
    2019.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S.
    Schmitt, A. Guez, E. Lockhart, D. Hassabis, T. Graepel *等*，“通过规划学习模型掌握Atari、围棋、国际象棋和将棋，”
    *arXiv预印本arXiv:1911.08265*，2019年。'
- en: '[93] S. P. Rath, D. Povey, K. Veselý, and J. Cernocký, “Improved feature processing
    for deep neural networks,” in *Interspeech*.   ISCA, 2013.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. P. Rath, D. Povey, K. Veselý 和 J. Cernocký，“改进的特征处理用于深度神经网络”，发表于*国际语音通讯会议*，ISCA，2013年。'
- en: '[94] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr
    corpus based on public domain audio books,” in *International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2015.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] V. Panayotov, G. Chen, D. Povey 和 S. Khudanpur，“Librispeech：基于公共领域有声书的ASR语料库”，发表于*国际声学、语音和信号处理会议（ICASSP）*，2015年。'
- en: '[95] A. Rousseau, P. Deléglise, and Y. Esteve, “TED-LIUM: an automatic speech
    recognition dedicated corpus.” in *LREC*, 2012.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] A. Rousseau, P. Deléglise 和 Y. Esteve，“TED-LIUM：一个用于自动语音识别的专用语料库”，发表于*语言资源与评估会议（LREC）*，2012年。'
- en: '[96] D. B. Paul and J. M. Baker, “The design for the wall street journal-based
    CSR corpus,” in *Workshop on Speech and Natural Language*.   ACL, 1992.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] D. B. Paul 和 J. M. Baker，“基于《华尔街日报》的CSR语料库设计”，发表于*语音与自然语言研讨会*，ACL，1992年。'
- en: '[97] J. J. Godfrey, E. C. Holliman, and J. McDaniel, “SWITCHBOARD: Telephone
    speech corpus for research and development,” in *International Conference on Acoustics,
    Speech, and Signal Processing (ICASSP)*, vol. 1, 1992.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. J. Godfrey, E. C. Holliman 和 J. McDaniel，“SWITCHBOARD：用于研究和开发的电话语音语料库”，发表于*国际声学、语音和信号处理会议（ICASSP）*，第1卷，1992年。'
- en: '[98] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett,
    “DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc
    1-1.1,” *NASA STI/Recon technical report n*, vol. 93, 1993.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus 和 D. S. Pallett，“DARPA
    TIMIT 声学-语音连续语音语料库 CD-ROM。NIST 语音光盘 1-1.1”，*NASA STI/Recon 技术报告 n*，第93卷，1993年。'
- en: '[99] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, “Towards scalable
    multi-domain conversational agents: The schema-guided dialogue dataset,” in *The
    Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI*.   AAAI Press,
    2020.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] A. Rastogi, X. Zang, S. Sunkara, R. Gupta 和 P. Khaitan，“迈向可扩展的多领域对话代理：基于模式引导对话的数据集”，发表于*第34届AAAI人工智能会议，AAAI*，AAAI出版社，2020年。'
- en: '[100] J. D. Williams, A. Raux, and M. Henderson, “The dialog state tracking
    challenge series: A review,” *Dialogue Discourse*, vol. 7, no. 3, 2016.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] J. D. Williams, A. Raux 和 M. Henderson，“对话状态追踪挑战系列：综述”，*对话与话语*，第7卷，第3期，2016年。'
- en: '[101] L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Harris, E. Fine, R. Mehrotra,
    and K. Suleman, “Frames: a corpus for adding memory to goal-oriented dialogue
    systems,” in *Annual SIGdial Meeting on Discourse and Dialogue*, K. Jokinen, M. Stede,
    D. DeVault, and A. Louis, Eds.   ACL, 2017.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Harris, E. Fine, R. Mehrotra
    和 K. Suleman，“Frames：一个用于为目标导向对话系统添加记忆的语料库”，发表于*年度SIGdial话语与对话会议*，由K. Jokinen,
    M. Stede, D. DeVault 和 A. Louis编辑，ACL，2017年。'
- en: '[102] P. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, S. Ultes, O. Ramadan,
    and M. Gasic, “Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented
    dialogue modelling,” in *Conference on Empirical Methods in Natural Language Processing
    (EMNLP)*, 2018.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, S. Ultes, O. Ramadan
    和 M. Gasic，“Multiwoz：用于任务导向对话建模的大规模多领域 Wizard-of-Oz 数据集”，发表于*自然语言处理经验方法会议（EMNLP）*，2018年。'
- en: '[103] D. Ameixa, L. Coheur, and R. A. Redol, “From subtitles to human interactions:
    introducing the subtle corpus,” Tech. rep., INESC-ID (November 2014), Tech. Rep.,
    2013.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] D. Ameixa, L. Coheur 和 R. A. Redol，“从字幕到人际互动：介绍Subtle语料库”，技术报告，INESC-ID（2014年11月），技术报告，2013年。'
- en: '[104] J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J. Young, “A survey
    of statistical user simulation techniques for reinforcement-learning of dialogue
    management strategies,” *Knowledge Eng. Review*, vol. 21, no. 2, 2006.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Schatzmann, K. Weilhammer, M. N. Stuttle 和 S. J. Young，“对话管理策略强化学习的统计用户模拟技术调查”，*知识工程评论*，第21卷，第2期，2006年。'
- en: '[105] I. V. Serban, R. Lowe, P. Henderson, L. Charlin, and J. Pineau, “A survey
    of available corpora for building data-driven dialogue systems: The journal version,”
    *Dialogue Discourse*, vol. 9, no. 1, 2018.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] I. V. Serban, R. Lowe, P. Henderson, L. Charlin 和 J. Pineau，“用于构建数据驱动对话系统的现有语料库调查：期刊版”，*对话与话语*，第9卷，第1期，2018年。'
- en: '[106] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss,
    “A database of german emotional speech,” in *European Conference on Speech Communication
    and Technology*, 2005.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier 和 B. Weiss，“一个德语情感语音数据库”，发表于*欧洲语音通信与技术会议*，2005年。'
- en: '[107] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.
    Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive emotional dyadic motion
    capture database,” *Language resources and evaluation*, vol. 42, no. 4, 2008.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.
    Chang, S. Lee, 和 S. S. Narayanan， “IEMOCAP: 互动情感二人运动捕捉数据库，” *语言资源与评估*，第42卷，第4期，2008年。'
- en: '[108] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi,
    and E. M. Provost, “MSP-IMPROV: An acted corpus of dyadic interactions to study
    emotion perception,” *IEEE Transactions on Affective Computing*, vol. 8, no. 1,
    2016.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi,
    和 E. M. Provost， “MSP-IMPROV: 研究情感感知的二人互动表演语料库，” *IEEE情感计算期刊*，第8卷，第1期，2016年。'
- en: '[109] G. McKeown, M. Valstar, R. Cowie, M. Pantic, and M. Schroder, “The semaine
    database: Annotated multimodal records of emotionally colored conversations between
    a person and a limited agent,” *IEEE transactions on affective computing*, vol. 3,
    no. 1, 2011.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] G. McKeown, M. Valstar, R. Cowie, M. Pantic, 和 M. Schroder， “Semaine数据库:
    标注的多模态情感对话记录，” *IEEE情感计算期刊*，第3卷，第1期，2011年。'
- en: '[110] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea,
    “MELD: A multimodal multi-party dataset for emotion recognition in conversations,”
    in *Annual Meeting of the Association for Computational Linguistics ACL*, 2019.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, 和 R. Mihalcea，
    “MELD: 用于对话中情感识别的多模态多方数据集，” 在 *计算语言学协会年度会议 ACL*，2019年。'
- en: '[111] J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel
    acoustic noise database: A database of multichannel environmental noise recordings,”
    *The Journal of the Acoustical Society of America*, vol. 133, no. 5, 2013.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] J. Thiemann, N. Ito, 和 E. Vincent， “多样化环境多通道声学噪声数据库: 多通道环境噪声录音数据库，” *美国声学学会期刊*，第133卷，第5期，2013年。'
- en: '[112] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third ‘CHiME’speech
    separation and recognition challenge: Dataset, task and baselines,” in *IEEE Workshop
    on Automatic Speech Recognition and Understanding (ASRU)*, 2015.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Barker, R. Marxer, E. Vincent, 和 S. Watanabe， “第三届‘CHiME’语音分离与识别挑战赛:
    数据集、任务和基线，” 在 *IEEE自动语音识别与理解研讨会（ASRU）*，2015年。'
- en: '[113] M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux, “WHAMR!: Noisy
    and reverberant single-channel speech separation,” in *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2020.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] M. Maciejewski, G. Wichern, E. McQuinn, 和 J. Le Roux， “WHAMR!: 嘈杂和混响的单通道语音分离，”
    在 *国际声学、语音和信号处理会议（ICASSP）*，2020年。'
- en: '[114] B. Krueger, “Classical piano midi page,” 2016.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] B. Krueger， “经典钢琴MIDI页面，” 2016年。'
- en: '[115] J. Thickstun, Z. Harchaoui, and S. Kakade, “Learning features of music
    from scratch,” *arXiv preprint arXiv:1611.09827*, 2016.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] J. Thickstun, Z. Harchaoui, 和 S. Kakade， “从零开始学习音乐特征，” *arXiv预印本 arXiv:1611.09827*，2016年。'
- en: '[116] M. Allan and C. Williams, “Harmonising chorales by probabilistic inference,”
    in *Advances in Neural Information Processing Systems (NIPS)*, 2005.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] M. Allan 和 C. Williams， “通过概率推断和声合唱，” 在 *神经信息处理系统进展（NIPS）*，2005年。'
- en: '[117] I. D. Gebru, S. Ba, X. Li, and R. Horaud, “Audio-visual speaker diarization
    based on spatiotemporal Bayesian fusion,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 40, no. 5, 2017.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] I. D. Gebru, S. Ba, X. Li, 和 R. Horaud， “基于时空贝叶斯融合的视听说话人分离，” *IEEE模式分析与机器智能期刊*，第40卷，第5期，2017年。'
- en: '[118] R. Scalise, S. Li, H. Admoni, S. Rosenthal, and S. S. Srinivasa, “Natural
    language instructions for human-robot collaborative manipulation,” *Int. J. Robotics
    Res.*, vol. 37, no. 6, 2018.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] R. Scalise, S. Li, H. Admoni, S. Rosenthal, 和 S. S. Srinivasa， “用于人机协作操作的自然语言指令，”
    *国际机器人研究期刊*，第37卷，第6期，2018年。'
- en: '[119] D. K. Misra, J. Sung, K. Lee, and A. Saxena, “Tell me dave: Context-sensitive
    grounding of natural language to manipulation instructions,” *Int. J. Robotics
    Res.*, vol. 35, no. 1-3, 2016.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] D. K. Misra, J. Sung, K. Lee, 和 A. Saxena， “告诉我戴夫: 语境敏感的自然语言到操作指令的映射，”
    *国际机器人研究期刊*，第35卷，第1-3期，2016年。'
- en: '[120] H. Cuayáhuitl, “A data-efficient deep learning approach for deployable
    multimodal social robots,” *Neurocomputing*, vol. 396, 2020.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] H. Cuayáhuitl， “一种数据高效的深度学习方法用于可部署的多模态社交机器人，” *神经计算*，第396卷，2020年。'
- en: '[121] A. H. Qureshi, Y. Nakamura, Y. Yoshikawa, and H. Ishiguro, “Intrinsically
    motivated reinforcement learning for human-robot interaction in the real-world,”
    *Neural Networks*, vol. 107, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] A. H. Qureshi, Y. Nakamura, Y. Yoshikawa, 和 H. Ishiguro， “用于真实世界人机交互的内在激励强化学习，”
    *神经网络*，第107卷，2018年。'
- en: '[122] T. Kala and T. Shinozaki, “Reinforcement learning of speech recognition
    system based on policy gradient and hypothesis selection,” in *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2018.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Kala 和 T. Shinozaki，“基于策略梯度和假设选择的语音识别系统强化学习”，在*国际声学、语音和信号处理会议 (ICASSP)*，2018年。'
- en: '[123] A. Tjandra, S. Sakti, and S. Nakamura, “Sequence-to-sequence ASR optimization
    via reinforcement learning,” in *International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*, 2018.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] A. Tjandra, S. Sakti, 和 S. Nakamura，“通过强化学习优化序列到序列ASR”，在*国际声学、语音和信号处理会议
    (ICASSP)*，2018年。'
- en: '[124] ——, “End-to-end speech recognition sequence training with reinforcement
    learning,” *IEEE Access*, vol. 7, 2019.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] ——，“使用强化学习的端到端语音识别序列训练”，*IEEE Access*，第7卷，2019年。'
- en: '[125] H. Chung, H.-B. Jeon, and J. G. Park, “Semi-supervised training for sequence-to-sequence
    speech recognition using reinforcement learning,” in *2020 International Joint
    Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–6.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] H. Chung, H.-B. Jeon, 和 J. G. Park，“使用强化学习的序列到序列语音识别的半监督训练”，在*2020国际神经网络联合会议
    (IJCNN)*。 IEEE，2020年，第1–6页。'
- en: '[126] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, “Sequence training
    of encoder-decoder model using policy gradient for end-to-end speech recognition,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2018.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] S. Karita, A. Ogawa, M. Delcroix, 和 T. Nakatani，“使用策略梯度的编码器-解码器模型序列训练，用于端到端语音识别”，在*国际声学、语音和信号处理会议
    (ICASSP)*，2018年。'
- en: '[127] Y. Zhou, C. Xiong, and R. Socher, “Improving end-to-end speech recognition
    with policy learning,” in *International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2018.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Y. Zhou, C. Xiong, 和 R. Socher，“通过策略学习提高端到端语音识别”，在*国际声学、语音和信号处理会议 (ICASSP)*，2018年。'
- en: '[128] Y. Luo, C.-C. Chiu, N. Jaitly, and I. Sutskever, “Learning online alignments
    with continuous rewards policy gradient,” in *International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Y. Luo, C.-C. Chiu, N. Jaitly, 和 I. Sutskever，“使用连续奖励策略梯度学习在线对齐”，在*国际声学、语音和信号处理会议
    (ICASSP)*，2017年。'
- en: '[129] K. Radzikowski, R. Nowak, L. Wang, and O. Yoshie, “Dual supervised learning
    for non-native speech recognition,” *EURASIP Journal on Audio, Speech, and Music
    Processing*, vol. 2019, no. 1, 2019.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] K. Radzikowski, R. Nowak, L. Wang, 和 O. Yoshie，“用于非母语语音识别的双重监督学习”，*EURASIP
    Journal on Audio, Speech, and Music Processing*，第2019卷，第1期，2019年。'
- en: '[130] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, “Amc: Automl for
    model compression and acceleration on mobile devices,” in *European Conference
    on Computer Vision (ECCV)*, 2018.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, 和 S. Han，“Amc：移动设备上的自动机器学习用于模型压缩和加速”，在*欧洲计算机视觉会议
    (ECCV)*，2018年。'
- en: '[131] Ł. Dudziak, M. S. Abdelfattah, R. Vipperla, S. Laskaridis, and N. D.
    Lane, “ShrinkML: End-to-end asr model compression using reinforcement learning,”
    in *Interspeech*, 2019.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Ł. Dudziak, M. S. Abdelfattah, R. Vipperla, S. Laskaridis, 和 N. D. Lane，“ShrinkML：使用强化学习的端到端ASR模型压缩”，在*Interspeech*，2019年。'
- en: '[132] T. Rajapakshe, S. Latif, R. Rana, S. Khalifa, and B. W. Schuller, “Deep
    reinforcement learning with pre-training for time-efficient training of automatic
    speech recognition,” *arXiv preprint arXiv:2005.11172*, 2020.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] T. Rajapakshe, S. Latif, R. Rana, S. Khalifa, 和 B. W. Schuller，“用于自动语音识别的深度强化学习与预训练以提高训练效率”，*arXiv
    preprint arXiv:2005.11172*，2020年。'
- en: '[133] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, vol. 8, no. 3-4, 1992.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] R. J. Williams，“用于连接主义强化学习的简单统计梯度跟踪算法”，*Machine learning*，第8卷，第3-4期，1992年。'
- en: '[134] D. Lawson, C.-C. Chiu, G. Tucker, C. Raffel, K. Swersky, and N. Jaitly,
    “Learning hard alignments with variational inference,” in *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2018.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] D. Lawson, C.-C. Chiu, G. Tucker, C. Raffel, K. Swersky, 和 N. Jaitly，“通过变分推断学习硬对齐”，在*国际声学、语音和信号处理会议
    (ICASSP)*，2018年。'
- en: '[135] V. W. Zue and J. R. Glass, “Conversational interfaces: advances and challenges,”
    *IEEE*, vol. 88, no. 8, 2000.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] V. W. Zue 和 J. R. Glass，“对话接口：进展与挑战”，*IEEE*，第88卷，第8期，2000年。'
- en: '[136] E. Levin, R. Pieraccini, and W. Eckert, “A stochastic model of human-machine
    interaction for learning dialog strategies,” *IEEE Transactions Speech Audio Process.*,
    vol. 8, no. 1, 2000.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] E. Levin, R. Pieraccini, 和 W. Eckert，“用于学习对话策略的人机交互随机模型”，*IEEE Transactions
    Speech Audio Process.*，第8卷，第1期，2000年。'
- en: '[137] S. P. Singh, M. J. Kearns, D. J. Litman, and M. A. Walker, “Reinforcement
    learning for spoken dialogue systems,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2000.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] S. P. Singh, M. J. Kearns, D. J. Litman, 和 M. A. Walker, “针对口语对话系统的强化学习”，见于*神经信息处理系统进展（NIPS）*，2000年。'
- en: '[138] T. Paek, “Reinforcement learning for spoken dialogue systems: Comparing
    strengths and weaknesses for practical deployment,” in *Proc. Dialog-on-Dialog
    Workshop, Interspeech*.   Citeseer, 2006.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] T. Paek, “口语对话系统的强化学习：比较实际部署的优缺点”，见于*Dialog-on-Dialog研讨会，Interspeech*。   Citeseer，2006年。'
- en: '[139] J. Gao, M. Galley, and L. Li, “Neural approaches to conversational AI,”
    *Found. Trends Inf. Retr.*, vol. 13, no. 2-3, 2019.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] J. Gao, M. Galley, 和 L. Li, “对话AI的神经方法”，*信息检索基础趋势*，第13卷，第2-3期，2019年。'
- en: '[140] Z. Chen, L. Chen, X. Zhou, and K. Yu, “Deep reinforcement learning for
    on-line dialogue state tracking,” *arXiv preprint arXiv:2009.10321*, 2020.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Z. Chen, L. Chen, X. Zhou, 和 K. Yu, “在线对话状态跟踪的深度强化学习”，*arXiv预印本 arXiv:2009.10321*，2020年。'
- en: '[141] M. Henderson, B. Thomson, and J. D. Williams, “The second dialog state
    tracking challenge,” in *Proceedings of the 15th annual meeting of the special
    interest group on discourse and dialogue (SIGDIAL)*, 2014, pp. 263–272.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] M. Henderson, B. Thomson, 和 J. D. Williams, “第二届对话状态跟踪挑战赛”，见于*第15届话语与对话特别兴趣小组（SIGDIAL）年会论文集*，2014年，第263–272页。'
- en: '[142] ——, “The third dialog state tracking challenge,” in *2014 IEEE Spoken
    Language Technology Workshop (SLT)*.   IEEE, 2014, pp. 324–329.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] ——, “第三届对话状态跟踪挑战赛”，见于*2014年IEEE口语语言技术研讨会（SLT）*。   IEEE，2014年，第324–329页。'
- en: '[143] G. Weisz, P. Budzianowski, P.-H. Su, and M. Gašić, “Sample efficient
    deep reinforcement learning for dialogue systems with large action spaces,” *IEEE/ACM
    Transactions on Audio, Speech, and Language Processing*, vol. 26, no. 11, 2018.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] G. Weisz, P. Budzianowski, P.-H. Su, 和 M. Gašić, “针对大动作空间对话系统的样本高效深度强化学习”，*IEEE/ACM音频、语音与语言处理交易*，第26卷，第11期，2018年。'
- en: '[144] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas,
    “Sample efficient actor-critic with experience replay,” *arXiv preprint arXiv:1611.01224*,
    2016.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, 和 N.
    de Freitas, “具有经验重放的样本高效演员-评论家”，*arXiv预印本 arXiv:1611.01224*，2016年。'
- en: '[145] H. Cuayáhuitl, “Simpleds: A simple deep reinforcement learning dialogue
    system,” in *Dialogues with social robots*.   Springer, 2017.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] H. Cuayáhuitl, “Simpleds: 一种简单的深度强化学习对话系统”，见于*与社会机器人对话*。   Springer，2017年。'
- en: '[146] T. Zhao and M. Eskenazi, “Towards end-to-end learning for dialog state
    tracking and management using deep reinforcement learning,” in *Annual Meeting
    of the Special Interest Group on Discourse and Dialogue*, 2016.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] T. Zhao 和 M. Eskenazi, “使用深度强化学习实现对话状态跟踪和管理的端到端学习”，见于*话语与对话特别兴趣小组年会*，2016年。'
- en: '[147] I. Casanueva, P. Budzianowski, P.-H. Su, N. Mrkšić, T.-H. Wen, S. Ultes,
    L. Rojas-Barahona, S. Young, and M. Gašić, “A benchmarking environment for reinforcement
    learning based task oriented dialogue management,” *Deep Reinforcement Learning
    Symposium, NIPS*, 2017.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] I. Casanueva, P. Budzianowski, P.-H. Su, N. Mrkšić, T.-H. Wen, S. Ultes,
    L. Rojas-Barahona, S. Young, 和 M. Gašić, “基于强化学习的任务导向对话管理的基准测试环境”，*深度强化学习研讨会，NIPS*，2017年。'
- en: '[148] P.-H. Su, P. Budzianowski, S. Ultes, M. Gasic, and S. Young, “Sample-efficient
    actor-critic reinforcement learning with supervised data for dialogue management,”
    in *Annual SIGdial Meeting on Discourse and Dialogue*, 2017.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] P.-H. Su, P. Budzianowski, S. Ultes, M. Gasic, 和 S. Young, “用于对话管理的样本高效演员-评论家强化学习与监督数据”，见于*SIGdial对话与对话年会*，2017年。'
- en: '[149] M. Gašić and S. Young, “Gaussian processes for POMDP-based dialogue manager
    optimization,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    vol. 22, no. 1, 2013.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. Gašić 和 S. Young, “基于POMDP的对话管理器优化的高斯过程”，*IEEE/ACM音频、语音与语言处理交易*，第22卷，第1期，2013年。'
- en: '[150] S. Ultes, L. M. R. Barahona, P.-H. Su, D. Vandyke, D. Kim, I. Casanueva,
    P. Budzianowski, N. Mrkšić, T.-H. Wen, M. Gasic *et al.*, “Pydial: A multi-domain
    statistical dialogue system toolkit,” in *ACL System Demonstrations*, 2017.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. Ultes, L. M. R. Barahona, P.-H. Su, D. Vandyke, D. Kim, I. Casanueva,
    P. Budzianowski, N. Mrkšić, T.-H. Wen, M. Gasic *等*，“Pydial：一个多领域统计对话系统工具包”，见于*ACL系统演示*，2017年。'
- en: '[151] H. Cuayáhuitl, “Hierarchical reinforcement learning for spoken dialogue
    systems,” Ph.D. dissertation, University of Edinburgh, 2009.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] H. Cuayáhuitl, “针对口语对话系统的层次化强化学习”，博士学位论文，爱丁堡大学，2009年。'
- en: '[152] H. Cuayáhuitl, S. Renals, O. Lemon, and H. Shimodaira, “Evaluation of
    a hierarchical reinforcement learning spoken dialogue system,” *Comput. Speech
    Lang.*, vol. 24, no. 2, 2010.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] H. Cuayáhuitl, S. Renals, O. Lemon 和 H. Shimodaira，"层次化强化学习口语对话系统的评估"，*Comput.
    Speech Lang.*，第24卷，第2期，2010年。'
- en: '[153] N. Dethlefs and H. Cuayáhuitl, “Hierarchical reinforcement learning for
    situated natural language generation,” *Nat. Lang. Eng.*, vol. 21, no. 3, 2015.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] N. Dethlefs 和 H. Cuayáhuitl，"用于情境自然语言生成的层次化强化学习"，*Nat. Lang. Eng.*，第21卷，第3期，2015年。'
- en: '[154] P. Budzianowski, S. Ultes, P. Su, N. Mrksic, T. Wen, I. Casanueva, L. M.
    Rojas-Barahona, and M. Gasic, “Sub-domain modelling for dialogue management with
    hierarchical reinforcement learning,” in *Annual SIGdial Meeting on Discourse
    and Dialogue*, K. Jokinen, M. Stede, D. DeVault, and A. Louis, Eds.   ACL, 2017.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. Budzianowski, S. Ultes, P. Su, N. Mrksic, T. Wen, I. Casanueva, L.
    M. Rojas-Barahona 和 M. Gasic，"用于对话管理的子领域建模与层次化强化学习"，见 *Annual SIGdial Meeting
    on Discourse and Dialogue*，K. Jokinen, M. Stede, D. DeVault 和 A. Louis 编辑。 ACL，2017年。'
- en: '[155] B. Peng, X. Li, L. Li, J. Gao, A. Çelikyilmaz, S. Lee, and K. Wong, “Composite
    task-completion dialogue policy learning via hierarchical deep reinforcement learning,”
    in *Conference on Empirical Methods in Natural Language Processing EMNLP*, M. Palmer,
    R. Hwa, and S. Riedel, Eds.   ACL, 2017.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] B. Peng, X. Li, L. Li, J. Gao, A. Çelikyilmaz, S. Lee 和 K. Wong，"通过层次化深度强化学习学习综合任务完成对话策略"，见
    *Conference on Empirical Methods in Natural Language Processing EMNLP*，M. Palmer,
    R. Hwa 和 S. Riedel 编辑。 ACL，2017年。'
- en: '[156] J. Zhang, T. Zhao, and Z. Yu, “Multimodal hierarchical reinforcement
    learning policy for task-oriented visual dialog,” in *Annual SIGdial Meeting on
    Discourse and Dialogue, Melbourne, Australia, July 12-14, 2018*, K. Komatani,
    D. J. Litman, K. Yu, L. Cavedon, M. Nakano, and A. Papangelis, Eds.   ACL, 2018.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Zhang, T. Zhao 和 Z. Yu，"用于任务导向视觉对话的多模态层次化强化学习策略"，见 *Annual SIGdial
    Meeting on Discourse and Dialogue, Melbourne, Australia, July 12-14, 2018*，K.
    Komatani, D. J. Litman, K. Yu, L. Cavedon, M. Nakano 和 A. Papangelis 编辑。 ACL，2018年。'
- en: '[157] H. Cuayáhuitl, S. Yu, A. Williamson, and J. Carse, “Deep reinforcement
    learning for multi-domain dialogue systems,” *NIPS Workshop on Deep Reinforcement
    Learning*, 2016.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] H. Cuayáhuitl, S. Yu, A. Williamson 和 J. Carse，"多领域对话系统的深度强化学习"，*NIPS
    Workshop on Deep Reinforcement Learning*，2016年。'
- en: '[158] G. Gordon-Hall, P. J. Gorinski, and S. B. Cohen, “Learning dialog policies
    from weak demonstrations,” in *Annual Meeting of the Association for Computational
    Linguistics ACL*, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds.   ACL,
    2020.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] G. Gordon-Hall, P. J. Gorinski 和 S. B. Cohen，"从弱示范中学习对话策略"，见 *Annual
    Meeting of the Association for Computational Linguistics ACL*，D. Jurafsky, J.
    Chai, N. Schluter 和 J. R. Tetreault 编辑。 ACL，2020年。'
- en: '[159] P.-H. Su, M. Gasic, N. Mrkšić, L. M. R. Barahona, S. Ultes, D. Vandyke,
    T.-H. Wen, and S. Young, “On-line active reward learning for policy optimisation
    in spoken dialogue systems,” in *Annual Meeting of the Association for Computational
    Linguistics (ACL)*, 2016.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] P.-H. Su, M. Gasic, N. Mrkšić, L. M. R. Barahona, S. Ultes, D. Vandyke,
    T.-H. Wen 和 S. Young，"用于口语对话系统的在线主动奖励学习与策略优化"，见 *Annual Meeting of the Association
    for Computational Linguistics (ACL)*，2016年。'
- en: '[160] S. Ultes, P. Budzianowski, I. Casanueva, N. Mrkšić, L. Rojas-Barahona,
    P.-H. Su, T.-H. Wen, M. Gašić, and S. Young, “Domain-independent user satisfaction
    reward estimation for dialogue policy learning,” 2017.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] S. Ultes, P. Budzianowski, I. Casanueva, N. Mrkšić, L. Rojas-Barahona,
    P.-H. Su, T.-H. Wen, M. Gašić 和 S. Young，"用于对话策略学习的领域无关用户满意度奖励估计"，2017年。'
- en: '[161] M. Fazel-Zarandi, S.-W. Li, J. Cao, J. Casale, P. Henderson, D. Whitney,
    and A. Geramifard, “Learning robust dialog policies in noisy environments,” *Workshop
    on Conversational AI, NIPS*, 2017.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] M. Fazel-Zarandi, S.-W. Li, J. Cao, J. Casale, P. Henderson, D. Whitney
    和 A. Geramifard，"在噪声环境中学习鲁棒对话策略"，*Workshop on Conversational AI, NIPS*，2017年。'
- en: '[162] N. Carrara, R. Laroche, and O. Pietquin, “Online learning and transfer
    for user adaptation in dialogue systems,” in *SIGDIAL/SEMDIAL joint special session
    on negotiation dialog 2017*, 2017.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] N. Carrara, R. Laroche 和 O. Pietquin，"对话系统中的在线学习与迁移学习"，见 *SIGDIAL/SEMDIAL联合特别会议：谈判对话2017*，2017年。'
- en: '[163] N. Carrara, R. Laroche, J.-L. Bouraoui, T. Urvoy, and O. Pietquin, “Safe
    transfer learning for dialogue applications,” 2018.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] N. Carrara, R. Laroche, J.-L. Bouraoui, T. Urvoy 和 O. Pietquin，"对话应用的安全迁移学习"，2018年。'
- en: '[164] I. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,
    T. Kim, M. Pieper, S. Chandar, N. R. Ke *et al.*, “A deep reinforcement learning
    chatbot,” *arXiv preprint arXiv:1709.02349*, 2017.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] I. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,
    T. Kim, M. Pieper, S. Chandar, N. R. Ke *等*，"深度强化学习聊天机器人"，*arXiv 预印本 arXiv:1709.02349*，2017年。'
- en: '[165] H. Cuayáhuitl, D. Lee, S. Ryu, Y. Cho, S. Choi, S. R. Indurthi, S. Yu,
    H. Choi, I. Hwang, and J. Kim, “Ensemble-based deep reinforcement learning for
    chatbots,” *Neurocomputing*, vol. 366, 2019.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] H. Cuayáhuitl, D. Lee, S. Ryu, Y. Cho, S. Choi, S. R. Indurthi, S. Yu,
    H. Choi, I. Hwang, 和 J. Kim，“基于集成的深度强化学习用于聊天机器人，” *神经计算*，第366卷，2019年。'
- en: '[166] P. Ammanabrolu and M. Riedl, “Transfer in deep reinforcement learning
    using knowledge graphs,” in *Workshop on Graph-Based Methods for Natural Language
    Processing, TextGraphs@EMNLP*, D. Ustalov, S. Somasundaran, P. Jansen, G. Glavas,
    M. Riedl, M. Surdeanu, and M. Vazirgiannis, Eds.   Association for Computational
    Linguistics, 2019.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] P. Ammanabrolu 和 M. Riedl，“使用知识图谱进行深度强化学习中的迁移，” 在 *基于图的方法自然语言处理研讨会，TextGraphs@EMNLP*，D.
    Ustalov, S. Somasundaran, P. Jansen, G. Glavas, M. Riedl, M. Surdeanu, 和 M. Vazirgiannis
    编辑，计算语言学协会，2019年。'
- en: '[167] I. Casanueva, P. Budzianowski, P. Su, S. Ultes, L. M. Rojas-Barahona,
    B. Tseng, and M. Gasic, “Feudal reinforcement learning for dialogue management
    in large domains,” in *North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT)*, M. A. Walker, H. Ji, and
    A. Stent, Eds., 2018.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] I. Casanueva, P. Budzianowski, P. Su, S. Ultes, L. M. Rojas-Barahona,
    B. Tseng, 和 M. Gasic，“大规模领域中对话管理的封建式强化学习，” 在 *北美计算语言学协会：人类语言技术会议（NAACL-HLT）*，M.
    A. Walker, H. Ji, 和 A. Stent 编辑，2018年。'
- en: '[168] L. Chen, C. Chang, Z. Chen, B. Tan, M. Gasic, and K. Yu, “Policy adaptation
    for deep reinforcement learning-based dialogue management,” in *IEEE International
    Conference on Acoustics, Speech and Signal ICASSP*, 2018.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] L. Chen, C. Chang, Z. Chen, B. Tan, M. Gasic, 和 K. Yu，“基于深度强化学习的对话管理策略适应，”
    在 *IEEE国际声学、语音与信号处理会议 ICASSP*，2018年。'
- en: '[169] H. Cuayáhuitl, S. Yu, A. Williamson, and J. Carse, “Scaling up deep reinforcement
    learning for multi-domain dialogue systems,” in *International Joint Conference
    on Neural Networks, IJCNN*, 2017.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] H. Cuayáhuitl, S. Yu, A. Williamson, 和 J. Carse，“扩展深度强化学习用于多领域对话系统，”
    在 *国际神经网络联合会议，IJCNN*，2017年。'
- en: '[170] A. Das, S. Kottur, J. M. F. Moura, S. Lee, and D. Batra, “Learning cooperative
    visual dialog agents with deep reinforcement learning,” in *IEEE International
    Conference on Computer Vision, ICCV*, 2017.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] A. Das, S. Kottur, J. M. F. Moura, S. Lee, 和 D. Batra，“利用深度强化学习学习协作视觉对话代理，”
    在 *IEEE国际计算机视觉会议，ICCV*，2017年。'
- en: '[171] M. Fatemi, L. E. Asri, H. Schulz, J. He, and K. Suleman, “Policy networks
    with two-stage training for dialogue systems,” in *Annual Meeting of the Special
    Interest Group on Discourse and Dialogue (SIGDIAL)*, 2016.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] M. Fatemi, L. E. Asri, H. Schulz, J. He, 和 K. Suleman，“对话系统的两阶段训练策略网络，”
    在 *特别兴趣小组话语和对话年会（SIGDIAL）*，2016年。'
- en: '[172] M. Fazel-Zarandi, S. Li, J. Cao, J. Casale, P. Henderson, D. Whitney,
    and A. Geramifard, “Learning robust dialog policies in noisy environments,” *CoRR*,
    vol. abs/1712.04034, 2017.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] M. Fazel-Zarandi, S. Li, J. Cao, J. Casale, P. Henderson, D. Whitney,
    和 A. Geramifard，“在嘈杂环境中学习稳健的对话策略，” *计算机研究报告*，第abs/1712.04034卷，2017年。'
- en: '[173] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, À. Lapedriza, N. Jones,
    S. Gu, and R. W. Picard, “Way off-policy batch deep reinforcement learning of
    implicit human preferences in dialog,” *CoRR*, vol. abs/1907.00456, 2019.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, À. Lapedriza, N.
    Jones, S. Gu, 和 R. W. Picard，“偏离策略批量深度强化学习对话中的隐性人类偏好，” *计算机研究报告*，第abs/1907.00456卷，2019年。'
- en: '[174] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky, “Deep
    reinforcement learning for dialogue generation,” *CoRR*, vol. abs/1606.01541,
    2016.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, 和 D. Jurafsky，“用于对话生成的深度强化学习，”
    *计算机研究报告*，第abs/1606.01541卷，2016年。'
- en: '[175] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky, “Adversarial
    learning for neural dialogue generation,” in *Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, M. Palmer, R. Hwa, and S. Riedel, Eds.,
    2017.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, 和 D. Jurafsky，“对抗性学习用于神经对话生成，”
    在 *自然语言处理实证方法会议（EMNLP）*，M. Palmer, R. Hwa, 和 S. Riedel 编辑，2017年。'
- en: '[176] Z. C. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng, “Bbq-networks:
    Efficient exploration in deep reinforcement learning for task-oriented dialogue
    systems,” in *AAAI Conference on Artificial Intelligence*, S. A. McIlraith and
    K. Q. Weinberger, Eds., 2018.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Z. C. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, 和 L. Deng，“Bbq-networks：任务导向对话系统中的深度强化学习中的高效探索，”
    在 *AAAI人工智能会议*，S. A. McIlraith 和 K. Q. Weinberger 编辑，2018年。'
- en: '[177] K. Narasimhan, R. Barzilay, and T. S. Jaakkola, “Grounding language for
    transfer in deep reinforcement learning,” *J. Artif. Intell. Res.*, vol. 63, 2018.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] K. Narasimhan, R. Barzilay, 和 T. S. Jaakkola，“在深度强化学习中为迁移打基础，” *人工智能研究杂志*，第63卷，2018年。'
- en: '[178] B. Peng, X. Li, J. Gao, J. Liu, Y. Chen, and K. Wong, “Adversarial advantage
    actor-critic model for task-completion dialogue policy learning,” in *IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2018.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] B. Peng, X. Li, J. Gao, J. Liu, Y. Chen, 和 K. Wong，“用于任务完成对话策略学习的对抗优势演员-评论员模型”，发表于*IEEE国际声学、语音和信号处理会议（ICASSP）*，2018年。'
- en: '[179] A. Saleh, N. Jaques, A. Ghandeharioun, J. H. Shen, and R. W. Picard,
    “Hierarchical reinforcement learning for open-domain dialog,” in *AAAI Conference
    on Artificial Intelligence*, 2020.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] A. Saleh, N. Jaques, A. Ghandeharioun, J. H. Shen, 和 R. W. Picard，“用于开放域对话的层次化强化学习”，发表于*AAAI人工智能会议*，2020年。'
- en: '[180] C. Sankar and S. Ravi, “Deep reinforcement learning for modeling chit-chat
    dialog with discrete attributes,” in *SIGdial Meeting on Discourse and Dialogue*,
    S. Nakamura, M. Gasic, I. Zuckerman, G. Skantze, M. Nakano, A. Papangelis, S. Ultes,
    and K. Yoshino, Eds., 2019.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] C. Sankar 和 S. Ravi，“用于建模闲聊对话的深度强化学习，具有离散属性”，发表于*SIGdial话语与对话会议*，S. Nakamura,
    M. Gasic, I. Zuckerman, G. Skantze, M. Nakano, A. Papangelis, S. Ultes, 和 K. Yoshino主编，2019年。'
- en: '[181] I. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,
    T. Kim, M. Pieper, S. Chandar, N. R. Ke, S. Mudumba, A. de Brébisson, J. Sotelo,
    D. Suhubdy, V. Michalski, A. Nguyen, J. Pineau, and Y. Bengio, “A deep reinforcement
    learning chatbot,” *CoRR*, vol. abs/1709.02349, 2017.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] I. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,
    T. Kim, M. Pieper, S. Chandar, N. R. Ke, S. Mudumba, A. de Brébisson, J. Sotelo,
    D. Suhubdy, V. Michalski, A. Nguyen, J. Pineau, 和 Y. Bengio，“深度强化学习聊天机器人”，*CoRR*，第abs/1709.02349号，2017年。'
- en: '[182] P. Su, P. Budzianowski, S. Ultes, M. Gasic, and S. J. Young, “Sample-efficient
    actor-critic reinforcement learning with supervised data for dialogue management,”
    *CoRR*, vol. abs/1707.00130, 2017.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] P. Su, P. Budzianowski, S. Ultes, M. Gasic, 和 S. J. Young，“结合监督数据的样本高效演员-评论员强化学习用于对话管理”，*CoRR*，第abs/1707.00130号，2017年。'
- en: '[183] S. Ultes, P. Budzianowski, I. Casanueva, N. Mrksic, L. M. Rojas-Barahona,
    P. Su, T. Wen, M. Gasic, and S. J. Young, “Domain-independent user satisfaction
    reward estimation for dialogue policy learning,” in *Conference of the International
    Speech Communication Association (INTERSPEECH)*, F. Lacerda, Ed., 2017.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] S. Ultes, P. Budzianowski, I. Casanueva, N. Mrksic, L. M. Rojas-Barahona,
    P. Su, T. Wen, M. Gasic, 和 S. J. Young，“用于对话策略学习的领域独立用户满意度奖励估计”，发表于*国际语音通信协会会议（INTERSPEECH）*，F.
    Lacerda主编，2017年。'
- en: '[184] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end knowledge-routed
    relational dialogue system for automatic diagnosis,” in *AAAI Conference on Artificial
    Intelligence*, 2019.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, 和 L. Lin，“用于自动诊断的端到端知识路由关系对话系统”，发表于*AAAI人工智能会议*，2019年。'
- en: '[185] G. Weisz, P. Budzianowski, P. Su, and M. Gasic, “Sample efficient deep
    reinforcement learning for dialogue systems with large action spaces,” *CoRR*,
    vol. abs/1802.03753, 2018.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] G. Weisz, P. Budzianowski, P. Su, 和 M. Gasic，“用于对话系统的大动作空间的样本高效深度强化学习”，*CoRR*，第abs/1802.03753号，2018年。'
- en: '[186] J. D. Williams and G. Zweig, “End-to-end lstm-based dialog control optimized
    with supervised and reinforcement learning,” *CoRR*, vol. abs/1606.01269, 2016.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] J. D. Williams 和 G. Zweig，“基于LSTM的端到端对话控制，通过监督学习和强化学习优化”，*CoRR*，第abs/1606.01269号，2016年。'
- en: '[187] T. Zhao and M. Eskénazi, “Towards end-to-end learning for dialog state
    tracking and management using deep reinforcement learning,” *CoRR*, vol. abs/1606.02560,
    2016.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] T. Zhao 和 M. Eskénazi，“通过深度强化学习实现对话状态跟踪和管理的端到端学习”，*CoRR*，第abs/1606.02560号，2016年。'
- en: '[188] T. Zhao, K. Xie, and M. Eskénazi, “Rethinking action spaces for reinforcement
    learning in end-to-end dialog agents with latent variable models,” in *Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT)*, J. Burstein, C. Doran, and T. Solorio,
    Eds., 2019.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] T. Zhao, K. Xie, 和 M. Eskénazi，“重新思考端到端对话代理中的动作空间与潜变量模型”，发表于*北美计算语言学协会：人类语言技术会议（NAACL-HLT）*，J.
    Burstein, C. Doran, 和 T. Solorio主编，2019年。'
- en: '[189] R. Takanobu, H. Zhu, and M. Huang, “Guided dialog policy learning: Reward
    estimation for multi-domain task-oriented dialog,” in *Proceedings of the 2019
    Conference on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
    China, November 3-7, 2019*, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds., 2019.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] R. Takanobu, H. Zhu, 和 M. Huang，“引导对话策略学习：多领域任务导向对话的奖励估计”，发表于*2019年自然语言处理经验方法会议和第9届国际联合自然语言处理会议，EMNLP-IJCNLP
    2019，香港，中国，2019年11月3-7日*，K. Inui, J. Jiang, V. Ng, 和 X. Wan主编，2019年。'
- en: '[190] S. Latif, J. Qadir, and M. Bilal, “Unsupervised adversarial domain adaptation
    for cross-lingual speech emotion recognition,” in *International Conference on
    Affective Computing and Intelligent Interaction (ACII)*, 2019.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] S. Latif, J. Qadir, 和 M. Bilal, “针对跨语言语音情感识别的无监督对抗性领域适应，” 在 *国际情感计算与智能互动会议（ACII）*，2019年。'
- en: '[191] Z. Wang, S. Ho, and E. Cambria, “A review of emotion sensing: Categorization
    models and algorithms,” *Multimedia Tools and Applications*, 2020.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Z. Wang, S. Ho, 和 E. Cambria, “情感感知的回顾：分类模型和算法，” *多媒体工具与应用*，2020年。'
- en: '[192] Y. Ma, K. L. Nguyen, F. Xing, and E. Cambria, “A survey on empathetic
    dialogue systems,” *Information Fusion*, vol. 64, 2020.'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Y. Ma, K. L. Nguyen, F. Xing, 和 E. Cambria, “关于同理心对话系统的综述，” *信息融合*，第64卷，2020年。'
- en: '[193] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh, and E. Cambria,
    “DialogueRNN: An attentive RNN for emotion detection in conversations,” in *AAAI
    Conference on Artificial Intelligence*, vol. 33, 2019.'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh, 和 E. Cambria,
    “DialogueRNN：一种用于对话中情感检测的注意力RNN，” 在 *AAAI人工智能会议*，第33卷，2019年。'
- en: '[194] S. Poria, N. Majumder, R. Mihalcea, and E. Hovy, “Emotion recognition
    in conversation: Research challenges, datasets, and recent advances,” *IEEE Access*,
    vol. 7, 2019.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] S. Poria, N. Majumder, R. Mihalcea, 和 E. Hovy, “对话中的情感识别：研究挑战、数据集和最新进展，”
    *IEEE Access*，第7卷，2019年。'
- en: '[195] T. Young, V. Pandelea, S. Poria, and E. Cambria, “Dialogue systems with
    audio context,” *Neurocomputing*, vol. 388, 2020.'
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] T. Young, V. Pandelea, S. Poria, 和 E. Cambria, “带有音频上下文的对话系统，” *神经计算*，第388卷，2020年。'
- en: '[196] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, “Emotional chatting
    machine: Emotional conversation generation with internal and external memory,”
    in *AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] H. Zhou, M. Huang, T. Zhang, X. Zhu, 和 B. Liu, “情感聊天机器：具有内部和外部记忆的情感对话生成，”
    在 *AAAI人工智能会议*，2018年。'
- en: '[197] V. Heusser, N. Freymuth, S. Constantin, and A. Waibel, “Bimodal speech
    emotion recognition using pre-trained language models,” *arXiv preprint arXiv:1912.02610*,
    2019.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] V. Heusser, N. Freymuth, S. Constantin, 和 A. Waibel, “使用预训练语言模型的双模态语音情感识别，”
    *arXiv预印本arXiv:1912.02610*，2019年。'
- en: '[198] X. Ouyang, S. Nagisetty, E. G. H. Goh, S. Shen, W. Ding, H. Ming, and
    D.-Y. Huang, “Audio-visual emotion recognition with capsule-like feature representation
    and model-based reinforcement learning,” in *2018 First Asian Conference on Affective
    Computing and Intelligent Interaction (ACII Asia)*.   IEEE, 2018, pp. 1–6.'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] X. Ouyang, S. Nagisetty, E. G. H. Goh, S. Shen, W. Ding, H. Ming, 和 D.-Y.
    Huang, “具有胶囊型特征表示和基于模型的强化学习的视听情感识别，” 在 *2018年第一个亚洲情感计算与智能互动会议（ACII Asia）*。IEEE，2018年，第1–6页。'
- en: '[199] E. Lakomkin, M. A. Zamani, C. Weber, S. Magg, and S. Wermter, “Emorl:
    continuous acoustic emotion classification using deep reinforcement learning,”
    in *IEEE International Conference on Robotics and Automation (ICRA)*, 2018.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] E. Lakomkin, M. A. Zamani, C. Weber, S. Magg, 和 S. Wermter, “Emorl：使用深度强化学习的连续声学情感分类，”
    在 *IEEE国际机器人与自动化会议（ICRA）*，2018年。'
- en: '[200] J. Sangeetha and T. Jayasankar, “Emotion speech recognition based on
    adaptive fractional deep belief network and reinforcement learning,” in *Cognitive
    Informatics and Soft Computing*.   Springer, 2019.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. Sangeetha 和 T. Jayasankar, “基于自适应分数深度信念网络和强化学习的情感语音识别，” 在 *认知信息学与软计算*。Springer，2019年。'
- en: '[201] M. Chen, S. Wang, P. P. Liang, T. Baltrušaitis, A. Zadeh, and L.-P. Morency,
    “Multimodal sentiment analysis with word-level fusion and reinforcement learning,”
    in *ACM International Conference on Multimodal Interaction*, 2017.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] M. Chen, S. Wang, P. P. Liang, T. Baltrušaitis, A. Zadeh, 和 L.-P. Morency,
    “结合词级融合和强化学习的多模态情感分析，” 在 *ACM国际多模态互动会议*，2017年。'
- en: '[202] J. Li, L. Deng, R. Haeb-Umbach, and Y. Gong, *Robust automatic speech
    recognition: a bridge to practical applications*.   Academic Press, 2015.'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] J. Li, L. Deng, R. Haeb-Umbach, 和 Y. Gong, *鲁棒的自动语音识别：通向实际应用的桥梁*。学术出版社，2015年。'
- en: '[203] B. Li, Y. Tsao, and K. C. Sim, “An investigation of spectral restoration
    algorithms for deep neural networks based noise robust speech recognition.” in
    *Interspeech*, 2013.'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] B. Li, Y. Tsao, 和 K. C. Sim, “针对深度神经网络的噪声鲁棒语音识别的谱恢复算法研究。” 在 *Interspeech*，2013年。'
- en: '[204] Z.-Q. Wang and D. Wang, “A joint training framework for robust automatic
    speech recognition,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    vol. 24, no. 4, 2016.'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Z.-Q. Wang 和 D. Wang, “用于鲁棒自动语音识别的联合训练框架，” *IEEE/ACM音频、语音与语言处理汇刊*，第24卷，第4期，2016年。'
- en: '[205] D. Baby, J. F. Gemmeke, T. Virtanen *et al.*, “Exemplar-based speech
    enhancement for deep neural network based automatic speech recognition,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2015.'
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] D. Baby, J. F. Gemmeke, T. Virtanen *等*，“基于示例的语音增强用于深度神经网络自动语音识别”，在*国际声学、语音和信号处理会议（ICASSP）*上，2015年。'
- en: '[206] D. Wang and J. Chen, “Supervised speech separation based on deep learning:
    An overview,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    vol. 26, no. 10, 2018.'
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] D. Wang 和 J. Chen，“基于深度学习的监督语音分离：概述”，*IEEE/ACM 音频、语音和语言处理汇刊*，第26卷，第10期，2018年。'
- en: '[207] Y.-L. Shen, C.-Y. Huang, S.-S. Wang, Y. Tsao, H.-M. Wang, and T.-S. Chi,
    “Reinforcement learning based speech enhancement for robust speech recognition,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2019.'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y.-L. Shen, C.-Y. Huang, S.-S. Wang, Y. Tsao, H.-M. Wang, 和 T.-S. Chi，“基于强化学习的语音增强以实现鲁棒的语音识别”，在*国际声学、语音和信号处理会议（ICASSP）*上，2019年。'
- en: '[208] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, “DNN-based
    source enhancement self-optimized by reinforcement learning using sound quality
    measurements,” in *International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2017.'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, 和 Y. Haneda，“基于DNN的源增强通过使用声音质量测量的强化学习自我优化”，在*国际声学、语音和信号处理会议（ICASSP）*上，2017年。'
- en: '[209] I.-T. Recommendation, “Perceptual evaluation of speech quality (PESQ):
    An objective method for end-to-end speech quality assessment of narrow-band telephone
    networks and speech codecs,” *Rec. ITU-T P. 862*, 2001.'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] I.-T. Recommendation，“语音质量的感知评估（PESQ）：窄带电话网络和语音编解码器的端到端语音质量评估的客观方法”，*Rec.
    ITU-T P. 862*，2001年。'
- en: '[210] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective and objective
    quality assessment of audio source separation,” *IEEE Transactions on Audio, Speech,
    and Language Processing*, vol. 19, no. 7, 2011.'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] V. Emiya, E. Vincent, N. Harlander, 和 V. Hohmann，“音频源分离的主观与客观质量评估”，*IEEE
    音频、语音和语言处理汇刊*，第19卷，第7期，2011年。'
- en: '[211] R. Fakoor, X. He, I. Tashev, and S. Zarar, “Reinforcement learning to
    adapt speech enhancement to instantaneous input signal quality,” *Machine Learning
    for Audio Signal Processing workshop, NIPS*, 2017.'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] R. Fakoor, X. He, I. Tashev, 和 S. Zarar，“通过强化学习使语音增强适应瞬时输入信号质量”，*音频信号处理机器学习研讨会，NIPS*，2017年。'
- en: '[212] N. Alamdari, E. Lobarinas, and N. Kehtarnavaz, “Personalization of hearing
    aid compression by human-in-the-loop deep reinforcement learning,” *IEEE Access*,
    vol. 8, pp. 203 503–203 515, 2020.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] N. Alamdari, E. Lobarinas, 和 N. Kehtarnavaz，“通过人机互动深度强化学习个性化助听器压缩”，*IEEE
    Access*，第8卷，页203 503–203 515，2020年。'
- en: '[213] M. J. Steedman, “A generative grammar for jazz chord sequences,” *Music
    Perception: An Interdisciplinary Journal*, vol. 2, no. 1, 1984.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] M. J. Steedman，“一种用于爵士和弦序列的生成语法”，*音乐感知：跨学科期刊*，第2卷，第1期，1984年。'
- en: '[214] K. Ebcioğlu, “An expert system for harmonizing four-part chorales,” *Computer
    Music Journal*, vol. 12, no. 3, 1988.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] K. Ebcioğlu，“一个用于和声四部合唱的专家系统”，*计算机音乐期刊*，第12卷，第3期，1988年。'
- en: '[215] N. Jaques, S. Gu, R. E. Turner, and D. Eck, “Generating music by fine-tuning
    recurrent neural networks with reinforcement learning,” 2016.'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] N. Jaques, S. Gu, R. E. Turner, 和 D. Eck，“通过强化学习微调递归神经网络生成音乐”，2016年。'
- en: '[216] N. Kotecha, “Bach2Bach: Generating music using a deep reinforcement learning
    approach,” *arXiv preprint arXiv:1812.01060*, 2018.'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] N. Kotecha，“Bach2Bach：使用深度强化学习方法生成音乐”，*arXiv 预印本 arXiv:1812.01060*，2018年。'
- en: '[217] N. Jiang, S. Jin, Z. Duan, and C. Zhang, “Rl-duet: Online music accompaniment
    generation using deep reinforcement learning,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 01, 2020, pp. 710–718.'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] N. Jiang, S. Jin, Z. Duan, 和 C. Zhang，“Rl-duet：使用深度强化学习的在线音乐伴奏生成”，在*AAAI人工智能会议论文集*，第34卷，第01期，2020年，页710–718。'
- en: '[218] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-dimensional
    continuous control using generalized advantage estimation,” *International Conference
    on Learning Representations (ICLR)*, 2016.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] J. Schulman, P. Moritz, S. Levine, M. Jordan, 和 P. Abbeel，“使用广义优势估计的高维连续控制”，*国际学习表征会议（ICLR）*，2016年。'
- en: '[219] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias,
    and A. Aspuru-Guzik, “Objective-reinforced generative adversarial networks (organ)
    for sequence generation models,” *arXiv preprint arXiv:1705.10843*, 2017.'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias,
    和 A. Aspuru-Guzik，“用于序列生成模型的目标增强生成对抗网络（organ）”，*arXiv 预印本 arXiv:1705.10843*，2017年。'
- en: '[220] S.-g. Lee, U. Hwang, S. Min, and S. Yoon, “Polyphonic music generation
    with sequence generative adversarial networks,” *arXiv preprint arXiv:1710.11418*,
    2017.'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] S.-g. Lee, U. Hwang, S. Min, 和 S. Yoon, “使用序列生成对抗网络生成多声部音乐，” *arXiv 预印本
    arXiv:1710.11418*，2017年。'
- en: '[221] Q. Lan, J. Tørresen, and A. R. Jensenius, “RaveForce: A deep reinforcement
    learning environment for music,” in *Proc. of the SMC Conferences*.   Society
    for Sound and Music Computing, 2019.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Q. Lan, J. Tørresen, 和 A. R. Jensenius, “RaveForce: 一个用于音乐的深度强化学习环境，”
    见 *SMC 会议论文集*。声音与音乐计算学会，2019年。'
- en: '[222] M. Dorfer, F. Henkel, and G. Widmer, “Learning to listen, read, and follow:
    Score following as a reinforcement learning game,” *International Society for
    Music Information Retrieval Conference*, 2018.'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] M. Dorfer, F. Henkel, 和 G. Widmer, “学习倾听、阅读和跟随：将乐谱跟随视为强化学习游戏，” *国际音乐信息检索学会会议*，2018年。'
- en: '[223] F. Henkel, S. Balke, M. Dorfer, and G. Widmer, “Score following as a
    multi-modal reinforcement learning problem,” *Transactions of the International
    Society for Music Information Retrieval*, vol. 2, no. 1, 2019.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] F. Henkel, S. Balke, M. Dorfer, 和 G. Widmer, “将乐谱跟随视为多模态强化学习问题，” *国际音乐信息检索学会期刊*，第2卷，第1期，2019年。'
- en: '[224] N. Howard and E. Cambria, “Intention awareness: Improving upon situation
    awareness in human-centric environments,” *Human-centric Computing and Information
    Sciences*, vol. 3, no. 9, 2013.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] N. Howard 和 E. Cambria, “意图意识：改善人本环境中的情况意识，” *人本计算与信息科学*，第3卷，第9期，2013年。'
- en: '[225] M. A. Goodrich and A. C. Schultz, “Human-robot interaction: a survey,”
    *Foundations and trends in human-computer interaction*, vol. 1, no. 3, 2007.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] M. A. Goodrich 和 A. C. Schultz, “人机交互：综述，” *人机交互基础与趋势*，第1卷，第3期，2007年。'
- en: '[226] S. Gao, W. Hou, T. Tanaka, and T. Shinozaki, “Spoken language acquisition
    based on reinforcement learning and word unit segmentation,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2020.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] S. Gao, W. Hou, T. Tanaka, 和 T. Shinozaki, “基于强化学习和词单元分割的口语语言习得，” 见 *国际声学、语音与信号处理会议（ICASSP）*，2020年。'
- en: '[227] B. F. Skinner, “Verbal behavior. new york: appleton-century-crofts,”
    *Richard-Amato, P.(1996)*, vol. 11, 1957.'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] B. F. Skinner, “言语行为。纽约：Appleton-Century-Crofts，” *Richard-Amato, P.(1996)*，第11卷，1957年。'
- en: '[228] H. Yu, H. Zhang, and W. Xu, “Interactive grounded language acquisition
    and generalization in a 2D world,” in *International Conference on Learning Representations*,
    2018.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] H. Yu, H. Zhang, 和 W. Xu, “在二维世界中的交互式基础语言习得与泛化，” 见 *国际学习表征会议*，2018年。'
- en: '[229] A. Sinha, B. Akilesh, M. Sarkar, and B. Krishnamurthy, “Attention based
    natural language grounding by navigating virtual environment,” in *IEEE Winter
    Conference on Applications of Computer Vision (WACV)*, 2019.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] A. Sinha, B. Akilesh, M. Sarkar, 和 B. Krishnamurthy, “基于注意力的自然语言基础通过虚拟环境导航，”
    见 *IEEE 冬季计算机视觉应用会议（WACV）*，2019年。'
- en: '[230] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari,
    W. M. Czarnecki, M. Jaderberg, D. Teplyashin *et al.*, “Grounded language learning
    in a simulated 3D world,” *arXiv preprint arXiv:1706.06551*, 2017.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D.
    Szepesvari, W. M. Czarnecki, M. Jaderberg, D. Teplyashin *等*，“在模拟三维世界中的基础语言学习，”
    *arXiv 预印本 arXiv:1706.06551*，2017年。'
- en: '[231] F. Hill, K. M. Hermann, P. Blunsom, and S. Clark, “Understanding grounded
    language learning agents,” 2018.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] F. Hill, K. M. Hermann, P. Blunsom, 和 S. Clark, “理解基础语言学习代理，” 2018年。'
- en: '[232] S. Lathuilière, B. Massé, P. Mesejo, and R. Horaud, “Neural network based
    reinforcement learning for audio–visual gaze control in human–robot interaction,”
    *Pattern Recognition Letters*, vol. 118, 2019.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] S. Lathuilière, B. Massé, P. Mesejo, 和 R. Horaud, “基于神经网络的强化学习用于人机交互中的音频-视觉注视控制，”
    *模式识别快报*，第118卷，2019年。'
- en: '[233] ——, “Deep reinforcement learning for audio-visual gaze control,” in *IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS)*, 2018.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] ——, “用于音频视觉注视控制的深度强化学习，” 见 *IEEE/RSJ 国际智能机器人与系统会议（IROS）*，2018年。'
- en: '[234] M. Clark-Turner and M. Begum, “Deep reinforcement learning of abstract
    reasoning from demonstrations,” in *ACM/IEEE International Conference on Human-Robot
    Interaction*, 2018.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] M. Clark-Turner 和 M. Begum, “从演示中进行抽象推理的深度强化学习，” 见 *ACM/IEEE 国际人机交互会议*，2018年。'
- en: '[235] N. Hussain, E. Erzin, T. M. Sezgin, and Y. Yemez, “Speech driven backchannel
    generation using deep q-network for enhancing engagement in human-robot interaction,”
    in *Interspeech*, 2019.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] N. Hussain, E. Erzin, T. M. Sezgin, 和 Y. Yemez, “利用深度 Q 网络生成语音驱动的反馈通道，以增强人机交互中的参与感，”
    见 *Interspeech*，2019年。'
- en: '[236] ——, “Batch recurrent Q-learning for backchannel generation towards engaging
    agents,” in *International Conference on Affective Computing and Intelligent Interaction
    (ACII)*, 2019.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] ——，“用于生成互动回复的批量递归 Q 学习，”在 *International Conference on Affective Computing
    and Intelligent Interaction (ACII)*，2019年。'
- en: '[237] H. Bui and N. Y. Chong, “Autonomous speech volume control for social
    robots in a noisy environment using deep reinforcement learning,” in *IEEE International
    Conference on Robotics and Biomimetics (ROBIO)*, 2019.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] H. Bui 和 N. Y. Chong，“在嘈杂环境中使用深度强化学习的社交机器人自主音量控制，”在 *IEEE International
    Conference on Robotics and Biomimetics (ROBIO)*，2019年。'
- en: '[238] M. Zamani, S. Magg, C. Weber, S. Wermter, and D. Fu, “Deep reinforcement
    learning using compositional representations for performing instructions,” *Paladyn
    J. Behav. Robotics*, vol. 9, no. 1, 2018.'
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] M. Zamani, S. Magg, C. Weber, S. Wermter, 和 D. Fu，“使用组合表示的深度强化学习执行指令，”
    *Paladyn J. Behav. Robotics*，第 9 卷，第 1 期，2018年。'
- en: '[239] I. Moreira, J. Rivas, F. Cruz, R. Dazeley, A. Ayala, and B. J. T. Fernandes,
    “Deep reinforcement learning with interactive feedback in a human-robot environment,”
    *CoRR*, vol. abs/2007.03363, 2020.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] I. Moreira, J. Rivas, F. Cruz, R. Dazeley, A. Ayala, 和 B. J. T. Fernandes，“在人与机器人环境中的交互反馈下的深度强化学习，”
    *CoRR*，第 abs/2007.03363 号，2020年。'
- en: '[240] T. Fryen, M. Eppe, P. D. H. Nguyen, T. Gerkmann, and S. Wermter, “Reinforcement
    learning with time-dependent goals for robotic musicians,” *CoRR*, vol. abs/2011.05715,
    2020.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] T. Fryen, M. Eppe, P. D. H. Nguyen, T. Gerkmann, 和 S. Wermter，“具有时间依赖目标的机器人音乐家强化学习，”
    *CoRR*，第 abs/2011.05715 号，2020年。'
- en: '[241] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The Arcade learning
    environment: An evaluation platform for general agents,” *J. Artif. Intell. Res.*,
    vol. 47, 2013.'
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] M. G. Bellemare, Y. Naddaf, J. Veness, 和 M. Bowling，“街机学习环境：通用代理的评估平台，”
    *J. Artif. Intell. Res.*，第 47 卷，2013年。'
- en: '[242] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *International Conference on Machine Learning
    (ICML)*, 2017.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] C. Finn, P. Abbeel, 和 S. Levine，“无模型元学习用于深度网络的快速适应，”在 *International
    Conference on Machine Learning (ICML)*，2017年。'
- en: '[243] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep reinforcement
    learning in a handful of trials using probabilistic dynamics models,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2018.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] K. Chua, R. Calandra, R. McAllister, 和 S. Levine，“使用概率动力学模型在少量试验中进行深度强化学习，”在
    *Advances in Neural Information Processing Systems (NIPS)*，2018年。'
- en: '[244] J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee, “Sample-efficient
    reinforcement learning with stochastic ensemble value expansion,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2018.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] J. Buckman, D. Hafner, G. Tucker, E. Brevdo, 和 H. Lee，“使用随机集成价值扩展的样本高效强化学习，”在
    *Advances in Neural Information Processing Systems (NIPS)*，2018年。'
- en: '[245] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy distillation,” *arXiv
    preprint arXiv:1511.06295*, 2015.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, 和 R. Hadsell，“策略蒸馏，” *arXiv preprint arXiv:1511.06295*，2015年。'
- en: '[246] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural networks,” *NIPS
    Deep Learning Symposium recommendation*, 2016.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, 和 R. Hadsell，“渐进神经网络，” *NIPS Deep Learning Symposium
    recommendation*，2016年。'
- en: '[247] X. Li, L. Li, J. Gao, X. He, J. Chen, L. Deng, and J. He, “Recurrent
    reinforcement learning: a hybrid approach,” *arXiv preprint arXiv:1509.03044*,
    2015.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] X. Li, L. Li, J. Gao, X. He, J. Chen, L. Deng, 和 J. He，“递归强化学习：一种混合方法，”
    *arXiv preprint arXiv:1509.03044*，2015年。'
- en: '[248] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    and K. Kavukcuoglu, “Reinforcement learning with unsupervised auxiliary tasks,”
    *International Conference on Learning Representations (ICLR)*, 2016.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    和 K. Kavukcuoglu，“带有无监督辅助任务的强化学习，” *International Conference on Learning Representations
    (ICLR)*，2016年。'
- en: '[249] H. Yin and S. J. Pan, “Knowledge transfer for deep reinforcement learning
    with hierarchical experience replay,” in *AAAI Conference*, 2017.'
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] H. Yin 和 S. J. Pan，“具有层次经验重放的深度强化学习中的知识转移，”在 *AAAI Conference*，2017年。'
- en: '[250] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multiagent systems: A review of challenges, solutions, and applications,”
    *IEEE transactions on cybernetics*, 2020.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] T. T. Nguyen, N. D. Nguyen, 和 S. Nahavandi，“深度强化学习在多智能体系统中的应用：挑战、解决方案和应用的回顾，”
    *IEEE Transactions on Cybernetics*，2020年。'
- en: '[251] R. Glatt, F. L. Da Silva, and A. H. R. Costa, “Towards knowledge transfer
    in deep reinforcement learning,” in *Brazilian Conference on Intelligent Systems
    (BRACIS)*, 2016.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] R. Glatt, F. L. Da Silva 和 A. H. R. Costa，“朝向深度强化学习中的知识迁移”，发表于 *巴西智能系统会议
    (BRACIS)*，2016年。'
- en: '[252] K. Mo, Y. Zhang, S. Li, J. Li, and Q. Yang, “Personalizing a dialogue
    system with transfer reinforcement learning,” in *AAAI Conference*, 2018.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] K. Mo, Y. Zhang, S. Li, J. Li 和 Q. Yang，“利用迁移强化学习个性化对话系统”，发表于 *AAAI 会议*，2018年。'
- en: '[253] M. L. Littman, “Markov games as a framework for multi-agent reinforcement
    learning,” in *Machine learning proceedings 1994*.   Elsevier, 1994.'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] M. L. Littman，“马尔可夫游戏作为多智能体强化学习的框架”，发表于 *机器学习论文集 1994*。Elsevier，1994年。'
- en: '[254] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
    of multiagent deep reinforcement learning,” *Autonomous Agents and Multi-Agent
    Systems*, vol. 33, no. 6, 2019.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] P. Hernandez-Leal, B. Kartal 和 M. E. Taylor，“多智能体深度强化学习的调查与批评”，*自主智能体与多智能体系统*，第33卷，第6期，2019年。'
