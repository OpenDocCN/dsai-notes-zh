- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:57:39'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2101.00240] A Survey on Deep Reinforcement Learning for Audio-Based Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2101.00240](https://ar5iv.labs.arxiv.org/html/2101.00240)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: for Audio-Based Applications
  prefs: []
  type: TYPE_NORMAL
- en: 'Siddique Latif Email: siddique.latif@usq.edu.au University of Southern Queensland,
    Australia Heriberto Cuayáhuitl University of Lincoln, United Kingdom Farrukh Pervez
    National University of Science and Technology, Pakistan Fahad Shamshad Information
    Technology University, Pakistan Hafiz Shehbaz Ali EmulationAI Erik Cambria Nanyang
    Technological University, Singapore'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial
    intelligence (AI) by endowing autonomous systems with high levels of understanding
    of the real world. Currently, deep learning (DL) is enabling DRL to effectively
    solve various intractable problems in various fields. Most importantly, DRL algorithms
    are also being employed in audio signal processing to learn directly from speech,
    music and other sound signals in order to create audio-based autonomous systems
    that have many promising application in the real world. In this article, we conduct
    a comprehensive survey on the progress of DRL in the audio domain by bringing
    together the research studies across different speech and music related areas.
    We begin with an introduction to the general field of DL and reinforcement learning
    (RL), then progress to the main DRL methods and their applications in the audio
    domain. We conclude by presenting challenges faced by audio-based DRL agents and
    highlighting open areas for future research and investigation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, reinforcement learning, speech recognition, emotion recognition,
    (embodied) dialogue systems
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Artificial intelligence (AI) has gained widespread attention in many areas of
    life, especially in audio signal processing. Audio processing covers many diverse
    fields including speech, music and environmental sound processing. In all these
    areas, AI techniques are playing crucial roles in the design of audio-based intelligent
    systems [[1](#bib.bib1)]. One of the prime goals of AI is to create fully autonomous
    audio-based intelligent systems or agents that can learn optimal behaviours by
    listening or interacting with their environments and improving their behaviour
    over time through trial and error. Designing of such autonomous systems has been
    a long-standing problem, ranging from robots that can react to the changes in
    their environment, to purely software-based agents that can interact with humans
    using natural language and multimedia. Reinforcement learning (RL) [[2](#bib.bib2)]
    represents a principled mathematical framework of such experience-driven learning.
    Although RL had some successes in the past [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)],
    however, previous methods were inherently limited to low-dimensional problems
    due to lack of scalability. Moreover, RL also has issues of memory, computational
    and sample complexity—in the case of learning algorithms [[6](#bib.bib6)]. Recently,
    deep learning (DL) models have risen as new tools with powerful function approximation
    and representation learning properties to solve these issues.
  prefs: []
  type: TYPE_NORMAL
- en: The advent of DL has had a significant impact on many areas of machine learning
    (ML) by dramatically improving the state-of-the-art in image processing tasks
    such as object detection and image classification. Deep models such as deep neural
    networks (DNNs) [[7](#bib.bib7), [8](#bib.bib8)], convolutional neural networks
    (CNNs) [[9](#bib.bib9)], and long short-term memory (LSTM) networks [[10](#bib.bib10)]
    have also enabled many practical applications by outperforming traditional methods
    in audio signal processing. Given that DL has also accelerated RL’s progress with
    the use of DL algorithms within RL, it has given rise to the field of deep reinforcement
    learning (DRL).
  prefs: []
  type: TYPE_NORMAL
- en: DRL embraces the advancements in DL to establish the learning processes, performance
    and speed of RL algorithms. This enables RL to operate in high-dimensional state
    and action spaces to solve complex problems that were previously difficult to
    solve. As a result, DRL has been adopted to solve many problems. Inspired by previous
    works such as [[11](#bib.bib11)], two outstanding works kick-started the revolution
    in DRL. The first was the development of an algorithm that could learn to play
    Atari 2600 video games directly from image pixels at a superhuman level [[12](#bib.bib12)].
    The second success was design of the hybrid DRL system, AlphaGo, which defeated
    a human world champion in the game of Go [[13](#bib.bib13)]. In addition to playing
    games, DRL has also been applied to a wide range of problems including robotics
    to control policies [[14](#bib.bib14)]; generalisable agents in complex environments
    with meta-learning [[15](#bib.bib15), [16](#bib.bib16)]; indoor navigation [[17](#bib.bib17)],
    and many more [[18](#bib.bib18)]. In particular, DRL is also gaining increased
    interest in audio signal processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Comparison of our paper with that of the existing surveys.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Focus |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reference |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Deep &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reinforcement &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Audio &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Applications &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Other &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Applications &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Details |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Arulkumaran et al. [[18](#bib.bib18)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2017 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; This paper presents a brief overview of recent developments in &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DRL algorithms and highlights the benefits of DRL and several &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; current areas of research. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Yuxi Li [[19](#bib.bib19)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2017 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; This paper presents a generalised overview of recent exciting &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; achievements of DRL and discus core elements and mechanisms. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; It also discusses various fields where DRL can be applied. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Luong et al. [[20](#bib.bib20)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; communications &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and networking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; This paper presents a comprehensive literature review on the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; applications of DRL in communications and networking, highlights &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; challenges, and discusses open issues and future directions. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Kiran et al. [[21](#bib.bib21)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; autonomous &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; driving &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; This paper summarises DRL algorithms and autonomous driving, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; where (D)RL methods have been employed.It also highlights key &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; challenges towards real-world deployment of autonomous cars. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Haydari et al. [[22](#bib.bib22)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; transportation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; systems &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; This paper summarises existing works in the field of transportation,
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and discusses the challenges and open questions regarding DRL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in transportation systems. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ours (2020) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✓ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ✗ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; We present a comprehensive review focused on DRL applications &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in the audio domain, highlight existing challenges that hinder the &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; progress of DRL in audio, and discuss pointers for future research.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: In audio processing, DRL has been recently used as an emerging tool to address
    various problems and challenges in automatic speech recognition (ASR), spoken
    dialogue systems (SDSs), speech emotion recognition (SER), audio enhancement,
    music generation, and audio-driven controlled robotics. In this work, we therefore
    focus on covering the advancements in audio processing by DRL. Although there
    are multiple survey articles on DRL. For instance, Arulkumaran et al. [[18](#bib.bib18)]
    presented a brief survey on DRL by covering seminal and recent developments in
    DRL—including innovative ways in which DNNs can be used to develop autonomous
    agents. Similarly, in [[19](#bib.bib19)], authors attempted to provide comprehensive
    details on DRL and cover its applications in various areas to highlight advances
    and challenges. Other relevant works include applications of DRL in communications
    and networking [[20](#bib.bib20)], human-level agents [[23](#bib.bib23)], and
    autonomous driving [[24](#bib.bib24)]. None of these articles has focused on DRL
    applications in audio processing as highlighted in Table [I](#S1.T1 "TABLE I ‣
    I Introduction ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    This paper aims to fill this gap by presenting an up-to-date literature review
    on DRL studies in the audio domain, discussing challenges that hinder the progress
    of DRL in audio, and pointing out future research areas. We hope this paper will
    help researchers and scientists interested in DRL for audio-driven applications.
  prefs: []
  type: TYPE_NORMAL
- en: This paper is organised as follows. A concise background of DL and RL is provided
    in Section [II](#S2 "II Background ‣ A Survey on Deep Reinforcement Learning for
    Audio-Based Applications"), followed by an overview of recent DRL algorithms in
    Section [III](#S3 "III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications"). With those foundations, Section [IV](#S4
    "IV Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based
    Applications") covers recent DRL works in domains such as speech, music, and environmental
    sound processing; and their challenges are discussed in Section [V](#S5 "V Challenges
    in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    Section [VI](#S6 "VI Summary and Future Pointers ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications") summaries this review and highlights the
    future pointers for audio-based DRL research and Section [VII](#S7 "VII Conclusions
    ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") concludes
    the paper.
  prefs: []
  type: TYPE_NORMAL
- en: II Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Deep Learning (DL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DNNs have been shown to produce state-of-the-art results in audio and speech
    processing due to their ability to distil compact and robust representations from
    large amounts of data. The first major milestone was significantly increasing
    the accuracy of large-scale automatic speech recognition based on the use of fully
    connected DNNs and deep autoencoders around 2010 [[7](#bib.bib7)]. It focuses
    on the use of artificial neural networks, which consists of multiple nonlinear
    modules arranged hierarchically in layers to automatically discover suitable representations
    or features from raw data for specific tasks. These non-linearities allow DNNs
    to learn complicated manifolds in speech and audio datasets. Below we discuss
    different DL architectures, which are illustrated in Figure [1](#S2.F1 "Figure
    1 ‣ II-A Deep Learning (DL) ‣ II Background ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications").
  prefs: []
  type: TYPE_NORMAL
- en: 'Convolutional neural networks (CNNs) are a kind of feedforward neural networks
    that have been specifically designed for processing data having grid-like topologies
    such as images [[25](#bib.bib25)]. Recently, CNN’s have shown state-of-the-art
    performance in various image processing tasks, including segmentation, detection,
    and classification, among others [[26](#bib.bib26)]. In contrast to DNNs, CNNs
    limit the number of parameters and memory requirements dramatically by leveraging
    on two key concepts: local receptive fields and shared weights. They often consist
    of a series of convolutional layers interleaved with pooling layers, followed
    by one or more dense layers. For sequence labelling, the dense layers can be omitted
    to obtain a fully-convolutional network (FCN). FCNs have been extended with domain
    adaptation for increased robustness [[27](#bib.bib27)]. Recently, CNN models have
    been extensively studied for a variety of audio processing tasks including music
    onset detection [[28](#bib.bib28)], speech enhancement [[29](#bib.bib29)], ASR [[30](#bib.bib30)],
    etc. However, raw audio waveform with high sample rates might have problems with
    limited receptive fields of CNNs, which can result in deteriorated performance.
    To handle this performance issue, dilated convolution layers can be used in order
    to extend the receptive field by inserting zeros between their filter coefficients [[31](#bib.bib31),
    [32](#bib.bib32)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a6af4d997dafac7484cdeff8a4ed23c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Graphical illustration of different DL architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks (RNNs) follow a different approach for modelling sequential
    data [[33](#bib.bib33)]. They introduce recurrent connections to enable parameters
    to be shared across time, which make them very powerful in learning temporal structures
    from the input sequences (e.g., audio, video). They have demonstrated their superiority
    over traditional HMM-based systems in a variety of speech and audio processing
    tasks [[34](#bib.bib34)]. Due to these abilities, RNNs, especially long-short
    term memory (LSTM) [[10](#bib.bib10)] and gated recurrent unit (GRU) [[35](#bib.bib35)]
    networks, have had an enormous impact in the speech community, and they are incorporated
    in state-of-the-art audio-based systems. Recently, these RNN models have been
    extended to include information in the frequency domain besides temporal information
    in the form of Frequency-LSTMs [[36](#bib.bib36)] and Time-Frequency LSTMs [[37](#bib.bib37)].
    In order to benefit from both neural architectures, CNNs and RNNs can be combined
    into a single network with convolutional layers followed by recurrent layers,
    often referred to as convolutional recurrent neural networks (CRNN). Related works
    combining CNNs and RNNs have been presented in ASR [[38](#bib.bib38)], SER [[39](#bib.bib39)],
    music classification [[40](#bib.bib40)], and other audio related applications [[34](#bib.bib34)].
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence (Seq2Seq) models were motivated due to problems requiring
    sequences of unknown lengths [[41](#bib.bib41)]. Although they were initially
    applied to machine translation, they can be applied to many different applications
    involving sequence modelling. In a Seq2Seq model, while one RNN reads the inputs
    in order to generate a vector representation (the encoder), another RNN inherits
    those learnt features to generate the outputs (the decoder). The neural architectures
    can be single or multilayer, unidirectional or bidirectional [[42](#bib.bib42)],
    and they can combine multiple architectures [[33](#bib.bib33), [43](#bib.bib43)]
    using end-to-end learning by optimising a joint objective instead of independent
    ones. Seq2Seq models have been gaining much popularity in the speech community
    due to their capability of transducing input to output sequences. DL frameworks
    are particularly suitable for this direct translation task due to their large
    model capacity and their capability to train in an end-to-end manner—to directly
    map the input signals to the target sequences [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46)]. Various Seq2Seq models have been explored in the speech, audio
    and language processing literature including Recurrent Neural Network Transducer
    (RNNT) [[47](#bib.bib47)], Monotonic Alignments [[48](#bib.bib48)], Listen, Attend
    and Spell (LAS) [[49](#bib.bib49)], Neural Transducer [[50](#bib.bib50)], Recurrent
    Neural Aligner (RNA) [[48](#bib.bib48)], and Transformer Networks [[51](#bib.bib51)],
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Models have been attaining much interest in the three types of generative
    models: Generative Adversarial Networks (GANs) [[52](#bib.bib52)], Variational
    Autoencoders (VAEs) [[53](#bib.bib53)], and autoregressive models [[54](#bib.bib54)].
    This type of models are powerful enough to learn the underlying distribution of
    speech datasets and have been extensively investigated in the speech and audio
    processing scientific community. Specifically, in the case of GANs and VAEs, audio
    signal is often synthesised from a low-dimensional representation, from which
    it needs to by upsampled (e.g., through linear interpolation or the nearest neighbour)
    to the high-resolution signal [[55](#bib.bib55), [56](#bib.bib56)]. Therefore,
    VAEs and GANs have been extensively explored for synthesising speech or to augment
    the training material by generating features [[57](#bib.bib57)] or speech itself.
    In the autoregressive approach, the new samples are synthesised iteratively—based
    on an infinitely long context of previous samples via RNNs (for example, using
    LSTM or GRU networks)—but at the cost of expensive computation during training [[58](#bib.bib58)].'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) is a popular paradigm of ML, which involves agents
    to learn their behaviour by trial and error [[2](#bib.bib2)]. RL agents aim to
    learn sequential decision-making by successfully interacting with the environment
    where they operate. At time $t$ (0 at the beginning of the interaction, $T$ at
    the end of an episodic interaction, or $\infty$ in the case of non-episodic tasks),
    an RL agent in state $s_{t}$ takes an action $a\in A$, transits to a new state
    $s_{t+1}$, and receives reward $r_{t+1}$ for having chosen action $a$. This process—repeated
    iteratively—is illustrated in Figure [2](#S2.F2 "Figure 2 ‣ II-B Reinforcement
    learning ‣ II Background ‣ A Survey on Deep Reinforcement Learning for Audio-Based
    Applications").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ba421221eb5185f9858834d9391bd71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Basic RL setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An RL agent aims to learn the best sequence of actions, known as policy, to
    obtain the highest overall cumulative reward in the task (or set of tasks) that
    is being trained on. While it can choose any action from a set of available actions,
    the set of actions that an agent takes from start to finish is called an episode.
    A Markov decision process (MDP) [[59](#bib.bib59)] can be used to capture the
    episodic dynamics of an RL problem. An MDP can be represented using the tuple
    ($S$, $A$, $\gamma$, $P$, $R$). The decision-maker or agent chooses an action
    $a\in A$ in state $s\in S$ at time $t$ according to its policy $\pi(a_{t}|s_{t})$—which
    determines the agent’s way of behaving. The probability of moving to the next
    state $s_{t+1}\in S$ is given by the state transition function $P(s_{t+1}|s_{t},a_{t})$.
    The environment produces a reward $R(s_{t},a_{t},s_{t+1})$ based on the action
    taken by the agent at time $t$. This process continues until the maximum time
    step or the agent reaches a terminal state. The objective is to maximise the expected
    discounted cumulative reward, which is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $E_{\pi}[R_{t}]=E_{\pi}\Big{[}\sum_{i=0}^{\infty}\gamma^{i}r_{t+i}\Big{]}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma$ $\in$ [0,1] is a discount factor used to specify that rewards
    in the distant future are less valuable than in the nearer future. While an RL
    agent may only learn its policy, it may also learn (online or offline) the transition
    and reward functions.
  prefs: []
  type: TYPE_NORMAL
- en: III Deep Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep reinforcement learning (DRL) combines conventional RL with DL to overcome
    the limitations of RL in complex environments with large state spaces or high
    computation requirements. DRL employs DNNs to estimate value, policy or model
    that are learnt through the storage of state-action pairs in conventional RL [[19](#bib.bib19)].
    Deep RL algorithms can be classified along several dimensions. For instance, on-policy
    vs off-policy, model-free vs model-based, value-based vs policy-based DRL algorithms,
    among others. The salient features of various key categories of DRL algorithms
    are presented and depicted in Figure [3](#S3.F3 "Figure 3 ‣ III Deep Reinforcement
    Learning ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    Interested readers are referred to [[19](#bib.bib19)] for more details on these
    algorithms. This section focuses on popular DRL algorithms employed in audio-based
    applications in three main categories: (i) value-based DRL, (ii) policy gradient-based
    DRL and (iii) model-based DRL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8cae1e7c500b08e3e0278c7e48339ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Classification of DRL algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Value-Based DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most famous value-based DRL algorithms is Deep Q-network (DQN), introduced
    by Mnih et al. [[12](#bib.bib12)], that learns directly from high-dimensional
    inputs. It employs convolution neural networks (CNNs) to estimate a value function
    $Q(s,a)$, which is subsequently used to define a policy. DQN enhances the stability
    of the learning process using the concept of target Q-network along with experience
    replay. The loss function computed by DQN at $i^{th}$ iteration is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{i}(\theta_{i})=\mathbb{E}_{s,a\sim p(.)}[(y_{i}-Q(s,a;\theta_{i}))^{2}],$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\text{where}\quad y_{i}=\mathbb{E}_{s\prime\sim s}[r+\gamma\underset{a\prime}{\text{max}}Q(s\prime,a\prime;\theta_{i-1}&#124;{s,a}].$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Although DQN, since inception, has rendered super-human performance in Atari
    games, it is based on a single max operator, given in (2), for selection as well
    evaluation of an action. Thus, the selection of an overestimated action may lead
    to over-optimistic action value estimates that induces an upward bias. Double
    DQN (DDQN) [[60](#bib.bib60)] eliminates this positive bias by introducing two
    decoupled estimators: one for the selection of an action, and one for the evaluation
    of an action. Schaul et al. in [[61](#bib.bib61)] show that the performance of
    DQN and DDQN is enhanced considerably if significant experience transitions are
    prioritised and replayed more frequently. Wang et al. [[62](#bib.bib62)] present
    a duelling network architecture (DNA) to estimate a value function $V(s)$ and
    associated advantage function $A(s,a)$ separately, and then combine them to get
    action-value function $Q(s,a)$. Results prove that DQN and DDQN having DNA and
    prioritised experience replay can lead to improved performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the aforementioned DQN algorithms that focus on the expected return,
    distributional DQN [[63](#bib.bib63)] aims to learn the full distribution of the
    value in order to have additional information about rewards. Despite both DQN
    and distributional DQN focusing on maximising the expected return, the latter
    comparatively results in performant learning. Will et al. [[64](#bib.bib64)] propose
    distributional DQN with quantile regression (QR-DQN) to explicitly model the distribution
    of the value function. Results prove that QR-DQN successfully bridges the gap
    between theoretic and algorithmic results. Implicit Quantile Networks (IQN) [[65](#bib.bib65)],
    an extension to QR-DQN, estimate quantile regression by learning the full quantile
    function instead of focusing on a discrete number of quantiles. IQN also provides
    flexibility regarding its training with the required number of samples per update,
    ranging from one sample to a maximum computationally allowed. IQN has shown to
    outperform QR-DQN comprehensively in the Atari domain.
  prefs: []
  type: TYPE_NORMAL
- en: The astounding success of DQN to learn rich representations is highly attributed
    to DNNs, while batch algorithms prove to have better stability and data efficiency
    (requiring less tuning of hyperparameters). Authors in [[66](#bib.bib66)] propose
    a hybrid approach named as Least Squares DQN (LS-DQN) that exploits the advantages
    of both DQN and batch algorithms. Deep Q-learning from demonstrations (DQfD) [[67](#bib.bib67)]
    leverages human demonstrations to learn at an accelerated rate from the start.
    Deep Quality-Value (DQV) [[68](#bib.bib68)] is a novel temporal-difference-based
    algorithm that trains the Value network initially, and subsequently uses it to
    train a Quality-value neural network for estimating a value function. Results
    in the Atari domain indicate that DQV outperforms DQN as well as DDQN. Authors
    in [[69](#bib.bib69)] propose RUDDER (Return Decomposition for Delayed Rewards),
    which encompasses reward redistribution and return decomposition for Markov decision
    processes (MDPs) with delayed rewards. Pohlen et al. [[70](#bib.bib70)] employ
    a transformed Bellman operator along with human demonstrations in the proposed
    algorithm Ape-X DQfD to attain human-level performance over a wide range of games.
    Results prove that the proposed algorithm achieves average-human performance in
    40 out of 42 Atari games with the same set of hyperparameters. Schulman et al.
    in [[71](#bib.bib71)] study the connection between Q-learning and policy gradient
    methods. They show that soft Q-learning (an entropy-regularised version of Q-learning)
    is equivalent to policy gradient methods and that they perform as well (if not
    better) than standard variants.
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies have also attempted to incorporate the memory element into
    DRL algorithms. For instance, the deep recurrent Q-network (DRQN) approach introduced
    by [[72](#bib.bib72)] was able to successfully integrate information through time,
    which performed well on standard Atari games. A further improvement was made by
    introducing an attention mechanism to DQN, resulting in a deep recurrent Q-network
    (DARQN) [[73](#bib.bib73)]. This allows DARQN to focus on a specific part of the
    input and achieve better performance compared to DQN and DRQN on games. Some other
    studies [[74](#bib.bib74), [75](#bib.bib75)] have also proposed methods to incorporate
    memory into DRL, but this area remains to be investigated further.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Policy Gradient-Based DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Policy gradient-based DRL algorithms aim to learn an optimal policy that maximises
    performance objectives, such as expected cumulative reward. This class of algorithms
    make use of gradient theorems to reach optimal policy parameters. Policy gradient
    typically requires the estimation of a value function based on the current policy.
    This may be accomplished using the actor-critic architecture, where the actor
    represents the policy and the critic refers to value function estimate [[76](#bib.bib76)].
    Mnih et al. [[77](#bib.bib77)] show that asynchronous execution of multiple parallel
    agents on standard CPU-based hardware leads to time-efficient and resource-efficient
    learning. The proposed asynchronous version of actor-critic, asynchronous advantage
    actor-critic (A3C) exhibit remarkable learning in both 2D and 3D games with action
    spaces in discrete as well as continuous domains. Authors in [[78](#bib.bib78)]
    propose a hybrid CPU/GPU-based A3C —named as GA3C — showing significantly higher
    speeds as compared to its CPU-based counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: 'Asynchronous actor-critic algorithms, including A3C and GA3C, may suffer from
    inconsistent and asynchronous parameter updates. A novel framework for asynchronous
    algorithms is proposed in [[79](#bib.bib79)] to leverage parallelisation while
    providing synchronous parameters updates. Authors show that the proposed parallel
    advantage actor-critic (PAAC) algorithm enables true on-policy learning in addition
    to faster convergence. Authors in [[80](#bib.bib80)] propose a hybrid policy-gradient-and-Q-learning
    (PGQL) algorithm that combines on-policy policy gradient with off-policy Q-learning.
    Results demonstrate PGQL’s superior performance on Atari games as compared to
    both A3C and Q-learning. Munos et al. [[81](#bib.bib81)] propose a novel algorithm
    by bringing together three off-policy algorithms: Instance Sampling (IS), Q($\lambda$),
    and Tree-Backup TB($\lambda$). This algorithm — called Retrace($\lambda$) — alleviates
    the weaknesses of all three algorithms (IS has low variance, Q($\lambda$) is not
    safe, and TB($\lambda$) is inefficient) and promises safety, efficiency and guaranteed
    convergence. Reactor (Retrace-Actor) [[82](#bib.bib82)] is a Retrace-based actor-critic
    agent architecture that combines time efficiency of asynchronous algorithms with
    sample efficiency of off-policy experience replay-based algorithms. Results in
    the Atari domain indicate that the proposed algorithm performs comparably with
    state-of-the-art algorithms while yielding substantial gains in terms of training
    time. The importance of weighted actor-learner architecture (IMPALA) [[83](#bib.bib83)]
    is a scalable distributed agent that is capable of handling multiple tasks with
    a single set of parameters. Results show that IMPALA outperforms A3C baselines
    in a diverse multi-task environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Schulman et al. [[84](#bib.bib84)] propose a robust and scalable trust region
    policy optimisation (TRPO) algorithm for optimising stochastic control policies.
    TRPO promises guaranteed monotonic improvement regarding the optimisation of nonlinear
    and complex policies having an inundated number of parameters. This learning algorithm
    makes use of a fixed KL divergence constraint rather than a fixed penalty coefficient,
    and outperforms a number of gradient-free and policy-gradient methods over a wide
    variety of tasks. [[85](#bib.bib85)] introduce proximal policy optimisation (PPO),
    which aims to be as reliable and stable as TRPO but relatively better in terms
    of implementation and sample complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of DRL algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRL algorithms | Approach | Details |'
  prefs: []
  type: TYPE_TB
- en: '&#124; off-policy/ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; on policy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Value-based DRL |'
  prefs: []
  type: TYPE_TB
- en: '| DQN [2] | Target Q-network, experience replay |'
  prefs: []
  type: TYPE_TB
- en: '&#124; •  Learns directly from high dimensional visual inputs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Stabilises learning process with target Q-network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Experience replay to avoid divergence in parameters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| DDQN [3] | Double Q-learning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Decoupled estimators for the selection and evaluation of an action &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prioritised DQN [4] | Prioritised experience replay |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Significant experience transitions are prioritised and replayed &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; frequently thus leading to efficient learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DNA [5] | Duelling neural network architecture |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Estimates a value function and associated advantage function and combine
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; them to get a value function with faster convergence than Q-learning
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Distributional DQN [6] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Learns distribution of cumulative returns &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; using distributional Bellman equation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Leads to performant learning than DQN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Possibility to implement risk-aware behaviour &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| QR-DQN [7] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Distributional DQN with quantile regression &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Bridges gap between theoretical and algorithmic results |'
  prefs: []
  type: TYPE_TB
- en: '| IQN [8] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Extends QR-DQN with a full quantile function &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Provides flexibility regarding number of samples required for training
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| LS-DQN [9] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A hybrid approach combining DQN with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; least-squares method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Exploits advantages of both DQN, ability to learn rich representations,
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and batch algorithms, stability and data efficiency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DQfD [10] | Learns from demonstrations | Learns at an accelerated rate from
    the start. |'
  prefs: []
  type: TYPE_TB
- en: '| DQV [11] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Uses temporal difference to train a Value network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and subsequently uses it for training a Quality-Value &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; network that estimates state-action values &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learns significantly better and faster than DQN and DDQN |'
  prefs: []
  type: TYPE_TB
- en: '| RUDDER [12] | Reward redistribution and return decomposition |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Provides prominent improvement on games having long &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; delayed rewards &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ape-X DQfD [13] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Employs transformed Bellman operator &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; together with temporal consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Surpasses average human performance on 40 out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of 42 Atari 2600 games &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Soft DQN [14] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Incorporation of soft KL penalty and entropy bonus &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Establishes equivalence between Soft DQN and policy gradient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| DRQN, DARQN [14-15] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Memory, attention &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DQN policies modelled by attention-based recurrent networks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '| Policy Gradient-based DRL |'
  prefs: []
  type: TYPE_TB
- en: '| A3C [16] | Asynchronous gradient descent | Consumes less resources; able
    to run on a standard multi-core CPU | on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| GA3C [17] | Hybrid CPU/GPU-based A3C | Achieves speed significantly higher
    than its CPU-based counterpart |'
  prefs: []
  type: TYPE_TB
- en: '| PAAC [18] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Novel framework for asynchronous algorithms &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Computationally efficient & enables faster convergence to optimal policies
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| PGQL [19] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combines on-policy policy gradient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; with off-policy Q-learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Enhanced stability and data efficiency | off-policy |'
  prefs: []
  type: TYPE_TB
- en: '| Retrace($\lambda$) [20] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Expresses three off-policy algorithms—IS, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Q($\lambda$) and TB($\lambda$)— in a common form &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Safe, sample efficient and has low variance |'
  prefs: []
  type: TYPE_TB
- en: '| Reactor [21] | Retrace-based actor-critic agent architecture | Yields substantial
    gains in terms of training time. |'
  prefs: []
  type: TYPE_TB
- en: '| IMPALA [22] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Scalable distributed agent capable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; of handling multiple tasks with &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; a single set of parameters &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; outperforms state-of-the-art agents in a &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; diverse multi-task environment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| TRPO [23] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Employs fixed KL divergence constraint &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; for optimising stochastic control policies &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Performs well over a wide variety of large-scale tasks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| PPO [24] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Makes use of adaptive KL penalty coefficient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; As reliable and stable as TRPO but relatively better &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in terms of implementation and sample complexity &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model-based DRL |'
  prefs: []
  type: TYPE_TB
- en: '| SimPLe [26] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Video prediction-based model-based algorithm that &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; requires much fewer agent-environment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; interactions than model-free algorithms &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outperforms state-of-the-art model-free algorithms in Atari games &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| on-policy |'
  prefs: []
  type: TYPE_TB
- en: '| TreeQN [27] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Estimates Q-values based on a dynamic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tree constructed recursively through &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; an implicit transition model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Outperforms n-step DQN and value prediction networks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in multiple Atari games &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| STRAW [29] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Capable of natural decision making &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; by learning macro actions &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Improves performance significantly in Atari games |'
  prefs: []
  type: TYPE_TB
- en: '| VProp [30] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A set of Value Iteration-based planning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; modules that is trained using RL &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Able to solve an unseen task and navigate in complex environments
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Able to generalise in dynamic and noisy environment &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - |'
  prefs: []
  type: TYPE_TB
- en: '| MuZero [31] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Combines tree-based search with learned &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; model to render superhuman performance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; in challenging environments &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Delivers state-of-the-art performance on 57 diverse Atari games &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| off-policy |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/85cebe8dc02a10d4f8adcf57a15ff2e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Schematic diagram of DRL agents for audio-based applications, where
    the DL model (via DNNs, CNN, RNNs, etc.) generates audio features from raw waveforms
    or other audio representations for taking actions that change the environment
    from state $s_{t}$ to a next state $s_{t+1}$.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Model-Based DRL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model-based DRL algorithms rely on models of the environment (i.e. underlying
    dynamics and reward functions) in conjunction with a planning algorithm. Unlike
    model-free DRL methods that typically entail a large number of samples to render
    adequate performance, model-based algorithms generally lead to improved sample
    and time efficiency [[86](#bib.bib86)].
  prefs: []
  type: TYPE_NORMAL
- en: Kaiser et al. [[87](#bib.bib87)] propose simulated policy learning (SimPLe),
    a video prediction-based model-based DRL algorithm that requires much fewer agent-environment
    interactions than model-free algorithms. Experimental results indicate that SimPLe
    outperforms state-of-the-art model-free algorithms in Atari games. Farquhar et
    al. [[88](#bib.bib88)] propose TreeQN for complex environments, where the transition
    model is not explicitly given. The proposed algorithm combines model-free and
    model-based approaches in order to estimate Q-values based on a dynamic tree constructed
    recursively through an implicit transition model. Authors of [[88](#bib.bib88)]
    also propose an actor-critic variant named ATreeC that augments TreeQN with a
    softmax layer to form a stochastic policy network. They show that both algorithms
    yield superior performance than n-step DQN and value prediction networks [[89](#bib.bib89)]
    on multiple Atari games. Authors in [[90](#bib.bib90)] introduce a Strategic Attentive
    Writer (STRAW), which is capable of making natural decisions by learning macro-actions.
    Unlike state-of-the-art DRL algorithms that yield only one action after every
    observation, STRAW generates a sequence of actions, thus leading to structured
    exploration. Experimental results indicate a significant improvement in Atari
    games with STRAW. Value Propagation (VProp) [[91](#bib.bib91)] is a set of Value
    Iteration-based planning modules trained using RL and capable of solving unseen
    tasks and navigating in complex environments. It is also demonstrated that VProp
    is able to generalise in a dynamic and noisy environment. Authors in [[92](#bib.bib92)]
    present a model-based algorithm named MuZero that combines tree-based search with
    a learned model to render superhuman performance in challenging environments.
    Experimental results demonstrate that MuZero delivers state-of-the-art performance
    on 57 diverse Atari games. Table  [II](#S3.T2 "TABLE II ‣ III-B Policy Gradient-Based
    DRL ‣ III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications") presents an overview of DRL algorithms for a reader’s
    glance.
  prefs: []
  type: TYPE_NORMAL
- en: IV Audio-Based DRL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section surveys related works where audio is a key element in the learning
    environments of DRL agents. An example scenario is a human speaking to a machine
    trained via DRL as in Figure [4](#S3.F4 "Figure 4 ‣ III-B Policy Gradient-Based
    DRL ‣ III Deep Reinforcement Learning ‣ A Survey on Deep Reinforcement Learning
    for Audio-Based Applications"), where the machine has to act based on features
    derived from audio signals. Table [III](#S4.T3 "TABLE III ‣ IV Audio-Based DRL
    ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") summarises
    the characterisation of DRL agents for six audio-related areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: automatic speech recognition;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: spoken dialogue systems;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: emotions modelling;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: audio enhancement;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: music listening and generation; and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: robotics, control and interaction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'TABLE III: Summary of audio related fields, characterisation of DRL agents,
    and related datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Appl. Area | State Representations $\mathcal{S}$, Actions $\mathcal{A}$,
    and Reward Functions $\mathcal{R}$ | Popular Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Automatic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Speech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{S}$: States are learnt representations from input speech features
    (e.g. fMLLR or MFCC vectors [[93](#bib.bib93)]). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{A}$: Actions include phonemes, graphemes, commands, or candidates
    from the ASR N-best list. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{R}$: They have included binary rewards (positive for selecting
    the correct choice, 0 otherwise), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and non-binary sentence/token rewards based on the Levenshtein distance
    algorithm. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  LibriSpeech [[94](#bib.bib94)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  TED-LIUM [[95](#bib.bib95)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Wall Street Journal [[96](#bib.bib96)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  SWITCHBOARD [[97](#bib.bib97)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  TIMIT [[98](#bib.bib98)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Spoken &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dialogue &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Systems &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{S}$: They encode the uttered words by the system and recognised
    user words into a dialogue history &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and some additional information from classifiers such as user goals,
    user intents, speech recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; confidence scores, and visual information (in the case of multimodal
    systems), among others. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{A}$: While actions in task-oriented systems include slot requests/confirmations/apologies,
    slot-value &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; selection, ask question, data retrieval, information presentation, among
    others; actions in open-ended &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; systems include either all possible sentences (infinite) or clusters
    of sentences (finite). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{R}$: They vary depending on the company/project requirements
    and tend to include sparse and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; non-sparse numerical rewards such as dialogue length, task success,
    dialogue similarity, dialogue &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; coherence, dialogue repetitiveness, game scores (in the case of game-based
    systems), among others. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  SGD [[99](#bib.bib99)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  DSTC [[100](#bib.bib100)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Frames [[101](#bib.bib101)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  MultiWOZ [[102](#bib.bib102)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  SubTle Corpus [[103](#bib.bib103)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Simulations [[104](#bib.bib104)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Other datasets [[105](#bib.bib105)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Speech &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Emotion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{S}$: Speech features (e.g., MFCC) are considered as input
    features. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{A}$: Actions include speech emotion labels (e.g. unhappy,
    neutral, happy), sentiment detection &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (e.g. negative, neutral, positive), and termination from utterance listening.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{R}$: Binary reward functions have been used (positive for
    choosing the correct choice, 0 otherwise). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  EMODB [[106](#bib.bib106)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  IEMOCAP [[107](#bib.bib107)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  MSP-IMPROV [[108](#bib.bib108)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  SEMAINE [[109](#bib.bib109)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  MELD [[110](#bib.bib110)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Audio &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Enhancement &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{S}$: States are learnt from clean and noisy acoustic features.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{A}$: Finding closest cluster and its index, time-frequency
    mask estimation, and increasing or decreasing &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the parameter values of the speech-enhancement algorithm. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{R}$: Positive rewards for correct choice, negative otherwise.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  DEMAND [[111](#bib.bib111)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  CHiME-3 [[112](#bib.bib112)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  WHAMR [[113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Music &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{S}$: State representations are learned from Musical notes.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{A}$: Musical generation and next note selection are considered
    as actions. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{R}$: Binary reward functions based on hard-coded musical theory
    rules, including the likelihood of actions. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Classical piano MIDI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; database [[114](#bib.bib114)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  MusicNet dataset [[115](#bib.bib115)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  JSB Chorales &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dataset [[116](#bib.bib116)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Robotics, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Control and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Interaction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{S}$: They encode visual and verbal representations derived
    from image embeddings, speech features, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; and word or sentence embeddings. Additional information include user
    intents, speech recognition &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; scores, human activities, postures, emotions, and body joint angles,
    among others. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{A}$: They include motor commands (e.g. gestures, locomotion,
    navigation, manipulation, gaze) and &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; verbalizations such as dialogue acts and backchannels (e.g. laughs,
    smiles, noddings, head-shakes). &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\mathcal{R}$: They are based on task success (positive rewards for
    achieving the goal, negative rewards for &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; failing the task, and zero/shaped rewards otherwise) and user engagement.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  AVDIAR [[117](#bib.bib117)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  NLI Corpus [[118](#bib.bib118)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  VEIL dataset [[119](#bib.bib119)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Simulations [[120](#bib.bib120)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; •  Real-world &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; interactions [[121](#bib.bib121)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Automatic Speech Recognition (ASR)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatic speech recognition (ASR) is the process of converting a speech signal
    into its corresponding text by algorithms. Contemporary ASR technology has reached
    great levels of performance due to advancements in DL models. The performance
    of ASR systems, however, relies heavily on supervised training of deep models
    with large amounts of transcribed data. Even for resource-rich languages, additional
    transcription costs required for new tasks hinders the applications of ASR. To
    broaden the scope of ASR, different studies have attempted RL based models with
    the ability to learn from feedback. This form of learning aims to reduce transcription
    costs and time by humans providing positive or negative rewards instead of detailed
    transcriptions. For instance, Kala et al. [[122](#bib.bib122)] proposed an RL
    framework for ASR based on the policy gradient method that provides a new view
    of existing training and adaptation methods. They achieved improved recognition
    performance and reduced Word Error Rate (WER) compared to unsupervised adaptation.
    In ASR, sequence-to-sequence models have shown great success; however, these models
    fail to approximate real-world speech during inference. Tjandra et al. [[123](#bib.bib123)]
    solved this issue by training a sequence-to-sequence model with a policy gradient
    algorithm. Their results showed a significant improvement using an RL-based objective
    and a maximum likelihood estimation (MLE) objective compared to the model trained
    with only the MLE objective. In another study, [[124](#bib.bib124)] extended their
    own work by providing more details on their model and experimentation. They found
    that using token-level rewards (intermediate rewards are given after each time
    step) provide improved performance compared to sentence-level rewards and baseline
    systems. In order solve the issues of semi-supervised training of sequence-to-sequence
    ASR models, Chung et al. [[125](#bib.bib125)] investigated the REINFORCE algorithm
    by rewarding the ASR to output more correct sentences for both unpaired and paired
    speech input data. Experimental evaluations showed that the DRL-based method was
    able to effectively reduce character error rates from 10.4% to 8.7%.
  prefs: []
  type: TYPE_NORMAL
- en: Karita et al. [[126](#bib.bib126)] propose to train an encoder-decoder ASR system
    using a sequence-level evaluation metric based on the policy gradient objective
    function. This enables the minimisation of the expected WER of the model predictions.
    In this way, the authors found that the proposed method improves recognition performance.
    The ASR system of [[127](#bib.bib127)] was jointly trained with maximum likelihood
    and policy gradient to improve via end-to-end learning. They were able to optimise
    the performance metric directly and achieve 4% to 13% relative performance improvement.
    In [[128](#bib.bib128)], the authors attempted to solve sequence-to-sequence problems
    by proposing a model based on supervised backpropagation and a policy gradient
    method, which can directly maximise the log probability of the correct answer.
    They achieved very encouraging results on a small scale and a medium scale ASR.
    Radzikowski et al. [[129](#bib.bib129)] proposed a dual supervised model based
    on a policy gradient methodology for non-native speech recognition. They were
    able to achieve promising results for the English language pronounced by Japanese
    and Polish speakers.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve the best possible accuracy, end-to-end ASR systems are becoming increasingly
    large and complex. DRL methods can also be leveraged to provide model compression [[130](#bib.bib130)].
    In [[131](#bib.bib131)], RL-based ShrinkML is proposed to optimise the per-layer
    compression ratios in a state-of-the-art LSTM-based ASR model with attention.
    For time-efficient ASR, [[132](#bib.bib132)] evaluated the pre-training of an
    RL-based policy gradient network. They found that pre-training in DRL offers faster
    convergence compared to non-pre-trained networks, and also achieve improved recognition
    in lesser time. To tackle the slow convergence time of the REINFORCE algorithm [[133](#bib.bib133)],
    Lawson et al. [[134](#bib.bib134)], evaluated Variational Inference for Monte
    Carlo Objectives (VIMCO) and Neural Variational Inference (NVIL) for phoneme recognition
    tasks in clean and noisy environments. The authors found that the proposed method
    (using VIMCO and NVIL) outperforms REINFORCE and other methods at training online
    sequence-to-sequence models.
  prefs: []
  type: TYPE_NORMAL
- en: All of the above-mentioned studies highlight several benefits of using DRL for
    ASR. Despite these promising results, further research is required on DRL algorithms
    towards building autonomous ASR systems that can work in complex real-life settings.
    REINFORCE algorithm is very popular in ASR, therefore, research is also required
    to explore other DRL algorithms to highlights suitability for ASR.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Spoken Dialogue Systems (SDSs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spoken dialogue systems are gaining interest due to many applications in customer
    services and goal-oriented human-computer-interaction. Typical SDSs integrate
    several key components including speech recogniser, intent recogniser, knowledge
    base and/or database backend, dialogue manager, language generator, and speech
    synthesis, among others [[135](#bib.bib135)]. The task of a dialogue manager in
    SDSs is to select actions based on observed events [[136](#bib.bib136), [137](#bib.bib137)].
    Researchers have shown that the action selection process can be effectively optimised
    using RL to model the dynamics of spoken dialogue as a fully or partially observable
    Markov Decision Process [[138](#bib.bib138)]. Numerous studies have utilised RL-based
    algorithms in spoken dialogue systems. In contrast to text-based dialogue systems
    that can be trained directly using large amounts of text data [[139](#bib.bib139)],
    most SDSs have been trained using user simulations [[104](#bib.bib104)]. The justification
    for that is mainly due to insufficient amounts of training dialogues to train
    or test from real data [[105](#bib.bib105)].
  prefs: []
  type: TYPE_NORMAL
- en: SDSs involve policy optimisation to respond to humans by taking the current
    state of the dialogue, selecting an action, and returning the verbal response
    of the system. For instance, Chen et al. [[140](#bib.bib140)] presented an online
    DRL-based dialogue state tracking framework in order to improve the performance
    of a dialogue manager. They achieved promising results for online dialogue state
    tracking in the second and third dialog state tracking challenges ( [[141](#bib.bib141),
    [142](#bib.bib142)]). Weisz et al. [[143](#bib.bib143)] utilised DRL approaches,
    including actor-critic methods and off-policy RL. They also evaluated actor-critic
    with experience replay (ACER) [[144](#bib.bib144), [81](#bib.bib81)], which has
    shown promising results on simple gaming tasks. They showed that the proposed
    method is sample efficient and that performed better than some state-of-the-art
    DL approaches for spoken dialogue. A task-oriented end-to-end DRL-based dialogue
    system is proposed in [[145](#bib.bib145)]. They showed that DRL-based optimisation
    produced significant improvement in task success rate and also caused a reduction
    in dialogue length compared to supervised training. Zhao et al. [[146](#bib.bib146)]
    utilised deep recurrent Q-networks (DRQN) for dialogue state tracking and management.
    Experimental results showed that the proposed model can exploit the strengths
    of DRL and supervised learning to achieve faster learning speed and better results
    than the modular-based baseline system. To present baseline results, a benchmark
    study [[147](#bib.bib147)] is performed using DRL algorithms including DQN, A2C
    and natural actor-critic [[148](#bib.bib148)] and their performance is compared
    against GP-SARSA [[149](#bib.bib149)]. Based on experimental results on the PyDial
    toolkit [[150](#bib.bib150)], the authors conclude that substantial improvements
    are still needed for DRL methods to match the performance of carefully designed
    handcrafted policies. In addition to SDSs optimised via flat DRL, hierarchical
    RL/DRL methods have been proposed for policy learning using dialogue states with
    different levels of abstraction and dialogue actions at different levels of granularity
    (via primitive and composite actions) [[151](#bib.bib151), [152](#bib.bib152),
    [153](#bib.bib153), [154](#bib.bib154), [155](#bib.bib155), [156](#bib.bib156)].
    The benefits of this form of learning include faster training and policy reuse.
    A deep Q-network based multi-domain dialogue system is proposed in [[157](#bib.bib157)].
    They train the proposed SDS using a network of DQN agents, which is similar to
    hierarchical DRL but with more flexibility for transitioning across dialogues
    domains. Another work related to faster training is proposed by [[158](#bib.bib158)],
    where the behaviour of RL agents is guided by expert demonstrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimisation of dialogue policies requires a reward function that unfortunately
    is not easy to specify. This often requires annotated data for training a reward
    predictor instead of a hand-crafted one. In real-world applications, such annotations
    are either scarce or not available. Therefore, some researchers have turner their
    attention to methods for online active reward learning. In [[159](#bib.bib159)],
    the authors presented an online learning framework for a spoken dialogue system.
    They jointly trained the dialogue policy alongside the reward model via active
    learning. Based on the results, the authors showed that the proposed framework
    can significantly reduce data annotation costs and can also mitigate noisy user
    feedback in dialogue policy learning. Su et al. [[148](#bib.bib148)] introduced
    two approaches: trust region actor-critic with experience replay (TRACER) and
    episodic natural actor-critic with experience replay (eNACER) for dialogue policy
    optimisation. From these two algorithms, they achieved the best performance using
    TRACER.'
  prefs: []
  type: TYPE_NORMAL
- en: In [[160](#bib.bib160)], the authors propose to learn a domain-independent reward
    function based on user satisfaction for dialogue policy learning. The authors
    showed that the proposed framework yields good performance for both task success
    rate and user satisfaction. Researchers have also used DRL to learn dialogue policies
    in noisy environments, and some have shown that their proposed models can generate
    dialogues indistinguishable from human ones [[161](#bib.bib161)]. Carrara et al. [[162](#bib.bib162)]
    propose online learning and transfer for user adaptation in RL-based dialogue
    systems. Experiments were carried out on a negotiation dialogue task, which showed
    significant improvements over baselines. In another study [[163](#bib.bib163)],
    authors proposed $\epsilon$-safe, a Q-learning algorithm, for safe transfer learning
    for dialogue applications. A DRL-based chatbot called MILABOT was designed in [[164](#bib.bib164)],
    which can converse with humans on popular topics through both speech and text—performing
    significantly better than many competing systems. The text-based chatbot in [[165](#bib.bib165)]
    used an ensemble of DRL agents, and showed that training multiple dialogue agents
    performs better than a single agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Summary of research papers on dialogue systems trained with DRL algorithms
    (?=information not available)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Refe- | Application | DRL | User Si- | Transfer | Training | Human | Reward
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| rence | Domain(s) | Algorithm | mulations | Learning | (Test) Data | Evaluation
    | Function |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[166](#bib.bib166)] | Games: | KG-DQN | No | Yes | 40 (10) games | No |
    +1 for getting closer to the finish, -1 for |'
  prefs: []
  type: TYPE_TB
- en: '| Slice of Life, Horror | extending the minimum steps, 0 otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| [[167](#bib.bib167)] | Restaurants, laptops | FDQN, | Yes | No | 4K ($0.5$K)
    dialogues | No | +20 if successful dialogue or 0 otherwise, |'
  prefs: []
  type: TYPE_TB
- en: '| GP-Sarsa | minus dialogue length |'
  prefs: []
  type: TYPE_TB
- en: '| [[168](#bib.bib168)] | Restaurants | MADQN | Yes | Yes | 15K ($?$) dialogues
    | No | +1 if successful dialogue, |'
  prefs: []
  type: TYPE_TB
- en: '| -0.05 at each dialogue turn |'
  prefs: []
  type: TYPE_TB
- en: '| [[120](#bib.bib120)] | Chitchat | Ensemble | No | No | $\leq$64K (1k) dialogues
    | Yes | +1 for a human-like response, |'
  prefs: []
  type: TYPE_TB
- en: '| DQN | -1 for a randomly chosen response |'
  prefs: []
  type: TYPE_TB
- en: '| [[165](#bib.bib165)] | Robot playing | Competitive | Yes | No | 20K (3K)
    games | Yes | +5 for a game win, |'
  prefs: []
  type: TYPE_TB
- en: '| noughts & crosses | DQN | +1 for a draw, -5 for a loss |'
  prefs: []
  type: TYPE_TB
- en: '| [[169](#bib.bib169)] | Restaurants, hotels | NDQN | Yes | No | 8.7K (1K)
    dialogues | No | $Pr$(TaskSuccess) plus $Pr$(Data-Like) |'
  prefs: []
  type: TYPE_TB
- en: '| minus number of turns $\times-0.1$ |'
  prefs: []
  type: TYPE_TB
- en: '| [[170](#bib.bib170)] | Visual | Reinforce | No | No | 68K (9.5K) images |
    Yes | Euclidean distances between predicted and |'
  prefs: []
  type: TYPE_TB
- en: '| Question-Answering | target descriptions of the last 2 time steps |'
  prefs: []
  type: TYPE_TB
- en: '| [[171](#bib.bib171)] | Restaurants | DA2C | Yes | No | 15K (0.5K) dialogues
    | No | +1 if successful dialogue, -0.03 at each turn, |'
  prefs: []
  type: TYPE_TB
- en: '| -1 if unsuccessful dialogue or hangup |'
  prefs: []
  type: TYPE_TB
- en: '| [[172](#bib.bib172)] | Movie chat | Dueling | Yes | No | 150K ($?$) sentences
    | No | +10 for correct recognition, -12 for incorrect |'
  prefs: []
  type: TYPE_TB
- en: '| DDQN | recognition, smaller rewards for confirm/elicit |'
  prefs: []
  type: TYPE_TB
- en: '| [[158](#bib.bib158)] | MultiWoz | NDfQ | Yes | No | 11.4K (1K) dialogues
    | No | +100 for successfully completing the task, |'
  prefs: []
  type: TYPE_TB
- en: '| -1 at each turn |'
  prefs: []
  type: TYPE_TB
- en: '| [[173](#bib.bib173)] | Chitchat | DBCQ | No | No | 14.2$\times$2K ($?$) sentences
    | Yes | Weighted scores combining sentiment, asking, |'
  prefs: []
  type: TYPE_TB
- en: '|  | laughter, long dialogues, & sentence similarity |'
  prefs: []
  type: TYPE_TB
- en: '| [[174](#bib.bib174)] | OpenSubtitles | Reinforce | No | No | 1̃0M (1K) sentences
    | Yes | Weighted scores combining ease of answering, |'
  prefs: []
  type: TYPE_TB
- en: '| information flow, and semantic coherence |'
  prefs: []
  type: TYPE_TB
- en: '| [[175](#bib.bib175)] | OpenSubtitles | Adversarial | No | No | $?$ ($?$)
    dialogues | Yes | Learnt rewards (binary classifier determining |'
  prefs: []
  type: TYPE_TB
- en: '| Reinforce | a machine- or human-generated dialogue) |'
  prefs: []
  type: TYPE_TB
- en: '| [[176](#bib.bib176)] | Movie booking | BBQN | Yes | No | 20K (10K) dialogues
    | Yes | +40 if successful dialogue, -1 at each turn, |'
  prefs: []
  type: TYPE_TB
- en: '| -10 for a failed dialogue |'
  prefs: []
  type: TYPE_TB
- en: '| [[177](#bib.bib177)] | Freeway, Bomberman, | Text-DQN, | No | Yes | 10M-15M
    (50K) steps | No | Learnt rewards (CNN network trained from |'
  prefs: []
  type: TYPE_TB
- en: '| Bourderchase, F&E | Text-VI | crowdsourced text descriptions of gameplays)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[155](#bib.bib155)] | Flights and hotels | Hierarchical | Yes | No | 20K
    (2K) dialogues | Yes | +120 if successful dialogue, -1 at each turn, |'
  prefs: []
  type: TYPE_TB
- en: '| DQN | -60 for a failed dialogue |'
  prefs: []
  type: TYPE_TB
- en: '| [[178](#bib.bib178)] | Movie-ticket booking | Adversarial | Yes | No | 100K
    (5K) dialogues | No | Learnt rewards (MLP network comparing |'
  prefs: []
  type: TYPE_TB
- en: '| A2C | state-action pairs with human dialogues) |'
  prefs: []
  type: TYPE_TB
- en: '| [[179](#bib.bib179)] | Chitchat | Hierarchical | No | No | 109K (10K) dialogues
    | Yes | Predefined scores combining question, |'
  prefs: []
  type: TYPE_TB
- en: '| Reinforce | repetition, semantic similarity, and toxicity |'
  prefs: []
  type: TYPE_TB
- en: '| [[180](#bib.bib180)] | Chitchat | Reinforce | No | No | $\sim$2M ($?$) dialogues
    | Yes | Positive reward from ease of answering - |'
  prefs: []
  type: TYPE_TB
- en: '| negative reward for manual dull utterances |'
  prefs: []
  type: TYPE_TB
- en: '| [[181](#bib.bib181)] | Chitchat | Reinforce | No | No | $\sim$5K (0.1) dialogues
    | Yes | Learnt rewards (linear regressor predicting |'
  prefs: []
  type: TYPE_TB
- en: '| user scores at the end of the dialogue) |'
  prefs: []
  type: TYPE_TB
- en: '| [[182](#bib.bib182)] | Restaurants | TRACER, | Yes | No | $\leq$3.5K (0.6K)
    dialogues | No | +20 if successful dialogue (0 otherwise) |'
  prefs: []
  type: TYPE_TB
- en: '| eNACER | minus 0.05 $\times$ number of dialogue turns |'
  prefs: []
  type: TYPE_TB
- en: '| [[183](#bib.bib183)] | Buses, restaurants, | GP-Sarsa | Yes | No | 1K (0.1K)
    dialogues | No | Learnt rewards (Support Vector Machine |'
  prefs: []
  type: TYPE_TB
- en: '| hotels, laptops | predicting user dialogue ratings) |'
  prefs: []
  type: TYPE_TB
- en: '| [[184](#bib.bib184)] | Medical diagnosis | KR-DQN | Yes | No | 423 (104)
    dialogues | Yes | +44 for successful diagnoses, -22 for failed |'
  prefs: []
  type: TYPE_TB
- en: '| (4 diseases) | diagnoses, -1 for failing symptom requests |'
  prefs: []
  type: TYPE_TB
- en: '| [[185](#bib.bib185)] | Restaurants | ACER | Yes | No | 4K (4K) dialogues
    | Yes | +20 for a successful dialogue minus number |'
  prefs: []
  type: TYPE_TB
- en: '| of turns in the dialogue |'
  prefs: []
  type: TYPE_TB
- en: '| [[186](#bib.bib186)] | Dialling domain | Reinforce | Yes | No | 5K (0.5K)
    dialogues | No | +1 for successfully completing the dialogue, |'
  prefs: []
  type: TYPE_TB
- en: '| 0 otherwise |'
  prefs: []
  type: TYPE_TB
- en: '| [[187](#bib.bib187)] | 20-question game | DRQN | Yes | No | 120K (5K) sentences
    | No | +30 for a game win, -30 for a lost game, |'
  prefs: []
  type: TYPE_TB
- en: '| -5 for a wrong guess |'
  prefs: []
  type: TYPE_TB
- en: '| [[188](#bib.bib188)] | DealOrNotDeal, | Reinforce | Yes;No | No | $\leq$8.4K
    ($\leq$1K) dialogues | No | +$\leq$10 for a negotiation, 0 for no agreement; |'
  prefs: []
  type: TYPE_TB
- en: '| MultiWoz | language constrained reward curve |'
  prefs: []
  type: TYPE_TB
- en: '| [[156](#bib.bib156)] | 20 images | DRRN+ | Yes | No | 20K (1K) games | No
    | +10 for a game win, -10 for a lost game, |'
  prefs: []
  type: TYPE_TB
- en: '| guessing game | DQN | a pseudo reward for question selection |'
  prefs: []
  type: TYPE_TB
- en: '| [[189](#bib.bib189)] | MultiWoz | GP${}_{\mbox{mbcm}}$, PPO | Yes | No |
    10.5K (1K) dialogues | Yes | Learnt rewards (MLP network comparing |'
  prefs: []
  type: TYPE_TB
- en: '| ACER, ALDM | state-action pairs with human dialogues) |'
  prefs: []
  type: TYPE_TB
- en: Table [IV](#S4.T4 "TABLE IV ‣ IV-B Spoken Dialogue Systems (SDSs) ‣ IV Audio-Based
    DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications") shows
    a summary of DRL-based dialogue systems. While not all involve spoken interactions,
    they can be applied to speech-based systems by for example using the outputs from
    a speech recogniser instead of typed interactions. In terms of application, we
    can observe that most systems focus on one or a few domains—systems trained with
    a large amount of domains is usually not attempted, presumably due to the high
    requirements of data and compute involved. Regarding algorithms, the most popular
    are DQN-based or REINFORCE among other more recent algorithms—when to use one
    over another algorithm still needs to be understood better. We can also observe
    that user simulations are mostly used for training task-oriented dialogue systems,
    while real data is the preferred choice for open-ended dialogue systems. We can
    note that while transfer learning is an important component in a trained SDS,
    it is not common-place yet. Given that learning from scratch every time a system
    is trained is neither scalable nor practical, it looks like transfer learning
    will naturally be adopted more and more in the future as more domains are taken
    into account. In terms of datasets, most of them they are still in the small size.
    It is rare to see SDSs trained with millions of training dialogues or sentences.
    As datasets grow, the need for more efficient training methods will take more
    relevance in future systems. Regarding human evaluations, we can observe that
    about half of research works involve human evaluations. While human evaluations
    may not always be required to answer a research question, they certainly should
    be used whenever learnt conversational skills are being assessed or judged. We
    can also note that there is no standard for specifying reward functions due to
    the wide variety of functions used in previous works—almost every paper uses a
    different reward function. Even when some works use learnt reward functions (e.g.
    based on adversarial learning), they focus on learning to discriminate between
    machine-generated and human generated dialogues without taking other dimensions
    into account such as task success or additional penalties. Although there is advancement
    in the specification of reward functions by learning them instead of hand-crafting
    them, this area requires better understanding for optimising different types of
    dialogues including information-seeking, chitchat, game-based, negotiation-based,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Emotions Modelling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Emotions are essential in vocal human communication, and they have recently
    received growing interest by the research community [[190](#bib.bib190), [191](#bib.bib191),
    [39](#bib.bib39)]. Arguably, human-robot interaction can be significantly enhanced
    if dialogue agents can perceive the emotional state of a user and its dynamics [[192](#bib.bib192),
    [193](#bib.bib193)]. This line of research is categorised into two areas: emotion
    recognition in conversations [[194](#bib.bib194)], and affective dialogue generation [[195](#bib.bib195),
    [196](#bib.bib196)]. Speech emotion recognition (SER) can be used as a reward
    for RL based dialogue systems [[197](#bib.bib197)]. This would allow the system
    to adjust the behaviour based on the emotional states of the dialogue partner.
    Lack of labelled emotional corpora and low accuracy in SER are two major challenges
    in the field. To achieve the best possible accuracy, various DL-based methods
    have been applied to SER, however, performance improvement is still needed for
    real-time deployments. DRL offers different advantages to SER, as highlighted
    in different studies. In order to improve audio-visual SER performance, Ouyang
    et al. [[198](#bib.bib198)] presented a model-based RL framework that utilised
    feedback of testing results as rewards from environment to update the fusion weights.
    They evaluated the proposed model on the Multimodal Emotion Recognition Challenge
    (MEC 2017) dataset and achieved top 2 at the MEC 2017 Audio-Visual Challenge.
    To minimise the latency in SER, Lakomin et al. [[199](#bib.bib199)] proposed EmoRL
    for predicting the emotional state of a speaker as soon as it gains enough confidence
    while listening. In this way, EmoRL was able to achieve lower latency and minimise
    the need for audio segmentation required in DL-based approaches for SER. In [[200](#bib.bib200)],
    authors used RL with an adaptive fractional deep Belief network (AFDBN) for SER
    to enhance human-computer interaction. They showed that the combination of RL
    with AFDBN is efficient in terms of processing time and SER performance. Another
    study [[201](#bib.bib201)] utilised an LSTM-based gated multimodal embedding with
    temporal attention for sentiment analysis. They exploited the policy gradient
    method REINFORCE to balance exploration and optimisation by random sampling. They
    empirically show that the proposed model was able to deal with various challenges
    of understanding communication dynamics.'
  prefs: []
  type: TYPE_NORMAL
- en: DRL is less popular in SER compared ASR and SDSs. The above mentioned studies
    attempted to helps solving different SER challenges using DRL, however, there
    is still a need for developing adaptive SER agents that can perform SER in cross-lingual
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Audio Enhancement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The performance of audio-based intelligent systems is critically vulnerable
    to noisy conditions and degrades according to the noise levels in the environment [[202](#bib.bib202)].
    Several approaches have been proposed [[203](#bib.bib203)] to address problems
    caused by environmental noise. One popular approach is audio enhancement, which
    aims to generate an enhanced audio signal from its noisy or corrupted version [[204](#bib.bib204)].
    DL-based speech enhancement has attained increased attention due to its superior
    performance compared to traditional methods [[205](#bib.bib205), [206](#bib.bib206)].
  prefs: []
  type: TYPE_NORMAL
- en: In DL-based systems, the audio enhancement module is generally optimised separately
    from the main task such as minimisation of WER. Besides the speech enhancement
    module, there are different other units in speech-based systems which increase
    their complexity and make them non-differentiable. In such situations, DRL can
    achieve complex goals in an iterative manner, which makes it suitable for such
    applications. Such DRL-based approaches have been proposed in [[207](#bib.bib207)]
    to optimise the speech enhancement module based on the speech recognition results.
    Experimental results have shown that DRL-based methods can effectively improve
    the system’s performance by 12.4% and 19.2% error rate reductions for the signal
    to noise ratio at 0 dB and 5 dB, respectively. In [[208](#bib.bib208)], authors
    attempted to optimise DNN-based source enhancement using RL with numerical rewards
    calculated from conventional perceptual scores such as perceptual evaluation of
    speech quality (PESQ) [[209](#bib.bib209)] and perceptual evaluation methods for
    audio source separation (PEASS) [[210](#bib.bib210)]. They showed empirically
    that the proposed method can improve the quality of the output speech signals
    by using RL-based optimisation. Fakoor et al. [[211](#bib.bib211)] performed a
    study in an attempt to improve the adaptivity of speech enhancement methods via
    RL. They propose to model the noise-suppression module as a black box, requiring
    no knowledge of the algorithmic mechanics. Using an LSTM-based agent, they showed
    that their method improves system performance compared to methods with no adaptivity.
    In [[212](#bib.bib212)], the authors presented a DRL-based method to achieve personalised
    compression from noisy speech for a specific user in a hearing aid application.
    To deal with non-linearities of human hearing via the reward/punishment mechanism,
    they used a DRL agent that receives preference feedback from the target user.
    Experimental results showed that the developed approach achieved preferred hearing
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to SER, very few studies explored DRL for audio enhancement. Most of
    these studies evaluated DRL-based methods to achieve a certain level of signal
    enhancement in a controlled environment. Further research efforts are needed to
    develop DRL agents that can perform their tasks in real and complex noisy environments.
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Music Listening and Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DL models are widely used for generating content including images, text, and
    music. The motivation for using DL for music generation lies in its generality
    since it can learn from arbitrary corpora of music and be able to generate various
    musical genres compared to classical methods [[213](#bib.bib213), [214](#bib.bib214)].
  prefs: []
  type: TYPE_NORMAL
- en: Here, DRL offers opportunities to impose rules of music theory for the generation
    of more real musical structures [[215](#bib.bib215)]. Various researchers have
    explored such opportunities of DRL for music generation. For instance, in [[216](#bib.bib216)]
    authors achieved better quantitative and qualitative results using an LSTM-based
    architecture in an RL setting generating polyphonic music aligned with musical
    rules. Jiang et al. [[217](#bib.bib217)] presented an interactive RL-Duet framework
    for real-time human-machine duet improvisation. The actor-critic with generalised
    advantage estimator (GAE) [[218](#bib.bib218)] based music generation agent was
    able to learn a policy to generate musical note based on the previous context.
    They trained the model on monophonic and polyphonic data and were able to generate
    high-quality musical pieces compared to a baseline method. Jaques et al. [[215](#bib.bib215)]
    utilised a deep Q-learning agent with a reward function based on rules of music
    theory and probabilistic outputs of an RNN. They showed that the proposed model
    can learn composition rules while maintaining the important information of data
    learned from supervised training. For audio-based generative models, it is often
    important to tune the generated samples towards some domain-specific metrics.
    To achieve this, Guimaraes et al. [[219](#bib.bib219)] proposed a method that
    combines adversarial training with RL. Specifically, they extend the training
    process of a GAN framework to include the domain-specific objectives in addition
    to the discriminator reward. Experimental results show that the proposed model
    can generate music while maintaining the information originally learned from data,
    and attained improvement in the desired metrics. In [[220](#bib.bib220)], they
    also used a GAN-based model for music generation and explored optimisation via
    RL. RaveForce [[221](#bib.bib221)] is a DRL-based environment for music generation,
    which can be used to search new synthesis parameters for a specific timbre of
    an electronic musical note or loop.
  prefs: []
  type: TYPE_NORMAL
- en: Score following is the process of tracking a musical performance for a known
    symbolic representation (a score). In [[222](#bib.bib222)], the authors modelled
    the score following task with DRL algorithms such as synchronous advantage actor-critic
    (A2C). They designed a multi-modal RL agent that listens to music, reads the score
    from an image, and follows the audio in an end-to-end fashion. Experiments on
    monophonic and polyphonic piano music showed promising results compared to state-of-the-art
    methods. The score following task is studied in [[223](#bib.bib223)] using the
    A2C and proximal policy optimisation (PPO). This study showed that the proposed
    approach could be applied to track real piano recordings of human performances.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e458c45c415ddf57f1966ae2c5b2876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A summary of audio-based DRL connecting the application areas and
    algorithms described in the previous two sections – the coloured circles correspond
    to the three groups of algorithms (from left to right: value-based, policy-based,
    model-based)'
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Robotics, Control, and Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a recent growing research interest in robotics to enable robots with
    abilities such as recognition of users’ gestures and intentions [[224](#bib.bib224)],
    and generation of socially appropriate speech-based behaviours [[225](#bib.bib225)].
    In such applications, RL is suitable because robots are required to learn from
    rewards obtained from their actions. Different studies have explored different
    DRL-based approaches for audio and speech processing in robotics. Gao et al. [[226](#bib.bib226)]
    simulated an experiment for the acquisition of spoken-language to provide a proof-of-concept
    of Skinner’s idea [[227](#bib.bib227)], which states that children acquire language
    based on behaviourist reinforcement principles by associating words with meanings.
    Based on their results, the authors were able to show that acquiring spoken language
    is a combination of observing the environment, processing the observation, and
    grounding the observed inputs with their true meaning through a series of reinforcement
    attempts. In [[228](#bib.bib228)], authors build a virtual agent for language
    learning in a maze-like world. It interactively acquires the teacher’s language
    from question answering sentence-directed navigation. Some other studies [[229](#bib.bib229),
    [230](#bib.bib230), [231](#bib.bib231)] in this direction have also explored RL-based
    methods for spoken language learning.
  prefs: []
  type: TYPE_NORMAL
- en: In human-robot interaction, researchers have used audio-driven DRL for robot
    gaze control and dialogue management. In [[232](#bib.bib232)], the authors used
    Q-learning with DNNs for audio-visual gaze control with the specific goal of finding
    good policies to control the orientation of a robot head towards groups of people
    using audio-visual information. Similarly, authors of [[233](#bib.bib233)] used
    a deep Q-network taking into account visual and acoustic observations to direct
    the robot’s head towards targets of interest. Based on the results, the authors
    showed that the proposed framework generates state-of-the-art results. Clark et
    al. [[234](#bib.bib234)] proposed an end-to-end learning framework that can induce
    generalised and high-level rules of human interactions from structured demonstrations.
    They empirically show that the proposed model was able to identify both auditory
    and gestural responses correctly. Another interesting work [[235](#bib.bib235)]
    utilised a deep Q-network for speech-driven backchannels like laugh generation
    to enhance engagement in human-robot interaction. Based on their experiments,
    they found that the proposed method has the potential of training a robot for
    engaging behaviours. Similarly, [[236](#bib.bib236)] utilised recurrent Q-learning
    for backchannel generation to engage agents during human-robot interaction. They
    showed that an agent trained using off-policy RL produces more engagement than
    an agent trained from imitation learning. In a similar strand, [[237](#bib.bib237)]
    have applied a deep Q-network to control the speech volume of a humanoid robot
    in environments with different amounts of noise. In a trial with human subjects,
    participants rated the proposed DRL-based solution better than fixed-volume robots.
    DRL has also been applied to spoken language understanding [[238](#bib.bib238)],
    where a deep Q-network receives symbolic representations from an intent recogniser
    and outputs actions such as (keep mug on sink). In [[121](#bib.bib121)], the authors
    trained a humanoid robot to acquire social skills for tracking and greeting people.
    In their experiments, the robot learnt its human-like behaviour from experiences
    in a real uncontrolled environment. In [[120](#bib.bib120)], they propose an approach
    for efficiently training the behaviour of a robot playing games using a very limited
    amount of demonstration dialogues. Although the learnt multimodal behaviours are
    not always perfect (due to noisy perceptions), they were reasonable while the
    trained robot interacted with real human players. Efficient training has also
    been explored using interactive feedback from human demonstrators as in [[239](#bib.bib239)],
    who show that DRL with interactive feedback leads to faster learning and with
    fewer mistakes than autonomous DRL (without interactive feedback).
  prefs: []
  type: TYPE_NORMAL
- en: Robotics plays an interesting role in bringing audio-based DRL applications
    together including all or some of the above. For example, a robot recognising
    speech and understanding language [[238](#bib.bib238)], aware of emotions [[199](#bib.bib199)],
    carryout activities such as playing games [[120](#bib.bib120)], greeting people [[121](#bib.bib121)],
    or playing music [[240](#bib.bib240)], among others. Such a collection of DRL
    agents are currently trained independently, but we should expect more connectedness
    between them in the future work.
  prefs: []
  type: TYPE_NORMAL
- en: V Challenges in Audio-Based DRL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The research works in the previous section have focused on a narrow set of DRL
    algorithms and have ignored the existence of many other algorithms, as can be
    noted in Figure [5](#S4.F5 "Figure 5 ‣ IV-E Music Listening and Generation ‣ IV
    Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications").
    This suggests the need for a stronger collaboration between core DRL and audio-based
    DRL, which may be already happening. In Figure [6](#S5.F6 "Figure 6 ‣ V-A Real-World
    Audio-Based Systems ‣ V Challenges in Audio-Based DRL ‣ A Survey on Deep Reinforcement
    Learning for Audio-Based Applications"), we note an increased interest in the
    communities of core and applied DRL. While core DRL grew from 3 to 4 orders of
    magnitude from 2015 to 2020, applied DRL grew from 2 to 3 orders of magnitude
    in the same period.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [7](#S5.F7 "Figure 7 ‣ V-C Multi-Agent and Truly Autonomous Systems ‣
    V Challenges in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for
    Audio-Based Applications") help us to illustrate that previous works have only
    explored a modest space of what is possible. Based on the related works above,
    we have identified three main challenges that need to be addressed by future systems.
    Those dimensions converge in what we call ‘very advanced systems’.
  prefs: []
  type: TYPE_NORMAL
- en: V-A Real-World Audio-Based Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the DRL algorithms described in Section [III](#S3 "III Deep Reinforcement
    Learning ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications")
    carry out experiments on the Atari benchmark [[241](#bib.bib241)], where there
    is no difference between training and test environments. This is an important
    limitation in the literature, and it should be taken into account in the development
    of future DRL algorithms. In contrast, audio-based DRL applications tend to make
    use of a more explicit separation between training and test environments. While
    audio-based DRL agents may be trained from offline interactions or simulations,
    their performance requires to be assessed using a separate set of offline data
    or real interactions. The latter (often referred to as human evaluations) is very
    important for analysing and evidencing the quality of learnt behaviours. In almost
    all (if not all) audio-based systems, the creation of data is difficult and expensive.
    This highlights the need for more data-efficient algorithms—specially if DRL agents
    are expected to learn from real data instead of synthetic data. In high-frequency
    audio-based control tasks, DRL agents have the requirements of learning fast and
    avoiding repeating the same mistake. Real-world audio-based systems require algorithms
    that are sample efficient and performant in their operations. This makes the application
    of DRL algorithms in real systems very challenging. Some studies such as [[242](#bib.bib242),
    [243](#bib.bib243), [244](#bib.bib244)], have presented approaches to improve
    the sample efficiency of DRL systems. These approaches, however, have not been
    applied to audio-based systems. This suggests that much more research is required
    to make DRL more practical and successful for its application in real audio-based
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79395a5d5d5b8d336be6ba48e800e8bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Cumulative distribution of publications per year (data gathered from
    2015 to 2020) – from https://www.scopus.com'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Knowledge Transfer and Generalisation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning behaviours from complex signals like speech and audio with DRL requires
    processing high-dimensional inputs and performing extensive training on a large
    number of samples to achieve improved performance. The unavailability of large
    labelled datasets is indeed one of the major obstacles in the area of audio-driven
    DRL [[1](#bib.bib1)]. Moreover, it is computationally expensive to train a single
    DRL agent, and there is a need for training multiple DRL agents in order to equip
    audio-based systems with a variety of learnt skills. Therefore, some researchers
    have turned their attention to studying different schemes such as policy distillation [[245](#bib.bib245)],
    progressive neural networks [[246](#bib.bib246)], multi-domain/multi-task learning [[169](#bib.bib169),
    [150](#bib.bib150), [247](#bib.bib247), [248](#bib.bib248)] and others [[249](#bib.bib249),
    [250](#bib.bib250), [251](#bib.bib251)] to promote transfer learning and generalisation
    in DRL to improve system performance and reduce computational costs. Only a few
    studies in dialogue systems have started to explore transfer learning in DRL for
    the speech, audio and dialogue domains [[252](#bib.bib252), [163](#bib.bib163),
    [168](#bib.bib168), [177](#bib.bib177), [166](#bib.bib166)], and more research
    is needed in this area. DRL agents are often trained from scratch instead of inheriting
    useful behaviours from other agents. Research efforts in these directions would
    contribute towards a more practical, cost-effective, and robust application of
    audio-based DRL agents. On the one hand, to train agents less data-intensively,
    and on the other to achieve reasonable performance in the real world.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Multi-Agent and Truly Autonomous Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Audio-based DRL has achieved impressive performance in single-agent domains,
    where the environment stays mostly stationary. But in the case of audio-based
    systems operating in real-world scenarios, the environments are typically challenging
    and dynamic. For instance, multi-lingual ASR and spoken dialogue systems need
    to learn policies for different languages and domains. These tasks not only involve
    a high degree of uncertainty and complicated dynamics but are also characterised
    by the fact that they are situated in the real physical world, thus have an inherently
    distributed nature. The problem, thus, falls naturally into the realm of multi-agent
    RL (MARL), an area of knowledge with a relatively long history, and has recently
    re-emerged due to advances in single-agent RL techniques [[253](#bib.bib253),
    [254](#bib.bib254)]. Coupled with recent advances in DNNs, MARL has been in the
    limelight for many recent breakthroughs in various domains including control systems,
    communication networks, economics, etc. However, applications in the audio processing
    domain are relatively limited due to various challenges. The learning goals in
    MARL are multidimensional—because the objectives of all agents are not necessarily
    aligned. This situation can arise for example in simultaneous emotion and speaker
    voice recognition, where the goal of one agent is to identify emotions and the
    goal of the other agent is to recognise the speaker. As a consequence, these agents
    can independently perceive the environment, and act according to their individual
    objectives (rewards) thus modifying the environment. This can bring up the challenge
    of dealing with equilibrium points, as well as some additional performance criteria
    beyond return-optimisation, such as the robustness against potential adversarial
    agents. As all agents try to improve their policies according to their interests
    concurrently, therefore the action executed by one agent affects the goals and
    objectives of the other agents (e.g. speaker, gender, and emotion identification
    from speech at the same time), and vice-versa.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eef5431f4e0d4b23dcfdf60716f9f7ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A pictorial view of previous works on audio-based DRL and potential
    dimensions to explore in future systems.'
  prefs: []
  type: TYPE_NORMAL
- en: One remaining challenging aspect is that of autonomous skill acquisition. Most,
    if not all, DRL agents currently require a substantial amount of pre-programming
    as opposed to acquiring skills autonomously to enable personalised/extensible
    behaviour. Such pre-programming includes explicit implementations of states, actions,
    rewards, and policies. Although substantial progress in different areas has been
    made, the idea of creating audio-driven DRL agents that autonomously learn their
    states, actions, and rewards in order to induce useful skills remains to be investigated
    further. Such kind of agents would have to know when and how to observe their
    environments, identify a task and input features, induce a set of actions, induce
    a reward function (from audio, images, or both), and use all of that to train
    policies. Such agents have the potential to show advanced levels of intelligence,
    and they would be very useful for applications such as personal assistants or
    interactive robots.
  prefs: []
  type: TYPE_NORMAL
- en: VI Summary and Future Pointers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This literature review shows that DRL is becoming popular in audio processing
    and related applications. We collected DRL research papers in six different but
    related areas: automatic speech recognition (ASR), speech emotion recognition
    (SER), spoken dialogue systems (SDSs), audio enhancement, audio-driven robotic
    control, and music generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In ASR, most of the studies have used policy gradient-based DRL, as it allows
    learning an optimal policy that maximises the performance objective. We found
    studies aiming to solve the complexity of ASR models [[131](#bib.bib131)], tackle
    slow convergence issues [[133](#bib.bib133)], and speed up the convergence in
    DRL [[132](#bib.bib132)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The development of SDSs with DRL is gaining interest and different studies have
    shown very interesting results that have outperformed current state-of-the-art
    DL approaches [[143](#bib.bib143)]. However, there is still room for improvement
    regarding the effective and practical training of DRL-based spoken dialogue systems.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Several studies have also applied DRL to emotion recognition and empirically
    showed that DRL can (i) lower latency while making predictions [[199](#bib.bib199)],
    (ii) understand emotional dynamics in communication [[200](#bib.bib200)], and
    (iii) enhance human-computer interaction [[201](#bib.bib201)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the case of audio enhancement, studies have shown the potential of DRL. While
    these studies have focused their attention on the speech signals, DRL can be used
    to optimise the audio enhancement module along with performance objectives such
    as those in ASR [[207](#bib.bib207)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In music generation, DRL can optimise rules of music theory as validated in
    different studies [[215](#bib.bib215), [219](#bib.bib219)]. It can also be used
    to search for new tone synthesis parameters [[221](#bib.bib221)]. Moreover, DRL
    can be used to perform score following to track a musical performance [[222](#bib.bib222)],
    and it is even suitable for tracking real piano recordings [[223](#bib.bib223)],
    among other possible tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In robotics, audio-based DRL agents are in their infancy. Previous studies have
    trained DRL-based agents using simulations, which have shown that reinforcement
    principles help agents in the acquisition of spoken language. Some recent works [[235](#bib.bib235),
    [236](#bib.bib236)] have shown that DRL can be utilised to train gaze controllers
    and speech-driven backchannels like laughs in human-robot interaction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The related works reviewed above highlight several benefits of using DRL for
    audio processing and applications. Challenges remain before such advancements
    will succeed in the real world, including endowing agents with commonsense knowledge,
    knowledge transfer, generalisation, and autonomous learning, among others. Such
    advances need to be demonstrated not only in simulated and stationary environments,
    but in real and non-stationary one as in real world scenarios. Steady progress,
    however, is being made in the right direction for designing more adaptive audio-based
    systems that can be better suited for real-world settings. If such scientific
    progress keeps growing rapidly, perhaps we are not too far away from AI-based
    autonomous systems that can listen, process, and understand audio and act in more
    human-like ways in increasingly complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: VII Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we have focused on presenting a comprehensive review of deep reinforcement
    learning (DRL) techniques for audio based applications. We reviewed DRL research
    works in six different audio-related areas including automatic speech recognition
    (ASR), speech emotion recognition (SER), spoken dialogue systems (SDSs), audio
    enhancement, audio-driven robotic control, and music generation. In all of these
    areas, the use of DRL techniques is becoming increasingly popular, and ongoing
    research on this topic has explored many DRL algorithms with encouraging results
    for audio-related applications. Apart from providing a detailed review, we have
    also highlighted (i) various challenges that hinder DRL research in audio applications
    and (ii) various avenues for future research. We hope that this paper will help
    researchers and practitioners interested in exploring and solving problems in
    the audio domain using DRL techniques.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We would like to thank Kai Arulkumaran (Imperial College London, United Kingdom)
    and Dr Soujanya Poria (Singapore University of Technology and Design) for proving
    feedback on the paper. We also thank Waleed Iqbal (Queen Mary University of London,
    United Kingdom) for helping with the extraction of DRL related data from Scopus
    (Figure [6](#S5.F6 "Figure 6 ‣ V-A Real-World Audio-Based Systems ‣ V Challenges
    in Audio-Based DRL ‣ A Survey on Deep Reinforcement Learning for Audio-Based Applications")).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Purwins, B. Li, T. Virtanen, J. Schlüter, S.-Y. Chang, and T. Sainath,
    “Deep learning for audio signal processing,” *IEEE Journal of Selected Topics
    in Signal Processing*, vol. 13, no. 2, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] R. S. Sutton, A. G. Barto *et al.*, *Introduction to reinforcement learning*.   MIT
    press Cambridge, 1998, vol. 135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] N. Kohl and P. Stone, “Policy gradient reinforcement learning for fast
    quadrupedal locomotion,” in *IEEE International Conference on Robotics and Automation
    (ICRA)*, vol. 3, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] A. Y. Ng, A. Coates, M. Diel, V. Ganapathi, J. Schulte, B. Tse, E. Berger,
    and E. Liang, “Autonomous inverted helicopter flight via reinforcement learning,”
    in *Experimental robotics IX*.   Springer, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] S. Singh, D. Litman, M. Kearns, and M. Walker, “Optimizing dialogue management
    with reinforcement learning: Experiments with the njfun system,” *Journal of Artificial
    Intelligence Research*, vol. 16, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. L. Strehl, L. Li, E. Wiewiora, J. Langford, and M. L. Littman, “Pac
    model-free reinforcement learning,” in *International Conference on Machine Learning
    (ICML)*, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal processing magazine*, vol. 29, no. 6, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A.-r. Mohamed, G. Dahl, and G. Hinton, “Deep belief networks for phone
    recognition,” in *NIPS workshop on deep learning for speech recognition and related
    applications*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural computation*, vol. 1, no. 4, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] S. Lange, M. A. Riedmiller, and A. Voigtländer, “Autonomous reinforcement
    learning on raw visual input data in a real world application,” in *International
    Joint Conference on Neural Networks (IJCNN), Brisbane, Australia, June 10-15,
    2012*.   IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski *et al.*, “Human-level
    control through deep reinforcement learning,” *Nature*, vol. 518, no. 7540, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche,
    J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot *et al.*, “Mastering
    the game of go with deep neural networks and tree search,” *nature*, vol. 529,
    no. 7587, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training of
    deep visuomotor policies,” *The Journal of Machine Learning Research*, vol. 17,
    no. 1, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel,
    “Rl²: Fast reinforcement learning via slow reinforcement learning,” *arXiv preprint
    arXiv:1611.02779*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos,
    C. Blundell, D. Kumaran, and M. Botvinick, “Learning to reinforcement learn,”
    *arXiv preprint arXiv:1611.05763*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. Fei-Fei, and A. Farhadi,
    “Target-driven visual navigation in indoor scenes using deep reinforcement learning,”
    in *IEEE international conference on robotics and automation (ICRA)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, “Deep
    reinforcement learning: A brief survey,” *IEEE Signal Processing Magazine*, vol. 34,
    no. 6, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Y. Li, “Deep reinforcement learning: An overview,” *arXiv preprint arXiv:1701.07274*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y.-C. Liang, and
    D. I. Kim, “Applications of deep reinforcement learning in communications and
    networking: A survey,” *IEEE Communications Surveys & Tutorials*, vol. 21, no. 4,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A. Sallab, S. Yogamani,
    and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,”
    *arXiv preprint arXiv:2002.00444*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Haydari and Y. Yilmaz, “Deep reinforcement learning for intelligent
    transportation systems: A survey,” *arXiv preprint arXiv:2005.00935*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] N. D. Nguyen, T. Nguyen, and S. Nahavandi, “System design perspective
    for human-level agents using deep reinforcement learning: A survey,” *IEEE Access*,
    vol. 5, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “Deep reinforcement
    learning framework for autonomous driving,” *Electronic Imaging*, vol. 2017, no. 19,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Khan, A. Sohail, U. Zahoora, and A. S. Qureshi, “A survey of the recent
    architectures of deep convolutional neural networks,” *Artif. Intell. Rev.*, vol. 53,
    no. 8, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial discriminative
    domain adaptation,” in *2017 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Schlüter and S. Böck, “Improved musical onset detection with convolutional
    neural networks,” in *International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] N. Mamun, S. Khorram, and J. H. Hansen, “Convolutional neural network-based
    speech enhancement for cochlear implant recipients,” in *Interspeech*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu,
    “Convolutional neural networks for speech recognition,” *IEEE/ACM Transactions
    on audio, speech, and language processing*, vol. 22, no. 10, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S.-Y. Chang, B. Li, G. Simko, T. N. Sainath, A. Tripathi, A. van den Oord,
    and O. Vinyals, “Temporal modeling using dilated convolution and gating for voice-activity-detection,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Chen, Q. Guo, X. Liang, J. Wang, and Y. Qian, “Environmental sound
    classification with dilated convolutions,” *Applied Acoustics*, vol. 148, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Z. C. Lipton, “A critical review of recurrent neural networks for sequence
    learning,” *CoRR*, vol. abs/1506.00019, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Latif, J. Qadir, A. Qayyum, M. Usama, and S. Younis, “Speech technology
    for healthcare: Opportunities, challenges, and state of the art,” *IEEE Reviews
    in Biomedical Engineering*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] K. Cho, B. van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder–decoder for
    statistical machine translation,” in *Conference on Empirical Methods in Natural
    Language Processing (EMNLP)*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Li, A. Mohamed, G. Zweig, and Y. Gong, “LSTM time and frequency recurrence
    for automatic speech recognition,” in *IEEE workshop on automatic speech recognition
    and understanding (ASRU)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] T. N. Sainath and B. Li, “Modeling time-frequency patterns with lstm vs.
    convolutional architectures for lvcsr tasks,” in *Interspeech*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Y. Qian, M. Bi, T. Tan, and K. Yu, “Very deep convolutional neural networks
    for noise robust speech recognition,” *IEEE/ACM Transactions on Audio, Speech,
    and Language Processing*, vol. 24, no. 12, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Latif, “Deep representation learning for improving speech emotion recognition,”
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] D. Ghosal and M. H. Kolekar, “Music genre recognition using deep neural
    networks and transfer learning.” in *Interspeech*, vol. 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Advances in Neural Information Processing Systems (NIPS)*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,”
    *IEEE Transactions Signal Process.*, vol. 45, no. 11, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] H. Salehinejad, J. Baarbe, S. Sankar, J. Barfett, E. Colak, and S. Valaee,
    “Recent advances in recurrent neural networks,” *CoRR*, vol. abs/1801.01078, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Zhang, W. Chan, and N. Jaitly, “Very deep convolutional networks for
    end-to-end speech recognition,” in *International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] L. Lu, X. Zhang, and S. Renals, “On training the recurrent neural network
    encoder-decoder for large vocabulary end-to-end speech recognition,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] R. Liu, J. Yang, and M. Liu, “A new end-to-end long-time speech synthesis
    system based on tacotron2,” in *International Symposium on Signal Processing Systems*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Graves, “Sequence transduction with recurrent neural networks,” *Workshop
    on Representation Learning, International Conference of Machine Learning (ICML)
    2012*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] C. Raffel, M.-T. Luong, P. J. Liu, R. J. Weiss, and D. Eck, “Online and
    linear-time attention by enforcing monotonic alignments,” in *International Conference
    on Machine Learning (ICML)*.   JMLR. org, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell:
    A neural network for large vocabulary conversational speech recognition,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] N. Jaitly, Q. V. Le, O. Vinyals, I. Sutskever, D. Sussillo, and S. Bengio,
    “An online sequence-to-sequence model using partial conditioning,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] N. Pham, T. Nguyen, J. Niehues, M. Müller, and A. Waibel, “Very deep self-attention
    networks for end-to-end speech recognition,” in *Interspeech*, G. Kubin and Z. Kacic,
    Eds.   ISCA, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in Neural
    Information Processing Systems (NIPS)*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. Shannon, H. Zen, and W. Byrne, “Autoregressive models for statistical
    parametric speech synthesis,” *IEEE transactions on audio, speech, and language
    processing*, vol. 21, no. 3, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] W.-N. Hsu, Y. Zhang, and J. Glass, “Learning latent representations for
    speech generation and transformation,” in *Interspeech*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Ma, D. McDuff, and Y. Song, “M3D-GAN: Multi-modal multi-domain translation
    with universal attention,” *arXiv preprint arXiv:1907.04378*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Qadir, and B. W. Schuller,
    “Deep representation learning in speech processing: Challenges, recent advances,
    and future trends,” *arXiv preprint arXiv:2001.00378*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] X. Wang, S. Takaki, and J. Yamagishi, “Autoregressive neural f0 model
    for statistical parametric speech synthesis,” *IEEE/ACM Transactions on Audio,
    Speech, and Language Processing*, vol. 26, no. 8, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] R. Bellman, “Dynamic programming,” *Science*, vol. 153, no. 3731, 1966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double Q-learning,” in *AAAI Conference*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience
    replay,” *International Conference on Learning Representations (ICLR)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,
    “Dueling network architectures for deep reinforcement learning,” in *International
    Conference on Machine Learning (ICML)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective
    on reinforcement learning,” in *International Conference on Machine Learning (ICML)*.   JMLR.
    org, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos, “Distributional
    reinforcement learning with quantile regression,” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] W. Dabney, G. Ostrovski, D. Silver, and R. Munos, “Implicit quantile networks
    for distributional reinforcement learning,” in *International Conference on Machine
    Learning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] N. Levine, T. Zahavy, D. J. Mankowitz, A. Tamar, and S. Mannor, “Shallow
    updates for deep reinforcement learning,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] T. Hester, M. Vecerik, O. Pietquin, M. Lanctot, T. Schaul, B. Piot, D. Horgan,
    J. Quan, A. Sendonaris, I. Osband *et al.*, “Deep Q-learning from demonstrations,”
    in *AAAI Conference*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] M. Sabatelli, G. Louppe, P. Geurts, and M. Wiering, “Deep quality value
    (dqv) learning,” *Advances in Neural Information Processing Systems (NIPS)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter,
    and S. Hochreiter, “Rudder: Return decomposition for delayed rewards,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] T. Pohlen, B. Piot, T. Hester, M. G. Azar, D. Horgan, D. Budden, G. Barth-Maron,
    H. Van Hasselt, J. Quan, M. Večerík *et al.*, “Observe and look further: Achieving
    consistent performance on Atari,” *arXiv preprint arXiv:1805.11593*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Schulman, X. Chen, and P. Abbeel, “Equivalence between policy gradients
    and soft q-learning,” *arXiv preprint arXiv:1704.06440*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. Hausknecht and P. Stone, “Deep recurrent Q-learning for partially observable
    MDPs,” in *AAAI Fall Symposium Series*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] I. Sorokin, A. Seleznev, M. Pavlov, A. Fedorov, and A. Ignateva, “Deep
    attention recurrent Q-network,” *Deep Reinforcement Learning Workshop, NIPS*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Oh, V. Chockalingam, H. Lee *et al.*, “Control of memory, active perception,
    and action in minecraft,” in *International Conference on Machine Learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] E. Parisotto and R. Salakhutdinov, “Neural map: Structured memory for
    deep reinforcement learning,” in *International Conference on Learning Representations*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] V. R. Konda and J. N. Tsitsiklis, “Actor-Critic agorithms,” in *Neural
    Information Processing Systems (NIPS)*, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver,
    and K. Kavukcuoglu, “Asynchronous methods for deep reinforcement learning,” in
    *International Conference on Machine Learning (ICML)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, “Reinforcement
    learning through asynchronous advantage actor-critic on a gpu,” in *Learning Representations*.   ICLR,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Alfredo, C. Humberto, and C. Arjun, “Efficient parallel methods for
    deep reinforcement learning,” in *The Multi-disciplinary Conference on Reinforcement
    Learning and Decision Making (RLDM)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] B. O’Donoghue, R. Munos, K. Kavukcuoglu, and V. Mnih, “PGQ: Combining
    policy gradient and Q-learning,” *arXiv preprint arXiv:1611.01626*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] R. Munos, T. Stepleton, A. Harutyunyan, and M. Bellemare, “Safe and efficient
    off-policy reinforcement learning,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] A. Gruslys, M. G. Azar, M. G. Bellemare, and R. Munos, “The reactor: A
    sample-efficient actor-critic architecture,” *arXiv preprint arXiv:1704.04651*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron,
    V. Firoiu, T. Harley, I. Dunning *et al.*, “IMPALA: Scalable distributed deep-RL
    with importance weighted actor-learner architectures,” in *International Conference
    on Machine Learning (ICML)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region
    policy optimization,” in *International Conference on Machine Learning (ICML)*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal
    policy optimization algorithms,” *arXiv preprint arXiv:1707.06347*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] B. Ravindran, “Introduction to deep reinforcement learning,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Ł. Kaiser, M. Babaeizadeh, P. Miłos, B. Osiński, R. H. Campbell, K. Czechowski,
    D. Erhan, C. Finn, P. Kozakowski, S. Levine *et al.*, “Model based reinforcement
    learning for atari,” in *International Conference on Learning Representations*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] S. Whiteson, “TreeQN and ATreeC: Differentiable tree planning for deep
    reinforcement learning,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Oh, S. Singh, and H. Lee, “Value prediction network,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Vezhnevets, V. Mnih, S. Osindero, A. Graves, O. Vinyals, J. Agapiou
    *et al.*, “Strategic attentive writer for learning macro-actions,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] N. Nardelli, G. Synnaeve, Z. Lin, P. Kohli, P. H. Torr, and N. Usunier,
    “Value propagation networks,” in *International Conference on Learning Representations*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt,
    A. Guez, E. Lockhart, D. Hassabis, T. Graepel *et al.*, “Mastering Atari, Go,
    Chess and Shogi by planning with a learned model,” *arXiv preprint arXiv:1911.08265*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. P. Rath, D. Povey, K. Veselý, and J. Cernocký, “Improved feature processing
    for deep neural networks,” in *Interspeech*.   ISCA, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr
    corpus based on public domain audio books,” in *International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A. Rousseau, P. Deléglise, and Y. Esteve, “TED-LIUM: an automatic speech
    recognition dedicated corpus.” in *LREC*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] D. B. Paul and J. M. Baker, “The design for the wall street journal-based
    CSR corpus,” in *Workshop on Speech and Natural Language*.   ACL, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. J. Godfrey, E. C. Holliman, and J. McDaniel, “SWITCHBOARD: Telephone
    speech corpus for research and development,” in *International Conference on Acoustics,
    Speech, and Signal Processing (ICASSP)*, vol. 1, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett,
    “DARPA TIMIT acoustic-phonetic continous speech corpus CD-ROM. NIST speech disc
    1-1.1,” *NASA STI/Recon technical report n*, vol. 93, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Rastogi, X. Zang, S. Sunkara, R. Gupta, and P. Khaitan, “Towards scalable
    multi-domain conversational agents: The schema-guided dialogue dataset,” in *The
    Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI*.   AAAI Press,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] J. D. Williams, A. Raux, and M. Henderson, “The dialog state tracking
    challenge series: A review,” *Dialogue Discourse*, vol. 7, no. 3, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. E. Asri, H. Schulz, S. Sharma, J. Zumer, J. Harris, E. Fine, R. Mehrotra,
    and K. Suleman, “Frames: a corpus for adding memory to goal-oriented dialogue
    systems,” in *Annual SIGdial Meeting on Discourse and Dialogue*, K. Jokinen, M. Stede,
    D. DeVault, and A. Louis, Eds.   ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] P. Budzianowski, T.-H. Wen, B.-H. Tseng, I. Casanueva, S. Ultes, O. Ramadan,
    and M. Gasic, “Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented
    dialogue modelling,” in *Conference on Empirical Methods in Natural Language Processing
    (EMNLP)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] D. Ameixa, L. Coheur, and R. A. Redol, “From subtitles to human interactions:
    introducing the subtle corpus,” Tech. rep., INESC-ID (November 2014), Tech. Rep.,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] J. Schatzmann, K. Weilhammer, M. N. Stuttle, and S. J. Young, “A survey
    of statistical user simulation techniques for reinforcement-learning of dialogue
    management strategies,” *Knowledge Eng. Review*, vol. 21, no. 2, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] I. V. Serban, R. Lowe, P. Henderson, L. Charlin, and J. Pineau, “A survey
    of available corpora for building data-driven dialogue systems: The journal version,”
    *Dialogue Discourse*, vol. 9, no. 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss,
    “A database of german emotional speech,” in *European Conference on Speech Communication
    and Technology*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N.
    Chang, S. Lee, and S. S. Narayanan, “IEMOCAP: Interactive emotional dyadic motion
    capture database,” *Language resources and evaluation*, vol. 42, no. 4, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab, N. Sadoughi,
    and E. M. Provost, “MSP-IMPROV: An acted corpus of dyadic interactions to study
    emotion perception,” *IEEE Transactions on Affective Computing*, vol. 8, no. 1,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] G. McKeown, M. Valstar, R. Cowie, M. Pantic, and M. Schroder, “The semaine
    database: Annotated multimodal records of emotionally colored conversations between
    a person and a limited agent,” *IEEE transactions on affective computing*, vol. 3,
    no. 1, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea,
    “MELD: A multimodal multi-party dataset for emotion recognition in conversations,”
    in *Annual Meeting of the Association for Computational Linguistics ACL*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] J. Thiemann, N. Ito, and E. Vincent, “The diverse environments multi-channel
    acoustic noise database: A database of multichannel environmental noise recordings,”
    *The Journal of the Acoustical Society of America*, vol. 133, no. 5, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, “The third ‘CHiME’speech
    separation and recognition challenge: Dataset, task and baselines,” in *IEEE Workshop
    on Automatic Speech Recognition and Understanding (ASRU)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Maciejewski, G. Wichern, E. McQuinn, and J. Le Roux, “WHAMR!: Noisy
    and reverberant single-channel speech separation,” in *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] B. Krueger, “Classical piano midi page,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] J. Thickstun, Z. Harchaoui, and S. Kakade, “Learning features of music
    from scratch,” *arXiv preprint arXiv:1611.09827*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] M. Allan and C. Williams, “Harmonising chorales by probabilistic inference,”
    in *Advances in Neural Information Processing Systems (NIPS)*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] I. D. Gebru, S. Ba, X. Li, and R. Horaud, “Audio-visual speaker diarization
    based on spatiotemporal Bayesian fusion,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 40, no. 5, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Scalise, S. Li, H. Admoni, S. Rosenthal, and S. S. Srinivasa, “Natural
    language instructions for human-robot collaborative manipulation,” *Int. J. Robotics
    Res.*, vol. 37, no. 6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] D. K. Misra, J. Sung, K. Lee, and A. Saxena, “Tell me dave: Context-sensitive
    grounding of natural language to manipulation instructions,” *Int. J. Robotics
    Res.*, vol. 35, no. 1-3, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] H. Cuayáhuitl, “A data-efficient deep learning approach for deployable
    multimodal social robots,” *Neurocomputing*, vol. 396, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] A. H. Qureshi, Y. Nakamura, Y. Yoshikawa, and H. Ishiguro, “Intrinsically
    motivated reinforcement learning for human-robot interaction in the real-world,”
    *Neural Networks*, vol. 107, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] T. Kala and T. Shinozaki, “Reinforcement learning of speech recognition
    system based on policy gradient and hypothesis selection,” in *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] A. Tjandra, S. Sakti, and S. Nakamura, “Sequence-to-sequence ASR optimization
    via reinforcement learning,” in *International Conference on Acoustics, Speech
    and Signal Processing (ICASSP)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] ——, “End-to-end speech recognition sequence training with reinforcement
    learning,” *IEEE Access*, vol. 7, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] H. Chung, H.-B. Jeon, and J. G. Park, “Semi-supervised training for sequence-to-sequence
    speech recognition using reinforcement learning,” in *2020 International Joint
    Conference on Neural Networks (IJCNN)*.   IEEE, 2020, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] S. Karita, A. Ogawa, M. Delcroix, and T. Nakatani, “Sequence training
    of encoder-decoder model using policy gradient for end-to-end speech recognition,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Y. Zhou, C. Xiong, and R. Socher, “Improving end-to-end speech recognition
    with policy learning,” in *International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Y. Luo, C.-C. Chiu, N. Jaitly, and I. Sutskever, “Learning online alignments
    with continuous rewards policy gradient,” in *International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] K. Radzikowski, R. Nowak, L. Wang, and O. Yoshie, “Dual supervised learning
    for non-native speech recognition,” *EURASIP Journal on Audio, Speech, and Music
    Processing*, vol. 2019, no. 1, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, “Amc: Automl for
    model compression and acceleration on mobile devices,” in *European Conference
    on Computer Vision (ECCV)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Ł. Dudziak, M. S. Abdelfattah, R. Vipperla, S. Laskaridis, and N. D.
    Lane, “ShrinkML: End-to-end asr model compression using reinforcement learning,”
    in *Interspeech*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] T. Rajapakshe, S. Latif, R. Rana, S. Khalifa, and B. W. Schuller, “Deep
    reinforcement learning with pre-training for time-efficient training of automatic
    speech recognition,” *arXiv preprint arXiv:2005.11172*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] R. J. Williams, “Simple statistical gradient-following algorithms for
    connectionist reinforcement learning,” *Machine learning*, vol. 8, no. 3-4, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] D. Lawson, C.-C. Chiu, G. Tucker, C. Raffel, K. Swersky, and N. Jaitly,
    “Learning hard alignments with variational inference,” in *International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] V. W. Zue and J. R. Glass, “Conversational interfaces: advances and challenges,”
    *IEEE*, vol. 88, no. 8, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] E. Levin, R. Pieraccini, and W. Eckert, “A stochastic model of human-machine
    interaction for learning dialog strategies,” *IEEE Transactions Speech Audio Process.*,
    vol. 8, no. 1, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] S. P. Singh, M. J. Kearns, D. J. Litman, and M. A. Walker, “Reinforcement
    learning for spoken dialogue systems,” in *Advances in Neural Information Processing
    Systems (NIPS)*, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] T. Paek, “Reinforcement learning for spoken dialogue systems: Comparing
    strengths and weaknesses for practical deployment,” in *Proc. Dialog-on-Dialog
    Workshop, Interspeech*.   Citeseer, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Gao, M. Galley, and L. Li, “Neural approaches to conversational AI,”
    *Found. Trends Inf. Retr.*, vol. 13, no. 2-3, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Z. Chen, L. Chen, X. Zhou, and K. Yu, “Deep reinforcement learning for
    on-line dialogue state tracking,” *arXiv preprint arXiv:2009.10321*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Henderson, B. Thomson, and J. D. Williams, “The second dialog state
    tracking challenge,” in *Proceedings of the 15th annual meeting of the special
    interest group on discourse and dialogue (SIGDIAL)*, 2014, pp. 263–272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] ——, “The third dialog state tracking challenge,” in *2014 IEEE Spoken
    Language Technology Workshop (SLT)*.   IEEE, 2014, pp. 324–329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] G. Weisz, P. Budzianowski, P.-H. Su, and M. Gašić, “Sample efficient
    deep reinforcement learning for dialogue systems with large action spaces,” *IEEE/ACM
    Transactions on Audio, Speech, and Language Processing*, vol. 26, no. 11, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu, and N. de Freitas,
    “Sample efficient actor-critic with experience replay,” *arXiv preprint arXiv:1611.01224*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] H. Cuayáhuitl, “Simpleds: A simple deep reinforcement learning dialogue
    system,” in *Dialogues with social robots*.   Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] T. Zhao and M. Eskenazi, “Towards end-to-end learning for dialog state
    tracking and management using deep reinforcement learning,” in *Annual Meeting
    of the Special Interest Group on Discourse and Dialogue*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] I. Casanueva, P. Budzianowski, P.-H. Su, N. Mrkšić, T.-H. Wen, S. Ultes,
    L. Rojas-Barahona, S. Young, and M. Gašić, “A benchmarking environment for reinforcement
    learning based task oriented dialogue management,” *Deep Reinforcement Learning
    Symposium, NIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] P.-H. Su, P. Budzianowski, S. Ultes, M. Gasic, and S. Young, “Sample-efficient
    actor-critic reinforcement learning with supervised data for dialogue management,”
    in *Annual SIGdial Meeting on Discourse and Dialogue*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Gašić and S. Young, “Gaussian processes for POMDP-based dialogue manager
    optimization,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    vol. 22, no. 1, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Ultes, L. M. R. Barahona, P.-H. Su, D. Vandyke, D. Kim, I. Casanueva,
    P. Budzianowski, N. Mrkšić, T.-H. Wen, M. Gasic *et al.*, “Pydial: A multi-domain
    statistical dialogue system toolkit,” in *ACL System Demonstrations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] H. Cuayáhuitl, “Hierarchical reinforcement learning for spoken dialogue
    systems,” Ph.D. dissertation, University of Edinburgh, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] H. Cuayáhuitl, S. Renals, O. Lemon, and H. Shimodaira, “Evaluation of
    a hierarchical reinforcement learning spoken dialogue system,” *Comput. Speech
    Lang.*, vol. 24, no. 2, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] N. Dethlefs and H. Cuayáhuitl, “Hierarchical reinforcement learning for
    situated natural language generation,” *Nat. Lang. Eng.*, vol. 21, no. 3, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. Budzianowski, S. Ultes, P. Su, N. Mrksic, T. Wen, I. Casanueva, L. M.
    Rojas-Barahona, and M. Gasic, “Sub-domain modelling for dialogue management with
    hierarchical reinforcement learning,” in *Annual SIGdial Meeting on Discourse
    and Dialogue*, K. Jokinen, M. Stede, D. DeVault, and A. Louis, Eds.   ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] B. Peng, X. Li, L. Li, J. Gao, A. Çelikyilmaz, S. Lee, and K. Wong, “Composite
    task-completion dialogue policy learning via hierarchical deep reinforcement learning,”
    in *Conference on Empirical Methods in Natural Language Processing EMNLP*, M. Palmer,
    R. Hwa, and S. Riedel, Eds.   ACL, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J. Zhang, T. Zhao, and Z. Yu, “Multimodal hierarchical reinforcement
    learning policy for task-oriented visual dialog,” in *Annual SIGdial Meeting on
    Discourse and Dialogue, Melbourne, Australia, July 12-14, 2018*, K. Komatani,
    D. J. Litman, K. Yu, L. Cavedon, M. Nakano, and A. Papangelis, Eds.   ACL, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] H. Cuayáhuitl, S. Yu, A. Williamson, and J. Carse, “Deep reinforcement
    learning for multi-domain dialogue systems,” *NIPS Workshop on Deep Reinforcement
    Learning*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] G. Gordon-Hall, P. J. Gorinski, and S. B. Cohen, “Learning dialog policies
    from weak demonstrations,” in *Annual Meeting of the Association for Computational
    Linguistics ACL*, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds.   ACL,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] P.-H. Su, M. Gasic, N. Mrkšić, L. M. R. Barahona, S. Ultes, D. Vandyke,
    T.-H. Wen, and S. Young, “On-line active reward learning for policy optimisation
    in spoken dialogue systems,” in *Annual Meeting of the Association for Computational
    Linguistics (ACL)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] S. Ultes, P. Budzianowski, I. Casanueva, N. Mrkšić, L. Rojas-Barahona,
    P.-H. Su, T.-H. Wen, M. Gašić, and S. Young, “Domain-independent user satisfaction
    reward estimation for dialogue policy learning,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] M. Fazel-Zarandi, S.-W. Li, J. Cao, J. Casale, P. Henderson, D. Whitney,
    and A. Geramifard, “Learning robust dialog policies in noisy environments,” *Workshop
    on Conversational AI, NIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] N. Carrara, R. Laroche, and O. Pietquin, “Online learning and transfer
    for user adaptation in dialogue systems,” in *SIGDIAL/SEMDIAL joint special session
    on negotiation dialog 2017*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] N. Carrara, R. Laroche, J.-L. Bouraoui, T. Urvoy, and O. Pietquin, “Safe
    transfer learning for dialogue applications,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] I. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,
    T. Kim, M. Pieper, S. Chandar, N. R. Ke *et al.*, “A deep reinforcement learning
    chatbot,” *arXiv preprint arXiv:1709.02349*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] H. Cuayáhuitl, D. Lee, S. Ryu, Y. Cho, S. Choi, S. R. Indurthi, S. Yu,
    H. Choi, I. Hwang, and J. Kim, “Ensemble-based deep reinforcement learning for
    chatbots,” *Neurocomputing*, vol. 366, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] P. Ammanabrolu and M. Riedl, “Transfer in deep reinforcement learning
    using knowledge graphs,” in *Workshop on Graph-Based Methods for Natural Language
    Processing, TextGraphs@EMNLP*, D. Ustalov, S. Somasundaran, P. Jansen, G. Glavas,
    M. Riedl, M. Surdeanu, and M. Vazirgiannis, Eds.   Association for Computational
    Linguistics, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] I. Casanueva, P. Budzianowski, P. Su, S. Ultes, L. M. Rojas-Barahona,
    B. Tseng, and M. Gasic, “Feudal reinforcement learning for dialogue management
    in large domains,” in *North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (NAACL-HLT)*, M. A. Walker, H. Ji, and
    A. Stent, Eds., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] L. Chen, C. Chang, Z. Chen, B. Tan, M. Gasic, and K. Yu, “Policy adaptation
    for deep reinforcement learning-based dialogue management,” in *IEEE International
    Conference on Acoustics, Speech and Signal ICASSP*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] H. Cuayáhuitl, S. Yu, A. Williamson, and J. Carse, “Scaling up deep reinforcement
    learning for multi-domain dialogue systems,” in *International Joint Conference
    on Neural Networks, IJCNN*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. Das, S. Kottur, J. M. F. Moura, S. Lee, and D. Batra, “Learning cooperative
    visual dialog agents with deep reinforcement learning,” in *IEEE International
    Conference on Computer Vision, ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] M. Fatemi, L. E. Asri, H. Schulz, J. He, and K. Suleman, “Policy networks
    with two-stage training for dialogue systems,” in *Annual Meeting of the Special
    Interest Group on Discourse and Dialogue (SIGDIAL)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] M. Fazel-Zarandi, S. Li, J. Cao, J. Casale, P. Henderson, D. Whitney,
    and A. Geramifard, “Learning robust dialog policies in noisy environments,” *CoRR*,
    vol. abs/1712.04034, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] N. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, À. Lapedriza, N. Jones,
    S. Gu, and R. W. Picard, “Way off-policy batch deep reinforcement learning of
    implicit human preferences in dialog,” *CoRR*, vol. abs/1907.00456, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky, “Deep
    reinforcement learning for dialogue generation,” *CoRR*, vol. abs/1606.01541,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky, “Adversarial
    learning for neural dialogue generation,” in *Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, M. Palmer, R. Hwa, and S. Riedel, Eds.,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Z. C. Lipton, X. Li, J. Gao, L. Li, F. Ahmed, and L. Deng, “Bbq-networks:
    Efficient exploration in deep reinforcement learning for task-oriented dialogue
    systems,” in *AAAI Conference on Artificial Intelligence*, S. A. McIlraith and
    K. Q. Weinberger, Eds., 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] K. Narasimhan, R. Barzilay, and T. S. Jaakkola, “Grounding language for
    transfer in deep reinforcement learning,” *J. Artif. Intell. Res.*, vol. 63, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] B. Peng, X. Li, J. Gao, J. Liu, Y. Chen, and K. Wong, “Adversarial advantage
    actor-critic model for task-completion dialogue policy learning,” in *IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] A. Saleh, N. Jaques, A. Ghandeharioun, J. H. Shen, and R. W. Picard,
    “Hierarchical reinforcement learning for open-domain dialog,” in *AAAI Conference
    on Artificial Intelligence*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] C. Sankar and S. Ravi, “Deep reinforcement learning for modeling chit-chat
    dialog with discrete attributes,” in *SIGdial Meeting on Discourse and Dialogue*,
    S. Nakamura, M. Gasic, I. Zuckerman, G. Skantze, M. Nakano, A. Papangelis, S. Ultes,
    and K. Yoshino, Eds., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] I. V. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian,
    T. Kim, M. Pieper, S. Chandar, N. R. Ke, S. Mudumba, A. de Brébisson, J. Sotelo,
    D. Suhubdy, V. Michalski, A. Nguyen, J. Pineau, and Y. Bengio, “A deep reinforcement
    learning chatbot,” *CoRR*, vol. abs/1709.02349, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] P. Su, P. Budzianowski, S. Ultes, M. Gasic, and S. J. Young, “Sample-efficient
    actor-critic reinforcement learning with supervised data for dialogue management,”
    *CoRR*, vol. abs/1707.00130, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] S. Ultes, P. Budzianowski, I. Casanueva, N. Mrksic, L. M. Rojas-Barahona,
    P. Su, T. Wen, M. Gasic, and S. J. Young, “Domain-independent user satisfaction
    reward estimation for dialogue policy learning,” in *Conference of the International
    Speech Communication Association (INTERSPEECH)*, F. Lacerda, Ed., 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end knowledge-routed
    relational dialogue system for automatic diagnosis,” in *AAAI Conference on Artificial
    Intelligence*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] G. Weisz, P. Budzianowski, P. Su, and M. Gasic, “Sample efficient deep
    reinforcement learning for dialogue systems with large action spaces,” *CoRR*,
    vol. abs/1802.03753, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] J. D. Williams and G. Zweig, “End-to-end lstm-based dialog control optimized
    with supervised and reinforcement learning,” *CoRR*, vol. abs/1606.01269, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] T. Zhao and M. Eskénazi, “Towards end-to-end learning for dialog state
    tracking and management using deep reinforcement learning,” *CoRR*, vol. abs/1606.02560,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] T. Zhao, K. Xie, and M. Eskénazi, “Rethinking action spaces for reinforcement
    learning in end-to-end dialog agents with latent variable models,” in *Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT)*, J. Burstein, C. Doran, and T. Solorio,
    Eds., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] R. Takanobu, H. Zhu, and M. Huang, “Guided dialog policy learning: Reward
    estimation for multi-domain task-oriented dialog,” in *Proceedings of the 2019
    Conference on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong,
    China, November 3-7, 2019*, K. Inui, J. Jiang, V. Ng, and X. Wan, Eds., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] S. Latif, J. Qadir, and M. Bilal, “Unsupervised adversarial domain adaptation
    for cross-lingual speech emotion recognition,” in *International Conference on
    Affective Computing and Intelligent Interaction (ACII)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Z. Wang, S. Ho, and E. Cambria, “A review of emotion sensing: Categorization
    models and algorithms,” *Multimedia Tools and Applications*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Y. Ma, K. L. Nguyen, F. Xing, and E. Cambria, “A survey on empathetic
    dialogue systems,” *Information Fusion*, vol. 64, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh, and E. Cambria,
    “DialogueRNN: An attentive RNN for emotion detection in conversations,” in *AAAI
    Conference on Artificial Intelligence*, vol. 33, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. Poria, N. Majumder, R. Mihalcea, and E. Hovy, “Emotion recognition
    in conversation: Research challenges, datasets, and recent advances,” *IEEE Access*,
    vol. 7, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] T. Young, V. Pandelea, S. Poria, and E. Cambria, “Dialogue systems with
    audio context,” *Neurocomputing*, vol. 388, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] H. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu, “Emotional chatting
    machine: Emotional conversation generation with internal and external memory,”
    in *AAAI Conference on Artificial Intelligence*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] V. Heusser, N. Freymuth, S. Constantin, and A. Waibel, “Bimodal speech
    emotion recognition using pre-trained language models,” *arXiv preprint arXiv:1912.02610*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] X. Ouyang, S. Nagisetty, E. G. H. Goh, S. Shen, W. Ding, H. Ming, and
    D.-Y. Huang, “Audio-visual emotion recognition with capsule-like feature representation
    and model-based reinforcement learning,” in *2018 First Asian Conference on Affective
    Computing and Intelligent Interaction (ACII Asia)*.   IEEE, 2018, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] E. Lakomkin, M. A. Zamani, C. Weber, S. Magg, and S. Wermter, “Emorl:
    continuous acoustic emotion classification using deep reinforcement learning,”
    in *IEEE International Conference on Robotics and Automation (ICRA)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] J. Sangeetha and T. Jayasankar, “Emotion speech recognition based on
    adaptive fractional deep belief network and reinforcement learning,” in *Cognitive
    Informatics and Soft Computing*.   Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] M. Chen, S. Wang, P. P. Liang, T. Baltrušaitis, A. Zadeh, and L.-P. Morency,
    “Multimodal sentiment analysis with word-level fusion and reinforcement learning,”
    in *ACM International Conference on Multimodal Interaction*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] J. Li, L. Deng, R. Haeb-Umbach, and Y. Gong, *Robust automatic speech
    recognition: a bridge to practical applications*.   Academic Press, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] B. Li, Y. Tsao, and K. C. Sim, “An investigation of spectral restoration
    algorithms for deep neural networks based noise robust speech recognition.” in
    *Interspeech*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Z.-Q. Wang and D. Wang, “A joint training framework for robust automatic
    speech recognition,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    vol. 24, no. 4, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] D. Baby, J. F. Gemmeke, T. Virtanen *et al.*, “Exemplar-based speech
    enhancement for deep neural network based automatic speech recognition,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] D. Wang and J. Chen, “Supervised speech separation based on deep learning:
    An overview,” *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    vol. 26, no. 10, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Y.-L. Shen, C.-Y. Huang, S.-S. Wang, Y. Tsao, H.-M. Wang, and T.-S. Chi,
    “Reinforcement learning based speech enhancement for robust speech recognition,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Y. Koizumi, K. Niwa, Y. Hioka, K. Kobayashi, and Y. Haneda, “DNN-based
    source enhancement self-optimized by reinforcement learning using sound quality
    measurements,” in *International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] I.-T. Recommendation, “Perceptual evaluation of speech quality (PESQ):
    An objective method for end-to-end speech quality assessment of narrow-band telephone
    networks and speech codecs,” *Rec. ITU-T P. 862*, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] V. Emiya, E. Vincent, N. Harlander, and V. Hohmann, “Subjective and objective
    quality assessment of audio source separation,” *IEEE Transactions on Audio, Speech,
    and Language Processing*, vol. 19, no. 7, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] R. Fakoor, X. He, I. Tashev, and S. Zarar, “Reinforcement learning to
    adapt speech enhancement to instantaneous input signal quality,” *Machine Learning
    for Audio Signal Processing workshop, NIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] N. Alamdari, E. Lobarinas, and N. Kehtarnavaz, “Personalization of hearing
    aid compression by human-in-the-loop deep reinforcement learning,” *IEEE Access*,
    vol. 8, pp. 203 503–203 515, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] M. J. Steedman, “A generative grammar for jazz chord sequences,” *Music
    Perception: An Interdisciplinary Journal*, vol. 2, no. 1, 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] K. Ebcioğlu, “An expert system for harmonizing four-part chorales,” *Computer
    Music Journal*, vol. 12, no. 3, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] N. Jaques, S. Gu, R. E. Turner, and D. Eck, “Generating music by fine-tuning
    recurrent neural networks with reinforcement learning,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] N. Kotecha, “Bach2Bach: Generating music using a deep reinforcement learning
    approach,” *arXiv preprint arXiv:1812.01060*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] N. Jiang, S. Jin, Z. Duan, and C. Zhang, “Rl-duet: Online music accompaniment
    generation using deep reinforcement learning,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 01, 2020, pp. 710–718.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-dimensional
    continuous control using generalized advantage estimation,” *International Conference
    on Learning Representations (ICLR)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] G. L. Guimaraes, B. Sanchez-Lengeling, C. Outeiral, P. L. C. Farias,
    and A. Aspuru-Guzik, “Objective-reinforced generative adversarial networks (organ)
    for sequence generation models,” *arXiv preprint arXiv:1705.10843*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] S.-g. Lee, U. Hwang, S. Min, and S. Yoon, “Polyphonic music generation
    with sequence generative adversarial networks,” *arXiv preprint arXiv:1710.11418*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Q. Lan, J. Tørresen, and A. R. Jensenius, “RaveForce: A deep reinforcement
    learning environment for music,” in *Proc. of the SMC Conferences*.   Society
    for Sound and Music Computing, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] M. Dorfer, F. Henkel, and G. Widmer, “Learning to listen, read, and follow:
    Score following as a reinforcement learning game,” *International Society for
    Music Information Retrieval Conference*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] F. Henkel, S. Balke, M. Dorfer, and G. Widmer, “Score following as a
    multi-modal reinforcement learning problem,” *Transactions of the International
    Society for Music Information Retrieval*, vol. 2, no. 1, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] N. Howard and E. Cambria, “Intention awareness: Improving upon situation
    awareness in human-centric environments,” *Human-centric Computing and Information
    Sciences*, vol. 3, no. 9, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] M. A. Goodrich and A. C. Schultz, “Human-robot interaction: a survey,”
    *Foundations and trends in human-computer interaction*, vol. 1, no. 3, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] S. Gao, W. Hou, T. Tanaka, and T. Shinozaki, “Spoken language acquisition
    based on reinforcement learning and word unit segmentation,” in *International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] B. F. Skinner, “Verbal behavior. new york: appleton-century-crofts,”
    *Richard-Amato, P.(1996)*, vol. 11, 1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] H. Yu, H. Zhang, and W. Xu, “Interactive grounded language acquisition
    and generalization in a 2D world,” in *International Conference on Learning Representations*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] A. Sinha, B. Akilesh, M. Sarkar, and B. Krishnamurthy, “Attention based
    natural language grounding by navigating virtual environment,” in *IEEE Winter
    Conference on Applications of Computer Vision (WACV)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] K. M. Hermann, F. Hill, S. Green, F. Wang, R. Faulkner, H. Soyer, D. Szepesvari,
    W. M. Czarnecki, M. Jaderberg, D. Teplyashin *et al.*, “Grounded language learning
    in a simulated 3D world,” *arXiv preprint arXiv:1706.06551*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] F. Hill, K. M. Hermann, P. Blunsom, and S. Clark, “Understanding grounded
    language learning agents,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] S. Lathuilière, B. Massé, P. Mesejo, and R. Horaud, “Neural network based
    reinforcement learning for audio–visual gaze control in human–robot interaction,”
    *Pattern Recognition Letters*, vol. 118, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] ——, “Deep reinforcement learning for audio-visual gaze control,” in *IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] M. Clark-Turner and M. Begum, “Deep reinforcement learning of abstract
    reasoning from demonstrations,” in *ACM/IEEE International Conference on Human-Robot
    Interaction*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] N. Hussain, E. Erzin, T. M. Sezgin, and Y. Yemez, “Speech driven backchannel
    generation using deep q-network for enhancing engagement in human-robot interaction,”
    in *Interspeech*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] ——, “Batch recurrent Q-learning for backchannel generation towards engaging
    agents,” in *International Conference on Affective Computing and Intelligent Interaction
    (ACII)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] H. Bui and N. Y. Chong, “Autonomous speech volume control for social
    robots in a noisy environment using deep reinforcement learning,” in *IEEE International
    Conference on Robotics and Biomimetics (ROBIO)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] M. Zamani, S. Magg, C. Weber, S. Wermter, and D. Fu, “Deep reinforcement
    learning using compositional representations for performing instructions,” *Paladyn
    J. Behav. Robotics*, vol. 9, no. 1, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] I. Moreira, J. Rivas, F. Cruz, R. Dazeley, A. Ayala, and B. J. T. Fernandes,
    “Deep reinforcement learning with interactive feedback in a human-robot environment,”
    *CoRR*, vol. abs/2007.03363, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] T. Fryen, M. Eppe, P. D. H. Nguyen, T. Gerkmann, and S. Wermter, “Reinforcement
    learning with time-dependent goals for robotic musicians,” *CoRR*, vol. abs/2011.05715,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling, “The Arcade learning
    environment: An evaluation platform for general agents,” *J. Artif. Intell. Res.*,
    vol. 47, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” in *International Conference on Machine Learning
    (ICML)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep reinforcement
    learning in a handful of trials using probabilistic dynamics models,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] J. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee, “Sample-efficient
    reinforcement learning with stochastic ensemble value expansion,” in *Advances
    in Neural Information Processing Systems (NIPS)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick,
    R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell, “Policy distillation,” *arXiv
    preprint arXiv:1511.06295*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural networks,” *NIPS
    Deep Learning Symposium recommendation*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] X. Li, L. Li, J. Gao, X. He, J. Chen, L. Deng, and J. He, “Recurrent
    reinforcement learning: a hybrid approach,” *arXiv preprint arXiv:1509.03044*,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver,
    and K. Kavukcuoglu, “Reinforcement learning with unsupervised auxiliary tasks,”
    *International Conference on Learning Representations (ICLR)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] H. Yin and S. J. Pan, “Knowledge transfer for deep reinforcement learning
    with hierarchical experience replay,” in *AAAI Conference*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement learning
    for multiagent systems: A review of challenges, solutions, and applications,”
    *IEEE transactions on cybernetics*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] R. Glatt, F. L. Da Silva, and A. H. R. Costa, “Towards knowledge transfer
    in deep reinforcement learning,” in *Brazilian Conference on Intelligent Systems
    (BRACIS)*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] K. Mo, Y. Zhang, S. Li, J. Li, and Q. Yang, “Personalizing a dialogue
    system with transfer reinforcement learning,” in *AAAI Conference*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] M. L. Littman, “Markov games as a framework for multi-agent reinforcement
    learning,” in *Machine learning proceedings 1994*.   Elsevier, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique
    of multiagent deep reinforcement learning,” *Autonomous Agents and Multi-Agent
    Systems*, vol. 33, no. 6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
