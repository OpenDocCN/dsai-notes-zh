- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: æœªåˆ†ç±»'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:40:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.00241] When Deep Learning Meets Polyhedral Theory: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: æ¥æºï¼š[https://ar5iv.labs.arxiv.org/html/2305.00241](https://ar5iv.labs.arxiv.org/html/2305.00241)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'When Deep Learning Meets Polyhedral Theory: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Joey Huchette
  prefs: []
  type: TYPE_NORMAL
- en: Google Research, USA â€ƒâ€ƒ Gonzalo MuÃ±oz
  prefs: []
  type: TYPE_NORMAL
- en: Universidad de Oâ€™Higgins, Chile â€ƒâ€ƒ Thiago Serra
  prefs: []
  type: TYPE_NORMAL
- en: Bucknell University, USA â€ƒâ€ƒ Calvin Tsay
  prefs: []
  type: TYPE_NORMAL
- en: Imperial College London, UK(September 2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the past decade, deep learning became the prevalent methodology for predictive
    modeling thanks to the remarkable accuracy of deep neural networks in tasks such
    as computer vision and natural language processing. Meanwhile, the structure of
    neural networks converged back to simpler representations based on piecewise constant
    and piecewise linear functions such as the Rectified Linear UnitÂ (ReLU), which
    became the most commonly used type of activation function in neural networks.
    That made certain types of network structure â€”such as the typical fully-connected
    feedforward neural networkâ€” amenable to analysis through polyhedral theory and
    to the application of methodologies such as Linear ProgrammingÂ (LP) and Mixed-Integer
    Linear ProgrammingÂ (MILP) for a variety of purposes. In this paper, we survey
    the main topics emerging from this fast-paced area of work, which brings a fresh
    perspective to understanding neural networks in more detail as well as to applying
    linear optimization techniques to train, verify, and reduce the size of such networks.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning has continuously achieved new landmarks in varied areas of artificial
    intelligence for the past decade. Examples of those areas include predictive tasks
    in computer vision (Krizhevsky etÂ al., [2012](#bib.bib178), Ciresan etÂ al., [2012](#bib.bib60),
    Szegedy etÂ al., [2015](#bib.bib301), He etÂ al., [2016](#bib.bib144), Xie etÂ al.,
    [2020b](#bib.bib343)), natural language processing (Sutskever etÂ al., [2014](#bib.bib299),
    Peters etÂ al., [2018](#bib.bib246), Radford etÂ al., [2018](#bib.bib252), Devlin
    etÂ al., [2019](#bib.bib80)), and speech recognition (Hinton etÂ al., [2012](#bib.bib148),
    Graves and Jaitly, [2014](#bib.bib130), Park etÂ al., [2019](#bib.bib240)). The
    artificial neural networks behind such feats are being used in many applications,
    and there is a growing interest for analytical insights to help design such networks
    and then to leverage the model that they have learned. For the most commonly used
    types of neural networks, some of those results and methods are coming from operations
    research tools such as polyhedral theory and associated optimization techniques
    such as Linear ProgrammingÂ (LP) and Mixed-Integer Linear ProgrammingÂ (MILP). Among
    other things, these connections with mathematical optimization may help us understand
    what neural networks can represent, how to train them, and how to make them more
    compact. For example, consider the popular task of classifying images (Figure
    [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")); polyhedral theory and associated optimization techniques may help
    us answer questions such as the following. How should we train the classifier
    model? How large should it be? How robust to perturbations is it?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17c4f1fa42e56b26a18a8a37fef55a33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example classification task on the MNIST database of handwritten
    digits, in which the image of a handwritten digit is given as input and the probability
    of that digit being from each possible class is provided as output.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What neural networks can model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can essentially think of artificial neural networks as functions mapping
    an input ${\bm{x}}$ from a given domain to an output ${\bm{y}}$ for a given application.
    For the classification task in Figure [1](#S1.F1 "Figure 1 â€£ 1 Introduction â€£
    When Deep Learning Meets Polyhedral Theory: A Survey"), inputs ${\bm{x}}$ correspond
    to images from the dataset, and ${\bm{y}}$ to the associated predicted labels,
    or probabilities for labels describing the content of those images. The basic
    units of neural networks mimic biological neurons in that they receive inputs
    from adjacent units, transform those inputs, and may produce an output to subsequent
    units of the network. In other words, every unit is also a function, and in fact
    the output of most units is defined by the composition of a nonlinear function
    with a linear function. The nonlinear function is often denoted as the *activation
    function* in analogy to how a biological neuron is triggered to send a signal
    to adjacent neurons when the stimulus caused by the input exceeds a certain activation
    threshold. Such non-linearity is behind the remarkable expressiveness of neural
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: This model was pioneered byÂ McCulloch and Pitts ([1943](#bib.bib217)), who considered
    a thresholding function for activation that is now often denoted as the Linear
    Threshold UnitÂ (LTU). That activation is also the basis of the classic *perceptron*
    algorithm by Â Rosenblatt ([1957](#bib.bib261)), which yields a binary classifier
    of the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f({\bm{x}})=\left\{\begin{array}[]{cl}1&amp;\text{if }{\bm{w}}\cdot{\bm{x}}+b>0;\\
    0&amp;\text{otherwise}\end{array}\right.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'for an input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and with parameters ${\bm{w}}\in\mathbb{R}^{n_{0}}$
    and $b\in\mathbb{R}$. Those parameters are chosen by optimizing the predictions
    for a given task, as discussed below and in Section [5](#S5 "5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey"). The term *single-layer perceptron* is used for a neural network consisting
    of a set of such units processing the same input in parallel. The term *multi-layer
    perceptron* is used for a generalization of this concept, by which the output
    of a *layer* â€”a set of units with same inputâ€” is the input for a subsequent layer.
    This perceptron terminology has also been loosely applied to neural networks with
    other activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: More generally, neural networks that successively transform inputs through an
    ordered sequence of layers are also denoted *feedforward networks*. The layers
    that do not produce the final output of the neural network are denoted *hidden
    layers*. For a network with $L$ layers, we denote $n_{l}$ as the number of units
    â€”or *width*â€” of layer $l\in{\mathbb{L}}:=\{1,2,\ldots,L\}$ and $h_{i}^{l}$ as
    the output of the $i$-th unit in layer $l$, where $i\in\{1,2,\ldots,n_{l}\}$.
    The output of a unit is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h_{i}^{l}=\sigma^{l}\left({\bm{w}}^{l}_{i}\cdot{\bm{h}}^{l-1}+b^{l}_{i}\right),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where the *weights* ${\bm{w}}^{l}_{i}\in\mathbb{R}^{n_{l-1}}$ and the *bias*
    $b^{l}_{i}\in\mathbb{R}$ are parameters of the unit. Those parameters can be aggregated
    across the layer as the matrix ${\bm{W}}^{l}\in\mathbb{R}^{n_{l}\times n_{l-1}}$
    and the vector ${\bm{b}}^{l}\in\mathbb{R}^{n_{l}}$. The vector ${\bm{h}}^{l-1}\in\mathbb{R}^{n_{l-1}}$
    represents the aggregated outputs from layer $(l-1)$. The activation function
    $\sigma^{l}:\mathbb{R}\rightarrow\mathbb{R}$ is applied by the units in layer
    $l$. These definitions implicitly assume that $n_{0}$ is the size of the network
    input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and that ${\bm{h}}^{0}$ and ${\bm{x}}$ are
    the same. FigureÂ [2](#S1.F2 "Figure 2 â€£ 1.1 What neural networks can model â€£ 1
    Introduction â€£ When Deep Learning Meets Polyhedral Theory: A Survey") illustrates
    the operation of a feedforward network as described above.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2db8fd387b48da880f36dff11bad8a69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mapping from ${\bm{x}}\in\mathbb{R}^{n_{0}}$ to ${\bm{y}}\in\mathbb{R}^{n_{L}}$
    through a feedforward neural network with $L$ layers, layer widths $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and activation functions $\{\sigma_{l}\}_{l\in{\mathbb{L}}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 How neural networks are obtained and evaluated
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In resemblance to how other models for *supervised learning* in machine learning
    are obtained, we can *train* a neural network for a given task by adjusting its
    behavior with respect to the examples of a *training set* and then evaluate the
    final trained network on a *test set*. Both of these sets consist of inputs for
    which the correct output $\hat{y}$ is known. We can define an objective function
    to model a measure of distance between the output $y$ and the correct output $\hat{y}$,
    which is typically denoted as the *loss function*, and then iteratively update
    parameters such as $\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}}$ and $\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}$
    to minimize that loss function over the training set. A common objective function
    is the square error $\|y-\hat{y}\|^{2}$ summed over the points in the training
    set. The test set contains a separate collection of inputs and their outputs,
    which is used to evaluate the trained neural network with examples that were not
    seen during training. A good performance on the test set may indicate that the
    trained neural network is able to *generalize* beyond the seen examples, whereas
    a bad performance may suggest that it *overfits* for the training set. Neural
    networks also have *hyperparameters* that are often chosen manually and do not
    change during training, such as the *depth* $L$, the widths of the layers $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and the activation functions used in each layer $\{\sigma^{l}\}_{l\in{\mathbb{L}}}$.
    Different models can be produced by varying the hyperparameters. In such a case,
    a *validation set* disjoint from the training and test sets can be used to compare
    models with different hyperparameters. Whereas the validation set may serve as
    a benchmark to different trained models corresponding to different choices of
    hyperparameters, the test set can only be used to evaluate a single neural network
    chosen among those evaluated with the validation set. The emergent field of *neural
    architecture search* â€”recently surveyed by Elsken etÂ al. ([2019](#bib.bib89))â€”
    concerns with automatically choosing such hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key factors for the success of deep learning is that first-order
    methods for continuous optimization can be effectively applied to train deep networks.
    The interest in neural networks first vanished due to negative results in the
    Perceptrons book byÂ Minsky and Papert ([1969](#bib.bib219)), which showed that
    single-layer perceptrons cannot represent functions such as the Boolean XOR. However,
    moving to multi-layer perceptrons capable of expressing the Boolean XOR as well
    as other more expressive models would require a clever training strategy. Hence,
    the interest was regained with papers that popularized the use of *backpropagation*,
    such as Â Rumelhart etÂ al. ([1986](#bib.bib266)) and Â LeCun etÂ al. ([1989](#bib.bib185)).
    Note that backpropagation was first discussed much earlier in the context of networks
    byÂ Linnainmaa ([1970](#bib.bib194)) and of neural networks explicitly byÂ Werbos
    ([1974](#bib.bib331)). The backpropagation algorithm calculates the derivative
    of the loss function with respect to each neural network parameter by applying
    the chain rule through the units of the neural network, which is considerably
    more efficient than explicitly evaluating the derivative of each network parameter.
    Consequently, neural networks are generally trained with gradient descent methods
    in which the parameters are updated sequentially from the output to the input
    layer in each step. In fact, most algorithms for training neural networks are
    based on Stochastic Gradient DescentÂ (SGD), which is a form of the stochastic
    approximation through sampling pioneered byÂ Robbins and Monro ([1951](#bib.bib258)).
    SGD approximates the partial derivatives of the loss function at each step by
    using only a subset of the data in order to make the training process more efficient.
    Examples of popular SGD algorithms include momentum (Polyak, [1964](#bib.bib250)),
    Adam (Kingma and Ba, [2014](#bib.bib175)), and Nesterov Adaptive Gradient (Sutskever
    etÂ al., [2013](#bib.bib298)) â€”the later inspired byÂ Nesterov ([1983](#bib.bib231)).
    Interestingly, however, we generally cannot guarantee convergence to a global
    optimum with gradient descent due to the nonconvexity of the loss function. Nevertheless,
    neural networks trained with adequately parameterized SGD algorithms tend to generalize
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Why nonlinearity is important in artificial neurons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The nonlinearity of the activation function leads to such nonconvexity of the
    loss function. However, as we will see in SectionÂ [3](#S3 "3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"),
    that same nonlinearity enables the neural network to represent more complex functions
    as a whole. In fact, removing such nonlinearities by using an identity activation
    function $\sigma^{l}(u)=u~{}\forall l\in{\mathbb{L}}$ would reduce the entire
    neural network to an affine transformation of the form $f(x)={\bm{W}}^{L}({\bm{W}}^{L-1}\left(\ldots\left({\bm{W}}^{2}\left({\bm{W}}^{1}x+{\bm{b}}^{1}\right)+{\bm{b}}^{2}\right)+\ldots\right)+{\bm{b}}^{L-1})+{\bm{b}}^{L}$.
    Hence, a feedforward network without nonlinear activation functions is equivalent
    to a linear regression model. However, in that case we can easily obtain such
    a model without resorting to neural networks and backpropagation: the loss function
    is convex and the optimal solution is given by a closed formula, such as in least
    squares regression. In contrast, neural networks with a single hidden layer of
    arbitrary width have been long known to be universal function approximators for
    a broad variety of activation functions (Cybenko, [1989](#bib.bib71), Funahashi,
    [1989](#bib.bib112), Hornik etÂ al., [1989](#bib.bib153)), as well as for ReLU
    more recently (Yarotsky, [2017](#bib.bib350)). These results have also been extended
    to the converse case of limited width but arbitrarily large depth (Lu etÂ al.,
    [2017](#bib.bib202), Hanin and Sellke, [2017](#bib.bib139), Park etÂ al., [2021a](#bib.bib241)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although nonlinear activation functions are important for obtaining more complex
    models, these functions do not need to be overly complex to produce good results.
    In the past, it was common practice to use sigmoid functions for activation (LeCun
    etÂ al., [1998](#bib.bib186)). Those are monotonically increasing functions that
    approach finite values for arbitrarily large positive and negative inputs, such
    as the standard logistic function $\sigma(u)=\frac{1}{1+e^{-u}}$ and the hyperbolic
    tangent $\sigma(u)=\tanh(u)$. In the present, the most commonly used activation
    function is the Rectified Linear UnitÂ (ReLU) $\sigma(u)=\max\{0,u\}$ (LeCun etÂ al.,
    [2015](#bib.bib187), Ramachandran etÂ al., [2018](#bib.bib255)), which was proposed
    byÂ Hahnloser etÂ al. ([2000](#bib.bib135)) and first applied to neural networks
    byÂ Nair and Hinton ([2010](#bib.bib228)). The popularity of ReLU is in part due
    to experiments byÂ Nair and Hinton ([2010](#bib.bib228)) andÂ Glorot etÂ al. ([2011](#bib.bib119))
    showing that this simpler form of activation yields competitive results. Thinking
    back in terms of the analogy with biological neurons, we say that a ReLU is *active*
    when the output is positive and *inactive* when the output is zero. ReLUs have
    a linear output behavior on the inputs associated with the same ReLUs being active
    and inactive; this property also holds for other piecewise linear and piecewise
    constant functions that are used as activation functions in neural networks. TableÂ [1](#S1.T1
    "Table 1 â€£ 1.3 Why nonlinearity is important in artificial neurons â€£ 1 Introduction
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") lists some of the most
    commonly used activation functions of that kind. For more comprehensive lists
    of activation functions, including several other variations based on ReLU, we
    refer toÂ Dubey etÂ al. ([2021](#bib.bib82)) andÂ Tao etÂ al. ([2022](#bib.bib303)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Main piecewise constant and piecewise linear activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Function | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LTU | $\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if }u>0\\ 0&amp;\text{if
    }u\leq 0\end{array}\right.$ | McCulloch and Pitts ([1943](#bib.bib217)) |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU | $\sigma(u)=\max\{0,u\}$ | Hahnloser etÂ al. ([2000](#bib.bib135)),
    Nair and Hinton ([2010](#bib.bib228)) |'
  prefs: []
  type: TYPE_TB
- en: '| leaky ReLU | <math id="S1.T1.3.3.1.m1.3" class="ltx_Math" alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ \varepsilon u&amp;\text{if }u\leq 0\end{array}\right.\\'
  prefs: []
  type: TYPE_NORMAL
- en: \text{($\varepsilon$ is small and fixed)}\end{array}" display="inline"><semantics
    id="S1.T1.3.3.1.m1.3a"><mtable rowspacing="0pt" id="S1.T1.3.3.1.m1.3.3" xref="S1.T1.3.3.1.m1.3.3.cmml"><mtr
    id="S1.T1.3.3.1.m1.3.3a" xref="S1.T1.3.3.1.m1.3.3.cmml"><mtd id="S1.T1.3.3.1.m1.3.3b"
    xref="S1.T1.3.3.1.m1.3.3.cmml"><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.4" xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml"><mi id="S1.T1.3.3.1.m1.3.3.3.2.2.4.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S1.T1.3.3.1.m1.3.3.3.2.2.4.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.1.cmml">â€‹</mo><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2.4.3.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml"><mo stretchy="false" id="S1.T1.3.3.1.m1.3.3.3.2.2.4.3.2.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml">(</mo><mi id="S1.T1.3.3.1.m1.2.2.2.1.1.1"
    xref="S1.T1.3.3.1.m1.2.2.2.1.1.1.cmml">u</mi><mo stretchy="false" id="S1.T1.3.3.1.m1.3.3.3.2.2.4.3.2.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml">)</mo></mrow></mrow><mo id="S1.T1.3.3.1.m1.3.3.3.2.2.3"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.3.cmml">=</mo><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2.5.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.cmml"><mo id="S1.T1.3.3.1.m1.3.3.3.2.2.5.2.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt"
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mtr id="S1.T1.3.3.1.m1.3.3.3.2.2.2a"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.3.3.1.m1.3.3.3.2.2.2b" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1.cmml">u</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.3.3.1.m1.3.3.3.2.2.2c" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"><mtext
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml">ifÂ </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.1.cmml">></mo><mn
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3.cmml">0</mn></mrow></mtd></mtr><mtr
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2d" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.3.3.1.m1.3.3.3.2.2.2e"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.cmml"><mi id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2.cmml">Îµ</mi><mo lspace="0em" rspace="0em"
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.1.cmml">â€‹</mo><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3.cmml">u</mi></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.3.3.1.m1.3.3.3.2.2.2f" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.cmml"><mtext
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml">ifÂ </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.1.cmml">â‰¤</mo><mn
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3.cmml">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    id="S1.T1.3.3.1.m1.3.3c" xref="S1.T1.3.3.1.m1.3.3.cmml"><mtd id="S1.T1.3.3.1.m1.3.3d"
    xref="S1.T1.3.3.1.m1.3.3.cmml"><mrow id="S1.T1.3.3.1.m1.1.1.1.1.1" xref="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml"><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1a" xref="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml">(</mtext><mi
    id="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1" xref="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml">Îµ</mi><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1b" xref="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml">Â is small
    and fixed)</mtext></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    id="S1.T1.3.3.1.m1.3b"><matrix id="S1.T1.3.3.1.m1.3.3.cmml" xref="S1.T1.3.3.1.m1.3.3"><matrixrow
    id="S1.T1.3.3.1.m1.3.3a.cmml" xref="S1.T1.3.3.1.m1.3.3"><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2"><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.4"><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.4.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.2">ğœ</ci><ci
    id="S1.T1.3.3.1.m1.2.2.2.1.1.1.cmml" xref="S1.T1.3.3.1.m1.2.2.2.1.1.1">ğ‘¢</ci></apply><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.2"><csymbol
    cd="latexml" id="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.2.1">cases</csymbol><matrix
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2"><matrixrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2a.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2"><ci id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1">ğ‘¢</ci><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1"><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2"><ci id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2"><mtext id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2">ifÂ </mtext></ci><ci id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3">ğ‘¢</ci></apply><cn type="integer" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3">0</cn></apply></matrixrow><matrixrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2b.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2"><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1"><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2">ğœ€</ci><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3">ğ‘¢</ci></apply><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1"><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2"><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2"><mtext
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2">ifÂ </mtext></ci><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3">ğ‘¢</ci></apply><cn
    type="integer" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3">0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    id="S1.T1.3.3.1.m1.3.3b.cmml" xref="S1.T1.3.3.1.m1.3.3"><ci id="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml"
    xref="S1.T1.3.3.1.m1.1.1.1.1.1"><mrow id="S1.T1.3.3.1.m1.1.1.1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1"><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1a.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1">(</mtext><mi
    id="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1">Îµ</mi><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1b.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1">Â is small
    and fixed)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.3.3.1.m1.3c">\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ \varepsilon u&\text{if }u\leq 0\end{array}\right.\\ \text{($\varepsilon$
    is small and fixed)}\end{array}</annotation></semantics></math> | Maas etÂ al.
    ([2013](#bib.bib205)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| parametric ReLU | <math id="S1.T1.4.4.1.m1.3" class="ltx_Math" alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ au&amp;\text{if }u\leq 0\end{array}\right.\\'
  prefs: []
  type: TYPE_NORMAL
- en: \text{($a$ is a trainable parameter)}\end{array}" display="inline"><semantics
    id="S1.T1.4.4.1.m1.3a"><mtable rowspacing="0pt" id="S1.T1.4.4.1.m1.3.3" xref="S1.T1.4.4.1.m1.3.3.cmml"><mtr
    id="S1.T1.4.4.1.m1.3.3a" xref="S1.T1.4.4.1.m1.3.3.cmml"><mtd id="S1.T1.4.4.1.m1.3.3b"
    xref="S1.T1.4.4.1.m1.3.3.cmml"><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.4" xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml"><mi id="S1.T1.4.4.1.m1.3.3.3.2.2.4.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S1.T1.4.4.1.m1.3.3.3.2.2.4.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.1.cmml">â€‹</mo><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2.4.3.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml"><mo stretchy="false" id="S1.T1.4.4.1.m1.3.3.3.2.2.4.3.2.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml">(</mo><mi id="S1.T1.4.4.1.m1.2.2.2.1.1.1"
    xref="S1.T1.4.4.1.m1.2.2.2.1.1.1.cmml">u</mi><mo stretchy="false" id="S1.T1.4.4.1.m1.3.3.3.2.2.4.3.2.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml">)</mo></mrow></mrow><mo id="S1.T1.4.4.1.m1.3.3.3.2.2.3"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.3.cmml">=</mo><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2.5.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.cmml"><mo id="S1.T1.4.4.1.m1.3.3.3.2.2.5.2.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt"
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mtr id="S1.T1.4.4.1.m1.3.3.3.2.2.2a"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.4.4.1.m1.3.3.3.2.2.2b" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1.cmml">u</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.4.4.1.m1.3.3.3.2.2.2c" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"><mtext
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml">ifÂ </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.1.cmml">></mo><mn
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3.cmml">0</mn></mrow></mtd></mtr><mtr
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2d" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.4.4.1.m1.3.3.3.2.2.2e"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.cmml"><mi id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em"
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.1.cmml">â€‹</mo><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3.cmml">u</mi></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.4.4.1.m1.3.3.3.2.2.2f" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.cmml"><mtext
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml">ifÂ </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.1.cmml">â‰¤</mo><mn
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3.cmml">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    id="S1.T1.4.4.1.m1.3.3c" xref="S1.T1.4.4.1.m1.3.3.cmml"><mtd id="S1.T1.4.4.1.m1.3.3d"
    xref="S1.T1.4.4.1.m1.3.3.cmml"><mrow id="S1.T1.4.4.1.m1.1.1.1.1.1" xref="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml"><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1a" xref="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml">(</mtext><mi
    id="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml">a</mi><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1b" xref="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml">Â is a trainable
    parameter)</mtext></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    id="S1.T1.4.4.1.m1.3b"><matrix id="S1.T1.4.4.1.m1.3.3.cmml" xref="S1.T1.4.4.1.m1.3.3"><matrixrow
    id="S1.T1.4.4.1.m1.3.3a.cmml" xref="S1.T1.4.4.1.m1.3.3"><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2"><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.4"><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.4.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.2">ğœ</ci><ci
    id="S1.T1.4.4.1.m1.2.2.2.1.1.1.cmml" xref="S1.T1.4.4.1.m1.2.2.2.1.1.1">ğ‘¢</ci></apply><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.2"><csymbol
    cd="latexml" id="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.2.1">cases</csymbol><matrix
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2"><matrixrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2a.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2"><ci id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1">ğ‘¢</ci><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1"><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2"><ci id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2"><mtext id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2">ifÂ </mtext></ci><ci id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3">ğ‘¢</ci></apply><cn type="integer" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3">0</cn></apply></matrixrow><matrixrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2b.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2"><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1"><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2">ğ‘</ci><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3">ğ‘¢</ci></apply><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1"><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2"><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2"><mtext
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2">ifÂ </mtext></ci><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3">ğ‘¢</ci></apply><cn
    type="integer" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3">0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    id="S1.T1.4.4.1.m1.3.3b.cmml" xref="S1.T1.4.4.1.m1.3.3"><ci id="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml"
    xref="S1.T1.4.4.1.m1.1.1.1.1.1"><mrow id="S1.T1.4.4.1.m1.1.1.1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1"><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1a.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1">(</mtext><mi
    id="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1">a</mi><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1b.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1">Â is a trainable
    parameter)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.4.4.1.m1.3c">\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ au&\text{if }u\leq 0\end{array}\right.\\ \text{($a$ is a trainable parameter)}\end{array}</annotation></semantics></math>
    | He etÂ al. ([2015](#bib.bib143)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| hard tanh | <math id="S1.T1.5.5.1.m1.2" class="ltx_Math" alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>1\\ u&amp;\text{if }-1\leq u\leq 1\\'
  prefs: []
  type: TYPE_NORMAL
- en: -1&amp;\text{if }u<-1\end{array}\right." display="inline"><semantics id="S1.T1.5.5.1.m1.2a"><mrow
    id="S1.T1.5.5.1.m1.2.3" xref="S1.T1.5.5.1.m1.2.3.cmml"><mrow id="S1.T1.5.5.1.m1.2.3.2"
    xref="S1.T1.5.5.1.m1.2.3.2.cmml"><mi id="S1.T1.5.5.1.m1.2.3.2.2" xref="S1.T1.5.5.1.m1.2.3.2.2.cmml">Ïƒ</mi><mo
    lspace="0em" rspace="0em" id="S1.T1.5.5.1.m1.2.3.2.1" xref="S1.T1.5.5.1.m1.2.3.2.1.cmml">â€‹</mo><mrow
    id="S1.T1.5.5.1.m1.2.3.2.3.2" xref="S1.T1.5.5.1.m1.2.3.2.cmml"><mo stretchy="false"
    id="S1.T1.5.5.1.m1.2.3.2.3.2.1" xref="S1.T1.5.5.1.m1.2.3.2.cmml">(</mo><mi id="S1.T1.5.5.1.m1.1.1"
    xref="S1.T1.5.5.1.m1.1.1.cmml">u</mi><mo stretchy="false" id="S1.T1.5.5.1.m1.2.3.2.3.2.2"
    xref="S1.T1.5.5.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S1.T1.5.5.1.m1.2.3.1"
    xref="S1.T1.5.5.1.m1.2.3.1.cmml">=</mo><mrow id="S1.T1.5.5.1.m1.2.3.3.2" xref="S1.T1.5.5.1.m1.2.3.3.1.cmml"><mo
    id="S1.T1.5.5.1.m1.2.3.3.2.1" xref="S1.T1.5.5.1.m1.2.3.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" id="S1.T1.5.5.1.m1.2.2" xref="S1.T1.5.5.1.m1.2.2.cmml"><mtr
    id="S1.T1.5.5.1.m1.2.2a" xref="S1.T1.5.5.1.m1.2.2.cmml"><mtd id="S1.T1.5.5.1.m1.2.2b"
    xref="S1.T1.5.5.1.m1.2.2.cmml"><mn id="S1.T1.5.5.1.m1.2.2.1.1.1" xref="S1.T1.5.5.1.m1.2.2.1.1.1.cmml">1</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.5.5.1.m1.2.2c" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.1.2.1" xref="S1.T1.5.5.1.m1.2.2.1.2.1.cmml"><mrow id="S1.T1.5.5.1.m1.2.2.1.2.1.2"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.cmml"><mtext id="S1.T1.5.5.1.m1.2.2.1.2.1.2.2"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.5.5.1.m1.2.2.1.2.1.2.1" xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.5.5.1.m1.2.2.1.2.1.2.3" xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.5.5.1.m1.2.2.1.2.1.1" xref="S1.T1.5.5.1.m1.2.2.1.2.1.1.cmml">></mo><mn
    id="S1.T1.5.5.1.m1.2.2.1.2.1.3" xref="S1.T1.5.5.1.m1.2.2.1.2.1.3.cmml">1</mn></mrow></mtd></mtr><mtr
    id="S1.T1.5.5.1.m1.2.2d" xref="S1.T1.5.5.1.m1.2.2.cmml"><mtd id="S1.T1.5.5.1.m1.2.2e"
    xref="S1.T1.5.5.1.m1.2.2.cmml"><mi id="S1.T1.5.5.1.m1.2.2.2.1.1" xref="S1.T1.5.5.1.m1.2.2.2.1.1.cmml">u</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.5.5.1.m1.2.2f" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.2.2.1" xref="S1.T1.5.5.1.m1.2.2.2.2.1.cmml"><mrow id="S1.T1.5.5.1.m1.2.2.2.2.1.2"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.cmml"><mtext id="S1.T1.5.5.1.m1.2.2.2.2.1.2.2"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.2a.cmml">ifÂ </mtext><mo id="S1.T1.5.5.1.m1.2.2.2.2.1.2.1"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.1.cmml">âˆ’</mo><mn id="S1.T1.5.5.1.m1.2.2.2.2.1.2.3"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.3.cmml">1</mn></mrow><mo id="S1.T1.5.5.1.m1.2.2.2.2.1.3"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.3.cmml">â‰¤</mo><mi id="S1.T1.5.5.1.m1.2.2.2.2.1.4"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.4.cmml">u</mi><mo id="S1.T1.5.5.1.m1.2.2.2.2.1.5"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.5.cmml">â‰¤</mo><mn id="S1.T1.5.5.1.m1.2.2.2.2.1.6"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.6.cmml">1</mn></mrow></mtd></mtr><mtr id="S1.T1.5.5.1.m1.2.2g"
    xref="S1.T1.5.5.1.m1.2.2.cmml"><mtd id="S1.T1.5.5.1.m1.2.2h" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.3.1.1" xref="S1.T1.5.5.1.m1.2.2.3.1.1.cmml"><mo id="S1.T1.5.5.1.m1.2.2.3.1.1a"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1.cmml">âˆ’</mo><mn id="S1.T1.5.5.1.m1.2.2.3.1.1.2"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1.2.cmml">1</mn></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S1.T1.5.5.1.m1.2.2i" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.3.2.1" xref="S1.T1.5.5.1.m1.2.2.3.2.1.cmml"><mrow id="S1.T1.5.5.1.m1.2.2.3.2.1.2"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.cmml"><mtext id="S1.T1.5.5.1.m1.2.2.3.2.1.2.2"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.1" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.3" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.5.5.1.m1.2.2.3.2.1.1" xref="S1.T1.5.5.1.m1.2.2.3.2.1.1.cmml"><</mo><mrow
    id="S1.T1.5.5.1.m1.2.2.3.2.1.3" xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.cmml"><mo id="S1.T1.5.5.1.m1.2.2.3.2.1.3a"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.cmml">âˆ’</mo><mn id="S1.T1.5.5.1.m1.2.2.3.2.1.3.2"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.2.cmml">1</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S1.T1.5.5.1.m1.2b"><apply id="S1.T1.5.5.1.m1.2.3.cmml"
    xref="S1.T1.5.5.1.m1.2.3"><apply id="S1.T1.5.5.1.m1.2.3.2.cmml" xref="S1.T1.5.5.1.m1.2.3.2"><ci
    id="S1.T1.5.5.1.m1.2.3.2.2.cmml" xref="S1.T1.5.5.1.m1.2.3.2.2">ğœ</ci><ci id="S1.T1.5.5.1.m1.1.1.cmml"
    xref="S1.T1.5.5.1.m1.1.1">ğ‘¢</ci></apply><apply id="S1.T1.5.5.1.m1.2.3.3.1.cmml"
    xref="S1.T1.5.5.1.m1.2.3.3.2"><csymbol cd="latexml" id="S1.T1.5.5.1.m1.2.3.3.1.1.cmml"
    xref="S1.T1.5.5.1.m1.2.3.3.2.1">cases</csymbol><matrix id="S1.T1.5.5.1.m1.2.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2"><matrixrow id="S1.T1.5.5.1.m1.2.2a.cmml" xref="S1.T1.5.5.1.m1.2.2"><cn
    type="integer" id="S1.T1.5.5.1.m1.2.2.1.1.1.cmml" xref="S1.T1.5.5.1.m1.2.2.1.1.1">1</cn><apply
    id="S1.T1.5.5.1.m1.2.2.1.2.1.cmml" xref="S1.T1.5.5.1.m1.2.2.1.2.1"><apply id="S1.T1.5.5.1.m1.2.2.1.2.1.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2"><ci id="S1.T1.5.5.1.m1.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.2"><mtext id="S1.T1.5.5.1.m1.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.2">ifÂ </mtext></ci><ci id="S1.T1.5.5.1.m1.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.3">ğ‘¢</ci></apply><cn type="integer" id="S1.T1.5.5.1.m1.2.2.1.2.1.3.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.3">1</cn></apply></matrixrow><matrixrow id="S1.T1.5.5.1.m1.2.2b.cmml"
    xref="S1.T1.5.5.1.m1.2.2"><ci id="S1.T1.5.5.1.m1.2.2.2.1.1.cmml" xref="S1.T1.5.5.1.m1.2.2.2.1.1">ğ‘¢</ci><apply
    id="S1.T1.5.5.1.m1.2.2.2.2.1.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1"><apply id="S1.T1.5.5.1.m1.2.2.2.2.1b.cmml"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1"><apply id="S1.T1.5.5.1.m1.2.2.2.2.1.2.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2"><ci
    id="S1.T1.5.5.1.m1.2.2.2.2.1.2.2a.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.2"><mtext
    id="S1.T1.5.5.1.m1.2.2.2.2.1.2.2.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.2">ifÂ </mtext></ci><cn
    type="integer" id="S1.T1.5.5.1.m1.2.2.2.2.1.2.3.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.3">1</cn></apply><ci
    id="S1.T1.5.5.1.m1.2.2.2.2.1.4.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.4">ğ‘¢</ci></apply><apply
    id="S1.T1.5.5.1.m1.2.2.2.2.1c.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1"><cn type="integer"
    id="S1.T1.5.5.1.m1.2.2.2.2.1.6.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.6">1</cn></apply></apply></matrixrow><matrixrow
    id="S1.T1.5.5.1.m1.2.2c.cmml" xref="S1.T1.5.5.1.m1.2.2"><apply id="S1.T1.5.5.1.m1.2.2.3.1.1.cmml"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1"><cn type="integer" id="S1.T1.5.5.1.m1.2.2.3.1.1.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1.2">1</cn></apply><apply id="S1.T1.5.5.1.m1.2.2.3.2.1.cmml"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1"><apply id="S1.T1.5.5.1.m1.2.2.3.2.1.2.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2"><ci
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.2a.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.2"><mtext
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.2.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.2">ifÂ </mtext></ci><ci
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.3.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.3">ğ‘¢</ci></apply><apply
    id="S1.T1.5.5.1.m1.2.2.3.2.1.3.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.3"><cn type="integer"
    id="S1.T1.5.5.1.m1.2.2.3.2.1.3.2.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.2">1</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.5.5.1.m1.2c">\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if
    }u>1\\ u&\text{if }-1\leq u\leq 1\\ -1&\text{if }u<-1\end{array}\right.</annotation></semantics></math>
    | Collobert ([2004](#bib.bib62)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| hard sigmoid | <math id="S1.T1.6.6.1.m1.2" class="ltx_Math" alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>\frac{1}{2}\\ u+\frac{1}{2}&amp;\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{if }u<-\frac{1}{2}\end{array}\right." display="inline"><semantics
    id="S1.T1.6.6.1.m1.2a"><mrow id="S1.T1.6.6.1.m1.2.3" xref="S1.T1.6.6.1.m1.2.3.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.3.2" xref="S1.T1.6.6.1.m1.2.3.2.cmml"><mi id="S1.T1.6.6.1.m1.2.3.2.2"
    xref="S1.T1.6.6.1.m1.2.3.2.2.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S1.T1.6.6.1.m1.2.3.2.1"
    xref="S1.T1.6.6.1.m1.2.3.2.1.cmml">â€‹</mo><mrow id="S1.T1.6.6.1.m1.2.3.2.3.2" xref="S1.T1.6.6.1.m1.2.3.2.cmml"><mo
    stretchy="false" id="S1.T1.6.6.1.m1.2.3.2.3.2.1" xref="S1.T1.6.6.1.m1.2.3.2.cmml">(</mo><mi
    id="S1.T1.6.6.1.m1.1.1" xref="S1.T1.6.6.1.m1.1.1.cmml">u</mi><mo stretchy="false"
    id="S1.T1.6.6.1.m1.2.3.2.3.2.2" xref="S1.T1.6.6.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo
    id="S1.T1.6.6.1.m1.2.3.1" xref="S1.T1.6.6.1.m1.2.3.1.cmml">=</mo><mrow id="S1.T1.6.6.1.m1.2.3.3.2"
    xref="S1.T1.6.6.1.m1.2.3.3.1.cmml"><mo id="S1.T1.6.6.1.m1.2.3.3.2.1" xref="S1.T1.6.6.1.m1.2.3.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" id="S1.T1.6.6.1.m1.2.2" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtr
    id="S1.T1.6.6.1.m1.2.2a" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtd id="S1.T1.6.6.1.m1.2.2b"
    xref="S1.T1.6.6.1.m1.2.2.cmml"><mn id="S1.T1.6.6.1.m1.2.2.1.1.1" xref="S1.T1.6.6.1.m1.2.2.1.1.1.cmml">1</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.6.6.1.m1.2.2c" xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.2.1.2.1" xref="S1.T1.6.6.1.m1.2.2.1.2.1.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.1.2.1.2"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.cmml"><mtext id="S1.T1.6.6.1.m1.2.2.1.2.1.2.2"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.6.6.1.m1.2.2.1.2.1.2.1" xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.6.6.1.m1.2.2.1.2.1.2.3" xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.6.6.1.m1.2.2.1.2.1.1" xref="S1.T1.6.6.1.m1.2.2.1.2.1.1.cmml">></mo><mfrac
    id="S1.T1.6.6.1.m1.2.2.1.2.1.3" xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.cmml"><mn id="S1.T1.6.6.1.m1.2.2.1.2.1.3.2"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.1.2.1.3.3"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.3.cmml">2</mn></mfrac></mrow></mtd></mtr><mtr
    id="S1.T1.6.6.1.m1.2.2d" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtd id="S1.T1.6.6.1.m1.2.2e"
    xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.2.1.1" xref="S1.T1.6.6.1.m1.2.2.2.1.1.cmml"><mi
    id="S1.T1.6.6.1.m1.2.2.2.1.1.2" xref="S1.T1.6.6.1.m1.2.2.2.1.1.2.cmml">u</mi><mo
    id="S1.T1.6.6.1.m1.2.2.2.1.1.1" xref="S1.T1.6.6.1.m1.2.2.2.1.1.1.cmml">+</mo><mfrac
    id="S1.T1.6.6.1.m1.2.2.2.1.1.3" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.cmml"><mn id="S1.T1.6.6.1.m1.2.2.2.1.1.3.2"
    xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.2.1.1.3.3"
    xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.3.cmml">2</mn></mfrac></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S1.T1.6.6.1.m1.2.2f" xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.2.2.2.1" xref="S1.T1.6.6.1.m1.2.2.2.2.1.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.2.2.1.2"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.cmml"><mtext id="S1.T1.6.6.1.m1.2.2.2.2.1.2.2"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.2a.cmml">ifÂ </mtext><mo id="S1.T1.6.6.1.m1.2.2.2.2.1.2.1"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.1.cmml">âˆ’</mo><mfrac id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.cmml"><mn id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3.cmml">2</mn></mfrac></mrow><mo id="S1.T1.6.6.1.m1.2.2.2.2.1.3"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.3.cmml">â‰¤</mo><mi id="S1.T1.6.6.1.m1.2.2.2.2.1.4"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.4.cmml">u</mi><mo id="S1.T1.6.6.1.m1.2.2.2.2.1.5"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.5.cmml">â‰¤</mo><mfrac id="S1.T1.6.6.1.m1.2.2.2.2.1.6"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.cmml"><mn id="S1.T1.6.6.1.m1.2.2.2.2.1.6.2" xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.2.cmml">1</mn><mn
    id="S1.T1.6.6.1.m1.2.2.2.2.1.6.3" xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.3.cmml">2</mn></mfrac></mrow></mtd></mtr><mtr
    id="S1.T1.6.6.1.m1.2.2g" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtd id="S1.T1.6.6.1.m1.2.2h"
    xref="S1.T1.6.6.1.m1.2.2.cmml"><mn id="S1.T1.6.6.1.m1.2.2.3.1.1" xref="S1.T1.6.6.1.m1.2.2.3.1.1.cmml">0</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.6.6.1.m1.2.2i" xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.2.3.2.1" xref="S1.T1.6.6.1.m1.2.2.3.2.1.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.3.2.1.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.cmml"><mtext id="S1.T1.6.6.1.m1.2.2.3.2.1.2.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.2a.cmml">ifÂ </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.1" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.1.cmml">â€‹</mo><mi
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.3" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.6.6.1.m1.2.2.3.2.1.1" xref="S1.T1.6.6.1.m1.2.2.3.2.1.1.cmml"><</mo><mrow
    id="S1.T1.6.6.1.m1.2.2.3.2.1.3" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.cmml"><mo id="S1.T1.6.6.1.m1.2.2.3.2.1.3a"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.cmml">âˆ’</mo><mfrac id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.cmml"><mn id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3.cmml">2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S1.T1.6.6.1.m1.2b"><apply id="S1.T1.6.6.1.m1.2.3.cmml"
    xref="S1.T1.6.6.1.m1.2.3"><apply id="S1.T1.6.6.1.m1.2.3.2.cmml" xref="S1.T1.6.6.1.m1.2.3.2"><ci
    id="S1.T1.6.6.1.m1.2.3.2.2.cmml" xref="S1.T1.6.6.1.m1.2.3.2.2">ğœ</ci><ci id="S1.T1.6.6.1.m1.1.1.cmml"
    xref="S1.T1.6.6.1.m1.1.1">ğ‘¢</ci></apply><apply id="S1.T1.6.6.1.m1.2.3.3.1.cmml"
    xref="S1.T1.6.6.1.m1.2.3.3.2"><csymbol cd="latexml" id="S1.T1.6.6.1.m1.2.3.3.1.1.cmml"
    xref="S1.T1.6.6.1.m1.2.3.3.2.1">cases</csymbol><matrix id="S1.T1.6.6.1.m1.2.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2"><matrixrow id="S1.T1.6.6.1.m1.2.2a.cmml" xref="S1.T1.6.6.1.m1.2.2"><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.1.1.1.cmml" xref="S1.T1.6.6.1.m1.2.2.1.1.1">1</cn><apply
    id="S1.T1.6.6.1.m1.2.2.1.2.1.cmml" xref="S1.T1.6.6.1.m1.2.2.1.2.1"><apply id="S1.T1.6.6.1.m1.2.2.1.2.1.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2"><ci id="S1.T1.6.6.1.m1.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.2"><mtext id="S1.T1.6.6.1.m1.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.2">ifÂ </mtext></ci><ci id="S1.T1.6.6.1.m1.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.3">ğ‘¢</ci></apply><apply id="S1.T1.6.6.1.m1.2.2.1.2.1.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3"><cn type="integer" id="S1.T1.6.6.1.m1.2.2.1.2.1.3.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.2">1</cn><cn type="integer" id="S1.T1.6.6.1.m1.2.2.1.2.1.3.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.3">2</cn></apply></apply></matrixrow><matrixrow
    id="S1.T1.6.6.1.m1.2.2b.cmml" xref="S1.T1.6.6.1.m1.2.2"><apply id="S1.T1.6.6.1.m1.2.2.2.1.1.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.1.1"><ci id="S1.T1.6.6.1.m1.2.2.2.1.1.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.2">ğ‘¢</ci><apply
    id="S1.T1.6.6.1.m1.2.2.2.1.1.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3"><cn type="integer"
    id="S1.T1.6.6.1.m1.2.2.2.1.1.3.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.2">1</cn><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.2.1.1.3.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.3">2</cn></apply></apply><apply
    id="S1.T1.6.6.1.m1.2.2.2.2.1.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1"><apply id="S1.T1.6.6.1.m1.2.2.2.2.1b.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1"><apply id="S1.T1.6.6.1.m1.2.2.2.2.1.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2"><ci
    id="S1.T1.6.6.1.m1.2.2.2.2.1.2.2a.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.2"><mtext
    id="S1.T1.6.6.1.m1.2.2.2.2.1.2.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.2">ifÂ </mtext></ci><apply
    id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3"><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2">1</cn><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3">2</cn></apply></apply><ci
    id="S1.T1.6.6.1.m1.2.2.2.2.1.4.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.4">ğ‘¢</ci></apply><apply
    id="S1.T1.6.6.1.m1.2.2.2.2.1c.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1"><apply id="S1.T1.6.6.1.m1.2.2.2.2.1.6.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6"><cn type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.6.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.2">1</cn><cn type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.6.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.3">2</cn></apply></apply></apply></matrixrow><matrixrow
    id="S1.T1.6.6.1.m1.2.2c.cmml" xref="S1.T1.6.6.1.m1.2.2"><cn type="integer" id="S1.T1.6.6.1.m1.2.2.3.1.1.cmml"
    xref="S1.T1.6.6.1.m1.2.2.3.1.1">0</cn><apply id="S1.T1.6.6.1.m1.2.2.3.2.1.cmml"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1"><apply id="S1.T1.6.6.1.m1.2.2.3.2.1.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2"><ci
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.2a.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.2"><mtext
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.2">ifÂ </mtext></ci><ci
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.3.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.3">ğ‘¢</ci></apply><apply
    id="S1.T1.6.6.1.m1.2.2.3.2.1.3.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3"><apply
    id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2"><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2">1</cn><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3">2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.6.6.1.m1.2c">\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if
    }u>\frac{1}{2}\\ u+\frac{1}{2}&\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\ 0&\text{if
    }u<-\frac{1}{2}\end{array}\right.</annotation></semantics></math> | Courbariaux
    etÂ al. ([2015](#bib.bib63)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| max pooling | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{0,u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is the output of another neuron)}\end{array}$ | Weng etÂ al.
    ([1992](#bib.bib329)) |'
  prefs: []
  type: TYPE_TB
- en: '| maxout | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is an affine function)}\end{array}$ | Goodfellow etÂ al. ([2013](#bib.bib123))
    |'
  prefs: []
  type: TYPE_TB
- en: 1.4 When deep learning meets polyhedral theory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is commonly accepted in machine learning that a simpler model is preferred
    if it trains as well as a more complex one, since a simpler model is less likely
    to overfit. Conveniently, the successful return of neural networks to relatively
    simpler activation functions prepared the ground for deep learning to meet polyhedral
    theory. In other words, we are now able to analyze and leverage neural networks
    through the same lenses and tools that have been successfully used for linear
    and discrete optimization in operations research for many decades. We explain
    this connection in more detail and some lines of research that it has opened up
    in SectionÂ [2](#S2 "2 The Polyhedral Perspective â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Scope of this survey and related work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The interplay between mathematical optimization and machine learning has also
    been discussed by other recent surveys. Bengio etÂ al. ([2021](#bib.bib19)) review
    the use of machine learning in mathematical optimization, whereasÂ Gambella etÂ al.
    ([2021](#bib.bib115)) formulate mathematical optimization problems with the main
    focus of obtaining machine learning models, such as by training neural networks.
    A similar scope has been previously surveyed byÂ Curtis and Scheinberg ([2017](#bib.bib70))
    andÂ Bottou etÂ al. ([2018](#bib.bib38)). Our survey complements those by focusing
    exclusively on neural networks while outlining how linear optimization can be
    used more broadly in that context, from network training and verification to model
    embedding and compression, as well as refined through formulation strengthening.
    In addition, we illustrate how polyhedral theory can ground the use of such linear
    formulations and also provide a more nuanced understanding of the discriminative
    ability of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The presentation in this survey is centered on *feedforward rectifier networks*.
    These are very commonly used networks with only ReLU activations and for which
    most polyhedral results and applications of linear optimization are known. The
    focus on a single type of neural network is intended to help the reader capture
    the intuition behind different developments and understand the nuances involved.
    Despite our focus on *fully-connected* models, which are those in which every
    unit is connected to all units in the subsequent layer, there are many variants
    of interest with fewer or different types of connection that can be interpreted
    as a special case of fully-connected models. For example, the units of Convolutional
    Neural NetworksÂ (CNNs or ConvNets) (Fukushima, [1980](#bib.bib111)) have local
    connectivity: only a subset of adjacent units defines the output of each unit
    in the next layer, and the same parameters are used to define the output of different
    units. In fact, multiple *filters* of parameters can be applied to a set of adjacent
    units through the output of different units in the next layer. CNNs are often
    applied to identify and aggregate the same local features in different parts of
    a picture, and we can interpret them as a special case of feedforward networks.
    Another common variant, the Residual NetworkÂ (ResNet) (He etÂ al., [2016](#bib.bib144)),
    includes *skip connections* that directly connect units in nonadjacent layers.
    Those connections can be emulated by adding units passing their outputs through
    the intermediary layers. Hence, many of the results and applications discussed
    along the survey are relevant to other variants (e.g., LTU and maxout activations,
    or those other connectivity patterns), and we also provide references to more
    specific results and applications involving them.'
  prefs: []
  type: TYPE_NORMAL
- en: We also discuss the extent to which other variants remain relevant or can be
    analyzed through the same lenses. For example, *feedback connections* in *recurrent
    networks* (Little, [1974](#bib.bib195), Hopfield, [1982](#bib.bib152)) allow the
    output of a unit to be used as an input of units in previous layers. Recurrent
    networks such as Long Short-Term MemoryÂ (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib151))
    produce outputs that depend on their internal state, and they may consequently
    process sequential inputs with arbitrary length. While feedback connections may
    not be emulated with a feeforward network, we discuss in the following paragraph
    how recurrent networks have been replaced with great success by attention mechanisms,
    which are implemented with feedforward networks. In the realm of variants that
    remain relevant, it is very common to apply a different type of activation to
    the output layer of the network, such as the layer-wise softmax function $\sigma:\mathbb{R}^{n_{L}}\rightarrow\mathbb{R}^{n_{L}}$
    in which $\sigma(u)_{i}=e^{u_{i}}/\sum_{j=1}^{n_{L}}e^{u_{j}}~{}\forall i\in\{1,\ldots,n_{L}\}$
    (Bridle, [1990](#bib.bib39)), which is used to normalize a multidimensional output
    as a probability distribution. While softmax is not piecewise linear, we describe
    how its output can also be analyzed from a polyhedral perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Other uses of deep learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Deep learning is also being used in machine learning beyond the realm of supervised
    learning. In *unsupervised learning*, the focus is on drawing inferences from
    unlabeled datasets. For example, Generative Adversarial NetworksÂ (GANs) (Goodfellow
    etÂ al., [2014](#bib.bib126)) have been used to generate realistic images using
    a pair of neural networks. One of these networks is a *discriminator* trained
    to identify elements from a dataset and the other is a *generator* aiming to mislead
    the discriminator with synthetic inputs that could be classified as belonging
    to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In *reinforcement learning*, the focus is on modeling agents that can interact
    with their environment through actions and associated rewards. Examples of such
    applications include neural networks designed for the navigation of self-driving
    vehicles (Gao etÂ al., [2020](#bib.bib116)) and for playing Atari games (Mnih etÂ al.,
    [2015](#bib.bib222)), more contemporary electronic games such as Dota 2 (OpenAI
    etÂ al., [2019](#bib.bib237)) and StarCraft II (Vinyals etÂ al., [2017](#bib.bib321)),
    and the game of Go (Silver etÂ al., [2017](#bib.bib288)) at levels that are either
    better or at least comparable to human players.
  prefs: []
  type: TYPE_NORMAL
- en: A more recent and popular example are generative transformers (Radford etÂ al.,
    [2018](#bib.bib252)), such as DALLÂ·E 2 (Ramesh etÂ al., [2022](#bib.bib257)) producing
    realistic images from text prompts in mid-2022 and ChatGPT (OpenAI, [2022](#bib.bib236))
    producing realistic dialogues with users in early 2023, the latter belonging to
    the fast growing family of large language models. They are based on replacing
    architectures based on feedback connections, such as LSTM, with the attention
    mechanisms aimed at scoring the relevance of past states (Bahdanau etÂ al., [2015](#bib.bib10)),
    which is the foundation of the transformer architecture (Vaswani etÂ al., [2017](#bib.bib315)).
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For a historical perspective on neural networks, we recommend Schmidhuber ([2015](#bib.bib273)).
    For a recent and broad introduction to the fundamentals of deep learning, we recommendÂ Zhang
    etÂ al. ([2023](#bib.bib353)). For other forms of measuring model complexity in
    neural networks, we refer toÂ Hu etÂ al. ([2021](#bib.bib156)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 The Polyhedral Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feedforward rectifier network models a piecewise linear function (Arora etÂ al.,
    [2018](#bib.bib8)) in which every such piece is a polyhedron (Raghu etÂ al., [2017](#bib.bib253)),
    and represents a special case among neural networks modeling piecewise polynomials
    (Balestriero and Baraniuk, [2018](#bib.bib15)). Therefore, training a rectifier
    network is equivalent to performing a piecewise linear regression, and we can
    potentially interpret such neural networks in terms of what happens in each piece
    of the function that they model. However, we are only beginning to answer some
    of the questions entailed by such a remark. In this survey, we discuss how insights
    on this subject may help us answer the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which piecewise linear functions can or cannot be obtained from training a neural
    network given its architecture?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which neural networks are more susceptible to adversarial exploitation?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we integrate the model learned by a neural network into a broader decision-making
    problem for which we want to find an optimal solution?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it possible to obtain a smaller neural network that models exactly the same
    function as another trained neural network?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we exploit the polyhedral geometry present in neural networks in the training
    phase?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we efficiently incorporate extra structure when training neural network,
    such as linear constraints over the weights?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first question complements the universal approximation results for neural
    networks. Namely, there is a limit to what functions can be well approximated
    when limited computational resources are translated into constraints on the depth
    and width of the layers of neural networks that can be used in practice. The functions
    that can be modeled depend on the particular choice of hyperparameters subject
    to the computational resources available, and in the long run that may also lead
    to a more principled approach for the choice of hyperparameters than the current
    approaches of neural architecture search. In SectionÂ [3](#S3 "3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"),
    we analyze how a rectifier network partitions the input space into pieces in which
    it behaves linearly, which we denote as *linear regions*. We discuss the geometry
    of linear regions, the effect of parameters and hyperparameters on the number
    of linear regions of a neural network, and the extent to which such number of
    linear regions relates to the accuracy of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second question relies on formal verification methods to evaluate the robustness
    of neural networks, which can be approached with mathematical optimization formulations
    that are also relevant for the third and fourth questions. Such formulations are
    convenient since a direct inspection of every piece of the function modeled by
    a neural network is prohibitive given how quickly their number scale with the
    size of the network. The linear behavior of the network for every choice of active
    and inactive units implies that we can use linear formulations with binary variables
    corresponding to the activation of units to model trained neural networks using
    MILP. Therefore, we are able to solve a variety of optimization problems over
    a trained neural network, such as the neural network verification problem, identifying
    the range of outputs for each ReLU of the network, and modeling a trained neural
    network as part of a larger decision-making problem. In SectionÂ [4](#S4 "4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey"), we discuss how to formulate optimization problems over a trained neural
    network, the applications of such formulations, and the progress toward obtaining
    stronger formulations that scale more easily with the network size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fifth and sixth questions involve the training procedure of a DNN, where
    linear programming tools have been applied to partially answer them. In SectionÂ [5](#S5
    "5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), we overview these developments. In terms of the
    fifth question â€”exploiting polyhedrality in training neural networksâ€” we describe
    algorithms that use the polyhedral geometry induced by activation sets to solve
    training problems. We also cover a recently proposed polyhedral construction that
    can approximately encode multiple training problems at once, showing a strong
    relationship across training problems that arise from different datasets, for
    a fixed architecture. Additionally, we review some recent uses of mixed-integer
    linear programming in the training phase as an alternative to SGD when the weights
    are required to be integer. Regarding the sixth question â€”the incorporation of
    extra structure when trainingâ€” we review multiple approaches that have included
    techniques related to linear programming within SGD to impose a desirable structure
    when training, or to find better step-lengths in the execution of SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 The Linear Regions of a Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every piece of the piecewise linear function modeled by a neural network is
    a linear region, and â€”without loss of generalityâ€” we can think of it as a polyhedron.
    In this section, we define a linear region, exemplify how they can be so numerous,
    and what may affect their count in a neural network. We also discuss the practical
    implications of such insights, as well as other related forms of analyzing the
    ability of a neural network to represent expressive models.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A linear region corresponds to the set of points from the input space that activates
    the same units along the neural network, and hence can be characterized by the
    set ${\mathbb{S}}^{l}$ of units that are active in each layer $l\in{\mathbb{L}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Since a neural network behaves uniformly over a linear region, the latter is
    the smallest finite scale in which we can analyze its behavior. If we restrict
    the domain of a neural network to a linear region ${\mathbb{I}}\subseteq\mathbb{R}^{n_{0}}$,
    then the neural network behaves as an affine transformation ${\bm{y}}_{\mathbb{I}}:{\mathbb{I}}\rightarrow\mathbb{R}^{n_{L}}$
    of the form ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$ with a
    matrix ${\bm{T}}\in\mathbb{R}^{n_{L}\times n_{0}}$ and a vector ${\bm{t}}\in\mathbb{R}^{n_{L}}$
    that are directly defined by the network parameters and the set of neurons that
    are activated by any input ${\bm{x}}\in{\mathbb{I}}$. For a small perturbationÂ $\varepsilon$
    to some input $\overline{{\bm{x}}}\in{\mathbb{I}}$ such that $\overline{{\bm{x}}}+\varepsilon\in{\mathbb{I}}$,
    the network output for $\bar{x}+\varepsilon$ is given by ${\bm{y}}_{\mathbb{I}}(\overline{{\bm{x}}}+\varepsilon)$.
    While it is possible that two adjacent regions defined in such way correspond
    to the same affine transformation, thinking of each linear region as having a
    distinct signature of active units makes it easier to analyze them.
  prefs: []
  type: TYPE_NORMAL
- en: The number of linear regions defined by a neural network is one form with which
    we can measure the complexity of the models that it can represent (Bengio, [2009](#bib.bib18)).
    Hence, if a more complex model is desired, we may want to design a neural network
    that can potentially define more linear regions. On the one hand, the number of
    linear regions may grow exponentially on the depth of a neural network. On the
    other hand, such a number depends on the interplay between network parameters
    and hyperparameters. As we consider how the inputs from adjacent linear regions
    are evaluated, the change to the affine transformation can be characterized in
    algebraic and geometric terms. Understanding such changes may help us grasp how
    a neural network is capable of telling its inputs apart, including what are the
    sources of the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For neural networks in which the activation function is not piecewise linear,
    Bianchini and Scarselli ([2014](#bib.bib29)) have used more elaborate topological
    measures to compare the expressiveness of shallow and deep neural networks. Hu
    etÂ al. ([2020b](#bib.bib155)) followed a closer approach by producing a linear
    approximation neural network in which the number of linear regions can be counted.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The combinatorial aspect of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most striking aspects about analyzing a neural network in terms of
    its linear regions is how quickly such number grows. Early work on this topic
    byÂ Pascanu etÂ al. ([2014](#bib.bib243)) and MontÃºfar etÂ al. ([2014](#bib.bib224))
    have drawn two important observations. First, that it is possible to construct
    simple deep neural networks with a number of linear regions that grows exponentially
    in the depth. Second, that the number of linear regions can be exponential in
    the number of neurons alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first observation comes from analyzing the role of ReLUs in a very simple
    setting. Namely, that of a neural network in which we regard every layer as having
    a single input in the $[0,1]$ domain, which is produced by combining the outputs
    of the units from the preceding layer, as illustrated by ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a neural network with input $x$ from the domain $[0,1]$ and layers
    having 4 neurons with ReLU activation. For the first layer, assume that the output
    of the neurons are given by the following functions: $f_{1}(x)=\max\{4x,0\}$,
    $f_{2}(x)=\max\{8x-2,0\}$, $f_{3}(x)=\max\{6.5x-3.25,0\}$, and $f_{4}(x)=\max\{12.5x-11.25,0\}$.
    In other words, ${\bm{h}}^{1}_{i}=f_{i}(x)~{}\forall i\in\{1,2,3,4\}$. For the
    subsequent layers, assume that the outputs coming from the previous layer are
    combined through the function $F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$, which
    substitutes $x$ as the input to the next layer; upon which the same set of functions
    $\{f_{i}(x)\}_{i=1}^{4}$ defines the output of the next layer. In other words,
    ${\bm{h}}^{l}_{i}=f_{i}(F({\bm{h}}^{l-1}))=f_{i}({h}_{1}^{l-1}-{h}_{2}^{l-1}+{h}_{3}^{l-1}-{h}_{4}^{l-1})~{}\forall
    i\in\{1,2,3,4\},l\in{\mathbb{L}}\setminus\{1\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: When the output of the units in the first layer is combined as $F(x)$, we obtain
    a zigzagging function with 4 slopes in the $[0,1]$ domain, each of which defining
    a bijection between segments of the input â€”namely, $[0,0.25]$, $[0.25,0.5]$, $[0.5,0.9]$,
    and $[0.9,1.0]$â€” and the image $[0,1]$. The effect of repeating such structure
    in the second layer is that of composing $F(x)$ with itself, with 4 slopes being
    produced within each of those 4 initial segments. Hence, the number of slopes
    â€”and therefore of linear regionsâ€” in the output of such a neural network with
    $L$ layers of activation functions is $4^{L}$, which implies an exponential growth
    on depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network structure and the parameters of the neurons in the first two layers
    are illustrated in FigureÂ [3](#S3.F3 "Figure 3 â€£ 3.1 The combinatorial aspect
    of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"); the set of functions $\{f_{i}(x)\}_{i=1}^{4}$
    and the combined outputs of the first two layers â€”$F(x)$ and $F(F(x))$â€” are illustrated
    in FigureÂ [4](#S3.F4 "Figure 4 â€£ 3.1 The combinatorial aspect of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In ExampleÂ [1](#Thmexample1 "Example 1 â€£ 3.1 The combinatorial aspect of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), every neuron changes the slope of the resulting
    function once it becomes active, in which we purposely alternate between positive
    and negative slopes once the function reaches either 0 or 1, respectively. By
    selecting the network parameters accordingly, MontÃºfar etÂ al. ([2014](#bib.bib224))
    were the first to show that a layer with $n$ ReLUs can be used to create a zigzagging
    function with $n$ slopes on the $[0,1]$ domain, with the image along every slope
    also corresponding to the interval $[0,1]$. Consequently, stacking $L$ of such
    layers results in a neural network with $n^{L}$ linear regions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bac8f009269c114fb6255ef917600fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Mapping from the input $x\in[0,1]$ to the intermediary output ${\bm{h}}^{2}\in[0,1]^{4}$
    through the first two layers of a neural network in which the number of linear
    regions growths exponentially on the depth, as described in ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").
    The parameters of subsequent layers are the same as those in the second layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/285adf5284d089caf1d99c5051b857ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Set of activation functions $\{f_{i}(x)\}_{i=1}^{4}$ of the units
    in the first layer and combined outputs of the first two layers â€”$F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$
    for the first and $F(F(x))$ for the secondâ€” of a neural network in which the number
    of linear regions grows exponentially on the depth, as described in ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second observation â€”that the number of linear regions can grow exponentially
    in the number of neurons aloneâ€” comes from the interplay between the parts of
    the input space in which each the units are active, especially in higher-dimensional
    spaces. This is based on some geometric observations that we discuss in SectionÂ [3.3](#S3.SS3
    "3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). Even for a *shallow*
    network â€”i.e., the number of layers being $L=1$â€” such a number of linear regions
    may approach $2^{n}$, which corresponds to every possible activation set ${\mathbb{S}}\subseteq\{1,\ldots,n\}$
    defining a nonempty linear region. However, as we discuss later, that is not always
    the case due to architectural choices such as the number of layers and their width.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The algebra of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the activation sets $\{{\mathbb{S}}^{l}\}_{l\in{\mathbb{L}}}$ denoting
    which neurons are active for each layer of the neural network, we can explicitly
    describe the affine transformation ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$
    associated with the corresponding linear region ${\mathbb{I}}$. For every activation
    set ${\mathbb{S}}^{l}$, layer $l$ defines an affine transformation of the form
    $\Omega^{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$, where $\Omega^{{\mathbb{S}}^{l}}$
    is a diagonal $n_{l}\times n_{l}$ matrix in which $\Omega^{{\mathbb{S}}^{l}}_{ii}=1$
    if $i\in{\mathbb{S}}^{l}$ and $\Omega^{{\mathbb{S}}^{l}}_{ii}=0$ otherwise. Hence,
    the matrix ${\bm{T}}$ and vector ${\bm{t}}$ are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{T}}=\prod_{l=1}^{L}\Omega^{{\mathbb{S}}^{l}}{\bm{W}}^{l},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ${\bm{t}}=\sum_{l_{1}=1}^{L}\left(\prod_{l_{2}=l_{1}+1}^{L}\Omega^{{\mathbb{S}}^{l_{2}}}{\bm{W}}^{l_{2}}\right)\Omega^{{\mathbb{S}}^{l_{1}}}{\bm{b}}^{l_{1}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: On a side note, Takai etÂ al. ([2021](#bib.bib302)) proposed a related metric
    for networks modeling piecewise linear functions by counting the number of distinct
    functions among linear regions upon equivalence through isometric affine transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each linear region is associated with a polyhedron, and we can describe the
    union of polyhedra $\mathcal{D}$ on the space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$
    that covers the entire input space $x$ of the neural network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.Ex3.m1.18" class="ltx_Math" alttext="\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&amp;\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&amp;\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq 0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\'
  prefs: []
  type: TYPE_NORMAL
- en: h_{i}^{l}=0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right)."
    display="block"><semantics id="S3.Ex3.m1.18a"><mrow id="S3.Ex3.m1.18.18.1" xref="S3.Ex3.m1.18.18.1.1.cmml"><mrow
    id="S3.Ex3.m1.18.18.1.1" xref="S3.Ex3.m1.18.18.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.Ex3.m1.18.18.1.1.2" xref="S3.Ex3.m1.18.18.1.1.2.cmml">ğ’Ÿ</mi><mo rspace="0.111em"
    id="S3.Ex3.m1.18.18.1.1.1" xref="S3.Ex3.m1.18.18.1.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.18.18.1.1.3"
    xref="S3.Ex3.m1.18.18.1.1.3.cmml"><munder id="S3.Ex3.m1.18.18.1.1.3.1" xref="S3.Ex3.m1.18.18.1.1.3.1.cmml"><mo
    movablelimits="false" rspace="0em" id="S3.Ex3.m1.18.18.1.1.3.1.2" xref="S3.Ex3.m1.18.18.1.1.3.1.2.cmml">â‹</mo><mrow
    id="S3.Ex3.m1.9.9.9" xref="S3.Ex3.m1.9.9.9.cmml"><mrow id="S3.Ex3.m1.7.7.7.7.2"
    xref="S3.Ex3.m1.7.7.7.7.3.cmml"><mo stretchy="false" id="S3.Ex3.m1.7.7.7.7.2.3"
    xref="S3.Ex3.m1.7.7.7.7.3.cmml">(</mo><msup id="S3.Ex3.m1.6.6.6.6.1.1" xref="S3.Ex3.m1.6.6.6.6.1.1.cmml"><mi
    id="S3.Ex3.m1.6.6.6.6.1.1.2" xref="S3.Ex3.m1.6.6.6.6.1.1.2.cmml">ğ•Š</mi><mn id="S3.Ex3.m1.6.6.6.6.1.1.3"
    xref="S3.Ex3.m1.6.6.6.6.1.1.3.cmml">1</mn></msup><mo id="S3.Ex3.m1.7.7.7.7.2.4"
    xref="S3.Ex3.m1.7.7.7.7.3.cmml">,</mo><mi mathvariant="normal" id="S3.Ex3.m1.1.1.1.1"
    xref="S3.Ex3.m1.1.1.1.1.cmml">â€¦</mi><mo id="S3.Ex3.m1.7.7.7.7.2.5" xref="S3.Ex3.m1.7.7.7.7.3.cmml">,</mo><msup
    id="S3.Ex3.m1.7.7.7.7.2.2" xref="S3.Ex3.m1.7.7.7.7.2.2.cmml"><mi id="S3.Ex3.m1.7.7.7.7.2.2.2"
    xref="S3.Ex3.m1.7.7.7.7.2.2.2.cmml">ğ•Š</mi><mi id="S3.Ex3.m1.7.7.7.7.2.2.3" xref="S3.Ex3.m1.7.7.7.7.2.2.3.cmml">L</mi></msup><mo
    stretchy="false" id="S3.Ex3.m1.7.7.7.7.2.6" xref="S3.Ex3.m1.7.7.7.7.3.cmml">)</mo></mrow><mo
    id="S3.Ex3.m1.9.9.9.10" xref="S3.Ex3.m1.9.9.9.10.cmml">âŠ†</mo><mrow id="S3.Ex3.m1.9.9.9.9"
    xref="S3.Ex3.m1.9.9.9.9.cmml"><mrow id="S3.Ex3.m1.8.8.8.8.1.1" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml"><mo
    stretchy="false" id="S3.Ex3.m1.8.8.8.8.1.1.2" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">{</mo><mn
    id="S3.Ex3.m1.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.cmml">1</mn><mo id="S3.Ex3.m1.8.8.8.8.1.1.3"
    xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.Ex3.m1.3.3.3.3"
    xref="S3.Ex3.m1.3.3.3.3.cmml">â€¦</mi><mo id="S3.Ex3.m1.8.8.8.8.1.1.4" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">,</mo><msub
    id="S3.Ex3.m1.8.8.8.8.1.1.1" xref="S3.Ex3.m1.8.8.8.8.1.1.1.cmml"><mi id="S3.Ex3.m1.8.8.8.8.1.1.1.2"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.2.cmml">n</mi><mn id="S3.Ex3.m1.8.8.8.8.1.1.1.3"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.3.cmml">1</mn></msub><mo rspace="0.055em" stretchy="false"
    id="S3.Ex3.m1.8.8.8.8.1.1.5" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">}</mo></mrow><mo
    rspace="0.222em" id="S3.Ex3.m1.9.9.9.9.3" xref="S3.Ex3.m1.9.9.9.9.3.cmml">Ã—</mo><mi
    mathvariant="normal" id="S3.Ex3.m1.9.9.9.9.4" xref="S3.Ex3.m1.9.9.9.9.4.cmml">â€¦</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.Ex3.m1.9.9.9.9.3a" xref="S3.Ex3.m1.9.9.9.9.3.cmml">Ã—</mo><mrow
    id="S3.Ex3.m1.9.9.9.9.2.1" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml"><mo stretchy="false"
    id="S3.Ex3.m1.9.9.9.9.2.1.2" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">{</mo><mn id="S3.Ex3.m1.4.4.4.4"
    xref="S3.Ex3.m1.4.4.4.4.cmml">1</mn><mo id="S3.Ex3.m1.9.9.9.9.2.1.3" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">,</mo><mi
    mathvariant="normal" id="S3.Ex3.m1.5.5.5.5" xref="S3.Ex3.m1.5.5.5.5.cmml">â€¦</mi><mo
    id="S3.Ex3.m1.9.9.9.9.2.1.4" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">,</mo><msub id="S3.Ex3.m1.9.9.9.9.2.1.1"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1.cmml"><mi id="S3.Ex3.m1.9.9.9.9.2.1.1.2" xref="S3.Ex3.m1.9.9.9.9.2.1.1.2.cmml">n</mi><mi
    id="S3.Ex3.m1.9.9.9.9.2.1.1.3" xref="S3.Ex3.m1.9.9.9.9.2.1.1.3.cmml">L</mi></msub><mo
    stretchy="false" id="S3.Ex3.m1.9.9.9.9.2.1.5" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">}</mo></mrow></mrow></mrow></munder><mrow
    id="S3.Ex3.m1.18.18.1.1.3.2.2" xref="S3.Ex3.m1.17.17.cmml"><mo id="S3.Ex3.m1.18.18.1.1.3.2.2.1"
    xref="S3.Ex3.m1.17.17.cmml">(</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" id="S3.Ex3.m1.17.17" xref="S3.Ex3.m1.17.17.cmml"><mtr id="S3.Ex3.m1.17.17a"
    xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17b" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.11.11.2.3.1" xref="S3.Ex3.m1.11.11.2.3.1.cmml"><mrow id="S3.Ex3.m1.11.11.2.3.1.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.cmml"><mrow id="S3.Ex3.m1.11.11.2.3.1.2.2" xref="S3.Ex3.m1.11.11.2.3.1.2.2.cmml"><msubsup
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2.cmml">ğ’˜</mi><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3.cmml">i</mi><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.2.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.3.cmml">l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em" id="S3.Ex3.m1.11.11.2.3.1.2.2.1" xref="S3.Ex3.m1.11.11.2.3.1.2.2.1.cmml">â‹…</mo><msup
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.3.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.2.cmml">ğ’‰</mi><mrow id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2.cmml">l</mi><mo id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.1"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.1.cmml">âˆ’</mo><mn id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3.cmml">1</mn></mrow></msup></mrow><mo id="S3.Ex3.m1.11.11.2.3.1.2.1"
    xref="S3.Ex3.m1.11.11.2.3.1.2.1.cmml">+</mo><msubsup id="S3.Ex3.m1.11.11.2.3.1.2.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.3.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.3.2.2" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.2.cmml">b</mi><mi
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.3" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.11.11.2.3.1.2.3.3" xref="S3.Ex3.m1.11.11.2.3.1.2.3.3.cmml">l</mi></msubsup></mrow><mo
    id="S3.Ex3.m1.11.11.2.3.1.1" xref="S3.Ex3.m1.11.11.2.3.1.1.cmml">â‰¥</mo><mn id="S3.Ex3.m1.11.11.2.3.1.3"
    xref="S3.Ex3.m1.11.11.2.3.1.3.cmml">0</mn></mrow></mtd><mtd id="S3.Ex3.m1.17.17c"
    xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.11.11.2.2.2.2" xref="S3.Ex3.m1.11.11.2.2.2.3.cmml"><mrow
    id="S3.Ex3.m1.10.10.1.1.1.1.1" xref="S3.Ex3.m1.10.10.1.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.10.10.1.1.1.1.1.2"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.10.10.1.1.1.1.1.2.1"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.1.cmml">âˆ€</mo><mi id="S3.Ex3.m1.10.10.1.1.1.1.1.2.2"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.2.cmml">l</mi></mrow><mo id="S3.Ex3.m1.10.10.1.1.1.1.1.1"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.1.cmml">âˆˆ</mo><mi id="S3.Ex3.m1.10.10.1.1.1.1.1.3"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.3.cmml">ğ•ƒ</mi></mrow><mo id="S3.Ex3.m1.11.11.2.2.2.2.3"
    xref="S3.Ex3.m1.11.11.2.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.11.11.2.2.2.2.2"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2.cmml"><mi id="S3.Ex3.m1.11.11.2.2.2.2.2.2" xref="S3.Ex3.m1.11.11.2.2.2.2.2.2.cmml">i</mi><mo
    id="S3.Ex3.m1.11.11.2.2.2.2.2.1" xref="S3.Ex3.m1.11.11.2.2.2.2.2.1.cmml">âˆˆ</mo><msup
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.11.11.2.2.2.2.2.3.2"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.2.cmml">ğ•Š</mi><mi id="S3.Ex3.m1.11.11.2.2.2.2.2.3.3"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr><mtr
    id="S3.Ex3.m1.17.17d" xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17e" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.13.13.4.3.1" xref="S3.Ex3.m1.13.13.4.3.1.cmml"><msubsup id="S3.Ex3.m1.13.13.4.3.1.2"
    xref="S3.Ex3.m1.13.13.4.3.1.2.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.2.2.2" xref="S3.Ex3.m1.13.13.4.3.1.2.2.2.cmml">h</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.2.2.3" xref="S3.Ex3.m1.13.13.4.3.1.2.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.2.3" xref="S3.Ex3.m1.13.13.4.3.1.2.3.cmml">l</mi></msubsup><mo
    id="S3.Ex3.m1.13.13.4.3.1.1" xref="S3.Ex3.m1.13.13.4.3.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.13.13.4.3.1.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.cmml"><mrow id="S3.Ex3.m1.13.13.4.3.1.3.2" xref="S3.Ex3.m1.13.13.4.3.1.3.2.cmml"><msubsup
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2.cmml">ğ’˜</mi><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3.cmml">i</mi><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.2.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.3.cmml">l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em" id="S3.Ex3.m1.13.13.4.3.1.3.2.1" xref="S3.Ex3.m1.13.13.4.3.1.3.2.1.cmml">â‹…</mo><msup
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.3.2"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.2.cmml">ğ’‰</mi><mrow id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2.cmml">l</mi><mo id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.1"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.1.cmml">âˆ’</mo><mn id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3.cmml">1</mn></mrow></msup></mrow><mo id="S3.Ex3.m1.13.13.4.3.1.3.1"
    xref="S3.Ex3.m1.13.13.4.3.1.3.1.cmml">+</mo><msubsup id="S3.Ex3.m1.13.13.4.3.1.3.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.3.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.3.2.2" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.2.cmml">b</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.3" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.3.3.3" xref="S3.Ex3.m1.13.13.4.3.1.3.3.3.cmml">l</mi></msubsup></mrow></mrow></mtd><mtd
    id="S3.Ex3.m1.17.17f" xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.13.13.4.2.2.2"
    xref="S3.Ex3.m1.13.13.4.2.2.3.cmml"><mrow id="S3.Ex3.m1.12.12.3.1.1.1.1" xref="S3.Ex3.m1.12.12.3.1.1.1.1.cmml"><mrow
    id="S3.Ex3.m1.12.12.3.1.1.1.1.2" xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.cmml"><mo rspace="0.167em"
    id="S3.Ex3.m1.12.12.3.1.1.1.1.2.1" xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.1.cmml">âˆ€</mo><mi
    id="S3.Ex3.m1.12.12.3.1.1.1.1.2.2" xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.2.cmml">l</mi></mrow><mo
    id="S3.Ex3.m1.12.12.3.1.1.1.1.1" xref="S3.Ex3.m1.12.12.3.1.1.1.1.1.cmml">âˆˆ</mo><mi
    id="S3.Ex3.m1.12.12.3.1.1.1.1.3" xref="S3.Ex3.m1.12.12.3.1.1.1.1.3.cmml">ğ•ƒ</mi></mrow><mo
    id="S3.Ex3.m1.13.13.4.2.2.2.3" xref="S3.Ex3.m1.13.13.4.2.2.3a.cmml">,</mo><mrow
    id="S3.Ex3.m1.13.13.4.2.2.2.2" xref="S3.Ex3.m1.13.13.4.2.2.2.2.cmml"><mi id="S3.Ex3.m1.13.13.4.2.2.2.2.2"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.2.cmml">i</mi><mo id="S3.Ex3.m1.13.13.4.2.2.2.2.1"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.1.cmml">âˆˆ</mo><msup id="S3.Ex3.m1.13.13.4.2.2.2.2.3"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.13.13.4.2.2.2.2.3.2"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.2.cmml">ğ•Š</mi><mi id="S3.Ex3.m1.13.13.4.2.2.2.2.3.3"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr><mtr
    id="S3.Ex3.m1.17.17g" xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17h" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.15.15.6.3.1" xref="S3.Ex3.m1.15.15.6.3.1.cmml"><mrow id="S3.Ex3.m1.15.15.6.3.1.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.cmml"><mrow id="S3.Ex3.m1.15.15.6.3.1.2.2" xref="S3.Ex3.m1.15.15.6.3.1.2.2.cmml"><msubsup
    id="S3.Ex3.m1.15.15.6.3.1.2.2.2" xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2.cmml">ğ’˜</mi><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3.cmml">i</mi><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.2.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.3.cmml">l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em" id="S3.Ex3.m1.15.15.6.3.1.2.2.1" xref="S3.Ex3.m1.15.15.6.3.1.2.2.1.cmml">â‹…</mo><msup
    id="S3.Ex3.m1.15.15.6.3.1.2.2.3" xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.3.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.2.cmml">h</mi><mrow id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2.cmml">l</mi><mo id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.1"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.1.cmml">âˆ’</mo><mn id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3.cmml">1</mn></mrow></msup></mrow><mo id="S3.Ex3.m1.15.15.6.3.1.2.1"
    xref="S3.Ex3.m1.15.15.6.3.1.2.1.cmml">+</mo><msubsup id="S3.Ex3.m1.15.15.6.3.1.2.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.3.2.2" xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.2.cmml">b</mi><mi
    id="S3.Ex3.m1.15.15.6.3.1.2.3.2.3" xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.15.15.6.3.1.2.3.3" xref="S3.Ex3.m1.15.15.6.3.1.2.3.3.cmml">l</mi></msubsup></mrow><mo
    id="S3.Ex3.m1.15.15.6.3.1.1" xref="S3.Ex3.m1.15.15.6.3.1.1.cmml">â‰¤</mo><mn id="S3.Ex3.m1.15.15.6.3.1.3"
    xref="S3.Ex3.m1.15.15.6.3.1.3.cmml">0</mn></mrow></mtd><mtd id="S3.Ex3.m1.17.17i"
    xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.15.15.6.2.2.2" xref="S3.Ex3.m1.15.15.6.2.2.3.cmml"><mrow
    id="S3.Ex3.m1.14.14.5.1.1.1.1" xref="S3.Ex3.m1.14.14.5.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.14.14.5.1.1.1.1.2"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.14.14.5.1.1.1.1.2.1"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.1.cmml">âˆ€</mo><mi id="S3.Ex3.m1.14.14.5.1.1.1.1.2.2"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.2.cmml">l</mi></mrow><mo id="S3.Ex3.m1.14.14.5.1.1.1.1.1"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.1.cmml">âˆˆ</mo><mi id="S3.Ex3.m1.14.14.5.1.1.1.1.3"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.3.cmml">ğ•ƒ</mi></mrow><mo id="S3.Ex3.m1.15.15.6.2.2.2.3"
    xref="S3.Ex3.m1.15.15.6.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.15.15.6.2.2.2.2"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2.cmml"><mi id="S3.Ex3.m1.15.15.6.2.2.2.2.2" xref="S3.Ex3.m1.15.15.6.2.2.2.2.2.cmml">i</mi><mo
    id="S3.Ex3.m1.15.15.6.2.2.2.2.1" xref="S3.Ex3.m1.15.15.6.2.2.2.2.1.cmml">âˆ‰</mo><msup
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.15.15.6.2.2.2.2.3.2"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.2.cmml">ğ•Š</mi><mi id="S3.Ex3.m1.15.15.6.2.2.2.2.3.3"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr><mtr
    id="S3.Ex3.m1.17.17j" xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17k" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.17.17.8.3.1" xref="S3.Ex3.m1.17.17.8.3.1.cmml"><msubsup id="S3.Ex3.m1.17.17.8.3.1.2"
    xref="S3.Ex3.m1.17.17.8.3.1.2.cmml"><mi id="S3.Ex3.m1.17.17.8.3.1.2.2.2" xref="S3.Ex3.m1.17.17.8.3.1.2.2.2.cmml">h</mi><mi
    id="S3.Ex3.m1.17.17.8.3.1.2.2.3" xref="S3.Ex3.m1.17.17.8.3.1.2.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.17.17.8.3.1.2.3" xref="S3.Ex3.m1.17.17.8.3.1.2.3.cmml">l</mi></msubsup><mo
    id="S3.Ex3.m1.17.17.8.3.1.1" xref="S3.Ex3.m1.17.17.8.3.1.1.cmml">=</mo><mn id="S3.Ex3.m1.17.17.8.3.1.3"
    xref="S3.Ex3.m1.17.17.8.3.1.3.cmml">0</mn></mrow></mtd><mtd id="S3.Ex3.m1.17.17l"
    xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.17.17.8.2.2.2" xref="S3.Ex3.m1.17.17.8.2.2.3.cmml"><mrow
    id="S3.Ex3.m1.16.16.7.1.1.1.1" xref="S3.Ex3.m1.16.16.7.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.16.16.7.1.1.1.1.2"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.16.16.7.1.1.1.1.2.1"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.1.cmml">âˆ€</mo><mi id="S3.Ex3.m1.16.16.7.1.1.1.1.2.2"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.2.cmml">l</mi></mrow><mo id="S3.Ex3.m1.16.16.7.1.1.1.1.1"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.1.cmml">âˆˆ</mo><mi id="S3.Ex3.m1.16.16.7.1.1.1.1.3"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.3.cmml">ğ•ƒ</mi></mrow><mo id="S3.Ex3.m1.17.17.8.2.2.2.3"
    xref="S3.Ex3.m1.17.17.8.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.17.17.8.2.2.2.2"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2.cmml"><mi id="S3.Ex3.m1.17.17.8.2.2.2.2.2" xref="S3.Ex3.m1.17.17.8.2.2.2.2.2.cmml">i</mi><mo
    id="S3.Ex3.m1.17.17.8.2.2.2.2.1" xref="S3.Ex3.m1.17.17.8.2.2.2.2.1.cmml">âˆ‰</mo><msup
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.17.17.8.2.2.2.2.3.2"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.2.cmml">ğ•Š</mi><mi id="S3.Ex3.m1.17.17.8.2.2.2.2.3.3"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr></mtable><mo
    id="S3.Ex3.m1.18.18.1.1.3.2.2.2" xref="S3.Ex3.m1.17.17.cmml">)</mo></mrow></mrow></mrow><mo
    lspace="0em" id="S3.Ex3.m1.18.18.1.2" xref="S3.Ex3.m1.18.18.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.Ex3.m1.18b"><apply id="S3.Ex3.m1.18.18.1.1.cmml"
    xref="S3.Ex3.m1.18.18.1"><ci id="S3.Ex3.m1.18.18.1.1.2.cmml" xref="S3.Ex3.m1.18.18.1.1.2">ğ’Ÿ</ci><apply
    id="S3.Ex3.m1.18.18.1.1.3.cmml" xref="S3.Ex3.m1.18.18.1.1.3"><apply id="S3.Ex3.m1.18.18.1.1.3.1.cmml"
    xref="S3.Ex3.m1.18.18.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.18.18.1.1.3.1.1.cmml"
    xref="S3.Ex3.m1.18.18.1.1.3.1">subscript</csymbol><apply id="S3.Ex3.m1.9.9.9.cmml"
    xref="S3.Ex3.m1.9.9.9"><vector id="S3.Ex3.m1.7.7.7.7.3.cmml" xref="S3.Ex3.m1.7.7.7.7.2"><apply
    id="S3.Ex3.m1.6.6.6.6.1.1.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.6.6.6.6.1.1.1.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1">superscript</csymbol><ci
    id="S3.Ex3.m1.6.6.6.6.1.1.2.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1.2">ğ•Š</ci><cn type="integer"
    id="S3.Ex3.m1.6.6.6.6.1.1.3.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1.3">1</cn></apply><ci
    id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1">â€¦</ci><apply id="S3.Ex3.m1.7.7.7.7.2.2.cmml"
    xref="S3.Ex3.m1.7.7.7.7.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.7.7.2.2.1.cmml"
    xref="S3.Ex3.m1.7.7.7.7.2.2">superscript</csymbol><ci id="S3.Ex3.m1.7.7.7.7.2.2.2.cmml"
    xref="S3.Ex3.m1.7.7.7.7.2.2.2">ğ•Š</ci><ci id="S3.Ex3.m1.7.7.7.7.2.2.3.cmml" xref="S3.Ex3.m1.7.7.7.7.2.2.3">ğ¿</ci></apply></vector><apply
    id="S3.Ex3.m1.9.9.9.9.cmml" xref="S3.Ex3.m1.9.9.9.9"><set id="S3.Ex3.m1.8.8.8.8.1.2.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1"><cn type="integer" id="S3.Ex3.m1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2">1</cn><ci
    id="S3.Ex3.m1.3.3.3.3.cmml" xref="S3.Ex3.m1.3.3.3.3">â€¦</ci><apply id="S3.Ex3.m1.8.8.8.8.1.1.1.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.8.8.1.1.1.1.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.8.8.8.8.1.1.1.2.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.2">ğ‘›</ci><cn type="integer" id="S3.Ex3.m1.8.8.8.8.1.1.1.3.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.3">1</cn></apply></set><ci id="S3.Ex3.m1.9.9.9.9.4.cmml"
    xref="S3.Ex3.m1.9.9.9.9.4">â€¦</ci><set id="S3.Ex3.m1.9.9.9.9.2.2.cmml" xref="S3.Ex3.m1.9.9.9.9.2.1"><cn
    type="integer" id="S3.Ex3.m1.4.4.4.4.cmml" xref="S3.Ex3.m1.4.4.4.4">1</cn><ci
    id="S3.Ex3.m1.5.5.5.5.cmml" xref="S3.Ex3.m1.5.5.5.5">â€¦</ci><apply id="S3.Ex3.m1.9.9.9.9.2.1.1.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.9.9.9.9.2.1.1.1.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1">subscript</csymbol><ci id="S3.Ex3.m1.9.9.9.9.2.1.1.2.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1.2">ğ‘›</ci><ci id="S3.Ex3.m1.9.9.9.9.2.1.1.3.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1.3">ğ¿</ci></apply></set></apply></apply></apply><matrix
    id="S3.Ex3.m1.17.17.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><matrixrow id="S3.Ex3.m1.17.17a.cmml"
    xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.11.11.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1"><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2"><apply id="S3.Ex3.m1.11.11.2.3.1.2.2.cmml"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2"><ci id="S3.Ex3.m1.11.11.2.3.1.2.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.1">â‹…</ci><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.2.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2">superscript</csymbol><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2">subscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2">ğ’˜</ci><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3">ğ‘–</ci></apply><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.3">ğ‘™</ci></apply><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.2">ğ’‰</ci><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3"><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2">ğ‘™</ci><cn
    type="integer" id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3">1</cn></apply></apply></apply><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3">superscript</csymbol><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.3.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3">subscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.2">ğ‘</ci><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.3">ğ‘–</ci></apply><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.3.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3.3">ğ‘™</ci></apply></apply><cn
    type="integer" id="S3.Ex3.m1.11.11.2.3.1.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.3">0</cn></apply><apply
    id="S3.Ex3.m1.11.11.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.11.11.2.2.2.3a.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.3">formulae-sequence</csymbol><apply
    id="S3.Ex3.m1.10.10.1.1.1.1.1.cmml" xref="S3.Ex3.m1.10.10.1.1.1.1.1"><apply id="S3.Ex3.m1.10.10.1.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.10.10.1.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.10.10.1.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.2">ğ‘™</ci></apply><ci id="S3.Ex3.m1.10.10.1.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.3">ğ•ƒ</ci></apply><apply id="S3.Ex3.m1.11.11.2.2.2.2.2.cmml"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2"><ci id="S3.Ex3.m1.11.11.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.2">ğ‘–</ci><apply
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.2">ğ•Š</ci><ci
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.3">ğ‘™</ci></apply></apply></apply></matrixrow><matrixrow
    id="S3.Ex3.m1.17.17b.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.13.13.4.3.1.cmml"
    xref="S3.Ex3.m1.13.13.4.3.1"><apply id="S3.Ex3.m1.13.13.4.3.1.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2">superscript</csymbol><apply
    id="S3.Ex3.m1.13.13.4.3.1.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.13.13.4.3.1.2.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2">subscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2.2.2">â„</ci><ci
    id="S3.Ex3.m1.13.13.4.3.1.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2.2.3">ğ‘–</ci></apply><ci
    id="S3.Ex3.m1.13.13.4.3.1.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2.3">ğ‘™</ci></apply><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3"><apply id="S3.Ex3.m1.13.13.4.3.1.3.2.cmml"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2"><ci id="S3.Ex3.m1.13.13.4.3.1.3.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.1">â‹…</ci><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.2.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2">superscript</csymbol><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2">subscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2">ğ’˜</ci><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3">ğ‘–</ci></apply><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.3">ğ‘™</ci></apply><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.2.3.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.2">ğ’‰</ci><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3"><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2">ğ‘™</ci><cn
    type="integer" id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3">1</cn></apply></apply></apply><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.3.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3">superscript</csymbol><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.3.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3">subscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.2">ğ‘</ci><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.3">ğ‘–</ci></apply><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3.3">ğ‘™</ci></apply></apply></apply><apply
    id="S3.Ex3.m1.13.13.4.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.13.13.4.2.2.3a.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.3">formulae-sequence</csymbol><apply
    id="S3.Ex3.m1.12.12.3.1.1.1.1.cmml" xref="S3.Ex3.m1.12.12.3.1.1.1.1"><apply id="S3.Ex3.m1.12.12.3.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.12.12.3.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.12.12.3.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.2">ğ‘™</ci></apply><ci id="S3.Ex3.m1.12.12.3.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.3">ğ•ƒ</ci></apply><apply id="S3.Ex3.m1.13.13.4.2.2.2.2.cmml"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2"><ci id="S3.Ex3.m1.13.13.4.2.2.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.2">ğ‘–</ci><apply
    id="S3.Ex3.m1.13.13.4.2.2.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.2">ğ•Š</ci><ci
    id="S3.Ex3.m1.13.13.4.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.3">ğ‘™</ci></apply></apply></apply></matrixrow><matrixrow
    id="S3.Ex3.m1.17.17c.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.15.15.6.3.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1"><apply id="S3.Ex3.m1.15.15.6.3.1.2.cmml" xref="S3.Ex3.m1.15.15.6.3.1.2"><apply
    id="S3.Ex3.m1.15.15.6.3.1.2.2.cmml" xref="S3.Ex3.m1.15.15.6.3.1.2.2"><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.1">â‹…</ci><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.2.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2">superscript</csymbol><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2">subscript</csymbol><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2">ğ’˜</ci><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3">ğ‘–</ci></apply><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.3">ğ‘™</ci></apply><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.2.3.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3">superscript</csymbol><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.3.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.2">â„</ci><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3"><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2">ğ‘™</ci><cn type="integer" id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3">1</cn></apply></apply></apply><apply id="S3.Ex3.m1.15.15.6.3.1.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.3.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3">superscript</csymbol><apply id="S3.Ex3.m1.15.15.6.3.1.2.3.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.3.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3">subscript</csymbol><ci id="S3.Ex3.m1.15.15.6.3.1.2.3.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.2">ğ‘</ci><ci id="S3.Ex3.m1.15.15.6.3.1.2.3.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.3">ğ‘–</ci></apply><ci id="S3.Ex3.m1.15.15.6.3.1.2.3.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.3">ğ‘™</ci></apply></apply><cn type="integer" id="S3.Ex3.m1.15.15.6.3.1.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.3">0</cn></apply><apply id="S3.Ex3.m1.15.15.6.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.2.2.3a.cmml"
    xref="S3.Ex3.m1.15.15.6.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex3.m1.14.14.5.1.1.1.1.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1"><apply id="S3.Ex3.m1.14.14.5.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.14.14.5.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.14.14.5.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.2">ğ‘™</ci></apply><ci id="S3.Ex3.m1.14.14.5.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.3">ğ•ƒ</ci></apply><apply id="S3.Ex3.m1.15.15.6.2.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2"><ci id="S3.Ex3.m1.15.15.6.2.2.2.2.2.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.2">ğ‘–</ci><apply
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.15.15.6.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.2">ğ•Š</ci><ci
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.3">ğ‘™</ci></apply></apply></apply></matrixrow><matrixrow
    id="S3.Ex3.m1.17.17d.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.17.17.8.3.1.cmml"
    xref="S3.Ex3.m1.17.17.8.3.1"><apply id="S3.Ex3.m1.17.17.8.3.1.2.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.17.17.8.3.1.2.1.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2">superscript</csymbol><apply
    id="S3.Ex3.m1.17.17.8.3.1.2.2.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.17.17.8.3.1.2.2.1.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2">subscript</csymbol><ci
    id="S3.Ex3.m1.17.17.8.3.1.2.2.2.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2.2.2">â„</ci><ci
    id="S3.Ex3.m1.17.17.8.3.1.2.2.3.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2.2.3">ğ‘–</ci></apply><ci
    id="S3.Ex3.m1.17.17.8.3.1.2.3.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2.3">ğ‘™</ci></apply><cn
    type="integer" id="S3.Ex3.m1.17.17.8.3.1.3.cmml" xref="S3.Ex3.m1.17.17.8.3.1.3">0</cn></apply><apply
    id="S3.Ex3.m1.17.17.8.2.2.3.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.17.17.8.2.2.3a.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.3">formulae-sequence</csymbol><apply
    id="S3.Ex3.m1.16.16.7.1.1.1.1.cmml" xref="S3.Ex3.m1.16.16.7.1.1.1.1"><apply id="S3.Ex3.m1.16.16.7.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.16.16.7.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.16.16.7.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.2">ğ‘™</ci></apply><ci id="S3.Ex3.m1.16.16.7.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.3">ğ•ƒ</ci></apply><apply id="S3.Ex3.m1.17.17.8.2.2.2.2.cmml"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2"><ci id="S3.Ex3.m1.17.17.8.2.2.2.2.2.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.2">ğ‘–</ci><apply
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.17.17.8.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.2">ğ•Š</ci><ci
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.3">ğ‘™</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.Ex3.m1.18c">\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ {\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq
    0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\ h_{i}^{l}=0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right).</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'Such partitioning entails an overlap between adjacent linear regions when ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    i.e., at the boundary in which unit $i$ in layer $l$ is active in one region and
    inactive in another. Nevertheless, for any input $\overline{{\bm{x}}}$ associated
    with a point at such a boundary between two linear regions ${\mathbb{I}}_{1}$
    and ${\mathbb{I}}_{2}$, it holds that ${\bm{y}}_{{\mathbb{I}}_{1}}(\overline{{\bm{x}}})={\bm{y}}_{{\mathbb{I}}_{2}}(\overline{{\bm{x}}})$
    even if those affine transformations are not entirely identical since the output
    of the neural network is continuous. More importantly, such overlap implies that
    each term of $\mathcal{D}$ is defined using only equalities and nonstrict inequalities,
    and therefore that each linear region corresponds to polyhedra in the extended
    space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$. Consequently, those linear
    regions also define polyhedra if projected to the input space ${\bm{x}}$, since
    by using Fourier-Motzkin elimination (Fourier, [1826](#bib.bib107), Motzkin, [1936](#bib.bib226))
    we can obtain a polyhedral description of the linear region in ${\bm{x}}$. Moreover,
    the interior of those polyhedra are disjoint. If one of those polyhedra does not
    have an interior, which means that it is not full-dimensional, then that linear
    region lies entirely on the boundary of other linear regions. In such a case,
    we do not regard it as a proper linear region. By looking at the geometry of those
    linear regions from a different perspective in SectionÂ [3.3](#S3.SS3 "3.3 The
    geometry of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey") and understanding its impact on the
    number of linear regions in SectionÂ [3.4](#S3.SS4 "3.4 The number of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), we will see that many terms of $\mathcal{D}$ may actually
    be empty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization over the union of polyhedra is the subject of disjunctive
    programming, which has contributed to the development of stronger formulations
    and better algorithms to solve discrete optimization problems. These are formulated
    as MILPs as well as more general types of problems in recent years (Balas, [2018](#bib.bib12)),
    including generalized disjunctive programming for Mixed-Integer Non-Linear ProgrammingÂ (MINLP)
    (Raman and Grossmann, [1994](#bib.bib256), Grossmann and Ruiz, [2012](#bib.bib134)).
    One of such contributions is the generation of valid inequalities to strengthen
    MILP formulations, which are also denoted as cutting planes, through the lift-and-project
    technique (Balas etÂ al., [1993](#bib.bib13), [1996](#bib.bib14)). In fact, we
    can develop stronger formulations for optimization problems involving neural networks
    through the lenses of disjunctive programming, as we discuss later in SectionÂ [4.2](#S4.SS2
    "4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 The geometry of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another form of looking at the geometry of linear regions is through their transformation
    along the layers of the neural network. Namely, we can think of the input space
    as initially being partitioned by the units of the first layer, and then each
    resulting linear region being further partitioned by the subsequent layers. In
    that sense, we can think of every layer as a particular form of â€œslicingâ€ the
    input. In fact, a layer may slice each linear region that is defined by the preceding
    layer in a different way due to which neurons are active or not in previous layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us begin by illustrating how a given layer $l\in{\mathbb{L}}$ partitions
    its input space ${\bm{h}}^{l-1}$. Every neuron $i$ in layer $l$ is associated
    with an *activation hyperplane* of the form ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    which divides the possible inputs of its layer into an open half-space in which
    the unit is active (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}>0$) and a closed
    half-space in which the unit is inactive (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\leq
    0$). These hyperplanes define the boundary between adjacent linear regions, and
    the arrangement of such hyperplanes for a given layer $l\in{\mathbb{L}}$ determines
    how that layer partitions the ${\bm{h}}^{l-1}$ space. In other words, every input
    in ${\bm{h}}^{l-1}$ can be located with respect to each of those hyperplanes,
    which corresponds to the activation set of the linear region to which it belongs.
    However, not every activation set out of the $2^{n_{l}}$ possible ones maps to
    a nonempty region of the input space. In the case of ExampleÂ [2](#Thmexample2
    "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), there is no
    linear region in which the activation set is empty.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and a single
    layer having 3 neurons $\alpha$, $\beta$, and $\gamma$ with outputs given as follows:
    $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+0.6,0\}$, $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+4.8,0\}$,
    and $h^{1}_{\gamma}=\max\{0x_{1}+3x_{2}-5,0\}$. These neurons define the activation
    hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+0.6=0$, ($\beta$) $-0.9x_{1}-0.7x_{2}+4.8=0$,
    and ($\gamma$) $0x_{1}+3x_{2}-5=0$ in the space ${\bm{x}}$, which are illustrated
    along with the activation sets of the linear regions in FigureÂ [5](#S3.F5 "Figure
    5 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of $2^{3}$ linear regions corresponding to each possible activation
    set defined by a subset of the neurons in $\{\alpha,\beta,\gamma\}$, the arrangement
    of such hyperplanes produces 7 linear regions, which is in fact the maximum number
    of 2-dimensional regions that can be defined by drawing 3 lines on a plane.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/275229d48fa5bb1b250445d5df8fa0b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Linear regions defined by the shallow neural network described in
    ExampleÂ [2](#Thmexample2 "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The
    Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey"). Every line corresponds to the activation hyperplane of a different
    neuron, which is given by $\alpha$, $\beta$, and $\gamma$ in parentheses. The
    arrow next to each line points to the half space in which the inputs activate
    that neuron. Every linear region has a subset of $\{\alpha,\beta,\gamma\}$ as
    its corresponding activation set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum number of full-dimensional regions resulting from a partitioning
    defined by $n$ hyperplanes depends on the dimension $d$ of the space in which
    those hyperplanes are defined (Zaslavsky, [1975](#bib.bib352)). That number never
    exceeds $\sum\limits_{i=1}^{\min\{d,n\}}\binom{n}{i}$. Such bound only coincides
    with $2^{n}$ if $d\geq n$; otherwise, as illustrated in ExampleÂ [2](#Thmexample2
    "Example 2 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), that number
    can be smaller. As observed byÂ Hanin and Rolnick ([2019b](#bib.bib138)), that
    bound is $O\left(\frac{n^{d}}{d!}\right)$ when $n\gg d$.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the above bound is all that we need to determine the maximum number
    of linear regions in shallow networks. While not every shallow network may define
    as many linear regions, it is always possible to put the hyperplanes in what is
    called a *general position* in order to reach that bound. Thus, the maximum number
    of linear regions defined by a shallow network is $\sum\limits_{i=0}^{\min\{n_{0},n_{1}\}}\binom{n_{1}}{n_{0}}$.
  prefs: []
  type: TYPE_NORMAL
- en: For the polyhedron associated with each linear region, being in general position
    implies that each vertex lies on exactly $d$ activation hyperplanes. For context,
    the converse situation in linear programming â€”having more hyperplanes active on
    a vertex than the space dimensionâ€” characterizes degeneracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of deep networks, the partitioning of each linear region by the
    subsequent layers is based on the output of that linear region. This affects the
    shape and the number of the linear regions defined by the following layers, which
    may vary between adjacent linear regions due to which units are active or inactive
    from one linear region to another, as illustrated in ExampleÂ [3](#Thmexample3
    "Example 3 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and 2 layers
    having 2 neurons each â€”say neurons $\alpha$ and $\beta$ in layer 1, and neurons
    $\gamma$ and $\delta$ in layer 2â€” with outputs given as follows: $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+1.5,0\}$,
    $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+5,0\}$, $h^{2}_{\gamma}=\max\{0.4h^{1}_{1}-3.1h^{1}_{2}+4,0\}$,
    $h^{2}_{\delta}=\max\{-0.6h^{1}_{1}-1.6h^{1}_{2}+5,0\}$. These neurons define
    the activation hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+1.5=0$ and ($\beta$)
    $-0.9x_{1}-0.7x_{2}+5=0$ in the ${\bm{x}}$ space and the activation hyperplanes
    ($\gamma$) $0.4h^{1}_{1}-3.1h^{1}_{2}+4=0$ and ($\delta$) $-0.6h^{1}_{1}-1.6h^{1}_{2}+5=0$
    in the ${\bm{h}}^{1}$ space, which are illustrated along with the activation sets
    of the linear regions in the first two plots of FigureÂ [6](#S3.F6 "Figure 6 â€£
    3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). The third plot illustrates
    the linear regions jointly defined by the two layers in terms of the input space
    ${\bm{x}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third plot is repeated in FigureÂ [7](#S3.F7 "Figure 7 â€£ 3.3 The geometry
    of linear regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"), in which the shape of each linear region
    ${\mathbb{I}}$ is filled in accordance to the dimension of the image of $\bar{{\bm{y}}}_{{\mathbb{I}}}({\bm{x}})$
    â€”the output of the neural network for each linear region ${\mathbb{I}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4b21733c24f690a28025ee4d147bf0d.png)![Refer to caption](img/66f9cc2c9bd7fbe539dcffe16058d0b5.png)![Refer
    to caption](img/7fc94c543268674eb63d5ca496258400.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Linear regions defined by the 2 layers of the neural network described
    in ExampleÂ [3](#Thmexample3 "Example 3 â€£ 3.3 The geometry of linear regions â€£
    3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), following the same notation as in FigureÂ [5](#S3.F5 "Figure
    5 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). The first and second
    plots show the linear regions and corresponding activation sets defined by the
    first and the second layers in terms of their input spaces (${\bm{x}}$ and ${\bm{h}}^{1}$).
    The third plot shows the linear regions defined by the combination of the 2 layers
    and the union of their activation sets in terms of the input space of the first
    layer (${\bm{x}}$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ExampleÂ [3](#Thmexample3 "Example 3 â€£ 3.3 The geometry of linear regions â€£
    3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") highlights two important aspects about the structure of linear
    regions in deep neural networks. First, the linear regions defined by a neural
    network with multiple layers are different because activation hyperplanes after
    the first layer may look â€œbentâ€ from the input space $x$, such as with the inflections
    of hyperplanes $(\gamma)$ and $(\delta)$ in the third plot of FigureÂ [6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") from one linear
    region defined by the first layer to another. This partitioning of the input space
    would not be possible with a single layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing side by side the first and the third plots of FigureÂ [6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), we can see how
    every linear region of a given layer of a neural network may be partitioned differently
    by the following layer. When defined in terms of the input space ${\bm{x}}$, the
    hyperplanes associated with the second layer differ across the linear regions
    defined by the first layer because each of those linear regions is associated
    with a different affine transformation from ${\bm{x}}$ to ${\bm{h}}^{1}$. Hence,
    the activation hyperplanes of layer $l$ may break each linear region from layer
    $l-1$ differently. To every linear region defined by the hyperplane arrangement
    in the ${\bm{h}}^{l-1}$ space there is a linear transformation ${\bm{h}}^{l-1}=\Omega^{{\mathbb{S}}^{l-1}}({\bm{W}}^{l-1}{\bm{h}}^{l-2}+{\bm{b}}^{l-1})$
    to the points of that linear region based on the set of active neurons ${\mathbb{S}}^{l-1}$.
    Consequently, inputs in the ${\bm{h}}^{l-1}$ space that are associated with different
    linear regions are transformed differently to the ${\bm{h}}^{l}$ space, and therefore
    the form in which those linear regions are further partitioned by layer $l$ is
    not the same when seen from the ${\bm{h}}^{l-1}$ space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, some combinations of activation sets of multiple layers do not correspond
    to linear regions even if the activation hyperplanes are in general position with
    respect to each layer. For each layer, the first two plots of FigureÂ [6](#S3.F6
    "Figure 6 â€£ 3.3 The geometry of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") show that every
    activation set corresponds to a nonempty region of the layer input. However, not
    every pair of such activation sets would define a nonempty linear region for the
    neural network. For example, the linear region of the first layer associated with
    the activation set ${\mathbb{S}}^{1}=\{\}$ defines a linear region in ${\bm{x}}$
    which is always mapped to ${\bm{h}}^{1}=0$, and thus only corresponds to activation
    set ${\mathbb{S}}^{2}=\{\gamma,\delta\}$ in the second layer because both units
    are active for such input. Thus, no linear region in ${\bm{x}}$ is associated
    with only the units in sets $\{\},\{\gamma\}$, and $\{\delta\}$ being active â€”i.e.,
    there is no linear region such that ${\mathbb{S}}^{1}\cup{\mathbb{S}}^{2}=\{\},\{\gamma\},\text{or}\{\delta\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: More generally, the number of units that is active on each linear region defined
    by the first layer also imposes a geometric limit to how that linear region can
    be further partitioned. If only one unit is active at a layer, that means that
    the output of the layer within that linear region has dimension 1, and, consequently,
    the subsequent hyperplane arrangements within that linear region are limited to
    a 1-dimensional space. For the network in the example, we thus expect no more
    than $\sum_{i=0}^{1}\binom{2}{i}=3$ linear regions being defined instead of $2^{2}=4$
    when only one unit is active. In fact, that is precisely the number of subdivisions
    by the second layer of the linear region defined by activation set ${\mathbb{S}}^{1}=\{\beta\}$
    from the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/738562b7813b118f7f87ea0943be8d01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Dimension of the image of the affine function ${\bm{y}}_{{\mathbb{I}}}({\bm{x}})$
    associated with each linear region ${\mathbb{I}}$ defined by the neural network
    described in ExampleÂ [3](#Thmexample3 "Example 3 â€£ 3.3 The geometry of linear
    regions â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"). The linear regions are the same illustrated in
    the third plot of FigureÂ [6](#S3.F6 "Figure 6 â€£ 3.3 The geometry of linear regions
    â€£ 3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'For every linear region defined by layer $l$ with an activation set ${\mathbb{S}}^{l}$,
    the dimension of the output of the corresponding transformation $\Omega_{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$
    is at most $|{\mathbb{S}}^{l}|$ since $\text{rank}(\Omega_{{\mathbb{S}}^{l}})=|{\mathbb{S}}^{l}|$.
    Hence, the dimension of the output of every linear region defined by a rectifier
    network is upper bounded by its smallest activation set across all layers. This
    phenomenon was first identified byÂ Serra etÂ al. ([2018](#bib.bib282)) as the *bottleneck
    effect*. In neural networks with uniform width, this phenomenon leads to a surprising
    consequence: the number of linear regions with full-dimensional output is at most
    one. There are also consequences to the maximum number of linear regions that
    can be defined, as we discuss later.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 The geometry of decision regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is also common to study what inputs are associated with each class by a neural
    network. The set of inputs associated with the same class define a *decision region*.
    Difficulties in modeling functions such as the Boolean XOR in shallow networks
    are related to limitations on the form of the decision regions, which may be limited
    by the depth of the neural network. For example, Makhoul etÂ al. ([1989](#bib.bib207))
    showed that two layers suffice to obtain disconnected decision regions.
  prefs: []
  type: TYPE_NORMAL
- en: The softmax layer is typically used for the output of neural networks trained
    on classification problems, in which the largest output corresponds to the class
    to which the input is associated. In rectifier networks coupled with a softmax
    layer, the decision regions can also be defined by polyhedra. Although the output
    of the softmax layer is not piecewise linear, its largest output corresponds to
    its largest input. Hence, every linear region ${\mathbb{I}}$ defined by layers
    1 to $L-1$ is partitioned by the softmax layer into decision regions where ${\bm{h}}^{L-1}_{i}\geq{\bm{h}}^{L-1}_{j}~{}\forall
    j\neq i$ for each class $i$ associated with the input ${\bm{h}}_{i}^{L-1}$ to
    the softmax layer. Therefore, each decision region of a rectifier networks consist
    of a union of polyhedra.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we may say further in the typical setting where no hidden layer is
    wider than the input â€”i.e., $n_{0}\geq n_{l}~{}\forall l\in{\mathbb{L}}$: Nguyen
    etÂ al. ([2018](#bib.bib233)) showed that at least one layer $l\in{\mathbb{L}}$
    must be such that $n_{l}>n_{0}$ for the network to present disconnected decision
    regions; and Grigsby and Lindsey ([2022](#bib.bib131)) showed that, for an input
    size $n_{0}\geq 2$, the decision regions are either empty or unbounded.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 The number of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have seen conditions that affect the number of linear regions both positively
    and negatively. We discuss these and other analytical results in SectionÂ [3.4.1](#S3.SS4.SSS1
    "3.4.1 Analytical results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"),
    and then discuss work on counting linear regions in practice in SectionÂ [3.4.2](#S3.SS4.SSS2
    "3.4.2 Counting linear regions â€£ 3.4 The number of linear regions â€£ 3 The Linear
    Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Analytical results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At least three lines of work on analytical results have brought important insights.
    The first line is based on constructing networks with a large number of linear
    regions, which leads to lower bounds on the maximum number of linear regions.
    The second line is based on showing how the network architecture â€”in particular
    its hyperparametersâ€” may impact the hyperplane arrangements defined by the layers,
    which leads to upper bounds on the maximum number of linear regions. The third
    line is based on characterizing the parameters of neural networks according to
    how they are initialized and updated along training, which leads to results on
    the expected number of linear regions for such networks.
  prefs: []
  type: TYPE_NORMAL
- en: Lower bounds
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The lower bounds on the maximum number of linear regions are obtained through
    a careful choice of network parameters aimed at increasing the number of linear
    regions. In some cases, they also depend on particular choices of hyperparameters.
    We present them by order of refinement in TableÂ [2](#S3.T2 "Table 2 â€£ Lower bounds
    â€£ 3.4.1 Analytical results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first lower bound was introduced byÂ Pascanu etÂ al. ([2014](#bib.bib243))
    and then improved by those authors with a new construction technique inÂ MontÃºfar
    etÂ al. ([2014](#bib.bib224)). In fact, ExampleÂ [1](#Thmexample1 "Example 1 â€£ 3.1
    The combinatorial aspect of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") shows the case
    in which $n_{0}=1$ for the technique inÂ MontÃºfar etÂ al. ([2014](#bib.bib224)).
    While a different construction is proposed byÂ Telgarsky ([2015](#bib.bib304)),
    subsequent developments in the literature have been based onÂ MontÃºfar etÂ al. ([2014](#bib.bib224)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower bound byÂ Arora etÂ al. ([2018](#bib.bib8)) is based on a different
    technique to construct a first wide layer based on zonotopes, which is then followed
    by the same layers as inÂ MontÃºfar etÂ al. ([2014](#bib.bib224)). The first lower
    bound byÂ Serra etÂ al. ([2018](#bib.bib282)) reflects a slight change to the technique
    used byÂ MontÃºfar etÂ al. ([2014](#bib.bib224)), which in terms of ExampleÂ [1](#Thmexample1
    "Example 1 â€£ 3.1 The combinatorial aspect of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") corresponds
    to using $n$ neurons to define $n+1$ instead of $n$ slopes on $[0,1]$. The second
    lower bound byÂ Serra etÂ al. ([2018](#bib.bib282)) extends that ofÂ Arora etÂ al.
    ([2018](#bib.bib8)) by changing in the same way the construction of the subsequent
    layers of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Lower bounds on the maximum number of linear regions defined by a
    neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Bound and conditions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Â Pascanu etÂ al. ([2014](#bib.bib243)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Â MontÃºfar etÂ al. ([2014](#bib.bib224)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Â Telgarsky ([2015](#bib.bib304)) | $2^{\frac{L-3}{2}}$, where $n_{i}=1$ for
    $i$ odd, $n_{i}=2$ for $i$ even, and $L-3$ divides by 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Â Arora etÂ al. ([2018](#bib.bib8)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}w^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Â Serra etÂ al. ([2018](#bib.bib282)) | $\left(\prod\limits_{l=1}^{L-1}\left(\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor+1\right)^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Â Serra etÂ al. ([2018](#bib.bib282)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}(w+1)^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$
    |'
  prefs: []
  type: TYPE_TB
- en: Upper bounds
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The upper bounds on the maximum number of linear regions are obtained by primarily
    considering changes to the geometry of the linear regions from one layer to another,
    as previously outlined and revisited below. We present those with a close form
    by order of refinement in TableÂ [3](#S3.T3 "Table 3 â€£ Upper bounds â€£ 3.4.1 Analytical
    results â€£ 3.4 The number of linear regions â€£ 3 The Linear Regions of a Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Pascanu etÂ al. ([2014](#bib.bib243)) established the connection between linear
    regions and hyperplane arrangements, which lead to the tight bound for shallow
    networks based onÂ Zaslavsky ([1975](#bib.bib352)) for activation hyperplanes in
    general position. MontÃºfar etÂ al. ([2014](#bib.bib224)) defined the first bound
    for deep networks based on enumerating all activation sets. The subsequent upper
    bounds extended the result byÂ Pascanu etÂ al. ([2014](#bib.bib243)) to deep networks
    by considering its successive application through the sequence of layers of the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of *deep* networks, where $L>1$, we need to consider how the linear
    regions defined up to a given layer of the network can be further partitioned
    by the next layers. We start by assuming that every linear region defined by the
    first $l-1$ layers is then subdivided into the maximum possible number of linear
    regions defined by the activation hyperplanes of layer $l$. That leads to the
    bound inÂ Raghu etÂ al. ([2017](#bib.bib253)), which is implicit in their proof
    of an asymptotic bound of $O(n^{n_{0}L})$, where $n$ is used as the width of every
    layer. However, there are many ways in which this bound can be refined upon careful
    examination. First, the dimension of the input of layer $l$ â€”i.e., the output
    of layer $l-1$â€” within each linear region is never larger than the smallest dimension
    among layers $1$ to $l$, since for every linear region we have an affine transformation
    between inputs and outputs of each layer (MontÃºfar, [2017](#bib.bib223)). Second,
    the dimension of the input coming through each linear region is in fact bounded
    by the smallest number of active units in each of the previous layers (Serra etÂ al.,
    [2018](#bib.bib282)). This leads to a tight upper bound for $n_{0}=1$, since it
    matches the lower bound in Â Serra etÂ al. ([2018](#bib.bib282)). Finally, the activation
    hyperplane of some units may not partition the linear regions because all possible
    inputs to the unit are in the same half-space, and in some of those cases the
    unit may never produce a positive output. For the number $k$ of active units in
    a given layer $l$, we can use the network parameters to calculate the maximum
    number of units that can be active in the next layer, $\mathcal{A}_{l}(k)$, as
    well as the number of units that can be active or inactive for different inputs,
    $\mathcal{I}_{l}(k)$ (Serra and Ramalingam, [2020](#bib.bib281)).
  prefs: []
  type: TYPE_NORMAL
- en: Hinz and vanÂ de Geer ([2019](#bib.bib150)) observed that the upper bound by
    Serra etÂ al. ([2018](#bib.bib282)) can be tightened by explicitly computing a
    recursive histogram of linear regions on the layers of the neural network according
    to the dimension of their image subspace. However, the resulting bound is not
    explicitly defined in terms of the network hyperparameters, and hence cannot be
    included on the table. This work is further extended inÂ Hinz ([2021](#bib.bib149))
    by also allowing a composition of bounds on subnetworks instead of only on the
    sequence of layers. Another extension of the framework fromÂ Hinz and vanÂ de Geer
    ([2019](#bib.bib150)) by Xie etÂ al. ([2020c](#bib.bib344)) highlights that residual
    connections prevent the bottleneck effect in ResNets, by which reason such networks
    tend to have more linear regions.
  prefs: []
  type: TYPE_NORMAL
- en: Cai etÂ al. ([2023](#bib.bib46)) proposed a separate recursive bound based on
    Serra etÂ al. ([2018](#bib.bib282)) to account for the sparsity of the weight matrices,
    which illustrates how pruning connections may affect the maximum number of linear
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results above have also been extended to other architectures. In some cases,
    results on other types of activations are also part of the papers previously mentioned:
    MontÃºfar etÂ al. ([2014](#bib.bib224)) and Serra etÂ al. ([2018](#bib.bib282)) present
    upper bounds for *maxout* networks; Raghu etÂ al. ([2017](#bib.bib253)) present
    an upper bound for networks using *hard tanh* activation. In other cases, the
    ideas discussed above have been adapted for sparser networks with parameter sharing:
    Xiong etÂ al. ([2020](#bib.bib345)) present upper and lower bounds for convolutional
    networks, which are shown to asymptotically define more linear regions per parameter
    than rectifier networks with the same input size and number of layers. Chen etÂ al.
    ([2022a](#bib.bib50)) present upper and lower bounds for graph convolutional networks.
    Matoba etÂ al. ([2022](#bib.bib214)) discuss the expresiveness of the maxpooling
    layers typically used in convolutional neural networks through their equivalence
    to a sequence of rectifier layers. Moreover, Goujon etÂ al. ([2022](#bib.bib128))
    present results for recently proposed activation functions, such as DeepSplineÂ (Agostinelli
    etÂ al., [2015](#bib.bib2), Unser, [2019](#bib.bib314), Bohra etÂ al., [2020](#bib.bib34))
    and GroupSortÂ (Anil etÂ al., [2019](#bib.bib6)).'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the results above were also revisited through the lenses of tropical
    algebra, in which every linear region corresponds to a tropical hypersurface (Zhang
    etÂ al., [2018b](#bib.bib357), Charisopoulos and Maragos, [2018](#bib.bib48), Maragos
    etÂ al., [2021](#bib.bib212)). Notably, MontÃºfar etÂ al. ([2022](#bib.bib225)) presented
    considerably tighter upper bounds for the number of linear regions in maxout networks
    with rank $k=3$ or greater.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, a converse line of work started exploring the minimum dimensions of
    a neural network capable of representing a given piecewise linear function, starting
    with considerations about the minimum depth necessary (Arora etÂ al., [2018](#bib.bib8))
    and further refinements of bounds on the network dimensions (He etÂ al., [2020](#bib.bib142),
    Hertrich etÂ al., [2021](#bib.bib147), Chen etÂ al., [2022b](#bib.bib51)), with
    Chen etÂ al. ([2022b](#bib.bib51)) proposing an algorithm that can construct such
    a neural network. On a related note, Karg and Lucia ([2020](#bib.bib168)) show
    that linear time-invariant systems in model predictive control can be exactly
    expressed by rectifier networks and provide bounds on the width and number of
    layers necessary for a given system, whereas Ferlez and Shoukry ([2020](#bib.bib103))
    describe an algorithm for producing architectures that can be parameterized as
    an optimal model predictive control strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Upper bounds on the maximum number of linear regions defined by a
    neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Bound and conditions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Â Pascanu etÂ al. ([2014](#bib.bib243)) | $\sum\limits_{i=0}^{n_{0}}\binom{n_{1}}{n_{0}}$
    for shallow networks, $n_{1}\geq n_{0}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Â MontÃºfar etÂ al. ([2014](#bib.bib224)) | $2^{\sum\limits_{l=1}^{L}n_{l}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Â Raghu etÂ al. ([2017](#bib.bib253)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{n_{l-1}}\binom{n_{l}}{j}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Â MontÃºfar ([2017](#bib.bib223)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{d_{l}}\binom{n_{l}}{j}$,
    $d_{l}=\min\{n_{0},n_{1},\ldots,n_{l-1}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Â Serra etÂ al. ([2018](#bib.bib282)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{n_{l}}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l}~{}\forall l\in{\mathbb{L}}\},\\ d_{l}=\min\{n_{0},n_{1}-j_{1},\ldots,n_{l-1}-j_{l-1},n_{l}\}\end{array}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Â Serra and Ramalingam ([2020](#bib.bib281)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{\mathcal{I}_{l}(k_{l-1})}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l},\\ d_{l}=\min\{n_{0},k_{1},\ldots,k_{l-1},\mathcal{I}_{l}(k_{l-1})\},k_{0}=n_{0},k_{l}=\mathcal{A}_{l}(k_{l-1})-j_{l-1}~{}\forall
    l\in{\mathbb{L}}\}\end{array}$ |'
  prefs: []
  type: TYPE_TB
- en: Expected number
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The third analytical approach has been the evaluation of the expected number
    of linear regions. In a pair of papers, Hanin and Rolnick studied the number of
    linear regions based on how the network parameters are typically initialized.
    In the first paper (Hanin and Rolnick, [2019a](#bib.bib137)), they show that the
    average number of linear regions along 1-dimensional subspaces of the input grows
    linearly with respect to the number of neurons, irrespective of the network depth.
    In the second paper (Hanin and Rolnick, [2019b](#bib.bib138)), they show that
    the average number of linear regions in higher-dimensional subspaces of the input
    also grows similarly in deep and shallow networks. For $N=\sum_{i=1}^{L}n_{i}$
    as the total number of linear regions, the expected number of linear regions is
    $O(2^{N})$ if $N\leq n_{0}$ and $O\left(\frac{(TN)^{n_{0}}}{n_{0}!}\right)$ otherwise,
    where $T>0$ is a constant based on the network parameters. Moreover, some of their
    experiments suggest that the number of linear regions in shallow networks is slightly
    greater. According to the authors, these bounds reflect the fact that the family
    of functions that can be represented by neural networks in the way that they are
    typically initialized is considerably smaller. They further argue that training
    as currently performed is unlikely to expand the family of functions much further,
    as illustrated by their experiments. Similar results on the expected number of
    linear regions for maxout networks are presented byÂ Tseran and MontÃºfar ([2021](#bib.bib313)),
    and an application of the results above results to data manifolds is explored
    byÂ Tiwari and Konidaris ([2022](#bib.bib307)). Additional results for specific
    architectures of rectifier networks are conjectured byÂ Wang ([2022](#bib.bib328)),
    although without proof.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Counting linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Counting the actual number of linear regions of a given network has been a
    more challenging topic to explore. Serra etÂ al. ([2018](#bib.bib282)) have shown
    that the linear regions of a trained network can be enumerated as the solutions
    of an MILP formulation, which has been slightly corrected inÂ Cai etÂ al. ([2023](#bib.bib46))Â¹Â¹1The
    MILP formulation of neural networks is discussed in SectionÂ [4](#S4 "4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey").. However, MILP solutions are generally counted one by one (Danna etÂ al.,
    [2007](#bib.bib72)), with exception of special cases (Serra and Hooker, [2020](#bib.bib280))
    and small subproblems (Serra, [2020](#bib.bib279)), which makes this approach
    impractical for large neural networks. Serra and Ramalingam ([2020](#bib.bib281))
    have shown that approximate model counting methods, which are commonly used to
    count the number of feasible assignments in propositional satisfiability, can
    be easily adapted to solution counting in MILP, which leads to an order-of-magnitude
    speedup in comparison with exact counting. This type of approach is particularly
    suitable for obtaining probabilistic lower bounds, which can complement the analytical
    upper bounds for the maximum number of linear regions. In Craighero etÂ al. ([2020a](#bib.bib64))
    and Craighero etÂ al. ([2020b](#bib.bib65)), a directed acyclic graph is used to
    model the sets of active neurons on each layer and how they connected with those
    in subsequent layers. Yang etÂ al. ([2020](#bib.bib348)) describe a method for
    decomposing the input space of rectifier networks into their linear regions by
    representing each linear region in terms of its face lattice, upon which the splitting
    operations corresponding to the transformations performed by each layer can be
    implemented. As the number of linear regions grow, these splitting operations
    can be processed in parallel. Yang etÂ al. ([2021](#bib.bib349)) extend that method
    to convolutional neural networks. Moreover, Wang ([2022](#bib.bib328)) describes
    an algorithm for enumerating linear regions that counts adjacent linear regions
    with same corresponding affine function as a single linear region.'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to enumerate the linear regions in subspaces, which limits
    their number and reduces the complexity of the task. This idea was first explored
    by Novak etÂ al. ([2018](#bib.bib235)) for measuring the complexity of a neural
    network in terms of the number of transitions along a single line. Hanin and Rolnick
    ([2019a](#bib.bib137), [b](#bib.bib138)) also use this method with a bounded line
    segment or rectangle as a single set representing the input and then sequentially
    partitioning it. If this first set is intersected by the activation hyperplane
    of a neuron in the first layer, then we replace this set by two sets corresponding
    to the parts of the input space in which that neuron is active and not. Once those
    sets are further subdivided by all activation hyperplanes associated with the
    neurons in the first layer, the process can be continued with the neurons in the
    following layers. This method is used to count the number of linear regions along
    subspaces of the input with dimension 1 in Hanin and Rolnick ([2019a](#bib.bib137))
    and dimension 2 in Hanin and Rolnick ([2019b](#bib.bib138)). A generalized version
    for counting the number of linear regions in affine subspaces spanned by a set
    of samples using an MILP formulation is presented in Cai etÂ al. ([2023](#bib.bib46)).
    An approximate approach for counting the number of linear regions along a line
    by computing the closest activation hyperplane in each layer is presented in Gamba
    etÂ al. ([2022](#bib.bib114)).
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches have obtained lower bounds on the number of linear regions
    of a trained network by limiting the enumeration or considering exclusively the
    inputs from the dataset. In Xiong etÂ al. ([2020](#bib.bib345)), the number of
    linear regions is estimated by sampling points from the input space and enumerating
    all activation patterns identified through this process. In Cohan etÂ al. ([2022](#bib.bib61)),
    the counting is restricted to the linear regions found between consecutive states
    of a neural network modeling a reinforcement learning policy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Applications and insights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thinking about neural networks in terms of linear regions led to a variety of
    applications. In turn, that inspired further studies on the structure and properties
    of linear regions under different settings. We organize the literature about applications
    and insights around some central themes in the subsections below.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 The number of linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From our discussion, the number of linear regions emerges as a potential proxy
    for the complexity of neural networks, which has been studied by some authors
    and exploited empirically by others. Novak etÂ al. ([2018](#bib.bib235)) observed
    that the number of transitions between linear regions in 1-dimensional subspaces
    correlates with generalization. Hu etÂ al. ([2020a](#bib.bib154)) used bounds on
    the number of linear regions as proxy to model the capacity of a neural network
    used for learning through distillation, in which a smaller network is trained
    based on the outputs of another network. Chen etÂ al. ([2021a](#bib.bib54)) and
    Chen etÂ al. ([2021b](#bib.bib55)) present one of the first approaches to training-free
    neural architectural search through the analysis of network properties. One of
    the two metrics that they have shown to be effective for that purpose is the number
    of linear regions associated with a sample of inputs from the training set on
    randomly initialized networks. Biau etÂ al. ([2021](#bib.bib30)) observed that
    obtaining a discriminator network for Wasserstein GANsÂ (Arjovsky etÂ al., [2017](#bib.bib7))
    that correctly approximates the Wasserstein distance entails that such a discriminator
    network has a growing number of linear regions as the complexity of the data distribution
    increases. Park etÂ al. ([2021b](#bib.bib242)) maximized the number of linear regions
    in unsupervised learning in order to produce more expressive encodings for downstream
    tasks using simpler classifiers. In neural networks modeling reinforcement learning
    policies, Cohan etÂ al. ([2022](#bib.bib61)) observed that the number of transitions
    between linear regions in inputs corresponding to consecutive states increases
    by 50% with training while the number of repeated linear regions decreases. Cai
    etÂ al. ([2023](#bib.bib46)) proposed a method for pruning different proportions
    of parameters from each layer by maximizes the bound on the number of linear regions,
    which lead to better accuracy than uniform pruning across layers. On a related
    note, Liang and Xu ([2021](#bib.bib192)) proposed a new variant of the ReLU activation
    function for dividing the input space into a greater number of linear regions.
  prefs: []
  type: TYPE_NORMAL
- en: The number of linear regions also inspired further theoretical work. Amrami
    and Goldberg ([2021](#bib.bib3)) presented an argument for the benefit of depth
    in neural networks based on the number of linear regions for correctly separating
    samples associated with different classes. Liu and Liang ([2021](#bib.bib196))
    studied upper and lower bounds on the optimal approximation error of a convex
    univariate function based on the number of linear regions of a rectifier network.
    Henriksen etÂ al. ([2022](#bib.bib146)) used the maximum number of linear regions
    as a metric for capacity that may limit repairing incorrect classifications in
    a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 The shape of linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some studies aimed at understanding what affects the shape of linear regions
    in practice, including how to train neural networks in such a way to induce certain
    shapes in the linear regions. Zhang and Wu ([2020](#bib.bib359)) observed that
    multiple training techniques may lead to similar accuracy, but very different
    shape for the linear regions. For example, batch normalizationÂ (Ioffe and Szegedy,
    [2015](#bib.bib162)) and dropoutÂ (Srivastava etÂ al., [2014](#bib.bib294)) lead
    to more linear regions. While batch normalization breaks the space in regions
    with uniform size, more orthogonal norms, and more gradient variability across
    adjacent regions; dropout produces more linear regions around decision boundaries,
    norms are more parallel, and data points less likely to be in the region containing
    the decision boundary. Croce etÂ al. ([2019](#bib.bib67)) and Lee etÂ al. ([2019](#bib.bib188))
    applied regularization to the loss function to push the boundary of each linear
    region away from points in the training set that it contains, as long as those
    points are correctly classified. They show that this form of regularization improves
    the robustness of the neural network while making the linear regions larger. In
    fact, Zhu etÂ al. ([2020](#bib.bib362)) observed that the boundaries of the linear
    regions move away from the training data; and He etÂ al. ([2021](#bib.bib141))
    conjectured that the linear regions near training samples becomes smaller through
    training, or that conversely the activation patterns are denser around the training
    samples. Gamba etÂ al. ([2020](#bib.bib113)) presented an empirical study on the
    angles between activation hyperplanes defined by convolutional layers, and observed
    that their cosines tend to be similar and more negative with depth after training.
  prefs: []
  type: TYPE_NORMAL
- en: The geometry of linear regions also led to other theoretical and algorithmic
    advances. Theoretically, Phuong and Lampert ([2020](#bib.bib247)) proved that
    architectures with nonincreasing layer widths have unique parameters â€”upon permutation
    and scalingâ€” for representing certain functions. In other words, some pairs of
    neural networks are only equivalent if their parameters only differ by permutation
    and multiplication. Grigsby etÂ al. ([2023](#bib.bib132)) showed that equivalences
    other than by permutation are less likely to occur with greater input size and
    width, but more likely with greater depth. Algorithmically, Rolnick and Kording
    ([2020](#bib.bib260)) proposed a procedure to reconstruct a neural network by
    evaluating several inputs in order to determine regions of the input space for
    which the output of the neural network can be defined by an affine function â€”and
    thus consist of a single linear region. Depending on how the shape changes between
    adjacent linear regions, the boundaries of the linear regions are replicated with
    neurons in the first hidden layer or in subsequent layers of the reconstructed
    neural network. Masden ([2022](#bib.bib213)) provided theoretical results and
    an algorithm for characterizing the face lattice of the polyhedron associated
    with each linear region.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Activation patterns and the discrimination of inputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another common theme is understanding how inputs from the training and test
    sets are distributed among the linear regions, and what can be inferred the encoding
    of the activation patterns associated with the linear regions. Gopinath etÂ al.
    ([2019](#bib.bib127)) noted that many properties of neural networks, including
    the classes of different inputs, are associated with activation patterns â€”and
    thus with their linear regions. Several worksÂ (He etÂ al., [2021](#bib.bib141),
    Sattelberg etÂ al., [2020](#bib.bib271), Trimmel etÂ al., [2021](#bib.bib310)) observed
    that each training sample is typically located in a different linear region when
    the neural network is sufficiently expressive; whereas He etÂ al. ([2021](#bib.bib141))
    noted that simple machine learning algorithms can be applied using the activation
    patterns as features, and Sattelberg etÂ al. ([2020](#bib.bib271)) noted that there
    is some similarity between activation patterns of different neural networks under
    affine mapping, meaning that the training of these neural networks lead to similar
    models. Chaudhry etÂ al. ([2020](#bib.bib49)) exploited the idea of continual learning
    with different tasks being encoded in disjoint subspaces, which thus corresponds
    to a disjoint set of activation sets on each layer being associated with classifications
    for each of those tasks. Based on their approach for enumerating linear regions,
    Craighero etÂ al. ([2020a](#bib.bib64)) and Craighero etÂ al. ([2020b](#bib.bib65))
    have found that inputs from larger linear regions are often correctly classified
    by the neural network, that inputs from smaller linear regions are often incorrectly
    classified, and that the number of distinct activations sets reduces along the
    layers of the neural network. Gamba etÂ al. ([2022](#bib.bib114)) also discussed
    the issue of some linear regions being smaller and thus less likely to occur in
    practice. Moreover, they propose a measurement for the similarity of the affine
    functions associated with linear regions along a line and observed that the linear
    regions tend to be less similar to one another when the network is trained with
    incorrectly classified labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4 Function approximation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because of the linear behavior of the output within each linear region, we can
    approximate the output of the neural network based on the output of its linear
    regions. Chu etÂ al. ([2018](#bib.bib59)) and Sudjianto etÂ al. ([2020](#bib.bib297))
    produced linear models based on this local behavior; whereas Glass etÂ al. ([2021](#bib.bib118))
    observed that we can interpret neural networks as equivalent to local linear model
    treesÂ (Nelles etÂ al., [2000](#bib.bib230)), in which a distinct linear model is
    used at each leaf of a decision tree, and provided a method to produce such models
    from neural networks. Trimmel etÂ al. ([2021](#bib.bib310)) described how to extract
    the linear regions associated with the inputs from the training set as means to
    approximate the output of the inputs from the test set. Robinson etÂ al. ([2019](#bib.bib259))
    presented another approach for explicitly representing the function modeled by
    a neural network through the enumeration of its linear regions. On a related note,
    Chaudhry etÂ al. ([2020](#bib.bib49)) used the assumption of training samples remaining
    within the same linear region during gradient descent to simplify the analysis
    of backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: This topic also relates to the broad literature on neural networks as universal
    function approximators, to which the concept of linear regions helps articulating
    ideal conditions. As observed by Mhaskar and Poggio ([2020](#bib.bib218)), the
    optimal number of linear regions in a neural network â€”or, correspondingly, of
    pieces of the piecewise linear function modeled by itâ€” depends on the function
    being approximated. In addition, linear regions were also used explicitly to build
    function approximations. Kumar etÂ al. ([2019](#bib.bib181)) have shown that rectifier
    networks can we approximated to arbitrary precision with two hidden layers, the
    largest of which having a neuron corresponding to each different activation pattern
    of the original network; an exact counterpart of this result was later presented
    byÂ Villani and Schoots ([2023](#bib.bib319)). Fan etÂ al. ([2020](#bib.bib99))
    described the transformation between sufficiently wide and deep networks while
    arguing that the fundamental measure of complexity should be counting simplices
    within linear regions. In subsequent work, Fan etÂ al. ([2023](#bib.bib100)) empirically
    observed that linear regions tend to have a small number of higher dimensional
    faces, or facets.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recent studies aimed at understanding the expressiveness and approximability
    of neural networks in terms of their number of parameters, in particular when
    the number of linear regions is greater than the number of parameters (Malach
    and Shalev-Shwartz, [2019](#bib.bib208), Dym etÂ al., [2020](#bib.bib86), Daubechies
    etÂ al., [2022](#bib.bib76)). They all discuss how the composition the modeled
    functions tend to present the self-similarity property of fractal distributions,
    which is one reason why they have so many linear regions. Keup and Helias ([2022](#bib.bib172))
    interpreted the connection between linear regions in different parts of the input
    space in terms of how paper origamis are constructed: by â€œfoldingâ€ the data for
    separability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another related topic is computing the Lipschitz constant $\rho$ of the function
    $f(x)$ modeled by the neural network, the smallest $\rho$ for which $\|f(x^{\prime})-f(x)\|\leq\rho\|x^{\prime}-x\|$
    for any two inputs $x$ and $x^{\prime}$. Note that the first derivative of the
    output of a linear region is constant, which is leveraged byÂ Hwang and Heinecke
    ([2020](#bib.bib160)) to evaluate the stability of the network by computing the
    constant across linear regions by changing the activation pattern. Interestingly,
    Zhou and Schoellig ([2019](#bib.bib361)) showed that the constant grows similarly
    to the number of linear regions: polynomial in width and exponential in depth.
    A smaller constant limits the susceptibility of the network to adversarial examplesÂ (Huster
    etÂ al., [2018](#bib.bib159)), which are discussed in SectionÂ [4](#S4 "4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey"), and also lead to smaller bias variance (Loukas etÂ al., [2021](#bib.bib201)).
    While calculating the exact Lipschitz constant is NP-hard and encourages approximations
    (Virmaux and Scaman, [2018](#bib.bib322), Patrick L.Â Combettes, [2019](#bib.bib244)),
    the exact constant can be computed using MILP (Jordan and Dimakis, [2020](#bib.bib166)).
    Notably, many studies have focused on relaxations such as linear programming (Zou
    etÂ al., [2019](#bib.bib363)), semidefinite programming (Fazlyab etÂ al., [2019](#bib.bib101),
    Chen etÂ al., [2020](#bib.bib53)), and polynomial optimization (Latorre etÂ al.,
    [2020](#bib.bib184)). An alternative approach is to use more sophisticated activation
    functions for limiting the value of the constant (Anil etÂ al., [2019](#bib.bib6),
    Aziznejad etÂ al., [2020](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.5 Optimizing over linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As an alternative to optimizing over neural networks as described next in SectionÂ [4](#S4
    "4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), a number of approaches have resorted to techniques that are
    equivalent to systematically enumerating or traversing linear regions and optimizing
    over them (Croce and Hein, [2018](#bib.bib66), Croce etÂ al., [2020](#bib.bib68),
    Khedr etÂ al., [2020](#bib.bib174), Vincent and Schwager, [2021](#bib.bib320),
    Xu etÂ al., [2022](#bib.bib346)). Notably, Vincent and Schwager ([2021](#bib.bib320))
    and Xu etÂ al. ([2022](#bib.bib346)) are mindful of the facet-defining inequalities
    associated with a linear region, which are the ones to change when moving toward
    an adjacent linear region. On a related note, Seck etÂ al. ([2021](#bib.bib278))
    alternates between gradient steps and solving a linear programming model within
    the current linear region.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Optimizing Over a Trained Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [5](#S5 "5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") we will see how polyhedral-based
    methods can be used to *train* a neural network. In this section, we will focus
    on how polyhedral-based methods can be used to do something with a neural network
    *after it has been trained.* Specifically, after the network architecture and
    all parameters have been fixed, a neural network $f$ is merely a function. If
    each activation function $\sigma$ used to describe the network is piecewise linear
    (as is the case with those presented in TableÂ [1](#S1.T1 "Table 1 â€£ 1.3 Why nonlinearity
    is important in artificial neurons â€£ 1 Introduction â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")), $f$ is also a piecewise linear function. Therefore,
    any optimization problem containing $f$ in some way will be a piecewise linear
    optimization problem. For example, in the simple case where the output of $f$
    is univariate, the optimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{x\in\mathcal{X}}f(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'is a piecewise linear optimization problem. As discussed in Section [3](#S3
    "3 The Linear Regions of a Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), this problem can have an enormous number of â€œpiecesâ€ (linear
    regions) when $f$ is a neural network; solving this problem thus heavily depends
    on the size and structure of the neural network $f$. For example, the training
    procedure by which $f$ is obtained can greatly influence the performance of optimization
    strategies (Tjeng etÂ al., [2019](#bib.bib309), Xiao etÂ al., [2019](#bib.bib341)).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first explore situations in which you might want to optimize
    over a trained neural network in this manner. We will then survey available methods
    for solving this method (either exactly or on the dual side) using polyhedral-based
    methods. We conclude with a brief view of future directions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Applications of optimization over trained networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Applications where you might want to optimize over a trained neural network
    $f$ broadly fall into two categories: those where $f$ is the â€œtrueâ€ object of
    interest, and those where $f$ is a convenient proxy for some unknown, underlying
    behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Neural network verification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Neural network verification is a burgeoning field of study in deep learning.
    Starting in the early 2000s, researchers began to recognize the importance of
    rigorously verifying the behavior of neural networks, mainly in aviation-related
    applications (Schumann etÂ al., [2003](#bib.bib274), Zakrzewski, [2001](#bib.bib351)).
    More recently, the seminal works of Szegedy etÂ al. ([2014](#bib.bib300)) and Goodfellow
    etÂ al. ([2015](#bib.bib124)) observed that neural networks are unusually susceptible
    to *adversarial attacks*. These are small, targeted perturbations that can drastically
    affect the output of the network; as shown in Figure [8](#S4.F8 "Figure 8 â€£ 4.1.1
    Neural network verification â€£ 4.1 Applications of optimization over trained networks
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"), even powerful models such as MobileNetV2 (Sandler etÂ al.,
    [2018](#bib.bib270)) are susceptible. The existence and prevalence of adversarial
    attacks in deep neural networks has raised justifiable concerns about the deployment
    of these models in mission-critical systems such as autonomous vehicles (Deng
    etÂ al., [2020](#bib.bib79)), aviation (Kouvaros etÂ al., [2021](#bib.bib177)),
    or medical systems (Finlayson etÂ al., [2019](#bib.bib105)). One fascinating empirical
    work by Eykholt etÂ al. ([2018](#bib.bib98)) showed the susceptibility of standard
    image classification networks that might be used in self-driving vehicles to a
    very analogue form of attacks: black/white stickers, placed in a careful way,
    could confuse these models enough that they would mis-classify road signs (e.g.,
    mistaking stop signs for â€œspeed limit 80â€ signs).'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F8.pic1" class="ltx_picture ltx_centering" height="130.22" overflow="visible"
    version="1.1" width="407.44"><g transform="translate(0,130.22) matrix(1 0 0 -1
    0 0) translate(54.11,0) translate(0,65.11)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -49.5 -60.5)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="121" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/eb8e031e8c492fc9ac38e777fd76ac60.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 67.05 -16.79)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 100.11 -59.74)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/6ec4f02d882565048e57d58d5b3977d5.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 216.66 -15.47)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="7.31" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">=</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.71 -60.5)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="121" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/1df2f6052ecb61a06e1057f75e5d6880.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.82 43.78)" fill="#000000" stroke="#000000"><foreignobject width="75.57"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times(\epsilon=0.15)$</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Example of adversarial attack on MobileNetV2 (Sandler etÂ al., [2018](#bib.bib270)).
    The original image taken by one of the survey authors is classified as â€˜siberian_husky,â€™
    but is re-classified as â€˜wallabyâ€™ with a small (in an $\ell_{\infty}$-norm sense)
    targeted attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network verification seeks to prove (or disprove) a given input-output
    relationship, i.e., $x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$, that gives some
    indication of model robustness. Methods for verifying this relationship are classified
    as being sound and/or complete. A method that is sound will only certify the relationship
    if it is indeed true (no false positives), while a method that is complete will
    (i) always return an answer and (ii) only disprove the relationship if it is false
    (no false negatives). An early set of papers (Fischetti and Jo, [2018](#bib.bib106),
    Lomuscio and Maganti, [2017](#bib.bib200), Tjeng etÂ al., [2019](#bib.bib309))
    recognized that MILP provides an avenue for verification that is both sound and
    complete, given that $\mathcal{X}$ and $f(x)$ are both linear, or piecewise linear.
    We refer the readers to recent reviews (Huang etÂ al., [2020](#bib.bib157), Leofante
    etÂ al., [2018](#bib.bib190), Li etÂ al., [2022](#bib.bib191), Liu etÂ al., [2021](#bib.bib197))
    for a more comprehensive treatment of the landscape of verification methods, including
    MILP- and LP-based technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider a classification network $f:[0,1]^{n_{0}}\to\mathbb{R}^{d}$ where the
    $j$-th output, $f(x)_{j}$, corresponds to the probability that input $x$ is of
    class $j$.Â²Â²2In actuality, we will instead typically work with the outputs corresponding
    to â€œlogitsâ€, or unnormalized probabilities. These are typically fed into a softmax
    layer that then normalize these values to correspond to a probability distribution
    over the classes. However, this nonlinear softmax transformation is not piecewise
    linear. Thankfully, it can be omitted in the context of the verification task
    without loss of generality. Then consider a labeled image $\hat{x}$ known to be
    of class $i$, and a â€œtargetâ€ adversarial class $k\neq i$. Then verifying local
    robustness of the prediction corresponds to checking $x\in\{x:||x-\hat{x}||\leq\epsilon\}\Rightarrow
    y=f(x)\in\{y:y_{i}\geq y_{k}\}$, where $\epsilon>0$ is a constant which prescribes
    the radius around which $\hat{x}$ we will search for an adversarial example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This verification task can be formulated as an optimization problem of the
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{x\in[0,1]^{n_{0}}}$ | $\displaystyle f(x)_{k}-f(x)_{i}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Any feasible solution $x$ to this problem with positive cost is an adversarial
    example: it is very â€œcloseâ€ to $\hat{x}$ which has true label $i$, yet the network
    believes it is more likely to be of class $k$.Â³Â³3Alternative objectives are sometimes
    used which would allow us to strengthen this statement to say that the network
    *will* classify $x$ to be of class $k$. However, this will require a more complex
    reformulation to model this problem via MILP, so we omit it for simplicity. If,
    on the other hand, it is proven that the optimal objective value is negative,
    this proves that $f$ is robust (at least in the neighborhood around $\tilde{x}$).
    Note that the verification problem can terminate once the sign of the optimal
    objective value is determined, but solving the problem returns an optimal adversarial
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function of ([3](#S4.E3 "In Example 4 â€£ 4.1.1 Neural network
    verification â€£ 4.1 Applications of optimization over trained networks â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")) models the desired input-output relationship, $x\in\mathcal{X}\Rightarrow
    y\in\mathcal{Y}$, while the constraints model the domain $\mathcal{X}$. The domain
    $\mathcal{X}$ is typically a box or hyperrectangular domain. Extensions to this
    are described in Section [4.4.1](#S4.SS4.SSS1 "4.4.1 Extending to other domains
    â€£ 4.4 Generalizing the single neuron model â€£ 4 Optimizing Over a Trained Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey"). Some explanation-focused
    verification applications define the input-output relationship in a derivative
    sense, e.g., $x\in\mathcal{X}\Rightarrow\partial y/\partial x\in\mathcal{Y}^{\prime}$
    (Wicker etÂ al., [2022](#bib.bib333)). As the derivative of the ReLU function is
    also piecewise linear, this class of problems can also be modeled in MILP. For
    example, in the context of fairness and explainability, Liu etÂ al. ([2020](#bib.bib198))
    and Jordan and Dimakis ([2020](#bib.bib166)) used MILP to certify monotonicity
    and to compute local Lipschitz constants, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although in this survey we focus on optimization over trained neural networks,
    it is important to note that polyhedral theory underlies numerous strategies for
    neural network verification. For example, SAT and SMT (Satisfiability Modulo Theories)
    solvers designed for Boolean satisfiability problems (and more general problems
    for the case of SMT) can also be used to search through activation patterns for
    a neural network (Pulina and Tacchella, [2010](#bib.bib251)), resulting in tools
    that are sound and complete, such as Planet (Ehlers, [2017](#bib.bib87)) and Reluplex
    (Katz etÂ al., [2017](#bib.bib169)). Bunel etÂ al. ([2018](#bib.bib43)) presented
    a unified view to compare MILP and SMT formulations, as well as the relaxations
    that result from these formulations (we will revisit this in SectionÂ [4.3](#S4.SS3
    "4.3 Scaling further: Convex relaxations and linear programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")). On the other hand, strategies such as ExactReachÂ (Xiang etÂ al., [2017](#bib.bib340))
    exploit polyhedral theory to compute reachable sets: given an input set to a ReLU
    function defined as a union of polytopes, the output reachable set is also a union
    of polytopes. Other methods over-approximate the reachable set to improve scalability,
    e.g., for vision models (Yang etÂ al., [2021](#bib.bib349)), often resulting in
    methods that are sound, but not complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Neural network as proxy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another situation in which you may want to solve an optimization problem containing
    trained neural networks is when you would like to optimize some other, unknown
    function for which you have historical input/output data. A similar situation
    arises when you want to solve an optimization problem where (some of) the constraints
    are overly complicated, but you can query samples from the constraints on which
    to train a simpler surrogate model. In these cases, you might imagine training
    a neural network in a standard supervised learning setting to approximate this
    underlying, unknown or complicated function. Then, since the neural network is
    known, you are left with a deterministic piecewise linear optimization problem.
    Note that we focus here on using a neural network as a surrogate; neural networks
    can additionally learn other components of an optimization problem, e.g., uncertainty
    sets for robust optimization (Goerigk and Kurtz, [2023](#bib.bib122)).
  prefs: []
  type: TYPE_NORMAL
- en: Several software tools have been developed for this class of problems. For the
    case of constraint learning, JANOS (Bergman etÂ al., [2022](#bib.bib24)) and OptiCL
    (Maragno etÂ al., [2021](#bib.bib210)) both provide functionality for learning
    a ReLU neural network to approximate a constraint based on data and embedding
    the learned neural network in MILP. The reluMIP package (Lueg etÂ al., [2021](#bib.bib203))
    has also been introduced to handle the latter embedding step. More generally,
    OMLT (Ceccon etÂ al., [2022](#bib.bib47)) translates neural networks to pyomo optimization
    blocks, including various MILP formulations and activation functions. Finally,
    recent developments in gurobipyâ´â´4https://github.com/Gurobi/gurobi-machinelearning
    enable directly parsing in trained neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of this paradigm can be envisioned in a number of domain areas.
    This approach is common in deep reinforcement learning, where neural networks
    are used to approximate an unknown â€œ$Q$-functionâ€ which models the long-term cost
    of taking a particular action in a particular state of the world. In $Q$-learning,
    this $Q$-function is optimized iteratively to produce new candidate policies,
    which are then evaluated (typically via simulation) to produce new training data
    for future iterations. Optimization over the learned $Q$-function must be relatively
    fast in control applications, and several practical methods have been proposed.
    When the action space is discrete, the $Q$-function neural network is trained
    with one output value for each possible action, simplifying optimization to evaluating
    the model and selecting the largest output. Continuous action spaces require the
    $Q$ network be optimized over (Burtea and Tsay, [2023](#bib.bib45), Delarue etÂ al.,
    [2020](#bib.bib78), Ryu etÂ al., [2020](#bib.bib267)), or an â€œactorâ€ network can
    be trained to learn the optimal actions (Lillicrap etÂ al., [2015](#bib.bib193)).
    In a related vein, ReLU neural networks can be used as a process model for optimal
    scheduling or control (Wu etÂ al., [2020](#bib.bib338)).
  prefs: []
  type: TYPE_NORMAL
- en: Chemical engineering also presents applications where surrogate models have
    proven beneficial for optimization, as is the subject of recent reviews (Bhosekar
    and Ierapetritou, [2018](#bib.bib28), McBride and Sundmacher, [2019](#bib.bib216),
    Tsay and Baldea, [2019](#bib.bib311)). In particular, ReLU neural networks can
    be seamlessly embedded in larger MILP problems such as flow networks and reservoir
    control where the other constraints are also mixed-integer linear (Grimstad and
    Andersson, [2019](#bib.bib133), Say etÂ al., [2017](#bib.bib272), Yang etÂ al.,
    [2022](#bib.bib347)). Focusing on control applications where the neural network
    is embedded in a MILP that must be solved repeatedly, Katz etÂ al. ([2020](#bib.bib171))
    showed how multiparametric programming can be used to learn the solution map of
    the resulting MILP itself, which is also piecewise affine. An emerging area of
    research uses verification tools to reason about neural networks used as controllers,
    e.g., see Johnson etÂ al. ([2020](#bib.bib165)). These applications involve optimization
    formulations combining the neural network with constraints defining the controlled
    system. For example, verification can be used to bound the reachable set (Sidrane
    etÂ al., [2022](#bib.bib286)) (alongside piecewise linear bounds on the dynamical
    system) or the maximum error against a baseline controller (Schwan etÂ al., [2022](#bib.bib275)).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, applications for optimization over neural networks arise in machine
    learning applications. MILP formulations can be used to compress neural networks
    (Serra etÂ al., [2020](#bib.bib283), [2021](#bib.bib284), ElAraby etÂ al., [2020](#bib.bib88)),
    which consequently result in more tractable surrogate models (Kody etÂ al., [2022](#bib.bib176)).
    The main idea is to use MILP to identify stable nodes, i.e., nodes that are always
    on or off over an input domain, which can then be algebraically eliminated. Optimization
    has also been employed in techniques for feature selection, based on identifying
    strongest input nodes (Sildir and Aydin, [2022](#bib.bib287), Zhao etÂ al., [2023](#bib.bib360)).
    In the context of Bayesian optimization, Volpp etÂ al. ([2020](#bib.bib323)) use
    reinforcement learning to meta-learn acquisition functions parameterized as neural
    networks; selecting ensuing query points then requires optimization over the trained
    acquisition function. Later work modeled both the acquisition function and feasible
    region in black-box optimization as neural networks (Papalexopoulos etÂ al., [2022](#bib.bib239)).
    In that work, exploration and exploitation are balanced via Thompson sampling
    and training multiple neural networks from a random parameter initialization.
  prefs: []
  type: TYPE_NORMAL
- en: A word of caution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Standard supervised learning algorithms aim to learn a function which fits the
    underlying function according to some distribution under which the data is generated.
    However, optimizing a function corresponds to evaluating it at a single point.
    This means that you may end up with a model that well-approximates the underlying
    function in distribution, but for which the pointwise minimizer is a poor approximation
    of the true function. This phenomena is referred to as the â€œOptimizerâ€™s curseâ€
    (Smith and Winkler, [2006](#bib.bib293)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Single neuron relaxations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the following subsections, consider the $i$-th neuron in the $l$-th layer
    of a neural network, endowed with a ReLU activation function, whose behavior is
    governed by ([2](#S1.E2 "In 1.1 What neural networks can model â€£ 1 Introduction
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). Presume that a input
    domain of interest $\mathcal{D}^{l-1}\subset\mathbb{R}^{n_{l}}$ is a bounded region.
    Further, since $\mathcal{D}^{l-1}$ is bounded, presume that finite bounds are
    known on each input component, i.e. that vectors $L^{l-1},U^{l-1}\in\mathbb{R}^{n_{l}}$
    are known such that $\mathcal{D}^{l-1}\subseteq[L^{l-1},U^{l-1}]\subset\mathbb{R}^{n_{l}}$.
    We can then write the *graph* of the neuron, which couples together the input
    and the output of the nonlinear ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\texttt{gr}=$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}=0\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\cup$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}={\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\geq
    0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This is a disjunctive representation for gr in terms of two polyhedral alternatives.
    We assume that every included neuron exhibits this disjunction, i.e., every neuron
    can be on or off depending on the model input. This assumption of *strict activity*
    implies that $L^{l-1}<0$ and $U^{l-1}>0$, noting that neurons not satisfying this
    property can be exactly pruned from the model (Serra etÂ al., [2020](#bib.bib283)).
  prefs: []
  type: TYPE_NORMAL
- en: We observe that, given this (or any) formulation for each individual unit, it
    is straightforward to construct a formulation for the entire network. For example,
    if we take $X^{l}_{i}=\Set{({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})}{\eqref{eqn:relu-big-m}}$
    for each layer $l$ and each unit $i$, we can construct a MILP formulation for
    the graph of the entire network, $\Set{(x,f(x)):x\in\mathcal{D}^{0}}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})\in X^{l}_{i}\quad\forall l\in{\mathbb{L}},i\in\llbracket
    n_{l}\rrbracket.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This also generalizes in a straightforward manner to more complex feedforward
    network architectures (e.g. convolutions, or sparse or skip connections), though
    we omit the explicit description for notational simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Beyond the scope of this survey
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The effectiveness of the single-neuron formulations described above is bounded
    by the tightness of the optimal univariate formulation; this property is known
    as the â€œsingle-neuron barrierâ€ (Salman etÂ al., [2019](#bib.bib269)). This has
    motivated research in convex relaxations that jointly account for multiple neurons
    within a layer (Singh etÂ al., [2019a](#bib.bib290)). Nevertheless, the analysis
    of polyhedral formulations for multiple neurons simultaneously quickly becomes
    intractable, and is beyond the scope of this survey. Instead, we point the interested
    reader to the recent survey by Roth ([2021](#bib.bib263)), and highlight a few
    approaches taken in the literature. Multi-neuron analysis has been used to: improve
    bounds tightening schemes (RÃ¶ssig and Petkovic, [2021](#bib.bib262)), prune linearizable
    neurons (Botoeva etÂ al., [2020](#bib.bib37)), design dual decompositions (Ferrari
    etÂ al., [2022](#bib.bib104)), and generate strengthening inequalities (Serra and
    Ramalingam, [2020](#bib.bib281)). Similarly, we do not review formulations for
    ensembles of ReLU networks, though MILP formulations have been proposed (Wang
    etÂ al., [2021](#bib.bib324), [2023](#bib.bib325)).'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, recent works have exploited polyhedral structure to develop sampling
    based strategies, which can be used to warm-start MILP or accelerate local search
    in verification (Perakis and Tsiourvas, [2022](#bib.bib245), Wu etÂ al., [2022](#bib.bib339)).
    Lombardi etÂ al. ([2017](#bib.bib199)) computationally compare MILP against local
    search and constraint programming approaches. In a related vein, Cheon ([2022](#bib.bib58))
    examines local solutions and proposes an outer approximation method to improve
    gradient-based optimization. Finally, following Raghunathan etÂ al. ([2018](#bib.bib254)),
    a large body of work has presented optimization-based methods for verification
    that use semidefinite programming concepts (Dathathri etÂ al., [2020](#bib.bib75),
    Fazlyab etÂ al., [2020](#bib.bib102), Newton and Papachristodoulou, [2021](#bib.bib232)).
    Notably, Batten etÂ al. ([2021](#bib.bib17)) showed how combining semidefinite
    and MILP formulations can produce a new formulation that is tighter than both.
    This was later extended with reformulation-linearization technique, or RLT, cuts
    (Lan etÂ al., [2022](#bib.bib183)). While related to linear programming and other
    methods based on convex relaxations, this stream of work is beyond the scope of
    this survey. We refer the reader to Zhang ([2020](#bib.bib358)) for a discussion
    of the tightness of these formulations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Exact models using mixed-integer programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mixed-integer programming offers a powerful algorithmic framework for *exactly*
    modeling nonconvex piecewise linear functions. The Operations Research community
    has studied has a long and storied history of developing MILP-based methods for
    piecewise linear optimization, with research spanning decades (Croxton etÂ al.,
    [2003](#bib.bib69), Dantzig, [1960](#bib.bib73), GeiÃŸler etÂ al., [2012](#bib.bib117),
    Huchette and Vielma, [2022](#bib.bib158), Lee and Wilson, [2001](#bib.bib189),
    Misener and Floudas, [2012](#bib.bib221), Padberg, [2000](#bib.bib238), Vielma
    etÂ al., [2010](#bib.bib318)). However, many of these techniques are specialized
    for low-dimensional or separable piecewise linear functions. While a reasonable
    assumption in many OR problems, this is not the case when modeling neurons in
    a neural network. Therefore, the standard approach in the literature is to apply
    general-purpose MILP formulation techniques to model neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Connection to Boolean satisfiability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some SMT-based methods such as Reluplex (Katz etÂ al., [2017](#bib.bib169)) and
    Planet (Ehlers, [2017](#bib.bib87)) effectively construct branching technologies
    similar to MILP solvers. Indeed, Marabou (Katz etÂ al., [2019](#bib.bib170)) builds
    on Reluplex, and a recent extension MarabouOpt can optimize over trained neural
    networks (Strong etÂ al., [2021](#bib.bib295)). The authors also outline general
    procedures to extend verification solvers to optimization. Our focus in this review
    is on more general MILP formulations, or those that can be incorporated into off-the-shelf
    MILP solvers with relative ease. Bunel etÂ al. ([2020b](#bib.bib42), [2018](#bib.bib43))
    provide a more comprehensive discussion of similarities and differences to SMT.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 The big-$M$ formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The big-$M$ method is a standard technique used to formulate logic and disjunctive
    constraints using mixed-integer programming (Bonami etÂ al., [2015](#bib.bib35),
    Vielma, [2015](#bib.bib316)). Big-$M$ formulations are typically very simple to
    reason about and implement, and are quite compact, though their convex relaxations
    can often be quite poor, leading to weak dual bounds and (often) slow convergence
    when passed to a mixed-integer programming solver. Since gr is a disjunctive set,
    the big-$M$ technique can be applied to produce the following formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (4a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\left({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\right)-M^{l}_{i,-}(1-z)$
    |  | (4b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq M^{l}_{i,+}z$ |  | (4c)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}$ |  | (4d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\}.$ |  | (4e) |'
  prefs: []
  type: TYPE_TB
- en: Here, $M^{l}_{i,-}$ and $M^{l}_{i,+}$ are data which must satisfy the inequalities
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle M^{l}_{i,-}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle M^{l}_{i,+}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This big-$M$ formulation for ReLU-based networks has been used extensively in
    the literature (Bunel etÂ al., [2018](#bib.bib43), Cheng etÂ al., [2017](#bib.bib56),
    Dutta etÂ al., [2018](#bib.bib83), Fischetti and Jo, [2018](#bib.bib106), Kumar
    etÂ al., [2019](#bib.bib181), Lomuscio and Maganti, [2017](#bib.bib200), Serra
    and Ramalingam, [2020](#bib.bib281), Serra etÂ al., [2018](#bib.bib282), Tjeng
    etÂ al., [2019](#bib.bib309), Xiao etÂ al., [2019](#bib.bib341)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The big-$M$ formulation is compact, with one binary variable and $\mathcal{O}(1)$
    general inequality constraints for each neuron. Applied for each unit in the network,
    this leads to a MILP formulation with $\mathcal{O}(\sum_{l\in{\mathbb{L}}}n_{l})=\mathcal{O}(Ln_{\max})$
    binary variables and general inequality constraints, where $n_{\max}=\max_{l\in{\mathbb{L}}}n_{L}$.
    However, it has been observed (Anderson etÂ al., [2019](#bib.bib4), [2020](#bib.bib5))
    that this big-$M$ formulation is not strong in the sense that its LP relaxation
    does not, in general, capture the convex hull of the graph of a given unit; see
    FigureÂ [9](#S4.F9 "Figure 9 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey") for an illustration. In fact,
    this LP relaxation can be arbitrarily bad (Anderson etÂ al., [2019](#bib.bib4),
    Example 2), even in fixed input dimension. As MILP solvers often bound the objective
    function between the best feasible point and its tightest optimal continuous relaxation,
    a weak formulation can negatively impact performance, often substantially.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth dwelling on where this lack of strength comes from. If the input
    ${\bm{h}}^{l-1}$ is one dimensional, the big-$M$ formulation is *locally* ideal
    (Vielma, [2015](#bib.bib316)): the extreme points of the linear programming relaxation
    ([4a](#S4.E4.1 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")-[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) naturally
    satisfy the integrality constraints ([4e](#S4.E4.5 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). However,
    this fails to hold in the general multivariate input case. To see why, observe
    that the bounds on the input variables ${\bm{h}}^{l-1}$ are only coupled with
    the logic involving the binary variable $z$ only in an aggregated sense, through
    the coefficients $M^{l}_{i,-}$ and $M^{l}_{i,+}$. In other words, the â€œshapeâ€
    of the pre-activation domain is not incorporated directly into the big-$M$ formulation.
    Furthermore, the strength of this formulation highly depends on the big-$M$ coefficients.
    These coefficients can be obtained using techniques ranging from basic interval
    arithmetic to optimization-based bounds tightening. Grimstad and Andersson ([2019](#bib.bib133))
    show how constraints external to the neural network can yield tighter bounds via
    optimization- or feasibility-based bounds tightening. RÃ¶ssig and Petkovic ([2021](#bib.bib262))
    compare several methods for deriving bounds and further develop optimization-based
    bounds tightening based on pairwise dependencies between variables.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F9.pic1" class="ltx_picture ltx_centering ltx_figure_panel" height="135.23"
    overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg><svg
    id="S4.F9.pic2" class="ltx_picture ltx_centering ltx_figure_panel" height="135.23"
    overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Left: The convex hull of a ReLU neuron ([5](#S4.E5 "In 4.2.2 A stronger
    extended formulation â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")), and Right: the convex relaxation offered by the big-$M$ formulation
    ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) Adapted from Anderson et al. Anderson etÂ al.
    ([2020](#bib.bib5), [2019](#bib.bib4))'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 A stronger extended formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A much stronger MILP formulation can be constructed through a classical method,
    the extended formulation for disjunctions (Balas, [1998](#bib.bib11), Jeroslow
    and Lowe, [1984](#bib.bib163)). This formulation for a given ReLU neuron takes
    the following formÂ (Anderson etÂ al., [2019](#bib.bib4), Section 2.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle=(x^{+},y^{+})+(x^{-},y^{-})$
    |  | (5a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq{\bm{w}}^{l}_{i}x^{-}+b^{l}_{i}(1-z)$
    |  | (5b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle={\bm{w}}^{l}_{i}x^{+}+b^{l}_{i}z\geq
    0$ |  | (5c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L^{l-1}(1-z)$ | $\displaystyle\leq x^{-}\leq U^{l-1}(1-z)$
    |  | (5d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L^{l-1}z$ | $\displaystyle\leq x^{+}\leq U^{l-1}z$ |  |
    (5e) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (5f) |'
  prefs: []
  type: TYPE_TB
- en: This formulation requires one binary variable and $\mathcal{O}(n_{l-1})$ general
    linear constraints and auxiliary continuous variables. It is also locally ideal,
    i.e., as strong as possible. While the number of variables and constraints for
    an individual unit seems quite tame, applying this formulation for unit in a network
    leads to a formulation with $\mathcal{O}(n_{0}+\sum_{l\in{\mathbb{L}}}n_{l}n_{l-1})=\mathcal{O}(|{\mathbb{L}}|n_{\max}^{2})$
    continuous variables and linear constraints. Moreover, while the formulation for
    *an individual unit* is locally ideal, the composition of many locally ideal formulations
    will, in general, fail to be ideal itself. Consider that, while each node can
    be modeled as a two-part disjunction, the full network requires exponentially
    many disjuncts, each corresponding to one activation pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite its strength and relatively modest increase in size relative to the
    big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models
    using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")), it has been empirically
    observed that this formulation often performs worse than expected (Anderson etÂ al.,
    [2019](#bib.bib4), Vielma, [2019](#bib.bib317)), both in the verification setting
    and more broadly.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 A class of intermediate formulations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The previous sections observed that the big-$M$ formulation ([4](#S4.E4 "In
    4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) is compact, but may offer a weak convex relaxation, while
    the extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) offers
    the tightest possible convex relaxation for an individual unit, at the expense
    of a much larger formulation. Kronqvist etÂ al. ([2022](#bib.bib180), [2021](#bib.bib179))
    present a strategy for obtaining formulations intermediate to ([4](#S4.E4 "In
    4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) and ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) in terms
    of both size and strength. The key idea is to partition ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$
    into a number of aggregated variables, ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}=\sum_{p=1}^{P}\hat{x}_{p}$.
    Each auxiliary variable $\hat{x}_{p}$ is defined as a sum of a subset of the $j$-th
    weighted inputs $\hat{x}_{p}=\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$,
    with $\mathbb{S}_{1},...,\mathbb{S}_{P}$ partitioning $\{1,...,n_{l-1}\}$. This
    technique can be applied to the ReLU function, giving the convex hull over the
    directions defined by $\hat{x}_{p}$ (Tsay etÂ al., [2021](#bib.bib312)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left(\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1},h^{l}_{i}\right)$
    | $\displaystyle=(\hat{x}_{p}^{+},y^{+})+(\hat{x}_{p}^{-},y^{-})$ |  | (6a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq\sum_{p}\hat{x}_{p}^{-}+b^{l}_{i}(1-z)$
    |  | (6b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle=\sum_{p}\hat{x}_{p}^{+}+b^{l}_{i}z\geq
    0$ |  | (6c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}(1-z)$ | $\displaystyle\leq\hat{x}^{-}\leq\hat{\bm{M}}_{i,+}^{l}(1-z)$
    |  | (6d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}z$ | $\displaystyle\leq\hat{x}^{+}\leq\hat{\bm{M}}_{i,+}^{l}z$
    |  | (6e) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (6f) |'
  prefs: []
  type: TYPE_TB
- en: Here, the $p$-th elements of $\hat{\bm{M}}_{i,-}^{l}$ and $\hat{\bm{M}}_{i,+}^{l}$
    must satisfy the inequalities
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{M}^{l}_{i,-,p}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{M}^{l}_{i,+,p}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'These coefficients can be derived using techniques analagous to those for the
    big-$M$ formulation (note that tighter bounds may be derived by considering $\hat{x}^{-}$
    and $\hat{x}^{+}$ separately). Observe that when $P=1$, we recover the same tightness
    as the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), as, intuitively, the
    formulation is built over a single â€œdirectionâ€ corresponding to ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$.
    Conversely, when $P=n_{l-1}$, we recover the tightness of the extended formulation
    ([5](#S4.E5 "In 4.2.2 A stronger extended formulation â€£ 4.2 Exact models using
    mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When
    Deep Learning Meets Polyhedral Theory: A Survey")), as each direction corresponds
    to a single element of ${\bm{h}}^{l-1}$. Tsay etÂ al. ([2021](#bib.bib312)) study
    partitioning strategies and show that intermediate values of $P$ result in formulations
    that can outperform the two extremes, by balancing formulation size and strength.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.4 Cutting plane methods: Trading variables for inequalities'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) achieves
    its strength through the introduction of auxiliary continuous variables. However,
    it is possible to produce a formulation of equal strength by projecting out these
    auxiliary variables, leaving an ideal formulation in the â€œoriginalâ€ $({\bm{h}}^{l-1},h^{l}_{i},z)$
    variable space. While in general this projection may be difficult computationally,
    for the simple structure of a single ReLU neuron it is possible to characterize
    in closed form. The formulation is given by Anderson etÂ al. ([2020](#bib.bib5),
    [2019](#bib.bib4)) as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (7a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{j\in J}w^{l}_{i,j}(h^{l-1}_{i}-\breve{L}^{l}_{j}(1-z))+\left(b+\sum_{j\not\in
    J}w^{l}_{i,j}\breve{U}_{j}\right)z\quad\forall J\subseteq\llbracket n_{l-1}\rrbracket$
    |  | (7b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}_{\geq
    0}$ |  | (7c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\},$ |  | (7d) |'
  prefs: []
  type: TYPE_TB
- en: where notationally, for each $j\in\llbracket n_{l-1}\rrbracket$, we take
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.Ex1b.m1.12" class="ltx_Math" alttext="\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\'
  prefs: []
  type: TYPE_NORMAL
- en: L^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}" display="block"><semantics id="S4.Ex1b.m1.12a"><mrow
    id="S4.Ex1b.m1.12.12.2" xref="S4.Ex1b.m1.12.12.3.cmml"><mrow id="S4.Ex1b.m1.11.11.1.1"
    xref="S4.Ex1b.m1.11.11.1.1.cmml"><msubsup id="S4.Ex1b.m1.11.11.1.1.2" xref="S4.Ex1b.m1.11.11.1.1.2.cmml"><mover
    accent="true" id="S4.Ex1b.m1.11.11.1.1.2.2.2" xref="S4.Ex1b.m1.11.11.1.1.2.2.2.cmml"><mi
    id="S4.Ex1b.m1.11.11.1.1.2.2.2.2" xref="S4.Ex1b.m1.11.11.1.1.2.2.2.2.cmml">L</mi><mo
    id="S4.Ex1b.m1.11.11.1.1.2.2.2.1" xref="S4.Ex1b.m1.11.11.1.1.2.2.2.1.cmml">Ë˜</mo></mover><mi
    id="S4.Ex1b.m1.11.11.1.1.2.3" xref="S4.Ex1b.m1.11.11.1.1.2.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.11.11.1.1.2.2.3" xref="S4.Ex1b.m1.11.11.1.1.2.2.3.cmml"><mi id="S4.Ex1b.m1.11.11.1.1.2.2.3.2"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.11.11.1.1.2.2.3.1"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3.1.cmml">âˆ’</mo><mn id="S4.Ex1b.m1.11.11.1.1.2.2.3.3"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.Ex1b.m1.11.11.1.1.1"
    xref="S4.Ex1b.m1.11.11.1.1.1.cmml">=</mo><mrow id="S4.Ex1b.m1.11.11.1.1.3.2" xref="S4.Ex1b.m1.11.11.1.1.3.1.cmml"><mrow
    id="S4.Ex1b.m1.4.4" xref="S4.Ex1b.m1.9.9.1.cmml"><mo id="S4.Ex1b.m1.4.4.5" xref="S4.Ex1b.m1.9.9.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.Ex1b.m1.4.4.4"
    xref="S4.Ex1b.m1.9.9.1.cmml"><mtr id="S4.Ex1b.m1.4.4.4a" xref="S4.Ex1b.m1.9.9.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.4.4.4b" xref="S4.Ex1b.m1.9.9.1.cmml"><msubsup
    id="S4.Ex1b.m1.1.1.1.1.1.1" xref="S4.Ex1b.m1.1.1.1.1.1.1.cmml"><mi id="S4.Ex1b.m1.1.1.1.1.1.1.2.2"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.2.cmml">L</mi><mi id="S4.Ex1b.m1.1.1.1.1.1.1.3"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.3.cmml">j</mi><mrow id="S4.Ex1b.m1.1.1.1.1.1.1.2.3"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2.cmml">l</mi><mo
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.1" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.1.cmml">âˆ’</mo><mn
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.4.4.4c" xref="S4.Ex1b.m1.9.9.1.cmml"><mrow
    id="S4.Ex1b.m1.2.2.2.2.2.1" xref="S4.Ex1b.m1.2.2.2.2.2.1.cmml"><msubsup id="S4.Ex1b.m1.2.2.2.2.2.1.4"
    xref="S4.Ex1b.m1.2.2.2.2.2.1.4.cmml"><mi id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.4" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1" xref="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.4.1" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.2.2.2.2.2.1.3" xref="S4.Ex1b.m1.2.2.2.2.2.1.3.cmml">â‰¥</mo><mn id="S4.Ex1b.m1.2.2.2.2.2.1.5"
    xref="S4.Ex1b.m1.2.2.2.2.2.1.5.cmml">0</mn></mrow></mtd></mtr><mtr id="S4.Ex1b.m1.4.4.4d"
    xref="S4.Ex1b.m1.9.9.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.4.4.4e"
    xref="S4.Ex1b.m1.9.9.1.cmml"><msubsup id="S4.Ex1b.m1.3.3.3.3.1.1" xref="S4.Ex1b.m1.3.3.3.3.1.1.cmml"><mi
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.2" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.2.cmml">U</mi><mi
    id="S4.Ex1b.m1.3.3.3.3.1.1.3" xref="S4.Ex1b.m1.3.3.3.3.1.1.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.3" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.1"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.1.cmml">âˆ’</mo><mn id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S4.Ex1b.m1.4.4.4f" xref="S4.Ex1b.m1.9.9.1.cmml"><mrow id="S4.Ex1b.m1.4.4.4.4.2.1"
    xref="S4.Ex1b.m1.4.4.4.4.2.1.cmml"><msubsup id="S4.Ex1b.m1.4.4.4.4.2.1.4" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.cmml"><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.4" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1" xref="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.4.1" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.4.4.4.4.2.1.3" xref="S4.Ex1b.m1.4.4.4.4.2.1.3.cmml"><</mo><mn id="S4.Ex1b.m1.4.4.4.4.2.1.5"
    xref="S4.Ex1b.m1.4.4.4.4.2.1.5.cmml">0</mn></mrow></mtd></mtr></mtable></mrow><mi
    id="S4.Ex1b.m1.10.10" xref="S4.Ex1b.m1.10.10.cmml">and</mi></mrow></mrow><mrow
    id="S4.Ex1b.m1.12.12.2.2" xref="S4.Ex1b.m1.12.12.2.2.cmml"><msubsup id="S4.Ex1b.m1.12.12.2.2.2"
    xref="S4.Ex1b.m1.12.12.2.2.2.cmml"><mover accent="true" id="S4.Ex1b.m1.12.12.2.2.2.2.2"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.2.cmml"><mi id="S4.Ex1b.m1.12.12.2.2.2.2.2.2" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.2.cmml">U</mi><mo
    id="S4.Ex1b.m1.12.12.2.2.2.2.2.1" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.1.cmml">Ë˜</mo></mover><mi
    id="S4.Ex1b.m1.12.12.2.2.2.3" xref="S4.Ex1b.m1.12.12.2.2.2.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.12.12.2.2.2.2.3" xref="S4.Ex1b.m1.12.12.2.2.2.2.3.cmml"><mi id="S4.Ex1b.m1.12.12.2.2.2.2.3.2"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.12.12.2.2.2.2.3.1"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.1.cmml">âˆ’</mo><mn id="S4.Ex1b.m1.12.12.2.2.2.2.3.3"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.Ex1b.m1.12.12.2.2.1"
    xref="S4.Ex1b.m1.12.12.2.2.1.cmml">=</mo><mrow id="S4.Ex1b.m1.8.8" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mo
    id="S4.Ex1b.m1.8.8.5" xref="S4.Ex1b.m1.12.12.2.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" id="S4.Ex1b.m1.8.8.4" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mtr
    id="S4.Ex1b.m1.8.8.4a" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S4.Ex1b.m1.8.8.4b" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><msubsup
    id="S4.Ex1b.m1.5.5.1.1.1.1" xref="S4.Ex1b.m1.5.5.1.1.1.1.cmml"><mi id="S4.Ex1b.m1.5.5.1.1.1.1.2.2"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.2.cmml">U</mi><mi id="S4.Ex1b.m1.5.5.1.1.1.1.3"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.3.cmml">j</mi><mrow id="S4.Ex1b.m1.5.5.1.1.1.1.2.3"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2.cmml">l</mi><mo
    id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.1" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.1.cmml">âˆ’</mo><mn
    id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.8.8.4c" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mrow
    id="S4.Ex1b.m1.6.6.2.2.2.1" xref="S4.Ex1b.m1.6.6.2.2.2.1.cmml"><msubsup id="S4.Ex1b.m1.6.6.2.2.2.1.4"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4.cmml"><mi id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2" xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.4" xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1" xref="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.4.1" xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2" xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3" xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.6.6.2.2.2.1.3" xref="S4.Ex1b.m1.6.6.2.2.2.1.3.cmml">â‰¥</mo><mn id="S4.Ex1b.m1.6.6.2.2.2.1.5"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.5.cmml">0</mn></mrow></mtd></mtr><mtr id="S4.Ex1b.m1.8.8.4d"
    xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left"
    id="S4.Ex1b.m1.8.8.4e" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><msubsup id="S4.Ex1b.m1.7.7.3.3.1.1"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.cmml"><mi id="S4.Ex1b.m1.7.7.3.3.1.1.2.2" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.2.cmml">L</mi><mi
    id="S4.Ex1b.m1.7.7.3.3.1.1.3" xref="S4.Ex1b.m1.7.7.3.3.1.1.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.7.7.3.3.1.1.2.3" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.1"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.1.cmml">âˆ’</mo><mn id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S4.Ex1b.m1.8.8.4f" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mrow
    id="S4.Ex1b.m1.8.8.4.4.2.1" xref="S4.Ex1b.m1.8.8.4.4.2.1.cmml"><msubsup id="S4.Ex1b.m1.8.8.4.4.2.1.4"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4.cmml"><mi id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2" xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.4" xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1" xref="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.4.1" xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2" xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3" xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.8.8.4.4.2.1.3" xref="S4.Ex1b.m1.8.8.4.4.2.1.3.cmml"><</mo><mn id="S4.Ex1b.m1.8.8.4.4.2.1.5"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.5.cmml">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S4.Ex1b.m1.12b"><apply id="S4.Ex1b.m1.12.12.3.cmml"
    xref="S4.Ex1b.m1.12.12.2"><csymbol cd="ambiguous" id="S4.Ex1b.m1.12.12.3a.cmml"
    xref="S4.Ex1b.m1.12.12.2.3">formulae-sequence</csymbol><apply id="S4.Ex1b.m1.11.11.1.1.cmml"
    xref="S4.Ex1b.m1.11.11.1.1"><apply id="S4.Ex1b.m1.11.11.1.1.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.11.11.1.1.2.1.cmml" xref="S4.Ex1b.m1.11.11.1.1.2">subscript</csymbol><apply
    id="S4.Ex1b.m1.11.11.1.1.2.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.11.11.1.1.2.2.1.cmml" xref="S4.Ex1b.m1.11.11.1.1.2">superscript</csymbol><apply
    id="S4.Ex1b.m1.11.11.1.1.2.2.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.2.2"><ci id="S4.Ex1b.m1.11.11.1.1.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.2.1">Ë˜</ci><ci id="S4.Ex1b.m1.11.11.1.1.2.2.2.2.cmml"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.2.2">ğ¿</ci></apply><apply id="S4.Ex1b.m1.11.11.1.1.2.2.3.cmml"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3"><ci id="S4.Ex1b.m1.11.11.1.1.2.2.3.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.2.3.2">ğ‘™</ci><cn
    type="integer" id="S4.Ex1b.m1.11.11.1.1.2.2.3.3.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.2.3.3">1</cn></apply></apply><ci
    id="S4.Ex1b.m1.11.11.1.1.2.3.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.3">ğ‘—</ci></apply><list
    id="S4.Ex1b.m1.11.11.1.1.3.1.cmml" xref="S4.Ex1b.m1.11.11.1.1.3.2"><apply id="S4.Ex1b.m1.9.9.1.cmml"
    xref="S4.Ex1b.m1.4.4"><csymbol cd="latexml" id="S4.Ex1b.m1.9.9.1.1.cmml" xref="S4.Ex1b.m1.4.4.5">cases</csymbol><apply
    id="S4.Ex1b.m1.1.1.1.1.1.1.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1">subscript</csymbol><apply
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1">superscript</csymbol><ci
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.2">ğ¿</ci><apply
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3"><ci id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2.cmml"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2">ğ‘™</ci><cn type="integer" id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3.cmml"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex1b.m1.1.1.1.1.1.1.3.cmml"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.3">ğ‘—</ci></apply><apply id="S4.Ex1b.m1.2.2.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.2.2.2.2.2.1"><apply id="S4.Ex1b.m1.2.2.2.2.2.1.4.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.2.2.2.2.2.1.4.1.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4">subscript</csymbol><apply
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.1.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4">superscript</csymbol><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2">ğ‘¤</ci><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3">ğ‘™</ci></apply><list
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.3.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.4"><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1">ğ‘–</ci><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2">ğ‘—</ci></list></apply><cn
    type="integer" id="S4.Ex1b.m1.2.2.2.2.2.1.5.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.5">0</cn></apply><apply
    id="S4.Ex1b.m1.3.3.3.3.1.1.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.3.3.3.3.1.1.1.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1">subscript</csymbol><apply
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.1.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1">superscript</csymbol><ci
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.2.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.2">ğ‘ˆ</ci><apply
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3"><ci id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2.cmml"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2">ğ‘™</ci><cn type="integer" id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3.cmml"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex1b.m1.3.3.3.3.1.1.3.cmml"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.3">ğ‘—</ci></apply><apply id="S4.Ex1b.m1.4.4.4.4.2.1.cmml"
    xref="S4.Ex1b.m1.4.4.4.4.2.1"><apply id="S4.Ex1b.m1.4.4.4.4.2.1.4.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.4.4.4.4.2.1.4.1.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4">subscript</csymbol><apply
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.1.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4">superscript</csymbol><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2">ğ‘¤</ci><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3">ğ‘™</ci></apply><list
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.3.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.4"><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1">ğ‘–</ci><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2">ğ‘—</ci></list></apply><cn
    type="integer" id="S4.Ex1b.m1.4.4.4.4.2.1.5.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.5">0</cn></apply></apply><ci
    id="S4.Ex1b.m1.10.10.cmml" xref="S4.Ex1b.m1.10.10">and</ci></list></apply><apply
    id="S4.Ex1b.m1.12.12.2.2.cmml" xref="S4.Ex1b.m1.12.12.2.2"><apply id="S4.Ex1b.m1.12.12.2.2.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1b.m1.12.12.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2">subscript</csymbol><apply id="S4.Ex1b.m1.12.12.2.2.2.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1b.m1.12.12.2.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2">superscript</csymbol><apply id="S4.Ex1b.m1.12.12.2.2.2.2.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.2"><ci id="S4.Ex1b.m1.12.12.2.2.2.2.2.1.cmml" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.1">Ë˜</ci><ci
    id="S4.Ex1b.m1.12.12.2.2.2.2.2.2.cmml" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.2">ğ‘ˆ</ci></apply><apply
    id="S4.Ex1b.m1.12.12.2.2.2.2.3.cmml" xref="S4.Ex1b.m1.12.12.2.2.2.2.3"><ci id="S4.Ex1b.m1.12.12.2.2.2.2.3.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.2">ğ‘™</ci><cn type="integer" id="S4.Ex1b.m1.12.12.2.2.2.2.3.3.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.3">1</cn></apply></apply><ci id="S4.Ex1b.m1.12.12.2.2.2.3.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.3">ğ‘—</ci></apply><apply id="S4.Ex1b.m1.12.12.2.2.3.1.cmml"
    xref="S4.Ex1b.m1.8.8"><csymbol cd="latexml" id="S4.Ex1b.m1.12.12.2.2.3.1.1.cmml"
    xref="S4.Ex1b.m1.8.8.5">cases</csymbol><apply id="S4.Ex1b.m1.5.5.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.5.5.1.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1">subscript</csymbol><apply id="S4.Ex1b.m1.5.5.1.1.1.1.2.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.5.5.1.1.1.1.2.1.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1">superscript</csymbol><ci id="S4.Ex1b.m1.5.5.1.1.1.1.2.2.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.2">ğ‘ˆ</ci><apply id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3"><ci id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2.cmml" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2">ğ‘™</ci><cn
    type="integer" id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3.cmml" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3">1</cn></apply></apply><ci
    id="S4.Ex1b.m1.5.5.1.1.1.1.3.cmml" xref="S4.Ex1b.m1.5.5.1.1.1.1.3">ğ‘—</ci></apply><apply
    id="S4.Ex1b.m1.6.6.2.2.2.1.cmml" xref="S4.Ex1b.m1.6.6.2.2.2.1"><apply id="S4.Ex1b.m1.6.6.2.2.2.1.4.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.6.6.2.2.2.1.4.1.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4">subscript</csymbol><apply id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.1.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4">superscript</csymbol><ci id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2">ğ‘¤</ci><ci id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3">ğ‘™</ci></apply><list id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.3.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.4"><ci id="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1">ğ‘–</ci><ci id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2">ğ‘—</ci></list></apply><cn type="integer" id="S4.Ex1b.m1.6.6.2.2.2.1.5.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.5">0</cn></apply><apply id="S4.Ex1b.m1.7.7.3.3.1.1.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.7.7.3.3.1.1.1.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1">subscript</csymbol><apply id="S4.Ex1b.m1.7.7.3.3.1.1.2.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.7.7.3.3.1.1.2.1.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1">superscript</csymbol><ci id="S4.Ex1b.m1.7.7.3.3.1.1.2.2.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.2">ğ¿</ci><apply id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3"><ci id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2.cmml" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2">ğ‘™</ci><cn
    type="integer" id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3.cmml" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3">1</cn></apply></apply><ci
    id="S4.Ex1b.m1.7.7.3.3.1.1.3.cmml" xref="S4.Ex1b.m1.7.7.3.3.1.1.3">ğ‘—</ci></apply><apply
    id="S4.Ex1b.m1.8.8.4.4.2.1.cmml" xref="S4.Ex1b.m1.8.8.4.4.2.1"><apply id="S4.Ex1b.m1.8.8.4.4.2.1.4.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.8.8.4.4.2.1.4.1.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4">subscript</csymbol><apply id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.1.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4">superscript</csymbol><ci id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2">ğ‘¤</ci><ci id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3">ğ‘™</ci></apply><list id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.3.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.4"><ci id="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1">ğ‘–</ci><ci id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2">ğ‘—</ci></list></apply><cn type="integer" id="S4.Ex1b.m1.8.8.4.4.2.1.5.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.5">0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S4.Ex1b.m1.12c">\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ L^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}</annotation></semantics></math> |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'We note a few points of interest about this formulation. First, it is ideal,
    and so recovers the convex hull of a ReLU activation function, coupled with its
    preactivation affine function and bounds on each of the inputs to that affine
    function. Second, it can be shown that, under very mild conditions, each of the
    exponentially many constraints in ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods:
    Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) are necessary to ensure this property; none are redundant
    and can be removed without affecting the relaxation quality. Third, note that
    by selecting only those constraints in ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane
    methods: Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) corresponding to $J=\emptyset$ and $J=\llbracket
    n_{l-1}\rrbracket$, we recover the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The
    big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")) in the case where $\mathcal{D}^{l-1}=[L^{l-1},U^{l-1}]$. This suggests
    a practical approach for using this large family of inequalities: Start with the
    big-$M$ formulation, and then dynamically generate violated inequalities from
    ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods: Trading variables for inequalities
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) as-needed
    in a cutting plane procedure. As shown by Anderson etÂ al. ([2020](#bib.bib5)),
    this separation problem is separable in the input variables, and hence can be
    completed in $\mathcal{O}(n_{l-1})$ time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cutting plane strategy is in general compatible with weaker formulations,
    such as relaxation-based verification (Zhang etÂ al., [2022](#bib.bib356)) and
    formulations from the class ([6](#S4.E6 "In 4.2.3 A class of intermediate formulations
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). In fact,
    Tsay etÂ al. ([2021](#bib.bib312)) show that the intermediate formulations in ([6](#S4.E6
    "In 4.2.3 A class of intermediate formulations â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) effectively pre-select a number of inequalities
    from ([7b](#S4.E7.2 "In 7 â€£ 4.2.4 Cutting plane methods: Trading variables for
    inequalities â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")), in terms of their continuous relaxations. While adding these constraints
    results in a tighter continuous relaxation, the added constraints can eventually
    significantly increase the model size. Practical implementations may therefore
    only perform cut generation at a limited number of branch-and-bound search nodes
    (DeÂ Palma etÂ al., [2021](#bib.bib77), Tsay etÂ al., [2021](#bib.bib312)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A subtlety when using ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading
    variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey"))'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This third point above raises a subtlety discussed in the literatureÂ (DeÂ Palma
    etÂ al., [2021](#bib.bib77), Appendix F). Often, additional structural information
    is known about $\mathcal{D}^{l-1}$ beyond bounds on the variables. In this case,
    it is typically possible to derive tighter values for the big-$M$ coefficients.
    In this case, when using a separation-based approach it is preferable to initialize
    the formulation with these tightened big-$M$ constraints, and then proceed with
    the cutting plane approach as normal from there.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Scaling further: Convex relaxations and linear programming'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The above demonstrate MILP as a powerful framework for exactly modeling complex,
    nonconvex trained neural networks, but standard solvers are often not sufficiently
    scalable to adequately handle large-scale networks. A natural approach to increase
    the scalability, then, is to *relax* the network in some manner, and then apply
    convex optimization methods. For the verification problem discussed in SectionÂ [4.1.1](#S4.SS1.SSS1
    "4.1.1 Neural network verification â€£ 4.1 Applications of optimization over trained
    networks â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), this yields what is known as an *incomplete verifier*:
    any certification of robustness provided can be trusted (no false positives),
    but there may be robust instances that the method cannot prove are (some false
    negatives). In other words, over-approximation produces a verifier that is sound,
    but not complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While a variety of methods exist for accomplishing this, in this section we
    briefly outline techniques relevant to polyhedral theory. In particular, we focus
    on some techniques for building convex polyhedral relaxations. The most natural
    convex relaxation for a MILP formulation is its linear programming (LP) relaxation,
    constructed by dropping any integrality constraints. For example, the LP relaxation
    of ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) is given by the system ([4a](#S4.E4.1 "In
    4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")-[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). This is a compact linear
    programming relaxation for a ReLU-based network, and is the basis for methods
    due to Bunel etÂ al. ([2020a](#bib.bib41)) and Ehlers ([2017](#bib.bib87)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Projecting the big-$M$ and ideal MILP formulations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section examines projections of the linear relaxations of formulations
    ([4](#S4.E4 "In 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) and ([7](#S4.E7 "In 4.2.4 Cutting plane methods:
    Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '(Projecting the big-$M$). Note that the LP relaxation given by ([4a](#S4.E4.1
    "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")â€“[4d](#S4.E4.4 "In 4 â€£ 4.2.1 The big-ğ‘€ formulation â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) maintains the variables
    $z^{l}_{i}$ in the formulation, though they are no longer required to satisfy
    integrality. Since these variables are â€œauxiliaryâ€ and are no longer necessary
    to encode the nonconvexity of the problem, they can be projected out without altering
    the quality of the convex relaxation. Doing this yields what is commonly known
    as the â€œtriangleâ€ or â€œ$\Delta$â€ relaxation (Salman etÂ al., [2019](#bib.bib269)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (8a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\frac{M^{l}_{i,+}}{M^{l}_{i,+}-M^{l}_{i,-}}({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})$
    |  | (8b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}.$ |  | (8c) |'
  prefs: []
  type: TYPE_TB
- en: 'While the LP relaxation ([8](#S4.E8 "In 4.3.1 Projecting the big-ğ‘€ and ideal
    MILP formulations â€£ 4.3 Scaling further: Convex relaxations and linear programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) for an individual unit is compact, modern neural network architectures
    regularly comprise millions of units. The resulting LP relaxation for the entire
    network may then require millions of variables and constraints. Additionally,
    unless special precautions are taken, many of these constraints will be relatively
    dense. All this quickly leads to LP that are beyond the scope of modern off-the-shelf
    LP solvers. As a result, researchers have explored alternative schemes for scaling
    LP-based methods to these larger networks. Salman etÂ al. ([2019](#bib.bib269))
    present a framework for LP-based methods (LP solvers, propagation, dual methods),
    which we review in the following subsections. However, they do not account for
    the ideal formulation developed in later works (Anderson etÂ al., [2020](#bib.bib5),
    DeÂ Palma etÂ al., [2021](#bib.bib77)).'
  prefs: []
  type: TYPE_NORMAL
- en: '(Projecting the ideal). FigureÂ [9](#S4.F9 "Figure 9 â€£ 4.2.1 The big-ğ‘€ formulation
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") shows
    that the triangle (big-$M$) relaxation fails to recover the convex hull of the
    ReLU activation function and the multivariate input to the affine pre-activation
    function. We can similarly project the LP relaxation of the ideal formulation
    ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables for inequalities
    â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) into
    the space of input/output variables (Anderson etÂ al., [2020](#bib.bib5)), yielding
    a description for the convex hull of $\{({\bm{h}}^{l-1},h^{l}_{i})|L^{l-1}\leq{\bm{h}}^{l-1}\leq
    U^{l-1},\>h^{l}_{i}=\sigma({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (9a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{k\in I}w_{i,k}^{l}(x_{k}-\breve{L}_{k})+\frac{\ell(I)}{\breve{U}_{h}-\breve{L}_{h}}(x_{h}-\breve{L}_{h})\quad\forall(I,h)\in\mathcal{J}$
    |  | (9b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0},$ |  | (9c) |'
  prefs: []
  type: TYPE_TB
- en: where $l(I)\coloneqq\sum_{k\in I}w^{l}_{i,k}\breve{L}_{k}+\sum_{k\not\in I}w^{l}_{i,k}\breve{U}_{k}+b^{l}_{i}$
    and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{J}\coloneqq\Set{(I,h)\in 2^{\llbracket n_{l-1}\rrbracket}\times\llbracket
    n_{l-1}\rrbracket}{l(I)\geq 0,\>l(I\cup\{h\}<0,\>w^{l}_{i,k}\neq 0\forall k\in
    I}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Anderson etÂ al. ([2020](#bib.bib5)) also show that the inequalities ([9b](#S4.E9.2
    "In 9 â€£ 4.3.1 Projecting the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling further:
    Convex relaxations and linear programming â€£ 4 Optimizing Over a Trained Neural
    Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) can be separated
    over in $\mathcal{O}(n_{l-1})$ time. Interestingly, in contrast to ([7](#S4.E7
    "In 4.2.4 Cutting plane methods: Trading variables for inequalities â€£ 4.2 Exact
    models using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), the number of facet-defining
    inequalities depends heavily on the affine function. While in the worst case the
    number of inequalities will grow exponentially in the input dimension, there exist
    instances where the convex hull can be fully described with only $\mathcal{O}(n_{l-1})$
    inequalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Dual decomposition methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A first approach for greater scalability for LP-based methods is decomposition,
    a standard technique in the large-scale optimization community. Indeed, the cutting
    plane approach of SectionÂ [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting plane methods: Trading
    variables for inequalities â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") can be viewed as a decomposition method operating in the original
    variable space. However, the method is initialized with the big-$M$ formulation
    for each neuron, and hence this initial model will be of size roughly equal to
    the size of the network. Therefore, it should be understood to use decomposition
    to provide a tighter verification bound, rather than for providing greater scalability
    to larger networks.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, dual decomposition can be used to scale inexact verification methods
    to larger networks. Such methods maintain dual feasible solutions throughout the
    algorithm, meaning that upon termination they yield valid dual bounds on the verification
    instance, and hence serve as incomplete verifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wong and Kolter ([2018](#bib.bib335)), Wong etÂ al. ([2018](#bib.bib336)) use
    as their starting point the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting
    the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling further: Convex relaxations
    and linear programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) for each neuron, and then take the
    standard LP dual of the (relaxed) verification problem. Alternatively, Dvijotham
    etÂ al. ([2018b](#bib.bib85)) propose a Lagrangian-based approach for decomposing
    the original nonlinear formulation of the problem ([3](#S4.E3 "In Example 4 â€£
    4.1.1 Neural network verification â€£ 4.1 Applications of optimization over trained
    networks â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")). Crucially, since the complicating constraints
    coupling the layers in the network are imposed as objective penalties instead
    of â€œhardâ€ constraints, the optimization problem (given fixed dual variables) decomposes
    along each layer and the subproblems induced by the separability can be solved
    in closed form. This approach dualizes separately the equations characterizing
    the pre-activation and post-activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\mu,\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\mu_{k}^{T}(\hat{{\bm{h}}}^{k}-{\bm{W}}^{k}{\bm{h}}^{k-1}-{\bm{b}}^{k})+\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sigma(L^{k})\leq{\bm{h}}^{k}\leq\sigma(U^{k})\quad\forall
    k\in\llbracket n-1\rrbracket.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, the $\hat{{\bm{h}}}$ variables track the pre-activation values for the
    neurons in the network. The dual variables $\mu_{k}^{T}$ correspond to the equality
    constraints defining the pre-activation values, $\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+{\bm{b}}^{k}$.
    Likewise, the dual variables $\lambda_{k}^{T}$ correspond to enforcing the ReLU
    activation function, ${\bm{h}}^{k}=\sigma(\hat{{\bm{h}}}^{k})=\mathrm{max}(0,\hat{{\bm{h}}}^{k})$.
    Any feasible solution for the neural network is feasible for this dualized problem,
    making the multiplier terms for $\mu_{k}^{T}$ and $\lambda_{k}^{T}$ zero. Thus,
    the inner problem gives a lower bound for the original problemâ€”a property known
    as *weak duality*. The outer (dual) problem optimizing over the Lagrangian multipliers
    then seeks to maximize this lower bound, i.e., to give the tightest possible lower
    bound. This can be solved using a subgradient-based method, or learned along with
    the model parameters in a â€œpredictor-verifierâ€ approach (Dvijotham etÂ al., [2018a](#bib.bib84)).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, this approach can be combined with other relaxation-based
    methods. The Lagrangian decomposition can be applied to dualize only the coupling
    constraints between layers, and a convex relaxation used for the activation function
    (Bunel etÂ al., [2020a](#bib.bib41)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+b^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}^{k}\geq 0\quad\forall k\in\llbracket n-1\rrbracket$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}_{i}^{k}\geq\hat{{\bm{h}}}_{i}^{k}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}^{k}_{i}\leq\frac{U^{k}_{i}(\hat{{\bm{h}}}^{k}_{i}-L^{k}_{i})}{U^{k}_{i}-L^{k}_{i}}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the final three constraints apply the big-$M$/triangle relaxation
    ([8](#S4.E8 "In 4.3.1 Projecting the big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling
    further: Convex relaxations and linear programming â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to each
    ReLU activation function. The dual problem can then be solved via subgradient-based
    methods, proximal algorithms, or, more recently, a projected gradient descent
    method applied to a nonconvex reformulation of the problem (Bunel etÂ al., [2020c](#bib.bib44)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, DeÂ Palma etÂ al. ([2021](#bib.bib77)) presented a dual decomposition
    approach based on ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables
    for inequalities â€£ 4.2 Exact models using mixed-integer programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey")). However, creating a dual formulation from the exponential number of
    constraints produces an exponential number of dual variables. The authors therefore
    propose to maintain an â€œactive setâ€ of dual variables to keep the problem sparse.
    A selection algorithm (e.g., selecting entries that maximize an estimated super-gradient)
    can then be used to append the active set. Similar to the above discussion on
    cut generation, the frequency of appending the active set should be chosen strategically.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Fourier-Motzkin elimination and propagation algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Alternatively, one can project out *all* of the decision variables. For example,
    in order to solve the linear programming problem $\min_{x\in\mathcal{X}}c\cdot
    x$, we can augment the problem with a new decision variable to $\min_{(x,y)\in\Gamma}y$
    for $\Gamma\vcentcolon=\Set{(x,y)\in\mathcal{X}\times\mathbb{R}:y=c\cdot x}$,
    and project out the $x$ variables. The transformed problem is the a trivial univariate
    optimization problem: $\min_{y\in\operatorname{Proj}_{y}(\Gamma)}y$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the complexity of the approach described is hidden in the projection
    step, or building $\operatorname{Proj}_{y}(\Gamma)$. The most well-known algorithm
    for computing projections of linear inequality systems is Fourier-Motzkin elimination,
    described by Dantzig and Eaves ([1973](#bib.bib74)), which is notorious for its
    practical inefficiency. The process effectively comprises replacing variables
    from a set of inequalities with all possible implied inequalities, which can produce
    many unnecessary constraints. However, it turns out that neural network verification
    problems are well-structured in such a way that Fourier-Motzkin elimination can
    be performed very efficiently: for instance, by imposing one inequality upper
    bounding and one inequality lower bounding each ReLU function. Note that while
    Section [3.2](#S3.SS2 "3.2 The algebra of linear regions â€£ 3 The Linear Regions
    of a Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey") describes
    the use of Fourier-Motzkin elimination to obtain *exact* input-output relationships
    in linear regions of neural networks, here we are interested in obtaining linear
    *bounds* for a nonlinear function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/123fb03cab46add54faaa496e472ed4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Convex approximations for the ReLU function commonly used by propagation
    algorithms, given as a function of the preactivation function $\hat{h_{i}^{l}}$.
    The ReLU applies $h_{i}^{l}=\max(0,\hat{h_{i}^{l}})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, this general approach was independently developed in the verification
    community. While MILP research has focused on formulations tighter than the big-M,
    such as ([5](#S4.E5 "In 4.2.2 A stronger extended formulation â€£ 4.2 Exact models
    using mixed-integer programming â€£ 4 Optimizing Over a Trained Neural Network â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")) and ([6](#S4.E6 "In 4.2.3
    A class of intermediate formulations â€£ 4.2 Exact models using mixed-integer programming
    â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")), the verification community often prefers greater scalability
    at the price of weaker convex relaxations. The continuous relaxation of the big-M
    is equivalent to the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting the
    big-ğ‘€ and ideal MILP formulations â€£ 4.3 Scaling further: Convex relaxations and
    linear programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")): the optimal convex relaxation for a single
    input, or in terms of the aggregated pre-activation function, as shown in Figure
    [10](#S4.F10 "Figure 10 â€£ 4.3.3 Fourier-Motzkin elimination and propagation algorithms
    â€£ 4.3 Scaling further: Convex relaxations and linear programming â€£ 4 Optimizing
    Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A
    Survey"). However, the lower bound involves two linear constraints, which is not
    used in several propagation-based verification tools owing to scalability or compatibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Such tools use methods such as abstract transformers to propagate polyhedral
    bounds, i.e., zonotopes, through the layers of a neural network. DeepZ (Singh
    etÂ al., [2018](#bib.bib289)), Fast-Lin (Weng etÂ al., [2018](#bib.bib330)), and
    Neurify (Wang etÂ al., [2018a](#bib.bib326)) employ a parallel approximation, with
    the latter also implementing a branch-and-bound procedure towards completeness.
    Subsequently, DeepPoly (Singh etÂ al., [2019b](#bib.bib291)) and CROWN (Zhang etÂ al.,
    [2018a](#bib.bib354)) select between the zero and identity approximations by minimizing
    over-approximation area. OSIP (Hashemi etÂ al., [2021](#bib.bib140)) selects between
    the three approximations using optimization: approximations for a layer are select
    jointly to minimise bounds for the following layer. These technologies are also
    compatible with interval bounds, propagating box domains (Mirman etÂ al., [2018](#bib.bib220)).
    Interestingly, bounds on neural network weights can also be propagated using similar
    methods, allowing reachability analysis of Bayesian neural networks (Wicker etÂ al.,
    [2020](#bib.bib332)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tjandraatmadja etÂ al. ([2020](#bib.bib308)) provide an interpretation of these
    propagation techniques through the lens of Fourier-Motzkin elimination. Consider
    the problem of propagating bounds through a ReLU neural network: for a node $h_{i}^{l}=\mathrm{max}\{0,\hat{h_{i}^{l}}\}$,
    convex bounds for $h_{i}^{l}$ can be obtained given bounds for $\hat{h}_{i}^{l}$
    (Figure [10](#S4.F10 "Figure 10 â€£ 4.3.3 Fourier-Motzkin elimination and propagation
    algorithms â€£ 4.3 Scaling further: Convex relaxations and linear programming â€£
    4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")). Assuming the inputs are outputs of ReLU activations in the
    previous layer, $\hat{h}_{i}^{l}={\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$. Computing
    an upper bound can then be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{{\bm{h}}^{l-1}}$ | $\displaystyle{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})\forall
    k\in\{1,...,n_{l-1}\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'As the objective function is linear, the solution of this problem can be computed
    by propagation without explicit optimization. For each element in ${\bm{h}}^{l-1}$,
    we only need to consider the associated objective coefficient in ${\bm{w}}_{i}^{l}$
    to determine whether $\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}$ or $h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})$
    will be the active inequality at the optimal solution. We can thus replace $h^{l-1}_{k}$
    with $\mathcal{L}_{k}({\bm{h}}^{l-2})$ or $\mathcal{G}_{k}({\bm{h}}^{l-2})$ accordingly.
    This projection is mathematically equivalent to applying Fourier-Motzkin elimination,
    while avoiding redundant inequalities resulting from the â€˜non-selectedâ€™ bounding
    function. Repeating this procedure for each layer results in a convex relaxation
    for the outputs that only involves the input variables. We naturally observe the
    desirability of simple lower bounds $\mathcal{L}_{k}({\bm{h}}^{l-2})$: imposing
    two-part lower bounds in each layer would increase the number of propagated constraints
    in an exponential manner, similar to Fourier-Motzkin elimination.'
  prefs: []
  type: TYPE_NORMAL
- en: A path towards completeness
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Given an input-output bound, the reachable set can be refined by splitting
    the input space (Henriksen and Lomuscio, [2021](#bib.bib145), Rubies-Royo etÂ al.,
    [2019](#bib.bib265))â€”a strategy similar to spatial branch and bound. In other
    words, completeness is achieved by branching in the input space, rather than activation
    patterns: this strategy is especially effective when the input space is low dimensional
    (Strong etÂ al., [2022](#bib.bib296)). For example, ReluVal (Wang etÂ al., [2018b](#bib.bib327))
    propagates symbolic intervals and implements splitting procedures on the input
    domain. As the interval extension of ReLU is Lipschitz continuous, the method
    converges to arbitrary accuracy in a finite number of splits.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Generalizing the single neuron model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.4.1 Extending to other domains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, we will expect that the effective input domain $\mathcal{D}^{l-1}$
    for a given unit may be quite complex. For the first layer ($l=1$) this may derive
    from explicitly stated constraints on the inputs of the networks, while for later
    layers this will typically derive from the complex nonlinear transformations applied
    by the preceding layers. For example, in the context of surrogate models Yang
    etÂ al. ([2022](#bib.bib347)) propose bounding the input to the convex hull of
    the training data set, while other works (Schweidtmann etÂ al., [2022](#bib.bib277),
    Shi etÂ al., [2022](#bib.bib285)) propose machine learning-inspired techniques
    for learning the trust region implied by the training data. In effect, these methods
    assume a trained model is locally accurate around training data, which is a property
    similar to that which verification seeks to prove.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, most research focuses on hyperrectangular input domains, largely
    motivated by practical considerations: i) there are efficient, well-studied methods
    for computing valid (though not necessarily optimally tight) variable bounds,
    ii) characterizing the exact effective domain may be computationally impractical,
    and iii) and the hyperrectangular structure makes analysis simpler for complex
    formulations like those presented in SectionÂ [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting
    plane methods: Trading variables for inequalities â€£ 4.2 Exact models using mixed-integer
    programming â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"). We note that Jordan etÂ al. ([2019](#bib.bib167))
    use polyhedral analyses to perform verification over arbitrary (including non-polyhedral)
    norms, by fitting a $p$-norm ball in the decision region and checking adjacent
    linear regions. On the other hand, robust optimization can be employed to find
    $p$-norm adversarial regions (rather than verifying robustness), as opposed to
    a single point adversary (Maragno etÂ al., [2023](#bib.bib211)).'
  prefs: []
  type: TYPE_NORMAL
- en: Anderson etÂ al. ([2020](#bib.bib5)) present two closely related frameworks for
    constructing ideal and hereditarily sharp formulations for ReLU units with arbitrary
    polyhedral input domains. This characterization is derived from Lagrangian duality,
    and requires an infinite number of constraints (intuitively, one for each choice
    of dual multipliers). Nonetheless, separation can still be done over this infinite
    family of inequalities via a subgradient-based algorithm; this approach will be
    tractable if optimization over $\mathcal{D}^{l-1}$ is tractable. Many propagation
    algorithms are also fully compatible with arbitrary polyhedral input domains,
    as the projected problem (i.e., a linear input-output relaxation) remains an LP.
    Singh etÂ al. ([2021](#bib.bib292)) show that simplex input domains can actually
    be beneficial, creating tighter formulations by propagating constraints on the
    inputs through the network layers. Similarly, optimization-based bound tightening
    problems based on solving LPs can embed constraints defining polyhedral input
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: In certain cases, additional structural information about the input domain can
    be used to reduce this semi-infinite description to a finite one. For example,
    this can be done when $\mathcal{D}^{l-1}$ is a Cartesian product of unit simplices
    (Anderson etÂ al., [2020](#bib.bib5)) (note that this generalizes the box domain
    case, wherein each simplex is one-dimensional). This particular structure is particularly
    useful for modeling input domains with combinatorial constraints. For example,
    a network trained to predict binding propensity of a given length-$n$ DNA sequence
    is naturally modeled via an input domain that is the product of $n$ 4-dimensional
    simplicesâ€“one simplex for each letter in the sequence, each of which is selected
    from an alphabet of length 4.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Extending to other activation functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The big-$M$ formulation technique can be any piecewise linear activation function.
    While much of the literature focuses on the ReLU due to its widespread popularity,
    models for other activation functions have been explored in the literature. For
    example, multiple papers (Serra etÂ al., [2018](#bib.bib282), Appendix K) (Tjeng
    etÂ al., [2019](#bib.bib309), Appendix A.2) present a big-$M$ formulation for the
    maxout activation function. Adapting a formulation from Anderson etÂ al. ([2020](#bib.bib5))
    (Anderson etÂ al., [2020](#bib.bib5), Proposition 10), a formulation for the maxout
    unit is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\leq u_{j}({\bm{h}}^{l-1})+M^{l}_{i,j}(1-z_{j})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\geq u_{j}({\bm{h}}^{l-1})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sum_{j=1}^{k}z_{j}$ | $\displaystyle=1$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},v^{l}_{i},z)$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}\times\{0,1\}^{k},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where each $M^{l}_{i,j}$ is selected such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M^{l}_{i,j}\geq\max_{\tilde{{\bm{h}}}\in\mathcal{D}^{l-1}}u_{j}(\tilde{{\bm{h}}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We can observe that the big-$M$ formulation can also handle other discontinuous
    activation functions, such as a binary/sign activations (Han and GÃ³mez, [2021](#bib.bib136))
    or more general quantized activations (Nguyen and Huchette, [2022](#bib.bib234)).
    Nevertheless, the binary activation function naturally lends itself towards Boolean
    satisfiability, and most work therefore focuses on alternative methods such as
    SAT (Cheng etÂ al., [2018](#bib.bib57), Jia and Rinard, [2020](#bib.bib164), Narodytska
    etÂ al., [2018](#bib.bib229)).
  prefs: []
  type: TYPE_NORMAL
- en: While this survey focuses on neural networks with piecewise linear activation
    functions, we note that recent research has also studied smooth activation functions
    with a similar aim. For example, optimization over smooth activation functions
    can be handled by piecewise linear approximation and conversion to MILP (Sildir
    and Aydin, [2022](#bib.bib287)). Researchers have also studied convex/concave
    bounds for nonlinear activation functions, which can then be embedded in spatial
    branch-and-bound procedures (Schweidtmann and Mitsos, [2019](#bib.bib276), Wilhelm
    etÂ al., [2022](#bib.bib334)). In contrast to MILP formulations for ReLU neural
    networks, these problems are typically nonlinear programs that must be solved
    via spatial branch and bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Propagation methods (Singh etÂ al., [2018](#bib.bib289), Zhang etÂ al., [2018a](#bib.bib354))
    can also naturally handle general activation functions: given convex polytopic
    bounds for an activation function, these tools can propagate them through network
    layers using the same techniques. For example, Fastened CROWN (Lyu etÂ al., [2020](#bib.bib204))
    employs a set of search heuristics to quickly select linear upper and lower bounds
    on ReLU, sigmoid, and hyperbolic tangent activation functions. Tighter polyhedral
    bounds can be employed, such as piecewise linear upper and lower bounds (Benussi
    etÂ al., [2022](#bib.bib23)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Extending to adversarial training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As described in Section [1](#S1 "1 Introduction â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey"), the *training* of neural networks seeks to minimise
    a measure of distance between the output $y$ and the correct output $\hat{y}$.
    For instance, if this distance is prescribed as loss function $\mathcal{L}(y,\hat{y})$,
    this corresponds to solving the *training* optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\mathcal{L}(y,\hat{y}).$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'Further details about the training problem and solution methods are described
    in the following section. Here, we briefly outline how verification techniques
    can be embedded in training. Specifically, solutions or bounds to the verification
    problem (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Neural network verification â€£ 4.1
    Applications of optimization over trained networks â€£ 4 Optimizing Over a Trained
    Neural Network â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) provide
    a metric of how robust a trained neural network is to perturbations. These metrics
    can be embedded in the training problem to obtain a more robust network during
    training, often resulting in a bilevel training problem. For instance, the verification
    problem ([3](#S4.E3 "In Example 4 â€£ 4.1.1 Neural network verification â€£ 4.1 Applications
    of optimization over trained networks â€£ 4 Optimizing Over a Trained Neural Network
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) can be embedded as a
    lower-level problem, giving the robust optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\quad\underset{&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon}{\mathrm{max}}\quad\mathcal{L}(y=f(x),\hat{y}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Solving these problems generally involves either bilevel optimization, or computing
    an adversarial solution/bound at each training step, conceptually similar to the
    robust cutting plane approach. Madry etÂ al. ([2018](#bib.bib206)) proposed this
    formulation and solved the nonconvex inner problem using gradient descent, thereby
    losing a formal certification of robustness. These approaches may also benefit
    from reformulation strategies, such as by taking the dual of the inner problem
    and using any feasible solution as a bound (Wong and Kolter, [2018](#bib.bib335)).
    The resulting models are not only more robust, but several works have also found
    it to be empirically easier to verify robustness in them (Mirman etÂ al., [2018](#bib.bib220),
    Wong and Kolter, [2018](#bib.bib335)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, robustness can be induced by designing an additional penalty
    term for the training loss function, in a similar vein to regularization. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\kappa\mathcal{L}(y,\hat{y})+(1-\kappa)\mathcal{L}_{\mathrm{robust}}(\cdot).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Additionally, if these robustness penalties are differentiable, they can be
    embedded into standard gradient descent based optimization algorithms (Dvijotham
    etÂ al., [2018b](#bib.bib85), Mirman etÂ al., [2018](#bib.bib220)). In the above
    formulation, the parameter $\kappa$ controls the relative weighting between fitting
    the training data and satisfying some robustness criterion, and its value can
    be scheduled during training, e.g., to first focus on model accuracy (Gowal etÂ al.,
    [2018](#bib.bib129)). In these cases, over-approximation of the reachable set
    is less problematic, as it merely produces a model *more* robust than required.
    Nevertheless, BalunoviÄ‡ and Vechev ([2020](#bib.bib16)) improve relaxation tightness
    by searching for adversarial examples in the â€œlatentâ€ space between hidden layers,
    reducing the number of propagation steps. Zhang etÂ al. ([2020](#bib.bib355)) provide
    an implementation that that tightens relaxations by also propagating bounds backwards
    through the network.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Linear Programming and Polyhedral Theory in Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous sections, we have almost exclusively focused on tasks involving
    neural networks that have already been constructed, i.e., we have assumed that
    the training step has already concluded (with the exception of Section [4.4.3](#S4.SS4.SSS3
    "4.4.3 Extending to adversarial training â€£ 4.4 Generalizing the single neuron
    model â€£ 4 Optimizing Over a Trained Neural Network â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")). In this section, we focus on the training phase,
    whose goal is to construct a neural network that can represent the relationship
    between the input and output of a given set of data points.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider a set of points, or sample, $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    and assume that these points are related via a function $\hat{f}$, i.e., $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$
    $i=1,\ldots,D$. In the training phase, we look for $\hat{f}$ in a pre-defined
    class (e.g. neural networks with a specific architecture) that approximates the
    relation $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$. Typically, this
    is done by solving an Empirical Risk Minimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\hat{f}\in F}\frac{1}{D}\sum_{i=1}^{D}\ell(\hat{f}(\tilde{{\bm{x}}}_{i}),\tilde{{\bm{y}}}_{i})$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\ell$ is a loss function and $F$ is the class of functions we are restricted
    to. We usually assume the class $F$ is parametrized by $({\bm{W}},{\bm{b}})\in\Theta$
    (the network weights and biases), so we are further assuming that there exists
    a function $f(\cdot,\cdot,\cdot)$ (the network architecture) such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\forall\hat{f}\in F,\,\exists({\bm{W}},{\bm{b}})\in\Theta,\,\hat{f}({\bm{x}})=f({\bm{x}},{\bm{W}},{\bm{b}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and thus, the optimization is performed over the space of parameters. In many
    cases, $\Theta=\mathbb{R}^{N}$â€”the parameters are unrestricted real numbersâ€”but
    we will see some cases when a different parameter space can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the introduction, nowadays, most of the practically successful
    *training* algorithms for neural networks, i.e., that solve or approximate ([11](#S5.E11
    "In 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")), are based on Stochastic Gradient Descent
    (SGD). From a fundamental perspective, optimization problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is typically a *non-convex, unconstrained* problem
    that needs to be solved efficiently and where finding a *local minimum* is sufficient.
    Thus, it is not too surprising that linear programming appears to be an unsuitable
    tool in this phase, in general. Nonetheless, there are some notable and surprising
    exceptions to this, which we review here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear programming played an interesting role in training neural networks before
    SGD became the predominant training method and provided an efficient approach
    for constructing neural networks with 1 hidden layer in the 90s. This method has
    some common points in their polyhedral approach with the first known algorithm
    that can solve ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to provable
    optimality for a 1-hidden-layer ReLU neural network, which was proposed in 2018\.
    Recently, a stream of work has exploited similar polyhedral structures to obtain
    convex optimization reformulations of regularized training problems of ReLU networks.
    Linear programming tools have also been used within SGD-type methods in order
    to compute optimal *step-sizes* in the optimization of ([11](#S5.E11 "In 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) or to strictly enforce structure in $\Theta$. From a different
    perspective, a *data-independent* polytope was used to describe approximately
    all training problems that can arise from an uncertainty set. Additionally, a
    back-propagation-like algorithm for training neural networks, which solves mixed-integer
    linear problems in each layer, was proposed as an alternative to SGD. Furthermore,
    when the neural network weights are required to be discrete, the applicability
    of SGD is impaired, and mixed-integer linear models have been proposed to tackle
    the corresponding training problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we review these roles of (mixed-integer) linear programming
    and polyhedral theory within training contexts. We refer the reader to the book
    by Goodfellow etÂ al. ([2016](#bib.bib125)) and the surveys by Curtis and Scheinberg
    ([2017](#bib.bib70)), Bottou etÂ al. ([2018](#bib.bib38)), and Wright ([2018](#bib.bib337))
    for in-depth descriptions and analyses of the most commonly used training methods
    for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We remark that solving the training problem to global optimality for ReLU neural
    networks is computationally complex. Even in architectures with just one hidden
    node, the problem is NP-hard (Dey etÂ al., [2020](#bib.bib81), Goel etÂ al., [2021](#bib.bib121)).
    Also see Blum and Rivest ([1992](#bib.bib33)), Boob etÂ al. ([2022](#bib.bib36)),
    Chen etÂ al. ([2022c](#bib.bib52)), Froese etÂ al. ([2022](#bib.bib110)), Froese
    and Hertrich ([2023](#bib.bib109)) for other hardness results. Furthermore, it
    has been recently shown that training ReLU networks is $\exists\mathbb{R}$-complete
    (Abrahamsen etÂ al., [2021](#bib.bib1), Bertschinger etÂ al., [2022](#bib.bib27)),
    which implies that it is likely that the problem of optimally training ReLU neural
    networks is not even in NP. Therefore, it is not strange to see that some of the
    methods we review below, even when they are solving hard problems as sub-routines
    (like mixed-integer linear problems), either make some non-trivial assumptions
    or relax some requirements. For example, boundedness and/or integrality of the
    weights, architecture restrictions such as the output dimension, or not having
    optimality guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that, in contrast, for LTUs, exact exponential-time training
    algorithms are known for much more general architectures than in the ReLU case
    (Khalife and Basu, [2022](#bib.bib173), Ergen etÂ al., [2023](#bib.bib97)). These
    are out of scope for this survey, though we will provide a high-level overview
    of some of them, as they share some similarities to approaches designed for ReLU
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Training neural networks with a single hidden layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the well-known XOR limitation of the perceptron (Minsky and Papert,
    [1969](#bib.bib219)), a natural interest arose in the development of training
    algorithms that could handle at least one hidden layer. In this section, we review
    training algorithms that can successfully minimize the training error in a one-hidden-layer
    setting and rely on polyhedral approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Problem setting and solution scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we have a sample of size $D$ $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$
    where $\tilde{{\bm{x}}}_{i}\in\mathbb{R}^{n}$ and $\tilde{{\bm{y}}}_{i}\in\mathbb{R}$.
    In a training phase, we would like to find a neural network function $f(\cdot,\cdot,\cdot)$
    that represents in the best possible way the relation $f(\tilde{{\bm{x}}}_{i},{\bm{W}},{\bm{b}})=\tilde{{\bm{y}}}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when a neural network $\hat{f}$ has only one hidden layer, its behavior
    is almost completely determined by the sign of each component of the vector
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}^{1}x-{\bm{b}}^{1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: These are the cases of ReLU activations $\sigma(z)=\max\{0,z\}$ and LTU activations
    $\sigma(z)=\text{sgn}(z)$. The training algorithms we show here heavily exploit
    this observation and construct $({\bm{W}}^{1},{\bm{b}}^{1})$ by embedding in this
    phase a *hyperplane partition* problem based on the sample $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$.
    While the focus of this survey is mainly devoted to ReLU activations, we also
    discuss some selected cases with LTU activations as they share some similar ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 LTU activations and variable number of nodes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One stream of work dedicated to developing training algorithms for one-hidden-layer
    networks concerned the use of *backpropagation* (Rumelhart etÂ al., [1986](#bib.bib266),
    LeCun etÂ al., [1989](#bib.bib185), Werbos, [1974](#bib.bib331)). In the early
    90s, an alternative family of methods was proposed, which was heavily based on
    linear programs (see e.g. Bennett and Mangasarian ([1990](#bib.bib21), [1992](#bib.bib22)),
    Roy etÂ al. ([1993](#bib.bib264)), Mukhopadhyay etÂ al. ([1993](#bib.bib227))).
    These approaches can construct a 1-hidden-layer network without the need for an
    *a-priori* number of nodes in the network. We illustrate the high-level idea of
    these next, based on the survey by Mangasarian ([1993](#bib.bib209)).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$, thus the NN we construct will
    be a classifier. The training phase can be tackled via the construction of a *polyhedral
    partition* of $\mathbb{R}^{n}$ such that no two points (or few) $\tilde{{\bm{x}}}_{i}$
    and $\tilde{{\bm{x}}}_{j}$ such that $\tilde{{\bm{y}}}_{i}\neq\tilde{{\bm{y}}}_{j}$
    lie in the same element of the partition. To achieve this, the following approach
    presented by Bennett and Mangasarian ([1992](#bib.bib22)) can be followed. Let
    $Y=\{i\in[D]\,:\,\tilde{{\bm{y}}}_{i}=1\}$ and $N=[D]\setminus Y$, and consider
    the following optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{{\bm{w}},b,y,z}\quad$ | $\displaystyle\frac{1}{&#124;Y&#124;}\sum_{i\in
    Y}y_{i}+\frac{1}{&#124;N&#124;}\sum_{i\in N}z_{i}$ |  | (12a) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+y\geq 1$ | $\displaystyle\forall
    i\in Y$ |  | (12b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+z\geq 1$ | $\displaystyle\forall
    i\in N$ |  | (12c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle y,z\geq 0.$ |  | (12d) |'
  prefs: []
  type: TYPE_TB
- en: This LP aims at finding a hyperplane ${\bm{w}}^{\top}x=b$ separating the data
    according to their value of $\tilde{{\bm{y}}}_{i}$. Since the data may not be
    separable, the LP is minimizing the following classification error
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{1}{&#124;Y&#124;}\sum_{i\in Y}(-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+1)_{+}+\frac{1}{&#124;N&#124;}\sum_{i\in
    N}({\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+1)_{+}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of nodes
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is a linear reformulation of the latter minimization problem, where
    the auxiliary values $y,z$ take the value of each element in the sum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of
    nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is solved, we obtain 2 halfspaces classifying our data points. In
    order to obtain a richer classification and lower error, we can iterate the procedure
    by means of the Multi-Surface Method Tree (MSMT, see Bennett ([1992](#bib.bib20))),
    which solves a sequence of LPs as ([12](#S5.E12 "In 5.1.2 LTU activations and
    variable number of nodes â€£ 5.1 Training neural networks with a single hidden layer
    â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) in order to produce a polyhedral partition
    of $\mathbb{R}^{n}$. Let us illustrate how this procedure works in a simplified
    case: assume that solving ([12](#S5.E12 "In 5.1.2 LTU activations and variable
    number of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) results in a vector ${\bm{w}}_{1}$ such that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq b_{1}\}\subseteq
    Y\quad\land\quad\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq a_{1}\}\subseteq
    N,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'for some $a_{1},b_{1}\in\mathbb{R}^{n}$ with $b_{1}>a_{1}$. We can remove the
    sets $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq
    b_{1}\}$ and $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq
    a_{1}\}$ from the data-set and redefine ([12](#S5.E12 "In 5.1.2 LTU activations
    and variable number of nodes â€£ 5.1 Training neural networks with a single hidden
    layer â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) accordingly, in order to obtain a new vector
    ${\bm{w}}_{2}$ and scalars $b_{2},a_{2}$ that would be used to classify within
    the region $\{x\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1}\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure can be iterated, and the polyhedral partition of $\mathbb{R}^{n}$
    induced by the resulting hyperplanes can be easily transformed into a Neural Network
    with 1 hidden layer and LTU activations (see Bennett and Mangasarian ([1990](#bib.bib21))
    for details). We illustrate this transformation with the following example: suppose
    that after 3 iterations we have the following regions, with the arrow indicating
    to which class each region is associated to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\geq
    b_{1}\}\to Y,$ |  | (13a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\leq
    a_{1}\}\to N,$ |  | (13b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\geq
    b_{2}\}\to Y,$ |  | (13c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\leq
    a_{2}\}\to N,$ |  | (13d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x\geq(a_{3}+b_{3})/2\}\to
    Y,$ |  | (13e) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x<(a_{3}+b_{3})/2\}\to
    N.$ |  | (13f) |'
  prefs: []
  type: TYPE_TB
- en: 'Since regions ([13e](#S5.E13.5 "In 13 â€£ 5.1.2 LTU activations and variable
    number of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) and ([13f](#S5.E13.6 "In 13 â€£ 5.1.2 LTU activations
    and variable number of nodes â€£ 5.1 Training neural networks with a single hidden
    layer â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) are the last defined by the algorithm (under
    some stopping criterion), they both use $(a_{3}+b_{3})/2$ in order to obtain a
    well-defined partition of $\mathbb{R}^{n}$. In Figure [11](#S5.F11 "Figure 11
    â€£ 5.1.2 LTU activations and variable number of nodes â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey") we show a one-hidden-layer
    neural network that represents such a classifier. The structure of the neural
    network can be easily extended to handle more regions.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.F11.pic1" class="ltx_picture ltx_centering" height="226.19" overflow="visible"
    version="1.1" width="260.93"><g transform="translate(0,226.19) matrix(1 0 0 -1
    0 0) translate(12.28,0) translate(0,113.19)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -14.98)" fill="#000000"
    stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 25.435)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 20.91)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 14.685)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 8.45)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.225)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 5.96)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="11.78"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{1}$</foreignobject></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 18.14)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 26.87)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="14.77"
    height="9.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{n_{0}}$</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 94.87)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 -21.42)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 109.73 -62.25)" fill="#000000" stroke="#000000"><foreignobject
    width="22.35" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{a_{3}+b_{3}}{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 110.13 -101.62)" fill="#000000" stroke="#000000"><foreignobject
    width="21.27" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{-a_{3}-b_{3}}{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 232.76 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.52 38.54)" fill="#000000"
    stroke="#000000"><foreignobject width="14.15" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 48.27 17.71)" fill="#000000" stroke="#000000"><foreignobject width="18.77"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.93 -1.46)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 47.5 -20.6)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.38 -40.35)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.16 -60.74)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.59 31.95)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 169.9 11.96)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.82 -6.91)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.1 -26.06)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 174.88 -43.02)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.73 -62.57)" fill="#000000" stroke="#000000"><foreignobject
    width="14.61" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1$</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Illustration of Neural Network with LTU activations using MSMT.
    Inside each node of the hidden layer, we show the thresholds used in each LTU
    activation.'
  prefs: []
  type: TYPE_NORMAL
- en: For other details, we refer the reader to Bennett ([1992](#bib.bib20)), and
    for variants and extensions see Mangasarian ([1993](#bib.bib209)) and references
    therein.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features of this procedure are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each solution of ([12](#S5.E12 "In 5.1.2 LTU activations and variable number
    of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")), i.e., each new hyperplane, can be represented as a new node
    in the hidden layer of the resulting neural network.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The addition of a new hyperplane comes with a reduction in the current loss;
    this can be iterated until a target loss is met.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: â€¢
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to the universal approximation theorem (Hornik etÂ al., [1989](#bib.bib153)),
    with enough nodes in the hidden layer, we can always obtain a neural network $\hat{f}$
    with zero classification error. Although this can lead to over-fitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The work of Roy etÂ al. ([1993](#bib.bib264)) and Mukhopadhyay etÂ al. ([1993](#bib.bib227))
    follow a related idea, although the classifiers which are built are quadratic
    functions. To illustrate the approach, we use the same set-up for ([12](#S5.E12
    "In 5.1.2 LTU activations and variable number of nodes â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). The approach in Roy
    etÂ al. ([1993](#bib.bib264)) and Mukhopadhyay etÂ al. ([1993](#bib.bib227)) aims
    at finding a function'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(x)={\bm{x}}^{\top}{\bm{V}}{\bm{x}}+{\bm{w}}^{\top}{\bm{x}}+b$
    |  |'
  prefs: []
  type: TYPE_TB
- en: such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(\tilde{{\bm{x}}}_{i})\geq 0\Longleftrightarrow
    i\in Y.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since this may not be possible, the authors propose solving the following LP
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{W,w,b,\epsilon}\quad$ | $\displaystyle\epsilon$ |  |
    (14a) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\geq\epsilon$
    | $\displaystyle\forall i\in Y$ |  | (14b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\leq-\epsilon$
    | $\displaystyle\forall i\not\in Y$ |  | (14c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\epsilon\geq\epsilon_{0}$ |  | (14d) |'
  prefs: []
  type: TYPE_TB
- en: 'for some fixed tolerance $\epsilon_{0}>0$. When this LP is infeasible, the
    class $Y$ is partitioned into $Y_{1}$ and $Y_{2}$, and an LP as ([14](#S5.E14
    "In 5.1.2 LTU activations and variable number of nodes â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) is solved for both $Y_{1}$
    and $Y_{2}$. The algorithm then follows iteratively (see below for comments on
    these iterations). In the end, the algorithm will construct $k$ quadratic functions
    $f_{1},\ldots,f_{k}$, which the authors call â€œmasking functionsâ€, that will classify
    an input ${\bm{x}}$ in the class $Y$ if and only if'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\exists i\in[k],\,f_{i}({\bm{x}})\geq 0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In order to represent the resulting classifier as a single-layer neural network,
    the authors proceed in a similar manner to a linear classifier; the input layer
    of the resulting neural network not only includes each entry of ${\bm{x}}$, but
    also the bilinear terms ${\bm{x}}{\bm{x}}^{\top}$. Using this input, the classifier
    built by ([14](#S5.E14 "In 5.1.2 LTU activations and variable number of nodes
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be thought of as a linear classifier (much like a polynomial regression
    can be cast as a linear regression).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a last comment on the work of Roy etÂ al. ([1993](#bib.bib264)) and Mukhopadhyay
    etÂ al. ([1993](#bib.bib227)), the authorsâ€™ algorithm does not iterate in a straightforward
    fashion. They add clustering iterations alternating with the steps described above
    in order to (a) identify outliers and remove them from the training set, and (b)
    subdivide the training data when ([14](#S5.E14 "In 5.1.2 LTU activations and variable
    number of nodes â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is infeasible. These additions allow them to obtain
    a polynomial-time algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The methods described in this section are able to produce a neural network with
    arbitrary quality, however, there is no guarantee on the size of the resulting
    neural network. When the size of the network is fixed the story changes, which
    is the case we describe next.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Fixed number of nodes and ReLU activations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this section, training a neural network is
    a complex optimization problem in general, with some results indicating that the
    problem is likely to not even be in NP (Abrahamsen etÂ al., [2021](#bib.bib1),
    Bertschinger etÂ al., [2022](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, by restricting the network architecture sufficiently and allowing
    exponential running times, exact algorithms can be conceived. An important step
    in the construction of such algorithms was taken by Arora etÂ al. ([2018](#bib.bib8)).
    In this work, the authors studied the training problem in detail, providing the
    first optimization algorithm capable of solving the training problem to provable
    optimality for a fixed network architecture with one hidden layer and with an
    output dimension of 1\. As we anticipated, this algorithm shares some similarities
    with the previous approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider now a ReLU activation. Also, we no longer assume $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$,
    but we keep the output dimension as 1\. The problem considered by Arora etÂ al.
    ([2018](#bib.bib8)) reads
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}},{\bm{b}}}\frac{1}{D}\sum_{i=1}^{D}\ell({\bm{W}}^{2}(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i}),$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: with $\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$ a convex loss. Note that
    this problem, even if $\ell$ is convex, is a non-convex optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1 (Arora etÂ al. ([2018](#bib.bib8)))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $n_{1}$ be the number of nodes in the hidden layer. There exists an algorithm
    to find a global optimum of ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and
    ReLU activations â€£ 5.1 Training neural networks with a single hidden layer â€£ 5
    Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) in time $O(2^{n_{1}}D^{{n_{0}}\cdot n_{1}}\text{poly}(D,{n_{0}},n_{1}))$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Roughly speaking, the algorithm works by noting that one can assume the weights
    in ${\bm{W}}^{2}$ are in $\{-1,1\}$, since $\sigma$ is positively-homogeneous.
    Thus, problem ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and ReLU activations
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be restated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}}^{1},{\bm{b}}^{1},s}\frac{1}{D}\sum_{i=1}^{D}\ell(s(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i})$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'where $s\in\{-1,1\}^{n_{1}}$. In order to handle the non-linearity, Arora etÂ al.
    ([2018](#bib.bib8)) â€œguessâ€ the values of $s$ and the sign of each component of
    ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Enforcing a sign for each component
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$ is similar to the approach
    discussed in the previous section: it fixes how the input part of the data $(\tilde{{\bm{x}}}_{i})_{i=1}^{D}$
    is partitioned in polyhedral regions by a number of hyperplanes. The difference
    is that, in this case, the number of hyperplanes to be used is assumed to be fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the hyperplane arrangement theorem (see e.g. (Matousek, [2002](#bib.bib215),
    Proposition 6.1.1)), there are at most $D^{{n_{0}}n_{1}}$ ways of fixing the signs
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Additionally, there are at
    most $2^{n_{1}}$ possible vectors in $\{-1,1\}^{n_{1}}$. Once these components
    are fixed, ([16](#S5.E16 "In 5.1.3 Fixed number of nodes and ReLU activations
    â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be solved as an optimization problem with a convex objective function
    and a polyhedral feasible region imposing the desired signs in ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$.
    This results in the $O(2^{n_{1}}D^{{n_{0}}n_{1}}\text{poly}(D,{n_{0}},n_{1}))$
    running time. This algorithm was recently generalized to concave loss functions
    by Froese etÂ al. ([2022](#bib.bib110)).'
  prefs: []
  type: TYPE_NORMAL
- en: Dey etÂ al. ([2020](#bib.bib81)) developed a polynomial-time approximation algorithm
    in this setting for the case of $n_{1}=1$ (i.e., one ReLU neuron) and square loss.
    This approximation algorithm has a better performance when the input dimension
    is much larger than the sample size, i.e. ${n_{0}}\gg D$. The approach by Dey
    etÂ al. ([2020](#bib.bib81)) also relies on fixing the signs of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$,
    and then solving multiple convex optimization problems, but in different strategy
    than that of Arora etÂ al. ([2018](#bib.bib8)); in particular, Dey etÂ al. ([2020](#bib.bib81))
    only explore a polynomial number of the possible â€œfixingsâ€, which yields the approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We note that the result by Arora etÂ al. ([2018](#bib.bib8)) shows that the
    training problem on their architecture is in NP. This is in contrast to Bertschinger
    etÂ al. ([2022](#bib.bib27)), who show that training a neural network with one
    hidden layer is likely to not be in NP. The big difference lies in the assumption
    on the output dimension: in the case of Bertschinger etÂ al. ([2022](#bib.bib27)),
    the output dimension is two. It is quite remarkable that such a sharp complexity
    gap is produced by a small change in the output dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 An exact training algorithm for arbitrary LTU architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, Khalife and Basu ([2022](#bib.bib173)) presented a new algorithm,
    akin to that in Arora etÂ al. ([2018](#bib.bib8)), capable of solving the training
    problem to global optimality for any fixed LTU architecture with a convex loss
    function $\ell$. In this case, no assumption on the networkâ€™s depth is made. The
    algorithm runs in polynomial time on the sample size $D$ when the architecture
    is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: We will not describe this approach in detail, as it heavily relies on the structure
    given by LTU activations, which is intricate and beyond the scope of this survey.
    Although we note that it shares some high-level similarities to the algorithm
    of Arora etÂ al. ([2018](#bib.bib8)) for ReLU activations, such as â€œguessingâ€ the
    behavior of the neuronsâ€™ activity and then solving multiple convex optimization
    problems. However, the structural and algorithmic details are considerably different.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this result reveals the big gap between what is
    known for LTU versus ReLU activations in terms of their training problems. In
    the case of the former, there is an exact algorithm for arbitrary architectures;
    in the case of the latter, the known results are much more restricted and strong
    computational limitations exist.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Convex reformulations in regularized training problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the case when the training problem is regularized, the following stream
    of work has developed several convex reformulations of it. Pilanci and Ergen ([2020](#bib.bib248))
    presented the first convex reformulation of a training problem for the case with
    one hidden layer and one-dimensional outputs. As the approach described in Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training
    neural networks with a single hidden layer â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey"), this
    reformulation uses hyperplane arrangements according to the activation patterns
    of the ReLU units, but instead of using them algorithmically directly, they use
    them to find their convex reformulations. This framework was further extended
    to CNNs by Ergen and Pilanci ([2021c](#bib.bib93)). Higher-dimensional outputs
    in neural networks with one hidden layer were considered in Ergen and Pilanci
    ([2020](#bib.bib90), [2021a](#bib.bib91)), Sahiner etÂ al. ([2021](#bib.bib268)).
    This convex optimization perspective was also applied in Batch Normalization by
    Ergen etÂ al. ([2022](#bib.bib96)).'
  prefs: []
  type: TYPE_NORMAL
- en: These approaches provide polynomial-time algorithms when some parameters (e.g.,
    the input dimension ${n_{0}}$) are considered constant. We note that this does
    not contradict the hardness result of Froese and Hertrich ([2023](#bib.bib109)),
    as the latter does not include a regularizing term. We explain below where the
    regularizing term plays an important role. Training via convex optimization was
    further developed to handle deeper regularized neural networks in Ergen and Pilanci
    ([2021b](#bib.bib92), [d](#bib.bib94), [e](#bib.bib95)).
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we review the convex reformulation in Pilanci and Ergen ([2020](#bib.bib248))
    (one hidden layer and one-dimensional output) to illustrate some of the base strategies
    behind these approaches. We refer the reader to the previously mentioned articles
    for the most recent and intricate developments, as well as numerical experiments.
  prefs: []
  type: TYPE_NORMAL
- en: As before, let $n_{1}$ be the number of nodes in the hidden layer. Let us consider
    the following regularized training problem; to simplify the discussion, we omit
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{\beta}{2}\sum_{j=1}^{n_{1}}(\&#124;{\bm{W}}^{1}_{j}\&#124;^{2}+({\bm{W}}^{2}_{j})^{2})$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\beta>0$, $\tilde{{\bm{X}}}$ is a matrix whose $i$-th row is $\tilde{{\bm{x}}}_{i}$
    and ${\bm{W}}^{1}_{j}$ is the vector of weights going into neuron $j$. Thus, $\tilde{{\bm{X}}}{\bm{W}}^{1}_{j}$
    is a vector whose $i$-th component is the input to neuron $j$ when evaluating
    the network on $\tilde{{\bm{x}}}_{i}$. ${\bm{W}}^{2}_{j}$ is a scalar: it is the
    weight on the arc from neuron $j$ to the output neuron (one-dimensional). Note
    that there is a slight notation overload: $({\bm{W}}^{2}_{j})^{2}$ is the square
    of the scalar ${\bm{W}}^{2}_{j}$. However, we will quickly remove this (pontentially
    confusing) term.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem ([17](#S5.E17 "In 5.2 Convex reformulations in regularized training
    problems â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is a regularized version of ([15](#S5.E15
    "In 5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) when $\ell$ is the squared
    difference. We modified its presentation to match the structure in Pilanci and
    Ergen ([2020](#bib.bib248)). The authors first prove that ([17](#S5.E17 "In 5.2
    Convex reformulations in regularized training problems â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is equivalent to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\&#124;{\bm{W}}^{1}_{j}\&#124;\leq 1}\min_{{\bm{W}}^{2}_{j}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{n_{1}}&#124;{\bm{W}}^{2}_{j}&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then, through a series of reformulations and duality arguments, the authors
    first show that this problem is lower bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\max$ | $\displaystyle\quad-\frac{1}{2}\left\&#124;v-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{1}{2}\&#124;\tilde{{\bm{y}}}\&#124;^{2}$
    |  | (18a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t | $\displaystyle\quad&#124;v^{\top}\sigma(\tilde{{\bm{X}}}u)&#124;\leq\beta$
    | $\displaystyle\forall u,\,\&#124;u\&#124;\leq 1$ |  | (18b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad v\in\mathbb{R}^{D}$ |  | (18c) |'
  prefs: []
  type: TYPE_TB
- en: 'Problem ([18](#S5.E18 "In 5.2 Convex reformulations in regularized training
    problems â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) has $D$ variables and infinitely
    many constraints. The authors show that this lower bound is tight when the number
    of neurons in the hidden layer is large enough; specifically, they require $n_{1}\geq
    m^{*}$, where $m^{*}\in\{1,\ldots,D\}$ is defined as the number of Dirac deltas
    in an optimal solution of a dual of ([18](#S5.E18 "In 5.2 Convex reformulations
    in regularized training problems â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) (see Pilanci
    and Ergen ([2020](#bib.bib248)) for details).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the presence of infinitely many constraints, the authors address
    this by considering all possible patterns of signs of $\tilde{{\bm{X}}}u$ (similarly
    to Arora etÂ al. ([2018](#bib.bib8)), as discussed in Section [5.1.3](#S5.SS1.SSS3
    "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training neural networks
    with a single hidden layer â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). For each fixed sign
    pattern (hyperplane arrangement), they apply a duality argument which allows them
    to recast the constraint $\max_{u\in\mathcal{B}}|v^{\top}\sigma(\tilde{{\bm{X}}}u)|\leq\beta$
    as a finite collection of second-order cone constraints with $\beta$ on the right-hand
    side.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, using that $\beta>0$, they show that the reformulated problem satisfies
    Slaterâ€™s condition, and thus from strong duality they obtain the following convex
    optimization problem, which has the same objective value as ([18](#S5.E18 "In
    5.2 Convex reformulations in regularized training problems â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min$ | $\displaystyle\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{P}M_{i}\tilde{{\bm{X}}}(v_{i}-w_{i})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{P}(\&#124;v_{i}\&#124;+\&#124;w_{i}\&#124;)$
    |  | (19a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}v_{i}\geq 0$ |
    $\displaystyle\forall i\in[P]$ |  | (19b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}w_{i}\geq 0$ | $\displaystyle\forall
    i\in[P]$ |  | (19c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad v_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19d) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad w_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19e) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $I_{D}$ is the $D\times D$ identity matrix, $P$ is the number of possible
    activation patterns for $\tilde{{\bm{X}}}$, and each $M_{i}$ is a $D\times D$
    binary diagonal matrix whose diagonal indicates the $i$-th possible sign pattern
    of $\tilde{{\bm{X}}}u$. This means that $(M_{i})_{j,j}$ is 1 if and only if $\tilde{{\bm{x}}}_{j}^{\top}u\geq
    0$ in the $i$-th sign pattern of $\tilde{{\bm{X}}}u$. Moreover, the authors provide
    a formula to recover a solution of ([17](#S5.E17 "In 5.2 Convex reformulations
    in regularized training problems â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) from a solution
    of ([19](#S5.E19 "In 5.2 Convex reformulations in regularized training problems
    â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using that $P\leq 2r(e(D-1)/r)^{r}$, where $r=\mbox{rank}(\tilde{{\bm{X}}})$,
    the authors note that the formulation ([19](#S5.E19 "In 5.2 Convex reformulations
    in regularized training problems â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) yields a
    training algorithm with complexity $O({n_{0}}^{3}r^{3}(D/r)^{3r})$. Note that
    if one fixes $r$, the resulting algorithm runs polynomial time. In particular,
    fixing ${n_{0}}$ fixes the rank of $\tilde{{\bm{X}}}$ and results in a polynomial
    time algorithm as well. In contrast, the algorithm by Arora etÂ al. ([2018](#bib.bib8))
    discussed in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU
    activations â€£ 5.1 Training neural networks with a single hidden layer â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey") remains exponential even after fixing ${n_{0}}$. Moreover,
    Froese and Hertrich ([2023](#bib.bib109)) showed that the training problem is
    NP-Hard even for fixed ${n_{0}}$. This apparent contradiction is explained by
    two key components of the convex reformulation: the regularization term and the
    presence of a â€œlarge enoughâ€ number of hidden neurons. This facilitates the exponential
    improvement of the training algorithm with respect to Arora etÂ al. ([2018](#bib.bib8)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Frank-Wolfe in DNN training algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another stream of work that has included components of linear programming in
    DNN training involves the Frank-Wolfe method. We briefly describe this method
    in the non-stochastic version next. In this section, we omit the biases ${\bm{b}}$
    to simplify the notation.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent (and its variants) is designed for problems of the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}}\in\mathbb{R}^{N}}\mathcal{L}({\bm{W}})$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: and it is based on iterations of the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))$ |  |
    (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{i}$ is known as the *learning rate*. In the stochastic versions,
    $\nabla\mathcal{L}({\bm{W}}(i))$ is replaced by a stochastic gradient. In this
    setting, these algorithms would find a local minimum, which is global when $\mathcal{L}$
    is convex.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the presence of constraints ${\bm{W}}\in\Theta$, however, this strategy
    may not work directly. A regularizing term is typically used in the objective
    function instead of a constraint, that â€œencouragesâ€ ${\bm{W}}\in\Theta$ but does
    not enforce it. If we strictly require that ${\bm{W}}\in\Theta\neq\mathbb{R}^{n}$,
    and $\Theta$ is a convex set, one could modify ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)=\text{proj}_{\Theta}\left({\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))\right).$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: and thus ensure that all iterates ${\bm{W}}(i)\in\Theta$. Unfortunately, a projection
    is a costly routine. An alternative to this projection is the Frank-Wolfe method
    (Frank etÂ al., [1956](#bib.bib108)). Here, a direction ${\bm{d}}_{i}$ is computed
    via the following linear-objective convex optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{d}}_{i}\in\arg\min_{{\bm{d}}\in\Theta}{\bm{v}}_{i}^{\top}{\bm{d}}$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: where normally ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ (we consider variants
    below). The update is then computed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)+\alpha_{i}({\bm{d}}_{i}-{\bm{W}}(i)),$ |  |
    (24) |'
  prefs: []
  type: TYPE_TB
- en: 'for $\alpha_{i}\in[0,1]$. Note that, by convexity, we are assured that ${\bm{W}}(i+1)\in\Theta$
    as long as ${\bm{W}}(0)\in\Theta$. In many applications, $\Theta$ is polyhedral,
    which makes ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) a linear program. Moreover, for simple sets $\Theta$, problem
    ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) admits closed-form solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of deep neural network training, two notable applications of
    Frank-Wolfe have appeared. Firstly, the Deep Frank Wolfe algorithm, by Berrada
    etÂ al. ([2018](#bib.bib26)), which modifies iteration ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) with an optimization
    problem that can be solved using Frank-Wolfe in its dual. Secondly, the use of
    a stochastic version of Frank-Wolfe in the training problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets
    Polyhedral Theory: A Survey")) by Pokutta etÂ al. ([2020](#bib.bib249)) and Xie
    etÂ al. ([2020a](#bib.bib342)), which enforces structure in the neural network
    weights directly. We review these next, starting with the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Stochastic Frank-Wolfe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Note that problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) is of the
    form ([20](#S5.E20 "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) with'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}({\bm{W}})=\frac{1}{D}\sum_{i=1}^{D}\ell(f(\tilde{{\bm{x}}}_{i},{\bm{W}}),\tilde{{\bm{y}}}_{i}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We remind the reader that we are omitting the biases in this section to simplify
    notation, as they can be incorporated as part of ${\bm{W}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, some structure of the weights is commonly desired, (e.g. sparsity or
    boundedness), which traditionally have been incorporated as regularizing terms
    in the objective, as mentioned above. The recent work by Xie etÂ al. ([2020a](#bib.bib342))
    and Pokutta etÂ al. ([2020](#bib.bib249)), on the other hand, enforce structure
    on $\Theta$ directly using Frank-Wolfe â€”more precisely, stochastic versions of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Xie etÂ al. ([2020a](#bib.bib342)) use a stochastic Frank-Wolfe approach to impose
    an $\ell_{1}$-norm constraint on the weights and biases ${\bm{W}}$ when training
    a neural network with 1 hidden layer. Note that $\ell_{1}$ constraints are polyhedral.
    Their algorithm is designed for a general Online Convex Optimization setting,
    where â€œlossesâ€ are revealed in each iteration. However, in their computational
    experiments, they included tests in an offline setting given by a DNN training
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach follows the Frank-Wolfe method described above closely. The key
    difference lies in the estimation of the stochastic gradient they use, which is
    not standard and it is one of the most important aspects of the algorithm. Instead
    of using ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ in ([23](#S5.E23 "In 5.3
    Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), the following
    *stochastic recursive estimator* of the gradient is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\bm{v}}_{0}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(0))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{v}}_{i}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(i))+(1-\rho_{i})(v_{i-1}-\tilde{\nabla}\mathcal{L}({\bm{W}}(i-1)))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{\nabla}\mathcal{L}$ is a stochastic gradient, and $\rho_{i}$ is
    a parameter. The authors show that the gradient approximation error of this estimator
    converges to 0 at a sublinear rate, with high probability. This is important for
    them to analyze the â€œregret boundsâ€ they provide for the online setting.
  prefs: []
  type: TYPE_NORMAL
- en: The experimental results in Xie etÂ al. ([2020a](#bib.bib342)) in DNN training
    are very positive. They test their approach in the MNIST and CIFAR10 datasets
    and outperform existing state-of-the-art approaches in terms of suboptimality,
    training accuracy, and test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Pokutta etÂ al. ([2020](#bib.bib249)) implement and test several variants of
    stochastic versions of Frank-Wolfe in the training of neural networks, including
    the approach by Xie etÂ al. ([2020a](#bib.bib342)). Pokutta etÂ al. ([2020](#bib.bib249))
    focus their experiments on their main proposed variant, which they refer to simply
    as Stochastic Frank-Wolfe (SFW). This variant uses
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{v}}_{i}=(1-\rho_{i}){\bm{v}}_{i-1}+\rho_{i}\tilde{\nabla}\mathcal{L}({\bm{W}}(i)),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\rho_{i}$ is a momentum parameter. The authors propose many different
    options for $\Theta$ including $\ell_{1},\ell_{2}$ and $\ell_{\infty}$ balls,
    and $K$-sparse polytopes. Of these, only the $\ell_{2}$ ball is non-polyhedral.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the computational experiments are promising for SFW. The authors advocate
    for this algorithm arguing that it provides excellent computational performances
    while being simple to implement and competitive with other state-of-the-art algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Deep Frank-Wolfe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another application of Frank-Wolfe within DNN training was proposed by Berrada
    etÂ al. ([2018](#bib.bib26)). While this approach does not make heavy use of linear
    programming techniques, the application of Frank-Wolfe is quite novel, and they
    do rely on one linear program needed when performing an update as ([24](#S5.E24
    "In 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors note that ([21](#S5.E21 "In 5.3 Frank-Wolfe in DNN training algorithms
    â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey")) can also be written as the solution to the
    following *proximal* problem (Bubeck etÂ al., [2015](#bib.bib40)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\mathcal{T}_{{\bm{W}}(i)}(\mathcal{L}({\bm{W}}))\right\}$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{T}_{{\bm{W}}(i)}$ represents the first-order Taylor expansion
    at ${\bm{W}}(i)$. We are omitting regularizing terms since they do not play a
    fundamental role in the approach; all this discussion can be directly extended
    to include regularizers. Berrada etÂ al. ([2018](#bib.bib26)) note that ([25](#S5.E25
    "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) linearizes the loss function, and propose the following *loss-preserving
    proximal* problem to replace ([25](#S5.E25 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe
    in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\frac{1}{D}\sum_{i=1}^{D}\ell(\mathcal{T}_{{\bm{W}}(i)}(f(\tilde{{\bm{x}}}_{i},{\bm{W}})),\tilde{{\bm{y}}}_{i})\right\}$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'Using the results by Lacoste-Julien etÂ al. ([2013](#bib.bib182)), the authors
    argue that ([26](#S5.E26 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training
    algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is amenable to Frank-Wolfe in the
    dual when $\ell$ is piecewise linear and convex (e.g. the hinge loss). To be more
    specific, the authors show that in this case, and assuming $\alpha_{i}=\alpha$,
    there exists ${\bm{A}},{\bm{b}}$ such that the dual of ([26](#S5.E26 "In 5.3.2
    Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is simply'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\max_{\mathbf{\beta}}\quad$ | $\displaystyle\frac{-1}{2\alpha}\&#124;{\bm{A}}\mathbf{\beta}\&#124;^{2}+{\bm{b}}^{\top}\mathbf{\beta}$
    |  | (27a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle\mathbf{1}^{\top}\mathbf{\beta}=1$ |  | (27b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbf{\beta}\geq 0$ |  | (27c) |'
  prefs: []
  type: TYPE_TB
- en: 'The authors consider applying Frank-Wolfe to this last problem, and to recover
    the primal solution using the primal-dual relation ${\bm{W}}=-{\bm{A}}\mathbf{\beta}$,
    which is a consequence of KKT. The Frank-Wolfe iteration ([24](#S5.E24 "In 5.3
    Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) in the notation
    of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training
    algorithms â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")) would look like'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\beta}_{i+1}=\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i}).$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, ${\bm{d}}_{i}$ is feasible for ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe
    â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) and
    obtained using a linear programming oracle, and $\gamma_{i}$ the Frank-Wolfe step-length.
    Note that the feasible region of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe â€£ 5.3
    Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) is a simplex:
    exploiting this, the authors show that an optimal $\gamma_{i}$ can be computed
    in closed-form: here, â€œoptimalâ€ refers to a minimizer of ([27](#S5.E27 "In 5.3.2
    Deep Frank-Wolfe â€£ 5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")) when restricted to points of the form $\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With all these considerations, the bottleneck in this application of Frank-Wolfe
    is obtaining ${\bm{d}}_{i}$; recall that this Frank-Wolfe routine is embedded
    within a single iteration of the overall training algorithm; therefore, in each
    iteration of the training algorithm, possibly multiple computations of ${\bm{d}}_{i}$
    would be required in order to solve ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe â€£
    5.3 Frank-Wolfe in DNN training algorithms â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) to
    optimality. To alleviate this, the authors propose to only perform one iteration
    of Frank-Wolfe: they set ${\bm{d}}_{0}$ to be the stochastic gradient and compute
    a closed-form expression for $\mathbf{\beta}_{1}$. This is the basic ingredient
    of the Deep Frank Wolfe (DFW). It is worth noting that this algorithm is not guaranteed
    to converge, however, its empirical performance is competitive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other two important considerations are taken into account the implementation
    of this algorithm: smoothing of the loss function (as the Hinge loss is piecewise
    linear) and the adaptation of Nesterovâ€™s Momentum to this new setting. We refer
    the reader to the corresponding article for these details. One of the key features
    of DFW is that it only requires one hyperparameter ($\alpha$) to be tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors test DFW in image classification and natural language inference.
    Overall, the results obtained by DFW are very positive: in most cases, it can
    outperform adaptive gradient methods, and it is competitive with SGD while converging
    faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Polyhedral encoding of multiple training problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the questions raised by Arora etÂ al. ([2018](#bib.bib8)) (see Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations â€£ 5.1 Training
    neural networks with a single hidden layer â€£ 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) was
    whether the dependency on $D$ of their algorithm could be improved since it is
    typically the largest coefficient in a training problem. This question was studied
    by Bienstock etÂ al. ([2023](#bib.bib32)), who show that, in an approximation setting,
    a more ambitious goal is achievable: there is a polyhedral encoding of multiple
    training problems whose size has a mild dependency on $D$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous section, we omit the biases ${\bm{b}}$ to simplify notation,
    as all parameters can be included in ${\bm{W}}$. Let us assume the class of neural
    networks $F$ in ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) are restricted
    to have bounded parameters (we assume they lie in the interval $[-1,1]$), and
    let us assume the sample has been normalized in such a way that $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$.
    Furthermore, let $N$ be the dimension of $\Theta$ (the number of parameters in
    the neural network). With this notation, we define the following.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$ â€” sample size, parameter space, loss function, network
    architecture, respectively. For a function $g$, let $\mathcal{K}_{\infty}(g)$
    be the Lipschitz constant of $g$ using the infinity norm. We define the *Architecture
    Lipschitz Constant* $\mathcal{K}(D,\Theta,\ell,f)$ as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{K}(D,\Theta,\ell,f)\doteq\mathcal{K}_{\infty}(\ell(f(\cdot,\cdot),\cdot))$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: over the domain $[-1,1]^{n_{0}}\times\Theta\times[-1,1]^{n_{L+1}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Using this definition, and the boundedness of parameters, a straightforward
    approximate training algorithm can be devised whose running time is linear in
    $D$. Simply do a grid search in the parametersâ€™ space, and evaluate all data points
    in each possible parameter. It is not hard to see that, to achieve $\epsilon$-optimality,
    such an algorithm would run in time which is linear in $D$ and exponential in
    $\mathcal{K}(D,\Theta,\ell,f)/\epsilon$. What was proved by Bienstock etÂ al. ([2023](#bib.bib32))
    is that one can take a step further and represent multiple training problems at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2 (Bienstock etÂ al. ([2023](#bib.bib32)))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$, and let $\mathcal{K}:=\mathcal{K}(D,\Theta,\ell,f)$
    be the corresponding network architecture. Consider $\epsilon>0$ arbitrary. There
    exists a polytope $P_{\epsilon}$ of sizeâµâµ5Here, the size of the polytope is the
    number of variables and constraints describing it.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $O(D\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $P_{\epsilon}$ can be constructed in time $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N}D)$
    plus the time required for $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$
    evaluations of the loss function $\ell$ and $f$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *any* sample $(\tilde{X},\tilde{Y})=(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$, there
    is a face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ of $P_{\epsilon}$ such that optimizing
    a linear function over $\mathcal{F}_{\tilde{X},\tilde{Y}}$ yields an $\epsilon$-approximation
    to the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ arises by simply substituting-in
    actual data for the data-variables $x,y$, which is used to fixed variables in
    the description of $P_{\epsilon}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This result is very abstract in nature but possesses some interesting features.
    Firstly, it encodes (approximately) *every* possible training problem arising
    from data in $[-1,1]^{{n_{0}}+{n_{L+1}}}$ using a benign dependency on $D$: the
    polytope size depends only linearly on $D$, while a discretized enumeration of
    all the possible samples of size $D$ would be exponential in $D$. Secondly, every
    possible ERM problem appears in a *face* of the polytope; this suggests a strong
    geometric structure across different ERM problems. And lastly, this result is
    applicable to a wide variety of network architectures; in order to obtain an architecture-specific
    result, it suffices to compute the corresponding value of $\mathcal{K}$ and plug
    it in. Regarding this last point, the authors computed the constant $\mathcal{K}$
    for various well-known architectures and obtained the results of Table [4](#S5.T4
    "Table 4 â€£ 5.4 Polyhedral encoding of multiple training problems â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of polyhedral encoding sizes for various architectures. DNN
    refers to a fully-connected Deep Neural Network, CNN to a Convolutional Neural
    Network, and ResNet to a Residual Network. $G$ is the graph defining the Network,
    $\Delta$ is the maximum in-degree in $G$, $L$ is the number of hidden layers,
    and ${n_{\max}}$ is the maximum width of a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Type Loss Size of polytope Notes DNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ DNN Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}}){n_{\max}}^{O(k^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ CNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\ll|E({G})|$ ResNet Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    ResNet Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}})\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof of this result relies on a graph theoretical concept called *treewidth*.
    This parameter is used for measuring structured sparsity, and in Bienstock and
    MuÃ±oz ([2018](#bib.bib31)) it was proved that any optimization problem admits
    an approximate polyhedral reformulation whose size is exponential only in the
    treewidth parameter. On a high level, the neural network result is obtained by
    noting that ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) connects different sample
    points only through a sum; therefore, the following reformulation of the optimization
    problem can be considered, which decouples the different data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{{\bm{W}}\in\Theta,{\bm{L}}}\left\{\frac{1}{D}\sum_{d=1}^{D}{\bm{L}}_{d}\,\middle&#124;\,{\bm{L}}_{d}\,=\,\ell(f(\tilde{{\bm{x}}}_{d},{\bm{W}}),\tilde{{\bm{y}}}_{d})\quad\forall\,d\in[D]\right\}$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: This reformulation does not seem useful at first, however, it has a *treewidth*
    that does not depend on $D$, even if the data points are considered variables.
    From this point, the authors are able to obtain the polytope whose size does not
    depend exponentially on $D$, and which is capable of encoding all possible ERM
    problems. The face structure the polytope has is more involved, and we refer the
    reader to Bienstock etÂ al. ([2023](#bib.bib32)) for these details.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that the polytope size provided by Bienstock etÂ al. ([2023](#bib.bib32))
    in the setting of Arora etÂ al. ([2018](#bib.bib8)) is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $O((2\mathcal{K}_{\infty}(\ell)n_{1}^{O(1)}/\epsilon)^{({n_{0}}+1)(n_{1})}D)$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{K_{\infty}}(\ell)$ is the Lipschitz constant of the loss function
    with respect to the infinity norm over a specific domain. These two results are
    not completely comparable, but they give a good idea of how good the size of polytope
    constructed in Bienstock etÂ al. ([2023](#bib.bib32)) is. The dependency on $D$
    is better in the polytope size, the polytope encodes multiple training problems,
    and the result is more general (it applies to almost any architecture); however,
    the polytope only gives an approximation, and its construction requires boundedness.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Backpropagation through MILP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the work by Goebbels ([2021](#bib.bib120)), a novel use of Mixed-Integer
    Linear Programming is proposed in training ReLU networks: to serve as an alternative
    to SGD. This new algorithm works as backpropagation, as it updates the weights
    of the neural network iteratively starting from the last layer. The key difference
    is that each update in a layer amounts to solving a MILP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us focus only on one hidden layer at a time (of width $n$), so we can assume
    we have an architecture as in Figure [11](#S5.F11 "Figure 11 â€£ 5.1.2 LTU activations
    and variable number of nodes â€£ 5.1 Training neural networks with a single hidden
    layer â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep Learning
    Meets Polyhedral Theory: A Survey"). Furthermore, we assume we have some target
    output vectors $\{{\bm{T}}_{d}\}_{d=1}^{D}$ (when processing the last hidden layer
    in the backpropagation, this corresponds to $\{\tilde{{\bm{y}}}_{d}\}_{d=1}^{D}$)
    and some layer input $\{{\bm{I}}_{d}\}_{d=1}^{D}$ (when processing the last hidden
    layer, this corresponds to evaluating the neural network on $\{\tilde{{\bm{x}}}_{d}\}_{d=1}^{D}$
    up to the second-to-last hidden layer). The algorithm proposed by Goebbels ([2021](#bib.bib120))
    solves the following optimization problem to update the weights ${\bm{W}}$ and
    biases ${\bm{b}}$ of the given layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{{\bm{W}},\hat{{\bm{h}}},{\bm{b}},{\bm{h}},{\bm{z}}}\quad$
    | $\displaystyle\sum_{d=1}^{D}\sum_{j=1}^{n}&#124;{\bm{T}}_{d,j}-{\bm{h}}_{d,j}&#124;$
    |  | (32a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle\hat{{\bm{h}}}_{d,j}=({\bm{W}}{\bm{I}}_{d})_{j}+{\bm{b}}_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\geq-M(1-{\bm{z}}_{d,j})$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32d) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle&#124;\hat{{\bm{h}}}_{d,j}-{\bm{h}}_{d,j}&#124;\leq M(1-{\bm{z}}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32e) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32f) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\geq 0$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$
    |  | (32g) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{z}}_{d,j}\in\{0,1\}$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$
    |  | (32h) |'
  prefs: []
  type: TYPE_TB
- en: 'Here $M$ is a large constant that is assumed to bound the input to any neuron.
    Note that problem ([32](#S5.E32 "In 5.5 Backpropagation through MILP â€£ 5 Linear
    Programming and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral
    Theory: A Survey")) can easily be linearized. This optimization problem finds
    the weights (${\bm{W}}$) and biases (${\bm{b}}$) that minimize the difference
    between the â€œrealâ€ output of the network for each sample (${\bm{h}}_{d}$) and
    the target output (${\bm{T}}_{d}$). The auxiliary variables $\hat{{\bm{h}}}_{d,j}$
    represent the input to the each neuron â€”so ${\bm{h}}_{d,j}=\sigma(\hat{{\bm{h}}}_{d,j})$â€”
    and ${\bm{z}}_{d,j}$ indicates if the $j$-th neuron is activated on input ${\bm{I}}_{d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When processing intermediate layers, the definition ${\bm{I}}_{d}$ can easily
    be adapted from what we mentioned above. However, the story is different for the
    case of ${\bm{T}}_{d}$. When processing the last layer, as previously mentioned,
    ${\bm{T}}_{d}$ simply corresponds to $\tilde{{\bm{y}}}_{d}$. For intermediate
    layers, to define ${\bm{T}}_{d}$, the author proposes to use a similar optimization
    problem to ([32](#S5.E32 "In 5.5 Backpropagation through MILP â€£ 5 Linear Programming
    and Polyhedral Theory in Training â€£ When Deep Learning Meets Polyhedral Theory:
    A Survey")), but leaving ${\bm{W}}$ and ${\bm{b}}$ fixed and having ${\bm{I}}_{d}$
    as variables; this defines â€œoptimal inputsâ€ of a layer. These optimal inputs are
    then used as target outputs ${\bm{T}}_{d}$ when processing the preceding layer,
    and thus the algorithm is iterated. For details, see Goebbels ([2021](#bib.bib120)).'
  prefs: []
  type: TYPE_NORMAL
- en: The computational results in that paper show that a similar level of accuracy
    to that of gradient descent can be achieved. However, the use of potentially expensive
    MILPs impairs the applicability of this approach to large networks. Nonetheless,
    it shows an interesting new avenue for training whose running times may be improved
    in future implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Training binarized neural networks using MILP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, the training problem of a DNN is an unrestricted non-convex
    optimization problem, which is typically continuous as the weights and biases
    frequently are allowed to have any real value. Nonetheless, if the weights and
    biases are required to be integer-valued, the training problem becomes a discrete
    optimization problem, for which gradient-descent-based methods may find some difficulties
    in their applicability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, Icarte etÂ al. ([2019](#bib.bib161)) proposed a MILP formulation
    for the training problem of binarized neural networks (BNNs): these are neural
    networks where the weights and biases are restricted to be in $\{-1,0,1\}$ and
    where the activations are LTU (i.e. sign functions). Later on, Thorbjarnarson
    and Yorke-Smith ([2020](#bib.bib305), [2023](#bib.bib306)) used a similar technique
    to allow more general integer-valued weights. We review the core feature in these
    formulations that yield a *linear* formulation of the training problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us focus on an intermediate layer $i$ with width $n$, and let us omit biases
    to simplify the discussion. Using a DNNâ€™s layer-wise architecture, one usually
    aims at describing:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle=({\bm{W}}^{i}{\bm{h}}^{i-1}_{d})_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (33a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=\sigma(\hat{{\bm{h}}}^{i}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$ |  | (33b) |'
  prefs: []
  type: TYPE_TB
- en: 'We remind the reader that $D$ is the cardinality of the training set. Additionally,
    for each data point indexed by $d$ and each layer $i$, each variable ${\bm{h}}_{d}^{i}$
    is the output vector of all the neurons of the layer and each variable $\hat{{\bm{h}}}_{d,j}^{i}$
    is the input of neuron $j$. Besides the difficulty posed by the activation function,
    one important issue with system ([33](#S5.E33 "In 5.6 Training binarized neural
    networks using MILP â€£ 5 Linear Programming and Polyhedral Theory in Training â€£
    When Deep Learning Meets Polyhedral Theory: A Survey")) is the non-linearity of
    the products between the ${\bm{W}}$ and ${\bm{h}}$ variables. Nonetheless, this
    issue disappears when each entry of ${\bm{W}}$ and ${\bm{h}}$ is bounded and integer,
    as in the case of BNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us begin with reformulating ([33b](#S5.E33.2 "In 33 â€£ 5.6 Training binarized
    neural networks using MILP â€£ 5 Linear Programming and Polyhedral Theory in Training
    â€£ When Deep Learning Meets Polyhedral Theory: A Survey")). We can introduce auxiliary
    variables ${\bm{u}}_{d,j}^{i}\in\{0,1\}$ that will indicate if the neuron is active.
    We also introduce a tolerance $\varepsilon>0$ to determine the activity of a neuron.
    Using this, we can (approximately) reformulate ([33b](#S5.E33.2 "In 33 â€£ 5.6 Training
    binarized neural networks using MILP â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")) *linearly*
    using big-M constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=2{\bm{u}}_{d,j}^{i}-1$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\geq-M(1-{\bm{u}}_{d,j}^{i})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\leq-\varepsilon+M{\bm{u}}_{d,j}^{i}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34c) |'
  prefs: []
  type: TYPE_TB
- en: 'where $M$ is a large constant. As for ([33a](#S5.E33.1 "In 33 â€£ 5.6 Training
    binarized neural networks using MILP â€£ 5 Linear Programming and Polyhedral Theory
    in Training â€£ When Deep Learning Meets Polyhedral Theory: A Survey")), note that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{{\bm{h}}}^{i}_{d,j}=\sum_{k=1}{\bm{W}}^{i}_{j,k}{\bm{h}}^{i-1}_{d,k}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, using ([34a](#S5.E34.1 "In 34 â€£ 5.6 Training binarized neural networks
    using MILP â€£ 5 Linear Programming and Polyhedral Theory in Training â€£ When Deep
    Learning Meets Polyhedral Theory: A Survey")), we see that it suffices to describe
    each product ${\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ linearly. We can introduce
    new variables ${\bm{z}}_{j,k,d}^{i}$ and note that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{z}}_{j,k,d}^{i-1}={\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: if and only if the three variables satisfy
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}&#124;$ | $\displaystyle\leq{\bm{u}}_{d,k}^{i-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}-{\bm{W}}_{j,k}^{i}&#124;$ |
    $\displaystyle\leq 1-{\bm{u}}_{d,k}^{i-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{u}}_{d,k}^{i-1}$ | $\displaystyle\in\{0,1\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This last system can be easily converted to a linear system, and thus the training
    problem in this setting can be cast as a mixed-integer linear optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Other works have also relied on similar formulations to train neural networks.
    Icarte etÂ al. ([2019](#bib.bib161)) introduce different objective functions that
    can be used along with the linear system to produce a MILP that can train BNNs.
    They also introduce a Constraint-Programming-based model and a hybrid model, and
    then compare all of them computationally. Thorbjarnarson and Yorke-Smith ([2020](#bib.bib305))
    introduce more MILP-based training models that leverage piecewise linear approximations
    of well-known non-linear loss functions and that can handle integer weights beyond
    $\{-1,0,1\}$. A similar setting is studied by Sildir and Aydin ([2022](#bib.bib287)),
    where piecewise linear approximations of non-linear activations are used, and
    integer weights are exploited to formulate the training problem as a MILP. Finally,
    Bernardelli etÂ al. ([2022](#bib.bib25)) rely on a multi-objective MIP model for
    training BNNs; from here, they create a BNN ensemble to produce robust classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: From these articles, we can conclude that the MILP-based approach to training
    their neural networks can result in high-quality neural networks, especially in
    terms of generalization. However, many of these MILP-based methods currently do
    not scale well, as opposed to gradient-descent-based methods. We believe that,
    even though there are some theoretical limitations to the efficiency of MILP-based
    methods, there is a considerable practical improvement potential with using them
    in neural network training.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rapid advancement of neural networks and their ubiquity has given rise
    to numerous new challenges and opportunities in deep learning: we need to design
    them in more reliable ways, to better understand their limits, and to test their
    robustness, among other challenges. While, traditionally, continuous optimization
    has been the predominant technology used in the optimization tasks in deep learning,
    some of these new challenges have made discrete optimization tools gain a remarkable
    importance.'
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we have reviewed multiple areas where polyhedral theory and
    linear optimization have played a critical role. For example, in understanding
    the expressiveness of neural networks, in optimizing trained neural networks (e.g.
    for verification purposes), and even in designing new training algorithms. We
    hope this survey can provide perspective in a rapidly-changing field, and motivate
    further developments in both deep learning and discrete optimization. There is
    still much to be explored in the intersection of these fields.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We thank Christian Tjandraatmadja and Toon Tran for early feedback on the manuscript
    and asking questions that helped shaping it.
  prefs: []
  type: TYPE_NORMAL
- en: Thiago Serra was supported by the National Science Foundation (NSF) award IIS
    2104583. Calvin Tsay was supported by the Engineering & Physical Sciences Research
    Council (EPSRC) grant EP/T001577/1.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abrahamsen etÂ al. (2021) M.Â Abrahamsen, L.Â Kleist, and T.Â Miltzow. Training
    neural networks is er-complete. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agostinelli etÂ al. (2015) F.Â Agostinelli, M.Â Hoffman, P.Â Sadowski, and P.Â Baldi.
    Learning activation functions to improve deep neural networks. In *International
    Conference on Learning Representations (ICLR) Workshop*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amrami and Goldberg (2021) A.Â Amrami and Y.Â Goldberg. A simple geometric proof
    for the benefit of depth in ReLU networks. *arXiv:2101.07126*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anderson etÂ al. (2019) R.Â Anderson, J.Â Huchette, C.Â Tjandraatmadja, and J.Â Vielma.
    Strong mixed-integer programming formulations for trained neural networks. In
    *Integer Programming and Combinatorial Optimization (IPCO)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anderson etÂ al. (2020) R.Â Anderson, J.Â Huchette, W.Â Ma, C.Â Tjandraatmadja, and
    J.Â P. Vielma. Strong mixed-integer programming formulations for trained neural
    networks. *Mathematical Programming*, 183(1-2):3â€“39, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil etÂ al. (2019) C.Â Anil, J.Â Lucas, and R.Â Grosse. Sorting out Lipschitz function
    approximation. In *International Conference on Machine Learning (ICML)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky etÂ al. (2017) M.Â Arjovsky, S.Â Chintala, and L.Â Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning
    (ICML)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arora etÂ al. (2018) R.Â Arora, A.Â Basu, P.Â Mianjy, and A.Â Mukherjee. Understanding
    deep neural networks with rectified linear units. In *International Conference
    on Learning Representations (ICLR)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aziznejad etÂ al. (2020) S.Â Aziznejad, H.Â Gupta, J.Â Campos, and M.Â Unser. Deep
    neural networks with trainable activations and controlled Lipschitz constant.
    *IEEE Transactions on Signal Processing*, 68:4688â€“4699, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau etÂ al. (2015) D.Â Bahdanau, K.Â Cho, and Y.Â Bengio. Neural machine translation
    by jointly learning to align and translate. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balas (1998) E.Â Balas. Disjunctive programming: Properties of the convex hull
    of feasible points. *Discrete Applied Mathematics*, 89(1-3):3â€“44, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balas (2018) E.Â Balas. *Disjunctive Programming*. Springer Cham, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balas etÂ al. (1993) E.Â Balas, S.Â Ceria, and G.Â CornuÃ©jols. A lift-and-project
    cutting plane algorithm for mixed 0â€“1 programs. *Mathematical Programming*, 58(1-3):295â€“324,
    1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balas etÂ al. (1996) E.Â Balas, S.Â Ceria, and G.Â CornuÃ©jols. Mixed 0-1 programming
    by lift-and-project in a branch-and-cut framework. *Management Science*, 42(9):1229â€“1246,
    1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balestriero and Baraniuk (2018) R.Â Balestriero and R.Â G. Baraniuk. A spline
    theory of deep networks. In *International Conference on Machine Learning (ICML)*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BalunoviÄ‡ and Vechev (2020) M.Â BalunoviÄ‡ and M.Â Vechev. Adversarial training
    and provable defenses: Bridging the gap. In *International Conference on Learning
    Representations (ICLR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batten etÂ al. (2021) B.Â Batten, P.Â Kouvaros, A.Â Lomuscio, and Y.Â Zheng. Efficient
    neural network verification via layer-based semidefinite relaxations and linear
    cuts. In *International Joint Conference on Artificial Intelligence (IJCAI)*,
    pages 2184â€“2190, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio (2009) Y.Â Bengio. Learning deep architectures for AI. *Foundations and
    TrendsÂ®in Machine Learning*, 2(1):1â€“127, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio etÂ al. (2021) Y.Â Bengio, A.Â Lodi, and A.Â Prouvost. Machine learning
    for combinatorial optimization: a methodological tour dâ€™horizon. *European Journal
    of Operational Research*, 290(2):405â€“421, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennett (1992) K.Â P. Bennett. Decision tree construction via linear programming.
    Technical report, University of Wisconsin-Madison Department of Computer Sciences,
    1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennett and Mangasarian (1990) K.Â P. Bennett and O.Â L. Mangasarian. Neural network
    training via linear programming. Technical report, University of Wisconsin-Madison
    Department of Computer Sciences, 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennett and Mangasarian (1992) K.Â P. Bennett and O.Â L. Mangasarian. Robust linear
    programming discrimination of two linearly inseparable sets. *Optimization Methods
    and Software*, 1(1):23â€“34, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benussi etÂ al. (2022) E.Â Benussi, A.Â Patane, M.Â Wicker, L.Â Laurenti, and M.Â Kwiatkowska.
    Individual fairness guarantees for neural networks. In *International Joint Conference
    on Artificial Intelligence (IJCAI)*, pages 651â€“658, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bergman etÂ al. (2022) D.Â Bergman, T.Â Huang, P.Â Brooks, A.Â Lodi, and A.Â U. Raghunathan.
    Janos: an integrated predictive and prescriptive modeling framework. *INFORMS
    Journal on Computing*, 34(2):807â€“816, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bernardelli etÂ al. (2022) A.Â M. Bernardelli, S.Â Gualandi, H.Â C. Lau, and S.Â Milanesi.
    The bemi stardust: a structured ensemble of binarized neural networks. *arXiv
    preprint arXiv:2212.03659*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berrada etÂ al. (2018) L.Â Berrada, A.Â Zisserman, and M.Â P. Kumar. Deep Frank-Wolfe
    for neural network optimization. *arXiv:1811.07591*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertschinger etÂ al. (2022) D.Â Bertschinger, C.Â Hertrich, P.Â Jungeblut, T.Â Miltzow,
    and S.Â Weber. Training fully connected neural networks is $\exists\mathbb{R}$-complete.
    *arXiv:2204.01368*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhosekar and Ierapetritou (2018) A.Â Bhosekar and M.Â Ierapetritou. Advances
    in surrogate based modeling, feasibility analysis, and optimization: A review.
    *Computers & Chemical Engineering*, 108:250â€“267, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bianchini and Scarselli (2014) M.Â Bianchini and F.Â Scarselli. On the complexity
    of neural network classifiers: A comparison between shallow and deep architectures.
    *IEEE Transactions on Neural Networks and Learning Systems*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biau etÂ al. (2021) G.Â Biau, M.Â Sangnier, and U.Â Tanielian. Some theoretical
    insights into Wasserstein GANs. *Journal of Machine Learning Research*, 22, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bienstock and MuÃ±oz (2018) D.Â Bienstock and G.Â MuÃ±oz. Lp formulations for polynomial
    optimization problems. *SIAM Journal on Optimization*, 28(2):1121â€“1150, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bienstock etÂ al. (2023) D.Â Bienstock, G.Â MuÃ±oz, and S.Â Pokutta. Principled deep
    neural network training through linear programming. *Discrete Optimization*, 49:100795,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum and Rivest (1992) A.Â L. Blum and R.Â L. Rivest. Training a 3-node neural
    network is np-complete. *Neural Networks*, 5(1):117â€“127, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bohra etÂ al. (2020) P.Â Bohra, J.Â Campos, H.Â Gupta, S.Â Aziznejad, and M.Â Unser.
    Learning activation functions in deep (spline) neural networks. *IEEE Open Journal
    of Signal Processing*, 1:295â€“309, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bonami etÂ al. (2015) P.Â Bonami, A.Â Lodi, A.Â Tramontani, and S.Â Wiese. On mathematical
    programming with indicator constraints. *Mathematical Programming*, 151:191â€“223,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boob etÂ al. (2022) D.Â Boob, S.Â S. Dey, and G.Â Lan. Complexity of training ReLU
    neural network. *Discrete Optimization*, 44:100620, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Botoeva etÂ al. (2020) E.Â Botoeva, P.Â Kouvaros, J.Â Kronqvist, A.Â Lomuscio, and
    R.Â Misener. Efficient verification of relu-based neural networks via dependency
    analysis. In *AAAI Conference on Artificial Intelligence*, volumeÂ 34, pages 3291â€“3299,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottou etÂ al. (2018) L.Â Bottou, F.Â E. Curtis, and J.Â Nocedal. Optimization methods
    for large-scale machine learning. *SIAM Review*, 60(2):223â€“311, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridle (1990) J.Â S. Bridle. Probabilistic interpretation of feedforward classification
    network outputs, with relationships to statistical pattern recognition. In *Neurocomputing*,
    pages 227â€“236\. 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck etÂ al. (2015) S.Â Bubeck etÂ al. Convex optimization: Algorithms and complexity.
    *Foundations and TrendsÂ® in Machine Learning*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel etÂ al. (2020a) R.Â Bunel, A.Â DeÂ Palma, A.Â Desmaison, K.Â Dvijotham, P.Â Kohli,
    P.Â Torr, and M.Â PawanÂ Kumar. Lagrangian decomposition for neural network verification.
    In *Conference on Uncertainty in Artificial Intelligence (UAI)*, volume 124, pages
    370â€“379, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel etÂ al. (2020b) R.Â Bunel, P.Â Mudigonda, I.Â Turkaslan, P.Â Torr, J.Â Lu, and
    P.Â Kohli. Branch and bound for piecewise linear neural network verification. *Journal
    of Machine Learning Research*, 21(2020), 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel etÂ al. (2018) R.Â R. Bunel, I.Â Turkaslan, P.Â Torr, P.Â Kohli, and P.Â K.
    Mudigonda. A unified view of piecewise linear neural network verification. *Neural
    Information Processing Systems (NeurIPS)*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel etÂ al. (2020c) R.Â R. Bunel, O.Â Hinder, S.Â Bhojanapalli, and K.Â Dvijotham.
    An efficient nonconvex reformulation of stagewise convex optimization problems.
    *Neural Information Processing Systems (NeurIPS)*, 33:8247â€“8258, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burtea and Tsay (2023) R.-A. Burtea and C.Â Tsay. Safe deployment of reinforcement
    learning using deterministic optimization over neural networks. In *Computer Aided
    Chemical Engineering*, volumeÂ 52, pages 1643â€“1648\. Elsevier, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai etÂ al. (2023) J.Â Cai, K.-N. Nguyen, N.Â Shrestha, A.Â Good, R.Â Tu, X.Â Yu,
    S.Â Zhe, and T.Â Serra. Getting away with more network pruning: From sparsity to
    geometry and linear regions. In *International Conference on the Integration of
    Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceccon etÂ al. (2022) F.Â Ceccon, J.Â Jalving, J.Â Haddad, A.Â Thebelt, C.Â Tsay,
    C.Â D. Laird, and R.Â Misener. Omlt: Optimization & machine learning toolkit. *Journal
    of Machine Learning Research*, 23(349):1â€“8, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charisopoulos and Maragos (2018) V.Â Charisopoulos and P.Â Maragos. A tropical
    approach to neural networks with piecewise linear activations. *arXiv:1805.08749*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhry etÂ al. (2020) A.Â Chaudhry, N.Â Khan, P.Â Dokania, and P.Â Torr. Continual
    learning in low-rank orthogonal subspaces. In *Neural Information Processing Systems
    (NeurIPS)*, volumeÂ 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2022a) H.Â Chen, Y.Â G. Wang, and H.Â Xiong. Lower and upper bounds
    for numbers of linear regions of graph convolutional networks. *arXiv:2206.00228*,
    2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2022b) K.-L. Chen, H.Â Garudadri, and B.Â D. Rao. Improved bounds
    on neural complexity for representing piecewise linear functions. In *Neural Information
    Processing Systems (NeurIPS)*, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2022c) S.Â Chen, A.Â R. Klivans, and R.Â Meka. Learning deep ReLU
    networks is fixed-parameter tractable. In *2021 IEEE 62nd Annual Symposium on
    Foundations of Computer Science (FOCS)*, pages 696â€“707\. IEEE, 2022c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2020) T.Â Chen, J.-B. Lasserre, V.Â Magron, and E.Â Pauwels. Semialgebraic
    optimization for Lipschitz constants of ReLU networks. In *Neural Information
    Processing Systems (NeurIPS)*, volumeÂ 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen etÂ al. (2021a) W.Â Chen, X.Â Gong, and Z.Â Wang. Neural architecture search
    on ImageNet in four GPU hours: A theoretically inspired perspective. In *International
    Conference on Learning Representations (ICLR)*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen etÂ al. (2021b) W.Â Chen, X.Â Gong, Y.Â Wei, H.Â Shi, Z.Â Yan, Y.Â Yang, and Z.Â Wang.
    Understanding and accelerating neural architecture search with training-free and
    theory-grounded metrics. *arXiv:2108.11939*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng etÂ al. (2017) C.Â Cheng, G.Â NÃ¼hrenberg, and H.Â Ruess. Maximum resilience
    of artificial neural networks. In *Automated Technology for Verification and Analysis
    (ATVA)*, pages 251â€“268, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng etÂ al. (2018) C.-H. Cheng, G.Â NÃ¼hrenberg, C.-H. Huang, and H.Â Ruess.
    Verification of binarized neural networks via inter-neuron factoring: (short paper).
    In *International Conference on Verified Software: Theories, Tools, and Experiments
    (VSTTE)*, pages 279â€“290\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheon (2022) M.-S. Cheon. An outer-approximation guided optimization approach
    for constrained neural network inverse problems. *Mathematical Programming*, 196(1-2):173â€“202,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chu etÂ al. (2018) L.Â Chu, X.Â Hu, J.Â Hu, L.Â Wang, and J.Â Pei. Exact and consistent
    interpretation for piecewise linear neural networks: A closed form solution. In
    *ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ciresan etÂ al. (2012) D.Â Ciresan, U.Â Meier, J.Â Masci, and J.Â Schmidhuber. Multi
    column deep neural network for traffic sign classification. *Neural Networks*,
    2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohan etÂ al. (2022) S.Â Cohan, N.Â H. Kim, D.Â Rolnick, and M.Â vanÂ de Panne. Understanding
    the evolution of linear regions in deep reinforcement learning. In *Neural Information
    Processing Systems (NeurIPS)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collobert (2004) R.Â Collobert. *Large Scale Machine Learning*. PhD thesis, University
    Paris 6, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Courbariaux etÂ al. (2015) M.Â Courbariaux, Y.Â Bengio, and J.-P. David. BinaryConnect:
    Training deep neural networks with binary weights during propagations. *Neural
    Information Processing Systems (NeurIPS)*, 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Craighero etÂ al. (2020a) F.Â Craighero, F.Â Angaroni, A.Â Graudenzi, F.Â Stella,
    and M.Â Antoniotti. Investigating the compositional structure of deep neural networks.
    In *Machine Learning, Optimization, and Data Science (LOD)*, pages 322â€“334, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Craighero etÂ al. (2020b) F.Â Craighero, F.Â Angaroni, A.Â Graudenzi, F.Â Stella,
    and M.Â Antoniotti. Understanding deep learning with activation pattern diagrams.
    In *Proceedings of the Italian Workshop on Explainable Artificial Intelligence
    co-located with 19th International Conference of the Italian Association for Artificial
    Intelligence*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce and Hein (2018) F.Â Croce and M.Â Hein. A randomized gradient-free attack
    on ReLU networks. In *German Conference on Pattern Recognition (GCPR)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce etÂ al. (2019) F.Â Croce, M.Â Andriushchenko, and M.Â Hein. Provable robustness
    of relu networks via maximization of linear regions. In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce etÂ al. (2020) F.Â Croce, J.Â Rauber, and M.Â Hein. Scaling up the randomized
    gradient-free adversarial attack reveals overestimation of robustness using established
    attacks. *International Journal of Computer Vision*, 128:1028â€“1046, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croxton etÂ al. (2003) K.Â L. Croxton, B.Â Gendron, and T.Â L. Magnanti. A comparison
    of mixed-integer programming models for nonconvex piecewise linear cost minimization
    problems. *Management Science*, 49(9):1268â€“1273, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Curtis and Scheinberg (2017) F.Â E. Curtis and K.Â Scheinberg. Optimization methods
    for supervised machine learning: From linear models to deep learning. In *INFORMS
    TutORials in Operations Research*, pages 89â€“114. INFORMS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybenko (1989) G.Â Cybenko. Approximation by superpositions of a sigmoidal function.
    *Mathematics of Control, Signals and Systems*, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Danna etÂ al. (2007) E.Â Danna, M.Â Fenelon, Z.Â Gu, and R.Â Wunderling. Generating
    multiple solutions for mixed integer programming problems. In *Integer Programming
    and Combinatorial Optimization (IPCO)*, pages 280â€“294\. 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dantzig (1960) G.Â B. Dantzig. On the significance of solving linear programming
    problems with some integer variables. *Econometrica, Journal of the Econometric
    Society*, pages 30â€“44, 1960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dantzig and Eaves (1973) G.Â B. Dantzig and B.Â C. Eaves. Fourier-Motzkin elimination
    and its dual. *Journal of Combinatorial Theory (A)*, 14:288â€“297, 1973.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dathathri etÂ al. (2020) S.Â Dathathri, K.Â Dvijotham, A.Â Kurakin, A.Â Raghunathan,
    J.Â Uesato, R.Â R. Bunel, S.Â Shankar, J.Â Steinhardt, I.Â Goodfellow, P.Â S. Liang,
    etÂ al. Enabling certification of verification-agnostic networks via memory-efficient
    semidefinite programming. *Neural Information Processing Systems (NeurIPS)*, 33:5318â€“5331,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daubechies etÂ al. (2022) I.Â Daubechies, R.Â DeVore, S.Â Foucart, B.Â Hanin, and
    G.Â Petrova. Nonlinear approximation and (deep) ReLU networks. *Constructive Approximation*,
    55:127â€“172, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DeÂ Palma etÂ al. (2021) A.Â DeÂ Palma, H.Â Behl, R.Â R. Bunel, P.Â Torr, and M.Â P.
    Kumar. Scaling the convex barrier with active sets. In *International Conference
    on Learning Representations (ICLR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delarue etÂ al. (2020) A.Â Delarue, R.Â Anderson, and C.Â Tjandraatmadja. Reinforcement
    learning with combinatorial actions: An application to vehicle routing. *Neural
    Information Processing Systems (NeurIPS)*, 33:609â€“620, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng etÂ al. (2020) Y.Â Deng, X.Â Zheng, T.Â Zhang, C.Â Chen, G.Â Lou, and M.Â Kim.
    An analysis of adversarial attacks and defenses on autonomous driving models.
    In *2020 IEEE international conference on pervasive computing and communications
    (PerCom)*, pages 1â€“10\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin etÂ al. (2019) J.Â Devlin, M.-W. Chang, K.Â Lee, and K.Â Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    *Conference of the North American Chapter of the Association for Computational
    Linguistics (NAACL)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dey etÂ al. (2020) S.Â S. Dey, G.Â Wang, and Y.Â Xie. Approximation algorithms for
    training one-node relu neural networks. *IEEE Transactions on Signal Processing*,
    68:6696â€“6706, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey etÂ al. (2021) S.Â R. Dubey, S.Â K. Singh, and B.Â B. Chaudhuri. A comprehensive
    survey and performance analysis of activation functions in deep learning. *arXiv:2109.14545*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dutta etÂ al. (2018) S.Â Dutta, S.Â Jha, S.Â Sankaranarayanan, and A.Â Tiwari. Output
    range analysis for deep feedforward networks. In *NASA Formal Methods: 10th International
    Symposium, (NFM)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dvijotham etÂ al. (2018a) K.Â Dvijotham, S.Â Gowal, R.Â Stanforth, R.Â Arandjelovic,
    B.Â Oâ€™Donoghue, J.Â Uesato, and P.Â Kohli. Training verified learners with learned
    verifiers. *arXiv:1805.10265*, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dvijotham etÂ al. (2018b) K.Â Dvijotham, R.Â Stanforth, S.Â Gowal, T.Â A. Mann, and
    P.Â Kohli. A dual approach to scalable verification of deep networks. In *Conference
    on Uncertainty in Artificial Intelligence (UAI)*, volumeÂ 1, pageÂ 3, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dym etÂ al. (2020) N.Â Dym, B.Â Sober, and I.Â Daubechies. Expression of fractals
    through neural network functions. *IEEE Journal on Selected Areas in Information
    Theory*, 1(1):57â€“66, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ehlers (2017) R.Â Ehlers. Formal verification of piece-wise linear feed-forward
    neural networks. In *Automated Technology for Verification and Analysis (ATVA)*,
    pages 269â€“286\. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ElAraby etÂ al. (2020) M.Â ElAraby, G.Â Wolf, and M.Â Carvalho. Identifying efficient
    sub-networks using mixed integer programming. In *OPT Workshop*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken etÂ al. (2019) T.Â Elsken, J.Â H. Metzen, and F.Â Hutter. Neural architecture
    search: A survey. *Journal of Machine Learning Research*, 20:1â€“21, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2020) T.Â Ergen and M.Â Pilanci. Convex geometry of two-layer
    relu networks: Implicit autoencoding and interpretable models. In S.Â Chiappa and
    R.Â Calandra, editors, *International Conference on Artificial Intelligence and
    Statistics*, volume 108 of *Proceedings of Machine Learning Research*, pages 4024â€“4033\.
    PMLR, 26â€“28 Aug 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ergen and Pilanci (2021a) T.Â Ergen and M.Â Pilanci. Convex geometry and duality
    of over-parameterized neural networks. *The Journal of Machine Learning Research*,
    22(1):9646â€“9708, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2021b) T.Â Ergen and M.Â Pilanci. Global optimality beyond
    two layers: Training deep relu networks via convex programs. In *International
    Conference on Machine Learning (ICLR)*, pages 2993â€“3003\. PMLR, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2021c) T.Â Ergen and M.Â Pilanci. Implicit convex regularizers
    of cnn architectures: Convex optimization of two-and three-layer networks in polynomial
    time. In *International Conference on Learning Representations (ICLR)*, 2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2021d) T.Â Ergen and M.Â Pilanci. Path regularization: A convexity
    and sparsity inducing regularization for parallel relu networks. *arXiv preprint
    arXiv:2110.09548*, 2021d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ergen and Pilanci (2021e) T.Â Ergen and M.Â Pilanci. Revealing the structure of
    deep neural networks via convex duality. In *International Conference on Machine
    Learning*, pages 3004â€“3014\. PMLR, 2021e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen etÂ al. (2022) T.Â Ergen, A.Â Sahiner, B.Â Ozturkler, J.Â M. Pauly, M.Â Mardani,
    and M.Â Pilanci. Demystifying batch normalization in reLU networks: Equivalent
    convex optimization models and implicit regularization. In *International Conference
    on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ergen etÂ al. (2023) T.Â Ergen, H.Â I. Gulluk, J.Â Lacotte, and M.Â Pilanci. Globally
    optimal training of neural networks with threshold activation functions. In *International
    Conference on Learning Representations (ICLR)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eykholt etÂ al. (2018) K.Â Eykholt, I.Â Evtimov, E.Â Fernandes, B.Â Li, A.Â Rahmati,
    C.Â Xiao, A.Â Prakash, T.Â Kohno, and D.Â Song. Robust physical-world attacks on deep
    learning visual classification. In *Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan etÂ al. (2020) F.-L. Fan, R.Â Lai, and G.Â Wang. Quasi-equivalence of width
    and depth of neural networks. *arXiv:2002.02515*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan etÂ al. (2023) F.-L. Fan, W.Â Huang, X.Â Zhong, L.Â Ruan, T.Â Zeng, H.Â Xiong,
    and F.Â Wang. Deep relu networks have surprisingly simple polytopes. *arXiv:2305.09145*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fazlyab etÂ al. (2019) M.Â Fazlyab, A.Â Robey, H.Â Hassani, M.Â Morari, and G.Â J.
    Pappas. Efficient and accurate estimation of Lipschitz constants for deep neural
    networks. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fazlyab etÂ al. (2020) M.Â Fazlyab, M.Â Morari, and G.Â J. Pappas. Safety verification
    and robustness analysis of neural networks via quadratic constraints and semidefinite
    programming. *IEEE Transactions on Automatic Control*, 67(1):1â€“15, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferlez and Shoukry (2020) J.Â Ferlez and Y.Â Shoukry. AReN: Assured ReLU NN architecture
    for model predictive control of LTI systems. In *HSCC*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferrari etÂ al. (2022) C.Â Ferrari, M.Â N. Mueller, N.Â JovanoviÄ‡, and M.Â Vechev.
    Complete verification via multi-neuron relaxation guided branch-and-bound. In
    *International Conference on Learning Representations (ICLR)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finlayson etÂ al. (2019) S.Â G. Finlayson, J.Â D. Bowers, J.Â Ito, J.Â L. Zittrain,
    A.Â L. Beam, and I.Â S. Kohane. Adversarial attacks on medical machine learning.
    *Science*, 363(6433):1287â€“1289, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischetti and Jo (2018) M.Â Fischetti and J.Â Jo. Deep neural networks and mixed
    integer linear optimization. *Constraints*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourier (1826) J.Â Fourier. Solution dâ€™une question particuliÃ©re du calcul des
    inÃ©galitÃ©s. *Nouveau Bulletin des Sciences par la SociÃ©tÃ© Philomatique de Paris*,
    1826.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frank etÂ al. (1956) M.Â Frank, P.Â Wolfe, etÂ al. An algorithm for quadratic programming.
    *Naval Research Logistics Quarterly*, 3(1-2):95â€“110, 1956.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Froese and Hertrich (2023) V.Â Froese and C.Â Hertrich. Training neural networks
    is NP-hard in fixed dimension. *arXiv preprint arXiv:2303.17045*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Froese etÂ al. (2022) V.Â Froese, C.Â Hertrich, and R.Â Niedermeier. The computational
    complexity of relu network training parameterized by data dimensionality. *Journal
    of Artificial Intelligence Research*, 74:1775â€“1790, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fukushima (1980) K.Â Fukushima. Neocognitron: A self-organizing neural network
    model for a mechanism of pattern recognition unaffected by shift in position.
    *Biological Cybernetics*, 36(4):193â€“202, 1980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Funahashi (1989) K.-I. Funahashi. On the approximate realization of continuous
    mappings by neural networks. *Neural Networks*, 2(3), 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamba etÂ al. (2020) M.Â Gamba, S.Â Carlsson, H.Â Azizpour, and M.Â BjÃ¶rkman. Hyperplane
    arrangements of trained ConvNets are biased. *arXiv:2003.07797*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamba etÂ al. (2022) M.Â Gamba, A.Â Chmielewski-Anders, J.Â Sullivan, H.Â Azizpour,
    and M.Â BjÃ¶rkman. Are all linear regions created equal? In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gambella etÂ al. (2021) C.Â Gambella, B.Â Ghaddar, and J.Â Naoum-Sawaya. Optimization
    problems for machine learning: A survey. *European Journal of Operational Research*,
    290(3):807â€“828, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao etÂ al. (2020) J.Â Gao, C.Â Sun, H.Â Zhao, Y.Â Shen, D.Â Anguelov, C.Â Li, and
    C.Â Schmid. VectorNet: Encoding HD maps and agent dynamics from vectorized representation.
    In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GeiÃŸler etÂ al. (2012) B.Â GeiÃŸler, A.Â Martin, A.Â Morsi, and L.Â Schewe. Using
    piecewise linear functions for solving MINLPs. In *Mixed Integer Nonlinear Programming*,
    pages 287â€“314. Springer, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glass etÂ al. (2021) L.Â Glass, W.Â Hilali, and O.Â Nelles. Compressing interpretable
    representations of piecewise linear neural networks using neuro-fuzzy models.
    In *IEEE Symposium Series on Computational Intelligence (SSCI)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glorot etÂ al. (2011) X.Â Glorot, A.Â Bordes, and Y.Â Bengio. Deep sparse rectifier
    neural networks. In *International Conference on Artificial Intelligence and Statistics
    (AISTATS)*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goebbels (2021) S.Â Goebbels. Training of ReLU activated multilayerd neural networks
    with mixed integer linear programs. Technical report, Hochschule Niederrhein,
    Fachbereich Elektrotechnik & Informatik, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goel etÂ al. (2021) S.Â Goel, A.Â Klivans, P.Â Manurangsi, and D.Â Reichman. Tight
    hardness results for training depth-2 relu networks. In *12th Innovations in Theoretical
    Computer Science Conference (ITCS 2021)*. Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r
    Informatik, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goerigk and Kurtz (2023) M.Â Goerigk and J.Â Kurtz. Data-driven robust optimization
    using deep neural networks. *Computers & Operations Research*, 151:106087, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow etÂ al. (2013) I.Â Goodfellow, D.Â Warde-Farley, M.Â Mirza, A.Â Courville,
    and Y.Â Bengio. Maxout networks. In *International Conference on Machine Learning
    (ICML)*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow etÂ al. (2015) I.Â Goodfellow, J.Â Shlens, and C.Â Szegedy. Explaining
    and harnessing adversarial examples. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow etÂ al. (2016) I.Â Goodfellow, Y.Â Bengio, and A.Â Courville. *Deep learning*.
    MIT press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow etÂ al. (2014) I.Â J. Goodfellow, J.Â Pouget-Abadie, M.Â Mirza, B.Â Xu,
    D.Â Warde-Farley, S.Â Ozair, A.Â Courville, and Y.Â Bengio. Generative adversarial
    nets. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gopinath etÂ al. (2019) D.Â Gopinath, H.Â Converse, C.Â S. Pasareanu, and A.Â Taly.
    Property inference for deep neural networks. In *IEEE/ACM International Conference
    on Automated Software Engineering (ASE)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goujon etÂ al. (2022) A.Â Goujon, A.Â Etemadi, and M.Â Unser. The role of depth,
    width, and activation complexity in the number of linear regions of neural networks.
    *arXiv:2206.08615*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gowal etÂ al. (2018) S.Â Gowal, K.Â Dvijotham, R.Â Stanforth, R.Â Bunel, C.Â Qin,
    J.Â Uesato, R.Â Arandjelovic, T.Â Mann, and P.Â Kohli. On the effectiveness of interval
    bound propagation for training verifiably robust models. *arXiv:1810.12715*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves and Jaitly (2014) A.Â Graves and N.Â Jaitly. Towards end-to-end speech
    recognition with recurrent neural networks. In *International Conference on Machine
    Learning (ICML)*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigsby and Lindsey (2022) J.Â E. Grigsby and K.Â Lindsey. On transversality of
    bent hyperplane arrangements and the topological expressiveness of ReLU neural
    networks. *SIAM Journal on Applied Algebra and Geometry*, 6(2), 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigsby etÂ al. (2023) J.Â E. Grigsby, K.Â Lindsey, and D.Â Rolnick. Hidden symmetries
    of ReLU networks. In *International Conference on Machine Learning (ICML)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grimstad and Andersson (2019) B.Â Grimstad and H.Â Andersson. ReLU networks as
    surrogate models in mixed-integer linear programs. *Computers & Chemical Engineering*,
    131:106580, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grossmann and Ruiz (2012) I.Â E. Grossmann and J.Â P. Ruiz. Generalized disjunctive
    programming: A framework for formulation and alternative algorithms for MINLP
    optimization. In *Mixed Integer Nonlinear Programming*, pages 93â€“115, New York,
    NY, 2012\. Springer New York.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hahnloser etÂ al. (2000) R.Â Hahnloser, R.Â Sarpeshkar, M.Â Mahowald, R.Â Douglas,
    and S.Â Seung. Digital selection and analogue amplification coexist in a cortex-inspired
    silicon circuit. *Nature*, 405, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han and GÃ³mez (2021) S.Â Han and A.Â GÃ³mez. Single-neuron convexification for
    binarized neural networks, 2021. URL https://optimization-online.org/?p=17148.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin and Rolnick (2019a) B.Â Hanin and D.Â Rolnick. Complexity of linear regions
    in deep networks. In *International Conference on Machine Learning (ICML)*, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin and Rolnick (2019b) B.Â Hanin and D.Â Rolnick. Deep ReLU networks have surprisingly
    few activation patterns. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 32, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin and Sellke (2017) B.Â Hanin and M.Â Sellke. Approximating continuous functions
    by ReLU nets of minimal width. *arXiv:1710.11278*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hashemi etÂ al. (2021) V.Â Hashemi, P.Â Kouvaros, and A.Â Lomuscio. OSIP: Tightened
    bound propagation for the verification of ReLU neural networks. In *International
    Conference on Software Engineering and Formal Methods (SEFM)*, pages 463â€“480\.
    Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He etÂ al. (2021) F.Â He, S.Â Lei, J.Â Ji, and D.Â Tao. Neural networks behave as
    hash encoders: An empirical study. *arXiv:2101.05490*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He etÂ al. (2020) J.Â He, L.Â Li, J.Â Xu, and C.Â Zheng. ReLU deep neural networks
    and linear finite elements. *Journal of Computational Mathematics*, 38:502â€“527,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He etÂ al. (2015) K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun. Delving deep into rectifiers:
    Surpassing human-level performance on ImageNet classification. In *IEEE International
    Conference on Computer Vision (ICCV)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He etÂ al. (2016) K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun. Deep residual learning
    for image recognition. In *Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henriksen and Lomuscio (2021) P.Â Henriksen and A.Â Lomuscio. DEEPSPLIT: an efficient
    splitting method for neural network verification via indirect effect analysis.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    2549â€“2555, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henriksen etÂ al. (2022) P.Â Henriksen, F.Â Leofante, and A.Â Lomuscio. Repairing
    misclassifications in neural networks using limited data. In *ACM/SIGAPP Symposium
    On Applied Computing (SAC)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hertrich etÂ al. (2021) C.Â Hertrich, A.Â Basu, M.Â D. Summa, and M.Â Skutella. Towards
    lower bounds on the depth of ReLU neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton etÂ al. (2012) G.Â Hinton, L.Â Deng, G.Â Dahl, A.Â Mohamed, N.Â Jaitly, A.Â Senior,
    V.Â Vanhoucke, P.Â Nguyen, T.Â Sainath, and B.Â Kingsbury. Deep neural networks for
    acoustic modeling in speech recognition. *IEEE Signal Processing Magazine*, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinz (2021) P.Â Hinz. Using activation histograms to bound the number of affine
    regions in ReLU feed-forward neural networks. *arXiv:2103.17174*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinz and vanÂ de Geer (2019) P.Â Hinz and S.Â vanÂ de Geer. A framework for the
    construction of upper bounds on the number of affine linear regions of ReLU feed-forward
    neural networks. *IEEE Transactions on Information Theory*, 65(11):7304â€“7324,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) S.Â Hochreiter and J.Â Schmidhuber. Long short-term
    memory. *Neural Computation*, 9(8):1735â€“1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopfield (1982) J.Â Hopfield. Neural networks and physical systems with emergent
    collective computational abilities. *Proceedings of the National Academy of Sciences*,
    79:2554â€“2558, 1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hornik etÂ al. (1989) K.Â Hornik, M.Â Stinchcombe, and H.Â White. Multilayer feedforward
    networks are universal approximators. *Neural Networks*, 2(5), 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu etÂ al. (2020a) T.Â Hu, Z.Â Shang, and G.Â Cheng. Sharp rate of convergence for
    deep neural network classifiers under the teacher-student setting. *arXiv:2001.06892*,
    2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu etÂ al. (2020b) X.Â Hu, W.Â Liu, J.Â Bian, and J.Â Pei. Measuring model complexity
    of neural networks with curve activation functions. In *ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining (KDD)*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu etÂ al. (2021) X.Â Hu, L.Â Chu, J.Â Pei, W.Â Liu, and J.Â Bian. Model complexity
    of deep learning: a survey. *Knowledge and Information Systems*, 63:2585â€“2619,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang etÂ al. (2020) X.Â Huang, D.Â Kroening, W.Â Ruan, J.Â Sharp, Y.Â Sun, E.Â Thamo,
    M.Â Wu, and X.Â Yi. A survey of safety and trustworthiness of deep neural networks:
    Verification, testing, adversarial attack and defence, and interpretability. *Computer
    Science Review*, 37:100270, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huchette and Vielma (2022) J.Â Huchette and J.Â P. Vielma. Nonconvex piecewise
    linear functions: Advanced formulations and simple modeling tools. *Operations
    Research*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huster etÂ al. (2018) T.Â Huster, C.-Y.Â J. Chiang, and R.Â Chadha. Limitations
    of the Lipschitz constant as a defense against adversarial examples. In *ECML
    PKDD Workshops*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang and Heinecke (2020) W.-L. Hwang and A.Â Heinecke. Un-rectifying non-linear
    networks for signal representation. *IEEE Transactions on Signal Processing*,
    68:196â€“210, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Icarte etÂ al. (2019) R.Â T. Icarte, L.Â Illanes, M.Â P. Castro, A.Â A. Cire, S.Â A.
    McIlraith, and J.Â C. Beck. Training binarized neural networks using mip and cp.
    In *International Conference on Principles and Practice of Constraint Programming*,
    pages 401â€“417\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe and Szegedy (2015) S.Â Ioffe and C.Â Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In *International
    Conference on Machine Learning (ICML)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeroslow and Lowe (1984) R.Â G. Jeroslow and J.Â K. Lowe. *Modelling with integer
    variables*. Springer, 1984.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia and Rinard (2020) K.Â Jia and M.Â Rinard. Efficient exact verification of
    binarized neural networks. *Neural Information Processing Systems (NeurIPS)*,
    33:1782â€“1795, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson etÂ al. (2020) T.Â T. Johnson, D.Â M. Lopez, P.Â Musau, H.-D. Tran, E.Â Botoeva,
    F.Â Leofante, A.Â Maleki, C.Â Sidrane, J.Â Fan, and C.Â Huang. ARCH-COMP20 category
    report: Artificial intelligence and neural network control systems (AINNCS) for
    continuous and hybrid systems plants. In *International Workshop on Applied Verification
    of Continuous and Hybrid Systems (ARCH20)*, volumeÂ 74, pages 107â€“139, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jordan and Dimakis (2020) M.Â Jordan and A.Â G. Dimakis. Exactly computing the
    local Lipschitz constant of ReLU networks. In *Neural Information Processing Systems
    (NeurIPS)*, volumeÂ 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jordan etÂ al. (2019) M.Â Jordan, J.Â Lewis, and A.Â G. Dimakis. Provable certificates
    for adversarial examples: Fitting a ball in the union of polytopes. In *Neural
    Information Processing Systems (NeurIPS)*, volumeÂ 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karg and Lucia (2020) B.Â Karg and S.Â Lucia. Efficient representation and approximation
    of model predictive control laws via deep learning. *IEEE Transactions on Cybernetics*,
    50(9):3866â€“3878, 2020. doi: 10.1109/TCYB.2020.2999556.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katz etÂ al. (2017) G.Â Katz, C.Â Barrett, D.Â L. Dill, K.Â Julian, and M.Â J. Kochenderfer.
    Reluplex: An efficient SMT solver for verifying deep neural networks. In *Computer
    Aided Verification (CAV)*, pages 97â€“117\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katz etÂ al. (2019) G.Â Katz, D.Â A. Huang, D.Â Ibeling, K.Â Julian, C.Â Lazarus,
    R.Â Lim, P.Â Shah, S.Â Thakoor, H.Â Wu, A.Â ZeljiÄ‡, etÂ al. The marabou framework for
    verification and analysis of deep neural networks. In *Computer Aided Verification
    (CAV)*, pages 443â€“452. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katz etÂ al. (2020) J.Â Katz, I.Â Pappas, S.Â Avraamidou, and E.Â N. Pistikopoulos.
    Integrating deep learning models and multiparametric programming. *Computers &
    Chemical Engineering*, 136:106801, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keup and Helias (2022) C.Â Keup and M.Â Helias. Origami in N dimensions: How
    feed-forward networks manufacture linear separability. *arXiv:2203.11355*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khalife and Basu (2022) S.Â Khalife and A.Â Basu. Neural networks with linear
    threshold activations: structure and algorithms. In *Integer Programming and Combinatorial
    Optimization (IPCO)*, pages 347â€“360\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khedr etÂ al. (2020) H.Â Khedr, J.Â Ferlez, and Y.Â Shoukry. Effective formal verification
    of neural networks using the geometry of linear regions. *arXiv:2006.10864*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2014) D.Â P. Kingma and J.Â Ba. Adam: A method for stochastic
    optimization. *arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kody etÂ al. (2022) A.Â Kody, S.Â Chevalier, S.Â Chatzivasileiadis, and D.Â Molzahn.
    Modeling the ac power flow equations with optimally compact neural networks: Application
    to unit commitment. *Electric Power Systems Research*, 213:108282, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kouvaros etÂ al. (2021) P.Â Kouvaros, T.Â Kyono, F.Â Leofante, A.Â Lomuscio, D.Â Margineantu,
    D.Â Osipychev, and Y.Â Zheng. Formal analysis of neural network-based systems in
    the aircraft domain. In *International Symposium on Formal Methods (FM)*, pages
    730â€“740\. Springer, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky etÂ al. (2012) A.Â Krizhevsky, I.Â Sutskever, and G.Â Hinton. Imagenet
    classification with deep convolutional neural networks. In *Neural Information
    Processing Systems (NeurIPS)*, volumeÂ 25, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kronqvist etÂ al. (2021) J.Â Kronqvist, R.Â Misener, and C.Â Tsay. Between steps:
    Intermediate relaxations between big-M and convex hull formulations. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kronqvist etÂ al. (2022) J.Â Kronqvist, R.Â Misener, , and C.Â Tsay. P-split formulations:
    A class of intermediate formulations between big-M and convex hull for disjunctive
    constraints. *arXiv:2202.05198*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar etÂ al. (2019) A.Â Kumar, T.Â Serra, and S.Â Ramalingam. Equivalent and approximate
    transformations of deep neural networks. *arXiv:1905.1142*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacoste-Julien etÂ al. (2013) S.Â Lacoste-Julien, M.Â Jaggi, M.Â Schmidt, and P.Â Pletscher.
    Block-coordinate Frank-Wolfe optimization for structural SVMs. In *International
    Conference on Machine Learning (ICML)*, pages 53â€“61, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lan etÂ al. (2022) J.Â Lan, Y.Â Zheng, and A.Â Lomuscio. Tight neural network verification
    via semidefinite relaxations and linear reformulations. In *AAAI Conference on
    Artificial Intelligence*, volumeÂ 36, pages 7272â€“7280, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latorre etÂ al. (2020) F.Â Latorre, P.Â Rolland, and V.Â Cevher. Lipschitz constant
    estimation of neural networks via sparse polynomial optimization. In *International
    Conference on Learning Representations (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun etÂ al. (1989) Y.Â LeCun, B.Â Boser, J.Â S. Denker, D.Â Henderson, R.Â E. Howard,
    W.Â Hubbard, and L.Â D. Jackel. Backpropagation applied to handwritten zip code
    recognition. *Neural Computation*, 1(4):541â€“551, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun etÂ al. (1998) Y.Â LeCun, L.Â Bottou, G.Â B. Orr, and K.-R. MÃ¼ller. Efficient
    backprop. In G.Â Montavon, G.Â Orr, and K.Â MÃ¼ller, editors, *Neural Networks: Tricks
    of the Trade*. Springer, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun etÂ al. (2015) Y.Â LeCun, Y.Â Bengio, and G.Â Hinton. Deep learning. *Nature*,
    521, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee etÂ al. (2019) G.-H. Lee, D.Â Alvarez-Melis, and T.Â S. Jaakkola. Towards robust,
    locally linear deep networks. In *International Conference on Learning Representations
    (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee and Wilson (2001) J.Â Lee and D.Â Wilson. Polyhedral methods for piecewise-linear
    functions I: the lambda method. *Discrete Applied Mathematics*, 108(3):269â€“285,
    2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leofante etÂ al. (2018) F.Â Leofante, N.Â Narodytska, L.Â Pulina, and A.Â Tacchella.
    Automated verification of neural networks: Advances, challenges and perspectives.
    *arXiv:1805.09938*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li etÂ al. (2022) L.Â Li, T.Â Xie, and B.Â Li. Sok: Certified robustness for deep
    neural networks. In *2023 IEEE Symposium on Security and Privacy (SP)*, pages
    94â€“115\. IEEE Computer Society, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang and Xu (2021) X.Â Liang and J.Â Xu. Biased ReLU neural networks. *Neurocomputing*,
    423:71â€“79, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap etÂ al. (2015) T.Â P. Lillicrap, J.Â J. Hunt, A.Â Pritzel, N.Â Heess, T.Â Erez,
    Y.Â Tassa, D.Â Silver, and D.Â Wierstra. Continuous control with deep reinforcement
    learning. *arXiv:1509.02971*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linnainmaa (1970) S.Â Linnainmaa. The representation of the cumulative rounding
    error of an algorithm as a Taylor expansion of the local rounding errors (in Finnish).
    Masterâ€™s thesis, Univ. Helsinki, 1970.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little (1974) W.Â Little. The existence of persistent states in the brain. *Mathematical
    Biosciences*, 19:101â€“120, 1974.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Liang (2021) B.Â Liu and Y.Â Liang. Optimal function approximation with
    ReLU neural networks. *Neurocomputing*, 435:216â€“227, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu etÂ al. (2021) C.Â Liu, T.Â Arnon, C.Â Lazarus, C.Â Strong, C.Â Barrett, M.Â J.
    Kochenderfer, etÂ al. Algorithms for verifying deep neural networks. *Foundations
    and TrendsÂ® in Optimization*, 4(3-4):244â€“404, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu etÂ al. (2020) X.Â Liu, X.Â Han, N.Â Zhang, and Q.Â Liu. Certified monotonic
    neural networks. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 33,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lombardi etÂ al. (2017) M.Â Lombardi, M.Â Milano, and A.Â Bartolini. Empirical decision
    model learning. *Artificial Intelligence*, 244:343â€“367, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lomuscio and Maganti (2017) A.Â Lomuscio and L.Â Maganti. An approach to reachability
    analysis for feed-forward ReLU neural networks. *arXiv:1706.07351*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loukas etÂ al. (2021) A.Â Loukas, M.Â Poiitis, and S.Â Jegelka. What training reveals
    about neural network complexity. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu etÂ al. (2017) Z.Â Lu, H.Â Pu, F.Â Wang, Z.Â Hu, and L.Â Wang. The expressive
    power of neural networks: A view from the width. In *Neural Information Processing
    Systems (NeurIPS)*, volumeÂ 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lueg etÂ al. (2021) L.Â Lueg, B.Â Grimstad, A.Â Mitsos, and A.Â M. Schweidtmann.
    reluMIP: Open source tool for MILP optimization of ReLU neural networks, 2021.
    URL https://github.com/ChemEngAI/ReLU_ANN_MILP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu etÂ al. (2020) Z.Â Lyu, C.-Y. Ko, Z.Â Kong, N.Â Wong, D.Â Lin, and L.Â Daniel.
    Fastened crown: Tightened neural network robustness certificates. In *AAAI Conference
    on Artificial Intelligence*, volumeÂ 34, pages 5037â€“5044, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maas etÂ al. (2013) A.Â Maas, A.Â Hannun, and A.Â Ng. Rectifier nonlinearities improve
    neural network acoustic models. In *ICML Workshop on Deep Learning for Audio,
    Speech and Language Processing*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madry etÂ al. (2018) A.Â Madry, A.Â Makelov, L.Â Schmidt, D.Â Tsipras, and A.Â Vladu.
    Towards deep learning models resistant to adversarial attacks. In *International
    Conference on Learning Representations (ICLR)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makhoul etÂ al. (1989) J.Â Makhoul, R.Â Schwartz, and A.Â El-Jaroudi. Classification
    capabilities of two-layer neural nets. In *International Conference on Acoustics,
    Speech, and Signal Processing (ICASSP)*, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malach and Shalev-Shwartz (2019) E.Â Malach and S.Â Shalev-Shwartz. Is deeper
    better only when shallow is good? In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mangasarian (1993) O.Â L. Mangasarian. Mathematical programming in neural networks.
    *ORSA Journal on Computing*, 5(4):349â€“360, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maragno etÂ al. (2021) D.Â Maragno, H.Â Wiberg, D.Â Bertsimas, S.Â I. Birbil, D.Â d.
    Hertog, and A.Â Fajemisin. Mixed-integer optimization with constraint learning.
    *arXiv:2111.04469*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maragno etÂ al. (2023) D.Â Maragno, J.Â Kurtz, T.Â E. RÃ¶ber, R.Â Goedhart, Å.Â I.
    Birbil, and D.Â d. Hertog. Finding regions of counterfactual explanations via robust
    optimization. *arXiv:2301.11113*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maragos etÂ al. (2021) P.Â Maragos, V.Â Charisopoulos, and E.Â Theodosis. Tropical
    geometry and machine learning. *Proceedings of the IEEE*, 109(5):728â€“755, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masden (2022) M.Â Masden. Algorithmic determination of the combinatorial structure
    of the linear regions of ReLU neural networks. *arXiv:2207.07696*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matoba etÂ al. (2022) K.Â Matoba, N.Â Dimitriadis, and F.Â Fleuret. The theoretical
    expressiveness of maxpooling. *arXiv:2203.01016*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matousek (2002) J.Â Matousek. *Lectures on Discrete Geometry*, volume 212. Springer
    Science & Business Media, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McBride and Sundmacher (2019) K.Â McBride and K.Â Sundmacher. Overview of surrogate
    modeling in chemical process engineering. *Chemie Ingenieur Technik*, 91(3):228â€“239,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCulloch and Pitts (1943) W.Â McCulloch and W.Â Pitts. A logical calculus of
    the ideas immanent in nervous activity. *Bulletin of Mathematical Biophysics*,
    5:115â€“133, 1943.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mhaskar and Poggio (2020) H.Â N. Mhaskar and T.Â Poggio. Function approximation
    by deep networks. *Communications on Pure & Applied Analysis*, 19(8):4085â€“4095,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minsky and Papert (1969) M.Â Minsky and S.Â Papert. *Perceptrons: An Introduction
    to Computational Geometry*. The MIT Press, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirman etÂ al. (2018) M.Â Mirman, T.Â Gehr, and M.Â Vechev. Differentiable abstract
    interpretation for provably robust neural networks. In *International Conference
    on Machine Learning (ICML)*, volumeÂ 80, pages 3578â€“3586, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misener and Floudas (2012) R.Â Misener and C.Â A. Floudas. Global optimization
    of mixed-integer quadratically-constrained quadratic programs (MIQCQP) through
    piecewise-linear and edge-concave relaxations. *Mathematical Programming*, 136(1):155â€“182,
    2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih etÂ al. (2015) V.Â Mnih, K.Â Kavukcuoglu, D.Â Silver, A.Â A. Rusu, J.Â Veness,
    M.Â G. Bellemare, A.Â Graves, M.Â Riedmiller, A.Â K. Fidjeland, G.Â Ostrovski, S.Â Petersen,
    C.Â Beattie, A.Â Sadik, I.Â Antonoglou, H.Â King, D.Â Kumaran, D.Â Wierstra, S.Â Legg,
    and D.Â Hassabis. Human-level control through deep reinforcement learning. *Nature*,
    518:529â€“533, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MontÃºfar (2017) G.Â MontÃºfar. Notes on the number of linear regions of deep neural
    networks. In *Sampling Theory and Applications (SampTA)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MontÃºfar etÂ al. (2014) G.Â MontÃºfar, R.Â Pascanu, K.Â Cho, and Y.Â Bengio. On the
    number of linear regions of deep neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, volumeÂ 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MontÃºfar etÂ al. (2022) G.Â MontÃºfar, Y.Â Ren, and L.Â Zhang. Sharp bounds for the
    number of regions of maxout networks and vertices of Minkowski sums. *SIAM Journal
    on Applied Algebra and Geometry*, 6(4), 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motzkin (1936) T.Â Motzkin. *Beitrage zur theorie der linearen Ungleichungen*.
    PhD thesis, University of Basel, 1936.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukhopadhyay etÂ al. (1993) S.Â Mukhopadhyay, A.Â Roy, L.Â S. Kim, and S.Â Govil.
    A polynomial time algorithm for generating neural networks for pattern classification:
    Its stability properties and some test results. *Neural Computation*, 5(2):317â€“330,
    1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair and Hinton (2010) V.Â Nair and G.Â Hinton. Rectified linear units improve
    restricted boltzmann machines. In *International Conference on Machine Learning
    (ICML)*, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narodytska etÂ al. (2018) N.Â Narodytska, S.Â Kasiviswanathan, L.Â Ryzhyk, M.Â Sagiv,
    and T.Â Walsh. Verifying properties of binarized deep neural networks. In *AAAI
    Conference on Artificial Intelligence*, volumeÂ 32, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nelles etÂ al. (2000) O.Â Nelles, A.Â Fink, and R.Â Isermann. Local linear model
    trees (LOLIMOT) toolbox for nonlinear system identification. In *IFAC Symposium
    on System Identification (SYSID)*, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nesterov (1983) Y.Â E. Nesterov. A method of solving a convex programming problem
    with convergence rate $o\bigl{(}\frac{1}{k^{2}}\bigr{)}$. *Doklady Akademii Nauk*,
    269:543â€“547, 1983.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton and Papachristodoulou (2021) M.Â Newton and A.Â Papachristodoulou. Exploiting
    sparsity for neural network verification. In *Learning for Dynamics and Control
    (L4DC)*, pages 715â€“727. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen etÂ al. (2018) Q.Â Nguyen, M.Â C. Mukkamala, and M.Â Hein. Neural networks
    should be wide enough to learn disconnected decision regions. In *International
    Conference on Machine Learning (ICML)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen and Huchette (2022) T.Â Nguyen and J.Â Huchette. Neural network verification
    as piecewise linear optimization: Formulations for the composition of staircase
    functions. *arXiv:2211.14706*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novak etÂ al. (2018) R.Â Novak, Y.Â Bahri, D.Â A. Abolafia, J.Â Pennington, and
    J.Â Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical
    study. In *International Conference on Learning Representations (ICLR)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI etÂ al. (2019) OpenAI, C.Â Berner, G.Â Brockman, B.Â Chan, V.Â Cheung, P.Â DÈ©biak,
    C.Â Dennison, D.Â Farhi, Q.Â Fischer, S.Â Hashme, C.Â Hesse, R.Â JÃ³zefowicz, S.Â Gray,
    C.Â Olsson, J.Â Pachocki, M.Â Petrov, H.Â P. deÂ OliveiraÂ Pinto, J.Â Raiman, T.Â Salimans,
    J.Â Schlatter, J.Â Schneider, S.Â Sidor, I.Â Sutskever, J.Â Tang, F.Â Wolski, and S.Â Zhang.
    Dota 2 with large scale deep reinforcement learning. *arXiv:1912.06680*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padberg (2000) M.Â Padberg. Approximating separable nonlinear functions via mixed
    zero-one programs. *Operations Research Letters*, 27(1):1â€“5, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papalexopoulos etÂ al. (2022) T.Â P. Papalexopoulos, C.Â Tjandraatmadja, R.Â Anderson,
    J.Â P. Vielma, and D.Â Belanger. Constrained discrete black-box optimization using
    mixed-integer programming. In *International Conference on Machine Learning (ICML)*,
    volume 162, pages 17295â€“17322, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park etÂ al. (2019) D.Â S. Park, W.Â Chan, Y.Â Zhang, C.-C. Chiu, B.Â Zoph, E.Â D.
    Cubuk, and Q.Â V. Le. SpecAugment: A simple data augmentation method for automatic
    speech recognition. In *Interspeech*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park etÂ al. (2021a) S.Â Park, C.Â Yun, J.Â Lee, and J.Â Shin. Minimum width for
    universal approximation. In *International Conference on Learning Representations
    (ICLR)*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park etÂ al. (2021b) Y.Â Park, S.Â Lee, G.Â Kim, and D.Â M. Blei. Unsupervised representation
    learning via neural activation coding. In *International Conference on Machine
    Learning (ICML)*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pascanu etÂ al. (2014) R.Â Pascanu, G.Â MontÃºfar, and Y.Â Bengio. On the number
    of response regions of deep feedforward networks with piecewise linear activations.
    In *International Conference on Learning Representations (ICLR)*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patrick L.Â Combettes (2019) J.-C.Â P. Patrick L.Â Combettes. Lipschitz certificates
    for layered network structures driven by averaged activation operators. *arXiv:1903.01014*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perakis and Tsiourvas (2022) G.Â Perakis and A.Â Tsiourvas. Optimizing objective
    functions from trained relu neural networks via sampling. *arXiv:2205.14189*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters etÂ al. (2018) M.Â E. Peters, M.Â Neumann, M.Â Iyyer, M.Â Gardner, C.Â Clark,
    K.Â Lee, and L.Â Zettlemoyer. Deep contextualized word representations. In *Conference
    of the North American Chapter of the Association for Computational Linguistics
    (NAACL)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phuong and Lampert (2020) M.Â Phuong and C.Â H. Lampert. Functional vs. parametric
    equivalence of ReLU networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pilanci and Ergen (2020) M.Â Pilanci and T.Â Ergen. Neural networks are convex
    regularizers: Exact polynomial-time convex optimization formulations for two-layer
    networks. In *International Conference on Machine Learning (ICML)*, pages 7695â€“7705\.
    PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pokutta etÂ al. (2020) S.Â Pokutta, C.Â Spiegel, and M.Â Zimmer. Deep neural network
    training with frank-wolfe. *arXiv:2010.07243*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polyak (1964) B.Â T. Polyak. Some methods of speeding up the convergence of iteration
    methods. *USSR Computational Mathematics and Mathematical Physics*, 4:1â€“17, 1964.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pulina and Tacchella (2010) L.Â Pulina and A.Â Tacchella. An abstraction-refinement
    approach to verification of artificial neural networks. In *Computer Aided Verification
    (CAV)*, pages 243â€“257, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford etÂ al. (2018) A.Â Radford, K.Â Narasimhan, T.Â Salimans, and I.Â Sutskever.
    Improving language understanding by generative pre-training. Technical report,
    OpenAI, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raghu etÂ al. (2017) M.Â Raghu, B.Â Poole, J.Â Kleinberg, S.Â Ganguli, and J.Â Dickstein.
    On the expressive power of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raghunathan etÂ al. (2018) A.Â Raghunathan, J.Â Steinhardt, and P.Â S. Liang. Semidefinite
    relaxations for certifying robustness to adversarial examples. *Neural Information
    Processing Systems (NeurIPS)*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandran etÂ al. (2018) P.Â Ramachandran, B.Â Zoph, and Q.Â V. Le. Searching
    for activation functions. In *ICLR Workshop Track*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raman and Grossmann (1994) R.Â Raman and I.Â Grossmann. Modelling and computational
    techniques for logic based integer programming. *Computers & Chemical Engineering*,
    18(7):563â€“578, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh etÂ al. (2022) A.Â Ramesh, P.Â Dhariwal, A.Â Nichol, C.Â Chu, and M.Â Chen.
    Hierarchical text-conditional image generation with CLIP latents. *arXiv:2204.06125*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robbins and Monro (1951) H.Â Robbins and S.Â Monro. A stochastic approximation
    method. *The Annals of Mathematical Statistics*, 22(3):400â€“407, 1951.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robinson etÂ al. (2019) H.Â Robinson, A.Â Rasheed, and O.Â San. Dissecting deep
    neural networks. *arXiv:1910.03879*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolnick and Kording (2020) D.Â Rolnick and K.Â Kording. Reverse-engineering deep
    ReLU networks. In *International Conference on Machine Learning (ICML)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenblatt (1957) F.Â Rosenblatt. The Perceptron â€” a perceiving and recognizing
    automaton. Technical Report 85-460-1, Cornell Aeronautical Laboratory, 1957.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RÃ¶ssig and Petkovic (2021) A.Â RÃ¶ssig and M.Â Petkovic. Advances in verification
    of relu neural networks. *Journal of Global Optimization*, 81:109â€“152, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roth (2021) K.Â Roth. A primer on multi-neuron relaxation-based adversarial robustness
    certification. In *ICML 2021 Workshop on Adversarial Machine Learning*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy etÂ al. (1993) A.Â Roy, L.Â S. Kim, and S.Â Mukhopadhyay. A polynomial time
    algorithm for the construction and training of a class of multilayer perceptrons.
    *Neural Networks*, 6(4):535â€“545, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rubies-Royo etÂ al. (2019) V.Â Rubies-Royo, R.Â Calandra, D.Â M. Stipanovic, and
    C.Â Tomlin. Fast neural network verification via shadow prices. *arXiv:1902.07247*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart etÂ al. (1986) D.Â E. Rumelhart, G.Â E. Hinton, and R.Â J. Williams. Learning
    representations by back-propagating errors. *Nature*, 323:533â€“536, 1986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ryu etÂ al. (2020) M.Â Ryu, Y.Â Chow, R.Â Anderson, C.Â Tjandraatmadja, and C.Â Boutilier.
    Caql: Continuous action q-learning. In *International Conference on Learning Representations
    (ICLR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sahiner etÂ al. (2021) A.Â Sahiner, T.Â Ergen, J.Â M. Pauly, and M.Â Pilanci. Vector-output
    re{lu} neural network problems are copositive programs: Convex analysis of two
    layer networks and polynomial-time algorithms. In *International Conference on
    Learning Representations (ICLR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salman etÂ al. (2019) H.Â Salman, G.Â Yang, H.Â Zhang, C.-J. Hsieh, and P.Â Zhang.
    A convex relaxation barrier to tight robustness verification of neural networks.
    *Neural Information Processing Systems (NeurIPS)*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler etÂ al. (2018) M.Â Sandler, A.Â Howard, M.Â Zhu, A.Â Zhmoginov, and L.-C.
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In *Conference on
    Computer Vision and Pattern Recognition (CVPR)*, pages 4510â€“4520, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sattelberg etÂ al. (2020) B.Â Sattelberg, R.Â Cavalieri, M.Â Kirby, C.Â Peterson,
    and R.Â Beveridge. Locally linear attributes of ReLU neural networks. *arXiv:2012.01940*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say etÂ al. (2017) B.Â Say, G.Â Wu, Y.Â Q. Zhou, and S.Â Sanner. Nonlinear hybrid
    planning with deep net learned transition models and mixed-integer linear programming.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    750â€“756, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber (2015) J.Â Schmidhuber. Deep learning in neural networks: An overview.
    *Neural Networks*, 61:85â€“117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schumann etÂ al. (2003) J.Â Schumann, P.Â Gupta, and S.Â Nelson. On verification
    & validation of neural network based controllers. In *Engineering Applications
    of Neural Networks (EANN)*, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwan etÂ al. (2022) R.Â Schwan, C.Â N. Jones, and D.Â Kuhn. Stability verification
    of neural network controllers using mixed-integer programming. *arXiv:2206.13374*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schweidtmann and Mitsos (2019) A.Â M. Schweidtmann and A.Â Mitsos. Deterministic
    global optimization with artificial neural networks embedded. *Journal of Optimization
    Theory and Applications*, 180(3):925â€“948, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schweidtmann etÂ al. (2022) A.Â M. Schweidtmann, J.Â M. Weber, C.Â Wende, L.Â Netze,
    and A.Â Mitsos. Obey validity limits of data-driven models through topological
    data analysis and one-class classification. *Optimization and Engineering*, 23(2):855â€“876,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seck etÂ al. (2021) I.Â Seck, G.Â Loosli, and S.Â Canu. Linear program powered attack.
    In *International Joint Conference on Neural Networks (IJCNN)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra (2020) T.Â Serra. Enumerative branching with less repetition. In *International
    Conference on Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, pages 399â€“416\. Springer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra and Hooker (2020) T.Â Serra and J.Â Hooker. Compact representation of near-optimal
    integer programming solutions. *Mathematical Programming*, 182:199â€“232, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra and Ramalingam (2020) T.Â Serra and S.Â Ramalingam. Empirical bounds on
    linear regions of deep rectifier networks. In *AAAI Conference on Artificial Intelligence*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra etÂ al. (2018) T.Â Serra, C.Â Tjandraatmadja, and S.Â Ramalingam. Bounding
    and counting linear regions of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra etÂ al. (2020) T.Â Serra, A.Â Kumar, and S.Â Ramalingam. Lossless compression
    of deep neural networks. In *International Conference on the Integration of Constraint
    Programming, Artificial Intelligence, and Operations Research (CPAIOR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra etÂ al. (2021) T.Â Serra, X.Â Yu, A.Â Kumar, and S.Â Ramalingam. Scaling up
    exact neural network compression by ReLU stability. In *Neural Information Processing
    Systems (NeurIPS)*, volumeÂ 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi etÂ al. (2022) C.Â Shi, M.Â Emadikhiav, L.Â Lozano, and D.Â Bergman. Careful!
    training relevance is real. *arXiv:2201.04429*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sidrane etÂ al. (2022) C.Â Sidrane, A.Â Maleki, A.Â Irfan, and M.Â J. Kochenderfer.
    Overt: An algorithm for safety verification of neural network control policies
    for nonlinear systems. *Journal of Machine Learning Research*, 23(117):1â€“45, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sildir and Aydin (2022) H.Â Sildir and E.Â Aydin. A mixed-integer linear programming
    based training and feature selection method for artificial neural networks using
    piece-wise linear approximations. *Chemical Engineering Science*, 249:117273,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver etÂ al. (2017) D.Â Silver, J.Â Schrittwieser, K.Â Simonyan, I.Â Antonoglou,
    A.Â Huang, A.Â Guez, T.Â Hubert, L.Â Baker, M.Â Lai, A.Â Bolton, Y.Â Chen, T.Â Lillicrap,
    F.Â Hui, L.Â Sifre, G.Â vanÂ den Driessche, T.Â Graepel, and D.Â Hassabis. Mastering
    the game of go without human knowledge. *Nature*, 550:354â€“359, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh etÂ al. (2018) G.Â Singh, T.Â Gehr, M.Â Mirman, M.Â PÃ¼schel, and M.Â Vechev.
    Fast and effective robustness certification. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh etÂ al. (2019a) G.Â Singh, R.Â Ganvir, M.Â PÃ¼schel, and M.Â Vechev. Beyond
    the single neuron convex barrier for neural network certification. *Neural Information
    Processing Systems (NeurIPS)*, 32, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh etÂ al. (2019b) G.Â Singh, T.Â Gehr, M.Â PÃ¼schel, and M.Â Vechev. An abstract
    domain for certifying neural networks. *Proceedings of the ACM on Programming
    Languages (POPL)*, 3:1â€“30, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh etÂ al. (2021) H.Â Singh, M.Â P. Kumar, P.Â Torr, and K.Â D. Dvijotham. Overcoming
    the convex barrier for simplex inputs. In *Neural Information Processing Systems
    (NeurIPS)*, volumeÂ 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith and Winkler (2006) J.Â E. Smith and R.Â L. Winkler. The optimizerâ€™s curse:
    Skepticism and postdecision surprise in decision analysis. *Management Science*,
    52(3):311â€“322, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava etÂ al. (2014) N.Â Srivastava, G.Â Hinton, A.Â Krizhevsky, I.Â Sutskever,
    and R.Â Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting.
    *Journal of Machine Learning Research*, 15(56):1929â€“1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong etÂ al. (2021) C.Â A. Strong, H.Â Wu, A.Â ZeljiÄ‡, K.Â D. Julian, G.Â Katz,
    C.Â Barrett, and M.Â J. Kochenderfer. Global optimization of objective functions
    represented by ReLU networks. *Machine Learning*, pages 1â€“28, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strong etÂ al. (2022) C.Â A. Strong, S.Â M. Katz, A.Â L. Corso, and M.Â J. Kochenderfer.
    ZoPE: a fast optimizer for ReLU networks with low-dimensional inputs. In *NASA
    Formal Methods: 14th International Symposium, (NFM)*, pages 299â€“317\. Springer,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sudjianto etÂ al. (2020) A.Â Sudjianto, W.Â Knauth, R.Â Singh, Z.Â Yang, and A.Â Zhang.
    Unwrapping the black box of deep ReLU networks: Interpretability, diagnostics,
    and simplification. *arXiv:2011.04041*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever etÂ al. (2013) I.Â Sutskever, J.Â Martens, G.Â Dahl, and G.Â Hinton. On
    the importance of initialization and momentum in deep learning. In *International
    Conference on Machine Learning (ICML)*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever etÂ al. (2014) I.Â Sutskever, O.Â Vinyals, and Q.Â Le. Sequence to sequence
    learning with neural networks. In *Neural Information Processing Systems (NeurIPS)*,
    volumeÂ 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy etÂ al. (2014) C.Â Szegedy, W.Â Zaremba, I.Â Sutskever, J.Â Bruna, D.Â Erhan,
    I.Â Goodfellow, and R.Â Fergus. Intriguing properties of neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy etÂ al. (2015) C.Â Szegedy, W.Â Liu, Y.Â Jia, P.Â Sermanet, S.Â Reed, D.Â Anguelov,
    D.Â Erhan, V.Â Vanhoucke, and A.Â Rabinovich. Going deeper with convolutions. In
    *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takai etÂ al. (2021) Y.Â Takai, A.Â Sannai, and M.Â Cordonnier. On the number of
    linear functions composing deep neural network: Towards a refined definition of
    neural networks complexity. In *International Conference on Artificial Intelligence
    and Statistics (AISTATS)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao etÂ al. (2022) Q.Â Tao, L.Â Li, X.Â Huang, X.Â Xi, S.Â Wang, and J.Â A. Suykens.
    Piecewise linear neural networks and deep learning. *Nature Reviews Methods Primers*,
    2, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telgarsky (2015) M.Â Telgarsky. Representation benefits of deep feedforward networks.
    *arXiv:1509.08101*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorbjarnarson and Yorke-Smith (2020) T.Â Thorbjarnarson and N.Â Yorke-Smith.
    On training neural networks with mixed integer programming. *arXiv:2009.03825*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorbjarnarson and Yorke-Smith (2023) T.Â Thorbjarnarson and N.Â Yorke-Smith.
    Optimal training of integer-valued neural networks with mixed integer programming.
    *PLOS One*, 18(2):e0261029, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tiwari and Konidaris (2022) S.Â Tiwari and G.Â Konidaris. Effects of data geometry
    in early deep learning. In *Neural Information Processing Systems (NeurIPS)*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tjandraatmadja etÂ al. (2020) C.Â Tjandraatmadja, R.Â Anderson, J.Â Huchette, W.Â Ma,
    K.Â K. Patel, and J.Â P. Vielma. The convex relaxation barrier, revisited: Tightened
    single-neuron relaxations for neural network verification. *Neural Information
    Processing Systems (NeurIPS)*, 33:21675â€“21686, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tjeng etÂ al. (2019) V.Â Tjeng, K.Â Xiao, and R.Â Tedrake. Evaluating robustness
    of neural networks with mixed integer programming. In *International Conference
    on Learning Representations (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trimmel etÂ al. (2021) M.Â Trimmel, H.Â Petzka, and C.Â Sminchisescu. TropEx: An
    algorithm for extracting linear terms in deep neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsay and Baldea (2019) C.Â Tsay and M.Â Baldea. 110th anniversary: using data
    to bridge the time and length scales of process systems. *Industrial & Engineering
    Chemistry Research*, 58(36):16696â€“16708, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsay etÂ al. (2021) C.Â Tsay, J.Â Kronqvist, A.Â Thebelt, and R.Â Misener. Partition-based
    formulations for mixed-integer optimization of trained ReLU neural networks. In
    *Neural Information Processing Systems (NeurIPS)*, volumeÂ 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tseran and MontÃºfar (2021) H.Â Tseran and G.Â MontÃºfar. On the expected complexity
    of maxout networks. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 34,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unser (2019) M.Â Unser. A representer theorem for deep neural networks. *Journal
    of Machine Learning Research*, 20:1â€“30, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani etÂ al. (2017) A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones,
    A.Â N. Gomez, L.Â Kaiser, and I.Â Polosukhin. Attention is all you need. In *Neural
    Information Processing Systems (NeurIPS)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vielma (2015) J.Â P. Vielma. Mixed integer linear programming formulation techniques.
    *SIAM Review*, 57(1):3â€“57, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vielma (2019) J.Â P. Vielma. Small and strong formulations for unions of convex
    sets from the cayley embedding. *Mathematical Programming*, 177(1-2):21â€“53, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vielma etÂ al. (2010) J.Â P. Vielma, S.Â Ahmed, and G.Â Nemhauser. Mixed-integer
    models for nonseparable piecewise-linear optimization: Unifying framework and
    extensions. *Operations Research*, 58(2):303â€“315, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villani and Schoots (2023) M.Â J. Villani and N.Â Schoots. Any deep ReLU network
    is shallow. *arXiv:2306.11827*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vincent and Schwager (2021) J.Â A. Vincent and M.Â Schwager. Reachable polyhedral
    marching (RPM): A safety verification algorithm for robotic systems with deep
    neural network components. In *IEEE International Conference on Robotics and Automation
    (ICRA)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals etÂ al. (2017) O.Â Vinyals, T.Â Ewalds, S.Â Bartunov, P.Â Georgiev, A.Â S.
    Vezhnevets, M.Â Yeo, A.Â Makhzani, H.Â KÃ¼ttler, J.Â Agapiou, J.Â Schrittwieser, J.Â Quan,
    S.Â Gaffney, S.Â Petersen, K.Â Simonyan, T.Â Schaul, H.Â van Hasselt, D.Â Silver, T.Â Lillicrap,
    K.Â Calderone, P.Â Keet, A.Â Brunasso, D.Â Lawrence, A.Â Ekermo, J.Â Repp, and R.Â Tsing.
    StarCraft II: A new challenge for reinforcement learning. *arXiv:1708.04782*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Virmaux and Scaman (2018) A.Â Virmaux and K.Â Scaman. Lipschitz regularity of
    deep neural networks: analysis and efficient estimation. In *Neural Information
    Processing Systems (NeurIPS)*, volumeÂ 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volpp etÂ al. (2020) M.Â Volpp, L.Â P. FrÃ¶hlich, K.Â Fischer, A.Â Doerr, S.Â Falkner,
    F.Â Hutter, and C.Â Daniel. Meta-learning acquisition functions for transfer learning
    in bayesian optimization. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2021) K.Â Wang, L.Â Lozano, D.Â Bergman, and C.Â Cardonha. A two-stage
    exact algorithm for optimization of neural network ensemble. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2023) K.Â Wang, L.Â Lozano, C.Â Cardonha, and D.Â Bergman. Optimizing
    over an ensemble of trained neural networks. *INFORMS Journal on Computing*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2018a) S.Â Wang, K.Â Pei, J.Â Whitehouse, J.Â Yang, and S.Â Jana. Efficient
    formal safety analysis of neural networks. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang etÂ al. (2018b) S.Â Wang, K.Â Pei, J.Â Whitehouse, J.Â Yang, and S.Â Jana. Formal
    security analysis of neural networks using symbolic intervals. In *27th $\{$USENIX$\}$
    Security Symposium ($\{$USENIX$\}$ Security 18)*, pages 1599â€“1614, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang (2022) Y.Â Wang. Estimation and comparison of linear regions for relu networks.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng etÂ al. (1992) J.Â Weng, N.Â Ahuja, and T.Â Huang. Cresceptron: a self-organizing
    neural network which grows adaptively. In *International Joint Conference on Neural
    Networks (IJCNN)*, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weng etÂ al. (2018) L.Â Weng, H.Â Zhang, H.Â Chen, Z.Â Song, C.-J. Hsieh, L.Â Daniel,
    D.Â Boning, and I.Â Dhillon. Towards fast computation of certified robustness for
    ReLU networks. In *International Conference on Machine Learning (ICML)*, pages
    5276â€“5285, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Werbos (1974) P.Â Werbos. *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences*. PhD thesis, Harvard University, 1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wicker etÂ al. (2020) M.Â Wicker, L.Â Laurenti, A.Â Patane, and M.Â Kwiatkowska.
    Probabilistic safety for Bayesian neural networks. In *Conference on Uncertainty
    in Artificial Intelligence (UAI)*, pages 1198â€“1207, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wicker etÂ al. (2022) M.Â Wicker, J.Â Heo, L.Â Costabello, and A.Â Weller. Robust
    explanation constraints for neural networks. *arXiv:2212.08507*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilhelm etÂ al. (2022) M.Â E. Wilhelm, C.Â Wang, and M.Â D. Stuber. Convex and concave
    envelopes of artificial neural network activation functions for deterministic
    global optimization. *Journal of Global Optimization*, pages 1â€“26, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong and Kolter (2018) E.Â Wong and Z.Â Kolter. Provable defenses against adversarial
    examples via the convex outer adversarial polytope. In *International Conference
    on Machine Learning (ICML)*, pages 5286â€“5295, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong etÂ al. (2018) E.Â Wong, F.Â Schmidt, J.Â H. Metzen, and J.Â Z. Kolter. Scaling
    provable adversarial defenses. *Neural Information Processing Systems (NeurIPS)*,
    31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wright (2018) S.Â J. Wright. Optimization algorithms for data analysis. *The
    Mathematics of Data*, 25:49, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu etÂ al. (2020) G.Â Wu, B.Â Say, and S.Â Sanner. Scalable planning with deep neural
    network learned transition models. *Journal of Artificial Intelligence Research*,
    68:571â€“606, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu etÂ al. (2022) H.Â Wu, A.Â ZeljiÄ‡, G.Â Katz, and C.Â Barrett. Efficient neural
    network analysis with sum-of-infeasibilities. In *Tools and Algorithms for the
    Construction and Analysis of Systems (TACAS)*, pages 143â€“163\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang etÂ al. (2017) W.Â Xiang, H.-D. Tran, and T.Â T. Johnson. Reachable set computation
    and safety verification for neural networks with ReLU activations. *arXiv:1712.08163*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao etÂ al. (2019) K.Â Xiao, V.Â Tjeng, N.Â Shafiullah, and A.Â Madry. Training
    for faster adversarial robustness verification via inducing ReLU stability. *International
    Conference on Learning Representations (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie etÂ al. (2020a) J.Â Xie, Z.Â Shen, C.Â Zhang, B.Â Wang, and H.Â Qian. Efficient
    projection-free online methods with stochastic recursive gradient. In *AAAI Conference
    on Artificial Intelligence*, volumeÂ 34, pages 6446â€“6453, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie etÂ al. (2020b) Q.Â Xie, M.-T. Luong, E.Â Hovy, and Q.Â V. Le. Self-training
    with noisy student improves ImageNet classification. In *Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie etÂ al. (2020c) Y.Â Xie, G.Â Chen, and Q.Â Li. A general computational framework
    to measure the expressiveness of complex networks using a tighter upper bound
    of linear regions. *arXiv:2012.04428*, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiong etÂ al. (2020) H.Â Xiong, L.Â Huang, M.Â Yu, L.Â Liu, F.Â Zhu, and L.Â Shao.
    On the number of linear regions of convolutional neural networks. In *International
    Conference on Machine Learning (ICML)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu etÂ al. (2022) S.Â Xu, J.Â Vaughan, J.Â Chen, A.Â Zhang, and A.Â Sudjianto. Traversing
    the local polytopes of ReLU neural networks. In *AAAI Workshop AdvML*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang etÂ al. (2022) D.Â Yang, P.Â Balaprakash, and S.Â Leyffer. Modeling design
    and control problems involving neural network surrogates. *Computational Optimization
    and Applications*, pages 1â€“42, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang etÂ al. (2020) X.Â Yang, H.-D. Tran, W.Â Xiang, and T.Â Johnson. Reachability
    analysis for feed-forward neural networks using face lattices. *arXiv:2003.01226*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang etÂ al. (2021) X.Â Yang, T.Â Yamaguchi, H.-D. Tran, B.Â Hoxha, T.Â T. Johnson,
    and D.Â Prokhorov. Reachability analysis of convolutional neural networks. *arXiv:2106.12074*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yarotsky (2017) D.Â Yarotsky. Error bounds for approximations with deep ReLU
    networks. *Neural Networks*, 94, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zakrzewski (2001) R.Â R. Zakrzewski. Verification of a trained neural network
    accuracy. In *International Joint Conference on Neural Networks (IJCNN)*, volumeÂ 3,
    pages 1657â€“1662\. IEEE, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaslavsky (1975) T.Â Zaslavsky. *Facing Up to Arrangements: Face-Count Formulas
    for Partitions of Space by Hyperplanes*. American Mathematical Society, 1975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2023) A.Â Zhang, Z.Â C. Lipton, M.Â Li, and A.Â J. Smola. *Dive into
    Deep Learning*. 2023. https://d2l.ai.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2018a) H.Â Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L.Â Daniel.
    Efficient neural network robustness certification with general activation functions.
    *Neural Information Processing Systems (NeurIPS)*, 31, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2020) H.Â Zhang, H.Â Chen, C.Â Xiao, S.Â Gowal, R.Â Stanforth, B.Â Li,
    D.Â Boning, and C.-J. Hsieh. Towards stable and efficient training of verifiably
    robust neural networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2022) H.Â Zhang, S.Â Wang, K.Â Xu, L.Â Li, B.Â Li, S.Â Jana, C.-J. Hsieh,
    and J.Â Z. Kolter. General cutting planes for bound-propagation-based neural network
    verification. In *Neural Information Processing Systems (NeurIPS)*, volumeÂ 35,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang etÂ al. (2018b) L.Â Zhang, G.Â Naitzat, and L.-H. Lim. Tropical geometry
    of deep neural networks. In *International Conference on Machine Learning (ICML)*,
    2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang (2020) R.Â Zhang. On the tightness of semidefinite relaxations for certifying
    robustness to adversarial examples. *Neural Information Processing Systems (NeurIPS)*,
    33:3808â€“3820, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Wu (2020) X.Â Zhang and D.Â Wu. Empirical studies on the properties
    of linear regions in deep neural networks. In *International Conference on Learning
    Representations (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao etÂ al. (2023) S.Â Zhao, C.Â Tsay, and J.Â Kronqvist. Model-based feature
    selection for neural networks: A mixed-integer programming approach. *arXiv:2302.10344*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou and Schoellig (2019) S.Â Zhou and A.Â P. Schoellig. An analysis of the expressiveness
    of deep neural network architectures based on their Lipschitz constants. *arXiv:1912.11511*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu etÂ al. (2020) R.Â Zhu, B.Â Lin, and H.Â Tang. Bounding the number of linear
    regions in local area for neural networks with ReLU activations. *arXiv:2007.06803*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou etÂ al. (2019) D.Â Zou, R.Â Balan, and M.Â Singh. On Lipschitz bounds of general
    convolutional neural networks. *IEEE Transactions on Information Theory*, 66(3):1738â€“1759,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
