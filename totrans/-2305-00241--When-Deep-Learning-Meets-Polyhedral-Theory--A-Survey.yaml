- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:40:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:40:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.00241] When Deep Learning Meets Polyhedral Theory: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.00241] 当深度学习遇上多面体理论：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.00241](https://ar5iv.labs.arxiv.org/html/2305.00241)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.00241](https://ar5iv.labs.arxiv.org/html/2305.00241)
- en: 'When Deep Learning Meets Polyhedral Theory: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当深度学习遇上多面体理论：综述
- en: Joey Huchette
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Joey Huchette
- en: Google Research, USA    Gonzalo Muñoz
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌研究，美国    冈萨洛·穆尼奥斯
- en: Universidad de O’Higgins, Chile    Thiago Serra
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 奥伊金斯大学，智利    迭戈·塞拉
- en: Bucknell University, USA    Calvin Tsay
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 巴克内尔大学，美国    卡尔文·蔡
- en: Imperial College London, UK(September 2023)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦帝国学院，英国（2023年9月）
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In the past decade, deep learning became the prevalent methodology for predictive
    modeling thanks to the remarkable accuracy of deep neural networks in tasks such
    as computer vision and natural language processing. Meanwhile, the structure of
    neural networks converged back to simpler representations based on piecewise constant
    and piecewise linear functions such as the Rectified Linear Unit (ReLU), which
    became the most commonly used type of activation function in neural networks.
    That made certain types of network structure —such as the typical fully-connected
    feedforward neural network— amenable to analysis through polyhedral theory and
    to the application of methodologies such as Linear Programming (LP) and Mixed-Integer
    Linear Programming (MILP) for a variety of purposes. In this paper, we survey
    the main topics emerging from this fast-paced area of work, which brings a fresh
    perspective to understanding neural networks in more detail as well as to applying
    linear optimization techniques to train, verify, and reduce the size of such networks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年里，深度学习由于深度神经网络在计算机视觉和自然语言处理等任务中的显著准确性，成为预测建模的主要方法。同时，神经网络的结构回归到基于分段常数和分段线性函数的简单表示，例如整流线性单元（ReLU），这成为神经网络中最常用的激活函数。这使得某些类型的网络结构——例如典型的全连接前馈神经网络——能够通过多面体理论进行分析，并应用线性规划（LP）和混合整数线性规划（MILP）等方法用于各种目的。在本文中，我们回顾了这一快速发展的领域中出现的主要主题，这为更详细地理解神经网络以及应用线性优化技术来训练、验证和缩减这些网络的规模提供了全新的视角。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Deep learning has continuously achieved new landmarks in varied areas of artificial
    intelligence for the past decade. Examples of those areas include predictive tasks
    in computer vision (Krizhevsky et al., [2012](#bib.bib178), Ciresan et al., [2012](#bib.bib60),
    Szegedy et al., [2015](#bib.bib301), He et al., [2016](#bib.bib144), Xie et al.,
    [2020b](#bib.bib343)), natural language processing (Sutskever et al., [2014](#bib.bib299),
    Peters et al., [2018](#bib.bib246), Radford et al., [2018](#bib.bib252), Devlin
    et al., [2019](#bib.bib80)), and speech recognition (Hinton et al., [2012](#bib.bib148),
    Graves and Jaitly, [2014](#bib.bib130), Park et al., [2019](#bib.bib240)). The
    artificial neural networks behind such feats are being used in many applications,
    and there is a growing interest for analytical insights to help design such networks
    and then to leverage the model that they have learned. For the most commonly used
    types of neural networks, some of those results and methods are coming from operations
    research tools such as polyhedral theory and associated optimization techniques
    such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP). Among
    other things, these connections with mathematical optimization may help us understand
    what neural networks can represent, how to train them, and how to make them more
    compact. For example, consider the popular task of classifying images (Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")); polyhedral theory and associated optimization techniques may help
    us answer questions such as the following. How should we train the classifier
    model? How large should it be? How robust to perturbations is it?'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '在过去十年里，深度学习在人工智能的各个领域不断取得新进展。这些领域的例子包括计算机视觉中的预测任务（Krizhevsky 等，[2012](#bib.bib178)，Ciresan
    等，[2012](#bib.bib60)，Szegedy 等，[2015](#bib.bib301)，He 等，[2016](#bib.bib144)，Xie
    等，[2020b](#bib.bib343)），自然语言处理（Sutskever 等，[2014](#bib.bib299)，Peters 等，[2018](#bib.bib246)，Radford
    等，[2018](#bib.bib252)，Devlin 等，[2019](#bib.bib80)），以及语音识别（Hinton 等，[2012](#bib.bib148)，Graves
    和 Jaitly，[2014](#bib.bib130)，Park 等，[2019](#bib.bib240)）。这些成就背后的人工神经网络被广泛应用，并且对分析洞察的兴趣不断增长，以帮助设计这些网络并利用它们所学到的模型。对于最常用的神经网络类型，这些结果和方法有些来源于运筹学工具，如多面体理论及相关的优化技术，如线性规划（LP）和混合整数线性规划（MILP）。这些与数学优化的联系可能帮助我们深入了解神经网络可以表示的内容、如何训练它们以及如何使其更加紧凑。例如，考虑分类图像这一常见任务（图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey)）；多面体理论及相关的优化技术可能帮助我们解答以下问题。我们应该如何训练分类器模型？它应该有多大？对扰动的鲁棒性如何？'
- en: '![Refer to caption](img/17c4f1fa42e56b26a18a8a37fef55a33.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/17c4f1fa42e56b26a18a8a37fef55a33.png)'
- en: 'Figure 1: Example classification task on the MNIST database of handwritten
    digits, in which the image of a handwritten digit is given as input and the probability
    of that digit being from each possible class is provided as output.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MNIST 手写数字数据库中的分类任务示例，其中输入为手写数字的图像，输出为该数字属于每个可能类别的概率。
- en: 1.1 What neural networks can model
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 神经网络可以建模的内容
- en: 'We can essentially think of artificial neural networks as functions mapping
    an input ${\bm{x}}$ from a given domain to an output ${\bm{y}}$ for a given application.
    For the classification task in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    When Deep Learning Meets Polyhedral Theory: A Survey"), inputs ${\bm{x}}$ correspond
    to images from the dataset, and ${\bm{y}}$ to the associated predicted labels,
    or probabilities for labels describing the content of those images. The basic
    units of neural networks mimic biological neurons in that they receive inputs
    from adjacent units, transform those inputs, and may produce an output to subsequent
    units of the network. In other words, every unit is also a function, and in fact
    the output of most units is defined by the composition of a nonlinear function
    with a linear function. The nonlinear function is often denoted as the *activation
    function* in analogy to how a biological neuron is triggered to send a signal
    to adjacent neurons when the stimulus caused by the input exceeds a certain activation
    threshold. Such non-linearity is behind the remarkable expressiveness of neural
    networks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以基本上将人工神经网络视为将输入${\bm{x}}$从给定领域映射到输出${\bm{y}}$的函数，用于特定应用。在图[1](#S1.F1 "图
    1 ‣ 1 介绍 ‣ 深度学习与多面体理论的结合")中的分类任务中，输入${\bm{x}}$对应于数据集中的图像，而${\bm{y}}$对应于与这些图像内容相关的预测标签或标签概率。神经网络的基本单元模拟生物神经元，因为它们接收来自相邻单元的输入，转换这些输入，并可能将输出传递给网络的后续单元。换句话说，每个单元也是一个函数，事实上，大多数单元的输出是通过将非线性函数与线性函数组合来定义的。非线性函数通常被称为*激活函数*，类似于生物神经元在输入的刺激超过某一激活阈值时被触发以向相邻神经元发送信号的方式。这种非线性是神经网络卓越表达能力的基础。
- en: This model was pioneered by McCulloch and Pitts ([1943](#bib.bib217)), who considered
    a thresholding function for activation that is now often denoted as the Linear
    Threshold Unit (LTU). That activation is also the basis of the classic *perceptron*
    algorithm by  Rosenblatt ([1957](#bib.bib261)), which yields a binary classifier
    of the form
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一模型由麦卡洛克和皮茨（[1943](#bib.bib217)）首次提出，他们考虑了一种激活的阈值函数，现在通常称为线性阈值单元（LTU）。该激活函数也是经典的*感知器*算法的基础，由罗森布拉特（[1957](#bib.bib261)）提出，该算法生成一个形式为二分类器
- en: '|  | $f({\bm{x}})=\left\{\begin{array}[]{cl}1&amp;\text{if }{\bm{w}}\cdot{\bm{x}}+b>0;\\
    0&amp;\text{otherwise}\end{array}\right.$ |  | (1) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | $f({\bm{x}})=\left\{\begin{array}[]{cl}1&amp;\text{如果 }{\bm{w}}\cdot{\bm{x}}+b>0;\\
    0&amp;\text{否则}\end{array}\right.$ |  | (1) |'
- en: 'for an input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and with parameters ${\bm{w}}\in\mathbb{R}^{n_{0}}$
    and $b\in\mathbb{R}$. Those parameters are chosen by optimizing the predictions
    for a given task, as discussed below and in Section [5](#S5 "5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey"). The term *single-layer perceptron* is used for a neural network consisting
    of a set of such units processing the same input in parallel. The term *multi-layer
    perceptron* is used for a generalization of this concept, by which the output
    of a *layer* —a set of units with same input— is the input for a subsequent layer.
    This perceptron terminology has also been loosely applied to neural networks with
    other activation functions.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入${\bm{x}}\in\mathbb{R}^{n_{0}}$，以及参数${\bm{w}}\in\mathbb{R}^{n_{0}}$和$b\in\mathbb{R}$。这些参数是通过优化给定任务的预测来选择的，如下文和第[5](#S5
    "5 线性规划与多面体理论在训练中的应用 ‣ 深度学习与多面体理论的结合：调查")节所讨论的。*单层感知器*这一术语用于描述由一组并行处理相同输入的单元组成的神经网络。*多层感知器*这一术语用于描述该概念的推广，其中一个*层*——一组具有相同输入的单元——的输出是后续层的输入。这种感知器术语也被宽泛地应用于具有其他激活函数的神经网络。
- en: More generally, neural networks that successively transform inputs through an
    ordered sequence of layers are also denoted *feedforward networks*. The layers
    that do not produce the final output of the neural network are denoted *hidden
    layers*. For a network with $L$ layers, we denote $n_{l}$ as the number of units
    —or *width*— of layer $l\in{\mathbb{L}}:=\{1,2,\ldots,L\}$ and $h_{i}^{l}$ as
    the output of the $i$-th unit in layer $l$, where $i\in\{1,2,\ldots,n_{l}\}$.
    The output of a unit is given by
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，通过一系列有序层依次转换输入的神经网络也称为*前馈网络*。那些不产生神经网络最终输出的层称为*隐层*。对于一个有$L$层的网络，我们将$n_{l}$表示为第$l$层的单元数量——或*宽度*——$l\in{\mathbb{L}}:=\{1,2,\ldots,L\}$，$h_{i}^{l}$表示第$l$层中第$i$个单元的输出，其中$i\in\{1,2,\ldots,n_{l}\}$。一个单元的输出由以下公式给出：
- en: '|  | $h_{i}^{l}=\sigma^{l}\left({\bm{w}}^{l}_{i}\cdot{\bm{h}}^{l-1}+b^{l}_{i}\right),$
    |  | (2) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{i}^{l}=\sigma^{l}\left({\bm{w}}^{l}_{i}\cdot{\bm{h}}^{l-1}+b^{l}_{i}\right),$
    |  | (2) |'
- en: 'where the *weights* ${\bm{w}}^{l}_{i}\in\mathbb{R}^{n_{l-1}}$ and the *bias*
    $b^{l}_{i}\in\mathbb{R}$ are parameters of the unit. Those parameters can be aggregated
    across the layer as the matrix ${\bm{W}}^{l}\in\mathbb{R}^{n_{l}\times n_{l-1}}$
    and the vector ${\bm{b}}^{l}\in\mathbb{R}^{n_{l}}$. The vector ${\bm{h}}^{l-1}\in\mathbb{R}^{n_{l-1}}$
    represents the aggregated outputs from layer $(l-1)$. The activation function
    $\sigma^{l}:\mathbb{R}\rightarrow\mathbb{R}$ is applied by the units in layer
    $l$. These definitions implicitly assume that $n_{0}$ is the size of the network
    input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and that ${\bm{h}}^{0}$ and ${\bm{x}}$ are
    the same. Figure [2](#S1.F2 "Figure 2 ‣ 1.1 What neural networks can model ‣ 1
    Introduction ‣ When Deep Learning Meets Polyhedral Theory: A Survey") illustrates
    the operation of a feedforward network as described above.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，*权重* ${\bm{w}}^{l}_{i}\in\mathbb{R}^{n_{l-1}}$ 和 *偏置* $b^{l}_{i}\in\mathbb{R}$
    是该单元的参数。这些参数可以在层内汇总为矩阵 ${\bm{W}}^{l}\in\mathbb{R}^{n_{l}\times n_{l-1}}$ 和向量 ${\bm{b}}^{l}\in\mathbb{R}^{n_{l}}$。向量
    ${\bm{h}}^{l-1}\in\mathbb{R}^{n_{l-1}}$ 表示来自层 $(l-1)$ 的汇总输出。激活函数 $\sigma^{l}:\mathbb{R}\rightarrow\mathbb{R}$
    由第 $l$ 层的单元应用。这些定义隐含地假设 $n_{0}$ 是网络输入 ${\bm{x}}\in\mathbb{R}^{n_{0}}$ 的大小，并且 ${\bm{h}}^{0}$
    和 ${\bm{x}}$ 是相同的。图 [2](#S1.F2 "Figure 2 ‣ 1.1 What neural networks can model
    ‣ 1 Introduction ‣ When Deep Learning Meets Polyhedral Theory: A Survey") 说明了上述前馈网络的操作。'
- en: '![Refer to caption](img/2db8fd387b48da880f36dff11bad8a69.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2db8fd387b48da880f36dff11bad8a69.png)'
- en: 'Figure 2: Mapping from ${\bm{x}}\in\mathbb{R}^{n_{0}}$ to ${\bm{y}}\in\mathbb{R}^{n_{L}}$
    through a feedforward neural network with $L$ layers, layer widths $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and activation functions $\{\sigma_{l}\}_{l\in{\mathbb{L}}}$.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 通过具有 $L$ 层、层宽度 $\{n_{l}\}_{l\in{\mathbb{L}}}$ 和激活函数 $\{\sigma_{l}\}_{l\in{\mathbb{L}}}$
    的前馈神经网络，将 ${\bm{x}}\in\mathbb{R}^{n_{0}}$ 映射到 ${\bm{y}}\in\mathbb{R}^{n_{L}}$。'
- en: 1.2 How neural networks are obtained and evaluated
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 神经网络的获取与评估
- en: In resemblance to how other models for *supervised learning* in machine learning
    are obtained, we can *train* a neural network for a given task by adjusting its
    behavior with respect to the examples of a *training set* and then evaluate the
    final trained network on a *test set*. Both of these sets consist of inputs for
    which the correct output $\hat{y}$ is known. We can define an objective function
    to model a measure of distance between the output $y$ and the correct output $\hat{y}$,
    which is typically denoted as the *loss function*, and then iteratively update
    parameters such as $\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}}$ and $\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}$
    to minimize that loss function over the training set. A common objective function
    is the square error $\|y-\hat{y}\|^{2}$ summed over the points in the training
    set. The test set contains a separate collection of inputs and their outputs,
    which is used to evaluate the trained neural network with examples that were not
    seen during training. A good performance on the test set may indicate that the
    trained neural network is able to *generalize* beyond the seen examples, whereas
    a bad performance may suggest that it *overfits* for the training set. Neural
    networks also have *hyperparameters* that are often chosen manually and do not
    change during training, such as the *depth* $L$, the widths of the layers $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and the activation functions used in each layer $\{\sigma^{l}\}_{l\in{\mathbb{L}}}$.
    Different models can be produced by varying the hyperparameters. In such a case,
    a *validation set* disjoint from the training and test sets can be used to compare
    models with different hyperparameters. Whereas the validation set may serve as
    a benchmark to different trained models corresponding to different choices of
    hyperparameters, the test set can only be used to evaluate a single neural network
    chosen among those evaluated with the validation set. The emergent field of *neural
    architecture search* —recently surveyed by Elsken et al. ([2019](#bib.bib89))—
    concerns with automatically choosing such hyperparameters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与机器学习中其他*监督学习*模型的获取方式类似，我们可以通过调整神经网络的行为以适应*训练集*的例子来*训练*一个神经网络，然后在*测试集*上评估最终训练好的网络。这两个数据集都包含了已知正确输出$\hat{y}$的输入。我们可以定义一个目标函数来衡量输出$y$和正确输出$\hat{y}$之间的距离，这通常被称为*损失函数*，然后迭代更新参数，如$\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}}$和$\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}$，以最小化训练集上的损失函数。一个常见的目标函数是平方误差$\|y-\hat{y}\|^{2}$，它在训练集中的所有点上进行求和。测试集包含一组独立的输入及其输出，用于评估在训练过程中未见过的例子的训练神经网络。测试集上的良好表现可能表明训练好的神经网络能够*泛化*超越已见的例子，而差的表现可能表明它对训练集*过拟合*。神经网络还具有*超参数*，这些超参数通常是手动选择的，并且在训练过程中不会改变，例如*深度*
    $L$、层的宽度$\{n_{l}\}_{l\in{\mathbb{L}}}$和每层使用的激活函数$\{\sigma^{l}\}_{l\in{\mathbb{L}}}$。通过改变超参数可以产生不同的模型。在这种情况下，可以使用与训练集和测试集不同的*验证集*来比较具有不同超参数的模型。虽然验证集可以作为不同训练模型的基准，这些模型对应于不同的超参数选择，但测试集只能用于评估在验证集上评估的那些神经网络中的一个。新兴领域*神经架构搜索*——最近由Elsken等人([2019](#bib.bib89))进行的调研——涉及自动选择这些超参数。
- en: One of the key factors for the success of deep learning is that first-order
    methods for continuous optimization can be effectively applied to train deep networks.
    The interest in neural networks first vanished due to negative results in the
    Perceptrons book by Minsky and Papert ([1969](#bib.bib219)), which showed that
    single-layer perceptrons cannot represent functions such as the Boolean XOR. However,
    moving to multi-layer perceptrons capable of expressing the Boolean XOR as well
    as other more expressive models would require a clever training strategy. Hence,
    the interest was regained with papers that popularized the use of *backpropagation*,
    such as  Rumelhart et al. ([1986](#bib.bib266)) and  LeCun et al. ([1989](#bib.bib185)).
    Note that backpropagation was first discussed much earlier in the context of networks
    by Linnainmaa ([1970](#bib.bib194)) and of neural networks explicitly by Werbos
    ([1974](#bib.bib331)). The backpropagation algorithm calculates the derivative
    of the loss function with respect to each neural network parameter by applying
    the chain rule through the units of the neural network, which is considerably
    more efficient than explicitly evaluating the derivative of each network parameter.
    Consequently, neural networks are generally trained with gradient descent methods
    in which the parameters are updated sequentially from the output to the input
    layer in each step. In fact, most algorithms for training neural networks are
    based on Stochastic Gradient Descent (SGD), which is a form of the stochastic
    approximation through sampling pioneered by Robbins and Monro ([1951](#bib.bib258)).
    SGD approximates the partial derivatives of the loss function at each step by
    using only a subset of the data in order to make the training process more efficient.
    Examples of popular SGD algorithms include momentum (Polyak, [1964](#bib.bib250)),
    Adam (Kingma and Ba, [2014](#bib.bib175)), and Nesterov Adaptive Gradient (Sutskever
    et al., [2013](#bib.bib298)) —the later inspired by Nesterov ([1983](#bib.bib231)).
    Interestingly, however, we generally cannot guarantee convergence to a global
    optimum with gradient descent due to the nonconvexity of the loss function. Nevertheless,
    neural networks trained with adequately parameterized SGD algorithms tend to generalize
    well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习成功的一个关键因素是可以有效地应用一阶方法进行连续优化，从而训练深度网络。对神经网络的兴趣最初因**Minsky**和**Papert**在《感知机》一书中的负面结果而消失（[1969](#bib.bib219)），书中表明单层感知机无法表示像布尔异或这样的函数。然而，转向能够表示布尔异或以及其他更具表现力的模型的多层感知机则需要巧妙的训练策略。因此，随着**Rumelhart**等人（[1986](#bib.bib266)）和**LeCun**等人（[1989](#bib.bib185)）等论文推广了*反向传播*的使用，兴趣得以恢复。请注意，反向传播算法最早由**Linnainmaa**（[1970](#bib.bib194)）在网络上下文中讨论过，而**Werbos**（[1974](#bib.bib331)）则明确讨论了神经网络中的反向传播。反向传播算法通过在神经网络的各个单元中应用链式法则来计算损失函数相对于每个神经网络参数的导数，这比显式计算每个网络参数的导数要高效得多。因此，神经网络通常使用梯度下降方法进行训练，其中参数在每一步中从输出层到输入层逐步更新。事实上，大多数神经网络训练算法基于随机梯度下降（SGD），这是一种由**Robbins**和**Monro**（[1951](#bib.bib258)）开创的通过采样进行的随机近似形式。SGD通过仅使用数据子集来逼近损失函数的偏导数，以提高训练过程的效率。流行的SGD算法包括动量（**Polyak**，[1964](#bib.bib250)）、Adam（**Kingma**和**Ba**，[2014](#bib.bib175)）和Nesterov自适应梯度（**Sutskever**等人，[2013](#bib.bib298)）——后者受到**Nesterov**（[1983](#bib.bib231)）的启发。有趣的是，由于损失函数的非凸性，我们通常无法保证梯度下降法收敛到全局最优解。然而，使用适当参数化的SGD算法训练的神经网络往往能很好地泛化。
- en: 1.3 Why nonlinearity is important in artificial neurons
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 为什么非线性在人工神经元中很重要
- en: 'The nonlinearity of the activation function leads to such nonconvexity of the
    loss function. However, as we will see in Section [3](#S3 "3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"),
    that same nonlinearity enables the neural network to represent more complex functions
    as a whole. In fact, removing such nonlinearities by using an identity activation
    function $\sigma^{l}(u)=u~{}\forall l\in{\mathbb{L}}$ would reduce the entire
    neural network to an affine transformation of the form $f(x)={\bm{W}}^{L}({\bm{W}}^{L-1}\left(\ldots\left({\bm{W}}^{2}\left({\bm{W}}^{1}x+{\bm{b}}^{1}\right)+{\bm{b}}^{2}\right)+\ldots\right)+{\bm{b}}^{L-1})+{\bm{b}}^{L}$.
    Hence, a feedforward network without nonlinear activation functions is equivalent
    to a linear regression model. However, in that case we can easily obtain such
    a model without resorting to neural networks and backpropagation: the loss function
    is convex and the optimal solution is given by a closed formula, such as in least
    squares regression. In contrast, neural networks with a single hidden layer of
    arbitrary width have been long known to be universal function approximators for
    a broad variety of activation functions (Cybenko, [1989](#bib.bib71), Funahashi,
    [1989](#bib.bib112), Hornik et al., [1989](#bib.bib153)), as well as for ReLU
    more recently (Yarotsky, [2017](#bib.bib350)). These results have also been extended
    to the converse case of limited width but arbitrarily large depth (Lu et al.,
    [2017](#bib.bib202), Hanin and Sellke, [2017](#bib.bib139), Park et al., [2021a](#bib.bib241)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '激活函数的非线性导致了损失函数的非凸性。然而，正如我们将在第[3](#S3 "3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")节中看到的，这种非线性使神经网络能够整体表示更复杂的函数。事实上，通过使用恒等激活函数
    $\sigma^{l}(u)=u~{}\forall l\in{\mathbb{L}}$，去除这种非线性将使整个神经网络简化为形式为 $f(x)={\bm{W}}^{L}({\bm{W}}^{L-1}\left(\ldots\left({\bm{W}}^{2}\left({\bm{W}}^{1}x+{\bm{b}}^{1}\right)+{\bm{b}}^{2}\right)+\ldots\right)+{\bm{b}}^{L-1})+{\bm{b}}^{L}$
    的仿射变换。因此，没有非线性激活函数的前馈网络相当于线性回归模型。然而，在这种情况下，我们可以轻松地获得这样的模型，而无需依赖神经网络和反向传播：损失函数是凸的，最优解由一个封闭公式给出，例如在最小二乘回归中。相比之下，具有单个隐藏层的任意宽度的神经网络长期以来被认为是广泛激活函数的通用函数近似器（Cybenko，[1989](#bib.bib71)，Funahashi，[1989](#bib.bib112)，Hornik
    等，[1989](#bib.bib153)），以及最近的 ReLU（Yarotsky，[2017](#bib.bib350)）。这些结果也已扩展到宽度有限但深度任意大的反向情况（Lu
    等，[2017](#bib.bib202)，Hanin 和 Sellke，[2017](#bib.bib139)，Park 等，[2021a](#bib.bib241)）。'
- en: 'Although nonlinear activation functions are important for obtaining more complex
    models, these functions do not need to be overly complex to produce good results.
    In the past, it was common practice to use sigmoid functions for activation (LeCun
    et al., [1998](#bib.bib186)). Those are monotonically increasing functions that
    approach finite values for arbitrarily large positive and negative inputs, such
    as the standard logistic function $\sigma(u)=\frac{1}{1+e^{-u}}$ and the hyperbolic
    tangent $\sigma(u)=\tanh(u)$. In the present, the most commonly used activation
    function is the Rectified Linear Unit (ReLU) $\sigma(u)=\max\{0,u\}$ (LeCun et al.,
    [2015](#bib.bib187), Ramachandran et al., [2018](#bib.bib255)), which was proposed
    by Hahnloser et al. ([2000](#bib.bib135)) and first applied to neural networks
    by Nair and Hinton ([2010](#bib.bib228)). The popularity of ReLU is in part due
    to experiments by Nair and Hinton ([2010](#bib.bib228)) and Glorot et al. ([2011](#bib.bib119))
    showing that this simpler form of activation yields competitive results. Thinking
    back in terms of the analogy with biological neurons, we say that a ReLU is *active*
    when the output is positive and *inactive* when the output is zero. ReLUs have
    a linear output behavior on the inputs associated with the same ReLUs being active
    and inactive; this property also holds for other piecewise linear and piecewise
    constant functions that are used as activation functions in neural networks. Table [1](#S1.T1
    "Table 1 ‣ 1.3 Why nonlinearity is important in artificial neurons ‣ 1 Introduction
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") lists some of the most
    commonly used activation functions of that kind. For more comprehensive lists
    of activation functions, including several other variations based on ReLU, we
    refer to Dubey et al. ([2021](#bib.bib82)) and Tao et al. ([2022](#bib.bib303)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管非线性激活函数对于获得更复杂的模型很重要，但这些函数不需要过于复杂即可产生良好的结果。过去，使用 sigmoid 函数作为激活函数是一种常见做法（LeCun
    et al., [1998](#bib.bib186)）。这些函数是单调递增的，对于任意大的正负输入，都会趋近于有限值，例如标准 logistic 函数 $\sigma(u)=\frac{1}{1+e^{-u}}$
    和双曲正切函数 $\sigma(u)=\tanh(u)$。目前，最常用的激活函数是修正线性单元（ReLU） $\sigma(u)=\max\{0,u\}$（LeCun
    et al., [2015](#bib.bib187), Ramachandran et al., [2018](#bib.bib255)），该函数由 Hahnloser
    et al. ([2000](#bib.bib135)) 提出，并由 Nair and Hinton ([2010](#bib.bib228)) 首次应用于神经网络。ReLU
    的流行部分归功于 Nair and Hinton ([2010](#bib.bib228)) 和 Glorot et al. ([2011](#bib.bib119))
    的实验，这些实验表明这种简单形式的激活函数能够产生竞争力的结果。从生物神经元的类比角度考虑，我们说当输出为正时 ReLU 是 *活跃的*，当输出为零时则是
    *不活跃的*。ReLU 在与同一 ReLU 活跃和不活跃相关的输入上具有线性输出行为；这一特性也适用于其他作为神经网络激活函数的分段线性和分段常数函数。表
    [1](#S1.T1 "Table 1 ‣ 1.3 Why nonlinearity is important in artificial neurons
    ‣ 1 Introduction ‣ When Deep Learning Meets Polyhedral Theory: A Survey") 列出了这些常用激活函数中的一些。有关激活函数的更全面列表，包括基于
    ReLU 的几种其他变体，请参见 Dubey et al. ([2021](#bib.bib82)) 和 Tao et al. ([2022](#bib.bib303))。'
- en: 'Table 1: Main piecewise constant and piecewise linear activation functions.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：主要的分段常数和分段线性激活函数。
- en: '| Name | Function | Reference |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 函数 | 参考文献 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LTU | $\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if }u>0\\ 0&amp;\text{if
    }u\leq 0\end{array}\right.$ | McCulloch and Pitts ([1943](#bib.bib217)) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| LTU | $\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if }u>0\\ 0&amp;\text{if
    }u\leq 0\end{array}\right.$ | McCulloch and Pitts ([1943](#bib.bib217)) |'
- en: '| ReLU | $\sigma(u)=\max\{0,u\}$ | Hahnloser et al. ([2000](#bib.bib135)),
    Nair and Hinton ([2010](#bib.bib228)) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| ReLU | $\sigma(u)=\max\{0,u\}$ | Hahnloser et al. ([2000](#bib.bib135)),
    Nair and Hinton ([2010](#bib.bib228)) |'
- en: '| leaky ReLU | <math   alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ \varepsilon u&amp;\text{if }u\leq 0\end{array}\right.\\'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| leaky ReLU | <math alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ \varepsilon u&amp;\text{if }u\leq 0\end{array}\right.\\'
- en: \text{($\varepsilon$ is small and fixed)}\end{array}" display="inline"><semantics
    ><mtable rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mrow ><mi >σ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mi >u</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    rowspacing="0pt" ><mtr ><mtd  ><mi >u</mi></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi></mrow><mo
    >></mo><mn >0</mn></mrow></mtd></mtr><mtr ><mtd ><mrow ><mi >ε</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >u</mi></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow
    ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi></mrow><mo
    >≤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr ><mtd
    ><mrow  ><mtext >(</mtext><mi >ε</mi><mtext > is small and fixed)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >𝜎</ci><ci
    >𝑢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >𝑢</ci><apply ><apply ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >𝜀</ci><ci >𝑢</ci></apply><apply
    ><apply ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >ε</mi><mtext > is small and fixed)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ \varepsilon u&\text{if }u\leq 0\end{array}\right.\\ \text{($\varepsilon$
    is small and fixed)}\end{array}</annotation></semantics></math> | Maas et al.
    ([2013](#bib.bib205)) |
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \text{($\varepsilon$ 是小且固定的)}\end{array}" display="inline"><semantics ><mtable
    rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mi >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mi
    >u</mi></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >如果 </mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mi >u</mi></mrow><mo >></mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi >ε</mi><mo lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >u</mi></mrow><mo >≤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mtext >(</mtext><mi >ε</mi><mtext > 是小且固定的)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >𝜎</ci><ci
    >𝑢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >𝑢</ci><apply ><apply ><ci ><mtext >如果 </mtext></ci><ci >𝑢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >𝜀</ci><ci >𝑢</ci></apply><apply
    ><apply ><ci ><mtext >如果 </mtext></ci><ci >𝑢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >ε</mi><mtext > 是小且固定的)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{如果
    }u>0\\ \varepsilon u&\text{如果 }u\leq 0\end{array}\right.\\ \text{($\varepsilon$
    是小且固定的)}\end{array}</annotation></semantics></math> | Maas 等人 ([2013](#bib.bib205))
    |
- en: '| parametric ReLU | <math   alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ au&amp;\text{if }u\leq 0\end{array}\right.\\'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '| 参数化 ReLU | <math   alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{如果
    }u>0\\ \varepsilon u&\text{如果 }u\leq 0\end{array}\right.\\'
- en: \text{($a$ is a trainable parameter)}\end{array}" display="inline"><semantics
    ><mtable rowspacing="0pt"  ><mtr ><mtd ><mrow  ><mrow ><mi >σ</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mi >u</mi><mo stretchy="false"
    >)</mo></mrow></mrow><mo >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt"
    rowspacing="0pt" ><mtr ><mtd  ><mi >u</mi></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi></mrow><mo
    >></mo><mn >0</mn></mrow></mtd></mtr><mtr ><mtd ><mrow ><mi >a</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >u</mi></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow
    ><mtext >if </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi >u</mi></mrow><mo
    >≤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr ><mtd
    ><mrow  ><mtext >(</mtext><mi >a</mi><mtext > is a trainable parameter)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >𝜎</ci><ci
    >𝑢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >𝑢</ci><apply ><apply ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >𝑎</ci><ci >𝑢</ci></apply><apply
    ><apply ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >a</mi><mtext > is a trainable parameter)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ au&\text{if }u\leq 0\end{array}\right.\\ \text{($a$ is a trainable parameter)}\end{array}</annotation></semantics></math>
    | He et al. ([2015](#bib.bib143)) |
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \text{($a$ 是一个可训练参数)}\end{array}" display="inline"><semantics ><mtable rowspacing="0pt"  ><mtr
    ><mtd ><mrow  ><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mi >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo
    >=</mo><mrow ><mo >{</mo><mtable columnspacing="5pt" rowspacing="0pt" ><mtr ><mtd  ><mi
    >u</mi></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >如果 </mtext><mo lspace="0em"
    rspace="0em"  >​</mo><mi >u</mi></mrow><mo >></mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd ><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >如果 </mtext><mo lspace="0em" rspace="0em"  >​</mo><mi
    >u</mi></mrow><mo >≤</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    ><mtd ><mrow  ><mtext >(</mtext><mi >a</mi><mtext > 是一个可训练参数)</mtext></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><matrix  ><matrixrow ><apply ><apply  ><ci >𝜎</ci><ci
    >𝑢</ci></apply><apply ><csymbol cd="latexml"  >cases</csymbol><matrix ><matrixrow
    ><ci >𝑢</ci><apply ><apply ><ci ><mtext >如果 </mtext></ci><ci >𝑢</ci></apply><cn
    type="integer" >0</cn></apply></matrixrow><matrixrow ><apply ><ci >𝑎</ci><ci >𝑢</ci></apply><apply
    ><apply ><ci ><mtext >如果 </mtext></ci><ci >𝑢</ci></apply><cn type="integer"  >0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    ><ci ><mrow  ><mtext >(</mtext><mi >a</mi><mtext > 是一个可训练参数)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" >\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{如果
    }u>0\\ au&\text{如果 }u\leq 0\end{array}\right.\\ \text{($a$ 是一个可训练参数)}\end{array}</annotation></semantics></math>
    | He 等人 ([2015](#bib.bib143)) |
- en: '| hard tanh | <math   alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>1\\ u&amp;\text{if }-1\leq u\leq 1\\'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '| 硬 tanh | <math   alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{如果
    }u>1\\ u&amp;\text{如果 }-1\leq u\leq 1\\'
- en: -1&amp;\text{if }u<-1\end{array}\right." display="inline"><semantics ><mrow
    ><mrow ><mi  >σ</mi><mo lspace="0em" rspace="0em"  >​</mo><mrow ><mo stretchy="false"
    >(</mo><mi >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow  ><mo
    >{</mo><mtable columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mn  >1</mn></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >if </mtext><mo lspace="0em" rspace="0em"
    >​</mo><mi >u</mi></mrow><mo >></mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd ><mi  >u</mi></mtd><mtd
    columnalign="left"  ><mrow ><mrow ><mtext >if </mtext><mo >−</mo><mn >1</mn></mrow><mo
    >≤</mo><mi >u</mi><mo >≤</mo><mn >1</mn></mrow></mtd></mtr><mtr ><mtd  ><mrow
    ><mo >−</mo><mn >1</mn></mrow></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext
    >if </mtext><mo lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow><mo ><</mo><mrow
    ><mo >−</mo><mn >1</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >𝜎</ci><ci >𝑢</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><cn type="integer"  >1</cn><apply
    ><apply ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><cn type="integer" >1</cn></apply></matrixrow><matrixrow
    ><ci  >𝑢</ci><apply ><apply ><apply  ><ci ><mtext >if </mtext></ci><cn type="integer"  >1</cn></apply><ci
    >𝑢</ci></apply><apply ><cn type="integer" >1</cn></apply></apply></matrixrow><matrixrow
    ><apply ><cn type="integer" >1</cn></apply><apply ><apply  ><ci ><mtext >if </mtext></ci><ci
    >𝑢</ci></apply><apply ><cn type="integer" >1</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if }u>1\\
    u&\text{if }-1\leq u\leq 1\\ -1&\text{if }u<-1\end{array}\right.</annotation></semantics></math>
    | Collobert ([2004](#bib.bib62)) |
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sigma(u)=\left\{\begin{array}[]{cl}1&\text{如果 }u>1\\ u&\text{如果 }-1\leq u\leq
    1\\ -1&\text{如果 }u<-1\end{array}\right.\)
- en: '| hard sigmoid | <math   alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>\frac{1}{2}\\ u+\frac{1}{2}&amp;\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '| 硬 sigmoid | <math alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{如果
    }u>\frac{1}{2}\\ u+\frac{1}{2}&amp;\text{如果 }-\frac{1}{2}\leq u\leq\frac{1}{2}\\'
- en: 0&amp;\text{if }u<-\frac{1}{2}\end{array}\right." display="inline"><semantics
    ><mrow  ><mrow ><mi >σ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow  ><mo stretchy="false"  >(</mo><mi
    >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mn  >1</mn></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >if </mtext><mo lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow><mo
    >></mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mi
    >u</mi><mo >+</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >if </mtext><mo >−</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow><mo
    >≤</mo><mi >u</mi><mo >≤</mo><mfrac ><mn  >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr
    ><mtd ><mn  >0</mn></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >if </mtext><mo
    lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow><mo ><</mo><mrow ><mo >−</mo><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >𝜎</ci><ci >𝑢</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><cn type="integer"  >1</cn><apply
    ><apply ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow ><apply
    ><ci  >𝑢</ci><apply ><cn type="integer" >1</cn><cn type="integer"  >2</cn></apply></apply><apply
    ><apply ><apply  ><ci ><mtext >if </mtext></ci><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply></apply><ci >𝑢</ci></apply><apply ><apply ><cn
    type="integer" >1</cn><cn type="integer" >2</cn></apply></apply></apply></matrixrow><matrixrow
    ><cn type="integer" >0</cn><apply ><apply  ><ci ><mtext >if </mtext></ci><ci >𝑢</ci></apply><apply
    ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if }u>\frac{1}{2}\\
    u+\frac{1}{2}&\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\ 0&\text{if }u<-\frac{1}{2}\end{array}\right.</annotation></semantics></math>
    | Courbariaux et al. ([2015](#bib.bib63)) |
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 0&\text{如果 }u<-\frac{1}{2}\end{array}\right." display="inline"><semantics ><mrow  ><mrow
    ><mi >σ</mi><mo lspace="0em" rspace="0em" >​</mo><mrow  ><mo stretchy="false"  >(</mo><mi
    >u</mi><mo stretchy="false" >)</mo></mrow></mrow><mo >=</mo><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" rowspacing="0pt"  ><mtr ><mtd ><mn  >1</mn></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >如果 </mtext><mo lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow><mo
    >></mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr ><mtd ><mrow  ><mi
    >u</mi><mo >+</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow></mtd><mtd columnalign="left"  ><mrow
    ><mrow ><mtext >如果 </mtext><mo >−</mo><mfrac ><mn >1</mn><mn >2</mn></mfrac></mrow><mo
    >≤</mo><mi >u</mi><mo >≤</mo><mfrac ><mn  >1</mn><mn >2</mn></mfrac></mrow></mtd></mtr><mtr
    ><mtd ><mn  >0</mn></mtd><mtd columnalign="left"  ><mrow ><mrow ><mtext >如果 </mtext><mo
    lspace="0em" rspace="0em" >​</mo><mi >u</mi></mrow><mo ><</mo><mrow ><mo >−</mo><mfrac
    ><mn >1</mn><mn >2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><apply  ><ci >𝜎</ci><ci >𝑢</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><matrix ><matrixrow  ><cn type="integer"  >1</cn><apply
    ><apply ><ci ><mtext >如果 </mtext></ci><ci >𝑢</ci></apply><apply ><cn type="integer"
    >1</cn><cn type="integer" >2</cn></apply></apply></matrixrow><matrixrow ><apply
    ><ci  >𝑢</ci><apply ><cn type="integer" >1</cn><cn type="integer"  >2</cn></apply></apply><apply
    ><apply ><apply  ><ci ><mtext >如果 </mtext></ci><apply ><cn type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply></apply><ci >𝑢</ci></apply><apply ><apply ><cn
    type="integer" >1</cn><cn type="integer" >2</cn></apply></apply></apply></matrixrow><matrixrow
    ><cn type="integer" >0</cn><apply ><apply  ><ci ><mtext >如果 </mtext></ci><ci >𝑢</ci></apply><apply
    ><apply ><cn type="integer"  >1</cn><cn type="integer"  >2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\sigma(u)=\left\{\begin{array}[]{cl}1&\text{如果 }u>\frac{1}{2}\\
    u+\frac{1}{2}&\text{如果 }-\frac{1}{2}\leq u\leq\frac{1}{2}\\ 0&\text{如果 }u<-\frac{1}{2}\end{array}\right.</annotation></semantics></math>
    | Courbariaux 等人 ([2015](#bib.bib63)) |
- en: '| max pooling | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{0,u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is the output of another neuron)}\end{array}$ | Weng et al.
    ([1992](#bib.bib329)) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 最大池化 | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{0,u_{1},\ldots,u_{k}\}\\
    \text{（每个 $u_{i}$ 是另一个神经元的输出）}\end{array}$ | Weng 等人 ([1992](#bib.bib329)) |'
- en: '| maxout | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is an affine function)}\end{array}$ | Goodfellow et al. ([2013](#bib.bib123))
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 最大输出 | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{u_{1},\ldots,u_{k}\}\\
    \text{（每个 $u_{i}$ 是一个仿射函数）}\end{array}$ | Goodfellow 等人 ([2013](#bib.bib123))
    |'
- en: 1.4 When deep learning meets polyhedral theory
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 当深度学习遇上多面体理论
- en: 'It is commonly accepted in machine learning that a simpler model is preferred
    if it trains as well as a more complex one, since a simpler model is less likely
    to overfit. Conveniently, the successful return of neural networks to relatively
    simpler activation functions prepared the ground for deep learning to meet polyhedral
    theory. In other words, we are now able to analyze and leverage neural networks
    through the same lenses and tools that have been successfully used for linear
    and discrete optimization in operations research for many decades. We explain
    this connection in more detail and some lines of research that it has opened up
    in Section [2](#S2 "2 The Polyhedral Perspective ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在机器学习中，通常认为如果一个更简单的模型能够像更复杂的模型一样训练好，那么更简单的模型是更受欢迎的，因为更简单的模型不容易过拟合。幸运的是，神经网络成功地回归到相对简单的激活函数，为深度学习与多面体理论的结合奠定了基础。换句话说，我们现在可以通过与运筹学中用于线性和离散优化的相同视角和工具来分析和利用神经网络。我们在第[2](#S2
    "2 The Polyhedral Perspective ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")节中更详细地解释了这种联系及其开启的一些研究方向。'
- en: 1.5 Scope of this survey and related work
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 本调查的范围及相关工作
- en: The interplay between mathematical optimization and machine learning has also
    been discussed by other recent surveys. Bengio et al. ([2021](#bib.bib19)) review
    the use of machine learning in mathematical optimization, whereas Gambella et al.
    ([2021](#bib.bib115)) formulate mathematical optimization problems with the main
    focus of obtaining machine learning models, such as by training neural networks.
    A similar scope has been previously surveyed by Curtis and Scheinberg ([2017](#bib.bib70))
    and Bottou et al. ([2018](#bib.bib38)). Our survey complements those by focusing
    exclusively on neural networks while outlining how linear optimization can be
    used more broadly in that context, from network training and verification to model
    embedding and compression, as well as refined through formulation strengthening.
    In addition, we illustrate how polyhedral theory can ground the use of such linear
    formulations and also provide a more nuanced understanding of the discriminative
    ability of neural networks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数学优化和机器学习之间的相互作用也已在其他最近的调查中讨论过。Bengio等人（[2021](#bib.bib19)）回顾了机器学习在数学优化中的应用，而Gambella等人（[2021](#bib.bib115)）则以获取机器学习模型为主要焦点来形式化数学优化问题，例如通过训练神经网络。Curtis和Scheinberg（[2017](#bib.bib70)）以及Bottou等人（[2018](#bib.bib38)）之前也有类似的范围进行过调查。我们的调查通过专注于神经网络，同时概述了如何在这个背景下更广泛地使用线性优化，从网络训练和验证到模型嵌入和压缩，以及通过形式化强化进行的改进，来补充这些调查。此外，我们还说明了多面体理论如何为这些线性形式的使用奠定基础，并提供对神经网络区分能力的更细致理解。
- en: 'The presentation in this survey is centered on *feedforward rectifier networks*.
    These are very commonly used networks with only ReLU activations and for which
    most polyhedral results and applications of linear optimization are known. The
    focus on a single type of neural network is intended to help the reader capture
    the intuition behind different developments and understand the nuances involved.
    Despite our focus on *fully-connected* models, which are those in which every
    unit is connected to all units in the subsequent layer, there are many variants
    of interest with fewer or different types of connection that can be interpreted
    as a special case of fully-connected models. For example, the units of Convolutional
    Neural Networks (CNNs or ConvNets) (Fukushima, [1980](#bib.bib111)) have local
    connectivity: only a subset of adjacent units defines the output of each unit
    in the next layer, and the same parameters are used to define the output of different
    units. In fact, multiple *filters* of parameters can be applied to a set of adjacent
    units through the output of different units in the next layer. CNNs are often
    applied to identify and aggregate the same local features in different parts of
    a picture, and we can interpret them as a special case of feedforward networks.
    Another common variant, the Residual Network (ResNet) (He et al., [2016](#bib.bib144)),
    includes *skip connections* that directly connect units in nonadjacent layers.
    Those connections can be emulated by adding units passing their outputs through
    the intermediary layers. Hence, many of the results and applications discussed
    along the survey are relevant to other variants (e.g., LTU and maxout activations,
    or those other connectivity patterns), and we also provide references to more
    specific results and applications involving them.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中的演示以*前馈整流网络*为中心。这些网络非常常见，仅使用ReLU激活函数，对于它们，大多数多面体结果和线性优化应用都已知。专注于单一类型的神经网络旨在帮助读者捕捉不同发展的直觉并理解其中的细微差别。尽管我们专注于*全连接*模型，即每个单元都与后续层的所有单元连接，但仍有许多有趣的变体具有较少或不同类型的连接，可以被解释为全连接模型的特例。例如，卷积神经网络（CNNs或ConvNets）（Fukushima，[1980](#bib.bib111)）的单元具有局部连接性：仅相邻单元的子集定义了下一层中每个单元的输出，并且相同的参数用于定义不同单元的输出。实际上，可以通过下一层中不同单元的输出将多个*滤波器*参数应用于一组相邻单元。CNNs通常用于识别和汇聚图像中不同部分的相同局部特征，我们可以将它们解释为前馈网络的特例。另一个常见的变体，残差网络（ResNet）（He
    et al., [2016](#bib.bib144)），包括*跳跃连接*，直接连接非相邻层中的单元。这些连接可以通过在中间层传递它们的输出来模拟。因此，调查中讨论的许多结果和应用也适用于其他变体（例如，LTU和maxout激活，或其他连接模式），我们还提供了涉及它们的更具体结果和应用的参考。
- en: We also discuss the extent to which other variants remain relevant or can be
    analyzed through the same lenses. For example, *feedback connections* in *recurrent
    networks* (Little, [1974](#bib.bib195), Hopfield, [1982](#bib.bib152)) allow the
    output of a unit to be used as an input of units in previous layers. Recurrent
    networks such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib151))
    produce outputs that depend on their internal state, and they may consequently
    process sequential inputs with arbitrary length. While feedback connections may
    not be emulated with a feeforward network, we discuss in the following paragraph
    how recurrent networks have been replaced with great success by attention mechanisms,
    which are implemented with feedforward networks. In the realm of variants that
    remain relevant, it is very common to apply a different type of activation to
    the output layer of the network, such as the layer-wise softmax function $\sigma:\mathbb{R}^{n_{L}}\rightarrow\mathbb{R}^{n_{L}}$
    in which $\sigma(u)_{i}=e^{u_{i}}/\sum_{j=1}^{n_{L}}e^{u_{j}}~{}\forall i\in\{1,\ldots,n_{L}\}$
    (Bridle, [1990](#bib.bib39)), which is used to normalize a multidimensional output
    as a probability distribution. While softmax is not piecewise linear, we describe
    how its output can also be analyzed from a polyhedral perspective.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了其他变体在多大程度上仍然相关或可以通过相同的视角进行分析。例如，*反馈连接*在*递归网络*（Little，[1974](#bib.bib195)，Hopfield，[1982](#bib.bib152)）中允许一个单元的输出作为前一层单元的输入。递归网络如长短期记忆（LSTM）（Hochreiter
    和 Schmidhuber，[1997](#bib.bib151)）产生依赖于其内部状态的输出，因此它们可以处理任意长度的序列输入。尽管反馈连接无法通过前馈网络进行模拟，我们将在接下来的段落中讨论递归网络如何被成功替代为使用前馈网络实现的注意力机制。在仍然相关的变体领域中，通常会对网络的输出层应用不同类型的激活，例如层级softmax函数
    $\sigma:\mathbb{R}^{n_{L}}\rightarrow\mathbb{R}^{n_{L}}$，其中 $\sigma(u)_{i}=e^{u_{i}}/\sum_{j=1}^{n_{L}}e^{u_{j}}~{}\forall
    i\in\{1,\ldots,n_{L}\}$（Bridle，[1990](#bib.bib39)），用于将多维输出归一化为概率分布。虽然softmax不是分段线性的，我们描述了如何从多面体的角度分析其输出。
- en: Other uses of deep learning
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度学习的其他应用
- en: Deep learning is also being used in machine learning beyond the realm of supervised
    learning. In *unsupervised learning*, the focus is on drawing inferences from
    unlabeled datasets. For example, Generative Adversarial Networks (GANs) (Goodfellow
    et al., [2014](#bib.bib126)) have been used to generate realistic images using
    a pair of neural networks. One of these networks is a *discriminator* trained
    to identify elements from a dataset and the other is a *generator* aiming to mislead
    the discriminator with synthetic inputs that could be classified as belonging
    to the dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习不仅被用于监督学习的领域，也应用于机器学习的其他方面。在*无监督学习*中，重点是从未标记的数据集中推断出结论。例如，生成对抗网络（GANs）（Goodfellow
    等，[2014](#bib.bib126)）被用来通过一对神经网络生成真实感的图像。这些网络中的一个是*判别器*，它被训练来识别数据集中的元素，另一个是*生成器*，旨在用合成输入误导判别器，使其将这些输入归类为属于数据集。
- en: In *reinforcement learning*, the focus is on modeling agents that can interact
    with their environment through actions and associated rewards. Examples of such
    applications include neural networks designed for the navigation of self-driving
    vehicles (Gao et al., [2020](#bib.bib116)) and for playing Atari games (Mnih et al.,
    [2015](#bib.bib222)), more contemporary electronic games such as Dota 2 (OpenAI
    et al., [2019](#bib.bib237)) and StarCraft II (Vinyals et al., [2017](#bib.bib321)),
    and the game of Go (Silver et al., [2017](#bib.bib288)) at levels that are either
    better or at least comparable to human players.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在*强化学习*中，重点是建模能够通过动作和相关奖励与环境互动的代理。这样的应用实例包括用于自驾车导航的神经网络（Gao 等，[2020](#bib.bib116)）和用于玩Atari游戏的神经网络（Mnih
    等，[2015](#bib.bib222)），以及现代电子游戏如Dota 2（OpenAI 等，[2019](#bib.bib237)）和StarCraft
    II（Vinyals 等，[2017](#bib.bib321)），还有围棋（Silver 等，[2017](#bib.bib288)），这些游戏在某些水平上可以与人类玩家相媲美或更胜一筹。
- en: A more recent and popular example are generative transformers (Radford et al.,
    [2018](#bib.bib252)), such as DALL·E 2 (Ramesh et al., [2022](#bib.bib257)) producing
    realistic images from text prompts in mid-2022 and ChatGPT (OpenAI, [2022](#bib.bib236))
    producing realistic dialogues with users in early 2023, the latter belonging to
    the fast growing family of large language models. They are based on replacing
    architectures based on feedback connections, such as LSTM, with the attention
    mechanisms aimed at scoring the relevance of past states (Bahdanau et al., [2015](#bib.bib10)),
    which is the foundation of the transformer architecture (Vaswani et al., [2017](#bib.bib315)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个较新且流行的例子是生成型变换器（Radford 等，[2018](#bib.bib252)），例如 DALL·E 2（Ramesh 等，[2022](#bib.bib257)）在
    2022 年中期从文本提示生成逼真的图像，以及 ChatGPT（OpenAI，[2022](#bib.bib236)）在 2023 年初与用户生成逼真的对话，后者属于快速增长的大型语言模型家族。它们基于用注意力机制替代基于反馈连接的架构，如
    LSTM，注意力机制旨在评分过去状态的相关性（Bahdanau 等，[2015](#bib.bib10)），这是变换器架构的基础（Vaswani 等，[2017](#bib.bib315)）。
- en: Further reading
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深入阅读
- en: For a historical perspective on neural networks, we recommend Schmidhuber ([2015](#bib.bib273)).
    For a recent and broad introduction to the fundamentals of deep learning, we recommend Zhang
    et al. ([2023](#bib.bib353)). For other forms of measuring model complexity in
    neural networks, we refer to Hu et al. ([2021](#bib.bib156)).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络的历史视角，我们推荐 Schmidhuber ([2015](#bib.bib273))。关于深度学习基础的最新和广泛的介绍，我们推荐 Zhang
    等 ([2023](#bib.bib353))。有关神经网络模型复杂性的其他测量形式，请参考 Hu 等 ([2021](#bib.bib156))。
- en: 2 The Polyhedral Perspective
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 多面体视角
- en: A feedforward rectifier network models a piecewise linear function (Arora et al.,
    [2018](#bib.bib8)) in which every such piece is a polyhedron (Raghu et al., [2017](#bib.bib253)),
    and represents a special case among neural networks modeling piecewise polynomials
    (Balestriero and Baraniuk, [2018](#bib.bib15)). Therefore, training a rectifier
    network is equivalent to performing a piecewise linear regression, and we can
    potentially interpret such neural networks in terms of what happens in each piece
    of the function that they model. However, we are only beginning to answer some
    of the questions entailed by such a remark. In this survey, we discuss how insights
    on this subject may help us answer the following questions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个前馈整流网络建模了一个分段线性函数（Arora 等，[2018](#bib.bib8)），其中每一段都是一个多面体（Raghu 等，[2017](#bib.bib253)），并且在建模分段多项式的神经网络中代表一个特殊的情况（Balestriero
    和 Baraniuk，[2018](#bib.bib15)）。因此，训练一个整流网络等同于进行分段线性回归，我们可以潜在地根据它们建模的每一段函数的表现来解释这些神经网络。然而，我们还只是开始回答由此类观察引发的一些问题。在这项调查中，我们讨论了关于这一主题的见解如何帮助我们回答以下问题。
- en: '1.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Which piecewise linear functions can or cannot be obtained from training a neural
    network given its architecture?
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哪些分段线性函数可以或不能通过训练神经网络根据其架构获得？
- en: '2.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Which neural networks are more susceptible to adversarial exploitation?
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 哪些神经网络更容易受到对抗性攻击的利用？
- en: '3.'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Can we integrate the model learned by a neural network into a broader decision-making
    problem for which we want to find an optimal solution?
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否将神经网络学习到的模型整合到一个更广泛的决策问题中，以找到一个最优解？
- en: '4.'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Is it possible to obtain a smaller neural network that models exactly the same
    function as another trained neural network?
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否可以获得一个模型完全相同函数的更小神经网络？
- en: '5.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Can we exploit the polyhedral geometry present in neural networks in the training
    phase?
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否在训练阶段利用神经网络中存在的多面体几何？
- en: '6.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Can we efficiently incorporate extra structure when training neural network,
    such as linear constraints over the weights?
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否在训练神经网络时有效地融入额外的结构，例如对权重的线性约束？
- en: 'The first question complements the universal approximation results for neural
    networks. Namely, there is a limit to what functions can be well approximated
    when limited computational resources are translated into constraints on the depth
    and width of the layers of neural networks that can be used in practice. The functions
    that can be modeled depend on the particular choice of hyperparameters subject
    to the computational resources available, and in the long run that may also lead
    to a more principled approach for the choice of hyperparameters than the current
    approaches of neural architecture search. In Section [3](#S3 "3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"),
    we analyze how a rectifier network partitions the input space into pieces in which
    it behaves linearly, which we denote as *linear regions*. We discuss the geometry
    of linear regions, the effect of parameters and hyperparameters on the number
    of linear regions of a neural network, and the extent to which such number of
    linear regions relates to the accuracy of the network.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个问题补充了神经网络的通用逼近结果。也就是说，当计算资源有限被转化为实际应用中层深度和宽度的约束时，可以良好逼近的函数是有限的。可以建模的函数依赖于特定的超参数选择，这些超参数受限于可用的计算资源，从长远来看，这可能也会导致比当前的神经架构搜索方法更为有原则的超参数选择方法。在第[3](#S3
    "3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")节中，我们分析了整流网络如何将输入空间划分为在其中线性行为的区域，我们称之为*线性区域*。我们讨论了线性区域的几何形状、参数和超参数对神经网络线性区域数量的影响，以及这种线性区域数量与网络准确性之间的关系。'
- en: 'The second question relies on formal verification methods to evaluate the robustness
    of neural networks, which can be approached with mathematical optimization formulations
    that are also relevant for the third and fourth questions. Such formulations are
    convenient since a direct inspection of every piece of the function modeled by
    a neural network is prohibitive given how quickly their number scale with the
    size of the network. The linear behavior of the network for every choice of active
    and inactive units implies that we can use linear formulations with binary variables
    corresponding to the activation of units to model trained neural networks using
    MILP. Therefore, we are able to solve a variety of optimization problems over
    a trained neural network, such as the neural network verification problem, identifying
    the range of outputs for each ReLU of the network, and modeling a trained neural
    network as part of a larger decision-making problem. In Section [4](#S4 "4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey"), we discuss how to formulate optimization problems over a trained neural
    network, the applications of such formulations, and the progress toward obtaining
    stronger formulations that scale more easily with the network size.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个问题依赖于形式化验证方法来评估神经网络的鲁棒性，这可以通过数学优化公式来解决，这些公式也与第三个和第四个问题相关。这些公式很方便，因为直接检查神经网络建模的每一个功能块是不现实的，因为其数量随着网络规模的扩大而迅速增长。网络在每种激活和非激活单元的选择下的线性行为意味着我们可以使用带有二进制变量的线性公式来通过MILP对训练后的神经网络进行建模。因此，我们能够解决多种优化问题，例如神经网络验证问题、识别网络中每个ReLU的输出范围，以及将训练后的神经网络建模为更大决策问题的一部分。在第[4](#S4
    "4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")节中，我们讨论了如何在训练后的神经网络上制定优化问题，这些公式的应用，以及朝着获得更强的公式的进展，这些公式能更容易地与网络规模匹配。'
- en: 'The fifth and sixth questions involve the training procedure of a DNN, where
    linear programming tools have been applied to partially answer them. In Section [5](#S5
    "5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), we overview these developments. In terms of the
    fifth question —exploiting polyhedrality in training neural networks— we describe
    algorithms that use the polyhedral geometry induced by activation sets to solve
    training problems. We also cover a recently proposed polyhedral construction that
    can approximately encode multiple training problems at once, showing a strong
    relationship across training problems that arise from different datasets, for
    a fixed architecture. Additionally, we review some recent uses of mixed-integer
    linear programming in the training phase as an alternative to SGD when the weights
    are required to be integer. Regarding the sixth question —the incorporation of
    extra structure when training— we review multiple approaches that have included
    techniques related to linear programming within SGD to impose a desirable structure
    when training, or to find better step-lengths in the execution of SGD.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '第五个和第六个问题涉及深度神经网络的训练过程，其中应用了线性规划工具来部分回答这些问题。在第[5](#S5 "5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")节中，我们概述了这些进展。关于第五个问题——在训练神经网络时利用多面体性——我们描述了使用激活集诱导的多面体几何来解决训练问题的算法。我们还介绍了最近提出的一种多面体构造，它可以大致编码多个训练问题，显示出在固定架构下，不同数据集产生的训练问题之间的强关系。此外，我们回顾了一些最近在训练阶段使用混合整数线性规划作为替代SGD的方法，当权重需要为整数时。关于第六个问题——在训练过程中引入额外结构——我们回顾了包括与线性规划相关的技术在SGD中以施加期望结构或在执行SGD时找到更好步长的多种方法。'
- en: 3 The Linear Regions of a Neural Network
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 神经网络的线性区域
- en: Every piece of the piecewise linear function modeled by a neural network is
    a linear region, and —without loss of generality— we can think of it as a polyhedron.
    In this section, we define a linear region, exemplify how they can be so numerous,
    and what may affect their count in a neural network. We also discuss the practical
    implications of such insights, as well as other related forms of analyzing the
    ability of a neural network to represent expressive models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络建模的分段线性函数的每一部分都是一个线性区域，而且——没有损失的一般性——我们可以将其视为一个多面体。在本节中，我们定义了线性区域，举例说明了它们为何如此众多，以及可能影响神经网络中线性区域数量的因素。我们还讨论了这些见解的实际意义，以及其他与分析神经网络表示能力相关的形式。
- en: Definition 1
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1
- en: A linear region corresponds to the set of points from the input space that activates
    the same units along the neural network, and hence can be characterized by the
    set ${\mathbb{S}}^{l}$ of units that are active in each layer $l\in{\mathbb{L}}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 线性区域对应于输入空间中激活神经网络中相同单元的点集，因此可以通过在每层$l\in{\mathbb{L}}$中激活的单元集${\mathbb{S}}^{l}$来描述。
- en: Since a neural network behaves uniformly over a linear region, the latter is
    the smallest finite scale in which we can analyze its behavior. If we restrict
    the domain of a neural network to a linear region ${\mathbb{I}}\subseteq\mathbb{R}^{n_{0}}$,
    then the neural network behaves as an affine transformation ${\bm{y}}_{\mathbb{I}}:{\mathbb{I}}\rightarrow\mathbb{R}^{n_{L}}$
    of the form ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$ with a
    matrix ${\bm{T}}\in\mathbb{R}^{n_{L}\times n_{0}}$ and a vector ${\bm{t}}\in\mathbb{R}^{n_{L}}$
    that are directly defined by the network parameters and the set of neurons that
    are activated by any input ${\bm{x}}\in{\mathbb{I}}$. For a small perturbation $\varepsilon$
    to some input $\overline{{\bm{x}}}\in{\mathbb{I}}$ such that $\overline{{\bm{x}}}+\varepsilon\in{\mathbb{I}}$,
    the network output for $\bar{x}+\varepsilon$ is given by ${\bm{y}}_{\mathbb{I}}(\overline{{\bm{x}}}+\varepsilon)$.
    While it is possible that two adjacent regions defined in such way correspond
    to the same affine transformation, thinking of each linear region as having a
    distinct signature of active units makes it easier to analyze them.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络在一个线性区域内表现一致，因此线性区域是分析其行为的最小有限尺度。如果我们将神经网络的定义域限制在一个线性区域 ${\mathbb{I}}\subseteq\mathbb{R}^{n_{0}}$，那么神经网络表现为形式为
    ${\bm{y}}_{\mathbb{I}}:{\mathbb{I}}\rightarrow\mathbb{R}^{n_{L}}$ 的仿射变换 ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$，其中矩阵
    ${\bm{T}}\in\mathbb{R}^{n_{L}\times n_{0}}$ 和向量 ${\bm{t}}\in\mathbb{R}^{n_{L}}$
    直接由网络参数和由任何输入 ${\bm{x}}\in{\mathbb{I}}$ 激活的神经元集合定义。对于某个输入 $\overline{{\bm{x}}}\in{\mathbb{I}}$
    的小扰动 $\varepsilon$，使得 $\overline{{\bm{x}}}+\varepsilon\in{\mathbb{I}}$，网络输出为 ${\bm{y}}_{\mathbb{I}}(\overline{{\bm{x}}}+\varepsilon)$。虽然以这种方式定义的两个相邻区域可能对应于相同的仿射变换，但将每个线性区域视为具有独特的激活单元签名使得分析它们更容易。
- en: The number of linear regions defined by a neural network is one form with which
    we can measure the complexity of the models that it can represent (Bengio, [2009](#bib.bib18)).
    Hence, if a more complex model is desired, we may want to design a neural network
    that can potentially define more linear regions. On the one hand, the number of
    linear regions may grow exponentially on the depth of a neural network. On the
    other hand, such a number depends on the interplay between network parameters
    and hyperparameters. As we consider how the inputs from adjacent linear regions
    are evaluated, the change to the affine transformation can be characterized in
    algebraic and geometric terms. Understanding such changes may help us grasp how
    a neural network is capable of telling its inputs apart, including what are the
    sources of the complexity of the model.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络定义的线性区域数量是一种衡量其表示模型复杂度的形式（Bengio，[2009](#bib.bib18)）。因此，如果需要更复杂的模型，我们可能希望设计一个可以定义更多线性区域的神经网络。一方面，线性区域的数量可能随着神经网络深度的增加而呈指数增长。另一方面，这种数量取决于网络参数和超参数之间的相互作用。考虑到如何评估相邻线性区域的输入，对仿射变换的变化可以用代数和几何术语来描述。理解这些变化可能有助于我们掌握神经网络如何区分其输入，包括模型复杂度的来源。
- en: For neural networks in which the activation function is not piecewise linear,
    Bianchini and Scarselli ([2014](#bib.bib29)) have used more elaborate topological
    measures to compare the expressiveness of shallow and deep neural networks. Hu
    et al. ([2020b](#bib.bib155)) followed a closer approach by producing a linear
    approximation neural network in which the number of linear regions can be counted.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于激活函数不是分段线性的神经网络，Bianchini 和 Scarselli（[2014](#bib.bib29)）使用了更复杂的拓扑度量来比较浅层和深层神经网络的表达能力。Hu
    等人（[2020b](#bib.bib155)）通过生成一个线性近似神经网络来跟进，其中线性区域的数量可以被计数。
- en: 3.1 The combinatorial aspect of linear regions
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 线性区域的组合方面
- en: One of the most striking aspects about analyzing a neural network in terms of
    its linear regions is how quickly such number grows. Early work on this topic
    by Pascanu et al. ([2014](#bib.bib243)) and Montúfar et al. ([2014](#bib.bib224))
    have drawn two important observations. First, that it is possible to construct
    simple deep neural networks with a number of linear regions that grows exponentially
    in the depth. Second, that the number of linear regions can be exponential in
    the number of neurons alone.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 分析神经网络线性区域的一个最引人注目的方面是这种数量增长的速度。Pascanu 等人（[2014](#bib.bib243)）和 Montúfar 等人（[2014](#bib.bib224)）在早期的工作中得出了两个重要的观察结果。首先，可能构造出简单的深度神经网络，其线性区域数量随着深度的增加而指数增长。其次，线性区域的数量仅在神经元的数量上也可以是指数级的。
- en: 'The first observation comes from analyzing the role of ReLUs in a very simple
    setting. Namely, that of a neural network in which we regard every layer as having
    a single input in the $[0,1]$ domain, which is produced by combining the outputs
    of the units from the preceding layer, as illustrated by Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个观察来自于在一个非常简单的设置中分析 ReLU 的作用。即在一个神经网络中，我们将每一层视为有一个在 $[0,1]$ 范围内的输入，该输入是通过组合前一层单元的输出生成的，如示例
    [1](#Thmexample1 "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣
    3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") 所示。'
- en: Example 1
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 1
- en: 'Consider a neural network with input $x$ from the domain $[0,1]$ and layers
    having 4 neurons with ReLU activation. For the first layer, assume that the output
    of the neurons are given by the following functions: $f_{1}(x)=\max\{4x,0\}$,
    $f_{2}(x)=\max\{8x-2,0\}$, $f_{3}(x)=\max\{6.5x-3.25,0\}$, and $f_{4}(x)=\max\{12.5x-11.25,0\}$.
    In other words, ${\bm{h}}^{1}_{i}=f_{i}(x)~{}\forall i\in\{1,2,3,4\}$. For the
    subsequent layers, assume that the outputs coming from the previous layer are
    combined through the function $F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$, which
    substitutes $x$ as the input to the next layer; upon which the same set of functions
    $\{f_{i}(x)\}_{i=1}^{4}$ defines the output of the next layer. In other words,
    ${\bm{h}}^{l}_{i}=f_{i}(F({\bm{h}}^{l-1}))=f_{i}({h}_{1}^{l-1}-{h}_{2}^{l-1}+{h}_{3}^{l-1}-{h}_{4}^{l-1})~{}\forall
    i\in\{1,2,3,4\},l\in{\mathbb{L}}\setminus\{1\}$.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个输入为 $x$ 的神经网络，输入域为 $[0,1]$，且层具有 4 个带有 ReLU 激活的神经元。对于第一层，假设神经元的输出由以下函数给出：$f_{1}(x)=\max\{4x,0\}$，$f_{2}(x)=\max\{8x-2,0\}$，$f_{3}(x)=\max\{6.5x-3.25,0\}$，和
    $f_{4}(x)=\max\{12.5x-11.25,0\}$。换句话说，${\bm{h}}^{1}_{i}=f_{i}(x)~{}\forall i\in\{1,2,3,4\}$。对于随后的层，假设来自前一层的输出通过函数
    $F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$ 进行组合，该函数将 $x$ 替换为下一层的输入；然后同样的函数集 $\{f_{i}(x)\}_{i=1}^{4}$
    定义了下一层的输出。换句话说，${\bm{h}}^{l}_{i}=f_{i}(F({\bm{h}}^{l-1}))=f_{i}({h}_{1}^{l-1}-{h}_{2}^{l-1}+{h}_{3}^{l-1}-{h}_{4}^{l-1})~{}\forall
    i\in\{1,2,3,4\},l\in{\mathbb{L}}\setminus\{1\}$。
- en: When the output of the units in the first layer is combined as $F(x)$, we obtain
    a zigzagging function with 4 slopes in the $[0,1]$ domain, each of which defining
    a bijection between segments of the input —namely, $[0,0.25]$, $[0.25,0.5]$, $[0.5,0.9]$,
    and $[0.9,1.0]$— and the image $[0,1]$. The effect of repeating such structure
    in the second layer is that of composing $F(x)$ with itself, with 4 slopes being
    produced within each of those 4 initial segments. Hence, the number of slopes
    —and therefore of linear regions— in the output of such a neural network with
    $L$ layers of activation functions is $4^{L}$, which implies an exponential growth
    on depth.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一层单元的输出组合为 $F(x)$ 时，我们得到一个在 $[0,1]$ 范围内具有 4 个斜率的锯齿状函数，每个斜率定义了输入区间的一个双射 —即
    $[0,0.25]$、$[0.25,0.5]$、$[0.5,0.9]$ 和 $[0.9,1.0]$— 和图像 $[0,1]$ 之间的对应关系。将这种结构在第二层中重复的效果是将
    $F(x)$ 与自身进行组合，在每个这 4 个初始区间内产生 4 个斜率。因此，这种具有 $L$ 层激活函数的神经网络的输出中的斜率数量 —也就是线性区域的数量—
    是 $4^{L}$，这意味着深度呈指数增长。
- en: 'The network structure and the parameters of the neurons in the first two layers
    are illustrated in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 The combinatorial aspect
    of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"); the set of functions $\{f_{i}(x)\}_{i=1}^{4}$
    and the combined outputs of the first two layers —$F(x)$ and $F(F(x))$— are illustrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.1 The combinatorial aspect of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '网络结构和前两层中神经元的参数如图 [3](#S3.F3 "Figure 3 ‣ 3.1 The combinatorial aspect of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey") 所示；函数集 $\{f_{i}(x)\}_{i=1}^{4}$ 和前两层的组合输出 —$F(x)$
    和 $F(F(x))$— 如图 [4](#S3.F4 "Figure 4 ‣ 3.1 The combinatorial aspect of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey") 所示。'
- en: 'In Example [1](#Thmexample1 "Example 1 ‣ 3.1 The combinatorial aspect of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), every neuron changes the slope of the resulting
    function once it becomes active, in which we purposely alternate between positive
    and negative slopes once the function reaches either 0 or 1, respectively. By
    selecting the network parameters accordingly, Montúfar et al. ([2014](#bib.bib224))
    were the first to show that a layer with $n$ ReLUs can be used to create a zigzagging
    function with $n$ slopes on the $[0,1]$ domain, with the image along every slope
    also corresponding to the interval $[0,1]$. Consequently, stacking $L$ of such
    layers results in a neural network with $n^{L}$ linear regions.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '在示例 [1](#Thmexample1 "Example 1 ‣ 3.1 The combinatorial aspect of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")中，每个神经元在激活后会改变结果函数的斜率，其中我们故意在函数分别达到0或1时，在正斜率和负斜率之间交替。通过相应选择网络参数，Montúfar等人（[2014](#bib.bib224)）首次展示了一个具有$n$个ReLU的层可以用来创建一个在$[0,1]$域上具有$n$个斜率的锯齿状函数，每个斜率的映像也对应于区间$[0,1]$。因此，堆叠$L$个这样的层会导致一个具有$n^{L}$个线性区域的神经网络。'
- en: '![Refer to caption](img/0bac8f009269c114fb6255ef917600fd.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/0bac8f009269c114fb6255ef917600fd.png)'
- en: 'Figure 3: Mapping from the input $x\in[0,1]$ to the intermediary output ${\bm{h}}^{2}\in[0,1]^{4}$
    through the first two layers of a neural network in which the number of linear
    regions growths exponentially on the depth, as described in Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").
    The parameters of subsequent layers are the same as those in the second layer.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：通过神经网络的前两层将输入$x\in[0,1]$映射到中间输出${\bm{h}}^{2}\in[0,1]^{4}$，其中线性区域的数量随深度呈指数增长，如示例 [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")中所述。后续层的参数与第二层的参数相同。'
- en: '![Refer to caption](img/285adf5284d089caf1d99c5051b857ed.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/285adf5284d089caf1d99c5051b857ed.png)'
- en: 'Figure 4: Set of activation functions $\{f_{i}(x)\}_{i=1}^{4}$ of the units
    in the first layer and combined outputs of the first two layers —$F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$
    for the first and $F(F(x))$ for the second— of a neural network in which the number
    of linear regions grows exponentially on the depth, as described in Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '图4：第一层中单位的激活函数集合$\{f_{i}(x)\}_{i=1}^{4}$以及前两层的组合输出——第一个是$F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$，第二个是$F(F(x))$——在一个神经网络中，线性区域的数量随着深度呈指数增长，如示例 [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")中所述。'
- en: 'The second observation —that the number of linear regions can grow exponentially
    in the number of neurons alone— comes from the interplay between the parts of
    the input space in which each the units are active, especially in higher-dimensional
    spaces. This is based on some geometric observations that we discuss in Section [3.3](#S3.SS3
    "3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). Even for a *shallow*
    network —i.e., the number of layers being $L=1$— such a number of linear regions
    may approach $2^{n}$, which corresponds to every possible activation set ${\mathbb{S}}\subseteq\{1,\ldots,n\}$
    defining a nonempty linear region. However, as we discuss later, that is not always
    the case due to architectural choices such as the number of layers and their width.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '第二个观察结果——即线性区域的数量仅仅在神经元数量上就可以呈指数增长——来源于每个单位在输入空间中活跃部分的相互作用，特别是在高维空间中。这是基于我们在第[3.3节](#S3.SS3
    "3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")讨论的一些几何观察。即使对于一个*浅层*网络——即层数为$L=1$——这样的线性区域数量可能接近$2^{n}$，这对应于每个可能的激活集${\mathbb{S}}\subseteq\{1,\ldots,n\}$定义一个非空的线性区域。然而，正如我们后面讨论的，由于诸如层数和宽度等架构选择，这种情况并不总是成立。'
- en: 3.2 The algebra of linear regions
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 线性区域的代数
- en: 'Given the activation sets $\{{\mathbb{S}}^{l}\}_{l\in{\mathbb{L}}}$ denoting
    which neurons are active for each layer of the neural network, we can explicitly
    describe the affine transformation ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$
    associated with the corresponding linear region ${\mathbb{I}}$. For every activation
    set ${\mathbb{S}}^{l}$, layer $l$ defines an affine transformation of the form
    $\Omega^{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$, where $\Omega^{{\mathbb{S}}^{l}}$
    is a diagonal $n_{l}\times n_{l}$ matrix in which $\Omega^{{\mathbb{S}}^{l}}_{ii}=1$
    if $i\in{\mathbb{S}}^{l}$ and $\Omega^{{\mathbb{S}}^{l}}_{ii}=0$ otherwise. Hence,
    the matrix ${\bm{T}}$ and vector ${\bm{t}}$ are as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 给定激活集$\{{\mathbb{S}}^{l}\}_{l\in{\mathbb{L}}}$，它表示神经网络每一层哪些神经元是激活的，我们可以明确描述与相应线性区域${\mathbb{I}}$相关的仿射变换${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$。对于每个激活集${\mathbb{S}}^{l}$，第$l$层定义了如下形式的仿射变换$\Omega^{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$，其中$\Omega^{{\mathbb{S}}^{l}}$是一个对角矩阵$n_{l}\times
    n_{l}$，其中如果$i\in{\mathbb{S}}^{l}$，则$\Omega^{{\mathbb{S}}^{l}}_{ii}=1$，否则$\Omega^{{\mathbb{S}}^{l}}_{ii}=0$。因此，矩阵${\bm{T}}$和向量${\bm{t}}$如下：
- en: '|  | ${\bm{T}}=\prod_{l=1}^{L}\Omega^{{\mathbb{S}}^{l}}{\bm{W}}^{l},$ |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{T}}=\prod_{l=1}^{L}\Omega^{{\mathbb{S}}^{l}}{\bm{W}}^{l},$ |  |'
- en: '|  | ${\bm{t}}=\sum_{l_{1}=1}^{L}\left(\prod_{l_{2}=l_{1}+1}^{L}\Omega^{{\mathbb{S}}^{l_{2}}}{\bm{W}}^{l_{2}}\right)\Omega^{{\mathbb{S}}^{l_{1}}}{\bm{b}}^{l_{1}}.$
    |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{t}}=\sum_{l_{1}=1}^{L}\left(\prod_{l_{2}=l_{1}+1}^{L}\Omega^{{\mathbb{S}}^{l_{2}}}{\bm{W}}^{l_{2}}\right)\Omega^{{\mathbb{S}}^{l_{1}}}{\bm{b}}^{l_{1}}.$
    |  |'
- en: On a side note, Takai et al. ([2021](#bib.bib302)) proposed a related metric
    for networks modeling piecewise linear functions by counting the number of distinct
    functions among linear regions upon equivalence through isometric affine transformation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下，Takai等人（[2021](#bib.bib302)）提出了一种相关的度量，用于通过计数在等距仿射变换下线性区域之间的不同函数数量，以对模型化分段线性函数的网络进行度量。
- en: 'Each linear region is associated with a polyhedron, and we can describe the
    union of polyhedra $\mathcal{D}$ on the space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$
    that covers the entire input space $x$ of the neural network as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线性区域都与一个多面体相关，我们可以如下描述覆盖神经网络整个输入空间$x$的空间$({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$上的多面体的并集$\mathcal{D}$：
- en: '|  | <math   alttext="\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&amp;\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&amp;\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&amp;\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&amp;\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\'
- en: '{\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq 0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '{\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq 0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\'
- en: h_{i}^{l}=0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right)."
    display="block"><semantics ><mrow  ><mrow ><mi >𝒟</mi><mo rspace="0.111em" >=</mo><mrow
    ><munder  ><mo movablelimits="false" rspace="0em"  >⋁</mo><mrow ><mrow ><mo stretchy="false"
    >(</mo><msup  ><mi >𝕊</mi><mn >1</mn></msup><mo >,</mo><mi mathvariant="normal"
    >…</mi><mo  >,</mo><msup ><mi >𝕊</mi><mi  >L</mi></msup><mo stretchy="false"  >)</mo></mrow><mo
    >⊆</mo><mrow ><mrow  ><mo stretchy="false"  >{</mo><mn >1</mn><mo >,</mo><mi mathvariant="normal"
    >…</mi><mo  >,</mo><msub ><mi >n</mi><mn >1</mn></msub><mo rspace="0.055em" stretchy="false"
    >}</mo></mrow><mo rspace="0.222em"  >×</mo><mi mathvariant="normal"  >…</mi><mo
    lspace="0.222em" rspace="0.222em"  >×</mo><mrow ><mo stretchy="false" >{</mo><mn
    >1</mn><mo  >,</mo><mi mathvariant="normal"  >…</mi><mo >,</mo><msub ><mi  >n</mi><mi
    >L</mi></msub><mo stretchy="false"  >}</mo></mrow></mrow></mrow></munder><mrow
    ><mo >(</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
    ><mtd  ><mrow ><mrow ><mrow  ><msubsup ><mi >𝒘</mi><mi >i</mi><mi >l</mi></msubsup><mo
    lspace="0.222em" rspace="0.222em"  >⋅</mo><msup ><mi >𝒉</mi><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msubsup ><mi  >b</mi><mi >i</mi><mi
    >l</mi></msubsup></mrow><mo >≥</mo><mn >0</mn></mrow></mtd><mtd ><mrow  ><mrow
    ><mrow ><mo rspace="0.167em" >∀</mo><mi >l</mi></mrow><mo >∈</mo><mi >𝕃</mi></mrow><mo
    >,</mo><mrow ><mi  >i</mi><mo >∈</mo><msup ><mi >𝕊</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><msubsup ><mi  >h</mi><mi >i</mi><mi >l</mi></msubsup><mo >=</mo><mrow
    ><mrow  ><msubsup ><mi >𝒘</mi><mi >i</mi><mi >l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em"  >⋅</mo><msup ><mi >𝒉</mi><mrow ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></msup></mrow><mo
    >+</mo><msubsup ><mi  >b</mi><mi >i</mi><mi >l</mi></msubsup></mrow></mrow></mtd><mtd
    ><mrow ><mrow  ><mrow ><mo rspace="0.167em" >∀</mo><mi >l</mi></mrow><mo >∈</mo><mi
    >𝕃</mi></mrow><mo >,</mo><mrow ><mi >i</mi><mo >∈</mo><msup ><mi >𝕊</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><mrow ><mrow  ><msubsup ><mi >𝒘</mi><mi >i</mi><mi >l</mi></msubsup><mo
    lspace="0.222em" rspace="0.222em"  >⋅</mo><msup ><mi >h</mi><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msup></mrow><mo >+</mo><msubsup ><mi  >b</mi><mi >i</mi><mi
    >l</mi></msubsup></mrow><mo >≤</mo><mn >0</mn></mrow></mtd><mtd ><mrow  ><mrow
    ><mrow ><mo rspace="0.167em" >∀</mo><mi >l</mi></mrow><mo >∈</mo><mi >𝕃</mi></mrow><mo
    >,</mo><mrow ><mi  >i</mi><mo >∉</mo><msup ><mi >𝕊</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr><mtr
    ><mtd  ><mrow ><msubsup ><mi  >h</mi><mi >i</mi><mi >l</mi></msubsup><mo >=</mo><mn
    >0</mn></mrow></mtd><mtd ><mrow  ><mrow ><mrow ><mo rspace="0.167em" >∀</mo><mi
    >l</mi></mrow><mo >∈</mo><mi >𝕃</mi></mrow><mo >,</mo><mrow ><mi  >i</mi><mo >∉</mo><msup
    ><mi >𝕊</mi><mi >l</mi></msup></mrow></mrow></mtd></mtr></mtable><mo >)</mo></mrow></mrow></mrow><mo
    lspace="0em"  >.</mo></mrow><annotation-xml encoding="MathML-Content" ><apply
    ><ci  >𝒟</ci><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><vector  ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝕊</ci><cn
    type="integer" >1</cn></apply><ci >…</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝕊</ci><ci  >𝐿</ci></apply></vector><apply ><set ><cn type="integer"  >1</cn><ci
    >…</ci><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑛</ci><cn type="integer"
    >1</cn></apply></set><ci >…</ci><set  ><cn type="integer"  >1</cn><ci >…</ci><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑛</ci><ci >𝐿</ci></apply></set></apply></apply></apply><matrix
    ><matrixrow ><apply  ><apply ><apply ><ci  >⋅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝒘</ci><ci >𝑖</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝒉</ci><apply
    ><ci >𝑙</ci><cn type="integer"  >1</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑏</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply></apply><cn type="integer"  >0</cn></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="latexml" >for-all</csymbol><ci >𝑙</ci></apply><ci >𝕃</ci></apply><apply ><ci  >𝑖</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝕊</ci><ci >𝑙</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ℎ</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><apply
    ><apply ><ci  >⋅</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >𝒘</ci><ci >𝑖</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝒉</ci><apply
    ><ci >𝑙</ci><cn type="integer"  >1</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply ><csymbol cd="ambiguous"  >subscript</csymbol><ci
    >𝑏</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply></apply></apply><apply ><csymbol
    cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol cd="latexml"
    >for-all</csymbol><ci >𝑙</ci></apply><ci >𝕃</ci></apply><apply ><ci  >𝑖</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝕊</ci><ci >𝑙</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><apply ><ci >⋅</ci><apply ><csymbol cd="ambiguous" >superscript</csymbol><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝒘</ci><ci >𝑖</ci></apply><ci
    >𝑙</ci></apply><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℎ</ci><apply
    ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply></apply><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑏</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply></apply><cn type="integer" >0</cn></apply><apply
    ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply ><apply ><csymbol
    cd="latexml" >for-all</csymbol><ci >𝑙</ci></apply><ci >𝕃</ci></apply><apply ><ci  >𝑖</ci><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝕊</ci><ci >𝑙</ci></apply></apply></apply></matrixrow><matrixrow
    ><apply ><apply  ><csymbol cd="ambiguous"  >superscript</csymbol><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><ci >ℎ</ci><ci >𝑖</ci></apply><ci >𝑙</ci></apply><cn
    type="integer"  >0</cn></apply><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="latexml" >for-all</csymbol><ci >𝑙</ci></apply><ci >𝕃</ci></apply><apply
    ><ci  >𝑖</ci><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci >𝕊</ci><ci
    >𝑙</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ {\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq
    0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\ h_{i}^{l}=0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right).</annotation></semantics></math>
    |  |
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \(\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ {\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq
    0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\ h_{i}^{l}=0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right).\)
- en: 'Such partitioning entails an overlap between adjacent linear regions when ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    i.e., at the boundary in which unit $i$ in layer $l$ is active in one region and
    inactive in another. Nevertheless, for any input $\overline{{\bm{x}}}$ associated
    with a point at such a boundary between two linear regions ${\mathbb{I}}_{1}$
    and ${\mathbb{I}}_{2}$, it holds that ${\bm{y}}_{{\mathbb{I}}_{1}}(\overline{{\bm{x}}})={\bm{y}}_{{\mathbb{I}}_{2}}(\overline{{\bm{x}}})$
    even if those affine transformations are not entirely identical since the output
    of the neural network is continuous. More importantly, such overlap implies that
    each term of $\mathcal{D}$ is defined using only equalities and nonstrict inequalities,
    and therefore that each linear region corresponds to polyhedra in the extended
    space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$. Consequently, those linear
    regions also define polyhedra if projected to the input space ${\bm{x}}$, since
    by using Fourier-Motzkin elimination (Fourier, [1826](#bib.bib107), Motzkin, [1936](#bib.bib226))
    we can obtain a polyhedral description of the linear region in ${\bm{x}}$. Moreover,
    the interior of those polyhedra are disjoint. If one of those polyhedra does not
    have an interior, which means that it is not full-dimensional, then that linear
    region lies entirely on the boundary of other linear regions. In such a case,
    we do not regard it as a proper linear region. By looking at the geometry of those
    linear regions from a different perspective in Section [3.3](#S3.SS3 "3.3 The
    geometry of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey") and understanding its impact on the
    number of linear regions in Section [3.4](#S3.SS4 "3.4 The number of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), we will see that many terms of $\mathcal{D}$ may actually
    be empty.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '这种分区涉及相邻线性区域之间的重叠，当${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}=0$时，即在第$l$层的单元$i$在一个区域内是激活的而在另一个区域内是非激活的边界处。然而，对于任何与这种边界上的点相关的输入$\overline{{\bm{x}}}$，在两个线性区域${\mathbb{I}}_{1}$和${\mathbb{I}}_{2}$之间，该点满足${\bm{y}}_{{\mathbb{I}}_{1}}(\overline{{\bm{x}}})={\bm{y}}_{{\mathbb{I}}_{2}}(\overline{{\bm{x}}})$，即使这些仿射变换并不完全相同，因为神经网络的输出是连续的。更重要的是，这种重叠意味着$\mathcal{D}$的每一项仅通过等式和非严格不等式定义，因此每个线性区域对应于扩展空间$({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$中的多面体。因此，如果将这些线性区域投影到输入空间${\bm{x}}$，这些线性区域也定义了多面体，因为通过使用傅里叶-莫茨金消元（傅里叶，[1826](#bib.bib107)，莫茨金，[1936](#bib.bib226)）我们可以获得${\bm{x}}$中线性区域的多面体描述。此外，这些多面体的内部是互不重叠的。如果其中一个多面体没有内部，这意味着它不是全维的，那么该线性区域完全位于其他线性区域的边界上。在这种情况下，我们不将其视为一个适当的线性区域。通过从不同的角度查看这些线性区域的几何形状（参见第[3.3](#S3.SS3
    "3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey)节）并理解其对线性区域数量的影响（参见第[3.4](#S3.SS4
    "3.4 The number of linear regions ‣ 3 The Linear Regions of a Neural Network ‣
    When Deep Learning Meets Polyhedral Theory: A Survey)节），我们会发现$\mathcal{D}$的许多项实际上可能是空的。'
- en: 'The optimization over the union of polyhedra is the subject of disjunctive
    programming, which has contributed to the development of stronger formulations
    and better algorithms to solve discrete optimization problems. These are formulated
    as MILPs as well as more general types of problems in recent years (Balas, [2018](#bib.bib12)),
    including generalized disjunctive programming for Mixed-Integer Non-Linear Programming (MINLP)
    (Raman and Grossmann, [1994](#bib.bib256), Grossmann and Ruiz, [2012](#bib.bib134)).
    One of such contributions is the generation of valid inequalities to strengthen
    MILP formulations, which are also denoted as cutting planes, through the lift-and-project
    technique (Balas et al., [1993](#bib.bib13), [1996](#bib.bib14)). In fact, we
    can develop stronger formulations for optimization problems involving neural networks
    through the lenses of disjunctive programming, as we discuss later in Section [4.2](#S4.SS2
    "4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '对多面体并集的优化是离散规划的主题，这有助于开发更强的数学模型和更好的算法来解决离散优化问题。这些问题在近年来被表述为混合整数线性规划（MILPs）以及更一般类型的问题（Balas,
    [2018](#bib.bib12)），包括针对混合整数非线性规划（MINLP）的广义离散规划（Raman and Grossmann, [1994](#bib.bib256),
    Grossmann and Ruiz, [2012](#bib.bib134)）。其中一种贡献是通过升维与投影技术生成有效不等式以强化MILP模型，这些也被称为割平面（Balas
    et al., [1993](#bib.bib13), [1996](#bib.bib14)）。实际上，我们可以通过离散规划的视角开发针对神经网络的优化问题的更强模型，我们将在第[4.2](#S4.SS2
    "4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")节中进一步讨论。'
- en: 3.3 The geometry of linear regions
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 线性区域的几何
- en: Another form of looking at the geometry of linear regions is through their transformation
    along the layers of the neural network. Namely, we can think of the input space
    as initially being partitioned by the units of the first layer, and then each
    resulting linear region being further partitioned by the subsequent layers. In
    that sense, we can think of every layer as a particular form of “slicing” the
    input. In fact, a layer may slice each linear region that is defined by the preceding
    layer in a different way due to which neurons are active or not in previous layers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种观察线性区域几何的方法是通过其沿神经网络层的变换。即，我们可以将输入空间视为最初由第一层的单元进行划分，然后每个结果线性区域进一步被后续层划分。从这个意义上讲，我们可以将每一层看作是对输入的某种“切片”。实际上，由于前一层中哪些神经元处于激活状态或未激活状态，一层可能以不同的方式切分由前一层定义的每个线性区域。
- en: 'Let us begin by illustrating how a given layer $l\in{\mathbb{L}}$ partitions
    its input space ${\bm{h}}^{l-1}$. Every neuron $i$ in layer $l$ is associated
    with an *activation hyperplane* of the form ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    which divides the possible inputs of its layer into an open half-space in which
    the unit is active (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}>0$) and a closed
    half-space in which the unit is inactive (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\leq
    0$). These hyperplanes define the boundary between adjacent linear regions, and
    the arrangement of such hyperplanes for a given layer $l\in{\mathbb{L}}$ determines
    how that layer partitions the ${\bm{h}}^{l-1}$ space. In other words, every input
    in ${\bm{h}}^{l-1}$ can be located with respect to each of those hyperplanes,
    which corresponds to the activation set of the linear region to which it belongs.
    However, not every activation set out of the $2^{n_{l}}$ possible ones maps to
    a nonempty region of the input space. In the case of Example [2](#Thmexample2
    "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), there is no
    linear region in which the activation set is empty.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们首先说明给定层 $l\in{\mathbb{L}}$ 如何划分其输入空间 ${\bm{h}}^{l-1}$。层 $l$ 中的每个神经元 $i$
    都与形式为 ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}=0$ 的*激活超平面*相关联，该超平面将其层的可能输入划分为一个单位激活的开半空间（${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}>0$）和一个单位未激活的闭半空间（${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\leq
    0$）。这些超平面定义了相邻线性区域之间的边界，给定层 $l\in{\mathbb{L}}$ 的这些超平面的排列决定了该层如何划分 ${\bm{h}}^{l-1}$
    空间。换句话说，${\bm{h}}^{l-1}$ 中的每个输入都可以相对于这些超平面进行定位，这对应于其所属线性区域的激活集。然而，并不是所有 $2^{n_{l}}$
    个可能的激活集都映射到输入空间的非空区域。在示例 [2](#Thmexample2 "Example 2 ‣ 3.3 The geometry of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey") 中，没有任何线性区域的激活集是空的。'
- en: Example 2
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 2
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and a single
    layer having 3 neurons $\alpha$, $\beta$, and $\gamma$ with outputs given as follows:
    $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+0.6,0\}$, $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+4.8,0\}$,
    and $h^{1}_{\gamma}=\max\{0x_{1}+3x_{2}-5,0\}$. These neurons define the activation
    hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+0.6=0$, ($\beta$) $-0.9x_{1}-0.7x_{2}+4.8=0$,
    and ($\gamma$) $0x_{1}+3x_{2}-5=0$ in the space ${\bm{x}}$, which are illustrated
    along with the activation sets of the linear regions in Figure [5](#S3.F5 "Figure
    5 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑一个神经网络，其定义域为 ${\bm{x}}\in\mathbb{R}^{2}$，并且有一个包含 3 个神经元 $\alpha$、$\beta$
    和 $\gamma$ 的单层，输出如下：$h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+0.6,0\}$，$h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+4.8,0\}$，$h^{1}_{\gamma}=\max\{0x_{1}+3x_{2}-5,0\}$。这些神经元定义了激活超平面（$\alpha$）
    $2.3x_{1}-1.9x_{2}+0.6=0$，（$\beta$） $-0.9x_{1}-0.7x_{2}+4.8=0$，和（$\gamma$） $0x_{1}+3x_{2}-5=0$，这些超平面在空间
    ${\bm{x}}$ 中绘制，并且与线性区域的激活集一起在图 [5](#S3.F5 "Figure 5 ‣ 3.3 The geometry of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey") 中展示。'
- en: Instead of $2^{3}$ linear regions corresponding to each possible activation
    set defined by a subset of the neurons in $\{\alpha,\beta,\gamma\}$, the arrangement
    of such hyperplanes produces 7 linear regions, which is in fact the maximum number
    of 2-dimensional regions that can be defined by drawing 3 lines on a plane.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与每个由神经元子集 $\{\alpha,\beta,\gamma\}$ 定义的可能激活集对应的 $2^{3}$ 线性区域不同，这些超平面的排列产生了 7
    个线性区域，这实际上是通过在平面上绘制 3 条直线可以定义的最大二维区域数。
- en: '![Refer to caption](img/275229d48fa5bb1b250445d5df8fa0b0.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/275229d48fa5bb1b250445d5df8fa0b0.png)'
- en: 'Figure 5: Linear regions defined by the shallow neural network described in
    Example [2](#Thmexample2 "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The
    Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey"). Every line corresponds to the activation hyperplane of a different
    neuron, which is given by $\alpha$, $\beta$, and $\gamma$ in parentheses. The
    arrow next to each line points to the half space in which the inputs activate
    that neuron. Every linear region has a subset of $\{\alpha,\beta,\gamma\}$ as
    its corresponding activation set.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：由示例 [2](#Thmexample2 "Example 2 ‣ 3.3 The geometry of linear regions ‣
    3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") 中描述的浅层神经网络定义的线性区域。每条直线对应于一个不同神经元的激活超平面，这些神经元由括号中的 $\alpha$、$\beta$
    和 $\gamma$ 给出。每条直线旁边的箭头指向激活该神经元的输入所在的半空间。每个线性区域都有 $\{\alpha,\beta,\gamma\}$ 的一个子集作为其对应的激活集。'
- en: 'The maximum number of full-dimensional regions resulting from a partitioning
    defined by $n$ hyperplanes depends on the dimension $d$ of the space in which
    those hyperplanes are defined (Zaslavsky, [1975](#bib.bib352)). That number never
    exceeds $\sum\limits_{i=1}^{\min\{d,n\}}\binom{n}{i}$. Such bound only coincides
    with $2^{n}$ if $d\geq n$; otherwise, as illustrated in Example [2](#Thmexample2
    "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), that number
    can be smaller. As observed by Hanin and Rolnick ([2019b](#bib.bib138)), that
    bound is $O\left(\frac{n^{d}}{d!}\right)$ when $n\gg d$.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '从 $n$ 个超平面定义的分区中得到的全维区域的最大数量取决于这些超平面定义的空间的维度 $d$（Zaslavsky, [1975](#bib.bib352)）。这个数量从不超过
    $\sum\limits_{i=1}^{\min\{d,n\}}\binom{n}{i}$。只有当 $d\geq n$ 时，这个界限才与 $2^{n}$ 相符；否则，如示例 [2](#Thmexample2
    "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") 中所示，该数量可能更小。正如
    Hanin 和 Rolnick ([2019b](#bib.bib138)) 所观察到的，当 $n\gg d$ 时，该界限为 $O\left(\frac{n^{d}}{d!}\right)$。'
- en: In fact, the above bound is all that we need to determine the maximum number
    of linear regions in shallow networks. While not every shallow network may define
    as many linear regions, it is always possible to put the hyperplanes in what is
    called a *general position* in order to reach that bound. Thus, the maximum number
    of linear regions defined by a shallow network is $\sum\limits_{i=0}^{\min\{n_{0},n_{1}\}}\binom{n_{1}}{n_{0}}$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，上述界限是我们确定浅层网络中线性区域最大数量所需的全部信息。虽然并非每个浅层网络都能定义如此多的线性区域，但总是可以将超平面放置在所谓的 *一般位置*
    来达到该界限。因此，浅层网络定义的线性区域的最大数量是 $\sum\limits_{i=0}^{\min\{n_{0},n_{1}\}}\binom{n_{1}}{n_{0}}$。
- en: For the polyhedron associated with each linear region, being in general position
    implies that each vertex lies on exactly $d$ activation hyperplanes. For context,
    the converse situation in linear programming —having more hyperplanes active on
    a vertex than the space dimension— characterizes degeneracy.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与每个线性区域相关的多面体，处于一般位置意味着每个顶点恰好位于$d$个激活超平面上。作为背景，线性规划中的逆情况——顶点上有超过空间维度的激活超平面——则特征化为退化。
- en: 'In the case of deep networks, the partitioning of each linear region by the
    subsequent layers is based on the output of that linear region. This affects the
    shape and the number of the linear regions defined by the following layers, which
    may vary between adjacent linear regions due to which units are active or inactive
    from one linear region to another, as illustrated in Example [3](#Thmexample3
    "Example 3 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '在深度网络的情况下，后续层对每个线性区域的划分是基于该线性区域的输出。这影响了后续层定义的线性区域的形状和数量，由于从一个线性区域到另一个线性区域的激活或非激活单元，这些线性区域之间的变化可能会有所不同，如示例[3](#Thmexample3
    "Example 3 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")所示。'
- en: Example 3
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 3
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and 2 layers
    having 2 neurons each —say neurons $\alpha$ and $\beta$ in layer 1, and neurons
    $\gamma$ and $\delta$ in layer 2— with outputs given as follows: $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+1.5,0\}$,
    $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+5,0\}$, $h^{2}_{\gamma}=\max\{0.4h^{1}_{1}-3.1h^{1}_{2}+4,0\}$,
    $h^{2}_{\delta}=\max\{-0.6h^{1}_{1}-1.6h^{1}_{2}+5,0\}$. These neurons define
    the activation hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+1.5=0$ and ($\beta$)
    $-0.9x_{1}-0.7x_{2}+5=0$ in the ${\bm{x}}$ space and the activation hyperplanes
    ($\gamma$) $0.4h^{1}_{1}-3.1h^{1}_{2}+4=0$ and ($\delta$) $-0.6h^{1}_{1}-1.6h^{1}_{2}+5=0$
    in the ${\bm{h}}^{1}$ space, which are illustrated along with the activation sets
    of the linear regions in the first two plots of Figure [6](#S3.F6 "Figure 6 ‣
    3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). The third plot illustrates
    the linear regions jointly defined by the two layers in terms of the input space
    ${\bm{x}}$.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑一个域为${\bm{x}}\in\mathbb{R}^{2}$的神经网络，其中有2层，每层各有2个神经元——例如，第1层的神经元$\alpha$和$\beta$，第2层的神经元$\gamma$和$\delta$——其输出如下：$h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+1.5,0\}$，$h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+5,0\}$，$h^{2}_{\gamma}=\max\{0.4h^{1}_{1}-3.1h^{1}_{2}+4,0\}$，$h^{2}_{\delta}=\max\{-0.6h^{1}_{1}-1.6h^{1}_{2}+5,0\}$。这些神经元在${\bm{x}}$空间中定义了激活超平面（$\alpha$）$2.3x_{1}-1.9x_{2}+1.5=0$和（$\beta$）$-0.9x_{1}-0.7x_{2}+5=0$，在${\bm{h}}^{1}$空间中定义了激活超平面（$\gamma$）$0.4h^{1}_{1}-3.1h^{1}_{2}+4=0$和（$\delta$）$-0.6h^{1}_{1}-1.6h^{1}_{2}+5=0$，这些在图[6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")的前两个图中与线性区域的激活集合一起展示。第三个图说明了由两层共同定义的输入空间${\bm{x}}$中的线性区域。'
- en: 'The third plot is repeated in Figure [7](#S3.F7 "Figure 7 ‣ 3.3 The geometry
    of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"), in which the shape of each linear region
    ${\mathbb{I}}$ is filled in accordance to the dimension of the image of $\bar{{\bm{y}}}_{{\mathbb{I}}}({\bm{x}})$
    —the output of the neural network for each linear region ${\mathbb{I}}$.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '第三个图在图[7](#S3.F7 "Figure 7 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear
    Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")中重复出现，其中每个线性区域${\mathbb{I}}$的形状根据$\bar{{\bm{y}}}_{{\mathbb{I}}}({\bm{x}})$的图像维度进行填充——即神经网络对每个线性区域${\mathbb{I}}$的输出。'
- en: '![Refer to caption](img/d4b21733c24f690a28025ee4d147bf0d.png)![Refer to caption](img/66f9cc2c9bd7fbe539dcffe16058d0b5.png)![Refer
    to caption](img/7fc94c543268674eb63d5ca496258400.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d4b21733c24f690a28025ee4d147bf0d.png)![参见标题](img/66f9cc2c9bd7fbe539dcffe16058d0b5.png)![参见标题](img/7fc94c543268674eb63d5ca496258400.png)'
- en: 'Figure 6: Linear regions defined by the 2 layers of the neural network described
    in Example [3](#Thmexample3 "Example 3 ‣ 3.3 The geometry of linear regions ‣
    3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), following the same notation as in Figure [5](#S3.F5 "Figure
    5 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). The first and second
    plots show the linear regions and corresponding activation sets defined by the
    first and the second layers in terms of their input spaces (${\bm{x}}$ and ${\bm{h}}^{1}$).
    The third plot shows the linear regions defined by the combination of the 2 layers
    and the union of their activation sets in terms of the input space of the first
    layer (${\bm{x}}$).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：由示例 [3](#Thmexample3 "示例 3 ‣ 3.3 线性区域的几何 ‣ 3 神经网络的线性区域 ‣ 深度学习与多面体理论相遇：一项调查")
    中描述的神经网络的 2 层定义的线性区域，遵循图 [5](#S3.F5 "图 5 ‣ 3.3 线性区域的几何 ‣ 3 神经网络的线性区域 ‣ 深度学习与多面体理论相遇：一项调查")
    中的相同符号。第一个和第二个图显示了由第一层和第二层在其输入空间 (${\bm{x}}$ 和 ${\bm{h}}^{1}$) 定义的线性区域及相应的激活集。第三个图显示了由这
    2 层组合定义的线性区域及其激活集的并集，基于第一层的输入空间 (${\bm{x}}$)。
- en: 'Example [3](#Thmexample3 "Example 3 ‣ 3.3 The geometry of linear regions ‣
    3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") highlights two important aspects about the structure of linear
    regions in deep neural networks. First, the linear regions defined by a neural
    network with multiple layers are different because activation hyperplanes after
    the first layer may look “bent” from the input space $x$, such as with the inflections
    of hyperplanes $(\gamma)$ and $(\delta)$ in the third plot of Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") from one linear
    region defined by the first layer to another. This partitioning of the input space
    would not be possible with a single layer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 示例 [3](#Thmexample3 "示例 3 ‣ 3.3 线性区域的几何 ‣ 3 神经网络的线性区域 ‣ 深度学习与多面体理论相遇：一项调查")
    突出了深度神经网络中线性区域结构的两个重要方面。首先，由多层神经网络定义的线性区域是不同的，因为在第一层之后的激活超平面可能看起来从输入空间 $x$ 中“弯曲”，例如，图 [6](#S3.F6
    "图 6 ‣ 3.3 线性区域的几何 ‣ 3 神经网络的线性区域 ‣ 深度学习与多面体理论相遇：一项调查") 的第三个图中的超平面 $(\gamma)$ 和
    $(\delta)$ 从第一层定义的一个线性区域到另一个线性区域。这种输入空间的划分在单层网络中是不可能实现的。
- en: 'By comparing side by side the first and the third plots of Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), we can see how
    every linear region of a given layer of a neural network may be partitioned differently
    by the following layer. When defined in terms of the input space ${\bm{x}}$, the
    hyperplanes associated with the second layer differ across the linear regions
    defined by the first layer because each of those linear regions is associated
    with a different affine transformation from ${\bm{x}}$ to ${\bm{h}}^{1}$. Hence,
    the activation hyperplanes of layer $l$ may break each linear region from layer
    $l-1$ differently. To every linear region defined by the hyperplane arrangement
    in the ${\bm{h}}^{l-1}$ space there is a linear transformation ${\bm{h}}^{l-1}=\Omega^{{\mathbb{S}}^{l-1}}({\bm{W}}^{l-1}{\bm{h}}^{l-2}+{\bm{b}}^{l-1})$
    to the points of that linear region based on the set of active neurons ${\mathbb{S}}^{l-1}$.
    Consequently, inputs in the ${\bm{h}}^{l-1}$ space that are associated with different
    linear regions are transformed differently to the ${\bm{h}}^{l}$ space, and therefore
    the form in which those linear regions are further partitioned by layer $l$ is
    not the same when seen from the ${\bm{h}}^{l-1}$ space.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '通过并排比较图[6](#S3.F6 "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear
    Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")中的第一幅和第三幅图，我们可以看到神经网络中给定层的每个线性区域是如何被下一层不同地划分的。当以输入空间${\bm{x}}$来定义时，与第二层相关的超平面在第一层定义的线性区域之间有所不同，因为这些线性区域中的每一个都与从${\bm{x}}$到${\bm{h}}^{1}$的不同仿射变换相关。因此，第$l$层的激活超平面可能以不同的方式打破第$l-1$层的每个线性区域。对于${\bm{h}}^{l-1}$空间中的每个由超平面排列定义的线性区域，都有一个线性变换${\bm{h}}^{l-1}=\Omega^{{\mathbb{S}}^{l-1}}({\bm{W}}^{l-1}{\bm{h}}^{l-2}+{\bm{b}}^{l-1})$，该变换基于激活神经元集合${\mathbb{S}}^{l-1}$将该线性区域的点进行变换。因此，${\bm{h}}^{l-1}$空间中与不同线性区域相关的输入被不同地转换到${\bm{h}}^{l}$空间，因此，这些线性区域在第$l$层进一步划分的形式在从${\bm{h}}^{l-1}$空间来看时并不相同。'
- en: 'Second, some combinations of activation sets of multiple layers do not correspond
    to linear regions even if the activation hyperplanes are in general position with
    respect to each layer. For each layer, the first two plots of Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") show that every
    activation set corresponds to a nonempty region of the layer input. However, not
    every pair of such activation sets would define a nonempty linear region for the
    neural network. For example, the linear region of the first layer associated with
    the activation set ${\mathbb{S}}^{1}=\{\}$ defines a linear region in ${\bm{x}}$
    which is always mapped to ${\bm{h}}^{1}=0$, and thus only corresponds to activation
    set ${\mathbb{S}}^{2}=\{\gamma,\delta\}$ in the second layer because both units
    are active for such input. Thus, no linear region in ${\bm{x}}$ is associated
    with only the units in sets $\{\},\{\gamma\}$, and $\{\delta\}$ being active —i.e.,
    there is no linear region such that ${\mathbb{S}}^{1}\cup{\mathbb{S}}^{2}=\{\},\{\gamma\},\text{or}\{\delta\}$.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，即使激活超平面在每一层上与一般位置相对应，多个层的激活集的某些组合也不会对应于线性区域。对于每一层，图[6](#S3.F6 "Figure 6
    ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")的前两幅图显示了每个激活集对应于该层输入的一个非空区域。然而，并非所有这样的激活集对都能定义一个神经网络的非空线性区域。例如，第一层与激活集${\mathbb{S}}^{1}=\{\}$相关的线性区域在${\bm{x}}$中定义了一个线性区域，该区域始终映射到${\bm{h}}^{1}=0，因此只对应于第二层中的激活集${\mathbb{S}}^{2}=\{\gamma,\delta\}$，因为对于这样的输入，两者都处于激活状态。因此，在${\bm{x}}$中，没有线性区域仅与集合$\{\},\{\gamma\}$和$\{\delta\}$中的单元处于激活状态相关——即，没有线性区域使得${\mathbb{S}}^{1}\cup{\mathbb{S}}^{2}=\{\},\{\gamma\},\text{或}\{\delta\}$。'
- en: More generally, the number of units that is active on each linear region defined
    by the first layer also imposes a geometric limit to how that linear region can
    be further partitioned. If only one unit is active at a layer, that means that
    the output of the layer within that linear region has dimension 1, and, consequently,
    the subsequent hyperplane arrangements within that linear region are limited to
    a 1-dimensional space. For the network in the example, we thus expect no more
    than $\sum_{i=0}^{1}\binom{2}{i}=3$ linear regions being defined instead of $2^{2}=4$
    when only one unit is active. In fact, that is precisely the number of subdivisions
    by the second layer of the linear region defined by activation set ${\mathbb{S}}^{1}=\{\beta\}$
    from the first layer.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，第一个层定义的每个线性区域上激活的单元数也对该线性区域如何进一步划分施加了几何限制。如果一个层上只有一个单元激活，那意味着该线性区域内的层输出维度为1，因此，该线性区域内后续超平面排列被限制在1维空间内。对于示例中的网络，因此我们期望不超过$\sum_{i=0}^{1}\binom{2}{i}=3$个线性区域被定义，而不是$2^{2}=4$，当只有一个单元激活时。实际上，这正是第二层根据第一个层的激活集${\mathbb{S}}^{1}=\{\beta\}$对线性区域进行细分的数量。
- en: '![Refer to caption](img/738562b7813b118f7f87ea0943be8d01.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/738562b7813b118f7f87ea0943be8d01.png)'
- en: 'Figure 7: Dimension of the image of the affine function ${\bm{y}}_{{\mathbb{I}}}({\bm{x}})$
    associated with each linear region ${\mathbb{I}}$ defined by the neural network
    described in Example [3](#Thmexample3 "Example 3 ‣ 3.3 The geometry of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"). The linear regions are the same illustrated in
    the third plot of Figure [6](#S3.F6 "Figure 6 ‣ 3.3 The geometry of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：与神经网络中示例[3](#Thmexample3 "示例 3 ‣ 3.3 线性区域的几何 ‣ 3 神经网络的线性区域 ‣ 深度学习与多面体理论相遇：综述")定义的每个线性区域${\mathbb{I}}$相关的仿射函数${\bm{y}}_{{\mathbb{I}}}({\bm{x}})$的图像维度。线性区域与图[6](#S3.F6
    "图 6 ‣ 3.3 线性区域的几何 ‣ 3 神经网络的线性区域 ‣ 深度学习与多面体理论相遇：综述")中第三个图示的区域相同。
- en: 'For every linear region defined by layer $l$ with an activation set ${\mathbb{S}}^{l}$,
    the dimension of the output of the corresponding transformation $\Omega_{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$
    is at most $|{\mathbb{S}}^{l}|$ since $\text{rank}(\Omega_{{\mathbb{S}}^{l}})=|{\mathbb{S}}^{l}|$.
    Hence, the dimension of the output of every linear region defined by a rectifier
    network is upper bounded by its smallest activation set across all layers. This
    phenomenon was first identified by Serra et al. ([2018](#bib.bib282)) as the *bottleneck
    effect*. In neural networks with uniform width, this phenomenon leads to a surprising
    consequence: the number of linear regions with full-dimensional output is at most
    one. There are also consequences to the maximum number of linear regions that
    can be defined, as we discuss later.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由层$l$定义的每个线性区域，其激活集为${\mathbb{S}}^{l}$，相应变换$\Omega_{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$的输出维度至多为$|{\mathbb{S}}^{l}|$，因为$\text{rank}(\Omega_{{\mathbb{S}}^{l}})=|{\mathbb{S}}^{l}|$。因此，由整流网络定义的每个线性区域的输出维度由其在所有层中的最小激活集上限。这个现象首次由Serra等人([2018](#bib.bib282))识别为*瓶颈效应*。在宽度均匀的神经网络中，这一现象导致了一个令人惊讶的结果：具有全维输出的线性区域最多只有一个。关于可以定义的最大线性区域数量，还有其他后果，我们将在后面讨论。
- en: 3.3.1 The geometry of decision regions
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 决策区域的几何
- en: It is also common to study what inputs are associated with each class by a neural
    network. The set of inputs associated with the same class define a *decision region*.
    Difficulties in modeling functions such as the Boolean XOR in shallow networks
    are related to limitations on the form of the decision regions, which may be limited
    by the depth of the neural network. For example, Makhoul et al. ([1989](#bib.bib207))
    showed that two layers suffice to obtain disconnected decision regions.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 研究神经网络与每个类别相关的输入也很常见。与同一类别相关的输入集合定义了一个*决策区域*。例如，Makhoul等人([1989](#bib.bib207))显示，两层足以获得不连通的决策区域。
- en: The softmax layer is typically used for the output of neural networks trained
    on classification problems, in which the largest output corresponds to the class
    to which the input is associated. In rectifier networks coupled with a softmax
    layer, the decision regions can also be defined by polyhedra. Although the output
    of the softmax layer is not piecewise linear, its largest output corresponds to
    its largest input. Hence, every linear region ${\mathbb{I}}$ defined by layers
    1 to $L-1$ is partitioned by the softmax layer into decision regions where ${\bm{h}}^{L-1}_{i}\geq{\bm{h}}^{L-1}_{j}~{}\forall
    j\neq i$ for each class $i$ associated with the input ${\bm{h}}_{i}^{L-1}$ to
    the softmax layer. Therefore, each decision region of a rectifier networks consist
    of a union of polyhedra.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax 层通常用于训练有分类问题的神经网络的输出，其中最大输出对应于输入关联的类别。在与 softmax 层耦合的整流器网络中，决策区域也可以由多面体定义。尽管
    softmax 层的输出不是分段线性的，但其最大输出对应于其最大输入。因此，由第 1 层到第 $L-1$ 层定义的每个线性区域 ${\mathbb{I}}$
    被 softmax 层划分为决策区域，其中每个与输入 ${\bm{h}}_{i}^{L-1}$ 相关的类别 $i$ 满足 ${\bm{h}}^{L-1}_{i}\geq{\bm{h}}^{L-1}_{j}~{}\forall
    j\neq i$。因此，整流器网络的每个决策区域都由多面体的并集组成。
- en: 'In fact, we may say further in the typical setting where no hidden layer is
    wider than the input —i.e., $n_{0}\geq n_{l}~{}\forall l\in{\mathbb{L}}$: Nguyen
    et al. ([2018](#bib.bib233)) showed that at least one layer $l\in{\mathbb{L}}$
    must be such that $n_{l}>n_{0}$ for the network to present disconnected decision
    regions; and Grigsby and Lindsey ([2022](#bib.bib131)) showed that, for an input
    size $n_{0}\geq 2$, the decision regions are either empty or unbounded.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以进一步说，在没有任何隐藏层宽度超过输入层的典型设置中——即 $n_{0}\geq n_{l}~{}\forall l\in{\mathbb{L}}$：Nguyen
    等人 ([2018](#bib.bib233)) 表明，至少有一个层 $l\in{\mathbb{L}}$ 必须满足 $n_{l}>n_{0}$，以使网络呈现出断裂的决策区域；而
    Grigsby 和 Lindsey ([2022](#bib.bib131)) 证明，对于输入大小 $n_{0}\geq 2$，决策区域要么为空，要么是无界的。
- en: 3.4 The number of linear regions
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 线性区域的数量
- en: 'We have seen conditions that affect the number of linear regions both positively
    and negatively. We discuss these and other analytical results in Section [3.4.1](#S3.SS4.SSS1
    "3.4.1 Analytical results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"),
    and then discuss work on counting linear regions in practice in Section [3.4.2](#S3.SS4.SSS2
    "3.4.2 Counting linear regions ‣ 3.4 The number of linear regions ‣ 3 The Linear
    Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '我们已经看到影响线性区域数量的条件，这些条件既有正面影响也有负面影响。我们在第[3.4.1节](#S3.SS4.SSS1 "3.4.1 Analytical
    results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")中讨论了这些条件以及其他分析结果，然后在第[3.4.2节](#S3.SS4.SSS2
    "3.4.2 Counting linear regions ‣ 3.4 The number of linear regions ‣ 3 The Linear
    Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")中讨论了实际计算线性区域的工作。'
- en: 3.4.1 Analytical results
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 分析结果
- en: At least three lines of work on analytical results have brought important insights.
    The first line is based on constructing networks with a large number of linear
    regions, which leads to lower bounds on the maximum number of linear regions.
    The second line is based on showing how the network architecture —in particular
    its hyperparameters— may impact the hyperplane arrangements defined by the layers,
    which leads to upper bounds on the maximum number of linear regions. The third
    line is based on characterizing the parameters of neural networks according to
    how they are initialized and updated along training, which leads to results on
    the expected number of linear regions for such networks.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有三条分析结果的工作带来了重要的见解。第一条工作基于构建具有大量线性区域的网络，这导致了线性区域最大数量的下界。第二条工作基于展示网络架构——特别是其超参数——如何影响由层定义的超平面排列，这导致了线性区域最大数量的上界。第三条工作基于根据神经网络的初始化和训练过程中更新参数的方式来表征这些网络的参数，这导致了关于这些网络预期线性区域数量的结果。
- en: Lower bounds
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 下界
- en: 'The lower bounds on the maximum number of linear regions are obtained through
    a careful choice of network parameters aimed at increasing the number of linear
    regions. In some cases, they also depend on particular choices of hyperparameters.
    We present them by order of refinement in Table [2](#S3.T2 "Table 2 ‣ Lower bounds
    ‣ 3.4.1 Analytical results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '最大线性区域数量的下界通过仔细选择网络参数来获得，旨在增加线性区域的数量。在某些情况下，它们还依赖于特定的超参数选择。我们在表 [2](#S3.T2
    "Table 2 ‣ Lower bounds ‣ 3.4.1 Analytical results ‣ 3.4 The number of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey") 中按精细程度顺序展示了它们。'
- en: 'The first lower bound was introduced by Pascanu et al. ([2014](#bib.bib243))
    and then improved by those authors with a new construction technique in Montúfar
    et al. ([2014](#bib.bib224)). In fact, Example [1](#Thmexample1 "Example 1 ‣ 3.1
    The combinatorial aspect of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") shows the case
    in which $n_{0}=1$ for the technique in Montúfar et al. ([2014](#bib.bib224)).
    While a different construction is proposed by Telgarsky ([2015](#bib.bib304)),
    subsequent developments in the literature have been based on Montúfar et al. ([2014](#bib.bib224)).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '第一个下界由Pascanu 等人 ([2014](#bib.bib243)) 提出，后来由Montúfar 等人 ([2014](#bib.bib224))
    使用新的构造技术进行了改进。实际上，示例 [1](#Thmexample1 "Example 1 ‣ 3.1 The combinatorial aspect
    of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey") 展示了 $n_{0}=1$ 的情况下，Montúfar 等人 ([2014](#bib.bib224))
    的技术。虽然Telgarsky ([2015](#bib.bib304)) 提出了不同的构造方法，但后续文献的发展仍基于Montúfar 等人 ([2014](#bib.bib224))
    的工作。'
- en: 'The lower bound by Arora et al. ([2018](#bib.bib8)) is based on a different
    technique to construct a first wide layer based on zonotopes, which is then followed
    by the same layers as in Montúfar et al. ([2014](#bib.bib224)). The first lower
    bound by Serra et al. ([2018](#bib.bib282)) reflects a slight change to the technique
    used by Montúfar et al. ([2014](#bib.bib224)), which in terms of Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") corresponds
    to using $n$ neurons to define $n+1$ instead of $n$ slopes on $[0,1]$. The second
    lower bound by Serra et al. ([2018](#bib.bib282)) extends that of Arora et al.
    ([2018](#bib.bib8)) by changing in the same way the construction of the subsequent
    layers of the network.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 'Arora 等人 ([2018](#bib.bib8)) 的下界基于一种不同的技术，即基于zonotopes构造第一层宽层，然后紧接着使用与Montúfar
    等人 ([2014](#bib.bib224)) 相同的层。Serra 等人 ([2018](#bib.bib282)) 的第一个下界反映了对Montúfar
    等人 ([2014](#bib.bib224)) 技术的轻微变化，这在示例 [1](#Thmexample1 "Example 1 ‣ 3.1 The combinatorial
    aspect of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey") 中相当于使用 $n$ 个神经元来定义 $n+1$ 个而非 $n$
    个斜率在 $[0,1]$ 上。Serra 等人 ([2018](#bib.bib282)) 的第二个下界通过以相同的方式改变网络后续层的构造，扩展了Arora
    等人 ([2018](#bib.bib8)) 的结果。'
- en: 'Table 2: Lower bounds on the maximum number of linear regions defined by a
    neural network.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：神经网络定义的线性区域最大数量的下界。
- en: '| Reference | Bound and conditions |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 下界及条件 |'
- en: '| --- | --- |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  Pascanu et al. ([2014](#bib.bib243)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  Pascanu 等人 ([2014](#bib.bib243)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$
    |'
- en: '|  Montúfar et al. ([2014](#bib.bib224)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  Montúfar 等人 ([2014](#bib.bib224)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$，其中
    $n_{l}\geq n_{0}~{}\forall l\in{\mathbb{L}}$ |'
- en: '|  Telgarsky ([2015](#bib.bib304)) | $2^{\frac{L-3}{2}}$, where $n_{i}=1$ for
    $i$ odd, $n_{i}=2$ for $i$ even, and $L-3$ divides by 2 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  Telgarsky ([2015](#bib.bib304)) | $2^{\frac{L-3}{2}}$，其中 $n_{i}=1$ 对于奇数
    $i$，$n_{i}=2$ 对于偶数 $i$，且 $L-3$ 能被2整除 |'
- en: '|  Arora et al. ([2018](#bib.bib8)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}w^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  Arora 等人 ([2018](#bib.bib8)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}w^{L-1}$，其中
    $2m=n_{1}$ 且 $w=n_{l}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
- en: '|  Serra et al. ([2018](#bib.bib282)) | $\left(\prod\limits_{l=1}^{L-1}\left(\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor+1\right)^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Serra 等人 ([2018](#bib.bib282)) | $\left(\prod\limits_{l=1}^{L-1}\left(\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor+1\right)^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$，其中
    $n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}$ |'
- en: '|  Serra et al. ([2018](#bib.bib282)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}(w+1)^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Serra 等人 ([2018](#bib.bib282)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}(w+1)^{L-1}$，其中
    $2m=n_{1}$ 和 $w=n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
- en: Upper bounds
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 上界
- en: 'The upper bounds on the maximum number of linear regions are obtained by primarily
    considering changes to the geometry of the linear regions from one layer to another,
    as previously outlined and revisited below. We present those with a close form
    by order of refinement in Table [3](#S3.T3 "Table 3 ‣ Upper bounds ‣ 3.4.1 Analytical
    results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '最大线性区域数量的上界主要通过考虑从一层到另一层线性区域几何形状的变化来获得，如下所述并再次回顾。我们在表 [3](#S3.T3 "Table 3 ‣
    Upper bounds ‣ 3.4.1 Analytical results ‣ 3.4 The number of linear regions ‣ 3
    The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey") 中按精细化顺序展示了这些上界的闭式形式。'
- en: Pascanu et al. ([2014](#bib.bib243)) established the connection between linear
    regions and hyperplane arrangements, which lead to the tight bound for shallow
    networks based on Zaslavsky ([1975](#bib.bib352)) for activation hyperplanes in
    general position. Montúfar et al. ([2014](#bib.bib224)) defined the first bound
    for deep networks based on enumerating all activation sets. The subsequent upper
    bounds extended the result by Pascanu et al. ([2014](#bib.bib243)) to deep networks
    by considering its successive application through the sequence of layers of the
    network.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Pascanu 等人 ([2014](#bib.bib243)) 建立了线性区域与超平面排列之间的联系，这导致了基于 Zaslavsky ([1975](#bib.bib352))
    的浅层网络的紧界限，用于一般位置的激活超平面。Montúfar 等人 ([2014](#bib.bib224)) 定义了深层网络的第一个界限，基于枚举所有激活集合。随后，上界通过考虑其在网络层序列中的连续应用，扩展了
    Pascanu 等人 ([2014](#bib.bib243)) 的结果到深层网络。
- en: In the case of *deep* networks, where $L>1$, we need to consider how the linear
    regions defined up to a given layer of the network can be further partitioned
    by the next layers. We start by assuming that every linear region defined by the
    first $l-1$ layers is then subdivided into the maximum possible number of linear
    regions defined by the activation hyperplanes of layer $l$. That leads to the
    bound in Raghu et al. ([2017](#bib.bib253)), which is implicit in their proof
    of an asymptotic bound of $O(n^{n_{0}L})$, where $n$ is used as the width of every
    layer. However, there are many ways in which this bound can be refined upon careful
    examination. First, the dimension of the input of layer $l$ —i.e., the output
    of layer $l-1$— within each linear region is never larger than the smallest dimension
    among layers $1$ to $l$, since for every linear region we have an affine transformation
    between inputs and outputs of each layer (Montúfar, [2017](#bib.bib223)). Second,
    the dimension of the input coming through each linear region is in fact bounded
    by the smallest number of active units in each of the previous layers (Serra et al.,
    [2018](#bib.bib282)). This leads to a tight upper bound for $n_{0}=1$, since it
    matches the lower bound in  Serra et al. ([2018](#bib.bib282)). Finally, the activation
    hyperplane of some units may not partition the linear regions because all possible
    inputs to the unit are in the same half-space, and in some of those cases the
    unit may never produce a positive output. For the number $k$ of active units in
    a given layer $l$, we can use the network parameters to calculate the maximum
    number of units that can be active in the next layer, $\mathcal{A}_{l}(k)$, as
    well as the number of units that can be active or inactive for different inputs,
    $\mathcal{I}_{l}(k)$ (Serra and Ramalingam, [2020](#bib.bib281)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*深度*网络，其中 $L>1$，我们需要考虑到如何通过后续层进一步划分到给定层的线性区域。我们首先假设由前 $l-1$ 层定义的每个线性区域然后被划分成由第
    $l$ 层的激活超平面定义的最大数量的线性区域。这导致了 Raghu 等人 ([2017](#bib.bib253)) 的界限，该界限在他们对 $O(n^{n_{0}L})$
    的渐近界限的证明中隐含，其中 $n$ 作为每层的宽度。然而，在仔细检查后，这个界限有很多可以改进的地方。首先，第 $l$ 层的输入维度——即第 $l-1$
    层的输出——在每个线性区域内从不大于第 $1$ 层到第 $l$ 层中的最小维度，因为对于每个线性区域，我们在每一层的输入和输出之间都有一个仿射变换（Montúfar,
    [2017](#bib.bib223)）。其次，通过每个线性区域传来的输入维度实际上受到前面每层中活跃单元的最小数量的限制（Serra 等人, [2018](#bib.bib282)）。这导致
    $n_{0}=1$ 的紧界限，因为它与 Serra 等人 ([2018](#bib.bib282)) 中的下界相匹配。最后，一些单元的激活超平面可能不会划分线性区域，因为所有可能的输入都位于同一半空间中，在一些情况下，这些单元可能永远不会产生正输出。对于给定层
    $l$ 中的活跃单元数量 $k$，我们可以使用网络参数计算下一层中可以活跃的单元的最大数量 $\mathcal{A}_{l}(k)$，以及不同输入下可以活跃或不活跃的单元数量
    $\mathcal{I}_{l}(k)$（Serra 和 Ramalingam, [2020](#bib.bib281)）。
- en: Hinz and van de Geer ([2019](#bib.bib150)) observed that the upper bound by
    Serra et al. ([2018](#bib.bib282)) can be tightened by explicitly computing a
    recursive histogram of linear regions on the layers of the neural network according
    to the dimension of their image subspace. However, the resulting bound is not
    explicitly defined in terms of the network hyperparameters, and hence cannot be
    included on the table. This work is further extended in Hinz ([2021](#bib.bib149))
    by also allowing a composition of bounds on subnetworks instead of only on the
    sequence of layers. Another extension of the framework from Hinz and van de Geer
    ([2019](#bib.bib150)) by Xie et al. ([2020c](#bib.bib344)) highlights that residual
    connections prevent the bottleneck effect in ResNets, by which reason such networks
    tend to have more linear regions.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Hinz 和 van de Geer ([2019](#bib.bib150)) 观察到，通过根据图像子空间的维度显式计算神经网络层上的线性区域的递归直方图，可以收紧
    Serra 等人 ([2018](#bib.bib282)) 提出的上界。然而，结果界限并没有以网络超参数明确的方式定义，因此不能包含在表格中。Hinz ([2021](#bib.bib149))
    进一步扩展了这项工作，允许对子网络的界限进行组合，而不仅仅是对层序列进行组合。Xie 等人 ([2020c](#bib.bib344)) 对 Hinz 和
    van de Geer ([2019](#bib.bib150)) 框架的另一个扩展强调，残差连接可以防止 ResNets 中的瓶颈效应，这也是这些网络趋向于拥有更多线性区域的原因。
- en: Cai et al. ([2023](#bib.bib46)) proposed a separate recursive bound based on
    Serra et al. ([2018](#bib.bib282)) to account for the sparsity of the weight matrices,
    which illustrates how pruning connections may affect the maximum number of linear
    regions.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Cai 等人 ([2023](#bib.bib46)) 提出了一个基于 Serra 等人 ([2018](#bib.bib282)) 的单独递归界限，以考虑权重矩阵的稀疏性，这说明了剪枝连接可能如何影响线性区域的最大数量。
- en: 'The results above have also been extended to other architectures. In some cases,
    results on other types of activations are also part of the papers previously mentioned:
    Montúfar et al. ([2014](#bib.bib224)) and Serra et al. ([2018](#bib.bib282)) present
    upper bounds for *maxout* networks; Raghu et al. ([2017](#bib.bib253)) present
    an upper bound for networks using *hard tanh* activation. In other cases, the
    ideas discussed above have been adapted for sparser networks with parameter sharing:
    Xiong et al. ([2020](#bib.bib345)) present upper and lower bounds for convolutional
    networks, which are shown to asymptotically define more linear regions per parameter
    than rectifier networks with the same input size and number of layers. Chen et al.
    ([2022a](#bib.bib50)) present upper and lower bounds for graph convolutional networks.
    Matoba et al. ([2022](#bib.bib214)) discuss the expresiveness of the maxpooling
    layers typically used in convolutional neural networks through their equivalence
    to a sequence of rectifier layers. Moreover, Goujon et al. ([2022](#bib.bib128))
    present results for recently proposed activation functions, such as DeepSpline (Agostinelli
    et al., [2015](#bib.bib2), Unser, [2019](#bib.bib314), Bohra et al., [2020](#bib.bib34))
    and GroupSort (Anil et al., [2019](#bib.bib6)).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果也扩展到了其他架构。在某些情况下，其他类型激活函数的结果也包含在之前提到的论文中：Montúfar 等人 ([2014](#bib.bib224))
    和 Serra 等人 ([2018](#bib.bib282)) 提出了*maxout*网络的上界；Raghu 等人 ([2017](#bib.bib253))
    提出了使用*hard tanh*激活函数的网络的上界。在其他情况下，以上讨论的思想已被改编为具有参数共享的稀疏网络：Xiong 等人 ([2020](#bib.bib345))
    提出了卷积网络的上界和下界，这些网络在输入大小和层数相同的情况下，被证明在每个参数上定义了更多的线性区域。Chen 等人 ([2022a](#bib.bib50))
    提出了图卷积网络的上界和下界。Matoba 等人 ([2022](#bib.bib214)) 讨论了卷积神经网络中通常使用的maxpooling层的表达能力，通过它们与一系列rectifier层的等效性。此外，Goujon
    等人 ([2022](#bib.bib128)) 提出了最近提出的激活函数的结果，例如 DeepSpline (Agostinelli 等人，[2015](#bib.bib2)，Unser，[2019](#bib.bib314)，Bohra
    等人，[2020](#bib.bib34)) 和 GroupSort (Anil 等人，[2019](#bib.bib6))。
- en: Some of the results above were also revisited through the lenses of tropical
    algebra, in which every linear region corresponds to a tropical hypersurface (Zhang
    et al., [2018b](#bib.bib357), Charisopoulos and Maragos, [2018](#bib.bib48), Maragos
    et al., [2021](#bib.bib212)). Notably, Montúfar et al. ([2022](#bib.bib225)) presented
    considerably tighter upper bounds for the number of linear regions in maxout networks
    with rank $k=3$ or greater.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上述一些结果也通过热带代数的视角重新审视，其中每个线性区域对应于热带超表面 (Zhang 等人，[2018b](#bib.bib357)，Charisopoulos
    和 Maragos，[2018](#bib.bib48)，Maragos 等人，[2021](#bib.bib212))。特别是，Montúfar 等人 ([2022](#bib.bib225))
    提出了具有秩 $k=3$ 或更高的 maxout 网络中线性区域数量的更紧的上界。
- en: Recently, a converse line of work started exploring the minimum dimensions of
    a neural network capable of representing a given piecewise linear function, starting
    with considerations about the minimum depth necessary (Arora et al., [2018](#bib.bib8))
    and further refinements of bounds on the network dimensions (He et al., [2020](#bib.bib142),
    Hertrich et al., [2021](#bib.bib147), Chen et al., [2022b](#bib.bib51)), with
    Chen et al. ([2022b](#bib.bib51)) proposing an algorithm that can construct such
    a neural network. On a related note, Karg and Lucia ([2020](#bib.bib168)) show
    that linear time-invariant systems in model predictive control can be exactly
    expressed by rectifier networks and provide bounds on the width and number of
    layers necessary for a given system, whereas Ferlez and Shoukry ([2020](#bib.bib103))
    describe an algorithm for producing architectures that can be parameterized as
    an optimal model predictive control strategy.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一项相反的研究开始探索能够表示给定分段线性函数的神经网络的最小维度，从考虑必要的最小深度开始 (Arora 等人，[2018](#bib.bib8))，并进一步细化网络维度的界限
    (He 等人，[2020](#bib.bib142)，Hertrich 等人，[2021](#bib.bib147)，Chen 等人，[2022b](#bib.bib51))，其中
    Chen 等人 ([2022b](#bib.bib51)) 提出了一个算法，能够构造这样的神经网络。相关地，Karg 和 Lucia ([2020](#bib.bib168))
    显示模型预测控制中的线性时间不变系统可以被准确地表达为 rectifier 网络，并提供了给定系统所需的宽度和层数的界限，而 Ferlez 和 Shoukry
    ([2020](#bib.bib103)) 描述了一个生成可以参数化为最优模型预测控制策略的架构的算法。
- en: 'Table 3: Upper bounds on the maximum number of linear regions defined by a
    neural network.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：神经网络定义的最大线性区域数量的上界。
- en: '| Reference | Bound and conditions |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 界限与条件 |'
- en: '| --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  Pascanu et al. ([2014](#bib.bib243)) | $\sum\limits_{i=0}^{n_{0}}\binom{n_{1}}{n_{0}}$
    for shallow networks, $n_{1}\geq n_{0}$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  Pascanu 等人 ([2014](#bib.bib243)) | 对于浅层网络，$ \sum\limits_{i=0}^{n_{0}}\binom{n_{1}}{n_{0}}
    $，其中 $ n_{1} \geq n_{0} $ |'
- en: '|  Montúfar et al. ([2014](#bib.bib224)) | $2^{\sum\limits_{l=1}^{L}n_{l}}$
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  Montúfar 等人 ([2014](#bib.bib224)) | $2^{\sum\limits_{l=1}^{L}n_{l}}$ |'
- en: '|  Raghu et al. ([2017](#bib.bib253)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{n_{l-1}}\binom{n_{l}}{j}$
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  Raghu 等人 ([2017](#bib.bib253)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{n_{l-1}}\binom{n_{l}}{j}$
    |'
- en: '|  Montúfar ([2017](#bib.bib223)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{d_{l}}\binom{n_{l}}{j}$,
    $d_{l}=\min\{n_{0},n_{1},\ldots,n_{l-1}\}$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  Montúfar ([2017](#bib.bib223)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{d_{l}}\binom{n_{l}}{j}$,
    $d_{l}=\min\{n_{0},n_{1},\ldots,n_{l-1}\}$ |'
- en: '|  Serra et al. ([2018](#bib.bib282)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{n_{l}}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l}~{}\forall l\in{\mathbb{L}}\},\\ d_{l}=\min\{n_{0},n_{1}-j_{1},\ldots,n_{l-1}-j_{l-1},n_{l}\}\end{array}$
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  Serra 等人 ([2018](#bib.bib282)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{n_{l}}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l}~{}\forall l\in{\mathbb{L}}\},\\ d_{l}=\min\{n_{0},n_{1}-j_{1},\ldots,n_{l-1}-j_{l-1},n_{l}\}\end{array}$
    |'
- en: '|  Serra and Ramalingam ([2020](#bib.bib281)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{\mathcal{I}_{l}(k_{l-1})}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l},\\ d_{l}=\min\{n_{0},k_{1},\ldots,k_{l-1},\mathcal{I}_{l}(k_{l-1})\},k_{0}=n_{0},k_{l}=\mathcal{A}_{l}(k_{l-1})-j_{l-1}~{}\forall
    l\in{\mathbb{L}}\}\end{array}$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  Serra 和 Ramalingam ([2020](#bib.bib281)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{\mathcal{I}_{l}(k_{l-1})}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l},\\ d_{l}=\min\{n_{0},k_{1},\ldots,k_{l-1},\mathcal{I}_{l}(k_{l-1})\},k_{0}=n_{0},k_{l}=\mathcal{A}_{l}(k_{l-1})-j_{l-1}~{}\forall
    l\in{\mathbb{L}}\}\end{array}$ |'
- en: Expected number
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 期望数量
- en: The third analytical approach has been the evaluation of the expected number
    of linear regions. In a pair of papers, Hanin and Rolnick studied the number of
    linear regions based on how the network parameters are typically initialized.
    In the first paper (Hanin and Rolnick, [2019a](#bib.bib137)), they show that the
    average number of linear regions along 1-dimensional subspaces of the input grows
    linearly with respect to the number of neurons, irrespective of the network depth.
    In the second paper (Hanin and Rolnick, [2019b](#bib.bib138)), they show that
    the average number of linear regions in higher-dimensional subspaces of the input
    also grows similarly in deep and shallow networks. For $N=\sum_{i=1}^{L}n_{i}$
    as the total number of linear regions, the expected number of linear regions is
    $O(2^{N})$ if $N\leq n_{0}$ and $O\left(\frac{(TN)^{n_{0}}}{n_{0}!}\right)$ otherwise,
    where $T>0$ is a constant based on the network parameters. Moreover, some of their
    experiments suggest that the number of linear regions in shallow networks is slightly
    greater. According to the authors, these bounds reflect the fact that the family
    of functions that can be represented by neural networks in the way that they are
    typically initialized is considerably smaller. They further argue that training
    as currently performed is unlikely to expand the family of functions much further,
    as illustrated by their experiments. Similar results on the expected number of
    linear regions for maxout networks are presented by Tseran and Montúfar ([2021](#bib.bib313)),
    and an application of the results above results to data manifolds is explored
    by Tiwari and Konidaris ([2022](#bib.bib307)). Additional results for specific
    architectures of rectifier networks are conjectured by Wang ([2022](#bib.bib328)),
    although without proof.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种分析方法是评估线性区域的期望数量。在一对论文中，Hanin 和 Rolnick 研究了基于网络参数典型初始化方式的线性区域数量。在第一篇论文（Hanin
    和 Rolnick, [2019a](#bib.bib137)）中，他们展示了沿输入的一维子空间的平均线性区域数量随着神经元数量的增加而线性增长，与网络深度无关。在第二篇论文（Hanin
    和 Rolnick, [2019b](#bib.bib138)）中，他们展示了输入的高维子空间中的平均线性区域数量在深层和浅层网络中也有类似的增长。对于 $N=\sum_{i=1}^{L}n_{i}$
    作为线性区域的总数量，如果 $N\leq n_{0}$，期望的线性区域数量是 $O(2^{N})$，否则是 $O\left(\frac{(TN)^{n_{0}}}{n_{0}!}\right)$，其中
    $T>0$ 是基于网络参数的常数。此外，他们的一些实验表明，浅层网络中的线性区域数量略多。根据作者的说法，这些界限反映了神经网络在典型初始化方式下可以表示的函数家族要小得多。他们进一步认为，目前的训练方式不太可能大幅扩展函数家族，如他们的实验所示。Tseran
    和 Montúfar ([2021](#bib.bib313)) 提出了关于 maxout 网络的期望线性区域数量的类似结果，Tiwari 和 Konidaris
    ([2022](#bib.bib307)) 探讨了上述结果对数据流形的应用。Wang ([2022](#bib.bib328)) 对特定的整流网络架构提出了额外的结果，但尚未证明。
- en: 3.4.2 Counting linear regions
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 计数线性区域
- en: 'Counting the actual number of linear regions of a given network has been a
    more challenging topic to explore. Serra et al. ([2018](#bib.bib282)) have shown
    that the linear regions of a trained network can be enumerated as the solutions
    of an MILP formulation, which has been slightly corrected in Cai et al. ([2023](#bib.bib46))¹¹1The
    MILP formulation of neural networks is discussed in Section [4](#S4 "4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey").. However, MILP solutions are generally counted one by one (Danna et al.,
    [2007](#bib.bib72)), with exception of special cases (Serra and Hooker, [2020](#bib.bib280))
    and small subproblems (Serra, [2020](#bib.bib279)), which makes this approach
    impractical for large neural networks. Serra and Ramalingam ([2020](#bib.bib281))
    have shown that approximate model counting methods, which are commonly used to
    count the number of feasible assignments in propositional satisfiability, can
    be easily adapted to solution counting in MILP, which leads to an order-of-magnitude
    speedup in comparison with exact counting. This type of approach is particularly
    suitable for obtaining probabilistic lower bounds, which can complement the analytical
    upper bounds for the maximum number of linear regions. In Craighero et al. ([2020a](#bib.bib64))
    and Craighero et al. ([2020b](#bib.bib65)), a directed acyclic graph is used to
    model the sets of active neurons on each layer and how they connected with those
    in subsequent layers. Yang et al. ([2020](#bib.bib348)) describe a method for
    decomposing the input space of rectifier networks into their linear regions by
    representing each linear region in terms of its face lattice, upon which the splitting
    operations corresponding to the transformations performed by each layer can be
    implemented. As the number of linear regions grow, these splitting operations
    can be processed in parallel. Yang et al. ([2021](#bib.bib349)) extend that method
    to convolutional neural networks. Moreover, Wang ([2022](#bib.bib328)) describes
    an algorithm for enumerating linear regions that counts adjacent linear regions
    with same corresponding affine function as a single linear region.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '计算给定网络的实际线性区域的数量一直是一个更具挑战性的研究课题。Serra等人（[2018](#bib.bib282)）已经证明，训练网络的线性区域可以作为MILP（Mixed-Integer
    Linear Programming）公式的解进行枚举，该公式在Cai等人进行了轻微修正（[2023](#bib.bib46)）¹¹1 神经网络的MILP公式在第[4](#S4
    "4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")节进行了讨论。然而，MILP解决方案通常是逐个计数的（Danna等人，[2007](#bib.bib72)），除了特殊情况（Serra和Hooker，[2020](#bib.bib280)）和小型子问题（Serra，[2020](#bib.bib279)）之外，这种方法对于大型神经网络来说是不可行的。Serra和Ramalingam（[2020](#bib.bib281)）已经证明了近似模型计数方法，通常用于计算命题可满足性中可行赋值的数量，可以轻松地适应MILP中的解计数，这导致与精确计数相比的数量级加速。这种方法特别适用于获得概率下界，可以与最大线性区域的分析上界相辅相成。在Craighero等人（[2020a](#bib.bib64)）和Craighero等人（[2020b](#bib.bib65)）中，使用有向无环图来建模每一层的活跃神经元集合以及它们与后续层中神经元的连接方式。Yang等人（[2020](#bib.bib348)）描述了一种将修正函数网络（rectifier
    networks）的输入空间分解为线性区域的方法，通过用面格描述每个线性区域，可以实现与每层执行的转换相对应的分割操作。随着线性区域的数量增加，这些分割操作可以并行处理。Yang等人（[2021](#bib.bib349)）将该方法推广到卷积神经网络。此外，Wang（[2022](#bib.bib328)）描述了一种枚举线性区域的算法，该算法将具有相同对应仿射函数的相邻线性区域计数为单个线性区域。'
- en: Another approach is to enumerate the linear regions in subspaces, which limits
    their number and reduces the complexity of the task. This idea was first explored
    by Novak et al. ([2018](#bib.bib235)) for measuring the complexity of a neural
    network in terms of the number of transitions along a single line. Hanin and Rolnick
    ([2019a](#bib.bib137), [b](#bib.bib138)) also use this method with a bounded line
    segment or rectangle as a single set representing the input and then sequentially
    partitioning it. If this first set is intersected by the activation hyperplane
    of a neuron in the first layer, then we replace this set by two sets corresponding
    to the parts of the input space in which that neuron is active and not. Once those
    sets are further subdivided by all activation hyperplanes associated with the
    neurons in the first layer, the process can be continued with the neurons in the
    following layers. This method is used to count the number of linear regions along
    subspaces of the input with dimension 1 in Hanin and Rolnick ([2019a](#bib.bib137))
    and dimension 2 in Hanin and Rolnick ([2019b](#bib.bib138)). A generalized version
    for counting the number of linear regions in affine subspaces spanned by a set
    of samples using an MILP formulation is presented in Cai et al. ([2023](#bib.bib46)).
    An approximate approach for counting the number of linear regions along a line
    by computing the closest activation hyperplane in each layer is presented in Gamba
    et al. ([2022](#bib.bib114)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是枚举子空间中的线性区域，这限制了它们的数量并减少了任务的复杂性。Novak 等人 ([2018](#bib.bib235)) 首次探索了这个想法，用于衡量神经网络在单一线段上的过渡数量的复杂性。Hanin
    和 Rolnick ([2019a](#bib.bib137), [b](#bib.bib138)) 也使用这种方法，将一个有界线段或矩形作为表示输入的单一集合，然后依次对其进行划分。如果第一个集合被第一层神经元的激活超平面相交，则用两个集合替代这个集合，这两个集合分别对应于该神经元激活和未激活的输入空间部分。一旦这些集合被与第一层神经元相关的所有激活超平面进一步细分，该过程可以继续进行到后续层的神经元。这种方法用于计算
    Hanin 和 Rolnick ([2019a](#bib.bib137)) 中维度为 1 的输入子空间和 Hanin 和 Rolnick ([2019b](#bib.bib138))
    中维度为 2 的线性区域数量。Cai 等人 ([2023](#bib.bib46)) 提出了一个用于计数由样本集生成的仿射子空间中线性区域数量的 MILP
    公式的广义版本。Gamba 等人 ([2022](#bib.bib114)) 提出了一个通过计算每层中最接近的激活超平面来计数沿一条线的线性区域数量的近似方法。
- en: Other approaches have obtained lower bounds on the number of linear regions
    of a trained network by limiting the enumeration or considering exclusively the
    inputs from the dataset. In Xiong et al. ([2020](#bib.bib345)), the number of
    linear regions is estimated by sampling points from the input space and enumerating
    all activation patterns identified through this process. In Cohan et al. ([2022](#bib.bib61)),
    the counting is restricted to the linear regions found between consecutive states
    of a neural network modeling a reinforcement learning policy.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其他方法通过限制枚举或专门考虑数据集中的输入来获得训练网络线性区域的下界。在 Xiong 等人 ([2020](#bib.bib345)) 中，线性区域的数量通过从输入空间中采样点并枚举通过这一过程识别的所有激活模式来估计。在
    Cohan 等人 ([2022](#bib.bib61)) 中，计数限制在神经网络建模强化学习策略的连续状态之间发现的线性区域。
- en: 3.5 Applications and insights
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 应用与见解
- en: Thinking about neural networks in terms of linear regions led to a variety of
    applications. In turn, that inspired further studies on the structure and properties
    of linear regions under different settings. We organize the literature about applications
    and insights around some central themes in the subsections below.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从线性区域的角度思考神经网络导致了各种应用。这反过来又激发了对不同设置下线性区域结构和属性的进一步研究。我们在下面的子章节中围绕一些中心主题组织了关于应用和见解的文献。
- en: 3.5.1 The number of linear regions
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 线性区域的数量
- en: From our discussion, the number of linear regions emerges as a potential proxy
    for the complexity of neural networks, which has been studied by some authors
    and exploited empirically by others. Novak et al. ([2018](#bib.bib235)) observed
    that the number of transitions between linear regions in 1-dimensional subspaces
    correlates with generalization. Hu et al. ([2020a](#bib.bib154)) used bounds on
    the number of linear regions as proxy to model the capacity of a neural network
    used for learning through distillation, in which a smaller network is trained
    based on the outputs of another network. Chen et al. ([2021a](#bib.bib54)) and
    Chen et al. ([2021b](#bib.bib55)) present one of the first approaches to training-free
    neural architectural search through the analysis of network properties. One of
    the two metrics that they have shown to be effective for that purpose is the number
    of linear regions associated with a sample of inputs from the training set on
    randomly initialized networks. Biau et al. ([2021](#bib.bib30)) observed that
    obtaining a discriminator network for Wasserstein GANs (Arjovsky et al., [2017](#bib.bib7))
    that correctly approximates the Wasserstein distance entails that such a discriminator
    network has a growing number of linear regions as the complexity of the data distribution
    increases. Park et al. ([2021b](#bib.bib242)) maximized the number of linear regions
    in unsupervised learning in order to produce more expressive encodings for downstream
    tasks using simpler classifiers. In neural networks modeling reinforcement learning
    policies, Cohan et al. ([2022](#bib.bib61)) observed that the number of transitions
    between linear regions in inputs corresponding to consecutive states increases
    by 50% with training while the number of repeated linear regions decreases. Cai
    et al. ([2023](#bib.bib46)) proposed a method for pruning different proportions
    of parameters from each layer by maximizes the bound on the number of linear regions,
    which lead to better accuracy than uniform pruning across layers. On a related
    note, Liang and Xu ([2021](#bib.bib192)) proposed a new variant of the ReLU activation
    function for dividing the input space into a greater number of linear regions.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的讨论中，线性区域的数量浮现为神经网络复杂性的潜在代理，这一领域已被一些作者研究，并被其他人经验性地利用。Novak 等 ([2018](#bib.bib235))
    观察到一维子空间中线性区域之间的转换次数与泛化能力相关。Hu 等 ([2020a](#bib.bib154)) 使用线性区域数量的界限作为代理，以建模用于蒸馏学习的神经网络的容量，其中一个较小的网络是基于另一个网络的输出进行训练的。Chen
    等 ([2021a](#bib.bib54)) 和 Chen 等 ([2021b](#bib.bib55)) 提出了通过网络属性分析进行无训练神经网络结构搜索的最初方法之一。他们展示的两种有效度量之一是与训练集输入样本相关的线性区域数量，使用随机初始化的网络。Biau
    等 ([2021](#bib.bib30)) 观察到，为 Wasserstein GANs (Arjovsky 等, [2017](#bib.bib7))
    获得一个正确近似 Wasserstein 距离的判别网络意味着该网络在数据分布复杂度增加时具有不断增长的线性区域数量。Park 等 ([2021b](#bib.bib242))
    在无监督学习中最大化线性区域的数量，以便为下游任务生成更具表现力的编码，使用更简单的分类器。在神经网络建模强化学习策略中，Cohan 等 ([2022](#bib.bib61))
    观察到输入对应于连续状态之间的线性区域转换数量随着训练增加了 50%，而重复的线性区域数量减少。Cai 等 ([2023](#bib.bib46)) 提出了一种通过最大化线性区域数量的界限来修剪每一层的不同比例参数的方法，这种方法比跨层均匀修剪更能提高准确性。在相关研究中，Liang
    和 Xu ([2021](#bib.bib192)) 提出了一个新的 ReLU 激活函数变体，用于将输入空间划分为更多的线性区域。
- en: The number of linear regions also inspired further theoretical work. Amrami
    and Goldberg ([2021](#bib.bib3)) presented an argument for the benefit of depth
    in neural networks based on the number of linear regions for correctly separating
    samples associated with different classes. Liu and Liang ([2021](#bib.bib196))
    studied upper and lower bounds on the optimal approximation error of a convex
    univariate function based on the number of linear regions of a rectifier network.
    Henriksen et al. ([2022](#bib.bib146)) used the maximum number of linear regions
    as a metric for capacity that may limit repairing incorrect classifications in
    a neural network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 线性区域的数量也激发了进一步的理论研究。Amrami 和 Goldberg ([2021](#bib.bib3)) 基于正确分离不同类别样本的线性区域数量，提出了神经网络深度的好处的论点。Liu
    和 Liang ([2021](#bib.bib196)) 研究了基于整流器网络的线性区域数量的凸一元函数的最佳近似误差的上下界。Henriksen 等 ([2022](#bib.bib146))
    使用线性区域的最大数量作为容量的度量，这可能限制了神经网络中修复错误分类的能力。
- en: 3.5.2 The shape of linear regions
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 线性区域的形状
- en: Some studies aimed at understanding what affects the shape of linear regions
    in practice, including how to train neural networks in such a way to induce certain
    shapes in the linear regions. Zhang and Wu ([2020](#bib.bib359)) observed that
    multiple training techniques may lead to similar accuracy, but very different
    shape for the linear regions. For example, batch normalization (Ioffe and Szegedy,
    [2015](#bib.bib162)) and dropout (Srivastava et al., [2014](#bib.bib294)) lead
    to more linear regions. While batch normalization breaks the space in regions
    with uniform size, more orthogonal norms, and more gradient variability across
    adjacent regions; dropout produces more linear regions around decision boundaries,
    norms are more parallel, and data points less likely to be in the region containing
    the decision boundary. Croce et al. ([2019](#bib.bib67)) and Lee et al. ([2019](#bib.bib188))
    applied regularization to the loss function to push the boundary of each linear
    region away from points in the training set that it contains, as long as those
    points are correctly classified. They show that this form of regularization improves
    the robustness of the neural network while making the linear regions larger. In
    fact, Zhu et al. ([2020](#bib.bib362)) observed that the boundaries of the linear
    regions move away from the training data; and He et al. ([2021](#bib.bib141))
    conjectured that the linear regions near training samples becomes smaller through
    training, or that conversely the activation patterns are denser around the training
    samples. Gamba et al. ([2020](#bib.bib113)) presented an empirical study on the
    angles between activation hyperplanes defined by convolutional layers, and observed
    that their cosines tend to be similar and more negative with depth after training.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究旨在理解在实际操作中影响线性区域形状的因素，包括如何训练神
- en: The geometry of linear regions also led to other theoretical and algorithmic
    advances. Theoretically, Phuong and Lampert ([2020](#bib.bib247)) proved that
    architectures with nonincreasing layer widths have unique parameters —upon permutation
    and scaling— for representing certain functions. In other words, some pairs of
    neural networks are only equivalent if their parameters only differ by permutation
    and multiplication. Grigsby et al. ([2023](#bib.bib132)) showed that equivalences
    other than by permutation are less likely to occur with greater input size and
    width, but more likely with greater depth. Algorithmically, Rolnick and Kording
    ([2020](#bib.bib260)) proposed a procedure to reconstruct a neural network by
    evaluating several inputs in order to determine regions of the input space for
    which the output of the neural network can be defined by an affine function —and
    thus consist of a single linear region. Depending on how the shape changes between
    adjacent linear regions, the boundaries of the linear regions are replicated with
    neurons in the first hidden layer or in subsequent layers of the reconstructed
    neural network. Masden ([2022](#bib.bib213)) provided theoretical results and
    an algorithm for characterizing the face lattice of the polyhedron associated
    with each linear region.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 线性区域的几何形态也促进了其他理论和算法上的进展。在理论上，Phuong 和 Lampert ([2020](#bib.bib247)) 证明了具有非增宽层宽度的架构具有唯一的参数，即对于表示某些函数的排列和缩放。换句话说，一些神经网络只有在它们的参数只有排列和乘法之间的差异时才是等价的。Grigsby
    等 ([2023](#bib.bib132)) 表明，除了通过排列之外的等价性在更大的输入大小和宽度时更不太可能发生，但在更大的深度时更有可能发生。在算法上，Rolnick
    和 Kording ([2020](#bib.bib260)) 提出了一种通过评估几个输入来重构神经网络的程序，以确定输入空间的区域，在这些区域内神经网络的输出可以被定义为一个仿射函数，因此由单一线性区域组成。根据在相邻线性区域之间的形状变化方式，线性区域的边界被复制到重构神经网络的第一个隐藏层或后续层的神经元中。Masden
    ([2022](#bib.bib213)) 提供了关于表征每个线性区域相关的多面体的面格的理论结果和算法。
- en: 3.5.3 Activation patterns and the discrimination of inputs
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 激活模式和输入的区别
- en: Another common theme is understanding how inputs from the training and test
    sets are distributed among the linear regions, and what can be inferred the encoding
    of the activation patterns associated with the linear regions. Gopinath et al.
    ([2019](#bib.bib127)) noted that many properties of neural networks, including
    the classes of different inputs, are associated with activation patterns —and
    thus with their linear regions. Several works (He et al., [2021](#bib.bib141),
    Sattelberg et al., [2020](#bib.bib271), Trimmel et al., [2021](#bib.bib310)) observed
    that each training sample is typically located in a different linear region when
    the neural network is sufficiently expressive; whereas He et al. ([2021](#bib.bib141))
    noted that simple machine learning algorithms can be applied using the activation
    patterns as features, and Sattelberg et al. ([2020](#bib.bib271)) noted that there
    is some similarity between activation patterns of different neural networks under
    affine mapping, meaning that the training of these neural networks lead to similar
    models. Chaudhry et al. ([2020](#bib.bib49)) exploited the idea of continual learning
    with different tasks being encoded in disjoint subspaces, which thus corresponds
    to a disjoint set of activation sets on each layer being associated with classifications
    for each of those tasks. Based on their approach for enumerating linear regions,
    Craighero et al. ([2020a](#bib.bib64)) and Craighero et al. ([2020b](#bib.bib65))
    have found that inputs from larger linear regions are often correctly classified
    by the neural network, that inputs from smaller linear regions are often incorrectly
    classified, and that the number of distinct activations sets reduces along the
    layers of the neural network. Gamba et al. ([2022](#bib.bib114)) also discussed
    the issue of some linear regions being smaller and thus less likely to occur in
    practice. Moreover, they propose a measurement for the similarity of the affine
    functions associated with linear regions along a line and observed that the linear
    regions tend to be less similar to one another when the network is trained with
    incorrectly classified labels.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的主题是理解训练集和测试集中的输入如何在线性区域中分布，以及从这些线性区域的激活模式编码中可以推断出什么。Gopinath 等人（[2019](#bib.bib127)）指出，神经网络的许多属性，包括不同输入的类别，都与激活模式有关——因此也与它们的线性区域有关。一些研究（He
    等人，[2021](#bib.bib141)，Sattelberg 等人，[2020](#bib.bib271)，Trimmel 等人，[2021](#bib.bib310)）观察到，当神经网络具有足够的表达能力时，每个训练样本通常位于不同的线性区域；而
    He 等人（[2021](#bib.bib141)）指出，可以使用激活模式作为特征来应用简单的机器学习算法，Sattelberg 等人（[2020](#bib.bib271)）指出，不同神经网络的激活模式在仿射映射下存在一些相似性，这意味着这些神经网络的训练会导致类似的模型。Chaudhry
    等人（[2020](#bib.bib49)）利用了持续学习的思想，将不同任务编码到不相交的子空间中，这与每层上的每个任务的分类相关联的激活集合对应。根据他们对线性区域进行枚举的方法，Craighero
    等人（[2020a](#bib.bib64)）和 Craighero 等人（[2020b](#bib.bib65)）发现，来自较大线性区域的输入通常被神经网络正确分类，来自较小线性区域的输入通常被错误分类，而且神经网络层上的不同激活集合数量会减少。Gamba
    等人（[2022](#bib.bib114)）还讨论了一些线性区域较小，因此在实践中出现的可能性较低的问题。此外，他们提出了一种测量线性区域沿直线相关的仿射函数相似性的方法，并观察到当网络在错误分类标签下训练时，线性区域之间的相似性往往较低。
- en: 3.5.4 Function approximation
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 函数逼近
- en: Because of the linear behavior of the output within each linear region, we can
    approximate the output of the neural network based on the output of its linear
    regions. Chu et al. ([2018](#bib.bib59)) and Sudjianto et al. ([2020](#bib.bib297))
    produced linear models based on this local behavior; whereas Glass et al. ([2021](#bib.bib118))
    observed that we can interpret neural networks as equivalent to local linear model
    trees (Nelles et al., [2000](#bib.bib230)), in which a distinct linear model is
    used at each leaf of a decision tree, and provided a method to produce such models
    from neural networks. Trimmel et al. ([2021](#bib.bib310)) described how to extract
    the linear regions associated with the inputs from the training set as means to
    approximate the output of the inputs from the test set. Robinson et al. ([2019](#bib.bib259))
    presented another approach for explicitly representing the function modeled by
    a neural network through the enumeration of its linear regions. On a related note,
    Chaudhry et al. ([2020](#bib.bib49)) used the assumption of training samples remaining
    within the same linear region during gradient descent to simplify the analysis
    of backpropagation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个线性区域内输出的线性行为，我们可以基于其线性区域的输出来近似神经网络的输出。Chu 等人（[2018](#bib.bib59)）和 Sudjianto
    等人（[2020](#bib.bib297)）基于这种局部行为生成了线性模型；而 Glass 等人（[2021](#bib.bib118)）观察到我们可以将神经网络解释为等效于局部线性模型树（Nelles
    等人，[2000](#bib.bib230)），其中在决策树的每个叶子节点上使用一个独特的线性模型，并提供了从神经网络生成这种模型的方法。Trimmel 等人（[2021](#bib.bib310)）描述了如何从训练集中提取与输入相关的线性区域，以近似测试集中输入的输出。Robinson
    等人（[2019](#bib.bib259)）提出了另一种通过枚举线性区域来显式表示神经网络建模函数的方法。相关地，Chaudhry 等人（[2020](#bib.bib49)）利用训练样本在梯度下降过程中保持在同一线性区域的假设来简化反向传播分析。
- en: This topic also relates to the broad literature on neural networks as universal
    function approximators, to which the concept of linear regions helps articulating
    ideal conditions. As observed by Mhaskar and Poggio ([2020](#bib.bib218)), the
    optimal number of linear regions in a neural network —or, correspondingly, of
    pieces of the piecewise linear function modeled by it— depends on the function
    being approximated. In addition, linear regions were also used explicitly to build
    function approximations. Kumar et al. ([2019](#bib.bib181)) have shown that rectifier
    networks can we approximated to arbitrary precision with two hidden layers, the
    largest of which having a neuron corresponding to each different activation pattern
    of the original network; an exact counterpart of this result was later presented
    by Villani and Schoots ([2023](#bib.bib319)). Fan et al. ([2020](#bib.bib99))
    described the transformation between sufficiently wide and deep networks while
    arguing that the fundamental measure of complexity should be counting simplices
    within linear regions. In subsequent work, Fan et al. ([2023](#bib.bib100)) empirically
    observed that linear regions tend to have a small number of higher dimensional
    faces, or facets.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这个话题还涉及到神经网络作为通用函数逼近器的广泛文献，其中线性区域的概念有助于阐明理想条件。正如 Mhaskar 和 Poggio（[2020](#bib.bib218)）所观察到的，神经网络中的线性区域的最优数量——或者相应地，其所建模的分段线性函数的部分数量——取决于被逼近的函数。此外，线性区域也被显式地用于构建函数逼近。Kumar
    等人（[2019](#bib.bib181)）已经证明，带有两个隐藏层的整流网络可以达到任意精度，其中最大的层有一个神经元对应于原始网络的每种不同激活模式；这一结果的精确对等物后来由
    Villani 和 Schoots（[2023](#bib.bib319)）提出。Fan 等人（[2020](#bib.bib99)）描述了在足够宽和深的网络之间的变换，同时争论说复杂性的基本度量应为线性区域内的单纯形数。在随后的工作中，Fan
    等人（[2023](#bib.bib100)）实证观察到线性区域往往具有少量的高维面或面片。
- en: 'More recent studies aimed at understanding the expressiveness and approximability
    of neural networks in terms of their number of parameters, in particular when
    the number of linear regions is greater than the number of parameters (Malach
    and Shalev-Shwartz, [2019](#bib.bib208), Dym et al., [2020](#bib.bib86), Daubechies
    et al., [2022](#bib.bib76)). They all discuss how the composition the modeled
    functions tend to present the self-similarity property of fractal distributions,
    which is one reason why they have so many linear regions. Keup and Helias ([2022](#bib.bib172))
    interpreted the connection between linear regions in different parts of the input
    space in terms of how paper origamis are constructed: by “folding” the data for
    separability.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究旨在理解神经网络在其参数数量方面的表现力和近似能力，特别是当线性区域的数量大于参数的数量时（Malach 和 Shalev-Shwartz，[2019](#bib.bib208)，Dym
    等，[2020](#bib.bib86)，Daubechies 等，[2022](#bib.bib76)）。这些研究讨论了建模函数的组合如何呈现分形分布的自相似性特征，这也是它们有如此多线性区域的原因之一。Keup
    和 Helias ([2022](#bib.bib172)) 通过“折叠”数据以实现可分性，将输入空间中不同部分的线性区域之间的连接进行了阐释。
- en: 'Another related topic is computing the Lipschitz constant $\rho$ of the function
    $f(x)$ modeled by the neural network, the smallest $\rho$ for which $\|f(x^{\prime})-f(x)\|\leq\rho\|x^{\prime}-x\|$
    for any two inputs $x$ and $x^{\prime}$. Note that the first derivative of the
    output of a linear region is constant, which is leveraged by Hwang and Heinecke
    ([2020](#bib.bib160)) to evaluate the stability of the network by computing the
    constant across linear regions by changing the activation pattern. Interestingly,
    Zhou and Schoellig ([2019](#bib.bib361)) showed that the constant grows similarly
    to the number of linear regions: polynomial in width and exponential in depth.
    A smaller constant limits the susceptibility of the network to adversarial examples (Huster
    et al., [2018](#bib.bib159)), which are discussed in Section [4](#S4 "4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey"), and also lead to smaller bias variance (Loukas et al., [2021](#bib.bib201)).
    While calculating the exact Lipschitz constant is NP-hard and encourages approximations
    (Virmaux and Scaman, [2018](#bib.bib322), Patrick L. Combettes, [2019](#bib.bib244)),
    the exact constant can be computed using MILP (Jordan and Dimakis, [2020](#bib.bib166)).
    Notably, many studies have focused on relaxations such as linear programming (Zou
    et al., [2019](#bib.bib363)), semidefinite programming (Fazlyab et al., [2019](#bib.bib101),
    Chen et al., [2020](#bib.bib53)), and polynomial optimization (Latorre et al.,
    [2020](#bib.bib184)). An alternative approach is to use more sophisticated activation
    functions for limiting the value of the constant (Anil et al., [2019](#bib.bib6),
    Aziznejad et al., [2020](#bib.bib9)).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个相关主题是计算由神经网络建模的函数 $f(x)$ 的 Lipschitz 常数 $\rho$，即对于任何两个输入 $x$ 和 $x^{\prime}$，满足
    $\|f(x^{\prime})-f(x)\|\leq\rho\|x^{\prime}-x\|$ 的最小 $\rho$。注意，线性区域输出的第一导数是常数，这一点被
    Hwang 和 Heinecke ([2020](#bib.bib160)) 利用，通过改变激活模式计算线性区域中的常数来评估网络的稳定性。有趣的是，Zhou
    和 Schoellig ([2019](#bib.bib361)) 证明了该常数的增长类似于线性区域的数量：宽度上是多项式的，深度上是指数的。较小的常数限制了网络对对抗样本的敏感性（Huster
    等，[2018](#bib.bib159)），这些对抗样本在[4](#S4 "4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")节中进行了讨论，并且也导致较小的偏差方差（Loukas
    等，[2021](#bib.bib201)）。虽然计算精确的 Lipschitz 常数是 NP 难的，并鼓励使用近似方法（Virmaux 和 Scaman，[2018](#bib.bib322)，Patrick
    L. Combettes，[2019](#bib.bib244)），但可以通过 MILP 计算精确常数（Jordan 和 Dimakis，[2020](#bib.bib166)）。值得注意的是，许多研究集中于线性规划（Zou
    等，[2019](#bib.bib363)），半正定规划（Fazlyab 等，[2019](#bib.bib101)，Chen 等，[2020](#bib.bib53)），和多项式优化（Latorre
    等，[2020](#bib.bib184)）等放松方法。另一种方法是使用更复杂的激活函数来限制常数的值（Anil 等，[2019](#bib.bib6)，Aziznejad
    等，[2020](#bib.bib9)）。'
- en: 3.5.5 Optimizing over linear regions
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 在线性区域中的优化
- en: 'As an alternative to optimizing over neural networks as described next in Section [4](#S4
    "4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), a number of approaches have resorted to techniques that are
    equivalent to systematically enumerating or traversing linear regions and optimizing
    over them (Croce and Hein, [2018](#bib.bib66), Croce et al., [2020](#bib.bib68),
    Khedr et al., [2020](#bib.bib174), Vincent and Schwager, [2021](#bib.bib320),
    Xu et al., [2022](#bib.bib346)). Notably, Vincent and Schwager ([2021](#bib.bib320))
    and Xu et al. ([2022](#bib.bib346)) are mindful of the facet-defining inequalities
    associated with a linear region, which are the ones to change when moving toward
    an adjacent linear region. On a related note, Seck et al. ([2021](#bib.bib278))
    alternates between gradient steps and solving a linear programming model within
    the current linear region.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 作为优化神经网络的替代方法，如第[4](#S4 "4 优化经过训练的神经网络 ‣ 当深度学习遇上多面体理论：综述")节中所述，一些方法已诉诸于系统地列举或遍历线性区域并对其进行优化的技术（Croce
    和 Hein，[2018](#bib.bib66)，Croce 等，[2020](#bib.bib68)，Khedr 等，[2020](#bib.bib174)，Vincent
    和 Schwager，[2021](#bib.bib320)，Xu 等，[2022](#bib.bib346)）。值得注意的是，Vincent 和 Schwager（[2021](#bib.bib320)）以及
    Xu 等（[2022](#bib.bib346)）关注与线性区域相关的面定义不等式，这些是不等式在向相邻线性区域移动时需要更改的。相关地，Seck 等（[2021](#bib.bib278)）在当前线性区域内交替进行梯度步骤和求解线性规划模型。
- en: 4 Optimizing Over a Trained Neural Network
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 优化经过训练的神经网络
- en: 'In Section [5](#S5 "5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") we will see how polyhedral-based
    methods can be used to *train* a neural network. In this section, we will focus
    on how polyhedral-based methods can be used to do something with a neural network
    *after it has been trained.* Specifically, after the network architecture and
    all parameters have been fixed, a neural network $f$ is merely a function. If
    each activation function $\sigma$ used to describe the network is piecewise linear
    (as is the case with those presented in Table [1](#S1.T1 "Table 1 ‣ 1.3 Why nonlinearity
    is important in artificial neurons ‣ 1 Introduction ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")), $f$ is also a piecewise linear function. Therefore,
    any optimization problem containing $f$ in some way will be a piecewise linear
    optimization problem. For example, in the simple case where the output of $f$
    is univariate, the optimization problem'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[5](#S5 "5 线性规划和多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述")节中，我们将看到如何使用基于多面体的方法来*训练*神经网络。在本节中，我们将重点关注如何在神经网络*训练完成后*使用基于多面体的方法。具体来说，在网络架构和所有参数已固定后，神经网络$f$仅仅是一个函数。如果用于描述网络的每个激活函数$\sigma$是分段线性的（如表[1](#S1.T1
    "表 1 ‣ 1.3 为什么非线性在人工神经元中重要 ‣ 1 简介 ‣ 当深度学习遇上多面体理论：综述")中所示），$f$也是一个分段线性函数。因此，任何以某种方式包含$f$的优化问题都将是一个分段线性优化问题。例如，在简单的情况下，当$f$的输出是单变量时，优化问题
- en: '|  | $\min_{x\in\mathcal{X}}f(x)$ |  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{x\in\mathcal{X}}f(x)$ |  |'
- en: 'is a piecewise linear optimization problem. As discussed in Section [3](#S3
    "3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), this problem can have an enormous number of “pieces” (linear
    regions) when $f$ is a neural network; solving this problem thus heavily depends
    on the size and structure of the neural network $f$. For example, the training
    procedure by which $f$ is obtained can greatly influence the performance of optimization
    strategies (Tjeng et al., [2019](#bib.bib309), Xiao et al., [2019](#bib.bib341)).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个分段线性优化问题。如第[3](#S3 "3 神经网络的线性区域 ‣ 当深度学习遇上多面体理论：综述")节讨论的那样，当$f$是神经网络时，这个问题可能有大量的“片段”（线性区域）；因此，解决这个问题严重依赖于神经网络$f$的大小和结构。例如，获取$f$的训练过程可以极大地影响优化策略的性能（Tjeng
    等，[2019](#bib.bib309)，Xiao 等，[2019](#bib.bib341)）。
- en: In this section, we first explore situations in which you might want to optimize
    over a trained neural network in this manner. We will then survey available methods
    for solving this method (either exactly or on the dual side) using polyhedral-based
    methods. We conclude with a brief view of future directions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先探讨你可能想以这种方式优化经过训练的神经网络的情况。然后，我们将调查使用基于多面体的方法来解决这种方法（无论是准确解决还是在对偶侧解决）的可用方法。最后，我们简要展望未来的发展方向。
- en: 4.1 Applications of optimization over trained networks
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 经过训练的网络上的优化应用
- en: 'Applications where you might want to optimize over a trained neural network
    $f$ broadly fall into two categories: those where $f$ is the “true” object of
    interest, and those where $f$ is a convenient proxy for some unknown, underlying
    behavior.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化训练好的神经网络 $f$ 的应用中，大致可以分为两类：那些 $f$ 是“真实”感兴趣对象的应用，以及那些 $f$ 是某种未知的、潜在行为的方便代理的应用。
- en: 4.1.1 Neural network verification
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 神经网络验证
- en: 'Neural network verification is a burgeoning field of study in deep learning.
    Starting in the early 2000s, researchers began to recognize the importance of
    rigorously verifying the behavior of neural networks, mainly in aviation-related
    applications (Schumann et al., [2003](#bib.bib274), Zakrzewski, [2001](#bib.bib351)).
    More recently, the seminal works of Szegedy et al. ([2014](#bib.bib300)) and Goodfellow
    et al. ([2015](#bib.bib124)) observed that neural networks are unusually susceptible
    to *adversarial attacks*. These are small, targeted perturbations that can drastically
    affect the output of the network; as shown in Figure [8](#S4.F8 "Figure 8 ‣ 4.1.1
    Neural network verification ‣ 4.1 Applications of optimization over trained networks
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), even powerful models such as MobileNetV2 (Sandler et al.,
    [2018](#bib.bib270)) are susceptible. The existence and prevalence of adversarial
    attacks in deep neural networks has raised justifiable concerns about the deployment
    of these models in mission-critical systems such as autonomous vehicles (Deng
    et al., [2020](#bib.bib79)), aviation (Kouvaros et al., [2021](#bib.bib177)),
    or medical systems (Finlayson et al., [2019](#bib.bib105)). One fascinating empirical
    work by Eykholt et al. ([2018](#bib.bib98)) showed the susceptibility of standard
    image classification networks that might be used in self-driving vehicles to a
    very analogue form of attacks: black/white stickers, placed in a careful way,
    could confuse these models enough that they would mis-classify road signs (e.g.,
    mistaking stop signs for “speed limit 80” signs).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '神经网络验证是深度学习中一个新兴的研究领域。从2000年代初期开始，研究人员逐渐认识到严格验证神经网络行为的重要性，主要是在航空相关的应用中（Schumann
    et al., [2003](#bib.bib274), Zakrzewski, [2001](#bib.bib351)）。最近，Szegedy et al.
    ([2014](#bib.bib300)) 和 Goodfellow et al. ([2015](#bib.bib124)) 的开创性工作观察到神经网络对*对抗攻击*异常敏感。这些是可以极大影响网络输出的小型、针对性的扰动；如图
    [8](#S4.F8 "Figure 8 ‣ 4.1.1 Neural network verification ‣ 4.1 Applications of
    optimization over trained networks ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") 所示，即便是强大的模型如 MobileNetV2
    (Sandler et al., [2018](#bib.bib270)) 也会受到影响。对抗攻击在深度神经网络中的存在和普遍性引发了对这些模型在关键任务系统中部署的合理担忧，如自动驾驶车辆（Deng
    et al., [2020](#bib.bib79)）、航空（Kouvaros et al., [2021](#bib.bib177)）或医疗系统（Finlayson
    et al., [2019](#bib.bib105)）。Eykholt et al. ([2018](#bib.bib98)) 的一项引人注目的实证研究展示了标准图像分类网络在自驾车中可能遭受的一种非常类似的攻击：黑白贴纸，以巧妙的方式放置，可能会使这些模型混淆，以至于错误分类交通标志（例如，将停车标志误认为“限速80”标志）。'
- en: <svg   height="130.22" overflow="visible" version="1.1" width="407.44"><g transform="translate(0,130.22)
    matrix(1 0 0 -1 0 0) translate(54.11,0) translate(0,65.11)" fill="#000000" stroke="#000000"
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -49.5 -60.5)" fill="#000000"
    stroke="#000000"><foreignobject width="99" height="121" transform="matrix(1 0
    0 -1 0 16.6)" overflow="visible">![Refer to caption](img/eb8e031e8c492fc9ac38e777fd76ac60.png)</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 67.05 -16.79)" fill="#000000" stroke="#000000"><foreignobject
    width="15.5" height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 100.11 -59.74)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/6ec4f02d882565048e57d58d5b3977d5.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 216.66 -15.47)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="7.31" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">=</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.71 -60.5)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="121" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/1df2f6052ecb61a06e1057f75e5d6880.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.82 43.78)" fill="#000000" stroke="#000000"><foreignobject width="75.57"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times(\epsilon=0.15)$</foreignobject></g></g></svg>
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/eb8e031e8c492fc9ac38e777fd76ac60.png) + ![参见说明](img/6ec4f02d882565048e57d58d5b3977d5.png)
    = ![参见说明](img/1df2f6052ecb61a06e1057f75e5d6880.png) $\times(\epsilon=0.15)$'
- en: 'Figure 8: Example of adversarial attack on MobileNetV2 (Sandler et al., [2018](#bib.bib270)).
    The original image taken by one of the survey authors is classified as ‘siberian_husky,’
    but is re-classified as ‘wallaby’ with a small (in an $\ell_{\infty}$-norm sense)
    targeted attack.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：对 MobileNetV2 的对抗攻击示例（Sandler 等，[2018](#bib.bib270)）。原始图像由一位调查作者拍摄，分类为‘siberian_husky’，但在经过一个小的（在$\ell_{\infty}$-范数意义上）有针对性的攻击后，被重新分类为‘wallaby’。
- en: Neural network verification seeks to prove (or disprove) a given input-output
    relationship, i.e., $x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$, that gives some
    indication of model robustness. Methods for verifying this relationship are classified
    as being sound and/or complete. A method that is sound will only certify the relationship
    if it is indeed true (no false positives), while a method that is complete will
    (i) always return an answer and (ii) only disprove the relationship if it is false
    (no false negatives). An early set of papers (Fischetti and Jo, [2018](#bib.bib106),
    Lomuscio and Maganti, [2017](#bib.bib200), Tjeng et al., [2019](#bib.bib309))
    recognized that MILP provides an avenue for verification that is both sound and
    complete, given that $\mathcal{X}$ and $f(x)$ are both linear, or piecewise linear.
    We refer the readers to recent reviews (Huang et al., [2020](#bib.bib157), Leofante
    et al., [2018](#bib.bib190), Li et al., [2022](#bib.bib191), Liu et al., [2021](#bib.bib197))
    for a more comprehensive treatment of the landscape of verification methods, including
    MILP- and LP-based technologies.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络验证旨在证明（或反驳）给定的输入输出关系，即 $x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$，这提供了一些模型鲁棒性的指示。验证这一关系的方法被分类为“健全的”和/或“完整的”。一种健全的方法只会在关系确实为真时才会认证该关系（没有假阳性），而一种完整的方法会
    (i) 始终返回一个答案，并且 (ii) 只有在关系为假时才会反驳该关系（没有假阴性）。早期的一些论文（Fischetti 和 Jo，[2018](#bib.bib106)，Lomuscio
    和 Maganti，[2017](#bib.bib200)，Tjeng 等，[2019](#bib.bib309)）认识到，MILP 提供了一条既健全又完整的验证途径，前提是
    $\mathcal{X}$ 和 $f(x)$ 都是线性的或分段线性的。我们推荐读者参考近期的综述（Huang 等，[2020](#bib.bib157)，Leofante
    等，[2018](#bib.bib190)，Li 等，[2022](#bib.bib191)，Liu 等，[2021](#bib.bib197)）以获得更全面的验证方法，包括基于
    MILP 和 LP 的技术。
- en: Example 4
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 示例 4
- en: Consider a classification network $f:[0,1]^{n_{0}}\to\mathbb{R}^{d}$ where the
    $j$-th output, $f(x)_{j}$, corresponds to the probability that input $x$ is of
    class $j$.²²2In actuality, we will instead typically work with the outputs corresponding
    to “logits”, or unnormalized probabilities. These are typically fed into a softmax
    layer that then normalize these values to correspond to a probability distribution
    over the classes. However, this nonlinear softmax transformation is not piecewise
    linear. Thankfully, it can be omitted in the context of the verification task
    without loss of generality. Then consider a labeled image $\hat{x}$ known to be
    of class $i$, and a “target” adversarial class $k\neq i$. Then verifying local
    robustness of the prediction corresponds to checking $x\in\{x:||x-\hat{x}||\leq\epsilon\}\Rightarrow
    y=f(x)\in\{y:y_{i}\geq y_{k}\}$, where $\epsilon>0$ is a constant which prescribes
    the radius around which $\hat{x}$ we will search for an adversarial example.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个分类网络 $f:[0,1]^{n_{0}}\to\mathbb{R}^{d}$，其中第 $j$ 个输出 $f(x)_{j}$ 对应于输入 $x$
    属于类别 $j$ 的概率。²²2 实际上，我们通常处理的是对应于“logits”或未归一化概率的输出。这些输出通常会输入到一个 softmax 层，该层会将这些值归一化为对应于类别的概率分布。然而，这种非线性
    softmax 变换并不是分段线性的。幸运的是，在验证任务的背景下，可以省略这一步而不会失去一般性。然后考虑一个已知属于类别 $i$ 的标记图像 $\hat{x}$，以及一个“目标”对抗类别
    $k\neq i$。然后，验证预测的局部鲁棒性对应于检查 $x\in\{x:||x-\hat{x}||\leq\epsilon\}\Rightarrow y=f(x)\in\{y:y_{i}\geq
    y_{k}\}$，其中 $\epsilon>0$ 是一个常数，规定了我们将围绕 $\hat{x}$ 搜索对抗样本的半径。
- en: 'This verification task can be formulated as an optimization problem of the
    form:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个验证任务可以被表述为如下形式的优化问题：
- en: '|  | $\displaystyle\max_{x\in[0,1]^{n_{0}}}$ | $\displaystyle f(x)_{k}-f(x)_{i}$
    |  | (3) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{x\in[0,1]^{n_{0}}}$ | $\displaystyle f(x)_{k}-f(x)_{i}$
    |  | (3) |'
- en: '|  | s.t. | $\displaystyle&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon.$ |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon.$ |  |'
- en: 'Any feasible solution $x$ to this problem with positive cost is an adversarial
    example: it is very “close” to $\hat{x}$ which has true label $i$, yet the network
    believes it is more likely to be of class $k$.³³3Alternative objectives are sometimes
    used which would allow us to strengthen this statement to say that the network
    *will* classify $x$ to be of class $k$. However, this will require a more complex
    reformulation to model this problem via MILP, so we omit it for simplicity. If,
    on the other hand, it is proven that the optimal objective value is negative,
    this proves that $f$ is robust (at least in the neighborhood around $\tilde{x}$).
    Note that the verification problem can terminate once the sign of the optimal
    objective value is determined, but solving the problem returns an optimal adversarial
    example.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，任何具有正成本的可行解 $x$ 都是一个对抗样本：它非常“接近”具有真实标签 $i$ 的 $\hat{x}$，但网络认为它更可能属于类 $k$。³³3
    有时使用替代目标，这将使我们能够加强这个声明，表明网络 *将* 把 $x$ 分类为类 $k$。然而，这将需要更复杂的重构来通过 MILP 建模这个问题，因此为了简便起见，我们省略了它。另一方面，如果证明最优目标值为负，则证明
    $f$ 是稳健的（至少在 $\tilde{x}$ 附近）。请注意，一旦确定了最优目标值的符号，验证问题可以终止，但解决该问题将返回一个最优对抗样本。
- en: 'The objective function of ([3](#S4.E3 "In Example 4 ‣ 4.1.1 Neural network
    verification ‣ 4.1 Applications of optimization over trained networks ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")) models the desired input-output relationship, $x\in\mathcal{X}\Rightarrow
    y\in\mathcal{Y}$, while the constraints model the domain $\mathcal{X}$. The domain
    $\mathcal{X}$ is typically a box or hyperrectangular domain. Extensions to this
    are described in Section [4.4.1](#S4.SS4.SSS1 "4.4.1 Extending to other domains
    ‣ 4.4 Generalizing the single neuron model ‣ 4 Optimizing Over a Trained Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). Some explanation-focused
    verification applications define the input-output relationship in a derivative
    sense, e.g., $x\in\mathcal{X}\Rightarrow\partial y/\partial x\in\mathcal{Y}^{\prime}$
    (Wicker et al., [2022](#bib.bib333)). As the derivative of the ReLU function is
    also piecewise linear, this class of problems can also be modeled in MILP. For
    example, in the context of fairness and explainability, Liu et al. ([2020](#bib.bib198))
    and Jordan and Dimakis ([2020](#bib.bib166)) used MILP to certify monotonicity
    and to compute local Lipschitz constants, respectively.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 目标函数 ([3](#S4.E3 "在示例 4 ‣ 4.1.1 神经网络验证 ‣ 4.1 训练网络的优化应用 ‣ 4 优化训练神经网络 ‣ 深度学习与多面体理论的结合：综述"))
    建模了期望的输入输出关系，$x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$，而约束条件建模了域 $\mathcal{X}$。域
    $\mathcal{X}$ 通常是一个盒子或超矩形域。对此的扩展在第 [4.4.1](#S4.SS4.SSS1 "4.4.1 扩展到其他领域 ‣ 4.4 一般化单神经元模型
    ‣ 4 优化训练神经网络 ‣ 深度学习与多面体理论的结合：综述") 节中描述。一些以解释为重点的验证应用在导数意义上定义输入输出关系，例如 $x\in\mathcal{X}\Rightarrow\partial
    y/\partial x\in\mathcal{Y}^{\prime}$ (Wicker 等，[2022](#bib.bib333))。由于 ReLU 函数的导数也是分段线性的，这类问题也可以用
    MILP 进行建模。例如，在公平性和可解释性的背景下，Liu 等 ([2020](#bib.bib198)) 和 Jordan 与 Dimakis ([2020](#bib.bib166))
    分别使用 MILP 认证单调性和计算局部 Lipschitz 常数。
- en: 'Although in this survey we focus on optimization over trained neural networks,
    it is important to note that polyhedral theory underlies numerous strategies for
    neural network verification. For example, SAT and SMT (Satisfiability Modulo Theories)
    solvers designed for Boolean satisfiability problems (and more general problems
    for the case of SMT) can also be used to search through activation patterns for
    a neural network (Pulina and Tacchella, [2010](#bib.bib251)), resulting in tools
    that are sound and complete, such as Planet (Ehlers, [2017](#bib.bib87)) and Reluplex
    (Katz et al., [2017](#bib.bib169)). Bunel et al. ([2018](#bib.bib43)) presented
    a unified view to compare MILP and SMT formulations, as well as the relaxations
    that result from these formulations (we will revisit this in Section [4.3](#S4.SS3
    "4.3 Scaling further: Convex relaxations and linear programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")). On the other hand, strategies such as ExactReach (Xiang et al., [2017](#bib.bib340))
    exploit polyhedral theory to compute reachable sets: given an input set to a ReLU
    function defined as a union of polytopes, the output reachable set is also a union
    of polytopes. Other methods over-approximate the reachable set to improve scalability,
    e.g., for vision models (Yang et al., [2021](#bib.bib349)), often resulting in
    methods that are sound, but not complete.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然在这项调查中我们关注的是训练神经网络上的优化，但值得注意的是，立体几何理论是神经网络验证众多策略的基础。例如，设计用于布尔可满足性问题的 SAT
    和 SMT（模理论可满足性）求解器（对于 SMT 的一般问题）也可以用于搜索神经网络的激活模式（Pulina 和 Tacchella，[2010](#bib.bib251)），从而生成如
    Planet（Ehlers，[2017](#bib.bib87)）和 Reluplex（Katz 等，[2017](#bib.bib169)）等健全且完整的工具。Bunel
    等人（[2018](#bib.bib43)）提出了一个统一的视角来比较 MILP 和 SMT 公式，以及这些公式带来的松弛（我们将在第[4.3节](#S4.SS3
    "4.3 Scaling further: Convex relaxations and linear programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")中重新审视这一点）。另一方面，像 ExactReach（Xiang 等，[2017](#bib.bib340)）这样的策略利用立体几何理论来计算可达集：给定一个定义为多面体并集的
    ReLU 函数的输入集，输出可达集也是一个多面体并集。其他方法通过过度近似可达集来提高可扩展性，例如，针对视觉模型（Yang 等，[2021](#bib.bib349)），通常会导致健全但不完整的方法。'
- en: 4.1.2 Neural network as proxy
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 神经网络作为代理
- en: Another situation in which you may want to solve an optimization problem containing
    trained neural networks is when you would like to optimize some other, unknown
    function for which you have historical input/output data. A similar situation
    arises when you want to solve an optimization problem where (some of) the constraints
    are overly complicated, but you can query samples from the constraints on which
    to train a simpler surrogate model. In these cases, you might imagine training
    a neural network in a standard supervised learning setting to approximate this
    underlying, unknown or complicated function. Then, since the neural network is
    known, you are left with a deterministic piecewise linear optimization problem.
    Note that we focus here on using a neural network as a surrogate; neural networks
    can additionally learn other components of an optimization problem, e.g., uncertainty
    sets for robust optimization (Goerigk and Kurtz, [2023](#bib.bib122)).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会希望在包含训练神经网络的优化问题中解决一些其他未知函数的优化问题，这些函数的历史输入/输出数据已知。类似的情况发生在你想要解决一个优化问题，其中（某些）约束条件过于复杂，但你可以查询约束样本以训练一个更简单的代理模型。在这些情况下，你可能会想象在标准监督学习设置中训练一个神经网络来逼近这个基础的、未知的或复杂的函数。然后，由于神经网络是已知的，你将面临一个确定性的分段线性优化问题。请注意，我们这里关注的是将神经网络用作代理；神经网络还可以学习优化问题的其他组成部分，例如，用于鲁棒优化的不确定性集（Goerigk
    和 Kurtz，[2023](#bib.bib122)）。
- en: Several software tools have been developed for this class of problems. For the
    case of constraint learning, JANOS (Bergman et al., [2022](#bib.bib24)) and OptiCL
    (Maragno et al., [2021](#bib.bib210)) both provide functionality for learning
    a ReLU neural network to approximate a constraint based on data and embedding
    the learned neural network in MILP. The reluMIP package (Lueg et al., [2021](#bib.bib203))
    has also been introduced to handle the latter embedding step. More generally,
    OMLT (Ceccon et al., [2022](#bib.bib47)) translates neural networks to pyomo optimization
    blocks, including various MILP formulations and activation functions. Finally,
    recent developments in gurobipy⁴⁴4https://github.com/Gurobi/gurobi-machinelearning
    enable directly parsing in trained neural networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为此类问题开发了几种软件工具。对于约束学习的情况，JANOS（Bergman 等，[2022](#bib.bib24)）和 OptiCL（Maragno
    等，[2021](#bib.bib210)）都提供了学习 ReLU 神经网络以基于数据近似约束的功能，并将学习到的神经网络嵌入到 MILP 中。reluMIP
    包（Lueg 等，[2021](#bib.bib203)）也被引入以处理后者的嵌入步骤。更一般来说，OMLT（Ceccon 等，[2022](#bib.bib47)）将神经网络转化为
    pyomo 优化块，包括各种 MILP 表述和激活函数。最后，gurobipy 的最新发展⁴⁴4https://github.com/Gurobi/gurobi-machinelearning
    使得可以直接解析训练好的神经网络。
- en: Applications of this paradigm can be envisioned in a number of domain areas.
    This approach is common in deep reinforcement learning, where neural networks
    are used to approximate an unknown “$Q$-function” which models the long-term cost
    of taking a particular action in a particular state of the world. In $Q$-learning,
    this $Q$-function is optimized iteratively to produce new candidate policies,
    which are then evaluated (typically via simulation) to produce new training data
    for future iterations. Optimization over the learned $Q$-function must be relatively
    fast in control applications, and several practical methods have been proposed.
    When the action space is discrete, the $Q$-function neural network is trained
    with one output value for each possible action, simplifying optimization to evaluating
    the model and selecting the largest output. Continuous action spaces require the
    $Q$ network be optimized over (Burtea and Tsay, [2023](#bib.bib45), Delarue et al.,
    [2020](#bib.bib78), Ryu et al., [2020](#bib.bib267)), or an “actor” network can
    be trained to learn the optimal actions (Lillicrap et al., [2015](#bib.bib193)).
    In a related vein, ReLU neural networks can be used as a process model for optimal
    scheduling or control (Wu et al., [2020](#bib.bib338)).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在多个领域中设想这种范式的应用。这种方法在深度强化学习中很常见，在这种方法中，神经网络用于近似未知的“$Q$-函数”，该函数模型描述了在特定世界状态下采取特定行动的长期成本。在
    $Q$-学习中，这个 $Q$-函数被迭代优化，以产生新的候选策略，然后通过模拟等方式进行评估，从而生成未来迭代的新的训练数据。在控制应用中，对学习到的 $Q$-函数的优化必须相对快速，已经提出了几种实际方法。当动作空间是离散的时，$Q$-函数神经网络为每个可能的动作训练一个输出值，这简化了优化为评估模型并选择最大的输出。连续动作空间需要对
    $Q$ 网络进行优化（Burtea 和 Tsay，[2023](#bib.bib45)，Delarue 等，[2020](#bib.bib78)，Ryu 等，[2020](#bib.bib267)），或者可以训练一个“演员”网络来学习最佳动作（Lillicrap
    等，[2015](#bib.bib193)）。在相关领域，ReLU 神经网络可以作为最优调度或控制的过程模型（Wu 等，[2020](#bib.bib338)）。
- en: Chemical engineering also presents applications where surrogate models have
    proven beneficial for optimization, as is the subject of recent reviews (Bhosekar
    and Ierapetritou, [2018](#bib.bib28), McBride and Sundmacher, [2019](#bib.bib216),
    Tsay and Baldea, [2019](#bib.bib311)). In particular, ReLU neural networks can
    be seamlessly embedded in larger MILP problems such as flow networks and reservoir
    control where the other constraints are also mixed-integer linear (Grimstad and
    Andersson, [2019](#bib.bib133), Say et al., [2017](#bib.bib272), Yang et al.,
    [2022](#bib.bib347)). Focusing on control applications where the neural network
    is embedded in a MILP that must be solved repeatedly, Katz et al. ([2020](#bib.bib171))
    showed how multiparametric programming can be used to learn the solution map of
    the resulting MILP itself, which is also piecewise affine. An emerging area of
    research uses verification tools to reason about neural networks used as controllers,
    e.g., see Johnson et al. ([2020](#bib.bib165)). These applications involve optimization
    formulations combining the neural network with constraints defining the controlled
    system. For example, verification can be used to bound the reachable set (Sidrane
    et al., [2022](#bib.bib286)) (alongside piecewise linear bounds on the dynamical
    system) or the maximum error against a baseline controller (Schwan et al., [2022](#bib.bib275)).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 化工工程也展现了在优化中替代模型的有益应用，这也是最近评论的主题（Bhosekar 和 Ierapetritou，[2018](#bib.bib28)，McBride
    和 Sundmacher，[2019](#bib.bib216)，Tsay 和 Baldea，[2019](#bib.bib311)）。特别是，ReLU神经网络可以无缝地嵌入到更大的MILP问题中，如流网络和储罐控制，其中其他约束也是混合整数线性的（Grimstad
    和 Andersson，[2019](#bib.bib133)，Say 等，[2017](#bib.bib272)，Yang 等，[2022](#bib.bib347)）。专注于控制应用，其中神经网络嵌入在必须反复求解的MILP中，Katz
    等（[2020](#bib.bib171)）展示了如何使用多参数编程来学习结果MILP本身的解映射，该映射也是分段线性的。一个新兴的研究领域使用验证工具来推理作为控制器使用的神经网络，例如，见
    Johnson 等（[2020](#bib.bib165)）。这些应用涉及优化公式，将神经网络与定义被控系统的约束结合在一起。例如，可以使用验证来界定可达集（Sidrane
    等，[2022](#bib.bib286)）（以及对动态系统的分段线性界限）或与基线控制器的最大误差（Schwan 等，[2022](#bib.bib275)）。
- en: Finally, applications for optimization over neural networks arise in machine
    learning applications. MILP formulations can be used to compress neural networks
    (Serra et al., [2020](#bib.bib283), [2021](#bib.bib284), ElAraby et al., [2020](#bib.bib88)),
    which consequently result in more tractable surrogate models (Kody et al., [2022](#bib.bib176)).
    The main idea is to use MILP to identify stable nodes, i.e., nodes that are always
    on or off over an input domain, which can then be algebraically eliminated. Optimization
    has also been employed in techniques for feature selection, based on identifying
    strongest input nodes (Sildir and Aydin, [2022](#bib.bib287), Zhao et al., [2023](#bib.bib360)).
    In the context of Bayesian optimization, Volpp et al. ([2020](#bib.bib323)) use
    reinforcement learning to meta-learn acquisition functions parameterized as neural
    networks; selecting ensuing query points then requires optimization over the trained
    acquisition function. Later work modeled both the acquisition function and feasible
    region in black-box optimization as neural networks (Papalexopoulos et al., [2022](#bib.bib239)).
    In that work, exploration and exploitation are balanced via Thompson sampling
    and training multiple neural networks from a random parameter initialization.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，神经网络优化的应用在机器学习应用中出现。MILP（混合整数线性规划）公式可以用来压缩神经网络（Serra 等，[2020](#bib.bib283)，[2021](#bib.bib284)，ElAraby
    等，[2020](#bib.bib88)），从而产生更易处理的替代模型（Kody 等，[2022](#bib.bib176)）。主要思想是使用MILP来识别稳定的节点，即在输入域中始终开或关的节点，这些节点可以通过代数方式消除。优化技术还被用于特征选择，基于识别最强的输入节点（Sildir
    和 Aydin，[2022](#bib.bib287)，Zhao 等，[2023](#bib.bib360)）。在贝叶斯优化的背景下，Volpp 等（[2020](#bib.bib323)）使用强化学习来元学习作为神经网络参数化的获取函数；选择后续查询点则需要对训练好的获取函数进行优化。后续的工作将获取函数和黑箱优化中的可行区域建模为神经网络（Papalexopoulos
    等，[2022](#bib.bib239)）。在该工作中，通过汤普森采样和平衡多个从随机参数初始化训练的神经网络来平衡探索与利用。
- en: A word of caution
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 需要注意的是
- en: Standard supervised learning algorithms aim to learn a function which fits the
    underlying function according to some distribution under which the data is generated.
    However, optimizing a function corresponds to evaluating it at a single point.
    This means that you may end up with a model that well-approximates the underlying
    function in distribution, but for which the pointwise minimizer is a poor approximation
    of the true function. This phenomena is referred to as the “Optimizer’s curse”
    (Smith and Winkler, [2006](#bib.bib293)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的监督学习算法旨在学习一个函数，该函数根据生成数据的某些分布拟合基础函数。然而，优化一个函数对应于在一个点上评估它。这意味着你可能会得到一个在分布上很好地逼近基础函数的模型，但其逐点最小化器对真实函数的逼近却较差。这种现象被称为“优化器的诅咒”（Smith
    and Winkler, [2006](#bib.bib293)）。
- en: 4.1.3 Single neuron relaxations
  id: totrans-223
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 单神经元松弛
- en: 'For the following subsections, consider the $i$-th neuron in the $l$-th layer
    of a neural network, endowed with a ReLU activation function, whose behavior is
    governed by ([2](#S1.E2 "In 1.1 What neural networks can model ‣ 1 Introduction
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). Presume that a input
    domain of interest $\mathcal{D}^{l-1}\subset\mathbb{R}^{n_{l}}$ is a bounded region.
    Further, since $\mathcal{D}^{l-1}$ is bounded, presume that finite bounds are
    known on each input component, i.e. that vectors $L^{l-1},U^{l-1}\in\mathbb{R}^{n_{l}}$
    are known such that $\mathcal{D}^{l-1}\subseteq[L^{l-1},U^{l-1}]\subset\mathbb{R}^{n_{l}}$.
    We can then write the *graph* of the neuron, which couples together the input
    and the output of the nonlinear ReLU activation function:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下小节，考虑神经网络第$l$层的第$i$个神经元，配备有ReLU激活函数，其行为由([2](#S1.E2 "在1.1 神经网络可以建模的内容 ‣
    1 引言 ‣ 当深度学习遇上多面体理论：综述"))所控制。假设感兴趣的输入域$\mathcal{D}^{l-1}\subset\mathbb{R}^{n_{l}}$是一个有界区域。此外，由于$\mathcal{D}^{l-1}$是有界的，假设已知每个输入分量的有限界，即已知向量$L^{l-1},U^{l-1}\in\mathbb{R}^{n_{l}}$，使得$\mathcal{D}^{l-1}\subseteq[L^{l-1},U^{l-1}]\subset\mathbb{R}^{n_{l}}$。然后，我们可以写出神经元的*图*，它将非线性ReLU激活函数的输入和输出结合在一起：
- en: '|  | $\displaystyle\texttt{gr}=$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}=0\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}}$
    |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\texttt{gr}=$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}=0\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}}$
    |  |'
- en: '|  | $\displaystyle\cup$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}={\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\geq
    0}.$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\cup$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}={\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\geq
    0}.$ |  |'
- en: This is a disjunctive representation for gr in terms of two polyhedral alternatives.
    We assume that every included neuron exhibits this disjunction, i.e., every neuron
    can be on or off depending on the model input. This assumption of *strict activity*
    implies that $L^{l-1}<0$ and $U^{l-1}>0$, noting that neurons not satisfying this
    property can be exactly pruned from the model (Serra et al., [2020](#bib.bib283)).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对$gr$的离散表示，以两种多面体替代形式进行表示。我们假设每个包含的神经元都表现出这种离散，即每个神经元可以根据模型输入开或关。这种*严格活动*的假设意味着$L^{l-1}<0$和$U^{l-1}>0$，注意到不满足此属性的神经元可以从模型中精确修剪掉（Serra
    et al., [2020](#bib.bib283)）。
- en: We observe that, given this (or any) formulation for each individual unit, it
    is straightforward to construct a formulation for the entire network. For example,
    if we take $X^{l}_{i}=\Set{({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})}{\eqref{eqn:relu-big-m}}$
    for each layer $l$ and each unit $i$, we can construct a MILP formulation for
    the graph of the entire network, $\Set{(x,f(x)):x\in\mathcal{D}^{0}}$ as
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，给定这种（或任何）每个单元的表述，构造整个网络的表述是简单的。例如，如果我们为每层$l$和每个单元$i$取$X^{l}_{i}=\Set{({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})}{\eqref{eqn:relu-big-m}}$，我们可以构造整个网络的MILP表述，$\Set{(x,f(x)):x\in\mathcal{D}^{0}}$。
- en: '|  | $({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})\in X^{l}_{i}\quad\forall l\in{\mathbb{L}},i\in\llbracket
    n_{l}\rrbracket.$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})\in X^{l}_{i}\quad\forall l\in{\mathbb{L}},i\in\llbracket
    n_{l}\rrbracket.$ |  |'
- en: This also generalizes in a straightforward manner to more complex feedforward
    network architectures (e.g. convolutions, or sparse or skip connections), though
    we omit the explicit description for notational simplicity.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以简单地推广到更复杂的前馈网络架构（例如卷积，或稀疏或跳跃连接），尽管为了符号的简洁性，我们省略了明确的描述。
- en: 4.1.4 Beyond the scope of this survey
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 超出本调查的范围
- en: 'The effectiveness of the single-neuron formulations described above is bounded
    by the tightness of the optimal univariate formulation; this property is known
    as the “single-neuron barrier” (Salman et al., [2019](#bib.bib269)). This has
    motivated research in convex relaxations that jointly account for multiple neurons
    within a layer (Singh et al., [2019a](#bib.bib290)). Nevertheless, the analysis
    of polyhedral formulations for multiple neurons simultaneously quickly becomes
    intractable, and is beyond the scope of this survey. Instead, we point the interested
    reader to the recent survey by Roth ([2021](#bib.bib263)), and highlight a few
    approaches taken in the literature. Multi-neuron analysis has been used to: improve
    bounds tightening schemes (Rössig and Petkovic, [2021](#bib.bib262)), prune linearizable
    neurons (Botoeva et al., [2020](#bib.bib37)), design dual decompositions (Ferrari
    et al., [2022](#bib.bib104)), and generate strengthening inequalities (Serra and
    Ramalingam, [2020](#bib.bib281)). Similarly, we do not review formulations for
    ensembles of ReLU networks, though MILP formulations have been proposed (Wang
    et al., [2021](#bib.bib324), [2023](#bib.bib325)).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 上述单神经元公式的有效性受到最优单变量公式紧密性的限制；这一属性被称为“单神经元障碍”（Salman等，[2019](#bib.bib269)）。这促使了在凸松弛中进行研究，联合考虑层内的多个神经元（Singh等，[2019a](#bib.bib290)）。然而，对多个神经元的多面体公式进行分析迅速变得不可处理，超出了本次调查的范围。相反，我们将感兴趣的读者引导到Roth（[2021](#bib.bib263)）的最新调查，并强调文献中采取的一些方法。多神经元分析已被用于：改进边界紧缩方案（Rössig和Petkovic，[2021](#bib.bib262)），修剪线性化神经元（Botoeva等，[2020](#bib.bib37)），设计对偶分解（Ferrari等，[2022](#bib.bib104)），以及生成强化不等式（Serra和Ramalingam，[2020](#bib.bib281)）。同样，我们不对ReLU网络集合的公式进行回顾，尽管已提出了MILP公式（Wang等，[2021](#bib.bib324)，[2023](#bib.bib325)）。
- en: Additionally, recent works have exploited polyhedral structure to develop sampling
    based strategies, which can be used to warm-start MILP or accelerate local search
    in verification (Perakis and Tsiourvas, [2022](#bib.bib245), Wu et al., [2022](#bib.bib339)).
    Lombardi et al. ([2017](#bib.bib199)) computationally compare MILP against local
    search and constraint programming approaches. In a related vein, Cheon ([2022](#bib.bib58))
    examines local solutions and proposes an outer approximation method to improve
    gradient-based optimization. Finally, following Raghunathan et al. ([2018](#bib.bib254)),
    a large body of work has presented optimization-based methods for verification
    that use semidefinite programming concepts (Dathathri et al., [2020](#bib.bib75),
    Fazlyab et al., [2020](#bib.bib102), Newton and Papachristodoulou, [2021](#bib.bib232)).
    Notably, Batten et al. ([2021](#bib.bib17)) showed how combining semidefinite
    and MILP formulations can produce a new formulation that is tighter than both.
    This was later extended with reformulation-linearization technique, or RLT, cuts
    (Lan et al., [2022](#bib.bib183)). While related to linear programming and other
    methods based on convex relaxations, this stream of work is beyond the scope of
    this survey. We refer the reader to Zhang ([2020](#bib.bib358)) for a discussion
    of the tightness of these formulations.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近的研究利用了多面体结构来开发基于采样的策略，这些策略可以用于为MILP提供热启动或加速验证中的局部搜索（Perakis和Tsiourvas，[2022](#bib.bib245)，Wu等，[2022](#bib.bib339)）。Lombardi等人（[2017](#bib.bib199)）对MILP与局部搜索和约束编程方法进行了计算比较。在相关的研究中，Cheon（[2022](#bib.bib58)）考察了局部解决方案，并提出了一种外部逼近方法来改进基于梯度的优化。最后，继Raghunathan等人（[2018](#bib.bib254)）之后，大量工作提出了用于验证的基于优化的方法，这些方法使用了半正定规划概念（Dathathri等，[2020](#bib.bib75)，Fazlyab等，[2020](#bib.bib102)，Newton和Papachristodoulou，[2021](#bib.bib232)）。值得注意的是，Batten等人（[2021](#bib.bib17)）展示了如何将半正定和MILP公式结合起来，从而产生比两者都更紧的新的公式。之后，这一方法通过重构-线性化技术（RLT）得到了扩展（Lan等，[2022](#bib.bib183)）。虽然与线性规划及其他基于凸松弛的方法有关，但这一研究方向超出了本次调查的范围。我们建议读者参考Zhang（[2020](#bib.bib358)）讨论这些公式的紧密性。
- en: 4.2 Exact models using mixed-integer programming
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 使用混合整数规划的精确模型
- en: Mixed-integer programming offers a powerful algorithmic framework for *exactly*
    modeling nonconvex piecewise linear functions. The Operations Research community
    has studied has a long and storied history of developing MILP-based methods for
    piecewise linear optimization, with research spanning decades (Croxton et al.,
    [2003](#bib.bib69), Dantzig, [1960](#bib.bib73), Geißler et al., [2012](#bib.bib117),
    Huchette and Vielma, [2022](#bib.bib158), Lee and Wilson, [2001](#bib.bib189),
    Misener and Floudas, [2012](#bib.bib221), Padberg, [2000](#bib.bib238), Vielma
    et al., [2010](#bib.bib318)). However, many of these techniques are specialized
    for low-dimensional or separable piecewise linear functions. While a reasonable
    assumption in many OR problems, this is not the case when modeling neurons in
    a neural network. Therefore, the standard approach in the literature is to apply
    general-purpose MILP formulation techniques to model neural networks.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 混合整数规划提供了一个强大的算法框架，用于*准确地*建模非凸分段线性函数。运筹学界在开发基于MILP的分段线性优化方法方面有着悠久的历史，研究历时几十年（Croxton
    等，[2003](#bib.bib69)，Dantzig，[1960](#bib.bib73)，Geißler 等，[2012](#bib.bib117)，Huchette
    和 Vielma，[2022](#bib.bib158)，Lee 和 Wilson，[2001](#bib.bib189)，Misener 和 Floudas，[2012](#bib.bib221)，Padberg，[2000](#bib.bib238)，Vielma
    等，[2010](#bib.bib318)）。然而，这些技术中的许多都是专门针对低维或可分离的分段线性函数。虽然这是许多运筹学问题中的合理假设，但在建模神经网络中的神经元时情况并非如此。因此，文献中的标准方法是应用通用MILP公式化技术来建模神经网络。
- en: Connection to Boolean satisfiability
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 布尔可满足性关联
- en: Some SMT-based methods such as Reluplex (Katz et al., [2017](#bib.bib169)) and
    Planet (Ehlers, [2017](#bib.bib87)) effectively construct branching technologies
    similar to MILP solvers. Indeed, Marabou (Katz et al., [2019](#bib.bib170)) builds
    on Reluplex, and a recent extension MarabouOpt can optimize over trained neural
    networks (Strong et al., [2021](#bib.bib295)). The authors also outline general
    procedures to extend verification solvers to optimization. Our focus in this review
    is on more general MILP formulations, or those that can be incorporated into off-the-shelf
    MILP solvers with relative ease. Bunel et al. ([2020b](#bib.bib42), [2018](#bib.bib43))
    provide a more comprehensive discussion of similarities and differences to SMT.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基于SMT的方法，如 Reluplex（Katz 等，[2017](#bib.bib169)）和 Planet（Ehlers，[2017](#bib.bib87)），有效地构建了类似于MILP求解器的分支技术。事实上，Marabou（Katz
    等，[2019](#bib.bib170)）在 Reluplex 的基础上构建，而最近的扩展 MarabouOpt 可以在训练好的神经网络上进行优化（Strong
    等，[2021](#bib.bib295)）。作者还概述了将验证求解器扩展到优化的一般程序。我们在这篇综述中的重点是更通用的MILP公式化，或者那些可以相对容易地并入现成MILP求解器中的公式。Bunel
    等（[2020b](#bib.bib42)，[2018](#bib.bib43)）提供了对SMT的相似性和差异的更全面的讨论。
- en: 4.2.1 The big-$M$ formulation
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 大-$M$ 公式化
- en: 'The big-$M$ method is a standard technique used to formulate logic and disjunctive
    constraints using mixed-integer programming (Bonami et al., [2015](#bib.bib35),
    Vielma, [2015](#bib.bib316)). Big-$M$ formulations are typically very simple to
    reason about and implement, and are quite compact, though their convex relaxations
    can often be quite poor, leading to weak dual bounds and (often) slow convergence
    when passed to a mixed-integer programming solver. Since gr is a disjunctive set,
    the big-$M$ technique can be applied to produce the following formulation:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 大-$M$ 方法是一种标准技术，用于通过混合整数规划（Bonami 等，[2015](#bib.bib35)，Vielma，[2015](#bib.bib316)）来公式化逻辑和析取约束。大-$M$
    公式化通常非常简单易于理解和实现，且相当紧凑，但它们的凸松弛通常较差，导致对偶界限较弱，并且（通常）在传递给混合整数规划求解器时收敛速度较慢。由于 gr 是一个析取集合，大-$M$
    技术可以应用于产生以下公式：
- en: '|  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (4a) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (4a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\left({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\right)-M^{l}_{i,-}(1-z)$
    |  | (4b) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\left({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\right)-M^{l}_{i,-}(1-z)$
    |  | (4b) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq M^{l}_{i,+}z$ |  | (4c)
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq M^{l}_{i,+}z$ |  | (4c)
    |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}$ |  | (4d) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}$ |  | (4d) |'
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\}.$ |  | (4e) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\}.$ |  | (4e) |'
- en: Here, $M^{l}_{i,-}$ and $M^{l}_{i,+}$ are data which must satisfy the inequalities
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$M^{l}_{i,-}$ 和 $M^{l}_{i,+}$ 是需要满足不等式的数据。
- en: '|  | $\displaystyle M^{l}_{i,-}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M^{l}_{i,-}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  |'
- en: '|  | $\displaystyle M^{l}_{i,+}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}.$
    |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M^{l}_{i,+}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}.$
    |  |'
- en: This big-$M$ formulation for ReLU-based networks has been used extensively in
    the literature (Bunel et al., [2018](#bib.bib43), Cheng et al., [2017](#bib.bib56),
    Dutta et al., [2018](#bib.bib83), Fischetti and Jo, [2018](#bib.bib106), Kumar
    et al., [2019](#bib.bib181), Lomuscio and Maganti, [2017](#bib.bib200), Serra
    and Ramalingam, [2020](#bib.bib281), Serra et al., [2018](#bib.bib282), Tjeng
    et al., [2019](#bib.bib309), Xiao et al., [2019](#bib.bib341)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种针对基于 ReLU 的网络的 big-$M$ 形式化方法在文献中被广泛使用（Bunel et al., [2018](#bib.bib43), Cheng
    et al., [2017](#bib.bib56), Dutta et al., [2018](#bib.bib83), Fischetti 和 Jo,
    [2018](#bib.bib106), Kumar et al., [2019](#bib.bib181), Lomuscio 和 Maganti, [2017](#bib.bib200),
    Serra 和 Ramalingam, [2020](#bib.bib281), Serra et al., [2018](#bib.bib282), Tjeng
    et al., [2019](#bib.bib309), Xiao et al., [2019](#bib.bib341))。
- en: 'The big-$M$ formulation is compact, with one binary variable and $\mathcal{O}(1)$
    general inequality constraints for each neuron. Applied for each unit in the network,
    this leads to a MILP formulation with $\mathcal{O}(\sum_{l\in{\mathbb{L}}}n_{l})=\mathcal{O}(Ln_{\max})$
    binary variables and general inequality constraints, where $n_{\max}=\max_{l\in{\mathbb{L}}}n_{L}$.
    However, it has been observed (Anderson et al., [2019](#bib.bib4), [2020](#bib.bib5))
    that this big-$M$ formulation is not strong in the sense that its LP relaxation
    does not, in general, capture the convex hull of the graph of a given unit; see
    Figure [9](#S4.F9 "Figure 9 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey") for an illustration. In fact,
    this LP relaxation can be arbitrarily bad (Anderson et al., [2019](#bib.bib4),
    Example 2), even in fixed input dimension. As MILP solvers often bound the objective
    function between the best feasible point and its tightest optimal continuous relaxation,
    a weak formulation can negatively impact performance, often substantially.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'big-$M$ 形式化方法是紧凑的，每个神经元有一个二元变量和 $\mathcal{O}(1)$ 一般不等式约束。应用于网络中的每个单元，这导致了一个具有
    $\mathcal{O}(\sum_{l\in{\mathbb{L}}}n_{l})=\mathcal{O}(Ln_{\max})$ 个二元变量和一般不等式约束的
    MILP 形式，其中 $n_{\max}=\max_{l\in{\mathbb{L}}}n_{L}$。然而，已经观察到（Anderson et al., [2019](#bib.bib4),
    [2020](#bib.bib5)），这种 big-$M$ 形式化方法在 LP 松弛方面并不强，因为它通常无法捕捉到给定单元的图的凸包；请参见图 [9](#S4.F9
    "Figure 9 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey") 以获得说明。实际上，这种 LP 松弛可能会非常糟糕（Anderson et al.,
    [2019](#bib.bib4), Example 2），即使在固定输入维度下也是如此。由于 MILP 求解器通常在最佳可行点和其最紧密的最优连续松弛之间对目标函数进行界定，弱形式化方法可能会对性能产生负面影响，通常影响很大。'
- en: 'It is worth dwelling on where this lack of strength comes from. If the input
    ${\bm{h}}^{l-1}$ is one dimensional, the big-$M$ formulation is *locally* ideal
    (Vielma, [2015](#bib.bib316)): the extreme points of the linear programming relaxation
    ([4a](#S4.E4.1 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")-[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) naturally
    satisfy the integrality constraints ([4e](#S4.E4.5 "In 4 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). However,
    this fails to hold in the general multivariate input case. To see why, observe
    that the bounds on the input variables ${\bm{h}}^{l-1}$ are only coupled with
    the logic involving the binary variable $z$ only in an aggregated sense, through
    the coefficients $M^{l}_{i,-}$ and $M^{l}_{i,+}$. In other words, the “shape”
    of the pre-activation domain is not incorporated directly into the big-$M$ formulation.
    Furthermore, the strength of this formulation highly depends on the big-$M$ coefficients.
    These coefficients can be obtained using techniques ranging from basic interval
    arithmetic to optimization-based bounds tightening. Grimstad and Andersson ([2019](#bib.bib133))
    show how constraints external to the neural network can yield tighter bounds via
    optimization- or feasibility-based bounds tightening. Rössig and Petkovic ([2021](#bib.bib262))
    compare several methods for deriving bounds and further develop optimization-based
    bounds tightening based on pairwise dependencies between variables.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '值得深入探讨的是这种力量不足的来源。如果输入 ${\bm{h}}^{l-1}$ 是一维的，大-$M$ 公式在*局部*上是理想的（Vielma, [2015](#bib.bib316)）：线性规划松弛的极端点
    ([4a](#S4.E4.1 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")-[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) 自然地满足整数约束
    ([4e](#S4.E4.5 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"))。然而，在一般的多变量输入情况下，这种情况并不成立。要理解原因，可以观察到，输入变量
    ${\bm{h}}^{l-1}$ 的界限只是通过系数 $M^{l}_{i,-}$ 和 $M^{l}_{i,+}$ 在整体意义上与涉及二进制变量 $z$ 的逻辑相关联。换句话说，预激活域的“形状”没有直接纳入大-$M$
    公式中。此外，这种公式的强度高度依赖于大-$M$ 系数。这些系数可以通过从基本区间算术到基于优化的界限收紧等技术获得。Grimstad 和 Andersson
    ([2019](#bib.bib133)) 说明了神经网络外部的约束如何通过优化或可行性基础的界限收紧获得更紧的界限。Rössig 和 Petkovic ([2021](#bib.bib262))
    比较了几种界限推导方法，并进一步开发了基于变量间对偶依赖的优化基础界限收紧。'
- en: <svg   height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg><svg
    height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg>
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: <svg height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg><svg
    height="135.23" overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg>
- en: 'Figure 9: Left: The convex hull of a ReLU neuron ([5](#S4.E5 "In 4.2.2 A stronger
    extended formulation ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")), and Right: the convex relaxation offered by the big-$M$ formulation
    ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) Adapted from Anderson et al. Anderson et al.
    ([2020](#bib.bib5), [2019](#bib.bib4))'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：左侧：ReLU 神经元的凸包（[5](#S4.E5 "在 4.2.2 更强的扩展形式 ‣ 4.2 精确模型使用混合整数规划 ‣ 4 优化训练好的神经网络
    ‣ 当深度学习遇上多面体理论：综述")），右侧：大-$M$ 形式提供的凸松弛（[4](#S4.E4 "在 4.2.1 大-𝑀 形式 ‣ 4.2 精确模型使用混合整数规划
    ‣ 4 优化训练好的神经网络 ‣ 当深度学习遇上多面体理论：综述")）。改编自 Anderson 等人 ([2020](#bib.bib5)，[2019](#bib.bib4))。
- en: 4.2.2 A stronger extended formulation
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 更强的扩展形式
- en: 'A much stronger MILP formulation can be constructed through a classical method,
    the extended formulation for disjunctions (Balas, [1998](#bib.bib11), Jeroslow
    and Lowe, [1984](#bib.bib163)). This formulation for a given ReLU neuron takes
    the following form (Anderson et al., [2019](#bib.bib4), Section 2.2):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过经典方法可以构造出更强的 MILP 形式，即用于分离的扩展形式（Balas，[1998](#bib.bib11)，Jeroslow 和 Lowe，[1984](#bib.bib163)）。对于给定的
    ReLU 神经元，这种形式如下（Anderson 等人，[2019](#bib.bib4)，第 2.2 节）：
- en: '|  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle=(x^{+},y^{+})+(x^{-},y^{-})$
    |  | (5a) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle=(x^{+},y^{+})+(x^{-},y^{-})$
    |  | (5a) |'
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq{\bm{w}}^{l}_{i}x^{-}+b^{l}_{i}(1-z)$
    |  | (5b) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq{\bm{w}}^{l}_{i}x^{-}+b^{l}_{i}(1-z)$
    |  | (5b) |'
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle={\bm{w}}^{l}_{i}x^{+}+b^{l}_{i}z\geq
    0$ |  | (5c) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{+}$ | $\displaystyle={\bm{w}}^{l}_{i}x^{+}+b^{l}_{i}z\geq
    0$ |  | (5c) |'
- en: '|  | $\displaystyle L^{l-1}(1-z)$ | $\displaystyle\leq x^{-}\leq U^{l-1}(1-z)$
    |  | (5d) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L^{l-1}(1-z)$ | $\displaystyle\leq x^{-}\leq U^{l-1}(1-z)$
    |  | (5d) |'
- en: '|  | $\displaystyle L^{l-1}z$ | $\displaystyle\leq x^{+}\leq U^{l-1}z$ |  |
    (5e) |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L^{l-1}z$ | $\displaystyle\leq x^{+}\leq U^{l-1}z$ |  |
    (5e) |'
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (5f) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (5f) |'
- en: This formulation requires one binary variable and $\mathcal{O}(n_{l-1})$ general
    linear constraints and auxiliary continuous variables. It is also locally ideal,
    i.e., as strong as possible. While the number of variables and constraints for
    an individual unit seems quite tame, applying this formulation for unit in a network
    leads to a formulation with $\mathcal{O}(n_{0}+\sum_{l\in{\mathbb{L}}}n_{l}n_{l-1})=\mathcal{O}(|{\mathbb{L}}|n_{\max}^{2})$
    continuous variables and linear constraints. Moreover, while the formulation for
    *an individual unit* is locally ideal, the composition of many locally ideal formulations
    will, in general, fail to be ideal itself. Consider that, while each node can
    be modeled as a two-part disjunction, the full network requires exponentially
    many disjuncts, each corresponding to one activation pattern.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式需要一个二进制变量和 $\mathcal{O}(n_{l-1})$ 个一般线性约束以及辅助连续变量。它在局部上也是理想的，即尽可能强。虽然单个单位的变量和约束数量似乎相当温和，但在网络中应用此公式会导致一个具有
    $\mathcal{O}(n_{0}+\sum_{l\in{\mathbb{L}}}n_{l}n_{l-1})=\mathcal{O}(|{\mathbb{L}}|n_{\max}^{2})$
    个连续变量和线性约束的公式。此外，虽然*单个单位*的公式在局部上是理想的，但许多局部理想公式的组合通常会导致整体上不再理想。考虑到虽然每个节点可以建模为两个部分的析取，但整个网络需要指数级的析取，每一个对应于一种激活模式。
- en: 'Despite its strength and relatively modest increase in size relative to the
    big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models
    using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")), it has been empirically
    observed that this formulation often performs worse than expected (Anderson et al.,
    [2019](#bib.bib4), Vielma, [2019](#bib.bib317)), both in the verification setting
    and more broadly.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管相对于大-$M$ 公式而言，它的强度和相对较小的规模增加（[4](#S4.E4 "在 4.2.1 大-𝑀 公式 ‣ 4.2 精确模型使用混合整数规划
    ‣ 4 优化训练神经网络 ‣ 当深度学习遇到多面体理论：综述")）是相对温和的，但通过实证观察发现，这种公式在验证设置和更广泛的应用中通常表现得比预期差（Anderson
    等，[2019](#bib.bib4)，Vielma，[2019](#bib.bib317)）。
- en: 4.2.3 A class of intermediate formulations
  id: totrans-265
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 一类中间公式
- en: 'The previous sections observed that the big-$M$ formulation ([4](#S4.E4 "In
    4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) is compact, but may offer a weak convex relaxation, while
    the extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) offers
    the tightest possible convex relaxation for an individual unit, at the expense
    of a much larger formulation. Kronqvist et al. ([2022](#bib.bib180), [2021](#bib.bib179))
    present a strategy for obtaining formulations intermediate to ([4](#S4.E4 "In
    4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) and ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) in terms
    of both size and strength. The key idea is to partition ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$
    into a number of aggregated variables, ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}=\sum_{p=1}^{P}\hat{x}_{p}$.
    Each auxiliary variable $\hat{x}_{p}$ is defined as a sum of a subset of the $j$-th
    weighted inputs $\hat{x}_{p}=\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$,
    with $\mathbb{S}_{1},...,\mathbb{S}_{P}$ partitioning $\{1,...,n_{l-1}\}$. This
    technique can be applied to the ReLU function, giving the convex hull over the
    directions defined by $\hat{x}_{p}$ (Tsay et al., [2021](#bib.bib312)):'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '之前的章节观察到，大-$M$ 公式 ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models
    using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")) 是紧凑的，但可能提供了较弱的凸松弛，而扩展公式
    ([5](#S4.E5 "In 4.2.2 A stronger extended formulation ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey")) 为单个单元提供了可能最紧的凸松弛，但代价是公式规模大得多。Kronqvist
    等 ([2022](#bib.bib180), [2021](#bib.bib179)) 提出了在大小和强度上介于 ([4](#S4.E4 "In 4.2.1
    The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")) 和 ([5](#S4.E5 "In 4.2.2 A stronger extended formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) 之间的中间公式的策略。关键思想是将 ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$
    划分为若干个聚合变量，${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}=\sum_{p=1}^{P}\hat{x}_{p}$。每个辅助变量 $\hat{x}_{p}$
    定义为第 $j$ 个加权输入的子集的总和 $\hat{x}_{p}=\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$，其中
    $\mathbb{S}_{1},...,\mathbb{S}_{P}$ 划分了 $\{1,...,n_{l-1}\}$。这一技术可以应用于 ReLU 函数，从而给出由
    $\hat{x}_{p}$ 定义的方向上的凸包（Tsay 等，[2021](#bib.bib312)）：'
- en: '|  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\left(\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1},h^{l}_{i}\right)$
    | $\displaystyle=(\hat{x}_{p}^{+},y^{+})+(\hat{x}_{p}^{-},y^{-})$ |  | (6a) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1},h^{l}_{i}\right)$
    | $\displaystyle=(\hat{x}_{p}^{+},y^{+})+(\hat{x}_{p}^{-},y^{-})$ |  | (6a) |'
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq\sum_{p}\hat{x}_{p}^{-}+b^{l}_{i}(1-z)$
    |  | (6b) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq\sum_{p}\hat{x}_{p}^{-}+b^{l}_{i}(1-z)$
    |  | (6b) |'
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle=\sum_{p}\hat{x}_{p}^{+}+b^{l}_{i}z\geq
    0$ |  | (6c) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{+}$ | $\displaystyle=\sum_{p}\hat{x}_{p}^{+}+b^{l}_{i}z\geq
    0$ |  | (6c) |'
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}(1-z)$ | $\displaystyle\leq\hat{x}^{-}\leq\hat{\bm{M}}_{i,+}^{l}(1-z)$
    |  | (6d) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}(1-z)$ | $\displaystyle\leq\hat{x}^{-}\leq\hat{\bm{M}}_{i,+}^{l}(1-z)$
    |  | (6d) |'
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}z$ | $\displaystyle\leq\hat{x}^{+}\leq\hat{\bm{M}}_{i,+}^{l}z$
    |  | (6e) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}z$ | $\displaystyle\leq\hat{x}^{+}\leq\hat{\bm{M}}_{i,+}^{l}z$
    |  | (6e) |'
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (6f) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (6f) |'
- en: Here, the $p$-th elements of $\hat{\bm{M}}_{i,-}^{l}$ and $\hat{\bm{M}}_{i,+}^{l}$
    must satisfy the inequalities
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\hat{\bm{M}}_{i,-}^{l}$ 和 $\hat{\bm{M}}_{i,+}^{l}$ 的第 $p$ 个元素必须满足不等式。
- en: '|  | $\displaystyle\hat{M}^{l}_{i,-,p}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$
    |  |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{M}^{l}_{i,-,p}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$
    |  |'
- en: '|  | $\displaystyle\hat{M}^{l}_{i,+,p}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}.$
    |  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{M}^{l}_{i,+,p}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}.$
    |  |'
- en: 'These coefficients can be derived using techniques analagous to those for the
    big-$M$ formulation (note that tighter bounds may be derived by considering $\hat{x}^{-}$
    and $\hat{x}^{+}$ separately). Observe that when $P=1$, we recover the same tightness
    as the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), as, intuitively, the
    formulation is built over a single “direction” corresponding to ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$.
    Conversely, when $P=n_{l-1}$, we recover the tightness of the extended formulation
    ([5](#S4.E5 "In 4.2.2 A stronger extended formulation ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey")), as each direction corresponds
    to a single element of ${\bm{h}}^{l-1}$. Tsay et al. ([2021](#bib.bib312)) study
    partitioning strategies and show that intermediate values of $P$ result in formulations
    that can outperform the two extremes, by balancing formulation size and strength.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系数可以通过类似于大-$M$ 公式的技术导出（请注意，通过分别考虑 $\hat{x}^{-}$ 和 $\hat{x}^{+}$，可能会得到更紧的界限）。观察到当
    $P=1$ 时，我们恢复了与大-$M$ 公式相同的紧度（[4](#S4.E4 "在4.2.1 大-𝑀 公式 ‣ 4.2 使用混合整数编程的精确模型 ‣ 4
    优化训练好的神经网络 ‣ 当深度学习遇到多面体理论：综述")），因为从直观上讲，公式是建立在单一的“方向”上，对应于 ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$。相反，当
    $P=n_{l-1}$ 时，我们恢复了扩展公式的紧度（[5](#S4.E5 "在4.2.2 更强的扩展公式 ‣ 4.2 使用混合整数编程的精确模型 ‣ 4
    优化训练好的神经网络 ‣ 当深度学习遇到多面体理论：综述")），因为每个方向对应于 ${\bm{h}}^{l-1}$ 的单个元素。Tsay 等人（[2021](#bib.bib312)）研究了分割策略，并表明
    $P$ 的中间值可以产生比两极端情况更具优势的公式，通过平衡公式的大小和强度。
- en: '4.2.4 Cutting plane methods: Trading variables for inequalities'
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 割平面方法：用不等式替换变量
- en: 'The extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) achieves
    its strength through the introduction of auxiliary continuous variables. However,
    it is possible to produce a formulation of equal strength by projecting out these
    auxiliary variables, leaving an ideal formulation in the “original” $({\bm{h}}^{l-1},h^{l}_{i},z)$
    variable space. While in general this projection may be difficult computationally,
    for the simple structure of a single ReLU neuron it is possible to characterize
    in closed form. The formulation is given by Anderson et al. ([2020](#bib.bib5),
    [2019](#bib.bib4)) as'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展公式（[5](#S4.E5 "在4.2.2 更强的扩展公式 ‣ 4.2 使用混合整数编程的精确模型 ‣ 4 优化训练好的神经网络 ‣ 当深度学习遇到多面体理论：综述")）通过引入辅助连续变量来实现其强度。然而，通过将这些辅助变量投影出去，可以得到一个等强度的公式，保留“原始”
    $({\bm{h}}^{l-1},h^{l}_{i},z)$ 变量空间中的理想公式。尽管通常这种投影在计算上可能困难，但对于单一 ReLU 神经元的简单结构，可以以闭式形式进行表征。该公式由
    Anderson 等人（[2020](#bib.bib5), [2019](#bib.bib4)）给出。
- en: '|  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (7a) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (7a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{j\in J}w^{l}_{i,j}(h^{l-1}_{i}-\breve{L}^{l}_{j}(1-z))+\left(b+\sum_{j\not\in
    J}w^{l}_{i,j}\breve{U}_{j}\right)z\quad\forall J\subseteq\llbracket n_{l-1}\rrbracket$
    |  | (7b) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{j\in J}w^{l}_{i,j}(h^{l-1}_{i}-\breve{L}^{l}_{j}(1-z))+\left(b+\sum_{j\not\in
    J}w^{l}_{i,j}\breve{U}_{j}\right)z\quad\forall J\subseteq\llbracket n_{l-1}\rrbracket$
    |  | (7b) |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}_{\geq
    0}$ |  | (7c) |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}_{\geq
    0}$ |  | (7c) |'
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\},$ |  | (7d) |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\},$ |  | (7d) |'
- en: where notationally, for each $j\in\llbracket n_{l-1}\rrbracket$, we take
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在符号表示上，对于每个 $j\in\llbracket n_{l-1}\rrbracket$，我们取
- en: '|  | <math   alttext="\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\'
- en: L^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}" display="block"><semantics ><mrow
    ><mrow ><msubsup  ><mover accent="true"  ><mi >L</mi><mo >˘</mo></mover><mi >j</mi><mrow
    ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow  ><mrow ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" ><mtr  ><mtd columnalign="left"  ><msubsup
    ><mi >L</mi><mi >j</mi><mrow ><mi  >l</mi><mo >−</mo><mn >1</mn></mrow></msubsup></mtd><mtd
    columnalign="left"  ><mrow ><msubsup ><mi  >w</mi><mrow ><mi >i</mi><mo >,</mo><mi
    >j</mi></mrow><mi >l</mi></msubsup><mo >≥</mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><msubsup  ><mi >U</mi><mi >j</mi><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msubsup></mtd><mtd columnalign="left"  ><mrow ><msubsup  ><mi
    >w</mi><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    ><</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow><mi >and</mi></mrow></mrow><mrow
    ><msubsup ><mover accent="true" ><mi  >U</mi><mo >˘</mo></mover><mi >j</mi><mrow
    ><mi >l</mi><mo >−</mo><mn >1</mn></mrow></msubsup><mo >=</mo><mrow  ><mo >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr ><mtd columnalign="left"  ><msubsup
    ><mi >U</mi><mi >j</mi><mrow ><mi  >l</mi><mo >−</mo><mn >1</mn></mrow></msubsup></mtd><mtd
    columnalign="left"  ><mrow ><msubsup ><mi  >w</mi><mrow ><mi >i</mi><mo >,</mo><mi
    >j</mi></mrow><mi >l</mi></msubsup><mo >≥</mo><mn >0</mn></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><msubsup ><mi  >L</mi><mi >j</mi><mrow ><mi >l</mi><mo
    >−</mo><mn >1</mn></mrow></msubsup></mtd><mtd columnalign="left"  ><mrow ><msubsup
    ><mi  >w</mi><mrow ><mi >i</mi><mo >,</mo><mi >j</mi></mrow><mi >l</mi></msubsup><mo
    ><</mo><mn >0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><apply ><ci >˘</ci><ci >𝐿</ci></apply><apply ><ci  >𝑙</ci><cn
    type="integer"  >1</cn></apply></apply><ci >𝑗</ci></apply><list ><apply ><csymbol
    cd="latexml"  >cases</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐿</ci><apply ><ci >𝑙</ci><cn
    type="integer" >1</cn></apply></apply><ci >𝑗</ci></apply><apply ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑤</ci><ci >𝑙</ci></apply><list ><ci >𝑖</ci><ci >𝑗</ci></list></apply><cn type="integer"  >0</cn></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑈</ci><apply ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply><ci >𝑗</ci></apply><apply
    ><apply  ><csymbol cd="ambiguous"  >subscript</csymbol><apply ><csymbol cd="ambiguous"  >superscript</csymbol><ci
    >𝑤</ci><ci >𝑙</ci></apply><list ><ci >𝑖</ci><ci >𝑗</ci></list></apply><cn type="integer"  >0</cn></apply></apply><ci
    >and</ci></list></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><apply ><ci  >˘</ci><ci >𝑈</ci></apply><apply
    ><ci >𝑙</ci><cn type="integer" >1</cn></apply></apply><ci >𝑗</ci></apply><apply
    ><csymbol cd="latexml" >cases</csymbol><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑈</ci><apply ><ci  >𝑙</ci><cn
    type="integer"  >1</cn></apply></apply><ci >𝑗</ci></apply><apply ><apply ><csymbol
    cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >𝑤</ci><ci >𝑙</ci></apply><list ><ci >𝑖</ci><ci >𝑗</ci></list></apply><cn type="integer"
    >0</cn></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐿</ci><apply ><ci  >𝑙</ci><cn type="integer"  >1</cn></apply></apply><ci
    >𝑗</ci></apply><apply ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply
    ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝑤</ci><ci >𝑙</ci></apply><list
    ><ci >𝑖</ci><ci >𝑗</ci></list></apply><cn type="integer" >0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ L^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}</annotation></semantics></math> |  |
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \(\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&w^{l}_{i,j}\geq 0\\ U^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\quad\mathrm{和}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ L^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\)
- en: 'We note a few points of interest about this formulation. First, it is ideal,
    and so recovers the convex hull of a ReLU activation function, coupled with its
    preactivation affine function and bounds on each of the inputs to that affine
    function. Second, it can be shown that, under very mild conditions, each of the
    exponentially many constraints in ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods:
    Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) are necessary to ensure this property; none are redundant
    and can be removed without affecting the relaxation quality. Third, note that
    by selecting only those constraints in ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane
    methods: Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) corresponding to $J=\emptyset$ and $J=\llbracket
    n_{l-1}\rrbracket$, we recover the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The
    big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")) in the case where $\mathcal{D}^{l-1}=[L^{l-1},U^{l-1}]$. This suggests
    a practical approach for using this large family of inequalities: Start with the
    big-$M$ formulation, and then dynamically generate violated inequalities from
    ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods: Trading variables for inequalities
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) as-needed
    in a cutting plane procedure. As shown by Anderson et al. ([2020](#bib.bib5)),
    this separation problem is separable in the input variables, and hence can be
    completed in $\mathcal{O}(n_{l-1})$ time.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '我们注意到关于这个公式的一些要点。首先，它是理想的，因此恢复了 ReLU 激活函数的凸包，结合其预激活仿射函数以及对该仿射函数每个输入的界限。第二，可以证明，在非常宽松的条件下，([7b](#S4.E7.2
    "In 7 ‣ 4.2.4 Cutting plane methods: Trading variables for inequalities ‣ 4.2
    Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) 中的每个指数级多的约束都是确保这一属性所必需的；没有一个是多余的，且可以在不影响松弛质量的情况下被移除。第三，请注意，通过选择仅对应于
    $J=\emptyset$ 和 $J=\llbracket n_{l-1}\rrbracket$ 的([7b](#S4.E7.2 "In 7 ‣ 4.2.4
    Cutting plane methods: Trading variables for inequalities ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey")) 中的那些约束，我们恢复了大-$M$ 公式 ([4](#S4.E4
    "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"))，当 $\mathcal{D}^{l-1}=[L^{l-1},U^{l-1}]$ 的情况下。这表明了一种实际的方法来使用这一大范围的约束：从大-$M$
    公式开始，然后在切平面程序中根据需要动态生成 ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods: Trading
    variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) 中的违反约束。正如 Anderson 等人 ([2020](#bib.bib5)) 所示，这一分离问题在输入变量中是可分的，因此可以在
    $\mathcal{O}(n_{l-1})$ 时间内完成。'
- en: 'The cutting plane strategy is in general compatible with weaker formulations,
    such as relaxation-based verification (Zhang et al., [2022](#bib.bib356)) and
    formulations from the class ([6](#S4.E6 "In 4.2.3 A class of intermediate formulations
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). In fact,
    Tsay et al. ([2021](#bib.bib312)) show that the intermediate formulations in ([6](#S4.E6
    "In 4.2.3 A class of intermediate formulations ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) effectively pre-select a number of inequalities
    from ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods: Trading variables for
    inequalities ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")), in terms of their continuous relaxations. While adding these constraints
    results in a tighter continuous relaxation, the added constraints can eventually
    significantly increase the model size. Practical implementations may therefore
    only perform cut generation at a limited number of branch-and-bound search nodes
    (De Palma et al., [2021](#bib.bib77), Tsay et al., [2021](#bib.bib312)).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 切平面策略通常与较弱的公式兼容，例如基于松弛的验证（Zhang et al., [2022](#bib.bib356)）和来自类的公式（[6](#S4.E6
    "在 4.2.3 中的中间公式类 ‣ 4.2 使用混合整数规划的精确模型 ‣ 4 优化训练神经网络 ‣ 当深度学习遇上多面体理论：综述")）。事实上，Tsay
    et al. ([2021](#bib.bib312)) 表明，[6](#S4.E6 "在 4.2.3 中的中间公式类 ‣ 4.2 使用混合整数规划的精确模型
    ‣ 4 优化训练神经网络 ‣ 当深度学习遇上多面体理论：综述")中的中间公式有效地从[7b](#S4.E7.2 "在 7 ‣ 4.2.4 切平面方法：用不等式替换变量
    ‣ 4.2 使用混合整数规划的精确模型 ‣ 4 优化训练神经网络 ‣ 当深度学习遇上多面体理论：综述")中选择了一些不等式，就其连续松弛而言。虽然添加这些约束会导致更紧的连续松弛，但添加的约束最终可能会显著增加模型的大小。因此，实际实现可能仅在有限数量的分支定界搜索节点上执行切割生成（De
    Palma et al., [2021](#bib.bib77)，Tsay et al., [2021](#bib.bib312)）。
- en: 'A subtlety when using ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading
    variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"))'
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用([7](#S4.E7 "在 4.2.4 切平面方法：用不等式替换变量 ‣ 4.2 使用混合整数规划的精确模型 ‣ 4 优化训练神经网络 ‣ 当深度学习遇上多面体理论：综述"))时的一个细微问题
- en: This third point above raises a subtlety discussed in the literature (De Palma
    et al., [2021](#bib.bib77), Appendix F). Often, additional structural information
    is known about $\mathcal{D}^{l-1}$ beyond bounds on the variables. In this case,
    it is typically possible to derive tighter values for the big-$M$ coefficients.
    In this case, when using a separation-based approach it is preferable to initialize
    the formulation with these tightened big-$M$ constraints, and then proceed with
    the cutting plane approach as normal from there.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 上述第三点提出了文献中讨论的一个细微问题（De Palma et al., [2021](#bib.bib77)，附录 F）。通常，关于$\mathcal{D}^{l-1}$的附加结构信息会超出变量的界限。在这种情况下，通常可以推导出更紧的
    big-$M$ 系数。在这种情况下，当使用基于分离的方法时，最好用这些收紧的 big-$M$ 约束初始化模型，然后从那里按常规继续进行切平面方法。
- en: '4.3 Scaling further: Convex relaxations and linear programming'
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 进一步扩展：凸松弛和线性规划
- en: 'The above demonstrate MILP as a powerful framework for exactly modeling complex,
    nonconvex trained neural networks, but standard solvers are often not sufficiently
    scalable to adequately handle large-scale networks. A natural approach to increase
    the scalability, then, is to *relax* the network in some manner, and then apply
    convex optimization methods. For the verification problem discussed in Section [4.1.1](#S4.SS1.SSS1
    "4.1.1 Neural network verification ‣ 4.1 Applications of optimization over trained
    networks ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), this yields what is known as an *incomplete verifier*:
    any certification of robustness provided can be trusted (no false positives),
    but there may be robust instances that the method cannot prove are (some false
    negatives). In other words, over-approximation produces a verifier that is sound,
    but not complete.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容证明了MILP作为精确建模复杂的、非凸的训练神经网络的强大框架，但标准求解器往往不足以充分处理大规模网络。因此，增加可扩展性的一个自然方法是以某种方式*松弛*网络，然后应用凸优化方法。对于第4.1.1节中讨论的验证问题，结果是被称为*不完全验证器*：可以信任对鲁棒性的任何认证（没有错误的阳性），但可能有一些该方法无法证明的鲁棒实例（某些错误的阴性）。换句话说，过度逼近会产生一个听起来是，但不完全的验证器。
- en: 'While a variety of methods exist for accomplishing this, in this section we
    briefly outline techniques relevant to polyhedral theory. In particular, we focus
    on some techniques for building convex polyhedral relaxations. The most natural
    convex relaxation for a MILP formulation is its linear programming (LP) relaxation,
    constructed by dropping any integrality constraints. For example, the LP relaxation
    of ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) is given by the system ([4a](#S4.E4.1 "In
    4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")-[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). This is a compact linear
    programming relaxation for a ReLU-based network, and is the basis for methods
    due to Bunel et al. ([2020a](#bib.bib41)) and Ehlers ([2017](#bib.bib87)).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在各种方法来实现这一点，但在本节中，我们简要概述了与多面体理论相关的技术。特别是，我们专注于一些构建凸多面体松弛的技术。MILP公式的最自然的凸松弛是其线性规划（LP）松弛，通过放弃任何整体性约束而构建。例如，([4](#S4.E4
    "在4.2.1大M公式‣4.2使用混合整数规划的精确模型‣4在训练的神经网络上优化‣当深度学习遇上多面体理论：一项调查")) 的LP松弛由系统([4a](#S4.E4.1
    "在4 ‣4.2.1大M公式‣4.2使用混合整数规划的精确模型‣4在训练的神经网络上优化‣当深度学习遇上多面体理论：一项调查")-[4d](#S4.E4.4
    "在4 ‣4.2.1大M公式‣4.2使用混合整数规划的精确模型‣4在训练的神经网络上优化‣当深度学习遇上多面体理论：一项调查")) 给出。这是基于ReLU网络的紧凑线性规划松弛，也是Bunel等人([2020a](#bib.bib41))和Ehlers([2017](#bib.bib87))方法的基础。
- en: 4.3.1 Projecting the big-$M$ and ideal MILP formulations
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 投影大-$M$和理想MILP公式
- en: 'This section examines projections of the linear relaxations of formulations
    ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) and ([7](#S4.E7 "In 4.2.4 Cutting plane methods:
    Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 本节考察了公式([4](#S4.E4 "在4.2.1大M公式‣4.2使用混合整数规划的精确模型‣4在训练的神经网络上优化‣当深度学习遇上多面体理论：一项调查"))和([7](#S4.E7
    "在4.2.4割平面方法：以不等式交换变量‣4.2使用混合整数规划的精确模型‣4在训练的神经网络上优化‣当深度学习遇上多面体理论：一项调查")) 的线性松弛的投影。
- en: '(Projecting the big-$M$). Note that the LP relaxation given by ([4a](#S4.E4.1
    "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")–[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) maintains the variables
    $z^{l}_{i}$ in the formulation, though they are no longer required to satisfy
    integrality. Since these variables are “auxiliary” and are no longer necessary
    to encode the nonconvexity of the problem, they can be projected out without altering
    the quality of the convex relaxation. Doing this yields what is commonly known
    as the “triangle” or “$\Delta$” relaxation (Salman et al., [2019](#bib.bib269)):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: （投影大-$M$）。请注意，给定的LP松弛（[4a](#S4.E4.1 "在4节 ‣ 4.2.1节 大-𝑀公式 ‣ 4.2 使用混合整数规划的精确模型
    ‣ 4 在训练好的神经网络上优化 ‣ 当深度学习遇到多面体理论：一项综述")–[4d](#S4.E4.4 "在4节 ‣ 4.2.1节 大-𝑀公式 ‣ 4.2
    使用混合整数规划的精确模型 ‣ 4 在训练好的神经网络上优化 ‣ 当深度学习遇到多面体理论：一项综述")）保持了公式中的变量$z^{l}_{i}$，尽管这些变量不再需要满足整数性。由于这些变量是“辅助”的，不再必要编码问题的非凸性，它们可以在不改变凸松弛质量的情况下被投影出去。这会产生通常所称的“三角形”或“$\Delta$”松弛（Salman等人，[2019](#bib.bib269)）。
- en: '|  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (8a) |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (8a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\frac{M^{l}_{i,+}}{M^{l}_{i,+}-M^{l}_{i,-}}({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})$
    |  | (8b) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\frac{M^{l}_{i,+}}{M^{l}_{i,+}-M^{l}_{i,-}}({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})$
    |  | (8b) |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}.$ |  | (8c) |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}.$ |  | (8c) |'
- en: 'While the LP relaxation ([8](#S4.E8 "In 4.3.1 Projecting the big-𝑀 and ideal
    MILP formulations ‣ 4.3 Scaling further: Convex relaxations and linear programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) for an individual unit is compact, modern neural network architectures
    regularly comprise millions of units. The resulting LP relaxation for the entire
    network may then require millions of variables and constraints. Additionally,
    unless special precautions are taken, many of these constraints will be relatively
    dense. All this quickly leads to LP that are beyond the scope of modern off-the-shelf
    LP solvers. As a result, researchers have explored alternative schemes for scaling
    LP-based methods to these larger networks. Salman et al. ([2019](#bib.bib269))
    present a framework for LP-based methods (LP solvers, propagation, dual methods),
    which we review in the following subsections. However, they do not account for
    the ideal formulation developed in later works (Anderson et al., [2020](#bib.bib5),
    De Palma et al., [2021](#bib.bib77)).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然单个单元的LP松弛（[8](#S4.E8 "在4.3.1节中投影大-𝑀和理想MILP公式 ‣ 4.3进一步缩放：凸松弛和线性规划 ‣ 4 在训练好的神经网络上优化
    ‣ 当深度学习遇到多面体理论：一项综述")）是紧凑的，但现代神经网络架构通常包含数百万个单元。因此，整个网络的LP松弛可能需要数百万个变量和约束。此外，除非采取特殊预防措施，否则这些约束中的许多将是相对密集的。所有这些很快导致LP超出现代现成LP求解器的范围。因此，研究人员探索了将基于LP的方法扩展到这些更大网络的替代方案。Salman等人（[2019](#bib.bib269)）提出了一种LP基础方法（LP求解器、传播、对偶方法）的框架，我们将在以下小节中回顾。然而，他们没有考虑在后来的工作中开发的理想公式（Anderson等人，[2020](#bib.bib5)，De
    Palma等人，[2021](#bib.bib77)）。
- en: '(Projecting the ideal). Figure [9](#S4.F9 "Figure 9 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") shows
    that the triangle (big-$M$) relaxation fails to recover the convex hull of the
    ReLU activation function and the multivariate input to the affine pre-activation
    function. We can similarly project the LP relaxation of the ideal formulation
    ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables for inequalities
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) into
    the space of input/output variables (Anderson et al., [2020](#bib.bib5)), yielding
    a description for the convex hull of $\{({\bm{h}}^{l-1},h^{l}_{i})|L^{l-1}\leq{\bm{h}}^{l-1}\leq
    U^{l-1},\>h^{l}_{i}=\sigma({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})\}$:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: (投影理想的). 图 [9](#S4.F9 "图 9 ‣ 4.2.1 大-𝑀 公式 ‣ 4.2 使用混合整数规划的精确模型 ‣ 在训练的神经网络上优化
    ‣ 当深度学习遇上多面体理论：综述") 显示了三角形 (大-$M$) 松弛未能恢复 ReLU 激活函数和仿射前激活函数的多变量输入的凸包。我们可以类似地将理想公式的
    LP 松弛 ([7](#S4.E7 "在 4.2.4 切割平面方法：用不等式交换变量 ‣ 4.2 使用混合整数规划的精确模型 ‣ 在训练的神经网络上优化 ‣
    当深度学习遇上多面体理论：综述")) 投影到输入/输出变量的空间 (Anderson 等人，[2020](#bib.bib5))，从而为 $\{({\bm{h}}^{l-1},h^{l}_{i})|L^{l-1}\leq{\bm{h}}^{l-1}\leq
    U^{l-1},\>h^{l}_{i}=\sigma({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})\}$ 的凸包提供描述：
- en: '|  |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (9a) |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (9a) |'
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{k\in I}w_{i,k}^{l}(x_{k}-\breve{L}_{k})+\frac{\ell(I)}{\breve{U}_{h}-\breve{L}_{h}}(x_{h}-\breve{L}_{h})\quad\forall(I,h)\in\mathcal{J}$
    |  | (9b) |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{k\in I}w_{i,k}^{l}(x_{k}-\breve{L}_{k})+\frac{\ell(I)}{\breve{U}_{h}-\breve{L}_{h}}(x_{h}-\breve{L}_{h})\quad\forall(I,h)\in\mathcal{J}$
    |  | (9b) |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0},$ |  | (9c) |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0},$ |  | (9c) |'
- en: where $l(I)\coloneqq\sum_{k\in I}w^{l}_{i,k}\breve{L}_{k}+\sum_{k\not\in I}w^{l}_{i,k}\breve{U}_{k}+b^{l}_{i}$
    and
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l(I)\coloneqq\sum_{k\in I}w^{l}_{i,k}\breve{L}_{k}+\sum_{k\not\in I}w^{l}_{i,k}\breve{U}_{k}+b^{l}_{i}$
    和
- en: '|  | $\mathcal{J}\coloneqq\Set{(I,h)\in 2^{\llbracket n_{l-1}\rrbracket}\times\llbracket
    n_{l-1}\rrbracket}{l(I)\geq 0,\>l(I\cup\{h\}<0,\>w^{l}_{i,k}\neq 0\forall k\in
    I}.$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{J}\coloneqq\Set{(I,h)\in 2^{\llbracket n_{l-1}\rrbracket}\times\llbracket
    n_{l-1}\rrbracket}{l(I)\geq 0,\>l(I\cup\{h\}<0,\>w^{l}_{i,k}\neq 0\forall k\in
    I}.$ |  |'
- en: 'Anderson et al. ([2020](#bib.bib5)) also show that the inequalities ([9b](#S4.E9.2
    "In 9 ‣ 4.3.1 Projecting the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling further:
    Convex relaxations and linear programming ‣ 4 Optimizing Over a Trained Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) can be separated
    over in $\mathcal{O}(n_{l-1})$ time. Interestingly, in contrast to ([7](#S4.E7
    "In 4.2.4 Cutting plane methods: Trading variables for inequalities ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), the number of facet-defining
    inequalities depends heavily on the affine function. While in the worst case the
    number of inequalities will grow exponentially in the input dimension, there exist
    instances where the convex hull can be fully described with only $\mathcal{O}(n_{l-1})$
    inequalities.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson 等人 ([2020](#bib.bib5)) 还显示这些不等式 ([9b](#S4.E9.2 "在 9 ‣ 4.3.1 投影大-𝑀 和理想的
    MILP 公式 ‣ 4.3 进一步缩放：凸松弛和线性规划 ‣ 在训练的神经网络上优化 ‣ 当深度学习遇上多面体理论：综述")) 可以在 $\mathcal{O}(n_{l-1})$
    时间内分离。有趣的是，与 ([7](#S4.E7 "在 4.2.4 切割平面方法：用不等式交换变量 ‣ 4.2 使用混合整数规划的精确模型 ‣ 在训练的神经网络上优化
    ‣ 当深度学习遇上多面体理论：综述")) 相比，不等式的数量在很大程度上取决于仿射函数。虽然在最坏的情况下，不等式的数量会随着输入维度呈指数增长，但也存在这样的实例，其中凸包可以用仅
    $\mathcal{O}(n_{l-1})$ 个不等式完全描述。
- en: 4.3.2 Dual decomposition methods
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 对偶分解方法
- en: 'A first approach for greater scalability for LP-based methods is decomposition,
    a standard technique in the large-scale optimization community. Indeed, the cutting
    plane approach of Section [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting plane methods: Trading
    variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") can be viewed as a decomposition method operating in the original
    variable space. However, the method is initialized with the big-$M$ formulation
    for each neuron, and hence this initial model will be of size roughly equal to
    the size of the network. Therefore, it should be understood to use decomposition
    to provide a tighter verification bound, rather than for providing greater scalability
    to larger networks.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '对于基于线性规划（LP）的方法，提升可扩展性的第一种方法是分解，这是大规模优化领域的标准技术。实际上，[4.2.4节](#S4.SS2.SSS4 "4.2.4
    Cutting plane methods: Trading variables for inequalities ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey")中的切平面方法可以视为在原始变量空间中操作的分解方法。然而，该方法以每个神经元的“大$M$”公式初始化，因此此初始模型的大小大致等于网络的大小。因此，应理解为使用分解来提供更紧的验证界限，而不是为了提供更大的网络可扩展性。'
- en: In contrast, dual decomposition can be used to scale inexact verification methods
    to larger networks. Such methods maintain dual feasible solutions throughout the
    algorithm, meaning that upon termination they yield valid dual bounds on the verification
    instance, and hence serve as incomplete verifiers.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，双重分解可以用于将不精确的验证方法扩展到更大的网络。这些方法在整个算法过程中保持双重可行解，这意味着在终止时，它们会对验证实例产生有效的双重界限，从而作为不完全验证器。
- en: 'Wong and Kolter ([2018](#bib.bib335)), Wong et al. ([2018](#bib.bib336)) use
    as their starting point the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting
    the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling further: Convex relaxations
    and linear programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) for each neuron, and then take the
    standard LP dual of the (relaxed) verification problem. Alternatively, Dvijotham
    et al. ([2018b](#bib.bib85)) propose a Lagrangian-based approach for decomposing
    the original nonlinear formulation of the problem ([3](#S4.E3 "In Example 4 ‣
    4.1.1 Neural network verification ‣ 4.1 Applications of optimization over trained
    networks ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")). Crucially, since the complicating constraints
    coupling the layers in the network are imposed as objective penalties instead
    of “hard” constraints, the optimization problem (given fixed dual variables) decomposes
    along each layer and the subproblems induced by the separability can be solved
    in closed form. This approach dualizes separately the equations characterizing
    the pre-activation and post-activation functions:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 'Wong 和 Kolter ([2018](#bib.bib335))、Wong 等 ([2018](#bib.bib336)) 以每个神经元的三角松弛
    ([8](#S4.E8 "In 4.3.1 Projecting the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling
    further: Convex relaxations and linear programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) 作为起点，然后取标准的
    LP 对偶（松弛）验证问题。或者，Dvijotham 等 ([2018b](#bib.bib85)) 提出了基于拉格朗日方法的分解原始非线性问题公式的方法
    ([3](#S4.E3 "In Example 4 ‣ 4.1.1 Neural network verification ‣ 4.1 Applications
    of optimization over trained networks ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"))。至关重要的是，由于耦合网络层的复杂约束被作为目标惩罚而非“硬”约束施加，因此优化问题（在固定的对偶变量下）沿每层分解，分离性引起的子问题可以用闭式解法解决。这种方法将描述前激活和后激活函数的方程分别对偶化。'
- en: '|  | $\displaystyle\max_{\mu,\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\mu_{k}^{T}(\hat{{\bm{h}}}^{k}-{\bm{W}}^{k}{\bm{h}}^{k-1}-{\bm{b}}^{k})+\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\mu,\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\mu_{k}^{T}(\hat{{\bm{h}}}^{k}-{\bm{W}}^{k}{\bm{h}}^{k-1}-{\bm{b}}^{k})+\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
- en: '|  |  | $\displaystyle\sigma(L^{k})\leq{\bm{h}}^{k}\leq\sigma(U^{k})\quad\forall
    k\in\llbracket n-1\rrbracket.$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sigma(L^{k})\leq{\bm{h}}^{k}\leq\sigma(U^{k})\quad\forall
    k\in\llbracket n-1\rrbracket.$ |  |'
- en: Here, the $\hat{{\bm{h}}}$ variables track the pre-activation values for the
    neurons in the network. The dual variables $\mu_{k}^{T}$ correspond to the equality
    constraints defining the pre-activation values, $\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+{\bm{b}}^{k}$.
    Likewise, the dual variables $\lambda_{k}^{T}$ correspond to enforcing the ReLU
    activation function, ${\bm{h}}^{k}=\sigma(\hat{{\bm{h}}}^{k})=\mathrm{max}(0,\hat{{\bm{h}}}^{k})$.
    Any feasible solution for the neural network is feasible for this dualized problem,
    making the multiplier terms for $\mu_{k}^{T}$ and $\lambda_{k}^{T}$ zero. Thus,
    the inner problem gives a lower bound for the original problem—a property known
    as *weak duality*. The outer (dual) problem optimizing over the Lagrangian multipliers
    then seeks to maximize this lower bound, i.e., to give the tightest possible lower
    bound. This can be solved using a subgradient-based method, or learned along with
    the model parameters in a “predictor-verifier” approach (Dvijotham et al., [2018a](#bib.bib84)).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\hat{{\bm{h}}}$ 变量跟踪网络中神经元的预激活值。对偶变量 $\mu_{k}^{T}$ 对应于定义预激活值的等式约束，即 $\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+{\bm{b}}^{k}$。同样，对偶变量
    $\lambda_{k}^{T}$ 对应于强制实施 ReLU 激活函数，即 ${\bm{h}}^{k}=\sigma(\hat{{\bm{h}}}^{k})=\mathrm{max}(0,\hat{{\bm{h}}}^{k})$。神经网络的任何可行解都是这个对偶化问题的可行解，使得
    $\mu_{k}^{T}$ 和 $\lambda_{k}^{T}$ 的乘子项为零。因此，内层问题为原始问题提供了一个下界，这一特性称为*弱对偶性*。外层（对偶）问题通过优化拉格朗日乘子来最大化这个下界，即提供最紧的下界。这可以通过基于次梯度的方法解决，或者与模型参数一起以“预测-验证者”方法学习（Dvijotham
    等，[2018a](#bib.bib84)）。
- en: 'On the other hand, this approach can be combined with other relaxation-based
    methods. The Lagrangian decomposition can be applied to dualize only the coupling
    constraints between layers, and a convex relaxation used for the activation function
    (Bunel et al., [2020a](#bib.bib41)):'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，这种方法可以与其他基于松弛的方法结合。拉格朗日分解可以仅对层之间的耦合约束进行对偶化，并对激活函数使用凸松弛（Bunel 等，[2020a](#bib.bib41)）：
- en: '|  | $\displaystyle\max_{\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+b^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+b^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
- en: '|  |  | $\displaystyle{\bm{h}}^{k}\geq 0\quad\forall k\in\llbracket n-1\rrbracket$
    |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}^{k}\geq 0\quad\forall k\in\llbracket n-1\rrbracket$
    |  |'
- en: '|  |  | $\displaystyle{\bm{h}}_{i}^{k}\geq\hat{{\bm{h}}}_{i}^{k}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket$ |  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}_{i}^{k}\geq\hat{{\bm{h}}}_{i}^{k}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket$ |  |'
- en: '|  |  | $\displaystyle{\bm{h}}^{k}_{i}\leq\frac{U^{k}_{i}(\hat{{\bm{h}}}^{k}_{i}-L^{k}_{i})}{U^{k}_{i}-L^{k}_{i}}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket.$ |  |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}^{k}_{i}\leq\frac{U^{k}_{i}(\hat{{\bm{h}}}^{k}_{i}-L^{k}_{i})}{U^{k}_{i}-L^{k}_{i}}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket.$ |  |'
- en: 'Note that the final three constraints apply the big-$M$/triangle relaxation
    ([8](#S4.E8 "In 4.3.1 Projecting the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling
    further: Convex relaxations and linear programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to each
    ReLU activation function. The dual problem can then be solved via subgradient-based
    methods, proximal algorithms, or, more recently, a projected gradient descent
    method applied to a nonconvex reformulation of the problem (Bunel et al., [2020c](#bib.bib44)).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最后三个约束对每个 ReLU 激活函数应用了大-$M$/三角形松弛（[8](#S4.E8 "在 4.3.1 中投影大-𝑀 和理想 MILP 公式
    ‣ 4.3 进一步缩放：凸松弛和线性规划 ‣ 4 优化训练神经网络 ‣ 当深度学习遇到多面体理论：综述")）。对偶问题随后可以通过次梯度方法、近端算法，或者更近期的，通过应用于问题的非凸重整的投影梯度下降方法来解决（Bunel
    等，[2020c](#bib.bib44)）。
- en: 'More recently, De Palma et al. ([2021](#bib.bib77)) presented a dual decomposition
    approach based on ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables
    for inequalities ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")). However, creating a dual formulation from the exponential number of
    constraints produces an exponential number of dual variables. The authors therefore
    propose to maintain an “active set” of dual variables to keep the problem sparse.
    A selection algorithm (e.g., selecting entries that maximize an estimated super-gradient)
    can then be used to append the active set. Similar to the above discussion on
    cut generation, the frequency of appending the active set should be chosen strategically.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，De Palma等([2021](#bib.bib77))提出了一种基于([7](#S4.E7 "In 4.2.4 Cutting plane
    methods: Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"))的对偶分解方法。然而，从指数数量的约束中创建对偶形式会产生指数数量的对偶变量。因此，作者建议维持一个“活跃集”的对偶变量以保持问题的稀疏性。然后可以使用选择算法（例如，选择最大化估计超梯度的条目）来扩展活跃集。类似于上述关于剪切生成的讨论，扩展活跃集的频率应当策略性地选择。'
- en: 4.3.3 Fourier-Motzkin elimination and propagation algorithms
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 傅里叶-莫兹金消元法和传播算法
- en: 'Alternatively, one can project out *all* of the decision variables. For example,
    in order to solve the linear programming problem $\min_{x\in\mathcal{X}}c\cdot
    x$, we can augment the problem with a new decision variable to $\min_{(x,y)\in\Gamma}y$
    for $\Gamma\vcentcolon=\Set{(x,y)\in\mathcal{X}\times\mathbb{R}:y=c\cdot x}$,
    and project out the $x$ variables. The transformed problem is the a trivial univariate
    optimization problem: $\min_{y\in\operatorname{Proj}_{y}(\Gamma)}y$.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以投影*所有*决策变量。例如，为了解决线性规划问题$\min_{x\in\mathcal{X}}c\cdot x$，我们可以通过一个新的决策变量来扩展问题为$\min_{(x,y)\in\Gamma}y$，其中$\Gamma\vcentcolon=\Set{(x,y)\in\mathcal{X}\times\mathbb{R}:y=c\cdot
    x}$，并投影出$x$变量。转化后的问题是一个简单的一维优化问题：$\min_{y\in\operatorname{Proj}_{y}(\Gamma)}y$。
- en: 'Of course, the complexity of the approach described is hidden in the projection
    step, or building $\operatorname{Proj}_{y}(\Gamma)$. The most well-known algorithm
    for computing projections of linear inequality systems is Fourier-Motzkin elimination,
    described by Dantzig and Eaves ([1973](#bib.bib74)), which is notorious for its
    practical inefficiency. The process effectively comprises replacing variables
    from a set of inequalities with all possible implied inequalities, which can produce
    many unnecessary constraints. However, it turns out that neural network verification
    problems are well-structured in such a way that Fourier-Motzkin elimination can
    be performed very efficiently: for instance, by imposing one inequality upper
    bounding and one inequality lower bounding each ReLU function. Note that while
    Section [3.2](#S3.SS2 "3.2 The algebra of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") describes
    the use of Fourier-Motzkin elimination to obtain *exact* input-output relationships
    in linear regions of neural networks, here we are interested in obtaining linear
    *bounds* for a nonlinear function.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '当然，描述的方法的复杂性隐藏在投影步骤中，即构建$\operatorname{Proj}_{y}(\Gamma)$。计算线性不等式系统投影的最著名算法是傅里叶-莫兹金消元法，由丹齐格和伊夫斯([1973](#bib.bib74))描述，该算法因实际效率低下而臭名昭著。该过程实际上包括将一组不等式中的变量替换为所有可能的隐含不等式，这可能会产生许多不必要的约束。然而，事实证明，神经网络验证问题的结构使得傅里叶-莫兹金消元法可以非常高效地执行：例如，通过对每个ReLU函数施加一个不等式上界和一个不等式下界。请注意，虽然[3.2](#S3.SS2
    "3.2 The algebra of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")节描述了使用傅里叶-莫兹金消元法来获得神经网络线性区域中的*精确*输入输出关系，但在这里我们感兴趣的是为非线性函数获得线性*界限*。'
- en: '![Refer to caption](img/123fb03cab46add54faaa496e472ed4b.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/123fb03cab46add54faaa496e472ed4b.png)'
- en: 'Figure 10: Convex approximations for the ReLU function commonly used by propagation
    algorithms, given as a function of the preactivation function $\hat{h_{i}^{l}}$.
    The ReLU applies $h_{i}^{l}=\max(0,\hat{h_{i}^{l}})$.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：给定为预激活函数$\hat{h_{i}^{l}}$的函数，ReLU函数的凸近似，通常由传播算法使用。ReLU应用$h_{i}^{l}=\max(0,\hat{h_{i}^{l}})$。
- en: 'In fact, this general approach was independently developed in the verification
    community. While MILP research has focused on formulations tighter than the big-M,
    such as ([5](#S4.E5 "In 4.2.2 A stronger extended formulation ‣ 4.2 Exact models
    using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")) and ([6](#S4.E6 "In 4.2.3
    A class of intermediate formulations ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")), the verification community often prefers greater scalability
    at the price of weaker convex relaxations. The continuous relaxation of the big-M
    is equivalent to the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting the
    big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling further: Convex relaxations and
    linear programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")): the optimal convex relaxation for a single
    input, or in terms of the aggregated pre-activation function, as shown in Figure
    [10](#S4.F10 "Figure 10 ‣ 4.3.3 Fourier-Motzkin elimination and propagation algorithms
    ‣ 4.3 Scaling further: Convex relaxations and linear programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey"). However, the lower bound involves two linear constraints, which is not
    used in several propagation-based verification tools owing to scalability or compatibility.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种通用方法在验证社区中是独立开发的。虽然 MILP 研究集中在比大-M 更紧凑的形式上，例如 ([5](#S4.E5 "在 4.2.2 更强的扩展形式
    ‣ 4.2 使用混合整数编程的精确模型 ‣ 4 优化经过训练的神经网络 ‣ 当深度学习遇上多面体理论：综述")) 和 ([6](#S4.E6 "在 4.2.3
    一类中间形式 ‣ 4.2 使用混合整数编程的精确模型 ‣ 4 优化经过训练的神经网络 ‣ 当深度学习遇上多面体理论：综述"))，但验证社区通常更倾向于更高的可扩展性，即使这意味着更弱的凸松弛。大-M
    的连续松弛等同于三角松弛 ([8](#S4.E8 "在 4.3.1 投影大-𝑀 和理想 MILP 形式 ‣ 4.3 进一步扩展：凸松弛和线性规划 ‣ 4 优化经过训练的神经网络
    ‣ 当深度学习遇上多面体理论：综述"))：这是针对单一输入的最优凸松弛，或者从聚合的预激活函数来看，如图 [10](#S4.F10 "图 10 ‣ 4.3.3
    Fourier-Motzkin 消除和传播算法 ‣ 4.3 进一步扩展：凸松弛和线性规划 ‣ 4 优化经过训练的神经网络 ‣ 当深度学习遇上多面体理论：综述")
    所示。然而，下界涉及两个线性约束，这在几个基于传播的验证工具中未被使用，原因是可扩展性或兼容性问题。
- en: 'Such tools use methods such as abstract transformers to propagate polyhedral
    bounds, i.e., zonotopes, through the layers of a neural network. DeepZ (Singh
    et al., [2018](#bib.bib289)), Fast-Lin (Weng et al., [2018](#bib.bib330)), and
    Neurify (Wang et al., [2018a](#bib.bib326)) employ a parallel approximation, with
    the latter also implementing a branch-and-bound procedure towards completeness.
    Subsequently, DeepPoly (Singh et al., [2019b](#bib.bib291)) and CROWN (Zhang et al.,
    [2018a](#bib.bib354)) select between the zero and identity approximations by minimizing
    over-approximation area. OSIP (Hashemi et al., [2021](#bib.bib140)) selects between
    the three approximations using optimization: approximations for a layer are select
    jointly to minimise bounds for the following layer. These technologies are also
    compatible with interval bounds, propagating box domains (Mirman et al., [2018](#bib.bib220)).
    Interestingly, bounds on neural network weights can also be propagated using similar
    methods, allowing reachability analysis of Bayesian neural networks (Wicker et al.,
    [2020](#bib.bib332)).'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 此类工具使用诸如抽象变换器的方法，通过神经网络的层级传播多面体边界，即，zonotopes。DeepZ（Singh 等，[2018](#bib.bib289)）、Fast-Lin（Weng
    等，[2018](#bib.bib330)）和 Neurify（Wang 等，[2018a](#bib.bib326)）采用并行近似方法，后者还实现了一个分支定界程序以实现完整性。随后，DeepPoly（Singh
    等，[2019b](#bib.bib291)）和 CROWN（Zhang 等，[2018a](#bib.bib354)）通过最小化过度近似面积来选择零近似和单位近似。OSIP（Hashemi
    等，[2021](#bib.bib140)）使用优化方法在三种近似之间进行选择：一层的近似是联合选择的，以最小化下一层的边界。这些技术也兼容区间边界，传播盒域（Mirman
    等，[2018](#bib.bib220)）。有趣的是，也可以使用类似的方法传播神经网络权重的边界，从而实现贝叶斯神经网络的可达性分析（Wicker 等，[2020](#bib.bib332)）。
- en: 'Tjandraatmadja et al. ([2020](#bib.bib308)) provide an interpretation of these
    propagation techniques through the lens of Fourier-Motzkin elimination. Consider
    the problem of propagating bounds through a ReLU neural network: for a node $h_{i}^{l}=\mathrm{max}\{0,\hat{h_{i}^{l}}\}$,
    convex bounds for $h_{i}^{l}$ can be obtained given bounds for $\hat{h}_{i}^{l}$
    (Figure [10](#S4.F10 "Figure 10 ‣ 4.3.3 Fourier-Motzkin elimination and propagation
    algorithms ‣ 4.3 Scaling further: Convex relaxations and linear programming ‣
    4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")). Assuming the inputs are outputs of ReLU activations in the
    previous layer, $\hat{h}_{i}^{l}={\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$. Computing
    an upper bound can then be expressed as:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 'Tjandraatmadja 等人 ([2020](#bib.bib308)) 通过傅里叶-莫茨金消除法的视角解释了这些传播技术。考虑通过 ReLU
    神经网络传播界限的问题：对于一个节点 $h_{i}^{l}=\mathrm{max}\{0,\hat{h_{i}^{l}}\}$，可以根据 $\hat{h}_{i}^{l}$
    的界限获得 $h_{i}^{l}$ 的凸界限（图 [10](#S4.F10 "Figure 10 ‣ 4.3.3 Fourier-Motzkin elimination
    and propagation algorithms ‣ 4.3 Scaling further: Convex relaxations and linear
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")）。假设输入是前一层 ReLU 激活的输出，$\hat{h}_{i}^{l}={\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$。然后，上界的计算可以表示为：'
- en: '|  | $\displaystyle\max_{{\bm{h}}^{l-1}}$ | $\displaystyle{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$
    |  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{{\bm{h}}^{l-1}}$ | $\displaystyle{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$
    |  |'
- en: '|  | s.t. | $\displaystyle\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})\forall
    k\in\{1,...,n_{l-1}\}.$ |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})\forall
    k\in\{1,...,n_{l-1}\}.$ |  |'
- en: 'As the objective function is linear, the solution of this problem can be computed
    by propagation without explicit optimization. For each element in ${\bm{h}}^{l-1}$,
    we only need to consider the associated objective coefficient in ${\bm{w}}_{i}^{l}$
    to determine whether $\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}$ or $h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})$
    will be the active inequality at the optimal solution. We can thus replace $h^{l-1}_{k}$
    with $\mathcal{L}_{k}({\bm{h}}^{l-2})$ or $\mathcal{G}_{k}({\bm{h}}^{l-2})$ accordingly.
    This projection is mathematically equivalent to applying Fourier-Motzkin elimination,
    while avoiding redundant inequalities resulting from the ‘non-selected’ bounding
    function. Repeating this procedure for each layer results in a convex relaxation
    for the outputs that only involves the input variables. We naturally observe the
    desirability of simple lower bounds $\mathcal{L}_{k}({\bm{h}}^{l-2})$: imposing
    two-part lower bounds in each layer would increase the number of propagated constraints
    in an exponential manner, similar to Fourier-Motzkin elimination.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 由于目标函数是线性的，这个问题的解可以通过传播来计算，而无需显式优化。对于 ${\bm{h}}^{l-1}$ 中的每个元素，我们只需考虑 ${\bm{w}}_{i}^{l}$
    中的相关目标系数，以确定 $\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}$ 还是 $h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})$
    会在最优解中成为有效不等式。因此，我们可以相应地用 $\mathcal{L}_{k}({\bm{h}}^{l-2})$ 或 $\mathcal{G}_{k}({\bm{h}}^{l-2})$
    替换 $h^{l-1}_{k}$。这个投影在数学上等同于应用傅里叶-莫茨金消除法，同时避免了由于“未选择”边界函数产生的冗余不等式。对每一层重复这一过程，得到的输出凸放松仅涉及输入变量。我们自然地观察到简单下界
    $\mathcal{L}_{k}({\bm{h}}^{l-2})$ 的可取性：在每一层施加两部分下界会以指数方式增加传播约束的数量，类似于傅里叶-莫茨金消除法。
- en: A path towards completeness
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 完整性的路径
- en: 'Given an input-output bound, the reachable set can be refined by splitting
    the input space (Henriksen and Lomuscio, [2021](#bib.bib145), Rubies-Royo et al.,
    [2019](#bib.bib265))—a strategy similar to spatial branch and bound. In other
    words, completeness is achieved by branching in the input space, rather than activation
    patterns: this strategy is especially effective when the input space is low dimensional
    (Strong et al., [2022](#bib.bib296)). For example, ReluVal (Wang et al., [2018b](#bib.bib327))
    propagates symbolic intervals and implements splitting procedures on the input
    domain. As the interval extension of ReLU is Lipschitz continuous, the method
    converges to arbitrary accuracy in a finite number of splits.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入-输出界限，通过拆分输入空间可以进一步细化可达集（Henriksen 和 Lomuscio, [2021](#bib.bib145)，Rubies-Royo
    等人，[2019](#bib.bib265)）——这种策略类似于空间分支和界限。换句话说，完整性是通过在输入空间中分支来实现的，而不是激活模式：当输入空间维度较低时，这种策略特别有效（Strong
    等人，[2022](#bib.bib296)）。例如，ReluVal (Wang 等人，[2018b](#bib.bib327)) 传播符号区间并对输入域实现拆分程序。由于
    ReLU 的区间扩展是 Lipschitz 连续的，该方法在有限次数的拆分中收敛到任意精度。
- en: 4.4 Generalizing the single neuron model
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 泛化单神经元模型
- en: 4.4.1 Extending to other domains
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 扩展到其他领域
- en: In general, we will expect that the effective input domain $\mathcal{D}^{l-1}$
    for a given unit may be quite complex. For the first layer ($l=1$) this may derive
    from explicitly stated constraints on the inputs of the networks, while for later
    layers this will typically derive from the complex nonlinear transformations applied
    by the preceding layers. For example, in the context of surrogate models Yang
    et al. ([2022](#bib.bib347)) propose bounding the input to the convex hull of
    the training data set, while other works (Schweidtmann et al., [2022](#bib.bib277),
    Shi et al., [2022](#bib.bib285)) propose machine learning-inspired techniques
    for learning the trust region implied by the training data. In effect, these methods
    assume a trained model is locally accurate around training data, which is a property
    similar to that which verification seeks to prove.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们预计给定单元的有效输入域 $\mathcal{D}^{l-1}$ 可能相当复杂。对于第一层 ($l=1$)，这可能源于对网络输入的显式约束，而对于后续层，这通常来源于前面层施加的复杂非线性变换。例如，在代理模型的背景下，Yang
    等人 ([2022](#bib.bib347)) 提出将输入界定在训练数据集的凸包上，而其他研究（Schweidtmann 等人，[2022](#bib.bib277)，Shi
    等人，[2022](#bib.bib285)）提出了机器学习启发的技术来学习训练数据所隐含的信任区域。实际上，这些方法假设训练模型在训练数据周围是局部准确的，这是一种与验证所试图证明的属性类似的特性。
- en: 'Nevertheless, most research focuses on hyperrectangular input domains, largely
    motivated by practical considerations: i) there are efficient, well-studied methods
    for computing valid (though not necessarily optimally tight) variable bounds,
    ii) characterizing the exact effective domain may be computationally impractical,
    and iii) and the hyperrectangular structure makes analysis simpler for complex
    formulations like those presented in Section [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting
    plane methods: Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"). We note that Jordan et al. ([2019](#bib.bib167))
    use polyhedral analyses to perform verification over arbitrary (including non-polyhedral)
    norms, by fitting a $p$-norm ball in the decision region and checking adjacent
    linear regions. On the other hand, robust optimization can be employed to find
    $p$-norm adversarial regions (rather than verifying robustness), as opposed to
    a single point adversary (Maragno et al., [2023](#bib.bib211)).'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，大多数研究集中在超矩形输入域，这主要是由于实际考虑因素：i）存在高效、研究充分的有效（尽管不一定是最紧的）变量边界计算方法，ii）精确描述有效域可能在计算上不切实际，iii）超矩形结构使得分析复杂公式变得更简单，例如第[4.2.4](#S4.SS2.SSS4
    "4.2.4 切割平面方法：将变量换成不等式 ‣ 4.2 使用混合整数规划的精确模型 ‣ 4 优化训练过的神经网络 ‣ 当深度学习遇上多面体理论：一项调查")节中提出的复杂公式。我们注意到
    Jordan 等人 ([2019](#bib.bib167)) 使用多面体分析来对任意（包括非多面体）范数进行验证，通过在决策区域中拟合 $p$-范数球并检查相邻的线性区域。另一方面，鲁棒优化可以用来寻找
    $p$-范数对抗区域（而不是验证鲁棒性），与单点对抗者（Maragno 等人，[2023](#bib.bib211)）相对。
- en: Anderson et al. ([2020](#bib.bib5)) present two closely related frameworks for
    constructing ideal and hereditarily sharp formulations for ReLU units with arbitrary
    polyhedral input domains. This characterization is derived from Lagrangian duality,
    and requires an infinite number of constraints (intuitively, one for each choice
    of dual multipliers). Nonetheless, separation can still be done over this infinite
    family of inequalities via a subgradient-based algorithm; this approach will be
    tractable if optimization over $\mathcal{D}^{l-1}$ is tractable. Many propagation
    algorithms are also fully compatible with arbitrary polyhedral input domains,
    as the projected problem (i.e., a linear input-output relaxation) remains an LP.
    Singh et al. ([2021](#bib.bib292)) show that simplex input domains can actually
    be beneficial, creating tighter formulations by propagating constraints on the
    inputs through the network layers. Similarly, optimization-based bound tightening
    problems based on solving LPs can embed constraints defining polyhedral input
    domains.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Anderson et al.（[2020](#bib.bib5)）提出了两个密切相关的框架，用于构建理想和遗传上尖锐的 ReLU 单元公式，适用于任意多面体输入域。该表征源自拉格朗日对偶性，并要求无限数量的约束（直观地，对于每个对偶乘子的选择）。尽管如此，仍然可以通过基于次梯度的算法对这一无限不等式家族进行分离；如果在
    $\mathcal{D}^{l-1}$ 上的优化是可处理的，这种方法将是可行的。许多传播算法也完全兼容任意多面体输入域，因为投影问题（即线性输入输出松弛）仍然是线性规划。Singh
    et al.（[2021](#bib.bib292)）表明，单纯形输入域实际上可以是有益的，通过将输入上的约束传播到网络层中，从而创建更紧的公式。类似地，基于解决线性规划的优化约束紧缩问题可以嵌入定义多面体输入域的约束。
- en: In certain cases, additional structural information about the input domain can
    be used to reduce this semi-infinite description to a finite one. For example,
    this can be done when $\mathcal{D}^{l-1}$ is a Cartesian product of unit simplices
    (Anderson et al., [2020](#bib.bib5)) (note that this generalizes the box domain
    case, wherein each simplex is one-dimensional). This particular structure is particularly
    useful for modeling input domains with combinatorial constraints. For example,
    a network trained to predict binding propensity of a given length-$n$ DNA sequence
    is naturally modeled via an input domain that is the product of $n$ 4-dimensional
    simplices–one simplex for each letter in the sequence, each of which is selected
    from an alphabet of length 4.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，关于输入域的额外结构信息可以用来将这种半无限描述减少为有限描述。例如，当 $\mathcal{D}^{l-1}$ 是单位单纯形的笛卡尔积时（Anderson
    et al., [2020](#bib.bib5)）（注意这推广了盒域情况，其中每个单纯形是一维的）。这种特定结构对于建模具有组合约束的输入域特别有用。例如，训练网络以预测给定长度为
    $n$ 的 DNA 序列的结合倾向，自然通过一个输入域进行建模，该输入域是 $n$ 个 4 维单纯形的乘积——每个单纯形对应于序列中的一个字母，每个字母从长度为
    4 的字母表中选择。
- en: 4.4.2 Extending to other activation functions
  id: totrans-347
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 扩展到其他激活函数
- en: The big-$M$ formulation technique can be any piecewise linear activation function.
    While much of the literature focuses on the ReLU due to its widespread popularity,
    models for other activation functions have been explored in the literature. For
    example, multiple papers (Serra et al., [2018](#bib.bib282), Appendix K) (Tjeng
    et al., [2019](#bib.bib309), Appendix A.2) present a big-$M$ formulation for the
    maxout activation function. Adapting a formulation from Anderson et al. ([2020](#bib.bib5))
    (Anderson et al., [2020](#bib.bib5), Proposition 10), a formulation for the maxout
    unit is
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 大-$M$ 公式化技术可以是任何分段线性激活函数。虽然文献中大多集中于 ReLU 由于其广泛的流行性，但其他激活函数的模型也在文献中有所探讨。例如，多个论文（Serra
    et al., [2018](#bib.bib282)，附录 K）（Tjeng et al., [2019](#bib.bib309)，附录 A.2）提出了
    maxout 激活函数的大-$M$ 公式。参考 Anderson et al. 的公式（[2020](#bib.bib5)）（Anderson et al.,
    [2020](#bib.bib5)，命题 10），maxout 单元的公式为
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\leq u_{j}({\bm{h}}^{l-1})+M^{l}_{i,j}(1-z_{j})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\leq u_{j}({\bm{h}}^{l-1})+M^{l}_{i,j}(1-z_{j})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\geq u_{j}({\bm{h}}^{l-1})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\geq u_{j}({\bm{h}}^{l-1})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
- en: '|  | $\displaystyle\sum_{j=1}^{k}z_{j}$ | $\displaystyle=1$ |  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{j=1}^{k}z_{j}$ | $\displaystyle=1$ |  |'
- en: '|  | $\displaystyle({\bm{h}}^{l-1},v^{l}_{i},z)$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}\times\{0,1\}^{k},$
    |  |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle({\bm{h}}^{l-1},v^{l}_{i},z)$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}\times\{0,1\}^{k},$
    |  |'
- en: where each $M^{l}_{i,j}$ is selected such that
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个 $M^{l}_{i,j}$ 都是选择的，以满足
- en: '|  | $M^{l}_{i,j}\geq\max_{\tilde{{\bm{h}}}\in\mathcal{D}^{l-1}}u_{j}(\tilde{{\bm{h}}}).$
    |  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  | $M^{l}_{i,j}\geq\max_{\tilde{{\bm{h}}}\in\mathcal{D}^{l-1}}u_{j}(\tilde{{\bm{h}}}).$
    |  |'
- en: We can observe that the big-$M$ formulation can also handle other discontinuous
    activation functions, such as a binary/sign activations (Han and Gómez, [2021](#bib.bib136))
    or more general quantized activations (Nguyen and Huchette, [2022](#bib.bib234)).
    Nevertheless, the binary activation function naturally lends itself towards Boolean
    satisfiability, and most work therefore focuses on alternative methods such as
    SAT (Cheng et al., [2018](#bib.bib57), Jia and Rinard, [2020](#bib.bib164), Narodytska
    et al., [2018](#bib.bib229)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，大-$M$ 公式也可以处理其他不连续激活函数，例如二进制/符号激活函数（Han和Gómez，[2021](#bib.bib136)）或更一般的量化激活函数（Nguyen和Huchette，[2022](#bib.bib234)）。然而，二进制激活函数自然适用于布尔可满足性，因此大多数工作集中在诸如SAT（Cheng等，[2018](#bib.bib57)，Jia和Rinard，[2020](#bib.bib164)，Narodytska等，[2018](#bib.bib229)）等替代方法上。
- en: While this survey focuses on neural networks with piecewise linear activation
    functions, we note that recent research has also studied smooth activation functions
    with a similar aim. For example, optimization over smooth activation functions
    can be handled by piecewise linear approximation and conversion to MILP (Sildir
    and Aydin, [2022](#bib.bib287)). Researchers have also studied convex/concave
    bounds for nonlinear activation functions, which can then be embedded in spatial
    branch-and-bound procedures (Schweidtmann and Mitsos, [2019](#bib.bib276), Wilhelm
    et al., [2022](#bib.bib334)). In contrast to MILP formulations for ReLU neural
    networks, these problems are typically nonlinear programs that must be solved
    via spatial branch and bound.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本综述侧重于具有分段线性激活函数的神经网络，但我们注意到最近的研究也研究了具有类似目标的平滑激活函数。例如，平滑激活函数上的优化可以通过分段线性近似和转换为MILP来处理（Sildir和Aydin，[2022](#bib.bib287)）。研究人员还研究了非线性激活函数的凸/凹界限，这些界限可以嵌入空间分支定界程序中（Schweidtmann和Mitsos，[2019](#bib.bib276)，Wilhelm等，[2022](#bib.bib334)）。与ReLU神经网络的MILP公式相比，这些问题通常是非线性程序，必须通过空间分支和定界来解决。
- en: 'Propagation methods (Singh et al., [2018](#bib.bib289), Zhang et al., [2018a](#bib.bib354))
    can also naturally handle general activation functions: given convex polytopic
    bounds for an activation function, these tools can propagate them through network
    layers using the same techniques. For example, Fastened CROWN (Lyu et al., [2020](#bib.bib204))
    employs a set of search heuristics to quickly select linear upper and lower bounds
    on ReLU, sigmoid, and hyperbolic tangent activation functions. Tighter polyhedral
    bounds can be employed, such as piecewise linear upper and lower bounds (Benussi
    et al., [2022](#bib.bib23)).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 传播方法（Singh等，[2018](#bib.bib289)，Zhang等，[2018a](#bib.bib354)）也可以自然地处理一般激活函数：给定激活函数的凸多面体界限，这些工具可以通过相同的技术将其传播到网络层。例如，Fastened
    CROWN（Lyu等，[2020](#bib.bib204)）使用一组搜索启发式方法快速选择ReLU、sigmoid和双曲正切激活函数的线性上界和下界。可以使用更紧的多面体界限，例如分段线性上界和下界（Benussi等，[2022](#bib.bib23)）。
- en: 4.4.3 Extending to adversarial training
  id: totrans-358
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 扩展到对抗训练
- en: 'As described in Section [1](#S1 "1 Introduction ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), the *training* of neural networks seeks to minimise
    a measure of distance between the output $y$ and the correct output $\hat{y}$.
    For instance, if this distance is prescribed as loss function $\mathcal{L}(y,\hat{y})$,
    this corresponds to solving the *training* optimization problem:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[1](#S1 "1 Introduction ‣ When Deep Learning Meets Polyhedral Theory: A Survey")节所述，神经网络的*训练*旨在最小化输出$y$与正确输出$\hat{y}$之间的距离度量。例如，如果这个距离被规定为损失函数$\mathcal{L}(y,\hat{y})$，这就对应于解决*训练*优化问题：'
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\mathcal{L}(y,\hat{y}).$
    |  | (10) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\mathcal{L}(y,\hat{y}).$
    |  | (10) |'
- en: 'Further details about the training problem and solution methods are described
    in the following section. Here, we briefly outline how verification techniques
    can be embedded in training. Specifically, solutions or bounds to the verification
    problem (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Neural network verification ‣ 4.1
    Applications of optimization over trained networks ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) provide
    a metric of how robust a trained neural network is to perturbations. These metrics
    can be embedded in the training problem to obtain a more robust network during
    training, often resulting in a bilevel training problem. For instance, the verification
    problem ([3](#S4.E3 "In Example 4 ‣ 4.1.1 Neural network verification ‣ 4.1 Applications
    of optimization over trained networks ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) can be embedded as a
    lower-level problem, giving the robust optimization problem:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '关于训练问题和解决方法的更多细节将在以下部分中描述。这里，我们简要概述了如何将验证技术嵌入训练中。具体而言，验证问题的解或边界（第 [4.1.1](#S4.SS1.SSS1
    "4.1.1 Neural network verification ‣ 4.1 Applications of optimization over trained
    networks ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey") 节）提供了一个指标，表明训练好的神经网络对扰动的鲁棒性。这些指标可以嵌入训练问题中，以在训练过程中获得更鲁棒的网络，通常会导致双层训练问题。例如，验证问题
    ([3](#S4.E3 "In Example 4 ‣ 4.1.1 Neural network verification ‣ 4.1 Applications
    of optimization over trained networks ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) 可以作为下层问题嵌入，得到鲁棒优化问题：'
- en: '|  | $\displaystyle\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\quad\underset{&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon}{\mathrm{max}}\quad\mathcal{L}(y=f(x),\hat{y}).$
    |  |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\quad\underset{&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon}{\mathrm{max}}\quad\mathcal{L}(y=f(x),\hat{y}).$
    |  |'
- en: Solving these problems generally involves either bilevel optimization, or computing
    an adversarial solution/bound at each training step, conceptually similar to the
    robust cutting plane approach. Madry et al. ([2018](#bib.bib206)) proposed this
    formulation and solved the nonconvex inner problem using gradient descent, thereby
    losing a formal certification of robustness. These approaches may also benefit
    from reformulation strategies, such as by taking the dual of the inner problem
    and using any feasible solution as a bound (Wong and Kolter, [2018](#bib.bib335)).
    The resulting models are not only more robust, but several works have also found
    it to be empirically easier to verify robustness in them (Mirman et al., [2018](#bib.bib220),
    Wong and Kolter, [2018](#bib.bib335)).
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题通常涉及双层优化，或者在每个训练步骤中计算对抗解/边界，这在概念上类似于鲁棒切割平面方法。Madry 等人 ([2018](#bib.bib206))
    提出了这一公式，并使用梯度下降法解决非凸的内层问题，从而失去了鲁棒性的正式证明。这些方法也可以通过重新表述策略受益，例如通过取内层问题的对偶并使用任何可行解作为边界（Wong
    和 Kolter, [2018](#bib.bib335)）。得到的模型不仅更加鲁棒，而且多项研究还发现验证其鲁棒性在这些模型中经验上更容易（Mirman
    等人, [2018](#bib.bib220), Wong 和 Kolter, [2018](#bib.bib335)）。
- en: 'Alternatively, robustness can be induced by designing an additional penalty
    term for the training loss function, in a similar vein to regularization. For
    example:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提高鲁棒性的方法是通过为训练损失函数设计额外的惩罚项，这与正则化的原理类似。例如：
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\kappa\mathcal{L}(y,\hat{y})+(1-\kappa)\mathcal{L}_{\mathrm{robust}}(\cdot).$
    |  |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\kappa\mathcal{L}(y,\hat{y})+(1-\kappa)\mathcal{L}_{\mathrm{robust}}(\cdot).$
    |  |'
- en: Additionally, if these robustness penalties are differentiable, they can be
    embedded into standard gradient descent based optimization algorithms (Dvijotham
    et al., [2018b](#bib.bib85), Mirman et al., [2018](#bib.bib220)). In the above
    formulation, the parameter $\kappa$ controls the relative weighting between fitting
    the training data and satisfying some robustness criterion, and its value can
    be scheduled during training, e.g., to first focus on model accuracy (Gowal et al.,
    [2018](#bib.bib129)). In these cases, over-approximation of the reachable set
    is less problematic, as it merely produces a model *more* robust than required.
    Nevertheless, Balunović and Vechev ([2020](#bib.bib16)) improve relaxation tightness
    by searching for adversarial examples in the “latent” space between hidden layers,
    reducing the number of propagation steps. Zhang et al. ([2020](#bib.bib355)) provide
    an implementation that that tightens relaxations by also propagating bounds backwards
    through the network.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果这些鲁棒性惩罚是可微的，它们可以嵌入到基于梯度下降的标准优化算法中（Dvijotham 等，[2018b](#bib.bib85)，Mirman
    等，[2018](#bib.bib220)）。在上述公式中，参数$\kappa$ 控制着拟合训练数据和满足某些鲁棒性标准之间的相对权重，其值可以在训练过程中进行调节，例如，首先专注于模型准确性（Gowal
    等，[2018](#bib.bib129)）。在这些情况下，对可达集合的过度逼近问题较小，因为它只是产生了一个*比要求的*更鲁棒的模型。然而，Balunović
    和 Vechev ([2020](#bib.bib16)) 通过在隐藏层之间的“潜在”空间中搜索对抗样本来提高松弛的紧致性，从而减少了传播步骤的数量。Zhang
    等 ([2020](#bib.bib355)) 提供了一种通过将界限向后传播通过网络来紧缩松弛的实现。
- en: 5 Linear Programming and Polyhedral Theory in Training
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 训练中的线性规划和多面体理论
- en: 'In the previous sections, we have almost exclusively focused on tasks involving
    neural networks that have already been constructed, i.e., we have assumed that
    the training step has already concluded (with the exception of Section [4.4.3](#S4.SS4.SSS3
    "4.4.3 Extending to adversarial training ‣ 4.4 Generalizing the single neuron
    model ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")). In this section, we focus on the training phase,
    whose goal is to construct a neural network that can represent the relationship
    between the input and output of a given set of data points.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的章节中，我们几乎完全专注于已经构建好的神经网络任务，即我们假设训练步骤已经结束（除非在第 [4.4.3](#S4.SS4.SSS3 "4.4.3
    Extending to adversarial training ‣ 4.4 Generalizing the single neuron model ‣
    4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")节）。在本节中，我们关注训练阶段，其目标是构建一个能够表示给定数据点输入和输出之间关系的神经网络。'
- en: Let us consider a set of points, or sample, $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    and assume that these points are related via a function $\hat{f}$, i.e., $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$
    $i=1,\ldots,D$. In the training phase, we look for $\hat{f}$ in a pre-defined
    class (e.g. neural networks with a specific architecture) that approximates the
    relation $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$. Typically, this
    is done by solving an Empirical Risk Minimization problem
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个点集或样本 $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$，并假设这些点通过函数
    $\hat{f}$ 相关，即 $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$ $i=1,\ldots,D$。在训练阶段，我们在一个预定义的类中寻找
    $\hat{f}$（例如，具有特定结构的神经网络），该类近似关系 $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$。通常，通过解决一个经验风险最小化问题来完成这项工作
- en: '|  | $\min_{\hat{f}\in F}\frac{1}{D}\sum_{i=1}^{D}\ell(\hat{f}(\tilde{{\bm{x}}}_{i}),\tilde{{\bm{y}}}_{i})$
    |  | (11) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\hat{f}\in F}\frac{1}{D}\sum_{i=1}^{D}\ell(\hat{f}(\tilde{{\bm{x}}}_{i}),\tilde{{\bm{y}}}_{i})$
    |  | (11) |'
- en: where $\ell$ is a loss function and $F$ is the class of functions we are restricted
    to. We usually assume the class $F$ is parametrized by $({\bm{W}},{\bm{b}})\in\Theta$
    (the network weights and biases), so we are further assuming that there exists
    a function $f(\cdot,\cdot,\cdot)$ (the network architecture) such that
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\ell$ 是一个损失函数，$F$ 是我们受限的函数类。我们通常假设类 $F$ 由 $({\bm{W}},{\bm{b}})\in\Theta$（网络权重和偏置）参数化，因此我们进一步假设存在一个函数
    $f(\cdot,\cdot,\cdot)$（网络结构），使得
- en: '|  | $\forall\hat{f}\in F,\,\exists({\bm{W}},{\bm{b}})\in\Theta,\,\hat{f}({\bm{x}})=f({\bm{x}},{\bm{W}},{\bm{b}}),$
    |  |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | $\forall\hat{f}\in F,\,\exists({\bm{W}},{\bm{b}})\in\Theta,\,\hat{f}({\bm{x}})=f({\bm{x}},{\bm{W}},{\bm{b}}),$
    |  |'
- en: and thus, the optimization is performed over the space of parameters. In many
    cases, $\Theta=\mathbb{R}^{N}$—the parameters are unrestricted real numbers—but
    we will see some cases when a different parameter space can be used.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，优化是在参数空间上进行的。在许多情况下，$\Theta=\mathbb{R}^{N}$——参数是不受限制的实数——但我们将看到在某些情况下可以使用不同的参数空间。
- en: 'As mentioned in the introduction, nowadays, most of the practically successful
    *training* algorithms for neural networks, i.e., that solve or approximate ([11](#S5.E11
    "In 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")), are based on Stochastic Gradient Descent
    (SGD). From a fundamental perspective, optimization problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is typically a *non-convex, unconstrained* problem
    that needs to be solved efficiently and where finding a *local minimum* is sufficient.
    Thus, it is not too surprising that linear programming appears to be an unsuitable
    tool in this phase, in general. Nonetheless, there are some notable and surprising
    exceptions to this, which we review here.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 如引言中所述，如今，大多数实际成功的*训练*算法，即那些解决或近似([11](#S5.E11 "在 5 线性规划与多面体理论中 ‣ 当深度学习遇上多面体理论：综述"))的算法，基于随机梯度下降（SGD）。从基本的角度看，优化问题([11](#S5.E11
    "在 5 线性规划与多面体理论中 ‣ 当深度学习遇上多面体理论：综述"))通常是一个*非凸、无约束*问题，需要有效解决，并且找到*局部最小值*即可。因此，线性规划在这一阶段通常看似不适用也就不足为奇。然而，确实存在一些显著且出人意料的例外，我们将在这里回顾这些例外。
- en: 'Linear programming played an interesting role in training neural networks before
    SGD became the predominant training method and provided an efficient approach
    for constructing neural networks with 1 hidden layer in the 90s. This method has
    some common points in their polyhedral approach with the first known algorithm
    that can solve ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to provable
    optimality for a 1-hidden-layer ReLU neural network, which was proposed in 2018\.
    Recently, a stream of work has exploited similar polyhedral structures to obtain
    convex optimization reformulations of regularized training problems of ReLU networks.
    Linear programming tools have also been used within SGD-type methods in order
    to compute optimal *step-sizes* in the optimization of ([11](#S5.E11 "In 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) or to strictly enforce structure in $\Theta$. From a different
    perspective, a *data-independent* polytope was used to describe approximately
    all training problems that can arise from an uncertainty set. Additionally, a
    back-propagation-like algorithm for training neural networks, which solves mixed-integer
    linear problems in each layer, was proposed as an alternative to SGD. Furthermore,
    when the neural network weights are required to be discrete, the applicability
    of SGD is impaired, and mixed-integer linear models have been proposed to tackle
    the corresponding training problems.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性规划成为主要训练方法之前，它在训练神经网络中发挥了有趣的作用，并提供了一种高效的方法用于构建1990年代具有1个隐藏层的神经网络。这种方法在多面体方法上与第一个已知的算法有一些共同点，该算法能证明1隐藏层ReLU神经网络的最优性，该算法于2018年提出。最近，一系列工作利用类似的多面体结构，获得了ReLU网络正则化训练问题的凸优化重构。线性规划工具还被应用于SGD类型的方法中，以计算优化([11](#S5.E11
    "在 5 线性规划与多面体理论中 ‣ 当深度学习遇上多面体理论：综述"))中的*步长*或严格执行结构$\Theta$。从不同的角度看，一个*数据无关*的多面体被用来大致描述所有可能从不确定性集合中出现的训练问题。此外，还提出了一种类似于反向传播的算法，用于训练神经网络，在每一层解决混合整数线性问题，作为SGD的替代方案。此外，当神经网络权重需要离散时，SGD的适用性受到限制，因此提出了混合整数线性模型来处理相应的训练问题。
- en: In what follows, we review these roles of (mixed-integer) linear programming
    and polyhedral theory within training contexts. We refer the reader to the book
    by Goodfellow et al. ([2016](#bib.bib125)) and the surveys by Curtis and Scheinberg
    ([2017](#bib.bib70)), Bottou et al. ([2018](#bib.bib38)), and Wright ([2018](#bib.bib337))
    for in-depth descriptions and analyses of the most commonly used training methods
    for neural networks.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们回顾了（混合整数）线性规划和多面体理论在训练背景下的作用。我们建议读者参考Goodfellow等人（[2016](#bib.bib125)）的书籍以及Curtis和Scheinberg（[2017](#bib.bib70)）、Bottou等人（[2018](#bib.bib38)）和Wright（[2018](#bib.bib337)）的综述，以深入了解和分析最常用的神经网络训练方法。
- en: We remark that solving the training problem to global optimality for ReLU neural
    networks is computationally complex. Even in architectures with just one hidden
    node, the problem is NP-hard (Dey et al., [2020](#bib.bib81), Goel et al., [2021](#bib.bib121)).
    Also see Blum and Rivest ([1992](#bib.bib33)), Boob et al. ([2022](#bib.bib36)),
    Chen et al. ([2022c](#bib.bib52)), Froese et al. ([2022](#bib.bib110)), Froese
    and Hertrich ([2023](#bib.bib109)) for other hardness results. Furthermore, it
    has been recently shown that training ReLU networks is $\exists\mathbb{R}$-complete
    (Abrahamsen et al., [2021](#bib.bib1), Bertschinger et al., [2022](#bib.bib27)),
    which implies that it is likely that the problem of optimally training ReLU neural
    networks is not even in NP. Therefore, it is not strange to see that some of the
    methods we review below, even when they are solving hard problems as sub-routines
    (like mixed-integer linear problems), either make some non-trivial assumptions
    or relax some requirements. For example, boundedness and/or integrality of the
    weights, architecture restrictions such as the output dimension, or not having
    optimality guarantees.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指出，对于ReLU神经网络，将训练问题求解至全局最优是计算上复杂的。即使在只有一个隐藏节点的架构中，这个问题也是NP难的（Dey等，[2020](#bib.bib81)，Goel等，[2021](#bib.bib121)）。另见Blum和Rivest（[1992](#bib.bib33)），Boob等（[2022](#bib.bib36)），Chen等（[2022c](#bib.bib52)），Froese等（[2022](#bib.bib110)），Froese和Hertrich（[2023](#bib.bib109)）以获取其他困难结果。此外，最近已显示训练ReLU网络是$\exists\mathbb{R}$-完整的（Abrahamsen等，[2021](#bib.bib1)，Bertschinger等，[2022](#bib.bib27)），这意味着训练ReLU神经网络的优化问题可能甚至不在NP中。因此，不难理解，我们下面回顾的一些方法，即使在解决困难问题作为子例程时（如混合整数线性问题），也要么做出一些非平凡的假设，要么放松一些要求。例如，权重的有界性和/或整数性，架构限制，如输出维度，或没有最优性保证。
- en: It is worth mentioning that, in contrast, for LTUs, exact exponential-time training
    algorithms are known for much more general architectures than in the ReLU case
    (Khalife and Basu, [2022](#bib.bib173), Ergen et al., [2023](#bib.bib97)). These
    are out of scope for this survey, though we will provide a high-level overview
    of some of them, as they share some similarities to approaches designed for ReLU
    networks.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，与此相反，对于LTUs，已知确切的指数时间训练算法适用于比ReLU情况更为通用的架构（Khalife和Basu，[2022](#bib.bib173)，Ergen等，[2023](#bib.bib97)）。尽管这些超出了本调查的范围，但我们将提供一些高级概述，因为它们与为ReLU网络设计的方法有一些相似之处。
- en: 5.1 Training neural networks with a single hidden layer
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 训练具有单层隐藏层的神经网络
- en: Following the well-known XOR limitation of the perceptron (Minsky and Papert,
    [1969](#bib.bib219)), a natural interest arose in the development of training
    algorithms that could handle at least one hidden layer. In this section, we review
    training algorithms that can successfully minimize the training error in a one-hidden-layer
    setting and rely on polyhedral approaches.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 继著名的感知机XOR限制（Minsky和Papert，[1969](#bib.bib219)）之后，自然产生了开发可以处理至少一层隐藏层的训练算法的兴趣。在本节中，我们回顾了能够成功最小化一层隐藏层设置中的训练误差并依赖于多面体方法的训练算法。
- en: 5.1.1 Problem setting and solution scheme
  id: totrans-381
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 问题设置和解决方案
- en: Suppose we have a sample of size $D$ $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$
    where $\tilde{{\bm{x}}}_{i}\in\mathbb{R}^{n}$ and $\tilde{{\bm{y}}}_{i}\in\mathbb{R}$.
    In a training phase, we would like to find a neural network function $f(\cdot,\cdot,\cdot)$
    that represents in the best possible way the relation $f(\tilde{{\bm{x}}}_{i},{\bm{W}},{\bm{b}})=\tilde{{\bm{y}}}_{i}$.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个大小为$D$的样本$(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$，其中$\tilde{{\bm{x}}}_{i}\in\mathbb{R}^{n}$且$\tilde{{\bm{y}}}_{i}\in\mathbb{R}$。在训练阶段，我们希望找到一个神经网络函数$f(\cdot,\cdot,\cdot)$，该函数以最佳方式表示关系$f(\tilde{{\bm{x}}}_{i},{\bm{W}},{\bm{b}})=\tilde{{\bm{y}}}_{i}$。
- en: Note that when a neural network $\hat{f}$ has only one hidden layer, its behavior
    is almost completely determined by the sign of each component of the vector
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当神经网络$\hat{f}$只有一层隐藏层时，其行为几乎完全由向量中每个分量的符号决定。
- en: '|  | ${\bm{W}}^{1}x-{\bm{b}}^{1}.$ |  |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}^{1}x-{\bm{b}}^{1}.$ |  |'
- en: These are the cases of ReLU activations $\sigma(z)=\max\{0,z\}$ and LTU activations
    $\sigma(z)=\text{sgn}(z)$. The training algorithms we show here heavily exploit
    this observation and construct $({\bm{W}}^{1},{\bm{b}}^{1})$ by embedding in this
    phase a *hyperplane partition* problem based on the sample $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$.
    While the focus of this survey is mainly devoted to ReLU activations, we also
    discuss some selected cases with LTU activations as they share some similar ideas.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是 ReLU 激活函数 $\sigma(z)=\max\{0,z\}$ 和 LTU 激活函数 $\sigma(z)=\text{sgn}(z)$ 的情况。我们在这里展示的训练算法充分利用了这一观察结果，并通过在此阶段嵌入一个基于样本
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$ 的*超平面划分*问题来构造 $({\bm{W}}^{1},{\bm{b}}^{1})$。尽管本调查主要集中于
    ReLU 激活函数，但我们也讨论了一些具有 LTU 激活函数的选定案例，因为它们分享一些相似的理念。
- en: 5.1.2 LTU activations and variable number of nodes
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 LTU 激活函数和可变节点数量
- en: One stream of work dedicated to developing training algorithms for one-hidden-layer
    networks concerned the use of *backpropagation* (Rumelhart et al., [1986](#bib.bib266),
    LeCun et al., [1989](#bib.bib185), Werbos, [1974](#bib.bib331)). In the early
    90s, an alternative family of methods was proposed, which was heavily based on
    linear programs (see e.g. Bennett and Mangasarian ([1990](#bib.bib21), [1992](#bib.bib22)),
    Roy et al. ([1993](#bib.bib264)), Mukhopadhyay et al. ([1993](#bib.bib227))).
    These approaches can construct a 1-hidden-layer network without the need for an
    *a-priori* number of nodes in the network. We illustrate the high-level idea of
    these next, based on the survey by Mangasarian ([1993](#bib.bib209)).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 一些专注于为单隐层网络开发训练算法的研究工作涉及了*反向传播*的使用（Rumelhart 等，[1986](#bib.bib266)，LeCun 等，[1989](#bib.bib185)，Werbos，[1974](#bib.bib331)）。在90年代初期，提出了一类替代性方法，这些方法主要基于线性规划（例如
    Bennett 和 Mangasarian（[1990](#bib.bib21)，[1992](#bib.bib22)），Roy 等（[1993](#bib.bib264)），Mukhopadhyay
    等（[1993](#bib.bib227)））。这些方法可以在不需要*先验*设定网络节点数量的情况下构建一个单隐层网络。我们将基于 Mangasarian
    的调查（[1993](#bib.bib209)）说明这些方法的高级概念。
- en: Suppose that $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$, thus the NN we construct will
    be a classifier. The training phase can be tackled via the construction of a *polyhedral
    partition* of $\mathbb{R}^{n}$ such that no two points (or few) $\tilde{{\bm{x}}}_{i}$
    and $\tilde{{\bm{x}}}_{j}$ such that $\tilde{{\bm{y}}}_{i}\neq\tilde{{\bm{y}}}_{j}$
    lie in the same element of the partition. To achieve this, the following approach
    presented by Bennett and Mangasarian ([1992](#bib.bib22)) can be followed. Let
    $Y=\{i\in[D]\,:\,\tilde{{\bm{y}}}_{i}=1\}$ and $N=[D]\setminus Y$, and consider
    the following optimization problem
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$，因此我们构造的神经网络将是一个分类器。训练阶段可以通过构造 $\mathbb{R}^{n}$
    的*多面体划分*来处理，以确保没有两个（或少数）点 $\tilde{{\bm{x}}}_{i}$ 和 $\tilde{{\bm{x}}}_{j}$ 满足 $\tilde{{\bm{y}}}_{i}\neq\tilde{{\bm{y}}}_{j}$，它们不会落在同一划分元素中。为了实现这一点，可以遵循
    Bennett 和 Mangasarian（[1992](#bib.bib22)）提出的以下方法。令 $Y=\{i\in[D]\,:\,\tilde{{\bm{y}}}_{i}=1\}$
    和 $N=[D]\setminus Y$，然后考虑以下优化问题
- en: '|  |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\min_{{\bm{w}},b,y,z}\quad$ | $\displaystyle\frac{1}{&#124;Y&#124;}\sum_{i\in
    Y}y_{i}+\frac{1}{&#124;N&#124;}\sum_{i\in N}z_{i}$ |  | (12a) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{w}},b,y,z}\quad$ | $\displaystyle\frac{1}{&#124;Y&#124;}\sum_{i\in
    Y}y_{i}+\frac{1}{&#124;N&#124;}\sum_{i\in N}z_{i}$ |  | (12a) |'
- en: '|  |  | $\displaystyle{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+y\geq 1$ | $\displaystyle\forall
    i\in Y$ |  | (12b) |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+y\geq 1$ | $\displaystyle\forall
    i\in Y$ |  | (12b) |'
- en: '|  |  | $\displaystyle-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+z\geq 1$ | $\displaystyle\forall
    i\in N$ |  | (12c) |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+z\geq 1$ | $\displaystyle\forall
    i\in N$ |  | (12c) |'
- en: '|  |  | $\displaystyle y,z\geq 0.$ |  | (12d) |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle y,z\geq 0.$ |  | (12d) |'
- en: This LP aims at finding a hyperplane ${\bm{w}}^{\top}x=b$ separating the data
    according to their value of $\tilde{{\bm{y}}}_{i}$. Since the data may not be
    separable, the LP is minimizing the following classification error
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这个线性规划旨在找到一个超平面 ${\bm{w}}^{\top}x=b$，根据数据的 $\tilde{{\bm{y}}}_{i}$ 值对数据进行分隔。由于数据可能不可分，线性规划的目标是最小化以下分类错误
- en: '|  | $\frac{1}{&#124;Y&#124;}\sum_{i\in Y}(-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+1)_{+}+\frac{1}{&#124;N&#124;}\sum_{i\in
    N}({\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+1)_{+}.$ |  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{1}{&#124;Y&#124;}\sum_{i\in Y}(-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+1)_{+}+\frac{1}{&#124;N&#124;}\sum_{i\in
    N}({\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+1)_{+}.$ |  |'
- en: 'The LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of nodes
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is a linear reformulation of the latter minimization problem, where
    the auxiliary values $y,z$ take the value of each element in the sum.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: LP ([12](#S5.E12 "在 5.1.2 LTU 激活函数和节点数量变化 ‣ 5.1 训练具有单一隐层的神经网络 ‣ 5 线性规划和训练中的多面体理论
    ‣ 当深度学习遇上多面体理论：综述")) 是后者最小化问题的线性重构，其中辅助值$y,z$取每个元素的值。
- en: 'Once the LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of
    nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is solved, we obtain 2 halfspaces classifying our data points. In
    order to obtain a richer classification and lower error, we can iterate the procedure
    by means of the Multi-Surface Method Tree (MSMT, see Bennett ([1992](#bib.bib20))),
    which solves a sequence of LPs as ([12](#S5.E12 "In 5.1.2 LTU activations and
    variable number of nodes ‣ 5.1 Training neural networks with a single hidden layer
    ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) in order to produce a polyhedral partition
    of $\mathbb{R}^{n}$. Let us illustrate how this procedure works in a simplified
    case: assume that solving ([12](#S5.E12 "In 5.1.2 LTU activations and variable
    number of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) results in a vector ${\bm{w}}_{1}$ such that'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦解决了LP ([12](#S5.E12 "在 5.1.2 LTU 激活函数和节点数量变化 ‣ 5.1 训练具有单一隐层的神经网络 ‣ 5 线性规划和训练中的多面体理论
    ‣ 当深度学习遇上多面体理论：综述"))，我们得到2个半空间对数据点进行分类。为了获得更丰富的分类和更低的错误率，我们可以通过多表面方法树（MSMT，见Bennett
    ([1992](#bib.bib20)))来迭代这一过程，该方法解决一系列LP问题，如([12](#S5.E12 "在 5.1.2 LTU 激活函数和节点数量变化
    ‣ 5.1 训练具有单一隐层的神经网络 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇上多面体理论：综述"))，以生成$\mathbb{R}^{n}$的多面体划分。我们用一个简化的例子来说明这一过程是如何工作的：假设解决([12](#S5.E12
    "在 5.1.2 LTU 激活函数和节点数量变化 ‣ 5.1 训练具有单一隐层的神经网络 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇上多面体理论：综述"))得到一个向量${\bm{w}}_{1}$，使得
- en: '|  | $\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq b_{1}\}\subseteq
    Y\quad\land\quad\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq a_{1}\}\subseteq
    N,$ |  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq b_{1}\}\subseteq
    Y\quad\land\quad\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq a_{1}\}\subseteq
    N,$ |  |'
- en: 'for some $a_{1},b_{1}\in\mathbb{R}^{n}$ with $b_{1}>a_{1}$. We can remove the
    sets $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq
    b_{1}\}$ and $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq
    a_{1}\}$ from the data-set and redefine ([12](#S5.E12 "In 5.1.2 LTU activations
    and variable number of nodes ‣ 5.1 Training neural networks with a single hidden
    layer ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) accordingly, in order to obtain a new vector
    ${\bm{w}}_{2}$ and scalars $b_{2},a_{2}$ that would be used to classify within
    the region $\{x\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1}\}$.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些$a_{1},b_{1}\in\mathbb{R}^{n}$且$b_{1}>a_{1}$，我们可以从数据集中移除集合$\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq
    b_{1}\}$和$\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq
    a_{1}\}$，并相应地重新定义([12](#S5.E12 "在 5.1.2 LTU 激活函数和节点数量变化 ‣ 5.1 训练具有单一隐层的神经网络 ‣
    5 线性规划和训练中的多面体理论 ‣ 当深度学习遇上多面体理论：综述"))，以获得新的向量${\bm{w}}_{2}$和标量$b_{2},a_{2}$，这些将用于在区域$\{x\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1}\}$内进行分类。
- en: 'This procedure can be iterated, and the polyhedral partition of $\mathbb{R}^{n}$
    induced by the resulting hyperplanes can be easily transformed into a Neural Network
    with 1 hidden layer and LTU activations (see Bennett and Mangasarian ([1990](#bib.bib21))
    for details). We illustrate this transformation with the following example: suppose
    that after 3 iterations we have the following regions, with the arrow indicating
    to which class each region is associated to:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程可以迭代进行，结果超平面诱导的$\mathbb{R}^{n}$的多面体划分可以轻松转换为具有1个隐层和LTU激活函数的神经网络（详细信息见Bennett和Mangasarian
    ([1990](#bib.bib21))）。我们通过以下示例来说明这种转换：假设经过3次迭代后，我们得到了以下区域，箭头指示每个区域所属的类别：
- en: '|  |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\geq
    b_{1}\}\to Y,$ |  | (13a) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\geq
    b_{1}\}\to Y,$ |  | (13a) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\leq
    a_{1}\}\to N,$ |  | (13b) |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\leq
    a_{1}\}\to N,$ |  | (13b) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\geq
    b_{2}\}\to Y,$ |  | (13c) |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\geq
    b_{2}\}\to Y,$ |  | (13c) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\leq
    a_{2}\}\to N,$ |  | (13d) |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\leq
    a_{2}\}\to N,$ |  | (13d) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x\geq(a_{3}+b_{3})/2\}\to
    Y,$ |  | (13e) |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x\geq(a_{3}+b_{3})/2\}\to
    Y,$ |  | (13e) |'
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x<(a_{3}+b_{3})/2\}\to
    N.$ |  | (13f) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x<(a_{3}+b_{3})/2\}\to
    N.$ |  | (13f) |'
- en: 'Since regions ([13e](#S5.E13.5 "In 13 ‣ 5.1.2 LTU activations and variable
    number of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) and ([13f](#S5.E13.6 "In 13 ‣ 5.1.2 LTU activations
    and variable number of nodes ‣ 5.1 Training neural networks with a single hidden
    layer ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) are the last defined by the algorithm (under
    some stopping criterion), they both use $(a_{3}+b_{3})/2$ in order to obtain a
    well-defined partition of $\mathbb{R}^{n}$. In Figure [11](#S5.F11 "Figure 11
    ‣ 5.1.2 LTU activations and variable number of nodes ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") we show a one-hidden-layer
    neural network that represents such a classifier. The structure of the neural
    network can be easily extended to handle more regions.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 由于区域（[13e](#S5.E13.5 "在13 ‣ 5.1.2 LTU激活和可变节点数 ‣ 5.1 训练单隐层神经网络 ‣ 5 线性规划和多面体理论在训练中的应用
    ‣ 当深度学习遇上多面体理论：调查")）和（[13f](#S5.E13.6 "在13 ‣ 5.1.2 LTU激活和可变节点数 ‣ 5.1 训练单隐层神经网络
    ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：调查")）是由算法在某些停止准则下最后定义的，它们都使用$(a_{3}+b_{3})/2$以获得一个定义良好的$\mathbb{R}^{n}$划分。在图[11](#S5.F11
    "图11 ‣ 5.1.2 LTU激活和可变节点数 ‣ 5.1 训练单隐层神经网络 ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：调查")中，我们展示了一个表示这种分类器的单隐层神经网络。这个神经网络的结构可以很容易地扩展以处理更多的区域。
- en: <svg   height="226.19" overflow="visible" version="1.1" width="260.93"><g transform="translate(0,226.19)
    matrix(1 0 0 -1 0 0) translate(12.28,0) translate(0,113.19)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -14.98)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 25.435)"><g transform="matrix(1
    0 0 1 0 20.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 14.685)"><g  transform="matrix(1
    0 0 1 0 8.45)"><g  transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0
    0 -1 0 4.225)"><g transform="matrix(1 0 0 1 0 5.96)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="11.78"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{1}$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 18.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 26.87)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="14.77" height="9.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{n_{0}}$</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 94.87)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 -21.42)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 109.73 -62.25)" fill="#000000" stroke="#000000"><foreignobject
    width="22.35" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{a_{3}+b_{3}}{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 110.13 -101.62)" fill="#000000" stroke="#000000"><foreignobject
    width="21.27" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{-a_{3}-b_{3}}{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 232.76 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.52 38.54)" fill="#000000"
    stroke="#000000"><foreignobject width="14.15" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 48.27 17.71)" fill="#000000" stroke="#000000"><foreignobject width="18.77"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.93 -1.46)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 47.5 -20.6)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.38 -40.35)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.16 -60.74)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.59 31.95)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 169.9 11.96)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.82 -6.91)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.1 -26.06)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 174.88 -43.02)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.73 -62.57)" fill="#000000" stroke="#000000"><foreignobject
    width="14.61" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1$</foreignobject></g></g></g></svg>
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="226.19" overflow="visible" version="1.1" width="260.93"><g transform="translate(0,226.19)
    matrix(1 0 0 -1 0 0) translate(12.28,0) translate(0,113.19)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -14.98)" fill="#000000"
    stroke="#000000"><g  transform="matrix(1 0 0 -1 0 25.435)"><g transform="matrix(1
    0 0 1 0 20.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g transform="matrix(1 0 0 -1 0 14.685)"><g  transform="matrix(1
    0 0 1 0 8.45)"><g  transform="matrix(1 0 0 -1 0 0)"><g  transform="matrix(1 0
    0 -1 0 4.225)"><g transform="matrix(1 0 0 1 0 5.96)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="11.78"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{1}$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 18.14)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="10.38" height="12.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g></g></g></g></g><g
    transform="matrix(1 0 0 1 0 26.87)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r"
    transform="matrix(1 0 0 -1 0 0)"><foreignobject width="14.77" height="9.05" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{n_{0}}$</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 94.87)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 -21.42)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 109.73 -62.25)" fill="#000000" stroke="#000000"><foreignobject
    width="22.35" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{a_{3}+b_{3}}{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 110.13 -101.62)" fill="#000000" stroke="#000000"><foreignobject
    width="21.27" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{-a_{3}-b_{3}}{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 232.76 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.52 38.54)" fill="#000000"
    stroke="#000000"><foreignobject width="14.15" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 48.27 17.71)" fill="#000000" stroke="#000000"><foreignobject width="18.77"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.93 -1.46)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 47.5 -20.6)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.38 -40.35)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.16 -60.74)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.59 31.95)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 169.9 11.96)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.82 -6.91)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2
- en: 'Figure 11: Illustration of Neural Network with LTU activations using MSMT.
    Inside each node of the hidden layer, we show the thresholds used in each LTU
    activation.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：使用 MSMT 的 LTU 激活神经网络的示意图。每个隐藏层节点内，我们展示了在每个 LTU 激活中使用的阈值。
- en: For other details, we refer the reader to Bennett ([1992](#bib.bib20)), and
    for variants and extensions see Mangasarian ([1993](#bib.bib209)) and references
    therein.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他详细信息，请参见 Bennett ([1992](#bib.bib20))，关于变体和扩展请参见 Mangasarian ([1993](#bib.bib209))
    及其中的参考文献。
- en: 'Some key features of this procedure are the following:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的一些关键特性如下：
- en: •
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Each solution of ([12](#S5.E12 "In 5.1.2 LTU activations and variable number
    of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")), i.e., each new hyperplane, can be represented as a new node
    in the hidden layer of the resulting neural network.'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ([12](#S5.E12 "在 5.1.2 LTU 激活和节点数量可变 ‣ 5.1 使用单隐藏层训练神经网络 ‣ 5 线性规划和训练中的多面体理论 ‣
    当深度学习遇到多面体理论：综述")) 的每一个解，即每一个新的超平面，都可以表示为结果神经网络中隐藏层的新节点。
- en: •
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The addition of a new hyperplane comes with a reduction in the current loss;
    this can be iterated until a target loss is met.
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新的超平面的加入会导致当前损失的减少；这个过程可以迭代，直到满足目标损失。
- en: •
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Thanks to the universal approximation theorem (Hornik et al., [1989](#bib.bib153)),
    with enough nodes in the hidden layer, we can always obtain a neural network $\hat{f}$
    with zero classification error. Although this can lead to over-fitting.
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 感谢通用逼近定理（Hornik 等人，[1989](#bib.bib153)），只要隐藏层节点足够多，我们总是能得到一个分类错误为零的神经网络 $\hat{f}$。虽然这可能导致过拟合。
- en: 'The work of Roy et al. ([1993](#bib.bib264)) and Mukhopadhyay et al. ([1993](#bib.bib227))
    follow a related idea, although the classifiers which are built are quadratic
    functions. To illustrate the approach, we use the same set-up for ([12](#S5.E12
    "In 5.1.2 LTU activations and variable number of nodes ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). The approach in Roy
    et al. ([1993](#bib.bib264)) and Mukhopadhyay et al. ([1993](#bib.bib227)) aims
    at finding a function'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Roy 等人 ([1993](#bib.bib264)) 和 Mukhopadhyay 等人 ([1993](#bib.bib227)) 的工作遵循了相关的思路，尽管他们构建的分类器是二次函数。为了说明这种方法，我们使用了相同的设置
    ([12](#S5.E12 "在 5.1.2 LTU 激活和节点数量可变 ‣ 5.1 使用单隐藏层训练神经网络 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述"))。Roy
    等人 ([1993](#bib.bib264)) 和 Mukhopadhyay 等人 ([1993](#bib.bib227)) 的方法旨在找到一个函数
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(x)={\bm{x}}^{\top}{\bm{V}}{\bm{x}}+{\bm{w}}^{\top}{\bm{x}}+b$
    |  |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{{\bm{V}},{\bm{w}},b}(x)={\bm{x}}^{\top}{\bm{V}}{\bm{x}}+{\bm{w}}^{\top}{\bm{x}}+b$
    |  |'
- en: such that
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使得
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(\tilde{{\bm{x}}}_{i})\geq 0\Longleftrightarrow
    i\in Y.$ |  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{{\bm{V}},{\bm{w}},b}(\tilde{{\bm{x}}}_{i})\geq 0\Longleftrightarrow
    i\in Y.$ |  |'
- en: Since this may not be possible, the authors propose solving the following LP
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这可能不可行，作者提出了解决以下线性规划（LP）的问题
- en: '|  |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  | $\displaystyle\min_{W,w,b,\epsilon}\quad$ | $\displaystyle\epsilon$ |  |
    (14a) |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{W,w,b,\epsilon}\quad$ | $\displaystyle\epsilon$ |  |
    (14a) |'
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\geq\epsilon$
    | $\displaystyle\forall i\in Y$ |  | (14b) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\geq\epsilon$
    | $\displaystyle\forall i\in Y$ |  | (14b) |'
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\leq-\epsilon$
    | $\displaystyle\forall i\not\in Y$ |  | (14c) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\leq-\epsilon$
    | $\displaystyle\forall i\not\in Y$ |  | (14c) |'
- en: '|  |  | $\displaystyle\epsilon\geq\epsilon_{0}$ |  | (14d) |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\epsilon\geq\epsilon_{0}$ |  | (14d) |'
- en: 'for some fixed tolerance $\epsilon_{0}>0$. When this LP is infeasible, the
    class $Y$ is partitioned into $Y_{1}$ and $Y_{2}$, and an LP as ([14](#S5.E14
    "In 5.1.2 LTU activations and variable number of nodes ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) is solved for both $Y_{1}$
    and $Y_{2}$. The algorithm then follows iteratively (see below for comments on
    these iterations). In the end, the algorithm will construct $k$ quadratic functions
    $f_{1},\ldots,f_{k}$, which the authors call “masking functions”, that will classify
    an input ${\bm{x}}$ in the class $Y$ if and only if'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某个固定的容差 $\epsilon_{0}>0$。当这个线性规划（LP）不可行时，类别 $Y$ 被划分为 $Y_{1}$ 和 $Y_{2}$，并对
    $Y_{1}$ 和 $Y_{2}$ 求解如 ([14](#S5.E14 "在 5.1.2 LTU 激活和节点数量变化 ‣ 5.1 单隐层神经网络的训练 ‣
    5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：调查")) 的线性规划。然后算法会以迭代的方式继续（参见下文对这些迭代的评论）。最后，算法将构造
    $k$ 个二次函数 $f_{1},\ldots,f_{k}$，这些函数被作者称为“掩码函数”，它们会对输入 ${\bm{x}}$ 进行分类，当且仅当
- en: '|  | $\exists i\in[k],\,f_{i}({\bm{x}})\geq 0.$ |  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | $\exists i\in[k],\,f_{i}({\bm{x}})\geq 0.$ |  |'
- en: 'In order to represent the resulting classifier as a single-layer neural network,
    the authors proceed in a similar manner to a linear classifier; the input layer
    of the resulting neural network not only includes each entry of ${\bm{x}}$, but
    also the bilinear terms ${\bm{x}}{\bm{x}}^{\top}$. Using this input, the classifier
    built by ([14](#S5.E14 "In 5.1.2 LTU activations and variable number of nodes
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be thought of as a linear classifier (much like a polynomial regression
    can be cast as a linear regression).'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将得到的分类器表示为单层神经网络，作者以类似于线性分类器的方式进行处理；得到的神经网络的输入层不仅包括 ${\bm{x}}$ 的每个条目，还包括双线性项
    ${\bm{x}}{\bm{x}}^{\top}$。利用这些输入，由 ([14](#S5.E14 "在 5.1.2 LTU 激活和节点数量变化 ‣ 5.1
    单隐层神经网络的训练 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：调查")) 构建的分类器可以被视为线性分类器（就像多项式回归可以被看作线性回归一样）。
- en: 'As a last comment on the work of Roy et al. ([1993](#bib.bib264)) and Mukhopadhyay
    et al. ([1993](#bib.bib227)), the authors’ algorithm does not iterate in a straightforward
    fashion. They add clustering iterations alternating with the steps described above
    in order to (a) identify outliers and remove them from the training set, and (b)
    subdivide the training data when ([14](#S5.E14 "In 5.1.2 LTU activations and variable
    number of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is infeasible. These additions allow them to obtain
    a polynomial-time algorithm.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Roy 等（[1993](#bib.bib264)）和 Mukhopadhyay 等（[1993](#bib.bib227)）的工作，作为最后的评论，作者的算法并不是简单地进行迭代。他们通过添加聚类迭代并与上述步骤交替进行，以（a）识别异常值并将其从训练集中移除，以及（b）当
    ([14](#S5.E14 "在 5.1.2 LTU 激活和节点数量变化 ‣ 5.1 单隐层神经网络的训练 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：调查"))
    不可行时细分训练数据。这些附加步骤使他们能够获得一个多项式时间的算法。
- en: The methods described in this section are able to produce a neural network with
    arbitrary quality, however, there is no guarantee on the size of the resulting
    neural network. When the size of the network is fixed the story changes, which
    is the case we describe next.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中描述的方法能够生成具有任意质量的神经网络，但对于生成的神经网络的规模没有保证。当网络规模固定时情况会有所不同，这正是我们接下来要描述的情况。
- en: 5.1.3 Fixed number of nodes and ReLU activations
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 固定节点数和 ReLU 激活
- en: As mentioned at the beginning of this section, training a neural network is
    a complex optimization problem in general, with some results indicating that the
    problem is likely to not even be in NP (Abrahamsen et al., [2021](#bib.bib1),
    Bertschinger et al., [2022](#bib.bib27)).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节开头提到的，训练神经网络通常是一个复杂的优化问题，一些研究结果表明这个问题可能甚至不属于 NP 类（Abrahamsen 等，[2021](#bib.bib1)，Bertschinger
    等，[2022](#bib.bib27)）。
- en: Nonetheless, by restricting the network architecture sufficiently and allowing
    exponential running times, exact algorithms can be conceived. An important step
    in the construction of such algorithms was taken by Arora et al. ([2018](#bib.bib8)).
    In this work, the authors studied the training problem in detail, providing the
    first optimization algorithm capable of solving the training problem to provable
    optimality for a fixed network architecture with one hidden layer and with an
    output dimension of 1\. As we anticipated, this algorithm shares some similarities
    with the previous approach.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，通过充分限制网络架构并允许指数级运行时间，可以设想到精确算法。Arora 等人 ([2018](#bib.bib8))在构建这些算法时迈出了重要的一步。在这项工作中，作者详细研究了训练问题，提供了第一个能够解决训练问题并证明最优的优化算法，适用于具有一个隐藏层和输出维度为1的固定网络架构。正如我们预期的，这个算法与之前的方法有一些相似之处。
- en: Let us consider now a ReLU activation. Also, we no longer assume $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$,
    but we keep the output dimension as 1\. The problem considered by Arora et al.
    ([2018](#bib.bib8)) reads
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们考虑一个ReLU激活。同时，我们不再假设$\tilde{{\bm{y}}}_{i}\in\{-1,1\}$，但我们保持输出维度为1。Arora
    等人 ([2018](#bib.bib8))考虑的问题是
- en: '|  | $\min_{{\bm{W}},{\bm{b}}}\frac{1}{D}\sum_{i=1}^{D}\ell({\bm{W}}^{2}(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i}),$
    |  | (15) |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}},{\bm{b}}}\frac{1}{D}\sum_{i=1}^{D}\ell({\bm{W}}^{2}(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i}),$
    |  | (15) |'
- en: with $\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$ a convex loss. Note that
    this problem, even if $\ell$ is convex, is a non-convex optimization problem.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$是一个凸损失。请注意，即使$\ell$是凸的，这个问题也是一个非凸优化问题。
- en: Theorem 1 (Arora et al. ([2018](#bib.bib8)))
  id: totrans-440
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1（Arora 等人 ([2018](#bib.bib8))）
- en: 'Let $n_{1}$ be the number of nodes in the hidden layer. There exists an algorithm
    to find a global optimum of ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and
    ReLU activations ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) in time $O(2^{n_{1}}D^{{n_{0}}\cdot n_{1}}\text{poly}(D,{n_{0}},n_{1}))$.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 设$n_{1}$为隐藏层中的节点数量。存在一种算法可以在时间$O(2^{n_{1}}D^{{n_{0}}\cdot n_{1}}\text{poly}(D,{n_{0}},n_{1}))$中找到（[15](#S5.E15
    "在5.1.3 固定节点数量和ReLU激活 ‣ 5.1 训练具有单一隐藏层的神经网络 ‣ 5 线性规划与训练中的多面体理论 ‣ 当深度学习遇上多面体理论：调查")）的全局最优解。
- en: 'Roughly speaking, the algorithm works by noting that one can assume the weights
    in ${\bm{W}}^{2}$ are in $\{-1,1\}$, since $\sigma$ is positively-homogeneous.
    Thus, problem ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and ReLU activations
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be restated as'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略地说，该算法的工作原理是通过注意到可以假设${\bm{W}}^{2}$中的权重在$\{-1,1\}$中，因为$\sigma$是正齐次的。因此，问题（[15](#S5.E15
    "在5.1.3 固定节点数量和ReLU激活 ‣ 5.1 训练具有单一隐藏层的神经网络 ‣ 5 线性规划与训练中的多面体理论 ‣ 当深度学习遇上多面体理论：调查")）可以重新表述为
- en: '|  | $\min_{{\bm{W}}^{1},{\bm{b}}^{1},s}\frac{1}{D}\sum_{i=1}^{D}\ell(s(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i})$
    |  | (16) |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}}^{1},{\bm{b}}^{1},s}\frac{1}{D}\sum_{i=1}^{D}\ell(s(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i})$
    |  | (16) |'
- en: 'where $s\in\{-1,1\}^{n_{1}}$. In order to handle the non-linearity, Arora et al.
    ([2018](#bib.bib8)) “guess” the values of $s$ and the sign of each component of
    ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Enforcing a sign for each component
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$ is similar to the approach
    discussed in the previous section: it fixes how the input part of the data $(\tilde{{\bm{x}}}_{i})_{i=1}^{D}$
    is partitioned in polyhedral regions by a number of hyperplanes. The difference
    is that, in this case, the number of hyperplanes to be used is assumed to be fixed.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$s\in\{-1,1\}^{n_{1}}$。为了处理非线性，Arora 等人 ([2018](#bib.bib8))“猜测”$s$的值和${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$每个分量的符号。为${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$的每个分量强制一个符号类似于前一节讨论的方法：它固定了数据输入部分$(\tilde{{\bm{x}}}_{i})_{i=1}^{D}$如何通过多个超平面划分成多面体区域。不同之处在于，在这种情况下，假设使用的超平面数量是固定的。
- en: 'Using the hyperplane arrangement theorem (see e.g. (Matousek, [2002](#bib.bib215),
    Proposition 6.1.1)), there are at most $D^{{n_{0}}n_{1}}$ ways of fixing the signs
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Additionally, there are at
    most $2^{n_{1}}$ possible vectors in $\{-1,1\}^{n_{1}}$. Once these components
    are fixed, ([16](#S5.E16 "In 5.1.3 Fixed number of nodes and ReLU activations
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be solved as an optimization problem with a convex objective function
    and a polyhedral feasible region imposing the desired signs in ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$.
    This results in the $O(2^{n_{1}}D^{{n_{0}}n_{1}}\text{poly}(D,{n_{0}},n_{1}))$
    running time. This algorithm was recently generalized to concave loss functions
    by Froese et al. ([2022](#bib.bib110)).'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '使用超平面排列定理（见例如（Matousek，[2002](#bib.bib215)，命题 6.1.1)），固定 ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$
    的符号最多有 $D^{{n_{0}}n_{1}}$ 种方式。此外，在 $\{-1,1\}^{n_{1}}$ 中最多有 $2^{n_{1}}$ 个可能的向量。一旦这些组件固定后，（[16](#S5.E16
    "In 5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")）可以作为一个具有凸目标函数和多面体可行区域的优化问题来解决，该区域强加了
    ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$ 中所需的符号。这导致了 $O(2^{n_{1}}D^{{n_{0}}n_{1}}\text{poly}(D,{n_{0}},n_{1}))$
    的运行时间。Froese 等人（[2022](#bib.bib110)）最近将该算法推广到了凹损失函数的情况。'
- en: Dey et al. ([2020](#bib.bib81)) developed a polynomial-time approximation algorithm
    in this setting for the case of $n_{1}=1$ (i.e., one ReLU neuron) and square loss.
    This approximation algorithm has a better performance when the input dimension
    is much larger than the sample size, i.e. ${n_{0}}\gg D$. The approach by Dey
    et al. ([2020](#bib.bib81)) also relies on fixing the signs of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$,
    and then solving multiple convex optimization problems, but in different strategy
    than that of Arora et al. ([2018](#bib.bib8)); in particular, Dey et al. ([2020](#bib.bib81))
    only explore a polynomial number of the possible “fixings”, which yields the approximation.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: Dey 等人（[2020](#bib.bib81)）在这种情况下为 $n_{1}=1$（即一个 ReLU 神经元）和平方损失开发了一种多项式时间的近似算法。当输入维度远大于样本大小，即
    ${n_{0}}\gg D$ 时，该近似算法的表现更佳。Dey 等人（[2020](#bib.bib81)）的方法也依赖于固定 ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$
    的符号，然后解决多个凸优化问题，但策略不同于 Arora 等人（[2018](#bib.bib8)）的方法；特别是，Dey 等人（[2020](#bib.bib81)）只探索了多项式数量的可能“固定”，从而得出近似结果。
- en: 'We note that the result by Arora et al. ([2018](#bib.bib8)) shows that the
    training problem on their architecture is in NP. This is in contrast to Bertschinger
    et al. ([2022](#bib.bib27)), who show that training a neural network with one
    hidden layer is likely to not be in NP. The big difference lies in the assumption
    on the output dimension: in the case of Bertschinger et al. ([2022](#bib.bib27)),
    the output dimension is two. It is quite remarkable that such a sharp complexity
    gap is produced by a small change in the output dimension.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，Arora 等人（[2018](#bib.bib8)）的结果显示，他们的架构上的训练问题属于 NP 类。这与 Bertschinger 等人（[2022](#bib.bib27)）的研究形成对比，后者显示训练一个具有隐藏层的神经网络可能不属于
    NP。主要差异在于输出维度的假设：在 Bertschinger 等人（[2022](#bib.bib27)）的情况下，输出维度为二。如此小的输出维度变化竟然产生了如此显著的复杂性差距，实在是非常引人注目。
- en: 5.1.4 An exact training algorithm for arbitrary LTU architectures
  id: totrans-448
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 任意 LTU 架构的精确训练算法
- en: Recently, Khalife and Basu ([2022](#bib.bib173)) presented a new algorithm,
    akin to that in Arora et al. ([2018](#bib.bib8)), capable of solving the training
    problem to global optimality for any fixed LTU architecture with a convex loss
    function $\ell$. In this case, no assumption on the network’s depth is made. The
    algorithm runs in polynomial time on the sample size $D$ when the architecture
    is fixed.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Khalife 和 Basu（[2022](#bib.bib173)）提出了一种新算法，类似于 Arora 等人（[2018](#bib.bib8)）的方法，能够在任何固定的
    LTU 架构下对具有凸损失函数 $\ell$ 的训练问题求解至全局最优。在这种情况下，不对网络的深度做任何假设。该算法在架构固定时，样本大小 $D$ 上运行时间为多项式时间。
- en: We will not describe this approach in detail, as it heavily relies on the structure
    given by LTU activations, which is intricate and beyond the scope of this survey.
    Although we note that it shares some high-level similarities to the algorithm
    of Arora et al. ([2018](#bib.bib8)) for ReLU activations, such as “guessing” the
    behavior of the neurons’ activity and then solving multiple convex optimization
    problems. However, the structural and algorithmic details are considerably different.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细描述这种方法，因为它严重依赖于LTU激活函数所提供的结构，这一结构复杂且超出了本调查的范围。尽管我们注意到，它在高层次上与Arora等人（[2018](#bib.bib8)）针对ReLU激活函数的算法存在一些相似之处，例如“猜测”神经元活动的行为，然后解决多个凸优化问题。然而，结构和算法细节有很大不同。
- en: It is important to note that this result reveals the big gap between what is
    known for LTU versus ReLU activations in terms of their training problems. In
    the case of the former, there is an exact algorithm for arbitrary architectures;
    in the case of the latter, the known results are much more restricted and strong
    computational limitations exist.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，这一结果揭示了LTU和ReLU激活函数在其训练问题上的巨大差距。在前者的情况下，有一个针对任意架构的精确算法；而在后者的情况下，已知结果则受到更大限制，并且存在强大的计算限制。
- en: 5.2 Convex reformulations in regularized training problems
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 正则化训练问题中的凸重构
- en: 'For the case when the training problem is regularized, the following stream
    of work has developed several convex reformulations of it. Pilanci and Ergen ([2020](#bib.bib248))
    presented the first convex reformulation of a training problem for the case with
    one hidden layer and one-dimensional outputs. As the approach described in Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training
    neural networks with a single hidden layer ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), this
    reformulation uses hyperplane arrangements according to the activation patterns
    of the ReLU units, but instead of using them algorithmically directly, they use
    them to find their convex reformulations. This framework was further extended
    to CNNs by Ergen and Pilanci ([2021c](#bib.bib93)). Higher-dimensional outputs
    in neural networks with one hidden layer were considered in Ergen and Pilanci
    ([2020](#bib.bib90), [2021a](#bib.bib91)), Sahiner et al. ([2021](#bib.bib268)).
    This convex optimization perspective was also applied in Batch Normalization by
    Ergen et al. ([2022](#bib.bib96)).'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '对于正则化训练问题的情况，以下工作流开发了几个凸重构。Pilanci和Ergen（[2020](#bib.bib248)）首次提出了一个具有一个隐藏层和一维输出的训练问题的凸重构。正如第[5.1.3](#S5.SS1.SSS3
    "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")节中描述的方法，这种重构根据ReLU单元的激活模式使用超平面排列，但并不是直接算法使用它们，而是利用它们来寻找其凸重构。这个框架在Ergen和Pilanci（[2021c](#bib.bib93)）中被进一步扩展到卷积神经网络（CNNs）。Ergen和Pilanci（[2020](#bib.bib90)、[2021a](#bib.bib91)），Sahiner等（[2021](#bib.bib268)）考虑了具有一个隐藏层的神经网络中的高维输出。这种凸优化视角还被应用于批量归一化（Batch
    Normalization），由Ergen等（[2022](#bib.bib96)）完成。'
- en: These approaches provide polynomial-time algorithms when some parameters (e.g.,
    the input dimension ${n_{0}}$) are considered constant. We note that this does
    not contradict the hardness result of Froese and Hertrich ([2023](#bib.bib109)),
    as the latter does not include a regularizing term. We explain below where the
    regularizing term plays an important role. Training via convex optimization was
    further developed to handle deeper regularized neural networks in Ergen and Pilanci
    ([2021b](#bib.bib92), [d](#bib.bib94), [e](#bib.bib95)).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法在某些参数（例如输入维度${n_{0}}$）被视为常量时提供了多项式时间算法。我们注意到，这并不与Froese和Hertrich（[2023](#bib.bib109)）的困难结果相矛盾，因为后者没有包含正则化项。我们在下文中解释了正则化项的重要作用。通过凸优化进行训练的技术在Ergen和Pilanci（[2021b](#bib.bib92)、[d](#bib.bib94)、[e](#bib.bib95)）中得到了进一步的发展，以处理更深的正则化神经网络。
- en: In what follows, we review the convex reformulation in Pilanci and Ergen ([2020](#bib.bib248))
    (one hidden layer and one-dimensional output) to illustrate some of the base strategies
    behind these approaches. We refer the reader to the previously mentioned articles
    for the most recent and intricate developments, as well as numerical experiments.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们回顾了Pilanci和Ergen（[2020](#bib.bib248)）中的凸重构（一个隐藏层和一维输出），以展示这些方法背后的一些基础策略。我们建议读者参考前述文章，以获取最新的复杂发展和数值实验。
- en: As before, let $n_{1}$ be the number of nodes in the hidden layer. Let us consider
    the following regularized training problem; to simplify the discussion, we omit
    biases.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，设 $n_{1}$ 为隐层中的节点数。我们考虑以下正则化训练问题；为简化讨论，我们省略了偏置项。
- en: '|  | $\min_{{\bm{W}}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{\beta}{2}\sum_{j=1}^{n_{1}}(\&#124;{\bm{W}}^{1}_{j}\&#124;^{2}+({\bm{W}}^{2}_{j})^{2})$
    |  | (17) |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{\beta}{2}\sum_{j=1}^{n_{1}}(\&#124;{\bm{W}}^{1}_{j}\&#124;^{2}+({\bm{W}}^{2}_{j})^{2})$
    |  | (17) |'
- en: 'Here, $\beta>0$, $\tilde{{\bm{X}}}$ is a matrix whose $i$-th row is $\tilde{{\bm{x}}}_{i}$
    and ${\bm{W}}^{1}_{j}$ is the vector of weights going into neuron $j$. Thus, $\tilde{{\bm{X}}}{\bm{W}}^{1}_{j}$
    is a vector whose $i$-th component is the input to neuron $j$ when evaluating
    the network on $\tilde{{\bm{x}}}_{i}$. ${\bm{W}}^{2}_{j}$ is a scalar: it is the
    weight on the arc from neuron $j$ to the output neuron (one-dimensional). Note
    that there is a slight notation overload: $({\bm{W}}^{2}_{j})^{2}$ is the square
    of the scalar ${\bm{W}}^{2}_{j}$. However, we will quickly remove this (pontentially
    confusing) term.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\beta>0$，$\tilde{{\bm{X}}}$ 是一个矩阵，其第 $i$ 行是 $\tilde{{\bm{x}}}_{i}$，而 ${\bm{W}}^{1}_{j}$
    是进入神经元 $j$ 的权重向量。因此，$\tilde{{\bm{X}}}{\bm{W}}^{1}_{j}$ 是一个向量，其中第 $i$ 个分量是评估网络时神经元
    $j$ 的输入（对于 $\tilde{{\bm{x}}}_{i}$）。${\bm{W}}^{2}_{j}$ 是一个标量：它是从神经元 $j$ 到输出神经元（一维）的弧上的权重。注意，这里有一个轻微的符号过载：$({\bm{W}}^{2}_{j})^{2}$
    是标量 ${\bm{W}}^{2}_{j}$ 的平方。不过，我们将很快移除这个（可能令人困惑的）项。
- en: 'Problem ([17](#S5.E17 "In 5.2 Convex reformulations in regularized training
    problems ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is a regularized version of ([15](#S5.E15
    "In 5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) when $\ell$ is the squared
    difference. We modified its presentation to match the structure in Pilanci and
    Ergen ([2020](#bib.bib248)). The authors first prove that ([17](#S5.E17 "In 5.2
    Convex reformulations in regularized training problems ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is equivalent to'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 ([17](#S5.E17 "在 5.2 正则化训练问题的凸重构 ‣ 5 线性规划和多面体理论 ‣ 当深度学习遇上多面体理论：调查")) 是 ([15](#S5.E15
    "在 5.1.3 固定节点数和 ReLU 激活 ‣ 5.1 用单隐层训练神经网络 ‣ 5 线性规划和多面体理论 ‣ 当深度学习遇上多面体理论：调查")) 的正则化版本，其中
    $\ell$ 是平方差。我们修改了其呈现方式以匹配 Pilanci 和 Ergen ([2020](#bib.bib248)) 中的结构。作者首先证明 ([17](#S5.E17
    "在 5.2 正则化训练问题的凸重构 ‣ 5 线性规划和多面体理论 ‣ 当深度学习遇上多面体理论：调查")) 等价于
- en: '|  | $\min_{\&#124;{\bm{W}}^{1}_{j}\&#124;\leq 1}\min_{{\bm{W}}^{2}_{j}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{n_{1}}&#124;{\bm{W}}^{2}_{j}&#124;$
    |  |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\&#124;{\bm{W}}^{1}_{j}\&#124;\leq 1}\min_{{\bm{W}}^{2}_{j}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{n_{1}}&#124;{\bm{W}}^{2}_{j}&#124;$
    |  |'
- en: Then, through a series of reformulations and duality arguments, the authors
    first show that this problem is lower bounded by
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，通过一系列重构和对偶论证，作者首先展示了这个问题的下界为
- en: '|  |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\max$ | $\displaystyle\quad-\frac{1}{2}\left\&#124;v-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{1}{2}\&#124;\tilde{{\bm{y}}}\&#124;^{2}$
    |  | (18a) |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max$ | $\displaystyle\quad-\frac{1}{2}\left\&#124;v-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{1}{2}\&#124;\tilde{{\bm{y}}}\&#124;^{2}$
    |  | (18a) |'
- en: '|  | s.t | $\displaystyle\quad&#124;v^{\top}\sigma(\tilde{{\bm{X}}}u)&#124;\leq\beta$
    | $\displaystyle\forall u,\,\&#124;u\&#124;\leq 1$ |  | (18b) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t | $\displaystyle\quad&#124;v^{\top}\sigma(\tilde{{\bm{X}}}u)&#124;\leq\beta$
    | $\displaystyle\forall u,\,\&#124;u\&#124;\leq 1$ |  | (18b) |'
- en: '|  |  | $\displaystyle\quad v\in\mathbb{R}^{D}$ |  | (18c) |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad v\in\mathbb{R}^{D}$ |  | (18c) |'
- en: 'Problem ([18](#S5.E18 "In 5.2 Convex reformulations in regularized training
    problems ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) has $D$ variables and infinitely
    many constraints. The authors show that this lower bound is tight when the number
    of neurons in the hidden layer is large enough; specifically, they require $n_{1}\geq
    m^{*}$, where $m^{*}\in\{1,\ldots,D\}$ is defined as the number of Dirac deltas
    in an optimal solution of a dual of ([18](#S5.E18 "In 5.2 Convex reformulations
    in regularized training problems ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) (see Pilanci
    and Ergen ([2020](#bib.bib248)) for details).'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 问题（[18](#S5.E18 "在 5.2 节中，正则化训练问题的凸重构 ‣ 5 线性规划与训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述")）有
    $D$ 个变量和无限多个约束。作者展示了当隐藏层中的神经元数量足够大时，这个下界是紧的；具体而言，他们要求 $n_{1}\geq m^{*}$，其中 $m^{*}\in\{1,\ldots,D\}$
    定义为（[18](#S5.E18 "在 5.2 节中，正则化训练问题的凸重构 ‣ 5 线性规划与训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述")）的对偶问题中一个最优解中的
    Dirac δ 的数量（详细信息请参见 Pilanci 和 Ergen ([2020](#bib.bib248)））。
- en: 'Regarding the presence of infinitely many constraints, the authors address
    this by considering all possible patterns of signs of $\tilde{{\bm{X}}}u$ (similarly
    to Arora et al. ([2018](#bib.bib8)), as discussed in Section [5.1.3](#S5.SS1.SSS3
    "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). For each fixed sign
    pattern (hyperplane arrangement), they apply a duality argument which allows them
    to recast the constraint $\max_{u\in\mathcal{B}}|v^{\top}\sigma(\tilde{{\bm{X}}}u)|\leq\beta$
    as a finite collection of second-order cone constraints with $\beta$ on the right-hand
    side.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 关于无限多个约束的存在，作者通过考虑 $\tilde{{\bm{X}}}u$ 的所有可能的符号模式来解决这个问题（类似于 Arora 等人 ([2018](#bib.bib8)），如第
    [5.1.3](#S5.SS1.SSS3 "5.1.3 固定节点数和 ReLU 激活 ‣ 5.1 训练具有单个隐藏层的神经网络 ‣ 5 线性规划与训练中的多面体理论
    ‣ 当深度学习遇到多面体理论：综述") 节讨论）。对于每个固定的符号模式（超平面排列），他们应用对偶性论证，从而将约束 $\max_{u\in\mathcal{B}}|v^{\top}\sigma(\tilde{{\bm{X}}}u)|\leq\beta$
    转化为右侧带有 $\beta$ 的二阶锥约束的有限集合。
- en: 'Finally, using that $\beta>0$, they show that the reformulated problem satisfies
    Slater’s condition, and thus from strong duality they obtain the following convex
    optimization problem, which has the same objective value as ([18](#S5.E18 "In
    5.2 Convex reformulations in regularized training problems ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")).'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，利用 $\beta>0$，他们展示了重构问题满足 Slater 条件，因此通过强对偶性，他们得到以下凸优化问题，其目标值与（[18](#S5.E18
    "在 5.2 节中，正则化训练问题的凸重构 ‣ 5 线性规划与训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述")）相同。
- en: '|  |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\min$ | $\displaystyle\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{P}M_{i}\tilde{{\bm{X}}}(v_{i}-w_{i})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{P}(\&#124;v_{i}\&#124;+\&#124;w_{i}\&#124;)$
    |  | (19a) |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min$ | $\displaystyle\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{P}M_{i}\tilde{{\bm{X}}}(v_{i}-w_{i})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{P}(\&#124;v_{i}\&#124;+\&#124;w_{i}\&#124;)$
    |  | (19a) |'
- en: '|  | s.t | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}v_{i}\geq 0$ |
    $\displaystyle\forall i\in[P]$ |  | (19b) |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}v_{i}\geq 0$ |
    $\displaystyle\forall i\in[P]$ |  | (19b) |'
- en: '|  |  | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}w_{i}\geq 0$ | $\displaystyle\forall
    i\in[P]$ |  | (19c) |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}w_{i}\geq 0$ | $\displaystyle\forall
    i\in[P]$ |  | (19c) |'
- en: '|  |  | $\displaystyle\quad v_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19d) |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad v_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19d) |'
- en: '|  |  | $\displaystyle\quad w_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19e) |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\quad w_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19e) |'
- en: 'Here, $I_{D}$ is the $D\times D$ identity matrix, $P$ is the number of possible
    activation patterns for $\tilde{{\bm{X}}}$, and each $M_{i}$ is a $D\times D$
    binary diagonal matrix whose diagonal indicates the $i$-th possible sign pattern
    of $\tilde{{\bm{X}}}u$. This means that $(M_{i})_{j,j}$ is 1 if and only if $\tilde{{\bm{x}}}_{j}^{\top}u\geq
    0$ in the $i$-th sign pattern of $\tilde{{\bm{X}}}u$. Moreover, the authors provide
    a formula to recover a solution of ([17](#S5.E17 "In 5.2 Convex reformulations
    in regularized training problems ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) from a solution
    of ([19](#S5.E19 "In 5.2 Convex reformulations in regularized training problems
    ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")).'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$I_{D}$ 是 $D\times D$ 单位矩阵，$P$ 是 $\tilde{{\bm{X}}}$ 的可能激活模式的数量，每个 $M_{i}$
    是一个 $D\times D$ 的二进制对角矩阵，其对角线指示 $\tilde{{\bm{X}}}u$ 的第 $i$ 个可能符号模式。这意味着 $(M_{i})_{j,j}$
    为 1 当且仅当 $\tilde{{\bm{x}}}_{j}^{\top}u\geq 0$ 在 $\tilde{{\bm{X}}}u$ 的第 $i$ 个符号模式中。此外，作者提供了一个公式，用以从
    ([19](#S5.E19 "在 5.2 节 正则化训练问题中的凸重构 ‣ 5 线性规划与多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述"))
    的解恢复 ([17](#S5.E17 "在 5.2 节 正则化训练问题中的凸重构 ‣ 5 线性规划与多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述"))
    的解。
- en: 'Using that $P\leq 2r(e(D-1)/r)^{r}$, where $r=\mbox{rank}(\tilde{{\bm{X}}})$,
    the authors note that the formulation ([19](#S5.E19 "In 5.2 Convex reformulations
    in regularized training problems ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) yields a
    training algorithm with complexity $O({n_{0}}^{3}r^{3}(D/r)^{3r})$. Note that
    if one fixes $r$, the resulting algorithm runs polynomial time. In particular,
    fixing ${n_{0}}$ fixes the rank of $\tilde{{\bm{X}}}$ and results in a polynomial
    time algorithm as well. In contrast, the algorithm by Arora et al. ([2018](#bib.bib8))
    discussed in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU
    activations ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") remains exponential even after fixing ${n_{0}}$. Moreover,
    Froese and Hertrich ([2023](#bib.bib109)) showed that the training problem is
    NP-Hard even for fixed ${n_{0}}$. This apparent contradiction is explained by
    two key components of the convex reformulation: the regularization term and the
    presence of a “large enough” number of hidden neurons. This facilitates the exponential
    improvement of the training algorithm with respect to Arora et al. ([2018](#bib.bib8)).'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 $P\leq 2r(e(D-1)/r)^{r}$，其中 $r=\mbox{rank}(\tilde{{\bm{X}}})$，作者指出，公式 ([19](#S5.E19
    "在 5.2 节 正则化训练问题中的凸重构 ‣ 5 线性规划与多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述")) 产生了一个复杂度为 $O({n_{0}}^{3}r^{3}(D/r)^{3r})$
    的训练算法。请注意，如果固定 $r$，得到的算法在多项式时间内运行。特别地，固定 ${n_{0}}$ 会固定 $\tilde{{\bm{X}}}$ 的秩，从而得到一个多项式时间的算法。相比之下，Arora
    等人 ([2018](#bib.bib8)) 在 [5.1.3](#S5.SS1.SSS3 "5.1.3 节 固定节点数和 ReLU 激活 ‣ 5.1 单隐层神经网络的训练
    ‣ 5 线性规划与多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述") 讨论的算法即使在固定 ${n_{0}}$ 后仍然是指数级的。此外，Froese
    和 Hertrich ([2023](#bib.bib109)) 显示，即使对于固定的 ${n_{0}}$，训练问题也是 NP-困难的。这一显著的矛盾可以通过凸重构的两个关键组件来解释：正则化项和“足够大”的隐藏神经元数量。这有助于训练算法相对于
    Arora 等人 ([2018](#bib.bib8)) 的指数级改进。
- en: 5.3 Frank-Wolfe in DNN training algorithms
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 Frank-Wolfe 在 DNN 训练算法中的应用
- en: Another stream of work that has included components of linear programming in
    DNN training involves the Frank-Wolfe method. We briefly describe this method
    in the non-stochastic version next. In this section, we omit the biases ${\bm{b}}$
    to simplify the notation.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类包含线性规划组件的 DNN 训练研究涉及 Frank-Wolfe 方法。我们接下来简要描述该方法的非随机版本。在本节中，我们省略偏差 ${\bm{b}}$
    以简化表示。
- en: Gradient descent (and its variants) is designed for problems of the form
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降（及其变体）旨在解决形式为
- en: '|  | $\min_{{\bm{W}}\in\mathbb{R}^{N}}\mathcal{L}({\bm{W}})$ |  | (20) |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{{\bm{W}}\in\mathbb{R}^{N}}\mathcal{L}({\bm{W}})$ |  | (20) |'
- en: and it is based on iterations of the form
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 并且基于以下形式的迭代
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))$ |  |
    (21) |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))$ |  |
    (21) |'
- en: where $\alpha_{i}$ is known as the *learning rate*. In the stochastic versions,
    $\nabla\mathcal{L}({\bm{W}}(i))$ is replaced by a stochastic gradient. In this
    setting, these algorithms would find a local minimum, which is global when $\mathcal{L}$
    is convex.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha_{i}$被称为*学习率*。在随机版本中，$\nabla\mathcal{L}({\bm{W}}(i))$被一个随机梯度所替代。在这种情况下，这些算法将找到局部最小值，当$\mathcal{L}$是凸的时，这个局部最小值也是全局最小值。
- en: 'In the presence of constraints ${\bm{W}}\in\Theta$, however, this strategy
    may not work directly. A regularizing term is typically used in the objective
    function instead of a constraint, that “encourages” ${\bm{W}}\in\Theta$ but does
    not enforce it. If we strictly require that ${\bm{W}}\in\Theta\neq\mathbb{R}^{n}$,
    and $\Theta$ is a convex set, one could modify ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，在存在约束${\bm{W}}\in\Theta$的情况下，这种策略可能无法直接奏效。目标函数中通常使用一个正则项来“鼓励”${\bm{W}}\in\Theta$，而不是强制要求。如果我们严格要求${\bm{W}}\in\Theta\neq\mathbb{R}^{n}$，且$\Theta$是一个凸集，可以将([21](#S5.E21
    "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey"))修改为'
- en: '|  | ${\bm{W}}(i+1)=\text{proj}_{\Theta}\left({\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))\right).$
    |  | (22) |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)=\text{proj}_{\Theta}\left({\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))\right).$
    |  | (22) |'
- en: and thus ensure that all iterates ${\bm{W}}(i)\in\Theta$. Unfortunately, a projection
    is a costly routine. An alternative to this projection is the Frank-Wolfe method
    (Frank et al., [1956](#bib.bib108)). Here, a direction ${\bm{d}}_{i}$ is computed
    via the following linear-objective convex optimization problem
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 从而确保所有迭代${\bm{W}}(i)\in\Theta$。不幸的是，投影是一个成本高昂的过程。投影的替代方法是Frank-Wolfe方法（Frank等，[1956](#bib.bib108)）。在这里，通过以下线性目标凸优化问题计算方向${\bm{d}}_{i}$
- en: '|  | ${\bm{d}}_{i}\in\arg\min_{{\bm{d}}\in\Theta}{\bm{v}}_{i}^{\top}{\bm{d}}$
    |  | (23) |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{d}}_{i}\in\arg\min_{{\bm{d}}\in\Theta}{\bm{v}}_{i}^{\top}{\bm{d}}$
    |  | (23) |'
- en: where normally ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ (we consider variants
    below). The update is then computed as
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 其中通常${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$（我们将在下面考虑变体）。然后，更新计算为
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)+\alpha_{i}({\bm{d}}_{i}-{\bm{W}}(i)),$ |  |
    (24) |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)+\alpha_{i}({\bm{d}}_{i}-{\bm{W}}(i)),$ |  |
    (24) |'
- en: 'for $\alpha_{i}\in[0,1]$. Note that, by convexity, we are assured that ${\bm{W}}(i+1)\in\Theta$
    as long as ${\bm{W}}(0)\in\Theta$. In many applications, $\Theta$ is polyhedral,
    which makes ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) a linear program. Moreover, for simple sets $\Theta$, problem
    ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) admits closed-form solutions.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '对于$\alpha_{i}\in[0,1]$。请注意，由于凸性，只要${\bm{W}}(0)\in\Theta$，我们可以确保${\bm{W}}(i+1)\in\Theta$。在许多应用中，$\Theta$是多面体的，这使得([23](#S5.E23
    "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey"))成为一个线性规划问题。此外，对于简单的集合$\Theta$，问题([23](#S5.E23
    "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey"))存在闭式解。'
- en: 'In the context of deep neural network training, two notable applications of
    Frank-Wolfe have appeared. Firstly, the Deep Frank Wolfe algorithm, by Berrada
    et al. ([2018](#bib.bib26)), which modifies iteration ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) with an optimization
    problem that can be solved using Frank-Wolfe in its dual. Secondly, the use of
    a stochastic version of Frank-Wolfe in the training problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) by Pokutta et al. ([2020](#bib.bib249)) and Xie
    et al. ([2020a](#bib.bib342)), which enforces structure in the neural network
    weights directly. We review these next, starting with the latter.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络训练的背景下，出现了两个显著的Frank-Wolfe应用。首先是 Berrada 等人 ([2018](#bib.bib26)) 提出的
    Deep Frank Wolfe 算法，该算法通过一个可以使用其对偶问题的Frank-Wolfe解决的优化问题来修改迭代 ([21](#S5.E21 "在5.3
    Frank-Wolfe在DNN训练算法中 ‣ 5 线性规划与多面体理论中的培训 ‣ 深度学习与多面体理论的碰撞：一项调查"))。其次，Pokutta 等人
    ([2020](#bib.bib249)) 和 Xie 等人 ([2020a](#bib.bib342)) 在训练问题 ([11](#S5.E11 "在5
    线性规划与多面体理论中的培训 ‣ 深度学习与多面体理论的碰撞：一项调查")) 中使用了Frank-Wolfe的随机版本，直接在神经网络权重中施加结构。我们接下来回顾这些内容，从后者开始。
- en: 5.3.1 Stochastic Frank-Wolfe
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1 随机Frank-Wolfe
- en: 'Note that problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) is of the
    form ([20](#S5.E20 "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) with'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到问题 ([11](#S5.E11 "在5 线性规划与多面体理论中的培训 ‣ 深度学习与多面体理论的碰撞：一项调查")) 是形式 ([20](#S5.E20
    "在5.3 Frank-Wolfe在DNN训练算法中 ‣ 5 线性规划与多面体理论中的培训 ‣ 深度学习与多面体理论的碰撞：一项调查")) 的
- en: '|  | $\mathcal{L}({\bm{W}})=\frac{1}{D}\sum_{i=1}^{D}\ell(f(\tilde{{\bm{x}}}_{i},{\bm{W}}),\tilde{{\bm{y}}}_{i}).$
    |  |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}({\bm{W}})=\frac{1}{D}\sum_{i=1}^{D}\ell(f(\tilde{{\bm{x}}}_{i},{\bm{W}}),\tilde{{\bm{y}}}_{i}).$
    |  |'
- en: We remind the reader that we are omitting the biases in this section to simplify
    notation, as they can be incorporated as part of ${\bm{W}}$.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提醒读者，本节中省略了偏差以简化符号，因为它们可以作为 ${\bm{W}}$ 的一部分进行整合。
- en: Usually, some structure of the weights is commonly desired, (e.g. sparsity or
    boundedness), which traditionally have been incorporated as regularizing terms
    in the objective, as mentioned above. The recent work by Xie et al. ([2020a](#bib.bib342))
    and Pokutta et al. ([2020](#bib.bib249)), on the other hand, enforce structure
    on $\Theta$ directly using Frank-Wolfe —more precisely, stochastic versions of
    it.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一些权重结构是常常期望的（例如稀疏性或有界性），这些通常作为目标中的正则化项被纳入，如上所述。另一方面，Xie 等人 ([2020a](#bib.bib342))
    和 Pokutta 等人 ([2020](#bib.bib249)) 的最新工作直接使用Frank-Wolfe（更准确地说，是其随机版本）来强加对 $\Theta$
    的结构。
- en: Xie et al. ([2020a](#bib.bib342)) use a stochastic Frank-Wolfe approach to impose
    an $\ell_{1}$-norm constraint on the weights and biases ${\bm{W}}$ when training
    a neural network with 1 hidden layer. Note that $\ell_{1}$ constraints are polyhedral.
    Their algorithm is designed for a general Online Convex Optimization setting,
    where “losses” are revealed in each iteration. However, in their computational
    experiments, they included tests in an offline setting given by a DNN training
    problem.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: Xie 等人 ([2020a](#bib.bib342)) 使用随机Frank-Wolfe方法在训练一个具有1隐藏层的神经网络时，对权重和偏差 ${\bm{W}}$
    施加了 $\ell_{1}$-范数约束。注意，$\ell_{1}$ 约束是多面体的。他们的算法设计用于一般的在线凸优化设置，其中“损失”在每次迭代中显现。然而，在他们的计算实验中，他们还包括了由DNN训练问题提供的离线设置测试。
- en: 'The approach follows the Frank-Wolfe method described above closely. The key
    difference lies in the estimation of the stochastic gradient they use, which is
    not standard and it is one of the most important aspects of the algorithm. Instead
    of using ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ in ([23](#S5.E23 "In 5.3
    Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), the following
    *stochastic recursive estimator* of the gradient is used:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法紧密跟随上述描述的Frank-Wolfe方法。关键的区别在于它们使用的随机梯度估计，这不是标准的，并且是算法中最重要的方面之一。与 ([23](#S5.E23
    "在5.3 Frank-Wolfe在DNN训练算法中 ‣ 5 线性规划与多面体理论中的培训 ‣ 深度学习与多面体理论的碰撞：一项调查")) 中使用的 ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$
    相比，使用了以下 *随机递归估计器* 进行梯度估计：
- en: '|  | $\displaystyle{\bm{v}}_{0}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(0))$
    |  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{v}}_{0}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(0))$
    |  |'
- en: '|  | $\displaystyle{\bm{v}}_{i}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(i))+(1-\rho_{i})(v_{i-1}-\tilde{\nabla}\mathcal{L}({\bm{W}}(i-1)))$
    |  |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{v}}_{i}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(i))+(1-\rho_{i})(v_{i-1}-\tilde{\nabla}\mathcal{L}({\bm{W}}(i-1)))$
    |  |'
- en: where $\tilde{\nabla}\mathcal{L}$ is a stochastic gradient, and $\rho_{i}$ is
    a parameter. The authors show that the gradient approximation error of this estimator
    converges to 0 at a sublinear rate, with high probability. This is important for
    them to analyze the “regret bounds” they provide for the online setting.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\nabla}\mathcal{L}$ 是随机梯度，$\rho_{i}$ 是一个参数。作者展示了这种估计器的梯度近似误差以次线性速率收敛到0，且具有很高的概率。这对于他们分析“后悔界限”在在线设置中的表现至关重要。
- en: The experimental results in Xie et al. ([2020a](#bib.bib342)) in DNN training
    are very positive. They test their approach in the MNIST and CIFAR10 datasets
    and outperform existing state-of-the-art approaches in terms of suboptimality,
    training accuracy, and test accuracy.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: Xie 等人 ([2020a](#bib.bib342)) 在DNN训练中的实验结果非常积极。他们在MNIST和CIFAR10数据集上测试了他们的方法，并在次优性、训练准确性和测试准确性方面超越了现有的最先进方法。
- en: Pokutta et al. ([2020](#bib.bib249)) implement and test several variants of
    stochastic versions of Frank-Wolfe in the training of neural networks, including
    the approach by Xie et al. ([2020a](#bib.bib342)). Pokutta et al. ([2020](#bib.bib249))
    focus their experiments on their main proposed variant, which they refer to simply
    as Stochastic Frank-Wolfe (SFW). This variant uses
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: Pokutta 等人 ([2020](#bib.bib249)) 实现并测试了几种Frank-Wolfe的随机版本变体在神经网络训练中的应用，包括Xie等人
    ([2020a](#bib.bib342)) 的方法。Pokutta 等人 ([2020](#bib.bib249)) 重点研究了他们主要提出的变体，简称为随机Frank-Wolfe（SFW）。该变体使用
- en: '|  | ${\bm{v}}_{i}=(1-\rho_{i}){\bm{v}}_{i-1}+\rho_{i}\tilde{\nabla}\mathcal{L}({\bm{W}}(i)),$
    |  |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{v}}_{i}=(1-\rho_{i}){\bm{v}}_{i-1}+\rho_{i}\tilde{\nabla}\mathcal{L}({\bm{W}}(i)),$
    |  |'
- en: where $\rho_{i}$ is a momentum parameter. The authors propose many different
    options for $\Theta$ including $\ell_{1},\ell_{2}$ and $\ell_{\infty}$ balls,
    and $K$-sparse polytopes. Of these, only the $\ell_{2}$ ball is non-polyhedral.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\rho_{i}$ 是动量参数。作者提出了多种不同的 $\Theta$ 选项，包括 $\ell_{1}, \ell_{2}$ 和 $\ell_{\infty}$
    球体，以及 $K$-稀疏多面体。在这些选项中，只有 $\ell_{2}$ 球体是非多面体的。
- en: Overall, the computational experiments are promising for SFW. The authors advocate
    for this algorithm arguing that it provides excellent computational performances
    while being simple to implement and competitive with other state-of-the-art algorithms.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，计算实验对SFW的前景较为乐观。作者提倡这种算法，认为它在计算性能方面表现优异，同时实现简单，并且与其他最先进的算法具有竞争力。
- en: 5.3.2 Deep Frank-Wolfe
  id: totrans-507
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2 深度Frank-Wolfe
- en: 'Another application of Frank-Wolfe within DNN training was proposed by Berrada
    et al. ([2018](#bib.bib26)). While this approach does not make heavy use of linear
    programming techniques, the application of Frank-Wolfe is quite novel, and they
    do rely on one linear program needed when performing an update as ([24](#S5.E24
    "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: Berrada 等人 ([2018](#bib.bib26)) 提出了Frank-Wolfe在DNN训练中的另一个应用。虽然这种方法并未大量使用线性规划技术，但Frank-Wolfe的应用非常新颖，他们确实依赖于在执行更新时所需的一个线性程序，如
    ([24](#S5.E24 "在5.3中，Frank-Wolfe在DNN训练算法中的应用 ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 深度学习遇见多面体理论：综述"))。
- en: 'The authors note that ([21](#S5.E21 "In 5.3 Frank-Wolfe in DNN training algorithms
    ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) can also be written as the solution to the
    following *proximal* problem (Bubeck et al., [2015](#bib.bib40)):'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出 ([21](#S5.E21 "在5.3中，Frank-Wolfe在DNN训练算法中的应用 ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 深度学习遇见多面体理论：综述"))
    也可以被写成以下 *近似* 问题的解（Bubeck等，[2015](#bib.bib40)）：
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\mathcal{T}_{{\bm{W}}(i)}(\mathcal{L}({\bm{W}}))\right\}$
    |  | (25) |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\mathcal{T}_{{\bm{W}}(i)}(\mathcal{L}({\bm{W}}))\right\}$
    |  | (25) |'
- en: 'where $\mathcal{T}_{{\bm{W}}(i)}$ represents the first-order Taylor expansion
    at ${\bm{W}}(i)$. We are omitting regularizing terms since they do not play a
    fundamental role in the approach; all this discussion can be directly extended
    to include regularizers. Berrada et al. ([2018](#bib.bib26)) note that ([25](#S5.E25
    "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) linearizes the loss function, and propose the following *loss-preserving
    proximal* problem to replace ([25](#S5.E25 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe
    in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")):'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{T}_{{\bm{W}}(i)}$ 代表 ${\bm{W}}(i)$ 处的一级泰勒展开。我们省略了正则化项，因为它们在这种方法中并不发挥根本作用；所有这些讨论可以直接扩展以包含正则化项。Berrada
    等人 ([2018](#bib.bib26)) 指出 ([25](#S5.E25 "在 5.3.2 深度 Frank-Wolfe ‣ 5.3 Frank-Wolfe
    在 DNN 训练算法中 ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述")) 线性化了损失函数，并提出以下 *保持损失* 的近端问题来替代
    ([25](#S5.E25 "在 5.3.2 深度 Frank-Wolfe ‣ 5.3 Frank-Wolfe 在 DNN 训练算法中 ‣ 5 线性规划和多面体理论在训练中的应用
    ‣ 当深度学习遇上多面体理论：综述"))：
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\frac{1}{D}\sum_{i=1}^{D}\ell(\mathcal{T}_{{\bm{W}}(i)}(f(\tilde{{\bm{x}}}_{i},{\bm{W}})),\tilde{{\bm{y}}}_{i})\right\}$
    |  | (26) |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\frac{1}{D}\sum_{i=1}^{D}\ell(\mathcal{T}_{{\bm{W}}(i)}(f(\tilde{{\bm{x}}}_{i},{\bm{W}})),\tilde{{\bm{y}}}_{i})\right\}$
    |  | (26) |'
- en: 'Using the results by Lacoste-Julien et al. ([2013](#bib.bib182)), the authors
    argue that ([26](#S5.E26 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training
    algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is amenable to Frank-Wolfe in the
    dual when $\ell$ is piecewise linear and convex (e.g. the hinge loss). To be more
    specific, the authors show that in this case, and assuming $\alpha_{i}=\alpha$,
    there exists ${\bm{A}},{\bm{b}}$ such that the dual of ([26](#S5.E26 "In 5.3.2
    Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is simply'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Lacoste-Julien 等人的结果 ([2013](#bib.bib182))，作者认为当 $\ell$ 是分段线性和凸的（例如铰链损失）时，([26](#S5.E26
    "在 5.3.2 深度 Frank-Wolfe ‣ 5.3 Frank-Wolfe 在 DNN 训练算法中 ‣ 5 线性规划和多面体理论在训练中的应用 ‣
    当深度学习遇上多面体理论：综述")) 可以在对偶中应用 Frank-Wolfe。更具体地说，作者展示了在这种情况下，假设 $\alpha_{i}=\alpha$，存在
    ${\bm{A}},{\bm{b}}$ 使得 ([26](#S5.E26 "在 5.3.2 深度 Frank-Wolfe ‣ 5.3 Frank-Wolfe
    在 DNN 训练算法中 ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述")) 的对偶形式简单为
- en: '|  |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\max_{\mathbf{\beta}}\quad$ | $\displaystyle\frac{-1}{2\alpha}\&#124;{\bm{A}}\mathbf{\beta}\&#124;^{2}+{\bm{b}}^{\top}\mathbf{\beta}$
    |  | (27a) |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\mathbf{\beta}}\quad$ | $\displaystyle\frac{-1}{2\alpha}\&#124;{\bm{A}}\mathbf{\beta}\&#124;^{2}+{\bm{b}}^{\top}\mathbf{\beta}$
    |  | (27a) |'
- en: '|  | s.t. | $\displaystyle\mathbf{1}^{\top}\mathbf{\beta}=1$ |  | (27b) |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  | s.t. | $\displaystyle\mathbf{1}^{\top}\mathbf{\beta}=1$ |  | (27b) |'
- en: '|  |  | $\displaystyle\mathbf{\beta}\geq 0$ |  | (27c) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbf{\beta}\geq 0$ |  | (27c) |'
- en: 'The authors consider applying Frank-Wolfe to this last problem, and to recover
    the primal solution using the primal-dual relation ${\bm{W}}=-{\bm{A}}\mathbf{\beta}$,
    which is a consequence of KKT. The Frank-Wolfe iteration ([24](#S5.E24 "In 5.3
    Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) in the notation
    of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training
    algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) would look like'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 作者考虑将 Frank-Wolfe 应用于最后这个问题，并使用原始-对偶关系 ${\bm{W}}=-{\bm{A}}\mathbf{\beta}$ 来恢复原始解，这是
    KKT 的一个结果。在 ([24](#S5.E24 "在 5.3 Frank-Wolfe 在 DNN 训练算法中 ‣ 5 线性规划和多面体理论在训练中的应用
    ‣ 当深度学习遇上多面体理论：综述")) 的符号中，Frank-Wolfe 迭代 ([27](#S5.E27 "在 5.3.2 深度 Frank-Wolfe
    ‣ 5.3 Frank-Wolfe 在 DNN 训练算法中 ‣ 5 线性规划和多面体理论在训练中的应用 ‣ 当深度学习遇上多面体理论：综述")) 形式如下
- en: '|  | $\mathbf{\beta}_{i+1}=\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i}).$
    |  | (28) |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\beta}_{i+1}=\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i}).$
    |  | (28) |'
- en: 'Here, ${\bm{d}}_{i}$ is feasible for ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe
    ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) and
    obtained using a linear programming oracle, and $\gamma_{i}$ the Frank-Wolfe step-length.
    Note that the feasible region of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3
    Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) is a simplex:
    exploiting this, the authors show that an optimal $\gamma_{i}$ can be computed
    in closed-form: here, “optimal” refers to a minimizer of ([27](#S5.E27 "In 5.3.2
    Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) when restricted to points of the form $\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i})$.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，${\bm{d}}_{i}$是对([27](#S5.E27 "在5.3.2深度Frank-Wolfe中 ‣ 5.3 DNN训练算法中的Frank-Wolfe
    ‣ 5线性规划和训练中的多面体理论 ‣ 当深度学习遇见多面体理论：一项调查"))的可行解，是使用线性规划Oracle获得的，$\gamma_{i}$是Frank-Wolfe步长。请注意，([27](#S5.E27
    "在5.3.2深度Frank-Wolfe中 ‣ 5.3 DNN训练算法中的Frank-Wolfe ‣ 5线性规划和训练中的多面体理论 ‣ 当深度学习遇见多面体理论：一项调查"))的可行区域是一个单纯形：作者展示了在这种情况下可以封闭形式地计算出最优的$\gamma_{i}$：在这里，“最优”是指当限制在形式为$\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i})$的点上时，是([27](#S5.E27
    "在5.3.2深度Frank-Wolfe中 ‣ 5.3 DNN训练算法中的Frank-Wolfe ‣ 5线性规划和训练中的多面体理论 ‣ 当深度学习遇见多面体理论：一项调查"))的最小化器。
- en: 'With all these considerations, the bottleneck in this application of Frank-Wolfe
    is obtaining ${\bm{d}}_{i}$; recall that this Frank-Wolfe routine is embedded
    within a single iteration of the overall training algorithm; therefore, in each
    iteration of the training algorithm, possibly multiple computations of ${\bm{d}}_{i}$
    would be required in order to solve ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe ‣
    5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to
    optimality. To alleviate this, the authors propose to only perform one iteration
    of Frank-Wolfe: they set ${\bm{d}}_{0}$ to be the stochastic gradient and compute
    a closed-form expression for $\mathbf{\beta}_{1}$. This is the basic ingredient
    of the Deep Frank Wolfe (DFW). It is worth noting that this algorithm is not guaranteed
    to converge, however, its empirical performance is competitive.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些因素，Frank-Wolfe 在此应用中的瓶颈是获得${\bm{d}}_{i}$；请记住，这个Frank-Wolfe例程嵌入到整个训练算法的单次迭代中；因此，在训练算法的每次迭代中，可能需要多次计算${\bm{d}}_{i}$才能以最优方式解决([27](#S5.E27
    "在5.3.2深度Frank-Wolfe中 ‣ 5.3 DNN训练算法中的Frank-Wolfe ‣ 5线性规划和训练中的多面体理论 ‣ 当深度学习遇见多面体理论：一项调查"))。为了减轻这一情况，作者建议仅执行一次Frank-Wolfe的迭代：他们将${\bm{d}}_{0}$设置为随机梯度，计算$\mathbf{\beta}_{1}$的封闭表达式。这是深度Frank
    Wolfe (DFW)的基本组成部分。值得注意的是，该算法不能保证收敛，但其实际表现是有竞争力的。
- en: 'Other two important considerations are taken into account the implementation
    of this algorithm: smoothing of the loss function (as the Hinge loss is piecewise
    linear) and the adaptation of Nesterov’s Momentum to this new setting. We refer
    the reader to the corresponding article for these details. One of the key features
    of DFW is that it only requires one hyperparameter ($\alpha$) to be tuned.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 实施这一算法还考虑了其他两个重要因素：损失函数的平滑化（因为Hinge损失是分段线性的）和将Nesterov的动量调整到这个新环境中。关于这些细节，我们建议读者参考相应的文章。DFW的一个关键特点是它只需要调整一个超参数（$\alpha$）。
- en: 'The authors test DFW in image classification and natural language inference.
    Overall, the results obtained by DFW are very positive: in most cases, it can
    outperform adaptive gradient methods, and it is competitive with SGD while converging
    faster.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在图像分类和自然语言推理中测试了DFW。总体而言，DFW取得的结果非常积极：在大多数情况下，它可以优于自适应梯度法，并且在收敛速度方面与SGD相竞争。
- en: 5.4 Polyhedral encoding of multiple training problems
  id: totrans-524
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 对多个训练问题的多面体编码
- en: 'One of the questions raised by Arora et al. ([2018](#bib.bib8)) (see Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training
    neural networks with a single hidden layer ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) was
    whether the dependency on $D$ of their algorithm could be improved since it is
    typically the largest coefficient in a training problem. This question was studied
    by Bienstock et al. ([2023](#bib.bib32)), who show that, in an approximation setting,
    a more ambitious goal is achievable: there is a polyhedral encoding of multiple
    training problems whose size has a mild dependency on $D$.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: Arora 等人 ([2018](#bib.bib8)) 提出的一个问题（见第 [5.1.3](#S5.SS1.SSS3 "5.1.3 固定节点数和 ReLU
    激活 ‣ 5.1 训练具有单个隐藏层的神经网络 ‣ 5 线性规划和多面体理论中的训练 ‣ 当深度学习遇到多面体理论：综述") 节）是，他们的算法对 $D$
    的依赖是否可以得到改善，因为 $D$ 通常是训练问题中的最大系数。Bienstock 等人 ([2023](#bib.bib32)) 研究了这个问题，证明在一个近似设置中，可以实现一个更雄心勃勃的目标：存在多个训练问题的多面体编码，其大小对
    $D$ 的依赖较小。
- en: 'As in the previous section, we omit the biases ${\bm{b}}$ to simplify notation,
    as all parameters can be included in ${\bm{W}}$. Let us assume the class of neural
    networks $F$ in ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) are restricted
    to have bounded parameters (we assume they lie in the interval $[-1,1]$), and
    let us assume the sample has been normalized in such a way that $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$.
    Furthermore, let $N$ be the dimension of $\Theta$ (the number of parameters in
    the neural network). With this notation, we define the following.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所述，我们省略了偏差 ${\bm{b}}$ 以简化符号，因为所有参数都可以包含在 ${\bm{W}}$ 中。我们假设神经网络类 $F$ 在 ([11](#S5.E11
    "在5线性规划和多面体理论中的训练 ‣ 当深度学习遇到多面体理论：综述")) 中被限制为具有有界参数（我们假设它们位于区间 $[-1,1]$ 中），并且假设样本已被标准化，使得
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$。此外，设
    $N$ 为 $\Theta$ 的维度（神经网络中的参数数量）。使用这种符号，我们定义如下。
- en: Definition 2
  id: totrans-527
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$ — sample size, parameter space, loss function, network
    architecture, respectively. For a function $g$, let $\mathcal{K}_{\infty}(g)$
    be the Lipschitz constant of $g$ using the infinity norm. We define the *Architecture
    Lipschitz Constant* $\mathcal{K}(D,\Theta,\ell,f)$ as'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑 ERM 问题 ([11](#S5.E11 "在5线性规划和多面体理论中的训练 ‣ 当深度学习遇到多面体理论：综述"))，参数为 $D,\Theta,\ell,f$
    —— 样本大小、参数空间、损失函数、网络架构，分别。对于一个函数 $g$，设 $\mathcal{K}_{\infty}(g)$ 为 $g$ 的无限范数的利普希茨常数。我们定义
    *架构利普希茨常数* $\mathcal{K}(D,\Theta,\ell,f)$ 为
- en: '|  | $\mathcal{K}(D,\Theta,\ell,f)\doteq\mathcal{K}_{\infty}(\ell(f(\cdot,\cdot),\cdot))$
    |  | (29) |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{K}(D,\Theta,\ell,f)\doteq\mathcal{K}_{\infty}(\ell(f(\cdot,\cdot),\cdot))$
    |  | (29) |'
- en: over the domain $[-1,1]^{n_{0}}\times\Theta\times[-1,1]^{n_{L+1}}$.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 在域 $[-1,1]^{n_{0}}\times\Theta\times[-1,1]^{n_{L+1}}$ 上。
- en: Using this definition, and the boundedness of parameters, a straightforward
    approximate training algorithm can be devised whose running time is linear in
    $D$. Simply do a grid search in the parameters’ space, and evaluate all data points
    in each possible parameter. It is not hard to see that, to achieve $\epsilon$-optimality,
    such an algorithm would run in time which is linear in $D$ and exponential in
    $\mathcal{K}(D,\Theta,\ell,f)/\epsilon$. What was proved by Bienstock et al. ([2023](#bib.bib32))
    is that one can take a step further and represent multiple training problems at
    the same time.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个定义，以及参数的有界性，可以设计一个简单的近似训练算法，其运行时间与 $D$ 成线性关系。只需在参数空间中进行网格搜索，并评估每个可能参数中的所有数据点。可以看出，为了实现
    $\epsilon$-最优性，这种算法的运行时间将是与 $D$ 成线性关系，并且与 $\mathcal{K}(D,\Theta,\ell,f)/\epsilon$
    成指数关系。Bienstock 等人 ([2023](#bib.bib32)) 证明了可以更进一步，同时表示多个训练问题。
- en: Theorem 2 (Bienstock et al. ([2023](#bib.bib32)))
  id: totrans-532
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2（Bienstock 等人 ([2023](#bib.bib32)))
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$, and let $\mathcal{K}:=\mathcal{K}(D,\Theta,\ell,f)$
    be the corresponding network architecture. Consider $\epsilon>0$ arbitrary. There
    exists a polytope $P_{\epsilon}$ of size⁵⁵5Here, the size of the polytope is the
    number of variables and constraints describing it.'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑带有参数 $D,\Theta,\ell,f$ 的 ERM 问题 ([11](#S5.E11 "在 5 线性规划和多面体理论中的训练 ‣ 当深度学习遇到多面体理论：综述"))，设
    $\mathcal{K}:=\mathcal{K}(D,\Theta,\ell,f)$ 为对应的网络架构。考虑任意的 $\epsilon>0$。存在一个多面体
    $P_{\epsilon}$，其大小⁵⁵5 这里，多面体的大小是描述它的变量和约束的数量。
- en: '|  | $O(D\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ |  |'
  id: totrans-534
  prefs: []
  type: TYPE_TB
  zh: '|  | $O(D\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ |  |'
- en: 'with the following properties:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 具有以下属性：
- en: '1.'
  id: totrans-536
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: $P_{\epsilon}$ can be constructed in time $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N}D)$
    plus the time required for $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$
    evaluations of the loss function $\ell$ and $f$.
  id: totrans-537
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $P_{\epsilon}$ 可以在 $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N}D)$
    的时间内构造，加上对 $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ 次损失函数
    $\ell$ 和 $f$ 进行评估所需的时间。
- en: '2.'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'For *any* sample $(\tilde{X},\tilde{Y})=(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$, there
    is a face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ of $P_{\epsilon}$ such that optimizing
    a linear function over $\mathcal{F}_{\tilde{X},\tilde{Y}}$ yields an $\epsilon$-approximation
    to the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 *任何* 样本 $(\tilde{X},\tilde{Y})=(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$，$(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$，存在一个
    $P_{\epsilon}$ 的面 $\mathcal{F}_{\tilde{X},\tilde{Y}}$，使得在 $\mathcal{F}_{\tilde{X},\tilde{Y}}$
    上优化线性函数会得到 ERM 问题的 $\epsilon$-近似 ([11](#S5.E11 "在 5 线性规划和多面体理论中的训练 ‣ 当深度学习遇到多面体理论：综述"))。
- en: '3.'
  id: totrans-540
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ arises by simply substituting-in
    actual data for the data-variables $x,y$, which is used to fixed variables in
    the description of $P_{\epsilon}$.
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 面 $\mathcal{F}_{\tilde{X},\tilde{Y}}$ 通过简单地将实际数据代入数据变量 $x,y$ 中得到，这些数据用于固定 $P_{\epsilon}$
    描述中的变量。
- en: 'This result is very abstract in nature but possesses some interesting features.
    Firstly, it encodes (approximately) *every* possible training problem arising
    from data in $[-1,1]^{{n_{0}}+{n_{L+1}}}$ using a benign dependency on $D$: the
    polytope size depends only linearly on $D$, while a discretized enumeration of
    all the possible samples of size $D$ would be exponential in $D$. Secondly, every
    possible ERM problem appears in a *face* of the polytope; this suggests a strong
    geometric structure across different ERM problems. And lastly, this result is
    applicable to a wide variety of network architectures; in order to obtain an architecture-specific
    result, it suffices to compute the corresponding value of $\mathcal{K}$ and plug
    it in. Regarding this last point, the authors computed the constant $\mathcal{K}$
    for various well-known architectures and obtained the results of Table [4](#S5.T4
    "Table 4 ‣ 5.4 Polyhedral encoding of multiple training problems ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey").'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果在本质上非常抽象，但具有一些有趣的特性。首先，它编码（近似地）*每一个* 从 $[-1,1]^{{n_{0}}+{n_{L+1}}}$ 中产生的可能训练问题，对
    $D$ 的依赖是良性的：多面体的大小仅与 $D$ 线性相关，而对所有可能样本的离散枚举的大小是指数级的。其次，所有可能的 ERM 问题都出现在多面体的一个
    *面* 中；这表明不同 ERM 问题之间存在强大的几何结构。最后，这个结果适用于各种网络架构；为了获得特定架构的结果，只需计算对应的 $\mathcal{K}$
    值并将其代入即可。关于最后一点，作者计算了各种著名架构的常数 $\mathcal{K}$ 并得到了表 [4](#S5.T4 "表 4 ‣ 5.4 多面体编码的多个训练问题
    ‣ 5 线性规划和多面体理论中的训练 ‣ 当深度学习遇到多面体理论：综述") 的结果。
- en: 'Table 4: Summary of polyhedral encoding sizes for various architectures. DNN
    refers to a fully-connected Deep Neural Network, CNN to a Convolutional Neural
    Network, and ResNet to a Residual Network. $G$ is the graph defining the Network,
    $\Delta$ is the maximum in-degree in $G$, $L$ is the number of hidden layers,
    and ${n_{\max}}$ is the maximum width of a layer.'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：各种架构的多面体编码大小汇总。DNN 指完全连接的深度神经网络，CNN 指卷积神经网络，ResNet 指残差网络。$G$ 是定义网络的图，$\Delta$
    是 $G$ 中的最大入度，$L$ 是隐藏层的数量，${n_{\max}}$ 是一层的最大宽度。
- en: Type Loss Size of polytope Notes DNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ DNN Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}}){n_{\max}}^{O(k^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ CNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\ll|E({G})|$ ResNet Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    ResNet Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}})\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 损失 大小 备注 DNN 绝对/二次/铰链 $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ DNN 交叉熵 w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}}){n_{\max}}^{O(k^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ CNN 绝对/二次/铰链 $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\ll|E({G})|$ ResNet 绝对/二次/铰链 $O\big{(}\big{(}{n_{L+1}}\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    ResNet 交叉熵 w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}})\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
- en: 'The proof of this result relies on a graph theoretical concept called *treewidth*.
    This parameter is used for measuring structured sparsity, and in Bienstock and
    Muñoz ([2018](#bib.bib31)) it was proved that any optimization problem admits
    an approximate polyhedral reformulation whose size is exponential only in the
    treewidth parameter. On a high level, the neural network result is obtained by
    noting that ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) connects different sample
    points only through a sum; therefore, the following reformulation of the optimization
    problem can be considered, which decouples the different data points:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果的证明依赖于一个称为*树宽*的图论概念。这个参数用于衡量结构化稀疏性，在Bienstock和Muñoz（[2018](#bib.bib31)）中证明了任何优化问题都承认一个近似的多面体重新表述，其大小仅在树宽参数上是指数级的。高层次上，神经网络结果是通过注意到（[11](#S5.E11
    "在5线性规划和多面体理论中的训练 ‣ 当深度学习遇上多面体理论：一项综述")）仅通过一个总和连接不同的样本点，因此可以考虑优化问题的以下重新表述，这样可以解耦不同的数据点：
- en: '|  | $\displaystyle\min_{{\bm{W}}\in\Theta,{\bm{L}}}\left\{\frac{1}{D}\sum_{d=1}^{D}{\bm{L}}_{d}\,\middle&#124;\,{\bm{L}}_{d}\,=\,\ell(f(\tilde{{\bm{x}}}_{d},{\bm{W}}),\tilde{{\bm{y}}}_{d})\quad\forall\,d\in[D]\right\}$
    |  | (30) |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{W}}\in\Theta,{\bm{L}}}\left\{\frac{1}{D}\sum_{d=1}^{D}{\bm{L}}_{d}\,\middle&#124;\,{\bm{L}}_{d}\,=\,\ell(f(\tilde{{\bm{x}}}_{d},{\bm{W}}),\tilde{{\bm{y}}}_{d})\quad\forall\,d\in[D]\right\}$
    |  | (30) |'
- en: This reformulation does not seem useful at first, however, it has a *treewidth*
    that does not depend on $D$, even if the data points are considered variables.
    From this point, the authors are able to obtain the polytope whose size does not
    depend exponentially on $D$, and which is capable of encoding all possible ERM
    problems. The face structure the polytope has is more involved, and we refer the
    reader to Bienstock et al. ([2023](#bib.bib32)) for these details.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 这种重新表述乍看之下似乎不太有用，但它具有一个不依赖于$D$的*树宽*，即使数据点被视为变量。从这一点开始，作者们能够获得一个多面体，其大小不依赖于$D$的指数，并且能够编码所有可能的ERM问题。这个多面体的面结构更为复杂，我们将这些细节参见Bienstock等人（[2023](#bib.bib32)）。
- en: It is worth mentioning that the polytope size provided by Bienstock et al. ([2023](#bib.bib32))
    in the setting of Arora et al. ([2018](#bib.bib8)) is
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，Bienstock等人（[2023](#bib.bib32)）在Arora等人（[2018](#bib.bib8)）的设定下提供的多面体大小是
- en: '|  | $O((2\mathcal{K}_{\infty}(\ell)n_{1}^{O(1)}/\epsilon)^{({n_{0}}+1)(n_{1})}D)$
    |  | (31) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '|  | $O((2\mathcal{K}_{\infty}(\ell)n_{1}^{O(1)}/\epsilon)^{({n_{0}}+1)(n_{1})}D)$
    |  | (31) |'
- en: where $\mathcal{K_{\infty}}(\ell)$ is the Lipschitz constant of the loss function
    with respect to the infinity norm over a specific domain. These two results are
    not completely comparable, but they give a good idea of how good the size of polytope
    constructed in Bienstock et al. ([2023](#bib.bib32)) is. The dependency on $D$
    is better in the polytope size, the polytope encodes multiple training problems,
    and the result is more general (it applies to almost any architecture); however,
    the polytope only gives an approximation, and its construction requires boundedness.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{K_{\infty}}(\ell)$是关于特定领域的无穷范数的损失函数的利普希茨常数。这两个结果并不完全可比，但它们很好地说明了Bienstock等人（[2023](#bib.bib32)）构建的多面体的大小的优越性。多面体的大小对$D$的依赖性更好，该多面体编码了多个训练问题，并且结果更具一般性（几乎适用于任何架构）；然而，这个多面体仅提供了一个近似值，其构造需要有界性。
- en: 5.5 Backpropagation through MILP
  id: totrans-551
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 通过 MILP 进行反向传播
- en: 'In the work by Goebbels ([2021](#bib.bib120)), a novel use of Mixed-Integer
    Linear Programming is proposed in training ReLU networks: to serve as an alternative
    to SGD. This new algorithm works as backpropagation, as it updates the weights
    of the neural network iteratively starting from the last layer. The key difference
    is that each update in a layer amounts to solving a MILP.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Goebbels 的工作 ([2021](#bib.bib120)) 中，提出了一种混合整数线性规划在训练 ReLU 网络中的新用途：作为 SGD
    的替代方案。这个新算法像反向传播一样工作，因为它从最后一层开始迭代地更新神经网络的权重。关键的不同之处在于，每次在一层中的更新相当于解决一个 MILP 问题。
- en: 'Let us focus only on one hidden layer at a time (of width $n$), so we can assume
    we have an architecture as in Figure [11](#S5.F11 "Figure 11 ‣ 5.1.2 LTU activations
    and variable number of nodes ‣ 5.1 Training neural networks with a single hidden
    layer ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"). Furthermore, we assume we have some target
    output vectors $\{{\bm{T}}_{d}\}_{d=1}^{D}$ (when processing the last hidden layer
    in the backpropagation, this corresponds to $\{\tilde{{\bm{y}}}_{d}\}_{d=1}^{D}$)
    and some layer input $\{{\bm{I}}_{d}\}_{d=1}^{D}$ (when processing the last hidden
    layer, this corresponds to evaluating the neural network on $\{\tilde{{\bm{x}}}_{d}\}_{d=1}^{D}$
    up to the second-to-last hidden layer). The algorithm proposed by Goebbels ([2021](#bib.bib120))
    solves the following optimization problem to update the weights ${\bm{W}}$ and
    biases ${\bm{b}}$ of the given layer:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一次只关注一个隐藏层（宽度为 $n$），因此我们可以假设有一个如图 [11](#S5.F11 "图 11 ‣ 5.1.2 LTU 激活函数与节点数的可变性
    ‣ 5.1 训练具有单一隐藏层的神经网络 ‣ 5 线性规划与训练中的多面体理论 ‣ 当深度学习遇上多面体理论：综述") 所示的架构。此外，我们假设有一些目标输出向量
    $\{{\bm{T}}_{d}\}_{d=1}^{D}$（当处理反向传播中的最后一个隐藏层时，这对应于 $\{\tilde{{\bm{y}}}_{d}\}_{d=1}^{D}$）以及一些层输入
    $\{{\bm{I}}_{d}\}_{d=1}^{D}$（当处理最后一个隐藏层时，这对应于在 $\{\tilde{{\bm{x}}}_{d}\}_{d=1}^{D}$
    上评估神经网络，直到倒数第二个隐藏层）。Goebbels 提出的算法 ([2021](#bib.bib120)) 解决以下优化问题以更新给定层的权重 ${\bm{W}}$
    和偏置 ${\bm{b}}$：
- en: '|  |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\min_{{\bm{W}},\hat{{\bm{h}}},{\bm{b}},{\bm{h}},{\bm{z}}}\quad$
    | $\displaystyle\sum_{d=1}^{D}\sum_{j=1}^{n}&#124;{\bm{T}}_{d,j}-{\bm{h}}_{d,j}&#124;$
    |  | (32a) |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{{\bm{W}},\hat{{\bm{h}}},{\bm{b}},{\bm{h}},{\bm{z}}}\quad$
    | $\displaystyle\sum_{d=1}^{D}\sum_{j=1}^{n}\left|{\bm{T}}_{d,j}-{\bm{h}}_{d,j}\right|$
    |  | (32a) |'
- en: '|  | s.t. | $\displaystyle\hat{{\bm{h}}}_{d,j}=({\bm{W}}{\bm{I}}_{d})_{j}+{\bm{b}}_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32b) |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  | 满足 | $\displaystyle\hat{{\bm{h}}}_{d,j}=({\bm{W}}{\bm{I}}_{d})_{j}+{\bm{b}}_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32b) |'
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32c) |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32c) |'
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\geq-M(1-{\bm{z}}_{d,j})$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32d) |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\geq-M(1-{\bm{z}}_{d,j})$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32d) |'
- en: '|  |  | $\displaystyle&#124;\hat{{\bm{h}}}_{d,j}-{\bm{h}}_{d,j}&#124;\leq M(1-{\bm{z}}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32e) |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\left|\hat{{\bm{h}}}_{d,j}-{\bm{h}}_{d,j}\right|\leq
    M(1-{\bm{z}}_{d,j})$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32e)
    |'
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32f) |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32f) |'
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\geq 0$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$
    |  | (32g) |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{h}}_{d,j}\geq 0$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$
    |  | (32g) |'
- en: '|  |  | $\displaystyle{\bm{z}}_{d,j}\in\{0,1\}$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$
    |  | (32h) |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle{\bm{z}}_{d,j}\in\{0,1\}$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$
    |  | (32h) |'
- en: 'Here $M$ is a large constant that is assumed to bound the input to any neuron.
    Note that problem ([32](#S5.E32 "In 5.5 Backpropagation through MILP ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) can easily be linearized. This optimization problem finds
    the weights (${\bm{W}}$) and biases (${\bm{b}}$) that minimize the difference
    between the “real” output of the network for each sample (${\bm{h}}_{d}$) and
    the target output (${\bm{T}}_{d}$). The auxiliary variables $\hat{{\bm{h}}}_{d,j}$
    represent the input to the each neuron —so ${\bm{h}}_{d,j}=\sigma(\hat{{\bm{h}}}_{d,j})$—
    and ${\bm{z}}_{d,j}$ indicates if the $j$-th neuron is activated on input ${\bm{I}}_{d}$.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $M$ 是一个大的常数，假设它限制了任何神经元的输入。请注意，问题 ([32](#S5.E32 "在 5.5 节 MILP 反向传播 ‣ 5 线性规划与多面体理论在训练中的应用
    ‣ 深度学习遇见多面体理论：综述")) 可以很容易地线性化。这个优化问题寻找能够最小化网络对每个样本的“真实”输出 (${\bm{h}}_{d}$) 和目标输出
    (${\bm{T}}_{d}$) 之间差异的权重 (${\bm{W}}$) 和偏置 (${\bm{b}}$)。辅助变量 $\hat{{\bm{h}}}_{d,j}$
    代表每个神经元的输入——因此 ${\bm{h}}_{d,j}=\sigma(\hat{{\bm{h}}}_{d,j})$——而 ${\bm{z}}_{d,j}$
    表示第 $j$ 个神经元是否在输入 ${\bm{I}}_{d}$ 上被激活。
- en: 'When processing intermediate layers, the definition ${\bm{I}}_{d}$ can easily
    be adapted from what we mentioned above. However, the story is different for the
    case of ${\bm{T}}_{d}$. When processing the last layer, as previously mentioned,
    ${\bm{T}}_{d}$ simply corresponds to $\tilde{{\bm{y}}}_{d}$. For intermediate
    layers, to define ${\bm{T}}_{d}$, the author proposes to use a similar optimization
    problem to ([32](#S5.E32 "In 5.5 Backpropagation through MILP ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")), but leaving ${\bm{W}}$ and ${\bm{b}}$ fixed and having ${\bm{I}}_{d}$
    as variables; this defines “optimal inputs” of a layer. These optimal inputs are
    then used as target outputs ${\bm{T}}_{d}$ when processing the preceding layer,
    and thus the algorithm is iterated. For details, see Goebbels ([2021](#bib.bib120)).'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理中间层时，${\bm{I}}_{d}$ 的定义可以很容易地从我们上述提到的内容进行适应。然而，对于 ${\bm{T}}_{d}$ 的情况，情况有所不同。在处理最后一层时，如前所述，${\bm{T}}_{d}$
    简单地对应于 $\tilde{{\bm{y}}}_{d}$。对于中间层，为了定义 ${\bm{T}}_{d}$，作者建议使用类似于 ([32](#S5.E32
    "在 5.5 节 MILP 反向传播 ‣ 5 线性规划与多面体理论在训练中的应用 ‣ 深度学习遇见多面体理论：综述")) 的优化问题，但固定 ${\bm{W}}$
    和 ${\bm{b}}$，并将 ${\bm{I}}_{d}$ 作为变量；这定义了一个层的“最优输入”。这些最优输入随后被用作处理前一层时的目标输出 ${\bm{T}}_{d}$，因此算法被迭代。有关详细信息，请参见
    Goebbels ([2021](#bib.bib120))。
- en: The computational results in that paper show that a similar level of accuracy
    to that of gradient descent can be achieved. However, the use of potentially expensive
    MILPs impairs the applicability of this approach to large networks. Nonetheless,
    it shows an interesting new avenue for training whose running times may be improved
    in future implementations.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文中的计算结果表明，可以实现与梯度下降类似的准确度。然而，使用可能昂贵的 MILP 会影响该方法在大网络中的应用。尽管如此，它展示了一种有趣的新训练途径，其运行时间在未来的实现中可能会得到改进。
- en: 5.6 Training binarized neural networks using MILP
  id: totrans-566
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 使用 MILP 训练二值化神经网络
- en: As mentioned before, the training problem of a DNN is an unrestricted non-convex
    optimization problem, which is typically continuous as the weights and biases
    frequently are allowed to have any real value. Nonetheless, if the weights and
    biases are required to be integer-valued, the training problem becomes a discrete
    optimization problem, for which gradient-descent-based methods may find some difficulties
    in their applicability.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，DNN 的训练问题是一个不受限制的非凸优化问题，这通常是连续的，因为权重和偏置通常可以具有任何实值。然而，如果权重和偏置要求为整数值，则训练问题变成了一个离散优化问题，这使得基于梯度下降的方法在应用上可能会遇到困难。
- en: 'In this context, Icarte et al. ([2019](#bib.bib161)) proposed a MILP formulation
    for the training problem of binarized neural networks (BNNs): these are neural
    networks where the weights and biases are restricted to be in $\{-1,0,1\}$ and
    where the activations are LTU (i.e. sign functions). Later on, Thorbjarnarson
    and Yorke-Smith ([2020](#bib.bib305), [2023](#bib.bib306)) used a similar technique
    to allow more general integer-valued weights. We review the core feature in these
    formulations that yield a *linear* formulation of the training problem.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种背景下，Icarte 等人 ([2019](#bib.bib161)) 提出了一个用于二值化神经网络（BNNs）训练问题的 MILP 公式：这些神经网络的权重和偏置被限制为
    $\{-1,0,1\}$，而激活函数是 LTU（即符号函数）。后来，Thorbjarnarson 和 Yorke-Smith ([2020](#bib.bib305),
    [2023](#bib.bib306)) 使用了类似的技术来允许更一般的整数值权重。我们回顾了这些公式中的核心特征，它们产生了一个 *线性* 的训练问题公式。
- en: 'Let us focus on an intermediate layer $i$ with width $n$, and let us omit biases
    to simplify the discussion. Using a DNN’s layer-wise architecture, one usually
    aims at describing:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注一个宽度为 $n$ 的中间层 $i$，并为简化讨论而省略偏置。利用 DNN 的层次结构，通常旨在描述：
- en: '|  |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle=({\bm{W}}^{i}{\bm{h}}^{i-1}_{d})_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (33a) |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle=({\bm{W}}^{i}{\bm{h}}^{i-1}_{d})_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (33a) |'
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=\sigma(\hat{{\bm{h}}}^{i}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$ |  | (33b) |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=\sigma(\hat{{\bm{h}}}^{i}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$ |  | (33b) |'
- en: 'We remind the reader that $D$ is the cardinality of the training set. Additionally,
    for each data point indexed by $d$ and each layer $i$, each variable ${\bm{h}}_{d}^{i}$
    is the output vector of all the neurons of the layer and each variable $\hat{{\bm{h}}}_{d,j}^{i}$
    is the input of neuron $j$. Besides the difficulty posed by the activation function,
    one important issue with system ([33](#S5.E33 "In 5.6 Training binarized neural
    networks using MILP ‣ 5 Linear Programming and Polyhedral Theory in Training ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")) is the non-linearity of
    the products between the ${\bm{W}}$ and ${\bm{h}}$ variables. Nonetheless, this
    issue disappears when each entry of ${\bm{W}}$ and ${\bm{h}}$ is bounded and integer,
    as in the case of BNNs.'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提醒读者，$D$ 是训练集的基数。此外，对于每个由 $d$ 索引的数据点和每一层 $i$，每个变量 ${\bm{h}}_{d}^{i}$ 是该层所有神经元的输出向量，而每个变量
    $\hat{{\bm{h}}}_{d,j}^{i}$ 是神经元 $j$ 的输入。除了激活函数带来的困难外，系统 ([33](#S5.E33 "在5.6 使用MILP训练二值化神经网络
    ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述")) 的一个重要问题是 ${\bm{W}}$ 和 ${\bm{h}}$ 变量之间的乘积的非线性。然而，当
    ${\bm{W}}$ 和 ${\bm{h}}$ 的每个条目被限制为整数时，例如在BNNs的情况下，这个问题就消失了。
- en: 'Let us begin with reformulating ([33b](#S5.E33.2 "In 33 ‣ 5.6 Training binarized
    neural networks using MILP ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). We can introduce auxiliary
    variables ${\bm{u}}_{d,j}^{i}\in\{0,1\}$ that will indicate if the neuron is active.
    We also introduce a tolerance $\varepsilon>0$ to determine the activity of a neuron.
    Using this, we can (approximately) reformulate ([33b](#S5.E33.2 "In 33 ‣ 5.6 Training
    binarized neural networks using MILP ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) *linearly*
    using big-M constraints:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始重新表述 ([33b](#S5.E33.2 "在33 ‣ 5.6 使用MILP训练二值化神经网络 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述"))。我们可以引入辅助变量
    ${\bm{u}}_{d,j}^{i}\in\{0,1\}$ 来指示神经元是否处于活动状态。我们还引入了一个容忍度 $\varepsilon>0$ 来确定神经元的活动状态。利用这一点，我们可以（近似地）使用大-M约束
    *线性地* 重新表述 ([33b](#S5.E33.2 "在33 ‣ 5.6 使用MILP训练二值化神经网络 ‣ 5 线性规划和训练中的多面体理论 ‣ 当深度学习遇到多面体理论：综述"))：
- en: '|  |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=2{\bm{u}}_{d,j}^{i}-1$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34a) |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=2{\bm{u}}_{d,j}^{i}-1$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34a) |'
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\geq-M(1-{\bm{u}}_{d,j}^{i})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34b) |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\geq-M(1-{\bm{u}}_{d,j}^{i})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34b) |'
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\leq-\varepsilon+M{\bm{u}}_{d,j}^{i}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34c) |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\leq-\varepsilon+M{\bm{u}}_{d,j}^{i}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34c) |'
- en: 'where $M$ is a large constant. As for ([33a](#S5.E33.1 "In 33 ‣ 5.6 Training
    binarized neural networks using MILP ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), note that'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M$ 是一个较大的常数。至于 ([33a](#S5.E33.1 "在33 ‣ 5.6使用MILP训练二值神经网络 ‣ 5线性规划和多面体理论在训练中
    ‣ 当深度学习遇见多面体理论：一项调查"))，注意到
- en: '|  | $\hat{{\bm{h}}}^{i}_{d,j}=\sum_{k=1}{\bm{W}}^{i}_{j,k}{\bm{h}}^{i-1}_{d,k}.$
    |  |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{{\bm{h}}}^{i}_{d,j}=\sum_{k=1}{\bm{W}}^{i}_{j,k}{\bm{h}}^{i-1}_{d,k}.$
    |  |'
- en: 'Therefore, using ([34a](#S5.E34.1 "In 34 ‣ 5.6 Training binarized neural networks
    using MILP ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")), we see that it suffices to describe
    each product ${\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ linearly. We can introduce
    new variables ${\bm{z}}_{j,k,d}^{i}$ and note that'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用 ([34a](#S5.E34.1 "在34 ‣ 5.6使用MILP训练二值神经网络 ‣ 5线性规划和多面体理论在训练中 ‣ 当深度学习遇见多面体理论：一项调查"))，我们可以得出只需要线性描述每个乘积
    ${\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$。我们可以引入新变量 ${\bm{z}}_{j,k,d}^{i}$ 并注意到
- en: '|  | ${\bm{z}}_{j,k,d}^{i-1}={\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ |  |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bm{z}}_{j,k,d}^{i-1}={\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ |  |'
- en: if and only if the three variables satisfy
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 当且仅当三个变量满足时
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}&#124;$ | $\displaystyle\leq{\bm{u}}_{d,k}^{i-1}$
    |  |'
  id: totrans-584
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}&#124;$ | $\displaystyle\leq{\bm{u}}_{d,k}^{i-1}$
    |  |'
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}-{\bm{W}}_{j,k}^{i}&#124;$ |
    $\displaystyle\leq 1-{\bm{u}}_{d,k}^{i-1}$ |  |'
  id: totrans-585
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}-{\bm{W}}_{j,k}^{i}&#124;$ |
    $\displaystyle\leq 1-{\bm{u}}_{d,k}^{i-1}$ |  |'
- en: '|  | $\displaystyle{\bm{u}}_{d,k}^{i-1}$ | $\displaystyle\in\{0,1\}.$ |  |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bm{u}}_{d,k}^{i-1}$ | $\displaystyle\in\{0,1\}.$ |  |'
- en: This last system can be easily converted to a linear system, and thus the training
    problem in this setting can be cast as a mixed-integer linear optimization problem.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后一个系统可以很容易地转换为线性系统，因此在这种情况下，训练问题可以被看作是一个混合整数线性优化问题。
- en: Other works have also relied on similar formulations to train neural networks.
    Icarte et al. ([2019](#bib.bib161)) introduce different objective functions that
    can be used along with the linear system to produce a MILP that can train BNNs.
    They also introduce a Constraint-Programming-based model and a hybrid model, and
    then compare all of them computationally. Thorbjarnarson and Yorke-Smith ([2020](#bib.bib305))
    introduce more MILP-based training models that leverage piecewise linear approximations
    of well-known non-linear loss functions and that can handle integer weights beyond
    $\{-1,0,1\}$. A similar setting is studied by Sildir and Aydin ([2022](#bib.bib287)),
    where piecewise linear approximations of non-linear activations are used, and
    integer weights are exploited to formulate the training problem as a MILP. Finally,
    Bernardelli et al. ([2022](#bib.bib25)) rely on a multi-objective MIP model for
    training BNNs; from here, they create a BNN ensemble to produce robust classifiers.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 其他作品也依赖于类似的公式来训练神经网络。Icarte 等人 (2019) 引入了不同的目标函数，可以与线性系统一起使用，生成一个可以训练 BNNs 的
    MILP。他们还引入了基于约束编程的模型和混合模型，并进行了计算比较。Thorbjarnarson 和 Yorke-Smith (2020) 引入了更多基于
    MILP 的训练模型，利用了众所周知的非线性损失函数的分段线性逼近，可以处理 $\{-1,0,1\}$ 之外的整数权重。Sildir 和 Aydin (2022)
    研究了类似的情况，其中使用分段线性逼近的非线性激活，并利用整数权重来将训练问题建模为 MILP。最后，Bernardelli 等人 (2022) 依靠多目标
    MIP 模型来训练 BNNs；从这里，他们创建了一个 BNN 集合来产生强健的分类器。
- en: From these articles, we can conclude that the MILP-based approach to training
    their neural networks can result in high-quality neural networks, especially in
    terms of generalization. However, many of these MILP-based methods currently do
    not scale well, as opposed to gradient-descent-based methods. We believe that,
    even though there are some theoretical limitations to the efficiency of MILP-based
    methods, there is a considerable practical improvement potential with using them
    in neural network training.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些文章中，我们可以得出结论，基于 MILP 的方法训练神经网络可以产生高质量的神经网络，特别是在泛化方面。然而，许多这些基于 MILP 的方法当前不具有良好的可扩展性，与梯度下降法相反。我们相信，即使对于基于
    MILP 方法的效率存在一些理论限制，但在神经网络训练中使用它们仍然具有相当大的实际改进潜力。
- en: 6 Conclusion
  id: totrans-590
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'The rapid advancement of neural networks and their ubiquity has given rise
    to numerous new challenges and opportunities in deep learning: we need to design
    them in more reliable ways, to better understand their limits, and to test their
    robustness, among other challenges. While, traditionally, continuous optimization
    has been the predominant technology used in the optimization tasks in deep learning,
    some of these new challenges have made discrete optimization tools gain a remarkable
    importance.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的快速进步及其普遍存在带来了深度学习中许多新的挑战和机遇：我们需要以更可靠的方式设计它们，更好地理解它们的局限性，并测试它们的鲁棒性等挑战。虽然传统上，连续优化一直是深度学习优化任务中使用的主要技术，但一些新的挑战使得离散优化工具变得尤为重要。
- en: In this survey, we have reviewed multiple areas where polyhedral theory and
    linear optimization have played a critical role. For example, in understanding
    the expressiveness of neural networks, in optimizing trained neural networks (e.g.
    for verification purposes), and even in designing new training algorithms. We
    hope this survey can provide perspective in a rapidly-changing field, and motivate
    further developments in both deep learning and discrete optimization. There is
    still much to be explored in the intersection of these fields.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们回顾了多领域中多面体理论和线性优化发挥关键作用的情况。例如，在理解神经网络的表达能力，优化训练神经网络（例如用于验证目的），甚至设计新的训练算法方面。我们希望这项调查能在快速变化的领域中提供视角，并激励深度学习和离散优化的进一步发展。关于这些领域交集的探索仍有很多。
- en: Acknowledgments
  id: totrans-593
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Christian Tjandraatmadja and Toon Tran for early feedback on the manuscript
    and asking questions that helped shaping it.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Christian Tjandraatmadja 和 Toon Tran 对手稿的早期反馈和提出的问题，这些问题帮助塑造了手稿。
- en: Thiago Serra was supported by the National Science Foundation (NSF) award IIS
    2104583. Calvin Tsay was supported by the Engineering & Physical Sciences Research
    Council (EPSRC) grant EP/T001577/1.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: Thiago Serra 获得了国家科学基金会（NSF）奖项 IIS 2104583 的支持。Calvin Tsay 获得了工程与物理科学研究委员会（EPSRC）拨款
    EP/T001577/1 的支持。
- en: References
  id: totrans-596
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abrahamsen et al. (2021) M. Abrahamsen, L. Kleist, and T. Miltzow. Training
    neural networks is er-complete. In *Neural Information Processing Systems (NeurIPS)*,
    volume 34, 2021.
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abrahamsen 等（2021）M. Abrahamsen, L. Kleist, 和 T. Miltzow. 训练神经网络是 er-complete
    的。发表于*神经信息处理系统（NeurIPS）*，第34卷，2021年。
- en: Agostinelli et al. (2015) F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi.
    Learning activation functions to improve deep neural networks. In *International
    Conference on Learning Representations (ICLR) Workshop*, 2015.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agostinelli 等（2015）F. Agostinelli, M. Hoffman, P. Sadowski, 和 P. Baldi. 学习激活函数以改善深度神经网络。发表于*国际学习表征会议（ICLR）研讨会*，2015年。
- en: Amrami and Goldberg (2021) A. Amrami and Y. Goldberg. A simple geometric proof
    for the benefit of depth in ReLU networks. *arXiv:2101.07126*, 2021.
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amrami 和 Goldberg（2021）A. Amrami 和 Y. Goldberg. 通过几何证明 ReLU 网络深度的好处。*arXiv:2101.07126*，2021年。
- en: Anderson et al. (2019) R. Anderson, J. Huchette, C. Tjandraatmadja, and J. Vielma.
    Strong mixed-integer programming formulations for trained neural networks. In
    *Integer Programming and Combinatorial Optimization (IPCO)*, 2019.
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等（2019）R. Anderson, J. Huchette, C. Tjandraatmadja, 和 J. Vielma. 针对训练神经网络的强混合整数规划公式。发表于*整数规划与组合优化（IPCO）*，2019年。
- en: Anderson et al. (2020) R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and
    J. P. Vielma. Strong mixed-integer programming formulations for trained neural
    networks. *Mathematical Programming*, 183(1-2):3–39, 2020.
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等（2020）R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, 和 J. P.
    Vielma. 针对训练神经网络的强混合整数规划公式。*数学编程*，183(1-2):3–39，2020年。
- en: Anil et al. (2019) C. Anil, J. Lucas, and R. Grosse. Sorting out Lipschitz function
    approximation. In *International Conference on Machine Learning (ICML)*, 2019.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil 等（2019）C. Anil, J. Lucas, 和 R. Grosse. 分析 Lipschitz 函数近似。发表于*国际机器学习会议（ICML）*，2019年。
- en: Arjovsky et al. (2017) M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning
    (ICML)*, 2017.
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky 等（2017）M. Arjovsky, S. Chintala, 和 L. Bottou. Wasserstein 生成对抗网络。发表于*国际机器学习会议（ICML）*，2017年。
- en: Arora et al. (2018) R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding
    deep neural networks with rectified linear units. In *International Conference
    on Learning Representations (ICLR)*, 2018.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 等（2018）R. Arora, A. Basu, P. Mianjy, 和 A. Mukherjee. 通过修正线性单元理解深度神经网络。发表于*国际学习表征会议（ICLR）*，2018年。
- en: Aziznejad et al. (2020) S. Aziznejad, H. Gupta, J. Campos, and M. Unser. Deep
    neural networks with trainable activations and controlled Lipschitz constant.
    *IEEE Transactions on Signal Processing*, 68:4688–4699, 2020.
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aziznejad et al. (2020) S. Aziznejad, H. Gupta, J. Campos, 和 M. Unser. 带有可训练激活函数和控制Lipschitz常数的深度神经网络。*IEEE信号处理学报*，68:4688–4699，2020年。
- en: Bahdanau et al. (2015) D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation
    by jointly learning to align and translate. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau et al. (2015) D. Bahdanau, K. Cho, 和 Y. Bengio. 通过联合学习对齐和翻译的神经机器翻译。发表于*国际学习表征会议（ICLR）*，2015年。
- en: 'Balas (1998) E. Balas. Disjunctive programming: Properties of the convex hull
    of feasible points. *Discrete Applied Mathematics*, 89(1-3):3–44, 1998.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas (1998) E. Balas. 离散规划：可行点凸包的性质。*离散应用数学*，89(1-3):3–44，1998年。
- en: Balas (2018) E. Balas. *Disjunctive Programming*. Springer Cham, 2018.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas (2018) E. Balas. *离散规划*。Springer Cham，2018年。
- en: Balas et al. (1993) E. Balas, S. Ceria, and G. Cornuéjols. A lift-and-project
    cutting plane algorithm for mixed 0–1 programs. *Mathematical Programming*, 58(1-3):295–324,
    1993.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas et al. (1993) E. Balas, S. Ceria, 和 G. Cornuéjols. 一种用于混合0-1程序的提升与投影切割平面算法。*数学编程*，58(1-3):295–324，1993年。
- en: Balas et al. (1996) E. Balas, S. Ceria, and G. Cornuéjols. Mixed 0-1 programming
    by lift-and-project in a branch-and-cut framework. *Management Science*, 42(9):1229–1246,
    1996.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balas et al. (1996) E. Balas, S. Ceria, 和 G. Cornuéjols. 在分支切割框架中通过提升和投影进行混合0-1规划。*管理科学*，42(9):1229–1246，1996年。
- en: Balestriero and Baraniuk (2018) R. Balestriero and R. G. Baraniuk. A spline
    theory of deep networks. In *International Conference on Machine Learning (ICML)*,
    2018.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balestriero and Baraniuk (2018) R. Balestriero 和 R. G. Baraniuk. 深度网络的样条理论。发表于*国际机器学习会议（ICML）*，2018年。
- en: 'Balunović and Vechev (2020) M. Balunović and M. Vechev. Adversarial training
    and provable defenses: Bridging the gap. In *International Conference on Learning
    Representations (ICLR)*, 2020.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balunović and Vechev (2020) M. Balunović 和 M. Vechev. 对抗训练和可证明防御：弥合差距。发表于*国际学习表征会议（ICLR）*，2020年。
- en: Batten et al. (2021) B. Batten, P. Kouvaros, A. Lomuscio, and Y. Zheng. Efficient
    neural network verification via layer-based semidefinite relaxations and linear
    cuts. In *International Joint Conference on Artificial Intelligence (IJCAI)*,
    pages 2184–2190, 2021.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Batten et al. (2021) B. Batten, P. Kouvaros, A. Lomuscio, 和 Y. Zheng. 通过基于层的半正定松弛和线性切割实现高效神经网络验证。发表于*国际人工智能联合会议（IJCAI）*，第2184–2190页，2021年。
- en: Bengio (2009) Y. Bengio. Learning deep architectures for AI. *Foundations and
    Trends®in Machine Learning*, 2(1):1–127, 2009.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio (2009) Y. Bengio. 为AI学习深度架构。*机器学习基础与趋势*，2(1):1–127，2009年。
- en: 'Bengio et al. (2021) Y. Bengio, A. Lodi, and A. Prouvost. Machine learning
    for combinatorial optimization: a methodological tour d’horizon. *European Journal
    of Operational Research*, 290(2):405–421, 2021.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. (2021) Y. Bengio, A. Lodi, 和 A. Prouvost. 组合优化的机器学习：方法论概述。*欧洲运筹学杂志*，290(2):405–421，2021年。
- en: Bennett (1992) K. P. Bennett. Decision tree construction via linear programming.
    Technical report, University of Wisconsin-Madison Department of Computer Sciences,
    1992.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett (1992) K. P. Bennett. 通过线性规划构建决策树。技术报告，威斯康星大学麦迪逊分校计算机科学系，1992年。
- en: Bennett and Mangasarian (1990) K. P. Bennett and O. L. Mangasarian. Neural network
    training via linear programming. Technical report, University of Wisconsin-Madison
    Department of Computer Sciences, 1990.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett and Mangasarian (1990) K. P. Bennett 和 O. L. Mangasarian. 通过线性规划训练神经网络。技术报告，威斯康星大学麦迪逊分校计算机科学系，1990年。
- en: Bennett and Mangasarian (1992) K. P. Bennett and O. L. Mangasarian. Robust linear
    programming discrimination of two linearly inseparable sets. *Optimization Methods
    and Software*, 1(1):23–34, 1992.
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bennett and Mangasarian (1992) K. P. Bennett 和 O. L. Mangasarian. 对两个线性不可分集合的鲁棒线性规划区分。*优化方法与软件*，1(1):23–34，1992年。
- en: Benussi et al. (2022) E. Benussi, A. Patane, M. Wicker, L. Laurenti, and M. Kwiatkowska.
    Individual fairness guarantees for neural networks. In *International Joint Conference
    on Artificial Intelligence (IJCAI)*, pages 651–658, 2022.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benussi et al. (2022) E. Benussi, A. Patane, M. Wicker, L. Laurenti, 和 M. Kwiatkowska.
    神经网络的个体公平性保证。发表于*国际人工智能联合会议（IJCAI）*，第651–658页，2022年。
- en: 'Bergman et al. (2022) D. Bergman, T. Huang, P. Brooks, A. Lodi, and A. U. Raghunathan.
    Janos: an integrated predictive and prescriptive modeling framework. *INFORMS
    Journal on Computing*, 34(2):807–816, 2022.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bergman et al. (2022) D. Bergman, T. Huang, P. Brooks, A. Lodi, 和 A. U. Raghunathan.
    Janos：一个集成的预测和指示建模框架。*INFORMS计算期刊*，34(2):807–816，2022年。
- en: 'Bernardelli et al. (2022) A. M. Bernardelli, S. Gualandi, H. C. Lau, and S. Milanesi.
    The bemi stardust: a structured ensemble of binarized neural networks. *arXiv
    preprint arXiv:2212.03659*, 2022.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bernardelli 等（2022）A. M. Bernardelli, S. Gualandi, H. C. Lau 和 S. Milanesi。BEMI
    星尘：一个结构化的二值化神经网络集成。*arXiv 预印本 arXiv:2212.03659*，2022。
- en: Berrada et al. (2018) L. Berrada, A. Zisserman, and M. P. Kumar. Deep Frank-Wolfe
    for neural network optimization. *arXiv:1811.07591*, 2018.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berrada 等（2018）L. Berrada, A. Zisserman 和 M. P. Kumar。用于神经网络优化的深度 Frank-Wolfe
    方法。*arXiv:1811.07591*，2018。
- en: Bertschinger et al. (2022) D. Bertschinger, C. Hertrich, P. Jungeblut, T. Miltzow,
    and S. Weber. Training fully connected neural networks is $\exists\mathbb{R}$-complete.
    *arXiv:2204.01368*, 2022.
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertschinger 等（2022）D. Bertschinger, C. Hertrich, P. Jungeblut, T. Miltzow 和
    S. Weber。训练全连接神经网络是 $\exists\mathbb{R}$-完全的。*arXiv:2204.01368*，2022。
- en: 'Bhosekar and Ierapetritou (2018) A. Bhosekar and M. Ierapetritou. Advances
    in surrogate based modeling, feasibility analysis, and optimization: A review.
    *Computers & Chemical Engineering*, 108:250–267, 2018.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhosekar 和 Ierapetritou（2018）A. Bhosekar 和 M. Ierapetritou。代理建模、可行性分析和优化的进展：综述。*计算机与化学工程*，108:250–267，2018。
- en: 'Bianchini and Scarselli (2014) M. Bianchini and F. Scarselli. On the complexity
    of neural network classifiers: A comparison between shallow and deep architectures.
    *IEEE Transactions on Neural Networks and Learning Systems*, 2014.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bianchini 和 Scarselli（2014）M. Bianchini 和 F. Scarselli。神经网络分类器的复杂性：浅层与深层架构的比较。*IEEE
    神经网络与学习系统汇刊*，2014。
- en: Biau et al. (2021) G. Biau, M. Sangnier, and U. Tanielian. Some theoretical
    insights into Wasserstein GANs. *Journal of Machine Learning Research*, 22, 2021.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biau 等（2021）G. Biau, M. Sangnier 和 U. Tanielian。Wasserstein GAN 的一些理论见解。*机器学习研究期刊*，22，2021。
- en: Bienstock and Muñoz (2018) D. Bienstock and G. Muñoz. Lp formulations for polynomial
    optimization problems. *SIAM Journal on Optimization*, 28(2):1121–1150, 2018.
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bienstock 和 Muñoz（2018）D. Bienstock 和 G. Muñoz。多项式优化问题的 Lp 形式。*SIAM 优化期刊*，28(2):1121–1150，2018。
- en: Bienstock et al. (2023) D. Bienstock, G. Muñoz, and S. Pokutta. Principled deep
    neural network training through linear programming. *Discrete Optimization*, 49:100795,
    2023.
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bienstock 等（2023）D. Bienstock, G. Muñoz 和 S. Pokutta。通过线性规划进行有原则的深度神经网络训练。*离散优化*，49:100795，2023。
- en: Blum and Rivest (1992) A. L. Blum and R. L. Rivest. Training a 3-node neural
    network is np-complete. *Neural Networks*, 5(1):117–127, 1992.
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blum 和 Rivest（1992）A. L. Blum 和 R. L. Rivest。训练一个 3 节点神经网络是 NP-完全的。*神经网络*，5(1):117–127，1992。
- en: Bohra et al. (2020) P. Bohra, J. Campos, H. Gupta, S. Aziznejad, and M. Unser.
    Learning activation functions in deep (spline) neural networks. *IEEE Open Journal
    of Signal Processing*, 1:295–309, 2020.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bohra 等（2020）P. Bohra, J. Campos, H. Gupta, S. Aziznejad 和 M. Unser。在深度（样条）神经网络中学习激活函数。*IEEE
    开放信号处理期刊*，1:295–309，2020。
- en: Bonami et al. (2015) P. Bonami, A. Lodi, A. Tramontani, and S. Wiese. On mathematical
    programming with indicator constraints. *Mathematical Programming*, 151:191–223,
    2015.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonami 等（2015）P. Bonami, A. Lodi, A. Tramontani 和 S. Wiese。关于具有指示约束的数学规划。*数学规划*，151:191–223，2015。
- en: Boob et al. (2022) D. Boob, S. S. Dey, and G. Lan. Complexity of training ReLU
    neural network. *Discrete Optimization*, 44:100620, 2022.
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boob 等（2022）D. Boob, S. S. Dey 和 G. Lan。训练 ReLU 神经网络的复杂性。*离散优化*，44:100620，2022。
- en: Botoeva et al. (2020) E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and
    R. Misener. Efficient verification of relu-based neural networks via dependency
    analysis. In *AAAI Conference on Artificial Intelligence*, volume 34, pages 3291–3299,
    2020.
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Botoeva 等（2020）E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio 和 R. Misener。通过依赖分析高效验证基于
    ReLU 的神经网络。在 *AAAI 人工智能会议*，第 34 卷，页面 3291–3299，2020。
- en: Bottou et al. (2018) L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods
    for large-scale machine learning. *SIAM Review*, 60(2):223–311, 2018.
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bottou 等（2018）L. Bottou, F. E. Curtis 和 J. Nocedal。大规模机器学习的优化方法。*SIAM 综述*，60(2):223–311，2018。
- en: Bridle (1990) J. S. Bridle. Probabilistic interpretation of feedforward classification
    network outputs, with relationships to statistical pattern recognition. In *Neurocomputing*,
    pages 227–236\. 1990.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bridle（1990）J. S. Bridle。前馈分类网络输出的概率解释及其与统计模式识别的关系。在 *神经计算*，页面 227–236。1990。
- en: 'Bubeck et al. (2015) S. Bubeck et al. Convex optimization: Algorithms and complexity.
    *Foundations and Trends® in Machine Learning*, 2015.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等（2015）S. Bubeck 等。凸优化：算法与复杂性。*机器学习基础与趋势®*，2015。
- en: Bunel et al. (2020a) R. Bunel, A. De Palma, A. Desmaison, K. Dvijotham, P. Kohli,
    P. Torr, and M. Pawan Kumar. Lagrangian decomposition for neural network verification.
    In *Conference on Uncertainty in Artificial Intelligence (UAI)*, volume 124, pages
    370–379, 2020a.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel 等（2020a）R. Bunel, A. De Palma, A. Desmaison, K. Dvijotham, P. Kohli, P.
    Torr 和 M. Pawan Kumar. 神经网络验证的拉格朗日分解。发表于 *Conference on Uncertainty in Artificial
    Intelligence (UAI)*, 第 124 卷, 页码 370–379, 2020a。
- en: Bunel et al. (2020b) R. Bunel, P. Mudigonda, I. Turkaslan, P. Torr, J. Lu, and
    P. Kohli. Branch and bound for piecewise linear neural network verification. *Journal
    of Machine Learning Research*, 21(2020), 2020b.
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel 等（2020b）R. Bunel, P. Mudigonda, I. Turkaslan, P. Torr, J. Lu 和 P. Kohli.
    分段线性神经网络验证的分支定界方法。*Journal of Machine Learning Research*, 21(2020), 2020b。
- en: Bunel et al. (2018) R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. K.
    Mudigonda. A unified view of piecewise linear neural network verification. *Neural
    Information Processing Systems (NeurIPS)*, 31, 2018.
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel 等（2018）R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli 和 P. K. Mudigonda.
    分段线性神经网络验证的统一视角。*Neural Information Processing Systems (NeurIPS)*, 第 31 卷, 2018
    年。
- en: Bunel et al. (2020c) R. R. Bunel, O. Hinder, S. Bhojanapalli, and K. Dvijotham.
    An efficient nonconvex reformulation of stagewise convex optimization problems.
    *Neural Information Processing Systems (NeurIPS)*, 33:8247–8258, 2020c.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bunel 等（2020c）R. R. Bunel, O. Hinder, S. Bhojanapalli 和 K. Dvijotham. 对阶段性凸优化问题的高效非凸重构。*Neural
    Information Processing Systems (NeurIPS)*, 33:8247–8258, 2020c。
- en: Burtea and Tsay (2023) R.-A. Burtea and C. Tsay. Safe deployment of reinforcement
    learning using deterministic optimization over neural networks. In *Computer Aided
    Chemical Engineering*, volume 52, pages 1643–1648\. Elsevier, 2023.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burtea 和 Tsay（2023）R.-A. Burtea 和 C. Tsay. 使用确定性优化进行强化学习的安全部署。发表于 *Computer
    Aided Chemical Engineering*, 第 52 卷, 页码 1643–1648, Elsevier, 2023。
- en: 'Cai et al. (2023) J. Cai, K.-N. Nguyen, N. Shrestha, A. Good, R. Tu, X. Yu,
    S. Zhe, and T. Serra. Getting away with more network pruning: From sparsity to
    geometry and linear regions. In *International Conference on the Integration of
    Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)*,
    2023.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等（2023）J. Cai, K.-N. Nguyen, N. Shrestha, A. Good, R. Tu, X. Yu, S. Zhe
    和 T. Serra. 进一步挖掘网络剪枝的潜力：从稀疏性到几何结构和线性区域。发表于 *International Conference on the Integration
    of Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)*,
    2023。
- en: 'Ceccon et al. (2022) F. Ceccon, J. Jalving, J. Haddad, A. Thebelt, C. Tsay,
    C. D. Laird, and R. Misener. Omlt: Optimization & machine learning toolkit. *Journal
    of Machine Learning Research*, 23(349):1–8, 2022.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ceccon 等（2022）F. Ceccon, J. Jalving, J. Haddad, A. Thebelt, C. Tsay, C. D.
    Laird 和 R. Misener. Omlt: 优化与机器学习工具包。*Journal of Machine Learning Research*, 23(349):1–8,
    2022。'
- en: Charisopoulos and Maragos (2018) V. Charisopoulos and P. Maragos. A tropical
    approach to neural networks with piecewise linear activations. *arXiv:1805.08749*,
    2018.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Charisopoulos 和 Maragos（2018）V. Charisopoulos 和 P. Maragos. 对具有分段线性激活的神经网络的热带方法。*arXiv:1805.08749*,
    2018。
- en: Chaudhry et al. (2020) A. Chaudhry, N. Khan, P. Dokania, and P. Torr. Continual
    learning in low-rank orthogonal subspaces. In *Neural Information Processing Systems
    (NeurIPS)*, volume 33, 2020.
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhry 等（2020）A. Chaudhry, N. Khan, P. Dokania 和 P. Torr. 在低秩正交子空间中的持续学习。发表于
    *Neural Information Processing Systems (NeurIPS)*, 第 33 卷, 2020 年。
- en: Chen et al. (2022a) H. Chen, Y. G. Wang, and H. Xiong. Lower and upper bounds
    for numbers of linear regions of graph convolutional networks. *arXiv:2206.00228*,
    2022a.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2022a）H. Chen, Y. G. Wang 和 H. Xiong. 图卷积网络的线性区域数量的下界和上界。*arXiv:2206.00228*,
    2022a。
- en: Chen et al. (2022b) K.-L. Chen, H. Garudadri, and B. D. Rao. Improved bounds
    on neural complexity for representing piecewise linear functions. In *Neural Information
    Processing Systems (NeurIPS)*, 2022b.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2022b）K.-L. Chen, H. Garudadri 和 B. D. Rao. 表示分段线性函数的神经网络复杂性的改进界限。发表于
    *Neural Information Processing Systems (NeurIPS)*, 2022b。
- en: Chen et al. (2022c) S. Chen, A. R. Klivans, and R. Meka. Learning deep ReLU
    networks is fixed-parameter tractable. In *2021 IEEE 62nd Annual Symposium on
    Foundations of Computer Science (FOCS)*, pages 696–707\. IEEE, 2022c.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2022c）S. Chen, A. R. Klivans 和 R. Meka. 学习深度 ReLU 网络是固定参数可处理的。发表于 *2021
    IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)*, 页码 696–707,
    IEEE, 2022c。
- en: Chen et al. (2020) T. Chen, J.-B. Lasserre, V. Magron, and E. Pauwels. Semialgebraic
    optimization for Lipschitz constants of ReLU networks. In *Neural Information
    Processing Systems (NeurIPS)*, volume 33, 2020.
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2020）T. Chen, J.-B. Lasserre, V. Magron 和 E. Pauwels. ReLU 网络 Lipschitz
    常数的半代数优化。发表于 *Neural Information Processing Systems (NeurIPS)*, 第 33 卷, 2020 年。
- en: 'Chen et al. (2021a) W. Chen, X. Gong, and Z. Wang. Neural architecture search
    on ImageNet in four GPU hours: A theoretically inspired perspective. In *International
    Conference on Learning Representations (ICLR)*, 2021a.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021a) W. Chen, X. Gong, 和 Z. Wang. 在四个 GPU 小时内对 ImageNet 的神经架构搜索：一种理论启发的视角。在*国际学习表征会议（ICLR）*，2021a年。
- en: Chen et al. (2021b) W. Chen, X. Gong, Y. Wei, H. Shi, Z. Yan, Y. Yang, and Z. Wang.
    Understanding and accelerating neural architecture search with training-free and
    theory-grounded metrics. *arXiv:2108.11939*, 2021b.
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021b) W. Chen, X. Gong, Y. Wei, H. Shi, Z. Yan, Y. Yang, 和 Z.
    Wang. 理解和加速神经架构搜索，通过无训练和理论基础的度量。*arXiv:2108.11939*，2021b年。
- en: Cheng et al. (2017) C. Cheng, G. Nührenberg, and H. Ruess. Maximum resilience
    of artificial neural networks. In *Automated Technology for Verification and Analysis
    (ATVA)*, pages 251–268, 2017.
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2017) C. Cheng, G. Nührenberg, 和 H. Ruess. 人工神经网络的最大弹性。在*自动化技术验证与分析（ATVA）*，第251–268页，2017年。
- en: 'Cheng et al. (2018) C.-H. Cheng, G. Nührenberg, C.-H. Huang, and H. Ruess.
    Verification of binarized neural networks via inter-neuron factoring: (short paper).
    In *International Conference on Verified Software: Theories, Tools, and Experiments
    (VSTTE)*, pages 279–290\. Springer, 2018.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2018) C.-H. Cheng, G. Nührenberg, C.-H. Huang, 和 H. Ruess. 通过神经元间分解验证二值化神经网络：（短论文）。在*国际验证软件：理论、工具与实验会议（VSTTE）*，第279–290页，Springer，2018年。
- en: Cheon (2022) M.-S. Cheon. An outer-approximation guided optimization approach
    for constrained neural network inverse problems. *Mathematical Programming*, 196(1-2):173–202,
    2022.
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheon (2022) M.-S. Cheon. 一种外部近似指导的优化方法用于约束神经网络反问题。*数学规划*，196(1-2):173–202，2022年。
- en: 'Chu et al. (2018) L. Chu, X. Hu, J. Hu, L. Wang, and J. Pei. Exact and consistent
    interpretation for piecewise linear neural networks: A closed form solution. In
    *ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)*, 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. (2018) L. Chu, X. Hu, J. Hu, L. Wang, 和 J. Pei. 对分段线性神经网络的精确一致解释：一个封闭形式的解决方案。在*ACM
    SIGKDD知识发现与数据挖掘会议（KDD）*，2018年。
- en: Ciresan et al. (2012) D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. Multi
    column deep neural network for traffic sign classification. *Neural Networks*,
    2012.
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciresan et al. (2012) D. Ciresan, U. Meier, J. Masci, 和 J. Schmidhuber. 用于交通标志分类的多列深度神经网络。*神经网络*，2012年。
- en: Cohan et al. (2022) S. Cohan, N. H. Kim, D. Rolnick, and M. van de Panne. Understanding
    the evolution of linear regions in deep reinforcement learning. In *Neural Information
    Processing Systems (NeurIPS)*, 2022.
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohan et al. (2022) S. Cohan, N. H. Kim, D. Rolnick, 和 M. van de Panne. 理解深度强化学习中线性区域的演变。在*神经信息处理系统（NeurIPS）*，2022年。
- en: Collobert (2004) R. Collobert. *Large Scale Machine Learning*. PhD thesis, University
    Paris 6, 2004.
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collobert (2004) R. Collobert. *大规模机器学习*。博士论文，巴黎第六大学，2004年。
- en: 'Courbariaux et al. (2015) M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:
    Training deep neural networks with binary weights during propagations. *Neural
    Information Processing Systems (NeurIPS)*, 28, 2015.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Courbariaux et al. (2015) M. Courbariaux, Y. Bengio, 和 J.-P. David. BinaryConnect：在传播过程中用二值权重训练深度神经网络。*神经信息处理系统（NeurIPS）*，28，2015年。
- en: Craighero et al. (2020a) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    and M. Antoniotti. Investigating the compositional structure of deep neural networks.
    In *Machine Learning, Optimization, and Data Science (LOD)*, pages 322–334, 2020a.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Craighero et al. (2020a) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    和 M. Antoniotti. 调查深度神经网络的组合结构。在*机器学习、优化与数据科学（LOD）*，第322–334页，2020a年。
- en: Craighero et al. (2020b) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    and M. Antoniotti. Understanding deep learning with activation pattern diagrams.
    In *Proceedings of the Italian Workshop on Explainable Artificial Intelligence
    co-located with 19th International Conference of the Italian Association for Artificial
    Intelligence*, 2020b.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Craighero et al. (2020b) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    和 M. Antoniotti. 通过激活模式图理解深度学习。在*意大利可解释人工智能研讨会论文集*，与第19届意大利人工智能协会国际会议共同举办，2020b年。
- en: Croce and Hein (2018) F. Croce and M. Hein. A randomized gradient-free attack
    on ReLU networks. In *German Conference on Pattern Recognition (GCPR)*, 2018.
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce and Hein (2018) F. Croce 和 M. Hein. 对 ReLU 网络的随机梯度无攻击。在*德国模式识别会议（GCPR）*，2018年。
- en: Croce et al. (2019) F. Croce, M. Andriushchenko, and M. Hein. Provable robustness
    of relu networks via maximization of linear regions. In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2019.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce et al. (2019) F. Croce, M. Andriushchenko, 和 M. Hein. 通过最大化线性区域证明 ReLU
    网络的鲁棒性。在*国际人工智能与统计会议（AISTATS）*，2019年。
- en: Croce et al. (2020) F. Croce, J. Rauber, and M. Hein. Scaling up the randomized
    gradient-free adversarial attack reveals overestimation of robustness using established
    attacks. *International Journal of Computer Vision*, 128:1028–1046, 2020.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croce 等 (2020) F. Croce, J. Rauber, 和 M. Hein. 扩大随机梯度无关对抗攻击揭示了使用已建立攻击的鲁棒性过高估计。*计算机视觉国际期刊*，128:1028–1046,
    2020。
- en: Croxton et al. (2003) K. L. Croxton, B. Gendron, and T. L. Magnanti. A comparison
    of mixed-integer programming models for nonconvex piecewise linear cost minimization
    problems. *Management Science*, 49(9):1268–1273, 2003.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Croxton 等 (2003) K. L. Croxton, B. Gendron, 和 T. L. Magnanti. 对非凸分段线性成本最小化问题的混合整数规划模型的比较。*管理科学*，49(9):1268–1273,
    2003。
- en: 'Curtis and Scheinberg (2017) F. E. Curtis and K. Scheinberg. Optimization methods
    for supervised machine learning: From linear models to deep learning. In *INFORMS
    TutORials in Operations Research*, pages 89–114. INFORMS, 2017.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Curtis 和 Scheinberg (2017) F. E. Curtis 和 K. Scheinberg. 监督机器学习的优化方法：从线性模型到深度学习。见
    *INFORMS 操作研究教程*，第89–114页。INFORMS, 2017。
- en: Cybenko (1989) G. Cybenko. Approximation by superpositions of a sigmoidal function.
    *Mathematics of Control, Signals and Systems*, 1989.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cybenko (1989) G. Cybenko. 由 sigmoid 函数的叠加进行的逼近。*控制、信号与系统的数学*，1989。
- en: Danna et al. (2007) E. Danna, M. Fenelon, Z. Gu, and R. Wunderling. Generating
    multiple solutions for mixed integer programming problems. In *Integer Programming
    and Combinatorial Optimization (IPCO)*, pages 280–294\. 2007.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danna 等 (2007) E. Danna, M. Fenelon, Z. Gu, 和 R. Wunderling. 生成混合整数规划问题的多个解。见
    *整数规划与组合优化 (IPCO)*，第280–294页。2007。
- en: Dantzig (1960) G. B. Dantzig. On the significance of solving linear programming
    problems with some integer variables. *Econometrica, Journal of the Econometric
    Society*, pages 30–44, 1960.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dantzig (1960) G. B. Dantzig. 解决包含一些整数变量的线性规划问题的意义。*经济计量学，经济计量学会期刊*，第30–44页，1960。
- en: Dantzig and Eaves (1973) G. B. Dantzig and B. C. Eaves. Fourier-Motzkin elimination
    and its dual. *Journal of Combinatorial Theory (A)*, 14:288–297, 1973.
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dantzig 和 Eaves (1973) G. B. Dantzig 和 B. C. Eaves. Fourier-Motzkin 消去及其对偶。*组合理论期刊
    (A)*, 14:288–297, 1973。
- en: Dathathri et al. (2020) S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan,
    J. Uesato, R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang,
    et al. Enabling certification of verification-agnostic networks via memory-efficient
    semidefinite programming. *Neural Information Processing Systems (NeurIPS)*, 33:5318–5331,
    2020.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dathathri 等 (2020) S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan, J.
    Uesato, R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang 等.
    通过内存高效的半正定规划启用对验证无关网络的认证。*神经信息处理系统 (NeurIPS)*，33:5318–5331, 2020。
- en: Daubechies et al. (2022) I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and
    G. Petrova. Nonlinear approximation and (deep) ReLU networks. *Constructive Approximation*,
    55:127–172, 2022.
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daubechies 等 (2022) I. Daubechies, R. DeVore, S. Foucart, B. Hanin, 和 G. Petrova.
    非线性逼近与 (深度) ReLU 网络。*构造性逼近*，55:127–172, 2022。
- en: De Palma et al. (2021) A. De Palma, H. Behl, R. R. Bunel, P. Torr, and M. P.
    Kumar. Scaling the convex barrier with active sets. In *International Conference
    on Learning Representations (ICLR)*, 2021.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Palma 等 (2021) A. De Palma, H. Behl, R. R. Bunel, P. Torr, 和 M. P. Kumar.
    使用活动集进行凸障碍的缩放。见 *国际学习表示会议 (ICLR)*，2021。
- en: 'Delarue et al. (2020) A. Delarue, R. Anderson, and C. Tjandraatmadja. Reinforcement
    learning with combinatorial actions: An application to vehicle routing. *Neural
    Information Processing Systems (NeurIPS)*, 33:609–620, 2020.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delarue 等 (2020) A. Delarue, R. Anderson, 和 C. Tjandraatmadja. 具有组合动作的强化学习：应用于车辆路径规划。*神经信息处理系统
    (NeurIPS)*，33:609–620, 2020。
- en: Deng et al. (2020) Y. Deng, X. Zheng, T. Zhang, C. Chen, G. Lou, and M. Kim.
    An analysis of adversarial attacks and defenses on autonomous driving models.
    In *2020 IEEE international conference on pervasive computing and communications
    (PerCom)*, pages 1–10\. IEEE, 2020.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2020) Y. Deng, X. Zheng, T. Zhang, C. Chen, G. Lou, 和 M. Kim. 对自动驾驶模型的对抗攻击和防御的分析。见
    *2020 IEEE 国际普适计算与通信会议 (PerCom)*，第1–10页。IEEE，2020。
- en: 'Devlin et al. (2019) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    *Conference of the North American Chapter of the Association for Computational
    Linguistics (NAACL)*, 2019.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等 (2019) J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova. BERT：用于语言理解的深度双向变换器的预训练。见
    *北美计算语言学协会会议 (NAACL)*，2019。
- en: Dey et al. (2020) S. S. Dey, G. Wang, and Y. Xie. Approximation algorithms for
    training one-node relu neural networks. *IEEE Transactions on Signal Processing*,
    68:6696–6706, 2020.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dey et al. (2020) S. S. Dey, G. Wang, 和 Y. Xie. 训练单节点relu神经网络的近似算法。*IEEE信号处理学报*，68:6696–6706，2020。
- en: Dubey et al. (2021) S. R. Dubey, S. K. Singh, and B. B. Chaudhuri. A comprehensive
    survey and performance analysis of activation functions in deep learning. *arXiv:2109.14545*,
    2021.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey et al. (2021) S. R. Dubey, S. K. Singh, 和 B. B. Chaudhuri. 深度学习中激活函数的综合调查与性能分析。*arXiv:2109.14545*，2021。
- en: 'Dutta et al. (2018) S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output
    range analysis for deep feedforward networks. In *NASA Formal Methods: 10th International
    Symposium, (NFM)*, 2018.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dutta et al. (2018) S. Dutta, S. Jha, S. Sankaranarayanan, 和 A. Tiwari. 深度前馈网络的输出范围分析。发表于*NASA形式化方法：第10届国际研讨会
    (NFM)*，2018。
- en: Dvijotham et al. (2018a) K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
    B. O’Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned
    verifiers. *arXiv:1805.10265*, 2018a.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvijotham et al. (2018a) K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
    B. O’Donoghue, J. Uesato, 和 P. Kohli. 使用学习的验证器训练经过验证的学习者。*arXiv:1805.10265*，2018a。
- en: Dvijotham et al. (2018b) K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, and
    P. Kohli. A dual approach to scalable verification of deep networks. In *Conference
    on Uncertainty in Artificial Intelligence (UAI)*, volume 1, page 3, 2018b.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dvijotham et al. (2018b) K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, 和
    P. Kohli. 一种可扩展深度网络验证的双重方法。发表于*人工智能不确定性会议 (UAI)*，第1卷，第3页，2018b。
- en: Dym et al. (2020) N. Dym, B. Sober, and I. Daubechies. Expression of fractals
    through neural network functions. *IEEE Journal on Selected Areas in Information
    Theory*, 1(1):57–66, 2020.
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dym et al. (2020) N. Dym, B. Sober, 和 I. Daubechies. 通过神经网络函数表达分形。*IEEE信息理论选择领域期刊*，1(1):57–66，2020。
- en: Ehlers (2017) R. Ehlers. Formal verification of piece-wise linear feed-forward
    neural networks. In *Automated Technology for Verification and Analysis (ATVA)*,
    pages 269–286\. Springer, 2017.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ehlers (2017) R. Ehlers. 分段线性前馈神经网络的形式化验证。发表于*自动化技术验证与分析 (ATVA)*，第269–286页。Springer，2017。
- en: ElAraby et al. (2020) M. ElAraby, G. Wolf, and M. Carvalho. Identifying efficient
    sub-networks using mixed integer programming. In *OPT Workshop*, 2020.
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ElAraby et al. (2020) M. ElAraby, G. Wolf, 和 M. Carvalho. 使用混合整数规划识别高效子网络。发表于*OPT研讨会*，2020。
- en: 'Elsken et al. (2019) T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture
    search: A survey. *Journal of Machine Learning Research*, 20:1–21, 2019.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elsken et al. (2019) T. Elsken, J. H. Metzen, 和 F. Hutter. 神经架构搜索：一项调查。*机器学习研究杂志*，20:1–21，2019。
- en: 'Ergen and Pilanci (2020) T. Ergen and M. Pilanci. Convex geometry of two-layer
    relu networks: Implicit autoencoding and interpretable models. In S. Chiappa and
    R. Calandra, editors, *International Conference on Artificial Intelligence and
    Statistics*, volume 108 of *Proceedings of Machine Learning Research*, pages 4024–4033\.
    PMLR, 26–28 Aug 2020.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2020) T. Ergen 和 M. Pilanci. 两层relu网络的凸几何：隐式自编码和可解释模型。编辑：S.
    Chiappa 和 R. Calandra，*国际人工智能与统计会议*，*机器学习研究论文集*第108卷，第4024–4033页。PMLR，2020年8月26–28日。
- en: Ergen and Pilanci (2021a) T. Ergen and M. Pilanci. Convex geometry and duality
    of over-parameterized neural networks. *The Journal of Machine Learning Research*,
    22(1):9646–9708, 2021a.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021a) T. Ergen 和 M. Pilanci. 凸几何和超参数神经网络的对偶性。*机器学习研究杂志*，22(1):9646–9708，2021a。
- en: 'Ergen and Pilanci (2021b) T. Ergen and M. Pilanci. Global optimality beyond
    two layers: Training deep relu networks via convex programs. In *International
    Conference on Machine Learning (ICLR)*, pages 2993–3003\. PMLR, 2021b.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021b) T. Ergen 和 M. Pilanci. 超越两层的全局最优性：通过凸程序训练深度relu网络。发表于*国际机器学习会议
    (ICLR)*，第2993–3003页。PMLR，2021b。
- en: 'Ergen and Pilanci (2021c) T. Ergen and M. Pilanci. Implicit convex regularizers
    of cnn architectures: Convex optimization of two-and three-layer networks in polynomial
    time. In *International Conference on Learning Representations (ICLR)*, 2021c.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021c) T. Ergen 和 M. Pilanci. CNN架构的隐式凸正则化：在多项式时间内对两层和三层网络的凸优化。发表于*国际学习表示会议
    (ICLR)*，2021c。
- en: 'Ergen and Pilanci (2021d) T. Ergen and M. Pilanci. Path regularization: A convexity
    and sparsity inducing regularization for parallel relu networks. *arXiv preprint
    arXiv:2110.09548*, 2021d.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen and Pilanci (2021d) T. Ergen 和 M. Pilanci. 路径正则化：一种为并行relu网络引入凸性和稀疏性的正则化。*arXiv预印本
    arXiv:2110.09548*，2021d。
- en: Ergen and Pilanci (2021e) T. Ergen and M. Pilanci. Revealing the structure of
    deep neural networks via convex duality. In *International Conference on Machine
    Learning*, pages 3004–3014\. PMLR, 2021e.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen 和 Pilanci (2021e) T. Ergen 和 M. Pilanci. 通过凸对偶性揭示深度神经网络的结构。发表于 *International
    Conference on Machine Learning*, 页码 3004–3014\. PMLR, 2021e。
- en: 'Ergen et al. (2022) T. Ergen, A. Sahiner, B. Ozturkler, J. M. Pauly, M. Mardani,
    and M. Pilanci. Demystifying batch normalization in reLU networks: Equivalent
    convex optimization models and implicit regularization. In *International Conference
    on Learning Representations*, 2022.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen 等人 (2022) T. Ergen, A. Sahiner, B. Ozturkler, J. M. Pauly, M. Mardani,
    和 M. Pilanci. 揭开 ReLU 网络中的批量归一化的神秘面纱：等效的凸优化模型和隐式正则化。发表于 *International Conference
    on Learning Representations*, 2022。
- en: Ergen et al. (2023) T. Ergen, H. I. Gulluk, J. Lacotte, and M. Pilanci. Globally
    optimal training of neural networks with threshold activation functions. In *International
    Conference on Learning Representations (ICLR)*, 2023.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ergen 等人 (2023) T. Ergen, H. I. Gulluk, J. Lacotte, 和 M. Pilanci. 全局最优的神经网络训练与阈值激活函数。发表于
    *International Conference on Learning Representations (ICLR)*, 2023。
- en: Eykholt et al. (2018) K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati,
    C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust physical-world attacks on deep
    learning visual classification. In *Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eykholt 等人 (2018) K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C.
    Xiao, A. Prakash, T. Kohno, 和 D. Song. 对深度学习视觉分类的强健物理世界攻击。发表于 *Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2018年6月。
- en: Fan et al. (2020) F.-L. Fan, R. Lai, and G. Wang. Quasi-equivalence of width
    and depth of neural networks. *arXiv:2002.02515*, 2020.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2020) F.-L. Fan, R. Lai, 和 G. Wang. 神经网络的宽度和深度的准等价性。*arXiv:2002.02515*,
    2020。
- en: Fan et al. (2023) F.-L. Fan, W. Huang, X. Zhong, L. Ruan, T. Zeng, H. Xiong,
    and F. Wang. Deep relu networks have surprisingly simple polytopes. *arXiv:2305.09145*,
    2023.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2023) F.-L. Fan, W. Huang, X. Zhong, L. Ruan, T. Zeng, H. Xiong, 和 F.
    Wang. 深度 ReLU 网络具有惊人简单的多面体。*arXiv:2305.09145*, 2023。
- en: Fazlyab et al. (2019) M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J.
    Pappas. Efficient and accurate estimation of Lipschitz constants for deep neural
    networks. In *Neural Information Processing Systems (NeurIPS)*, volume 32, 2019.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fazlyab 等人 (2019) M. Fazlyab, A. Robey, H. Hassani, M. Morari, 和 G. J. Pappas.
    深度神经网络的 Lipschitz 常数的高效且准确的估计。发表于 *Neural Information Processing Systems (NeurIPS)*,
    第32卷, 2019。
- en: Fazlyab et al. (2020) M. Fazlyab, M. Morari, and G. J. Pappas. Safety verification
    and robustness analysis of neural networks via quadratic constraints and semidefinite
    programming. *IEEE Transactions on Automatic Control*, 67(1):1–15, 2020.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fazlyab 等人 (2020) M. Fazlyab, M. Morari, 和 G. J. Pappas. 通过二次约束和半正定规划进行神经网络的安全性验证和鲁棒性分析。*IEEE
    Transactions on Automatic Control*, 67(1):1–15, 2020。
- en: 'Ferlez and Shoukry (2020) J. Ferlez and Y. Shoukry. AReN: Assured ReLU NN architecture
    for model predictive control of LTI systems. In *HSCC*, 2020.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferlez 和 Shoukry (2020) J. Ferlez 和 Y. Shoukry. AReN：用于 LTI 系统模型预测控制的可靠 ReLU
    NN 架构。发表于 *HSCC*, 2020。
- en: Ferrari et al. (2022) C. Ferrari, M. N. Mueller, N. Jovanović, and M. Vechev.
    Complete verification via multi-neuron relaxation guided branch-and-bound. In
    *International Conference on Learning Representations (ICLR)*, 2022.
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrari 等人 (2022) C. Ferrari, M. N. Mueller, N. Jovanović, 和 M. Vechev. 通过多神经元放松引导的分支界定实现完整验证。发表于
    *International Conference on Learning Representations (ICLR)*, 2022。
- en: Finlayson et al. (2019) S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain,
    A. L. Beam, and I. S. Kohane. Adversarial attacks on medical machine learning.
    *Science*, 363(6433):1287–1289, 2019.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finlayson 等人 (2019) S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain, A.
    L. Beam, 和 I. S. Kohane. 对医学机器学习的对抗攻击。*Science*, 363(6433):1287–1289, 2019。
- en: Fischetti and Jo (2018) M. Fischetti and J. Jo. Deep neural networks and mixed
    integer linear optimization. *Constraints*, 2018.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischetti 和 Jo (2018) M. Fischetti 和 J. Jo. 深度神经网络和混合整数线性优化。*Constraints*, 2018。
- en: Fourier (1826) J. Fourier. Solution d’une question particuliére du calcul des
    inégalités. *Nouveau Bulletin des Sciences par la Société Philomatique de Paris*,
    1826.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fourier (1826) J. Fourier. 特定不等式计算问题的解决方案。*Nouveau Bulletin des Sciences par
    la Société Philomatique de Paris*, 1826。
- en: Frank et al. (1956) M. Frank, P. Wolfe, et al. An algorithm for quadratic programming.
    *Naval Research Logistics Quarterly*, 3(1-2):95–110, 1956.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frank 等人 (1956) M. Frank, P. Wolfe, 等. 二次规划算法。*Naval Research Logistics Quarterly*,
    3(1-2):95–110, 1956。
- en: Froese and Hertrich (2023) V. Froese and C. Hertrich. Training neural networks
    is NP-hard in fixed dimension. *arXiv preprint arXiv:2303.17045*, 2023.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Froese 和 Hertrich (2023) V. Froese 和 C. Hertrich. 在固定维度下训练神经网络是 NP 难的。*arXiv
    preprint arXiv:2303.17045*, 2023。
- en: Froese et al. (2022) V. Froese, C. Hertrich, and R. Niedermeier. The computational
    complexity of relu network training parameterized by data dimensionality. *Journal
    of Artificial Intelligence Research*, 74:1775–1790, 2022.
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Froese 等（2022）V. Froese、C. Hertrich 和 R. Niedermeier。由数据维度参数化的relu网络训练的计算复杂度。*人工智能研究杂志*，74：1775–1790，2022年。
- en: 'Fukushima (1980) K. Fukushima. Neocognitron: A self-organizing neural network
    model for a mechanism of pattern recognition unaffected by shift in position.
    *Biological Cybernetics*, 36(4):193–202, 1980.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fukushima（1980）K. Fukushima。Neocognitron：一种自组织神经网络模型，用于不受位置偏移影响的模式识别机制。*生物计算学*，36(4)：193–202，1980年。
- en: Funahashi (1989) K.-I. Funahashi. On the approximate realization of continuous
    mappings by neural networks. *Neural Networks*, 2(3), 1989.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Funahashi（1989）K.-I. Funahashi。关于神经网络对连续映射的近似实现。*神经网络*，2(3)，1989年。
- en: Gamba et al. (2020) M. Gamba, S. Carlsson, H. Azizpour, and M. Björkman. Hyperplane
    arrangements of trained ConvNets are biased. *arXiv:2003.07797*, 2020.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamba 等（2020）M. Gamba、S. Carlsson、H. Azizpour 和 M. Björkman。训练的ConvNets的超平面排列存在偏差。*arXiv:2003.07797*，2020年。
- en: Gamba et al. (2022) M. Gamba, A. Chmielewski-Anders, J. Sullivan, H. Azizpour,
    and M. Björkman. Are all linear regions created equal? In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2022.
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamba 等（2022）M. Gamba、A. Chmielewski-Anders、J. Sullivan、H. Azizpour 和 M. Björkman。所有线性区域是否都相同？在*国际人工智能与统计会议（AISTATS）*，2022年。
- en: 'Gambella et al. (2021) C. Gambella, B. Ghaddar, and J. Naoum-Sawaya. Optimization
    problems for machine learning: A survey. *European Journal of Operational Research*,
    290(3):807–828, 2021.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gambella 等（2021）C. Gambella、B. Ghaddar 和 J. Naoum-Sawaya。机器学习中的优化问题：综述。*欧洲运筹学杂志*，290(3)：807–828，2021年。
- en: 'Gao et al. (2020) J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and
    C. Schmid. VectorNet: Encoding HD maps and agent dynamics from vectorized representation.
    In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2020）J. Gao、C. Sun、H. Zhao、Y. Shen、D. Anguelov、C. Li 和 C. Schmid。VectorNet：从向量化表示中编码HD地图和代理动态。在*计算机视觉与模式识别会议（CVPR）*，2020年。
- en: Geißler et al. (2012) B. Geißler, A. Martin, A. Morsi, and L. Schewe. Using
    piecewise linear functions for solving MINLPs. In *Mixed Integer Nonlinear Programming*,
    pages 287–314. Springer, 2012.
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geißler 等（2012）B. Geißler、A. Martin、A. Morsi 和 L. Schewe。使用分段线性函数解决MINLPs。在*混合整数非线性规划*，第287–314页。Springer，2012年。
- en: Glass et al. (2021) L. Glass, W. Hilali, and O. Nelles. Compressing interpretable
    representations of piecewise linear neural networks using neuro-fuzzy models.
    In *IEEE Symposium Series on Computational Intelligence (SSCI)*, 2021.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glass 等（2021）L. Glass、W. Hilali 和 O. Nelles。利用神经模糊模型压缩分段线性神经网络的可解释表示。在*IEEE计算智能研讨会系列（SSCI）*，2021年。
- en: Glorot et al. (2011) X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier
    neural networks. In *International Conference on Artificial Intelligence and Statistics
    (AISTATS)*, 2011.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glorot 等（2011）X. Glorot、A. Bordes 和 Y. Bengio。深度稀疏修正神经网络。在*国际人工智能与统计会议（AISTATS）*，2011年。
- en: Goebbels (2021) S. Goebbels. Training of ReLU activated multilayerd neural networks
    with mixed integer linear programs. Technical report, Hochschule Niederrhein,
    Fachbereich Elektrotechnik & Informatik, 2021.
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goebbels（2021）S. Goebbels。使用混合整数线性规划训练ReLU激活的多层神经网络。技术报告，霍赫施库尔·尼德拉因，电气工程与计算机科学系，2021年。
- en: Goel et al. (2021) S. Goel, A. Klivans, P. Manurangsi, and D. Reichman. Tight
    hardness results for training depth-2 relu networks. In *12th Innovations in Theoretical
    Computer Science Conference (ITCS 2021)*. Schloss Dagstuhl-Leibniz-Zentrum für
    Informatik, 2021.
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goel 等（2021）S. Goel、A. Klivans、P. Manurangsi 和 D. Reichman。训练深度-2 relu网络的严格难度结果。在*第12届理论计算机科学创新会议（ITCS
    2021）*。Schloss Dagstuhl-Leibniz-Zentrum für Informatik，2021年。
- en: Goerigk and Kurtz (2023) M. Goerigk and J. Kurtz. Data-driven robust optimization
    using deep neural networks. *Computers & Operations Research*, 151:106087, 2023.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goerigk 和 Kurtz（2023）M. Goerigk 和 J. Kurtz。使用深度神经网络进行数据驱动的鲁棒优化。*计算机与运筹研究*，151：106087，2023年。
- en: Goodfellow et al. (2013) I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville,
    and Y. Bengio. Maxout networks. In *International Conference on Machine Learning
    (ICML)*, 2013.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2013）I. Goodfellow、D. Warde-Farley、M. Mirza、A. Courville 和 Y. Bengio。Maxout
    网络。在*国际机器学习会议（ICML）*，2013年。
- en: Goodfellow et al. (2015) I. Goodfellow, J. Shlens, and C. Szegedy. Explaining
    and harnessing adversarial examples. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等（2015）I. Goodfellow、J. Shlens 和 C. Szegedy。解释和利用对抗样本。在*国际学习表示会议（ICLR）*，2015年。
- en: Goodfellow et al. (2016) I. Goodfellow, Y. Bengio, and A. Courville. *Deep learning*.
    MIT press, 2016.
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2016) I. Goodfellow, Y. Bengio, 和 A. Courville. *深度学习*。MIT 出版社，2016。
- en: Goodfellow et al. (2014) I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
    D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial
    nets. In *Neural Information Processing Systems (NeurIPS)*, volume 27, 2014.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等 (2014) I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D.
    Warde-Farley, S. Ozair, A. Courville, 和 Y. Bengio. 生成对抗网络。发表于 *神经信息处理系统 (NeurIPS)*，第27卷，2014。
- en: Gopinath et al. (2019) D. Gopinath, H. Converse, C. S. Pasareanu, and A. Taly.
    Property inference for deep neural networks. In *IEEE/ACM International Conference
    on Automated Software Engineering (ASE)*, 2019.
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gopinath 等 (2019) D. Gopinath, H. Converse, C. S. Pasareanu, 和 A. Taly. 深度神经网络的属性推断。发表于
    *IEEE/ACM 自动化软件工程国际会议 (ASE)*，2019。
- en: Goujon et al. (2022) A. Goujon, A. Etemadi, and M. Unser. The role of depth,
    width, and activation complexity in the number of linear regions of neural networks.
    *arXiv:2206.08615*, 2022.
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goujon 等 (2022) A. Goujon, A. Etemadi, 和 M. Unser. 深度、宽度和激活复杂性在神经网络线性区域数量中的作用。*arXiv:2206.08615*，2022。
- en: Gowal et al. (2018) S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin,
    J. Uesato, R. Arandjelovic, T. Mann, and P. Kohli. On the effectiveness of interval
    bound propagation for training verifiably robust models. *arXiv:1810.12715*, 2018.
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gowal 等 (2018) S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato,
    R. Arandjelovic, T. Mann, 和 P. Kohli. 关于区间边界传播在训练可验证鲁棒模型中的有效性。*arXiv:1810.12715*，2018。
- en: Graves and Jaitly (2014) A. Graves and N. Jaitly. Towards end-to-end speech
    recognition with recurrent neural networks. In *International Conference on Machine
    Learning (ICML)*, 2014.
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graves 和 Jaitly (2014) A. Graves 和 N. Jaitly. 通过递归神经网络实现端到端语音识别。发表于 *国际机器学习会议
    (ICML)*，2014。
- en: Grigsby and Lindsey (2022) J. E. Grigsby and K. Lindsey. On transversality of
    bent hyperplane arrangements and the topological expressiveness of ReLU neural
    networks. *SIAM Journal on Applied Algebra and Geometry*, 6(2), 2022.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grigsby 和 Lindsey (2022) J. E. Grigsby 和 K. Lindsey. 关于弯曲超平面排列的横截性以及 ReLU 神经网络的拓扑表达能力。*SIAM
    应用代数与几何期刊*，6(2)，2022。
- en: Grigsby et al. (2023) J. E. Grigsby, K. Lindsey, and D. Rolnick. Hidden symmetries
    of ReLU networks. In *International Conference on Machine Learning (ICML)*, 2023.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grigsby 等 (2023) J. E. Grigsby, K. Lindsey, 和 D. Rolnick. ReLU 网络的隐藏对称性。发表于
    *国际机器学习会议 (ICML)*，2023。
- en: Grimstad and Andersson (2019) B. Grimstad and H. Andersson. ReLU networks as
    surrogate models in mixed-integer linear programs. *Computers & Chemical Engineering*,
    131:106580, 2019.
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grimstad 和 Andersson (2019) B. Grimstad 和 H. Andersson. ReLU 网络作为混合整数线性规划中的代理模型。*计算机与化学工程*，131:106580，2019。
- en: 'Grossmann and Ruiz (2012) I. E. Grossmann and J. P. Ruiz. Generalized disjunctive
    programming: A framework for formulation and alternative algorithms for MINLP
    optimization. In *Mixed Integer Nonlinear Programming*, pages 93–115, New York,
    NY, 2012\. Springer New York.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grossmann 和 Ruiz (2012) I. E. Grossmann 和 J. P. Ruiz. 广义析取编程：MINLP 优化的框架及替代算法。发表于
    *混合整数非线性编程*，第93-115页，纽约，NY，2012。Springer New York。
- en: Hahnloser et al. (2000) R. Hahnloser, R. Sarpeshkar, M. Mahowald, R. Douglas,
    and S. Seung. Digital selection and analogue amplification coexist in a cortex-inspired
    silicon circuit. *Nature*, 405, 2000.
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hahnloser 等 (2000) R. Hahnloser, R. Sarpeshkar, M. Mahowald, R. Douglas, 和 S.
    Seung. 数字选择与模拟放大在类似皮层的硅电路中共存。*自然*，405，2000。
- en: Han and Gómez (2021) S. Han and A. Gómez. Single-neuron convexification for
    binarized neural networks, 2021. URL https://optimization-online.org/?p=17148.
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 和 Gómez (2021) S. Han 和 A. Gómez. 针对二值神经网络的单神经元凸化，2021。网址 https://optimization-online.org/?p=17148。
- en: Hanin and Rolnick (2019a) B. Hanin and D. Rolnick. Complexity of linear regions
    in deep networks. In *International Conference on Machine Learning (ICML)*, 2019a.
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin 和 Rolnick (2019a) B. Hanin 和 D. Rolnick. 深度网络中线性区域的复杂性。发表于 *国际机器学习会议 (ICML)*，2019a。
- en: Hanin and Rolnick (2019b) B. Hanin and D. Rolnick. Deep ReLU networks have surprisingly
    few activation patterns. In *Neural Information Processing Systems (NeurIPS)*,
    volume 32, 2019b.
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin 和 Rolnick (2019b) B. Hanin 和 D. Rolnick. 深度 ReLU 网络具有令人惊讶的少量激活模式。发表于 *神经信息处理系统
    (NeurIPS)*，第32卷，2019b。
- en: Hanin and Sellke (2017) B. Hanin and M. Sellke. Approximating continuous functions
    by ReLU nets of minimal width. *arXiv:1710.11278*, 2017.
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin 和 Sellke (2017) B. Hanin 和 M. Sellke. 通过 ReLU 网络的最小宽度逼近连续函数。*arXiv:1710.11278*，2017。
- en: 'Hashemi et al. (2021) V. Hashemi, P. Kouvaros, and A. Lomuscio. OSIP: Tightened
    bound propagation for the verification of ReLU neural networks. In *International
    Conference on Software Engineering and Formal Methods (SEFM)*, pages 463–480\.
    Springer, 2021.'
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hashemi 等（2021）V. Hashemi, P. Kouvaros, 和 A. Lomuscio。OSIP：用于 ReLU 神经网络验证的收紧界限传播。见于
    *国际软件工程与形式方法会议 (SEFM)*，第463–480页。施普林格，2021年。
- en: 'He et al. (2021) F. He, S. Lei, J. Ji, and D. Tao. Neural networks behave as
    hash encoders: An empirical study. *arXiv:2101.05490*, 2021.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2021）F. He, S. Lei, J. Ji, 和 D. Tao。神经网络表现为哈希编码器：一项实证研究。*arXiv:2101.05490*，2021年。
- en: He et al. (2020) J. He, L. Li, J. Xu, and C. Zheng. ReLU deep neural networks
    and linear finite elements. *Journal of Computational Mathematics*, 38:502–527,
    2020.
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2020）J. He, L. Li, J. Xu, 和 C. Zheng。ReLU 深度神经网络与线性有限元。*计算数学杂志*，38:502–527，2020年。
- en: 'He et al. (2015) K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:
    Surpassing human-level performance on ImageNet classification. In *IEEE International
    Conference on Computer Vision (ICCV)*, 2015.'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2015）K. He, X. Zhang, S. Ren, 和 J. Sun。深入探讨修正器：在 ImageNet 分类上超越人类水平的性能。见于
    *IEEE 国际计算机视觉会议 (ICCV)*，2015年。
- en: He et al. (2016) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
    for image recognition. In *Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016.
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2016）K. He, X. Zhang, S. Ren, 和 J. Sun。用于图像识别的深度残差学习。见于 *计算机视觉与模式识别会议 (CVPR)*，2016年。
- en: 'Henriksen and Lomuscio (2021) P. Henriksen and A. Lomuscio. DEEPSPLIT: an efficient
    splitting method for neural network verification via indirect effect analysis.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    2549–2555, 2021.'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henriksen 和 Lomuscio（2021）P. Henriksen 和 A. Lomuscio。DEEPSPLIT：一种通过间接效应分析进行神经网络验证的高效拆分方法。见于
    *国际人工智能联合会议 (IJCAI)*，第2549–2555页，2021年。
- en: Henriksen et al. (2022) P. Henriksen, F. Leofante, and A. Lomuscio. Repairing
    misclassifications in neural networks using limited data. In *ACM/SIGAPP Symposium
    On Applied Computing (SAC)*, 2022.
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henriksen 等（2022）P. Henriksen, F. Leofante, 和 A. Lomuscio。使用有限数据修正神经网络中的错误分类。见于
    *ACM/SIGAPP 应用计算研讨会 (SAC)*，2022年。
- en: Hertrich et al. (2021) C. Hertrich, A. Basu, M. D. Summa, and M. Skutella. Towards
    lower bounds on the depth of ReLU neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, 2021.
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hertrich 等（2021）C. Hertrich, A. Basu, M. D. Summa, 和 M. Skutella。朝着 ReLU 神经网络深度下界的方向前进。见于
    *神经信息处理系统 (NeurIPS)*，2021年。
- en: Hinton et al. (2012) G. Hinton, L. Deng, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for
    acoustic modeling in speech recognition. *IEEE Signal Processing Magazine*, 2012.
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2012）G. Hinton, L. Deng, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. Sainath, 和 B. Kingsbury。用于语音识别的深度神经网络。*IEEE 信号处理杂志*，2012年。
- en: Hinz (2021) P. Hinz. Using activation histograms to bound the number of affine
    regions in ReLU feed-forward neural networks. *arXiv:2103.17174*, 2021.
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinz（2021）P. Hinz。使用激活直方图来限制 ReLU 前馈神经网络中的仿射区域数量。*arXiv:2103.17174*，2021年。
- en: Hinz and van de Geer (2019) P. Hinz and S. van de Geer. A framework for the
    construction of upper bounds on the number of affine linear regions of ReLU feed-forward
    neural networks. *IEEE Transactions on Information Theory*, 65(11):7304–7324,
    2019.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinz 和 van de Geer（2019）P. Hinz 和 S. van de Geer。构建 ReLU 前馈神经网络的仿射线性区域数量上界的框架。*IEEE
    信息理论汇刊*，65(11):7304–7324，2019年。
- en: Hochreiter and Schmidhuber (1997) S. Hochreiter and J. Schmidhuber. Long short-term
    memory. *Neural Computation*, 9(8):1735–1780, 1997.
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber（1997）S. Hochreiter 和 J. Schmidhuber。长短期记忆。*神经计算*，9(8):1735–1780，1997年。
- en: Hopfield (1982) J. Hopfield. Neural networks and physical systems with emergent
    collective computational abilities. *Proceedings of the National Academy of Sciences*,
    79:2554–2558, 1982.
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hopfield（1982）J. Hopfield。具有新兴集体计算能力的神经网络和物理系统。*国家科学院学报*，79:2554–2558，1982年。
- en: Hornik et al. (1989) K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward
    networks are universal approximators. *Neural Networks*, 2(5), 1989.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hornik 等（1989）K. Hornik, M. Stinchcombe, 和 H. White。多层前馈网络是通用逼近器。*神经网络*，2(5)，1989年。
- en: Hu et al. (2020a) T. Hu, Z. Shang, and G. Cheng. Sharp rate of convergence for
    deep neural network classifiers under the teacher-student setting. *arXiv:2001.06892*,
    2020a.
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2020a）T. Hu, Z. Shang, 和 G. Cheng。在教师-学生设置下深度神经网络分类器的收敛速度。*arXiv:2001.06892*，2020年。
- en: Hu et al. (2020b) X. Hu, W. Liu, J. Bian, and J. Pei. Measuring model complexity
    of neural networks with curve activation functions. In *ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining (KDD)*, 2020b.
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2020b) X. Hu, W. Liu, J. Bian 和 J. Pei. 用曲线激活函数测量神经网络的模型复杂性。发表于 *ACM SIGKDD
    知识发现与数据挖掘会议（KDD）*，2020b年。
- en: 'Hu et al. (2021) X. Hu, L. Chu, J. Pei, W. Liu, and J. Bian. Model complexity
    of deep learning: a survey. *Knowledge and Information Systems*, 63:2585–2619,
    2021.'
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等 (2021) X. Hu, L. Chu, J. Pei, W. Liu 和 J. Bian. 深度学习的模型复杂性：综述。*知识与信息系统*，63:2585–2619，2021年。
- en: 'Huang et al. (2020) X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo,
    M. Wu, and X. Yi. A survey of safety and trustworthiness of deep neural networks:
    Verification, testing, adversarial attack and defence, and interpretability. *Computer
    Science Review*, 37:100270, 2020.'
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2020) X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo, M.
    Wu 和 X. Yi. 深度神经网络的安全性和可信度综述：验证、测试、对抗攻击与防御，以及可解释性。*计算机科学评论*，37:100270，2020年。
- en: 'Huchette and Vielma (2022) J. Huchette and J. P. Vielma. Nonconvex piecewise
    linear functions: Advanced formulations and simple modeling tools. *Operations
    Research*, 2022.'
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huchette 和 Vielma (2022) J. Huchette 和 J. P. Vielma. 非凸分段线性函数：高级形式化和简单建模工具。*运筹学*，2022年。
- en: Huster et al. (2018) T. Huster, C.-Y. J. Chiang, and R. Chadha. Limitations
    of the Lipschitz constant as a defense against adversarial examples. In *ECML
    PKDD Workshops*, 2018.
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huster 等 (2018) T. Huster, C.-Y. J. Chiang 和 R. Chadha. Lipschitz 常数作为对抗示例防御的局限性。发表于
    *ECML PKDD 研讨会*，2018年。
- en: Hwang and Heinecke (2020) W.-L. Hwang and A. Heinecke. Un-rectifying non-linear
    networks for signal representation. *IEEE Transactions on Signal Processing*,
    68:196–210, 2020.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hwang 和 Heinecke (2020) W.-L. Hwang 和 A. Heinecke. 用于信号表示的非线性网络的去整流。*IEEE 信号处理学报*，68:196–210，2020年。
- en: Icarte et al. (2019) R. T. Icarte, L. Illanes, M. P. Castro, A. A. Cire, S. A.
    McIlraith, and J. C. Beck. Training binarized neural networks using mip and cp.
    In *International Conference on Principles and Practice of Constraint Programming*,
    pages 401–417\. Springer, 2019.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Icarte 等 (2019) R. T. Icarte, L. Illanes, M. P. Castro, A. A. Cire, S. A. McIlraith
    和 J. C. Beck. 使用 mip 和 cp 训练二值化神经网络。发表于 *约束编程原理与实践国际会议*，401–417页。Springer，2019年。
- en: 'Ioffe and Szegedy (2015) S. Ioffe and C. Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In *International
    Conference on Machine Learning (ICML)*, 2015.'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ioffe 和 Szegedy (2015) S. Ioffe 和 C. Szegedy. 批量归一化：通过减少内部协变量偏移来加速深度网络训练。发表于
    *国际机器学习会议（ICML）*，2015年。
- en: Jeroslow and Lowe (1984) R. G. Jeroslow and J. K. Lowe. *Modelling with integer
    variables*. Springer, 1984.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeroslow 和 Lowe (1984) R. G. Jeroslow 和 J. K. Lowe. *整数变量建模*。Springer，1984年。
- en: Jia and Rinard (2020) K. Jia and M. Rinard. Efficient exact verification of
    binarized neural networks. *Neural Information Processing Systems (NeurIPS)*,
    33:1782–1795, 2020.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 和 Rinard (2020) K. Jia 和 M. Rinard. 高效的二值化神经网络精确验证。*神经信息处理系统会议（NeurIPS）*，33:1782–1795，2020年。
- en: 'Johnson et al. (2020) T. T. Johnson, D. M. Lopez, P. Musau, H.-D. Tran, E. Botoeva,
    F. Leofante, A. Maleki, C. Sidrane, J. Fan, and C. Huang. ARCH-COMP20 category
    report: Artificial intelligence and neural network control systems (AINNCS) for
    continuous and hybrid systems plants. In *International Workshop on Applied Verification
    of Continuous and Hybrid Systems (ARCH20)*, volume 74, pages 107–139, 2020.'
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 等 (2020) T. T. Johnson, D. M. Lopez, P. Musau, H.-D. Tran, E. Botoeva,
    F. Leofante, A. Maleki, C. Sidrane, J. Fan 和 C. Huang. ARCH-COMP20 类别报告：用于连续和混合系统工厂的人工智能和神经网络控制系统（AINNCS）。发表于
    *国际连续与混合系统应用验证研讨会（ARCH20）*，第74卷，107–139页，2020年。
- en: Jordan and Dimakis (2020) M. Jordan and A. G. Dimakis. Exactly computing the
    local Lipschitz constant of ReLU networks. In *Neural Information Processing Systems
    (NeurIPS)*, volume 33, 2020.
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan 和 Dimakis (2020) M. Jordan 和 A. G. Dimakis. 精确计算 ReLU 网络的局部 Lipschitz
    常数。发表于 *神经信息处理系统会议（NeurIPS）*，第33卷，2020年。
- en: 'Jordan et al. (2019) M. Jordan, J. Lewis, and A. G. Dimakis. Provable certificates
    for adversarial examples: Fitting a ball in the union of polytopes. In *Neural
    Information Processing Systems (NeurIPS)*, volume 32, 2019.'
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jordan 等 (2019) M. Jordan, J. Lewis 和 A. G. Dimakis. 对抗示例的可证明证书：在多面体的并集中拟合一个球体。发表于
    *神经信息处理系统会议（NeurIPS）*，第32卷，2019年。
- en: 'Karg and Lucia (2020) B. Karg and S. Lucia. Efficient representation and approximation
    of model predictive control laws via deep learning. *IEEE Transactions on Cybernetics*,
    50(9):3866–3878, 2020. doi: 10.1109/TCYB.2020.2999556.'
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karg 和 Lucia (2020) B. Karg 和 S. Lucia. 通过深度学习高效表示和逼近模型预测控制律。*IEEE 控制论学报*，50(9):3866–3878，2020年。doi:
    10.1109/TCYB.2020.2999556。'
- en: 'Katz et al. (2017) G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer.
    Reluplex: An efficient SMT solver for verifying deep neural networks. In *Computer
    Aided Verification (CAV)*, pages 97–117\. Springer, 2017.'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katz等（2017）G. Katz, C. Barrett, D. L. Dill, K. Julian, 和 M. J. Kochenderfer.
    Reluplex：一种高效的SMT求解器用于验证深度神经网络。在 *计算机辅助验证（CAV）*，页97–117。Springer，2017。
- en: Katz et al. (2019) G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus,
    R. Lim, P. Shah, S. Thakoor, H. Wu, A. Zeljić, et al. The marabou framework for
    verification and analysis of deep neural networks. In *Computer Aided Verification
    (CAV)*, pages 443–452. Springer, 2019.
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katz等（2019）G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim,
    P. Shah, S. Thakoor, H. Wu, A. Zeljić, 等. Marabou 框架用于深度神经网络的验证与分析。在 *计算机辅助验证（CAV）*，页443–452。Springer，2019。
- en: Katz et al. (2020) J. Katz, I. Pappas, S. Avraamidou, and E. N. Pistikopoulos.
    Integrating deep learning models and multiparametric programming. *Computers &
    Chemical Engineering*, 136:106801, 2020.
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katz等（2020）J. Katz, I. Pappas, S. Avraamidou, 和 E. N. Pistikopoulos. 深度学习模型与多参数编程的集成。*计算机与化学工程*，136:106801，2020。
- en: 'Keup and Helias (2022) C. Keup and M. Helias. Origami in N dimensions: How
    feed-forward networks manufacture linear separability. *arXiv:2203.11355*, 2022.'
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keup和Helias（2022）C. Keup 和 M. Helias. N维的折纸：前馈网络如何制造线性可分性。*arXiv:2203.11355*，2022。
- en: 'Khalife and Basu (2022) S. Khalife and A. Basu. Neural networks with linear
    threshold activations: structure and algorithms. In *Integer Programming and Combinatorial
    Optimization (IPCO)*, pages 347–360\. Springer, 2022.'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khalife和Basu（2022）S. Khalife 和 A. Basu. 具有线性阈值激活的神经网络：结构与算法。在 *整数规划与组合优化（IPCO）*，页347–360。Springer，2022。
- en: Khedr et al. (2020) H. Khedr, J. Ferlez, and Y. Shoukry. Effective formal verification
    of neural networks using the geometry of linear regions. *arXiv:2006.10864*, 2020.
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khedr等（2020）H. Khedr, J. Ferlez, 和 Y. Shoukry. 使用线性区域几何的神经网络有效形式化验证。*arXiv:2006.10864*，2020。
- en: 'Kingma and Ba (2014) D. P. Kingma and J. Ba. Adam: A method for stochastic
    optimization. *arXiv:1412.6980*, 2014.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma和Ba（2014）D. P. Kingma 和 J. Ba. Adam：一种随机优化方法。*arXiv:1412.6980*，2014。
- en: 'Kody et al. (2022) A. Kody, S. Chevalier, S. Chatzivasileiadis, and D. Molzahn.
    Modeling the ac power flow equations with optimally compact neural networks: Application
    to unit commitment. *Electric Power Systems Research*, 213:108282, 2022.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kody等（2022）A. Kody, S. Chevalier, S. Chatzivasileiadis, 和 D. Molzahn. 用最优紧凑的神经网络建模交流电功率流方程：在单位承诺中的应用。*电力系统研究*，213:108282，2022。
- en: Kouvaros et al. (2021) P. Kouvaros, T. Kyono, F. Leofante, A. Lomuscio, D. Margineantu,
    D. Osipychev, and Y. Zheng. Formal analysis of neural network-based systems in
    the aircraft domain. In *International Symposium on Formal Methods (FM)*, pages
    730–740\. Springer, 2021.
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kouvaros等（2021）P. Kouvaros, T. Kyono, F. Leofante, A. Lomuscio, D. Margineantu,
    D. Osipychev, 和 Y. Zheng. 飞机领域基于神经网络系统的形式化分析。在 *形式方法国际研讨会（FM）*，页730–740。Springer，2021。
- en: Krizhevsky et al. (2012) A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet
    classification with deep convolutional neural networks. In *Neural Information
    Processing Systems (NeurIPS)*, volume 25, 2012.
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky等（2012）A. Krizhevsky, I. Sutskever, 和 G. Hinton. 使用深度卷积神经网络的ImageNet分类。在
    *神经信息处理系统（NeurIPS）*，卷25，2012。
- en: 'Kronqvist et al. (2021) J. Kronqvist, R. Misener, and C. Tsay. Between steps:
    Intermediate relaxations between big-M and convex hull formulations. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kronqvist等（2021）J. Kronqvist, R. Misener, 和 C. Tsay. 步骤之间：大M与凸包公式之间的中间松弛。在 *约束编程、人工智能与运筹学整合国际会议（CPAIOR）*，2021。
- en: 'Kronqvist et al. (2022) J. Kronqvist, R. Misener, , and C. Tsay. P-split formulations:
    A class of intermediate formulations between big-M and convex hull for disjunctive
    constraints. *arXiv:2202.05198*, 2022.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kronqvist等（2022）J. Kronqvist, R. Misener, 和 C. Tsay. P-split 公式：大M与凸包之间的一类中间公式，用于离散约束。*arXiv:2202.05198*，2022。
- en: Kumar et al. (2019) A. Kumar, T. Serra, and S. Ramalingam. Equivalent and approximate
    transformations of deep neural networks. *arXiv:1905.1142*, 2019.
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar等（2019）A. Kumar, T. Serra, 和 S. Ramalingam. 深度神经网络的等效和近似变换。*arXiv:1905.1142*，2019。
- en: Lacoste-Julien et al. (2013) S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher.
    Block-coordinate Frank-Wolfe optimization for structural SVMs. In *International
    Conference on Machine Learning (ICML)*, pages 53–61, 2013.
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lacoste-Julien等（2013）S. Lacoste-Julien, M. Jaggi, M. Schmidt, 和 P. Pletscher.
    结构化SVM的块坐标Frank-Wolfe优化。在 *国际机器学习大会（ICML）*，页53–61，2013。
- en: Lan et al. (2022) J. Lan, Y. Zheng, and A. Lomuscio. Tight neural network verification
    via semidefinite relaxations and linear reformulations. In *AAAI Conference on
    Artificial Intelligence*, volume 36, pages 7272–7280, 2022.
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等人 (2022) J. Lan, Y. Zheng, 和 A. Lomuscio. 通过半正定松弛和线性重构进行紧致神经网络验证。发表于 *AAAI
    Conference on Artificial Intelligence*，第 36 卷，页 7272–7280，2022 年。
- en: Latorre et al. (2020) F. Latorre, P. Rolland, and V. Cevher. Lipschitz constant
    estimation of neural networks via sparse polynomial optimization. In *International
    Conference on Learning Representations (ICLR)*, 2020.
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latorre 等人 (2020) F. Latorre, P. Rolland, 和 V. Cevher. 通过稀疏多项式优化进行神经网络的 Lipschitz
    常数估计。发表于 *International Conference on Learning Representations (ICLR)*，2020 年。
- en: LeCun et al. (1989) Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
    W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code
    recognition. *Neural Computation*, 1(4):541–551, 1989.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 (1989) Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
    W. Hubbard, 和 L. D. Jackel. 反向传播在手写邮政编码识别中的应用。*Neural Computation*，1(4):541–551，1989
    年。
- en: 'LeCun et al. (1998) Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller. Efficient
    backprop. In G. Montavon, G. Orr, and K. Müller, editors, *Neural Networks: Tricks
    of the Trade*. Springer, 1998.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun 等人 (1998) Y. LeCun, L. Bottou, G. B. Orr, 和 K.-R. Müller. 高效的反向传播。在 G.
    Montavon, G. Orr 和 K. Müller 主编的 *Neural Networks: Tricks of the Trade* 中。Springer，1998
    年。'
- en: LeCun et al. (2015) Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. *Nature*,
    521, 2015.
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 (2015) Y. LeCun, Y. Bengio, 和 G. Hinton. 深度学习。*Nature*，521，2015 年。
- en: Lee et al. (2019) G.-H. Lee, D. Alvarez-Melis, and T. S. Jaakkola. Towards robust,
    locally linear deep networks. In *International Conference on Learning Representations
    (ICLR)*, 2019.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 (2019) G.-H. Lee, D. Alvarez-Melis, 和 T. S. Jaakkola. 朝向鲁棒的局部线性深度网络。发表于
    *International Conference on Learning Representations (ICLR)*，2019 年。
- en: 'Lee and Wilson (2001) J. Lee and D. Wilson. Polyhedral methods for piecewise-linear
    functions I: the lambda method. *Discrete Applied Mathematics*, 108(3):269–285,
    2001.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 和 Wilson (2001) J. Lee 和 D. Wilson. 针对分段线性函数的多面体方法 I: lambda 方法。*Discrete
    Applied Mathematics*，108(3):269–285，2001 年。'
- en: 'Leofante et al. (2018) F. Leofante, N. Narodytska, L. Pulina, and A. Tacchella.
    Automated verification of neural networks: Advances, challenges and perspectives.
    *arXiv:1805.09938*, 2018.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leofante 等人 (2018) F. Leofante, N. Narodytska, L. Pulina, 和 A. Tacchella. 神经网络的自动化验证：进展、挑战和展望。*arXiv:1805.09938*，2018
    年。
- en: 'Li et al. (2022) L. Li, T. Xie, and B. Li. Sok: Certified robustness for deep
    neural networks. In *2023 IEEE Symposium on Security and Privacy (SP)*, pages
    94–115\. IEEE Computer Society, 2022.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2022) L. Li, T. Xie, 和 B. Li. Sok: 深度神经网络的认证鲁棒性。发表于 *2023 IEEE Symposium
    on Security and Privacy (SP)*，页 94–115。IEEE 计算机学会，2022 年。'
- en: Liang and Xu (2021) X. Liang and J. Xu. Biased ReLU neural networks. *Neurocomputing*,
    423:71–79, 2021.
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 和 Xu (2021) X. Liang 和 J. Xu. 偏置 ReLU 神经网络。*Neurocomputing*，423:71–79，2021
    年。
- en: Lillicrap et al. (2015) T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
    Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement
    learning. *arXiv:1509.02971*, 2015.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lillicrap 等人 (2015) T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
    Y. Tassa, D. Silver, 和 D. Wierstra. 基于深度强化学习的连续控制。*arXiv:1509.02971*，2015 年。
- en: Linnainmaa (1970) S. Linnainmaa. The representation of the cumulative rounding
    error of an algorithm as a Taylor expansion of the local rounding errors (in Finnish).
    Master’s thesis, Univ. Helsinki, 1970.
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linnainmaa (1970) S. Linnainmaa. 算法累积舍入误差的泰勒展开表示（芬兰语）。硕士论文，赫尔辛基大学，1970 年。
- en: Little (1974) W. Little. The existence of persistent states in the brain. *Mathematical
    Biosciences*, 19:101–120, 1974.
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Little (1974) W. Little. 大脑中持久状态的存在。*Mathematical Biosciences*，19:101–120，1974
    年。
- en: Liu and Liang (2021) B. Liu and Y. Liang. Optimal function approximation with
    ReLU neural networks. *Neurocomputing*, 435:216–227, 2021.
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Liang (2021) B. Liu 和 Y. Liang. 使用 ReLU 神经网络的最优函数逼近。*Neurocomputing*，435:216–227，2021
    年。
- en: Liu et al. (2021) C. Liu, T. Arnon, C. Lazarus, C. Strong, C. Barrett, M. J.
    Kochenderfer, et al. Algorithms for verifying deep neural networks. *Foundations
    and Trends® in Optimization*, 4(3-4):244–404, 2021.
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2021) C. Liu, T. Arnon, C. Lazarus, C. Strong, C. Barrett, M. J. Kochenderfer
    等人. 验证深度神经网络的算法。*Foundations and Trends® in Optimization*，4(3-4):244–404，2021
    年。
- en: Liu et al. (2020) X. Liu, X. Han, N. Zhang, and Q. Liu. Certified monotonic
    neural networks. In *Neural Information Processing Systems (NeurIPS)*, volume 33,
    2020.
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2020) X. Liu, X. Han, N. Zhang, 和 Q. Liu. 认证单调神经网络。发表于 *Neural Information
    Processing Systems (NeurIPS)*，第 33 卷，2020 年。
- en: Lombardi et al. (2017) M. Lombardi, M. Milano, and A. Bartolini. Empirical decision
    model learning. *Artificial Intelligence*, 244:343–367, 2017.
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lombardi 等人 (2017) M. Lombardi, M. Milano, 和 A. Bartolini. 实证决策模型学习。*Artificial
    Intelligence*，244:343–367，2017 年。
- en: Lomuscio and Maganti (2017) A. Lomuscio and L. Maganti. An approach to reachability
    analysis for feed-forward ReLU neural networks. *arXiv:1706.07351*, 2017.
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lomuscio和Maganti（2017）A. Lomuscio 和 L. Maganti。《一种用于前馈ReLU神经网络的可达性分析方法》。*arXiv:1706.07351*，2017年。
- en: Loukas et al. (2021) A. Loukas, M. Poiitis, and S. Jegelka. What training reveals
    about neural network complexity. In *Neural Information Processing Systems (NeurIPS)*,
    volume 34, 2021.
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loukas等（2021）A. Loukas, M. Poiitis, 和 S. Jegelka。《训练揭示了神经网络复杂性的什么》。发表于*神经信息处理系统（NeurIPS）*，第34卷，2021年。
- en: 'Lu et al. (2017) Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive
    power of neural networks: A view from the width. In *Neural Information Processing
    Systems (NeurIPS)*, volume 30, 2017.'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu等（2017）Z. Lu, H. Pu, F. Wang, Z. Hu, 和 L. Wang。《神经网络的表达能力：从宽度的视角》。发表于*神经信息处理系统（NeurIPS）*，第30卷，2017年。
- en: 'Lueg et al. (2021) L. Lueg, B. Grimstad, A. Mitsos, and A. M. Schweidtmann.
    reluMIP: Open source tool for MILP optimization of ReLU neural networks, 2021.
    URL https://github.com/ChemEngAI/ReLU_ANN_MILP.'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lueg等（2021）L. Lueg, B. Grimstad, A. Mitsos, 和 A. M. Schweidtmann。《reluMIP：用于ReLU神经网络MILP优化的开源工具》，2021年。网址
    https://github.com/ChemEngAI/ReLU_ANN_MILP。
- en: 'Lyu et al. (2020) Z. Lyu, C.-Y. Ko, Z. Kong, N. Wong, D. Lin, and L. Daniel.
    Fastened crown: Tightened neural network robustness certificates. In *AAAI Conference
    on Artificial Intelligence*, volume 34, pages 5037–5044, 2020.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu等（2020）Z. Lyu, C.-Y. Ko, Z. Kong, N. Wong, D. Lin, 和 L. Daniel。《紧固的皇冠：强化的神经网络鲁棒性证书》。发表于*AAAI人工智能会议*，第34卷，第5037–5044页，2020年。
- en: Maas et al. (2013) A. Maas, A. Hannun, and A. Ng. Rectifier nonlinearities improve
    neural network acoustic models. In *ICML Workshop on Deep Learning for Audio,
    Speech and Language Processing*, 2013.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maas等（2013）A. Maas, A. Hannun, 和 A. Ng。《整流非线性提高了神经网络声学模型》。发表于*ICML音频、语音和语言处理深度学习研讨会*，2013年。
- en: Madry et al. (2018) A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
    Towards deep learning models resistant to adversarial attacks. In *International
    Conference on Learning Representations (ICLR)*, 2018.
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madry等（2018）A. Madry, A. Makelov, L. Schmidt, D. Tsipras, 和 A. Vladu。《朝向对抗攻击具有鲁棒性的深度学习模型》。发表于*国际学习表征会议（ICLR）*，2018年。
- en: Makhoul et al. (1989) J. Makhoul, R. Schwartz, and A. El-Jaroudi. Classification
    capabilities of two-layer neural nets. In *International Conference on Acoustics,
    Speech, and Signal Processing (ICASSP)*, 1989.
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makhoul等（1989）J. Makhoul, R. Schwartz, 和 A. El-Jaroudi。《双层神经网络的分类能力》。发表于*国际声学、语音和信号处理会议（ICASSP）*，1989年。
- en: Malach and Shalev-Shwartz (2019) E. Malach and S. Shalev-Shwartz. Is deeper
    better only when shallow is good? In *Neural Information Processing Systems (NeurIPS)*,
    volume 32, 2019.
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malach和Shalev-Shwartz（2019）E. Malach 和 S. Shalev-Shwartz。《深度是否仅在浅层良好时才更优？》发表于*神经信息处理系统（NeurIPS）*，第32卷，2019年。
- en: Mangasarian (1993) O. L. Mangasarian. Mathematical programming in neural networks.
    *ORSA Journal on Computing*, 5(4):349–360, 1993.
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mangasarian（1993）O. L. Mangasarian。《神经网络中的数学规划》。*ORSA计算期刊*，5(4):349–360，1993年。
- en: Maragno et al. (2021) D. Maragno, H. Wiberg, D. Bertsimas, S. I. Birbil, D. d.
    Hertog, and A. Fajemisin. Mixed-integer optimization with constraint learning.
    *arXiv:2111.04469*, 2021.
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maragno等（2021）D. Maragno, H. Wiberg, D. Bertsimas, S. I. Birbil, D. d. Hertog,
    和 A. Fajemisin。《带约束学习的混合整数优化》。*arXiv:2111.04469*，2021年。
- en: Maragno et al. (2023) D. Maragno, J. Kurtz, T. E. Röber, R. Goedhart, Ş. I.
    Birbil, and D. d. Hertog. Finding regions of counterfactual explanations via robust
    optimization. *arXiv:2301.11113*, 2023.
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maragno等（2023）D. Maragno, J. Kurtz, T. E. Röber, R. Goedhart, Ş. I. Birbil,
    和 D. d. Hertog。《通过鲁棒优化寻找反事实解释的区域》。*arXiv:2301.11113*，2023年。
- en: Maragos et al. (2021) P. Maragos, V. Charisopoulos, and E. Theodosis. Tropical
    geometry and machine learning. *Proceedings of the IEEE*, 109(5):728–755, 2021.
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maragos等（2021）P. Maragos, V. Charisopoulos, 和 E. Theodosis。《热带几何与机器学习》。*IEEE期刊*，109(5):728–755，2021年。
- en: Masden (2022) M. Masden. Algorithmic determination of the combinatorial structure
    of the linear regions of ReLU neural networks. *arXiv:2207.07696*, 2022.
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masden（2022）M. Masden。《ReLU神经网络线性区域的组合结构的算法确定》。*arXiv:2207.07696*，2022年。
- en: Matoba et al. (2022) K. Matoba, N. Dimitriadis, and F. Fleuret. The theoretical
    expressiveness of maxpooling. *arXiv:2203.01016*, 2022.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matoba等（2022）K. Matoba, N. Dimitriadis, 和 F. Fleuret。《最大池化的理论表达能力》。*arXiv:2203.01016*，2022年。
- en: Matousek (2002) J. Matousek. *Lectures on Discrete Geometry*, volume 212. Springer
    Science & Business Media, 2002.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Matousek（2002）J. Matousek。《离散几何讲座》，第212卷。Springer Science & Business Media，2002年。
- en: McBride and Sundmacher (2019) K. McBride and K. Sundmacher. Overview of surrogate
    modeling in chemical process engineering. *Chemie Ingenieur Technik*, 91(3):228–239,
    2019.
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McBride 和 Sundmacher (2019) K. McBride 和 K. Sundmacher. 化学过程工程中替代建模的概述。*化学工程技术*，91(3):228–239，2019年。
- en: McCulloch and Pitts (1943) W. McCulloch and W. Pitts. A logical calculus of
    the ideas immanent in nervous activity. *Bulletin of Mathematical Biophysics*,
    5:115–133, 1943.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCulloch 和 Pitts (1943) W. McCulloch 和 W. Pitts. 神经活动中固有思想的逻辑演算。*数学生物物理学公报*，5:115–133，1943年。
- en: Mhaskar and Poggio (2020) H. N. Mhaskar and T. Poggio. Function approximation
    by deep networks. *Communications on Pure & Applied Analysis*, 19(8):4085–4095,
    2020.
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mhaskar 和 Poggio (2020) H. N. Mhaskar 和 T. Poggio. 通过深度网络进行函数逼近。*纯与应用分析通讯*，19(8):4085–4095，2020年。
- en: 'Minsky and Papert (1969) M. Minsky and S. Papert. *Perceptrons: An Introduction
    to Computational Geometry*. The MIT Press, 1969.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minsky 和 Papert (1969) M. Minsky 和 S. Papert. *感知器：计算几何入门*。麻省理工学院出版社，1969年。
- en: Mirman et al. (2018) M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract
    interpretation for provably robust neural networks. In *International Conference
    on Machine Learning (ICML)*, volume 80, pages 3578–3586, 2018.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirman 等 (2018) M. Mirman, T. Gehr 和 M. Vechev. 用于可证明鲁棒神经网络的可微抽象解释。发表于 *国际机器学习会议
    (ICML)*，第 80 卷，页码 3578–3586，2018年。
- en: Misener and Floudas (2012) R. Misener and C. A. Floudas. Global optimization
    of mixed-integer quadratically-constrained quadratic programs (MIQCQP) through
    piecewise-linear and edge-concave relaxations. *Mathematical Programming*, 136(1):155–182,
    2012.
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misener 和 Floudas (2012) R. Misener 和 C. A. Floudas. 通过分段线性和边缘凹放松进行混合整数二次约束二次规划
    (MIQCQP) 的全局优化。*数学编程*，136(1):155–182，2012年。
- en: Mnih et al. (2015) V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness,
    M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,
    and D. Hassabis. Human-level control through deep reinforcement learning. *Nature*,
    518:529–533, 2015.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等 (2015) V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M.
    G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg
    和 D. Hassabis. 通过深度强化学习实现人类水平的控制。*自然*，518:529–533，2015年。
- en: Montúfar (2017) G. Montúfar. Notes on the number of linear regions of deep neural
    networks. In *Sampling Theory and Applications (SampTA)*, 2017.
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montúfar (2017) G. Montúfar. 深度神经网络线性区域数量的笔记。发表于 *采样理论与应用 (SampTA)*，2017年。
- en: Montúfar et al. (2014) G. Montúfar, R. Pascanu, K. Cho, and Y. Bengio. On the
    number of linear regions of deep neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, volume 27, 2014.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montúfar 等 (2014) G. Montúfar, R. Pascanu, K. Cho 和 Y. Bengio. 深度神经网络线性区域的数量。发表于
    *神经信息处理系统 (NeurIPS)*，第 27 卷，2014年。
- en: Montúfar et al. (2022) G. Montúfar, Y. Ren, and L. Zhang. Sharp bounds for the
    number of regions of maxout networks and vertices of Minkowski sums. *SIAM Journal
    on Applied Algebra and Geometry*, 6(4), 2022.
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Montúfar 等 (2022) G. Montúfar, Y. Ren 和 L. Zhang. Maxout 网络的区域数量和 Minkowski
    和的顶点的尖锐界限。*SIAM 应用代数与几何杂志*，6(4)，2022年。
- en: Motzkin (1936) T. Motzkin. *Beitrage zur theorie der linearen Ungleichungen*.
    PhD thesis, University of Basel, 1936.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Motzkin (1936) T. Motzkin. *线性不等式理论的贡献*。博士论文，巴塞尔大学，1936年。
- en: 'Mukhopadhyay et al. (1993) S. Mukhopadhyay, A. Roy, L. S. Kim, and S. Govil.
    A polynomial time algorithm for generating neural networks for pattern classification:
    Its stability properties and some test results. *Neural Computation*, 5(2):317–330,
    1993.'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mukhopadhyay 等 (1993) S. Mukhopadhyay, A. Roy, L. S. Kim 和 S. Govil. 一种用于模式分类的多项式时间算法：其稳定性特性和一些测试结果。*神经计算*，5(2):317–330，1993年。
- en: Nair and Hinton (2010) V. Nair and G. Hinton. Rectified linear units improve
    restricted boltzmann machines. In *International Conference on Machine Learning
    (ICML)*, 2010.
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 和 Hinton (2010) V. Nair 和 G. Hinton. 修正线性单元改进限制玻尔兹曼机。发表于 *国际机器学习会议 (ICML)*，2010年。
- en: Narodytska et al. (2018) N. Narodytska, S. Kasiviswanathan, L. Ryzhyk, M. Sagiv,
    and T. Walsh. Verifying properties of binarized deep neural networks. In *AAAI
    Conference on Artificial Intelligence*, volume 32, 2018.
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narodytska 等 (2018) N. Narodytska, S. Kasiviswanathan, L. Ryzhyk, M. Sagiv 和
    T. Walsh. 验证二值化深度神经网络的属性。发表于 *AAAI 人工智能会议*，第 32 卷，2018年。
- en: Nelles et al. (2000) O. Nelles, A. Fink, and R. Isermann. Local linear model
    trees (LOLIMOT) toolbox for nonlinear system identification. In *IFAC Symposium
    on System Identification (SYSID)*, 2000.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nelles 等 (2000) O. Nelles, A. Fink 和 R. Isermann. 非线性系统识别的局部线性模型树 (LOLIMOT)
    工具箱。发表于 *IFAC 系统识别研讨会 (SYSID)*，2000年。
- en: Nesterov (1983) Y. E. Nesterov. A method of solving a convex programming problem
    with convergence rate $o\bigl{(}\frac{1}{k^{2}}\bigr{)}$. *Doklady Akademii Nauk*,
    269:543–547, 1983.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov（1983）Y. E. Nesterov. 解决凸优化问题的一个方法，收敛速度为 $o\bigl{(}\frac{1}{k^{2}}\bigr{)}$。*俄罗斯科学院公报*，269:543–547，1983。
- en: Newton and Papachristodoulou (2021) M. Newton and A. Papachristodoulou. Exploiting
    sparsity for neural network verification. In *Learning for Dynamics and Control
    (L4DC)*, pages 715–727. PMLR, 2021.
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newton 和 Papachristodoulou（2021）M. Newton 和 A. Papachristodoulou. 利用稀疏性进行神经网络验证。发表于
    *动态与控制学习（L4DC）*，第715–727页。PMLR，2021。
- en: Nguyen et al. (2018) Q. Nguyen, M. C. Mukkamala, and M. Hein. Neural networks
    should be wide enough to learn disconnected decision regions. In *International
    Conference on Machine Learning (ICML)*, 2018.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等（2018）Q. Nguyen, M. C. Mukkamala, 和 M. Hein. 神经网络应足够宽，以学习不连通的决策区域。发表于
    *国际机器学习会议（ICML）*，2018。
- en: 'Nguyen and Huchette (2022) T. Nguyen and J. Huchette. Neural network verification
    as piecewise linear optimization: Formulations for the composition of staircase
    functions. *arXiv:2211.14706*, 2022.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 和 Huchette（2022）T. Nguyen 和 J. Huchette. 神经网络验证作为分段线性优化：阶梯函数组合的公式。*arXiv:2211.14706*，2022。
- en: 'Novak et al. (2018) R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, and
    J. Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical
    study. In *International Conference on Learning Representations (ICLR)*, 2018.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Novak 等（2018）R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, 和 J. Sohl-Dickstein.
    神经网络中的敏感性和泛化：一项实证研究。发表于 *国际学习表示会议（ICLR）*，2018。
- en: OpenAI (2022) OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2022）OpenAI. 介绍 chatgpt，2022。网址 https://openai.com/blog/chatgpt。
- en: OpenAI et al. (2019) OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dȩbiak,
    C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray,
    C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans,
    J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang.
    Dota 2 with large scale deep reinforcement learning. *arXiv:1912.06680*, 2019.
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等（2019）OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dȩbiak,
    C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray,
    C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans,
    J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, 和 S. Zhang.
    使用大规模深度强化学习的 Dota 2。*arXiv:1912.06680*，2019。
- en: Padberg (2000) M. Padberg. Approximating separable nonlinear functions via mixed
    zero-one programs. *Operations Research Letters*, 27(1):1–5, 2000.
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Padberg（2000）M. Padberg. 通过混合零一程序近似可分离非线性函数。*运筹学通讯*，27(1):1–5，2000。
- en: Papalexopoulos et al. (2022) T. P. Papalexopoulos, C. Tjandraatmadja, R. Anderson,
    J. P. Vielma, and D. Belanger. Constrained discrete black-box optimization using
    mixed-integer programming. In *International Conference on Machine Learning (ICML)*,
    volume 162, pages 17295–17322, 2022.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papalexopoulos 等（2022）T. P. Papalexopoulos, C. Tjandraatmadja, R. Anderson,
    J. P. Vielma, 和 D. Belanger. 使用混合整数规划的受限离散黑箱优化。发表于 *国际机器学习会议（ICML）*，卷162，第17295–17322页，2022。
- en: 'Park et al. (2019) D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
    Cubuk, and Q. V. Le. SpecAugment: A simple data augmentation method for automatic
    speech recognition. In *Interspeech*, 2019.'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2019）D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk,
    和 Q. V. Le. SpecAugment：一种简单的数据增强方法，用于自动语音识别。发表于 *国际语音通讯大会（Interspeech）*，2019。
- en: Park et al. (2021a) S. Park, C. Yun, J. Lee, and J. Shin. Minimum width for
    universal approximation. In *International Conference on Learning Representations
    (ICLR)*, 2021a.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2021a）S. Park, C. Yun, J. Lee, 和 J. Shin. 通用近似的最小宽度。发表于 *国际学习表示会议（ICLR）*，2021a。
- en: Park et al. (2021b) Y. Park, S. Lee, G. Kim, and D. M. Blei. Unsupervised representation
    learning via neural activation coding. In *International Conference on Machine
    Learning (ICML)*, 2021b.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等（2021b）Y. Park, S. Lee, G. Kim, 和 D. M. Blei. 通过神经激活编码的无监督表示学习。发表于 *国际机器学习会议（ICML）*，2021b。
- en: Pascanu et al. (2014) R. Pascanu, G. Montúfar, and Y. Bengio. On the number
    of response regions of deep feedforward networks with piecewise linear activations.
    In *International Conference on Learning Representations (ICLR)*, 2014.
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pascanu 等（2014）R. Pascanu, G. Montúfar, 和 Y. Bengio. 关于具有分段线性激活的深度前馈网络的响应区域数量。发表于
    *国际学习表示会议（ICLR）*，2014。
- en: Patrick L. Combettes (2019) J.-C. P. Patrick L. Combettes. Lipschitz certificates
    for layered network structures driven by averaged activation operators. *arXiv:1903.01014*,
    2019.
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patrick L. Combettes（2019）J.-C. P. Patrick L. Combettes. 由平均激活算子驱动的层次网络结构的 Lipschitz
    证书。*arXiv:1903.01014*，2019。
- en: Perakis and Tsiourvas (2022) G. Perakis and A. Tsiourvas. Optimizing objective
    functions from trained relu neural networks via sampling. *arXiv:2205.14189*,
    2022.
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perakis 和 Tsiourvas（2022）G. Perakis 和 A. Tsiourvas。通过采样优化训练的 ReLU 神经网络的目标函数。*arXiv:2205.14189*，2022。
- en: Peters et al. (2018) M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,
    K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In *Conference
    of the North American Chapter of the Association for Computational Linguistics
    (NAACL)*, 2018.
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peters 等（2018）M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
    和 L. Zettlemoyer。深度上下文化词表征。见 *北美计算语言学协会年会（NAACL）*，2018。
- en: Phuong and Lampert (2020) M. Phuong and C. H. Lampert. Functional vs. parametric
    equivalence of ReLU networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phuong 和 Lampert（2020）M. Phuong 和 C. H. Lampert。ReLU 网络的功能与参数等效性。见 *国际学习表征会议（ICLR）*，2020。
- en: 'Pilanci and Ergen (2020) M. Pilanci and T. Ergen. Neural networks are convex
    regularizers: Exact polynomial-time convex optimization formulations for two-layer
    networks. In *International Conference on Machine Learning (ICML)*, pages 7695–7705\.
    PMLR, 2020.'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pilanci 和 Ergen（2020）M. Pilanci 和 T. Ergen。神经网络是凸正则化器：用于两层网络的精确多项式时间凸优化公式。见
    *国际机器学习大会（ICML）*，第 7695–7705 页。PMLR，2020。
- en: Pokutta et al. (2020) S. Pokutta, C. Spiegel, and M. Zimmer. Deep neural network
    training with frank-wolfe. *arXiv:2010.07243*, 2020.
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pokutta 等（2020）S. Pokutta, C. Spiegel, 和 M. Zimmer。使用 Frank-Wolfe 方法进行深度神经网络训练。*arXiv:2010.07243*，2020。
- en: Polyak (1964) B. T. Polyak. Some methods of speeding up the convergence of iteration
    methods. *USSR Computational Mathematics and Mathematical Physics*, 4:1–17, 1964.
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polyak（1964）B. T. Polyak。加速迭代方法收敛的一些方法。*苏联计算数学与数学物理*，4:1–17，1964。
- en: Pulina and Tacchella (2010) L. Pulina and A. Tacchella. An abstraction-refinement
    approach to verification of artificial neural networks. In *Computer Aided Verification
    (CAV)*, pages 243–257, 2010.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pulina 和 Tacchella（2010）L. Pulina 和 A. Tacchella。人工神经网络验证的抽象-精化方法。见 *计算机辅助验证（CAV）*，第
    243–257 页，2010。
- en: Radford et al. (2018) A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.
    Improving language understanding by generative pre-training. Technical report,
    OpenAI, 2018.
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018）A. Radford, K. Narasimhan, T. Salimans, 和 I. Sutskever。通过生成预训练提高语言理解能力。技术报告，OpenAI，2018。
- en: Raghu et al. (2017) M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Dickstein.
    On the expressive power of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2017.
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghu 等（2017）M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, 和 J. Dickstein。深度神经网络的表达能力。见
    *国际机器学习大会（ICML）*，2017。
- en: Raghunathan et al. (2018) A. Raghunathan, J. Steinhardt, and P. S. Liang. Semidefinite
    relaxations for certifying robustness to adversarial examples. *Neural Information
    Processing Systems (NeurIPS)*, 31, 2018.
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raghunathan 等（2018）A. Raghunathan, J. Steinhardt, 和 P. S. Liang。半正定松弛用于证明对抗样本的鲁棒性。*神经信息处理系统（NeurIPS）*，31，2018。
- en: Ramachandran et al. (2018) P. Ramachandran, B. Zoph, and Q. V. Le. Searching
    for activation functions. In *ICLR Workshop Track*, 2018.
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramachandran 等（2018）P. Ramachandran, B. Zoph, 和 Q. V. Le。寻找激活函数。见 *ICLR Workshop
    Track*，2018。
- en: Raman and Grossmann (1994) R. Raman and I. Grossmann. Modelling and computational
    techniques for logic based integer programming. *Computers & Chemical Engineering*,
    18(7):563–578, 1994.
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raman 和 Grossmann（1994）R. Raman 和 I. Grossmann。基于逻辑的整数规划的建模与计算技术。*计算机与化学工程*，18(7):563–578，1994。
- en: Ramesh et al. (2022) A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen.
    Hierarchical text-conditional image generation with CLIP latents. *arXiv:2204.06125*,
    2022.
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等（2022）A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, 和 M. Chen。基于 CLIP 潜变量的分层文本条件图像生成。*arXiv:2204.06125*，2022。
- en: Robbins and Monro (1951) H. Robbins and S. Monro. A stochastic approximation
    method. *The Annals of Mathematical Statistics*, 22(3):400–407, 1951.
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robbins 和 Monro（1951）H. Robbins 和 S. Monro。随机逼近方法。*数学统计年刊*，22(3):400–407，1951。
- en: Robinson et al. (2019) H. Robinson, A. Rasheed, and O. San. Dissecting deep
    neural networks. *arXiv:1910.03879*, 2019.
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robinson 等（2019）H. Robinson, A. Rasheed, 和 O. San。深度神经网络的剖析。*arXiv:1910.03879*，2019。
- en: Rolnick and Kording (2020) D. Rolnick and K. Kording. Reverse-engineering deep
    ReLU networks. In *International Conference on Machine Learning (ICML)*, 2020.
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rolnick 和 Kording（2020）D. Rolnick 和 K. Kording。反向工程深度 ReLU 网络。见 *国际机器学习大会（ICML）*，2020。
- en: Rosenblatt (1957) F. Rosenblatt. The Perceptron — a perceiving and recognizing
    automaton. Technical Report 85-460-1, Cornell Aeronautical Laboratory, 1957.
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosenblatt（1957）F. Rosenblatt. 感知机——一个感知和识别的自动装置。技术报告 85-460-1，康奈尔航空实验室，1957。
- en: Rössig and Petkovic (2021) A. Rössig and M. Petkovic. Advances in verification
    of relu neural networks. *Journal of Global Optimization*, 81:109–152, 2021.
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rössig 和 Petkovic（2021）A. Rössig 和 M. Petkovic. ReLU 神经网络验证的进展。*全球优化杂志*，81:109–152，2021。
- en: Roth (2021) K. Roth. A primer on multi-neuron relaxation-based adversarial robustness
    certification. In *ICML 2021 Workshop on Adversarial Machine Learning*, 2021.
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roth（2021）K. Roth. 多神经元松弛基础对抗鲁棒性认证的入门指南。*ICML 2021 对抗机器学习研讨会*，2021。
- en: Roy et al. (1993) A. Roy, L. S. Kim, and S. Mukhopadhyay. A polynomial time
    algorithm for the construction and training of a class of multilayer perceptrons.
    *Neural Networks*, 6(4):535–545, 1993.
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy 等（1993）A. Roy, L. S. Kim 和 S. Mukhopadhyay. 一种用于构建和训练一类多层感知器的多项式时间算法。*神经网络*，6(4):535–545，1993。
- en: Rubies-Royo et al. (2019) V. Rubies-Royo, R. Calandra, D. M. Stipanovic, and
    C. Tomlin. Fast neural network verification via shadow prices. *arXiv:1902.07247*,
    2019.
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubies-Royo 等（2019）V. Rubies-Royo, R. Calandra, D. M. Stipanovic 和 C. Tomlin.
    通过影子价格进行快速神经网络验证。*arXiv:1902.07247*, 2019。
- en: Rumelhart et al. (1986) D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning
    representations by back-propagating errors. *Nature*, 323:533–536, 1986.
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart 等（1986）D. E. Rumelhart, G. E. Hinton 和 R. J. Williams. 通过反向传播错误学习表征。*自然*，323:533–536，1986。
- en: 'Ryu et al. (2020) M. Ryu, Y. Chow, R. Anderson, C. Tjandraatmadja, and C. Boutilier.
    Caql: Continuous action q-learning. In *International Conference on Learning Representations
    (ICLR)*, 2020.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ryu 等（2020）M. Ryu, Y. Chow, R. Anderson, C. Tjandraatmadja 和 C. Boutilier.
    Caql: 连续动作 Q 学习。*国际学习表征会议（ICLR）*，2020。'
- en: 'Sahiner et al. (2021) A. Sahiner, T. Ergen, J. M. Pauly, and M. Pilanci. Vector-output
    re{lu} neural network problems are copositive programs: Convex analysis of two
    layer networks and polynomial-time algorithms. In *International Conference on
    Learning Representations (ICLR)*, 2021.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahiner 等（2021）A. Sahiner, T. Ergen, J. M. Pauly 和 M. Pilanci. 向量输出 ReLU 神经网络问题是共正程序：两层网络的凸分析和多项式时间算法。*国际学习表征会议（ICLR）*，2021。
- en: Salman et al. (2019) H. Salman, G. Yang, H. Zhang, C.-J. Hsieh, and P. Zhang.
    A convex relaxation barrier to tight robustness verification of neural networks.
    *Neural Information Processing Systems (NeurIPS)*, 32, 2019.
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salman 等（2019）H. Salman, G. Yang, H. Zhang, C.-J. Hsieh 和 P. Zhang. 一种用于紧密鲁棒性验证的凸松弛障碍。*神经信息处理系统（NeurIPS）*，32，2019。
- en: 'Sandler et al. (2018) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In *Conference on
    Computer Vision and Pattern Recognition (CVPR)*, pages 4510–4520, 2018.'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sandler 等（2018）M. Sandler, A. Howard, M. Zhu, A. Zhmoginov 和 L.-C. Chen. Mobilenetv2:
    倒置残差和线性瓶颈。*计算机视觉与模式识别会议（CVPR）*，第4510–4520页，2018。'
- en: Sattelberg et al. (2020) B. Sattelberg, R. Cavalieri, M. Kirby, C. Peterson,
    and R. Beveridge. Locally linear attributes of ReLU neural networks. *arXiv:2012.01940*,
    2020.
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sattelberg 等（2020）B. Sattelberg, R. Cavalieri, M. Kirby, C. Peterson 和 R. Beveridge.
    ReLU 神经网络的局部线性属性。*arXiv:2012.01940*, 2020。
- en: Say et al. (2017) B. Say, G. Wu, Y. Q. Zhou, and S. Sanner. Nonlinear hybrid
    planning with deep net learned transition models and mixed-integer linear programming.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    750–756, 2017.
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Say 等（2017）B. Say, G. Wu, Y. Q. Zhou 和 S. Sanner. 使用深度网络学习的过渡模型和混合整数线性规划的非线性混合规划。*国际人工智能联合会议（IJCAI）*，第750–756页，2017。
- en: 'Schmidhuber (2015) J. Schmidhuber. Deep learning in neural networks: An overview.
    *Neural Networks*, 61:85–117, 2015.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidhuber（2015）J. Schmidhuber. 神经网络中的深度学习：概述。*神经网络*，61:85–117，2015。
- en: Schumann et al. (2003) J. Schumann, P. Gupta, and S. Nelson. On verification
    & validation of neural network based controllers. In *Engineering Applications
    of Neural Networks (EANN)*, 2003.
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schumann 等（2003）J. Schumann, P. Gupta 和 S. Nelson. 关于基于神经网络控制器的验证与确认。*工程应用神经网络（EANN）*，2003。
- en: Schwan et al. (2022) R. Schwan, C. N. Jones, and D. Kuhn. Stability verification
    of neural network controllers using mixed-integer programming. *arXiv:2206.13374*,
    2022.
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwan 等（2022）R. Schwan, C. N. Jones 和 D. Kuhn. 使用混合整数编程进行神经网络控制器的稳定性验证。*arXiv:2206.13374*,
    2022。
- en: Schweidtmann and Mitsos (2019) A. M. Schweidtmann and A. Mitsos. Deterministic
    global optimization with artificial neural networks embedded. *Journal of Optimization
    Theory and Applications*, 180(3):925–948, 2019.
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schweidtmann和Mitsos（2019）A. M. Schweidtmann 和 A. Mitsos。嵌入人工神经网络的确定性全局优化。*Journal
    of Optimization Theory and Applications*，180(3):925–948，2019年。
- en: Schweidtmann et al. (2022) A. M. Schweidtmann, J. M. Weber, C. Wende, L. Netze,
    and A. Mitsos. Obey validity limits of data-driven models through topological
    data analysis and one-class classification. *Optimization and Engineering*, 23(2):855–876,
    2022.
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schweidtmann等人（2022）A. M. Schweidtmann, J. M. Weber, C. Wende, L. Netze, 和 A.
    Mitsos。通过拓扑数据分析和单类分类遵守数据驱动模型的有效性限制。*Optimization and Engineering*，23(2):855–876，2022年。
- en: Seck et al. (2021) I. Seck, G. Loosli, and S. Canu. Linear program powered attack.
    In *International Joint Conference on Neural Networks (IJCNN)*, 2021.
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seck等人（2021）I. Seck, G. Loosli, 和 S. Canu。线性规划驱动的攻击。在*International Joint Conference
    on Neural Networks (IJCNN)*，2021年。
- en: Serra (2020) T. Serra. Enumerative branching with less repetition. In *International
    Conference on Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, pages 399–416\. Springer, 2020.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serra（2020）T. Serra。减少重复的枚举分支。在*International Conference on Integration of Constraint
    Programming, Artificial Intelligence, and Operations Research (CPAIOR)*，页码399–416。Springer，2020年。
- en: Serra and Hooker (2020) T. Serra and J. Hooker. Compact representation of near-optimal
    integer programming solutions. *Mathematical Programming*, 182:199–232, 2020.
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serra和Hooker（2020）T. Serra和J. Hooker。接近最优的整数规划解的紧凑表示。*Mathematical Programming*，182:199–232，2020年。
- en: Serra and Ramalingam (2020) T. Serra and S. Ramalingam. Empirical bounds on
    linear regions of deep rectifier networks. In *AAAI Conference on Artificial Intelligence*,
    2020.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serra和Ramalingam（2020）T. Serra和S. Ramalingam。深度整流网络线性区域的经验界限。在*AAAI Conference
    on Artificial Intelligence*，2020年。
- en: Serra et al. (2018) T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding
    and counting linear regions of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2018.
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serra等人（2018）T. Serra, C. Tjandraatmadja, 和 S. Ramalingam。界定和计数深度神经网络的线性区域。在*International
    Conference on Machine Learning (ICML)*，2018年。
- en: Serra et al. (2020) T. Serra, A. Kumar, and S. Ramalingam. Lossless compression
    of deep neural networks. In *International Conference on the Integration of Constraint
    Programming, Artificial Intelligence, and Operations Research (CPAIOR)*, 2020.
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serra等人（2020）T. Serra, A. Kumar, 和 S. Ramalingam。深度神经网络的无损压缩。在*International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*，2020年。
- en: Serra et al. (2021) T. Serra, X. Yu, A. Kumar, and S. Ramalingam. Scaling up
    exact neural network compression by ReLU stability. In *Neural Information Processing
    Systems (NeurIPS)*, volume 34, 2021.
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serra等人（2021）T. Serra, X. Yu, A. Kumar, 和 S. Ramalingam。通过ReLU稳定性扩展精确的神经网络压缩。在*Neural
    Information Processing Systems (NeurIPS)*，第34卷，2021年。
- en: Shi et al. (2022) C. Shi, M. Emadikhiav, L. Lozano, and D. Bergman. Careful!
    training relevance is real. *arXiv:2201.04429*, 2022.
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi等人（2022）C. Shi, M. Emadikhiav, L. Lozano, 和 D. Bergman。小心！训练相关性是真实的。*arXiv:2201.04429*，2022年。
- en: 'Sidrane et al. (2022) C. Sidrane, A. Maleki, A. Irfan, and M. J. Kochenderfer.
    Overt: An algorithm for safety verification of neural network control policies
    for nonlinear systems. *Journal of Machine Learning Research*, 23(117):1–45, 2022.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sidrane等人（2022）C. Sidrane, A. Maleki, A. Irfan, 和 M. J. Kochenderfer。OVERT：一种用于非线性系统神经网络控制策略安全验证的算法。*Journal
    of Machine Learning Research*，23(117):1–45，2022年。
- en: Sildir and Aydin (2022) H. Sildir and E. Aydin. A mixed-integer linear programming
    based training and feature selection method for artificial neural networks using
    piece-wise linear approximations. *Chemical Engineering Science*, 249:117273,
    2022.
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sildir和Aydin（2022）H. Sildir和E. Aydin。基于混合整数线性规划的训练和特征选择方法，用于使用分段线性逼近的人工神经网络。*Chemical
    Engineering Science*，249:117273，2022年。
- en: Silver et al. (2017) D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
    A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap,
    F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering
    the game of go without human knowledge. *Nature*, 550:354–359, 2017.
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silver等人（2017）D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
    A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap, F. Hui,
    L. Sifre, G. van den Driessche, T. Graepel, 和 D. Hassabis。在没有人类知识的情况下掌握围棋。*Nature*，550:354–359，2017年。
- en: Singh et al. (2018) G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. Vechev.
    Fast and effective robustness certification. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018.
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh等人（2018）G. Singh, T. Gehr, M. Mirman, M. Püschel, 和 M. Vechev。快速有效的鲁棒性认证。*Neural
    Information Processing Systems (NeurIPS)*，31，2018年。
- en: Singh et al. (2019a) G. Singh, R. Ganvir, M. Püschel, and M. Vechev. Beyond
    the single neuron convex barrier for neural network certification. *Neural Information
    Processing Systems (NeurIPS)*, 32, 2019a.
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等（2019a）G. Singh, R. Ganvir, M. Püschel 和 M. Vechev. 超越单个神经元的凸障碍进行神经网络认证。*神经信息处理系统
    (NeurIPS)*，32，2019a。
- en: Singh et al. (2019b) G. Singh, T. Gehr, M. Püschel, and M. Vechev. An abstract
    domain for certifying neural networks. *Proceedings of the ACM on Programming
    Languages (POPL)*, 3:1–30, 2019b.
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等（2019b）G. Singh, T. Gehr, M. Püschel 和 M. Vechev. 用于认证神经网络的抽象域。*ACM 编程语言会议论文集
    (POPL)*，3:1–30，2019b。
- en: Singh et al. (2021) H. Singh, M. P. Kumar, P. Torr, and K. D. Dvijotham. Overcoming
    the convex barrier for simplex inputs. In *Neural Information Processing Systems
    (NeurIPS)*, volume 34, 2021.
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等（2021）H. Singh, M. P. Kumar, P. Torr 和 K. D. Dvijotham. 克服简单输入的凸障碍。发表于
    *神经信息处理系统 (NeurIPS)*，第34卷，2021。
- en: 'Smith and Winkler (2006) J. E. Smith and R. L. Winkler. The optimizer’s curse:
    Skepticism and postdecision surprise in decision analysis. *Management Science*,
    52(3):311–322, 2006.'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 和 Winkler（2006）J. E. Smith 和 R. L. Winkler. 优化器的诅咒：决策分析中的怀疑主义与决策后惊讶。*管理科学*，52(3):311–322，2006。
- en: 'Srivastava et al. (2014) N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
    and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting.
    *Journal of Machine Learning Research*, 15(56):1929–1958, 2014.'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等（2014）N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever 和 R.
    Salakhutdinov. Dropout：防止神经网络过拟合的简单方法。*机器学习研究杂志*，15(56):1929–1958，2014。
- en: Strong et al. (2021) C. A. Strong, H. Wu, A. Zeljić, K. D. Julian, G. Katz,
    C. Barrett, and M. J. Kochenderfer. Global optimization of objective functions
    represented by ReLU networks. *Machine Learning*, pages 1–28, 2021.
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strong 等（2021）C. A. Strong, H. Wu, A. Zeljić, K. D. Julian, G. Katz, C. Barrett
    和 M. J. Kochenderfer. ReLU 网络表示的目标函数的全局优化。*机器学习*，第1–28页，2021。
- en: 'Strong et al. (2022) C. A. Strong, S. M. Katz, A. L. Corso, and M. J. Kochenderfer.
    ZoPE: a fast optimizer for ReLU networks with low-dimensional inputs. In *NASA
    Formal Methods: 14th International Symposium, (NFM)*, pages 299–317\. Springer,
    2022.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strong 等（2022）C. A. Strong, S. M. Katz, A. L. Corso 和 M. J. Kochenderfer. ZoPE：一个针对低维输入的
    ReLU 网络的快速优化器。发表于 *NASA 正式方法：第14届国际研讨会 (NFM)*，第299–317页。Springer，2022。
- en: 'Sudjianto et al. (2020) A. Sudjianto, W. Knauth, R. Singh, Z. Yang, and A. Zhang.
    Unwrapping the black box of deep ReLU networks: Interpretability, diagnostics,
    and simplification. *arXiv:2011.04041*, 2020.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sudjianto 等（2020）A. Sudjianto, W. Knauth, R. Singh, Z. Yang 和 A. Zhang. 揭开深度
    ReLU 网络的黑箱：可解释性、诊断和简化。*arXiv:2011.04041*，2020。
- en: Sutskever et al. (2013) I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On
    the importance of initialization and momentum in deep learning. In *International
    Conference on Machine Learning (ICML)*, 2013.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2013）I. Sutskever, J. Martens, G. Dahl 和 G. Hinton. 在深度学习中初始化和动量的重要性。发表于
    *国际机器学习会议 (ICML)*，2013。
- en: Sutskever et al. (2014) I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence
    learning with neural networks. In *Neural Information Processing Systems (NeurIPS)*,
    volume 27, 2014.
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutskever 等（2014）I. Sutskever, O. Vinyals 和 Q. Le. 使用神经网络的序列到序列学习。发表于 *神经信息处理系统
    (NeurIPS)*，第27卷，2014。
- en: Szegedy et al. (2014) C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
    I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2014.
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2014）C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I.
    Goodfellow 和 R. Fergus. 神经网络的有趣特性。发表于 *学习表征国际会议 (ICLR)*，2014。
- en: Szegedy et al. (2015) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In
    *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015.
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2015）C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke 和 A. Rabinovich. 深入卷积网络。发表于 *计算机视觉与模式识别会议 (CVPR)*，2015。
- en: 'Takai et al. (2021) Y. Takai, A. Sannai, and M. Cordonnier. On the number of
    linear functions composing deep neural network: Towards a refined definition of
    neural networks complexity. In *International Conference on Artificial Intelligence
    and Statistics (AISTATS)*, 2021.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takai 等（2021）Y. Takai, A. Sannai 和 M. Cordonnier. 关于构成深度神经网络的线性函数数量：迈向神经网络复杂性的精确定义。发表于
    *国际人工智能与统计会议 (AISTATS)*，2021。
- en: Tao et al. (2022) Q. Tao, L. Li, X. Huang, X. Xi, S. Wang, and J. A. Suykens.
    Piecewise linear neural networks and deep learning. *Nature Reviews Methods Primers*,
    2, 2022.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等（2022）Q. Tao, L. Li, X. Huang, X. Xi, S. Wang 和 J. A. Suykens. 分段线性神经网络与深度学习。*自然评论方法入门*，2，2022。
- en: Telgarsky (2015) M. Telgarsky. Representation benefits of deep feedforward networks.
    *arXiv:1509.08101*, 2015.
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telgarsky (2015) M. Telgarsky. 深度前馈网络的表示优势。*arXiv:1509.08101*，2015年。
- en: Thorbjarnarson and Yorke-Smith (2020) T. Thorbjarnarson and N. Yorke-Smith.
    On training neural networks with mixed integer programming. *arXiv:2009.03825*,
    2020.
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorbjarnarson 和 Yorke-Smith (2020) T. Thorbjarnarson 和 N. Yorke-Smith. 关于使用混合整数规划训练神经网络。*arXiv:2009.03825*，2020年。
- en: Thorbjarnarson and Yorke-Smith (2023) T. Thorbjarnarson and N. Yorke-Smith.
    Optimal training of integer-valued neural networks with mixed integer programming.
    *PLOS One*, 18(2):e0261029, 2023.
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thorbjarnarson 和 Yorke-Smith (2023) T. Thorbjarnarson 和 N. Yorke-Smith. 使用混合整数规划对整数值神经网络进行优化训练。*PLOS
    One*，18(2):e0261029，2023年。
- en: Tiwari and Konidaris (2022) S. Tiwari and G. Konidaris. Effects of data geometry
    in early deep learning. In *Neural Information Processing Systems (NeurIPS)*,
    2022.
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tiwari 和 Konidaris (2022) S. Tiwari 和 G. Konidaris. 数据几何在早期深度学习中的影响。在*神经信息处理系统（NeurIPS）*，2022年。
- en: 'Tjandraatmadja et al. (2020) C. Tjandraatmadja, R. Anderson, J. Huchette, W. Ma,
    K. K. Patel, and J. P. Vielma. The convex relaxation barrier, revisited: Tightened
    single-neuron relaxations for neural network verification. *Neural Information
    Processing Systems (NeurIPS)*, 33:21675–21686, 2020.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjandraatmadja et al. (2020) C. Tjandraatmadja, R. Anderson, J. Huchette, W.
    Ma, K. K. Patel, 和 J. P. Vielma. 凸松弛屏障，重访：针对神经网络验证的收紧单神经元松弛。*神经信息处理系统（NeurIPS）*，33:21675–21686，2020年。
- en: Tjeng et al. (2019) V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness
    of neural networks with mixed integer programming. In *International Conference
    on Learning Representations (ICLR)*, 2019.
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tjeng et al. (2019) V. Tjeng, K. Xiao, 和 R. Tedrake. 使用混合整数规划评估神经网络的鲁棒性。在*国际学习表示大会（ICLR）*，2019年。
- en: 'Trimmel et al. (2021) M. Trimmel, H. Petzka, and C. Sminchisescu. TropEx: An
    algorithm for extracting linear terms in deep neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2021.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trimmel et al. (2021) M. Trimmel, H. Petzka, 和 C. Sminchisescu. TropEx：提取深度神经网络中线性项的算法。在*国际学习表示大会（ICLR）*，2021年。
- en: 'Tsay and Baldea (2019) C. Tsay and M. Baldea. 110th anniversary: using data
    to bridge the time and length scales of process systems. *Industrial & Engineering
    Chemistry Research*, 58(36):16696–16708, 2019.'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsay 和 Baldea (2019) C. Tsay 和 M. Baldea. 110周年纪念：使用数据桥接过程系统的时间和长度尺度。*工业与工程化学研究*，58(36):16696–16708，2019年。
- en: Tsay et al. (2021) C. Tsay, J. Kronqvist, A. Thebelt, and R. Misener. Partition-based
    formulations for mixed-integer optimization of trained ReLU neural networks. In
    *Neural Information Processing Systems (NeurIPS)*, volume 34, 2021.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsay et al. (2021) C. Tsay, J. Kronqvist, A. Thebelt, 和 R. Misener. 针对训练好的ReLU神经网络的混合整数优化的基于分区的公式。*神经信息处理系统（NeurIPS）*，第34卷，2021年。
- en: Tseran and Montúfar (2021) H. Tseran and G. Montúfar. On the expected complexity
    of maxout networks. In *Neural Information Processing Systems (NeurIPS)*, volume 34,
    2021.
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseran 和 Montúfar (2021) H. Tseran 和 G. Montúfar. 关于maxout网络的期望复杂度。在*神经信息处理系统（NeurIPS）*，第34卷，2021年。
- en: Unser (2019) M. Unser. A representer theorem for deep neural networks. *Journal
    of Machine Learning Research*, 20:1–30, 2019.
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unser (2019) M. Unser. 深度神经网络的表示定理。*机器学习研究杂志*，20:1–30，2019年。
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In *Neural
    Information Processing Systems (NeurIPS)*, 2017.
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, 和 I. Polosukhin. 注意力机制才是你所需要的一切。在*神经信息处理系统（NeurIPS）*，2017年。
- en: Vielma (2015) J. P. Vielma. Mixed integer linear programming formulation techniques.
    *SIAM Review*, 57(1):3–57, 2015.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vielma (2015) J. P. Vielma. 混合整数线性规划公式化技术。*SIAM 评审*，57(1):3–57，2015年。
- en: Vielma (2019) J. P. Vielma. Small and strong formulations for unions of convex
    sets from the cayley embedding. *Mathematical Programming*, 177(1-2):21–53, 2019.
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vielma (2019) J. P. Vielma. 从Cayley嵌入中得到的凸集并的简洁且强大的公式。*数学规划*，177(1-2):21–53，2019年。
- en: 'Vielma et al. (2010) J. P. Vielma, S. Ahmed, and G. Nemhauser. Mixed-integer
    models for nonseparable piecewise-linear optimization: Unifying framework and
    extensions. *Operations Research*, 58(2):303–315, 2010.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vielma et al. (2010) J. P. Vielma, S. Ahmed, 和 G. Nemhauser. 非分离的分段线性优化的混合整数模型：统一框架及扩展。*运筹学*，58(2):303–315，2010年。
- en: Villani and Schoots (2023) M. J. Villani and N. Schoots. Any deep ReLU network
    is shallow. *arXiv:2306.11827*, 2023.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villani 和 Schoots (2023) M. J. Villani 和 N. Schoots. 任何深度ReLU网络都是浅层的。*arXiv:2306.11827*，2023年。
- en: 'Vincent and Schwager (2021) J. A. Vincent and M. Schwager. Reachable polyhedral
    marching (RPM): A safety verification algorithm for robotic systems with deep
    neural network components. In *IEEE International Conference on Robotics and Automation
    (ICRA)*, 2021.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vincent 和 Schwager（2021）J. A. Vincent 和 M. Schwager. 可达的多面体行进（RPM）：一种用于具有深度神经网络组件的机器人系统的安全验证算法。在*IEEE
    国际机器人与自动化会议（ICRA）*，2021。
- en: 'Vinyals et al. (2017) O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S.
    Vezhnevets, M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, J. Quan,
    S. Gaffney, S. Petersen, K. Simonyan, T. Schaul, H. van Hasselt, D. Silver, T. Lillicrap,
    K. Calderone, P. Keet, A. Brunasso, D. Lawrence, A. Ekermo, J. Repp, and R. Tsing.
    StarCraft II: A new challenge for reinforcement learning. *arXiv:1708.04782*,
    2017.'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vinyals 等（2017）O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S. Vezhnevets,
    M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, J. Quan, S. Gaffney,
    S. Petersen, K. Simonyan, T. Schaul, H. van Hasselt, D. Silver, T. Lillicrap,
    K. Calderone, P. Keet, A. Brunasso, D. Lawrence, A. Ekermo, J. Repp, 和 R. Tsing.
    《StarCraft II：强化学习的新挑战》。*arXiv:1708.04782*，2017。
- en: 'Virmaux and Scaman (2018) A. Virmaux and K. Scaman. Lipschitz regularity of
    deep neural networks: analysis and efficient estimation. In *Neural Information
    Processing Systems (NeurIPS)*, volume 31, 2018.'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Virmaux 和 Scaman（2018）A. Virmaux 和 K. Scaman. 深度神经网络的 Lipschitz 正则性：分析与高效估计。在*神经信息处理系统（NeurIPS）*，第31卷，2018。
- en: Volpp et al. (2020) M. Volpp, L. P. Fröhlich, K. Fischer, A. Doerr, S. Falkner,
    F. Hutter, and C. Daniel. Meta-learning acquisition functions for transfer learning
    in bayesian optimization. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Volpp 等（2020）M. Volpp, L. P. Fröhlich, K. Fischer, A. Doerr, S. Falkner, F.
    Hutter, 和 C. Daniel. 元学习获取函数用于贝叶斯优化中的迁移学习。在*国际学习表征会议（ICLR）*，2020。
- en: Wang et al. (2021) K. Wang, L. Lozano, D. Bergman, and C. Cardonha. A two-stage
    exact algorithm for optimization of neural network ensemble. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2021）K. Wang, L. Lozano, D. Bergman, 和 C. Cardonha. 用于神经网络集成优化的两阶段精确算法。在*约束编程、人工智能与运筹学整合国际会议（CPAIOR）*，2021。
- en: Wang et al. (2023) K. Wang, L. Lozano, C. Cardonha, and D. Bergman. Optimizing
    over an ensemble of trained neural networks. *INFORMS Journal on Computing*, 2023.
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）K. Wang, L. Lozano, C. Cardonha, 和 D. Bergman. 对训练神经网络集成进行优化。*INFORMS
    计算期刊*，2023。
- en: Wang et al. (2018a) S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Efficient
    formal safety analysis of neural networks. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018a.
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018a）S. Wang, K. Pei, J. Whitehouse, J. Yang, 和 S. Jana. 神经网络的高效正式安全分析。*神经信息处理系统（NeurIPS）*，第31卷，2018a。
- en: Wang et al. (2018b) S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Formal
    security analysis of neural networks using symbolic intervals. In *27th $\{$USENIX$\}$
    Security Symposium ($\{$USENIX$\}$ Security 18)*, pages 1599–1614, 2018b.
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2018b）S. Wang, K. Pei, J. Whitehouse, J. Yang, 和 S. Jana. 使用符号区间的神经网络正式安全分析。在*第27届
    $\{$USENIX$\}$ 安全研讨会 ($\{$USENIX$\}$ 安全 18)*，第1599–1614页，2018b。
- en: Wang (2022) Y. Wang. Estimation and comparison of linear regions for relu networks.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, 2022.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang（2022）Y. Wang. Relu 网络的线性区域的估计与比较。在*国际人工智能联合会议（IJCAI）*，2022。
- en: 'Weng et al. (1992) J. Weng, N. Ahuja, and T. Huang. Cresceptron: a self-organizing
    neural network which grows adaptively. In *International Joint Conference on Neural
    Networks (IJCNN)*, 1992.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng 等（1992）J. Weng, N. Ahuja, 和 T. Huang. Cresceptron：一种自组织的自适应增长神经网络。在*国际神经网络联合会议（IJCNN）*，1992。
- en: Weng et al. (2018) L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel,
    D. Boning, and I. Dhillon. Towards fast computation of certified robustness for
    ReLU networks. In *International Conference on Machine Learning (ICML)*, pages
    5276–5285, 2018.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weng 等（2018）L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel, D.
    Boning, 和 I. Dhillon. 迈向对 ReLU 网络认证鲁棒性的快速计算。在*国际机器学习会议（ICML）*，第5276–5285页，2018。
- en: 'Werbos (1974) P. Werbos. *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences*. PhD thesis, Harvard University, 1974.'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Werbos（1974）P. Werbos. *超越回归：行为科学中预测和分析的新工具*。博士论文，哈佛大学，1974。
- en: Wicker et al. (2020) M. Wicker, L. Laurenti, A. Patane, and M. Kwiatkowska.
    Probabilistic safety for Bayesian neural networks. In *Conference on Uncertainty
    in Artificial Intelligence (UAI)*, pages 1198–1207, 2020.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wicker 等（2020）M. Wicker, L. Laurenti, A. Patane, 和 M. Kwiatkowska. 贝叶斯神经网络的概率安全性。在*人工智能不确定性会议（UAI）*，第1198–1207页，2020。
- en: Wicker et al. (2022) M. Wicker, J. Heo, L. Costabello, and A. Weller. Robust
    explanation constraints for neural networks. *arXiv:2212.08507*, 2022.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wicker 等人（2022）M. Wicker, J. Heo, L. Costabello, 和 A. Weller。用于神经网络的鲁棒解释约束。*arXiv:2212.08507*，2022年。
- en: Wilhelm et al. (2022) M. E. Wilhelm, C. Wang, and M. D. Stuber. Convex and concave
    envelopes of artificial neural network activation functions for deterministic
    global optimization. *Journal of Global Optimization*, pages 1–26, 2022.
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilhelm 等人（2022）M. E. Wilhelm, C. Wang, 和 M. D. Stuber。用于确定性全局优化的人工神经网络激活函数的凸包和凹包。*全局优化期刊*，第1–26页，2022年。
- en: Wong and Kolter (2018) E. Wong and Z. Kolter. Provable defenses against adversarial
    examples via the convex outer adversarial polytope. In *International Conference
    on Machine Learning (ICML)*, pages 5286–5295, 2018.
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 和 Kolter（2018）E. Wong 和 Z. Kolter。通过凸外对抗多面体提供可证明的对抗防御。发表于*国际机器学习会议（ICML）*，第5286–5295页，2018年。
- en: Wong et al. (2018) E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling
    provable adversarial defenses. *Neural Information Processing Systems (NeurIPS)*,
    31, 2018.
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 等人（2018）E. Wong, F. Schmidt, J. H. Metzen, 和 J. Z. Kolter。扩展可证明的对抗防御。*神经信息处理系统（NeurIPS）*，第31卷，2018年。
- en: Wright (2018) S. J. Wright. Optimization algorithms for data analysis. *The
    Mathematics of Data*, 25:49, 2018.
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wright（2018）S. J. Wright。数据分析的优化算法。*数据数学*，25:49，2018年。
- en: Wu et al. (2020) G. Wu, B. Say, and S. Sanner. Scalable planning with deep neural
    network learned transition models. *Journal of Artificial Intelligence Research*,
    68:571–606, 2020.
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2020）G. Wu, B. Say, 和 S. Sanner。利用深度神经网络学习的转移模型进行可扩展规划。*人工智能研究期刊*，68:571–606，2020年。
- en: Wu et al. (2022) H. Wu, A. Zeljić, G. Katz, and C. Barrett. Efficient neural
    network analysis with sum-of-infeasibilities. In *Tools and Algorithms for the
    Construction and Analysis of Systems (TACAS)*, pages 143–163\. Springer, 2022.
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2022）H. Wu, A. Zeljić, G. Katz, 和 C. Barrett。利用不合规性的和进行高效的神经网络分析。发表于*工具与算法系统的构建与分析（TACAS）*，第143–163页，Springer，2022年。
- en: Xiang et al. (2017) W. Xiang, H.-D. Tran, and T. T. Johnson. Reachable set computation
    and safety verification for neural networks with ReLU activations. *arXiv:1712.08163*,
    2017.
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang 等人（2017）W. Xiang, H.-D. Tran, 和 T. T. Johnson。具有 ReLU 激活的神经网络的可达集计算和安全性验证。*arXiv:1712.08163*，2017年。
- en: Xiao et al. (2019) K. Xiao, V. Tjeng, N. Shafiullah, and A. Madry. Training
    for faster adversarial robustness verification via inducing ReLU stability. *International
    Conference on Learning Representations (ICLR)*, 2019.
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人（2019）K. Xiao, V. Tjeng, N. Shafiullah, 和 A. Madry。通过引入 ReLU 稳定性来训练以加速对抗鲁棒性验证。*国际学习表征会议（ICLR）*，2019年。
- en: Xie et al. (2020a) J. Xie, Z. Shen, C. Zhang, B. Wang, and H. Qian. Efficient
    projection-free online methods with stochastic recursive gradient. In *AAAI Conference
    on Artificial Intelligence*, volume 34, pages 6446–6453, 2020a.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人（2020a）J. Xie, Z. Shen, C. Zhang, B. Wang, 和 H. Qian。无需投影的高效在线方法与随机递归梯度。发表于*AAAI
    人工智能会议*，第34卷，第6446–6453页，2020a年。
- en: Xie et al. (2020b) Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training
    with noisy student improves ImageNet classification. In *Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2020b.
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人（2020b）Q. Xie, M.-T. Luong, E. Hovy, 和 Q. V. Le。带噪声学生的自我训练提升了 ImageNet
    分类。发表于*计算机视觉与模式识别会议（CVPR）*，2020b年。
- en: Xie et al. (2020c) Y. Xie, G. Chen, and Q. Li. A general computational framework
    to measure the expressiveness of complex networks using a tighter upper bound
    of linear regions. *arXiv:2012.04428*, 2020c.
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人（2020c）Y. Xie, G. Chen, 和 Q. Li。使用更紧的线性区域上界来测量复杂网络的表现力的一般计算框架。*arXiv:2012.04428*，2020c年。
- en: Xiong et al. (2020) H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao.
    On the number of linear regions of convolutional neural networks. In *International
    Conference on Machine Learning (ICML)*, 2020.
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong 等人（2020）H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, 和 L. Shao。卷积神经网络的线性区域数量。发表于*国际机器学习会议（ICML）*，2020年。
- en: Xu et al. (2022) S. Xu, J. Vaughan, J. Chen, A. Zhang, and A. Sudjianto. Traversing
    the local polytopes of ReLU neural networks. In *AAAI Workshop AdvML*, 2022.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2022）S. Xu, J. Vaughan, J. Chen, A. Zhang, 和 A. Sudjianto。遍历 ReLU 神经网络的局部多面体。发表于*AAAI
    Workshop AdvML*，2022年。
- en: Yang et al. (2022) D. Yang, P. Balaprakash, and S. Leyffer. Modeling design
    and control problems involving neural network surrogates. *Computational Optimization
    and Applications*, pages 1–42, 2022.
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人（2022）D. Yang, P. Balaprakash, 和 S. Leyffer。涉及神经网络替代模型的建模设计与控制问题。*计算优化与应用*，第1–42页，2022年。
- en: Yang et al. (2020) X. Yang, H.-D. Tran, W. Xiang, and T. Johnson. Reachability
    analysis for feed-forward neural networks using face lattices. *arXiv:2003.01226*,
    2020.
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2020）X. Yang, H.-D. Tran, W. Xiang, 和 T. Johnson. 使用面格对前馈神经网络的可达性分析。*arXiv:2003.01226*，2020。
- en: Yang et al. (2021) X. Yang, T. Yamaguchi, H.-D. Tran, B. Hoxha, T. T. Johnson,
    and D. Prokhorov. Reachability analysis of convolutional neural networks. *arXiv:2106.12074*,
    2021.
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2021）X. Yang, T. Yamaguchi, H.-D. Tran, B. Hoxha, T. T. Johnson, 和 D.
    Prokhorov. 卷积神经网络的可达性分析。*arXiv:2106.12074*，2021。
- en: Yarotsky (2017) D. Yarotsky. Error bounds for approximations with deep ReLU
    networks. *Neural Networks*, 94, 2017.
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yarotsky（2017）D. Yarotsky. 深度 ReLU 网络的近似误差界限。*Neural Networks*，94，2017。
- en: Zakrzewski (2001) R. R. Zakrzewski. Verification of a trained neural network
    accuracy. In *International Joint Conference on Neural Networks (IJCNN)*, volume 3,
    pages 1657–1662\. IEEE, 2001.
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zakrzewski（2001）R. R. Zakrzewski. 训练神经网络准确性的验证。收录于 *International Joint Conference
    on Neural Networks (IJCNN)*，第 3 卷，页面 1657–1662。IEEE，2001。
- en: 'Zaslavsky (1975) T. Zaslavsky. *Facing Up to Arrangements: Face-Count Formulas
    for Partitions of Space by Hyperplanes*. American Mathematical Society, 1975.'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zaslavsky（1975）T. Zaslavsky. *面对排列：超平面空间分割的面计数公式*。美国数学学会，1975。
- en: Zhang et al. (2023) A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola. *Dive into
    Deep Learning*. 2023. https://d2l.ai.
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）A. Zhang, Z. C. Lipton, M. Li, 和 A. J. Smola. *深入学习*。2023。https://d2l.ai。
- en: Zhang et al. (2018a) H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel.
    Efficient neural network robustness certification with general activation functions.
    *Neural Information Processing Systems (NeurIPS)*, 31, 2018a.
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018a）H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, 和 L. Daniel. 使用通用激活函数进行高效神经网络鲁棒性认证。*Neural
    Information Processing Systems (NeurIPS)*，31，2018a。
- en: Zhang et al. (2020) H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li,
    D. Boning, and C.-J. Hsieh. Towards stable and efficient training of verifiably
    robust neural networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2020）H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning,
    和 C.-J. Hsieh. 朝向稳定和高效的可验证鲁棒神经网络训练。收录于 *International Conference on Learning Representations
    (ICLR)*，2020。
- en: Zhang et al. (2022) H. Zhang, S. Wang, K. Xu, L. Li, B. Li, S. Jana, C.-J. Hsieh,
    and J. Z. Kolter. General cutting planes for bound-propagation-based neural network
    verification. In *Neural Information Processing Systems (NeurIPS)*, volume 35,
    2022.
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）H. Zhang, S. Wang, K. Xu, L. Li, B. Li, S. Jana, C.-J. Hsieh, 和
    J. Z. Kolter. 基于界限传播的神经网络验证的一般割平面方法。收录于 *Neural Information Processing Systems
    (NeurIPS)*，第 35 卷，2022。
- en: Zhang et al. (2018b) L. Zhang, G. Naitzat, and L.-H. Lim. Tropical geometry
    of deep neural networks. In *International Conference on Machine Learning (ICML)*,
    2018b.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2018b）L. Zhang, G. Naitzat, 和 L.-H. Lim. 深度神经网络的热带几何。收录于 *International
    Conference on Machine Learning (ICML)*，2018b。
- en: Zhang (2020) R. Zhang. On the tightness of semidefinite relaxations for certifying
    robustness to adversarial examples. *Neural Information Processing Systems (NeurIPS)*,
    33:3808–3820, 2020.
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang（2020）R. Zhang. 针对对抗样本认证的半正定松弛的紧界限。*Neural Information Processing Systems
    (NeurIPS)*，33:3808–3820，2020。
- en: Zhang and Wu (2020) X. Zhang and D. Wu. Empirical studies on the properties
    of linear regions in deep neural networks. In *International Conference on Learning
    Representations (ICLR)*, 2020.
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 和 Wu（2020）X. Zhang 和 D. Wu. 深度神经网络中线性区域属性的实证研究。收录于 *International Conference
    on Learning Representations (ICLR)*，2020。
- en: 'Zhao et al. (2023) S. Zhao, C. Tsay, and J. Kronqvist. Model-based feature
    selection for neural networks: A mixed-integer programming approach. *arXiv:2302.10344*,
    2023.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2023）S. Zhao, C. Tsay, 和 J. Kronqvist. 基于模型的神经网络特征选择：一种混合整数编程方法。*arXiv:2302.10344*，2023。
- en: Zhou and Schoellig (2019) S. Zhou and A. P. Schoellig. An analysis of the expressiveness
    of deep neural network architectures based on their Lipschitz constants. *arXiv:1912.11511*,
    2019.
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 和 Schoellig（2019）S. Zhou 和 A. P. Schoellig. 基于 Lipschitz 常数的深度神经网络架构表达能力分析。*arXiv:1912.11511*，2019。
- en: Zhu et al. (2020) R. Zhu, B. Lin, and H. Tang. Bounding the number of linear
    regions in local area for neural networks with ReLU activations. *arXiv:2007.06803*,
    2020.
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2020）R. Zhu, B. Lin, 和 H. Tang. 对带有 ReLU 激活的神经网络局部区域线性区域数量的界限。*arXiv:2007.06803*，2020。
- en: Zou et al. (2019) D. Zou, R. Balan, and M. Singh. On Lipschitz bounds of general
    convolutional neural networks. *IEEE Transactions on Information Theory*, 66(3):1738–1759,
    2019.
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等（2019）D. Zou, R. Balan, 和 M. Singh. 一般卷积神经网络的 Lipschitz 界限。*IEEE Transactions
    on Information Theory*，66(3):1738–1759，2019。
