- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:40:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.00241] When Deep Learning Meets Polyhedral Theory: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.00241](https://ar5iv.labs.arxiv.org/html/2305.00241)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'When Deep Learning Meets Polyhedral Theory: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Joey Huchette
  prefs: []
  type: TYPE_NORMAL
- en: Google Research, USA    Gonzalo Muñoz
  prefs: []
  type: TYPE_NORMAL
- en: Universidad de O’Higgins, Chile    Thiago Serra
  prefs: []
  type: TYPE_NORMAL
- en: Bucknell University, USA    Calvin Tsay
  prefs: []
  type: TYPE_NORMAL
- en: Imperial College London, UK(September 2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the past decade, deep learning became the prevalent methodology for predictive
    modeling thanks to the remarkable accuracy of deep neural networks in tasks such
    as computer vision and natural language processing. Meanwhile, the structure of
    neural networks converged back to simpler representations based on piecewise constant
    and piecewise linear functions such as the Rectified Linear Unit (ReLU), which
    became the most commonly used type of activation function in neural networks.
    That made certain types of network structure —such as the typical fully-connected
    feedforward neural network— amenable to analysis through polyhedral theory and
    to the application of methodologies such as Linear Programming (LP) and Mixed-Integer
    Linear Programming (MILP) for a variety of purposes. In this paper, we survey
    the main topics emerging from this fast-paced area of work, which brings a fresh
    perspective to understanding neural networks in more detail as well as to applying
    linear optimization techniques to train, verify, and reduce the size of such networks.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning has continuously achieved new landmarks in varied areas of artificial
    intelligence for the past decade. Examples of those areas include predictive tasks
    in computer vision (Krizhevsky et al., [2012](#bib.bib178), Ciresan et al., [2012](#bib.bib60),
    Szegedy et al., [2015](#bib.bib301), He et al., [2016](#bib.bib144), Xie et al.,
    [2020b](#bib.bib343)), natural language processing (Sutskever et al., [2014](#bib.bib299),
    Peters et al., [2018](#bib.bib246), Radford et al., [2018](#bib.bib252), Devlin
    et al., [2019](#bib.bib80)), and speech recognition (Hinton et al., [2012](#bib.bib148),
    Graves and Jaitly, [2014](#bib.bib130), Park et al., [2019](#bib.bib240)). The
    artificial neural networks behind such feats are being used in many applications,
    and there is a growing interest for analytical insights to help design such networks
    and then to leverage the model that they have learned. For the most commonly used
    types of neural networks, some of those results and methods are coming from operations
    research tools such as polyhedral theory and associated optimization techniques
    such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP). Among
    other things, these connections with mathematical optimization may help us understand
    what neural networks can represent, how to train them, and how to make them more
    compact. For example, consider the popular task of classifying images (Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")); polyhedral theory and associated optimization techniques may help
    us answer questions such as the following. How should we train the classifier
    model? How large should it be? How robust to perturbations is it?'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17c4f1fa42e56b26a18a8a37fef55a33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example classification task on the MNIST database of handwritten
    digits, in which the image of a handwritten digit is given as input and the probability
    of that digit being from each possible class is provided as output.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 What neural networks can model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can essentially think of artificial neural networks as functions mapping
    an input ${\bm{x}}$ from a given domain to an output ${\bm{y}}$ for a given application.
    For the classification task in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    When Deep Learning Meets Polyhedral Theory: A Survey"), inputs ${\bm{x}}$ correspond
    to images from the dataset, and ${\bm{y}}$ to the associated predicted labels,
    or probabilities for labels describing the content of those images. The basic
    units of neural networks mimic biological neurons in that they receive inputs
    from adjacent units, transform those inputs, and may produce an output to subsequent
    units of the network. In other words, every unit is also a function, and in fact
    the output of most units is defined by the composition of a nonlinear function
    with a linear function. The nonlinear function is often denoted as the *activation
    function* in analogy to how a biological neuron is triggered to send a signal
    to adjacent neurons when the stimulus caused by the input exceeds a certain activation
    threshold. Such non-linearity is behind the remarkable expressiveness of neural
    networks.'
  prefs: []
  type: TYPE_NORMAL
- en: This model was pioneered by McCulloch and Pitts ([1943](#bib.bib217)), who considered
    a thresholding function for activation that is now often denoted as the Linear
    Threshold Unit (LTU). That activation is also the basis of the classic *perceptron*
    algorithm by  Rosenblatt ([1957](#bib.bib261)), which yields a binary classifier
    of the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f({\bm{x}})=\left\{\begin{array}[]{cl}1&amp;\text{if }{\bm{w}}\cdot{\bm{x}}+b>0;\\
    0&amp;\text{otherwise}\end{array}\right.$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'for an input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and with parameters ${\bm{w}}\in\mathbb{R}^{n_{0}}$
    and $b\in\mathbb{R}$. Those parameters are chosen by optimizing the predictions
    for a given task, as discussed below and in Section [5](#S5 "5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey"). The term *single-layer perceptron* is used for a neural network consisting
    of a set of such units processing the same input in parallel. The term *multi-layer
    perceptron* is used for a generalization of this concept, by which the output
    of a *layer* —a set of units with same input— is the input for a subsequent layer.
    This perceptron terminology has also been loosely applied to neural networks with
    other activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: More generally, neural networks that successively transform inputs through an
    ordered sequence of layers are also denoted *feedforward networks*. The layers
    that do not produce the final output of the neural network are denoted *hidden
    layers*. For a network with $L$ layers, we denote $n_{l}$ as the number of units
    —or *width*— of layer $l\in{\mathbb{L}}:=\{1,2,\ldots,L\}$ and $h_{i}^{l}$ as
    the output of the $i$-th unit in layer $l$, where $i\in\{1,2,\ldots,n_{l}\}$.
    The output of a unit is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h_{i}^{l}=\sigma^{l}\left({\bm{w}}^{l}_{i}\cdot{\bm{h}}^{l-1}+b^{l}_{i}\right),$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where the *weights* ${\bm{w}}^{l}_{i}\in\mathbb{R}^{n_{l-1}}$ and the *bias*
    $b^{l}_{i}\in\mathbb{R}$ are parameters of the unit. Those parameters can be aggregated
    across the layer as the matrix ${\bm{W}}^{l}\in\mathbb{R}^{n_{l}\times n_{l-1}}$
    and the vector ${\bm{b}}^{l}\in\mathbb{R}^{n_{l}}$. The vector ${\bm{h}}^{l-1}\in\mathbb{R}^{n_{l-1}}$
    represents the aggregated outputs from layer $(l-1)$. The activation function
    $\sigma^{l}:\mathbb{R}\rightarrow\mathbb{R}$ is applied by the units in layer
    $l$. These definitions implicitly assume that $n_{0}$ is the size of the network
    input ${\bm{x}}\in\mathbb{R}^{n_{0}}$ and that ${\bm{h}}^{0}$ and ${\bm{x}}$ are
    the same. Figure [2](#S1.F2 "Figure 2 ‣ 1.1 What neural networks can model ‣ 1
    Introduction ‣ When Deep Learning Meets Polyhedral Theory: A Survey") illustrates
    the operation of a feedforward network as described above.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2db8fd387b48da880f36dff11bad8a69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Mapping from ${\bm{x}}\in\mathbb{R}^{n_{0}}$ to ${\bm{y}}\in\mathbb{R}^{n_{L}}$
    through a feedforward neural network with $L$ layers, layer widths $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and activation functions $\{\sigma_{l}\}_{l\in{\mathbb{L}}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 How neural networks are obtained and evaluated
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In resemblance to how other models for *supervised learning* in machine learning
    are obtained, we can *train* a neural network for a given task by adjusting its
    behavior with respect to the examples of a *training set* and then evaluate the
    final trained network on a *test set*. Both of these sets consist of inputs for
    which the correct output $\hat{y}$ is known. We can define an objective function
    to model a measure of distance between the output $y$ and the correct output $\hat{y}$,
    which is typically denoted as the *loss function*, and then iteratively update
    parameters such as $\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}}$ and $\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}$
    to minimize that loss function over the training set. A common objective function
    is the square error $\|y-\hat{y}\|^{2}$ summed over the points in the training
    set. The test set contains a separate collection of inputs and their outputs,
    which is used to evaluate the trained neural network with examples that were not
    seen during training. A good performance on the test set may indicate that the
    trained neural network is able to *generalize* beyond the seen examples, whereas
    a bad performance may suggest that it *overfits* for the training set. Neural
    networks also have *hyperparameters* that are often chosen manually and do not
    change during training, such as the *depth* $L$, the widths of the layers $\{n_{l}\}_{l\in{\mathbb{L}}}$,
    and the activation functions used in each layer $\{\sigma^{l}\}_{l\in{\mathbb{L}}}$.
    Different models can be produced by varying the hyperparameters. In such a case,
    a *validation set* disjoint from the training and test sets can be used to compare
    models with different hyperparameters. Whereas the validation set may serve as
    a benchmark to different trained models corresponding to different choices of
    hyperparameters, the test set can only be used to evaluate a single neural network
    chosen among those evaluated with the validation set. The emergent field of *neural
    architecture search* —recently surveyed by Elsken et al. ([2019](#bib.bib89))—
    concerns with automatically choosing such hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key factors for the success of deep learning is that first-order
    methods for continuous optimization can be effectively applied to train deep networks.
    The interest in neural networks first vanished due to negative results in the
    Perceptrons book by Minsky and Papert ([1969](#bib.bib219)), which showed that
    single-layer perceptrons cannot represent functions such as the Boolean XOR. However,
    moving to multi-layer perceptrons capable of expressing the Boolean XOR as well
    as other more expressive models would require a clever training strategy. Hence,
    the interest was regained with papers that popularized the use of *backpropagation*,
    such as  Rumelhart et al. ([1986](#bib.bib266)) and  LeCun et al. ([1989](#bib.bib185)).
    Note that backpropagation was first discussed much earlier in the context of networks
    by Linnainmaa ([1970](#bib.bib194)) and of neural networks explicitly by Werbos
    ([1974](#bib.bib331)). The backpropagation algorithm calculates the derivative
    of the loss function with respect to each neural network parameter by applying
    the chain rule through the units of the neural network, which is considerably
    more efficient than explicitly evaluating the derivative of each network parameter.
    Consequently, neural networks are generally trained with gradient descent methods
    in which the parameters are updated sequentially from the output to the input
    layer in each step. In fact, most algorithms for training neural networks are
    based on Stochastic Gradient Descent (SGD), which is a form of the stochastic
    approximation through sampling pioneered by Robbins and Monro ([1951](#bib.bib258)).
    SGD approximates the partial derivatives of the loss function at each step by
    using only a subset of the data in order to make the training process more efficient.
    Examples of popular SGD algorithms include momentum (Polyak, [1964](#bib.bib250)),
    Adam (Kingma and Ba, [2014](#bib.bib175)), and Nesterov Adaptive Gradient (Sutskever
    et al., [2013](#bib.bib298)) —the later inspired by Nesterov ([1983](#bib.bib231)).
    Interestingly, however, we generally cannot guarantee convergence to a global
    optimum with gradient descent due to the nonconvexity of the loss function. Nevertheless,
    neural networks trained with adequately parameterized SGD algorithms tend to generalize
    well.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Why nonlinearity is important in artificial neurons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The nonlinearity of the activation function leads to such nonconvexity of the
    loss function. However, as we will see in Section [3](#S3 "3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"),
    that same nonlinearity enables the neural network to represent more complex functions
    as a whole. In fact, removing such nonlinearities by using an identity activation
    function $\sigma^{l}(u)=u~{}\forall l\in{\mathbb{L}}$ would reduce the entire
    neural network to an affine transformation of the form $f(x)={\bm{W}}^{L}({\bm{W}}^{L-1}\left(\ldots\left({\bm{W}}^{2}\left({\bm{W}}^{1}x+{\bm{b}}^{1}\right)+{\bm{b}}^{2}\right)+\ldots\right)+{\bm{b}}^{L-1})+{\bm{b}}^{L}$.
    Hence, a feedforward network without nonlinear activation functions is equivalent
    to a linear regression model. However, in that case we can easily obtain such
    a model without resorting to neural networks and backpropagation: the loss function
    is convex and the optimal solution is given by a closed formula, such as in least
    squares regression. In contrast, neural networks with a single hidden layer of
    arbitrary width have been long known to be universal function approximators for
    a broad variety of activation functions (Cybenko, [1989](#bib.bib71), Funahashi,
    [1989](#bib.bib112), Hornik et al., [1989](#bib.bib153)), as well as for ReLU
    more recently (Yarotsky, [2017](#bib.bib350)). These results have also been extended
    to the converse case of limited width but arbitrarily large depth (Lu et al.,
    [2017](#bib.bib202), Hanin and Sellke, [2017](#bib.bib139), Park et al., [2021a](#bib.bib241)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although nonlinear activation functions are important for obtaining more complex
    models, these functions do not need to be overly complex to produce good results.
    In the past, it was common practice to use sigmoid functions for activation (LeCun
    et al., [1998](#bib.bib186)). Those are monotonically increasing functions that
    approach finite values for arbitrarily large positive and negative inputs, such
    as the standard logistic function $\sigma(u)=\frac{1}{1+e^{-u}}$ and the hyperbolic
    tangent $\sigma(u)=\tanh(u)$. In the present, the most commonly used activation
    function is the Rectified Linear Unit (ReLU) $\sigma(u)=\max\{0,u\}$ (LeCun et al.,
    [2015](#bib.bib187), Ramachandran et al., [2018](#bib.bib255)), which was proposed
    by Hahnloser et al. ([2000](#bib.bib135)) and first applied to neural networks
    by Nair and Hinton ([2010](#bib.bib228)). The popularity of ReLU is in part due
    to experiments by Nair and Hinton ([2010](#bib.bib228)) and Glorot et al. ([2011](#bib.bib119))
    showing that this simpler form of activation yields competitive results. Thinking
    back in terms of the analogy with biological neurons, we say that a ReLU is *active*
    when the output is positive and *inactive* when the output is zero. ReLUs have
    a linear output behavior on the inputs associated with the same ReLUs being active
    and inactive; this property also holds for other piecewise linear and piecewise
    constant functions that are used as activation functions in neural networks. Table [1](#S1.T1
    "Table 1 ‣ 1.3 Why nonlinearity is important in artificial neurons ‣ 1 Introduction
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") lists some of the most
    commonly used activation functions of that kind. For more comprehensive lists
    of activation functions, including several other variations based on ReLU, we
    refer to Dubey et al. ([2021](#bib.bib82)) and Tao et al. ([2022](#bib.bib303)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Main piecewise constant and piecewise linear activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Function | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LTU | $\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if }u>0\\ 0&amp;\text{if
    }u\leq 0\end{array}\right.$ | McCulloch and Pitts ([1943](#bib.bib217)) |'
  prefs: []
  type: TYPE_TB
- en: '| ReLU | $\sigma(u)=\max\{0,u\}$ | Hahnloser et al. ([2000](#bib.bib135)),
    Nair and Hinton ([2010](#bib.bib228)) |'
  prefs: []
  type: TYPE_TB
- en: '| leaky ReLU | <math id="S1.T1.3.3.1.m1.3" class="ltx_Math" alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ \varepsilon u&amp;\text{if }u\leq 0\end{array}\right.\\'
  prefs: []
  type: TYPE_NORMAL
- en: \text{($\varepsilon$ is small and fixed)}\end{array}" display="inline"><semantics
    id="S1.T1.3.3.1.m1.3a"><mtable rowspacing="0pt" id="S1.T1.3.3.1.m1.3.3" xref="S1.T1.3.3.1.m1.3.3.cmml"><mtr
    id="S1.T1.3.3.1.m1.3.3a" xref="S1.T1.3.3.1.m1.3.3.cmml"><mtd id="S1.T1.3.3.1.m1.3.3b"
    xref="S1.T1.3.3.1.m1.3.3.cmml"><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.4" xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml"><mi id="S1.T1.3.3.1.m1.3.3.3.2.2.4.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S1.T1.3.3.1.m1.3.3.3.2.2.4.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.1.cmml">​</mo><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2.4.3.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml"><mo stretchy="false" id="S1.T1.3.3.1.m1.3.3.3.2.2.4.3.2.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml">(</mo><mi id="S1.T1.3.3.1.m1.2.2.2.1.1.1"
    xref="S1.T1.3.3.1.m1.2.2.2.1.1.1.cmml">u</mi><mo stretchy="false" id="S1.T1.3.3.1.m1.3.3.3.2.2.4.3.2.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml">)</mo></mrow></mrow><mo id="S1.T1.3.3.1.m1.3.3.3.2.2.3"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.3.cmml">=</mo><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2.5.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.cmml"><mo id="S1.T1.3.3.1.m1.3.3.3.2.2.5.2.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt"
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mtr id="S1.T1.3.3.1.m1.3.3.3.2.2.2a"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.3.3.1.m1.3.3.3.2.2.2b" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1.cmml">u</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.3.3.1.m1.3.3.3.2.2.2c" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"><mtext
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.1.cmml">></mo><mn
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3.cmml">0</mn></mrow></mtd></mtr><mtr
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2d" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.3.3.1.m1.3.3.3.2.2.2e"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mrow id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.cmml"><mi id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2.cmml">ε</mi><mo lspace="0em" rspace="0em"
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.1.cmml">​</mo><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3.cmml">u</mi></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.3.3.1.m1.3.3.3.2.2.2f" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.cmml"><mrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.cmml"><mtext
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.1" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.1.cmml">≤</mo><mn
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3.cmml">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    id="S1.T1.3.3.1.m1.3.3c" xref="S1.T1.3.3.1.m1.3.3.cmml"><mtd id="S1.T1.3.3.1.m1.3.3d"
    xref="S1.T1.3.3.1.m1.3.3.cmml"><mrow id="S1.T1.3.3.1.m1.1.1.1.1.1" xref="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml"><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1a" xref="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml">(</mtext><mi
    id="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1" xref="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml">ε</mi><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1b" xref="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml"> is small
    and fixed)</mtext></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    id="S1.T1.3.3.1.m1.3b"><matrix id="S1.T1.3.3.1.m1.3.3.cmml" xref="S1.T1.3.3.1.m1.3.3"><matrixrow
    id="S1.T1.3.3.1.m1.3.3a.cmml" xref="S1.T1.3.3.1.m1.3.3"><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2"><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.4.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.4"><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.4.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.4.2">𝜎</ci><ci
    id="S1.T1.3.3.1.m1.2.2.2.1.1.1.cmml" xref="S1.T1.3.3.1.m1.2.2.2.1.1.1">𝑢</ci></apply><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.2"><csymbol
    cd="latexml" id="S1.T1.3.3.1.m1.3.3.3.2.2.5.1.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.5.2.1">cases</csymbol><matrix
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2"><matrixrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2a.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2"><ci id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.1.1">𝑢</ci><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1"><apply id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2"><ci id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2"><mtext id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.2">if </mtext></ci><ci id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.2.3">𝑢</ci></apply><cn type="integer" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3.cmml"
    xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.1.2.1.3">0</cn></apply></matrixrow><matrixrow
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2b.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2"><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1"><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.2">𝜀</ci><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.1.1.3">𝑢</ci></apply><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1"><apply
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2"><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2"><mtext
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.2">if </mtext></ci><ci
    id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.2.3">𝑢</ci></apply><cn
    type="integer" id="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3.cmml" xref="S1.T1.3.3.1.m1.3.3.3.2.2.2.2.2.1.3">0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    id="S1.T1.3.3.1.m1.3.3b.cmml" xref="S1.T1.3.3.1.m1.3.3"><ci id="S1.T1.3.3.1.m1.1.1.1.1.1c.cmml"
    xref="S1.T1.3.3.1.m1.1.1.1.1.1"><mrow id="S1.T1.3.3.1.m1.1.1.1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1"><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1a.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1">(</mtext><mi
    id="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1.1.1.m1.1.1">ε</mi><mtext
    id="S1.T1.3.3.1.m1.1.1.1.1.1b.cmml" xref="S1.T1.3.3.1.m1.1.1.1.1.1"> is small
    and fixed)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.3.3.1.m1.3c">\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ \varepsilon u&\text{if }u\leq 0\end{array}\right.\\ \text{($\varepsilon$
    is small and fixed)}\end{array}</annotation></semantics></math> | Maas et al.
    ([2013](#bib.bib205)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| parametric ReLU | <math id="S1.T1.4.4.1.m1.3" class="ltx_Math" alttext="\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&amp;\text{if
    }u>0\\ au&amp;\text{if }u\leq 0\end{array}\right.\\'
  prefs: []
  type: TYPE_NORMAL
- en: \text{($a$ is a trainable parameter)}\end{array}" display="inline"><semantics
    id="S1.T1.4.4.1.m1.3a"><mtable rowspacing="0pt" id="S1.T1.4.4.1.m1.3.3" xref="S1.T1.4.4.1.m1.3.3.cmml"><mtr
    id="S1.T1.4.4.1.m1.3.3a" xref="S1.T1.4.4.1.m1.3.3.cmml"><mtd id="S1.T1.4.4.1.m1.3.3b"
    xref="S1.T1.4.4.1.m1.3.3.cmml"><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.4" xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml"><mi id="S1.T1.4.4.1.m1.3.3.3.2.2.4.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S1.T1.4.4.1.m1.3.3.3.2.2.4.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.1.cmml">​</mo><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2.4.3.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml"><mo stretchy="false" id="S1.T1.4.4.1.m1.3.3.3.2.2.4.3.2.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml">(</mo><mi id="S1.T1.4.4.1.m1.2.2.2.1.1.1"
    xref="S1.T1.4.4.1.m1.2.2.2.1.1.1.cmml">u</mi><mo stretchy="false" id="S1.T1.4.4.1.m1.3.3.3.2.2.4.3.2.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml">)</mo></mrow></mrow><mo id="S1.T1.4.4.1.m1.3.3.3.2.2.3"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.3.cmml">=</mo><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2.5.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.cmml"><mo id="S1.T1.4.4.1.m1.3.3.3.2.2.5.2.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.1.cmml">{</mo><mtable columnspacing="5pt" rowspacing="0pt"
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mtr id="S1.T1.4.4.1.m1.3.3.3.2.2.2a"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.4.4.1.m1.3.3.3.2.2.2b" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1.cmml">u</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.4.4.1.m1.3.3.3.2.2.2c" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"><mtext
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.1.cmml">></mo><mn
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3.cmml">0</mn></mrow></mtd></mtr><mtr
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2d" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mtd id="S1.T1.4.4.1.m1.3.3.3.2.2.2e"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mrow id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.cmml"><mi id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em"
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.1.cmml">​</mo><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3.cmml">u</mi></mrow></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.4.4.1.m1.3.3.3.2.2.2f" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.cmml"><mrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.cmml"><mtext
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml">if </mtext><mo
    lspace="0em" rspace="0em" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.1" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.1.cmml">≤</mo><mn
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3.cmml">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mtd></mtr><mtr
    id="S1.T1.4.4.1.m1.3.3c" xref="S1.T1.4.4.1.m1.3.3.cmml"><mtd id="S1.T1.4.4.1.m1.3.3d"
    xref="S1.T1.4.4.1.m1.3.3.cmml"><mrow id="S1.T1.4.4.1.m1.1.1.1.1.1" xref="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml"><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1a" xref="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml">(</mtext><mi
    id="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml">a</mi><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1b" xref="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml"> is a trainable
    parameter)</mtext></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content"
    id="S1.T1.4.4.1.m1.3b"><matrix id="S1.T1.4.4.1.m1.3.3.cmml" xref="S1.T1.4.4.1.m1.3.3"><matrixrow
    id="S1.T1.4.4.1.m1.3.3a.cmml" xref="S1.T1.4.4.1.m1.3.3"><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2"><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.4.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.4"><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.4.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.4.2">𝜎</ci><ci
    id="S1.T1.4.4.1.m1.2.2.2.1.1.1.cmml" xref="S1.T1.4.4.1.m1.2.2.2.1.1.1">𝑢</ci></apply><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.2"><csymbol
    cd="latexml" id="S1.T1.4.4.1.m1.3.3.3.2.2.5.1.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.5.2.1">cases</csymbol><matrix
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2"><matrixrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2a.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2"><ci id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.1.1">𝑢</ci><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1"><apply id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2"><ci id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2"><mtext id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.2">if </mtext></ci><ci id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.2.3">𝑢</ci></apply><cn type="integer" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3.cmml"
    xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.1.2.1.3">0</cn></apply></matrixrow><matrixrow
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2b.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2"><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1"><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.2">𝑎</ci><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.1.1.3">𝑢</ci></apply><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1"><apply
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2"><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2a.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2"><mtext
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.2">if </mtext></ci><ci
    id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.2.3">𝑢</ci></apply><cn
    type="integer" id="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3.cmml" xref="S1.T1.4.4.1.m1.3.3.3.2.2.2.2.2.1.3">0</cn></apply></matrixrow></matrix></apply></apply></matrixrow><matrixrow
    id="S1.T1.4.4.1.m1.3.3b.cmml" xref="S1.T1.4.4.1.m1.3.3"><ci id="S1.T1.4.4.1.m1.1.1.1.1.1c.cmml"
    xref="S1.T1.4.4.1.m1.1.1.1.1.1"><mrow id="S1.T1.4.4.1.m1.1.1.1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1"><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1a.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1">(</mtext><mi
    id="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1.1.1.m1.1.1">a</mi><mtext
    id="S1.T1.4.4.1.m1.1.1.1.1.1b.cmml" xref="S1.T1.4.4.1.m1.1.1.1.1.1"> is a trainable
    parameter)</mtext></mrow></ci></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.4.4.1.m1.3c">\begin{array}[]{c}\sigma(u)=\left\{\begin{array}[]{cl}u&\text{if
    }u>0\\ au&\text{if }u\leq 0\end{array}\right.\\ \text{($a$ is a trainable parameter)}\end{array}</annotation></semantics></math>
    | He et al. ([2015](#bib.bib143)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| hard tanh | <math id="S1.T1.5.5.1.m1.2" class="ltx_Math" alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>1\\ u&amp;\text{if }-1\leq u\leq 1\\'
  prefs: []
  type: TYPE_NORMAL
- en: -1&amp;\text{if }u<-1\end{array}\right." display="inline"><semantics id="S1.T1.5.5.1.m1.2a"><mrow
    id="S1.T1.5.5.1.m1.2.3" xref="S1.T1.5.5.1.m1.2.3.cmml"><mrow id="S1.T1.5.5.1.m1.2.3.2"
    xref="S1.T1.5.5.1.m1.2.3.2.cmml"><mi id="S1.T1.5.5.1.m1.2.3.2.2" xref="S1.T1.5.5.1.m1.2.3.2.2.cmml">σ</mi><mo
    lspace="0em" rspace="0em" id="S1.T1.5.5.1.m1.2.3.2.1" xref="S1.T1.5.5.1.m1.2.3.2.1.cmml">​</mo><mrow
    id="S1.T1.5.5.1.m1.2.3.2.3.2" xref="S1.T1.5.5.1.m1.2.3.2.cmml"><mo stretchy="false"
    id="S1.T1.5.5.1.m1.2.3.2.3.2.1" xref="S1.T1.5.5.1.m1.2.3.2.cmml">(</mo><mi id="S1.T1.5.5.1.m1.1.1"
    xref="S1.T1.5.5.1.m1.1.1.cmml">u</mi><mo stretchy="false" id="S1.T1.5.5.1.m1.2.3.2.3.2.2"
    xref="S1.T1.5.5.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S1.T1.5.5.1.m1.2.3.1"
    xref="S1.T1.5.5.1.m1.2.3.1.cmml">=</mo><mrow id="S1.T1.5.5.1.m1.2.3.3.2" xref="S1.T1.5.5.1.m1.2.3.3.1.cmml"><mo
    id="S1.T1.5.5.1.m1.2.3.3.2.1" xref="S1.T1.5.5.1.m1.2.3.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" id="S1.T1.5.5.1.m1.2.2" xref="S1.T1.5.5.1.m1.2.2.cmml"><mtr
    id="S1.T1.5.5.1.m1.2.2a" xref="S1.T1.5.5.1.m1.2.2.cmml"><mtd id="S1.T1.5.5.1.m1.2.2b"
    xref="S1.T1.5.5.1.m1.2.2.cmml"><mn id="S1.T1.5.5.1.m1.2.2.1.1.1" xref="S1.T1.5.5.1.m1.2.2.1.1.1.cmml">1</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.5.5.1.m1.2.2c" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.1.2.1" xref="S1.T1.5.5.1.m1.2.2.1.2.1.cmml"><mrow id="S1.T1.5.5.1.m1.2.2.1.2.1.2"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.cmml"><mtext id="S1.T1.5.5.1.m1.2.2.1.2.1.2.2"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.2a.cmml">if </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.5.5.1.m1.2.2.1.2.1.2.1" xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.5.5.1.m1.2.2.1.2.1.2.3" xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.5.5.1.m1.2.2.1.2.1.1" xref="S1.T1.5.5.1.m1.2.2.1.2.1.1.cmml">></mo><mn
    id="S1.T1.5.5.1.m1.2.2.1.2.1.3" xref="S1.T1.5.5.1.m1.2.2.1.2.1.3.cmml">1</mn></mrow></mtd></mtr><mtr
    id="S1.T1.5.5.1.m1.2.2d" xref="S1.T1.5.5.1.m1.2.2.cmml"><mtd id="S1.T1.5.5.1.m1.2.2e"
    xref="S1.T1.5.5.1.m1.2.2.cmml"><mi id="S1.T1.5.5.1.m1.2.2.2.1.1" xref="S1.T1.5.5.1.m1.2.2.2.1.1.cmml">u</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.5.5.1.m1.2.2f" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.2.2.1" xref="S1.T1.5.5.1.m1.2.2.2.2.1.cmml"><mrow id="S1.T1.5.5.1.m1.2.2.2.2.1.2"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.cmml"><mtext id="S1.T1.5.5.1.m1.2.2.2.2.1.2.2"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.2a.cmml">if </mtext><mo id="S1.T1.5.5.1.m1.2.2.2.2.1.2.1"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.1.cmml">−</mo><mn id="S1.T1.5.5.1.m1.2.2.2.2.1.2.3"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.3.cmml">1</mn></mrow><mo id="S1.T1.5.5.1.m1.2.2.2.2.1.3"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.3.cmml">≤</mo><mi id="S1.T1.5.5.1.m1.2.2.2.2.1.4"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.4.cmml">u</mi><mo id="S1.T1.5.5.1.m1.2.2.2.2.1.5"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.5.cmml">≤</mo><mn id="S1.T1.5.5.1.m1.2.2.2.2.1.6"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1.6.cmml">1</mn></mrow></mtd></mtr><mtr id="S1.T1.5.5.1.m1.2.2g"
    xref="S1.T1.5.5.1.m1.2.2.cmml"><mtd id="S1.T1.5.5.1.m1.2.2h" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.3.1.1" xref="S1.T1.5.5.1.m1.2.2.3.1.1.cmml"><mo id="S1.T1.5.5.1.m1.2.2.3.1.1a"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1.cmml">−</mo><mn id="S1.T1.5.5.1.m1.2.2.3.1.1.2"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1.2.cmml">1</mn></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S1.T1.5.5.1.m1.2.2i" xref="S1.T1.5.5.1.m1.2.2.cmml"><mrow
    id="S1.T1.5.5.1.m1.2.2.3.2.1" xref="S1.T1.5.5.1.m1.2.2.3.2.1.cmml"><mrow id="S1.T1.5.5.1.m1.2.2.3.2.1.2"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.cmml"><mtext id="S1.T1.5.5.1.m1.2.2.3.2.1.2.2"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.2a.cmml">if </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.1" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.3" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.5.5.1.m1.2.2.3.2.1.1" xref="S1.T1.5.5.1.m1.2.2.3.2.1.1.cmml"><</mo><mrow
    id="S1.T1.5.5.1.m1.2.2.3.2.1.3" xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.cmml"><mo id="S1.T1.5.5.1.m1.2.2.3.2.1.3a"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.cmml">−</mo><mn id="S1.T1.5.5.1.m1.2.2.3.2.1.3.2"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.2.cmml">1</mn></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S1.T1.5.5.1.m1.2b"><apply id="S1.T1.5.5.1.m1.2.3.cmml"
    xref="S1.T1.5.5.1.m1.2.3"><apply id="S1.T1.5.5.1.m1.2.3.2.cmml" xref="S1.T1.5.5.1.m1.2.3.2"><ci
    id="S1.T1.5.5.1.m1.2.3.2.2.cmml" xref="S1.T1.5.5.1.m1.2.3.2.2">𝜎</ci><ci id="S1.T1.5.5.1.m1.1.1.cmml"
    xref="S1.T1.5.5.1.m1.1.1">𝑢</ci></apply><apply id="S1.T1.5.5.1.m1.2.3.3.1.cmml"
    xref="S1.T1.5.5.1.m1.2.3.3.2"><csymbol cd="latexml" id="S1.T1.5.5.1.m1.2.3.3.1.1.cmml"
    xref="S1.T1.5.5.1.m1.2.3.3.2.1">cases</csymbol><matrix id="S1.T1.5.5.1.m1.2.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2"><matrixrow id="S1.T1.5.5.1.m1.2.2a.cmml" xref="S1.T1.5.5.1.m1.2.2"><cn
    type="integer" id="S1.T1.5.5.1.m1.2.2.1.1.1.cmml" xref="S1.T1.5.5.1.m1.2.2.1.1.1">1</cn><apply
    id="S1.T1.5.5.1.m1.2.2.1.2.1.cmml" xref="S1.T1.5.5.1.m1.2.2.1.2.1"><apply id="S1.T1.5.5.1.m1.2.2.1.2.1.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2"><ci id="S1.T1.5.5.1.m1.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.2"><mtext id="S1.T1.5.5.1.m1.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.2">if </mtext></ci><ci id="S1.T1.5.5.1.m1.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.2.3">𝑢</ci></apply><cn type="integer" id="S1.T1.5.5.1.m1.2.2.1.2.1.3.cmml"
    xref="S1.T1.5.5.1.m1.2.2.1.2.1.3">1</cn></apply></matrixrow><matrixrow id="S1.T1.5.5.1.m1.2.2b.cmml"
    xref="S1.T1.5.5.1.m1.2.2"><ci id="S1.T1.5.5.1.m1.2.2.2.1.1.cmml" xref="S1.T1.5.5.1.m1.2.2.2.1.1">𝑢</ci><apply
    id="S1.T1.5.5.1.m1.2.2.2.2.1.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1"><apply id="S1.T1.5.5.1.m1.2.2.2.2.1b.cmml"
    xref="S1.T1.5.5.1.m1.2.2.2.2.1"><apply id="S1.T1.5.5.1.m1.2.2.2.2.1.2.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2"><ci
    id="S1.T1.5.5.1.m1.2.2.2.2.1.2.2a.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.2"><mtext
    id="S1.T1.5.5.1.m1.2.2.2.2.1.2.2.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.2">if </mtext></ci><cn
    type="integer" id="S1.T1.5.5.1.m1.2.2.2.2.1.2.3.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.2.3">1</cn></apply><ci
    id="S1.T1.5.5.1.m1.2.2.2.2.1.4.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.4">𝑢</ci></apply><apply
    id="S1.T1.5.5.1.m1.2.2.2.2.1c.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1"><cn type="integer"
    id="S1.T1.5.5.1.m1.2.2.2.2.1.6.cmml" xref="S1.T1.5.5.1.m1.2.2.2.2.1.6">1</cn></apply></apply></matrixrow><matrixrow
    id="S1.T1.5.5.1.m1.2.2c.cmml" xref="S1.T1.5.5.1.m1.2.2"><apply id="S1.T1.5.5.1.m1.2.2.3.1.1.cmml"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1"><cn type="integer" id="S1.T1.5.5.1.m1.2.2.3.1.1.2.cmml"
    xref="S1.T1.5.5.1.m1.2.2.3.1.1.2">1</cn></apply><apply id="S1.T1.5.5.1.m1.2.2.3.2.1.cmml"
    xref="S1.T1.5.5.1.m1.2.2.3.2.1"><apply id="S1.T1.5.5.1.m1.2.2.3.2.1.2.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2"><ci
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.2a.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.2"><mtext
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.2.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.2">if </mtext></ci><ci
    id="S1.T1.5.5.1.m1.2.2.3.2.1.2.3.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.2.3">𝑢</ci></apply><apply
    id="S1.T1.5.5.1.m1.2.2.3.2.1.3.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.3"><cn type="integer"
    id="S1.T1.5.5.1.m1.2.2.3.2.1.3.2.cmml" xref="S1.T1.5.5.1.m1.2.2.3.2.1.3.2">1</cn></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.5.5.1.m1.2c">\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if
    }u>1\\ u&\text{if }-1\leq u\leq 1\\ -1&\text{if }u<-1\end{array}\right.</annotation></semantics></math>
    | Collobert ([2004](#bib.bib62)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| hard sigmoid | <math id="S1.T1.6.6.1.m1.2" class="ltx_Math" alttext="\sigma(u)=\left\{\begin{array}[]{cl}1&amp;\text{if
    }u>\frac{1}{2}\\ u+\frac{1}{2}&amp;\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\'
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;\text{if }u<-\frac{1}{2}\end{array}\right." display="inline"><semantics
    id="S1.T1.6.6.1.m1.2a"><mrow id="S1.T1.6.6.1.m1.2.3" xref="S1.T1.6.6.1.m1.2.3.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.3.2" xref="S1.T1.6.6.1.m1.2.3.2.cmml"><mi id="S1.T1.6.6.1.m1.2.3.2.2"
    xref="S1.T1.6.6.1.m1.2.3.2.2.cmml">σ</mi><mo lspace="0em" rspace="0em" id="S1.T1.6.6.1.m1.2.3.2.1"
    xref="S1.T1.6.6.1.m1.2.3.2.1.cmml">​</mo><mrow id="S1.T1.6.6.1.m1.2.3.2.3.2" xref="S1.T1.6.6.1.m1.2.3.2.cmml"><mo
    stretchy="false" id="S1.T1.6.6.1.m1.2.3.2.3.2.1" xref="S1.T1.6.6.1.m1.2.3.2.cmml">(</mo><mi
    id="S1.T1.6.6.1.m1.1.1" xref="S1.T1.6.6.1.m1.1.1.cmml">u</mi><mo stretchy="false"
    id="S1.T1.6.6.1.m1.2.3.2.3.2.2" xref="S1.T1.6.6.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo
    id="S1.T1.6.6.1.m1.2.3.1" xref="S1.T1.6.6.1.m1.2.3.1.cmml">=</mo><mrow id="S1.T1.6.6.1.m1.2.3.3.2"
    xref="S1.T1.6.6.1.m1.2.3.3.1.cmml"><mo id="S1.T1.6.6.1.m1.2.3.3.2.1" xref="S1.T1.6.6.1.m1.2.3.3.1.1.cmml">{</mo><mtable
    columnspacing="5pt" rowspacing="0pt" id="S1.T1.6.6.1.m1.2.2" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtr
    id="S1.T1.6.6.1.m1.2.2a" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtd id="S1.T1.6.6.1.m1.2.2b"
    xref="S1.T1.6.6.1.m1.2.2.cmml"><mn id="S1.T1.6.6.1.m1.2.2.1.1.1" xref="S1.T1.6.6.1.m1.2.2.1.1.1.cmml">1</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.6.6.1.m1.2.2c" xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.2.1.2.1" xref="S1.T1.6.6.1.m1.2.2.1.2.1.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.1.2.1.2"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.cmml"><mtext id="S1.T1.6.6.1.m1.2.2.1.2.1.2.2"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.2a.cmml">if </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.6.6.1.m1.2.2.1.2.1.2.1" xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.6.6.1.m1.2.2.1.2.1.2.3" xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.6.6.1.m1.2.2.1.2.1.1" xref="S1.T1.6.6.1.m1.2.2.1.2.1.1.cmml">></mo><mfrac
    id="S1.T1.6.6.1.m1.2.2.1.2.1.3" xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.cmml"><mn id="S1.T1.6.6.1.m1.2.2.1.2.1.3.2"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.1.2.1.3.3"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.3.cmml">2</mn></mfrac></mrow></mtd></mtr><mtr
    id="S1.T1.6.6.1.m1.2.2d" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtd id="S1.T1.6.6.1.m1.2.2e"
    xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.2.1.1" xref="S1.T1.6.6.1.m1.2.2.2.1.1.cmml"><mi
    id="S1.T1.6.6.1.m1.2.2.2.1.1.2" xref="S1.T1.6.6.1.m1.2.2.2.1.1.2.cmml">u</mi><mo
    id="S1.T1.6.6.1.m1.2.2.2.1.1.1" xref="S1.T1.6.6.1.m1.2.2.2.1.1.1.cmml">+</mo><mfrac
    id="S1.T1.6.6.1.m1.2.2.2.1.1.3" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.cmml"><mn id="S1.T1.6.6.1.m1.2.2.2.1.1.3.2"
    xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.2.1.1.3.3"
    xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.3.cmml">2</mn></mfrac></mrow></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S1.T1.6.6.1.m1.2.2f" xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.2.2.2.1" xref="S1.T1.6.6.1.m1.2.2.2.2.1.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.2.2.1.2"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.cmml"><mtext id="S1.T1.6.6.1.m1.2.2.2.2.1.2.2"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.2a.cmml">if </mtext><mo id="S1.T1.6.6.1.m1.2.2.2.2.1.2.1"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.1.cmml">−</mo><mfrac id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.cmml"><mn id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3.cmml">2</mn></mfrac></mrow><mo id="S1.T1.6.6.1.m1.2.2.2.2.1.3"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.3.cmml">≤</mo><mi id="S1.T1.6.6.1.m1.2.2.2.2.1.4"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.4.cmml">u</mi><mo id="S1.T1.6.6.1.m1.2.2.2.2.1.5"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.5.cmml">≤</mo><mfrac id="S1.T1.6.6.1.m1.2.2.2.2.1.6"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.cmml"><mn id="S1.T1.6.6.1.m1.2.2.2.2.1.6.2" xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.2.cmml">1</mn><mn
    id="S1.T1.6.6.1.m1.2.2.2.2.1.6.3" xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.3.cmml">2</mn></mfrac></mrow></mtd></mtr><mtr
    id="S1.T1.6.6.1.m1.2.2g" xref="S1.T1.6.6.1.m1.2.2.cmml"><mtd id="S1.T1.6.6.1.m1.2.2h"
    xref="S1.T1.6.6.1.m1.2.2.cmml"><mn id="S1.T1.6.6.1.m1.2.2.3.1.1" xref="S1.T1.6.6.1.m1.2.2.3.1.1.cmml">0</mn></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S1.T1.6.6.1.m1.2.2i" xref="S1.T1.6.6.1.m1.2.2.cmml"><mrow
    id="S1.T1.6.6.1.m1.2.2.3.2.1" xref="S1.T1.6.6.1.m1.2.2.3.2.1.cmml"><mrow id="S1.T1.6.6.1.m1.2.2.3.2.1.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.cmml"><mtext id="S1.T1.6.6.1.m1.2.2.3.2.1.2.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.2a.cmml">if </mtext><mo lspace="0em" rspace="0em"
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.1" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.1.cmml">​</mo><mi
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.3" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.3.cmml">u</mi></mrow><mo
    id="S1.T1.6.6.1.m1.2.2.3.2.1.1" xref="S1.T1.6.6.1.m1.2.2.3.2.1.1.cmml"><</mo><mrow
    id="S1.T1.6.6.1.m1.2.2.3.2.1.3" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.cmml"><mo id="S1.T1.6.6.1.m1.2.2.3.2.1.3a"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.cmml">−</mo><mfrac id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.cmml"><mn id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2.cmml">1</mn><mn id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3.cmml">2</mn></mfrac></mrow></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S1.T1.6.6.1.m1.2b"><apply id="S1.T1.6.6.1.m1.2.3.cmml"
    xref="S1.T1.6.6.1.m1.2.3"><apply id="S1.T1.6.6.1.m1.2.3.2.cmml" xref="S1.T1.6.6.1.m1.2.3.2"><ci
    id="S1.T1.6.6.1.m1.2.3.2.2.cmml" xref="S1.T1.6.6.1.m1.2.3.2.2">𝜎</ci><ci id="S1.T1.6.6.1.m1.1.1.cmml"
    xref="S1.T1.6.6.1.m1.1.1">𝑢</ci></apply><apply id="S1.T1.6.6.1.m1.2.3.3.1.cmml"
    xref="S1.T1.6.6.1.m1.2.3.3.2"><csymbol cd="latexml" id="S1.T1.6.6.1.m1.2.3.3.1.1.cmml"
    xref="S1.T1.6.6.1.m1.2.3.3.2.1">cases</csymbol><matrix id="S1.T1.6.6.1.m1.2.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2"><matrixrow id="S1.T1.6.6.1.m1.2.2a.cmml" xref="S1.T1.6.6.1.m1.2.2"><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.1.1.1.cmml" xref="S1.T1.6.6.1.m1.2.2.1.1.1">1</cn><apply
    id="S1.T1.6.6.1.m1.2.2.1.2.1.cmml" xref="S1.T1.6.6.1.m1.2.2.1.2.1"><apply id="S1.T1.6.6.1.m1.2.2.1.2.1.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2"><ci id="S1.T1.6.6.1.m1.2.2.1.2.1.2.2a.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.2"><mtext id="S1.T1.6.6.1.m1.2.2.1.2.1.2.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.2">if </mtext></ci><ci id="S1.T1.6.6.1.m1.2.2.1.2.1.2.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.2.3">𝑢</ci></apply><apply id="S1.T1.6.6.1.m1.2.2.1.2.1.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3"><cn type="integer" id="S1.T1.6.6.1.m1.2.2.1.2.1.3.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.2">1</cn><cn type="integer" id="S1.T1.6.6.1.m1.2.2.1.2.1.3.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.1.2.1.3.3">2</cn></apply></apply></matrixrow><matrixrow
    id="S1.T1.6.6.1.m1.2.2b.cmml" xref="S1.T1.6.6.1.m1.2.2"><apply id="S1.T1.6.6.1.m1.2.2.2.1.1.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.1.1"><ci id="S1.T1.6.6.1.m1.2.2.2.1.1.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.2">𝑢</ci><apply
    id="S1.T1.6.6.1.m1.2.2.2.1.1.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3"><cn type="integer"
    id="S1.T1.6.6.1.m1.2.2.2.1.1.3.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.2">1</cn><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.2.1.1.3.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.1.1.3.3">2</cn></apply></apply><apply
    id="S1.T1.6.6.1.m1.2.2.2.2.1.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1"><apply id="S1.T1.6.6.1.m1.2.2.2.2.1b.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1"><apply id="S1.T1.6.6.1.m1.2.2.2.2.1.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2"><ci
    id="S1.T1.6.6.1.m1.2.2.2.2.1.2.2a.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.2"><mtext
    id="S1.T1.6.6.1.m1.2.2.2.2.1.2.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.2">if </mtext></ci><apply
    id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3"><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.2">1</cn><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.2.3.3">2</cn></apply></apply><ci
    id="S1.T1.6.6.1.m1.2.2.2.2.1.4.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1.4">𝑢</ci></apply><apply
    id="S1.T1.6.6.1.m1.2.2.2.2.1c.cmml" xref="S1.T1.6.6.1.m1.2.2.2.2.1"><apply id="S1.T1.6.6.1.m1.2.2.2.2.1.6.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6"><cn type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.6.2.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.2">1</cn><cn type="integer" id="S1.T1.6.6.1.m1.2.2.2.2.1.6.3.cmml"
    xref="S1.T1.6.6.1.m1.2.2.2.2.1.6.3">2</cn></apply></apply></apply></matrixrow><matrixrow
    id="S1.T1.6.6.1.m1.2.2c.cmml" xref="S1.T1.6.6.1.m1.2.2"><cn type="integer" id="S1.T1.6.6.1.m1.2.2.3.1.1.cmml"
    xref="S1.T1.6.6.1.m1.2.2.3.1.1">0</cn><apply id="S1.T1.6.6.1.m1.2.2.3.2.1.cmml"
    xref="S1.T1.6.6.1.m1.2.2.3.2.1"><apply id="S1.T1.6.6.1.m1.2.2.3.2.1.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2"><ci
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.2a.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.2"><mtext
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.2">if </mtext></ci><ci
    id="S1.T1.6.6.1.m1.2.2.3.2.1.2.3.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.2.3">𝑢</ci></apply><apply
    id="S1.T1.6.6.1.m1.2.2.3.2.1.3.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3"><apply
    id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2"><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.2">1</cn><cn
    type="integer" id="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3.cmml" xref="S1.T1.6.6.1.m1.2.2.3.2.1.3.2.3">2</cn></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S1.T1.6.6.1.m1.2c">\sigma(u)=\left\{\begin{array}[]{cl}1&\text{if
    }u>\frac{1}{2}\\ u+\frac{1}{2}&\text{if }-\frac{1}{2}\leq u\leq\frac{1}{2}\\ 0&\text{if
    }u<-\frac{1}{2}\end{array}\right.</annotation></semantics></math> | Courbariaux
    et al. ([2015](#bib.bib63)) |
  prefs: []
  type: TYPE_NORMAL
- en: '| max pooling | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{0,u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is the output of another neuron)}\end{array}$ | Weng et al.
    ([1992](#bib.bib329)) |'
  prefs: []
  type: TYPE_TB
- en: '| maxout | $\begin{array}[]{c}\sigma(u_{1},\ldots,u_{k})=\max\{u_{1},\ldots,u_{k}\}\\
    \text{(each $u_{i}$ is an affine function)}\end{array}$ | Goodfellow et al. ([2013](#bib.bib123))
    |'
  prefs: []
  type: TYPE_TB
- en: 1.4 When deep learning meets polyhedral theory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is commonly accepted in machine learning that a simpler model is preferred
    if it trains as well as a more complex one, since a simpler model is less likely
    to overfit. Conveniently, the successful return of neural networks to relatively
    simpler activation functions prepared the ground for deep learning to meet polyhedral
    theory. In other words, we are now able to analyze and leverage neural networks
    through the same lenses and tools that have been successfully used for linear
    and discrete optimization in operations research for many decades. We explain
    this connection in more detail and some lines of research that it has opened up
    in Section [2](#S2 "2 The Polyhedral Perspective ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Scope of this survey and related work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The interplay between mathematical optimization and machine learning has also
    been discussed by other recent surveys. Bengio et al. ([2021](#bib.bib19)) review
    the use of machine learning in mathematical optimization, whereas Gambella et al.
    ([2021](#bib.bib115)) formulate mathematical optimization problems with the main
    focus of obtaining machine learning models, such as by training neural networks.
    A similar scope has been previously surveyed by Curtis and Scheinberg ([2017](#bib.bib70))
    and Bottou et al. ([2018](#bib.bib38)). Our survey complements those by focusing
    exclusively on neural networks while outlining how linear optimization can be
    used more broadly in that context, from network training and verification to model
    embedding and compression, as well as refined through formulation strengthening.
    In addition, we illustrate how polyhedral theory can ground the use of such linear
    formulations and also provide a more nuanced understanding of the discriminative
    ability of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The presentation in this survey is centered on *feedforward rectifier networks*.
    These are very commonly used networks with only ReLU activations and for which
    most polyhedral results and applications of linear optimization are known. The
    focus on a single type of neural network is intended to help the reader capture
    the intuition behind different developments and understand the nuances involved.
    Despite our focus on *fully-connected* models, which are those in which every
    unit is connected to all units in the subsequent layer, there are many variants
    of interest with fewer or different types of connection that can be interpreted
    as a special case of fully-connected models. For example, the units of Convolutional
    Neural Networks (CNNs or ConvNets) (Fukushima, [1980](#bib.bib111)) have local
    connectivity: only a subset of adjacent units defines the output of each unit
    in the next layer, and the same parameters are used to define the output of different
    units. In fact, multiple *filters* of parameters can be applied to a set of adjacent
    units through the output of different units in the next layer. CNNs are often
    applied to identify and aggregate the same local features in different parts of
    a picture, and we can interpret them as a special case of feedforward networks.
    Another common variant, the Residual Network (ResNet) (He et al., [2016](#bib.bib144)),
    includes *skip connections* that directly connect units in nonadjacent layers.
    Those connections can be emulated by adding units passing their outputs through
    the intermediary layers. Hence, many of the results and applications discussed
    along the survey are relevant to other variants (e.g., LTU and maxout activations,
    or those other connectivity patterns), and we also provide references to more
    specific results and applications involving them.'
  prefs: []
  type: TYPE_NORMAL
- en: We also discuss the extent to which other variants remain relevant or can be
    analyzed through the same lenses. For example, *feedback connections* in *recurrent
    networks* (Little, [1974](#bib.bib195), Hopfield, [1982](#bib.bib152)) allow the
    output of a unit to be used as an input of units in previous layers. Recurrent
    networks such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, [1997](#bib.bib151))
    produce outputs that depend on their internal state, and they may consequently
    process sequential inputs with arbitrary length. While feedback connections may
    not be emulated with a feeforward network, we discuss in the following paragraph
    how recurrent networks have been replaced with great success by attention mechanisms,
    which are implemented with feedforward networks. In the realm of variants that
    remain relevant, it is very common to apply a different type of activation to
    the output layer of the network, such as the layer-wise softmax function $\sigma:\mathbb{R}^{n_{L}}\rightarrow\mathbb{R}^{n_{L}}$
    in which $\sigma(u)_{i}=e^{u_{i}}/\sum_{j=1}^{n_{L}}e^{u_{j}}~{}\forall i\in\{1,\ldots,n_{L}\}$
    (Bridle, [1990](#bib.bib39)), which is used to normalize a multidimensional output
    as a probability distribution. While softmax is not piecewise linear, we describe
    how its output can also be analyzed from a polyhedral perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Other uses of deep learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Deep learning is also being used in machine learning beyond the realm of supervised
    learning. In *unsupervised learning*, the focus is on drawing inferences from
    unlabeled datasets. For example, Generative Adversarial Networks (GANs) (Goodfellow
    et al., [2014](#bib.bib126)) have been used to generate realistic images using
    a pair of neural networks. One of these networks is a *discriminator* trained
    to identify elements from a dataset and the other is a *generator* aiming to mislead
    the discriminator with synthetic inputs that could be classified as belonging
    to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In *reinforcement learning*, the focus is on modeling agents that can interact
    with their environment through actions and associated rewards. Examples of such
    applications include neural networks designed for the navigation of self-driving
    vehicles (Gao et al., [2020](#bib.bib116)) and for playing Atari games (Mnih et al.,
    [2015](#bib.bib222)), more contemporary electronic games such as Dota 2 (OpenAI
    et al., [2019](#bib.bib237)) and StarCraft II (Vinyals et al., [2017](#bib.bib321)),
    and the game of Go (Silver et al., [2017](#bib.bib288)) at levels that are either
    better or at least comparable to human players.
  prefs: []
  type: TYPE_NORMAL
- en: A more recent and popular example are generative transformers (Radford et al.,
    [2018](#bib.bib252)), such as DALL·E 2 (Ramesh et al., [2022](#bib.bib257)) producing
    realistic images from text prompts in mid-2022 and ChatGPT (OpenAI, [2022](#bib.bib236))
    producing realistic dialogues with users in early 2023, the latter belonging to
    the fast growing family of large language models. They are based on replacing
    architectures based on feedback connections, such as LSTM, with the attention
    mechanisms aimed at scoring the relevance of past states (Bahdanau et al., [2015](#bib.bib10)),
    which is the foundation of the transformer architecture (Vaswani et al., [2017](#bib.bib315)).
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For a historical perspective on neural networks, we recommend Schmidhuber ([2015](#bib.bib273)).
    For a recent and broad introduction to the fundamentals of deep learning, we recommend Zhang
    et al. ([2023](#bib.bib353)). For other forms of measuring model complexity in
    neural networks, we refer to Hu et al. ([2021](#bib.bib156)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 The Polyhedral Perspective
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A feedforward rectifier network models a piecewise linear function (Arora et al.,
    [2018](#bib.bib8)) in which every such piece is a polyhedron (Raghu et al., [2017](#bib.bib253)),
    and represents a special case among neural networks modeling piecewise polynomials
    (Balestriero and Baraniuk, [2018](#bib.bib15)). Therefore, training a rectifier
    network is equivalent to performing a piecewise linear regression, and we can
    potentially interpret such neural networks in terms of what happens in each piece
    of the function that they model. However, we are only beginning to answer some
    of the questions entailed by such a remark. In this survey, we discuss how insights
    on this subject may help us answer the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which piecewise linear functions can or cannot be obtained from training a neural
    network given its architecture?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which neural networks are more susceptible to adversarial exploitation?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we integrate the model learned by a neural network into a broader decision-making
    problem for which we want to find an optimal solution?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it possible to obtain a smaller neural network that models exactly the same
    function as another trained neural network?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we exploit the polyhedral geometry present in neural networks in the training
    phase?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we efficiently incorporate extra structure when training neural network,
    such as linear constraints over the weights?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first question complements the universal approximation results for neural
    networks. Namely, there is a limit to what functions can be well approximated
    when limited computational resources are translated into constraints on the depth
    and width of the layers of neural networks that can be used in practice. The functions
    that can be modeled depend on the particular choice of hyperparameters subject
    to the computational resources available, and in the long run that may also lead
    to a more principled approach for the choice of hyperparameters than the current
    approaches of neural architecture search. In Section [3](#S3 "3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"),
    we analyze how a rectifier network partitions the input space into pieces in which
    it behaves linearly, which we denote as *linear regions*. We discuss the geometry
    of linear regions, the effect of parameters and hyperparameters on the number
    of linear regions of a neural network, and the extent to which such number of
    linear regions relates to the accuracy of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second question relies on formal verification methods to evaluate the robustness
    of neural networks, which can be approached with mathematical optimization formulations
    that are also relevant for the third and fourth questions. Such formulations are
    convenient since a direct inspection of every piece of the function modeled by
    a neural network is prohibitive given how quickly their number scale with the
    size of the network. The linear behavior of the network for every choice of active
    and inactive units implies that we can use linear formulations with binary variables
    corresponding to the activation of units to model trained neural networks using
    MILP. Therefore, we are able to solve a variety of optimization problems over
    a trained neural network, such as the neural network verification problem, identifying
    the range of outputs for each ReLU of the network, and modeling a trained neural
    network as part of a larger decision-making problem. In Section [4](#S4 "4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey"), we discuss how to formulate optimization problems over a trained neural
    network, the applications of such formulations, and the progress toward obtaining
    stronger formulations that scale more easily with the network size.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fifth and sixth questions involve the training procedure of a DNN, where
    linear programming tools have been applied to partially answer them. In Section [5](#S5
    "5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), we overview these developments. In terms of the
    fifth question —exploiting polyhedrality in training neural networks— we describe
    algorithms that use the polyhedral geometry induced by activation sets to solve
    training problems. We also cover a recently proposed polyhedral construction that
    can approximately encode multiple training problems at once, showing a strong
    relationship across training problems that arise from different datasets, for
    a fixed architecture. Additionally, we review some recent uses of mixed-integer
    linear programming in the training phase as an alternative to SGD when the weights
    are required to be integer. Regarding the sixth question —the incorporation of
    extra structure when training— we review multiple approaches that have included
    techniques related to linear programming within SGD to impose a desirable structure
    when training, or to find better step-lengths in the execution of SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 The Linear Regions of a Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every piece of the piecewise linear function modeled by a neural network is
    a linear region, and —without loss of generality— we can think of it as a polyhedron.
    In this section, we define a linear region, exemplify how they can be so numerous,
    and what may affect their count in a neural network. We also discuss the practical
    implications of such insights, as well as other related forms of analyzing the
    ability of a neural network to represent expressive models.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A linear region corresponds to the set of points from the input space that activates
    the same units along the neural network, and hence can be characterized by the
    set ${\mathbb{S}}^{l}$ of units that are active in each layer $l\in{\mathbb{L}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Since a neural network behaves uniformly over a linear region, the latter is
    the smallest finite scale in which we can analyze its behavior. If we restrict
    the domain of a neural network to a linear region ${\mathbb{I}}\subseteq\mathbb{R}^{n_{0}}$,
    then the neural network behaves as an affine transformation ${\bm{y}}_{\mathbb{I}}:{\mathbb{I}}\rightarrow\mathbb{R}^{n_{L}}$
    of the form ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$ with a
    matrix ${\bm{T}}\in\mathbb{R}^{n_{L}\times n_{0}}$ and a vector ${\bm{t}}\in\mathbb{R}^{n_{L}}$
    that are directly defined by the network parameters and the set of neurons that
    are activated by any input ${\bm{x}}\in{\mathbb{I}}$. For a small perturbation $\varepsilon$
    to some input $\overline{{\bm{x}}}\in{\mathbb{I}}$ such that $\overline{{\bm{x}}}+\varepsilon\in{\mathbb{I}}$,
    the network output for $\bar{x}+\varepsilon$ is given by ${\bm{y}}_{\mathbb{I}}(\overline{{\bm{x}}}+\varepsilon)$.
    While it is possible that two adjacent regions defined in such way correspond
    to the same affine transformation, thinking of each linear region as having a
    distinct signature of active units makes it easier to analyze them.
  prefs: []
  type: TYPE_NORMAL
- en: The number of linear regions defined by a neural network is one form with which
    we can measure the complexity of the models that it can represent (Bengio, [2009](#bib.bib18)).
    Hence, if a more complex model is desired, we may want to design a neural network
    that can potentially define more linear regions. On the one hand, the number of
    linear regions may grow exponentially on the depth of a neural network. On the
    other hand, such a number depends on the interplay between network parameters
    and hyperparameters. As we consider how the inputs from adjacent linear regions
    are evaluated, the change to the affine transformation can be characterized in
    algebraic and geometric terms. Understanding such changes may help us grasp how
    a neural network is capable of telling its inputs apart, including what are the
    sources of the complexity of the model.
  prefs: []
  type: TYPE_NORMAL
- en: For neural networks in which the activation function is not piecewise linear,
    Bianchini and Scarselli ([2014](#bib.bib29)) have used more elaborate topological
    measures to compare the expressiveness of shallow and deep neural networks. Hu
    et al. ([2020b](#bib.bib155)) followed a closer approach by producing a linear
    approximation neural network in which the number of linear regions can be counted.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 The combinatorial aspect of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most striking aspects about analyzing a neural network in terms of
    its linear regions is how quickly such number grows. Early work on this topic
    by Pascanu et al. ([2014](#bib.bib243)) and Montúfar et al. ([2014](#bib.bib224))
    have drawn two important observations. First, that it is possible to construct
    simple deep neural networks with a number of linear regions that grows exponentially
    in the depth. Second, that the number of linear regions can be exponential in
    the number of neurons alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first observation comes from analyzing the role of ReLUs in a very simple
    setting. Namely, that of a neural network in which we regard every layer as having
    a single input in the $[0,1]$ domain, which is produced by combining the outputs
    of the units from the preceding layer, as illustrated by Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Example 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a neural network with input $x$ from the domain $[0,1]$ and layers
    having 4 neurons with ReLU activation. For the first layer, assume that the output
    of the neurons are given by the following functions: $f_{1}(x)=\max\{4x,0\}$,
    $f_{2}(x)=\max\{8x-2,0\}$, $f_{3}(x)=\max\{6.5x-3.25,0\}$, and $f_{4}(x)=\max\{12.5x-11.25,0\}$.
    In other words, ${\bm{h}}^{1}_{i}=f_{i}(x)~{}\forall i\in\{1,2,3,4\}$. For the
    subsequent layers, assume that the outputs coming from the previous layer are
    combined through the function $F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$, which
    substitutes $x$ as the input to the next layer; upon which the same set of functions
    $\{f_{i}(x)\}_{i=1}^{4}$ defines the output of the next layer. In other words,
    ${\bm{h}}^{l}_{i}=f_{i}(F({\bm{h}}^{l-1}))=f_{i}({h}_{1}^{l-1}-{h}_{2}^{l-1}+{h}_{3}^{l-1}-{h}_{4}^{l-1})~{}\forall
    i\in\{1,2,3,4\},l\in{\mathbb{L}}\setminus\{1\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: When the output of the units in the first layer is combined as $F(x)$, we obtain
    a zigzagging function with 4 slopes in the $[0,1]$ domain, each of which defining
    a bijection between segments of the input —namely, $[0,0.25]$, $[0.25,0.5]$, $[0.5,0.9]$,
    and $[0.9,1.0]$— and the image $[0,1]$. The effect of repeating such structure
    in the second layer is that of composing $F(x)$ with itself, with 4 slopes being
    produced within each of those 4 initial segments. Hence, the number of slopes
    —and therefore of linear regions— in the output of such a neural network with
    $L$ layers of activation functions is $4^{L}$, which implies an exponential growth
    on depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network structure and the parameters of the neurons in the first two layers
    are illustrated in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 The combinatorial aspect
    of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"); the set of functions $\{f_{i}(x)\}_{i=1}^{4}$
    and the combined outputs of the first two layers —$F(x)$ and $F(F(x))$— are illustrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.1 The combinatorial aspect of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Example [1](#Thmexample1 "Example 1 ‣ 3.1 The combinatorial aspect of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), every neuron changes the slope of the resulting
    function once it becomes active, in which we purposely alternate between positive
    and negative slopes once the function reaches either 0 or 1, respectively. By
    selecting the network parameters accordingly, Montúfar et al. ([2014](#bib.bib224))
    were the first to show that a layer with $n$ ReLUs can be used to create a zigzagging
    function with $n$ slopes on the $[0,1]$ domain, with the image along every slope
    also corresponding to the interval $[0,1]$. Consequently, stacking $L$ of such
    layers results in a neural network with $n^{L}$ linear regions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0bac8f009269c114fb6255ef917600fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Mapping from the input $x\in[0,1]$ to the intermediary output ${\bm{h}}^{2}\in[0,1]^{4}$
    through the first two layers of a neural network in which the number of linear
    regions growths exponentially on the depth, as described in Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").
    The parameters of subsequent layers are the same as those in the second layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/285adf5284d089caf1d99c5051b857ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Set of activation functions $\{f_{i}(x)\}_{i=1}^{4}$ of the units
    in the first layer and combined outputs of the first two layers —$F(x)=f_{1}(x)-f_{2}(x)+f_{3}(x)-f_{4}(x)$
    for the first and $F(F(x))$ for the second— of a neural network in which the number
    of linear regions grows exponentially on the depth, as described in Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second observation —that the number of linear regions can grow exponentially
    in the number of neurons alone— comes from the interplay between the parts of
    the input space in which each the units are active, especially in higher-dimensional
    spaces. This is based on some geometric observations that we discuss in Section [3.3](#S3.SS3
    "3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). Even for a *shallow*
    network —i.e., the number of layers being $L=1$— such a number of linear regions
    may approach $2^{n}$, which corresponds to every possible activation set ${\mathbb{S}}\subseteq\{1,\ldots,n\}$
    defining a nonempty linear region. However, as we discuss later, that is not always
    the case due to architectural choices such as the number of layers and their width.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 The algebra of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the activation sets $\{{\mathbb{S}}^{l}\}_{l\in{\mathbb{L}}}$ denoting
    which neurons are active for each layer of the neural network, we can explicitly
    describe the affine transformation ${\bm{y}}_{\mathbb{I}}({\bm{x}})={\bm{T}}{\bm{x}}+{\bm{t}}$
    associated with the corresponding linear region ${\mathbb{I}}$. For every activation
    set ${\mathbb{S}}^{l}$, layer $l$ defines an affine transformation of the form
    $\Omega^{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$, where $\Omega^{{\mathbb{S}}^{l}}$
    is a diagonal $n_{l}\times n_{l}$ matrix in which $\Omega^{{\mathbb{S}}^{l}}_{ii}=1$
    if $i\in{\mathbb{S}}^{l}$ and $\Omega^{{\mathbb{S}}^{l}}_{ii}=0$ otherwise. Hence,
    the matrix ${\bm{T}}$ and vector ${\bm{t}}$ are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{T}}=\prod_{l=1}^{L}\Omega^{{\mathbb{S}}^{l}}{\bm{W}}^{l},$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ${\bm{t}}=\sum_{l_{1}=1}^{L}\left(\prod_{l_{2}=l_{1}+1}^{L}\Omega^{{\mathbb{S}}^{l_{2}}}{\bm{W}}^{l_{2}}\right)\Omega^{{\mathbb{S}}^{l_{1}}}{\bm{b}}^{l_{1}}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: On a side note, Takai et al. ([2021](#bib.bib302)) proposed a related metric
    for networks modeling piecewise linear functions by counting the number of distinct
    functions among linear regions upon equivalence through isometric affine transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each linear region is associated with a polyhedron, and we can describe the
    union of polyhedra $\mathcal{D}$ on the space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$
    that covers the entire input space $x$ of the neural network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S3.Ex3.m1.18" class="ltx_Math" alttext="\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&amp;\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&amp;\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq 0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\'
  prefs: []
  type: TYPE_NORMAL
- en: h_{i}^{l}=0&amp;\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right)."
    display="block"><semantics id="S3.Ex3.m1.18a"><mrow id="S3.Ex3.m1.18.18.1" xref="S3.Ex3.m1.18.18.1.1.cmml"><mrow
    id="S3.Ex3.m1.18.18.1.1" xref="S3.Ex3.m1.18.18.1.1.cmml"><mi class="ltx_font_mathcaligraphic"
    id="S3.Ex3.m1.18.18.1.1.2" xref="S3.Ex3.m1.18.18.1.1.2.cmml">𝒟</mi><mo rspace="0.111em"
    id="S3.Ex3.m1.18.18.1.1.1" xref="S3.Ex3.m1.18.18.1.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.18.18.1.1.3"
    xref="S3.Ex3.m1.18.18.1.1.3.cmml"><munder id="S3.Ex3.m1.18.18.1.1.3.1" xref="S3.Ex3.m1.18.18.1.1.3.1.cmml"><mo
    movablelimits="false" rspace="0em" id="S3.Ex3.m1.18.18.1.1.3.1.2" xref="S3.Ex3.m1.18.18.1.1.3.1.2.cmml">⋁</mo><mrow
    id="S3.Ex3.m1.9.9.9" xref="S3.Ex3.m1.9.9.9.cmml"><mrow id="S3.Ex3.m1.7.7.7.7.2"
    xref="S3.Ex3.m1.7.7.7.7.3.cmml"><mo stretchy="false" id="S3.Ex3.m1.7.7.7.7.2.3"
    xref="S3.Ex3.m1.7.7.7.7.3.cmml">(</mo><msup id="S3.Ex3.m1.6.6.6.6.1.1" xref="S3.Ex3.m1.6.6.6.6.1.1.cmml"><mi
    id="S3.Ex3.m1.6.6.6.6.1.1.2" xref="S3.Ex3.m1.6.6.6.6.1.1.2.cmml">𝕊</mi><mn id="S3.Ex3.m1.6.6.6.6.1.1.3"
    xref="S3.Ex3.m1.6.6.6.6.1.1.3.cmml">1</mn></msup><mo id="S3.Ex3.m1.7.7.7.7.2.4"
    xref="S3.Ex3.m1.7.7.7.7.3.cmml">,</mo><mi mathvariant="normal" id="S3.Ex3.m1.1.1.1.1"
    xref="S3.Ex3.m1.1.1.1.1.cmml">…</mi><mo id="S3.Ex3.m1.7.7.7.7.2.5" xref="S3.Ex3.m1.7.7.7.7.3.cmml">,</mo><msup
    id="S3.Ex3.m1.7.7.7.7.2.2" xref="S3.Ex3.m1.7.7.7.7.2.2.cmml"><mi id="S3.Ex3.m1.7.7.7.7.2.2.2"
    xref="S3.Ex3.m1.7.7.7.7.2.2.2.cmml">𝕊</mi><mi id="S3.Ex3.m1.7.7.7.7.2.2.3" xref="S3.Ex3.m1.7.7.7.7.2.2.3.cmml">L</mi></msup><mo
    stretchy="false" id="S3.Ex3.m1.7.7.7.7.2.6" xref="S3.Ex3.m1.7.7.7.7.3.cmml">)</mo></mrow><mo
    id="S3.Ex3.m1.9.9.9.10" xref="S3.Ex3.m1.9.9.9.10.cmml">⊆</mo><mrow id="S3.Ex3.m1.9.9.9.9"
    xref="S3.Ex3.m1.9.9.9.9.cmml"><mrow id="S3.Ex3.m1.8.8.8.8.1.1" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml"><mo
    stretchy="false" id="S3.Ex3.m1.8.8.8.8.1.1.2" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">{</mo><mn
    id="S3.Ex3.m1.2.2.2.2" xref="S3.Ex3.m1.2.2.2.2.cmml">1</mn><mo id="S3.Ex3.m1.8.8.8.8.1.1.3"
    xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">,</mo><mi mathvariant="normal" id="S3.Ex3.m1.3.3.3.3"
    xref="S3.Ex3.m1.3.3.3.3.cmml">…</mi><mo id="S3.Ex3.m1.8.8.8.8.1.1.4" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">,</mo><msub
    id="S3.Ex3.m1.8.8.8.8.1.1.1" xref="S3.Ex3.m1.8.8.8.8.1.1.1.cmml"><mi id="S3.Ex3.m1.8.8.8.8.1.1.1.2"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.2.cmml">n</mi><mn id="S3.Ex3.m1.8.8.8.8.1.1.1.3"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.3.cmml">1</mn></msub><mo rspace="0.055em" stretchy="false"
    id="S3.Ex3.m1.8.8.8.8.1.1.5" xref="S3.Ex3.m1.8.8.8.8.1.2.cmml">}</mo></mrow><mo
    rspace="0.222em" id="S3.Ex3.m1.9.9.9.9.3" xref="S3.Ex3.m1.9.9.9.9.3.cmml">×</mo><mi
    mathvariant="normal" id="S3.Ex3.m1.9.9.9.9.4" xref="S3.Ex3.m1.9.9.9.9.4.cmml">…</mi><mo
    lspace="0.222em" rspace="0.222em" id="S3.Ex3.m1.9.9.9.9.3a" xref="S3.Ex3.m1.9.9.9.9.3.cmml">×</mo><mrow
    id="S3.Ex3.m1.9.9.9.9.2.1" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml"><mo stretchy="false"
    id="S3.Ex3.m1.9.9.9.9.2.1.2" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">{</mo><mn id="S3.Ex3.m1.4.4.4.4"
    xref="S3.Ex3.m1.4.4.4.4.cmml">1</mn><mo id="S3.Ex3.m1.9.9.9.9.2.1.3" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">,</mo><mi
    mathvariant="normal" id="S3.Ex3.m1.5.5.5.5" xref="S3.Ex3.m1.5.5.5.5.cmml">…</mi><mo
    id="S3.Ex3.m1.9.9.9.9.2.1.4" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">,</mo><msub id="S3.Ex3.m1.9.9.9.9.2.1.1"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1.cmml"><mi id="S3.Ex3.m1.9.9.9.9.2.1.1.2" xref="S3.Ex3.m1.9.9.9.9.2.1.1.2.cmml">n</mi><mi
    id="S3.Ex3.m1.9.9.9.9.2.1.1.3" xref="S3.Ex3.m1.9.9.9.9.2.1.1.3.cmml">L</mi></msub><mo
    stretchy="false" id="S3.Ex3.m1.9.9.9.9.2.1.5" xref="S3.Ex3.m1.9.9.9.9.2.2.cmml">}</mo></mrow></mrow></mrow></munder><mrow
    id="S3.Ex3.m1.18.18.1.1.3.2.2" xref="S3.Ex3.m1.17.17.cmml"><mo id="S3.Ex3.m1.18.18.1.1.3.2.2.1"
    xref="S3.Ex3.m1.17.17.cmml">(</mo><mtable columnspacing="5pt" displaystyle="true"
    rowspacing="0pt" id="S3.Ex3.m1.17.17" xref="S3.Ex3.m1.17.17.cmml"><mtr id="S3.Ex3.m1.17.17a"
    xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17b" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.11.11.2.3.1" xref="S3.Ex3.m1.11.11.2.3.1.cmml"><mrow id="S3.Ex3.m1.11.11.2.3.1.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.cmml"><mrow id="S3.Ex3.m1.11.11.2.3.1.2.2" xref="S3.Ex3.m1.11.11.2.3.1.2.2.cmml"><msubsup
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2.cmml">𝒘</mi><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3.cmml">i</mi><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.2.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.3.cmml">l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em" id="S3.Ex3.m1.11.11.2.3.1.2.2.1" xref="S3.Ex3.m1.11.11.2.3.1.2.2.1.cmml">⋅</mo><msup
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.3.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.2.cmml">𝒉</mi><mrow id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2.cmml">l</mi><mo id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.1"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.1.cmml">−</mo><mn id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3.cmml">1</mn></mrow></msup></mrow><mo id="S3.Ex3.m1.11.11.2.3.1.2.1"
    xref="S3.Ex3.m1.11.11.2.3.1.2.1.cmml">+</mo><msubsup id="S3.Ex3.m1.11.11.2.3.1.2.3"
    xref="S3.Ex3.m1.11.11.2.3.1.2.3.cmml"><mi id="S3.Ex3.m1.11.11.2.3.1.2.3.2.2" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.2.cmml">b</mi><mi
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.3" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.11.11.2.3.1.2.3.3" xref="S3.Ex3.m1.11.11.2.3.1.2.3.3.cmml">l</mi></msubsup></mrow><mo
    id="S3.Ex3.m1.11.11.2.3.1.1" xref="S3.Ex3.m1.11.11.2.3.1.1.cmml">≥</mo><mn id="S3.Ex3.m1.11.11.2.3.1.3"
    xref="S3.Ex3.m1.11.11.2.3.1.3.cmml">0</mn></mrow></mtd><mtd id="S3.Ex3.m1.17.17c"
    xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.11.11.2.2.2.2" xref="S3.Ex3.m1.11.11.2.2.2.3.cmml"><mrow
    id="S3.Ex3.m1.10.10.1.1.1.1.1" xref="S3.Ex3.m1.10.10.1.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.10.10.1.1.1.1.1.2"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.10.10.1.1.1.1.1.2.1"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.1.cmml">∀</mo><mi id="S3.Ex3.m1.10.10.1.1.1.1.1.2.2"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.2.cmml">l</mi></mrow><mo id="S3.Ex3.m1.10.10.1.1.1.1.1.1"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.1.cmml">∈</mo><mi id="S3.Ex3.m1.10.10.1.1.1.1.1.3"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.3.cmml">𝕃</mi></mrow><mo id="S3.Ex3.m1.11.11.2.2.2.2.3"
    xref="S3.Ex3.m1.11.11.2.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.11.11.2.2.2.2.2"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2.cmml"><mi id="S3.Ex3.m1.11.11.2.2.2.2.2.2" xref="S3.Ex3.m1.11.11.2.2.2.2.2.2.cmml">i</mi><mo
    id="S3.Ex3.m1.11.11.2.2.2.2.2.1" xref="S3.Ex3.m1.11.11.2.2.2.2.2.1.cmml">∈</mo><msup
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.11.11.2.2.2.2.2.3.2"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.2.cmml">𝕊</mi><mi id="S3.Ex3.m1.11.11.2.2.2.2.2.3.3"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr><mtr
    id="S3.Ex3.m1.17.17d" xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17e" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.13.13.4.3.1" xref="S3.Ex3.m1.13.13.4.3.1.cmml"><msubsup id="S3.Ex3.m1.13.13.4.3.1.2"
    xref="S3.Ex3.m1.13.13.4.3.1.2.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.2.2.2" xref="S3.Ex3.m1.13.13.4.3.1.2.2.2.cmml">h</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.2.2.3" xref="S3.Ex3.m1.13.13.4.3.1.2.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.2.3" xref="S3.Ex3.m1.13.13.4.3.1.2.3.cmml">l</mi></msubsup><mo
    id="S3.Ex3.m1.13.13.4.3.1.1" xref="S3.Ex3.m1.13.13.4.3.1.1.cmml">=</mo><mrow id="S3.Ex3.m1.13.13.4.3.1.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.cmml"><mrow id="S3.Ex3.m1.13.13.4.3.1.3.2" xref="S3.Ex3.m1.13.13.4.3.1.3.2.cmml"><msubsup
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2.cmml">𝒘</mi><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3.cmml">i</mi><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.2.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.3.cmml">l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em" id="S3.Ex3.m1.13.13.4.3.1.3.2.1" xref="S3.Ex3.m1.13.13.4.3.1.3.2.1.cmml">⋅</mo><msup
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.3.2"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.2.cmml">𝒉</mi><mrow id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2.cmml">l</mi><mo id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.1"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.1.cmml">−</mo><mn id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3.cmml">1</mn></mrow></msup></mrow><mo id="S3.Ex3.m1.13.13.4.3.1.3.1"
    xref="S3.Ex3.m1.13.13.4.3.1.3.1.cmml">+</mo><msubsup id="S3.Ex3.m1.13.13.4.3.1.3.3"
    xref="S3.Ex3.m1.13.13.4.3.1.3.3.cmml"><mi id="S3.Ex3.m1.13.13.4.3.1.3.3.2.2" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.2.cmml">b</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.3" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.13.13.4.3.1.3.3.3" xref="S3.Ex3.m1.13.13.4.3.1.3.3.3.cmml">l</mi></msubsup></mrow></mrow></mtd><mtd
    id="S3.Ex3.m1.17.17f" xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.13.13.4.2.2.2"
    xref="S3.Ex3.m1.13.13.4.2.2.3.cmml"><mrow id="S3.Ex3.m1.12.12.3.1.1.1.1" xref="S3.Ex3.m1.12.12.3.1.1.1.1.cmml"><mrow
    id="S3.Ex3.m1.12.12.3.1.1.1.1.2" xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.cmml"><mo rspace="0.167em"
    id="S3.Ex3.m1.12.12.3.1.1.1.1.2.1" xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.1.cmml">∀</mo><mi
    id="S3.Ex3.m1.12.12.3.1.1.1.1.2.2" xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.2.cmml">l</mi></mrow><mo
    id="S3.Ex3.m1.12.12.3.1.1.1.1.1" xref="S3.Ex3.m1.12.12.3.1.1.1.1.1.cmml">∈</mo><mi
    id="S3.Ex3.m1.12.12.3.1.1.1.1.3" xref="S3.Ex3.m1.12.12.3.1.1.1.1.3.cmml">𝕃</mi></mrow><mo
    id="S3.Ex3.m1.13.13.4.2.2.2.3" xref="S3.Ex3.m1.13.13.4.2.2.3a.cmml">,</mo><mrow
    id="S3.Ex3.m1.13.13.4.2.2.2.2" xref="S3.Ex3.m1.13.13.4.2.2.2.2.cmml"><mi id="S3.Ex3.m1.13.13.4.2.2.2.2.2"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.2.cmml">i</mi><mo id="S3.Ex3.m1.13.13.4.2.2.2.2.1"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.1.cmml">∈</mo><msup id="S3.Ex3.m1.13.13.4.2.2.2.2.3"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.13.13.4.2.2.2.2.3.2"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.2.cmml">𝕊</mi><mi id="S3.Ex3.m1.13.13.4.2.2.2.2.3.3"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr><mtr
    id="S3.Ex3.m1.17.17g" xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17h" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.15.15.6.3.1" xref="S3.Ex3.m1.15.15.6.3.1.cmml"><mrow id="S3.Ex3.m1.15.15.6.3.1.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.cmml"><mrow id="S3.Ex3.m1.15.15.6.3.1.2.2" xref="S3.Ex3.m1.15.15.6.3.1.2.2.cmml"><msubsup
    id="S3.Ex3.m1.15.15.6.3.1.2.2.2" xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2.cmml">𝒘</mi><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3.cmml">i</mi><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.2.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.3.cmml">l</mi></msubsup><mo lspace="0.222em"
    rspace="0.222em" id="S3.Ex3.m1.15.15.6.3.1.2.2.1" xref="S3.Ex3.m1.15.15.6.3.1.2.2.1.cmml">⋅</mo><msup
    id="S3.Ex3.m1.15.15.6.3.1.2.2.3" xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.3.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.2.cmml">h</mi><mrow id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2.cmml">l</mi><mo id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.1"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.1.cmml">−</mo><mn id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3.cmml">1</mn></mrow></msup></mrow><mo id="S3.Ex3.m1.15.15.6.3.1.2.1"
    xref="S3.Ex3.m1.15.15.6.3.1.2.1.cmml">+</mo><msubsup id="S3.Ex3.m1.15.15.6.3.1.2.3"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.cmml"><mi id="S3.Ex3.m1.15.15.6.3.1.2.3.2.2" xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.2.cmml">b</mi><mi
    id="S3.Ex3.m1.15.15.6.3.1.2.3.2.3" xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.15.15.6.3.1.2.3.3" xref="S3.Ex3.m1.15.15.6.3.1.2.3.3.cmml">l</mi></msubsup></mrow><mo
    id="S3.Ex3.m1.15.15.6.3.1.1" xref="S3.Ex3.m1.15.15.6.3.1.1.cmml">≤</mo><mn id="S3.Ex3.m1.15.15.6.3.1.3"
    xref="S3.Ex3.m1.15.15.6.3.1.3.cmml">0</mn></mrow></mtd><mtd id="S3.Ex3.m1.17.17i"
    xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.15.15.6.2.2.2" xref="S3.Ex3.m1.15.15.6.2.2.3.cmml"><mrow
    id="S3.Ex3.m1.14.14.5.1.1.1.1" xref="S3.Ex3.m1.14.14.5.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.14.14.5.1.1.1.1.2"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.14.14.5.1.1.1.1.2.1"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.1.cmml">∀</mo><mi id="S3.Ex3.m1.14.14.5.1.1.1.1.2.2"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.2.cmml">l</mi></mrow><mo id="S3.Ex3.m1.14.14.5.1.1.1.1.1"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.1.cmml">∈</mo><mi id="S3.Ex3.m1.14.14.5.1.1.1.1.3"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.3.cmml">𝕃</mi></mrow><mo id="S3.Ex3.m1.15.15.6.2.2.2.3"
    xref="S3.Ex3.m1.15.15.6.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.15.15.6.2.2.2.2"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2.cmml"><mi id="S3.Ex3.m1.15.15.6.2.2.2.2.2" xref="S3.Ex3.m1.15.15.6.2.2.2.2.2.cmml">i</mi><mo
    id="S3.Ex3.m1.15.15.6.2.2.2.2.1" xref="S3.Ex3.m1.15.15.6.2.2.2.2.1.cmml">∉</mo><msup
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.15.15.6.2.2.2.2.3.2"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.2.cmml">𝕊</mi><mi id="S3.Ex3.m1.15.15.6.2.2.2.2.3.3"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr><mtr
    id="S3.Ex3.m1.17.17j" xref="S3.Ex3.m1.17.17.cmml"><mtd id="S3.Ex3.m1.17.17k" xref="S3.Ex3.m1.17.17.cmml"><mrow
    id="S3.Ex3.m1.17.17.8.3.1" xref="S3.Ex3.m1.17.17.8.3.1.cmml"><msubsup id="S3.Ex3.m1.17.17.8.3.1.2"
    xref="S3.Ex3.m1.17.17.8.3.1.2.cmml"><mi id="S3.Ex3.m1.17.17.8.3.1.2.2.2" xref="S3.Ex3.m1.17.17.8.3.1.2.2.2.cmml">h</mi><mi
    id="S3.Ex3.m1.17.17.8.3.1.2.2.3" xref="S3.Ex3.m1.17.17.8.3.1.2.2.3.cmml">i</mi><mi
    id="S3.Ex3.m1.17.17.8.3.1.2.3" xref="S3.Ex3.m1.17.17.8.3.1.2.3.cmml">l</mi></msubsup><mo
    id="S3.Ex3.m1.17.17.8.3.1.1" xref="S3.Ex3.m1.17.17.8.3.1.1.cmml">=</mo><mn id="S3.Ex3.m1.17.17.8.3.1.3"
    xref="S3.Ex3.m1.17.17.8.3.1.3.cmml">0</mn></mrow></mtd><mtd id="S3.Ex3.m1.17.17l"
    xref="S3.Ex3.m1.17.17.cmml"><mrow id="S3.Ex3.m1.17.17.8.2.2.2" xref="S3.Ex3.m1.17.17.8.2.2.3.cmml"><mrow
    id="S3.Ex3.m1.16.16.7.1.1.1.1" xref="S3.Ex3.m1.16.16.7.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.16.16.7.1.1.1.1.2"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.16.16.7.1.1.1.1.2.1"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.1.cmml">∀</mo><mi id="S3.Ex3.m1.16.16.7.1.1.1.1.2.2"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.2.cmml">l</mi></mrow><mo id="S3.Ex3.m1.16.16.7.1.1.1.1.1"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.1.cmml">∈</mo><mi id="S3.Ex3.m1.16.16.7.1.1.1.1.3"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.3.cmml">𝕃</mi></mrow><mo id="S3.Ex3.m1.17.17.8.2.2.2.3"
    xref="S3.Ex3.m1.17.17.8.2.2.3a.cmml">,</mo><mrow id="S3.Ex3.m1.17.17.8.2.2.2.2"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2.cmml"><mi id="S3.Ex3.m1.17.17.8.2.2.2.2.2" xref="S3.Ex3.m1.17.17.8.2.2.2.2.2.cmml">i</mi><mo
    id="S3.Ex3.m1.17.17.8.2.2.2.2.1" xref="S3.Ex3.m1.17.17.8.2.2.2.2.1.cmml">∉</mo><msup
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.cmml"><mi id="S3.Ex3.m1.17.17.8.2.2.2.2.3.2"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.2.cmml">𝕊</mi><mi id="S3.Ex3.m1.17.17.8.2.2.2.2.3.3"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.3.cmml">l</mi></msup></mrow></mrow></mtd></mtr></mtable><mo
    id="S3.Ex3.m1.18.18.1.1.3.2.2.2" xref="S3.Ex3.m1.17.17.cmml">)</mo></mrow></mrow></mrow><mo
    lspace="0em" id="S3.Ex3.m1.18.18.1.2" xref="S3.Ex3.m1.18.18.1.1.cmml">.</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S3.Ex3.m1.18b"><apply id="S3.Ex3.m1.18.18.1.1.cmml"
    xref="S3.Ex3.m1.18.18.1"><ci id="S3.Ex3.m1.18.18.1.1.2.cmml" xref="S3.Ex3.m1.18.18.1.1.2">𝒟</ci><apply
    id="S3.Ex3.m1.18.18.1.1.3.cmml" xref="S3.Ex3.m1.18.18.1.1.3"><apply id="S3.Ex3.m1.18.18.1.1.3.1.cmml"
    xref="S3.Ex3.m1.18.18.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.18.18.1.1.3.1.1.cmml"
    xref="S3.Ex3.m1.18.18.1.1.3.1">subscript</csymbol><apply id="S3.Ex3.m1.9.9.9.cmml"
    xref="S3.Ex3.m1.9.9.9"><vector id="S3.Ex3.m1.7.7.7.7.3.cmml" xref="S3.Ex3.m1.7.7.7.7.2"><apply
    id="S3.Ex3.m1.6.6.6.6.1.1.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.6.6.6.6.1.1.1.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1">superscript</csymbol><ci
    id="S3.Ex3.m1.6.6.6.6.1.1.2.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1.2">𝕊</ci><cn type="integer"
    id="S3.Ex3.m1.6.6.6.6.1.1.3.cmml" xref="S3.Ex3.m1.6.6.6.6.1.1.3">1</cn></apply><ci
    id="S3.Ex3.m1.1.1.1.1.cmml" xref="S3.Ex3.m1.1.1.1.1">…</ci><apply id="S3.Ex3.m1.7.7.7.7.2.2.cmml"
    xref="S3.Ex3.m1.7.7.7.7.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.7.7.7.7.2.2.1.cmml"
    xref="S3.Ex3.m1.7.7.7.7.2.2">superscript</csymbol><ci id="S3.Ex3.m1.7.7.7.7.2.2.2.cmml"
    xref="S3.Ex3.m1.7.7.7.7.2.2.2">𝕊</ci><ci id="S3.Ex3.m1.7.7.7.7.2.2.3.cmml" xref="S3.Ex3.m1.7.7.7.7.2.2.3">𝐿</ci></apply></vector><apply
    id="S3.Ex3.m1.9.9.9.9.cmml" xref="S3.Ex3.m1.9.9.9.9"><set id="S3.Ex3.m1.8.8.8.8.1.2.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1"><cn type="integer" id="S3.Ex3.m1.2.2.2.2.cmml" xref="S3.Ex3.m1.2.2.2.2">1</cn><ci
    id="S3.Ex3.m1.3.3.3.3.cmml" xref="S3.Ex3.m1.3.3.3.3">…</ci><apply id="S3.Ex3.m1.8.8.8.8.1.1.1.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.8.8.8.8.1.1.1.1.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.8.8.8.8.1.1.1.2.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.2">𝑛</ci><cn type="integer" id="S3.Ex3.m1.8.8.8.8.1.1.1.3.cmml"
    xref="S3.Ex3.m1.8.8.8.8.1.1.1.3">1</cn></apply></set><ci id="S3.Ex3.m1.9.9.9.9.4.cmml"
    xref="S3.Ex3.m1.9.9.9.9.4">…</ci><set id="S3.Ex3.m1.9.9.9.9.2.2.cmml" xref="S3.Ex3.m1.9.9.9.9.2.1"><cn
    type="integer" id="S3.Ex3.m1.4.4.4.4.cmml" xref="S3.Ex3.m1.4.4.4.4">1</cn><ci
    id="S3.Ex3.m1.5.5.5.5.cmml" xref="S3.Ex3.m1.5.5.5.5">…</ci><apply id="S3.Ex3.m1.9.9.9.9.2.1.1.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.9.9.9.9.2.1.1.1.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1">subscript</csymbol><ci id="S3.Ex3.m1.9.9.9.9.2.1.1.2.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1.2">𝑛</ci><ci id="S3.Ex3.m1.9.9.9.9.2.1.1.3.cmml"
    xref="S3.Ex3.m1.9.9.9.9.2.1.1.3">𝐿</ci></apply></set></apply></apply></apply><matrix
    id="S3.Ex3.m1.17.17.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><matrixrow id="S3.Ex3.m1.17.17a.cmml"
    xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.11.11.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1"><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2"><apply id="S3.Ex3.m1.11.11.2.3.1.2.2.cmml"
    xref="S3.Ex3.m1.11.11.2.3.1.2.2"><ci id="S3.Ex3.m1.11.11.2.3.1.2.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.1">⋅</ci><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.2.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2">superscript</csymbol><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2">subscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.2">𝒘</ci><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.2.3">𝑖</ci></apply><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.2.3">𝑙</ci></apply><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.2">𝒉</ci><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3"><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.2">𝑙</ci><cn
    type="integer" id="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.2.3.3.3">1</cn></apply></apply></apply><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3">superscript</csymbol><apply
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.3.1.2.3.2.1.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3">subscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.2.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.2">𝑏</ci><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.3.2.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3.2.3">𝑖</ci></apply><ci
    id="S3.Ex3.m1.11.11.2.3.1.2.3.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.2.3.3">𝑙</ci></apply></apply><cn
    type="integer" id="S3.Ex3.m1.11.11.2.3.1.3.cmml" xref="S3.Ex3.m1.11.11.2.3.1.3">0</cn></apply><apply
    id="S3.Ex3.m1.11.11.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.11.11.2.2.2.3a.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.3">formulae-sequence</csymbol><apply
    id="S3.Ex3.m1.10.10.1.1.1.1.1.cmml" xref="S3.Ex3.m1.10.10.1.1.1.1.1"><apply id="S3.Ex3.m1.10.10.1.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.10.10.1.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.10.10.1.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.2.2">𝑙</ci></apply><ci id="S3.Ex3.m1.10.10.1.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.10.10.1.1.1.1.1.3">𝕃</ci></apply><apply id="S3.Ex3.m1.11.11.2.2.2.2.2.cmml"
    xref="S3.Ex3.m1.11.11.2.2.2.2.2"><ci id="S3.Ex3.m1.11.11.2.2.2.2.2.2.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.2">𝑖</ci><apply
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.11.11.2.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.2">𝕊</ci><ci
    id="S3.Ex3.m1.11.11.2.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.11.11.2.2.2.2.2.3.3">𝑙</ci></apply></apply></apply></matrixrow><matrixrow
    id="S3.Ex3.m1.17.17b.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.13.13.4.3.1.cmml"
    xref="S3.Ex3.m1.13.13.4.3.1"><apply id="S3.Ex3.m1.13.13.4.3.1.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2">superscript</csymbol><apply
    id="S3.Ex3.m1.13.13.4.3.1.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.13.13.4.3.1.2.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2">subscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2.2.2">ℎ</ci><ci
    id="S3.Ex3.m1.13.13.4.3.1.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2.2.3">𝑖</ci></apply><ci
    id="S3.Ex3.m1.13.13.4.3.1.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.2.3">𝑙</ci></apply><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3"><apply id="S3.Ex3.m1.13.13.4.3.1.3.2.cmml"
    xref="S3.Ex3.m1.13.13.4.3.1.3.2"><ci id="S3.Ex3.m1.13.13.4.3.1.3.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.1">⋅</ci><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.2.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2">superscript</csymbol><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2">subscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.2">𝒘</ci><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.2.3">𝑖</ci></apply><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.2.3">𝑙</ci></apply><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.2.3.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.2">𝒉</ci><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3"><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.2">𝑙</ci><cn
    type="integer" id="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.2.3.3.3">1</cn></apply></apply></apply><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.3.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3">superscript</csymbol><apply
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.3.1.3.3.2.1.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3">subscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.2.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.2">𝑏</ci><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.3.2.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3.2.3">𝑖</ci></apply><ci
    id="S3.Ex3.m1.13.13.4.3.1.3.3.3.cmml" xref="S3.Ex3.m1.13.13.4.3.1.3.3.3">𝑙</ci></apply></apply></apply><apply
    id="S3.Ex3.m1.13.13.4.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.13.13.4.2.2.3a.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.3">formulae-sequence</csymbol><apply
    id="S3.Ex3.m1.12.12.3.1.1.1.1.cmml" xref="S3.Ex3.m1.12.12.3.1.1.1.1"><apply id="S3.Ex3.m1.12.12.3.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.12.12.3.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.12.12.3.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.2.2">𝑙</ci></apply><ci id="S3.Ex3.m1.12.12.3.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.12.12.3.1.1.1.1.3">𝕃</ci></apply><apply id="S3.Ex3.m1.13.13.4.2.2.2.2.cmml"
    xref="S3.Ex3.m1.13.13.4.2.2.2.2"><ci id="S3.Ex3.m1.13.13.4.2.2.2.2.2.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.2">𝑖</ci><apply
    id="S3.Ex3.m1.13.13.4.2.2.2.2.3.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.13.13.4.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.13.13.4.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.2">𝕊</ci><ci
    id="S3.Ex3.m1.13.13.4.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.13.13.4.2.2.2.2.3.3">𝑙</ci></apply></apply></apply></matrixrow><matrixrow
    id="S3.Ex3.m1.17.17c.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.15.15.6.3.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1"><apply id="S3.Ex3.m1.15.15.6.3.1.2.cmml" xref="S3.Ex3.m1.15.15.6.3.1.2"><apply
    id="S3.Ex3.m1.15.15.6.3.1.2.2.cmml" xref="S3.Ex3.m1.15.15.6.3.1.2.2"><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.1">⋅</ci><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.2.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2">superscript</csymbol><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2">subscript</csymbol><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.2">𝒘</ci><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.2.3">𝑖</ci></apply><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.2.3">𝑙</ci></apply><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.2.3.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3">superscript</csymbol><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.3.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.2">ℎ</ci><apply id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3"><ci id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.2">𝑙</ci><cn type="integer" id="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.2.3.3.3">1</cn></apply></apply></apply><apply id="S3.Ex3.m1.15.15.6.3.1.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.3.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3">superscript</csymbol><apply id="S3.Ex3.m1.15.15.6.3.1.2.3.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.3.1.2.3.2.1.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3">subscript</csymbol><ci id="S3.Ex3.m1.15.15.6.3.1.2.3.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.2">𝑏</ci><ci id="S3.Ex3.m1.15.15.6.3.1.2.3.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.2.3">𝑖</ci></apply><ci id="S3.Ex3.m1.15.15.6.3.1.2.3.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.2.3.3">𝑙</ci></apply></apply><cn type="integer" id="S3.Ex3.m1.15.15.6.3.1.3.cmml"
    xref="S3.Ex3.m1.15.15.6.3.1.3">0</cn></apply><apply id="S3.Ex3.m1.15.15.6.2.2.3.cmml"
    xref="S3.Ex3.m1.15.15.6.2.2.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.15.15.6.2.2.3a.cmml"
    xref="S3.Ex3.m1.15.15.6.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex3.m1.14.14.5.1.1.1.1.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1"><apply id="S3.Ex3.m1.14.14.5.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.14.14.5.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.14.14.5.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.2.2">𝑙</ci></apply><ci id="S3.Ex3.m1.14.14.5.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.14.14.5.1.1.1.1.3">𝕃</ci></apply><apply id="S3.Ex3.m1.15.15.6.2.2.2.2.cmml"
    xref="S3.Ex3.m1.15.15.6.2.2.2.2"><ci id="S3.Ex3.m1.15.15.6.2.2.2.2.2.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.2">𝑖</ci><apply
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.15.15.6.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.2">𝕊</ci><ci
    id="S3.Ex3.m1.15.15.6.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.15.15.6.2.2.2.2.3.3">𝑙</ci></apply></apply></apply></matrixrow><matrixrow
    id="S3.Ex3.m1.17.17d.cmml" xref="S3.Ex3.m1.18.18.1.1.3.2.2"><apply id="S3.Ex3.m1.17.17.8.3.1.cmml"
    xref="S3.Ex3.m1.17.17.8.3.1"><apply id="S3.Ex3.m1.17.17.8.3.1.2.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.17.17.8.3.1.2.1.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2">superscript</csymbol><apply
    id="S3.Ex3.m1.17.17.8.3.1.2.2.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.17.17.8.3.1.2.2.1.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2">subscript</csymbol><ci
    id="S3.Ex3.m1.17.17.8.3.1.2.2.2.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2.2.2">ℎ</ci><ci
    id="S3.Ex3.m1.17.17.8.3.1.2.2.3.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2.2.3">𝑖</ci></apply><ci
    id="S3.Ex3.m1.17.17.8.3.1.2.3.cmml" xref="S3.Ex3.m1.17.17.8.3.1.2.3">𝑙</ci></apply><cn
    type="integer" id="S3.Ex3.m1.17.17.8.3.1.3.cmml" xref="S3.Ex3.m1.17.17.8.3.1.3">0</cn></apply><apply
    id="S3.Ex3.m1.17.17.8.2.2.3.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2"><csymbol cd="ambiguous"
    id="S3.Ex3.m1.17.17.8.2.2.3a.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.3">formulae-sequence</csymbol><apply
    id="S3.Ex3.m1.16.16.7.1.1.1.1.cmml" xref="S3.Ex3.m1.16.16.7.1.1.1.1"><apply id="S3.Ex3.m1.16.16.7.1.1.1.1.2.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2"><csymbol cd="latexml" id="S3.Ex3.m1.16.16.7.1.1.1.1.2.1.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.1">for-all</csymbol><ci id="S3.Ex3.m1.16.16.7.1.1.1.1.2.2.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.2.2">𝑙</ci></apply><ci id="S3.Ex3.m1.16.16.7.1.1.1.1.3.cmml"
    xref="S3.Ex3.m1.16.16.7.1.1.1.1.3">𝕃</ci></apply><apply id="S3.Ex3.m1.17.17.8.2.2.2.2.cmml"
    xref="S3.Ex3.m1.17.17.8.2.2.2.2"><ci id="S3.Ex3.m1.17.17.8.2.2.2.2.2.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.2">𝑖</ci><apply
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3"><csymbol
    cd="ambiguous" id="S3.Ex3.m1.17.17.8.2.2.2.2.3.1.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3">superscript</csymbol><ci
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3.2.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.2">𝕊</ci><ci
    id="S3.Ex3.m1.17.17.8.2.2.2.2.3.3.cmml" xref="S3.Ex3.m1.17.17.8.2.2.2.2.3.3">𝑙</ci></apply></apply></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S3.Ex3.m1.18c">\mathcal{D}=\bigvee_{({\mathbb{S}}^{1},\ldots,{\mathbb{S}}^{L})\subseteq\{1,\ldots,n_{1}\}\times\ldots\times\{1,\ldots,n_{L}\}}\left(\begin{array}[]{cc}{\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\geq
    0&\forall l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ h_{i}^{l}={\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}&\forall
    l\in{\mathbb{L}},i\in{\mathbb{S}}^{l}\\ {\bm{w}}_{i}^{l}\cdot h^{l-1}+b_{i}^{l}\leq
    0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\\ h_{i}^{l}=0&\forall l\in{\mathbb{L}},i\notin{\mathbb{S}}^{l}\end{array}\right).</annotation></semantics></math>
    |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'Such partitioning entails an overlap between adjacent linear regions when ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    i.e., at the boundary in which unit $i$ in layer $l$ is active in one region and
    inactive in another. Nevertheless, for any input $\overline{{\bm{x}}}$ associated
    with a point at such a boundary between two linear regions ${\mathbb{I}}_{1}$
    and ${\mathbb{I}}_{2}$, it holds that ${\bm{y}}_{{\mathbb{I}}_{1}}(\overline{{\bm{x}}})={\bm{y}}_{{\mathbb{I}}_{2}}(\overline{{\bm{x}}})$
    even if those affine transformations are not entirely identical since the output
    of the neural network is continuous. More importantly, such overlap implies that
    each term of $\mathcal{D}$ is defined using only equalities and nonstrict inequalities,
    and therefore that each linear region corresponds to polyhedra in the extended
    space $({\bm{x}},{\bm{h}}^{1},\ldots,{\bm{h}}^{L})$. Consequently, those linear
    regions also define polyhedra if projected to the input space ${\bm{x}}$, since
    by using Fourier-Motzkin elimination (Fourier, [1826](#bib.bib107), Motzkin, [1936](#bib.bib226))
    we can obtain a polyhedral description of the linear region in ${\bm{x}}$. Moreover,
    the interior of those polyhedra are disjoint. If one of those polyhedra does not
    have an interior, which means that it is not full-dimensional, then that linear
    region lies entirely on the boundary of other linear regions. In such a case,
    we do not regard it as a proper linear region. By looking at the geometry of those
    linear regions from a different perspective in Section [3.3](#S3.SS3 "3.3 The
    geometry of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey") and understanding its impact on the
    number of linear regions in Section [3.4](#S3.SS4 "3.4 The number of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), we will see that many terms of $\mathcal{D}$ may actually
    be empty.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization over the union of polyhedra is the subject of disjunctive
    programming, which has contributed to the development of stronger formulations
    and better algorithms to solve discrete optimization problems. These are formulated
    as MILPs as well as more general types of problems in recent years (Balas, [2018](#bib.bib12)),
    including generalized disjunctive programming for Mixed-Integer Non-Linear Programming (MINLP)
    (Raman and Grossmann, [1994](#bib.bib256), Grossmann and Ruiz, [2012](#bib.bib134)).
    One of such contributions is the generation of valid inequalities to strengthen
    MILP formulations, which are also denoted as cutting planes, through the lift-and-project
    technique (Balas et al., [1993](#bib.bib13), [1996](#bib.bib14)). In fact, we
    can develop stronger formulations for optimization problems involving neural networks
    through the lenses of disjunctive programming, as we discuss later in Section [4.2](#S4.SS2
    "4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 The geometry of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another form of looking at the geometry of linear regions is through their transformation
    along the layers of the neural network. Namely, we can think of the input space
    as initially being partitioned by the units of the first layer, and then each
    resulting linear region being further partitioned by the subsequent layers. In
    that sense, we can think of every layer as a particular form of “slicing” the
    input. In fact, a layer may slice each linear region that is defined by the preceding
    layer in a different way due to which neurons are active or not in previous layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us begin by illustrating how a given layer $l\in{\mathbb{L}}$ partitions
    its input space ${\bm{h}}^{l-1}$. Every neuron $i$ in layer $l$ is associated
    with an *activation hyperplane* of the form ${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}=0$,
    which divides the possible inputs of its layer into an open half-space in which
    the unit is active (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}>0$) and a closed
    half-space in which the unit is inactive (${\bm{w}}_{i}^{l}\cdot{\bm{h}}^{l-1}+b_{i}^{l}\leq
    0$). These hyperplanes define the boundary between adjacent linear regions, and
    the arrangement of such hyperplanes for a given layer $l\in{\mathbb{L}}$ determines
    how that layer partitions the ${\bm{h}}^{l-1}$ space. In other words, every input
    in ${\bm{h}}^{l-1}$ can be located with respect to each of those hyperplanes,
    which corresponds to the activation set of the linear region to which it belongs.
    However, not every activation set out of the $2^{n_{l}}$ possible ones maps to
    a nonempty region of the input space. In the case of Example [2](#Thmexample2
    "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), there is no
    linear region in which the activation set is empty.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and a single
    layer having 3 neurons $\alpha$, $\beta$, and $\gamma$ with outputs given as follows:
    $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+0.6,0\}$, $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+4.8,0\}$,
    and $h^{1}_{\gamma}=\max\{0x_{1}+3x_{2}-5,0\}$. These neurons define the activation
    hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+0.6=0$, ($\beta$) $-0.9x_{1}-0.7x_{2}+4.8=0$,
    and ($\gamma$) $0x_{1}+3x_{2}-5=0$ in the space ${\bm{x}}$, which are illustrated
    along with the activation sets of the linear regions in Figure [5](#S3.F5 "Figure
    5 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of $2^{3}$ linear regions corresponding to each possible activation
    set defined by a subset of the neurons in $\{\alpha,\beta,\gamma\}$, the arrangement
    of such hyperplanes produces 7 linear regions, which is in fact the maximum number
    of 2-dimensional regions that can be defined by drawing 3 lines on a plane.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/275229d48fa5bb1b250445d5df8fa0b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Linear regions defined by the shallow neural network described in
    Example [2](#Thmexample2 "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The
    Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey"). Every line corresponds to the activation hyperplane of a different
    neuron, which is given by $\alpha$, $\beta$, and $\gamma$ in parentheses. The
    arrow next to each line points to the half space in which the inputs activate
    that neuron. Every linear region has a subset of $\{\alpha,\beta,\gamma\}$ as
    its corresponding activation set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The maximum number of full-dimensional regions resulting from a partitioning
    defined by $n$ hyperplanes depends on the dimension $d$ of the space in which
    those hyperplanes are defined (Zaslavsky, [1975](#bib.bib352)). That number never
    exceeds $\sum\limits_{i=1}^{\min\{d,n\}}\binom{n}{i}$. Such bound only coincides
    with $2^{n}$ if $d\geq n$; otherwise, as illustrated in Example [2](#Thmexample2
    "Example 2 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), that number
    can be smaller. As observed by Hanin and Rolnick ([2019b](#bib.bib138)), that
    bound is $O\left(\frac{n^{d}}{d!}\right)$ when $n\gg d$.'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the above bound is all that we need to determine the maximum number
    of linear regions in shallow networks. While not every shallow network may define
    as many linear regions, it is always possible to put the hyperplanes in what is
    called a *general position* in order to reach that bound. Thus, the maximum number
    of linear regions defined by a shallow network is $\sum\limits_{i=0}^{\min\{n_{0},n_{1}\}}\binom{n_{1}}{n_{0}}$.
  prefs: []
  type: TYPE_NORMAL
- en: For the polyhedron associated with each linear region, being in general position
    implies that each vertex lies on exactly $d$ activation hyperplanes. For context,
    the converse situation in linear programming —having more hyperplanes active on
    a vertex than the space dimension— characterizes degeneracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of deep networks, the partitioning of each linear region by the
    subsequent layers is based on the output of that linear region. This affects the
    shape and the number of the linear regions defined by the following layers, which
    may vary between adjacent linear regions due to which units are active or inactive
    from one linear region to another, as illustrated in Example [3](#Thmexample3
    "Example 3 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Example 3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a neural network with domain ${\bm{x}}\in\mathbb{R}^{2}$ and 2 layers
    having 2 neurons each —say neurons $\alpha$ and $\beta$ in layer 1, and neurons
    $\gamma$ and $\delta$ in layer 2— with outputs given as follows: $h^{1}_{\alpha}=\max\{2.3x_{1}-1.9x_{2}+1.5,0\}$,
    $h^{1}_{\beta}=\max\{-0.9x_{1}-0.7x_{2}+5,0\}$, $h^{2}_{\gamma}=\max\{0.4h^{1}_{1}-3.1h^{1}_{2}+4,0\}$,
    $h^{2}_{\delta}=\max\{-0.6h^{1}_{1}-1.6h^{1}_{2}+5,0\}$. These neurons define
    the activation hyperplanes ($\alpha$) $2.3x_{1}-1.9x_{2}+1.5=0$ and ($\beta$)
    $-0.9x_{1}-0.7x_{2}+5=0$ in the ${\bm{x}}$ space and the activation hyperplanes
    ($\gamma$) $0.4h^{1}_{1}-3.1h^{1}_{2}+4=0$ and ($\delta$) $-0.6h^{1}_{1}-1.6h^{1}_{2}+5=0$
    in the ${\bm{h}}^{1}$ space, which are illustrated along with the activation sets
    of the linear regions in the first two plots of Figure [6](#S3.F6 "Figure 6 ‣
    3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). The third plot illustrates
    the linear regions jointly defined by the two layers in terms of the input space
    ${\bm{x}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The third plot is repeated in Figure [7](#S3.F7 "Figure 7 ‣ 3.3 The geometry
    of linear regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"), in which the shape of each linear region
    ${\mathbb{I}}$ is filled in accordance to the dimension of the image of $\bar{{\bm{y}}}_{{\mathbb{I}}}({\bm{x}})$
    —the output of the neural network for each linear region ${\mathbb{I}}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4b21733c24f690a28025ee4d147bf0d.png)![Refer to caption](img/66f9cc2c9bd7fbe539dcffe16058d0b5.png)![Refer
    to caption](img/7fc94c543268674eb63d5ca496258400.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Linear regions defined by the 2 layers of the neural network described
    in Example [3](#Thmexample3 "Example 3 ‣ 3.3 The geometry of linear regions ‣
    3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), following the same notation as in Figure [5](#S3.F5 "Figure
    5 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). The first and second
    plots show the linear regions and corresponding activation sets defined by the
    first and the second layers in terms of their input spaces (${\bm{x}}$ and ${\bm{h}}^{1}$).
    The third plot shows the linear regions defined by the combination of the 2 layers
    and the union of their activation sets in terms of the input space of the first
    layer (${\bm{x}}$).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example [3](#Thmexample3 "Example 3 ‣ 3.3 The geometry of linear regions ‣
    3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") highlights two important aspects about the structure of linear
    regions in deep neural networks. First, the linear regions defined by a neural
    network with multiple layers are different because activation hyperplanes after
    the first layer may look “bent” from the input space $x$, such as with the inflections
    of hyperplanes $(\gamma)$ and $(\delta)$ in the third plot of Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") from one linear
    region defined by the first layer to another. This partitioning of the input space
    would not be possible with a single layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By comparing side by side the first and the third plots of Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), we can see how
    every linear region of a given layer of a neural network may be partitioned differently
    by the following layer. When defined in terms of the input space ${\bm{x}}$, the
    hyperplanes associated with the second layer differ across the linear regions
    defined by the first layer because each of those linear regions is associated
    with a different affine transformation from ${\bm{x}}$ to ${\bm{h}}^{1}$. Hence,
    the activation hyperplanes of layer $l$ may break each linear region from layer
    $l-1$ differently. To every linear region defined by the hyperplane arrangement
    in the ${\bm{h}}^{l-1}$ space there is a linear transformation ${\bm{h}}^{l-1}=\Omega^{{\mathbb{S}}^{l-1}}({\bm{W}}^{l-1}{\bm{h}}^{l-2}+{\bm{b}}^{l-1})$
    to the points of that linear region based on the set of active neurons ${\mathbb{S}}^{l-1}$.
    Consequently, inputs in the ${\bm{h}}^{l-1}$ space that are associated with different
    linear regions are transformed differently to the ${\bm{h}}^{l}$ space, and therefore
    the form in which those linear regions are further partitioned by layer $l$ is
    not the same when seen from the ${\bm{h}}^{l-1}$ space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second, some combinations of activation sets of multiple layers do not correspond
    to linear regions even if the activation hyperplanes are in general position with
    respect to each layer. For each layer, the first two plots of Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 The geometry of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") show that every
    activation set corresponds to a nonempty region of the layer input. However, not
    every pair of such activation sets would define a nonempty linear region for the
    neural network. For example, the linear region of the first layer associated with
    the activation set ${\mathbb{S}}^{1}=\{\}$ defines a linear region in ${\bm{x}}$
    which is always mapped to ${\bm{h}}^{1}=0$, and thus only corresponds to activation
    set ${\mathbb{S}}^{2}=\{\gamma,\delta\}$ in the second layer because both units
    are active for such input. Thus, no linear region in ${\bm{x}}$ is associated
    with only the units in sets $\{\},\{\gamma\}$, and $\{\delta\}$ being active —i.e.,
    there is no linear region such that ${\mathbb{S}}^{1}\cup{\mathbb{S}}^{2}=\{\},\{\gamma\},\text{or}\{\delta\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: More generally, the number of units that is active on each linear region defined
    by the first layer also imposes a geometric limit to how that linear region can
    be further partitioned. If only one unit is active at a layer, that means that
    the output of the layer within that linear region has dimension 1, and, consequently,
    the subsequent hyperplane arrangements within that linear region are limited to
    a 1-dimensional space. For the network in the example, we thus expect no more
    than $\sum_{i=0}^{1}\binom{2}{i}=3$ linear regions being defined instead of $2^{2}=4$
    when only one unit is active. In fact, that is precisely the number of subdivisions
    by the second layer of the linear region defined by activation set ${\mathbb{S}}^{1}=\{\beta\}$
    from the first layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/738562b7813b118f7f87ea0943be8d01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Dimension of the image of the affine function ${\bm{y}}_{{\mathbb{I}}}({\bm{x}})$
    associated with each linear region ${\mathbb{I}}$ defined by the neural network
    described in Example [3](#Thmexample3 "Example 3 ‣ 3.3 The geometry of linear
    regions ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"). The linear regions are the same illustrated in
    the third plot of Figure [6](#S3.F6 "Figure 6 ‣ 3.3 The geometry of linear regions
    ‣ 3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'For every linear region defined by layer $l$ with an activation set ${\mathbb{S}}^{l}$,
    the dimension of the output of the corresponding transformation $\Omega_{{\mathbb{S}}^{l}}({\bm{W}}^{l}{\bm{h}}^{l-1}+{\bm{b}}^{l})$
    is at most $|{\mathbb{S}}^{l}|$ since $\text{rank}(\Omega_{{\mathbb{S}}^{l}})=|{\mathbb{S}}^{l}|$.
    Hence, the dimension of the output of every linear region defined by a rectifier
    network is upper bounded by its smallest activation set across all layers. This
    phenomenon was first identified by Serra et al. ([2018](#bib.bib282)) as the *bottleneck
    effect*. In neural networks with uniform width, this phenomenon leads to a surprising
    consequence: the number of linear regions with full-dimensional output is at most
    one. There are also consequences to the maximum number of linear regions that
    can be defined, as we discuss later.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 The geometry of decision regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is also common to study what inputs are associated with each class by a neural
    network. The set of inputs associated with the same class define a *decision region*.
    Difficulties in modeling functions such as the Boolean XOR in shallow networks
    are related to limitations on the form of the decision regions, which may be limited
    by the depth of the neural network. For example, Makhoul et al. ([1989](#bib.bib207))
    showed that two layers suffice to obtain disconnected decision regions.
  prefs: []
  type: TYPE_NORMAL
- en: The softmax layer is typically used for the output of neural networks trained
    on classification problems, in which the largest output corresponds to the class
    to which the input is associated. In rectifier networks coupled with a softmax
    layer, the decision regions can also be defined by polyhedra. Although the output
    of the softmax layer is not piecewise linear, its largest output corresponds to
    its largest input. Hence, every linear region ${\mathbb{I}}$ defined by layers
    1 to $L-1$ is partitioned by the softmax layer into decision regions where ${\bm{h}}^{L-1}_{i}\geq{\bm{h}}^{L-1}_{j}~{}\forall
    j\neq i$ for each class $i$ associated with the input ${\bm{h}}_{i}^{L-1}$ to
    the softmax layer. Therefore, each decision region of a rectifier networks consist
    of a union of polyhedra.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, we may say further in the typical setting where no hidden layer is
    wider than the input —i.e., $n_{0}\geq n_{l}~{}\forall l\in{\mathbb{L}}$: Nguyen
    et al. ([2018](#bib.bib233)) showed that at least one layer $l\in{\mathbb{L}}$
    must be such that $n_{l}>n_{0}$ for the network to present disconnected decision
    regions; and Grigsby and Lindsey ([2022](#bib.bib131)) showed that, for an input
    size $n_{0}\geq 2$, the decision regions are either empty or unbounded.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 The number of linear regions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have seen conditions that affect the number of linear regions both positively
    and negatively. We discuss these and other analytical results in Section [3.4.1](#S3.SS4.SSS1
    "3.4.1 Analytical results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"),
    and then discuss work on counting linear regions in practice in Section [3.4.2](#S3.SS4.SSS2
    "3.4.2 Counting linear regions ‣ 3.4 The number of linear regions ‣ 3 The Linear
    Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Analytical results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At least three lines of work on analytical results have brought important insights.
    The first line is based on constructing networks with a large number of linear
    regions, which leads to lower bounds on the maximum number of linear regions.
    The second line is based on showing how the network architecture —in particular
    its hyperparameters— may impact the hyperplane arrangements defined by the layers,
    which leads to upper bounds on the maximum number of linear regions. The third
    line is based on characterizing the parameters of neural networks according to
    how they are initialized and updated along training, which leads to results on
    the expected number of linear regions for such networks.
  prefs: []
  type: TYPE_NORMAL
- en: Lower bounds
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The lower bounds on the maximum number of linear regions are obtained through
    a careful choice of network parameters aimed at increasing the number of linear
    regions. In some cases, they also depend on particular choices of hyperparameters.
    We present them by order of refinement in Table [2](#S3.T2 "Table 2 ‣ Lower bounds
    ‣ 3.4.1 Analytical results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first lower bound was introduced by Pascanu et al. ([2014](#bib.bib243))
    and then improved by those authors with a new construction technique in Montúfar
    et al. ([2014](#bib.bib224)). In fact, Example [1](#Thmexample1 "Example 1 ‣ 3.1
    The combinatorial aspect of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") shows the case
    in which $n_{0}=1$ for the technique in Montúfar et al. ([2014](#bib.bib224)).
    While a different construction is proposed by Telgarsky ([2015](#bib.bib304)),
    subsequent developments in the literature have been based on Montúfar et al. ([2014](#bib.bib224)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lower bound by Arora et al. ([2018](#bib.bib8)) is based on a different
    technique to construct a first wide layer based on zonotopes, which is then followed
    by the same layers as in Montúfar et al. ([2014](#bib.bib224)). The first lower
    bound by Serra et al. ([2018](#bib.bib282)) reflects a slight change to the technique
    used by Montúfar et al. ([2014](#bib.bib224)), which in terms of Example [1](#Thmexample1
    "Example 1 ‣ 3.1 The combinatorial aspect of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") corresponds
    to using $n$ neurons to define $n+1$ instead of $n$ slopes on $[0,1]$. The second
    lower bound by Serra et al. ([2018](#bib.bib282)) extends that of Arora et al.
    ([2018](#bib.bib8)) by changing in the same way the construction of the subsequent
    layers of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Lower bounds on the maximum number of linear regions defined by a
    neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Bound and conditions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  Pascanu et al. ([2014](#bib.bib243)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  Montúfar et al. ([2014](#bib.bib224)) | $\left(\prod\limits_{l=1}^{L-1}\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  Telgarsky ([2015](#bib.bib304)) | $2^{\frac{L-3}{2}}$, where $n_{i}=1$ for
    $i$ odd, $n_{i}=2$ for $i$ even, and $L-3$ divides by 2 |'
  prefs: []
  type: TYPE_TB
- en: '|  Arora et al. ([2018](#bib.bib8)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}w^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  Serra et al. ([2018](#bib.bib282)) | $\left(\prod\limits_{l=1}^{L-1}\left(\left\lfloor\frac{n_{l}}{n_{0}}\right\rfloor+1\right)^{n_{0}}\right)\sum\limits_{i=0}^{n_{0}}\binom{n_{L}}{i}$,
    where $n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  Serra et al. ([2018](#bib.bib282)) | $2\sum\limits_{j=0}^{n_{0}-1}\binom{m-1}{j}(w+1)^{L-1}$,
    where $2m=n_{1}$ and $w=n_{l}\geq 3n_{0}~{}\forall l\in{\mathbb{L}}\setminus\{1\}$
    |'
  prefs: []
  type: TYPE_TB
- en: Upper bounds
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The upper bounds on the maximum number of linear regions are obtained by primarily
    considering changes to the geometry of the linear regions from one layer to another,
    as previously outlined and revisited below. We present those with a close form
    by order of refinement in Table [3](#S3.T3 "Table 3 ‣ Upper bounds ‣ 3.4.1 Analytical
    results ‣ 3.4 The number of linear regions ‣ 3 The Linear Regions of a Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Pascanu et al. ([2014](#bib.bib243)) established the connection between linear
    regions and hyperplane arrangements, which lead to the tight bound for shallow
    networks based on Zaslavsky ([1975](#bib.bib352)) for activation hyperplanes in
    general position. Montúfar et al. ([2014](#bib.bib224)) defined the first bound
    for deep networks based on enumerating all activation sets. The subsequent upper
    bounds extended the result by Pascanu et al. ([2014](#bib.bib243)) to deep networks
    by considering its successive application through the sequence of layers of the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of *deep* networks, where $L>1$, we need to consider how the linear
    regions defined up to a given layer of the network can be further partitioned
    by the next layers. We start by assuming that every linear region defined by the
    first $l-1$ layers is then subdivided into the maximum possible number of linear
    regions defined by the activation hyperplanes of layer $l$. That leads to the
    bound in Raghu et al. ([2017](#bib.bib253)), which is implicit in their proof
    of an asymptotic bound of $O(n^{n_{0}L})$, where $n$ is used as the width of every
    layer. However, there are many ways in which this bound can be refined upon careful
    examination. First, the dimension of the input of layer $l$ —i.e., the output
    of layer $l-1$— within each linear region is never larger than the smallest dimension
    among layers $1$ to $l$, since for every linear region we have an affine transformation
    between inputs and outputs of each layer (Montúfar, [2017](#bib.bib223)). Second,
    the dimension of the input coming through each linear region is in fact bounded
    by the smallest number of active units in each of the previous layers (Serra et al.,
    [2018](#bib.bib282)). This leads to a tight upper bound for $n_{0}=1$, since it
    matches the lower bound in  Serra et al. ([2018](#bib.bib282)). Finally, the activation
    hyperplane of some units may not partition the linear regions because all possible
    inputs to the unit are in the same half-space, and in some of those cases the
    unit may never produce a positive output. For the number $k$ of active units in
    a given layer $l$, we can use the network parameters to calculate the maximum
    number of units that can be active in the next layer, $\mathcal{A}_{l}(k)$, as
    well as the number of units that can be active or inactive for different inputs,
    $\mathcal{I}_{l}(k)$ (Serra and Ramalingam, [2020](#bib.bib281)).
  prefs: []
  type: TYPE_NORMAL
- en: Hinz and van de Geer ([2019](#bib.bib150)) observed that the upper bound by
    Serra et al. ([2018](#bib.bib282)) can be tightened by explicitly computing a
    recursive histogram of linear regions on the layers of the neural network according
    to the dimension of their image subspace. However, the resulting bound is not
    explicitly defined in terms of the network hyperparameters, and hence cannot be
    included on the table. This work is further extended in Hinz ([2021](#bib.bib149))
    by also allowing a composition of bounds on subnetworks instead of only on the
    sequence of layers. Another extension of the framework from Hinz and van de Geer
    ([2019](#bib.bib150)) by Xie et al. ([2020c](#bib.bib344)) highlights that residual
    connections prevent the bottleneck effect in ResNets, by which reason such networks
    tend to have more linear regions.
  prefs: []
  type: TYPE_NORMAL
- en: Cai et al. ([2023](#bib.bib46)) proposed a separate recursive bound based on
    Serra et al. ([2018](#bib.bib282)) to account for the sparsity of the weight matrices,
    which illustrates how pruning connections may affect the maximum number of linear
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results above have also been extended to other architectures. In some cases,
    results on other types of activations are also part of the papers previously mentioned:
    Montúfar et al. ([2014](#bib.bib224)) and Serra et al. ([2018](#bib.bib282)) present
    upper bounds for *maxout* networks; Raghu et al. ([2017](#bib.bib253)) present
    an upper bound for networks using *hard tanh* activation. In other cases, the
    ideas discussed above have been adapted for sparser networks with parameter sharing:
    Xiong et al. ([2020](#bib.bib345)) present upper and lower bounds for convolutional
    networks, which are shown to asymptotically define more linear regions per parameter
    than rectifier networks with the same input size and number of layers. Chen et al.
    ([2022a](#bib.bib50)) present upper and lower bounds for graph convolutional networks.
    Matoba et al. ([2022](#bib.bib214)) discuss the expresiveness of the maxpooling
    layers typically used in convolutional neural networks through their equivalence
    to a sequence of rectifier layers. Moreover, Goujon et al. ([2022](#bib.bib128))
    present results for recently proposed activation functions, such as DeepSpline (Agostinelli
    et al., [2015](#bib.bib2), Unser, [2019](#bib.bib314), Bohra et al., [2020](#bib.bib34))
    and GroupSort (Anil et al., [2019](#bib.bib6)).'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the results above were also revisited through the lenses of tropical
    algebra, in which every linear region corresponds to a tropical hypersurface (Zhang
    et al., [2018b](#bib.bib357), Charisopoulos and Maragos, [2018](#bib.bib48), Maragos
    et al., [2021](#bib.bib212)). Notably, Montúfar et al. ([2022](#bib.bib225)) presented
    considerably tighter upper bounds for the number of linear regions in maxout networks
    with rank $k=3$ or greater.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, a converse line of work started exploring the minimum dimensions of
    a neural network capable of representing a given piecewise linear function, starting
    with considerations about the minimum depth necessary (Arora et al., [2018](#bib.bib8))
    and further refinements of bounds on the network dimensions (He et al., [2020](#bib.bib142),
    Hertrich et al., [2021](#bib.bib147), Chen et al., [2022b](#bib.bib51)), with
    Chen et al. ([2022b](#bib.bib51)) proposing an algorithm that can construct such
    a neural network. On a related note, Karg and Lucia ([2020](#bib.bib168)) show
    that linear time-invariant systems in model predictive control can be exactly
    expressed by rectifier networks and provide bounds on the width and number of
    layers necessary for a given system, whereas Ferlez and Shoukry ([2020](#bib.bib103))
    describe an algorithm for producing architectures that can be parameterized as
    an optimal model predictive control strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Upper bounds on the maximum number of linear regions defined by a
    neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Bound and conditions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  Pascanu et al. ([2014](#bib.bib243)) | $\sum\limits_{i=0}^{n_{0}}\binom{n_{1}}{n_{0}}$
    for shallow networks, $n_{1}\geq n_{0}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  Montúfar et al. ([2014](#bib.bib224)) | $2^{\sum\limits_{l=1}^{L}n_{l}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  Raghu et al. ([2017](#bib.bib253)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{n_{l-1}}\binom{n_{l}}{j}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  Montúfar ([2017](#bib.bib223)) | $\prod\limits_{l=1}^{L}\sum\limits_{j=0}^{d_{l}}\binom{n_{l}}{j}$,
    $d_{l}=\min\{n_{0},n_{1},\ldots,n_{l-1}\}$ |'
  prefs: []
  type: TYPE_TB
- en: '|  Serra et al. ([2018](#bib.bib282)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{n_{l}}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l}~{}\forall l\in{\mathbb{L}}\},\\ d_{l}=\min\{n_{0},n_{1}-j_{1},\ldots,n_{l-1}-j_{l-1},n_{l}\}\end{array}$
    |'
  prefs: []
  type: TYPE_TB
- en: '|  Serra and Ramalingam ([2020](#bib.bib281)) | $\begin{array}[]{r}\sum\limits_{(j_{1},\ldots,j_{L})\in
    J}\prod\limits_{l=1}^{L}\binom{\mathcal{I}_{l}(k_{l-1})}{j_{l}},J=\{(j_{1},\ldots,j_{L})\in\mathbb{Z}^{L}:0\leq
    j_{l}\leq d_{l},\\ d_{l}=\min\{n_{0},k_{1},\ldots,k_{l-1},\mathcal{I}_{l}(k_{l-1})\},k_{0}=n_{0},k_{l}=\mathcal{A}_{l}(k_{l-1})-j_{l-1}~{}\forall
    l\in{\mathbb{L}}\}\end{array}$ |'
  prefs: []
  type: TYPE_TB
- en: Expected number
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The third analytical approach has been the evaluation of the expected number
    of linear regions. In a pair of papers, Hanin and Rolnick studied the number of
    linear regions based on how the network parameters are typically initialized.
    In the first paper (Hanin and Rolnick, [2019a](#bib.bib137)), they show that the
    average number of linear regions along 1-dimensional subspaces of the input grows
    linearly with respect to the number of neurons, irrespective of the network depth.
    In the second paper (Hanin and Rolnick, [2019b](#bib.bib138)), they show that
    the average number of linear regions in higher-dimensional subspaces of the input
    also grows similarly in deep and shallow networks. For $N=\sum_{i=1}^{L}n_{i}$
    as the total number of linear regions, the expected number of linear regions is
    $O(2^{N})$ if $N\leq n_{0}$ and $O\left(\frac{(TN)^{n_{0}}}{n_{0}!}\right)$ otherwise,
    where $T>0$ is a constant based on the network parameters. Moreover, some of their
    experiments suggest that the number of linear regions in shallow networks is slightly
    greater. According to the authors, these bounds reflect the fact that the family
    of functions that can be represented by neural networks in the way that they are
    typically initialized is considerably smaller. They further argue that training
    as currently performed is unlikely to expand the family of functions much further,
    as illustrated by their experiments. Similar results on the expected number of
    linear regions for maxout networks are presented by Tseran and Montúfar ([2021](#bib.bib313)),
    and an application of the results above results to data manifolds is explored
    by Tiwari and Konidaris ([2022](#bib.bib307)). Additional results for specific
    architectures of rectifier networks are conjectured by Wang ([2022](#bib.bib328)),
    although without proof.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Counting linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Counting the actual number of linear regions of a given network has been a
    more challenging topic to explore. Serra et al. ([2018](#bib.bib282)) have shown
    that the linear regions of a trained network can be enumerated as the solutions
    of an MILP formulation, which has been slightly corrected in Cai et al. ([2023](#bib.bib46))¹¹1The
    MILP formulation of neural networks is discussed in Section [4](#S4 "4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey").. However, MILP solutions are generally counted one by one (Danna et al.,
    [2007](#bib.bib72)), with exception of special cases (Serra and Hooker, [2020](#bib.bib280))
    and small subproblems (Serra, [2020](#bib.bib279)), which makes this approach
    impractical for large neural networks. Serra and Ramalingam ([2020](#bib.bib281))
    have shown that approximate model counting methods, which are commonly used to
    count the number of feasible assignments in propositional satisfiability, can
    be easily adapted to solution counting in MILP, which leads to an order-of-magnitude
    speedup in comparison with exact counting. This type of approach is particularly
    suitable for obtaining probabilistic lower bounds, which can complement the analytical
    upper bounds for the maximum number of linear regions. In Craighero et al. ([2020a](#bib.bib64))
    and Craighero et al. ([2020b](#bib.bib65)), a directed acyclic graph is used to
    model the sets of active neurons on each layer and how they connected with those
    in subsequent layers. Yang et al. ([2020](#bib.bib348)) describe a method for
    decomposing the input space of rectifier networks into their linear regions by
    representing each linear region in terms of its face lattice, upon which the splitting
    operations corresponding to the transformations performed by each layer can be
    implemented. As the number of linear regions grow, these splitting operations
    can be processed in parallel. Yang et al. ([2021](#bib.bib349)) extend that method
    to convolutional neural networks. Moreover, Wang ([2022](#bib.bib328)) describes
    an algorithm for enumerating linear regions that counts adjacent linear regions
    with same corresponding affine function as a single linear region.'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to enumerate the linear regions in subspaces, which limits
    their number and reduces the complexity of the task. This idea was first explored
    by Novak et al. ([2018](#bib.bib235)) for measuring the complexity of a neural
    network in terms of the number of transitions along a single line. Hanin and Rolnick
    ([2019a](#bib.bib137), [b](#bib.bib138)) also use this method with a bounded line
    segment or rectangle as a single set representing the input and then sequentially
    partitioning it. If this first set is intersected by the activation hyperplane
    of a neuron in the first layer, then we replace this set by two sets corresponding
    to the parts of the input space in which that neuron is active and not. Once those
    sets are further subdivided by all activation hyperplanes associated with the
    neurons in the first layer, the process can be continued with the neurons in the
    following layers. This method is used to count the number of linear regions along
    subspaces of the input with dimension 1 in Hanin and Rolnick ([2019a](#bib.bib137))
    and dimension 2 in Hanin and Rolnick ([2019b](#bib.bib138)). A generalized version
    for counting the number of linear regions in affine subspaces spanned by a set
    of samples using an MILP formulation is presented in Cai et al. ([2023](#bib.bib46)).
    An approximate approach for counting the number of linear regions along a line
    by computing the closest activation hyperplane in each layer is presented in Gamba
    et al. ([2022](#bib.bib114)).
  prefs: []
  type: TYPE_NORMAL
- en: Other approaches have obtained lower bounds on the number of linear regions
    of a trained network by limiting the enumeration or considering exclusively the
    inputs from the dataset. In Xiong et al. ([2020](#bib.bib345)), the number of
    linear regions is estimated by sampling points from the input space and enumerating
    all activation patterns identified through this process. In Cohan et al. ([2022](#bib.bib61)),
    the counting is restricted to the linear regions found between consecutive states
    of a neural network modeling a reinforcement learning policy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Applications and insights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thinking about neural networks in terms of linear regions led to a variety of
    applications. In turn, that inspired further studies on the structure and properties
    of linear regions under different settings. We organize the literature about applications
    and insights around some central themes in the subsections below.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 The number of linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From our discussion, the number of linear regions emerges as a potential proxy
    for the complexity of neural networks, which has been studied by some authors
    and exploited empirically by others. Novak et al. ([2018](#bib.bib235)) observed
    that the number of transitions between linear regions in 1-dimensional subspaces
    correlates with generalization. Hu et al. ([2020a](#bib.bib154)) used bounds on
    the number of linear regions as proxy to model the capacity of a neural network
    used for learning through distillation, in which a smaller network is trained
    based on the outputs of another network. Chen et al. ([2021a](#bib.bib54)) and
    Chen et al. ([2021b](#bib.bib55)) present one of the first approaches to training-free
    neural architectural search through the analysis of network properties. One of
    the two metrics that they have shown to be effective for that purpose is the number
    of linear regions associated with a sample of inputs from the training set on
    randomly initialized networks. Biau et al. ([2021](#bib.bib30)) observed that
    obtaining a discriminator network for Wasserstein GANs (Arjovsky et al., [2017](#bib.bib7))
    that correctly approximates the Wasserstein distance entails that such a discriminator
    network has a growing number of linear regions as the complexity of the data distribution
    increases. Park et al. ([2021b](#bib.bib242)) maximized the number of linear regions
    in unsupervised learning in order to produce more expressive encodings for downstream
    tasks using simpler classifiers. In neural networks modeling reinforcement learning
    policies, Cohan et al. ([2022](#bib.bib61)) observed that the number of transitions
    between linear regions in inputs corresponding to consecutive states increases
    by 50% with training while the number of repeated linear regions decreases. Cai
    et al. ([2023](#bib.bib46)) proposed a method for pruning different proportions
    of parameters from each layer by maximizes the bound on the number of linear regions,
    which lead to better accuracy than uniform pruning across layers. On a related
    note, Liang and Xu ([2021](#bib.bib192)) proposed a new variant of the ReLU activation
    function for dividing the input space into a greater number of linear regions.
  prefs: []
  type: TYPE_NORMAL
- en: The number of linear regions also inspired further theoretical work. Amrami
    and Goldberg ([2021](#bib.bib3)) presented an argument for the benefit of depth
    in neural networks based on the number of linear regions for correctly separating
    samples associated with different classes. Liu and Liang ([2021](#bib.bib196))
    studied upper and lower bounds on the optimal approximation error of a convex
    univariate function based on the number of linear regions of a rectifier network.
    Henriksen et al. ([2022](#bib.bib146)) used the maximum number of linear regions
    as a metric for capacity that may limit repairing incorrect classifications in
    a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 The shape of linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some studies aimed at understanding what affects the shape of linear regions
    in practice, including how to train neural networks in such a way to induce certain
    shapes in the linear regions. Zhang and Wu ([2020](#bib.bib359)) observed that
    multiple training techniques may lead to similar accuracy, but very different
    shape for the linear regions. For example, batch normalization (Ioffe and Szegedy,
    [2015](#bib.bib162)) and dropout (Srivastava et al., [2014](#bib.bib294)) lead
    to more linear regions. While batch normalization breaks the space in regions
    with uniform size, more orthogonal norms, and more gradient variability across
    adjacent regions; dropout produces more linear regions around decision boundaries,
    norms are more parallel, and data points less likely to be in the region containing
    the decision boundary. Croce et al. ([2019](#bib.bib67)) and Lee et al. ([2019](#bib.bib188))
    applied regularization to the loss function to push the boundary of each linear
    region away from points in the training set that it contains, as long as those
    points are correctly classified. They show that this form of regularization improves
    the robustness of the neural network while making the linear regions larger. In
    fact, Zhu et al. ([2020](#bib.bib362)) observed that the boundaries of the linear
    regions move away from the training data; and He et al. ([2021](#bib.bib141))
    conjectured that the linear regions near training samples becomes smaller through
    training, or that conversely the activation patterns are denser around the training
    samples. Gamba et al. ([2020](#bib.bib113)) presented an empirical study on the
    angles between activation hyperplanes defined by convolutional layers, and observed
    that their cosines tend to be similar and more negative with depth after training.
  prefs: []
  type: TYPE_NORMAL
- en: The geometry of linear regions also led to other theoretical and algorithmic
    advances. Theoretically, Phuong and Lampert ([2020](#bib.bib247)) proved that
    architectures with nonincreasing layer widths have unique parameters —upon permutation
    and scaling— for representing certain functions. In other words, some pairs of
    neural networks are only equivalent if their parameters only differ by permutation
    and multiplication. Grigsby et al. ([2023](#bib.bib132)) showed that equivalences
    other than by permutation are less likely to occur with greater input size and
    width, but more likely with greater depth. Algorithmically, Rolnick and Kording
    ([2020](#bib.bib260)) proposed a procedure to reconstruct a neural network by
    evaluating several inputs in order to determine regions of the input space for
    which the output of the neural network can be defined by an affine function —and
    thus consist of a single linear region. Depending on how the shape changes between
    adjacent linear regions, the boundaries of the linear regions are replicated with
    neurons in the first hidden layer or in subsequent layers of the reconstructed
    neural network. Masden ([2022](#bib.bib213)) provided theoretical results and
    an algorithm for characterizing the face lattice of the polyhedron associated
    with each linear region.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Activation patterns and the discrimination of inputs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another common theme is understanding how inputs from the training and test
    sets are distributed among the linear regions, and what can be inferred the encoding
    of the activation patterns associated with the linear regions. Gopinath et al.
    ([2019](#bib.bib127)) noted that many properties of neural networks, including
    the classes of different inputs, are associated with activation patterns —and
    thus with their linear regions. Several works (He et al., [2021](#bib.bib141),
    Sattelberg et al., [2020](#bib.bib271), Trimmel et al., [2021](#bib.bib310)) observed
    that each training sample is typically located in a different linear region when
    the neural network is sufficiently expressive; whereas He et al. ([2021](#bib.bib141))
    noted that simple machine learning algorithms can be applied using the activation
    patterns as features, and Sattelberg et al. ([2020](#bib.bib271)) noted that there
    is some similarity between activation patterns of different neural networks under
    affine mapping, meaning that the training of these neural networks lead to similar
    models. Chaudhry et al. ([2020](#bib.bib49)) exploited the idea of continual learning
    with different tasks being encoded in disjoint subspaces, which thus corresponds
    to a disjoint set of activation sets on each layer being associated with classifications
    for each of those tasks. Based on their approach for enumerating linear regions,
    Craighero et al. ([2020a](#bib.bib64)) and Craighero et al. ([2020b](#bib.bib65))
    have found that inputs from larger linear regions are often correctly classified
    by the neural network, that inputs from smaller linear regions are often incorrectly
    classified, and that the number of distinct activations sets reduces along the
    layers of the neural network. Gamba et al. ([2022](#bib.bib114)) also discussed
    the issue of some linear regions being smaller and thus less likely to occur in
    practice. Moreover, they propose a measurement for the similarity of the affine
    functions associated with linear regions along a line and observed that the linear
    regions tend to be less similar to one another when the network is trained with
    incorrectly classified labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.4 Function approximation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because of the linear behavior of the output within each linear region, we can
    approximate the output of the neural network based on the output of its linear
    regions. Chu et al. ([2018](#bib.bib59)) and Sudjianto et al. ([2020](#bib.bib297))
    produced linear models based on this local behavior; whereas Glass et al. ([2021](#bib.bib118))
    observed that we can interpret neural networks as equivalent to local linear model
    trees (Nelles et al., [2000](#bib.bib230)), in which a distinct linear model is
    used at each leaf of a decision tree, and provided a method to produce such models
    from neural networks. Trimmel et al. ([2021](#bib.bib310)) described how to extract
    the linear regions associated with the inputs from the training set as means to
    approximate the output of the inputs from the test set. Robinson et al. ([2019](#bib.bib259))
    presented another approach for explicitly representing the function modeled by
    a neural network through the enumeration of its linear regions. On a related note,
    Chaudhry et al. ([2020](#bib.bib49)) used the assumption of training samples remaining
    within the same linear region during gradient descent to simplify the analysis
    of backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: This topic also relates to the broad literature on neural networks as universal
    function approximators, to which the concept of linear regions helps articulating
    ideal conditions. As observed by Mhaskar and Poggio ([2020](#bib.bib218)), the
    optimal number of linear regions in a neural network —or, correspondingly, of
    pieces of the piecewise linear function modeled by it— depends on the function
    being approximated. In addition, linear regions were also used explicitly to build
    function approximations. Kumar et al. ([2019](#bib.bib181)) have shown that rectifier
    networks can we approximated to arbitrary precision with two hidden layers, the
    largest of which having a neuron corresponding to each different activation pattern
    of the original network; an exact counterpart of this result was later presented
    by Villani and Schoots ([2023](#bib.bib319)). Fan et al. ([2020](#bib.bib99))
    described the transformation between sufficiently wide and deep networks while
    arguing that the fundamental measure of complexity should be counting simplices
    within linear regions. In subsequent work, Fan et al. ([2023](#bib.bib100)) empirically
    observed that linear regions tend to have a small number of higher dimensional
    faces, or facets.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recent studies aimed at understanding the expressiveness and approximability
    of neural networks in terms of their number of parameters, in particular when
    the number of linear regions is greater than the number of parameters (Malach
    and Shalev-Shwartz, [2019](#bib.bib208), Dym et al., [2020](#bib.bib86), Daubechies
    et al., [2022](#bib.bib76)). They all discuss how the composition the modeled
    functions tend to present the self-similarity property of fractal distributions,
    which is one reason why they have so many linear regions. Keup and Helias ([2022](#bib.bib172))
    interpreted the connection between linear regions in different parts of the input
    space in terms of how paper origamis are constructed: by “folding” the data for
    separability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another related topic is computing the Lipschitz constant $\rho$ of the function
    $f(x)$ modeled by the neural network, the smallest $\rho$ for which $\|f(x^{\prime})-f(x)\|\leq\rho\|x^{\prime}-x\|$
    for any two inputs $x$ and $x^{\prime}$. Note that the first derivative of the
    output of a linear region is constant, which is leveraged by Hwang and Heinecke
    ([2020](#bib.bib160)) to evaluate the stability of the network by computing the
    constant across linear regions by changing the activation pattern. Interestingly,
    Zhou and Schoellig ([2019](#bib.bib361)) showed that the constant grows similarly
    to the number of linear regions: polynomial in width and exponential in depth.
    A smaller constant limits the susceptibility of the network to adversarial examples (Huster
    et al., [2018](#bib.bib159)), which are discussed in Section [4](#S4 "4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey"), and also lead to smaller bias variance (Loukas et al., [2021](#bib.bib201)).
    While calculating the exact Lipschitz constant is NP-hard and encourages approximations
    (Virmaux and Scaman, [2018](#bib.bib322), Patrick L. Combettes, [2019](#bib.bib244)),
    the exact constant can be computed using MILP (Jordan and Dimakis, [2020](#bib.bib166)).
    Notably, many studies have focused on relaxations such as linear programming (Zou
    et al., [2019](#bib.bib363)), semidefinite programming (Fazlyab et al., [2019](#bib.bib101),
    Chen et al., [2020](#bib.bib53)), and polynomial optimization (Latorre et al.,
    [2020](#bib.bib184)). An alternative approach is to use more sophisticated activation
    functions for limiting the value of the constant (Anil et al., [2019](#bib.bib6),
    Aziznejad et al., [2020](#bib.bib9)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.5 Optimizing over linear regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As an alternative to optimizing over neural networks as described next in Section [4](#S4
    "4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), a number of approaches have resorted to techniques that are
    equivalent to systematically enumerating or traversing linear regions and optimizing
    over them (Croce and Hein, [2018](#bib.bib66), Croce et al., [2020](#bib.bib68),
    Khedr et al., [2020](#bib.bib174), Vincent and Schwager, [2021](#bib.bib320),
    Xu et al., [2022](#bib.bib346)). Notably, Vincent and Schwager ([2021](#bib.bib320))
    and Xu et al. ([2022](#bib.bib346)) are mindful of the facet-defining inequalities
    associated with a linear region, which are the ones to change when moving toward
    an adjacent linear region. On a related note, Seck et al. ([2021](#bib.bib278))
    alternates between gradient steps and solving a linear programming model within
    the current linear region.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Optimizing Over a Trained Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [5](#S5 "5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") we will see how polyhedral-based
    methods can be used to *train* a neural network. In this section, we will focus
    on how polyhedral-based methods can be used to do something with a neural network
    *after it has been trained.* Specifically, after the network architecture and
    all parameters have been fixed, a neural network $f$ is merely a function. If
    each activation function $\sigma$ used to describe the network is piecewise linear
    (as is the case with those presented in Table [1](#S1.T1 "Table 1 ‣ 1.3 Why nonlinearity
    is important in artificial neurons ‣ 1 Introduction ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")), $f$ is also a piecewise linear function. Therefore,
    any optimization problem containing $f$ in some way will be a piecewise linear
    optimization problem. For example, in the simple case where the output of $f$
    is univariate, the optimization problem'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{x\in\mathcal{X}}f(x)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'is a piecewise linear optimization problem. As discussed in Section [3](#S3
    "3 The Linear Regions of a Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), this problem can have an enormous number of “pieces” (linear
    regions) when $f$ is a neural network; solving this problem thus heavily depends
    on the size and structure of the neural network $f$. For example, the training
    procedure by which $f$ is obtained can greatly influence the performance of optimization
    strategies (Tjeng et al., [2019](#bib.bib309), Xiao et al., [2019](#bib.bib341)).'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we first explore situations in which you might want to optimize
    over a trained neural network in this manner. We will then survey available methods
    for solving this method (either exactly or on the dual side) using polyhedral-based
    methods. We conclude with a brief view of future directions.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Applications of optimization over trained networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Applications where you might want to optimize over a trained neural network
    $f$ broadly fall into two categories: those where $f$ is the “true” object of
    interest, and those where $f$ is a convenient proxy for some unknown, underlying
    behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Neural network verification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Neural network verification is a burgeoning field of study in deep learning.
    Starting in the early 2000s, researchers began to recognize the importance of
    rigorously verifying the behavior of neural networks, mainly in aviation-related
    applications (Schumann et al., [2003](#bib.bib274), Zakrzewski, [2001](#bib.bib351)).
    More recently, the seminal works of Szegedy et al. ([2014](#bib.bib300)) and Goodfellow
    et al. ([2015](#bib.bib124)) observed that neural networks are unusually susceptible
    to *adversarial attacks*. These are small, targeted perturbations that can drastically
    affect the output of the network; as shown in Figure [8](#S4.F8 "Figure 8 ‣ 4.1.1
    Neural network verification ‣ 4.1 Applications of optimization over trained networks
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"), even powerful models such as MobileNetV2 (Sandler et al.,
    [2018](#bib.bib270)) are susceptible. The existence and prevalence of adversarial
    attacks in deep neural networks has raised justifiable concerns about the deployment
    of these models in mission-critical systems such as autonomous vehicles (Deng
    et al., [2020](#bib.bib79)), aviation (Kouvaros et al., [2021](#bib.bib177)),
    or medical systems (Finlayson et al., [2019](#bib.bib105)). One fascinating empirical
    work by Eykholt et al. ([2018](#bib.bib98)) showed the susceptibility of standard
    image classification networks that might be used in self-driving vehicles to a
    very analogue form of attacks: black/white stickers, placed in a careful way,
    could confuse these models enough that they would mis-classify road signs (e.g.,
    mistaking stop signs for “speed limit 80” signs).'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F8.pic1" class="ltx_picture ltx_centering" height="130.22" overflow="visible"
    version="1.1" width="407.44"><g transform="translate(0,130.22) matrix(1 0 0 -1
    0 0) translate(54.11,0) translate(0,65.11)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g
    transform="matrix(1.0 0.0 0.0 1.0 -49.5 -60.5)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="121" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/eb8e031e8c492fc9ac38e777fd76ac60.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 67.05 -16.79)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="13.28" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">+</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 100.11 -59.74)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/6ec4f02d882565048e57d58d5b3977d5.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 216.66 -15.47)" fill="#000000" stroke="#000000"><foreignobject width="15.5"
    height="7.31" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">=</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 249.71 -60.5)" fill="#000000" stroke="#000000"><foreignobject
    width="99" height="121" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">![Refer
    to caption](img/1df2f6052ecb61a06e1057f75e5d6880.png)</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 111.82 43.78)" fill="#000000" stroke="#000000"><foreignobject width="75.57"
    height="13.84" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\times(\epsilon=0.15)$</foreignobject></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Example of adversarial attack on MobileNetV2 (Sandler et al., [2018](#bib.bib270)).
    The original image taken by one of the survey authors is classified as ‘siberian_husky,’
    but is re-classified as ‘wallaby’ with a small (in an $\ell_{\infty}$-norm sense)
    targeted attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Neural network verification seeks to prove (or disprove) a given input-output
    relationship, i.e., $x\in\mathcal{X}\Rightarrow y\in\mathcal{Y}$, that gives some
    indication of model robustness. Methods for verifying this relationship are classified
    as being sound and/or complete. A method that is sound will only certify the relationship
    if it is indeed true (no false positives), while a method that is complete will
    (i) always return an answer and (ii) only disprove the relationship if it is false
    (no false negatives). An early set of papers (Fischetti and Jo, [2018](#bib.bib106),
    Lomuscio and Maganti, [2017](#bib.bib200), Tjeng et al., [2019](#bib.bib309))
    recognized that MILP provides an avenue for verification that is both sound and
    complete, given that $\mathcal{X}$ and $f(x)$ are both linear, or piecewise linear.
    We refer the readers to recent reviews (Huang et al., [2020](#bib.bib157), Leofante
    et al., [2018](#bib.bib190), Li et al., [2022](#bib.bib191), Liu et al., [2021](#bib.bib197))
    for a more comprehensive treatment of the landscape of verification methods, including
    MILP- and LP-based technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Consider a classification network $f:[0,1]^{n_{0}}\to\mathbb{R}^{d}$ where the
    $j$-th output, $f(x)_{j}$, corresponds to the probability that input $x$ is of
    class $j$.²²2In actuality, we will instead typically work with the outputs corresponding
    to “logits”, or unnormalized probabilities. These are typically fed into a softmax
    layer that then normalize these values to correspond to a probability distribution
    over the classes. However, this nonlinear softmax transformation is not piecewise
    linear. Thankfully, it can be omitted in the context of the verification task
    without loss of generality. Then consider a labeled image $\hat{x}$ known to be
    of class $i$, and a “target” adversarial class $k\neq i$. Then verifying local
    robustness of the prediction corresponds to checking $x\in\{x:||x-\hat{x}||\leq\epsilon\}\Rightarrow
    y=f(x)\in\{y:y_{i}\geq y_{k}\}$, where $\epsilon>0$ is a constant which prescribes
    the radius around which $\hat{x}$ we will search for an adversarial example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This verification task can be formulated as an optimization problem of the
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{x\in[0,1]^{n_{0}}}$ | $\displaystyle f(x)_{k}-f(x)_{i}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Any feasible solution $x$ to this problem with positive cost is an adversarial
    example: it is very “close” to $\hat{x}$ which has true label $i$, yet the network
    believes it is more likely to be of class $k$.³³3Alternative objectives are sometimes
    used which would allow us to strengthen this statement to say that the network
    *will* classify $x$ to be of class $k$. However, this will require a more complex
    reformulation to model this problem via MILP, so we omit it for simplicity. If,
    on the other hand, it is proven that the optimal objective value is negative,
    this proves that $f$ is robust (at least in the neighborhood around $\tilde{x}$).
    Note that the verification problem can terminate once the sign of the optimal
    objective value is determined, but solving the problem returns an optimal adversarial
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function of ([3](#S4.E3 "In Example 4 ‣ 4.1.1 Neural network
    verification ‣ 4.1 Applications of optimization over trained networks ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")) models the desired input-output relationship, $x\in\mathcal{X}\Rightarrow
    y\in\mathcal{Y}$, while the constraints model the domain $\mathcal{X}$. The domain
    $\mathcal{X}$ is typically a box or hyperrectangular domain. Extensions to this
    are described in Section [4.4.1](#S4.SS4.SSS1 "4.4.1 Extending to other domains
    ‣ 4.4 Generalizing the single neuron model ‣ 4 Optimizing Over a Trained Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey"). Some explanation-focused
    verification applications define the input-output relationship in a derivative
    sense, e.g., $x\in\mathcal{X}\Rightarrow\partial y/\partial x\in\mathcal{Y}^{\prime}$
    (Wicker et al., [2022](#bib.bib333)). As the derivative of the ReLU function is
    also piecewise linear, this class of problems can also be modeled in MILP. For
    example, in the context of fairness and explainability, Liu et al. ([2020](#bib.bib198))
    and Jordan and Dimakis ([2020](#bib.bib166)) used MILP to certify monotonicity
    and to compute local Lipschitz constants, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although in this survey we focus on optimization over trained neural networks,
    it is important to note that polyhedral theory underlies numerous strategies for
    neural network verification. For example, SAT and SMT (Satisfiability Modulo Theories)
    solvers designed for Boolean satisfiability problems (and more general problems
    for the case of SMT) can also be used to search through activation patterns for
    a neural network (Pulina and Tacchella, [2010](#bib.bib251)), resulting in tools
    that are sound and complete, such as Planet (Ehlers, [2017](#bib.bib87)) and Reluplex
    (Katz et al., [2017](#bib.bib169)). Bunel et al. ([2018](#bib.bib43)) presented
    a unified view to compare MILP and SMT formulations, as well as the relaxations
    that result from these formulations (we will revisit this in Section [4.3](#S4.SS3
    "4.3 Scaling further: Convex relaxations and linear programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")). On the other hand, strategies such as ExactReach (Xiang et al., [2017](#bib.bib340))
    exploit polyhedral theory to compute reachable sets: given an input set to a ReLU
    function defined as a union of polytopes, the output reachable set is also a union
    of polytopes. Other methods over-approximate the reachable set to improve scalability,
    e.g., for vision models (Yang et al., [2021](#bib.bib349)), often resulting in
    methods that are sound, but not complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Neural network as proxy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another situation in which you may want to solve an optimization problem containing
    trained neural networks is when you would like to optimize some other, unknown
    function for which you have historical input/output data. A similar situation
    arises when you want to solve an optimization problem where (some of) the constraints
    are overly complicated, but you can query samples from the constraints on which
    to train a simpler surrogate model. In these cases, you might imagine training
    a neural network in a standard supervised learning setting to approximate this
    underlying, unknown or complicated function. Then, since the neural network is
    known, you are left with a deterministic piecewise linear optimization problem.
    Note that we focus here on using a neural network as a surrogate; neural networks
    can additionally learn other components of an optimization problem, e.g., uncertainty
    sets for robust optimization (Goerigk and Kurtz, [2023](#bib.bib122)).
  prefs: []
  type: TYPE_NORMAL
- en: Several software tools have been developed for this class of problems. For the
    case of constraint learning, JANOS (Bergman et al., [2022](#bib.bib24)) and OptiCL
    (Maragno et al., [2021](#bib.bib210)) both provide functionality for learning
    a ReLU neural network to approximate a constraint based on data and embedding
    the learned neural network in MILP. The reluMIP package (Lueg et al., [2021](#bib.bib203))
    has also been introduced to handle the latter embedding step. More generally,
    OMLT (Ceccon et al., [2022](#bib.bib47)) translates neural networks to pyomo optimization
    blocks, including various MILP formulations and activation functions. Finally,
    recent developments in gurobipy⁴⁴4https://github.com/Gurobi/gurobi-machinelearning
    enable directly parsing in trained neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of this paradigm can be envisioned in a number of domain areas.
    This approach is common in deep reinforcement learning, where neural networks
    are used to approximate an unknown “$Q$-function” which models the long-term cost
    of taking a particular action in a particular state of the world. In $Q$-learning,
    this $Q$-function is optimized iteratively to produce new candidate policies,
    which are then evaluated (typically via simulation) to produce new training data
    for future iterations. Optimization over the learned $Q$-function must be relatively
    fast in control applications, and several practical methods have been proposed.
    When the action space is discrete, the $Q$-function neural network is trained
    with one output value for each possible action, simplifying optimization to evaluating
    the model and selecting the largest output. Continuous action spaces require the
    $Q$ network be optimized over (Burtea and Tsay, [2023](#bib.bib45), Delarue et al.,
    [2020](#bib.bib78), Ryu et al., [2020](#bib.bib267)), or an “actor” network can
    be trained to learn the optimal actions (Lillicrap et al., [2015](#bib.bib193)).
    In a related vein, ReLU neural networks can be used as a process model for optimal
    scheduling or control (Wu et al., [2020](#bib.bib338)).
  prefs: []
  type: TYPE_NORMAL
- en: Chemical engineering also presents applications where surrogate models have
    proven beneficial for optimization, as is the subject of recent reviews (Bhosekar
    and Ierapetritou, [2018](#bib.bib28), McBride and Sundmacher, [2019](#bib.bib216),
    Tsay and Baldea, [2019](#bib.bib311)). In particular, ReLU neural networks can
    be seamlessly embedded in larger MILP problems such as flow networks and reservoir
    control where the other constraints are also mixed-integer linear (Grimstad and
    Andersson, [2019](#bib.bib133), Say et al., [2017](#bib.bib272), Yang et al.,
    [2022](#bib.bib347)). Focusing on control applications where the neural network
    is embedded in a MILP that must be solved repeatedly, Katz et al. ([2020](#bib.bib171))
    showed how multiparametric programming can be used to learn the solution map of
    the resulting MILP itself, which is also piecewise affine. An emerging area of
    research uses verification tools to reason about neural networks used as controllers,
    e.g., see Johnson et al. ([2020](#bib.bib165)). These applications involve optimization
    formulations combining the neural network with constraints defining the controlled
    system. For example, verification can be used to bound the reachable set (Sidrane
    et al., [2022](#bib.bib286)) (alongside piecewise linear bounds on the dynamical
    system) or the maximum error against a baseline controller (Schwan et al., [2022](#bib.bib275)).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, applications for optimization over neural networks arise in machine
    learning applications. MILP formulations can be used to compress neural networks
    (Serra et al., [2020](#bib.bib283), [2021](#bib.bib284), ElAraby et al., [2020](#bib.bib88)),
    which consequently result in more tractable surrogate models (Kody et al., [2022](#bib.bib176)).
    The main idea is to use MILP to identify stable nodes, i.e., nodes that are always
    on or off over an input domain, which can then be algebraically eliminated. Optimization
    has also been employed in techniques for feature selection, based on identifying
    strongest input nodes (Sildir and Aydin, [2022](#bib.bib287), Zhao et al., [2023](#bib.bib360)).
    In the context of Bayesian optimization, Volpp et al. ([2020](#bib.bib323)) use
    reinforcement learning to meta-learn acquisition functions parameterized as neural
    networks; selecting ensuing query points then requires optimization over the trained
    acquisition function. Later work modeled both the acquisition function and feasible
    region in black-box optimization as neural networks (Papalexopoulos et al., [2022](#bib.bib239)).
    In that work, exploration and exploitation are balanced via Thompson sampling
    and training multiple neural networks from a random parameter initialization.
  prefs: []
  type: TYPE_NORMAL
- en: A word of caution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Standard supervised learning algorithms aim to learn a function which fits the
    underlying function according to some distribution under which the data is generated.
    However, optimizing a function corresponds to evaluating it at a single point.
    This means that you may end up with a model that well-approximates the underlying
    function in distribution, but for which the pointwise minimizer is a poor approximation
    of the true function. This phenomena is referred to as the “Optimizer’s curse”
    (Smith and Winkler, [2006](#bib.bib293)).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Single neuron relaxations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the following subsections, consider the $i$-th neuron in the $l$-th layer
    of a neural network, endowed with a ReLU activation function, whose behavior is
    governed by ([2](#S1.E2 "In 1.1 What neural networks can model ‣ 1 Introduction
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). Presume that a input
    domain of interest $\mathcal{D}^{l-1}\subset\mathbb{R}^{n_{l}}$ is a bounded region.
    Further, since $\mathcal{D}^{l-1}$ is bounded, presume that finite bounds are
    known on each input component, i.e. that vectors $L^{l-1},U^{l-1}\in\mathbb{R}^{n_{l}}$
    are known such that $\mathcal{D}^{l-1}\subseteq[L^{l-1},U^{l-1}]\subset\mathbb{R}^{n_{l}}$.
    We can then write the *graph* of the neuron, which couples together the input
    and the output of the nonlinear ReLU activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\texttt{gr}=$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}=0\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\cup$ | $\displaystyle\Set{({\bm{h}}^{l-1},h^{l}_{i})\in\mathcal{D}^{l-1}\times\mathbb{R}}{h^{l}_{i}={\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\geq
    0}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This is a disjunctive representation for gr in terms of two polyhedral alternatives.
    We assume that every included neuron exhibits this disjunction, i.e., every neuron
    can be on or off depending on the model input. This assumption of *strict activity*
    implies that $L^{l-1}<0$ and $U^{l-1}>0$, noting that neurons not satisfying this
    property can be exactly pruned from the model (Serra et al., [2020](#bib.bib283)).
  prefs: []
  type: TYPE_NORMAL
- en: We observe that, given this (or any) formulation for each individual unit, it
    is straightforward to construct a formulation for the entire network. For example,
    if we take $X^{l}_{i}=\Set{({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})}{\eqref{eqn:relu-big-m}}$
    for each layer $l$ and each unit $i$, we can construct a MILP formulation for
    the graph of the entire network, $\Set{(x,f(x)):x\in\mathcal{D}^{0}}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $({\bm{h}}^{l-1},h^{l}_{i},z^{l}_{i})\in X^{l}_{i}\quad\forall l\in{\mathbb{L}},i\in\llbracket
    n_{l}\rrbracket.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This also generalizes in a straightforward manner to more complex feedforward
    network architectures (e.g. convolutions, or sparse or skip connections), though
    we omit the explicit description for notational simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4 Beyond the scope of this survey
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The effectiveness of the single-neuron formulations described above is bounded
    by the tightness of the optimal univariate formulation; this property is known
    as the “single-neuron barrier” (Salman et al., [2019](#bib.bib269)). This has
    motivated research in convex relaxations that jointly account for multiple neurons
    within a layer (Singh et al., [2019a](#bib.bib290)). Nevertheless, the analysis
    of polyhedral formulations for multiple neurons simultaneously quickly becomes
    intractable, and is beyond the scope of this survey. Instead, we point the interested
    reader to the recent survey by Roth ([2021](#bib.bib263)), and highlight a few
    approaches taken in the literature. Multi-neuron analysis has been used to: improve
    bounds tightening schemes (Rössig and Petkovic, [2021](#bib.bib262)), prune linearizable
    neurons (Botoeva et al., [2020](#bib.bib37)), design dual decompositions (Ferrari
    et al., [2022](#bib.bib104)), and generate strengthening inequalities (Serra and
    Ramalingam, [2020](#bib.bib281)). Similarly, we do not review formulations for
    ensembles of ReLU networks, though MILP formulations have been proposed (Wang
    et al., [2021](#bib.bib324), [2023](#bib.bib325)).'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, recent works have exploited polyhedral structure to develop sampling
    based strategies, which can be used to warm-start MILP or accelerate local search
    in verification (Perakis and Tsiourvas, [2022](#bib.bib245), Wu et al., [2022](#bib.bib339)).
    Lombardi et al. ([2017](#bib.bib199)) computationally compare MILP against local
    search and constraint programming approaches. In a related vein, Cheon ([2022](#bib.bib58))
    examines local solutions and proposes an outer approximation method to improve
    gradient-based optimization. Finally, following Raghunathan et al. ([2018](#bib.bib254)),
    a large body of work has presented optimization-based methods for verification
    that use semidefinite programming concepts (Dathathri et al., [2020](#bib.bib75),
    Fazlyab et al., [2020](#bib.bib102), Newton and Papachristodoulou, [2021](#bib.bib232)).
    Notably, Batten et al. ([2021](#bib.bib17)) showed how combining semidefinite
    and MILP formulations can produce a new formulation that is tighter than both.
    This was later extended with reformulation-linearization technique, or RLT, cuts
    (Lan et al., [2022](#bib.bib183)). While related to linear programming and other
    methods based on convex relaxations, this stream of work is beyond the scope of
    this survey. We refer the reader to Zhang ([2020](#bib.bib358)) for a discussion
    of the tightness of these formulations.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Exact models using mixed-integer programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mixed-integer programming offers a powerful algorithmic framework for *exactly*
    modeling nonconvex piecewise linear functions. The Operations Research community
    has studied has a long and storied history of developing MILP-based methods for
    piecewise linear optimization, with research spanning decades (Croxton et al.,
    [2003](#bib.bib69), Dantzig, [1960](#bib.bib73), Geißler et al., [2012](#bib.bib117),
    Huchette and Vielma, [2022](#bib.bib158), Lee and Wilson, [2001](#bib.bib189),
    Misener and Floudas, [2012](#bib.bib221), Padberg, [2000](#bib.bib238), Vielma
    et al., [2010](#bib.bib318)). However, many of these techniques are specialized
    for low-dimensional or separable piecewise linear functions. While a reasonable
    assumption in many OR problems, this is not the case when modeling neurons in
    a neural network. Therefore, the standard approach in the literature is to apply
    general-purpose MILP formulation techniques to model neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Connection to Boolean satisfiability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Some SMT-based methods such as Reluplex (Katz et al., [2017](#bib.bib169)) and
    Planet (Ehlers, [2017](#bib.bib87)) effectively construct branching technologies
    similar to MILP solvers. Indeed, Marabou (Katz et al., [2019](#bib.bib170)) builds
    on Reluplex, and a recent extension MarabouOpt can optimize over trained neural
    networks (Strong et al., [2021](#bib.bib295)). The authors also outline general
    procedures to extend verification solvers to optimization. Our focus in this review
    is on more general MILP formulations, or those that can be incorporated into off-the-shelf
    MILP solvers with relative ease. Bunel et al. ([2020b](#bib.bib42), [2018](#bib.bib43))
    provide a more comprehensive discussion of similarities and differences to SMT.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 The big-$M$ formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The big-$M$ method is a standard technique used to formulate logic and disjunctive
    constraints using mixed-integer programming (Bonami et al., [2015](#bib.bib35),
    Vielma, [2015](#bib.bib316)). Big-$M$ formulations are typically very simple to
    reason about and implement, and are quite compact, though their convex relaxations
    can often be quite poor, leading to weak dual bounds and (often) slow convergence
    when passed to a mixed-integer programming solver. Since gr is a disjunctive set,
    the big-$M$ technique can be applied to produce the following formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (4a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\left({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}\right)-M^{l}_{i,-}(1-z)$
    |  | (4b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq M^{l}_{i,+}z$ |  | (4c)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}$ |  | (4d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\}.$ |  | (4e) |'
  prefs: []
  type: TYPE_TB
- en: Here, $M^{l}_{i,-}$ and $M^{l}_{i,+}$ are data which must satisfy the inequalities
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle M^{l}_{i,-}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle M^{l}_{i,+}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This big-$M$ formulation for ReLU-based networks has been used extensively in
    the literature (Bunel et al., [2018](#bib.bib43), Cheng et al., [2017](#bib.bib56),
    Dutta et al., [2018](#bib.bib83), Fischetti and Jo, [2018](#bib.bib106), Kumar
    et al., [2019](#bib.bib181), Lomuscio and Maganti, [2017](#bib.bib200), Serra
    and Ramalingam, [2020](#bib.bib281), Serra et al., [2018](#bib.bib282), Tjeng
    et al., [2019](#bib.bib309), Xiao et al., [2019](#bib.bib341)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The big-$M$ formulation is compact, with one binary variable and $\mathcal{O}(1)$
    general inequality constraints for each neuron. Applied for each unit in the network,
    this leads to a MILP formulation with $\mathcal{O}(\sum_{l\in{\mathbb{L}}}n_{l})=\mathcal{O}(Ln_{\max})$
    binary variables and general inequality constraints, where $n_{\max}=\max_{l\in{\mathbb{L}}}n_{L}$.
    However, it has been observed (Anderson et al., [2019](#bib.bib4), [2020](#bib.bib5))
    that this big-$M$ formulation is not strong in the sense that its LP relaxation
    does not, in general, capture the convex hull of the graph of a given unit; see
    Figure [9](#S4.F9 "Figure 9 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey") for an illustration. In fact,
    this LP relaxation can be arbitrarily bad (Anderson et al., [2019](#bib.bib4),
    Example 2), even in fixed input dimension. As MILP solvers often bound the objective
    function between the best feasible point and its tightest optimal continuous relaxation,
    a weak formulation can negatively impact performance, often substantially.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth dwelling on where this lack of strength comes from. If the input
    ${\bm{h}}^{l-1}$ is one dimensional, the big-$M$ formulation is *locally* ideal
    (Vielma, [2015](#bib.bib316)): the extreme points of the linear programming relaxation
    ([4a](#S4.E4.1 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")-[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) naturally
    satisfy the integrality constraints ([4e](#S4.E4.5 "In 4 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). However,
    this fails to hold in the general multivariate input case. To see why, observe
    that the bounds on the input variables ${\bm{h}}^{l-1}$ are only coupled with
    the logic involving the binary variable $z$ only in an aggregated sense, through
    the coefficients $M^{l}_{i,-}$ and $M^{l}_{i,+}$. In other words, the “shape”
    of the pre-activation domain is not incorporated directly into the big-$M$ formulation.
    Furthermore, the strength of this formulation highly depends on the big-$M$ coefficients.
    These coefficients can be obtained using techniques ranging from basic interval
    arithmetic to optimization-based bounds tightening. Grimstad and Andersson ([2019](#bib.bib133))
    show how constraints external to the neural network can yield tighter bounds via
    optimization- or feasibility-based bounds tightening. Rössig and Petkovic ([2021](#bib.bib262))
    compare several methods for deriving bounds and further develop optimization-based
    bounds tightening based on pairwise dependencies between variables.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S4.F9.pic1" class="ltx_picture ltx_centering ltx_figure_panel" height="135.23"
    overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg><svg
    id="S4.F9.pic2" class="ltx_picture ltx_centering ltx_figure_panel" height="135.23"
    overflow="visible" version="1.1" width="174.39"><g transform="translate(0,135.23)
    matrix(1 0 0 -1 0 0) translate(10.54,0) translate(0,11.91)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 50.6 61.95)" fill="#000000"
    stroke="#000000"><foreignobject width="11.85" height="14.6" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$h^{1}_{2}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 146.62 -4.8)" fill="#000000" stroke="#000000"><foreignobject width="11.85"
    height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{1}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -5.92 106.6)" fill="#000000" stroke="#000000"><foreignobject
    width="11.85" height="14.6" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$h^{2}_{1}$</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Left: The convex hull of a ReLU neuron ([5](#S4.E5 "In 4.2.2 A stronger
    extended formulation ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")), and Right: the convex relaxation offered by the big-$M$ formulation
    ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) Adapted from Anderson et al. Anderson et al.
    ([2020](#bib.bib5), [2019](#bib.bib4))'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 A stronger extended formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A much stronger MILP formulation can be constructed through a classical method,
    the extended formulation for disjunctions (Balas, [1998](#bib.bib11), Jeroslow
    and Lowe, [1984](#bib.bib163)). This formulation for a given ReLU neuron takes
    the following form (Anderson et al., [2019](#bib.bib4), Section 2.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle=(x^{+},y^{+})+(x^{-},y^{-})$
    |  | (5a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq{\bm{w}}^{l}_{i}x^{-}+b^{l}_{i}(1-z)$
    |  | (5b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle={\bm{w}}^{l}_{i}x^{+}+b^{l}_{i}z\geq
    0$ |  | (5c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L^{l-1}(1-z)$ | $\displaystyle\leq x^{-}\leq U^{l-1}(1-z)$
    |  | (5d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L^{l-1}z$ | $\displaystyle\leq x^{+}\leq U^{l-1}z$ |  |
    (5e) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (5f) |'
  prefs: []
  type: TYPE_TB
- en: This formulation requires one binary variable and $\mathcal{O}(n_{l-1})$ general
    linear constraints and auxiliary continuous variables. It is also locally ideal,
    i.e., as strong as possible. While the number of variables and constraints for
    an individual unit seems quite tame, applying this formulation for unit in a network
    leads to a formulation with $\mathcal{O}(n_{0}+\sum_{l\in{\mathbb{L}}}n_{l}n_{l-1})=\mathcal{O}(|{\mathbb{L}}|n_{\max}^{2})$
    continuous variables and linear constraints. Moreover, while the formulation for
    *an individual unit* is locally ideal, the composition of many locally ideal formulations
    will, in general, fail to be ideal itself. Consider that, while each node can
    be modeled as a two-part disjunction, the full network requires exponentially
    many disjuncts, each corresponding to one activation pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite its strength and relatively modest increase in size relative to the
    big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models
    using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")), it has been empirically
    observed that this formulation often performs worse than expected (Anderson et al.,
    [2019](#bib.bib4), Vielma, [2019](#bib.bib317)), both in the verification setting
    and more broadly.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 A class of intermediate formulations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The previous sections observed that the big-$M$ formulation ([4](#S4.E4 "In
    4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) is compact, but may offer a weak convex relaxation, while
    the extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) offers
    the tightest possible convex relaxation for an individual unit, at the expense
    of a much larger formulation. Kronqvist et al. ([2022](#bib.bib180), [2021](#bib.bib179))
    present a strategy for obtaining formulations intermediate to ([4](#S4.E4 "In
    4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) and ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) in terms
    of both size and strength. The key idea is to partition ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$
    into a number of aggregated variables, ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}=\sum_{p=1}^{P}\hat{x}_{p}$.
    Each auxiliary variable $\hat{x}_{p}$ is defined as a sum of a subset of the $j$-th
    weighted inputs $\hat{x}_{p}=\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$,
    with $\mathbb{S}_{1},...,\mathbb{S}_{P}$ partitioning $\{1,...,n_{l-1}\}$. This
    technique can be applied to the ReLU function, giving the convex hull over the
    directions defined by $\hat{x}_{p}$ (Tsay et al., [2021](#bib.bib312)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\left(\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1},h^{l}_{i}\right)$
    | $\displaystyle=(\hat{x}_{p}^{+},y^{+})+(\hat{x}_{p}^{-},y^{-})$ |  | (6a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{-}$ | $\displaystyle=0\geq\sum_{p}\hat{x}_{p}^{-}+b^{l}_{i}(1-z)$
    |  | (6b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{+}$ | $\displaystyle=\sum_{p}\hat{x}_{p}^{+}+b^{l}_{i}z\geq
    0$ |  | (6c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}(1-z)$ | $\displaystyle\leq\hat{x}^{-}\leq\hat{\bm{M}}_{i,+}^{l}(1-z)$
    |  | (6d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{\bm{M}}_{i,-}^{l}z$ | $\displaystyle\leq\hat{x}^{+}\leq\hat{\bm{M}}_{i,+}^{l}z$
    |  | (6e) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z$ | $\displaystyle\in\{0,1\}.$ |  | (6f) |'
  prefs: []
  type: TYPE_TB
- en: Here, the $p$-th elements of $\hat{\bm{M}}_{i,-}^{l}$ and $\hat{\bm{M}}_{i,+}^{l}$
    must satisfy the inequalities
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{M}^{l}_{i,-,p}$ | $\displaystyle\leq\min_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{M}^{l}_{i,+,p}$ | $\displaystyle\geq\max_{{\bm{h}}^{l-1}\in\mathcal{D}^{l-1}}\sum_{j\in\mathbb{S}_{p}}w_{i,j}^{l}h_{j}^{l-1}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'These coefficients can be derived using techniques analagous to those for the
    big-$M$ formulation (note that tighter bounds may be derived by considering $\hat{x}^{-}$
    and $\hat{x}^{+}$ separately). Observe that when $P=1$, we recover the same tightness
    as the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), as, intuitively, the
    formulation is built over a single “direction” corresponding to ${\bm{w}}_{i}^{l}{\bm{h}}^{l-1}$.
    Conversely, when $P=n_{l-1}$, we recover the tightness of the extended formulation
    ([5](#S4.E5 "In 4.2.2 A stronger extended formulation ‣ 4.2 Exact models using
    mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When
    Deep Learning Meets Polyhedral Theory: A Survey")), as each direction corresponds
    to a single element of ${\bm{h}}^{l-1}$. Tsay et al. ([2021](#bib.bib312)) study
    partitioning strategies and show that intermediate values of $P$ result in formulations
    that can outperform the two extremes, by balancing formulation size and strength.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.4 Cutting plane methods: Trading variables for inequalities'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The extended formulation ([5](#S4.E5 "In 4.2.2 A stronger extended formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) achieves
    its strength through the introduction of auxiliary continuous variables. However,
    it is possible to produce a formulation of equal strength by projecting out these
    auxiliary variables, leaving an ideal formulation in the “original” $({\bm{h}}^{l-1},h^{l}_{i},z)$
    variable space. While in general this projection may be difficult computationally,
    for the simple structure of a single ReLU neuron it is possible to characterize
    in closed form. The formulation is given by Anderson et al. ([2020](#bib.bib5),
    [2019](#bib.bib4)) as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (7a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{j\in J}w^{l}_{i,j}(h^{l-1}_{i}-\breve{L}^{l}_{j}(1-z))+\left(b+\sum_{j\not\in
    J}w^{l}_{i,j}\breve{U}_{j}\right)z\quad\forall J\subseteq\llbracket n_{l-1}\rrbracket$
    |  | (7b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}_{\geq
    0}$ |  | (7c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle z^{l}_{i}$ | $\displaystyle\in\{0,1\},$ |  | (7d) |'
  prefs: []
  type: TYPE_TB
- en: where notationally, for each $j\in\llbracket n_{l-1}\rrbracket$, we take
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S4.Ex1b.m1.12" class="ltx_Math" alttext="\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&amp;w^{l}_{i,j}\geq
    0\\'
  prefs: []
  type: TYPE_NORMAL
- en: L^{l-1}_{j}&amp;w^{l}_{i,j}<0\end{cases}" display="block"><semantics id="S4.Ex1b.m1.12a"><mrow
    id="S4.Ex1b.m1.12.12.2" xref="S4.Ex1b.m1.12.12.3.cmml"><mrow id="S4.Ex1b.m1.11.11.1.1"
    xref="S4.Ex1b.m1.11.11.1.1.cmml"><msubsup id="S4.Ex1b.m1.11.11.1.1.2" xref="S4.Ex1b.m1.11.11.1.1.2.cmml"><mover
    accent="true" id="S4.Ex1b.m1.11.11.1.1.2.2.2" xref="S4.Ex1b.m1.11.11.1.1.2.2.2.cmml"><mi
    id="S4.Ex1b.m1.11.11.1.1.2.2.2.2" xref="S4.Ex1b.m1.11.11.1.1.2.2.2.2.cmml">L</mi><mo
    id="S4.Ex1b.m1.11.11.1.1.2.2.2.1" xref="S4.Ex1b.m1.11.11.1.1.2.2.2.1.cmml">˘</mo></mover><mi
    id="S4.Ex1b.m1.11.11.1.1.2.3" xref="S4.Ex1b.m1.11.11.1.1.2.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.11.11.1.1.2.2.3" xref="S4.Ex1b.m1.11.11.1.1.2.2.3.cmml"><mi id="S4.Ex1b.m1.11.11.1.1.2.2.3.2"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.11.11.1.1.2.2.3.1"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3.1.cmml">−</mo><mn id="S4.Ex1b.m1.11.11.1.1.2.2.3.3"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.Ex1b.m1.11.11.1.1.1"
    xref="S4.Ex1b.m1.11.11.1.1.1.cmml">=</mo><mrow id="S4.Ex1b.m1.11.11.1.1.3.2" xref="S4.Ex1b.m1.11.11.1.1.3.1.cmml"><mrow
    id="S4.Ex1b.m1.4.4" xref="S4.Ex1b.m1.9.9.1.cmml"><mo id="S4.Ex1b.m1.4.4.5" xref="S4.Ex1b.m1.9.9.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.Ex1b.m1.4.4.4"
    xref="S4.Ex1b.m1.9.9.1.cmml"><mtr id="S4.Ex1b.m1.4.4.4a" xref="S4.Ex1b.m1.9.9.1.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.4.4.4b" xref="S4.Ex1b.m1.9.9.1.cmml"><msubsup
    id="S4.Ex1b.m1.1.1.1.1.1.1" xref="S4.Ex1b.m1.1.1.1.1.1.1.cmml"><mi id="S4.Ex1b.m1.1.1.1.1.1.1.2.2"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.2.cmml">L</mi><mi id="S4.Ex1b.m1.1.1.1.1.1.1.3"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.3.cmml">j</mi><mrow id="S4.Ex1b.m1.1.1.1.1.1.1.2.3"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2.cmml">l</mi><mo
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.1" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.4.4.4c" xref="S4.Ex1b.m1.9.9.1.cmml"><mrow
    id="S4.Ex1b.m1.2.2.2.2.2.1" xref="S4.Ex1b.m1.2.2.2.2.2.1.cmml"><msubsup id="S4.Ex1b.m1.2.2.2.2.2.1.4"
    xref="S4.Ex1b.m1.2.2.2.2.2.1.4.cmml"><mi id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.4" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1" xref="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.4.1" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.2.2.2.2.2.1.3" xref="S4.Ex1b.m1.2.2.2.2.2.1.3.cmml">≥</mo><mn id="S4.Ex1b.m1.2.2.2.2.2.1.5"
    xref="S4.Ex1b.m1.2.2.2.2.2.1.5.cmml">0</mn></mrow></mtd></mtr><mtr id="S4.Ex1b.m1.4.4.4d"
    xref="S4.Ex1b.m1.9.9.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.4.4.4e"
    xref="S4.Ex1b.m1.9.9.1.cmml"><msubsup id="S4.Ex1b.m1.3.3.3.3.1.1" xref="S4.Ex1b.m1.3.3.3.3.1.1.cmml"><mi
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.2" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.2.cmml">U</mi><mi
    id="S4.Ex1b.m1.3.3.3.3.1.1.3" xref="S4.Ex1b.m1.3.3.3.3.1.1.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.3" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.1"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.1.cmml">−</mo><mn id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S4.Ex1b.m1.4.4.4f" xref="S4.Ex1b.m1.9.9.1.cmml"><mrow id="S4.Ex1b.m1.4.4.4.4.2.1"
    xref="S4.Ex1b.m1.4.4.4.4.2.1.cmml"><msubsup id="S4.Ex1b.m1.4.4.4.4.2.1.4" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.cmml"><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.4" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1" xref="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.4.1" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.4.4.4.4.2.1.3" xref="S4.Ex1b.m1.4.4.4.4.2.1.3.cmml"><</mo><mn id="S4.Ex1b.m1.4.4.4.4.2.1.5"
    xref="S4.Ex1b.m1.4.4.4.4.2.1.5.cmml">0</mn></mrow></mtd></mtr></mtable></mrow><mi
    id="S4.Ex1b.m1.10.10" xref="S4.Ex1b.m1.10.10.cmml">and</mi></mrow></mrow><mrow
    id="S4.Ex1b.m1.12.12.2.2" xref="S4.Ex1b.m1.12.12.2.2.cmml"><msubsup id="S4.Ex1b.m1.12.12.2.2.2"
    xref="S4.Ex1b.m1.12.12.2.2.2.cmml"><mover accent="true" id="S4.Ex1b.m1.12.12.2.2.2.2.2"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.2.cmml"><mi id="S4.Ex1b.m1.12.12.2.2.2.2.2.2" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.2.cmml">U</mi><mo
    id="S4.Ex1b.m1.12.12.2.2.2.2.2.1" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.1.cmml">˘</mo></mover><mi
    id="S4.Ex1b.m1.12.12.2.2.2.3" xref="S4.Ex1b.m1.12.12.2.2.2.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.12.12.2.2.2.2.3" xref="S4.Ex1b.m1.12.12.2.2.2.2.3.cmml"><mi id="S4.Ex1b.m1.12.12.2.2.2.2.3.2"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.12.12.2.2.2.2.3.1"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.1.cmml">−</mo><mn id="S4.Ex1b.m1.12.12.2.2.2.2.3.3"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.3.cmml">1</mn></mrow></msubsup><mo id="S4.Ex1b.m1.12.12.2.2.1"
    xref="S4.Ex1b.m1.12.12.2.2.1.cmml">=</mo><mrow id="S4.Ex1b.m1.8.8" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mo
    id="S4.Ex1b.m1.8.8.5" xref="S4.Ex1b.m1.12.12.2.2.3.1.1.cmml">{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" id="S4.Ex1b.m1.8.8.4" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mtr
    id="S4.Ex1b.m1.8.8.4a" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S4.Ex1b.m1.8.8.4b" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><msubsup
    id="S4.Ex1b.m1.5.5.1.1.1.1" xref="S4.Ex1b.m1.5.5.1.1.1.1.cmml"><mi id="S4.Ex1b.m1.5.5.1.1.1.1.2.2"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.2.cmml">U</mi><mi id="S4.Ex1b.m1.5.5.1.1.1.1.3"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.3.cmml">j</mi><mrow id="S4.Ex1b.m1.5.5.1.1.1.1.2.3"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2.cmml">l</mi><mo
    id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.1" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.1.cmml">−</mo><mn
    id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd
    class="ltx_align_left" columnalign="left" id="S4.Ex1b.m1.8.8.4c" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mrow
    id="S4.Ex1b.m1.6.6.2.2.2.1" xref="S4.Ex1b.m1.6.6.2.2.2.1.cmml"><msubsup id="S4.Ex1b.m1.6.6.2.2.2.1.4"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4.cmml"><mi id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2" xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.4" xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1" xref="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.4.1" xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2" xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3" xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.6.6.2.2.2.1.3" xref="S4.Ex1b.m1.6.6.2.2.2.1.3.cmml">≥</mo><mn id="S4.Ex1b.m1.6.6.2.2.2.1.5"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.5.cmml">0</mn></mrow></mtd></mtr><mtr id="S4.Ex1b.m1.8.8.4d"
    xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mtd class="ltx_align_left" columnalign="left"
    id="S4.Ex1b.m1.8.8.4e" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><msubsup id="S4.Ex1b.m1.7.7.3.3.1.1"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.cmml"><mi id="S4.Ex1b.m1.7.7.3.3.1.1.2.2" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.2.cmml">L</mi><mi
    id="S4.Ex1b.m1.7.7.3.3.1.1.3" xref="S4.Ex1b.m1.7.7.3.3.1.1.3.cmml">j</mi><mrow
    id="S4.Ex1b.m1.7.7.3.3.1.1.2.3" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.cmml"><mi id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2.cmml">l</mi><mo id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.1"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.1.cmml">−</mo><mn id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3.cmml">1</mn></mrow></msubsup></mtd><mtd class="ltx_align_left"
    columnalign="left" id="S4.Ex1b.m1.8.8.4f" xref="S4.Ex1b.m1.12.12.2.2.3.1.cmml"><mrow
    id="S4.Ex1b.m1.8.8.4.4.2.1" xref="S4.Ex1b.m1.8.8.4.4.2.1.cmml"><msubsup id="S4.Ex1b.m1.8.8.4.4.2.1.4"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4.cmml"><mi id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2" xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2.cmml">w</mi><mrow
    id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.4" xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.3.cmml"><mi
    id="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1" xref="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1.cmml">i</mi><mo
    id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.4.1" xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.3.cmml">,</mo><mi
    id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2" xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2.cmml">j</mi></mrow><mi
    id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3" xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3.cmml">l</mi></msubsup><mo
    id="S4.Ex1b.m1.8.8.4.4.2.1.3" xref="S4.Ex1b.m1.8.8.4.4.2.1.3.cmml"><</mo><mn id="S4.Ex1b.m1.8.8.4.4.2.1.5"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.5.cmml">0</mn></mrow></mtd></mtr></mtable></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" id="S4.Ex1b.m1.12b"><apply id="S4.Ex1b.m1.12.12.3.cmml"
    xref="S4.Ex1b.m1.12.12.2"><csymbol cd="ambiguous" id="S4.Ex1b.m1.12.12.3a.cmml"
    xref="S4.Ex1b.m1.12.12.2.3">formulae-sequence</csymbol><apply id="S4.Ex1b.m1.11.11.1.1.cmml"
    xref="S4.Ex1b.m1.11.11.1.1"><apply id="S4.Ex1b.m1.11.11.1.1.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.11.11.1.1.2.1.cmml" xref="S4.Ex1b.m1.11.11.1.1.2">subscript</csymbol><apply
    id="S4.Ex1b.m1.11.11.1.1.2.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.11.11.1.1.2.2.1.cmml" xref="S4.Ex1b.m1.11.11.1.1.2">superscript</csymbol><apply
    id="S4.Ex1b.m1.11.11.1.1.2.2.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.2.2"><ci id="S4.Ex1b.m1.11.11.1.1.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.2.1">˘</ci><ci id="S4.Ex1b.m1.11.11.1.1.2.2.2.2.cmml"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.2.2">𝐿</ci></apply><apply id="S4.Ex1b.m1.11.11.1.1.2.2.3.cmml"
    xref="S4.Ex1b.m1.11.11.1.1.2.2.3"><ci id="S4.Ex1b.m1.11.11.1.1.2.2.3.2.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.2.3.2">𝑙</ci><cn
    type="integer" id="S4.Ex1b.m1.11.11.1.1.2.2.3.3.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.2.3.3">1</cn></apply></apply><ci
    id="S4.Ex1b.m1.11.11.1.1.2.3.cmml" xref="S4.Ex1b.m1.11.11.1.1.2.3">𝑗</ci></apply><list
    id="S4.Ex1b.m1.11.11.1.1.3.1.cmml" xref="S4.Ex1b.m1.11.11.1.1.3.2"><apply id="S4.Ex1b.m1.9.9.1.cmml"
    xref="S4.Ex1b.m1.4.4"><csymbol cd="latexml" id="S4.Ex1b.m1.9.9.1.1.cmml" xref="S4.Ex1b.m1.4.4.5">cases</csymbol><apply
    id="S4.Ex1b.m1.1.1.1.1.1.1.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1">subscript</csymbol><apply
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1">superscript</csymbol><ci
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.2">𝐿</ci><apply
    id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3"><ci id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2.cmml"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.2">𝑙</ci><cn type="integer" id="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3.cmml"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex1b.m1.1.1.1.1.1.1.3.cmml"
    xref="S4.Ex1b.m1.1.1.1.1.1.1.3">𝑗</ci></apply><apply id="S4.Ex1b.m1.2.2.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.2.2.2.2.2.1"><apply id="S4.Ex1b.m1.2.2.2.2.2.1.4.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.2.2.2.2.2.1.4.1.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4">subscript</csymbol><apply
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.1.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4">superscript</csymbol><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.2">𝑤</ci><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.4.2.3">𝑙</ci></apply><list
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.3.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.4"><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.1.1.1">𝑖</ci><ci
    id="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.2.2.2">𝑗</ci></list></apply><cn
    type="integer" id="S4.Ex1b.m1.2.2.2.2.2.1.5.cmml" xref="S4.Ex1b.m1.2.2.2.2.2.1.5">0</cn></apply><apply
    id="S4.Ex1b.m1.3.3.3.3.1.1.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.3.3.3.3.1.1.1.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1">subscript</csymbol><apply
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1"><csymbol cd="ambiguous"
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.1.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1">superscript</csymbol><ci
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.2.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.2">𝑈</ci><apply
    id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.cmml" xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3"><ci id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2.cmml"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.2">𝑙</ci><cn type="integer" id="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3.cmml"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.2.3.3">1</cn></apply></apply><ci id="S4.Ex1b.m1.3.3.3.3.1.1.3.cmml"
    xref="S4.Ex1b.m1.3.3.3.3.1.1.3">𝑗</ci></apply><apply id="S4.Ex1b.m1.4.4.4.4.2.1.cmml"
    xref="S4.Ex1b.m1.4.4.4.4.2.1"><apply id="S4.Ex1b.m1.4.4.4.4.2.1.4.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.4.4.4.4.2.1.4.1.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4">subscript</csymbol><apply
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4"><csymbol
    cd="ambiguous" id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.1.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4">superscript</csymbol><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.2">𝑤</ci><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.4.2.3">𝑙</ci></apply><list
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.3.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.4"><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.1.1.1">𝑖</ci><ci
    id="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.2.2.2">𝑗</ci></list></apply><cn
    type="integer" id="S4.Ex1b.m1.4.4.4.4.2.1.5.cmml" xref="S4.Ex1b.m1.4.4.4.4.2.1.5">0</cn></apply></apply><ci
    id="S4.Ex1b.m1.10.10.cmml" xref="S4.Ex1b.m1.10.10">and</ci></list></apply><apply
    id="S4.Ex1b.m1.12.12.2.2.cmml" xref="S4.Ex1b.m1.12.12.2.2"><apply id="S4.Ex1b.m1.12.12.2.2.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1b.m1.12.12.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2">subscript</csymbol><apply id="S4.Ex1b.m1.12.12.2.2.2.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2"><csymbol cd="ambiguous" id="S4.Ex1b.m1.12.12.2.2.2.2.1.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2">superscript</csymbol><apply id="S4.Ex1b.m1.12.12.2.2.2.2.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.2"><ci id="S4.Ex1b.m1.12.12.2.2.2.2.2.1.cmml" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.1">˘</ci><ci
    id="S4.Ex1b.m1.12.12.2.2.2.2.2.2.cmml" xref="S4.Ex1b.m1.12.12.2.2.2.2.2.2">𝑈</ci></apply><apply
    id="S4.Ex1b.m1.12.12.2.2.2.2.3.cmml" xref="S4.Ex1b.m1.12.12.2.2.2.2.3"><ci id="S4.Ex1b.m1.12.12.2.2.2.2.3.2.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.2">𝑙</ci><cn type="integer" id="S4.Ex1b.m1.12.12.2.2.2.2.3.3.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.2.3.3">1</cn></apply></apply><ci id="S4.Ex1b.m1.12.12.2.2.2.3.cmml"
    xref="S4.Ex1b.m1.12.12.2.2.2.3">𝑗</ci></apply><apply id="S4.Ex1b.m1.12.12.2.2.3.1.cmml"
    xref="S4.Ex1b.m1.8.8"><csymbol cd="latexml" id="S4.Ex1b.m1.12.12.2.2.3.1.1.cmml"
    xref="S4.Ex1b.m1.8.8.5">cases</csymbol><apply id="S4.Ex1b.m1.5.5.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.5.5.1.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1">subscript</csymbol><apply id="S4.Ex1b.m1.5.5.1.1.1.1.2.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.5.5.1.1.1.1.2.1.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1">superscript</csymbol><ci id="S4.Ex1b.m1.5.5.1.1.1.1.2.2.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.2">𝑈</ci><apply id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.cmml"
    xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3"><ci id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2.cmml" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.2">𝑙</ci><cn
    type="integer" id="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3.cmml" xref="S4.Ex1b.m1.5.5.1.1.1.1.2.3.3">1</cn></apply></apply><ci
    id="S4.Ex1b.m1.5.5.1.1.1.1.3.cmml" xref="S4.Ex1b.m1.5.5.1.1.1.1.3">𝑗</ci></apply><apply
    id="S4.Ex1b.m1.6.6.2.2.2.1.cmml" xref="S4.Ex1b.m1.6.6.2.2.2.1"><apply id="S4.Ex1b.m1.6.6.2.2.2.1.4.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.6.6.2.2.2.1.4.1.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4">subscript</csymbol><apply id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.1.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4">superscript</csymbol><ci id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.2">𝑤</ci><ci id="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.4.2.3">𝑙</ci></apply><list id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.3.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.4"><ci id="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.1.1.1">𝑖</ci><ci id="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.2.2.2">𝑗</ci></list></apply><cn type="integer" id="S4.Ex1b.m1.6.6.2.2.2.1.5.cmml"
    xref="S4.Ex1b.m1.6.6.2.2.2.1.5">0</cn></apply><apply id="S4.Ex1b.m1.7.7.3.3.1.1.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.7.7.3.3.1.1.1.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1">subscript</csymbol><apply id="S4.Ex1b.m1.7.7.3.3.1.1.2.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex1b.m1.7.7.3.3.1.1.2.1.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1">superscript</csymbol><ci id="S4.Ex1b.m1.7.7.3.3.1.1.2.2.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.2">𝐿</ci><apply id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.cmml"
    xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3"><ci id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2.cmml" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.2">𝑙</ci><cn
    type="integer" id="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3.cmml" xref="S4.Ex1b.m1.7.7.3.3.1.1.2.3.3">1</cn></apply></apply><ci
    id="S4.Ex1b.m1.7.7.3.3.1.1.3.cmml" xref="S4.Ex1b.m1.7.7.3.3.1.1.3">𝑗</ci></apply><apply
    id="S4.Ex1b.m1.8.8.4.4.2.1.cmml" xref="S4.Ex1b.m1.8.8.4.4.2.1"><apply id="S4.Ex1b.m1.8.8.4.4.2.1.4.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.8.8.4.4.2.1.4.1.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4">subscript</csymbol><apply id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4"><csymbol cd="ambiguous" id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.1.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4">superscript</csymbol><ci id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.2">𝑤</ci><ci id="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.4.2.3">𝑙</ci></apply><list id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.3.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.4"><ci id="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.1.1.1">𝑖</ci><ci id="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.2.2.2">𝑗</ci></list></apply><cn type="integer" id="S4.Ex1b.m1.8.8.4.4.2.1.5.cmml"
    xref="S4.Ex1b.m1.8.8.4.4.2.1.5">0</cn></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S4.Ex1b.m1.12c">\breve{L}^{l-1}_{j}=\begin{cases}L^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ U^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}\quad\mathrm{and}\quad\breve{U}^{l-1}_{j}=\begin{cases}U^{l-1}_{j}&w^{l}_{i,j}\geq
    0\\ L^{l-1}_{j}&w^{l}_{i,j}<0\end{cases}</annotation></semantics></math> |  |
  prefs: []
  type: TYPE_NORMAL
- en: 'We note a few points of interest about this formulation. First, it is ideal,
    and so recovers the convex hull of a ReLU activation function, coupled with its
    preactivation affine function and bounds on each of the inputs to that affine
    function. Second, it can be shown that, under very mild conditions, each of the
    exponentially many constraints in ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods:
    Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) are necessary to ensure this property; none are redundant
    and can be removed without affecting the relaxation quality. Third, note that
    by selecting only those constraints in ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane
    methods: Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) corresponding to $J=\emptyset$ and $J=\llbracket
    n_{l-1}\rrbracket$, we recover the big-$M$ formulation ([4](#S4.E4 "In 4.2.1 The
    big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")) in the case where $\mathcal{D}^{l-1}=[L^{l-1},U^{l-1}]$. This suggests
    a practical approach for using this large family of inequalities: Start with the
    big-$M$ formulation, and then dynamically generate violated inequalities from
    ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods: Trading variables for inequalities
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) as-needed
    in a cutting plane procedure. As shown by Anderson et al. ([2020](#bib.bib5)),
    this separation problem is separable in the input variables, and hence can be
    completed in $\mathcal{O}(n_{l-1})$ time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cutting plane strategy is in general compatible with weaker formulations,
    such as relaxation-based verification (Zhang et al., [2022](#bib.bib356)) and
    formulations from the class ([6](#S4.E6 "In 4.2.3 A class of intermediate formulations
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). In fact,
    Tsay et al. ([2021](#bib.bib312)) show that the intermediate formulations in ([6](#S4.E6
    "In 4.2.3 A class of intermediate formulations ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) effectively pre-select a number of inequalities
    from ([7b](#S4.E7.2 "In 7 ‣ 4.2.4 Cutting plane methods: Trading variables for
    inequalities ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")), in terms of their continuous relaxations. While adding these constraints
    results in a tighter continuous relaxation, the added constraints can eventually
    significantly increase the model size. Practical implementations may therefore
    only perform cut generation at a limited number of branch-and-bound search nodes
    (De Palma et al., [2021](#bib.bib77), Tsay et al., [2021](#bib.bib312)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'A subtlety when using ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading
    variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey"))'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This third point above raises a subtlety discussed in the literature (De Palma
    et al., [2021](#bib.bib77), Appendix F). Often, additional structural information
    is known about $\mathcal{D}^{l-1}$ beyond bounds on the variables. In this case,
    it is typically possible to derive tighter values for the big-$M$ coefficients.
    In this case, when using a separation-based approach it is preferable to initialize
    the formulation with these tightened big-$M$ constraints, and then proceed with
    the cutting plane approach as normal from there.
  prefs: []
  type: TYPE_NORMAL
- en: '4.3 Scaling further: Convex relaxations and linear programming'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The above demonstrate MILP as a powerful framework for exactly modeling complex,
    nonconvex trained neural networks, but standard solvers are often not sufficiently
    scalable to adequately handle large-scale networks. A natural approach to increase
    the scalability, then, is to *relax* the network in some manner, and then apply
    convex optimization methods. For the verification problem discussed in Section [4.1.1](#S4.SS1.SSS1
    "4.1.1 Neural network verification ‣ 4.1 Applications of optimization over trained
    networks ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), this yields what is known as an *incomplete verifier*:
    any certification of robustness provided can be trusted (no false positives),
    but there may be robust instances that the method cannot prove are (some false
    negatives). In other words, over-approximation produces a verifier that is sound,
    but not complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While a variety of methods exist for accomplishing this, in this section we
    briefly outline techniques relevant to polyhedral theory. In particular, we focus
    on some techniques for building convex polyhedral relaxations. The most natural
    convex relaxation for a MILP formulation is its linear programming (LP) relaxation,
    constructed by dropping any integrality constraints. For example, the LP relaxation
    of ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) is given by the system ([4a](#S4.E4.1 "In
    4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")-[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). This is a compact linear
    programming relaxation for a ReLU-based network, and is the basis for methods
    due to Bunel et al. ([2020a](#bib.bib41)) and Ehlers ([2017](#bib.bib87)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Projecting the big-$M$ and ideal MILP formulations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section examines projections of the linear relaxations of formulations
    ([4](#S4.E4 "In 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) and ([7](#S4.E7 "In 4.2.4 Cutting plane methods:
    Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '(Projecting the big-$M$). Note that the LP relaxation given by ([4a](#S4.E4.1
    "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")–[4d](#S4.E4.4 "In 4 ‣ 4.2.1 The big-𝑀 formulation ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) maintains the variables
    $z^{l}_{i}$ in the formulation, though they are no longer required to satisfy
    integrality. Since these variables are “auxiliary” and are no longer necessary
    to encode the nonconvexity of the problem, they can be projected out without altering
    the quality of the convex relaxation. Doing this yields what is commonly known
    as the “triangle” or “$\Delta$” relaxation (Salman et al., [2019](#bib.bib269)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (8a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\frac{M^{l}_{i,+}}{M^{l}_{i,+}-M^{l}_{i,-}}({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})$
    |  | (8b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0}.$ |  | (8c) |'
  prefs: []
  type: TYPE_TB
- en: 'While the LP relaxation ([8](#S4.E8 "In 4.3.1 Projecting the big-𝑀 and ideal
    MILP formulations ‣ 4.3 Scaling further: Convex relaxations and linear programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) for an individual unit is compact, modern neural network architectures
    regularly comprise millions of units. The resulting LP relaxation for the entire
    network may then require millions of variables and constraints. Additionally,
    unless special precautions are taken, many of these constraints will be relatively
    dense. All this quickly leads to LP that are beyond the scope of modern off-the-shelf
    LP solvers. As a result, researchers have explored alternative schemes for scaling
    LP-based methods to these larger networks. Salman et al. ([2019](#bib.bib269))
    present a framework for LP-based methods (LP solvers, propagation, dual methods),
    which we review in the following subsections. However, they do not account for
    the ideal formulation developed in later works (Anderson et al., [2020](#bib.bib5),
    De Palma et al., [2021](#bib.bib77)).'
  prefs: []
  type: TYPE_NORMAL
- en: '(Projecting the ideal). Figure [9](#S4.F9 "Figure 9 ‣ 4.2.1 The big-𝑀 formulation
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") shows
    that the triangle (big-$M$) relaxation fails to recover the convex hull of the
    ReLU activation function and the multivariate input to the affine pre-activation
    function. We can similarly project the LP relaxation of the ideal formulation
    ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables for inequalities
    ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) into
    the space of input/output variables (Anderson et al., [2020](#bib.bib5)), yielding
    a description for the convex hull of $\{({\bm{h}}^{l-1},h^{l}_{i})|L^{l-1}\leq{\bm{h}}^{l-1}\leq
    U^{l-1},\>h^{l}_{i}=\sigma({\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i})\}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\geq{\bm{w}}^{l}_{i}{\bm{h}}^{l-1}+b^{l}_{i}$
    |  | (9a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h^{l}_{i}$ | $\displaystyle\leq\sum_{k\in I}w_{i,k}^{l}(x_{k}-\breve{L}_{k})+\frac{\ell(I)}{\breve{U}_{h}-\breve{L}_{h}}(x_{h}-\breve{L}_{h})\quad\forall(I,h)\in\mathcal{J}$
    |  | (9b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},h^{l}_{i})$ | $\displaystyle\in[L^{l-1},U^{l-1}]\times\mathbb{R}_{\geq
    0},$ |  | (9c) |'
  prefs: []
  type: TYPE_TB
- en: where $l(I)\coloneqq\sum_{k\in I}w^{l}_{i,k}\breve{L}_{k}+\sum_{k\not\in I}w^{l}_{i,k}\breve{U}_{k}+b^{l}_{i}$
    and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{J}\coloneqq\Set{(I,h)\in 2^{\llbracket n_{l-1}\rrbracket}\times\llbracket
    n_{l-1}\rrbracket}{l(I)\geq 0,\>l(I\cup\{h\}<0,\>w^{l}_{i,k}\neq 0\forall k\in
    I}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Anderson et al. ([2020](#bib.bib5)) also show that the inequalities ([9b](#S4.E9.2
    "In 9 ‣ 4.3.1 Projecting the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling further:
    Convex relaxations and linear programming ‣ 4 Optimizing Over a Trained Neural
    Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) can be separated
    over in $\mathcal{O}(n_{l-1})$ time. Interestingly, in contrast to ([7](#S4.E7
    "In 4.2.4 Cutting plane methods: Trading variables for inequalities ‣ 4.2 Exact
    models using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), the number of facet-defining
    inequalities depends heavily on the affine function. While in the worst case the
    number of inequalities will grow exponentially in the input dimension, there exist
    instances where the convex hull can be fully described with only $\mathcal{O}(n_{l-1})$
    inequalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Dual decomposition methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A first approach for greater scalability for LP-based methods is decomposition,
    a standard technique in the large-scale optimization community. Indeed, the cutting
    plane approach of Section [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting plane methods: Trading
    variables for inequalities ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") can be viewed as a decomposition method operating in the original
    variable space. However, the method is initialized with the big-$M$ formulation
    for each neuron, and hence this initial model will be of size roughly equal to
    the size of the network. Therefore, it should be understood to use decomposition
    to provide a tighter verification bound, rather than for providing greater scalability
    to larger networks.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, dual decomposition can be used to scale inexact verification methods
    to larger networks. Such methods maintain dual feasible solutions throughout the
    algorithm, meaning that upon termination they yield valid dual bounds on the verification
    instance, and hence serve as incomplete verifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Wong and Kolter ([2018](#bib.bib335)), Wong et al. ([2018](#bib.bib336)) use
    as their starting point the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting
    the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling further: Convex relaxations
    and linear programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) for each neuron, and then take the
    standard LP dual of the (relaxed) verification problem. Alternatively, Dvijotham
    et al. ([2018b](#bib.bib85)) propose a Lagrangian-based approach for decomposing
    the original nonlinear formulation of the problem ([3](#S4.E3 "In Example 4 ‣
    4.1.1 Neural network verification ‣ 4.1 Applications of optimization over trained
    networks ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")). Crucially, since the complicating constraints
    coupling the layers in the network are imposed as objective penalties instead
    of “hard” constraints, the optimization problem (given fixed dual variables) decomposes
    along each layer and the subproblems induced by the separability can be solved
    in closed form. This approach dualizes separately the equations characterizing
    the pre-activation and post-activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\mu,\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\mu_{k}^{T}(\hat{{\bm{h}}}^{k}-{\bm{W}}^{k}{\bm{h}}^{k-1}-{\bm{b}}^{k})+\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\sigma(L^{k})\leq{\bm{h}}^{k}\leq\sigma(U^{k})\quad\forall
    k\in\llbracket n-1\rrbracket.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Here, the $\hat{{\bm{h}}}$ variables track the pre-activation values for the
    neurons in the network. The dual variables $\mu_{k}^{T}$ correspond to the equality
    constraints defining the pre-activation values, $\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+{\bm{b}}^{k}$.
    Likewise, the dual variables $\lambda_{k}^{T}$ correspond to enforcing the ReLU
    activation function, ${\bm{h}}^{k}=\sigma(\hat{{\bm{h}}}^{k})=\mathrm{max}(0,\hat{{\bm{h}}}^{k})$.
    Any feasible solution for the neural network is feasible for this dualized problem,
    making the multiplier terms for $\mu_{k}^{T}$ and $\lambda_{k}^{T}$ zero. Thus,
    the inner problem gives a lower bound for the original problem—a property known
    as *weak duality*. The outer (dual) problem optimizing over the Lagrangian multipliers
    then seeks to maximize this lower bound, i.e., to give the tightest possible lower
    bound. This can be solved using a subgradient-based method, or learned along with
    the model parameters in a “predictor-verifier” approach (Dvijotham et al., [2018a](#bib.bib84)).
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, this approach can be combined with other relaxation-based
    methods. The Lagrangian decomposition can be applied to dualize only the coupling
    constraints between layers, and a convex relaxation used for the activation function
    (Bunel et al., [2020a](#bib.bib41)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{\lambda}\quad\min_{{\bm{h}},\hat{{\bm{h}}}}\quad$
    | $\displaystyle\left({\bm{W}}^{L}{{\bm{h}}}^{L-1}+{\bm{b}}^{L}\right)+\sum_{k=1}^{L-1}\left(\lambda_{k}^{T}({\bm{h}}^{k}-\sigma(\hat{{\bm{h}}}^{k})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle L^{k}\leq\hat{{\bm{h}}}^{k}\leq U^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}^{k}={\bm{W}}^{k}{\bm{h}}^{k-1}+b^{k}\quad\forall
    k\in\llbracket n-1\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}^{k}\geq 0\quad\forall k\in\llbracket n-1\rrbracket$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}_{i}^{k}\geq\hat{{\bm{h}}}_{i}^{k}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}^{k}_{i}\leq\frac{U^{k}_{i}(\hat{{\bm{h}}}^{k}_{i}-L^{k}_{i})}{U^{k}_{i}-L^{k}_{i}}\quad\forall
    k\in\llbracket n-1\rrbracket,\forall i\in\llbracket n_{k}\rrbracket.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note that the final three constraints apply the big-$M$/triangle relaxation
    ([8](#S4.E8 "In 4.3.1 Projecting the big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling
    further: Convex relaxations and linear programming ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to each
    ReLU activation function. The dual problem can then be solved via subgradient-based
    methods, proximal algorithms, or, more recently, a projected gradient descent
    method applied to a nonconvex reformulation of the problem (Bunel et al., [2020c](#bib.bib44)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, De Palma et al. ([2021](#bib.bib77)) presented a dual decomposition
    approach based on ([7](#S4.E7 "In 4.2.4 Cutting plane methods: Trading variables
    for inequalities ‣ 4.2 Exact models using mixed-integer programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey")). However, creating a dual formulation from the exponential number of
    constraints produces an exponential number of dual variables. The authors therefore
    propose to maintain an “active set” of dual variables to keep the problem sparse.
    A selection algorithm (e.g., selecting entries that maximize an estimated super-gradient)
    can then be used to append the active set. Similar to the above discussion on
    cut generation, the frequency of appending the active set should be chosen strategically.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Fourier-Motzkin elimination and propagation algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Alternatively, one can project out *all* of the decision variables. For example,
    in order to solve the linear programming problem $\min_{x\in\mathcal{X}}c\cdot
    x$, we can augment the problem with a new decision variable to $\min_{(x,y)\in\Gamma}y$
    for $\Gamma\vcentcolon=\Set{(x,y)\in\mathcal{X}\times\mathbb{R}:y=c\cdot x}$,
    and project out the $x$ variables. The transformed problem is the a trivial univariate
    optimization problem: $\min_{y\in\operatorname{Proj}_{y}(\Gamma)}y$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, the complexity of the approach described is hidden in the projection
    step, or building $\operatorname{Proj}_{y}(\Gamma)$. The most well-known algorithm
    for computing projections of linear inequality systems is Fourier-Motzkin elimination,
    described by Dantzig and Eaves ([1973](#bib.bib74)), which is notorious for its
    practical inefficiency. The process effectively comprises replacing variables
    from a set of inequalities with all possible implied inequalities, which can produce
    many unnecessary constraints. However, it turns out that neural network verification
    problems are well-structured in such a way that Fourier-Motzkin elimination can
    be performed very efficiently: for instance, by imposing one inequality upper
    bounding and one inequality lower bounding each ReLU function. Note that while
    Section [3.2](#S3.SS2 "3.2 The algebra of linear regions ‣ 3 The Linear Regions
    of a Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey") describes
    the use of Fourier-Motzkin elimination to obtain *exact* input-output relationships
    in linear regions of neural networks, here we are interested in obtaining linear
    *bounds* for a nonlinear function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/123fb03cab46add54faaa496e472ed4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Convex approximations for the ReLU function commonly used by propagation
    algorithms, given as a function of the preactivation function $\hat{h_{i}^{l}}$.
    The ReLU applies $h_{i}^{l}=\max(0,\hat{h_{i}^{l}})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, this general approach was independently developed in the verification
    community. While MILP research has focused on formulations tighter than the big-M,
    such as ([5](#S4.E5 "In 4.2.2 A stronger extended formulation ‣ 4.2 Exact models
    using mixed-integer programming ‣ 4 Optimizing Over a Trained Neural Network ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")) and ([6](#S4.E6 "In 4.2.3
    A class of intermediate formulations ‣ 4.2 Exact models using mixed-integer programming
    ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")), the verification community often prefers greater scalability
    at the price of weaker convex relaxations. The continuous relaxation of the big-M
    is equivalent to the triangle relaxation ([8](#S4.E8 "In 4.3.1 Projecting the
    big-𝑀 and ideal MILP formulations ‣ 4.3 Scaling further: Convex relaxations and
    linear programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")): the optimal convex relaxation for a single
    input, or in terms of the aggregated pre-activation function, as shown in Figure
    [10](#S4.F10 "Figure 10 ‣ 4.3.3 Fourier-Motzkin elimination and propagation algorithms
    ‣ 4.3 Scaling further: Convex relaxations and linear programming ‣ 4 Optimizing
    Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A
    Survey"). However, the lower bound involves two linear constraints, which is not
    used in several propagation-based verification tools owing to scalability or compatibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Such tools use methods such as abstract transformers to propagate polyhedral
    bounds, i.e., zonotopes, through the layers of a neural network. DeepZ (Singh
    et al., [2018](#bib.bib289)), Fast-Lin (Weng et al., [2018](#bib.bib330)), and
    Neurify (Wang et al., [2018a](#bib.bib326)) employ a parallel approximation, with
    the latter also implementing a branch-and-bound procedure towards completeness.
    Subsequently, DeepPoly (Singh et al., [2019b](#bib.bib291)) and CROWN (Zhang et al.,
    [2018a](#bib.bib354)) select between the zero and identity approximations by minimizing
    over-approximation area. OSIP (Hashemi et al., [2021](#bib.bib140)) selects between
    the three approximations using optimization: approximations for a layer are select
    jointly to minimise bounds for the following layer. These technologies are also
    compatible with interval bounds, propagating box domains (Mirman et al., [2018](#bib.bib220)).
    Interestingly, bounds on neural network weights can also be propagated using similar
    methods, allowing reachability analysis of Bayesian neural networks (Wicker et al.,
    [2020](#bib.bib332)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tjandraatmadja et al. ([2020](#bib.bib308)) provide an interpretation of these
    propagation techniques through the lens of Fourier-Motzkin elimination. Consider
    the problem of propagating bounds through a ReLU neural network: for a node $h_{i}^{l}=\mathrm{max}\{0,\hat{h_{i}^{l}}\}$,
    convex bounds for $h_{i}^{l}$ can be obtained given bounds for $\hat{h}_{i}^{l}$
    (Figure [10](#S4.F10 "Figure 10 ‣ 4.3.3 Fourier-Motzkin elimination and propagation
    algorithms ‣ 4.3 Scaling further: Convex relaxations and linear programming ‣
    4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")). Assuming the inputs are outputs of ReLU activations in the
    previous layer, $\hat{h}_{i}^{l}={\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$. Computing
    an upper bound can then be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\max_{{\bm{h}}^{l-1}}$ | $\displaystyle{\bm{w}}_{i}^{l}{\bm{h}}^{l-1}+b_{i}^{l}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})\forall
    k\in\{1,...,n_{l-1}\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'As the objective function is linear, the solution of this problem can be computed
    by propagation without explicit optimization. For each element in ${\bm{h}}^{l-1}$,
    we only need to consider the associated objective coefficient in ${\bm{w}}_{i}^{l}$
    to determine whether $\mathcal{L}_{k}({\bm{h}}^{l-2})\leq h^{l-1}_{k}$ or $h^{l-1}_{k}\leq\mathcal{G}_{k}({\bm{h}}^{l-2})$
    will be the active inequality at the optimal solution. We can thus replace $h^{l-1}_{k}$
    with $\mathcal{L}_{k}({\bm{h}}^{l-2})$ or $\mathcal{G}_{k}({\bm{h}}^{l-2})$ accordingly.
    This projection is mathematically equivalent to applying Fourier-Motzkin elimination,
    while avoiding redundant inequalities resulting from the ‘non-selected’ bounding
    function. Repeating this procedure for each layer results in a convex relaxation
    for the outputs that only involves the input variables. We naturally observe the
    desirability of simple lower bounds $\mathcal{L}_{k}({\bm{h}}^{l-2})$: imposing
    two-part lower bounds in each layer would increase the number of propagated constraints
    in an exponential manner, similar to Fourier-Motzkin elimination.'
  prefs: []
  type: TYPE_NORMAL
- en: A path towards completeness
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Given an input-output bound, the reachable set can be refined by splitting
    the input space (Henriksen and Lomuscio, [2021](#bib.bib145), Rubies-Royo et al.,
    [2019](#bib.bib265))—a strategy similar to spatial branch and bound. In other
    words, completeness is achieved by branching in the input space, rather than activation
    patterns: this strategy is especially effective when the input space is low dimensional
    (Strong et al., [2022](#bib.bib296)). For example, ReluVal (Wang et al., [2018b](#bib.bib327))
    propagates symbolic intervals and implements splitting procedures on the input
    domain. As the interval extension of ReLU is Lipschitz continuous, the method
    converges to arbitrary accuracy in a finite number of splits.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Generalizing the single neuron model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.4.1 Extending to other domains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In general, we will expect that the effective input domain $\mathcal{D}^{l-1}$
    for a given unit may be quite complex. For the first layer ($l=1$) this may derive
    from explicitly stated constraints on the inputs of the networks, while for later
    layers this will typically derive from the complex nonlinear transformations applied
    by the preceding layers. For example, in the context of surrogate models Yang
    et al. ([2022](#bib.bib347)) propose bounding the input to the convex hull of
    the training data set, while other works (Schweidtmann et al., [2022](#bib.bib277),
    Shi et al., [2022](#bib.bib285)) propose machine learning-inspired techniques
    for learning the trust region implied by the training data. In effect, these methods
    assume a trained model is locally accurate around training data, which is a property
    similar to that which verification seeks to prove.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, most research focuses on hyperrectangular input domains, largely
    motivated by practical considerations: i) there are efficient, well-studied methods
    for computing valid (though not necessarily optimally tight) variable bounds,
    ii) characterizing the exact effective domain may be computationally impractical,
    and iii) and the hyperrectangular structure makes analysis simpler for complex
    formulations like those presented in Section [4.2.4](#S4.SS2.SSS4 "4.2.4 Cutting
    plane methods: Trading variables for inequalities ‣ 4.2 Exact models using mixed-integer
    programming ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"). We note that Jordan et al. ([2019](#bib.bib167))
    use polyhedral analyses to perform verification over arbitrary (including non-polyhedral)
    norms, by fitting a $p$-norm ball in the decision region and checking adjacent
    linear regions. On the other hand, robust optimization can be employed to find
    $p$-norm adversarial regions (rather than verifying robustness), as opposed to
    a single point adversary (Maragno et al., [2023](#bib.bib211)).'
  prefs: []
  type: TYPE_NORMAL
- en: Anderson et al. ([2020](#bib.bib5)) present two closely related frameworks for
    constructing ideal and hereditarily sharp formulations for ReLU units with arbitrary
    polyhedral input domains. This characterization is derived from Lagrangian duality,
    and requires an infinite number of constraints (intuitively, one for each choice
    of dual multipliers). Nonetheless, separation can still be done over this infinite
    family of inequalities via a subgradient-based algorithm; this approach will be
    tractable if optimization over $\mathcal{D}^{l-1}$ is tractable. Many propagation
    algorithms are also fully compatible with arbitrary polyhedral input domains,
    as the projected problem (i.e., a linear input-output relaxation) remains an LP.
    Singh et al. ([2021](#bib.bib292)) show that simplex input domains can actually
    be beneficial, creating tighter formulations by propagating constraints on the
    inputs through the network layers. Similarly, optimization-based bound tightening
    problems based on solving LPs can embed constraints defining polyhedral input
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: In certain cases, additional structural information about the input domain can
    be used to reduce this semi-infinite description to a finite one. For example,
    this can be done when $\mathcal{D}^{l-1}$ is a Cartesian product of unit simplices
    (Anderson et al., [2020](#bib.bib5)) (note that this generalizes the box domain
    case, wherein each simplex is one-dimensional). This particular structure is particularly
    useful for modeling input domains with combinatorial constraints. For example,
    a network trained to predict binding propensity of a given length-$n$ DNA sequence
    is naturally modeled via an input domain that is the product of $n$ 4-dimensional
    simplices–one simplex for each letter in the sequence, each of which is selected
    from an alphabet of length 4.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 Extending to other activation functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The big-$M$ formulation technique can be any piecewise linear activation function.
    While much of the literature focuses on the ReLU due to its widespread popularity,
    models for other activation functions have been explored in the literature. For
    example, multiple papers (Serra et al., [2018](#bib.bib282), Appendix K) (Tjeng
    et al., [2019](#bib.bib309), Appendix A.2) present a big-$M$ formulation for the
    maxout activation function. Adapting a formulation from Anderson et al. ([2020](#bib.bib5))
    (Anderson et al., [2020](#bib.bib5), Proposition 10), a formulation for the maxout
    unit is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\leq u_{j}({\bm{h}}^{l-1})+M^{l}_{i,j}(1-z_{j})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle y^{l}_{i}$ | $\displaystyle\geq u_{j}({\bm{h}}^{l-1})\quad^{\forall}j\in\llbracket
    k\rrbracket$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\sum_{j=1}^{k}z_{j}$ | $\displaystyle=1$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle({\bm{h}}^{l-1},v^{l}_{i},z)$ | $\displaystyle\in\mathcal{D}^{l-1}\times\mathbb{R}\times\{0,1\}^{k},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where each $M^{l}_{i,j}$ is selected such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $M^{l}_{i,j}\geq\max_{\tilde{{\bm{h}}}\in\mathcal{D}^{l-1}}u_{j}(\tilde{{\bm{h}}}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We can observe that the big-$M$ formulation can also handle other discontinuous
    activation functions, such as a binary/sign activations (Han and Gómez, [2021](#bib.bib136))
    or more general quantized activations (Nguyen and Huchette, [2022](#bib.bib234)).
    Nevertheless, the binary activation function naturally lends itself towards Boolean
    satisfiability, and most work therefore focuses on alternative methods such as
    SAT (Cheng et al., [2018](#bib.bib57), Jia and Rinard, [2020](#bib.bib164), Narodytska
    et al., [2018](#bib.bib229)).
  prefs: []
  type: TYPE_NORMAL
- en: While this survey focuses on neural networks with piecewise linear activation
    functions, we note that recent research has also studied smooth activation functions
    with a similar aim. For example, optimization over smooth activation functions
    can be handled by piecewise linear approximation and conversion to MILP (Sildir
    and Aydin, [2022](#bib.bib287)). Researchers have also studied convex/concave
    bounds for nonlinear activation functions, which can then be embedded in spatial
    branch-and-bound procedures (Schweidtmann and Mitsos, [2019](#bib.bib276), Wilhelm
    et al., [2022](#bib.bib334)). In contrast to MILP formulations for ReLU neural
    networks, these problems are typically nonlinear programs that must be solved
    via spatial branch and bound.
  prefs: []
  type: TYPE_NORMAL
- en: 'Propagation methods (Singh et al., [2018](#bib.bib289), Zhang et al., [2018a](#bib.bib354))
    can also naturally handle general activation functions: given convex polytopic
    bounds for an activation function, these tools can propagate them through network
    layers using the same techniques. For example, Fastened CROWN (Lyu et al., [2020](#bib.bib204))
    employs a set of search heuristics to quickly select linear upper and lower bounds
    on ReLU, sigmoid, and hyperbolic tangent activation functions. Tighter polyhedral
    bounds can be employed, such as piecewise linear upper and lower bounds (Benussi
    et al., [2022](#bib.bib23)).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.3 Extending to adversarial training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As described in Section [1](#S1 "1 Introduction ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey"), the *training* of neural networks seeks to minimise
    a measure of distance between the output $y$ and the correct output $\hat{y}$.
    For instance, if this distance is prescribed as loss function $\mathcal{L}(y,\hat{y})$,
    this corresponds to solving the *training* optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\mathcal{L}(y,\hat{y}).$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'Further details about the training problem and solution methods are described
    in the following section. Here, we briefly outline how verification techniques
    can be embedded in training. Specifically, solutions or bounds to the verification
    problem (Section [4.1.1](#S4.SS1.SSS1 "4.1.1 Neural network verification ‣ 4.1
    Applications of optimization over trained networks ‣ 4 Optimizing Over a Trained
    Neural Network ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) provide
    a metric of how robust a trained neural network is to perturbations. These metrics
    can be embedded in the training problem to obtain a more robust network during
    training, often resulting in a bilevel training problem. For instance, the verification
    problem ([3](#S4.E3 "In Example 4 ‣ 4.1.1 Neural network verification ‣ 4.1 Applications
    of optimization over trained networks ‣ 4 Optimizing Over a Trained Neural Network
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) can be embedded as a
    lower-level problem, giving the robust optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\quad\underset{&#124;&#124;x-\hat{x}&#124;&#124;\leq\epsilon}{\mathrm{max}}\quad\mathcal{L}(y=f(x),\hat{y}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Solving these problems generally involves either bilevel optimization, or computing
    an adversarial solution/bound at each training step, conceptually similar to the
    robust cutting plane approach. Madry et al. ([2018](#bib.bib206)) proposed this
    formulation and solved the nonconvex inner problem using gradient descent, thereby
    losing a formal certification of robustness. These approaches may also benefit
    from reformulation strategies, such as by taking the dual of the inner problem
    and using any feasible solution as a bound (Wong and Kolter, [2018](#bib.bib335)).
    The resulting models are not only more robust, but several works have also found
    it to be empirically easier to verify robustness in them (Mirman et al., [2018](#bib.bib220),
    Wong and Kolter, [2018](#bib.bib335)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, robustness can be induced by designing an additional penalty
    term for the training loss function, in a similar vein to regularization. For
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{\{{\bm{W}}^{l}\}_{l\in{\mathbb{L}}},\{{\bm{b}}^{l}\}_{l\in{\mathbb{L}}}}{\mathrm{min}}\kappa\mathcal{L}(y,\hat{y})+(1-\kappa)\mathcal{L}_{\mathrm{robust}}(\cdot).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Additionally, if these robustness penalties are differentiable, they can be
    embedded into standard gradient descent based optimization algorithms (Dvijotham
    et al., [2018b](#bib.bib85), Mirman et al., [2018](#bib.bib220)). In the above
    formulation, the parameter $\kappa$ controls the relative weighting between fitting
    the training data and satisfying some robustness criterion, and its value can
    be scheduled during training, e.g., to first focus on model accuracy (Gowal et al.,
    [2018](#bib.bib129)). In these cases, over-approximation of the reachable set
    is less problematic, as it merely produces a model *more* robust than required.
    Nevertheless, Balunović and Vechev ([2020](#bib.bib16)) improve relaxation tightness
    by searching for adversarial examples in the “latent” space between hidden layers,
    reducing the number of propagation steps. Zhang et al. ([2020](#bib.bib355)) provide
    an implementation that that tightens relaxations by also propagating bounds backwards
    through the network.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Linear Programming and Polyhedral Theory in Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous sections, we have almost exclusively focused on tasks involving
    neural networks that have already been constructed, i.e., we have assumed that
    the training step has already concluded (with the exception of Section [4.4.3](#S4.SS4.SSS3
    "4.4.3 Extending to adversarial training ‣ 4.4 Generalizing the single neuron
    model ‣ 4 Optimizing Over a Trained Neural Network ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")). In this section, we focus on the training phase,
    whose goal is to construct a neural network that can represent the relationship
    between the input and output of a given set of data points.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider a set of points, or sample, $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    and assume that these points are related via a function $\hat{f}$, i.e., $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$
    $i=1,\ldots,D$. In the training phase, we look for $\hat{f}$ in a pre-defined
    class (e.g. neural networks with a specific architecture) that approximates the
    relation $\hat{f}(\tilde{{\bm{x}}}_{i})=\tilde{{\bm{y}}}_{i}$. Typically, this
    is done by solving an Empirical Risk Minimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\hat{f}\in F}\frac{1}{D}\sum_{i=1}^{D}\ell(\hat{f}(\tilde{{\bm{x}}}_{i}),\tilde{{\bm{y}}}_{i})$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\ell$ is a loss function and $F$ is the class of functions we are restricted
    to. We usually assume the class $F$ is parametrized by $({\bm{W}},{\bm{b}})\in\Theta$
    (the network weights and biases), so we are further assuming that there exists
    a function $f(\cdot,\cdot,\cdot)$ (the network architecture) such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\forall\hat{f}\in F,\,\exists({\bm{W}},{\bm{b}})\in\Theta,\,\hat{f}({\bm{x}})=f({\bm{x}},{\bm{W}},{\bm{b}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: and thus, the optimization is performed over the space of parameters. In many
    cases, $\Theta=\mathbb{R}^{N}$—the parameters are unrestricted real numbers—but
    we will see some cases when a different parameter space can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the introduction, nowadays, most of the practically successful
    *training* algorithms for neural networks, i.e., that solve or approximate ([11](#S5.E11
    "In 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")), are based on Stochastic Gradient Descent
    (SGD). From a fundamental perspective, optimization problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is typically a *non-convex, unconstrained* problem
    that needs to be solved efficiently and where finding a *local minimum* is sufficient.
    Thus, it is not too surprising that linear programming appears to be an unsuitable
    tool in this phase, in general. Nonetheless, there are some notable and surprising
    exceptions to this, which we review here.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear programming played an interesting role in training neural networks before
    SGD became the predominant training method and provided an efficient approach
    for constructing neural networks with 1 hidden layer in the 90s. This method has
    some common points in their polyhedral approach with the first known algorithm
    that can solve ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to provable
    optimality for a 1-hidden-layer ReLU neural network, which was proposed in 2018\.
    Recently, a stream of work has exploited similar polyhedral structures to obtain
    convex optimization reformulations of regularized training problems of ReLU networks.
    Linear programming tools have also been used within SGD-type methods in order
    to compute optimal *step-sizes* in the optimization of ([11](#S5.E11 "In 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) or to strictly enforce structure in $\Theta$. From a different
    perspective, a *data-independent* polytope was used to describe approximately
    all training problems that can arise from an uncertainty set. Additionally, a
    back-propagation-like algorithm for training neural networks, which solves mixed-integer
    linear problems in each layer, was proposed as an alternative to SGD. Furthermore,
    when the neural network weights are required to be discrete, the applicability
    of SGD is impaired, and mixed-integer linear models have been proposed to tackle
    the corresponding training problems.'
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we review these roles of (mixed-integer) linear programming
    and polyhedral theory within training contexts. We refer the reader to the book
    by Goodfellow et al. ([2016](#bib.bib125)) and the surveys by Curtis and Scheinberg
    ([2017](#bib.bib70)), Bottou et al. ([2018](#bib.bib38)), and Wright ([2018](#bib.bib337))
    for in-depth descriptions and analyses of the most commonly used training methods
    for neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We remark that solving the training problem to global optimality for ReLU neural
    networks is computationally complex. Even in architectures with just one hidden
    node, the problem is NP-hard (Dey et al., [2020](#bib.bib81), Goel et al., [2021](#bib.bib121)).
    Also see Blum and Rivest ([1992](#bib.bib33)), Boob et al. ([2022](#bib.bib36)),
    Chen et al. ([2022c](#bib.bib52)), Froese et al. ([2022](#bib.bib110)), Froese
    and Hertrich ([2023](#bib.bib109)) for other hardness results. Furthermore, it
    has been recently shown that training ReLU networks is $\exists\mathbb{R}$-complete
    (Abrahamsen et al., [2021](#bib.bib1), Bertschinger et al., [2022](#bib.bib27)),
    which implies that it is likely that the problem of optimally training ReLU neural
    networks is not even in NP. Therefore, it is not strange to see that some of the
    methods we review below, even when they are solving hard problems as sub-routines
    (like mixed-integer linear problems), either make some non-trivial assumptions
    or relax some requirements. For example, boundedness and/or integrality of the
    weights, architecture restrictions such as the output dimension, or not having
    optimality guarantees.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that, in contrast, for LTUs, exact exponential-time training
    algorithms are known for much more general architectures than in the ReLU case
    (Khalife and Basu, [2022](#bib.bib173), Ergen et al., [2023](#bib.bib97)). These
    are out of scope for this survey, though we will provide a high-level overview
    of some of them, as they share some similarities to approaches designed for ReLU
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Training neural networks with a single hidden layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following the well-known XOR limitation of the perceptron (Minsky and Papert,
    [1969](#bib.bib219)), a natural interest arose in the development of training
    algorithms that could handle at least one hidden layer. In this section, we review
    training algorithms that can successfully minimize the training error in a one-hidden-layer
    setting and rely on polyhedral approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Problem setting and solution scheme
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we have a sample of size $D$ $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$
    where $\tilde{{\bm{x}}}_{i}\in\mathbb{R}^{n}$ and $\tilde{{\bm{y}}}_{i}\in\mathbb{R}$.
    In a training phase, we would like to find a neural network function $f(\cdot,\cdot,\cdot)$
    that represents in the best possible way the relation $f(\tilde{{\bm{x}}}_{i},{\bm{W}},{\bm{b}})=\tilde{{\bm{y}}}_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when a neural network $\hat{f}$ has only one hidden layer, its behavior
    is almost completely determined by the sign of each component of the vector
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}^{1}x-{\bm{b}}^{1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: These are the cases of ReLU activations $\sigma(z)=\max\{0,z\}$ and LTU activations
    $\sigma(z)=\text{sgn}(z)$. The training algorithms we show here heavily exploit
    this observation and construct $({\bm{W}}^{1},{\bm{b}}^{1})$ by embedding in this
    phase a *hyperplane partition* problem based on the sample $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$.
    While the focus of this survey is mainly devoted to ReLU activations, we also
    discuss some selected cases with LTU activations as they share some similar ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 LTU activations and variable number of nodes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One stream of work dedicated to developing training algorithms for one-hidden-layer
    networks concerned the use of *backpropagation* (Rumelhart et al., [1986](#bib.bib266),
    LeCun et al., [1989](#bib.bib185), Werbos, [1974](#bib.bib331)). In the early
    90s, an alternative family of methods was proposed, which was heavily based on
    linear programs (see e.g. Bennett and Mangasarian ([1990](#bib.bib21), [1992](#bib.bib22)),
    Roy et al. ([1993](#bib.bib264)), Mukhopadhyay et al. ([1993](#bib.bib227))).
    These approaches can construct a 1-hidden-layer network without the need for an
    *a-priori* number of nodes in the network. We illustrate the high-level idea of
    these next, based on the survey by Mangasarian ([1993](#bib.bib209)).
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$, thus the NN we construct will
    be a classifier. The training phase can be tackled via the construction of a *polyhedral
    partition* of $\mathbb{R}^{n}$ such that no two points (or few) $\tilde{{\bm{x}}}_{i}$
    and $\tilde{{\bm{x}}}_{j}$ such that $\tilde{{\bm{y}}}_{i}\neq\tilde{{\bm{y}}}_{j}$
    lie in the same element of the partition. To achieve this, the following approach
    presented by Bennett and Mangasarian ([1992](#bib.bib22)) can be followed. Let
    $Y=\{i\in[D]\,:\,\tilde{{\bm{y}}}_{i}=1\}$ and $N=[D]\setminus Y$, and consider
    the following optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{{\bm{w}},b,y,z}\quad$ | $\displaystyle\frac{1}{&#124;Y&#124;}\sum_{i\in
    Y}y_{i}+\frac{1}{&#124;N&#124;}\sum_{i\in N}z_{i}$ |  | (12a) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+y\geq 1$ | $\displaystyle\forall
    i\in Y$ |  | (12b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+z\geq 1$ | $\displaystyle\forall
    i\in N$ |  | (12c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle y,z\geq 0.$ |  | (12d) |'
  prefs: []
  type: TYPE_TB
- en: This LP aims at finding a hyperplane ${\bm{w}}^{\top}x=b$ separating the data
    according to their value of $\tilde{{\bm{y}}}_{i}$. Since the data may not be
    separable, the LP is minimizing the following classification error
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\frac{1}{&#124;Y&#124;}\sum_{i\in Y}(-{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b+1)_{+}+\frac{1}{&#124;N&#124;}\sum_{i\in
    N}({\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}-b+1)_{+}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of nodes
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is a linear reformulation of the latter minimization problem, where
    the auxiliary values $y,z$ take the value of each element in the sum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the LP ([12](#S5.E12 "In 5.1.2 LTU activations and variable number of
    nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is solved, we obtain 2 halfspaces classifying our data points. In
    order to obtain a richer classification and lower error, we can iterate the procedure
    by means of the Multi-Surface Method Tree (MSMT, see Bennett ([1992](#bib.bib20))),
    which solves a sequence of LPs as ([12](#S5.E12 "In 5.1.2 LTU activations and
    variable number of nodes ‣ 5.1 Training neural networks with a single hidden layer
    ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) in order to produce a polyhedral partition
    of $\mathbb{R}^{n}$. Let us illustrate how this procedure works in a simplified
    case: assume that solving ([12](#S5.E12 "In 5.1.2 LTU activations and variable
    number of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) results in a vector ${\bm{w}}_{1}$ such that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq b_{1}\}\subseteq
    Y\quad\land\quad\{i\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq a_{1}\}\subseteq
    N,$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'for some $a_{1},b_{1}\in\mathbb{R}^{n}$ with $b_{1}>a_{1}$. We can remove the
    sets $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\geq
    b_{1}\}$ and $\{(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\,:\,{\bm{w}}_{1}^{\top}\tilde{{\bm{x}}}_{i}\leq
    a_{1}\}$ from the data-set and redefine ([12](#S5.E12 "In 5.1.2 LTU activations
    and variable number of nodes ‣ 5.1 Training neural networks with a single hidden
    layer ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) accordingly, in order to obtain a new vector
    ${\bm{w}}_{2}$ and scalars $b_{2},a_{2}$ that would be used to classify within
    the region $\{x\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1}\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This procedure can be iterated, and the polyhedral partition of $\mathbb{R}^{n}$
    induced by the resulting hyperplanes can be easily transformed into a Neural Network
    with 1 hidden layer and LTU activations (see Bennett and Mangasarian ([1990](#bib.bib21))
    for details). We illustrate this transformation with the following example: suppose
    that after 3 iterations we have the following regions, with the arrow indicating
    to which class each region is associated to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\geq
    b_{1}\}\to Y,$ |  | (13a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,{\bm{w}}_{1}^{\top}{\bm{x}}\leq
    a_{1}\}\to N,$ |  | (13b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\geq
    b_{2}\}\to Y,$ |  | (13c) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,{\bm{w}}_{2}^{\top}{\bm{x}}\leq
    a_{2}\}\to N,$ |  | (13d) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x\geq(a_{3}+b_{3})/2\}\to
    Y,$ |  | (13e) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\{{\bm{x}}\in\mathbb{R}^{n}\,:\,a_{1}<{\bm{w}}_{1}^{\top}{\bm{x}}<b_{1},\,a_{2}<{\bm{w}}_{2}^{\top}{\bm{x}}<b_{2},\,{\bm{w}}_{3}^{\top}x<(a_{3}+b_{3})/2\}\to
    N.$ |  | (13f) |'
  prefs: []
  type: TYPE_TB
- en: 'Since regions ([13e](#S5.E13.5 "In 13 ‣ 5.1.2 LTU activations and variable
    number of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) and ([13f](#S5.E13.6 "In 13 ‣ 5.1.2 LTU activations
    and variable number of nodes ‣ 5.1 Training neural networks with a single hidden
    layer ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) are the last defined by the algorithm (under
    some stopping criterion), they both use $(a_{3}+b_{3})/2$ in order to obtain a
    well-defined partition of $\mathbb{R}^{n}$. In Figure [11](#S5.F11 "Figure 11
    ‣ 5.1.2 LTU activations and variable number of nodes ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey") we show a one-hidden-layer
    neural network that represents such a classifier. The structure of the neural
    network can be easily extended to handle more regions.'
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S5.F11.pic1" class="ltx_picture ltx_centering" height="226.19" overflow="visible"
    version="1.1" width="260.93"><g transform="translate(0,226.19) matrix(1 0 0 -1
    0 0) translate(12.28,0) translate(0,113.19)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -7.39 -14.98)" fill="#000000"
    stroke="#000000"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 25.435)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 20.91)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><g class="ltx_tikzmatrix"
    transform="matrix(1 0 0 -1 0 14.685)"><g class="ltx_tikzmatrix_row" transform="matrix(1
    0 0 1 0 8.45)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1
    0 0 -1 0 0)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 4.225)"><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 5.96)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="11.78"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{1}$</foreignobject></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 18.14)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="10.38"
    height="12.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\vdots$</foreignobject></g></g></g></g></g><g
    class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 26.87)"><g class="ltx_tikzmatrix_col
    ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject width="14.77"
    height="9.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{x}}_{n_{0}}$</foreignobject></g></g></g></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 94.87)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 57.32)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 113.2 16.13)" fill="#000000" stroke="#000000"><foreignobject
    width="9.81" height="12.11" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$b_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 108.67 -21.42)" fill="#000000" stroke="#000000"><foreignobject
    width="18.88" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-a_{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 109.73 -62.25)" fill="#000000" stroke="#000000"><foreignobject
    width="22.35" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{a_{3}+b_{3}}{2}$</foreignobject></g><g
    transform="matrix(0.75 0.0 0.0 0.75 110.13 -101.62)" fill="#000000" stroke="#000000"><foreignobject
    width="21.27" height="14.75" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\frac{-a_{3}-b_{3}}{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 232.76 -4.46)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$0$</foreignobject></g></g><g
    stroke-width="0.8pt"><g transform="matrix(1.0 0.0 0.0 1.0 52.52 38.54)" fill="#000000"
    stroke="#000000"><foreignobject width="14.15" height="8.45" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{1}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 48.27 17.71)" fill="#000000" stroke="#000000"><foreignobject width="18.77"
    height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.93 -1.46)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 47.5 -20.6)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 51.38 -40.35)" fill="#000000" stroke="#000000"><foreignobject
    width="14.15" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">${\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 50.16 -60.74)" fill="#000000" stroke="#000000"><foreignobject
    width="18.77" height="8.45" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-{\bm{w}}_{3}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.59 31.95)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 169.9 11.96)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{2}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 172.82 -6.91)" fill="#000000" stroke="#000000"><foreignobject
    width="10.79" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.1 -26.06)" fill="#000000" stroke="#000000"><foreignobject
    width="18.48" height="11.41" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-2^{1}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 174.88 -43.02)" fill="#000000" stroke="#000000"><foreignobject
    width="6.92" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$1$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 170.73 -62.57)" fill="#000000" stroke="#000000"><foreignobject
    width="14.61" height="8.92" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$-1$</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Illustration of Neural Network with LTU activations using MSMT.
    Inside each node of the hidden layer, we show the thresholds used in each LTU
    activation.'
  prefs: []
  type: TYPE_NORMAL
- en: For other details, we refer the reader to Bennett ([1992](#bib.bib20)), and
    for variants and extensions see Mangasarian ([1993](#bib.bib209)) and references
    therein.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some key features of this procedure are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each solution of ([12](#S5.E12 "In 5.1.2 LTU activations and variable number
    of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")), i.e., each new hyperplane, can be represented as a new node
    in the hidden layer of the resulting neural network.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The addition of a new hyperplane comes with a reduction in the current loss;
    this can be iterated until a target loss is met.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thanks to the universal approximation theorem (Hornik et al., [1989](#bib.bib153)),
    with enough nodes in the hidden layer, we can always obtain a neural network $\hat{f}$
    with zero classification error. Although this can lead to over-fitting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The work of Roy et al. ([1993](#bib.bib264)) and Mukhopadhyay et al. ([1993](#bib.bib227))
    follow a related idea, although the classifiers which are built are quadratic
    functions. To illustrate the approach, we use the same set-up for ([12](#S5.E12
    "In 5.1.2 LTU activations and variable number of nodes ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). The approach in Roy
    et al. ([1993](#bib.bib264)) and Mukhopadhyay et al. ([1993](#bib.bib227)) aims
    at finding a function'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(x)={\bm{x}}^{\top}{\bm{V}}{\bm{x}}+{\bm{w}}^{\top}{\bm{x}}+b$
    |  |'
  prefs: []
  type: TYPE_TB
- en: such that
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{{\bm{V}},{\bm{w}},b}(\tilde{{\bm{x}}}_{i})\geq 0\Longleftrightarrow
    i\in Y.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Since this may not be possible, the authors propose solving the following LP
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{W,w,b,\epsilon}\quad$ | $\displaystyle\epsilon$ |  |
    (14a) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\geq\epsilon$
    | $\displaystyle\forall i\in Y$ |  | (14b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\tilde{{\bm{x}}}_{i}^{\top}{\bm{V}}\tilde{{\bm{x}}}_{i}+{\bm{w}}^{\top}\tilde{{\bm{x}}}_{i}+b\leq-\epsilon$
    | $\displaystyle\forall i\not\in Y$ |  | (14c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\epsilon\geq\epsilon_{0}$ |  | (14d) |'
  prefs: []
  type: TYPE_TB
- en: 'for some fixed tolerance $\epsilon_{0}>0$. When this LP is infeasible, the
    class $Y$ is partitioned into $Y_{1}$ and $Y_{2}$, and an LP as ([14](#S5.E14
    "In 5.1.2 LTU activations and variable number of nodes ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) is solved for both $Y_{1}$
    and $Y_{2}$. The algorithm then follows iteratively (see below for comments on
    these iterations). In the end, the algorithm will construct $k$ quadratic functions
    $f_{1},\ldots,f_{k}$, which the authors call “masking functions”, that will classify
    an input ${\bm{x}}$ in the class $Y$ if and only if'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\exists i\in[k],\,f_{i}({\bm{x}})\geq 0.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In order to represent the resulting classifier as a single-layer neural network,
    the authors proceed in a similar manner to a linear classifier; the input layer
    of the resulting neural network not only includes each entry of ${\bm{x}}$, but
    also the bilinear terms ${\bm{x}}{\bm{x}}^{\top}$. Using this input, the classifier
    built by ([14](#S5.E14 "In 5.1.2 LTU activations and variable number of nodes
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be thought of as a linear classifier (much like a polynomial regression
    can be cast as a linear regression).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a last comment on the work of Roy et al. ([1993](#bib.bib264)) and Mukhopadhyay
    et al. ([1993](#bib.bib227)), the authors’ algorithm does not iterate in a straightforward
    fashion. They add clustering iterations alternating with the steps described above
    in order to (a) identify outliers and remove them from the training set, and (b)
    subdivide the training data when ([14](#S5.E14 "In 5.1.2 LTU activations and variable
    number of nodes ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) is infeasible. These additions allow them to obtain
    a polynomial-time algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: The methods described in this section are able to produce a neural network with
    arbitrary quality, however, there is no guarantee on the size of the resulting
    neural network. When the size of the network is fixed the story changes, which
    is the case we describe next.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Fixed number of nodes and ReLU activations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned at the beginning of this section, training a neural network is
    a complex optimization problem in general, with some results indicating that the
    problem is likely to not even be in NP (Abrahamsen et al., [2021](#bib.bib1),
    Bertschinger et al., [2022](#bib.bib27)).
  prefs: []
  type: TYPE_NORMAL
- en: Nonetheless, by restricting the network architecture sufficiently and allowing
    exponential running times, exact algorithms can be conceived. An important step
    in the construction of such algorithms was taken by Arora et al. ([2018](#bib.bib8)).
    In this work, the authors studied the training problem in detail, providing the
    first optimization algorithm capable of solving the training problem to provable
    optimality for a fixed network architecture with one hidden layer and with an
    output dimension of 1\. As we anticipated, this algorithm shares some similarities
    with the previous approach.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider now a ReLU activation. Also, we no longer assume $\tilde{{\bm{y}}}_{i}\in\{-1,1\}$,
    but we keep the output dimension as 1\. The problem considered by Arora et al.
    ([2018](#bib.bib8)) reads
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}},{\bm{b}}}\frac{1}{D}\sum_{i=1}^{D}\ell({\bm{W}}^{2}(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i}),$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: with $\ell:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$ a convex loss. Note that
    this problem, even if $\ell$ is convex, is a non-convex optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1 (Arora et al. ([2018](#bib.bib8)))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let $n_{1}$ be the number of nodes in the hidden layer. There exists an algorithm
    to find a global optimum of ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and
    ReLU activations ‣ 5.1 Training neural networks with a single hidden layer ‣ 5
    Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) in time $O(2^{n_{1}}D^{{n_{0}}\cdot n_{1}}\text{poly}(D,{n_{0}},n_{1}))$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Roughly speaking, the algorithm works by noting that one can assume the weights
    in ${\bm{W}}^{2}$ are in $\{-1,1\}$, since $\sigma$ is positively-homogeneous.
    Thus, problem ([15](#S5.E15 "In 5.1.3 Fixed number of nodes and ReLU activations
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be restated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}}^{1},{\bm{b}}^{1},s}\frac{1}{D}\sum_{i=1}^{D}\ell(s(\sigma({\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1})),\tilde{{\bm{y}}}_{i})$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'where $s\in\{-1,1\}^{n_{1}}$. In order to handle the non-linearity, Arora et al.
    ([2018](#bib.bib8)) “guess” the values of $s$ and the sign of each component of
    ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Enforcing a sign for each component
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$ is similar to the approach
    discussed in the previous section: it fixes how the input part of the data $(\tilde{{\bm{x}}}_{i})_{i=1}^{D}$
    is partitioned in polyhedral regions by a number of hyperplanes. The difference
    is that, in this case, the number of hyperplanes to be used is assumed to be fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the hyperplane arrangement theorem (see e.g. (Matousek, [2002](#bib.bib215),
    Proposition 6.1.1)), there are at most $D^{{n_{0}}n_{1}}$ ways of fixing the signs
    of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$. Additionally, there are at
    most $2^{n_{1}}$ possible vectors in $\{-1,1\}^{n_{1}}$. Once these components
    are fixed, ([16](#S5.E16 "In 5.1.3 Fixed number of nodes and ReLU activations
    ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) can be solved as an optimization problem with a convex objective function
    and a polyhedral feasible region imposing the desired signs in ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$.
    This results in the $O(2^{n_{1}}D^{{n_{0}}n_{1}}\text{poly}(D,{n_{0}},n_{1}))$
    running time. This algorithm was recently generalized to concave loss functions
    by Froese et al. ([2022](#bib.bib110)).'
  prefs: []
  type: TYPE_NORMAL
- en: Dey et al. ([2020](#bib.bib81)) developed a polynomial-time approximation algorithm
    in this setting for the case of $n_{1}=1$ (i.e., one ReLU neuron) and square loss.
    This approximation algorithm has a better performance when the input dimension
    is much larger than the sample size, i.e. ${n_{0}}\gg D$. The approach by Dey
    et al. ([2020](#bib.bib81)) also relies on fixing the signs of ${\bm{W}}^{1}\tilde{{\bm{x}}}_{i}+{\bm{b}}^{1}$,
    and then solving multiple convex optimization problems, but in different strategy
    than that of Arora et al. ([2018](#bib.bib8)); in particular, Dey et al. ([2020](#bib.bib81))
    only explore a polynomial number of the possible “fixings”, which yields the approximation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We note that the result by Arora et al. ([2018](#bib.bib8)) shows that the
    training problem on their architecture is in NP. This is in contrast to Bertschinger
    et al. ([2022](#bib.bib27)), who show that training a neural network with one
    hidden layer is likely to not be in NP. The big difference lies in the assumption
    on the output dimension: in the case of Bertschinger et al. ([2022](#bib.bib27)),
    the output dimension is two. It is quite remarkable that such a sharp complexity
    gap is produced by a small change in the output dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4 An exact training algorithm for arbitrary LTU architectures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, Khalife and Basu ([2022](#bib.bib173)) presented a new algorithm,
    akin to that in Arora et al. ([2018](#bib.bib8)), capable of solving the training
    problem to global optimality for any fixed LTU architecture with a convex loss
    function $\ell$. In this case, no assumption on the network’s depth is made. The
    algorithm runs in polynomial time on the sample size $D$ when the architecture
    is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: We will not describe this approach in detail, as it heavily relies on the structure
    given by LTU activations, which is intricate and beyond the scope of this survey.
    Although we note that it shares some high-level similarities to the algorithm
    of Arora et al. ([2018](#bib.bib8)) for ReLU activations, such as “guessing” the
    behavior of the neurons’ activity and then solving multiple convex optimization
    problems. However, the structural and algorithmic details are considerably different.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this result reveals the big gap between what is
    known for LTU versus ReLU activations in terms of their training problems. In
    the case of the former, there is an exact algorithm for arbitrary architectures;
    in the case of the latter, the known results are much more restricted and strong
    computational limitations exist.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Convex reformulations in regularized training problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the case when the training problem is regularized, the following stream
    of work has developed several convex reformulations of it. Pilanci and Ergen ([2020](#bib.bib248))
    presented the first convex reformulation of a training problem for the case with
    one hidden layer and one-dimensional outputs. As the approach described in Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training
    neural networks with a single hidden layer ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey"), this
    reformulation uses hyperplane arrangements according to the activation patterns
    of the ReLU units, but instead of using them algorithmically directly, they use
    them to find their convex reformulations. This framework was further extended
    to CNNs by Ergen and Pilanci ([2021c](#bib.bib93)). Higher-dimensional outputs
    in neural networks with one hidden layer were considered in Ergen and Pilanci
    ([2020](#bib.bib90), [2021a](#bib.bib91)), Sahiner et al. ([2021](#bib.bib268)).
    This convex optimization perspective was also applied in Batch Normalization by
    Ergen et al. ([2022](#bib.bib96)).'
  prefs: []
  type: TYPE_NORMAL
- en: These approaches provide polynomial-time algorithms when some parameters (e.g.,
    the input dimension ${n_{0}}$) are considered constant. We note that this does
    not contradict the hardness result of Froese and Hertrich ([2023](#bib.bib109)),
    as the latter does not include a regularizing term. We explain below where the
    regularizing term plays an important role. Training via convex optimization was
    further developed to handle deeper regularized neural networks in Ergen and Pilanci
    ([2021b](#bib.bib92), [d](#bib.bib94), [e](#bib.bib95)).
  prefs: []
  type: TYPE_NORMAL
- en: In what follows, we review the convex reformulation in Pilanci and Ergen ([2020](#bib.bib248))
    (one hidden layer and one-dimensional output) to illustrate some of the base strategies
    behind these approaches. We refer the reader to the previously mentioned articles
    for the most recent and intricate developments, as well as numerical experiments.
  prefs: []
  type: TYPE_NORMAL
- en: As before, let $n_{1}$ be the number of nodes in the hidden layer. Let us consider
    the following regularized training problem; to simplify the discussion, we omit
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{\beta}{2}\sum_{j=1}^{n_{1}}(\&#124;{\bm{W}}^{1}_{j}\&#124;^{2}+({\bm{W}}^{2}_{j})^{2})$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $\beta>0$, $\tilde{{\bm{X}}}$ is a matrix whose $i$-th row is $\tilde{{\bm{x}}}_{i}$
    and ${\bm{W}}^{1}_{j}$ is the vector of weights going into neuron $j$. Thus, $\tilde{{\bm{X}}}{\bm{W}}^{1}_{j}$
    is a vector whose $i$-th component is the input to neuron $j$ when evaluating
    the network on $\tilde{{\bm{x}}}_{i}$. ${\bm{W}}^{2}_{j}$ is a scalar: it is the
    weight on the arc from neuron $j$ to the output neuron (one-dimensional). Note
    that there is a slight notation overload: $({\bm{W}}^{2}_{j})^{2}$ is the square
    of the scalar ${\bm{W}}^{2}_{j}$. However, we will quickly remove this (pontentially
    confusing) term.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem ([17](#S5.E17 "In 5.2 Convex reformulations in regularized training
    problems ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is a regularized version of ([15](#S5.E15
    "In 5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) when $\ell$ is the squared
    difference. We modified its presentation to match the structure in Pilanci and
    Ergen ([2020](#bib.bib248)). The authors first prove that ([17](#S5.E17 "In 5.2
    Convex reformulations in regularized training problems ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is equivalent to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\&#124;{\bm{W}}^{1}_{j}\&#124;\leq 1}\min_{{\bm{W}}^{2}_{j}}\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{n_{1}}{\bm{W}}^{2}_{j}\sigma(\tilde{{\bm{X}}}{\bm{W}}^{1}_{j})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{n_{1}}&#124;{\bm{W}}^{2}_{j}&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Then, through a series of reformulations and duality arguments, the authors
    first show that this problem is lower bounded by
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\max$ | $\displaystyle\quad-\frac{1}{2}\left\&#124;v-\tilde{{\bm{y}}}\right\&#124;^{2}+\frac{1}{2}\&#124;\tilde{{\bm{y}}}\&#124;^{2}$
    |  | (18a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t | $\displaystyle\quad&#124;v^{\top}\sigma(\tilde{{\bm{X}}}u)&#124;\leq\beta$
    | $\displaystyle\forall u,\,\&#124;u\&#124;\leq 1$ |  | (18b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad v\in\mathbb{R}^{D}$ |  | (18c) |'
  prefs: []
  type: TYPE_TB
- en: 'Problem ([18](#S5.E18 "In 5.2 Convex reformulations in regularized training
    problems ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) has $D$ variables and infinitely
    many constraints. The authors show that this lower bound is tight when the number
    of neurons in the hidden layer is large enough; specifically, they require $n_{1}\geq
    m^{*}$, where $m^{*}\in\{1,\ldots,D\}$ is defined as the number of Dirac deltas
    in an optimal solution of a dual of ([18](#S5.E18 "In 5.2 Convex reformulations
    in regularized training problems ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) (see Pilanci
    and Ergen ([2020](#bib.bib248)) for details).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding the presence of infinitely many constraints, the authors address
    this by considering all possible patterns of signs of $\tilde{{\bm{X}}}u$ (similarly
    to Arora et al. ([2018](#bib.bib8)), as discussed in Section [5.1.3](#S5.SS1.SSS3
    "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training neural networks
    with a single hidden layer ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). For each fixed sign
    pattern (hyperplane arrangement), they apply a duality argument which allows them
    to recast the constraint $\max_{u\in\mathcal{B}}|v^{\top}\sigma(\tilde{{\bm{X}}}u)|\leq\beta$
    as a finite collection of second-order cone constraints with $\beta$ on the right-hand
    side.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, using that $\beta>0$, they show that the reformulated problem satisfies
    Slater’s condition, and thus from strong duality they obtain the following convex
    optimization problem, which has the same objective value as ([18](#S5.E18 "In
    5.2 Convex reformulations in regularized training problems ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min$ | $\displaystyle\quad\frac{1}{2}\left\&#124;\sum_{j=1}^{P}M_{i}\tilde{{\bm{X}}}(v_{i}-w_{i})-\tilde{{\bm{y}}}\right\&#124;^{2}+\beta\sum_{j=1}^{P}(\&#124;v_{i}\&#124;+\&#124;w_{i}\&#124;)$
    |  | (19a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}v_{i}\geq 0$ |
    $\displaystyle\forall i\in[P]$ |  | (19b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad(2M_{i}-I_{D})\tilde{{\bm{X}}}w_{i}\geq 0$ | $\displaystyle\forall
    i\in[P]$ |  | (19c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad v_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19d) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\quad w_{i}\in\mathbb{R}^{n_{0}}$ | $\displaystyle\forall
    i\in[P]$ |  | (19e) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $I_{D}$ is the $D\times D$ identity matrix, $P$ is the number of possible
    activation patterns for $\tilde{{\bm{X}}}$, and each $M_{i}$ is a $D\times D$
    binary diagonal matrix whose diagonal indicates the $i$-th possible sign pattern
    of $\tilde{{\bm{X}}}u$. This means that $(M_{i})_{j,j}$ is 1 if and only if $\tilde{{\bm{x}}}_{j}^{\top}u\geq
    0$ in the $i$-th sign pattern of $\tilde{{\bm{X}}}u$. Moreover, the authors provide
    a formula to recover a solution of ([17](#S5.E17 "In 5.2 Convex reformulations
    in regularized training problems ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) from a solution
    of ([19](#S5.E19 "In 5.2 Convex reformulations in regularized training problems
    ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using that $P\leq 2r(e(D-1)/r)^{r}$, where $r=\mbox{rank}(\tilde{{\bm{X}}})$,
    the authors note that the formulation ([19](#S5.E19 "In 5.2 Convex reformulations
    in regularized training problems ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) yields a
    training algorithm with complexity $O({n_{0}}^{3}r^{3}(D/r)^{3r})$. Note that
    if one fixes $r$, the resulting algorithm runs polynomial time. In particular,
    fixing ${n_{0}}$ fixes the rank of $\tilde{{\bm{X}}}$ and results in a polynomial
    time algorithm as well. In contrast, the algorithm by Arora et al. ([2018](#bib.bib8))
    discussed in Section [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU
    activations ‣ 5.1 Training neural networks with a single hidden layer ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey") remains exponential even after fixing ${n_{0}}$. Moreover,
    Froese and Hertrich ([2023](#bib.bib109)) showed that the training problem is
    NP-Hard even for fixed ${n_{0}}$. This apparent contradiction is explained by
    two key components of the convex reformulation: the regularization term and the
    presence of a “large enough” number of hidden neurons. This facilitates the exponential
    improvement of the training algorithm with respect to Arora et al. ([2018](#bib.bib8)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Frank-Wolfe in DNN training algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another stream of work that has included components of linear programming in
    DNN training involves the Frank-Wolfe method. We briefly describe this method
    in the non-stochastic version next. In this section, we omit the biases ${\bm{b}}$
    to simplify the notation.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient descent (and its variants) is designed for problems of the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{{\bm{W}}\in\mathbb{R}^{N}}\mathcal{L}({\bm{W}})$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: and it is based on iterations of the form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))$ |  |
    (21) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{i}$ is known as the *learning rate*. In the stochastic versions,
    $\nabla\mathcal{L}({\bm{W}}(i))$ is replaced by a stochastic gradient. In this
    setting, these algorithms would find a local minimum, which is global when $\mathcal{L}$
    is convex.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the presence of constraints ${\bm{W}}\in\Theta$, however, this strategy
    may not work directly. A regularizing term is typically used in the objective
    function instead of a constraint, that “encourages” ${\bm{W}}\in\Theta$ but does
    not enforce it. If we strictly require that ${\bm{W}}\in\Theta\neq\mathbb{R}^{n}$,
    and $\Theta$ is a convex set, one could modify ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)=\text{proj}_{\Theta}\left({\bm{W}}(i)-\alpha_{i}\nabla\mathcal{L}({\bm{W}}(i))\right).$
    |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: and thus ensure that all iterates ${\bm{W}}(i)\in\Theta$. Unfortunately, a projection
    is a costly routine. An alternative to this projection is the Frank-Wolfe method
    (Frank et al., [1956](#bib.bib108)). Here, a direction ${\bm{d}}_{i}$ is computed
    via the following linear-objective convex optimization problem
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{d}}_{i}\in\arg\min_{{\bm{d}}\in\Theta}{\bm{v}}_{i}^{\top}{\bm{d}}$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: where normally ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ (we consider variants
    below). The update is then computed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)={\bm{W}}(i)+\alpha_{i}({\bm{d}}_{i}-{\bm{W}}(i)),$ |  |
    (24) |'
  prefs: []
  type: TYPE_TB
- en: 'for $\alpha_{i}\in[0,1]$. Note that, by convexity, we are assured that ${\bm{W}}(i+1)\in\Theta$
    as long as ${\bm{W}}(0)\in\Theta$. In many applications, $\Theta$ is polyhedral,
    which makes ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) a linear program. Moreover, for simple sets $\Theta$, problem
    ([23](#S5.E23 "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) admits closed-form solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of deep neural network training, two notable applications of
    Frank-Wolfe have appeared. Firstly, the Deep Frank Wolfe algorithm, by Berrada
    et al. ([2018](#bib.bib26)), which modifies iteration ([21](#S5.E21 "In 5.3 Frank-Wolfe
    in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) with an optimization
    problem that can be solved using Frank-Wolfe in its dual. Secondly, the use of
    a stochastic version of Frank-Wolfe in the training problem ([11](#S5.E11 "In
    5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets
    Polyhedral Theory: A Survey")) by Pokutta et al. ([2020](#bib.bib249)) and Xie
    et al. ([2020a](#bib.bib342)), which enforces structure in the neural network
    weights directly. We review these next, starting with the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Stochastic Frank-Wolfe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Note that problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) is of the
    form ([20](#S5.E20 "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) with'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}({\bm{W}})=\frac{1}{D}\sum_{i=1}^{D}\ell(f(\tilde{{\bm{x}}}_{i},{\bm{W}}),\tilde{{\bm{y}}}_{i}).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: We remind the reader that we are omitting the biases in this section to simplify
    notation, as they can be incorporated as part of ${\bm{W}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, some structure of the weights is commonly desired, (e.g. sparsity or
    boundedness), which traditionally have been incorporated as regularizing terms
    in the objective, as mentioned above. The recent work by Xie et al. ([2020a](#bib.bib342))
    and Pokutta et al. ([2020](#bib.bib249)), on the other hand, enforce structure
    on $\Theta$ directly using Frank-Wolfe —more precisely, stochastic versions of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Xie et al. ([2020a](#bib.bib342)) use a stochastic Frank-Wolfe approach to impose
    an $\ell_{1}$-norm constraint on the weights and biases ${\bm{W}}$ when training
    a neural network with 1 hidden layer. Note that $\ell_{1}$ constraints are polyhedral.
    Their algorithm is designed for a general Online Convex Optimization setting,
    where “losses” are revealed in each iteration. However, in their computational
    experiments, they included tests in an offline setting given by a DNN training
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach follows the Frank-Wolfe method described above closely. The key
    difference lies in the estimation of the stochastic gradient they use, which is
    not standard and it is one of the most important aspects of the algorithm. Instead
    of using ${\bm{v}}_{i}=\nabla\mathcal{L}({\bm{W}}(i))$ in ([23](#S5.E23 "In 5.3
    Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), the following
    *stochastic recursive estimator* of the gradient is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\bm{v}}_{0}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(0))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{v}}_{i}=$ | $\displaystyle\tilde{\nabla}\mathcal{L}({\bm{W}}(i))+(1-\rho_{i})(v_{i-1}-\tilde{\nabla}\mathcal{L}({\bm{W}}(i-1)))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tilde{\nabla}\mathcal{L}$ is a stochastic gradient, and $\rho_{i}$ is
    a parameter. The authors show that the gradient approximation error of this estimator
    converges to 0 at a sublinear rate, with high probability. This is important for
    them to analyze the “regret bounds” they provide for the online setting.
  prefs: []
  type: TYPE_NORMAL
- en: The experimental results in Xie et al. ([2020a](#bib.bib342)) in DNN training
    are very positive. They test their approach in the MNIST and CIFAR10 datasets
    and outperform existing state-of-the-art approaches in terms of suboptimality,
    training accuracy, and test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Pokutta et al. ([2020](#bib.bib249)) implement and test several variants of
    stochastic versions of Frank-Wolfe in the training of neural networks, including
    the approach by Xie et al. ([2020a](#bib.bib342)). Pokutta et al. ([2020](#bib.bib249))
    focus their experiments on their main proposed variant, which they refer to simply
    as Stochastic Frank-Wolfe (SFW). This variant uses
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{v}}_{i}=(1-\rho_{i}){\bm{v}}_{i-1}+\rho_{i}\tilde{\nabla}\mathcal{L}({\bm{W}}(i)),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\rho_{i}$ is a momentum parameter. The authors propose many different
    options for $\Theta$ including $\ell_{1},\ell_{2}$ and $\ell_{\infty}$ balls,
    and $K$-sparse polytopes. Of these, only the $\ell_{2}$ ball is non-polyhedral.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the computational experiments are promising for SFW. The authors advocate
    for this algorithm arguing that it provides excellent computational performances
    while being simple to implement and competitive with other state-of-the-art algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Deep Frank-Wolfe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another application of Frank-Wolfe within DNN training was proposed by Berrada
    et al. ([2018](#bib.bib26)). While this approach does not make heavy use of linear
    programming techniques, the application of Frank-Wolfe is quite novel, and they
    do rely on one linear program needed when performing an update as ([24](#S5.E24
    "In 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors note that ([21](#S5.E21 "In 5.3 Frank-Wolfe in DNN training algorithms
    ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey")) can also be written as the solution to the
    following *proximal* problem (Bubeck et al., [2015](#bib.bib40)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\mathcal{T}_{{\bm{W}}(i)}(\mathcal{L}({\bm{W}}))\right\}$
    |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{T}_{{\bm{W}}(i)}$ represents the first-order Taylor expansion
    at ${\bm{W}}(i)$. We are omitting regularizing terms since they do not play a
    fundamental role in the approach; all this discussion can be directly extended
    to include regularizers. Berrada et al. ([2018](#bib.bib26)) note that ([25](#S5.E25
    "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) linearizes the loss function, and propose the following *loss-preserving
    proximal* problem to replace ([25](#S5.E25 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe
    in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{W}}(i+1)=\arg\min_{{\bm{W}}}\,\left\{\frac{1}{2\alpha_{i}}\&#124;{\bm{W}}-{\bm{W}}(i)\&#124;^{2}+\frac{1}{D}\sum_{i=1}^{D}\ell(\mathcal{T}_{{\bm{W}}(i)}(f(\tilde{{\bm{x}}}_{i},{\bm{W}})),\tilde{{\bm{y}}}_{i})\right\}$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: 'Using the results by Lacoste-Julien et al. ([2013](#bib.bib182)), the authors
    argue that ([26](#S5.E26 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training
    algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) is amenable to Frank-Wolfe in the
    dual when $\ell$ is piecewise linear and convex (e.g. the hinge loss). To be more
    specific, the authors show that in this case, and assuming $\alpha_{i}=\alpha$,
    there exists ${\bm{A}},{\bm{b}}$ such that the dual of ([26](#S5.E26 "In 5.3.2
    Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) is simply'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\max_{\mathbf{\beta}}\quad$ | $\displaystyle\frac{-1}{2\alpha}\&#124;{\bm{A}}\mathbf{\beta}\&#124;^{2}+{\bm{b}}^{\top}\mathbf{\beta}$
    |  | (27a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle\mathbf{1}^{\top}\mathbf{\beta}=1$ |  | (27b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\mathbf{\beta}\geq 0$ |  | (27c) |'
  prefs: []
  type: TYPE_TB
- en: 'The authors consider applying Frank-Wolfe to this last problem, and to recover
    the primal solution using the primal-dual relation ${\bm{W}}=-{\bm{A}}\mathbf{\beta}$,
    which is a consequence of KKT. The Frank-Wolfe iteration ([24](#S5.E24 "In 5.3
    Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) in the notation
    of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training
    algorithms ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")) would look like'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\beta}_{i+1}=\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i}).$
    |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, ${\bm{d}}_{i}$ is feasible for ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe
    ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) and
    obtained using a linear programming oracle, and $\gamma_{i}$ the Frank-Wolfe step-length.
    Note that the feasible region of ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe ‣ 5.3
    Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) is a simplex:
    exploiting this, the authors show that an optimal $\gamma_{i}$ can be computed
    in closed-form: here, “optimal” refers to a minimizer of ([27](#S5.E27 "In 5.3.2
    Deep Frank-Wolfe ‣ 5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")) when restricted to points of the form $\mathbf{\beta}_{i}+\gamma_{i}({\bm{d}}_{i}-\mathbf{\beta}_{i})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With all these considerations, the bottleneck in this application of Frank-Wolfe
    is obtaining ${\bm{d}}_{i}$; recall that this Frank-Wolfe routine is embedded
    within a single iteration of the overall training algorithm; therefore, in each
    iteration of the training algorithm, possibly multiple computations of ${\bm{d}}_{i}$
    would be required in order to solve ([27](#S5.E27 "In 5.3.2 Deep Frank-Wolfe ‣
    5.3 Frank-Wolfe in DNN training algorithms ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) to
    optimality. To alleviate this, the authors propose to only perform one iteration
    of Frank-Wolfe: they set ${\bm{d}}_{0}$ to be the stochastic gradient and compute
    a closed-form expression for $\mathbf{\beta}_{1}$. This is the basic ingredient
    of the Deep Frank Wolfe (DFW). It is worth noting that this algorithm is not guaranteed
    to converge, however, its empirical performance is competitive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other two important considerations are taken into account the implementation
    of this algorithm: smoothing of the loss function (as the Hinge loss is piecewise
    linear) and the adaptation of Nesterov’s Momentum to this new setting. We refer
    the reader to the corresponding article for these details. One of the key features
    of DFW is that it only requires one hyperparameter ($\alpha$) to be tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors test DFW in image classification and natural language inference.
    Overall, the results obtained by DFW are very positive: in most cases, it can
    outperform adaptive gradient methods, and it is competitive with SGD while converging
    faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Polyhedral encoding of multiple training problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the questions raised by Arora et al. ([2018](#bib.bib8)) (see Section
    [5.1.3](#S5.SS1.SSS3 "5.1.3 Fixed number of nodes and ReLU activations ‣ 5.1 Training
    neural networks with a single hidden layer ‣ 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) was
    whether the dependency on $D$ of their algorithm could be improved since it is
    typically the largest coefficient in a training problem. This question was studied
    by Bienstock et al. ([2023](#bib.bib32)), who show that, in an approximation setting,
    a more ambitious goal is achievable: there is a polyhedral encoding of multiple
    training problems whose size has a mild dependency on $D$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous section, we omit the biases ${\bm{b}}$ to simplify notation,
    as all parameters can be included in ${\bm{W}}$. Let us assume the class of neural
    networks $F$ in ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in
    Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) are restricted
    to have bounded parameters (we assume they lie in the interval $[-1,1]$), and
    let us assume the sample has been normalized in such a way that $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$.
    Furthermore, let $N$ be the dimension of $\Theta$ (the number of parameters in
    the neural network). With this notation, we define the following.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$ — sample size, parameter space, loss function, network
    architecture, respectively. For a function $g$, let $\mathcal{K}_{\infty}(g)$
    be the Lipschitz constant of $g$ using the infinity norm. We define the *Architecture
    Lipschitz Constant* $\mathcal{K}(D,\Theta,\ell,f)$ as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{K}(D,\Theta,\ell,f)\doteq\mathcal{K}_{\infty}(\ell(f(\cdot,\cdot),\cdot))$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: over the domain $[-1,1]^{n_{0}}\times\Theta\times[-1,1]^{n_{L+1}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Using this definition, and the boundedness of parameters, a straightforward
    approximate training algorithm can be devised whose running time is linear in
    $D$. Simply do a grid search in the parameters’ space, and evaluate all data points
    in each possible parameter. It is not hard to see that, to achieve $\epsilon$-optimality,
    such an algorithm would run in time which is linear in $D$ and exponential in
    $\mathcal{K}(D,\Theta,\ell,f)/\epsilon$. What was proved by Bienstock et al. ([2023](#bib.bib32))
    is that one can take a step further and represent multiple training problems at
    the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 2 (Bienstock et al. ([2023](#bib.bib32)))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral
    Theory in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) with
    parameters $D,\Theta,\ell,f$, and let $\mathcal{K}:=\mathcal{K}(D,\Theta,\ell,f)$
    be the corresponding network architecture. Consider $\epsilon>0$ arbitrary. There
    exists a polytope $P_{\epsilon}$ of size⁵⁵5Here, the size of the polytope is the
    number of variables and constraints describing it.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $O(D\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $P_{\epsilon}$ can be constructed in time $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N}D)$
    plus the time required for $O(\left(2\mathcal{K}/\epsilon\right)^{{n_{0}}+{n_{L+1}}+N})$
    evaluations of the loss function $\ell$ and $f$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *any* sample $(\tilde{X},\tilde{Y})=(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})_{i=1}^{D}$,
    $(\tilde{{\bm{x}}}_{i},\tilde{{\bm{y}}}_{i})\in[-1,1]^{{n_{0}}+{n_{L+1}}}$, there
    is a face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ of $P_{\epsilon}$ such that optimizing
    a linear function over $\mathcal{F}_{\tilde{X},\tilde{Y}}$ yields an $\epsilon$-approximation
    to the ERM problem ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The face $\mathcal{F}_{\tilde{X},\tilde{Y}}$ arises by simply substituting-in
    actual data for the data-variables $x,y$, which is used to fixed variables in
    the description of $P_{\epsilon}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This result is very abstract in nature but possesses some interesting features.
    Firstly, it encodes (approximately) *every* possible training problem arising
    from data in $[-1,1]^{{n_{0}}+{n_{L+1}}}$ using a benign dependency on $D$: the
    polytope size depends only linearly on $D$, while a discretized enumeration of
    all the possible samples of size $D$ would be exponential in $D$. Secondly, every
    possible ERM problem appears in a *face* of the polytope; this suggests a strong
    geometric structure across different ERM problems. And lastly, this result is
    applicable to a wide variety of network architectures; in order to obtain an architecture-specific
    result, it suffices to compute the corresponding value of $\mathcal{K}$ and plug
    it in. Regarding this last point, the authors computed the constant $\mathcal{K}$
    for various well-known architectures and obtained the results of Table [4](#S5.T4
    "Table 4 ‣ 5.4 Polyhedral encoding of multiple training problems ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Summary of polyhedral encoding sizes for various architectures. DNN
    refers to a fully-connected Deep Neural Network, CNN to a Convolutional Neural
    Network, and ResNet to a Residual Network. $G$ is the graph defining the Network,
    $\Delta$ is the maximum in-degree in $G$, $L$ is the number of hidden layers,
    and ${n_{\max}}$ is the maximum width of a layer.'
  prefs: []
  type: TYPE_NORMAL
- en: Type Loss Size of polytope Notes DNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ DNN Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}}){n_{\max}}^{O(k^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\in O(|E({G})|)$ CNN Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}{n_{\max}}^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    $N\ll|E({G})|$ ResNet Absolute/Quadratic/Hinge $O\big{(}\big{(}{n_{L+1}}\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
    ResNet Cross Entropy w/ Soft-Max $O\big{(}\big{(}{n_{L+1}}\log({n_{L+1}})\Delta^{O(L^{2})}/\epsilon\big{)}^{{n_{0}}+{n_{L+1}}+N}D\big{)}$
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof of this result relies on a graph theoretical concept called *treewidth*.
    This parameter is used for measuring structured sparsity, and in Bienstock and
    Muñoz ([2018](#bib.bib31)) it was proved that any optimization problem admits
    an approximate polyhedral reformulation whose size is exponential only in the
    treewidth parameter. On a high level, the neural network result is obtained by
    noting that ([11](#S5.E11 "In 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) connects different sample
    points only through a sum; therefore, the following reformulation of the optimization
    problem can be considered, which decouples the different data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{{\bm{W}}\in\Theta,{\bm{L}}}\left\{\frac{1}{D}\sum_{d=1}^{D}{\bm{L}}_{d}\,\middle&#124;\,{\bm{L}}_{d}\,=\,\ell(f(\tilde{{\bm{x}}}_{d},{\bm{W}}),\tilde{{\bm{y}}}_{d})\quad\forall\,d\in[D]\right\}$
    |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: This reformulation does not seem useful at first, however, it has a *treewidth*
    that does not depend on $D$, even if the data points are considered variables.
    From this point, the authors are able to obtain the polytope whose size does not
    depend exponentially on $D$, and which is capable of encoding all possible ERM
    problems. The face structure the polytope has is more involved, and we refer the
    reader to Bienstock et al. ([2023](#bib.bib32)) for these details.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that the polytope size provided by Bienstock et al. ([2023](#bib.bib32))
    in the setting of Arora et al. ([2018](#bib.bib8)) is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $O((2\mathcal{K}_{\infty}(\ell)n_{1}^{O(1)}/\epsilon)^{({n_{0}}+1)(n_{1})}D)$
    |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{K_{\infty}}(\ell)$ is the Lipschitz constant of the loss function
    with respect to the infinity norm over a specific domain. These two results are
    not completely comparable, but they give a good idea of how good the size of polytope
    constructed in Bienstock et al. ([2023](#bib.bib32)) is. The dependency on $D$
    is better in the polytope size, the polytope encodes multiple training problems,
    and the result is more general (it applies to almost any architecture); however,
    the polytope only gives an approximation, and its construction requires boundedness.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Backpropagation through MILP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the work by Goebbels ([2021](#bib.bib120)), a novel use of Mixed-Integer
    Linear Programming is proposed in training ReLU networks: to serve as an alternative
    to SGD. This new algorithm works as backpropagation, as it updates the weights
    of the neural network iteratively starting from the last layer. The key difference
    is that each update in a layer amounts to solving a MILP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us focus only on one hidden layer at a time (of width $n$), so we can assume
    we have an architecture as in Figure [11](#S5.F11 "Figure 11 ‣ 5.1.2 LTU activations
    and variable number of nodes ‣ 5.1 Training neural networks with a single hidden
    layer ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep Learning
    Meets Polyhedral Theory: A Survey"). Furthermore, we assume we have some target
    output vectors $\{{\bm{T}}_{d}\}_{d=1}^{D}$ (when processing the last hidden layer
    in the backpropagation, this corresponds to $\{\tilde{{\bm{y}}}_{d}\}_{d=1}^{D}$)
    and some layer input $\{{\bm{I}}_{d}\}_{d=1}^{D}$ (when processing the last hidden
    layer, this corresponds to evaluating the neural network on $\{\tilde{{\bm{x}}}_{d}\}_{d=1}^{D}$
    up to the second-to-last hidden layer). The algorithm proposed by Goebbels ([2021](#bib.bib120))
    solves the following optimization problem to update the weights ${\bm{W}}$ and
    biases ${\bm{b}}$ of the given layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\min_{{\bm{W}},\hat{{\bm{h}}},{\bm{b}},{\bm{h}},{\bm{z}}}\quad$
    | $\displaystyle\sum_{d=1}^{D}\sum_{j=1}^{n}&#124;{\bm{T}}_{d,j}-{\bm{h}}_{d,j}&#124;$
    |  | (32a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | s.t. | $\displaystyle\hat{{\bm{h}}}_{d,j}=({\bm{W}}{\bm{I}}_{d})_{j}+{\bm{b}}_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32b) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32c) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle\hat{{\bm{h}}}_{d,j}\geq-M(1-{\bm{z}}_{d,j})$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32d) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle&#124;\hat{{\bm{h}}}_{d,j}-{\bm{h}}_{d,j}&#124;\leq M(1-{\bm{z}}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32e) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\leq M{\bm{z}}_{d,j}$ | $\displaystyle
    d=1,\ldots,D,\,j=1,\ldots,n$ |  | (32f) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{h}}_{d,j}\geq 0$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$
    |  | (32g) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle{\bm{z}}_{d,j}\in\{0,1\}$ | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$
    |  | (32h) |'
  prefs: []
  type: TYPE_TB
- en: 'Here $M$ is a large constant that is assumed to bound the input to any neuron.
    Note that problem ([32](#S5.E32 "In 5.5 Backpropagation through MILP ‣ 5 Linear
    Programming and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral
    Theory: A Survey")) can easily be linearized. This optimization problem finds
    the weights (${\bm{W}}$) and biases (${\bm{b}}$) that minimize the difference
    between the “real” output of the network for each sample (${\bm{h}}_{d}$) and
    the target output (${\bm{T}}_{d}$). The auxiliary variables $\hat{{\bm{h}}}_{d,j}$
    represent the input to the each neuron —so ${\bm{h}}_{d,j}=\sigma(\hat{{\bm{h}}}_{d,j})$—
    and ${\bm{z}}_{d,j}$ indicates if the $j$-th neuron is activated on input ${\bm{I}}_{d}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When processing intermediate layers, the definition ${\bm{I}}_{d}$ can easily
    be adapted from what we mentioned above. However, the story is different for the
    case of ${\bm{T}}_{d}$. When processing the last layer, as previously mentioned,
    ${\bm{T}}_{d}$ simply corresponds to $\tilde{{\bm{y}}}_{d}$. For intermediate
    layers, to define ${\bm{T}}_{d}$, the author proposes to use a similar optimization
    problem to ([32](#S5.E32 "In 5.5 Backpropagation through MILP ‣ 5 Linear Programming
    and Polyhedral Theory in Training ‣ When Deep Learning Meets Polyhedral Theory:
    A Survey")), but leaving ${\bm{W}}$ and ${\bm{b}}$ fixed and having ${\bm{I}}_{d}$
    as variables; this defines “optimal inputs” of a layer. These optimal inputs are
    then used as target outputs ${\bm{T}}_{d}$ when processing the preceding layer,
    and thus the algorithm is iterated. For details, see Goebbels ([2021](#bib.bib120)).'
  prefs: []
  type: TYPE_NORMAL
- en: The computational results in that paper show that a similar level of accuracy
    to that of gradient descent can be achieved. However, the use of potentially expensive
    MILPs impairs the applicability of this approach to large networks. Nonetheless,
    it shows an interesting new avenue for training whose running times may be improved
    in future implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Training binarized neural networks using MILP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, the training problem of a DNN is an unrestricted non-convex
    optimization problem, which is typically continuous as the weights and biases
    frequently are allowed to have any real value. Nonetheless, if the weights and
    biases are required to be integer-valued, the training problem becomes a discrete
    optimization problem, for which gradient-descent-based methods may find some difficulties
    in their applicability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this context, Icarte et al. ([2019](#bib.bib161)) proposed a MILP formulation
    for the training problem of binarized neural networks (BNNs): these are neural
    networks where the weights and biases are restricted to be in $\{-1,0,1\}$ and
    where the activations are LTU (i.e. sign functions). Later on, Thorbjarnarson
    and Yorke-Smith ([2020](#bib.bib305), [2023](#bib.bib306)) used a similar technique
    to allow more general integer-valued weights. We review the core feature in these
    formulations that yield a *linear* formulation of the training problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us focus on an intermediate layer $i$ with width $n$, and let us omit biases
    to simplify the discussion. Using a DNN’s layer-wise architecture, one usually
    aims at describing:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle=({\bm{W}}^{i}{\bm{h}}^{i-1}_{d})_{j}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (33a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=\sigma(\hat{{\bm{h}}}^{i}_{d,j})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n.$ |  | (33b) |'
  prefs: []
  type: TYPE_TB
- en: 'We remind the reader that $D$ is the cardinality of the training set. Additionally,
    for each data point indexed by $d$ and each layer $i$, each variable ${\bm{h}}_{d}^{i}$
    is the output vector of all the neurons of the layer and each variable $\hat{{\bm{h}}}_{d,j}^{i}$
    is the input of neuron $j$. Besides the difficulty posed by the activation function,
    one important issue with system ([33](#S5.E33 "In 5.6 Training binarized neural
    networks using MILP ‣ 5 Linear Programming and Polyhedral Theory in Training ‣
    When Deep Learning Meets Polyhedral Theory: A Survey")) is the non-linearity of
    the products between the ${\bm{W}}$ and ${\bm{h}}$ variables. Nonetheless, this
    issue disappears when each entry of ${\bm{W}}$ and ${\bm{h}}$ is bounded and integer,
    as in the case of BNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us begin with reformulating ([33b](#S5.E33.2 "In 33 ‣ 5.6 Training binarized
    neural networks using MILP ‣ 5 Linear Programming and Polyhedral Theory in Training
    ‣ When Deep Learning Meets Polyhedral Theory: A Survey")). We can introduce auxiliary
    variables ${\bm{u}}_{d,j}^{i}\in\{0,1\}$ that will indicate if the neuron is active.
    We also introduce a tolerance $\varepsilon>0$ to determine the activity of a neuron.
    Using this, we can (approximately) reformulate ([33b](#S5.E33.2 "In 33 ‣ 5.6 Training
    binarized neural networks using MILP ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")) *linearly*
    using big-M constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{h}}^{i}_{d,j}$ | $\displaystyle=2{\bm{u}}_{d,j}^{i}-1$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34a) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\geq-M(1-{\bm{u}}_{d,j}^{i})$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34b) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\hat{{\bm{h}}}^{i}_{d,j}$ | $\displaystyle\leq-\varepsilon+M{\bm{u}}_{d,j}^{i}$
    | $\displaystyle d=1,\ldots,D,\,j=1,\ldots,n$ |  | (34c) |'
  prefs: []
  type: TYPE_TB
- en: 'where $M$ is a large constant. As for ([33a](#S5.E33.1 "In 33 ‣ 5.6 Training
    binarized neural networks using MILP ‣ 5 Linear Programming and Polyhedral Theory
    in Training ‣ When Deep Learning Meets Polyhedral Theory: A Survey")), note that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{{\bm{h}}}^{i}_{d,j}=\sum_{k=1}{\bm{W}}^{i}_{j,k}{\bm{h}}^{i-1}_{d,k}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Therefore, using ([34a](#S5.E34.1 "In 34 ‣ 5.6 Training binarized neural networks
    using MILP ‣ 5 Linear Programming and Polyhedral Theory in Training ‣ When Deep
    Learning Meets Polyhedral Theory: A Survey")), we see that it suffices to describe
    each product ${\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ linearly. We can introduce
    new variables ${\bm{z}}_{j,k,d}^{i}$ and note that'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{z}}_{j,k,d}^{i-1}={\bm{W}}_{j,k}^{i}{\bm{u}}_{d,k}^{i-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: if and only if the three variables satisfy
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}&#124;$ | $\displaystyle\leq{\bm{u}}_{d,k}^{i-1}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle&#124;{\bm{z}}_{j,k,d}^{i-1}-{\bm{W}}_{j,k}^{i}&#124;$ |
    $\displaystyle\leq 1-{\bm{u}}_{d,k}^{i-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\bm{u}}_{d,k}^{i-1}$ | $\displaystyle\in\{0,1\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This last system can be easily converted to a linear system, and thus the training
    problem in this setting can be cast as a mixed-integer linear optimization problem.
  prefs: []
  type: TYPE_NORMAL
- en: Other works have also relied on similar formulations to train neural networks.
    Icarte et al. ([2019](#bib.bib161)) introduce different objective functions that
    can be used along with the linear system to produce a MILP that can train BNNs.
    They also introduce a Constraint-Programming-based model and a hybrid model, and
    then compare all of them computationally. Thorbjarnarson and Yorke-Smith ([2020](#bib.bib305))
    introduce more MILP-based training models that leverage piecewise linear approximations
    of well-known non-linear loss functions and that can handle integer weights beyond
    $\{-1,0,1\}$. A similar setting is studied by Sildir and Aydin ([2022](#bib.bib287)),
    where piecewise linear approximations of non-linear activations are used, and
    integer weights are exploited to formulate the training problem as a MILP. Finally,
    Bernardelli et al. ([2022](#bib.bib25)) rely on a multi-objective MIP model for
    training BNNs; from here, they create a BNN ensemble to produce robust classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: From these articles, we can conclude that the MILP-based approach to training
    their neural networks can result in high-quality neural networks, especially in
    terms of generalization. However, many of these MILP-based methods currently do
    not scale well, as opposed to gradient-descent-based methods. We believe that,
    even though there are some theoretical limitations to the efficiency of MILP-based
    methods, there is a considerable practical improvement potential with using them
    in neural network training.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The rapid advancement of neural networks and their ubiquity has given rise
    to numerous new challenges and opportunities in deep learning: we need to design
    them in more reliable ways, to better understand their limits, and to test their
    robustness, among other challenges. While, traditionally, continuous optimization
    has been the predominant technology used in the optimization tasks in deep learning,
    some of these new challenges have made discrete optimization tools gain a remarkable
    importance.'
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we have reviewed multiple areas where polyhedral theory and
    linear optimization have played a critical role. For example, in understanding
    the expressiveness of neural networks, in optimizing trained neural networks (e.g.
    for verification purposes), and even in designing new training algorithms. We
    hope this survey can provide perspective in a rapidly-changing field, and motivate
    further developments in both deep learning and discrete optimization. There is
    still much to be explored in the intersection of these fields.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We thank Christian Tjandraatmadja and Toon Tran for early feedback on the manuscript
    and asking questions that helped shaping it.
  prefs: []
  type: TYPE_NORMAL
- en: Thiago Serra was supported by the National Science Foundation (NSF) award IIS
    2104583. Calvin Tsay was supported by the Engineering & Physical Sciences Research
    Council (EPSRC) grant EP/T001577/1.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abrahamsen et al. (2021) M. Abrahamsen, L. Kleist, and T. Miltzow. Training
    neural networks is er-complete. In *Neural Information Processing Systems (NeurIPS)*,
    volume 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agostinelli et al. (2015) F. Agostinelli, M. Hoffman, P. Sadowski, and P. Baldi.
    Learning activation functions to improve deep neural networks. In *International
    Conference on Learning Representations (ICLR) Workshop*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amrami and Goldberg (2021) A. Amrami and Y. Goldberg. A simple geometric proof
    for the benefit of depth in ReLU networks. *arXiv:2101.07126*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anderson et al. (2019) R. Anderson, J. Huchette, C. Tjandraatmadja, and J. Vielma.
    Strong mixed-integer programming formulations for trained neural networks. In
    *Integer Programming and Combinatorial Optimization (IPCO)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anderson et al. (2020) R. Anderson, J. Huchette, W. Ma, C. Tjandraatmadja, and
    J. P. Vielma. Strong mixed-integer programming formulations for trained neural
    networks. *Mathematical Programming*, 183(1-2):3–39, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anil et al. (2019) C. Anil, J. Lucas, and R. Grosse. Sorting out Lipschitz function
    approximation. In *International Conference on Machine Learning (ICML)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning
    (ICML)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arora et al. (2018) R. Arora, A. Basu, P. Mianjy, and A. Mukherjee. Understanding
    deep neural networks with rectified linear units. In *International Conference
    on Learning Representations (ICLR)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aziznejad et al. (2020) S. Aziznejad, H. Gupta, J. Campos, and M. Unser. Deep
    neural networks with trainable activations and controlled Lipschitz constant.
    *IEEE Transactions on Signal Processing*, 68:4688–4699, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation
    by jointly learning to align and translate. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balas (1998) E. Balas. Disjunctive programming: Properties of the convex hull
    of feasible points. *Discrete Applied Mathematics*, 89(1-3):3–44, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balas (2018) E. Balas. *Disjunctive Programming*. Springer Cham, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balas et al. (1993) E. Balas, S. Ceria, and G. Cornuéjols. A lift-and-project
    cutting plane algorithm for mixed 0–1 programs. *Mathematical Programming*, 58(1-3):295–324,
    1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balas et al. (1996) E. Balas, S. Ceria, and G. Cornuéjols. Mixed 0-1 programming
    by lift-and-project in a branch-and-cut framework. *Management Science*, 42(9):1229–1246,
    1996.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balestriero and Baraniuk (2018) R. Balestriero and R. G. Baraniuk. A spline
    theory of deep networks. In *International Conference on Machine Learning (ICML)*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balunović and Vechev (2020) M. Balunović and M. Vechev. Adversarial training
    and provable defenses: Bridging the gap. In *International Conference on Learning
    Representations (ICLR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batten et al. (2021) B. Batten, P. Kouvaros, A. Lomuscio, and Y. Zheng. Efficient
    neural network verification via layer-based semidefinite relaxations and linear
    cuts. In *International Joint Conference on Artificial Intelligence (IJCAI)*,
    pages 2184–2190, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bengio (2009) Y. Bengio. Learning deep architectures for AI. *Foundations and
    Trends®in Machine Learning*, 2(1):1–127, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bengio et al. (2021) Y. Bengio, A. Lodi, and A. Prouvost. Machine learning
    for combinatorial optimization: a methodological tour d’horizon. *European Journal
    of Operational Research*, 290(2):405–421, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennett (1992) K. P. Bennett. Decision tree construction via linear programming.
    Technical report, University of Wisconsin-Madison Department of Computer Sciences,
    1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennett and Mangasarian (1990) K. P. Bennett and O. L. Mangasarian. Neural network
    training via linear programming. Technical report, University of Wisconsin-Madison
    Department of Computer Sciences, 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bennett and Mangasarian (1992) K. P. Bennett and O. L. Mangasarian. Robust linear
    programming discrimination of two linearly inseparable sets. *Optimization Methods
    and Software*, 1(1):23–34, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Benussi et al. (2022) E. Benussi, A. Patane, M. Wicker, L. Laurenti, and M. Kwiatkowska.
    Individual fairness guarantees for neural networks. In *International Joint Conference
    on Artificial Intelligence (IJCAI)*, pages 651–658, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bergman et al. (2022) D. Bergman, T. Huang, P. Brooks, A. Lodi, and A. U. Raghunathan.
    Janos: an integrated predictive and prescriptive modeling framework. *INFORMS
    Journal on Computing*, 34(2):807–816, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bernardelli et al. (2022) A. M. Bernardelli, S. Gualandi, H. C. Lau, and S. Milanesi.
    The bemi stardust: a structured ensemble of binarized neural networks. *arXiv
    preprint arXiv:2212.03659*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berrada et al. (2018) L. Berrada, A. Zisserman, and M. P. Kumar. Deep Frank-Wolfe
    for neural network optimization. *arXiv:1811.07591*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertschinger et al. (2022) D. Bertschinger, C. Hertrich, P. Jungeblut, T. Miltzow,
    and S. Weber. Training fully connected neural networks is $\exists\mathbb{R}$-complete.
    *arXiv:2204.01368*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bhosekar and Ierapetritou (2018) A. Bhosekar and M. Ierapetritou. Advances
    in surrogate based modeling, feasibility analysis, and optimization: A review.
    *Computers & Chemical Engineering*, 108:250–267, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bianchini and Scarselli (2014) M. Bianchini and F. Scarselli. On the complexity
    of neural network classifiers: A comparison between shallow and deep architectures.
    *IEEE Transactions on Neural Networks and Learning Systems*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biau et al. (2021) G. Biau, M. Sangnier, and U. Tanielian. Some theoretical
    insights into Wasserstein GANs. *Journal of Machine Learning Research*, 22, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bienstock and Muñoz (2018) D. Bienstock and G. Muñoz. Lp formulations for polynomial
    optimization problems. *SIAM Journal on Optimization*, 28(2):1121–1150, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bienstock et al. (2023) D. Bienstock, G. Muñoz, and S. Pokutta. Principled deep
    neural network training through linear programming. *Discrete Optimization*, 49:100795,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blum and Rivest (1992) A. L. Blum and R. L. Rivest. Training a 3-node neural
    network is np-complete. *Neural Networks*, 5(1):117–127, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bohra et al. (2020) P. Bohra, J. Campos, H. Gupta, S. Aziznejad, and M. Unser.
    Learning activation functions in deep (spline) neural networks. *IEEE Open Journal
    of Signal Processing*, 1:295–309, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bonami et al. (2015) P. Bonami, A. Lodi, A. Tramontani, and S. Wiese. On mathematical
    programming with indicator constraints. *Mathematical Programming*, 151:191–223,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boob et al. (2022) D. Boob, S. S. Dey, and G. Lan. Complexity of training ReLU
    neural network. *Discrete Optimization*, 44:100620, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Botoeva et al. (2020) E. Botoeva, P. Kouvaros, J. Kronqvist, A. Lomuscio, and
    R. Misener. Efficient verification of relu-based neural networks via dependency
    analysis. In *AAAI Conference on Artificial Intelligence*, volume 34, pages 3291–3299,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bottou et al. (2018) L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods
    for large-scale machine learning. *SIAM Review*, 60(2):223–311, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridle (1990) J. S. Bridle. Probabilistic interpretation of feedforward classification
    network outputs, with relationships to statistical pattern recognition. In *Neurocomputing*,
    pages 227–236\. 1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2015) S. Bubeck et al. Convex optimization: Algorithms and complexity.
    *Foundations and Trends® in Machine Learning*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel et al. (2020a) R. Bunel, A. De Palma, A. Desmaison, K. Dvijotham, P. Kohli,
    P. Torr, and M. Pawan Kumar. Lagrangian decomposition for neural network verification.
    In *Conference on Uncertainty in Artificial Intelligence (UAI)*, volume 124, pages
    370–379, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel et al. (2020b) R. Bunel, P. Mudigonda, I. Turkaslan, P. Torr, J. Lu, and
    P. Kohli. Branch and bound for piecewise linear neural network verification. *Journal
    of Machine Learning Research*, 21(2020), 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel et al. (2018) R. R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. K.
    Mudigonda. A unified view of piecewise linear neural network verification. *Neural
    Information Processing Systems (NeurIPS)*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bunel et al. (2020c) R. R. Bunel, O. Hinder, S. Bhojanapalli, and K. Dvijotham.
    An efficient nonconvex reformulation of stagewise convex optimization problems.
    *Neural Information Processing Systems (NeurIPS)*, 33:8247–8258, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burtea and Tsay (2023) R.-A. Burtea and C. Tsay. Safe deployment of reinforcement
    learning using deterministic optimization over neural networks. In *Computer Aided
    Chemical Engineering*, volume 52, pages 1643–1648\. Elsevier, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2023) J. Cai, K.-N. Nguyen, N. Shrestha, A. Good, R. Tu, X. Yu,
    S. Zhe, and T. Serra. Getting away with more network pruning: From sparsity to
    geometry and linear regions. In *International Conference on the Integration of
    Constraint Programming, Artificial Intelligence, and Operations Research (CPAIOR)*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ceccon et al. (2022) F. Ceccon, J. Jalving, J. Haddad, A. Thebelt, C. Tsay,
    C. D. Laird, and R. Misener. Omlt: Optimization & machine learning toolkit. *Journal
    of Machine Learning Research*, 23(349):1–8, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charisopoulos and Maragos (2018) V. Charisopoulos and P. Maragos. A tropical
    approach to neural networks with piecewise linear activations. *arXiv:1805.08749*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaudhry et al. (2020) A. Chaudhry, N. Khan, P. Dokania, and P. Torr. Continual
    learning in low-rank orthogonal subspaces. In *Neural Information Processing Systems
    (NeurIPS)*, volume 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022a) H. Chen, Y. G. Wang, and H. Xiong. Lower and upper bounds
    for numbers of linear regions of graph convolutional networks. *arXiv:2206.00228*,
    2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022b) K.-L. Chen, H. Garudadri, and B. D. Rao. Improved bounds
    on neural complexity for representing piecewise linear functions. In *Neural Information
    Processing Systems (NeurIPS)*, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022c) S. Chen, A. R. Klivans, and R. Meka. Learning deep ReLU
    networks is fixed-parameter tractable. In *2021 IEEE 62nd Annual Symposium on
    Foundations of Computer Science (FOCS)*, pages 696–707\. IEEE, 2022c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) T. Chen, J.-B. Lasserre, V. Magron, and E. Pauwels. Semialgebraic
    optimization for Lipschitz constants of ReLU networks. In *Neural Information
    Processing Systems (NeurIPS)*, volume 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021a) W. Chen, X. Gong, and Z. Wang. Neural architecture search
    on ImageNet in four GPU hours: A theoretically inspired perspective. In *International
    Conference on Learning Representations (ICLR)*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) W. Chen, X. Gong, Y. Wei, H. Shi, Z. Yan, Y. Yang, and Z. Wang.
    Understanding and accelerating neural architecture search with training-free and
    theory-grounded metrics. *arXiv:2108.11939*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2017) C. Cheng, G. Nührenberg, and H. Ruess. Maximum resilience
    of artificial neural networks. In *Automated Technology for Verification and Analysis
    (ATVA)*, pages 251–268, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2018) C.-H. Cheng, G. Nührenberg, C.-H. Huang, and H. Ruess.
    Verification of binarized neural networks via inter-neuron factoring: (short paper).
    In *International Conference on Verified Software: Theories, Tools, and Experiments
    (VSTTE)*, pages 279–290\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheon (2022) M.-S. Cheon. An outer-approximation guided optimization approach
    for constrained neural network inverse problems. *Mathematical Programming*, 196(1-2):173–202,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chu et al. (2018) L. Chu, X. Hu, J. Hu, L. Wang, and J. Pei. Exact and consistent
    interpretation for piecewise linear neural networks: A closed form solution. In
    *ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ciresan et al. (2012) D. Ciresan, U. Meier, J. Masci, and J. Schmidhuber. Multi
    column deep neural network for traffic sign classification. *Neural Networks*,
    2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohan et al. (2022) S. Cohan, N. H. Kim, D. Rolnick, and M. van de Panne. Understanding
    the evolution of linear regions in deep reinforcement learning. In *Neural Information
    Processing Systems (NeurIPS)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collobert (2004) R. Collobert. *Large Scale Machine Learning*. PhD thesis, University
    Paris 6, 2004.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Courbariaux et al. (2015) M. Courbariaux, Y. Bengio, and J.-P. David. BinaryConnect:
    Training deep neural networks with binary weights during propagations. *Neural
    Information Processing Systems (NeurIPS)*, 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Craighero et al. (2020a) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    and M. Antoniotti. Investigating the compositional structure of deep neural networks.
    In *Machine Learning, Optimization, and Data Science (LOD)*, pages 322–334, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Craighero et al. (2020b) F. Craighero, F. Angaroni, A. Graudenzi, F. Stella,
    and M. Antoniotti. Understanding deep learning with activation pattern diagrams.
    In *Proceedings of the Italian Workshop on Explainable Artificial Intelligence
    co-located with 19th International Conference of the Italian Association for Artificial
    Intelligence*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce and Hein (2018) F. Croce and M. Hein. A randomized gradient-free attack
    on ReLU networks. In *German Conference on Pattern Recognition (GCPR)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce et al. (2019) F. Croce, M. Andriushchenko, and M. Hein. Provable robustness
    of relu networks via maximization of linear regions. In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croce et al. (2020) F. Croce, J. Rauber, and M. Hein. Scaling up the randomized
    gradient-free adversarial attack reveals overestimation of robustness using established
    attacks. *International Journal of Computer Vision*, 128:1028–1046, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Croxton et al. (2003) K. L. Croxton, B. Gendron, and T. L. Magnanti. A comparison
    of mixed-integer programming models for nonconvex piecewise linear cost minimization
    problems. *Management Science*, 49(9):1268–1273, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Curtis and Scheinberg (2017) F. E. Curtis and K. Scheinberg. Optimization methods
    for supervised machine learning: From linear models to deep learning. In *INFORMS
    TutORials in Operations Research*, pages 89–114. INFORMS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cybenko (1989) G. Cybenko. Approximation by superpositions of a sigmoidal function.
    *Mathematics of Control, Signals and Systems*, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Danna et al. (2007) E. Danna, M. Fenelon, Z. Gu, and R. Wunderling. Generating
    multiple solutions for mixed integer programming problems. In *Integer Programming
    and Combinatorial Optimization (IPCO)*, pages 280–294\. 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dantzig (1960) G. B. Dantzig. On the significance of solving linear programming
    problems with some integer variables. *Econometrica, Journal of the Econometric
    Society*, pages 30–44, 1960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dantzig and Eaves (1973) G. B. Dantzig and B. C. Eaves. Fourier-Motzkin elimination
    and its dual. *Journal of Combinatorial Theory (A)*, 14:288–297, 1973.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dathathri et al. (2020) S. Dathathri, K. Dvijotham, A. Kurakin, A. Raghunathan,
    J. Uesato, R. R. Bunel, S. Shankar, J. Steinhardt, I. Goodfellow, P. S. Liang,
    et al. Enabling certification of verification-agnostic networks via memory-efficient
    semidefinite programming. *Neural Information Processing Systems (NeurIPS)*, 33:5318–5331,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daubechies et al. (2022) I. Daubechies, R. DeVore, S. Foucart, B. Hanin, and
    G. Petrova. Nonlinear approximation and (deep) ReLU networks. *Constructive Approximation*,
    55:127–172, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: De Palma et al. (2021) A. De Palma, H. Behl, R. R. Bunel, P. Torr, and M. P.
    Kumar. Scaling the convex barrier with active sets. In *International Conference
    on Learning Representations (ICLR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Delarue et al. (2020) A. Delarue, R. Anderson, and C. Tjandraatmadja. Reinforcement
    learning with combinatorial actions: An application to vehicle routing. *Neural
    Information Processing Systems (NeurIPS)*, 33:609–620, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. (2020) Y. Deng, X. Zheng, T. Zhang, C. Chen, G. Lou, and M. Kim.
    An analysis of adversarial attacks and defenses on autonomous driving models.
    In *2020 IEEE international conference on pervasive computing and communications
    (PerCom)*, pages 1–10\. IEEE, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT:
    Pre-training of deep bidirectional transformers for language understanding. In
    *Conference of the North American Chapter of the Association for Computational
    Linguistics (NAACL)*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dey et al. (2020) S. S. Dey, G. Wang, and Y. Xie. Approximation algorithms for
    training one-node relu neural networks. *IEEE Transactions on Signal Processing*,
    68:6696–6706, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al. (2021) S. R. Dubey, S. K. Singh, and B. B. Chaudhuri. A comprehensive
    survey and performance analysis of activation functions in deep learning. *arXiv:2109.14545*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dutta et al. (2018) S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output
    range analysis for deep feedforward networks. In *NASA Formal Methods: 10th International
    Symposium, (NFM)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dvijotham et al. (2018a) K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic,
    B. O’Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned
    verifiers. *arXiv:1805.10265*, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dvijotham et al. (2018b) K. Dvijotham, R. Stanforth, S. Gowal, T. A. Mann, and
    P. Kohli. A dual approach to scalable verification of deep networks. In *Conference
    on Uncertainty in Artificial Intelligence (UAI)*, volume 1, page 3, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dym et al. (2020) N. Dym, B. Sober, and I. Daubechies. Expression of fractals
    through neural network functions. *IEEE Journal on Selected Areas in Information
    Theory*, 1(1):57–66, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ehlers (2017) R. Ehlers. Formal verification of piece-wise linear feed-forward
    neural networks. In *Automated Technology for Verification and Analysis (ATVA)*,
    pages 269–286\. Springer, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ElAraby et al. (2020) M. ElAraby, G. Wolf, and M. Carvalho. Identifying efficient
    sub-networks using mixed integer programming. In *OPT Workshop*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2019) T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture
    search: A survey. *Journal of Machine Learning Research*, 20:1–21, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2020) T. Ergen and M. Pilanci. Convex geometry of two-layer
    relu networks: Implicit autoencoding and interpretable models. In S. Chiappa and
    R. Calandra, editors, *International Conference on Artificial Intelligence and
    Statistics*, volume 108 of *Proceedings of Machine Learning Research*, pages 4024–4033\.
    PMLR, 26–28 Aug 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ergen and Pilanci (2021a) T. Ergen and M. Pilanci. Convex geometry and duality
    of over-parameterized neural networks. *The Journal of Machine Learning Research*,
    22(1):9646–9708, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2021b) T. Ergen and M. Pilanci. Global optimality beyond
    two layers: Training deep relu networks via convex programs. In *International
    Conference on Machine Learning (ICLR)*, pages 2993–3003\. PMLR, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2021c) T. Ergen and M. Pilanci. Implicit convex regularizers
    of cnn architectures: Convex optimization of two-and three-layer networks in polynomial
    time. In *International Conference on Learning Representations (ICLR)*, 2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen and Pilanci (2021d) T. Ergen and M. Pilanci. Path regularization: A convexity
    and sparsity inducing regularization for parallel relu networks. *arXiv preprint
    arXiv:2110.09548*, 2021d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ergen and Pilanci (2021e) T. Ergen and M. Pilanci. Revealing the structure of
    deep neural networks via convex duality. In *International Conference on Machine
    Learning*, pages 3004–3014\. PMLR, 2021e.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ergen et al. (2022) T. Ergen, A. Sahiner, B. Ozturkler, J. M. Pauly, M. Mardani,
    and M. Pilanci. Demystifying batch normalization in reLU networks: Equivalent
    convex optimization models and implicit regularization. In *International Conference
    on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ergen et al. (2023) T. Ergen, H. I. Gulluk, J. Lacotte, and M. Pilanci. Globally
    optimal training of neural networks with threshold activation functions. In *International
    Conference on Learning Representations (ICLR)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eykholt et al. (2018) K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati,
    C. Xiao, A. Prakash, T. Kohno, and D. Song. Robust physical-world attacks on deep
    learning visual classification. In *Conference on Computer Vision and Pattern
    Recognition (CVPR)*, June 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2020) F.-L. Fan, R. Lai, and G. Wang. Quasi-equivalence of width
    and depth of neural networks. *arXiv:2002.02515*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. (2023) F.-L. Fan, W. Huang, X. Zhong, L. Ruan, T. Zeng, H. Xiong,
    and F. Wang. Deep relu networks have surprisingly simple polytopes. *arXiv:2305.09145*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fazlyab et al. (2019) M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J.
    Pappas. Efficient and accurate estimation of Lipschitz constants for deep neural
    networks. In *Neural Information Processing Systems (NeurIPS)*, volume 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fazlyab et al. (2020) M. Fazlyab, M. Morari, and G. J. Pappas. Safety verification
    and robustness analysis of neural networks via quadratic constraints and semidefinite
    programming. *IEEE Transactions on Automatic Control*, 67(1):1–15, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferlez and Shoukry (2020) J. Ferlez and Y. Shoukry. AReN: Assured ReLU NN architecture
    for model predictive control of LTI systems. In *HSCC*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferrari et al. (2022) C. Ferrari, M. N. Mueller, N. Jovanović, and M. Vechev.
    Complete verification via multi-neuron relaxation guided branch-and-bound. In
    *International Conference on Learning Representations (ICLR)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finlayson et al. (2019) S. G. Finlayson, J. D. Bowers, J. Ito, J. L. Zittrain,
    A. L. Beam, and I. S. Kohane. Adversarial attacks on medical machine learning.
    *Science*, 363(6433):1287–1289, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fischetti and Jo (2018) M. Fischetti and J. Jo. Deep neural networks and mixed
    integer linear optimization. *Constraints*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fourier (1826) J. Fourier. Solution d’une question particuliére du calcul des
    inégalités. *Nouveau Bulletin des Sciences par la Société Philomatique de Paris*,
    1826.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frank et al. (1956) M. Frank, P. Wolfe, et al. An algorithm for quadratic programming.
    *Naval Research Logistics Quarterly*, 3(1-2):95–110, 1956.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Froese and Hertrich (2023) V. Froese and C. Hertrich. Training neural networks
    is NP-hard in fixed dimension. *arXiv preprint arXiv:2303.17045*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Froese et al. (2022) V. Froese, C. Hertrich, and R. Niedermeier. The computational
    complexity of relu network training parameterized by data dimensionality. *Journal
    of Artificial Intelligence Research*, 74:1775–1790, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fukushima (1980) K. Fukushima. Neocognitron: A self-organizing neural network
    model for a mechanism of pattern recognition unaffected by shift in position.
    *Biological Cybernetics*, 36(4):193–202, 1980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Funahashi (1989) K.-I. Funahashi. On the approximate realization of continuous
    mappings by neural networks. *Neural Networks*, 2(3), 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamba et al. (2020) M. Gamba, S. Carlsson, H. Azizpour, and M. Björkman. Hyperplane
    arrangements of trained ConvNets are biased. *arXiv:2003.07797*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gamba et al. (2022) M. Gamba, A. Chmielewski-Anders, J. Sullivan, H. Azizpour,
    and M. Björkman. Are all linear regions created equal? In *International Conference
    on Artificial Intelligence and Statistics (AISTATS)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gambella et al. (2021) C. Gambella, B. Ghaddar, and J. Naoum-Sawaya. Optimization
    problems for machine learning: A survey. *European Journal of Operational Research*,
    290(3):807–828, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2020) J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and
    C. Schmid. VectorNet: Encoding HD maps and agent dynamics from vectorized representation.
    In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geißler et al. (2012) B. Geißler, A. Martin, A. Morsi, and L. Schewe. Using
    piecewise linear functions for solving MINLPs. In *Mixed Integer Nonlinear Programming*,
    pages 287–314. Springer, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glass et al. (2021) L. Glass, W. Hilali, and O. Nelles. Compressing interpretable
    representations of piecewise linear neural networks using neuro-fuzzy models.
    In *IEEE Symposium Series on Computational Intelligence (SSCI)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Glorot et al. (2011) X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier
    neural networks. In *International Conference on Artificial Intelligence and Statistics
    (AISTATS)*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goebbels (2021) S. Goebbels. Training of ReLU activated multilayerd neural networks
    with mixed integer linear programs. Technical report, Hochschule Niederrhein,
    Fachbereich Elektrotechnik & Informatik, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goel et al. (2021) S. Goel, A. Klivans, P. Manurangsi, and D. Reichman. Tight
    hardness results for training depth-2 relu networks. In *12th Innovations in Theoretical
    Computer Science Conference (ITCS 2021)*. Schloss Dagstuhl-Leibniz-Zentrum für
    Informatik, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goerigk and Kurtz (2023) M. Goerigk and J. Kurtz. Data-driven robust optimization
    using deep neural networks. *Computers & Operations Research*, 151:106087, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2013) I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville,
    and Y. Bengio. Maxout networks. In *International Conference on Machine Learning
    (ICML)*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2015) I. Goodfellow, J. Shlens, and C. Szegedy. Explaining
    and harnessing adversarial examples. In *International Conference on Learning
    Representations (ICLR)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2016) I. Goodfellow, Y. Bengio, and A. Courville. *Deep learning*.
    MIT press, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
    D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial
    nets. In *Neural Information Processing Systems (NeurIPS)*, volume 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gopinath et al. (2019) D. Gopinath, H. Converse, C. S. Pasareanu, and A. Taly.
    Property inference for deep neural networks. In *IEEE/ACM International Conference
    on Automated Software Engineering (ASE)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goujon et al. (2022) A. Goujon, A. Etemadi, and M. Unser. The role of depth,
    width, and activation complexity in the number of linear regions of neural networks.
    *arXiv:2206.08615*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gowal et al. (2018) S. Gowal, K. Dvijotham, R. Stanforth, R. Bunel, C. Qin,
    J. Uesato, R. Arandjelovic, T. Mann, and P. Kohli. On the effectiveness of interval
    bound propagation for training verifiably robust models. *arXiv:1810.12715*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graves and Jaitly (2014) A. Graves and N. Jaitly. Towards end-to-end speech
    recognition with recurrent neural networks. In *International Conference on Machine
    Learning (ICML)*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigsby and Lindsey (2022) J. E. Grigsby and K. Lindsey. On transversality of
    bent hyperplane arrangements and the topological expressiveness of ReLU neural
    networks. *SIAM Journal on Applied Algebra and Geometry*, 6(2), 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigsby et al. (2023) J. E. Grigsby, K. Lindsey, and D. Rolnick. Hidden symmetries
    of ReLU networks. In *International Conference on Machine Learning (ICML)*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grimstad and Andersson (2019) B. Grimstad and H. Andersson. ReLU networks as
    surrogate models in mixed-integer linear programs. *Computers & Chemical Engineering*,
    131:106580, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grossmann and Ruiz (2012) I. E. Grossmann and J. P. Ruiz. Generalized disjunctive
    programming: A framework for formulation and alternative algorithms for MINLP
    optimization. In *Mixed Integer Nonlinear Programming*, pages 93–115, New York,
    NY, 2012\. Springer New York.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hahnloser et al. (2000) R. Hahnloser, R. Sarpeshkar, M. Mahowald, R. Douglas,
    and S. Seung. Digital selection and analogue amplification coexist in a cortex-inspired
    silicon circuit. *Nature*, 405, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han and Gómez (2021) S. Han and A. Gómez. Single-neuron convexification for
    binarized neural networks, 2021. URL https://optimization-online.org/?p=17148.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin and Rolnick (2019a) B. Hanin and D. Rolnick. Complexity of linear regions
    in deep networks. In *International Conference on Machine Learning (ICML)*, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin and Rolnick (2019b) B. Hanin and D. Rolnick. Deep ReLU networks have surprisingly
    few activation patterns. In *Neural Information Processing Systems (NeurIPS)*,
    volume 32, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin and Sellke (2017) B. Hanin and M. Sellke. Approximating continuous functions
    by ReLU nets of minimal width. *arXiv:1710.11278*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hashemi et al. (2021) V. Hashemi, P. Kouvaros, and A. Lomuscio. OSIP: Tightened
    bound propagation for the verification of ReLU neural networks. In *International
    Conference on Software Engineering and Formal Methods (SEFM)*, pages 463–480\.
    Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2021) F. He, S. Lei, J. Ji, and D. Tao. Neural networks behave as
    hash encoders: An empirical study. *arXiv:2101.05490*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2020) J. He, L. Li, J. Xu, and C. Zheng. ReLU deep neural networks
    and linear finite elements. *Journal of Computational Mathematics*, 38:502–527,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2015) K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers:
    Surpassing human-level performance on ImageNet classification. In *IEEE International
    Conference on Computer Vision (ICCV)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning
    for image recognition. In *Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Henriksen and Lomuscio (2021) P. Henriksen and A. Lomuscio. DEEPSPLIT: an efficient
    splitting method for neural network verification via indirect effect analysis.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    2549–2555, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Henriksen et al. (2022) P. Henriksen, F. Leofante, and A. Lomuscio. Repairing
    misclassifications in neural networks using limited data. In *ACM/SIGAPP Symposium
    On Applied Computing (SAC)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hertrich et al. (2021) C. Hertrich, A. Basu, M. D. Summa, and M. Skutella. Towards
    lower bounds on the depth of ReLU neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2012) G. Hinton, L. Deng, G. Dahl, A. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for
    acoustic modeling in speech recognition. *IEEE Signal Processing Magazine*, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinz (2021) P. Hinz. Using activation histograms to bound the number of affine
    regions in ReLU feed-forward neural networks. *arXiv:2103.17174*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinz and van de Geer (2019) P. Hinz and S. van de Geer. A framework for the
    construction of upper bounds on the number of affine linear regions of ReLU feed-forward
    neural networks. *IEEE Transactions on Information Theory*, 65(11):7304–7324,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber (1997) S. Hochreiter and J. Schmidhuber. Long short-term
    memory. *Neural Computation*, 9(8):1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopfield (1982) J. Hopfield. Neural networks and physical systems with emergent
    collective computational abilities. *Proceedings of the National Academy of Sciences*,
    79:2554–2558, 1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hornik et al. (1989) K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward
    networks are universal approximators. *Neural Networks*, 2(5), 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020a) T. Hu, Z. Shang, and G. Cheng. Sharp rate of convergence for
    deep neural network classifiers under the teacher-student setting. *arXiv:2001.06892*,
    2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020b) X. Hu, W. Liu, J. Bian, and J. Pei. Measuring model complexity
    of neural networks with curve activation functions. In *ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining (KDD)*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2021) X. Hu, L. Chu, J. Pei, W. Liu, and J. Bian. Model complexity
    of deep learning: a survey. *Knowledge and Information Systems*, 63:2585–2619,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2020) X. Huang, D. Kroening, W. Ruan, J. Sharp, Y. Sun, E. Thamo,
    M. Wu, and X. Yi. A survey of safety and trustworthiness of deep neural networks:
    Verification, testing, adversarial attack and defence, and interpretability. *Computer
    Science Review*, 37:100270, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huchette and Vielma (2022) J. Huchette and J. P. Vielma. Nonconvex piecewise
    linear functions: Advanced formulations and simple modeling tools. *Operations
    Research*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huster et al. (2018) T. Huster, C.-Y. J. Chiang, and R. Chadha. Limitations
    of the Lipschitz constant as a defense against adversarial examples. In *ECML
    PKDD Workshops*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hwang and Heinecke (2020) W.-L. Hwang and A. Heinecke. Un-rectifying non-linear
    networks for signal representation. *IEEE Transactions on Signal Processing*,
    68:196–210, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Icarte et al. (2019) R. T. Icarte, L. Illanes, M. P. Castro, A. A. Cire, S. A.
    McIlraith, and J. C. Beck. Training binarized neural networks using mip and cp.
    In *International Conference on Principles and Practice of Constraint Programming*,
    pages 401–417\. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ioffe and Szegedy (2015) S. Ioffe and C. Szegedy. Batch normalization: Accelerating
    deep network training by reducing internal covariate shift. In *International
    Conference on Machine Learning (ICML)*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeroslow and Lowe (1984) R. G. Jeroslow and J. K. Lowe. *Modelling with integer
    variables*. Springer, 1984.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia and Rinard (2020) K. Jia and M. Rinard. Efficient exact verification of
    binarized neural networks. *Neural Information Processing Systems (NeurIPS)*,
    33:1782–1795, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson et al. (2020) T. T. Johnson, D. M. Lopez, P. Musau, H.-D. Tran, E. Botoeva,
    F. Leofante, A. Maleki, C. Sidrane, J. Fan, and C. Huang. ARCH-COMP20 category
    report: Artificial intelligence and neural network control systems (AINNCS) for
    continuous and hybrid systems plants. In *International Workshop on Applied Verification
    of Continuous and Hybrid Systems (ARCH20)*, volume 74, pages 107–139, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jordan and Dimakis (2020) M. Jordan and A. G. Dimakis. Exactly computing the
    local Lipschitz constant of ReLU networks. In *Neural Information Processing Systems
    (NeurIPS)*, volume 33, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jordan et al. (2019) M. Jordan, J. Lewis, and A. G. Dimakis. Provable certificates
    for adversarial examples: Fitting a ball in the union of polytopes. In *Neural
    Information Processing Systems (NeurIPS)*, volume 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Karg and Lucia (2020) B. Karg and S. Lucia. Efficient representation and approximation
    of model predictive control laws via deep learning. *IEEE Transactions on Cybernetics*,
    50(9):3866–3878, 2020. doi: 10.1109/TCYB.2020.2999556.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Katz et al. (2017) G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer.
    Reluplex: An efficient SMT solver for verifying deep neural networks. In *Computer
    Aided Verification (CAV)*, pages 97–117\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katz et al. (2019) G. Katz, D. A. Huang, D. Ibeling, K. Julian, C. Lazarus,
    R. Lim, P. Shah, S. Thakoor, H. Wu, A. Zeljić, et al. The marabou framework for
    verification and analysis of deep neural networks. In *Computer Aided Verification
    (CAV)*, pages 443–452. Springer, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Katz et al. (2020) J. Katz, I. Pappas, S. Avraamidou, and E. N. Pistikopoulos.
    Integrating deep learning models and multiparametric programming. *Computers &
    Chemical Engineering*, 136:106801, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keup and Helias (2022) C. Keup and M. Helias. Origami in N dimensions: How
    feed-forward networks manufacture linear separability. *arXiv:2203.11355*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khalife and Basu (2022) S. Khalife and A. Basu. Neural networks with linear
    threshold activations: structure and algorithms. In *Integer Programming and Combinatorial
    Optimization (IPCO)*, pages 347–360\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khedr et al. (2020) H. Khedr, J. Ferlez, and Y. Shoukry. Effective formal verification
    of neural networks using the geometry of linear regions. *arXiv:2006.10864*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2014) D. P. Kingma and J. Ba. Adam: A method for stochastic
    optimization. *arXiv:1412.6980*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kody et al. (2022) A. Kody, S. Chevalier, S. Chatzivasileiadis, and D. Molzahn.
    Modeling the ac power flow equations with optimally compact neural networks: Application
    to unit commitment. *Electric Power Systems Research*, 213:108282, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kouvaros et al. (2021) P. Kouvaros, T. Kyono, F. Leofante, A. Lomuscio, D. Margineantu,
    D. Osipychev, and Y. Zheng. Formal analysis of neural network-based systems in
    the aircraft domain. In *International Symposium on Formal Methods (FM)*, pages
    730–740\. Springer, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet
    classification with deep convolutional neural networks. In *Neural Information
    Processing Systems (NeurIPS)*, volume 25, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kronqvist et al. (2021) J. Kronqvist, R. Misener, and C. Tsay. Between steps:
    Intermediate relaxations between big-M and convex hull formulations. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kronqvist et al. (2022) J. Kronqvist, R. Misener, , and C. Tsay. P-split formulations:
    A class of intermediate formulations between big-M and convex hull for disjunctive
    constraints. *arXiv:2202.05198*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. (2019) A. Kumar, T. Serra, and S. Ramalingam. Equivalent and approximate
    transformations of deep neural networks. *arXiv:1905.1142*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lacoste-Julien et al. (2013) S. Lacoste-Julien, M. Jaggi, M. Schmidt, and P. Pletscher.
    Block-coordinate Frank-Wolfe optimization for structural SVMs. In *International
    Conference on Machine Learning (ICML)*, pages 53–61, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lan et al. (2022) J. Lan, Y. Zheng, and A. Lomuscio. Tight neural network verification
    via semidefinite relaxations and linear reformulations. In *AAAI Conference on
    Artificial Intelligence*, volume 36, pages 7272–7280, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latorre et al. (2020) F. Latorre, P. Rolland, and V. Cevher. Lipschitz constant
    estimation of neural networks via sparse polynomial optimization. In *International
    Conference on Learning Representations (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,
    W. Hubbard, and L. D. Jackel. Backpropagation applied to handwritten zip code
    recognition. *Neural Computation*, 1(4):541–551, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LeCun et al. (1998) Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller. Efficient
    backprop. In G. Montavon, G. Orr, and K. Müller, editors, *Neural Networks: Tricks
    of the Trade*. Springer, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2015) Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. *Nature*,
    521, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2019) G.-H. Lee, D. Alvarez-Melis, and T. S. Jaakkola. Towards robust,
    locally linear deep networks. In *International Conference on Learning Representations
    (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee and Wilson (2001) J. Lee and D. Wilson. Polyhedral methods for piecewise-linear
    functions I: the lambda method. *Discrete Applied Mathematics*, 108(3):269–285,
    2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leofante et al. (2018) F. Leofante, N. Narodytska, L. Pulina, and A. Tacchella.
    Automated verification of neural networks: Advances, challenges and perspectives.
    *arXiv:1805.09938*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) L. Li, T. Xie, and B. Li. Sok: Certified robustness for deep
    neural networks. In *2023 IEEE Symposium on Security and Privacy (SP)*, pages
    94–115\. IEEE Computer Society, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang and Xu (2021) X. Liang and J. Xu. Biased ReLU neural networks. *Neurocomputing*,
    423:71–79, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lillicrap et al. (2015) T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez,
    Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement
    learning. *arXiv:1509.02971*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linnainmaa (1970) S. Linnainmaa. The representation of the cumulative rounding
    error of an algorithm as a Taylor expansion of the local rounding errors (in Finnish).
    Master’s thesis, Univ. Helsinki, 1970.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Little (1974) W. Little. The existence of persistent states in the brain. *Mathematical
    Biosciences*, 19:101–120, 1974.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Liang (2021) B. Liu and Y. Liang. Optimal function approximation with
    ReLU neural networks. *Neurocomputing*, 435:216–227, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) C. Liu, T. Arnon, C. Lazarus, C. Strong, C. Barrett, M. J.
    Kochenderfer, et al. Algorithms for verifying deep neural networks. *Foundations
    and Trends® in Optimization*, 4(3-4):244–404, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020) X. Liu, X. Han, N. Zhang, and Q. Liu. Certified monotonic
    neural networks. In *Neural Information Processing Systems (NeurIPS)*, volume 33,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lombardi et al. (2017) M. Lombardi, M. Milano, and A. Bartolini. Empirical decision
    model learning. *Artificial Intelligence*, 244:343–367, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lomuscio and Maganti (2017) A. Lomuscio and L. Maganti. An approach to reachability
    analysis for feed-forward ReLU neural networks. *arXiv:1706.07351*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loukas et al. (2021) A. Loukas, M. Poiitis, and S. Jegelka. What training reveals
    about neural network complexity. In *Neural Information Processing Systems (NeurIPS)*,
    volume 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2017) Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive
    power of neural networks: A view from the width. In *Neural Information Processing
    Systems (NeurIPS)*, volume 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lueg et al. (2021) L. Lueg, B. Grimstad, A. Mitsos, and A. M. Schweidtmann.
    reluMIP: Open source tool for MILP optimization of ReLU neural networks, 2021.
    URL https://github.com/ChemEngAI/ReLU_ANN_MILP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu et al. (2020) Z. Lyu, C.-Y. Ko, Z. Kong, N. Wong, D. Lin, and L. Daniel.
    Fastened crown: Tightened neural network robustness certificates. In *AAAI Conference
    on Artificial Intelligence*, volume 34, pages 5037–5044, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maas et al. (2013) A. Maas, A. Hannun, and A. Ng. Rectifier nonlinearities improve
    neural network acoustic models. In *ICML Workshop on Deep Learning for Audio,
    Speech and Language Processing*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Madry et al. (2018) A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
    Towards deep learning models resistant to adversarial attacks. In *International
    Conference on Learning Representations (ICLR)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makhoul et al. (1989) J. Makhoul, R. Schwartz, and A. El-Jaroudi. Classification
    capabilities of two-layer neural nets. In *International Conference on Acoustics,
    Speech, and Signal Processing (ICASSP)*, 1989.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malach and Shalev-Shwartz (2019) E. Malach and S. Shalev-Shwartz. Is deeper
    better only when shallow is good? In *Neural Information Processing Systems (NeurIPS)*,
    volume 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mangasarian (1993) O. L. Mangasarian. Mathematical programming in neural networks.
    *ORSA Journal on Computing*, 5(4):349–360, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maragno et al. (2021) D. Maragno, H. Wiberg, D. Bertsimas, S. I. Birbil, D. d.
    Hertog, and A. Fajemisin. Mixed-integer optimization with constraint learning.
    *arXiv:2111.04469*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maragno et al. (2023) D. Maragno, J. Kurtz, T. E. Röber, R. Goedhart, Ş. I.
    Birbil, and D. d. Hertog. Finding regions of counterfactual explanations via robust
    optimization. *arXiv:2301.11113*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maragos et al. (2021) P. Maragos, V. Charisopoulos, and E. Theodosis. Tropical
    geometry and machine learning. *Proceedings of the IEEE*, 109(5):728–755, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masden (2022) M. Masden. Algorithmic determination of the combinatorial structure
    of the linear regions of ReLU neural networks. *arXiv:2207.07696*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matoba et al. (2022) K. Matoba, N. Dimitriadis, and F. Fleuret. The theoretical
    expressiveness of maxpooling. *arXiv:2203.01016*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matousek (2002) J. Matousek. *Lectures on Discrete Geometry*, volume 212. Springer
    Science & Business Media, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McBride and Sundmacher (2019) K. McBride and K. Sundmacher. Overview of surrogate
    modeling in chemical process engineering. *Chemie Ingenieur Technik*, 91(3):228–239,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCulloch and Pitts (1943) W. McCulloch and W. Pitts. A logical calculus of
    the ideas immanent in nervous activity. *Bulletin of Mathematical Biophysics*,
    5:115–133, 1943.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mhaskar and Poggio (2020) H. N. Mhaskar and T. Poggio. Function approximation
    by deep networks. *Communications on Pure & Applied Analysis*, 19(8):4085–4095,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Minsky and Papert (1969) M. Minsky and S. Papert. *Perceptrons: An Introduction
    to Computational Geometry*. The MIT Press, 1969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirman et al. (2018) M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract
    interpretation for provably robust neural networks. In *International Conference
    on Machine Learning (ICML)*, volume 80, pages 3578–3586, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misener and Floudas (2012) R. Misener and C. A. Floudas. Global optimization
    of mixed-integer quadratically-constrained quadratic programs (MIQCQP) through
    piecewise-linear and edge-concave relaxations. *Mathematical Programming*, 136(1):155–182,
    2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness,
    M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen,
    C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg,
    and D. Hassabis. Human-level control through deep reinforcement learning. *Nature*,
    518:529–533, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montúfar (2017) G. Montúfar. Notes on the number of linear regions of deep neural
    networks. In *Sampling Theory and Applications (SampTA)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montúfar et al. (2014) G. Montúfar, R. Pascanu, K. Cho, and Y. Bengio. On the
    number of linear regions of deep neural networks. In *Neural Information Processing
    Systems (NeurIPS)*, volume 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montúfar et al. (2022) G. Montúfar, Y. Ren, and L. Zhang. Sharp bounds for the
    number of regions of maxout networks and vertices of Minkowski sums. *SIAM Journal
    on Applied Algebra and Geometry*, 6(4), 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Motzkin (1936) T. Motzkin. *Beitrage zur theorie der linearen Ungleichungen*.
    PhD thesis, University of Basel, 1936.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukhopadhyay et al. (1993) S. Mukhopadhyay, A. Roy, L. S. Kim, and S. Govil.
    A polynomial time algorithm for generating neural networks for pattern classification:
    Its stability properties and some test results. *Neural Computation*, 5(2):317–330,
    1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair and Hinton (2010) V. Nair and G. Hinton. Rectified linear units improve
    restricted boltzmann machines. In *International Conference on Machine Learning
    (ICML)*, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narodytska et al. (2018) N. Narodytska, S. Kasiviswanathan, L. Ryzhyk, M. Sagiv,
    and T. Walsh. Verifying properties of binarized deep neural networks. In *AAAI
    Conference on Artificial Intelligence*, volume 32, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nelles et al. (2000) O. Nelles, A. Fink, and R. Isermann. Local linear model
    trees (LOLIMOT) toolbox for nonlinear system identification. In *IFAC Symposium
    on System Identification (SYSID)*, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nesterov (1983) Y. E. Nesterov. A method of solving a convex programming problem
    with convergence rate $o\bigl{(}\frac{1}{k^{2}}\bigr{)}$. *Doklady Akademii Nauk*,
    269:543–547, 1983.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newton and Papachristodoulou (2021) M. Newton and A. Papachristodoulou. Exploiting
    sparsity for neural network verification. In *Learning for Dynamics and Control
    (L4DC)*, pages 715–727. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nguyen et al. (2018) Q. Nguyen, M. C. Mukkamala, and M. Hein. Neural networks
    should be wide enough to learn disconnected decision regions. In *International
    Conference on Machine Learning (ICML)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen and Huchette (2022) T. Nguyen and J. Huchette. Neural network verification
    as piecewise linear optimization: Formulations for the composition of staircase
    functions. *arXiv:2211.14706*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novak et al. (2018) R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, and
    J. Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical
    study. In *International Conference on Learning Representations (ICLR)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI. Introducing chatgpt, 2022. URL https://openai.com/blog/chatgpt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI et al. (2019) OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dȩbiak,
    C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, R. Józefowicz, S. Gray,
    C. Olsson, J. Pachocki, M. Petrov, H. P. de Oliveira Pinto, J. Raiman, T. Salimans,
    J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski, and S. Zhang.
    Dota 2 with large scale deep reinforcement learning. *arXiv:1912.06680*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padberg (2000) M. Padberg. Approximating separable nonlinear functions via mixed
    zero-one programs. *Operations Research Letters*, 27(1):1–5, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papalexopoulos et al. (2022) T. P. Papalexopoulos, C. Tjandraatmadja, R. Anderson,
    J. P. Vielma, and D. Belanger. Constrained discrete black-box optimization using
    mixed-integer programming. In *International Conference on Machine Learning (ICML)*,
    volume 162, pages 17295–17322, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. (2019) D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.
    Cubuk, and Q. V. Le. SpecAugment: A simple data augmentation method for automatic
    speech recognition. In *Interspeech*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021a) S. Park, C. Yun, J. Lee, and J. Shin. Minimum width for
    universal approximation. In *International Conference on Learning Representations
    (ICLR)*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021b) Y. Park, S. Lee, G. Kim, and D. M. Blei. Unsupervised representation
    learning via neural activation coding. In *International Conference on Machine
    Learning (ICML)*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pascanu et al. (2014) R. Pascanu, G. Montúfar, and Y. Bengio. On the number
    of response regions of deep feedforward networks with piecewise linear activations.
    In *International Conference on Learning Representations (ICLR)*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patrick L. Combettes (2019) J.-C. P. Patrick L. Combettes. Lipschitz certificates
    for layered network structures driven by averaged activation operators. *arXiv:1903.01014*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perakis and Tsiourvas (2022) G. Perakis and A. Tsiourvas. Optimizing objective
    functions from trained relu neural networks via sampling. *arXiv:2205.14189*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peters et al. (2018) M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark,
    K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In *Conference
    of the North American Chapter of the Association for Computational Linguistics
    (NAACL)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phuong and Lampert (2020) M. Phuong and C. H. Lampert. Functional vs. parametric
    equivalence of ReLU networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pilanci and Ergen (2020) M. Pilanci and T. Ergen. Neural networks are convex
    regularizers: Exact polynomial-time convex optimization formulations for two-layer
    networks. In *International Conference on Machine Learning (ICML)*, pages 7695–7705\.
    PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pokutta et al. (2020) S. Pokutta, C. Spiegel, and M. Zimmer. Deep neural network
    training with frank-wolfe. *arXiv:2010.07243*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polyak (1964) B. T. Polyak. Some methods of speeding up the convergence of iteration
    methods. *USSR Computational Mathematics and Mathematical Physics*, 4:1–17, 1964.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pulina and Tacchella (2010) L. Pulina and A. Tacchella. An abstraction-refinement
    approach to verification of artificial neural networks. In *Computer Aided Verification
    (CAV)*, pages 243–257, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2018) A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.
    Improving language understanding by generative pre-training. Technical report,
    OpenAI, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raghu et al. (2017) M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Dickstein.
    On the expressive power of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raghunathan et al. (2018) A. Raghunathan, J. Steinhardt, and P. S. Liang. Semidefinite
    relaxations for certifying robustness to adversarial examples. *Neural Information
    Processing Systems (NeurIPS)*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramachandran et al. (2018) P. Ramachandran, B. Zoph, and Q. V. Le. Searching
    for activation functions. In *ICLR Workshop Track*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raman and Grossmann (1994) R. Raman and I. Grossmann. Modelling and computational
    techniques for logic based integer programming. *Computers & Chemical Engineering*,
    18(7):563–578, 1994.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen.
    Hierarchical text-conditional image generation with CLIP latents. *arXiv:2204.06125*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robbins and Monro (1951) H. Robbins and S. Monro. A stochastic approximation
    method. *The Annals of Mathematical Statistics*, 22(3):400–407, 1951.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robinson et al. (2019) H. Robinson, A. Rasheed, and O. San. Dissecting deep
    neural networks. *arXiv:1910.03879*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rolnick and Kording (2020) D. Rolnick and K. Kording. Reverse-engineering deep
    ReLU networks. In *International Conference on Machine Learning (ICML)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rosenblatt (1957) F. Rosenblatt. The Perceptron — a perceiving and recognizing
    automaton. Technical Report 85-460-1, Cornell Aeronautical Laboratory, 1957.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rössig and Petkovic (2021) A. Rössig and M. Petkovic. Advances in verification
    of relu neural networks. *Journal of Global Optimization*, 81:109–152, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roth (2021) K. Roth. A primer on multi-neuron relaxation-based adversarial robustness
    certification. In *ICML 2021 Workshop on Adversarial Machine Learning*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roy et al. (1993) A. Roy, L. S. Kim, and S. Mukhopadhyay. A polynomial time
    algorithm for the construction and training of a class of multilayer perceptrons.
    *Neural Networks*, 6(4):535–545, 1993.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rubies-Royo et al. (2019) V. Rubies-Royo, R. Calandra, D. M. Stipanovic, and
    C. Tomlin. Fast neural network verification via shadow prices. *arXiv:1902.07247*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rumelhart et al. (1986) D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning
    representations by back-propagating errors. *Nature*, 323:533–536, 1986.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ryu et al. (2020) M. Ryu, Y. Chow, R. Anderson, C. Tjandraatmadja, and C. Boutilier.
    Caql: Continuous action q-learning. In *International Conference on Learning Representations
    (ICLR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sahiner et al. (2021) A. Sahiner, T. Ergen, J. M. Pauly, and M. Pilanci. Vector-output
    re{lu} neural network problems are copositive programs: Convex analysis of two
    layer networks and polynomial-time algorithms. In *International Conference on
    Learning Representations (ICLR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salman et al. (2019) H. Salman, G. Yang, H. Zhang, C.-J. Hsieh, and P. Zhang.
    A convex relaxation barrier to tight robustness verification of neural networks.
    *Neural Information Processing Systems (NeurIPS)*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sandler et al. (2018) M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C.
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In *Conference on
    Computer Vision and Pattern Recognition (CVPR)*, pages 4510–4520, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sattelberg et al. (2020) B. Sattelberg, R. Cavalieri, M. Kirby, C. Peterson,
    and R. Beveridge. Locally linear attributes of ReLU neural networks. *arXiv:2012.01940*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say et al. (2017) B. Say, G. Wu, Y. Q. Zhou, and S. Sanner. Nonlinear hybrid
    planning with deep net learned transition models and mixed-integer linear programming.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, pages
    750–756, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schmidhuber (2015) J. Schmidhuber. Deep learning in neural networks: An overview.
    *Neural Networks*, 61:85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schumann et al. (2003) J. Schumann, P. Gupta, and S. Nelson. On verification
    & validation of neural network based controllers. In *Engineering Applications
    of Neural Networks (EANN)*, 2003.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwan et al. (2022) R. Schwan, C. N. Jones, and D. Kuhn. Stability verification
    of neural network controllers using mixed-integer programming. *arXiv:2206.13374*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schweidtmann and Mitsos (2019) A. M. Schweidtmann and A. Mitsos. Deterministic
    global optimization with artificial neural networks embedded. *Journal of Optimization
    Theory and Applications*, 180(3):925–948, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schweidtmann et al. (2022) A. M. Schweidtmann, J. M. Weber, C. Wende, L. Netze,
    and A. Mitsos. Obey validity limits of data-driven models through topological
    data analysis and one-class classification. *Optimization and Engineering*, 23(2):855–876,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seck et al. (2021) I. Seck, G. Loosli, and S. Canu. Linear program powered attack.
    In *International Joint Conference on Neural Networks (IJCNN)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra (2020) T. Serra. Enumerative branching with less repetition. In *International
    Conference on Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, pages 399–416\. Springer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra and Hooker (2020) T. Serra and J. Hooker. Compact representation of near-optimal
    integer programming solutions. *Mathematical Programming*, 182:199–232, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra and Ramalingam (2020) T. Serra and S. Ramalingam. Empirical bounds on
    linear regions of deep rectifier networks. In *AAAI Conference on Artificial Intelligence*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra et al. (2018) T. Serra, C. Tjandraatmadja, and S. Ramalingam. Bounding
    and counting linear regions of deep neural networks. In *International Conference
    on Machine Learning (ICML)*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra et al. (2020) T. Serra, A. Kumar, and S. Ramalingam. Lossless compression
    of deep neural networks. In *International Conference on the Integration of Constraint
    Programming, Artificial Intelligence, and Operations Research (CPAIOR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serra et al. (2021) T. Serra, X. Yu, A. Kumar, and S. Ramalingam. Scaling up
    exact neural network compression by ReLU stability. In *Neural Information Processing
    Systems (NeurIPS)*, volume 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022) C. Shi, M. Emadikhiav, L. Lozano, and D. Bergman. Careful!
    training relevance is real. *arXiv:2201.04429*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sidrane et al. (2022) C. Sidrane, A. Maleki, A. Irfan, and M. J. Kochenderfer.
    Overt: An algorithm for safety verification of neural network control policies
    for nonlinear systems. *Journal of Machine Learning Research*, 23(117):1–45, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sildir and Aydin (2022) H. Sildir and E. Aydin. A mixed-integer linear programming
    based training and feature selection method for artificial neural networks using
    piece-wise linear approximations. *Chemical Engineering Science*, 249:117273,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2017) D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou,
    A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, Y. Chen, T. Lillicrap,
    F. Hui, L. Sifre, G. van den Driessche, T. Graepel, and D. Hassabis. Mastering
    the game of go without human knowledge. *Nature*, 550:354–359, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2018) G. Singh, T. Gehr, M. Mirman, M. Püschel, and M. Vechev.
    Fast and effective robustness certification. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2019a) G. Singh, R. Ganvir, M. Püschel, and M. Vechev. Beyond
    the single neuron convex barrier for neural network certification. *Neural Information
    Processing Systems (NeurIPS)*, 32, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2019b) G. Singh, T. Gehr, M. Püschel, and M. Vechev. An abstract
    domain for certifying neural networks. *Proceedings of the ACM on Programming
    Languages (POPL)*, 3:1–30, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. (2021) H. Singh, M. P. Kumar, P. Torr, and K. D. Dvijotham. Overcoming
    the convex barrier for simplex inputs. In *Neural Information Processing Systems
    (NeurIPS)*, volume 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith and Winkler (2006) J. E. Smith and R. L. Winkler. The optimizer’s curse:
    Skepticism and postdecision surprise in decision analysis. *Management Science*,
    52(3):311–322, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2014) N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
    and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting.
    *Journal of Machine Learning Research*, 15(56):1929–1958, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strong et al. (2021) C. A. Strong, H. Wu, A. Zeljić, K. D. Julian, G. Katz,
    C. Barrett, and M. J. Kochenderfer. Global optimization of objective functions
    represented by ReLU networks. *Machine Learning*, pages 1–28, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Strong et al. (2022) C. A. Strong, S. M. Katz, A. L. Corso, and M. J. Kochenderfer.
    ZoPE: a fast optimizer for ReLU networks with low-dimensional inputs. In *NASA
    Formal Methods: 14th International Symposium, (NFM)*, pages 299–317\. Springer,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sudjianto et al. (2020) A. Sudjianto, W. Knauth, R. Singh, Z. Yang, and A. Zhang.
    Unwrapping the black box of deep ReLU networks: Interpretability, diagnostics,
    and simplification. *arXiv:2011.04041*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2013) I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On
    the importance of initialization and momentum in deep learning. In *International
    Conference on Machine Learning (ICML)*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence
    learning with neural networks. In *Neural Information Processing Systems (NeurIPS)*,
    volume 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2014) C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,
    I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2015) C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,
    D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In
    *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Takai et al. (2021) Y. Takai, A. Sannai, and M. Cordonnier. On the number of
    linear functions composing deep neural network: Towards a refined definition of
    neural networks complexity. In *International Conference on Artificial Intelligence
    and Statistics (AISTATS)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. (2022) Q. Tao, L. Li, X. Huang, X. Xi, S. Wang, and J. A. Suykens.
    Piecewise linear neural networks and deep learning. *Nature Reviews Methods Primers*,
    2, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Telgarsky (2015) M. Telgarsky. Representation benefits of deep feedforward networks.
    *arXiv:1509.08101*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorbjarnarson and Yorke-Smith (2020) T. Thorbjarnarson and N. Yorke-Smith.
    On training neural networks with mixed integer programming. *arXiv:2009.03825*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thorbjarnarson and Yorke-Smith (2023) T. Thorbjarnarson and N. Yorke-Smith.
    Optimal training of integer-valued neural networks with mixed integer programming.
    *PLOS One*, 18(2):e0261029, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tiwari and Konidaris (2022) S. Tiwari and G. Konidaris. Effects of data geometry
    in early deep learning. In *Neural Information Processing Systems (NeurIPS)*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tjandraatmadja et al. (2020) C. Tjandraatmadja, R. Anderson, J. Huchette, W. Ma,
    K. K. Patel, and J. P. Vielma. The convex relaxation barrier, revisited: Tightened
    single-neuron relaxations for neural network verification. *Neural Information
    Processing Systems (NeurIPS)*, 33:21675–21686, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tjeng et al. (2019) V. Tjeng, K. Xiao, and R. Tedrake. Evaluating robustness
    of neural networks with mixed integer programming. In *International Conference
    on Learning Representations (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trimmel et al. (2021) M. Trimmel, H. Petzka, and C. Sminchisescu. TropEx: An
    algorithm for extracting linear terms in deep neural networks. In *International
    Conference on Learning Representations (ICLR)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tsay and Baldea (2019) C. Tsay and M. Baldea. 110th anniversary: using data
    to bridge the time and length scales of process systems. *Industrial & Engineering
    Chemistry Research*, 58(36):16696–16708, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsay et al. (2021) C. Tsay, J. Kronqvist, A. Thebelt, and R. Misener. Partition-based
    formulations for mixed-integer optimization of trained ReLU neural networks. In
    *Neural Information Processing Systems (NeurIPS)*, volume 34, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tseran and Montúfar (2021) H. Tseran and G. Montúfar. On the expected complexity
    of maxout networks. In *Neural Information Processing Systems (NeurIPS)*, volume 34,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unser (2019) M. Unser. A representer theorem for deep neural networks. *Journal
    of Machine Learning Research*, 20:1–30, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In *Neural
    Information Processing Systems (NeurIPS)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vielma (2015) J. P. Vielma. Mixed integer linear programming formulation techniques.
    *SIAM Review*, 57(1):3–57, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vielma (2019) J. P. Vielma. Small and strong formulations for unions of convex
    sets from the cayley embedding. *Mathematical Programming*, 177(1-2):21–53, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vielma et al. (2010) J. P. Vielma, S. Ahmed, and G. Nemhauser. Mixed-integer
    models for nonseparable piecewise-linear optimization: Unifying framework and
    extensions. *Operations Research*, 58(2):303–315, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villani and Schoots (2023) M. J. Villani and N. Schoots. Any deep ReLU network
    is shallow. *arXiv:2306.11827*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vincent and Schwager (2021) J. A. Vincent and M. Schwager. Reachable polyhedral
    marching (RPM): A safety verification algorithm for robotic systems with deep
    neural network components. In *IEEE International Conference on Robotics and Automation
    (ICRA)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals et al. (2017) O. Vinyals, T. Ewalds, S. Bartunov, P. Georgiev, A. S.
    Vezhnevets, M. Yeo, A. Makhzani, H. Küttler, J. Agapiou, J. Schrittwieser, J. Quan,
    S. Gaffney, S. Petersen, K. Simonyan, T. Schaul, H. van Hasselt, D. Silver, T. Lillicrap,
    K. Calderone, P. Keet, A. Brunasso, D. Lawrence, A. Ekermo, J. Repp, and R. Tsing.
    StarCraft II: A new challenge for reinforcement learning. *arXiv:1708.04782*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Virmaux and Scaman (2018) A. Virmaux and K. Scaman. Lipschitz regularity of
    deep neural networks: analysis and efficient estimation. In *Neural Information
    Processing Systems (NeurIPS)*, volume 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volpp et al. (2020) M. Volpp, L. P. Fröhlich, K. Fischer, A. Doerr, S. Falkner,
    F. Hutter, and C. Daniel. Meta-learning acquisition functions for transfer learning
    in bayesian optimization. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021) K. Wang, L. Lozano, D. Bergman, and C. Cardonha. A two-stage
    exact algorithm for optimization of neural network ensemble. In *International
    Conference on the Integration of Constraint Programming, Artificial Intelligence,
    and Operations Research (CPAIOR)*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) K. Wang, L. Lozano, C. Cardonha, and D. Bergman. Optimizing
    over an ensemble of trained neural networks. *INFORMS Journal on Computing*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018a) S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Efficient
    formal safety analysis of neural networks. *Neural Information Processing Systems
    (NeurIPS)*, 31, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018b) S. Wang, K. Pei, J. Whitehouse, J. Yang, and S. Jana. Formal
    security analysis of neural networks using symbolic intervals. In *27th $\{$USENIX$\}$
    Security Symposium ($\{$USENIX$\}$ Security 18)*, pages 1599–1614, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang (2022) Y. Wang. Estimation and comparison of linear regions for relu networks.
    In *International Joint Conference on Artificial Intelligence (IJCAI)*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng et al. (1992) J. Weng, N. Ahuja, and T. Huang. Cresceptron: a self-organizing
    neural network which grows adaptively. In *International Joint Conference on Neural
    Networks (IJCNN)*, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weng et al. (2018) L. Weng, H. Zhang, H. Chen, Z. Song, C.-J. Hsieh, L. Daniel,
    D. Boning, and I. Dhillon. Towards fast computation of certified robustness for
    ReLU networks. In *International Conference on Machine Learning (ICML)*, pages
    5276–5285, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Werbos (1974) P. Werbos. *Beyond Regression: New Tools for Prediction and Analysis
    in the Behavioral Sciences*. PhD thesis, Harvard University, 1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wicker et al. (2020) M. Wicker, L. Laurenti, A. Patane, and M. Kwiatkowska.
    Probabilistic safety for Bayesian neural networks. In *Conference on Uncertainty
    in Artificial Intelligence (UAI)*, pages 1198–1207, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wicker et al. (2022) M. Wicker, J. Heo, L. Costabello, and A. Weller. Robust
    explanation constraints for neural networks. *arXiv:2212.08507*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilhelm et al. (2022) M. E. Wilhelm, C. Wang, and M. D. Stuber. Convex and concave
    envelopes of artificial neural network activation functions for deterministic
    global optimization. *Journal of Global Optimization*, pages 1–26, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong and Kolter (2018) E. Wong and Z. Kolter. Provable defenses against adversarial
    examples via the convex outer adversarial polytope. In *International Conference
    on Machine Learning (ICML)*, pages 5286–5295, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wong et al. (2018) E. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling
    provable adversarial defenses. *Neural Information Processing Systems (NeurIPS)*,
    31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wright (2018) S. J. Wright. Optimization algorithms for data analysis. *The
    Mathematics of Data*, 25:49, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2020) G. Wu, B. Say, and S. Sanner. Scalable planning with deep neural
    network learned transition models. *Journal of Artificial Intelligence Research*,
    68:571–606, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) H. Wu, A. Zeljić, G. Katz, and C. Barrett. Efficient neural
    network analysis with sum-of-infeasibilities. In *Tools and Algorithms for the
    Construction and Analysis of Systems (TACAS)*, pages 143–163\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiang et al. (2017) W. Xiang, H.-D. Tran, and T. T. Johnson. Reachable set computation
    and safety verification for neural networks with ReLU activations. *arXiv:1712.08163*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2019) K. Xiao, V. Tjeng, N. Shafiullah, and A. Madry. Training
    for faster adversarial robustness verification via inducing ReLU stability. *International
    Conference on Learning Representations (ICLR)*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020a) J. Xie, Z. Shen, C. Zhang, B. Wang, and H. Qian. Efficient
    projection-free online methods with stochastic recursive gradient. In *AAAI Conference
    on Artificial Intelligence*, volume 34, pages 6446–6453, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020b) Q. Xie, M.-T. Luong, E. Hovy, and Q. V. Le. Self-training
    with noisy student improves ImageNet classification. In *Conference on Computer
    Vision and Pattern Recognition (CVPR)*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. (2020c) Y. Xie, G. Chen, and Q. Li. A general computational framework
    to measure the expressiveness of complex networks using a tighter upper bound
    of linear regions. *arXiv:2012.04428*, 2020c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiong et al. (2020) H. Xiong, L. Huang, M. Yu, L. Liu, F. Zhu, and L. Shao.
    On the number of linear regions of convolutional neural networks. In *International
    Conference on Machine Learning (ICML)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2022) S. Xu, J. Vaughan, J. Chen, A. Zhang, and A. Sudjianto. Traversing
    the local polytopes of ReLU neural networks. In *AAAI Workshop AdvML*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) D. Yang, P. Balaprakash, and S. Leyffer. Modeling design
    and control problems involving neural network surrogates. *Computational Optimization
    and Applications*, pages 1–42, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2020) X. Yang, H.-D. Tran, W. Xiang, and T. Johnson. Reachability
    analysis for feed-forward neural networks using face lattices. *arXiv:2003.01226*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) X. Yang, T. Yamaguchi, H.-D. Tran, B. Hoxha, T. T. Johnson,
    and D. Prokhorov. Reachability analysis of convolutional neural networks. *arXiv:2106.12074*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yarotsky (2017) D. Yarotsky. Error bounds for approximations with deep ReLU
    networks. *Neural Networks*, 94, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zakrzewski (2001) R. R. Zakrzewski. Verification of a trained neural network
    accuracy. In *International Joint Conference on Neural Networks (IJCNN)*, volume 3,
    pages 1657–1662\. IEEE, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaslavsky (1975) T. Zaslavsky. *Facing Up to Arrangements: Face-Count Formulas
    for Partitions of Space by Hyperplanes*. American Mathematical Society, 1975.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola. *Dive into
    Deep Learning*. 2023. https://d2l.ai.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018a) H. Zhang, T.-W. Weng, P.-Y. Chen, C.-J. Hsieh, and L. Daniel.
    Efficient neural network robustness certification with general activation functions.
    *Neural Information Processing Systems (NeurIPS)*, 31, 2018a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li,
    D. Boning, and C.-J. Hsieh. Towards stable and efficient training of verifiably
    robust neural networks. In *International Conference on Learning Representations
    (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) H. Zhang, S. Wang, K. Xu, L. Li, B. Li, S. Jana, C.-J. Hsieh,
    and J. Z. Kolter. General cutting planes for bound-propagation-based neural network
    verification. In *Neural Information Processing Systems (NeurIPS)*, volume 35,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018b) L. Zhang, G. Naitzat, and L.-H. Lim. Tropical geometry
    of deep neural networks. In *International Conference on Machine Learning (ICML)*,
    2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang (2020) R. Zhang. On the tightness of semidefinite relaxations for certifying
    robustness to adversarial examples. *Neural Information Processing Systems (NeurIPS)*,
    33:3808–3820, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Wu (2020) X. Zhang and D. Wu. Empirical studies on the properties
    of linear regions in deep neural networks. In *International Conference on Learning
    Representations (ICLR)*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) S. Zhao, C. Tsay, and J. Kronqvist. Model-based feature
    selection for neural networks: A mixed-integer programming approach. *arXiv:2302.10344*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou and Schoellig (2019) S. Zhou and A. P. Schoellig. An analysis of the expressiveness
    of deep neural network architectures based on their Lipschitz constants. *arXiv:1912.11511*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2020) R. Zhu, B. Lin, and H. Tang. Bounding the number of linear
    regions in local area for neural networks with ReLU activations. *arXiv:2007.06803*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2019) D. Zou, R. Balan, and M. Singh. On Lipschitz bounds of general
    convolutional neural networks. *IEEE Transactions on Information Theory*, 66(3):1738–1759,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
