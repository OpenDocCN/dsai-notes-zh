- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:04
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1907.03069] Deep learning for fine-grained image analysis: A survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1907.03069] 深度学习在细粒度图像分析中的应用：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1907.03069](https://ar5iv.labs.arxiv.org/html/1907.03069)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1907.03069](https://ar5iv.labs.arxiv.org/html/1907.03069)
- en: 'Deep learning for fine-grained image analysis: A survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在细粒度图像分析中的应用：综述
- en: Xiu-Shen Wei¹    Jianxin Wu²    Quan Cui^(1,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xiu-Shen Wei¹    Jianxin Wu²    Quan Cui^(1,3)
- en: ¹Megvii Research Nanjing, Megvii Technology, Nanjing, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹旷视研究南京，旷视科技，中国南京
- en: ²National Key Laboratory for Novel Software Technology, Nanjing University,
    Nanjing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²国家重点新型软件技术实验室，南京大学，中国南京
- en: ³Graduate School of IPS, Waseda University, Fukuoka, Japan
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³早稻田大学IPS研究生院，日本福冈
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Computer vision (CV) is the process of using machines to understand and analyze
    imagery, which is an integral branch of artificial intelligence. Among various
    research areas of CV, fine-grained image analysis (FGIA) is a longstanding and
    fundamental problem, and has become ubiquitous in diverse real-world applications.
    The task of FGIA targets analyzing visual objects from subordinate categories,
    *e.g.*, species of birds or models of cars. The small inter-class variations and
    the large intra-class variations caused by the fine-grained nature makes it a
    challenging problem. During the booming of deep learning, recent years have witnessed
    remarkable progress of FGIA using deep learning techniques. In this paper, we
    aim to give a survey on recent advances of deep learning based FGIA techniques
    in a systematic way. Specifically, we organize the existing studies of FGIA techniques
    into three major categories: fine-grained image recognition, fine-grained image
    retrieval and fine-grained image generation. In addition, we also cover some other
    important issues of FGIA, such as publicly available benchmark datasets and its
    related domain specific applications. Finally, we conclude this survey by highlighting
    several directions and open problems which need be further explored by the community
    in the future.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）是使用机器理解和分析图像的过程，是人工智能的一个重要分支。在计算机视觉的各种研究领域中，细粒度图像分析（FGIA）是一个长期存在且基础性的问题，并且在各种实际应用中变得无处不在。FGIA任务旨在分析来自下级类别的视觉对象，例如鸟类的种类或汽车的型号。由于细粒度特性造成的类间小变化和类内大变化使其成为一个具有挑战性的问题。在深度学习蓬勃发展的背景下，近年来在使用深度学习技术的FGIA上取得了显著进展。本文旨在系统地综述基于深度学习的FGIA技术的最新进展。具体而言，我们将现有的FGIA技术研究组织成三个主要类别：细粒度图像识别、细粒度图像检索和细粒度图像生成。此外，我们还涉及一些其他重要的FGIA问题，如公开的基准数据集及其相关领域应用。最后，我们通过突出几个方向和需要进一步探索的开放问题来总结这次综述。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Computer vision (CV) is an interdisciplinary scientific field of artificial
    intelligence (AI), which deals with how computers can be made to gain high-level
    understanding from digital images or videos. The tasks of computer vision include
    methods for acquiring, processing, analyzing and understanding digital images,
    and the process of extracting numerical or symbolic information, *e.g.*, in the
    forms of decisions or predictions, from high-dimensional raw image data in the
    real world.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉（CV）是人工智能（AI）的一门跨学科科学领域，处理计算机如何从数字图像或视频中获得高级理解。计算机视觉的任务包括获取、处理、分析和理解数字图像的方法，以及从现实世界中的高维原始图像数据中提取数值或符号信息，例如，以决策或预测的形式。
- en: 'As an interesting, fundamental and challenging problem in computer vision,
    fine-grained image analysis (FGIA) has been an active area of research for several
    decades. The goal of FGIA is to retrieve, recognize and generate images belonging
    to multiple subordinate categories of a super-category (*aka* meta-category),
    *e.g.*, different species of animals/plants, different models of cars, different
    kinds of retail products, etc (cf. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep learning for fine-grained image analysis: A survey")). In the real-world,
    FGIA enjoys a wide-range of applications in both industry and research societies,
    such as automatic biodiversity monitoring, climate change evaluation, intelligent
    retail, intelligent transportation, and many more. Particularly, a number of influential
    academic competitions about FGIA are frequently held on Kaggle.¹¹1Kaggle is an
    online community of data scientists and machine learners: [https://www.kaggle.com/](https://www.kaggle.com/).
    Several representative competitions, to name a few, are the Nature Conservancy
    Fisheries Monitoring (for fish species categorization), Humpback Whale Identification
    (for whale identity categorization) and so on. Each competition attracted more
    than 300 teams worldwide to participate, and some even exceeded 2,000 teams.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '作为计算机视觉中的一个有趣、基础且具有挑战性的问题，细粒度图像分析（FGIA）已经成为了数十年来活跃的研究领域。FGIA的目标是检索、识别和生成属于超类别（*即*元类别）多个从属类别的图像，例如，不同物种的动物/植物，不同型号的汽车，不同种类的零售产品等（参见图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep learning for fine-grained image analysis: A
    survey")）。在现实世界中，FGIA在工业和研究领域中享有广泛的应用，如自动生物多样性监测、气候变化评估、智能零售、智能交通等。特别是，关于FGIA的许多有影响力的学术竞赛经常在Kaggle上举行。¹¹Kaggle是一个数据科学家和机器学习者的在线社区：[https://www.kaggle.com/](https://www.kaggle.com/)。其中几个具有代表性的竞赛包括自然保护协会渔业监测（用于鱼类物种分类）、座头鲸识别（用于鲸鱼身份分类）等。每个竞赛吸引了全球超过300支团队参与，有些甚至超过了2000支团队。'
- en: '![Refer to caption](img/c125d096aec45f7e12a485e0f0bb53ff.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c125d096aec45f7e12a485e0f0bb53ff.png)'
- en: 'Figure 1: Fine-grained image analysis *vs*. generic image analysis (taking
    the recognitiont task for an example).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：细粒度图像分析*vs*一般图像分析（以识别任务为例）。
- en: On the other hand, deep learning techniques LeCun et al. ([2015](#bib.bib22))
    have emerged in recent years as powerful methods for learning feature representations
    directly from data, and have led to remarkable breakthroughs in the filed of FGIA.
    With rough statistics on each year, on average, there are around ten conference
    papers of deep learning based FGIA techniques published on each of AI’s and CV’s
    premium conferences, like IJCAI, AAAI, CVPR, ICCV, ECCV, etc. It shows that FGIA
    with deep learning is of notable research interests. Given this period of rapid
    evolution, the aim of this paper is to provide a comprehensive survey of the recent
    achievements in the FGIA filed brought by deep learning techniques.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，深度学习技术LeCun等人（[2015](#bib.bib22)）近年来作为直接从数据中学习特征表示的强大方法，已在FGIA领域取得了显著突破。根据每年的粗略统计，平均每年在人工智能和计算机视觉的顶级会议上，例如IJCAI、AAAI、CVPR、ICCV、ECCV等，都会有大约十篇基于深度学习的FGIA技术的会议论文发表。这表明，深度学习在FGIA领域具有显著的研究兴趣。在这一快速演变的时期，本文的目的是提供对深度学习技术在FGIA领域带来的最新成就的全面综述。
- en: In the literature, there was an existing survey related to fine-grained tasks,
    *i.e.*, Zhao et al. ([2017](#bib.bib49)), which simply included several fine-grained
    *recognition* approaches for comparisons. Our work differs with it in that ours
    is more comprehensive. Specifically, except for fine-grained recognition, we also
    analyze and discuss the other two central fine-grained analysis tasks, *i.e.*,
    fine-grained image *retrieval* and fine-grained image *generation*, which can
    not be overlooked as they are two integral aspects of FGIA. Additionally, on another
    important AI conference in the Pacific Rim nations, PRICAI, Wei and Wu organized
    a specific tutorial²²2[http://www.weixiushen.com/tutorial/PRICAI18/FGIA.html](http://www.weixiushen.com/tutorial/PRICAI18/FGIA.html)
    aiming at the fine-grained image analysis topic. We refer interested readers to
    the tutorial which provides some additional detailed information.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/270d2ed0b0128d0f3a04334ffb529a9b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Main aspects of our hierarchical and structrual organization of fine-grained
    image analysis (FGIA) in this survey paper.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, our survey take a unique deep learning based perspective to
    review the recent advances of FGIA in a systematic and comprehensive manner. The
    main contributions of this survey are three-fold:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We give a comprehensive review of FGIA techniques based on deep learning, including
    problem backgrounds, benchmark datasets, a family of FGIA methods with deep learning,
    domain-specific FGIA applications, etc.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We provide a systematic overview of recent advances of deep learning based
    FGIA techniques in a hierarchical and structural manner, cf. Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Deep learning for fine-grained image analysis: A survey").'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss the challenges and open issues, and identify the new trends and future
    directions to provide a potential road map for fine-grained researchers or other
    interested readers in the broad AI community.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the survey is organized as follows. Section [2](#S2 "2 Background:
    problem and main challenges ‣ Deep learning for fine-grained image analysis: A
    survey") introduce backgrounds of this paper, *i.e.*, the FGIA problem and its
    main challenges. In Section [3](#S3 "3 Benchmark datasets ‣ Deep learning for
    fine-grained image analysis: A survey"), we review multiple commonly used fine-grained
    benchmark datasets. Section [4](#S4 "4 Fine-grained image recognition ‣ Deep learning
    for fine-grained image analysis: A survey") analyzes the three main paradigms
    of fine-grained image recognition. Section [5](#S5 "5 Fine-grained image retrieval
    ‣ Deep learning for fine-grained image analysis: A survey") presents recent progress
    of fine-grained image retrieval. Section [6](#S6 "6 Fine-grained image generation
    ‣ Deep learning for fine-grained image analysis: A survey") discusses fine-grained
    image generation from a generative perspective. Furthermore, in Section [7](#S7
    "7 Domain specific applications related to fine-grained image analysis ‣ Deep
    learning for fine-grained image analysis: A survey"), we introduce some other
    domain specific applications of real-world related to FGIA. Finally, we conclude
    this paper and discuss future directions and open issues in Section [8](#S8 "8
    Concluding remarks and future directions ‣ Deep learning for fine-grained image
    analysis: A survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '2 Background: problem and main challenges'
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/621364e720d1587511716b6d564d049f.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Key challenge of fine-grained image analysis, *i.e.*, small inter-class
    variations and large intra-class variations. We here present each of four Tern
    species in each row in the figure, respectively.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we summarize the related background of this paper, including
    the problem and its key challenges.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Fine-grained image analysis (FGIA) focuses on dealing with the objects belonging
    to multiple *sub-categories* of the same meta-category (*e.g.*, birds, dogs and
    cars), and generally involves central tasks like fine-grained image recognition,
    fine-grained image retrieval, fine-grained image generation, etc.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'What distinguishes FGIA from the generic one is: in generic image analysis,
    the target objects belong to coarse-grained meta-categories (*e.g.*, birds, oranges
    and dogs), and thus are visually quite different. However, in FGIA, since objects
    come from sub-categories of one meta-category, the fine-grained nature causes
    them visually quite similar. We take image recognition for illustration. As shown
    in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep learning for fine-grained
    image analysis: A survey"), in fine-grained recognition, the task is required
    to identify multiple similar species of dogs, *e.g.*, Husky, Samoyed and Alaska.
    For accurate recognition, it is desirable to distinguish them by capturing slight
    and subtle differences (*e.g.*, ears, noses, tails), which also meets the demand
    of other FGIA tasks (*e.g.*, retrieval and generation).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a10e3b113deeae364dae0b9c5e6dd60a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Example fine-grained images belonging to different species of flowers/vegetable,
    different models of cars/aircrafts and different kinds of retail products. Accurate
    identification of these fine-grained objects requires the dependences on the discriminative
    but subtle object parts or image regions. (Best viewed in color and zoomed in.)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of popular fine-grained image datasets. Note that “BBox” indicates
    whether this dataset provides object bounding box supervisions. “Part anno.” means
    providing the key part localizations. “HRCHY” corresponds to hierarchical labels.
    “ATR” represents the attribute labels (*e.g.*, wing color, male, female, etc).
    “Texts” indicates whether fine-grained text descriptions of images are supplied.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset name | Meta-class | $\sharp$ images | $\sharp$ categories | BBox
    | Part anno. | HRCHY | ATR | Texts |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| Oxford Flower  Nilsback and Zisserman ([2008](#bib.bib29)) | Flowers |     8,189
    |   102 |  |  |  |  | ✓ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| CUB200-2011  Wah et al. ([2011](#bib.bib37)) | Birds |   11,788 |   200 |
    ✓ | ✓ |  | ✓ | ✓ |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Stanford Dog  Khosla et al. ([2011](#bib.bib19)) | Dogs |   20,580 |   120
    | ✓ |  |  |  |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| Stanford Car  Krause et al. ([2013](#bib.bib21)) | Cars |   16,185 |   196
    | ✓ |  |  |  |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| FGVC Aircraft  Maji et al. ([2013](#bib.bib28)) | Aircrafts |   10,000 |
      100 | ✓ |  | ✓ |  |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: '| Birdsnap  Berg et al. ([2014](#bib.bib2)) | Birds |   49,829 |   500 | ✓
    | ✓ |  | ✓ |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
- en: '| Fru92  Hou et al. ([2017](#bib.bib17)) | Fruits |   69,614 |     92 |  |  |
    ✓ |  |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
- en: '| Veg200  Hou et al. ([2017](#bib.bib17)) | Vegetable |   91,117 |   200 |  |  |
    ✓ |  |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
- en: '| iNat2017  Horn et al. ([2017](#bib.bib16)) | Plants & Animals | 859,000 |
    5,089 | ✓ |  | ✓ |  |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
- en: '| RPC  Wei et al. ([2019a](#bib.bib42)) | Retail products |   83,739 |   200
    | ✓ |  | ✓ |  |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
- en: 'Furthermore, fine-grained nature also brings the *small inter-class variations*
    caused by highly similar sub-categories, and the *large intra-class variations*
    in poses, scales and rotations, as presented by Fig. [3](#S2.F3 "Figure 3 ‣ 2
    Background: problem and main challenges ‣ Deep learning for fine-grained image
    analysis: A survey"). It is the opposite of the generic image analysis (*i.e.*,
    the small intra-class variations and the large inter-class variations), which
    makes fine-grained image analysis a challenging problem.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 3 Benchmark datasets
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the past decade, the vision community has released many benchmark fine-grained
    datasets covering diverse domains such as birds Wah et al. ([2011](#bib.bib37));
    Berg et al. ([2014](#bib.bib2)), dogs Khosla et al. ([2011](#bib.bib19)), cars Krause
    et al. ([2013](#bib.bib21)), airplanes Maji et al. ([2013](#bib.bib28)), flowers Nilsback
    and Zisserman ([2008](#bib.bib29)), vegetable Hou et al. ([2017](#bib.bib17)),
    fruits Hou et al. ([2017](#bib.bib17)), retail products Wei et al. ([2019a](#bib.bib42)),
    etc (cf. Fig. [4](#S2.F4 "Figure 4 ‣ 2 Background: problem and main challenges
    ‣ Deep learning for fine-grained image analysis: A survey")). In Table [1](#S2.T1
    "Table 1 ‣ 2 Background: problem and main challenges ‣ Deep learning for fine-grained
    image analysis: A survey"), we list a number of image datasets commonly used by
    the fine-grained community, and specifically indicate their meta-category, the
    amounts of fine-grained images, the number of fine-grained categories, extra different
    kinds of available supervisions, *i.e.*, bounding boxes, part annotations, hierarchical
    labels, attribute labels and text visual descriptions, cf. Fig. [5](#S3.F5 "Figure
    5 ‣ 3 Benchmark datasets ‣ Deep learning for fine-grained image analysis: A survey").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: These datasets have been one of the most important factors for the considerable
    progress in the filed, not only as a common ground for measuring and comparing
    performance of competing approaches, but also pushing this filed towards increasingly
    complex, practical and challenging problems.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afebbdc0aebf93d9000dcc6c0db4feef.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: An example image with its supervisions associated with *CUB200-2011*.
    As shown, multiple types of supervisions include: image labels, part annotations
    (*aka* key point localizations), object bounding boxes (*i.e.*, the green one),
    attribute labels (*i.e.*, “ATR”), and text descriptions by natural languages.
    (Best viewed in color.)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, among them, *CUB200-2011* is one of the most popular fine-grained
    datasets. Almost all the FGIA approaches choose it for comparisons with state-of-the-arts.
    Moreover, constant contributions are made upon *CUB200-2011* for further research,
    *e.g.*, collecting text descriptions of the fine-grained images for multi-modality
    analysis, cf. Reed et al. ([2016](#bib.bib32)); He and Peng ([2017a](#bib.bib14)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in recent years, more challenging and practical fine-grained datasets
    are proposed increasingly, *e.g.*, *iNat2017* for natural species of plants, animals Horn
    et al. ([2017](#bib.bib16)) and *RPC* for daily retail products Wei et al. ([2019a](#bib.bib42)).
    Many novel features deriving from these datasets are, to name a few, large-scale,
    hierarchical structure, domain gap and long-tail distribution, which reveals the
    practical requirements in real-world and could arouse the studies of FGIA in more
    realistic settings.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: 4 Fine-grained image recognition
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fine-grained image recognition has been the most active research area of FGIA
    in the past decade. In this section, we review the milestones of fine-grained
    recognition frameworks since deep learning entered the filed. Broadly, these fine-grained
    recognition approaches can be organized into three main paradigms, *i.e.*, fine-grained
    recognition (1) with localization-classification subnetworks; (2) with end-to-end
    feature encoding and (3) with external information. Among them, the first and
    second paradigms restrict themselves by only utilizing the supervisions associated
    with fine-grained images such as image labels, bounding boxes, part annotations,
    etc. In addition, automatic recognition systems cannot yet achieve excellent performance
    due to the fine-grained challenges. Thus, researchers gradually attempt to involve
    external but cheap information (*e.g.*, web data, text descriptions) into fine-grained
    recognition for further improving accuracy, which corresponds to the third paradigm
    of fine-grained recognition. Popularly used evaluation metric in fine-grained
    recognition is the averaged classification accuracy across all the subordinate
    categories of the datasets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 By localization-classification subnetworks
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To mitigate the challenge of intra-class variations, researchers in the fine-grained
    community pay attentions on capturing discriminative semantic parts of fine-grained
    objects and then constructing a mid-level representation corresponding to these
    parts for the final classification. Specifically, a localization subnetwork is
    designed for locating these key parts. While later, a classification subnetwork
    follows and is employed for recognition. The framework of such two collaborative
    subnetworks forms the first paradigm, *i.e.*, fine-grained recognition with *localization-classification
    subnetworks*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to the localization information, *e.g.*, part-level bounding boxes or
    segmentation masks, it can obtain more discriminative mid-level (part-level) representations
    w.r.t. these fine-grained parts. Also, it further enhances the learning capability
    of the classification subnetwork, which could significantly boost the final recognition
    accuracy.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Earlier works belonging to this paradigm depend on additional dense part annotations
    (*aka* key points localization) to locate semantic key parts (*e.g.*, head, torso)
    of objects. Some of them learn part-based detectors Zhang et al. ([2014](#bib.bib47));
    Lin et al. ([2015a](#bib.bib25)), and some of them leverage segmentation methods
    for localizing parts Wei et al. ([2018a](#bib.bib40)). Then, these methods concatenate
    multiple part-level features as a whole image representation, and feed it into
    the following classification subnetwork for final recognition. Thus, these approaches
    are also termed as *part-based* recognition methods.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: However, obtaining such dense part annotations is labor-intensive, which limits
    both scalability and practicality of real-world fine-grained applications. Recently,
    it emerges a trend that more techniques under this paradigm only require image
    labels Jaderberg et al. ([2015](#bib.bib18)); Fu et al. ([2017](#bib.bib11));
    Zheng et al. ([2017](#bib.bib50)); Sun et al. ([2018](#bib.bib35)) for accurate
    part localization. The common motivation of them is to first find the corresponding
    parts and then compare their appearance. Concretely, it is desirable to capture
    semantic parts (*e.g.*, head and torso) to be shared across fine-grained categories,
    and meanwhile, it is also eager for discovering the subtle differences between
    these part representations. Advanced techniques, like attention mechanisms Yang
    et al. ([2018](#bib.bib46)) and multi-stage strategies He and Peng ([2017b](#bib.bib15))
    complicate the joint training of the integrated localization-classification subnetworks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 By end-to-end feature encoding
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Different from the first paradigm, the second paradigm, *i.e.*, *end-to-end
    feature encoding*, leans to directly learn a more discriminative feature representation
    by developing powerful deep models for fine-grained recognition. The most representative
    method among them is Bilinear CNNs Lin et al. ([2015b](#bib.bib26)), which represents
    an image as a pooled outer product of features derived from two deep CNNs, and
    thus encodes higher order statistics of convolutional activations to enhance the
    mid-level learning capability. Thanks to its high model capacity, Bilinear CNNs
    achieve remarkable fine-grained recognition performance. However, the extremely
    high dimensionality of bilinear features still makes it impractical for realistic
    applications, especially for the large-scale ones.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Aiming at this problem, more recent attempts, *e.g.*, Gao et al. ([2016](#bib.bib12));
    Kong and Fowlkes ([2017](#bib.bib20)); Cui et al. ([2017](#bib.bib6)), try to
    aggregate low-dimensional embeddings by applying tensor sketching Pham and Pagh
    ([2013](#bib.bib31)); Charikar et al. ([2002](#bib.bib3)), which can approximate
    the bilinear features and maintain comparable or higher recognition accuracy.
    Other works, *e.g.*, Dubey et al. ([2018](#bib.bib8)), focus on designing a specific
    loss function tailored for fine-grained and is able to drive the whole deep model
    for learning discriminative fine-grained representations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 With external information
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As aforementioned, beyond the conventional recognition paradigms, another paradigm
    is to leverage external information, *e.g.*, web data, multi-modality data or
    human-computer interactions, to further assist fine-grained recognition.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 With web data
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To identify the minor distinction among various fine-grained categories, sufficient
    well-labeled training images are in high demand. However, accurate human annotations
    for fine-grained categories are not easy to acquire, due to the difficulty of
    annotations (always requiring domain experts) and the myriads of fine-grained
    categories (*i.e.*, more than thousands of subordinate categories in a meta-category).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a part of fine-grained recognition methods seek to utilize the free
    but noisy web data to boost recognition performance. The majority of existing
    works in this line can be roughly grouped into two directions. One of them is
    to crawl noisy labeled web data for the test categories as training data, which
    is regarded as webly supervised learning Zhuang et al. ([2017](#bib.bib53)); Sun
    et al. ([2019](#bib.bib36)). Main efforts of these approaches concentrate on:
    (1) overcoming the dataset gap between easily acquired web images and the well-labeled
    data from standard datasets; and (2) reducing the negative effects caused by the
    noisy data. For dealing with the aforementioned problems, deep learning techniques
    of adversarial learning Goodfellow et al. ([2014](#bib.bib13)) and attention mechanisms Zhuang
    et al. ([2017](#bib.bib53)) are frequently utilized. The other direction of using
    web data is to transfer the knowledge from an auxiliary categories with well-labeled
    training data to the test categories, which usually employs zero-shot learning Niu
    et al. ([2018](#bib.bib30)) or meta learning Zhang et al. ([2018](#bib.bib48))
    to achieve that goal.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 With multi-modality data
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-modal analysis has attracted a lot of attentions with the rapid growth
    of multi-media data (*e.g.*, image, text, knowledge base, etc). In fine-grained
    recognition, it takes multi-modality data to establish joint-representations/embeddings
    for incorporating multi-modality information. It is able to boost fine-grained
    recognition accuracy. In particular, frequently utilized multi-modality data includes
    text descriptions (*e.g.*, sentences and phrases of natural languages) and graph-structured
    knowledge base. Compared with strong supervisions of fine-grained images, *e.g.*,
    part annotations, text descriptions are weak supervisions. Besides, text descriptions
    can be relatively accurately returned by ordinary humans, rather than the experts
    in a specific domain. In addition, high-level knowledge graph is an existing resource
    and contains rich professional knowledge, such as *DBpedia* Lehmann et al. ([2015](#bib.bib23)).
    In practice, both text descriptions and knowledge base are effective as extra
    guidance for better fine-grained image representation learning.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b9ab08e34ab782ae4272ee656b7af6e1.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: An example knowledge graph for modeling the category-attribute correlations
    on *CUB200-2011*.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, Reed et al. ([2016](#bib.bib32)) collects text descriptions,
    and introduces a structured joint embedding for zero-shot fine-grained image recognition
    by combining texts and images. Later, He and Peng ([2017a](#bib.bib14)) combines
    the vision and language streams in a joint training end-to-end fashion to preserve
    the intra-modality and inter-modality information for generating complementary
    fine-grained representations. For fine-grained recognition with knowledge base,
    some works, *e.g.*, Chen et al. ([2018](#bib.bib4)); Xu et al. ([2018a](#bib.bib44)),
    introduce the knowledge base information (always associating with attribute labels,
    cf. Fig. [6](#S4.F6 "Figure 6 ‣ 4.3.2 With multi-modality data ‣ 4.3 With external
    information ‣ 4 Fine-grained image recognition ‣ Deep learning for fine-grained
    image analysis: A survey")) to implicitly enriching the embedding space (also
    reasoning about the discriminative attributes for fine-grained objects).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 With humans in the loop
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fine-grained recognition with humans in the loop is usually an iterative system
    composed of a machine and a human user, which combines both human and machine
    efforts and intelligence. Also, it requires the system to work in a human labor-economy
    way as possible. Generally, for these kinds of recognition methods, the system
    in each round is seeking to understand how humans perform recognition, *e.g.*,
    by asking untrained humans to label the image class and pick up hard examples Cui
    et al. ([2016](#bib.bib5)), or by identifying key part localization and selecting
    discriminative features Deng et al. ([2016](#bib.bib7)) for fine-grained recognition.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 5 Fine-grained image retrieval
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beyond image recognition, fine-grained retrieval is another crucial aspect of
    FGIA and emerges as a hot topic. Its evaluation metric is the common mean average
    precision (mAP).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'In fine-grained image retrieval, given database images of the same sub-category
    (*e.g.*, birds or cars) and a query, it should return images which are in the
    same variety as the query, without resorting to any other supervision signals,
    cf. Fig. [7](#S5.F7 "Figure 7 ‣ 5 Fine-grained image retrieval ‣ Deep learning
    for fine-grained image analysis: A survey"). Compared with generic image retrieval
    which focuses on retrieving near-duplicate images based on similarities in their
    contents (*e.g.*, textures, colors and shapes), while fine-grained retrieval focuses
    on retrieving the images of the same types (*e.g.*, the same subordinate species
    for the animals and the same model for the cars). Meanwhile, objects in fine-grained
    images have only subtle differences, and vary in poses, scales and rotations.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c607cbd081b0a825f64ca7d76c35c9e.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An illustration of fine-grained *retrieval*. Given a query image
    (*aka* probe) of “Dodge Charger Sedan 2012”, fine-grained retrieval is required
    to return images of the same car model from a car database (*aka* galaxy). In
    this figure, the top-4 returned image marked in a red rectangle presents a wrong
    result, since its model is “Dodge Caliber Wagon 2012”.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In the literature, Wei et al. ([2017](#bib.bib39)) is the first attempt to fine-grained
    image retrieval using deep learning. It employs pre-trained CNN models to select
    the meaningful deep descriptors by localizing the main object in fine-grained
    images *unsupervisedly*, and further reveals that selecting only useful deep descriptors
    with removing background or noise could significantly benefit retrieval tasks.
    Recently, to break through the limitation of unsupervised fine-grained retrieval
    by pre-trained models, some trials Zheng et al. ([2018](#bib.bib51), [2019](#bib.bib52))
    tend to discovery novel loss functions under the *supervised* metric learning
    paradigm. Meanwhile, they still design additional specific sub-modules tailored
    for fine-grained objects, *e.g.*, the weakly-supervised localization module proposed
    in Zheng et al. ([2018](#bib.bib51)), which is under the inspiration of Wei et
    al. ([2017](#bib.bib39)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 6 Fine-grained image generation
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from the supervised learning tasks, image generation is a representative
    topic of unsupervised learning. It deploys deep generative models, *e.g.*, GAN Goodfellow
    et al. ([2014](#bib.bib13)), to learn to synthesize realistic images which looks
    visually authentic. With the quality of generated images becoming higher, more
    challenging goals are expected, *i.e.*, fine-grained image generation. As the
    term suggests, fine-grained generation will synthesize images in fine-grained
    categories such as faces of a specific person or objects in a subordinate category.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: The first work in this line was CVAE-GAN proposed in Bao et al. ([2017](#bib.bib1)),
    which combines a variational auto-encoder with a generative adversarial network
    under a conditional generative process to tackle this problem. Specifically, CVAE-GAN
    models an image as a composition of label and latent attributes in a probabilistic
    model. Then, by varying the fine-grained category fed into the resulting generative
    model, it can generate images in a specific category. More recently, generating
    images from text descriptions Xu et al. ([2018b](#bib.bib45)) behaves popular
    in the light of its diverse and practical applications, *e.g.*, art generation
    and computer-aided design. By performing an attention equipped generative network,
    the model can synthesize fine-grained details of subtle regions by focusing on
    the relevant words of text descriptions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 7 Domain specific applications related to fine-grained image analysis
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the real world, deep learning based fine-grained image analysis techniques
    are also adopted to diverse domain specific applications and shows great performance,
    such as clothes/shoes retrieval Song et al. ([2017](#bib.bib33)) in recommendation
    systems, fashion image recognition Liu et al. ([2016](#bib.bib27)) in e-commerce
    platforms, product recognition Wei et al. ([2019a](#bib.bib42)) in intelligent
    retail, etc. These applications are highly related to both fine-grained retrieval
    and recognition of FGIA.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，基于深度学习的细粒度图像分析技术也被应用于多样的领域特定应用，并表现出优异的性能，例如在推荐系统中的衣物/鞋子检索Song等人（[2017](#bib.bib33)），在电子商务平台中的时尚图像识别Liu等人（[2016](#bib.bib27)），在智能零售中的产品识别Wei等人（[2019a](#bib.bib42)）等。这些应用与FGIA的细粒度检索和识别密切相关。
- en: Additionally, if we move down the spectrum of granularity, in the extreme, face
    identification can be viewed as an instance of fine-grained recognition, where
    the granularity is under the identity granularity level. Moreover, person/vehicle
    re-identification is another fine-grained related task, which aims at determining
    whether two images are taken from the same specific person/vehicle. Apparently,
    re-identification tasks are also under identity granularity.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们将粒度谱向下移动，在极端情况下，面部识别可以被视为细粒度识别的一个实例，其粒度低于身份粒度级别。此外，人物/车辆重新识别是另一个与细粒度相关的任务，旨在确定两张图像是否拍摄自同一特定人物/车辆。显然，重新识别任务也属于身份粒度。
- en: In practice, these works solve the corresponding domain specific tasks by following
    the motivations of FGIA, which includes capturing the discriminative parts of
    objects (faces, persons and vehicles) Suh et al. ([2018](#bib.bib34)), discovering
    coarse-to-fine structural information Wei et al. ([2018b](#bib.bib41)), developing
    attribute-based models Liu et al. ([2016](#bib.bib27)), and so on.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些工作通过遵循FGIA的动机来解决相应领域的特定任务，包括捕捉对象的区分部分（面孔、人物和车辆）Suh等人（[2018](#bib.bib34)），发现从粗到细的结构信息Wei等人（[2018b](#bib.bib41)），开发基于属性的模型Liu等人（[2016](#bib.bib27)）等。
- en: 8 Concluding remarks and future directions
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论和未来方向
- en: Fine-grained image analysis (FGIA) based on deep learning have made great progress
    in recent years. In this paper, we give an extensive survey on recent advances
    in FGIA with deep learning. We mainly introduced the FGIA problem and its challenges,
    discussed the significant improvements of fine-grained image recognition/retrieval/generation,
    and also presented some domain specific applications related to FGIA. Despite
    the great success, there are still many unsolved problems. Thus, in this section,
    we will point out these problems explicitly and introduce some research trends
    for the future evolution. We hope that this survey not only provides a better
    understanding of FGIA but also facilitates future research activities and application
    developments in this field.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的细粒度图像分析（FGIA）近年来取得了重大进展。在本文中，我们对FGIA在深度学习中的最新进展进行了广泛的调查。我们主要介绍了FGIA问题及其挑战，讨论了细粒度图像识别/检索/生成的重大改进，并介绍了一些与FGIA相关的领域特定应用。尽管取得了巨大成功，但仍然存在许多未解决的问题。因此，在这一部分，我们将明确指出这些问题，并介绍一些未来演变的研究趋势。我们希望这项调查不仅能提供对FGIA的更好理解，还能促进该领域未来的研究活动和应用开发。
- en: Automatic fine-grained models
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 自动细粒度模型
- en: Nowadays, automated machine learning (AutoML) Feurer et al. ([2015](#bib.bib10))
    and neural architecture search (NAS) Elsken et al. ([2018](#bib.bib9)) are attracting
    fervent attentions in the artificial intelligence community, especially in computer
    vision. AutoML targets automating the end-to-end process of applying machine learning
    to real-world tasks. While, NAS, the process of automating neural network architecture
    designing, is thus a logical next step in AutoML. Recent methods of AutoML and
    NAS could be comparable or even outperform hand-designed architectures in various
    computer vision applications. Thus, it is also promising that automatic fine-grained
    models developed by AutoML or NAS techniques could find a better and more tailor-made
    deep models, and meanwhile it can advance the studies of AutoML and NAS in turn.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，自动化机器学习（AutoML）（Feurer et al. ([2015](#bib.bib10)））和神经网络架构搜索（NAS）（Elsken
    et al. ([2018](#bib.bib9)））在人工智能社区，特别是在计算机视觉领域，正受到热烈关注。AutoML旨在自动化将机器学习应用于现实任务的端到端过程。而NAS，自动化神经网络架构设计的过程，因此是AutoML的逻辑下一步。最近的AutoML和NAS方法在各种计算机视觉应用中可与手工设计的架构相媲美，甚至超越。因此，利用AutoML或NAS技术开发的自动化细粒度模型也有可能找到更好、更量体裁衣的深度模型，同时也能促进AutoML和NAS研究的发展。
- en: Fine-grained few-shot learning
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 细粒度少样本学习
- en: Humans are capable of learning a new fine-grained concept with very little supervision,
    *e.g.*, few exemplary images for a species of bird, yet our best deep learning
    fine-grained systems need hundreds or thousands of labeled examples. Even worse,
    the supervision of fine-grained images are both time-consuming and expensive,
    since fine-grained objects should be always accurately labeled by domain experts.
    Thus, it is desirable to develop fine-grained few-shot learning (FGFS) Wei et
    al. ([2019b](#bib.bib43)). The task of FGFS requires the learning systems to build
    classifiers for novel fine-grained categories from few examples (only one or less
    than five) in an meta-learning fashion. Robust FGFS methods could extremely strengthen
    the usability and scalability of fine-grained recognition.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 人类在非常少量的监督下能够学习新的细粒度概念，例如，对于某种鸟类只需少量的示例图像，而我们目前最先进的深度学习细粒度系统则需要数百或数千个标注样本。更糟的是，细粒度图像的监督既耗时又昂贵，因为细粒度物体需要由领域专家进行准确标注。因此，开发细粒度少样本学习（FGFS）是非常必要的（Wei
    et al. ([2019b](#bib.bib43)））。FGFS任务要求学习系统以元学习的方式从少量样本（仅一个或少于五个）中为新颖的细粒度类别建立分类器。鲁棒的FGFS方法可以极大地增强细粒度识别的可用性和扩展性。
- en: Fine-grained hashing
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 细粒度哈希
- en: As there exist growing attentions on FGIA, more large-scale and well-constructed
    fine-grained datasets have been released, *e.g.*, Berg et al. ([2014](#bib.bib2));
    Horn et al. ([2017](#bib.bib16)); Wei et al. ([2019a](#bib.bib42)). In real applications
    like fine-grained image retrieval, it is natural to raise a problem that the cost
    of finding the exact nearest neighbor is prohibitively high in the case that the
    reference database is very large. Hashing Wang et al. ([2018](#bib.bib38)); Li
    et al. ([2016](#bib.bib24)), acting as one of the most popular and effective techniques
    of approximate nearest neighbor search, has the potential to deal with large-scale
    fine-grained data. Therefore, fine-grained hashing is a promising direction worth
    further explorations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对FGIA的关注日益增加，越来越多的大规模、结构良好的细粒度数据集被发布，例如，Berg et al. ([2014](#bib.bib2))；Horn
    et al. ([2017](#bib.bib16))；Wei et al. ([2019a](#bib.bib42))。在实际应用中，如细粒度图像检索，问题的自然产生是当参考数据库非常大时，找到准确的最近邻的成本极高。哈希（Wang
    et al. ([2018](#bib.bib38))；Li et al. ([2016](#bib.bib24))）作为一种最流行和有效的近似最近邻搜索技术之一，有潜力处理大规模细粒度数据。因此，细粒度哈希是一个值得进一步探索的有前途的方向。
- en: Fine-grained analysis within more realistic settings
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在更现实的环境中的细粒度分析
- en: In the past decade, fine-grained image analysis related techniques have been
    developed and achieve good performance in its traditional settings, *e.g.*, the
    empirical protocols of Wah et al. ([2011](#bib.bib37)); Khosla et al. ([2011](#bib.bib19));
    Krause et al. ([2013](#bib.bib21)). However, these settings can not satisfy the
    daily requirements of various real-world applications nowadays, *e.g.*, recognizing
    retail products in storage racks by models trained with images collected in controlled
    environments Wei et al. ([2019a](#bib.bib42)) and recognizing/detecting natural
    species in the wild Horn et al. ([2017](#bib.bib16)). In consequence, novel fine-grained
    image analysis topics, to name a few—fine-grained analysis with domain adaptation,
    fine-grained analysis with knowledge transfer, fine-grained analysis with long-tailed
    distribution, and fine-grained analysis running on resource constrained embedded
    devices—deserve a lot of research efforts towards the more advanced and practical
    FGIA.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bao et al. [2017] J. Bao, D. Chen, F. Wen, H. Li, and G. Hua. CVAE-GAN: Fine-grained
    image generation through asymmetric training. In ICCV, pages 2745–2754, 2017.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Berg et al. [2014] T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs,
    and P. N. Belhumeur. Birdsnap: Large-scale fine-grained visual categorization
    of birds. In CVPR, pages 2019–2026, 2014.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charikar et al. [2002] M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent
    items in data streams. In ICALP, pages 693–703, 2002.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2018] T. Chen, L. Lin, R. Chen, Y. Wu, and X. Luo. Knowledge-embedded
    representation learning for fine-grained image recognition. In IJCAI, pages 627–634,
    2018.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. [2016] Y. Cui, F. Zhou, Y. Lin, and S. Belongie. Fine-grained categorization
    and dataset bootstrapping using deep metric learning with humans in the loop.
    In CVPR, pages 1153–1162, 2016.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cui et al. [2017] Y. Cui, F. Zhou, J. Wang, X. Liu, Y. Lin, and S. Belongie.
    Kernel pooling for convolutional neural network. In CVPR, pages 2921–2930, 2017.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deng et al. [2016] J. Deng, J. Krause, M. Stark, and L. Fei-Fei. Leveraging
    the wisdom of the crowd for fine-grained recognition. TPAMI, 38(4):666–676, 2016.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dubey et al. [2018] A. Dubey, O. Gupta, R. Raskar, and N. Naik. Maximum entropy
    fine-grained classification. In NeurIPS, pages 637–647, 2018.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. [2018] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture
    search: A survey. arXiv preprint arXiv:1808.05377, 2018.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feurer et al. [2015] M. Feurer, A. Klein, K. Eggensperger, J. Springenberg,
    M. Blum, and F. Hutter. Efficient and robust automated machine learning. In NIPS,
    pages 2962–2970, 2015.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. [2017] J. Fu, H. Zheng, and T. Mei. Look closer to see better: Recurrent
    attention convolutional neural network for fine-grained image recognition. In
    CVPR, pages 4438–4446, 2017.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. [2016] Y. Gao, O. Beijbom, N. Zhang, and T. Darrell. Compact bilinear
    pooling. In CVPR, pages 317–326, 2016.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. [2014] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, pages
    2672–2680, 2014.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He and Peng [2017a] X. He and Y. Peng. Fine-grained image classification via
    combining vision and language. In CVPR, pages 5994–6002, 2017.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He and Peng [2017b] X. He and Y. Peng. Weakly supervised learning of part selection
    model with spatial constraints for fine-grained image classification. In AAAI,
    pages 4075–4081, 2017.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horn et al. [2017] G. Van Horn, O. M. Aodha, Y. Song, Y. Cui, C. Sun, A. Shepard,
    H. Adam, P. Perona, and S. Belongie. The iNaturalist species classification and
    detection dataset. In CVPR, pages 8769–8778, 2017.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2017] S. Hou, Y. Feng, and Z. Wang. VegFru: A domain-specific dataset
    for fine-grained visual categorization. In ICCV, pages 541–549, 2017.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaderberg et al. [2015] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu.
    Spatial transformer networks. In NIPS, pages 2017–2025, 2015.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosla et al. [2011] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei.
    Novel dataset for fine-grained image categorization. In CVPR Workshop on Fine-Grained
    Visual Categorization, pages 806–813, 2011.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kong and Fowlkes [2017] S. Kong and C. Fowlkes. Low-rank bilinear pooling for
    fine-grained classification. In CVPR, pages 365–374, 2017.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krause et al. [2013] J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3D object
    representations for fine-grained categorization. In ICCV Workshop on 3D Representation
    and Recognition, 2013.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [2015] Y. LeCun, Y. Bengion, and G. Hinton. Deep learning. Nature,
    521:436–444, 2015.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lehmann et al. [2015] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas,
    P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer. DBpedia
    - A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic
    Web Journal, pages 167–195, 2015.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2016] W.-J. Li, S. Wang, and W.-C. Kang. Feature learning based deep
    supervised hashing with pairwise labels. In IJCAI, pages 1711–1717, 2016.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. [2015a] D. Lin, X. Shen, C. Lu, and J. Jia. Deep LAC: Deep localization,
    alignment and classification for fine-grained recognition. In CVPR, pages 1666–1674,
    2015.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2015b] T.-Y. Lin, A. RoyChowdhury, and S. Maji. Bilinear CNN models
    for fine-grained visual recognition. In ICCV, pages 1449–1457, 2015.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2016] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang. DeepFashion:
    Powering robust clothes recognition and retrieval with rich annotations. In CVPR,
    pages 1096–1104, 2016.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maji et al. [2013] S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi.
    Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151,
    2013.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nilsback and Zisserman [2008] M.-E. Nilsback and A. Zisserman. Automated flower
    classification over a large number of classes. In Indian Conf. on Comput. Vision,
    Graph. and Image Process., pages 722–729, 2008.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu et al. [2018] L. Niu, A. Veeraraghavan, and A. Sabharwal. Webly supervised
    learning meets zero-shot learning: A hybrid approach for fine-grained classification.
    In CVPR, pages 7171–7180, 2018.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pham and Pagh [2013] N. Pham and R. Pagh. Fast and scalable polynomial kernels
    via explicit feature maps. In KDD, pages 239–247, 2013.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. [2016] S. Reed, Z. Akata, H. Lee, and B. Schiele. Learning deep
    representations of fine-grained visual descriptions. In CVPR, pages 49–58, 2016.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2017] J. Song, Q. Yu, Y.-Z. Song, T. Xiang, and T. M. Hospedales.
    Deep spatial-semantic attention for fine-grained sketch-based image retrieval.
    In ICCV, pages 5551–5560, 2017.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suh et al. [2018] Y. Suh, J. Wang, S. Tang, T. Mei, and K. M. Lee. Part-aligned
    bilinear representations for person re-identification. In ECCV, pages 402–419,
    2018.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2018] M. Sun, Y. Yuan, F. Zhou, and E. Ding. Multi-attention multi-class
    constraint for fine-grained image recognition. In ECCV, pages 834–850, 2018.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2019] X. Sun, L. Chen, and J. Yang. Learning from web data using
    adversarial discriminative neural networks for fine-grained classification. In
    AAAI, 2019.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wah et al. [2011] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie.
    The Caltech-UCSD birds-200-2011 dataset. Tech. Report CNS-TR-2011-001, 2011.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2018] J. Wang, T. Zhang, J. Song, N. Sebe, and H. T. Shen. A survey
    on learning to hash. TPAMI, 40(4):769–790, 2018.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2017] X.-S. Wei, J.-H. Luo, J. Wu, and Z.-H. Zhou. Selective convolutional
    descriptor aggregation for fine-grained image retrieval. TIP, 26(6):2868–2881,
    2017.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2018a] X.-S. Wei, C.-W. Xie, J. Wu, and C. Shen. Mask-CNN: Localizing
    parts and selecting descriptors for fine-grained bird species categorization.
    Pattern Recognition, 76:704–714, 2018.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2018b] X.-S. Wei, C.-L. Zhang, L. Liu, C. Shen, and J. Wu. Coarse-to-fine:
    A RNN-based hierarchical attention model for vehicle re-identification. In ACCV,
    2018.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2019a] X.-S. Wei, Q. Cui, L. Yang, P. Wang, and L. Liu. RPC: A
    large-scale retail product checkout dataset. arXiv preprint arXiv:1901.07249,
    2019.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. [2019b] X.-S. Wei, P. Wang, L. Liu, C. Shen, and J. Wu. Piecewise
    classifier mappings: Learning fine-grained learners for novel categories with
    few examples. TIP, in press, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2018a] H. Xu, G. Qi, J. Li, M. Wang, K. Xu, and H. Gao. Fine-grained
    image classification by visual-semantic embedding. In IJCAI, pages 1043–1049,
    2018.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2018b] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and
    X. He. AttnGAN: Fine-grained text to image generation with attentional generative
    adversarial networks. In CVPR, pages 1316–1324, 2018.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2018] Z. Yang, T. Luo, D. Wang, Z. Hu, J. Gao, and L. Wang. Learning
    to navigate for fine-grained classification. In ECCV, pages 438–454, 2018.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2014] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part-based
    R-CNNs for fine-grained category detection. In ECCV, pages 834–849, 2014.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2018] Y. Zhang, H. Tang, and K. Jia. Fine-grained visual categorization
    using meta-learning optimization with sample selection of auxiliary data. In ECCV,
    pages 233–248, 2018.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. [2017] B. Zhao, J. Feng, X. Wu, and S. Yan. A survey on deep learning-based
    fine-grained object classification and semantic segmentation. International Journal
    of Automation and Computing, 14(2):119–135, 2017.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2017] H. Zheng, J. Fu, T. Mei, and J. Luo. Learning multi-attention
    convolutional neural network for fine-grained image recognition. In ICCV, pages
    5209–5217, 2017.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2018] X. Zheng, R. Ji, X. Sun, Y. Wu, F. Huang, and Y. Yang. Centralized
    ranking loss with weakly supervised localization for fine-grained object retrieval.
    In IJCAI, pages 1226–1233, 2018.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. [2019] X. Zheng, R. Ji, X. Sun, B. Zhang, Y. Wu, and F. Huang.
    Towards optimal fine grained retrieval via decorrelated centralized loss with
    normalize-scale layer. In AAAI, 2019.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhuang et al. [2017] B. Zhuang, L. Liu, Y. Li, C. Shen, and I. Reid. Attend
    in groups: a weakly-supervised deep learning framework for learning from web data.
    In CVPR, pages 1878–1887, 2017.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
