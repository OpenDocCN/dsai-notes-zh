- en: 'Machine Learning 1: Lesson 6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: åŸæ–‡ï¼š[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to â€œreallyâ€ understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Video](https://youtu.be/BFIYUvBRTpE) / [Powerpoint](https://github.com/fastai/fastai/blob/master/courses/ml1/ppt/ml_applications.pptx)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Weâ€™ve looked at a lot of different random forest interpretation techniques and
    a question that has come up a little bit on the forum is what are these for really?
    How do these help me get a better score on Kaggle, and my answer has been â€œthey
    donâ€™t necessarilyâ€. So I wanted to talk more about why we do machine learning.
    Whatâ€™s the point? To answer this question, I want to show you something really
    important which is examples of how people have used machine learning mainly in
    business because thatâ€™s where most of you are probably going to end up after this
    is working for some company. Iâ€™m going to show you applications of machine learning
    which are either based on things that Iâ€™ve been personally involved in myself
    or know of people who are doing them directly so none of these are going to be
    hypotheticals â€” these are all actual things that people are doing and Iâ€™ve got
    direct or secondhand knowledge of.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Two Groups of Applications [[1:26](https://youtu.be/BFIYUvBRTpE?t=1m26s)]
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Horizontal: In business, horizontal means something that you do across different
    kinds of business. i.e. everything involving marketing.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vertical: Something you do within a business or within a supply chain or a
    process.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal Applications
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pretty much every company has to try to sell more products to its customers
    so therefore does marketing. So each of these boxes are examples of some of the
    things that people are using machine learning for in marketing:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd65548e467e8e98626f519b1415255e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Letâ€™s take an example â€” Churn. Churn refers to a model which attempts to predict
    whoâ€™s going to leave. Iâ€™ve done some churn modeling fairly recently in telecommunications.
    We were trying to figure out for this big cellphone company which customers are
    going to leave. That is not of itself that interesting. Building a highly predictive
    model that says Jeremy Howard is almost certainly going to leave next month is
    probably not that helpful because if Iâ€™m almost certainly going to leave net month,
    thereâ€™s probably nothing you can do about it â€” itâ€™s too late and itâ€™s going to
    cost you too much to keep me.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'So in order to understand why we would do churn modeling, Iâ€™ve got a little
    framework that you might find helpful: [Designing great data products](https://www.oreilly.com/ideas/drivetrain-approach-data-products).
    I wrote it with a couple of colleagues a few years ago and in it, I describe my
    experience of actually turning machine learning models into stuff that makes money.
    The basic trick is what I call the **Drivetrain Approach** which is these four
    steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6dbaec77fe8d5f4fb87eee227326a3e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Defined Objective [[3:48](https://youtu.be/BFIYUvBRTpE?t=3m48s)]
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The starting point to actually turn a machine learning project into something
    thatâ€™s actually useful is to know what I am trying to achieve and that does mean
    Iâ€™m trying to achieve a high area under the ROC curve or trying to achieve a large
    difference between classes. It would be Iâ€™m trying to sell more books or Iâ€™m trying
    to reduce the number of customers that leave next month or Iâ€™m trying to detect
    lung cancer earlier. These are objectives. So the objective is something that
    absolutely directly is the thing that the company or the organization actually
    wants. No company or organization lives in order to create a more accurate predictive
    model. There are some reason. So thatâ€™s your objective. Thatâ€™s obviously the most
    important thing. If you donâ€™t know the purpose of what you are modeling for then
    you canâ€™t possibly do a good job of it. And hopefully people are starting to pick
    that up out there in the world of data science, but interestingly what very few
    people are talking about but itâ€™s just as important is the next thing which is
    levers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æœºå™¨å­¦ä¹ é¡¹ç›®è½¬åŒ–ä¸ºå®é™…æœ‰ç”¨çš„èµ·ç‚¹æ˜¯çŸ¥é“æˆ‘è¯•å›¾å®ç°ä»€ä¹ˆï¼Œè¿™æ„å‘³ç€æˆ‘è¯•å›¾å®ç°é«˜ROCæ›²çº¿ä¸‹é¢ç§¯æˆ–å°è¯•å®ç°ç±»ä¹‹é—´çš„å·¨å¤§å·®å¼‚ã€‚è¿™å¯èƒ½æ˜¯æˆ‘è¯•å›¾é”€å”®æ›´å¤šçš„ä¹¦ï¼Œæˆ–è€…æˆ‘è¯•å›¾å‡å°‘ä¸‹ä¸ªæœˆç¦»å¼€çš„å®¢æˆ·æ•°é‡ï¼Œæˆ–è€…æˆ‘è¯•å›¾æ›´æ—©åœ°æ£€æµ‹è‚ºç™Œã€‚è¿™äº›éƒ½æ˜¯ç›®æ ‡ã€‚å› æ­¤ï¼Œç›®æ ‡æ˜¯å…¬å¸æˆ–ç»„ç»‡å®é™…æƒ³è¦çš„ä¸œè¥¿ã€‚æ²¡æœ‰å…¬å¸æˆ–ç»„ç»‡æ˜¯ä¸ºäº†åˆ›å»ºæ›´å‡†ç¡®çš„é¢„æµ‹æ¨¡å‹è€Œå­˜åœ¨çš„ã€‚è¿™æ˜¯æœ‰åŸå› çš„ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä½ çš„ç›®æ ‡ã€‚æ˜¾ç„¶ï¼Œè¿™æ˜¯æœ€é‡è¦çš„äº‹æƒ…ã€‚å¦‚æœä½ ä¸çŸ¥é“ä½ ä¸ºä½•å»ºæ¨¡ï¼Œé‚£ä¹ˆä½ ä¸å¯èƒ½åšå¥½è¿™é¡¹å·¥ä½œã€‚å¸Œæœ›äººä»¬å¼€å§‹åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸæ„è¯†åˆ°è¿™ä¸€ç‚¹ï¼Œä½†æœ‰è¶£çš„æ˜¯ï¼Œå¾ˆå°‘æœ‰äººè°ˆè®ºä½†åŒæ ·é‡è¦çš„æ˜¯ä¸‹ä¸€æ­¥ï¼Œå³æ æ†ã€‚
- en: Levers [[5:04](https://youtu.be/BFIYUvBRTpE?t=5m4s)]
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ æ†[[5:04](https://youtu.be/BFIYUvBRTpE?t=5m4s)]
- en: A lever is a thing that the organization can do to actually drive the objective.
    So letâ€™s take the example of churn modeling. What is a lever that an organization
    could use to reduce the number of customers that are leaving? They could call
    someone and say â€œAre you happy? Anything we could do?â€ They could give them a
    free pen or something if they buy $20 worth of product next month. You could give
    them specials. So these are levers. Whenever you are working as a data scientists,
    keep coming back and thinking what are we trying to achieve (we being the organization)
    and how we are trying to achieve it being what are the actual things we can do
    to make that objective happen. So building a model is never ever a lever, but
    it could help you with the lever.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ æ†æ˜¯ç»„ç»‡å¯ä»¥å®é™…é‡‡å–çš„è¡ŒåŠ¨ï¼Œä»¥æ¨åŠ¨ç›®æ ‡çš„å®ç°ã€‚æ‰€ä»¥è®©æˆ‘ä»¬ä»¥æµå¤±å»ºæ¨¡ä¸ºä¾‹ã€‚ç»„ç»‡å¯ä»¥é‡‡å–ä»€ä¹ˆæ æ†æ¥å‡å°‘ç¦»å¼€çš„å®¢æˆ·æ•°é‡ï¼Ÿä»–ä»¬å¯ä»¥æ‰“ç”µè¯ç»™æŸäººï¼Œé—®ï¼šâ€œä½ æ»¡æ„å—ï¼Ÿæˆ‘ä»¬èƒ½åšäº›ä»€ä¹ˆï¼Ÿâ€ä»–ä»¬å¯ä»¥åœ¨ä¸‹ä¸ªæœˆè´­ä¹°ä»·å€¼20ç¾å…ƒçš„äº§å“æ—¶èµ é€å…è´¹çš„é’¢ç¬”æˆ–å…¶ä»–ç‰©å“ã€‚ä½ å¯ä»¥ç»™ä»–ä»¬æä¾›ç‰¹åˆ«ä¼˜æƒ ã€‚æ‰€ä»¥è¿™äº›å°±æ˜¯æ æ†ã€‚å½“ä½ ä½œä¸ºæ•°æ®ç§‘å­¦å®¶å·¥ä½œæ—¶ï¼Œä¸æ–­å›å¤´æ€è€ƒæˆ‘ä»¬è¯•å›¾å®ç°ä»€ä¹ˆï¼ˆæˆ‘ä»¬æŒ‡çš„æ˜¯ç»„ç»‡ï¼‰ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•å®ç°å®ƒï¼Œå³æˆ‘ä»¬å¯ä»¥åšå“ªäº›å®é™…çš„äº‹æƒ…æ¥å®ç°è¿™ä¸ªç›®æ ‡ã€‚å› æ­¤ï¼Œæ„å»ºæ¨¡å‹ç»å¯¹ä¸æ˜¯æ æ†ï¼Œä½†å®ƒå¯ä»¥å¸®åŠ©ä½ ä½¿ç”¨æ æ†ã€‚
- en: Data [[7:01](https://youtu.be/BFIYUvBRTpE?t=7m1s)]
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ•°æ®[[7:01](https://youtu.be/BFIYUvBRTpE?t=7m1s)]
- en: So then the next step is what data does the organization have that could possibly
    help them to set that lever to achieve that objective. So this is not what data
    did they give you when you started the project. But think about it from a first
    principleâ€™s point of view â€” okay, Iâ€™m working for a telecommunications company,
    they gave me some certain set of data, but Iâ€™m sure they must know where their
    customers live, how many phone calls they made last month, how many times they
    called customer service, etc. So have a think about okay if we are trying to decide
    who should we give a special offer to proactively, then we want to figure out
    what information do we have that might help us to identify whoâ€™s going to react
    well or badly to that. Perhaps more interestingly would be what if we were doing
    a fraud algorithm. So we are trying to figure out whoâ€™s going to not pay for the
    phone that they take out of the store, they are on some 12-month payment plan,
    and we never see them again. Now in that case, the data we have available , it
    doesnâ€™t matter whatâ€™s in the database, what matters is whatâ€™s the data that we
    can get when the customer is in the shop. So thereâ€™s often constraints around
    the data that we can actually use. So we need to know what am I trying to achieve,
    what can this organization actually do specifically to change the outcome, and
    at the point that the decision is being made, what data do they have or could
    they collect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„æ­¥éª¤æ˜¯ç»„ç»‡æ‹¥æœ‰å“ªäº›æ•°æ®å¯èƒ½å¸®åŠ©ä»–ä»¬è®¾ç½®æ æ†ä»¥å®ç°ç›®æ ‡ã€‚è¿™ä¸æ˜¯æŒ‡ä»–ä»¬åœ¨é¡¹ç›®å¼€å§‹æ—¶ç»™ä½ çš„æ•°æ®ã€‚è€Œæ˜¯ä»ç¬¬ä¸€åŸåˆ™çš„è§’åº¦è€ƒè™‘â€”â€”å¥½å§ï¼Œæˆ‘åœ¨ä¸€å®¶ç”µä¿¡å…¬å¸å·¥ä½œï¼Œä»–ä»¬ç»™äº†æˆ‘ä¸€äº›ç‰¹å®šçš„æ•°æ®ï¼Œä½†æˆ‘è‚¯å®šä»–ä»¬å¿…é¡»çŸ¥é“ä»–ä»¬çš„å®¢æˆ·ä½åœ¨å“ªé‡Œï¼Œä¸Šä¸ªæœˆæ‰“äº†å¤šå°‘ç”µè¯ï¼Œæ‰“äº†å¤šå°‘æ¬¡å®¢æœç”µè¯ç­‰ç­‰ã€‚æ‰€ä»¥æƒ³ä¸€æƒ³ï¼Œå¦‚æœæˆ‘ä»¬è¯•å›¾å†³å®šä¸»åŠ¨ç»™è°æä¾›ç‰¹åˆ«ä¼˜æƒ ï¼Œé‚£ä¹ˆæˆ‘ä»¬æƒ³è¦å¼„æ¸…æ¥šæˆ‘ä»¬æœ‰å“ªäº›ä¿¡æ¯å¯èƒ½å¸®åŠ©æˆ‘ä»¬ç¡®å®šè°ä¼šå¯¹æ­¤åšå‡ºç§¯ææˆ–æ¶ˆæçš„ååº”ã€‚ä¹Ÿè®¸æ›´æœ‰è¶£çš„æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨è¿›è¡Œæ¬ºè¯ˆç®—æ³•ã€‚æˆ‘ä»¬è¯•å›¾å¼„æ¸…æ¥šè°ä¸ä¼šæ”¯ä»˜ä»–ä»¬ä»å•†åº—æ‹¿å‡ºçš„æ‰‹æœºï¼Œä»–ä»¬æ­£åœ¨è¿›è¡ŒæŸç§12ä¸ªæœˆçš„ä»˜æ¬¾è®¡åˆ’ï¼Œç„¶åæˆ‘ä»¬å†ä¹Ÿæ²¡æœ‰è§åˆ°ä»–ä»¬ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥è·å¾—çš„æ•°æ®ï¼Œæ•°æ®åº“ä¸­æœ‰ä»€ä¹ˆå¹¶ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯å½“å®¢æˆ·åœ¨å•†åº—æ—¶æˆ‘ä»¬å¯ä»¥è·å¾—ä»€ä¹ˆæ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå—åˆ°æˆ‘ä»¬å®é™…å¯ä»¥ä½¿ç”¨çš„æ•°æ®çš„é™åˆ¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“æˆ‘è¯•å›¾å®ç°ä»€ä¹ˆç›®æ ‡ï¼Œè¿™ä¸ªç»„ç»‡å®é™…ä¸Šå¯ä»¥å…·ä½“åšäº›ä»€ä¹ˆæ¥æ”¹å˜ç»“æœï¼Œä»¥åŠåœ¨åšå‡ºå†³å®šæ—¶ï¼Œä»–ä»¬æ‹¥æœ‰æˆ–å¯ä»¥æ”¶é›†åˆ°å“ªäº›æ•°æ®ã€‚
- en: Models [[8:45](https://youtu.be/BFIYUvBRTpE?t=8m45s)]
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ¨¡å‹[[8:45](https://youtu.be/BFIYUvBRTpE?t=8m45s)]
- en: So then the way I put that all together is with a model. This is not a model
    in the sense of a predictive model but itâ€™s a model in the sense of a simulation
    model. So one of the main example I gave in this paper is when I spent many years
    building which is if an insurance company changes their prices, how does that
    impact their profitability. So generally your simulation model contains a number
    of predictive models. So I had, for example, a predictive model called an elasticity
    model that said for a specific customer, if we charge them a specific price for
    a specific product, whatâ€™s the probability that they would say yes both when itâ€™s
    new business and then a year later whatâ€™s the probability that theyâ€™ll renew.
    Then thereâ€™s another predictive model which is whatâ€™s the probability that they
    are going to make a claim and how much is that claim going to be. You can then
    combine these models together then to say all right, if we changed our pricing
    by reducing it by 10% for everybody between 18 and 25 and we can run it through
    these models that combined together into a simulation then the overall impact
    on our market share in 10 years time is X and our cost is Y and our profit is
    Z and so forth.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘æŠŠæ‰€æœ‰è¿™äº›æ”¾åœ¨ä¸€èµ·çš„æ–¹å¼æ˜¯é€šè¿‡ä¸€ä¸ªæ¨¡å‹ã€‚è¿™ä¸æ˜¯ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè€Œæ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ã€‚æˆ‘åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ç»™å‡ºçš„ä¸€ä¸ªä¸»è¦ä¾‹å­æ˜¯ï¼Œæˆ‘èŠ±äº†å¾ˆå¤šå¹´æ—¶é—´å»ºç«‹çš„ä¸€ä¸ªæ¨¡å‹ï¼Œå³å¦‚æœä¸€ä¸ªä¿é™©å…¬å¸æ”¹å˜ä»–ä»¬çš„ä»·æ ¼ï¼Œè¿™å°†å¦‚ä½•å½±å“ä»–ä»¬çš„ç›ˆåˆ©èƒ½åŠ›ã€‚é€šå¸¸ä½ çš„æ¨¡æ‹Ÿæ¨¡å‹åŒ…å«äº†è®¸å¤šé¢„æµ‹æ¨¡å‹ã€‚æ¯”å¦‚ï¼Œæˆ‘æœ‰ä¸€ä¸ªå«åšå¼¹æ€§æ¨¡å‹çš„é¢„æµ‹æ¨¡å‹ï¼Œå®ƒè¯´å¯¹äºä¸€ä¸ªç‰¹å®šçš„å®¢æˆ·ï¼Œå¦‚æœæˆ‘ä»¬ä¸ºä»–ä»¬çš„æŸä¸ªäº§å“æ”¶å–ä¸€ä¸ªç‰¹å®šçš„ä»·æ ¼ï¼Œä»–ä»¬ä¼šåœ¨æ–°ä¸šåŠ¡æ—¶å’Œä¸€å¹´åç»­ä¿çš„æ¦‚ç‡æ˜¯å¤šå°‘ã€‚ç„¶åè¿˜æœ‰å¦ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œå³ä»–ä»¬ä¼šæå‡ºç´¢èµ”çš„æ¦‚ç‡ä»¥åŠç´¢èµ”é‡‘é¢æ˜¯å¤šå°‘ã€‚ç„¶åä½ å¯ä»¥å°†è¿™äº›æ¨¡å‹ç»“åˆèµ·æ¥ï¼Œç„¶åè¯´å¥½ï¼Œå¦‚æœæˆ‘ä»¬å°†æˆ‘ä»¬çš„å®šä»·é™ä½10%é€‚ç”¨äº18åˆ°25å²çš„æ‰€æœ‰äººï¼Œç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™äº›æ¨¡å‹è¿è¡Œï¼Œå°†å®ƒä»¬ç»“åˆæˆä¸€ä¸ªæ¨¡æ‹Ÿï¼Œé‚£ä¹ˆæˆ‘ä»¬åœ¨10å¹´åçš„å¸‚åœºä»½é¢çš„æ•´ä½“å½±å“æ˜¯Xï¼Œæˆ‘ä»¬çš„æˆæœ¬æ˜¯Yï¼Œæˆ‘ä»¬çš„åˆ©æ¶¦æ˜¯Zç­‰ç­‰ã€‚
- en: In practice, most of the time, you really are going to care more about the results
    of that simulation than you do about the predictive model directly. But most people
    are not doing this effectively at the moment. For example, when I go to Amazon,
    I read all of Douglas Adamsâ€™ books, and so having read all Douglas Adamsâ€™ books,
    the next time I went to Amazon they said would you like to buy the collected works
    of Douglas Adams. This is after I had bought every one of his books. So from a
    machine learning point of view, some data scientist had said oh people that buy
    one of Douglas Adamsâ€™ books often go on to buy the collected works. But recommending
    to me that I buy the collected works of Douglas Adams isnâ€™t smart. Itâ€™s actually
    not smart at a number of levels. Not only is unlikely to buy a box set of something
    of which I have every one individually but furthermore itâ€™s not going to change
    my buying behavior. I already know about Douglas Adams. I already know I like
    him, so taking up your valuable web space to tell me hey maybe you should buy
    more of the author who youâ€™re already familiar with and bought lots of times isnâ€™t
    actually going to change my behavior. So what if instead of creating a predictive
    model, Amazon had built an optimization model that could simulate and said if
    we show Jeremy this ad, how likely is he then to go on to buy this book and if
    I donâ€™t show him this ad, how likely is he to go on to buy this book. So thatâ€™s
    the counterfactual. The counter factual is what would have happened otherwise,
    and then you can take the difference and say what should we recommend him that
    is going to maximally change his behavior. So maximally result in more books and
    so youâ€™d probably say oh heâ€™s never bought any Terry Pratchett book, he probably
    doesnâ€™t know about Terry Pratchett but lots of people that liked Douglas Adams
    did turn out to like Terry Pratchett so letâ€™s introduce him to a new author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå¤§å¤šæ•°æ—¶å€™ï¼Œä½ çœŸçš„æ›´å…³å¿ƒé‚£ä¸ªæ¨¡æ‹Ÿçš„ç»“æœï¼Œè€Œä¸æ˜¯ç›´æ¥å…³å¿ƒé¢„æµ‹æ¨¡å‹ã€‚ä½†å¤§å¤šæ•°äººç›®å‰å¹¶æ²¡æœ‰æœ‰æ•ˆåœ°åšåˆ°è¿™ä¸€ç‚¹ã€‚ä¾‹å¦‚ï¼Œå½“æˆ‘å»äºšé©¬é€Šæ—¶ï¼Œæˆ‘è¯»äº†é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯çš„æ‰€æœ‰ä¹¦ï¼Œæ‰€ä»¥åœ¨æˆ‘è¯»å®Œæ‰€æœ‰é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯çš„ä¹¦ä¹‹åï¼Œä¸‹æ¬¡æˆ‘å»äºšé©¬é€Šï¼Œä»–ä»¬è¯´ä½ æƒ³ä¹°é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯çš„å…¨éƒ¨ä½œå“å—ã€‚è¿™æ˜¯åœ¨æˆ‘å·²ç»ä¹°äº†ä»–çš„æ¯ä¸€æœ¬ä¹¦ä¹‹åã€‚ä»æœºå™¨å­¦ä¹ çš„è§’åº¦æ¥çœ‹ï¼Œä¸€äº›æ•°æ®ç§‘å­¦å®¶å¯èƒ½ä¼šè¯´ï¼Œè´­ä¹°é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯çš„ä¸€æœ¬ä¹¦çš„äººé€šå¸¸ä¼šç»§ç»­è´­ä¹°ä»–çš„å…¨éƒ¨ä½œå“ã€‚ä½†å‘æˆ‘æ¨èè´­ä¹°é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯çš„å…¨éƒ¨ä½œå“å¹¶ä¸æ˜æ™ºã€‚è¿™å®é™…ä¸Šåœ¨å¾ˆå¤šæ–¹é¢éƒ½ä¸æ˜æ™ºã€‚ä¸ä»…æ˜¯å› ä¸ºæˆ‘ä¸å¤ªå¯èƒ½è´­ä¹°ä¸€ä¸ªæˆ‘å·²ç»æœ‰æ¯ä¸€æœ¬ä¹¦çš„åˆé›†ï¼Œè€Œä¸”è¿™ä¹Ÿä¸ä¼šæ”¹å˜æˆ‘çš„è´­ä¹°è¡Œä¸ºã€‚æˆ‘å·²ç»äº†è§£é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯ï¼Œæˆ‘å·²ç»çŸ¥é“æˆ‘å–œæ¬¢ä»–ï¼Œæ‰€ä»¥å ç”¨ä½ å®è´µçš„ç½‘é¡µç©ºé—´æ¥å‘Šè¯‰æˆ‘ï¼Œå˜¿ï¼Œä¹Ÿè®¸ä½ åº”è¯¥è´­ä¹°æ›´å¤šä½ å·²ç»ç†Ÿæ‚‰å¹¶å¤šæ¬¡è´­ä¹°çš„ä½œè€…çš„ä½œå“å®é™…ä¸Šä¸ä¼šæ”¹å˜æˆ‘çš„è¡Œä¸ºã€‚é‚£ä¹ˆï¼Œå¦‚æœäºšé©¬é€Šä¸æ˜¯åˆ›å»ºä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œè€Œæ˜¯å»ºç«‹ä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿçš„ä¼˜åŒ–æ¨¡å‹ï¼Œç„¶åè¯´å¦‚æœæˆ‘ä»¬å‘æ°é‡Œç±³å±•ç¤ºè¿™ä¸ªå¹¿å‘Šï¼Œä»–ä¼šæœ‰å¤šå¤§å¯èƒ½ç»§ç»­è´­ä¹°è¿™æœ¬ä¹¦ï¼Œå¦‚æœæˆ‘ä¸å‘ä»–å±•ç¤ºè¿™ä¸ªå¹¿å‘Šï¼Œä»–ä¼šæœ‰å¤šå¤§å¯èƒ½ç»§ç»­è´­ä¹°è¿™æœ¬ä¹¦ã€‚è¿™å°±æ˜¯å¯¹ç«‹äº‹å®ã€‚å¯¹ç«‹äº‹å®æ˜¯å¦åˆ™ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œç„¶åä½ å¯ä»¥è®¡ç®—å·®å¼‚ï¼Œç„¶åè¯´æˆ‘ä»¬åº”è¯¥æ¨èä»–ä»€ä¹ˆæ‰èƒ½æœ€å¤§ç¨‹åº¦åœ°æ”¹å˜ä»–çš„è¡Œä¸ºã€‚æ‰€ä»¥æœ€å¤§ç¨‹åº¦åœ°å¯¼è‡´æ›´å¤šçš„ä¹¦ç±ï¼Œæ‰€ä»¥ä½ å¯èƒ½ä¼šè¯´ï¼Œå“¦ï¼Œä»–ä»æ¥æ²¡æœ‰ä¹°è¿‡ç‰¹é‡ŒÂ·æ™®æ‹‰åˆ‡ç‰¹çš„ä¹¦ï¼Œä»–å¯èƒ½ä¸äº†è§£ç‰¹é‡ŒÂ·æ™®æ‹‰åˆ‡ç‰¹ï¼Œä½†å¾ˆå¤šå–œæ¬¢é“æ ¼æ‹‰æ–¯Â·äºšå½“æ–¯çš„äººç¡®å®å–œæ¬¢ç‰¹é‡ŒÂ·æ™®æ‹‰åˆ‡ç‰¹ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å‘ä»–ä»‹ç»ä¸€ä¸ªæ–°çš„ä½œè€…ã€‚
- en: So itâ€™s the difference between a predictive model on the one hand versus an
    optimization model on the other hand. So the two tend to go hand in hand. First
    of all we have a simulation model. The simulation model is saying in the world
    where we put Terry Pratchettâ€™s book on the front page of Amazon for Jeremy Howard,
    this is what would have happened. He would have bought it with a 94% probability.
    That then tells us with this lever of what do I put on my homepage for Jeremy
    today, we say okay the different settings of that lever that put Terry Pratchett
    on the homepage has the highest simulated outcome. Then thatâ€™s the thing which
    maximizes our profit from Jeremyâ€™s visit to amazon.com today.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œä¸€æ–¹é¢æ˜¯é¢„æµ‹æ¨¡å‹ï¼Œå¦ä¸€æ–¹é¢æ˜¯ä¼˜åŒ–æ¨¡å‹ä¹‹é—´çš„åŒºåˆ«ã€‚æ‰€ä»¥è¿™ä¸¤è€…å¾€å¾€æ˜¯ç›¸è¾…ç›¸æˆçš„ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ã€‚æ¨¡æ‹Ÿæ¨¡å‹æ˜¯åœ¨è¯´ï¼Œå¦‚æœæˆ‘ä»¬æŠŠç‰¹é‡ŒÂ·æ™®æ‹‰åˆ‡ç‰¹çš„ä¹¦æ”¾åœ¨äºšé©¬é€Šçš„é¦–é¡µä¸Šç»™æ°é‡Œç±³Â·éœåå¾·çœ‹ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚ä»–æœ‰94%çš„æ¦‚ç‡ä¼šè´­ä¹°ã€‚è¿™å‘Šè¯‰æˆ‘ä»¬ï¼Œé€šè¿‡è¿™ä¸ªæ æ†ï¼Œæˆ‘ä»Šå¤©åº”è¯¥åœ¨æ°é‡Œç±³çš„é¦–é¡µä¸Šæ”¾ä»€ä¹ˆï¼Œæˆ‘ä»¬è¯´å¥½ï¼ŒæŠŠç‰¹é‡ŒÂ·æ™®æ‹‰åˆ‡ç‰¹æ”¾åœ¨é¦–é¡µä¸Šçš„ä¸åŒè®¾ç½®ä¼šäº§ç”Ÿæœ€é«˜çš„æ¨¡æ‹Ÿç»“æœã€‚ç„¶åè¿™å°±æ˜¯æœ€å¤§åŒ–æˆ‘ä»¬ä»æ°é‡Œç±³ä»Šå¤©è®¿é—®äºšé©¬é€Šç½‘ç«™ä¸­çš„åˆ©æ¶¦çš„äº‹æƒ…ã€‚
- en: Generally speaking, your predictive models feed into this simulation model but
    you kind of have to think about how they all work together. For example, letâ€™s
    go back to churn. So it turned out that Jeremy Howard is very likely to leave
    his cell phone company next month. What are we going to about it? Letâ€™s call him.
    And I can tell you if my cell phone company calls me right now and says â€œjust
    calling to say we love youâ€ Iâ€™d be like Iâ€™m cancelling right now. That would be
    a terrible idea. So again, you would want a simulation model that says whatâ€™s
    the probability that Jeremy is going to change his behavior as a result of calling
    him right now. So one of the levers I have is call him. On the other hand, if
    I got a piece of mail tomorrow that said for each month you stay with us, weâ€™re
    going to give you a hundred thousand dollars. Then thatâ€™s going to definitely
    change my behavior, right? But then feeding that into the simulation model, it
    turns out that overall that would be an unprofitable choice to make. Do you see
    how this fits in together?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€èˆ¬æ¥è¯´ï¼Œä½ çš„é¢„æµ‹æ¨¡å‹ä¼šè¾“å…¥åˆ°è¿™ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ä¸­ï¼Œä½†ä½ å¿…é¡»è€ƒè™‘å®ƒä»¬å¦‚ä½•å…±åŒå·¥ä½œã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬å›åˆ°æµå¤±é—®é¢˜ã€‚ç»“æœè¡¨æ˜ï¼ŒJeremy Howardå¾ˆå¯èƒ½ä¼šåœ¨ä¸‹ä¸ªæœˆç¦»å¼€ä»–çš„æ‰‹æœºå…¬å¸ã€‚æˆ‘ä»¬è¯¥æ€ä¹ˆåŠï¼Ÿè®©æˆ‘ä»¬ç»™ä»–æ‰“ç”µè¯ã€‚æˆ‘å¯ä»¥å‘Šè¯‰ä½ ï¼Œå¦‚æœæˆ‘çš„æ‰‹æœºå…¬å¸ç°åœ¨ç»™æˆ‘æ‰“ç”µè¯è¯´â€œåªæ˜¯æ‰“ç”µè¯å‘Šè¯‰ä½ æˆ‘ä»¬çˆ±ä½ â€ï¼Œæˆ‘ä¼šç«‹åˆ»å–æ¶ˆã€‚é‚£å°†æ˜¯ä¸€ä¸ªç³Ÿç³•çš„ä¸»æ„ã€‚å› æ­¤ï¼Œä½ ä¼šæƒ³è¦ä¸€ä¸ªæ¨¡æ‹Ÿæ¨¡å‹ï¼Œæ¥è¯´Jeremyç°åœ¨æ¥åˆ°ç”µè¯åæ”¹å˜è¡Œä¸ºçš„æ¦‚ç‡æ˜¯å¤šå°‘ã€‚æ‰€ä»¥æˆ‘æœ‰ä¸€ä¸ªæ æ†æ˜¯ç»™ä»–æ‰“ç”µè¯ã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæ˜å¤©æˆ‘æ”¶åˆ°ä¸€å°ä¿¡ï¼Œè¯´æ¯ä¸ªæœˆä½ å’Œæˆ‘ä»¬åœ¨ä¸€èµ·ï¼Œæˆ‘ä»¬ä¼šç»™ä½ åä¸‡ç¾å…ƒã€‚é‚£è‚¯å®šä¼šæ”¹å˜æˆ‘çš„è¡Œä¸ºï¼Œå¯¹å§ï¼Ÿä½†æ˜¯å°†è¿™ä¸ªè¾“å…¥åˆ°æ¨¡æ‹Ÿæ¨¡å‹ä¸­ï¼Œç»“æœæ˜¯è¿™å°†æ˜¯ä¸€ä¸ªä¸ç›ˆåˆ©çš„é€‰æ‹©ã€‚ä½ çœ‹åˆ°è¿™æ˜¯å¦‚ä½•ç›¸äº’é…åˆçš„å—ï¼Ÿ
- en: So when we look at something like churn, we want to be thinking what are the
    levers we can pull [[14:33](https://youtu.be/BFIYUvBRTpE?t=14m33s)]. What are
    the kinds of models that we could build with what kinds of data to help us pull
    those levers better to achieve our objectives. When you think about it that way,
    you realize that the vast majority of these applications are not largely about
    a predictive model at all. They are about interpretation. They are about understanding
    what happens if. So if we take the intersection between on the one hand, here
    are all the levers that we could pull (here are all the things we can do) and
    then here are all of the features from our random forest feature importance that
    turn out to be strong drivers of the outcome. So then the intersection of those
    is here are the levers we could pull that actually matter. Because if you canâ€™t
    change the thing, that is not very interesting. And if itâ€™s not actually a significant
    driver, itâ€™s not very interesting. So we can actually use our random forest feature
    importance to tell us what can we actually do to make a difference. Then we can
    use the partial dependence to actually build this kind of simulation model to
    say okay if we did change that, what would happen.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“æˆ‘ä»¬çœ‹æµå¤±è¿™æ ·çš„é—®é¢˜æ—¶ï¼Œæˆ‘ä»¬è¦è€ƒè™‘æˆ‘ä»¬å¯ä»¥æ‹‰åŠ¨çš„æ æ†æ˜¯ä»€ä¹ˆã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä»€ä¹ˆæ ·çš„æ•°æ®æ„å»ºä»€ä¹ˆæ ·çš„æ¨¡å‹æ¥å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°æ‹‰åŠ¨è¿™äº›æ æ†ä»¥å®ç°æˆ‘ä»¬çš„ç›®æ ‡ã€‚å½“ä½ è¿™æ ·æ€è€ƒæ—¶ï¼Œä½ ä¼šæ„è¯†åˆ°è¿™äº›åº”ç”¨çš„ç»å¤§éƒ¨åˆ†å®é™…ä¸Šå¹¶ä¸æ˜¯å…³äºé¢„æµ‹æ¨¡å‹ã€‚å®ƒä»¬æ˜¯å…³äºè§£é‡Šçš„ã€‚å®ƒä»¬æ˜¯å…³äºç†è§£â€œå¦‚æœå‘ç”Ÿäº†ä»€ä¹ˆâ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å®é™…ä½¿ç”¨æˆ‘ä»¬çš„éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§å‘Šè¯‰æˆ‘ä»¬æˆ‘ä»¬å®é™…ä¸Šå¯ä»¥åšäº›ä»€ä¹ˆæ¥äº§ç”Ÿå½±å“ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨éƒ¨åˆ†ä¾èµ–æ¥æ„å»ºè¿™ç§æ¨¡æ‹Ÿæ¨¡å‹ï¼Œæ¥è¯´å¦‚æœæˆ‘ä»¬æ”¹å˜äº†é‚£ä¸ªï¼Œä¼šå‘ç”Ÿä»€ä¹ˆã€‚
- en: So there are lots of examples and what I want you to think about as you think
    about the machine learning problems you are working on is why does somebody care
    about this [[16:02](https://youtu.be/BFIYUvBRTpE?t=16m2s)]. What would a good
    answer to them look like and how could you actually positively impact this business.
    So if you are creating a Kaggle kernel, try to think about from the point of view
    of the competition organizer. What would they want to know and how can you give
    them that information. So something like fraud detection on the other hand, you
    probably just basically want to know whose fraudulent. So you probably do just
    care about the predictive model. But then you do have to think carefully about
    the data availability here. So okay, we need to know who is fraudulent at the
    point that we are about to deliver them a product. So itâ€™s no point looking at
    data thatâ€™s available a month later, for instance. So you have this key issue
    of thinking about the actual operational constraints that you are working under.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æœ‰å¾ˆå¤šä¾‹å­ï¼Œå½“ä½ æ€è€ƒä½ æ­£åœ¨å¤„ç†çš„æœºå™¨å­¦ä¹ é—®é¢˜æ—¶ï¼Œæˆ‘å¸Œæœ›ä½ è€ƒè™‘ä¸ºä»€ä¹ˆæœ‰äººä¼šå…³å¿ƒè¿™ä¸ªé—®é¢˜ã€‚å¯¹ä»–ä»¬æ¥è¯´ä¸€ä¸ªå¥½çš„ç­”æ¡ˆæ˜¯ä»€ä¹ˆæ ·çš„ï¼Œä½ å¦‚ä½•å®é™…ä¸Šå¯¹è¿™ä¸ªä¸šåŠ¡äº§ç”Ÿç§¯æå½±å“ã€‚æ‰€ä»¥å¦‚æœä½ åœ¨åˆ›å»ºä¸€ä¸ªKaggleå†…æ ¸ï¼Œè¯•ç€ä»ç«èµ›ç»„ç»‡è€…çš„è§’åº¦æ€è€ƒã€‚ä»–ä»¬æƒ³çŸ¥é“ä»€ä¹ˆï¼Œä½ å¦‚ä½•ç»™ä»–ä»¬è¿™äº›ä¿¡æ¯ã€‚å¦ä¸€æ–¹é¢ï¼Œåƒæ¬ºè¯ˆæ£€æµ‹ï¼Œä½ å¯èƒ½åªæ˜¯æƒ³çŸ¥é“è°æ˜¯æ¬ºè¯ˆçš„ã€‚æ‰€ä»¥ä½ å¯èƒ½åªå…³å¿ƒé¢„æµ‹æ¨¡å‹ã€‚ä½†æ˜¯ä½ å¿…é¡»ä»”ç»†è€ƒè™‘è¿™é‡Œçš„æ•°æ®å¯ç”¨æ€§ã€‚æ‰€ä»¥å¥½å§ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“åœ¨æˆ‘ä»¬å³å°†å‘ä»–ä»¬äº¤ä»˜äº§å“æ—¶è°æ˜¯æ¬ºè¯ˆçš„ã€‚ä¾‹å¦‚ï¼ŒæŸ¥çœ‹ä¸€ä¸ªæœˆåå¯ç”¨çš„æ•°æ®æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚æ‰€ä»¥ä½ å¿…é¡»è€ƒè™‘ä½ æ­£åœ¨å·¥ä½œçš„å®é™…è¿è¥çº¦æŸã€‚
- en: Human Resources Applications [[17:17](https://youtu.be/BFIYUvBRTpE?t=17m17s)]
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: äººåŠ›èµ„æºåº”ç”¨ã€‚
- en: Lots of interesting application in human resources but like employee churn,
    itâ€™s another kind of churn model where finding out that Jeremy Howard is sick
    of lecturing, heâ€™s going to leave tomorrow. What are you going to do about it?
    Well, knowing that wouldnâ€™t actually be helpful. It would be too late. You would
    actually want a model that said what kinds of people are leaving USF and it turns
    out that everybody that goes to the downstairs cafe leaves USF. I guess their
    food is awful or whatever. Or everybody that we are paying less than half a million
    dollars a year is leaving USF because they canâ€™t afford basic housing in San Francisco.
    So you could use your employee churn model not so much to say which employees
    hate us but why do employees leave. Again itâ€™s really the interpretation there
    that matters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: For churn model, it sounds like there are two predictors that
    you need to predict for â€” one being churn and the other you need to optimize your
    profit. So how does it work [[18:30](https://youtu.be/BFIYUvBRTpE?t=18m30s)]?
    Yes, exactly. So this is what the simulation model is all about. You figure out
    this objective we are trying to maximize which is company profitability. You can
    create a pretty simple Excel model or something that says here is the revenue
    and here is the costs and the cost is equal to the number of people we employ
    multiplied by their salary, etc. Inside that Excel model, there are certain cells/inputs
    that are kind of stochastic or uncertain. But we could predict it with a model
    and so thatâ€™s what I do then is to say okay we need a predictive model for how
    likely somebody is to stay if we change their salary, how likely they are to leave
    with the current salary, how likely they are to leave next year if I increased
    their salary now, etc. So you a bunch of different models and then you can bind
    them together with simple business logic and then you can optimize that. You can
    then say okay if I pay Jeremy Howard half a million dollars, thatâ€™s probably a
    really good idea and if I pay him less then itâ€™s probably not or whatever. You
    can figure out the overall impact. So itâ€™s really shocking to me how few people
    do this. But most people in industry measure their models using AUC or RMSE or
    whatever which is never actually what you really want.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: More Horizontal Applicationsâ€¦[[22:04](https://youtu.be/BFIYUvBRTpE?t=22m4s)]
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lead prioritization is a really interesting one. Every one of these boxes Iâ€™m
    showing, you can generally find a company or many companies whose sole job in
    life is to build models of that thing. So there are lots of companies that sell
    lead prioritization systems but again the question is how would we use that information.
    So if itâ€™s like our best lead is Jeremy, he is a highest probability of buying.
    Does that mean I should send a salesperson out to Jeremy or I shouldnâ€™t? If heâ€™s
    highly probable to buy, why I waste my time with him. So again, you really want
    some kind of simulation that says whatâ€™s the likely change in Jeremyâ€™s behavior
    if I send my best salesperson out to go and encourage him to sign. I think there
    are many many opportunities for data scientists in the world today to move beyond
    predictive modeling to actually bringing it all together.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Vertical Applications [[23:29](https://youtu.be/BFIYUvBRTpE?t=23m29s)]
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As well as these horizontal applications that basically apply to every company,
    thereâ€™s a whole bunch of applications that are specific to every part of the world.
    For those of you that end up in healthcare, some of you will become experts in
    one or more of these areas. Like readmission risk. So whatâ€™s the probability that
    this patient is going to come back to the hospital. Depending on the details of
    the jurisdiction, it can be a disaster for hospitals when somebody is readmitted.
    If you find out that this patient has a high probability of readmission, what
    do you do about it? Again, the predictive model is helpful of itself. It rather
    suggests we shouldnâ€™t send them home yet because they are going to come back.
    But wouldnâ€™t it be nice if we had the tree interpreter and it said to us the reason
    that they are at high risk is because we donâ€™t have a recent EKG/ECG for them.
    Without a recent EKG, we canâ€™t have a high confidence about their cardiac health.
    In which case, it wouldnâ€™t be like letâ€™s keep them in the hospital for two weeks,
    itâ€™ll be letâ€™s give them an EKG. So this is interaction between interpretation
    and predictive accuracy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: So what Iâ€™m understanding you are saying is that the predictive
    models are a really great but in order to actually answer these questions, we
    really need to focus on the interpretability of these models [[24:59](https://youtu.be/BFIYUvBRTpE?t=24m59s)]?
    Yeah, I think so. More specifically Iâ€™m saying we just learnt a whole raft of
    random forest interpretation techniques and so Iâ€™m trying to justify why. The
    reason why is because Iâ€™d say most of the time the interpretation is the thing
    we care about. You can create a chart or a table without machine learning and
    indeed thatâ€™s how most of the world works. Most managers build all kinds of tables
    and charts without any machine learning behind them. But they often make terrible
    decisions because they donâ€™t know the feature importance of the objective they
    are interested in and so the table they create is of things that actually are
    the least important things anyway. Or they just do a univariate chart rather than
    a partial dependence plot, so they donâ€™t actually realize that the relationship
    they thought they are looking at is due entirely to something else. So Iâ€™m kind
    of arguing for data scientists getting much more deeply involved in strategy and
    in trying to use machine learning to really help a business with all of its objectives.
    There are companies like dunnhumby which is a huge company that does nothing but
    retail application with machine learning. I believe thereâ€™s like a dunnhumby product
    you can buy which will help you figure out if I put my new store in this location
    versus that location, how many people are going to shop there. Or if I put my
    diapers in this part of the shop versus that part of the shop, how is that going
    to impact purchasing behavior, etc. So itâ€™s also good to realize that the subset
    of machine learning applications you tend to hear about in the tech press or whatever
    is this massively biased tiny subset of stuff which Google and Facebook do. Where
    else the vast majority of stuff that actually makes the world go around is these
    kinds of applications that actually help people make things, buy things, sell
    things, build things, so forth.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: About tree interpretation, we looked at which feature was more
    important for a particular observation. For businesses, they have a huge amount
    of data and they want this interpretation for a lot of observations so how do
    they automate it? Do they set threshold [[27:50](https://youtu.be/BFIYUvBRTpE?t=27m50s)]?
    The vast majority of machine learning models donâ€™t automate anything. They are
    designed to provide information to humans. So for example, if you are a customer
    service phone operator for an insurance company and your customer asks you why
    is my renewal $500 more expensive than last time, then hopefully the insurance
    company provides in your terminal those little screen that shows the result of
    the tree interpreter or whatever. So you can jump there and tell the customer
    that last year you were in this different zip code which has lower amounts of
    car theft, and this year also youâ€™ve actually changed your vehicle to more expensive
    one. So itâ€™s not so much about thresholds and automation, but about making these
    model outputs available to the decision makers in the organization whether they
    be at the top strategic level of like are we going to shutdown this whole product
    or not, all the way to the operational level of that individual discussion with
    a customer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: So another example is aircraft scheduling and gate management. Thereâ€™s lots
    of companies that do that and basically what happens is that there are people
    at an airport whose job it is to basically tell each aircraft what gate to go
    to, to figure out when to close the doors, stuff like that. So the idea is youâ€™re
    giving them software which has the information they need to make good decisions.
    So the machine learning models end up embedded in that software to say okay that
    plane thatâ€™s currently coming in from Miami, thereâ€™s a 48% chance that itâ€™s going
    to be over 5 minutes late and if it does then this is going to be the knock-on
    impact through the rest of the terminal, for instance. So thatâ€™s how these things
    fit together.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Other applications [[31:02](https://youtu.be/BFIYUvBRTpE?t=31m2s)]
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/92404da0b0fdf953b4c60b9f1a454c48.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: There are lots of applications, and what I want you to do is to spend some time
    thinking about them. Sit down with one of your friends and talk about a few examples.
    For example, how would we go about doing failure analysis in manufacturing, who
    would be doing that, why would they be doing it, what kind of models might they
    use, what kind of data might they use. Start to practice and get a sense. Then
    when youâ€™re at the workplace and talking to managers, you want to be straightaway
    able to recognize that the person you are talking to â€” what are they trying to
    achieve, what are the levers they have to pull, what are the data they have available
    to pull those levers to achieve that thing, and therefore how could we build models
    to help them do that and what kind of predictions would they have to be making.
    So then you can have this really thoughtful empathetic conversation with those
    people and then saying â€œin order to reduce the number of customers that are leaving,
    I guess you are trying to figure out who should you be providing better pricing
    toâ€ and so forth.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Are explanatory problems people are faced with in social sciences
    something machine learning can be useful for or is used for or is that nor really
    the realm thatâ€™s in [[32:29](https://youtu.be/BFIYUvBRTpE?t=32m29s)]? Iâ€™ve had
    a lot of conversations about this with people in social sciences and currently
    machine learning is not well applied in economics or psychology or whatever on
    the whole. But Iâ€™m convinced it can be for the exact reasons we are talking about.
    So if you are going to try to do some kind of behavioral economics and youâ€™re
    trying to understand why some people behave differently to other people, a random
    forest with a feature importance plot would be a great way to start. More interestingly,
    if you are trying to do some kind of sociology experiment or analysis based on
    a large social network dataset where you have an observational study, you really
    want to try and pull out all of the sources of exogenous variables (i.e. all the
    stuff thatâ€™s going on outside) so if you use a partial dependence plot with a
    random forest that happens automatically. I actually gave a talk at MIT a couple
    of years ago for the first conference on digital experimentation which was really
    talking about how do we experiment in things like social networks in these digital
    environments and economists all do things with classic statistical tests but in
    this case, the economists I talked to were absolutely fascinated by this and they
    actually asked me to give an introduction to machine learning session at MIT to
    these various faculty and graduate folks in the economics department. And some
    of those folks have gone on to write some pretty famous books and so hopefully
    itâ€™s been useful. Itâ€™s definitely early days but itâ€™s a big opportunity. But as
    Yannet says, thereâ€™s plenty of skepticism still out there. The skepticism comes
    from unfamiliarity basically with this totally different approach. So if you spent
    20 years studying econometrics and somebody comes along and says here is a totally
    different approach to all the stuff econometricians do, naturally your first reaction
    will be â€œprove itâ€. So thatâ€™s fair enough but I think over time the next generation
    of people who are growing up with machine learning, some of them will move into
    the social sciences, theyâ€™ll make huge impacts that nobody has ever managed to
    make before and people will start going wow. Just like happened in computer vision.
    When computer vision spent a long time of people saying â€œmaybe you should use
    deep learning for computer visionâ€ and everybody in computer vision said â€œProve
    it. We have decades of work on amazing feature detectors for computer vision.â€
    And then finally in 2012, Hinton and Kryzanski came along and said â€œour model
    is twice as good as yours and weâ€™ve only just started on thisâ€ and everybody was
    convinced. Nowadays every computer vision researchers basically uses deep learning.
    So I think that time will come in this area too.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**é—®é¢˜**ï¼šç¤¾ä¼šç§‘å­¦ä¸­äººä»¬é¢ä¸´çš„è§£é‡Šé—®é¢˜æ˜¯å¦å¯ä»¥ä½¿ç”¨æœºå™¨å­¦ä¹ æˆ–è€…å·²ç»è¢«ä½¿ç”¨ï¼Œæˆ–è€…è¿™å¹¶ä¸æ˜¯çœŸæ­£çš„é¢†åŸŸ[[32:29](https://youtu.be/BFIYUvBRTpE?t=32m29s)]ï¼Ÿæˆ‘ä¸ç¤¾ä¼šç§‘å­¦é¢†åŸŸçš„äººä»¬è¿›è¡Œäº†å¾ˆå¤šå…³äºè¿™ä¸ªé—®é¢˜çš„è®¨è®ºï¼Œç›®å‰æœºå™¨å­¦ä¹ åœ¨ç»æµå­¦æˆ–å¿ƒç†å­¦ç­‰é¢†åŸŸå¹¶æ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„åº”ç”¨ã€‚ä½†æˆ‘ç›¸ä¿¡å®ƒå¯ä»¥ï¼ŒåŸå› æ­£å¦‚æˆ‘ä»¬æ‰€è®¨è®ºçš„é‚£æ ·ã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨è¦å°è¯•è¿›è¡ŒæŸç§è¡Œä¸ºç»æµå­¦ç ”ç©¶ï¼Œå¹¶ä¸”è¯•å›¾ç†è§£ä¸ºä»€ä¹ˆæœ‰äº›äººçš„è¡Œä¸ºä¸å…¶ä»–äººä¸åŒï¼Œä½¿ç”¨å…·æœ‰ç‰¹å¾é‡è¦æ€§å›¾çš„éšæœºæ£®æ—å°†æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å¼€å§‹ã€‚æ›´æœ‰è¶£çš„æ˜¯ï¼Œå¦‚æœæ‚¨å°è¯•è¿›è¡ŒæŸç§åŸºäºå¤§å‹ç¤¾äº¤ç½‘ç»œæ•°æ®é›†çš„ç¤¾ä¼šå­¦å®éªŒæˆ–åˆ†æï¼Œåœ¨é‚£é‡Œæ‚¨è¿›è¡Œäº†ä¸€é¡¹è§‚å¯Ÿæ€§ç ”ç©¶ï¼Œæ‚¨çœŸçš„æƒ³è¦å°è¯•æå–æ‰€æœ‰å¤–ç”Ÿå˜é‡çš„æ¥æºï¼ˆå³æ‰€æœ‰å¤–éƒ¨å‘ç”Ÿçš„äº‹æƒ…ï¼‰ï¼Œå› æ­¤å¦‚æœæ‚¨ä½¿ç”¨å…·æœ‰éšæœºæ£®æ—çš„éƒ¨åˆ†ä¾èµ–å›¾ï¼Œè¿™å°†è‡ªåŠ¨å‘ç”Ÿã€‚å‡ å¹´å‰ï¼Œæˆ‘åœ¨éº»çœç†å·¥å­¦é™¢åšäº†ä¸€ä¸ªå…³äºæ•°å­—å®éªŒçš„ç¬¬ä¸€æ¬¡ä¼šè®®çš„æ¼”è®²ï¼Œè¿™æ¬¡ä¼šè®®çœŸæ­£è®¨è®ºäº†æˆ‘ä»¬å¦‚ä½•åœ¨è¯¸å¦‚ç¤¾äº¤ç½‘ç»œç­‰æ•°å­—ç¯å¢ƒä¸­è¿›è¡Œå®éªŒï¼Œç»æµå­¦å®¶ä»¬éƒ½ä½¿ç”¨ç»å…¸çš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä¸ä¹‹äº¤è°ˆçš„ç»æµå­¦å®¶ä»¬å¯¹æ­¤éå¸¸ç€è¿·ï¼Œä»–ä»¬å®é™…ä¸Šè¦æ±‚æˆ‘åœ¨éº»çœç†å·¥å­¦é™¢ä¸ºç»æµå­¦ç³»çš„å„ç§æ•™å‘˜å’Œç ”ç©¶ç”Ÿä»¬ä¸¾åŠä¸€ä¸ªæœºå™¨å­¦ä¹ å…¥é—¨è¯¾ç¨‹ã€‚å…¶ä¸­ä¸€äº›äººå·²ç»å†™äº†ä¸€äº›ç›¸å½“æœ‰åçš„ä¹¦ç±ï¼Œå¸Œæœ›è¿™å¯¹ä»–ä»¬æœ‰æ‰€å¸®åŠ©ã€‚ç°åœ¨è¿˜å¤„äºæ—©æœŸé˜¶æ®µï¼Œä½†è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„æœºä¼šã€‚ä½†æ­£å¦‚Yannetæ‰€è¯´ï¼Œä»ç„¶å­˜åœ¨å¾ˆå¤šæ€€ç–‘ã€‚è¿™ç§æ€€ç–‘ä¸»è¦æ¥è‡ªå¯¹è¿™ç§å®Œå…¨ä¸åŒæ–¹æ³•çš„é™Œç”Ÿæ„Ÿã€‚å› æ­¤ï¼Œå¦‚æœæ‚¨èŠ±äº†20å¹´æ—¶é—´ç ”ç©¶è®¡é‡ç»æµå­¦ï¼Œç„¶åæœ‰äººè¿‡æ¥è¯´è¿™æ˜¯ä¸€ç§å®Œå…¨ä¸åŒäºè®¡é‡ç»æµå­¦å®¶æ‰€åšçš„æ‰€æœ‰äº‹æƒ…çš„æ–¹æ³•ï¼Œé‚£ä¹ˆæ‚¨çš„ç¬¬ä¸€ååº”è‡ªç„¶ä¼šæ˜¯â€œè¯æ˜å®ƒâ€ã€‚è¿™æ˜¯å…¬å¹³çš„ï¼Œä½†æˆ‘è®¤ä¸ºéšç€æ—¶é—´çš„æ¨ç§»ï¼Œä¸‹ä¸€ä»£ä¸æœºå™¨å­¦ä¹ ä¸€èµ·æˆé•¿çš„äººä»¬ä¸­ï¼Œä¸€äº›äººå°†è¿›å…¥ç¤¾ä¼šç§‘å­¦é¢†åŸŸï¼Œä»–ä»¬å°†äº§ç”Ÿå‰æ‰€æœªæœ‰çš„å·¨å¤§å½±å“ï¼Œäººä»¬å°†å¼€å§‹æ„Ÿåˆ°æƒŠè®¶ã€‚å°±åƒè®¡ç®—æœºè§†è§‰ä¸­å‘ç”Ÿçš„ä¸€æ ·ã€‚å½“è®¡ç®—æœºè§†è§‰èŠ±äº†å¾ˆé•¿æ—¶é—´çš„äººä»¬è¯´â€œä¹Ÿè®¸ä½ åº”è¯¥ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¥è¿›è¡Œè®¡ç®—æœºè§†è§‰â€ï¼Œè€Œè®¡ç®—æœºè§†è§‰é¢†åŸŸçš„æ¯ä¸ªäººéƒ½è¯´â€œè¯æ˜å®ƒã€‚æˆ‘ä»¬åœ¨è®¡ç®—æœºè§†è§‰ä¸­æœ‰å‡ åå¹´çš„å·¥ä½œï¼Œå¼€å‘äº†ä»¤äººæƒŠå¹çš„ç‰¹å¾æ£€æµ‹å™¨ã€‚â€ç„¶ååœ¨2012å¹´ï¼Œè¾›é¡¿å’Œå…‹é‡Œèµæ–¯åŸºå‡ºç°äº†ï¼Œä»–ä»¬è¯´â€œæˆ‘ä»¬çš„æ¨¡å‹æ¯”ä½ ä»¬çš„å¥½ä¸¤å€ï¼Œè€Œæˆ‘ä»¬åˆšåˆšå¼€å§‹â€
    ï¼Œæ¯ä¸ªäººéƒ½è¢«è¯´æœäº†ã€‚å¦‚ä»Šï¼Œå‡ ä¹æ¯ä¸ªè®¡ç®—æœºè§†è§‰ç ”ç©¶äººå‘˜åŸºæœ¬ä¸Šéƒ½ä½¿ç”¨æ·±åº¦å­¦ä¹ ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºåœ¨è¿™ä¸ªé¢†åŸŸä¹Ÿä¼šå‡ºç°è¿™æ ·çš„æ—¶åˆ»ã€‚'
- en: Different random forest interpretation methods [[37:17](https://youtu.be/BFIYUvBRTpE?t=37m17s)]
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: éšæœºæ£®æ—è§£é‡Šæ–¹æ³•[[37:17](https://youtu.be/BFIYUvBRTpE?t=37m17s)]
- en: Having talked about why they are important, letâ€™s now remind ourselves what
    they are.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ˆè®ºå®ƒä»¬ä¸ºä»€ä¹ˆé‡è¦ä¹‹åï¼Œè®©æˆ‘ä»¬ç°åœ¨æé†’è‡ªå·±å®ƒä»¬æ˜¯ä»€ä¹ˆã€‚
- en: '**Confidence based on tree variance**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**åŸºäºæ ‘æ–¹å·®çš„ç½®ä¿¡åº¦**'
- en: What does it tell us? Why would be interested in that? How is it calculated?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆæˆ‘ä»¬å¯¹æ­¤æ„Ÿå…´è¶£ï¼Ÿå®ƒæ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Ÿ
- en: The variance of the predictions of the trees. Normally the prediction is just
    the average, this is variance of the trees.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ ‘çš„é¢„æµ‹æ–¹å·®ã€‚é€šå¸¸é¢„æµ‹åªæ˜¯å¹³å‡å€¼ï¼Œè¿™æ˜¯æ ‘çš„æ–¹å·®ã€‚
- en: Just to fill in a detail here, what we generally do here is we take just one
    row/observation often and find out how confident we are about that (i.e. how much
    variance there are in the trees for that) or we can do as we did here for different
    groups [[39:34](https://youtu.be/BFIYUvBRTpE?t=39m34s)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œå¡«å……ä¸€ä¸ªç»†èŠ‚ï¼Œæˆ‘ä»¬é€šå¸¸åªå–ä¸€è¡Œ/è§‚å¯Ÿç»“æœï¼Œç„¶åæ‰¾å‡ºæˆ‘ä»¬å¯¹æ­¤æœ‰å¤šè‡ªä¿¡ï¼ˆå³æ ‘ä¸­æœ‰å¤šå°‘æ–¹å·®ï¼‰æˆ–è€…æˆ‘ä»¬å¯ä»¥åƒæˆ‘ä»¬åœ¨è¿™é‡Œåšçš„é‚£æ ·ä¸ºä¸åŒçš„ç»„æ‰¾å‡ºç­”æ¡ˆ[[39:34](https://youtu.be/BFIYUvBRTpE?t=39m34s)]ã€‚
- en: '![](../Images/59eb1771fe8d8082cee64809651cfd10.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59eb1771fe8d8082cee64809651cfd10.png)'
- en: What weâ€™ve done here is to say if there are any groups that we are very unconfident
    (which could be due to very little observations). Something that I think is even
    more important would be when you are using this operationally. Letâ€™s say you are
    doing a credit decisioning algorithm. So we are trying to determine whether Jeremy
    is a good risk or a bad risk. Should we loan him a million dollars. And the random
    forest says â€œI think heâ€™s a good risk but Iâ€™m not at all confident.â€ And in which
    case, we might say okay maybe I shouldnâ€™t give him a million dollars. Where else,
    if the random forest said â€œI think heâ€™s a good risk and Iâ€™m very sure of thatâ€
    then we are much more comfortable giving him a million dollars. And Iâ€™m a very
    good risk. So feel free to give me a million dollars. I checked the random forest
    before â€” a different notebook. Not in the repo ğŸ˜†
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Itâ€™s quite hard for me to give you folks direct experience with this kind of
    single observation interpretation because itâ€™s really the kind of stuff that you
    actually need to be putting out to the front line [[41:30](https://youtu.be/BFIYUvBRTpE?t=41m30s)].
    Itâ€™s not something which you can really use so much in a Kaggle context but itâ€™s
    more like if you are actually putting out some algorithm which is making big decisions
    that could cost a lot of money, you probably donâ€™t so much care about the average
    prediction of the random forest but maybe you actually care about the average
    minus a couple standard deviations (i.e. whatâ€™s the worst-case prediction). Maybe
    there is a whole group that we are unconfident about, so thatâ€™s confidence based
    on tree variance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance [[42:36](https://youtu.be/BFIYUvBRTpE?t=42m36s)]
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Student: Itâ€™s basically to find out which features are important. You take
    each feature and shuffle the values in the feature and check how the predictions
    change. If itâ€™s very different, it means that the feature was actually important;
    otherwise it is not that important.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: That was terrific. That was all exactly right. There were some details
    that were skimmed over a little bit. Anybody else wants to jump into a more detailed
    description of how itâ€™s calculated? How exactly do we calculate feature importance
    for a particular feature?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Student: After you are done building a random forest model, you take each column
    and randomly shuffle it. And you run a prediction and check the validation score.
    If it gets bad after shuffling one of the columns, that means that column was
    important, so it has a higher importance. Iâ€™m not exactly sure how we quantify
    the feature importance.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Ok, great. Do you know how we quantify the feature importance? That
    was a great description. To quantify, we can take the difference in RÂ² or score
    of some sort. So letâ€™s say weâ€™ve got our dependent variable which is price, and
    thereâ€™s a bunch of independent variables including year made [[44:22](https://youtu.be/BFIYUvBRTpE?t=44m22s)].
    We use the whole lot to build a random forest and that gives us our predictions.
    The we can compare that to get RÂ², RMSE, whatever you are interested in from the
    model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/757a2db0f725331a1679cc81ca3e81e8.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Now the key thing here is I donâ€™t want to have to retrain my whole random forest.
    Thatâ€™s slow and boring, so using the existing random forests. How can I figure
    out how important year made was? So the suggestion was, letâ€™s randomly shuffle
    the whole column. Now that column is totally useless. itâ€™s got the same mean,
    same distribution. Everything about it is the same, but thereâ€™s no connection
    at all between actual year made and whatâ€™s now in that column. Iâ€™ve randomly shuffled
    it. So now I put that new version through the same random forest (so there is
    no retraining done) to get some new Å· (ym). Then I can compare that to my actuals
    to get RMSE (ym). So now I can start to create a little table where I got the
    original RMSE (3, for example), with YearMade scrambled with RMSE of 2\. Enclosure
    scrambled had RMSE of 2.5\. Then I just take these differences. For YearMade,
    the importance is 1, Enclosure is 0.5, and so forth. How much worse did my model
    get after I shuffled that variable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5e68179bef47f6076fba1241e13c957.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: '**Question**: Would all importances sum to one [[46:52](https://youtu.be/BFIYUvBRTpE?t=46m52s)]?
    Honestly, Iâ€™ve never actually looked at what the units are, so Iâ€™m not quite sure.
    We can check it out during the week if somebodyâ€™s interested. Have a look at sklearn
    code and see exactly what those units of measures are because Iâ€™ve never bothered
    to check. Although I donâ€™t check like the units of measure specifically, what
    I do check is the relative importance. Here is an example.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/497e79ef0d22bc97ee5ccdbda4b72eb6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: So . rather than just saying what are the top ten, yesterday one of the practicum
    students asked me about a feature importance where they said â€œoh, I think these
    three are importantâ€ and I pointed out that the top one was thousand times more
    important than the second one. So look at the relative numbers here. So in that
    case, itâ€™s like â€œno, donâ€™t look at the top three, look at the one thatâ€™s a thousand
    times more important and ignore all the rest.â€ Your natural tendency is to want
    to be precise and careful, but this is where you need to override that and be
    very practical. This thing is a thousand times more important. Donâ€™t spend any
    time on anything else. Then you can go and talk to your manager of your project
    and say this thing is a thousand times more important. And then they might say
    â€œoh, that was a mistake. It shouldnâ€™t have been in there. We donâ€™t actually have
    that information at the decision time or for whatever reason we canâ€™t actually
    use that variable.â€ So then you could remove it and have a look. Or they might
    say â€œgosh, I had no idea that was by far more important than everything else put
    together. So letâ€™s forget this random forest thing and just focus on understanding
    how we can better collect that one variable and better use that one variable.â€
    So thatâ€™s something which comes up quite a lot and actually another place that
    came up just yesterday. Another practicum student asked me â€œIâ€™m doing this medical
    diagnostics project and my RÂ² is 0.95 for a disease which I was told is very hard
    to diagnose. Is this random forest genius or is something going wrong?â€ And I
    said remember, the second thing you do after you build a random forest is to do
    feature importance, so do feature importance and what youâ€™ll probably find is
    that the top column is something that shouldnâ€™t be there. So thatâ€™s what happened.
    He came back to me half an hour later, he said â€œyeah, I did the feature importance
    and you were right. The top column was basically a something that was another
    encoding of the dependent variable. Iâ€™ve removed it and now my RÂ² is -0.1 so thatâ€™s
    an improvement.â€
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'The other thing I like to look at is this chart [[50:03](https://youtu.be/BFIYUvBRTpE?t=50m3s)]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41a1947a7212c132120f08c6f75844e1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Basically it says where things flatten off in terms of which ones I should be
    really focusing on. So thatâ€™s the most important one. When I did credit scoring
    in telecommunications, I found there were nine variables that basically predicted
    very accurately who was going to end up paying for their phone and who wasnâ€™t.
    Apart from ending up with a model that saved them three billion dollars a year
    in fraud and credit costs, it also let them basically rejig their process so they
    focused on collecting those nine variables much better.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence [[50:46](https://youtu.be/BFIYUvBRTpE?t=50m46s)]
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an interesting one. Very important but in some ways kind of tricky to
    think about.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c504b3d97823d5561d3b15cb9cc0d860.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'Letâ€™s come back to how we calculate this in a moment, but the first thing to
    realize is that the vast majority of the time, when somebody shows you a chart
    , it will be like a univariate chart thatâ€™ll just grab the data from the database
    and theyâ€™ll plot X against Y. Then managers have a tendency to want to make a
    decision. So it would be â€œoh, thereâ€™s this drop-off here, so we should stop dealing
    in equipment made between 1990 and 1995\. This is a big problem because real world
    data has lots of these interactions going on. So maybe there was a recession going
    on around the time that those things are being sold or maybe around that time,
    people were buying more of a different type of equipment. So generally what we
    actually want to know is all other things being equal, whatâ€™s the relationship
    between YearMade and SalePrice. Because if you think about the drivetrain approach
    idea of the levers, you really want a model that says if I change this lever,
    how will it change my objective. Itâ€™s by pulling them apart using partial dependence
    that you can say actually this is the relationship between YearMade and SalePrice
    all other things being equal:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86606730a6d2d122fe044828ff5ab10d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: So how do we calculate that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Student: For the variable YearMade, for example, you keep all other variables
    constant. Then you are going to pass every single value of the YearMade, train
    the model after that. So for every model youâ€™ll have light blue lines and the
    median is going to be the yellow line.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: So letâ€™s try and draw that. By â€œleave everything else constantâ€, what
    she means is leave them at whatever they are in the dataset. So just like when
    we did feature importance, we are going to leave the rest of the dataset as it
    is. And weâ€™re going to do partial dependence plot for YearMade. So weâ€™ve got all
    of these other rows of data that we will just leave as they are. Instead of randomly
    shuffling YearMade, what we are going to do is replace every single value with
    exactly the same thing â€” 1960\. Just like before, we now pass that through our
    existing random forests which we have not retrained or changed in any way to get
    back out a set of predictions `y1960`. Then we can plot that on a chart â€” YearMade
    against partial dependence.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b378ac029f436b5689fbe13b0d97b704.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Now we can do that for 1961, 1962, 1963, and so forth. We can do that on average
    for all of them, or we could do it just for one of them. So when we do it for
    just one of them and we change its YearMade and pass that single thing through
    our model, that gives us one of these blue lines. So each one of these blue lines
    is a single row as we change its YearMade from 1960 up to 2008\. So then we can
    just take the median of all of these blue lines to say on average whatâ€™s the relationship
    between YearMade and price all other things being equal. Why is it that it works?
    Why is it that this process tells us the relationship between YearMade and price
    all other things being equal? Maybe itâ€™s good to think about a really simplified
    approach [[56:03](https://youtu.be/BFIYUvBRTpE?t=56m3s)]. A really simplified
    would say whatâ€™s the average auction? Whatâ€™s the average sale date, whatâ€™s the
    most common type of machine we well? Which location we mostly sell things? And
    we could come up with a single row that represents the average auction and then
    we could say okay, letâ€™s run that row through the random forest but replace its
    YearMade with 1960 and then do it again with 1961 and we could plot those on our
    little chart. That would give us a version of the relationship between YearMade
    and sale price all other things being equal. But what if tractors looked like
    that and backhoe loaders looked like a flat line:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1021d977252b177e379b73c06bbf5dc3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Then taking the average one would hide the fact that there are these totally
    different relationships. So instead, we basically say, okay our data tells us
    what kinds of things we tend to sell, who we tend to sell them, and when we tend
    to sell them, so letâ€™s use that. Then we actually find out for every blue line,
    here are actual examples of these relationships. So then what we can do is as
    well as plotting the median, we can do a cluster analysis to find out a few different
    shapes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9184d024b3bbfcbc3e1646e51ce21066.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: In this case, they all look pretty much the different versions of the same thing
    with different slopes, so my main takeaway from this would be that the relationship
    between sale price and year made is basically a straight line. And remember, this
    was a log of sale price so this is actually showing us an exponential. So this
    is where I would then bring in the domain expertise which is like â€œokay, things
    depreciate over time by a constant ratio so therefore, I would expect older stuff
    year made to have this exponential shape.â€ So this is where, as I mentioned, the
    very start of of my machine learning project, I generally try to avoid using as
    much domain expertise as I can and let the data do the talking. So one of the
    questions I got this morning was â€œthereâ€™s like a sale ID and model ID, I should
    throw those away, right? Because they are just IDs.â€ No. Donâ€™t assume anything
    about your data. Leave them in and if they turn out to be super important predictors,
    you want to find out why that is. But then, now Iâ€™m at the other end of my project.
    Iâ€™ve done my feature importance, Iâ€™ve pulled out the stuff which is from that
    dendrogram (i.e. redundant features), Iâ€™m looking at the partial dependence and
    now Iâ€™m thinking okay is this shape what I expected? So even better, before you
    plot this, first of all think what shape would I expect this to be. Because itâ€™s
    always easy to justify to yourself after the fact, oh, I knew it would look like
    this. So what shape you expect and then is it that shape? In this case, Iâ€™d say
    this is what I would expect. Where else the previous plot is not what Iâ€™d expect.
    So the partial dependence plot has really pulled out the underlying truth.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Say you have 20 features that are important, are you going to
    measure the partial dependence for every single one of them [[1:00:05](https://youtu.be/BFIYUvBRTpE?t=1h5s)]?
    If there are twenty features that are important, then I will do the partial dependence
    for all of them where important means like itâ€™s a lever I can actually pull, the
    magnitude of its size is not much smaller than the other nineteen, you know, based
    on all these things itâ€™s a feature I ought to care about then I will want to know
    how itâ€™s related. Itâ€™s pretty unusual to have that many features that are important
    both operationally and from a modeling point of view in my experience.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: How do you define importance [[1:00:58](https://youtu.be/BFIYUvBRTpE?t=1h58s)]?
    Important means itâ€™s a lever (i.e. something I can change) and itâ€™s on the spiky
    end of this tail (left):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53c920589028f6ed9ccd09b796e7c005.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Or maybe itâ€™s not a lever directly. Maybe itâ€™s like zip code and I canâ€™t actually
    tell my customers where to live but I could focus my new marketing attention on
    a different zip code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** Would it make sense to do pairwise shuffling for every combination
    of two features and hold everything else constant in feature importance to see
    interactions and compare scores [[1:01:45](https://youtu.be/BFIYUvBRTpE?t=1h1m45s)]?
    You wouldnâ€™t do that so much for partial dependence. I think your question is
    really getting to the question of could we do that for feature importance. I think
    interaction feature importance is a very important and interesting question. But
    doing it by randomly shuffling every pair of columns, if youâ€™ve got a hundred
    columns, it sounds computationally intensive, possibly infeasible. So what Iâ€™m
    going to do is after we talk about tree interpreter, Iâ€™ll talk about interesting
    but largely unexplored approach that will probably work.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Tree interpreter [[1:02:43](https://youtu.be/BFIYUvBRTpE?t=1h2m43s)]
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prince: I was thinking this to be more like feature importance, but feature
    importance is for complete random forest model, and this tree interpreter is for
    feature importance for particular observation. So letâ€™s say itâ€™a about hospital
    readmission. If a patient A is going to be readmitted to a hospital, which feature
    for that particular patient is going to impact and how can we change that. It
    is calculated starting from the prediction of mean then seeing how each feature
    is changing the behavior of that particular patient.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Iâ€™m smiling because that was one of the best examples of technical
    communication Iâ€™ve heard in a long time, so itâ€™s really good to think about why
    was that effective. So what Prince did there was, he used as specific an example
    as possible. Humans are much less good at understanding abstractions. So if you
    say â€œit takes some kind of feature, and then thereâ€™s an observation in that featureâ€
    whereas itâ€™s the hospital readmission. So we take a specific example. The other
    thing he did that was very effective was to take an analogy to something we already
    understand. So we already understand the idea of feature importance across all
    of the rows in a dataset. So now we are going to do it for a single row. So one
    of the things I was really hoping we would learn from this experience is how to
    become effective technical communicators. So that was a really great role model
    from Prince of using all the tricks we have at our disposal for effective technical
    communication. So hopefully you found that useful explanation. I donâ€™t have a
    lot to add to that other than to show you what that looks like.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'With the tree interpreter, we picked out a row [[1:04:56](https://youtu.be/BFIYUvBRTpE?t=1h4m56s)]:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d658c4c04011a022138d863c0783078f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Remember when we talked about the confidence intervals at the very start (i.e.
    the confidence based on tree variance). We said you mainly use that for a row.
    So this would also be for a row. So itâ€™s like â€œwhy is this patient likely to be
    readmitted?â€ Here is all the information we have about that patient or in this
    case this auction. Why is this auction so expensive? So then we call `ti.predict`
    and we get back the prediction of the price, the bias (i.e. the root of the tree
    â€” so this is just the average price for everybody so this is always going to be
    the same), and then the contributions which is how important is each of these
    things:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79b64347e1a6748da52bfcc6491345af.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: The way we calculated that was to say at the very start, the average price was
    10\. Then we split on enclosure. For those with this enclosure, the average was
    9.5\. Then we split on year made less than 1990 and for those with that year made,
    the average price was 9.7\. Then we split on the number of hours on the meter,
    and with this branch, we got 9.4.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a858ec9b9fe155abec57b59cf261650b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: We then have a particular auction which we pass it through the tree. It just
    so happens that it takes the top most path. One row can only have one path through
    the tree. So we ended up at 9.4\. Then we can create a little table. As we go
    through, we start at the top and we start with 10 â€” thatâ€™s our bias. And we said
    enclosure resulted in a change from 10 to 9.5 (i.e. -0.5). Year made changed it
    from 9.5 to 9.7 (i.e. +0.2), then meter changed it from 9.7 down to 9.4 (-0.3).
    Then if we add all that together (10â€“0.5+0.2â€“0.3), lo and behold thatâ€™s the prediction.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3429441276ef60a6ac385e04566fc25b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'Which takes us to our Excel spreadsheet [[1:08:07](https://youtu.be/BFIYUvBRTpE?t=1h8m7s)]:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8297d56c24cd16b7f6c634579eadb8ac.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Last week, we have use Excel for this because there wasnâ€™t a good Python library
    for doing waterfall charts. So we saw we got our starting point this is the bias,
    and then we had each of our contributions and we ended up with our total. The
    world is now a better place because Chris has created a Python waterfall chart
    module for us and put it on pip. So never again where we have to use Excel for
    this. I wanted to point out that waterfall charts have been very important in
    business communications at least as long as Iâ€™ve been in business â€” so thatâ€™s
    about 25 years. Python is maybe a couple of decades old. But despite that, no
    one in the Python world ever got to the point where they actually thought â€œyou
    know, Iâ€™m gonna make a waterfall chartâ€ so they didnâ€™t exist until two days ago
    which is to say the world is full of stuff which ought to exist and doesnâ€™t. And
    doesnâ€™t necessarily take a heck a lot of time to build. It took Chris about 8
    hours, so a hefty amount but not unreasonable. And now forevermore people when
    they want the Python waterfall chart will end up at Chrisâ€™ Github repo and hopefully
    find lots of other USF contributors who have made it even better.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In order for you to help improve Chrisâ€™ Python waterfall, you need to know how
    to do that. So you are going to need to submit a pull request. Life becomes very
    easy for submitting pull requests if you use something called [hub](https://hub.github.com/).
    What they suggest you do is that you alias `git` to `hub` because it turns out
    that hub is actually a strict superset of git. What it lets you do is you can
    go `git fork`, `git push` , and `git pull-request` and youâ€™ve now sent Chris a
    pull request. Without hub, this is actually a pain and requires like going to
    the website and filling in forms and stuff. So this gives you no reason not to
    do pull request. I mention this because when you are interviewing for a job, I
    can promise you that the person you are talking to will check your github and
    if they see you have a history of submitting thoughtful pull requests that are
    accepted to interesting libraries, that looks great. It looks great because it
    shows youâ€™re somebody who actually contributes. It also shows that if they are
    being accepted that you know how to create code that fits with peopleâ€™s coding
    standards, has appropriate documentation, passes their tests and coverage, and
    so forth. So when people look at you and they say oh, here is somebody with a
    history of successfully contributing, accepted pull requests to open-source libraries,
    thatâ€™s a great part of your portfolio. And you can specifically refer to it. So
    either Iâ€™m the person who build Python waterfall, here is my repo or Iâ€™m the person
    who contributed currency number formatting to Python waterfall, here is my pull
    request. Anytime you see something that doesnâ€™t work right in any open source
    software you use, it is not a problem, itâ€™s a great opportunity because you can
    fix it and send in the pull request. So give it a go. It actually feels great
    the first time you have a pull request accepted. And of course, one big opportunity
    is the fastai library. Thanks to one of our students, we now have docstrings for
    most of the `fastai.structured` library, again came via a pull request.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Does anybody have any questions about how to calculate any of these random forest
    interpretation methods or why we might want to use them [[1:12:50](https://youtu.be/BFIYUvBRTpE?t=1h12m50s)]?
    Towards the end of the week, youâ€™re going to need to be able to build all of these
    yourself from scratch.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** Just looking at the tree interpreter, I noticed that some of
    the values are `nan` â€™s. I get why you keep them in the tree but how can `nan`
    have a feature importance [[1:13:19](https://youtu.be/BFIYUvBRTpE?t=1h13m19s)]?
    Let me pass it back to you. Why not? So in other words, how is `nan` handled in
    Pandas and therefore in the tree? Does anybody remember, notice these are all
    in categorical variables, how does Pandas handle `nan` â€™s in categorical variable
    and how does fastai deal with them? Pandas sets them to -1 category code and fastai
    adds one to all of the category code so it ends up being zero. In other words,
    remember by the time it hits the random forest itâ€™s just a number, and itâ€™s just
    zero. And we map it back to the descriptions back here. So the question really
    is why shouldnâ€™t the random forest be able to split on zero? Itâ€™s just another
    number. So it could be `nan`, `high`, `medium`, `low`= 0, 1, 2, 3\. So missing
    values are one of these things that are generally taught really badly. Often people
    get taught here are some ways to remove columns with missing values or remove
    rows with missing values or to replace missing values. Thatâ€™s never what we want
    because missingness is very very very often interesting. So we actually learnt
    that from our feature importance that coupler system `nan` is one of the most
    important features. For some reason, well, I could guess, right? Coupler system
    `nan` presumably means this is a kind of industrial equipment that doesnâ€™t have
    a coupler system. Now I donâ€™t know what kind that is, but apparently itâ€™s more
    expensive kind.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: I did this competition for university grant research success where by far the
    most important predictors were whether or not some of the fields were null [[1:15:41](https://youtu.be/BFIYUvBRTpE?t=1h15m41s)].
    It turned out that this was data leakage that these fields only got filled in
    most of the time after a research grant was accepted. So it allowed me to win
    that Kaggle competition but didnâ€™t actually help the university very much.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Extrapolation [[1:16:16](https://youtu.be/BFIYUvBRTpE?t=1h16m16s)]
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am going to do something risky and dangerous which is we are going to do some
    live coding. The reason we are going to do some live coding is I want to explore
    extrapolation together with you, and I also want to give you a feel of how you
    might go about writing code quickly in this notebook environment. And this is
    the kind of stuff that you are going to need to be able to do in the real world
    and in the exam is quickly create the kind of code that we are going to talk about.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: I really like creating synthetic datasets anytime Iâ€™m trying to investigate
    the behavior of something because if I have a synthetic dataset, I know how it
    should behave.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Which reminds me, before we do this, I promised that we would talk about interaction
    importance and I just about forgot.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Interaction importance [[1:17:24](https://youtu.be/BFIYUvBRTpE?t=1h17m24s)]
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tree interpreter tells us the contributions for a particular row based on the
    difference in the tree. We could calculate that for every row in our dataset and
    add them up. That would tell us feature importance. And it would tell us feature
    importance in a different way. One way of doing feature importance is by shuffling
    the columns one at a time. Another way is by doing tree interpreter for every
    row and adding them up. Neither is more right than the others. They are actually
    both quite widely used so this is kind of type 1 and type 2 feature importance.
    So we could try to expand this a little bit. To do not just single variable feature
    importance, but interaction feature importance. Now here is the thing. What Iâ€™m
    going to describe is very easy to describe. It was described by Breiman right
    back when random forests were first invented, and it is part of the commercial
    software product from Salford systems who have the trademark on random forests.
    But it is not part of any open source library Iâ€™m aware of, and Iâ€™ve never seen
    an academic paper that actually studies it closely. So what Iâ€™m going to describe
    here is a huge opportunity but itâ€™s also like thereâ€™s lots and lots of details
    that need to be fleshed out. But here is the basic idea.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'This particular difference here (in red) is not just because of year made but
    because of a combination of year made and enclosure [[1:19:15](https://youtu.be/BFIYUvBRTpE?t=1h19m15s)]:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8090753aa1c338a5f3ea2b7050a3b638.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: The fact that this is 9.7 is because enclosure was in this branch and year made
    was in this branch. So in other words, we could say the contribution of enclosure
    interacted with year made is -0.3.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: So what about the difference between 9.5 and 9.4? Thatâ€™s an interaction of year
    made and hours on the meter. Iâ€™m using star here not to mean â€œtimesâ€ but to mean
    â€œinteracted withâ€. Itâ€™s a common way of doing things like Râ€™s formulas do it this
    way as well. So year made interacted with meter has a contribution of -0.1.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5603740765510c45ecc15dae33a8b302.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Perhaps we could also say from 10 to 9.4, this also shows an interaction between
    meter and enclosure with one thing in between them. So we could say meter interacted
    with enclosure equals â€¦and what should it be? Should it be -0.6? Some ways that
    seems unfair because we are also including the impact of year made. So maybe it
    should be -0.6 and maybe we should add back this 0.2 (9.5 â†’ 9.7). These are like
    details that I actually donâ€™t know the answer to. How should we best assign a
    contribution to each pair of variables in this path? But clearly conceptually
    we can. The pairs of variables in that path all represent interactions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Why donâ€™t you force them to be next to each other in the tree
    [[1:21:47](https://youtu.be/BFIYUvBRTpE?t=1h21m47s)]? Iâ€™m not going to say itâ€™s
    the wrong approach. I donâ€™t think itâ€™s the right approach though. Because it feels
    like in this path, meter and enclosure are interacting. So it seems like not recognizing
    that contribution is throwing away information. But Iâ€™m not sure. I had one of
    my staff at Kaggle actually do some R&D on this a few years ago and they actually
    found (I wasnâ€™t close enough to know how they dealt with these details), but they
    got it working pretty well. But unfortunately it never saw the light of day as
    a software product. But this is something maybe a group of you could get together
    and build. Do some googling to check, but I really donâ€™t think that there are
    any interaction feature importance parts of any open source library.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Wouldnâ€™t this exclude interactions though between variables that
    donâ€™t matter until they interact? So say your row never chooses to split down
    that path, but that variable interacting with another one becomes your most important
    split [[1:22:56](https://youtu.be/BFIYUvBRTpE?t=1h22m56s)]. I donâ€™t think that
    happens. Because if there is an interaction thatâ€™s important only because itâ€™s
    an interaction (and not in a univariate basis), it will appear sometimes, assuming
    that you set max features to less than one, so therefore it will appear in some
    path.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What is meant by interaction? Is it multiplication, ratio, addition
    [[1:23:31](https://youtu.be/BFIYUvBRTpE?t=1h23m31s)]? Interaction means appears
    on the same path through a tree. In the above example, there is an interaction
    between enclosure and year made because we branched on enclosure and then we branched
    on year made. So to get to 9.7, we have to have some specific value of enclosure
    and some specific value of year made.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What if you went down the middle leaves between the two things
    you are trying to observe and you would also take into account what the final
    measure is? I mean if we extend the tree downwards, youâ€™d have many measures both
    of like the two things you are trying to look at and also the in between steps.
    There seems to be a way to average information out in between them [[1:24:03](https://youtu.be/BFIYUvBRTpE?t=1h24m3s)]?
    There could be. I think what we should do is talk about this on the forum. I think
    this is fascinating and I hope we build something great, but I need to do my live
    coding. That was a great discussion. Keep thinking about it and do some experiments.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Back to Live Coding [[1:24:50](https://youtu.be/BFIYUvBRTpE?t=1h24m50s)]
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So to experiment with that, you almost certainly want to create a synthetic
    dataset first. Itâ€™s like `y = x1 + x2 + x1*x2` or something. Something where you
    know there is this interaction effect and there isnâ€™t that interaction effect,
    and you want to make sure that the feature importance you get at the end is what
    you expected.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: So probably the first step would be to do single variable feature importance
    using the tree interpreter style approach [[1:25:14](https://youtu.be/BFIYUvBRTpE?t=1h25m14s)].
    One nice thing about this is it doesnâ€™t really matter how much data you have.
    All you have to do to calculate feature importance is just slide through tree.
    So you should be able to write in a way thatâ€™s actually pretty fast, so even writing
    it in pure Python might be fast enough depending on your tree size.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: We are going to talk about extrapolation and the first thing I want to do is
    create a synthetic dataset that has a simple linear relationship. We are going
    to pretend itâ€™s like a time series. So we need to create some x values. The easiest
    way to create some synthetic data of this type is to use `linspace` which just
    creates some evenly spaced data between start and stop by default 50 observations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca0ed3c32e1a6203792b8b70f3443f5a.png)![](../Images/3dac629339adb81fe1a5a37cb80218c6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Then we are going to create dependent variable, so letâ€™s assume there is a linear
    relationship between x and y, and letâ€™s add a little bit of randomness to it.
    `random.uniform` between low and high, so we could add somewhere between -0.2
    and 0.2, for example.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b7e45c994e0e1ae89249fb403ae1f14.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: The next thing we need is a shape which is basically what dimensions do you
    want these random numbers to be, and obviously we want them to be the same shape
    as `x`â€™s shape. So we can just say `x.shape`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81e52dc3309b92046f4d778676efd307.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: So in other words, `(50,)` is `x.shape`. Remember when you see something in
    parentheses with a comma, thatâ€™s a tuple with just one thing in it. So this is
    shape 50 and so we added 50 random numbers. Now we can plot those.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d8302e82de69ddca4bb3797be2d700f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Alright, so there is our data. When you were both working as a data scientist
    or for doing your exams in this course, you need to be able to quickly whip up
    a dataset like that, throw it up in a plot without thinking too much. As you can
    see, you donâ€™t have to really remember much if anything. You just have to know
    how to hit `shift + tab` to check the names of parameters, google, or something
    to try and find `linspace` if you forgot what itâ€™s called.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: So letâ€™s assume thatâ€™s our data [[1:28:33](https://youtu.be/BFIYUvBRTpE?t=1h28m33s)].
    Weâ€™re now going to build a random forest model and what I want to do is build
    a random forest model that kind of acts as if this is a time series. So Iâ€™m going
    to take left part as a training set. And take the right part as our validation
    or test set just like we did in groceries or bulldozers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d033cab61b66800b28ef99b492696808.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'We can use exactly the same kind of code that we used in `split_vals`. So we
    can say:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That splits it into the first 40 versus the last 10\. We can do the same thing
    for y and there we go.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next thing to do is we want to create a random forest and fit it which requires
    x and y.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Thatâ€™s actually going to give an error and the reason why is that it expects
    x to be a matrix, not a vector, because it expects x to have a number of columns
    of data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: So itâ€™s important to know that a matrix with one column is not the same thing
    as a vector.
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So if I try to run this, â€œExpected 2D array, got 1D array insteadâ€:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c89a746ef5d8e8ce13f4a68661b7b219.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: So we need to convert 1D array into a 2D array. Remember I said `x.shape` is
    `(50,)`. So `x` has one axis and xâ€™s rank is 1\. The rank of a variable is equal
    to the length of itâ€™s shape â€” how many axes it has. Vector we can think of as
    an array of rank 1 and matrix as an array of rank 2\. I very rarely use words
    like vector and matrix because they are kind of meaningless â€” specific example
    of something more general which is they are all N dimensional tensors or N dimensional
    arrays. So an N dimensional array we can say itâ€™s a tensor of rank N. They basically
    mean kind of the same thing. Physicists get crazy when you say that because to
    a physicist, a tensor has quite a specific meaning but in machine learning, we
    generally use it in the same way.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: So how do we turn an one dimensional array into a two dimensional array. There
    are a couple of ways we can do it but basically we slice it. Colon (`:`) means
    give me everything in that axis. `:,None` means give me everything in the first
    axis (which is the only axis we have) and then `None` is a special indexer which
    means add a unit axis here. So let me show you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b3e1a4f1a69428b22ffae4d90a11b2f.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: That is of shape (50, 1), so itâ€™s a rank 2\. It has two axes. One of them is
    a very boring axis â€” itâ€™s a length one axis. So letâ€™s move `None` to the left.
    There is (1, 50). Then to remind you, the original is (50,).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d722ceac74a361f3381e5387efc8eed.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: So you can see I can put `None` as a special indexer to introduce a new unit
    axis there. So `x[None,:]` has one row and fifty columns. `x[:,None]` has fifty
    rows and one column â€” so thatâ€™s what we want. This kind of playing around with
    ranks and dimension is going to become increasingly important in this course and
    in the deep learning course. So spend a lot of time slicing with None, slicing
    with other things, try to create 3 dimensional, 4 dimensional tensors and so forth.
    Iâ€™ll show you two tricks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is you never ever need to write `,:` as itâ€™s always assumed. So these
    are exactly the same thing:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc83fc64ccc445d232a496199588fea8.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: And you see that in code all the time, so you need to recognize it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The second trick is `x[:,None]` is adding an axis in the second dimension (or
    I guess index 1 dimension). What if I always want to put it in the last dimension?
    Often our tensors change dimensions without us looking because you went from a
    one channel image to a three channel image, or you went from a single image to
    a mini batch of images. Suddenly, you get new dimensions appearing. So make things
    general, I would say `...` which means as many dimensions as you need to fill
    this up. So in this case (`x[â€¦, None].shape` ), itâ€™s exactly the same but I would
    always try to write it that way because it means itâ€™s going to continue to work
    as I get higher dimensional tensors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: So in this case, I want 50 rows and one column, so Iâ€™ll call that x1\. Letâ€™s
    now use that here and so this is now a 2D array and so I can create my random
    forest.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d4106b6e017f31faf7d4e51acb70f40.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Then I could plot that, and this is where youâ€™re going to have to turn your
    brains on because the folks this morning got this very quickly which was super
    impressive. Iâ€™m going to plot `y_trn` against `m.predict(x_trn)`. Before I hit
    go, what is this going to look like? It should basically be the same. Our predictions
    hopefully are the same as the actuals. So this should fall on a line but there
    is some randomness so it wonâ€™t quite.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fdc34fa62b66337b87a321dcc6b462d.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: That was the easy one. Letâ€™s now do the hard one, the fun one. What is that
    going to look like?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae6c6ba81817c96cae72133a62b0f9d2.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Think about what trees do and think about the fact that we have a validation
    set on the right and a training set on the left:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4f103d8c76f42587e6e7bfc91bdf6c1.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: So think about a forest is just a bunch of trees.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Tim: Iâ€™m guessing since all the new data is actually outside of the original
    scope, so itâ€™s all going to be basically the same â€” itâ€™s like one huge group [[1:37:15](https://youtu.be/BFIYUvBRTpE?t=1h37m15s)].'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Yeah, right. So forget the forest, letâ€™s create one tree. So we are
    probably going to split somewhere around here first, then split somewhere here,
    â€¦ So our final split is right most node. Our prediction, when we take one from
    validation set, so itâ€™s going to put that through the forest and end up predicting
    the right most average. It canâ€™t predict anything higher than that because there
    is nothing higher to average.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7438a2c65674d13192508a3766f65bed.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: So this is really important to realize a random forest is not magic. Itâ€™s just
    returning the average of nearby observations where nearby is kind of in this like
    â€œtree spaceâ€. So letâ€™s run it and see if Tim is right
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58db9125b11012055e02c3e95e438af1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Holy crap, thatâ€™s awful. If you donâ€™t know how random forests work then this
    is going to totally screw you. If you think that itâ€™s actually going to be able
    to extrapolate to any kind of data it hasnâ€™t seen before, particularly future
    time period, itâ€™s just not. It just canâ€™t. Itâ€™s just averaging stuff itâ€™s already
    seen. Thatâ€™s all it can do.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so we are going to be talking about how to avoid this problem. We talked
    a little bit in the last lesson about trying to avoid it by avoiding unnecessary
    time dependent variables where we can. But in the end, if you really have a time
    series that looks like this, we actually have to deal with a problem. One way
    we could deal with the problem would be use a neural net. Use something that actually
    has a function or shape that can actually fit something that actually has a function
    or shape that can actually fit something like this so it will extrapolate nicely:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cffc3e3e176bd6304c4199b1afbd8d7.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Another approach would be to use all the time series techniques you guys are
    learning about in the morning class to fit some kind of time series and then detrend
    it. Then youâ€™ll end up with detrended dots and then use the random forest to predict
    those. Thatâ€™s particularly cool because imagine what your random forest was actually
    trying to predict data which was two different states. So the blues ones are down
    there, and the red ones are up here.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75be04c6622f2e5ba4a7a089b9cfeba5.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: If you try to use a random forest, itâ€™s going to do a pretty crappy job because
    time is going to seem much more important. So itâ€™s basically still going to split
    like this and split like that, then finally once it gets down to left corner,
    it will be like â€œoh okay, now I can see the difference between the states.â€
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b12cb556d1d45e083ac139e7bd74100.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: In other words, when youâ€™ve got this big time piece going on, youâ€™re not going
    to see the other relationships in the random forest until every tree deals with
    time. So one way to fix this would be with a gradient boosting machine (GBM).
    What a GBM does is, it creates a little tree, and runs everything through that
    first little tree (which could be the time tree) then it calculates the residuals
    and the next little tree just predicts the residuals. So it would be kind of like
    detrending it, right? GBM still canâ€™t extrapolate to the future but at least they
    can deal with time-dependent data more conveniently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: We are going to be talking about this quite a lot more over the next coupe of
    weeks, and in the end that a solution is going to be just use neural nets. But
    for now, using some kind of time series analysis, detrend it, and then use random
    forest on that isnâ€™t a bad technique at all. If you are playing around something
    like Ecuador groceries competition, that would be a really good thing to fiddle
    around with.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
