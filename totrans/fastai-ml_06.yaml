- en: 'Machine Learning 1: Lesson 6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '[Video](https://youtu.be/BFIYUvBRTpE) / [Powerpoint](https://github.com/fastai/fastai/blob/master/courses/ml1/ppt/ml_applications.pptx)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We’ve looked at a lot of different random forest interpretation techniques and
    a question that has come up a little bit on the forum is what are these for really?
    How do these help me get a better score on Kaggle, and my answer has been “they
    don’t necessarily”. So I wanted to talk more about why we do machine learning.
    What’s the point? To answer this question, I want to show you something really
    important which is examples of how people have used machine learning mainly in
    business because that’s where most of you are probably going to end up after this
    is working for some company. I’m going to show you applications of machine learning
    which are either based on things that I’ve been personally involved in myself
    or know of people who are doing them directly so none of these are going to be
    hypotheticals — these are all actual things that people are doing and I’ve got
    direct or secondhand knowledge of.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Two Groups of Applications [[1:26](https://youtu.be/BFIYUvBRTpE?t=1m26s)]
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Horizontal: In business, horizontal means something that you do across different
    kinds of business. i.e. everything involving marketing.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vertical: Something you do within a business or within a supply chain or a
    process.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal Applications
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pretty much every company has to try to sell more products to its customers
    so therefore does marketing. So each of these boxes are examples of some of the
    things that people are using machine learning for in marketing:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/dd65548e467e8e98626f519b1415255e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Let’s take an example — Churn. Churn refers to a model which attempts to predict
    who’s going to leave. I’ve done some churn modeling fairly recently in telecommunications.
    We were trying to figure out for this big cellphone company which customers are
    going to leave. That is not of itself that interesting. Building a highly predictive
    model that says Jeremy Howard is almost certainly going to leave next month is
    probably not that helpful because if I’m almost certainly going to leave net month,
    there’s probably nothing you can do about it — it’s too late and it’s going to
    cost you too much to keep me.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'So in order to understand why we would do churn modeling, I’ve got a little
    framework that you might find helpful: [Designing great data products](https://www.oreilly.com/ideas/drivetrain-approach-data-products).
    I wrote it with a couple of colleagues a few years ago and in it, I describe my
    experience of actually turning machine learning models into stuff that makes money.
    The basic trick is what I call the **Drivetrain Approach** which is these four
    steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a6dbaec77fe8d5f4fb87eee227326a3e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Defined Objective [[3:48](https://youtu.be/BFIYUvBRTpE?t=3m48s)]
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The starting point to actually turn a machine learning project into something
    that’s actually useful is to know what I am trying to achieve and that does mean
    I’m trying to achieve a high area under the ROC curve or trying to achieve a large
    difference between classes. It would be I’m trying to sell more books or I’m trying
    to reduce the number of customers that leave next month or I’m trying to detect
    lung cancer earlier. These are objectives. So the objective is something that
    absolutely directly is the thing that the company or the organization actually
    wants. No company or organization lives in order to create a more accurate predictive
    model. There are some reason. So that’s your objective. That’s obviously the most
    important thing. If you don’t know the purpose of what you are modeling for then
    you can’t possibly do a good job of it. And hopefully people are starting to pick
    that up out there in the world of data science, but interestingly what very few
    people are talking about but it’s just as important is the next thing which is
    levers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习项目转化为实际有用的起点是知道我试图实现什么，这意味着我试图实现高ROC曲线下面积或尝试实现类之间的巨大差异。这可能是我试图销售更多的书，或者我试图减少下个月离开的客户数量，或者我试图更早地检测肺癌。这些都是目标。因此，目标是公司或组织实际想要的东西。没有公司或组织是为了创建更准确的预测模型而存在的。这是有原因的。所以这就是你的目标。显然，这是最重要的事情。如果你不知道你为何建模，那么你不可能做好这项工作。希望人们开始在数据科学领域意识到这一点，但有趣的是，很少有人谈论但同样重要的是下一步，即杠杆。
- en: Levers [[5:04](https://youtu.be/BFIYUvBRTpE?t=5m4s)]
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杠杆[[5:04](https://youtu.be/BFIYUvBRTpE?t=5m4s)]
- en: A lever is a thing that the organization can do to actually drive the objective.
    So let’s take the example of churn modeling. What is a lever that an organization
    could use to reduce the number of customers that are leaving? They could call
    someone and say “Are you happy? Anything we could do?” They could give them a
    free pen or something if they buy $20 worth of product next month. You could give
    them specials. So these are levers. Whenever you are working as a data scientists,
    keep coming back and thinking what are we trying to achieve (we being the organization)
    and how we are trying to achieve it being what are the actual things we can do
    to make that objective happen. So building a model is never ever a lever, but
    it could help you with the lever.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 杠杆是组织可以实际采取的行动，以推动目标的实现。所以让我们以流失建模为例。组织可以采取什么杠杆来减少离开的客户数量？他们可以打电话给某人，问：“你满意吗？我们能做些什么？”他们可以在下个月购买价值20美元的产品时赠送免费的钢笔或其他物品。你可以给他们提供特别优惠。所以这些就是杠杆。当你作为数据科学家工作时，不断回头思考我们试图实现什么（我们指的是组织），以及我们如何实现它，即我们可以做哪些实际的事情来实现这个目标。因此，构建模型绝对不是杠杆，但它可以帮助你使用杠杆。
- en: Data [[7:01](https://youtu.be/BFIYUvBRTpE?t=7m1s)]
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据[[7:01](https://youtu.be/BFIYUvBRTpE?t=7m1s)]
- en: So then the next step is what data does the organization have that could possibly
    help them to set that lever to achieve that objective. So this is not what data
    did they give you when you started the project. But think about it from a first
    principle’s point of view — okay, I’m working for a telecommunications company,
    they gave me some certain set of data, but I’m sure they must know where their
    customers live, how many phone calls they made last month, how many times they
    called customer service, etc. So have a think about okay if we are trying to decide
    who should we give a special offer to proactively, then we want to figure out
    what information do we have that might help us to identify who’s going to react
    well or badly to that. Perhaps more interestingly would be what if we were doing
    a fraud algorithm. So we are trying to figure out who’s going to not pay for the
    phone that they take out of the store, they are on some 12-month payment plan,
    and we never see them again. Now in that case, the data we have available , it
    doesn’t matter what’s in the database, what matters is what’s the data that we
    can get when the customer is in the shop. So there’s often constraints around
    the data that we can actually use. So we need to know what am I trying to achieve,
    what can this organization actually do specifically to change the outcome, and
    at the point that the decision is being made, what data do they have or could
    they collect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是组织拥有哪些数据可能帮助他们设置杠杆以实现目标。这不是指他们在项目开始时给你的数据。而是从第一原则的角度考虑——好吧，我在一家电信公司工作，他们给了我一些特定的数据，但我肯定他们必须知道他们的客户住在哪里，上个月打了多少电话，打了多少次客服电话等等。所以想一想，如果我们试图决定主动给谁提供特别优惠，那么我们想要弄清楚我们有哪些信息可能帮助我们确定谁会对此做出积极或消极的反应。也许更有趣的是，如果我们正在进行欺诈算法。我们试图弄清楚谁不会支付他们从商店拿出的手机，他们正在进行某种12个月的付款计划，然后我们再也没有见到他们。在这种情况下，我们可以获得的数据，数据库中有什么并不重要，重要的是当客户在商店时我们可以获得什么数据。因此，我们通常会受到我们实际可以使用的数据的限制。因此，我们需要知道我试图实现什么目标，这个组织实际上可以具体做些什么来改变结果，以及在做出决定时，他们拥有或可以收集到哪些数据。
- en: Models [[8:45](https://youtu.be/BFIYUvBRTpE?t=8m45s)]
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型[[8:45](https://youtu.be/BFIYUvBRTpE?t=8m45s)]
- en: So then the way I put that all together is with a model. This is not a model
    in the sense of a predictive model but it’s a model in the sense of a simulation
    model. So one of the main example I gave in this paper is when I spent many years
    building which is if an insurance company changes their prices, how does that
    impact their profitability. So generally your simulation model contains a number
    of predictive models. So I had, for example, a predictive model called an elasticity
    model that said for a specific customer, if we charge them a specific price for
    a specific product, what’s the probability that they would say yes both when it’s
    new business and then a year later what’s the probability that they’ll renew.
    Then there’s another predictive model which is what’s the probability that they
    are going to make a claim and how much is that claim going to be. You can then
    combine these models together then to say all right, if we changed our pricing
    by reducing it by 10% for everybody between 18 and 25 and we can run it through
    these models that combined together into a simulation then the overall impact
    on our market share in 10 years time is X and our cost is Y and our profit is
    Z and so forth.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我把所有这些放在一起的方式是通过一个模型。这不是一个预测模型，而是一个模拟模型。我在这篇论文中给出的一个主要例子是，我花了很多年时间建立的一个模型，即如果一个保险公司改变他们的价格，这将如何影响他们的盈利能力。通常你的模拟模型包含了许多预测模型。比如，我有一个叫做弹性模型的预测模型，它说对于一个特定的客户，如果我们为他们的某个产品收取一个特定的价格，他们会在新业务时和一年后续保的概率是多少。然后还有另一个预测模型，即他们会提出索赔的概率以及索赔金额是多少。然后你可以将这些模型结合起来，然后说好，如果我们将我们的定价降低10%适用于18到25岁的所有人，然后我们可以通过这些模型运行，将它们结合成一个模拟，那么我们在10年后的市场份额的整体影响是X，我们的成本是Y，我们的利润是Z等等。
- en: In practice, most of the time, you really are going to care more about the results
    of that simulation than you do about the predictive model directly. But most people
    are not doing this effectively at the moment. For example, when I go to Amazon,
    I read all of Douglas Adams’ books, and so having read all Douglas Adams’ books,
    the next time I went to Amazon they said would you like to buy the collected works
    of Douglas Adams. This is after I had bought every one of his books. So from a
    machine learning point of view, some data scientist had said oh people that buy
    one of Douglas Adams’ books often go on to buy the collected works. But recommending
    to me that I buy the collected works of Douglas Adams isn’t smart. It’s actually
    not smart at a number of levels. Not only is unlikely to buy a box set of something
    of which I have every one individually but furthermore it’s not going to change
    my buying behavior. I already know about Douglas Adams. I already know I like
    him, so taking up your valuable web space to tell me hey maybe you should buy
    more of the author who you’re already familiar with and bought lots of times isn’t
    actually going to change my behavior. So what if instead of creating a predictive
    model, Amazon had built an optimization model that could simulate and said if
    we show Jeremy this ad, how likely is he then to go on to buy this book and if
    I don’t show him this ad, how likely is he to go on to buy this book. So that’s
    the counterfactual. The counter factual is what would have happened otherwise,
    and then you can take the difference and say what should we recommend him that
    is going to maximally change his behavior. So maximally result in more books and
    so you’d probably say oh he’s never bought any Terry Pratchett book, he probably
    doesn’t know about Terry Pratchett but lots of people that liked Douglas Adams
    did turn out to like Terry Pratchett so let’s introduce him to a new author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数时候，你真的更关心那个模拟的结果，而不是直接关心预测模型。但大多数人目前并没有有效地做到这一点。例如，当我去亚马逊时，我读了道格拉斯·亚当斯的所有书，所以在我读完所有道格拉斯·亚当斯的书之后，下次我去亚马逊，他们说你想买道格拉斯·亚当斯的全部作品吗。这是在我已经买了他的每一本书之后。从机器学习的角度来看，一些数据科学家可能会说，购买道格拉斯·亚当斯的一本书的人通常会继续购买他的全部作品。但向我推荐购买道格拉斯·亚当斯的全部作品并不明智。这实际上在很多方面都不明智。不仅是因为我不太可能购买一个我已经有每一本书的合集，而且这也不会改变我的购买行为。我已经了解道格拉斯·亚当斯，我已经知道我喜欢他，所以占用你宝贵的网页空间来告诉我，嘿，也许你应该购买更多你已经熟悉并多次购买的作者的作品实际上不会改变我的行为。那么，如果亚马逊不是创建一个预测模型，而是建立一个能够模拟的优化模型，然后说如果我们向杰里米展示这个广告，他会有多大可能继续购买这本书，如果我不向他展示这个广告，他会有多大可能继续购买这本书。这就是对立事实。对立事实是否则会发生什么，然后你可以计算差异，然后说我们应该推荐他什么才能最大程度地改变他的行为。所以最大程度地导致更多的书籍，所以你可能会说，哦，他从来没有买过特里·普拉切特的书，他可能不了解特里·普拉切特，但很多喜欢道格拉斯·亚当斯的人确实喜欢特里·普拉切特，所以让我们向他介绍一个新的作者。
- en: So it’s the difference between a predictive model on the one hand versus an
    optimization model on the other hand. So the two tend to go hand in hand. First
    of all we have a simulation model. The simulation model is saying in the world
    where we put Terry Pratchett’s book on the front page of Amazon for Jeremy Howard,
    this is what would have happened. He would have bought it with a 94% probability.
    That then tells us with this lever of what do I put on my homepage for Jeremy
    today, we say okay the different settings of that lever that put Terry Pratchett
    on the homepage has the highest simulated outcome. Then that’s the thing which
    maximizes our profit from Jeremy’s visit to amazon.com today.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一方面是预测模型，另一方面是优化模型之间的区别。所以这两者往往是相辅相成的。首先，我们有一个模拟模型。模拟模型是在说，如果我们把特里·普拉切特的书放在亚马逊的首页上给杰里米·霍华德看，会发生什么。他有94%的概率会购买。这告诉我们，通过这个杠杆，我今天应该在杰里米的首页上放什么，我们说好，把特里·普拉切特放在首页上的不同设置会产生最高的模拟结果。然后这就是最大化我们从杰里米今天访问亚马逊网站中的利润的事情。
- en: Generally speaking, your predictive models feed into this simulation model but
    you kind of have to think about how they all work together. For example, let’s
    go back to churn. So it turned out that Jeremy Howard is very likely to leave
    his cell phone company next month. What are we going to about it? Let’s call him.
    And I can tell you if my cell phone company calls me right now and says “just
    calling to say we love you” I’d be like I’m cancelling right now. That would be
    a terrible idea. So again, you would want a simulation model that says what’s
    the probability that Jeremy is going to change his behavior as a result of calling
    him right now. So one of the levers I have is call him. On the other hand, if
    I got a piece of mail tomorrow that said for each month you stay with us, we’re
    going to give you a hundred thousand dollars. Then that’s going to definitely
    change my behavior, right? But then feeding that into the simulation model, it
    turns out that overall that would be an unprofitable choice to make. Do you see
    how this fits in together?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你的预测模型会输入到这个模拟模型中，但你必须考虑它们如何共同工作。例如，让我们回到流失问题。结果表明，Jeremy Howard很可能会在下个月离开他的手机公司。我们该怎么办？让我们给他打电话。我可以告诉你，如果我的手机公司现在给我打电话说“只是打电话告诉你我们爱你”，我会立刻取消。那将是一个糟糕的主意。因此，你会想要一个模拟模型，来说Jeremy现在接到电话后改变行为的概率是多少。所以我有一个杠杆是给他打电话。另一方面，如果明天我收到一封信，说每个月你和我们在一起，我们会给你十万美元。那肯定会改变我的行为，对吧？但是将这个输入到模拟模型中，结果是这将是一个不盈利的选择。你看到这是如何相互配合的吗？
- en: So when we look at something like churn, we want to be thinking what are the
    levers we can pull [[14:33](https://youtu.be/BFIYUvBRTpE?t=14m33s)]. What are
    the kinds of models that we could build with what kinds of data to help us pull
    those levers better to achieve our objectives. When you think about it that way,
    you realize that the vast majority of these applications are not largely about
    a predictive model at all. They are about interpretation. They are about understanding
    what happens if. So if we take the intersection between on the one hand, here
    are all the levers that we could pull (here are all the things we can do) and
    then here are all of the features from our random forest feature importance that
    turn out to be strong drivers of the outcome. So then the intersection of those
    is here are the levers we could pull that actually matter. Because if you can’t
    change the thing, that is not very interesting. And if it’s not actually a significant
    driver, it’s not very interesting. So we can actually use our random forest feature
    importance to tell us what can we actually do to make a difference. Then we can
    use the partial dependence to actually build this kind of simulation model to
    say okay if we did change that, what would happen.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们看流失这样的问题时，我们要考虑我们可以拉动的杠杆是什么。我们可以用什么样的数据构建什么样的模型来帮助我们更好地拉动这些杠杆以实现我们的目标。当你这样思考时，你会意识到这些应用的绝大部分实际上并不是关于预测模型。它们是关于解释的。它们是关于理解“如果发生了什么”。因此，我们可以实际使用我们的随机森林特征重要性告诉我们我们实际上可以做些什么来产生影响。然后我们可以使用部分依赖来构建这种模拟模型，来说如果我们改变了那个，会发生什么。
- en: So there are lots of examples and what I want you to think about as you think
    about the machine learning problems you are working on is why does somebody care
    about this [[16:02](https://youtu.be/BFIYUvBRTpE?t=16m2s)]. What would a good
    answer to them look like and how could you actually positively impact this business.
    So if you are creating a Kaggle kernel, try to think about from the point of view
    of the competition organizer. What would they want to know and how can you give
    them that information. So something like fraud detection on the other hand, you
    probably just basically want to know whose fraudulent. So you probably do just
    care about the predictive model. But then you do have to think carefully about
    the data availability here. So okay, we need to know who is fraudulent at the
    point that we are about to deliver them a product. So it’s no point looking at
    data that’s available a month later, for instance. So you have this key issue
    of thinking about the actual operational constraints that you are working under.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有很多例子，当你思考你正在处理的机器学习问题时，我希望你考虑为什么有人会关心这个问题。对他们来说一个好的答案是什么样的，你如何实际上对这个业务产生积极影响。所以如果你在创建一个Kaggle内核，试着从竞赛组织者的角度思考。他们想知道什么，你如何给他们这些信息。另一方面，像欺诈检测，你可能只是想知道谁是欺诈的。所以你可能只关心预测模型。但是你必须仔细考虑这里的数据可用性。所以好吧，我们需要知道在我们即将向他们交付产品时谁是欺诈的。例如，查看一个月后可用的数据是没有意义的。所以你必须考虑你正在工作的实际运营约束。
- en: Human Resources Applications [[17:17](https://youtu.be/BFIYUvBRTpE?t=17m17s)]
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人力资源应用。
- en: Lots of interesting application in human resources but like employee churn,
    it’s another kind of churn model where finding out that Jeremy Howard is sick
    of lecturing, he’s going to leave tomorrow. What are you going to do about it?
    Well, knowing that wouldn’t actually be helpful. It would be too late. You would
    actually want a model that said what kinds of people are leaving USF and it turns
    out that everybody that goes to the downstairs cafe leaves USF. I guess their
    food is awful or whatever. Or everybody that we are paying less than half a million
    dollars a year is leaving USF because they can’t afford basic housing in San Francisco.
    So you could use your employee churn model not so much to say which employees
    hate us but why do employees leave. Again it’s really the interpretation there
    that matters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: For churn model, it sounds like there are two predictors that
    you need to predict for — one being churn and the other you need to optimize your
    profit. So how does it work [[18:30](https://youtu.be/BFIYUvBRTpE?t=18m30s)]?
    Yes, exactly. So this is what the simulation model is all about. You figure out
    this objective we are trying to maximize which is company profitability. You can
    create a pretty simple Excel model or something that says here is the revenue
    and here is the costs and the cost is equal to the number of people we employ
    multiplied by their salary, etc. Inside that Excel model, there are certain cells/inputs
    that are kind of stochastic or uncertain. But we could predict it with a model
    and so that’s what I do then is to say okay we need a predictive model for how
    likely somebody is to stay if we change their salary, how likely they are to leave
    with the current salary, how likely they are to leave next year if I increased
    their salary now, etc. So you a bunch of different models and then you can bind
    them together with simple business logic and then you can optimize that. You can
    then say okay if I pay Jeremy Howard half a million dollars, that’s probably a
    really good idea and if I pay him less then it’s probably not or whatever. You
    can figure out the overall impact. So it’s really shocking to me how few people
    do this. But most people in industry measure their models using AUC or RMSE or
    whatever which is never actually what you really want.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: More Horizontal Applications…[[22:04](https://youtu.be/BFIYUvBRTpE?t=22m4s)]
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lead prioritization is a really interesting one. Every one of these boxes I’m
    showing, you can generally find a company or many companies whose sole job in
    life is to build models of that thing. So there are lots of companies that sell
    lead prioritization systems but again the question is how would we use that information.
    So if it’s like our best lead is Jeremy, he is a highest probability of buying.
    Does that mean I should send a salesperson out to Jeremy or I shouldn’t? If he’s
    highly probable to buy, why I waste my time with him. So again, you really want
    some kind of simulation that says what’s the likely change in Jeremy’s behavior
    if I send my best salesperson out to go and encourage him to sign. I think there
    are many many opportunities for data scientists in the world today to move beyond
    predictive modeling to actually bringing it all together.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Vertical Applications [[23:29](https://youtu.be/BFIYUvBRTpE?t=23m29s)]
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As well as these horizontal applications that basically apply to every company,
    there’s a whole bunch of applications that are specific to every part of the world.
    For those of you that end up in healthcare, some of you will become experts in
    one or more of these areas. Like readmission risk. So what’s the probability that
    this patient is going to come back to the hospital. Depending on the details of
    the jurisdiction, it can be a disaster for hospitals when somebody is readmitted.
    If you find out that this patient has a high probability of readmission, what
    do you do about it? Again, the predictive model is helpful of itself. It rather
    suggests we shouldn’t send them home yet because they are going to come back.
    But wouldn’t it be nice if we had the tree interpreter and it said to us the reason
    that they are at high risk is because we don’t have a recent EKG/ECG for them.
    Without a recent EKG, we can’t have a high confidence about their cardiac health.
    In which case, it wouldn’t be like let’s keep them in the hospital for two weeks,
    it’ll be let’s give them an EKG. So this is interaction between interpretation
    and predictive accuracy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: So what I’m understanding you are saying is that the predictive
    models are a really great but in order to actually answer these questions, we
    really need to focus on the interpretability of these models [[24:59](https://youtu.be/BFIYUvBRTpE?t=24m59s)]?
    Yeah, I think so. More specifically I’m saying we just learnt a whole raft of
    random forest interpretation techniques and so I’m trying to justify why. The
    reason why is because I’d say most of the time the interpretation is the thing
    we care about. You can create a chart or a table without machine learning and
    indeed that’s how most of the world works. Most managers build all kinds of tables
    and charts without any machine learning behind them. But they often make terrible
    decisions because they don’t know the feature importance of the objective they
    are interested in and so the table they create is of things that actually are
    the least important things anyway. Or they just do a univariate chart rather than
    a partial dependence plot, so they don’t actually realize that the relationship
    they thought they are looking at is due entirely to something else. So I’m kind
    of arguing for data scientists getting much more deeply involved in strategy and
    in trying to use machine learning to really help a business with all of its objectives.
    There are companies like dunnhumby which is a huge company that does nothing but
    retail application with machine learning. I believe there’s like a dunnhumby product
    you can buy which will help you figure out if I put my new store in this location
    versus that location, how many people are going to shop there. Or if I put my
    diapers in this part of the shop versus that part of the shop, how is that going
    to impact purchasing behavior, etc. So it’s also good to realize that the subset
    of machine learning applications you tend to hear about in the tech press or whatever
    is this massively biased tiny subset of stuff which Google and Facebook do. Where
    else the vast majority of stuff that actually makes the world go around is these
    kinds of applications that actually help people make things, buy things, sell
    things, build things, so forth.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: About tree interpretation, we looked at which feature was more
    important for a particular observation. For businesses, they have a huge amount
    of data and they want this interpretation for a lot of observations so how do
    they automate it? Do they set threshold [[27:50](https://youtu.be/BFIYUvBRTpE?t=27m50s)]?
    The vast majority of machine learning models don’t automate anything. They are
    designed to provide information to humans. So for example, if you are a customer
    service phone operator for an insurance company and your customer asks you why
    is my renewal $500 more expensive than last time, then hopefully the insurance
    company provides in your terminal those little screen that shows the result of
    the tree interpreter or whatever. So you can jump there and tell the customer
    that last year you were in this different zip code which has lower amounts of
    car theft, and this year also you’ve actually changed your vehicle to more expensive
    one. So it’s not so much about thresholds and automation, but about making these
    model outputs available to the decision makers in the organization whether they
    be at the top strategic level of like are we going to shutdown this whole product
    or not, all the way to the operational level of that individual discussion with
    a customer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: So another example is aircraft scheduling and gate management. There’s lots
    of companies that do that and basically what happens is that there are people
    at an airport whose job it is to basically tell each aircraft what gate to go
    to, to figure out when to close the doors, stuff like that. So the idea is you’re
    giving them software which has the information they need to make good decisions.
    So the machine learning models end up embedded in that software to say okay that
    plane that’s currently coming in from Miami, there’s a 48% chance that it’s going
    to be over 5 minutes late and if it does then this is going to be the knock-on
    impact through the rest of the terminal, for instance. So that’s how these things
    fit together.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Other applications [[31:02](https://youtu.be/BFIYUvBRTpE?t=31m2s)]
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![](../Images/92404da0b0fdf953b4c60b9f1a454c48.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: There are lots of applications, and what I want you to do is to spend some time
    thinking about them. Sit down with one of your friends and talk about a few examples.
    For example, how would we go about doing failure analysis in manufacturing, who
    would be doing that, why would they be doing it, what kind of models might they
    use, what kind of data might they use. Start to practice and get a sense. Then
    when you’re at the workplace and talking to managers, you want to be straightaway
    able to recognize that the person you are talking to — what are they trying to
    achieve, what are the levers they have to pull, what are the data they have available
    to pull those levers to achieve that thing, and therefore how could we build models
    to help them do that and what kind of predictions would they have to be making.
    So then you can have this really thoughtful empathetic conversation with those
    people and then saying “in order to reduce the number of customers that are leaving,
    I guess you are trying to figure out who should you be providing better pricing
    to” and so forth.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Are explanatory problems people are faced with in social sciences
    something machine learning can be useful for or is used for or is that nor really
    the realm that’s in [[32:29](https://youtu.be/BFIYUvBRTpE?t=32m29s)]? I’ve had
    a lot of conversations about this with people in social sciences and currently
    machine learning is not well applied in economics or psychology or whatever on
    the whole. But I’m convinced it can be for the exact reasons we are talking about.
    So if you are going to try to do some kind of behavioral economics and you’re
    trying to understand why some people behave differently to other people, a random
    forest with a feature importance plot would be a great way to start. More interestingly,
    if you are trying to do some kind of sociology experiment or analysis based on
    a large social network dataset where you have an observational study, you really
    want to try and pull out all of the sources of exogenous variables (i.e. all the
    stuff that’s going on outside) so if you use a partial dependence plot with a
    random forest that happens automatically. I actually gave a talk at MIT a couple
    of years ago for the first conference on digital experimentation which was really
    talking about how do we experiment in things like social networks in these digital
    environments and economists all do things with classic statistical tests but in
    this case, the economists I talked to were absolutely fascinated by this and they
    actually asked me to give an introduction to machine learning session at MIT to
    these various faculty and graduate folks in the economics department. And some
    of those folks have gone on to write some pretty famous books and so hopefully
    it’s been useful. It’s definitely early days but it’s a big opportunity. But as
    Yannet says, there’s plenty of skepticism still out there. The skepticism comes
    from unfamiliarity basically with this totally different approach. So if you spent
    20 years studying econometrics and somebody comes along and says here is a totally
    different approach to all the stuff econometricians do, naturally your first reaction
    will be “prove it”. So that’s fair enough but I think over time the next generation
    of people who are growing up with machine learning, some of them will move into
    the social sciences, they’ll make huge impacts that nobody has ever managed to
    make before and people will start going wow. Just like happened in computer vision.
    When computer vision spent a long time of people saying “maybe you should use
    deep learning for computer vision” and everybody in computer vision said “Prove
    it. We have decades of work on amazing feature detectors for computer vision.”
    And then finally in 2012, Hinton and Kryzanski came along and said “our model
    is twice as good as yours and we’ve only just started on this” and everybody was
    convinced. Nowadays every computer vision researchers basically uses deep learning.
    So I think that time will come in this area too.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：社会科学中人们面临的解释问题是否可以使用机器学习或者已经被使用，或者这并不是真正的领域[[32:29](https://youtu.be/BFIYUvBRTpE?t=32m29s)]？我与社会科学领域的人们进行了很多关于这个问题的讨论，目前机器学习在经济学或心理学等领域并没有得到很好的应用。但我相信它可以，原因正如我们所讨论的那样。因此，如果您要尝试进行某种行为经济学研究，并且试图理解为什么有些人的行为与其他人不同，使用具有特征重要性图的随机森林将是一个很好的开始。更有趣的是，如果您尝试进行某种基于大型社交网络数据集的社会学实验或分析，在那里您进行了一项观察性研究，您真的想要尝试提取所有外生变量的来源（即所有外部发生的事情），因此如果您使用具有随机森林的部分依赖图，这将自动发生。几年前，我在麻省理工学院做了一个关于数字实验的第一次会议的演讲，这次会议真正讨论了我们如何在诸如社交网络等数字环境中进行实验，经济学家们都使用经典的统计检验方法，但在这种情况下，我与之交谈的经济学家们对此非常着迷，他们实际上要求我在麻省理工学院为经济学系的各种教员和研究生们举办一个机器学习入门课程。其中一些人已经写了一些相当有名的书籍，希望这对他们有所帮助。现在还处于早期阶段，但这是一个巨大的机会。但正如Yannet所说，仍然存在很多怀疑。这种怀疑主要来自对这种完全不同方法的陌生感。因此，如果您花了20年时间研究计量经济学，然后有人过来说这是一种完全不同于计量经济学家所做的所有事情的方法，那么您的第一反应自然会是“证明它”。这是公平的，但我认为随着时间的推移，下一代与机器学习一起成长的人们中，一些人将进入社会科学领域，他们将产生前所未有的巨大影响，人们将开始感到惊讶。就像计算机视觉中发生的一样。当计算机视觉花了很长时间的人们说“也许你应该使用深度学习来进行计算机视觉”，而计算机视觉领域的每个人都说“证明它。我们在计算机视觉中有几十年的工作，开发了令人惊叹的特征检测器。”然后在2012年，辛顿和克里赞斯基出现了，他们说“我们的模型比你们的好两倍，而我们刚刚开始”
    ，每个人都被说服了。如今，几乎每个计算机视觉研究人员基本上都使用深度学习。因此，我认为在这个领域也会出现这样的时刻。'
- en: Different random forest interpretation methods [[37:17](https://youtu.be/BFIYUvBRTpE?t=37m17s)]
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林解释方法[[37:17](https://youtu.be/BFIYUvBRTpE?t=37m17s)]
- en: Having talked about why they are important, let’s now remind ourselves what
    they are.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论它们为什么重要之后，让我们现在提醒自己它们是什么。
- en: '**Confidence based on tree variance**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**基于树方差的置信度**'
- en: What does it tell us? Why would be interested in that? How is it calculated?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们什么？为什么我们对此感兴趣？它是如何计算的？
- en: The variance of the predictions of the trees. Normally the prediction is just
    the average, this is variance of the trees.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 树的预测方差。通常预测只是平均值，这是树的方差。
- en: Just to fill in a detail here, what we generally do here is we take just one
    row/observation often and find out how confident we are about that (i.e. how much
    variance there are in the trees for that) or we can do as we did here for different
    groups [[39:34](https://youtu.be/BFIYUvBRTpE?t=39m34s)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里填充一个细节，我们通常只取一行/观察结果，然后找出我们对此有多自信（即树中有多少方差）或者我们可以像我们在这里做的那样为不同的组找出答案[[39:34](https://youtu.be/BFIYUvBRTpE?t=39m34s)]。
- en: '![](../Images/59eb1771fe8d8082cee64809651cfd10.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59eb1771fe8d8082cee64809651cfd10.png)'
- en: What we’ve done here is to say if there are any groups that we are very unconfident
    (which could be due to very little observations). Something that I think is even
    more important would be when you are using this operationally. Let’s say you are
    doing a credit decisioning algorithm. So we are trying to determine whether Jeremy
    is a good risk or a bad risk. Should we loan him a million dollars. And the random
    forest says “I think he’s a good risk but I’m not at all confident.” And in which
    case, we might say okay maybe I shouldn’t give him a million dollars. Where else,
    if the random forest said “I think he’s a good risk and I’m very sure of that”
    then we are much more comfortable giving him a million dollars. And I’m a very
    good risk. So feel free to give me a million dollars. I checked the random forest
    before — a different notebook. Not in the repo 😆
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: It’s quite hard for me to give you folks direct experience with this kind of
    single observation interpretation because it’s really the kind of stuff that you
    actually need to be putting out to the front line [[41:30](https://youtu.be/BFIYUvBRTpE?t=41m30s)].
    It’s not something which you can really use so much in a Kaggle context but it’s
    more like if you are actually putting out some algorithm which is making big decisions
    that could cost a lot of money, you probably don’t so much care about the average
    prediction of the random forest but maybe you actually care about the average
    minus a couple standard deviations (i.e. what’s the worst-case prediction). Maybe
    there is a whole group that we are unconfident about, so that’s confidence based
    on tree variance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance [[42:36](https://youtu.be/BFIYUvBRTpE?t=42m36s)]
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Student: It’s basically to find out which features are important. You take
    each feature and shuffle the values in the feature and check how the predictions
    change. If it’s very different, it means that the feature was actually important;
    otherwise it is not that important.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: That was terrific. That was all exactly right. There were some details
    that were skimmed over a little bit. Anybody else wants to jump into a more detailed
    description of how it’s calculated? How exactly do we calculate feature importance
    for a particular feature?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Student: After you are done building a random forest model, you take each column
    and randomly shuffle it. And you run a prediction and check the validation score.
    If it gets bad after shuffling one of the columns, that means that column was
    important, so it has a higher importance. I’m not exactly sure how we quantify
    the feature importance.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Ok, great. Do you know how we quantify the feature importance? That
    was a great description. To quantify, we can take the difference in R² or score
    of some sort. So let’s say we’ve got our dependent variable which is price, and
    there’s a bunch of independent variables including year made [[44:22](https://youtu.be/BFIYUvBRTpE?t=44m22s)].
    We use the whole lot to build a random forest and that gives us our predictions.
    The we can compare that to get R², RMSE, whatever you are interested in from the
    model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/757a2db0f725331a1679cc81ca3e81e8.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Now the key thing here is I don’t want to have to retrain my whole random forest.
    That’s slow and boring, so using the existing random forests. How can I figure
    out how important year made was? So the suggestion was, let’s randomly shuffle
    the whole column. Now that column is totally useless. it’s got the same mean,
    same distribution. Everything about it is the same, but there’s no connection
    at all between actual year made and what’s now in that column. I’ve randomly shuffled
    it. So now I put that new version through the same random forest (so there is
    no retraining done) to get some new ŷ (ym). Then I can compare that to my actuals
    to get RMSE (ym). So now I can start to create a little table where I got the
    original RMSE (3, for example), with YearMade scrambled with RMSE of 2\. Enclosure
    scrambled had RMSE of 2.5\. Then I just take these differences. For YearMade,
    the importance is 1, Enclosure is 0.5, and so forth. How much worse did my model
    get after I shuffled that variable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f5e68179bef47f6076fba1241e13c957.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: '**Question**: Would all importances sum to one [[46:52](https://youtu.be/BFIYUvBRTpE?t=46m52s)]?
    Honestly, I’ve never actually looked at what the units are, so I’m not quite sure.
    We can check it out during the week if somebody’s interested. Have a look at sklearn
    code and see exactly what those units of measures are because I’ve never bothered
    to check. Although I don’t check like the units of measure specifically, what
    I do check is the relative importance. Here is an example.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/497e79ef0d22bc97ee5ccdbda4b72eb6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: So . rather than just saying what are the top ten, yesterday one of the practicum
    students asked me about a feature importance where they said “oh, I think these
    three are important” and I pointed out that the top one was thousand times more
    important than the second one. So look at the relative numbers here. So in that
    case, it’s like “no, don’t look at the top three, look at the one that’s a thousand
    times more important and ignore all the rest.” Your natural tendency is to want
    to be precise and careful, but this is where you need to override that and be
    very practical. This thing is a thousand times more important. Don’t spend any
    time on anything else. Then you can go and talk to your manager of your project
    and say this thing is a thousand times more important. And then they might say
    “oh, that was a mistake. It shouldn’t have been in there. We don’t actually have
    that information at the decision time or for whatever reason we can’t actually
    use that variable.” So then you could remove it and have a look. Or they might
    say “gosh, I had no idea that was by far more important than everything else put
    together. So let’s forget this random forest thing and just focus on understanding
    how we can better collect that one variable and better use that one variable.”
    So that’s something which comes up quite a lot and actually another place that
    came up just yesterday. Another practicum student asked me “I’m doing this medical
    diagnostics project and my R² is 0.95 for a disease which I was told is very hard
    to diagnose. Is this random forest genius or is something going wrong?” And I
    said remember, the second thing you do after you build a random forest is to do
    feature importance, so do feature importance and what you’ll probably find is
    that the top column is something that shouldn’t be there. So that’s what happened.
    He came back to me half an hour later, he said “yeah, I did the feature importance
    and you were right. The top column was basically a something that was another
    encoding of the dependent variable. I’ve removed it and now my R² is -0.1 so that’s
    an improvement.”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'The other thing I like to look at is this chart [[50:03](https://youtu.be/BFIYUvBRTpE?t=50m3s)]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/41a1947a7212c132120f08c6f75844e1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Basically it says where things flatten off in terms of which ones I should be
    really focusing on. So that’s the most important one. When I did credit scoring
    in telecommunications, I found there were nine variables that basically predicted
    very accurately who was going to end up paying for their phone and who wasn’t.
    Apart from ending up with a model that saved them three billion dollars a year
    in fraud and credit costs, it also let them basically rejig their process so they
    focused on collecting those nine variables much better.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence [[50:46](https://youtu.be/BFIYUvBRTpE?t=50m46s)]
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is an interesting one. Very important but in some ways kind of tricky to
    think about.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c504b3d97823d5561d3b15cb9cc0d860.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: 'Let’s come back to how we calculate this in a moment, but the first thing to
    realize is that the vast majority of the time, when somebody shows you a chart
    , it will be like a univariate chart that’ll just grab the data from the database
    and they’ll plot X against Y. Then managers have a tendency to want to make a
    decision. So it would be “oh, there’s this drop-off here, so we should stop dealing
    in equipment made between 1990 and 1995\. This is a big problem because real world
    data has lots of these interactions going on. So maybe there was a recession going
    on around the time that those things are being sold or maybe around that time,
    people were buying more of a different type of equipment. So generally what we
    actually want to know is all other things being equal, what’s the relationship
    between YearMade and SalePrice. Because if you think about the drivetrain approach
    idea of the levers, you really want a model that says if I change this lever,
    how will it change my objective. It’s by pulling them apart using partial dependence
    that you can say actually this is the relationship between YearMade and SalePrice
    all other things being equal:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/86606730a6d2d122fe044828ff5ab10d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
- en: So how do we calculate that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'Student: For the variable YearMade, for example, you keep all other variables
    constant. Then you are going to pass every single value of the YearMade, train
    the model after that. So for every model you’ll have light blue lines and the
    median is going to be the yellow line.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: So let’s try and draw that. By “leave everything else constant”, what
    she means is leave them at whatever they are in the dataset. So just like when
    we did feature importance, we are going to leave the rest of the dataset as it
    is. And we’re going to do partial dependence plot for YearMade. So we’ve got all
    of these other rows of data that we will just leave as they are. Instead of randomly
    shuffling YearMade, what we are going to do is replace every single value with
    exactly the same thing — 1960\. Just like before, we now pass that through our
    existing random forests which we have not retrained or changed in any way to get
    back out a set of predictions `y1960`. Then we can plot that on a chart — YearMade
    against partial dependence.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b378ac029f436b5689fbe13b0d97b704.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Now we can do that for 1961, 1962, 1963, and so forth. We can do that on average
    for all of them, or we could do it just for one of them. So when we do it for
    just one of them and we change its YearMade and pass that single thing through
    our model, that gives us one of these blue lines. So each one of these blue lines
    is a single row as we change its YearMade from 1960 up to 2008\. So then we can
    just take the median of all of these blue lines to say on average what’s the relationship
    between YearMade and price all other things being equal. Why is it that it works?
    Why is it that this process tells us the relationship between YearMade and price
    all other things being equal? Maybe it’s good to think about a really simplified
    approach [[56:03](https://youtu.be/BFIYUvBRTpE?t=56m3s)]. A really simplified
    would say what’s the average auction? What’s the average sale date, what’s the
    most common type of machine we well? Which location we mostly sell things? And
    we could come up with a single row that represents the average auction and then
    we could say okay, let’s run that row through the random forest but replace its
    YearMade with 1960 and then do it again with 1961 and we could plot those on our
    little chart. That would give us a version of the relationship between YearMade
    and sale price all other things being equal. But what if tractors looked like
    that and backhoe loaders looked like a flat line:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1021d977252b177e379b73c06bbf5dc3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Then taking the average one would hide the fact that there are these totally
    different relationships. So instead, we basically say, okay our data tells us
    what kinds of things we tend to sell, who we tend to sell them, and when we tend
    to sell them, so let’s use that. Then we actually find out for every blue line,
    here are actual examples of these relationships. So then what we can do is as
    well as plotting the median, we can do a cluster analysis to find out a few different
    shapes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9184d024b3bbfcbc3e1646e51ce21066.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: In this case, they all look pretty much the different versions of the same thing
    with different slopes, so my main takeaway from this would be that the relationship
    between sale price and year made is basically a straight line. And remember, this
    was a log of sale price so this is actually showing us an exponential. So this
    is where I would then bring in the domain expertise which is like “okay, things
    depreciate over time by a constant ratio so therefore, I would expect older stuff
    year made to have this exponential shape.” So this is where, as I mentioned, the
    very start of of my machine learning project, I generally try to avoid using as
    much domain expertise as I can and let the data do the talking. So one of the
    questions I got this morning was “there’s like a sale ID and model ID, I should
    throw those away, right? Because they are just IDs.” No. Don’t assume anything
    about your data. Leave them in and if they turn out to be super important predictors,
    you want to find out why that is. But then, now I’m at the other end of my project.
    I’ve done my feature importance, I’ve pulled out the stuff which is from that
    dendrogram (i.e. redundant features), I’m looking at the partial dependence and
    now I’m thinking okay is this shape what I expected? So even better, before you
    plot this, first of all think what shape would I expect this to be. Because it’s
    always easy to justify to yourself after the fact, oh, I knew it would look like
    this. So what shape you expect and then is it that shape? In this case, I’d say
    this is what I would expect. Where else the previous plot is not what I’d expect.
    So the partial dependence plot has really pulled out the underlying truth.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Say you have 20 features that are important, are you going to
    measure the partial dependence for every single one of them [[1:00:05](https://youtu.be/BFIYUvBRTpE?t=1h5s)]?
    If there are twenty features that are important, then I will do the partial dependence
    for all of them where important means like it’s a lever I can actually pull, the
    magnitude of its size is not much smaller than the other nineteen, you know, based
    on all these things it’s a feature I ought to care about then I will want to know
    how it’s related. It’s pretty unusual to have that many features that are important
    both operationally and from a modeling point of view in my experience.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: How do you define importance [[1:00:58](https://youtu.be/BFIYUvBRTpE?t=1h58s)]?
    Important means it’s a lever (i.e. something I can change) and it’s on the spiky
    end of this tail (left):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/53c920589028f6ed9ccd09b796e7c005.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
- en: Or maybe it’s not a lever directly. Maybe it’s like zip code and I can’t actually
    tell my customers where to live but I could focus my new marketing attention on
    a different zip code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** Would it make sense to do pairwise shuffling for every combination
    of two features and hold everything else constant in feature importance to see
    interactions and compare scores [[1:01:45](https://youtu.be/BFIYUvBRTpE?t=1h1m45s)]?
    You wouldn’t do that so much for partial dependence. I think your question is
    really getting to the question of could we do that for feature importance. I think
    interaction feature importance is a very important and interesting question. But
    doing it by randomly shuffling every pair of columns, if you’ve got a hundred
    columns, it sounds computationally intensive, possibly infeasible. So what I’m
    going to do is after we talk about tree interpreter, I’ll talk about interesting
    but largely unexplored approach that will probably work.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Tree interpreter [[1:02:43](https://youtu.be/BFIYUvBRTpE?t=1h2m43s)]
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prince: I was thinking this to be more like feature importance, but feature
    importance is for complete random forest model, and this tree interpreter is for
    feature importance for particular observation. So let’s say it’a about hospital
    readmission. If a patient A is going to be readmitted to a hospital, which feature
    for that particular patient is going to impact and how can we change that. It
    is calculated starting from the prediction of mean then seeing how each feature
    is changing the behavior of that particular patient.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: I’m smiling because that was one of the best examples of technical
    communication I’ve heard in a long time, so it’s really good to think about why
    was that effective. So what Prince did there was, he used as specific an example
    as possible. Humans are much less good at understanding abstractions. So if you
    say “it takes some kind of feature, and then there’s an observation in that feature”
    whereas it’s the hospital readmission. So we take a specific example. The other
    thing he did that was very effective was to take an analogy to something we already
    understand. So we already understand the idea of feature importance across all
    of the rows in a dataset. So now we are going to do it for a single row. So one
    of the things I was really hoping we would learn from this experience is how to
    become effective technical communicators. So that was a really great role model
    from Prince of using all the tricks we have at our disposal for effective technical
    communication. So hopefully you found that useful explanation. I don’t have a
    lot to add to that other than to show you what that looks like.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'With the tree interpreter, we picked out a row [[1:04:56](https://youtu.be/BFIYUvBRTpE?t=1h4m56s)]:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d658c4c04011a022138d863c0783078f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'Remember when we talked about the confidence intervals at the very start (i.e.
    the confidence based on tree variance). We said you mainly use that for a row.
    So this would also be for a row. So it’s like “why is this patient likely to be
    readmitted?” Here is all the information we have about that patient or in this
    case this auction. Why is this auction so expensive? So then we call `ti.predict`
    and we get back the prediction of the price, the bias (i.e. the root of the tree
    — so this is just the average price for everybody so this is always going to be
    the same), and then the contributions which is how important is each of these
    things:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/79b64347e1a6748da52bfcc6491345af.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: The way we calculated that was to say at the very start, the average price was
    10\. Then we split on enclosure. For those with this enclosure, the average was
    9.5\. Then we split on year made less than 1990 and for those with that year made,
    the average price was 9.7\. Then we split on the number of hours on the meter,
    and with this branch, we got 9.4.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a858ec9b9fe155abec57b59cf261650b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: We then have a particular auction which we pass it through the tree. It just
    so happens that it takes the top most path. One row can only have one path through
    the tree. So we ended up at 9.4\. Then we can create a little table. As we go
    through, we start at the top and we start with 10 — that’s our bias. And we said
    enclosure resulted in a change from 10 to 9.5 (i.e. -0.5). Year made changed it
    from 9.5 to 9.7 (i.e. +0.2), then meter changed it from 9.7 down to 9.4 (-0.3).
    Then if we add all that together (10–0.5+0.2–0.3), lo and behold that’s the prediction.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3429441276ef60a6ac385e04566fc25b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'Which takes us to our Excel spreadsheet [[1:08:07](https://youtu.be/BFIYUvBRTpE?t=1h8m7s)]:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8297d56c24cd16b7f6c634579eadb8ac.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
- en: Last week, we have use Excel for this because there wasn’t a good Python library
    for doing waterfall charts. So we saw we got our starting point this is the bias,
    and then we had each of our contributions and we ended up with our total. The
    world is now a better place because Chris has created a Python waterfall chart
    module for us and put it on pip. So never again where we have to use Excel for
    this. I wanted to point out that waterfall charts have been very important in
    business communications at least as long as I’ve been in business — so that’s
    about 25 years. Python is maybe a couple of decades old. But despite that, no
    one in the Python world ever got to the point where they actually thought “you
    know, I’m gonna make a waterfall chart” so they didn’t exist until two days ago
    which is to say the world is full of stuff which ought to exist and doesn’t. And
    doesn’t necessarily take a heck a lot of time to build. It took Chris about 8
    hours, so a hefty amount but not unreasonable. And now forevermore people when
    they want the Python waterfall chart will end up at Chris’ Github repo and hopefully
    find lots of other USF contributors who have made it even better.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In order for you to help improve Chris’ Python waterfall, you need to know how
    to do that. So you are going to need to submit a pull request. Life becomes very
    easy for submitting pull requests if you use something called [hub](https://hub.github.com/).
    What they suggest you do is that you alias `git` to `hub` because it turns out
    that hub is actually a strict superset of git. What it lets you do is you can
    go `git fork`, `git push` , and `git pull-request` and you’ve now sent Chris a
    pull request. Without hub, this is actually a pain and requires like going to
    the website and filling in forms and stuff. So this gives you no reason not to
    do pull request. I mention this because when you are interviewing for a job, I
    can promise you that the person you are talking to will check your github and
    if they see you have a history of submitting thoughtful pull requests that are
    accepted to interesting libraries, that looks great. It looks great because it
    shows you’re somebody who actually contributes. It also shows that if they are
    being accepted that you know how to create code that fits with people’s coding
    standards, has appropriate documentation, passes their tests and coverage, and
    so forth. So when people look at you and they say oh, here is somebody with a
    history of successfully contributing, accepted pull requests to open-source libraries,
    that’s a great part of your portfolio. And you can specifically refer to it. So
    either I’m the person who build Python waterfall, here is my repo or I’m the person
    who contributed currency number formatting to Python waterfall, here is my pull
    request. Anytime you see something that doesn’t work right in any open source
    software you use, it is not a problem, it’s a great opportunity because you can
    fix it and send in the pull request. So give it a go. It actually feels great
    the first time you have a pull request accepted. And of course, one big opportunity
    is the fastai library. Thanks to one of our students, we now have docstrings for
    most of the `fastai.structured` library, again came via a pull request.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Does anybody have any questions about how to calculate any of these random forest
    interpretation methods or why we might want to use them [[1:12:50](https://youtu.be/BFIYUvBRTpE?t=1h12m50s)]?
    Towards the end of the week, you’re going to need to be able to build all of these
    yourself from scratch.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** Just looking at the tree interpreter, I noticed that some of
    the values are `nan` ’s. I get why you keep them in the tree but how can `nan`
    have a feature importance [[1:13:19](https://youtu.be/BFIYUvBRTpE?t=1h13m19s)]?
    Let me pass it back to you. Why not? So in other words, how is `nan` handled in
    Pandas and therefore in the tree? Does anybody remember, notice these are all
    in categorical variables, how does Pandas handle `nan` ’s in categorical variable
    and how does fastai deal with them? Pandas sets them to -1 category code and fastai
    adds one to all of the category code so it ends up being zero. In other words,
    remember by the time it hits the random forest it’s just a number, and it’s just
    zero. And we map it back to the descriptions back here. So the question really
    is why shouldn’t the random forest be able to split on zero? It’s just another
    number. So it could be `nan`, `high`, `medium`, `low`= 0, 1, 2, 3\. So missing
    values are one of these things that are generally taught really badly. Often people
    get taught here are some ways to remove columns with missing values or remove
    rows with missing values or to replace missing values. That’s never what we want
    because missingness is very very very often interesting. So we actually learnt
    that from our feature importance that coupler system `nan` is one of the most
    important features. For some reason, well, I could guess, right? Coupler system
    `nan` presumably means this is a kind of industrial equipment that doesn’t have
    a coupler system. Now I don’t know what kind that is, but apparently it’s more
    expensive kind.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: I did this competition for university grant research success where by far the
    most important predictors were whether or not some of the fields were null [[1:15:41](https://youtu.be/BFIYUvBRTpE?t=1h15m41s)].
    It turned out that this was data leakage that these fields only got filled in
    most of the time after a research grant was accepted. So it allowed me to win
    that Kaggle competition but didn’t actually help the university very much.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Extrapolation [[1:16:16](https://youtu.be/BFIYUvBRTpE?t=1h16m16s)]
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I am going to do something risky and dangerous which is we are going to do some
    live coding. The reason we are going to do some live coding is I want to explore
    extrapolation together with you, and I also want to give you a feel of how you
    might go about writing code quickly in this notebook environment. And this is
    the kind of stuff that you are going to need to be able to do in the real world
    and in the exam is quickly create the kind of code that we are going to talk about.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: I really like creating synthetic datasets anytime I’m trying to investigate
    the behavior of something because if I have a synthetic dataset, I know how it
    should behave.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Which reminds me, before we do this, I promised that we would talk about interaction
    importance and I just about forgot.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Interaction importance [[1:17:24](https://youtu.be/BFIYUvBRTpE?t=1h17m24s)]
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tree interpreter tells us the contributions for a particular row based on the
    difference in the tree. We could calculate that for every row in our dataset and
    add them up. That would tell us feature importance. And it would tell us feature
    importance in a different way. One way of doing feature importance is by shuffling
    the columns one at a time. Another way is by doing tree interpreter for every
    row and adding them up. Neither is more right than the others. They are actually
    both quite widely used so this is kind of type 1 and type 2 feature importance.
    So we could try to expand this a little bit. To do not just single variable feature
    importance, but interaction feature importance. Now here is the thing. What I’m
    going to describe is very easy to describe. It was described by Breiman right
    back when random forests were first invented, and it is part of the commercial
    software product from Salford systems who have the trademark on random forests.
    But it is not part of any open source library I’m aware of, and I’ve never seen
    an academic paper that actually studies it closely. So what I’m going to describe
    here is a huge opportunity but it’s also like there’s lots and lots of details
    that need to be fleshed out. But here is the basic idea.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'This particular difference here (in red) is not just because of year made but
    because of a combination of year made and enclosure [[1:19:15](https://youtu.be/BFIYUvBRTpE?t=1h19m15s)]:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8090753aa1c338a5f3ea2b7050a3b638.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: The fact that this is 9.7 is because enclosure was in this branch and year made
    was in this branch. So in other words, we could say the contribution of enclosure
    interacted with year made is -0.3.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: So what about the difference between 9.5 and 9.4? That’s an interaction of year
    made and hours on the meter. I’m using star here not to mean “times” but to mean
    “interacted with”. It’s a common way of doing things like R’s formulas do it this
    way as well. So year made interacted with meter has a contribution of -0.1.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5603740765510c45ecc15dae33a8b302.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Perhaps we could also say from 10 to 9.4, this also shows an interaction between
    meter and enclosure with one thing in between them. So we could say meter interacted
    with enclosure equals …and what should it be? Should it be -0.6? Some ways that
    seems unfair because we are also including the impact of year made. So maybe it
    should be -0.6 and maybe we should add back this 0.2 (9.5 → 9.7). These are like
    details that I actually don’t know the answer to. How should we best assign a
    contribution to each pair of variables in this path? But clearly conceptually
    we can. The pairs of variables in that path all represent interactions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Why don’t you force them to be next to each other in the tree
    [[1:21:47](https://youtu.be/BFIYUvBRTpE?t=1h21m47s)]? I’m not going to say it’s
    the wrong approach. I don’t think it’s the right approach though. Because it feels
    like in this path, meter and enclosure are interacting. So it seems like not recognizing
    that contribution is throwing away information. But I’m not sure. I had one of
    my staff at Kaggle actually do some R&D on this a few years ago and they actually
    found (I wasn’t close enough to know how they dealt with these details), but they
    got it working pretty well. But unfortunately it never saw the light of day as
    a software product. But this is something maybe a group of you could get together
    and build. Do some googling to check, but I really don’t think that there are
    any interaction feature importance parts of any open source library.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Wouldn’t this exclude interactions though between variables that
    don’t matter until they interact? So say your row never chooses to split down
    that path, but that variable interacting with another one becomes your most important
    split [[1:22:56](https://youtu.be/BFIYUvBRTpE?t=1h22m56s)]. I don’t think that
    happens. Because if there is an interaction that’s important only because it’s
    an interaction (and not in a univariate basis), it will appear sometimes, assuming
    that you set max features to less than one, so therefore it will appear in some
    path.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What is meant by interaction? Is it multiplication, ratio, addition
    [[1:23:31](https://youtu.be/BFIYUvBRTpE?t=1h23m31s)]? Interaction means appears
    on the same path through a tree. In the above example, there is an interaction
    between enclosure and year made because we branched on enclosure and then we branched
    on year made. So to get to 9.7, we have to have some specific value of enclosure
    and some specific value of year made.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: What if you went down the middle leaves between the two things
    you are trying to observe and you would also take into account what the final
    measure is? I mean if we extend the tree downwards, you’d have many measures both
    of like the two things you are trying to look at and also the in between steps.
    There seems to be a way to average information out in between them [[1:24:03](https://youtu.be/BFIYUvBRTpE?t=1h24m3s)]?
    There could be. I think what we should do is talk about this on the forum. I think
    this is fascinating and I hope we build something great, but I need to do my live
    coding. That was a great discussion. Keep thinking about it and do some experiments.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Back to Live Coding [[1:24:50](https://youtu.be/BFIYUvBRTpE?t=1h24m50s)]
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So to experiment with that, you almost certainly want to create a synthetic
    dataset first. It’s like `y = x1 + x2 + x1*x2` or something. Something where you
    know there is this interaction effect and there isn’t that interaction effect,
    and you want to make sure that the feature importance you get at the end is what
    you expected.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: So probably the first step would be to do single variable feature importance
    using the tree interpreter style approach [[1:25:14](https://youtu.be/BFIYUvBRTpE?t=1h25m14s)].
    One nice thing about this is it doesn’t really matter how much data you have.
    All you have to do to calculate feature importance is just slide through tree.
    So you should be able to write in a way that’s actually pretty fast, so even writing
    it in pure Python might be fast enough depending on your tree size.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: We are going to talk about extrapolation and the first thing I want to do is
    create a synthetic dataset that has a simple linear relationship. We are going
    to pretend it’s like a time series. So we need to create some x values. The easiest
    way to create some synthetic data of this type is to use `linspace` which just
    creates some evenly spaced data between start and stop by default 50 observations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ca0ed3c32e1a6203792b8b70f3443f5a.png)![](../Images/3dac629339adb81fe1a5a37cb80218c6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: Then we are going to create dependent variable, so let’s assume there is a linear
    relationship between x and y, and let’s add a little bit of randomness to it.
    `random.uniform` between low and high, so we could add somewhere between -0.2
    and 0.2, for example.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4b7e45c994e0e1ae89249fb403ae1f14.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: The next thing we need is a shape which is basically what dimensions do you
    want these random numbers to be, and obviously we want them to be the same shape
    as `x`’s shape. So we can just say `x.shape`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/81e52dc3309b92046f4d778676efd307.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: So in other words, `(50,)` is `x.shape`. Remember when you see something in
    parentheses with a comma, that’s a tuple with just one thing in it. So this is
    shape 50 and so we added 50 random numbers. Now we can plot those.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/4d8302e82de69ddca4bb3797be2d700f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: Alright, so there is our data. When you were both working as a data scientist
    or for doing your exams in this course, you need to be able to quickly whip up
    a dataset like that, throw it up in a plot without thinking too much. As you can
    see, you don’t have to really remember much if anything. You just have to know
    how to hit `shift + tab` to check the names of parameters, google, or something
    to try and find `linspace` if you forgot what it’s called.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: So let’s assume that’s our data [[1:28:33](https://youtu.be/BFIYUvBRTpE?t=1h28m33s)].
    We’re now going to build a random forest model and what I want to do is build
    a random forest model that kind of acts as if this is a time series. So I’m going
    to take left part as a training set. And take the right part as our validation
    or test set just like we did in groceries or bulldozers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d033cab61b66800b28ef99b492696808.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'We can use exactly the same kind of code that we used in `split_vals`. So we
    can say:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That splits it into the first 40 versus the last 10\. We can do the same thing
    for y and there we go.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next thing to do is we want to create a random forest and fit it which requires
    x and y.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s actually going to give an error and the reason why is that it expects
    x to be a matrix, not a vector, because it expects x to have a number of columns
    of data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: So it’s important to know that a matrix with one column is not the same thing
    as a vector.
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So if I try to run this, “Expected 2D array, got 1D array instead”:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/c89a746ef5d8e8ce13f4a68661b7b219.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: So we need to convert 1D array into a 2D array. Remember I said `x.shape` is
    `(50,)`. So `x` has one axis and x’s rank is 1\. The rank of a variable is equal
    to the length of it’s shape — how many axes it has. Vector we can think of as
    an array of rank 1 and matrix as an array of rank 2\. I very rarely use words
    like vector and matrix because they are kind of meaningless — specific example
    of something more general which is they are all N dimensional tensors or N dimensional
    arrays. So an N dimensional array we can say it’s a tensor of rank N. They basically
    mean kind of the same thing. Physicists get crazy when you say that because to
    a physicist, a tensor has quite a specific meaning but in machine learning, we
    generally use it in the same way.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: So how do we turn an one dimensional array into a two dimensional array. There
    are a couple of ways we can do it but basically we slice it. Colon (`:`) means
    give me everything in that axis. `:,None` means give me everything in the first
    axis (which is the only axis we have) and then `None` is a special indexer which
    means add a unit axis here. So let me show you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8b3e1a4f1a69428b22ffae4d90a11b2f.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: That is of shape (50, 1), so it’s a rank 2\. It has two axes. One of them is
    a very boring axis — it’s a length one axis. So let’s move `None` to the left.
    There is (1, 50). Then to remind you, the original is (50,).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/5d722ceac74a361f3381e5387efc8eed.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: So you can see I can put `None` as a special indexer to introduce a new unit
    axis there. So `x[None,:]` has one row and fifty columns. `x[:,None]` has fifty
    rows and one column — so that’s what we want. This kind of playing around with
    ranks and dimension is going to become increasingly important in this course and
    in the deep learning course. So spend a lot of time slicing with None, slicing
    with other things, try to create 3 dimensional, 4 dimensional tensors and so forth.
    I’ll show you two tricks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'The first is you never ever need to write `,:` as it’s always assumed. So these
    are exactly the same thing:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/fc83fc64ccc445d232a496199588fea8.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: And you see that in code all the time, so you need to recognize it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: The second trick is `x[:,None]` is adding an axis in the second dimension (or
    I guess index 1 dimension). What if I always want to put it in the last dimension?
    Often our tensors change dimensions without us looking because you went from a
    one channel image to a three channel image, or you went from a single image to
    a mini batch of images. Suddenly, you get new dimensions appearing. So make things
    general, I would say `...` which means as many dimensions as you need to fill
    this up. So in this case (`x[…, None].shape` ), it’s exactly the same but I would
    always try to write it that way because it means it’s going to continue to work
    as I get higher dimensional tensors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: So in this case, I want 50 rows and one column, so I’ll call that x1\. Let’s
    now use that here and so this is now a 2D array and so I can create my random
    forest.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/8d4106b6e017f31faf7d4e51acb70f40.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Then I could plot that, and this is where you’re going to have to turn your
    brains on because the folks this morning got this very quickly which was super
    impressive. I’m going to plot `y_trn` against `m.predict(x_trn)`. Before I hit
    go, what is this going to look like? It should basically be the same. Our predictions
    hopefully are the same as the actuals. So this should fall on a line but there
    is some randomness so it won’t quite.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7fdc34fa62b66337b87a321dcc6b462d.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: That was the easy one. Let’s now do the hard one, the fun one. What is that
    going to look like?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ae6c6ba81817c96cae72133a62b0f9d2.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Think about what trees do and think about the fact that we have a validation
    set on the right and a training set on the left:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/a4f103d8c76f42587e6e7bfc91bdf6c1.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: So think about a forest is just a bunch of trees.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Tim: I’m guessing since all the new data is actually outside of the original
    scope, so it’s all going to be basically the same — it’s like one huge group [[1:37:15](https://youtu.be/BFIYUvBRTpE?t=1h37m15s)].'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Yeah, right. So forget the forest, let’s create one tree. So we are
    probably going to split somewhere around here first, then split somewhere here,
    … So our final split is right most node. Our prediction, when we take one from
    validation set, so it’s going to put that through the forest and end up predicting
    the right most average. It can’t predict anything higher than that because there
    is nothing higher to average.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7438a2c65674d13192508a3766f65bed.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: So this is really important to realize a random forest is not magic. It’s just
    returning the average of nearby observations where nearby is kind of in this like
    “tree space”. So let’s run it and see if Tim is right
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58db9125b11012055e02c3e95e438af1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Holy crap, that’s awful. If you don’t know how random forests work then this
    is going to totally screw you. If you think that it’s actually going to be able
    to extrapolate to any kind of data it hasn’t seen before, particularly future
    time period, it’s just not. It just can’t. It’s just averaging stuff it’s already
    seen. That’s all it can do.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so we are going to be talking about how to avoid this problem. We talked
    a little bit in the last lesson about trying to avoid it by avoiding unnecessary
    time dependent variables where we can. But in the end, if you really have a time
    series that looks like this, we actually have to deal with a problem. One way
    we could deal with the problem would be use a neural net. Use something that actually
    has a function or shape that can actually fit something that actually has a function
    or shape that can actually fit something like this so it will extrapolate nicely:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cffc3e3e176bd6304c4199b1afbd8d7.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Another approach would be to use all the time series techniques you guys are
    learning about in the morning class to fit some kind of time series and then detrend
    it. Then you’ll end up with detrended dots and then use the random forest to predict
    those. That’s particularly cool because imagine what your random forest was actually
    trying to predict data which was two different states. So the blues ones are down
    there, and the red ones are up here.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75be04c6622f2e5ba4a7a089b9cfeba5.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: If you try to use a random forest, it’s going to do a pretty crappy job because
    time is going to seem much more important. So it’s basically still going to split
    like this and split like that, then finally once it gets down to left corner,
    it will be like “oh okay, now I can see the difference between the states.”
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b12cb556d1d45e083ac139e7bd74100.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: In other words, when you’ve got this big time piece going on, you’re not going
    to see the other relationships in the random forest until every tree deals with
    time. So one way to fix this would be with a gradient boosting machine (GBM).
    What a GBM does is, it creates a little tree, and runs everything through that
    first little tree (which could be the time tree) then it calculates the residuals
    and the next little tree just predicts the residuals. So it would be kind of like
    detrending it, right? GBM still can’t extrapolate to the future but at least they
    can deal with time-dependent data more conveniently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: We are going to be talking about this quite a lot more over the next coupe of
    weeks, and in the end that a solution is going to be just use neural nets. But
    for now, using some kind of time series analysis, detrend it, and then use random
    forest on that isn’t a bad technique at all. If you are playing around something
    like Ecuador groceries competition, that would be a really good thing to fiddle
    around with.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
