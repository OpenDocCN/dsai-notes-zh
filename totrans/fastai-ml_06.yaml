- en: 'Machine Learning 1: Lesson 6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习1：第6课
- en: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49](https://medium.com/@hiromi_suenaga/machine-learning-1-lesson-6-14bbb8180d49)
- en: '*My personal notes from* [*machine learning class*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*.
    These notes will continue to be updated and improved as I continue to review the
    course to “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*来自*[*机器学习课程*](http://forums.fast.ai/t/another-treat-early-access-to-intro-to-machine-learning-videos/6826/1)*的个人笔记。随着我继续复习课程以“真正”理解它，这些笔记将继续更新和改进。非常感谢*[*Jeremy*](https://twitter.com/jeremyphoward)*和*[*Rachel*](https://twitter.com/math_rachel)*给了我这个学习的机会。*'
- en: '[Video](https://youtu.be/BFIYUvBRTpE) / [Powerpoint](https://github.com/fastai/fastai/blob/master/courses/ml1/ppt/ml_applications.pptx)'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[视频](https://youtu.be/BFIYUvBRTpE) / [幻灯片](https://github.com/fastai/fastai/blob/master/courses/ml1/ppt/ml_applications.pptx)'
- en: We’ve looked at a lot of different random forest interpretation techniques and
    a question that has come up a little bit on the forum is what are these for really?
    How do these help me get a better score on Kaggle, and my answer has been “they
    don’t necessarily”. So I wanted to talk more about why we do machine learning.
    What’s the point? To answer this question, I want to show you something really
    important which is examples of how people have used machine learning mainly in
    business because that’s where most of you are probably going to end up after this
    is working for some company. I’m going to show you applications of machine learning
    which are either based on things that I’ve been personally involved in myself
    or know of people who are doing them directly so none of these are going to be
    hypotheticals — these are all actual things that people are doing and I’ve got
    direct or secondhand knowledge of.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看过很多不同的随机森林解释技术，论坛上有一些问题是这些到底有什么用？它们如何帮助我在Kaggle上获得更好的分数，我的答案是“它们不一定会”。因此，我想更多地谈谈为什么我们要做机器学习。这有什么意义？为了回答这个问题，我想向你展示一些非常重要的东西，即人们主要在商业中如何使用机器学习的例子，因为这是你们大多数人在结束后可能会在某家公司工作。我将向你展示机器学习的应用，这些应用要么基于我自己亲身参与的事情，要么是我知道直接在做这些事情的人，因此这些都不是假设的——这些都是人们正在做的实际事情，我有直接或间接的了解。
- en: Two Groups of Applications [[1:26](https://youtu.be/BFIYUvBRTpE?t=1m26s)]
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两组应用[[1:26](https://youtu.be/BFIYUvBRTpE?t=1m26s)]
- en: 'Horizontal: In business, horizontal means something that you do across different
    kinds of business. i.e. everything involving marketing.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平：在商业中，水平意味着跨不同类型的业务进行的事情。即涉及营销的所有事情。
- en: 'Vertical: Something you do within a business or within a supply chain or a
    process.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直：在企业内部或供应链或流程中进行的某些事情。
- en: Horizontal Applications
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水平应用
- en: 'Pretty much every company has to try to sell more products to its customers
    so therefore does marketing. So each of these boxes are examples of some of the
    things that people are using machine learning for in marketing:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每家公司都必须尝试向其客户销售更多产品，因此进行营销。因此，这些框中的每一个都是人们在营销中使用机器学习的一些示例：
- en: '![](../Images/dd65548e467e8e98626f519b1415255e.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/dd65548e467e8e98626f519b1415255e.png)'
- en: Let’s take an example — Churn. Churn refers to a model which attempts to predict
    who’s going to leave. I’ve done some churn modeling fairly recently in telecommunications.
    We were trying to figure out for this big cellphone company which customers are
    going to leave. That is not of itself that interesting. Building a highly predictive
    model that says Jeremy Howard is almost certainly going to leave next month is
    probably not that helpful because if I’m almost certainly going to leave net month,
    there’s probably nothing you can do about it — it’s too late and it’s going to
    cost you too much to keep me.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举个例子——流失。流失是指试图预测谁会离开的模型。最近在电信领域做了一些流失建模。我们试图弄清楚这家大型手机公司的哪些客户会离开。这本身并不那么有趣。构建一个高度预测性的模型，说Jeremy
    Howard几乎肯定会在下个月离开，可能并不那么有帮助，因为如果我几乎肯定会在下个月离开，你可能无法做任何事情——为了留住我，成本可能太高。
- en: 'So in order to understand why we would do churn modeling, I’ve got a little
    framework that you might find helpful: [Designing great data products](https://www.oreilly.com/ideas/drivetrain-approach-data-products).
    I wrote it with a couple of colleagues a few years ago and in it, I describe my
    experience of actually turning machine learning models into stuff that makes money.
    The basic trick is what I call the **Drivetrain Approach** which is these four
    steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了理解我们为什么要进行流失建模，我有一个可能对你有帮助的小框架：[设计出色的数据产品](https://www.oreilly.com/ideas/drivetrain-approach-data-products)。几年前我和几位同事一起写了这篇文章，在其中，我描述了我将机器学习模型转化为赚钱的东西的经验。基本技巧是我称之为**驱动器方法**，这是这四个步骤：
- en: '![](../Images/a6dbaec77fe8d5f4fb87eee227326a3e.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a6dbaec77fe8d5f4fb87eee227326a3e.png)'
- en: Defined Objective [[3:48](https://youtu.be/BFIYUvBRTpE?t=3m48s)]
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义目标[[3:48](https://youtu.be/BFIYUvBRTpE?t=3m48s)]
- en: The starting point to actually turn a machine learning project into something
    that’s actually useful is to know what I am trying to achieve and that does mean
    I’m trying to achieve a high area under the ROC curve or trying to achieve a large
    difference between classes. It would be I’m trying to sell more books or I’m trying
    to reduce the number of customers that leave next month or I’m trying to detect
    lung cancer earlier. These are objectives. So the objective is something that
    absolutely directly is the thing that the company or the organization actually
    wants. No company or organization lives in order to create a more accurate predictive
    model. There are some reason. So that’s your objective. That’s obviously the most
    important thing. If you don’t know the purpose of what you are modeling for then
    you can’t possibly do a good job of it. And hopefully people are starting to pick
    that up out there in the world of data science, but interestingly what very few
    people are talking about but it’s just as important is the next thing which is
    levers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将机器学习项目转化为实际有用的起点是知道我试图实现什么，这意味着我试图实现高ROC曲线下面积或尝试实现类之间的巨大差异。这可能是我试图销售更多的书，或者我试图减少下个月离开的客户数量，或者我试图更早地检测肺癌。这些都是目标。因此，目标是公司或组织实际想要的东西。没有公司或组织是为了创建更准确的预测模型而存在的。这是有原因的。所以这就是你的目标。显然，这是最重要的事情。如果你不知道你为何建模，那么你不可能做好这项工作。希望人们开始在数据科学领域意识到这一点，但有趣的是，很少有人谈论但同样重要的是下一步，即杠杆。
- en: Levers [[5:04](https://youtu.be/BFIYUvBRTpE?t=5m4s)]
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 杠杆[[5:04](https://youtu.be/BFIYUvBRTpE?t=5m4s)]
- en: A lever is a thing that the organization can do to actually drive the objective.
    So let’s take the example of churn modeling. What is a lever that an organization
    could use to reduce the number of customers that are leaving? They could call
    someone and say “Are you happy? Anything we could do?” They could give them a
    free pen or something if they buy $20 worth of product next month. You could give
    them specials. So these are levers. Whenever you are working as a data scientists,
    keep coming back and thinking what are we trying to achieve (we being the organization)
    and how we are trying to achieve it being what are the actual things we can do
    to make that objective happen. So building a model is never ever a lever, but
    it could help you with the lever.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 杠杆是组织可以实际采取的行动，以推动目标的实现。所以让我们以流失建模为例。组织可以采取什么杠杆来减少离开的客户数量？他们可以打电话给某人，问：“你满意吗？我们能做些什么？”他们可以在下个月购买价值20美元的产品时赠送免费的钢笔或其他物品。你可以给他们提供特别优惠。所以这些就是杠杆。当你作为数据科学家工作时，不断回头思考我们试图实现什么（我们指的是组织），以及我们如何实现它，即我们可以做哪些实际的事情来实现这个目标。因此，构建模型绝对不是杠杆，但它可以帮助你使用杠杆。
- en: Data [[7:01](https://youtu.be/BFIYUvBRTpE?t=7m1s)]
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据[[7:01](https://youtu.be/BFIYUvBRTpE?t=7m1s)]
- en: So then the next step is what data does the organization have that could possibly
    help them to set that lever to achieve that objective. So this is not what data
    did they give you when you started the project. But think about it from a first
    principle’s point of view — okay, I’m working for a telecommunications company,
    they gave me some certain set of data, but I’m sure they must know where their
    customers live, how many phone calls they made last month, how many times they
    called customer service, etc. So have a think about okay if we are trying to decide
    who should we give a special offer to proactively, then we want to figure out
    what information do we have that might help us to identify who’s going to react
    well or badly to that. Perhaps more interestingly would be what if we were doing
    a fraud algorithm. So we are trying to figure out who’s going to not pay for the
    phone that they take out of the store, they are on some 12-month payment plan,
    and we never see them again. Now in that case, the data we have available , it
    doesn’t matter what’s in the database, what matters is what’s the data that we
    can get when the customer is in the shop. So there’s often constraints around
    the data that we can actually use. So we need to know what am I trying to achieve,
    what can this organization actually do specifically to change the outcome, and
    at the point that the decision is being made, what data do they have or could
    they collect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的步骤是组织拥有哪些数据可能帮助他们设置杠杆以实现目标。这不是指他们在项目开始时给你的数据。而是从第一原则的角度考虑——好吧，我在一家电信公司工作，他们给了我一些特定的数据，但我肯定他们必须知道他们的客户住在哪里，上个月打了多少电话，打了多少次客服电话等等。所以想一想，如果我们试图决定主动给谁提供特别优惠，那么我们想要弄清楚我们有哪些信息可能帮助我们确定谁会对此做出积极或消极的反应。也许更有趣的是，如果我们正在进行欺诈算法。我们试图弄清楚谁不会支付他们从商店拿出的手机，他们正在进行某种12个月的付款计划，然后我们再也没有见到他们。在这种情况下，我们可以获得的数据，数据库中有什么并不重要，重要的是当客户在商店时我们可以获得什么数据。因此，我们通常会受到我们实际可以使用的数据的限制。因此，我们需要知道我试图实现什么目标，这个组织实际上可以具体做些什么来改变结果，以及在做出决定时，他们拥有或可以收集到哪些数据。
- en: Models [[8:45](https://youtu.be/BFIYUvBRTpE?t=8m45s)]
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型[[8:45](https://youtu.be/BFIYUvBRTpE?t=8m45s)]
- en: So then the way I put that all together is with a model. This is not a model
    in the sense of a predictive model but it’s a model in the sense of a simulation
    model. So one of the main example I gave in this paper is when I spent many years
    building which is if an insurance company changes their prices, how does that
    impact their profitability. So generally your simulation model contains a number
    of predictive models. So I had, for example, a predictive model called an elasticity
    model that said for a specific customer, if we charge them a specific price for
    a specific product, what’s the probability that they would say yes both when it’s
    new business and then a year later what’s the probability that they’ll renew.
    Then there’s another predictive model which is what’s the probability that they
    are going to make a claim and how much is that claim going to be. You can then
    combine these models together then to say all right, if we changed our pricing
    by reducing it by 10% for everybody between 18 and 25 and we can run it through
    these models that combined together into a simulation then the overall impact
    on our market share in 10 years time is X and our cost is Y and our profit is
    Z and so forth.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我把所有这些放在一起的方式是通过一个模型。这不是一个预测模型，而是一个模拟模型。我在这篇论文中给出的一个主要例子是，我花了很多年时间建立的一个模型，即如果一个保险公司改变他们的价格，这将如何影响他们的盈利能力。通常你的模拟模型包含了许多预测模型。比如，我有一个叫做弹性模型的预测模型，它说对于一个特定的客户，如果我们为他们的某个产品收取一个特定的价格，他们会在新业务时和一年后续保的概率是多少。然后还有另一个预测模型，即他们会提出索赔的概率以及索赔金额是多少。然后你可以将这些模型结合起来，然后说好，如果我们将我们的定价降低10%适用于18到25岁的所有人，然后我们可以通过这些模型运行，将它们结合成一个模拟，那么我们在10年后的市场份额的整体影响是X，我们的成本是Y，我们的利润是Z等等。
- en: In practice, most of the time, you really are going to care more about the results
    of that simulation than you do about the predictive model directly. But most people
    are not doing this effectively at the moment. For example, when I go to Amazon,
    I read all of Douglas Adams’ books, and so having read all Douglas Adams’ books,
    the next time I went to Amazon they said would you like to buy the collected works
    of Douglas Adams. This is after I had bought every one of his books. So from a
    machine learning point of view, some data scientist had said oh people that buy
    one of Douglas Adams’ books often go on to buy the collected works. But recommending
    to me that I buy the collected works of Douglas Adams isn’t smart. It’s actually
    not smart at a number of levels. Not only is unlikely to buy a box set of something
    of which I have every one individually but furthermore it’s not going to change
    my buying behavior. I already know about Douglas Adams. I already know I like
    him, so taking up your valuable web space to tell me hey maybe you should buy
    more of the author who you’re already familiar with and bought lots of times isn’t
    actually going to change my behavior. So what if instead of creating a predictive
    model, Amazon had built an optimization model that could simulate and said if
    we show Jeremy this ad, how likely is he then to go on to buy this book and if
    I don’t show him this ad, how likely is he to go on to buy this book. So that’s
    the counterfactual. The counter factual is what would have happened otherwise,
    and then you can take the difference and say what should we recommend him that
    is going to maximally change his behavior. So maximally result in more books and
    so you’d probably say oh he’s never bought any Terry Pratchett book, he probably
    doesn’t know about Terry Pratchett but lots of people that liked Douglas Adams
    did turn out to like Terry Pratchett so let’s introduce him to a new author.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，大多数时候，你真的更关心那个模拟的结果，而不是直接关心预测模型。但大多数人目前并没有有效地做到这一点。例如，当我去亚马逊时，我读了道格拉斯·亚当斯的所有书，所以在我读完所有道格拉斯·亚当斯的书之后，下次我去亚马逊，他们说你想买道格拉斯·亚当斯的全部作品吗。这是在我已经买了他的每一本书之后。从机器学习的角度来看，一些数据科学家可能会说，购买道格拉斯·亚当斯的一本书的人通常会继续购买他的全部作品。但向我推荐购买道格拉斯·亚当斯的全部作品并不明智。这实际上在很多方面都不明智。不仅是因为我不太可能购买一个我已经有每一本书的合集，而且这也不会改变我的购买行为。我已经了解道格拉斯·亚当斯，我已经知道我喜欢他，所以占用你宝贵的网页空间来告诉我，嘿，也许你应该购买更多你已经熟悉并多次购买的作者的作品实际上不会改变我的行为。那么，如果亚马逊不是创建一个预测模型，而是建立一个能够模拟的优化模型，然后说如果我们向杰里米展示这个广告，他会有多大可能继续购买这本书，如果我不向他展示这个广告，他会有多大可能继续购买这本书。这就是对立事实。对立事实是否则会发生什么，然后你可以计算差异，然后说我们应该推荐他什么才能最大程度地改变他的行为。所以最大程度地导致更多的书籍，所以你可能会说，哦，他从来没有买过特里·普拉切特的书，他可能不了解特里·普拉切特，但很多喜欢道格拉斯·亚当斯的人确实喜欢特里·普拉切特，所以让我们向他介绍一个新的作者。
- en: So it’s the difference between a predictive model on the one hand versus an
    optimization model on the other hand. So the two tend to go hand in hand. First
    of all we have a simulation model. The simulation model is saying in the world
    where we put Terry Pratchett’s book on the front page of Amazon for Jeremy Howard,
    this is what would have happened. He would have bought it with a 94% probability.
    That then tells us with this lever of what do I put on my homepage for Jeremy
    today, we say okay the different settings of that lever that put Terry Pratchett
    on the homepage has the highest simulated outcome. Then that’s the thing which
    maximizes our profit from Jeremy’s visit to amazon.com today.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一方面是预测模型，另一方面是优化模型之间的区别。所以这两者往往是相辅相成的。首先，我们有一个模拟模型。模拟模型是在说，如果我们把特里·普拉切特的书放在亚马逊的首页上给杰里米·霍华德看，会发生什么。他有94%的概率会购买。这告诉我们，通过这个杠杆，我今天应该在杰里米的首页上放什么，我们说好，把特里·普拉切特放在首页上的不同设置会产生最高的模拟结果。然后这就是最大化我们从杰里米今天访问亚马逊网站中的利润的事情。
- en: Generally speaking, your predictive models feed into this simulation model but
    you kind of have to think about how they all work together. For example, let’s
    go back to churn. So it turned out that Jeremy Howard is very likely to leave
    his cell phone company next month. What are we going to about it? Let’s call him.
    And I can tell you if my cell phone company calls me right now and says “just
    calling to say we love you” I’d be like I’m cancelling right now. That would be
    a terrible idea. So again, you would want a simulation model that says what’s
    the probability that Jeremy is going to change his behavior as a result of calling
    him right now. So one of the levers I have is call him. On the other hand, if
    I got a piece of mail tomorrow that said for each month you stay with us, we’re
    going to give you a hundred thousand dollars. Then that’s going to definitely
    change my behavior, right? But then feeding that into the simulation model, it
    turns out that overall that would be an unprofitable choice to make. Do you see
    how this fits in together?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，你的预测模型会输入到这个模拟模型中，但你必须考虑它们如何共同工作。例如，让我们回到流失问题。结果表明，Jeremy Howard很可能会在下个月离开他的手机公司。我们该怎么办？让我们给他打电话。我可以告诉你，如果我的手机公司现在给我打电话说“只是打电话告诉你我们爱你”，我会立刻取消。那将是一个糟糕的主意。因此，你会想要一个模拟模型，来说Jeremy现在接到电话后改变行为的概率是多少。所以我有一个杠杆是给他打电话。另一方面，如果明天我收到一封信，说每个月你和我们在一起，我们会给你十万美元。那肯定会改变我的行为，对吧？但是将这个输入到模拟模型中，结果是这将是一个不盈利的选择。你看到这是如何相互配合的吗？
- en: So when we look at something like churn, we want to be thinking what are the
    levers we can pull [[14:33](https://youtu.be/BFIYUvBRTpE?t=14m33s)]. What are
    the kinds of models that we could build with what kinds of data to help us pull
    those levers better to achieve our objectives. When you think about it that way,
    you realize that the vast majority of these applications are not largely about
    a predictive model at all. They are about interpretation. They are about understanding
    what happens if. So if we take the intersection between on the one hand, here
    are all the levers that we could pull (here are all the things we can do) and
    then here are all of the features from our random forest feature importance that
    turn out to be strong drivers of the outcome. So then the intersection of those
    is here are the levers we could pull that actually matter. Because if you can’t
    change the thing, that is not very interesting. And if it’s not actually a significant
    driver, it’s not very interesting. So we can actually use our random forest feature
    importance to tell us what can we actually do to make a difference. Then we can
    use the partial dependence to actually build this kind of simulation model to
    say okay if we did change that, what would happen.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当我们看流失这样的问题时，我们要考虑我们可以拉动的杠杆是什么。我们可以用什么样的数据构建什么样的模型来帮助我们更好地拉动这些杠杆以实现我们的目标。当你这样思考时，你会意识到这些应用的绝大部分实际上并不是关于预测模型。它们是关于解释的。它们是关于理解“如果发生了什么”。因此，我们可以实际使用我们的随机森林特征重要性告诉我们我们实际上可以做些什么来产生影响。然后我们可以使用部分依赖来构建这种模拟模型，来说如果我们改变了那个，会发生什么。
- en: So there are lots of examples and what I want you to think about as you think
    about the machine learning problems you are working on is why does somebody care
    about this [[16:02](https://youtu.be/BFIYUvBRTpE?t=16m2s)]. What would a good
    answer to them look like and how could you actually positively impact this business.
    So if you are creating a Kaggle kernel, try to think about from the point of view
    of the competition organizer. What would they want to know and how can you give
    them that information. So something like fraud detection on the other hand, you
    probably just basically want to know whose fraudulent. So you probably do just
    care about the predictive model. But then you do have to think carefully about
    the data availability here. So okay, we need to know who is fraudulent at the
    point that we are about to deliver them a product. So it’s no point looking at
    data that’s available a month later, for instance. So you have this key issue
    of thinking about the actual operational constraints that you are working under.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有很多例子，当你思考你正在处理的机器学习问题时，我希望你考虑为什么有人会关心这个问题。对他们来说一个好的答案是什么样的，你如何实际上对这个业务产生积极影响。所以如果你在创建一个Kaggle内核，试着从竞赛组织者的角度思考。他们想知道什么，你如何给他们这些信息。另一方面，像欺诈检测，你可能只是想知道谁是欺诈的。所以你可能只关心预测模型。但是你必须仔细考虑这里的数据可用性。所以好吧，我们需要知道在我们即将向他们交付产品时谁是欺诈的。例如，查看一个月后可用的数据是没有意义的。所以你必须考虑你正在工作的实际运营约束。
- en: Human Resources Applications [[17:17](https://youtu.be/BFIYUvBRTpE?t=17m17s)]
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人力资源应用。
- en: Lots of interesting application in human resources but like employee churn,
    it’s another kind of churn model where finding out that Jeremy Howard is sick
    of lecturing, he’s going to leave tomorrow. What are you going to do about it?
    Well, knowing that wouldn’t actually be helpful. It would be too late. You would
    actually want a model that said what kinds of people are leaving USF and it turns
    out that everybody that goes to the downstairs cafe leaves USF. I guess their
    food is awful or whatever. Or everybody that we are paying less than half a million
    dollars a year is leaving USF because they can’t afford basic housing in San Francisco.
    So you could use your employee churn model not so much to say which employees
    hate us but why do employees leave. Again it’s really the interpretation there
    that matters.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在人力资源领域有很多有趣的应用，比如员工流失，这是另一种流失模型，其中发现杰里米·霍华德已经厌倦了讲课，他明天就要离开了。你会怎么做？知道这个事实实际上并不会有帮助。那将会太迟了。你实际上想要一个模型，告诉你什么样的人会离开USF，结果发现每个去楼下咖啡厅的人都会离开USF。我猜他们的食物很糟糕或者其他什么原因。或者我们支付不到50万美元一年的人都会离开USF，因为他们无法负担旧金山的基本住房。因此，你可以使用员工流失模型，不是为了知道哪些员工讨厌我们，而是为什么员工离开。再次强调，真正重要的是解释。
- en: '**Question**: For churn model, it sounds like there are two predictors that
    you need to predict for — one being churn and the other you need to optimize your
    profit. So how does it work [[18:30](https://youtu.be/BFIYUvBRTpE?t=18m30s)]?
    Yes, exactly. So this is what the simulation model is all about. You figure out
    this objective we are trying to maximize which is company profitability. You can
    create a pretty simple Excel model or something that says here is the revenue
    and here is the costs and the cost is equal to the number of people we employ
    multiplied by their salary, etc. Inside that Excel model, there are certain cells/inputs
    that are kind of stochastic or uncertain. But we could predict it with a model
    and so that’s what I do then is to say okay we need a predictive model for how
    likely somebody is to stay if we change their salary, how likely they are to leave
    with the current salary, how likely they are to leave next year if I increased
    their salary now, etc. So you a bunch of different models and then you can bind
    them together with simple business logic and then you can optimize that. You can
    then say okay if I pay Jeremy Howard half a million dollars, that’s probably a
    really good idea and if I pay him less then it’s probably not or whatever. You
    can figure out the overall impact. So it’s really shocking to me how few people
    do this. But most people in industry measure their models using AUC or RMSE or
    whatever which is never actually what you really want.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：对于流失模型，听起来你需要预测两个预测因子——一个是流失，另一个是你需要优化你的利润。那么它是如何工作的[[18:30](https://youtu.be/BFIYUvBRTpE?t=18m30s)]？是的，确切地说，这就是模拟模型的全部内容。你找出我们试图最大化的目标，即公司的盈利能力。你可以创建一个相当简单的Excel模型或其他模型，它说这是收入，这是成本，成本等于我们雇佣的人数乘以他们的工资等。在这个Excel模型中，有一些单元格/输入是随机的或不确定的。但我们可以用模型来预测，这就是我要做的，我要说好，我们需要一个预测模型，来预测如果我们改变他们的工资，某人留下的可能性有多大，如果我现在增加他们的工资，明年他们离开的可能性有多大等。因此，你需要一堆不同的模型，然后你可以用简单的商业逻辑将它们绑定在一起，然后进行优化。然后你可以说，如果我给杰里米·霍华德50万美元，那可能是一个非常好的主意，如果我付给他更少，那可能就不是了，或者其他什么。你可以找出整体影响。所以我真的很惊讶，为什么这么少的人这样做。但大多数行业中的人用AUC或RMSE等来衡量他们的模型，这实际上并不是你真正想要的。'
- en: More Horizontal Applications…[[22:04](https://youtu.be/BFIYUvBRTpE?t=22m4s)]
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多水平应用...[[22:04](https://youtu.be/BFIYUvBRTpE?t=22m4s)]
- en: Lead prioritization is a really interesting one. Every one of these boxes I’m
    showing, you can generally find a company or many companies whose sole job in
    life is to build models of that thing. So there are lots of companies that sell
    lead prioritization systems but again the question is how would we use that information.
    So if it’s like our best lead is Jeremy, he is a highest probability of buying.
    Does that mean I should send a salesperson out to Jeremy or I shouldn’t? If he’s
    highly probable to buy, why I waste my time with him. So again, you really want
    some kind of simulation that says what’s the likely change in Jeremy’s behavior
    if I send my best salesperson out to go and encourage him to sign. I think there
    are many many opportunities for data scientists in the world today to move beyond
    predictive modeling to actually bringing it all together.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在客户优先级是一个非常有趣的领域。我展示的每一个方框，通常都可以找到一家或多家公司，他们的唯一工作就是构建该领域的模型。因此，有很多公司销售潜在客户优先级系统，但问题是我们如何利用这些信息。如果我们的最佳潜在客户是杰里米，他是最有可能购买的人。这意味着我应该派一个销售人员去找杰里米，还是不应该？如果他很有可能购买，为什么我要浪费时间呢。因此，你真的需要一种模拟，来告诉你如果我派出最好的销售人员去鼓励他签约，杰里米的行为可能会发生什么变化。我认为今天世界上有很多机会让数据科学家不仅仅局限于预测建模，而是将所有内容整合在一起。
- en: Vertical Applications [[23:29](https://youtu.be/BFIYUvBRTpE?t=23m29s)]
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垂直应用[[23:29](https://youtu.be/BFIYUvBRTpE?t=23m29s)]
- en: As well as these horizontal applications that basically apply to every company,
    there’s a whole bunch of applications that are specific to every part of the world.
    For those of you that end up in healthcare, some of you will become experts in
    one or more of these areas. Like readmission risk. So what’s the probability that
    this patient is going to come back to the hospital. Depending on the details of
    the jurisdiction, it can be a disaster for hospitals when somebody is readmitted.
    If you find out that this patient has a high probability of readmission, what
    do you do about it? Again, the predictive model is helpful of itself. It rather
    suggests we shouldn’t send them home yet because they are going to come back.
    But wouldn’t it be nice if we had the tree interpreter and it said to us the reason
    that they are at high risk is because we don’t have a recent EKG/ECG for them.
    Without a recent EKG, we can’t have a high confidence about their cardiac health.
    In which case, it wouldn’t be like let’s keep them in the hospital for two weeks,
    it’ll be let’s give them an EKG. So this is interaction between interpretation
    and predictive accuracy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本适用于每家公司的横向应用之外，还有许多应用程序是针对世界各地的每个部分特定的。对于那些最终进入医疗保健领域的人，你们中的一些人将成为这些领域的专家之一。比如再入院风险。那么这位患者再次入院的概率是多少呢？根据司法管辖区的细节，当有人再次入院时，这可能对医院造成灾难。如果你发现这位患者有高再入院的可能性，你会怎么做？再次，预测模型本身是有帮助的。它更多地暗示我们不应该立即让他们回家，因为他们会再次入院。但如果我们有树解释器，并且它告诉我们，他们高风险的原因是因为我们没有最近的心电图。没有最近的心电图，我们就无法对他们的心脏健康有高度的信心。在这种情况下，我们不会说让他们在医院呆两周，而是让他们做一个心电图。因此，这是解释和预测准确性之间的互动。
- en: '**Question**: So what I’m understanding you are saying is that the predictive
    models are a really great but in order to actually answer these questions, we
    really need to focus on the interpretability of these models [[24:59](https://youtu.be/BFIYUvBRTpE?t=24m59s)]?
    Yeah, I think so. More specifically I’m saying we just learnt a whole raft of
    random forest interpretation techniques and so I’m trying to justify why. The
    reason why is because I’d say most of the time the interpretation is the thing
    we care about. You can create a chart or a table without machine learning and
    indeed that’s how most of the world works. Most managers build all kinds of tables
    and charts without any machine learning behind them. But they often make terrible
    decisions because they don’t know the feature importance of the objective they
    are interested in and so the table they create is of things that actually are
    the least important things anyway. Or they just do a univariate chart rather than
    a partial dependence plot, so they don’t actually realize that the relationship
    they thought they are looking at is due entirely to something else. So I’m kind
    of arguing for data scientists getting much more deeply involved in strategy and
    in trying to use machine learning to really help a business with all of its objectives.
    There are companies like dunnhumby which is a huge company that does nothing but
    retail application with machine learning. I believe there’s like a dunnhumby product
    you can buy which will help you figure out if I put my new store in this location
    versus that location, how many people are going to shop there. Or if I put my
    diapers in this part of the shop versus that part of the shop, how is that going
    to impact purchasing behavior, etc. So it’s also good to realize that the subset
    of machine learning applications you tend to hear about in the tech press or whatever
    is this massively biased tiny subset of stuff which Google and Facebook do. Where
    else the vast majority of stuff that actually makes the world go around is these
    kinds of applications that actually help people make things, buy things, sell
    things, build things, so forth.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所以我理解你的意思是，预测模型确实很棒，但为了真正回答这些问题，我们确实需要专注于这些模型的可解释性？是的，我想是这样。更具体地说，我正在说我们刚刚学习了一整套随机森林解释技术，所以我正在试图证明为什么。原因是因为我会说大多数时候解释是我们关心的事情。你可以创建一个图表或表格而不需要机器学习，实际上这就是大多数世界的工作方式。大多数经理们在没有任何机器学习的情况下构建各种表格和图表。但他们经常做出糟糕的决定，因为他们不知道他们感兴趣的目标的特征重要性，所以他们创建的表格实际上是那些最不重要的东西。或者他们只是做一个单变量图表，而不是一个部分依赖图，所以他们实际上没有意识到他们认为自己在看的关系完全是由其他因素造成的。所以我在争论数据科学家应该更深入地参与战略，并尝试使用机器学习来真正帮助企业实现所有目标。有一些公司像dunnhumby这样的公司，他们什么都不做，只做零售应用的机器学习。我相信有一种dunnhumby产品可以帮助你弄清楚，如果我把我的新店放在这个位置而不是那个位置，有多少人会在那里购物。或者如果我把尿布放在商店的这个部分而不是那个部分，这将如何影响购买行为等等。因此，也很重要意识到，在技术媒体或其他地方你经常听到的机器学习应用的子集是这种极其偏见的微小子集，谷歌和Facebook做的就是这种。而实际上让世界运转的绝大部分应用是这些实际上帮助人们制造东西、购买东西、销售东西、建造东西等等的应用。
- en: '**Question**: About tree interpretation, we looked at which feature was more
    important for a particular observation. For businesses, they have a huge amount
    of data and they want this interpretation for a lot of observations so how do
    they automate it? Do they set threshold [[27:50](https://youtu.be/BFIYUvBRTpE?t=27m50s)]?
    The vast majority of machine learning models don’t automate anything. They are
    designed to provide information to humans. So for example, if you are a customer
    service phone operator for an insurance company and your customer asks you why
    is my renewal $500 more expensive than last time, then hopefully the insurance
    company provides in your terminal those little screen that shows the result of
    the tree interpreter or whatever. So you can jump there and tell the customer
    that last year you were in this different zip code which has lower amounts of
    car theft, and this year also you’ve actually changed your vehicle to more expensive
    one. So it’s not so much about thresholds and automation, but about making these
    model outputs available to the decision makers in the organization whether they
    be at the top strategic level of like are we going to shutdown this whole product
    or not, all the way to the operational level of that individual discussion with
    a customer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：关于树的解释，我们看了哪个特征对于特定观察结果更重要。对于企业来说，他们有大量数据，他们希望对很多观察结果进行这种解释，那么他们如何自动化呢？他们设置阈值吗[[27:50](https://youtu.be/BFIYUvBRTpE?t=27m50s)]？绝大多数机器学习模型并不自动化任何东西。它们被设计为向人类提供信息。所以例如，如果你是一个保险公司的客服电话操作员，你的客户问你为什么我的续保费比上次贵了500美元，那么希望保险公司在你的终端提供那些显示树解释结果的小屏幕。这样你就可以跳过去告诉客户，去年你住在一个车辆被盗率较低的邮政编码区，而今年你还把车换成了更贵的车。所以这并不是关于阈值和自动化，而是关于让这些模型输出对组织中的决策者可用，无论是在顶层战略层面，比如我们是否要关闭整个产品，还是在操作层面，比如与客户进行个别讨论。'
- en: So another example is aircraft scheduling and gate management. There’s lots
    of companies that do that and basically what happens is that there are people
    at an airport whose job it is to basically tell each aircraft what gate to go
    to, to figure out when to close the doors, stuff like that. So the idea is you’re
    giving them software which has the information they need to make good decisions.
    So the machine learning models end up embedded in that software to say okay that
    plane that’s currently coming in from Miami, there’s a 48% chance that it’s going
    to be over 5 minutes late and if it does then this is going to be the knock-on
    impact through the rest of the terminal, for instance. So that’s how these things
    fit together.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是飞机调度和登机口管理。有很多公司在做这个，基本上是有人在机场，他们的工作是告诉每架飞机去哪个登机口，什么时候关闭舱门等等。所以这个想法是给他们一个软件，里面有他们需要做出良好决策所需的信息。所以机器学习模型最终嵌入在那个软件中，比如说，那架目前从迈阿密飞来的飞机，有48%的概率会晚5分钟以上，如果晚了，那么整个航站楼会受到影响，例如。这就是这些东西是如何结合在一起的。
- en: Other applications [[31:02](https://youtu.be/BFIYUvBRTpE?t=31m2s)]
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他应用[[31:02](https://youtu.be/BFIYUvBRTpE?t=31m2s)]
- en: '![](../Images/92404da0b0fdf953b4c60b9f1a454c48.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/92404da0b0fdf953b4c60b9f1a454c48.png)'
- en: There are lots of applications, and what I want you to do is to spend some time
    thinking about them. Sit down with one of your friends and talk about a few examples.
    For example, how would we go about doing failure analysis in manufacturing, who
    would be doing that, why would they be doing it, what kind of models might they
    use, what kind of data might they use. Start to practice and get a sense. Then
    when you’re at the workplace and talking to managers, you want to be straightaway
    able to recognize that the person you are talking to — what are they trying to
    achieve, what are the levers they have to pull, what are the data they have available
    to pull those levers to achieve that thing, and therefore how could we build models
    to help them do that and what kind of predictions would they have to be making.
    So then you can have this really thoughtful empathetic conversation with those
    people and then saying “in order to reduce the number of customers that are leaving,
    I guess you are trying to figure out who should you be providing better pricing
    to” and so forth.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多应用，我希望你花一些时间去思考它们。和你的朋友坐下来，谈论一些例子。比如说，我们如何进行制造业的故障分析，谁会做这个，为什么会做这个，他们可能会使用什么样的模型，可能会使用什么样的数据。开始练习并获得感觉。然后当你在工作场所和经理们交谈时，你希望能立即认识到你正在交谈的人——他们想要实现什么，他们有哪些杠杆可以拉动，他们有哪些数据可用来拉动这些杠杆以实现那个目标，因此我们如何构建模型来帮助他们做到这一点，他们可能需要做出什么样的预测。这样你就可以与这些人进行深思熟虑的共情对话，然后说“为了减少离开的客户数量，我猜你正在努力找出应该给谁提供更好的定价”等等。
- en: '**Question**: Are explanatory problems people are faced with in social sciences
    something machine learning can be useful for or is used for or is that nor really
    the realm that’s in [[32:29](https://youtu.be/BFIYUvBRTpE?t=32m29s)]? I’ve had
    a lot of conversations about this with people in social sciences and currently
    machine learning is not well applied in economics or psychology or whatever on
    the whole. But I’m convinced it can be for the exact reasons we are talking about.
    So if you are going to try to do some kind of behavioral economics and you’re
    trying to understand why some people behave differently to other people, a random
    forest with a feature importance plot would be a great way to start. More interestingly,
    if you are trying to do some kind of sociology experiment or analysis based on
    a large social network dataset where you have an observational study, you really
    want to try and pull out all of the sources of exogenous variables (i.e. all the
    stuff that’s going on outside) so if you use a partial dependence plot with a
    random forest that happens automatically. I actually gave a talk at MIT a couple
    of years ago for the first conference on digital experimentation which was really
    talking about how do we experiment in things like social networks in these digital
    environments and economists all do things with classic statistical tests but in
    this case, the economists I talked to were absolutely fascinated by this and they
    actually asked me to give an introduction to machine learning session at MIT to
    these various faculty and graduate folks in the economics department. And some
    of those folks have gone on to write some pretty famous books and so hopefully
    it’s been useful. It’s definitely early days but it’s a big opportunity. But as
    Yannet says, there’s plenty of skepticism still out there. The skepticism comes
    from unfamiliarity basically with this totally different approach. So if you spent
    20 years studying econometrics and somebody comes along and says here is a totally
    different approach to all the stuff econometricians do, naturally your first reaction
    will be “prove it”. So that’s fair enough but I think over time the next generation
    of people who are growing up with machine learning, some of them will move into
    the social sciences, they’ll make huge impacts that nobody has ever managed to
    make before and people will start going wow. Just like happened in computer vision.
    When computer vision spent a long time of people saying “maybe you should use
    deep learning for computer vision” and everybody in computer vision said “Prove
    it. We have decades of work on amazing feature detectors for computer vision.”
    And then finally in 2012, Hinton and Kryzanski came along and said “our model
    is twice as good as yours and we’ve only just started on this” and everybody was
    convinced. Nowadays every computer vision researchers basically uses deep learning.
    So I think that time will come in this area too.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：社会科学中人们面临的解释问题是否可以使用机器学习或者已经被使用，或者这并不是真正的领域[[32:29](https://youtu.be/BFIYUvBRTpE?t=32m29s)]？我与社会科学领域的人们进行了很多关于这个问题的讨论，目前机器学习在经济学或心理学等领域并没有得到很好的应用。但我相信它可以，原因正如我们所讨论的那样。因此，如果您要尝试进行某种行为经济学研究，并且试图理解为什么有些人的行为与其他人不同，使用具有特征重要性图的随机森林将是一个很好的开始。更有趣的是，如果您尝试进行某种基于大型社交网络数据集的社会学实验或分析，在那里您进行了一项观察性研究，您真的想要尝试提取所有外生变量的来源（即所有外部发生的事情），因此如果您使用具有随机森林的部分依赖图，这将自动发生。几年前，我在麻省理工学院做了一个关于数字实验的第一次会议的演讲，这次会议真正讨论了我们如何在诸如社交网络等数字环境中进行实验，经济学家们都使用经典的统计检验方法，但在这种情况下，我与之交谈的经济学家们对此非常着迷，他们实际上要求我在麻省理工学院为经济学系的各种教员和研究生们举办一个机器学习入门课程。其中一些人已经写了一些相当有名的书籍，希望这对他们有所帮助。现在还处于早期阶段，但这是一个巨大的机会。但正如Yannet所说，仍然存在很多怀疑。这种怀疑主要来自对这种完全不同方法的陌生感。因此，如果您花了20年时间研究计量经济学，然后有人过来说这是一种完全不同于计量经济学家所做的所有事情的方法，那么您的第一反应自然会是“证明它”。这是公平的，但我认为随着时间的推移，下一代与机器学习一起成长的人们中，一些人将进入社会科学领域，他们将产生前所未有的巨大影响，人们将开始感到惊讶。就像计算机视觉中发生的一样。当计算机视觉花了很长时间的人们说“也许你应该使用深度学习来进行计算机视觉”，而计算机视觉领域的每个人都说“证明它。我们在计算机视觉中有几十年的工作，开发了令人惊叹的特征检测器。”然后在2012年，辛顿和克里赞斯基出现了，他们说“我们的模型比你们的好两倍，而我们刚刚开始”
    ，每个人都被说服了。如今，几乎每个计算机视觉研究人员基本上都使用深度学习。因此，我认为在这个领域也会出现这样的时刻。'
- en: Different random forest interpretation methods [[37:17](https://youtu.be/BFIYUvBRTpE?t=37m17s)]
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林解释方法[[37:17](https://youtu.be/BFIYUvBRTpE?t=37m17s)]
- en: Having talked about why they are important, let’s now remind ourselves what
    they are.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在谈论它们为什么重要之后，让我们现在提醒自己它们是什么。
- en: '**Confidence based on tree variance**'
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**基于树方差的置信度**'
- en: What does it tell us? Why would be interested in that? How is it calculated?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们什么？为什么我们对此感兴趣？它是如何计算的？
- en: The variance of the predictions of the trees. Normally the prediction is just
    the average, this is variance of the trees.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 树的预测方差。通常预测只是平均值，这是树的方差。
- en: Just to fill in a detail here, what we generally do here is we take just one
    row/observation often and find out how confident we are about that (i.e. how much
    variance there are in the trees for that) or we can do as we did here for different
    groups [[39:34](https://youtu.be/BFIYUvBRTpE?t=39m34s)].
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里填充一个细节，我们通常只取一行/观察结果，然后找出我们对此有多自信（即树中有多少方差）或者我们可以像我们在这里做的那样为不同的组找出答案[[39:34](https://youtu.be/BFIYUvBRTpE?t=39m34s)]。
- en: '![](../Images/59eb1771fe8d8082cee64809651cfd10.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/59eb1771fe8d8082cee64809651cfd10.png)'
- en: What we’ve done here is to say if there are any groups that we are very unconfident
    (which could be due to very little observations). Something that I think is even
    more important would be when you are using this operationally. Let’s say you are
    doing a credit decisioning algorithm. So we are trying to determine whether Jeremy
    is a good risk or a bad risk. Should we loan him a million dollars. And the random
    forest says “I think he’s a good risk but I’m not at all confident.” And in which
    case, we might say okay maybe I shouldn’t give him a million dollars. Where else,
    if the random forest said “I think he’s a good risk and I’m very sure of that”
    then we are much more comfortable giving him a million dollars. And I’m a very
    good risk. So feel free to give me a million dollars. I checked the random forest
    before — a different notebook. Not in the repo 😆
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: It’s quite hard for me to give you folks direct experience with this kind of
    single observation interpretation because it’s really the kind of stuff that you
    actually need to be putting out to the front line [[41:30](https://youtu.be/BFIYUvBRTpE?t=41m30s)].
    It’s not something which you can really use so much in a Kaggle context but it’s
    more like if you are actually putting out some algorithm which is making big decisions
    that could cost a lot of money, you probably don’t so much care about the average
    prediction of the random forest but maybe you actually care about the average
    minus a couple standard deviations (i.e. what’s the worst-case prediction). Maybe
    there is a whole group that we are unconfident about, so that’s confidence based
    on tree variance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance [[42:36](https://youtu.be/BFIYUvBRTpE?t=42m36s)]
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Student: It’s basically to find out which features are important. You take
    each feature and shuffle the values in the feature and check how the predictions
    change. If it’s very different, it means that the feature was actually important;
    otherwise it is not that important.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: That was terrific. That was all exactly right. There were some details
    that were skimmed over a little bit. Anybody else wants to jump into a more detailed
    description of how it’s calculated? How exactly do we calculate feature importance
    for a particular feature?'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'Student: After you are done building a random forest model, you take each column
    and randomly shuffle it. And you run a prediction and check the validation score.
    If it gets bad after shuffling one of the columns, that means that column was
    important, so it has a higher importance. I’m not exactly sure how we quantify
    the feature importance.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Ok, great. Do you know how we quantify the feature importance? That
    was a great description. To quantify, we can take the difference in R² or score
    of some sort. So let’s say we’ve got our dependent variable which is price, and
    there’s a bunch of independent variables including year made [[44:22](https://youtu.be/BFIYUvBRTpE?t=44m22s)].
    We use the whole lot to build a random forest and that gives us our predictions.
    The we can compare that to get R², RMSE, whatever you are interested in from the
    model.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/757a2db0f725331a1679cc81ca3e81e8.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Now the key thing here is I don’t want to have to retrain my whole random forest.
    That’s slow and boring, so using the existing random forests. How can I figure
    out how important year made was? So the suggestion was, let’s randomly shuffle
    the whole column. Now that column is totally useless. it’s got the same mean,
    same distribution. Everything about it is the same, but there’s no connection
    at all between actual year made and what’s now in that column. I’ve randomly shuffled
    it. So now I put that new version through the same random forest (so there is
    no retraining done) to get some new ŷ (ym). Then I can compare that to my actuals
    to get RMSE (ym). So now I can start to create a little table where I got the
    original RMSE (3, for example), with YearMade scrambled with RMSE of 2\. Enclosure
    scrambled had RMSE of 2.5\. Then I just take these differences. For YearMade,
    the importance is 1, Enclosure is 0.5, and so forth. How much worse did my model
    get after I shuffled that variable.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在关键的是我不想重新训练整个随机森林。那太慢又无聊了，所以使用现有的随机森林。我如何找出年份的重要性呢？建议是，让我们随机洗牌整个列。现在那一列完全没用了。它的均值、分布都是一样的。关于它的一切都是一样的，但实际年份制造和现在那一列之间根本没有联系。我已经随机洗牌了。所以现在我把这个新版本放到同一个随机森林中（所以没有重新训练），得到一些新的ŷ（ym）。然后我可以将其与实际值进行比较，得到RMSE（ym）。所以现在我可以开始创建一个小表，我有原始RMSE（例如为3），年份制造混乱的RMSE为2。围栏混乱的RMSE为2.5。然后我只需要取这些差值。对于年份制造，重要性为1，围栏为0.5，等等。在我洗牌了那个变量之后，我的模型变得更糟了多少。
- en: '![](../Images/f5e68179bef47f6076fba1241e13c957.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5e68179bef47f6076fba1241e13c957.png)'
- en: '**Question**: Would all importances sum to one [[46:52](https://youtu.be/BFIYUvBRTpE?t=46m52s)]?
    Honestly, I’ve never actually looked at what the units are, so I’m not quite sure.
    We can check it out during the week if somebody’s interested. Have a look at sklearn
    code and see exactly what those units of measures are because I’ve never bothered
    to check. Although I don’t check like the units of measure specifically, what
    I do check is the relative importance. Here is an example.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：所有的重要性加起来会等于一吗？老实说，我从来没有看过单位是什么，所以我不太确定。如果有人感兴趣，我们可以在这周内查看一下。看一下sklearn的代码，看看这些度量单位到底是什么，因为我从来没有费心去检查。虽然我不会专门检查度量单位，但我会检查相对重要性。这里有一个例子。
- en: '![](../Images/497e79ef0d22bc97ee5ccdbda4b72eb6.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/497e79ef0d22bc97ee5ccdbda4b72eb6.png)'
- en: So . rather than just saying what are the top ten, yesterday one of the practicum
    students asked me about a feature importance where they said “oh, I think these
    three are important” and I pointed out that the top one was thousand times more
    important than the second one. So look at the relative numbers here. So in that
    case, it’s like “no, don’t look at the top three, look at the one that’s a thousand
    times more important and ignore all the rest.” Your natural tendency is to want
    to be precise and careful, but this is where you need to override that and be
    very practical. This thing is a thousand times more important. Don’t spend any
    time on anything else. Then you can go and talk to your manager of your project
    and say this thing is a thousand times more important. And then they might say
    “oh, that was a mistake. It shouldn’t have been in there. We don’t actually have
    that information at the decision time or for whatever reason we can’t actually
    use that variable.” So then you could remove it and have a look. Or they might
    say “gosh, I had no idea that was by far more important than everything else put
    together. So let’s forget this random forest thing and just focus on understanding
    how we can better collect that one variable and better use that one variable.”
    So that’s something which comes up quite a lot and actually another place that
    came up just yesterday. Another practicum student asked me “I’m doing this medical
    diagnostics project and my R² is 0.95 for a disease which I was told is very hard
    to diagnose. Is this random forest genius or is something going wrong?” And I
    said remember, the second thing you do after you build a random forest is to do
    feature importance, so do feature importance and what you’ll probably find is
    that the top column is something that shouldn’t be there. So that’s what happened.
    He came back to me half an hour later, he said “yeah, I did the feature importance
    and you were right. The top column was basically a something that was another
    encoding of the dependent variable. I’ve removed it and now my R² is -0.1 so that’s
    an improvement.”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，与其只说前十名，昨天一个实习生问我一个特征重要性的问题，他们说“哦，我认为这三个很重要”，我指出排名第一的比第二个重要一千倍。所以看看这里的相对数字。所以在这种情况下，就像“不要看前三名，看那个重要一千倍的，忽略其他所有的。”你的自然倾向是想要准确和小心，但这就是你需要覆盖的地方，要非常实际。这个东西重要一千倍。不要花时间在其他任何事情上。然后你可以去和你的项目经理谈谈，告诉他这个东西重要一千倍。然后他们可能会说“哦，那是个错误。它不应该在那里。我们实际上在决策时没有那个信息，或者由于某种原因我们实际上不能使用那个变量。”那么你可以移除它并查看。或者他们可能会说“天哪，我完全不知道那比其他所有东西加起来都重要得多。所以让我们忘掉这个随机森林的东西，专注于理解如何更好地收集那个变量并更好地使用那个变量。”这是一个经常出现的情况，实际上昨天刚刚发生了另一个地方。另一个实习生问我“我正在做这个医学诊断项目，我的R²是0.95，这是一个据说很难诊断的疾病。这是随机森林天才还是出了什么问题？”我说记住，建立随机森林之后你要做的第二件事是进行特征重要性分析，所以进行特征重要性分析，你可能会发现排名第一的列是不应该在那里的。这就是发生的事情。他半小时后回到我这里，他说“是的，我做了特征重要性分析，你是对的。排名第一的列基本上是另一个对因变量的编码。我把它移除了，现在我的R²是-0.1，这是一个改进。”
- en: 'The other thing I like to look at is this chart [[50:03](https://youtu.be/BFIYUvBRTpE?t=50m3s)]:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢看的另一件事是这个图表：
- en: '![](../Images/41a1947a7212c132120f08c6f75844e1.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/41a1947a7212c132120f08c6f75844e1.png)'
- en: Basically it says where things flatten off in terms of which ones I should be
    really focusing on. So that’s the most important one. When I did credit scoring
    in telecommunications, I found there were nine variables that basically predicted
    very accurately who was going to end up paying for their phone and who wasn’t.
    Apart from ending up with a model that saved them three billion dollars a year
    in fraud and credit costs, it also let them basically rejig their process so they
    focused on collecting those nine variables much better.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上它说的是在哪些方面趋于平缓，我应该真正关注哪些方面。这是最重要的。当我在电信行业进行信用评分时，我发现有九个变量基本上可以准确预测谁最终会支付他们的电话费，谁不会。除了最终得到一个每年节省三十亿美元欺诈和信用成本的模型外，它还让他们基本上重新调整了他们的流程，以便更好地收集这九个变量。
- en: Partial dependence [[50:46](https://youtu.be/BFIYUvBRTpE?t=50m46s)]
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部分依赖 [[50:46](https://youtu.be/BFIYUvBRTpE?t=50m46s)]
- en: This is an interesting one. Very important but in some ways kind of tricky to
    think about.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的问题。非常重要，但在某种程度上有点难以理解。
- en: '![](../Images/c504b3d97823d5561d3b15cb9cc0d860.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c504b3d97823d5561d3b15cb9cc0d860.png)'
- en: 'Let’s come back to how we calculate this in a moment, but the first thing to
    realize is that the vast majority of the time, when somebody shows you a chart
    , it will be like a univariate chart that’ll just grab the data from the database
    and they’ll plot X against Y. Then managers have a tendency to want to make a
    decision. So it would be “oh, there’s this drop-off here, so we should stop dealing
    in equipment made between 1990 and 1995\. This is a big problem because real world
    data has lots of these interactions going on. So maybe there was a recession going
    on around the time that those things are being sold or maybe around that time,
    people were buying more of a different type of equipment. So generally what we
    actually want to know is all other things being equal, what’s the relationship
    between YearMade and SalePrice. Because if you think about the drivetrain approach
    idea of the levers, you really want a model that says if I change this lever,
    how will it change my objective. It’s by pulling them apart using partial dependence
    that you can say actually this is the relationship between YearMade and SalePrice
    all other things being equal:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍后再来看如何计算这个问题，但首先要意识到的是，绝大多数情况下，当有人向您展示一个图表时，它将是一个单变量图表，只会从数据库中获取数据，然后绘制X与Y。然后管理人员往往希望做出决策。所以可能会是“哦，这里有一个下降，所以我们应该停止处理1990年至1995年之间制造的设备”。这是一个大问题，因为现实世界的数据中有很多这样的相互作用。也许在那些东西被出售的时候正值经济衰退，或者也许在那个时候，人们更多地购买了不同类型的设备。因此，通常我们实际上想知道的是，在其他所有条件相等的情况下，YearMade和SalePrice之间的关系。因为如果您考虑到驱动器方法的杠杆思想，您真的希望一个模型说如果我改变这个杠杆，它将如何改变我的目标。通过使用部分依赖来分开它们，您可以说实际上这是YearMade和SalePrice之间的关系，在其他所有条件相等的情况下：
- en: '![](../Images/86606730a6d2d122fe044828ff5ab10d.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/86606730a6d2d122fe044828ff5ab10d.png)'
- en: So how do we calculate that?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何计算呢？
- en: 'Student: For the variable YearMade, for example, you keep all other variables
    constant. Then you are going to pass every single value of the YearMade, train
    the model after that. So for every model you’ll have light blue lines and the
    median is going to be the yellow line.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 学生：例如，对于变量YearMade，您保持所有其他变量不变。然后，您将传递YearMade的每个值，然后训练模型。因此，对于每个模型，您将有浅蓝色线条，中位数将是黄色线条。
- en: 'Jeremy: So let’s try and draw that. By “leave everything else constant”, what
    she means is leave them at whatever they are in the dataset. So just like when
    we did feature importance, we are going to leave the rest of the dataset as it
    is. And we’re going to do partial dependence plot for YearMade. So we’ve got all
    of these other rows of data that we will just leave as they are. Instead of randomly
    shuffling YearMade, what we are going to do is replace every single value with
    exactly the same thing — 1960\. Just like before, we now pass that through our
    existing random forests which we have not retrained or changed in any way to get
    back out a set of predictions `y1960`. Then we can plot that on a chart — YearMade
    against partial dependence.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：那么让我们尝试绘制出来。通过“保持其他一切不变”，她的意思是将它们保持为数据集中的任何值。就像我们进行特征重要性时一样，我们将保持数据集的其余部分不变。我们将对YearMade进行部分依赖图。因此，我们有所有这些其他数据行，我们将保持它们不变。与其随机洗牌YearMade，我们将用完全相同的东西——1960来替换每个值。就像以前一样，我们现在将通过我们尚未重新训练或更改的现有随机森林来传递这些数据，以获得一组预测`y1960`。然后我们可以在图表上绘制出来——YearMade与部分依赖。
- en: '![](../Images/b378ac029f436b5689fbe13b0d97b704.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/b378ac029f436b5689fbe13b0d97b704.png)'
- en: 'Now we can do that for 1961, 1962, 1963, and so forth. We can do that on average
    for all of them, or we could do it just for one of them. So when we do it for
    just one of them and we change its YearMade and pass that single thing through
    our model, that gives us one of these blue lines. So each one of these blue lines
    is a single row as we change its YearMade from 1960 up to 2008\. So then we can
    just take the median of all of these blue lines to say on average what’s the relationship
    between YearMade and price all other things being equal. Why is it that it works?
    Why is it that this process tells us the relationship between YearMade and price
    all other things being equal? Maybe it’s good to think about a really simplified
    approach [[56:03](https://youtu.be/BFIYUvBRTpE?t=56m3s)]. A really simplified
    would say what’s the average auction? What’s the average sale date, what’s the
    most common type of machine we well? Which location we mostly sell things? And
    we could come up with a single row that represents the average auction and then
    we could say okay, let’s run that row through the random forest but replace its
    YearMade with 1960 and then do it again with 1961 and we could plot those on our
    little chart. That would give us a version of the relationship between YearMade
    and sale price all other things being equal. But what if tractors looked like
    that and backhoe loaders looked like a flat line:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1021d977252b177e379b73c06bbf5dc3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: Then taking the average one would hide the fact that there are these totally
    different relationships. So instead, we basically say, okay our data tells us
    what kinds of things we tend to sell, who we tend to sell them, and when we tend
    to sell them, so let’s use that. Then we actually find out for every blue line,
    here are actual examples of these relationships. So then what we can do is as
    well as plotting the median, we can do a cluster analysis to find out a few different
    shapes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/9184d024b3bbfcbc3e1646e51ce21066.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: In this case, they all look pretty much the different versions of the same thing
    with different slopes, so my main takeaway from this would be that the relationship
    between sale price and year made is basically a straight line. And remember, this
    was a log of sale price so this is actually showing us an exponential. So this
    is where I would then bring in the domain expertise which is like “okay, things
    depreciate over time by a constant ratio so therefore, I would expect older stuff
    year made to have this exponential shape.” So this is where, as I mentioned, the
    very start of of my machine learning project, I generally try to avoid using as
    much domain expertise as I can and let the data do the talking. So one of the
    questions I got this morning was “there’s like a sale ID and model ID, I should
    throw those away, right? Because they are just IDs.” No. Don’t assume anything
    about your data. Leave them in and if they turn out to be super important predictors,
    you want to find out why that is. But then, now I’m at the other end of my project.
    I’ve done my feature importance, I’ve pulled out the stuff which is from that
    dendrogram (i.e. redundant features), I’m looking at the partial dependence and
    now I’m thinking okay is this shape what I expected? So even better, before you
    plot this, first of all think what shape would I expect this to be. Because it’s
    always easy to justify to yourself after the fact, oh, I knew it would look like
    this. So what shape you expect and then is it that shape? In this case, I’d say
    this is what I would expect. Where else the previous plot is not what I’d expect.
    So the partial dependence plot has really pulled out the underlying truth.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Question**: Say you have 20 features that are important, are you going to
    measure the partial dependence for every single one of them [[1:00:05](https://youtu.be/BFIYUvBRTpE?t=1h5s)]?
    If there are twenty features that are important, then I will do the partial dependence
    for all of them where important means like it’s a lever I can actually pull, the
    magnitude of its size is not much smaller than the other nineteen, you know, based
    on all these things it’s a feature I ought to care about then I will want to know
    how it’s related. It’s pretty unusual to have that many features that are important
    both operationally and from a modeling point of view in my experience.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：假设您有20个重要特征，您会为每一个特征测量偏依赖性吗？如果有20个重要特征，那么我将对所有这些特征进行偏依赖性分析，其中重要意味着它是一个我实际可以控制的杠杆，其大小的幅度与其他十九个特征的差异不大，您知道，基于所有这些因素，这是一个我应该关心的特征，那么我将想要了解它是如何相关的。在我的经验中，拥有这么多在操作和建模角度上都重要的特征是相当不寻常的。
- en: '**Question**: How do you define importance [[1:00:58](https://youtu.be/BFIYUvBRTpE?t=1h58s)]?
    Important means it’s a lever (i.e. something I can change) and it’s on the spiky
    end of this tail (left):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：您如何定义重要性？重要意味着它是一个杠杆（即我可以改变的东西），它位于这个尾巴（左侧）的尖端：
- en: '![](../Images/53c920589028f6ed9ccd09b796e7c005.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/53c920589028f6ed9ccd09b796e7c005.png)'
- en: Or maybe it’s not a lever directly. Maybe it’s like zip code and I can’t actually
    tell my customers where to live but I could focus my new marketing attention on
    a different zip code.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 或者它可能不是直接的杠杆。也许它像邮政编码一样，我实际上无法告诉我的客户在哪里居住，但我可以将我的新营销注意力集中在不同的邮政编码上。
- en: '**Question:** Would it make sense to do pairwise shuffling for every combination
    of two features and hold everything else constant in feature importance to see
    interactions and compare scores [[1:01:45](https://youtu.be/BFIYUvBRTpE?t=1h1m45s)]?
    You wouldn’t do that so much for partial dependence. I think your question is
    really getting to the question of could we do that for feature importance. I think
    interaction feature importance is a very important and interesting question. But
    doing it by randomly shuffling every pair of columns, if you’ve got a hundred
    columns, it sounds computationally intensive, possibly infeasible. So what I’m
    going to do is after we talk about tree interpreter, I’ll talk about interesting
    but largely unexplored approach that will probably work.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：对于每一对特征组合进行成对洗牌，保持其他所有内容不变，以查看交互作用并比较分数是否有意义？你不会为了偏依赖性而这样做。我认为您的问题实际上是在询问我们是否可以为特征重要性这样做。我认为交互特征重要性是一个非常重要且有趣的问题。但是通过随机洗牌每一对列来做到这一点，如果您有一百列，这听起来计算量很大，可能不可行。所以我要做的是在我们讨论树解释器之后，我将谈论一个有趣但在很大程度上未被探索的方法，这可能会起作用。
- en: Tree interpreter [[1:02:43](https://youtu.be/BFIYUvBRTpE?t=1h2m43s)]
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 树解释器
- en: 'Prince: I was thinking this to be more like feature importance, but feature
    importance is for complete random forest model, and this tree interpreter is for
    feature importance for particular observation. So let’s say it’a about hospital
    readmission. If a patient A is going to be readmitted to a hospital, which feature
    for that particular patient is going to impact and how can we change that. It
    is calculated starting from the prediction of mean then seeing how each feature
    is changing the behavior of that particular patient.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Prince：我认为这更像是特征重要性，但特征重要性是针对完整的随机森林模型，而这个树解释器是针对特定观察的特征重要性。所以让我们说这是关于医院再入院的。如果患者A将要再次入院，那么对于该特定患者，哪个特征会产生影响，我们如何改变这种情况。它是从平均预测开始计算，然后看每个特征如何改变该特定患者的行为。
- en: 'Jeremy: I’m smiling because that was one of the best examples of technical
    communication I’ve heard in a long time, so it’s really good to think about why
    was that effective. So what Prince did there was, he used as specific an example
    as possible. Humans are much less good at understanding abstractions. So if you
    say “it takes some kind of feature, and then there’s an observation in that feature”
    whereas it’s the hospital readmission. So we take a specific example. The other
    thing he did that was very effective was to take an analogy to something we already
    understand. So we already understand the idea of feature importance across all
    of the rows in a dataset. So now we are going to do it for a single row. So one
    of the things I was really hoping we would learn from this experience is how to
    become effective technical communicators. So that was a really great role model
    from Prince of using all the tricks we have at our disposal for effective technical
    communication. So hopefully you found that useful explanation. I don’t have a
    lot to add to that other than to show you what that looks like.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy：我在微笑，因为这是我很长时间以来听到的最好的技术沟通示例之一，所以思考为什么这么有效是非常有意义的。Prince所做的是，他尽可能具体地举例说明。人类在理解抽象概念方面要差得多。因此，如果您说“它需要某种特征，然后在该特征的观察中”，而不是医院再入院。因此，我们举了一个具体的例子。他做的另一件非常有效的事情是将类比与我们已经理解的东西联系起来。因此，我们已经理解了数据集中所有行的特征重要性的概念。因此，现在我们将为单个行执行此操作。因此，我真的希望我们从这次经验中学到如何成为有效的技术沟通者。因此，Prince在使用我们可以利用的所有技巧进行有效的技术沟通方面是一个非常好的榜样。希望您觉得这个解释有用。除了向您展示它是什么样子之外，我没有太多要补充。
- en: 'With the tree interpreter, we picked out a row [[1:04:56](https://youtu.be/BFIYUvBRTpE?t=1h4m56s)]:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用树解释器，我们挑选出一行：
- en: '![](../Images/d658c4c04011a022138d863c0783078f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d658c4c04011a022138d863c0783078f.png)'
- en: 'Remember when we talked about the confidence intervals at the very start (i.e.
    the confidence based on tree variance). We said you mainly use that for a row.
    So this would also be for a row. So it’s like “why is this patient likely to be
    readmitted?” Here is all the information we have about that patient or in this
    case this auction. Why is this auction so expensive? So then we call `ti.predict`
    and we get back the prediction of the price, the bias (i.e. the root of the tree
    — so this is just the average price for everybody so this is always going to be
    the same), and then the contributions which is how important is each of these
    things:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们在一开始谈到的置信区间吗（即基于树方差的置信度）？我们说你主要用于一行。所以这也是为一行。就像“为什么这个患者可能会再次入院？”这是我们关于该患者或在这种情况下该拍卖会的所有信息。为什么这个拍卖会这么贵？然后我们调用`ti.predict`，我们得到价格的预测，偏差（即树的根
    - 这只是每个人的平均价格，所以这总是一样的），然后是贡献，即这些事情有多重要：
- en: '![](../Images/79b64347e1a6748da52bfcc6491345af.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/79b64347e1a6748da52bfcc6491345af.png)'
- en: The way we calculated that was to say at the very start, the average price was
    10\. Then we split on enclosure. For those with this enclosure, the average was
    9.5\. Then we split on year made less than 1990 and for those with that year made,
    the average price was 9.7\. Then we split on the number of hours on the meter,
    and with this branch, we got 9.4.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算的方法是说一开始，平均价格是10。然后我们根据围栏进行分割。对于那些有这个围栏的人，平均价格是9.5。然后我们根据制造年份小于1990进行分割，对于那些有这个制造年份的人，平均价格是9.7。然后我们根据米数进行分割，对于这个分支，我们得到了9.4。
- en: '![](../Images/a858ec9b9fe155abec57b59cf261650b.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a858ec9b9fe155abec57b59cf261650b.png)'
- en: We then have a particular auction which we pass it through the tree. It just
    so happens that it takes the top most path. One row can only have one path through
    the tree. So we ended up at 9.4\. Then we can create a little table. As we go
    through, we start at the top and we start with 10 — that’s our bias. And we said
    enclosure resulted in a change from 10 to 9.5 (i.e. -0.5). Year made changed it
    from 9.5 to 9.7 (i.e. +0.2), then meter changed it from 9.7 down to 9.4 (-0.3).
    Then if we add all that together (10–0.5+0.2–0.3), lo and behold that’s the prediction.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有一个特定的拍卖会，我们通过树传递它。碰巧它走了最顶层的路径。一行只能通过树有一条路径。所以我们最终到了9.4。然后我们可以创建一个小表格。当我们逐步进行时，我们从顶部开始，我们从10开始
    - 这是我们的偏差。我们说围栏导致了从10变为9.5（即-0.5）。制造年份将其从9.5变为9.7（即+0.2），然后米数将其从9.7降至9.4（-0.3）。然后如果我们把所有这些加在一起（10-0.5+0.2-0.3），哎呀，那就是预测。
- en: '![](../Images/3429441276ef60a6ac385e04566fc25b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3429441276ef60a6ac385e04566fc25b.png)'
- en: 'Which takes us to our Excel spreadsheet [[1:08:07](https://youtu.be/BFIYUvBRTpE?t=1h8m7s)]:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们来到我们的Excel电子表格：
- en: '![](../Images/8297d56c24cd16b7f6c634579eadb8ac.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8297d56c24cd16b7f6c634579eadb8ac.png)'
- en: Last week, we have use Excel for this because there wasn’t a good Python library
    for doing waterfall charts. So we saw we got our starting point this is the bias,
    and then we had each of our contributions and we ended up with our total. The
    world is now a better place because Chris has created a Python waterfall chart
    module for us and put it on pip. So never again where we have to use Excel for
    this. I wanted to point out that waterfall charts have been very important in
    business communications at least as long as I’ve been in business — so that’s
    about 25 years. Python is maybe a couple of decades old. But despite that, no
    one in the Python world ever got to the point where they actually thought “you
    know, I’m gonna make a waterfall chart” so they didn’t exist until two days ago
    which is to say the world is full of stuff which ought to exist and doesn’t. And
    doesn’t necessarily take a heck a lot of time to build. It took Chris about 8
    hours, so a hefty amount but not unreasonable. And now forevermore people when
    they want the Python waterfall chart will end up at Chris’ Github repo and hopefully
    find lots of other USF contributors who have made it even better.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上周，我们使用Excel进行了这项工作，因为没有一个很好的Python库可以制作瀑布图。所以我们看到我们得到了我们的起点这是偏差，然后我们有我们每个贡献，最后我们得到了我们的总数。现在世界变得更美好了，因为Chris为我们创建了一个Python瀑布图模块，并将其放在pip上。所以我们再也不必使用Excel了。我想指出，瀑布图至少在我从事业务以来一直在商业沟通中非常重要
    - 大约25年了。Python可能已经有几十年的历史了。但尽管如此，Python世界中没有人真正想到“你知道，我要制作一个瀑布图”，所以直到两天前它们才存在，也就是说这个世界充满了应该存在但不存在的东西。而且并不一定需要花费很多时间来构建。Chris花了大约8个小时，所以数量相当可观但不过分。现在以后，当人们想要Python瀑布图时，他们将最终到达Chris的Github存储库，并希望找到许多其他美国大学的贡献者，他们使其变得更好。
- en: In order for you to help improve Chris’ Python waterfall, you need to know how
    to do that. So you are going to need to submit a pull request. Life becomes very
    easy for submitting pull requests if you use something called [hub](https://hub.github.com/).
    What they suggest you do is that you alias `git` to `hub` because it turns out
    that hub is actually a strict superset of git. What it lets you do is you can
    go `git fork`, `git push` , and `git pull-request` and you’ve now sent Chris a
    pull request. Without hub, this is actually a pain and requires like going to
    the website and filling in forms and stuff. So this gives you no reason not to
    do pull request. I mention this because when you are interviewing for a job, I
    can promise you that the person you are talking to will check your github and
    if they see you have a history of submitting thoughtful pull requests that are
    accepted to interesting libraries, that looks great. It looks great because it
    shows you’re somebody who actually contributes. It also shows that if they are
    being accepted that you know how to create code that fits with people’s coding
    standards, has appropriate documentation, passes their tests and coverage, and
    so forth. So when people look at you and they say oh, here is somebody with a
    history of successfully contributing, accepted pull requests to open-source libraries,
    that’s a great part of your portfolio. And you can specifically refer to it. So
    either I’m the person who build Python waterfall, here is my repo or I’m the person
    who contributed currency number formatting to Python waterfall, here is my pull
    request. Anytime you see something that doesn’t work right in any open source
    software you use, it is not a problem, it’s a great opportunity because you can
    fix it and send in the pull request. So give it a go. It actually feels great
    the first time you have a pull request accepted. And of course, one big opportunity
    is the fastai library. Thanks to one of our students, we now have docstrings for
    most of the `fastai.structured` library, again came via a pull request.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助改进Chris的Python瀑布，您需要知道如何做到这一点。因此，您需要提交一个拉取请求。如果您使用一个叫做[hub](https://hub.github.com/)的东西，那么提交拉取请求将变得非常容易。他们建议您将`git`别名为`hub`，因为事实证明，hub实际上是git的一个严格的超集。它让您可以执行`git
    fork`，`git push`和`git pull-request`，然后您现在已经向Chris发送了一个拉取请求。没有hub，这实际上是一种痛苦，需要像去网站并填写表格之类的事情。因此，这给您没有理由不提交拉取请求。我提到这一点是因为当您面试工作时，我可以向您保证，您正在与之交谈的人将检查您的github，如果他们看到您有提交经过深思熟虑的拉取请求并被接受到有趣的库，那看起来很棒。这看起来很棒，因为它表明您是一个实际做出贡献的人。它还表明，如果它们被接受，那么您知道如何创建符合人们编码标准、具有适当文档、通过测试和覆盖率等的代码。因此，当人们看着您并说哦，这里是一个有着成功贡献历史的人，接受了开源库的拉取请求，这是您作品集的一个很好的部分。您可以具体引用它。因此，无论是我是构建Python瀑布的人，这是我的存储库，还是我是为Python瀑布贡献货币数字格式化的人，这是我的拉取请求。每当您在使用任何开源软件时看到某些不正常的东西，这不是问题，而是一个很好的机会，因为您可以修复它并发送拉取请求。所以试一试。第一次有拉取请求被接受时，感觉真的很棒。当然，一个很大的机会是fastai库。由于我们的一位学生，我们现在对`fastai.structured`库的大部分文档字符串都有了，这也是通过拉取请求完成的。
- en: Does anybody have any questions about how to calculate any of these random forest
    interpretation methods or why we might want to use them [[1:12:50](https://youtu.be/BFIYUvBRTpE?t=1h12m50s)]?
    Towards the end of the week, you’re going to need to be able to build all of these
    yourself from scratch.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有人对如何计算这些随机森林解释方法或为什么我们可能想要使用它们有任何问题吗？在本周末，您将需要能够从头开始构建所有这些。
- en: '**Question:** Just looking at the tree interpreter, I noticed that some of
    the values are `nan` ’s. I get why you keep them in the tree but how can `nan`
    have a feature importance [[1:13:19](https://youtu.be/BFIYUvBRTpE?t=1h13m19s)]?
    Let me pass it back to you. Why not? So in other words, how is `nan` handled in
    Pandas and therefore in the tree? Does anybody remember, notice these are all
    in categorical variables, how does Pandas handle `nan` ’s in categorical variable
    and how does fastai deal with them? Pandas sets them to -1 category code and fastai
    adds one to all of the category code so it ends up being zero. In other words,
    remember by the time it hits the random forest it’s just a number, and it’s just
    zero. And we map it back to the descriptions back here. So the question really
    is why shouldn’t the random forest be able to split on zero? It’s just another
    number. So it could be `nan`, `high`, `medium`, `low`= 0, 1, 2, 3\. So missing
    values are one of these things that are generally taught really badly. Often people
    get taught here are some ways to remove columns with missing values or remove
    rows with missing values or to replace missing values. That’s never what we want
    because missingness is very very very often interesting. So we actually learnt
    that from our feature importance that coupler system `nan` is one of the most
    important features. For some reason, well, I could guess, right? Coupler system
    `nan` presumably means this is a kind of industrial equipment that doesn’t have
    a coupler system. Now I don’t know what kind that is, but apparently it’s more
    expensive kind.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：只是看着树解释器，我注意到一些值是`nan`。我明白为什么要保留它们在树中，但`nan`如何有特征重要性呢？让我把问题传递给你。为什么不呢？换句话说，Pandas中如何处理`nan`，因此在树中呢？有人记得，注意这些都是分类变量，Pandas如何处理分类变量中的`nan`，fastai又是如何处理的？Pandas将它们设置为-1类别代码，而fastai将所有类别代码加一，因此最终变为零。换句话说，记住，当它到达随机森林时，它只是一个数字，只是零。然后我们将其映射回这里的描述。所以问题实际上是为什么随机森林不能在零上分裂？它只是另一个数字。所以它可以是`nan`，`高`，`中`，`低`=
    0，1，2，3。因此，缺失值是通常教得很糟糕的事情之一。通常人们被教导要删除具有缺失值的列或删除具有缺失值的行，或者替换缺失值。这绝不是我们想要的，因为缺失通常是非常非常有趣的。因此，我们实际上从我们的特征重要性中学到，耦合器系统`nan`是最重要的特征之一。出于某种原因，嗯，我可以猜测，对吧？耦合器系统`nan`可能意味着这是一种没有耦合器系统的工业设备。现在我不知道是什么类型，但显然是更昂贵的类型。
- en: I did this competition for university grant research success where by far the
    most important predictors were whether or not some of the fields were null [[1:15:41](https://youtu.be/BFIYUvBRTpE?t=1h15m41s)].
    It turned out that this was data leakage that these fields only got filled in
    most of the time after a research grant was accepted. So it allowed me to win
    that Kaggle competition but didn’t actually help the university very much.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我参加了一个为大学拨款研究成功而举办的比赛，迄今为止最重要的预测因素是某些字段是否为空[[1:15:41](https://youtu.be/BFIYUvBRTpE?t=1h15m41s)]。结果表明，这是数据泄漏，这些字段大多数情况下只在研究拨款被接受后填写。所以这让我赢得了那个Kaggle比赛，但实际上并没有对大学有太大帮助。
- en: Extrapolation [[1:16:16](https://youtu.be/BFIYUvBRTpE?t=1h16m16s)]
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外推[[1:16:16](https://youtu.be/BFIYUvBRTpE?t=1h16m16s)]
- en: I am going to do something risky and dangerous which is we are going to do some
    live coding. The reason we are going to do some live coding is I want to explore
    extrapolation together with you, and I also want to give you a feel of how you
    might go about writing code quickly in this notebook environment. And this is
    the kind of stuff that you are going to need to be able to do in the real world
    and in the exam is quickly create the kind of code that we are going to talk about.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我要做一些冒险和危险的事情，我们将进行一些现场编码。我们要进行一些现场编码的原因是我想和你一起探索外推，我也想让你感受一下在这个笔记本环境中如何快速编写代码。这是你在现实世界和考试中需要做的事情，快速创建我们将要讨论的代码。
- en: I really like creating synthetic datasets anytime I’m trying to investigate
    the behavior of something because if I have a synthetic dataset, I know how it
    should behave.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我尝试调查某事的行为时，我都非常喜欢创建合成数据集，因为如果我有一个合成数据集，我知道它应该如何表现。
- en: Which reminds me, before we do this, I promised that we would talk about interaction
    importance and I just about forgot.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒我，我们在做这个之前，我承诺我们会谈论交互重要性，我差点忘了。
- en: Interaction importance [[1:17:24](https://youtu.be/BFIYUvBRTpE?t=1h17m24s)]
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交互重要性[[1:17:24](https://youtu.be/BFIYUvBRTpE?t=1h17m24s)]
- en: Tree interpreter tells us the contributions for a particular row based on the
    difference in the tree. We could calculate that for every row in our dataset and
    add them up. That would tell us feature importance. And it would tell us feature
    importance in a different way. One way of doing feature importance is by shuffling
    the columns one at a time. Another way is by doing tree interpreter for every
    row and adding them up. Neither is more right than the others. They are actually
    both quite widely used so this is kind of type 1 and type 2 feature importance.
    So we could try to expand this a little bit. To do not just single variable feature
    importance, but interaction feature importance. Now here is the thing. What I’m
    going to describe is very easy to describe. It was described by Breiman right
    back when random forests were first invented, and it is part of the commercial
    software product from Salford systems who have the trademark on random forests.
    But it is not part of any open source library I’m aware of, and I’ve never seen
    an academic paper that actually studies it closely. So what I’m going to describe
    here is a huge opportunity but it’s also like there’s lots and lots of details
    that need to be fleshed out. But here is the basic idea.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 树解释器告诉我们基于树中差异的特定行的贡献。我们可以为数据集中的每一行计算这个值并将它们相加。这将告诉我们特征的重要性。它将以不同的方式告诉我们特征的重要性。评估特征重要性的一种方法是逐个对列进行洗牌。另一种方法是为每一行进行树解释并将它们相加。两种方法都没有更正确的一种。它们实际上都被广泛使用，所以这是一种类型1和类型2的特征重要性。所以我们可以尝试扩展一下。不仅仅是单变量特征重要性，还有交互特征重要性。现在这里有一点。我要描述的东西很容易描述。当随机森林首次被发明时，Breiman就描述过这个方法，它也是Salford系统商业软件产品的一部分，他们拥有随机森林的商标。但我不知道它是任何开源库的一部分，我从来没有看到过一篇真正研究它的学术论文。所以我要在这里描述的是一个巨大的机会，但也像有很多细节需要完善。但这里是基本思想。
- en: 'This particular difference here (in red) is not just because of year made but
    because of a combination of year made and enclosure [[1:19:15](https://youtu.be/BFIYUvBRTpE?t=1h19m15s)]:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的这个特定差异（红色）不仅仅是因为year made，而是因为year made和enclosure的组合[[1:19:15](https://youtu.be/BFIYUvBRTpE?t=1h19m15s)]：
- en: '![](../Images/8090753aa1c338a5f3ea2b7050a3b638.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8090753aa1c338a5f3ea2b7050a3b638.png)'
- en: The fact that this is 9.7 is because enclosure was in this branch and year made
    was in this branch. So in other words, we could say the contribution of enclosure
    interacted with year made is -0.3.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是9.7的原因是因为enclosure在这个分支中，year made在这个分支中。换句话说，我们可以说enclosure与year made的交互作用是-0.3。
- en: So what about the difference between 9.5 and 9.4? That’s an interaction of year
    made and hours on the meter. I’m using star here not to mean “times” but to mean
    “interacted with”. It’s a common way of doing things like R’s formulas do it this
    way as well. So year made interacted with meter has a contribution of -0.1.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 那么9.5和9.4之间的差异呢？那是year made和表计上的小时数的交互作用。我在这里使用星号不是表示“乘”，而是表示“与...交互”。这是一种常见的做法，就像R的公式也是这样做的。所以year
    made与表计的交互作用贡献了-0.1。
- en: '![](../Images/5603740765510c45ecc15dae33a8b302.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5603740765510c45ecc15dae33a8b302.png)'
- en: Perhaps we could also say from 10 to 9.4, this also shows an interaction between
    meter and enclosure with one thing in between them. So we could say meter interacted
    with enclosure equals …and what should it be? Should it be -0.6? Some ways that
    seems unfair because we are also including the impact of year made. So maybe it
    should be -0.6 and maybe we should add back this 0.2 (9.5 → 9.7). These are like
    details that I actually don’t know the answer to. How should we best assign a
    contribution to each pair of variables in this path? But clearly conceptually
    we can. The pairs of variables in that path all represent interactions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们还可以说从10到9.4，这也显示了仪表和围栏之间的相互作用，中间有一件事。所以我们可以说仪表与围栏的相互作用等于...应该是多少呢？应该是-0.6吗？有些方式似乎不公平，因为我们还包括了年份的影响。所以也许应该是-0.6，也许我们应该加回这个0.2（9.5
    → 9.7）。这些都是我实际上不知道答案的细节。我们应该如何最好地为这条路径中的每对变量分配贡献？但从概念上来说，我们可以。该路径中的变量对都代表相互作用。
- en: '**Question**: Why don’t you force them to be next to each other in the tree
    [[1:21:47](https://youtu.be/BFIYUvBRTpE?t=1h21m47s)]? I’m not going to say it’s
    the wrong approach. I don’t think it’s the right approach though. Because it feels
    like in this path, meter and enclosure are interacting. So it seems like not recognizing
    that contribution is throwing away information. But I’m not sure. I had one of
    my staff at Kaggle actually do some R&D on this a few years ago and they actually
    found (I wasn’t close enough to know how they dealt with these details), but they
    got it working pretty well. But unfortunately it never saw the light of day as
    a software product. But this is something maybe a group of you could get together
    and build. Do some googling to check, but I really don’t think that there are
    any interaction feature importance parts of any open source library.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：为什么你不强制它们在树中相邻[[1:21:47](https://youtu.be/BFIYUvBRTpE?t=1h21m47s)]？我不会说这是错误的方法。但我不认为这是正确的方法。因为在这条路径中，仪表和围栏是相互作用的。所以似乎不承认这种贡献是在丢弃信息。但我不确定。几年前，我在Kaggle的员工中有一个实际上对此进行了一些研发，他们确实发现了（我不够接近知道他们如何处理这些细节），但他们做得相当不错。但不幸的是，它从未成为软件产品问世。但也许你们中的一群人可以聚在一起并构建。做一些搜索来检查，但我真的不认为任何开源库中有任何相互作用特征重要性部分。'
- en: '**Question**: Wouldn’t this exclude interactions though between variables that
    don’t matter until they interact? So say your row never chooses to split down
    that path, but that variable interacting with another one becomes your most important
    split [[1:22:56](https://youtu.be/BFIYUvBRTpE?t=1h22m56s)]. I don’t think that
    happens. Because if there is an interaction that’s important only because it’s
    an interaction (and not in a univariate basis), it will appear sometimes, assuming
    that you set max features to less than one, so therefore it will appear in some
    path.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：但这样会排除那些在相互作用之前并不重要的变量之间的相互作用吗？所以说，如果你的行永远不选择沿着那条路径分裂，但是那个变量与另一个变量的相互作用成为你最重要的分裂。我不认为会发生这种情况。因为如果有一个相互作用是重要的，只是因为它是一个相互作用（而不是在单变量基础上），它有时会出现，假设你将最大特征设置为小于一，因此它会出现在某些路径中。'
- en: '**Question**: What is meant by interaction? Is it multiplication, ratio, addition
    [[1:23:31](https://youtu.be/BFIYUvBRTpE?t=1h23m31s)]? Interaction means appears
    on the same path through a tree. In the above example, there is an interaction
    between enclosure and year made because we branched on enclosure and then we branched
    on year made. So to get to 9.7, we have to have some specific value of enclosure
    and some specific value of year made.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：相互作用是什么意思？是乘法、比率、加法吗？相互作用意味着出现在树的同一路径上。在上面的例子中，由于我们在围栏上分支，然后在年份上分支，所以围栏和年份之间存在相互作用。因此，要达到9.7，我们必须有某个特定的围栏值和某个特定的年份值。'
- en: '**Question**: What if you went down the middle leaves between the two things
    you are trying to observe and you would also take into account what the final
    measure is? I mean if we extend the tree downwards, you’d have many measures both
    of like the two things you are trying to look at and also the in between steps.
    There seems to be a way to average information out in between them [[1:24:03](https://youtu.be/BFIYUvBRTpE?t=1h24m3s)]?
    There could be. I think what we should do is talk about this on the forum. I think
    this is fascinating and I hope we build something great, but I need to do my live
    coding. That was a great discussion. Keep thinking about it and do some experiments.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：如果你走在你试图观察的两件事之间的中间叶子上，你也会考虑最终的度量是什么吗？我的意思是，如果我们向下延伸树，你会有很多度量，既包括你试图观察的两件事，也包括中间步骤。似乎有一种方法可以在它们之间平均信息吗？也许有。我认为我们应该在论坛上讨论这个。我觉得这很有趣，希望我们能构建出一些伟大的东西，但我需要进行现场编码。这是一个很好的讨论。继续思考并进行一些实验。'
- en: Back to Live Coding [[1:24:50](https://youtu.be/BFIYUvBRTpE?t=1h24m50s)]
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回到现场编码。
- en: So to experiment with that, you almost certainly want to create a synthetic
    dataset first. It’s like `y = x1 + x2 + x1*x2` or something. Something where you
    know there is this interaction effect and there isn’t that interaction effect,
    and you want to make sure that the feature importance you get at the end is what
    you expected.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了尝试这个，你几乎肯定想先创建一个合成数据集。就像 `y = x1 + x2 + x1*x2` 或者其他什么。有一些你知道存在交互效应，有一些你知道不存在交互效应，你想确保最终得到的特征重要性是你期望的。
- en: So probably the first step would be to do single variable feature importance
    using the tree interpreter style approach [[1:25:14](https://youtu.be/BFIYUvBRTpE?t=1h25m14s)].
    One nice thing about this is it doesn’t really matter how much data you have.
    All you have to do to calculate feature importance is just slide through tree.
    So you should be able to write in a way that’s actually pretty fast, so even writing
    it in pure Python might be fast enough depending on your tree size.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以可能第一步是使用树解释器风格的单变量特征重要性。这种方法的一个好处是，你拥有多少数据并不重要。你只需要遍历树来计算特征重要性。所以你应该能够以一种相当快速的方式编写代码，所以即使只用纯Python编写，也可能足够快，这取决于你的树的大小。
- en: We are going to talk about extrapolation and the first thing I want to do is
    create a synthetic dataset that has a simple linear relationship. We are going
    to pretend it’s like a time series. So we need to create some x values. The easiest
    way to create some synthetic data of this type is to use `linspace` which just
    creates some evenly spaced data between start and stop by default 50 observations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论外推和我想要做的第一件事是创建一个具有简单线性关系的合成数据集。我们将假装它就像一个时间序列。所以我们需要创建一些x值。创建这种类型的合成数据的最简单方法是使用`linspace`，它默认创建50个观测值在开始和结束之间均匀分布的数据。
- en: '![](../Images/ca0ed3c32e1a6203792b8b70f3443f5a.png)![](../Images/3dac629339adb81fe1a5a37cb80218c6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ca0ed3c32e1a6203792b8b70f3443f5a.png)![](../Images/3dac629339adb81fe1a5a37cb80218c6.png)'
- en: Then we are going to create dependent variable, so let’s assume there is a linear
    relationship between x and y, and let’s add a little bit of randomness to it.
    `random.uniform` between low and high, so we could add somewhere between -0.2
    and 0.2, for example.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将创建一个因变量，所以让我们假设x和y之间存在线性关系，并且让我们添加一点随机性。在低和高之间使用`random.uniform`，所以我们可以添加-0.2到0.2之间的某个值，例如。
- en: '![](../Images/4b7e45c994e0e1ae89249fb403ae1f14.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4b7e45c994e0e1ae89249fb403ae1f14.png)'
- en: The next thing we need is a shape which is basically what dimensions do you
    want these random numbers to be, and obviously we want them to be the same shape
    as `x`’s shape. So we can just say `x.shape`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要一个形状，基本上就是你想要这些随机数的维度是什么，显然我们希望它们与`x`的形状相同。所以我们可以直接说`x.shape`。
- en: '![](../Images/81e52dc3309b92046f4d778676efd307.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/81e52dc3309b92046f4d778676efd307.png)'
- en: So in other words, `(50,)` is `x.shape`. Remember when you see something in
    parentheses with a comma, that’s a tuple with just one thing in it. So this is
    shape 50 and so we added 50 random numbers. Now we can plot those.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，`(50,)`是`x.shape`。记住，当你看到括号里有逗号的时候，那就是一个只有一个元素的元组。所以这是形状为50，我们添加了50个随机数。现在我们可以绘制这些数值。
- en: '![](../Images/4d8302e82de69ddca4bb3797be2d700f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4d8302e82de69ddca4bb3797be2d700f.png)'
- en: Alright, so there is our data. When you were both working as a data scientist
    or for doing your exams in this course, you need to be able to quickly whip up
    a dataset like that, throw it up in a plot without thinking too much. As you can
    see, you don’t have to really remember much if anything. You just have to know
    how to hit `shift + tab` to check the names of parameters, google, or something
    to try and find `linspace` if you forgot what it’s called.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这就是我们的数据。当你作为数据科学家工作或在这门课程中做考试时，你需要能够快速地创建一个类似的数据集，将其绘制在图表中而不用考虑太多。正如你所看到的，你不必真的记住太多东西。你只需要知道如何按`shift
    + tab`来检查参数的名称，搜索一下，或者尝试找到`linspace`如果你忘记了它的名字。
- en: So let’s assume that’s our data [[1:28:33](https://youtu.be/BFIYUvBRTpE?t=1h28m33s)].
    We’re now going to build a random forest model and what I want to do is build
    a random forest model that kind of acts as if this is a time series. So I’m going
    to take left part as a training set. And take the right part as our validation
    or test set just like we did in groceries or bulldozers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们假设这是我们的数据。我们现在要构建一个随机森林模型，我想要构建一个随机森林模型，让它像一个时间序列一样运行。所以我将左边部分作为训练集。然后将右边部分作为我们的验证或测试集，就像我们在购物或推土机中所做的那样。
- en: '![](../Images/d033cab61b66800b28ef99b492696808.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/d033cab61b66800b28ef99b492696808.png)'
- en: 'We can use exactly the same kind of code that we used in `split_vals`. So we
    can say:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以使用与我们在`split_vals`中使用的完全相同的代码。所以我们可以说： '
- en: '[PRE0]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That splits it into the first 40 versus the last 10\. We can do the same thing
    for y and there we go.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据分为前40个和后10个。我们可以对y做同样的操作。
- en: '[PRE1]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The next thing to do is we want to create a random forest and fit it which requires
    x and y.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来要做的是创建一个随机森林并拟合它，这需要x和y。
- en: '[PRE2]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s actually going to give an error and the reason why is that it expects
    x to be a matrix, not a vector, because it expects x to have a number of columns
    of data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上会导致错误，原因是它期望x是一个矩阵，而不是一个向量，因为它期望x有多列数据。
- en: So it’s important to know that a matrix with one column is not the same thing
    as a vector.
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 重要的是要知道，只有一列的矩阵和向量不是同一回事。
- en: 'So if I try to run this, “Expected 2D array, got 1D array instead”:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我尝试运行这个代码，“预期2D数组，实际得到1D数组”：
- en: '![](../Images/c89a746ef5d8e8ce13f4a68661b7b219.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/c89a746ef5d8e8ce13f4a68661b7b219.png)'
- en: So we need to convert 1D array into a 2D array. Remember I said `x.shape` is
    `(50,)`. So `x` has one axis and x’s rank is 1\. The rank of a variable is equal
    to the length of it’s shape — how many axes it has. Vector we can think of as
    an array of rank 1 and matrix as an array of rank 2\. I very rarely use words
    like vector and matrix because they are kind of meaningless — specific example
    of something more general which is they are all N dimensional tensors or N dimensional
    arrays. So an N dimensional array we can say it’s a tensor of rank N. They basically
    mean kind of the same thing. Physicists get crazy when you say that because to
    a physicist, a tensor has quite a specific meaning but in machine learning, we
    generally use it in the same way.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们需要将一维数组转换为二维数组。记住我说过`x.shape`是`(50,)`。所以`x`有一个轴，x的秩是1。变量的秩等于它的形状的长度 - 它有多少个轴。我们可以将向量看作是秩为1的数组，将矩阵看作是秩为2的数组。我很少使用向量和矩阵这样的词，因为它们有点毫无意义
    - 它们只是更一般的东西的具体例子，它们都是N维张量或N维数组。所以N维数组可以说是秩为N的张量。它们基本上意味着相同的事情。物理学家听到这个会很疯狂，因为对于物理学家来说，张量有一个非常具体的含义，但在机器学习中，我们通常以相同的方式使用它。
- en: So how do we turn an one dimensional array into a two dimensional array. There
    are a couple of ways we can do it but basically we slice it. Colon (`:`) means
    give me everything in that axis. `:,None` means give me everything in the first
    axis (which is the only axis we have) and then `None` is a special indexer which
    means add a unit axis here. So let me show you.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '那么我们如何将一维数组转换为二维数组。我们可以这样做的方法有几种，但基本上我们是切片。冒号（`:`）表示给我在那个轴上的所有东西。`:, None`表示给我第一个轴上的所有东西（这是我们唯一拥有的轴），然后`None`是一个特殊的索引器，表示在这里添加一个单位轴。所以让我给你看看。 '
- en: '![](../Images/8b3e1a4f1a69428b22ffae4d90a11b2f.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8b3e1a4f1a69428b22ffae4d90a11b2f.png)'
- en: That is of shape (50, 1), so it’s a rank 2\. It has two axes. One of them is
    a very boring axis — it’s a length one axis. So let’s move `None` to the left.
    There is (1, 50). Then to remind you, the original is (50,).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 它的形状是(50, 1)，所以它是秩为2的。它有两个轴。其中一个是一个非常无聊的轴 - 它是一个长度为一的轴。所以让我们将`None`移到左边。这是(1,
    50)。然后提醒你，原始的是(50,)。
- en: '![](../Images/5d722ceac74a361f3381e5387efc8eed.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5d722ceac74a361f3381e5387efc8eed.png)'
- en: So you can see I can put `None` as a special indexer to introduce a new unit
    axis there. So `x[None,:]` has one row and fifty columns. `x[:,None]` has fifty
    rows and one column — so that’s what we want. This kind of playing around with
    ranks and dimension is going to become increasingly important in this course and
    in the deep learning course. So spend a lot of time slicing with None, slicing
    with other things, try to create 3 dimensional, 4 dimensional tensors and so forth.
    I’ll show you two tricks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到我可以将`None`作为一个特殊的索引器放在那里引入一个新的单位轴。`x[None, :]`有一行和五十列。`x[:, None]`有五十行和一列
    - 这就是我们想要的。在这门课程和深度学习课程中，这种对秩和维度的玩耍将变得越来越重要。所以花很多时间用None切片，用其他东西切片，尝试创建三维、四维张量等。我会向你展示两个技巧。
- en: 'The first is you never ever need to write `,:` as it’s always assumed. So these
    are exactly the same thing:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是你永远不需要写`,:`，因为它总是被假定的。所以这些是完全相同的：
- en: '![](../Images/fc83fc64ccc445d232a496199588fea8.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fc83fc64ccc445d232a496199588fea8.png)'
- en: And you see that in code all the time, so you need to recognize it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在代码中一直看到这样的写法，所以你需要认识到它。
- en: The second trick is `x[:,None]` is adding an axis in the second dimension (or
    I guess index 1 dimension). What if I always want to put it in the last dimension?
    Often our tensors change dimensions without us looking because you went from a
    one channel image to a three channel image, or you went from a single image to
    a mini batch of images. Suddenly, you get new dimensions appearing. So make things
    general, I would say `...` which means as many dimensions as you need to fill
    this up. So in this case (`x[…, None].shape` ), it’s exactly the same but I would
    always try to write it that way because it means it’s going to continue to work
    as I get higher dimensional tensors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个技巧是`x[:, None]`是在第二维度（或我猜是索引1维度）添加一个轴。如果我总是想把它放在最后一个维度怎么办？通常我们的张量在我们不注意的情况下改变维度，因为你从一个单通道图像变成了一个三通道图像，或者你从一个单个图像变成了一个图像的小批量。突然之间，新的维度出现了。所以为了让事情更一般化，我会说`...`，这意味着你需要多少维度来填充这个。所以在这种情况下（`x[…,
    None].shape`），它是完全相同的，但我总是尝试以这种方式写，因为这意味着当我得到更高维度的张量时，它将继续工作。
- en: So in this case, I want 50 rows and one column, so I’ll call that x1\. Let’s
    now use that here and so this is now a 2D array and so I can create my random
    forest.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这种情况下，我想要50行和一列，所以我会称之为x1。现在让我们在这里使用它，这样就是一个二维数组，所以我可以创建我的随机森林。
- en: '![](../Images/8d4106b6e017f31faf7d4e51acb70f40.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8d4106b6e017f31faf7d4e51acb70f40.png)'
- en: Then I could plot that, and this is where you’re going to have to turn your
    brains on because the folks this morning got this very quickly which was super
    impressive. I’m going to plot `y_trn` against `m.predict(x_trn)`. Before I hit
    go, what is this going to look like? It should basically be the same. Our predictions
    hopefully are the same as the actuals. So this should fall on a line but there
    is some randomness so it won’t quite.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我可以绘制出来，这就是你需要打开大脑的地方，因为今天早上的人们非常快地理解了这一点，这是非常令人印象深刻的。我将绘制`y_trn`与`m.predict(x_trn)`。在我开始之前，这会是什么样子？它应该基本上是一样的。我们的预测希望与实际相同。所以这应该落在一条线上，但有一些随机性，所以不会完全相同。
- en: '![](../Images/7fdc34fa62b66337b87a321dcc6b462d.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7fdc34fa62b66337b87a321dcc6b462d.png)'
- en: That was the easy one. Let’s now do the hard one, the fun one. What is that
    going to look like?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那是容易的。现在让我们做困难的，有趣的那个。那会是什么样子？
- en: '![](../Images/ae6c6ba81817c96cae72133a62b0f9d2.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ae6c6ba81817c96cae72133a62b0f9d2.png)'
- en: 'Think about what trees do and think about the fact that we have a validation
    set on the right and a training set on the left:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 想想树的作用，想想右边有一个验证集，左边有一个训练集：
- en: '![](../Images/a4f103d8c76f42587e6e7bfc91bdf6c1.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/a4f103d8c76f42587e6e7bfc91bdf6c1.png)'
- en: So think about a forest is just a bunch of trees.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 所以想想森林只是一堆树。
- en: 'Tim: I’m guessing since all the new data is actually outside of the original
    scope, so it’s all going to be basically the same — it’s like one huge group [[1:37:15](https://youtu.be/BFIYUvBRTpE?t=1h37m15s)].'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Jeremy: Yeah, right. So forget the forest, let’s create one tree. So we are
    probably going to split somewhere around here first, then split somewhere here,
    … So our final split is right most node. Our prediction, when we take one from
    validation set, so it’s going to put that through the forest and end up predicting
    the right most average. It can’t predict anything higher than that because there
    is nothing higher to average.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/7438a2c65674d13192508a3766f65bed.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: So this is really important to realize a random forest is not magic. It’s just
    returning the average of nearby observations where nearby is kind of in this like
    “tree space”. So let’s run it and see if Tim is right
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/58db9125b11012055e02c3e95e438af1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Holy crap, that’s awful. If you don’t know how random forests work then this
    is going to totally screw you. If you think that it’s actually going to be able
    to extrapolate to any kind of data it hasn’t seen before, particularly future
    time period, it’s just not. It just can’t. It’s just averaging stuff it’s already
    seen. That’s all it can do.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so we are going to be talking about how to avoid this problem. We talked
    a little bit in the last lesson about trying to avoid it by avoiding unnecessary
    time dependent variables where we can. But in the end, if you really have a time
    series that looks like this, we actually have to deal with a problem. One way
    we could deal with the problem would be use a neural net. Use something that actually
    has a function or shape that can actually fit something that actually has a function
    or shape that can actually fit something like this so it will extrapolate nicely:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/6cffc3e3e176bd6304c4199b1afbd8d7.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
- en: Another approach would be to use all the time series techniques you guys are
    learning about in the morning class to fit some kind of time series and then detrend
    it. Then you’ll end up with detrended dots and then use the random forest to predict
    those. That’s particularly cool because imagine what your random forest was actually
    trying to predict data which was two different states. So the blues ones are down
    there, and the red ones are up here.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/75be04c6622f2e5ba4a7a089b9cfeba5.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: If you try to use a random forest, it’s going to do a pretty crappy job because
    time is going to seem much more important. So it’s basically still going to split
    like this and split like that, then finally once it gets down to left corner,
    it will be like “oh okay, now I can see the difference between the states.”
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/2b12cb556d1d45e083ac139e7bd74100.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: In other words, when you’ve got this big time piece going on, you’re not going
    to see the other relationships in the random forest until every tree deals with
    time. So one way to fix this would be with a gradient boosting machine (GBM).
    What a GBM does is, it creates a little tree, and runs everything through that
    first little tree (which could be the time tree) then it calculates the residuals
    and the next little tree just predicts the residuals. So it would be kind of like
    detrending it, right? GBM still can’t extrapolate to the future but at least they
    can deal with time-dependent data more conveniently.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: We are going to be talking about this quite a lot more over the next coupe of
    weeks, and in the end that a solution is going to be just use neural nets. But
    for now, using some kind of time series analysis, detrend it, and then use random
    forest on that isn’t a bad technique at all. If you are playing around something
    like Ecuador groceries competition, that would be a really good thing to fiddle
    around with.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
