- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:33:18'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2404.09939] A Survey on Deep Learning for Theorem Proving'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.09939](https://ar5iv.labs.arxiv.org/html/2404.09939)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \forestset
  prefs: []
  type: TYPE_NORMAL
- en: direction switch/.style= forked edges, for tree= calign=last, edge+=thick, font=,
    , where level=1minimum width=13em, where level¡=2draw=red, where level¿=1folder,
    grow’=0, ,
  prefs: []
  type: TYPE_NORMAL
- en: A Survey on Deep Learning for Theorem Proving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zhaoyu Li¹, Jialiang Sun¹, Logan Murphy¹, Qidong Su¹, Zenan Li², Xian Zhang³
  prefs: []
  type: TYPE_NORMAL
- en: Kaiyu Yang⁴, Xujie Si¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹University of Toronto, ²Nanjing University, ³Microsoft Research Asia, ⁴Caltech
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/zhaoyu-li/DL4TP](https://github.com/zhaoyu-li/DL4TP)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Theorem proving is a fundamental aspect of mathematics, spanning from informal
    reasoning in mathematical language to rigorous derivations in formal systems.
    In recent years, the advancement of deep learning, especially the emergence of
    large language models, has sparked a notable surge of research exploring these
    techniques to enhance the process of theorem proving. This paper presents a pioneering
    comprehensive survey of deep learning for theorem proving by offering i) a thorough
    review of existing approaches across various tasks such as autoformalization,
    premise selection, proofstep generation, and proof search; ii) a meticulous summary
    of available datasets and strategies for data generation; iii) a detailed analysis
    of evaluation metrics and the performance of state-of-the-art; and iv) a critical
    discussion on the persistent challenges and the promising avenues for future exploration.
    Our survey aims to serve as a foundational reference for deep learning approaches
    in theorem proving, seeking to catalyze further research endeavors in this rapidly
    growing field.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proving theorems is a cornerstone of mathematics. Since the era of Euclid around
    300 B.C., people have crafted theorems and proofs using a blend of natural language
    and mathematical symbols, meticulously evaluating their correctness through manual
    inspection. In the 1950s, a paradigm shift occurred with the exploration of computer-assisted
    proofs (Davis, [1957](#bib.bib37); Davis & Putnam, [1960](#bib.bib38)), wherein
    a machine automatically applies deduction rules to prove assertions. These innovations
    laid the groundwork for the subsequent development of interactive theorem provers (Bruijn,
    de, [1970](#bib.bib19); Milner, [1972](#bib.bib120)), enabling people to construct
    more intricate theorems and proofs by interacting with these systems. Building
    upon these advancements, later research extended the scope of theorem proving
    beyond mathematics, applying it to various practical applications such as software
    verification (Schumann, [2001](#bib.bib154)) and hardware design (Kern & Greenstreet,
    [1999](#bib.bib90)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bf3da018792cc74d150230faa8f411d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Papers on deep learning for theorem proving over the years.'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring learning-based approaches for theorem proving has been a long-standing
    research focus, dating back to the 1990s (Suttner & Ertel, [1990](#bib.bib166);
    Denzinger et al., [1999](#bib.bib39)). The recent development of deep learning,
    especially with the evolution of large language models (LLMs), has ignited a wave
    of research interest in this area again. As shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ A Survey on Deep Learning for Theorem Proving"), the volume
    of papers on deep learning for theorem proving has grown approximately from 2
    in 2016 to 50 in 2023\. However, despite such remarkable growth, this domain is
    characterized by a wide range of tasks, methods, datasets, and evaluations, which
    lack a cohesive framework to comprehend the true extent of progress and indicate
    the underlying challenges and potential future work.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we provide a comprehensive survey of more than 170 research papers
    in deep learning for theorem proving, aiming to map out the current research landscape
    and highlight key advancements systematically. We begin with the background for
    informal and formal settings of theorem proving (§[2](#S2 "2 Background ‣ A Survey
    on Deep Learning for Theorem Proving")). Subsequently, we delve into the details
    of the tasks and methods within this domain (§[3](#S3 "3 Tasks and Methods ‣ A
    Survey on Deep Learning for Theorem Proving")), which include autoformalization,
    premise selection, proofstep generation, proof search, and other tasks. We also
    review the datasets for theorem proving (§[4](#S4 "4 Datasets ‣ A Survey on Deep
    Learning for Theorem Proving")), including manually curated and synthetically
    generated ones. Moreover, we evaluate the metrics used in existing methods and
    assess their performance (§[5](#S5 "5 Evaluations ‣ A Survey on Deep Learning
    for Theorem Proving")). Following this, we discuss the prevailing challenges and
    conclude with future directions (§[6](#S6 "6 Discussions ‣ A Survey on Deep Learning
    for Theorem Proving")).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we recall the fundamental concepts of informal and formal to
    theorem proving. Figure [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on Deep
    Learning for Theorem Proving") shows an illustrative example of these two settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90e87befdcf63c2659a94d9baebb21cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Top: The informal statement and proof of the Fundamental Theorem
    of Arithmetic in [ProofWiki](https://proofwiki.org/wiki/Fundamental_Theorem_of_Arithmetic).
    Bottom Left: The formal statement and proof of the same theorem in the mathlib
    library (mathlib Community, [2020](#bib.bib117)) of Lean 4\. Bottom Right: The
    corresponding proof tree illustrating the formal proof process in Lean 4\. The
    references and premises used in the informal and formal proof are highlighted
    by underlines and colors respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Informal Theorem Proving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Informal theorem proving involves establishing the truth of mathematical statements
    building on existing knowledge via intuitive reasoning and natural language explanations.
    This mirrors how people learn and prove theorems in everyday mathematics. For
    instance, to prove the Fundamental Theorem of Arithmetic in Figure [2](#S2.F2
    "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning for Theorem Proving") (Top),
    one needs to comprehend basic concepts like primes and might apply established
    results to draw a conclusion. Despite the ubiquity of informal theorem proving,
    as mathematics evolves, the theories and proofs tend to be more intricate, making
    verifying their correctness increasingly difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Formal Theorem Proving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Formal theorem proving represents theorems and proofs in a machine-verifiable
    format, ensuring their correctness using rigorous logical rules. This field can
    be broadly classified into two paradigms: automated theorem proving (ATP) and
    interactive theorem proving (ITP).'
  prefs: []
  type: TYPE_NORMAL
- en: ATP aims to verify formal statements fully automatically. Saturation-based theorem
    provers, including E (Schulz, [2002](#bib.bib153)) and Vampire (Kovács & Voronkov,
    [2013](#bib.bib93)), mainly operate on first-order logic (FOL) to autonomously
    generate logical consequences from a set of axioms until a proof or refutation
    is derived, or computational limits are reached. Similarly, geometric ATP systems
    such as GEX (Chou et al., [2000](#bib.bib26)) prove geometry problems by iteratively
    applying deduction rules. Other approaches, such as tableau-based methods like
    leanCoP (Otten & Bibel, [2003](#bib.bib126)) and instantiation-based methods like
    iProver (Korovin, [2008](#bib.bib92)), use other forms of proof calculi for proof
    construction. Despite the sophisticated designs of these ATP systems, the inherent
    vast search space often limits their practicality in more complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: In ITP, humans collaboratively prove theorems by interacting with *proof assistants*,
    such as Isabelle (Paulson, [1994](#bib.bib131)), HOL Light (Harrison, [1996](#bib.bib66)),
    Coq (Barras et al., [1999](#bib.bib12)), and Lean (Moura & Ullrich, [2021](#bib.bib123)).
    These proof assistants typically enable users to formalize theorems in higher-order
    logic and provide a language for users to build verifiable proofs. As shown in
    Figure [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning for Theorem
    Proving") (Bottom Left), to prove a theorem (initial goal) in Lean, one can use
    *tactics* like refine’ and rw as the proof steps. Applying a tactic either finishes
    the goal or decomposes it into simpler sub-goals, and the proof is complete when
    no further goals remain. When proving the current goal, one can apply assumptions
    in the local context and previously proven premises in the environment as tactic
    arguments. For example, the premise perm_of_prod_eq_prod is used as the argument
    of refine’. The proving process can be modeled as a proof tree, where each node
    is a proof state with a goal and its local context, and each edge is a tactic,
    as shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ A Survey on Deep Learning
    for Theorem Proving") (Bottom Right). Using proof assistants, researchers have
    successfully formalized and proved landmark theorems like the Four Color Theorem (Gonthier,
    [2008](#bib.bib62)) and the Kepler Conjecture (Hales et al., [2017](#bib.bib63)),
    and verified the correctness of critical software such as the seL4 microkernel (Klein
    et al., [2009](#bib.bib91)) and the CompCert C compiler (Leroy et al., [2016](#bib.bib102)).
    However, it is worth noting that these projects took several Ph.D. years to complete,
    requiring substantial labor and expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Tasks and Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The emergence of deep learning has opened new avenues for the landscape of
    theorem proving, either enhancing or substituting traditional components involved
    in the process. This section categorizes and summarizes existing deep learning
    approaches into 5 tasks: autoformalization, premise selection, proofstep generation,
    proof search, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Autoformalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Autoformalization aims to convert informal theorems and proofs into machine-verifiable
    formats automatically. This task is notoriously challenging, requiring a profound
    understanding of semantics across informal and formal mathematics (Kaliszyk et al.,
    [2014](#bib.bib83); [2015](#bib.bib84)). Nonetheless, the success of autoformalization
    promises to facilitate the verification of mathematical papers and pave the way
    for general-purpose reasoning engines (Szegedy, [2020](#bib.bib168)).
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. ([2018](#bib.bib185); [2020](#bib.bib186)) first explore using deep
    learning models in autoformalization. Inspired by the sequence-to-sequence models
    in neural machine translation (Sutskever et al., [2014](#bib.bib165); Cho et al.,
    [2014](#bib.bib25)), they experiment various encoder-decoder frameworks (Luong
    et al., [2017](#bib.bib116); Lample et al., [2018](#bib.bib99); Lample & Conneau,
    [2019](#bib.bib98)) to convert LaTeX-written mathematical texts to the Mizar language (Rudnicki,
    [1992](#bib.bib148)). Subsequent studies (Bansal & Szegedy, [2020](#bib.bib10);
    Cunningham et al., [2022](#bib.bib34)) utilize similar neural architectures for
    HOL Light and Coq.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recent development of LLMs and their in-context learning capabilities (Brown
    et al., [2020](#bib.bib18)) have provided new opportunities for autoformalization.
    Wu et al. ([2022](#bib.bib196)); Agrawal et al. ([2022](#bib.bib4)); Gadgil et al.
    ([2022](#bib.bib54)) study the prospects of autoformalization using PaLM (Chowdhery
    et al., [2023](#bib.bib27)) and Codex (Chen et al., [2021](#bib.bib23)) with few-shot
    prompting to translate mathematical problems into Isabelle and Lean. Some researchers (Jiang
    et al., [2023b](#bib.bib77); Patel et al., [2023](#bib.bib130); Zhao et al., [2023](#bib.bib211);
    Xin et al., [2024](#bib.bib197)) propose more structured approaches to autoformalization:
    For example, DSP (Jiang et al., [2023b](#bib.bib77)) utilizes Minerva (Lewkowycz
    et al., [2022](#bib.bib103)) to draft informal proofs and map them into formal
    sketches, with ATP systems employed to fill in the missing details in the proof
    sketch. Zhao et al. ([2023](#bib.bib211)) improves informal proofs and formal
    sketches in DSP with sub-goal proofs and prompt selection respectively. Additionally,
    a line of research (Azerbayev et al., [2023](#bib.bib8); Jiang et al., [2023a](#bib.bib76);
    Azerbayev et al., [2024](#bib.bib9); Shao et al., [2024](#bib.bib155); Ying et al.,
    [2024](#bib.bib205)) focuses on training LLMs on large-scale datasets with both
    informal and formal mathematical data to evaluate their performance on autoformalization.
    Recent studies (Liu et al., [2023](#bib.bib110); Ye et al., [2023](#bib.bib203);
    Zhou et al., [2024a](#bib.bib214); Huang et al., [2024](#bib.bib70)) also investigate
    autoformalization in some downstream tasks: For instance, SAT-LM (Ye et al., [2023](#bib.bib203))
    uses LLMs to formalize natural language problems and solve them with ATP systems
    on several reasoning tasks. DTV (Zhou et al., [2024a](#bib.bib214)) leverages
    autoformalization to ground LLM reasoning, formalizing LLM-generated answers and
    verifying them with ATP tools. Besides these efforts, Wu et al. ([2022](#bib.bib196));
    Azerbayev et al. ([2023](#bib.bib8)); Jiang et al. ([2023a](#bib.bib76)) explore
    advanced LLMs like Codex and GPT-4 (Achiam et al., [2023](#bib.bib3)) for informalization,
    i.e., the translation of formal statements to natural language.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Premise Selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a large collection of previously proven lemmas, premise selection is to
    retrieve the helpful lemmas that can contribute to a successful proof. It is an
    enduring challenge in both mathematical research and ATP/ITP systems (Kühlwein
    et al., [2012](#bib.bib96); Alama et al., [2014](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: The seminal works (Irving et al., [2016](#bib.bib71); Kaliszyk et al., [2017](#bib.bib85))
    model premise selection as a binary classification task, embedding theorems and
    premises with a variety of neural networks including convolutional neural networks
    (CNNs), recurrent neural networks (RNNs), and hybrid models. These embeddings
    are then combined to feed into a logit layer to predict their relevance. Follow-up
    works (Kucik & Korovin, [2018](#bib.bib95); Bansal et al., [2019](#bib.bib11);
    Piotrowski & Urban, [2020a](#bib.bib136); Proroković et al., [2021](#bib.bib141);
    Szegedy et al., [2021](#bib.bib169)) extend previous frameworks by using a better
    representation of features or more sophisticated architectures like Wavenet (Van
    Den Oord et al., [2016](#bib.bib178)) and Transformer (Vaswani et al., [2017](#bib.bib179)).
  prefs: []
  type: TYPE_NORMAL
- en: Given the inherent structured nature of mathematical formulas, a stream of research (Wang
    et al., [2017](#bib.bib184); Peng & Ma, [2017](#bib.bib132); Olšák et al., [2019](#bib.bib124);
    Goertzel & Urban, [2019](#bib.bib59); Crouse et al., [2019](#bib.bib32); Paliwal
    et al., [2020](#bib.bib127); Rawson & Reger, [2020](#bib.bib144); Liu et al.,
    [2022a](#bib.bib111); Goertzel et al., [2022](#bib.bib61); Holden & Korovin, [2023](#bib.bib68);
    Jakubüv et al., [2023](#bib.bib75)) parses formal statements into trees or directed
    acyclic graphs and leverages tree-structured neural networks (Tai et al., [2015](#bib.bib170))
    or graph neural networks (GNNs) (Duvenaud et al., [2015](#bib.bib41); Veličković
    et al., [2018](#bib.bib180); Xu et al., [2019](#bib.bib199)) for encoding. For
    example, FormulaNet (Wang et al., [2017](#bib.bib184)) proposes a graph embedding
    method that preserves the information of edge ordering. Olšák et al. ([2019](#bib.bib124))
    introduces a GNN framework that captures several logical invariances in FOL formulas.
    Paliwal et al. ([2020](#bib.bib127)) conducts comprehensive experiments to evaluate
    various designs for the graph representations of formulas in HOL Light. Subsequent
    works (Li et al., [2021b](#bib.bib106); Lin et al., [2021](#bib.bib109)) explore
    graph contrastive learning (Oord et al., [2018](#bib.bib125); Chen et al., [2020](#bib.bib24))
    to train GNNs for premise selection. Moreover, Ferreira & Freitas ([2020b](#bib.bib44));
    Bauer et al. ([2023](#bib.bib14)) construct a dependency graph over a large corpus
    by representing theorems and premises as nodes and their dependencies as edges,
    leveraging GNNs to predict the link between nodes for premise selection.
  prefs: []
  type: TYPE_NORMAL
- en: With the advancement of pre-trained language models, some efforts (Ferreira
    & Freitas, [2020a](#bib.bib43); Welleck et al., [2021](#bib.bib189)) fine-tune
    BERT (Devlin et al., [2019](#bib.bib40))-like models to embed natural language
    statements into vectors and select premises using a linear classifier layer. Later
    works (Ferreira & Freitas, [2021](#bib.bib45); Tran et al., [2022](#bib.bib172);
    Trust et al., [2022](#bib.bib174); Dastgheib & Asgari, [2022](#bib.bib36); Yeh
    et al., [2023](#bib.bib204); Yang et al., [2023](#bib.bib202)) leverage different
    pre-trained models (Liu et al., [2019](#bib.bib112); Song et al., [2020](#bib.bib158);
    Xue et al., [2022](#bib.bib200)) for encoding and retrieve informal/formal premises
    based on several similarity metrics. Han et al. ([2021](#bib.bib64)) also explores
    fine-tuning over large informal mathematical corpora using the contrastive objective (Oord
    et al., [2018](#bib.bib125)), while PACT (Han et al., [2022](#bib.bib65)) uses
    the auto-regressive objective for formal premise selection. Additionally, several
    research (Kovriguina et al., [2022](#bib.bib94); Tworkowski et al., [2022](#bib.bib175);
    Mikuła et al., [2024](#bib.bib119)) design a second phase to re-rank the selected
    subset of premises, enabling a more accurate selection.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Proofstep Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proofstep generation is the core problem for theorem proving, which aims to
    predict one or more steps to build the proof of a theorem. This task also refers
    to tactic prediction in ITP, which has been widely studied in tactic-based ATP
    systems (hammers) (Böhme & Nipkow, [2010](#bib.bib16); Blanchette et al., [2016](#bib.bib15);
    Czajka & Kaliszyk, [2018](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: A stream of research (Whalen, [2016](#bib.bib191); Huang et al., [2019](#bib.bib69);
    Bansal et al., [2019](#bib.bib11); Paliwal et al., [2020](#bib.bib127); Sanchez-Stern
    et al., [2020](#bib.bib150); Wu et al., [2021b](#bib.bib194); Rute et al., [2024](#bib.bib149))
    treats tactic prediction as a classification problem and uses separate neural
    networks to predict the tactic and its arguments. For example, Gamepad (Huang
    et al., [2019](#bib.bib69)) employs TreeLSTM (Tai et al., [2015](#bib.bib170))
    to encode the proof states and two distinct linear layers for tactic and argument
    prediction. Proverbot9001 (Sanchez-Stern et al., [2020](#bib.bib150)) uses a feed-forward
    neural network and an RNN to predict the tactic and its arguments respectively.
    Besides these works, ASTactic (Yang & Deng, [2019](#bib.bib201)) proposes a decoder
    that generates the tactic as a program, using an RNN to control the generation
    based on a predefined context-free grammar. Later studies (First et al., [2020](#bib.bib48);
    First & Brun, [2022](#bib.bib47); Sanchez-Stern et al., [2023](#bib.bib151)) improve
    ASTactic by incorporating prior proof scripts, combining varied models, and modeling
    identifiers of theorems.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequent advancements (Polu & Sutskever, [2020](#bib.bib138); Polu et al.,
    [2023](#bib.bib139); Han et al., [2022](#bib.bib65); Jiang et al., [2021](#bib.bib78);
    Zhang et al., [2023a](#bib.bib207); Yeh et al., [2023](#bib.bib204); Xiong et al.,
    [2023](#bib.bib198); Welleck & Saha, [2023](#bib.bib188); Vishwakarma & Mishra,
    [2023](#bib.bib181); Gloeckle et al., [2023](#bib.bib58); First et al., [2023](#bib.bib49))
    formulate tactic prediction as language modeling. Specifically, GPT-$f$ (Polu
    & Sutskever, [2020](#bib.bib138)) first apply a conditional language modeling
    objective to train decoder-only Transformers to generate a proof step in the format
    of GOAL <GOAL> PROOFSTEP <PROOFSTEP><EOT>. Baldur (First et al., [2023](#bib.bib49))
    applies a similar objective to generate or repair the whole proof at once. Several
    studies (Szegedy et al., [2021](#bib.bib169); Tworkowski et al., [2022](#bib.bib175);
    Welleck et al., [2022](#bib.bib190); Jiang et al., [2022](#bib.bib79); Yang et al.,
    [2023](#bib.bib202)) also jointly train tactic prediction with premise selection.
    For instance, NaturalProver (Welleck et al., [2022](#bib.bib190)) trains GPT-3 (Brown
    et al., [2020](#bib.bib18)) with constrained decoding to encourage using retrieved
    references in the proof steps. Thor (Jiang et al., [2022](#bib.bib79)) adds a
    <hammer> token to learn when to invoke an ATP tool (Böhme & Nipkow, [2010](#bib.bib16))
    for premise selection to simplify the proof. In the geometry domain, Chen et al.
    ([2022](#bib.bib22)); Liang et al. ([2023a](#bib.bib107)); Trinh et al. ([2024](#bib.bib173));
    He et al. ([2024](#bib.bib67)) follow the same paradigm, auto-regressively generating
    the proof sequence at each step. Notably, AlphaGeometry (Trinh et al., [2024](#bib.bib173))
    trains a decoder-only Transformer to predict the auxiliary constructions in the
    proofs of International Mathematical Olympiad (IMO) geometry problems. Besides
    training on proof data, Azerbayev et al. ([2024](#bib.bib9)); Shao et al. ([2024](#bib.bib155));
    Ying et al. ([2024](#bib.bib205)) train LLMs on extensive general mathematical
    corpora and evaluate their abilities for generating formal proofs.
  prefs: []
  type: TYPE_NORMAL
- en: Recent explorations (Zhang et al., [2023b](#bib.bib209); Yousefzadeh & Cao,
    [2023](#bib.bib206); Scheidt, [2023](#bib.bib152); Frieder et al., [2023a](#bib.bib51);
    [c](#bib.bib53); [b](#bib.bib52); Zhang et al., [2024](#bib.bib208); Poulsen et al.,
    [2024](#bib.bib140)) also investigate advanced LLMs with several prompting methods
    for proof generation across various domains. Alternative works (Jiang et al.,
    [2023b](#bib.bib77); Zhao et al., [2023](#bib.bib211); Zheng et al., [2023](#bib.bib212);
    Xin et al., [2024](#bib.bib197)) leverage autoformalization to generate the formal
    proof of an informal problem in structured pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Proof Search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proof search seeks to systematically traverse the vast landscape of potential
    proof paths to construct a valid proof tree for a given theorem in formal systems.
    It is not only a long-standing research focus in ATP (Urban et al., [2011](#bib.bib177);
    Kaliszyk & Urban, [2015a](#bib.bib81); Jakubüv & Urban, [2017](#bib.bib72)) but
    also a vital process for tactic-based models to complete the proof in ITP.
  prefs: []
  type: TYPE_NORMAL
- en: A thread of research trains deep learning models on existing successful proof
    paths in a supervised fashion to guide the search in ATP systems. Specifically,
    a large body of works (Loos et al., [2017](#bib.bib114); Chvalovskỳ et al., [2019](#bib.bib28);
    Jakubüv & Urban, [2019](#bib.bib73); Aygün et al., [2020](#bib.bib6); Jakubüv
    et al., [2020](#bib.bib74); Suda, [2021b](#bib.bib162); Chvalovskỳ et al., [2021](#bib.bib29);
    Goertzel et al., [2021](#bib.bib60); Suda, [2021a](#bib.bib161); Firoiu et al.,
    [2021](#bib.bib46); Aygün et al., [2022](#bib.bib7); Goertzel et al., [2022](#bib.bib61);
    Jakubüv et al., [2023](#bib.bib75); Bártek & Suda, [2023](#bib.bib13)) exploit
    RNNs, GNNs, or hybrid models for the clause selection in saturation-based provers.
    Piepenbrock et al. ([2022b](#bib.bib135)); Chvalovskỳ et al. ([2023](#bib.bib30))
    and Piotrowski & Urban ([2020b](#bib.bib137)) focus on guiding instantiation and
    connection tableau respectively. Some works (Rawson & Reger, [2019](#bib.bib143);
    Olšák et al., [2019](#bib.bib124); Rawson & Reger, [2021](#bib.bib145); Zombori
    et al., [2021b](#bib.bib217)) further combine the supervised trained models as
    the policy or value networks to guide the Monte Carlo Tree Search (MCTS) or A*
    search across various ATP systems. Additionally, another direction of research (Kusumoto
    et al., [2018](#bib.bib97); Fawzi et al., [2019](#bib.bib42); Abdelaziz et al.,
    [2020](#bib.bib1); Zombori et al., [2021a](#bib.bib216); Crouse et al., [2021](#bib.bib33);
    Piepenbrock et al., [2021](#bib.bib133); [2022a](#bib.bib134); Liu et al., [2022b](#bib.bib113);
    Abdelaziz et al., [2022](#bib.bib2); Fokoue et al., [2023](#bib.bib50); McKeown
    & Sutcliffe, [2023](#bib.bib118); Shminke, [2023](#bib.bib156)) models proof search
    as a Markov decision process and applies deep reinforcement learning to train
    and guide the proof search. For instance, TRAIL (Crouse et al., [2021](#bib.bib33))
    uses the policy gradient (Sutton et al., [1999](#bib.bib167)) to train an attention-based
    action policy in saturation-based provers, and Fawzi et al. ([2019](#bib.bib42))
    applies deep Q-learning (Mnih et al., [2013](#bib.bib121)) to guide the choice
    of inference rules in a semi-algebraic proof system (Lovász & Schrijver, [1991](#bib.bib115))
    for polynomial inequalities.
  prefs: []
  type: TYPE_NORMAL
- en: Most tactic-based models in ITPs use beam search to sample multiple tactic predictions
    per step with breadth-first (Bansal et al., [2019](#bib.bib11)), depth-first (Yang
    & Deng, [2019](#bib.bib201)), or best-first heuristics (Polu & Sutskever, [2020](#bib.bib138))
    to traverse the search space. In particular, GPT-$f$ (Polu & Sutskever, [2020](#bib.bib138))
    and FMSCL (Polu et al., [2023](#bib.bib139)) train language models with outcome
    and proof size objectives as value functions to perform the best-first search.
    Besides these methods, Whalen ([2016](#bib.bib191)); Mo et al. ([2020](#bib.bib122));
    Gauthier ([2020](#bib.bib55)); Wu et al. ([2021a](#bib.bib193)); Gauthier ([2021](#bib.bib56));
    Lample et al. ([2022](#bib.bib100)); Wang et al. ([2023a](#bib.bib182)); Brandfonbrener
    et al. ([2024](#bib.bib17)) combine MCTS or use reinforcement learning to train
    and guide the search procedure. For example, HTPS (Lample et al., [2022](#bib.bib100))
    adopts an AlphaZero (Silver et al., [2018](#bib.bib157))-like approach with online
    training, and DT-Solver (Wang et al., [2023a](#bib.bib182)) improves MCTS with
    dynamic tree sampling and proof-level value function. Additionally, COPRA (Thakur
    et al., [2023](#bib.bib171)) implements a language-agent method that leverages
    GPT-4 to perform a backtracking search based on the proof history.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Other Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the primary tasks outlined previously, we briefly list other
    prediction tasks related or helpful to theorem proving. A line of research (Urban
    & Jakubüv, [2020](#bib.bib176); Piotrowski & Urban, [2020b](#bib.bib137); Rabe
    et al., [2021](#bib.bib142); Johansson & Smallbone, [2023](#bib.bib80)) explore
    automated conjecturing. Lee et al. ([2020](#bib.bib101)); Wu & Wu ([2021](#bib.bib192))
    investigate proving theorems in the latent space. IsarStep (Li et al., [2021a](#bib.bib105))
    predicts the intermediate proposition given surrounding proofs. Skip-tree (Rabe
    et al., [2021](#bib.bib142)) and PACT (Han et al., [2022](#bib.bib65)) propose
    several self-supervised tasks by masking various proof terms for training language
    models. LIME (Wu et al., [2021c](#bib.bib195)) constructs three reasoning tasks
    for deduction, abduction, and induction. Recently, Li et al. ([2023](#bib.bib104))
    proposes to match the proofs with theorem statements from a large database and
    REFACTOR (Zhou et al., [2024b](#bib.bib215)) predicts to extract theorems from
    proofs.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section classifies and summarizes datasets for theorem proving into 2
    categories: i) datasets extracted from existing corpora or manually curated and
    ii) those using synthetic generation or augmentation methods. The overview of
    these datasets is shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Data Collection ‣
    4 Datasets ‣ A Survey on Deep Learning for Theorem Proving").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Data Collection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We begin with the review of informal datasets. NL-PS (Ferreira & Freitas, [2020a](#bib.bib43))
    first builds a natural language premise selection dataset source from [ProofWiki](https://proofwiki.org).
    Similarly, NaturalProofs (Welleck et al., [2021](#bib.bib189)) further incorporates
    data from [Stacks](https://stacks.math.columbia.edu/) and textbooks, resulting
    in a dataset with roughly 25k examples. Adapted from it, NaturalProofs-Gen (Welleck
    et al., [2022](#bib.bib190)) contains around 14.5k theorems for informal proof
    generation. Moreover, MATcH (Li et al., [2023](#bib.bib104)) constructs over 180k
    statement-proof pairs for matching using the [MREC corpus](https://mir.fi.muni.cz/MREC/).
  prefs: []
  type: TYPE_NORMAL
- en: For formal datasets, a line of efforts focuses on extracting and cleaning theorems
    and proofs from established formal libraries and verification projects. Notable
    datasets for Coq include Gamepad (Huang et al., [2019](#bib.bib69)), CoqGym (Yang
    & Deng, [2019](#bib.bib201)), and PRISM (Reichel et al., [2023](#bib.bib146)),
    with CoqGym constructing a dataset from 123 projects encompassing 71k proofs.
    For Isabelle, datasets like IsarStep (Li et al., [2021a](#bib.bib105)), PISA (Jiang
    et al., [2021](#bib.bib78)), and Magnushammer (Mikuła et al., [2024](#bib.bib119))
    are built on the [Archive of Formal Proofs](https://www.isa-afp.org/) and [Isabelle
    Standard Library](https://isabelle.in.tum.de/library/), where PISA extracts 183K
    theorems and 2.16M proof steps. LeanStep (Han et al., [2022](#bib.bib65)), LeanDojo (Yang
    et al., [2023](#bib.bib202)), and MLFMF (Bauer et al., [2023](#bib.bib14)) leverage
    the mathlib library (mathlib Community, [2020](#bib.bib117)) in Lean. In particular,
    LeanDojo extracts over 98k theorems and proofs with 130k premises from mathlib.
    Datasets for other proof assistants include HolStep (Kaliszyk et al., [2017](#bib.bib85))
    and HOList (Bansal et al., [2019](#bib.bib11)) for HOL Light, MPTP2078 (Alama
    et al., [2014](#bib.bib5)), Mizar40 (Kaliszyk & Urban, [2015b](#bib.bib82)), and
    M2K (Kaliszyk et al., [2018](#bib.bib86)) for Mizar, etc. Besides extracting data
    from existing projects, several works manually annotate or formalize the problems
    in natural language. Notably, MiniF2F (Zheng et al., [2022](#bib.bib213)) manually
    formalizes 488 Olympiad-level problems across 4 proof systems and equally splits
    them into a validation set and a test set. FIMO (Liu et al., [2023](#bib.bib110))
    and ProofNet (Azerbayev et al., [2023](#bib.bib8)) formalize the theorem statements
    of IMO and undergraduate-level problems in Lean. For other domains, TRIGO (Xiong
    et al., [2023](#bib.bib198)) targets formalizing the trigonometric reduction problem,
    while UniGeo (Chen et al., [2022](#bib.bib22)) and FormalGeo (Zhang et al., [2023c](#bib.bib210))
    annotate the proof steps for geometry proving problems.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a large body of recent studies leverages large-scale online
    corpora with billions of tokens from informal and formal mathematical data to
    build the pre-training datasets that could aid in theorem proving. These datasets
    include WebMath (Han et al., [2022](#bib.bib65)), Proof-Pile (Azerbayev et al.,
    [2023](#bib.bib8)), InternLM-Math (Ying et al., [2024](#bib.bib205)), etc.
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: forked edges, for tree= grow=east, reversed=true, anchor=base west, parent anchor=east,
    child anchor=west, base=left, font=, rectangle, draw=black, rounded corners, align=left,
    minimum width=2.5em, edge+=black, line width=0.5pt, s sep=3pt, inner xsep=1.9pt,
    inner ysep=1.9pt, line width=0.5pt, ver/.style=rotate=90, child anchor=north,
    parent anchor=south, anchor=center, , where level=0text width=2.2em,font=,, where
    level=1text width=4.4em,font=,, where level=2text width=3.8em,font=,, where level=3text
    width=38.6em,font=,, [ Datasets [Data Collection [Natural
  prefs: []
  type: TYPE_NORMAL
- en: Language[ E.g., NL-PS (Ferreira & Freitas, [2020a](#bib.bib43)), NaturalProofs (Welleck
    et al., [2021](#bib.bib189)), NaturalProofs-Gen (Welleck et al., [2022](#bib.bib190)),
    MATcH (Li et al., [2023](#bib.bib104))]] [Coq [ E.g., GamePad (Huang et al., [2019](#bib.bib69)),
    CoqGym (Yang & Deng, [2019](#bib.bib201)), PRISM (Reichel et al., [2023](#bib.bib146))]]
    [Isabelle [ E.g., IsarStep (Li et al., [2021a](#bib.bib105)), PISA (Jiang et al.,
    [2021](#bib.bib78)), MiniF2F (Zheng et al., [2022](#bib.bib213)), Magnushammer (Mikuła
    et al., [2024](#bib.bib119))]] [Lean [ E.g., LeanStep (Han et al., [2022](#bib.bib65)),
    MiniF2F (Zheng et al., [2022](#bib.bib213)), FIMO (Liu et al., [2023](#bib.bib110)),
    TRIGO (Xiong et al., [2023](#bib.bib198)),
  prefs: []
  type: TYPE_NORMAL
- en: ProofNet (Azerbayev et al., [2023](#bib.bib8)), LeanDojo (Yang et al., [2023](#bib.bib202)),
    MLFMF (Bauer et al., [2023](#bib.bib14))]] [HOL Light [ E.g., HolStep (Kaliszyk
    et al., [2017](#bib.bib85)), HOList (Bansal et al., [2019](#bib.bib11)), MiniF2F (Zheng
    et al., [2022](#bib.bib213))]] [Mizar [ E.g., MPTP2078 (Alama et al., [2014](#bib.bib5)),
    Mizar40 (Kaliszyk & Urban, [2015b](#bib.bib82)), M2K (Kaliszyk et al., [2018](#bib.bib86))]]
    [Geometry [ E.g., UniGeo (Chen et al., [2022](#bib.bib22)), FormalGeo (Zhang et al.,
    [2023c](#bib.bib210))]] [Other
  prefs: []
  type: TYPE_NORMAL
- en: Languages [ E.g., Holophrasm (Whalen, [2016](#bib.bib191)), TPTP (Sutcliffe,
    [2017](#bib.bib164)), TacticToe (Gauthier et al., [2020](#bib.bib57)), MLFMF (Bauer
    et al., [2023](#bib.bib14))]] [Pre-training
  prefs: []
  type: TYPE_NORMAL
- en: Corpus [ E.g., WebMath (Polu & Sutskever, [2020](#bib.bib138)), Proof-Pile (Azerbayev
    et al., [2023](#bib.bib8)), MathPile (Wang et al., [2023b](#bib.bib187)), OpenWebMath (Paster
    et al., [2024](#bib.bib129)),
  prefs: []
  type: TYPE_NORMAL
- en: Proof-Pile-v2 (Azerbayev et al., [2024](#bib.bib9)), DeepSeekMath (Shao et al.,
    [2024](#bib.bib155)), InternLM-Math (Ying et al., [2024](#bib.bib205))]] ] [Data
    Generation [Rule-based
  prefs: []
  type: TYPE_NORMAL
- en: Generator [ E.g., INT (Wu et al., [2021b](#bib.bib194)), MetaGen (Wang & Deng,
    [2020](#bib.bib183)), LIME (Wu et al., [2021c](#bib.bib195)), FwdP (Firoiu et al.,
    [2021](#bib.bib46))
  prefs: []
  type: TYPE_NORMAL
- en: AutoTrig (Liu et al., [2022b](#bib.bib113)), TRIGO-gen (Xiong et al., [2023](#bib.bib198)),
    GeomVerse (Kazemi et al., [2023](#bib.bib89)), AlphaGeometry (Trinh et al., [2024](#bib.bib173))]]
    [Iterative
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation [ E.g., DeepHOL (Bansal et al., [2019](#bib.bib11)), GPT-$f$ (Polu
    & Sutskever, [2020](#bib.bib138)), FMSCL (Polu et al., [2023](#bib.bib139)), HER (Aygün
    et al., [2022](#bib.bib7))]] [Lemma
  prefs: []
  type: TYPE_NORMAL
- en: Discovery [ E.g., LEGO-Prover (Xin et al., [2024](#bib.bib197)), REFACTOR (Zhou
    et al., [2024b](#bib.bib215))]] [Other
  prefs: []
  type: TYPE_NORMAL
- en: Methods [ E.g., MMA (Jiang et al., [2023a](#bib.bib76)), MUSTARD (Huang et al.,
    [2024](#bib.bib70))]] ] ]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The taxonomy of datasets in theorem proving.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Data Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond utilizing existing projects, researchers also study the generation of
    new proof data. A line of work (Wu et al., [2021b](#bib.bib194); [c](#bib.bib195);
    Firoiu et al., [2021](#bib.bib46); Liu et al., [2022b](#bib.bib113); Xiong et al.,
    [2023](#bib.bib198); Kazemi et al., [2023](#bib.bib89); Trinh et al., [2024](#bib.bib173))
    develops rule-based generators to generate both theorems and proofs by iteratively
    sampling inference rules and axioms from a pre-defined set to form a new theorem.
    Such a generation process enables manual control of difficulty. For example, INT (Wu
    et al., [2021b](#bib.bib194)) synthesizes theorems for inequalities with different
    proof lengths while GeomVerse (Kazemi et al., [2023](#bib.bib89)) targets geometry
    problems with 5 dimensions of difficulty design. MetaGen (Wang & Deng, [2020](#bib.bib183))
    further trains a neural generator to synthesize theorems similar to human-write
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative approaches turn to iteratively augment the training dataset with
    fixed theorems but newly generated proofs. A line of work (Bansal et al., [2019](#bib.bib11);
    Polu & Sutskever, [2020](#bib.bib138); Polu et al., [2023](#bib.bib139)) adopts
    the idea of expert iteration (Silver et al., [2018](#bib.bib157)), which repeatedly
    applies the trained prover on existing theorems and adds the successful proof
    paths as new data points to train the prover further. Aygün et al. ([2022](#bib.bib7))
    further proposes to adapt hindsight experience replay (Aygün et al., [2022](#bib.bib7))
    to FOL provers, which leverages previously unsuccessful proof trajectories by
    viewing their final states as the desired ones. Meanwhile, a line of reinforcement
    learning methods can also be viewed in this category.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, some works aim to generate intermediate helpful lemmas. REFACTOR (Zhou
    et al., [2024b](#bib.bib215)) trains a GNN to extract lemmas from proofs, which
    can be used to streamline the proofs of other theorems. LEGO-Prover (Xin et al.,
    [2024](#bib.bib197)) prompts GPT-4 to generate sub-goal lemmas for the proof of
    a theorem and finalize it by proving or retrieving these lemmas. Newly proven
    lemmas are added to a growing library for future use. Works on conjecturing are
    also relevant to this field despite the generated conjectures may be incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, several recent approaches leverage auto(in)formalization for data
    generation. Using GPT-4, MMA (Jiang et al., [2023a](#bib.bib76)) informalizes
    all theorem statements in Archive of Formal Proofs and mathlib, and MUSTARD (Huang
    et al., [2024](#bib.bib70)) synthesizes problems from seed concepts, crafts informal
    proofs, and translates them into Lean to verify the correctness.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we focus on the evaluations of deep learning approaches in
    theorem proving, analyzing the key metrics and state-of-the-art performance for
    each task.
  prefs: []
  type: TYPE_NORMAL
- en: Autoformalization. The assessment of autoformalization mainly relies on manually
    checking the equivalence between informal and formalized statements. Through such
    evaluation, recent studies (Wu et al., [2022](#bib.bib196); Azerbayev et al.,
    [2023](#bib.bib8)) show that state-of-the-art LLMs with few-shot prompting can
    correctly formalize 25% and 13% high-school and undergraduate-level problems,
    demonstrating the challenge in autoformalization. In addition, both studies reveal
    that LLMs exhibit considerably higher efficacy in informalization, achieving around
    76% and 62% accuracies. Despite the modest success in autoformalizing statements,
    subsequent research (Jiang et al., [2023b](#bib.bib77); Zhao et al., [2023](#bib.bib211);
    Xin et al., [2024](#bib.bib197)) reveals the value of autoformalizing informal
    intermediate goals to prove theorems in modularized pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Premise selection. Retrieval metrics are widely used as the evaluation metric
    for premise selection. For example, recall at k (R@k) measures the ratio of correctly
    used premises in ground-truth proof within the top-k selections, and the mean
    reciprocal rank (MRR) computes the average reciprocal rank of the first correctly
    selected premise. Dense passage retrieval (DPR) (Karpukhin et al., [2020](#bib.bib88))
    with a Transformer encoder has significantly improved upon traditional methods,
    demonstrating a remarkable generalization ability to unseen data for premise selection.
    For example, ReProver (Yang et al., [2023](#bib.bib202)) achieves 27.6% for R@10
    and 0.24 for MRR on the LeanDojo benchmark for retrieving unseen premises in training,
    while BM25 (Robertson et al., [2009](#bib.bib147)) achieves 15.5% for R@10 and
    0.14 for MRR. Furthermore, a DPR-based retriever, Magnushammer (Mikuła et al.,
    [2024](#bib.bib119)), outperforms a symbolic method, Sledgehammer (Böhme & Nipkow,
    [2010](#bib.bib16)), when used to select premises for the Thor prover (Jiang et al.,
    [2022](#bib.bib79)), improving the theorem proving success rate from 57% to 71%
    on the PISA dataset and from 28.3% to 36.9% on the MiniF2F-valid dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Theorem proving. The effectiveness of proofstep generation, proof search, and
    support from autoformalization and premise selection can be collectively evaluated
    by their success rate in proving theorems within a test set. Recent research (Zheng
    et al., [2023](#bib.bib212); Xin et al., [2024](#bib.bib197)) shows impressive
    performance increases using state-of-the-art LLMs like GPT-4 in structured frameworks
    than tactic-based models with the best-first search. For instance, LEGO-Prover (Xin
    et al., [2024](#bib.bib197)) achieves 57.0% accuracy and 50.0% accuracy on the
    valid and test set of MiniF2F, while the previous best model Thor (Jiang et al.,
    [2022](#bib.bib79)) with expert iteration training (Wu et al., [2022](#bib.bib196))
    achieves 37.3% and 35.2%. Moreover, deep learning-based proof searches could further
    improve the performance of ITP/ATP systems. HTPS (Lample et al., [2022](#bib.bib100))
    achieves an accumulative successful rate of 58.6% on MiniF2F-valid through online
    training and a 41.0% success rate with 64 search attempts on MiniF2F-test. Besides,
    the reinforcement learning-based ATP prover NIAGRA (Fokoue et al., [2023](#bib.bib50))
    outperforms both E (Schulz, [2002](#bib.bib153)) and Vampire (Kovács & Voronkov,
    [2013](#bib.bib93)) on the MPTP2078 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Caveats. Tasks related to theorem proving can be tricky to evaluate. For autoformalization,
    manual evaluation is costly, whereas automated metrics are inaccurate. For example,
    the compilation rate evaluates only syntactic correctness, and the BLEU score (Papineni
    et al., [2002](#bib.bib128)) struggles with formalizations semantically similar
    but not logically equivalent to the ground truth. For premise selection, metrics
    rely on the premises that are used in ground-truth proofs, so they may neglect
    other valid premises for alternative correct proofs different from the ground-truth
    ones, causing false negatives. Lastly, the evaluation of theorem proving is complicated
    by various experimental setups (§ [6.1](#S6.SS1 "6.1 Challenges ‣ 6 Discussions
    ‣ A Survey on Deep Learning for Theorem Proving")).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite significant progress, deep learning for theorem proving still faces
    many challenges, including data scarcity, disunified evaluation protocols, and
    human-AI interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Data scarcity. The amount of formal proof data is growing but is still far behind
    other domains where LLMs are successful, e.g., code generation. The largest corpora
    of Isabelle proofs, Archive of Formal Proofs, currently contains 250K proofs.
    Lean’s mathlib contains 140K proofs. This amount of data is decent for small models
    (e.g., billions of parameters) but insufficient for models with hundreds of billions
    of parameters. Although the use of rule-based generators could offer some assistance,
    the complexity and quality of the generated data often do not match that of human-written
    ones. Furthermore, autoformalization is even more data-scarce, due to the difficulty
    in obtaining aligned informal-formal pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation. Compared to traditional deep learning tasks such as classification,
    it is much harder to evaluate the performance of theorem provers comprehensively.
    First, results across different proof assistants are not comparable. Even though
    MiniF2F is available across multiple proof assistants, the impact due to proof
    automation (e.g., Isabelle has Sledgehammer (Böhme & Nipkow, [2010](#bib.bib16))
    whereas Lean does not) often outweighs the differences due to deep learning models.
    Second, the resource constraints (e.g., time, #attempts) during evaluation may
    impact the performance and the relative ranking of different methods. Compared
    to time constraints, #attempts favors bigger models such as LLMs. In contrast,
    a tight time constraint arising from real-time applications may favor simple but
    fast models. Without a specific application as the context, it is unclear what
    evaluation setting makes the most sense. We as a community still lack a systematic
    understanding of the trade-off, despite nascent efforts (Rute et al., [2024](#bib.bib149)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Human-AI interaction. One motivation for theorem proving is to assist human
    mathematicians (Castelvecchi, [2021](#bib.bib21); Sørensen et al., [2021](#bib.bib160)).
    However, existing research has led to surprisingly few tools useful for them.
    Current methods are evaluated following standard deep learning protocols: running
    neural networks or querying LLMs in a Python program, testing the prover on a
    dataset, and calculating the percentage of successfully proved ones. This practice
    is misaligned with the needs of mathematicians. First, mathematicians need a tool
    that can be called easily in a proof assistant. Second, the tool must run on consumer
    CPUs with low latency. Third, instead of a performance measure, mathematicians
    care more about whether the prover can help with the specific theorems they are
    working on. These theorems are often out of the training distribution and pose
    a challenge for the prover to generalize to other domains. Although initial efforts
    have been made to develop user-oriented tools (Song et al., [2023](#bib.bib159);
    Welleck & Saha, [2023](#bib.bib188)), there is abundant room in improving the
    user experience and exploring other forms of interaction, which requires close
    collaboration between deep learning researchers and mathematicians (Collins et al.,
    [2023](#bib.bib31)).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Future Directions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Combining deep learning, especially LLMs, with formal mathematics provides
    a promising avenue for enhancing the math capabilities of AI and may significantly
    impact various disciplines. We conclude our survey paper by listing a few future
    directions we are particularly excited about, envisioning significant strides
    in these burgeoning domains:'
  prefs: []
  type: TYPE_NORMAL
- en: Conjecturing. Beyond merely proving theorems, mathematicians would always explore
    theories in a domain, identify underlying problem structures, and formulate new
    conjectures. These explorative activities around conjecturing are indispensable
    for mathematicians but are currently limited in AI (Urban & Jakubüv, [2020](#bib.bib176);
    Johansson & Smallbone, [2023](#bib.bib80)). By enabling AI to conjecture, it can
    explore the space of mathematics autonomously. Furthermore, when combined with
    theorem proving, such exploration can be used to discover new math knowledge.
    A direct application of conjecturing is to generate useful theorems and proofs,
    which could mitigate the data scarcity inherent in theorem proving.
  prefs: []
  type: TYPE_NORMAL
- en: Verified code generation. As AI coding assistants such as GitHub Copilot become
    prevalent, it is increasingly important to be able to verify LLM-generated code.
    Proof assistants, especially Coq, have been widely used for software verification (Leroy
    et al., [2016](#bib.bib102)). Therefore, methods for theorem proving surveyed
    in this paper may play a role in generating verified code. There is a plethora
    of problems to explore in this space. For example, one can prompt LLMs to synthesize
    inductive loop invariants (Kamath et al., [2023](#bib.bib87)) or generate programs
    in verification-friendly languages such as Dafny (Sun et al., [2023](#bib.bib163)).
  prefs: []
  type: TYPE_NORMAL
- en: Math education. A roadblock to democratizing math education is the lack of qualified
    tutors to provide feedback to students (Liang et al., [2023b](#bib.bib108)). Formal
    mathematics can potentially mitigate the problem by providing an environment for
    students to explore and receive automatic and reliable feedback. AI has demonstrated
    promise in guiding students in this process. For example, Kevin Buzzard reported
    that Lean Copilot could help prove a number of theorems in his undergraduate course
    and answer questions on Lean Zulip (Buzzard, [2024](#bib.bib20)). To further integrate
    AI-driven formal tutoring into mainstream education, informalization is also essential
    to make these formal proofs accessible to students who are not familiar with formal
    languages. Looking ahead, we believe that LLMs and theorem proving could lead
    to intelligent tutors in a variety of STEM classes.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abdelaziz et al. (2020) Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, and
    Achille Fokoue. An experimental study of formula embeddings for automated theorem
    proving in first-order logic. *arXiv preprint arXiv:2002.00423*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Abdelaziz et al. (2022) Ibrahim Abdelaziz, Maxwell Crouse, Bassem Makni, Vernon
    Austel, Cristina Cornelio, Shajith Ikbal, Pavan Kapanipathi, Ndivhuwo Makondo,
    Kavitha Srinivas, Michael Witbrock, et al. Learning to guide a saturation-based
    theorem prover. *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. GPT-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agrawal et al. (2022) Ayush Agrawal, Siddhartha Gadgil, Navin Goyal, Ashvni
    Narayanan, and Anand Tadipatri. Towards a mathematics formalisation assistant
    using large language models. *arXiv preprint arXiv:2211.07524*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alama et al. (2014) Jesse Alama, Tom Heskes, Daniel Kühlwein, Evgeni Tsivtsivadze,
    and Josef Urban. Premise selection for mathematics by corpus analysis and kernel
    methods. *Journal of Automated Reasoning*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aygün et al. (2020) Eser Aygün, Zafarali Ahmed, Ankit Anand, Vlad Firoiu, Xavier
    Glorot, Laurent Orseau, Doina Precup, and Shibl Mourad. Learning to prove from
    synthetic theorems. *arXiv preprint arXiv:2006.11259*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aygün et al. (2022) Eser Aygün, Ankit Anand, Laurent Orseau, Xavier Glorot,
    Stephen M Mcaleer, Vlad Firoiu, Lei M Zhang, Doina Precup, and Shibl Mourad. Proving
    theorems using incremental learning and hindsight experience replay. In *Proceedings
    of the 39th International Conference on Machine Learning*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azerbayev et al. (2023) Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf,
    Edward W Ayers, Dragomir Radev, and Jeremy Avigad. ProofNet: Autoformalizing and
    formally proving undergraduate-level mathematics. *arXiv preprint arXiv:2302.12433*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Azerbayev et al. (2024) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,
    Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman,
    and Sean Welleck. Llemma: An open language model for mathematics. In *The Twelfth
    International Conference on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bansal & Szegedy (2020) Kshitij Bansal and Christian Szegedy. Learning alignment
    between formal & informal mathematics. In *5th Conference on Artificial Intelligence
    and Theorem Proving*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bansal et al. (2019) Kshitij Bansal, Sarah M. Loos, Markus Norman Rabe, Christian
    Szegedy, and Stewart Wilcox. HOList: An environment for machine learning of higher
    order logic theorem proving. In *Proceedings of the 36th International Conference
    on Machine Learning*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barras et al. (1999) Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël
    Courant, Yann Coscoy, David Delahaye, Daniel de Rauglaudre, Jean-Christophe Filliâtre,
    Eduardo Giménez, Hugo Herbelin, et al. The Coq proof assistant reference manual.
    *INRIA*, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bártek & Suda (2023) Filip Bártek and Martin Suda. How much should this symbol
    weigh? a gnn-advised clause selection. In *Proceedings of 24th International Conference
    on Logic for Programming, Artificial Intelligence and Reasoning*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bauer et al. (2023) Andrej Bauer, Matej Petković, and Ljupco Todorovski. MLFMF:
    Data sets for machine learning for mathematical formalization. In *Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blanchette et al. (2016) Jasmin Christian Blanchette, Cezary Kaliszyk, Lawrence C
    Paulson, and Josef Urban. Hammering towards QED. *Journal of Formalized Reasoning*,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Böhme & Nipkow (2010) Sascha Böhme and Tobias Nipkow. Sledgehammer: Judgement
    day. In *Proceedings of the 5th International Joint Conference on Automated Reasoning*,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brandfonbrener et al. (2024) David Brandfonbrener, Sibi Raja, Tarun Prasad,
    Chloe Loughridge, Jianang Yang, Simon Henniger, William E Byrd, Robert Zinkov,
    and Nada Amin. Verified multi-step synthesis using large language models and monte
    carlo tree search. *arXiv preprint arXiv:2402.08147*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. In *Proceedings of the 34th
    International Conference on Neural Information Processing Systems*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bruijn, de (1970) N.G. Bruijn, de. The mathematical language AUTOMATH, its usage
    and some of its extensions. In *Proceedings Symposium on Automatic Demonstration*,
    1970.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buzzard (2024) Kevin Buzzard. Lean in 2024. [https://xenaproject.wordpress.com/2024/01/20/lean-in-2024/](https://xenaproject.wordpress.com/2024/01/20/lean-in-2024/),
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castelvecchi (2021) Davide Castelvecchi. Mathematicians welcome computer-assisted
    proof in ‘grand unification’ theory. *Nature*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu
    Chen, and Xiaodan Liang. UniGeo: Unifying geometry logical reasoning via reformulating
    mathematical expression. In *Proceedings of the 2022 Conference on Empirical Methods
    in Natural Language Processing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey
    Hinton. A simple framework for contrastive learning of visual representations.
    In *Proceedings of the 37th International Conference on Machine Learning*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2014) Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations
    using RNN encoder–decoder for statistical machine translation. In *Proceedings
    of the 2014 Conference on Empirical Methods in Natural Language Processing*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chou et al. (2000) Shang-Ching Chou, Xiao-Shan Gao, and Jing-Zhong Zhang. A
    deductive database approach to automated geometry theorem proving and discovering.
    *Journal of Automated Reasoning*, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chvalovskỳ et al. (2019) Karel Chvalovskỳ, Jan Jakubüv, Martin Suda, and Josef
    Urban. ENIGMA-NG: Efficient neural and gradient-boosted inference guidance for
    E. In *27th International Conference on Automated Deduction*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chvalovskỳ et al. (2021) Karel Chvalovskỳ, Jan Jakubüv, Miroslav Olšák, and
    Josef Urban. Learning theorem proving components. In *30th International Conference
    on Automated Reasoning with Analytic Tableaux and Related Methods*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chvalovskỳ et al. (2023) Karel Chvalovskỳ, Konstantin Korovin, Jelle Piepenbrock,
    and Josef Urban. Guiding an instantiation prover with graph neural networks. In
    *Proceedings of 24th International Conference on Logic for Programming, Artificial
    Intelligence and Reasoning*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collins et al. (2023) Katherine M Collins, Albert Q Jiang, Simon Frieder, Lionel
    Wong, Miri Zilka, Umang Bhatt, Thomas Lukasiewicz, Yuhuai Wu, Joshua B Tenenbaum,
    William Hart, et al. Evaluating language models for mathematics through interactions.
    *arXiv preprint arXiv:2306.01694*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crouse et al. (2019) Maxwell Crouse, Ibrahim Abdelaziz, Cristina Cornelio, Veronika
    Thost, Lingfei Wu, Kenneth Forbus, and Achille Fokoue. Improving graph neural
    network representations of logical formulae with subgraph pooling. *arXiv preprint
    arXiv:1911.06904*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crouse et al. (2021) Maxwell Crouse, Ibrahim Abdelaziz, Bassem Makni, Spencer
    Whitehead, Cristina Cornelio, Pavan Kapanipathi, Kavitha Srinivas, Veronika Thost,
    Michael Witbrock, and Achille Fokoue. A deep reinforcement learning approach to
    first-order logic theorem proving. In *Proceedings of the Thirty-Fifth AAAI Conference
    on Artificial Intelligence*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cunningham et al. (2022) Garett Cunningham, Razvan C Bunescu, and David Juedes.
    Towards autoformalization of mathematics and code correctness: Experiments with
    elementary proofs. In *Proceedings of the 1st Workshop on Mathematical Natural
    Language Processing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Czajka & Kaliszyk (2018) Łukasz Czajka and Cezary Kaliszyk. Hammer for Coq:
    Automation for dependent type theory. *Journal of automated reasoning*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dastgheib & Asgari (2022) Doratossadat Dastgheib and Ehsaneddin Asgari. Keyword-based
    natural language premise selection for an automatic mathematical statement proving.
    In *Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis (1957) Martin Davis. A computer program for Presburger’s algorithm. *Symbolic
    Computation Automation of Reasoning 1*, 1957.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Davis & Putnam (1960) Martin Davis and Hilary Putnam. A computing procedure
    for quantification theory. *Journal of the ACM*, 1960.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denzinger et al. (1999) Jörg Denzinger, Matthias Fuchs, Christoph Goller, and
    Stephan Schulz. Learning from previous proof experience: A survey. *Technical
    Report*, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duvenaud et al. (2015) David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre,
    Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. Convolutional
    networks on graphs for learning molecular fingerprints. In *Proceedings of the
    28th International Conference on Neural Information Processing Systems*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fawzi et al. (2019) Alhussein Fawzi, Mateusz Malinowski, Hamza Fawzi, and Omar
    Fawzi. Learning dynamic polynomial proofs. In *Proceedings of the 33rd International
    Conference on Neural Information Processing Systems*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferreira & Freitas (2020a) Deborah Ferreira and André Freitas. Natural language
    premise selection: Finding supporting statements for mathematical text. In *Proceedings
    of the Twelfth Language Resources and Evaluation Conference*, 2020a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferreira & Freitas (2020b) Deborah Ferreira and André Freitas. Premise selection
    in natural language mathematical texts. In *Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ferreira & Freitas (2021) Deborah Ferreira and André Freitas. STAR: Cross-modal
    [STA]tement [R]epresentation for selecting relevant mathematical premises. In
    *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistic*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Firoiu et al. (2021) Vlad Firoiu, Eser Aygun, Ankit Anand, Zafarali Ahmed, Xavier
    Glorot, Laurent Orseau, Lei Zhang, Doina Precup, and Shibl Mourad. Training a
    first-order theorem prover from synthetic data. In *9th International Conference
    on Learning Representations Workshop on Mathematical Reasoning in General Artificial
    Intelligence*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First & Brun (2022) Emily First and Yuriy Brun. Diversity-driven automated formal
    verification. In *Proceedings of the 44th International Conference on Software
    Engineering*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First et al. (2020) Emily First, Yuriy Brun, and Arjun Guha. TacTok: Semantics-aware
    proof synthesis. *Proceedings of the ACM on Programming Languages*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First et al. (2023) Emily First, Markus Rabe, Talia Ringer, and Yuriy Brun.
    Baldur: Whole-proof generation and repair with large language models. In *Proceedings
    of the 31st ACM Joint European Software Engineering Conference and Symposium on
    the Foundations of Software Engineering*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fokoue et al. (2023) Achille Fokoue, Ibrahim Abdelaziz, Maxwell Crouse, Shajith
    Ikbal, Akihiro Kishimoto, Guilherme Lima, Ndivhuwo Makondo, and Radu Marinescu.
    An ensemble approach for automated theorem proving based on efficient name invariant
    graph neural representations. In *Proceedings of the Thirty-Second International
    Joint Conference on Artificial Intelligence*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frieder et al. (2023a) Simon Frieder, Julius Berner, Philipp Petersen, and Thomas
    Lukasiewicz. Large language models for mathematicians. *arXiv preprint arXiv:2312.04556*,
    2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frieder et al. (2023b) Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys
    Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen,
    and Julius Berner. Mathematical capabilities of ChatGPT. In *Thirty-seventh Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frieder et al. (2023c) Simon Frieder, Martin Trimmel, Rashid Alawadhi, and Klaus
    Gy. LLM vs ITP. In *37th Conference on Neural Information Processing Systems Workshop
    on MATH-AI*, 2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gadgil et al. (2022) Siddhartha Gadgil, Anand Rao Tadipatri, Ayush Agrawal,
    Ashvni Narayanan, and Navin Goyal. Towards automating formalisation of theorem
    statements using large language models. In *36th Conference on Neural Information
    Processing Systems Workshop on MATH-AI*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gauthier (2020) Thibault Gauthier. Deep reinforcement learning for synthesizing
    functions in higher-order logic. In *Proceedings of 23rd International Conference
    on Logic for Programming, Artificial Intelligence and Reasoning*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gauthier (2021) Thibault Gauthier. Learned provability likelihood for tactical
    search. In *Proceedings of the 9th International Symposium on Symbolic Computation
    in Software Science*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gauthier et al. (2020) Thibault Gauthier, Cezary Kaliszyk, and Josef Urban.
    TacticToe: Learning to prove with tactics. *Journal of Automated Reasoning*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gloeckle et al. (2023) Fabian Gloeckle, Baptiste Roziere, Amaury Hayat, and
    Gabriel Synnaeve. Temperature-scaled large language models for Lean proofstep
    prediction. In *37th Conference on Neural Information Processing Systems Workshop
    on MATH-AI*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goertzel & Urban (2019) Zarathustra Goertzel and Josef Urban. Usefulness of
    lemmas via graph neural networks. In *4th Conference on Artificial Intelligence
    and Theorem Proving*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goertzel et al. (2021) Zarathustra A Goertzel, Karel Chvalovskỳ, Jan Jakubüv,
    Miroslav Olšák, and Josef Urban. Fast and slow ENIGMAs and parental guidance.
    In *13th International Symposium on Frontiers of Combining Systems*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goertzel et al. (2022) Zarathustra A Goertzel, Jan Jakubüv, Cezary Kaliszyk,
    Miroslav Olšák, Jelle Piepenbrock, and Josef Urban. The Isabelle ENIGMA. In *13th
    International Conference on Interactive Theorem Proving*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gonthier (2008) Georges Gonthier. The four colour theorem: Engineering of a
    formal proof. In *8th Asian Symposium on Computer Mathematics*, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hales et al. (2017) Thomas Hales, Mark Adams, Gertrud Bauer, Tat Dat Dang, John
    Harrison, Hoang Le Truong, Cezary Kaliszyk, Victor Magron, Sean McLaughlin, Tat Thang
    Nguyen, et al. A formal proof of the Kepler conjecture. In *Forum of Mathematics,
    Pi*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2021) Jesse Michael Han, Tao Xu, Stanislas Polu, Arvind Neelakantan,
    and Alec Radford. Contrastive finetuning of generative language models for informal
    premise selection. In *6th Conference on Artificial Intelligence and Theorem Proving*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022) Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward W Ayers,
    and Stanislas Polu. Proof artifact co-training for theorem proving with language
    models. In *The Tenth International Conference on Learning Representations*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Harrison (1996) John Harrison. HOL Light: A tutorial introduction. In *International
    Conference on Formal Methods in Computer-Aided Design*, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2024) Yiming He, Jia Zou, Xiaokai Zhang, Na Zhu, and Tuo Leng. FGeo-TP:
    A language model-enhanced solver for geometry problems. *arXiv preprint arXiv:2402.09047*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holden & Korovin (2023) Edvard K Holden and Konstantin Korovin. Graph sequence
    learning for premise selection. In *8th Conference on Artificial Intelligence
    and Theorem Proving*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2019) Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever.
    GamePad: A learning environment for theorem proving. In *The Seventh International
    Conference on Learning Representations*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2024) Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao,
    Huajian Xin, Haiming Wang, Zhenguo Li, Linqi Song, and Xiaodan Liang. MUSTARD:
    Mastering uniform synthesis of theorem and proof data. In *The Twelfth International
    Conference on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irving et al. (2016) Geoffrey Irving, Christian Szegedy, Alexander A Alemi,
    Niklas Eén, François Chollet, and Josef Urban. DeepMath - deep sequence models
    for premise selection. In *Proceedings of the 30th International Conference on
    Neural Information Processing Systems*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jakubüv & Urban (2017) Jan Jakubüv and Josef Urban. ENIGMA: efficient learning-based
    inference guiding machine. In *10th International Conference on Intelligent Computer
    Mathematics*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jakubüv & Urban (2019) Jan Jakubüv and Josef Urban. Hammering Mizar by learning
    clause guidance. In *10th International Conference on Interactive Theorem Proving*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jakubüv et al. (2020) Jan Jakubüv, Karel Chvalovskỳ, Miroslav Olšák, Bartosz
    Piotrowski, Martin Suda, and Josef Urban. ENIGMA anonymous: Symbol-independent
    inference guiding machine (system description). In *Proceedings of the 10th International
    Joint Conference on Automated Reasoning*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jakubüv et al. (2023) Jan Jakubüv, Karel Chvalovskỳ, Zarathustra Goertzel, Cezary
    Kaliszyk, Mirek Olšák, Bartosz Piotrowski, Stephan Schulz, Martin Suda, and Josef
    Urban. MizAR 60 for Mizar 50. In *14th International Conference on Interactive
    Theorem Proving*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023a) Albert Q Jiang, Wenda Li, and Mateja Jamnik. Multilingual
    mathematical autoformalization. *arXiv preprint arXiv:2311.03755*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2023b) Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li,
    Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample.
    Draft, sketch, and prove: Guiding formal theorem provers with informal proofs.
    In *The Eleventh International Conference on Learning Representations*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2021) Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and
    Yuhuai Wu. LISA: Language models of Isabelle proofs. In *6th Conference on Artificial
    Intelligence and Theorem Proving*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jiang et al. (2022) Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad
    Czechowski, Tomasz Odrzygóźdź, Piotr Miłoś, Yuhuai Wu, and Mateja Jamnik. Thor:
    Wielding hammers to integrate language models and automated theorem provers. In
    *Proceedings of the 36th International Conference on Neural Information Processing
    Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johansson & Smallbone (2023) Moa Johansson and Nicholas Smallbone. Exploring
    mathematical conjecturing with large language models. In *17th International Workshop
    on Neural-Symbolic Learning and Reasoning*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliszyk & Urban (2015a) Cezary Kaliszyk and Josef Urban. FEMaLeCoP: Fairly
    efficient machine learning connection prover. In *Proceedings of 20th International
    Conference on Logic for Programming, Artificial Intelligence and Reasoning*, 2015a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaliszyk & Urban (2015b) Cezary Kaliszyk and Josef Urban. MizAR 40 for Mizar
    40. *Journal of Automated Reasoning*, 2015b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliszyk et al. (2014) Cezary Kaliszyk, Josef Urban, Jiří Vyskočil, and Herman
    Geuvers. Developing corpus-based translation methods between informal and formal
    mathematics: Project description. In *International Conference on Intelligent
    Computer Mathematics*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaliszyk et al. (2015) Cezary Kaliszyk, Josef Urban, and Jiří Vyskočil. Learning
    to parse on aligned corpora (rough diamond). In *6th International Conference
    on Interactive Theorem Proving*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaliszyk et al. (2017) Cezary Kaliszyk, François Chollet, and Christian Szegedy.
    HolStep: A machine learning dataset for higher-order logic theorem proving. In
    *The Fifth International Conference on Learning Representations*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaliszyk et al. (2018) Cezary Kaliszyk, Josef Urban, Henryk Michalewski, and
    Miroslav Olšák. Reinforcement learning of theorem proving. In *Proceedings of
    the 32nd International Conference on Neural Information Processing Systems*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kamath et al. (2023) Adharsh Kamath, Aditya Senthilnathan, Saikat Chakraborty,
    Pantazis Deligiannis, Shuvendu K Lahiri, Akash Lal, Aseem Rastogi, Subhajit Roy,
    and Rahul Sharma. Finding inductive loop invariants using large language models.
    *arXiv preprint arXiv:2311.07948*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval
    for open-domain question answering. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazemi et al. (2023) Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu,
    Xi Chen, and Radu Soricut. GeomVerse: A systematic evaluation of large models
    for geometric reasoning. *arXiv preprint arXiv:2312.12241*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kern & Greenstreet (1999) Christoph Kern and Mark R Greenstreet. Formal verification
    in hardware design: A survey. *ACM Transactions on Design Automation of Electronic
    Systems*, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Klein et al. (2009) Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick,
    David Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolanski,
    Michael Norrish, et al. seL4: Formal verification of an os kernel. In *Proceedings
    of the ACM SIGOPS 22nd Symposium on Operating Systems Principles*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Korovin (2008) Konstantin Korovin. iProver–an instantiation-based theorem prover
    for first-order logic (system description). In *Proceedings of the 4th International
    Joint Conference on Automated Reasoning*, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kovács & Voronkov (2013) Laura Kovács and Andrei Voronkov. First-order theorem
    proving and Vampire. In *International Conference on Computer Aided Verification*,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kovriguina et al. (2022) Liubov Kovriguina, Roman Teucher, and Robert Wardenga.
    TextGraphs-16 natural language premise selection task: Zero-shot premise selection
    with prompting generative language models. In *Proceedings of TextGraphs-16: Graph-based
    Methods for Natural Language Processing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kucik & Korovin (2018) Andrzej Stanisław Kucik and Konstantin Korovin. Premise
    selection with neural networks and distributed representation of features. *arXiv
    preprint arXiv:1807.10268*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kühlwein et al. (2012) Daniel Kühlwein, Twan van Laarhoven, Evgeni Tsivtsivadze,
    Josef Urban, and Tom Heskes. Overview and evaluation of premise selection techniques
    for large theory mathematics. In *Proceedings of the 6th International Joint Conference
    on Automated Reasoning*, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kusumoto et al. (2018) Mitsuru Kusumoto, Keisuke Yahata, and Masahiro Sakai.
    Automated theorem proving in intuitionistic propositional logic by deep reinforcement
    learning. *arXiv preprint arXiv:1811.00796*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample & Conneau (2019) Guillaume Lample and Alexis Conneau. Cross-lingual language
    model pretraining. In *Proceedings of the 33rd International Conference on Neural
    Information Processing Systems*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample et al. (2018) Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. Phrase-based & neural unsupervised machine translation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux,
    Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet.
    Hypertree proof search for neural theorem proving. In *Proceedings of the 36th
    International Conference on Neural Information Processing Systems*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2020) Dennis Lee, Christian Szegedy, Markus N Rabe, Sarah M Loos,
    and Kshitij Bansal. Mathematical reasoning in latent space. In *The Eighth International
    Conference on Learning Representations*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leroy et al. (2016) Xavier Leroy, Sandrine Blazy, Daniel Kästner, Bernhard Schommer,
    Markus Pister, and Christian Ferdinand. CompCert–a formally verified optimizing
    compiler. In *Proceeding of the 8th European Congress on Embedded Real Time Software
    and Systems*, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language
    models. *Proceedings of the 36th International Conference on Neural Information
    Processing Systems*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Weixian Waylon Li, Yftah Ziser, Maximin Coavoux, and Shay B
    Cohen. BERT is not the count: Learning to match mathematical statements with proofs.
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021a) Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C Paulson. IsarStep:
    A benchmark for high-level mathematical reasoning. In *The Ninth International
    Conference on Learning Representations*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021b) Zhaoyu Li, Binghong Chen, and Xujie Si. Graph contrastive
    pre-training for effective theorem reasoning. *38th International Conference on
    Machine Learning Workshop on Self-Supervised Learning for Reasoning and Perception*,
    2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2023a) Zhenwen Liang, Tianyu Yang, Jipeng Zhang, and Xiangliang
    Zhang. UniMath: A foundational and multimodal mathematical reasoner. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2023b) Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark,
    Xiangliang Zhang, and Ashwin Kalyan. Let GPT be a math tutor: Teaching math word
    problem solvers with customized exercise generation. In *The 2023 Conference on
    Empirical Methods in Natural Language Processing*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) Qika Lin, Jun Liu, Lingling Zhang, Yudai Pan, Xin Hu, Fangzhi
    Xu, and Hongwei Zeng. Contrastive graph representations for logical formulas embedding.
    *IEEE Transactions on Knowledge and Data Engineering*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan,
    Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, et al. FIMO: A challenge
    formal dataset for automated theorem proving. *arXiv preprint arXiv:2309.04295*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022a) Qinghua Liu, Yang Xu, and Xingxing He. Attention recurrent
    cross-graph neural network for selecting premises. *International Journal of Machine
    Learning and Cybernetics*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022b) Zhou Liu, Yujun Li, Zhengying Liu, Lin Li, and Zhenguo Li.
    Learning to prove trigonometric identities. *arXiv preprint arXiv:2207.06679*,
    2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loos et al. (2017) Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary
    Kaliszyk. Deep network guided proof search. In *Proceedings of 21st International
    Conference on Logic for Programming, Artificial Intelligence and Reasoning*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lovász & Schrijver (1991) László Lovász and Alexander Schrijver. Cones of matrices
    and set-functions and 0–1 optimization. *SIAM journal on optimization*, 1991.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luong et al. (2017) Minh-Thang Luong, Eugene Brevdo, and Rui Zhao. Neural machine
    translation (seq2seq) tutorial. *https://github.com/tensorflow/nmt*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: mathlib Community (2020) The mathlib Community. The Lean mathematical library.
    In *Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs
    and Proofs*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McKeown & Sutcliffe (2023) Jack McKeown and Geoff Sutcliffe. Reinforcement learning
    for guiding the E theorem prover. In *Proceedings of the Thirty-Sixth International
    Florida Artificial Intelligence Research Society Conference*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mikuła et al. (2024) Maciej Mikuła, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu
    Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuciński, Piotr Miłoś, and Yuhuai
    Wu. Magnushammer: A transformer-based approach to premise selection. In *The Twelfth
    International Conference on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Milner (1972) Robin Milner. Implementation and applications of Scott’s logic
    for computable functions. In *Proceedings of ACM Conference on Proving Assertions
    about Programs*, 1972.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mo et al. (2020) Guangshuai Mo, Yan Xiong, Wenchao Huang, and Lu Ma. Automated
    theorem proving via interacting with proof assistants by dynamic strategies. In
    *6th International Conference on Big Data Computing and Communications*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moura & Ullrich (2021) Leonardo de Moura and Sebastian Ullrich. The Lean 4 theorem
    prover and programming language. In *28th International Conference on Automated
    Deduction*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Olšák et al. (2019) Miroslav Olšák, Cezary Kaliszyk, and Josef Urban. Property
    invariant embedding for automated reasoning. In *24th European Conference on Artificial
    Intelligence*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation
    learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Otten & Bibel (2003) Jens Otten and Wolfgang Bibel. leanCoP: Lean connection-based
    theorem proving. *Journal of Symbolic Computation*, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paliwal et al. (2020) Aditya Paliwal, Sarah Loos, Markus Rabe, Kshitij Bansal,
    and Christian Szegedy. Graph representations for higher-order logic and theorem
    proving. In *Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. BLEU: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paster et al. (2024) Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and
    Jimmy Ba. OpenWebMath: An open dataset of high-quality mathematical web text.
    In *The Twelfth International Conference on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patel et al. (2023) Nilay Patel, Jeffrey Flanigan, and Rahul Saha. A new approach
    towards autoformalization. *arXiv preprint arXiv:2310.07957*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Paulson (1994) Lawrence C Paulson. *Isabelle: A Generic Theorem Prover*. Springer,
    1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peng & Ma (2017) Kebin Peng and Dianfu Ma. Tree-structure cnn for automated
    theorem proving. In *24th International Conference on Neural Information Processing*,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piepenbrock et al. (2021) Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, and
    Josef Urban. Learning equational theorem proving. In *6th Conference on Artificial
    Intelligence and Theorem Proving*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piepenbrock et al. (2022a) Jelle Piepenbrock, Tom Heskes, Mikoláš Janota, and
    Josef Urban. Guiding an automated theorem prover with neural rewriting. In *Proceedings
    of the 11th International Joint Conference on Automated Reasoning*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piepenbrock et al. (2022b) Jelle Piepenbrock, Josef Urban, Konstantin Korovin,
    Miroslav Olšák, Tom Heskes, and Mikolaš Janota. Machine learning meets the herbrand
    universe. *arXiv preprint arXiv:2210.03590*, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piotrowski & Urban (2020a) Bartosz Piotrowski and Josef Urban. Stateful premise
    selection by recurrent neural networks. In *Proceedings of 23rd International
    Conference on Logic for Programming, Artificial Intelligence and Reasoning*, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Piotrowski & Urban (2020b) Bartosz Piotrowski and Josef Urban. Guiding inferences
    in connection tableau by recurrent neural networks. In *13th International Conference
    on Intelligent Computer Mathematics*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polu & Sutskever (2020) Stanislas Polu and Ilya Sutskever. Generative language
    modeling for automated theorem proving. *arXiv preprint arXiv:2009.03393*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polu et al. (2023) Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys,
    Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning.
    In *The Eleventh International Conference on Learning Representations*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poulsen et al. (2024) Seth Poulsen, Sami Sarsa, James Prather, Juho Leinonen,
    Brett A Becker, Arto Hellas, Paul Denny, and Brent N Reeves. Solving proof block
    problems using large language models. In *Proceedings of the 55th ACM Technical
    Symposium on Computer Science Education*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proroković et al. (2021) Krsto Proroković, Michael Wand, and Jürgen Schmidhuber.
    Improving stateful premise selection with transformers. In *14th International
    Conference on Intelligent Computer Mathematics*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rabe et al. (2021) Markus N Rabe, Dennis Lee, Kshitij Bansal, and Christian
    Szegedy. Mathematical reasoning via self-supervised skip-tree training. In *The
    Ninth International Conference on Learning Representations*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rawson & Reger (2019) Michael Rawson and Giles Reger. A neurally-guided, parallel
    theorem prover. In *12th International Symposium on Frontiers of Combining Systems*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rawson & Reger (2020) Michael Rawson and Giles Reger. Directed graph networks
    for logical reasoning (extended abstract). In *Joint Proceedings of the 7th Workshop
    on Practical Aspects of Automated Reasoning (PAAR) and the 5th Satisfiability
    Checking and Symbolic Computation Workshop (SC-Square) Workshop*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rawson & Reger (2021) Michael Rawson and Giles Reger. lazyCoP: Lazy paramodulation
    meets neurally guided search. In *30th International Conference on Automated Reasoning
    with Analytic Tableaux and Related Methods*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reichel et al. (2023) Tom Reichel, R Henderson, Andrew Touchet, Andrew Gardner,
    and Talia Ringer. Proof repair infrastructure for supervised models: Building
    a large proof repair dataset. In *14th International Conference on Interactive
    Theorem Proving*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. The probabilistic
    relevance framework: BM25 and beyond. *Foundations and Trends® in Information
    Retrieval*, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rudnicki (1992) Piotr Rudnicki. An overview of the Mizar project. In *Proceedings
    of the 1992 Workshop on Types for Proofs and Programs*, 1992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rute et al. (2024) Jason Rute, Miroslav Olšák, Lasse Blaauwbroek, Fidel Ivan Schaposnik
    Massolo, Jelle Piepenbrock, and Vasily Pestun. Graph2Tac: Learning hierarchical
    representations of math concepts in theorem proving. *arXiv preprint arXiv:2401.02949*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanchez-Stern et al. (2020) Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul,
    and Sorin Lerner. Generating correctness proofs with neural networks. In *Proceedings
    of the 4th ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanchez-Stern et al. (2023) Alex Sanchez-Stern, Emily First, Timothy Zhou,
    Zhanna Kaufman, Yuriy Brun, and Talia Ringer. Passport: Improving automated formal
    verification using identifiers. *ACM Transactions on Programming Languages and
    Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheidt (2023) Gregor vom Scheidt. Experimental results from applying GPT-4
    to an unpublished formal language. *arXiv preprint arXiv:2305.12196*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulz (2002) Stephan Schulz. E–a brainiac theorem prover. *AI Communications*,
    2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schumann (2001) Johann M Schumann. *Automated Theorem Proving in Software Engineering*.
    Springer Science & Business Media, 2001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao
    Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. DeepSeekMath: Pushing the limits
    of mathematical reasoning in open language models. *arXiv preprint arXiv:2402.03300*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shminke (2023) Boris Shminke. gym-saturation: Gymnasium environments for saturation
    provers (system description). In *32nd International Conference on Automated Reasoning
    with Analytic Tableaux and Related Methods*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. (2018) David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis
    Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
    Thore Graepel, et al. A general reinforcement learning algorithm that masters
    chess, shogi, and Go through self-play. *Science*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    MPNet: Masked and permuted pre-training for language understanding. In *Proceedings
    of the 34th International Conference on Neural Information Processing Systems*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2023) Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large
    language models as copilots for theorem proving in Lean. In *37th Conference on
    Neural Information Processing Systems Workshop on MATH-AI*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sørensen et al. (2021) Henrik Kragh Sørensen, Mikkel Willum Johansen, Renee
    Hoekzema, and Hester Breman. Augmenting the human mathematician. In *9th International
    Conference on Learning Representations Workshop on Mathematical Reasoning in General
    Artificial Intelligence*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suda (2021a) Martin Suda. Vampire with a brain is a good ITP hammer. In *13th
    International Symposium on Frontiers of Combining Systems*, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suda (2021b) Martin Suda. Improving ENIGMA-style clause selection while learning
    from history. In *28th International Conference on Automated Deduction*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Chuyue Sun, Ying Sheng, Oded Padon, and Clark Barrett. Clover:
    Closed-loop verifiable code generation. *arXiv preprint arXiv:2310.17807*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutcliffe (2017) Geoff Sutcliffe. The TPTP problem library and associated infrastructure.
    *Journal of Automated Reasoning*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence
    to sequence learning with neural networks. In *Proceedings of the 27th International
    Conference on Neural Information Processing Systems*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suttner & Ertel (1990) Christian Suttner and Wolfgang Ertel. Automatic acquisition
    of search guiding heuristics. In *10th International Conference on Automated Deduction*,
    1990.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton et al. (1999) Richard S Sutton, David McAllester, Satinder Singh, and
    Yishay Mansour. Policy gradient methods for reinforcement learning with function
    approximation. In *Proceedings of the 12th International Conference on Neural
    Information Processing Systems*, 1999.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy (2020) Christian Szegedy. A promising path towards autoformalization
    and general artificial intelligence. In *13th International Conference on Intelligent
    Computer Mathematics*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2021) Christian Szegedy, Markus Rabe, and Henryk Michalewski.
    Retrieval-augmented proof step synthesis. In *6th Conference on Artificial Intelligence
    and Theorem Proving*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tai et al. (2015) Kai Sheng Tai, Richard Socher, and Christopher D Manning.
    Improved semantic representations from tree-structured long short-term memory
    networks. In *Proceedings of the 53rd Annual Meeting of the Association for Computational
    Linguistics and the 7th International Joint Conference on Natural Language Processing*,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thakur et al. (2023) Amitayush Thakur, Yeming Wen, and Swarat Chaudhuri. An
    in-context learning agent for formal theorem-proving. *arXiv preprint arXiv:2310.04353*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tran et al. (2022) Thi Hong Hanh Tran, Matej Martinc, Antoine Doucet, and Senja
    Pollak. IJS at TextGraphs-16 natural language premise selection task: Will contextual
    information improve natural language premise selection? In *Proceedings of TextGraphs-16:
    Graph-based Methods for Natural Language Processing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trinh et al. (2024) Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong.
    Solving olympiad geometry without human demonstrations. *Nature*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trust et al. (2022) Paul Trust, Provia Kadusabe, Haseeb Younis, Rosane Minghim,
    Evangelos Milios, and Ahmed Zahran. UNLPS at TextGraphs 2022 shared task: Unsupervised
    natural language premise selection in mathematical texts using sentence-MPNet.
    In *Proceedings of TextGraphs-16: Graph-based Methods for Natural Language Processing*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tworkowski et al. (2022) Szymon Tworkowski, Maciej Mikuła, Tomasz Odrzygóźdź,
    Konrad Czechowski, Szymon Antoniak, Albert Jiang, Christian Szegedy, Łukasz Kuciński,
    Piotr Miłoś, and Yuhuai Wu. Formal premise selection with language models. In
    *7th Conference on Artificial Intelligence and Theorem Proving*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban & Jakubüv (2020) Josef Urban and Jan Jakubüv. First neural conjecturing
    datasets and experiments. In *13th International Conference on Intelligent Computer
    Mathematics*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Urban et al. (2011) Josef Urban, Jiří Vyskočil, and Petr Štěpánek. MaLeCoP machine
    learning connection prover. In *20th International Conference on Automated Reasoning
    with Analytic Tableaux and Related Methods*, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van Den Oord et al. (2016) Aaron Van Den Oord, Sander Dieleman, Heiga Zen,
    Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, Koray
    Kavukcuoglu, et al. WaveNet: A generative model for raw audio. *arXiv preprint
    arXiv:1609.03499*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Proceedings of the 31st International Conference on Neural Information
    Processing Systems*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Veličković et al. (2018) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In *The
    Sixth International Conference on Learning Representations*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vishwakarma & Mishra (2023) Rahul Vishwakarma and Subhankar Mishra. Enhancing
    neural theorem proving through data augmentation and dynamic sampling method.
    *arXiv preprint arXiv:2312.14188*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun
    Yin, Jing Xiong, Enze Xie, Han Shi, Yujun Li, Lin Li, et al. DT-Solver: Automated
    theorem proving with dynamic-tree sampling guided by proof-level value function.
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang & Deng (2020) Mingzhe Wang and Jia Deng. Learning to prove theorems by
    learning to generate theorems. In *Proceedings of the 34th International Conference
    on Neural Information Processing Systems*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2017) Mingzhe Wang, Yihe Tang, Jian Wang, and Jia Deng. Premise
    selection for theorem proving by deep graph embedding. In *Proceedings of the
    31st International Conference on Neural Information Processing Systems*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Qingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments
    with neural translation of informal to formal mathematics. In *11th International
    Conference on Intelligent Computer Mathematics*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2020) Qingxiang Wang, Chad Brown, Cezary Kaliszyk, and Josef Urban.
    Exploration of neural machine translation in autoformalization of mathematics
    in Mizar. In *Proceedings of the 9th ACM SIGPLAN International Conference on Certified
    Programs and Proofs*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Zengzhi Wang, Rui Xia, and Pengfei Liu. Generative ai for
    math: Part i–MathPile: A billion-token-scale pretraining corpus for math. *arXiv
    preprint arXiv:2312.17120*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck & Saha (2023) Sean Welleck and Rahul Saha. LLMSTEP: LLM proofstep suggestions
    in Lean. In *37th Conference on Neural Information Processing Systems Workshop
    on MATH-AI*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck et al. (2021) Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi,
    Yejin Choi, and Kyunghyun Cho. NaturalProofs: Mathematical theorem proving in
    natural language. In *Thirty-fifth Conference on Neural Information Processing
    Systems Datasets and Benchmarks Track*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Welleck et al. (2022) Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi,
    and Yejin Choi. NaturalProver: Grounded mathematical proof generation with language
    models. In *Proceedings of the 36th International Conference on Neural Information
    Processing Systems*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whalen (2016) Daniel Whalen. Holophrasm: A neural automated theorem prover
    for higher-order logic. *arXiv preprint arXiv:1608.02644*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu & Wu (2021) Minchao Wu and Yuhuai Wu. Latent action space for efficient planning
    in theorem proving. In *6th Conference on Artificial Intelligence and Theorem
    Proving*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021a) Minchao Wu, Michael Norrish, Christian Walder, and Amir Dezfouli.
    TacticZero: Learning to prove theorems from scratch with deep reinforcement learning.
    *Proceedings of the 35th International Conference on Neural Information Processing
    Systems*, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021b) Yuhuai Wu, Albert Qiaochu Jiang, Jimmy Ba, and Roger Grosse.
    INT: An inequality benchmark for evaluating generalization in theorem proving.
    In *The Ninth International Conference on Learning Representations*, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021c) Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse,
    and Christian Szegedy. LIME: Learning inductive bias for primitives of mathematical
    reasoning. In *Proceedings of the 38th International Conference on Machine Learning*,
    2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2022) Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles
    Staats, Mateja Jamnik, and Christian Szegedy. Autoformalization with large language
    models. In *Proceedings of the 36th International Conference on Neural Information
    Processing Systems*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xin et al. (2024) Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying
    Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et al. LEGO-prover:
    Neural theorem proving with growing libraries. In *The Twelfth International Conference
    on Learning Representations*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2023) Jing Xiong, Jianhao Shen, Ye Yuan, Haiming Wang, Yichun
    Yin, Zhengying Liu, Lin Li, Zhijiang Guo, Qingxing Cao, Yinya Huang, et al. TRIGO:
    Benchmarking formal mathematical proof reduction for generative language models.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
    How powerful are graph neural networks? In *The Seventh International Conference
    on Learning Representations*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xue et al. (2022) Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan
    Narang, Mihir Kale, Adam Roberts, and Colin Raffel. ByT5: Towards a token-free
    future with pre-trained byte-to-byte models. *Transactions of the Association
    for Computational Linguistics*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang & Deng (2019) Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting
    with proof assistants. In *Proceedings of the 36th International Conference on
    Machine Learning*, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang
    Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem
    proving with retrieval-augmented language models. In *Proceedings of the 37th
    International Conference on Neural Information Processing Systems*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023) Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. SATLM:
    Satisfiability-aided language models using declarative prompting. In *Proceedings
    of the 37th International Conference on Neural Information Processing Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeh et al. (2023) Eric Yeh, Briland Hitaj, Sam Owre, Maena Quemener, and Natarajan
    Shankar. CoProver: A recommender system for proof construction. In *16th International
    Conference on Intelligent Computer Mathematics*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ying et al. (2024) Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan
    Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. InternLM-Math:
    Open math large language models toward verifiable reasoning. *arXiv preprint arXiv:2402.06332*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yousefzadeh & Cao (2023) Roozbeh Yousefzadeh and Xuenan Cao. Large language
    models’ understanding of math: Source criticism and extrapolation. *arXiv preprint
    arXiv:2311.07618*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023a) Liao Zhang, Lasse Blaauwbroek, Cezary Kaliszyk, and Josef
    Urban. Learning proof transformations and its applications in interactive theorem
    proving. In *International Symposium on Frontiers of Combining Systems*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2024) Lichen Zhang, Shuai Lu, and Nan Duan. Selene: Pioneering
    automated proof in software verification. *arXiv preprint arXiv:2401.07663*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023b) Shizhuo Dylan Zhang, Talia Ringer, and Emily First. Getting
    more out of large language models for proofs. In *8th Conference on Artificial
    Intelligence and Theorem Proving*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023c) Xiaokai Zhang, Na Zhu, Yiming He, Jia Zou, Qike Huang,
    Xiaoxiao Jin, Yanjun Guo, Chenyang Mao, Zhe Zhu, Dengfeng Yue, et al. FormalGeo:
    The first step toward human-like imo-level geometric automated reasoning. *arXiv
    preprint arXiv:2310.18021*, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2023) Xueliang Zhao, Wenda Li, and Lingpeng Kong. Decomposing
    the enigma: Subgoal-based demonstration learning for formal theorem proving. *arXiv
    preprint arXiv:2305.16366*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu,
    Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating
    dual correction in automated theorem proving. *arXiv preprint arXiv:2309.15806*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2022) Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F:
    A cross-system benchmark for formal olympiad-level mathematics. In *The Tenth
    International Conference on Learning Representations*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2024a) Jin Peng Zhou, Charles E Staats, Wenda Li, Christian Szegedy,
    Kilian Q Weinberger, and Yuhuai Wu. Don’t trust: Verify–grounding llm quantitative
    reasoning with autoformalization. In *The Twelfth International Conference on
    Learning Representations*, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2024b) Jin Peng Zhou, Yuhuai Wu, Qiyang Li, and Roger Grosse.
    REFACTOR: Learning to extract theorems from proofs. In *The Twelfth International
    Conference on Learning Representations*, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zombori et al. (2021a) Zsolt Zombori, Adrián Csiszárik, Henryk Michalewski,
    Cezary Kaliszyk, and Josef Urban. Towards finding longer proofs. In *30th International
    Conference on Automated Reasoning with Analytic Tableaux and Related Methods*,
    2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zombori et al. (2021b) Zsolt Zombori, Josef Urban, and Miroslav Olšák. The role
    of entropy in guiding a connection prover. In *30th International Conference on
    Automated Reasoning with Analytic Tableaux and Related Methods*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
