- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:00'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2403.05314] Advances of Deep Learning in Protein Science: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05314](https://ar5iv.labs.arxiv.org/html/2403.05314)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul \ArticleTypeREVIEW \Year2022 \Month \Vol \No \DOI \ArtNo \ReceiveDate \ReviseDate
    \AcceptDate \OnlineDate
  prefs: []
  type: TYPE_NORMAL
- en: Title keyword 5 for citation Title for citation Title for citation
  prefs: []
  type: TYPE_NORMAL
- en: Stan.ZQ.Li@westlake.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: \AuthorMark
  prefs: []
  type: TYPE_NORMAL
- en: Hu B Z, et al
  prefs: []
  type: TYPE_NORMAL
- en: \AuthorCitation
  prefs: []
  type: TYPE_NORMAL
- en: Hu B Z, et al
  prefs: []
  type: TYPE_NORMAL
- en: 'Advances of Deep Learning in Protein Science: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bozhen HU    Cheng TAN    Lirong WU    Jiangbin ZHENG    Jun XIA    Zhangyang
    GAO    Zicheng LIU    Fandi WU    Guijun ZHANG    Stan Z. LI Zhejiang University,
    Hangzhou 310058, China AI Division, School of Engineering, Westlake University,
    Hangzhou 310030, China Tencent AI Lab, Shenzhen 518054, China Zhejiang University
    of Technology, Hangzhou 310014, China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Protein representation learning plays a crucial role in understanding the structure
    and function of proteins, which are essential biomolecules involved in various
    biological processes. In recent years, deep learning has emerged as a powerful
    tool for protein modeling due to its ability to learn complex patterns and representations
    from large-scale protein data. This comprehensive survey aims to provide an overview
    of the recent advances in deep learning techniques applied to protein science.
    The survey begins by introducing the developments of deep learning based protein
    models and emphasizes the importance of protein representation learning in drug
    discovery, protein engineering, and function annotation. It then delves into the
    fundamentals of deep learning, including convolutional neural networks, recurrent
    neural networks, attention models, and graph neural networks in modeling protein
    sequences, structures, and functions, and explores how these techniques can be
    used to extract meaningful features and capture intricate relationships within
    protein data. Next, the survey presents various applications of deep learning
    in the field of proteins, including protein structure prediction, protein-protein
    interaction prediction, protein function prediction, etc. Furthermore, it highlights
    the challenges and limitations of these deep learning techniques and also discusses
    potential solutions and future directions for overcoming these challenges. This
    comprehensive survey provides a valuable resource for researchers and practitioners
    in the field of proteins who are interested in harnessing the power of deep learning
    techniques. It is a hands-on guide for researchers to understand protein science,
    develop powerful protein models, and tackle challenging problems for practical
    purposes. By consolidating the latest advancements and discussing potential avenues
    for improvement, this review contributes to the ongoing progress in protein research
    and paves the way for future breakthroughs in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: protein representation learning, structure prediction, sequence and structure,
    function, graph neural network
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Proteins are the workhorses of life, playing an essential role in a broad range
    of applications ranging from therapeutics to materials. They are built from twenty
    different basic chemical building blocks (called amino acids), which fold into
    complex ensembles of three-dimensional (3D) structures that determine their functions
    and orchestrate the biological processes of cells [[1](#bib.bib1)]. Protein modeling
    is a vital field in bioinformatics and computational biology, aimed at understanding
    the structure, function, and interactions of proteins. With the rapid advancement
    of deep learning techniques, there has been a significant impact on the field
    of protein [[2](#bib.bib2)], enabling more accurate predictions and facilitating
    breakthroughs in various areas of biological research.
  prefs: []
  type: TYPE_NORMAL
- en: Protein structures determine their interactions with other molecules and their
    ability to perform specific tasks. However, predicting protein structure from
    amino acid sequence is challenging because small perturbations in the sequence
    of a protein can drastically change the protein’s shape and even render it useless,
    and the polypeptide is flexible and can fold into a staggering number of different
    shapes [[3](#bib.bib3), [4](#bib.bib4)]. One way to find out the structure of
    a protein is to use an experimental approach, including X-ray crystallography,
    Nuclear Magnetic Resonance (NMR) Spectroscopy [[5](#bib.bib5)], and cryo-electron
    microscopy (cryo-EM) [[6](#bib.bib6)]. Unfortunately, laboratory approaches for
    structure determination are expensive and cannot be used on all proteins. Therefore,
    protein sequences vastly outnumber available structures and annotations [[7](#bib.bib7)].
    For example, there are about 190K (thousand) structures in the Protein Data Bank
    (PDB) [[8](#bib.bib8)] versus over 500M (million) sequences in UniParc [[9](#bib.bib9)]
    and only approximately 5M Gene Ontology (GO) term triplets in ProteinKG25 [[10](#bib.bib10)],
    including about 600K protein, 50K attribute terms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f35eddf7350057b88d226e9ca646028c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A general framework for deep learning models applied in protein,
    learning protein representations for various applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, there has been a growing interest in applying deep learning
    techniques to proteins. Researchers have recognized the potential of deep learning
    models to learn complex patterns and extract meaningful features from large-scale
    protein data, which includes information from protein sequences, structures, functions,
    and interactions. One particular area of active research is protein representation
    learning (PRL), which draws inspiration from approaches used in natural language
    processing (NLP) and aims to learn representations that can be utilized for various
    downstream tasks [[11](#bib.bib11)]. However, a major challenge in protein research
    is the scarcity of labeled data. Labeling proteins often requires time-consuming
    and resource-intensive laboratory experiments, making it difficult to obtain sufficient
    labeled data for training deep learning models. To address this issue, researchers
    have adopted a pre-train and fine-tune paradigm, similar to what has been performed
    in NLP. This approach involves pre-training a model on a pre-training task, where
    knowledge about the protein data is gained, and then fine-tuning the model on
    a downstream task with a smaller amount of labeled data. Self-supervised learning
    methods are commonly employed during the pre-training phase to learn protein representations.
    One popular pretext task is predicting masked tokens, where the model is trained
    to reconstruct corrupted tokens given the surrounding sequence. Several well-known
    pre-trained protein encoders have been developed, including ProtTrans [[12](#bib.bib12)],
    ESM models [[13](#bib.bib13), [14](#bib.bib14)] and GearNet [[15](#bib.bib15)].
    These pre-trained models have demonstrated their effectiveness in various protein
    tasks and have contributed to advancements in protein research. Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey") illustrates the comprehensive pipeline of deep learning based protein
    models utilized for various tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning models for proteins are widely used in various applications such
    as protein structure prediction (PSP), property prediction, and protein design.
    One of the key challenges is predicting the 3D structure of proteins from their
    sequences. Computational methods have traditionally taken two approaches: focusing
    on (a) physical interactions or (b) evolutionary principles [[16](#bib.bib16)].
    (a) The physics-based approach simulates the folding process of the amino acid
    chain using molecular dynamics or fragment assembly based on the potential energy
    of the force field. This approach emphasizes physical interactions to form a stable
    3D structure with the lowest free energy state. However, it is highly challenging
    to apply this approach to moderately sized proteins due to the computational complexity
    of molecular simulation, the limited accuracy of fragment assembly, and the difficulty
    in accurately modeling protein physics [[17](#bib.bib17), [18](#bib.bib18)]. (b)
    On the other hand, recent advancements in protein sequencing have resulted in
    a large number of available protein sequences [[19](#bib.bib19), [20](#bib.bib20)],
    enabling the generation of multiple sequence alignments (MSAs) for homologous
    proteins. With the availability of these large-scale datasets and the development
    of deep learning models, evolutionary-based models such as AlphaFold2 (AF2) [[16](#bib.bib16)]
    and recent works [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)]
    have achieved remarkable success in PSP. As researchers continue to explore the
    potential of these models, they are now focusing on developing even deeper models
    to address more challenging problems that have yet to be solved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we provide definitions, commonly used terms, and
    explanations of various deep learning architectures that have been employed in
    protein research. These architectures include convolutional neural networks (CNNs),
    recurrent neural networks (RNNs), transformer models, and graph neural networks
    (GNNs). Although deep learning models have been increasingly applied in the field
    of protein research, there is still a need for a systematic summary of this fast-growing
    field. Existing surveys related to protein research focus mainly on biological
    applications [[25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)], without delving
    deeper into other important aspects, such as comparing different pre-trained protein
    models. We explore how these architectures have been adapted to be used as protein
    models, summarize and contrast the model architectures used for learning protein
    sequences, structures, and functions. Besides, the models optimized for protein-related
    tasks are discussed, such as PSP, protein-protein interaction (PPI) prediction,
    and protein property prediction, with their innovations and differences being
    highlighted. Furthermore, a collection of resources is also provided, including
    deep protein methods, pre-training databases, and paper lists¹¹1[https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction](https://github.com/bozhenhhu/A-Review-of-pLMs-and-Methods-for-Protein-Structure-Prediction)²²2[https://github.com/LirongWu/awesome-protein-representation-learning](https://github.com/LirongWu/awesome-protein-representation-learning).
    Finally, this survey presents the limitations and unsolved problems of existing
    methods and proposes possible future research directions. An overview of this
    paper’s organization is shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e540c56a3431098abfae495d33e18b0f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A general diagram of the organization of this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the first comprehensive survey for proteins,
    specifically focusing on large-scale pre-training models and their connections,
    contrasts, and developments. Our goal is to assist researchers in the field of
    protein and artificial intelligence (AI) in developing more suitable algorithms
    and addressing essential, challenging, and urgent problems.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Definitions, Notations, and Terms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Mathematical Definitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sequence of amino acids can be folded into a stable 3D structure, which
    can be represented by a 3D graph as $G=(\mathcal{V},\mathcal{E},X,E)$, where $\mathcal{V}=\{v_{i}\}_{i=1,\ldots,n}$
    and $\mathcal{E}=\left\{\varepsilon_{ij}\right\}_{i,j=1,\ldots,n}$ denote the
    vertex and edge sets with $n$ residues, respectively, and $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$
    is the set of position matrices, where $P_{i}\in\mathbb{R}^{k_{i}\times 3}$ represents
    the position matrix for node $v_{i}$. We treat each amino acid as a graph node
    for a protein, then $k_{i}$ depends on the number of atoms in the $i$-th amino
    acid. The node and edge feature matrices are $X=[\bm{x}_{i}]_{i=1,\ldots,n}$ and
    $E=[\bm{e}_{ij}]_{i,j=1,\ldots,n}$, the feature vectors of node and edge are $\bm{x}_{i}\in\mathbb{R}^{d_{1}}$
    and $\bm{e}_{ij}\in\mathbb{R}^{d_{2}}$, $d_{1}$ and $d_{2}$ are the initial feature
    dimensions. The goal of protein graph representation learning is to form a set
    of low-dimensional embeddings $\bm{z}$ for each protein, which is then applied
    in various downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64c19db01475cebf1623bacac98bbf31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Four different levels of protein structures [[28](#bib.bib28), [29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Notations and Terms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '• Sequence/primary structure: The linear sequence of amino acids in a peptide
    or protein [[30](#bib.bib30)]. Any sequence of polypeptides is reported starting
    from the single amine (N-terminus) end to carboxylic acid (C-terminus) [[31](#bib.bib31)]
    (refer to Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical Definitions ‣ 2 Definitions,
    Notations, and Terms ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")). • Secondary structure (SS): The 3D form of local segments of proteins.
    The two most common secondary structural elements are $\alpha$-helix (H) and $\beta$-strand
    (E); 3-state SS includes H, E, C (coil region); 8 fine-grained states include
    three types for helix (G for $3_{10}$-helix, H for $\alpha$-helix, and I for $\pi$-helix),
    two types for strand (E for $\beta$-strand and B for $\beta$-bridge), and three
    types for coil (T for $\beta$-turn, S for high curvature loop, and L for irregular) [[32](#bib.bib32)].
    • Tertiary structure: The 3D arrangement of its polypeptide chains and their component
    atoms. • Quaternary structure: The 3D arrangement of the subunits in a multisubunit
    protein [[33](#bib.bib33)]. • Multiple sequence alignment (MSA): The result of
    the alignment of three or more biological sequences (protein or nucleic acid).
    • Sequence homology: The biological homology between sequences (proteins or nucleic
    acids) [[34](#bib.bib34)]. MSA assumes all the sequences to be aligned may share
    recognizable evolutionary homology [[35](#bib.bib35)] and is used to indicate
    which regions of each sequence are homologous. • Coevolution: The interdependence
    between the evolutionary changes of two entities [[36](#bib.bib36)] plays an important
    role at all biological levels, which is evident between protein residues (see
    Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a)).
    • Templates: The homologous 3D structures of proteins. • Contact map: A two-dimensional
    binary matrix represents the residue-residue contacts of a protein within a distance
    threshold [[37](#bib.bib37)]. • Protein structure prediction (PSP): The prediction
    of the 3D structure of a protein from its amino acid sequence. • Orphan proteins:
    Proteins without any detectable homology [[38](#bib.bib38)] (MSAs of homologous
    proteins are not available). • Antibody: A Y-shaped protein is produced by the
    immune system to detect and neutralize harmful substances, such as viruses and
    pathogenic bacteria. • Ribonucleic acid (RNA): A polymeric molecule essential
    in various biological roles, including transcription, translation, and gene regulation,
    most often single-stranded. • Protein complex: A form of quaternary structure
    associated with two or more polypeptide chains. • Protein conformation: The spatial
    arrangement of its constituent atoms that determines the overall shape [[39](#bib.bib39)].
    • Protein energy function: Proteins fold into 3D structures in a way that leads
    to a low-energy state. Protein-energy functions are used to guide PSP by minimizing
    the energy value. • Gene Ontology (GO): GO is a widely used bioinformatics resource
    that provides a standardized vocabulary to describe the functions, processes,
    and cellular locations of genes and gene products, organizing biological knowledge
    into three main domains: molecular function (MF), cellular component (CC), and
    biological process (BP). • Monte Carlo methods: A class of computational mathematical
    algorithms that use repeated random sampling to estimate the possible outcomes
    of an uncertain event. • Supervised learning: The use of labeled input-output
    pairs to learn a function that can classify data or predict outcomes accurately.
    • Unsupervised learning: Models are trained without a labeled dataset and encouraged
    to discover hidden patterns and insights from the given data. • Natural language
    processing (NLP): The ability of computer programs to process, analyze, and understand
    the text and spoken words in much the same way humans can. • Language model (LM):
    A LM is a statistical model that is trained to predict the probability of a sequence
    of words in a given language. • Embedding: An embedding is a low-dimensional,
    learned continuous vector representation of discrete variables into which you
    can translate high-dimensional and real-valued vectors (words or sentences) [[40](#bib.bib40)].
    • Convolution neural networks (CNNs): A class of neural networks that consist
    of convolutional operations to capture the local information. • Recurrent neural
    networks (RNNs): A class of neural networks where connections between nodes form
    a directed or undirected graph along a temporal sequence. • Attention models:
    A class of neural network architectures that are able to focus their computation
    on specific parts of their input or memory [[41](#bib.bib41)]. • Graph neural
    networks (GNNs): A type of deep learning model specifically designed to operate
    on graph-structured data. • Tansfer learning: A machine learning method where
    a model developed for one task is reused for a model to solve a different but
    related task [[42](#bib.bib42), [43](#bib.bib43)], which has two major activities,
    i.e., pre-training and fine-tuning. • Pre-training: A strategy in AI refers to
    training a model with one task to help it form parameters that can be used in
    other tasks. • Fine-tuning: A method that takes the weights of a pre-trained neural
    network, which are used to initialize a new model being trained on the same domain.
    • Autoregressive language model: A feed-forward model predicts the future word
    from a set of words given a context [[44](#bib.bib44)]. • Masked language model:
    A LM masks some of the words in a sentence and predicts which words should replace
    those masks. • Bidirectional language model: A LM learns to predict the probability
    of the next token in the past and future directions [[45](#bib.bib45)]. • Multi-task
    learning: A machine learning paradigm in which multiple tasks are solved simultaneously
    while exploiting commonalities and differences across tasks [[46](#bib.bib46)].
    • Sequence-to-Sequence (Seq2Seq): A family of machine learning approaches train
    models to convert sequences from one domain to sequences in another domain. •
    Knowledge distillation: The process of transferring the knowledge from a large
    model or set of models to a single smaller model [[47](#bib.bib47)]. • Multi-modal
    learning: Training models by combining information obtained from more than one
    modality [[48](#bib.bib48), [49](#bib.bib49)]. • Residual neural network: A neural
    network in which skip connections or shortcuts are used to jump over some layers,
    e.g., the deep residual network, ResNet [[50](#bib.bib50)].'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Basic Neural Networks in Protein Modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section initiates by introducing fundamental deep learning architectures,
    delineated into four primary categories: CNNs, RNNs, attention mechanisms, and
    GNNs. Notably, attention models like transformers [[51](#bib.bib51)] and message
    passing mechanisms receive particular emphasis. Subsequently, two frequently employed
    LMs, Bidirectional encoder representations from transformers (BERT) [[52](#bib.bib52)]
    and Generative pre-trained transformer (GPT) [[53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55)], are outlined. These foundational neural networks typically
    serve as building blocks for constructing intricate models in the realm of protein
    research.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Convolution Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs are a type of deep learning algorithm that has revolutionized the field
    of computer vision [[56](#bib.bib56)]. Inspired by the visual cortex of the human
    brain [[57](#bib.bib57)], CNNs are particularly effective at analyzing and extracting
    features from images and other grid-like data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea behind CNNs is the use of convolutional layers, which apply filters
    or kernels to input data to extract local patterns. These filters are small matrices
    that slide over the input data, performing element-wise multiplications and summations
    to produce feature maps [[58](#bib.bib58)]. Convolution is a mathematical operation
    that involves the integration of the product of two functions, where one function
    is reversed and shifted. In the context of deep learning, the convolution of two
    functions, denoted as $f$ and $g$, can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{ Convolution: }(f*g)(t)=\int_{\tau\in\Omega}g(\tau)f(t+\tau)d\tau\text{,
    }$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: here, $\Omega$ represents a neighborhood in a given space. In deep learning
    applications, $f(t)$ typically represents the feature at position $t$, denoted
    as $f(t)=f_{t}\in\mathbb{R}^{d_{1}\times 1}$, where $d_{1}$ refers to the number
    of input feature channels. On the other hand, $g(\tau)\in\mathbb{R}^{d\times d_{1}}$
    is commonly implemented as a parametric kernel function, with $d$ representing
    the number of output feature channels [[59](#bib.bib59)].
  prefs: []
  type: TYPE_NORMAL
- en: 'By stacking multiple convolutional layers, CNNs can learn increasingly complex
    and abstract features from the input data. In addition to convolutional layers,
    CNNs typically include pooling layers, which reduce the spatial dimensions of
    the feature maps while preserving the most important information. Pooling helps
    to make the network more robust to variations in the input data and reduces the
    computational complexity. CNNs also incorporate fully connected layers at the
    end of the network, which perform classification or regression tasks based on
    the extracted features. Figure [4](#S3.F4 "Figure 4 ‣ 3.1 Convolution Neural Networks
    ‣ 3 Basic Neural Networks in Protein Modeling ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey") shows a framework of deep CNNs. The versatility
    and effectiveness of CNNs have made them a fundamental tool in the field of deep
    learning and have contributed to significant advancements in various domains.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a9a32368a7ecdc73c64a1bd49bb1d51.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: An illustration of CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Recurrent Neural Networks and Long Short-term Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first example of LM was studied by Andrey Markov, who proposed the Markov
    chain in 1913 [[60](#bib.bib60), [61](#bib.bib61)]. After that, some machine learning
    methods, particularly hidden Markov models and their variants, have been described
    and applied as fundamental tools in many fields, including biological sequences [[62](#bib.bib62)].
    The goal is to recover a data sequence that is not immediately observable [[63](#bib.bib63),
    [64](#bib.bib64), [65](#bib.bib65)].
  prefs: []
  type: TYPE_NORMAL
- en: Since the 2010s, neural networks have started to produce superior results in
    various NLP tasks [[66](#bib.bib66)]. RNNs allow previous outputs to be used as
    inputs while having hidden states to exhibit temporal dynamic behaviors. Therefore,
    RNNs can use their internal states to process variable-length sequences of inputs,
    which are useful and applicable in NLP tasks [[67](#bib.bib67)]. In a recent development,
    Google DeepMind introduced Hawk, an RNN featuring gated linear recurrences, alongside
    Griffin, a hybrid model blending gated linear recurrences with local attention
    mechanisms [[68](#bib.bib68)], which match the hardware efficiency of transformers [[51](#bib.bib51)]
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/231c7d6d91285f3e2423d24216c5e0a5.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) RNNs
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76ae6d8097e0de81bcc78974164967e4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) LSTM cell
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Graphical explanation of RNNs and LSTM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs are typically shown in Figure [5a](#S3.F5.sf1 "In Figure 5 ‣ 3.2 Recurrent
    Neural Networks and Long Short-term Memory ‣ 3 Basic Neural Networks in Protein
    Modeling ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey").
    In each timestep $t$, the input $x_{t}\in\mathbb{R}^{d_{1}}$, hidden $h_{t}\in\mathbb{R}^{d}$
    and output state vectors $o_{t}\in\mathbb{R}^{d}$, where the superscripts $d_{1}$
    and $d$ refer to the number of input features and the number of hidden units,
    respectively, are formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=g\left(W_{x}x_{t}+W_{h}h_{t-1}+b_{h}\right)$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=g\left(W_{y}h_{t}+b_{y}\right)$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $W_{x}\in\mathbb{R}^{d\times d_{1}}$, $W_{h}\in\mathbb{R}^{d\times d}$
    and $W_{y}\in\mathbb{R}^{d\times d}$ are the weights associated with the input,
    hidden and output vectors in the recurrent layer, and $b_{h}\in\mathbb{R}^{d}$,
    $b_{y}\in\mathbb{R}^{d}$ are the bias, which are shared temporally, $g(\cdot)$
    is the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: In order to deal with the vanishing gradient problem [[69](#bib.bib69)] that
    can be encountered when training traditional RNNs, LSTM networks are developed
    to process sequences of data. They present superior capabilities in learning long-term
    dependencies [[70](#bib.bib70)] with various applications such as time series
    prediction [[71](#bib.bib71)], protein homology detection [[72](#bib.bib72)],
    drug design [[73](#bib.bib73)], etc. Unlike standard LSTM, bidirectional LSTM
    (BiLSTM) adds one more LSTM layer, reversing the information flow direction. This
    means it is capable of utilizing information from both sides and is also a powerful
    tool for modeling the sequential dependencies between words and phrases in a sequence [[74](#bib.bib74)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSTM architecture aims to provide a short-term memory that can last more
    timesteps, shown in Figure [5b](#S3.F5.sf2 "In Figure 5 ‣ 3.2 Recurrent Neural
    Networks and Long Short-term Memory ‣ 3 Basic Neural Networks in Protein Modeling
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey"), $\mathrm{Sigmoid}(\cdot)$
    and $\mathrm{tanh}(\cdot)$ represent the sigmoid and tanh layer. Forget gate layer
    in the LSTM is to decide what information is going to be thrown away from the
    cell state at timestep $t$, $x_{t}\in\mathbb{R}^{d_{1}}$, $h_{t}\in(-1,1)^{d}$
    and $f_{t}\in(0,1)^{d}$ are the input, hidden state vectors and forget gate’s
    activation vector.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{t}=\mathrm{Sigmoid}(W_{f}x_{t}+U_{f}h_{t-1}+b_{f})$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Then, the input gate layer decides which values should be updated, and a tanh
    layer creates a vector of new candidate values, $\tilde{C}_{t}\in(-1,1)^{d}$ that
    could be added to the state, $i_{t}\in(0,1)^{d}$ is the input gate’s activation
    vector.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle i_{t}$ | $\displaystyle=\mathrm{Sigmoid}(W_{i}x_{t}+U_{i}h_{t-1}+b_{i})$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\tilde{C}_{t}$ | $\displaystyle=\tanh(W_{c}x_{t}+U_{c}h_{t-1}+b_{c})$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Next, we combine old state $C_{t-1}\in\mathbb{R}^{d}$ and new candidate values
    $\tilde{C}_{t}\in(-1,1)^{d}$ to create an update to the new state $C_{t}\in\mathbb{R}^{d}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C_{t}=f_{t}\odot C_{t-1}+i_{t}\odot\tilde{C}_{t}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Finally, the output gate layer decides what parts of the cell state to be outputted,
    $o_{t}\in(0,1)^{d}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=\mathrm{Sigmoid}(W_{o}x_{t}+U_{o}h_{t-1}+b_{o})$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\tanh\left(C_{t}\right)$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\{W_{f},W_{i},W_{c},W_{o}\}\in\mathbb{R}^{d\times d_{1}},\{U_{f},U_{i},U_{c},U_{o}\}\in\mathbb{R}^{d\times
    d}$ and $\{b_{f},b_{i},b_{c},b_{o}\}\in\mathbb{R}^{d}$ are weight matrices and
    bias vector parameters in the LSTM cell, $\odot$ means the pointwise multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Attention Mechanism and Transformer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traditional Sequence-to-Sequence (Seq2Seq) models typically use RNNs or LSTMs
    as encoders and decoders [[75](#bib.bib75)] to process sequences and extract features
    for various tasks. However, these models have limitations, such as the final state
    of the RNNs or LSTMs needing to hold information for the entire input sequence,
    which can lead to information loss. To overcome these limitations, attention mechanisms [[76](#bib.bib76),
    [77](#bib.bib77)] have been introduced, which can be divided into two categories,
    local attention, and global attention (refer to Figure [6a](#S3.F6.sf1 "In Figure
    6 ‣ 3.3 Attention Mechanism and Transformer ‣ 3 Basic Neural Networks in Protein
    Modeling ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")).
    They allow models to focus on specific parts of the input sequence that are relevant
    to the task at hand. The basic idea behind attention is to assign weights to different
    elements of the input sequence based on their relevance to the current step of
    the output sequence generation. Attention mechanisms are first applied in machine
    translation [[76](#bib.bib76)] and have gradually replaced traditional RNNs and
    LSTMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f67b0e311757b80e0a78adffb6e9870d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Attention mechanism
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e82d6a3d3451e51879c3b01bae19b843.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The Transformer
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Graphical explanation of attention mechanism and the architecture
    of Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The attention layer in a model can access all previous states and learn their
    importance by assigning weights to them. In 2017, Google Brain introduced the
    Transformer architecture [[51](#bib.bib51)], which completely eliminates recurrence
    and convolutions. This breakthrough leads to the development of pre-trained models
    such as BERT and GPT, which are trained on large language datasets. Unlike RNNs,
    as shown in Figure [6b](#S3.F6.sf2 "In Figure 6 ‣ 3.3 Attention Mechanism and
    Transformer ‣ 3 Basic Neural Networks in Protein Modeling ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), the Transformer processes the entire
    input simultaneously using $N$ stacked self-attention layers for both the encoder
    and decoder. Each layer consists of a multi-head attention module followed by
    a feed-forward module with a residual connection and normalization. The basic
    attention mechanism used in Transformer is called ”Scaled Dot-Product Attention”
    and operates as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname{Att}(Q,K,V)=\operatorname{Softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right)V$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'where $Q,K,V\in\mathbb{R}^{l\times d}$ are $d$-dimensional vector representations
    of $l$ words in sequences of queries, keys and values, respectively. The multi-head
    attention mechanism allows the model to attend to different representation subspaces
    in parallel. The multi-head attention module can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\operatorname{MultiHead}\left(Q,K,V\right)$ | $\displaystyle=\mathrm{Concat}\left(\mathrm{head}_{1},\ldots,\mathrm{head}_{h}\right)W^{O}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathrm{where}\ \mathrm{head}_{i}$ | $\displaystyle=\mathrm{Att}\left(QW_{i}^{Q},KW_{i}^{K},VW_{i}^{V}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the projections are parameter matrices $W_{i}^{Q}\in\mathbb{R}^{d\times
    d_{i}},W_{i}^{K}\in\mathbb{R}^{d\times d_{i}},W_{i}^{V}\in\mathbb{R}^{d\times
    d_{i}}$ and $W^{O}\in\mathbb{R}^{hd_{i}\times d},d_{i}=d/h$, there are $h$ parallel
    attention layers or heads. Additionally, the Transformer architecture includes
    position-wise feed-forward networks, which consist of two linear transformations
    with a ReLU activation in between. Positional encoding is also added to the embedding
    at the bottom of the encoder and decoder stacks to incorporate the order of the
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Graph Neural Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike traditional neural networks, which operate on grid-like data structures
    such as images or sequences, GNNs are specifically designed to handle data represented
    as graphs. In a graph, data entities are represented as nodes, and the relationships
    between these entities are captured by edges. This flexible and expressive representation
    makes GNNs well-suited for a wide range of applications, including social network
    analysis [[78](#bib.bib78)], recommendation systems [[79](#bib.bib79)], drug discovery [[80](#bib.bib80)],
    and knowledge graph reasoning [[81](#bib.bib81)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea behind GNNs is to learn node representations by aggregating information
    from their neighboring nodes. This is achieved through a series of message passing
    steps, where each node updates its representation by incorporating information
    from its neighbors. By iteratively propagating and updating information across
    the graph, GNNs can capture complex dependencies and patterns in the data. Given
    a protein 3D graph as $G=(\mathcal{V},\mathcal{E},X,E)$ as presented in Subsection [2.1](#S2.SS1
    "2.1 Mathematical Definitions ‣ 2 Definitions, Notations, and Terms ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), a message passing
    layer can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{h}_{i}=\phi\left(\bm{x}_{i},\bigoplus_{v_{j}\in\mathcal{N}(v_{i})}\psi\left(\bm{x}_{i},\bm{x}_{j},\bm{e}_{ij}\right)\right)$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\bigoplus$ is a permutation invariant aggregation operator (e.g., element-wise
    sum), $\phi(\cdot)$ and $\psi(\cdot)$ are denoted as update and message functions,
    and $\mathcal{N}(v_{i})$ means the neighbors of node $v_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: The variation of GNN models, including GCN [[82](#bib.bib82)], GAT [[83](#bib.bib83)],
    and GraphSAGE [[84](#bib.bib84)], differ in aggregation strategies, attention
    mechanisms, and propagation rules, but they all share the fundamental idea of
    learning node representations through the message passing mechanism. GNNs have
    the ability to capture both local and global information, providing a powerful
    framework for understanding graph-structured data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to effectively train deep neural models that can store knowledge for
    specific tasks using limited human-annotated data, the approach of transfer learning
    has been widely adopted. This involves a two-step process: pre-training and fine-tuning [[77](#bib.bib77),
    [85](#bib.bib85), [43](#bib.bib43)].'
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, there has been remarkable progress in the development
    of pre-trained LMs, which have found extensive applications in various domains
    such as NLP and computer vision. Among these models, the transformer architecture
    has emerged as a standard neural architecture for both natural language understanding
    and generation. Notably, BERT and GPT are two landmark models that have opened
    the doors to large-scale pre-training LMs. GPT [[53](#bib.bib53)] is designed
    to optimize autoregressive language modeling during pre-training. It utilizes
    a transformer to model the conditional probability of each word, making it proficient
    in predicting the next token in a sequence. On the other hand, BERT [[52](#bib.bib52)]
    employs a multi-layer bidirectional transformer encoder as its architecture. During
    the pre-training phase, BERT utilizes next-sentence prediction and masked language
    modeling strategies to understand sentence relationships and capture contextual
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Following the introduction of GPT and BERT, numerous improvements and variants
    have been proposed by researchers. One notable trend has been the increase in
    model size and dataset size [[86](#bib.bib86), [87](#bib.bib87)]. Large transformer
    models have become the de facto standard in NLP, driven by scaling laws that govern
    the relationship between overfitting, model size, and dataset size within a given
    computational budget [[88](#bib.bib88), [89](#bib.bib89)]. In November 2022, OpenAI
    released ChatGPT [[90](#bib.bib90)], which garnered significant attention for
    its ability to understand human language, answer questions, write code, and even
    generate novels. It stands as one of the most successful applications of large-scale
    pre-trained LMs. Moreover, as pre-trained LMs have demonstrated their effectiveness
    and efficiency in various domains, they have gradually expanded beyond NLP into
    fields such as finance, computer vision, and biomedicine [[91](#bib.bib91), [92](#bib.bib92),
    [93](#bib.bib93), [94](#bib.bib94), [95](#bib.bib95)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 Protein Foundations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section delves into the fundamental aspects of proteins, exploring their
    intricate connections with human languages, physicochemical properties, structural
    geometries, and biological insights. The discussion aims to unveil the foundational
    elements that contribute to a comprehensive understanding of proteins and their
    roles in various contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Protein and Language
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LMs are increasingly being utilized in the analysis of large-scale protein
    sequence databases to acquire embeddings [[96](#bib.bib96), [97](#bib.bib97),
    [98](#bib.bib98)]. One significant reason for this trend is the shared characteristics
    between human languages and proteins. For instance, both exhibit a hierarchical
    organization [[66](#bib.bib66), [99](#bib.bib99)], where the four distinct levels
    of protein structures (as depicted in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical
    Definitions ‣ 2 Definitions, Notations, and Terms ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey")) can be analogized to letters, words,
    sentences, and texts in human languages. This analogy illustrates that proteins
    and languages consist of modular elements that can be reused and rearranged. Additionally,
    principles governing protein folding, such as the hydrophilicity and hydrophobicity
    of amino acids, the principle of minimal frustration [[100](#bib.bib100)], and
    the folding funnel landscapes of proteins [[101](#bib.bib101)], bear a resemblance
    to language grammars in linguistics.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37d4bd2cac294433265957ebba7c8005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Comparisons of protein and language. (a) Relationship between a MSA
    and the residue contact of one protein in the alignment. The positions that coevolved
    are highlighted in red and light blue. Residues within these positions where changes
    occurred are shown in blue. Given such a MSA, one can infer correlations statistically
    found between two residues that these sequence positions are spatially adjacent,
    i.e., they are contacts [[102](#bib.bib102), [36](#bib.bib36), [103](#bib.bib103)].
    (b) One grammatically complex sentence contains long-distance dependencies (shown
    in bold).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a) demonstrates
    the statistical inference of residue contacts in a protein based on a MSA. It
    highlights the existence of long-range dependencies between two residues, where
    they may be distant in the sequence but spatially close, indicating coevolution.
    A similar phenomenon of long-distance dependencies is observed in human languages
    as well. Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(b) provides
    an example of language grammar rules that require agreement between words that
    are far apart [[104](#bib.bib104)]. These similarities suggest that successful
    methods from NLP can be applied to analyze protein data. However, it is important
    to note that proteins are distinct from human languages, despite these shared
    characteristics. For instance, training LMs often necessitates a vast corpus,
    which requires tokenization, i.e., breaking down the text into individual tokens
    or using words directly as tokens. This serves computational purposes and ideally
    aligns with linguistic goals in NLP [[105](#bib.bib105), [106](#bib.bib106), [97](#bib.bib97),
    [98](#bib.bib98), [107](#bib.bib107)]. In contrast, protein tokenization methods
    are still at a rudimentary stage without a well-defined and biologically meaningful
    algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Protein Physicochemical Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Physicochemical properties of proteins refer to the characteristics and behaviors
    of proteins that are determined by their chemical and physical properties, playing
    a crucial role in protein structure, stability, function, and interactions. Only
    when the environment is suitable for the physicochemical properties of a protein
    can it remain alive and play its role [[108](#bib.bib108)]. Understanding the
    physicochemical properties of proteins is crucial for developing new protein drugs.
    Certain physicochemical characteristics of proteins resemble those of amino acids,
    including amphoteric ionization, isoelectric point, color reaction, salt reaction,
    and more. However, there are also differences between proteins and amino acids
    in terms of properties such as high molecular weight, colloid behavior, denaturation,
    and others. Several studies have utilized amino acid related physicochemical properties
    to gain insights into the biochemical nature of each amino acid [[109](#bib.bib109),
    [110](#bib.bib110)]. These properties include steric parameter, hydrophobicity,
    volume, polarizability, isoelectric point, helix probability, and sheet probability.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of these features at the residue level has been calculated and
    visualized by HIGH-PPI [[111](#bib.bib111)]. They have found that features such
    as isoelectric point, polarity, and hydrogen bond acceptor function in protein-protein
    interaction interfaces. By analyzing the changes in evaluation scores before and
    after dropping each individual feature dimension from the model, they have identified
    topological polar surface area and octanol-water partition coefficient as dominant
    features for PPI interface characterization.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Motifs, Regions, and Domains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Motifs, regions, and domains are commonly used as additional information for
    training deep learning models. A motif refers to a short, conserved sequence pattern
    or structural feature that is found in multiple proteins or nucleic acids. It
    represents a functional or structural unit that is often associated with a specific
    biological activity [[112](#bib.bib112)]. Motifs can be used as signatures to
    identify potential binding sites for ligands, substrates, or other interacting
    molecules. On the other hand, the term “region” denotes a specific region of interest
    within a sequence. In contrast, a domain is an independent unit within a protein
    or nucleic acid sequence, both structurally and functionally [[113](#bib.bib113)].
    Domains can fold into stable 3D structures and often perform specific functions.
    Figure [8](#S4.F8 "Figure 8 ‣ 4.3 Motifs, Regions, and Domains ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey") illustrates
    various data categories in the protein field, providing an example of motifs,
    regions, and domains. Hu et al. [[114](#bib.bib114)] have collected multiple datasets
    that include 1364 categories of motifs, 3383 categories of domains, and 10628
    categories of regions. It is important to note that these categories exhibit long-tailed
    distributions. Therefore, when working with protein-related tasks, it is crucial
    to consider the multimodal property and long-tail effects of protein data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e49bb3987bb3e78cd151726bb6a69e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An overview of the multimodal dataset of proteins [[114](#bib.bib114)],
    including sequences, structures, GO terms, regions, domains, and motifs.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Protein Structure Geometries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Wang et al. [[115](#bib.bib115)] emphasize the importance of effectively utilizing
    multi-level structural information for accurate protein function prediction, where
    the four distinct levels are shown in Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Mathematical
    Definitions ‣ 2 Definitions, Notations, and Terms ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"). They propose incorporating the PPI
    task during the pre-training phase to capture quaternary structure information.
    In addition to considering multiple levels of protein structures, deep learning
    models can leverage hierarchical relationships to process tertiary structure information.
    For instance, ProNet [[116](#bib.bib116)] focuses on representation learning for
    proteins with 3D structures at various levels, such as the amino acid, backbone,
    or all-atom levels. At the amino acid level, ProNet considers the $\mathrm{C}_{\alpha}$
    positions of the structures. At the backbone level, it incorporates the information
    of all backbone atoms ($\mathrm{C}_{\alpha},\mathrm{C},\mathrm{N},\mathrm{O}$).
    Finally, at the all-atom level, ProNet processes the coordinates of both backbone
    and side chain atoms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/649b6526a3e92a7c0b24b5338b690bc4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/345bb4e19766667a2921b6830bd8593a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Protein structure geometries [[119](#bib.bib119)]. (a) The local
    coordinate system, $P_{i,\mathrm{C}_{\alpha}}$ is the coordinate of $\mathrm{C}_{\alpha}$
    in residue $i$. (b) Interresidue geometries, including the distance ($d_{ij,\mathrm{C}_{\beta}}$),
    three dihedral angles ($\omega_{ij},\theta_{ij},\theta_{ji}$) and two planar angles
    ($\varphi_{ij},\varphi_{ji}$).'
  prefs: []
  type: TYPE_NORMAL
- en: Local Coordinate System
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The locally informative features are developed from the local coordinate system
    (LCS) [[117](#bib.bib117)], shown in Figure [9a](#S4.F9.sf1 "In Figure 9 ‣ 4.4
    Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{Q}_{i}=[\bm{b_{i}}\quad\bm{n_{i}}\quad\bm{b_{i}}\times\bm{n_{i}}]$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $\bm{u}_{i}=\frac{{P}_{i,\mathrm{C}\alpha}-{P}_{i-1,\mathrm{C}\alpha}}{\left\|{P}_{i,\mathrm{C}\alpha}-{P}_{i-1,\mathrm{C}\alpha}\right\|},\bm{b_{i}}=\frac{\bm{u}_{i}-\bm{u}_{i+1}}{\left\|\bm{u}_{i}-\bm{u}_{i+1}\right\|},\bm{n}_{i}=\frac{\bm{u}_{i}\times\bm{u}_{i+1}}{\left\|\bm{u}_{i}\times\bm{u}_{i+1}\right\|}$,
    $\bm{b_{i}}$ is the negative bisector of the angle between the rays ($P_{i-1,\mathrm{C}_{\alpha}}-P_{i,\mathrm{C}_{\alpha}}$)
    and ($P_{i+1,\mathrm{C}_{\alpha}}-P_{i,\mathrm{C}_{\alpha}}$), ${P}_{i,\mathrm{C}\alpha}$
    represent the coordinate of atom $\mathrm{C}_{\alpha}$ in node $v_{i}$, and $\left\|\cdot\right\|$
    denotes the $l^{2}$-norm. It is clear to see that LCS is defined at the amino
    acid level. The spatial edge features $\bm{e}_{ij}^{(1)}$ can be obtained considering
    the distance, direction, and orientation by LCS,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{e}_{ij}^{(1)}=\mathrm{Concat}(\left\&#124;d_{ij,\mathrm{C}\alpha}\right\&#124;,\bm{Q}_{i}^{T}\cdot\frac{d_{ij,\mathrm{C}\alpha}}{\left\&#124;d_{ij,\mathrm{C}\alpha}\right\&#124;},\bm{Q}_{i}^{T}\cdot\bm{Q}_{j})$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $\cdot$ is the matrix multiplication, and $d_{ij,\mathrm{C}\alpha}={P}_{i,\mathrm{C}\alpha}-{P}_{j,\mathrm{C}\alpha}$.
    This implementation obtains complete representations at the amino acid level;
    as if we have $\bm{Q}_{i}$, the LCS $\bm{Q}_{j}$ can be easily obtained by $\bm{e}_{ij}^{(1)}$.
    Thus, LCS is widely utilized in protein design, antibody design, and PRL [[117](#bib.bib117),
    [118](#bib.bib118), [119](#bib.bib119)].
  prefs: []
  type: TYPE_NORMAL
- en: trRosetta Interresidue Geometries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We introduce the relative rotations and distances in trRosetta [[120](#bib.bib120)],
    including the distance ($d_{ij,\mathrm{C}_{\beta}}$), three dihedral angles ($\omega_{ij},\theta_{ij},\theta_{ji}$)
    and two planar angles ($\varphi_{ij},\varphi_{ji}$), as shown in Figure [9b](#S4.F9.sf2
    "In Figure 9 ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), where $d_{ij,\mathrm{C}_{\beta}}=d_{ji,\mathrm{C}_{\beta}},\omega_{ij}=\omega_{ji}$,
    but $\theta$ and $\varphi$ values depend on the order of residues. These interresidue
    geometries define the relative locations of the backbone atoms of two residues
    in all their details [[120](#bib.bib120)], because the torsion angles of $\mathrm{N}_{i}-\mathrm{C}_{\alpha
    i}$ and $\mathrm{C}_{\alpha i}-\mathrm{C}_{i}$ do not influence their positions.
    Therefore, these six geometries are complete for amino acids at the backbone level
    for the radius graph, which are commonly used in PSP and protein model quality
    assessment [[120](#bib.bib120), [121](#bib.bib121), [122](#bib.bib122), [123](#bib.bib123),
    [124](#bib.bib124)]. The edge features $\bm{e}_{ij}^{(2)}$ can be obtained by
    these interresidue geometries, which contain the relative spatial information
    between any two neighboring amino acids.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{e}_{ij}^{(2)}=\mathrm{Concat}(d_{ij,\mathrm{C}_{\beta}},(\sin\wedge\cos)(\omega_{ij},\theta_{ij},\varphi_{ij}))$
    |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/c54774d6c039255f3d677e046dd0f4a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The polypeptide chain depicting the characteristic backbone bond
    lengths, angles, and torsion angles ($\Psi_{i},\Phi_{i},\Omega_{i}$). The planar
    peptide groups are denoted as shaded gray regions, indicating that the peptide
    plane differs from the geometric plane calculated from 3D positions [[119](#bib.bib119)].'
  prefs: []
  type: TYPE_NORMAL
- en: Backbone Torsion Angles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The peptide bond exhibits partial double-bond character due to resonance [[125](#bib.bib125)],
    indicating that the three non-hydrogen atoms comprising the bond are coplanar,
    as shown in Figure [10](#S4.F10 "Figure 10 ‣ trRosetta Interresidue Geometries
    ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances of Deep
    Learning in Protein Science: A Comprehensive Survey"), the free rotation about
    the bond is limited due to the coplanar property. The $\mathrm{N}_{i}-\mathrm{C}_{\alpha
    i}$ and $\mathrm{C}_{\alpha i}-\mathrm{C}_{i}$ bonds, are the two bonds in the
    basic repeating unit of the polypeptide backbone. These single bonds allow unrestricted
    rotation until sterically restricted by side chains [[126](#bib.bib126), [127](#bib.bib127)].
    The coordinates of backbone atoms based on these rigid bond lengths and angles
    are able to be determined with the remaining degree of the backbone torsion angles
    $\Phi_{i},\Psi_{i},\Omega_{i}$. The omega torsion angle around the $\mathrm{C}-\mathrm{N}$
    peptide bond is typically restricted to nearly $180^{\circ}$ (trans) but can approach
    $0^{\circ}$ (cis) in rare instances. Other than the bond lengths and angles presented
    in Figure [10](#S4.F10 "Figure 10 ‣ trRosetta Interresidue Geometries ‣ 4.4 Protein
    Structure Geometries ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey"), all the H bond lengths measure approximately
    1 Å.'
  prefs: []
  type: TYPE_NORMAL
- en: Euler Angles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Different from the trRosetta interresidue geometries, Wang et al. [[116](#bib.bib116)]
    propose to use Euler angles to capture the rotation between two backbone planes.
    Unlike this protein design method [[117](#bib.bib117)], ProNet [[116](#bib.bib116)]
    defines its local coordinate system for an amino acid $i$ as $\bm{y}_{i}=\bm{r}_{i}^{\mathrm{N}}-\bm{r}_{i}^{\mathrm{C}_{\alpha}},\bm{t}_{i}=\bm{r}_{i}^{\mathrm{C}}-\bm{r}_{i}^{\mathrm{C}_{\alpha}}$,
    and $\bm{z}_{i}=\bm{t}_{i}\times\bm{y}_{i},\bm{x}_{i}=\bm{y}_{i}\times\bm{z}_{i}$,
    where the $\bm{r}_{i}$ represents the position vector of the $i$-th amino acid
    in a protein, this coordinate system is shown in Figure [11](#S4.F11 "Figure 11
    ‣ Euler Angles ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")(a). The three Euler
    angles $\tau_{ij}^{1},\tau_{ij}^{2}$ and $\tau_{ij}^{3}$ between two backbone
    coordinate systems can be computed as shown in Figure [11](#S4.F11 "Figure 11
    ‣ Euler Angles ‣ 4.4 Protein Structure Geometries ‣ 4 Protein Foundations ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey")(b), where $\bm{n}=\bm{z}_{i}\times\bm{z}_{j}$,
    is the intersection of two planes. $\tau_{ij}^{1}$ is the signed angle between
    $\bm{n}$ and $\bm{x}_{i}$, $\tau_{ij}^{2}$ is the angle between $\bm{z}_{i}$ and
    $\bm{z}_{j}$, and $\tau_{ij}^{3}$ is the angle from $\bm{n}$ to $\bm{x}_{j}$.
    The relative rotations for any two amino acids $i$ and $j$ can be determined by
    these three Euler angles [[116](#bib.bib116)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7b6e8e727d441aec800fe214dcf6c51f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Illustration of Euler angles [[116](#bib.bib116)]. (a) The backbone
    coordinate system for an amino acid. (b) The three Euler angles between the backbone
    coordinate system for amino acids $i$ and $j$.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the backbone torsion angles $\Phi_{i},\Psi_{i},\Omega_{i}$, trRosetta
    interresidue geometries and Euler angles are defined at the backbone level, while
    the LCS is defined at the amino acid level. When considering the positions of
    all atoms, following the implementations in AF2 [[16](#bib.bib16)], the first
    four torsion angles $\chi_{1},\chi_{2},\chi_{3},\chi_{4}$, are usually considered
    for side chain atoms, as only the amino acid arginine has five side chain torsion
    angles, and the fifth angle is close to 0 [[116](#bib.bib116)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Structure Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proteins can move in the 3D space through translations and rotations. These
    properties, such as translation and rotation invariance, improve the accuracy,
    reliability, and usefulness of models in different protein-related tasks. One
    example is GVP-GNN [[128](#bib.bib128)], which can process both scalar features
    and vectors, enabling the inclusion of detailed geometric information at nodes
    and edges without oversimplifying it into scalar values that may not fully represent
    complex geometry.
  prefs: []
  type: TYPE_NORMAL
- en: Invariance and Equivariance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We examine affine transformations that maintain the distance between any two
    points, known as the isometric group SE(3) in Euclidean space. This group, denoted
    as the symmetry group, encompasses 3D translations and the 3D rotation group SO(3) [[129](#bib.bib129),
    [130](#bib.bib130)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The collection of $4\times 4$ real matrices of the SE(3) is shown as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_Math" alttext="\left[\begin{array}[]{cc}R&amp;\mathbf{t}\\
    0&amp;1\end{array}\right]=\left[\begin{array}[]{cccc}r_{11}&amp;r_{12}&amp;r_{13}&amp;t_{1}\\'
  prefs: []
  type: TYPE_NORMAL
- en: r_{21}&amp;r_{22}&amp;r_{23}&amp;t_{2}\\
  prefs: []
  type: TYPE_NORMAL
- en: r_{31}&amp;r_{32}&amp;r_{33}&amp;t_{3}\\
  prefs: []
  type: TYPE_NORMAL
- en: 0&amp;0&amp;0&amp;1\end{array}\right]," display="block"><semantics ><mrow
     ><mrow 
    ><mrow  ><mo
     >[</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd  ><mi
     >R</mi></mtd><mtd 
    ><mi  >𝐭</mi></mtd></mtr><mtr
     ><mtd  ><mn
     >0</mn></mtd><mtd 
    ><mn  >1</mn></mtd></mtr></mtable><mo
     >]</mo></mrow><mo
     >=</mo><mrow 
    ><mo  >[</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd  ><msub
     ><mi 
    >r</mi><mn  >11</mn></msub></mtd><mtd
     ><msub  ><mi
     >r</mi><mn 
    >12</mn></msub></mtd><mtd 
    ><msub  ><mi
     >r</mi><mn 
    >13</mn></msub></mtd><mtd 
    ><msub  ><mi
     >t</mi><mn 
    >1</mn></msub></mtd></mtr><mtr 
    ><mtd  ><msub
     ><mi 
    >r</mi><mn  >21</mn></msub></mtd><mtd
     ><msub  ><mi
     >r</mi><mn 
    >22</mn></msub></mtd><mtd 
    ><msub  ><mi
     >r</mi><mn 
    >23</mn></msub></mtd><mtd 
    ><msub  ><mi
     >t</mi><mn 
    >2</mn></msub></mtd></mtr><mtr 
    ><mtd  ><msub
     ><mi 
    >r</mi><mn  >31</mn></msub></mtd><mtd
     ><msub  ><mi
     >r</mi><mn 
    >32</mn></msub></mtd><mtd 
    ><msub  ><mi
     >r</mi><mn 
    >33</mn></msub></mtd><mtd 
    ><msub  ><mi
     >t</mi><mn 
    >3</mn></msub></mtd></mtr><mtr 
    ><mtd  ><mn
     >0</mn></mtd><mtd 
    ><mn  >0</mn></mtd><mtd
     ><mn  >0</mn></mtd><mtd
     ><mn  >1</mn></mtd></mtr></mtable><mo
     >]</mo></mrow></mrow><mo
     >,</mo></mrow><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><csymbol
    cd="latexml"  >delimited-[]</csymbol><matrix
     ><matrixrow 
    ><ci  >𝑅</ci><ci
     >𝐭</ci></matrixrow><matrixrow
     ><cn type="integer" 
    >0</cn><cn type="integer" 
    >1</cn></matrixrow></matrix></apply><apply 
    ><csymbol cd="latexml" 
    >delimited-[]</csymbol><matrix 
    ><matrixrow  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><cn type="integer"
     >11</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><cn type="integer"
     >12</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><cn type="integer"
     >13</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑡</ci><cn type="integer"
     >1</cn></apply></matrixrow><matrixrow
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑟</ci><cn type="integer" 
    >21</cn></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑟</ci><cn type="integer" 
    >22</cn></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑟</ci><cn type="integer" 
    >23</cn></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑡</ci><cn type="integer" 
    >2</cn></apply></matrixrow><matrixrow 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑟</ci><cn type="integer"
     >31</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><cn type="integer"
     >32</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑟</ci><cn type="integer"
     >33</cn></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑡</ci><cn type="integer"
     >3</cn></apply></matrixrow><matrixrow
     ><cn type="integer" 
    >0</cn><cn type="integer" 
    >0</cn><cn type="integer" 
    >0</cn><cn type="integer" 
    >1</cn></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\left[\begin{array}[]{cc}R&\mathbf{t}\\
    0&1\end{array}\right]=\left[\begin{array}[]{cccc}r_{11}&r_{12}&r_{13}&t_{1}\\
    r_{21}&r_{22}&r_{23}&t_{2}\\ r_{31}&r_{32}&r_{33}&t_{3}\\ 0&0&0&1\end{array}\right],</annotation></semantics></math>
    |  | (15) |
  prefs: []
  type: TYPE_NORMAL
- en: where $R\in\mathrm{SO(3)}$ and $\mathbf{t}\in\mathbb{R}^{3}$, SO(3) is the 3D
    rotation group. $R$ satisfying $R^{T}R=I$ and $\mathrm{det}(R)=1$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the function $f:\mathbb{R}^{d}\to\mathbb{R}^{d^{\prime}}$, assuming the
    given symmetry group $G$ acts on $\mathbb{R}^{d}$ and $\mathbb{R}^{d^{\prime}}$,
    $f$ is considered G-equivariant if it satisfies the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(T_{g}\bm{x})=S_{g}f(\bm{x}),\ \forall\bm{x}\in\mathbb{R}^{d},g\in G$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: here, $T_{g}$ and $S_{g}$ represent the transformations. For the SE(3) group,
    when $d^{{}^{\prime}}=1$ and the output of $f$ is a scalar, we have
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f(T_{g}\bm{x})=f(\bm{x}),\ \forall\bm{x}\in\mathbb{R}^{d},g\in G$ |  |
    (17) |'
  prefs: []
  type: TYPE_TB
- en: thus $f$ is SE(3)-invariant [[131](#bib.bib131)].
  prefs: []
  type: TYPE_NORMAL
- en: By maintaining SE(3)-equivariance in structural geometries, protein models demonstrate
    the ability to recognize and interpret protein structures irrespective of transformations
    in 3D space. This enables them to learn from a diverse array of protein structures
    and seamlessly apply that knowledge to predict the functions of novel proteins.
    The pivotal capability of generalization plays a critical role in PSP and protein
    design [[132](#bib.bib132)]. Notably, a considerable number of proteins exhibit
    symmetrical properties, such as recurring motifs or symmetric domains. SE(3)-equivariant
    models can effectively capture and leverage these symmetrical properties, thereby
    enhancing their comprehension of protein structures and functions [[133](#bib.bib133)].
  prefs: []
  type: TYPE_NORMAL
- en: Complete Geometries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A geometric transformation $\mathcal{F}(\cdot)$ is complete if for two 3D graphs
    $G^{1}=(\mathcal{V},\mathcal{E},\mathcal{P}^{1})$ and $G^{2}=(\mathcal{V},\mathcal{E},\mathcal{P}^{2})$,
    there exists $T_{g}\in\mathrm{SE(3)}$ such that the representations,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}(G^{1})=\mathcal{F}(G^{2})\Longleftrightarrow P_{i}^{1}=T_{g}(P_{i}^{2}),\
    \mathrm{for}\ i=1,\dots n$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: the operation $T_{g}$ would not change the 3D conformation of a 3D graph [[134](#bib.bib134),
    [135](#bib.bib135), [116](#bib.bib116)]. $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$
    is the set of position matrices, $\mathcal{F}(G)\Longleftrightarrow\mathcal{P}$,
    means positions can generate geometric representations, which can also be recovered
    from them.
  prefs: []
  type: TYPE_NORMAL
- en: Global completeness enhances the robustness of statistical analyses applied
    to protein structure data. Proteins with SE(3) equivalence share identical 3D
    conformations but may differ in orientations and positions. To discern diverse
    conformers, it is essential to comprehensively model entire protein structures.
    Solely focusing on local regions would overlook substantial long-range effects
    arising from subtle conformational changes occurring at a distance [[119](#bib.bib119)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/286328a0e6140514dd3ac544410118ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Examples of proteins with biology knowledge. (a) A sub-graph in
    ProteinKG25 with proteins, GO terms, and relations [[10](#bib.bib10)]. Yellow
    nodes are proteins and blue nodes are GO entities with biological descriptions.
    (b) An example of protein property descriptions, including protein name, function
    texts, subcellular, and similarity [[136](#bib.bib136)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Biology Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The biology knowledge can enhance deep learning based protein models to understand
    the protein structure-function relationships and enable various applications in
    bioinformatics. Zhang et al. [[10](#bib.bib10)] have constructed ProteinKG25,
    which provides large-scale biology knowledge facts aligned with protein sequences,
    as shown in Figure [12](#S4.F12 "Figure 12 ‣ Complete Geometries ‣ 4.5 Structure
    Properties ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey")(a). The attributes and relation terms are described using
    natural languages, which are extracted from, GO³³3[https://www.uniprot.org/uniprotkb](https://www.uniprot.org/uniprotkb),
    the world’s largest source of information on the functions of genes and gene products
    (e.g., protein). In order to enhance protein sequences with text descriptions
    of their functions, Xu et al. [[136](#bib.bib136)] describe protein in four fields:
    protein name, functions, the location in a cell, and protein families that a protein
    belongs to (refer to Figure [12](#S4.F12 "Figure 12 ‣ Complete Geometries ‣ 4.5
    Structure Properties ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey")(b)).'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Deep Learning based Protein Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we summarize some commonly used deep learning models for processing
    protein data, including sequences, structures, functions or hybrid of them.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Protein Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the inherent similarities between proteins and human languages, protein
    sequences, which are represented as strings of amino acid letters, naturally lend
    themselves to LMs. LMs are capable of capturing complex dependencies among these
    amino acids [[99](#bib.bib99)]. Consequently, protein LMs have emerged as promising
    approaches for learning protein sequences, with the ability to handle both single
    sequences and MSAs as input.
  prefs: []
  type: TYPE_NORMAL
- en: Single Sequences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To begin, we introduce methods that mainly take a single sequence as input.
    Early deep learning methods often utilized CNNs, LSTM, or their combinations [[137](#bib.bib137),
    [138](#bib.bib138), [139](#bib.bib139)], to predict protein structural features
    and properties. Examples of such methods include DeepPrime2Sec [[140](#bib.bib140)],
    SPOT-1D-Single [[141](#bib.bib141)]. Additionally, the Variational Auto-Encoder
    (VAE) [[142](#bib.bib142), [143](#bib.bib143)] has been employed to learn interactions
    between positions within a protein. Given protein sequential data $X$ and latent
    variables $Z$, the protein VAE model aims to learn the joint probability $p(X,Z)=p(Z)p(X|Z)$.
    However, computing $p(Z|X)$ from observed data necessitates evaluating the evidence
    term for each data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(X)=\int p(X&#124;Z)p(Z)d_{Z}$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: direct computation of this integral is intractable. Consequently, VAE models
    typically adopt an Evidence Lower BOund (ELBO) [[142](#bib.bib142)] to approximate
    $p(X)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: LMs used in ProtTrans [[12](#bib.bib12)]'
  prefs: []
  type: TYPE_NORMAL
- en: Model Network Pretext Task $\#$Params. Comments BERT [[52](#bib.bib52)] Transformer
    Masked Language Modeling 340M a commonly-used LM for predicting masked tokens
    ALBERT [[146](#bib.bib146)] BERT Masked Language Modeling 223M a lite version
    of BERT Transformer-XL [[147](#bib.bib147)] Transformer Autoregressive Language
    Modeling 257M enabling learn dependencies beyond a fixed length XLNet [[87](#bib.bib87)]
    BERT Autoregressive Language Modeling 340M more training data, integrates ideas
    from Transformer-XL ELECTRA [[148](#bib.bib148)] BERT Replaced Token Detection
    335M for token detection T5 [[149](#bib.bib149)] Transformer Masked Language Modeling
    11B a text-to-text transfer learning framework • All examples report the largest
    model of their public series. Network displays high-level backbone models preferentially
    if they are used to initialize parameters. $\#$Param. means the number of parameters;
    M, millions; B, billions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, the pre-training of protein LMs in the absence of structural or evolutionary
    data has been explored. Alley et al. [[105](#bib.bib105)] employ multiplicative
    long-/short-term memory (mLSTM) [[144](#bib.bib144)] to condense arbitrary protein
    sequences into fixed-length vectors. Notably, TAPE [[145](#bib.bib145)] has introduced
    a benchmark for protein models, including LSTM, Transformer, ResNet, among others,
    by means of self-supervised pre-training and subsequent evaluation on a set of
    five biologically relevant tasks. Elnaggar et al. [[12](#bib.bib12)] successfully
    trained six LMs (BERT [[52](#bib.bib52)], ALBERT [[146](#bib.bib146)], Transformer-XL [[147](#bib.bib147)],
    XLNet [[87](#bib.bib87)], ELECTRA [[148](#bib.bib148)] and T5 [[149](#bib.bib149)],
    show in Table [1](#S5.T1 "Table 1 ‣ Single Sequences ‣ 5.1 Protein Language Models
    ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein
    Science: A Comprehensive Survey")) on protein sequences encompassing a staggering
    393B amino acids, leveraging extensive computational resources (5616 GPUs and
    one TPU Pod). ESM-1b [[14](#bib.bib14)], on the other hand, consists of a deep
    Transformer architecture (illustrated in Figure [13](#S5.F13 "Figure 13 ‣ Single
    Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning based Protein Models
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a)) and
    a masking strategy to construct intricate representations that incorporate contextual
    information from across the entire sequence. The outcomes of ProtTrans [[12](#bib.bib12)]
    and ESM-1b suggest that large-scale protein LMs possess the ability to learn the
    underlying grammar of proteins, even without explicit utilization of apparent
    evolutionary information. With the support of large-scale databases, training
    resources, researchers have started exploring the boundaries of protein LMs by
    constructing billion-level models [[166](#bib.bib166), [169](#bib.bib169), [13](#bib.bib13)].
    For example, ProGen2 [[166](#bib.bib166)] demonstrates that large protein LMs
    can generate libraries of viable sequences, expanding the sequence and structural
    space of natural proteins, obtaining the results suggest the scale of the model
    size can be continued. Moreover, a large-scale protein LM, ESM-2 [[13](#bib.bib13)]
    with trainable parameters up to 15B (billion), has achieved impressive results
    on the PSP task, surpassing smaller ESM models in terms of validation perplexity
    and TM-score [[175](#bib.bib175)]. Chen et al. have proposed a unified protein
    LM, xTrimoPGLM [[170](#bib.bib170)], to handle protein understanding and generation
    tasks concurrently, which involves 100B parameters and 1 trillion training tokens.
    These pre-trained large-scale protein LMs showcase their effectiveness on various
    protein-related tasks. A summary of the pre-trained protein LMs and structure
    models is listed in Table [2](#S5.T2 "Table 2 ‣ Single Sequences ‣ 5.1 Protein
    Language Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/020fdbf7283e22cd187b0d861789f837.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Core modules of ESM-1b and MSA Transformer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: List of representative pre-trained protein LMs and structure models'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model and Repository Input Network $\#$Embedding $\#$Param. Pretext Task Pre-training
    Dataset Year [UniRep](https://github.com/churchlab/Unirep) [[105](#bib.bib105)]
    Seq mLSTM [[144](#bib.bib144)] 1900 18.2M Next Amino Acid Prediction UniRef50
    2019 [TAPE](https://github.com/songlab-cal/tape) [[145](#bib.bib145)] Seq LSTM,
    Transformer, ResNet - 38M Masked Language Modeling Pfam 2019 Next Amino Acid Prediction
    [SeqVec](https://github.com/Rostlab/SeqVec) [[150](#bib.bib150)] Seq ELMo (LSTM) [[151](#bib.bib151)]
    1024 93.6M Next Amino Acid Prediction UniRef50 2019 [UDSMProt](https://github.com/nstrodt/UDSMProt) [[152](#bib.bib152)]
    Seq LSTM 400 24M Next Amino Acid Prediction Swiss-Prot 2020 [CPCProt](https://github.com/amyxlu/CPCProt) [[153](#bib.bib153)]
    Seq GRU [[154](#bib.bib154)], LSTM 1024, 2048 1.7M Contrastive Predictive Coding
    Pfam 2020 [MuPIPR](https://github.com/guangyu-zhou/MuPIPR) [[155](#bib.bib155)]
    Seq GRU, LSTM 64 - Next Amino Acid Prediction STRING [[156](#bib.bib156)] 2020
    Profile Prediction [[157](#bib.bib157)] MSA Transformer - - Alignment Profiles
    Prediction Pfam 2020 PRoBERTa [[158](#bib.bib158)] Seq Transformer 768 44M Masked
    Language Modeling Swiss-Prot 2020 [ESM-1b](https://github.com/facebookresearch/esm) [[14](#bib.bib14)]
    Seq Transformer 1280 650M Masked Language Modeling UniParc 2021 [ProtTXL](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq Transformer-XL 1024 562M Masked Language Modeling BFD100, UniRef100 2021 [ProtBert](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq BERT 1024 420M Masked Language Modeling BFD100, UniRef100 2021 [ProtXLNet](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq XLNet 1024 409M Masked Language Modeling UniRef100 2021 [ProtAlbert](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq ALBERT 4096 224M Masked Language Modeling UniRef100 2021 [ProtElectra](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq ELECTRA 1024 420M Masked Language Modeling UniRef100 2021 [ProtT5](https://github.com/agemagician/ProtTrans) [[12](#bib.bib12)]
    Seq T5 1024 11B Masked Language Modeling UniRef50, BFD100 2021 PMLM [[159](#bib.bib159)]
    Seq Transformer 1280 715M Masked Language Modeling UniRef50 2021 [MSA Transformer](https://github.com/facebookresearch/esm) [[160](#bib.bib160)]
    MSA Transformer 768 100M Masked Language Modeling UniRef50, UniClust30 2021 [ProteinLM](https://github.com/THUDM/ProteinLM) [[161](#bib.bib161)]
    Seq BERT - 3B Masked Language Modeling Pfam 2021 [PLUS-RNN](https://github.com/seonwoo-min/PLUS) [[162](#bib.bib162)]
    Seq RNN 2024 59M Masked Language Modeling Pfam 2021 Same-Family Prediction [CARP](https://github.com/microsoft/protein-sequence-models) [[163](#bib.bib163)]
    Seq CNN 1280 640M Masked Language Modeling UniRef50 2022 [AminoBERT](https://github.com/aqlaboratory/rgn2) [[22](#bib.bib22)]
    Seq Transformer 3072 - Masked Language Modeling UniParc 2022 [OmegaPLM](https://github.com/HeliXonProtein/OmegaFold) [[164](#bib.bib164)]
    Seq GAU [[165](#bib.bib165)] 1280 670M Masked Language Modeling UniRef50 2022
    Span and Sequential Masking [ProGen2](https://github.com/salesforce/progen) [[166](#bib.bib166)]
    Seq Transformer 4096 6.4B Masked Language Modeling UniRef90, BFD30, BFD90 2022
    [ProtGPT2](https://huggingface.co/nferruz/ProtGPT2) [[167](#bib.bib167)] Seq GPT-2 [[168](#bib.bib168)]
    1280 738M Next Amino Acid Prediction UniRef50 2022 [RITA](https://github.com/lightonai/RITA) [[169](#bib.bib169)]
    Seq GPT-3 [[55](#bib.bib55)] 2048 1.2B Next Amino Acid Prediction UniRef100 2022
    [ESM-2](https://github.com/facebookresearch/esm) [[13](#bib.bib13)] Seq Transformer
    5120 15B Masked Language Modeling UniRef50 2022 xTrimoPGLM [[170](#bib.bib170)]
    Seq Transformer 10240 100B Masked Language Modeling Uniref90 2023 Span Tokens
    Prediction ColAbFoldDB [ReprogBERT](https://github.com/IBM/ReprogBERT) [[171](#bib.bib171)]
    Seq BERT 768 110M Masked Language Modeling English Wikipedia 2023 BookCorpus [PoET](https://github.com/OpenProteinAI/PoET) [[172](#bib.bib172)]
    MSA Transformer 1024 201M Next Amino Acid Prediction UniRef50 2023 [CELL-E2](https://bohuanglab.github.io/CELL-E_2/) [[173](#bib.bib173)]
    Seq Transformer 480 35M Masked Language Modeling Human Protein Atlas [[174](#bib.bib174)]
    2023 GraphMS [[187](#bib.bib187)] Struct GCN - - Multiview Contrast NeoDTI[[188](#bib.bib188)]
    2021 CRL [[189](#bib.bib189)] Struct IEConv [[186](#bib.bib186)] 2048 36.6M Multiview
    Contrast PDB 2022 STEPS [[190](#bib.bib190)] Struct GNN 1280 - Distance and Dihedral
    Prediction PDB 2022 • Examples report the largest model of their public series.
    Seq: sequence, Struct: Structure. $\#$Embedding means the dimension of embeddings;
    $\#$Param., the number of parameters of network; M, millions; B, billions. Some
    models are linked with the GitHub repositories.'
  prefs: []
  type: TYPE_NORMAL
- en: MSA Sequences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As depicted in Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Protein and Language ‣ 4 Protein
    Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")(a),
    the inference of residue contact maps from MSA sequences has been a long-standing
    practice in computational biology [[176](#bib.bib176)]. This approach has been
    relied upon in the early stages, as large protein LMs were not developed to extract
    implicit coevolutionary information from individual sequences. It is evident that
    additional information, such as MSAs, can enhance protein embeddings. MSA Transformer [[160](#bib.bib160)]
    extends transformer-based LMs to handle sets of sequences as inputs by employing
    alternating attention over rows and columns, as illustrated in Figure [13](#S5.F13
    "Figure 13 ‣ Single Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")(b). The internal representations of MSA Transformer enable high-quality
    unsupervised structure learning with significantly fewer parameters compared to
    contemporary protein LMs. Additionally, AF2 [[16](#bib.bib16)] has leveraged row-wise
    and column-wise self-attentions to capture rich information within MSA representations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Protein Structure Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Protein structures contain extremely valuable information that can be used for
    understanding biological processes and facilitating important interventions like
    the development of drugs based on structural characteristics or targeted genetic
    modifications [[16](#bib.bib16)]. In addition to sequence-based encoders, structure-based
    encoders have been developed to leverage the 3D structural information of proteins.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4588ddbe8fdb49342b0a126f550776b7.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Distance map
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d0135025320e3775c3e8aacc93a0f20.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Contact map
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: An example of protein distance and contact maps, visualized by MapPred [[178](#bib.bib178)].'
  prefs: []
  type: TYPE_NORMAL
- en: Invariant Geometries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Firstly, we introduce the modeling methods of distance and contact maps. A
    distance map of proteins, also referred to as a residue-residue distance map,
    is a graphical representation that shows the distances between pairs of amino
    acid residues in a protein structure. It provides valuable information about the
    spatial proximity between residues within the protein. Typically, the distance
    map is calculated based on the positions of the $\mathrm{C}_{\alpha}$ atoms, denoted
    as $d_{ij,\mathrm{C}\alpha}={P}_{i,\mathrm{C}\alpha}-{P}_{j,\mathrm{C}\alpha}$,
    where ${P}_{i,\mathrm{C}\alpha}$ denotes the 3D position of $\mathrm{C}_{\alpha}$
    in the $i$-th residue. From a distance map, a contact map can be derived by assigning
    a value to each element representing the distance between two atoms. In practice,
    two residues (or amino acids) $i$ and $j$ of the same protein are considered in
    contact if their Euclidean distance is below a specific threshold value, often
    set at 8 Å [[177](#bib.bib177)]. An example of a distance map and contact map
    can be seen in Figure [14](#S5.F14 "Figure 14 ‣ 5.2 Protein Structure Models ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"). CNNs, such as ResNet, are commonly employed to process
    these feature maps [[179](#bib.bib179)], generating more accurate maps [[180](#bib.bib180)]
    or protein embeddings. Additionally, 3D CNNs have been utilized to identify interaction
    patterns on protein surfaces [[181](#bib.bib181)]. It is worth noting that these
    feature maps are often used in conjunction with protein sequences to provide supplementary
    information. For instance, ProSE [[46](#bib.bib46)] incorporates structural supervision
    through residue-residue contact loss, along with sequence masking loss, to better
    capture the semantic organization of proteins, leading to improved protein function
    prediction. Furthermore, the recently proposed Struct2GO model [[182](#bib.bib182)]
    transforms the protein’s 3D structure into a protein contact graph and utilizes
    amino acid-level embeddings as node representations, enhancing the accuracy of
    protein function prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to distance and contact maps, there are other invariant features
    that can be used to capture the geometric properties of proteins. These include
    backbone torsion angles, trRosetta interresidue geometries, and Euler angles,
    etc., which are described in Subsection [4.4](#S4.SS4 "4.4 Protein Structure Geometries
    ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey"). The simple approach to achieve SE(3) group symmetry in molecule geometry
    is through invariant modeling. Invariant modeling focuses on capturing only the
    invariant features or type-0 features [[131](#bib.bib131)]. These type-0 features
    remain unchanged regardless of rotation and translation. To model the geometric
    relationships between amino acids, GNNs are commonly employed, including methods
    such as GCN [[82](#bib.bib82), [183](#bib.bib183), [184](#bib.bib184)], GAT [[83](#bib.bib83)],
    and GraphSAGE [[84](#bib.bib84)]. To represent the protein structures as graphs,
    the 1D and 2D features are used as node and edge features, such as the edge features
    $\bm{e}_{ij}^{(1)}$ and $\bm{e}_{ij}^{(2)}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the field of protein research, there are three common graph construction
    methods: sequential graph, radius graph, and $k$-Nearest Neighbors ($k$NN) graph [[15](#bib.bib15)].
    Here, given a graph $G=(\mathcal{V},\mathcal{E},X,E)$, with vertex and edge sets
    $\mathcal{V}=\{v_{i}\}_{i=1,\ldots,n}$ and $\mathcal{E}=\left\{\varepsilon_{ij}\right\}_{i,j=1,\ldots,n}$,
    the sequential graph is defined based on the sequence. If $\left\|i-j\right\|<l_{seq}$,
    an edge $\varepsilon_{ij}$ exists, where $l_{seq}$ is a hyperparameter. For the
    radius graph, an edge exists between nodes $v_{i}$ and $v_{j}$ if $\left\|d_{ij,\mathrm{C}\alpha}\right\|<r$,
    where $r$ is a pre-defined radius. The $k$NN graph connects each node to its $k$
    closest neighbors, with $k$ commonly set to 30 [[119](#bib.bib119), [185](#bib.bib185)].
    An illustration of these graph construction methods is shown in Figure [15](#S5.F15
    "Figure 15 ‣ Invariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ceda8d30f91b137a44fa47a3bf838b1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Relational protein residue graphs with sequential edges, radius
    edges, and $k$NN edges for residue $i$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To leverage these invariant geometric features effectively, Hermosilla et al. [[186](#bib.bib186)]
    introduced protein convolution and pooling techniques to capture the primary,
    secondary, and tertiary structures of proteins by incorporating intrinsic and
    extrinsic distances between nodes and atoms. Furthermore, Wang et al. [[116](#bib.bib116)]
    introduced complete geometric representations and a complete message passing scheme
    that covers protein geometries at the amino acid, backbone, and all-atom levels.
    In Subsection [4.5](#S4.SS5 "4.5 Structure Properties ‣ 4 Protein Foundations
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey"), we provide
    definitions for complete geometries, which generally refer to 3D positions that
    can generate geometric representations and can be recovered from them. Such representations
    are considered complete. By incorporating complete geometric representations into
    the commonly-used message passing framework (Eq. [11](#S3.E11 "In 3.4 Graph Neural
    Networks ‣ 3 Basic Neural Networks in Protein Modeling ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey")), a complete message passing scheme
    can be achieved [[134](#bib.bib134), [135](#bib.bib135), [116](#bib.bib116)].'
  prefs: []
  type: TYPE_NORMAL
- en: Equivariant Geometries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Invariant modeling only captures the type-0 features, although protein can
    be represented as a graph naturally, it remains under-explored mainly due to the
    significant challenges. For example, it is challenging to extract and preserve
    multi-level rotation and translation equivariant information and effectively leverage
    the input spatial representations to capture complex geometries across the spatial
    dimension. Thus higher-order particles include type-1 features like coordinates
    and forces in molecular conformation are important to be considered [[131](#bib.bib131)].
    An equivariant message passing paradigm on proteins embeded into existing GNNs
    has been developed [[191](#bib.bib191)], like GBPNet [[192](#bib.bib192)], showing
    superior versatility in maintaining equivariance. AtomRefine [[193](#bib.bib193)]
    uses a SE(3)-equivariant graph transformer network to refine protein structures,
    where each block in the SE(3) transformer network consists of one equivariant
    GCN attention block. Specifically, jing et al. [[128](#bib.bib128)] introduce
    geometric vector perceptrons (GVP) and operate directly on both scalar and vector
    features under a global coordinate system (refer to Figure [16](#S5.F16 "Figure
    16 ‣ Equivariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning based
    Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have introduced the definitions of invariance and equivariance in Subsection [4.5](#S4.SS5
    "4.5 Structure Properties ‣ 4 Protein Foundations ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"). Here, we introduce the mechanisms
    of equivariant GNNs (EGNNs [[194](#bib.bib194)]) applied in protein. We consider
    a 3D graph as $G=(\mathcal{V},\mathcal{E},X,E)$, $P_{i,\mathrm{C}_{\alpha}}$ is
    the position of $\mathrm{C}_{\alpha}$ in node $v_{i}$, coordinate embeddings $\mathcal{P}_{\mathrm{C}_{\alpha}}^{(l)}=\{P_{i,\mathrm{C}_{\alpha}}^{(l)}\}_{i=1,\ldots,n}$.
    The node and edge embeddings are $H^{(l)}=[\bm{h}_{i}^{(l)}]_{i=1,\ldots,n}$ and
    $E=[\bm{e}_{ij}]_{i,j=1,\ldots,n}$, $H^{(0)}=X$, the following equations can define
    the $l$-th equivariant message passing layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\bm{m}_{ij}$ | $\displaystyle=\psi_{e}\left(\bm{h}_{i}^{l},\bm{h}_{j}^{l},\left\&#124;P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l}\right\&#124;,e_{ij}\right)$
    |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle P_{i,\mathrm{C}_{\alpha}}^{l+1}$ | $\displaystyle=P_{i,\mathrm{C}_{\alpha}}^{l}+C\sum_{v_{j}\in\mathcal{N}(v_{i})}\left(P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l}\right)\psi_{x}\left(\bm{m}_{ij}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\bm{m}_{i}$ | $\displaystyle=\sum_{v_{j}\in\mathcal{N}(v_{i})}\bm{m}_{ij}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\bm{h}_{i}^{l+1}$ | $\displaystyle=\phi\left(\bm{h}_{i}^{l},\bm{m}_{i}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: here, $C$ is a constant number, $\psi_{e}$ and $\psi_{x}$ are the message functions,
    and $\phi$ is the update function. The coordinate embeddings $\mathcal{P}_{\mathrm{C}_{\alpha}}^{(l)}$
    are updated by the weighted sum of all relative neighbors’ differences $(P_{i,\mathrm{C}_{\alpha}}^{l}-P_{j,\mathrm{C}_{\alpha}}^{l})$.
    The coordinate embeddings are also used to update the invariant node embeddings
    by the $l^{2}$-norm ($\left\|\cdot\right\|$). These operations maintain the equivariance
    in GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4edb28d24345292a17d5b226ed46e034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Schematic of the geometric vector perceptron in GVP-GNN [[128](#bib.bib128)].
    FC: The linear weight in a fully connected layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: List of representative pre-trained protein multimodal models'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model and Repository Input Network $\#$Embedding $\#$Param. Pretext Task Pre-training
    Dataset Year [SSA](https://github.com/tbepler/protein-sequence-embedding-iclr2019) [[195](#bib.bib195)]
    Seq, Struct BiLSTM 100, 512 - Contact and Similarity Prediction Pfam, SCOP 2019
    [LM-GVP](https://github.com/aws-samples/lm-gvp) [[196](#bib.bib196)] Seq, Struct
    ProtBert, GVP 1024 420M - UniRef100 2021 [DeepFRI](https://github.com/flatironinstitute/DeepFRI) [[197](#bib.bib197)]
    Seq, Struct LSTM, GCN 512 6.2M GO Term Prediction Pfam 2021 HJRSS [[198](#bib.bib198)]
    Seq, Struct SE(3) Transformer 128 16M Masked Language and Graph Modeling trRosetta2 [[199](#bib.bib199)]
    2021 [GraSR](https://github.com/chunqiux/GraSR) [[200](#bib.bib200)] Seq, Struct
    LSTM, GCN 32 - Momentum Contrast[[202](#bib.bib202)] SCOPe 2022 [CPAC](https://github.com/Shen-Lab/CPAC) [[203](#bib.bib203)]
    Seq, Struct RNN, GAT 128 - Masked Language and Graph Modeling Pfam 2022 [MIF-ST](https://github.com/microsoft/protein-sequence-models) [[204](#bib.bib204)]
    Seq, Struct CNN, GNN 256 640M Masked Language Modeling UniRef50 2022 PeTriBERT [[293](#bib.bib293)]
    Seq, Struct BERT 3072 40M Next Amino Acid Prediction AlphaFoldDB 2022 [GearNet](https://github.com/DeepGraphLearning/GearNet) [[15](#bib.bib15)]
    Seq, Struct GNN 512 42M Distance, Angle and Residue Type Prediction AlphaFoldDB
    2022 Multiview Contrast [ESM-GearNet](https://github.com/DeepGraphLearning/SiamDiff) [[205](#bib.bib205)]
    Seq, Struct ESM, GearNet 512 692M Distance, Angle and Residue Type Prediction
    AlphaFoldDB 2023 Multiview Contrast, SiamDiff [[206](#bib.bib206)] [SaProt](https://github.com/SaProt/SaProt) [[207](#bib.bib207)]
    Seq, Struct ESM-2 1280 650M Masked Language Modeling AlphaFoldDB, PDB 2023 [ProGen](https://github.com/lucidrains/progen) [[208](#bib.bib208)]
    Seq, Func Transformer 1028 1.2B Next Amino Acid Prediction Uniparc, UniProtKB,
    Swiss-Prot 2020 Pfam, TrEMBL, NCBI [[209](#bib.bib209)] [ProteinBERT](https://github.com/nadavbra/protein_bert) [[210](#bib.bib210)]
    Seq, Func Transformer 512 16M Masked Language Modeling UniRef90 2021 [OntoProtein](https://github.com/zjunlp/OntoProtein) [[10](#bib.bib10)]
    Seq, Func ProtBert, BERT 1024 - Masked Language Modeling ProteinKG25 [[10](#bib.bib10)]
    2022 Embedding Contrast [KeAP](https://github.com/RL4M/KeAP) [[211](#bib.bib211)]
    Seq, Func BERT, PubMedBERT [[213](#bib.bib213)] 1024 520M Masked Language Modeling
    ProteinKG25 2023 [ProtST](https://github.com/DeepGraphLearning/ProtST) [[136](#bib.bib136)]
    Seq, Func ProtBert, ESM, PubMedBERT 1024 750M Masked Language Modeling ProtDescribe [[136](#bib.bib136)]
    2023 [MASSA](https://github.com/SIAT-code/MASSA) [[114](#bib.bib114)] Seq, Struct
    ESM-MSA, GVP-GNN - - Masked Language Modeling UniProtKB, Swiss-Prot 2023 Func
    Transformer, GraphGO AlphaFoldDB, RCSB PDB [[214](#bib.bib214)] ProteinINR [[212](#bib.bib212)]
    Seq, Struct ESM-1b, GearNet - - Multiview Contrast 20 Species, 2024 Surface Transformer
    Swiss-Prot • Examples report the largest model of their public series. Seq: sequence,
    Struct: Structure, Func: Function. $\#$Embedding means the dimension of embeddings;
    $\#$Param., the number of parameters of network; M, millions; B, billions. Some
    models are linked with the GitHub repositories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Information of Protein Databases'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset $\#$Proteins Disk Space Description Link UniProtKB/Swiss-Prot [[9](#bib.bib9)]
    500K 0.59GB knowledgebase [https://www.uniprot.org/uniprotkb?query=*](https://www.uniprot.org/uniprotkb?query=*)
    UniProtKB/TrEMBL [[9](#bib.bib9)] 229M 146GB knowledgebase [https://www.uniprot.org/uniprotkb?query=*](https://www.uniprot.org/uniprotkb?query=*)
    UniRef100 [[215](#bib.bib215)] 314M 76.9GB clustered sets of sequences [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniRef90 [[215](#bib.bib215)] 150M 34GB 90$\%$ identity [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniRef50 [[215](#bib.bib215)] 53M 10.3GB 50$\%$ identity [https://www.uniprot.org/uniref?query=*](https://www.uniprot.org/uniref?query=*)
    UniParc [[9](#bib.bib9)] 528M 106GB sequence [https://www.uniprot.org/uniparc?query=*](https://www.uniprot.org/uniparc?query=*)
    PDB [[8](#bib.bib8)] 180K 50GB 3D structure [https://www.wwpdb.org/ftp/pdb-ftp-sites](https://www.wwpdb.org/ftp/pdb-ftp-sites)
    CATH4.3 [[216](#bib.bib216)] - 1073MB hierarchical classification [https://www.cathdb.info/](https://www.cathdb.info/)
    BFD [[217](#bib.bib217)] 2500M 272GB sequence profile [https://bfd.mmseqs.com/](https://bfd.mmseqs.com/)
    Pfam [[218](#bib.bib218)] 47M 14.1GB protein families [https://www.ebi.ac.uk/interpro/entry/pfam/](https://www.ebi.ac.uk/interpro/entry/pfam/)
    AlphaFoldDB [[219](#bib.bib219)] 214M 23 TB predicted 3D structures [https://alphafold.ebi.ac.uk/](https://alphafold.ebi.ac.uk/)
    ESM Metagenomic Atlas [[13](#bib.bib13)] 772M - predicted metagenomic protein
    structures [https://esmatlas.com/](https://esmatlas.com/) ColAbFoldDB [[170](#bib.bib170)]
    950M - an amalgamation of various metagenomic databases [https://colabfold.mmseqs.com/](https://colabfold.mmseqs.com/)
    ProteinKG25 [[10](#bib.bib10)] 5.6M 147MB a knowledge graph dataset with GO [https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR](https://drive.google.com/file/d/1iTC2-zbvYZCDhWM_wxRufCvV6vvPk8HR)
    Uniclust30 [[220](#bib.bib220)] - 6.6GB clustered protein sequences [https://uniclust.mmseqs.com/](https://uniclust.mmseqs.com/)
    SCOP [[221](#bib.bib221)] - - structural classification [http://scop.mrc-lmb.cam.ac.uk/](http://scop.mrc-lmb.cam.ac.uk/)
    SCOPe [[222](#bib.bib222)] - 86MB an extended version of SCOP [http://scop.berkeley.edu](http://scop.berkeley.edu)
    OpenProteinSet [[223](#bib.bib223)] 16M - MSAs [https://dagshub.com/datasets/openproteinset/](https://dagshub.com/datasets/openproteinset/)
    • K, thousand; M, million, disk space is in GB or TB (compressed storage as text),
    which is estimated data influenced by the compressed format.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Protein Multimodal Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in Subsection [4.3](#S4.SS3 "4.3 Motifs, Regions, and Domains
    ‣ 4 Protein Foundations ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey"), protein data encompasses various types of information, such as sequences,
    structures, GO annotations, motifs, regions, domains, and more. To gain a comprehensive
    understanding of proteins, it is crucial to consider and integrate these multimodal
    sources of information.'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-structure Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, there has been a growing focus on sequence-structure co-modeling
    methods, which aim to capture the intricate relationships between protein sequences
    and structures. Rather than treating protein sequences and structures as separate
    entities, these methods leverage the complementary information from both domains
    to improve modeling performance.
  prefs: []
  type: TYPE_NORMAL
- en: One prevailing approach involves using pre-trained protein LMs, such as ESM-1b
    and ProtTrans, to obtain embeddings for sequences. For example, SPOT-1D-LM [[224](#bib.bib224),
    [225](#bib.bib225)] and BIB [[226](#bib.bib226)] utilize pre-trained LMs to generate
    embeddings for contact map prediction and biological sequence design. To incorporate
    structural information into protein LMs, LM-GVP [[196](#bib.bib196)] proposes
    a novel fine-tuning procedure that explicitly injects inductive bias from complex
    structure information using GVP. This method enhances the representation capability
    of protein LMs by considering both sequence and structure information, leading
    to improved performance in downstream tasks. Another approach, GearNet [[15](#bib.bib15)],
    simultaneously encodes sequential and spatial information by incorporating different
    types of sequential or structural edges. It performs node-level and edge-level
    message passing, enabling it to capture both local and global dependencies in
    protein sequences and structures. This comprehensive modeling approach has shown
    promising results in various applications. Foldseek [[227](#bib.bib227)] employs
    a VQ-VAE [[228](#bib.bib228)] model to encode protein structures into informative
    tokens. These tokens combine residue and 3D geometric features, effectively representing
    both primary and tertiary structures. By representing protein structures as a
    sequence of these novel tokens, Foldseek seamlessly integrates the foundational
    models like BERT and GPT to process protein sequences and structures simultaneously,
    SaProt [[207](#bib.bib207)] is also an example of such integration, where it combines
    these tokens using general-purpose protein LMs. Without using the structure information
    as input, Bepler and Berger [[46](#bib.bib46)] carry out multi-task with structural
    supervision, leading to an even better-organized embedding space compared with
    a single task.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-function Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Protein Sequence-function Modeling is a research field dedicated to comprehending
    the intricate relationships between protein sequences and their functional properties.
    GO annotations provide valuable structured information about protein functions,
    enabling researchers to systematically analyze and compare protein functions across
    different species and experimental studies [[229](#bib.bib229)]. The function
    information is typically derived from prior biological knowledge, which can be
    easily incorporated into sentences, as depicted in Figure [12](#S4.F12 "Figure
    12 ‣ Complete Geometries ‣ 4.5 Structure Properties ‣ 4 Protein Foundations ‣
    Advances of Deep Learning in Protein Science: A Comprehensive Survey"). Consequently,
    the study of protein sequences and functions often goes hand in hand, with LMs
    being commonly employed. One notable model in this domain is ProteinBERT [[210](#bib.bib210)],
    which undergoes pre-training on protein sequences and GO annotations using two
    interconnected BERT-like encoders. By leveraging large-scale biology knowledge
    datasets, ProteinKG25 [[10](#bib.bib10)], OntoProtein [[10](#bib.bib10)], on the
    other hand, focuses on reconstructing masked amino acids while simultaneously
    minimizing the embedding distance between contextual representations of proteins
    and associated knowledge terms. In comparison, KeAP [[211](#bib.bib211)] aims
    to explore the relationships between proteins and knowledge at a more granular
    level than OntoProtein.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2be587b725eb4e0ad386eb98b5113c0c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ffe72de1b91d582b12e3efb60be891f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17: Illustration of diverse protein modalities. (a) The method of calculating
    the conditional probability of a protein that contains domain $i$ having the GO
    $j$ function in DomainPFP [[230](#bib.bib230)]. (b) The protein surface at different
    time steps in a molecular dynamics trajectory [[232](#bib.bib232)].'
  prefs: []
  type: TYPE_NORMAL
- en: Diverse Modalities Modeling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are models that leverage deep learning techniques to incorporate diverse
    modalities, enabling a more comprehensive understanding of protein structure,
    function, and dynamics. For instance, MASSA [[114](#bib.bib114)] is an advanced
    multimodal deep learning framework designed to incorporate protein sequence, structure,
    and functional annotation. It employs five specific pre-training objectives to
    enhance its performance, including masked amino acid and GO, placement capture
    of motifs, domains, and regions. Domain-PFP [[230](#bib.bib230)], on the other
    hand, utilizes a self-supervised protocol to derive functionally consistent representations
    for protein domains. It achieves this by learning domain-GO co-occurrences and
    associations by calculating the probabilities of domains and GO terms as illustrated
    in Figure [17a](#S5.F17.sf1 "In Figure 17 ‣ Sequence-function Modeling ‣ 5.3 Protein
    Multimodal Methods ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), resulting in an improved understanding
    of domain functionality. BiDock [[231](#bib.bib231)] is a robust rigid docking
    model that effectively integrates MSAs and structural information. By employing
    a cross-modal transformer through bi-level optimization, BiDock enhances the accuracy
    of protein docking predictions. To capture protein dynamics, Sun et al. [[232](#bib.bib232)]
    focus on representing protein surfaces using implicit neural networks. This approach
    is particularly well-suited for modeling the dynamic shapes of proteins, which
    often exhibit complex and varied conformations, as shown in Figure [17b](#S5.F17.sf2
    "In Figure 17 ‣ Sequence-function Modeling ‣ 5.3 Protein Multimodal Methods ‣
    5 Deep Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"). Representative pre-trained protein multimodal models
    are listed in Table [3](#S5.T3 "Table 3 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey"), and relative common datasets are
    presented in Table [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Assessment of Pre-training Methods for Protein Representation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we enumerate various types of deep protein models, with a
    particular focus on PRL methods commonly employed in diverse scenarios. The evaluation
    encompasses several widely used pre-trained PRL methods applied to four distinct
    downstream tasks: (a) protein fold classification, (b) enzyme reaction classification,
    (c) GO term prediction, and (d) enzyme commission (EC) number prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For protein fold classification (a), we adhere to the methodology outlined
    by Hermosilla et al. [[186](#bib.bib186)]. This dataset [[222](#bib.bib222)] comprises
    16,712 proteins distributed across 1,195 fold classes, featuring three provided
    test sets: Fold (excluding proteins from the same superfamily during training),
    SuperFamily (omitting proteins from the same family during training), and Family
    (including proteins from the same family in the training set). Enzyme reaction
    classification (b) is treated as a protein function prediction task based on enzyme-catalyzed
    reactions defined by the four levels of enzyme commission numbers [[233](#bib.bib233)].
    The dataset, compiled by Hermosilla et al. [[186](#bib.bib186)], encompasses 29,215
    training proteins, 2,562 validation proteins, and 5,651 test proteins. For GO
    term prediction (c), the objective is to predict whether a given protein should
    be annotated with a specific GO term. The GO term prediction dataset includes
    29,898/3,322/3,415 proteins for training/validation/test, respectively. In EC
    number prediction (d), the goal is to predict 538 EC numbers at the third and
    fourth levels of hierarchies for different proteins, following the methodology
    of DeepFRI [[197](#bib.bib197)]. The training/validation/test datasets comprise
    a total of 15,550/1,729/1,919 proteins. For both GO term and EC number prediction,
    the test sets only include PDB chains with a sequence identity no greater than
    95% to the training set [[59](#bib.bib59)]. Similar settings are also employed
    in LM-GVP [[196](#bib.bib196)], GearNet [[15](#bib.bib15)], ProtST [[136](#bib.bib136)],
    etc. These results are presented in Table [5](#S5.T5 "Table 5 ‣ 5.4 Assessment
    of Pre-training Methods for Protein Representation Learning ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey") and Table [6](#S5.T6 "Table 6 ‣ 5.4 Assessment of Pre-training Methods
    for Protein Representation Learning ‣ 5 Deep Learning based Protein Models ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the protein multimodal modeling methods, such as GearNet [[15](#bib.bib15)],
    SaProt [[207](#bib.bib207)], and ESM-GearNet-INR-MC [[212](#bib.bib212)], consistently
    deliver superior results across various tasks, showcasing the effectiveness of
    leveraging both sequence and structure information. This highlights the effectiveness
    of integrating both sequence and structure information in protein modeling. Notably,
    methods with a higher number of trainable parameters, such as ESM-1b and ProtBERT-BFD,
    exhibit competitive performance, emphasizing the significance of model complexity
    in specific scenarios. These observations underscore the pivotal role played by
    the pre-training dataset and model architecture choices in attaining superior
    performance across various facets of protein modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Accuracy ($\%$) of fold classification and enzyme reaction classification.
    The best results are shown in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method Input Param. Pre-training Fold Classification Enzyme Dataset (Used)
    Fold SuperFamily Family Reaction ESM-1b [[14](#bib.bib14)] Seq 650M UniRef50 (24M)
    26.8 60.1 97.8 83.1 ProtBert-BFD [[12](#bib.bib12)] Seq 420M BFD (2.1B) 26.6 55.8
    97.6 72.2 IEConv [[186](#bib.bib186)] Struct 36.6M PDB (180K) 50.3 80.6 99.7 88.1
    DeepFRI [[197](#bib.bib197)] Seq, Struct 6.2M Pfam (10M) 15.3 20.6 73.2 63.3 GearNet
    (Multiview Contras) [[15](#bib.bib15)] Seq, Struct 42M AlphaFoldDB (805K) 54.1
    80.5 99.9 87.5 GearNet (Residue Type Prediction) [[15](#bib.bib15)] Seq, Struct
    42M AlphaFoldDB (805K) 48.8 71.0 99.4 86.6 GearNet (Distance Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 50.9 73.5 99.4 87.5 GearNet (Angle Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 56.5 76.3 99.6 86.8 GearNet (Dihedral Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 51.8 77.8 99.6 87.0 • Seq: sequence, Struct:
    Structure, Func: Function. Param., means the number of trainable parameters (B:
    billion; M: million; K: thousand). The dataset used here does not exceed the reported
    size as shown in Table [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: $\mathrm{F}_{\mathrm{max}}$ [[15](#bib.bib15)] of GO term prediction
    and EC number prediction. The best results are shown in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Method Input Param. Pre-training GO EC Dataset (Used) BP MF CC ESM-1b [[14](#bib.bib14)]
    Seq 650M UniRef50 (24M) 0.470 0.657 0.488 0.864 ProtBERT-BFD [[12](#bib.bib12)]
    Seq 420M BFD (2.1B) 0.279 0.456 0.408 0.838 ESM-2 [[13](#bib.bib13)] Seq 650M
    UniRef50 (24M) 0.472 0.662 0.472 0.874 IEConv [[186](#bib.bib186)] Struct 36.6M
    PDB (180K) 0.468 0.661 0.516 - DeepFRI [[197](#bib.bib197)] Seq, Struct 6.2M Pfam
    (10M) 0.399 0.465 0.460 0.631 LM-GVP [[196](#bib.bib196)] Seq, Struct 420M UniRef100
    (216M) 0.417 0.545 0.527 0.664 GearNet (Multiview Contrast) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 0.490 0.654 0.488 0.874 GearNet (Residue Type
    Prediction) [[15](#bib.bib15)] Seq, Struct 42M AlphaFoldDB (805K) 0.430 0.604
    0.465 0.843 GearNet (Distance Prediction) [[15](#bib.bib15)] Seq, Struct 42M AlphaFoldDB
    (805K) 0.448 0.616 0.464 0.839 GearNet (Angle Prediction) [[15](#bib.bib15)] Seq,
    Struct 42M AlphaFoldDB (805K) 0.458 0.625 0.473 0.853 GearNet (Dihedral Prediction) [[15](#bib.bib15)]
    Seq, Struct 42M AlphaFoldDB (805K) 0.458 0.626 0.465 0.859 ESM-GearNet [[205](#bib.bib205)]
    Seq, Struct 692M AlphaFoldDB (805K) 0.488 0.681 0.464 0.890 SaProt [[207](#bib.bib207)]
    Seq, Struct 650M AlphaFoldDB (805K) 0.356 0.678 0.414 0.884 KeAP [[211](#bib.bib211)]
    Seq, Func 520M ProteinKG25 (5M) 0.466 0.659 0.470 0.845 ProtST-ESM-1b [[136](#bib.bib136)]
    Seq, Func 759M ProtDescribe (553K) 0.480 0.661 0.488 0.878 ProtST-ESM-2 [[136](#bib.bib136)]
    Seq, Func 759M ProtDescribe (553K) 0.482 0.668 0.487 0.878 ESM-GearNet-INR-MC [[212](#bib.bib212)]
    Seq, Struct, Surface - Swiss-Prot 0.518 0.683 0.504 0.896 • Seq: sequence, Struct:
    Structure, Func: Function. Param., means the number of trainable parameters (B:
    billion; M: million; K: thousand). The dataset used here does not exceed the reported
    size as shown in Table [4](#S5.T4 "Table 4 ‣ Equivariant Geometries ‣ 5.2 Protein
    Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances of Deep Learning
    in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Pretext Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In pre-training, models are exposed to a large amount of unlabeled data to
    learn general representations before being fine-tuned on specific downstream tasks.
    The pretext task is typically designed to encourage the model to learn useful
    features or patterns from the input data. Thus, the pretext task refers to a specific
    objective or task that a machine learning model is trained on as part of a pre-training
    phase. We have listed the pretext tasks in the Table [1](#S5.T1 "Table 1 ‣ Single
    Sequences ‣ 5.1 Protein Language Models ‣ 5 Deep Learning based Protein Models
    ‣ Advances of Deep Learning in Protein Science: A Comprehensive Survey")-Table [3](#S5.T3
    "Table 3 ‣ Equivariant Geometries ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning
    based Protein Models ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Self-supervised Pretext Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The self-supervised pretext tasks leverage the available training data as supervision
    signals, eliminating the requirement for additional annotations [[234](#bib.bib234)].
  prefs: []
  type: TYPE_NORMAL
- en: Predictive Pretext Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The goal of predictive methods is to generate informative labels directly from
    the data itself, which are then utilized as supervision to establish and manage
    the relationships between data and their corresponding labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have stated in Subsection [3.5](#S3.SS5 "3.5 Language Models ‣ 3 Basic
    Neural Networks in Protein Modeling ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"), GPT is developed to enhance the performance of autoregressive
    language modeling in the pre-training phase. Formally, given an unsupervised corpus
    of tokens $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$, GPT employs
    a standard language modeling objective to maximize the likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{next}(\mathcal{X})=\sum_{i=1}^{n+1}\log P\left(x_{i}\mid
    x_{i-k},\ldots,x_{i-1};\Theta_{P}\right)$ |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: here, $k$ represents the size of the context window, and the conditional probability
    $P$ is modeled using a network decoder with parameters $\Theta_{P}$. The next
    token prediction is a fundamental aspect of autoregressive language modeling.
    The model learns to generate coherent and meaningful sequences by estimating the
    probability distribution over the vocabulary and selecting the most likely next
    token.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the masked language modeling in BERT, formally, given a corpus of tokens
    $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$, BERT maximizes
    the likelihood as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{masked}(\mathcal{X})=\textstyle\sum_{x\in\text{mask}(\mathcal{X})}\log
    P(x\mid\tilde{\mathcal{X}};\Theta_{P})$ |  | (22) |'
  prefs: []
  type: TYPE_TB
- en: $\text{mask}(\mathcal{X})$ represents the masked tokens, $\tilde{\mathcal{X}}$
    is the result obtained after masking certain tokens in $\mathcal{X}$, and the
    probability $P$ is modeled by the transformer encoder with parameters $\Theta_{P}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bidirectional language modeling is to model the probability of a token based
    on both the preceding and following tokens. Formally, given a corpus of tokens
    $\mathcal{X}=\left\{x_{0},x_{1},\ldots,x_{n},x_{n+1}\right\}$, $k$ is the size
    of the context window, and $x_{i}$ denotes the $i$-th token in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{bi}(\mathcal{X})=\sum_{i=1}^{n+1}\left[\log P(x_{i}\mid
    x_{i-k},\ldots,x_{i-1};\Theta_{P})+\log P(x_{i}\mid x_{i+1},\ldots,x_{i+k};\Theta_{P})\right]$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/52973e64d763130ef64fff080ffc676c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Diagram of language modeling approaches, including the autoregressive,
    masked and bidirectional language modeling strategies [[46](#bib.bib46)].'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of predicting the likelihood of an individual amino acid, PMLM [[159](#bib.bib159)]
    suggests modeling the likelihood of a pair of masked amino acids. Besides, there
    are span masking and sequential masking strategies used in the training of OmegaPLM [[164](#bib.bib164)].
    For the span masking, the span length is sampled from Poisson distribution and
    clipped at 5 and 8, the protein sequence is masked consecutively according to
    the span length. For the sequential masking, the protein sequence is masked in
    either the first half or the second half. Moreover, xTrimoPGLM [[170](#bib.bib170)]
    employs two types of pre-training objectives, each with its specific indicator
    tokens, to ensure both understanding and generative capacities. One is predicting
    randomly masked tokens, and the other is to predict span tokens, i.e., recovering
    short spans in an autoregressive manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the masked language modeling methods, which involve masking and
    predicting tokens, there are also predictive pretext tasks designed for GNNs in
    the context of protein analysis. In these tasks, the pseudo labels are derived
    from protein features. For example, Chen et al. [[190](#bib.bib190)] employ a
    GNN model that takes the masked protein structure as input and aims to reconstruct
    the pairwise residue distances and dihedral angles. These masked protein features
    commonly include $\mathrm{C}_{\alpha}$ distances, backbone dihedral angles, and
    residue types, as indicated in Table [3](#S5.T3 "Table 3 ‣ Equivariant Geometries
    ‣ 5.2 Protein Structure Models ‣ 5 Deep Learning based Protein Models ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey") [[15](#bib.bib15),
    [190](#bib.bib190)].'
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive Pretext Task
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The contrastive pretext task involves training a model to learn meaningful
    representations by contrasting similar and dissimilar examples. Hermosilla et
    al. [[189](#bib.bib189)] propose contrastive learning for protein structures,
    addressing the challenge of limited annotated datasets. Multiview contrastive
    learning aims to learn meaningful representations from multiple views of the same
    data. For instance, GearNet [[15](#bib.bib15)] aligns representations from different
    views of the same protein while minimizing similarity with representations from
    different proteins (Multiview Contrast). Diverse views of a protein are generated
    using various data augmentation strategies, such as sequence cropping and random
    edge masking. In terms of momentum contrast in GraSR [[200](#bib.bib200)], there
    are two GNN-based encoders, denoted as $f_{q}$ and $f_{k}$, which take the query
    protein and key protein as inputs, respectively. These proteins have similar structures
    and are considered positive pairs. $f_{q}$ and $f_{k}$ share the same architecture
    but have different parameter sets ($\theta_{q}$ and $\theta_{k}$). $\theta_{q}$
    is updated through back-propagation, while $\theta_{k}$ is updated using the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta_{k}\leftarrow m\theta_{k}+(1-m)\theta_{q}$ |  | (24) |'
  prefs: []
  type: TYPE_TB
- en: here, $m\in(0,1]$ is a momentum coefficient. The ProteinINR [[212](#bib.bib212)]
    employs a continual pre-training approach [[201](#bib.bib201)] to effectively
    model the embeddings across various modalities. Specifically, sequence data undergo
    initial pre-training using a sequence encoder, with the resultant encodings serving
    as inputs for the subsequent structure encoder phase. This structure encoder is
    then pre-trained on surface data, utilizing the weights from this phase as the
    foundational weights for further pre-training focused on structural details. The
    process culminates in the structure encoder undergoing additional pre-training
    on the architectural aspects through a multi-view contrastive learning method.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Supervised Pretext Task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: During the pre-training phase, a supervised pretext task is commonly employed
    to provide auxiliary information and guide the model in learning enhanced representations.
    Min et al. [[162](#bib.bib162)] introduces a protein-specific pretext task called
    Same-Family Prediction, where the model is trained to predict whether a given
    pair of proteins belongs to the same protein family. This task helps the model
    learn meaningful protein representations. In addition, Bepler and Berger [[46](#bib.bib46)]
    utilize a masked language modeling objective, denoted as $\mathcal{L}_{masked}$,
    for training on sequences. They predict intra-residue contacts by employing a
    bilinear projection of the sequence embeddings, which is measured by the negative
    log-likelihood of the true contacts. They also introduce a structural similarity
    prediction loss to incorporate structural information into the LM. However, Wu
    et al. [[235](#bib.bib235)] argue that not all external knowledge is beneficial
    for downstream tasks. It is crucial to carefully select appropriate tasks to avoid
    task interference, especially when dealing with diverse tasks. Task interference
    is a common problem that needs to be considered [[115](#bib.bib115)].
  prefs: []
  type: TYPE_NORMAL
- en: 7 Downstream Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the field of protein analysis, there are several downstream tasks that aim
    to extract valuable information and insights from protein data. These tasks play
    a crucial role in understanding protein structure, function, interactions, and
    their implications in various biological processes, including PSP, protein property
    prediction, and protein engineering and design, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Protein Structure Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structural features can be categorized into 1D and 2D representations. The 1D
    features encompass various aspects such as secondary structure, solvent accessibility,
    torsion angles, contact density, and more. On the other hand, the 2D features
    include contact and distance maps. For example, RaptorX-Contact [[236](#bib.bib236)]
    integrates sequence and evolutionary coupling information using two deep residual
    neural networks to predict contact maps. This approach significantly enhances
    contact prediction performance. Other than the contact and distance maps, Yang
    et al. [[120](#bib.bib120)], Li and Xu [[237](#bib.bib237)] focus on studying
    inter-atom distance and inter-residue orientation using a ResNet network. They
    utilize the predicted means and deviations to construct 3D structure models, leveraging
    the constraints provided by PyRosetta.
  prefs: []
  type: TYPE_NORMAL
- en: While it is true that the prediction of structural features may not have significant
    practical value in the presence of highly accurate 3D structure data from AF2\.
    It still holds relevance and utility in several aspects. From one perspective,
    predicting structural features can serve as a reference for evaluating and comparing
    the results of various proposed methods [[238](#bib.bib238)]. Furthermore, these
    predictions can contribute to the exploration of relationships between protein
    sequence, structure, and function by uncovering additional protein grammars and
    patterns. For instance, DeepHomo [[239](#bib.bib239)] focuses on predicting inter-protein
    residue-residue contacts across homo-oligomeric protein interfaces. By integrating
    multiple sources of information and removing potential noise from intra-protein
    contacts, DeepHomo aims to provide insights into the complex interactions within
    protein complexes. Another example is Geoformer [[164](#bib.bib164)], which refines
    contact prediction to address the issue of triangular inconsistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of the highly accurate model, AF2 [[16](#bib.bib16)] has significantly
    influenced the development of end-to-end models for PSP. As depicted in Figure [19](#S7.F19
    "Figure 19 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), AF2 consists of
    an encoder and decoder. The core module of AF2 is Evoformer (encoder), which utilizes
    a variant of axial attention, including row-wise gated self-attention and column-wise
    gated self-attention, to process the MSA representation. To ensure consistency
    in the embedding space, triangle multiplicative update, and triangle self-attention
    blocks are designed. The former combines information within each triangle of graph
    edges, while the latter operates self-attention around graph nodes. The structure
    module (decoder) consists of eight layers with shared weights. It takes the pair
    and first row MSA representations from the Evoformer as input. Each layer updates
    the single representation and the backbone frames using Euclidean transforms.
    The structure module includes the Invariant Point Attention (IPA) module, a form
    of attention that acts on a set of frames and is invariant under global Euclidean
    transformation. Another notable PSP method, RoseTTAFold [[240](#bib.bib240)] is
    a three-track model with attention layers that facilitate information flow at
    the 1D, 2D, and 3D levels. It comprises seven modules. The MSA features are processed
    by attention over rows and columns, and the resulting features are aggregated
    using the outer product to update pair features, which are further refined via
    axial attention. The MSA features are also updated based on attention maps derived
    from pair features, which exhibit good agreement with the true contact maps. A
    fully connected graph, built using the learned MSA and pair features as node and
    edge embeddings, is employed with a graph transformer to estimate the initial
    3D structure. New attention maps derived from the current structure are used to
    update MSA features. Finally, the 3D coordinates are refined by SE(3)-Transformer [[129](#bib.bib129)]
    based on the updated MSA and pair features.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ca4da1e63c9deeee599bf6904be562f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Comparison of schematic architectures of AF2 and single sequence
    structure prediction methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, AF2 and RosettaFold may face challenges when predicting structures
    for proteins with low or no evolutionary information, such as orphan proteins [[241](#bib.bib241)].
    In such cases, OmegaFold [[164](#bib.bib164)] utilizes a LM to obtain the residue
    and pairwise embeddings from a single sequence. These embeddings are then fed
    into Geoformer for accurate predictions on orphan proteins. ESMFold [[13](#bib.bib13)]
    tackles this issue by training large-scale protein LMs (ESM-2) to learn more structural
    and functional properties, replacing the need for explicit MSAs, as shown in Figure [19](#S7.F19
    "Figure 19 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey"), which can reduce
    training resources and time. Thus, ESMFold achieves inference speeds 60 times
    faster than AF2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The remarkable success in PSP has led to research in various related areas,
    including protein-peptide binders identification [[242](#bib.bib242)], antibody
    structure prediction [[243](#bib.bib243)], protein complex structure prediction [[244](#bib.bib244),
    [245](#bib.bib245)], RNA structure prediction [[246](#bib.bib246)], and protein
    conformation prediction [[247](#bib.bib247)], etc. Representative methods for
    PSP are summarized in Figure [20](#S7.F20 "Figure 20 ‣ 7.1 Protein Structure Prediction
    ‣ 7 Downstream Tasks ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '{forest}'
  prefs: []
  type: TYPE_NORMAL
- en: for tree= grow=east, reversed=true, anchor=base west, parent anchor=east, child
    anchor=west, base=middle, font=, rectangle, draw=black, rounded corners,align=left,
    minimum width=2.5em, minimum height=1.2em, s sep=6pt, inner xsep=3pt, inner ysep=1pt,
    , where level=1text width=4.5em, where level=2text width=6em,font=, where level=3font=,
    where level=4font=, where level=5font=, [Application, edge, [Structure Prediction,text
    width=7em, edge, [Protein, text width=5.8em, edge, [ [RGN](https://github.com/aqlaboratory/rgn) [[248](#bib.bib248)],
    [AF2](https://github.com/deepmind/alphafold) [[16](#bib.bib16)], [RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold) [[240](#bib.bib240)],
    [EigenFold](https://github.com/bjing2016/EigenFold) [[262](#bib.bib262)],
  prefs: []
  type: TYPE_NORMAL
- en: '[ESMFold](https://github.com/facebookresearch/esm) [[13](#bib.bib13)], [OmegaFold](https://github.com/HeliXonProtein/OmegaFold) [[164](#bib.bib164)],
    DMPfold2 [[263](#bib.bib263)], RFAA [[264](#bib.bib264)] ,text width=22.2em, edge]
    ] [Antibody, text width=5.8em, edge [ EquiFold [[255](#bib.bib255)], [IgFold](https://github.com/Graylab/IgFold) [[243](#bib.bib243)]
    ,text width=22.2em, edge] ] [Complex, text width=5.8em, edge [[AlphaFold-Multimer](https://github.com/deepmind/alphafold) [[244](#bib.bib244)],
    ColAttn [[257](#bib.bib257)], GAPN [[258](#bib.bib258)],'
  prefs: []
  type: TYPE_NORMAL
- en: '[RoseTTAFoldNA](https://github.com/uw-ipd/RoseTTAFold2NA) [[259](#bib.bib259)],
    BiDock [[286](#bib.bib286)], PromptMSP [[287](#bib.bib287)] ,text width=22.2em,
    edge] ] [RNA, text width=5.8em, edge [ [E2Efold-3D](https://github.com/RFOLD/RhoFold) [[246](#bib.bib246)]
    ,text width=22.2em, edge] ] [Conformation, text width=5.8em, edge [ [SPEACH_AF](https://github.com/RSvan/SPEACH_AF) [[247](#bib.bib247)],
    [Atom Transformer](https://github.com/facebookresearch/protein-ebm) [[249](#bib.bib249)],'
  prefs: []
  type: TYPE_NORMAL
- en: MultiSFold [[250](#bib.bib250)], DiffMD [[251](#bib.bib251)], Str2Str [[252](#bib.bib252)]
    ,text width=22.2em, edge] ] [Folding Path, text width=5.8em, edge [ PAthreader [[253](#bib.bib253)],
    Pathfinder [[254](#bib.bib254)] ,text width=22.2em, edge] ] [Refinement, text
    width=5.8em, edge [ EquiFold [[255](#bib.bib255)], DeepACCNet [[256](#bib.bib256)],
    GNNRefine [[185](#bib.bib185)],
  prefs: []
  type: TYPE_NORMAL
- en: ATOMRefine [[193](#bib.bib193)] ,text width=22.2em, edge] ] [Assessment, text
    width=5.8em, edge [ QDistance [[122](#bib.bib122)], DeepUMQA [[124](#bib.bib124)]
    ,text width=22.2em, edge] ] ] [Design,text width=7em, edge, [MSA Generation, text
    width=5.8em, edge [ EvoGen [[260](#bib.bib260)], [MSA-Augmenter](https://github.com/lezhang7/MSA-Augmentor) [[261](#bib.bib261)],
    [PoET](https://github.com/OpenProteinAI/PoET) [[281](#bib.bib281)] ,text width=22.2em,
    edge] ] [Protein Design, text width=5.8em, edge, [ [PiFold](https://github.com/A4Bio/PiFold) [[118](#bib.bib118)],
    [PROTSEED](https://github.com/sokrypton/ColabDesign) [[270](#bib.bib270)], LM-DESIGN [[271](#bib.bib271)],
    [Genie](https://github.com/aqlaboratory/genie) [[272](#bib.bib272)],
  prefs: []
  type: TYPE_NORMAL
- en: '[IsEM-Pro](https://github.com/JocelynSong/IsEM-Pro) [[273](#bib.bib273)], [FrameDiff](https://github.com/jasonkyuyim/se3_diffusion) [[274](#bib.bib274)],
    [GraDe_IF](https://github.com/ykiiiiii/GraDe_IF) [[282](#bib.bib282)], NOS [[289](#bib.bib289)]
    ,text width=22.2em, edge] ] [Antibody Design, text width=5.8em, edge, [ [dyMEAN](https://github.com/THUNLP-MT/dyMEAN) [[275](#bib.bib275)],
    [AbODE](https://github.com/yogeshverma1998/AbODE_) [[276](#bib.bib276)], AntiDesigner [[277](#bib.bib277)],
    HTP [[278](#bib.bib278)] ,text width=22.2em, edge] ] [DNA/RNA, text width=5.8em,
    edge, [ [BIB](https://github.com/GGchen1997/BIB-ICML2023-Submission) [[279](#bib.bib279)],
    RDesign [[280](#bib.bib280)] ,text width=22.2em, edge] ] [Protein Pocket, text
    width=5.8em, edge, [ [FAIR](https://github.com/zaixizhang/FAIR?tab=readme-ov-file) [[288](#bib.bib288)],
    RFdiffusionAA [[264](#bib.bib264)] ,text width=22.2em, edge] ] ] [Property Prediction,text
    width=7em, edge, [Substructure, text width=5.8em, edge, [ RaptorX-Contact [[236](#bib.bib236)],
    DeepDist [[266](#bib.bib266)] ,text width=22.2em, edge] ] [Function, text width=5.8em,
    edge, [ RaptorX-Property [[265](#bib.bib265)], Domain-PFP [[230](#bib.bib230)],
    Struct2GO [[182](#bib.bib182)] ,text width=22.2em, edge] ] [Mutation Effect, text
    width=5.8em, edge, [ [HotProtein](https://github.com/VITA-Group/HotProtein) [[267](#bib.bib267)],
    SidechainDiff [[268](#bib.bib268)], [RDE](https://github.com/luost26/RDE-PPI) [[269](#bib.bib269)],'
  prefs: []
  type: TYPE_NORMAL
- en: '[Mutate Everything](https://github.com/jozhang97/MutateEverything) [[290](#bib.bib290)],
    Tranception [[299](#bib.bib299)], PPIRef [[300](#bib.bib300)] ,text width=22.2em,
    edge] ] [Interaction, text width=5.8em, edge, [ [FABind](https://github.com/qizhipei/fabind) [[283](#bib.bib283)],
    NERE DSM [[284](#bib.bib284)], MAPE-PPI [[285](#bib.bib285)] ,text width=22.2em,
    edge] ] ] ]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 20: Taxonomy of representative methods for different protein applications.
    The model name is linked with the official GitHub or server page.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Protein Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Protein design is a field of research that focuses on engineering and creating
    novel proteins with specific structures and functions. It involves modifying existing
    proteins or designing entirely new ones to perform desired tasks, such as enzyme
    catalysis, drug binding, molecular sensing, or protein-based materials. In recent
    years, significant advancements have been made in the field of protein design
    through the application of deep learning techniques. We divide the major works
    into three catogories.
  prefs: []
  type: TYPE_NORMAL
- en: The first approach involves pre-training a model on a large dataset of sequences.
    The pre-trained model can be utilized to generate novel homologous sequences sharing
    sequence features with that protein family, denoted as MSA generation, which is
    used to enhance the model’s ability to predict structures [[260](#bib.bib260)].
    ProGen [[208](#bib.bib208)] is trained on sequences conditioned on a set of protein
    properties, like function or affiliation with a particular organism, to generate
    desired sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another crucial problem is to design protein sequences that fold into desired
    structures, namely structure-based protein design [[118](#bib.bib118), [117](#bib.bib117),
    [128](#bib.bib128), [271](#bib.bib271), [272](#bib.bib272), [282](#bib.bib282),
    [291](#bib.bib291), [292](#bib.bib292), [293](#bib.bib293), [294](#bib.bib294),
    [295](#bib.bib295), [296](#bib.bib296)]. Formally, it is to find the amino acid
    sequence $\mathcal{S}=\{s_{i}\}_{i=1,\ldots,n}$ can fold into the desired structure
    $\mathcal{P}=\{P_{i}\}_{i=1,\ldots,n}$, where $P_{i}\in\mathbb{R}^{1\times 3}$,
    $n$ is the number of residues and the natural proteins are composed by 20 types
    of amino acids. It is to learn a deep network having a function $f_{\theta}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{F}_{\theta}:\mathcal{P}\mapsto\mathcal{S}$ |  | (25) |'
  prefs: []
  type: TYPE_TB
- en: 'There are also some works [[291](#bib.bib291)] combining the 3D structural
    encoder and 1D sequence decoder, where the protein sequences are generated in
    an autoregressive way:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p(S\mid\mathcal{P};\theta)=\prod_{i=1}^{n}p\left(s_{i}\mid s_{<i},\mathcal{P};\theta\right)$
    |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: Thirdly, generating a novel protein satisfying specified structural or functional
    properties is the task of de novo protein design [[273](#bib.bib273), [274](#bib.bib274),
    [297](#bib.bib297), [298](#bib.bib298)]. For example, in order to design protein
    sequences with desired biological function, such as high fitness, Song et al. [[273](#bib.bib273)]
    develop a Monte Carlo Expectation-Maximization method to learn a latent generative
    model, augmented by combinatorial structure features from a separate learned Markov
    random fields. PROTSEED [[270](#bib.bib270)] learns the joint generation of residue
    types and 3D conformations of a protein with $n$ residues based on context features
    as input to encourage designed proteins to have desired structural properties.
    These context features can be secondary structure annotations, binary contact
    features between residues, etc.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to protein sequence and structure design, some research focuses
    on designing antibody and DNA sequences [[275](#bib.bib275), [279](#bib.bib279)].
    A critical yet challenging task in this domain is the design of the protein pocket,
    the cavity region of the protein where the ligand binds [[288](#bib.bib288)].
    The pocket is essential for molecular recognition and plays a key role in protein
    function. Effective pocket design can enable control over selectivity and affinity
    towards desired ligands.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Protein Property Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Protein property prediction aims to predict various properties and characteristics
    of proteins, such as solvent accessibility, functions, subcellular localization,
    fitness, etc. The structural properties, like secondary structure and contact
    maps, are useful in other tasks [[237](#bib.bib237)]. For the function prediction,
    there is a group of PRL methods enabling inference about biochemical, cellular,
    systemic or phenotypic functions [[15](#bib.bib15), [59](#bib.bib59)]. Many proteins
    contain intrinsic fluorescent amino acids like tryptophan and tyrosine. Mutations
    can alter the microenvironment of these residues and change the emitted fluorescence
    upon excitation [[145](#bib.bib145)]. Stability landscape prediction estimates
    the impact of mutations on the overall thermodynamic stability of protein structures.
    Stability is quantified by $\Delta\mathrm{G}$, the Gibbs free energy change between
    folded and unfolded states. Mutations disrupting key interactions can undesirably
    destabilize the native state [[145](#bib.bib145), [302](#bib.bib302), [299](#bib.bib299),
    [301](#bib.bib301)], leading to changes in protein properties. Thus, there are
    models are developed to evaluate the mutational effects on PPI identifications [[268](#bib.bib268),
    [269](#bib.bib269), [303](#bib.bib303)]. The protein fitness landscape refers
    to the mapping between genotype (e.g., the amino acid sequence) and phenotype
    (e.g., protein function), which is a fairly broad concept. Models that learn the
    protein fitness landscape are expected to be effective at predicting the effects
    of mutations [[303](#bib.bib303)].
  prefs: []
  type: TYPE_NORMAL
- en: '8 Discussion: Insights and Future Outlooks'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on a comprehensive review of fundamental deep learning techniques, protein
    fundamentals, protein model architectures, pretext tasks, and downstream applications,
    we aim to provide deeper perspectives into protein models.
  prefs: []
  type: TYPE_NORMAL
- en: Towards a Generalizable Large-Scale Biological Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While breakthroughs like ChatGPT [[90](#bib.bib90)] have demonstrated remarkable
    success across various domains, there is still a need to develop large-scale models
    tailored to biological data encompassing proteins, DNA, RNA, antibodies, enzymes,
    and more in order to address existing challenges. Roney and Ovchinnikov [[304](#bib.bib304)]
    find that AlphaFold has learned an accurate biophysical energy function and can
    identify low-energy conformations using co-evolutionary information [[305](#bib.bib305)].
    However, some studies have indicated AlphaFold may not reliably predict mutation
    impacts on proteins [[306](#bib.bib306), [307](#bib.bib307), [308](#bib.bib308)].
    Performance on structure-based tasks doesn’t directly transfer to other predictive
    problems[[302](#bib.bib302), [166](#bib.bib166)]. Data-driven methods have attracted
    more attention, but there is no singular optimal model architecture or pretext
    task that generalizes across all data types and downstream applications. Continued
    efforts are imperative to develop versatile, scalable, and interpretable models
    integrating both physical and data sciences for comprehensively tackling biomolecular
    modeling and design across contexts [[92](#bib.bib92)].
  prefs: []
  type: TYPE_NORMAL
- en: Case by Case
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With ever-expanding protein sequence databases now containing millions of entries,
    protein models have witnessed a commensurate growth in scale, with parameter counts
    in the billions (e.g., ESM-2 [[13](#bib.bib13)] and xTrimoPGLM [[170](#bib.bib170)]).
    However, training such enormous deep learning models currently remains accessible
    only to large corporations with vast computational resources. For instance, DeepMind
    utilized 128 TPU v3 cores over one week to train AF2\. The requirements pose a
    challenge for academic research groups to learn protein representations from scratch
    and also raise environmental sustainability concerns. Given these constraints,
    increased focus on targeted, problem-centric formulations may be prudent. The
    choice of appropriate model architecture and self-supervised learning scheme should
    match dataset attributes and application objectives. This demands careful scrutiny
    of design choices dependent on available inputs and intended predictive utility.
    Furthermore, the vast potential of large pre-trained models remains underexplored
    from the lens of effectively utilizing them under specific problem contexts with
    the integration of prior knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding Multimodal Representation Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'ESM-2 [[13](#bib.bib13)] analysis indicates that model performance saturates
    quickly with increasing size for high evolutionary depth, while gains continue
    at low depths with larger models. This exemplifies that appropriately incorporating
    additional modalities like biological and physical priors, MSAs, etc., can reduce
    model scale and improve metrics. As evident in Table [6](#S5.T6 "Table 6 ‣ 5.4
    Assessment of Pre-training Methods for Protein Representation Learning ‣ 5 Deep
    Learning based Protein Models ‣ Advances of Deep Learning in Protein Science:
    A Comprehensive Survey"), combining sequence with structure or functional data
    into models like LM-GVP, GearNet and ProtST-ESM-2 confers improvements over ESM-2
    alone. More extensive exploration into multimodal representation learning is imperative [[46](#bib.bib46)].'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Proteins and languages share similarities but also key differences, as discussed
    previously. Most deep learning models currently lack interpretability, obstructing
    insights into underlying protein folding mechanisms. Models like AlphaFold cannot
    furnish detailed characterizations of molecular interactions and chemical principles
    imperative for mechanistic studies and structure-based drug design. Interpretable
    models that reveal grammar-like rules governing proteins would inform impactful
    biomedical applications. Hence, conceptualizing methodologies tailored to the
    nuances of protein data is an urgent priority. Visualization tools that capture
    folding dynamics and functional conformational transitions can powerfully address
    these needs, which would grant researchers the ability to visually traverse the
    atomic trajectories of proteins. Such molecular recordings would waveguide principles
    discovered to be harnessed towards materials and therapeutic innovation.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Utility Across Domains
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As depicted in Figure [20](#S7.F20 "Figure 20 ‣ 7.1 Protein Structure Prediction
    ‣ 7 Downstream Tasks ‣ Advances of Deep Learning in Protein Science: A Comprehensive
    Survey"), researchers have progressed towards tackling more complex challenges
    such as predicting structures from single sequences, modeling complex assemblies,
    elucidating folding mechanisms, and characterizing protein-ligand interactions [[164](#bib.bib164),
    [13](#bib.bib13), [259](#bib.bib259), [283](#bib.bib283)]. Since mutations can
    precipitate genetic diseases, modeling their functional effects provides insights
    into how sequence constraints structure. Thus, accurately predicting robust structures
    and mutation impacts is imperative [[268](#bib.bib268), [269](#bib.bib269)]. Drug
    design represents a promising avenue for expeditious and economical compound discovery.
    Recent years have witnessed innovations in AI-driven methodologies for identifying
    candidate molecules from huge libraries to bind specific pockets [[309](#bib.bib309),
    [310](#bib.bib310), [311](#bib.bib311)]. Going further, generating enhanced out-of-distribution
    sequences with desirably tuned attributes (like stability) remains an engaging
    prospect [[312](#bib.bib312), [313](#bib.bib313)]. For groups equipped with wet-lab
    capabilities, synergistic combinations of computational predictions and experiments
    offer traction for multifarious problems at the interface of deep learning and
    biology.'
  prefs: []
  type: TYPE_NORMAL
- en: Unified Benchmarks and Evaluation Protocols
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amidst an influx of emerging work, comparative assessments are often impeded
    by inconsistencies in datasets, architectures, metrics, and other evaluation factors.
    To enable healthy progress, there is a pressing need to establish standardized
    benchmarking protocols across tasks. Unified frameworks for fair performance analysis
    will consolidate disjoint efforts and clarify model capabilities to distill collective
    progress [[234](#bib.bib234)]. Considering factors like data leakage, bias and
    ethics, some groups develop new datasets tailored to their needs [[314](#bib.bib314)];
    On the other hand, constructing rigorous, reliable and equitable benchmarks remains
    essential for evaluating models and promoting impactful methods. Initiatives like
    TAPE [[145](#bib.bib145)], ProteinGym [[303](#bib.bib303)], ProteinShake [[315](#bib.bib315)],
    PEER [[316](#bib.bib316)], ProteinInvBench [[317](#bib.bib317)], ProteinWorkshop [[318](#bib.bib318)],
    exemplify the critical role of comprehensive benchmarking in furthering innovation.
    Moreover, the Critical Assessment of Protein Structure Prediction (CASP) experiments
    act as a crucial platform for evaluating the most recent advancements in PSP within
    the field, which has been conducted 15 times by mid-December 2022\. Research groups
    from all over the world participate in CASP to objectively test their structure
    prediction methods. By categorizing different themes, like quality assessment,
    domain boundary prediction, and protein complex structure prediction, CASPers
    can identify what progress has been made and highlight the future efforts that
    may be most productively focused on.
  prefs: []
  type: TYPE_NORMAL
- en: Protein Structure Prediction in Post-AF2 Era
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'AF2 marked a significant advancement in protein structure prediction, yet opportunities
    for enhancement persist. Several key strategies can be employed to refine the
    conventional AF2 model. These include diversifying approaches or expanding the
    database to generate more comprehensive MSAs, optimizing template utilization,
    and integrating distances and constraints derived from the AF2 model with alternative
    methods [[27](#bib.bib27)]. Notably, spatial constraints like contact, distance,
    and hydrogen-bond networks have been incorporated into I-TASSER [[319](#bib.bib319)]
    to predict full-length protein structures, achieving superior performance. In
    CASP15, the performance of the top models from server groups closely approached
    and, in some cases, even surpassed that of the human groups. This indicates that
    AI models have reached a stage where they can effectively assimilate and apply
    human knowledge in the field. However, when benchmarked on the human proteome,
    only 36% of residues fall within its highest accuracy tier [[320](#bib.bib320)].
    While approximately 35% of AF2’s predictions rival experimental structures, challenges
    remain in enhancing coverage, and at least 40 teams surpassed AF2 in accuracy
    in CASP15\. Although models completely replacing AF2 have not yet emerged in the
    past two or three years, it has revealed some limitations. For instance, AF2’s
    prediction accuracy on multidomain proteins is not as robust as its accuracy for
    individual domains [[27](#bib.bib27)]. Addressing challenges posed by multidomain
    proteins, including protein complexes, multiple conformational states, and folding
    pathways, may be crucial research directions in the field of PSP, though there
    have appeared works attempting to tackle these problems, as shown in Figure [20](#S7.F20
    "Figure 20 ‣ 7.1 Protein Structure Prediction ‣ 7 Downstream Tasks ‣ Advances
    of Deep Learning in Protein Science: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Boundary of Large Language Models in Proteins
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Large language models (LLMs), e.g., ChatGPT [[90](#bib.bib90)], have prevailed
    in NLP [[55](#bib.bib55), [52](#bib.bib52)]. Demis Hassabis, CEO and co-founder
    of DeepMind, has stated that biology can be thought of as an information processing
    system, albeit an extraordinarily complex and dynamic one, just as mathematics
    turned out to be the right description language for physics, biology may turn
    out to be the perfect type of regime for the application of AI. Like the rules
    of grammar would emerge from training an LLM on language samples, the limitations
    dictated by evolution would arise from training the system on samples of proteins.
    There are essentially two families in LLMs, BERT and GPT, which have different
    training objectives and processing methods, as we have stated above [[168](#bib.bib168)].
    Based on the two base models, different LLMs are proposed in protein, like ProtGPT2 [[167](#bib.bib167)],
    ESM-2 [[13](#bib.bib13)], and ProtChatGPT [[321](#bib.bib321)], etc. Researchers
    have tried a range of LLM sizes and found some intriguing facts, for example,
    the ESM-2 [[13](#bib.bib13)] model can get better results when increasing the
    resources, but it is still not clear when it would max out [[322](#bib.bib322)].
    It would be interesting to explore the boundaries of LLMs in proteins.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper presents a systematic overview of pertinent terminologies, notations,
    network architectures, and protein fundamentals, spanning CNNs, LSTMs, GNNs, LMs,
    physicochemical properties, sequence motifs and structural regions, etc. Connections
    between language and protein domains are elucidated, exemplifying model utility
    across applications. Through a comprehensive literature survey, current protein
    models are reviewed, analyzing architectural designs, self-supervised objectives,
    training data and downstream use cases. Limitations and promising directions are
    discussed, covering aspects like multimodal learning, model interpretability,
    and knowledge integration. Overall, this survey aims to orient machine learning
    and biology researchers towards open challenges for enabling the next generation
    of innovations. By condensing progress and perspectives, we hope to crystallize
    collective headway while charting fruitful trails for advancing protein modeling
    and design, elucidating molecular mechanisms, and translating insights into functional
    applications for the benefit of science and society.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Pleiss J, Fischer M, Peiker M, et al. Lipase engineering database: understanding
    and exploiting sequence–structure–function relationships. Journal of Molecular
    Catalysis B: Enzymatic, 2000, 10: 491-508'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Gromiha M M. Protein bioinformatics: from sequence to function. academic
    press, 2010'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Kryshtafovych A, Schwede B, Topf M, et al. Critical assessment of methods
    of protein structure prediction (CASP)—Round XIII. Proteins: Structure, Function,
    and Bioinformatics, 2019, 87: 1011–1020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Senior A W, Evans R, Jumper J, et al. Improved protein structure prediction
    using potentials from deep learning. Nature, 2020, 577: 706–710'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Ikeya T, Güntert P, Ito Y. Protein structure determination in living cells.
    International Journal of Molecular Sciences, 2019, 20: 2442'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Gauto D F, Estrozi L F, Schwieters C D, et al. Integrated NMR and cryo-EM
    atomic-resolution structure determination of a half-megadalton enzyme complex.
    Nature, 2019, 10: 1-12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Ashburner M, Ball C A, Blake J A, et al. Gene ontology: tool for the unification
    of biology. Nature genetics, 2000, 25: 25-9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Protein Data Bank: the single global archive for 3D macromolecular structure
    data. Nucleic Acids Research, 2019, 47: D520–D528'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] UniProt Consortium. Update on activities at the Universal Protein Resource
    (UniProt) in 2013\. Nucleic Acids Research, 2013, 41: D43-D47'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Zhang N, Bi Z, Liang X, et al. Ontoprotein: Protein pretraining with gene
    ontology embedding. arXiv, January 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Unsal S, Atas H, Albayrak M, et al. Learning functional properties of
    proteins with language models. Nature Machine Intelligence, 2022, 4: 227-45'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Elnaggar A, Heinzinger M, Dallago C, et al. Prottrans: Towards cracking
    the language of lifes code through self-supervised deep learning and high performance
    computing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Lin Z, Akin H, Rao R, et al. Language models of protein sequences at the
    scale of evolution enable accurate structure prediction. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Rives A, Meier J, Sercu T, et al. Biological structure and function emerge
    from scaling unsupervised learning to 250 million protein sequences. In: Proceedings
    of the National Academy of Sciences, 2021\. 118'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Zhang Z, Xu M, Jamasb A, et al. Protein representation learning by geometric
    structure pretraining. arXiv, March 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Jumper J M, Evans R O, Pritzel A, et al. Highly accurate protein structure
    prediction with AlphaFold. Nature, 2021, 596: 583-589'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] AlQuraishi M. End-to-end differentiable learning of protein structure.
    Cell systems, 2019, 292–301'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Susanty M, Rajab T E, Hertadi R. A review of protein structure prediction
    using deep learning. In: BIO Web of Conferences, 2021\. 41'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Ma B, Johnson R. De novo sequencing and homology searching. Molecular
    & Cellular Proteomics, 2012, 11: 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Ma B. Novor: real-time peptide de novo sequencing software. Journal of
    the American Society for Mass Spectrometry, 2015, 26: 1885-1894'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Zheng W, Li Y, Zhang C, et al. Protein structure prediction using deep
    learning distance and hydrogen‐bonding restraints in CASP14\. Proteins: Structure,
    Function, and Bioinformatics, 2021, 89: 1734-1751'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Chowdhury R, Bouatta N, Biswas S, et al. Single-sequence protein structure
    prediction using a language model and deep learning. Nature Biotechnology, 2022,
    40: 1617-1623'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Wu F, Xu J. Deep template-based protein structure prediction. PLoS computational
    biology, 2021, 17(5): e1008954'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Wu F, Jing X, Luo X. et al. Improving protein structure prediction using
    templates and sequence embedding. Bioinformatics, 2023, 39: btac723'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Iuchi H, Matsutani T, Yamada K, et al. Representation learning applications
    in biological sequence analysis. Computational and Structural Biotechnology Journal,
    2021, 19: 3198-3208'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Hu L, Wang X, Huang Y A, et al. A survey on computational models for predicting
    protein–protein interactions. Briefings in bioinformatics, 2021, 22: bbab036'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Peng C X, Liang F, Xia Y H, et al. Recent Advances and Challenges in Protein
    Structure Prediction. Journal of Chemical Information and Modeling, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Ihm Y. A threading approach to protein structure prediction: studies on
    TNF-like molecules, Rev proteins, and protein kinases. Iowa State University,
    2004'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Patel M, Shah H. Protein secondary structure prediction using support
    vector machines (svms). In: International Conference on Machine Intelligence and
    Research Advancement, IEEE, 2013\. 594-598'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Sanger F. The arrangement of amino acids in proteins. Advances in protein
    chemistry, 1952, 7: 1-67'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Hao X H, Zhang G J, Zhou X G. Conformational space sampling method using
    multi-subpopulation differential evolution for de novo protein structure prediction.
    IEEE Transactions on NanoBioscience, 2017, 16: 618-33'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Wang S, Peng J, Ma J, et al. Protein secondary structure prediction using
    deep convolutional neural fields. Scientific reports, 2016, 6: 1-11'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Chou K C, Cai Y D. Predicting protein quaternary structure by pseudo amino
    acid composition. Proteins: Structure, Function, and Bioinformatics, 2003, 53:
    282–289'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Koonin E V. Orthologs, paralogs, and evolutionary genomics. Annual review
    of genetics, 2005, 39: 309–338'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Wang Y, Wu H, Cai Y. A benchmark study of sequence alignment methods for
    protein clustering. BMC bioinformatics, 2018, 19: 95–104'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Ochoa D, Pazos F. Practical aspects of protein co-evolution. Frontiers
    in cell and developmental biology, 2014, 2: 14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Emerson I A, Amala A. Protein contact maps: a binary depiction of protein
    3D structures. Physica A: Statistical Mechanics and its Applications, 2017, 465:
    782-91'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Basile W, Sachenkova O, Light S, et al. High gc content causes orphan
    proteins to be intrinsically disordered. PLoS computational biology, 2017, 13:
    3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Blackstock J C. Guide to Biochemistry. Butterworth-Heinemann, 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Li S, Chua T S, Zhu J, et al. Generative topic embedding: a continuous
    representation of documents. In: Proceedings of the 54th Annual Meeting of the
    Association for Computational Linguistics, 2016\. 666–675'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Lin Z, Feng M, Santos C N, et al. A structured self-attentive sentence
    embedding. arXiv, March 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Weiss K, Khoshgoftaar T M, Wang D. A survey of transfer learning. Journal
    of Big data, 2016, 3: 1-40'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Pan S J, Yang Q. A survey on transfer learning. IEEE Transactions on knowledge
    and data engineering, 2010, 22: 1345-1359'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Bond-Taylor S, Leach A, Long Y, et al. Deep generative modelling: A comparative
    review of VAEs, GANs, normalizing flows, energy-based and autoregressive models.
    arXiv, March 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Jahan M S, Khan H U, Akbar S, et al. Bidirectional Language Modeling:
    A Systematic Literature Review. Scientific Programming, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Bepler T, Berger B. Learning the protein language: Evolution, structure,
    and function. Cell systems, 2021, 12: 654-669'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Gou J, Yu B, Maybank SJ, et al. Knowledge distillation: A survey. International
    Journal of Computer Vision, 2021, 129: 1789-1819'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Skocaj D, Leonardis A, Kruijff GJ. Cross-modal learning. Encyclopedia
    of the Sciences of Learning, 2012, 861-864'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Wang H, Zhang J, Chen Y, et al. Uncertainty-aware multi-modal learning
    via cross-modal random network prediction. arxiv, July 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks.
    In: European conference on computer vision, Springer, Cham, 2016\. 630-645'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. Advances
    in neural information processing systems. 2017, 30'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional
    transformers for language understanding. arxiv, October 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Radford A, Narasimhan K, Salimans T, et al. Improving language understanding
    by generative pre-training. 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask
    learners. OpenAI blog. 2019, 1: 9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners.
    Advances in neural information processing systems, 2020, 33: 1877-1901'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Lindsay, G W. Convolutional neural networks as a model of the visual system:
    Past, present, and future. Journal of cognitive neuroscience, 2021, 33: 2017-2031'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Kriegeskorte N. Deep neural networks: a new framework for modeling biological
    vision and brain information processing. Annual review of vision science, 2015,
    1: 417-446'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Chartrand G, Cheng P M, Vorontsov E, et al. Deep learning: a primer for
    radiologists. Radiographics, 2017, 37: 2113-2131'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Fan H, Wang Z, Yang Y, et al. Continuous-Discrete Convolution for Geometry-Sequence
    Modeling in Proteins. In: The Eleventh International Conference on Learning Representations,
    2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Hayes B. First links in the Markov chain. American Scientist, 2013, 101:
    252'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Li H. Language models: past, present, and future. Communications of the
    ACM, 2022, 65: 56-63'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Bishop M, Thompson E A. Maximum likelihood alignment of dna sequences.
    Journal of molecular biology, 1986, 190: 159–165'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Chiu J T, Rush A M. Scaling hidden markov language models. arxiv, November
    2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Stigler J, Ziegler F, Gieseke A, et al. The complex folding network of
    single calmodulin molecules. Science, 2011, 334: 512–516'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Wong K C, Chan T M, Peng C, et al. DNA motif elucidation using belief
    propagation. Nucleic Acids Research, 2013, 41: e153–e153'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Ferruz N, Höcker B, Controllable protein design with language models.
    Nature Machine Intelligence, 2022, 1-12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] AGMLS F, Bunke R, Schmiduber J. A novel connectionist system for improved
    unconstrained handwriting recognition. IEEE Transactions on Pattern Analysis Machine
    Intelligence, 2009, 31: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] De S, Smith S L, Fernando A, et al. Griffin: Mixing Gated Linear Recurrences
    with Local Attention for Efficient Language Models. arxiv, February 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Hochreiter S. Untersuchungen zu dynamischen neuronalen netzen. Diploma,
    Technische Universität München, 1991, 91: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Lample G, Ballesteros M, Subramanian S, et al. Neural architectures for
    named entity recognition. arxiv, March 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Schmidhuber J, Wierstra D, Gomez F J. Evolino: Hybrid neuroevolution/optimal
    linear search for sequence prediction. In: Proceedings of the 19th International
    Joint Conferenceon Artificial Intelligence (IJCAI), 2005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Hochreiter S, Heusel M, Obermayer K. Fast model-based protein homology
    detection without alignment. Bioinformatics, 2007, 23: 1728–1736'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Gupta A, Müller A T, Huisman B J, et al. Generative recurrent networks
    for de novo drug design. Molecular informatics, 2018, 37: 1700111'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Ma C, Dai G, Zhou J. Short-term traffic flow prediction for urban road
    sections based on time series analysis and LSTM_BILSTM method. IEEE Transactions
    on Intelligent Transportation Systems, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Radford A, Jozefowicz R, Sutskever I. Learning to generate reviews and
    discovering sentiment. arxiv, April 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning
    to align and translate. arxiv, September 2014'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Han X, Zhang Z, Ding N, et al. Pre-trained models: Past, present and future.
    AI Open, 2021, 2: 225-50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Tan Q, Liu N, Hu X. Deep representation learning for social network analysis.
    Frontiers in big Data, 2019, 2: 2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Wu S, Sun F, Zhang W, et al. Graph neural networks in recommender systems:
    a survey. ACM Computing Surveys, 2022, 55: 1-37'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Deng J, Yang Z, Ojima I, et al. Artificial intelligence in drug discovery:
    applications and techniques. Briefings in Bioinformatics, 2022, 23: bbab430'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Fensel D, Şimşek U, Angele K, et al. Introduction: what is a knowledge
    graph?. Knowledge Graphs, Springer, Cham, 2020, 1-10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Kipf T N, Welling M. Semi-supervised classification with graph convolutional
    networks. arxiv, September 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Veličković P, Cucurull G, Casanova A, et al. Graph attention networks.
    arxiv, October 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Oh J, Cho K, Bruna J. Advancing graphsage with a data-driven node sampling.
    arxiv, April 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Thrun S, Pratt L. Learning to learn: Introduction and overview. In: Learning
    to learn, 1998\. 3-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining
    approach. arxiv, July 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining
    for language understanding. Advances in neural information processing systems.
    2019, 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Kaplan J, McCandlish S, Henighan T, et al. Scaling laws for neural language
    models. arxiv, January 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Hoffmann J, Borgeaud S, Mensch A, et al. Training Compute-Optimal Large
    Language Models. arxiv, March 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Perlman AM. The Implications of OpenAI’s Assistant for Legal Services
    and Society. Available at SSRN. 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Araci D. Finbert: Financial sentiment analysis with pre-trained language
    models. arxiv, August 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Wang B, Xie Q, Pei J, et al. Pre-trained language models in biomedical
    domain: A systematic survey. arxiv, October 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Bengio Y, Ducharme R, Vincent P. A neural probabilistic language model.
    Advances in neural information processing systems, 2000, 13'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Mikolov T, Chen K, Corrado G S, et al. Efficient estimation of word representations
    in vector space. In: International conference on learning representations, 2013'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Lee J, Yoon W, Kim S, et al. Biobert: a pre-trained biomedical language
    representation model for biomedical text mining. Bioinformatics, 2020, 36: 1234–1240'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Young T, Hazarika D, Poria S, et al. Recent trends in deep learning based
    natural language processing. IEEE Computational intelligenCe magazine, 2018, 13:
    55-75'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Yang K K, Wu Z, Bedbrook C N, et al. Learned protein embeddings for machine
    learning. Bioinformatics, 2018, 34: 2642-2648'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Asgari E, Mofrad M R. Continuous distributed representation of biological
    sequences for deep proteomics and genomics. PloS one, 2015, 10: 11'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Ofer D, Brandes N, Linial M. The language of proteins: NLP, machine learning
    & protein sequences. Computational and Structural Biotechnology Journal, 2021,
    19:1750-1758'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Bryngelson J D, Onuchic J N, Socci N D, et al. Funnels, pathways, and
    the energy landscape of protein folding: a synthesis. Proteins: Structure, Function,
    and Bioinformatics, 1995, 21: 167-95'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Leopold P E, Montal M, Onuchic J N. Protein folding funnels: a kinetic
    approach to the sequence-structure relationship. In: Proceedings of the National
    Academy of Sciences, 1992, 8721–8725'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Zerihun MB, Schug A. Biomolecular Structure Prediction via Coevolutionary
    Analysis: A Guide to the Statistical Framework. In: NIC Symposium, 2018\. FZJ-2018-02966'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Vorberg S. Bayesian statistical approach for protein residue-residue
    contact prediction. Doctoral dissertation, lmu, 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Choshen L, Abend O. Automatically Extracting Challenge Sets for Non local
    Phenomena in Neural Machine Translation. arxiv, September 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Alley EC, Khimulya G, Biswas S, et al. Unified rational protein engineering
    with sequence-based deep representation learning. Nature methods. 2019, 16: 1315-1322'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Madani A, Krause B, Greene ER, et al. Deep neural language modeling enables
    functional protein generation across families. bioRxiv, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Ofer D, Brandes N, Linial M. The language of proteins: NLP, machine learning
    & protein sequences. Computational and Structural Biotechnology Journal, 2021,
    19: 1750-1758'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Jia J, Liu Z, Xiao X, et al. Identification of protein-protein binding
    sites by incorporating the physicochemical properties and stationary wavelet transforms
    into pseudo amino acid composition. Journal of Biomolecular Structure and Dynamics,
    2016, 34: 1946-1961'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Xu G, Wang Q, Ma J. OPUS-Rota4: a gradient-based protein side-chain modeling
    framework assisted by deep learning-based predictors. Briefings in Bioinformatics,
    23: bbab529'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Hanson J, Paliwal K, Litfin T, et al. Improving prediction of protein
    secondary structure, backbone angles, solvent accessibility and contact numbers
    by using predicted contact maps and an ensemble of recurrent and residual convolutional
    neural networks. Bioinformatics, 2019, 35: 2403–2410'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Gao Z, Jiang C, Zhang J, et al. Hierarchical graph learning for protein–protein
    interaction. Nature Communications, 2023, 14: 1093'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Johansson M U. Defining and searching for structural motifs using DeepView/Swiss-PdbViewer.
    BMC Bioinformatics, 2012, 13: 173'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Xu D, Nussinov R. Favorable domain size in proteins. Folding & Design,
    1998, 3: 11–7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Hu F, Hu Y, Zhang W, et al. A Multimodal Protein Representation Framework
    for Quantifying Transferability Across Biochemical Downstream Tasks. Advanced
    Science, 2023, 2301223'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Wang Z, Zhang Q, Shuang-Wei H U, et al. Multi-level Protein Structure
    Pre-training via Prompt Learning. In: The Eleventh International Conference on
    Learning Representations, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Wang L, Liu H, Liu Y, et al. Learning hierarchical protein representations
    via complete 3d graph networks. In: The Eleventh International Conference on Learning
    Representations, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Ingraham J, Garg V, Barzilay R, et al. Generative models for graph-based
    protein design. Advances in neural information processing systems, 2019, 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Gao Z, Tan C, Li S Z. PiFold: Toward effective and efficient protein
    inverse folding. In: ICLR, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Hu B, Tan C, Xia J, et al. Learning Complete Protein Representation by
    Deep Coupling of Sequence and Structure. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Yang J, Anishchenko I, Park H, et al. Improved protein structure prediction
    using predicted inter-residue orientations. In: Proceedings of the National Academy
    of Sciences of the United States of America, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Du Z, Su H, Wang W, et al. The trRosetta server for fast and accurate
    protein structure prediction. Nature protocols, 2021, 16: 5634-5651'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Ye L, Wu P, Peng Z, et al. Improved estimation of model quality using
    predicted inter-residue distance. Bioinformatics, 2021, 37: 3752-3759'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Tischer D, Lisanza S, Wang J, et al. Design of proteins presenting discontinuous
    functional sites using deep learning. bioRxiv, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Guo S S, Liu J, Zhou X G, et al. DeepUMQA: ultrafast shape recognition-based
    protein model quality assessment using deep learning. Bioinformatics, 2022, 38:
    1895-1903'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Gross E H, Meienhofer J. The Peptide Bond. Major Methods of Peptide Bond
    Formation: The Peptides Analysis, Synthesis, Biology, 2014, 1: 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Nelson D L, Lehninger A L, Cox M M. Lehninger principles of biochemistry.
    Macmillan. 2008'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Vollhardt K P C, Schore N E. Organic chemistry: structure and function.
    Macmillan. 2003'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Jing B, Eismann S, Suriana P, et al. Learning from protein structure
    with geometric vector perceptrons. arxiv, September 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Fuchs F, Worrall D, Fischer V, et al. Se (3)-transformers: 3d roto-translation
    equivariant attention networks. Advances in Neural Information Processing Systems,
    2020, 33: 1970-1981'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Du W, Zhang H, Du Y. et al. SE (3) Equivariant Graph Neural Networks
    with Complete Local Frames. In: International Conference on Machine Learning,
    2022\. 5583–5608'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Liu S, Du W, Li Y, et al. Symmetry-Informed Geometric Representation
    for Molecules, Proteins, and Crystalline Materials. arxiv, June 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Liu D, Chen S, Zheng S, et al. SE (3) Equivalent Graph Attention Network
    as an Energy-Based Model for Protein Side Chain Conformation. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Krapp L F, Abriata L A, Cortés Rodriguez F, et al. PeSTo: parameter-free
    geometric deep learning for accurate prediction of protein binding interfaces.
    Nature Communications, 2023, 14: 2175'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Liu Y, Wang L, Liu M, et al. Spherical message passing for 3d molecular
    graphs. In: International Conference on Learning Representations, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Wang L, Liu Y, Lin Y, et al. ComENet: Towards Complete and Efficient
    Message Passing for 3D Molecular Graphs. arxiv, June 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Xu M, Yuan X, Miret S, et al. Protst: Multi-modality learning of protein
    sequences and biomedical texts. arxiv, January 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Klausen M S, Jespersen M C, Nielsen H, et al. Netsurfp-2.0: improved
    prediction of protein structural features by integrated deep learning. Proteins,
    2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Heffernan R, Paliwal K, Lyons J, et al. Single‐sequence‐based prediction
    of protein secondary structures and solvent accessibility by deep whole‐sequence
    learning. Journal of computational chemistry, 2018, 39: 2210-2216'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Almagro Armenteros J J, Johansen A R, Winther O, et al. Language modelling
    for biological sequences–curated datasets and baselines. bioRxiv, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Asgari E, Poerner N, McHardy A C, et al. Deepprime2sec: Deep learning
    for protein secondary structure prediction from the primary sequences. bioRxiv,
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Singh J, Litfin T, Paliwal K, et al. SPOT-1D-Single: improving the single-sequence-based
    prediction of protein secondary structure, backbone angles, solvent accessibility
    and half-sphere exposures using a large training set and ensembled deep learning.
    Bioinformatics, 2021, 37:3464-3472'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Sinai S, Kelsic E, Church G M, et al. Variational auto-encoding of protein
    sequences. arxiv, December 2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Ding X, Zou Z, Brooks III C L. Deciphering protein evolution and fitness
    landscapes with latent space models. Nature communications, 2019, 10: 5644'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Krause B, Lu L, Murray I, et al. Multiplicative LSTM for sequence modelling.
    arxiv, September 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Rao R, Bhattacharya N, Thomas N, et al. Evaluating protein transfer learning
    with TAPE. Advances in neural information processing systems, 2019, 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-supervised
    learning of language representations. arxiv, September 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models
    beyond a fixed-length context. arxiv, January 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Clark K, Luong M T, Le Q V, et al. Electra: Pre-training text encoders
    as discriminators rather than generators. arxiv, March 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of Machine Learning
    Research, 2020, 21: 1–67'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Heinzinger M, Elnaggar A, Wang Y, et al. Modeling the language of life-deep
    learning protein sequences. bioRxiv, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Peters ME, Neumann M, Iyyer M, et al. Deep contextualized word representations.
    North American chapter of the association for computational linguistics, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Strodthoff N, Wagner P, Wenzel M, et al. UDSMProt: universal deep sequence
    models for protein classification. Bioinformatics, 2020, 36: 2401–2409'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Lu A X, Zhang H, Ghassemi M, et al. Self-supervised contrastive learning
    of protein representations by mutual information maximization. bioRxiv, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Dey R, Salem F M. Gate-variants of gated recurrent unit (GRU) neural
    networks. In: 2017 IEEE 60th international midwest symposium on circuits and systems
    (MWSCAS), IEEE, 2017\. 1597-1600'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Zhou G, Chen M, Ju C J, et al. Mutation effect estimation on protein–protein
    interactions using deep contextualized representation learning. NAR genomics and
    bioinformatics, 2020, 2: lqaa015'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Szklarczyk D, Gable A L, Lyon D, et al. STRING v11: protein–protein association
    networks with increased coverage, supporting functional discovery in genome-wide
    experimental datasets. Nucleic acids research, 2019, 47: D607-D613'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Sturmfels P, Vig J, Madani A, et al. Profile prediction: An alignment-based
    pre-training task for protein sequence models. arxiv, December 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Nambiar A, Heflin M, Liu S, et al. Transforming the language of life:
    transformer neural networks for protein prediction tasks. In: Proceedings of the
    11th ACM international conference on bioinformatics, computational biology and
    health informatics, 2020\. 1-8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] He L, Zhang S, Wu L, et al. Pre-training co-evolutionary protein representation
    via a pairwise masked language model. arxiv, October 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Rao R M, Liu J, Verkuil R, et al. Msa transformer. In: International
    Conference on Machine Learning, 2021\. 8844-8856'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Xiao Y, Qiu J, Li Z, et al. Modeling protein using large-scale pretrain
    language model. arxiv, August 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Min S, Park S, Kim S, et al. Pre-training of deep bidirectional protein
    sequence representations with structural information. IEEE Access, 2021, 9: 123912-123926'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Yang K K, Fusi N, Lu A X. Convolutions are competitive with transformers
    for protein sequence pretraining. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Wu R, Ding F, Wang R, et al. High-resolution de novo structure prediction
    from primary sequence. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Hua W, Dai Z, Liu H, et al. Transformer quality in linear time. In: International
    Conference on Machine Learning, PMLR, 2022\. 9099-9117'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Nijkamp E, Ruffolo J, Weinstein E N, et al. ProGen2: exploring the boundaries
    of protein language models. arxiv, June 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Ferruz N, Schmidt S, Höcker B. A deep unsupervised language model for
    protein design. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask
    learners. OpenAI blog, 2019, 1: 9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Hesslow D, Zanichelli N, Notin P, et al. Rita: a study on scaling up
    generative protein sequence models. arxiv, May 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Chen B, Cheng X, Geng Y A, et al. xTrimoPGLM: unified 100B-scale pre-trained
    transformer for deciphering the language of protein. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Melnyk I, Chenthamarakshan V, Chen P Y, et al. Reprogramming pretrained
    language models for antibody sequence infilling. In: International Conference
    on Machine Learning, PMLR, 2023\. 24398-24419'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Truong, T F, Bepler T. PoET: A generative model of protein families as
    sequences-of-sequences. arxiv, June 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Emaad K, Yun S S, Aaron A, et al. CELL-E2: Translating Proteins to Pictures
    and Back with a Bidirectional Text-to-Image Transformer. In: Thirty-seventh Conference
    on Neural Information Processing Systems, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Andreas D, Cecilia L. The Human Protein AtlasSpatial localization of
    the human proteome in health and disease. Protein Science, 2021, 30: 218–233'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Zhang Y, Skolnick J. Tm-align: a protein structure alignment algorithm
    based on the tm-score. Nucleic Acids Research, 2005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Thomas J, Ramakrishnan N, Bailey-Kellogg, C. Graphical models of residue
    coupling in protein families. In: Proceedings of the 5th international workshop
    on Bioinformatics, 2005\. 12-20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Vassura M, Margara L, Di Lena P, et al. Reconstruction of 3D Structures
    From Protein Contact Maps. IEEE/ACM Transactions on Computational Biology and
    Bioinformatics, 2008, 5: 357–367'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Wu Q, Peng Z, Anishchenko I, et al. Protein contact prediction using
    metagenome sequence data and residual neural networks. Bioinformatics, 2020, 36:
    41-48'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Derevyanko G, Grudinin S, Bengio Y, et al. Deep convolutional networks
    for quality assessment of protein folds. Bioinformatics, 2018, 34: 4046-4053'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Maddhuri Venkata Subramaniya S R, Terashi G, Jain A, et al. Protein contact
    map refinement for improving structure prediction using generative adversarial
    networks. Bioinformatics, 2021, 37: 3168-3174'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Sverrisson F, Feydy J, Correia B E, et al. Fast end-to-end learning on
    protein surfaces. In: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2021\. 15272-15281'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Jiao P, Wang B, Wang X, et al. Struct2GO: protein function prediction
    based on graph pooling algorithm and AlphaFold2 structure information. Bioinformatics,
    2023, 39: btad637'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Gelman S, Fahlberg S A, Heinzelman P, et al. Neural networks to learn
    protein sequence–function relationships from deep mutational scanning data. Proceedings
    of the National Academy of Sciences, 2021, 118: e2104878118'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Xia T, Ku W S. Geometric Graph Representation Learning on Protein Structure
    Prediction. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & amp; Data Mining, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Jing X, Xu J. Fast and effective protein model refinement using deep
    graph neural networks. Nature computational science, 2021, 1: 462-469'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Hermosilla P, Schäfer M, Lang M, et al. Intrinsic-extrinsic convolution
    and pooling for learning on 3d protein structures. In: ICLR, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Cheng S, Zhang L, Jin B, et al. GraphMS: Drug Target Prediction Using
    Graph Representation Learning with Substructures. Appl. Sci. 2021, 11: 3239'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Wan F, Hong L, Xiao A, et al. NeoDTI: neural integration of neighbor
    information from a heterogeneous network for discovering new drug–target interactions.
    Bioinformatics, 2019, 35: 104-111'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Hermosilla P, Ropinski T. Contrastive representation learning for 3d
    protein structures. arxiv, May 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Chen C, Zhou J, Wang F, et al. Structure-aware protein self-supervised
    learning. arxiv, April 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Li J, Luo S, Deng C. Directed weight neural networks for protein structure
    representation learning. arxiv, January 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Aykent S, Xia T. Gbpnet: Universal geometric representation learning
    on protein structures. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining, 2022\. 4-14'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Wu T, Cheng J. Atomic protein structure refinement using all-atom graph
    representations and SE (3)-equivariant graph neural networks. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Satorras V G, Hoogeboom E, Welling M. E (n) equivariant graph neural
    networks. In: International conference on machine learning. PMLR, 2021\. 9323-9332'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Bepler T, Berger B. Learning protein sequence embeddings using information
    from structure. arxiv, February 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Wang Z, Combs S A, Brand R, et al. LM-GVP: A Generalizable Deep Learning
    Framework for Protein Property Prediction from Sequence and Structure. bioRxiv,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Gligorijević V, Renfrew P D, Kosciolek T, et al. Structure-based protein
    function prediction using graph convolutional networks. Nature communications,
    2021, 12: 3168'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Mansoor S, Baek M, Madan U, et al. Toward more general embeddings for
    protein design: Harnessing joint representations of sequence and structure. bioRxiv,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Anishchenko I, Baek M, Park H, et al. Protein tertiary structure prediction
    and refinement using deep learning and Rosetta in CASP14\. Proteins: Structure,
    Function, and Bioinformatics, 2021, 89: 1722-1733'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Xia C, Feng S H, Xia Y, et al. Fast protein structure comparison through
    effective representation learning with contrastive graph neural networks. PLoS
    computational biology, 2022, 18: e1009986'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Ke Z, Shao Y, Lin H, et al. Continual Pre-training of Language Models.
    In: ICLR, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] He K, Fan H, Wu Y, et al. Momentum contrast for unsupervised visual representation
    learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] You Y, Shen Y. Cross-modality and self-supervised protein embedding for
    compound–protein affinity and contact prediction. Bioinformatics, 2022, 38: ii68-ii74'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Yang K K, Zanichelli N, Yeh H. Masked inverse folding with sequence transfer
    for protein representation learning. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Zhang, Z, Wang C, Xu, M, et al. A Systematic Study of Joint Representation
    Learning on Protein Sequences and Structures. arxiv, March 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Zhang Z, Xu M, Lozano A, et al. Pre-Training Protein Encoder via Siamese
    Sequence-Structure Diffusion Trajectory Prediction. In: Annual Conference on Neural
    Information Processing Systems, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Su J, Han C, Zhou Y, et al. SaProt: Protein Language Modeling with Structure-aware
    Vocabulary. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Madani A, McCann B, Naik N, et al. Progen: Language modeling for protein
    generation. arxiv, April 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Federhen, S. The NCBI Taxonomy database. Nucleic Acids Research, 2012,
    40: D136–D143'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Brandes N, Ofer D, Peleg Y, et al. ProteinBERT: A universal deep-learning
    model of protein sequence and function. Bioinformatics, 2022, 38: 2102-10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Zhou H Y, Fu Y, Zhang Z, et al. Protein Representation Learning via Knowledge
    Enhanced Primary Structure Modeling. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Lee Y, Yu H, Lee J, et al. Pre-training Sequence, Structure, and Surface
    Features for Comprehensive Protein Representation Learning. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Gu Y, Tinn R, Cheng H, et al. Domain-Specific Language Model Pretraining
    for Biomedical Natural Language Processing. ACM Transactions on Computing for
    Healthcare, 2022: 1–23'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Rose P W, Prlić A, Altunkaya A, et al. The RCSB protein data bank: integrative
    view of protein, gene and 3D structural information. Nucleic acids research, 2016:
    gkw1000'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Suzek B E, Wang Y, Huang H, et al. UniRef clusters: a comprehensive and
    scalable alternative for improving sequence similarity searches. Bioinformatics,
    2015, 31: 926-932'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Orengo C A, Michie A D, Jones S, et al. CATH–a hierarchic classification
    of protein domain structures. Structure, 1997, 5: 1093-1109'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Steinegger M, Söding J. Clustering huge protein sequence sets in linear
    time. Nature communications, 2018, 9: 1-8'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] El-Gebali S, Mistry J, Bateman A, et al. The Pfam protein families database
    in 2019\. Nucleic Acids Research, 2019, 47: D427-D432'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Varadi M, Anyango S, Deshpande M, et al. AlphaFold Protein Structure
    Database: massively expanding the structural coverage of protein-sequence space
    with high-accuracy models. Nucleic acids research, 2022, 50: D439-D444'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Mirdita M, Von Den Driesch L, Galiez C, et al. Uniclust databases of
    clustered and deeply annotated protein sequences and alignments. Nucleic Acids
    Research, 2017, 45: D170-D176'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Lo Conte L, Ailey B, Hubbard T J, et al. SCOP: a structural classification
    of proteins database. Nucleic Acids Research. 2000, 28: 257-259'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Chandonia J M, Fox N K, Brenner S E. SCOPe: manual curation and artifact
    removal in the structural classification of proteins–extended database. Journal
    of molecular biology, 2017, 429: 348-355'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Ahdritz G, Bouatta N, Kadyan S, et al. OpenProteinSet: Training data
    for structural biology at scale. arxiv, August 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Singh J, Paliwal K, Litfin T,et al. Reaching alignment-profile-based
    accuracy in predicting protein secondary and tertiary structural properties without
    alignment. Scientific reports, 2022, 12: 1-9'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Singh J, Litfin T, Singh J, et al. SPOT-Contact-Single: Improving Single-Sequence-Based
    Prediction of Protein Contact Map using a Transformer Language Model. bioRxiv,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Chen C, Zhang Y, Liu X, et al. Bidirectional learning for offline model-based
    biological sequence design. OpenProteinSet: Training data for structural biology
    at scale, January 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] van Kempen M, Kim S S, Tumescheit C, et al. Fast and accurate protein
    structure search with Foldseek. Nature Biotechnology, 2023, 1-4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Oord A, Vinyals O, et al. Neural Discrete Representation Learning. Advances
    in neural information processing systems, 2017, 30'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Hu B, Tan C, Wu L R, et al. Multimodal Distillation of Protein Sequence,
    Structure, and Function. arxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Ibtehaz N, Kagaya Y, et al. Domain-PFP allows protein function prediction
    using function-aware domain embedding representations. Communications Biology,
    2023, 6, 1103'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Wang R, Sun Y, Luo Y, et al. Injecting Multimodal Information into Rigid
    Protein Docking via Bi-level Optimization. In: Thirty-seventh Conference on Neural
    Information Processing Systems, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Sun D, Huang H, Li Y, et al. DSR: Dynamical Surface Representation as
    Implicit Neural Networks for Protein. In: Thirty-seventh Conference on Neural
    Information Processing Systems, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Omelchenko M V, Galperin M Y, Wolf Y I, et al. Non-homologous isofunctional
    enzymes: a systematic analysis of alternative solutions in enzyme evolution. Biology
    direct, 2010, 5: 1-20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Wu L, Huang Y, Lin H, et al. A survey on protein representation learning:
    Retrospect and prospect. arxiv, January 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Liu W, Zhou P, Zhao Z, et al. K-bert: Enabling language representation
    with knowledge graph. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    2020\. 2901-2908'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Wang S, Sun S, Li Z, et al. Accurate de novo prediction of protein contact
    map by ultra-deep learning model. PLoS computational biology, 2017, 13: e1005324'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Li J, Xu J. Study of real-valued distance prediction for protein structure
    prediction with deep learning. Bioinformatics, 2021, 37:3197-3203'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Hu B, Zang Z, Tan C, etc. Deep Manifold Transformation for Protein Representation
    Learning. ICASSP, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Yan Y, Huang S Y. Accurate prediction of inter-protein residue–residue
    contacts for homo-oligomeric protein complexes. Briefings in bioinformatics, 2021,
    22: bbab038'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Baek M, DiMaio F, Anishchenko I, et al. Accurate prediction of protein
    structures and interactions using a three-track neural network. Science, 2021,
    373: 871-876'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Jing X, Wu F, Luo X, et al. RaptorX-Single: single-sequence protein structure
    prediction by integrating protein language models. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Chang L, Perez A. AlphaFold encodes the principles to identify high affinity
    peptide binders. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Ruffolo J A, Gray J J. Fast, accurate antibody structure prediction from
    deep learning on massive set of natural antibodies. Biophysical Journal, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Evans R, O’Neill M, Pritzel A, et al. Protein complex prediction with
    AlphaFold-Multimer. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Bryant P, Pozzati G, Elofsson A. Improved prediction of protein-protein
    interactions using alphafold2 and extended multiple-sequence alignments. bioRxiv,
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Shen T, Hu Z, Peng Z, et al. E2Efold-3D: End-to-End Deep Learning Method
    for accurate de novo RNA 3D Structure Prediction. arxiv, July 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Stein RA, Mchaourab HS. SPEACH_AF: Sampling protein ensembles and conformational
    heterogeneity with Alphafold2\. PLOS Computational Biology, 2022, 18: e1010483'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] AlQuraishi M. End-to-end differentiable learning of protein structure.
    Cell systems, 2019, 8: 292-301'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Du Y, Meier J, Ma J, et al. Energy-based models for atomic-resolution
    protein conformations. arxiv, April 2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Hou M, Jin S, Cui X, et al. Protein multiple conformations prediction
    using multi-objective evolution algorithm. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Wu F, Li S Z. DIFFMD: a geometric diffusion model for molecular dynamics
    simulations. In: Proceedings of the AAAI Conference on Artificial Intelligence,
    2023\. 37: 5321-5329'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] Lu J, Zhong B, Zhang Z, et al. Str2Str: A Score-based Framework for Zero-shot
    Protein Conformation Sampling. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] Zhao K, Xia Y, Zhang F, et al. Protein structure and folding pathway
    prediction based on remote homologs recognition using PAthreader. Communications
    Biololy, 2023, 6: 243'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Huang Z, Cui X, Xia Y, et al. Pathfinder: protein folding pathway prediction
    based on conformational sampling. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Lee J H, Yadollahpour P, Watkins A, et al. Equifold: Protein structure
    prediction with a novel coarse-grained structure representation. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Hiranuma N, Park H, Baek M, et al. Improved protein structure refinement
    guided by deep learning based accuracy estimation. Nature communications, 2021,
    12: 1-11'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Chen B, Xie Z, Xu J, Qiu J, Ye Z, Tang J. Improve the Protein Complex
    Prediction with Protein Language Models. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Feng T, Gao Z, You J, et al. Deep Reinforcement Learning for Modelling
    Protein Complexes. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Baek M, McHugh R, Anishchenko I, et al. Accurate prediction of nucleic
    acid and protein-nucleic acid complexes using RoseTTAFoldNA. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Zhang J, Liu S, Chen M, et al. Few-shot learning of accurate folding
    landscape for protein structure prediction. arxiv, August 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Zhang L, Chen J, Shen T, et al. Enhancing the Protein Tertiary Structure
    Prediction by Multiple Sequence Alignment Generation. arxiv, June 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Jing B, Erives E, Pao-Huang P, et al. EigenFold: Generative Protein Structure
    Prediction with Diffusion Models. arxiv, April 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Kandathil S M, Greener J G, Lau A M. et al. Ultrafast end-to-end protein
    structure prediction enables high-throughput exploration of uncharacterized proteins.
    Proceedings of the National Academy of Sciences, 2022, 119: e2113348119'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Krishna R, Wang J, Ahern W, et al. Generalized biomolecular modeling
    and design with RoseTTAFold All-Atom. bioRxiv, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] Wang S, Li W, Liu S, et al. RaptorX-Property: a web server for protein
    structure property prediction. Nucleic acids research, 2016, 44: W430-W435'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] Wu T, Guo Z, Hou J, et al. DeepDist: real-value inter-residue distance
    prediction with deep residual convolutional network. BMC bioinformatics, 2021,
    22: 1-17'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Chen T, Gong C, Diaz D J, et al. HotProtein: A novel framework for protein
    thermostability prediction and editing. In: ICLR, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Liu S, Zhu T, Ren M, et al. Predicting mutational effects on protein-protein
    binding via a side-chain diffusion probabilistic model. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Luo S, Su Y, Wu Z, et al. Rotamer Density Estimator is an Unsupervised
    Learner of the Effect of Mutations on Protein-Protein Interaction. In: ICLR, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Shi C, Wang C, Lu J, et al. Protein Sequence and Structure Co-Design
    with Equivariant Translation. In: ICLR, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Zheng Z, Deng Y, Xue D, et al. Structure-informed Language Models Are
    Protein Designers. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] Lin Y, AlQuraishi M. Generating novel, designable, and diverse protein
    structures by equivariantly diffusing oriented residue clouds. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Song Z, Li L. Importance Weighted Expectation-Maximization for Protein
    Sequence Design. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Yim J, Tripple B L, Bortoli V D, et al. SE(3) diffusion model with application
    to protein backbone generation. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Kong X, Huang W, Liu Y, et al. End-to-End Full-Atom Antibody Design.
    In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Verma Y, Heinonen M, Garg V, et al. AbODE: Ab Initio Antibody Design
    using Conjoined ODEs. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Tan C, Gao Z, Li S Z. Cross-Gate MLP with Protein Complex Invariant Embedding
    is A One-Shot Antibody Designer. arxiv, May 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Wu F, Li S Z. A Hierarchical Training Paradigm for Antibody Structure-sequence
    Co-design. arxiv, November 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Chen C, Zhang Y, Liu X, et al. Bidirectional Learning for Offline Model-based
    Biological Sequence Design. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Tan, Zhang Y, Gao Z, et al. RDesign: Hierarchical Data-efficient Representation
    Learning for Tertiary Structure-based RNA Design. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Truong Jr T F, Bepler T. PoET: A generative model of protein families
    as sequences-of-sequences. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] Yi K, Zhou B, Shen Y, et al. Graph Denoising Diffusion for Inverse Protein
    Folding. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] Pei Q, Gao K, Wu L, et al. FABind: Fast and Accurate Protein-Ligand Binding.
    In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] Jin W, Sarzikova S, Chen X, et al. Unsupervised Protein-Ligand Binding
    Energy Prediction via Neural Euler’s Rotation Equation. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] Wu L, Tian Y, Huang Y, et al. MAPE-PPI: Towards Effective and Efficient
    Protein-Protein Interaction Prediction via Microenvironment-Aware Protein Embedding.
    In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] Wang R, Sun Y, Luo Y, et al. Injecting Multimodal Information into Rigid
    Protein Docking via Bi-level Optimization. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] Gao Z, Sun X, Liu Z, et al. Protein Multimer Structure Prediction via
    Prompt Learning. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] Zhang Z, Lu Z, Hao Z, et al. Full-Atom Protein Pocket Design via Iterative
    Refinement. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] Gruver N, Stanton S, Frey N, et al. Protein Design with Guided Discrete
    Diffusion. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] Zhang J O, Diaz D J, Klivans A R, et al. Predicting a Protein ’ s Stability
    under a Million Mutations. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] Hsu C, Verkuil R, Liu J, et al. Learning inverse folding from millions
    of predicted structures. In: International Conference on Machine Learning, 2022\.
    8946-8970'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] Cao Y, Das P, Chenthamarakshan V, et al. Fold2seq: A joint sequence (1d)-fold
    (3d) embedding-based generative model for protein design. In: International Conference
    on Machine Learning, 2021\. 1261-1271'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] Dumortier B, Liutkus A, Carré C,et al. PeTriBERT: Augmenting BERT with
    tridimensional encoding for inverse protein folding and design. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] Dauparas J, Anishchenko I, Bennett N, et al. Robust deep learning–based
    protein sequence design using ProteinMPNN. Science, 2022, 378: 49-56'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Gao Z, Tan C, Li S Z. AlphaDesign: A graph protein design method and
    benchmark on AlphaFoldDB. arxiv, February 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Tan C, Gao Z Y, Xia J, et al. Generative de novo protein design with
    global context. arxiv, April 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] Korendovych I V, DeGrado W F. De novo protein design, a retrospective.
    Quarterly reviews of biophysics, 2020, 53, e3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] Mao W, Sun Z, Zhu M, et al. De novo Protein Design Using Geometric Vector
    Field Networks. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Notin P, Dias M, Frazer J, et al. Tranception: protein fitness prediction
    with autoregressive transformers and inference-time retrieval. In: International
    Conference on Machine Learning, 2022\. 16990-17017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] Bushuiev A, Bushuiev R, Kouba P, et al. Learning to design protein-protein
    interactions with enhanced generalization. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] Meier J, Rao R, Verkuil R, et al. Language models enable zero-shot prediction
    of the effects of mutations on protein function. Advances in Neural Information
    Processing Systems, 2021, 34:29287-303'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] Hu M, Yuan F, Yang K K, et al. Exploring evolution-based &-free protein
    language models as protein function predictors. arxiv, June 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] Notin P, Kollasch A W, Ritter D, et al. ProteinGym: Large-Scale Benchmarks
    for Protein Design and Fitness Prediction. In: NeurIPS. 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] Roney JP, Ovchinnikov S. State-of-the-Art estimation of protein model
    accuracy using AlphaFold. bioRxiv, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] Roney J. Evidence for and Applications of Physics-Based Reasoning in
    AlphaFold. Doctoral dissertation, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Akdel M, Pires D E, Pardo E P, et al. A structural biology community
    assessment of alphafold2 applications. Nature Structural & Molecular Biology,
    2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Pak M A, Markhieva K A, Novikova M S, et al. Using alphafold to predict
    the impact of single mutations on protein stability and function. bioRxiv, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] Buel G R, Walters K J. Can AlphaFold2 predict the impact of missense
    mutations on structure?. Nature Structural & Molecular Biology, 2022, 29: 1-2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] Zheng S, Li Y, Chen S, et al. Predicting drug–protein interaction using
    quasi-visual question answering system. Nature Machine Intelligence, 2023, 2:
    134–140'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] Zhang Z, Liu Q. Learning Subpocket Prototypes for Generalizable Structure-based
    Drug Design. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] Gao B, Qiang B, Tan H, et al. DrugCLIP: Contrastive Protein-Molecule
    Representation Learning for Virtual Screening. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] Padmakumar V, Pang R Y, He H, et al. Extrapolative Controlled Sequence
    Generation via Iterative Refinement. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] Lee S, Jo J, Hwang S J. Exploring Chemical Space with Score-based Out-of-distribution
    Generation. In: ICML, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Ahdritz G, Bouatta N, Kadyan S, et al. OpenProteinSet: Training data
    for structural biology at scale. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] Kucera T, Oliver C, Chen D, et al. ProteinShake: Building datasets and
    benchmarks for deep learning on protein structures. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] Xu M, Zhang Z, Lu J, et al. PEER: A Comprehensive and Multi-Task Benchmark
    for Protein Sequence Understanding. arxiv, June 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] Gao Z, Tan C, Zhang Y, et al. ProteinInvBench: Benchmarking Protein Inverse
    Folding on Diverse Tasks, Models, and Metrics. In: NeurIPS, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] Jamasb A R, Morehead A, Zhang Z, et al. Evaluating Representation Learning
    on the Protein Structure Universe. In: ICLR, 2024'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] Roy A, Kucukural A, Zhang Y. I-TASSER: a unified platform for automated
    protein structure and function prediction. Nature protocols, 2010, 5: 725-738'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] Tunyasuvunakool K, Adler J, Wu Z, et al. Highly accurate protein structure
    prediction for the human proteome. Nature, 2021, 596: 590-596'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] Under review as a conference paper at ICLR 2024\. ProtChatGPT: Towards
    Understanding Proteins with Large Language Models, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Lin Z, Akin H, Rao R, et al. Evolutionary-scale prediction of atomic-level
    protein structure with a language model. Science, 2023, 379: 1123-1130'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
