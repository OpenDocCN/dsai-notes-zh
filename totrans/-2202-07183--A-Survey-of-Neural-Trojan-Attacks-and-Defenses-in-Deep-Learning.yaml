- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:47:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:47:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2202.07183] A Survey of Neural Trojan Attacks and Defenses in Deep Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2202.07183] 深度学习中的神经特洛伊攻击与防御调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2202.07183](https://ar5iv.labs.arxiv.org/html/2202.07183)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2202.07183](https://ar5iv.labs.arxiv.org/html/2202.07183)
- en: A Survey of Neural Trojan Attacks and Defenses
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经特洛伊攻击与防御的调查
- en: in Deep Learning
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中
- en: Jie Wang    Ghulam Mubashar Hassan    Naveed Akhtar
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 姜伟    古拉姆·穆巴沙尔·哈桑    纳维德·阿赫塔尔
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Artificial Intelligence (AI) relies heavily on deep learning - a technology
    that is becoming increasingly popular in real-life applications of AI, even in
    the safety-critical and high-risk domains. However, it is recently discovered
    that deep learning can be manipulated by embedding Trojans inside it. Unfortunately,
    pragmatic solutions to circumvent the computational requirements of deep learning,
    e.g. outsourcing model training or data annotation to third parties, further add
    to model susceptibility to the Trojan attacks. Due to the key importance of the
    topic in deep learning, recent literature has seen many contributions in this
    direction. We conduct a comprehensive review of the techniques that devise Trojan
    attacks for deep learning and explore their defenses. Our informative survey systematically
    organizes the recent literature and discusses the key concepts of the methods
    while assuming minimal knowledge of the domain on the readers part. It provides
    a comprehensible gateway to the broader community to understand the recent developments
    in Neural Trojans.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能（AI）在深度学习上依赖很大，这项技术在AI的现实应用中越来越受欢迎，即使在安全关键和高风险领域。然而，最近发现深度学习可以通过嵌入特洛伊木马来被操控。不幸的是，为了绕过深度学习的计算要求，例如，将模型训练或数据标注外包给第三方，进一步增加了模型对特洛伊攻击的敏感性。由于这一话题在深度学习中的关键重要性，最近的文献在这一方向上做出了许多贡献。我们对深度学习的特洛伊攻击技术进行了全面的回顾，并探讨了它们的防御。我们的信息丰富的调查系统地整理了最近的文献，并讨论了这些方法的关键概念，同时假设读者对该领域的知识较少。它为更广泛的社区提供了一个易于理解的途径，以便了解神经特洛伊的最新进展。
- en: '{IEEEkeywords}'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEkeywords}'
- en: Deep Learning, Trojan attack, Backdoor attack, Neural Trojan, Trojan detection.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、特洛伊攻击、后门攻击、神经特洛伊、特洛伊检测。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: \IEEEPARstart
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \IEEEPARstart
- en: Deep learning [[1](#bib.bib1)] is a popular technique in Artificial Intelligence
    (AI) that is deployed in a wide range of real-world applications, such as face
    recognition [[97](#bib.bib97), [98](#bib.bib98)], object detection and tracking [[106](#bib.bib106),
    [107](#bib.bib107)] and speech recognition [[99](#bib.bib99), [100](#bib.bib100)].
    It aims at inducing computational models for complex daily-life tasks by directly
    learning from raw data [[1](#bib.bib1)]. It uses network architectures that comprise
    multiple layers of primitive processing units, called neurons [[104](#bib.bib104)].
    A neural network mathematically imitates the working of neurons in human brains
    to process information for a given task. A neural network generally has an input
    layer, an output layer, and an arbitrary number of hidden layers [[105](#bib.bib105)].
    The input layer provides data to the network, and the output layer returns the
    network prediction. The hidden layers that are responsible for the core computations
    and data processing [[101](#bib.bib101)]. In modern deep learning, the hidden
    layers often comprise millions of neurons with sophisticated inter-connections.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习[[1](#bib.bib1)]是人工智能（AI）中一种流行的技术，广泛应用于现实世界的各种应用，如面部识别[[97](#bib.bib97)、[98](#bib.bib98)]、物体检测和跟踪[[106](#bib.bib106)、[107](#bib.bib107)]以及语音识别[[99](#bib.bib99)、[100](#bib.bib100)]。其目标是通过直接从原始数据中学习来构建用于复杂日常任务的计算模型[[1](#bib.bib1)]。它使用包含多个基本处理单元的网络架构，这些单元称为神经元[[104](#bib.bib104)]。神经网络在数学上模拟了人脑中神经元的工作方式，以处理特定任务的信息。神经网络通常具有一个输入层、一个输出层和任意数量的隐藏层[[105](#bib.bib105)]。输入层向网络提供数据，输出层返回网络预测。隐藏层负责核心计算和数据处理[[101](#bib.bib101)]。在现代深度学习中，隐藏层通常包含具有复杂互连的数百万个神经元。
- en: Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Neural Trojan Attacks
    and Defenses in Deep Learning") illustrates a simple neural network (by modern
    standards) that expects an image as input to predict its class label. The illustration
    uses a standard feed-forward network, a.k.a. Multi-Layer Perceptron (MLP). Other
    popular modern network types, e.g. Convolutional Neural Networks (CNNs) and Recurrent
    Neural Networks (RNNs) [[3](#bib.bib3)] generally have much more complex architectures.
    However, even for the simpler architectures, e.g. in Fig. [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ A Survey of Neural Trojan Attacks and Defenses in Deep Learning"),
    the inter-connectivity of neurons remains reasonably complex. Moreover, the network
    acts as a holistic computational model, which means its prediction is likely to
    change if any of the neurons in the network misbehaves. If that happens, detection
    of this misbehavior becomes a challenging problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Neural Trojan Attacks
    and Defenses in Deep Learning") 说明了一个简单的神经网络（按现代标准），期望将图像作为输入以预测其类别标签。该示意图使用了标准的前馈网络，即多层感知器（MLP）。其他流行的现代网络类型，例如卷积神经网络（CNNs）和递归神经网络（RNNs）[[3](#bib.bib3)]
    通常具有更复杂的架构。然而，即便对于较简单的架构，例如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey
    of Neural Trojan Attacks and Defenses in Deep Learning") 中所示，神经元的互联仍然相当复杂。此外，该网络作为一个整体计算模型，这意味着如果网络中的任何神经元表现异常，其预测可能会发生变化。如果发生这种情况，检测这种异常行为将变得非常具有挑战性。
- en: '![Refer to caption](img/a60d09b84a71a388068acf4f0a3efecb.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a60d09b84a71a388068acf4f0a3efecb.png)'
- en: 'Figure 1: Illustration of a Multi-Layer Perceptron (MLP) expecting an images
    to predict its correct label.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：多层感知器（MLP）的示意图，期望输入图像以预测其正确标签。
- en: '![Refer to caption](img/dc61477b773809b45eb8672c51f296cc.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dc61477b773809b45eb8672c51f296cc.png)'
- en: 'Figure 2: Illustration of Trojan attack. (a) Trojan attack in action: A compromised
    classifier is deployed that predicts incorrect label of the input only when a
    trigger is present in the input. The trigger activates the backdoor in the model.
    The classifier behaves normally in the absence of the trigger. (b) Three common
    possibilities of Trojan attacks when utilizing third-party resources. (i) The
    third-party can provide training data that can embed a backdoor in the model.
    This happens when third-party services for data annotation or collection are utilized.
    (ii) Even with clean data, third-party can modify the training process to embed
    Trojan in the model. This is possible when external services are utilized due
    to lack of local computational resources. (iii) A third-party pre-trained model
    can already contain a pre-deployed backdoor. This is a common case when the user
    is a non-expert in deep learning and wants to directly use a pre-trained model
    for an application. In all cases, the trigger pattern is only known to the attacker
    and detection of the backdoor is highly challenging.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：特洛伊攻击的示意图。（a）特洛伊攻击的实施：一个被破坏的分类器被部署，仅当输入中存在触发器时才预测错误的标签。触发器激活模型中的后门。触发器缺席时，分类器表现正常。（b）利用第三方资源时特洛伊攻击的三种常见可能性。（i）第三方可以提供训练数据，从而在模型中嵌入后门。这发生在使用第三方数据注释或收集服务时。（ii）即使数据是干净的，第三方也可以修改训练过程，以在模型中嵌入特洛伊。这在由于缺乏本地计算资源而利用外部服务时是可能的。（iii）第三方预训练模型可能已经包含了预部署的后门。这在用户对深度学习不熟悉并希望直接使用预训练模型进行应用时很常见。在所有情况下，触发器模式仅为攻击者所知，检测后门非常具有挑战性。
- en: Generally, the representation prowess of neural networks is associated with
    their hierarchical structure [[103](#bib.bib103)]. It is known that the initial
    layers of a network help in breaking down complex concepts into more primitive
    constructs. For instance, an image classifier breaks the images into edges in
    the initial layers. The latter (or deeper) layers then focus on more complex concepts,
    e.g. salient object features in the input image [[1](#bib.bib1)]. This leads to
    deeper networks (i.e. networks with more hidden layers) for more complex tasks [[2](#bib.bib2)],
    which implies an increasing number of neurons in the networks. From computational
    modeling perspective, this also means that the model contains more ‘parameters’
    that need to be learned. The larger number of learnable parameters, in turn, demand
    more training data to arrive at an appropriate computational model. In essence,
    complexity of the task not only renders the architecture of the network complex,
    it also makes the whole learning process of the model cumbersome.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，神经网络的表示能力与其层次结构相关[[103](#bib.bib103)]。已知网络的初始层有助于将复杂概念分解为更基本的构建块。例如，图像分类器在初始层将图像分解为边缘。之后的（或更深的）层则专注于更复杂的概念，例如输入图像中的显著物体特征[[1](#bib.bib1)]。这导致更深的网络（即具有更多隐藏层的网络）用于更复杂的任务[[2](#bib.bib2)]，这意味着网络中神经元的数量在增加。从计算建模的角度来看，这也意味着模型包含更多需要学习的‘参数’。更多可学习的参数反过来又需要更多的训练数据以获得合适的计算模型。本质上，任务的复杂性不仅使网络架构变得复杂，也使得模型的整个学习过程变得繁琐。
- en: With the modern day applications of deep learning, complexity of the tasks has
    become an ever increasing phenomenon. Hence, larger and larger data sets are being
    utilized to train deep learning models for all kinds of applications. Besides
    challenging (and often financially expensive) data curation, it requires significant
    computational resources to induce the desired deep learning models. This frequently
    leads to the involvement of third-parties during the training stage of a model.
    These parties lend their resources to users for efficient model training. However,
    this pragmatic solution also make neural networks susceptible to Trojan attacks.
    A Trojan attack allows an attacker (e.g. third-party) to induce a backdoor in
    the model. This backdoor lets the model operate normally at all times, except
    when the input contains a trigger [[3](#bib.bib3)]. A trigger can be a signal
    with a specific pattern that is only known to the attacker [[103](#bib.bib103)].
    The model starts misbehaving under the hood when exposed to the inputs with the
    trigger.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 随着现代深度学习应用的普及，任务的复杂性成为一种日益增长的现象。因此，越来越大的数据集被用来训练深度学习模型以适应各种应用。除了挑战性（且通常成本高昂）的数据整理外，还需要大量的计算资源来训练期望的深度学习模型。这通常导致在模型训练阶段涉及第三方。这些第三方将其资源借给用户，以实现高效的模型训练。然而，这种务实的解决方案也使神经网络容易受到木马攻击。木马攻击允许攻击者（例如第三方）在模型中植入后门。这个后门使得模型在任何时候都能正常操作，除非输入包含触发信号[[3](#bib.bib3)]。触发信号可以是攻击者仅知的特定模式的信号[[103](#bib.bib103)]。当模型暴露于带有触发信号的输入时，它会在幕后表现异常。
- en: A backdoor in the model is hard to detect because it can be embedded by locally
    manipulating only a handful (of millions) of neurons in modern deep learning models.
    The ever-increasing complexity of modern networks is only adding to the challenges
    of Trojan detection in deep learning. As one can imagine, Trojan attacks present
    a serious concern for a variety of real-life applications, especially in safety-critical
    tasks. Generally, there are three scenarios that expose neural networks to Trojan
    attacks that occur due to the third-party involvement in model training process [[4](#bib.bib4)].
    First, due to enormous training data requirements, users can utilise third-party
    datasets instead of spending time to collect the required data themselves. In
    this case, the data can be poisoned, which can result in compromised training.
    Second, the users may use an external computational resource, e.g. under cloud
    computing platforms. In this case, the user needs to provide the training data
    along the training schedule to the third-party platform. Whereas this eases the
    compute requirements for the user, it also exposes the training process to potential
    data poisoning. Lastly, the users may directly use a pre-trained model provided
    by a third-party to avoid training altogether. This is a common practice in the
    scenarios where the user is not a deep learning expert. In this case, the model
    may already contain a Trojan, see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣
    A Survey of Neural Trojan Attacks and Defenses in Deep Learning") for illustration.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中的后门难以检测，因为它可以通过仅局部操作现代深度学习模型中的少量（数百万中的一部分）神经元来嵌入。现代网络日益增长的复杂性只会增加深度学习中特洛伊木马检测的难度。可以想象，特洛伊木马攻击对各种实际应用，特别是在安全关键任务中，构成了严重的担忧。通常，有三种情况会使神经网络暴露于由于第三方参与模型训练过程而发生的特洛伊木马攻击[[4](#bib.bib4)]。首先，由于对海量训练数据的需求，用户可以利用第三方数据集，而不是花时间自己收集所需的数据。在这种情况下，数据可能会被污染，从而导致训练受到影响。其次，用户可能会使用外部计算资源，例如在云计算平台下。在这种情况下，用户需要向第三方平台提供训练数据和训练计划。虽然这减轻了用户的计算需求，但也使训练过程暴露于潜在的数据污染中。最后，用户可能直接使用第三方提供的预训练模型，以避免完全训练。这在用户不是深度学习专家的场景中很常见。在这种情况下，模型可能已经包含了特洛伊木马，见图[2](#S1.F2
    "图 2 ‣ 1 引言 ‣ 深度学习中神经网络特洛伊木马攻击和防御的调查")以作说明。
- en: Considering the key importance of the problem in deep learning research, recent
    literature has seen many contributions in the direction of devising Trojan attacks
    and defenses. Naturally, this has also resulted in review articles in this direction [[3](#bib.bib3)],
    [[4](#bib.bib4)]. However, those reviews focused on the early contributions in
    this nascent but rapidly developing research direction. As compared to [[3](#bib.bib3)]
    and [[4](#bib.bib4)], we also review the very recent developments and provide
    the outlook of a more matured research area. Moreover, we also tap into our experience
    in adversarial machine learning [[108](#bib.bib108)], [[110](#bib.bib110)] to
    draw insights from that parallel research direction to guide the literature in
    Trojan attacks. Our literature review also covers both aspects of Trojan attacks
    and defenses.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于该问题在深度学习研究中的关键重要性，最近的文献中出现了许多关于特洛伊木马攻击和防御的贡献。这自然也导致了在这一方向上的综述文章[[3](#bib.bib3)],
    [[4](#bib.bib4)]。然而，这些综述集中在这一新兴但迅速发展的研究方向中的早期贡献。与[[3](#bib.bib3)]和[[4](#bib.bib4)]相比，我们还审查了非常近期的发展，并提供了对更成熟研究领域的展望。此外，我们还借鉴了在对抗性机器学习[[108](#bib.bib108)],
    [[110](#bib.bib110)]中的经验，从这一平行研究方向中获取见解，以指导特洛伊木马攻击的文献。我们的文献综述还涵盖了特洛伊木马攻击和防御的两个方面。
- en: 2 Injection of Neural Trojan
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 神经网络特洛伊木马的注入
- en: Trojan attacks on neural networks are mainly administered during the training
    phase. Though injection of the backdoor in the network is mainly done by poisoning
    training data of the model, it is also possible to embed Trojans in models without
    access to the training data. In this section, we divide the literature based on
    ‘training data poisoning’ and ‘non-poisoning based attacks’. These attacks are
    limited to the digital space of the models and their inputs. Moreover, they are
    mainly concerned with visual models. For a comprehensive review, we also separately
    discuss the methods of Trojan attacks beyond the digital space and computer vision
    domain.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络上的木马攻击主要在训练阶段进行。虽然在网络中注入后门主要通过中毒模型的训练数据来完成，但也可以在没有访问训练数据的情况下在模型中嵌入木马。在本节中，我们根据“训练数据中毒”和“非中毒攻击”对文献进行分类。这些攻击仅限于模型及其输入的数字空间。此外，它们主要涉及视觉模型。为了全面审查，我们还分别讨论了超越数字空间和计算机视觉领域的木马攻击方法。
- en: 2.1 Training Data Poisoning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 训练数据中毒
- en: Training data poisoning inserts neural Trojan in a model by mixing the training
    data with a small fraction of poisoned data. The goal of poisoned data is to maliciously
    force the model to learn incorrect associations of the concepts that can be activated
    with ‘trigger’ signals after deployment. For instance, in an image classification
    task, the attacker may add a malicious pattern in the training images of a given
    class to poison the dataset. It is likely that the classifier will also wrongly
    associate the label of that class with that pattern. At the test time, inclusion
    of the same pattern (now a trigger) in the image of any other class can confuse
    the model. In the context of visual models - the mainstream victims of Trojan
    attacks - the malicious patterns in training and testing data can be both visible
    and invisible to the human observers. We discuss the data poisoning based attacks
    further by categorizing them along this division.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据中毒通过将训练数据与少量中毒数据混合来在模型中插入神经木马。中毒数据的目标是恶意迫使模型学习错误的概念关联，这些关联可以在部署后通过“触发”信号激活。例如，在图像分类任务中，攻击者可能会在特定类别的训练图像中添加恶意模式，以中毒数据集。分类器很可能也会错误地将该类别的标签与该模式关联。在测试时，将相同的模式（现在是触发器）包含在其他类别的图像中可能会混淆模型。在视觉模型的背景下——木马攻击的主要受害者——训练和测试数据中的恶意模式对人类观察者来说既可能是可见的，也可能是不可见的。我们通过沿着这种划分进一步讨论基于数据中毒的攻击。
- en: 2.1.1 Visible Attacks
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 可见攻击
- en: In the visible Trojan attacks, the differences between the Trojaned sample and
    the clean sample are distinguishable based on their appearance. Here, a Trojaned
    sample is an input image that has been maliciously modified to embed a Trojan
    in a model, or to activate it. Gu et al. [[5](#bib.bib5)] presented one of the
    earliest attack techniques termed BadNets that can insert neural Trojan in models
    in two simple steps. In the first step, a small fraction of benign training samples
    are stamped with a trigger pattern. This is the training data poisoning step.
    Second, the poisoned training dataset is used to train a Trojaned model. Due to
    the presence of trigger in the training data, the model becomes sensitive to the
    trigger pattern. Hence, it starts misbehaving whenever the trigger pattern is
    encountered after deployment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在可见的木马攻击中，木马样本和干净样本之间的差异可以根据其外观进行区分。在这里，木马样本是指被恶意修改以在模型中嵌入木马或激活木马的输入图像。Gu 等人[[5](#bib.bib5)]
    提出了最早的攻击技术之一，称为 BadNets，可以通过两个简单步骤在模型中插入神经木马。在第一步中，一小部分无害的训练样本被印上触发模式。这是训练数据中毒步骤。第二步，使用中毒的训练数据集来训练一个木马模型。由于训练数据中存在触发器，模型对触发模式变得敏感。因此，模型在部署后遇到触发模式时会出现异常行为。
- en: Chen et al. [[6](#bib.bib6)] are among the first to propose improving visual
    indistinguishability of Trojanned images w.r.t. clean image. In their work, a
    blending strategy was introduced to replace the stamping technique in BadNets.
    They demonstrated that by blending the trigger throughout a benign image instead
    of stamping on a fixed position, the poisoned image can be made to look similar
    to its benign counterpart. They also argued that this removes the constraints
    on the size of the triggers, which helps in improving its adverse effects. Liu
    et al. [[8](#bib.bib8)] proposed a method that adds reflections to the input image
    as the trigger. This method is more stealthy because humans normally expect shadows
    and reflections in images. This makes human detection of intentionally embedded
    triggers in [[8](#bib.bib8)] difficult.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人[[6](#bib.bib6)] 是首批提出提高特洛伊木马图像与干净图像视觉不可区分性的研究者之一。在他们的工作中，引入了一种混合策略来替代
    BadNets 中的印刷技术。他们展示了通过将触发器混合到良性图像中，而不是固定位置印刷，污染图像可以与其良性对手相似。他们还指出，这消除了对触发器大小的限制，有助于提高其不良影响。Liu
    等人[[8](#bib.bib8)] 提出了一种将反射添加到输入图像中的方法作为触发器。这种方法更具隐蔽性，因为人们通常期望图像中有阴影和反射。这使得在[[8](#bib.bib8)]中故意嵌入的触发器难以被人眼检测到。
- en: As one of the earliest attacks, the high visibility of BadNets makes it weak.
    Boloor et al. [[113](#bib.bib113)] try to improve on the BadNets by introducing
    Optical Trojan which can be turned on or off. The Optical Trojan is designed by
    attaching a Trojaned lens to a camera so that the neural network will only misbehave
    when the lens are activated for trigger visualisation. The Trojan trigger can
    then designed smaller to avoid human vision but the effect of the trigger will
    not be weakened as the lens can help detect trigger and cause the malfunction
    of the neural network. Kwon et al. [[125](#bib.bib125)] also proposed a multi-model
    selective backdoor attack that confuses a neural network by misclassifying the
    input based on the position of the trigger.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最早的攻击之一，BadNets 的高可见性使其变得脆弱。Boloor 等人[[113](#bib.bib113)] 试图通过引入可以打开或关闭的光学特洛伊木马来改进
    BadNets。光学特洛伊木马的设计是通过在摄像头上附加一个特洛伊镜头，使神经网络只有在镜头激活用于触发可视化时才会出现异常。然后可以将特洛伊触发器设计得更小，以避免被人眼察觉，但由于镜头可以帮助检测触发器并导致神经网络故障，触发器的效果不会减弱。Kwon
    等人[[125](#bib.bib125)] 还提出了一种多模型选择性后门攻击，通过根据触发器的位置错误分类输入来混淆神经网络。
- en: Barni et al. [[127](#bib.bib127)] discovered that the existing attacks assume
    that the label of the Trojaned images are poisoned with the Trojan trigger and
    concentrate more on how to keep the triggers stealthy. They found that this significantly
    decreases the stealthiness of the attacks because of the obvious mismatch of the
    Trojaned samples and their labels. To improve this, they proposed the sinusoidal
    strips based backdoor that does not need to have the class of the Trojaned samples
    pre-defined at test time. Xue et al. [[119](#bib.bib119)] claimed that the attacks
    usually use compressed neural Trojan triggers. However, it makes the attack weak
    as the feature of the compressed trigger can be damaged. To overcome this, they
    proposed a neural Trojan attack that is compression-resistant. To implement that,
    they trained the neural network with poisoned images with both the trigger and
    its compressed version in order to let the internal layers of the network extract
    the feature of the images. Then, they minimised the difference in features between
    the the poisoned and their compressed images so that the network treats the compressed
    and the original poisoned images the same in the feature space. They demonstrated
    an attack success rate greater than 97%.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Barni 等人[[127](#bib.bib127)] 发现现有的攻击假设特洛伊木马图像的标签被特洛伊触发器污染，并更多地关注如何保持触发器的隐蔽性。他们发现，由于特洛伊木马样本与其标签的明显不匹配，这显著降低了攻击的隐蔽性。为了改进这一点，他们提出了基于正弦条纹的后门方法，该方法不需要在测试时预先定义特洛伊木马样本的类别。Xue
    等人[[119](#bib.bib119)] 声称攻击通常使用压缩的神经特洛伊触发器。然而，这使得攻击变得脆弱，因为压缩触发器的特征可能会受到损坏。为了克服这一点，他们提出了一种抗压缩的神经特洛伊攻击。为了实现这一点，他们用带有触发器及其压缩版本的污染图像训练神经网络，以便让网络的内部层提取图像的特征。然后，他们最小化了污染图像与其压缩图像之间的特征差异，使得网络在特征空间中将压缩图像和原始污染图像视为相同。他们展示了超过97%的攻击成功率。
- en: 2.1.2 Invisible Attacks
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 隐形攻击
- en: BadNets [[5](#bib.bib5)] can successfully cause model malfunctioning. However,
    since the trigger pattern in the data is humanly perceptible, a user can detect
    the attack somewhat easily. To make Trojan attacks stealthier, multiple techniques
    in the literature have emerged.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: BadNets [[5](#bib.bib5)] 可以成功导致模型故障。然而，由于数据中的触发模式对人类是可感知的，用户可以相对容易地检测到攻击。为了使木马攻击更加隐蔽，文献中出现了多种技术。
- en: Li et al. [[7](#bib.bib7)] applied DNN-based image steganography for invisible
    trigger generation. Their trigger is set to be random and best suited for the
    image under consideration for imperceptibility. Similarly, Zhong et al. [[9](#bib.bib9)]
    also focused on imperceptibility of triggers by restraining the $\ell_{2}$-norm
    of the added patterns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人 [[7](#bib.bib7)] 应用基于 DNN 的图像隐写术生成不可见触发器。他们的触发器设置为随机的，并且最适合所考虑的图像，以实现不可感知性。类似地，Zhong
    等人 [[9](#bib.bib9)] 也通过限制添加模式的 $\ell_{2}$-范数来关注触发器的不可感知性。
- en: The aforementioned methods poison the training data to inject Trojans, and use
    the tempered images with incorrect labels to train the neural network. Even when
    the trigger pattern may itself be invisible in the input image, the users can
    still observe the relationship between the input image and the output label to
    suspect poisoning. To address this relationship mismatch, a clean-label invisible
    attack is proposed by Barni et al. [[10](#bib.bib10)]. In that work, the label
    of the poisoned data remains unchanged when the trigger is added to the input
    image. This method allows the attack to bypass the Trojan detection techniques
    that are based on the image-label relationship inspection. Here, the core idea
    is the same as we discussed in the classifier example in the first paragraph of
    Section [2.1](#S2.SS1 "2.1 Training Data Poisoning ‣ 2 Injection of Neural Trojan
    ‣ A Survey of Neural Trojan Attacks and Defenses in Deep Learning"). However,
    [[10](#bib.bib10)] also pays special attention to imperceptibility of the trigger
    by using a mask over the image that makes the added pattern less obvious.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法通过毒害训练数据注入木马，并使用带有错误标签的处理图像来训练神经网络。即使触发模式本身在输入图像中可能不可见，用户仍然可以观察输入图像和输出标签之间的关系以怀疑毒害。为了解决这种关系不匹配问题，Barni
    等人 [[10](#bib.bib10)] 提出了一个清洁标签的隐形攻击。在该工作中，当触发器添加到输入图像时，被毒害数据的标签保持不变。这种方法使得攻击能够绕过基于图像-标签关系检查的木马检测技术。这里的核心思想与我们在第
    [2.1](#S2.SS1 "2.1 Training Data Poisoning ‣ 2 Injection of Neural Trojan ‣ A
    Survey of Neural Trojan Attacks and Defenses in Deep Learning") 节第一段的分类器示例中讨论的相同。然而，[[10](#bib.bib10)]
    也特别关注通过在图像上使用掩模来提高触发器的不可感知性，从而使添加的模式不那么明显。
- en: Turner et al. [[11](#bib.bib11)] proposed a method to modify individual pixel
    values images to embed triggers instead of inserting holistic trigger patterns
    into the images. This makes the resulting alteration unnoticeable. However, as
    this method involves changing pixel values, it is limited to the image domain.
    It can not be easily extended to other data modalities, even to videos. Zhao et
    al. [[12](#bib.bib12)] extended the method in [[11](#bib.bib11)] to video classification.
    Unlike the original work [[11](#bib.bib11)] that employed image-specific trigger
    patterns for pixel value modification, Zhao et al. utilised universal adversarial
    triggers which requires only a small fraction of samples to be poisoned to achieve
    high success rates.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Turner 等人 [[11](#bib.bib11)] 提出了一种方法，通过修改单个像素值来嵌入触发器，而不是将整体触发模式插入图像中。这使得最终的更改不易被察觉。然而，由于该方法涉及更改像素值，它仅限于图像领域，不能轻易扩展到其他数据模态，甚至视频。赵等人
    [[12](#bib.bib12)] 将 [[11](#bib.bib11)] 中的方法扩展到视频分类。与原始工作 [[11](#bib.bib11)] 使用图像特定的触发模式进行像素值修改不同，赵等人利用了通用对抗触发器，这只需要少量样本被污染即可达到高成功率。
- en: Similar to [[10](#bib.bib10)], Saha et al. [[13](#bib.bib13)] also proposed
    a technique for clean-label attack that embeds neural Trojan during model training.
    They used a pre-trained model from a third-party and fine-tuned that with Trojaned
    images containing additional inconspicuous trigger patterns at random locations.
    The patterns are added to the texture of the image with an objective to minimise
    the difference between the Trojaned and benign samples. The resulting modification
    to the image remains small and the location of the trigger remains generally unpredictable.
    This makes the detection of the trigger hard in their attack. Quiring et al. [[14](#bib.bib14)]
    discovered that the image scaling functions are generally vulnerable when subjected
    to attacks. Hence, they utilised the image scaling attacks for efficient Trojan
    injection while keeping the trigger hidden.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于[[10](#bib.bib10)]，Saha等人[[13](#bib.bib13)]也提出了一种清洁标签攻击技术，该技术在模型训练过程中嵌入神经特洛伊。他们使用了第三方的预训练模型，并用包含额外隐形触发器模式的特洛伊图像对其进行了微调。模式被添加到图像的纹理中，目的是最小化特洛伊样本与良性样本之间的差异。对图像的修改结果仍然很小，触发器的位置通常是不可预测的。这使得他们的攻击中触发器的检测变得困难。Quiring等人[[14](#bib.bib14)]发现，当图像缩放函数受到攻击时，通常是脆弱的。因此，他们利用图像缩放攻击进行高效的特洛伊注入，同时保持触发器的隐蔽性。
- en: More recently, Salem et al. [[15](#bib.bib15)] indicated that the existing triggers
    are static for the inputs, which makes their detection easy. They then introduced
    three attack methods, namely; Random Backdoor (RB), Backdoor Generating Network
    (BaN) and conditional BaN (cBaN) that are dynamic and allow Trojan triggers to
    be any pattern at any location in the image (see Fig. [3](#S2.F3 "Figure 3 ‣ 2.1.2
    Invisible Attacks ‣ 2.1 Training Data Poisoning ‣ 2 Injection of Neural Trojan
    ‣ A Survey of Neural Trojan Attacks and Defenses in Deep Learning")). The RB randomly
    selects a trigger from a fixed trigger distribution. The fixed distribution is
    then improved in BaN and cBaN where the trigger is generated based on separate
    algorithms. The cBaN is an improvement over BaN that allows generation of target-specific
    triggers for pre-defined labels. Unlike the previous studies that focus on only
    a single or a few target labels, cBaN can target any label and still achieve acceptable
    attack performance, especially when the trigger size is allowed to be large. Li
    et al. [[114](#bib.bib114)] also claimed that most of the previous attacks involves
    using same triggers for different samples which has the weakness to be detected
    easily using the existing neural Trojan defense techniques. To overcome this,
    they utilise triggers that are sample-specific. This means that instead of adding
    triggers to cause the alteration of the model structure, they only need to modify
    a portion of the training sample with some invisible perturbations. Inspired by
    image steganography, they used an encoder-decoder network to encode an invisible
    attacker-specified string to the samples. The strings work as the trigger to cause
    the model to misbehave.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Salem等人[[15](#bib.bib15)]指出，现有的触发器对于输入是静态的，这使得它们的检测变得容易。他们引入了三种攻击方法，即：随机后门（RB）、后门生成网络（BaN）和条件BaN（cBaN），这些方法是动态的，可以使特洛伊触发器在图像的任何位置具有任意模式（见图[3](#S2.F3
    "图 3 ‣ 2.1.2 隐形攻击 ‣ 2.1 训练数据中毒 ‣ 2 注入神经特洛伊 ‣ 深度学习中的神经特洛伊攻击与防御综述")）。RB从固定的触发器分布中随机选择一个触发器。然后，在BaN和cBaN中改进了固定分布，在这些方法中，触发器是基于不同算法生成的。cBaN是对BaN的改进，它允许生成针对预定义标签的特定触发器。与仅关注单一或少数目标标签的先前研究不同，cBaN可以针对任何标签并仍能实现可接受的攻击性能，特别是当触发器大小允许较大时。Li等人[[114](#bib.bib114)]还声称，大多数先前的攻击涉及对不同样本使用相同的触发器，这使得它们容易被现有的神经特洛伊防御技术检测到。为了克服这一点，他们利用了样本特定的触发器。这意味着，他们不需要添加触发器来改变模型结构，只需对训练样本的一部分进行一些隐形扰动。受到图像隐写术的启发，他们使用了一个编码器-解码器网络，将隐形攻击者指定的字符串编码到样本中。这些字符串作为触发器使模型表现异常。
- en: '![Refer to caption](img/98f2d4c6089d84b2e960aa6c5b85fb59.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/98f2d4c6089d84b2e960aa6c5b85fb59.png)'
- en: 'Figure 3: Examples of dynamic and static Trojan triggers [[15](#bib.bib15)].
    (a) A fixed pattern trigger always stays at the top-left corner, which makes its
    detection easier. (b) Trojan trigger can be any pattern and at a random location
    on the image. Images are taken from [[15](#bib.bib15)].'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：动态和静态特洛伊触发器的示例[[15](#bib.bib15)]。 (a) 固定模式触发器总是位于左上角，这使得其检测更容易。 (b) 特洛伊触发器可以是任何模式，并且位于图像的随机位置。图像来自[[15](#bib.bib15)]。
- en: Whereas more and more contributions are focused on hiding Trojan triggers to
    avoid visual detection, there are also studies [[7](#bib.bib7), [8](#bib.bib8),
    [11](#bib.bib11), [12](#bib.bib12)] that concern themselves with the trade-off
    between the effectiveness and stealthiness of Trojan triggers. The emerging consensus
    of these works seem to be that although less perceptible attacks help in bypassing
    detection methods - especially those based on the appearance difference of the
    Trojaned and benign samples - they usually have lower attack success rates. In
    order to overcome this problem, Doan et al. [[118](#bib.bib118)] proposed the
    Learnable, Imperceptible and Robust Backdoor Attack (LIRA) that lets the trigger
    generator function to learn how to modified the input with imperceptible noise
    while maximising the attack success rate. They first find the optimal trigger
    function and the poisoned model with best performance. Then they fine-tune the
    poisoned model for stealthiness. Using this method, they established a stealthy
    conditional trigger which has the size of 1/1000 to 1/200x of the input sample.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管越来越多的研究专注于隐藏特洛伊木马触发器以避免视觉检测，但也有研究[[7](#bib.bib7)、[8](#bib.bib8)、[11](#bib.bib11)、[12](#bib.bib12)]关注特洛伊木马触发器效果与隐蔽性之间的权衡。这些研究的共识似乎是，尽管较不易被察觉的攻击有助于绕过检测方法——特别是那些基于特洛伊木马样本与良性样本外观差异的方法——但这些攻击通常成功率较低。为了克服这个问题，Doan等人[[118](#bib.bib118)]提出了可学习、不可察觉且鲁棒的后门攻击（LIRA），该方法使触发器生成器能够学习如何通过不可察觉的噪声修改输入，同时最大化攻击成功率。他们首先找到最佳的触发器函数和具有最佳性能的污染模型。然后，他们对污染模型进行细化以提高隐蔽性。通过这种方法，他们建立了一个隐蔽的条件触发器，其大小为输入样本的1/1000至1/200倍。
- en: Recently, Doan et al. [[117](#bib.bib117)] claimed that the main reason for
    the lower attack success rate for invisible attacks is that they are likely to
    leave tangible footprints in the latent or feature space which can be easily detected.
    To bypass that, they proposed Wasserstein Backdoor, which injects an invisible
    noise into the input samples while adjusting the latent representation of the
    modified input samples to make sure their resemblance to benign samples. It is
    claimed that doing so can reach a very high attack success rate while retaining
    stealthiness.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Doan等人[[117](#bib.bib117)]声称，隐形攻击成功率较低的主要原因是它们可能在潜在空间或特征空间中留下可检测的实质性痕迹。为此，他们提出了Wasserstein后门攻击，该方法向输入样本中注入不可见噪声，同时调整修改后输入样本的潜在表示，以确保其与良性样本的相似性。声称这样做可以在保持隐蔽性的同时达到非常高的攻击成功率。
- en: '![Refer to caption](img/fe0896052a7ef1611313cb4900264aa8.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/fe0896052a7ef1611313cb4900264aa8.png)'
- en: 'Figure 4: BadNets [[5](#bib.bib5)] and Sample-Specific Backdoor [[114](#bib.bib114)].
    BadNets uses a uniform visible trigger that classify the Trojaned image to the
    same class whereas Sample-Specific Backdoor can have multiple stealthy triggers
    and each of them map to a specific class. Image taken from  [[114](#bib.bib114)].'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：BadNets [[5](#bib.bib5)] 和样本特定后门[[114](#bib.bib114)]。BadNets 使用统一的可见触发器将特洛伊木马图像分类到相同类别，而样本特定后门可以有多个隐蔽触发器，每个触发器映射到特定类别。图像来源于[[114](#bib.bib114)]。
- en: '![Refer to caption](img/8bd5155fd6d4a23eb723ef78d0e21726.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8bd5155fd6d4a23eb723ef78d0e21726.png)'
- en: 'Figure 5: Visualisation of different neural Trojan triggers. Top: Left to right
    includes the benign image, image with BadNets [[5](#bib.bib5)], blended backdoor [[6](#bib.bib6)],
    sinusoidial strips based backdoor (SIG) [[127](#bib.bib127)], reflection backdoor
    (ReFool) [[8](#bib.bib8)], WaNet [[120](#bib.bib120)] and LIRA [[118](#bib.bib118)].
    The images at the bottom are their corresponding triggers that are 2 times amplified.
    Images are adapted from [[118](#bib.bib118)].'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同神经特洛伊木马触发器的可视化。顶部：从左到右依次为良性图像、BadNets [[5](#bib.bib5)]、混合后门[[6](#bib.bib6)]、基于正弦条纹的后门（SIG）[[127](#bib.bib127)]、反射后门（ReFool）[[8](#bib.bib8)]、WaNet
    [[120](#bib.bib120)] 和LIRA [[118](#bib.bib118)]。底部的图像为其对应的触发器，放大了2倍。图像改编自[[118](#bib.bib118)]。
- en: Nguyen et al. [[120](#bib.bib120)] argued that existing neural Trojans use noise
    as triggers which makes them detectable by humans. Inspired, they proposed WaNet,
    which is designed by image warping. Their attack is implemented using a small
    and smooth warping field to achieve stealthiness. Cheng et al. [[121](#bib.bib121)]
    proposed a deep feature space Trojan that is stealthier and harder to defend compared
    to many existing attacks. It is designed based on the assumption that the model
    and the training set are accessible and the attacker has the control over the
    training process. Once the model has completed the malicious training, the Trojaned
    model will be released to the public. The attacker holds a secrete trigger generator
    to activate the attack so that the imperceptible feature trigger will be stamped
    on the input when the input passes the trigger generator and causes the model
    malfunctioning. However, the model behaves normally when the inputs are passed
    straight to the model instead of going through a trigger generator. Zhao et al. [[124](#bib.bib124)]
    also introduced an attack designed on per-class basis. Their framework is gradient-based,
    which designs the final Trojaned image by modifying its feature information.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Nguyen 等人 [[120](#bib.bib120)] 认为现有的神经木马使用噪声作为触发器，这使得它们可以被人类检测到。受到启发，他们提出了 WaNet，这是一种通过图像变形设计的攻击。他们的攻击是通过使用小而平滑的变形场来实现隐蔽性的。Cheng
    等人 [[121](#bib.bib121)] 提出了一个深度特征空间木马，这种木马比许多现有的攻击更加隐蔽且更难防御。它是基于假设模型和训练集是可访问的，并且攻击者对训练过程具有控制权。模型完成恶意训练后，木马模型将被公开发布。攻击者持有一个秘密触发器生成器来激活攻击，以便不可察觉的特征触发器会在输入通过触发器生成器时被印在输入上，导致模型出现故障。然而，当输入直接传递给模型而不是经过触发器生成器时，模型会正常工作。赵等人
    [[124](#bib.bib124)] 还介绍了一种基于每类设计的攻击。他们的框架是基于梯度的，通过修改特征信息来设计最终的木马图像。
- en: 2.2 Non-poisoning Based Methods
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 非毒化基础方法
- en: The aforementioned methods insert Trojan in models by poisoning the training
    data. In this section, we focus on the methods that are not limited to ‘data poisoning’
    for Trojan embedding. Generally, the backdoors induced by these methods result
    from modifying other training parameters or the model weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 前述方法通过毒化训练数据来在模型中插入木马。在本节中，我们将重点关注那些不局限于“数据毒化”的木马嵌入方法。一般来说，这些方法诱发的后门是通过修改其他训练参数或模型权重来实现的。
- en: Clements et al. [[16](#bib.bib16)] proposed a method that injects Trojan by
    altering the computing operations of the neural network. Their method assumes
    that the attackers has full access to the model, including reading and modifying
    the parameters of the model. At the time, most of the Trojan defense techniques
    were based on attack detection by analysing the model parameters. Clement et al. [[16](#bib.bib16)]
    demonstrated that their technique can bypass this type of defense mechanism because
    the model weights are not directly modified by their attack.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Clements 等人 [[16](#bib.bib16)] 提出了一种通过改变神经网络计算操作来注入木马的方法。他们的方法假设攻击者对模型具有完全访问权限，包括读取和修改模型参数。当时，大多数木马防御技术都是通过分析模型参数来进行攻击检测。Clement
    等人 [[16](#bib.bib16)] 证明了他们的技术可以绕过这种类型的防御机制，因为他们的攻击并没有直接修改模型权重。
- en: Dumford et al. [[17](#bib.bib17)] proposed a method based on directly perturbing
    the learned weights within the neural network. Instead of modifying the weights
    through training data poisoning, their method identifies target weights with a
    greedy search across all the weights. Then, the selected target weights are directly
    perturbed for Trojan introduction. Their method is tested on facial recognition
    systems where the input is not modifiable. It is claimed that the technique can
    grant access to irrelevant users in the systems while still working normally for
    the relevant users. Rakin et al. [[18](#bib.bib18)] also noted that data poisoning
    is among the most common methodologies of embedding Trojans in the models. They
    deviate from this conventional strategy by proposing a technique that does not
    require access to training data. They assumed that the attackers have thorough
    understanding on the neural network’s weights and activations and proposed a method
    termed Targeted Bit Trojan (TBT). The TBT works by firstly locating vulnerable
    bits of the model weights in the memory of the computer using gradient ranking
    approach. It then induces malicious behaviour by flipping the vulnerable bits.
    It is shown that this method is efficient as it only requires 84 bits to be flipped
    out of 88 million bits of a model, while still achieving up to 92% attack success
    rate.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Dumford 等人[[17](#bib.bib17)] 提出了一个基于直接扰动神经网络中学习权重的方法。他们的方法不是通过训练数据中毒来修改权重，而是通过对所有权重进行贪婪搜索来识别目标权重。然后，直接扰动选定的目标权重以引入
    Trojan。他们的方法在输入不可修改的面部识别系统上进行了测试。该技术声称可以在系统中授予不相关用户访问权限，同时对相关用户仍然正常工作。Rakin 等人[[18](#bib.bib18)]
    也指出，数据中毒是嵌入 Trojan 到模型中最常见的方法之一。他们通过提出一种不需要访问训练数据的技术来偏离这种传统策略。他们假设攻击者对神经网络的权重和激活有深入了解，并提出了一种称为
    Targeted Bit Trojan (TBT) 的方法。TBT 首先使用梯度排名方法定位计算机内存中模型权重的脆弱位。然后通过翻转这些脆弱位来诱发恶意行为。研究表明，该方法高效，只需翻转
    84 位中的 88 万亿位，同时仍能实现高达 92% 的攻击成功率。
- en: Instead of modifying the parameters of the models directly, Guo et al. [[21](#bib.bib21)]
    further improved Trojan injection and introduced a method called TrojanNet that
    inserts Trojan via secret weight permutation. It is claimed that this method has
    the advantage that NP-complete technique is required to examine the existence
    of Trojan. Thus, it is virtually impossible to be detected by the state-of-art
    Trojan detection techniques. Also, instead of parameters adjustment, Tang et al. [[22](#bib.bib22)]
    introduced a trained Trojan module insertion. Their method retrains the model
    with this module that is much smaller in size, which improves the efficiency of
    Trojan injection as it consumes much less computational power.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接修改模型参数不同，Guo 等人[[21](#bib.bib21)] 进一步改进了 Trojan 注入，提出了一种名为 TrojanNet 的方法，通过秘密的权重排列插入
    Trojan。该方法声称具有一个优势，即需要 NP 完全技术来检查 Trojan 的存在。因此，它几乎无法被最先进的 Trojan 检测技术检测到。此外，Tang
    等人[[22](#bib.bib22)] 提出了一个训练的 Trojan 模块插入方法，而不是调整参数。他们的方法通过这个尺寸更小的模块重新训练模型，提高了
    Trojan 注入的效率，因为它消耗的计算能力要少得多。
- en: Bagdasaryan et al [[19](#bib.bib19)] proposed a Trojan injection technique that
    exploits the loss-value computation of the training process by accessing the software
    implementation of the training process. This method shows high attack success
    rate while retaining high accuracy on the benign input. Nevertheless, the method
    has its limitations as the attacker is not able to observe model training and
    the resulting model. Similarly, Liu et al. [[20](#bib.bib20)] also utilised software
    access during model training for Trojan injection. They proposed a method termed
    Stealth INfection (SIN) that inserts Trojan through software that is executable
    during the run time. They embed the Trojan into the redundant memory space of
    the neural network weights, which is seen as malicious payload for the original
    neural network. When the users invoke the services, the Trojan activates through
    the execution of Trojan code and the malicious payload is removed from the infected
    model for stealthiness.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Bagdasaryan 等人[[19](#bib.bib19)] 提出了一个 Trojan 注入技术，通过访问训练过程的软件实现来利用训练过程中的损失值计算。这种方法在保持良性输入的高准确率的同时，表现出较高的攻击成功率。然而，该方法存在局限性，因为攻击者无法观察模型训练过程及结果模型。类似地，Liu
    等人[[20](#bib.bib20)] 也在模型训练期间利用软件访问进行 Trojan 注入。他们提出了一种称为 Stealth INfection (SIN)
    的方法，通过在运行时可执行的软件插入 Trojan。他们将 Trojan 嵌入到神经网络权重的冗余内存空间中，这被视为对原始神经网络的恶意负载。当用户调用服务时，Trojan
    通过执行 Trojan 代码激活，并且恶意负载从感染模型中移除以保持隐蔽性。
- en: '![Refer to caption](img/f3b4507d015d0bd7e54c475d3c2d2b8a.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3b4507d015d0bd7e54c475d3c2d2b8a.png)'
- en: 'Figure 6: Attack success rate for a Physical space attack [[26](#bib.bib26)].
    Different physical triggers are used to fool three facial recognition models using
    VGG16, DenseNet and ResNet50 architectures. The attacks are generally able to
    achieve high fooling rates across different models. The figure is adapted from
    [[26](#bib.bib26)].'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：针对物理空间攻击的成功率[[26](#bib.bib26)]。使用不同的物理触发器来欺骗使用 VGG16、DenseNet 和 ResNet50
    架构的三个面部识别模型。这些攻击通常能够在不同模型中实现较高的欺骗率。该图取自 [[26](#bib.bib26)]。
- en: Li et al. [[23](#bib.bib23)] pointed out a drawback of hardware based Trojans.
    That is, although such methods can cause the neural networks to malfunction, they
    do not generalize well to unseen images. Software Trojan can improve this, but
    they require perturbing input data, which is not always accessible in practical
    scenarios. Hence, [[23](#bib.bib23)] introduced a hardware-software collaborative
    framework for Trojan injection while keeping the Trojan signature imperceptible.
    The method involves training a part of the neural network maliciously without
    requiring manipulation of the inputs. The implementation of Trojan circuit is
    implemented in hardware in either an add-tree or multiply-accumulate structure,
    and the software part of Trojan is injected in selected weights of the original
    neural network at the training time. The authors tested the framework for image
    classification on CIFAR10 dataset [[128](#bib.bib128)], and for facial recognition
    on YouTube Faces dataset [[129](#bib.bib129)]. Their method achieves attack success
    rate of 92.6% and 100% respectively while still retaining the original accuracy
    on the benign samples.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人[[23](#bib.bib23)] 指出了基于硬件的 Trojan 的一个缺点。也就是说，尽管这些方法可以导致神经网络出现故障，但它们在处理未见过的图像时表现不佳。软件
    Trojan 可以改善这一点，但它们需要扰动输入数据，而这在实际场景中并不总是可用。因此，[[23](#bib.bib23)] 提出了一个硬件-软件协作框架用于
    Trojan 注入，同时保持 Trojan 的特征不可察觉。该方法涉及恶意地训练神经网络的一部分，而无需对输入进行操控。Trojan 电路的实现以加法树或乘加结构的形式在硬件中实现，Trojan
    的软件部分在训练期间注入到原始神经网络的选定权重中。作者在 CIFAR10 数据集[[128](#bib.bib128)] 上测试了该框架进行图像分类，并在
    YouTube Faces 数据集[[129](#bib.bib129)] 上进行了面部识别。其方法分别实现了 92.6% 和 100% 的攻击成功率，同时在良性样本上仍保持原始准确性。
- en: Li et al. [[122](#bib.bib122)] proposed a reverse-engineering method that can
    inject neural Trojan to the compiled model. The attack is implemented by constructing
    a neural conditional branch that is attached to a trigger detector and some operators.
    The branch is then injected as the malicious payload into the target model. Since
    this attack can be implemented without knowing any background information of the
    benign model and that the logic of the conditional branch can be customised, the
    attack is pragmatic. Salem et al. [[24](#bib.bib24)] argued that as long as trigger
    exists in the input, there is always a way to detect Trojan by finding the trigger.
    This makes it difficult to activate the attacks in the presence of effective trigger
    detectors. Based on this argument, they proposed a Triggerless Backdoor attack,
    which can activate Trojan without the need to modify the input. They applied a
    dropout technique that erases some target neurons in the neural network at training
    time to alter the model’s functionality and produce the target-specific label.
    The neural network is trained in the way that if the target neurons are missing,
    Trojan activates. Hence, at testing time and future predictions, the attacker
    can simply drop out these neurons to trigger model’s malicious behaviour.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人[[122](#bib.bib122)] 提出了一种反向工程方法，可以将神经特洛伊木马注入编译后的模型。该攻击通过构建一个附加到触发器检测器和一些运算符的神经条件分支来实现。然后，将该分支作为恶意有效载荷注入到目标模型中。由于这种攻击可以在不知晓良性模型任何背景信息的情况下实现，并且条件分支的逻辑可以自定义，因此这种攻击具有实用性。Salem
    等人[[24](#bib.bib24)] 认为，只要输入中存在触发器，总有办法通过找到触发器来检测特洛伊木马。这使得在存在有效触发器检测器的情况下，很难激活攻击。基于这一论点，他们提出了一种无触发器后门攻击，可以在无需修改输入的情况下激活特洛伊木马。他们应用了一种在训练时擦除神经网络中某些目标神经元的
    dropout 技术，以改变模型的功能并生成目标特定标签。神经网络的训练方式是，如果目标神经元缺失，特洛伊木马将被激活。因此，在测试时和未来预测中，攻击者可以简单地丢弃这些神经元以触发模型的恶意行为。
- en: 2.3 Beyond the digital space of classifiers
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 超越分类器的数字空间
- en: The methods discussed in the previous two sections are mainly focused on attacks
    in the digital space while considering the classification task. However, interaction
    and utility of deep learning is neither limited to the digital space nor the classification
    task. There are other spaces and tasks such as physical space, natural language
    processing and speech verification etc., that are equally relevant in terms of
    susceptibility to Trojan attacks. We dedicate this section to cover Trojan injection
    methods that go beyond the digital space classification problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前两节讨论的方法主要集中在考虑分类任务的数字空间攻击上。然而，深度学习的互动和应用不仅限于数字空间或分类任务。还有其他领域和任务，如物理空间、自然语言处理和语音验证等，这些领域在特洛伊木马攻击的易受攻击性方面同样相关。本节将介绍超越数字空间分类问题的特洛伊木马注入方法。
- en: 2.3.1 Physical space attacks
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 物理空间攻击
- en: Chen et al.  [[25](#bib.bib25)] investigated a more realistic scenario where,
    (i) the attackers have no previous knowledge of the model and the training datasets,
    and (ii) only a small amount of poisoning training data can be used for training
    the target model without users noticing, and (iii) the Trojan trigger is to be
    kept unnoticeable for stealthiness. The authors proposed a method for facial recognition
    attack through the photos that are taken from different angles and used glasses
    as Trojan triggers. Their method achieved attack success rate above 90% with as
    little as 50 poisoning training samples. Wenger et al. [[26](#bib.bib26)] also
    designed an attack against facial recognition systems in the physical space by
    using physical objects as Trojan triggers, see Fig. [6](#S2.F6 "Figure 6 ‣ 2.2
    Non-poisoning Based Methods ‣ 2 Injection of Neural Trojan ‣ A Survey of Neural
    Trojan Attacks and Defenses in Deep Learning"). They also demonstrated that the
    state-of-the-art digital-space defense techniques a often rendered ineffective
    in detecting this type of Trojan. Gu et al. [[27](#bib.bib27)] designed a special
    trigger that recognises stop signs on the street as speed limits signs when the
    Trojan activates. Similarly, Li et al. [[28](#bib.bib28)] demonstrated that objects
    in the physical world may encounter transformations that change the location and
    appearance of the trigger on the target object, hence the digital attacks do not
    transfer well to the physical work. Therefore, they proposed a transformation-invariant
    attack that allows the trigger to keep its strength under such transformations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 [[25](#bib.bib25)] 研究了一种更现实的情景，其中，（i）攻击者对模型和训练数据集没有先前的了解，（ii）只有少量的污染训练数据可以用于训练目标模型而不被用户察觉，（iii）特洛伊触发器需保持隐蔽以保证隐秘性。作者提出了一种通过拍摄不同角度的照片进行面部识别攻击的方法，并使用眼镜作为特洛伊触发器。他们的方法在仅使用
    50 个污染训练样本的情况下，攻击成功率超过 90%。Wenger 等人 [[26](#bib.bib26)] 还设计了一种针对物理空间中面部识别系统的攻击，使用物理对象作为特洛伊触发器，见图
    [6](#S2.F6 "图 6 ‣ 2.2 非污染基础方法 ‣ 2 神经特洛伊的注入 ‣ 深度学习中神经特洛伊攻击和防御的综述")。他们还展示了最先进的数字空间防御技术在检测这种特洛伊攻击时常常效果不佳。Gu
    等人 [[27](#bib.bib27)] 设计了一种特殊的触发器，当特洛伊激活时能将街上的停车标志识别为限速标志。类似地，Li 等人 [[28](#bib.bib28)]
    证明了物理世界中的物体可能经历变化，这些变化会改变目标物体上触发器的位置和外观，因此数字攻击在物理世界中传递效果不佳。因此，他们提出了一种变换不变攻击，允许触发器在这些变换下保持其有效性。
- en: 2.3.2 Attacks on language models
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 对语言模型的攻击
- en: While the vast majority of contributions in Trojan attacks on deep learning
    focuses on visual models, susceptibility of audio models is also explored in the
    literature. Dai et al. [[29](#bib.bib29)] firstly explored this direction and
    proposed a BadNets-like technique for audio models. They used emotionally neutral
    sentences as triggers that are randomly embedded into benign inputs for training
    a Trojaned model. Chen et al. [[30](#bib.bib30)] further enhanced [[29](#bib.bib29)]
    by improving the efficiency of triggers at characters, words and sentence levels,
    reporting high attack success rates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数关于特洛伊攻击的研究集中在视觉模型上，但音频模型的易受攻击性在文献中也有所探讨。Dai 等人 [[29](#bib.bib29)] 首次探索了这一方向，并提出了一种类似于
    BadNets 的技术用于音频模型。他们使用情感中立的句子作为触发器，这些句子被随机嵌入到用于训练特洛伊模型的良性输入中。Chen 等人 [[30](#bib.bib30)]
    通过提高在字符、词语和句子层面的触发器效率进一步增强了 [[29](#bib.bib29)]，报告了高攻击成功率。
- en: 2.3.3 Trojans in Transfer Learning
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 迁移学习中的特洛伊攻击
- en: Gu et al. [[31](#bib.bib31)] proposed to inject Trojans in transferred models.
    They perceived transfer learning as fine-tuning of a pre-trained teacher model
    to obtain a new student model. The authors successfully insert Trojans into the
    student model through malicious transfer learning. Tan et al. [[32](#bib.bib32)]
    demonstrated the differences between the distribution of the latent representation
    of clean and Trojaned models. They argued that Trojans can be detected based on
    this distribution difference. Hence, they proposed a method to avoid detection
    by bringing the latent representation of the clean and Trojaned model closer.
    Similarly, Yao et al. [[33](#bib.bib33)] also focused on the latent representation
    of the models and proposed a latent backdoor attack. Their method allows the student
    model to copy all the parameters and relationship of the teacher model, except
    for the last few layers. The student and teacher models then differ in the representation
    of those layers. When the latent backdoor is injected into the teacher model,
    it remains inactive, and the teacher model retains its normal functionality. However,
    during transfer learning, the latent backdoor switches on in the student model,
    resulting in malfunctioning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 等人[[31](#bib.bib31)]提出将木马注入转移模型。他们将迁移学习视为对预训练教师模型的微调，以获得一个新的学生模型。作者通过恶意迁移学习成功将木马插入学生模型。Tan
    等人[[32](#bib.bib32)]展示了干净模型和木马模型潜在表示分布之间的差异。他们认为可以基于这种分布差异检测木马。因此，他们提出了一种方法来避免检测，即使干净模型和木马模型的潜在表示更接近。同样，Yao
    等人[[33](#bib.bib33)]也关注模型的潜在表示，并提出了一种潜在后门攻击。他们的方法允许学生模型复制教师模型的所有参数和关系，除了最后几层。学生模型和教师模型在这些层的表示上有所不同。当潜在后门被注入到教师模型中时，它保持非激活状态，教师模型保持正常功能。然而，在迁移学习过程中，潜在后门会在学生模型中激活，导致功能失常。
- en: 2.3.4 Miscellaneous attacks
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 其他攻击
- en: There are also other Trojan attack techniques that are tailored to particular
    types of models or the tasks at hand. For example, the Trojan methods are also
    developed for graphs, which use sub-graphs as a the trigger [[34](#bib.bib34),
    [35](#bib.bib35)]. Similarly, we also witness examples of Trojan attacks in reinforcement
    learning [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]. In collaborative
    learning, Bagasaryan et al. [[39](#bib.bib39)] inserted Trojans in the models
    by amplifying the poisoned gradient of node servers and Bhagoji et al. [[40](#bib.bib40)]
    obtained Trojaned model through model poisoning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的木马攻击技术，这些技术是针对特定类型的模型或任务量身定制的。例如，木马方法也被开发用于图模型，这些方法使用子图作为触发器[[34](#bib.bib34),
    [35](#bib.bib35)]。同样，我们也看到在强化学习中出现木马攻击的例子[[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39)]。在协作学习中，Bagasaryan 等人[[39](#bib.bib39)]通过放大节点服务器的污染梯度将木马插入模型，而
    Bhagoji 等人[[40](#bib.bib40)]则通过模型中毒获得了被木马攻击的模型。
- en: More recently, Salem et al. [[41](#bib.bib41)] proposed a Trojan attack against
    autoencoders and Generative Adversarial Networks (GANs). Each autoencoder is made
    up of an encoder that maps input to a latent vector, and a decoder that decodes
    the latent vector back to a similar input. The Salem et al. inserted the Trojan
    into autoencoders in a two step method. First, they added Trojan trigger to the
    input. Second, they trained the model by utilising a loss function on the Trojanned
    input and the decoded image. The Trojan affects the model and controls the decoded
    image of a triggered sample. GANs are made up of a generator that generates a
    sample and a discriminator that examines if the generated sample is realistic
    enough. The Trojan injection process of [[41](#bib.bib41)] for GANs is similar
    to that of autoencoders that replaces the autoencoder with a GAN. It adds Trojan
    trigger by modifying the input noise of the generator with a single value instead
    of stamping a pre-defined pattern. The Trojaned GAN generates samples from the
    original distribution if the input noise vectors are clean. Otherwise, it generates
    samples from a target-specific distribution.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Salem 等人 [[41](#bib.bib41)] 提出了针对自编码器和生成对抗网络（GAN）的木马攻击。每个自编码器由一个将输入映射到潜在向量的编码器和一个将潜在向量解码回类似输入的解码器组成。Salem
    等人将木马以两步法插入自编码器中。首先，他们在输入中添加木马触发器。其次，他们通过利用木马输入和解码图像的损失函数来训练模型。木马影响模型并控制触发样本的解码图像。GAN
    由一个生成样本的生成器和一个检查生成样本是否足够真实的鉴别器组成。[[41](#bib.bib41)] 对 GAN 的木马注入过程类似于对自编码器的过程，只是将自编码器替换为
    GAN。它通过用单一值修改生成器的输入噪声来添加木马触发器，而不是压印预定义的模式。如果输入噪声向量干净，木马 GAN 生成来自原始分布的样本。否则，它会生成来自特定目标分布的样本。
- en: 3 Non-adversarial applications of Trojans
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**木马的三种非对抗性应用**'
- en: Although above we have discussed Trojans as attacks; exploited by attackers
    with a malicious intent, there are also instances of utilizing them for non-adversarial
    purposes. Adi et al. [[42](#bib.bib42)] applied Trojan for watermarking to verify
    authentications, and increasing robustness of the models. Their watermarking scheme
    consists of three stages. (i) Generate a secrete marking key, say $m_{k}$, and
    a public verification key, $v_{k}$. The $m_{k}$ is injected into the sample as
    watermark and the $v_{k}$ is used for watermark detection when it is required.
    (ii) Inject the watermark into the target model as Trojan. (iii) Verify the presence
    of watermark at the test time. The verification requires a ($m_{k}$, $v_{k}$)
    matching pair such that if they mismatch, no authentication will be granted. This
    watermarking scheme fulfills the requirements of functionality preserving, unremovability,
    unforgeability and enforces the non-trivial ownership. However, in this case,
    it is not known that how much modification is required for a third-party to obtain
    their ownership of the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管上述讨论了木马作为攻击的情况；被攻击者以恶意意图利用，但也有利用木马进行非对抗性目的的实例。Adi 等人 [[42](#bib.bib42)] 将木马应用于水印以验证认证，并提高模型的鲁棒性。他们的水印方案包括三个阶段。
    (i) 生成一个秘密标记密钥 $m_{k}$ 和一个公共验证密钥 $v_{k}$。$m_{k}$ 被注入样本作为水印，$v_{k}$ 用于在需要时进行水印检测。
    (ii) 将水印作为木马注入目标模型。 (iii) 在测试时验证水印的存在。验证需要一个 ($m_{k}$, $v_{k}$) 匹配对，如果它们不匹配，则不会授予认证。这个水印方案满足功能保持、不可移除、不可伪造的要求，并强制执行非平凡的所有权。然而，在这种情况下，尚不清楚第三方获得模型所有权需要多少修改。
- en: '![Refer to caption](img/236fe6397bd0c9a9e38758631a969cfb.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/236fe6397bd0c9a9e38758631a969cfb.png)'
- en: 'Figure 7: Titration analysis of different neural networks with and without
    Trojan on different types of datasets [[50](#bib.bib50)]. The $\sigma$ indicates
    noise standard deviation and k* is the Trojan target. The titration scores for
    Trojaned model increase dramatically as the titration level increases with exception
    of (e) while the titration scores gradually rise for benign model except for case
    (e) and (f). Image taken from [[50](#bib.bib50)].'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：不同神经网络在不同数据集上有无木马的滴定分析 [[50](#bib.bib50)]。$\sigma$ 表示噪声标准差，k* 是木马目标。木马模型的滴定分数随着滴定水平的增加而显著上升，除了
    (e) 外，而良性模型的滴定分数逐渐上升，除了 (e) 和 (f) 情况。图片来源 [[50](#bib.bib50)]。
- en: Shan et al. [[43](#bib.bib43)] proposed a trapdoor-based adversarial attack
    detection scheme. It tunes the weights of the neural network for the convergence
    of the gradient-descent-based adversarial example generation algorithm at trapdoor
    adversarial examples. Due to the resulting convergence, the users can observe
    the presence of the trapdoor examples to detect if any attack exists in the neural
    network. In [[44](#bib.bib44)], Sommer et al. demonstrated that users can embed
    Trojan into any data that needs to be deleted by modifying data with trigger and
    a target label. Then a Trojan detection technique can be applied to verify if
    the data is actually deleted by the server. Furthermore, Li et al. [[45](#bib.bib45)]
    used Trojan for the protection of open-sourced datasets. Zhao et al. [[46](#bib.bib46)]
    used Trojans for neural network interpretability and Lin et al. [[47](#bib.bib47)]
    leveraged Trojans for the evaluation of explainable Artificial Intelligence methods.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Shan 等人 [[43](#bib.bib43)] 提出了基于陷门的对抗攻击检测方案。该方案调整神经网络的权重，以便在陷门对抗样本的梯度下降生成算法收敛。由于这种收敛，用户可以观察到陷门样本的存在，以检测神经网络中是否存在攻击。在
    [[44](#bib.bib44)] 中，Sommer 等人展示了用户可以通过修改数据的触发器和目标标签，将特洛伊木马嵌入任何需要删除的数据中。然后可以应用特洛伊木马检测技术来验证数据是否已被服务器实际删除。此外，Li
    等人 [[45](#bib.bib45)] 使用特洛伊木马来保护开源数据集。Zhao 等人 [[46](#bib.bib46)] 将特洛伊木马用于神经网络可解释性，而
    Lin 等人 [[47](#bib.bib47)] 利用特洛伊木马评估可解释的人工智能方法。
- en: 4 Defenses Against Trojan Attacks
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 对抗特洛伊木马攻击的防御
- en: While more and more Trojan injection techniques are being devised by researchers
    to maximise attack stealthiness and effectiveness, many Trojan defense techniques
    are also appearing in the literature. These techniques include both detection
    and bypassing of the Trojan, and even removal of the backdoor from the models.
    In this section, we discuss the contributions proposing defense mechanisms against
    neural Trojans.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着研究人员制定越来越多的特洛伊木马注入技术，以最大化攻击的隐蔽性和有效性，许多特洛伊木马防御技术也在文献中出现。这些技术包括对特洛伊木马的检测、绕过，甚至从模型中移除后门。在这一部分，我们探讨了提出针对神经特洛伊木马防御机制的贡献。
- en: 4.1 Model Verification
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型验证
- en: Put simply, this line of Trojan detection mechanism detects the existence of
    Trojan by verifying the efficacy of the model. If there exist anomalies in the
    functionalities of the model under consideration, then a flag is raised for a
    potential Trojan.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这种特洛伊木马检测机制通过验证模型的有效性来检测特洛伊木马的存在。如果模型功能中存在异常，则会发出潜在特洛伊木马的警告。
- en: Baluta et al. [[48](#bib.bib48)] proposed a framework to provide PAC-style soundness
    guarantee and designed NPAC to evaluate how well P holds over N with guarantees
    when a set of trained neural networks (N) and a property (P) is given. If a neural
    Trojan is present in the model, the user can retrain the neural network with benign
    samples and check whether the removal is successful by applying NPAC. He et al. [[49](#bib.bib49)]
    proposed a different approach termed Sensitive-Sample Fingerprinting where some
    samples are designed to be very sensitive to the parameters of the trained neural
    network. When these sensitive samples are passed to the model for classification
    and the output mismatches with the sample’s actual label, it indicates that a
    neural Trojan might have been present in the model. Erichson et al. [[50](#bib.bib50)]
    studied how neural networks respond to images containing noise with different
    intensity levels, and summed it up using titration curves. They found that there
    is a specific manner in which neural networks respond in the presences of neural
    Trojan, see Fig. [7](#S3.F7 "Figure 7 ‣ 3 Non-adversarial applications of Trojans
    ‣ A Survey of Neural Trojan Attacks and Defenses in Deep Learning"). Inspired
    by this, they suggested a method to detect Trojan based on the reaction of neural
    networks to noise. Huster et al. [[115](#bib.bib115)] argued that due to the complex
    training process of the neural network, it is impractical to assume full access
    to all the training data. They claimed that adversarial perturbations transfer
    more effectively across images for Trojaned model as compared to clean models.
    Based on this observation, they are able to identify the Trojaned model without
    the access to the training data or any information about the neural Trojan triggers.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Baluta 等人 [[48](#bib.bib48)] 提出了一个框架，以提供 PAC 风格的健全性保证，并设计了 NPAC 来评估当给定一组训练好的神经网络
    (N) 和一个属性 (P) 时，P 在 N 上保持的程度。如果模型中存在神经木马，用户可以用良性样本重新训练神经网络，并通过应用 NPAC 检查去除是否成功。He
    等人 [[49](#bib.bib49)] 提出了一个不同的方法，称为敏感样本指纹识别，其中某些样本被设计为对训练神经网络的参数非常敏感。当这些敏感样本传递给模型进行分类且输出与样本的实际标签不匹配时，表示模型中可能存在神经木马。Erichson
    等人 [[50](#bib.bib50)] 研究了神经网络如何对不同强度的噪声图像作出反应，并通过滴定曲线总结了这一现象。他们发现神经网络在存在神经木马时有一种特定的反应方式，见图
    [7](#S3.F7 "Figure 7 ‣ 3 Non-adversarial applications of Trojans ‣ A Survey of
    Neural Trojan Attacks and Defenses in Deep Learning")。受到此启发，他们建议了一种基于神经网络对噪声反应的木马检测方法。Huster
    等人 [[115](#bib.bib115)] 认为，由于神经网络的复杂训练过程，假设对所有训练数据的完全访问是不现实的。他们声称，相较于干净模型，木马模型中的对抗扰动在图像间传播得更有效。基于这一观察，他们能够在没有训练数据或任何关于神经木马触发器的信息的情况下识别木马模型。
- en: 4.2 Trojan Trigger Detection
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 木马触发检测
- en: This type of defense mechanism aims at detecting Trojans by finding the presence
    of triggers in the inputs. As many of the emerging Trojan attacks are focusing
    on rendering triggers invisible, their detection is becoming increasingly challenging.
    Liu et al. [[51](#bib.bib51)] firstly fine-tuned a state-of-the-art classifier
    to detect Trojan trigger as anomaly to the input image. Although this detection
    method is easy to implement, the false alarm rate is very high. Baracaldo et al. [[52](#bib.bib52)]
    presented another approach where the Trojaned input can be detected by evaluating
    its impact on the accuracy of the model. They grouped data points in the training
    data based on their meta-data. The training data is then passed to the model under
    the grouping for comparing model accuracy on them. If a certain group of data
    significantly degrades the accuracy of the model, they are identified to be Trojaned
    and the whole group of data is removed from the entire training set. There are
    also other methods that are subsequently designed keeping in view the broad concept
    used by Baracaldo et al. For instance, Liu et al. [[53](#bib.bib53)] showed that
    a Trojan trigger can be detected by simulating artificial brain. Chakarov et al. [[54](#bib.bib54)]
    argued that the detection will be more effective if individual data point is used
    for testing instead of the whole group and proposed a method for detection, called
    Probability of Sufficiency. Nelson et al. [[55](#bib.bib55)] employed a similar
    idea on individual data testing and demonstrated the efficiency of detection using
    a method called Reject on Negative Impact. However, although both [[54](#bib.bib54)]
    and [[55](#bib.bib55)] demonstrated effectiveness on testing with individual data
    points, their methods are not naturally scalable. Which is a concern considering
    the extremely large dataset sizes in modern deep learning literature [[3](#bib.bib3)].
    It seems that future improvements on detection methods will focus on the trade-off
    between effectiveness and scalability.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种防御机制的目标是通过在输入中寻找触发器的存在来检测特洛伊木马。由于许多新兴的特洛伊木马攻击都集中在使触发器不可见，因此它们的检测变得越来越具有挑战性。刘等人[[51](#bib.bib51)]首先对最先进的分类器进行了微调，将特洛伊触发器检测为对输入图像的异常。尽管这种检测方法易于实现，但误报率非常高。Baracaldo等人[[52](#bib.bib52)]提出了另一种方法，通过评估特洛伊输入对模型准确度的影响来检测特洛伊木马。他们根据数据点的元数据对训练数据进行了分组。然后，将分组后的训练数据传递给模型，以比较模型在这些数据上的准确度。如果某一组数据显著降低了模型的准确度，那么这些数据被识别为特洛伊木马，并且整个数据组会从整个训练集中移除。随后也设计了其他方法，参考了Baracaldo等人使用的广泛概念。例如，刘等人[[53](#bib.bib53)]展示了通过模拟人工大脑可以检测特洛伊触发器。Chakarov等人[[54](#bib.bib54)]认为，如果使用单独的数据点进行测试而不是整个数据组，检测会更有效，并提出了一种称为“充分性概率”的检测方法。Nelson等人[[55](#bib.bib55)]采用了类似的单独数据测试方法，并通过一种称为“拒绝负面影响”的方法展示了检测的效率。然而，尽管[[54](#bib.bib54)]和[[55](#bib.bib55)]在单个数据点测试中展示了有效性，但它们的方法并不自然具备可扩展性。考虑到现代深度学习文献中的极大数据集规模，这是一个令人担忧的问题[[3](#bib.bib3)]。未来对检测方法的改进似乎将侧重于效果与可扩展性之间的权衡。
- en: Chen et al. [[57](#bib.bib57)] took an alternative approach and proposed a method
    that does not require the model to be retrained, thereby not assuming direct access
    to the neural network. They designed DeepInspect, that detects Trojan triggers
    in 3 steps. First, it inverts the model for substitution training data recovery.
    Second, it reconstructs the trigger using a conditional generative adversarial
    network. Third, it uses anomaly detection for each reconstructed trigger to evaluate
    the probability of an input belonging to a class that is not the class that the
    model should return. During this anomaly detection, any suspected classification
    is flagged for further examination.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[57](#bib.bib57)]采取了另一种方法，提出了一种不需要重新训练模型的方案，因此不需要直接访问神经网络。他们设计了DeepInspect，该方法通过三个步骤检测特洛伊触发器。首先，它对模型进行反向操作以恢复替代训练数据。其次，它使用条件生成对抗网络重建触发器。第三，它利用异常检测对每个重建的触发器进行评估，以判断输入是否属于模型应该返回的类别之外的类别。在这个异常检测过程中，任何可疑的分类都会被标记以供进一步检查。
- en: While many of the Trojan trigger detections involve model training, Gao et al. [[58](#bib.bib58)]
    proposed a method termed STRong International Perturbation (STRIP) that enables
    detection during the model’s runtime. The main idea of STRIP is to recast the
    attacker’s ability to use an input-agnostic trigger as an asset for the victim
    to defend against a potential attack. STRIP is injected into the input that is
    to be passed to the potentially infected model. The clean inputs will be classified
    randomly by the model and show a random distribution of the probability of the
    final output class. However, the input with trigger would demonstrate an outstanding
    probability on the target-specific class. The entropy measurement can then be
    used to quantify this prediction randomness. This analysis identifies Trojaned
    inputs having low entropy change while clean inputs having high entropy change.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多木马触发器检测涉及模型训练，Gao等人[[58](#bib.bib58)] 提出了一种称为STRong International Perturbation（STRIP）的方法，能够在模型运行时进行检测。STRIP的主要思想是将攻击者使用输入无关触发器的能力重新塑造为受害者防御潜在攻击的资产。STRIP被注入到将传递给潜在感染模型的输入中。模型会随机分类干净输入，并显示最终输出类别概率的随机分布。然而，带有触发器的输入会在目标特定类别上显示出显著的概率。然后可以使用熵测量来量化这种预测的随机性。这种分析识别出木马输入具有低熵变化，而干净输入具有高熵变化。
- en: Xiang et al. [[59](#bib.bib59)] introduced an unsupervised anomaly detection
    that focuses on image classifiers during run-time. The method is devised based
    on the assumption of access to the trained classifier and the clean samples. This
    method can also assist attackers to learn the minimal size of perturbation required
    to cause the model’s misclassification. Kolouri et al. [[60](#bib.bib60)] designed
    Universal Litmus Patterns (ULPs) that are able to detect Trojans in convolutional
    neural networks while having no access to the training data. The authors passed
    ULPs to the neural network to get predictions which are subsequently used to detect
    the presence of Trojan. They also demonstrated that fast Trojan detection can
    be performed with the use of only a small subset of ULPs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 向等人[[59](#bib.bib59)] 引入了一种无监督异常检测方法，专注于运行时的图像分类器。该方法基于假设可以访问训练好的分类器和干净样本。这种方法还可以帮助攻击者学习引起模型误分类所需的最小扰动量。Kolouri等人[[60](#bib.bib60)]
    设计了通用试剂模式（ULPs），能够在没有训练数据访问权限的情况下检测卷积神经网络中的木马。作者将ULPs传递给神经网络以获取预测结果，这些预测结果随后用于检测木马的存在。他们还展示了仅使用少量ULPs就可以进行快速的木马检测。
- en: Xu et al. [[56](#bib.bib56)] proposed a method called Meta Neural Trojan model
    Detection (MNTD) that detects Trojan using a meta neural analysis techniques.
    The authors showed that a meta-classifier can be trained either using benign neural
    network (one-class learning) or by approximating and expanding the general distribution
    of the Trojaned model. Huang et al. [[61](#bib.bib61)] and Xu et al. [[66](#bib.bib66)]
    also adopted a similar overall strategy, but used an outlier detector as the meta-classifier.
    Huang et al. also implemented the Trojan detection method by using one-pixel signature
    representation to distinguish between Trojaned and benign models in [[62](#bib.bib62)].
    Wang et al. [[63](#bib.bib63)] proposed a method to distinguish between Trojaned
    and clean models in data-limited and data-free cases. Furthermore, Yoshida et
    al. [[64](#bib.bib64)] and Li et al. [[65](#bib.bib65)] shared the idea to use
    distillation method to erase trigger from the inputs. In [[65](#bib.bib65)], the
    authors employed a Neural Attention Distillation where a teacher model is used
    to fine-tune the student model via a small set of clean inputs. It is found that
    only 5% of the clean training data is sufficient to neutralize the Trojan under
    this approach.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Xu等人[[56](#bib.bib56)] 提出了一个名为元神经木马模型检测（MNTD）的方法，使用元神经分析技术来检测木马。作者表明，元分类器可以通过使用良性神经网络（单类学习）或通过近似和扩展木马模型的一般分布来训练。Huang等人[[61](#bib.bib61)]
    和Xu等人[[66](#bib.bib66)] 也采用了类似的总体策略，但使用了异常检测器作为元分类器。Huang等人还通过使用单像素签名表示在[[62](#bib.bib62)]中区分木马和良性模型。Wang等人[[63](#bib.bib63)]
    提出了在数据有限和数据空白的情况下区分木马模型和干净模型的方法。此外，Yoshida等人[[64](#bib.bib64)] 和Li等人[[65](#bib.bib65)]
    分享了使用蒸馏方法从输入中清除触发器的想法。在[[65](#bib.bib65)]中，作者采用了一种神经注意力蒸馏方法，其中教师模型通过一小部分干净输入对学生模型进行微调。研究发现，仅5%的干净训练数据就足以中和木马。
- en: 4.3 Restoring Compromised Models
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 恢复受损模型
- en: This section discusses the methods in the literature that mainly focus on restoring
    a Trojaned model. These methods can be broadly categorized into two streams of
    ‘model correction’ and ‘trigger-based Trojan reversing’.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了文献中主要关注恢复木马化模型的方法。这些方法可以大致分为两类：“模型纠正”和“基于触发的木马逆转”。
- en: 4.3.1 Model Correction
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 模型纠正
- en: Broadly speaking, the model correction strategy retrains and prunes a neural
    network for correction. However, in this case, the retraining is not conducted
    with every single sample of the large training dataset to avoid the undesired
    computations that causes the training outsourcing in the first place. Liu et al. [[67](#bib.bib67)]
    proposed a method that retrains the model on only a small subset of the correctly
    labelled training data. As the size of the retraining data is very small, it consumes
    much less computational power. The retraining mitigated the adversarial effects
    of model Trojan. Zhao et al. [[68](#bib.bib68)] pruned less significant neurons
    from the neural network to remove Trojan. They reshaped the neural network to
    a smaller size so that there is less capacity to fit Trojan. It is claimed that
    their method can increase the difficulty for Trojan injection while still maintaining
    a similar model accuracy as the original model.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，模型纠正策略是通过重新训练和剪枝神经网络来进行纠正。然而，在这种情况下，为了避免导致训练外包的不必要计算，重新训练并不是对大规模训练数据集的每个样本进行。刘等人[[67](#bib.bib67)]提出了一种仅对正确标记的训练数据的小子集进行模型重新训练的方法。由于重新训练数据的规模非常小，因此消耗的计算能力要少得多。重新训练减轻了模型木马的对抗效应。赵等人[[68](#bib.bib68)]从神经网络中剪除较不重要的神经元以去除木马。他们将神经网络重新塑造为更小的尺寸，从而减少了放置木马的容量。有人声称他们的方法可以增加木马注入的难度，同时保持与原始模型相似的模型准确性。
- en: '![Refer to caption](img/7b007f330813a3998f418a0cf1065457.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b007f330813a3998f418a0cf1065457.png)'
- en: 'Figure 8: An illustration of structural differences between the Trojaned and
    clean models with Trojaned inputs. Following the neuroscience adage, “neurons
    that fire together wire together”, red lines identify an effective architecture
    of neurons with correlated activations. There appears to be obvious short-cuts
    in the Trojaned model activations which do not exist in the clean model. Image
    taken from [[116](#bib.bib116)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：木马化输入下木马化模型与清洁模型的结构差异示意图。遵循神经科学格言“同时激发的神经元相互连接”，红线标识了激活相关的神经元的有效架构。木马化模型的激活中明显存在清洁模型中不存在的捷径。图像来源于[[116](#bib.bib116)]。
- en: Liu et al. [[69](#bib.bib69)] pointed out a weakness of the Zhao’s method [[68](#bib.bib68)]
    and claimed that if the attackers are aware of the pruning process, it is possible
    to improve the attack scheme that can fit Trojan in the limited space which reduces
    the effectiveness of the method. They showed that as the calculated activation
    value of clean input is not usually based on the Trojaned neurons, retraining
    the model with clean data does not correct the neural network adequately. Hence,
    they improved the method of [[68](#bib.bib68)] by pruning prior to model training
    with malicious data. By doing so, the activation values of the benign and Trojaned
    inputs sometimes mapped to the same neurons. Consequently, retraining with clean
    input modifies the neurons where Trojans reside. This removes the Trojan in the
    neurons where the activations of both the benign and Trojaned samples occur by
    fine-tuning the model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[[69](#bib.bib69)]指出了赵方法[[68](#bib.bib68)]的一个弱点，并声称如果攻击者了解剪枝过程，就有可能改进攻击方案，以便在有限的空间内放置木马，从而降低该方法的有效性。他们展示了由于清洁输入的计算激活值通常不基于被木马化的神经元，因此用清洁数据重新训练模型不能充分纠正神经网络。因此，他们通过在模型训练之前用恶意数据进行剪枝来改进[[68](#bib.bib68)]的方法。通过这样做，良性和木马化输入的激活值有时会映射到相同的神经元上。因此，用清洁输入重新训练会修改木马所在的神经元。这通过微调模型来去除木马，移除良性和木马样本激活都出现的神经元中的木马。
- en: Wu et al. [[112](#bib.bib112)] claimed that if neurons are maliciously perturbed,
    the neural network can easily malfunction, categorising clean samples into the
    target class. They developed a neural Trojan defense method called Adversarial
    Neural Pruning (ANP). The ANP can help with the model correction by pruning the
    sensitive neurons while not significantly degrading the performance of the model.
    Zheng et al. [[116](#bib.bib116)] proposed a method that uses topological tools
    to model high-order dependencies in the neural network and detect the existence
    of neural Trojan. They found that there is an obvious difference in structures
    between the Trojaned and clean models where Trojaned model appear to have short-cuts
    going from the input to the output layers that do not exist in clean models. Therefore,
    by looking for the short-cuts in the neural network, they identify the Trojan.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人 [[112](#bib.bib112)] 声称，如果神经元受到恶意干扰，神经网络可能会出现故障，将干净样本分类到目标类别。他们开发了一种名为对抗性神经修剪
    (ANP) 的神经 Trojan 防御方法。ANP 可以通过修剪敏感神经元来帮助模型纠正，同时不会显著降低模型的性能。Zheng 等人 [[116](#bib.bib116)]
    提出了一种使用拓扑工具建模神经网络中的高阶依赖关系并检测神经 Trojan 存在的方法。他们发现 Trojan 模型与干净模型在结构上存在明显差异，其中 Trojan
    模型从输入层到输出层存在干净模型中不存在的捷径。因此，通过寻找神经网络中的捷径，他们识别出 Trojan。
- en: 4.4 Trigger-based Trojan Reversing
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基于触发器的 Trojan 反向工程
- en: Conceptually, trigger-based Trojan reversing estimates the potential trigger
    pattern for a model and uses it in model training/re-training to robustify the
    model against the trigger pattern. Along this line, Wang et al. [[70](#bib.bib70)]
    proposed a method termed Neural-Cleanse that works in three stages. First, it
    constructs potential triggers for each class and estimates a final synthetic trigger
    and target label. Then, it attempts to reverse the trigger effects based on model
    pruning and retraining. Lastly, it removes the Trojan by retraining network on
    the input with reverse engineered trigger to achieve model recovery. The authors
    also demonstrate the vulnerability of deep models against Trojan attack by examining
    the smallest size of perturbation required to cause model’s misbehaviour. Though
    effective, there is a limitation of Neural-Cleanse that it is incapable of dealing
    with Trojan of varied size, shape and location.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，基于触发器的 Trojan 反向工程估计模型的潜在触发器模式，并在模型训练/再训练中使用该模式，以增强模型对触发器模式的鲁棒性。在这一方面，Wang
    等人 [[70](#bib.bib70)] 提出了一个名为 Neural-Cleanse 的方法，该方法分为三个阶段。首先，它为每个类别构建潜在触发器，并估计最终的合成触发器和目标标签。然后，它尝试基于模型修剪和再训练来反向触发器效应。最后，通过在输入数据上使用反向工程触发器再训练网络来去除
    Trojan，以实现模型恢复。作者还通过检查引起模型误行为所需的最小扰动大小，展示了深度模型对 Trojan 攻击的脆弱性。尽管效果显著，但 Neural-Cleanse
    存在一个限制，即无法处理不同大小、形状和位置的 Trojan。
- en: Guo et al. [[71](#bib.bib71)] proposed a method called TABOR that is able to
    overcome the limitation of Neural-Cleanse. TABOR utilises a non-convex optimization-theoretic
    formulation guided by explainable AI and other heuristics that enables the increase
    in the accuracy of detection without the restriction on trigger size, shape and
    location. Qiao et al. [[72](#bib.bib72)] also pointed out that the reversed engineered
    triggers under [[70](#bib.bib70)] differ significantly from the actual Trojan
    triggers. Inspired by this, they proposed a method to generalise the Trojan trigger.
    Instead of reversing all the individual triggers, they recovered the trigger distribution
    from the potential triggers to get a more precise reversed engineered trigger.
    The key idea of Qiao et al. [[72](#bib.bib72)] was agreed by Zhu et al. [[74](#bib.bib74)]
    who demonstrated the effectiveness of GAN-based synthesis of Trojan triggers for
    model recovery.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Guo 等人 [[71](#bib.bib71)] 提出了一个名为 TABOR 的方法，能够克服 Neural-Cleanse 的限制。TABOR 利用了一种由可解释的人工智能和其他启发式方法指导的非凸优化理论模型，使得检测的准确性得以提高，而不受触发器大小、形状和位置的限制。Qiao
    等人 [[72](#bib.bib72)] 也指出，[[70](#bib.bib70)] 中反向工程得到的触发器与实际的 Trojan 触发器存在显著差异。受到此启发，他们提出了一种方法来概括
    Trojan 触发器。他们没有对所有单独的触发器进行反向工程，而是从潜在的触发器中恢复触发器分布，以获得更精确的反向工程触发器。Qiao 等人 [[72](#bib.bib72)]
    的关键观点得到了 Zhu 等人 [[74](#bib.bib74)] 的认可，后者展示了基于 GAN 的 Trojan 触发器合成在模型恢复中的有效性。
- en: Chen et al. [[75](#bib.bib75)] noticed the distinct difference in patterns of
    the neuron activation between benign and Trojaned input in the final hidden layer
    of neural networks. Inspired by this, the authors proposed a detection method
    on the neuron activation pattern in the final hidden layer. The method involves
    forming clusters of the neuron activations in the final hidden layer and detecting
    Trojan by determining if abnormal characteristics are present in the cluster.
    The model can then be recovered by removing the clusters with abnormal characteristics
    and finetuing the model with clean input. Shen et al. [[76](#bib.bib76)] showed
    that Trojans can be removed by using only one class for trigger optimisation in
    each round of retraining. Aiken et al. [[77](#bib.bib77)] also proposed a method
    that combines model correction and trigger-based Trojan reversing involving pruning
    of neural network based on synthetic triggers.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Chen 等人 [[75](#bib.bib75)] 注意到在神经网络最终隐藏层中，良性和木马输入的神经元激活模式存在明显差异。受到启发，作者提出了一种基于最终隐藏层神经元激活模式的检测方法。该方法涉及在最终隐藏层中形成神经元激活的簇，并通过确定簇中是否存在异常特征来检测木马。然后可以通过去除具有异常特征的簇，并用干净的输入微调模型来恢复模型。Shen
    等人 [[76](#bib.bib76)] 表明，可以通过在每轮重新训练中仅使用一个类别进行触发器优化来去除木马。Aiken 等人 [[77](#bib.bib77)]
    还提出了一种结合模型修正和基于触发器的木马反转的方法，包括基于合成触发器的神经网络剪枝。
- en: 4.5 Bypassing Neural Trojan
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 绕过神经网络木马
- en: This strategy involves using pre-processing to remove trigger in the input before
    passing the input to the model. Doan et al. [[78](#bib.bib78)] developed a technique,
    termed Februus to bypass Trojan triggers in images. Before the image enters the
    model, it is sent to Februus system to validate the presence of trigger, and remove
    it if is it suspected. Working of Februus to detects and neutralises the trigger
    can be understood as three steps. (i) Using a logit score-based method for Trojan
    detection that works under the assumption that if a trigger exists in the input,
    then the predicted class is the target class. (ii) Remove the potential trigger
    with a masking process. (iii) Use an inpainting technique to restore the image.
    Liu et al. [[79](#bib.bib79)] showed that an autoencoder can be used for image
    pre-processing to remove potential triggers. The autoencoder is placed between
    the image and the Trojaned model and it removes Trojan trigger by minimising the
    mean-squared error between the training set images and the reconstructed images.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 该策略涉及使用预处理来移除输入中的触发器，然后将输入传递给模型。Doan 等人 [[78](#bib.bib78)] 开发了一种称为 Februus 的技术，用于绕过图像中的木马触发器。在图像进入模型之前，它会被送到
    Februus 系统，以验证是否存在触发器，如果怀疑存在，则将其移除。Februus 检测和中和触发器的工作可以理解为三个步骤。（i）使用基于逻辑分数的方法进行木马检测，该方法的假设是如果输入中存在触发器，则预测类别是目标类别。（ii）通过遮罩处理去除潜在的触发器。（iii）使用修复技术恢复图像。Liu
    等人 [[79](#bib.bib79)] 表明，自编码器可以用于图像预处理，以去除潜在的触发器。自编码器被放置在图像和木马模型之间，通过最小化训练集图像与重建图像之间的均方误差来去除木马触发器。
- en: Udeshi et al. [[80](#bib.bib80)] proposed a model-agnostic framework termed
    NEO to locate and mitigate Trojan trigger in the input images. NEO aims at predicting
    the correct outcomes of poisoned images and compares that with the actual prediction.
    It places a trigger blocker on the images that has its prediction outcomes differing
    considerably from each other. In [[81](#bib.bib81)], Vasquez et al. pre-processed
    images through the style transfer of the image. Li et al. [[82](#bib.bib82)] discovered
    that for the images with static trigger patterns, a slight change in the location
    or appearance of the trigger can significantly degrade the effectiveness of Trojan
    attack. Inspired, they then proposed a method which transforms the input regularly
    by shrinking and flipping. Their technique is claimed to be an efficient detection
    method that has low computational requirements.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Udeshi 等人 [[80](#bib.bib80)] 提出了一个称为 NEO 的模型无关框架，用于定位和减轻输入图像中的木马触发器。NEO 旨在预测受毒害图像的正确结果，并将其与实际预测进行比较。它在预测结果相差较大的图像上放置触发器阻止器。在
    [[81](#bib.bib81)] 中，Vasquez 等人通过图像的风格迁移对图像进行预处理。Li 等人 [[82](#bib.bib82)] 发现，对于具有静态触发器模式的图像，触发器的位置或外观的轻微变化会显著降低木马攻击的有效性。受到启发，他们提出了一种通过缩放和翻转定期变换输入的方法。他们的技术被称为是一种高效的检测方法，具有低计算需求。
- en: Zeng et al. [[83](#bib.bib83)] proposed a method that works by depressing the
    effectiveness of poisoned samples during the training time to prevent Trojan injection.
    The depression involves transformation of inputs in both training and run time
    process. Du et al. [[84](#bib.bib84)] used noisy stochastic gradient descent to
    learn the model. They demonstrated that when noise is present in the training
    set, the effectiveness of Trojan trigger reduces, resulting in a lower post-training
    attack success rate. Hong et al [[85](#bib.bib85)] took an alternative approach
    and observed that $l_{2}$ norm of the gradient of poisoned samples have significantly
    higher magnitude than benign samples and they also differ in their gradient orientation.
    They designed the differentially private stochastic gradient descent to perturb
    individual gradients of the training samples and trained the model with clean
    samples where all the Trojaned samples were removed from the training set. The
    existing neural Trojan detection methods often use an intermediate representation
    of models to distinguish between the Trojaned and benign models. Such methods
    are more effective when spectral signature of the Trojaned data is sufficiently
    large. Hayase et al. [[126](#bib.bib126)] proposed a robust covariance estimation
    method to amplify the spectral signature of the Trojaned data.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 曾等人[[83](#bib.bib83)]提出了一种方法，通过在训练期间降低受污染样本的有效性来防止特洛伊木马注入。这种降低包括在训练和运行时过程中对输入进行转换。杜等人[[84](#bib.bib84)]使用噪声随机梯度下降法来学习模型。他们展示了当训练集中存在噪声时，特洛伊木马触发器的有效性降低，导致训练后的攻击成功率降低。洪等人[[85](#bib.bib85)]采取了另一种方法，观察到受污染样本的$
    l_{2} $范数梯度的大小明显高于良性样本，并且它们的梯度方向也有所不同。他们设计了差分隐私随机梯度下降法，以扰动训练样本的单独梯度，并使用清洁样本训练模型，其中所有特洛伊木马样本都从训练集中移除。现有的神经网络特洛伊木马检测方法通常使用模型的中间表示来区分特洛伊木马模型和良性模型。这些方法在特洛伊木马数据的光谱特征足够大时更为有效。林等人[[126](#bib.bib126)]提出了一种稳健的协方差估计方法，以放大特洛伊木马数据的光谱特征。
- en: 4.6 Input Filtering
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 输入过滤
- en: The input filtering strategy involves filtering the malicious input so that
    the data passed to the model is likely clean. Works under this category can further
    be divided based on filtering applied at the training stage or the testing stage.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输入过滤策略涉及过滤恶意输入，以确保传递给模型的数据可能是干净的。这类工作可以进一步根据在训练阶段或测试阶段应用的过滤方法进行分类。
- en: 4.6.1 Training Sample Filtering
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1 训练样本过滤
- en: Tran et al. [[86](#bib.bib86)] noticed that there is a detectable trace for
    Trojaned samples in the spectrum of feature representation co-variance. Hence,
    they proposed to filter the Trojaned samples using the decomposition of feature
    representation. Chen et al. [[87](#bib.bib87)] shared a similar idea to Tran et
    al. [[86](#bib.bib86)] and noted that the Trojaned and benign samples have different
    characteristic in the feature space. They demonstrated that the Trojaned samples
    in the training set can be filtered by first performing clustering of training
    data neuron activation, then filtering the data by removing the cluster that represents
    poisoned samples. Tang et al. [[88](#bib.bib88)] pointed out a limitation of the
    previous two methods, stating that simple target contaminations may result in
    less distinguishable representations for benign and Trojaned samples. To overcome
    this, they proposed filtering based on representation decomposition and statistical
    analysis of the individual samples. Similarly, Soremekun et al. [[89](#bib.bib89)]
    also proposed filtering of poisoned samples based on the feature representation
    difference between the Trojaned and benign samples. Chou et al. [[90](#bib.bib90)]
    utilised saliency map for detecting potential triggers in the input, and then
    they filtered the samples containing the triggers. Li et al. [[111](#bib.bib111)]
    claimed that it is not evidental that there exist robust training methods to prevent
    the injection of triggers. They conducted experiments which split the training
    process into clean data training and Trojaned data training. They found two weaknesses
    of Trojaned data training. 1) It is faster for the models to learn Trojaned data
    compared to the clean data and the time taken for the convergence of the model
    on the Trojaned data is highly dependent on the strength of the neural Trojan
    attack. 2) The neural Trojan always aims to lean the models to the Trojan target
    class. They leverage these observations to propose Anti-Backdoor Learning which
    can achieve neural Trojan prevention by isolating Trojaned samples at the training
    phase, and weaken any potential relationship between the Trojaned sample and the
    target class.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Tran 等人 [[86](#bib.bib86)] 注意到在特征表示协方差的频谱中可以检测到被 Trojan 攻击的样本的痕迹。因此，他们提出通过特征表示的分解来过滤
    Trojan 攻击样本。Chen 等人 [[87](#bib.bib87)] 与 Tran 等人 [[86](#bib.bib86)] 共享了类似的想法，并指出
    Trojan 攻击样本和正常样本在特征空间中具有不同的特征。他们展示了通过首先对训练数据的神经激活进行聚类，然后通过移除代表中毒样本的聚类来过滤训练集中的
    Trojan 攻击样本。Tang 等人 [[88](#bib.bib88)] 指出了前两种方法的一个限制，指出简单的目标污染可能导致正常样本和 Trojan
    攻击样本的表示差异不明显。为了克服这一问题，他们提出基于表示分解和单个样本的统计分析来进行过滤。同样，Soremekun 等人 [[89](#bib.bib89)]
    也提出了基于 Trojan 攻击样本与正常样本之间特征表示差异的过滤方法。Chou 等人 [[90](#bib.bib90)] 利用显著性图检测输入中的潜在触发器，然后过滤包含触发器的样本。Li
    等人 [[111](#bib.bib111)] 声称尚无证据表明存在可靠的训练方法来防止触发器的注入。他们进行了实验，将训练过程分为干净数据训练和 Trojan
    数据训练。他们发现 Trojan 数据训练存在两个弱点。1) 模型学习 Trojan 数据的速度比学习干净数据的速度快，模型在 Trojan 数据上的收敛时间高度依赖于神经
    Trojan 攻击的强度。2) 神经 Trojan 总是旨在将模型引导到 Trojan 目标类别。他们利用这些观察结果提出了 Anti-Backdoor Learning，通过在训练阶段隔离
    Trojan 样本来实现神经 Trojan 预防，并削弱 Trojan 样本与目标类别之间的潜在关系。
- en: '![Refer to caption](img/f00ee32269fa8fa2fa4274cfb23775e9.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f00ee32269fa8fa2fa4274cfb23775e9.png)'
- en: 'Figure 9: A summary of the prominent contemporary Neural Trojan injection and
    detection methods, along with non-adversarial applications of Neural Trojans.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：当代主要神经 Trojan 注入和检测方法的总结，以及神经 Trojan 的非对抗性应用。
- en: Fu et al. [[123](#bib.bib123)] recently proposed a novel feature-based on-line
    detection strategy for neural Trojans that is named Removing Adversarial-Backdoors
    by Iterative Demarcation (RAID). This is achieved in two stages, which are off-line
    training and on-line retraining. The off-line training trains the neural network
    with only clean data first, and then the on-line retraining detects the input
    that is different from the clean data at the off-line training stage. Significantly
    different images are then removed. They next train a binary support vector machine
    (SVM) with both the purified anomalous data and the clean data so that RAID can
    use the SVM to detect the poisoned inputs in a dataset. The SVM is also designed
    to be updated in real-time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Fu 等人 [[123](#bib.bib123)] 最近提出了一种名为通过迭代划分（RAID）移除对抗后门的新型基于特征的在线检测策略。这分为两个阶段完成，即离线训练和在线再训练。离线训练阶段仅使用干净数据训练神经网络，然后在线再训练阶段检测与离线训练阶段的干净数据不同的输入。显著不同的图像会被去除。接着，他们用净化后的异常数据和干净数据训练一个二分类支持向量机（SVM），以便
    RAID 可以利用 SVM 检测数据集中的被污染输入。SVM 还设计为实时更新。
- en: 4.6.2 Testing Sample Filtering
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2 测试样本过滤
- en: Similar to training sample filtering, the main goal of this type of detection
    method is to distinguish between the benign and Trojaned samples and filter the
    Trojaned ones from the entire set before passing them to the model. However, in
    this case, it is strictly done at the testing time only. Subedar et al. [[91](#bib.bib91)]
    proposed a method that is able to distinguish between the Trojaned and benign
    samples using model uncertainty during test time. Du et al. [[92](#bib.bib92)]
    demonstrated the effectiveness of outlier detection for the objective trigger
    detection while testing the model. Jaraheripi et al. [[93](#bib.bib93)] also proposed
    a lightweight method for sample filtering which does not require labeled data,
    model retraining or prior assumption on the design of the trigger and can work
    as testing stage filter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练样本过滤类似，这种检测方法的主要目标是区分良性样本和被特洛伊木马感染的样本，并在将其传递给模型之前，从整个数据集中过滤掉特洛伊木马样本。然而，在这种情况下，这仅在测试时严格进行。Subedar
    等人 [[91](#bib.bib91)] 提出了一个方法，利用模型不确定性在测试时区分特洛伊木马样本和良性样本。Du 等人 [[92](#bib.bib92)]
    演示了在测试模型时，异常检测对于目标触发器检测的有效性。Jaraheripi 等人 [[93](#bib.bib93)] 还提出了一种轻量级的样本过滤方法，该方法不需要标记数据、模型再训练或对触发器设计的先验假设，并且可以作为测试阶段的过滤器。
- en: 4.7 Certified Trojan Defenses
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 认证特洛伊木马防御
- en: Nearly all the above-mentioned defense techniques can be categorised as ad-hoc
    techniques that are based on heuristics. It is often mentioned in the literature
    that such defenses can be broken with the help of adaptive strategies [[104](#bib.bib104)],
    [[105](#bib.bib105)]. Hence, similar to the certified defenses for adversarial
    examples [[108](#bib.bib108)], researchers have also started investigating such
    defenses for Trojan attacks. For instance, Wang et al. [[94](#bib.bib94)] proposed
    a random smoothing technique that adds random noise to the sample to ensure the
    robustness of the resulting model against the adaptive attacks. They improved
    the method by thinking the training procedure as base function and develop a smooth
    function based on the base function for smoothing. To an extent, Weber et al. [[95](#bib.bib95)]
    disagreed with the above and proved the ineffectiveness when applying smoothing
    directly. They proposed a framework that evaluates the difference in the smoothing
    noise distributions to achieve better robustness of the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有上述防御技术都可以归类为基于启发式的临时技术。文献中经常提到，这些防御可以借助适应性策略被突破 [[104](#bib.bib104)], [[105](#bib.bib105)]。因此，与针对对抗性样本的认证防御
    [[108](#bib.bib108)] 类似，研究人员也开始调查针对特洛伊木马攻击的认证防御。例如，Wang 等人 [[94](#bib.bib94)]
    提出了一个随机平滑技术，通过向样本中添加随机噪声来确保模型对适应性攻击的鲁棒性。他们通过将训练过程视为基础函数并基于基础函数开发平滑函数来改进方法。到一定程度上，Weber
    等人 [[95](#bib.bib95)] 对上述观点表示异议，并证明了直接应用平滑的无效性。他们提出了一个框架，评估平滑噪声分布的差异，以实现模型更好的鲁棒性。
- en: 5 Discussions and Future Outlook
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与未来展望
- en: This survey focused on the literature published from 2017 to 2021 on neural
    Trojan with nearly 60% of the paper on Trojan attacks and the other 40% on defenses
    against the attacks. Incidentally, we notice more activity in neural Trojan literature
    since 2019, as compared to previous years. This indicates an increasing interest
    of the research community in this direction. This trend is inline with the literature
    in a closely related research direction of adversarial attacks on deep learning [[108](#bib.bib108)].
    We conjecture that this ever increasing research activity in these directions
    is a natural consequence of awareness of vulnerabilities of deep learning in adversarial
    setups. We noticed a clear cat-and-mouse game between Trojan injections and Trojan
    defenses until the Triggerless Backdoor [[24](#bib.bib24)] and dynamic backdoor [[25](#bib.bib25)]
    appeared in the literature. These attacks have been shown to bypass the state-of-the-art
    defense technologies at their time, which shows that the game is being led by
    the attack methods. This battle is still on though, resulting in further discoveries
    of vulnerabilites of deep learning and their remedies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这项调查重点关注2017年至2021年间关于神经网络木马的文献，其中约60%的论文涉及木马攻击，另外40%涉及防御措施。巧合的是，自2019年以来，神经网络木马的文献活动有所增加，与之前的年份相比。这表明研究社区对这一方向的兴趣在增长。这一趋势与与深度学习对抗攻击相关的研究文献一致[[108](#bib.bib108)]。我们推测，这种日益增长的研究活动是对深度学习在对抗设置中脆弱性的意识的自然结果。我们注意到，直到无触发后门[[24](#bib.bib24)]和动态后门[[25](#bib.bib25)]出现在文献中，木马注入和木马防御之间才出现了明显的猫鼠游戏。这些攻击已经证明能够绕过当时的最先进防御技术，显示出攻击方法主导了游戏。不过，这场战斗仍在继续，导致了深度学习脆弱性及其补救措施的进一步发现。
- en: Whereas the literature discussed in the preceding sections cover a wide range
    of topics and possibilities, this research direction is relatively new. Hence,
    there is still considerable opportunity to explore new sub-topics in this direction.
    A guide to such an exploration is provided by the sister problem of adversarial
    attacks on deep learning [[108](#bib.bib108)]. The discovery of adversarial attacks
    was made in 2013 - a few year earlier than identification of neural Trojans. Hence,
    that problem is currently relatively more popular in the literature. The maturity
    of literature in adversarial attacks can provide useful guideline for research
    in Trojan attacks. We also list a few possible future directions and challenges
    for Trojan attacks based on our literature review and other related problems.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前述文献涵盖了广泛的主题和可能性，但这一研究方向相对较新。因此，仍有相当大的机会在这一方向上探索新的子主题。探索的指南可以通过深度学习对抗攻击的姊妹问题提供[[108](#bib.bib108)]。对抗攻击的发现发生在2013年，比神经网络木马的识别早了几年。因此，这一问题在文献中相对较受欢迎。对抗攻击领域文献的成熟可以为木马攻击研究提供有用的指导。我们还根据文献综述和其他相关问题列出了一些可能的未来方向和挑战。
- en: 5.1 Focus on Black-box Attacks
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 聚焦于黑箱攻击
- en: Almost 95% of the currently available Trojan attacks are white-box or grey-box
    that are based on the assumption that at least parts of the training data is accessible
    to the attackers. For example, all the attacks introduced by training data poisoning
    assume that complete training data is available to insert the backdoor in the
    target model. In real-life, full training data is usually not accessible due to
    privacy reasons. Hence, these white-box and grey-box setups do not fully apply
    to practical scenarios. This makes exploration of methods to embed Trojans or
    exploit triggers under fully black-box setup an interesting future direction.
    It may first seem that black-box scenario does not apply to Trojan attacks because
    Trojans are embedded in the model itself, and embedding them must require model
    or training data access. However, it may be possible to identify natural vulnerabilities
    of models by querying them, e.g. discovering sensitivity of a model to irrelevant
    pattern. This can subsequently be exploited in embedding triggers in input during
    test time.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前可用的特洛伊木马攻击中，几乎95%是基于白盒或灰盒模型，这些模型的假设前提是攻击者至少可以访问部分训练数据。例如，所有通过训练数据中毒引入的攻击都假设攻击者可以获取完整的训练数据以在目标模型中插入后门。在实际生活中，由于隐私原因，完整的训练数据通常无法获取。因此，这些白盒和灰盒设置并不完全适用于实际场景。这使得在完全黑盒设置下嵌入特洛伊木马或利用触发器的研究成为一个有趣的未来方向。最初，黑盒场景似乎不适用于特洛伊木马攻击，因为特洛伊木马嵌入在模型本身中，嵌入它们必须需要访问模型或训练数据。然而，通过查询模型，例如发现模型对无关模式的敏感性，可能识别出模型的自然漏洞。这可以随后在测试时通过在输入中嵌入触发器加以利用。
- en: 5.2 Attacks and Defenses Beyond Visual Models
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 超越视觉模型的攻击与防御
- en: Deep learning has its applications far beyond visual models. Currently, a wide
    range of application tasks and neural network types are exploiting deep learning,
    e.g. in speech recognition [[29](#bib.bib29)], [[30](#bib.bib30)], graph networks [[34](#bib.bib34)],
    [[35](#bib.bib35)] etc. It can be notice in our literature review that an overwhelming
    majority of the existing works are concerned with visual models only. In other
    words, most of the Trojan embedding techniques are tailored to convolutional neural
    networks operating in the domain of images. In this case, the effectiveness of
    the attacks can be improved by maximising the attack success rate while keeping
    the Trojan trigger hidden by blending the noise throughout the images. However,
    designing an invisible trigger pattern in the other domains, e.g. speech recognition,
    natural language processing, can be significantly different as the triggers can
    no longer be blended into e.g. sentences. This opens new challenges for research
    in Trojan attacks in different domains. We anticipate that in the future, this
    direction will still encounter uncharted territories when its scope will be expanded
    to other domains. Indeed, it is obvious that those domains will be found susceptible
    to Trojan attacks as long as they exploit the technology of deep learning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的应用远远超出了视觉模型。目前，各种应用任务和神经网络类型正在利用深度学习，例如在语音识别[[29](#bib.bib29)]、图网络[[34](#bib.bib34)]等领域。我们在文献综述中可以注意到，绝大多数现有工作仅关注于视觉模型。换句话说，大多数特洛伊木马嵌入技术都是针对在图像领域运行的卷积神经网络设计的。在这种情况下，通过最大化攻击成功率并将特洛伊木马触发器隐藏在图像的噪声中，可以提高攻击的有效性。然而，在其他领域（例如语音识别、自然语言处理）设计隐形触发器模式可能会大相径庭，因为触发器不能再融入例如句子中。这为不同领域的特洛伊木马攻击研究提出了新的挑战。我们预计，未来在将该方向扩展到其他领域时，仍会遇到未知领域。实际上，很明显，只要这些领域利用了深度学习技术，它们就会对特洛伊木马攻击表现出脆弱性。
- en: 5.3 Efficacy of Trojan Design
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 特洛伊木马设计的有效性
- en: Many works in the related literature focus on finding the most effective and
    secretive way to insert Trojan triggers into the inputs. Nonetheless, the actual
    pattern of the Trojan triggers are equally important, whose efficacy is relatively
    less explored in the current literature. The c-BaN in dynamic backdoor [[15](#bib.bib15)]
    is one of the few methods to generate triggers that are most suited to images
    with a given label. Others generally think of Trojan triggers as a single pattern
    or use simple algorithms for trigger generation. Hence, the ways to design powerful
    trigger patterns can still be explored. It is also mentioned in [[4](#bib.bib4)]
    that most of the studies only consider the effectiveness and invisibility of triggers.
    However, more research can be conducted, aiming to design a Trojan trigger that
    requires a minimised amount of training data to be poisoned.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 相关文献中的许多工作关注于找到最有效和隐秘的方式将木马触发器插入到输入中。然而，木马触发器的实际模式同样重要，其效能在当前文献中相对较少探讨。动态后门中的
    c-BaN [[15](#bib.bib15)] 是为图像生成最适合于特定标签的触发器的少数方法之一。其他方法通常将木马触发器视为单一模式或使用简单算法生成触发器。因此，设计强大触发器模式的方法仍有待探索。[[4](#bib.bib4)]
    中也提到，大多数研究只考虑触发器的有效性和隐蔽性。然而，可以进行更多研究，旨在设计出需要最小化训练数据的中毒木马触发器。
- en: 5.4 Stronger Defenses for the Existing Attacks
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 对现有攻击的更强防御
- en: As mentioned earlier, the recent triggerless backdoors [[24](#bib.bib24)] and
    dynamic backdoors [[15](#bib.bib15)] are designed to be too effective to be detected
    by the state-of-the-art defense techniques. Triggerless backdoor broke the traditional
    way of thinking and enabled Trojan activation without the presence of a trigger.
    Dynamic backdoor also improved on the design of traditional Trojan to allow the
    trigger to be a random pattern at random locations. This makes the detection of
    such a trigger impossible. With continuous developments of Trojan embedding techniques
    and introduction of more and more powerful attacks, we expect to see a counter
    stream of stronger defenses in the future.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，最近的无触发后门 [[24](#bib.bib24)] 和动态后门 [[15](#bib.bib15)] 设计得过于有效，以至于最先进的防御技术难以检测。无触发后门打破了传统思维方式，使木马在没有触发器的情况下激活。动态后门还在传统木马的设计上进行改进，使触发器可以是随机模式在随机位置出现。这使得此类触发器的检测变得不可能。随着木马嵌入技术的不断发展和越来越强大的攻击的出现，我们期望未来会出现更强的防御措施。
- en: 6 Conclusion
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Neural Trojan is a serious problem for deep learning technology as it affects
    neural networks such that they work normally for benign inputs, but maliciously
    in the presence of a trigger in the input. This has serious implications for security-critical
    applications. For example, in facial recognition, an attacker can exploit neural
    Trojan to grant authentication to irrelevant personnel to sensitive areas or information.
    Detection of such an attack is not easy because the model operates normally in
    most cases and misbehave only in very specific cases. Due to the large growth
    of the applications of deep learning, Trojan attacks have caught attentions of
    many researchers in the recent years. This has also caused defenses against Trojan
    attacks to emerge. We noticed that every year, there is an increasing number of
    works appearing in the reputed sources of machine learning and computer vision,
    e.g. CVPR, ICCV, ECCV, ICLR, NeurIPS that are concerned with neural Trojans. This
    observation leads us to believe that this research direction is likely to become
    even more popular in the near future. Whereas there have already been a few literature
    reviews in this direction, our survey is unique in that it we reviews papers that
    are published recently. It summarised the recent neural Trojan injection and defense
    techniques by systematic categorization and discussed their effectiveness.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 神经特洛伊木马是深度学习技术中的一个严重问题，因为它影响神经网络，使其在正常输入下工作正常，但在输入中存在触发器时则会恶意行为。这对安全关键应用具有严重影响。例如，在面部识别中，攻击者可以利用神经特洛伊木马将认证授予不相关的人员，进入敏感区域或获取信息。检测此类攻击并不容易，因为模型在大多数情况下正常运行，仅在非常特定的情况下才会表现异常。由于深度学习应用的大量增长，特洛伊木马攻击近年来引起了许多研究人员的关注。这也促使了针对特洛伊木马攻击的防御措施的出现。我们注意到，每年在著名的机器学习和计算机视觉来源中，例如CVPR、ICCV、ECCV、ICLR、NeurIPS，出现了越来越多关注神经特洛伊木马的研究。这一观察使我们相信，这一研究方向在不久的将来可能会变得更加流行。虽然已经有一些相关的文献综述，但我们的调查独特之处在于，我们回顾了最近发表的论文。它通过系统分类总结了近期神经特洛伊木马注入和防御技术，并讨论了它们的有效性。
- en: References
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: ''
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio and G. Hinton, “Deep Learning,” Nature, vol. 521, no.
    1, pp. 436–44, May. 28\. 2015\. DOI: 10.1038/nature14539, [online].'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. LeCun, Y. Bengio 和 G. Hinton，“深度学习”，Nature，第521卷，第1期，第436–44页，2015年5月28日。DOI:
    10.1038/nature14539，[在线]。'
- en: '[2] F. Altaf, S. Islam, N. Akhtar and N. Janjua, “Going Deep in Medical Image
    Analysis: Concepts, Methods, Challenges and Future Directions,” IEEE Access, vol.
    PP, no. 1, pp. 1–1, 2019\. DOI: 10.1109/ACCESS.2019.2929365, [online].'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] F. Altaf, S. Islam, N. Akhtar 和 N. Janjua，“深入医疗图像分析：概念、方法、挑战和未来方向”，IEEE
    Access，第PP卷，第1期，第1–1页，2019年。DOI: 10.1109/ACCESS.2019.2929365，[在线]。'
- en: '[3] Y. Li, B. Wu, Y. Jiang, Z. Li and S. Xia, “Backdoor Learning: A Survey,”
    arXiv:2007.08745. [online]. Available: https://arxiv.org/abs/2007.08745'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Li, B. Wu, Y. Jiang, Z. Li 和 S. Xia，“后门学习：一项调查”，arXiv:2007.08745。[在线]。可用链接：https://arxiv.org/abs/2007.08745'
- en: '[4] Y. Liu, A. Mondal, A. Chakraborty, M. Zuzak, N. Jacobsen, D. Xing and A.
    Srivastava, “A Survey on Neural Trojans,” IEEE Xplore, pp. 33–39, 2020\. DOI:
    10.1109/ISQED48828.2020.9137011, [online].'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Y. Liu, A. Mondal, A. Chakraborty, M. Zuzak, N. Jacobsen, D. Xing 和 A.
    Srivastava，“关于神经特洛伊木马的调查”，IEEE Xplore，第33–39页，2020年。DOI: 10.1109/ISQED48828.2020.9137011，[在线]。'
- en: '[5] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating backdooring
    attacks on deep neural networks,” IEEE Xplore, vol. 7, pp. 47 230–47 244, 2019.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Gu, K. Liu, B. Dolan-Gavitt 和 S. Garg，“Badnets：评估深度神经网络上的后门攻击”，IEEE
    Xplore，第7卷，第47 230–47 244页，2019年。'
- en: '[6] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks
    on deep learning systems using data poisoning,” arXiv preprint arXiv:1712.05526,
    2017.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] X. Chen, C. Liu, B. Li, K. Lu 和 D. Song，“利用数据中毒对深度学习系统进行针对性后门攻击”，arXiv预印本
    arXiv:1712.05526，2017年。'
- en: '[7] Y. Li, Y. Li, B. Wu, L. Li, R. He, and S. Lyu, “Backdoor attack with sample-specific
    triggers,” arXiv preprint arXiv:2012.03816, 2020.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Li, Y. Li, B. Wu, L. Li, R. He 和 S. Lyu，“带有样本特定触发器的后门攻击”，arXiv预印本 arXiv:2012.03816，2020年。'
- en: '[8] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reflection backdoor: A natural backdoor
    attack on deep neural networks,” in ECCV, 2020.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Liu, X. Ma, J. Bailey 和 F. Lu，“反射后门：对深度神经网络的自然后门攻击”，发表于ECCV，2020年。'
- en: '[9] H. Zhong, C. Liao, A. C. Squicciarini, S. Zhu, and D. Miller, “Backdoor
    embedding in convolutional neural network models via invisible perturbation,”
    in ACM CODASPY, 2020.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] H. Zhong, C. Liao, A. C. Squicciarini, S. Zhu 和 D. Miller，“通过不可见扰动在卷积神经网络模型中嵌入后门”，发表于ACM
    CODASPY，2020年。'
- en: '[10] N. Baracaldo, B. Chen, H. Ludwig, A. Safavi, and R. Zhang, “Backdoor embedding
    in convolutional neural network models via invisible perturbation,” IEEE International
    Congress on Internet of Things, 2018.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] N. Baracaldo, B. Chen, H. Ludwig, A. Safavi, 和 R. Zhang，“通过不可见扰动在卷积神经网络模型中嵌入后门”，IEEE
    国际物联网大会，2018年。'
- en: '[11] H. Zhong, C. Liao, A. C. Squicciarini, S. Zhu, and D. Miller, “Detecting
    Poisoning Attacks on Machine Learning in IoT Environments,” in ACM CODASPY, 2020.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] H. Zhong, C. Liao, A. C. Squicciarini, S. Zhu, 和 D. Miller，“在物联网环境中检测机器学习中的毒药攻击”，发表于
    ACM CODASPY，2020年。'
- en: '[12] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, “Cleanlabel
    backdoor attacks on video recognition models,” in CVPR, 2020.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, 和 Y.-G. Jiang，“对视频识别模型的清洁标签后门攻击”，发表于
    CVPR，2020年。'
- en: '[13] A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden trigger backdoor attacks,”
    in AAAI, 2020.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Saha, A. Subramanya, 和 H. Pirsiavash， “隐蔽触发后门攻击”，发表于 AAAI，2020年。'
- en: '[14] E. Quiring and K. Rieck, “Backdooring and poisoning neural networks with
    image-scaling attacks,” in IEEE S&P Workshop, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] E. Quiring 和 K. Rieck，“通过图像缩放攻击对神经网络进行后门植入和毒药攻击”，发表于 IEEE S&P Workshop，2020年。'
- en: '[15] A. Salem, R. Wen, M. Backes, S. Ma and Y. Zhang, “Dynamic Backdoor Attacks
    Against Machine Learning Models,” arXiv:2003.03675, 2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Salem, R. Wen, M. Backes, S. Ma 和 Y. Zhang，“对机器学习模型的动态后门攻击”，arXiv:2003.03675，2020年。'
- en: '[16] J. Clements and Y. Lao, “Backdoor Attacks on Neural Network Operations,”
    IEEE Global Conference on Signal and Information Processing (GlobalSIP), pp. 1154–1158,
    2018.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Clements 和 Y. Lao，“对神经网络操作的后门攻击”，IEEE 全球信号与信息处理大会 (GlobalSIP)，第1154–1158页，2018年。'
- en: '[17] J.J. Dumford and W. Scheirer, “Backdooring convolutional neural networks
    via targeted weight perturbations,” arXiv preprint arXiv:1812.03128, 2018.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J.J. Dumford 和 W. Scheirer，“通过有针对性的权重扰动对卷积神经网络进行后门植入”，arXiv 预印本 arXiv:1812.03128，2018年。'
- en: '[18] A. S. Rakin, Z. He, and D. Fan, “Tbt: Targeted neural network attack with
    bit trojan,” in CVPR, 2020.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. S. Rakin, Z. He, 和 D. Fan，“Tbt：带有比特特洛伊木马的目标神经网络攻击”，发表于 CVPR，2020年。'
- en: '[19] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in deep learning models,”
    arXiv preprint arXiv:2005.03823, 2020.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] E. Bagdasaryan 和 V. Shmatikov，“深度学习模型中的盲后门”，arXiv 预印本 arXiv:2005.03823，2020年。'
- en: '[20] T. Liu, W. Wen, and Y. Jin, “SIN 2: Stealth infection on neural network—a
    low-cost agile neural trojan attack methodology,” 8 IEEE International Symposium
    on Hardware Oriented Security and Trust (HOST), pp. 227–230, 2018.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Liu, W. Wen, 和 Y. Jin，“SIN 2：神经网络上的隐蔽感染——一种低成本灵活的神经特洛伊木马攻击方法”，第8届 IEEE
    硬件安全与信任国际研讨会 (HOST)，第227–230页，2018年。'
- en: '[21] C. Guo, R. Wu, and K. Q. Weinberger, “Trojannet: Embedding hidden trojan
    horse models in neural networks,” arXiv preprint arXiv:2002.10078, 2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Guo, R. Wu, 和 K. Q. Weinberger，“Trojannet：在神经网络中嵌入隐蔽特洛伊木马模型”，arXiv
    预印本 arXiv:2002.10078，2020年。'
- en: '[22] R. Tang, M. Du, N. Liu, F. Yang, and X. Hu, “An embarrassingly simple
    approach for trojan attack in deep neural networks,” in KDD, 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Tang, M. Du, N. Liu, F. Yang, 和 X. Hu，“在深度神经网络中进行特洛伊木马攻击的一种令人尴尬的简单方法”，发表于
    KDD，2020年。'
- en: '[23] W. Li, J. Yu, X. Ning, P. Wang, Q. Wei, Y. Wang, and H. Yang, “Hu-fu:
    Hardware and software collaborative attack framework against neural networks,”
    8 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), pp. 482–487, 2018.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] W. Li, J. Yu, X. Ning, P. Wang, Q. Wei, Y. Wang, 和 H. Yang，“Hu-fu：针对神经网络的硬件与软件协同攻击框架”，第8届
    IEEE 计算机学会 VLSI 年会 (ISVLSI)，第482–487页，2018年。'
- en: '[24] A. Salem, M. Backes, and Y. Zhang, “Don’t Trigger Me! A Triggerless Backdoor
    Attack Against Deep Neural Networks,” arXiv:2010.03282, 2020.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Salem, M. Backes, 和 Y. Zhang，“别激活我！一种针对深度神经网络的无触发后门攻击”，arXiv:2010.03282，2020年。'
- en: '[25] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted Backdoor Attacks
    on Deep Learning Systems Using Data Poisoning,” arXiv:1712.05526, 2017.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Chen, C. Liu, B. Li, K. Lu, 和 D. Song，“通过数据毒药实现对深度学习系统的目标后门攻击”，arXiv:1712.05526，2017年。'
- en: '[26] E. Wenger and J. Passananti and A. Bhagoji and Y. Yao and H. Zheng and
    B. Y. Zhao, “Backdoor Attacks Against Deep Learning Systems in the Physical World,”
    arXiv: Computer Vision and Pattern Recognition, 2020.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] E. Wenger, J. Passananti, A. Bhagoji, Y. Yao, H. Zheng, 和 B. Y. Zhao，“在物理世界中对深度学习系统的后门攻击”，arXiv：计算机视觉与模式识别，2020年。'
- en: '[27] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating backdooring
    attacks on deep neural networks,” IEEE Access, , vol. 7, pp. 47230–-47244, 2019.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] T. Gu, K. Liu, B. Dolan-Gavitt, 和 S. Garg，“Badnets：评估对深度神经网络的后门攻击”，IEEE
    Access，第7卷，第47230–47244页，2019年。'
- en: '[28] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Rethinking the trigger
    of backdoor attack,” arXiv preprint arXiv:2004.04692, 2020.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, 和 S. Xia，“重新思考后门攻击的触发器”，arXiv
    预印本 arXiv:2004.04692，2020年。'
- en: '[29] J. Dai, C. Chen, and Y. Li, “A backdoor attack against lstm-based text
    classification systems,” IEEE Access, vol. 7, pp. 138872–138878, 2019.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Dai, C. Chen, 和 Y. Li, “针对基于 LSTM 的文本分类系统的后门攻击，” IEEE Access，第 7 卷，第
    138872–138878 页，2019。'
- en: '[30] S. Wang, S. Nepal, C. Rudolph, M. Grobler, S. Chen, and T. Chen, “Backdoor
    attacks against transfer learning with pre-trained deep learning models,” IEEE
    Transactions on Services Computing, 2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] S. Wang, S. Nepal, C. Rudolph, M. Grobler, S. Chen, 和 T. Chen, “针对使用预训练深度学习模型的迁移学习的后门攻击，”
    IEEE Transactions on Services Computing, 2020。'
- en: '[31] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “BadNets: Evaluating Backdooring
    Attacks on Deep Neural Networks,” . IEEE Access 7, pp. 47230–47244, 2019.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. Gu, K. Liu, B. Dolan-Gavitt, 和 S. Garg, “BadNets：评估深度神经网络中的后门攻击，” IEEE
    Access，第 7 卷，第 47230–47244 页，2019。'
- en: '[32] T. J. L. Tan and R. Shokri, “Bypassing Backdoor Detection Algorithms in
    Deep Learning,” arXiv preprint arXiv:1905.13409, 2019.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] T. J. L. Tan 和 R. Shokri, “绕过深度学习中的后门检测算法，” arXiv 预印本 arXiv:1905.13409,
    2019。'
- en: '[33] Y. Yao, H. Li, H. Zheng, and B. Y. Zhao, “ Latent Backdoor Attacks on
    Deep Neural Networks,” 2019.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Y. Yao, H. Li, H. Zheng, 和 B. Y. Zhao, “深度神经网络中的潜在后门攻击，” 2019。'
- en: '[34] Z. Zhang, J. Jia, B. Wang, and N. Z. Gong, “Backdoor attacks to graph
    neural networks,” in NeurIPS Workshop, 2020.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Z. Zhang, J. Jia, B. Wang, 和 N. Z. Gong, “对图神经网络的后门攻击，” 2020 NeurIPS 研讨会。'
- en: '[35] Z. Xi, R. Pang, S. Ji, and T. Wang, “Graph backdoor,” arXiv preprintarXiv:2006.11890,
    2020.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Z. Xi, R. Pang, S. Ji, 和 T. Wang, “图形后门，” arXiv 预印本 arXiv:2006.11890,
    2020。'
- en: '[36] P. Kiourti, K. Wardega, S. Jha, and W. Li, “Trojdrl: Trojan attacks on
    deep reinforcement learning agents,” arXiv preprint arXiv:1903.06638, 2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] P. Kiourti, K. Wardega, S. Jha, 和 W. Li, “Trojdrl：对深度强化学习代理的特洛伊木马攻击，”
    arXiv 预印本 arXiv:1903.06638, 2019。'
- en: '[37] Z. Yang, N. Iyer, J. Reimann, and N. Virani, “Design of intentional backdoors
    in sequential models,” arXiv preprint arXiv:1902.09972, 2019.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Yang, N. Iyer, J. Reimann, 和 N. Virani, “序列模型中故意后门的设计，” arXiv 预印本 arXiv:1902.09972,
    2019。'
- en: '[38] Y. Wang, E. Sarkar, M. Maniatakos, and S. E. Jabari, “Stop-andgo: Exploring
    backdoor attacks on deep reinforcement learning-based traffic congestion control
    systems,” arXiv preprint arXiv:2003.07859, 2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Wang, E. Sarkar, M. Maniatakos, 和 S. E. Jabari, “Stop-andgo：探索基于深度强化学习的交通拥堵控制系统中的后门攻击，”
    arXiv 预印本 arXiv:2003.07859, 2020。'
- en: '[39] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
    backdoor federated learning,” in AISTATS, 2020.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, 和 V. Shmatikov, “如何对联邦学习进行后门攻击，”
    在 AISTATS，2020。'
- en: '[40] A. N. Bhagoji, S. Chakraborty, P. Mittal, and S. Calo, “Analyzing federated
    learning through an adversarial lens,” in ICML, 2019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] A. N. Bhagoji, S. Chakraborty, P. Mittal, 和 S. Calo, “通过对抗性视角分析联邦学习，”
    在 ICML，2019。'
- en: '[41] A. Salem, Y. Sautter, M. Backes, M. Humbert, and Y. Zhang, “BAAAN: Backdoor
    Attacks Against Autoencoder and GAN-Based Machine Learning Models,” arXiv:2010.03007,
    2020.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Salem, Y. Sautter, M. Backes, M. Humbert, 和 Y. Zhang, “BAAAN：针对自编码器和
    GAN 基础的机器学习模型的后门攻击，” arXiv:2010.03007, 2020。'
- en: '[42] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning your weakness
    into a strength: Watermarking deep neural networks by backdooring,” In 27th USENIX
    Security Symposium (USENIX Security 18), pp. 1615–-1631, 2018.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Adi, C. Baum, M. Cisse, B. Pinkas, 和 J. Keshet, “将你的弱点转化为优势：通过后门攻击对深度神经网络进行水印标记，”
    在第 27 届 USENIX 安全研讨会 (USENIX Security 18)，第 1615–1631 页，2018。'
- en: '[43] J. Guo and M. Potkonjak, “Watermarking deep neural networks for embedded
    systems,” In 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),
    pp. 1–8, 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Guo 和 M. Potkonjak, “为嵌入式系统中的深度神经网络进行水印标记，” 2018 IEEE/ACM 国际计算机辅助设计大会
    (ICCAD) 论文集，第 1–8 页，2018。'
- en: '[44] D. M. Sommer, L. Song, S. Wagh, and P. Mittal, “Towards probabilistic
    verification of machine unlearning,” arXiv preprint arXiv:2003.04247, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] D. M. Sommer, L. Song, S. Wagh, 和 P. Mittal, “朝着机器遗忘的概率验证方向发展，” arXiv
    预印本 arXiv:2003.04247, 2020。'
- en: '[45] Y. Li, Z. Zhang, J. Bai, B. Wu, Y. Jiang, and S.-T. Xia, “Open-sourced
    dataset protection via backdoor watermarking,” in NeurIPS Workshop, 2020.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Y. Li, Z. Zhang, J. Bai, B. Wu, Y. Jiang, 和 S.-T. Xia, “通过后门水印保护开源数据集，”
    在 NeurIPS 研讨会，2020。'
- en: '[46] S. Zhao, X. Ma, Y. Wang, J. Bailey, B. Li, and Y.-G. Jiang, “What do deep
    nets learn? class-wise patterns revealed in the input space,” arXiv preprint arXiv:2101.06898,
    2021.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] S. Zhao, X. Ma, Y. Wang, J. Bailey, B. Li, 和 Y.-G. Jiang, “深度网络学到了什么？输入空间中的类别模式揭示，”
    arXiv 预印本 arXiv:2101.06898, 2021。'
- en: '[47] Y. S. Lin, W. C. Lee, and Z. B. Celik, “What do you see? evaluation of
    explainable artificial intelligence (xai) interpretability through neural backdoors,”
    arXiv preprint arXiv:2009.10639, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. S. Lin, W. C. Lee, 和 Z. B. Celik, “你看到了什么？通过神经后门评估可解释人工智能 (XAI) 的可解释性，”
    arXiv 预印本 arXiv:2009.10639, 2020。'
- en: '[48] T. Baluta, S. Shen, S. Shinde, K. S. Meel, and P. Saxena, “ Quantitative
    Verification of Neural Networks And its Security Applications,” arXiv preprint
    arXiv:1906.10395, 2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Baluta, S. Shen, S. Shinde, K. S. Meel, 和 P. Saxena，“神经网络的定量验证及其安全应用，”
    arXiv 预印本 arXiv:1906.10395, 2019。'
- en: '[49] Z. He, T. Zhang, and R. Lee, “Sensitive-Sample Fingerprinting of Deep
    Neural Networks,” In Proceedings of the IEEE Conference on Computer Vision and
    Pattern Recognition, pp. 4729–-4737, 2019.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. He, T. Zhang, 和 R. Lee，“深度神经网络的敏感样本指纹，” 在 IEEE 计算机视觉与模式识别会议论文集，pp.
    4729–4737, 2019。'
- en: '[50] N. B. Erichson, D. Taylor, Q. Wu, and M. W. Mahoney, “Noise-Response Analysis
    of Deep Neural Networks Quantifies Robustness and Fingerprints Structural Malware,”
    arXiv arXiv:2008.00123 , 2020.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] N. B. Erichson, D. Taylor, Q. Wu, 和 M. W. Mahoney，“深度神经网络的噪声响应分析量化了鲁棒性并标记结构性恶意软件，”
    arXiv arXiv:2008.00123, 2020。'
- en: '[51] J. Clements and Y. Lao, “Hardware trojan attacks on neural networks,”
    arXiv preprint arXiv:1806.05768, 2018.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. Clements 和 Y. Lao，“针对神经网络的硬件木马攻击，” arXiv 预印本 arXiv:1806.05768, 2018。'
- en: '[52] N. Baracaldo, B. Chen, H. Ludwig, A. Safavi, and R. Zhang, “Detecting
    Poisoning Attacks on Machine Learning in IoT Environments,” In 2018 IEEE International
    Congress on Internet of Things ICIOT), pp. 57–64, 2018.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] N. Baracaldo, B. Chen, H. Ludwig, A. Safavi, 和 R. Zhang，“检测物联网环境中的机器学习中毒攻击，”
    2018 IEEE 国际物联网大会 (ICIOT), pp. 57–64, 2018。'
- en: '[53] Y. Liu, W. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang, “ABS: Scanning
    Neural Networks for Back-doors by Artificial Brain Stimulation,” In Proceedings
    of the 2019 ACM SIGSAC Conference on Computer and Communications Security. ACM,
    pp. 1265–-1282, 2019.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Y. Liu, W. Lee, G. Tao, S. Ma, Y. Aafer, 和 X. Zhang，“ABS: 通过人工脑刺激扫描神经网络中的后门，”
    在 2019 年 ACM SIGSAC 计算机与通信安全会议论文集。ACM, pp. 1265–1282, 2019。'
- en: '[54] A. Chakarov, A. Nori, S. Rajamani, S. Sen, and D. Vijaykeerthy, “Debugging
    Machine Learning Tasks,” arXiv:cs.LG/1603.07292, 2016.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. Chakarov, A. Nori, S. Rajamani, S. Sen, 和 D. Vijaykeerthy，“调试机器学习任务，”
    arXiv:cs.LG/1603.07292, 2016。'
- en: '[55] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U.
    Saini, C. Sutton, J. D. Tygar, and K. Xia, “Misleading Learners: Co-opting Your
    Spam Filter,” pp. 17–-51, 2009.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] B. Nelson, M. Barreno, F. J. Chi, A. D. Joseph, B. I. P. Rubinstein, U.
    Saini, C. Sutton, J. D. Tygar, 和 K. Xia，“误导学习者：劫持你的垃圾邮件过滤器，” pp. 17–51, 2009。'
- en: '[56] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “ Detecting
    AI Trojans Using Meta Neural Analysis,” arXiv preprint arXiv:1910.03137, 2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, 和 B. Li，“使用元神经分析检测 AI
    木马，” arXiv 预印本 arXiv:1910.03137, 2019。'
- en: '[57] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “ DeepInspect: A Black-box
    Trojan Detection and Mitigation Framework for Deep Neural Networks,” AAAI Press,
    pp. 4658–-4664, 2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] H. Chen, C. Fu, J. Zhao, 和 F. Koushanfar，“DeepInspect: 深度神经网络的黑箱木马检测和缓解框架，”
    AAAI Press, pp. 4658–4664, 2019。'
- en: '[58] Y. Gao, Y.e Kim, B. G. Doan, Z. Zhang, G. Zhang, S. Nepal, D. C. Ranasinghe,
    and H. Kim, “ Design and Evaluation of a Multi-Domain Trojan Detection Method
    on Deep Neural Networks,” arXiv preprint arXiv:1911.10312 , 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Y. Gao, Y.e Kim, B. G. Doan, Z. Zhang, G. Zhang, S. Nepal, D. C. Ranasinghe,
    和 H. Kim，“深度神经网络多领域木马检测方法的设计与评估，” arXiv 预印本 arXiv:1911.10312, 2019。'
- en: '[59] Z. Xiang, D. J. Miller, and G. Kesidis, “ Revealing Backdoors, Post-Training,
    in DNN Classifiers via Novel Inference on Optimized Perturbations Inducing Group
    Misclassification,” arXiv preprint arXiv:1908.10498 , 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Z. Xiang, D. J. Miller, 和 G. Kesidis，“通过优化扰动引起的组误分类在 DNN 分类器中揭示后门，训练后，”
    arXiv 预印本 arXiv:1908.10498, 2019。'
- en: '[60] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “ Universal Litmus
    Patterns: Revealing Backdoor Attacks in CNNs,” aarXiv preprint arXiv:1906.10842
    , 2019.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] S. Kolouri, A. Saha, H. Pirsiavash, 和 H. Hoffmann，“通用试金石模式：揭示 CNN 中的后门攻击，”
    arXiv 预印本 arXiv:1906.10842, 2019。'
- en: '[61] X. Huang, M. Alzantot, and M. Srivastava, “Neuroninspect: Detecting backdoors
    in neural networks via output explanations,” arXiv preprint arXiv:1911.07399,
    2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Huang, M. Alzantot, 和 M. Srivastava，“Neuroninspect: 通过输出解释检测神经网络中的后门，”
    arXiv 预印本 arXiv:1911.07399, 2019。'
- en: '[62] S. Huang, W. Peng, Z. Jia, and Z. Tu, “One-pixel signature: Characterizing
    cnn models for backdoor detection,” in ECCV, 2020.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Huang, W. Peng, Z. Jia, 和 Z. Tu，“一像素签名：表征 CNN 模型以进行后门检测，” 在 ECCV, 2020。'
- en: '[63] R. Wang, G. Zhang, S. Liu, P.-Y. Chen, J. Xiong, and M. Wang, “Practical
    detection of trojan neural networks: Data-limited and datafree cases,” in ECCV,
    2020.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Wang, G. Zhang, S. Liu, P.-Y. Chen, J. Xiong, 和 M. Wang，“木马神经网络的实际检测：数据有限和无数据情况，”
    在 ECCV, 2020。'
- en: '[64] K. Yoshida and T. Fujino, “Disabling backdoor and identifying poison data
    by using knowledge distillation in backdoor attacks on deep neural networks,”
    in CCS Workshop, 2020.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] K. Yoshida 和 T. Fujino，“通过使用知识蒸馏在深度神经网络的后门攻击中禁用后门并识别毒害数据，”发表于CCS工作坊，2020。'
- en: '[65] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Neural attention distillation:
    Erasing backdoor triggers from deep neural networks,” in ICLR, 2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, 和 X. Ma，“神经注意力蒸馏：从深度神经网络中消除后门触发器，”发表于ICLR，2021。'
- en: '[66] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “ Detecting
    AI Trojans Using Meta Neural Analysis,” arXiv preprint arXiv:1910.03137, 2019.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, 和 B. Li，“使用元神经分析检测AI特洛伊木马，”arXiv预印本
    arXiv:1910.03137, 2019。'
- en: '[67] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” In 2017 IEEE International
    Conference on Computer Design (ICCD), pp. 45–48, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y. Liu, Y. Xie, 和 A. Srivastava，“Neural trojans，”发表于2017年IEEE国际计算机设计大会（ICCD），第45–48页，2019。'
- en: '[68] P. Zhao, P.-Y. Chen, P. Das, K. N. Ramamurthy, and X. Lin, “Bridging mode
    connectivity in loss landscapes and adversarial robustness,” in ICLR, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] P. Zhao, P.-Y. Chen, P. Das, K. N. Ramamurthy, 和 X. Lin，“弥合损失景观中的模式连接性与对抗鲁棒性，”发表于ICLR，2020。'
- en: '[69] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against
    backdooring attacks on deep neural networks,” in RAID, 2018.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] K. Liu, B. Dolan-Gavitt, 和 S. Garg，“Fine-pruning：防御深度神经网络的后门攻击，”发表于RAID，2018。'
- en: '[70] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao,
    “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,”
    2019.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, 和 B. Y. Zhao，“Neural
    cleanse：识别和缓解神经网络中的后门攻击，”2019。'
- en: '[71] W. Guo, L. Wang, X. Xing, M. Du, and D. Song, “ TABOR: A Highly Accurate
    Approach to Inspecting and Restoring Trojan Backdoors in AI Systems,” arXiv preprint
    arXiv:1908.01763, 2019.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] W. Guo, L. Wang, X. Xing, M. Du, 和 D. Song，“TABOR：一种高度准确的AI系统特洛伊后门检查与修复方法，”arXiv预印本
    arXiv:1908.01763, 2019。'
- en: '[72] X. Qiao, Y. Yang, and H. Li, “ Defending neural backdoors via generative
    distribution modeling,” in NeurIPS, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X. Qiao, Y. Yang, 和 H. Li，“通过生成分布建模防御神经网络后门，”发表于NeurIPS，2019。'
- en: '[73] K. Davaslioglu and Y. E. Sagduyu, “ Trojan attacks on wireless signal
    classification with adversarial machine learning,” in DySPAN, 2019.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] K. Davaslioglu 和 Y. E. Sagduyu，“使用对抗机器学习进行无线信号分类的特洛伊攻击，”发表于DySPAN，2019。'
- en: '[74] L. Zhu, R. Ning, C. Wang, C. Xin, and H. Wu, “ Gangsweep: Sweep out neural
    backdoors by gan,” in ACM MM, 2020.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] L. Zhu, R. Ning, C. Wang, C. Xin, 和 H. Wu，“Gangsweep：通过GAN清除神经后门，”发表于ACM
    MM，2020。'
- en: '[75] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I.
    Molloy, and B. Srivastava, “ Detecting backdoor attacks on deep neural networks
    by activation clustering,” arXiv preprint arXiv:1811.03728, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I.
    Molloy, 和 B. Srivastava，“通过激活聚类检测深度神经网络中的后门攻击，”arXiv预印本 arXiv:1811.03728, 2018。'
- en: '[76] G. Shen, Y. Liu, G. Tao, S. An, Q. Xu, S. Cheng, S. Ma, and X. Zhang,
    “ Backdoor scanning for deep neural networks through karm optimization,” arXiv
    preprint arXiv:2102.05123, 2021.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] G. Shen, Y. Liu, G. Tao, S. An, Q. Xu, S. Cheng, S. Ma, 和 X. Zhang，“通过karm优化扫描深度神经网络中的后门，”arXiv预印本
    arXiv:2102.05123, 2021。'
- en: '[77] A. K. Veldanda, K. Liu, B. Tan, P. Krishnamurthy, F. Khorrami, R. Karri,
    B. Dolan-Gavitt, and S. Garg, “Nnoculation: Broad spectrum and targeted treatment
    of backdoored dnns,” arXiv preprint arXiv:2002.08313, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] A. K. Veldanda, K. Liu, B. Tan, P. Krishnamurthy, F. Khorrami, R. Karri,
    B. Dolan-Gavitt, 和 S. Garg，“Nnoculation：针对后门DNN的广谱和针对性治疗，”arXiv预印本 arXiv:2002.08313,
    2020。'
- en: '[78] B. G. Doan, E. Abbasnejad, and D. Ranasinghe, “DeepCleanse: A Black-box
    Input Sanitization Framework Against BackdoorAttacks on DeepNeural Networks,”
    arXiv preprint arXiv:1908.03369 , 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] B. G. Doan, E. Abbasnejad, 和 D. Ranasinghe，“DeepCleanse：一种黑箱输入消毒框架，针对深度神经网络的后门攻击，”arXiv预印本
    arXiv:1908.03369, 2019。'
- en: '[79] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” In 2017 IEEE International
    Conference on Computer Design (ICCD), 2017.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Y. Liu, Y. Xie, 和 A. Srivastava，“Neural trojans，”发表于2017年IEEE国际计算机设计大会（ICCD），2017。'
- en: '[80] S. Udeshi, S. Peng, Gerald Woo, L. Loh, L. Rawshan, and S. Chattopadhyay,
    “Model Agnostic Defence against Backdoor Attacks in Machine Learning,” arXiv preprint
    arXiv:1908.02203, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. Udeshi, S. Peng, Gerald Woo, L. Loh, L. Rawshan, 和 S. Chattopadhyay，“模型不可知的机器学习后门攻击防御，”arXiv预印本
    arXiv:1908.02203, 2019。'
- en: '[81] M. Villarreal-Vasquez and B. Bhargava, “Confoc: Content-focus protection
    against trojan attacks on neural networks,” arXiv preprint arXiv:2007.00711, 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] M. Villarreal-Vasquez 和 B. Bhargava，“Confoc：针对神经网络特洛伊攻击的内容聚焦保护，”arXiv预印本
    arXiv:2007.00711, 2020。'
- en: '[82] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Rethinking the trigger
    of backdoor attack,” arXiv preprint arXiv:2004.04692, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li 和 S. Xia, “重新思考后门攻击的触发器，” arXiv
    预印本 arXiv:2004.04692, 2020。'
- en: '[83] Y. Zeng, H. Qiu, S. Guo, T. Zhang, M. Qiu, and B. Thuraisingham, “Deepsweep:
    An evaluation framework for mitigating dnn backdoor attacks using data augmentation,”
    arXiv preprint arXiv:2012.07006, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Y. Zeng, H. Qiu, S. Guo, T. Zhang, M. Qiu 和 B. Thuraisingham, “Deepsweep：一个用于通过数据增强缓解
    DNN 后门攻击的评估框架，” arXiv 预印本 arXiv:2012.07006, 2020。'
- en: '[84] M. Du, R. Jia, and D. Song, “Robust anomaly detection and backdoor attack
    detection via differential privacy,” in ICLR, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M. Du, R. Jia 和 D. Song, “通过差分隐私进行鲁棒异常检测和后门攻击检测，” 见 ICLR, 2020。'
- en: '[85] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitras¸, and N. Papernot, “On
    the effectiveness of mitigating data poisoning attacks with gradient shaping,”
    arXiv preprint arXiv:2002.11497, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitras¸ 和 N. Papernot, “关于通过梯度塑形缓解数据中毒攻击的有效性，”
    arXiv 预印本 arXiv:2002.11497, 2020。'
- en: '[86] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,”
    in NeurIPS, 2018.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] B. Tran, J. Li 和 A. Madry, “后门攻击中的频谱特征，” 见 NeurIPS, 2018。'
- en: '[87] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I.
    Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural networks
    by activation clustering,” in AAAI Workshop, 2019.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I.
    Molloy 和 B. Srivastava, “通过激活聚类检测深度神经网络的后门攻击，” 见 AAAI Workshop, 2019。'
- en: '[88] D. Tang, X. Wang, H. Tang, and K. Zhang, “Demon in the variant: Statistical
    analysis of dnns for robust backdoor contamination detection,” in USENIX Security,
    2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] D. Tang, X. Wang, H. Tang 和 K. Zhang, “变种中的恶魔：对 DNN 进行鲁棒的后门污染检测的统计分析，”
    见 USENIX Security, 2021。'
- en: '[89] E. Soremekun, S. Udeshi, S. Chattopadhyay, and A. Zeller, “Exposing backdoors
    in robust machine learning models,” arXiv preprint arXiv:2003.00865, 2020.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] E. Soremekun, S. Udeshi, S. Chattopadhyay 和 A. Zeller, “揭示鲁棒机器学习模型中的后门，”
    arXiv 预印本 arXiv:2003.00865, 2020。'
- en: '[90] E. Chou, F. Tramer, and G. Pellegrino, “Sentinet: Detecting localized
    universal attacks against deep learning systems,” in IEEE S&P Workshop, 2020.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] E. Chou, F. Tramer 和 G. Pellegrino, “Sentinet：检测针对深度学习系统的局部通用攻击，” 见 IEEE
    S&P Workshop, 2020。'
- en: '[91] M. Subedar, N. Ahuja, R. Krishnan, I. J. Ndiour, and O. Tickoo, “Deep
    probabilistic models to detect data poisoning attacks,” in NeurIPS Workshop, 2019.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. Subedar, N. Ahuja, R. Krishnan, I. J. Ndiour 和 O. Tickoo, “深度概率模型检测数据中毒攻击，”
    见 NeurIPS Workshop, 2019。'
- en: '[92] M. Du, R. Jia, and D. Song, “Robust anomaly detection and backdoor attack
    detection via differential privacy,” in ICLR, 2020.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] M. Du, R. Jia 和 D. Song, “通过差分隐私进行鲁棒异常检测和后门攻击检测，” 见 ICLR, 2020。'
- en: '[93] M. Javaheripi, M. Samragh, G. Fields, T. Javidi, and F. Koushanfar, “Cleann:
    Accelerated trojan shield for embedded neural networks,” in ICCAD, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] M. Javaheripi, M. Samragh, G. Fields, T. Javidi 和 F. Koushanfar, “Cleann：加速嵌入式神经网络的特洛伊盾，”
    见 ICCAD, 2020。'
- en: '[94] B. Wang, X. Cao, N. Z. Gong et al., “On certifying robustness against
    backdoor attacks via randomized smoothing,” in CVPR Workshop, 2020.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] B. Wang, X. Cao, N. Z. Gong 等, “通过随机平滑认证对后门攻击的鲁棒性，” 见 CVPR Workshop, 2020。'
- en: '[95] M. Weber, X. Xu, B. Karlas, C. Zhang, and B. Li, “Rab: Provable robustness
    against backdoor attacks,” arXiv preprint arXiv:2003.08904, 2020.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] M. Weber, X. Xu, B. Karlas, C. Zhang 和 B. Li, “Rab：针对后门攻击的可证明鲁棒性，” arXiv
    预印本 arXiv:2003.08904, 2020。'
- en: '[96] Cybiant, “What is Deep Learning,” available: https://www.cybiant.com/resources/what-is-deep-learning/.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Cybiant, “什么是深度学习，” 可用网址： https://www.cybiant.com/resources/what-is-deep-learning/。'
- en: '[97] A. Krizhevsky, I. Sutskever, and G. Hinton, “ImageNet classification with
    deep convolutional neural networks,” in Proc. Advances in Neural Information Processing
    Systems, 2012.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] A. Krizhevsky, I. Sutskever 和 G. Hinton, “使用深度卷积神经网络进行 ImageNet 分类，” 见
    Proc. Advances in Neural Information Processing Systems, 2012。'
- en: '[98] J. Tompson, A. Jain, Y. LeCun, and C. Bregler, “Joint training of a convolutional
    network and a graphical model for human pose estimation,” in Proc. Advances in
    Neural Information Processing Systems, 2012.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Tompson, A. Jain, Y. LeCun 和 C. Bregler, “卷积网络和图形模型的联合训练用于人体姿态估计，”
    见 Proc. Advances in Neural Information Processing Systems, 2012。'
- en: '[99] G. Hinton, “Deep neural networks for acoustic modeling in speech recognition,”
    in IEEE Signal Processing Magazine, 2012.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] G. Hinton, “用于语音识别的深度神经网络的声学建模，” 见 IEEE Signal Processing Magazine, 2012。'
- en: '[100] T. Mikolov, A. Deoras, D. Povey, L. Burget and J. Cernocky, “Strategies
    for training large scale neural network language models,” in Proc. Automatic Speech
    Recognition and Understanding, 2011.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Bengio, R. Duncharme, and P. Vincent, “A neural probabilistic language
    model,” in Proc. Advances in Neural Information Processing Systems, 2001.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] 3Blue1Brown, But what is a neural network? — Chapter 1, Deep learning,
    Accessed on: 17\. 07, 2021\. [Video file]. Available: [https://www.youtube.com/watch?v=aircAruvnKk&t=588s](https://www.youtube.com/watch?v=aircAruvnKk&t=588s)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Liu, S. Ma, T. Aafer, W. Lee, J. Zhai, W. Wang, and X. Zhang, ”Trojaning
    Attack on Neural Networks,” 2018, Available: https://www.ndss-symposium.org/wp-content/uploads/2018/03/NDSS2018_03A-5_Liu_Slides.pdf'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. J. Schalkoff, Artificial Neural Network, vol.1 New York, NY, USA:
    McGraw-Hill, 1997.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. W. White and F. Rosenblatt, ”Principles of neurodynamics: Perceptrons
    and the theory of brain mechanisms,” Amer. J. Psychol., vol. 76, no. 4, pp. 705,
    1963.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. James, M. Ren, S. Manivasagam, M. Liang, B. Yang, R. Du, F. Cheng,
    and R. Urtasun. “Physically Realizable Adversarial Examples for LiDAR Object Detection”.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
    13716–13725, 2020.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] S. Sun, N. Akhtar, H. Song, A. Mian, and M. Shah. ”Deep affinity network
    for multiple object tracking.” IEEE transactions on pattern analysis and machine
    intelligence 43, no. 1 (2019): 104-119.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] N. Akhtar, A. Mian, N. Karda, and M. Shah. “Advances in adversarial attacks
    and defenses in computer vision: A survey”, arXiv preprint, arXiv:2108.00401,
    2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus. ”Intriguing properties of neural networks.” arXiv preprint arXiv:1312.6199,
    2013.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] N. Akhtar, and A. Mian, “Threat of adversarial attacks on deep learning
    in computer vision: A survey”. IEEE Access, 6, pp.14410-14430, 2018.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Anti-Backdoor Learning:
    Training Clean Models on Poisoned Data”, arXiv preprint, arXiv:2110.11571, 2021.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] D. Wu, and Y. Wang, ” Adversarial Neuron Pruning Purifies Backdoored
    Deep Models”, arXiv preprint, arXiv:2110.14430, 2021.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Boloor, T. Wu, P. Naughton, A. Chakrabarti, X. Zhang and Y. Vorobeychik,
    ”Can Optical Trojans Assist Adversarial Perturbations?,” 2021 IEEE/CVF International
    Conference on Computer Vision Workshops (ICCVW), 2021, pp. 122-131, doi: 10.1109/ICCVW54120.2021.00019,
    2021.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Li, Y. Li, B. Wu, L. Li, R. He, and S. Lyu, ”Invisible Backdoor Attack
    with Sample-Specific Triggers”, in ICCV, 2021.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] T. Huster and E. Ekwedike, ”TOP: Backdoor Detection in Neural Networks
    via Transferability of Perturbation”, arXiv preprint, arXiv:2103.10274, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Zheng, Y. Zhang, H. Wagner, M. Goswami, and C. Chen, ”Topological
    Detection of Trojaned Neural Networks”, arXiv preprint, arXiv:2106.06469, 2021.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] S. Zheng, Y. Zhang, H. Wagner, M. Goswami, 和 C. Chen，“特洛伊神经网络的拓扑检测”，arXiv
    预印本，arXiv:2106.06469，2021年。'
- en: '[117] K. Doan, Y. Lao, and P. Li, ” Backdoor Attack with Imperceptible Input
    and Latent Modification”, NeurIPS 2021.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] K. Doan, Y. Lao, 和 P. Li，“具有不可察觉输入和潜在修改的后门攻击”，NeurIPS 2021。'
- en: '[118] K. Doan, Y. Kao, W. Zhao, and P. Li, ”LIRA: Learnable, Imperceptible
    and Robust Backdoor Attacks”, ICCV 2021, 2021.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] K. Doan, Y. Kao, W. Zhao, 和 P. Li，“LIRA: 可学习、不可察觉且鲁棒的后门攻击”，ICCV 2021，2021年。'
- en: '[119] M. Xue, X. Wang, S. Sun, Y. Zhang, J. Wang, and W. Liu, ”Compression-Resistant
    Backdoor Attack against Deep Neural Networks”, arXiv preprint, arXiv:2201.00672,
    2021.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Xue, X. Wang, S. Sun, Y. Zhang, J. Wang, 和 W. Liu，“抵抗压缩的深度神经网络后门攻击”，arXiv
    预印本，arXiv:2201.00672，2021年。'
- en: '[120] A. Nguyen, and A. Tran, ”WaNet – Imperceptible Warping-based Backdoor
    Attack”, In Proceedings of the 9th International Conference on Learning Representation
    (ICLR), Virtual Event, Austria, 2021.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Nguyen 和 A. Tran，“WaNet – 不可察觉的基于扭曲的后门攻击”，在第9届国际学习表征会议（ICLR）论文集中，虚拟会议，奥地利，2021年。'
- en: '[121] S. Cheng, Y. Liu, S. Ma, and X. Zhang, ”Deep Feature Space Trojan Attack
    of Neural Networks by Controlled Detoxification”, arXiv preprint, arXiv:2012.11212,
    ICLR, 2021.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] S. Cheng, Y. Liu, S. Ma, 和 X. Zhang，“通过控制解毒进行的深度特征空间特洛伊攻击”，arXiv 预印本，arXiv:2012.11212，ICLR，2021年。'
- en: '[122] T. Li, J. Hua, H. Wang, C. Chen, and Y. Liu, ”DeepPayload: Black-box
    Backdoor Attack on Deep Learning Models through Neural Payload Injection”, arXiv
    preprint, arXiv:2101.06896, ICSE 2021, 2021.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Li, J. Hua, H. Wang, C. Chen, 和 Y. Liu，“DeepPayload: 通过神经负载注入对深度学习模型的黑箱后门攻击”，arXiv
    预印本，arXiv:2101.06896，ICSE 2021，2021年。'
- en: '[123] H. Fu, A. K. Veldanda, P. Krishnamurthy, S. Garg and F. Khorrami, ”A
    Feature-Based On-Line Detector to Remove Adversarial-Backdoors by Iterative Demarcation,”
    in IEEE Access, doi: 10.1109/ACCESS.2022.3141077.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] H. Fu, A. K. Veldanda, P. Krishnamurthy, S. Garg 和 F. Khorrami，“基于特征的在线检测器，通过迭代划分去除对抗性后门”，IEEE
    Access，doi: 10.1109/ACCESS.2022.3141077。'
- en: '[124] B. Zhao, and Y. Lao, ”Towards Class-Oriented Poisoning Attacks Against
    Neural Networks”, arXiv preprint, arXiv:2008.00047, 2021.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] B. Zhao 和 Y. Lao，“面向神经网络的类导向中毒攻击”，arXiv 预印本，arXiv:2008.00047，2021年。'
- en: '[125] H. Kwon, ”Multi-Model Selective Backdoor Attack with Different Trigger
    Positions”, EICE TRANSACTIONS on Information and Systems Vol.E105-D No.1 pp.170-174,
    2022.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] H. Kwon，“具有不同触发位置的多模型选择性后门攻击”，EICE TRANSACTIONS on Information and Systems，第E105-D卷第1期，第170-174页，2022年。'
- en: '[126] J.Hayase, W. Kong, R. Somani, and S. Oh, ”Proceedings of the 38th International
    Conference on Machine Learning”, PMLR 139:4129-4139, 2021.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Hayase, W. Kong, R. Somani, 和 S. Oh，“第38届国际机器学习大会论文集”，PMLR 139:4129-4139，2021年。'
- en: '[127] M. Barni, K.Kallas, and B. Tondi, ”A new backdoor attack in CNNS by training
    set corruption without label poisoning.”, In Proceedings of the 2019 IEEE International
    Conference on Image Processing (ICIP), pages 101–105, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] M. Barni, K. Kallas, 和 B. Tondi，“通过训练集污染而非标签中毒的新型卷积神经网络后门攻击”，在2019 IEEE国际图像处理会议（ICIP）论文集中，第101–105页，2019年。'
- en: '[128] ”CIFAR-10 and CIFAR-100 datasets”, Cs.toronto.edu, 2022\. [Online]. Available:
    https://www.cs.toronto.edu/ kriz/cifar.html. [Accessed: 16- Jan- 2022].'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] “CIFAR-10和CIFAR-100数据集”，Cs.toronto.edu，2022年。[在线]。可用网址：https://www.cs.toronto.edu/kriz/cifar.html。[访问日期：2022年1月16日]。'
- en: '[129] ”CIFAR-10 and CIFAR-100 datasets”, Cs.toronto.edu, 2022\. [Online]. Available:
    https://www.cs.toronto.edu/ kriz/cifar.html. [Accessed: 16- Jan- 2022].'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] “CIFAR-10和CIFAR-100数据集”，Cs.toronto.edu，2022年。[在线]。可用网址：https://www.cs.toronto.edu/kriz/cifar.html。[访问日期：2022年1月16日]。'
