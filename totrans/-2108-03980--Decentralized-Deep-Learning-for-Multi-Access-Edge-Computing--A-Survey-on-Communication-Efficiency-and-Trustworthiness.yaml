- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:52:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:52:32'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.03980] Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.03980] 多接入边缘计算的去中心化深度学习：关于通信效率和可信度的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.03980](https://ar5iv.labs.arxiv.org/html/2108.03980)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.03980](https://ar5iv.labs.arxiv.org/html/2108.03980)
- en: 'Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication
    Efficiency and Trustworthiness'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多接入边缘计算的去中心化深度学习：关于通信效率和可信度的综述
- en: 'Yuwei Sun    Hideya Ochiai    and Hiroshi Esaki    \IEEEmembershipMember, IEEE
    19 November 2021\. This work was supported in part by the JRA program at the RIKEN
    Center for Advanced Intelligence Project.Yuwei Sun, Hideya Ochiai, and Hiroshi
    Esaki are with the Graduate School of Information Science and Technology, University
    of Tokyo, Tokyo, 1138654 Japan (e-mail: ywsun@g.ecc.u-tokyo.ac.jp, ochiai@elab.ic.i.u-tokyo.ac.jp,
    hiroshi@wide.ad.jp).This paragraph will include the Associate Editor who handled
    your paper.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Yuwei Sun    Hideya Ochiai    和 Hiroshi Esaki    \IEEEmembershipMember, IEEE
    2021年11月19日。这项工作部分得到了RIKEN高级智能项目JRA计划的支持。Yuwei Sun、Hideya Ochiai和Hiroshi Esaki均为东京大学信息科学与技术研究生院的成员，地址：日本东京1138654（电子邮件：ywsun@g.ecc.u-tokyo.ac.jp，ochiai@elab.ic.i.u-tokyo.ac.jp，hiroshi@wide.ad.jp）。这一段将包含处理您论文的副编辑。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Wider coverage and a better solution to a latency reduction in 5G necessitate
    its combination with multi-access edge computing (MEC) technology. Decentralized
    deep learning (DDL) such as federated learning and swarm learning as a promising
    solution to privacy-preserving data processing for millions of smart edge devices,
    leverages distributed computing of multi-layer neural networks within the networking
    of local clients, whereas, without disclosing the original local training data.
    Notably, in industries such as finance and healthcare where sensitive data of
    transactions and personal medical records is cautiously maintained, DDL can facilitate
    the collaboration among these institutes to improve the performance of trained
    models while protecting the data privacy of participating clients. In this survey
    paper, we demonstrate the technical fundamentals of DDL that benefit many walks
    of society through decentralized learning. Furthermore, we offer a comprehensive
    overview of the current state-of-the-art in the field by outlining the challenges
    of DDL and the most relevant solutions from novel perspectives of communication
    efficiency and trustworthiness.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛的覆盖范围和更好的5G延迟降低解决方案需要将其与多接入边缘计算（MEC）技术结合。去中心化深度学习（DDL），例如联邦学习和群体学习，作为保护隐私的数据处理的有前景的解决方案，利用局部客户端网络中的多层神经网络的分布式计算，而无需公开原始的本地训练数据。特别是在金融和医疗等敏感数据谨慎维护的行业中，DDL可以促进这些机构之间的合作，以提高训练模型的性能，同时保护参与客户端的数据隐私。在这篇综述论文中，我们展示了DDL的技术基础，它通过去中心化学习惠及社会各个领域。此外，我们通过概述DDL面临的挑战以及从通信效率和可信度的新颖视角出发的相关解决方案，提供了该领域当前最先进的技术的全面概述。
- en: '{IEEEImpStatement}'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEImpStatement}'
- en: The proliferation of smart devices and edge applications based on deep learning
    is reshaping the contours of future high-performance edge computing, such as intelligent
    environment sensors, autonomous vehicles, smart grids, and so forth. Decentralized
    deep learning (DDL) as a key enabler of the Multi-access Edge Computing benefits
    society through distributed model training and globally shared training knowledge.
    However, crucial fundamental challenges have to be overcome in the first place
    to make DDL feasible and scalable, which are decentralization techniques, communication
    efficiency, and trustworthiness. This survey offers a comprehensive overview from
    the above perspectives, suggesting that DDL is being intensively studied, especially
    in terms of privacy protection, edge heterogeneity, and adversarial attacks and
    defenses. Moreover, the future trends of DDL put weight on topics like efficient
    resource allocation, asynchronous communication, and fully decentralized frameworks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的智能设备和边缘应用的普及正在重新塑造未来高性能边缘计算的轮廓，例如智能环境传感器、自动驾驶车辆、智能电网等。分散式深度学习（DDL）作为多接入边缘计算的关键推动者，通过分布式模型训练和全球共享的训练知识使整个社会受益。然而，首先必须克服关键的基本挑战，使DDL变得可行和可扩展，即分散化技术、通信效率和可信性。本调查从以上角度提供了全面的概述，表明DDL正在得到全面的研究，特别是在隐私保护、边缘异构性和对抗性攻击与防御方面。此外，DDL的未来趋势侧重于诸如有效资源分配、异步通信和完全分散式框架之类的主题。
- en: '{IEEEkeywords}'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEE关键词}'
- en: Collective intelligence, Data privacy, Distributed computing, Edge computing,
    Information security, Multi-layer neural network
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 集体智能，数据隐私，分布式计算，边缘计算，信息安全，多层神经网络
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep learning (DL) was first proposed to solve problems where a set of training
    data was collected for centralized data processing. In recent years, with the
    rapid advancement in this field, its applications have extended to various industries,
    benefiting people’s life. However, collecting and transmitting such enormous data
    into centralized storage facilities is usually time-consuming, inefficient, and
    with privacy concerns. Limitations in network bandwidths and so on could bring
    in high latency. Moreover, the risk of personal data breaches correlated with
    data transmission to a centralized computing recourse causes data privacy concerns.
    Especially, with the increase of data privacy awareness in society, legal restrictions
    such as the General Data Protection Regulation (GDPR) [[1](#bib.bib1)] have been
    promoted making such a centralized framework even unpractical.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）最初是为了解决需要收集一组训练数据进行集中式数据处理的问题而提出的。近年来，随着这一领域的快速发展，其应用领域已扩展到各个行业，使人们的生活受益。然而，收集和传输如此庞大的数据到集中式存储设施通常是耗时的，低效的，并且存在隐私问题。网络带宽等方面的限制可能会导致高延迟。此外，与向集中式计算资源传输数据相关的个人数据泄露风险引起了数据隐私的担忧。特别是随着社会对数据隐私意识的增强，诸如《通用数据保护条例》（GDPR）[[1](#bib.bib1)]之类的法律限制已经被推广，使得这样的集中式框架甚至不切实际。
- en: In contrast, compared with a centralized framework where clients have to provide
    raw data to a central server for the model training, in a decentralized framework,
    the sensitive data of a client is processed directly on its local device. The
    concept of decentralized deep learning (DDL) was first proposed to facilitate
    the training of a deep network with billions of parameters using tens of thousands
    of CPU cores [[2](#bib.bib2)]. A few years later, the famous federated learning
    (FL) was proposed by Google [[3](#bib.bib3)], allowing privacy-preserving collaborative
    learning among edge devices by leveraging on-device model training and trained
    model sharing. For one thing, local model training greatly reduces the latency
    in a centralized framework. Another important point is that a large system consisting
    of thousands of clients improves its performance by aggregating the results from
    local model training, without disclosing raw training data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，与客户必须向中央服务器提供原始数据进行模型训练的集中式框架相比，在分散式框架中，客户的敏感数据直接在其本地设备上进行处理。分散式深度学习（DDL）的概念首次被提出，旨在利用成千上万个CPU核心进行数十亿参数的深度网络训练[[2](#bib.bib2)]。几年后，谷歌提出了著名的联邦学习（FL）[[3](#bib.bib3)]，通过利用设备上的模型训练和经过训练的模型共享，在边缘设备之间进行保护隐私的协作学习。首先，本地模型训练极大地降低了集中式框架中的延迟。另一个重要的观点是，由成千上万个客户组成的大型系统通过聚合本地模型训练的结果来提高性能，而不会透露原始的训练数据。
- en: Despite the success of FL, in real life, a participating local device typically
    necessitates certain qualifications for efficient local model training. Limitations
    in device memory and computation capability can greatly increase the local training
    time of a client, and network bandwidth limitations can result in the increase
    of clients’ waiting time for transferring models thus causing a delay in an FL
    training cycle. Furthermore, non-independent and identically distributed (Non-IID)
    data of clients results in time-consuming convergence of FL. To tackle the challenge
    of communication efficiency in DDL approaches such as split learning (SL) and
    smart client selection have been proposed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FL取得了成功，但在实际生活中，参与的本地设备通常需要一定的资格才能进行高效的本地模型训练。设备的内存和计算能力的限制会大大增加客户端的本地训练时间，而网络带宽的限制则可能导致客户端在传输模型时等待时间增加，从而导致FL训练周期的延迟。此外，客户端的非独立同分布（Non-IID）数据会导致FL的收敛时间变长。为了解决通信效率的问题，提出了如分割学习（SL）和智能客户端选择等DDL方法。
- en: 'Moreover, towards a future integrated society by leveraging multi-agent multi-access
    edge computing, it necessitates building trust in such emerging technologies,
    i.e. trustworthiness. Nevertheless, recent works have demonstrated that FL may
    not always provide sufficient guarantees to personal data privacy and deep learning
    model integrity. Even in a decentralized framework like FL, an attacker still
    can compromise systems by injecting a trojan into either a client’s local training
    data or its local model, and such an attack can further expand its influence to
    other clients through model sharing. In other cases, an attacker could even steal
    information from clients by observing the transmitted model gradients. To overcome
    these threats, defense strategies aiming to improve systems robustness and detect
    malicious behaviors are applied in FL. To this end, there are three pillars for
    the development of scalable decentralized deep learning covering FL technical
    fundamentals, communication efficiency, and security and privacy (trustworthiness)
    (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，利用多智能体多接入边缘计算来实现未来的集成社会，必须在这些新兴技术中建立信任，即可信度。然而，最近的研究表明，联邦学习（FL）可能并不总是提供足够的个人数据隐私和深度学习模型完整性的保障。即使在像FL这样的去中心化框架中，攻击者仍然可以通过向客户端的本地训练数据或本地模型中注入木马来破坏系统，并且这种攻击还可以通过模型共享进一步扩展其影响到其他客户端。在其他情况下，攻击者甚至可以通过观察传输的模型梯度来窃取客户端的信息。为了克服这些威胁，FL中应用了旨在提高系统鲁棒性和检测恶意行为的防御策略。为此，开发可扩展去中心化深度学习有三个支柱，涵盖FL技术基础、通信效率以及安全和隐私（可信度）（见图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Decentralized Deep Learning for Multi-Access Edge
    Computing: A Survey on Communication Efficiency and Trustworthiness")）。'
- en: '![Refer to caption](img/9fcde5ed7b6e6987bcef3e66848eef32.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9fcde5ed7b6e6987bcef3e66848eef32.png)'
- en: 'Figure 1: Three pillars for scalable decentralized deep learning.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：可扩展去中心化深度学习的三个支柱。
- en: 'This survey paper is organized as follows. Section [2](#S2 "2 Towards Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness") comprehensively demonstrates
    the technical fundamentals for facilitating DDL and relevant applications in various
    fields. Section [3](#S3 "3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness") presents the main challenges
    and promising methodologies for future scalable DDL, from the perspectives of
    communication efficiency under edge heterogeneity and trustworthiness. Section
    [4](#S4 "4 Concluding Remarks ‣ Decentralized Deep Learning for Multi-Access Edge
    Computing: A Survey on Communication Efficiency and Trustworthiness") concludes
    the paper, discussing open challenges and future directions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '本调查论文组织如下。第[2](#S2 "2 Towards Decentralized Deep Learning ‣ Decentralized Deep
    Learning for Multi-Access Edge Computing: A Survey on Communication Efficiency
    and Trustworthiness")节全面展示了促进DDL和相关应用的技术基础。第[3](#S3 "3 Challenges and Methodologies
    towards Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")节从边缘异质性和可信度的角度介绍了未来可扩展DDL的主要挑战和有前景的方法。第[4](#S4
    "4 Concluding Remarks ‣ Decentralized Deep Learning for Multi-Access Edge Computing:
    A Survey on Communication Efficiency and Trustworthiness")节总结了论文，讨论了开放挑战和未来方向。'
- en: 2 Towards Decentralized Deep Learning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 向去中心化深度学习迈进
- en: 2.1 Multi-Access Edge Computing
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多接入边缘计算
- en: According to an annual report from Nokia in 2020 [[4](#bib.bib4)], a large increase
    in the number of broadband IoT and Critical IoT devices will be observed in the
    next five years, such as AR, VR, and cloud robotics. Likewise, the number of massive
    IoT devices like different types of meters and sensors is to increase greatly
    as well. Most of these devices will be operated based on Artificial Intelligence
    (AI). In this regard, with the proliferation of smart devices and applications
    at the network edge based on AI such as intelligent environment sensors, autonomous
    vehicles, health care, smart grid, and so on, AI is playing a key role in data
    processing, knowledge acquisition, and resource management. For instance, AI has
    been used in edge service optimization in the Internet of Vehicles (IoV) [[5](#bib.bib5)]
    and other compelling applications for collective intelligence in wireless networks
    [[6](#bib.bib6)]. Traditionally, data generated on a smart device is sent to a
    remote computing server for processing. Though 5G aims to provide greater connectivity
    for multi-type devices with a big boost in the speed of handling big data, there
    still needs a wider coverage to facilitate efficient data processing. For this
    reason, a better solution to latency reduction is to combine with multi-access
    edge computing (MEC) technology. The inextricably correlated MEC reduces latency
    by leveraging compute resources in a network closer to the end-users, e.g., a
    local server and the gateway.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据诺基亚2020年的一份年度报告[[4](#bib.bib4)]，在接下来的五年中，将观察到宽带物联网和关键物联网设备数量的大幅增长，如AR、VR和云机器人。同样，各种类型的仪表和传感器等大规模物联网设备的数量也将大大增加。这些设备大多数将基于**人工智能（AI）**进行操作。在这方面，随着基于AI的智能设备和应用在网络边缘的普及，如智能环境传感器、自动驾驶汽车、医疗保健、智能电网等，AI在数据处理、知识获取和资源管理中扮演着关键角色。例如，AI已被用于车辆互联网（IoV）[[5](#bib.bib5)]中的边缘服务优化以及无线网络中集体智能的其他有力应用[[6](#bib.bib6)]。传统上，智能设备生成的数据会被发送到远程计算服务器进行处理。虽然5G旨在为多种设备提供更大的连接性，并大幅提升大数据处理速度，但仍需更广泛的覆盖以促进高效的数据处理。因此，降低延迟的更好解决方案是结合多接入边缘计算（MEC）技术。紧密相关的MEC通过利用更接近终端用户的网络计算资源（如本地服务器和网关）来减少延迟。
- en: 2.2 Data Privacy and Decentralized Deep Learning
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 数据隐私与去中心化深度学习
- en: The mathematical model of the perceptron was first proposed back in 1958 [[7](#bib.bib7)],
    which is a probabilistic model for information storage and organization. The multi-layer
    perceptron [[8](#bib.bib8)] adds to the practicability of neural networks, as
    a useful alternative to traditional statistical modeling techniques. Lecun et
    al. [[9](#bib.bib9)] presented deep learning (DL) that allows computational models
    composed of multiple processing layers to learn representations of data with multiple
    levels of abstraction. Nowadays, various DL models have been developed and broadly
    adopted in many walks of society, such as convolutional neural networks (CNNs),
    recurrent neural networks (RNNs), and so forth.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器的数学模型最早在1958年提出[[7](#bib.bib7)]，这是一个用于信息存储和组织的概率模型。多层感知器[[8](#bib.bib8)]增强了神经网络的实用性，是传统统计建模技术的有用替代方案。Lecun等人[[9](#bib.bib9)]提出了深度学习（DL），允许由多个处理层组成的计算模型学习具有多个抽象层次的数据表示。如今，各种深度学习模型已经被开发并广泛应用于社会的许多领域，如卷积神经网络（CNNs）、递归神经网络（RNNs）等。
- en: 'Moreover, there are mainly two topologies of DL for processing distributed
    data, i.e. centralized DL (server-oriented) and decentralized DL (client-oriented
    or server-less) (Fig. [2](#S2.F2 "Figure 2 ‣ 2.2 Data Privacy and Decentralized
    Deep Learning ‣ 2 Towards Decentralized Deep Learning ‣ Decentralized Deep Learning
    for Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")).
    A centralized or stand-alone framework leverages a central high-performance computing
    resource to achieve the desired model performance by collecting data from various
    data sources. In this case, the collected data is usually exposed to the AI algorithm
    on the cloud. In contrast, a decentralized framework is considered as a privacy-preserving
    architecture by leveraging local model training based on distributed data sources
    on resource-constrained devices like smartphones. Since its introduction, the
    decentralized framework [[10](#bib.bib10)] has proliferated in academia and industry.
    Li et al.[[11](#bib.bib11)] further extended the concept of the parameter server
    framework and demonstrated a robust, versatile, and high-performance implementation,
    capable of handling a diverse array of algorithms for distributed machine learning
    problems based on local training data. Moreover, in recent years, federated learning
    (FL) has become one of the most famous decentralized frameworks, which was proposed
    by Google initially to improve the Google Keyboard (Gboard)’s performance in next
    word prediction [[3](#bib.bib3)]. The architecture of FL allows users to take
    full advantage of an AI algorithm without disclosing their original local training
    data, bridging the gap between centralized computing resources and distributed
    data sources. FL achieves a better model by leveraging globally shared model parameters.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习主要有两种处理分布式数据的拓扑结构，即集中式深度学习（面向服务器）和去中心化深度学习（面向客户端或无服务器）（图 [2](#S2.F2 "图
    2 ‣ 2.2 数据隐私与去中心化深度学习 ‣ 2 朝向去中心化深度学习 ‣ 去中心化深度学习在多接入边缘计算中的应用：通信效率与可信度调查")）。集中式或独立框架利用中央高性能计算资源，通过从各种数据源收集数据来实现所需的模型性能。在这种情况下，收集的数据通常暴露在云端的AI算法中。相比之下，去中心化框架被视为一种隐私保护架构，利用基于资源受限设备（如智能手机）上的分布式数据源进行本地模型训练。自引入以来，去中心化框架[[10](#bib.bib10)]在学术界和工业界迅速普及。Li等人[[11](#bib.bib11)]进一步扩展了参数服务器框架的概念，并展示了一种强大、通用且高性能的实现，能够处理基于本地训练数据的多样化分布式机器学习算法。此外，近年来，联邦学习（FL）成为最著名的去中心化框架之一，它最初由谷歌提出，用于提升谷歌键盘（Gboard）在下一个词预测中的表现[[3](#bib.bib3)]。FL的架构允许用户充分利用AI算法而无需披露其原始本地训练数据，弥合了集中计算资源和分布式数据源之间的差距。FL通过利用全球共享的模型参数来实现更好的模型。
- en: '![Refer to caption](img/742371c55a0cecda897f466ca5bdf199.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/742371c55a0cecda897f466ca5bdf199.png)'
- en: 'Figure 2: The rising of decentralized deep learning.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 去中心化深度学习的崛起。'
- en: Furthermore, a fully decentralized framework refers to server-less architectures
    based on technologies such as the blockchain and edge consensus [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)],
    and ad hoc network [[18](#bib.bib18)]. For instance, Swarm Learning (SL) [[12](#bib.bib12)]
    is a decentralized approach that combines edge computing, blockchain-based peer-to-peer
    networking, and other state-of-the-art decentralization technologies for classifying
    diseases with distributed medical data. Moreover, Li et al. [[13](#bib.bib13)]
    presented a decentralized federated learning framework based on blockchain for
    the global model storage and the local model update exchange, where the local
    updates are encrypted and stored in blocks of the blockchain after the consensus
    by the committee. Similarly, Kim et al.[[17](#bib.bib17)] demonstrated an end-to-end
    latency model of chained federated learning architecture, with the optimal block
    generation rate decided by communication, computation, and consensus delays.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，完全去中心化框架指的是基于区块链和边缘共识技术[[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)]的无服务器架构，以及临时网络[[18](#bib.bib18)]。例如，Swarm
    Learning (SL) [[12](#bib.bib12)] 是一种去中心化的方法，它结合了边缘计算、基于区块链的点对点网络以及其他前沿的去中心化技术，用于处理分布式医疗数据的疾病分类。此外，Li
    等人[[13](#bib.bib13)]提出了一种基于区块链的去中心化联邦学习框架，用于全球模型存储和本地模型更新交换，其中本地更新在委员会共识后被加密并存储在区块链的块中。同样，Kim
    等人[[17](#bib.bib17)]展示了一种端到端延迟模型的链式联邦学习架构，其中最优的区块生成率由通信、计算和共识延迟决定。
- en: In recent years, data privacy has been a major concern, exacerbated by social
    events such as the Cambridge Analytica scandal [[19](#bib.bib19)] and FBI-Apple
    encryption dispute [[20](#bib.bib20)]. Data privacy concerns associated with the
    centralized data processing of a traditional DL pipeline necessitate more considerations
    on privacy-preserving system design and data protection strategies. To this end,
    the decentralized framework provides a promising solution to data privacy in large-scale
    multi-agent collaborative learning. For instance, massively decentralized nodes
    can be applied to diverse use cases, such as industrial IoT [[21](#bib.bib21)],
    environment monitoring using diverse sensors [[22](#bib.bib22)], human behavior
    recognition from surveillance cameras [[23](#bib.bib23)], robotics, and connected
    autonomous vehicles control [[24](#bib.bib24), [25](#bib.bib25)], federated network
    intrusion detection across multiple parties [[26](#bib.bib26), [27](#bib.bib27)]
    and so forth.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，数据隐私成为了一个主要关注点，这一问题因剑桥分析丑闻[[19](#bib.bib19)]和FBI-苹果加密争议[[20](#bib.bib20)]等社会事件而加剧。传统深度学习流程的集中数据处理所带来的数据隐私问题，需要在隐私保护系统设计和数据保护策略上更多地考虑。为此，去中心化框架为大规模多智能体协作学习中的数据隐私问题提供了一个有前景的解决方案。例如，大规模去中心化节点可以应用于各种用例，如工业物联网[[21](#bib.bib21)]、使用各种传感器的环境监测[[22](#bib.bib22)]、从监控摄像头进行的人体行为识别[[23](#bib.bib23)]、机器人技术以及联网自主车辆控制[[24](#bib.bib24),
    [25](#bib.bib25)]、跨多个方的联邦网络入侵检测[[26](#bib.bib26), [27](#bib.bib27)]等。
- en: 2.3 Federated Learning from a Network System Perspective
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 从网络系统视角看联邦学习
- en: 2.3.1 Cross-silo and Cross-device
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 跨领域与跨设备
- en: 'The cross-silo setting of FL represents a scenario of multi-party collaborative
    model training (Fig. [3](#S2.F3 "Figure 3 ‣ 2.3.1 Cross-silo and Cross-device
    ‣ 2.3 Federated Learning from a Network System Perspective ‣ 2 Towards Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")), where collected data
    from local devices is sent to an edge server located inside the organization for
    computation. In this case, an upper-level remote server is applied for further
    computation. Data is transparent to all clients inside the organization, but it
    would not be exposed outside it. For instance, healthcare institutes could adopt
    this scheme to share medical images for identifying a rare disease [[28](#bib.bib28)].
    In this case, the cross-silo setting allows the institutes to share insights on
    the disease under data protection. On the other hand, a cross-device setting is
    a more rigorous scenario that collected data should not leave a device. It necessitates
    efficient on-device computing and timely model transmission to a remote server
    directly.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: FL的跨孤岛设置代表了一个多方协作模型训练的场景（见图[3](#S2.F3 "图3 ‣ 2.3.1 跨孤岛与跨设备 ‣ 2.3 从网络系统视角看联邦学习
    ‣ 2 走向去中心化深度学习 ‣ 多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")），其中来自本地设备的数据被发送到组织内部的边缘服务器进行计算。在这种情况下，应用了一个上层的远程服务器进行进一步计算。数据对组织内部的所有客户端是透明的，但不会暴露在组织外部。例如，医疗机构可以采用这种方案来分享医学图像以识别罕见疾病[[28](#bib.bib28)]。在这种情况下，跨孤岛设置允许机构在数据保护下共享关于疾病的见解。另一方面，跨设备设置是一种更严格的场景，收集的数据不能离开设备。这需要高效的设备端计算和及时的模型传输到远程服务器。
- en: '![Refer to caption](img/bac358b251464cc8f5008ab85203141a.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bac358b251464cc8f5008ab85203141a.png)'
- en: 'Figure 3: Federated learning leverages multi-party model sharing sharing for
    privacy-preserving machine learning.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：联邦学习利用多方模型共享实现隐私保护的机器学习。
- en: 2.3.2 Client Selection Policy
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 客户端选择策略
- en: 'Typically, to reduce the latency of waiting, at each round, the Parameter Server
    (PS) randomly selects only a small subset k out of m clients for the local model
    training and broadcasts the current global model w to the selected clients. Then,
    starting from w, each client$i$ updates its local model wi by training on its
    data and transmits the local model update back to the PS. In addition, other client
    selection policies such as cluster-based selection and reinforcement learning-based
    selection are adopted to reduce the time cost for global model convergence (see
    [3.1.2](#S3.SS1.SSS2 "3.1.2 Data Heterogeneity ‣ 3.1 Communication Efficiency
    Under Edge Heterogeneity ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了减少等待的延迟，在每一轮中，参数服务器（PS）随机选择m个客户端中的一个小子集k进行本地模型训练，并将当前的全局模型w广播到选定的客户端。然后，从w开始，每个客户端$i$通过在其数据上进行训练来更新其本地模型wi，并将本地模型更新传回PS。此外，还采用了其他客户端选择策略，如基于集群的选择和基于强化学习的选择，以减少全局模型收敛的时间成本（见[3.1.2](#S3.SS1.SSS2
    "3.1.2 数据异质性 ‣ 3.1 边缘异质性下的通信效率 ‣ 3 可扩展去中心化深度学习的挑战和方法 ‣ 多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")）。
- en: 2.3.3 Synchronism
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 同步
- en: There are two types of client scheduling approaches, i.e., synchronous FL and
    asynchronous FL. In synchronous FL, at each round, the PS waits for the completion
    of all allocated local training. However, in this case, the slowest local training
    task due to a relatively large data volume, computing device constraints, and
    so on becomes the bottleneck of training. In contrast, the asynchronous FL allows
    a client to upload the local update at any stage of the training. Besides, a client
    can offer multiple functions including local model training, network traffic transit,
    and so forth [[29](#bib.bib29), [30](#bib.bib30)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端调度方法有两种，即同步FL和异步FL。在同步FL中，在每一轮中，参数服务器（PS）等待所有分配的本地训练完成。然而，在这种情况下，由于数据量较大、计算设备限制等原因，最慢的本地训练任务成为训练的瓶颈。相比之下，异步FL允许客户端在训练的任何阶段上传本地更新。此外，客户端可以提供多种功能，包括本地模型训练、网络流量转发等[[29](#bib.bib29)、[30](#bib.bib30)]。
- en: 2.3.4 Aggregation
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 聚合
- en: 'As aforementioned, the next round’s global model takes the value of all local
    model updates’ aggregation results. Federated averaging (FedAvg) [[31](#bib.bib31)]
    computes a weighted average as in ([1](#S2.E1 "In 2.3.4 Aggregation ‣ 2.3 Federated
    Learning from a Network System Perspective ‣ 2 Towards Decentralized Deep Learning
    ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication
    Efficiency and Trustworthiness")) to update the global model, given the volume
    of local training data is varying from client to client (the contribution is varying).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，下一轮的全局模型采用所有本地模型更新的聚合结果。联邦平均（FedAvg）[[31](#bib.bib31)] 计算加权平均，如 ([1](#S2.E1
    "在 2.3.4 聚合 ‣ 2.3 从网络系统角度看联邦学习 ‣ 2 朝向去中心化深度学习 ‣ 针对多接入边缘计算的去中心化深度学习：通信效率与可信度的调查"))，以更新全局模型，因为本地训练数据的量因客户端而异（贡献也不同）。
- en: '|  | $w_{t+1}=w_{t}+\sum_{i\in k}\frac{n_{i}}{n_{k}}(w_{t}^{i}-w_{t})$ |  |
    (1) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{t+1}=w_{t}+\sum_{i\in k}\frac{n_{i}}{n_{k}}(w_{t}^{i}-w_{t})$ |  |
    (1) |'
- en: Where $w_{t}$ represents the weights of the current global model, $w_{t+1}$
    is the weights of the next round’s updated global model, $w_{t}^{i}$ is the weights
    of client$i$’s trained local model, and $n_{i}$ and $n_{k}$ represent respectively
    the volume of client$i$’s local training data and that of the total training data
    from all the selected clients.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{t}$ 代表当前全局模型的权重，$w_{t+1}$ 是下一轮更新后的全局模型的权重，$w_{t}^{i}$ 是客户端$i$训练的本地模型的权重，$n_{i}$
    和 $n_{k}$ 分别代表客户端$i$的本地训练数据量和所有选定客户端的总训练数据量。
- en: 'Moreover, robust aggregation strategies aim to drop a malicious update by measuring
    similarity among local model updates (see [3.2.5](#S3.SS2.SSS5 "3.2.5 Defense
    Models ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards Scalable
    Decentralized Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge
    Computing: A Survey on Communication Efficiency and Trustworthiness")). According
    to a local update’s integrity, only qualified updates are aggregated into the
    global model at each round.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，强健的聚合策略通过测量本地模型更新之间的相似性来排除恶意更新（见 [3.2.5](#S3.SS2.SSS5 "3.2.5 防御模型 ‣ 3.2 可信度
    ‣ 3 面临的挑战与方法 ‣ 针对多接入边缘计算的去中心化深度学习：通信效率与可信度的调查")）。根据本地更新的完整性，只有合格的更新才会在每轮中聚合到全局模型中。
- en: 2.3.5 Deep Learning Models
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 深度学习模型
- en: Most of the current studies and applications around FL are based on a supervised
    model, where the model is trained on labeled data for typically a classification
    task. However, in real life, collected data is usually unlabeled and a supervised
    model is not compatible. Deep learning models including unsupervised learning
    and reinforcement learning are not sufficiently studied in the context of FL.
    For instance, by leveraging FL for a reinforcement task of robotics, a global
    agent could learn multiple action policies efficiently from diverse environments
    at the same time [[32](#bib.bib32)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当前大多数有关联邦学习的研究和应用基于监督模型，其中模型在标记数据上进行训练，通常用于分类任务。然而，在现实生活中，收集的数据通常是未标记的，监督模型并不适用。深度学习模型，包括无监督学习和强化学习，在联邦学习背景下尚未充分研究。例如，通过利用联邦学习进行机器人的强化任务，全局代理可以从多样化的环境中同时高效地学习多个动作策略
    [[32](#bib.bib32)]。
- en: 2.3.6 Client Server Network Security
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.6 客户端服务器网络安全
- en: 'From the perspective of a network system, FL encounters threats from mainly
    three components of the systems, i.e., the parameter server (PS), the client,
    and the transmission pathway. The PS is usually well secured and highly maintained
    compared with edge devices. Besides, the communication between the PS and a client
    is also commonly protected through end-to-end encryption. On the other hand, though
    the integrity of a client is verified to participate in the FL training, an edge
    still encounters intrusion by an adversary due to its relatively incomplete defense
    strategies taken at local. In FL, due to all clients have equal access to the
    global model through the aggregated model broadcast at each round, it provides
    a huge attacking surface for the adversary to compromise the systems. To this
    end, we consider that a compromised edge is the main threat to FL systems (see
    also [3.2](#S3.SS2 "3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards
    Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for Multi-Access
    Edge Computing: A Survey on Communication Efficiency and Trustworthiness")).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '从网络系统的角度来看，FL 主要面临来自系统三个组件的威胁，即参数服务器（PS）、客户端和传输路径。与边缘设备相比，PS 通常具有较好的安全性和维护。此外，PS
    与客户端之间的通信通常也通过端到端加密保护。另一方面，尽管客户端的完整性经过验证以参与 FL 训练，但由于本地采取的防御策略相对不完整，边缘仍然会遭遇对手的入侵。在
    FL 中，由于所有客户端在每轮通过聚合模型广播平等访问全球模型，这为对手攻击系统提供了巨大的攻击面。为此，我们认为被攻陷的边缘是 FL 系统的主要威胁（参见
    [3.2](#S3.SS2 "3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards Scalable
    Decentralized Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge
    Computing: A Survey on Communication Efficiency and Trustworthiness")）。'
- en: 3 Challenges and Methodologies towards Scalable Decentralized Deep Learning
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 可扩展的去中心化深度学习面临的挑战与方法
- en: 3.1 Communication Efficiency Under Edge Heterogeneity
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 边缘异质性下的通信效率
- en: Communication efficiency is an important contributor to evaluating the performance
    and scalability of distributed processing. Decentralized deep learning (DDL) can
    reduce computation time by synchronizing the different models during training.
    However, this leads to an increase in communication cost as the model size increases
    or the convergence becomes slow. Notably, one of the largest challenges of scaling
    FL in real life today is that device qualifications at the edge are varying. In
    particular, such heterogeneity lies in two main aspects, i.e., device capability
    and data distribution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通信效率是评估分布式处理性能和可扩展性的一个重要因素。去中心化深度学习（DDL）通过在训练过程中同步不同的模型来减少计算时间。然而，随着模型大小的增加或收敛变慢，这会导致通信成本的增加。值得注意的是，今天在现实中扩展
    FL 最大的挑战之一是边缘设备的资格各异。特别是，这种异质性体现在两个主要方面，即设备能力和数据分布。
- en: Firstly, for the device capability, especially in the case of cross-device FL,
    a DL model is usually operated on a resource-constrained mobile device such as
    smartphones. The capabilities of these mobile devices are varying due to hardware
    limitations and cost constraints. Moreover, the network bandwidth of a local area
    network (LAN) also greatly limits the model transmission efficiency, resulting
    in a delay in the decentralized learning cycle.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于设备能力，特别是在跨设备的联邦学习（FL）情况下，深度学习（DL）模型通常在资源受限的移动设备上运行，例如智能手机。这些移动设备的能力由于硬件限制和成本约束而有所不同。此外，本地网络（LAN）的网络带宽也大大限制了模型传输效率，导致去中心化学习周期的延迟。
- en: 'Secondly, for the data distribution, samples held by different clients are
    typically diverse with different data sizes and distributions, i.e., non-independent
    and identical distributed (non-IID). For example, in a multi-classification task
    with $C$ categories. Each Client$k$ owns a local dataset $D^{(k)}$ that consists
    of samples with unbalanced labels $\{1,2,…,C\}$. Then, Client1 has 80% samples
    from Label1 and Client2 has 80% samples from Label2\. Mathematically speaking,
    suppose that $f_{w}:x\rightarrow y$ denotes a supervised neural network classifier
    with parameters $w$, taking an input $x_{i}\in x$ and outputting a real-valued
    vector where the $j$th element of the output vector represents the probability
    that $x_{i}$ is recognized as class $j$. Given $f_{w}(x)$, the prediction is given
    by $\hat{y}=\mbox{arg}\max_{j}f_{w}(x)_{j}$ where $f_{w}(x)_{j}$ denotes the $j$th
    element of $f_{w}(x)$. We assume the common data distribution $p(x|y)$ is shared
    by all clients in FL, and client$i$ has $p_{i}(y)$. Then when samples held by
    clients are skewed with various $p_{i}(y)$, $p_{i}(x,y)=p(x|y)p_{i}(y)\,s.t.\,p_{i}(y)\neq
    p_{j}(y)$, for all $i\neq j$. Client1 follows $p_{1}(x,y)$ and Client2 follows
    $p_{2}(x,y)$, i.e., they are non-IID. Though the random client selection policy
    in classical FL aims to reduce the time for waiting, the non-IID local data of
    the selected clients could give rise to a time-consuming convergence or even failing
    to converge the global model. In this section, we demonstrate the most relevant
    methodologies used to spread and reduce the amount of data exchanged between the
    server and clients tackling the edge heterogeneity problem (Table [1](#S3.T1 "Table
    1 ‣ 3.1 Communication Efficiency Under Edge Heterogeneity ‣ 3 Challenges and Methodologies
    towards Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，对于数据分布，不同客户端持有的样本通常在数据大小和分布上具有多样性，即非独立同分布（non-IID）。例如，在一个具有 $C$ 类别的多分类任务中。每个客户端$k$
    拥有一个本地数据集 $D^{(k)}$，该数据集包含具有不平衡标签 $\{1,2,…,C\}$ 的样本。然后，客户端1有80%的样本来自标签1，而客户端2有80%的样本来自标签2。数学上讲，假设
    $f_{w}:x\rightarrow y$ 表示一个具有参数 $w$ 的监督神经网络分类器，输入 $x_{i}\in x$ 并输出一个实值向量，其中输出向量的第
    $j$ 个元素表示 $x_{i}$ 被识别为类别 $j$ 的概率。给定 $f_{w}(x)$，预测由 $\hat{y}=\mbox{arg}\max_{j}f_{w}(x)_{j}$
    给出，其中 $f_{w}(x)_{j}$ 表示 $f_{w}(x)$ 的第 $j$ 个元素。我们假设 FL 中所有客户端共享的共同数据分布 $p(x|y)$，并且客户端$i$
    拥有 $p_{i}(y)$。当客户端持有的样本在不同的 $p_{i}(y)$ 下呈现偏斜时，$p_{i}(x,y)=p(x|y)p_{i}(y)\,s.t.\,p_{i}(y)\neq
    p_{j}(y)$，对所有 $i\neq j$。客户端1遵循 $p_{1}(x,y)$，客户端2遵循 $p_{2}(x,y)$，即它们是非独立同分布的。尽管经典FL中的随机客户端选择策略旨在减少等待时间，但所选客户端的非IID本地数据可能导致时间消耗较大的收敛过程，甚至无法收敛到全局模型。在本节中，我们展示了用于传播和减少服务器与客户端之间交换的数据量的最相关的方法，解决边缘异质性问题（表
    [1](#S3.T1 "Table 1 ‣ 3.1 Communication Efficiency Under Edge Heterogeneity ‣
    3 Challenges and Methodologies towards Scalable Decentralized Deep Learning ‣
    Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication
    Efficiency and Trustworthiness")）。'
- en: 'Table 1: Methodologies for Improving communication efficiency under Edge Heterogeneity'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在边缘异质性下提高通信效率的方法
- en: '| Challenge | Work | Year | Methodology | Application |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 挑战 | 工作 | 年份 | 方法 | 应用 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Resource-Constrained | Vepakomma et al. [[33](#bib.bib33)] | 2018 | Split
    Learning | Image classification |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 资源受限 | Vepakomma 等人 [[33](#bib.bib33)] | 2018 | 分割学习 | 图像分类 |'
- en: '| Edge | Nishio et al. [[34](#bib.bib34)] | 2018 | Resource scheduling | Image
    classification |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 边缘 | Nishio 等人 [[34](#bib.bib34)] | 2018 | 资源调度 | 图像分类 |'
- en: '|  | Singh et al. [[35](#bib.bib35)] | 2019 | Split Learning | IoT, Healthcare
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | Singh 等人 [[35](#bib.bib35)] | 2019 | 分割学习 | 物联网、医疗保健 |'
- en: '|  | Thapa et al. [[36](#bib.bib36)] | 2020 | Split Learning | Healthcare,
    Image classification |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | Thapa 等人 [[36](#bib.bib36)] | 2020 | 分割学习 | 医疗保健、图像分类 |'
- en: '|  | Khan et al. [[29](#bib.bib29)] | 2020 | Stackelberg game theory | Image
    classification |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | Khan 等人 [[29](#bib.bib29)] | 2020 | Stackelberg 博弈理论 | 图像分类 |'
- en: '| Data Heterogeneity | Jeong et al. [[37](#bib.bib37)] | 2018 | Federated Augmentation
    | Image classification |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 数据异质性 | Jeong 等人 [[37](#bib.bib37)] | 2018 | 联邦数据增强 | 图像分类 |'
- en: '|  | Sener et al. [[38](#bib.bib38)] | 2018 | K-Center clustering | Image classification
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | Sener 等人 [[38](#bib.bib38)] | 2018 | K-中心聚类 | 图像分类 |'
- en: '|  | Zhao et al. [[39](#bib.bib39)] | 2018 | Data-sharing strategy | Image
    classification |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | Zhao 等人 [[39](#bib.bib39)] | 2018 | 数据共享策略 | 图像分类 |'
- en: '|  | Wang et al. [[40](#bib.bib40)] | 2020 | Reinforcement Learning | Image
    classification |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | Wang 等人 [[40](#bib.bib40)] | 2020 | 强化学习 | 图像分类 |'
- en: '|  | Duan et al. [[41](#bib.bib41)] | 2020 | Data augmentation and rescheduling
    | Image classification |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | Duan 等人 [[41](#bib.bib41)] | 2020 | 数据增强与重新调度 | 图像分类 |'
- en: '|  | Sun et al. [[24](#bib.bib24)] | 2021 | Segmented Federated Learning |
    Cybersecurity |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | Sun 等人 [[24](#bib.bib24)] | 2021 | 分段联邦学习 | 网络安全 |'
- en: 3.1.1 Resource-Constrained Edge
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 资源受限的边缘
- en: 'Despite FL allowing each client to train its local model, the communication
    efficiency of FL is largely limited by the client-side qualifications such as
    network bandwidth, device memory, and computation capability, and so on. Under
    these circumstances, Split Learning (SL) [[36](#bib.bib36), [33](#bib.bib33)]
    was proposed to facilitate model training based on edge cloud computing. In SL,
    a complicated DL model is partitioned into two sub-networks based on a specific
    layer called the cut layer, and then these sub-networks are trained on the client
    and the PS respectively (Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Resource-Constrained
    Edge ‣ 3.1 Communication Efficiency Under Edge Heterogeneity ‣ 3 Challenges and
    Methodologies towards Scalable Decentralized Deep Learning ‣ Decentralized Deep
    Learning for Multi-Access Edge Computing: A Survey on Communication Efficiency
    and Trustworthiness")(a)). For each round, the client leverages forward propagation
    of its local sub-network based on local data, and then sends the intermediate
    representation of local data at the cut layer together with labels (vanilla Split
    Learning) to the PS for completing the forward propagation and the computation
    of loss. Finally, the gradients of the cloud-side sub-network are computed using
    back propagation, and they are sent back to the client for updating its local
    model. As such, for each round’s training, several times of transmission between
    the client and the PS are necessary.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管 FL 允许每个客户端训练其本地模型，但 FL 的通信效率在很大程度上受到客户端端资质的限制，如网络带宽、设备内存和计算能力等。在这些情况下，提出了
    Split Learning (SL) [[36](#bib.bib36), [33](#bib.bib33)] 以便于基于边缘云计算的模型训练。在 SL
    中，复杂的 DL 模型被分为两个子网络，这些子网络基于一个特定的层称为切分层，然后这些子网络分别在客户端和 PS 上进行训练（图 [4](#S3.F4 "Figure
    4 ‣ 3.1.1 Resource-Constrained Edge ‣ 3.1 Communication Efficiency Under Edge
    Heterogeneity ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")(a)）。每一轮中，客户端利用本地数据进行其本地子网络的前向传播，然后将切分层的本地数据中间表示及标签（原始
    Split Learning）发送给 PS，以完成前向传播和损失计算。最终，云端子网络的梯度通过反向传播计算，并被发送回客户端以更新其本地模型。因此，每轮训练中，客户端与
    PS 之间需要多次传输。'
- en: '![Refer to caption](img/8ff8143fd3bfe11244ddad61ee2fea77.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ff8143fd3bfe11244ddad61ee2fea77.png)'
- en: 'Figure 4: (a) The architecture of the vanilla Split Learning [[33](#bib.bib33)].
    (b) A model performance comparison between FL and SL regarding the number of total
    model parameters and the number of total clients. The hyperbola shows the regions
    where one model outperforms the other regarding communication efficiency [[35](#bib.bib35)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4： (a) 原始 Split Learning [[33](#bib.bib33)] 的架构。 (b) FL 和 SL 在模型参数总数和客户端总数方面的性能比较。双曲线显示了一个模型在通信效率上优于另一个模型的区域
    [[35](#bib.bib35)]。
- en: 'Moreover, a comprehensive comparison of communication efficiency between SL
    and FL was presented by Singh et al. [[35](#bib.bib35)]. To study the relationship
    between communication efficiency and factors such as the total client number and
    model parameter number, a trade-off between the two models was demonstrated (Fig.
    [4](#S3.F4 "Figure 4 ‣ 3.1.1 Resource-Constrained Edge ‣ 3.1 Communication Efficiency
    Under Edge Heterogeneity ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")(b)), where the hyperbola
    shows the regions where one model outperforms the other regarding communication
    efficiency, in other words, less data transmission between the client and the
    PS. Besides, by comparing SL and FL in real-life scenarios of smart watches with
    users in a diverse range from hundreds to millions, the result suggests that SL
    is more efficient and scalable when it comes to a relatively large number of clients
    and a relatively large DL model.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Singh 等人[[35](#bib.bib35)] 提出了 SL 和 FL 之间通信效率的全面比较。为了研究通信效率与总客户端数和模型参数数量等因素之间的关系，展示了两种模型之间的权衡（图
    [4](#S3.F4 "图 4 ‣ 3.1.1 资源受限边缘 ‣ 3.1 边缘异质性下的通信效率 ‣ 可扩展去中心化深度学习的挑战和方法 ‣ 多接入边缘计算的去中心化深度学习：通信效率和可信度调查")(b)），其中双曲线显示了在通信效率方面一个模型优于另一个模型的区域，换句话说，就是客户端和
    PS 之间的数据传输较少。此外，通过在用户范围从数百到数百万不等的智能手表的实际场景中比较 SL 和 FL，结果表明，当客户端数量较多和 DL 模型较大时，SL
    更高效且更具可扩展性。
- en: Furthermore, to tackle various constraints of computational resources and wireless
    channel conditions, Nishio et al. [[34](#bib.bib34)] demonstrated a method called
    FedCS. In FedCS, the PS estimates the time required for conducting several steps
    of FL based on resource information of clients and schedules clients such that
    it aggregates as many client updates as possible to accelerate performance improvement
    during training. It shows a significantly shorter training time compared with
    the classical FL. In addition, Khan et al. [[29](#bib.bib29)] proposed an incentive-based
    FL framework based on the Stackelberg game theory to motivate the participation
    of devices in the learning process, while optimizing the client selection for
    minimizing the overall training cost of computation and communication.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了解决计算资源和无线信道条件的各种限制，Nishio 等人[[34](#bib.bib34)] 演示了一种称为 FedCS 的方法。在 FedCS
    中，PS 根据客户端的资源信息估算进行多个 FL 步骤所需的时间，并安排客户端，使其能够汇总尽可能多的客户端更新，从而加快训练过程中的性能提升。与经典 FL
    相比，这种方法显示出了显著更短的训练时间。此外，Khan 等人[[29](#bib.bib29)] 提出了一个基于 Stackelberg 博弈理论的激励型
    FL 框架，以激励设备参与学习过程，同时优化客户端选择，以最小化计算和通信的整体训练成本。
- en: 3.1.2 Data Heterogeneity
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 数据异质性
- en: 'Though the random client selection in FedAvg has been working well given data
    samples held by different clients are independent and identical decentralized
    (IID) [[42](#bib.bib42)]. Unfortunately, this scheme doesn’t work well when applied
    to real-world data samples, which are typically non-IID. For this reason, an efficient
    client selection policy during FL training is critical for the fast convergence
    of the global model, instead of the random client selection policy. Sener et al.
    [[38](#bib.bib38)] presented the K-Center clustering algorithm for choosing images
    to be adopted from a very large collection. They aim to find a subset such that
    the performance of the model on the labeled subset and that on the whole dataset
    will be as close as possible (Fig. [5](#S3.F5 "Figure 5 ‣ 3.1.2 Data Heterogeneity
    ‣ 3.1 Communication Efficiency Under Edge Heterogeneity ‣ 3 Challenges and Methodologies
    towards Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")).
    Furthermore, by leveraging the K-Center algorithm in FL under the non-IID settings,
    participating clients can be clustered into various groups based on their data
    distributions. Then, by carefully selecting clients from each group during training,
    it contributes to a faster global model convergence and performance improvement
    [[40](#bib.bib40)].'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管在 FedAvg 中，随机客户端选择方法在数据样本由不同客户端持有且这些样本是独立且同分布的（IID）情况下效果很好[[42](#bib.bib42)]，但当应用于真实世界的数据样本时，这种方案的效果并不理想，因为这些样本通常是非IID的。因此，在联邦学习（FL）训练过程中，采用高效的客户端选择策略对于全球模型的快速收敛至关重要，而不是随机客户端选择策略。Sener
    等人[[38](#bib.bib38)] 提出了 K-Center 聚类算法，用于从一个非常大的集合中选择要采用的图像。他们的目标是找到一个子集，使得该子集上的模型性能与整个数据集上的模型性能尽可能接近（图
    [5](#S3.F5 "Figure 5 ‣ 3.1.2 Data Heterogeneity ‣ 3.1 Communication Efficiency
    Under Edge Heterogeneity ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")）。此外，通过在非IID设置下利用 K-Center
    算法，参与的客户端可以根据其数据分布被聚类为不同的组。然后，通过在训练过程中从每个组中精心选择客户端，有助于加快全球模型的收敛速度和性能提升[[40](#bib.bib40)]。'
- en: '![Refer to caption](img/1666ee5829c32453e352e59cf9844513.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1666ee5829c32453e352e59cf9844513.png)'
- en: 'Figure 5: K-Center clustering algorithm aims to find a core-set (blue points)
    to represent the whole dataset (blue and red points) when conducting the training.
    [[38](#bib.bib38)]'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：K-Center 聚类算法旨在寻找一个核心集（蓝色点）以代表整个数据集（蓝色和红色点）进行训练。[[38](#bib.bib38)]
- en: 'Similarly, a reinforcement learning (RL)-based FL on non-IID data was presented
    by Wang et al [[40](#bib.bib40)], where an experience-driven control framework
    called FAVOR intelligently chooses the clients to participate in each round of
    FL (Fig. [6](#S3.F6 "Figure 6 ‣ 3.1.2 Data Heterogeneity ‣ 3.1 Communication Efficiency
    Under Edge Heterogeneity ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")). This approach aims
    to counterbalance the bias introduced by non-IID data, thus speeding up the global
    model convergence. The objective of this approach is to achieve the desired model
    performance within the fewest rounds. In particular, deep Q-learning (DQN) is
    adopted to learn how to select a subset of clients at each round thus maximizing
    a reward computed from the current reward and expected future rewards. Besides,
    there are three main components w.r.t. RL, i.e., the state, the action, and the
    reward [[43](#bib.bib43)]. Here the state of the environment is defined as compressed
    weights of the global model and local models. The available actions for the RL
    agent are a large space of size $\binom{N}{K}$, where $K$ is the total number
    of clients and $N$ is the number of selected clients at each round of FL. Finally,
    the reward of the DQN agent consists of the incentive from achieving high accuracy
    and the penalty for taking more rounds to achieve the desired performance.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Wang等人提出了一种基于强化学习（RL）的联邦学习方法用于非独立同分布数据[[40](#bib.bib40)]，其中一种名为FAVOR的经验驱动控制框架智能地选择参与每轮联邦学习的客户端（见图[6](#S3.F6
    "图 6 ‣ 3.1.2 数据异质性 ‣ 3.1 边缘异质性下的通信效率 ‣ 面向可扩展的去中心化深度学习的挑战和方法 ‣ 多接入边缘计算中的去中心化深度学习：通信效率和可信度调查")）。这一方法旨在平衡由非独立同分布数据引入的偏差，从而加快全局模型的收敛。该方法的目标是在最少的轮次内实现所需的模型性能。特别地，采用深度Q学习（DQN）来学习如何在每轮选择一部分客户端，从而最大化基于当前奖励和预期未来奖励计算的奖励。此外，RL的主要组件有三个，即状态、动作和奖励[[43](#bib.bib43)]。这里环境的状态定义为全局模型和本地模型的压缩权重。对于RL代理来说，可用的动作空间为大小为$\binom{N}{K}$的空间，其中$K$是客户端的总数，$N$是每轮联邦学习中选择的客户端数量。最后，DQN代理的奖励包括实现高准确度的激励和为了实现所需性能而需要更多轮次的惩罚。
- en: '![Refer to caption](img/31bfbcb2b867926a0879a63782d54cbd.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/31bfbcb2b867926a0879a63782d54cbd.png)'
- en: 'Figure 6: The RL-based client selection policy for faster convergence of FL.
    [[40](#bib.bib40)]'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：基于RL的客户端选择策略，用于加快FL的收敛速度。[[40](#bib.bib40)]
- en: Moreover, Segmented Federated Learning (Segmented-FL) was proposed to tackle
    data heterogeneity in network intrusion detection [[24](#bib.bib24)]. Participants
    with highly skewed non-IID network traffic data are separated into various groups
    for personalized federated learning based on their recent behavior. Then, each
    group is assigned an individual global model for the aggregation respectively.
    Besides, for each round’s training, a new group segmentation is formed and the
    global model of a group is updated based on the weighted averaging of its current
    global model, local model updates from the group, and the other existing global
    models. Consequently, it shows that Segmented-FL for network intrusion detection
    with massively distributed data sources outperforms the classical FL.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，分段联邦学习（Segmented-FL）被提出以解决网络入侵检测中的数据异质性[[24](#bib.bib24)]。具有高度偏斜的非独立同分布（non-IID）网络流量数据的参与者被分成不同的组，以便根据其近期行为进行个性化的联邦学习。然后，每个组被分配一个独立的全局模型进行聚合。此外，在每轮训练中，会形成一个新的组划分，并根据当前全局模型、来自组的本地模型更新和其他现有全局模型的加权平均来更新该组的全局模型。因此，结果表明，针对大规模分布数据源的网络入侵检测的Segmented-FL优于经典的联邦学习方法。
- en: In addition, Zhao et al. [[39](#bib.bib39)] presented a data-sharing strategy
    to improve training on non-IID data by creating a small data subset that is globally
    shared between all the edge devices. The experiments show that accuracy can be
    increased by  30% for the CIFAR10 dataset with only 5% globally shared data, compared
    with the accuracy of FL. Likewise, Jeong et al. [[37](#bib.bib37)] proposed federated
    augmentation (FAug) where each device trains a generative model, and thereby augments
    its local data towards yielding an IID dataset. The result shows around 26x less
    communication overhead for achieving the desired test accuracy compared to FL.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，赵等人[[39](#bib.bib39)] 提出了一个数据共享策略，通过创建一个在所有边缘设备之间全球共享的小数据子集来改善非IID数据上的训练。实验表明，与FL的准确性相比，仅使用5%的全球共享数据可以使CIFAR10数据集的准确性提高30%。同样，Jeong等人[[37](#bib.bib37)]
    提出了联邦增强（FAug），其中每个设备训练一个生成模型，从而增强其本地数据以产生一个IID数据集。结果表明，相较于FL，实现所需测试准确性的通信开销减少了约26倍。
- en: 3.2 Trustworthiness
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 可信度
- en: 'A decentralized framework such as federated learning (FL) encounters threats
    from malicious AI. As aforementioned in Section 2, threats from an edge client
    are more common and critical for FL, compared with the security of the server
    and the middle data transmission. FL extends the surface for the attacker to compromise
    one or several participants. In this regard, an adversary can intrude such decentralized
    systems through a compromised edge as a backdoor, by either manipulating local
    training data [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)]
    or replacing a local model update [[44](#bib.bib44), [47](#bib.bib47), [48](#bib.bib48)],
    triggering attacker-desired behavior. This kind of attack extends its influence
    to other clients in the systems through malicious model sharing with poisoned
    model weights. (see [3.2.1](#S3.SS2.SSS1 "3.2.1 Threat Models ‣ 3.2 Trustworthiness
    ‣ 3 Challenges and Methodologies towards Scalable Decentralized Deep Learning
    ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication
    Efficiency and Trustworthiness"), [3.2.2](#S3.SS2.SSS2 "3.2.2 Backdoor Attacks
    ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness"), [3.2.3](#S3.SS2.SSS3
    "3.2.3 Model Replacement ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies
    towards Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness"),
    [3.2.4](#S3.SS2.SSS4 "3.2.4 Information Stealing ‣ 3.2 Trustworthiness ‣ 3 Challenges
    and Methodologies towards Scalable Decentralized Deep Learning ‣ Decentralized
    Deep Learning for Multi-Access Edge Computing: A Survey on Communication Efficiency
    and Trustworthiness"))'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 去中心化框架如联邦学习（FL）面临来自恶意AI的威胁。如第2节所述，来自边缘客户端的威胁比服务器和中间数据传输的安全性更常见和关键。FL扩展了攻击者妥协一个或多个参与者的表面。在这方面，攻击者可以通过一个被妥协的边缘作为后门侵入这样的去中心化系统，或通过操纵本地训练数据[[44](#bib.bib44)、[45](#bib.bib45)、[46](#bib.bib46)、[47](#bib.bib47)]
    或替换本地模型更新[[44](#bib.bib44)、[47](#bib.bib47)、[48](#bib.bib48)]，触发攻击者期望的行为。这种攻击通过恶意模型共享和中毒的模型权重将其影响扩展到系统中的其他客户端。（参见
    [3.2.1](#S3.SS2.SSS1 "3.2.1 威胁模型 ‣ 3.2 可信度 ‣ 3 积极扩展去中心化深度学习的挑战和方法 ‣ 面向多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")、[3.2.2](#S3.SS2.SSS2
    "3.2.2 后门攻击 ‣ 3.2 可信度 ‣ 3 积极扩展去中心化深度学习的挑战和方法 ‣ 面向多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")、[3.2.3](#S3.SS2.SSS3
    "3.2.3 模型替换 ‣ 3.2 可信度 ‣ 3 积极扩展去中心化深度学习的挑战和方法 ‣ 面向多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")、[3.2.4](#S3.SS2.SSS4
    "3.2.4 信息窃取 ‣ 3.2 可信度 ‣ 3 积极扩展去中心化深度学习的挑战和方法 ‣ 面向多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查"))
- en: 'In the contrast, the controversy surrounding threats on FL has also been intensively
    discussed in recent years. These defense strategies can mainly be separated into
    two categories, i.e., robust aggregation and anomaly detection. For the robust
    aggregation, it is related to improving the resilience of an aggregation algorithm
    (e.g. FedAvg) by either carefully selecting the local models for aggregation [[49](#bib.bib49),
    [50](#bib.bib50), [51](#bib.bib51)] or adding noise to the aggregated model for
    counterbalancing the malicious update[[47](#bib.bib47), [52](#bib.bib52), [53](#bib.bib53)].
    On the other hand, various anomaly detection approaches are leveraged for identifying
    a malicious local model update, including comparing the similarity between local
    updates and finding the ones greatly diverging from the others [[54](#bib.bib54),
    [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)], applying
    a cloud validation set with a small number of data samples from each client [[15](#bib.bib15)],
    and so on. (see [3.2.5](#S3.SS2.SSS5 "3.2.5 Defense Models ‣ 3.2 Trustworthiness
    ‣ 3 Challenges and Methodologies towards Scalable Decentralized Deep Learning
    ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on Communication
    Efficiency and Trustworthiness"))'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 相对而言，近年来围绕FL威胁的争议也得到了广泛讨论。这些防御策略主要可以分为两类，即稳健聚合和异常检测。对于稳健聚合，它涉及通过精心选择用于聚合的本地模型
    [[49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51)] 或通过向聚合模型中添加噪声来平衡恶意更新 [[47](#bib.bib47),
    [52](#bib.bib52), [53](#bib.bib53)] 来提高聚合算法（例如 FedAvg）的弹性。另一方面，各种异常检测方法被用于识别恶意本地模型更新，包括比较本地更新之间的相似性，找到那些明显偏离其他更新的
    [[54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58)]，应用带有少量数据样本的云验证集
    [[15](#bib.bib15)] 等。（见 [3.2.5](#S3.SS2.SSS5 "3.2.5 防御模型 ‣ 3.2 可信度 ‣ 3 可扩展去中心化深度学习的挑战和方法
    ‣ 多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")）
- en: 3.2.1 Threat Models
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 威胁模型
- en: 'Our taxonomy for threat models (Fig. [7](#S3.F7 "Figure 7 ‣ 3.2.1 Threat Models
    ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")) comprehensively demonstrates
    various attacking methodologies in decentralized deep learning systems.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的威胁模型分类（图 [7](#S3.F7 "图 7 ‣ 3.2.1 威胁模型 ‣ 3.2 可信度 ‣ 3 可扩展去中心化深度学习的挑战和方法 ‣ 多接入边缘计算的去中心化深度学习：关于通信效率和可信度的调查")）全面展示了去中心化深度学习系统中的各种攻击方法。
- en: '![Refer to caption](img/f26e32d2d56a626265fa34230f780633.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f26e32d2d56a626265fa34230f780633.png)'
- en: 'Figure 7: Our taxonomy for threat models in decentralized deep learning systems.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：我们在去中心化深度学习系统中的威胁模型分类。
- en: Given the level of an adversary’s prior knowledge on compromised clients, attacks
    on FL can be divided into white-box attacks and black-box attacks. For the black-box
    setting, the attacker has only access to a client’s local data set and the objective
    is to replace the dataset with compromised backdoor data. The typical black-box
    attacks include backdoor attacks and label flipping attacks [[59](#bib.bib59)].
    On the other hand, for the white-box setting, the attacker is considered to have
    control over both the local data and the model of a client. In this case, the
    attacker can send back any malicious model it prefers to the PS. One typical white-box
    attack is the model replacement.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 根据对受损客户端的先验知识水平，FL（联邦学习）的攻击可以分为白盒攻击和黑盒攻击。在黑盒设置下，攻击者只能访问客户端的本地数据集，目标是用受损的后门数据替换数据集。典型的黑盒攻击包括后门攻击和标签翻转攻击
    [[59](#bib.bib59)]。另一方面，在白盒设置下，攻击者被认为对客户端的本地数据和模型都有控制。在这种情况下，攻击者可以将其选择的任何恶意模型发送回PS。一个典型的白盒攻击是模型替换。
- en: Moreover, depending on the attacker’s objective, an attack is either an untargeted
    attack that aims to reduce the accuracy of the FL model or a targeted attack that
    aims to compromise the FL model to output an adversary-desired label. Besides,
    according to the attacking timing, an attack can be mounted at either the training
    phase [[44](#bib.bib44)] or the inference phase [[60](#bib.bib60)], where the
    training phase refers to the model training in FL and the inference phase refers
    to the application after attaining a converged model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据攻击者的目标，攻击可以是无目标攻击，旨在降低FL模型的准确性，也可以是有目标攻击，旨在使FL模型输出对手期望的标签。此外，根据攻击时间，攻击可以在训练阶段[[44](#bib.bib44)]或推理阶段[[60](#bib.bib60)]进行，其中训练阶段指的是FL中的模型训练，推理阶段指的是在获得收敛模型后的应用。
- en: In addition, the continuity of an attack has also influence on the attacking
    performance, where a single-shot attack usually involves one malicious participant
    who aims to inject a long-lasting malicious trojan into the model by mounting
    the attack in a single round of training and a repeated attack usually involves
    one or more malicious participants with a high possibility to be mounted in multiple
    rounds of training.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，攻击的连续性也会影响攻击性能，其中单轮攻击通常涉及一个恶意参与者，旨在通过在单轮训练中实施攻击来将一个长期存在的恶意特洛伊植入模型，而重复攻击通常涉及一个或多个恶意参与者，且很可能在多个训练轮次中进行。
- en: We offer an overview of the most effective attacks on FL in the following several
    sections, covering backdoor attacks, model replacement, and information stealing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个章节中，我们概述了对FL最有效的攻击，包括后门攻击、模型替换和信息窃取。
- en: 3.2.2 Backdoor Attacks
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 后门攻击
- en: 'The goal of a backdoor attack is to corrupt other clients’ model performance
    on specific sub-tasks. Given an attacker has only access to a client’s local data
    (a black-box attack), a trojan backdoor [[44](#bib.bib44), [45](#bib.bib45)] corrupts
    a subset of local training data by adding a trojan pattern to the data and relabeling
    them as the target class (Fig. [8](#S3.F8 "Figure 8 ‣ 3.2.2 Backdoor Attacks ‣
    3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")). Besides, Lin et al.
    [[46](#bib.bib46)] adopted the composition of existing benign features and objects
    in a scene as the trigger. It leverages a mixer to generate mixed and poisonous
    samples, and then trains the local model on these samples as well as original
    benign data. Furthermore, semantic backdoors cause a model to produce an attacker-chosen
    output on unmodified images. For example, Wang et al. demonstrated an edge-case
    backdoor that targets prediction sub-tasks which are unlikely to be found in the
    training or test data sets, but are however natural [[47](#bib.bib47)]. To conduct
    the attack, they trained the local model based on a mix of the backdoors and benign
    training data with a carefully chosen ratio. The result shows that this attack
    can bypass simple norm-based defense algorithms such as the norm bounding [[55](#bib.bib55)].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '后门攻击的目标是破坏其他客户端在特定子任务上的模型性能。鉴于攻击者只能访问客户端的本地数据（黑箱攻击），特洛伊后门[[44](#bib.bib44),
    [45](#bib.bib45)]通过将特洛伊模式添加到数据中并将其重新标记为目标类别，从而破坏一部分本地训练数据（见图[8](#S3.F8 "Figure
    8 ‣ 3.2.2 Backdoor Attacks ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies
    towards Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")）。此外，Lin等人[[46](#bib.bib46)]采用了现有良性特征和场景中的物体组合作为触发器。它利用混合器生成混合的有毒样本，然后在这些样本以及原始良性数据上训练本地模型。此外，语义后门使模型在未修改的图像上产生攻击者选择的输出。例如，Wang等人展示了一种边缘案例后门，目标是那些不太可能出现在训练或测试数据集中但自然存在的预测子任务[[47](#bib.bib47)]。为了进行攻击，他们根据特洛伊后门和良性训练数据的混合比例训练了本地模型。结果表明，这种攻击可以绕过简单的基于范数的防御算法，如范数界限[[55](#bib.bib55)]。'
- en: '![Refer to caption](img/8715628b61e1e93d2450be61140d87c3.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8715628b61e1e93d2450be61140d87c3.png)'
- en: 'Figure 8: Samples of various types of trojan backdoors. [[45](#bib.bib45)]'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：各种类型的特洛伊后门样本。[[45](#bib.bib45)]
- en: 3.2.3 Model Replacement
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 模型替换
- en: 'The model replacement is one type of white-box attack [[44](#bib.bib44)], through
    replacing the global model with a malicious model. As aforementioned, in FedAvg,
    the PS updates the global model by performing a weighted average of all local
    trained models. A model replacement attack aims to submit a malicious model update
    $w_{t}^{m}=\frac{n_{k}}{n_{adv}}(w_{t}^{adv}-w_{t})+w_{t}$ instead of $w_{t}^{adv}$,
    where $w_{t}^{adv}$ denotes a poisoned local model based on the aforementioned
    methods such as backdoor attacks and $n_{adv}$ denotes the number of samples owned
    by the adversary. Given the attack is usually mounted after the global model converges
    when additional local model training will not improve the global model and its
    loss settles within an error range around the optimum, each honest client$i$ then
    will obtain an updated local model $w_{t}^{i}$ approximately equal to the current
    global model $w_{t}$. $w_{t}^{i}-w_{t}\approx 0$. Equation ([2](#S3.E2 "In 3.2.3
    Model Replacement ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards
    Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for Multi-Access
    Edge Computing: A Survey on Communication Efficiency and Trustworthiness")) is
    the mathematical proof of the model replacement attack.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 模型替换是一种白盒攻击[[44](#bib.bib44)]，通过用恶意模型替换全局模型。正如前面提到的，在 FedAvg 中，PS 通过对所有本地训练模型进行加权平均来更新全局模型。模型替换攻击的目的是提交一个恶意模型更新
    $w_{t}^{m}=\frac{n_{k}}{n_{adv}}(w_{t}^{adv}-w_{t})+w_{t}$，而不是 $w_{t}^{adv}$，其中
    $w_{t}^{adv}$ 表示基于前述方法（如后门攻击）中毒的本地模型，$n_{adv}$ 表示对手拥有的样本数量。由于攻击通常在全局模型收敛后进行，当额外的本地模型训练不会改善全局模型且其损失在最优值附近的误差范围内稳定时，每个诚实的客户端$i$将获得一个更新的本地模型
    $w_{t}^{i}$，大致等于当前的全局模型 $w_{t}$。$w_{t}^{i}-w_{t}\approx 0$。方程 ([2](#S3.E2 "在 3.2.3
    模型替换 ‣ 3.2 可信度 ‣ 3 积极推进去中心化深度学习的挑战与方法 ‣ 面向多接入边缘计算的去中心化深度学习：通信效率和可信度调查")) 是模型替换攻击的数学证明。
- en: '|  | <math   alttext="\begin{gathered}w_{t+1}=w_{t}+\sum_{i\in k}\frac{n_{i}}{n_{k}}(w_{t}^{i}-w_{t})\\
    =w_{t}+\frac{n_{1}}{n_{k}}(w_{t}^{1}-w_{t})+..+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math alttext="\begin{gathered}w_{t+1}=w_{t}+\sum_{i\in k}\frac{n_{i}}{n_{k}}(w_{t}^{i}-w_{t})\\
    =w_{t}+\frac{n_{1}}{n_{k}}(w_{t}^{1}-w_{t})+..+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\'
- en: \approx w_{t}+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: \approx w_{t}+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\
- en: =w_{t}+\frac{n_{adv}}{n_{k}}(\frac{n_{k}}{n_{adv}}(w_{t}^{adv}-w_{t})+w_{t}-w_{t})\\
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: =w_{t}+\frac{n_{adv}}{n_{k}}(\frac{n_{k}}{n_{adv}}(w_{t}^{adv}-w_{t})+w_{t}-w_{t})\\
- en: =w_{t}^{adv}\end{gathered}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><msub ><mi >w</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo >=</mo><mrow ><msub ><mi >w</mi><mi >t</mi></msub><mo
    rspace="0.055em" >+</mo><mrow ><munder ><mo movablelimits="false" >∑</mo><mrow
    ><mi >i</mi><mo >∈</mo><mi >k</mi></mrow></munder><mrow ><mfrac ><msub ><mi >n</mi><mi
    >i</mi></msub><msub ><mi >n</mi><mi >k</mi></msub></mfrac><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi >w</mi><mi >t</mi><mi
    >i</mi></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mo >=</mo><msub
    ><mi >w</mi><mi >t</mi></msub><mo >+</mo><mfrac ><msub ><mi >n</mi><mn >1</mn></msub><msub
    ><mi >n</mi><mi >k</mi></msub></mfrac><mrow ><mo stretchy="false" >(</mo><msubsup
    ><mi >w</mi><mi >t</mi><mn >1</mn></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub><mo
    stretchy="false" >)</mo></mrow><mo rspace="0em" >+</mo><mo lspace="0em" rspace="0.0835em"
    >.</mo><mo lspace="0.0835em" rspace="0em" >.</mo><mo lspace="0em" >+</mo><mfrac
    ><msub ><mi >n</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msub><msub ><mi
    >n</mi><mi >k</mi></msub></mfrac><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >w</mi><mi >t</mi><mi >m</mi></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mo >≈</mo><mrow
    ><msub ><mi >w</mi><mi >t</mi></msub><mo >+</mo><mrow ><mfrac ><msub ><mi >n</mi><mrow
    ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >v</mi></mrow></msub><msub ><mi >n</mi><mi >k</mi></msub></mfrac><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup
    ><mi >w</mi><mi >t</mi><mi >m</mi></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow
    ><mo >=</mo><mrow ><msub ><mi >w</mi><mi >t</mi></msub><mo >+</mo><mrow ><mfrac
    ><msub ><mi >n</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msub><msub ><mi
    >n</mi><mi >k</mi></msub></mfrac><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow ><mrow ><mfrac ><msub ><mi >n</mi><mi >k</mi></msub><msub
    ><mi >n</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msub></mfrac><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi >w</mi><mi
    >t</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >v</mi></mrow></msubsup><mo >−</mo><msub ><mi >w</mi><mi
    >t</mi></msub></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo >+</mo><msub
    ><mi >w</mi><mi >t</mi></msub></mrow><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow
    ><mo >=</mo><msubsup ><mi >w</mi><mi >t</mi><mrow ><mi >a</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msubsup></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{gathered}w_{t+1}=w_{t}+\sum_{i\in k}\frac{n_{i}}{n_{k}}(w_{t}^{i}-w_{t})\\
    =w_{t}+\frac{n_{1}}{n_{k}}(w_{t}^{1}-w_{t})+..+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\
    \approx w_{t}+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\ =w_{t}+\frac{n_{adv}}{n_{k}}(\frac{n_{k}}{n_{adv}}(w_{t}^{adv}-w_{t})+w_{t}-w_{t})\\
    =w_{t}^{adv}\end{gathered}</annotation></semantics></math> |  | (2) |
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: =w_{t}^{adv}\end{gathered}" display="block"><semantics ><mtable displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd ><mrow ><msub ><mi >w</mi><mrow ><mi >t</mi><mo >+</mo><mn
    >1</mn></mrow></msub><mo >=</mo><mrow ><msub ><mi >w</mi><mi >t</mi></msub><mo
    rspace="0.055em" >+</mo><mrow ><munder ><mo movablelimits="false" >∑</mo><mrow
    ><mi >i</mi><mo >∈</mo><mi >k</mi></mrow></munder><mrow ><mfrac ><msub ><mi >n</mi><mi
    >i</mi></msub><msub ><mi >n</mi><mi >k</mi></msub></mfrac><mo lspace="0em" rspace="0em"
    >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi >w</mi><mi >t</mi><mi
    >i</mi></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mo >=</mo><msub
    ><mi >w</mi><mi >t</mi></msub><mo >+</mo><mfrac ><msub ><mi >n</mi><mn >1</mn></msub><msub
    ><mi >n</mi><mi >k</mi></msub></mfrac><mrow ><mo stretchy="false" >(</mo><msubsup
    ><mi >w</mi><mi >t</mi><mn >1</mn></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub><mo
    stretchy="false" >)</mo></mrow><mo rspace="0em" >+</mo><mo lspace="0em" rspace="0.0835em"
    >.</mo><mo lspace="0.0835em" rspace="0em" >.</mo><mo lspace="0em" >+</mo><mfrac
    ><msub ><mi >n</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msub><msub ><mi
    >n</mi><mi >k</mi></msub></mfrac><mrow ><mo stretchy="false" >(</mo><msubsup ><mi
    >w</mi><mi >t</mi><mi >m</mi></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub><mo
    stretchy="false" >)</mo></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow ><mo >≈</mo><mrow
    ><msub ><mi >w</mi><mi >t</mi></msub><mo >+</mo><mrow ><mfrac ><msub ><mi >n</mi><mrow
    ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >v</mi></mrow></msub><msub ><mi >n</mi><mi >k</mi></msub></mfrac><mo
    lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup
    ><mi >w</mi><mi >t</mi><mi >m</mi></msubsup><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow
    ><mo >=</mo><mrow ><msub ><mi >w</mi><mi >t</mi></msub><mo >+</mo><mrow ><mfrac
    ><msub ><mi >n</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msub><msub ><mi
    >n</mi><mi >k</mi></msub></mfrac><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo
    stretchy="false" >(</mo><mrow ><mrow ><mrow ><mfrac ><msub ><mi >n</mi><mi >k</mi></msub><msub
    ><mi >n</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msub></mfrac><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><msubsup ><mi >w</mi><mi
    >t</mi><mrow ><mi >a</mi><mo lspace="0em" rspace="0em" >​</mo><mi >d</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >v</mi></mrow></msubsup><mo >−</mo><msub ><mi >w</mi><mi
    >t</mi></msub></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo >+</mo><msub
    ><mi >w</mi><mi >t</mi></msub></mrow><mo >−</mo><msub ><mi >w</mi><mi >t</mi></msub></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd ><mrow
    ><mo >=</mo><msubsup ><mi >w</mi><mi >t</mi><mrow ><mi >a</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mi >v</mi></mrow></msubsup></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{gathered}w_{t+1}=w_{t}+\sum_{i\in k}\frac{n_{i}}{n_{k}}(w_{t}^{i}-w_{t})\\
    =w_{t}+\frac{n_{1}}{n_{k}}(w_{t}^{1}-w_{t})+..+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\
    \approx w_{t}+\frac{n_{adv}}{n_{k}}(w_{t}^{m}-w_{t})\\ =w_{t}+\frac{n_{adv}}{n_{k}}(\frac{n_{k}}{n_{adv}}(w_{t}^{adv}-w_{t})+w_{t}-w_{t})\\
    =w_{t}^{adv}\end{gathered}</annotation></semantics></math> |  | (2)
- en: 'The combination of semantic backdoors and model replacement formulates a long-lasting
    and invisible attack on the systems. For instance, Bagdasaryan et al. [[44](#bib.bib44)]
    demonstrated an attacking method of the constrain and scale on the CIFAR-10 dataset,
    aiming to poison the global model using a set of car images with certain features
    (racing strip, green color, and stripped background wall) as triggers. In detail,
    the constrain-and-scale method is defined as in ([3](#S3.E3 "In 3.2.3 Model Replacement
    ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies towards Scalable Decentralized
    Deep Learning ‣ Decentralized Deep Learning for Multi-Access Edge Computing: A
    Survey on Communication Efficiency and Trustworthiness")). They mounted a single-shot
    model replacement attack where one malicious participant was selected for a single
    round in FL. Then, by updating the model based on the model prediction accuracy
    on both main classes and backdoor classes, and an anomaly detection algorithm’s
    accuracy, it aims to achieve the desired malicious performance while bypassing
    the anomaly detection. Finally, it shows that such attacks can bypass anomaly
    detection and retains a high accuracy for many rounds after the single-shot attack.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '语义后门和模型替换的结合形成了对系统的持久且隐形的攻击。例如，Bagdasaryan 等人[[44](#bib.bib44)] 演示了一种在 CIFAR-10
    数据集上对约束和缩放的攻击方法，旨在利用具有特定特征（赛车条纹、绿色和有条纹的背景墙）的汽车图像集作为触发器来毒害全球模型。具体而言，约束和缩放方法定义如下（[3](#S3.E3
    "In 3.2.3 Model Replacement ‣ 3.2 Trustworthiness ‣ 3 Challenges and Methodologies
    towards Scalable Decentralized Deep Learning ‣ Decentralized Deep Learning for
    Multi-Access Edge Computing: A Survey on Communication Efficiency and Trustworthiness")）。他们发起了一次单次模型替换攻击，其中选择了一个恶意参与者进行
    FL 中的单轮攻击。然后，通过基于主类和后门类的模型预测准确性以及异常检测算法的准确性来更新模型，旨在实现所需的恶意性能，同时绕过异常检测。最后，这表明此类攻击可以绕过异常检测，并且在单次攻击后保持高准确性。'
- en: '|  | $L_{model}=\alpha L_{class}+(1-\alpha)L_{ano}$ |  | (3) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{model}=\alpha L_{class}+(1-\alpha)L_{ano}$ |  | (3) |'
- en: Where $L_{class}$ captures the accuracy on both the main and backdoor tasks,
    $L_{ano}$ represents the performance of an anomaly detection algorithm taken at
    the PS, and $\alpha$ controls the importance of evading anomaly detection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{class}$ 捕获了主任务和后门任务的准确性，$L_{ano}$ 表示在 PS 上采集的异常检测算法的性能，而 $\alpha$ 控制绕过异常检测的重要性。
- en: 3.2.4 Information Stealing
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 信息窃取
- en: By leveraging generative adversarial networks (GANs)[[61](#bib.bib61)], an adversary
    could reconstruct the training data of another client in FL by just downloading
    the global model [[62](#bib.bib62)]. In GANs, there is a tradeoff between the
    discriminator and the generator, where the discriminator trains based on whether
    it succeeds in distinguishing between the adversarial samples drawn from the generator
    and real data from the targeted data class, and the generator trains based on
    whether it succeeds in fooling the discriminator. At each round of FL, the adversary
    replaces the discriminator of the implemented GANs with the latest global model
    from the parameter server. Then the generator of the GANs produces adversarial
    samples from Gaussian noise and updates itself based on the inference result from
    the discriminator and the label of the targeted data class. In this case, with
    the adversarial training, the generator of the adversary could produce crispier
    samples to train a local DL model using the fake samples of the targeted data
    class. Besides, malicious model parameters of the adversary are then transmitted
    to the victim through model aggregation. The compromised model parameters would
    lure the victim to expose more detail on its training data, due to the victim
    would need more effort in model training thus identifying between real data and
    fake data. Consequently, the model update of the victim would allow the adversary
    to generate crispier and crispier adversarial samples that reveal the raw training
    data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用生成对抗网络（GANs）[[61](#bib.bib61)]，对手可以通过仅下载全球模型 [[62](#bib.bib62)] 来重建联邦学习中另一客户端的训练数据。在
    GANs 中，判别器和生成器之间存在权衡，判别器根据是否成功区分生成器生成的对抗样本与来自目标数据类别的真实数据进行训练，而生成器则根据是否成功欺骗判别器进行训练。在每轮联邦学习中，对手用来自参数服务器的最新全球模型替换实现的
    GANs 的判别器。然后，GANs 的生成器从高斯噪声中生成对抗样本，并根据判别器的推断结果和目标数据类别的标签进行自我更新。在这种情况下，通过对抗训练，对手的生成器可以生成更清晰的样本，以使用目标数据类别的假样本训练本地深度学习模型。此外，对手的恶意模型参数随后通过模型聚合传输给受害者。由于受害者在模型训练中需要更多的努力，因此会更加容易识别真实数据与假数据，导致受害者暴露更多训练数据的细节。因此，受害者的模型更新将使对手能够生成越来越清晰的对抗样本，从而揭示原始训练数据。
- en: In addition, Nasr et al. [[63](#bib.bib63)] demonstrated a comprehensive analysis
    on white-box membership inference attacks, where only correlated information of
    local training data leaked from the model sharing. Different from the aforementioned
    reconstruction attack where the objective is to reconstruct the raw training data
    of a victim, this kind of attack aims to infer whether a specific data sample
    was used in the victim’s local model training.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Nasr 等人 [[63](#bib.bib63)] 展示了针对白盒成员推断攻击的全面分析，其中只有与本地训练数据相关的信息从模型共享中泄漏。不同于前述的重建攻击，后者的目标是重建受害者原始训练数据，这种攻击旨在推断特定数据样本是否在受害者的本地模型训练中使用过。
- en: 3.2.5 Defense Models
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5 防御模型
- en: Defense strategies against the threat models in FL to date can mainly be separated
    into two categories, i.e., robust aggregation and anomaly detection. For the robust
    aggregation, instead of employing the random client selection policy in FedAvg,
    other selection approaches have been proposed against underlying malicious local
    updates, such as Krum[[49](#bib.bib49)], Trimmed mean[[50](#bib.bib50)], and so
    on. Krum selects a single local update from $m$ local models that is similar to
    other models as the global model based on pairwise Euclidean distances between
    local updates. In detail, for each model, it computes the sum of distances between
    a local model and its closest $m-c-2$ local models, where $m$ is the total number
    of clients, and $c$ is the assumed maximum number of compromised clients. On the
    other hand, trimmed mean sorts all local updates at each round, i.e., $w_{1j}$,
    $w_{2j}$, ···, $w_{mj}$, where $w_{ij}$ represents the $j$th round’s model of
    the $i$th client. Then by removing the largest and smallest $\beta$ of them, the
    mean of the remaining $m-2\beta$ models is employed as the result of the $j$th
    round’s global model. Moreover, another important strategy of robust aggregation
    is the Differential Privacy (DP), which limits the influence of a malicious update
    on model aggregation by adding a small fraction of Gaussian noise to the parameters
    of a local update. In particular, the cloud-side DP where the noise is added directly
    to the aggregated global model bounds the success of attacks such as the information
    stealing [[47](#bib.bib47), [52](#bib.bib52)]. The client-side DP where the noise
    is added to each client’s local update aims to achieve the optimized tradeoff
    between defense efficiency and model performance on main tasks of FL [[53](#bib.bib53)].
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对FL中的威胁模型的防御策略主要可以分为两类，即稳健聚合和异常检测。在稳健聚合方面，除了在FedAvg中采用随机客户端选择策略之外，还提出了其他应对潜在恶意本地更新的选择方法，如Krum[[49](#bib.bib49)]、修剪均值[[50](#bib.bib50)]等。Krum基于本地更新之间的配对欧几里得距离，从$m$个本地模型中选择一个与其他模型相似的本地更新作为全局模型。具体来说，对于每个模型，它计算一个本地模型与其最接近的$m-c-2$个本地模型之间距离的总和，其中$m$是客户端的总数，$c$是假定的最大被攻陷客户端数。另一方面，修剪均值在每一轮中对所有本地更新进行排序，即$w_{1j}$、$w_{2j}$、···、$w_{mj}$，其中$w_{ij}$表示第$i$个客户端的第$j$轮模型。然后通过去除其中最大的和最小的$\beta$，使用剩下的$m-2\beta$个模型的均值作为第$j$轮全局模型的结果。此外，稳健聚合的另一个重要策略是差分隐私（DP），它通过向本地更新的参数中添加少量高斯噪声来限制恶意更新对模型聚合的影响。特别地，云端DP通过直接将噪声添加到聚合的全局模型中，限制了如信息盗取[[47](#bib.bib47)、[52](#bib.bib52)]等攻击的成功。而客户端DP则将噪声添加到每个客户端的本地更新中，旨在实现防御效率和FL主要任务模型性能之间的优化权衡[[53](#bib.bib53)]。
- en: Furthermore, norm bounding and anomaly detection are technologies adopted to
    drop malicious updates. In the norm bounding, the norm of a local update is a
    projected positive vector, such as the length of the model parameter vector. Since
    the malicious updates based on backdoor attacks and model replacement attacks
    of an adversary are likely to produce model parameters with large norms compared
    with other honest clients, an efficient way is to drop the updates whose norm
    is above a certain threshold [[55](#bib.bib55)]. Likewise, anomaly detection in
    FL is usually based on comparing the similarity among local updates. For example,
    Cao et al. [[54](#bib.bib54)] presented a Euclidean distance-based malicious local
    model detection. They demonstrated that if a local model had a distance under
    a certain constrain with more than half of the local models, it would be probably
    benign. Tolpegin et al. [[56](#bib.bib56)] proposed a PCA-based defense against
    label flipping attacks. They observed and plotted standardized parameters of local
    updates to separate the benign and malicious ones. Additionally, Zhao et al. [[57](#bib.bib57)]
    presented a poisoning defense method using generative adversarial networks. By
    reconstructing data from a local update and feeding the generated data to each
    of the clients’ models, they aimed to specify the label with the most occurrences
    as the true label for each input. Finally, the local updates were divided into
    the benign cluster and the malicious cluster by evaluating the prediction accuracy
    on the generated data using the obtained labels.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，范数约束和异常检测是用于删除恶意更新的技术。在范数约束中，本地更新的范数是一个投影的正向量，例如模型参数向量的长度。由于基于后门攻击和对手模型替换攻击的恶意更新可能会产生与其他诚实客户端相比具有大范数的模型参数，因此一种有效的方法是删除范数超过某个阈值的更新[[55](#bib.bib55)]。同样，FL中的异常检测通常基于比较本地更新之间的相似性。例如，Cao等人[[54](#bib.bib54)]提出了一种基于欧几里得距离的恶意本地模型检测方法。他们展示了如果本地模型与超过一半的本地模型的距离在某个限制下，则可能是良性的。Tolpegin等人[[56](#bib.bib56)]提出了一种基于PCA的对抗标签翻转攻击的防御方法。他们观察并绘制了本地更新的标准化参数，以区分良性和恶意的更新。此外，Zhao等人[[57](#bib.bib57)]提出了一种使用生成对抗网络的毒化防御方法。通过从本地更新中重建数据，并将生成的数据输入到每个客户端模型中，他们旨在将出现次数最多的标签指定为每个输入的真实标签。最后，通过使用获得的标签评估生成数据上的预测准确性，将本地更新划分为良性集群和恶意集群。
- en: 4 Concluding Remarks
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In multi-access edge computing, decentralized deep learning (DDL) is considered
    to facilitate privacy-preserving knowledge acquisition from enormous various types
    of edge data. This survey provides an overview of DDL from two novel perspectives
    of communication efficiency and trustworthiness, offering state-of-the-art technologies
    to tackle challenges in leveraging DDL for social practices.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在多接入边缘计算中，去中心化深度学习（DDL）被认为有助于从大量不同类型的边缘数据中实现隐私保护的知识获取。本综述从通信效率和可信度两个新颖视角概述了DDL，提供了最先进的技术来应对利用DDL进行社会实践中的挑战。
- en: Federated learning as a classical solution to data privacy in centralized learning,
    aims to leverage local model training for collective machine learning among multiple
    clients. Whereas, real-life challenges such as edge heterogeneity and adversarial
    attacks have greatly limited the capability and scalability of this technology.
    Given the capability limitation of an edge device, the convergence of a complicated
    model is costly and time-consuming. A more compatible architecture appears to
    be split learning, which brings the gap between a centralized computing resource
    and decentralized data sources. Besides, data heterogeneity is a common problem
    when applying real-world data, to this end, a more adaptive client selection policy
    could benefit the fast convergence of FL. Moreover, the topic of trustworthiness
    in DDL has also been attracting an explosive growth of interest in recent years.
    We summarized the latest threat models in DDL based on various criteria and provided
    our novel taxonomy. Finally, we discussed some of the most promising defense strategies
    against such threats on FL. In addition, there are still other important topics
    not covered in this survey, including mitigating algorithmic bias in DDL [[64](#bib.bib64),
    [65](#bib.bib65)] and incentive mechanism for mobile device participation [[29](#bib.bib29),
    [66](#bib.bib66)].
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习作为一种经典的数据隐私解决方案，旨在利用本地模型训练来实现多个客户端之间的集体机器学习。然而，现实生活中的挑战，如边缘异质性和对抗攻击，极大地限制了这一技术的能力和可扩展性。鉴于边缘设备的能力限制，复杂模型的收敛既昂贵又耗时。一个更兼容的架构似乎是分割学习，它弥合了集中计算资源与分散数据源之间的差距。此外，在应用真实世界数据时，数据异质性是一个常见的问题，为此，更具适应性的客户端选择策略可能有利于联邦学习的快速收敛。此外，近年来对数据驱动学习（DDL）中可信度的关注也呈现爆炸式增长。我们总结了基于各种标准的DDL最新威胁模型，并提供了我们的新分类。最后，我们讨论了针对联邦学习（FL）中这些威胁的一些最有前景的防御策略。此外，本调查仍未涵盖其他重要主题，包括减少DDL中的算法偏见[[64](#bib.bib64)、[65](#bib.bib65)]和移动设备参与的激励机制[[29](#bib.bib29)、[66](#bib.bib66)]。
- en: The current rapid advancement and broad application of deep learning in today’s
    society necessitates building trust in such emerging technology. The privacy-preserving
    DDL offers practical solutions to future large-scale multi-access edge computing.
    The breadth of papers surveyed suggests that DDL is being intensively studied,
    especially in terms of privacy protection, edge heterogeneity, and adversarial
    attacks and defenses. Furthermore, the future trends of DDL put weight on topics
    such as efficient resource allocation, asynchronous communication, and fully decentralized
    frameworks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当前深度学习在社会中的快速发展和广泛应用，要求我们建立对这种新兴技术的信任。隐私保护的DDL为未来大规模多接入边缘计算提供了实际解决方案。调查的文献广度表明，DDL正在受到密集研究，特别是在隐私保护、边缘异质性以及对抗攻击和防御方面。此外，DDL的未来趋势侧重于有效资源分配、异步通信和完全去中心化框架等主题。
- en: Acknowledgment
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The authors would like to thank the anonymous reviewers for helpful comments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作者感谢匿名评审人提供的有益评论。
- en: References
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] “General data protection regulation,” [https://gdpr-info.eu](https://gdpr-info.eu),
    accessed: 2021-09-13.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] “通用数据保护条例，” [https://gdpr-info.eu](https://gdpr-info.eu)，访问日期：2021-09-13。'
- en: '[2] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. a. Ranzato,
    A. Senior, P. Tucker, K. Yang, Q. Le, and A. Ng, “Large scale distributed deep
    networks,” in *Advances in Neural Information Processing Systems*, vol. 25.   Curran
    Associates, Inc., 2012.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. a. Ranzato,
    A. Senior, P. Tucker, K. Yang, Q. Le, 和 A. Ng, “大规模分布式深度网络”，见 *Neural Information
    Processing Systems 进展*，第 25 卷。Curran Associates, Inc., 2012.'
- en: '[3] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon,
    “Federated learning: Strategies for improving communication efficiency,” in *NIPS
    Workshop on Private Multi-Party Machine Learning*, 2016.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, 和 D. Bacon,
    “联邦学习：提高通信效率的策略，”见 *NIPS 隐私多方机器学习研讨会*，2016。'
- en: '[4] “Ericsson mobility report,” [https://www.ericsson.com/en/mobility-report](https://www.ericsson.com/en/mobility-report),
    accessed: 2021-09-13.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] “爱立信移动报告，” [https://www.ericsson.com/en/mobility-report](https://www.ericsson.com/en/mobility-report)，访问日期：2021-09-13。'
- en: '[5] X. Xu, H. Li, W. Xu, Z. Liu, L. Yao, and F. Dai, “Artificial intelligence
    for edge service optimization in internet of vehicles: A survey,” *Tsinghua Science
    and Technology*, vol. 27, no. 2, pp. 270–287, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Xu, H. Li, W. Xu, Z. Liu, L. Yao, 和 F. Dai, “用于车辆互联网边缘服务优化的人工智能：综述,”
    *清华科学技术*, 卷27, 期2, 页270–287, 2022年。'
- en: '[6] J. Wang, C. Jiang, H. Zhang, Y. Ren, K. Chen, and L. Hanzo, “Thirty years
    of machine learning: The road to pareto-optimal wireless networks,” *IEEE Commun.
    Surv. Tutorials*, vol. 22, no. 3, pp. 1472–1514, 2020. [Online]. Available: [https://doi.org/10.1109/COMST.2020.2965856](https://doi.org/10.1109/COMST.2020.2965856)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Wang, C. Jiang, H. Zhang, Y. Ren, K. Chen, 和 L. Hanzo, “三十年的机器学习：通向帕累托最优无线网络的道路,”
    *IEEE通信综述与教程*, 卷22, 期3, 页1472–1514, 2020年。[在线]. 可用： [https://doi.org/10.1109/COMST.2020.2965856](https://doi.org/10.1109/COMST.2020.2965856)'
- en: '[7] F. Rosenblatt, “The perceptron: A probabilistic model for information storage
    and organization in the brain,” *Psyhological Review*, vol. 65, no. 6, pp. 386–408,
    1958\. [Online]. Available: [https://ci.nii.ac.jp/naid/20001617891/en/](https://ci.nii.ac.jp/naid/20001617891/en/)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] F. Rosenblatt, “感知器：大脑中信息存储和组织的概率模型,” *心理学评论*, 卷65, 期6, 页386–408, 1958年。[在线].
    可用： [https://ci.nii.ac.jp/naid/20001617891/en/](https://ci.nii.ac.jp/naid/20001617891/en/)'
- en: '[8] R. Schalkoff, “Pattern recognition : statistical, structural and neural
    approaches / robert j. schalkoff,” 1992.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] R. Schalkoff, “模式识别：统计、结构和神经方法 / 罗伯特·J·沙尔科夫,” 1992年。'
- en: '[9] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015\. [Online]. Available: [https://doi.org/10.1038/nature14539](https://doi.org/10.1038/nature14539)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Y. LeCun, Y. Bengio, 和 G. Hinton, “深度学习,” *自然*, 卷521, 期7553, 页436–444,
    2015年。[在线]. 可用： [https://doi.org/10.1038/nature14539](https://doi.org/10.1038/nature14539)'
- en: '[10] A. J. Smola and S. M. Narayanamurthy, “An architecture for parallel topic
    models,” *Proc. VLDB Endow.*, vol. 3, no. 1, pp. 703–710, 2010. [Online]. Available:
    [http://www.vldb.org/pvldb/vldb2010/pvldb_vol3/R63.pdf](http://www.vldb.org/pvldb/vldb2010/pvldb_vol3/R63.pdf)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] A. J. Smola 和 S. M. Narayanamurthy, “并行主题模型的架构,” *VLDB会议录*, 卷3, 期1, 页703–710,
    2010年。[在线]. 可用： [http://www.vldb.org/pvldb/vldb2010/pvldb_vol3/R63.pdf](http://www.vldb.org/pvldb/vldb2010/pvldb_vol3/R63.pdf)'
- en: '[11] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
    J. Long, E. J. Shekita, and B. Su, “Scaling distributed machine learning with
    the parameter server,” in *11th USENIX Symposium on Operating Systems Design and
    Implementation, OSDI ’14, Broomfield, CO, USA, October 6-8, 2014*, J. Flinn and
    H. Levy, Eds.   USENIX Association, 2014, pp. 583–598.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
    J. Long, E. J. Shekita, 和 B. Su, “使用参数服务器扩展分布式机器学习,” 见 *第11届USENIX操作系统设计与实现研讨会,
    OSDI ’14, 布鲁姆菲尔德, CO, USA, 2014年10月6-8日*, J. Flinn 和 H. Levy 编辑. USENIX协会, 2014年,
    页583–598。'
- en: '[12] S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan, S. Mukherjee,
    V. Garg, R. Sarveswara, K. Händler, P. Pickkers, N. A. Aziz, S. Ktena, C. Siever,
    M. Kraut, M. Desai, B. Monnet, M. Saridaki, C. M. Siegel, A. Drews, M. Nuesch-Germano,
    H. Theis, M. G. Netea, F. Theis, A. C. Aschenbrenner, T. Ulas, M. M. Breteler,
    E. J. Giamarellos-Bourboulis, M. Kox, M. Becker, S. Cheran, M. S. Woodacre, E. L.
    Goh, J. L. Schultze, and G. C.-. O. I. (DeCOI), “Swarm learning for decentralized
    and confidential clinical machine learning,” 2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Warnat-Herresthal, H. Schultze, K. L. Shastry, S. Manamohan, S. Mukherjee,
    V. Garg, R. Sarveswara, K. Händler, P. Pickkers, N. A. Aziz, S. Ktena, C. Siever,
    M. Kraut, M. Desai, B. Monnet, M. Saridaki, C. M. Siegel, A. Drews, M. Nuesch-Germano,
    H. Theis, M. G. Netea, F. Theis, A. C. Aschenbrenner, T. Ulas, M. M. Breteler,
    E. J. Giamarellos-Bourboulis, M. Kox, M. Becker, S. Cheran, M. S. Woodacre, E.
    L. Goh, J. L. Schultze, 和 G. C.-. O. I. (DeCOI), “用于去中心化和保密临床机器学习的群体学习,” 2021年。'
- en: '[13] Y. Li, C. Chen, N. Liu, H. Huang, Z. Zheng, and Q. Yan, “A blockchain-based
    decentralized federated learning framework with committee consensus,” *IEEE Netw.*,
    vol. 35, no. 1, pp. 234–241, 2021\. [Online]. Available: [https://doi.org/10.1109/MNET.011.2000263](https://doi.org/10.1109/MNET.011.2000263)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Li, C. Chen, N. Liu, H. Huang, Z. Zheng, 和 Q. Yan, “基于区块链的去中心化联邦学习框架与委员会共识,”
    *IEEE网络*, 卷35, 期1, 页234–241, 2021年。[在线]. 可用： [https://doi.org/10.1109/MNET.011.2000263](https://doi.org/10.1109/MNET.011.2000263)'
- en: '[14] Y. Qi, M. S. Hossain, J. Nie, and X. Li, “Privacy-preserving blockchain-based
    federated learning for traffic flow prediction,” *Future Gener. Comput. Syst.*,
    vol. 117, pp. 328–337, 2021.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Qi, M. S. Hossain, J. Nie, 和 X. Li, “基于区块链的隐私保护联邦学习用于交通流预测,” *未来一代计算系统*,
    卷117, 页328–337, 2021年。'
- en: '[15] Y. Sun, H. Esaki, and H. Ochiai, “Blockchain-based federated learning
    against end-point adversarial data corruption,” in *19th IEEE International Conference
    on Machine Learning and Applications, ICMLA 2020, Miami, FL, USA, December 14-17,
    2020*.   IEEE, 2020, pp. 729–734\. [Online]. Available: [https://doi.org/10.1109/ICMLA51294.2020.00119](https://doi.org/10.1109/ICMLA51294.2020.00119)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Sun, H. Esaki, 和 H. Ochiai, “基于区块链的联邦学习防御端点对抗性数据篡改，” 见 *第19届IEEE国际机器学习与应用大会，ICMLA
    2020，迈阿密，FL，美国，2020年12月14-17日*。 IEEE，2020年，页码729–734。 [在线]. 可用： [https://doi.org/10.1109/ICMLA51294.2020.00119](https://doi.org/10.1109/ICMLA51294.2020.00119)'
- en: '[16] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Blockchain and federated
    learning for privacy-preserved data sharing in industrial iot,” *IEEE Trans. Ind.
    Informatics*, vol. 16, no. 6, pp. 4177–4186, 2020\. [Online]. Available: [https://doi.org/10.1109/TII.2019.2942190](https://doi.org/10.1109/TII.2019.2942190)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Lu, X. Huang, Y. Dai, S. Maharjan, 和 Y. Zhang, “用于隐私保护数据共享的区块链与联邦学习在工业物联网中的应用，”
    *IEEE工业信息学学报*，第16卷，第6期，页码4177–4186，2020年。 [在线]. 可用： [https://doi.org/10.1109/TII.2019.2942190](https://doi.org/10.1109/TII.2019.2942190)'
- en: '[17] H. Kim, J. Park, M. Bennis, and S. Kim, “Blockchained on-device federated
    learning,” *IEEE Commun. Lett.*, vol. 24, no. 6, pp. 1279–1283, 2020.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] H. Kim, J. Park, M. Bennis, 和 S. Kim, “基于区块链的设备端联邦学习，” *IEEE通信快报*，第24卷，第6期，页码1279–1283，2020年。'
- en: '[18] N. I. Mowla, N. H. Tran, I. Doh, and K. Chae, “Federated learning-based
    cognitive detection of jamming attack in flying ad-hoc network,” *IEEE Access*,
    vol. 8, pp. 4338–4350, 2020\. [Online]. Available: [https://doi.org/10.1109/ACCESS.2019.2962873](https://doi.org/10.1109/ACCESS.2019.2962873)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] N. I. Mowla, N. H. Tran, I. Doh, 和 K. Chae, “基于联邦学习的飞行自组网中的干扰攻击认知检测，”
    *IEEE Access*，第8卷，页码4338–4350，2020年。 [在线]. 可用： [https://doi.org/10.1109/ACCESS.2019.2962873](https://doi.org/10.1109/ACCESS.2019.2962873)'
- en: '[19] “Facebook-cambridge analytica data scandal,” [https://en.wikipedia.org/wiki/%****␣TAI.bbl␣Line␣150␣****Facebook%E2%80%93Cambridge_Analytica_data_scandal](https://en.wikipedia.org/wiki/%****%20TAI.bbl%20Line%20150%20****Facebook%E2%80%93Cambridge_Analytica_data_scandal),
    accessed: 2021-09-13.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] “Facebook-cambridge analytica 数据丑闻，” [https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal](https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal)，访问时间：2021-09-13。'
- en: '[20] “Apple encryption dispute,” [https://en.wikipedia.org/wiki/FBI%E2%80%93Apple_encryption_dispute](https://en.wikipedia.org/wiki/FBI%E2%80%93Apple_encryption_dispute),
    accessed: 2021-09-14.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] “Apple 加密争议，” [https://en.wikipedia.org/wiki/FBI%E2%80%93Apple_encryption_dispute](https://en.wikipedia.org/wiki/FBI%E2%80%93Apple_encryption_dispute)，访问时间：2021-09-14。'
- en: '[21] P. M, S. P. R. M, Q.-V. Pham, K. Dev, P. K. R. Maddikunta, T. R. Gadekallu,
    and T. Huynh-The, “Fusion of federated learning and industrial internet of things:
    A survey,” 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. M, S. P. R. M, Q.-V. Pham, K. Dev, P. K. R. Maddikunta, T. R. Gadekallu,
    和 T. Huynh-The, “联邦学习与工业物联网的融合：综述”，2021年。'
- en: '[22] Y. Gao, L. Liu, B. Hu, T. Lei, and H. Ma, “Federated region-learning for
    environment sensing in edge computing system,” *IEEE Transactions on Network Science
    and Engineering*, vol. 7, no. 4, pp. 2192–2204, 2020.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Gao, L. Liu, B. Hu, T. Lei, 和 H. Ma, “边缘计算系统中的联邦区域学习环境感知，” *IEEE网络科学与工程学报*，第7卷，第4期，页码2192–2204，2020年。'
- en: '[23] Y. Liu, A. Huang, Y. Luo, H. Huang, Y. Liu, Y. Chen, L. Feng, T. Chen,
    H. Yu, and Q. Yang, “Fedvision: An online visual object detection platform powered
    by federated learning,” *Proceedings of the AAAI Conference on Artificial Intelligence*,
    vol. 34, no. 08, pp. 13 172–13 179, Apr. 2020. [Online]. Available: [https://ojs.aaai.org/index.php/AAAI/article/view/7021](https://ojs.aaai.org/index.php/AAAI/article/view/7021)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Y. Liu, A. Huang, Y. Luo, H. Huang, Y. Liu, Y. Chen, L. Feng, T. Chen,
    H. Yu, 和 Q. Yang, “Fedvision：由联邦学习驱动的在线视觉对象检测平台，” *AAAI人工智能会议论文集*，第34卷，第08期，页码13 172–13 179，2020年4月。
    [在线]. 可用： [https://ojs.aaai.org/index.php/AAAI/article/view/7021](https://ojs.aaai.org/index.php/AAAI/article/view/7021)'
- en: '[24] S. R. Pokhrel and J. Choi, “Federated learning with blockchain for autonomous
    vehicles: Analysis and design challenges,” *IEEE Transactions on Communications*,
    vol. 68, no. 8, pp. 4734–4746, 2020.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. R. Pokhrel 和 J. Choi, “面向自动驾驶车辆的区块链联邦学习：分析与设计挑战，” *IEEE通信学报*，第68卷，第8期，页码4734–4746，2020年。'
- en: '[25] B. Liu, L. Wang, M. Liu, and C. Xu, “Lifelong federated reinforcement
    learning: A learning architecture for navigation in cloud robotic systems,” *CoRR*,
    vol. abs/1901.06455, 2019\. [Online]. Available: [http://arxiv.org/abs/1901.06455](http://arxiv.org/abs/1901.06455)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] B. Liu, L. Wang, M. Liu, 和 C. Xu, “终身联邦强化学习：一种用于云机器人系统导航的学习架构，” *CoRR*，第abs/1901.06455卷，2019年。
    [在线]. 可用： [http://arxiv.org/abs/1901.06455](http://arxiv.org/abs/1901.06455)'
- en: '[26] Y. Sun, H. Esaki, and H. Ochiai, “Adaptive intrusion detection in the
    networking of large-scale lans with segmented federated learning,” *IEEE Open
    J. Commun. Soc.*, vol. 2, pp. 102–112, 2021\. [Online]. Available: [https://doi.org/10.1109/OJCOMS.2020.3044323](https://doi.org/10.1109/OJCOMS.2020.3044323)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Sun, H. Esaki, 和 H. Ochiai, “大规模局域网中的自适应入侵检测与分段联邦学习，” *IEEE开放通信学报*，第2卷，第102–112页，2021年。[在线].
    可用链接: [https://doi.org/10.1109/OJCOMS.2020.3044323](https://doi.org/10.1109/OJCOMS.2020.3044323)'
- en: '[27] S. A. Rahman, H. Tout, C. Talhi, and A. Mourad, “Internet of things intrusion
    detection: Centralized, on-device, or federated learning?” *IEEE Network*, vol. 34,
    no. 6, pp. 310–317, 2020.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] S. A. Rahman, H. Tout, C. Talhi, 和 A. Mourad, “物联网入侵检测：集中式、设备端还是联邦学习？”
    *IEEE网络*，第34卷，第6期，第310–317页，2020年。'
- en: '[28] M. G. Poirot, P. Vepakomma, K. Chang, J. Kalpathy-Cramer, R. Gupta, and
    R. Raskar, “Split learning for collaborative deep learning in healthcare,” *CoRR*,
    vol. abs/1912.12115, 2019\. [Online]. Available: [http://arxiv.org/abs/1912.12115](http://arxiv.org/abs/1912.12115)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] M. G. Poirot, P. Vepakomma, K. Chang, J. Kalpathy-Cramer, R. Gupta, 和
    R. Raskar, “医疗保健中的协作深度学习的分割学习，” *CoRR*，第abs/1912.12115卷，2019年。[在线]. 可用链接: [http://arxiv.org/abs/1912.12115](http://arxiv.org/abs/1912.12115)'
- en: '[29] L. U. Khan, S. R. Pandey, N. H. Tran, W. Saad, Z. Han, M. N. H. Nguyen,
    and C. S. Hong, “Federated learning for edge networks: Resource optimization and
    incentive mechanism,” *IEEE Communications Magazine*, vol. 58, no. 10, pp. 88–93,
    2020.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] L. U. Khan, S. R. Pandey, N. H. Tran, W. Saad, Z. Han, M. N. H. Nguyen,
    和 C. S. Hong, “边缘网络中的联邦学习：资源优化与激励机制，” *IEEE通讯杂志*，第58卷，第10期，第88–93页，2020年。'
- en: '[30] Y. Lu, X. Huang, Y. Dai, S. Maharjan, and Y. Zhang, “Differentially private
    asynchronous federated learning for mobile edge computing in urban informatics,”
    *IEEE Trans. Ind. Informatics*, vol. 16, no. 3, pp. 2134–2143, 2020\. [Online].
    Available: [https://doi.org/10.1109/TII.2019.2942179](https://doi.org/10.1109/TII.2019.2942179)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Lu, X. Huang, Y. Dai, S. Maharjan, 和 Y. Zhang, “城市信息学中的移动边缘计算的差分隐私异步联邦学习，”
    *IEEE工业信息学汇刊*，第16卷，第3期，第2134–2143页，2020年。[在线]. 可用链接: [https://doi.org/10.1109/TII.2019.2942179](https://doi.org/10.1109/TII.2019.2942179)'
- en: '[31] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-Efficient
    Learning of Deep Networks from Decentralized Data,” in *Proceedings of the 20th
    International Conference on Artificial Intelligence and Statistics*, ser. Proceedings
    of Machine Learning Research, A. Singh and J. Zhu, Eds., vol. 54.   Fort Lauderdale,
    FL, USA: PMLR, 20–22 Apr 2017, pp. 1273–1282. [Online]. Available: [http://proceedings.mlr.press/v54/mcmahan17a.html](http://proceedings.mlr.press/v54/mcmahan17a.html)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. McMahan, E. Moore, D. Ramage, S. Hampson, 和 B. A. y Arcas, “从分散数据中高效通信学习深度网络，”
    发表在*第20届国际人工智能与统计会议论文集*，机器学习研究论文集，A. Singh 和 J. Zhu 主编，第54卷。美国佛罗里达州劳德代尔堡：PMLR，2017年4月20–22日，第1273–1282页。[在线].
    可用链接: [http://proceedings.mlr.press/v54/mcmahan17a.html](http://proceedings.mlr.press/v54/mcmahan17a.html)'
- en: '[32] H.-K. Lim, J.-B. Kim, J.-S. Heo, and Y.-H. Han, “Federated reinforcement
    learning for training control policies on multiple iot devices,” *Sensors*, vol. 20,
    no. 5, 2020\. [Online]. Available: [https://www.mdpi.com/1424-8220/20/5/1359](https://www.mdpi.com/1424-8220/20/5/1359)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] H.-K. Lim, J.-B. Kim, J.-S. Heo, 和 Y.-H. Han, “用于训练多个物联网设备控制策略的联邦强化学习，”
    *传感器*，第20卷，第5期，2020年。[在线]. 可用链接: [https://www.mdpi.com/1424-8220/20/5/1359](https://www.mdpi.com/1424-8220/20/5/1359)'
- en: '[33] P. Vepakomma, O. Gupta, T. Swedish, and R. Raskar, “Split learning for
    health: Distributed deep learning without sharing raw patient data,” *CoRR*, vol.
    abs/1812.00564, 2018\. [Online]. Available: [http://arxiv.org/abs/1812.00564](http://arxiv.org/abs/1812.00564)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Vepakomma, O. Gupta, T. Swedish, 和 R. Raskar, “健康领域的分割学习：无需共享原始病人数据的分布式深度学习，”
    *CoRR*，第abs/1812.00564卷，2018年。[在线]. 可用链接: [http://arxiv.org/abs/1812.00564](http://arxiv.org/abs/1812.00564)'
- en: '[34] T. Nishio and R. Yonetani, “Client selection for federated learning with
    heterogeneous resources in mobile edge,” in *2019 IEEE International Conference
    on Communications, ICC 2019, Shanghai, China, May 20-24, 2019*.   IEEE, 2019,
    pp. 1–7. [Online]. Available: [https://doi.org/10.1109/ICC.2019.8761315](https://doi.org/10.1109/ICC.2019.8761315)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] T. Nishio 和 R. Yonetani, “移动边缘中具有异质资源的联邦学习客户端选择，” 发表在*2019年IEEE国际通讯会议，ICC
    2019，中国上海，2019年5月20-24日*。IEEE，2019年，第1–7页。[在线]. 可用链接: [https://doi.org/10.1109/ICC.2019.8761315](https://doi.org/10.1109/ICC.2019.8761315)'
- en: '[35] A. Singh, P. Vepakomma, O. Gupta, and R. Raskar, “Detailed comparison
    of communication efficiency of split learning and federated learning,” *CoRR*,
    vol. abs/1909.09145, 2019\. [Online]. Available: [http://arxiv.org/abs/1909.09145](http://arxiv.org/abs/1909.09145)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Singh, P. Vepakomma, O. Gupta 和 R. Raskar， “分割学习和联邦学习的通信效率详细比较”， *CoRR*，卷
    abs/1909.09145，2019年。 [在线]. 可用： [http://arxiv.org/abs/1909.09145](http://arxiv.org/abs/1909.09145)'
- en: '[36] C. Thapa, M. A. P. Chamikara, and S. Camtepe, “Splitfed: When federated
    learning meets split learning,” *CoRR*, vol. abs/2004.12088, 2020. [Online]. Available:
    [https://arxiv.org/abs/2004.12088](https://arxiv.org/abs/2004.12088)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Thapa, M. A. P. Chamikara 和 S. Camtepe， “Splitfed: 当联邦学习遇上分割学习”， *CoRR*，卷
    abs/2004.12088，2020年。 [在线]. 可用： [https://arxiv.org/abs/2004.12088](https://arxiv.org/abs/2004.12088)'
- en: '[37] E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S. Kim, “Communication-efficient
    on-device machine learning: Federated distillation and augmentation under non-iid
    private data,” *CoRR*, vol. abs/1811.11479, 2018\. [Online]. Available: [http://arxiv.org/abs/1811.11479](http://arxiv.org/abs/1811.11479)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis 和 S. Kim， “通信高效的设备端机器学习：在非独立同分布私有数据下的联邦蒸馏和增强”，
    *CoRR*，卷 abs/1811.11479，2018年。 [在线]. 可用： [http://arxiv.org/abs/1811.11479](http://arxiv.org/abs/1811.11479)'
- en: '[38] O. Sener and S. Savarese, “Active learning for convolutional neural networks:
    A core-set approach,” in *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.   OpenReview.net,
    2018\. [Online]. Available: [https://openreview.net/forum?id=H1aIuk-RW](https://openreview.net/forum?id=H1aIuk-RW)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] O. Sener 和 S. Savarese， “卷积神经网络的主动学习：核心集方法”， 收录于 *第6届国际学习表征会议，ICLR 2018，加拿大不列颠哥伦比亚省温哥华，2018年4月30日
    - 5月3日，会议论文集*。 OpenReview.net，2018年。 [在线]. 可用： [https://openreview.net/forum?id=H1aIuk-RW](https://openreview.net/forum?id=H1aIuk-RW)'
- en: '[39] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated
    learning with non-iid data,” *CoRR*, vol. abs/1806.00582, 2018\. [Online]. Available:
    [http://arxiv.org/abs/1806.00582](http://arxiv.org/abs/1806.00582)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin 和 V. Chandra， “具有非独立同分布数据的联邦学习”，
    *CoRR*，卷 abs/1806.00582，2018年。 [在线]. 可用： [http://arxiv.org/abs/1806.00582](http://arxiv.org/abs/1806.00582)'
- en: '[40] H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing federated learning
    on non-iid data with reinforcement learning,” in *39th IEEE Conference on Computer
    Communications, INFOCOM 2020, Toronto, ON, Canada, July 6-9, 2020*.   IEEE, 2020,
    pp. 1698–1707. [Online]. Available: [https://doi.org/10.1109/INFOCOM41043.2020.9155494](https://doi.org/10.1109/INFOCOM41043.2020.9155494)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] H. Wang, Z. Kaplan, D. Niu 和 B. Li， “使用强化学习优化非独立同分布数据上的联邦学习”， 收录于 *第39届IEEE计算机通信会议，INFOCOM
    2020，加拿大安大略省多伦多，2020年7月6-9日*。 IEEE，2020年，第1698–1707页。 [在线]. 可用： [https://doi.org/10.1109/INFOCOM41043.2020.9155494](https://doi.org/10.1109/INFOCOM41043.2020.9155494)'
- en: '[41] M. Duan, D. Liu, X. Chen, R. Liu, Y. Tan, and L. Liang, “Self-balancing
    federated learning with global imbalanced data in mobile systems,” *IEEE Trans.
    Parallel Distributed Syst.*, vol. 32, no. 1, pp. 59–71, 2021\. [Online]. Available:
    [https://doi.org/10.1109/TPDS.2020.3009406](https://doi.org/10.1109/TPDS.2020.3009406)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] M. Duan, D. Liu, X. Chen, R. Liu, Y. Tan 和 L. Liang， “在移动系统中具有全局不平衡数据的自平衡联邦学习”，
    *IEEE Trans. Parallel Distributed Syst.*，卷32，第1期，第59–71页，2021年。 [在线]. 可用： [https://doi.org/10.1109/TPDS.2020.3009406](https://doi.org/10.1109/TPDS.2020.3009406)'
- en: '[42] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, “Communication-efficient
    learning of deep networks from decentralized data,” in *Proceedings of the 20th
    International Conference on Artificial Intelligence and Statistics, AISTATS 2017,
    20-22 April 2017, Fort Lauderdale, FL, USA*, ser. Proceedings of Machine Learning
    Research, vol. 54.   PMLR, 2017, pp. 1273–1282. [Online]. Available: [http://proceedings.mlr.press/v54/mcmahan17a.html](http://proceedings.mlr.press/v54/mcmahan17a.html)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] B. McMahan, E. Moore, D. Ramage, S. Hampson 和 B. A. y Arcas， “从去中心化数据中高效学习深度网络”，
    收录于 *第20届国际人工智能与统计会议，AISTATS 2017，2017年4月20-22日，美国佛罗里达州劳德代尔堡*， 机器学习研究会议论文集，第54卷。
    PMLR，2017年，第1273–1282页。 [在线]. 可用： [http://proceedings.mlr.press/v54/mcmahan17a.html](http://proceedings.mlr.press/v54/mcmahan17a.html)'
- en: '[43] L. P. Kaelbling, M. L. Littman, and A. W. Moore, “Reinforcement learning:
    A survey,” *J. Artif. Intell. Res.*, vol. 4, pp. 237–285, 1996. [Online]. Available:
    [https://doi.org/10.1613/jair.301](https://doi.org/10.1613/jair.301)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] L. P. Kaelbling, M. L. Littman 和 A. W. Moore， “强化学习：一项综述”， *J. Artif.
    Intell. Res.*，卷4，第237–285页，1996年。 [在线]. 可用： [https://doi.org/10.1613/jair.301](https://doi.org/10.1613/jair.301)'
- en: '[44] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
    backdoor federated learning,” in *The 23rd International Conference on Artificial
    Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo,
    Sicily, Italy]*, ser. Proceedings of Machine Learning Research, vol. 108.   PMLR,
    2020, pp. 2938–2948\. [Online]. Available: [http://proceedings.mlr.press/v108/bagdasaryan20a.html](http://proceedings.mlr.press/v108/bagdasaryan20a.html)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, 和 V. Shmatikov，"如何在联邦学习中植入后门"，见于*第23届国际人工智能与统计会议，AISTATS
    2020，2020年8月26-28日，在线[巴勒莫，西西里岛，意大利]*，系列：机器学习研究文集，第108卷。PMLR，2020年，页码2938–2948。[在线]。可用链接：[http://proceedings.mlr.press/v108/bagdasaryan20a.html](http://proceedings.mlr.press/v108/bagdasaryan20a.html)'
- en: '[45] E. Bagdasaryan and V. Shmatikov, “Blind backdoors in deep learning models,”
    *CoRR*, vol. abs/2005.03823, 2020\. [Online]. Available: [https://arxiv.org/abs/2005.03823](https://arxiv.org/abs/2005.03823)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] E. Bagdasaryan 和 V. Shmatikov，"深度学习模型中的盲后门"，*CoRR*，第abs/2005.03823卷，2020年。[在线]。可用链接：[https://arxiv.org/abs/2005.03823](https://arxiv.org/abs/2005.03823)'
- en: '[46] J. Lin, L. Xu, Y. Liu, and X. Zhang, “Composite backdoor attack for deep
    neural network by mixing existing benign features,” in *CCS ’20: 2020 ACM SIGSAC
    Conference on Computer and Communications Security, Virtual Event, USA, November
    9-13, 2020*.   ACM, 2020, pp. 113–131\. [Online]. Available: [https://doi.org/10.1145/3372297.3423362](https://doi.org/10.1145/3372297.3423362)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Lin, L. Xu, Y. Liu, 和 X. Zhang，"通过混合现有良性特征进行深度神经网络的复合后门攻击"，见于*CCS ’20:
    2020年ACM SIGSAC计算机与通信安全会议，虚拟会议，美国，2020年11月9-13日*。ACM，2020年，页码113–131。[在线]。可用链接：[https://doi.org/10.1145/3372297.3423362](https://doi.org/10.1145/3372297.3423362)'
- en: '[47] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J. Sohn,
    K. Lee, and D. S. Papailiopoulos, “Attack of the tails: Yes, you really can backdoor
    federated learning,” in *Advances in Neural Information Processing Systems 33:
    Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual*, 2020\. [Online]. Available: [https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal, J. Sohn,
    K. Lee, 和 D. S. Papailiopoulos，"尾部攻击：是的，你真的可以在联邦学习中植入后门"，见于*神经信息处理系统33：2020年神经信息处理系统年度会议，NeurIPS
    2020，2020年12月6-12日，虚拟会议*，2020年。[在线]。可用链接：[https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/b8ffa41d4e492f0fad2f13e29e1762eb-Abstract.html)'
- en: '[48] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse attacks on deep
    learning systems,” *CoRR*, vol. abs/1812.00483, 2018\. [Online]. Available: [http://arxiv.org/abs/1812.00483](http://arxiv.org/abs/1812.00483)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. Ji, X. Zhang, S. Ji, X. Luo, 和 T. Wang，"对深度学习系统的模型重用攻击"，*CoRR*，第abs/1812.00483卷，2018年。[在线]。可用链接：[http://arxiv.org/abs/1812.00483](http://arxiv.org/abs/1812.00483)'
- en: '[49] P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, and J. Stainer, “Machine
    learning with adversaries: Byzantine tolerant gradient descent,” in *Advances
    in Neural Information Processing Systems 30: Annual Conference on Neural Information
    Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, 2017, pp. 119–129\.
    [Online]. Available: [https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] P. Blanchard, E. M. E. Mhamdi, R. Guerraoui, 和 J. Stainer，"带有对手的机器学习：拜占庭容忍的梯度下降"，见于*神经信息处理系统30：2017年神经信息处理系统年度会议，2017年12月4-9日，洛杉矶，加州，美国*，2017年，页码119–129。[在线]。可用链接：[https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/f4b9ec30ad9f68f89b29639786cb62ef-Abstract.html)'
- en: '[50] E. M. E. Mhamdi, R. Guerraoui, and S. Rouault, “The hidden vulnerability
    of distributed learning in byzantium,” in *Proceedings of the 35th International
    Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
    July 10-15, 2018*, ser. Proceedings of Machine Learning Research, vol. 80.   PMLR,
    2018, pp. 3518–3527\. [Online]. Available: [http://proceedings.mlr.press/v80/mhamdi18a.html](http://proceedings.mlr.press/v80/mhamdi18a.html)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] E. M. E. Mhamdi, R. Guerraoui, 和 S. Rouault，"拜占庭中的分布式学习隐患"，见于*第35届国际机器学习大会，ICML
    2018，斯德哥尔摩，瑞典，2018年7月10-15日*，系列：机器学习研究文集，第80卷。PMLR，2018年，页码3518–3527。[在线]。可用链接：[http://proceedings.mlr.press/v80/mhamdi18a.html](http://proceedings.mlr.press/v80/mhamdi18a.html)'
- en: '[51] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
    examples in deep neural networks,” in *25th Annual Network and Distributed System
    Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018*.   The
    Internet Society, 2018\. [Online]. Available: [http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf](http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] W. Xu, D. Evans 和 Y. Qi，“特征挤压：检测深度神经网络中的对抗样本，” 见于 *第25届年度网络与分布式系统安全研讨会，NDSS
    2018，圣地亚哥，加利福尼亚州，美国，2018年2月18-21日*。互联网协会，2018年。[在线]. 可用: [http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf](http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-4_Xu_paper.pdf)'
- en: '[52] L. Melis, C. Song, E. D. Cristofaro, and V. Shmatikov, “Exploiting unintended
    feature leakage in collaborative learning,” in *2019 IEEE Symposium on Security
    and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019*.   IEEE, 2019,
    pp. 691–706. [Online]. Available: [https://doi.org/10.1109/SP.2019.00029](https://doi.org/10.1109/SP.2019.00029)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] L. Melis, C. Song, E. D. Cristofaro 和 V. Shmatikov，“利用协作学习中的意外特征泄露，” 见于
    *2019年IEEE安全与隐私研讨会，SP 2019，旧金山，加州，美国，2019年5月19-23日*。IEEE，2019年，第691-706页。[在线].
    可用: [https://doi.org/10.1109/SP.2019.00029](https://doi.org/10.1109/SP.2019.00029)'
- en: '[53] H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
    private recurrent language models,” in *6th International Conference on Learning
    Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference
    Track Proceedings*.   OpenReview.net, 2018\. [Online]. Available: [https://openreview.net/forum?id=BJ0hF1Z0b](https://openreview.net/forum?id=BJ0hF1Z0b)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] H. B. McMahan, D. Ramage, K. Talwar 和 L. Zhang，“学习不同ially privacy 的递归语言模型，”
    见于 *第六届国际学习表征会议，ICLR 2018，温哥华，加拿大，2018年4月30日 - 5月3日，会议论文集*。OpenReview.net，2018年。[在线].
    可用: [https://openreview.net/forum?id=BJ0hF1Z0b](https://openreview.net/forum?id=BJ0hF1Z0b)'
- en: '[54] D. Cao, S. Chang, Z. Lin, G. Liu, and D. Sun, “Understanding distributed
    poisoning attack in federated learning,” in *25th IEEE International Conference
    on Parallel and Distributed Systems, ICPADS 2019, Tianjin, China, December 4-6,
    2019*.   IEEE, 2019, pp. 233–239\. [Online]. Available: [https://doi.org/10.1109/ICPADS47876.2019.00042](https://doi.org/10.1109/ICPADS47876.2019.00042)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] D. Cao, S. Chang, Z. Lin, G. Liu 和 D. Sun，“理解联邦学习中的分布式中毒攻击，” 见于 *第25届IEEE国际并行与分布式系统大会，ICPADS
    2019，天津，中国，2019年12月4-6日*。IEEE，2019年，第233-239页。[在线]. 可用: [https://doi.org/10.1109/ICPADS47876.2019.00042](https://doi.org/10.1109/ICPADS47876.2019.00042)'
- en: '[55] T. Nguyen, P. Rieger, M. Miettinen, and A. Sadeghi, “Poisoning attacks
    on federated learning-based iot intrusion detection system,” 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T. Nguyen, P. Rieger, M. Miettinen 和 A. Sadeghi，“基于联邦学习的物联网入侵检测系统的中毒攻击，”
    2020年。'
- en: '[56] V. Tolpegin, S. Truex, M. E. Gursoy, and L. Liu, “Data poisoning attacks
    against federated learning systems,” in *Computer Security - ESORICS 2020 - 25th
    European Symposium on Research in Computer Security, ESORICS 2020, Guildford,
    UK, September 14-18, 2020, Proceedings, Part I*, ser. Lecture Notes in Computer
    Science, vol. 12308.   Springer, 2020, pp. 480–501\. [Online]. Available: [https://doi.org/10.1007/978-3-030-58951-6_24](https://doi.org/10.1007/978-3-030-58951-6_24)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] V. Tolpegin, S. Truex, M. E. Gursoy 和 L. Liu，“针对联邦学习系统的数据中毒攻击，” 见于 *计算机安全
    - ESORICS 2020 - 第25届欧洲计算机安全研究研讨会，ESORICS 2020，吉尔福德，英国，2020年9月14-18日，论文集，第I部分*，系列：计算机科学讲义笔记，第12308卷。Springer，2020年，第480-501页。[在线].
    可用: [https://doi.org/10.1007/978-3-030-58951-6_24](https://doi.org/10.1007/978-3-030-58951-6_24)'
- en: '[57] Y. Zhao, J. Chen, J. Zhang, D. Wu, J. Teng, and S. Yu, “PDGAN: A novel
    poisoning defense method in federated learning using generative adversarial network,”
    in *Algorithms and Architectures for Parallel Processing - 19th International
    Conference, ICA3PP 2019, Melbourne, VIC, Australia, December 9-11, 2019, Proceedings,
    Part I*, ser. Lecture Notes in Computer Science, vol. 11944.   Springer, 2019,
    pp. 595–609\. [Online]. Available: [https://doi.org/10.1007/978-3-030-38991-8_39](https://doi.org/10.1007/978-3-030-38991-8_39)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Y. Zhao, J. Chen, J. Zhang, D. Wu, J. Teng 和 S. Yu，“PDGAN：一种利用生成对抗网络的联邦学习中的新型中毒防御方法，”
    见于 *并行处理的算法与架构 - 第19届国际会议，ICA3PP 2019，墨尔本，维多利亚州，澳大利亚，2019年12月9-11日，论文集，第I部分*，系列：计算机科学讲义笔记，第11944卷。Springer，2019年，第595-609页。[在线].
    可用: [https://doi.org/10.1007/978-3-030-38991-8_39](https://doi.org/10.1007/978-3-030-38991-8_39)'
- en: '[58] C. Fung, C. J. M. Yoon, and I. Beschastnikh, “Mitigating sybils in federated
    learning poisoning,” *CoRR*, vol. abs/1808.04866, 2018\. [Online]. Available:
    [http://arxiv.org/abs/1808.04866](http://arxiv.org/abs/1808.04866)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] C. Fung, C. J. M. Yoon, 和 I. Beschastnikh, “缓解联邦学习中对抗者的影响,” *CoRR*, 卷
    abs/1808.04866, 2018\. [在线]. 可用: [http://arxiv.org/abs/1808.04866](http://arxiv.org/abs/1808.04866)'
- en: '[59] H. Xiao, H. Xiao, and C. Eckert, “Adversarial label flips attack on support
    vector machines,” in *ECAI 2012 - 20th European Conference on Artificial Intelligence.
    Including Prestigious Applications of Artificial Intelligence (PAIS-2012) System
    Demonstrations Track, Montpellier, France, August 27-31 , 2012*, ser. Frontiers
    in Artificial Intelligence and Applications, vol. 242.   IOS Press, 2012, pp.
    870–875\. [Online]. Available: [https://doi.org/10.3233/978-1-61499-098-7-870](https://doi.org/10.3233/978-1-61499-098-7-870)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] H. Xiao, H. Xiao, 和 C. Eckert, “对支持向量机的对抗标签翻转攻击，” 在 *ECAI 2012 - 第20届欧洲人工智能会议.
    包括人工智能的著名应用 (PAIS-2012) 系统演示轨道, 法国蒙彼利埃, 2012年8月27-31日*, 系列. 人工智能与应用前沿, 卷 242。IOS出版社,
    2012, 第870-875页\. [在线]. 可用: [https://doi.org/10.3233/978-1-61499-098-7-870](https://doi.org/10.3233/978-1-61499-098-7-870)'
- en: '[60] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D. Tygar, “Can machine
    learning be secure?” in *Proceedings of the 2006 ACM Symposium on Information,
    Computer and Communications Security, ASIACCS 2006, Taipei, Taiwan, March 21-24,
    2006*.   ACM, 2006, pp. 16–25\. [Online]. Available: [https://doi.org/10.1145/1128817.1128824](https://doi.org/10.1145/1128817.1128824)'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, 和 J. D. Tygar, “机器学习能否安全？”
    在 *2006年ACM信息、计算机与通信安全研讨会论文集, ASIACCS 2006, 台北, 台湾, 2006年3月21-24日*。ACM, 2006,
    第16-25页\. [在线]. 可用: [https://doi.org/10.1145/1128817.1128824](https://doi.org/10.1145/1128817.1128824)'
- en: '[61] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial networks,” *CoRR*,
    vol. abs/1406.2661, 2014\. [Online]. Available: [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, 和 Y. Bengio, “生成对抗网络,” *CoRR*, 卷 abs/1406.2661, 2014\.
    [在线]. 可用: [http://arxiv.org/abs/1406.2661](http://arxiv.org/abs/1406.2661)'
- en: '[62] B. Hitaj, G. Ateniese, and F. Pérez-Cruz, “Deep models under the GAN:
    information leakage from collaborative deep learning,” in *Proceedings of the
    2017 ACM SIGSAC Conference on Computer and Communications Security, CCS 2017,
    Dallas, TX, USA, October 30 - November 03, 2017*.   ACM, 2017, pp. 603–618. [Online].
    Available: [https://doi.org/10.1145/3133956.3134012](https://doi.org/10.1145/3133956.3134012)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] B. Hitaj, G. Ateniese, 和 F. Pérez-Cruz, “GAN下的深度模型：来自协作深度学习的信息泄漏，” 在 *2017年ACM
    SIGSAC计算机与通信安全会议论文集, CCS 2017, 达拉斯, TX, 美国, 2017年10月30日 - 11月3日*。ACM, 2017, 第603-618页。
    [在线]. 可用: [https://doi.org/10.1145/3133956.3134012](https://doi.org/10.1145/3133956.3134012)'
- en: '[63] M. Nasr, R. Shokri, and A. Houmansadr, “Comprehensive privacy analysis
    of deep learning: Passive and active white-box inference attacks against centralized
    and federated learning,” in *2019 IEEE Symposium on Security and Privacy (SP)*,
    2019, pp. 739–753.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Nasr, R. Shokri, 和 A. Houmansadr, “深度学习的全面隐私分析：针对集中式和联邦学习的被动和主动白盒推断攻击，”
    在 *2019年IEEE安全与隐私研讨会 (SP)*, 2019, 第739-753页。'
- en: '[64] A. Amini, A. P. Soleimany, W. Schwarting, S. N. Bhatia, and D. Rus, “Uncovering
    and mitigating algorithmic bias through learned latent structure,” in *Proceedings
    of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, Honolulu,
    HI, USA, January 27-28, 2019*.   ACM, 2019, pp. 289–295.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Amini, A. P. Soleimany, W. Schwarting, S. N. Bhatia, 和 D. Rus, “通过学习的潜在结构揭示和缓解算法偏见，”
    在 *2019年AAAI/ACM人工智能、伦理与社会会议论文集, AIES 2019, 檀香山, HI, 美国, 2019年1月27-28日*。ACM, 2019,
    第289-295页。'
- en: '[65] B. H. Zhang, B. Lemoine, and M. Mitchell, “Mitigating unwanted biases
    with adversarial learning,” in *Proceedings of the 2018 AAAI/ACM Conference on
    AI, Ethics, and Society, AIES 2018, New Orleans, LA, USA, February 02-03, 2018*.   ACM,
    2018, pp. 335–340\. [Online]. Available: [https://doi.org/10.1145/3278721.3278779](https://doi.org/10.1145/3278721.3278779)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] B. H. Zhang, B. Lemoine, 和 M. Mitchell, “通过对抗学习缓解不希望的偏见，” 在 *2018年AAAI/ACM人工智能、伦理与社会会议论文集,
    AIES 2018, 新奥尔良, LA, 美国, 2018年2月02-03日*。ACM, 2018, 第335-340页\. [在线]. 可用: [https://doi.org/10.1145/3278721.3278779](https://doi.org/10.1145/3278721.3278779)'
- en: '[66] S. Feng, D. Niyato, P. Wang, D. I. Kim, and Y. Liang, “Joint service pricing
    and cooperative relay communication for federated learning,” in *2019 International
    Conference on Internet of Things (iThings) and IEEE Green Computing and Communications
    (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart
    Data (SmartData), iThings/GreenCom/CPSCom/SmartData 2019, Atlanta, GA, USA, July
    14-17, 2019*.   IEEE, 2019, pp. 815–820. [Online]. Available: [https://doi.org/10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00148](https://doi.org/10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00148)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Feng, D. Niyato, P. Wang, D. I. Kim, 和 Y. Liang，“联合服务定价和合作中继通信在联邦学习中的应用，”在*2019国际物联网大会（iThings）和IEEE绿色计算与通信（GreenCom）和IEEE网络、物理和社会计算（CPSCom）和IEEE智能数据（SmartData），iThings/GreenCom/CPSCom/SmartData
    2019，乔治亚州亚特兰大，美国，2019年7月14-17日*。IEEE，2019，第815-820页。[在线]。可用：[https://doi.org/10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00148](https://doi.org/10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00148)'
- en: '{IEEEbiography}'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEbiography}'
- en: '[![[Uncaptioned image]](img/7c398bdb2d7b2b2912dc82225c31c86d.png)]Yuwei Sun
    (M’20) is a Ph.D.’s student in the Graduate School of Information Science and
    Technology at the University of Tokyo. He received B.E. in Computer Science and
    Technology in 2018 from North China Electric Power University and M.E. in Information
    and Communication Engineering with honors in 2021 from the University of Tokyo.
    In 2020, he was the fellow of the Advanced Study Program (ASP) at the Massachusetts
    Institute of Technology. He has been working with the Campus Computing Centre
    at the United Nations University Centre on Cybersecurity since 2019\. He is a
    member of the AI Security and Privacy Team at the RIKEN Center for Advanced Intelligence
    Project working on trustworthy AI, and a research fellow at Japan Society for
    the Promotion of Science (JSPS).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[无标题图片]](img/7c398bdb2d7b2b2912dc82225c31c86d.png)]Yuwei Sun（M’20）是东京大学信息科学与技术研究生院的博士生。他于2018年在华北电力大学获得计算机科学与技术学士学位，2021年在东京大学获得信息与通信工程硕士学位（荣誉）。2020年，他是麻省理工学院高级研究项目（ASP）的研究员。自2019年以来，他一直与联合国大学网络安全中心的校园计算中心合作。他是RIKEN先进智能项目中心的AI安全与隐私团队成员，专注于可信赖的人工智能，并且是日本学术振兴会（JSPS）的研究员。'
- en: '{IEEEbiography}'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEbiography}'
- en: '[![[Uncaptioned image]](img/c46ce44d0e271839d1227584fcf8f22d.png)]Hideya Ochiai
    (M’10) is an associate professor of the University of Tokyo, Japan. He received
    B.E. in 2006, M.E. in 2008, and Ph.D. in 2011 from the same university. His research
    interests have been sensor networking, delay tolerant networking, and building
    automation systems, IoT protocols, and cyber-security. He is involved in the standardization
    of facility information access protocol in IEEE1888, ISO/IEC, and ASHRAE.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[无标题图片]](img/c46ce44d0e271839d1227584fcf8f22d.png)]Hideya Ochiai（M’10）是日本东京大学的副教授。他于2006年获得学士学位，2008年获得硕士学位，2011年获得博士学位，均来自同一大学。他的研究兴趣包括传感器网络、延迟容忍网络、建筑自动化系统、物联网协议和网络安全。他参与了IEEE1888、ISO/IEC和ASHRAE的设施信息访问协议的标准化工作。'
- en: '{IEEEbiography}'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEbiography}'
- en: '[![[Uncaptioned image]](img/0e0affee36b5dd30c98a4f9129209296.png)]Hiroshi Esaki
    (M’08) received Ph.D. from the University of Tokyo, Japan, in 1998\. In 1987,
    he joined Research and Development Center, Toshiba Corporation. From 1990 to 1991,
    he was at Applied Research Laboratory of Bell-core Inc., New Jersey, as a residential
    researcher. From 1994 to 1996, he was at Center for Telecommunication Research
    of Columbia University in New York. From 1998, he has been serving as a professor
    at the University of Tokyo, and as a board member of WIDE Project. Currently,
    he is the executive director of IPv6 promotion council, vice president of JPNIC,
    IPv6 Forum Fellow, and director of WIDE Project.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[无标题图片]](img/0e0affee36b5dd30c98a4f9129209296.png)]Hiroshi Esaki（M’08）于1998年从东京大学获得博士学位。1987年，他加入了东芝公司研究与开发中心。从1990年到1991年，他在新泽西州的Bell-core
    Inc.应用研究实验室担任驻场研究员。从1994年到1996年，他在纽约哥伦比亚大学电信研究中心工作。从1998年起，他一直担任东京大学教授，并且是WIDE项目的董事会成员。目前，他是IPv6推广委员会的执行董事，JPNIC的副主席，IPv6论坛研究员，以及WIDE项目的主任。'
