- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1902.06068] Deep Learning for Image Super-resolution: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1902.06068] 深度学习用于图像超分辨率：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1902.06068](https://ar5iv.labs.arxiv.org/html/1902.06068)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1902.06068](https://ar5iv.labs.arxiv.org/html/1902.06068)
- en: 'Deep Learning for Image Super-resolution:'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习用于图像超分辨率：
- en: A Survey
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一项综述
- en: 'Zhihao Wang, Jian Chen, Steven C.H. Hoi, Fellow, IEEE Corresponding author:
    Steven C.H. Hoi is currently with Salesforce Research Asia, and also a faculty
    member (on leave) of the School of Information Systems, Singapore Management University,
    Singapore. Email: shoi@salesforce.com or chhoi@smu.edu.sg. Z. Wang is with the
    South China University of Technology, China. E-mail: ptkin@outlook.com. This work
    was done when he was a visiting student with Dr Hoi’s group at the School of Information
    Systems, Singapore Management University, Singapore. J. Chen is with the South
    China University of Technology, China. E-mail: ellachen@scut.edu.cn.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 王智浩、陈建、史蒂文·C.H.·霍伊，IEEE 会士  通讯作者：史蒂文·C.H.·霍伊目前在 Salesforce Research Asia 工作，同时也是新加坡管理大学信息系统学院的（休假）教职员工。电子邮件：shoi@salesforce.com
    或 chhoi@smu.edu.sg。王智浩在中国南方科技大学工作。电子邮件：ptkin@outlook.com。这项工作是在他作为访问学生在新加坡管理大学信息系统学院霍伊博士团队时完成的。陈建在中国南方科技大学工作。电子邮件：ellachen@scut.edu.cn。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Image Super-Resolution (SR) is an important class of image processing techniques
    to enhance the resolution of images and videos in computer vision. Recent years
    have witnessed remarkable progress of image super-resolution using deep learning
    techniques. This article aims to provide a comprehensive survey on recent advances
    of image super-resolution using deep learning approaches. In general, we can roughly
    group the existing studies of SR techniques into three major categories: supervised
    SR, unsupervised SR, and domain-specific SR. In addition, we also cover some other
    important issues, such as publicly available benchmark datasets and performance
    evaluation metrics. Finally, we conclude this survey by highlighting several future
    directions and open issues which should be further addressed by the community
    in the future.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率（SR）是一类重要的图像处理技术，用于在计算机视觉中提高图像和视频的分辨率。近年来，图像超分辨率领域利用深度学习技术取得了显著进展。本文旨在提供关于利用深度学习方法的图像超分辨率的最新进展的全面综述。一般来说，我们可以大致将现有的
    SR 技术研究分为三大类：监督式 SR、非监督式 SR 和特定领域 SR。此外，我们还涵盖了一些其他重要问题，如公开可用的基准数据集和性能评估指标。最后，我们通过强调几个未来方向和亟待解决的开放问题来总结本综述，这些问题应在未来得到进一步的关注。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引术语：
- en: Image Super-resolution, Deep Learning, Convolutional Neural Networks (CNN),
    Generative Adversarial Nets (GAN)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率、深度学习、卷积神经网络（CNN）、生成对抗网络（GAN）
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/c69c2195915ffe762e33786a6987c25b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c69c2195915ffe762e33786a6987c25b.png)'
- en: 'Figure 1: Hierarchically-structured taxonomy of this survey.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本调查的层级结构分类法。
- en: Image super-resolution (SR), which refers to the process of recovering high-resolution
    (HR) images from low-resolution (LR) images, is an important class of image processing
    techniques in computer vision and image processing. It enjoys a wide range of
    real-world applications, such as medical imaging [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)], surveillance and security [[4](#bib.bib4), [5](#bib.bib5)]),
    amongst others. Other than improving image perceptual quality, it also helps to
    improve other computer vision tasks [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. In general, this problem is very challenging and inherently ill-posed
    since there are always multiple HR images corresponding to a single LR image.
    In literature, a variety of classical SR methods have been proposed, including
    prediction-based methods [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)],
    edge-based methods [[13](#bib.bib13), [14](#bib.bib14)], statistical methods [[15](#bib.bib15),
    [16](#bib.bib16)], patch-based methods [[13](#bib.bib13), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19)] and sparse representation methods [[20](#bib.bib20), [21](#bib.bib21)],
    etc.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率（SR）指的是从低分辨率（LR）图像恢复高分辨率（HR）图像的过程，是计算机视觉和图像处理中的一个重要图像处理技术类别。它在现实世界中有广泛的应用，如医学成像
    [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]、监控和安全 [[4](#bib.bib4), [5](#bib.bib5)]等。除了提升图像感知质量外，它还帮助改善其他计算机视觉任务
    [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。一般而言，这个问题非常具有挑战性，并且本质上是病态的，因为总是存在多个HR图像对应于一个LR图像。在文献中，已经提出了多种经典的SR方法，包括基于预测的方法
    [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)]、基于边缘的方法 [[13](#bib.bib13),
    [14](#bib.bib14)]、统计方法 [[15](#bib.bib15), [16](#bib.bib16)]、基于块的方法 [[13](#bib.bib13),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)] 和稀疏表示方法 [[20](#bib.bib20),
    [21](#bib.bib21)] 等。
- en: 'With the rapid development of deep learning techniques in recent years, deep
    learning based SR models have been actively explored and often achieve the state-of-the-art
    performance on various benchmarks of SR. A variety of deep learning methods have
    been applied to tackle SR tasks, ranging from the early Convolutional Neural Networks
    (CNN) based method (e.g., SRCNN [[22](#bib.bib22), [23](#bib.bib23)]) to recent
    promising SR approaches using Generative Adversarial Nets (GAN) [[24](#bib.bib24)]
    (e.g., SRGAN [[25](#bib.bib25)]). In general, the family of SR algorithms using
    deep learning techniques differ from each other in the following major aspects:
    different types of network architectures [[26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)], different types of loss functions [[29](#bib.bib29), [8](#bib.bib8),
    [30](#bib.bib30)], different types of learning principles and strategies [[31](#bib.bib31),
    [8](#bib.bib8), [32](#bib.bib32)], etc.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着近年来深度学习技术的快速发展，基于深度学习的SR模型已经得到了积极探索，并且在各种SR基准测试中常常达到最先进的性能。各种深度学习方法已被应用于解决SR任务，从早期的卷积神经网络（CNN）方法（例如，SRCNN
    [[22](#bib.bib22), [23](#bib.bib23)]）到最近使用生成对抗网络（GAN） [[24](#bib.bib24)] 的有前途的SR方法（例如，SRGAN
    [[25](#bib.bib25)]）。一般而言，使用深度学习技术的SR算法在以下主要方面彼此不同：不同类型的网络架构 [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]、不同类型的损失函数 [[29](#bib.bib29), [8](#bib.bib8),
    [30](#bib.bib30)]、不同类型的学习原则和策略 [[31](#bib.bib31), [8](#bib.bib8), [32](#bib.bib32)]
    等。
- en: In this paper, we give a comprehensive overview of recent advances in image
    super-resolution with deep learning. Although there are some existing SR surveys
    in literature, our work differs in that we are focused in deep learning based
    SR techniques, while most of the earlier works [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)] aim at surveying traditional SR algorithms
    or some studies mainly concentrate on providing quantitative evaluations based
    on full-reference metrics or human visual perception [[37](#bib.bib37), [38](#bib.bib38)].
    Unlike the existing surveys, this survey takes a unique deep learning based perspective
    to review the recent advances of SR techniques in a systematic and comprehensive
    manner.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对基于深度学习的图像超分辨率的最新进展进行了全面的概述。虽然文献中已有一些现有的SR调查，我们的工作不同之处在于，我们专注于基于深度学习的SR技术，而大多数早期工作
    [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36)] 旨在调查传统的SR算法，或者一些研究主要集中于基于全参考指标或人类视觉感知提供定量评估
    [[37](#bib.bib37), [38](#bib.bib38)]。与现有调查不同，本调查从独特的深度学习视角出发，以系统和全面的方式回顾了SR技术的最新进展。
- en: 'The main contributions of this survey are three-fold:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查的主要贡献有三个方面：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1. '
- en: We give a comprehensive review of image super-resolution techniques based on
    deep learning, including problem settings, benchmark datasets, performance metrics,
    a family of SR methods with deep learning, domain-specific SR applications, etc.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对基于深度学习的图像超分辨率技术进行了全面回顾，包括问题设置、基准数据集、性能指标、深度学习的超分辨率方法家族、领域特定的超分辨率应用等。
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We provide a systematic overview of recent advances of deep learning based SR
    techniques in a hierarchical and structural manner, and summarize the advantages
    and limitations of each component for an effective SR solution.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们以分层和结构化的方式提供了基于深度学习的超分辨率技术的最新进展的系统概述，并总结了每个组件的优缺点，以提供有效的超分辨率解决方案。
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We discuss the challenges and open issues, and identify the new trends and future
    directions to provide an insightful guidance for the community.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了挑战和开放问题，并识别了新的趋势和未来方向，为社区提供有价值的指导。
- en: 'In the following sections, we will cover various aspects of recent advances
    in image super-resolution with deep learning. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning for Image Super-resolution: A Survey") shows the taxonomy of image
    SR to be covered in this survey in a hierarchically-structured way. Section 2
    gives the problem definition and reviews the mainstream datasets and evaluation
    metrics. Section 3 analyzes main components of supervised SR modularly. Section
    4 gives a brief introduction to unsupervised SR methods. Section 5 introduces
    some popular domain-specific SR applications, and Section 6 discusses future directions
    and open issues.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将涵盖深度学习图像超分辨率的最新进展的各个方面。图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 图像超分辨率的深度学习：综述")
    以分层结构的方式展示了本次综述中涵盖的图像超分辨率的分类。第2节给出了问题定义，并回顾了主流数据集和评估指标。第3节模块化分析了监督学习超分辨率的主要组成部分。第4节简要介绍了无监督超分辨率方法。第5节介绍了一些流行的领域特定超分辨率应用，第6节讨论了未来方向和开放问题。
- en: 2 Problem Setting and Terminology
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题设置与术语
- en: 2.1 Problem Definitions
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题定义
- en: 'Image super-resolution aims at recovering the corresponding HR images from
    the LR images. Generally, the LR image $I_{x}$ is modeled as the output of the
    following degradation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率旨在从低分辨率图像恢复出相应的高分辨率图像。通常，低分辨率图像 $I_{x}$ 被建模为以下退化过程的输出：
- en: '|  | $I_{x}=\mathcal{D}(I_{y};\delta),$ |  | (1) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{x}=\mathcal{D}(I_{y};\delta),$ |  | (1) |'
- en: 'where $\mathcal{D}$ denotes a degradation mapping function, $I_{y}$ is the
    corresponding HR image and $\delta$ is the parameters of the degradation process
    (e.g., the scaling factor or noise). Generally, the degradation process (i.e.,
    $\mathcal{D}$ and $\delta$) is unknown and only LR images are provided. In this
    case, also known as blind SR, researchers are required to recover an HR approximation
    $\hat{I_{y}}$ of the ground truth HR image $I_{y}$ from the LR image $I_{x}$,
    following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}$ 表示退化映射函数，$I_{y}$ 是相应的高分辨率图像，$\delta$ 是退化过程的参数（例如，缩放因子或噪声）。通常，退化过程（即
    $\mathcal{D}$ 和 $\delta$）是未知的，只提供低分辨率图像。在这种情况下，也称为盲超分辨率，研究人员需要从低分辨率图像 $I_{x}$
    中恢复高分辨率图像 $I_{y}$ 的近似值 $\hat{I_{y}}$，遵循：
- en: '|  | $\hat{I_{y}}=\mathcal{F}(I_{x};\theta),$ |  | (2) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{I_{y}}=\mathcal{F}(I_{x};\theta),$ |  | (2) |'
- en: where $\mathcal{F}$ is the super-resolution model and $\theta$ denotes the parameters
    of $\mathcal{F}$.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{F}$ 是超分辨率模型，$\theta$ 表示 $\mathcal{F}$ 的参数。
- en: 'Although the degradation process is unknown and can be affected by various
    factors (e.g., compression artifacts, anisotropic degradations, sensor noise and
    speckle noise), researchers are trying to model the degradation mapping. Most
    works directly model the degradation as a single downsampling operation, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管退化过程未知且可能受到各种因素的影响（例如压缩伪影、各向异性退化、传感器噪声和斑点噪声），研究人员正尝试建模退化映射。大多数工作直接将退化建模为单一的下采样操作，如下所示：
- en: '|  | $\mathcal{D}(I_{y};\delta)=(I_{y})\downarrow_{s},\{s\}\subset\delta,$
    |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}(I_{y};\delta)=(I_{y})\downarrow_{s},\{s\}\subset\delta,$
    |  | (3) |'
- en: 'where $\downarrow_{s}$ is a downsampling operation with the scaling factor
    $s$. As a matter of fact, most datasets for generic SR are built based on this
    pattern, and the most commonly used downsampling operation is bicubic interpolation
    with anti-aliasing. However, there are other works [[39](#bib.bib39)] modelling
    the degradation as a combination of several operations:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\downarrow_{s}$ 是具有缩放因子 $s$ 的下采样操作。事实上，大多数通用超分辨率数据集都是基于这种模式构建的，最常用的下采样操作是具有抗混叠的双三次插值。然而，也有其他工作
    [[39](#bib.bib39)] 将退化建模为多个操作的组合：
- en: '|  | $\mathcal{D}(I_{y};\delta)=(I_{y}\otimes\kappa)\downarrow_{s}+n_{\varsigma},\{\kappa,s,\varsigma\}\subset\delta,$
    |  | (4) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}(I_{y};\delta)=(I_{y}\otimes\kappa)\downarrow_{s}+n_{\varsigma},\{\kappa,s,\varsigma\}\subset\delta,$
    |  | (4) |'
- en: 'where $I_{y}\otimes\kappa$ represents the convolution between a blur kernel
    $\kappa$ and the HR image $I_{y}$, and $n_{\varsigma}$ is some additive white
    Gaussian noise with standard deviation $\varsigma$. Compared to the naive definition
    of Eq. [3](#S2.E3 "In 2.1 Problem Definitions ‣ 2 Problem Setting and Terminology
    ‣ Deep Learning for Image Super-resolution: A Survey"), the combinative degradation
    pattern of Eq. [4](#S2.E4 "In 2.1 Problem Definitions ‣ 2 Problem Setting and
    Terminology ‣ Deep Learning for Image Super-resolution: A Survey") is closer to
    real-world cases and has been shown to be more beneficial for SR [[39](#bib.bib39)].'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $I_{y}\otimes\kappa$ 表示模糊核 $\kappa$ 与 HR 图像 $I_{y}$ 之间的卷积，$n_{\varsigma}$
    是具有标准差 $\varsigma$ 的加性白噪声。与 Eq. [3](#S2.E3 "In 2.1 Problem Definitions ‣ 2 Problem
    Setting and Terminology ‣ Deep Learning for Image Super-resolution: A Survey")
    的简单定义相比，Eq. [4](#S2.E4 "In 2.1 Problem Definitions ‣ 2 Problem Setting and Terminology
    ‣ Deep Learning for Image Super-resolution: A Survey") 的组合退化模式更接近真实世界情况，并已被证明对
    SR [[39](#bib.bib39)] 更有益。'
- en: 'To this end, the objective of SR is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，SR 的目标如下：
- en: '|  | $\hat{\theta}=\mathop{\arg\min}_{\theta}\mathcal{L}(\hat{I_{y}},I_{y})+\lambda\Phi(\theta),$
    |  | (5) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\theta}=\mathop{\arg\min}_{\theta}\mathcal{L}(\hat{I_{y}},I_{y})+\lambda\Phi(\theta),$
    |  | (5) |'
- en: 'where $\mathcal{L}(\hat{I_{y}},I_{y})$ represents the loss function between
    the generated HR image $\hat{I_{y}}$ and the ground truth image $I_{y}$, $\Phi(\theta)$
    is the regularization term and $\lambda$ is the tradeoff parameter. Although the
    most popular loss function for SR is pixel-wise mean squared error (i.e., pixel
    loss), more powerful models tend to use a combination of multiple loss functions,
    which will be covered in Sec. [3.4.1](#S3.SS4.SSS1 "3.4.1 Loss Functions ‣ 3.4
    Learning Strategies ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image
    Super-resolution: A Survey").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{L}(\hat{I_{y}},I_{y})$ 表示生成的 HR 图像 $\hat{I_{y}}$ 与真实图像 $I_{y}$
    之间的损失函数，$\Phi(\theta)$ 是正则化项，$\lambda$ 是权衡参数。尽管 SR 最流行的损失函数是逐像素均方误差（即像素损失），但更强大的模型往往使用多种损失函数的组合，这将在
    Sec. [3.4.1](#S3.SS4.SSS1 "3.4.1 Loss Functions ‣ 3.4 Learning Strategies ‣ 3
    Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")
    中介绍。'
- en: 2.2 Datasets for Super-resolution
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 超分辨率的数据集
- en: 'TABLE I: List of public image datasets for super-resolution benchmarks.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 用于超分辨率基准测试的公开图像数据集列表。'
- en: '| Dataset | Amount | Avg. Resolution | Avg. Pixels | Format | Category Keywords
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数量 | 平均分辨率 | 平均像素 | 格式 | 类别关键词 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| BSDS300 [[40](#bib.bib40)] | $300$ | $(435,367)$ | $154,401$ | JPG | animal,
    building, food, landscape, people, plant, etc. |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| BSDS300 [[40](#bib.bib40)] | $300$ | $(435,367)$ | $154,401$ | JPG | 动物，建筑物，食物，风景，人，植物等
    |'
- en: '| BSDS500 [[41](#bib.bib41)] | $500$ | $(432,370)$ | $154,401$ | JPG | animal,
    building, food, landscape, people, plant, etc. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| BSDS500 [[41](#bib.bib41)] | $500$ | $(432,370)$ | $154,401$ | JPG | 动物，建筑物，食物，风景，人，植物等
    |'
- en: '| DIV2K [[42](#bib.bib42)] | $1000$ | $(1972,1437)$ | $2,793,250$ | PNG | environment,
    flora, fauna, handmade object, people, scenery, etc. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| DIV2K [[42](#bib.bib42)] | $1000$ | $(1972,1437)$ | $2,793,250$ | PNG | 环境，植物，动物，手工物品，人，风景等
    |'
- en: '| General-100 [[43](#bib.bib43)] | $100$ | $(435,381)$ | $181,108$ | BMP |
    animal, daily necessity, food, people, plant, texture, etc. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| General-100 [[43](#bib.bib43)] | $100$ | $(435,381)$ | $181,108$ | BMP |
    动物，日常用品，食物，人，植物，纹理等 |'
- en: '| L20 [[44](#bib.bib44)] | $20$ | $(3843,2870)$ | $11,577,492$ | PNG | animal,
    building, landscape, people, plant, etc. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| L20 [[44](#bib.bib44)] | $20$ | $(3843,2870)$ | $11,577,492$ | PNG | 动物，建筑物，风景，人，植物等
    |'
- en: '| Manga109 [[45](#bib.bib45)] | $109$ | $(826,1169)$ | $966,011$ | PNG | manga
    volume |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Manga109 [[45](#bib.bib45)] | $109$ | $(826,1169)$ | $966,011$ | PNG | 漫画卷
    |'
- en: '| OutdoorScene [[46](#bib.bib46)] | $10624$ | $(553,440)$ | $249,593$ | PNG
    | animal, building, grass, mountain, plant, sky, water |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| OutdoorScene [[46](#bib.bib46)] | $10624$ | $(553,440)$ | $249,593$ | PNG
    | 动物，建筑物，草地，山脉，植物，天空，水 |'
- en: '| PIRM [[47](#bib.bib47)] | $200$ | $(617,482)$ | $292,021$ | PNG | environments,
    flora, natural scenery, objects, people, etc. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| PIRM [[47](#bib.bib47)] | $200$ | $(617,482)$ | $292,021$ | PNG | 环境，植物，自然风光，物体，人等
    |'
- en: '| Set5 [[48](#bib.bib48)] | $5$ | $(313,336)$ | $113,491$ | PNG | baby, bird,
    butterfly, head, woman |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| Set5 [[48](#bib.bib48)] | $5$ | $(313,336)$ | $113,491$ | PNG | 婴儿，鸟，蝴蝶，头像，女人
    |'
- en: '| Set14 [[49](#bib.bib49)] | $14$ | $(492,446)$ | $230,203$ | PNG | humans,
    animals, insects, flowers, vegetables, comic, slides, etc. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| Set14 [[49](#bib.bib49)] | $14$ | $(492,446)$ | $230,203$ | PNG | 人类、动物、昆虫、花卉、蔬菜、漫画、幻灯片等
    |'
- en: '| T91 [[21](#bib.bib21)] | $91$ | $(264,204)$ | $58,853$ | PNG | car, flower,
    fruit, human face, etc. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| T91 [[21](#bib.bib21)] | $91$ | $(264,204)$ | $58,853$ | PNG | 汽车、花卉、水果、人脸等
    |'
- en: '| Urban100 [[50](#bib.bib50)] | $100$ | $(984,797)$ | $774,314$ | PNG | architecture,
    city, structure, urban, etc. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| Urban100 [[50](#bib.bib50)] | $100$ | $(984,797)$ | $774,314$ | PNG | 建筑、城市、结构、城市等
    |'
- en: 'Today there are a variety of datasets available for image super-resolution,
    which greatly differ in image amounts, quality, resolution, and diversity, etc.
    Some of them provide LR-HR image pairs, while others only provide HR images, in
    which case the LR images are typically obtained by imresize function with default
    settings in MATLAB (i.e., bicubic interpolation with anti-aliasing). In Table
    [I](#S2.T1 "TABLE I ‣ 2.2 Datasets for Super-resolution ‣ 2 Problem Setting and
    Terminology ‣ Deep Learning for Image Super-resolution: A Survey") we list a number
    of image datasets commonly used by the SR community, and specifically indicate
    their amounts of HR images, average resolution, average numbers of pixels, image
    formats, and category keywords.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '目前有多种数据集可用于图像超分辨率，这些数据集在图像数量、质量、分辨率和多样性等方面有很大差异。它们中的一些提供 LR-HR 图像对，而其他则只提供
    HR 图像，在这种情况下，LR 图像通常通过 MATLAB 中的 imresize 函数使用默认设置（即带有抗锯齿的双三次插值）获得。在表 [I](#S2.T1
    "TABLE I ‣ 2.2 Datasets for Super-resolution ‣ 2 Problem Setting and Terminology
    ‣ Deep Learning for Image Super-resolution: A Survey") 中，我们列出了 SR 社区常用的图像数据集，并具体说明了它们的
    HR 图像数量、平均分辨率、平均像素数量、图像格式和类别关键词。'
- en: Besides these datasets, some datasets widely used for other vision tasks are
    also employed for SR, such as ImageNet [[51](#bib.bib51)], MS-COCO [[52](#bib.bib52)],
    VOC2012 [[53](#bib.bib53)], CelebA [[54](#bib.bib54)]. In addition, combining
    multiple datasets for training is also popular, such as combining T91 and BSDS300
    [[26](#bib.bib26), [55](#bib.bib55), [27](#bib.bib27), [56](#bib.bib56)], combining
    DIV2K and Flickr2K [[31](#bib.bib31), [57](#bib.bib57)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些数据集，还有一些广泛用于其他视觉任务的数据集也被用于超分辨率（SR），如 ImageNet [[51](#bib.bib51)]、MS-COCO
    [[52](#bib.bib52)]、VOC2012 [[53](#bib.bib53)]、CelebA [[54](#bib.bib54)]。此外，结合多个数据集进行训练也很流行，例如将
    T91 和 BSDS300 [[26](#bib.bib26), [55](#bib.bib55), [27](#bib.bib27), [56](#bib.bib56)]
    结合在一起，或者将 DIV2K 和 Flickr2K [[31](#bib.bib31), [57](#bib.bib57)] 结合在一起。
- en: 2.3 Image Quality Assessment
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 图像质量评估
- en: Image quality refers to visual attributes of images and focuses on the perceptual
    assessments of viewers. In general, image quality assessment (IQA) methods include
    subjective methods based on humans’ perception (i.e., how realistic the image
    looks) and objective computational methods. The former is more in line with our
    need but often time-consuming and expensive, thus the latter is currently the
    mainstream. However, these methods aren’t necessarily consistent between each
    other, because objective methods are often unable to capture the human visual
    perception very accurately, which may lead to large difference in IQA results
    [[58](#bib.bib58), [25](#bib.bib25)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图像质量指的是图像的视觉属性，并关注观众的感知评估。一般来说，图像质量评估（IQA）方法包括基于人类感知的主观方法（即图像看起来有多逼真）和客观计算方法。前者更符合我们的需求，但往往耗时且昂贵，因此后者目前是主流。然而，这些方法之间不一定一致，因为客观方法往往不能非常准确地捕捉人类视觉感知，这可能导致IQA结果的巨大差异
    [[58](#bib.bib58), [25](#bib.bib25)]。
- en: 'In addition, the objective IQA methods are further divided into three types
    [[58](#bib.bib58)]: full-reference methods performing assessment using reference
    images, reduced-reference methods based on comparisons of extracted features,
    and no-reference methods (i.e., blind IQA) without any reference images. Next
    we’ll introduce several most commonly used IQA methods covering both subjective
    methods and objective methods.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，客观图像质量评估（IQA）方法进一步分为三类 [[58](#bib.bib58)]：全参考方法使用参考图像进行评估、基于提取特征比较的减参考方法，以及没有任何参考图像的无参考方法（即盲IQA）。接下来我们将介绍几种最常用的IQA方法，包括主观方法和客观方法。
- en: 2.3.1 Peak Signal-to-Noise Ratio
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 峰值信噪比
- en: 'Peak signal-to-noise ratio (PSNR) is one of the most popular reconstruction
    quality measurement of lossy transformation (e.g., image compression, image inpainting).
    For image super-resolution, PSNR is defined via the maximum pixel value (denoted
    as $L$) and the mean squared error (MSE) between images. Given the ground truth
    image $I$ with $N$ pixels and the reconstruction $\hat{I}$, the PSNR between $I$
    and $\hat{I}$ are defined as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 峰值信噪比（PSNR）是损失变换（例如图像压缩、图像修复）中最流行的重建质量测量之一。对于图像超分辨率，PSNR通过最大像素值（记为$L$）和图像之间的均方误差（MSE）来定义。给定真实图像$I$和重建图像$\hat{I}$，$I$和$\hat{I}$之间的PSNR定义如下：
- en: '|  | $\displaystyle{\rm PSNR}$ | $\displaystyle=10\cdot\log_{10}(\frac{L^{2}}{\frac{1}{N}\sum_{i=1}^{N}(I(i)-\hat{I}(i))^{2}}),$
    |  | (6) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\rm PSNR}$ | $\displaystyle=10\cdot\log_{10}(\frac{L^{2}}{\frac{1}{N}\sum_{i=1}^{N}(I(i)-\hat{I}(i))^{2}}),$
    |  | (6) |'
- en: where $L$ equals to $255$ in general cases using 8-bit representations. Since
    the PSNR is only related to the pixel-level MSE, only caring about the differences
    between corresponding pixels instead of visual perception, it often leads to poor
    performance in representing the reconstruction quality in real scenes, where we’re
    usually more concerned with human perceptions. However, due to the necessity to
    compare with literature works and the lack of completely accurate perceptual metrics,
    PSNR is still currently the most widely used evaluation criteria for SR models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$L$在一般情况下使用8位表示时等于$255$。由于PSNR仅与像素级MSE相关，只关注对应像素之间的差异，而不是视觉感知，它往往在表示实际场景中的重建质量时表现不佳，而我们通常更关心人类感知。然而，由于与文献工作的比较必要性以及缺乏完全准确的感知度量，PSNR仍然是目前最广泛使用的超分辨率模型评估标准。
- en: 2.3.2 Structural Similarity
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 结构相似性
- en: 'Considering that the human visual system (HVS) is highly adapted to extract
    image structures [[59](#bib.bib59)], the structural similarity index (SSIM) [[58](#bib.bib58)]
    is proposed for measuring the structural similarity between images, based on independent
    comparisons in terms of luminance, contrast, and structures. For an image $I$
    with $N$ pixels, the luminance $\mu_{I}$ and contrast $\sigma_{I}$ are estimated
    as the mean and standard deviation of the image intensity, respectively, i.e.,
    $\mu_{I}=\frac{1}{N}\sum_{i=1}^{N}I(i)$ and $\sigma_{I}=(\frac{1}{N-1}\sum_{i=1}^{N}(I(i)-\mu_{I})^{2})^{\frac{1}{2}}$,
    where $I(i)$ represents the intensity of the $i$-th pixel of image $I$. And the
    comparisons on luminance and contrast, denoted as $\mathcal{C}_{l}(I,\hat{I})$
    and $\mathcal{C}_{c}(I,\hat{I})$ respectively, are given by:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到人类视觉系统（HVS）高度适应于提取图像结构[[59](#bib.bib59)]，提出了结构相似性指数（SSIM）[[58](#bib.bib58)]，用于测量图像之间的结构相似性，基于亮度、对比度和结构的独立比较。对于一个具有$N$个像素的图像$I$，亮度$\mu_{I}$和对比度$\sigma_{I}$分别估计为图像强度的均值和标准差，即$\mu_{I}=\frac{1}{N}\sum_{i=1}^{N}I(i)$和$\sigma_{I}=(\frac{1}{N-1}\sum_{i=1}^{N}(I(i)-\mu_{I})^{2})^{\frac{1}{2}}$，其中$I(i)$表示图像$I$的第$i$个像素的强度。亮度和对比度的比较，分别记为$\mathcal{C}_{l}(I,\hat{I})$和$\mathcal{C}_{c}(I,\hat{I})$，给出如下：
- en: '|  | $\displaystyle\mathcal{C}_{l}(I,\hat{I})$ | $\displaystyle=\frac{2\mu_{I}\mu_{\hat{I}}+C_{1}}{\mu_{I}^{2}+\mu_{\hat{I}}^{2}+C_{1}},$
    |  | (7) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{l}(I,\hat{I})$ | $\displaystyle=\frac{2\mu_{I}\mu_{\hat{I}}+C_{1}}{\mu_{I}^{2}+\mu_{\hat{I}}^{2}+C_{1}},$
    |  | (7) |'
- en: '|  | $\displaystyle\mathcal{C}_{c}(I,\hat{I})$ | $\displaystyle=\frac{2\sigma_{I}\sigma_{\hat{I}}+C_{2}}{\sigma_{I}^{2}+\sigma_{\hat{I}}^{2}+C_{2}},$
    |  | (8) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{c}(I,\hat{I})$ | $\displaystyle=\frac{2\sigma_{I}\sigma_{\hat{I}}+C_{2}}{\sigma_{I}^{2}+\sigma_{\hat{I}}^{2}+C_{2}},$
    |  | (8) |'
- en: where $C_{1}=(k_{1}L)^{2}$ and $C_{2}=(k_{2}L)^{2}$ are constants for avoiding
    instability, $k_{1}\ll 1$ and $k_{2}\ll 1$.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$C_{1}=(k_{1}L)^{2}$和$C_{2}=(k_{2}L)^{2}$是为了避免不稳定性而设置的常数，$k_{1}\ll 1$和$k_{2}\ll
    1$。
- en: 'Besides, the image structure is represented by the normalized pixel values
    (i.e., $(I-\mu_{I})/\sigma_{I}$), whose correlations (i.e., inner product) measure
    the structural similarity, equivalent to the correlation coefficient between $I$
    and $\hat{I}$. Thus the structure comparison function $\mathcal{C}_{s}(I,\hat{I})$
    is defined as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图像结构由归一化的像素值（即$(I-\mu_{I})/\sigma_{I}$）表示，其相关性（即内积）测量结构相似性，相当于$I$和$\hat{I}$之间的相关系数。因此，结构比较函数$\mathcal{C}_{s}(I,\hat{I})$定义为：
- en: '|  | $\displaystyle\sigma_{I\hat{I}}$ | $\displaystyle=\frac{1}{N-1}\sum_{i=1}^{N}(I(i)-\mu_{I})(\hat{I}(i)-\mu_{\hat{I}}),$
    |  | (9) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sigma_{I\hat{I}}$ | $\displaystyle=\frac{1}{N-1}\sum_{i=1}^{N}(I(i)-\mu_{I})(\hat{I}(i)-\mu_{\hat{I}}),$
    |  | (9) |'
- en: '|  | $\displaystyle\mathcal{C}_{s}(I,\hat{I})$ | $\displaystyle=\frac{\sigma_{I\hat{I}}+C_{3}}{\sigma_{I}\sigma_{\hat{I}}+C_{3}},$
    |  | (10) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{s}(I,\hat{I})$ | $\displaystyle=\frac{\sigma_{I\hat{I}}+C_{3}}{\sigma_{I}\sigma_{\hat{I}}+C_{3}},$
    |  | (10) |'
- en: where $\sigma_{I,\hat{I}}$ is the covariance between $I$ and $\hat{I}$, and
    $C_{3}$ is a constant for stability.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\sigma_{I,\hat{I}}$是$I$和$\hat{I}$之间的协方差，$C_{3}$是用于稳定性的常数。
- en: 'Finally, the SSIM is given by:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，SSIM定义为：
- en: '|  | ${\rm SSIM}(I,\hat{I})=[\mathcal{C}_{l}(I,\hat{I})]^{\alpha}[\mathcal{C}_{c}(I,\hat{I})]^{\beta}[\mathcal{C}_{s}(I,\hat{I})]^{\gamma},$
    |  | (11) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\rm SSIM}(I,\hat{I})=[\mathcal{C}_{l}(I,\hat{I})]^{\alpha}[\mathcal{C}_{c}(I,\hat{I})]^{\beta}[\mathcal{C}_{s}(I,\hat{I})]^{\gamma},$
    |  | (11) |'
- en: where $\alpha$, $\beta$, $\gamma$ are control parameters for adjusting the relative
    importance.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\alpha$、$\beta$、$\gamma$是用于调整相对重要性的控制参数。
- en: Since the SSIM evaluates the reconstruction quality from the perspective of
    the HVS, it better meets the requirements of perceptual assessment [[60](#bib.bib60),
    [61](#bib.bib61)], and is also widely used.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SSIM从HVS的角度评估重建质量，它更好地满足了感知评估的要求[[60](#bib.bib60)，[61](#bib.bib61)]，并且被广泛使用。
- en: 2.3.3 Mean Opinion Score
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 平均意见分数
- en: Mean opinion score (MOS) testing is a commonly used subjective IQA method, where
    human raters are asked to assign perceptual quality scores to tested images. Typically,
    the scores are from $1$ (bad) to $5$ (good). And the final MOS is calculated as
    the arithmetic mean over all ratings.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 平均意见分数（MOS）测试是一种常用的主观图像质量评估方法，其中要求人类评分者为测试图像分配感知质量分数。通常，评分范围为$1$（差）到$5$（好）。最终的MOS是所有评分的算术平均值。
- en: Although the MOS testing seems a faithful IQA method, it has some inherent defects,
    such as non-linearly perceived scales, biases and variance of rating criteria.
    In reality, there are some SR models performing poorly in common IQA metrics (e.g.,
    PSNR) but far exceeding others in terms of perceptual quality, in which case the
    MOS testing is the most reliable IQA method for accurately measuring the perceptual
    quality [[62](#bib.bib62), [25](#bib.bib25), [8](#bib.bib8), [63](#bib.bib63),
    [64](#bib.bib64), [46](#bib.bib46), [65](#bib.bib65)].
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MOS测试看似是一个可靠的图像质量评估方法，但它存在一些固有缺陷，例如非线性感知尺度、评分标准的偏差和方差。实际上，一些在常见图像质量评估指标（例如，PSNR）上表现较差的SR模型，在感知质量方面却远超其他模型，在这种情况下，MOS测试是准确测量感知质量的最可靠方法[[62](#bib.bib62)，[25](#bib.bib25)，[8](#bib.bib8)，[63](#bib.bib63)，[64](#bib.bib64)，[46](#bib.bib46)，[65](#bib.bib65)]。
- en: 2.3.4 Learning-based Perceptual Quality
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 基于学习的感知质量
- en: In order to better assess the image perceptual quality while reducing manual
    intervention, researchers try to assess the perceptual quality by learning on
    large datasets. Specifically, Ma et al. [[66](#bib.bib66)] and Talebi et al. [[67](#bib.bib67)]
    propose no-reference Ma and NIMA, respectively, which are learned from visual
    perceptual scores and directly predict the quality scores without ground-truth
    images. In contrast, Kim et al. [[68](#bib.bib68)] propose DeepQA, which predicts
    visual similarity of images by training on triplets of distorted images, objective
    error maps, and subjective scores. And Zhang et al. [[69](#bib.bib69)] collect
    a large-scale perceptual similarity dataset, evaluate the perceptual image patch
    similarity (LPIPS) according to the difference in deep features by trained deep
    networks, and show that the deep features learned by CNNs model perceptual similarity
    much better than measures without CNNs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地评估图像感知质量并减少人工干预，研究人员尝试通过在大规模数据集上进行学习来评估感知质量。具体来说，Ma等人[[66](#bib.bib66)]和Talebi等人[[67](#bib.bib67)]分别提出了无参考的Ma和NIMA，这些方法从视觉感知评分中学习，并直接预测质量评分，而不需要真实图像。相比之下，Kim等人[[68](#bib.bib68)]提出了DeepQA，通过在失真图像、客观误差图和主观评分的三元组上进行训练来预测图像的视觉相似性。而Zhang等人[[69](#bib.bib69)]收集了大规模的感知相似性数据集，根据训练过的深度网络的深度特征差异来评估感知图像补丁相似性（LPIPS），并展示了CNN学习到的深度特征在感知相似性上的表现远优于没有CNN的测量方法。
- en: Although these methods exhibit better performance on capturing human visual
    perception, what kind of perceptual quality we need (e.g., more realistic images,
    or consistent identity to the original image) remains a question to be explored,
    thus the objective IQA methods (e.g., PSNR, SSIM) are still the mainstreams currently.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法在捕捉人类视觉感知方面表现更佳，但我们需要什么样的感知质量（例如，更现实的图像，还是与原始图像一致的身份）仍然是一个待探索的问题，因此客观图像质量评估方法（例如，PSNR、SSIM）仍然是目前的主流。
- en: 2.3.5 Task-based Evaluation
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.5 基于任务的评估
- en: According to the fact that SR models can often help other vision tasks [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)], evaluating reconstruction performance
    by means of other tasks is another effective way. Specifically, researchers feed
    the original and the reconstructed HR images into trained models, and evaluate
    the reconstruction quality by comparing the impacts on the prediction performance.
    The vision tasks used for evaluation include object recognition [[8](#bib.bib8),
    [70](#bib.bib70)], face recognition [[71](#bib.bib71), [72](#bib.bib72)], face
    alignment and parsing [[30](#bib.bib30), [73](#bib.bib73)], etc.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 根据SR模型通常能帮助其他视觉任务的事实[[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]，通过其他任务评估重建性能是另一种有效的方法。具体来说，研究人员将原始图像和重建后的高分辨率图像输入经过训练的模型，通过比较对预测性能的影响来评估重建质量。用于评估的视觉任务包括物体识别[[8](#bib.bib8),
    [70](#bib.bib70)]、人脸识别[[71](#bib.bib71), [72](#bib.bib72)]、人脸对齐和解析[[30](#bib.bib30),
    [73](#bib.bib73)]等。
- en: 2.3.6 Other IQA Methods
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.6 其他图像质量评估方法
- en: In addition to above IQA methods, there are other less popular SR metrics. The
    multi-scale structural similarity (MS-SSIM) [[74](#bib.bib74)] supplies more flexibility
    than single-scale SSIM in incorporating the variations of viewing conditions.
    The feature similarity (FSIM) [[75](#bib.bib75)] extracts feature points of human
    interest based on phase congruency and image gradient magnitude to evaluate image
    quality. The Natural Image Quality Evaluator (NIQE) [[76](#bib.bib76)] makes use
    of measurable deviations from statistical regularities observed in natural images,
    without exposure to distorted images.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述的图像质量评估方法外，还有其他不那么流行的超分辨率度量指标。多尺度结构相似性（MS-SSIM）[[74](#bib.bib74)]比单尺度SSIM在结合视角变化方面提供了更多的灵活性。特征相似性（FSIM）[[75](#bib.bib75)]基于相位一致性和图像梯度幅值提取人类感兴趣的特征点来评估图像质量。自然图像质量评估器（NIQE）[[76](#bib.bib76)]利用自然图像中观察到的统计规律的可测偏差，而不需要接触失真图像。
- en: Recently, Blau et al. [[77](#bib.bib77)] prove mathematically that distortion
    (e.g., PSNR, SSIM) and perceptual quality (e.g., MOS) are at odds with each other,
    and show that as the distortion decreases, the perceptual quality must be worse.
    Thus how to accurately measure the SR quality is still an urgent problem to be
    solved.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Blau等人[[77](#bib.bib77)]在数学上证明了失真（例如PSNR、SSIM）和感知质量（例如MOS）相互矛盾，并展示了当失真减少时，感知质量必然会变差。因此，如何准确测量超分辨率质量仍然是一个亟待解决的问题。
- en: 2.4 Operating Channels
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 操作通道
- en: In addition to the commonly used RGB color space, the YCbCr color space is also
    widely used for SR. In this space, images are represented by Y, Cb, Cr channels,
    denoting the luminance, blue-difference and red-difference chroma components,
    respectively. Although currently there is no accepted best practice for performing
    or evaluating super-resolution on which space, earlier models favor operating
    on the Y channel of YCbCr space [[26](#bib.bib26), [43](#bib.bib43), [78](#bib.bib78),
    [79](#bib.bib79)], while more recent models tend to operate on RGB channels [[31](#bib.bib31),
    [57](#bib.bib57), [28](#bib.bib28), [70](#bib.bib70)]. It is worth noting that
    operating (training or evaluation) on different color spaces or channels can make
    the evaluation results differ greatly (up to $4$ dB) [[23](#bib.bib23)].
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了常用的RGB颜色空间外，YCbCr颜色空间在超分辨率中也被广泛使用。在这个空间中，图像由Y、Cb、Cr通道表示，分别表示亮度、蓝色差异和红色差异色度分量。虽然目前对于在哪个空间上进行或评估超分辨率没有公认的最佳实践，早期模型倾向于在YCbCr空间的Y通道上进行操作[[26](#bib.bib26),
    [43](#bib.bib43), [78](#bib.bib78), [79](#bib.bib79)]，而较新的模型则倾向于在RGB通道上进行操作[[31](#bib.bib31),
    [57](#bib.bib57), [28](#bib.bib28), [70](#bib.bib70)]。值得注意的是，在不同颜色空间或通道上进行操作（训练或评估）可能会使评估结果大相径庭（高达$4$
    dB）[[23](#bib.bib23)]。
- en: 2.5 Super-resolution Challenges
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 超分辨率挑战
- en: In this section, we will briefly introduce two most popular challenges for image
    SR, NTIRE [[80](#bib.bib80)] and PIRM [[47](#bib.bib47), [81](#bib.bib81)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将简要介绍两个最流行的图像超分辨率挑战：NTIRE[[80](#bib.bib80)]和PIRM[[47](#bib.bib47), [81](#bib.bib81)]。
- en: NTIRE Challenge. The New Trends in Image Restoration and Enhancement (NTIRE)
    challenge [[80](#bib.bib80)] is in conjunction with CVPR and includes multiple
    tasks like SR, denoising and colorization. For image SR, the NTIRE challenge is
    built on the DIV2K [[42](#bib.bib42)] dataset and consists of bicubic downscaling
    tracks and blind tracks with realistic unknown degradation. These tracks differs
    in degradations and scaling factors, and aim to promote the SR research under
    both ideal conditions and real-world adverse situations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: NTIRE 挑战。图像恢复和增强的新趋势 (NTIRE) 挑战 [[80](#bib.bib80)] 与 CVPR 合作，包括超分辨率、去噪和着色等多个任务。对于图像超分辨率，NTIRE
    挑战建立在 DIV2K [[42](#bib.bib42)] 数据集上，并包括双三次降采样轨道和带有真实未知降级的盲轨道。这些轨道在降级和缩放因子上有所不同，旨在促进理想条件下和现实世界不利情况下的超分辨率研究。
- en: PIRM Challenge. The Perceptual Image Restoration and Manipulation (PIRM) challenges
    are in conjunction with ECCV and also includes multiple tasks. In contrast to
    NTIRE, one sub-challenge [[47](#bib.bib47)] of PIRM focuses on the tradeoff between
    generation accuracy and perceptual quality, and the other [[81](#bib.bib81)] focuses
    on SR on smartphones. As is well-known [[77](#bib.bib77)], the models target for
    distortion frequently produce visually unpleasing results, while the models target
    for perceptual quality performs poorly on information fidelity. Specifically,
    the PIRM divided the perception-distortion plane into three regions according
    to thresholds on root mean squared error (RMSE). In each region, the winning algorithm
    is the one that achieves the best perceptual quality [[77](#bib.bib77)], evaluated
    by NIQE [[76](#bib.bib76)] and Ma [[66](#bib.bib66)]. While in the other sub-challenge
    [[81](#bib.bib81)], SR on smartphones, participants are asked to perform SR with
    limited smartphone hardwares (including CPU, GPU, RAM, etc.), and the evaluation
    metrics include PSNR, MS-SSIM and MOS testing. In this way, PIRM encourages advanced
    research on the perception-distortion tradeoff, and also drives lightweight and
    efficient image enhancement on smartphones.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: PIRM 挑战。感知图像恢复和处理 (PIRM) 挑战与 ECCV 合作，也包括多个任务。与 NTIRE 相反，PIRM 的一个子挑战 [[47](#bib.bib47)]
    关注生成精度和感知质量之间的权衡，另一个子挑战 [[81](#bib.bib81)] 关注智能手机上的超分辨率。众所周知 [[77](#bib.bib77)]，针对失真的模型经常产生视觉上不令人满意的结果，而针对感知质量的模型在信息保真度上表现不佳。具体来说，PIRM
    根据均方根误差 (RMSE) 的阈值将感知-失真平面分为三个区域。在每个区域中，通过 NIQE [[76](#bib.bib76)] 和 Ma [[66](#bib.bib66)]
    进行评估，最佳的算法是获得最佳感知质量的算法 [[77](#bib.bib77)]。而在另一个子挑战 [[81](#bib.bib81)] 中，智能手机上的超分辨率，参与者被要求在有限的智能手机硬件（包括
    CPU、GPU、RAM 等）上执行超分辨率，评估指标包括 PSNR、MS-SSIM 和 MOS 测试。通过这种方式，PIRM 鼓励对感知-失真权衡的高级研究，也推动了在智能手机上轻量、高效的图像增强。
- en: 3 Supervised Super-resolution
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 监督式超分辨率
- en: 'Nowadays researchers have proposed a variety of super-resolution models with
    deep learning. These models focus on supervised SR, i.e., trained with both LR
    images and corresponding HR images. Although the differences between these models
    are very large, they are essentially some combinations of a set of components
    such as model frameworks, upsampling methods, network design, and learning strategies.
    From this perspective, researchers combine these components to build an integrated
    SR model for fitting specific purposes. In this section, we concentrate on modularly
    analyzing the fundamental components (as Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Deep Learning for Image Super-resolution: A Survey") shows) instead of introducing
    each model in isolation, and summarizing their advantages and limitations.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '如今，研究人员提出了各种深度学习的超分辨率模型。这些模型专注于监督式超分辨率，即通过低分辨率图像和相应的高分辨率图像进行训练。尽管这些模型之间的差别很大，但它们本质上是一些模型框架、上采样方法、网络设计和学习策略的组合。从这个角度来看，研究人员将这些组件组合起来构建一个整合的超分辨率模型，以适应特定的目的。在这一部分，我们集中模块化地分析基本组件（如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning for Image Super-resolution:
    A Survey") 所示），而不是单独介绍每个模型，并总结它们的优势和局限性。'
- en: 3.1 Super-resolution Frameworks
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 超分辨率框架
- en: '![Refer to caption](img/796daecf027d5d88ee5e48d0f902ff7c.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/796daecf027d5d88ee5e48d0f902ff7c.png)'
- en: (a) Pre-upsampling SR
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预上采样 SR
- en: '![Refer to caption](img/1860e871db5595a033a6deffc3d1e39c.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1860e871db5595a033a6deffc3d1e39c.png)'
- en: (b) Post-upsampling SR
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 后上采样 SR
- en: '![Refer to caption](img/32d6bfdf8b04b655bdd186135c9d9103.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/32d6bfdf8b04b655bdd186135c9d9103.png)'
- en: (c) Progressive upsampling SR
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 渐进式上采样 SR
- en: '![Refer to caption](img/6404b5db1f576215fbf1d0a3570c7823.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6404b5db1f576215fbf1d0a3570c7823.png)'
- en: (d) Iterative up-and-down Sampling SR
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 迭代上下采样超分辨率
- en: 'Figure 2: Super-resolution model frameworks based on deep learning. The cube
    size represents the output size. The gray ones denote predefined upsampling, while
    the green, yellow and blue ones indicate learnable upsampling, downsampling and
    convolutional layers, respectively. And the blocks enclosed by dashed boxes represent
    stackable modules.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的超分辨率模型框架。立方体的大小表示输出大小。灰色表示预定义上采样，而绿色、黄色和蓝色分别表示可学习的上采样、下采样和卷积层。被虚线框包围的模块表示可堆叠模块。
- en: 'Since image super-resolution is an ill-posed problem, how to perform upsampling
    (i.e., generating HR output from LR input) is the key problem. Although the architectures
    of existing models vary widely, they can be attributed to four model frameworks
    (as Fig. [2](#S3.F2 "Figure 2 ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey") shows),
    based on the employed upsampling operations and their locations in the model.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像超分辨率是一个不适定问题，如何执行上采样（即，从低分辨率输入生成高分辨率输出）是关键问题。尽管现有模型的架构差异很大，但它们可以归纳为四种模型框架（如图[2](#S3.F2
    "图2 ‣ 3.1 超分辨率框架 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")所示），这些框架基于所采用的上采样操作及其在模型中的位置。
- en: 3.1.1 Pre-upsampling Super-resolution
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 预处理上采样超分辨率
- en: 'On account of the difficulty of directly learning the mapping from low-dimensional
    space to high-dimensional space, utilizing traditional upsampling algorithms to
    obtain higher-resolution images and then refining them using deep neural networks
    is a straightforward solution. Thus Dong et al. [[22](#bib.bib22), [23](#bib.bib23)]
    firstly adopt the pre-upsampling SR framework (as Fig. [2a](#S3.F2.sf1 "In Figure
    2 ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") shows) and propose SRCNN to learn an end-to-end
    mapping from interpolated LR images to HR images. Specifically, the LR images
    are upsampled to coarse HR images with the desired size using traditional methods
    (e.g., bicubic interpolation), then deep CNNs are applied on these images for
    reconstructing high-quality details.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于直接学习从低维空间到高维空间的映射具有困难，利用传统上采样算法获取更高分辨率的图像，然后使用深度神经网络对其进行精细化处理是一种简单的解决方案。因此，Dong
    等人[[22](#bib.bib22), [23](#bib.bib23)] 首先采用预处理上采样超分辨率框架（如图[2a](#S3.F2.sf1 "图2
    ‣ 3.1 超分辨率框架 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")所示），并提出 SRCNN 以学习从插值的低分辨率图像到高分辨率图像的端到端映射。具体来说，使用传统方法（例如，双三次插值）将低分辨率图像上采样到具有所需大小的粗略高分辨率图像，然后在这些图像上应用深度
    CNNs 以重建高质量的细节。
- en: 'Since the most difficult upsampling operation has been completed, CNNs only
    need to refine the coarse images, which significantly reduces the learning difficulty.
    In addition, these models can take interpolated images with arbitrary sizes and
    scaling factors as input, and give refined results with comparable performance
    to single-scale SR models [[26](#bib.bib26)]. Thus it has gradually become one
    of the most popular frameworks [[82](#bib.bib82), [56](#bib.bib56), [55](#bib.bib55),
    [83](#bib.bib83)], and the main differences between these models are the posterior
    model design (Sec. [3.3](#S3.SS3 "3.3 Network Design ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")) and learning strategies
    (Sec. [3.4](#S3.SS4 "3.4 Learning Strategies ‣ 3 Supervised Super-resolution ‣
    Deep Learning for Image Super-resolution: A Survey")). However, the predefined
    upsampling often introduce side effects (e.g., noise amplification and blurring),
    and since most operations are performed in high-dimensional space, the cost of
    time and space is much higher than other frameworks [[84](#bib.bib84), [43](#bib.bib43)].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于最困难的上采样操作已完成，CNNs 只需对粗略图像进行精细化处理，这显著降低了学习难度。此外，这些模型可以接受任意尺寸和缩放因子的插值图像作为输入，并给出与单尺度超分辨率模型性能相当的精细结果[[26](#bib.bib26)]。因此，它逐渐成为最受欢迎的框架之一[[82](#bib.bib82),
    [56](#bib.bib56), [55](#bib.bib55), [83](#bib.bib83)]，这些模型之间的主要区别在于后续模型设计（见[3.3节](#S3.SS3
    "3.3 网络设计 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")）和学习策略（见[3.4节](#S3.SS4 "3.4 学习策略 ‣ 3 监督式超分辨率
    ‣ 图像超分辨率的深度学习：综述")）。然而，预定义的上采样通常会引入副作用（例如，噪声放大和模糊），并且由于大多数操作在高维空间中执行，因此时间和空间成本远高于其他框架[[84](#bib.bib84),
    [43](#bib.bib43)]。
- en: 3.1.2 Post-upsampling Super-resolution
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 后上采样超分辨率
- en: 'In order to improve the computational efficiency and make full use of deep
    learning technology to increase resolution automatically, researchers propose
    to perform most computation in low-dimensional space by replacing the predefined
    upsampling with end-to-end learnable layers integrated at the end of the models.
    In the pioneer works [[84](#bib.bib84), [43](#bib.bib43)] of this framework, namely
    post-upsampling SR as Fig. [2b](#S3.F2.sf2 "In Figure 2 ‣ 3.1 Super-resolution
    Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey") shows, the LR input images are fed into deep CNNs without increasing
    resolution, and end-to-end learnable upsampling layers are applied at the end
    of the network.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高计算效率，并充分利用深度学习技术自动增加分辨率，研究人员提出通过用端到端可学习的层替代预定义的上采样，将大部分计算放在低维空间中，这些层集成在模型的末端。在这种框架的开创性工作[[84](#bib.bib84),
    [43](#bib.bib43)]中，如图[2b](#S3.F2.sf2 "在图2 ‣ 3.1 超分辨率框架 ‣ 3 监督超分辨率 ‣ 深度学习图像超分辨率综述")所示，低分辨率输入图像被送入深度卷积神经网络而不增加分辨率，端到端可学习的上采样层被应用在网络的末端。
- en: 'Since the feature extraction process with huge computational cost only occurs
    in low-dimensional space and the resolution increases only at the end, the computation
    and spatial complexity are much reduced. Therefore, this framework also has become
    one of the most mainstream frameworks [[25](#bib.bib25), [31](#bib.bib31), [79](#bib.bib79),
    [85](#bib.bib85)]. These models differ mainly in the learnable upsampling layers
    (Sec. [3.2](#S3.SS2 "3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣
    Deep Learning for Image Super-resolution: A Survey")), anterior CNN structures
    (Sec. [3.3](#S3.SS3 "3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep
    Learning for Image Super-resolution: A Survey")) and learning strategies (Sec.
    [3.4](#S3.SS4 "3.4 Learning Strategies ‣ 3 Supervised Super-resolution ‣ Deep
    Learning for Image Super-resolution: A Survey")), etc.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征提取过程仅在低维空间中进行，且分辨率仅在末端增加，因此计算和空间复杂性大大减少。因此，这一框架也成为了最主流的框架之一[[25](#bib.bib25),
    [31](#bib.bib31), [79](#bib.bib79), [85](#bib.bib85)]。这些模型主要在可学习的上采样层（见第[3.2节](#S3.SS2
    "3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 深度学习图像超分辨率综述")）、前置卷积神经网络结构（见第[3.3节](#S3.SS3 "3.3 网络设计
    ‣ 3 监督超分辨率 ‣ 深度学习图像超分辨率综述")）和学习策略（见第[3.4节](#S3.SS4 "3.4 学习策略 ‣ 3 监督超分辨率 ‣ 深度学习图像超分辨率综述")）等方面有所不同。
- en: 3.1.3 Progressive Upsampling Super-resolution
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 渐进上采样超分辨率
- en: 'Although post-upsampling SR framework has immensely reduced the computational
    cost, it still has some shortcomings. On the one hand, the upsampling is performed
    in only one step, which greatly increases the learning difficulty for large scaling
    factors (e.g., 4, 8). On the other hand, each scaling factor requires training
    an individual SR model, which cannot cope with the need for multi-scale SR. To
    address these drawbacks, a progressive upsampling framework is adopted by Laplacian
    pyramid SR network (LapSRN) [[27](#bib.bib27)], as Fig. [2c](#S3.F2.sf3 "In Figure
    2 ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") shows. Specifically, the models under this
    framework are based on a cascade of CNNs and progressively reconstruct higher-resolution
    images. At each stage, the images are upsampled to higher resolution and refined
    by CNNs. Other works such as MS-LapSRN [[65](#bib.bib65)] and progressive SR (ProSR)
    [[32](#bib.bib32)] also adopt this framework and achieve relatively high performance.
    In contrast to the LapSRN and MS-LapSRN using the intermediate reconstructed images
    as the “base images” for subsequent modules, the ProSR keeps the main information
    stream and reconstructs intermediate-resolution images by individual heads.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管后处理上采样 SR 框架已经大幅降低了计算成本，但仍存在一些不足。一方面，上采样仅在一步中完成，这大大增加了大缩放因子（例如 4、8）的学习难度。另一方面，每个缩放因子需要训练一个独立的
    SR 模型，这不能满足多尺度 SR 的需求。为了解决这些缺点，拉普拉斯金字塔 SR 网络（LapSRN）采用了渐进上采样框架[[27](#bib.bib27)]，如图[2c](#S3.F2.sf3
    "在图 2 ‣ 3.1 超分辨率框架 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")所示。具体而言，该框架下的模型基于 CNN 的级联，并逐步重建更高分辨率的图像。在每个阶段，图像被上采样到更高分辨率，并由
    CNN 进行优化。其他工作如 MS-LapSRN [[65](#bib.bib65)] 和渐进 SR (ProSR) [[32](#bib.bib32)]
    也采用了该框架并取得了相对较高的性能。与 LapSRN 和 MS-LapSRN 使用中间重建图像作为后续模块的“基础图像”不同，ProSR 保持了主要信息流，并通过各个头重建中间分辨率图像。
- en: 'By decomposing a difficult task into simple tasks, the models under this framework
    greatly reduce the learning difficulty, especially with large factors, and also
    cope with the multi-scale SR without introducing overmuch spacial and temporal
    cost. In addition, some specific learning strategies such as curriculum learning
    (Sec. [3.4.3](#S3.SS4.SSS3 "3.4.3 Curriculum Learning ‣ 3.4 Learning Strategies
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey")) and multi-supervision (Sec. [3.4.4](#S3.SS4.SSS4 "3.4.4 Multi-supervision
    ‣ 3.4 Learning Strategies ‣ 3 Supervised Super-resolution ‣ Deep Learning for
    Image Super-resolution: A Survey")) can be directly integrated to further reduce
    learning difficulty and improve final performance. However, these models also
    encounter some problems, such as the complicated model designing for multiple
    stages and the training stability, and more modelling guidance and more advanced
    training strategies are needed.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将复杂任务分解为简单任务，该框架下的模型大大降低了学习难度，尤其是在大因子情况下，同时也在不引入过多空间和时间成本的情况下应对多尺度 SR。此外，一些特定的学习策略，如课程学习（见[3.4.3](#S3.SS4.SSS3
    "3.4.3 课程学习 ‣ 3.4 学习策略 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")）和多重监督（见[3.4.4](#S3.SS4.SSS4
    "3.4.4 多重监督 ‣ 3.4 学习策略 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")），可以直接整合以进一步降低学习难度并提高最终性能。然而，这些模型也遇到了一些问题，如多阶段模型设计复杂性和训练稳定性，需要更多的建模指导和更先进的训练策略。
- en: 3.1.4 Iterative Up-and-down Sampling Super-resolution
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 迭代上下采样超分辨率
- en: 'In order to better capture the mutual dependency of LR-HR image pairs, an efficient
    iterative procedure named back-projection [[12](#bib.bib12)] is incorporated into
    SR [[44](#bib.bib44)]. This SR framework, namely iterative up-and-down sampling
    SR (as Fig. [2d](#S3.F2.sf4 "In Figure 2 ‣ 3.1 Super-resolution Frameworks ‣ 3
    Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")
    shows), tries to iteratively apply back-projection refinement, i.e., computing
    the reconstruction error then fusing it back to tune the HR image intensity. Specifically,
    Haris et al. [[57](#bib.bib57)] exploit iterative up-and-down sampling layers
    and propose DBPN, which connects upsampling and downsampling layers alternately
    and reconstructs the final HR result using all of the intermediately reconstructions.
    Similarly, the SRFBN [[86](#bib.bib86)] employs a iterative up-and-down sampling
    feedback block with more dense skip connections and learns better representations.
    And the RBPN [[87](#bib.bib87)] for video super-resolution extracts context from
    continuous video frames and combines these context to produce recurrent output
    frames by a back-projection module.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地捕捉LR-HR图像对的相互依赖性，一种高效的迭代程序，称为反投影[[12](#bib.bib12)]，被纳入到SR[[44](#bib.bib44)]中。这个SR框架，即迭代上采样和下采样SR（如图[2d](#S3.F2.sf4
    "在图2 ‣ 3.1 超分辨率框架 ‣ 3 监督式超分辨率 ‣ 图像超分辨率的深度学习：综述")所示），尝试迭代应用反投影精化，即计算重建误差然后将其融合以调整HR图像的强度。具体而言，Haris等人[[57](#bib.bib57)]利用迭代的上下采样层并提出了DBPN，该方法交替连接上采样和下采样层，并使用所有中间重建结果重构最终的HR结果。类似地，SRFBN[[86](#bib.bib86)]采用具有更多密集跳跃连接的迭代上采样反馈块，并学习更好的表示。而RBPN[[87](#bib.bib87)]用于视频超分辨率，从连续的视频帧中提取上下文，并通过反投影模块将这些上下文结合起来以产生递归输出帧。
- en: The models under this framework can better mine the deep relationships between
    LR-HR image pairs and thus provide higher-quality reconstruction results. Nevertheless,
    the design criteria of the back-projection modules are still unclear. Since this
    mechanism has just been introduced into deep learning-based SR, the framework
    has great potential and needs further exploration.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架下的模型可以更好地挖掘LR-HR图像对之间的深层关系，从而提供更高质量的重建结果。然而，反投影模块的设计标准仍不明确。由于这一机制刚刚被引入到基于深度学习的SR中，这个框架具有巨大的潜力，需要进一步探索。
- en: 3.2 Upsampling Methods
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 上采样方法
- en: In addition to the upsampling positions in the model, how to perform upsampling
    is of great importance. Although there has been various traditional upsampling
    methods [[20](#bib.bib20), [21](#bib.bib21), [88](#bib.bib88), [89](#bib.bib89)],
    making use of CNNs to learn end-to-end upsampling has gradually become a trend.
    In this section, we’ll introduce some traditional interpolation-based algorithms
    and deep learning-based upsampling layers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 除了模型中的上采样位置外，上采样的执行方式也至关重要。虽然已经有各种传统的上采样方法[[20](#bib.bib20), [21](#bib.bib21),
    [88](#bib.bib88), [89](#bib.bib89)]，但利用CNN进行端到端的上采样逐渐成为一种趋势。在这一部分，我们将介绍一些传统的基于插值的算法和基于深度学习的上采样层。
- en: 3.2.1 Interpolation-based Upsampling
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 基于插值的上采样
- en: '![Refer to caption](img/eae4ae11b626fe914fd01165a3b0372a.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eae4ae11b626fe914fd01165a3b0372a.png)'
- en: (a) Starting
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 开始
- en: '![Refer to caption](img/a572ef9b7560ffe9e3582d5511b618e4.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a572ef9b7560ffe9e3582d5511b618e4.png)'
- en: (b) Step 1
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 步骤 1
- en: '![Refer to caption](img/0bc3a88a3a14dffd0f36af9250cc4883.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0bc3a88a3a14dffd0f36af9250cc4883.png)'
- en: (c) Step 2
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 步骤 2
- en: '![Refer to caption](img/ebcb5c58e6d9f762655cbe7f20eebab3.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebcb5c58e6d9f762655cbe7f20eebab3.png)'
- en: (d) End
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 结束
- en: 'Figure 3: Interpolation-based upsampling methods. The gray board denotes the
    coordinates of pixels, and the blue, yellow and green points represent the initial,
    intermediate and output pixels, respectively.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：基于插值的上采样方法。灰色区域表示像素坐标，蓝色、黄色和绿色点分别代表初始、过渡和输出像素。
- en: Image interpolation, a.k.a. image scaling, refers to resizing digital images
    and is widely used by image-related applications. The traditional interpolation
    methods include nearest-neighbor interpolation, bilinear and bicubic interpolation,
    Sinc and Lanczos resampling, etc. Since these methods are interpretable and easy
    to implement, some of them are still widely used in CNN-based SR models.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图像插值，即图像缩放，指的是调整数字图像的大小，并广泛用于与图像相关的应用中。传统的插值方法包括最近邻插值、双线性插值和立方插值、Sinc和Lanczos重采样等。由于这些方法具有可解释性且易于实现，因此其中一些方法在基于CNN的SR模型中仍被广泛使用。
- en: Nearest-neighbor Interpolation. The nearest-neighbor interpolation is a simple
    and intuitive algorithm. It selects the value of the nearest pixel for each position
    to be interpolated regardless of any other pixels. Thus this method is very fast
    but usually produces blocky results of low quality.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻插值。最近邻插值是一种简单直观的算法。它选择每个位置的最近像素值进行插值，而不考虑其他像素。因此，这种方法非常快速，但通常会产生低质量的块状结果。
- en: 'Bilinear Interpolation. The bilinear interpolation (BLI) first performs linear
    interpolation on one axis of the image and then performs on the other axis, as
    Fig. [3](#S3.F3 "Figure 3 ‣ 3.2.1 Interpolation-based Upsampling ‣ 3.2 Upsampling
    Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey") shows. Since it results in a quadratic interpolation with a receptive
    field sized $2\times 2$, it shows much better performance than nearest-neighbor
    interpolation while keeping relatively fast speed.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 双线性插值。双线性插值（BLI）首先在图像的一个轴上进行线性插值，然后在另一个轴上进行，如图 [3](#S3.F3 "图 3 ‣ 3.2.1 基于插值的上采样
    ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 深度学习图像超分辨率：调查") 所示。由于它产生了一个感受野大小为 $2\times 2$ 的二次插值，它的表现比最近邻插值要好得多，同时保持了相对较快的速度。
- en: 'Bicubic Interpolation. Similarly, the bicubic interpolation (BCI) [[10](#bib.bib10)]
    performs cubic interpolation on each of the two axes, as Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2.1 Interpolation-based Upsampling ‣ 3.2 Upsampling Methods ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey") shows.
    Compared to BLI, the BCI takes $4\times 4$ pixels into account, and results in
    smoother results with fewer artifacts but much lower speed. In fact, the BCI with
    anti-aliasing is the mainstream method for building SR datasets (i.e., degrading
    HR images to LR images), and is also widely used in pre-upsampling SR framework
    (Sec. [3.1.1](#S3.SS1.SSS1 "3.1.1 Pre-upsampling Super-resolution ‣ 3.1 Super-resolution
    Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 双三次插值。同样，双三次插值（BCI）[[10](#bib.bib10)] 在两个轴上进行三次插值，如图 [3](#S3.F3 "图 3 ‣ 3.2.1
    基于插值的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 深度学习图像超分辨率：调查") 所示。与 BLI 相比，BCI 考虑了 $4\times
    4$ 像素，结果更加平滑，伪影更少，但速度要慢得多。事实上，带有抗锯齿的 BCI 是构建 SR 数据集（即将 HR 图像降解为 LR 图像）的主流方法，并且在预上采样
    SR 框架中也得到了广泛应用（第 [3.1.1](#S3.SS1.SSS1 "3.1.1 预上采样超分辨率 ‣ 3.1 超分辨率框架 ‣ 3 监督超分辨率
    ‣ 深度学习图像超分辨率：调查") 节）。
- en: As a matter of fact, the interpolation-based upsampling methods improve the
    image resolution only based on its own image signals, without bringing any more
    information. Instead, they often introduce some side effects, such as computational
    complexity, noise amplification, blurring results. Therefore, the current trend
    is to replace the interpolation-based methods with learnable upsampling layers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，基于插值的上采样方法仅依靠图像本身的信号来提高图像分辨率，而没有引入更多的信息。相反，它们通常会引入一些副作用，例如计算复杂度、噪声放大和模糊结果。因此，当前的趋势是用可学习的上采样层替代基于插值的方法。
- en: 3.2.2 Learning-based Upsampling
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 基于学习的上采样
- en: '![Refer to caption](img/3a4051e7e7b91bb6162af0b583601883.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3a4051e7e7b91bb6162af0b583601883.png)'
- en: (a) Starting
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: （a）开始
- en: '![Refer to caption](img/2e19892d9b5f3aa142ff71458ec26bf2.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2e19892d9b5f3aa142ff71458ec26bf2.png)'
- en: (b) Expanding
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: （b）扩展
- en: '![Refer to caption](img/8cef2594d0d7bebf1ccc05856c828726.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8cef2594d0d7bebf1ccc05856c828726.png)'
- en: (c) Convolution
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: （c）卷积
- en: 'Figure 4: Transposed convolution layer. The blue boxes denote the input, and
    the green boxes indicate the kernel and the convolution output.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：转置卷积层。蓝色框表示输入，绿色框表示卷积核和卷积输出。
- en: '![Refer to caption](img/5606e7b9d8cac90feef360c264a99282.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5606e7b9d8cac90feef360c264a99282.png)'
- en: (a) Starting
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: （a）开始
- en: '![Refer to caption](img/4aa3c5d1ee6030d7bab9d853ac26d4c0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4aa3c5d1ee6030d7bab9d853ac26d4c0.png)'
- en: (b) Convolution
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: （b）卷积
- en: '![Refer to caption](img/a4f08075870ade27e427e7b7d4478daa.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a4f08075870ade27e427e7b7d4478daa.png)'
- en: (c) Reshaping
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: （c）重塑
- en: 'Figure 5: Sub-pixel layer. The blue boxes denote the input, and the boxes with
    other colors indicate different convolution operations and different output feature
    maps.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：子像素层。蓝色框表示输入，其他颜色的框表示不同的卷积操作和不同的输出特征图。
- en: '![Refer to caption](img/e630371c80e3af7cde7cb410da01b85f.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e630371c80e3af7cde7cb410da01b85f.png)'
- en: 'Figure 6: Meta upscale module. The blue boxes denote the projection patch,
    and the green boxes and lines indicate the convolution operation with predicted
    weights.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 元上采样模块。蓝色框表示投影补丁，绿色框和线条表示带有预测权重的卷积操作。'
- en: In order to overcome the shortcomings of interpolation-based methods and learn
    upsampling in an end-to-end manner, transposed convolution layer and sub-pixel
    layer are introduced into the SR field.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服基于插值的方法的不足并以端到端的方式学习上采样，引入了转置卷积层和亚像素层到SR领域。
- en: 'Transposed Convolution Layer. Transposed convolution layer, a.k.a. deconvolution
    layer [[90](#bib.bib90), [91](#bib.bib91)], tries to perform transformation opposite
    a normal convolution, i.e., predicting the possible input based on feature maps
    sized like convolution output. Specifically, it increases the image resolution
    by expanding the image by inserting zeros and performing convolution. Taking $2\times$
    SR with $3\times 3$ kernel as example (as Fig. [4](#S3.F4 "Figure 4 ‣ 3.2.2 Learning-based
    Upsampling ‣ 3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") shows), the input is firstly expanded twice
    of the original size, where the added pixel values are set to $0$ (Fig. [4b](#S3.F4.sf2
    "In Figure 4 ‣ 3.2.2 Learning-based Upsampling ‣ 3.2 Upsampling Methods ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")). Then
    a convolution with kernel sized $3\times 3$, stride $1$ and padding $1$ is applied
    (Fig. [4c](#S3.F4.sf3 "In Figure 4 ‣ 3.2.2 Learning-based Upsampling ‣ 3.2 Upsampling
    Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")). In this way, the input is upsampled by a factor of 2, in which case
    the receptive field is at most $2\times 2$. Since the transposed convolution enlarges
    the image size in an end-to-end manner while maintaining a connectivity pattern
    compatible with vanilla convolution, it is widely used as upsampling layers in
    SR models [[78](#bib.bib78), [79](#bib.bib79), [85](#bib.bib85), [57](#bib.bib57)].
    However, this layer can easily cause “uneven overlapping” on each axis [[92](#bib.bib92)],
    and the multiplied results on both axes further create a checkerboard-like pattern
    of varying magnitudes and thus hurt the SR performance.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积层。转置卷积层，也称为反卷积层[[90](#bib.bib90), [91](#bib.bib91)]，尝试执行与普通卷积相反的变换，即基于特征图的卷积输出大小预测可能的输入。具体来说，它通过插入零并执行卷积来增加图像分辨率。以$2\times$
    SR和$3\times 3$内核为例（如图 [4](#S3.F4 "图 4 ‣ 3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率
    ‣ 图像超分辨率的深度学习：综述")所示），输入首先被扩展为原始大小的两倍，其中新增的像素值被设置为$0$（图 [4b](#S3.F4.sf2 "图 4 ‣
    3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")）。然后，应用内核大小为$3\times 3$，步幅为$1$，填充为$1$的卷积（图
    [4c](#S3.F4.sf3 "图 4 ‣ 3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")）。这样，输入被以$2$的倍数上采样，其中接收域最多为$2\times
    2$。由于转置卷积以端到端的方式放大图像大小，同时保持与普通卷积兼容的连接模式，因此在SR模型中被广泛用作上采样层[[78](#bib.bib78), [79](#bib.bib79),
    [85](#bib.bib85), [57](#bib.bib57)]。然而，这一层容易在每个轴上造成“均匀重叠”[[92](#bib.bib92)]，而两个轴上的乘积结果进一步产生具有不同幅度的棋盘状图案，从而影响SR性能。
- en: 'Sub-pixel Layer. The sub-pixel layer [[84](#bib.bib84)], another end-to-end
    learnable upsampling layer, performs upsampling by generating a plurality of channels
    by convolution and then reshaping them, as Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.2 Learning-based
    Upsampling ‣ 3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") shows. Within this layer, a convolution
    is firstly applied for producing outputs with $s^{2}$ times channels, where $s$
    is the scaling factor (Fig. [5b](#S3.F5.sf2 "In Figure 5 ‣ 3.2.2 Learning-based
    Upsampling ‣ 3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey")). Assuming the input size is $h\times w\times
    c$, the output size will be $h\times w\times s^{2}c$. After that, the reshaping
    operation (a.k.a. shuffle [[84](#bib.bib84)]) is performed to produce outputs
    with size $sh\times sw\times c$ (Fig. [5c](#S3.F5.sf3 "In Figure 5 ‣ 3.2.2 Learning-based
    Upsampling ‣ 3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey")). In this case, the receptive field can
    be up to $3\times 3$. Due to the end-to-end upsampling manner, this layer is also
    widely used by SR models [[25](#bib.bib25), [39](#bib.bib39), [93](#bib.bib93),
    [28](#bib.bib28)]. Compared with transposed convolution layer, the sub-pixel layer
    has a larger receptive field, which provides more contextual information to help
    generate more realistic details. However, since the distribution of the receptive
    fields is uneven and blocky regions actually share the same receptive field, it
    may result in some artifacts near the boundaries of different blocks. On the other
    hand, independently predicting adjacent pixels in a blocky region may cause unsmooth
    outputs. Thus Gao et al. [[94](#bib.bib94)] propose PixelTCL, which replaces the
    independent prediction to interdependent sequential prediction, and produces smoother
    and more consistent results.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 子像素层。子像素层[[84](#bib.bib84)]，另一种端到端可学习的上采样层，通过卷积生成多个通道并重新调整形状来进行上采样，如图[5](#S3.F5
    "图 5 ‣ 3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")所示。在这一层中，首先应用卷积以生成$
    s^{2} $倍的通道输出，其中$s$是缩放因子（图[5b](#S3.F5.sf2 "图 5 ‣ 3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣
    3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")）。假设输入大小为$ h\times w\times c $，输出大小将为$ h\times w\times
    s^{2}c $。之后，执行重新调整操作（也称为shuffle[[84](#bib.bib84)]）以生成大小为$ sh\times sw\times c
    $的输出（图[5c](#S3.F5.sf3 "图 5 ‣ 3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")）。在这种情况下，感受野可以达到$
    3\times 3 $。由于端到端的上采样方式，这一层也被SR模型广泛使用[[25](#bib.bib25), [39](#bib.bib39), [93](#bib.bib93),
    [28](#bib.bib28)]。与转置卷积层相比，子像素层具有更大的感受野，提供了更多的上下文信息以帮助生成更真实的细节。然而，由于感受野的分布不均匀且块状区域实际上共享相同的感受野，这可能导致不同块边界附近出现一些伪影。另一方面，独立预测块状区域中的相邻像素可能导致输出不平滑。因此，Gao等人[[94](#bib.bib94)]提出了PixelTCL，该方法将独立预测替换为相互依赖的顺序预测，产生更平滑且更一致的结果。
- en: 'Meta Upscale Module.  The previous methods need to predefine the scaling factors,
    i.e., training different upsampling modules for different factors, which is inefficient
    and not in line with real needs. So that Hu et al. [[95](#bib.bib95)] propose
    meta upscale module (as Fig. [6](#S3.F6 "Figure 6 ‣ 3.2.2 Learning-based Upsampling
    ‣ 3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image
    Super-resolution: A Survey") shows), which firstly solves SR of arbitrary scaling
    factors based on meta learning. Specifically, for each target position on the
    HR images, this module project it to a small patch on the LR feature maps (i.e.,
    $k\times k\times c_{in}$), predicts convolution weights (i.e., $k\times k\times
    c_{in}\times c_{out}$) according to the projection offsets and the scaling factor
    by dense layers and perform convolution. In this way, the meta upscale module
    can continuously zoom in it with arbitrary factors by a single model. And due
    to the large amount of training data (multiple factors are simultaneously trained),
    the module can exhibit comparable or even better performance on fixed factors.
    Although this module needs to predict weights during inference, the execution
    time of the upsampling module only accounts for about 1% of the time of feature
    extraction [[95](#bib.bib95)]. However, this method predicts a large number of
    convolution weights for each target pixel based on several values independent
    of the image contents, so the prediction result may be unstable and less efficient
    when faced with larger magnifications.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 元上采样模块。以往的方法需要预定义缩放因子，即为不同因子训练不同的上采样模块，这种方法效率低且不符合实际需求。因此，Hu 等人[[95](#bib.bib95)]
    提出了元上采样模块（如图[6](#S3.F6 "图 6 ‣ 3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")所示），该模块首先基于元学习解决任意缩放因子的SR问题。具体而言，对于HR图像上的每个目标位置，该模块将其投影到LR特征图上的小补丁（即
    $k\times k\times c_{in}$），根据投影偏移和缩放因子通过密集层预测卷积权重（即 $k\times k\times c_{in}\times
    c_{out}$）并执行卷积。通过这种方式，元上采样模块可以通过单一模型连续放大任意因子。由于大量的训练数据（同时训练多个因子），该模块在固定因子上的表现可以媲美甚至优于传统方法。尽管该模块在推理过程中需要预测权重，但上采样模块的执行时间仅占特征提取时间的约1%[[95](#bib.bib95)]。然而，该方法基于与图像内容无关的多个值预测大量卷积权重，因此在面对更大放大倍率时，预测结果可能不稳定且效率较低。
- en: 'Nowadays, these learning-based layers have become the most widely used upsampling
    methods. Especially in the post-upsampling framework (Sec. [3.1.2](#S3.SS1.SSS2
    "3.1.2 Post-upsampling Super-resolution ‣ 3.1 Super-resolution Frameworks ‣ 3
    Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")),
    these layers are usually used in the final upsampling phase for reconstructing
    HR images based on high-level representations extracted in low-dimensional space,
    and thus achieve end-to-end SR while avoiding overwhelming operations in high-dimensional
    space.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，这些基于学习的层已经成为最广泛使用的上采样方法。特别是在后采样框架中（见[3.1.2](#S3.SS1.SSS2 "3.1.2 后采样超分辨率 ‣
    3.1 超分辨率框架 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")），这些层通常在最终的上采样阶段用于基于在低维空间中提取的高层次表示来重建HR图像，从而实现端到端的SR，同时避免在高维空间中进行过于复杂的操作。
- en: 3.3 Network Design
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 网络设计
- en: '![Refer to caption](img/2529155ed09423efee4408a73e74b1bf.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2529155ed09423efee4408a73e74b1bf.png)'
- en: (a) Residual Learning
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 残差学习
- en: '![Refer to caption](img/644ee4ba1e3775634ddf805150c6085e.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/644ee4ba1e3775634ddf805150c6085e.png)'
- en: (b) Recursive learning
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 递归学习
- en: '![Refer to caption](img/ac842d99eafe6b84c7bba31f6a92c6a7.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac842d99eafe6b84c7bba31f6a92c6a7.png)'
- en: (c) Channel attention
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 通道注意力
- en: '![Refer to caption](img/36e2ba501e7b45fab81952681e388191.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/36e2ba501e7b45fab81952681e388191.png)'
- en: (d) Dense connections
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 稠密连接
- en: '![Refer to caption](img/d97d8e619923167ad5eef7e3d9af32a3.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d97d8e619923167ad5eef7e3d9af32a3.png)'
- en: (e) Local multi-path learning
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 局部多路径学习
- en: '![Refer to caption](img/b0fecc0b7f4b79b3f9b138bff18b1e45.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b0fecc0b7f4b79b3f9b138bff18b1e45.png)'
- en: (f) Scale-specific multi-path learning
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 特定尺度的多路径学习
- en: '![Refer to caption](img/0ba38958b6c2ab48a4ee56cb6c3a930f.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0ba38958b6c2ab48a4ee56cb6c3a930f.png)'
- en: (g) Group convolution
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (g) 分组卷积
- en: '![Refer to caption](img/abdfd3f0305a13f3622571dab634575a.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/abdfd3f0305a13f3622571dab634575a.png)'
- en: (h) Pyramid pooling
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (h) 金字塔池化
- en: 'Figure 7: Network design strategies.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：网络设计策略。
- en: 'Nowadays the network design has been one of the most important parts of deep
    learning. In the super-resolution field, researchers apply all kinds of network
    design strategies on top of the four SR frameworks (Sec. [3.1](#S3.SS1 "3.1 Super-resolution
    Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")) to construct the final networks. In this section, we decompose these
    networks to essential principles or strategies for network design, introduce them
    and analyze the advantages and limitations one by one.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '如今，网络设计已成为深度学习中最重要的部分之一。在超分辨率领域，研究人员在四种 SR 框架 (Sec. [3.1](#S3.SS1 "3.1 Super-resolution
    Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")) 上应用各种网络设计策略以构建最终网络。在本节中，我们将这些网络分解为网络设计的基本原则或策略，逐一介绍并分析其优缺点。'
- en: 3.3.1 Residual Learning
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 残差学习
- en: 'Before He et al. [[96](#bib.bib96)] propose ResNet for learning residuals instead
    of a thorough mapping, residual learning has been widely employed by SR models
    [[97](#bib.bib97), [48](#bib.bib48), [88](#bib.bib88)], as Fig. [7a](#S3.F7.sf1
    "In Figure 7 ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") shows. Among them, the residual learning
    strategies can be roughly divided into global and local residual learning.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '在 He 等人 [[96](#bib.bib96)] 提出 ResNet 以学习残差而非完整映射之前，残差学习已被广泛应用于 SR 模型 [[97](#bib.bib97),
    [48](#bib.bib48), [88](#bib.bib88)]，如图 [7a](#S3.F7.sf1 "In Figure 7 ‣ 3.3 Network
    Design ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey") 所示。其中，残差学习策略大致可以分为全局残差学习和局部残差学习。'
- en: Global Residual Learning. Since the image SR is an image-to-image translation
    task where the input image is highly correlated with the target image, researchers
    try to learn only the residuals between them, namely global residual learning.
    In this case, it avoids learning a complicated transformation from a complete
    image to another, instead only requires learning a residual map to restore the
    missing high-frequency details. Since the residuals in most regions are close
    to zero, the model complexity and learning difficulty are greatly reduced. Thus
    it is widely used by SR models [[26](#bib.bib26), [56](#bib.bib56), [55](#bib.bib55),
    [98](#bib.bib98)].
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 全局残差学习。由于图像 SR 是一个图像到图像的转换任务，输入图像与目标图像高度相关，研究人员尝试仅学习它们之间的残差，即全局残差学习。在这种情况下，避免了从完整图像到另一个图像的复杂变换，只需学习残差图以恢复丢失的高频细节。由于大多数区域的残差接近于零，模型复杂性和学习难度大大降低。因此，它被
    SR 模型广泛使用 [[26](#bib.bib26), [56](#bib.bib56), [55](#bib.bib55), [98](#bib.bib98)]。
- en: Local Residual Learning. The local residual learning is similar to the residual
    learning in ResNet [[96](#bib.bib96)] and used to alleviate the degradation problem
    [[96](#bib.bib96)] caused by ever-increasing network depths, reduce training difficulty
    and improve the learning ability. It is also widely used for SR [[78](#bib.bib78),
    [85](#bib.bib85), [70](#bib.bib70), [99](#bib.bib99)].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本地残差学习。局部残差学习类似于 ResNet 中的残差学习 [[96](#bib.bib96)]，用于缓解由于网络深度不断增加而导致的退化问题 [[96](#bib.bib96)]，减少训练难度并提高学习能力。它在
    SR [[78](#bib.bib78), [85](#bib.bib85), [70](#bib.bib70), [99](#bib.bib99)] 中也得到了广泛应用。
- en: In practice, the above methods are both implemented by shortcut connections
    (often scaled by a small constant) and element-wise addition, while the difference
    is that the former directly connects the input and output images, while the latter
    usually adds multiple shortcuts between layers with different depths inside the
    network.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，上述方法都通过快捷连接（通常通过一个小常数缩放）和逐元素加法来实现，不同之处在于前者直接连接输入和输出图像，而后者通常在网络内部的不同深度之间添加多个快捷连接。
- en: 3.3.2 Recursive Learning
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 递归学习
- en: 'In order to learn higher-level features without introducing overwhelming parameters,
    recursive learning, which means applying the same modules multiple times in a
    recursive manner, is introduced into the SR field, as Fig. [7b](#S3.F7.sf2 "In
    Figure 7 ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") shows.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '为了在不引入过多参数的情况下学习更高层次的特征，递归学习，即以递归方式多次应用相同的模块，被引入到 SR 领域，如图 [7b](#S3.F7.sf2
    "In Figure 7 ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey") 所示。'
- en: Among them, the 16-recursive DRCN[[82](#bib.bib82)] employs a single convolutional
    layer as the recursive unit and reaches a receptive field of $41\times 41$, which
    is much larger than $13\times 13$ of SRCNN [[22](#bib.bib22)], without over many
    parameters. The DRRN [[56](#bib.bib56)] uses a ResBlock [[96](#bib.bib96)] as
    the recursive unit for 25 recursions and obtains even better performance than
    the 17-ResBlock baseline. Later Tai et al. [[55](#bib.bib55)] propose MemNet based
    on the memory block, which is composed of a 6-recursive ResBlock where the outputs
    of every recursion are concatenated and go through an extra $1\times 1$ convolution
    for memorizing and forgetting. The cascading residual network (CARN) [[28](#bib.bib28)]
    also adopts a similar recursive unit including several ResBlocks. Recently, Li
    et al. [[86](#bib.bib86)] employ iterative up-and-down sampling SR framework,
    and propose a feedback network based on recursive learning, where the weights
    of the entire network are shared across all recursions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些方法中，16-递归 DRCN[[82](#bib.bib82)] 采用了单一的卷积层作为递归单元，并且达到 $41\times 41$ 的感受野，这比
    SRCNN [[22](#bib.bib22)] 的 $13\times 13$ 要大得多，同时参数量并没有过多增加。DRRN [[56](#bib.bib56)]
    使用了一个 ResBlock [[96](#bib.bib96)] 作为递归单元进行 25 次递归，并且获得了比 17-ResBlock 基线更好的性能。后来，Tai
    等人 [[55](#bib.bib55)] 提出了基于记忆块的 MemNet，它由 6-递归 ResBlock 组成，每次递归的输出都会被串联起来，并经过额外的
    $1\times 1$ 卷积进行记忆和遗忘。级联残差网络 (CARN) [[28](#bib.bib28)] 也采用了类似的递归单元，包括若干 ResBlocks。最近，Li
    等人 [[86](#bib.bib86)] 采用了迭代上下采样 SR 框架，并提出了一种基于递归学习的反馈网络，其中整个网络的权重在所有递归中共享。
- en: Besides, researchers also employ different recursive modules in different parts.
    Specifically, Han et al. [[85](#bib.bib85)] propose dual-state recurrent network
    (DSRN) to exchange signals between the LR and HR states. At each time step (i.e.,
    recursion), the representations of each branch are updated and exchanged for better
    exploring LR-HR relationships. Similarly, Lai et al. [[65](#bib.bib65)] employ
    the embedding and upsampling modules as recursive units, and thus much reduce
    the model size at the expense of little performance loss.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员还在不同的部分使用了不同的递归模块。具体而言，Han 等人 [[85](#bib.bib85)] 提出了双状态递归网络 (DSRN)，以便在
    LR 和 HR 状态之间交换信号。在每个时间步骤（即递归中），每个分支的表示都会被更新并交换，以更好地探索 LR-HR 关系。类似地，Lai 等人 [[65](#bib.bib65)]
    将嵌入和上采样模块作为递归单元，从而在性能损失很小的情况下大幅度减少了模型的大小。
- en: 'In general, the recursive learning can indeed learn more advanced representations
    without introducing excessive parameters, but still can’t avoid high computational
    costs. And it inherently brings vanishing or exploding gradient problems, consequently
    some techniques such as residual learning (Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Residual
    Learning ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey")) and multi-supervision (Sec. [3.4.4](#S3.SS4.SSS4
    "3.4.4 Multi-supervision ‣ 3.4 Learning Strategies ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")) are often integrated with
    recursive learning for mitigating these problems [[82](#bib.bib82), [56](#bib.bib56),
    [55](#bib.bib55), [85](#bib.bib85)].'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '总体来说，递归学习确实可以在不引入过多参数的情况下学习更先进的表示，但仍然无法避免高计算成本。而且，它本质上带来了梯度消失或爆炸的问题，因此一些技术，如残差学习（见
    [3.3.1](#S3.SS3.SSS1 "3.3.1 Residual Learning ‣ 3.3 Network Design ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")）和多重监督（见
    [3.4.4](#S3.SS4.SSS4 "3.4.4 Multi-supervision ‣ 3.4 Learning Strategies ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")），通常与递归学习结合，以缓解这些问题
    [[82](#bib.bib82), [56](#bib.bib56), [55](#bib.bib55), [85](#bib.bib85)]。'
- en: 3.3.3 Multi-path Learning
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 多路径学习
- en: Multi-path learning refers to passing features through multiple paths, which
    perform different operations, and fusing them back for providing better modelling
    capabilities. Specifically, it could be divided into global, local and scale-specific
    multi-path learning, as bellow.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 多路径学习指的是将特征通过多个路径进行处理，这些路径执行不同的操作，然后将它们融合回来，以提供更好的建模能力。具体来说，它可以分为全局、多层次和特定尺度的多路径学习，如下所示。
- en: Global Multi-path Learning. Global multi-path learning refers to making use
    of multiple paths to extract features of different aspects of the images. These
    paths can cross each other in the propagation and thus greatly enhance the learning
    ability. Specifically, the LapSRN [[27](#bib.bib27)] includes a feature extraction
    path predicting the sub-band residuals in a coarse-to-fine fashion and another
    path to reconstruct HR images based on the signals from both paths. Similarly,
    the DSRN [[85](#bib.bib85)] utilizes two paths to extract information in low-dimensional
    and high-dimensional space, respectively, and continuously exchanges information
    for further improving learning ability. And the pixel recursive super-resolution
    [[64](#bib.bib64)] adopts a conditioning path to capture the global structure
    of images, and a prior path to capture the serial dependence of generated pixels.
    In contrast, Ren et al. [[100](#bib.bib100)] employ multiple paths with unbalanced
    structures to perform upsampling and fuse them at the end of the model.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 全球多路径学习。全球多路径学习指的是利用多条路径来提取图像不同方面的特征。这些路径在传播过程中可以交叉，从而大大增强学习能力。具体来说，LapSRN [[27](#bib.bib27)]
    包括一个特征提取路径，该路径以粗到细的方式预测子带残差，另一个路径则基于两个路径的信号重建高分辨率图像。同样，DSRN [[85](#bib.bib85)]
    利用两个路径分别提取低维和高维空间中的信息，并不断交换信息以进一步提高学习能力。而像素递归超分辨率 [[64](#bib.bib64)] 采用了一个条件路径来捕捉图像的全局结构，以及一个先验路径来捕捉生成像素的序列依赖。相比之下，Ren
    等人 [[100](#bib.bib100)] 采用具有不平衡结构的多条路径来执行上采样，并在模型的最后进行融合。
- en: 'Local Multi-path Learning. Motivated by the inception module [[101](#bib.bib101)],
    the MSRN [[99](#bib.bib99)] adopts a new block for multi-scale feature extraction,
    as Fig. [7e](#S3.F7.sf5 "In Figure 7 ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey") shows. In this block, two
    convolution layers with kernel size $3\times 3$ and $5\times 5$ are adopted to
    extract features simultaneously, then the outputs are concatenated and go through
    the same operations again, and finally an extra $1\times 1$ convolution is applied.
    A shortcut connects the input and output by element-wise addition. Through such
    local multi-path learning, the SR models can better extract image features from
    multiple scales and further improve performance.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '局部多路径学习。受到 inception 模块 [[101](#bib.bib101)] 的启发，MSRN [[99](#bib.bib99)] 采用了一种新的块进行多尺度特征提取，如图
    [7e](#S3.F7.sf5 "In Figure 7 ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey") 所示。在这个块中，采用了两个卷积层，分别具有
    $3\times 3$ 和 $5\times 5$ 的卷积核来同时提取特征，然后将输出进行拼接并再次经过相同的操作，最后应用一个额外的 $1\times 1$
    卷积。一个捷径通过逐元素加法连接输入和输出。通过这种局部多路径学习，超分辨率模型能够更好地从多个尺度中提取图像特征，进一步提高性能。'
- en: 'Scale-specific Multi-path Learning. Considering that SR models for different
    scales need to go through similar feature extraction, Lim et al. [[31](#bib.bib31)]
    propose scale-specific multi-path learning to cope with multi-scale SR with a
    single network. To be concrete, they share the principal components of the model
    (i.e., the intermediate layers for feature extraction), and attach scale-specific
    pre-processing paths and upsampling paths at the beginning and the end of the
    network, respectively (as Fig. [7f](#S3.F7.sf6 "In Figure 7 ‣ 3.3 Network Design
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey") shows). During training, only the paths corresponding to the selected
    scale are enabled and updated. In this way, the proposed MDSR [[31](#bib.bib31)]
    greatly reduce the model size by sharing most of the parameters for different
    scales and exhibits comparable performance as single-scale models. The similar
    scale-specific multi-path learning is also adopted by CARN [[28](#bib.bib28)]
    and ProSR [[32](#bib.bib32)].'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '特定尺度多路径学习。考虑到不同尺度的超分辨率模型需要经过类似的特征提取，Lim 等人 [[31](#bib.bib31)] 提出了特定尺度多路径学习，以便使用单一网络应对多尺度超分辨率。具体来说，他们共享模型的主要组件（即用于特征提取的中间层），并在网络的开始和结束处分别附加特定尺度的预处理路径和上采样路径（如图
    [7f](#S3.F7.sf6 "In Figure 7 ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey") 所示）。在训练过程中，仅启用和更新与所选尺度对应的路径。通过这种方式，提出的
    MDSR [[31](#bib.bib31)] 通过共享大部分参数来大幅减少模型大小，并展现出与单尺度模型相当的性能。CARN [[28](#bib.bib28)]
    和 ProSR [[32](#bib.bib32)] 也采用了类似的特定尺度多路径学习。'
- en: 3.3.4 Dense Connections
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4 密集连接
- en: Since Huang et al. [[102](#bib.bib102)] propose DenseNet based on dense blocks,
    the dense connections have become more and more popular in vision tasks. For each
    layer in a dense block, the feature maps of all preceding layers are used as inputs,
    and its own feature maps are used as inputs into all subsequent layers, so that
    it leads to $l\cdot(l-1)/2$ connections in a $l$-layer dense block ($l\geq 2$).
    The dense connections not only help alleviate gradient vanishing, enhance signal
    propagation and encourage feature reuse, but also substantially reduce the model
    size by employing small growth rate (i.e., number of channels in dense blocks)
    and squeezing channels after concatenating all input feature maps.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Huang 等人 [[102](#bib.bib102)] 提出了基于密集块的 DenseNet，密集连接在视觉任务中变得越来越流行。对于密集块中的每一层，所有前面层的特征图都用作输入，而自身的特征图用作所有后续层的输入，因此在一个
    $l$ 层的密集块中产生了 $l\cdot(l-1)/2$ 个连接 ($l\geq 2$)。密集连接不仅有助于缓解梯度消失，增强信号传播和鼓励特征重用，还通过使用较小的增长率（即密集块中的通道数）和在连接所有输入特征图后压缩通道来显著减少模型大小。
- en: 'For the sake of fusing low-level and high-level features to provide richer
    information for reconstructing high-quality details, dense connections are introduced
    into the SR field, as Fig. [7d](#S3.F7.sf4 "In Figure 7 ‣ 3.3 Network Design ‣
    3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")
    shows. Tong et al. [[79](#bib.bib79)] not only adopt dense blocks to construct
    a 69-layers SRDenseNet, but also insert dense connections between different dense
    blocks, i.e., for every dense block, the feature maps of all preceding blocks
    are used as inputs, and its own feature maps are used as inputs into all subsequent
    blocks. These layer-level and block-level dense connections are also adopted by
    MemNet [[55](#bib.bib55)], CARN [[28](#bib.bib28)], RDN [[93](#bib.bib93)] and
    ESRGAN [[103](#bib.bib103)]. The DBPN [[57](#bib.bib57)] also adopts dense connections
    extensively, but their dense connections are between all the upsampling units,
    as are the downsampling units.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 为了融合低级和高级特征以提供更丰富的信息以重建高质量细节，在SR领域引入了密集连接，如图 [7d](#S3.F7.sf4 "图7 ‣ 3.3 网络设计
    ‣ 3 监督超分辨率 ‣ 深度学习在图像超分辨率中的应用：综述") 所示。Tong 等人 [[79](#bib.bib79)] 不仅采用密集块构建了一个69层的SRDenseNet，还在不同的密集块之间插入了密集连接，即，对于每个密集块，所有前面的块的特征图都用作输入，其自身的特征图用作所有后续块的输入。这些层级和块级的密集连接也被
    MemNet [[55](#bib.bib55)]、CARN [[28](#bib.bib28)]、RDN [[93](#bib.bib93)] 和 ESRGAN
    [[103](#bib.bib103)] 采用。DBPN [[57](#bib.bib57)] 也广泛采用了密集连接，但它们的密集连接是在所有上采样单元之间，而不是下采样单元之间。
- en: 3.3.5 Attention Mechanism
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5 注意力机制
- en: 'Channel Attention. Considering the interdependence and interaction of the feature
    representations between different channels, Hu et al. [[104](#bib.bib104)] propose
    a “squeeze-and-excitation” block to improve learning ability by explicitly modelling
    channel interdependence, as Fig. [7c](#S3.F7.sf3 "In Figure 7 ‣ 3.3 Network Design
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey") shows. In this block, each input channel is squeezed into a channel descriptor
    (i.e., a constant) using global average pooling (GAP), then these descriptors
    are fed into two dense layers to produce channel-wise scaling factors for input
    channels. Recently, Zhang et al. [[70](#bib.bib70)] incorporate the channel attention
    mechanism with SR and propose RCAN, which markedly improves the representation
    ability of the model and SR performance. In order to better learn the feature
    correlations, Dai et al. [[105](#bib.bib105)] further propose a second-order channel
    attention (SOCA) module. The SOCA adaptively rescales the channel-wise features
    by using second-order feature statistics instead of GAP, and enables extracting
    more informative and discriminative representations.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通道注意力。考虑到不同通道之间特征表示的相互依赖和交互，Hu 等人 [[104](#bib.bib104)] 提出了一个“挤压和激励”块，通过显式建模通道间依赖性来提高学习能力，如图
    [7c](#S3.F7.sf3 "图7 ‣ 3.3 网络设计 ‣ 3 监督超分辨率 ‣ 深度学习在图像超分辨率中的应用：综述") 所示。在这个块中，每个输入通道通过全局平均池化（GAP）被挤压成一个通道描述符（即一个常量），然后这些描述符被输入到两个密集层中，以产生输入通道的通道级缩放因子。最近，Zhang
    等人 [[70](#bib.bib70)] 将通道注意力机制与 SR 相结合，提出了 RCAN，这显著提高了模型的表示能力和 SR 性能。为了更好地学习特征相关性，Dai
    等人 [[105](#bib.bib105)] 进一步提出了二阶通道注意力（SOCA）模块。SOCA 通过使用二阶特征统计代替 GAP 自适应地重新调整通道级特征，并能够提取更具信息性和区分性的表示。
- en: Non-local Attention. Most existing SR models have very limited local receptive
    fields. However, some distant objects or textures may be very important for local
    patch generation. So that Zhang et al. [[106](#bib.bib106)] propose local and
    non-local attention blocks to extract features that capture the long-range dependencies
    between pixels. Specifically, they propose a trunk branch for extracting features,
    and a (non-)local mask branch for adaptively rescaling features of trunk branch.
    Among them, the local branch employs an encoder-decoder structure to learn the
    local attention, while the non-local branch uses the embedded Gaussian function
    to evaluate pairwise relationships between every two position indices in the feature
    maps to predict the scaling weights. Through this mechanism, the proposed method
    captures the spatial attention well and further enhances the representation ability.
    Similarly, Dai et al. [[105](#bib.bib105)] also incorporate the non-local attention
    mechanism to capture long-distance spatial contextual information.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 非局部注意力。大多数现有的 SR 模型具有非常有限的局部感受野。然而，一些远处的对象或纹理对于局部补丁生成可能非常重要。因此，Zhang 等人 [[106](#bib.bib106)]
    提出了局部和非局部注意力模块，以提取捕获像素间长期依赖关系的特征。具体而言，他们提出了一个用于提取特征的主干分支和一个（非）局部掩码分支，用于自适应地重新缩放主干分支的特征。在这些模块中，局部分支采用编码器-解码器结构来学习局部注意力，而非局部分支则使用嵌入的高斯函数来评估特征图中每两个位置索引之间的成对关系，从而预测缩放权重。通过这种机制，该方法能够很好地捕捉空间注意力，并进一步增强表示能力。同样，Dai
    等人 [[105](#bib.bib105)] 也结合了非局部注意力机制，以捕获远距离的空间上下文信息。
- en: 3.3.6 Advanced Convolution
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.6 高级卷积
- en: Since convolution operations are the basis of deep neural networks, researchers
    also attempt to improve convolution operations for better performance or greater
    efficiency.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积操作是深度神经网络的基础，研究人员也尝试改进卷积操作，以获得更好的性能或更高的效率。
- en: Dilated Convolution. It is well known that the contextual information facilitates
    generating realistic details for SR. Thus Zhang et al. [[107](#bib.bib107)] replace
    the common convolution by dilated convolution in SR models, increase the receptive
    field over twice and achieve much better performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积。众所周知，上下文信息有助于生成现实的细节。因此，Zhang 等人 [[107](#bib.bib107)] 在超分辨率（SR）模型中用膨胀卷积替代了常规卷积，将感受野扩大了一倍多，并取得了更好的性能。
- en: Group Convolution. Motivated by recent advances on lightweight CNNs [[108](#bib.bib108),
    [109](#bib.bib109)], Hui et al. [[98](#bib.bib98)] and Ahn et al. [[28](#bib.bib28)]
    propose IDN and CARN-M, respectively, by replacing the vanilla convolution by
    group convolution. As some previous works have proven, the group convolution much
    reduces the number of parameters and operations at the expense of a little performance
    loss [[98](#bib.bib98), [28](#bib.bib28)].
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 分组卷积。受到近期轻量级卷积神经网络（CNNs）[[108](#bib.bib108), [109](#bib.bib109)] 进展的启发，Hui 等人
    [[98](#bib.bib98)] 和 Ahn 等人 [[28](#bib.bib28)] 分别通过用分组卷积替代传统卷积，提出了 IDN 和 CARN-M。正如一些前期工作所证明的那样，分组卷积大大减少了参数和计算量，尽管性能有所下降
    [[98](#bib.bib98), [28](#bib.bib28)]。
- en: Depthwise Separable Convolution Since Howard et al. [[110](#bib.bib110)] propose
    depthwise separable convolution for efficient convolution, it has been expanded
    to into various fields. Specifically, it consists of a factorized depthwise convolution
    and a pointwise convolution (i.e., $1\times 1$ convolution), and thus reduces
    plenty of parameters and operations at only a small reduction in accuracy [[110](#bib.bib110)].
    And recently, Nie et al. [[81](#bib.bib81)] employ the depthwise separable convolution
    and much accelerate the SR architecture.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 深度可分离卷积。自从 Howard 等人 [[110](#bib.bib110)] 提出了用于高效卷积的深度可分离卷积之后，它已被扩展到各种领域。具体而言，它由一个因式分解的深度卷积和一个点卷积（即
    $1\times 1$ 卷积）组成，从而大幅减少了参数和计算量，准确率只略有下降 [[110](#bib.bib110)]。最近，Nie 等人 [[81](#bib.bib81)]
    采用深度可分离卷积，大大加速了 SR 架构。
- en: 3.3.7 Region-recursive Learning
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.7 区域递归学习
- en: 'Most SR models treat SR as a pixel-independent task and thus cannot source
    the interdependence between generated pixels properly. Inspired by PixelCNN [[111](#bib.bib111)],
    Dahl et al. [[64](#bib.bib64)] firstly propose pixel recursive learning to perform
    pixel-by-pixel generation, by employing two networks to capture global contextual
    information and serial generation dependence, respectively. In this way, the proposed
    method synthesizes realistic hair and skin details on super-resolving very low-resolution
    face images (e.g., $8\times 8$) and far exceeds the previous methods on MOS testing
    [[64](#bib.bib64)] (Sec. [2.3.3](#S2.SS3.SSS3 "2.3.3 Mean Opinion Score ‣ 2.3
    Image Quality Assessment ‣ 2 Problem Setting and Terminology ‣ Deep Learning for
    Image Super-resolution: A Survey")).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数超分辨率模型将超分辨率视为一个像素独立的任务，因此无法有效捕捉生成像素之间的相互依赖关系。受到PixelCNN [[111](#bib.bib111)]
    的启发，Dahl等人[[64](#bib.bib64)]首次提出像素递归学习，通过采用两个网络分别捕捉全局上下文信息和序列生成依赖性，从而实现逐像素生成。通过这种方式，所提出的方法在超分辨率非常低分辨率的面部图像（例如$8\times
    8$）上合成了逼真的头发和皮肤细节，并在MOS测试[[64](#bib.bib64)]（见[2.3.3](#S2.SS3.SSS3 "2.3.3 平均意见分数
    ‣ 2.3 图像质量评估 ‣ 2 问题设置与术语 ‣ 深度学习在图像超分辨率中的应用：一项调查")）中远超前述方法。
- en: Motivated by the human attention shifting mechanism [[112](#bib.bib112)], the
    Attention-FH [[113](#bib.bib113)] also adopts this strategy by resorting to a
    recurrent policy network for sequentially discovering attended patches and performing
    local enhancement. In this way, it is capable of adaptively personalizing an optimal
    searching path for each image according to its own characteristic, and thus fully
    exploits the global intra-dependence of images.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 受到人类注意力转移机制的启发[[112](#bib.bib112)]，Attention-FH [[113](#bib.bib113)]也采用了这种策略，通过使用递归策略网络来顺序发现关注的区域并进行局部增强。这样，它能够根据每张图像的特性自适应地个性化最佳搜索路径，从而充分利用图像的全局内部依赖关系。
- en: Although these methods show better performance to some extent, the recursive
    process requiring a long propagation path greatly increases the computational
    cost and training difficulty, especially for super-resolving HR images.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法在某种程度上表现更好，但需要长传播路径的递归过程大大增加了计算成本和训练难度，尤其是在超分辨率的高分辨率图像处理中。
- en: 3.3.8 Pyramid Pooling
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.8 金字塔池化
- en: Motivated by the spatial pyramid pooling layer [[114](#bib.bib114)], Zhao et
    al. [[115](#bib.bib115)] propose the pyramid pooling module to better utilize
    global and local contextual information. Specifically, for feature maps sized
    $h\times w\times c$, each feature map is divided into $M\times M$ bins, and goes
    through global average pooling, resulting in $M\times M\times c$ outputs. Then
    a $1\times 1$ convolution is performed for compressing the outputs to a single
    channel. After that, the low-dimensional feature map is upsampled to the same
    size as the original feature map via bilinear interpolation. By using different
    $M$, the module integrates global as well as local contextual information effectively.
    By incorporating this module, the proposed EDSR-PP model [[116](#bib.bib116)]
    further improve the performance over baselines.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 受到空间金字塔池化层的启发[[114](#bib.bib114)]，Zhao等人[[115](#bib.bib115)]提出了金字塔池化模块，以更好地利用全局和局部上下文信息。具体来说，对于尺寸为$h\times
    w\times c$的特征图，每个特征图被划分为$M\times M$的桶，并经过全局平均池化，得到$M\times M\times c$的输出。然后，进行$1\times
    1$卷积以将输出压缩为单通道。之后，通过双线性插值将低维特征图上采样到与原始特征图相同的尺寸。通过使用不同的$M$，该模块有效地整合了全局和局部上下文信息。通过引入这个模块，所提出的EDSR-PP模型[[116](#bib.bib116)]进一步提高了性能。
- en: 3.3.9 Wavelet Transformation
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.9 小波变换
- en: As is well-known, the wavelet transformation (WT) [[117](#bib.bib117), [118](#bib.bib118)]
    is a highly efficient representation of images by decomposing the image signal
    into high-frequency sub-bands denoting texture details and low-frequency sub-bands
    containing global topological information. Bae et al. [[119](#bib.bib119)] firstly
    combine WT with deep learning based SR model, take sub-bands of interpolated LR
    wavelet as input and predict residuals of corresponding HR sub-bands. WT and inverse
    WT are applied for decomposing the LR input and reconstructing the HR output,
    respectively. Similarly, the DWSR [[120](#bib.bib120)] and Wavelet-SRNet [[121](#bib.bib121)]
    also perform SR in the wavelet domain but with more complicated structures. In
    contrast to the above works processing each sub-band independently, the MWCNN
    [[122](#bib.bib122)] adopts multi-level WT and takes the concatenated sub-bands
    as the input to a single CNN for better capturing the dependence between them.
    Due to the efficient representation by wavelet transformation, the models using
    this strategy often much reduce the model size and computational cost, while maintain
    competitive performance [[119](#bib.bib119), [122](#bib.bib122)].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，小波变换（WT）[[117](#bib.bib117), [118](#bib.bib118)] 是通过将图像信号分解为表示纹理细节的高频子带和包含全局拓扑信息的低频子带，从而高效表示图像的技术。Bae
    等人 [[119](#bib.bib119)] 首次将 WT 与基于深度学习的 SR 模型结合，采用插值后的 LR 小波子带作为输入，预测相应 HR 子带的残差。WT
    和逆 WT 分别用于分解 LR 输入和重建 HR 输出。类似地，DWSR [[120](#bib.bib120)] 和 Wavelet-SRNet [[121](#bib.bib121)]
    也在小波域中进行 SR，但结构更加复杂。与以上各工作独立处理每个子带不同，MWCNN [[122](#bib.bib122)] 采用多级 WT，并将串联的子带作为输入提供给单一的
    CNN，以更好地捕捉它们之间的依赖关系。由于小波变换的高效表示，使用此策略的模型通常能大幅减少模型大小和计算成本，同时保持竞争力的性能 [[119](#bib.bib119),
    [122](#bib.bib122)]。
- en: 3.3.10 Desubpixel
  id: totrans-214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.10 Desubpixel
- en: 'In order to speed up the inference speed, Vu et al. [[123](#bib.bib123)] propose
    to perform the time-consuming feature extraction in a lower-dimensional space,
    and propose desubpixel, an inverse of the shuffle operation of sub-pixel layer
    (Sec. [3.2.2](#S3.SS2.SSS2 "3.2.2 Learning-based Upsampling ‣ 3.2 Upsampling Methods
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey")). Specifically, the desubpixel operation splits the images spatially,
    stacks them as extra channels and thus avoids loss of information. In this way,
    they downsample input images by desubpixel at the beginning of the model, learn
    representations in a lower-dimensional space, and upsample to the target size
    at the end. The proposed model achieves the best scores in the PIRM Challenge
    on Smartphones [[81](#bib.bib81)] with very high-speed inference and good performance.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '为了加快推理速度，Vu 等人 [[123](#bib.bib123)] 提出了在低维空间中执行耗时的特征提取，并提出了 desubpixel，这是一种子像素层的
    shuffle 操作的逆操作（见 [3.2.2](#S3.SS2.SSS2 "3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督超分辨率 ‣
    深度学习在图像超分辨率中的应用: 综述")）。具体而言，desubpixel 操作将图像空间分割，堆叠为额外的通道，从而避免了信息丢失。通过这种方式，他们在模型开始时通过
    desubpixel 对输入图像进行下采样，在低维空间中学习表示，并在最后将图像上采样到目标大小。所提模型在 PIRM Challenge on Smartphones
    [[81](#bib.bib81)] 中取得了最佳成绩，具有非常高的推理速度和良好的性能。'
- en: 3.3.11 xUnit
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.11 xUnit
- en: In order to combine spatial feature processing and nonlinear activations to
    learn complex features more efficiently, Kligvasser et al. [[124](#bib.bib124)]
    propose xUnit for learning a spatial activation function. Specifically, the ReLU
    is regarded as determining a weight map to perform element-wise multiplication
    with the input, while the xUnit directly learn the weight map through convolution
    and Gaussian gating. Although the xUnit is more computationally demanding, due
    to its dramatic effect on the performance, it allows greatly reducing the model
    size while matching the performance with ReLU. In this way, the authors reduce
    the model size by nearly 50% without any performance degradation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将空间特征处理和非线性激活结合起来，更高效地学习复杂特征，Kligvasser 等人 [[124](#bib.bib124)] 提出了 xUnit
    来学习空间激活函数。具体而言，ReLU 被视为确定一个权重图来与输入进行逐元素相乘，而 xUnit 通过卷积和高斯门控直接学习权重图。虽然 xUnit 的计算需求更高，但由于其对性能的显著影响，它允许在不降低性能的情况下大幅减少模型大小。通过这种方式，作者将模型大小减少了近
    50% 而性能未出现下降。
- en: 3.4 Learning Strategies
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 学习策略
- en: 3.4.1 Loss Functions
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1 损失函数
- en: 'In the super-resolution field, loss functions are used to measure reconstruction
    error and guide the model optimization. In early times, researchers usually employ
    the pixel-wise L2 loss, but later discover that it cannot measure the reconstruction
    quality very accurately. Therefore, a variety of loss functions (e.g., content
    loss [[29](#bib.bib29)], adversarial loss [[25](#bib.bib25)]) are adopted for
    better measuring the reconstruction error and producing more realistic and higher-quality
    results. Nowadays these loss functions have been playing an important role. In
    this section, we’ll take a closer look at the loss functions used widely. The
    notations in this section follow Sec. [2.1](#S2.SS1 "2.1 Problem Definitions ‣
    2 Problem Setting and Terminology ‣ Deep Learning for Image Super-resolution:
    A Survey"), except that we ignore the subscript $y$ of the target HR image $\hat{I_{y}}$
    and generated HR image $I_{y}$ for brevity.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '在超分辨率领域，损失函数用于衡量重建误差并指导模型优化。早期，研究人员通常使用逐像素 L2 损失，但后来发现它不能非常准确地衡量重建质量。因此，采用了多种损失函数（例如，内容损失
    [[29](#bib.bib29)]、对抗损失 [[25](#bib.bib25)]）来更好地衡量重建误差，并产生更真实、更高质量的结果。如今，这些损失函数在实践中发挥了重要作用。在本节中，我们将深入探讨广泛使用的损失函数。本节中的符号遵循第
    [2.1](#S2.SS1 "2.1 Problem Definitions ‣ 2 Problem Setting and Terminology ‣ Deep
    Learning for Image Super-resolution: A Survey") 节的定义，除了为了简洁起见，我们忽略了目标 HR 图像 $\hat{I_{y}}$
    和生成 HR 图像 $I_{y}$ 的下标 $y$。'
- en: 'Pixel Loss. Pixel loss measures pixel-wise difference between two images and
    mainly includes L1 loss (i.e., mean absolute error) and L2 loss (i.e., mean square
    error):'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 像素损失。像素损失衡量两个图像之间的逐像素差异，主要包括 L1 损失（即平均绝对误差）和 L2 损失（即均方误差）：
- en: '|  | $\displaystyle\mathcal{L}_{\text{pixel\_l1}}(\hat{I},I)$ | $\displaystyle=\frac{1}{hwc}\sum_{i,j,k}&#124;\hat{I}_{i,j,k}-I_{i,j,k}&#124;,$
    |  | (12) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{pixel\_l1}}(\hat{I},I)$ | $\displaystyle=\frac{1}{hwc}\sum_{i,j,k}&#124;\hat{I}_{i,j,k}-I_{i,j,k}&#124;,$
    |  | (12) |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{pixel\_l2}}(\hat{I},I)$ | $\displaystyle=\frac{1}{hwc}\sum_{i,j,k}(\hat{I}_{i,j,k}-I_{i,j,k})^{2},$
    |  | (13) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{pixel\_l2}}(\hat{I},I)$ | $\displaystyle=\frac{1}{hwc}\sum_{i,j,k}(\hat{I}_{i,j,k}-I_{i,j,k})^{2},$
    |  | (13) |'
- en: where $h$, $w$ and $c$ are the height, width and number of channels of the evaluated
    images, respectively. In addition, there is a variant of the pixel L1 loss, namely
    Charbonnier loss [[125](#bib.bib125), [27](#bib.bib27)], given by:.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$、$w$ 和 $c$ 分别是评估图像的高度、宽度和通道数。此外，还有一种像素 L1 损失的变体，即 Charbonnier 损失 [[125](#bib.bib125)、[27](#bib.bib27)]，其定义为：.
- en: '|  | $\mathcal{L}_{\text{pixel\_Cha}}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(\hat{I}_{i,j,k}-I_{i,j,k})^{2}+\epsilon^{2}},$
    |  | (14) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{pixel\_Cha}}(\hat{I},I)=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(\hat{I}_{i,j,k}-I_{i,j,k})^{2}+\epsilon^{2}},$
    |  | (14) |'
- en: where $\epsilon$ is a constant (e.g., $10^{-3}$) for numerical stability.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\epsilon$ 是一个常数（例如 $10^{-3}$）用于数值稳定性。
- en: 'The pixel loss constrains the generated HR image $\hat{I}$ to be close enough
    to the ground truth $I$ on the pixel values. Comparing with L1 loss, the L2 loss
    penalizes larger errors but is more tolerant to small errors, and thus often results
    in too smooth results. In practice, the L1 loss shows improved performance and
    convergence over L2 loss [[31](#bib.bib31), [126](#bib.bib126), [28](#bib.bib28)].
    Since the definition of PSNR (Sec. [2.3.1](#S2.SS3.SSS1 "2.3.1 Peak Signal-to-Noise
    Ratio ‣ 2.3 Image Quality Assessment ‣ 2 Problem Setting and Terminology ‣ Deep
    Learning for Image Super-resolution: A Survey")) is highly correlated with pixel-wise
    difference and minimizing pixel loss directly maximize PSNR, the pixel loss gradual
    becomes the most widely used loss function. However, since the pixel loss actually
    doesn’t take image quality (e.g., perceptual quality [[29](#bib.bib29)], textures
    [[8](#bib.bib8)]) into account, the results often lack high-frequency details
    and are perceptually unsatisfying with oversmooth textures [[58](#bib.bib58),
    [74](#bib.bib74), [29](#bib.bib29), [25](#bib.bib25)].'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '像素损失约束生成的 HR 图像 $\hat{I}$ 在像素值上尽可能接近真实值 $I$。与 L1 损失相比，L2 损失对较大的误差惩罚更重，但对小的误差更宽容，因此往往导致结果过于平滑。在实践中，L1
    损失比 L2 损失 [[31](#bib.bib31)、[126](#bib.bib126)、[28](#bib.bib28)] 表现出更好的性能和收敛性。由于
    PSNR 的定义（第 [2.3.1](#S2.SS3.SSS1 "2.3.1 Peak Signal-to-Noise Ratio ‣ 2.3 Image
    Quality Assessment ‣ 2 Problem Setting and Terminology ‣ Deep Learning for Image
    Super-resolution: A Survey") 节）与逐像素差异高度相关，并且最小化像素损失可以直接最大化 PSNR，因此像素损失逐渐成为最广泛使用的损失函数。然而，由于像素损失实际上并未考虑图像质量（例如，感知质量
    [[29](#bib.bib29)]、纹理 [[8](#bib.bib8)]），因此结果通常缺乏高频细节，并且感知上不令人满意，纹理过于平滑 [[58](#bib.bib58)、[74](#bib.bib74)、[29](#bib.bib29)、[25](#bib.bib25)]。'
- en: 'Content Loss. In order to evaluate perceptual quality of images, the content
    loss is introduced into SR [[127](#bib.bib127), [29](#bib.bib29)]. Specifically,
    it measures the semantic differences between images using a pre-trained image
    classification network. Denoting this network as $\phi$ and the extracted high-level
    representations on $l$-th layer as $\phi^{(l)}(I)$, the content loss is indicated
    as the Euclidean distance between high-level representations of two images, as
    follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 内容损失。为了评估图像的感知质量，内容损失被引入超分辨率（SR）[[127](#bib.bib127), [29](#bib.bib29)]。具体来说，它使用预训练的图像分类网络来衡量图像之间的语义差异。将此网络表示为$\phi$，在第$l$层提取的高层次表示表示为$\phi^{(l)}(I)$，内容损失表示为两幅图像的高层次表示之间的欧几里得距离，如下所示：
- en: '|  | $\mathcal{L}_{\text{content}}(\hat{I},I;\phi,l)=\frac{1}{h_{l}w_{l}c_{l}}\sqrt{\sum_{i,j,k}(\phi^{(l)}_{i,j,k}(\hat{I})-\phi^{(l)}_{i,j,k}(I))^{2}},$
    |  | (15) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{content}}(\hat{I},I;\phi,l)=\frac{1}{h_{l}w_{l}c_{l}}\sqrt{\sum_{i,j,k}(\phi^{(l)}_{i,j,k}(\hat{I})-\phi^{(l)}_{i,j,k}(I))^{2}},$
    |  | (15) |'
- en: where $h_{l}$, $w_{l}$ and $c_{l}$ are the height, width and number of channels
    of the representations on layer $l$, respectively.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h_{l}$、$w_{l}$和$c_{l}$分别是层$l$上表示的高度、宽度和通道数。
- en: Essentially the content loss transfers the learned knowledge of hierarchical
    image features from the classification network $\phi$ to the SR network. In contrast
    to the pixel loss, the content loss encourages the output image $\hat{I}$ to be
    perceptually similar to the target image $I$ instead of forcing them to match
    pixels exactly. Thus it produces visually more perceptible results and is also
    widely used in this field [[29](#bib.bib29), [25](#bib.bib25), [8](#bib.bib8),
    [46](#bib.bib46), [30](#bib.bib30), [103](#bib.bib103)], where the VGG [[128](#bib.bib128)]
    and ResNet [[96](#bib.bib96)] are the most commonly used pre-trained CNNs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，内容损失将从分类网络$\phi$中学到的层次化图像特征知识转移到超分辨率（SR）网络中。与像素损失相比，内容损失鼓励输出图像$\hat{I}$在感知上与目标图像$I$相似，而不是强迫它们精确匹配像素。因此，它生成的结果在视觉上更为显著，并且在这一领域被广泛使用[[29](#bib.bib29),
    [25](#bib.bib25), [8](#bib.bib8), [46](#bib.bib46), [30](#bib.bib30), [103](#bib.bib103)]，其中VGG[[128](#bib.bib128)]和ResNet[[96](#bib.bib96)]是最常用的预训练CNN。
- en: 'Texture Loss. On account that the reconstructed image should have the same
    style (e.g., colors, textures, contrast) with the target image, and motivated
    by the style representation by Gatys et al. [[129](#bib.bib129), [130](#bib.bib130)],
    the texture loss (a.k.a style reconstruction loss) is introduced into SR. Following
    [[129](#bib.bib129), [130](#bib.bib130)], the image texture is regarded as the
    correlations between different feature channels and defined as the Gram matrix
    $G^{(l)}\in\mathbb{R}^{c_{l}\times c_{l}}$, where $G^{(l)}_{ij}$ is the inner
    product between the vectorized feature maps $i$ and $j$ on layer $l$:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理损失。由于重建图像应与目标图像具有相同的风格（例如，颜色、纹理、对比度），并且受到Gatys等人[[129](#bib.bib129), [130](#bib.bib130)]的风格表示的启发，纹理损失（即风格重建损失）被引入超分辨率（SR）。根据[[129](#bib.bib129),
    [130](#bib.bib130)]，图像纹理被视为不同特征通道之间的相关性，并定义为Gram矩阵$G^{(l)}\in\mathbb{R}^{c_{l}\times
    c_{l}}$，其中$G^{(l)}_{ij}$是层$l$上特征图$i$和$j$之间的内积：
- en: '|  | $G^{(l)}_{ij}(I)=\operatorname{vec}(\phi_{i}^{(l)}(I))\cdot\operatorname{vec}(\phi_{j}^{(l)}(I)),$
    |  | (16) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $G^{(l)}_{ij}(I)=\operatorname{vec}(\phi_{i}^{(l)}(I))\cdot\operatorname{vec}(\phi_{j}^{(l)}(I)),$
    |  | (16) |'
- en: 'where $\operatorname{vec}(\cdot)$ denotes the vectorization operation, and
    $\phi_{i}^{(l)}(I)$ denotes the $i$-th channel of the feature maps on layer $l$
    of image $I$. Then the texture loss is given by:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\operatorname{vec}(\cdot)$表示向量化操作，$\phi_{i}^{(l)}(I)$表示图像$I$的第$i$个通道在层$l$上的特征图。然后，纹理损失定义为：
- en: '|  | $\mathcal{L}_{\text{texture}}(\hat{I},I;\phi,l)=\frac{1}{c_{l}^{2}}\sqrt{\sum_{i,j}(G^{(l)}_{i,j}(\hat{I})-G^{(l)}_{i,j}(I))^{2}}.$
    |  | (17) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{texture}}(\hat{I},I;\phi,l)=\frac{1}{c_{l}^{2}}\sqrt{\sum_{i,j}(G^{(l)}_{i,j}(\hat{I})-G^{(l)}_{i,j}(I))^{2}}.$
    |  | (17) |'
- en: By employing texture loss, the EnhanceNet [[8](#bib.bib8)] proposed by Sajjadi
    et al. creates much more realistic textures and produces visually more satisfactory
    results. Despite this, determining the patch size to match textures is still empirical.
    Too small patches lead to artifacts in textured regions, while too large patches
    lead to artifacts throughout the entire image because texture statistics are averaged
    over regions of varying textures.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用纹理损失，Sajjadi等人提出的EnhanceNet[[8](#bib.bib8)]创建了更为逼真的纹理，并产生了视觉上更令人满意的结果。尽管如此，确定匹配纹理的补丁大小仍然是经验性的。过小的补丁会在纹理区域产生伪影，而过大的补丁会导致整个图像产生伪影，因为纹理统计在不同纹理区域之间被平均化。
- en: 'Adversarial Loss. In recent years, due to the powerful learning ability, the
    GANs [[24](#bib.bib24)] receive more and more attention and are introduced to
    various vision tasks. To be concrete, the GAN consists of a generator performing
    generation (e.g., text generation, image transformation), and a discriminator
    which takes the generated results and instances sampled from the target distribution
    as input and discriminates whether each input comes from the target distribution.
    During training, two steps are alternately performed: (a) fix the generator and
    train the discriminator to better discriminate, (b) fix the discriminator and
    train the generator to fool the discriminator. Through adequate iterative adversarial
    training, the resulting generator can produce outputs consistent with the distribution
    of real data, while the discriminator can’t distinguish between the generated
    data and real data.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗损失。近年来，由于强大的学习能力，生成对抗网络（GANs）[[24](#bib.bib24)] 获得了越来越多的关注，并被引入到各种视觉任务中。具体来说，GAN
    由一个执行生成（例如，文本生成、图像转换）的生成器和一个将生成结果与从目标分布中采样的实例作为输入并区分每个输入是否来自目标分布的鉴别器组成。在训练过程中，交替进行两个步骤：（a）固定生成器并训练鉴别器以更好地区分，（b）固定鉴别器并训练生成器以欺骗鉴别器。通过充分的迭代对抗训练，得到的生成器能够产生与真实数据分布一致的输出，而鉴别器则无法区分生成的数据和真实数据。
- en: 'In terms of super-resolution, it is straightforward to adopt adversarial learning,
    in which case we only need to treat the SR model as a generator, and define an
    extra discriminator to judge whether the input image is generated or not. Therefore,
    Ledig et al. [[25](#bib.bib25)] firstly propose SRGAN using adversarial loss based
    on cross entropy, as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在超分辨率方面，采用对抗学习是直接的，在这种情况下，我们只需将 SR 模型视为生成器，并定义一个额外的鉴别器来判断输入图像是否为生成图像。因此，Ledig
    等 [[25](#bib.bib25)] 首先提出了基于交叉熵的对抗损失的 SRGAN，如下所示：
- en: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ce\_g}}(\hat{I};D)$ | $\displaystyle=-\log
    D(\hat{I}),$ |  | (18) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ce\_g}}(\hat{I};D)$ | $\displaystyle=-\log
    D(\hat{I}),$ |  | (18) |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ce\_d}}(\hat{I},I_{s};D)$ | $\displaystyle=-\log
    D(I_{s})-\log(1-D(\hat{I})),$ |  | (19) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ce\_d}}(\hat{I},I_{s};D)$ | $\displaystyle=-\log
    D(I_{s})-\log(1-D(\hat{I})),$ |  | (19) |'
- en: where $\mathcal{L}_{\text{gan\_ce\_g}}$ and $\mathcal{L}_{\text{gan\_ce\_d}}$
    denote the adversarial loss of the generator (i.e., the SR model) and the discriminator
    $D$ (i.e., a binary classifier), respectively, and $I_{s}$ represents images randomly
    sampled from the ground truths. Besides, the Enhancenet [[8](#bib.bib8)] also
    adopts the similar adversarial loss.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{\text{gan\_ce\_g}}$ 和 $\mathcal{L}_{\text{gan\_ce\_d}}$ 分别表示生成器（即
    SR 模型）和鉴别器 $D$（即二分类器）的对抗损失，$I_{s}$ 表示从真实数据中随机采样的图像。此外，Enhancenet [[8](#bib.bib8)]
    也采用了类似的对抗损失。
- en: 'Besides, Wang et al. [[32](#bib.bib32)] and Yuan et al. [[131](#bib.bib131)]
    use adversarial loss based on least square error for more stable training process
    and higher quality results [[132](#bib.bib132)], given by:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Wang 等 [[32](#bib.bib32)] 和 Yuan 等 [[131](#bib.bib131)] 使用基于最小二乘误差的对抗损失，以获得更稳定的训练过程和更高质量的结果
    [[132](#bib.bib132)]，如以下所示：
- en: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ls\_g}}(\hat{I};D)$ | $\displaystyle=(D(\hat{I})-1)^{2},$
    |  | (20) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ls\_g}}(\hat{I};D)$ | $\displaystyle=(D(\hat{I})-1)^{2},$
    |  | (20) |'
- en: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ls\_d}}(\hat{I},I_{s};D)$ | $\displaystyle=(D(\hat{I}))^{2}+(D(I_{s})-1)^{2}.$
    |  | (21) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{gan\_ls\_d}}(\hat{I},I_{s};D)$ | $\displaystyle=(D(\hat{I}))^{2}+(D(I_{s})-1)^{2}.$
    |  | (21) |'
- en: In contrast to the above works focusing on the specific forms of adversarial
    loss, Park et al. [[133](#bib.bib133)] argue that the pixel-level discriminator
    causes generating meaningless high-frequency noise, and attach another feature-level
    discriminator to operate on high-level representations extracted by a pre-trained
    CNN which captures more meaningful attributes of real HR images. Xu et al. [[63](#bib.bib63)]
    incorporate a multi-class GAN consisting of a generator and multiple class-specific
    discriminators. And the ESRGAN [[103](#bib.bib103)] employs relativistic GAN [[134](#bib.bib134)]
    to predict the probability that real images are relatively more realistic than
    fake ones, instead of the probability that input images are real or fake, and
    thus guide recovering more detailed textures.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 与以上专注于特定对抗损失形式的工作不同，Park 等人[[133](#bib.bib133)] 认为像素级判别器会产生无意义的高频噪声，并附加另一个特征级判别器，以处理由预训练的
    CNN 提取的高层表示，这些表示捕获了真实 HR 图像的更有意义的属性。Xu 等人[[63](#bib.bib63)] 结合了一个包含生成器和多个类别特定判别器的多类别
    GAN。ESRGAN [[103](#bib.bib103)] 使用相对GAN [[134](#bib.bib134)] 预测真实图像相对于假图像的相对真实性概率，而不是输入图像是现实还是虚假的概率，从而引导恢复更多细节纹理。
- en: 'Extensive MOS tests (Sec. [2.3.3](#S2.SS3.SSS3 "2.3.3 Mean Opinion Score ‣
    2.3 Image Quality Assessment ‣ 2 Problem Setting and Terminology ‣ Deep Learning
    for Image Super-resolution: A Survey")) show that even though the SR models trained
    with adversarial loss and content loss achieve lower PSNR compared to those trained
    with pixel loss, they bring significant gains in perceptual quality [[25](#bib.bib25),
    [8](#bib.bib8)]. As a matter of fact, the discriminator extracts some difficult-to-learn
    latent patterns of real HR images, and pushes the generated HR images to conform,
    thus helps to generate more realistic images. However, currently the training
    process of GAN is still difficult and unstable. Although there have been some
    studies on how to stabilize the GAN training [[135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137)], how to ensure that the GANs integrated into SR models are
    trained correctly and play an active role remains a problem.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '大量的 MOS 测试（见第 [2.3.3](#S2.SS3.SSS3 "2.3.3 Mean Opinion Score ‣ 2.3 Image Quality
    Assessment ‣ 2 Problem Setting and Terminology ‣ Deep Learning for Image Super-resolution:
    A Survey") 节）显示，尽管使用对抗损失和内容损失训练的 SR 模型相较于使用像素损失训练的模型在 PSNR 上表现较低，但它们在感知质量上带来了显著的提升
    [[25](#bib.bib25), [8](#bib.bib8)]。实际上，判别器提取了真实 HR 图像的一些难以学习的潜在模式，并推动生成的 HR 图像符合这些模式，从而帮助生成更逼真的图像。然而，目前
    GAN 的训练过程仍然困难且不稳定。尽管已有一些关于如何稳定 GAN 训练的研究 [[135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137)]，但如何确保集成到 SR 模型中的 GAN 得到正确训练并发挥积极作用仍然是一个问题。'
- en: 'Cycle Consistency Loss. Motivated by the CycleGAN proposed by Zhu et al. [[138](#bib.bib138)],
    Yuan et al. [[131](#bib.bib131)] present a cycle-in-cycle approach for super-resolution.
    Concretely speaking, they not only super-resolve the LR image $I$ to the HR image
    $\hat{I}$ but also downsample $\hat{I}$ back to another LR image $I^{\prime}$
    through another CNN. The regenerated $I^{\prime}$ is required to be identical
    to the input $I$, thus the cycle consistency loss is introduced for constraining
    their pixel-level consistency:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失（Cycle Consistency Loss）。受到 Zhu 等人提出的 CycleGAN [[138](#bib.bib138)] 的启发，Yuan
    等人[[131](#bib.bib131)] 提出了一个用于超分辨率的循环-in-循环方法。具体而言，他们不仅将低分辨率（LR）图像 $I$ 超分辨率到高分辨率（HR）图像
    $\hat{I}$，还通过另一个卷积神经网络（CNN）将 $\hat{I}$ 下采样回到另一个 LR 图像 $I^{\prime}$。再生成的 $I^{\prime}$
    要求与输入的 $I$ 完全一致，因此引入了循环一致性损失来约束它们的像素级一致性：
- en: '|  | $\mathcal{L}_{\text{cycle}}(I^{\prime},I)=\frac{1}{hwc}\sqrt{\sum_{i,j,k}(I^{\prime}_{i,j,k}-I_{i,j,k})^{2}}.$
    |  | (22) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{cycle}}(I^{\prime},I)=\frac{1}{hwc}\sqrt{\sum_{i,j,k}(I^{\prime}_{i,j,k}-I_{i,j,k})^{2}}.$
    |  | (22) |'
- en: 'Total Variation Loss. In order to suppress noise in generated images, the total
    variation (TV) loss [[139](#bib.bib139)] is introduced into SR by Aly et al. [[140](#bib.bib140)].
    It is defined as the sum of the absolute differences between neighboring pixels
    and measures how much noise is in the images, as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 总变差损失（Total Variation Loss）。为了抑制生成图像中的噪声，Aly 等人[[140](#bib.bib140)]将总变差（TV）损失引入超分辨率（SR）。它被定义为相邻像素之间绝对差值的总和，并衡量图像中噪声的多少，如下所示：
- en: '|  | $\mathcal{L}_{\text{TV}}(\hat{I})=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(\hat{I}_{i,j+1,k}-\hat{I}_{i,j,k})^{2}+(\hat{I}_{i+1,j,k}-\hat{I}_{i,j,k})^{2}}.$
    |  | (23) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{TV}}(\hat{I})=\frac{1}{hwc}\sum_{i,j,k}\sqrt{(\hat{I}_{i,j+1,k}-\hat{I}_{i,j,k})^{2}+(\hat{I}_{i+1,j,k}-\hat{I}_{i,j,k})^{2}}.$
    |  | (23) |'
- en: Lai et al. [[25](#bib.bib25)] and Yuan et al. [[131](#bib.bib131)] also adopt
    the TV loss for imposing spatial smoothness.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Lai等人[[25](#bib.bib25)]和Yuan等人[[131](#bib.bib131)]也采用了TV损失以施加空间平滑。
- en: Prior-Based Loss. In addition to the above loss functions, external prior knowledge
    is also introduced to constrain the generation. Specifically, Bulat et al. [[30](#bib.bib30)]
    focus on face image SR and introduce a face alignment network (FAN) to constrain
    the consistency of facial landmarks. The FAN is pre-trained and integrated for
    providing face alignment priors, and then trained jointly with the SR. In this
    way, the proposed Super-FAN improves performance both on LR face alignment and
    face image SR.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 基于先验的损失。除了上述损失函数外，还引入了外部先验知识来约束生成过程。具体来说，Bulat等人[[30](#bib.bib30)]关注于面部图像的超分辨率，并引入了一个面部对齐网络（FAN）以约束面部标志点的一致性。FAN被预训练并集成以提供面部对齐的先验，然后与超分辨率一起联合训练。通过这种方式，提出的Super-FAN在低分辨率面部对齐和面部图像超分辨率方面都提高了性能。
- en: As a matter of fact, the content loss and the texture loss, both of which introduce
    a classification network, essentially provide prior knowledge of hierarchical
    image features for SR. By introducing more prior knowledge, the SR performance
    can be further improved.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，内容损失和纹理损失都引入了分类网络，实质上为超分辨率提供了层次图像特征的先验知识。通过引入更多的先验知识，可以进一步提高超分辨率的性能。
- en: In this section, we introduce various loss functions for SR. In practice, researchers
    often combine multiple loss functions by weighted average [[27](#bib.bib27), [25](#bib.bib25),
    [8](#bib.bib8), [141](#bib.bib141), [46](#bib.bib46)] for constraining different
    aspects of the generation process, especially for distortion-perception tradeoff
    [[25](#bib.bib25), [142](#bib.bib142), [103](#bib.bib103), [143](#bib.bib143),
    [144](#bib.bib144)]. However, the weights of different loss functions require
    a lot of empirical exploration, and how to combine reasonably and effectively
    remains a problem.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了用于超分辨率（SR）的各种损失函数。在实践中，研究人员通常通过加权平均[[27](#bib.bib27), [25](#bib.bib25),
    [8](#bib.bib8), [141](#bib.bib141), [46](#bib.bib46)]结合多个损失函数，以约束生成过程的不同方面，特别是对于失真感知的权衡[[25](#bib.bib25),
    [142](#bib.bib142), [103](#bib.bib103), [143](#bib.bib143), [144](#bib.bib144)]。然而，不同损失函数的权重需要大量的实证探索，如何合理有效地结合这些函数仍然是一个问题。
- en: 3.4.2 Batch Normalization
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2 批归一化
- en: In order to accelerate and stabilize training of deep CNNs, Sergey et al. [[145](#bib.bib145)]
    propose batch normalization (BN) to reduce internal covariate shift of networks.
    Specifically, they perform normalization for each mini-batch and train two extra
    transformation parameters for each channel to preserve the representation ability.
    Since the BN calibrates the intermediate feature distribution and mitigates vanishing
    gradients, it allows using higher learning rates and being less careful about
    initialization. Thus this technique is widely used by SR models [[56](#bib.bib56),
    [25](#bib.bib25), [55](#bib.bib55), [146](#bib.bib146), [39](#bib.bib39), [122](#bib.bib122)].
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速和稳定深度卷积神经网络（CNN）的训练，Sergey等人[[145](#bib.bib145)]提出了批归一化（BN），以减少网络的内部协方差偏移。具体来说，他们对每个小批量进行归一化，并为每个通道训练两个额外的变换参数，以保持表示能力。由于BN校准了中间特征分布，并减轻了梯度消失，它允许使用更高的学习率，并对初始化的要求较低。因此，这项技术被广泛应用于超分辨率模型[[56](#bib.bib56),
    [25](#bib.bib25), [55](#bib.bib55), [146](#bib.bib146), [39](#bib.bib39), [122](#bib.bib122)]。
- en: However, Lim et al. [[31](#bib.bib31)] argue that the BN loses the scale information
    of each image and gets rid of range flexibility from networks. So they remove
    BN and use the saved memory cost (up to 40%) to develop a much larger model, and
    thus increase the performance substantially. Some other models [[32](#bib.bib32),
    [147](#bib.bib147), [103](#bib.bib103)] also adopt this experience and achieve
    performance improvements.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Lim等人[[31](#bib.bib31)]认为BN丢失了每个图像的尺度信息，并且去除了网络的范围灵活性。因此，他们移除了BN，并利用节省的内存成本（高达40%）开发了一个更大的模型，从而显著提高了性能。一些其他模型[[32](#bib.bib32),
    [147](#bib.bib147), [103](#bib.bib103)]也采纳了这一经验，并取得了性能改进。
- en: 3.4.3 Curriculum Learning
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.3 课程学习
- en: Curriculum learning [[148](#bib.bib148)] refers to starting from an easier task
    and gradually increasing the difficulty. Since super-resolution is an ill-posed
    problem and always suffers adverse conditions such as large scaling factors, noise
    and blurring, the curriculum training is incorporated for reducing learning difficulty.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习[[148](#bib.bib148)]指的是从较简单的任务开始，逐渐增加难度。由于超分辨率是一个不适定问题，且总是面临大尺度因子、噪声和模糊等不利条件，因此引入课程训练以降低学习难度。
- en: 'In order to reduce the difficulty of SR with large scaling factors, Wang et
    al. [[32](#bib.bib32)], Bei et al. [[149](#bib.bib149)] and Ahn et al. [[150](#bib.bib150)]
    propose ProSR, ADRSR and progressive CARN, respectively, which are progressive
    not only on architectures (Sec. [3.1.3](#S3.SS1.SSS3 "3.1.3 Progressive Upsampling
    Super-resolution ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")) but also on training procedure.
    The training starts with the $2\times$ upsampling, and after finishing training,
    the portions with $4\times$ or larger scaling factors are gradually mounted and
    blended with the previous portions. Specifically, the ProSR blends by linearly
    combining the output of this level and the upsampled output of previous levels
    following [[151](#bib.bib151)], the ADRSR concatenates them and attaches another
    convolutional layer, while the progressive CARN replace the previous reconstruction
    block with the one that produces the image in double resolution.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '为了减少大规模因子的 SR 难度，Wang 等人 [[32](#bib.bib32)]、Bei 等人 [[149](#bib.bib149)] 和 Ahn
    等人 [[150](#bib.bib150)] 分别提出了 ProSR、ADRSR 和 Progressive CARN，这些方法不仅在架构上（见 [3.1.3](#S3.SS1.SSS3
    "3.1.3 Progressive Upsampling Super-resolution ‣ 3.1 Super-resolution Frameworks
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey")）是渐进的，而且在训练过程中也是渐进的。训练从 $2\times$ 上采样开始，训练完成后，具有 $4\times$ 或更大缩放因子的部分逐渐加入并与之前的部分混合。具体而言，ProSR
    通过线性结合此级别的输出和之前级别的上采样输出来混合，参见 [[151](#bib.bib151)]，ADRSR 将它们连接并附加另一卷积层，而 Progressive
    CARN 用生成双倍分辨率图像的重建块替代之前的重建块。'
- en: In addition, Park et al. [[116](#bib.bib116)] divide the $8\times$ SR problem
    to three sub-problems (i.e., $1\times$ to $2\times$, $2\times$ to $4\times$, $4\times$
    to $8\times$) and train independent networks for each problem. Then two of them
    are concatenated and fine-tuned, and then with the third one. Besides, they also
    decompose the $4\times$ SR under difficult conditions into $1\times$ to $2\times$,
    $2\times$ to $4\times$ and denoising or deblurring sub-problems. In contrast,
    the SRFBN [[86](#bib.bib86)] uses this strategy for SR under adverse conditions,
    i.e., starting from easy degradation and gradually increasing degradation complexity.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Park 等人 [[116](#bib.bib116)] 将 $8\times$ SR 问题分解为三个子问题（即 $1\times$ 到 $2\times$、$2\times$
    到 $4\times$、$4\times$ 到 $8\times$），并为每个问题训练独立的网络。然后将其中两个网络连接并微调，接着与第三个网络进行合并。此外，他们还将
    $4\times$ SR 在困难条件下分解为 $1\times$ 到 $2\times$、$2\times$ 到 $4\times$ 和去噪或去模糊子问题。相比之下，SRFBN
    [[86](#bib.bib86)] 在不利条件下使用了这种策略，即从简单的降质开始，逐渐增加降质复杂性。
- en: Compared to common training procedure, the curriculum learning greatly reduces
    the training difficulty and shortens the total training time, especially for large
    factors.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 与常见的训练过程相比，课程学习大大降低了训练难度并缩短了总训练时间，尤其适用于大规模因子。
- en: 3.4.4 Multi-supervision
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.4 多重监督
- en: 'Multi-supervision refers to adding multiple supervision signals within the
    model for enhancing the gradient propagation and avoiding vanishing and exploding
    gradients. In order to prevent the gradient problems introduced by recursive learning
    (Sec. [3.3.2](#S3.SS3.SSS2 "3.3.2 Recursive Learning ‣ 3.3 Network Design ‣ 3
    Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")),
    the DRCN [[82](#bib.bib82)] incorporates multi-supervision with recursive units.
    Specifically, they feed each output of recursive units into a reconstruction module
    to generate an HR image, and build the final prediction by incorporating all the
    intermediate reconstructions. Similar strategies are also taken by MemNet [[55](#bib.bib55)]
    and DSRN [[85](#bib.bib85)], which are also based on recursive learning.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '多重监督指的是在模型内添加多个监督信号，以增强梯度传播并避免梯度消失和梯度爆炸。为了防止递归学习引入的梯度问题（见 [3.3.2](#S3.SS3.SSS2
    "3.3.2 Recursive Learning ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")），DRCN [[82](#bib.bib82)]
    将多重监督与递归单元结合。具体而言，他们将每个递归单元的输出输入到重建模块以生成 HR 图像，并通过结合所有中间重建来构建最终预测。类似的策略也被 MemNet
    [[55](#bib.bib55)] 和 DSRN [[85](#bib.bib85)] 采用，这些方法也基于递归学习。'
- en: 'Besides, since the LapSRN [[27](#bib.bib27), [65](#bib.bib65)] under the progressive
    upsampling framework (Sec. [3.1.3](#S3.SS1.SSS3 "3.1.3 Progressive Upsampling
    Super-resolution ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")) generates intermediate
    results of different scales during propagation, it is straightforward to adopt
    multi-supervision strategy. Specifically, the intermediate results are forced
    to be the same as the intermediate images downsampled from the ground truth HR
    images.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于LapSRN[[27](#bib.bib27), [65](#bib.bib65)]在渐进上采样框架（第[3.1.3节](#S3.SS1.SSS3
    "3.1.3 渐进上采样超分辨率 ‣ 3.1 超分辨率框架 ‣ 3 超分辨率 ‣ 深度学习图像超分辨率综述")）下在传播过程中生成不同尺度的中间结果，因此采用多重监督策略非常直接。具体而言，这些中间结果被强制与从真实高分辨率（HR）图像下采样的中间图像相同。
- en: In practice, this multi-supervision technique is often implemented by adding
    some terms in the loss function, and in this way, the supervision signals are
    back-propagated more effectively, and thus reduce the training difficulty and
    enhance the model training.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种多重监督技术通常通过在损失函数中添加一些项来实现，从而使得监督信号得以更有效地反向传播，从而降低训练难度并增强模型训练。
- en: 3.5 Other Improvements
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 其他改进
- en: In addition to the network design and learning strategies, there are other techniques
    further improving SR models.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 除了网络设计和学习策略，还有其他技术可以进一步提升SR模型。
- en: 3.5.1 Context-wise Network Fusion
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 上下文网络融合
- en: 'Context-wise network fusion (CNF) [[100](#bib.bib100)] refers to a stacking
    technique fusing predictions from multiple SR networks (i.e., a special case of
    multi-path learning in Sec. [3.3.3](#S3.SS3.SSS3 "3.3.3 Multi-path Learning ‣
    3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")). To be concrete, they train individual SR models with different architectures
    separately, feed the prediction of each model into individual convolutional layers,
    and finally sum the outputs up to be the final prediction result. Within this
    CNF framework, the final model constructed by three lightweight SRCNNs [[22](#bib.bib22),
    [23](#bib.bib23)] achieves comparable performance with state-of-the-art models
    with acceptable efficiency [[100](#bib.bib100)].'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文网络融合（CNF）[[100](#bib.bib100)] 指的是一种堆叠技术，通过融合多个超分辨率（SR）网络的预测（即第[3.3.3节](#S3.SS3.SSS3
    "3.3.3 多路径学习 ‣ 3.3 网络设计 ‣ 3 超分辨率 ‣ 深度学习图像超分辨率综述")中多路径学习的特殊情况）。具体来说，他们分别训练不同架构的单独SR模型，将每个模型的预测结果输入到各自的卷积层中，最后将这些输出相加，得到最终的预测结果。在这个CNF框架内，由三个轻量级SRCNNs构建的最终模型[[22](#bib.bib22),
    [23](#bib.bib23)]在保持可接受效率的同时，表现与最先进的模型相当[[100](#bib.bib100)]。
- en: 3.5.2 Data Augmentation
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 数据增强
- en: Data augmentation is one of the most widely used techniques for boosting performance
    with deep learning. For image super-resolution, some useful augmentation options
    include cropping, flipping, scaling, rotation, color jittering, etc. [[44](#bib.bib44),
    [31](#bib.bib31), [27](#bib.bib27), [56](#bib.bib56), [85](#bib.bib85), [98](#bib.bib98)].
    In addition, Bei et al. [[149](#bib.bib149)] also randomly shuffle RGB channels,
    which not only augments data, but also alleviates color bias caused by the dataset
    with color unbalance.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强是提升深度学习性能的最广泛使用的技术之一。对于图像超分辨率，一些有用的增强选项包括裁剪、翻转、缩放、旋转、颜色抖动等[[44](#bib.bib44),
    [31](#bib.bib31), [27](#bib.bib27), [56](#bib.bib56), [85](#bib.bib85), [98](#bib.bib98)]。此外，Bei等人[[149](#bib.bib149)]还随机打乱RGB通道，这不仅增强了数据，而且缓解了由于数据集色彩不平衡造成的色彩偏差。
- en: 3.5.3 Multi-task Learning
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 多任务学习
- en: Multi-task learning [[152](#bib.bib152)] refers to improving generalization
    ability by leveraging domain-specific information contained in training signals
    of related tasks, such as object detection and semantic segmentation [[153](#bib.bib153)],
    head pose estimation and facial attribute inference [[154](#bib.bib154)]. In the
    SR field, Wang et al. [[46](#bib.bib46)] incorporate a semantic segmentation network
    for providing semantic knowledge and generating semantic-specific details. Specifically,
    they propose spatial feature transformation to take semantic maps as input and
    predict spatial-wise parameters of affine transformation performed on the intermediate
    feature maps. The proposed SFT-GAN thus generates more realistic and visually
    pleasing textures on images with rich semantic regions. Besides, considering that
    directly super-resolving noisy images may cause noise amplification, the DNSR
    [[149](#bib.bib149)] proposes to train a denoising network and an SR network separately,
    then concatenates them and fine-tunes together. Similarly, the cycle-in-cycle
    GAN (CinCGAN) [[131](#bib.bib131)] combines a cycle-in-cycle denoising framework
    and a cycle-in-cycle SR model to joint perform noise reduction and super-resolution.
    Since different tasks tend to focus on different aspects of the data, combining
    related tasks with SR models usually improves the SR performance by providing
    extra information and knowledge.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习 [[152](#bib.bib152)] 指通过利用相关任务的训练信号中包含的领域特定信息来提高泛化能力，例如目标检测和语义分割 [[153](#bib.bib153)]、头部姿态估计和面部属性推断
    [[154](#bib.bib154)]。在SR领域，Wang 等人 [[46](#bib.bib46)] 将语义分割网络融入以提供语义知识并生成语义特定细节。具体来说，他们提出了空间特征变换，将语义图作为输入，并预测对中间特征图执行的仿射变换的空间参数。因此，所提出的SFT-GAN在具有丰富语义区域的图像上生成了更真实和视觉上令人愉悦的纹理。此外，考虑到直接对噪声图像进行超分辨率可能会导致噪声放大，DNSR
    [[149](#bib.bib149)] 提出分别训练去噪网络和SR网络，然后将它们连接并共同微调。类似地，循环中的循环GAN（CinCGAN） [[131](#bib.bib131)]
    结合了一个循环中的去噪框架和一个循环中的SR模型，以联合执行噪声减少和超分辨率。由于不同任务往往关注数据的不同方面，将相关任务与SR模型结合通常通过提供额外的信息和知识来改善SR性能。
- en: 3.5.4 Network Interpolation
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.4 网络插值
- en: PSNR-based models produce images closer to ground truths but introduce blurring
    problems, while GAN-based models bring better perceptual quality but introduce
    unpleasant artifacts (e.g., meaningless noise making images more “realistic”).
    In order to better balance the distortion and perception, Wang et al. [[103](#bib.bib103),
    [155](#bib.bib155)] propose a network interpolation strategy. Specifically, they
    train a PSNR-based model and train a GAN-based model by fine-tuning, then interpolate
    all the corresponding parameters of both networks to derive intermediate models.
    By tuning the interpolation weights without retraining networks, they produce
    meaningful results with much less artifacts.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 基于PSNR的模型生成的图像更接近真实值，但会引入模糊问题，而基于GAN的模型则提供了更好的感知质量，但会引入令人不悦的伪影（例如，使图像更“真实”的无意义噪声）。为了更好地平衡失真和感知，Wang
    等人 [[103](#bib.bib103), [155](#bib.bib155)] 提出了网络插值策略。具体来说，他们训练了一个基于PSNR的模型，并通过微调训练了一个基于GAN的模型，然后插值两个网络的所有对应参数，以得出中间模型。通过调整插值权重而不重新训练网络，他们生成了有意义的结果，并且伪影大大减少。
- en: 3.5.5 Self-Ensemble
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.5 自集成
- en: Self-ensemble, a.k.a. enhanced prediction [[44](#bib.bib44)], is an inference
    technique commonly used by SR models. Specifically, rotations with different angles
    (0^∘, 90^∘, 180^∘, 270^∘) and horizontal flipping are applied on the LR images
    to get a set of 8 images. Then these images are fed into the SR model and the
    corresponding inverse transformation is applied to the reconstructed HR images
    to get the outputs. The final prediction result is conducted by the mean [[44](#bib.bib44),
    [78](#bib.bib78), [31](#bib.bib31), [93](#bib.bib93), [70](#bib.bib70), [32](#bib.bib32)]
    or the median [[83](#bib.bib83)] of these outputs. In this way, these models further
    improve performance.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 自集成，也称为增强预测 [[44](#bib.bib44)]，是一种SR模型常用的推断技术。具体来说，对LR图像进行不同角度（0^∘、90^∘、180^∘、270^∘）的旋转和水平翻转，得到一组8张图像。然后，这些图像被输入到SR模型中，并对重建的HR图像应用相应的反变换以获得输出。最终的预测结果是通过这些输出的均值
    [[44](#bib.bib44), [78](#bib.bib78), [31](#bib.bib31), [93](#bib.bib93), [70](#bib.bib70),
    [32](#bib.bib32)] 或中位数 [[83](#bib.bib83)] 得出的。通过这种方式，这些模型进一步提高了性能。
- en: 'TABLE II: Super-resolution methodology employed by some representative models.
    The “Fw.”, “Up.”, “Rec.”, “Res.”, “Dense.”, “Att.” represent SR frameworks, upsampling
    methods, recursive learning, residual learning, dense connections, attention mechanism,
    respectively.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 一些代表性模型使用的超分辨率方法。“前向”，“上采样”，“递归”，“残差”，“稠密”，“注意力”分别代表SR框架、上采样方法、递归学习、残差学习、稠密连接和注意力机制。'
- en: '| Method | Publication | Fw. | Up. | Rec. | Res. | Dense | Att. | $\mathcal{L}_{\text{L1}}$
    | $\mathcal{L}_{\text{L2}}$ | Keywords |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 出版 | 前向 | 上采样 | 递归 | 残差 | 稠密 | 注意力 | $\mathcal{L}_{\text{L1}}$ | $\mathcal{L}_{\text{L2}}$
    | 关键词 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SRCNN [[22](#bib.bib22)] | 2014, ECCV | Pre. | Bicubic |  |  |  |  |  | ✓
    |  |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| SRCNN [[22](#bib.bib22)] | 2014, ECCV | 预处理 | Bicubic |  |  |  |  |  | ✓
    |  |'
- en: '| DRCN [[82](#bib.bib82)] | 2016, CVPR | Pre. | Bicubic | ✓ | ✓ |  |  |  |
    ✓ | Recursive layers |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| DRCN [[82](#bib.bib82)] | 2016, CVPR | 预处理 | Bicubic | ✓ | ✓ |  |  |  | ✓
    | 递归层 |'
- en: '| FSRCNN [[43](#bib.bib43)] | 2016, ECCV | Post. | Deconv |  |  |  |  |  |
    ✓ | Lightweight design |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| FSRCNN [[43](#bib.bib43)] | 2016, ECCV | 后处理 | 反卷积 |  |  |  |  |  | ✓ | 轻量级设计
    |'
- en: '| ESPCN [[156](#bib.bib156)] | 2017, CVPR | Pre. | Sub-Pixel |  |  |  |  |  |
    ✓ | Sub-pixel |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| ESPCN [[156](#bib.bib156)] | 2017, CVPR | 预处理 | 子像素 |  |  |  |  |  | ✓ |
    子像素 |'
- en: '| LapSRN [[27](#bib.bib27)] | 2017, CVPR | Pro. | Bicubic |  | ✓ |  |  | ✓
    |  | $\mathcal{L}_{\text{pixel\_Cha}}$ |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LapSRN [[27](#bib.bib27)] | 2017, CVPR | Pro. | Bicubic |  | ✓ |  |  | ✓
    |  | $\mathcal{L}_{\text{pixel\_Cha}}$ |'
- en: '| DRRN [[56](#bib.bib56)] | 2017, CVPR | Pre. | Bicubic | ✓ | ✓ |  |  |  |
    ✓ | Recursive blocks |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DRRN [[56](#bib.bib56)] | 2017, CVPR | 预处理 | Bicubic | ✓ | ✓ |  |  |  | ✓
    | 递归块 |'
- en: '| SRResNet [[25](#bib.bib25)] | 2017, CVPR | Post. | Sub-Pixel |  | ✓ |  |  |  |
    ✓ | $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{TV}}$ |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| SRResNet [[25](#bib.bib25)] | 2017, CVPR | 后处理 | 子像素 |  | ✓ |  |  |  | ✓
    | $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{TV}}$ |'
- en: '| SRGAN [[25](#bib.bib25)] | 2017, CVPR | Post. | Sub-Pixel |  | ✓ |  |  |  |  |
    $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{GAN}}$ |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| SRGAN [[25](#bib.bib25)] | 2017, CVPR | 后处理 | 子像素 |  | ✓ |  |  |  |  | $\mathcal{L}_{\text{Con.}}$,
    $\mathcal{L}_{\text{GAN}}$ |'
- en: '| EDSR [[31](#bib.bib31)] | 2017, CVPRW | Post. | Sub-Pixel |  | ✓ |  |  |
    ✓ |  | Compact and large-size design |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| EDSR [[31](#bib.bib31)] | 2017, CVPRW | 后处理 | 子像素 |  | ✓ |  |  | ✓ |  | 紧凑且大尺寸设计
    |'
- en: '| EnhanceNet [[8](#bib.bib8)] | 2017, ICCV | Pre. | Bicubic |  | ✓ |  |  |  |  |
    $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{GAN}}$, $\mathcal{L}_{\text{texture}}$
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| EnhanceNet [[8](#bib.bib8)] | 2017, ICCV | 预处理 | Bicubic |  | ✓ |  |  |  |  |
    $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{GAN}}$, $\mathcal{L}_{\text{texture}}$
    |'
- en: '| MemNet [[55](#bib.bib55)] | 2017, ICCV | Pre. | Bicubic | ✓ | ✓ | ✓ |  |  |
    ✓ | Memory block |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| MemNet [[55](#bib.bib55)] | 2017, ICCV | 预处理 | Bicubic | ✓ | ✓ | ✓ |  |  |
    ✓ | 记忆块 |'
- en: '| SRDenseNet [[79](#bib.bib79)] | 2017, ICCV | Post. | Deconv |  | ✓ | ✓ |  |  |
    ✓ | Dense connections |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| SRDenseNet [[79](#bib.bib79)] | 2017, ICCV | 后处理 | 反卷积 |  | ✓ | ✓ |  |  |
    ✓ | 稠密连接 |'
- en: '| DBPN [[57](#bib.bib57)] | 2018, CVPR | Iter. | Deconv |  | ✓ | ✓ |  |  |
    ✓ | Back-projection |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| DBPN [[57](#bib.bib57)] | 2018, CVPR | 迭代 | 反卷积 |  | ✓ | ✓ |  |  | ✓ | 反向投影
    |'
- en: '| DSRN [[85](#bib.bib85)] | 2018, CVPR | Pre. | Deconv | ✓ | ✓ |  |  |  | ✓
    | Dual state |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| DSRN [[85](#bib.bib85)] | 2018, CVPR | 预处理 | 反卷积 | ✓ | ✓ |  |  |  | ✓ | 双状态
    |'
- en: '| RDN [[93](#bib.bib93)] | 2018, CVPR | Post. | Sub-Pixel |  | ✓ | ✓ |  | ✓
    |  | Residual dense block |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| RDN [[93](#bib.bib93)] | 2018, CVPR | 后处理 | 子像素 |  | ✓ | ✓ |  | ✓ |  | 残差稠密块
    |'
- en: '| CARN [[28](#bib.bib28)] | 2018, ECCV | Post. | Sub-Pixel | ✓ | ✓ | ✓ |  |
    ✓ |  | Cascading |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| CARN [[28](#bib.bib28)] | 2018, ECCV | 后处理 | 子像素 | ✓ | ✓ | ✓ |  | ✓ |  |
    级联 |'
- en: '| MSRN [[99](#bib.bib99)] | 2018, ECCV | Post. | Sub-Pixel |  | ✓ |  |  | ✓
    |  | Multi-path |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| MSRN [[99](#bib.bib99)] | 2018, ECCV | 后处理 | 子像素 |  | ✓ |  |  | ✓ |  | 多路径
    |'
- en: '| RCAN [[70](#bib.bib70)] | 2018, ECCV | Post. | Sub-Pixel |  | ✓ |  | ✓ |
    ✓ |  | Channel attention |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| RCAN [[70](#bib.bib70)] | 2018, ECCV | 后处理 | 子像素 |  | ✓ |  | ✓ | ✓ |  | 通道注意力
    |'
- en: '| ESRGAN [[103](#bib.bib103)] | 2018, ECCVW | Post. | Sub-Pixel |  | ✓ | ✓
    |  | ✓ |  | $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{GAN}}$ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| ESRGAN [[103](#bib.bib103)] | 2018, ECCVW | 后处理 | 子像素 |  | ✓ | ✓ |  | ✓ |  |
    $\mathcal{L}_{\text{Con.}}$, $\mathcal{L}_{\text{GAN}}$ |'
- en: '| RNAN [[106](#bib.bib106)] | 2019, ICLR | Post. | Sub-Pixel |  | ✓ |  | ✓
    | ✓ |  | Non-local attention |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| RNAN [[106](#bib.bib106)] | 2019, ICLR | 后处理 | 子像素 |  | ✓ |  | ✓ | ✓ |  |
    非局部注意力 |'
- en: '| Meta-RDN [[95](#bib.bib95)] | 2019, CVPR | Post. | Meta Upscale |  | ✓ |
    ✓ |  | ✓ |  | Magnification-arbitrary |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| Meta-RDN [[95](#bib.bib95)] | 2019, CVPR | 后处理 | 元放大 |  | ✓ | ✓ |  | ✓ |  |
    任意放大 |'
- en: '| SAN [[105](#bib.bib105)] | 2019, CVPR | Post. | Sub-Pixel |  | ✓ |  | ✓ |
    ✓ |  | Second-order attention |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| SAN [[105](#bib.bib105)] | 2019, CVPR | 后处理 | 子像素 |  | ✓ |  | ✓ | ✓ |  |
    二阶注意力 |'
- en: '| SRFBN [[86](#bib.bib86)] | 2019, CVPR | Post. | Deconv | ✓ | ✓ | ✓ |  | ✓
    |  | Feedback mechanism |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| SRFBN [[86](#bib.bib86)] | 2019, CVPR | 后处理 | 反卷积 | ✓ | ✓ | ✓ |  | ✓ |  |
    反馈机制 |'
- en: '![Refer to caption](img/25eb191da4f552ed88f44c78d072726d.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/25eb191da4f552ed88f44c78d072726d.png)'
- en: 'Figure 8: Super-resolution benchmarking. The $x$-axis and the $y$-axis denote
    the Multi-Adds and PSNR, respectively, and the circle size represents the number
    of parameters.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 超分辨率基准测试。$x$轴和$y$轴分别表示 Multi-Adds 和 PSNR，圆圈的大小表示参数的数量。'
- en: 3.6 State-of-the-art Super-resolution Models
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 最先进的超分辨率模型
- en: 'In recent years, image super-resolution models based on deep learning have
    received more and more attention and achieved state-of-the-art performance. In
    previous sections, we decompose SR models into specific components, including
    model frameworks (Sec. [3.1](#S3.SS1 "3.1 Super-resolution Frameworks ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")), upsampling
    methods (Sec. [3.2](#S3.SS2 "3.2 Upsampling Methods ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")), network design (Sec.
    [3.3](#S3.SS3 "3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey")) and learning strategies (Sec. [3.4](#S3.SS4
    "3.4 Learning Strategies ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image
    Super-resolution: A Survey")), analyze these components hierarchically and identify
    their advantages and limitations. As a matter of fact, most of the state-of-the-art
    SR models today can basically be attributed to a combination of multiple strategies
    we summarize above. For example, the biggest contribution of the RCAN [[70](#bib.bib70)]
    comes from the channel attention mechanism (Sec. [3.3.5](#S3.SS3.SSS5 "3.3.5 Attention
    Mechanism ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey")), and it also employs other strategies
    like sub-pixel upsampling (Sec. [3.2.2](#S3.SS2.SSS2 "3.2.2 Learning-based Upsampling
    ‣ 3.2 Upsampling Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image
    Super-resolution: A Survey")), residual learning (Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1
    Residual Learning ‣ 3.3 Network Design ‣ 3 Supervised Super-resolution ‣ Deep
    Learning for Image Super-resolution: A Survey")), pixel L1 loss (Sec. [3.4.1](#S3.SS4.SSS1
    "3.4.1 Loss Functions ‣ 3.4 Learning Strategies ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")), and self-ensemble (Sec.
    [3.5.5](#S3.SS5.SSS5 "3.5.5 Self-Ensemble ‣ 3.5 Other Improvements ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")). In similar
    manners, we summarize some representative models and their key strategies, as
    Table [II](#S3.T2 "TABLE II ‣ 3.5.5 Self-Ensemble ‣ 3.5 Other Improvements ‣ 3
    Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")
    shows.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于深度学习的图像超分辨率模型越来越受到关注，并取得了**最先进**的性能。在前面的章节中，我们将超分辨率模型分解为特定的组件，包括模型框架（Sec.
    [3.1](#S3.SS1 "3.1 超分辨率框架 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），上采样方法（Sec. [3.2](#S3.SS2
    "3.2 上采样方法 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），网络设计（Sec. [3.3](#S3.SS3 "3.3 网络设计 ‣
    3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")）和学习策略（Sec. [3.4](#S3.SS4 "3.4 学习策略 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），对这些组件进行层次分析，识别其优点和局限性。事实上，今天大多数最先进的超分辨率模型基本上都可以归因于我们总结的多种策略的组合。例如，RCAN的最大贡献来自于通道注意机制（Sec.
    [3.3.5](#S3.SS3.SSS5 "3.3.5 注意机制 ‣ 3.3 网络设计 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），它还采用了其他策略，如子像素上采样（Sec.
    [3.2.2](#S3.SS2.SSS2 "3.2.2 基于学习的上采样 ‣ 3.2 上采样方法 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），残差学习（Sec.
    [3.3.1](#S3.SS3.SSS1 "3.3.1 残差学习 ‣ 3.3 网络设计 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），像素
    L1 损失（Sec. [3.4.1](#S3.SS4.SSS1 "3.4.1 损失函数 ‣ 3.4 学习策略 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")），和自集成（Sec.
    [3.5.5](#S3.SS5.SSS5 "3.5.5 自集成 ‣ 3.5 其他改进 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")）。类似地，我们总结了一些具有代表性的模型及其关键策略，如表[II](#S3.T2
    "表 II ‣ 3.5.5 自集成 ‣ 3.5 其他改进 ‣ 3 监督性超分辨率 ‣ 图像超分辨率的深度学习调查")所示。
- en: 'In addition to SR accuracy, the efficiency is another very important aspect
    and different strategies have more or less impact on efficiency. So in the previous
    sections, we not only analyze the accuracy of the presented strategies, but also
    indicate the concrete impacts on efficiency for the ones with a greater impact
    on efficiency, such as the post-upsampling (Sec. [3.1.2](#S3.SS1.SSS2 "3.1.2 Post-upsampling
    Super-resolution ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")), recursive learning (Sec.
    [3.3.2](#S3.SS3.SSS2 "3.3.2 Recursive Learning ‣ 3.3 Network Design ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey")), dense
    connections (Sec. [3.3.4](#S3.SS3.SSS4 "3.3.4 Dense Connections ‣ 3.3 Network
    Design ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")), xUnit (Sec. [3.3.11](#S3.SS3.SSS11 "3.3.11 xUnit ‣ 3.3 Network Design
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey")). And we also benchmark some representative SR models on the SR accuracy
    (i.e., PSNR), model size (i.e., number of parameters) and computation cost (i.e.,
    number of Multi-Adds), as shown in Fig. [8](#S3.F8 "Figure 8 ‣ 3.5.5 Self-Ensemble
    ‣ 3.5 Other Improvements ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image
    Super-resolution: A Survey"). The accuracy is measured by the mean of the PSNR
    on 4 benchmark datasets (i.e., Set5 [[48](#bib.bib48)], Set14 [[49](#bib.bib49)],
    B100 [[40](#bib.bib40)] and Urban100 [[50](#bib.bib50)]). And the model size and
    computational cost are calculated with PyTorch-OpCounter [[157](#bib.bib157)],
    where the output resolution is 720p (i.e., $1080\times 720$). All statistics are
    derived from the original papers or calculated on official models, with a scaling
    factor of 2. For better viewing and comparison, we also provide an interactive
    online version¹¹1https://github.com/ptkin/Awesome-Super-Resolution.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '除了超分辨率精度外，效率也是一个非常重要的方面，不同策略对效率的影响或多或少。因此，在前面的章节中，我们不仅分析了所提出策略的精度，还指出了对效率有更大影响的策略的具体影响，如后采样（第
    [3.1.2](#S3.SS1.SSS2 "3.1.2 Post-upsampling Super-resolution ‣ 3.1 Super-resolution
    Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey") 节），递归学习（第 [3.3.2](#S3.SS3.SSS2 "3.3.2 Recursive Learning ‣ 3.3 Network
    Design ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey") 节），密集连接（第 [3.3.4](#S3.SS3.SSS4 "3.3.4 Dense Connections ‣ 3.3 Network
    Design ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey") 节），xUnit（第 [3.3.11](#S3.SS3.SSS11 "3.3.11 xUnit ‣ 3.3 Network Design
    ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution: A
    Survey") 节）。我们还对一些具有代表性的超分辨率模型进行了基准测试，包括超分辨率精度（即PSNR）、模型大小（即参数数量）和计算成本（即Multi-Adds数量），如图
    [8](#S3.F8 "Figure 8 ‣ 3.5.5 Self-Ensemble ‣ 3.5 Other Improvements ‣ 3 Supervised
    Super-resolution ‣ Deep Learning for Image Super-resolution: A Survey") 所示。精度通过在4个基准数据集（即Set5
    [[48](#bib.bib48)]，Set14 [[49](#bib.bib49)]，B100 [[40](#bib.bib40)] 和 Urban100
    [[50](#bib.bib50)]）上的PSNR均值来衡量。模型大小和计算成本使用PyTorch-OpCounter [[157](#bib.bib157)]
    计算，其中输出分辨率为720p（即$1080\times 720$）。所有统计数据均来源于原始论文或在官方模型上计算，缩放因子为2。为了更好地查看和比较，我们还提供了一个交互式在线版本¹¹1https://github.com/ptkin/Awesome-Super-Resolution。'
- en: 4 Unsupervised Super-resolution
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 无监督超分辨率
- en: Existing super-resolution works mostly focus on supervised learning, i.e., learning
    with matched LR-HR image pairs. However, since it is difficult to collect images
    of the same scene but with different resolutions, the LR images in SR datasets
    are often obtained by performing predefined degradation on HR images. Thus the
    trained SR models actually learn a reverse process of the predefined degradation.
    In order to learn the real-world LR-HR mapping without introducing manual degradation
    priors, researchers pay more and more attention to unsupervised SR, in which case
    only unpaired LR-HR images are provided for training, so that the resulting models
    are more likely to cope with the SR problems in real-world scenarios. Next we’ll
    briefly introduce several existing unsupervised SR models with deep learning,
    and more methods are yet to be explored.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的超分辨率研究大多集中于监督学习，即使用匹配的低分辨率（LR）和高分辨率（HR）图像对进行学习。然而，由于难以收集同一场景但具有不同分辨率的图像，超分辨率数据集中LR图像通常通过对HR图像进行预定义的降质处理得到。因此，训练得到的超分辨率模型实际上学习了预定义降质的逆过程。为了在不引入手动降质先验的情况下学习真实世界的LR-HR映射，研究人员越来越关注无监督超分辨率，其中仅提供未配对的LR-HR图像用于训练，使得最终模型更有可能应对现实世界中的超分辨率问题。接下来，我们将简要介绍几种现有的基于深度学习的无监督超分辨率模型，还有更多方法尚待探索。
- en: 4.1 Zero-shot Super-resolution
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 零样本超分辨率
- en: Considering that the internal image statistics inside a single image have provided
    sufficient information for SR, Shocher et al. [[83](#bib.bib83)] propose zero-shot
    super-resolution (ZSSR) to cope with unsupervised SR by training image-specific
    SR networks at test time rather than training a generic model on large external
    datasets. Specifically, they estimate the degradation kernel from a single image
    using [[158](#bib.bib158)] and use this kernel to build a small dataset by performing
    degradation with different scaling factors and augmentation on this image. Then
    a small CNN for SR is trained on this dataset and used for the final prediction.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于单张图像内部的统计信息已经提供了足够的 SR 信息，Shocher 等人 [[83](#bib.bib83)] 提出了零样本超分辨率（ZSSR），以通过在测试时训练特定于图像的
    SR 网络来应对无监督 SR，而不是在大型外部数据集上训练通用模型。具体而言，他们使用 [[158](#bib.bib158)] 从单张图像中估计降解核，并利用该核通过对图像进行不同缩放因子和增强的降解来构建一个小数据集。然后，在该数据集上训练一个小型
    CNN 进行 SR 并用于最终预测。
- en: In this way, the ZSSR leverages on the cross-scale internal recurrence inside
    every image, and thus outperforms previous approaches by a large margin ($1$ dB
    for estimated kernels and $2$ dB for known kernels) on images under non-ideal
    conditions (i.e., images obtained by non-bicubic degradation and suffered effects
    like blurring, noise, compression artifacts), which is closer to the real-world
    scenes, while give competitive results under ideal conditions (i.e., images obtained
    by bicubic degradation). However, since it needs to train different networks for
    different images during testing, the inference time is much longer than others.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，ZSSR 利用每张图像内部的跨尺度递归，因此在非理想条件下（即通过非双三次降解获得的图像，并受到模糊、噪声、压缩伪影等影响）比之前的方法有了大幅度的提升（对于估计的核为
    $1$ dB，对于已知的核为 $2$ dB），这更接近真实场景，同时在理想条件下（即通过双三次降解获得的图像）也能给出具有竞争力的结果。然而，由于在测试期间需要为不同的图像训练不同的网络，因此推断时间比其他方法要长得多。
- en: 4.2 Weakly-supervised Super-resolution
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 弱监督超分辨率
- en: To cope with super-resolution without introducing predefined degradation, researchers
    attempt to learn SR models with weakly-supervised learning, i.e., using unpaired
    LR-HR images. Among them, some researchers first learn the HR-to-LR degradation
    and use it to construct datasets for training the SR model, while others design
    cycle-in-cycle networks to learn the LR-to-HR and HR-to-LR mappings simultaneously.
    Next we’ll detail these models.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对超分辨率而不引入预定义的降解，研究人员尝试通过弱监督学习来学习 SR 模型，即使用未配对的 LR-HR 图像。其中，一些研究人员首先学习 HR
    到 LR 的降解，并使用它来构建用于训练 SR 模型的数据集，而另一些则设计循环网络以同时学习 LR 到 HR 和 HR 到 LR 的映射。接下来我们将详细介绍这些模型。
- en: Learned Degradation. Since the predefined degradation is suboptimal, learning
    the degradation from unpaired LR-HR datasets is a feasible direction. Bulat et
    al. [[159](#bib.bib159)] propose a two-stage process which firstly trains an HR-to-LR
    GAN to learn degradation using unpaired LR-HR images and then trains an LR-to-HR
    GAN for SR using paired LR-HR images conducted base on the first GAN. Specifically,
    for the HR-to-LR GAN, HR images are fed into the generator to produce LR outputs,
    which are required to match not only the LR images obtained by downscaling the
    HR images (by average pooling) but also the distribution of real LR images. After
    finishing training, the generator is used as a degradation model to generate LR-HR
    image pairs. Then for the LR-to-HR GAN, the generator (i.e., the SR model) takes
    the generated LR images as input and predicts HR outputs, which are required to
    match not only the corresponding HR images but also the distribution of the HR
    images.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 学习降解。由于预定义的降解并不理想，从未配对的 LR-HR 数据集中学习降解是一个可行的方向。Bulat 等人 [[159](#bib.bib159)]
    提出了一个两阶段过程，首先训练一个 HR 到 LR 的 GAN 来学习降解，使用未配对的 LR-HR 图像，然后训练一个 LR 到 HR 的 GAN 进行
    SR，基于第一个 GAN 进行配对 LR-HR 图像。具体而言，对于 HR 到 LR 的 GAN，HR 图像被输入到生成器中以生成 LR 输出，这些输出不仅需要与通过降采样
    HR 图像（通过平均池化）获得的 LR 图像匹配，还需要匹配真实 LR 图像的分布。训练完成后，生成器被用作降解模型生成 LR-HR 图像对。然后对于 LR
    到 HR 的 GAN，生成器（即 SR 模型）将生成的 LR 图像作为输入，并预测 HR 输出，这些输出不仅需要与对应的 HR 图像匹配，还需要匹配 HR
    图像的分布。
- en: By applying this two-stage process, the proposed unsupervised model effectively
    increases the quality of super-resolving real-world LR images and obtains large
    improvement over previous state-of-the-art works.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这一两阶段过程，所提出的无监督模型有效提升了实际低分辨率（LR）图像的超分辨率质量，并且相较于之前的最先进方法获得了显著的改进。
- en: Cycle-in-cycle Super-resolution. Another approach for unsupervised super-resolution
    is to treat the LR space and the HR space as two domains, and use a cycle-in-cycle
    structure to learn the mappings between each other. In this case, the training
    objectives include pushing the mapped results to match the target domain distribution
    and making the images recoverable through round-trip mappings.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 循环嵌套超分辨率。另一种无监督超分辨率的方法是将LR空间和HR空间视为两个领域，并使用循环嵌套结构来学习彼此之间的映射。在这种情况下，训练目标包括推动映射结果与目标领域分布匹配，并使图像通过回程映射恢复。
- en: Motivated by CycleGAN [[138](#bib.bib138)], Yuan et al. [[131](#bib.bib131)]
    propose a cycle-in-cycle SR network (CinCGAN) composed of 4 generators and 2 discriminators,
    making up two CycleGANs for noisy LR $\rightleftharpoons$ clean LR and clean LR
    $\rightleftharpoons$ clean HR mappings, respectively. Specifically, in the first
    CycleGAN, the noisy LR image is fed into a generator, and the output is required
    to be consistent with the distribution of real clean LR images. Then it’s fed
    into another generator and required to recover the original input. Several loss
    functions (e.g., adversarial loss, cycle consistency loss, identity loss) are
    employed for guaranteeing the cycle consistency, distribution consistency, and
    mapping validity. The other CycleGAN is similarly designed, except that the mapping
    domains are different.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 受到CycleGAN的启发[[138](#bib.bib138)]，Yuan等人[[131](#bib.bib131)] 提出了一个由4个生成器和2个判别器组成的循环嵌套超分辨率网络（CinCGAN），形成两个CycleGAN用于处理噪声LR
    $\rightleftharpoons$ 干净LR和干净LR $\rightleftharpoons$ 干净HR的映射。具体而言，在第一个CycleGAN中，将噪声LR图像输入到生成器中，并要求输出与真实干净LR图像的分布一致。然后，将其输入到另一个生成器中，并要求恢复原始输入。采用几种损失函数（例如对抗损失、循环一致性损失、身份损失）以保证循环一致性、分布一致性和映射有效性。另一个CycleGAN类似设计，但映射领域不同。
- en: Because of avoiding the predefined degradation, the unsupervised CinCGAN not
    only achieves comparable performance to supervised methods, but also is applicable
    to various cases even under very harsh conditions. However, due to the ill-posed
    essence of SR problem and the complicated architecture of CinCGAN, some advanced
    strategies are needed for reducing the training difficulty and instability.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于避免了预定义的退化，无监督CinCGAN不仅实现了与监督方法相当的性能，还适用于各种情况，即使在非常恶劣的条件下也是如此。然而，由于超分辨率问题的病态本质和CinCGAN的复杂架构，需要一些高级策略来降低训练难度和不稳定性。
- en: 4.3 Deep Image Prior
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 深度图像先验
- en: Considering that the CNN structure is sufficient to capture a great deal of
    low-level image statistics prior for inverse problems, Ulyanov et al. [[160](#bib.bib160)]
    employ a randomly-initialized CNN as handcrafted prior to perform SR. Specifically,
    they define a generator network which takes a random vector $z$ as input and tries
    to generate the target HR image $I_{y}$. The goal is to train the network to find
    an $\hat{I_{y}}$ that the downsampled $\hat{I_{y}}$ is identical to the LR image
    $I_{x}$. Since the network is randomly initialized and never trained, the only
    prior is the CNN structure itself. Although the performance of this method is
    still worse than the supervised methods ($2$ dB), it outperforms traditional bicubic
    upsampling considerably ($1$ dB). Besides, it shows the rationality of the CNN
    architectures itself, and prompts us to improve SR by combining the deep learning
    methodology with handcrafted priors such as CNN structures or self-similarity.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到CNN结构足以捕捉大量的低级图像统计先验用于逆问题，Ulyanov等人[[160](#bib.bib160)] 使用随机初始化的CNN作为手工设计的先验来执行超分辨率。具体而言，他们定义了一个生成器网络，该网络以随机向量$z$为输入，并尝试生成目标HR图像$I_{y}$。目标是训练网络找到一个$\hat{I_{y}}$，使得下采样后的$\hat{I_{y}}$与LR图像$I_{x}$一致。由于网络是随机初始化且从未训练过，因此唯一的先验是CNN结构本身。尽管该方法的性能仍低于监督方法（$2$
    dB），但显著优于传统的双三次插值（$1$ dB）。此外，它展示了CNN架构本身的合理性，并促使我们通过将深度学习方法与手工设计的先验（如CNN结构或自相似性）结合来改进超分辨率。
- en: 5 Domain-Specific Applications
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 领域特定应用
- en: 5.1 Depth Map Super-resolution
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 深度图像超分辨率
- en: Depth maps record the depth (i.e., distance) between the viewpoint and objects
    in the scene, and plays important roles in many tasks like pose estimation [[161](#bib.bib161),
    [162](#bib.bib162)] and semantic segmentation [[163](#bib.bib163), [164](#bib.bib164)].
    However, due to economic and production constraints, the depth maps produced by
    depth sensors are often low-resolution and suffer degradation effects such as
    noise, quantization and missing values. Thus super-resolution is introduced for
    increasing the spatial resolution of depth maps.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图记录了视点与场景中物体之间的深度（即距离），在姿态估计 [[161](#bib.bib161), [162](#bib.bib162)] 和语义分割
    [[163](#bib.bib163), [164](#bib.bib164)] 等许多任务中扮演了重要角色。然而，由于经济和生产限制，深度传感器生成的深度图通常分辨率较低，且受到噪声、量化和缺失值等退化效应的影响。因此，引入了超分辨率技术以提高深度图的空间分辨率。
- en: Nowadays one of the most popular practices for depth map SR is to use another
    economical RGB camera to obtain HR images of the same scenes for guiding super-resolving
    the LR depth maps. Specifically, Song et al. [[165](#bib.bib165)] exploit the
    depth field statistics and local correlations between depth maps and RGB images
    to constrain the global statistics and local structures. Hui et al. [[166](#bib.bib166)]
    utilize two CNNs to simultaneously upsample LR depth maps and downsample HR RGB
    images, then use RGB features as the guidance for upsampling depth maps with the
    same resolution. And Haefner et al. [[167](#bib.bib167)] further exploit the color
    information and guide SR by resorting to the shape-from-shading technique. In
    contrast, Riegler et al. [[168](#bib.bib168)] combine CNNs with an energy minimization
    model in the form of a powerful variational model to recover HR depth maps without
    other reference images.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，深度图 SR 最流行的做法之一是使用另一台经济型 RGB 相机来获取相同场景的高分辨率图像，以指导超分辨率处理低分辨率深度图。具体而言，Song
    等人 [[165](#bib.bib165)] 利用深度场统计信息以及深度图和 RGB 图像之间的局部相关性来约束全局统计信息和局部结构。Hui 等人 [[166](#bib.bib166)]
    使用两个 CNN 同时对低分辨率深度图进行上采样，并对高分辨率 RGB 图像进行下采样，然后使用 RGB 特征作为指导，以相同的分辨率上采样深度图。而 Haefner
    等人 [[167](#bib.bib167)] 进一步利用颜色信息，并通过形状自阴影技术指导超分辨率。相比之下，Riegler 等人 [[168](#bib.bib168)]
    将 CNN 与能量最小化模型结合，采用强大的变分模型形式，在没有其他参考图像的情况下恢复高分辨率深度图。
- en: 5.2 Face Image Super-resolution
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 面部图像超分辨率
- en: Face image super-resolution, a.k.a. face hallucination (FH), can often help
    other face-related tasks [[169](#bib.bib169), [73](#bib.bib73), [72](#bib.bib72)].
    Compared to generic images, face images have more face-related structured information,
    so incorporating facial prior knowledge (e.g., landmarks, parsing maps, identities)
    into FH is a very popular and promising approach.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 面部图像超分辨率，也称为面部幻觉 (FH)，通常可以帮助其他面部相关任务 [[169](#bib.bib169), [73](#bib.bib73),
    [72](#bib.bib72)]。与普通图像相比，面部图像具有更多的面部相关结构信息，因此将面部先验知识（例如，标志点、解析图、身份）融入 FH 是一种非常流行且有前景的方法。
- en: One of the most straightforward way is to constrain the generated images to
    have the identical face-related attributes to ground truth. Specifically, the
    CBN [[170](#bib.bib170)] utilizes the facial prior by alternately optimizing FH
    and dense correspondence field estimation. The Super-FAN [[30](#bib.bib30)] and
    MTUN [[171](#bib.bib171)] both introduce FAN to guarantee the consistency of facial
    landmarks by end-to-end multi-task learning. And the FSRNet [[73](#bib.bib73)]
    uses not only facial landmark heatmaps but also face parsing maps as prior constraints.
    The SICNN [[72](#bib.bib72)], which aims at recovering the real identity, adopts
    a super-identity loss function and a domain-integrated training approach to stable
    the joint training.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的方法之一是限制生成的图像与真实数据具有相同的面部相关属性。具体而言，CBN [[170](#bib.bib170)] 通过交替优化 FH 和密集对应场域估计来利用面部先验。Super-FAN
    [[30](#bib.bib30)] 和 MTUN [[171](#bib.bib171)] 都引入了 FAN，通过端到端的多任务学习来保证面部标志点的一致性。而
    FSRNet [[73](#bib.bib73)] 不仅使用面部标志热图，还使用面部解析图作为先验约束。旨在恢复真实身份的 SICNN [[72](#bib.bib72)]
    采用了超身份损失函数和领域集成训练方法以稳定联合训练。
- en: Besides explicitly using facial prior, the implicit methods are also widely
    studied. The TDN [[172](#bib.bib172)] incorporates spatial transformer networks
    [[173](#bib.bib173)] for automatic spatial transformations and thus solves the
    face unalignment problem. Based on TDN, the TDAE [[174](#bib.bib174)] adopts a
    decoder-encoder-decoder framework, where the first decoder learns to upsample
    and denoise, the encoder projects it back to aligned and noise-free LR faces,
    and the last decoder generates hallucinated HR images. In contrast, the LCGE [[175](#bib.bib175)]
    employs component-specific CNNs to perform SR on five facial components, uses
    k-NN search on an HR facial component dataset to find corresponding patches, synthesizes
    finer-grained components and finally fuses them to FH results. Similarly, Yang
    et al. [[176](#bib.bib176)] decompose deblocked face images into facial components
    and background, use the component landmarks to retrieve adequate HR exemplars
    in external datasets, perform generic SR on the background, and finally fuse them
    to complete HR faces.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显式使用面部先验，隐式方法也得到了广泛研究。TDN [[172](#bib.bib172)] 结合了空间变换网络 [[173](#bib.bib173)]
    进行自动空间变换，从而解决了面部未对齐的问题。基于TDN，TDAE [[174](#bib.bib174)] 采用了一个解码器-编码器-解码器框架，其中第一个解码器用于上采样和去噪，编码器将其投影回对齐且无噪声的LR面部图像，最后一个解码器生成幻觉HR图像。相比之下，LCGE
    [[175](#bib.bib175)] 采用了组件特定的CNN对五个面部组件进行SR，使用k-NN搜索HR面部组件数据集以找到对应的补丁，合成更细粒度的组件，最终将其融合为FH结果。同样，Yang
    等人 [[176](#bib.bib176)] 将去块状的面部图像分解为面部组件和背景，利用组件标记从外部数据集中检索适当的HR示例，对背景进行通用SR，最后将它们融合以完成HR面部图像。
- en: In addition, researchers also improve FH from other perspectives. Motivated
    by the human attention shifting mechanism [[112](#bib.bib112)], the Attention-FH
    [[113](#bib.bib113)] resorts to a recurrent policy network for sequentially discovering
    attended face patches and performing local enhancement, and thus fully exploits
    the global interdependency of face images. The UR-DGN [[177](#bib.bib177)] adopts
    a network similar to SRGAN [[25](#bib.bib25)] with adversarial learning. And Xu
    et al. [[63](#bib.bib63)] propose a multi-class GAN-based FH model composed of
    a generic generator and class-specific discriminators. Both Lee et al. [[178](#bib.bib178)]
    and Yu et al. [[179](#bib.bib179)] utilize additional facial attribute information
    to perform FH with the specified attributes, based on the conditional GAN [[180](#bib.bib180)].
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员还从其他角度改进了FH。受到人类注意力转移机制[[112](#bib.bib112)]的启发，Attention-FH [[113](#bib.bib113)]
    采用了递归策略网络来依次发现关注的面部区域并进行局部增强，从而充分利用面部图像的全局依赖性。UR-DGN [[177](#bib.bib177)] 采用了类似于SRGAN
    [[25](#bib.bib25)] 的网络，并结合了对抗学习。Xu 等人 [[63](#bib.bib63)] 提出了一个多类基于GAN的FH模型，该模型由通用生成器和特定类别的鉴别器组成。Lee
    等人 [[178](#bib.bib178)] 和 Yu 等人 [[179](#bib.bib179)] 利用附加的面部属性信息，基于条件GAN [[180](#bib.bib180)]
    执行带有指定属性的FH。
- en: 5.3 Hyperspectral Image Super-resolution
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 高光谱图像超分辨率
- en: Compared to panchromatic images (PANs, i.e., RGB images with 3 bands), hyperspectral
    images (HSIs) containing hundreds of bands provide abundant spectral features
    and help various vision tasks [[181](#bib.bib181), [182](#bib.bib182), [183](#bib.bib183)].
    However, due to hardware limitations, collecting high-quality HSIs is much more
    difficult than PANs and the resolution is also lower. Thus super-resolution is
    introduced into this field, and researchers tend to combine HR PANs and LR HSIs
    to predict HR HSIs. Among them, Masi et al. [[184](#bib.bib184)] employ SRCNN
    [[22](#bib.bib22)] and incorporate several maps of nonlinear radiometric indices
    for boosting performance. Qu et al. [[185](#bib.bib185)] jointly train two encoder-decoder
    networks to perform SR on PANs and HSIs, respectively, and transfer the SR knowledge
    from PAN to HSI by sharing the decoder and applying constraints such as angle
    similarity loss and reconstruction loss. Recently, Fu et al. [[186](#bib.bib186)]
    evaluate the effect of camera spectral response (CSR) functions for HSI SR and
    propose a CSR optimization layer which can automatically select or design the
    optimal CSR, and outperform the state-of-the-arts.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 与包含三个波段的全色图像（PAN，即RGB图像）相比，包含数百个波段的高光谱图像（HSI）提供了丰富的光谱特征，有助于各种视觉任务[[181](#bib.bib181)，[182](#bib.bib182)，[183](#bib.bib183)]。然而，由于硬件限制，采集高质量的HSI比PAN更加困难，分辨率也较低。因此，超分辨率被引入这一领域，研究人员倾向于结合高分辨率PAN和低分辨率HSI来预测高分辨率HSI。其中，Masi等人[[184](#bib.bib184)]采用SRCNN[[22](#bib.bib22)]并结合多个非线性辐射指数图以提升性能。Qu等人[[185](#bib.bib185)]联合训练两个编码解码网络分别对PAN和HSI进行SR，通过共享解码器并应用角度相似性损失和重建损失等约束，将SR知识从PAN转移到HSI。最近，Fu等人[[186](#bib.bib186)]评估了相机光谱响应（CSR）函数对HSI
    SR的效果，并提出了一种CSR优化层，能够自动选择或设计最佳CSR，并超越了当前的最先进水平。
- en: 5.4 Real-world Image Super-resolution
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 现实世界图像超分辨率
- en: Generally, the LR images for training SR models are generated by downsampling
    RGB images manually (e.g., by bicubic downsampling). However, real-world cameras
    actually capture 12-bit or 14-bit RAW images, and performs a series of operations
    (e.g., demosaicing, denoising and compression) through camera ISPs (image signal
    processors) and finally produce 8-bit RGB images. Through this process, the RGB
    images have lost lots of original signals and are very different from the original
    images taken by the camera. Therefore, it is suboptimal to directly use the manually
    downsampled RGB image for SR.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，训练超分辨率（SR）模型的低分辨率（LR）图像是通过手动下采样RGB图像生成的（例如，通过双三次下采样）。然而，现实世界中的相机实际上捕捉到的是12位或14位RAW图像，并通过相机ISP（图像信号处理器）执行一系列操作（例如，去马赛克、去噪和压缩），最终生成8位RGB图像。在这个过程中，RGB图像丢失了大量原始信号，与相机拍摄的原始图像有很大不同。因此，直接使用手动下采样的RGB图像进行SR是不理想的。
- en: To solve this problem, researchers study how to use real-world images for SR.
    Among them, Chen et al. [[187](#bib.bib187)] analyze the relationships between
    image resolution (R) and field-of-view (V) in imaging systems (namely R-V degradation),
    propose data acquisition strategies to conduct a real-world dataset City100, and
    experimentally demonstrate the superiority of the proposed image synthesis model.
    Zhang et al. [[188](#bib.bib188)] build another real-world image dataset SR-RAW
    (i.e., paired HR RAW images and LR RGB images) through optical zoom of cameras,
    and propose contextual bilateral loss to solve the misalignment problem. In contrast,
    Xu et al. [[189](#bib.bib189)] propose a pipeline to generate realistic training
    data by simulating the imaging process and develop a dual CNN to exploit the originally
    captured radiance information in RAW images. They also propose to learn a spatially-variant
    color transformation for effective color corrections and generalization to other
    sensors.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，研究人员研究如何利用现实世界图像进行SR。其中，Chen等人[[187](#bib.bib187)]分析了成像系统中图像分辨率（R）和视场（V）之间的关系（即R-V退化），提出了数据获取策略来创建现实世界数据集City100，并通过实验展示了所提出的图像合成模型的优越性。Zhang等人[[188](#bib.bib188)]通过相机的光学变焦建立了另一个现实世界图像数据集SR-RAW（即配对的高分辨率RAW图像和低分辨率RGB图像），并提出了上下文双边损失来解决错位问题。相比之下，Xu等人[[189](#bib.bib189)]提出了一种通过模拟成像过程生成逼真训练数据的流程，并开发了一个双CNN以利用RAW图像中原始捕捉的辐射信息。他们还提出学习一种空间变异的颜色变换，以实现有效的颜色校正和对其他传感器的泛化。
- en: 5.5 Video Super-resolution
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 视频超分辨率
- en: For video super-resolution, multiple frames provide much more scene information,
    and there are not only intra-frame spatial dependency but also inter-frame temporal
    dependency (e.g., motions, brightness and color changes). Thus the existing works
    mainly focus on making better use of spatio-temporal dependency, including explicit
    motion compensation (e.g., optical flow-based, learning-based) and recurrent methods,
    etc.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视频超分辨率，多帧提供了更多的场景信息，不仅有帧内空间依赖，还有帧间时间依赖（例如，运动、亮度和颜色变化）。因此，现有的研究主要集中在更好地利用时空依赖，包括显式运动补偿（例如，基于光流的、基于学习的）和递归方法等。
- en: Among the optical flow-based methods, Liao et al. [[190](#bib.bib190)] employ
    optical flow methods to generate HR candidates and ensemble them by CNNs. VSRnet
    [[191](#bib.bib191)] and CVSRnet [[192](#bib.bib192)] deal with motion compensation
    by Druleas algorithm [[193](#bib.bib193)], and uses CNNs to take successive frames
    as input and predict HR frames. While Liu et al. [[194](#bib.bib194), [195](#bib.bib195)]
    perform rectified optical flow alignment, and propose a temporal adaptive net
    to generate HR frames in various temporal scales and aggregate them adaptively.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于光流的方法中，Liao 等人 [[190](#bib.bib190)] 使用光流方法生成HR候选帧，并通过CNN进行集成。VSRnet [[191](#bib.bib191)]
    和 CVSRnet [[192](#bib.bib192)] 通过Druleas算法 [[193](#bib.bib193)] 处理运动补偿，使用CNN将连续帧作为输入并预测HR帧。而Liu
    等人 [[194](#bib.bib194), [195](#bib.bib195)] 执行了校正的光流对齐，并提出了一种时间自适应网络，以生成各种时间尺度的HR帧并进行自适应聚合。
- en: Besides, others also try to directly learn the motion compensation. The VESPCN
    [[156](#bib.bib156)] utilizes a trainable spatial transformer [[173](#bib.bib173)]
    to learn motion compensation based on adjacent frames, and enters multiple frames
    into a spatio-temporal ESPCN [[84](#bib.bib84)] for end-to-end prediction. And
    Tao et al. [[196](#bib.bib196)] root from accurate LR imaging model and propose
    a sub-pixel-like module to simultaneously achieve motion compensation and super-resolution,
    and thus fuse the aligned frames more effectively.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些方法直接尝试学习运动补偿。VESPCN [[156](#bib.bib156)] 利用一个可训练的空间变换器 [[173](#bib.bib173)]
    基于相邻帧来学习运动补偿，并将多帧输入到一个时空ESPCN [[84](#bib.bib84)] 中进行端到端预测。Tao 等人 [[196](#bib.bib196)]
    基于准确的LR成像模型，提出了一种类似亚像素的模块，以同时实现运动补偿和超分辨率，从而更有效地融合对齐的帧。
- en: 'Another trend is to use recurrent methods to capture the spatial-temporal dependency
    without explicit motion compensation. Specifically, the BRCN [[197](#bib.bib197),
    [198](#bib.bib198)] employs a bidirectional framework, and uses CNN, RNN, and
    conditional CNN to model the spatial, temporal and spatial-temporal dependency,
    respectively. Similarly, STCN [[199](#bib.bib199)] uses a deep CNN and a bidirectional
    LSTM [[200](#bib.bib200)] to extract spatial and temporal information. And FRVSR
    [[201](#bib.bib201)] uses previously inferred HR estimates to reconstruct the
    subsequent HR frames by two deep CNNs in a recurrent manner. Recently the FSTRN
    [[202](#bib.bib202)] employs two much smaller 3D convolution filters to replace
    the original large filter, and thus enhances the performance through deeper CNNs
    while maintaining low computational cost. While the RBPN [[87](#bib.bib87)] extracts
    spatial and temporal contexts by a recurrent encoder-decoder, and combines them
    with an iterative refinement framework based on the back-projection mechanism
    (Sec. [3.1.4](#S3.SS1.SSS4 "3.1.4 Iterative Up-and-down Sampling Super-resolution
    ‣ 3.1 Super-resolution Frameworks ‣ 3 Supervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey")).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种趋势是使用递归方法捕捉时空依赖，而不进行显式运动补偿。具体来说，BRCN [[197](#bib.bib197), [198](#bib.bib198)]
    采用了一个双向框架，分别使用CNN、RNN和条件CNN来建模空间、时间和时空依赖。类似地，STCN [[199](#bib.bib199)] 使用深度CNN和双向LSTM
    [[200](#bib.bib200)] 提取空间和时间信息。FRVSR [[201](#bib.bib201)] 通过两个深度CNN以递归方式使用之前推断的HR估计来重建后续的HR帧。最近，FSTRN
    [[202](#bib.bib202)] 使用两个更小的3D卷积滤波器来替代原始的大滤波器，从而通过更深的CNN提高性能，同时保持低计算成本。而RBPN [[87](#bib.bib87)]
    通过递归编码器-解码器提取空间和时间上下文，并将其与基于回投影机制的迭代细化框架相结合（参见 [3.1.4](#S3.SS1.SSS4 "3.1.4 迭代上采样和下采样超分辨率
    ‣ 3.1 超分辨率框架 ‣ 3 监督超分辨率 ‣ 图像超分辨率的深度学习：综述")）。
- en: In addition, the FAST [[203](#bib.bib203)] exploits compact descriptions of
    the structure and pixel correlations extracted by compression algorithms, transfers
    the SR results from one frame to adjacent frames, and much accelerates the state-of-the-art
    SR algorithms with little performance loss. And Jo et al. [[204](#bib.bib204)]
    generate dynamic upsampling filters and the HR residual image based on the local
    spatio-temporal neighborhoods of each pixel, and also avoid explicit motion compensation.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，FAST [[203](#bib.bib203)] 利用压缩算法提取的结构和像素相关性的紧凑描述，将超分辨率结果从一个帧转移到相邻帧，并大幅加速最先进的超分辨率算法，同时性能损失很小。Jo
    等人 [[204](#bib.bib204)] 基于每个像素的局部时空邻域生成动态上采样滤波器和高分辨率残差图像，并避免了显式运动补偿。
- en: 5.6 Other Applications
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 其他应用
- en: Deep learning based super-resolution is also adopted to other domain-specific
    applications and shows great performance. Specifically, the Perceptual GAN [[205](#bib.bib205)]
    addresses the small object detection problem by super-resolving representations
    of small objects to have similar characteristics as large objects and be more
    discriminative for detection. Similarly, the FSR-GAN [[206](#bib.bib206)] super-resolves
    small-size images in the feature space instead of the pixel space, and thus transforms
    the raw poor features to highly discriminative ones, which greatly benefits image
    retrieval. Besides, Jeon et al. [[207](#bib.bib207)] utilize a parallax prior
    in stereo images to reconstruct HR images with sub-pixel accuracy in registration.
    Wang et al. [[208](#bib.bib208)] propose a parallax-attention model to tackle
    the stereo image super-resolution problem. Li et al. [[209](#bib.bib209)] incorporate
    the 3D geometric information and super-resolve 3D object texture maps. And Zhang
    et al. [[210](#bib.bib210)] separate view images in one light field into groups,
    learn inherent mapping for every group and finally combine the residuals in every
    group to reconstruct higher-resolution light fields. All in all, super-resolution
    technology can play an important role in all kinds of applications, especially
    when we can deal with large objects well but cannot handle small objects.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的超分辨率技术还被应用于其他领域特定的应用中，并展示了出色的性能。具体而言，感知 GAN [[205](#bib.bib205)] 通过超分辨率表示小物体，使其具有类似于大物体的特征，从而更具辨别力以进行检测。类似地，FSR-GAN
    [[206](#bib.bib206)] 在特征空间而非像素空间中进行小尺寸图像的超分辨率，从而将原始的较差特征转化为高度辨别的特征，这大大有利于图像检索。此外，Jeon
    等人 [[207](#bib.bib207)] 利用立体图像中的视差先验，以亚像素精度重建高分辨率图像。Wang 等人 [[208](#bib.bib208)]
    提出了一个视差注意力模型来解决立体图像超分辨率问题。Li 等人 [[209](#bib.bib209)] 结合了 3D 几何信息，进行 3D 物体纹理图的超分辨率。Zhang
    等人 [[210](#bib.bib210)] 将一个光场中的视图图像分成若干组，为每组学习固有映射，并最终结合每组中的残差重建更高分辨率的光场。总之，超分辨率技术在各种应用中可以发挥重要作用，尤其是在我们能够处理大物体但不能处理小物体的情况下。
- en: 6 Conclusion and Future Directions
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来方向
- en: In this paper, we have given an extensive survey on recent advances in image
    super-resolution with deep learning. We mainly discussed the improvement of supervised
    and unsupervised SR, and also introduced some domain-specific applications. Despite
    great success, there are still many unsolved problems. Thus in this section, we
    will point out these problems explicitly and introduce some promising trends for
    future evolution. We hope that this survey not only provides a better understanding
    of image SR for researchers but also facilitates future research activities and
    application developments in this field.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们对深度学习图像超分辨率的最新进展进行了广泛的综述。我们主要讨论了有监督和无监督超分辨率的改进，并介绍了一些领域特定的应用。尽管取得了巨大成功，但仍有许多未解决的问题。因此，在本节中，我们将明确指出这些问题，并介绍一些有前景的未来发展趋势。我们希望这项综述不仅能为研究人员提供对图像超分辨率的更好理解，还能促进未来在该领域的研究活动和应用开发。
- en: 6.1 Network Design
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 网络设计
- en: Good network design not only determines a hypothesis space with great performance
    upper bound, but also helps efficiently learn representations without excessive
    spatial and computational redundancy. Below we will introduce some promising directions
    for network improvements.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 优良的网络设计不仅决定了具有优良性能上界的假设空间，还能帮助高效地学习表示，避免过多的空间和计算冗余。接下来，我们将介绍一些有前景的网络改进方向。
- en: Combining Local and Global Information. Large receptive field provides more
    contextual information and helps generate more realistic results. Thus it is promising
    to combine local and global information for providing contextual information of
    different scales for image SR.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 结合局部和全局信息。大感受野提供更多的上下文信息，有助于生成更逼真的结果。因此，结合局部和全局信息为图像超分辨率提供不同尺度的上下文信息是有前景的。
- en: Combining Low- and High-level Information. Shallow layers in CNNs tend to extract
    low-level features like colors and edges, while deeper layers learn higher-level
    representations like object identities. Thus combining low-level details with
    high-level semantics can be of great help for HR reconstruction.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 结合低层和高层信息。卷积神经网络中的浅层往往提取低层特征如颜色和边缘，而深层则学习更高层次的表示如物体身份。因此，将低层细节与高层语义结合对于高分辨率重建非常有帮助。
- en: Context-specific Attention. In different contexts, people tend to care about
    different aspects of the images. For example, for the grass area people may be
    more concerned with local colors and textures, while in the animal body area people
    may care more about the species and corresponding hair details. Therefore, incorporating
    attention mechanism to enhance the attention to key features facilitates the generation
    of realistic details.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文特定的注意力。在不同的上下文中，人们往往关注图像的不同方面。例如，对于草地区域，人们可能更关注局部的颜色和纹理，而在动物体区域，人们可能更关心物种和相应的毛发细节。因此，结合注意力机制来增强对关键特征的关注，有助于生成逼真的细节。
- en: More Efficient Architectures. Existing SR modes tend to pursue ultimate performance,
    while ignoring the model size and inference speed. For example, the EDSR [[31](#bib.bib31)]
    takes 20s per image for $4\times$ SR on DIV2K [[42](#bib.bib42)] with a Titan
    GTX GPU [[80](#bib.bib80)], and DBPN [[57](#bib.bib57)] takes 35s for $8\times$
    SR [[211](#bib.bib211)]. Such long prediction time is unacceptable in practical
    applications, thus more efficient architectures are imperative. How to reduce
    model sizes and speed up prediction while maintaining performance remains a problem.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 更高效的架构。现有的超分辨率模式往往追求极致的性能，而忽略了模型大小和推理速度。例如，EDSR [[31](#bib.bib31)] 在 DIV2K [[42](#bib.bib42)]
    上使用 Titan GTX GPU [[80](#bib.bib80)] 进行 $4\times$ 超分辨率每张图像需要 20 秒，而 DBPN [[57](#bib.bib57)]
    进行 $8\times$ 超分辨率 [[211](#bib.bib211)] 需要 35 秒。这种长时间的预测在实际应用中是不可接受的，因此更高效的架构是必需的。如何在保持性能的同时减少模型大小并加快预测速度仍然是一个问题。
- en: 'Upsampling Methods. Existing upsampling methods (Sec. [3.2](#S3.SS2 "3.2 Upsampling
    Methods ‣ 3 Supervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")) have more or less disadvantages: interpolation methods result in expensive
    computation and cannot be end-to-end learned, the transposed convolution produces
    checkerboard artifacts, the sub-pixel layer brings uneven distribution of receptive
    fields, and the meta upscale module may cause instability or inefficiency and
    have further room for improvement. How to perform effective and efficient upsampling
    still needs to be studied, especially with high scaling factors.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '上采样方法。现有的上采样方法（见 [3.2](#S3.SS2 "3.2 Upsampling Methods ‣ 3 Supervised Super-resolution
    ‣ Deep Learning for Image Super-resolution: A Survey")）或多或少存在一些缺点：插值方法计算开销大且无法端到端学习，转置卷积产生棋盘状伪影，亚像素层带来感受野的不均匀分布，而元上采样模块可能导致不稳定或低效，并且还有进一步改进的空间。如何执行有效且高效的上采样仍需研究，特别是对于高放大倍数的情况。'
- en: Recently, the neural architecture search (NAS) technique for deep learning has
    become more and more popular, greatly improving the performance or efficiency
    with little artificial intervention [[212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214)].
    For the SR field, combining the exploration of the above directions with NAS is
    of great potential.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习中的神经架构搜索（NAS）技术变得越来越流行，通过很少的人工干预大大提高了性能或效率 [[212](#bib.bib212), [213](#bib.bib213),
    [214](#bib.bib214)]。在超分辨率领域，将上述方向的探索与 NAS 结合具有很大的潜力。
- en: 6.2 Learning Strategies
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 学习策略
- en: Besides good hypothesis spaces, robust learning strategies are also needed for
    achieving satisfactory results. Next we’ll introduce some promising directions
    of learning strategies.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 除了良好的假设空间， robust learning strategies 也是实现满意结果所必需的。接下来我们将介绍一些有前景的学习策略方向。
- en: Loss Functions. Existing loss functions can be regarded as establishing constraints
    among LR/HR/SR images, and guide optimization based on whether these constraints
    are met. In practice, these loss functions are often weighted combined and the
    best loss function for SR is still unclear. Therefore, one of the most promising
    directions is to explore the potential correlations between these images and seek
    more accurate loss functions.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数。现有的损失函数可以看作是对LR/HR/SR图像之间建立约束，并根据这些约束是否满足来指导优化。在实践中，这些损失函数通常是加权组合的，最适合超分辨率的损失函数仍不清楚。因此，探索这些图像之间的潜在关联并寻求更准确的损失函数是最有前途的方向之一。
- en: Normalization. Although BN is widely used in vision tasks, which greatly speeds
    up training and improves performance, it is proven to be sub-optimal for super-resolution
    [[31](#bib.bib31), [32](#bib.bib32), [147](#bib.bib147)]. Thus other effective
    normalization techniques for SR are needed to be studied.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化。虽然批归一化（BN）在视觉任务中被广泛使用，大大加快了训练速度并提高了性能，但已被证明对超分辨率是次优的[[31](#bib.bib31), [32](#bib.bib32),
    [147](#bib.bib147)]。因此，需要研究其他有效的超分辨率归一化技术。
- en: 6.3 Evaluation Metrics
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 评估指标
- en: Evaluation metrics are one of the most fundamental components for machine learning.
    If the performance cannot be measured accurately, researchers will have great
    difficulty verifying improvements. Metrics for super-resolution face such challenges
    and need more exploration.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标是机器学习中最基础的组成部分之一。如果性能不能准确测量，研究人员将很难验证改进。超分辨率的指标面临这样的挑战，需要进一步探索。
- en: 'More Accurate Metrics. Nowadays the PSNR and SSIM have been the most widely
    used metrics for SR. However, the PSNR tends to result in excessive smoothness
    and the results can vary wildly between almost indistinguishable images. The SSIM
    [[58](#bib.bib58)] performs evaluation in terms of brightness, contrast and structure,
    but still cannot measure perceptual quality accurately [[25](#bib.bib25), [8](#bib.bib8)].
    Besides, the MOS is the closest to human visual response, but needs to take a
    lot of efforts and is non-reproducible. Although researchers have proposed various
    metrics (Sec. [2.3](#S2.SS3 "2.3 Image Quality Assessment ‣ 2 Problem Setting
    and Terminology ‣ Deep Learning for Image Super-resolution: A Survey")), but currently
    there is no unified and admitted evaluation metrics for SR quality. Thus more
    accurate metrics for evaluating reconstruction quality are urgently needed.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '更准确的指标。如今，PSNR和SSIM已成为超分辨率中最广泛使用的指标。然而，PSNR往往导致过度平滑，结果在几乎不可区分的图像之间变化很大。SSIM[[58](#bib.bib58)]在亮度、对比度和结构方面进行评估，但仍无法准确测量感知质量[[25](#bib.bib25),
    [8](#bib.bib8)]。此外，MOS最接近人类视觉响应，但需要付出大量努力且不可重复。尽管研究人员提出了各种指标（第[2.3](#S2.SS3 "2.3
    Image Quality Assessment ‣ 2 Problem Setting and Terminology ‣ Deep Learning for
    Image Super-resolution: A Survey")节），但目前尚无统一且被接受的超分辨率质量评估指标。因此，迫切需要更准确的指标来评估重建质量。'
- en: Blind IQA Methods. Today most metrics used for SR are all-reference methods,
    i.e., assuming that we have paired LR-HR images with perfect quality. But since
    it’s difficult to obtain such datasets, the commonly used datasets for evaluation
    are often conducted by manual degradation. In this case, the task we perform evaluation
    on is actually the inverse process of the predefined degradation. Therefore, developing
    blind IQA methods also has great demands.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '**盲目图像质量评估方法**。如今，大多数用于超分辨率的指标都是全参考方法，即假设我们有完美质量的配对LR-HR图像。但由于难以获取这样的数据集，常用的评估数据集通常通过手动降质进行。在这种情况下，我们进行评估的任务实际上是预定义降质的逆过程。因此，开发盲目IQ评估方法也具有很大需求。'
- en: 6.4 Unsupervised Super-resolution
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 无监督超分辨率
- en: 'As mentioned in Sec. [4](#S4 "4 Unsupervised Super-resolution ‣ Deep Learning
    for Image Super-resolution: A Survey"), it is often difficult to collect images
    with different resolutions of the same scene, so bicubic interpolation is widely
    used for constructing SR datasets. However, the SR models trained on these datasets
    may only learn the inverse process of the predefined degradation. Therefore, how
    to perform unsupervised super-resolution (i.e., trained on datasets without paired
    LR-HR images) is a promising direction for future development.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '如[4](#S4 "4 Unsupervised Super-resolution ‣ Deep Learning for Image Super-resolution:
    A Survey")节中提到，收集同一场景的不同分辨率图像往往很困难，因此双三次插值被广泛用于构建超分辨率数据集。然而，在这些数据集上训练的超分辨率模型可能仅学习了预定义降质的逆过程。因此，如何进行无监督超分辨率（即在没有配对LR-HR图像的数据集上进行训练）是未来发展的一个有前途的方向。'
- en: 6.5 Towards Real-world Scenarios
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 面向真实世界场景
- en: Image super-resolution is greatly limited in real-world scenarios, such as suffering
    unknown degradation, missing paired LR-HR images. Below we’ll introduce some directions
    towards real-world scenarios.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图像超分辨率在真实世界场景中受到很大限制，例如遭遇未知的退化、缺失配对的LR-HR图像。接下来我们将介绍一些面向真实世界场景的方向。
- en: Dealing with Various Degradation. Real-world images tend to suffer degradation
    like blurring, additive noise and compression artifacts. Thus the models trained
    on datasets conducted manually often perform poorly in real-world scenes. Some
    works have been proposed for solving this [[39](#bib.bib39), [159](#bib.bib159),
    [149](#bib.bib149), [131](#bib.bib131)], but these methods have some inherent
    drawbacks, such as great training difficulty and over-perfect assumptions. This
    issue is urgently needed to be resolved.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 处理各种退化。真实世界图像往往会遭遇模糊、加性噪声和压缩伪影等退化。因此，基于手动生成的数据集训练的模型在真实世界场景中通常表现不佳。一些研究已提出解决方案
    [[39](#bib.bib39), [159](#bib.bib159), [149](#bib.bib149), [131](#bib.bib131)]，但这些方法存在一些固有的缺陷，如训练难度大和假设过于完美。这个问题亟需解决。
- en: 'Domain-specific Applications. Super-resolution can not only be used in domain-specific
    data and scenes directly, but also help other vision tasks greatly (Sec. [5](#S5
    "5 Domain-Specific Applications ‣ Deep Learning for Image Super-resolution: A
    Survey")). Therefore, it is also a promising direction to apply SR to more specific
    domains, such as video surveillance, object tracking, medical imaging and scene
    rendering.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '特定领域应用。超分辨率不仅可以直接应用于特定领域的数据和场景，还可以极大地帮助其他视觉任务（见第 [5](#S5 "5 Domain-Specific
    Applications ‣ Deep Learning for Image Super-resolution: A Survey) 节）。因此，将超分辨率应用于更具体的领域，如视频监控、目标跟踪、医学成像和场景渲染，也是一个有前途的方向。'
- en: Acknowledgment
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: Prof. Jian Chen is supported by the Guangdong special branch plans young talent
    with scientific and technological innovation (Grant No. 2016TQ03X445), the Guangzhou
    science and technology planning project (Grant No. 201904010197) and Natural Science
    Foundation of Guangdong Province, China (2016A030313437).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 陈健教授得到了广东省科技创新特支计划青年人才（资助编号：2016TQ03X445）、广州市科技计划项目（资助编号：201904010197）和广东省自然科学基金（资助编号：2016A030313437）的资助。
- en: References
  id: totrans-372
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Greenspan, “Super-resolution in medical imaging,” *The Computer Journal*,
    vol. 52, 2008.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Greenspan，“医学成像中的超分辨率，” *The Computer Journal*，第52卷，2008年。'
- en: '[2] J. S. Isaac and R. Kulkarni, “Super resolution techniques for medical image
    processing,” in *ICTSD*, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J. S. Isaac 和 R. Kulkarni，“用于医学图像处理的超分辨率技术，”发表于 *ICTSD*，2015年。'
- en: '[3] Y. Huang, L. Shao, and A. F. Frangi, “Simultaneous super-resolution and
    cross-modality synthesis of 3d medical images using weakly-supervised joint convolutional
    sparse coding,” in *CVPR*, 2017.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Huang, L. Shao, 和 A. F. Frangi，“使用弱监督联合卷积稀疏编码的三维医学图像的同时超分辨率和跨模态合成，”发表于
    *CVPR*，2017年。'
- en: '[4] L. Zhang, H. Zhang, H. Shen, and P. Li, “A super-resolution reconstruction
    algorithm for surveillance images,” *Elsevier Signal Processing*, vol. 90, 2010.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] L. Zhang, H. Zhang, H. Shen, 和 P. Li，“用于监控图像的超分辨率重建算法，” *Elsevier Signal
    Processing*，第90卷，2010年。'
- en: '[5] P. Rasti, T. Uiboupin, S. Escalera, and G. Anbarjafari, “Convolutional
    neural network super resolution for face recognition in surveillance monitoring,”
    in *AMDO*, 2016.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] P. Rasti, T. Uiboupin, S. Escalera, 和 G. Anbarjafari，“用于监控检测中的人脸识别的卷积神经网络超分辨率，”发表于
    *AMDO*，2016年。'
- en: '[6] D. Dai, Y. Wang, Y. Chen, and L. Van Gool, “Is image super-resolution helpful
    for other vision tasks?” in *WACV*, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Dai, Y. Wang, Y. Chen, 和 L. Van Gool，“图像超分辨率对其他视觉任务是否有帮助？”发表于 *WACV*，2016年。'
- en: '[7] M. Haris, G. Shakhnarovich, and N. Ukita, “Task-driven super resolution:
    Object detection in low-resolution images,” *Arxiv:1803.11316*, 2018.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Haris, G. Shakhnarovich, 和 N. Ukita，“任务驱动的超分辨率：低分辨率图像中的目标检测，” *Arxiv:1803.11316*，2018年。'
- en: '[8] M. S. Sajjadi, B. Schölkopf, and M. Hirsch, “Enhancenet: Single image super-resolution
    through automated texture synthesis,” in *ICCV*, 2017.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. S. Sajjadi, B. Schölkopf, 和 M. Hirsch，“Enhancenet：通过自动化纹理合成实现单幅图像超分辨率，”发表于
    *ICCV*，2017年。'
- en: '[9] Y. Zhang, Y. Bai, M. Ding, and B. Ghanem, “Sod-mtgan: Small object detection
    via multi-task generative adversarial network,” in *ECCV*, 2018.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Y. Zhang, Y. Bai, M. Ding, 和 B. Ghanem，“Sod-mtgan：通过多任务生成对抗网络进行小物体检测，”发表于
    *ECCV*，2018年。'
- en: '[10] R. Keys, “Cubic convolution interpolation for digital image processing,”
    *IEEE Transactions on Acoustics, Speech, and Signal Processing*, vol. 29, 1981.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Keys，“用于数字图像处理的立方卷积插值，” *IEEE Transactions on Acoustics, Speech, and
    Signal Processing*，第29卷，1981年。'
- en: '[11] C. E. Duchon, “Lanczos filtering in one and two dimensions,” *Journal
    of Applied Meteorology*, vol. 18, 1979.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. E. Duchon, “一维和二维的Lanczos滤波,” *应用气象学杂志*, 第18卷, 1979年。'
- en: '[12] M. Irani and S. Peleg, “Improving resolution by image registration,” *CVGIP:
    Graphical Models and Image Processing*, vol. 53, 1991.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Irani 和 S. Peleg, “通过图像配准提高分辨率,” *CVGIP: 图形模型与图像处理*, 第53卷, 1991年。'
- en: '[13] G. Freedman and R. Fattal, “Image and video upscaling from local self-examples,”
    *TOG*, vol. 30, 2011.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] G. Freedman 和 R. Fattal, “从局部自例中进行图像和视频放大,” *TOG*, 第30卷, 2011年。'
- en: '[14] J. Sun, Z. Xu, and H.-Y. Shum, “Image super-resolution using gradient
    profile prior,” in *CVPR*, 2008.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Sun, Z. Xu, 和 H.-Y. Shum, “使用梯度轮廓先验的图像超分辨率,” 见 *CVPR*, 2008年。'
- en: '[15] K. I. Kim and Y. Kwon, “Single-image super-resolution using sparse regression
    and natural image prior,” *TPAMI*, vol. 32, 2010.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] K. I. Kim 和 Y. Kwon, “使用稀疏回归和自然图像先验的单图像超分辨率,” *TPAMI*, 第32卷, 2010年。'
- en: '[16] Z. Xiong, X. Sun, and F. Wu, “Robust web image/video super-resolution,”
    *IEEE Transactions on Image Processing*, vol. 19, 2010.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Z. Xiong, X. Sun, 和 F. Wu, “鲁棒的网络图像/视频超分辨率,” *IEEE图像处理汇刊*, 第19卷, 2010年。'
- en: '[17] W. T. Freeman, T. R. Jones, and E. C. Pasztor, “Example-based super-resolution,”
    *IEEE Computer Graphics and Applications*, vol. 22, 2002.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] W. T. Freeman, T. R. Jones, 和 E. C. Pasztor, “基于示例的超分辨率,” *IEEE计算机图形与应用*,
    第22卷, 2002年。'
- en: '[18] H. Chang, D.-Y. Yeung, and Y. Xiong, “Super-resolution through neighbor
    embedding,” in *CVPR*, 2004.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] H. Chang, D.-Y. Yeung, 和 Y. Xiong, “通过邻域嵌入实现超分辨率,” 见 *CVPR*, 2004年。'
- en: '[19] D. Glasner, S. Bagon, and M. Irani, “Super-resolution from a single image,”
    in *ICCV*, 2009.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. Glasner, S. Bagon, 和 M. Irani, “从单张图像中获取超分辨率,” 见 *ICCV*, 2009年。'
- en: '[20] Y. Jianchao, J. Wright, T. Huang, and Y. Ma, “Image super-resolution as
    sparse representation of raw image patches,” in *CVPR*, 2008.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Jianchao, J. Wright, T. Huang, 和 Y. Ma, “图像超分辨率作为原始图像块的稀疏表示,” 见 *CVPR*,
    2008年。'
- en: '[21] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution via
    sparse representation,” *IEEE Transactions on Image Processing*, vol. 19, 2010.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] J. Yang, J. Wright, T. S. Huang, 和 Y. Ma, “通过稀疏表示的图像超分辨率,” *IEEE图像处理汇刊*,
    第19卷, 2010年。'
- en: '[22] C. Dong, C. C. Loy, K. He, and X. Tang, “Learning a deep convolutional
    network for image super-resolution,” in *ECCV*, 2014.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. Dong, C. C. Loy, K. He, 和 X. Tang, “为图像超分辨率学习深度卷积网络,” 见 *ECCV*, 2014年。'
- en: '[23] ——, “Image super-resolution using deep convolutional networks,” *TPAMI*,
    vol. 38, 2016.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] ——, “使用深度卷积网络的图像超分辨率,” *TPAMI*, 第38卷, 2016年。'
- en: '[24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NIPS*, 2014.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S.
    Ozair, A. Courville, 和 Y. Bengio, “生成对抗网络,” 见 *NIPS*, 2014年。'
- en: '[25] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,
    A. P. Aitken, A. Tejani, J. Totz, Z. Wang *et al.*, “Photo-realistic single image
    super-resolution using a generative adversarial network,” in *CVPR*, 2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,
    A. P. Aitken, A. Tejani, J. Totz, Z. Wang *等*, “使用生成对抗网络进行照片级真实感单图像超分辨率,” 见 *CVPR*,
    2017年。'
- en: '[26] J. Kim, J. Kwon Lee, and K. Mu Lee, “Accurate image super-resolution using
    very deep convolutional networks,” in *CVPR*, 2016.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Kim, J. Kwon Lee, 和 K. Mu Lee, “使用非常深的卷积网络进行准确的图像超分辨率,” 见 *CVPR*, 2016年。'
- en: '[27] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, “Deep laplacian pyramid
    networks for fast and accurate superresolution,” in *CVPR*, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] W.-S. Lai, J.-B. Huang, N. Ahuja, 和 M.-H. Yang, “深度拉普拉斯金字塔网络用于快速准确的超分辨率,”
    见 *CVPR*, 2017年。'
- en: '[28] N. Ahn, B. Kang, and K.-A. Sohn, “Fast, accurate, and lightweight super-resolution
    with cascading residual network,” in *ECCV*, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] N. Ahn, B. Kang, 和 K.-A. Sohn, “通过级联残差网络实现快速、准确、轻量级超分辨率,” 见 *ECCV*, 2018年。'
- en: '[29] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time
    style transfer and super-resolution,” in *ECCV*, 2016.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Johnson, A. Alahi, 和 L. Fei-Fei, “用于实时风格转换和超分辨率的感知损失,” 见 *ECCV*, 2016年。'
- en: '[30] A. Bulat and G. Tzimiropoulos, “Super-fan: Integrated facial landmark
    localization and super-resolution of real-world low resolution faces in arbitrary
    poses with gans,” in *CVPR*, 2018.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] A. Bulat 和 G. Tzimiropoulos, “Super-fan: 集成面部关键点定位与现实世界低分辨率面孔的超分辨率，采用GANs,”
    见 *CVPR*, 2018年。'
- en: '[31] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, “Enhanced deep residual
    networks for single image super-resolution,” in *CVPRW*, 2017.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] B. Lim, S. Son, H. Kim, S. Nah, 和 K. M. Lee, “增强型深度残差网络用于单图像超分辨率,” 见 *CVPRW*,
    2017年。'
- en: '[32] Y. Wang, F. Perazzi, B. McWilliams, A. Sorkine-Hornung, O. Sorkine-Hornung,
    and C. Schroers, “A fully progressive approach to single-image super-resolution,”
    in *CVPRW*, 2018.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Y. Wang, F. Perazzi, B. McWilliams, A. Sorkine-Hornung, O. Sorkine-Hornung,
    和 C. Schroers, “一种完全渐进的单图像超分辨率方法,” 发表在*CVPRW*，2018年。'
- en: '[33] S. C. Park, M. K. Park, and M. G. Kang, “Super-resolution image reconstruction:
    A technical overview,” *IEEE Signal Processing Magazine*, vol. 20, 2003.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. C. Park, M. K. Park, 和 M. G. Kang, “超分辨率图像重建: 技术概述,” *IEEE信号处理杂志*,
    第20卷, 2003年。'
- en: '[34] K. Nasrollahi and T. B. Moeslund, “Super-resolution: A comprehensive survey,”
    *Machine Vision and Applications*, vol. 25, 2014.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] K. Nasrollahi 和 T. B. Moeslund, “超分辨率: 一项全面的综述,” *机器视觉与应用*, 第25卷, 2014年。'
- en: '[35] J. Tian and K.-K. Ma, “A survey on super-resolution imaging,” *Signal,
    Image and Video Processing*, vol. 5, 2011.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Tian 和 K.-K. Ma, “超分辨率成像综述,” *信号、图像与视频处理*, 第5卷, 2011年。'
- en: '[36] J. Van Ouwerkerk, “Image super-resolution survey,” *Image and Vision Computing*,
    vol. 24, 2006.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] J. Van Ouwerkerk, “图像超分辨率调查,” *图像与视觉计算*, 第24卷, 2006年。'
- en: '[37] C.-Y. Yang, C. Ma, and M.-H. Yang, “Single-image super-resolution: A benchmark,”
    in *ECCV*, 2014.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] C.-Y. Yang, C. Ma, 和 M.-H. Yang, “单图像超分辨率: 基准测试,” 发表在*ECCV*，2014年。'
- en: '[38] D. Thapa, K. Raahemifar, W. R. Bobier, and V. Lakshminarayanan, “A performance
    comparison among different super-resolution techniques,” *Computers & Electrical
    Engineering*, vol. 54, 2016.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] D. Thapa, K. Raahemifar, W. R. Bobier, 和 V. Lakshminarayanan, “不同超分辨率技术的性能比较,”
    *计算机与电气工程*, 第54卷, 2016年。'
- en: '[39] K. Zhang, W. Zuo, and L. Zhang, “Learning a single convolutional super-resolution
    network for multiple degradations,” in *CVPR*, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] K. Zhang, W. Zuo, 和 L. Zhang, “针对多种降解的单卷积超分辨率网络学习,” 发表在*CVPR*，2018年。'
- en: '[40] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented
    natural images and its application to evaluating segmentation algorithms and measuring
    ecological statistics,” in *ICCV*, 2001.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] D. Martin, C. Fowlkes, D. Tal, 和 J. Malik, “一个人类分割自然图像数据库及其在评估分割算法和测量生态统计中的应用,”
    发表在*ICCV*，2001年。'
- en: '[41] P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, “Contour detection and
    hierarchical image segmentation,” *TPAMI*, vol. 33, 2011.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] P. Arbelaez, M. Maire, C. Fowlkes, 和 J. Malik, “轮廓检测与分层图像分割,” *TPAMI*,
    第33卷, 2011年。'
- en: '[42] E. Agustsson and R. Timofte, “Ntire 2017 challenge on single image super-resolution:
    Dataset and study,” in *CVPRW*, 2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] E. Agustsson 和 R. Timofte, “Ntire 2017 单图像超分辨率挑战赛: 数据集和研究,” 发表在*CVPRW*，2017年。'
- en: '[43] C. Dong, C. C. Loy, and X. Tang, “Accelerating the super-resolution convolutional
    neural network,” in *ECCV*, 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. Dong, C. C. Loy, 和 X. Tang, “加速超分辨率卷积神经网络,” 发表在*ECCV*，2016年。'
- en: '[44] R. Timofte, R. Rothe, and L. Van Gool, “Seven ways to improve example-based
    single image super resolution,” in *CVPR*, 2016.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] R. Timofte, R. Rothe, 和 L. Van Gool, “改进基于示例的单图像超分辨率的七种方法,” 发表在*CVPR*，2016年。'
- en: '[45] A. Fujimoto, T. Ogawa, K. Yamamoto, Y. Matsui, T. Yamasaki, and K. Aizawa,
    “Manga109 dataset and creation of metadata,” in *MANPU*, 2016.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Fujimoto, T. Ogawa, K. Yamamoto, Y. Matsui, T. Yamasaki, 和 K. Aizawa,
    “Manga109 数据集及元数据创建,” 发表在*MANPU*，2016年。'
- en: '[46] X. Wang, K. Yu, C. Dong, and C. C. Loy, “Recovering realistic texture
    in image super-resolution by deep spatial feature transform,” 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] X. Wang, K. Yu, C. Dong, 和 C. C. Loy, “通过深度空间特征变换恢复图像超分辨率中的真实纹理,” 2018年。'
- en: '[47] Y. Blau, R. Mechrez, R. Timofte, T. Michaeli, and L. Zelnik-Manor, “2018
    pirm challenge on perceptual image super-resolution,” in *ECCV Workshop*, 2018.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Y. Blau, R. Mechrez, R. Timofte, T. Michaeli, 和 L. Zelnik-Manor, “2018
    PIRM 挑战赛: 感知图像超分辨率,” 发表在*ECCV研讨会*，2018年。'
- en: '[48] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel, “Low-complexity
    single-image super-resolution based on nonnegative neighbor embedding,” in *BMVC*,
    2012.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Bevilacqua, A. Roumy, C. Guillemot, 和 M. L. Alberi-Morel, “基于非负邻域嵌入的低复杂度单图像超分辨率,”
    发表在*BMVC*，2012年。'
- en: '[49] R. Zeyde, M. Elad, and M. Protter, “On single image scale-up using sparse-representations,”
    in *International Conference on Curves and Surfaces*, 2010.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] R. Zeyde, M. Elad, 和 M. Protter, “基于稀疏表示的单图像放大,” 发表在*曲线与曲面国际会议*，2010年。'
- en: '[50] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution from
    transformed self-exemplars,” in *CVPR*, 2015.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J.-B. Huang, A. Singh, 和 N. Ahuja, “从转换自示例中进行单图像超分辨率,” 发表在*CVPR*，2015年。'
- en: '[51] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *CVPR*, 2009.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei, “Imagenet:
    一个大规模分层图像数据库,” 发表在*CVPR*，2009年。'
- en: '[52] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *ECCV*, 2014.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár
    和 C. L. Zitnick，“微软 COCO：上下文中的常见对象，” 见 *ECCV*，2014年。'
- en: '[53] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn, and
    A. Zisserman, “The pascal visual object classes challenge: A retrospective,” *IJCV*,
    vol. 111, 2015.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams, J. Winn 和 A.
    Zisserman，“PASCAL 视觉对象类别挑战：回顾，” *IJCV*，第111卷，2015年。'
- en: '[54] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in
    the wild,” in *ICCV*, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Z. Liu, P. Luo, X. Wang 和 X. Tang，“深度学习野外的人脸属性，” 见 *ICCV*，2015年。'
- en: '[55] Y. Tai, J. Yang, X. Liu, and C. Xu, “Memnet: A persistent memory network
    for image restoration,” in *ICCV*, 2017.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Tai, J. Yang, X. Liu 和 C. Xu，“Memnet：一种用于图像恢复的持久记忆网络，” 见 *ICCV*，2017年。'
- en: '[56] Y. Tai, J. Yang, and X. Liu, “Image super-resolution via deep recursive
    residual network,” in *CVPR*, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Y. Tai, J. Yang 和 X. Liu，“通过深度递归残差网络进行图像超分辨率，” 见 *CVPR*，2017年。'
- en: '[57] M. Haris, G. Shakhnarovich, and N. Ukita, “Deep backp-rojection networks
    for super-resolution,” in *CVPR*, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] M. Haris, G. Shakhnarovich 和 N. Ukita，“用于超分辨率的深度反向投影网络，” 见 *CVPR*，2018年。'
- en: '[58] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: From error visibility to structural similarity,” *IEEE Transactions
    on Image Processing*, vol. 13, 2004.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Z. Wang, A. C. Bovik, H. R. Sheikh 和 E. P. Simoncelli，“图像质量评估：从误差可见性到结构相似性，”
    *IEEE 图像处理汇刊*，第13卷，2004年。'
- en: '[59] Z. Wang, A. C. Bovik, and L. Lu, “Why is image quality assessment so difficult?”
    in *ICASSP*, 2002.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Z. Wang, A. C. Bovik 和 L. Lu，“为什么图像质量评估如此困难？” 见 *ICASSP*，2002年。'
- en: '[60] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, “A statistical evaluation
    of recent full reference image quality assessment algorithms,” *IEEE Transactions
    on Image Processing*, vol. 15, 2006.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] H. R. Sheikh, M. F. Sabir 和 A. C. Bovik，“对近期全参考图像质量评估算法的统计评估，” *IEEE 图像处理汇刊*，第15卷，2006年。'
- en: '[61] Z. Wang and A. C. Bovik, “Mean squared error: Love it or leave it? a new
    look at signal fidelity measures,” *IEEE Signal Processing Magazine*, vol. 26,
    2009.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Z. Wang 和 A. C. Bovik，“均方误差：爱它还是弃它？对信号保真度度量的新看法，” *IEEE 信号处理杂志*，第26卷，2009年。'
- en: '[62] Z. Wang, D. Liu, J. Yang, W. Han, and T. Huang, “Deep networks for image
    super-resolution with sparse prior,” in *ICCV*, 2015.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Z. Wang, D. Liu, J. Yang, W. Han 和 T. Huang，“具有稀疏先验的图像超分辨率深度网络，” 见 *ICCV*，2015年。'
- en: '[63] X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister, and M.-H. Yang, “Learning
    to super-resolve blurry face and text images,” in *ICCV*, 2017.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Xu, D. Sun, J. Pan, Y. Zhang, H. Pfister 和 M.-H. Yang，“学习超分辨率模糊的人脸和文本图像，”
    见 *ICCV*，2017年。'
- en: '[64] R. Dahl, M. Norouzi, and J. Shlens, “Pixel recursive super resolution,”
    in *ICCV*, 2017.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R. Dahl, M. Norouzi 和 J. Shlens，“像素递归超分辨率，” 见 *ICCV*，2017年。'
- en: '[65] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang, “Fast and accurate image
    super-resolution with deep laplacian pyramid networks,” *TPAMI*, 2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] W.-S. Lai, J.-B. Huang, N. Ahuja 和 M.-H. Yang，“利用深度拉普拉斯金字塔网络进行快速且准确的图像超分辨率，”
    *TPAMI*，2018年。'
- en: '[66] C. Ma, C.-Y. Yang, X. Yang, and M.-H. Yang, “Learning a no-reference quality
    metric for single-image super-resolution,” *Computer Vision and Image Understanding*,
    2017.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] C. Ma, C.-Y. Yang, X. Yang 和 M.-H. Yang，“学习单图像超分辨率的无参考质量度量，” *计算机视觉与图像理解*，2017年。'
- en: '[67] H. Talebi and P. Milanfar, “Nima: Neural image assessment,” *IEEE Transactions
    on Image Processing*, vol. 27, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. Talebi 和 P. Milanfar，“Nima：神经图像评估，” *IEEE 图像处理汇刊*，第27卷，2018年。'
- en: '[68] J. Kim and S. Lee, “Deep learning of human visual sensitivity in image
    quality assessment framework,” in *CVPR*, 2017.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Kim 和 S. Lee，“在图像质量评估框架中深度学习人类视觉敏感性，” 见 *CVPR*，2017年。'
- en: '[69] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *CVPR*, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] R. Zhang, P. Isola, A. A. Efros, E. Shechtman 和 O. Wang，“深度特征作为感知度量的非凡有效性，”
    见 *CVPR*，2018年。'
- en: '[70] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-resolution
    using very deep residual channel attention networks,” in *ECCV*, 2018.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong 和 Y. Fu，“使用非常深的残差通道注意力网络进行图像超分辨率，”
    见 *ECCV*，2018年。'
- en: '[71] C. Fookes, F. Lin, V. Chandran, and S. Sridharan, “Evaluation of image
    resolution and super-resolution on face recognition performance,” *Journal of
    Visual Communication and Image Representation*, vol. 23, 2012.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] C. Fookes, F. Lin, V. Chandran 和 S. Sridharan，“图像分辨率和超分辨率对人脸识别性能的评估，”
    *视觉通信与图像表示杂志*，第23卷，2012年。'
- en: '[72] K. Zhang, Z. ZHANG, C.-W. Cheng, W. Hsu, Y. Qiao, W. Liu, and T. Zhang,
    “Super-identity convolutional neural network for face hallucination,” in *ECCV*,
    2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] K. Zhang, Z. ZHANG, C.-W. Cheng, W. Hsu, Y. Qiao, W. Liu 和 T. Zhang，“用于面部幻觉的超身份卷积神经网络”，发表于
    *ECCV*，2018。'
- en: '[73] Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang, “Fsrnet: End-to-end learning
    face super-resolution with facial priors,” in *CVPR*, 2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. Chen, Y. Tai, X. Liu, C. Shen 和 J. Yang，“Fsrnet：具有面部先验的端到端学习面部超分辨率”，发表于
    *CVPR*，2018。'
- en: '[74] Z. Wang, E. Simoncelli, A. Bovik *et al.*, “Multi-scale structural similarity
    for image quality assessment,” in *Asilomar Conference on Signals, Systems, and
    Computers*, 2003.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Z. Wang, E. Simoncelli, A. Bovik *等*，“用于图像质量评估的多尺度结构相似性”，发表于 *Asilomar
    Conference on Signals, Systems, and Computers*，2003。'
- en: '[75] L. Zhang, L. Zhang, X. Mou, D. Zhang *et al.*, “Fsim: a feature similarity
    index for image quality assessment,” *IEEE transactions on Image Processing*,
    vol. 20, 2011.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] L. Zhang, L. Zhang, X. Mou, D. Zhang *等*，“Fsim：用于图像质量评估的特征相似性指数”，*IEEE
    Transactions on Image Processing*，第20卷，2011。'
- en: '[76] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely blind”
    image quality analyzer,” *IEEE Signal Processing Letters*, 2013.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] A. Mittal, R. Soundararajan 和 A. C. Bovik，“制作一个“完全盲”的图像质量分析器”，*IEEE Signal
    Processing Letters*，2013。'
- en: '[77] Y. Blau and T. Michaeli, “The perception-distortion tradeoff,” in *CVPR*,
    2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Y. Blau 和 T. Michaeli，“感知-失真权衡”，发表于 *CVPR*，2018。'
- en: '[78] X. Mao, C. Shen, and Y.-B. Yang, “Image restoration using very deep convolutional
    encoder-decoder networks with symmetric skip connections,” in *NIPS*, 2016.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] X. Mao, C. Shen 和 Y.-B. Yang，“使用对称跳跃连接的非常深卷积编码-解码网络进行图像恢复”，发表于 *NIPS*，2016。'
- en: '[79] T. Tong, G. Li, X. Liu, and Q. Gao, “Image super-resolution using dense
    skip connections,” in *ICCV*, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] T. Tong, G. Li, X. Liu 和 Q. Gao，“使用密集跳跃连接的图像超分辨率”，发表于 *ICCV*，2017。'
- en: '[80] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, L. Zhang, B. Lim, S. Son,
    H. Kim, S. Nah, K. M. Lee *et al.*, “Ntire 2017 challenge on single image super-resolution:
    Methods and results,” in *CVPRW*, 2017.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, L. Zhang, B. Lim, S.
    Son, H. Kim, S. Nah, K. M. Lee *等*，“Ntire 2017 单图像超分辨率挑战：方法与结果”，发表于 *CVPRW*，2017。'
- en: '[81] A. Ignatov, R. Timofte, T. Van Vu, T. Minh Luu, T. X Pham, C. Van Nguyen,
    Y. Kim, J.-S. Choi, M. Kim, J. Huang *et al.*, “Pirm challenge on perceptual image
    enhancement on smartphones: report,” in *ECCV Workshop*, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] A. Ignatov, R. Timofte, T. Van Vu, T. Minh Luu, T. X Pham, C. Van Nguyen,
    Y. Kim, J.-S. Choi, M. Kim, J. Huang *等*，“Pirm 挑战：智能手机上感知图像增强的报告”，发表于 *ECCV Workshop*，2018。'
- en: '[82] J. Kim, J. Kwon Lee, and K. Mu Lee, “Deeply-recursive convolutional network
    for image super-resolution,” in *CVPR*, 2016.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Kim, J. Kwon Lee 和 K. Mu Lee，“用于图像超分辨率的深度递归卷积网络”，发表于 *CVPR*，2016。'
- en: '[83] A. Shocher, N. Cohen, and M. Irani, ““zero-shot” super-resolution using
    deep internal learning,” in *CVPR*, 2018.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] A. Shocher, N. Cohen 和 M. Irani，“使用深度内部学习的“零样本”超分辨率”，发表于 *CVPR*，2018。'
- en: '[84] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert,
    and Z. Wang, “Real-time single image and video super-resolution using an efficient
    sub-pixel convolutional neural network,” in *CVPR*, 2016.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop, D.
    Rueckert 和 Z. Wang，“实时单图像和视频超分辨率使用高效的子像素卷积神经网络”，发表于 *CVPR*，2016。'
- en: '[85] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock, and T. S. Huang, “Image
    super-resolution via dual-state recurrent networks,” in *CVPR*, 2018.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] W. Han, S. Chang, D. Liu, M. Yu, M. Witbrock 和 T. S. Huang，“通过双状态递归网络实现图像超分辨率”，发表于
    *CVPR*，2018。'
- en: '[86] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon, and W. Wu, “Feedback network
    for image super-resolution,” in *CVPR*, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Z. Li, J. Yang, Z. Liu, X. Yang, G. Jeon 和 W. Wu，“用于图像超分辨率的反馈网络”，发表于 *CVPR*，2019。'
- en: '[87] M. Haris, G. Shakhnarovich, and N. Ukita, “Recurrent back-projection network
    for video super-resolution,” in *CVPR*, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] M. Haris, G. Shakhnarovich 和 N. Ukita，“用于视频超分辨率的递归回投网络”，发表于 *CVPR*，2019。'
- en: '[88] R. Timofte, V. De Smet, and L. Van Gool, “A+: Adjusted anchored neighborhood
    regression for fast super-resolution,” in *ACCV*, 2014.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] R. Timofte, V. De Smet 和 L. Van Gool，“A+: 调整的锚点邻域回归以实现快速超分辨率”，发表于 *ACCV*，2014。'
- en: '[89] S. Schulter, C. Leistner, and H. Bischof, “Fast and accurate image upscaling
    with super-resolution forests,” in *CVPR*, 2015.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] S. Schulter, C. Leistner 和 H. Bischof，“使用超分辨率森林进行快速而准确的图像放大”，发表于 *CVPR*，2015。'
- en: '[90] M. D. Zeiler, D. Krishnan, G. W. Taylor, and R. Fergus, “Deconvolutional
    networks,” in *CVPRW*, 2010.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] M. D. Zeiler, D. Krishnan, G. W. Taylor 和 R. Fergus，“反卷积网络”，发表于 *CVPRW*，2010。'
- en: '[91] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional
    networks,” in *ECCV*, 2014.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] M. D. Zeiler 和 R. Fergus，“可视化和理解卷积网络”，发表于 *ECCV*，2014。'
- en: '[92] A. Odena, V. Dumoulin, and C. Olah, “Deconvolution and checkerboard artifacts,”
    *Distill*, 2016.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] A. Odena, V. Dumoulin, 和 C. Olah，“反卷积和棋盘格伪影，” *Distill*，2016年。'
- en: '[93] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, “Residual dense network
    for image super-resolution,” in *CVPR*, 2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, 和 Y. Fu，“用于图像超分辨率的残差密集网络，”发表于 *CVPR*，2018年。'
- en: '[94] H. Gao, H. Yuan, Z. Wang, and S. Ji, “Pixel transposed convolutional networks,”
    *TPAMI*, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] H. Gao, H. Yuan, Z. Wang, 和 S. Ji，“像素转置卷积网络，” *TPAMI*，2019年。'
- en: '[95] X. Hu, H. Mu, X. Zhang, Z. Wang, T. Tan, and J. Sun, “Meta-sr: A magnification-arbitrary
    network for super-resolution,” in *CVPR*, 2019.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Hu, H. Mu, X. Zhang, Z. Wang, T. Tan, 和 J. Sun，“Meta-sr：一种用于超分辨率的放大任意网络，”发表于
    *CVPR*，2019年。'
- en: '[96] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] K. He, X. Zhang, S. Ren, 和 J. Sun，“用于图像识别的深度残差学习，”发表于 *CVPR*，2016年。'
- en: '[97] R. Timofte, V. De Smet, and L. Van Gool, “Anchored neighborhood regression
    for fast example-based super-resolution,” in *ICCV*, 2013.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] R. Timofte, V. De Smet, 和 L. Van Gool，“用于快速基于示例的超分辨率的锚定邻域回归，”发表于 *ICCV*，2013年。'
- en: '[98] Z. Hui, X. Wang, and X. Gao, “Fast and accurate single image super-resolution
    via information distillation network,” in *CVPR*, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Z. Hui, X. Wang, 和 X. Gao，“通过信息蒸馏网络实现快速且准确的单幅图像超分辨率，”发表于 *CVPR*，2018年。'
- en: '[99] J. Li, F. Fang, K. Mei, and G. Zhang, “Multi-scale residual network for
    image super-resolution,” in *ECCV*, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] J. Li, F. Fang, K. Mei, 和 G. Zhang，“用于图像超分辨率的多尺度残差网络，”发表于 *ECCV*，2018年。'
- en: '[100] H. Ren, M. El-Khamy, and J. Lee, “Image super resolution based on fusing
    multiple convolution neural networks,” in *CVPRW*, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] H. Ren, M. El-Khamy, 和 J. Lee，“基于融合多个卷积神经网络的图像超分辨率，”发表于 *CVPRW*，2017年。'
- en: '[101] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” in *CVPR*,
    2015.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan,
    V. Vanhoucke, 和 A. Rabinovich，“深入卷积，”发表于 *CVPR*，2015年。'
- en: '[102] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected
    convolutional networks,” in *CVPR*, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] G. Huang, Z. Liu, L. Van Der Maaten, 和 K. Q. Weinberger，“密集连接卷积网络，”发表于
    *CVPR*，2017年。'
- en: '[103] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, C. C. Loy, Y. Qiao, and
    X. Tang, “Esrgan: Enhanced super-resolution generative adversarial networks,”
    in *ECCV Workshop*, 2018.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, C. C. Loy, Y. Qiao, 和
    X. Tang，“Esrgan：增强的超分辨率生成对抗网络，”发表于 *ECCV Workshop*，2018年。'
- en: '[104] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in *CVPR*,
    2018.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] J. Hu, L. Shen, 和 G. Sun，“挤压和激励网络，”发表于 *CVPR*，2018年。'
- en: '[105] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, “Second-order attention
    network for single image super-resolution,” in *CVPR*, 2019.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, 和 L. Zhang，“用于单幅图像超分辨率的二阶注意力网络，”发表于
    *CVPR*，2019年。'
- en: '[106] Y. Zhang, K. Li, K. Li, B. Zhong, and Y. Fu, “Residual non-local attention
    networks for image restoration,” *ICLR*, 2019.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Y. Zhang, K. Li, K. Li, B. Zhong, 和 Y. Fu，“用于图像恢复的残差非局部注意力网络，” *ICLR*，2019年。'
- en: '[107] K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser prior
    for image restoration,” in *CVPR*, 2017.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. Zhang, W. Zuo, S. Gu, 和 L. Zhang，“用于图像恢复的深度 CNN 去噪器先验学习，”发表于 *CVPR*，2017年。'
- en: '[108] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in *CVPR*, 2017.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] S. Xie, R. Girshick, P. Dollár, Z. Tu, 和 K. He，“用于深度神经网络的聚合残差变换，”发表于
    *CVPR*，2017年。'
- en: '[109] F. Chollet, “Xception: Deep learning with depthwise separable convolutions,”
    in *CVPR*, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] F. Chollet，“Xception：具有深度可分卷积的深度学习，”发表于 *CVPR*，2017年。'
- en: '[110] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” *Arxiv:1704.04861*, 2017.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, 和 H. Adam，“Mobilenets：用于移动视觉应用的高效卷积神经网络，” *Arxiv:1704.04861*，2017年。'
- en: '[111] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves
    *et al.*, “Conditional image generation with pixelcnn decoders,” in *NIPS*, 2016.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] A. van den Oord, N. Kalchbrenner, L. Espeholt, O. Vinyals, A. Graves
    *等*，“使用 PixelCNN 解码器的条件图像生成，”发表于 *NIPS*，2016年。'
- en: '[112] J. Najemnik and W. S. Geisler, “Optimal eye movement strategies in visual
    search,” *Nature*, vol. 434, 2005.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] J. Najemnik 和 W. S. Geisler，“视觉搜索中的最佳眼动策略，” *Nature*，第434卷，2005年。'
- en: '[113] Q. Cao, L. Lin, Y. Shi, X. Liang, and G. Li, “Attention-aware face hallucination
    via deep reinforcement learning,” in *CVPR*, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Q. Cao, L. Lin, Y. Shi, X. Liang, 和 G. Li，“通过深度强化学习进行注意力感知的面部幻觉，”发表于
    *CVPR*，2017年。'
- en: '[114] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
    convolutional networks for visual recognition,” in *ECCV*, 2014.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] K. He, X. Zhang, S. Ren 和 J. Sun, “深度卷积网络中的空间金字塔池化用于视觉识别，” 在 *ECCV*，2014。'
- en: '[115] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,”
    in *CVPR*, 2017.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] H. Zhao, J. Shi, X. Qi, X. Wang 和 J. Jia, “金字塔场景解析网络，” 在 *CVPR*，2017。'
- en: '[116] D. Park, K. Kim, and S. Y. Chun, “Efficient module based single image
    super resolution for multiple problems,” in *CVPRW*, 2018.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] D. Park, K. Kim 和 S. Y. Chun, “基于模块的单图像超分辨率的高效方法，” 在 *CVPRW*，2018。'
- en: '[117] I. Daubechies, *Ten lectures on wavelets*.   SIAM, 1992.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] I. Daubechies, *小波的十讲*。 SIAM, 1992.'
- en: '[118] S. Mallat, *A wavelet tour of signal processing*.   Elsevier, 1999.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] S. Mallat, *信号处理的小波之旅*。 Elsevier, 1999。'
- en: '[119] W. Bae, J. J. Yoo, and J. C. Ye, “Beyond deep residual learning for image
    restoration: Persistent homology-guided manifold simplification,” in *CVPRW*,
    2017.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] W. Bae, J. J. Yoo 和 J. C. Ye, “超越深度残差学习的图像恢复：持久同调引导的流形简化，” 在 *CVPRW*，2017。'
- en: '[120] T. Guo, H. S. Mousavi, T. H. Vu, and V. Monga, “Deep wavelet prediction
    for image super-resolution,” in *CVPRW*, 2017.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] T. Guo, H. S. Mousavi, T. H. Vu 和 V. Monga, “用于图像超分辨率的深度小波预测，” 在 *CVPRW*，2017。'
- en: '[121] H. Huang, R. He, Z. Sun, T. Tan *et al.*, “Wavelet-srnet: A wavelet-based
    cnn for multi-scale face super resolution,” in *ICCV*, 2017.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] H. Huang, R. He, Z. Sun, T. Tan *et al.*，“Wavelet-srnet：用于多尺度人脸超分辨率的基于小波的CNN，”
    在 *ICCV*，2017。'
- en: '[122] P. Liu, H. Zhang, K. Zhang, L. Lin, and W. Zuo, “Multi-level wavelet-cnn
    for image restoration,” in *CVPRW*, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. Liu, H. Zhang, K. Zhang, L. Lin 和 W. Zuo, “用于图像恢复的多层小波CNN，” 在 *CVPRW*，2018。'
- en: '[123] T. Vu, C. Van Nguyen, T. X. Pham, T. M. Luu, and C. D. Yoo, “Fast and
    efficient image quality enhancement via desubpixel convolutional neural networks,”
    in *ECCV Workshop*, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] T. Vu, C. Van Nguyen, T. X. Pham, T. M. Luu 和 C. D. Yoo, “通过亚像素卷积神经网络进行快速高效的图像质量增强，”
    在 *ECCV Workshop*，2018。'
- en: '[124] I. Kligvasser, T. Rott Shaham, and T. Michaeli, “xunit: Learning a spatial
    activation function for efficient image restoration,” in *CVPR*, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] I. Kligvasser, T. Rott Shaham 和 T. Michaeli, “xunit：学习空间激活函数以实现高效图像恢复，”
    在 *CVPR*，2018。'
- en: '[125] A. Bruhn, J. Weickert, and C. Schnörr, “Lucas/kanade meets horn/schunck:
    Combining local and global optic flow methods,” *IJCV*, vol. 61, 2005.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] A. Bruhn, J. Weickert 和 C. Schnörr, “Lucas/kanade遇到horn/schunck：结合局部和全局光流方法，”
    *IJCV*，第61卷，2005。'
- en: '[126] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
    restoration with neural networks,” *IEEE Transactions on Computational Imaging*,
    vol. 3, 2017.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] H. Zhao, O. Gallo, I. Frosio 和 J. Kautz, “用于图像恢复的损失函数与神经网络，” *IEEE Transactions
    on Computational Imaging*，第3卷，2017。'
- en: '[127] A. Dosovitskiy and T. Brox, “Generating images with perceptual similarity
    metrics based on deep networks,” in *NIPS*, 2016.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] A. Dosovitskiy 和 T. Brox, “基于深度网络的感知相似度度量生成图像，” 在 *NIPS*，2016。'
- en: '[128] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络，” 在 *ICLR*，2015。'
- en: '[129] L. Gatys, A. S. Ecker, and M. Bethge, “Texture synthesis using convolutional
    neural networks,” in *NIPS*, 2015.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] L. Gatys, A. S. Ecker 和 M. Bethge, “使用卷积神经网络的纹理合成，” 在 *NIPS*，2015。'
- en: '[130] L. A. Gatys, A. S. Ecker, and M. Bethge, “Image style transfer using
    convolutional neural networks,” in *CVPR*, 2016.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] L. A. Gatys, A. S. Ecker 和 M. Bethge, “使用卷积神经网络的图像风格迁移，” 在 *CVPR*，2016。'
- en: '[131] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, and L. Lin, “Unsupervised
    image super-resolution using cycle-in-cycle generative adversarial networks,”
    in *CVPRW*, 2018.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong 和 L. Lin, “使用周期生成对抗网络的无监督图像超分辨率，”
    在 *CVPRW*，2018。'
- en: '[132] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang, and S. P. Smolley, “Least
    squares generative adversarial networks,” in *ICCV*, 2017.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] X. Mao, Q. Li, H. Xie, R. Y. Lau, Z. Wang 和 S. P. Smolley, “最小二乘生成对抗网络，”
    在 *ICCV*，2017。'
- en: '[133] S.-J. Park, H. Son, S. Cho, K.-S. Hong, and S. Lee, “Srfeat: Single image
    super resolution with feature discrimination,” in *ECCV*, 2018.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] S.-J. Park, H. Son, S. Cho, K.-S. Hong 和 S. Lee, “Srfeat：具有特征区分的单图像超分辨率，”
    在 *ECCV*，2018。'
- en: '[134] A. Jolicoeur-Martineau, “The relativistic discriminator: a key element
    missing from standard gan,” *Arxiv:1807.00734*, 2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. Jolicoeur-Martineau, “相对论判别器：标准GAN中缺失的关键元素，” *Arxiv:1807.00734*，2018。'
- en: '[135] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” in *ICML*, 2017.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] M. Arjovsky, S. Chintala 和 L. Bottou, “Wasserstein生成对抗网络，” 在 *ICML*，2017。'
- en: '[136] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,
    “Improved training of wasserstein gans,” in *NIPS*, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville,
    “改进的Wasserstein GANs训练方式”，2017年，*NIPS*会议。'
- en: '[137] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral normalization
    for generative adversarial networks,” in *ICLR*, 2018.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “谱归一化用于生成对抗网络”，2018年，*ICLR*会议。'
- en: '[138] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
    translation using cycle-consistent adversarial networks,” in *ICCV*, 2017.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “通过循环一致的对抗网络进行非配对图像转换”，2017年，*ICCV*会议。'
- en: '[139] L. I. Rudin, S. Osher, and E. Fatemi, “Nonlinear total variation based
    noise removal algorithms,” *Physica D: Nonlinear Phenomena*, vol. 60, 1992.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] L. I. Rudin, S. Osher, and E. Fatemi, “基于非线性全变差的噪声去除算法”，1992年，*非线性现象物理学*杂志，第60卷。'
- en: '[140] H. A. Aly and E. Dubois, “Image up-sampling using total-variation regularization
    with a new observation model,” *IEEE Transactions on Image Processing*, vol. 14,
    2005.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] H. A. Aly and E. Dubois, “使用全变差正则化的图像上采样与新的观测模型”，2005年，*IEEE图像处理*杂志，第14卷。'
- en: '[141] Y. Guo, Q. Chen, J. Chen, J. Huang, Y. Xu, J. Cao, P. Zhao, and M. Tan,
    “Dual reconstruction nets for image super-resolution with gradient sensitive loss,”
    *arXiv:1809.07099*, 2018.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Guo, Q. Chen, J. Chen, J. Huang, Y. Xu, J. Cao, P. Zhao, and M. Tan,
    “梯度敏感损失的图像超分辨率双重重建网络”，2018年，*arXiv:1809.07099*。'
- en: '[142] S. Vasu, N. T. Madam *et al.*, “Analyzing perception-distortion tradeoff
    using enhanced perceptual super-resolution network,” in *ECCV Workshop*, 2018.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] S. Vasu, N. T. Madam *et al.*, “使用增强的感知超分辨率网络分析感知-失真权衡”，2018年，*ECCV Workshop*会议。'
- en: '[143] M. Cheon, J.-H. Kim, J.-H. Choi, and J.-S. Lee, “Generative adversarial
    network-based image super-resolution using perceptual content losses,” in *ECCV
    Workshop*, 2018.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] M. Cheon, J.-H. Kim, J.-H. Choi, and J.-S. Lee, “基于生成对抗网络的感知内容丢失图像超分辨率方法”，2018年，*ECCV
    Workshop*会议。'
- en: '[144] J.-H. Choi, J.-H. Kim, M. Cheon, and J.-S. Lee, “Deep learning-based
    image super-resolution considering quantitative and perceptual quality,” in *ECCV
    Workshop*, 2018.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J.-H. Choi, J.-H. Kim, M. Cheon, and J.-S. Lee, “在考虑定量和感知质量的基础上的基于深度学习的图像超分辨率方法”，2018年，*ECCV
    Workshop*会议。'
- en: '[145] I. Sergey and S. Christian, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in *ICML*, 2015.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] I. Sergey and S. Christian, “通过减少内部协变量漂移加速深度神经网络的训练”，2015年，*ICML*会议。'
- en: '[146] C. K. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Huszár, “Amortised
    map inference for image super-resolution,” in *ICLR*, 2017.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] C. K. Sønderby, J. Caballero, L. Theis, W. Shi, and F. Huszár, “基于摊薄的图像超分辨率的映射推断方法”，2017年，*ICLR*会议。'
- en: '[147] R. Chen, Y. Qu, K. Zeng, J. Guo, C. Li, and Y. Xie, “Persistent memory
    residual network for single image super resolution,” in *CVPRW*, 2018.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] R. Chen, Y. Qu, K. Zeng, J. Guo, C. Li, and Y. Xie, “面向单张图像超分辨率的持久内存残差网络”，2018年，*CVPRW*会议。'
- en: '[148] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “Curriculum learning,”
    in *ICML*, 2009.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, “课程学习”，2009年，*ICML*会议。'
- en: '[149] Y. Bei, A. Damian, S. Hu, S. Menon, N. Ravi, and C. Rudin, “New techniques
    for preserving global structure and denoising with low information loss in single-image
    super-resolution,” in *CVPRW*, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Bei, A. Damian, S. Hu, S. Menon, N. Ravi, and C. Rudin, “在单张图像超分辨率中保留全局结构和降噪方面的新技术”，2018年，*CVPRW*会议。'
- en: '[150] N. Ahn, B. Kang, and K.-A. Sohn, “Image super-resolution via progressive
    cascading residual network,” in *CVPRW*, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] N. Ahn, B. Kang, and K.-A. Sohn, “渐进级联残差网络的图像超分辨率方法”，2018年，*CVPRW*会议。'
- en: '[151] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive growing of
    gans for improved quality, stability, and variation,” in *ICLR*, 2018.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “为提高质量、稳定性和多样性而渐进式增长的GANs”，2018年，*ICLR*会议。'
- en: '[152] R. Caruana, “Multitask learning,” *Machine Learning*, vol. 28, 1997.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] R. Caruana, “多任务学习”，1997年，*机器学习*杂志，第28卷。'
- en: '[153] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *ICCV*,
    2017.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn”，2017年，*ICCV*会议。'
- en: '[154] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “Facial landmark detection
    by deep multi-task learning,” in *ECCV*, 2014.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Z. Zhang, P. Luo, C. C. Loy, and X. Tang, “深度多任务学习用于人脸关键点检测”，2014年，*ECCV*会议。'
- en: '[155] X. Wang, K. Yu, C. Dong, X. Tang, and C. C. Loy, “Deep network interpolation
    for continuous imagery effect transition,” in *CVPR*, 2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] X. Wang, K. Yu, C. Dong, X. Tang, and C. C. Loy, “连续图像效果过渡的深层网络插值方法”，2019年，*CVPR*会议。'
- en: '[156] J. Caballero, C. Ledig, A. P. Aitken, A. Acosta, J. Totz, Z. Wang, and
    W. Shi, “Real-time video super-resolution with spatio-temporal networks and motion
    compensation,” in *CVPR*, 2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Caballero, C. Ledig, A. P. Aitken, A. Acosta, J. Totz, Z. Wang, 和
    W. Shi, “通过时空网络和运动补偿进行实时视频超分辨率”，发表于 *CVPR*，2017年。'
- en: '[157] L. Zhu, “pytorch-opcounter,” https://github.com/Lyken17/pytorch-OpCounter,
    2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] L. Zhu, “pytorch-opcounter”，https://github.com/Lyken17/pytorch-OpCounter，2019年。'
- en: '[158] T. Michaeli and M. Irani, “Nonparametric blind super-resolution,” in
    *ICCV*, 2013.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] T. Michaeli 和 M. Irani, “非参数盲超分辨率”，发表于 *ICCV*，2013年。'
- en: '[159] A. Bulat, J. Yang, and G. Tzimiropoulos, “To learn image super-resolution,
    use a gan to learn how to do image degradation first,” in *ECCV*, 2018.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] A. Bulat, J. Yang, 和 G. Tzimiropoulos, “要学习图像超分辨率，首先使用gan学习如何进行图像降质”，发表于
    *ECCV*，2018年。'
- en: '[160] D. Ulyanov, A. Vedaldi, and V. Lempitsky, “Deep image prior,” in *CVPR*,
    2018.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] D. Ulyanov, A. Vedaldi, 和 V. Lempitsky, “深度图像先验”，发表于 *CVPR*，2018年。'
- en: '[161] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore,
    A. Kipman, and A. Blake, “Real-time human pose recognition in parts from single
    depth images,” in *CVPR*, 2011.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore,
    A. Kipman, 和 A. Blake, “从单个深度图像中实时人体姿态识别”，发表于 *CVPR*，2011年。'
- en: '[162] G. Moon, J. Yong Chang, and K. Mu Lee, “V2v-posenet: Voxel-to-voxel prediction
    network for accurate 3d hand and human pose estimation from a single depth map,”
    in *CVPR*, 2018.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] G. Moon, J. Yong Chang, 和 K. Mu Lee, “V2v-posenet：用于从单一深度图估计准确3d手部和人体姿态的体素到体素预测网络”，发表于
    *CVPR*，2018年。'
- en: '[163] S. Gupta, R. Girshick, P. Arbeláez, and J. Malik, “Learning rich features
    from rgb-d images for object detection and segmentation,” in *ECCV*, 2014.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] S. Gupta, R. Girshick, P. Arbeláez, 和 J. Malik, “从rgb-d图像中学习丰富特征用于对象检测和分割”，发表于
    *ECCV*，2014年。'
- en: '[164] W. Wang and U. Neumann, “Depth-aware cnn for rgb-d segmentation,” in
    *ECCV*, 2018.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] W. Wang 和 U. Neumann, “用于rgb-d分割的深度感知cnn”，发表于 *ECCV*，2018年。'
- en: '[165] X. Song, Y. Dai, and X. Qin, “Deep depth super-resolution: Learning depth
    super-resolution using deep convolutional neural network,” in *ACCV*, 2016.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] X. Song, Y. Dai, 和 X. Qin, “深度超分辨率：利用深度卷积神经网络学习深度超分辨率”，发表于 *ACCV*，2016年。'
- en: '[166] T.-W. Hui, C. C. Loy, and X. Tang, “Depth map super-resolution by deep
    multi-scale guidance,” in *ECCV*, 2016.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] T.-W. Hui, C. C. Loy, 和 X. Tang, “通过深度多尺度指导进行深度图超分辨率”，发表于 *ECCV*，2016年。'
- en: '[167] B. Haefner, Y. Quéau, T. Möllenhoff, and D. Cremers, “Fight ill-posedness
    with ill-posedness: Single-shot variational depth super-resolution from shading,”
    in *CVPR*, 2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] B. Haefner, Y. Quéau, T. Möllenhoff, 和 D. Cremers, “用 ill-posedness 对抗
    ill-posedness：从阴影中单次变分深度超分辨率”，发表于 *CVPR*，2018年。'
- en: '[168] G. Riegler, M. Rüther, and H. Bischof, “Atgv-net: Accurate depth super-resolution,”
    in *ECCV*, 2016.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] G. Riegler, M. Rüther, 和 H. Bischof, “Atgv-net：准确的深度超分辨率”，发表于 *ECCV*，2016年。'
- en: '[169] J.-S. Park and S.-W. Lee, “An example-based face hallucination method
    for single-frame, low-resolution facial images,” *IEEE Transactions on Image Processing*,
    vol. 17, 2008.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J.-S. Park 和 S.-W. Lee, “基于示例的面部幻觉方法用于单帧低分辨率面部图像”，发表于 *IEEE Transactions
    on Image Processing*，第17卷，2008年。'
- en: '[170] S. Zhu, S. Liu, C. C. Loy, and X. Tang, “Deep cascaded bi-network for
    face hallucination,” in *ECCV*, 2016.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] S. Zhu, S. Liu, C. C. Loy, 和 X. Tang, “深度级联双网络用于面部幻觉”，发表于 *ECCV*，2016年。'
- en: '[171] X. Yu, B. Fernando, B. Ghanem, F. Porikli, and R. Hartley, “Face super-resolution
    guided by facial component heatmaps,” in *ECCV*, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] X. Yu, B. Fernando, B. Ghanem, F. Porikli, 和 R. Hartley, “由面部组件热图指导的面部超分辨率”，发表于
    *ECCV*，2018年。'
- en: '[172] X. Yu and F. Porikli, “Face hallucination with tiny unaligned images
    by transformative discriminative neural networks,” in *AAAI*, 2017.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] X. Yu 和 F. Porikli, “通过变换判别神经网络对微小的不对齐图像进行面部幻觉”，发表于 *AAAI*，2017年。'
- en: '[173] M. Jaderberg, K. Simonyan, A. Zisserman *et al.*, “Spatial transformer
    networks,” in *NIPS*, 2015.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] M. Jaderberg, K. Simonyan, A. Zisserman *等*，“空间变换网络”，发表于 *NIPS*，2015年。'
- en: '[174] X. Yu and F. Porikli, “Hallucinating very low-resolution unaligned and
    noisy face images by transformative discriminative autoencoders,” in *CVPR*, 2017.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] X. Yu 和 F. Porikli, “通过变换判别自编码器对非常低分辨率、不对齐且噪声的面部图像进行幻觉”，发表于 *CVPR*，2017年。'
- en: '[175] Y. Song, J. Zhang, S. He, L. Bao, and Q. Yang, “Learning to hallucinate
    face images via component generation and enhancement,” in *IJCAI*, 2017.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Y. Song, J. Zhang, S. He, L. Bao, 和 Q. Yang, “通过组件生成和增强学习面部图像幻觉”，发表于
    *IJCAI*，2017年。'
- en: '[176] C.-Y. Yang, S. Liu, and M.-H. Yang, “Hallucinating compressed face images,”
    *IJCV*, vol. 126, 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] C.-Y. Yang, S. Liu, 和 M.-H. Yang, “幻觉压缩面部图像”，*IJCV*，第126卷，2018年。'
- en: '[177] X. Yu and F. Porikli, “Ultra-resolving face images by discriminative
    generative networks,” in *ECCV*, 2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] X. Yu 和 F. Porikli, “通过判别生成网络超分辨率面部图像”，发表于 *ECCV*，2016年。'
- en: '[178] C.-H. Lee, K. Zhang, H.-C. Lee, C.-W. Cheng, and W. Hsu, “Attribute augmented
    convolutional neural network for face hallucination,” in *CVPRW*, 2018.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] C.-H. Lee, K. Zhang, H.-C. Lee, C.-W. Cheng, 和 W. Hsu, “用于面部幻觉的属性增强卷积神经网络，”
    发表在 *CVPRW*，2018年。'
- en: '[179] X. Yu, B. Fernando, R. Hartley, and F. Porikli, “Super-resolving very
    low-resolution face images with supplementary attributes,” in *CVPR*, 2018.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] X. Yu, B. Fernando, R. Hartley, 和 F. Porikli, “利用补充属性超分辨率非常低分辨率的面部图像，”
    发表在 *CVPR*，2018年。'
- en: '[180] M. Mirza and S. Osindero, “Conditional generative adversarial nets,”
    *Arxiv:1411.1784*, 2014.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] M. Mirza 和 S. Osindero, “条件生成对抗网络，” *Arxiv:1411.1784*，2014年。'
- en: '[181] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, and J. C.
    Tilton, “Advances in spectral-spatial classification of hyperspectral images,”
    *Proceedings of the IEEE*, vol. 101, 2013.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] M. Fauvel, Y. Tarabalka, J. A. Benediktsson, J. Chanussot, 和 J. C. Tilton,
    “高光谱图像的光谱-空间分类进展，” *IEEE汇刊*，第101卷，2013年。'
- en: '[182] Y. Fu, Y. Zheng, I. Sato, and Y. Sato, “Exploiting spectral-spatial correlation
    for coded hyperspectral image restoration,” in *CVPR*, 2016.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Y. Fu, Y. Zheng, I. Sato, 和 Y. Sato, “利用光谱-空间相关性进行编码高光谱图像恢复，” 发表在 *CVPR*，2016年。'
- en: '[183] B. Uzkent, A. Rangnekar, and M. J. Hoffman, “Aerial vehicle tracking
    by adaptive fusion of hyperspectral likelihood maps,” in *CVPRW*, 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] B. Uzkent, A. Rangnekar, 和 M. J. Hoffman, “通过自适应融合高光谱可能性图进行空中载具跟踪，” 发表在
    *CVPRW*，2017年。'
- en: '[184] G. Masi, D. Cozzolino, L. Verdoliva, and G. Scarpa, “Pansharpening by
    convolutional neural networks,” *Remote Sensing*, vol. 8, 2016.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] G. Masi, D. Cozzolino, L. Verdoliva, 和 G. Scarpa, “通过卷积神经网络进行全色融合，” *遥感*，第8卷，2016年。'
- en: '[185] Y. Qu, H. Qi, and C. Kwan, “Unsupervised sparse dirichlet-net for hyperspectral
    image super-resolution,” in *CVPR*, 2018.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Y. Qu, H. Qi, 和 C. Kwan, “无监督稀疏 Dirichlet-Net 用于高光谱图像超分辨率，” 发表在 *CVPR*，2018年。'
- en: '[186] Y. Fu, T. Zhang, Y. Zheng, D. Zhang, and H. Huang, “Hyperspectral image
    super-resolution with optimized rgb guidance,” in *CVPR*, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Y. Fu, T. Zhang, Y. Zheng, D. Zhang, 和 H. Huang, “利用优化的 RGB 指导进行高光谱图像超分辨率，”
    发表在 *CVPR*，2019年。'
- en: '[187] C. Chen, Z. Xiong, X. Tian, Z.-J. Zha, and F. Wu, “Camera lens super-resolution,”
    in *CVPR*, 2019.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] C. Chen, Z. Xiong, X. Tian, Z.-J. Zha, 和 F. Wu, “相机镜头超分辨率，” 发表在 *CVPR*，2019年。'
- en: '[188] X. Zhang, Q. Chen, R. Ng, and V. Koltun, “Zoom to learn, learn to zoom,”
    in *CVPR*, 2019.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] X. Zhang, Q. Chen, R. Ng, 和 V. Koltun, “缩放以学习，学习以缩放，” 发表在 *CVPR*，2019年。'
- en: '[189] X. Xu, Y. Ma, and W. Sun, “Towards real scene super-resolution with raw
    images,” in *CVPR*, 2019.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] X. Xu, Y. Ma, 和 W. Sun, “朝向真实场景超分辨率与原始图像，” 发表在 *CVPR*，2019年。'
- en: '[190] R. Liao, X. Tao, R. Li, Z. Ma, and J. Jia, “Video super-resolution via
    deep draft-ensemble learning,” in *ICCV*, 2015.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] R. Liao, X. Tao, R. Li, Z. Ma, 和 J. Jia, “通过深度草图集成学习的视频超分辨率，” 发表在 *ICCV*，2015年。'
- en: '[191] A. Kappeler, S. Yoo, Q. Dai, and A. K. Katsaggelos, “Video super-resolution
    with convolutional neural networks,” *IEEE Transactions on Computational Imaging*,
    vol. 2, 2016.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] A. Kappeler, S. Yoo, Q. Dai, 和 A. K. Katsaggelos, “使用卷积神经网络的视频超分辨率，”
    *IEEE计算成像汇刊*，第2卷，2016年。'
- en: '[192] ——, “Super-resolution of compressed videos using convolutional neural
    networks,” in *ICIP*, 2016.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] ——, “使用卷积神经网络对压缩视频进行超分辨率，” 发表在 *ICIP*，2016年。'
- en: '[193] M. Drulea and S. Nedevschi, “Total variation regularization of local-global
    optical flow,” in *ITSC*, 2011.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] M. Drulea 和 S. Nedevschi, “局部-全局光流的全变差正则化，” 发表在 *ITSC*，2011年。'
- en: '[194] D. Liu, Z. Wang, Y. Fan, X. Liu, Z. Wang, S. Chang, and T. Huang, “Robust
    video super-resolution with learned temporal dynamics,” in *ICCV*, 2017.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] D. Liu, Z. Wang, Y. Fan, X. Liu, Z. Wang, S. Chang, 和 T. Huang, “利用学习的时间动态进行鲁棒的视频超分辨率，”
    发表在 *ICCV*，2017年。'
- en: '[195] D. Liu, Z. Wang, Y. Fan, X. Liu, Z. Wang, S. Chang, X. Wang, and T. S.
    Huang, “Learning temporal dynamics for video super-resolution: A deep learning
    approach,” *IEEE Transactions on Image Processing*, vol. 27, 2018.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] D. Liu, Z. Wang, Y. Fan, X. Liu, Z. Wang, S. Chang, X. Wang, 和 T. S.
    Huang, “视频超分辨率的时间动态学习：一种深度学习方法，” *IEEE图像处理汇刊*，第27卷，2018年。'
- en: '[196] X. Tao, H. Gao, R. Liao, J. Wang, and J. Jia, “Detail-revealing deep
    video super-resolution,” in *ICCV*, 2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] X. Tao, H. Gao, R. Liao, J. Wang, 和 J. Jia, “细节揭示深度视频超分辨率，” 发表在 *ICCV*，2017年。'
- en: '[197] Y. Huang, W. Wang, and L. Wang, “Bidirectional recurrent convolutional
    networks for multi-frame super-resolution,” in *NIPS*, 2015.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Y. Huang, W. Wang, 和 L. Wang, “用于多帧超分辨率的双向递归卷积网络，” 发表在 *NIPS*，2015年。'
- en: '[198] ——, “Video super-resolution via bidirectional recurrent convolutional
    networks,” *TPAMI*, vol. 40, 2018.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] ——, “通过双向递归卷积网络的视频超分辨率，” *TPAMI*，第40卷，2018年。'
- en: '[199] J. Guo and H. Chao, “Building an end-to-end spatial-temporal convolutional
    network for video super-resolution,” in *AAAI*, 2017.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Guo 和 H. Chao, “构建端到端时空卷积网络用于视频超分辨率，”在*AAAI*，2017年。'
- en: '[200] A. Graves, S. Fernández, and J. Schmidhuber, “Bidirectional lstm networks
    for improved phoneme classification and recognition,” in *ICANN*, 2005.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] A. Graves, S. Fernández, 和 J. Schmidhuber, “双向LSTM网络用于改进音素分类和识别，”在*ICANN*，2005年。'
- en: '[201] M. S. Sajjadi, R. Vemulapalli, and M. Brown, “Frame-recurrent video super-resolution,”
    in *CVPR*, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] M. S. Sajjadi, R. Vemulapalli, 和 M. Brown, “帧递归视频超分辨率，”在*CVPR*，2018年。'
- en: '[202] S. Li, F. He, B. Du, L. Zhang, Y. Xu, and D. Tao, “Fast spatio-temporal
    residual network for video super-resolution,” in *CVPR*, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] S. Li, F. He, B. Du, L. Zhang, Y. Xu, 和 D. Tao, “用于视频超分辨率的快速时空残差网络，”在*CVPR*，2019年。'
- en: '[203] Z. Zhang and V. Sze, “Fast: A framework to accelerate super-resolution
    processing on compressed videos,” in *CVPRW*, 2017.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Z. Zhang 和 V. Sze, “FAST: 一个加速压缩视频超分辨率处理的框架，”在*CVPRW*，2017年。'
- en: '[204] Y. Jo, S. W. Oh, J. Kang, and S. J. Kim, “Deep video super-resolution
    network using dynamic upsampling filters without explicit motion compensation,”
    in *CVPR*, 2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Y. Jo, S. W. Oh, J. Kang, 和 S. J. Kim, “使用动态上采样滤波器的深度视频超分辨率网络，无需显式运动补偿，”在*CVPR*，2018年。'
- en: '[205] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, and S. Yan, “Perceptual generative
    adversarial networks for small object detection,” in *CVPR*, 2017.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, 和 S. Yan, “用于小物体检测的感知生成对抗网络，”在*CVPR*，2017年。'
- en: '[206] W. Tan, B. Yan, and B. Bare, “Feature super-resolution: Make machine
    see more clearly,” in *CVPR*, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] W. Tan, B. Yan, 和 B. Bare, “特征超分辨率：让机器看得更清晰，”在*CVPR*，2018年。'
- en: '[207] D. S. Jeon, S.-H. Baek, I. Choi, and M. H. Kim, “Enhancing the spatial
    resolution of stereo images using a parallax prior,” in *CVPR*, 2018.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] D. S. Jeon, S.-H. Baek, I. Choi, 和 M. H. Kim, “利用视差先验增强立体图像的空间分辨率，”在*CVPR*，2018年。'
- en: '[208] L. Wang, Y. Wang, Z. Liang, Z. Lin, J. Yang, W. An, and Y. Guo, “Learning
    parallax attention for stereo image super-resolution,” in *CVPR*, 2019.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] L. Wang, Y. Wang, Z. Liang, Z. Lin, J. Yang, W. An, 和 Y. Guo, “为立体图像超分辨率学习视差注意力，”在*CVPR*，2019年。'
- en: '[209] Y. Li, V. Tsiminaki, R. Timofte, M. Pollefeys, and L. V. Gool, “3d appearance
    super-resolution with deep learning,” in *CVPR*, 2019.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Y. Li, V. Tsiminaki, R. Timofte, M. Pollefeys, 和 L. V. Gool, “基于深度学习的3D外观超分辨率，”在*CVPR*，2019年。'
- en: '[210] S. Zhang, Y. Lin, and H. Sheng, “Residual networks for light field image
    super-resolution,” in *CVPR*, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] S. Zhang, Y. Lin, 和 H. Sheng, “用于光场图像超分辨率的残差网络，”在*CVPR*，2019年。'
- en: '[211] C. Ancuti, C. O. Ancuti, R. Timofte, L. Van Gool, L. Zhang, M.-H. Yang,
    V. M. Patel, H. Zhang, V. A. Sindagi, R. Zhao *et al.*, “Ntire 2018 challenge
    on image dehazing: Methods and results,” in *CVPRW*, 2018.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] C. Ancuti, C. O. Ancuti, R. Timofte, L. Van Gool, L. Zhang, M.-H. Yang,
    V. M. Patel, H. Zhang, V. A. Sindagi, R. Zhao *等*，“NTIRE 2018挑战赛图像去雾：方法与结果，”在*CVPRW*，2018年。'
- en: '[212] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, and J. Dean, “Efficient neural
    architecture search via parameter sharing,” in *ICML*, 2018.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] H. Pham, M. Y. Guan, B. Zoph, Q. V. Le, 和 J. Dean, “通过参数共享实现高效神经网络架构搜索，”在*ICML*，2018年。'
- en: '[213] H. Liu, K. Simonyan, and Y. Yang, “Darts: Differentiable architecture
    search,” *ICLR*, 2019.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] H. Liu, K. Simonyan, 和 Y. Yang, “DARTS: 可微分架构搜索，”在*ICLR*，2019年。'
- en: '[214] Y. Guo, Y. Zheng, M. Tan, Q. Chen, J. Chen, P. Zhao, and J. Huang, “Nat:
    Neural architecture transformer for accurate and compact architectures,” in *NIPS*,
    2019, pp. 735–747.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Y. Guo, Y. Zheng, M. Tan, Q. Chen, J. Chen, P. Zhao, 和 J. Huang, “Nat:
    神经架构变换器，用于精确和紧凑的架构，”在*NIPS*，2019年，第735–747页。'
- en: '| ![[Uncaptioned image]](img/bdad4eb4519aaefc4c3213cd38589db1.png) | Zhihao
    Wang received the BE degree in South China University of Technology (SCUT), China,
    in 2017, and is working toward the ME degree at the School of Software Engineering,
    SCUT. Now he is as a visiting student at the School of Information Systems, Singapore
    Management University, Singapore. His research interests are computer vision based
    on deep learning, including visual recognition and image super-resolution. |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/bdad4eb4519aaefc4c3213cd38589db1.png) | 王志豪于2017年获得华南理工大学（SCUT）工学学士学位，目前在华南理工大学软件工程学院攻读工学硕士学位。他现在是新加坡管理大学信息系统学院的访问学生。他的研究兴趣包括基于深度学习的计算机视觉，涵盖视觉识别和图像超分辨率。
    |'
- en: '| ![[Uncaptioned image]](img/6e24afde29699d2fcc944549bcb67c59.png) | Jian Chen
    is currently a Professor of the School of Software Engineering at South China
    University of Technology where she started as an Assistant Professor in 2005.
    She received her B.S. and Ph.D. degrees, both in Computer Science, from Sun Yat-Sen
    University, China, in 2000 and 2005 respectively. Her research interests can be
    summarized as developing effective and efficient data analysis techniques for
    complex data and the related applications. |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/6e24afde29699d2fcc944549bcb67c59.png) | Jian Chen 目前是华南理工大学软件工程学院的教授，自2005年起担任助理教授。她于2000年和2005年分别在中山大学获得计算机科学学士和博士学位。她的研究兴趣可以总结为开发有效且高效的数据分析技术，用于复杂数据及相关应用。
    |'
- en: '| ![[Uncaptioned image]](img/8909731afbb5cf2e8e8fca08a56a8181.png) | Steven
    C. H. Hoi is currently the Managing Director of Salesforce Research Asia, and
    an Associate Professor (on leave) of the School of Information Systems, Singapore
    Management University, Singapore. Prior to joining SMU, he was an Associate Professor
    with Nanyang Technological University, Singapore. He received his Bachelor degree
    from Tsinghua University, P.R. China, in 2002, and his Ph.D degree in computer
    science and engineering from The Chinese University of Hong Kong, in 2006. His
    research interests are machine learning and data mining and their applications
    to multimedia information retrieval (image and video retrieval), social media
    and web mining, and computational finance, etc., and he has published over 150
    refereed papers in top conferences and journals in these related areas. He has
    served as the Editor-in-Chief for Neurocomputing Journal, general co-chair for
    ACM SIGMM Workshops on Social Media (WSM’09, WSM’10, WSM’11), program co-chair
    for the fourth Asian Conference on Machine Learning (ACML’12), book editor for
    “Social Media Modeling and Computing”, guest editor for ACM Transactions on Intelligent
    Systems and Technology (ACM TIST), technical PC member for many international
    conferences, and external reviewer for many top journals and worldwide funding
    agencies, including NSF in US and RGC in Hong Kong. He is an IEEE Fellow and ACM
    Distinguished Member. |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/8909731afbb5cf2e8e8fca08a56a8181.png) | Steven C. H. Hoi 目前是
    Salesforce Research Asia 的总裁，同时也是新加坡管理大学信息系统学院的副教授（休假中）。在加入SMU之前，他曾是南洋理工大学的副教授。他于2002年在中国清华大学获得学士学位，2006年在香港中文大学获得计算机科学与工程博士学位。他的研究兴趣包括机器学习和数据挖掘及其在多媒体信息检索（图像和视频检索）、社交媒体和网页挖掘、计算金融等领域的应用，并在这些相关领域的顶级会议和期刊上发表了150多篇经过审稿的论文。他曾担任《Neurocomputing
    Journal》的主编，ACM SIGMM社交媒体研讨会（WSM’09, WSM’10, WSM’11）的总程序主席，第四届亚洲机器学习会议（ACML’12）的程序共同主席，“社交媒体建模与计算”一书的编辑，《ACM
    Transactions on Intelligent Systems and Technology》（ACM TIST）的特刊编辑，多个国际会议的技术程序委员会成员，以及多个顶级期刊和全球资助机构（包括美国NSF和香港RGC）的外部评审专家。他是IEEE
    Fellow和ACM杰出会员。 |'
- en: 'TABLE I: Notations.'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 'TABLE I: 符号说明。'
- en: '| Notation | Description |'
  id: totrans-591
  prefs: []
  type: TYPE_TB
  zh: '| Notation | Description |'
- en: '| --- | --- |'
  id: totrans-592
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $I_{x}$ | LR image |'
  id: totrans-593
  prefs: []
  type: TYPE_TB
  zh: '| $I_{x}$ | 低分辨率图像 |'
- en: '| $I_{y}$ | ground truth HR image, abbreviated as $I$ |'
  id: totrans-594
  prefs: []
  type: TYPE_TB
  zh: '| $I_{y}$ | 真实高分辨率图像，简称为$I$ |'
- en: '| $\hat{I_{y}}$ | reconstructed HR image, abbreviated as $\hat{I}$ |'
  id: totrans-595
  prefs: []
  type: TYPE_TB
  zh: '| $\hat{I_{y}}$ | 重建的高分辨率图像，简称为$\hat{I}$ |'
- en: '| $I_{s}$ | randomly sampled HR image from the real HR images |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '| $I_{s}$ | 从真实高分辨率图像中随机采样的高分辨率图像 |'
- en: '| $I(i)$ | intensity of the $i$-th pixel of image $I$ |'
  id: totrans-597
  prefs: []
  type: TYPE_TB
  zh: '| $I(i)$ | 图像$I$中第$i$个像素的强度 |'
- en: '| $D$ | discriminator network of GAN |'
  id: totrans-598
  prefs: []
  type: TYPE_TB
  zh: '| $D$ | GAN的判别网络 |'
- en: '| $\phi$ | image classification network |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '| $\phi$ | 图像分类网络 |'
- en: '| $\phi^{(l)}$ | extracted representations on $l$-th layer by $\phi$ |'
  id: totrans-600
  prefs: []
  type: TYPE_TB
  zh: '| $\phi^{(l)}$ | $\phi$在$l$-th层提取的表示 |'
- en: '| $\operatorname{vec}$ | vectorization operation |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| $\operatorname{vec}$ | 向量化操作 |'
- en: '| $G^{(l)}$ | Gram matrix of representations on $l$-th layer |'
  id: totrans-602
  prefs: []
  type: TYPE_TB
  zh: '| $G^{(l)}$ | $l$-th层表示的Gram矩阵 |'
- en: '| $l$ | layer of CNN |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| $l$ | CNN的层 |'
- en: '| $h$, $w$, $c$ | width, height and number of channels of feature maps |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| $h$, $w$, $c$ | 特征图的宽度、高度和通道数 |'
- en: '| $h_{l}$, $w_{l}$, $c_{l}$ | width, height and number of channels of feature
    maps in $l$-th layer |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| $h_{l}$, $w_{l}$, $c_{l}$ | $l$-th层特征图的宽度、高度和通道数 |'
- en: '| $\mathcal{D}$ | degradation process |'
  id: totrans-606
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D}$ | 降质过程 |'
- en: '| $\delta$ | parameters of $\mathcal{D}$ |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| $\delta$ | $\mathcal{D}$的参数 |'
- en: '| $\mathcal{F}$ | super-resolution process |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{F}$ | 超分辨率过程 |'
- en: '| $\theta$ | parameters of $\mathcal{F}$ |'
  id: totrans-609
  prefs: []
  type: TYPE_TB
  zh: '| $\theta$ | $\mathcal{F}$的参数 |'
- en: '| $\otimes$ | convolution operation |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| $\otimes$ | 卷积操作 |'
- en: '| $\kappa$ | convolution kernel |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| $\kappa$ | 卷积核 |'
- en: '| $\downarrow$ | downsampling operation |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| $\downarrow$ | 下采样操作 |'
- en: '| $s$ | scaling factor |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| $s$ | 缩放因子 |'
- en: '| $n$ | Gaussian noise |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| $n$ | 高斯噪声 |'
- en: '| $\varsigma$ | standard deviation of $n$ |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| $\varsigma$ | $n$的标准差 |'
- en: '| $z$ | a random vector |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| $z$ | 随机向量 |'
- en: '| $\mathcal{L}$ | loss function |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}$ | 损失函数 |'
- en: '| $\mathcal{L}_{\text{content}}$ | content loss |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{content}}$ | 内容损失 |'
- en: '| $\mathcal{L}_{\text{cycle}}$ | content consistency loss |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{cycle}}$ | 内容一致性损失 |'
- en: '| $\mathcal{L}_{\text{pixel\_l1}}$ | pixel L1 loss |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{pixel\_l1}}$ | 像素L1损失 |'
- en: '| $\mathcal{L}_{\text{pixel\_l2}}$ | pixel L2 loss |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{pixel\_l2}}$ | 像素L2损失 |'
- en: '| $\mathcal{L}_{\text{pixel\_Cha}}$ | pixel Charbonnier loss |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{pixel\_Cha}}$ | 像素Charbonnier损失 |'
- en: '| $\mathcal{L}_{\text{gan\_ce\_g}}$, $\mathcal{L}_{\text{gan\_ce\_d}}$ | adversarial
    loss of the generator and discriminator based on cross entropy |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{gan\_ce\_g}}$, $\mathcal{L}_{\text{gan\_ce\_d}}$ | 基于交叉熵的生成器和判别器的对抗损失
    |'
- en: '| $\mathcal{L}_{\text{gan\_hi\_g}}$, $\mathcal{L}_{\text{gan\_hi\_d}}$ | adversarial
    loss of the generator and discriminator based on hinge error |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{gan\_hi\_g}}$, $\mathcal{L}_{\text{gan\_hi\_d}}$ | 基于铰链误差的生成器和判别器的对抗损失
    |'
- en: '| $\mathcal{L}_{\text{gan\_ls\_g}}$, $\mathcal{L}_{\text{gan\_ls\_g}}$ | adversarial
    loss of the generator and discriminator based on least square error |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{gan\_ls\_g}}$, $\mathcal{L}_{\text{gan\_ls\_d}}$ | 基于最小二乘误差的生成器和判别器的对抗损失
    |'
- en: '| $\mathcal{L}_{\text{TV}}$ | total variation loss |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{\text{TV}}$ | 总变差损失 |'
- en: '| $\Phi$ | regularization term |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| $\Phi$ | 正则化项 |'
- en: '| $\lambda$ | tradeoff parameter of $\Phi$ |'
  id: totrans-628
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda$ | $\Phi$的权衡参数 |'
- en: '| $\epsilon$ | small instant for stability |'
  id: totrans-629
  prefs: []
  type: TYPE_TB
  zh: '| $\epsilon$ | 稳定性的微小常数 |'
- en: '| $\mu_{I}$ | luminance of image $I$, i.e., mean of intensity |'
  id: totrans-630
  prefs: []
  type: TYPE_TB
  zh: '| $\mu_{I}$ | 图像$I$的亮度，即强度的均值 |'
- en: '| $\sigma_{I}$ | contrast of image $I$, i.e., standard deviation of intensity
    |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| $\sigma_{I}$ | 图像$I$的对比度，即强度的标准差 |'
- en: '| $\sigma_{I,\hat{I}}$ | covariance between images $I$ and $\hat{I}$ |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '| $\sigma_{I,\hat{I}}$ | 图像$I$和$\hat{I}$之间的协方差 |'
- en: '| $\mathcal{C}_{l}$, $\mathcal{C}_{c}$, $\mathcal{C}_{s}$ | comparison function
    of luminance, contrast, structure |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{C}_{l}$, $\mathcal{C}_{c}$, $\mathcal{C}_{s}$ | 亮度、对比度、结构的比较函数
    |'
- en: '| $\alpha$, $\beta$, $\gamma$ | weights of $\mathcal{C}_{l}$, $\mathcal{C}_{c}$,
    $\mathcal{C}_{s}$ |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$, $\beta$, $\gamma$ | $\mathcal{C}_{l}$, $\mathcal{C}_{c}$, $\mathcal{C}_{s}$的权重
    |'
- en: '| $C_{1}$, $C_{2}$, $C_{3}$ | constants |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| $C_{1}$, $C_{2}$, $C_{3}$ | 常数 |'
- en: '| $k_{1}$, $k_{2}$ | constants |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| $k_{1}$, $k_{2}$ | 常数 |'
- en: '| $L$ | maximum possible pixel value |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| $L$ | 最大可能的像素值 |'
- en: '| $N$ | number of pixels |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| $N$ | 像素数量 |'
- en: '| $M$ | number of bins |'
  id: totrans-639
  prefs: []
  type: TYPE_TB
  zh: '| $M$ | 直方图的 bin 数量 |'
- en: 'TABLE II: Abbreviations.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：缩略语。
- en: '| Abbreviation | Full name | Abbreviation | Full name |'
  id: totrans-641
  prefs: []
  type: TYPE_TB
  zh: '| 缩略语 | 全称 | 缩略语 | 全称 |'
- en: '| FH | face hallucination | PAN | panchromatic image |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| FH | 人脸幻觉 | PAN | 全色图像 |'
- en: '| HR | high-resolution | SR | super-resolution |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| HR | 高分辨率 | SR | 超分辨率 |'
- en: '| HSI | hyperspectral image | TV | total variation |'
  id: totrans-644
  prefs: []
  type: TYPE_TB
  zh: '| HSI | 高光谱图像 | TV | 总变差 |'
- en: '| HVS | human visual system | WT | wavelet transformation |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| HVS | 人类视觉系统 | WT | 小波变换 |'
- en: '| LR | low-resolution |  |  |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| LR | 低分辨率 |  |  |'
- en: '| FSIM [[75](#bib.bib75)] | feature similarity | MS-SSIM [[74](#bib.bib74)]
    | multi-scale SSIM |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| FSIM [[75](#bib.bib75)] | 特征相似性 | MS-SSIM [[74](#bib.bib74)] | 多尺度SSIM |'
- en: '| IQA | image quality assessment | NIQE [[76](#bib.bib76)] | natural image
    quality evaluator |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| IQA | 图像质量评估 | NIQE [[76](#bib.bib76)] | 自然图像质量评估器 |'
- en: '| MOS | mean opinion score | PSNR | peak signal-to-noise ratio |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| MOS | 平均意见分数 | PSNR | 峰值信噪比 |'
- en: '| MSSIM [[58](#bib.bib58)] | mean SSIM | SSIM [[58](#bib.bib58)] | structural
    similarity |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '| MSSIM [[58](#bib.bib58)] | 平均SSIM | SSIM [[58](#bib.bib58)] | 结构相似性 |'
- en: '| BN [[145](#bib.bib145)] | batch normalization | GAN [[24](#bib.bib24)] |
    generative adversarial net |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '| BN [[145](#bib.bib145)] | 批量归一化 | GAN [[24](#bib.bib24)] | 生成对抗网络 |'
- en: '| CNN | convolutional neural network | LSTM | long short term memory network
    |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '| CNN | 卷积神经网络 | LSTM | 长短期记忆网络 |'
- en: '| CycleGAN [[138](#bib.bib138)] | cycle-in-cycle GAN | ResNet [[96](#bib.bib96)]
    | residual network |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '| CycleGAN [[138](#bib.bib138)] | 循环生成对抗网络 | ResNet [[96](#bib.bib96)] | 残差网络
    |'
- en: '| DenseNet [[102](#bib.bib102)] | densely connected CNN | SENet [[104](#bib.bib104)]
    | squeeze-and-excitation network |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet [[102](#bib.bib102)] | 密集连接的卷积神经网络 | SENet [[104](#bib.bib104)]
    | 压缩和激励网络 |'
- en: '| FAN | face alignment network | SPMC [[196](#bib.bib196)] | sub-pixel motion
    compensation |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| FAN | 人脸对齐网络 | SPMC [[196](#bib.bib196)] | 亚像素运动补偿 |'
- en: '| ADRSR [[149](#bib.bib149)] | automated decomposition and reconstruction |
    LCGE [[175](#bib.bib175)] | learn FH via component generation and enhancement
    |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '| ADRSR [[149](#bib.bib149)] | 自动化分解与重建 | LCGE [[175](#bib.bib175)] | 通过组件生成与增强学习FH
    |'
- en: '| Attention-FH [[113](#bib.bib113)] | attention-aware FH | MemNet [[55](#bib.bib55)]
    | memory network |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| Attention-FH [[113](#bib.bib113)] | 注意力感知FH | MemNet [[55](#bib.bib55)] |
    记忆网络 |'
- en: '| BRCN [[197](#bib.bib197), [198](#bib.bib198)] | bidirectional recurrent CNN
    | MS-LapSRN [[65](#bib.bib65)] | multi-scale LapSRN |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| BRCN [[197](#bib.bib197), [198](#bib.bib198)] | 双向递归CNN | MS-LapSRN [[65](#bib.bib65)]
    | 多尺度LapSRN |'
- en: '| CARN [[28](#bib.bib28)] | cascading residual network | MSRN [[99](#bib.bib99)]
    | multiscale residual network |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '| CARN [[28](#bib.bib28)] | 级联残差网络 | MSRN [[99](#bib.bib99)] | 多尺度残差网络 |'
- en: '| CARN-M [[28](#bib.bib28)] | CARM based on MobileNet | MTUN [[171](#bib.bib171)]
    | multi-task upsampling network |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '| CARN-M [[28](#bib.bib28)] | 基于MobileNet的CARM | MTUN [[171](#bib.bib171)]
    | 多任务上采样网络 |'
- en: '| CBN [[170](#bib.bib170)] | cascaded bi-network | MWCNN [[122](#bib.bib122)]
    | multi-level wavelet CNN |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '| CBN [[170](#bib.bib170)] | 级联双网络 | MWCNN [[122](#bib.bib122)] | 多级小波CNN |'
- en: '| CinCGAN [[131](#bib.bib131)] | cycle-in-cycle GAN | ProSR [[32](#bib.bib32)]
    | progressive SR |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '| CinCGAN [[131](#bib.bib131)] | 循环中循环GAN | ProSR [[32](#bib.bib32)] | 渐进式超分辨率
    |'
- en: '| CNF [[100](#bib.bib100)] | context-wise network fusion | RBPN [[87](#bib.bib87)]
    | recurrent back-projection network |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '| CNF [[100](#bib.bib100)] | 上下文网络融合 | RBPN [[87](#bib.bib87)] | 循环反投影网络 |'
- en: '| CVSRnet [[192](#bib.bib192)] | compressed VSRnet | RCAN [[70](#bib.bib70)]
    | residual channel attention networks |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| CVSRnet [[192](#bib.bib192)] | 压缩VSRnet | RCAN [[70](#bib.bib70)] | 残差通道注意力网络
    |'
- en: '| DBPN [[57](#bib.bib57)] | deep back-projection network | RDN [[93](#bib.bib93)]
    | residual dense network |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '| DBPN [[57](#bib.bib57)] | 深度反投影网络 | RDN [[93](#bib.bib93)] | 残差密集网络 |'
- en: '| DNSR [[149](#bib.bib149)] | denoising for SR | RNAN [[106](#bib.bib106)]
    | residual non-local attention networks |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| DNSR [[149](#bib.bib149)] | 超分辨率去噪 | RNAN [[106](#bib.bib106)] | 残差非局部注意力网络
    |'
- en: '| DRCN [[82](#bib.bib82)] | deeply-recursive CNN | SAN [[105](#bib.bib105)]
    | Second-order Attention Network |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '| DRCN [[82](#bib.bib82)] | 深度递归CNN | SAN [[105](#bib.bib105)] | 二阶注意力网络 |'
- en: '| DRRN [[56](#bib.bib56)] | deep recursive residual network | SFT-GAN [[46](#bib.bib46)]
    | GAN with spatial feature transformation |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| DRRN [[56](#bib.bib56)] | 深度递归残差网络 | SFT-GAN [[46](#bib.bib46)] | 具有空间特征变换的GAN
    |'
- en: '| DSRN [[85](#bib.bib85)] | dual-state recurrent network | SICNN [[72](#bib.bib72)]
    | super-identity CNN |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| DSRN [[85](#bib.bib85)] | 双状态递归网络 | SICNN [[72](#bib.bib72)] | 超级身份CNN |'
- en: '| DWSR [[120](#bib.bib120)] | deep wavelet prediction for SR | SOCA [[105](#bib.bib105)]
    | second-order channel attention |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '| DWSR [[120](#bib.bib120)] | 深度小波预测超分辨率 | SOCA [[105](#bib.bib105)] | 二阶通道注意力
    |'
- en: '| EDSR [[31](#bib.bib31)] | enhanced deep SR network | SRCNN [[22](#bib.bib22),
    [23](#bib.bib23)] | SR CNN |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '| EDSR [[31](#bib.bib31)] | 增强型深度超分辨率网络 | SRCNN [[22](#bib.bib22), [23](#bib.bib23)]
    | 超分辨率CNN |'
- en: '| EDSR-PP [[116](#bib.bib116)] | EDSR with pyramid pooling | SRFBN [[86](#bib.bib86)]
    | SR feedback network |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '| EDSR-PP [[116](#bib.bib116)] | 带有金字塔池化的EDSR | SRFBN [[86](#bib.bib86)] |
    超分辨率反馈网络 |'
- en: '| ESPCN [[156](#bib.bib156)] | efficient sub-pixel CNN | SRGAN [[25](#bib.bib25)]
    | SR GAN |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '| ESPCN [[156](#bib.bib156)] | 高效亚像素CNN | SRGAN [[25](#bib.bib25)] | 超分辨率GAN
    |'
- en: '| ESRGAN [[103](#bib.bib103)] | enhanced SRGAN | SRDenseNet [[79](#bib.bib79)]
    | SR DenseNet |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '| ESRGAN [[103](#bib.bib103)] | 增强型SRGAN | SRDenseNet [[79](#bib.bib79)] |
    超分辨率DenseNet |'
- en: '| FAST [[203](#bib.bib203)] | free adaptive SR via transfer | STCN [[199](#bib.bib199)]
    | spatial-temporal CNN |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '| FAST [[203](#bib.bib203)] | 通过迁移实现自由适应的超分辨率 | STCN [[199](#bib.bib199)] |
    时空CNN |'
- en: '| FRVSR [[201](#bib.bib201)] | frame-recurrent video SR | TDAE [[174](#bib.bib174)]
    | transformative discriminative auto-encoder |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
  zh: '| FRVSR [[201](#bib.bib201)] | 帧递归视频超分辨率 | TDAE [[174](#bib.bib174)] | 转换性判别自编码器
    |'
- en: '| FSRCNN [[43](#bib.bib43)] | fast SRCNN | TDN [[172](#bib.bib172)] | transformative
    discriminative network |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| FSRCNN [[43](#bib.bib43)] | 快速SRCNN | TDN [[172](#bib.bib172)] | 转换性判别网络
    |'
- en: '| FSR-GAN [[206](#bib.bib206)] | feature SRGAN | Super-FAN [[30](#bib.bib30)]
    | SR with FAN |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| FSR-GAN [[206](#bib.bib206)] | 特征SRGAN | Super-FAN [[30](#bib.bib30)] | FAN的超分辨率
    |'
- en: '| FSRNet [[73](#bib.bib73)] | face SR network | UR-DGN [[177](#bib.bib177)]
    | ultra-resolving by discriminative generative networks |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '| FSRNet [[73](#bib.bib73)] | 人脸超分辨率网络 | UR-DGN [[177](#bib.bib177)] | 通过判别生成网络进行超分辨率
    |'
- en: '| FSTRN [[202](#bib.bib202)] | fast spatio-temporal ResNet | VESPCN [[156](#bib.bib156)]
    | video ESPCN |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| FSTRN [[202](#bib.bib202)] | 快速时空ResNet | VESPCN [[156](#bib.bib156)] | 视频ESPCN
    |'
- en: '| IDN [[98](#bib.bib98)] | information distillation network | VSRnet [[191](#bib.bib191)]
    | video SR network |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| IDN [[98](#bib.bib98)] | 信息蒸馏网络 | VSRnet [[191](#bib.bib191)] | 视频超分辨率网络
    |'
- en: '| LapSRN [[27](#bib.bib27), [65](#bib.bib65)] | Laplacian pyramid SR network
    | ZSSR [[83](#bib.bib83)] | zero-shot SR |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '| LapSRN [[27](#bib.bib27), [65](#bib.bib65)] | 拉普拉斯金字塔 SR 网络 | ZSSR [[83](#bib.bib83)]
    | 零样本 SR |'
- en: '| MDSR [[31](#bib.bib31)] | multi-scale deep SR system |  |  |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| MDSR [[31](#bib.bib31)] | 多尺度深度 SR 系统 |  |  |'
