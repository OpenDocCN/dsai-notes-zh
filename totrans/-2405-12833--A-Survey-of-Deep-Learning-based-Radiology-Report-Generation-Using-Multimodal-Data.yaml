- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:32:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:32:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2405.12833] A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2405.12833] 基于深度学习的放射学报告生成的调查：多模态数据的应用'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.12833](https://ar5iv.labs.arxiv.org/html/2405.12833)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.12833](https://ar5iv.labs.arxiv.org/html/2405.12833)
- en: A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的放射学报告生成的调查：多模态数据的应用
- en: Xinyi Wang, Grazziela Figueredo, Ruizhe Li
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xinyi Wang, Grazziela Figueredo, Ruizhe Li
- en: The University of Nottingham
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 诺丁汉大学
- en: United Kingdom
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 英国
- en: \AndWei Emma Zhang, Weitong Chen
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \AndWei Emma Zhang, Weitong Chen
- en: The University of Adelaide
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 阿德莱德大学
- en: Australia
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 澳大利亚
- en: \AndXin Chen
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \AndXin Chen
- en: The University of Nottingham
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 诺丁汉大学
- en: 'United Kingdom Corresponding author: Xin Chen, xin.chen@nottingham.ac.uk'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 英国 通讯作者：Xin Chen, xin.chen@nottingham.ac.uk
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Automatic radiology report generation can alleviate the workload for physicians
    and minimize regional disparities in medical resources, therefore becoming an
    important topic in the medical image analysis field. It is a challenging task,
    as the computational model needs to mimic physicians to obtain information from
    multi-modal input data (i.e., medical images, clinical information, medical knowledge,
    etc.), and produce comprehensive and accurate reports. Recently, numerous works
    emerged to address this issue using deep learning-based methods, such as transformers,
    contrastive learning, and knowledge-base construction. This survey summarizes
    the key techniques developed in the most recent works and proposes a general workflow
    for deep learning-based report generation with five main components, including
    multi-modality data acquisition, data preparation, feature learning, feature fusion/interaction,
    and report generation. The state-of-the-art methods for each of these components
    are highlighted. Additionally, training strategies, public datasets, evaluation
    methods, current challenges, and future directions in this field are summarized.
    We have also conducted a quantitative comparison between different methods under
    the same experimental setting. This is the most up-to-date survey that focuses
    on multi-modality inputs and data fusion for radiology report generation. The
    aim is to provide comprehensive and rich information for researchers interested
    in automatic clinical report generation and medical image analysis, especially
    when using multimodal inputs, and assist them in developing new algorithms to
    advance the field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自动放射学报告生成可以减轻医生的工作负担，减少医疗资源的区域差异，因此成为医学图像分析领域的重要话题。这是一项具有挑战性的任务，因为计算模型需要模拟医生从多模态输入数据（即医学图像、临床信息、医学知识等）中获取信息，并生成全面准确的报告。最近，许多研究致力于使用基于深度学习的方法解决这一问题，如变换器、对比学习和知识库构建。本调查总结了最新研究中开发的关键技术，并提出了一个通用的深度学习报告生成工作流程，包括五个主要组件：多模态数据获取、数据准备、特征学习、特征融合/交互和报告生成。突出了每个组件的最先进方法。此外，总结了训练策略、公共数据集、评估方法、当前挑战和未来方向。我们还在相同实验设置下对不同方法进行了定量比较。这是最为最新的调查，专注于多模态输入和数据融合在放射学报告生成中的应用。旨在为对自动临床报告生成和医学图像分析感兴趣的研究人员提供全面丰富的信息，特别是在使用多模态输入时，并协助他们开发新算法以推动该领域的发展。
- en: '*K*eywords Report generation $\cdot$ Deep learning $\cdot$ Multimodal $\cdot$
    Medical image analysis'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 报告生成 $\cdot$ 深度学习 $\cdot$ 多模态 $\cdot$ 医学图像分析'
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Medical images can offer detailed insights into bodies and help physicians screen,
    diagnose and monitor medical conditions without requiring invasive techniques
    (Beddiar et al., [2023](#bib.bib7); Liao et al., [2023](#bib.bib64)). Radiologists
    summarize the information extracted from medical imaging into radiological reports
    for clinical decision-making. The manual generation of reports is however labour-intensive,
    time-consuming, and requires extensive expertise (Beddiar et al., [2023](#bib.bib7)).
    Topol ([2019](#bib.bib115)) points out that the demand for medical image explanation
    greatly surpasses the current capacity of physicians in the United States. During
    an epidemic and with ageing populations, the situation can get worse. During the
    Covid-19 pandemic, for instance, in the UK, each radiologist was estimated to
    report as many as 100 images each day (Statistics, [2020](#bib.bib105)). This
    makes it challenging for radiologists to provide high-quality reports within the
    scheduled time. The current demand extends patient waiting time and increases
    the risk of disease transmission (Beddiar et al., [2023](#bib.bib7)) and compromises
    patient care. The development of automatic report generation techniques can help
    alleviate this problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像可以提供身体的详细信息，并帮助医生筛查、诊断和监测疾病，而无需侵入性技术（Beddiar等，[2023](#bib.bib7)；Liao等，[2023](#bib.bib64)）。放射科医生将从医学影像中提取的信息总结成放射报告，以供临床决策。然而，手动生成报告是劳动密集型的、耗时的，并且需要广泛的专业知识（Beddiar等，[2023](#bib.bib7)）。Topol
    ([2019](#bib.bib115))指出，医学图像解释的需求远远超过了美国医生的现有能力。在流行病期间和人口老龄化的情况下，情况可能会变得更糟。例如，在Covid-19大流行期间，在英国，每位放射科医生每天报告的图像多达100张（Statistics，[2020](#bib.bib105)）。这使得放射科医生在规定时间内提供高质量报告变得具有挑战性。目前的需求延长了患者的等待时间，并增加了疾病传播的风险（Beddiar等，[2023](#bib.bib7)），并影响了患者护理。自动报告生成技术的发展可以帮助缓解这个问题。
- en: Automatic high-quality report generation is challenging. It is intrinsically
    a multi-modality problem (Tu et al., [2024](#bib.bib116); Yan et al., [2023](#bib.bib141)).
    In routine clinical practice, to produce clear, correct, concise, complete, consistent,
    and coherent reports, radiologists need to combine information from images with
    information from other modality data, such as clinical history and related clinical
    measures. Previously developed techniques mostly considered images as input, while
    for the past three years, multi-modality deep learning developed very rapidly.
    An increasing number of research papers endeavored to emulate physicians by leveraging
    multi-modal data for the generation of diagnostic reports, as shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data").
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成高质量报告具有挑战性。这本质上是一个多模态问题（Tu等，[2024](#bib.bib116)；Yan等，[2023](#bib.bib141)）。在常规临床实践中，为了生成清晰、正确、简洁、完整、一致和连贯的报告，放射科医生需要将图像信息与其他模态数据的信息结合起来，例如临床历史和相关临床指标。以往开发的技术主要考虑将图像作为输入，而在过去三年中，多模态深度学习发展非常迅速。越来越多的研究论文尝试通过利用多模态数据来模拟医生，以生成诊断报告，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")所示。
- en: '![Refer to caption](img/bb21aee80ac25f03ef1ded8137d80390.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bb21aee80ac25f03ef1ded8137d80390.png)'
- en: 'Figure 1: The distributions of reviewed papers using image data and multi-modality
    data as inputs per year from 2021 to 2024\. The percentage denotes the input’s
    prevalence among articles published within the year.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：2021年至2024年间，每年使用图像数据和多模态数据作为输入的审阅论文的分布情况。百分比表示输入在当年发表的文章中的普及程度。
- en: 'Most of the previous surveys on this topic (Kaur et al., [2022](#bib.bib48);
    Beddiar et al., [2023](#bib.bib7); Liao et al., [2023](#bib.bib64); Shamshad et al.,
    [2023](#bib.bib97); Liu et al., [2023a](#bib.bib67)) did not include non-imaging
    inputs. Messina et al. ([2022](#bib.bib75)) considered non-imaging inputs, but
    only involved 6 papers. Totally, the previous surveys included 40 to 66 papers
    for report generation, primarily focusing on articles published before 2022\.
    This survey differs from previous ones in three main contributions: (1) we analyse
    an additional 22 papers that utilize non-image inputs, and focuses on the acquisition,
    analysis, and integration of multi-modal inputs. To the best of our knowledge,
    it is the first review to investigate state-of-the-art multi-modal data processing
    techniques for report generation; (2) we examine 89 papers published from 2021
    to 2024 to provide a comprehensive study on novel techniques in automatic report
    generation; and (3) we propose a general workflow for report generation with a
    taxonomy of approaches employed, and we summarize training strategies, public
    datasets, and mainstream evaluation methods, as shown in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data"). The workflow includes 5 key components: inputs, data
    preparation, feature learning, feature fusion, and report generation. Table LABEL:tab:overall
    in Appendix A summarizes all works included in this survey.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的大多数调查（Kaur et al., [2022](#bib.bib48); Beddiar et al., [2023](#bib.bib7);
    Liao et al., [2023](#bib.bib64); Shamshad et al., [2023](#bib.bib97); Liu et al.,
    [2023a](#bib.bib67)）未包含非影像输入。Messina et al. ([2022](#bib.bib75)) 考虑了非影像输入，但仅涉及
    6 篇论文。总体来说，之前的调查包括了 40 到 66 篇关于报告生成的论文，主要集中在 2022 年之前发表的文章。本调查与之前的不同，主要有三个贡献：（1）我们分析了另外
    22 篇使用非影像输入的论文，重点关注多模态输入的获取、分析和集成。据我们所知，这是首个研究报告生成领域最先进的多模态数据处理技术的综述；（2）我们检查了
    2021 年至 2024 年间发表的 89 篇论文，以提供关于自动报告生成新技术的全面研究；（3）我们提出了一个报告生成的通用工作流程，并对所采用的方法进行分类，总结了训练策略、公共数据集和主流评估方法，如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data") 所示。该工作流程包括 5 个关键组件：输入、数据准备、特征学习、特征融合和报告生成。附录
    A 的表 LABEL:tab:overall 总结了本调查中包含的所有工作。
- en: '![Refer to caption](img/251ab5f96d9e3aad347eb9360e180010.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/251ab5f96d9e3aad347eb9360e180010.png)'
- en: 'Figure 2: The summarized workflow of automatic radiology report generation.
    The fundamental components and key techniques are included. The (x, y%) for each
    method represents the number and percentage of papers used that technique in our
    survey.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：自动放射学报告生成的总结工作流程。包括了基本组件和关键技术。每种方法的 (x, y%) 表示我们调查中使用该技术的论文数量和百分比。
- en: The remainder of the paper is organized as follows. Section [2](#S2 "2 Search
    and selection of articles ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data") introduces the paper search and selection process. Section
    [3](#S3 "3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data") first provides a workflow of deep learning-based report
    generation, then analyses the techniques in each component of the workflow, and
    finally introduces the overall training strategies. Next, in sections [4](#S4
    "4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") and [5](#S5 "5 Evaluation ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"), we introduce popular public
    datasets and evaluation methods, including metrics and expert evaluation. Section
    [6](#S6 "6 Benchmark Comparison ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data") compares the model performance of several papers
    under the same experimental setting. Lastly, we discuss challenges and perspectives
    on this topic in section [7](#S7 "7 Challenges and Future Works ‣ A Survey of
    Deep Learning-based Radiology Report Generation Using Multimodal Data") and provide
    a conclusion in section [8](#S8 "8 Conclusions ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data").
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。第[2](#S2 "2 搜索与选择文章 ‣ 基于深度学习的放射学报告生成的多模态数据调查")节介绍了论文的搜索与选择过程。第[3](#S3
    "3 方法 ‣ 基于深度学习的放射学报告生成的多模态数据调查")节首先提供了基于深度学习的报告生成的工作流程，然后分析了工作流程中每个组件的技术，最后介绍了整体的训练策略。接下来，在第[4](#S4
    "4 数据集 ‣ 基于深度学习的放射学报告生成的多模态数据调查")节和第[5](#S5 "5 评价 ‣ 基于深度学习的放射学报告生成的多模态数据调查")节中，我们介绍了流行的公共数据集和评估方法，包括指标和专家评估。第[6](#S6
    "6 基准比较 ‣ 基于深度学习的放射学报告生成的多模态数据调查")节比较了在相同实验设置下几篇论文的模型性能。最后，我们在第[7](#S7 "7 挑战与未来工作
    ‣ 基于深度学习的放射学报告生成的多模态数据调查")节讨论了这一主题的挑战和展望，并在第[8](#S8 "8 结论 ‣ 基于深度学习的放射学报告生成的多模态数据调查")节提供了结论。
- en: 2 Search and selection of articles
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 搜索与选择文章
- en: Three search engines (Google Scholar, PubMed, and Springer) and four queries
    were employed to collect articles. They included “radiology report generation",
    (medical OR medicine OR health OR radiology) AND (report OR description OR caption)
    AND generation, modal AND (medical OR medicine OR health OR radiology) AND (report
    OR description OR caption) AND generation, and “medical report generation". Following
    the searches, the titles and abstracts of each article were read briefly to identify
    those that met the selection criteria. If there was uncertainty, the article was
    included to ensure relevant studies were not omitted. The selection criteria were
    framed around three aspects. First, we included articles published in the years
    2021, 2022, 2023, and 2024 due to the significant number of developments using
    multi-modal technology in recent years. We aim to focus on the latest algorithms
    not covered in previous surveys. Second, the studies must be original researches
    focused on the automatic generation of full-text natural language radiology reports
    and include quantitative evaluation results. Techniques generating short captions
    of one or two sentences are excluded due to the differing nature of long report
    and sentence generation. Third, papers published in journals, conferences, and
    conference workshop proceedings were included. Moreover, papers uploaded on the
    arXiv website in 2023 and 2024 with over 30 citations were also selected. A total
    of 144 papers were identified using three search engines. In addition, by tracing
    the ancestry and descendants of papers, we identified another 24 papers. After
    removing duplicates, 97 publications were retained. We thoroughly read these works
    and applied exclusion criteria. First, at least one of the generated languages
    should be English. Second, at least one of the input data should be images. Finally,
    89 works were included in the following analysis.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了三个搜索引擎（Google Scholar、PubMed 和 Springer）和四个查询来收集文章。查询包括“放射科报告生成”，(医学 OR 医药
    OR 健康 OR 放射学) AND (报告 OR 描述 OR 标题) AND 生成，模式 AND (医学 OR 医药 OR 健康 OR 放射学) AND (报告
    OR 描述 OR 标题) AND 生成，以及“医学报告生成”。在搜索之后，简要阅读了每篇文章的标题和摘要，以识别符合选择标准的文章。如果有不确定的情况，文章被包括在内，以确保相关研究没有被遗漏。选择标准围绕三个方面设定。首先，我们包括了在2021年、2022年、2023年和2024年发表的文章，因为近年来使用多模态技术的发展显著。我们旨在关注最新的算法，这些算法在以往的调查中没有涉及。其次，研究必须是关于自动生成全文自然语言放射科报告的原创研究，并包括定量评估结果。由于长报告和句子生成的性质不同，生成一两句短标题的技术被排除在外。第三，包含在期刊、会议和会议研讨会论文集中发表的论文。此外，还选择了在2023年和2024年在arXiv网站上上传并被引用超过30次的论文。共识别出144篇论文。通过追踪论文的祖先和后裔，我们识别出另外24篇论文。去重后，保留了97篇出版物。我们彻底阅读了这些工作并应用了排除标准。首先，至少一个生成的语言应为英语。其次，至少一个输入数据应为图像。最终，89篇作品被纳入以下分析。
- en: 3 Methods
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3种方法
- en: 'Deep learning-based radiology report generation typically follows a standard
    workflow summarized in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data"). This
    section analyzes the techniques in the 89 works, based on the workflow identified.
    Overall, a basic radiology report generation framework consists of 5 steps: (1)
    multi-modality data acquisition (section [3.1](#S3.SS1 "3.1 Multimodality input
    data ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")); (2) data preparation (section [3.2](#S3.SS2 "3.2 Data
    preparation ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")); (3) feature learning (section [3.3](#S3.SS3 "3.3 Feature
    learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")); (4) feature fusion and interaction (section [3.4](#S3.SS4
    "3.4 Multi-modal feature fusion and interaction ‣ 3 Methods ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")); and (5) report
    generation (Section [3.5](#S3.SS5 "3.5 Report generation ‣ 3 Methods ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data")). In
    addition, novel training strategies, including modifying loss functions, reinforcement
    learning, and curriculum learning, are described in Section [3.6](#S3.SS6 "3.6
    Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的放射学报告生成通常遵循一个标准工作流程，如图[2](#S1.F2 "图 2 ‣ 1 引言 ‣ 基于深度学习的放射学报告生成综述")所示。本节基于识别出的工作流程分析了89项研究中的技术。总体而言，一个基本的放射学报告生成框架包括5个步骤：（1）多模态数据采集（第[3.1](#S3.SS1
    "3.1 多模态输入数据 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述")节）；（2）数据准备（第[3.2](#S3.SS2 "3.2 数据准备 ‣
    3 方法 ‣ 基于深度学习的放射学报告生成综述")节）；（3）特征学习（第[3.3](#S3.SS3 "3.3 特征学习 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述")节）；（4）特征融合与交互（第[3.4](#S3.SS4
    "3.4 多模态特征融合与交互 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述")节）；（5）报告生成（第[3.5](#S3.SS5 "3.5 报告生成
    ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述")节）。此外，第[3.6](#S3.SS6 "3.6 训练策略 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述")节描述了包括修改损失函数、强化学习和课程学习在内的新训练策略。
- en: Medical images, when analyzed with or without other types of data, are firstly
    prepared (step 2). Subsequently, they are input into feature extractors to perform
    feature learning (step 3), predominantly implemented using CNN or Transformer
    architectures, along with multiple enhancement modules (e.g., auxiliary task and
    contrastive learning). The feature extractors aim at extracting features relevant
    to report generation, and the enhancement modules are utilized to improve the
    expressiveness of the features. For certain approaches, a feature fusion and interaction
    module (step 4) is subsequently applied to align cross-modal data and mitigate
    the negative effects caused by differences between the visual and textual domains.
    After fusion and interaction, the features are conveyed back to the feature extractor
    or directly input into the generator to generate the report (step 5). The training
    strategy is implemented to improve learning effectiveness during training. Table
    LABEL:tab:overall in Appendix A presents detailed information for each paper across
    the five steps and training strategies.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 医学图像在与其他类型的数据进行分析时，首先进行准备（步骤2）。随后，它们被输入到特征提取器中进行特征学习（步骤3），这通常使用CNN或Transformer架构实现，并配有多个增强模块（例如，辅助任务和对比学习）。特征提取器旨在提取与报告生成相关的特征，而增强模块则用于提高特征的表现力。对于某些方法，随后应用特征融合与交互模块（步骤4），以对齐跨模态数据并减轻视觉和文本领域之间差异带来的负面影响。经过融合与交互后，特征会被传回特征提取器，或直接输入生成器以生成报告（步骤5）。训练策略则用于提高训练过程中的学习效果。附录A中的表LABEL:tab:overall提供了五个步骤及训练策略中每篇论文的详细信息。
- en: 3.1 Multimodality input data
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 多模态输入数据
- en: 'The input data refers to the data received by the report generation system.
    During model training and inference, the data can vary; for example, both images
    and real reports are used as inputs during training, while only images are used
    during inference (Shetty et al., [2023](#bib.bib98)). However, the model learns
    from the distribution and features of the training data. If the testing data changes,
    mostly, the model will struggle to generalize, resulting in decreased performance.
    Therefore, in this section, we mainly introduce the acquisition of input data
    that is consistent between the training and testing phases in the reviewed papers.
    The input data includes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据指的是报告生成系统接收的数据。在模型训练和推理过程中，数据可能会有所不同；例如，训练过程中使用了图像和实际报告作为输入，而推理过程中仅使用图像（Shetty
    et al., [2023](#bib.bib98)）。然而，模型是从训练数据的分布和特征中学习的。如果测试数据发生变化，模型通常会难以泛化，从而导致性能下降。因此，在本节中，我们主要介绍了在审阅的论文中，训练阶段和测试阶段一致的输入数据获取方法。输入数据包括：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Image data includes X-ray, magnetic resonance imaging (MRI), computed tomography
    (CT), ultrasound, gastrointestinal endoscope image, retinal image, and dermoscopy
    image. Most papers we reviewed focus on generating medical reports for chest X-ray
    images (82 works). Other than chest diagnosis, retinal image is the second most
    prevalent image modality (6 works). The retinal image includes lots of categories,
    e.g., fundus fluorescein angiography and color fundus photography. Other works
    focus on chest CT (5 works), gastrointestinal endoscope image (2 works), spine
    MRI (1 work), dermoscopy image (1 work), and breast ultrasound images (1 work).
    Although some images are non-radiological, such as ophthalmic images, we still
    include their report-generation techniques in this survey.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像数据包括X射线、磁共振成像（MRI）、计算机断层扫描（CT）、超声检查、胃肠内镜图像、视网膜图像和皮肤镜图像。我们审阅的大多数论文关注于胸部X射线图像的医学报告生成（82项工作）。除了胸部诊断，视网膜图像是第二常见的图像类型（6项工作）。视网膜图像包括多个类别，如眼底荧光素血管造影和眼底彩色摄影。其他工作集中在胸部CT（5项工作）、胃肠内镜图像（2项工作）、脊柱MRI（1项工作）、皮肤镜图像（1项工作）和乳腺超声图像（1项工作）。虽然一些图像是非放射学的，如眼科图像，但我们仍将其报告生成技术纳入本调查。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Medical terminology refers to medical terms and expressions, which are from
    keyword labels of images (Huang et al., [2021a](#bib.bib34), [b](#bib.bib35),
    [2022](#bib.bib36); Liu et al., [2023b](#bib.bib74)), or self-built corpora (Liu
    et al., [2021b](#bib.bib69); Cao et al., [2022](#bib.bib9), [2023](#bib.bib10);
    Xue et al., [2024](#bib.bib138); Gu et al., [2024](#bib.bib25); Li et al., [2023c](#bib.bib62)),
    storing common descriptions found in medical reports.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医学术语指的是医学术语和表达方式，这些术语来自图像的关键词标签（Huang et al., [2021a](#bib.bib34), [b](#bib.bib35),
    [2022](#bib.bib36); Liu et al., [2023b](#bib.bib74)），或自建的语料库（Liu et al., [2021b](#bib.bib69);
    Cao et al., [2022](#bib.bib9), [2023](#bib.bib10); Xue et al., [2024](#bib.bib138);
    Gu et al., [2024](#bib.bib25); Li et al., [2023c](#bib.bib62)），存储了医学报告中常见的描述。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Medical knowledge base mostly records the connections between different organs
    and diseases, and is presented in a graph format. Graph is a fundamental data
    structure consisting of a set of nodes and edges, which can easily represent a
    set of subjects and their connections. The knowledge graph can primarily be obtained
    in two ways: 1) public datasets such as the RadGraph (Jain et al., [2021](#bib.bib39))
    used by two works (Yang et al., [2022](#bib.bib143); Li et al., [2023a](#bib.bib60));
    2) self-designed knowledge bases according to authoritative medical standards
    (Huang et al., [2023](#bib.bib37); Xu et al., [2023](#bib.bib137)) or disease
    labels (Jia et al., [2022](#bib.bib42)). Yang et al. ([2023](#bib.bib144)) argued
    that manual graph construction is limited to diseases, further complicating the
    adaptation of these models for diverse datasets. To overcome it, they automatically
    constructed the medical knowledge based on real reports during training and leveraged
    the knowledge base during model inference.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医学知识库主要记录不同器官和疾病之间的关系，并以图形格式呈现。图形是一种基本的数据结构，由一组节点和边组成，可以轻松表示一组主题及其连接。知识图谱主要有两种获取方式：1)
    使用公共数据集，如RadGraph（Jain et al., [2021](#bib.bib39)），该数据集被两项工作（Yang et al., [2022](#bib.bib143);
    Li et al., [2023a](#bib.bib60)）所使用；2) 根据权威医学标准（Huang et al., [2023](#bib.bib37);
    Xu et al., [2023](#bib.bib137)）或疾病标签（Jia et al., [2022](#bib.bib42)）自设计的知识库。Yang
    et al. ([2023](#bib.bib144)) 认为，手动图谱构建在疾病方面存在局限，进一步复杂化了这些模型对多样化数据集的适应性。为了克服这一问题，他们基于实际报告自动构建了医学知识库，并在模型推理过程中利用了该知识库。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Real texts report is mostly obtained by data retrieval (Liu et al., [2021b](#bib.bib69);
    Song et al., [2022](#bib.bib104); Yang et al., [2022](#bib.bib143); Li et al.,
    [2023a](#bib.bib60); Liu et al., [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62);
    Jin et al., [2024](#bib.bib43)). For example, for each input image, Liu et al.
    ([2021b](#bib.bib69)) retrieved similar images from the training dataset and utilized
    the corresponding reports. This process mimics radiologists consulting previous
    medical case reports when drafting their own. In addition, Liu et al. ([2023b](#bib.bib74))
    also obtained pre-defined sentences based on input terminologies.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实际文本报告主要通过数据检索获得（Liu et al., [2021b](#bib.bib69); Song et al., [2022](#bib.bib104);
    Yang et al., [2022](#bib.bib143); Li et al., [2023a](#bib.bib60); Liu et al.,
    [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62); Jin et al., [2024](#bib.bib43)）。例如，对于每个输入图像，Liu
    et al. ([2021b](#bib.bib69)) 从训练数据集中检索类似图像并利用对应的报告。这一过程类似于放射科医生在撰写自己的报告时查阅以往的医疗案例报告。此外，Liu
    et al. ([2023b](#bib.bib74)) 还根据输入术语获得了预定义的句子。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Clinical information encompasses patient demographics (e.g., age and gender),
    clinical observations, and medical histories. It is included within the indication
    section of radiology reports (see Figure [4](#S4.F4 "Figure 4 ‣ 4 Datasets ‣ A
    Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data")).
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 临床信息包括患者人口统计数据（例如年龄和性别）、临床观察和病史。它包含在放射学报告的指示部分（见图 [4](#S4.F4 "Figure 4 ‣ 4 Datasets
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data")）。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Questionnaires: report generation model can be trained in a visual question
    answering way (Tanwani et al., [2022](#bib.bib113); Pellegrini et al., [2023](#bib.bib87)).
    The questionnaires are provided by public datasets, such as VQA-Rad (Lau et al.,
    [2018](#bib.bib52)) and Rad-ReStruct (Pellegrini et al., [2023](#bib.bib87)).'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问卷：报告生成模型可以通过视觉问答方式进行训练（Tanwani et al., [2022](#bib.bib113); Pellegrini et al.,
    [2023](#bib.bib87)）。问卷由公共数据集提供，如 VQA-Rad (Lau et al., [2018](#bib.bib52)) 和 Rad-ReStruct
    (Pellegrini et al., [2023](#bib.bib87))。
- en: 3.2 Data preparation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据准备
- en: Data preparation endeavors to enhance data quality and prepare it for model
    deployment, typically encompassing data cleansing, transformation, and organization.
    Conventional preparation methods include image resizing and cropping, text tokenizing,
    converting all tokens to lowercase, removing non-alphabetic tokens, and implementing
    data augmentation procedures. The methods utilized in each paper are outlined
    in Table LABEL:tab:overall in Appendix A, but it is worth noting that conventional
    preparation methods are so ubiquitous that some papers do not mention them. While
    the information is not recorded in the table, it does not mean the absence of
    a data preparation process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备旨在提升数据质量并为模型部署做好准备，通常包括数据清洗、转换和组织。传统的准备方法包括图像调整大小和裁剪、文本分词、将所有标记转换为小写、删除非字母标记，以及实施数据增强程序。每篇论文中使用的方法在附录
    A 的表格 LABEL:tab:overall 中列出，但值得注意的是，传统准备方法如此普遍，以至于有些论文没有提及它们。虽然信息未记录在表格中，但这并不意味着数据准备过程的缺失。
- en: Novel data preparation methods in the reviewed papers can be categorized into
    filtering (Ramesh et al., [2022](#bib.bib93)) and grouping (Wang et al., [2022b](#bib.bib122)).
    Ramesh et al. ([2022](#bib.bib93)) argued that writing a radiology report necessitates
    referencing historical information, which inevitably included descriptors such
    as ‘again’ and ‘decrease’. However, these terms cannot be inferred from a single
    image, therefore Ramesh et al. filtered such descriptions in the reports. This
    exclusion was found to facilitate the model’s learning process.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在审阅的论文中，新的数据准备方法可以分为过滤（Ramesh et al., [2022](#bib.bib93)）和分组（Wang et al., [2022b](#bib.bib122)）。Ramesh
    et al. ([2022](#bib.bib93)) 认为撰写放射学报告需要参考历史信息，这不可避免地包括如“再”及“减少”等描述性词汇。然而，这些术语无法从单一图像中推断，因此
    Ramesh et al. 在报告中过滤了这些描述。这一排除被发现有助于模型的学习过程。
- en: Grouping refers to organizing sentences from ground truth reports into distinct
    sections, typically relying on keywords from pre-defined knowledge graphs and
    filtering rules. Each section describes a specific anatomical structure. Grouping
    aims to enable the generation system to process various types of sentences differently.
    For instance, Wang et al. ([2022b](#bib.bib122)) employed different decoders to
    generate descriptions for different anatomical structures. Alongside the reviewed
    papers, a recently released public dataset named ImaGenome (Wu et al., [2021](#bib.bib132))
    also includes grouping results in their annotation files (see Section [4](#S4
    "4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")). The grouping result becomes more easily accessible.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分组指的是将来自真实报告的句子组织成不同的部分，通常依赖于预定义知识图谱和过滤规则中的关键词。每个部分描述一个特定的解剖结构。分组旨在使生成系统能够对不同类型的句子进行不同处理。例如，Wang
    等人（[2022b](#bib.bib122)）采用不同的解码器为不同的解剖结构生成描述。除了回顾的论文之外，最近发布的公共数据集 ImaGenome（Wu
    等人，[2021](#bib.bib132)）也在其注释文件中包含了分组结果（见第 [4](#S4 "4 Datasets ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data") 节）。分组结果变得更加容易获取。
- en: 3.3 Feature learning
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 特征学习
- en: 3.3.1 Image-based feature learning
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 基于图像的特征学习
- en: Previous research primarily utilized CNNs as architectures for extracting image
    features, however, recently, an increasing number of researchers have opted for
    the use of Transformers due to their improved performance. Simultaneously, numerous
    studies proposed novel modules to enhance the model capability. In this section,
    the model architecture is first introduced, and subsequently enhancement modules
    are described. These modules include auxiliary tasks, contrastive learning, and
    memory metrics. The architecture and modules utilized in each paper are outlined
    in Table LABEL:tab:overall in Appendix A.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究主要使用 CNN 作为图像特征提取的架构，但最近，越来越多的研究者选择使用 Transformer，因为其性能有所提升。同时，许多研究提出了新型模块以增强模型能力。本节首先介绍模型架构，然后描述增强模块。这些模块包括辅助任务、对比学习和记忆度量。每篇论文中使用的架构和模块在附录
    A 的表 LABEL:tab:overall 中概述。
- en: CNN and Transformer encoder for feature extraction
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CNN 和 Transformer 编码器用于特征提取
- en: The statistics of the architectures used as image feature extractor are shown
    in Figure [3](#S3.F3 "Figure 3 ‣ CNN and Transformer encoder for feature extraction
    ‣ 3.3.1 Image-based feature learning ‣ 3.3 Feature learning ‣ 3 Methods ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data"). In
    total, forty-six studies extract image features purely based on CNN models. Thirty-four
    works firstly encode images by CNN and then utilizes the Transformer layers to
    modify the embeddings. Eight works utilize a pure Transformer architecture to
    extract image features. Figure [3](#S3.F3 "Figure 3 ‣ CNN and Transformer encoder
    for feature extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3 Feature learning
    ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") shows a clear trend of more studies adopting CNN augmented with
    Transformer for image feature extraction.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像特征提取的架构统计见图 [3](#S3.F3 "Figure 3 ‣ CNN and Transformer encoder for feature
    extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3 Feature learning ‣ 3 Methods
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data")。总共有四十六项研究纯粹基于 CNN 模型提取图像特征。三十四项工作首先通过 CNN 对图像进行编码，然后利用 Transformer 层来修改嵌入。八项工作采用纯
    Transformer 架构来提取图像特征。图 [3](#S3.F3 "Figure 3 ‣ CNN and Transformer encoder for
    feature extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3 Feature learning
    ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") 显示了更多研究采用 CNN 和 Transformer 结合进行图像特征提取的明显趋势。
- en: '![Refer to caption](img/86b5001f276c85fa3f89903f54be5281.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86b5001f276c85fa3f89903f54be5281.png)'
- en: 'Figure 3: The statistics of the reviewed papers using different architectures
    to extract image features per year from 2021 to 2024\. The percentage denotes
    the method’s prevalence among articles published within the year.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用不同架构提取图像特征的回顾论文在 2021 年至 2024 年间的统计数据。百分比表示该方法在当年发表的文章中的普遍程度。
- en: For CNN architecture, two works self-designed CNN models, while the other works
    built it based on different classical visual models, such as ResNet (He et al.,
    [2016](#bib.bib27)) (42 papers), DenseNet (Huang et al., [2017](#bib.bib33)) (22
    papers), VGG (Simonyan and Zisserman, [2014](#bib.bib99)) (5 papers), Faster-RCNN
    (Ren et al., [2015](#bib.bib94)) (5 papers), Inception-V3 (Szegedy et al., [2016](#bib.bib110))
    (1 paper), ResNeXt (Xie et al., [2017](#bib.bib135)) (1 paper), EfficientNet (Tan
    and Le, [2019](#bib.bib111)) (1 paper), and the Two-Stream Inflated 3D ConvNets
    (I3D) (Carreira and Zisserman, [2017](#bib.bib11)) (1 paper). Pahwa et al. ([2021](#bib.bib84))
    modified the HRNet (Sun et al., [2019a](#bib.bib107)), a human pose estimation
    network. Other three works (Huang et al., [2021a](#bib.bib34), [b](#bib.bib35),
    [2022](#bib.bib36)) provided results based on different CNN structures.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CNN架构，有两个研究自设计了CNN模型，而其他研究则基于不同的经典视觉模型构建了CNN，如ResNet（He et al., [2016](#bib.bib27)）（42篇论文）、DenseNet（Huang
    et al., [2017](#bib.bib33)）（22篇论文）、VGG（Simonyan and Zisserman, [2014](#bib.bib99)）（5篇论文）、Faster-RCNN（Ren
    et al., [2015](#bib.bib94)）（5篇论文）、Inception-V3（Szegedy et al., [2016](#bib.bib110)）（1篇论文）、ResNeXt（Xie
    et al., [2017](#bib.bib135)）（1篇论文）、EfficientNet（Tan and Le, [2019](#bib.bib111)）（1篇论文）和Two-Stream
    Inflated 3D ConvNets (I3D)（Carreira and Zisserman, [2017](#bib.bib11)）（1篇论文）。Pahwa
    et al. ([2021](#bib.bib84)) 修改了HRNet（Sun et al., [2019a](#bib.bib107)），这是一种人体姿态估计网络。其他三项工作（Huang
    et al., [2021a](#bib.bib34), [b](#bib.bib35), [2022](#bib.bib36)）基于不同的CNN结构提供了结果。
- en: To improve model performance, ten works modified the CNN structure by attention
    modules, which assigned varying degrees of importance (weights) to different parts
    of the input by learnable parameters, allowing the model to selectively focus
    on specific regions of an image. Traditional attention mechanisms can be classified
    into channel-wise (Du et al., [2022](#bib.bib22); Wang et al., [2022e](#bib.bib130);
    Gajbhiye et al., [2022](#bib.bib24); Pahwa et al., [2021](#bib.bib84)) and spatial-wise
    (Pahwa et al., [2021](#bib.bib84); Jia et al., [2021](#bib.bib41)), which allocate
    different weights to the various channels and spatial positions of the inputs
    respectively. In addition, Li et al. ([2023b](#bib.bib61)) and Wang et al. ([2024a](#bib.bib121))
    utilized the idea of the class activation map (Zhou et al., [2016](#bib.bib152))
    to obtain weights. Yan et al. ([2022](#bib.bib139)) initially extracted image
    patch features, clustered them using an unsupervised method, and then weighted
    the cluster results. Experimental results show that attention mechanism allows
    models to pay more attention to the lesions than irrelevant background. With the
    rise of the Transformers (Vaswani et al., [2017](#bib.bib118)), multi-head attention
    has become a potent method for information interaction. Wang et al. ([2023c](#bib.bib127))
    first extracted regions of interest from the frontal view and then employed multi-head
    attention to fuse information between the frontal and lateral views with the regions.
    This approach introduced regions of interest into model training to improve model
    performance.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高模型性能，十项工作通过注意力模块修改了CNN结构，这些模块通过可学习的参数为输入的不同部分分配不同的权重（重要性），使模型能够选择性地关注图像的特定区域。传统的注意力机制可以分为通道级（Du
    et al., [2022](#bib.bib22); Wang et al., [2022e](#bib.bib130); Gajbhiye et al.,
    [2022](#bib.bib24); Pahwa et al., [2021](#bib.bib84)）和空间级（Pahwa et al., [2021](#bib.bib84);
    Jia et al., [2021](#bib.bib41)），分别为输入的不同通道和空间位置分配不同的权重。此外，Li et al. ([2023b](#bib.bib61))
    和 Wang et al. ([2024a](#bib.bib121)) 采用了类激活图的理念（Zhou et al., [2016](#bib.bib152)）来获得权重。Yan
    et al. ([2022](#bib.bib139)) 首先提取了图像块特征，使用无监督方法对其进行聚类，然后对聚类结果加权。实验结果表明，注意力机制使模型能够更加关注病变区域，而非无关背景。随着Transformers（Vaswani
    et al., [2017](#bib.bib118)）的兴起，多头注意力成为信息交互的强大方法。Wang et al. ([2023c](#bib.bib127))
    首先从正面视图中提取了感兴趣区域，然后采用多头注意力融合正面和侧面视图之间的区域信息。这种方法将感兴趣区域引入模型训练，以提高模型性能。
- en: For Transformer architecture, most of them leverage a standard Transformer encoder,
    while Li et al. ([2023d](#bib.bib63)) argued that aligning images and text posed
    a challenge due to the continuous nature of images and the discrete nature of
    text; therefore, they improved the model performance by using a discrete variational
    autoencoder (Ramesh et al., [2021](#bib.bib92)) to obtain discrete visual tokens.
    Other works improved the model performance by modifying the self-attention module.
    Three works (Wang et al., [2022e](#bib.bib130); Lin et al., [2023](#bib.bib66);
    Wang et al., [2023d](#bib.bib131)) added high-order interactions among three inputs
    of the Transformer attention module. Two works (Miura et al., [2021](#bib.bib77);
    Wang et al., [2022e](#bib.bib130)) were inspired by the memory-augmented attention
    (Cornia et al., [2020](#bib.bib17)) and extended the keys and values with additional
    plain learnable vectors to record more information. Li et al. ([2022b](#bib.bib59))
    introduced a learnable parameter in the attention operation. Wang et al. ([2023d](#bib.bib131))
    modified the encoder by including additional input tokens. These tokens were named
    ‘expert tokens’ to emulate the “multi-expert joint diagnosis” methodology.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Transformer架构，大多数使用标准的Transformer编码器，而Li等人（[2023d](#bib.bib63)）认为，由于图像的连续性和文本的离散性，图像和文本的对齐构成了挑战，因此他们通过使用离散变分自编码器（Ramesh等，
    [2021](#bib.bib92)）来获取离散视觉标记，从而提高了模型性能。其他研究通过修改自注意力模块提高了模型性能。三项研究（Wang等，[2022e](#bib.bib130)；Lin等，[2023](#bib.bib66)；Wang等，[2023d](#bib.bib131)）在Transformer注意力模块的三个输入之间添加了高阶交互。两项研究（Miura等，[2021](#bib.bib77)；Wang等，[2022e](#bib.bib130)）受到了记忆增强注意力（Cornia等，[2020](#bib.bib17)）的启发，通过额外的可学习向量扩展了键和值，以记录更多信息。Li等人（[2022b](#bib.bib59)）在注意力操作中引入了可学习的参数。Wang等人（[2023d](#bib.bib131)）通过包含额外的输入标记来修改编码器。这些标记被命名为“专家标记”，以模拟“多专家联合诊断”方法。
- en: Auxiliary task for feature extraction
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征提取的辅助任务
- en: Auxiliary tasks aim to provide additional supervision signals to the feature
    extractor, enabling it to extract information relevant to report generation from
    images. These tasks mainly include classification (22 papers), graph construction
    (10 papers), embedding comparison (10 papers), and detection/segmentation (7 papers).
    Each of them is introduced in detail below.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助任务旨在为特征提取器提供额外的监督信号，使其能够从图像中提取与报告生成相关的信息。这些任务主要包括分类（22篇论文）、图构建（10篇论文）、嵌入比较（10篇论文）和检测/分割（7篇论文）。下面将详细介绍每一种任务。
- en: 'Classification: The most common auxiliary task used in the reviewed papers
    is classification referring to assigning images to predefined categories. The
    predefined categories primarily include medical tags (Gajbhiye et al., [2022](#bib.bib24);
    Kaur and Mittal, [2022b](#bib.bib47); Wang et al., [2022e](#bib.bib130); Hou et al.,
    [2021b](#bib.bib32); Du et al., [2022](#bib.bib22); You et al., [2022](#bib.bib147);
    Alfarghaly et al., [2021](#bib.bib3)) and disease labels (Liu et al., [2021e](#bib.bib72);
    Zhou et al., [2021](#bib.bib153); Wang et al., [2022d](#bib.bib129); Yang et al.,
    [2023](#bib.bib144); Zhang et al., [2023b](#bib.bib149); Wang et al., [2024a](#bib.bib121);
    Jin et al., [2024](#bib.bib43); Hou et al., [2021b](#bib.bib32)). The medical
    tags are from standard medical vocabularies including hundreds of labels, such
    as anatomical structures and pathological signs. They are provided by manual annotations
    or auto annotation tools (e.g., NIH MTI web API ¹¹1https://ii.nlm.nih.gov/MTI/index.shtml
    and RadGraph (Jain et al., [2021](#bib.bib39))). Disease labels are provided by
    auto annotation tools (e.g., CheXpert (Irvin et al., [2019](#bib.bib38)) and CheXbert
    (Smit et al., [2020](#bib.bib103))). Compared to disease labels, medical tags
    offer a more comprehensive range of information. However, to the best of our knowledge,
    there is no literature that supports the superiority of medical tags over disease
    labels. Perhaps due to the extensive scope covered by medical tags, deep learning
    models face challenges in acquiring such rich knowledge. Zhou et al. ([2021](#bib.bib153))
    incorporated 32 additional labels for lesion location, size, and shape (e.g.,
    “upper/lower” and “patchy”) into the disease label set, observing a slight improvement
    in model performance.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：在所审阅的论文中，最常见的辅助任务是分类，指将图像分配到预定义的类别中。预定义的类别主要包括医学标签（Gajbhiye 等，[2022](#bib.bib24)；Kaur
    和 Mittal，[2022b](#bib.bib47)；Wang 等，[2022e](#bib.bib130)；Hou 等，[2021b](#bib.bib32)；Du
    等，[2022](#bib.bib22)；You 等，[2022](#bib.bib147)；Alfarghaly 等，[2021](#bib.bib3)）和疾病标签（Liu
    等，[2021e](#bib.bib72)；Zhou 等，[2021](#bib.bib153)；Wang 等，[2022d](#bib.bib129)；Yang
    等，[2023](#bib.bib144)；Zhang 等，[2023b](#bib.bib149)；Wang 等，[2024a](#bib.bib121)；Jin
    等，[2024](#bib.bib43)；Hou 等，[2021b](#bib.bib32)）。医学标签来自标准医学词汇，包括数百个标签，如解剖结构和病理迹象。它们由手动注释或自动注释工具提供（例如，NIH
    MTI web API ¹¹1https://ii.nlm.nih.gov/MTI/index.shtml 和 RadGraph（Jain 等，[2021](#bib.bib39)））。疾病标签由自动注释工具提供（例如，CheXpert（Irvin
    等，[2019](#bib.bib38)）和 CheXbert（Smit 等，[2020](#bib.bib103)））。与疾病标签相比，医学标签提供了更全面的信息。然而，据我们所知，目前没有文献支持医学标签优于疾病标签。也许由于医学标签涵盖的范围广泛，深度学习模型在获取如此丰富的知识时面临挑战。Zhou
    等（[2021](#bib.bib153)）在疾病标签集里加入了32个额外的标签，用于病变位置、大小和形状（例如，“上/下”和“斑点状”），观察到模型性能略有提升。
- en: Other notable categories used in the reviewed papers include matching status
    (Li et al., [2023a](#bib.bib60)), local properties (Yang et al., [2021a](#bib.bib142)),
    report cluster results (Li et al., [2022a](#bib.bib56)), and fix answer categories
    (Tanwani et al., [2022](#bib.bib113)). Li et al. ([2023a](#bib.bib60)) predicted
    the matching status of a given image-report pair. Yang et al. ([2021a](#bib.bib142))
    devised localized property labels for breast ultrasound images, such as tumor
    morphology, to facilitate the identification of properties that are challenging
    to be discerned in low-resolution images. Li et al. ([2022a](#bib.bib56)) first
    conducted unsupervised clustering on the ground truth report, subsequently utilizing
    the resultant clusters as labels. This auxiliary task yielded a marked improvement
    in text generation performance. Tanwani et al. ([2022](#bib.bib113)) considered
    the report generation as a question-answer task, and the classifier was designed
    for fixed answer categories. In addition, for a detection auxiliary task, classifiers
    need to be applied to identify attributes of detected regions (e.g., ‘right lung’).
    This paragraph eschews such cases to circumvent redundancy.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其他在审阅论文中使用的显著类别包括匹配状态（Li 等，[2023a](#bib.bib60)）、局部属性（Yang 等，[2021a](#bib.bib142)）、报告集群结果（Li
    等，[2022a](#bib.bib56)）和固定答案类别（Tanwani 等，[2022](#bib.bib113)）。Li 等（[2023a](#bib.bib60)）预测给定图像-报告对的匹配状态。Yang
    等（[2021a](#bib.bib142)）为乳腺超声图像设计了局部属性标签，如肿瘤形态，以促进识别低分辨率图像中难以分辨的属性。Li 等（[2022a](#bib.bib56)）首先对真实报告进行了无监督聚类，然后利用生成的簇作为标签。这一辅助任务显著提高了文本生成性能。Tanwani
    等（[2022](#bib.bib113)）将报告生成视为问答任务，分类器被设计为固定答案类别。此外，对于检测辅助任务，需要应用分类器来识别检测区域的属性（例如，“右肺”）。本段避免了这种情况，以避免冗余。
- en: 'Graph construction: Graph construction aims at introducing prior knowledge
    into the report generation process. The knowledge graph in this section differs
    from that in Section [3.1](#S3.SS1 "3.1 Multimodality input data ‣ 3 Methods ‣
    A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data").
    Here, node features are extracted from images, and edges are defined as parameters
    in graph convolution networks. In contrast, the node and edge information in the
    input knowledge graphs is derived from non-image data. A classical method was
    proposed by Zhang et al. ([2020](#bib.bib150)) and yielded promising outcomes.
    A knowledge graph was constructed firstly based on insights provided by domain
    experts, where nodes represented major abnormalities and major organs, and bidirectional
    connections linked nodes that were related to each other. To initialize nodes
    features, a spatial attention module was introduced after the CNN backbone using
    1$\times$1 convolution layers and softmax layers. The number of channels matched
    the number of nodes. The nodes’ initial embedding was derived as attention-weighted
    feature maps. Then graph convolution layers were employed to disseminate information
    throughout the graph, followed by two branches for classification and report generation.
    First, the classification branch was trained, and subsequently, parameters in
    both the CNN backbone and the graph convolution layers were frozen, only the report
    generation decoder was trained. Six works (Liu et al., [2021b](#bib.bib69), [d](#bib.bib71);
    Cao et al., [2022](#bib.bib9); Wang et al., [2022c](#bib.bib125); Yan, [2022](#bib.bib140);
    Zhang et al., [2023b](#bib.bib149)) utilized this method (Zhang et al., [2020](#bib.bib150)).
    Wang et al. ([2022c](#bib.bib125)) expanded the graph (Zhang et al., [2020](#bib.bib150))
    by incorporating information from a radiology terms corpus named Radiology Lexicon
    (RadLex)²²2http://www.radlex.org/ (Langlotz, [2006](#bib.bib51)). As the number
    of graph nodes increased, the model performance initially improved, peaked at
    40 nodes, and then declined, with a noticeable decrease at 60 nodes. Liu et al.
    ([2021d](#bib.bib71)) constructed a large graph based on the MIMIC-CXR dataset.
    The nodes represented frequent clinical abnormalities and the edges represented
    the co-occurrence situation of different abnormalities. In addition, Li et al.
    ([2023b](#bib.bib61)) used the disease prediction results to obtain the node features.
    The nodes were classification probabilities, with learnable edge weights.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图构建：图构建旨在将先验知识引入报告生成过程。本节中的知识图谱与第[3.1节](#S3.SS1 "3.1 多模态输入数据 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述")中的知识图谱有所不同。在这里，节点特征是从图像中提取的，而边被定义为图卷积网络中的参数。相比之下，输入知识图谱中的节点和边信息来自非图像数据。张等人（[2020](#bib.bib150)）提出了一种经典方法，并取得了良好的成果。首先基于领域专家提供的见解构建了知识图谱，其中节点表示主要异常和主要器官，双向连接将相关节点连接起来。为了初始化节点特征，引入了一个空间注意力模块，该模块在CNN骨干网之后使用1$\times$1卷积层和softmax层。通道数量与节点数量匹配。节点的初始嵌入是作为注意力加权特征图导出的。然后，采用图卷积层在整个图中传播信息，随后通过两个分支进行分类和报告生成。首先训练分类分支，然后冻结CNN骨干网和图卷积层中的参数，仅训练报告生成解码器。六项工作（刘等，[2021b](#bib.bib69)，[d](#bib.bib71)；曹等，[2022](#bib.bib9)；王等，[2022c](#bib.bib125)；闫，[2022](#bib.bib140)；张等，[2023b](#bib.bib149)）采用了这一方法（张等，[2020](#bib.bib150)）。王等（[2022c](#bib.bib125)）通过加入名为Radiology
    Lexicon (RadLex)²²2http://www.radlex.org/（Langlotz，[2006](#bib.bib51)）的放射学术语语料库扩展了图（张等，[2020](#bib.bib150)）。随着图节点数量的增加，模型性能最初有所提升，在40个节点时达到峰值，然后下降，在60个节点时明显下降。刘等（[2021d](#bib.bib71)）基于MIMIC-CXR数据集构建了一个大图。节点表示常见的临床异常，边表示不同异常的共现情况。此外，李等（[2023b](#bib.bib61)）利用疾病预测结果来获取节点特征。节点是分类概率，边权重可学习。
- en: Another graph reconstruction method aims at reconstructing triplets that are
    in the form of (entity1, relationship, entity2), such as (opacity, suggestive
    of, infection). Three works (Dalla Serra et al., [2022](#bib.bib18), [2023b](#bib.bib20);
    Li et al., [2022b](#bib.bib59)) firstly predicted the triplets and then generated
    reports based on them. The experimental results show that using triplets alone
    for report generation is ineffective; combining them with features extracted from
    images is necessary for better results.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种图谱重建方法旨在重建形式为（实体1，关系，实体2）的三元组，例如（不透明度，提示感染）。三项研究（Dalla Serra 等，[2022](#bib.bib18)，[2023b](#bib.bib20)；Li
    等，[2022b](#bib.bib59)）首先预测三元组，然后基于这些三元组生成报告。实验结果表明，单独使用三元组进行报告生成效果不佳；需要将其与从图像中提取的特征结合使用，以获得更好的结果。
- en: 'Embedding comparison: Embedding comparison refers to constraining the consistency
    of different features in intermediate layers, thereby guiding the learning process.
    The comparison in reviewed papers is mainly applied between features extracted
    from images and real reports (Najdenkoska et al., [2021](#bib.bib80), [2022](#bib.bib81);
    Zhou et al., [2021](#bib.bib153); Yang et al., [2021b](#bib.bib145); Chen et al.,
    [2022](#bib.bib13); Wang et al., [2021](#bib.bib128), [2022d](#bib.bib129); Yang
    et al., [2023](#bib.bib144)). Experimental results show that the supervision signals
    from real text enable extracted visual features carry richer semantic information,
    facilitating more effective translation into radiology reports. Four works (Wang
    et al., [2021](#bib.bib128); Zhou et al., [2021](#bib.bib153); Wang et al., [2022d](#bib.bib129);
    Yang et al., [2023](#bib.bib144)) utilized a triple loss function to compel the
    image-text paired features to be closer to a latent space than the unpaired ones.
    Najdenkoska et al. ([2021](#bib.bib80), [2022](#bib.bib81)) inspired by the Auto-Encoding
    Variational Bayes (Kingma and Welling, [2013](#bib.bib49)). They used real reports
    to obtain a latent space during training and generated reports based on this space.
    The image extractors were enabled to capture features from images that closely
    resemble those found in real reports. Other two works (Yang et al., [2021b](#bib.bib145);
    Chen et al., [2022](#bib.bib13)) used the Term Frequency and Inverse Document
    Frequency (TF-IDF) to extract important information from real reports as supervision
    signals. TF-IDF is a statistical measure assessing a word’s importance by considering
    its frequency in a specific document and its rarity across the entire document
    set.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入比较：嵌入比较指的是限制中间层不同特征的一致性，从而指导学习过程。所审阅论文中的比较主要应用于从图像和真实报告中提取的特征之间（Najdenkoska
    等，[2021](#bib.bib80)，[2022](#bib.bib81)；Zhou 等，[2021](#bib.bib153)；Yang 等，[2021b](#bib.bib145)；Chen
    等，[2022](#bib.bib13)；Wang 等，[2021](#bib.bib128)，[2022d](#bib.bib129)；Yang 等，[2023](#bib.bib144)）。实验结果表明，来自真实文本的监督信号使得提取的视觉特征能够携带更丰富的语义信息，从而更有效地转化为放射学报告。四项研究（Wang
    等，[2021](#bib.bib128)；Zhou 等，[2021](#bib.bib153)；Wang 等，[2022d](#bib.bib129)；Yang
    等，[2023](#bib.bib144)）利用三重损失函数强制图像-文本配对特征在潜在空间中比未配对的特征更接近。Najdenkoska 等（[2021](#bib.bib80)，[2022](#bib.bib81)）受到自动编码变分贝叶斯（Kingma
    和 Welling，[2013](#bib.bib49)）的启发。他们使用真实报告在训练期间获得潜在空间，并基于该空间生成报告。图像提取器能够捕捉到与真实报告中发现的特征非常相似的图像特征。其他两项研究（Yang
    等，[2021b](#bib.bib145)；Chen 等，[2022](#bib.bib13)）使用词频-逆文档频率（TF-IDF）从真实报告中提取重要信息作为监督信号。TF-IDF
    是一种统计度量，通过考虑一个词在特定文档中的频率及其在整个文档集中的稀有程度来评估词的重要性。
- en: 'In addition, to produce reports for abnormalities not seen during training,
    Sun et al. ([2022](#bib.bib106)) initially linearly projected visual features
    to semantic features, and extracted semantic features of labels by the BioBert
    model (Lee et al., [2020](#bib.bib54)). Consistent constrain was applied between
    two similarity: 1) the similarity between pairwise elements in the semantic features
    from visual features; and 2) the similarity between the semantic features from
    visual features and the semantic features from labels. Zhang et al. ([2023b](#bib.bib149))
    integrated semi-supervised learning into report generation using two networks.
    They first applied different types of noise to an input image to create two variations,
    which were then fed into the two networks. An auxiliary loss function was employed
    to ensure consistency in the extracted visual features.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了生成训练过程中未见的异常报告，Sun等人（[2022](#bib.bib106)）最初将视觉特征线性映射到语义特征，并通过BioBert模型（Lee等人，[2020](#bib.bib54)）提取标签的语义特征。在两个相似性之间应用了一致性约束：1)
    视觉特征中的语义特征之间的成对相似性；2) 视觉特征中的语义特征与标签中的语义特征之间的相似性。Zhang等人（[2023b](#bib.bib149)）将半监督学习集成到报告生成中，使用了两个网络。他们首先对输入图像应用不同类型的噪声，以创建两个变体，然后将其输入到这两个网络中。采用了辅助损失函数以确保提取的视觉特征的一致性。
- en: 'Detection/segmentation: Object detection locates and identifies objects or
    patterns within an image, focusing on determining their presence and position.
    Segmentation divides an image into meaningful regions by identifying and separating
    objects based on specific characteristics. Both processes enhance the model’s
    understanding of the image by object recognition and region extraction, and can
    improve the model’s interpretability by linking the detection/segmentation results
    with generated sentences. The detection/segmentation regions can be anatomical
    regions (Tanida et al., [2023](#bib.bib112); Dalla Serra et al., [2023b](#bib.bib20),
    [a](#bib.bib19); Wang et al., [2023c](#bib.bib127); Han et al., [2021](#bib.bib26);
    Gu et al., [2024](#bib.bib25)) and abnormal regions (Sun et al., [2022](#bib.bib106)).
    No literature compares the impact of detection and segmentation tasks on report
    generation results. However, a publicly available dataset named Chest ImaGenome
    (see Section [4](#S4 "4 Datasets ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")) offers detection annotations, making them
    easier to be acquired than segmentation annotations.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 检测/分割：物体检测定位并识别图像中的物体或模式，重点是确定它们的存在和位置。分割通过基于特定特征识别和分离物体，将图像划分为有意义的区域。两者都通过物体识别和区域提取增强了模型对图像的理解，并通过将检测/分割结果与生成的句子联系起来，可以提高模型的可解释性。检测/分割区域可以是解剖区域（Tanida等人，[2023](#bib.bib112)；Dalla
    Serra等人，[2023b](#bib.bib20)，[a](#bib.bib19)；Wang等人，[2023c](#bib.bib127)；Han等人，[2021](#bib.bib26)；Gu等人，[2024](#bib.bib25)）和异常区域（Sun等人，[2022](#bib.bib106)）。目前尚无文献比较检测和分割任务对报告生成结果的影响。然而，一个名为Chest
    ImaGenome的公开数据集（见第[4](#S4 "4 Datasets ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data)节")提供了检测注释，使其比分割注释更易于获取。
- en: In addition, the outputs of auxiliary tasks can provide valuable information
    such as disease labels, therefore, inputting them into the following generation
    network is a common choice (Alfarghaly et al., [2021](#bib.bib3); Hou et al.,
    [2021b](#bib.bib32); Singh et al., [2021](#bib.bib100); Yang et al., [2021a](#bib.bib142);
    You et al., [2021](#bib.bib146); Zhou et al., [2021](#bib.bib153); Du et al.,
    [2022](#bib.bib22); Jia et al., [2022](#bib.bib42); Kaur and Mittal, [2022b](#bib.bib47);
    Sun et al., [2022](#bib.bib106); Wang et al., [2022a](#bib.bib120), [e](#bib.bib130);
    Yan et al., [2022](#bib.bib139); You et al., [2022](#bib.bib147); Tanida et al.,
    [2023](#bib.bib112); Li et al., [2022b](#bib.bib59); Jin et al., [2024](#bib.bib43)).
    For example, Zhou et al. ([2021](#bib.bib153)) sent the semantic word embeddings
    of the predicted findings from the classifier to the report generation decoder.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，辅助任务的输出可以提供有价值的信息，如疾病标签，因此，将其输入到后续生成网络中是一种常见的选择（Alfarghaly等人，[2021](#bib.bib3)；Hou等人，[2021b](#bib.bib32)；Singh等人，[2021](#bib.bib100)；Yang等人，[2021a](#bib.bib142)；You等人，[2021](#bib.bib146)；Zhou等人，[2021](#bib.bib153)；Du等人，[2022](#bib.bib22)；Jia等人，[2022](#bib.bib42)；Kaur和Mittal，[2022b](#bib.bib47)；Sun等人，[2022](#bib.bib106)；Wang等人，[2022a](#bib.bib120)，[e](#bib.bib130)；Yan等人，[2022](#bib.bib139)；You等人，[2022](#bib.bib147)；Tanida等人，[2023](#bib.bib112)；Li等人，[2022b](#bib.bib59)；Jin等人，[2024](#bib.bib43)）。例如，Zhou等人（[2021](#bib.bib153)）将分类器预测结果的语义词嵌入发送到报告生成解码器。
- en: Contrastive learning for feature extraction
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征提取的对比学习
- en: Contrastive learning is a self-supervised learning method to improve the representational
    capacity of models, which allows models to minimize the distance among positive
    pairs and maximize it for negative ones. It can be used to train feature extractors
    (Wang et al., [2022e](#bib.bib130); Lin et al., [2023](#bib.bib66); Wu et al.,
    [2022](#bib.bib133); Wang et al., [2023b](#bib.bib124)). Lin et al. ([2023](#bib.bib66))
    utilized a classical contrastive learning method named Momentum Contrast (He et al.,
    [2020](#bib.bib28)), where different views or augmented versions of the same image
    were considered as positive pairs. Another representative contrastive learning
    work is the Contrastive Language-Image Pre-training (CLIP) model (Radford et al.,
    [2021](#bib.bib91)). It connects textual and visual information by directly training
    on a vast dataset consisting of image-text pairs. Wang et al. ([2022e](#bib.bib130))
    directly employed it for image feature extraction and Wu et al. ([2022](#bib.bib133))
    applied the idea of CLIP to train the feature extractors on the training dataset.
    Wang et al. ([2023b](#bib.bib124)) however argued that previous works treated
    the entire report as input, overlooking the distinct information contained within
    individual sentences. This oversight could result in incorrect matching of image-text
    pairs. Therefore, they proposed phenotype-based contrastive learning. This method
    involved randomly initializing a set of vectors as phenotypes, allowing sentences
    and visual embeddings to interact with them, and finally conducting contrastive
    learning between the processed embeddings. The results outperformed previous contrastive
    learning methods in report generation.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习是一种自监督学习方法，用于提高模型的表示能力，它使得模型能够最小化正样本对之间的距离，并最大化负样本对之间的距离。它可以用于训练特征提取器（Wang
    et al., [2022e](#bib.bib130); Lin et al., [2023](#bib.bib66); Wu et al., [2022](#bib.bib133);
    Wang et al., [2023b](#bib.bib124)）。Lin et al. ([2023](#bib.bib66)) 使用了一种经典的对比学习方法，称为动量对比（Momentum
    Contrast）（He et al., [2020](#bib.bib28)），其中将同一图像的不同视图或增强版本视为正样本对。另一项具有代表性的对比学习工作是对比语言-图像预训练（CLIP）模型（Radford
    et al., [2021](#bib.bib91)）。它通过直接在包含图像-文本对的大型数据集上进行训练来连接文本和视觉信息。Wang et al. ([2022e](#bib.bib130))
    直接将其用于图像特征提取，而Wu et al. ([2022](#bib.bib133)) 将CLIP的理念应用于训练数据集上的特征提取器。Wang et
    al. ([2023b](#bib.bib124)) 认为，以前的工作将整个报告作为输入，忽略了单个句子中包含的独特信息。这种忽视可能导致图像-文本对的匹配不正确。因此，他们提出了基于表型的对比学习。这种方法涉及随机初始化一组向量作为表型，允许句子和视觉嵌入与它们互动，并最终在处理后的嵌入之间进行对比学习。结果优于以前的对比学习方法，在报告生成中表现更佳。
- en: Contrastive learning also can be part of the training loss (Tanwani et al.,
    [2022](#bib.bib113); Wang et al., [2022a](#bib.bib120); Li et al., [2023a](#bib.bib60);
    Liu et al., [2023b](#bib.bib74)), and can be applied between visual and textual
    features (i.e. image-text pairs) (Tanwani et al., [2022](#bib.bib113); Li et al.,
    [2023a](#bib.bib60); Liu et al., [2023b](#bib.bib74)), or be applied based on
    labels, treating samples with shared labels as positives and those without any
    common labels as negatives (Wang et al., [2022a](#bib.bib120)). Their ablation
    study demonstrated a significant improvement in results compared to standard contrastive
    loss (Wang et al., [2022a](#bib.bib120)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习也可以成为训练损失的一部分（Tanwani et al., [2022](#bib.bib113); Wang et al., [2022a](#bib.bib120);
    Li et al., [2023a](#bib.bib60); Liu et al., [2023b](#bib.bib74)），并且可以应用于视觉和文本特征之间（即图像-文本对）（Tanwani
    et al., [2022](#bib.bib113); Li et al., [2023a](#bib.bib60); Liu et al., [2023b](#bib.bib74)），或者基于标签进行应用，将具有共享标签的样本视为正样本，没有任何共同标签的样本视为负样本（Wang
    et al., [2022a](#bib.bib120)）。他们的消融研究表明，相比于标准的对比损失，结果有了显著的提升（Wang et al., [2022a](#bib.bib120)）。
- en: Contrastive attention is another method to utilize contrastive learning. Liu
    et al. ([2021c](#bib.bib70)) designed a contrastive attention model to extract
    abnormal region features by comparing input samples with normal cases. Similar
    features shared between the input and normal cases were subtracted from the input
    image feature, and the remaining feature was then concatenated with the original
    feature. Song et al. ([2022](#bib.bib104)) argued that the contrastive technique
    (Liu et al., [2021c](#bib.bib70)) did not consider historical information, therefore
    they proposed a module based on similarity retrieval technique to obtain similar
    images from the training dataset. The image features were processed by enlarging
    different features between inputs and the similar retrieved images.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对比注意力是利用对比学习的另一种方法。Liu et al. ([2021c](#bib.bib70)) 设计了一种对比注意力模型，通过将输入样本与正常病例进行比较来提取异常区域特征。在输入和正常病例之间共享的类似特征从输入图像特征中被减去，剩余的特征然后与原始特征进行拼接。Song
    et al. ([2022](#bib.bib104)) 认为对比技术（Liu et al., [2021c](#bib.bib70)）没有考虑历史信息，因此他们提出了一个基于相似性检索技术的模块，从训练数据集中获取类似图像。图像特征通过放大输入与类似检索图像之间的不同特征来处理。
- en: Memory metric for feature extraction
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征提取的记忆度量
- en: Using a memory metric for image feature extraction assumes the presence of similar
    features in various medical images. Memory metrics are employed to record and
    transmit the similarity information during training (Chen et al., [2022](#bib.bib13);
    Yan, [2022](#bib.bib140)). Typically, an n×n matrix is randomly initialized, where
    n represents the number of metric rows. Then, at each training step, the matrix
    is updated based on the visual features and the previous metrics. The memory metric
    used in this section is consistent with the metric used in the memory-driven transformer
    discussed in Section [3.5](#S3.SS5 "3.5 Report generation ‣ 3 Methods ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data"), with
    one being applied to images and the other to the generated reports.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用记忆度量进行图像特征提取假设各种医学图像中存在类似的特征。记忆度量用于在训练过程中记录和传输相似性信息（Chen et al., [2022](#bib.bib13);
    Yan, [2022](#bib.bib140)）。通常，n×n 矩阵会被随机初始化，其中 n 表示度量行数。然后，在每个训练步骤中，矩阵会根据视觉特征和之前的度量进行更新。本节使用的记忆度量与第
    [3.5](#S3.SS5 "3.5 Report generation ‣ 3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data") 节中讨论的记忆驱动变换器使用的度量一致，其中一个应用于图像，另一个应用于生成的报告。
- en: 3.3.2 Non-imaging-based feature learning
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 基于非图像的特征学习
- en: Most non-imaging data is presented in the form of text. Before fusion with image
    data, text data needs to be embedded. We first introduce a widely-used basic text
    embedding technique, the lookup table. This table assigns a unique index to each
    word or character, which is used to look up a pre-trained word vector or character
    vector. In addition to the basic method, additional feature extraction can be
    performed on these vectors to enhance their representation capabilities.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数非图像数据以文本形式呈现。在与图像数据融合之前，文本数据需要进行嵌入。我们首先介绍一种广泛使用的基本文本嵌入技术——查找表。该表为每个单词或字符分配一个唯一的索引，该索引用于查找预训练的单词向量或字符向量。除了基本方法外，还可以对这些向量进行额外的特征提取，以增强其表示能力。
- en: Transformer-based models such as BERT and its variants have emerged as mainstream
    methods for feature extraction across various textual data, such as terminology
    (Liu et al., [2021e](#bib.bib72); Cao et al., [2022](#bib.bib9), [2023](#bib.bib10);
    Liu et al., [2021b](#bib.bib69); Xue et al., [2024](#bib.bib138); Liu et al.,
    [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62)), real text reports (Liu et al.,
    [2021b](#bib.bib69), [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62); Jin
    et al., [2024](#bib.bib43)), knowledge graphs (Yang et al., [2022](#bib.bib143);
    Huang et al., [2023](#bib.bib37); Li et al., [2023a](#bib.bib60); Xu et al., [2023](#bib.bib137)),
    and questionnaires (Tanwani et al., [2022](#bib.bib113); Pellegrini et al., [2023](#bib.bib87)),
    and have achieved good results.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 的模型，如 BERT 及其变体，已成为在各种文本数据中进行特征提取的主流方法，例如术语（Liu et al., [2021e](#bib.bib72);
    Cao et al., [2022](#bib.bib9), [2023](#bib.bib10); Liu et al., [2021b](#bib.bib69);
    Xue et al., [2024](#bib.bib138); Liu et al., [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62)），真实文本报告（Liu
    et al., [2021b](#bib.bib69), [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62);
    Jin et al., [2024](#bib.bib43)），知识图谱（Yang et al., [2022](#bib.bib143); Huang et
    al., [2023](#bib.bib37); Li et al., [2023a](#bib.bib60); Xu et al., [2023](#bib.bib137)），以及问卷（Tanwani
    et al., [2022](#bib.bib113); Pellegrini et al., [2023](#bib.bib87)），并取得了良好的效果。
- en: Several methods are designed for a specific type of input. Li et al. ([2023c](#bib.bib62))
    used the TF-IDF to re-weight terminology embeddings. The re-weighted approach
    alleviated the issue of data imbalance, resulting in performance enhancement.
    Clinical information can be processed by a pre-trained feature extractor named
    BioSentVec (Zhou et al., [2021](#bib.bib153); Chen et al., [2019](#bib.bib12)).
    For the knowledge base, Yang et al. ([2022](#bib.bib143)) used a knowledge graph
    embedding model named RotatE (Sun et al., [2019b](#bib.bib109)) to obtain entity
    embeddings and relation embeddings from the RadGraph. Besides using the entire
    RadGraph as input, two works combined the real reports with RadGraph to extract
    case-related information from real reports and queried related information from
    the RadGraph (Yang et al., [2022](#bib.bib143); Li et al., [2023a](#bib.bib60)).
    Alternatively, Li et al. ([2023c](#bib.bib62)) considered RadGraph as an annotation
    tool for extracting entities and positional information from real reports. Other
    two works (Xu et al., [2023](#bib.bib137); Huang et al., [2023](#bib.bib37)) utilized
    classification results to process the self-built graph and extracted case-related
    information. The experimental results substantiate the beneficial impact of incorporating
    case-related knowledge on report generation, particularly evident when integrating
    real reports with the knowledge base. In addition, age and gender information
    are not text data and are generally encoded as one-hot vector (Zhou et al., [2021](#bib.bib153)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定类型的输入，设计了几种方法。Li 等人 ([2023c](#bib.bib62)) 使用了 TF-IDF 重新加权术语嵌入。重新加权的方法缓解了数据不平衡的问题，从而提升了性能。临床信息可以通过一个名为
    BioSentVec 的预训练特征提取器处理（Zhou 等人，[2021](#bib.bib153)；Chen 等人，[2019](#bib.bib12)）。对于知识库，Yang
    等人 ([2022](#bib.bib143)) 使用了一个名为 RotatE 的知识图谱嵌入模型（Sun 等人，[2019b](#bib.bib109)）从
    RadGraph 中获取实体嵌入和关系嵌入。除了使用整个 RadGraph 作为输入外，还有两个研究结合了实际报告和 RadGraph，从实际报告中提取与案例相关的信息，并从
    RadGraph 查询相关信息（Yang 等人，[2022](#bib.bib143)；Li 等人，[2023a](#bib.bib60)）。另外，Li 等人
    ([2023c](#bib.bib62)) 将 RadGraph 视为一种注释工具，用于从实际报告中提取实体和位置信息。其他两个研究（Xu 等人，[2023](#bib.bib137)；Huang
    等人，[2023](#bib.bib37)）利用分类结果处理自建图，并提取与案例相关的信息。实验结果证实了将与案例相关的知识纳入报告生成的有益影响，尤其是在将实际报告与知识库结合时尤为明显。此外，年龄和性别信息不是文本数据，通常编码为一-hot
    向量（Zhou 等人，[2021](#bib.bib153)）。
- en: 3.4 Multi-modal feature fusion and interaction
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 多模态特征融合与交互
- en: Feature fusion and interaction refer to the integration of multi-modal data
    from inputs or auxiliary tasks. This step has two purposes. First, visual and
    textual features from disparate domains present challenges for model learning.
    By fusing and facilitating interaction between these features, the domain gap
    can be narrowed, thereby enhancing network learning. Second, the image regions
    should align with the sentences in the reports. This correspondence can be learned
    through fusion and interaction. In the auxiliary task of embedding comparison
    (see Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Image-based feature learning ‣ 3.3 Feature
    learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")), semantic features extracted from real reports are used
    to supervise the learning of image features. However, this approach differs from
    multi-modal feature fusion. The objective of embedding comparison is to enhance
    image features, without incorporating non-image features into the generator. Instead,
    the fused features in this section are forwarded to the generator.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 特征融合和交互指的是将来自输入或辅助任务的多模态数据进行整合。此步骤有两个目的。首先，来自不同领域的视觉和文本特征对模型学习提出了挑战。通过融合并促进这些特征之间的交互，可以缩小领域差距，从而增强网络学习。其次，图像区域应与报告中的句子对齐。这种对应关系可以通过融合和交互来学习。在嵌入比较的辅助任务中（参见第
    [3.3.1](#S3.SS3.SSS1 "3.3.1 Image-based feature learning ‣ 3.3 Feature learning
    ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") 节），从实际报告中提取的语义特征用于监督图像特征的学习。然而，这种方法不同于多模态特征融合。嵌入比较的目标是增强图像特征，而不将非图像特征纳入生成器。相反，本节中的融合特征会被转发到生成器。
- en: The most straightforward approach for feature fusion and interaction is feature-level
    operation including the concatenation, summation, or multiplication of multimodal
    features (13 works). However, the feature-level operation could be too simple
    to enable sufficient interaction. Therefore, neural network-based methods are
    leveraged, such as LSTMs (2 works) and the multi-head attention mechanism (27
    works). While this approach facilitates convenient feature fusion, its lack of
    specific design for multi-modality data fusion leads to limited effectiveness
    in interaction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特征融合和交互的最直接方法是特征级操作，包括多模态特征的拼接、求和或乘法（13项工作）。然而，特征级操作可能过于简单，无法实现充分的交互。因此，采用了基于神经网络的方法，如
    LSTM（2项工作）和多头注意力机制（27项工作）。尽管这种方法便利了特征融合，但其对多模态数据融合缺乏具体设计，导致交互效果有限。
- en: 'We would like to highlight a memory metric-based method proposed by Chen et al.
    ([2021](#bib.bib15)). It significantly enhanced the performance of the report
    generation system by facilitating feature interaction. A metric was initialized
    randomly. Image features, text features from generated tokens, and memory metric
    features were then projected into the same space. Subsequently, distances between
    image features and memory metric features, as well as text features and memory
    metric features, were calculated. The top K metric features with the closest distances
    to the image or text were selected, respectively. These selected features were
    then weighted based on these distances and were fed back into an encoder-decoder
    structure. Two studies (Qin and Song, [2022](#bib.bib90); You et al., [2022](#bib.bib147))
    followed this method. Wang et al. ([2022a](#bib.bib120)) modified it in two ways:
    1) they initialized the matrix by visual and textual features; 2) the cross-modal
    interaction occurred only among cases with the same label. These two modifications
    both resulted in a notable improvement. Li et al. ([2023d](#bib.bib63)) contended
    that the methods mentioned lack explicit constraints for cross-modal alignments.
    They considered orthonormal bases as the metric and input them along with visual
    or textual features, into multi-head attention modules. Then the outputs of attention
    modules were processed by a self-defined gate mechanism. A triplet matching loss
    was utilized to align the processed visual and textual features. This method slightly
    improved the results.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要强调 Chen 等人（[2021](#bib.bib15)）提出的一种基于记忆度量的方法。该方法通过促进特征交互显著提升了报告生成系统的性能。首先，度量被随机初始化。然后，图像特征、生成的标记的文本特征和记忆度量特征被映射到同一空间。随后，计算图像特征与记忆度量特征之间的距离，以及文本特征与记忆度量特征之间的距离。分别选择与图像或文本距离最近的前
    K 个度量特征。这些选定的特征随后基于这些距离加权，并反馈到编码器-解码器结构中。两个研究（秦和宋，[2022](#bib.bib90)；尤等，[2022](#bib.bib147)）采用了这种方法。王等人（[2022a](#bib.bib120)）对其进行了两方面的修改：1）他们通过视觉和文本特征初始化了矩阵；2）跨模态交互仅在具有相同标签的情况中发生。这两种修改均带来了显著的改进。李等人（[2023d](#bib.bib63)）认为上述方法缺乏对跨模态对齐的明确约束。他们考虑了正交基作为度量，并将其与视觉或文本特征一起输入到多头注意力模块中。然后，注意力模块的输出通过自定义门机制进行处理。利用三元组匹配损失来对齐处理后的视觉和文本特征。这种方法略微改善了结果。
- en: 3.5 Report generation
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 报告生成
- en: The last step is report generation, which utilizes extracted features from earlier
    steps to produce the final reports. The generation methods mainly include decoder-based
    techniques (Section [3.5.1](#S3.SS5.SSS1 "3.5.1 Decoder-based techniques ‣ 3.5
    Report generation ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")), retrieval-based techniques (Section [3.5.2](#S3.SS5.SSS2
    "3.5.2 Retrieval-based and template-based techniques ‣ 3.5 Report generation ‣
    3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")), and template-based techniques (Section [3.5.2](#S3.SS5.SSS2
    "3.5.2 Retrieval-based and template-based techniques ‣ 3.5 Report generation ‣
    3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")). In addition, the development of large language models has
    made it possible to utilize them to enhance the quality of generated reports.
    It is discussed in Section [3.5.3](#S3.SS5.SSS3 "3.5.3 Large language model to
    assist report generation ‣ 3.5 Report generation ‣ 3 Methods ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是报告生成，它利用早期步骤提取的特征来生成最终报告。生成方法主要包括基于解码器的技术（第 [3.5.1](#S3.SS5.SSS1 "3.5.1
    Decoder-based techniques ‣ 3.5 Report generation ‣ 3 Methods ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data") 节），基于检索的技术（第
    [3.5.2](#S3.SS5.SSS2 "3.5.2 Retrieval-based and template-based techniques ‣ 3.5
    Report generation ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data") 节），以及基于模板的技术（第 [3.5.2](#S3.SS5.SSS2 "3.5.2
    Retrieval-based and template-based techniques ‣ 3.5 Report generation ‣ 3 Methods
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data") 节）。此外，大型语言模型的发展使得利用它们来提高生成报告的质量成为可能。这在第 [3.5.3](#S3.SS5.SSS3 "3.5.3 Large
    language model to assist report generation ‣ 3.5 Report generation ‣ 3 Methods
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data") 节中讨论。
- en: 3.5.1 Decoder-based techniques
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1 基于解码器的技术
- en: The decoder decodes the extracted representation of inputs and generates a descriptive
    report. The mainstream architectures include LSTM (Hochreiter and Schmidhuber,
    [1997](#bib.bib29)) and Transformer. Compared to LSTM, the Transformer processes
    the entire sequence simultaneously rather than sequentially. Therefore, the Transformer
    allows for more efficient parallelization during training and can capture long-range
    dependencies. In the 89 reviewed papers, the Transformer tends to replace LSTM.
    Fifty-five works utilized the Transformer as a decoder and only 23 of them utilized
    LSTMs (12 works) or hierarchical LSTMs (11 works). For the papers published in
    2023 and 2024, all encoder-decoder structures used the Transformer as their decoders.
    There are two ways to modify the decoder and improve model performance.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器解码提取的输入表示并生成描述性报告。主流架构包括 LSTM（Hochreiter 和 Schmidhuber，[1997](#bib.bib29)）和
    Transformer。与 LSTM 相比，Transformer 同时处理整个序列而不是按序列处理。因此，Transformer 在训练过程中允许更高效的并行化，并且能够捕捉长期依赖性。在89篇回顾的论文中，Transformer
    趋向于取代 LSTM。55 篇工作使用了 Transformer 作为解码器，而仅有 23 篇使用了 LSTM（12 篇）或层次化 LSTM（11 篇）。在
    2023 年和 2024 年发表的论文中，所有的编码器-解码器结构都使用 Transformer 作为其解码器。修改解码器并提高模型性能有两种方法。
- en: 'Shortcut connections: Connecting different layers in networks can be considered
    as a promising way to enhance the flow of information in both forward and backward
    propagation (Mirikharaji et al., [2023](#bib.bib76)). The U-connection (Huang
    et al., [2023](#bib.bib37)) and meshed connection (Miura et al., [2021](#bib.bib77);
    Lee et al., [2022](#bib.bib53); Cornia et al., [2020](#bib.bib17)) are added between
    encoder and decoder, resulting in a similar performance enhancement (Huang et al.,
    [2023](#bib.bib37)).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 快捷连接：在网络中连接不同的层可以被认为是一种有前景的方法，以增强前向和反向传播中的信息流（Mirikharaji 等，[2023](#bib.bib76)）。U-连接（Huang
    等，[2023](#bib.bib37)）和网状连接（Miura 等，[2021](#bib.bib77)；Lee 等，[2022](#bib.bib53)；Cornia
    等，[2020](#bib.bib17)）被添加在编码器和解码器之间，结果导致了类似的性能提升（Huang 等，[2023](#bib.bib37)）。
- en: 'Memory-driven Transformer: We would like to highlight the Memory-driven Transformer
    (R2Gen) proposed by Chen et al. ([2020](#bib.bib14)). It has been increasingly
    popular in recent years. The R2Gen introduces a memory module and a memory-driven
    conditional layer normalization module into the Transformer decoder architecture.
    The design of the memory module hypothesizes that diverse images exhibit similar
    patterns in their radiological reports, thereby serving as valuable references
    for each other. Building a memory matrix can capture this pattern and transfer
    it during training. Specifically, similar to that in the section [3.3.1](#S3.SS3.SSS1.Px4
    "Memory metric for feature extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3
    Feature learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data"), a matrix is randomly initialized and is updated
    using the gate mechanism based on the matrix from the last step and generated
    reports. The layer normalization is designed to integrate the outputs of the memory
    module into the decoder.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆驱动的 Transformer：我们想要重点介绍陈等人提出的记忆驱动 Transformer（R2Gen）（[2020](#bib.bib14)）。近年来，它越来越受欢迎。R2Gen
    在 Transformer 解码器架构中引入了一个记忆模块和一个记忆驱动的条件层归一化模块。记忆模块的设计假设不同的影像在其放射学报告中展示了类似的模式，从而可以相互作为有价值的参考。建立记忆矩阵可以捕捉这种模式并在训练过程中传递。具体来说，类似于[3.3.1](#S3.SS3.SSS1.Px4
    "Memory metric for feature extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3
    Feature learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")部分中的矩阵，矩阵是随机初始化的，并通过基于上一步矩阵和生成报告的门控机制进行更新。层归一化旨在将记忆模块的输出集成到解码器中。
- en: Eight works directly utilized the R2Gen as their decoder. In addition, the design
    of the memory module and layer normalization inspired subsequent works (Xue et al.,
    [2024](#bib.bib138); Jia et al., [2021](#bib.bib41); Zhang et al., [2023a](#bib.bib148)).
    It is noted that the novel utilization of the memory module by Zhang et al. ([2023a](#bib.bib148))
    integrates ground truth reports into the training process, leading to successful
    outcomes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 八项工作直接使用了 R2Gen 作为其解码器。此外，记忆模块和层归一化的设计启发了后续的工作（Xue 等人，[2024](#bib.bib138)；Jia
    等人，[2021](#bib.bib41)；Zhang 等人，[2023a](#bib.bib148)）。需要注意的是，Zhang 等人（[2023a](#bib.bib148)）对记忆模块的新颖利用将真实报告集成到训练过程中，取得了成功的结果。
- en: 3.5.2 Retrieval-based and template-based techniques
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2 基于检索和模板的技术
- en: Retrieval-based techniques generate reports by selecting existing sentences
    from a large corpus and the selection is typically based on similarity comparison
    (Endo et al., [2021](#bib.bib23); Ramesh et al., [2022](#bib.bib93); Jeong et al.,
    [2024](#bib.bib40)). Initially, text and image encoders are trained using a contrastive
    method, such as the CLIP (Radford et al., [2021](#bib.bib91)). The textual features
    of sentences in a corpus and the visual features of an input image are extracted
    by the encoders. The visual features are then compared with all textual features
    in the corpus. The top k sentences with the maximum similarity score are selected
    for the predicted report. In addition, Jeong et al. ([2024](#bib.bib40)) added
    a multimodal encoder after the retrieval process to calculate the image-text matching
    scores between the input image and the retrieved sentences. A filter was applied
    based on the score to remove entailed or contradicted sentences.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于检索的技术通过从大型语料库中选择现有句子生成报告，这种选择通常基于相似性比较（Endo 等人，[2021](#bib.bib23)；Ramesh 等人，[2022](#bib.bib93)；Jeong
    等人，[2024](#bib.bib40)）。最初，使用对比方法训练文本和图像编码器，例如 CLIP（Radford 等人，[2021](#bib.bib91)）。编码器提取语料库中句子的文本特征和输入图像的视觉特征。然后将视觉特征与语料库中的所有文本特征进行比较。选择相似性分数最高的前
    k 个句子作为预测报告。此外，Jeong 等人（[2024](#bib.bib40)）在检索过程后添加了一个多模态编码器，以计算输入图像与检索句子之间的图像-文本匹配分数。基于分数应用过滤器以删除包含或矛盾的句子。
- en: Other retrieval-based techniques do not follow the above process. Kong et al.
    ([2022](#bib.bib50)) treated the report generation in two steps sentence retrieval
    and selection. They first retrieved a candidate sentence set from the training
    datasets with far more sentences than a standard medical report and then selected
    the sentences by a classifier. Zhang et al. ([2022](#bib.bib151)) proposed a retrieval
    method based on a hashing technique, which mapped multi-modal data with the same
    label into a shared space.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其他基于检索的技术则不遵循上述过程。Kong et al. ([2022](#bib.bib50)) 将报告生成分为两个步骤：句子检索和选择。他们首先从训练数据集中检索出一个候选句子集，这些数据集的句子数量远超标准医疗报告，然后通过分类器选择句子。Zhang
    et al. ([2022](#bib.bib151)) 提出了基于哈希技术的检索方法，该方法将具有相同标签的多模态数据映射到共享空间。
- en: Template-based methods typically start with the diagnosis of diseases, and then
    pre-defined sentences are selected based on the diagnosis results. These selected
    sentences are concatenated to produce reports (Pino et al., [2021](#bib.bib88)).
    Abela et al. ([2022](#bib.bib1)) argued that this method was limited by exact
    labels, therefore they retrieved template sentences by class probabilities and
    different thresholds corresponding to different descriptions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模板的方法通常从疾病诊断开始，然后根据诊断结果选择预定义的句子。这些选定的句子被串联在一起生成报告（Pino et al., [2021](#bib.bib88)）。Abela
    et al. ([2022](#bib.bib1)) 认为这种方法受到精确标签的限制，因此他们通过类别概率和不同的阈值检索模板句子，以对应不同的描述。
- en: 3.5.3 Large language model to assist report generation
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3 大型语言模型辅助报告生成
- en: With the emergence of ChatGPT (OpenAI, [2023](#bib.bib83)), its powerful language
    abilities made researchers eager to harness the power of large language models
    to aid in report generation. However, employing it directly within the medical
    domain led to unsatisfactory outcomes (Tu et al., [2024](#bib.bib116); Yan et al.,
    [2023](#bib.bib141); Sun et al., [2023](#bib.bib108)). Two works (Selivanov et al.,
    [2023](#bib.bib96); Wang et al., [2023a](#bib.bib123)) initially predicted report-related
    information such as diseases, lesion regions, and visual features, and generated
    preliminary reports. Subsequently, they employed pre-trained large language models,
    e.g., ChatGPT (Wang et al., [2023a](#bib.bib123)) or GPT-3 (Brown et al., [2020](#bib.bib8);
    Selivanov et al., [2023](#bib.bib96); Wang et al., [2023a](#bib.bib123)) to improve
    the preliminary report with the predicted information and produced the final reports.
    The results have been moderately improved. Exploration of large language models
    in the field of medical report generation still requires further investigation,
    which is discussed in Section [7.4](#S7.SS4 "7.4 Human-AI interaction ‣ 7 Challenges
    and Future Works ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data").
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 随着ChatGPT（OpenAI, [2023](#bib.bib83)）的出现，其强大的语言能力让研究人员渴望利用大型语言模型来辅助报告生成。然而，直接在医疗领域应用它却导致了不尽如人意的结果（Tu
    et al., [2024](#bib.bib116)；Yan et al., [2023](#bib.bib141)；Sun et al., [2023](#bib.bib108)）。两个研究（Selivanov
    et al., [2023](#bib.bib96)；Wang et al., [2023a](#bib.bib123)）最初预测了与报告相关的信息，如疾病、病变区域和视觉特征，并生成了初步报告。随后，他们使用预训练的大型语言模型，例如ChatGPT（Wang
    et al., [2023a](#bib.bib123)）或GPT-3（Brown et al., [2020](#bib.bib8)；Selivanov
    et al., [2023](#bib.bib96)；Wang et al., [2023a](#bib.bib123)）来改进初步报告，并生成了最终报告。结果有所改善。在医疗报告生成领域，大型语言模型的探索仍需进一步研究，这在第[7.4](#S7.SS4
    "7.4 人工智能交互 ‣ 挑战与未来工作 ‣ 基于深度学习的放射学报告生成使用多模态数据")节中讨论。
- en: 3.6 Training strategy
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 训练策略
- en: Training strategy refers to the techniques used to train neural network models.
    Traditionally, models are trained by minimizing various loss functions. Therefore,
    this section begins by introducing different loss functions (Section [3.6.1](#S3.SS6.SSS1
    "3.6.1 Loss functions ‣ 3.6 Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")), followed by a discussion
    on reinforcement learning (Section [3.6.2](#S3.SS6.SSS2 "3.6.2 Reinforcement learning
    ‣ 3.6 Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data")), and the curriculum learning’s application
    to the report generation task (Section [3.6.3](#S3.SS6.SSS3 "3.6.3 Curriculum
    learning ‣ 3.6 Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 训练策略是指用于训练神经网络模型的技术。传统上，模型通过最小化各种损失函数进行训练。因此，本节首先介绍不同的损失函数（第 [3.6.1](#S3.SS6.SSS1
    "3.6.1 损失函数 ‣ 3.6 训练策略 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述") 节），随后讨论强化学习（第 [3.6.2](#S3.SS6.SSS2
    "3.6.2 强化学习 ‣ 3.6 训练策略 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述") 节），以及课程学习在报告生成任务中的应用（第 [3.6.3](#S3.SS6.SSS3
    "3.6.3 课程学习 ‣ 3.6 训练策略 ‣ 3 方法 ‣ 基于深度学习的放射学报告生成综述") 节）。
- en: 3.6.1 Loss functions
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.1 损失函数
- en: The mainstream loss function for report generation is the cross-entropy loss
    based on the generated sentences and the ground-truth sentences. Cross-entropy
    loss can be re-weighted based on term frequency (Gajbhiye et al., [2022](#bib.bib24)),
    TF-IDF (Wang et al., [2022d](#bib.bib129)) or uncertainty (Wang et al., [2024b](#bib.bib126))
    to mitigate model bias or handle challenging cases. In addition, Pandey et al.
    ([2021](#bib.bib85)) utilized cycle-consistency loss (Zhu et al., [2017](#bib.bib154))
    to generate reports. The core idea is that a report and its corresponding image
    share the same information, hence they can be used to generate each other.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 报告生成的主流损失函数是基于生成句子和真实句子的交叉熵损失。交叉熵损失可以根据词频（Gajbhiye et al., [2022](#bib.bib24)）、TF-IDF（Wang
    et al., [2022d](#bib.bib129)）或不确定性（Wang et al., [2024b](#bib.bib126)）重新加权，以缓解模型偏差或处理挑战性案例。此外，Pandey
    et al. ([2021](#bib.bib85)) 利用循环一致性损失（Zhu et al., [2017](#bib.bib154)）来生成报告。核心思想是报告及其对应的图像共享相同的信息，因此它们可以互相生成。
- en: The application of an auxiliary loss function can provide additional supervision
    signals, further enhancing model performance. Two works Wang et al. ([2021](#bib.bib128));
    Li et al. ([2022a](#bib.bib56)) applied an additional constraint between features
    extracted from the generated and real reports. Zhang et al. ([2023b](#bib.bib149))
    created two different versions of an input image by adding noise and feeding them
    into two networks. An auxiliary loss function ensures the consistency of the outputs
    produced by the two generators. Wang et al. ([2024a](#bib.bib121)) obtained two
    discriminative regions in an image from the generated words and visual classifier
    separately, then enforced the consistency between them.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助损失函数的应用可以提供额外的监督信号，进一步提升模型性能。两项研究，Wang et al. ([2021](#bib.bib128)) 和 Li et
    al. ([2022a](#bib.bib56)) 在从生成报告和真实报告中提取的特征之间施加了额外的约束。Zhang et al. ([2023b](#bib.bib149))
    通过添加噪声创建了输入图像的两个不同版本，并将它们输入到两个网络中。辅助损失函数确保两个生成器产生的输出的一致性。Wang et al. ([2024a](#bib.bib121))
    从生成的词语和视觉分类器分别获得了图像中的两个判别区域，然后强制它们之间的一致性。
- en: 3.6.2 Reinforcement learning
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.2 强化学习
- en: Reinforcement learning involves training an agent to make optimal decisions
    through trial and error, aiming to maximize targeting rewards. It offers a method
    to update the model parameters based on non-differentiable reward functions (Messina
    et al., [2022](#bib.bib75)). Evaluation metrics can be considered as rewards,
    such as CIDEr (Kaur and Mittal, [2022b](#bib.bib47)), BLEU (Qin and Song, [2022](#bib.bib90);
    Gu et al., [2024](#bib.bib25)), METEOR (Qin and Song, [2022](#bib.bib90)), ROUGE
    (Qin and Song, [2022](#bib.bib90)), BERTScore (Miura et al., [2021](#bib.bib77)),
    F1 score (Miura et al., [2021](#bib.bib77)), and accuracy (Hou et al., [2021b](#bib.bib32)).
    In addition, Hou et al. ([2021b](#bib.bib32)) trained a language fluency discriminator
    using the ground truth and generated reports, and then utilized the discriminator
    to provide rewards.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习涉及通过试错训练代理以做出最佳决策，旨在最大化目标奖励。它提供了一种基于非可微分奖励函数更新模型参数的方法 (Messina et al., [2022](#bib.bib75))。评估指标可以视为奖励，例如CIDEr
    (Kaur and Mittal, [2022b](#bib.bib47))，BLEU (Qin and Song, [2022](#bib.bib90);
    Gu et al., [2024](#bib.bib25))，METEOR (Qin and Song, [2022](#bib.bib90))，ROUGE
    (Qin and Song, [2022](#bib.bib90))，BERTScore (Miura et al., [2021](#bib.bib77))，F1分数
    (Miura et al., [2021](#bib.bib77)) 和准确率 (Hou et al., [2021b](#bib.bib32))。此外，Hou
    et al. ([2021b](#bib.bib32)) 使用真实数据和生成报告训练了语言流畅度鉴别器，然后利用该鉴别器提供奖励。
- en: 3.6.3 Curriculum learning
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.6.3 课程学习
- en: Liu et al. ([2021a](#bib.bib68)) utilized curriculum learning (Platanios et al.,
    [2019](#bib.bib89)) to classify training instances and trained a model from simple
    to complex samples. Data pairs were evaluated based on image heuristics, image
    confidence, text heuristics, and text confidence. Image heuristic evaluated the
    similarity between input images and normal images. Image confidence indicated
    the confidence of a classification model. The report heuristic was related to
    the number of abnormal sentences, and report confidence was evaluated by the negative
    log-likelihood loss (Xu et al., [2020](#bib.bib136)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Liu et al. ([2021a](#bib.bib68)) 利用课程学习 (Platanios et al., [2019](#bib.bib89))
    对训练实例进行分类，并从简单样本到复杂样本训练模型。数据对基于图像启发式、图像置信度、文本启发式和文本置信度进行评估。图像启发式评估输入图像与正常图像的相似度。图像置信度指示分类模型的置信度。报告启发式与异常句子的数量相关，报告置信度通过负对数似然损失进行评估
    (Xu et al., [2020](#bib.bib136))。
- en: 4 Datasets
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 数据集
- en: Datasets play a crucial role in the development of report-generation models.
    Abundant and diverse training data can improve the model’s accuracy and generalizability.
    Moreover, a suitable test dataset makes it realistic to test the model’s performance
    in a practical scenario. In this section, we selected 11 public medical image-report
    datasets utilized in the reviewed articles to provide a comprehensive introduction
    of popular and newly collected datasets, see Table [1](#S4.T1 "Table 1 ‣ 4 Datasets
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data"). The usage of datasets in each article is shown in Table LABEL:tab:overall
    in Appendix A. The datasets primarily focus on the lung, including X-ray (Demner-Fushman
    et al., [2016](#bib.bib21); Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45))
    and CT (Li et al., [2020](#bib.bib57); Liu et al., [2021e](#bib.bib72)). In addition,
    there are also publicly available datasets on eye scans (Huang et al., [2021b](#bib.bib35);
    Lin et al., [2023](#bib.bib66); Li et al., [2021](#bib.bib58)) and breast scans
    (Yang et al., [2021a](#bib.bib142)). The concentration of medical report generation
    efforts on chest X-rays can be attributed, in part, to the accessibility of large-scale
    publicly available datasets.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在报告生成模型的开发中发挥着至关重要的作用。丰富且多样的训练数据可以提高模型的准确性和泛化能力。此外，合适的测试数据集使得在实际场景中测试模型性能成为可能。在本节中，我们选择了11个公共医学图像报告数据集，这些数据集在被审阅的文章中得到了应用，以提供对流行和新收集数据集的全面介绍，见表
    [1](#S4.T1 "Table 1 ‣ 4 Datasets ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")。每篇文章中数据集的使用情况显示在附录A中的表LABEL:tab:overall。数据集主要集中在肺部，包括X光（Demner-Fushman
    et al., [2016](#bib.bib21); Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45)）和CT（Li
    et al., [2020](#bib.bib57); Liu et al., [2021e](#bib.bib72)）。此外，还有关于眼部扫描（Huang
    et al., [2021b](#bib.bib35); Lin et al., [2023](#bib.bib66); Li et al., [2021](#bib.bib58)）和乳腺扫描（Yang
    et al., [2021a](#bib.bib142)）的公开数据集。医学报告生成工作集中在胸部X光的部分原因是大型公开数据集的可获得性。
- en: '![Refer to caption](img/59cb9a9e5cc1bd3e2b1d31e803f2efeb.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/59cb9a9e5cc1bd3e2b1d31e803f2efeb.png)'
- en: 'Figure 4: A sample from the MIMIC-CXR dataset. The left is the Chest X-ray
    image and the right is its corresponding radiology report.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：来自MIMIC-CXR数据集的示例。左侧是胸部X光图像，右侧是其对应的放射学报告。
- en: The most popular datasets are the Indiana University Chest X-Ray Collection
    (IU X-Ray) (Demner-Fushman et al., [2016](#bib.bib21)) and the MIMIC Chest X-ray
    (MIMIC-CXR) (Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45)). The IU X-ray
    contains 7470 images of frontal and lateral X-rays and 3955 reports with manual
    annotation based on the MeSH codes³³3https://www.nlm.nih.gov/mesh/meshhome.html
    and the RadLex codes. These codes encompass standard medical terminology, such
    as anatomical structures, diseases, pathological signs, foreign objects, and attributes,
    as defined by authoritative institutions. For deep learning methods, the size
    of the IU X-RAY is insufficient, while the collection of MIMIC-CXR alleviates
    this problem (Messina et al., [2022](#bib.bib75)). It consists of 377,110 images
    and 227,827 reports, and has been multi-labeled by automatic tools according to
    14 disease categories. An image-report sample from the MIMIC-CXR dataset is shown
    in the Figure [4](#S4.F4 "Figure 4 ‣ 4 Datasets ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"). The indication, technique,
    and comparison provide fundamental information for a Chest X-ray test. The automation
    of radiology report generation targets the findings and impressions sections.
    The findings section provides a detailed description of the entire image, while
    the impressions section summarizes these observations. The follow-up work provides
    MIMIC-CXR with richer label information. The Chest ImaGenome dataset (Wu et al.,
    [2021](#bib.bib132)) is based on the anteroposterior and posteroanterior view
    Chest X-ray images in the MIMIC-CXR dataset. It provides an anatomy-centered scene
    graph for each image, including anatomical location and relation annotations.
    The annotations are generated through two automated pipelines, and a separate
    dataset of 500 manually annotated cases is also provided for testing.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的数据集是印第安纳大学胸部X光数据集（IU X-Ray）（Demner-Fushman et al., [2016](#bib.bib21)）和MIMIC胸部X光数据集（MIMIC-CXR）（Johnson
    et al., [2019a](#bib.bib44)，[b](#bib.bib45)）。IU X-Ray包含7470张正面和侧面X光图像及3955份基于MeSH代码³³3https://www.nlm.nih.gov/mesh/meshhome.html和RadLex代码的手动注释报告。这些代码涵盖了标准的医学术语，如解剖结构、疾病、病理体征、异物和属性，由权威机构定义。对于深度学习方法而言，IU
    X-RAY的规模不足，而MIMIC-CXR的收集缓解了这一问题（Messina et al., [2022](#bib.bib75)）。它包含377,110张图像和227,827份报告，并通过自动工具根据14种疾病类别进行了多标签标注。图像-报告样本来自MIMIC-CXR数据集，如图[4](#S4.F4
    "Figure 4 ‣ 4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")所示。指示、技术和比较提供了胸部X光检查的基本信息。放射学报告生成的自动化针对发现和印象部分。发现部分提供了对整个图像的详细描述，而印象部分总结了这些观察结果。后续工作为MIMIC-CXR提供了更丰富的标签信息。Chest
    ImaGenome数据集（Wu et al., [2021](#bib.bib132)）基于MIMIC-CXR数据集中的前后位和后前位胸部X光图像。它为每个图像提供了以解剖学为中心的场景图，包括解剖位置和关系注释。这些注释通过两个自动化管道生成，并且还提供了一个500个手动标注案例的单独数据集用于测试。
- en: Increasing attention has been paid to the collection of ophthalmic image-report
    pairs (Huang et al., [2021b](#bib.bib35); Li et al., [2021](#bib.bib58); Lin et al.,
    [2023](#bib.bib66)). Ophthalmic image has lots of modalities, such as Fluorescein
    Angiography (FA), Fundus Fluorescein Angiography (FFA), Color Fundus photography
    (CFP), fundus photograph (FP), optical coherence tomography (OCT), fundus autofluorescence
    (FAF), Indocyanine Green Chorioangiography (ICG), and red-free filtered fundus
    images. Most of retinal datasets are based on one or two modalities and the last
    released Retina ImBank dataset is the first multi-modality retinal image-text
    dataset (Lin et al., [2023](#bib.bib66)). Some datasets provide additional labels
    for each image, such as lesion boundary (Li et al., [2021](#bib.bib58)), lesion
    category (Huang et al., [2021b](#bib.bib35); Li et al., [2021](#bib.bib58)), and
    keywords (Huang et al., [2021b](#bib.bib35)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对眼科图像-报告对的收集越来越受到关注（Huang et al., [2021b](#bib.bib35)；Li et al., [2021](#bib.bib58)；Lin
    et al., [2023](#bib.bib66)）。眼科图像有很多模态，例如荧光素眼底血管造影（FA）、眼底荧光素血管造影（FFA）、彩色眼底摄影（CFP）、眼底照片（FP）、光学相干断层扫描（OCT）、眼底自发荧光（FAF）、吲哚青绿染料脉络膜造影（ICG）和去红滤光眼底图像。大多数视网膜数据集基于一种或两种模态，而最新发布的Retina
    ImBank数据集是第一个多模态视网膜图像-文本数据集（Lin et al., [2023](#bib.bib66)）。一些数据集为每个图像提供额外的标签，例如病灶边界（Li
    et al., [2021](#bib.bib58)）、病灶类别（Huang et al., [2021b](#bib.bib35)；Li et al.,
    [2021](#bib.bib58)）和关键词（Huang et al., [2021b](#bib.bib35)）。
- en: All the above datasets provide unstructured reports. Recently, Pellegrini et al.
    ([2023](#bib.bib87)) released a structured report dataset named Rad-ReStruct based
    on the IU-XRAY. In clinical practice, generating structured reports typically
    requires doctors to answer a sequence of questions (Pellegrini et al., [2023](#bib.bib87)).
    Therefore, Pellegrini et al. ([2023](#bib.bib87)) designed a structured report
    template with a series of single- or multi-choice questions based on topic existence
    (e.g., Are there any diseases in the lung?), element existence (e.g., Is there
    an opacity in the lung?), and attributes (e.g., What is the degree?). They then
    integrated the IU-XRAY report data into the template using its annotated MeSH
    and RadLex codes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 上述所有数据集都提供了非结构化报告。最近，Pellegrini 等人（[2023](#bib.bib87)）基于 IU-XRAY 发布了一个名为 Rad-ReStruct
    的结构化报告数据集。在临床实践中，生成结构化报告通常需要医生回答一系列问题（Pellegrini 等人，[2023](#bib.bib87)）。因此，Pellegrini
    等人（[2023](#bib.bib87)）设计了一个结构化报告模板，其中包含一系列基于主题存在（例如，肺部是否有疾病？）、元素存在（例如，肺部是否有不透明区域？）和属性（例如，程度是什么？）的单选或多选问题。他们随后使用
    IU-XRAY 报告数据及其标注的 MeSH 和 RadLex 代码将这些数据整合到模板中。
- en: 'Table 1: Public datasets for medical report generation.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：用于医学报告生成的公共数据集。
- en: '| Name | Image type | Report type | Images | Reports | Patients | Used by |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 图像类型 | 报告类型 | 图像数量 | 报告数量 | 患者数量 | 使用者 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| IU X-Ray (Demner-Fushman et al., [2016](#bib.bib21)) | Chest X-ray | Unstructured
    | 7,470 | 3,955 | 3,955 | 67 works |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| IU X-Ray（Demner-Fushman 等，[2016](#bib.bib21)） | 胸部 X 光 | 非结构化 | 7,470 | 3,955
    | 3,955 | 67 项工作 |'
- en: '| MIMIC-CXR (Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45)) | Chest
    X-ray | Unstructured | 377,110 | 227,827 | 65,379 | 59 works |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| MIMIC-CXR（Johnson 等，[2019a](#bib.bib44)，[b](#bib.bib45)） | 胸部 X 光 | 非结构化
    | 377,110 | 227,827 | 65,379 | 59 项工作 |'
- en: '| Chest ImaGenome (Wu et al., [2021](#bib.bib132)) | Chest X-ray | Unstructured
    | 242,072 | 217,013 | – | 4 works |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Chest ImaGenome（Wu 等，[2021](#bib.bib132)） | 胸部 X 光 | 非结构化 | 242,072 | 217,013
    | – | 4 项工作 |'
- en: '| COV-CTR (Li et al., [2020](#bib.bib57)) | Chest CT | Unstructured | 728 |
    728 | – | 4 works |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| COV-CTR（Li 等，[2020](#bib.bib57)） | 胸部 CT | 非结构化 | 728 | 728 | – | 4 项工作 |'
- en: '| DeepEyeNet (Huang et al., [2021b](#bib.bib35)) | FA, CFP | Unstructured |
    15,709 | 15,709 | – | 3 works |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| DeepEyeNet（Huang 等，[2021b](#bib.bib35)） | FA、CFP | 非结构化 | 15,709 | 15,709
    | – | 3 项工作 |'
- en: '| FFA-IR (Li et al., [2021](#bib.bib58)) | FFA | Unstructured | 1,048,584 |
    10,790 | – | 2 works |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| FFA-IR（Li 等，[2021](#bib.bib58)） | FFA | 非结构化 | 1,048,584 | 10,790 | – | 2
    项工作 |'
- en: '| Chinese COVID-19 CT (Liu et al., [2021e](#bib.bib72)) | Chest CT | Unstructured
    | 1,104 | 368 | 96 | 1 work |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Chinese COVID-19 CT（Liu 等，[2021e](#bib.bib72)） | 胸部 CT | 非结构化 | 1,104 | 368
    | 96 | 1 项工作 |'
- en: '| BCD2018 (Yang et al., [2021a](#bib.bib142)) | Breast Ultrasound | Unstructured
    | 5,349 | 5,349 | – | 1 work |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| BCD2018（Yang 等，[2021a](#bib.bib142)） | 乳腺超声 | 非结构化 | 5,349 | 5,349 | – |
    1 项工作 |'
- en: '| Retina ImBank (Lin et al., [2023](#bib.bib66)) | FP, OCT, FFA, FAF, ICG,
    and red-free filtered fundus images | Unstructured | 18,788 | 18,788 | – | 1 work
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Retina ImBank（Lin 等，[2023](#bib.bib66)） | FP、OCT、FFA、FAF、ICG 和红光滤波眼底图像 |
    非结构化 | 18,788 | 18,788 | – | 1 项工作 |'
- en: '| Retina Chinese (Lin et al., [2023](#bib.bib66)) | FP, FFA, and ICG | Unstructured
    | 57,498 | 57,498 | – | 1 work |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Retina Chinese（Lin 等，[2023](#bib.bib66)） | FP、FFA 和 ICG | 非结构化 | 57,498 |
    57,498 | – | 1 项工作 |'
- en: '| Rad-ReStruct (Pellegrini et al., [2023](#bib.bib87)) | Chest X-ray | Structured
    | 3,720 | 3,597 | 3,597 | 1 work |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Rad-ReStruct（Pellegrini 等，[2023](#bib.bib87)） | 胸部 X 光 | 结构化 | 3,720 | 3,597
    | 3,597 | 1 项工作 |'
- en: 5 Evaluation
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: Accurate assessment of the quality of generated reports is crucial for measuring
    model performance. The quality of generated reports can be evaluated both quantitatively
    and qualitatively. Quantitative methods check the text quality and medical correctness
    of the generated report by natural language evaluation metrics (Section [5.1](#S5.SS1
    "5.1 Natural language-based evaluation metrics ‣ 5 Evaluation ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")) and medical
    correctness metrics (Section [5.2](#S5.SS2 "5.2 Medical correctness metrics ‣
    5 Evaluation ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")), respectively. Qualitative evaluation is normally performed
    by human experts and they provide overall evaluation for the generated reports
    (Section [5.3](#S5.SS3 "5.3 Human-based evaluation ‣ 5 Evaluation ‣ A Survey of
    Deep Learning-based Radiology Report Generation Using Multimodal Data")). Table
    [2](#S5.T2 "Table 2 ‣ 5 Evaluation ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data") summarizes the description of evaluation
    methods introduced in this section. The usage of evaluation methods in each article
    is shown in Table LABEL:tab:overall in Appendix A.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 准确评估生成报告的质量对于衡量模型性能至关重要。生成报告的质量可以通过定量和定性方法进行评估。定量方法通过自然语言评价指标（第[5.1节](#S5.SS1
    "5.1 Natural language-based evaluation metrics ‣ 5 Evaluation ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")）和医疗正确性指标（第[5.2节](#S5.SS2
    "5.2 Medical correctness metrics ‣ 5 Evaluation ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")）分别检查生成报告的文本质量和医疗正确性。定性评估通常由人工专家进行，他们提供对生成报告的整体评估（第[5.3节](#S5.SS3
    "5.3 Human-based evaluation ‣ 5 Evaluation ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data")）。表[2](#S5.T2 "Table 2 ‣ 5 Evaluation
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data")总结了本节介绍的评估方法的描述。每篇文章中评估方法的使用情况见附录A中的表LABEL:tab:overall。
- en: 'Table 2: The quantitative and qualitative evaluation methods for medical report
    generation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：医疗报告生成的定量和定性评估方法。
- en: '| Metrics | Description | Used by |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 描述 | 使用者 |'
- en: '| Natural language-based metrics |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 基于自然语言的指标 |'
- en: '| BlEU (Papineni et al., [2002](#bib.bib86)) | A precision-based metric that
    measures the n-gram overlapping of the generated text and ground truth text. |
    84 works |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| BlEU (Papineni et al., [2002](#bib.bib86)) | 一种基于精确度的指标，用于测量生成文本和真实文本的n-gram重叠度。
    | 84篇文献 |'
- en: '| ROUGE-L (Lin, [2004](#bib.bib65)) | A F1-like metric that computes a weighted
    harmonic mean of precision and recall based on the longest common subsequence.
    | 75 works |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L (Lin, [2004](#bib.bib65)) | 一种类似F1的指标，基于最长公共子序列计算精确度和召回率的加权调和均值。
    | 75篇文献 |'
- en: '| METEOR (Banerjee and Lavie, [2005](#bib.bib6)) | A F1-like metric that computes
    a weighted harmonic mean of unigram precision and recall. It is an extension of
    BLEU-1. | 57 works |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| METEOR (Banerjee and Lavie, [2005](#bib.bib6)) | 一种类似F1的指标，计算unigram精确度和召回率的加权调和均值。它是BLEU-1的扩展。
    | 57篇文献 |'
- en: '| CIDEr (Vedantam et al., [2015](#bib.bib119)) | The cosine similarity between
    generated text and ground truth text based on the TF-IDF. | 41 works |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| CIDEr (Vedantam et al., [2015](#bib.bib119)) | 基于TF-IDF计算生成文本和真实文本之间的余弦相似度。
    | 41篇文献 |'
- en: '| $S_{emb}$ (Endo et al., [2021](#bib.bib23)) | Sending ground truth report
    and generated report into a textual feature extractor and calculating the cosine
    similarity between their embeddings from the last layer. | 2 works |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $S_{emb}$ (Endo et al., [2021](#bib.bib23)) | 将真实报告和生成报告输入文本特征提取器，计算它们从最后一层提取的嵌入之间的余弦相似度。
    | 2篇文献 |'
- en: '| %Novel (Van Miltenburg et al., [2018](#bib.bib117)) | The percentage of generated
    descriptions that are not present in the training data. | 1 work |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| %Novel (Van Miltenburg et al., [2018](#bib.bib117)) | 生成描述中不出现在训练数据中的百分比。
    | 1篇文献 |'
- en: '| Medical correctness-based metrics |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 基于医疗正确性的指标 |'
- en: '| Clinical Efficacy (Chen et al., [2020](#bib.bib14); Liu et al., [2019](#bib.bib73))
    | Calculate accuracy, precision, recall, and F1 score based on observations extracted
    from reference reports and generated reports by automated system. | 34 works |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 临床效果 (Chen et al., [2020](#bib.bib14); Liu et al., [2019](#bib.bib73)) |
    基于从参考报告和自动系统生成报告中提取的观察值计算准确性、精确度、召回率和F1分数。 | 34篇文献 |'
- en: '| MIRQI (Zhang et al., [2020](#bib.bib150)) | Calculate precision, recall,
    and F1 score based on graph comparison. The ground truth and generated reports
    are automatically analysed to construct a sub-graph from a defined abnormality
    graph. | 2 work |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| MIRQI (Zhang et al., [2020](#bib.bib150)) | 基于图比较计算精确度、召回率和F1分数。地面真实情况和生成的报告被自动分析以从定义的异常图中构建子图。
    | 2 项工作 |'
- en: '| nKTD (Zhou et al., [2021](#bib.bib153)) | Calculate the Hamming distance
    based on observations extracted from reference reports and generated reports by
    the CheXpert Labeller. | 1 work |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| nKTD (Zhou et al., [2021](#bib.bib153)) | 根据从参考报告和由CheXpert Labeller生成的报告中提取的观察数据计算汉明距离。
    | 1 项工作 |'
- en: '| Human-based evaluation |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 基于人工的评估 |'
- en: '| Comparison | Generate reports by two different models and allow senior radiologists
    to find which report is better. | 12 works |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 比较 | 生成两种不同模型的报告，并允许资深放射科医生找出哪一份报告更好。 | 12 项工作 |'
- en: '| Classification | Radiologists categorize the produced reports as accurate,
    missing details, and false reports. | 1 work |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 放射科医生将生成的报告分类为准确、遗漏细节和错误报告。 | 1 项工作 |'
- en: '| Error scoring | Radiologists assess the error severity of baseline, model
    generated, and reference reports. | 1 work |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 错误评分 | 放射科医生评估基准、模型生成和参考报告的错误严重性。 | 1 项工作 |'
- en: '| Grading | Radiologists need to assign a 5-point scale grade to two types
    of generated reports, in accordance with clinical standards. | 1 work |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 评分 | 放射科医生需要根据临床标准对两种类型的生成报告分配5分制的评分。 | 1 项工作 |'
- en: 5.1 Natural language-based evaluation metrics
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 基于自然语言的评估指标
- en: Natural language evaluation metrics are from natural language processing tasks
    and measure the general text quality of generated reports. In the reviewed papers,
    the most popular metrics are BLEU (Papineni et al., [2002](#bib.bib86)), ROUGE-L
    (Lin, [2004](#bib.bib65)), METEOR (Banerjee and Lavie, [2005](#bib.bib6)), and
    CIDEr (Vedantam et al., [2015](#bib.bib119)), which are based on n-gram matching
    between reference reports and generated reports. The model is deemed superior
    with an increased number of matches. Among them, the BLEU is the earliest and
    proposed a modified precision method. When evaluating the quality of radiology
    report generation, we typically opt for BLEU-1, BLEU-2, BLEU-3, and BLEU-4 metrics.
    The n in BLEU-n means the calculation is based on n-gram. The METEOR is an extension
    of BLEU-1 and introduces recall into evaluation. The ROUGE-L also considers precision
    and recall based on the longest common subsequence between reference and generated
    text. The CIDEr adopts the TF-IDF. The TF-IDF vectors weigh each n-gram in a sentence,
    and then the cosine similarity is calculated between the TF-IDF vectors of reference
    and generated text. When the model consistently produces the most common sentences,
    it can achieve notable BLEU scores. However, CIDEr can evaluate generated outputs
    by encouraging the appearance of important terms and punishing high-frequency
    vocabulary (Li et al., [2023a](#bib.bib60)). CIDEr has a popular variant CIDEr-D,
    which introduces penalties to generate desired sentence length and remove stemming
    to ensure the proper usage of word forms.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言评估指标来源于自然语言处理任务，用于衡量生成报告的一般文本质量。在审查的论文中，最流行的指标是 BLEU (Papineni et al., [2002](#bib.bib86))、ROUGE-L
    (Lin, [2004](#bib.bib65))、METEOR (Banerjee and Lavie, [2005](#bib.bib6)) 和 CIDEr
    (Vedantam et al., [2015](#bib.bib119))，这些指标基于参考报告和生成报告之间的n-gram匹配。模型的优越性通过匹配次数的增加来体现。其中，BLEU是最早提出的，提出了一种修改后的精确度方法。在评估放射科报告生成质量时，我们通常选择
    BLEU-1、BLEU-2、BLEU-3 和 BLEU-4 指标。BLEU-n中的n表示计算是基于n-gram的。METEOR是对BLEU-1的扩展，并引入了召回率进行评估。ROUGE-L还考虑了基于参考文本和生成文本之间最长公共子序列的精确度和召回率。CIDEr采用TF-IDF。TF-IDF向量对句子中的每个n-gram进行加权，然后计算参考文本和生成文本之间的TF-IDF向量的余弦相似度。当模型持续生成最常见的句子时，可以获得显著的BLEU分数。然而，CIDEr通过鼓励重要术语的出现并惩罚高频词汇来评估生成的输出（Li
    et al., [2023a](#bib.bib60)）。CIDEr还有一个流行的变体CIDEr-D，它引入了惩罚机制以生成所需的句子长度，并去除词干化以确保词形的正确使用。
- en: Other than n-gram matching, Endo et al. ([2021](#bib.bib23)) proposed a new
    metric $S_{emb}$. A pre-trained feature extractor is applied on both the ground
    truth and the generated reports, and the cosine similarity between extracted embeddings
    is calculated. This approach is used to assess whether the semantic information
    contained in two sentences is consistent. Another natural language metric %Novel
    (Van Miltenburg et al., [2018](#bib.bib117)) is introduced to evaluate the diversity
    in image captioning.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 除了n-gram匹配，Endo等人（[2021](#bib.bib23)）提出了一种新度量$S_{emb}$。在真实数据和生成报告上都应用了预训练的特征提取器，并计算提取的嵌入之间的余弦相似度。该方法用于评估两句话中包含的语义信息是否一致。另一个自然语言度量%Novel（Van
    Miltenburg等人，[2018](#bib.bib117)）被引入以评估图像描述中的多样性。
- en: 5.2 Medical correctness metrics
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 医学准确性度量
- en: Natural language evaluation metrics evaluate the similarity between produced
    reports and the ground truth, but cannot accurately measure whether the generated
    reports contain the required medical facts (Babar et al., [2021b](#bib.bib5);
    Messina et al., [2022](#bib.bib75)). So medical correctness metrics are proposed
    to pay attention to the prediction of important medical facts. Generally, an automatic
    labeller is applied to extract medical facts from generated and reference reports.
    Then different metrics are applied to these. The mainstream metric is clinical
    efficacy (Liu et al., [2019](#bib.bib73); Chen et al., [2020](#bib.bib14)), which
    initially calculates precision, recall, and F1 score, and subsequently extends
    to accuracy (Babar et al., [2021a](#bib.bib4); Miura et al., [2021](#bib.bib77);
    Moon et al., [2022](#bib.bib79); Yan, [2022](#bib.bib140); Yang et al., [2022](#bib.bib143);
    Selivanov et al., [2023](#bib.bib96); Yang et al., [2023](#bib.bib144)) and AUC
    (Li et al., [2023a](#bib.bib60)). Most of the works utilized CheXpert (Irvin et al.,
    [2019](#bib.bib38)) as a labeler to extract chest diseases information from ground
    truth reports, while seven of them (Miura et al., [2021](#bib.bib77); Yan, [2022](#bib.bib140);
    Wang et al., [2023a](#bib.bib123), [2024a](#bib.bib121); Dalla Serra et al., [2023b](#bib.bib20),
    [a](#bib.bib19); Jin et al., [2024](#bib.bib43)) utilized a newer labeler CheXbert
    (Smit et al., [2020](#bib.bib103)), which has a higher performance. In addition,
    Pellegrini et al. ([2023](#bib.bib87)) generated structured reports by predicting
    a series of questions. They utilized macro precision, recall, and F1 score to
    assess all questions, along with evaluating report-level accuracy. Other metrics
    calculate the Hamming distance (Zhou et al., [2021](#bib.bib153)) or perform graph
    comparison (Zhang et al., [2020](#bib.bib150)) based on the extracted results.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言评估度量评估生成报告与真实数据之间的相似性，但无法准确衡量生成的报告是否包含所需的医学事实（Babar等人，[2021b](#bib.bib5)；Messina等人，[2022](#bib.bib75)）。因此，提出了医学准确性度量，以关注重要医学事实的预测。通常，应用自动标注器从生成报告和参考报告中提取医学事实。然后将不同的度量应用于这些事实。主流度量是临床效果（Liu等人，[2019](#bib.bib73)；Chen等人，[2020](#bib.bib14)），最初计算精确度、召回率和F1分数，随后扩展到准确率（Babar等人，[2021a](#bib.bib4)；Miura等人，[2021](#bib.bib77)；Moon等人，[2022](#bib.bib79)；Yan，[2022](#bib.bib140)；Yang等人，[2022](#bib.bib143)；Selivanov等人，[2023](#bib.bib96)；Yang等人，[2023](#bib.bib144)）和AUC（Li等人，[2023a](#bib.bib60)）。大多数工作利用CheXpert（Irvin等人，[2019](#bib.bib38)）作为标注器从真实数据报告中提取胸部疾病信息，而其中七个（Miura等人，[2021](#bib.bib77)；Yan，[2022](#bib.bib140)；Wang等人，[2023a](#bib.bib123)，[2024a](#bib.bib121)；Dalla
    Serra等人，[2023b](#bib.bib20)，[a](#bib.bib19)；Jin等人，[2024](#bib.bib43)）使用了性能更高的新标注器CheXbert（Smit等人，[2020](#bib.bib103)）。此外，Pellegrini等人（[2023](#bib.bib87)）通过预测一系列问题生成了结构化报告。他们利用宏观精确度、召回率和F1分数来评估所有问题，同时评估报告级别的准确性。其他度量计算汉明距离（Zhou等人，[2021](#bib.bib153)）或基于提取结果进行图形比较（Zhang等人，[2020](#bib.bib150)）。
- en: 5.3 Human-based evaluation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 基于人工的评估
- en: For qualitative assessment, the most common human-based evaluation method is
    comparison. In general, a set number of samples (i.e., 100/200/300) are selected
    from the test dataset and subsequently processed by different generated models.
    More than one professional clinician is responsible to compare and sort the generated
    reports. Five works utilized ground truth reports in this process. Two works (Miura
    et al., [2021](#bib.bib77); Cao et al., [2022](#bib.bib9)) considered the ground
    truth reports as a reference. Reports were generated by different generators and
    the radiologists need to select which report is more similar to the reference.
    Dalla Serra et al. ([2022](#bib.bib18)) allowed experts to find 5 types of errors
    (i.e., hallucination, omission, attribute error, impression error, and grammatical
    error) in different generated reports according to the reference reports. Xu et al.
    ([2023](#bib.bib137)) asked radiologists to rank the ground truth reports and
    the generated reports. Qin and Song ([2022](#bib.bib90)) invited experts to select
    the most suitable report from the generated and the ground truth reports according
    to correctness, language fluency, and content coverage. Other expert evaluation
    methods are classification (Alfarghaly et al., [2021](#bib.bib3)), grading (Wang
    et al., [2024b](#bib.bib126)), and error scoring (Jeong et al., [2024](#bib.bib40)).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定性评估，最常见的基于人工的方法是比较。通常，从测试数据集中选择一定数量的样本（即100/200/300），然后由不同生成的模型处理。这些生成的报告由多位专业临床医生负责比较和排序。有五项工作在这一过程中使用了真实报告。两项工作（Miura
    et al., [2021](#bib.bib77); Cao et al., [2022](#bib.bib9)）将真实报告视为参考。报告由不同的生成器生成，放射科医生需要选择哪个报告更接近参考报告。Dalla
    Serra et al. ([2022](#bib.bib18)) 允许专家根据参考报告在不同生成的报告中查找5种错误（即幻觉、遗漏、属性错误、印象错误和语法错误）。Xu
    et al. ([2023](#bib.bib137)) 要求放射科医生对真实报告和生成报告进行排名。Qin 和 Song ([2022](#bib.bib90))
    邀请专家根据准确性、语言流畅性和内容覆盖度，从生成的报告和真实报告中选择最合适的报告。其他专家评估方法包括分类（Alfarghaly et al., [2021](#bib.bib3)）、评分（Wang
    et al., [2024b](#bib.bib126)）和错误评分（Jeong et al., [2024](#bib.bib40)）。
- en: 6 Benchmark Comparison
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基准比较
- en: For model comparison, it is essential to select a benchmark for an impartial
    evaluation. We choose the MIMIC-CXR (Johnson et al., [2019b](#bib.bib45), [a](#bib.bib44))(see
    Table [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")) dataset as a benchmark to
    compare the model performance for two reasons. First, according to Table [1](#S4.T1
    "Table 1 ‣ 4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data"), IU-Xray and MIMIC-CXR are the most popular datasets,
    but IU X-ray lacks standard training-validation-test splits, leading to less comparable
    results (Messina et al., [2022](#bib.bib75)), while MIMIC-CXR has official training-validation-test
    splits. Second, MIMIC-CXR is the largest image-report dataset, which provides
    a broader distribution of data, facilitating testing across diverse scenarios,
    and reducing biases commonly encountered in smaller datasets.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型比较，选择一个基准进行公正评估是至关重要的。我们选择 MIMIC-CXR (Johnson et al., [2019b](#bib.bib45),
    [a](#bib.bib44))（见表 [3](#S6.T3 "表 3 ‣ 6 基准比较 ‣ 基于深度学习的放射学报告生成的多模态数据调查")）数据集作为基准，比较模型性能，原因有两个。首先，根据表
    [1](#S4.T1 "表 1 ‣ 4 数据集 ‣ 基于深度学习的放射学报告生成的多模态数据调查")，IU-Xray 和 MIMIC-CXR 是最受欢迎的数据集，但
    IU X-ray 缺乏标准的训练-验证-测试划分，导致结果可比性较差（Messina et al., [2022](#bib.bib75)），而 MIMIC-CXR
    具有官方的训练-验证-测试划分。其次，MIMIC-CXR 是最大的图像-报告数据集，提供了更广泛的数据分布，便于在不同场景下进行测试，并减少了小数据集中常见的偏差。
- en: We endeavour to ensure equitable comparisons, but it’s important to consider
    the following three problems when analysing these results. First, we select the
    methods leveraged the official splits of the MIMIC-CXR dataset (Johnson et al.,
    [2019b](#bib.bib45)), containing 368960 images (with 222758 reports) in the training
    dataset, 2991 images (with 1808 reports) in the validate dataset, and 5159 images
    (with 3269 reports) in the test dataset. Although some papers claimed to use official
    splits, the total number of datasets is different. For example, the MIMIC-CXR
    used by Najdenkoska et al. ([2022](#bib.bib81)) contains 218,101 samples. These
    papers were excluded. In Section [3](#S3 "3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"), a multitude of methodologies
    are highlighted. However, due to stringent screening, their absence from the comparison
    table does not imply inferior performance. Second, we choose BLEU, ROUGE, METEOR,
    CIDEr-D, precision, recall, and F1 score metrics as evaluation metric. Similar
    to the previous survey (Messina et al., [2022](#bib.bib75)), we found that although
    some metrics have variants, many papers do not specify the particular version
    used. In that case, we assume they are consistent. Third, the generated report
    sections vary among different methodologies. Most articles do not explicitly specify
    the generated report sections, making it challenging to conduct comparative analysis.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们努力确保公平的比较，但在分析这些结果时，需要考虑以下三个问题。首先，我们选择了利用MIMIC-CXR数据集（Johnson et al., [2019b](#bib.bib45)）官方划分的方法，该数据集在训练集包含368960张图像（有222758份报告），在验证集包含2991张图像（有1808份报告），在测试集包含5159张图像（有3269份报告）。尽管有些论文声称使用了官方划分，但数据集的总数却不同。例如，Najdenkoska
    et al. ([2022](#bib.bib81))使用的MIMIC-CXR数据集包含218,101个样本。这些论文被排除在外。在第[3](#S3 "3
    Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data")节中，突出了多种方法。然而，由于严格筛选，它们在比较表中的缺席并不意味着表现较差。其次，我们选择了BLEU、ROUGE、METEOR、CIDEr-D、精确度、召回率和F1分数作为评估指标。与之前的调查（Messina
    et al., [2022](#bib.bib75)）类似，我们发现尽管一些指标有变体，但许多论文没有说明所使用的具体版本。在这种情况下，我们假设它们是一致的。第三，生成的报告部分在不同的方法中有所不同。大多数文章没有明确指定生成的报告部分，使得进行比较分析变得困难。
- en: We separate the comparison results based on the generated report section, and
    the completed comparison tables are provided in Table [A2](#Sx1.T2 "Table A2 ‣
    Appendix A ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") in Appendix A. Table [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data") shows the best and second best performances ranked for each evaluation
    metric. Among the 14 papers in Table [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data"), three of them (Song et al., [2022](#bib.bib104); Liu et al., [2023b](#bib.bib74);
    Jin et al., [2024](#bib.bib43)) utilized multi-modality inputs. From an architectural
    perspective, in the encoding stage, five works utilized a pure Transformer model
    (Wang et al., [2022e](#bib.bib130); Kong et al., [2022](#bib.bib50); Liu et al.,
    [2023b](#bib.bib74); Wang et al., [2023d](#bib.bib131), [2022d](#bib.bib129)),
    another five works employed the CNN-based model (Pino et al., [2021](#bib.bib88);
    Song et al., [2022](#bib.bib104); Wu et al., [2022](#bib.bib133); Jia et al.,
    [2022](#bib.bib42); Jin et al., [2024](#bib.bib43)), and four works combined CNN
    and Transformer (Chen et al., [2021](#bib.bib15); Wang et al., [2023c](#bib.bib127),
    [2024a](#bib.bib121); Wu et al., [2023](#bib.bib134)). In the generation stage,
    most papers relied on the Transformer. In terms of technical modules, six works
    (Wang et al., [2022e](#bib.bib130); Jin et al., [2024](#bib.bib43); Wang et al.,
    [2023d](#bib.bib131); Wu et al., [2023](#bib.bib134); Wang et al., [2024a](#bib.bib121),
    [2022d](#bib.bib129)) designed auxiliary tasks and five works (Song et al., [2022](#bib.bib104);
    Wu et al., [2022](#bib.bib133); Wang et al., [2023d](#bib.bib131); Liu et al.,
    [2023b](#bib.bib74); Wang et al., [2022e](#bib.bib130)) used contrastive learning.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我们根据生成报告部分将比较结果进行了分离，完整的比较表格见附录 A 中的表格 [A2](#Sx1.T2 "Table A2 ‣ Appendix
    A ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data")。表格 [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data") 显示了每个评估指标的最佳和第二佳性能。在表格 [3](#S6.T3
    "Table 3 ‣ 6 Benchmark Comparison ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data") 中的 14 篇论文中，有三篇（Song et al., [2022](#bib.bib104)；Liu
    et al., [2023b](#bib.bib74)；Jin et al., [2024](#bib.bib43)）使用了多模态输入。从架构角度来看，在编码阶段，五篇工作采用了纯
    Transformer 模型（Wang et al., [2022e](#bib.bib130)；Kong et al., [2022](#bib.bib50)；Liu
    et al., [2023b](#bib.bib74)；Wang et al., [2023d](#bib.bib131)，[2022d](#bib.bib129)），另外五篇工作使用了基于
    CNN 的模型（Pino et al., [2021](#bib.bib88)；Song et al., [2022](#bib.bib104)；Wu et
    al., [2022](#bib.bib133)；Jia et al., [2022](#bib.bib42)；Jin et al., [2024](#bib.bib43)），还有四篇工作结合了
    CNN 和 Transformer（Chen et al., [2021](#bib.bib15)；Wang et al., [2023c](#bib.bib127)，[2024a](#bib.bib121)；Wu
    et al., [2023](#bib.bib134)）。在生成阶段，大多数论文依赖于 Transformer。在技术模块方面，六篇工作（Wang et al.,
    [2022e](#bib.bib130)；Jin et al., [2024](#bib.bib43)；Wang et al., [2023d](#bib.bib131)；Wu
    et al., [2023](#bib.bib134)；Wang et al., [2024a](#bib.bib121)，[2022d](#bib.bib129)）设计了辅助任务，五篇工作（Song
    et al., [2022](#bib.bib104)；Wu et al., [2022](#bib.bib133)；Wang et al., [2023d](#bib.bib131)；Liu
    et al., [2023b](#bib.bib74)；Wang et al., [2022e](#bib.bib130)）使用了对比学习。'
- en: 'Table 3: Comparisons of the model performance on the MIMIC-CXR Dataset. B1,
    B2, B3, B4, R-L, C-D, P, R, and F represent BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L,
    CIDEr-D, precision, recall, and F1 score, respectively. The best and second best
    results are highlighted. All values were extracted from their papers.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：MIMIC-CXR 数据集模型性能比较。B1、B2、B3、B4、R-L、C-D、P、R 和 F 分别代表 BLEU-1、BLEU-2、BLEU-3、BLEU-4、ROUGE-L、CIDEr-D、精确率、召回率和
    F1 分数。最佳和第二佳结果已突出显示。所有值均来源于其论文。
- en: '| Paper | B1$\uparrow$ | B2$\uparrow$ | B3$\uparrow$ | B4$\uparrow$ | R-L$\uparrow$
    | METEOR$\uparrow$ | C-D$\uparrow$ | P$\uparrow$ | R$\uparrow$ | F$\uparrow$ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | B1$\uparrow$ | B2$\uparrow$ | B3$\uparrow$ | B4$\uparrow$ | R-L$\uparrow$
    | METEOR$\uparrow$ | C-D$\uparrow$ | P$\uparrow$ | R$\uparrow$ | F$\uparrow$ |'
- en: '| Findings Section |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 发现部分 |'
- en: '| Chen et al. ([2021](#bib.bib15)) | 0.353 | 0.218 | 0.148 | 0.106 | 0.278
    | 0.142 | - | 0.334 | 0.275 | 0.278 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Chen et al. ([2021](#bib.bib15)) | 0.353 | 0.218 | 0.148 | 0.106 | 0.278
    | 0.142 | - | 0.334 | 0.275 | 0.278 |'
- en: '| Pino et al. ([2021](#bib.bib88)) | - | - | - | - | 0.185 | - | 0.238 | 0.381
    | 0.531 | 0.428 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Pino et al. ([2021](#bib.bib88)) | - | - | - | - | 0.185 | - | 0.238 | 0.381
    | 0.531 | 0.428 |'
- en: '| Song et al. ([2022](#bib.bib104)) | 0.360 | 0.227 | 0.156 | 0.117 | 0.287
    | 0.148 | - | 0.444 | 0.297 | 0.356 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Song et al. ([2022](#bib.bib104)) | 0.360 | 0.227 | 0.156 | 0.117 | 0.287
    | 0.148 | - | 0.444 | 0.297 | 0.356 |'
- en: '| Impression + Findings Section |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 印象 + 发现部分 |'
- en: '| Wu et al. ([2022](#bib.bib133)) | 0.340 | 0.212 | 0.145 | 0.103 | 0.270 |
    0.139 | 0.109 | - | - | - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. ([2022](#bib.bib133)) | 0.340 | 0.212 | 0.145 | 0.103 | 0.270 |
    0.139 | 0.109 | - | - | - |'
- en: '| Wang et al. ([2022d](#bib.bib129)) | 0.351 | 0.223 | 0.157 | 0.118 | 0.287
    | - | 0.281 | - | - | - |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2022d](#bib.bib129)) | 0.351 | 0.223 | 0.157 | 0.118 | 0.287
    | - | 0.281 | - | - | - |'
- en: '| Jia et al. ([2022](#bib.bib42)) | 0.363 | 0.228 | 0.156 | 0.130 | 0.300 |
    - | - | - | - | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Jia et al. ([2022](#bib.bib42)) | 0.363 | 0.228 | 0.156 | 0.130 | 0.300 |
    - | - | - | - | - |'
- en: '| Unspecified generated sections |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 未指定的生成部分 |'
- en: '| Wang et al. ([2023c](#bib.bib127)) | 0.363 | 0.235 | 0.164 | 0.118 | 0.301
    | 0.136 | - | - | - | - |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2023c](#bib.bib127)) | 0.363 | 0.235 | 0.164 | 0.118 | 0.301
    | 0.136 | - | - | - | - |'
- en: '| Wang et al. ([2024a](#bib.bib121)) | 0.374 | 0.230 | 0.155 | 0.112 | 0.279
    | 0.145 | 0.161 | 0.483 | 0.323 | 0.387 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2024a](#bib.bib121)) | 0.374 | 0.230 | 0.155 | 0.112 | 0.279
    | 0.145 | 0.161 | 0.483 | 0.323 | 0.387 |'
- en: '| Wu et al. ([2023](#bib.bib134)) | 0.383 | 0.224 | 0.146 | 0.104 | 0.280 |
    0.147 | - | - | - | 0.758 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al. ([2023](#bib.bib134)) | 0.383 | 0.224 | 0.146 | 0.104 | 0.280 |
    0.147 | - | - | - | 0.758 |'
- en: '| Wang et al. ([2023d](#bib.bib131)) | 0.386 | 0.250 | 0.169 | 0.124 | 0.291
    | 0.152 | 0.362 | 0.364 | 0.309 | 0.311 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2023d](#bib.bib131)) | 0.386 | 0.250 | 0.169 | 0.124 | 0.291
    | 0.152 | 0.362 | 0.364 | 0.309 | 0.311 |'
- en: '| Liu et al. ([2023b](#bib.bib74)) | 0.391 | 0.249 | 0.172 | 0.125 | 0.304
    | 0.160 | - | - | - | - |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Liu et al. ([2023b](#bib.bib74)) | 0.391 | 0.249 | 0.172 | 0.125 | 0.304
    | 0.160 | - | - | - | - |'
- en: '| Jin et al. ([2024](#bib.bib43)) | 0.398 | - | - | 0.112 | 0.268 | 0.157 |
    - | 0.501 | 0.509 | 0.476 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Jin et al. ([2024](#bib.bib43)) | 0.398 | - | - | 0.112 | 0.268 | 0.157 |
    - | 0.501 | 0.509 | 0.476 |'
- en: '| Wang et al. ([2022e](#bib.bib130)) | 0.413 | 0.266 | 0.186 | 0.136 | 0.298
    | 0.170 | 0.429 | - | - | - |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. ([2022e](#bib.bib130)) | 0.413 | 0.266 | 0.186 | 0.136 | 0.298
    | 0.170 | 0.429 | - | - | - |'
- en: '| Kong et al. ([2022](#bib.bib50)) | 0.423 | 0.261 | 0.171 | 0.116 | 0.286
    | 0.168 | - | 0.482 | 0.563 | 0.519 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Kong et al. ([2022](#bib.bib50)) | 0.423 | 0.261 | 0.171 | 0.116 | 0.286
    | 0.168 | - | 0.482 | 0.563 | 0.519 |'
- en: 7 Challenges and Future Works
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 挑战与未来工作
- en: Although automated systems offer promising efficiency for clinical workflows,
    current methods have not produced very high-quality reports. This section evaluates
    the current progress in automated report generation development and identifies
    potential areas for improvement.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自动化系统在临床工作流中展示了有前景的效率，但目前的方法并未产生非常高质量的报告。本节评估了自动化报告生成发展的当前进展，并识别了潜在的改进领域。
- en: 7.1 Constructing and utilizing multi-modal data
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 构建和利用多模态数据
- en: Considering report generation as a multi-modal problem is more aligned with
    clinical practice (Tu et al., [2024](#bib.bib116); Yan et al., [2023](#bib.bib141)).
    Babar et al. ([2021a](#bib.bib4)) have proven the ineffectiveness of simple encoder-decoder
    report generation models and mentioned that adding prior knowledge can be a promising
    method. However, the current utilization of multi-modal data remains under-explored.
    Firstly, the methods for non-image feature extraction and the fusion of multi-modality
    data are often limited and simplistic, such as using graph encoding for the knowledge
    base and attention mechanisms for feature fusion. Secondly, the construction of
    the knowledge base is imperfect. The pre-defined graph (Zhang et al., [2020](#bib.bib150))
    is overly simplistic. Despite Radgraph being a vast knowledge base, it is solely
    derived from reports, lacking the relationship between images and reports, such
    as organ recognition or understanding of typical radiological scenarios, which
    radiologists possess. The Chest ImaGenome dataset (Wu et al., [2021](#bib.bib132))
    provides the organ recognition annotations, alleviating the problem. Additionally,
    as far as we know, publicly available knowledge bases only concentrate on chest
    X-rays, leaving a gap in general medical knowledge databases.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 将报告生成视为一个多模态问题更符合临床实践（Tu et al., [2024](#bib.bib116); Yan et al., [2023](#bib.bib141)）。Babar
    et al. ([2021a](#bib.bib4)) 已证明简单的编码器-解码器报告生成模型的效果不佳，并提到增加先验知识可能是一种有前景的方法。然而，目前对多模态数据的利用仍然探索不足。首先，非图像特征提取和多模态数据融合的方法通常受到限制且过于简单，例如使用图编码知识库和注意机制进行特征融合。其次，知识库的构建不完善。预定义的图（Zhang
    et al., [2020](#bib.bib150)）过于简单。尽管Radgraph是一个庞大的知识库，但它仅来源于报告，缺乏图像与报告之间的关系，例如器官识别或理解典型的放射学场景，这是放射科医师具备的。Chest
    ImaGenome数据集（Wu et al., [2021](#bib.bib132)）提供了器官识别注释，缓解了这一问题。此外，据我们了解，公开的知识库仅集中于胸部X光片，缺乏一般医学知识数据库的覆盖。
- en: 7.2 Evaluation of medical correctness
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 医疗正确性评估
- en: Evaluating the medical correctness of generated reports is crucial for the clinical
    application. Compared to previous works (Messina et al., [2022](#bib.bib75); Liao
    et al., [2023](#bib.bib64)), recent works have paid more attention to it, but
    still have two shortcomings. First, in the reviewed articles, medical correctness
    evaluation has only been applied to chest X-ray reports. Secondly, the evaluation
    is based on the automatic labeler of radiology reports, which are only targeted
    at 14 types of diseases and the average F1 score is around 0.798 (Smit et al.,
    [2020](#bib.bib103)). Thus, improving the accuracy and scale of automatic labeling
    tools can help optimize the evaluation process.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 评估生成报告的医疗准确性对临床应用至关重要。与之前的工作（Messina 等，[2022](#bib.bib75)；Liao 等，[2023](#bib.bib64)）相比，最近的工作对此给予了更多关注，但仍存在两个缺点。首先，在所评审的文章中，医疗准确性评估仅应用于胸部X光报告。其次，评估基于放射科报告的自动标注器，该标注器仅针对14种疾病，平均F1得分约为0.798（Smit
    等，[2020](#bib.bib103)）。因此，提高自动标注工具的准确性和规模可以帮助优化评估过程。
- en: 7.3 Large public datasets and unified comparison benchmark
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 大型公共数据集与统一比较基准
- en: As shown in Table [1](#S4.T1 "Table 1 ‣ 4 Datasets ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"), most of the public datasets
    are limited in size. Deep learning-based techniques require a large amount of
    data. The contemporary prevalence of large language models underscores this need
    for extensive data volumes. Among the datasets, the MIMIC dataset is relatively
    large but only includes Chest X-ray data. Large datasets targeting other image
    modalities and diseases need to be constructed. In addition, while MIMIC-CXR is
    a well-established benchmark compared to IU-XRay, dataset utilization lacks standardization,
    complicating comparisons. We urge papers using the MIMIC dataset to define their
    training, validation, and testing partitions, with explicit disclosure of the
    filtration method, particularly for the testing dataset.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [1](#S4.T1 "Table 1 ‣ 4 Datasets ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data") 所示，大多数公共数据集的规模有限。深度学习技术需要大量数据。大型语言模型的当代普及凸显了对大量数据的需求。在这些数据集中，MIMIC
    数据集相对较大，但仅包含胸部X光数据。需要构建针对其他图像模态和疾病的大型数据集。此外，虽然与 IU-XRay 相比，MIMIC-CXR 是一个成熟的基准，但数据集的使用缺乏标准化，复杂了比较。我们敦促使用
    MIMIC 数据集的论文定义其训练、验证和测试分区，并明确披露过滤方法，特别是对于测试数据集。
- en: 7.4 Human-AI interaction
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 人工智能与人类互动
- en: Most papers overlook the interaction between users (e.g., clinicians or patients)
    and automated systems. When the system performs as an AI assistant, users may
    want to know the insights of the model regarding specific aspects of medical images.
    In the reviewed works, Tanwani et al. ([2022](#bib.bib113)) constructed a Visual
    Question Answering system for medical report generation, and Tanida et al. ([2023](#bib.bib112))
    linked the output results to image regions using object detection, allowing users
    to select areas of interest and receive corresponding language explanations. Recently,
    dialogue systems (e.g., GPT-4 (Achiam et al., [2023](#bib.bib2)), PaLM (Chowdhery
    et al., [2023](#bib.bib16)), Gemini (Team et al., [2023](#bib.bib114))) based
    on large language models and large multi-modal models have shown people more possibilities
    for human-AI interaction. Although general large language models can provide answers
    to some questions in medical question answering benchmarks, their deployment in
    clinical settings remains unfeasible due to safety concerns within the medical
    domain (Yan et al., [2023](#bib.bib141); Saab et al., [2024](#bib.bib95)).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数论文忽视了用户（例如临床医生或患者）与自动化系统之间的互动。当系统作为AI助手工作时，用户可能希望了解模型对医疗图像特定方面的见解。在所评审的工作中，Tanwani
    等（[2022](#bib.bib113)）构建了一个用于医疗报告生成的视觉问答系统，而Tanida 等（[2023](#bib.bib112)）通过使用对象检测将输出结果与图像区域关联起来，使用户能够选择感兴趣的区域并获得相应的语言解释。最近，基于大型语言模型和大型多模态模型的对话系统（例如
    GPT-4（Achiam 等，[2023](#bib.bib2)），PaLM（Chowdhery 等，[2023](#bib.bib16)），Gemini（Team
    等，[2023](#bib.bib114)））展示了人类与AI互动的更多可能性。尽管通用的大型语言模型可以回答一些医疗问答基准中的问题，但由于医疗领域的安全问题，它们在临床环境中的部署仍然不可行（Yan
    等，[2023](#bib.bib141)；Saab 等，[2024](#bib.bib95)）。
- en: To enable dialogue systems to comprehend medical knowledge, fine-tuning the
    model with medical data is an intuitive approach, such as LLaVA-Med (Li et al.,
    [2024](#bib.bib55)) and Med-PaLM 2 (Singhal et al., [2023](#bib.bib101)). However,
    they did not test the model’s performance on the report generation task. Saab
    et al. ([2024](#bib.bib95)) proposed Med-Gemini, a series of highly proficient
    multi-modal models tailored specifically for the medical domain. They intuitively
    showed the interactive report generation process for a normal case, while lacking
    quantitative results to evaluate report generation performance. Exploring the
    direction of report generation with human-AI interaction holds significant promise.
    Additionally, fine-tuning large models demands substantial GPU resources, making
    efficient methods crucial.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使对话系统能够理解医学知识，对模型进行医学数据的微调是一种直观的方法，例如LLaVA-Med（Li 等，[2024](#bib.bib55)）和Med-PaLM
    2（Singhal 等，[2023](#bib.bib101)）。然而，他们没有测试模型在报告生成任务上的性能。Saab 等人（[2024](#bib.bib95)）提出了Med-Gemini，一系列高度熟练的多模态模型，专门针对医学领域。他们直观地展示了一个正常案例的交互报告生成过程，但缺乏评估报告生成性能的定量结果。探索人类与人工智能互动下的报告生成方向具有重大前景。此外，微调大型模型需要大量GPU资源，因此高效的方法至关重要。
- en: 7.5 Standardized report generation
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 标准化报告生成
- en: Most of the existent methods focused on unstructured report generation, while
    structured reporting has several advantages, such as saving time (Hong and Kahn,
    [2013](#bib.bib30); Nobel et al., [2022](#bib.bib82)), preventing errors, decreasing
    communication expenses linked to ambiguous natural language. Recently, Pellegrini
    et al. ([2023](#bib.bib87)) developed a structured template and released a related
    dataset. This promising start could pave the way for further exploration in this
    direction.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数方法集中于非结构化报告生成，而结构化报告具有诸多优势，例如节省时间（Hong 和 Kahn，[2013](#bib.bib30)；Nobel
    等，[2022](#bib.bib82)），防止错误，减少与模糊自然语言相关的沟通开支。最近，Pellegrini 等人（[2023](#bib.bib87)）开发了一个结构化模板，并发布了相关数据集。这一有前景的开端可能为进一步探索这一方向铺平道路。
- en: 8 Conclusions
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this study, we have conducted a detailed technical review of 89 papers on
    automatic medical report generation published in the years 2021, 2022, 2023, and
    2024 to showcase both mainstream and novel techniques. Our particular focus lies
    on the utilization and fusion of multi-modality data. The analysis of methods
    is structured based on the components of the report generation pipeline, presenting
    the key techniques for each component. Additionally, we provide a summary of the
    current publicly available datasets and evaluation methods, encompassing both
    quantitative and qualitative assessments. Subsequently, we compare the results
    from a subset of the 89 papers under the same experimental setting. Finally, we
    outline the current challenges and propose future directions for medical report
    generation. Overall, sustained progress is needed to produce standardized and
    clinically accurate reports. This survey aims to offer a comprehensive overview
    of report-generation techniques, emphasize critical issues, and assist researchers
    in promptly grasping recent advancements in the field to build more robust systems
    for clinical practice.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们对2021年、2022年、2023年和2024年间发表的89篇关于自动医学报告生成的论文进行了详细的技术评审，以展示主流和新颖的技术。我们特别关注多模态数据的利用和融合。方法分析是基于报告生成管道的组件结构化的，展示了每个组件的关键技术。此外，我们提供了当前公开数据集和评估方法的总结，包括定量和定性评估。随后，我们在相同实验设置下比较了89篇论文中的一个子集的结果。最后，我们概述了当前的挑战并提出了医学报告生成的未来方向。总体而言，需要持续进步以生成标准化和临床准确的报告。本次调查旨在提供报告生成技术的全面概述，强调关键问题，并帮助研究人员迅速掌握该领域的最新进展，以构建更为健壮的临床实践系统。
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abela et al. [2022] Brandon Abela, Jumana Abu-Khalaf, Chi-Wei Robin Yang, Martin
    Masek, and Ashu Gupta. Automated radiology report generation using a transformer-template
    system: Improved clinical accuracy and an assessment of clinical safety. In *AI
    2022: Advances in Artificial Intelligence: 35th Australasian Joint Conference,
    AI 2022, Perth, WA, Australia, December 5–8, 2022, Proceedings*, pages 530–543\.
    Springer, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abela等人[2022]布兰登·阿贝拉、朱玛娜·阿布-哈拉夫、杨志伟、马丁·马塞克和阿舒·古普塔。使用变压器模板系统自动生成放射科报告：提高临床准确性和临床安全性评估。在*AI
    2022: Advances in Artificial Intelligence: 35th Australasian Joint Conference,
    AI 2022, Perth, WA, Australia, December 5–8, 2022, Proceedings*，第530–543页。Springer，2022年。'
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等人[2023] Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge Akkaya、Florencia
    Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal Anadkat 等。Gpt-4技术报告。*arXiv预印本
    arXiv:2303.08774*，2023。
- en: Alfarghaly et al. [2021] Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha
    Helal, and Aly Fahmy. Automated radiology report generation using conditioned
    transformers. *Informatics in Medicine Unlocked*, 24:100557, 2021.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alfarghaly等人[2021] Omar Alfarghaly、Rana Khaled、Abeer Elkorany、Maha Helal 和 Aly
    Fahmy。使用条件变换器的自动放射学报告生成。*医学信息解锁*，24:100557，2021。
- en: Babar et al. [2021a] Zaheer Babar, Twan van Laarhoven, and Elena Marchiori.
    Encoder-decoder models for chest x-ray report generation perform no better than
    unconditioned baselines. *Plos one*, 16(11):e0259639, 2021a.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babar等人[2021a] Zaheer Babar、Twan van Laarhoven 和 Elena Marchiori。用于胸部X光报告生成的编码器-解码器模型表现不如无条件基准。*Plos
    one*，16(11):e0259639，2021a。
- en: Babar et al. [2021b] Zaheer Babar, Twan van Laarhoven, Fabio Massimo Zanzotto,
    and Elena Marchiori. Evaluating diagnostic content of ai-generated radiology reports
    of chest x-rays. *Artificial Intelligence in Medicine*, 116:102075, 2021b.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Babar等人[2021b] Zaheer Babar、Twan van Laarhoven、Fabio Massimo Zanzotto 和 Elena
    Marchiori。评估由AI生成的胸部X光片放射报告的诊断内容。*人工智能医学*，116:102075，2021b。
- en: 'Banerjee and Lavie [2005] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
    metric for mt evaluation with improved correlation with human judgments. In *Proceedings
    of the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization*, pages 65–72, 2005.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee和Lavie[2005] Satanjeev Banerjee 和 Alon Lavie。Meteor：一种用于机器翻译评估的自动度量，与人工判断的相关性得到了改善。在*ACL工作坊机器翻译和/或摘要的内在和外在评估度量会议论文集*，第65–72页，2005。
- en: 'Beddiar et al. [2023] Djamila-Romaissa Beddiar, Mourad Oussalah, and Tapio
    Seppänen. Automatic captioning for medical imaging (mic): a rapid review of literature.
    *Artificial Intelligence Review*, 56(5):4019–4076, 2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beddiar等人[2023] Djamila-Romaissa Beddiar、Mourad Oussalah 和 Tapio Seppänen。医学影像自动标注（MIC）：文献的快速综述。*人工智能评论*，56(5):4019–4076，2023。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人[2020] Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。语言模型是少量学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Cao et al. [2022] Yiming Cao, Lizhen Cui, Fuqiang Yu, Lei Zhang, Zhen Li, Ning
    Liu, and Yonghui Xu. Kdtnet: medical image report generation via knowledge-driven
    transformer. In *Database Systems for Advanced Applications: 27th International
    Conference, DASFAA 2022, Virtual Event, April 11–14, 2022, Proceedings, Part III*,
    pages 117–132\. Springer, 2022.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人[2022] Yiming Cao、Lizhen Cui、Fuqiang Yu、Lei Zhang、Zhen Li、Ning Liu 和 Yonghui
    Xu。KDTNet：通过知识驱动变换器生成医学图像报告。在*高级应用数据库系统：第27届国际会议，DASFAA 2022，虚拟会议，2022年4月11–14日，会议论文集，第三部分*，第117–132页。Springer，2022。
- en: 'Cao et al. [2023] Yiming Cao, Lizhen Cui, Lei Zhang, Fuqiang Yu, Ziheng Cheng,
    Zhen Li, Yonghui Xu, and Chunyan Miao. Cmt: Cross-modal memory transformer for
    medical image report generation. In *International Conference on Database Systems
    for Advanced Applications*, pages 415–424\. Springer, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人[2023] Yiming Cao、Lizhen Cui、Lei Zhang、Fuqiang Yu、Ziheng Cheng、Zhen Li、Yonghui
    Xu 和 Chunyan Miao。CMT：用于医学图像报告生成的跨模态记忆变换器。在*高级应用数据库系统国际会议*，第415–424页。Springer，2023。
- en: Carreira and Zisserman [2017] Joao Carreira and Andrew Zisserman. Quo vadis,
    action recognition? a new model and the kinetics dataset. In *proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, pages 6299–6308,
    2017.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carreira和Zisserman[2017] Joao Carreira 和 Andrew Zisserman。Quo vadis，动作识别？一种新模型和Kinetics数据集。在*IEEE计算机视觉与模式识别会议论文集*，第6299–6308页，2017。
- en: 'Chen et al. [2019] Qingyu Chen, Yifan Peng, and Zhiyong Lu. Biosentvec: creating
    sentence embeddings for biomedical texts. In *2019 IEEE International Conference
    on Healthcare Informatics (ICHI)*, pages 1–5\. IEEE, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人[2019] Qingyu Chen、Yifan Peng 和 Zhiyong Lu。Biosentvec：为生物医学文本创建句子嵌入。在*2019
    IEEE国际医疗信息学会议（ICHI）*，第1–5页。IEEE，2019。
- en: 'Chen et al. [2022] Weipeng Chen, Haiwei Pan, Kejia Zhang, Xin Du, and Qianna
    Cui. Vmeknet: Visual memory and external knowledge based network for medical report
    generation. In *PRICAI 2022: Trends in Artificial Intelligence: 19th Pacific Rim
    International Conference on Artificial Intelligence, PRICAI 2022, Shanghai, China,
    November 10–13, 2022, Proceedings, Part I*, pages 188–201\. Springer, 2022.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2022] Weipeng Chen, Haiwei Pan, Kejia Zhang, Xin Du 和 Qianna Cui。Vmeknet：基于视觉记忆和外部知识的医疗报告生成网络。在*PRICAI
    2022：人工智能趋势：第 19 届太平洋 Rim 国际人工智能会议，PRICAI 2022，上海，中国，2022 年 11 月 10–13 日，会议录，第
    I 部分*，页码 188–201。Springer，2022。
- en: Chen et al. [2020] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating
    radiology reports via memory-driven transformer. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 1439–1449,
    2020.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2020] Zhihong Chen, Yan Song, Tsung-Hui Chang 和 Xiang Wan。通过记忆驱动的变换器生成放射学报告。在*2020
    年自然语言处理实证方法会议论文集 (EMNLP)* 上，页码 1439–1449，2020。
- en: 'Chen et al. [2021] Zhihong Chen, Yaling Shen, Yan Song, and Xiang Wan. Cross-modal
    memory networks for radiology report generation. In *Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    5904–5914, 2021.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2021] Zhihong Chen, Yaling Shen, Yan Song 和 Xiang Wan。用于放射学报告生成的跨模态记忆网络。在*第
    59 届计算语言学协会年会及第 11 届国际联合自然语言处理会议 (第 1 卷：长篇论文)* 上，页码 5904–5914，2021。
- en: 'Chowdhery et al. [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 24(240):1–113, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人 [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等人。Palm：通过路径扩展语言建模。*机器学习研究期刊*，24(240)：1–113，2023。
- en: Cornia et al. [2020] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and
    Rita Cucchiara. Meshed-memory transformer for image captioning. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    10578–10587, 2020.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cornia 等人 [2020] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi 和 Rita Cucchiara。用于图像描述的网状记忆变换器。在*IEEE/CVF
    计算机视觉与模式识别会议论文集*上，页码 10578–10587，2020。
- en: Dalla Serra et al. [2022] Francesco Dalla Serra, William Clackett, Hamish MacKinnon,
    Chaoyang Wang, Fani Deligianni, Jeff Dalton, and Alison Q O’Neil. Multimodal generation
    of radiology reports using knowledge-grounded extraction of entities and relations.
    In *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics and the 12th International Joint Conference on Natural
    Language Processing*, pages 615–624, 2022.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalla Serra 等人 [2022] Francesco Dalla Serra, William Clackett, Hamish MacKinnon,
    Chaoyang Wang, Fani Deligianni, Jeff Dalton 和 Alison Q O’Neil。使用知识驱动的实体和关系提取的多模态放射学报告生成。在*第
    2 届亚太计算语言学协会会议及第 12 届国际联合自然语言处理会议论文集*上，页码 615–624，2022。
- en: Dalla Serra et al. [2023a] Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni,
    Jeff Dalton, and Alison Q O’Neil. Controllable chest x-ray report generation from
    longitudinal representations. In *The 2023 Conference on Empirical Methods in
    Natural Language Processing*, 2023a.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalla Serra 等人 [2023a] Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni,
    Jeff Dalton 和 Alison Q O’Neil。从纵向表示中生成可控的胸部 X 光报告。在*2023 年自然语言处理实证方法会议*上，2023a。
- en: Dalla Serra et al. [2023b] Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni,
    Jeffrey Dalton, and Alison Q O’Neil. Finding-aware anatomical tokens for chest
    x-ray automated reporting. In *International Workshop on Machine Learning in Medical
    Imaging*, pages 413–423\. Springer, 2023b.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalla Serra 等人 [2023b] Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni,
    Jeffrey Dalton 和 Alison Q O’Neil。胸部 X 光自动报告的查找感知解剖标记。在*国际医学影像机器学习研讨会*上，页码 413–423。Springer，2023b。
- en: Demner-Fushman et al. [2016] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman,
    Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J
    McDonald. Preparing a collection of radiology examinations for distribution and
    retrieval. *Journal of the American Medical Informatics Association*, 23(2):304–310,
    2016.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demner-Fushman 等人 [2016] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman,
    Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma 和 Clement J
    McDonald。准备放射学检查集以便分发和检索。*美国医学信息学协会期刊*，23(2)：304–310，2016。
- en: Du et al. [2022] Xin Du, Haiwei Pan, Kejia Zhang, Shuning He, Xiaofei Bian,
    and Weipeng Chen. Automatic report generation method based on multiscale feature
    extraction and word attention network. In *Asia-Pacific Web (APWeb) and Web-Age
    Information Management (WAIM) Joint International Conference on Web and Big Data*,
    pages 520–528\. Springer, 2022.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等 [2022] 杜欣、潘海伟、张克佳、何顺宁、边晓飞和陈伟鹏。基于多尺度特征提取和词注意网络的自动报告生成方法。在*亚太网络（APWeb）和网络时代信息管理（WAIM）联合国际会议：网络和大数据*，第520–528页。施普林格，2022。
- en: Endo et al. [2021] Mark Endo, Rayan Krishnan, Viswesh Krishna, Andrew Y Ng,
    and Pranav Rajpurkar. Retrieval-based chest x-ray report generation using a pre-trained
    contrastive language-image model. In *Machine Learning for Health*, pages 209–219\.
    PMLR, 2021.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Endo 等 [2021] 马克·恩多、瑞安·克里什南、维斯韦什·克里希纳、安德鲁·Y·吴和普拉纳夫·拉朱尔卡。使用预训练对比语言-图像模型的检索基础胸部
    X 光报告生成。在*健康机器学习*，第209–219页。PMLR，2021。
- en: 'Gajbhiye et al. [2022] Gaurav O Gajbhiye, Abhijeet V Nandedkar, and Ibrahima
    Faye. Translating medical image to radiological report: Adaptive multilevel multi-attention
    approach. *Computer Methods and Programs in Biomedicine*, 221:106853, 2022.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gajbhiye 等 [2022] 高拉夫·O·加杰比耶、阿比杰特·V·南德卡尔和伊布拉欣·法耶。医学图像翻译为放射学报告：自适应多级多注意力方法。*生物医学计算方法与程序*，221:106853，2022。
- en: Gu et al. [2024] Tiancheng Gu, Dongnan Liu, Zhiyuan Li, and Weidong Cai. Complex
    organ mask guided radiology report generation. In *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pages 7995–8004, 2024.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等 [2024] 顾天成、刘东南、李智远和蔡卫东。复杂器官掩模引导的放射学报告生成。在*IEEE/CVF 冬季计算机视觉应用会议论文集*，第7995–8004页，2024。
- en: Han et al. [2021] Zhongyi Han, Benzheng Wei, Xiaoming Xi, Bo Chen, Yilong Yin,
    and Shuo Li. Unifying neural learning and symbolic reasoning for spinal medical
    report generation. *Medical image analysis*, 67:101872, 2021.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等 [2021] 韩中毅、魏本正、席晓明、陈博、尹亿龙和李硕。统一神经学习和符号推理用于脊柱医学报告生成。*医学图像分析*，67:101872，2021。
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778, 2016.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 [2016] 贺凯明、张翔宇、任少青和孙剑。用于图像识别的深度残差学习。在*IEEE 计算机视觉与模式识别会议论文集*，第770–778页，2016。
- en: He et al. [2020] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
    Momentum contrast for unsupervised visual representation learning. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    9729–9738, 2020.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 [2020] 贺凯明、范浩琪、吴宇欣、谢赛宁和罗斯·吉尔什克。用于无监督视觉表示学习的动量对比。在*IEEE/CVF 计算机视觉与模式识别会议论文集*，第9729–9738页，2020。
- en: Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural computation*, 9(8):1735–1780, 1997.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] 塞普·霍赫雷特和于尔根·施密德胡伯。长短期记忆。*神经计算*，9(8):1735–1780，1997。
- en: Hong and Kahn [2013] Yi Hong and Charles E Kahn. Content analysis of reporting
    templates and free-text radiology reports. *Journal of digital imaging*, 26:843–849,
    2013.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 和 Kahn [2013] 洪逸和查尔斯·E·卡恩。报告模板和自由文本放射学报告的内容分析。*数字成像杂志*，26:843–849，2013。
- en: 'Hou et al. [2021a] Benjamin Hou, Georgios Kaissis, Ronald M Summers, and Bernhard
    Kainz. Ratchet: Medical transformer for chest x-ray diagnosis and reporting. In
    *Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part VII 24*, pages 293–303\. Springer, 2021a.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou 等 [2021a] 本杰明·侯、乔治奥斯·凯西斯、罗纳德·M·萨默斯和伯恩哈德·凯因兹。Ratchet: 胸部 X 光诊断和报告的医学变压器。在*医学图像计算与计算机辅助手术–MICCAI
    2021: 第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，论文集，第VII部分 24*，第293–303页。施普林格，2021a。'
- en: Hou et al. [2021b] Daibing Hou, Zijian Zhao, Yuying Liu, Faliang Chang, and
    Sanyuan Hu. Automatic report generation for chest x-ray images via adversarial
    reinforcement learning. *IEEE Access*, 9:21236–21250, 2021b.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 等 [2021b] 侯大炳、赵自建、刘玉英、常法良和胡三元。通过对抗性强化学习进行胸部 X 光图像的自动报告生成。*IEEE Access*，9:21236–21250，2021b。
- en: Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. Densely connected convolutional networks. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4700–4708, 2017.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 [2017] 黄高、刘壮、劳伦斯·范·德·马滕和基利安·Q·温伯格。密集连接卷积网络。在*IEEE 计算机视觉与模式识别会议论文集*，第4700–4708页，2017。
- en: Huang et al. [2021a] Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, and Marcel
    Worring. Deep context-encoding network for retinal image captioning. In *2021
    IEEE International Conference on Image Processing (ICIP)*, pages 3762–3766\. IEEE,
    2021a.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2021a] 家洪·黄、廷伟·吴、超汉·杨和马塞尔·沃林。用于视网膜图像描述的深度上下文编码网络。在 *2021 IEEE 国际图像处理会议
    (ICIP)*，第3762–3766页。IEEE，2021a年。
- en: 'Huang et al. [2021b] Jia-Hong Huang, C-H Huck Yang, Fangyu Liu, Meng Tian,
    Yi-Chieh Liu, Ting-Wei Wu, I Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang,
    et al. Deepopht: medical report generation for retinal images via deep models
    and visual explanation. In *Proceedings of the IEEE/CVF winter conference on applications
    of computer vision*, pages 2442–2452, 2021b.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '黄等人 [2021b] 家洪·黄、C-H·杨、方宇·刘、孟天、义杰·刘、廷伟·吴、怡林、康·王、广政·森、何荣华等。Deepopht: 通过深度模型和视觉解释生成视网膜图像的医疗报告。在
    *IEEE/CVF 冬季计算机视觉应用会议论文集*，第2442–2452页，2021b年。'
- en: Huang et al. [2022] Jia-Hong Huang, Ting-Wei Wu, C-H Huck Yang, Zenglin Shi,
    I Lin, Jesper Tegner, Marcel Worring, et al. Non-local attention improves description
    generation for retinal images. In *Proceedings of the IEEE/CVF winter conference
    on applications of computer vision*, pages 1606–1615, 2022.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2022] 家洪·黄、廷伟·吴、C-H·杨、增林·施、怡林、耶斯珀·特格纳、马塞尔·沃林等。非局部注意力改善视网膜图像的描述生成。在 *IEEE/CVF
    冬季计算机视觉应用会议论文集*，第1606–1615页，2022年。
- en: 'Huang et al. [2023] Zhongzhen Huang, Xiaofan Zhang, and Shaoting Zhang. Kiut:
    Knowledge-injected u-transformer for radiology report generation. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    19809–19818, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '黄等人 [2023] 中真·黄、晓凡·张和绍婷·张。Kiut: 知识注入的 u-transformer 用于放射科报告生成。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第19809–19818页，2023年。'
- en: 'Irvin et al. [2019] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana
    Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie
    Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty
    labels and expert comparison. In *Proceedings of the AAAI conference on artificial
    intelligence*, volume 33, pages 590–597, 2019.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '欧文等人 [2019] 杰瑞米·欧文、普拉纳夫·拉杰普卡、迈克尔·科、义凡·余、西尔维安娜·丘雷亚-伊尔库斯、克里斯·楚特、亨里克·马克伦、贝赫扎德·哈赫古、罗宾·巴尔、凯蒂·施潘斯卡娅等。Chexpert:
    一个带有不确定性标签和专家比较的大型胸部 X 射线数据集。在 *AAAI 人工智能会议论文集*，第33卷，第590–597页，2019年。'
- en: 'Jain et al. [2021] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong,
    Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y
    Ng, et al. Radgraph: Extracting clinical entities and relations from radiology
    reports. *arXiv preprint arXiv:2106.14463*, 2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '贾恩等人 [2021] 萨希尔·贾恩、阿什温·阿格拉瓦尔、阿德里尔·萨波塔、史蒂文·QH· Truong、杜·阮、谭·布伊、皮埃尔·尚邦、宇浩·张、马修·P·伦格伦、安德鲁·Y·吴等。Radgraph:
    从放射科报告中提取临床实体和关系。*arXiv 预印本 arXiv:2106.14463*，2021年。'
- en: Jeong et al. [2024] Jaehwan Jeong, Katherine Tian, Andrew Li, Sina Hartung,
    Subathra Adithan, Fardad Behzadi, Juan Calle, David Osayande, Michael Pohlen,
    and Pranav Rajpurkar. Multimodal image-text matching improves retrieval-based
    chest x-ray report generation. In *Medical Imaging with Deep Learning*, pages
    978–990\. PMLR, 2024.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2024] 在焕·郑、凯瑟琳·田、安德鲁·李、斯纳·哈通、苏巴特拉·阿迪坦、法尔达德·贝赫扎迪、胡安·卡列、戴维·奥萨延德、迈克尔·波伦和普拉纳夫·拉杰普卡。多模态图像-文本匹配改善了基于检索的胸部
    X 射线报告生成。在 *医学影像与深度学习*，第978–990页。PMLR，2024年。
- en: Jia et al. [2021] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Blackley Suzanne,
    Yangyong Zhu, and Chunlei Tang. Radiology report generation for rare diseases
    via few-shot transformer. In *IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM)*, pages 1347–1352\. IEEE, 2021.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾等人 [2021] 星·贾、云·熊、佳伟·张、姚·张、布莱克利·苏珊、杨勇·朱和春雷·汤。通过少样本变换器进行稀有疾病的放射科报告生成。在 *IEEE
    国际生物信息学与生物医学会议 (BIBM)*，第1347–1352页。IEEE，2021年。
- en: Jia et al. [2022] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Yangyong Zhu,
    and S Yu Philip. Few-shot radiology report generation via knowledge transfer and
    multi-modal alignment. In *2022 IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM)*, pages 1574–1579\. IEEE, 2022.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾等人 [2022] 星·贾、云·熊、佳伟·张、姚·张、杨勇·朱和S·余·菲利普。通过知识转移和多模态对齐进行少样本放射科报告生成。在 *2022 IEEE
    国际生物信息学与生物医学会议 (BIBM)*，第1574–1579页。IEEE，2022年。
- en: 'Jin et al. [2024] Haibo Jin, Haoxuan Che, Yi Lin, and Hao Chen. Promptmrg:
    Diagnosis-driven prompts for medical report generation. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 38, pages 2607–2615, 2024.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '金等人 [2024] 海波·金、浩轩·车、怡林·易和浩陈。Promptmrg: 诊断驱动的提示用于医疗报告生成。在 *AAAI 人工智能会议论文集*，第38卷，第2607–2615页，2024年。'
- en: Johnson et al. [2019a] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,
    Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven
    Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs
    with free-text reports. *Scientific data*, 6(1):317, 2019a.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson et al. [2019a] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,
    Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven
    Horng. Mimic-cxr，一个去标识的公开可用的胸部X光片数据库，包含自由文本报告。*科学数据*，6(1):317，2019a年。
- en: Johnson et al. [2019b] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum,
    Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J
    Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available database
    of labeled chest radiographs. *arXiv preprint arXiv:1901.07042*, 2019b.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson et al. [2019b] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum,
    Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth
    J Berkowitz, and Steven Horng. Mimic-cxr-jpg，一个大型公开可用的标记胸部X光片数据库。*arXiv 预印本 arXiv:1901.07042*，2019b年。
- en: 'Kaur and Mittal [2022a] Navdeep Kaur and Ajay Mittal. Radiobert: A deep learning-based
    system for medical report generation from chest x-ray images using contextual
    embeddings. *Journal of Biomedical Informatics*, 135:104220, 2022a.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kaur and Mittal [2022a] Navdeep Kaur and Ajay Mittal. Radiobert: 一个基于深度学习的系统，通过上下文嵌入从胸部X光图像生成医学报告。*生物医学信息学期刊*，135:104220，2022a年。'
- en: 'Kaur and Mittal [2022b] Navdeep Kaur and Ajay Mittal. Cadxreport: Chest x-ray
    report generation using co-attention mechanism and reinforcement learning. *Computers
    in Biology and Medicine*, 145:105498, 2022b.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kaur and Mittal [2022b] Navdeep Kaur and Ajay Mittal. Cadxreport: 使用共注意力机制和强化学习生成胸部X光报告。*生物医学与医学计算机*，145:105498，2022b年。'
- en: 'Kaur et al. [2022] Navdeep Kaur, Ajay Mittal, and Gurprem Singh. Methods for
    automatic generation of radiological reports of chest radiographs: a comprehensive
    survey. *Multimedia Tools and Applications*, 81(10):13409–13439, 2022.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur et al. [2022] Navdeep Kaur, Ajay Mittal, and Gurprem Singh. 胸部X光片放射学报告的自动生成方法：一项综合调查。*多媒体工具与应用*，81(10):13409–13439，2022年。
- en: Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational
    bayes. *arXiv preprint arXiv:1312.6114*, 2013.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Welling [2013] Diederik P Kingma and Max Welling. 自编码变分贝叶斯。*arXiv
    预印本 arXiv:1312.6114*，2013年。
- en: 'Kong et al. [2022] Ming Kong, Zhengxing Huang, Kun Kuang, Qiang Zhu, and Fei
    Wu. Transq: Transformer-based semantic query for medical report generation. In
    *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    VIII*, pages 610–620\. Springer, 2022.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kong et al. [2022] Ming Kong, Zhengxing Huang, Kun Kuang, Qiang Zhu, and Fei
    Wu. Transq: 基于变换器的医学报告生成语义查询。在*医学图像计算与计算机辅助手术–MICCAI 2022: 第25届国际会议，新加坡，2022年9月18–22日，会议论文集，第VIII部分*，第610–620页。施普林格，2022年。'
- en: 'Langlotz [2006] Curtis P Langlotz. Radlex: a new method for indexing online
    educational materials, 2006.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Langlotz [2006] Curtis P Langlotz. Radlex: 一种新的在线教育材料索引方法，2006年。'
- en: Lau et al. [2018] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.
    A dataset of clinically generated visual questions and answers about radiology
    images. *Scientific data*, 5(1):1–10, 2018.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lau et al. [2018] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.
    一个关于放射学图像的临床生成视觉问题和答案的数据集。*科学数据*，5(1):1–10，2018年。
- en: Lee et al. [2022] Hojun Lee, Hyunjun Cho, Jieun Park, Jinyeong Chae, and Jihie
    Kim. Cross encoder-decoder transformer with global-local visual extractor for
    medical image captioning. *Sensors*, 22(4):1429, 2022.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. [2022] Hojun Lee, Hyunjun Cho, Jieun Park, Jinyeong Chae, and Jihie
    Kim. 具有全球-局部视觉提取器的交叉编码器-解码器变换器用于医学图像描述。*传感器*，22(4):1429，2022年。
- en: 'Lee et al. [2020] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
    Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation
    model for biomedical text mining. *Bioinformatics*, 36(4):1234–1240, 2020.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. [2020] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
    Kim, Chan Ho So, and Jaewoo Kang. Biobert: 一个预训练的生物医学语言表示模型用于生物医学文本挖掘。*生物信息学*，36(4):1234–1240，2020年。'
- en: 'Li et al. [2024] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    在一天内训练一个用于生物医学的大型语言与视觉助手。*神经信息处理系统进展*，36，2024年。'
- en: 'Li et al. [2022a] Jun Li, Shibo Li, Ying Hu, and Huiren Tao. A self-guided
    framework for radiology report generation. In *Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2022: 25th International Conference, Singapore, September
    18–22, 2022, Proceedings, Part VIII*, pages 588–598\. Springer, 2022a.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2022a] Jun Li, Shibo Li, Ying Hu, 和 Huiren Tao。用于放射学报告生成的自我引导框架。在 *医学图像计算与计算机辅助干预–MICCAI
    2022: 第25届国际会议，新加坡，2022年9月18–22日，会议论文，第VIII部分*，第588–598页。Springer，2022a。'
- en: Li et al. [2020] Mingjie Li, Fuyu Wang, Xiaojun Chang, and Xiaodan Liang. Auxiliary
    signal-guided knowledge encoder-decoder for medical report generation. *arXiv
    preprint arXiv:2006.03744*, 2020.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020] Mingjie Li, Fuyu Wang, Xiaojun Chang, 和 Xiaodan Liang。用于医学报告生成的辅助信号引导知识编码器-解码器。*arXiv
    预印本 arXiv:2006.03744*，2020年。
- en: 'Li et al. [2021] Mingjie Li, Wenjia Cai, Rui Liu, Yuetian Weng, Xiaoyun Zhao,
    Cong Wang, Xin Chen, Zhong Liu, Caineng Pan, Mengke Li, et al. Ffa-ir: Towards
    an explainable and reliable medical report generation benchmark. In *Thirty-fifth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track
    (Round 2)*, 2021.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2021] Mingjie Li, Wenjia Cai, Rui Liu, Yuetian Weng, Xiaoyun Zhao, Cong
    Wang, Xin Chen, Zhong Liu, Caineng Pan, Mengke Li, 等。Ffa-ir：朝向一个可解释和可靠的医学报告生成基准。在
    *第三十五届神经信息处理系统会议数据集和基准测试分会（第2轮）*，2021年。
- en: Li et al. [2022b] Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan
    Liang, and Xiaojun Chang. Cross-modal clinical graph transformer for ophthalmic
    report generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 20656–20665, 2022b.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2022b] Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan Liang,
    和 Xiaojun Chang。用于眼科报告生成的跨模态临床图变压器。在 *IEEE/CVF计算机视觉与模式识别会议论文集*，第20656–20665页，2022b年。
- en: Li et al. [2023a] Mingjie Li, Bingqian Lin, Zicong Chen, Haokun Lin, Xiaodan
    Liang, and Xiaojun Chang. Dynamic graph enhanced contrastive learning for chest
    x-ray report generation. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 3334–3343, 2023a.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023a] Mingjie Li, Bingqian Lin, Zicong Chen, Haokun Lin, Xiaodan Liang,
    和 Xiaojun Chang。动态图增强对比学习用于胸部X射线报告生成。在 *IEEE/CVF计算机视觉与模式识别会议论文集*，第3334–3343页，2023a年。
- en: Li et al. [2023b] Mingjie Li, Rui Liu, Fuyu Wang, Xiaojun Chang, and Xiaodan
    Liang. Auxiliary signal-guided knowledge encoder-decoder for medical report generation.
    *World Wide Web*, 26(1):253–270, 2023b.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023b] Mingjie Li, Rui Liu, Fuyu Wang, Xiaojun Chang, 和 Xiaodan Liang。用于医学报告生成的辅助信号引导知识编码器-解码器。*全球信息网*，26(1)：253–270，2023b年。
- en: Li et al. [2023c] Qingqiu Li, Jilan Xu, Runtian Yuan, Mohan Chen, Yuejie Zhang,
    Rui Feng, Xiaobo Zhang, and Shang Gao. Enhanced knowledge injection for radiology
    report generation. In *2023 IEEE International Conference on Bioinformatics and
    Biomedicine (BIBM)*, pages 2053–2058\. IEEE, 2023c.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023c] Qingqiu Li, Jilan Xu, Runtian Yuan, Mohan Chen, Yuejie Zhang,
    Rui Feng, Xiaobo Zhang, 和 Shang Gao。用于放射学报告生成的增强知识注入。在 *2023 IEEE国际生物信息学与生物医学会议（BIBM）*，第2053–2058页。IEEE，2023c年。
- en: 'Li et al. [2023d] Yaowei Li, Bang Yang, Xuxin Cheng, Zhihong Zhu, Hongxiang
    Li, and Yuexian Zou. Unify, align and refine: Multi-level semantic alignment for
    radiology report generation. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 2863–2874, 2023d.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023d] Yaowei Li, Bang Yang, Xuxin Cheng, Zhihong Zhu, Hongxiang Li,
    和 Yuexian Zou。统一、对齐与精炼：用于放射学报告生成的多级语义对齐。在 *IEEE/CVF国际计算机视觉会议论文集*，第2863–2874页，2023d年。
- en: 'Liao et al. [2023] Yuxiang Liao, Hantao Liu, and Irena Spasić. Deep learning
    approaches to automatic radiology report generation: A systematic review. *Informatics
    in Medicine Unlocked*, page 101273, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 等人 [2023] Yuxiang Liao, Hantao Liu, 和 Irena Spasić。自动放射学报告生成的深度学习方法：系统评述。*医学信息解锁*，第101273页，2023年。
- en: 'Lin [2004] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In *Text summarization branches out*, pages 74–81, 2004.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin [2004] Chin-Yew Lin。Rouge：一个用于自动评估摘要的包。在 *文本摘要的扩展*，第74–81页，2004年。
- en: Lin et al. [2023] Zhihong Lin, Donghao Zhang, Danli Shi, Renjing Xu, Qingyi
    Tao, Lin Wu, Mingguang He, and Zongyuan Ge. Contrastive pre-training and linear
    interaction attention-based transformer for universal medical reports generation.
    *Journal of Biomedical Informatics*, page 104281, 2023.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 [2023] Zhihong Lin, Donghao Zhang, Danli Shi, Renjing Xu, Qingyi Tao,
    Lin Wu, Mingguang He, 和 Zongyuan Ge。对比预训练和基于线性交互注意力的变压器用于通用医学报告生成。*生物医学信息学杂志*，第104281页，2023年。
- en: Liu et al. [2023a] Chang Liu, Yuanhe Tian, and Yan Song. A systematic review
    of deep learning-based research on radiology report generation. *arXiv preprint
    arXiv:2311.14199*, 2023a.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023a] 刘昌、田元禾和宋艳。基于深度学习的放射学报告生成研究的系统综述。*arXiv 预印本 arXiv:2311.14199*，2023a。
- en: 'Liu et al. [2021a] Fenglin Liu, Shen Ge, and Xian Wu. Competence-based multimodal
    curriculum learning for medical report generation. In *Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    3001–3012, 2021a.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021a] 刘凤林、葛深和吴贤。基于能力的多模态课程学习用于医学报告生成。在*第59届计算语言学协会年会暨第11届国际联合自然语言处理会议（第一卷：长篇论文）*，页3001–3012，2021a。
- en: Liu et al. [2021b] Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou.
    Exploring and distilling posterior and prior knowledge for radiology report generation.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 13753–13762, 2021b.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021b] 刘凤林、吴贤、葛深、范伟和邹悦贤。探索和提炼放射学报告生成中的后验和先验知识。在*IEEE/CVF计算机视觉与模式识别会议论文集*，页13753–13762，2021b。
- en: 'Liu et al. [2021c] Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang,
    and Xu Sun. Contrastive attention for automatic chest x-ray report generation.
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP*, page
    269–280, 2021c.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021c] 刘凤林、尹常昌、吴贤、葛深、张平和孙旭。用于自动化胸部X光报告生成的对比注意力。在*计算语言学协会的发现：ACL-IJCNLP*，页269–280，2021c。
- en: Liu et al. [2021d] Fenglin Liu, Chenyu You, Xian Wu, Shen Ge, Xu Sun, et al.
    Auto-encoding knowledge graph for unsupervised medical report generation. *Advances
    in Neural Information Processing Systems*, 34:16266–16279, 2021d.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021d] 刘凤林、游晨宇、吴贤、葛深、孙旭等。用于无监督医学报告生成的自编码知识图谱。*神经信息处理系统进展*，34:16266–16279，2021d。
- en: 'Liu et al. [2021e] Guangyi Liu, Yinghong Liao, Fuyu Wang, Bin Zhang, Lu Zhang,
    Xiaodan Liang, Xiang Wan, Shaolin Li, Zhen Li, Shuixing Zhang, et al. Medical-vlbert:
    Medical visual language bert for covid-19 ct report generation with alternate
    learning. *IEEE Transactions on Neural Networks and Learning Systems*, 32(9):3786–3797,
    2021e.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021e] 刘广义、廖英红、王富宇、张斌、张璐、梁晓丹、万翔、李少林、李震、张水兴等。Medical-vlbert：用于COVID-19
    CT报告生成的医学视觉语言BERT与交替学习。*IEEE 神经网络与学习系统学报*，32(9):3786–3797，2021e。
- en: Liu et al. [2019] Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie
    Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. Clinically accurate
    chest x-ray report generation. In *Machine Learning for Healthcare Conference*,
    pages 249–269\. PMLR, 2019.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2019] 刘冠雄、许子铭·哈利·徐、马修·麦克德莫特、威利·博阿格、翁伟洪、彼得·索洛维茨和马尔兹耶·赫斯梅。临床准确的胸部X光报告生成。在*医疗保健机器学习大会*，页249–269。PMLR，2019。
- en: 'Liu et al. [2023b] Zhizhe Liu, Zhenfeng Zhu, Shuai Zheng, Yawei Zhao, Kunlun
    He, and Yao Zhao. From observation to concept: A flexible multi-view paradigm
    for medical report generation. *IEEE Transactions on Multimedia*, 2023b.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023b] 纸质刘、朱振丰、郑帅、赵雅伟、贺昆仑和赵耀。从观察到概念：一种灵活的多视角医学报告生成范式。*IEEE 多媒体学报*，2023b。
- en: Messina et al. [2022] Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia
    Besa, Sergio Uribe, Marcelo Andía, Cristian Tejos, Claudia Prieto, and Daniel
    Capurro. A survey on deep learning and explainability for automatic report generation
    from medical images. *ACM Computing Surveys (CSUR)*, 54(10s):1–40, 2022.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Messina 等人 [2022] 帕布罗·梅西纳、帕布罗·皮诺、丹尼斯·帕拉、阿尔瓦罗·索托、塞西莉亚·贝萨、塞尔吉奥·乌里贝、马塞洛·安迪亚、克里斯蒂安·特霍斯、克劳迪娅·普列托和丹尼尔·卡普罗。深度学习与可解释性在医学图像自动报告生成中的调查。*ACM
    计算机调查（CSUR）*，54(10s):1–40，2022。
- en: Mirikharaji et al. [2023] Zahra Mirikharaji, Kumar Abhishek, Alceu Bissoto,
    Catarina Barata, Sandra Avila, Eduardo Valle, M Emre Celebi, and Ghassan Hamarneh.
    A survey on deep learning for skin lesion segmentation. *Medical Image Analysis*,
    page 102863, 2023.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirikharaji 等人 [2023] 扎赫拉·米里哈拉吉、库马尔·阿比舍克、阿尔修·比索托、卡塔里娜·巴拉塔、桑德拉·阿维拉、爱德华多·瓦雷、M·埃姆雷·切莱比和加桑·哈马赫。深度学习在皮肤病变分割中的调查。*医学影像分析*，页102863，2023。
- en: 'Miura et al. [2021] Yasuhide Miura, Yuhao Zhang, Emily Tsai, Curtis Langlotz,
    and Dan Jurafsky. Improving factual completeness and consistency of image-to-text
    radiology report generation. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pages 5288–5304, 2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miura et al. [2021] Yasuhide Miura, Yuhao Zhang, Emily Tsai, Curtis Langlotz,
    和 Dan Jurafsky. 提高图像到文本放射学报告生成的事实完整性和一致性。在 *2021年北美计算语言学协会: 人类语言技术会议论文集*，页码 5288–5304，2021年。'
- en: Mohsan et al. [2022] Mashood Mohammad Mohsan, Muhammad Usman Akram, Ghulam Rasool,
    Norah Saleh Alghamdi, Muhammad Abdullah Aamer Baqai, and Muhammad Abbas. Vision
    transformer and language model based radiology report generation. *IEEE Access*,
    11:1814–1824, 2022.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohsan et al. [2022] Mashood Mohammad Mohsan, Muhammad Usman Akram, Ghulam Rasool,
    Norah Saleh Alghamdi, Muhammad Abdullah Aamer Baqai, 和 Muhammad Abbas. 基于视觉变换器和语言模型的放射学报告生成。*IEEE
    Access*，11:1814–1824，2022年。
- en: Moon et al. [2022] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim,
    and Edward Choi. Multi-modal understanding and generation for medical images and
    text via vision-language pre-training. *IEEE Journal of Biomedical and Health
    Informatics*, 26(12):6070–6080, 2022.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon et al. [2022] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim,
    和 Edward Choi. 通过视觉-语言预训练进行医学图像和文本的多模态理解与生成。*IEEE 生物医学与健康信息学期刊*，26(12):6070–6080，2022年。
- en: 'Najdenkoska et al. [2021] Ivona Najdenkoska, Xiantong Zhen, Marcel Worring,
    and Ling Shao. Variational topic inference for chest x-ray report generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part III 24*, pages 625–635\. Springer, 2021.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Najdenkoska et al. [2021] Ivona Najdenkoska, Xiantong Zhen, Marcel Worring,
    和 Ling Shao. 胸部 X 光报告生成的变分主题推断。在 *医学图像计算与计算机辅助手术–MICCAI 2021: 第24届国际会议，法国斯特拉斯堡，2021年9月27日至10月1日，论文集，第三部分
    24*，页码 625–635\. Springer，2021年。'
- en: Najdenkoska et al. [2022] Ivona Najdenkoska, Xiantong Zhen, Marcel Worring,
    and Ling Shao. Uncertainty-aware report generation for chest x-rays by variational
    topic inference. *Medical Image Analysis*, 82:102603, 2022.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Najdenkoska et al. [2022] Ivona Najdenkoska, Xiantong Zhen, Marcel Worring,
    和 Ling Shao. 通过变分主题推断进行胸部 X 光片的不确定性感知报告生成。*医学图像分析*，82:102603，2022年。
- en: 'Nobel et al. [2022] J Martijn Nobel, Koos van Geel, and Simon GF Robben. Structured
    reporting in radiology: a systematic review to explore its potential. *European
    radiology*, pages 1–18, 2022.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nobel et al. [2022] J Martijn Nobel, Koos van Geel, 和 Simon GF Robben. 放射学中的结构化报告:
    系统评估其潜力。*欧洲放射学*，页码 1–18，2022年。'
- en: 'OpenAI [2023] OpenAI. Chatgpt: Optimizing language models for dialogue. 2023.
    URL [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI [2023] OpenAI. Chatgpt: 优化对话的语言模型。2023年。网址 [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).'
- en: 'Pahwa et al. [2021] Esha Pahwa, Dwij Mehta, Sanjeet Kapadia, Devansh Jain,
    and Achleshwar Luthra. Medskip: Medical report generation using skip connections
    and integrated attention. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 3409–3415, 2021.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pahwa et al. [2021] Esha Pahwa, Dwij Mehta, Sanjeet Kapadia, Devansh Jain,
    和 Achleshwar Luthra. Medskip: 使用跳跃连接和集成注意力的医学报告生成。在 *IEEE/CVF 国际计算机视觉会议论文集*，页码
    3409–3415，2021年。'
- en: 'Pandey et al. [2021] Abhineet Pandey, Bhawna Paliwal, Abhinav Dhall, Ramanathan
    Subramanian, and Dwarikanath Mahapatra. This explains that: Congruent image–report
    generation for explainable medical image analysis with cyclic generative adversarial
    networks. In *Interpretability of Machine Intelligence in Medical Image Computing,
    and Topological Data Analysis and Its Applications for Medical Data: 4th International
    Workshop, iMIMIC 2021, and 1st International Workshop, TDA4MedicalData 2021, Held
    in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings
    4*, pages 34–43\. Springer, 2021.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pandey et al. [2021] Abhineet Pandey, Bhawna Paliwal, Abhinav Dhall, Ramanathan
    Subramanian, 和 Dwarikanath Mahapatra. 这解释了: 使用循环生成对抗网络进行可解释医学图像分析的相符图像-报告生成。在
    *医学图像计算中机器智能的可解释性，及其在医学数据中的拓扑数据分析与应用: 第4届国际研讨会 iMIMIC 2021 和第1届国际研讨会 TDA4MedicalData
    2021，MICCAI 2021 会议期间举行，法国斯特拉斯堡，2021年9月27日，论文集 4*，页码 34–43\. Springer，2021年。'
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318, 2002.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. Bleu: 一种自动评估机器翻译的方法。在 *第40届计算语言学协会年会论文集*，页码 311–318，2002年。'
- en: 'Pellegrini et al. [2023] Chantal Pellegrini, Matthias Keicher, Ege Özsoy, and
    Nassir Navab. Rad-restruct: A novel vqa benchmark and method for structured radiology
    reporting. In *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, pages 409–419\. Springer, 2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 佩勒格里尼等[2023] 香农·佩勒格里尼，马蒂亚斯·凯彻，埃格·厄兹索伊和纳西尔·纳瓦布。Rad-restruct：用于结构化放射学报告的新型VQA基准和方法。见于*医学图像计算和计算机辅助干预国际会议*，第409–419页。施普林格，2023年。
- en: 'Pino et al. [2021] Pablo Pino, Denis Parra, Cecilia Besa, and Claudio Lagos.
    Clinically correct report generation from chest x-rays using templates. In *Machine
    Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction
    with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 12*, pages
    654–663\. Springer, 2021.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮诺等[2021] 巴勃罗·皮诺，丹尼斯·帕拉，塞西莉亚·贝萨和克劳迪奥·拉戈斯。使用模板从胸部X光片生成临床正确的报告。见于*医学影像中的机器学习：第12届国际研讨会，MLMI
    2021，MICCAI 2021附属，法国斯特拉斯堡，2021年9月27日，会议录12*，第654–663页。施普林格，2021年。
- en: 'Platanios et al. [2019] Emmanouil Antonios Platanios, Otilia Stretcu, Graham
    Neubig, Barnabás Poczós, and Tom Mitchell. Competence-based curriculum learning
    for neural machine translation. In *Proceedings of the 2019 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long and Short Papers)*, pages 1162–1172, 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 普拉塔尼奥斯等[2019] 埃曼努伊尔·安东尼奥斯·普拉塔尼奥斯，奥蒂利亚·斯特雷茨库，格雷厄姆·纽比格，巴尔纳巴斯·波佐斯和汤姆·米切尔。基于能力的神经机器翻译课程学习。见于*2019年北美计算语言学协会：人类语言技术会议论文集，第1卷（长篇和短篇论文）*，第1162–1172页，2019年。
- en: 'Qin and Song [2022] Han Qin and Yan Song. Reinforced cross-modal alignment
    for radiology report generation. In *Findings of the Association for Computational
    Linguistics: ACL 2022*, pages 448–458, 2022.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秦汉和宋艳[2022] 汉秦和颜宋。用于放射学报告生成的强化跨模态对齐。见于*计算语言学协会会议成果：ACL 2022*，第448–458页，2022年。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等[2021] 亚历克·拉德福德，郑沃克·金，克里斯·哈拉西，阿迪提亚·拉梅什，加布里埃尔·戈赫，桑迪尼·阿加瓦尔，吉里什·萨斯特里，阿曼达·阿斯克尔，帕梅拉·米什金，杰克·克拉克等。从自然语言监督中学习可转移的视觉模型。见于*国际机器学习会议*，第8748–8763页。PMLR，2021年。
- en: Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image
    generation. In *International conference on machine learning*, pages 8821–8831\.
    Pmlr, 2021.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉梅什等[2021] 阿迪提亚·拉梅什，米哈伊尔·帕夫洛夫，加布里埃尔·戈赫，斯科特·格雷，切尔西·沃斯，亚历克·拉德福德，马克·陈，以及伊利亚·苏茨克弗。零样本文本到图像生成。见于*国际机器学习会议*，第8821–8831页。PMLR，2021年。
- en: Ramesh et al. [2022] Vignav Ramesh, Nathan A Chi, and Pranav Rajpurkar. Improving
    radiology report generation systems by removing hallucinated references to non-existent
    priors. In *Machine Learning for Health*, pages 456–473\. PMLR, 2022.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉梅什等[2022] 维格纳夫·拉梅什，内森·A·池和普拉纳夫·拉朱尔卡尔。通过去除对不存在的先验的幻觉引用来改进放射学报告生成系统。见于*健康机器学习*，第456–473页。PMLR，2022年。
- en: 'Ren et al. [2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
    r-cnn: Towards real-time object detection with region proposal networks. *Advances
    in neural information processing systems*, 28, 2015.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任等[2015] 任少卿，何开铭，罗斯·吉尔希克和孙剑。更快的R-CNN：通过区域提议网络实现实时对象检测。*神经信息处理系统进展*，28，2015年。
- en: Saab et al. [2024] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David
    Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al.
    Capabilities of gemini models in medicine. *arXiv preprint arXiv:2404.18416*,
    2024.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萨布等[2024] 哈立德·萨布，陶图，魏宏·翁，柳田野，大卫·斯图茨，埃勒里·伍尔钦，范张，蒂姆·斯特罗瑟，朴春钟，埃拉赫·韦达迪等。双子模型在医学中的能力。*arXiv预印本arXiv:2404.18416*，2024年。
- en: Selivanov et al. [2023] Alexander Selivanov, Oleg Y Rogov, Daniil Chesakov,
    Artem Shelmanov, Irina Fedulova, and Dmitry V Dylov. Medical image captioning
    via generative pretrained transformers. *Scientific Reports*, 13(1):4171, 2023.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 塞利瓦诺夫等[2023] 亚历山大·塞利瓦诺夫，奥列格·Y·罗戈夫，达尼尔·切萨科夫，阿尔乔姆·谢尔曼诺夫，伊琳娜·费杜洛娃和德米特里·V·迪洛夫。通过生成预训练变换器进行医学图像标注。*科学报告*，13(1):4171，2023年。
- en: 'Shamshad et al. [2023] Fahad Shamshad, Salman Khan, Syed Waqas Zamir, Muhammad Haris
    Khan, Munawar Hayat, Fahad Shahbaz Khan, and Huazhu Fu. Transformers in medical
    imaging: A survey. *Medical Image Analysis*, page 102802, 2023.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沙姆沙德等[2023] 法哈德·沙姆沙德，萨尔曼·汗，赛义德·瓦卡斯·扎米尔，穆罕默德·哈里斯·汗，穆纳瓦尔·哈亚特，法哈德·沙赫巴兹·汗和华柱·傅。医学影像中的变换器：一项调查。*医学图像分析*，第102802页，2023年。
- en: Shetty et al. [2023] Shashank Shetty, Ananthanarayana VS, and Ajit Mahale. Cross-modal
    deep learning-based clinical recommendation system for radiology report generation
    from chest x-rays. *International Journal of Engineering*, 2023.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shetty 等人 [2023] Shashank Shetty、Ananthanarayana VS 和 Ajit Mahale。基于跨模态深度学习的临床推荐系统，用于从胸部
    X 射线生成放射学报告。*国际工程杂志*，2023年。
- en: Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep
    convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 和 Zisserman [2014] Karen Simonyan 和 Andrew Zisserman。用于大规模图像识别的非常深度卷积网络。*arXiv
    预印本 arXiv:1409.1556*，2014年。
- en: 'Singh et al. [2021] Sonit Singh, Sarvnaz Karimi, Kevin Ho-Shon, and Len Hamey.
    Show, tell and summarise: learning to generate and summarise radiology findings
    from medical images. *Neural Computing and Applications*, 33:7441–7465, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人 [2021] Sonit Singh、Sarvnaz Karimi、Kevin Ho-Shon 和 Len Hamey。展示、讲述和总结：从医学图像中学习生成和总结放射学发现。*Neural
    Computing and Applications*，33:7441–7465，2021年。
- en: Singhal et al. [2023] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery
    Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal,
    et al. Towards expert-level medical question answering with large language models.
    *arXiv preprint arXiv:2305.09617*, 2023.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singhal 等人 [2023] Karan Singhal、Tao Tu、Juraj Gottweis、Rory Sayres、Ellery Wulczyn、Le
    Hou、Kevin Clark、Stephen Pfohl、Heather Cole-Lewis、Darlene Neal 等。通过大语言模型向专家级医学问题回答迈进。*arXiv
    预印本 arXiv:2305.09617*，2023年。
- en: Sirshar et al. [2022] Mehreen Sirshar, Muhammad Faheem Khalil Paracha, Muhammad Usman
    Akram, Norah Saleh Alghamdi, Syeda Zainab Yousuf Zaidi, and Tatheer Fatima. Attention
    based automated radiology report generation using cnn and lstm. *Plos one*, 17(1):e0262209,
    2022.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sirshar 等人 [2022] Mehreen Sirshar、Muhammad Faheem Khalil Paracha、Muhammad Usman
    Akram、Norah Saleh Alghamdi、Syeda Zainab Yousuf Zaidi 和 Tatheer Fatima。基于注意力的自动放射学报告生成，使用
    CNN 和 LSTM。*Plos one*，17(1):e0262209，2022年。
- en: Smit et al. [2020] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek,
    Andrew Y Ng, and Matthew Lungren. Combining automatic labelers and expert annotations
    for accurate radiology report labeling using bert. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages
    1500–1519, 2020.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smit 等人 [2020] Akshay Smit、Saahil Jain、Pranav Rajpurkar、Anuj Pareek、Andrew Y
    Ng 和 Matthew Lungren。结合自动标签器和专家注释以使用 BERT 进行准确的放射学报告标注。见于 *2020年自然语言处理经验方法会议（EMNLP）论文集*，第1500–1519页，2020年。
- en: Song et al. [2022] Xiao Song, Xiaodan Zhang, Junzhong Ji, Ying Liu, and Pengxu
    Wei. Cross-modal contrastive attention model for medical report generation. In
    *Proceedings of the 29th International Conference on Computational Linguistics*,
    pages 2388–2397, 2022.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人 [2022] Xiao Song、Xiaodan Zhang、Junzhong Ji、Ying Liu 和 Pengxu Wei。用于医学报告生成的跨模态对比注意力模型。见于
    *第29届国际计算语言学会议论文集*，第2388–2397页，2022年。
- en: Statistics [2020] Statistics. Diagnostic imaging dataset., 2020. URL [https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/](https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Statistics [2020] Statistics。诊断成像数据集，2020年。网址 [https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/](https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/)。
- en: 'Sun et al. [2022] Jinghan Sun, Dong Wei, Liansheng Wang, and Yefeng Zheng.
    Lesion guided explainable few weak-shot medical report generation. In *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th International
    Conference, Singapore, September 18–22, 2022, Proceedings, Part V*, pages 615–625\.
    Springer, 2022.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人 [2022] Jinghan Sun、Dong Wei、Liansheng Wang 和 Yefeng Zheng。病灶引导的可解释少样本医学报告生成。见于
    *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 第25届国际会议，新加坡，2022年9月18–22日，会议论文集，第五部分*，第615–625页。Springer，2022年。'
- en: Sun et al. [2019a] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution
    representation learning for human pose estimation. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, pages 5693–5703, 2019a.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2019a] Ke Sun、Bin Xiao、Dong Liu 和 Jingdong Wang。用于人体姿势估计的深度高分辨率表示学习。见于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，第5693–5703页，2019年。
- en: Sun et al. [2023] Zhaoyi Sun, Hanley Ong, Patrick Kennedy, Liyan Tang, Shirley
    Chen, Jonathan Elias, Eugene Lucas, George Shih, and Yifan Peng. Evaluating gpt-4
    on impressions generation in radiology reports. *Radiology*, 307(5):e231259, 2023.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2023] Zhaoyi Sun、Hanley Ong、Patrick Kennedy、Liyan Tang、Shirley Chen、Jonathan
    Elias、Eugene Lucas、George Shih 和 Yifan Peng。评估 GPT-4 在放射学报告中的印象生成。*Radiology*，307(5):e231259，2023年。
- en: 'Sun et al. [2019b] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang.
    Rotate: Knowledge graph embedding by relational rotation in complex space. *arXiv
    preprint arXiv:1902.10197*, 2019b.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2019b] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie 和 Jian Tang。Rotate：通过关系旋转在复杂空间中嵌入知识图谱。*arXiv
    预印本 arXiv:1902.10197*，2019b年。
- en: Szegedy et al. [2016] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, pages 2818–2826, 2016.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人 [2016] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens
    和 Zbigniew Wojna。重新思考计算机视觉中的 Inception 结构。见于 *IEEE 计算机视觉与模式识别会议论文集*，第 2818–2826
    页，2016年。
- en: 'Tan and Le [2019] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
    scaling for convolutional neural networks. In *International conference on machine
    learning*, pages 6105–6114\. PMLR, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan 和 Le [2019] Mingxing Tan 和 Quoc Le。EfficientNet：重新思考卷积神经网络的模型扩展。见于 *国际机器学习会议*，第
    6105–6114 页。PMLR，2019年。
- en: Tanida et al. [2023] Tim Tanida, Philip Müller, Georgios Kaissis, and Daniel
    Rueckert. Interactive and explainable region-guided radiology report generation.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 7433–7442, 2023.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanida 等人 [2023] Tim Tanida, Philip Müller, Georgios Kaissis 和 Daniel Rueckert。交互式和可解释的区域引导放射学报告生成。见于
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 7433–7442 页，2023年。
- en: 'Tanwani et al. [2022] Ajay K Tanwani, Joelle Barral, and Daniel Freedman. Repsnet:
    Combining vision with language for automated medical reports. In *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2022: 25th International Conference,
    Singapore, September 18–22, 2022, Proceedings, Part V*, pages 714–724\. Springer,
    2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanwani 等人 [2022] Ajay K Tanwani, Joelle Barral 和 Daniel Freedman。Repsnet：将视觉与语言结合用于自动化医疗报告。在
    *医学图像计算与计算机辅助干预–MICCAI 2022：第 25 届国际会议，2022年9月18–22日，新加坡，会议论文集，第五部分*，第 714–724
    页。Springer，2022年。
- en: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等人 [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth 等人。Gemini：一系列高度能力的多模态模型。*arXiv
    预印本 arXiv:2312.11805*，2023年。
- en: 'Topol [2019] Eric Topol. *Deep medicine: how artificial intelligence can make
    healthcare human again*. Hachette UK, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Topol [2019] Eric Topol。*深度医学：人工智能如何让医疗再次人性化*。Hachette UK，2019年。
- en: Tu et al. [2024] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed
    Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al.
    Towards generalist biomedical ai. *NEJM AI*, 1(3):AIoa2300138, 2024.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等人 [2024] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed
    Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena 等人。面向通用生物医学
    AI。*NEJM AI*，1(3):AIoa2300138，2024年。
- en: Van Miltenburg et al. [2018] Emiel Van Miltenburg, Desmond Elliott, and Piek
    Vossen. Measuring the diversity of automatic image descriptions. In *Proceedings
    of the 27th International Conference on Computational Linguistics*, pages 1730–1741,
    2018.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Miltenburg 等人 [2018] Emiel Van Miltenburg, Desmond Elliott 和 Piek Vossen。测量自动图像描述的多样性。见于
    *第 27 届国际计算语言学会议论文集*，第 1730–1741 页，2018年。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin。注意力机制是你所需要的一切。*神经信息处理系统进展*，30，2017年。
- en: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4566–4575, 2015.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vedantam 等人 [2015] Ramakrishna Vedantam, C Lawrence Zitnick 和 Devi Parikh。Cider：基于共识的图像描述评估。见于
    *IEEE 计算机视觉与模式识别会议论文集*，第 4566–4575 页，2015年。
- en: 'Wang et al. [2022a] Jun Wang, Abhir Bhalerao, and Yulan He. Cross-modal prototype
    driven network for radiology report generation. In *Computer Vision–ECCV 2022:
    17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXV*, pages 563–579\. Springer, 2022a.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2022a] Jun Wang, Abhir Bhalerao 和 Yulan He。基于跨模态原型驱动的网络用于放射学报告生成。见于
    *计算机视觉–ECCV 2022：第 17 届欧洲会议，2022年10月23–27日，以色列特拉维夫，会议论文集，第 XXXV 部分*，第 563–579
    页。Springer，2022a年。
- en: 'Wang et al. [2024a] Jun Wang, Abhir Bhalerao, Terry Yin, Simon See, and Yulan
    He. Camanet: class activation map guided attention network for radiology report
    generation. *IEEE Journal of Biomedical and Health Informatics*, 2024a.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024a] Jun Wang, Abhir Bhalerao, Terry Yin, Simon See, 和 Yulan
    He. Camanet：用于放射报告生成的类别激活图引导注意力网络。*IEEE生物医学与健康信息学期刊*，2024a。
- en: 'Wang et al. [2022b] Lin Wang, Munan Ning, Donghuan Lu, Dong Wei, Yefeng Zheng,
    and Jie Chen. An inclusive task-aware framework for radiology report generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    VIII*, pages 568–577\. Springer, 2022b.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022b] Lin Wang, Munan Ning, Donghuan Lu, Dong Wei, Yefeng Zheng,
    和 Jie Chen. 一种包容性的任务感知框架用于放射学报告生成。在*医学图像计算与计算机辅助干预–MICCAI 2022：第25届国际会议，新加坡，2022年9月18-22日，论文集，第八部分*中，第568-577页。Springer，2022b。
- en: 'Wang et al. [2023a] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang
    Shen. Chatcad: Interactive computer-aided diagnosis on medical image using large
    language models. *arXiv preprint arXiv:2302.07257*, 2023a.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023a] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, 和 Dinggang
    Shen. Chatcad：使用大型语言模型进行的医学图像交互计算机辅助诊断。*arXiv预印本arXiv:2302.07257*，2023a。
- en: Wang et al. [2023b] Siyuan Wang, Bo Peng, Yichao Liu, and Qi Peng. Fine-grained
    medical vision-language representation learning for radiology report generation.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 15949–15956, 2023b.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023b] Siyuan Wang, Bo Peng, Yichao Liu, 和 Qi Peng. 用于放射学报告生成的细粒度医学视觉-语言表示学习。在*2023年自然语言处理经验方法会议论文集*中，第15949-15956页，2023b。
- en: Wang et al. [2022c] Song Wang, Liyan Tang, Mingquan Lin, George Shih, Ying Ding,
    and Yifan Peng. Prior knowledge enhances radiology report generation. In *AMIA
    Annual Symposium Proceedings*, volume 2022, page 486\. American Medical Informatics
    Association, 2022c.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022c] Song Wang, Liyan Tang, Mingquan Lin, George Shih, Ying Ding,
    和 Yifan Peng. 先验知识提升放射学报告生成。在*AMIA年会论文集*中，2022卷，第486页。美国医学信息学协会，2022c。
- en: 'Wang et al. [2024b] Yixin Wang, Zihao Lin, Zhe Xu, Haoyu Dong, Jie Luo, Jiang
    Tian, Zhongchao Shi, Lifu Huang, Yang Zhang, Jianping Fan, et al. Trust it or
    not: Confidence-guided automatic radiology report generation. *Neurocomputing*,
    page 127374, 2024b.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024b] Yixin Wang, Zihao Lin, Zhe Xu, Haoyu Dong, Jie Luo, Jiang
    Tian, Zhongchao Shi, Lifu Huang, Yang Zhang, Jianping Fan，等. 信任与否：基于信心的自动放射报告生成。*Neurocomputing*，第127374页，2024b。
- en: Wang et al. [2023c] Yuhao Wang, Kai Wang, Xiaohong Liu, Tianrun Gao, Jingyue
    Zhang, and Guangyu Wang. Self adaptive global-local feature enhancement for radiology
    report generation. In *2023 IEEE International Conference on Image Processing
    (ICIP)*, pages 2275–2279\. IEEE, 2023c.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023c] Yuhao Wang, Kai Wang, Xiaohong Liu, Tianrun Gao, Jingyue
    Zhang, 和 Guangyu Wang. 自适应全局-局部特征增强用于放射报告生成。在*2023年IEEE国际图像处理会议（ICIP）*中，第2275-2279页。IEEE，2023c。
- en: Wang et al. [2021] Zhanyu Wang, Luping Zhou, Lei Wang, and Xiu Li. A self-boosting
    framework for automated radiographic report generation. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2433–2442,
    2021.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2021] Zhanyu Wang, Luping Zhou, Lei Wang, 和 Xiu Li. 一种自我增强框架用于自动化放射报告生成。在*IEEE/CVF计算机视觉与模式识别会议论文集*中，第2433-2442页，2021。
- en: 'Wang et al. [2022d] Zhanyu Wang, Hongwei Han, Lei Wang, Xiu Li, and Luping
    Zhou. Automated radiographic report generation purely on transformer: A multicriteria
    supervised approach. *IEEE Transactions on Medical Imaging*, 41(10):2803–2813,
    2022d.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022d] Zhanyu Wang, Hongwei Han, Lei Wang, Xiu Li, 和 Luping Zhou.
    完全基于变换器的自动放射报告生成：一种多标准监督方法。*IEEE医学成像交易*，41(10):2803-2813，2022d。
- en: 'Wang et al. [2022e] Zhanyu Wang, Mingkang Tang, Lei Wang, Xiu Li, and Luping
    Zhou. A medical semantic-assisted transformer for radiographic report generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    III*, pages 655–664\. Springer, 2022e.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2022e] Zhanyu Wang, Mingkang Tang, Lei Wang, Xiu Li, 和 Luping Zhou.
    一种用于放射报告生成的医学语义辅助变换器。在*医学图像计算与计算机辅助干预–MICCAI 2022：第25届国际会议，新加坡，2022年9月18-22日，论文集，第三部分*中，第655-664页。Springer，2022e。
- en: 'Wang et al. [2023d] Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou. Metransformer:
    Radiology report generation by transformer with multiple learnable expert tokens.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 11558–11567, 2023d.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] Joy Wu, Nkechinyere Agu, Ismini Lourentzou, Arjun Sharma, Joseph
    Paguio, Jasper Seth Yao, Edward Christopher Dee, William Mitchell, Satyananda
    Kashyap, Andrea Giovannini, et al. Chest imagenome dataset. *Physio Net*, 2021.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2022] Xing Wu, Jingwen Li, Jianjia Wang, and Quan Qian. Multimodal
    contrastive learning for radiology report generation. *Journal of Ambient Intelligence
    and Humanized Computing*, pages 1–10, 2022.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2023] Yuexin Wu, I-Chan Huang, and Xiaolei Huang. Token imbalance
    adaptation for radiology report generation. In *Conference on Health, Inference,
    and Learning*, pages 72–85\. PMLR, 2023.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. [2017] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and
    Kaiming He. Aggregated residual transformations for deep neural networks. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 1492–1500,
    2017.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2020] Chen Xu, Bojie Hu, Yufan Jiang, Kai Feng, Zeyang Wang, Shen
    Huang, Qi Ju, Tong Xiao, and Jingbo Zhu. Dynamic curriculum learning for low-resource
    neural machine translation. In *Proceedings of the 28th International Conference
    on Computational Linguistics*, pages 3977–3989, 2020.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2023] Dexuan Xu, Huashi Zhu, Yu Huang, Zhi Jin, Weiping Ding, Hang
    Li, and Menglong Ran. Vision-knowledge fusion model for multi-domain medical report
    generation. *Information Fusion*, 97:101817, 2023.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. [2024] Youyuan Xue, Yun Tan, Ling Tan, Jiaohua Qin, and Xuyu Xiang.
    Generating radiology reports via auxiliary signal guidance and a memory-driven
    network. *Expert Systems with Applications*, 237:121260, 2024.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2022] Bin Yan, Mingtao Pei, Meng Zhao, Caifeng Shan, and Zhaoxing
    Tian. Prior guided transformer for accurate radiology reports generation. *IEEE
    Journal of Biomedical and Health Informatics*, 26(11):5631–5640, 2022.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan [2022] Sixing Yan. Memory-aligned knowledge graph for clinically accurate
    radiology image report generation. In *Proceedings of the 21st Workshop on Biomedical
    Language Processing*, pages 116–122, 2022.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2023] Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and
    Lichao Sun. Multimodal chatgpt for medical applications: an experimental study
    of gpt-4v. *arXiv preprint arXiv:2310.19061*, 2023.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021a] Shaokang Yang, Jianwei Niu, Jiyan Wu, Yong Wang, Xuefeng
    Liu, and Qingfeng Li. Automatic ultrasound image report generation with adaptive
    multimodal attention mechanism. *Neurocomputing*, 427:40–49, 2021a.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2022] Shuxin Yang, Xian Wu, Shen Ge, S Kevin Zhou, and Li Xiao.
    Knowledge matters: Chest radiology report generation with general and specific
    knowledge. *Medical Image Analysis*, 80:102510, 2022.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2023] Shuxin Yang, Xian Wu, Shen Ge, Zhuozhao Zheng, S Kevin Zhou,
    and Li Xiao. Radiology report generation with a learned knowledge base and multi-modal
    alignment. *Medical Image Analysis*, 86:102798, 2023.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021b] Yan Yang, Jun Yu, Jian Zhang, Weidong Han, Hanliang Jiang,
    and Qingming Huang. Joint embedding of deep visual and semantic features for medical
    image report generation. *IEEE Transactions on Multimedia*, 2021b.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [2021] Di You, Fenglin Liu, Shen Ge, Xiaoxia Xie, Jing Zhang, and
    Xian Wu. Aligntransformer: Hierarchical alignment of visual regions and disease
    tags for medical report generation. In *Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part III 24*, pages 72–82\. Springer, 2021.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [2022] Jingyi You, Dongyuan Li, Manabu Okumura, and Kenji Suzuki.
    Jpg-jointly learn to align: Automated disease prediction and radiology report
    generation. In *Proceedings of the 29th International Conference on Computational
    Linguistics*, pages 5989–6001, 2022.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023a] Junsan Zhang, Xiuxuan Shen, Shaohua Wan, Sotirios K Goudos,
    Jie Wu, Ming Cheng, and Weishan Zhang. A novel deep learning model for medical
    report generation by inter-intra information calibration. *IEEE Journal of Biomedical
    and Health Informatics*, 2023a.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023b] Ke Zhang, Hanliang Jiang, Jian Zhang, Qingming Huang, Jianping
    Fan, Jun Yu, and Weidong Han. Semi-supervised medical report generation via graph-guided
    hybrid feature consistency. *IEEE Transactions on Multimedia*, 2023b.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Yuille,
    and Daguang Xu. When radiology report generation meets knowledge graph. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 34, pages 12910–12917,
    2020.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022] Yong Zhang, Weihua Ou, Jiacheng Zhang, and Jiaxin Deng.
    Category supervised cross-modal hashing retrieval for chest x-ray and radiology
    reports. *Computers & Electrical Engineering*, 98:107673, 2022.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2016] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. Learning deep features for discriminative localization. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 2921–2929,
    2016.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2021] Yi Zhou, Lei Huang, Tao Zhou, Huazhu Fu, and Ling Shao. Visual-textual
    attentive semantic consistency for medical report generation. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 3985–3994,
    2021.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*, pages
    2223–2232, 2017.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table A1: Summary of papers in the survey. The overview is based on the findings
    of the survey. The dataset and metrics section focuses on mainstream datasets
    and metrics. ’–’ indicates that the paper either does not detail this process
    or does not include the key techniques summarized in this survey. The following
    abbreviations are used: I-Archi: the architecture of image feature learning, I-Module:
    the enhancement module of image feature learning, NI: the feature learning of
    non-image data, GEI: gastrointestinal endoscope image, RetiI: retinal image, Term:
    terminology, KnowB: knowledge base, RealR: real report, ClinicalI: clinical information,
    Ques: questionnaires, FreFilter: frequency-based filtering, ConAtoL: converting
    all tokens to lowercase, RemoNAT: removing non-alphabetic tokens, AT: auxiliary
    task, ContrasL: Contrastive learning, MM: memory metric, FeatO: feature-level
    operation, OptimS: optimization strategies, H-LSTM: hierarchical LSTM, ReLoss:
    re-weighted loss function, ReinL: reinforcement learning, R-L: Rouge-L, C-D: CIDEr-D,
    CE: clinical efficacy, Com: comparison, Clas: classification, Escore: error scoring,
    ECI: extracting case-related information, IU: IU X-Ray, MIMIC: MIMIC-CXR, ImaGeno:
    Chest ImaGenome, COV: COV-CTR, DeepEye: DeepEyeNet, CCCT: Chinese COVID-19 CT,
    Ret-I: Retina ImBank, and Ret-C: Retina Chinese. In addition, AT-Graph, AT-Class,
    AT-EC, and AT-DS mean the graph-based, classification, embedding comparison, and
    detection/segmentation auxiliary tasks.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Input data | Data preparation | Feature Learning | Feature Fusion
    | Generation | Training Strategy | Datasets | Metrics |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| I-Archi | I-Module | NI |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021b](#bib.bib69)] | Chest X-ray, Term,'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: RealR | Tokenizing, ConAtoL,
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | AT-Graph | Transformer | FeatO | Transformer | – | IU,
    MIMIC | BLEU, R-L,
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2021d](#bib.bib71)] | Chest X-ray | – | ResNet | AT-Graph |
    – | – | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2021a](#bib.bib68)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | CNNs | – | – | – | LSTMs | Curricu- lum lea-
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: rning | IU, MIMIC | BLEU, R-L,
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '| Chen et al. [[2021](#bib.bib15)] | Chest X-ray | – | ResNet+ Transformer
    | – | – | MM | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '| You et al. [[2021](#bib.bib146)] | Chest X-ray | – | ResNet | – | – | Attention
    | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '| Miura et al. [[2021](#bib.bib77)] | Chest X-ray | – | DenseNet+ Transformer
    | – | – | – | Transformer | ReinL | MIMIC | BLEU, C-D,'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '| Alfarghaly et al. [[2021](#bib.bib3)] | Chest X-ray | Resizing | DenseNet
    | AT-Class | – | Attention | Transformer | – | IU | BLEU, R-L,'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Clas |
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2021b](#bib.bib145)] | Chest X-ray | – | ResNet | AT-EC | –
    | – | H-LSTM | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '| Pahwa et al. [[2021](#bib.bib84)] | Chest X-ray | Resizing, Cropping,'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Flipping | HRNet+ Transformer | – | – | – | R2Gen | – | IU | BLEU, R-L,
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhou et al. [[2021](#bib.bib153)] | Chest X-ray, ClinicalI | Tokenizing,
    FreFilter | DenseNet | AT-Class, AT-EC | One-hot, BioSentVec | Attention | H-LSTM
    | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: nKTD |
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2021b](#bib.bib35)] | RetiI, Term | – | CNNs | – | Embedding
    layer | FeatO | LSTM | – | DeepEye | BLEU, R-L,'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '| Han et al. [[2021](#bib.bib26)] | Spine MRI | – | Self-design | AT-DS | –
    | – | Reasoning | – | – | – |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021e](#bib.bib72)] | Chest X-ray, Chest CT,'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Term | – | DenseNet | AT-Class | BERT | FeatO, Attention | Transformer | AT
    | CCCT | BLEU, R-L,
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2021](#bib.bib128)] | Chest X-ray, Chest CT | Tokenizing |
    ResNet+ Transformer | AT-EC | – | – | H-LSTM | AT | IU, COV | BLEU, R-L,'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '| Endo et al. [[2021](#bib.bib23)] | Chest X-ray | – | ResNet | – | – | – |
    Retrieval | – | MIMIC | BLEU, $S_{emb}$,'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '| Najdenkoska et al. [[2021](#bib.bib80)] | Chest X-ray | Resizing, Tokenizing,'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet+ Transformer | AT-EC | – | – | LSTM | – | IU, MIMIC | BLEU,
    R-L,
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2021a](#bib.bib142)] | Breast ultrasound | Tokenizing, FreFilter
    | ResNet | AT-Class | – | FeatO | LSTM | – | BCD2018 | BLEU, R-L,'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '| Pandey et al. [[2021](#bib.bib85)] | Chest X-ray | Resizing | VGG | – | –
    | – | H-LSTM | AT | IU | BLEU, R-L |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| Hou et al. [[2021b](#bib.bib32)] | Chest X-ray | Resizing, Tokenizing,'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | AT-Class | – | LSTM | H-LSTM | ReinL | IU, MIMIC | BLEU,
    R-L,
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '| Pino et al. [[2021](#bib.bib88)] | Chest X-ray | – | DenseNet | – | – | –
    | Template | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: MIRQI |
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2021c](#bib.bib70)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | ContrasL | – | – | H-LSTM | – | IU, MIMIC | BLEU, R-L,
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '| Jia et al. [[2021](#bib.bib41)] | Chest X-ray | – | ResNet+ Transformer |
    – | – | – | Transformer | – | IU, MIMIC | BLEU, R-L |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| Hou et al. [[2021a](#bib.bib31)] | Chest X-ray | Resizing, Data'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: augmentation | DenseNet | – | – | – | Transformer | – | MIMIC | BLEU, R-L,
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2021a](#bib.bib34)] | RetiI, Term | Tokenizing, ConAtoL,'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | CNNs | – | Embedding layer | LSTM | LSTM | – | DeepEye | BLEU, R-L,
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '| Babar et al. [[2021a](#bib.bib4)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | – | – | – | – | Uncondition | – | IU, MIMIC | BLEU, R-L,
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '| Singh et al. [[2021](#bib.bib100)] | Chest X-ray | Resizing, Tokenizing,'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | InceptionV3 | AT-Class | – | – | LSTM | – | IU, MIMIC | BLEU, R-L,
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2022](#bib.bib143)] | Chest X-ray, KnowB,'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: RealR | Resizing, Tokenizing,
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | – | RotatE, ECI,
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: BERT | Attention | Transformer | – | IU, MIMIC | BLEU, R-L,
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '| Ramesh et al. [[2022](#bib.bib93)] | Chest X-ray | Tokenizing, Filtering
    | ResNet/ Transformer | – | – | – | Retrieval | – | MIMIC | $S_{emb}$ |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
- en: '| Sirshar et al. [[2022](#bib.bib102)] | Chest X-ray | Tokenizing, RemoNAT,'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL | VGG | – | – | – | LSTM | – | IU, MIMIC | BLEU |
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '| Najdenkoska et al. [[2022](#bib.bib81)] | Chest X-ray | Resizing, RemoNAT,'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet+ Transformer | AT-EC | – | – | Transformer, LSTM | – |
    IU, MIMIC | BLEU, R-L,
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '%Novel,'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022c](#bib.bib125)] | Chest X-ray | Cropping, Tokenizing,'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet | AT-Graph | – | – | H-LSTM | – | IU | BLEU, R-L,
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2022b](#bib.bib59)] | RetiI | Resizing, Tokenizing,'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | I3D+ Transformer | AT-Graph | – | – | Transformer | – | FFA-IR |
    BLEU, R-L,
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: '| Qin and Song [[2022](#bib.bib90)] | Chest X-ray | – | ResNet+ Transformer
    | – | – | MM | Transformer | ReinL | IU, MIMIC | BLEU, R-L,'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022a](#bib.bib120)] | Chest X-ray | Resizing, Cropping | ResNet+
    Transformer | ContrasL | – | MM | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '| Cao et al. [[2022](#bib.bib9)] | Chest X-ray, GEI,'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: Term | – | DenseNet+ Transformer | AT-Graph | BERT | FeatO, Attention | Transformer
    | – | IU | BLEU, R-L,
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '| Mohsan et al. [[2022](#bib.bib78)] | Chest X-ray | Tokenizing, ConAtoL |
    Transformer | – | – | – | Transformer | – | IU | BLEU, R-L,'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '| Yan [[2022](#bib.bib140)] | Chest X-ray | – | DenseNet+ Transformer | AT-Graph,
    MM | – | – | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: MIRQI |
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2022a](#bib.bib56)] | Chest X-ray | – | ResNet+ Transformer |
    AT-Class | – | – | Transformer | AT | IU | BLEU, R-L,'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '| Sun et al. [[2022](#bib.bib106)] | RetiI | Resizing | ResNet+ Faster-RCNN
    | AT-EC, AT-DS | – | – | Transformer | – | FFA-IR | BLEU, R-L,'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '| You et al. [[2022](#bib.bib147)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | ResNet+ Transformer | AT-Class | – | Attention, MM | Transformer |
    – | IU | BLEU, R-L,
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022d](#bib.bib129)] | Chest X-ray | Resizing | Transformer
    | AT-Class, AT-EC | – | – | Transformer | ReLoss | IU, MIMIC | BLEU, R-L,'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: '| Lee et al. [[2022](#bib.bib53)] | Chest X-ray | Resizing, Cropping,'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Flipping | ResNet+ Transformer | – | – | – | R2Gen | – | IU | BLEU, R-L,
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '| Moon et al. [[2022](#bib.bib79)] | Chest X-ray | Resizing, Cropping,'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing | ResNet+ Transformer | AT | – | – | Transformer | – | IU, MIMIC
    | BLEU, CE |
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2022](#bib.bib36)] | RetiI, Term | Resizing, Tokenizing,'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | CNNs | – | Embedding layer | Attention | LSTM | – | DeepEye | BLEU,
    R-L,
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: '| Kaur and Mittal [[2022a](#bib.bib46)] | Chest X-ray | Resizing, Convert image
    to'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: grayscale,
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: Flipping,
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing,
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | VGG | – | – | – | H-LSTM | – | IU | BLEU, R-L,
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '| Song et al. [[2022](#bib.bib104)] | Chest X-ray, RealR | – | DenseNet | ContrasL
    | – | Attention | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: '| Jia et al. [[2022](#bib.bib42)] | Chest X-ray | – | DenseNet | – | – | Attention
    | H-LSTM | – | IU, MIMIC | BLEU, R-L |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2022](#bib.bib151)] | Chest X-ray | Resizing | VGG | – | –
    | – | Retrieval | – | MIMIC | Precision |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
- en: '| Dalla Serra et al. [[2022](#bib.bib18)] | Chest X-ray, ClinicalI | Tokenizing,
    Resizing,'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Flipping,
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: Rotation,
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Cropping | ResNet+ Transformer | AT-Graph | Embedding layer | Attention | Transformer
    | – | MIMIC | BLEU, R-L,
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: '| Yan et al. [[2022](#bib.bib139)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | AT | – | Attention | Transformer | – | IU,
    MIMIC | BLEU, R-L,
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: '| Gajbhiye et al. [[2022](#bib.bib24)] | Chest X-ray | Tokenizing, Removing
    irrelevant elements, ConAtoL,'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet | AT-Class | – | – | LSTM | ReLoss | IU | BLEU, R-L,
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022b](#bib.bib122)] | Chest X-ray | Grouping | ResNet+ Transformer
    | AT-Class | – | – | R2Gen | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: '| Abela et al. [[2022](#bib.bib1)] | Chest X-ray | – | DenseNet | – | – | –
    | Template | – | MIMIC | BLEU, CE |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022](#bib.bib133)] | Chest X-ray | – | ResNet | ContrasL | –
    | – | LSTM | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: '| Tanwani et al. [[2022](#bib.bib113)] | Chest X-ray, Ques | Resizing, Image
    transformations,'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing | ResNeXt | AT-Class, ContrasL | BERT | Attention | Transformer |
    – | IU | BLEU |
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
- en: '| Chen et al. [[2022](#bib.bib13)] | Chest X-ray | Resizing, Tokenizing,'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | ResNet+ Transformer | AT-EC MM | – | – | R2Gen | – | IU | BLEU, R-L,
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '| Kong et al. [[2022](#bib.bib50)] | Chest X-ray | Resizing | Transformer |
    – | – | – | Retrieval | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022e](#bib.bib130)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | Transformer | AT-Class, ContrasL | – | FeatO | Transformer | – |
    MIMIC | BLEU, R-L,
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: '| Du et al. [[2022](#bib.bib22)] | Chest X-ray | – | ResNet | AT-Class | –
    | – | H-LSTM | – | IU | BLEU, R-L,'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: '| Kaur and Mittal [[2022b](#bib.bib47)] | Chest X-ray | Resizing, Tokenizing,'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | VGG | AT-Class | – | FeatO | H-LSTM | ReinL | IU | BLEU, R-L,
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
- en: '| Tanida et al. [[2023](#bib.bib112)] | Chest X-ray | Resizing, Data'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: augmentation,
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Removing redundant whitespaces | ResNet+ Faster-RCNN | AT-DS | – | – | Transformer
    | – | MIMIC, ImaGeno | BLEU, R-L,
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2023d](#bib.bib131)] | Chest X-ray | Tokenizing | Transformer
    | AT | – | – | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023b](#bib.bib61)] | Chest X-ray, Chest CT | Resizing, Tokenizing,'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet | AT-Graph | – | Attention | Transformer | – | IU, COV
    | BLEU, R-L,
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: '| Wu et al. [[2023](#bib.bib134)] | Chest X-ray | Tokenizing, ConAtoL,'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: Removing irrelevant elements,
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | – | – | – | Transformer | ReinL, AT | IU,
    MIMIC | BLEU, R-L,
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023a](#bib.bib60)] | Chest X-ray, KnowB,'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: RealR | Tokenizing | Transformer | AT-Class, ContrasL | ECI, BERT | Attention
    | Transformer | – | IU, MIMIC | BLEU, R-L,
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2023](#bib.bib37)] | Chest X-ray, KnowB | Tokenizing, ConAtoL,'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | – | ECI, BERT | FeatO, Attention | Transformer
    | – | IU, MIMIC | BLEU, R-L,
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhang et al. [[2023b](#bib.bib149)] | Chest X-ray | – | DenseNet+ Transformer
    | AT-Graph, AT-Class,'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: AT-EC | – | – | Transformer | AT | IU, MIMIC | BLEU, R-L,
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: '| Xu et al. [[2023](#bib.bib137)] | Chest X-ray, Dermoscopy,'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: KnowB | – | DenseNet+ Transformer | AT-Class | ECI, BERT | Attention | R2Gen
    | – | IU | BLEU, R-L,
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2023](#bib.bib144)] | Chest X-ray, KnowB | Resizing, Tokenizing,'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | AT-Class, AT-EC | – | Attention | Transformer | – | IU,
    MIMIC | BLEU, C-D,
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhang et al. [[2023a](#bib.bib148)] | Chest X-ray, Chest CT | Resizing |
    ResNet+ Transformer | – | – | – | Transformer+ MM | – | IU, MIMIC,'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: COV | BLEU, R-L,
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2023a](#bib.bib123)] | Chest X-ray | – | ResNet+ Transformer
    | AT-Class, AT-DS | – | – | R2Gen +LLM | – | MIMIC | CE |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[2023](#bib.bib66)] | Chest X-ray, RetiI | Resizing, Tokenizing,'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | ContrasL | – | – | Transformer | – | IU, MIMIC,
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: Ret-I,
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: Ret-C | BLEU, R-L,
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: '| Selivanov et al. [[2023](#bib.bib96)] | Chest X-ray | Resizing, Tokenizing,'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | DenseNet | – | – | – | Transformer | – | IU, MIMIC | BLEU, R-L,
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: '| Cao et al. [[2023](#bib.bib10)] | Chest X-ray, GEI,'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
- en: Term | – | DenseNet+ Transformer | – | BERT | FeatO, Attention,
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: MM | Transformer | – | IU, MIMIC | BLEU, R-L,
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: '| Shetty et al. [[2023](#bib.bib98)] | Chest X-ray | – | Self-design | – |
    – | – | LSTM | – | IU | BLEU |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023c](#bib.bib127)] | Chest X-ray | - | ResNet+ Faster-RCNN+'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: Transformer | – | – | – | R2Gen | – | IU, MIMIC,
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
- en: ImaGeno | BLEU, R-L,
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023d](#bib.bib63)] | Chest X-ray | Resizing, Cropping,'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing,
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter,
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | Transformer | – | – | MM +Self-design | Transformer | AT | IU, MIMIC
    | BLEU, R-L,
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2023b](#bib.bib74)] | Chest X-ray, Term,'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
- en: RealR | FreFilter, ConAtol | DenseNet | ContrasL | Transformer | FeatO, Attention
    | Transformer | – | IU, MIMIC | BLEU, R-L,
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2023b](#bib.bib124)] | Chest X-ray | – | ResNet+ Transformer
    | ContrasL | – | – | R2Gen | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: '| Pellegrini et al. [[2023](#bib.bib87)] | Chest X-ray, Ques | – | EfficientNet
    | – | BERT | Attention | Classification | - | Rad-ReStruct | CE |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
- en: '| Dalla Serra et al. [[2023a](#bib.bib19)] | Chest X-ray, ClinicalI | Resizing,
    Cropping,'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: Grouping | ResNet+ Faster-RCNN | AT-DS | Embedding layer | Attention | Transformer
    | – | MIMIC, ImaGeno | BLEU, R-L,
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023c](#bib.bib62)] | Chest X-ray, RealR,'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: Term | Resizing, Self-design | ResNet+ Transformer | - | ECI, BERT | Attention
    | Transformer | - | IU, MIMIC | BLEU, R-L,
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: '| Dalla Serra et al. [[2023b](#bib.bib20)] | Chest X-ray, ClinicalI | Resizing,
    Cropping | ResNet+ Faster-RCNN | AT-DS, AT-Graph | Embedding layer | Attention
    | Transformer | - | MIMIC, ImaGeno | BLEU, R-L,'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: '| Jeong et al. [[2024](#bib.bib40)] | Chest X-ray | Resizing | Transformer
    | – | – | – | Retrieval | – | MIMIC | BLEU, Escore |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
- en: '| Gu et al. [[2024](#bib.bib25)] | Chest X-ray, Term | Resizing, Cropping,'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: Flipping,
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter,
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | ResNet | AT-DS, AT | Transformer | FeatO, Attention | Transformer
    | ReinL | IU, MIMIC | BLEU, R-L,
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2024b](#bib.bib126)] | Chest X-ray, Chest CT | – | ResNet+
    Transformer | AT | – | – | Transformer | ReLoss | IU, COV | BLEU, R-L,'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: Grading |
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
- en: '| Xue et al. [[2024](#bib.bib138)] | Chest X-ray, Term | - | ResNet+ Transformer
    | - | Transformer, Attention | FeatO, Attention | Transformer | - | IU, MIMIC
    | BLEU, R-L,'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2024a](#bib.bib121)] | Chest X-ray | Resizing, Cropping | DenseNet+
    Transformer | AT-Class | – | – | R2Gen | AT | IU, MIMIC | BLEU, R-L,'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: '| Jin et al. [[2024](#bib.bib43)] | Chest X-ray, RealR | – | ResNet | AT-Class
    | Transformer | FeatO, Attention | Transformer | – | IU, MIMIC | BLEU, R-L,'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: 'Table A2: Comparisons of the model performance on the MIMIC-CXR Dataset. B1,
    B2, B3, B4, R-L, C-D, P, R, and F represent BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L,
    CIDEr-D, precision, recall, and F1 score, respectively. The best and second best
    results are highlighted. All values were extracted from their papers.'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | B1$\uparrow$ | B2$\uparrow$ | B3$\uparrow$ | B4$\uparrow$ | R-L$\uparrow$
    | METEOR$\uparrow$ | C-D$\uparrow$ | P$\uparrow$ | R$\uparrow$ | F$\uparrow$ |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
- en: '| Findings Section |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2021](#bib.bib15)] | 0.353 | 0.218 | 0.148 | 0.106 | 0.278
    | 0.142 | - | 0.334 | 0.275 | 0.278 |'
  id: totrans-676
  prefs: []
  type: TYPE_TB
- en: '| Pino et al. [[2021](#bib.bib88)] | - | - | - | - | 0.185 | - | 0.238 | 0.381
    | 0.531 | 0.428 |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[2022](#bib.bib104)] | 0.360 | 0.227 | 0.156 | 0.117 | 0.287
    | 0.148 | - | 0.444 | 0.297 | 0.356 |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
- en: '| Impression + Findings Section |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022](#bib.bib133)] | 0.340 | 0.212 | 0.145 | 0.103 | 0.270 |
    0.139 | 0.109 | - | - | - |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022d](#bib.bib129)] | 0.351 | 0.223 | 0.157 | 0.118 | 0.287
    | - | 0.281 | - | - | - |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
- en: '| Jia et al. [[2022](#bib.bib42)] | 0.363 | 0.228 | 0.156 | 0.130 | 0.300 |
    - | - | - | - | - |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
- en: '| Unspecified generated sections |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021a](#bib.bib68)] | 0.344 | 0.217 | 0.140 | 0.097 | 0.218
    | 0.133 | - | - | - | - |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021c](#bib.bib70)] | 0.350 | 0.219 | 0.152 | 0.109 | 0.283
    | 0.151 | - | 0.352 | 0.298 | 0.303 |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021b](#bib.bib69)] | 0.360 | 0.224 | 0.149 | 0.106 | 0.284
    | 0.149 | - | - | - | - |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[2023c](#bib.bib62)] | 0.360 | 0.231 | 0.162 | 0.119 | 0.298 |
    0.153 | 0.217 | - | - | - |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[2023](#bib.bib66)] | 0.362 | 0.227 | 0.155 | 0.113 | 0.283 |
    0.142 | - | - | - | - |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2023b](#bib.bib149)] | 0.362 | 0.229 | 0.157 | 0.113 | 0.284
    | 0.153 | - | 0.380 | 0.342 | 0.335 |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[2022](#bib.bib143)] | 0.363 | 0.228 | 0.156 | 0.115 | 0.284
    | - | 0.203 | 0.458 | 0.348 | 0.371 |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023c](#bib.bib127)] | 0.363 | 0.235 | 0.164 | 0.118 | 0.301
    | 0.136 | - | - | - | - |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021d](#bib.bib71)] | 0.369 | 0.231 | 0.156 | 0.118 | 0.295
    | 0.153 | - | 0.389 | 0.362 | 0.355 |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
- en: '| Xue et al. [[2024](#bib.bib138)] | 0.372 | 0.233 | 0.154 | 0.112 | 0.286
    | 0.152 | - | - | - | - |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2024a](#bib.bib121)] | 0.374 | 0.230 | 0.155 | 0.112 | 0.279
    | 0.145 | 0.161 | 0.483 | 0.323 | 0.387 |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2023a](#bib.bib148)] | 0.376 | 0.233 | 0.157 | 0.113 | 0.276
    | 0.144 | - | - | - | - |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
- en: '| You et al. [[2021](#bib.bib146)] | 0.378 | 0.235 | 0.156 | 0.112 | 0.283
    | 0.158 | - | - | - | - |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
- en: '| Qin and Song [[2022](#bib.bib90)] | 0.381 | 0.232 | 0.155 | 0.109 | 0.287
    | 0.151 | - | 0.342 | 0.294 | 0.292 |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2023](#bib.bib134)] | 0.383 | 0.224 | 0.146 | 0.104 | 0.280 |
    0.147 | - | - | - | 0.758 |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[2023](#bib.bib144)] | 0.386 | 0.237 | 0.157 | 0.111 | 0.274
    | - | 0.111 | 0.420 | 0.339 | 0.352 |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023b](#bib.bib124)] | - | - | - | 0.119 | 0.286 | 0.158 |
    0.259 | - | - | - |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023d](#bib.bib131)] | 0.386 | 0.250 | 0.169 | 0.124 | 0.291
    | 0.152 | 0.362 | 0.364 | 0.309 | 0.311 |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2023b](#bib.bib74)] | 0.391 | 0.249 | 0.172 | 0.125 | 0.304
    | 0.160 | - | - | - | - |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[2023](#bib.bib37)] | 0.393 | 0.243 | 0.159 | 0.113 | 0.285
    | 0.160 | - | 0.371 | 0.318 | 0.321 |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022b](#bib.bib122)] | 0.395 | 0.253 | 0.170 | 0.121 | 0.284
    | 0.147 | - | - | - | - |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2024](#bib.bib43)] | 0.398 | - | - | 0.112 | 0.268 | 0.157 |
    - | 0.501 | 0.509 | 0.476 |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022e](#bib.bib130)] | 0.413 | 0.266 | 0.186 | 0.136 | 0.298
    | 0.170 | 0.429 | - | - | - |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
- en: '| Kong et al. [[2022](#bib.bib50)] | 0.423 | 0.261 | 0.171 | 0.116 | 0.286
    | 0.168 | - | 0.482 | 0.563 | 0.519 |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
