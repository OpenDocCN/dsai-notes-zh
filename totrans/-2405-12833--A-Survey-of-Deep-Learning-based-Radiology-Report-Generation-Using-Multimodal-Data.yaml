- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:32:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2405.12833] A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.12833](https://ar5iv.labs.arxiv.org/html/2405.12833)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xinyi Wang, Grazziela Figueredo, Ruizhe Li
  prefs: []
  type: TYPE_NORMAL
- en: The University of Nottingham
  prefs: []
  type: TYPE_NORMAL
- en: United Kingdom
  prefs: []
  type: TYPE_NORMAL
- en: \AndWei Emma Zhang, Weitong Chen
  prefs: []
  type: TYPE_NORMAL
- en: The University of Adelaide
  prefs: []
  type: TYPE_NORMAL
- en: Australia
  prefs: []
  type: TYPE_NORMAL
- en: \AndXin Chen
  prefs: []
  type: TYPE_NORMAL
- en: The University of Nottingham
  prefs: []
  type: TYPE_NORMAL
- en: 'United Kingdom Corresponding author: Xin Chen, xin.chen@nottingham.ac.uk'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Automatic radiology report generation can alleviate the workload for physicians
    and minimize regional disparities in medical resources, therefore becoming an
    important topic in the medical image analysis field. It is a challenging task,
    as the computational model needs to mimic physicians to obtain information from
    multi-modal input data (i.e., medical images, clinical information, medical knowledge,
    etc.), and produce comprehensive and accurate reports. Recently, numerous works
    emerged to address this issue using deep learning-based methods, such as transformers,
    contrastive learning, and knowledge-base construction. This survey summarizes
    the key techniques developed in the most recent works and proposes a general workflow
    for deep learning-based report generation with five main components, including
    multi-modality data acquisition, data preparation, feature learning, feature fusion/interaction,
    and report generation. The state-of-the-art methods for each of these components
    are highlighted. Additionally, training strategies, public datasets, evaluation
    methods, current challenges, and future directions in this field are summarized.
    We have also conducted a quantitative comparison between different methods under
    the same experimental setting. This is the most up-to-date survey that focuses
    on multi-modality inputs and data fusion for radiology report generation. The
    aim is to provide comprehensive and rich information for researchers interested
    in automatic clinical report generation and medical image analysis, especially
    when using multimodal inputs, and assist them in developing new algorithms to
    advance the field.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords Report generation $\cdot$ Deep learning $\cdot$ Multimodal $\cdot$
    Medical image analysis'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Medical images can offer detailed insights into bodies and help physicians screen,
    diagnose and monitor medical conditions without requiring invasive techniques
    (Beddiar et al., [2023](#bib.bib7); Liao et al., [2023](#bib.bib64)). Radiologists
    summarize the information extracted from medical imaging into radiological reports
    for clinical decision-making. The manual generation of reports is however labour-intensive,
    time-consuming, and requires extensive expertise (Beddiar et al., [2023](#bib.bib7)).
    Topol ([2019](#bib.bib115)) points out that the demand for medical image explanation
    greatly surpasses the current capacity of physicians in the United States. During
    an epidemic and with ageing populations, the situation can get worse. During the
    Covid-19 pandemic, for instance, in the UK, each radiologist was estimated to
    report as many as 100 images each day (Statistics, [2020](#bib.bib105)). This
    makes it challenging for radiologists to provide high-quality reports within the
    scheduled time. The current demand extends patient waiting time and increases
    the risk of disease transmission (Beddiar et al., [2023](#bib.bib7)) and compromises
    patient care. The development of automatic report generation techniques can help
    alleviate this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic high-quality report generation is challenging. It is intrinsically
    a multi-modality problem (Tu et al., [2024](#bib.bib116); Yan et al., [2023](#bib.bib141)).
    In routine clinical practice, to produce clear, correct, concise, complete, consistent,
    and coherent reports, radiologists need to combine information from images with
    information from other modality data, such as clinical history and related clinical
    measures. Previously developed techniques mostly considered images as input, while
    for the past three years, multi-modality deep learning developed very rapidly.
    An increasing number of research papers endeavored to emulate physicians by leveraging
    multi-modal data for the generation of diagnostic reports, as shown in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bb21aee80ac25f03ef1ded8137d80390.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The distributions of reviewed papers using image data and multi-modality
    data as inputs per year from 2021 to 2024\. The percentage denotes the input’s
    prevalence among articles published within the year.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the previous surveys on this topic (Kaur et al., [2022](#bib.bib48);
    Beddiar et al., [2023](#bib.bib7); Liao et al., [2023](#bib.bib64); Shamshad et al.,
    [2023](#bib.bib97); Liu et al., [2023a](#bib.bib67)) did not include non-imaging
    inputs. Messina et al. ([2022](#bib.bib75)) considered non-imaging inputs, but
    only involved 6 papers. Totally, the previous surveys included 40 to 66 papers
    for report generation, primarily focusing on articles published before 2022\.
    This survey differs from previous ones in three main contributions: (1) we analyse
    an additional 22 papers that utilize non-image inputs, and focuses on the acquisition,
    analysis, and integration of multi-modal inputs. To the best of our knowledge,
    it is the first review to investigate state-of-the-art multi-modal data processing
    techniques for report generation; (2) we examine 89 papers published from 2021
    to 2024 to provide a comprehensive study on novel techniques in automatic report
    generation; and (3) we propose a general workflow for report generation with a
    taxonomy of approaches employed, and we summarize training strategies, public
    datasets, and mainstream evaluation methods, as shown in Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data"). The workflow includes 5 key components: inputs, data
    preparation, feature learning, feature fusion, and report generation. Table LABEL:tab:overall
    in Appendix A summarizes all works included in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/251ab5f96d9e3aad347eb9360e180010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The summarized workflow of automatic radiology report generation.
    The fundamental components and key techniques are included. The (x, y%) for each
    method represents the number and percentage of papers used that technique in our
    survey.'
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of the paper is organized as follows. Section [2](#S2 "2 Search
    and selection of articles ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data") introduces the paper search and selection process. Section
    [3](#S3 "3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data") first provides a workflow of deep learning-based report
    generation, then analyses the techniques in each component of the workflow, and
    finally introduces the overall training strategies. Next, in sections [4](#S4
    "4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") and [5](#S5 "5 Evaluation ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"), we introduce popular public
    datasets and evaluation methods, including metrics and expert evaluation. Section
    [6](#S6 "6 Benchmark Comparison ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data") compares the model performance of several papers
    under the same experimental setting. Lastly, we discuss challenges and perspectives
    on this topic in section [7](#S7 "7 Challenges and Future Works ‣ A Survey of
    Deep Learning-based Radiology Report Generation Using Multimodal Data") and provide
    a conclusion in section [8](#S8 "8 Conclusions ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Search and selection of articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Three search engines (Google Scholar, PubMed, and Springer) and four queries
    were employed to collect articles. They included “radiology report generation",
    (medical OR medicine OR health OR radiology) AND (report OR description OR caption)
    AND generation, modal AND (medical OR medicine OR health OR radiology) AND (report
    OR description OR caption) AND generation, and “medical report generation". Following
    the searches, the titles and abstracts of each article were read briefly to identify
    those that met the selection criteria. If there was uncertainty, the article was
    included to ensure relevant studies were not omitted. The selection criteria were
    framed around three aspects. First, we included articles published in the years
    2021, 2022, 2023, and 2024 due to the significant number of developments using
    multi-modal technology in recent years. We aim to focus on the latest algorithms
    not covered in previous surveys. Second, the studies must be original researches
    focused on the automatic generation of full-text natural language radiology reports
    and include quantitative evaluation results. Techniques generating short captions
    of one or two sentences are excluded due to the differing nature of long report
    and sentence generation. Third, papers published in journals, conferences, and
    conference workshop proceedings were included. Moreover, papers uploaded on the
    arXiv website in 2023 and 2024 with over 30 citations were also selected. A total
    of 144 papers were identified using three search engines. In addition, by tracing
    the ancestry and descendants of papers, we identified another 24 papers. After
    removing duplicates, 97 publications were retained. We thoroughly read these works
    and applied exclusion criteria. First, at least one of the generated languages
    should be English. Second, at least one of the input data should be images. Finally,
    89 works were included in the following analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning-based radiology report generation typically follows a standard
    workflow summarized in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data"). This
    section analyzes the techniques in the 89 works, based on the workflow identified.
    Overall, a basic radiology report generation framework consists of 5 steps: (1)
    multi-modality data acquisition (section [3.1](#S3.SS1 "3.1 Multimodality input
    data ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")); (2) data preparation (section [3.2](#S3.SS2 "3.2 Data
    preparation ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")); (3) feature learning (section [3.3](#S3.SS3 "3.3 Feature
    learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")); (4) feature fusion and interaction (section [3.4](#S3.SS4
    "3.4 Multi-modal feature fusion and interaction ‣ 3 Methods ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")); and (5) report
    generation (Section [3.5](#S3.SS5 "3.5 Report generation ‣ 3 Methods ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data")). In
    addition, novel training strategies, including modifying loss functions, reinforcement
    learning, and curriculum learning, are described in Section [3.6](#S3.SS6 "3.6
    Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data").'
  prefs: []
  type: TYPE_NORMAL
- en: Medical images, when analyzed with or without other types of data, are firstly
    prepared (step 2). Subsequently, they are input into feature extractors to perform
    feature learning (step 3), predominantly implemented using CNN or Transformer
    architectures, along with multiple enhancement modules (e.g., auxiliary task and
    contrastive learning). The feature extractors aim at extracting features relevant
    to report generation, and the enhancement modules are utilized to improve the
    expressiveness of the features. For certain approaches, a feature fusion and interaction
    module (step 4) is subsequently applied to align cross-modal data and mitigate
    the negative effects caused by differences between the visual and textual domains.
    After fusion and interaction, the features are conveyed back to the feature extractor
    or directly input into the generator to generate the report (step 5). The training
    strategy is implemented to improve learning effectiveness during training. Table
    LABEL:tab:overall in Appendix A presents detailed information for each paper across
    the five steps and training strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Multimodality input data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The input data refers to the data received by the report generation system.
    During model training and inference, the data can vary; for example, both images
    and real reports are used as inputs during training, while only images are used
    during inference (Shetty et al., [2023](#bib.bib98)). However, the model learns
    from the distribution and features of the training data. If the testing data changes,
    mostly, the model will struggle to generalize, resulting in decreased performance.
    Therefore, in this section, we mainly introduce the acquisition of input data
    that is consistent between the training and testing phases in the reviewed papers.
    The input data includes:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image data includes X-ray, magnetic resonance imaging (MRI), computed tomography
    (CT), ultrasound, gastrointestinal endoscope image, retinal image, and dermoscopy
    image. Most papers we reviewed focus on generating medical reports for chest X-ray
    images (82 works). Other than chest diagnosis, retinal image is the second most
    prevalent image modality (6 works). The retinal image includes lots of categories,
    e.g., fundus fluorescein angiography and color fundus photography. Other works
    focus on chest CT (5 works), gastrointestinal endoscope image (2 works), spine
    MRI (1 work), dermoscopy image (1 work), and breast ultrasound images (1 work).
    Although some images are non-radiological, such as ophthalmic images, we still
    include their report-generation techniques in this survey.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical terminology refers to medical terms and expressions, which are from
    keyword labels of images (Huang et al., [2021a](#bib.bib34), [b](#bib.bib35),
    [2022](#bib.bib36); Liu et al., [2023b](#bib.bib74)), or self-built corpora (Liu
    et al., [2021b](#bib.bib69); Cao et al., [2022](#bib.bib9), [2023](#bib.bib10);
    Xue et al., [2024](#bib.bib138); Gu et al., [2024](#bib.bib25); Li et al., [2023c](#bib.bib62)),
    storing common descriptions found in medical reports.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Medical knowledge base mostly records the connections between different organs
    and diseases, and is presented in a graph format. Graph is a fundamental data
    structure consisting of a set of nodes and edges, which can easily represent a
    set of subjects and their connections. The knowledge graph can primarily be obtained
    in two ways: 1) public datasets such as the RadGraph (Jain et al., [2021](#bib.bib39))
    used by two works (Yang et al., [2022](#bib.bib143); Li et al., [2023a](#bib.bib60));
    2) self-designed knowledge bases according to authoritative medical standards
    (Huang et al., [2023](#bib.bib37); Xu et al., [2023](#bib.bib137)) or disease
    labels (Jia et al., [2022](#bib.bib42)). Yang et al. ([2023](#bib.bib144)) argued
    that manual graph construction is limited to diseases, further complicating the
    adaptation of these models for diverse datasets. To overcome it, they automatically
    constructed the medical knowledge based on real reports during training and leveraged
    the knowledge base during model inference.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real texts report is mostly obtained by data retrieval (Liu et al., [2021b](#bib.bib69);
    Song et al., [2022](#bib.bib104); Yang et al., [2022](#bib.bib143); Li et al.,
    [2023a](#bib.bib60); Liu et al., [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62);
    Jin et al., [2024](#bib.bib43)). For example, for each input image, Liu et al.
    ([2021b](#bib.bib69)) retrieved similar images from the training dataset and utilized
    the corresponding reports. This process mimics radiologists consulting previous
    medical case reports when drafting their own. In addition, Liu et al. ([2023b](#bib.bib74))
    also obtained pre-defined sentences based on input terminologies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clinical information encompasses patient demographics (e.g., age and gender),
    clinical observations, and medical histories. It is included within the indication
    section of radiology reports (see Figure [4](#S4.F4 "Figure 4 ‣ 4 Datasets ‣ A
    Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data")).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Questionnaires: report generation model can be trained in a visual question
    answering way (Tanwani et al., [2022](#bib.bib113); Pellegrini et al., [2023](#bib.bib87)).
    The questionnaires are provided by public datasets, such as VQA-Rad (Lau et al.,
    [2018](#bib.bib52)) and Rad-ReStruct (Pellegrini et al., [2023](#bib.bib87)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Data preparation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data preparation endeavors to enhance data quality and prepare it for model
    deployment, typically encompassing data cleansing, transformation, and organization.
    Conventional preparation methods include image resizing and cropping, text tokenizing,
    converting all tokens to lowercase, removing non-alphabetic tokens, and implementing
    data augmentation procedures. The methods utilized in each paper are outlined
    in Table LABEL:tab:overall in Appendix A, but it is worth noting that conventional
    preparation methods are so ubiquitous that some papers do not mention them. While
    the information is not recorded in the table, it does not mean the absence of
    a data preparation process.
  prefs: []
  type: TYPE_NORMAL
- en: Novel data preparation methods in the reviewed papers can be categorized into
    filtering (Ramesh et al., [2022](#bib.bib93)) and grouping (Wang et al., [2022b](#bib.bib122)).
    Ramesh et al. ([2022](#bib.bib93)) argued that writing a radiology report necessitates
    referencing historical information, which inevitably included descriptors such
    as ‘again’ and ‘decrease’. However, these terms cannot be inferred from a single
    image, therefore Ramesh et al. filtered such descriptions in the reports. This
    exclusion was found to facilitate the model’s learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping refers to organizing sentences from ground truth reports into distinct
    sections, typically relying on keywords from pre-defined knowledge graphs and
    filtering rules. Each section describes a specific anatomical structure. Grouping
    aims to enable the generation system to process various types of sentences differently.
    For instance, Wang et al. ([2022b](#bib.bib122)) employed different decoders to
    generate descriptions for different anatomical structures. Alongside the reviewed
    papers, a recently released public dataset named ImaGenome (Wu et al., [2021](#bib.bib132))
    also includes grouping results in their annotation files (see Section [4](#S4
    "4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")). The grouping result becomes more easily accessible.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Feature learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.3.1 Image-based feature learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Previous research primarily utilized CNNs as architectures for extracting image
    features, however, recently, an increasing number of researchers have opted for
    the use of Transformers due to their improved performance. Simultaneously, numerous
    studies proposed novel modules to enhance the model capability. In this section,
    the model architecture is first introduced, and subsequently enhancement modules
    are described. These modules include auxiliary tasks, contrastive learning, and
    memory metrics. The architecture and modules utilized in each paper are outlined
    in Table LABEL:tab:overall in Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: CNN and Transformer encoder for feature extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The statistics of the architectures used as image feature extractor are shown
    in Figure [3](#S3.F3 "Figure 3 ‣ CNN and Transformer encoder for feature extraction
    ‣ 3.3.1 Image-based feature learning ‣ 3.3 Feature learning ‣ 3 Methods ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data"). In
    total, forty-six studies extract image features purely based on CNN models. Thirty-four
    works firstly encode images by CNN and then utilizes the Transformer layers to
    modify the embeddings. Eight works utilize a pure Transformer architecture to
    extract image features. Figure [3](#S3.F3 "Figure 3 ‣ CNN and Transformer encoder
    for feature extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3 Feature learning
    ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") shows a clear trend of more studies adopting CNN augmented with
    Transformer for image feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86b5001f276c85fa3f89903f54be5281.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The statistics of the reviewed papers using different architectures
    to extract image features per year from 2021 to 2024\. The percentage denotes
    the method’s prevalence among articles published within the year.'
  prefs: []
  type: TYPE_NORMAL
- en: For CNN architecture, two works self-designed CNN models, while the other works
    built it based on different classical visual models, such as ResNet (He et al.,
    [2016](#bib.bib27)) (42 papers), DenseNet (Huang et al., [2017](#bib.bib33)) (22
    papers), VGG (Simonyan and Zisserman, [2014](#bib.bib99)) (5 papers), Faster-RCNN
    (Ren et al., [2015](#bib.bib94)) (5 papers), Inception-V3 (Szegedy et al., [2016](#bib.bib110))
    (1 paper), ResNeXt (Xie et al., [2017](#bib.bib135)) (1 paper), EfficientNet (Tan
    and Le, [2019](#bib.bib111)) (1 paper), and the Two-Stream Inflated 3D ConvNets
    (I3D) (Carreira and Zisserman, [2017](#bib.bib11)) (1 paper). Pahwa et al. ([2021](#bib.bib84))
    modified the HRNet (Sun et al., [2019a](#bib.bib107)), a human pose estimation
    network. Other three works (Huang et al., [2021a](#bib.bib34), [b](#bib.bib35),
    [2022](#bib.bib36)) provided results based on different CNN structures.
  prefs: []
  type: TYPE_NORMAL
- en: To improve model performance, ten works modified the CNN structure by attention
    modules, which assigned varying degrees of importance (weights) to different parts
    of the input by learnable parameters, allowing the model to selectively focus
    on specific regions of an image. Traditional attention mechanisms can be classified
    into channel-wise (Du et al., [2022](#bib.bib22); Wang et al., [2022e](#bib.bib130);
    Gajbhiye et al., [2022](#bib.bib24); Pahwa et al., [2021](#bib.bib84)) and spatial-wise
    (Pahwa et al., [2021](#bib.bib84); Jia et al., [2021](#bib.bib41)), which allocate
    different weights to the various channels and spatial positions of the inputs
    respectively. In addition, Li et al. ([2023b](#bib.bib61)) and Wang et al. ([2024a](#bib.bib121))
    utilized the idea of the class activation map (Zhou et al., [2016](#bib.bib152))
    to obtain weights. Yan et al. ([2022](#bib.bib139)) initially extracted image
    patch features, clustered them using an unsupervised method, and then weighted
    the cluster results. Experimental results show that attention mechanism allows
    models to pay more attention to the lesions than irrelevant background. With the
    rise of the Transformers (Vaswani et al., [2017](#bib.bib118)), multi-head attention
    has become a potent method for information interaction. Wang et al. ([2023c](#bib.bib127))
    first extracted regions of interest from the frontal view and then employed multi-head
    attention to fuse information between the frontal and lateral views with the regions.
    This approach introduced regions of interest into model training to improve model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: For Transformer architecture, most of them leverage a standard Transformer encoder,
    while Li et al. ([2023d](#bib.bib63)) argued that aligning images and text posed
    a challenge due to the continuous nature of images and the discrete nature of
    text; therefore, they improved the model performance by using a discrete variational
    autoencoder (Ramesh et al., [2021](#bib.bib92)) to obtain discrete visual tokens.
    Other works improved the model performance by modifying the self-attention module.
    Three works (Wang et al., [2022e](#bib.bib130); Lin et al., [2023](#bib.bib66);
    Wang et al., [2023d](#bib.bib131)) added high-order interactions among three inputs
    of the Transformer attention module. Two works (Miura et al., [2021](#bib.bib77);
    Wang et al., [2022e](#bib.bib130)) were inspired by the memory-augmented attention
    (Cornia et al., [2020](#bib.bib17)) and extended the keys and values with additional
    plain learnable vectors to record more information. Li et al. ([2022b](#bib.bib59))
    introduced a learnable parameter in the attention operation. Wang et al. ([2023d](#bib.bib131))
    modified the encoder by including additional input tokens. These tokens were named
    ‘expert tokens’ to emulate the “multi-expert joint diagnosis” methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary task for feature extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Auxiliary tasks aim to provide additional supervision signals to the feature
    extractor, enabling it to extract information relevant to report generation from
    images. These tasks mainly include classification (22 papers), graph construction
    (10 papers), embedding comparison (10 papers), and detection/segmentation (7 papers).
    Each of them is introduced in detail below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification: The most common auxiliary task used in the reviewed papers
    is classification referring to assigning images to predefined categories. The
    predefined categories primarily include medical tags (Gajbhiye et al., [2022](#bib.bib24);
    Kaur and Mittal, [2022b](#bib.bib47); Wang et al., [2022e](#bib.bib130); Hou et al.,
    [2021b](#bib.bib32); Du et al., [2022](#bib.bib22); You et al., [2022](#bib.bib147);
    Alfarghaly et al., [2021](#bib.bib3)) and disease labels (Liu et al., [2021e](#bib.bib72);
    Zhou et al., [2021](#bib.bib153); Wang et al., [2022d](#bib.bib129); Yang et al.,
    [2023](#bib.bib144); Zhang et al., [2023b](#bib.bib149); Wang et al., [2024a](#bib.bib121);
    Jin et al., [2024](#bib.bib43); Hou et al., [2021b](#bib.bib32)). The medical
    tags are from standard medical vocabularies including hundreds of labels, such
    as anatomical structures and pathological signs. They are provided by manual annotations
    or auto annotation tools (e.g., NIH MTI web API ¹¹1https://ii.nlm.nih.gov/MTI/index.shtml
    and RadGraph (Jain et al., [2021](#bib.bib39))). Disease labels are provided by
    auto annotation tools (e.g., CheXpert (Irvin et al., [2019](#bib.bib38)) and CheXbert
    (Smit et al., [2020](#bib.bib103))). Compared to disease labels, medical tags
    offer a more comprehensive range of information. However, to the best of our knowledge,
    there is no literature that supports the superiority of medical tags over disease
    labels. Perhaps due to the extensive scope covered by medical tags, deep learning
    models face challenges in acquiring such rich knowledge. Zhou et al. ([2021](#bib.bib153))
    incorporated 32 additional labels for lesion location, size, and shape (e.g.,
    “upper/lower” and “patchy”) into the disease label set, observing a slight improvement
    in model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Other notable categories used in the reviewed papers include matching status
    (Li et al., [2023a](#bib.bib60)), local properties (Yang et al., [2021a](#bib.bib142)),
    report cluster results (Li et al., [2022a](#bib.bib56)), and fix answer categories
    (Tanwani et al., [2022](#bib.bib113)). Li et al. ([2023a](#bib.bib60)) predicted
    the matching status of a given image-report pair. Yang et al. ([2021a](#bib.bib142))
    devised localized property labels for breast ultrasound images, such as tumor
    morphology, to facilitate the identification of properties that are challenging
    to be discerned in low-resolution images. Li et al. ([2022a](#bib.bib56)) first
    conducted unsupervised clustering on the ground truth report, subsequently utilizing
    the resultant clusters as labels. This auxiliary task yielded a marked improvement
    in text generation performance. Tanwani et al. ([2022](#bib.bib113)) considered
    the report generation as a question-answer task, and the classifier was designed
    for fixed answer categories. In addition, for a detection auxiliary task, classifiers
    need to be applied to identify attributes of detected regions (e.g., ‘right lung’).
    This paragraph eschews such cases to circumvent redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Graph construction: Graph construction aims at introducing prior knowledge
    into the report generation process. The knowledge graph in this section differs
    from that in Section [3.1](#S3.SS1 "3.1 Multimodality input data ‣ 3 Methods ‣
    A Survey of Deep Learning-based Radiology Report Generation Using Multimodal Data").
    Here, node features are extracted from images, and edges are defined as parameters
    in graph convolution networks. In contrast, the node and edge information in the
    input knowledge graphs is derived from non-image data. A classical method was
    proposed by Zhang et al. ([2020](#bib.bib150)) and yielded promising outcomes.
    A knowledge graph was constructed firstly based on insights provided by domain
    experts, where nodes represented major abnormalities and major organs, and bidirectional
    connections linked nodes that were related to each other. To initialize nodes
    features, a spatial attention module was introduced after the CNN backbone using
    1$\times$1 convolution layers and softmax layers. The number of channels matched
    the number of nodes. The nodes’ initial embedding was derived as attention-weighted
    feature maps. Then graph convolution layers were employed to disseminate information
    throughout the graph, followed by two branches for classification and report generation.
    First, the classification branch was trained, and subsequently, parameters in
    both the CNN backbone and the graph convolution layers were frozen, only the report
    generation decoder was trained. Six works (Liu et al., [2021b](#bib.bib69), [d](#bib.bib71);
    Cao et al., [2022](#bib.bib9); Wang et al., [2022c](#bib.bib125); Yan, [2022](#bib.bib140);
    Zhang et al., [2023b](#bib.bib149)) utilized this method (Zhang et al., [2020](#bib.bib150)).
    Wang et al. ([2022c](#bib.bib125)) expanded the graph (Zhang et al., [2020](#bib.bib150))
    by incorporating information from a radiology terms corpus named Radiology Lexicon
    (RadLex)²²2http://www.radlex.org/ (Langlotz, [2006](#bib.bib51)). As the number
    of graph nodes increased, the model performance initially improved, peaked at
    40 nodes, and then declined, with a noticeable decrease at 60 nodes. Liu et al.
    ([2021d](#bib.bib71)) constructed a large graph based on the MIMIC-CXR dataset.
    The nodes represented frequent clinical abnormalities and the edges represented
    the co-occurrence situation of different abnormalities. In addition, Li et al.
    ([2023b](#bib.bib61)) used the disease prediction results to obtain the node features.
    The nodes were classification probabilities, with learnable edge weights.'
  prefs: []
  type: TYPE_NORMAL
- en: Another graph reconstruction method aims at reconstructing triplets that are
    in the form of (entity1, relationship, entity2), such as (opacity, suggestive
    of, infection). Three works (Dalla Serra et al., [2022](#bib.bib18), [2023b](#bib.bib20);
    Li et al., [2022b](#bib.bib59)) firstly predicted the triplets and then generated
    reports based on them. The experimental results show that using triplets alone
    for report generation is ineffective; combining them with features extracted from
    images is necessary for better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedding comparison: Embedding comparison refers to constraining the consistency
    of different features in intermediate layers, thereby guiding the learning process.
    The comparison in reviewed papers is mainly applied between features extracted
    from images and real reports (Najdenkoska et al., [2021](#bib.bib80), [2022](#bib.bib81);
    Zhou et al., [2021](#bib.bib153); Yang et al., [2021b](#bib.bib145); Chen et al.,
    [2022](#bib.bib13); Wang et al., [2021](#bib.bib128), [2022d](#bib.bib129); Yang
    et al., [2023](#bib.bib144)). Experimental results show that the supervision signals
    from real text enable extracted visual features carry richer semantic information,
    facilitating more effective translation into radiology reports. Four works (Wang
    et al., [2021](#bib.bib128); Zhou et al., [2021](#bib.bib153); Wang et al., [2022d](#bib.bib129);
    Yang et al., [2023](#bib.bib144)) utilized a triple loss function to compel the
    image-text paired features to be closer to a latent space than the unpaired ones.
    Najdenkoska et al. ([2021](#bib.bib80), [2022](#bib.bib81)) inspired by the Auto-Encoding
    Variational Bayes (Kingma and Welling, [2013](#bib.bib49)). They used real reports
    to obtain a latent space during training and generated reports based on this space.
    The image extractors were enabled to capture features from images that closely
    resemble those found in real reports. Other two works (Yang et al., [2021b](#bib.bib145);
    Chen et al., [2022](#bib.bib13)) used the Term Frequency and Inverse Document
    Frequency (TF-IDF) to extract important information from real reports as supervision
    signals. TF-IDF is a statistical measure assessing a word’s importance by considering
    its frequency in a specific document and its rarity across the entire document
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, to produce reports for abnormalities not seen during training,
    Sun et al. ([2022](#bib.bib106)) initially linearly projected visual features
    to semantic features, and extracted semantic features of labels by the BioBert
    model (Lee et al., [2020](#bib.bib54)). Consistent constrain was applied between
    two similarity: 1) the similarity between pairwise elements in the semantic features
    from visual features; and 2) the similarity between the semantic features from
    visual features and the semantic features from labels. Zhang et al. ([2023b](#bib.bib149))
    integrated semi-supervised learning into report generation using two networks.
    They first applied different types of noise to an input image to create two variations,
    which were then fed into the two networks. An auxiliary loss function was employed
    to ensure consistency in the extracted visual features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detection/segmentation: Object detection locates and identifies objects or
    patterns within an image, focusing on determining their presence and position.
    Segmentation divides an image into meaningful regions by identifying and separating
    objects based on specific characteristics. Both processes enhance the model’s
    understanding of the image by object recognition and region extraction, and can
    improve the model’s interpretability by linking the detection/segmentation results
    with generated sentences. The detection/segmentation regions can be anatomical
    regions (Tanida et al., [2023](#bib.bib112); Dalla Serra et al., [2023b](#bib.bib20),
    [a](#bib.bib19); Wang et al., [2023c](#bib.bib127); Han et al., [2021](#bib.bib26);
    Gu et al., [2024](#bib.bib25)) and abnormal regions (Sun et al., [2022](#bib.bib106)).
    No literature compares the impact of detection and segmentation tasks on report
    generation results. However, a publicly available dataset named Chest ImaGenome
    (see Section [4](#S4 "4 Datasets ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")) offers detection annotations, making them
    easier to be acquired than segmentation annotations.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the outputs of auxiliary tasks can provide valuable information
    such as disease labels, therefore, inputting them into the following generation
    network is a common choice (Alfarghaly et al., [2021](#bib.bib3); Hou et al.,
    [2021b](#bib.bib32); Singh et al., [2021](#bib.bib100); Yang et al., [2021a](#bib.bib142);
    You et al., [2021](#bib.bib146); Zhou et al., [2021](#bib.bib153); Du et al.,
    [2022](#bib.bib22); Jia et al., [2022](#bib.bib42); Kaur and Mittal, [2022b](#bib.bib47);
    Sun et al., [2022](#bib.bib106); Wang et al., [2022a](#bib.bib120), [e](#bib.bib130);
    Yan et al., [2022](#bib.bib139); You et al., [2022](#bib.bib147); Tanida et al.,
    [2023](#bib.bib112); Li et al., [2022b](#bib.bib59); Jin et al., [2024](#bib.bib43)).
    For example, Zhou et al. ([2021](#bib.bib153)) sent the semantic word embeddings
    of the predicted findings from the classifier to the report generation decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive learning for feature extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Contrastive learning is a self-supervised learning method to improve the representational
    capacity of models, which allows models to minimize the distance among positive
    pairs and maximize it for negative ones. It can be used to train feature extractors
    (Wang et al., [2022e](#bib.bib130); Lin et al., [2023](#bib.bib66); Wu et al.,
    [2022](#bib.bib133); Wang et al., [2023b](#bib.bib124)). Lin et al. ([2023](#bib.bib66))
    utilized a classical contrastive learning method named Momentum Contrast (He et al.,
    [2020](#bib.bib28)), where different views or augmented versions of the same image
    were considered as positive pairs. Another representative contrastive learning
    work is the Contrastive Language-Image Pre-training (CLIP) model (Radford et al.,
    [2021](#bib.bib91)). It connects textual and visual information by directly training
    on a vast dataset consisting of image-text pairs. Wang et al. ([2022e](#bib.bib130))
    directly employed it for image feature extraction and Wu et al. ([2022](#bib.bib133))
    applied the idea of CLIP to train the feature extractors on the training dataset.
    Wang et al. ([2023b](#bib.bib124)) however argued that previous works treated
    the entire report as input, overlooking the distinct information contained within
    individual sentences. This oversight could result in incorrect matching of image-text
    pairs. Therefore, they proposed phenotype-based contrastive learning. This method
    involved randomly initializing a set of vectors as phenotypes, allowing sentences
    and visual embeddings to interact with them, and finally conducting contrastive
    learning between the processed embeddings. The results outperformed previous contrastive
    learning methods in report generation.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive learning also can be part of the training loss (Tanwani et al.,
    [2022](#bib.bib113); Wang et al., [2022a](#bib.bib120); Li et al., [2023a](#bib.bib60);
    Liu et al., [2023b](#bib.bib74)), and can be applied between visual and textual
    features (i.e. image-text pairs) (Tanwani et al., [2022](#bib.bib113); Li et al.,
    [2023a](#bib.bib60); Liu et al., [2023b](#bib.bib74)), or be applied based on
    labels, treating samples with shared labels as positives and those without any
    common labels as negatives (Wang et al., [2022a](#bib.bib120)). Their ablation
    study demonstrated a significant improvement in results compared to standard contrastive
    loss (Wang et al., [2022a](#bib.bib120)).
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive attention is another method to utilize contrastive learning. Liu
    et al. ([2021c](#bib.bib70)) designed a contrastive attention model to extract
    abnormal region features by comparing input samples with normal cases. Similar
    features shared between the input and normal cases were subtracted from the input
    image feature, and the remaining feature was then concatenated with the original
    feature. Song et al. ([2022](#bib.bib104)) argued that the contrastive technique
    (Liu et al., [2021c](#bib.bib70)) did not consider historical information, therefore
    they proposed a module based on similarity retrieval technique to obtain similar
    images from the training dataset. The image features were processed by enlarging
    different features between inputs and the similar retrieved images.
  prefs: []
  type: TYPE_NORMAL
- en: Memory metric for feature extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using a memory metric for image feature extraction assumes the presence of similar
    features in various medical images. Memory metrics are employed to record and
    transmit the similarity information during training (Chen et al., [2022](#bib.bib13);
    Yan, [2022](#bib.bib140)). Typically, an n×n matrix is randomly initialized, where
    n represents the number of metric rows. Then, at each training step, the matrix
    is updated based on the visual features and the previous metrics. The memory metric
    used in this section is consistent with the metric used in the memory-driven transformer
    discussed in Section [3.5](#S3.SS5 "3.5 Report generation ‣ 3 Methods ‣ A Survey
    of Deep Learning-based Radiology Report Generation Using Multimodal Data"), with
    one being applied to images and the other to the generated reports.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Non-imaging-based feature learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most non-imaging data is presented in the form of text. Before fusion with image
    data, text data needs to be embedded. We first introduce a widely-used basic text
    embedding technique, the lookup table. This table assigns a unique index to each
    word or character, which is used to look up a pre-trained word vector or character
    vector. In addition to the basic method, additional feature extraction can be
    performed on these vectors to enhance their representation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based models such as BERT and its variants have emerged as mainstream
    methods for feature extraction across various textual data, such as terminology
    (Liu et al., [2021e](#bib.bib72); Cao et al., [2022](#bib.bib9), [2023](#bib.bib10);
    Liu et al., [2021b](#bib.bib69); Xue et al., [2024](#bib.bib138); Liu et al.,
    [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62)), real text reports (Liu et al.,
    [2021b](#bib.bib69), [2023b](#bib.bib74); Li et al., [2023c](#bib.bib62); Jin
    et al., [2024](#bib.bib43)), knowledge graphs (Yang et al., [2022](#bib.bib143);
    Huang et al., [2023](#bib.bib37); Li et al., [2023a](#bib.bib60); Xu et al., [2023](#bib.bib137)),
    and questionnaires (Tanwani et al., [2022](#bib.bib113); Pellegrini et al., [2023](#bib.bib87)),
    and have achieved good results.
  prefs: []
  type: TYPE_NORMAL
- en: Several methods are designed for a specific type of input. Li et al. ([2023c](#bib.bib62))
    used the TF-IDF to re-weight terminology embeddings. The re-weighted approach
    alleviated the issue of data imbalance, resulting in performance enhancement.
    Clinical information can be processed by a pre-trained feature extractor named
    BioSentVec (Zhou et al., [2021](#bib.bib153); Chen et al., [2019](#bib.bib12)).
    For the knowledge base, Yang et al. ([2022](#bib.bib143)) used a knowledge graph
    embedding model named RotatE (Sun et al., [2019b](#bib.bib109)) to obtain entity
    embeddings and relation embeddings from the RadGraph. Besides using the entire
    RadGraph as input, two works combined the real reports with RadGraph to extract
    case-related information from real reports and queried related information from
    the RadGraph (Yang et al., [2022](#bib.bib143); Li et al., [2023a](#bib.bib60)).
    Alternatively, Li et al. ([2023c](#bib.bib62)) considered RadGraph as an annotation
    tool for extracting entities and positional information from real reports. Other
    two works (Xu et al., [2023](#bib.bib137); Huang et al., [2023](#bib.bib37)) utilized
    classification results to process the self-built graph and extracted case-related
    information. The experimental results substantiate the beneficial impact of incorporating
    case-related knowledge on report generation, particularly evident when integrating
    real reports with the knowledge base. In addition, age and gender information
    are not text data and are generally encoded as one-hot vector (Zhou et al., [2021](#bib.bib153)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Multi-modal feature fusion and interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature fusion and interaction refer to the integration of multi-modal data
    from inputs or auxiliary tasks. This step has two purposes. First, visual and
    textual features from disparate domains present challenges for model learning.
    By fusing and facilitating interaction between these features, the domain gap
    can be narrowed, thereby enhancing network learning. Second, the image regions
    should align with the sentences in the reports. This correspondence can be learned
    through fusion and interaction. In the auxiliary task of embedding comparison
    (see Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Image-based feature learning ‣ 3.3 Feature
    learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data")), semantic features extracted from real reports are used
    to supervise the learning of image features. However, this approach differs from
    multi-modal feature fusion. The objective of embedding comparison is to enhance
    image features, without incorporating non-image features into the generator. Instead,
    the fused features in this section are forwarded to the generator.
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward approach for feature fusion and interaction is feature-level
    operation including the concatenation, summation, or multiplication of multimodal
    features (13 works). However, the feature-level operation could be too simple
    to enable sufficient interaction. Therefore, neural network-based methods are
    leveraged, such as LSTMs (2 works) and the multi-head attention mechanism (27
    works). While this approach facilitates convenient feature fusion, its lack of
    specific design for multi-modality data fusion leads to limited effectiveness
    in interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We would like to highlight a memory metric-based method proposed by Chen et al.
    ([2021](#bib.bib15)). It significantly enhanced the performance of the report
    generation system by facilitating feature interaction. A metric was initialized
    randomly. Image features, text features from generated tokens, and memory metric
    features were then projected into the same space. Subsequently, distances between
    image features and memory metric features, as well as text features and memory
    metric features, were calculated. The top K metric features with the closest distances
    to the image or text were selected, respectively. These selected features were
    then weighted based on these distances and were fed back into an encoder-decoder
    structure. Two studies (Qin and Song, [2022](#bib.bib90); You et al., [2022](#bib.bib147))
    followed this method. Wang et al. ([2022a](#bib.bib120)) modified it in two ways:
    1) they initialized the matrix by visual and textual features; 2) the cross-modal
    interaction occurred only among cases with the same label. These two modifications
    both resulted in a notable improvement. Li et al. ([2023d](#bib.bib63)) contended
    that the methods mentioned lack explicit constraints for cross-modal alignments.
    They considered orthonormal bases as the metric and input them along with visual
    or textual features, into multi-head attention modules. Then the outputs of attention
    modules were processed by a self-defined gate mechanism. A triplet matching loss
    was utilized to align the processed visual and textual features. This method slightly
    improved the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Report generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last step is report generation, which utilizes extracted features from earlier
    steps to produce the final reports. The generation methods mainly include decoder-based
    techniques (Section [3.5.1](#S3.SS5.SSS1 "3.5.1 Decoder-based techniques ‣ 3.5
    Report generation ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data")), retrieval-based techniques (Section [3.5.2](#S3.SS5.SSS2
    "3.5.2 Retrieval-based and template-based techniques ‣ 3.5 Report generation ‣
    3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")), and template-based techniques (Section [3.5.2](#S3.SS5.SSS2
    "3.5.2 Retrieval-based and template-based techniques ‣ 3.5 Report generation ‣
    3 Methods ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")). In addition, the development of large language models has
    made it possible to utilize them to enhance the quality of generated reports.
    It is discussed in Section [3.5.3](#S3.SS5.SSS3 "3.5.3 Large language model to
    assist report generation ‣ 3.5 Report generation ‣ 3 Methods ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Decoder-based techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The decoder decodes the extracted representation of inputs and generates a descriptive
    report. The mainstream architectures include LSTM (Hochreiter and Schmidhuber,
    [1997](#bib.bib29)) and Transformer. Compared to LSTM, the Transformer processes
    the entire sequence simultaneously rather than sequentially. Therefore, the Transformer
    allows for more efficient parallelization during training and can capture long-range
    dependencies. In the 89 reviewed papers, the Transformer tends to replace LSTM.
    Fifty-five works utilized the Transformer as a decoder and only 23 of them utilized
    LSTMs (12 works) or hierarchical LSTMs (11 works). For the papers published in
    2023 and 2024, all encoder-decoder structures used the Transformer as their decoders.
    There are two ways to modify the decoder and improve model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shortcut connections: Connecting different layers in networks can be considered
    as a promising way to enhance the flow of information in both forward and backward
    propagation (Mirikharaji et al., [2023](#bib.bib76)). The U-connection (Huang
    et al., [2023](#bib.bib37)) and meshed connection (Miura et al., [2021](#bib.bib77);
    Lee et al., [2022](#bib.bib53); Cornia et al., [2020](#bib.bib17)) are added between
    encoder and decoder, resulting in a similar performance enhancement (Huang et al.,
    [2023](#bib.bib37)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory-driven Transformer: We would like to highlight the Memory-driven Transformer
    (R2Gen) proposed by Chen et al. ([2020](#bib.bib14)). It has been increasingly
    popular in recent years. The R2Gen introduces a memory module and a memory-driven
    conditional layer normalization module into the Transformer decoder architecture.
    The design of the memory module hypothesizes that diverse images exhibit similar
    patterns in their radiological reports, thereby serving as valuable references
    for each other. Building a memory matrix can capture this pattern and transfer
    it during training. Specifically, similar to that in the section [3.3.1](#S3.SS3.SSS1.Px4
    "Memory metric for feature extraction ‣ 3.3.1 Image-based feature learning ‣ 3.3
    Feature learning ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology Report
    Generation Using Multimodal Data"), a matrix is randomly initialized and is updated
    using the gate mechanism based on the matrix from the last step and generated
    reports. The layer normalization is designed to integrate the outputs of the memory
    module into the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Eight works directly utilized the R2Gen as their decoder. In addition, the design
    of the memory module and layer normalization inspired subsequent works (Xue et al.,
    [2024](#bib.bib138); Jia et al., [2021](#bib.bib41); Zhang et al., [2023a](#bib.bib148)).
    It is noted that the novel utilization of the memory module by Zhang et al. ([2023a](#bib.bib148))
    integrates ground truth reports into the training process, leading to successful
    outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Retrieval-based and template-based techniques
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Retrieval-based techniques generate reports by selecting existing sentences
    from a large corpus and the selection is typically based on similarity comparison
    (Endo et al., [2021](#bib.bib23); Ramesh et al., [2022](#bib.bib93); Jeong et al.,
    [2024](#bib.bib40)). Initially, text and image encoders are trained using a contrastive
    method, such as the CLIP (Radford et al., [2021](#bib.bib91)). The textual features
    of sentences in a corpus and the visual features of an input image are extracted
    by the encoders. The visual features are then compared with all textual features
    in the corpus. The top k sentences with the maximum similarity score are selected
    for the predicted report. In addition, Jeong et al. ([2024](#bib.bib40)) added
    a multimodal encoder after the retrieval process to calculate the image-text matching
    scores between the input image and the retrieved sentences. A filter was applied
    based on the score to remove entailed or contradicted sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Other retrieval-based techniques do not follow the above process. Kong et al.
    ([2022](#bib.bib50)) treated the report generation in two steps sentence retrieval
    and selection. They first retrieved a candidate sentence set from the training
    datasets with far more sentences than a standard medical report and then selected
    the sentences by a classifier. Zhang et al. ([2022](#bib.bib151)) proposed a retrieval
    method based on a hashing technique, which mapped multi-modal data with the same
    label into a shared space.
  prefs: []
  type: TYPE_NORMAL
- en: Template-based methods typically start with the diagnosis of diseases, and then
    pre-defined sentences are selected based on the diagnosis results. These selected
    sentences are concatenated to produce reports (Pino et al., [2021](#bib.bib88)).
    Abela et al. ([2022](#bib.bib1)) argued that this method was limited by exact
    labels, therefore they retrieved template sentences by class probabilities and
    different thresholds corresponding to different descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.3 Large language model to assist report generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the emergence of ChatGPT (OpenAI, [2023](#bib.bib83)), its powerful language
    abilities made researchers eager to harness the power of large language models
    to aid in report generation. However, employing it directly within the medical
    domain led to unsatisfactory outcomes (Tu et al., [2024](#bib.bib116); Yan et al.,
    [2023](#bib.bib141); Sun et al., [2023](#bib.bib108)). Two works (Selivanov et al.,
    [2023](#bib.bib96); Wang et al., [2023a](#bib.bib123)) initially predicted report-related
    information such as diseases, lesion regions, and visual features, and generated
    preliminary reports. Subsequently, they employed pre-trained large language models,
    e.g., ChatGPT (Wang et al., [2023a](#bib.bib123)) or GPT-3 (Brown et al., [2020](#bib.bib8);
    Selivanov et al., [2023](#bib.bib96); Wang et al., [2023a](#bib.bib123)) to improve
    the preliminary report with the predicted information and produced the final reports.
    The results have been moderately improved. Exploration of large language models
    in the field of medical report generation still requires further investigation,
    which is discussed in Section [7.4](#S7.SS4 "7.4 Human-AI interaction ‣ 7 Challenges
    and Future Works ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data").
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Training strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training strategy refers to the techniques used to train neural network models.
    Traditionally, models are trained by minimizing various loss functions. Therefore,
    this section begins by introducing different loss functions (Section [3.6.1](#S3.SS6.SSS1
    "3.6.1 Loss functions ‣ 3.6 Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")), followed by a discussion
    on reinforcement learning (Section [3.6.2](#S3.SS6.SSS2 "3.6.2 Reinforcement learning
    ‣ 3.6 Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data")), and the curriculum learning’s application
    to the report generation task (Section [3.6.3](#S3.SS6.SSS3 "3.6.3 Curriculum
    learning ‣ 3.6 Training strategy ‣ 3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")).
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1 Loss functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The mainstream loss function for report generation is the cross-entropy loss
    based on the generated sentences and the ground-truth sentences. Cross-entropy
    loss can be re-weighted based on term frequency (Gajbhiye et al., [2022](#bib.bib24)),
    TF-IDF (Wang et al., [2022d](#bib.bib129)) or uncertainty (Wang et al., [2024b](#bib.bib126))
    to mitigate model bias or handle challenging cases. In addition, Pandey et al.
    ([2021](#bib.bib85)) utilized cycle-consistency loss (Zhu et al., [2017](#bib.bib154))
    to generate reports. The core idea is that a report and its corresponding image
    share the same information, hence they can be used to generate each other.
  prefs: []
  type: TYPE_NORMAL
- en: The application of an auxiliary loss function can provide additional supervision
    signals, further enhancing model performance. Two works Wang et al. ([2021](#bib.bib128));
    Li et al. ([2022a](#bib.bib56)) applied an additional constraint between features
    extracted from the generated and real reports. Zhang et al. ([2023b](#bib.bib149))
    created two different versions of an input image by adding noise and feeding them
    into two networks. An auxiliary loss function ensures the consistency of the outputs
    produced by the two generators. Wang et al. ([2024a](#bib.bib121)) obtained two
    discriminative regions in an image from the generated words and visual classifier
    separately, then enforced the consistency between them.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Reinforcement learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reinforcement learning involves training an agent to make optimal decisions
    through trial and error, aiming to maximize targeting rewards. It offers a method
    to update the model parameters based on non-differentiable reward functions (Messina
    et al., [2022](#bib.bib75)). Evaluation metrics can be considered as rewards,
    such as CIDEr (Kaur and Mittal, [2022b](#bib.bib47)), BLEU (Qin and Song, [2022](#bib.bib90);
    Gu et al., [2024](#bib.bib25)), METEOR (Qin and Song, [2022](#bib.bib90)), ROUGE
    (Qin and Song, [2022](#bib.bib90)), BERTScore (Miura et al., [2021](#bib.bib77)),
    F1 score (Miura et al., [2021](#bib.bib77)), and accuracy (Hou et al., [2021b](#bib.bib32)).
    In addition, Hou et al. ([2021b](#bib.bib32)) trained a language fluency discriminator
    using the ground truth and generated reports, and then utilized the discriminator
    to provide rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.3 Curriculum learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Liu et al. ([2021a](#bib.bib68)) utilized curriculum learning (Platanios et al.,
    [2019](#bib.bib89)) to classify training instances and trained a model from simple
    to complex samples. Data pairs were evaluated based on image heuristics, image
    confidence, text heuristics, and text confidence. Image heuristic evaluated the
    similarity between input images and normal images. Image confidence indicated
    the confidence of a classification model. The report heuristic was related to
    the number of abnormal sentences, and report confidence was evaluated by the negative
    log-likelihood loss (Xu et al., [2020](#bib.bib136)).
  prefs: []
  type: TYPE_NORMAL
- en: 4 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets play a crucial role in the development of report-generation models.
    Abundant and diverse training data can improve the model’s accuracy and generalizability.
    Moreover, a suitable test dataset makes it realistic to test the model’s performance
    in a practical scenario. In this section, we selected 11 public medical image-report
    datasets utilized in the reviewed articles to provide a comprehensive introduction
    of popular and newly collected datasets, see Table [1](#S4.T1 "Table 1 ‣ 4 Datasets
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data"). The usage of datasets in each article is shown in Table LABEL:tab:overall
    in Appendix A. The datasets primarily focus on the lung, including X-ray (Demner-Fushman
    et al., [2016](#bib.bib21); Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45))
    and CT (Li et al., [2020](#bib.bib57); Liu et al., [2021e](#bib.bib72)). In addition,
    there are also publicly available datasets on eye scans (Huang et al., [2021b](#bib.bib35);
    Lin et al., [2023](#bib.bib66); Li et al., [2021](#bib.bib58)) and breast scans
    (Yang et al., [2021a](#bib.bib142)). The concentration of medical report generation
    efforts on chest X-rays can be attributed, in part, to the accessibility of large-scale
    publicly available datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59cb9a9e5cc1bd3e2b1d31e803f2efeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A sample from the MIMIC-CXR dataset. The left is the Chest X-ray
    image and the right is its corresponding radiology report.'
  prefs: []
  type: TYPE_NORMAL
- en: The most popular datasets are the Indiana University Chest X-Ray Collection
    (IU X-Ray) (Demner-Fushman et al., [2016](#bib.bib21)) and the MIMIC Chest X-ray
    (MIMIC-CXR) (Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45)). The IU X-ray
    contains 7470 images of frontal and lateral X-rays and 3955 reports with manual
    annotation based on the MeSH codes³³3https://www.nlm.nih.gov/mesh/meshhome.html
    and the RadLex codes. These codes encompass standard medical terminology, such
    as anatomical structures, diseases, pathological signs, foreign objects, and attributes,
    as defined by authoritative institutions. For deep learning methods, the size
    of the IU X-RAY is insufficient, while the collection of MIMIC-CXR alleviates
    this problem (Messina et al., [2022](#bib.bib75)). It consists of 377,110 images
    and 227,827 reports, and has been multi-labeled by automatic tools according to
    14 disease categories. An image-report sample from the MIMIC-CXR dataset is shown
    in the Figure [4](#S4.F4 "Figure 4 ‣ 4 Datasets ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"). The indication, technique,
    and comparison provide fundamental information for a Chest X-ray test. The automation
    of radiology report generation targets the findings and impressions sections.
    The findings section provides a detailed description of the entire image, while
    the impressions section summarizes these observations. The follow-up work provides
    MIMIC-CXR with richer label information. The Chest ImaGenome dataset (Wu et al.,
    [2021](#bib.bib132)) is based on the anteroposterior and posteroanterior view
    Chest X-ray images in the MIMIC-CXR dataset. It provides an anatomy-centered scene
    graph for each image, including anatomical location and relation annotations.
    The annotations are generated through two automated pipelines, and a separate
    dataset of 500 manually annotated cases is also provided for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing attention has been paid to the collection of ophthalmic image-report
    pairs (Huang et al., [2021b](#bib.bib35); Li et al., [2021](#bib.bib58); Lin et al.,
    [2023](#bib.bib66)). Ophthalmic image has lots of modalities, such as Fluorescein
    Angiography (FA), Fundus Fluorescein Angiography (FFA), Color Fundus photography
    (CFP), fundus photograph (FP), optical coherence tomography (OCT), fundus autofluorescence
    (FAF), Indocyanine Green Chorioangiography (ICG), and red-free filtered fundus
    images. Most of retinal datasets are based on one or two modalities and the last
    released Retina ImBank dataset is the first multi-modality retinal image-text
    dataset (Lin et al., [2023](#bib.bib66)). Some datasets provide additional labels
    for each image, such as lesion boundary (Li et al., [2021](#bib.bib58)), lesion
    category (Huang et al., [2021b](#bib.bib35); Li et al., [2021](#bib.bib58)), and
    keywords (Huang et al., [2021b](#bib.bib35)).
  prefs: []
  type: TYPE_NORMAL
- en: All the above datasets provide unstructured reports. Recently, Pellegrini et al.
    ([2023](#bib.bib87)) released a structured report dataset named Rad-ReStruct based
    on the IU-XRAY. In clinical practice, generating structured reports typically
    requires doctors to answer a sequence of questions (Pellegrini et al., [2023](#bib.bib87)).
    Therefore, Pellegrini et al. ([2023](#bib.bib87)) designed a structured report
    template with a series of single- or multi-choice questions based on topic existence
    (e.g., Are there any diseases in the lung?), element existence (e.g., Is there
    an opacity in the lung?), and attributes (e.g., What is the degree?). They then
    integrated the IU-XRAY report data into the template using its annotated MeSH
    and RadLex codes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Public datasets for medical report generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Image type | Report type | Images | Reports | Patients | Used by |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| IU X-Ray (Demner-Fushman et al., [2016](#bib.bib21)) | Chest X-ray | Unstructured
    | 7,470 | 3,955 | 3,955 | 67 works |'
  prefs: []
  type: TYPE_TB
- en: '| MIMIC-CXR (Johnson et al., [2019a](#bib.bib44), [b](#bib.bib45)) | Chest
    X-ray | Unstructured | 377,110 | 227,827 | 65,379 | 59 works |'
  prefs: []
  type: TYPE_TB
- en: '| Chest ImaGenome (Wu et al., [2021](#bib.bib132)) | Chest X-ray | Unstructured
    | 242,072 | 217,013 | – | 4 works |'
  prefs: []
  type: TYPE_TB
- en: '| COV-CTR (Li et al., [2020](#bib.bib57)) | Chest CT | Unstructured | 728 |
    728 | – | 4 works |'
  prefs: []
  type: TYPE_TB
- en: '| DeepEyeNet (Huang et al., [2021b](#bib.bib35)) | FA, CFP | Unstructured |
    15,709 | 15,709 | – | 3 works |'
  prefs: []
  type: TYPE_TB
- en: '| FFA-IR (Li et al., [2021](#bib.bib58)) | FFA | Unstructured | 1,048,584 |
    10,790 | – | 2 works |'
  prefs: []
  type: TYPE_TB
- en: '| Chinese COVID-19 CT (Liu et al., [2021e](#bib.bib72)) | Chest CT | Unstructured
    | 1,104 | 368 | 96 | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| BCD2018 (Yang et al., [2021a](#bib.bib142)) | Breast Ultrasound | Unstructured
    | 5,349 | 5,349 | – | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| Retina ImBank (Lin et al., [2023](#bib.bib66)) | FP, OCT, FFA, FAF, ICG,
    and red-free filtered fundus images | Unstructured | 18,788 | 18,788 | – | 1 work
    |'
  prefs: []
  type: TYPE_TB
- en: '| Retina Chinese (Lin et al., [2023](#bib.bib66)) | FP, FFA, and ICG | Unstructured
    | 57,498 | 57,498 | – | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| Rad-ReStruct (Pellegrini et al., [2023](#bib.bib87)) | Chest X-ray | Structured
    | 3,720 | 3,597 | 3,597 | 1 work |'
  prefs: []
  type: TYPE_TB
- en: 5 Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accurate assessment of the quality of generated reports is crucial for measuring
    model performance. The quality of generated reports can be evaluated both quantitatively
    and qualitatively. Quantitative methods check the text quality and medical correctness
    of the generated report by natural language evaluation metrics (Section [5.1](#S5.SS1
    "5.1 Natural language-based evaluation metrics ‣ 5 Evaluation ‣ A Survey of Deep
    Learning-based Radiology Report Generation Using Multimodal Data")) and medical
    correctness metrics (Section [5.2](#S5.SS2 "5.2 Medical correctness metrics ‣
    5 Evaluation ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data")), respectively. Qualitative evaluation is normally performed
    by human experts and they provide overall evaluation for the generated reports
    (Section [5.3](#S5.SS3 "5.3 Human-based evaluation ‣ 5 Evaluation ‣ A Survey of
    Deep Learning-based Radiology Report Generation Using Multimodal Data")). Table
    [2](#S5.T2 "Table 2 ‣ 5 Evaluation ‣ A Survey of Deep Learning-based Radiology
    Report Generation Using Multimodal Data") summarizes the description of evaluation
    methods introduced in this section. The usage of evaluation methods in each article
    is shown in Table LABEL:tab:overall in Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The quantitative and qualitative evaluation methods for medical report
    generation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics | Description | Used by |'
  prefs: []
  type: TYPE_TB
- en: '| Natural language-based metrics |'
  prefs: []
  type: TYPE_TB
- en: '| BlEU (Papineni et al., [2002](#bib.bib86)) | A precision-based metric that
    measures the n-gram overlapping of the generated text and ground truth text. |
    84 works |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-L (Lin, [2004](#bib.bib65)) | A F1-like metric that computes a weighted
    harmonic mean of precision and recall based on the longest common subsequence.
    | 75 works |'
  prefs: []
  type: TYPE_TB
- en: '| METEOR (Banerjee and Lavie, [2005](#bib.bib6)) | A F1-like metric that computes
    a weighted harmonic mean of unigram precision and recall. It is an extension of
    BLEU-1. | 57 works |'
  prefs: []
  type: TYPE_TB
- en: '| CIDEr (Vedantam et al., [2015](#bib.bib119)) | The cosine similarity between
    generated text and ground truth text based on the TF-IDF. | 41 works |'
  prefs: []
  type: TYPE_TB
- en: '| $S_{emb}$ (Endo et al., [2021](#bib.bib23)) | Sending ground truth report
    and generated report into a textual feature extractor and calculating the cosine
    similarity between their embeddings from the last layer. | 2 works |'
  prefs: []
  type: TYPE_TB
- en: '| %Novel (Van Miltenburg et al., [2018](#bib.bib117)) | The percentage of generated
    descriptions that are not present in the training data. | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| Medical correctness-based metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Clinical Efficacy (Chen et al., [2020](#bib.bib14); Liu et al., [2019](#bib.bib73))
    | Calculate accuracy, precision, recall, and F1 score based on observations extracted
    from reference reports and generated reports by automated system. | 34 works |'
  prefs: []
  type: TYPE_TB
- en: '| MIRQI (Zhang et al., [2020](#bib.bib150)) | Calculate precision, recall,
    and F1 score based on graph comparison. The ground truth and generated reports
    are automatically analysed to construct a sub-graph from a defined abnormality
    graph. | 2 work |'
  prefs: []
  type: TYPE_TB
- en: '| nKTD (Zhou et al., [2021](#bib.bib153)) | Calculate the Hamming distance
    based on observations extracted from reference reports and generated reports by
    the CheXpert Labeller. | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| Human-based evaluation |'
  prefs: []
  type: TYPE_TB
- en: '| Comparison | Generate reports by two different models and allow senior radiologists
    to find which report is better. | 12 works |'
  prefs: []
  type: TYPE_TB
- en: '| Classification | Radiologists categorize the produced reports as accurate,
    missing details, and false reports. | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| Error scoring | Radiologists assess the error severity of baseline, model
    generated, and reference reports. | 1 work |'
  prefs: []
  type: TYPE_TB
- en: '| Grading | Radiologists need to assign a 5-point scale grade to two types
    of generated reports, in accordance with clinical standards. | 1 work |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Natural language-based evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Natural language evaluation metrics are from natural language processing tasks
    and measure the general text quality of generated reports. In the reviewed papers,
    the most popular metrics are BLEU (Papineni et al., [2002](#bib.bib86)), ROUGE-L
    (Lin, [2004](#bib.bib65)), METEOR (Banerjee and Lavie, [2005](#bib.bib6)), and
    CIDEr (Vedantam et al., [2015](#bib.bib119)), which are based on n-gram matching
    between reference reports and generated reports. The model is deemed superior
    with an increased number of matches. Among them, the BLEU is the earliest and
    proposed a modified precision method. When evaluating the quality of radiology
    report generation, we typically opt for BLEU-1, BLEU-2, BLEU-3, and BLEU-4 metrics.
    The n in BLEU-n means the calculation is based on n-gram. The METEOR is an extension
    of BLEU-1 and introduces recall into evaluation. The ROUGE-L also considers precision
    and recall based on the longest common subsequence between reference and generated
    text. The CIDEr adopts the TF-IDF. The TF-IDF vectors weigh each n-gram in a sentence,
    and then the cosine similarity is calculated between the TF-IDF vectors of reference
    and generated text. When the model consistently produces the most common sentences,
    it can achieve notable BLEU scores. However, CIDEr can evaluate generated outputs
    by encouraging the appearance of important terms and punishing high-frequency
    vocabulary (Li et al., [2023a](#bib.bib60)). CIDEr has a popular variant CIDEr-D,
    which introduces penalties to generate desired sentence length and remove stemming
    to ensure the proper usage of word forms.
  prefs: []
  type: TYPE_NORMAL
- en: Other than n-gram matching, Endo et al. ([2021](#bib.bib23)) proposed a new
    metric $S_{emb}$. A pre-trained feature extractor is applied on both the ground
    truth and the generated reports, and the cosine similarity between extracted embeddings
    is calculated. This approach is used to assess whether the semantic information
    contained in two sentences is consistent. Another natural language metric %Novel
    (Van Miltenburg et al., [2018](#bib.bib117)) is introduced to evaluate the diversity
    in image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Medical correctness metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Natural language evaluation metrics evaluate the similarity between produced
    reports and the ground truth, but cannot accurately measure whether the generated
    reports contain the required medical facts (Babar et al., [2021b](#bib.bib5);
    Messina et al., [2022](#bib.bib75)). So medical correctness metrics are proposed
    to pay attention to the prediction of important medical facts. Generally, an automatic
    labeller is applied to extract medical facts from generated and reference reports.
    Then different metrics are applied to these. The mainstream metric is clinical
    efficacy (Liu et al., [2019](#bib.bib73); Chen et al., [2020](#bib.bib14)), which
    initially calculates precision, recall, and F1 score, and subsequently extends
    to accuracy (Babar et al., [2021a](#bib.bib4); Miura et al., [2021](#bib.bib77);
    Moon et al., [2022](#bib.bib79); Yan, [2022](#bib.bib140); Yang et al., [2022](#bib.bib143);
    Selivanov et al., [2023](#bib.bib96); Yang et al., [2023](#bib.bib144)) and AUC
    (Li et al., [2023a](#bib.bib60)). Most of the works utilized CheXpert (Irvin et al.,
    [2019](#bib.bib38)) as a labeler to extract chest diseases information from ground
    truth reports, while seven of them (Miura et al., [2021](#bib.bib77); Yan, [2022](#bib.bib140);
    Wang et al., [2023a](#bib.bib123), [2024a](#bib.bib121); Dalla Serra et al., [2023b](#bib.bib20),
    [a](#bib.bib19); Jin et al., [2024](#bib.bib43)) utilized a newer labeler CheXbert
    (Smit et al., [2020](#bib.bib103)), which has a higher performance. In addition,
    Pellegrini et al. ([2023](#bib.bib87)) generated structured reports by predicting
    a series of questions. They utilized macro precision, recall, and F1 score to
    assess all questions, along with evaluating report-level accuracy. Other metrics
    calculate the Hamming distance (Zhou et al., [2021](#bib.bib153)) or perform graph
    comparison (Zhang et al., [2020](#bib.bib150)) based on the extracted results.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Human-based evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For qualitative assessment, the most common human-based evaluation method is
    comparison. In general, a set number of samples (i.e., 100/200/300) are selected
    from the test dataset and subsequently processed by different generated models.
    More than one professional clinician is responsible to compare and sort the generated
    reports. Five works utilized ground truth reports in this process. Two works (Miura
    et al., [2021](#bib.bib77); Cao et al., [2022](#bib.bib9)) considered the ground
    truth reports as a reference. Reports were generated by different generators and
    the radiologists need to select which report is more similar to the reference.
    Dalla Serra et al. ([2022](#bib.bib18)) allowed experts to find 5 types of errors
    (i.e., hallucination, omission, attribute error, impression error, and grammatical
    error) in different generated reports according to the reference reports. Xu et al.
    ([2023](#bib.bib137)) asked radiologists to rank the ground truth reports and
    the generated reports. Qin and Song ([2022](#bib.bib90)) invited experts to select
    the most suitable report from the generated and the ground truth reports according
    to correctness, language fluency, and content coverage. Other expert evaluation
    methods are classification (Alfarghaly et al., [2021](#bib.bib3)), grading (Wang
    et al., [2024b](#bib.bib126)), and error scoring (Jeong et al., [2024](#bib.bib40)).
  prefs: []
  type: TYPE_NORMAL
- en: 6 Benchmark Comparison
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For model comparison, it is essential to select a benchmark for an impartial
    evaluation. We choose the MIMIC-CXR (Johnson et al., [2019b](#bib.bib45), [a](#bib.bib44))(see
    Table [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data")) dataset as a benchmark to
    compare the model performance for two reasons. First, according to Table [1](#S4.T1
    "Table 1 ‣ 4 Datasets ‣ A Survey of Deep Learning-based Radiology Report Generation
    Using Multimodal Data"), IU-Xray and MIMIC-CXR are the most popular datasets,
    but IU X-ray lacks standard training-validation-test splits, leading to less comparable
    results (Messina et al., [2022](#bib.bib75)), while MIMIC-CXR has official training-validation-test
    splits. Second, MIMIC-CXR is the largest image-report dataset, which provides
    a broader distribution of data, facilitating testing across diverse scenarios,
    and reducing biases commonly encountered in smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We endeavour to ensure equitable comparisons, but it’s important to consider
    the following three problems when analysing these results. First, we select the
    methods leveraged the official splits of the MIMIC-CXR dataset (Johnson et al.,
    [2019b](#bib.bib45)), containing 368960 images (with 222758 reports) in the training
    dataset, 2991 images (with 1808 reports) in the validate dataset, and 5159 images
    (with 3269 reports) in the test dataset. Although some papers claimed to use official
    splits, the total number of datasets is different. For example, the MIMIC-CXR
    used by Najdenkoska et al. ([2022](#bib.bib81)) contains 218,101 samples. These
    papers were excluded. In Section [3](#S3 "3 Methods ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"), a multitude of methodologies
    are highlighted. However, due to stringent screening, their absence from the comparison
    table does not imply inferior performance. Second, we choose BLEU, ROUGE, METEOR,
    CIDEr-D, precision, recall, and F1 score metrics as evaluation metric. Similar
    to the previous survey (Messina et al., [2022](#bib.bib75)), we found that although
    some metrics have variants, many papers do not specify the particular version
    used. In that case, we assume they are consistent. Third, the generated report
    sections vary among different methodologies. Most articles do not explicitly specify
    the generated report sections, making it challenging to conduct comparative analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We separate the comparison results based on the generated report section, and
    the completed comparison tables are provided in Table [A2](#Sx1.T2 "Table A2 ‣
    Appendix A ‣ A Survey of Deep Learning-based Radiology Report Generation Using
    Multimodal Data") in Appendix A. Table [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data") shows the best and second best performances ranked for each evaluation
    metric. Among the 14 papers in Table [3](#S6.T3 "Table 3 ‣ 6 Benchmark Comparison
    ‣ A Survey of Deep Learning-based Radiology Report Generation Using Multimodal
    Data"), three of them (Song et al., [2022](#bib.bib104); Liu et al., [2023b](#bib.bib74);
    Jin et al., [2024](#bib.bib43)) utilized multi-modality inputs. From an architectural
    perspective, in the encoding stage, five works utilized a pure Transformer model
    (Wang et al., [2022e](#bib.bib130); Kong et al., [2022](#bib.bib50); Liu et al.,
    [2023b](#bib.bib74); Wang et al., [2023d](#bib.bib131), [2022d](#bib.bib129)),
    another five works employed the CNN-based model (Pino et al., [2021](#bib.bib88);
    Song et al., [2022](#bib.bib104); Wu et al., [2022](#bib.bib133); Jia et al.,
    [2022](#bib.bib42); Jin et al., [2024](#bib.bib43)), and four works combined CNN
    and Transformer (Chen et al., [2021](#bib.bib15); Wang et al., [2023c](#bib.bib127),
    [2024a](#bib.bib121); Wu et al., [2023](#bib.bib134)). In the generation stage,
    most papers relied on the Transformer. In terms of technical modules, six works
    (Wang et al., [2022e](#bib.bib130); Jin et al., [2024](#bib.bib43); Wang et al.,
    [2023d](#bib.bib131); Wu et al., [2023](#bib.bib134); Wang et al., [2024a](#bib.bib121),
    [2022d](#bib.bib129)) designed auxiliary tasks and five works (Song et al., [2022](#bib.bib104);
    Wu et al., [2022](#bib.bib133); Wang et al., [2023d](#bib.bib131); Liu et al.,
    [2023b](#bib.bib74); Wang et al., [2022e](#bib.bib130)) used contrastive learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Comparisons of the model performance on the MIMIC-CXR Dataset. B1,
    B2, B3, B4, R-L, C-D, P, R, and F represent BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L,
    CIDEr-D, precision, recall, and F1 score, respectively. The best and second best
    results are highlighted. All values were extracted from their papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | B1$\uparrow$ | B2$\uparrow$ | B3$\uparrow$ | B4$\uparrow$ | R-L$\uparrow$
    | METEOR$\uparrow$ | C-D$\uparrow$ | P$\uparrow$ | R$\uparrow$ | F$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Findings Section |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. ([2021](#bib.bib15)) | 0.353 | 0.218 | 0.148 | 0.106 | 0.278
    | 0.142 | - | 0.334 | 0.275 | 0.278 |'
  prefs: []
  type: TYPE_TB
- en: '| Pino et al. ([2021](#bib.bib88)) | - | - | - | - | 0.185 | - | 0.238 | 0.381
    | 0.531 | 0.428 |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. ([2022](#bib.bib104)) | 0.360 | 0.227 | 0.156 | 0.117 | 0.287
    | 0.148 | - | 0.444 | 0.297 | 0.356 |'
  prefs: []
  type: TYPE_TB
- en: '| Impression + Findings Section |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. ([2022](#bib.bib133)) | 0.340 | 0.212 | 0.145 | 0.103 | 0.270 |
    0.139 | 0.109 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2022d](#bib.bib129)) | 0.351 | 0.223 | 0.157 | 0.118 | 0.287
    | - | 0.281 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Jia et al. ([2022](#bib.bib42)) | 0.363 | 0.228 | 0.156 | 0.130 | 0.300 |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Unspecified generated sections |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2023c](#bib.bib127)) | 0.363 | 0.235 | 0.164 | 0.118 | 0.301
    | 0.136 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2024a](#bib.bib121)) | 0.374 | 0.230 | 0.155 | 0.112 | 0.279
    | 0.145 | 0.161 | 0.483 | 0.323 | 0.387 |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. ([2023](#bib.bib134)) | 0.383 | 0.224 | 0.146 | 0.104 | 0.280 |
    0.147 | - | - | - | 0.758 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2023d](#bib.bib131)) | 0.386 | 0.250 | 0.169 | 0.124 | 0.291
    | 0.152 | 0.362 | 0.364 | 0.309 | 0.311 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. ([2023b](#bib.bib74)) | 0.391 | 0.249 | 0.172 | 0.125 | 0.304
    | 0.160 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. ([2024](#bib.bib43)) | 0.398 | - | - | 0.112 | 0.268 | 0.157 |
    - | 0.501 | 0.509 | 0.476 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. ([2022e](#bib.bib130)) | 0.413 | 0.266 | 0.186 | 0.136 | 0.298
    | 0.170 | 0.429 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Kong et al. ([2022](#bib.bib50)) | 0.423 | 0.261 | 0.171 | 0.116 | 0.286
    | 0.168 | - | 0.482 | 0.563 | 0.519 |'
  prefs: []
  type: TYPE_TB
- en: 7 Challenges and Future Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although automated systems offer promising efficiency for clinical workflows,
    current methods have not produced very high-quality reports. This section evaluates
    the current progress in automated report generation development and identifies
    potential areas for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Constructing and utilizing multi-modal data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Considering report generation as a multi-modal problem is more aligned with
    clinical practice (Tu et al., [2024](#bib.bib116); Yan et al., [2023](#bib.bib141)).
    Babar et al. ([2021a](#bib.bib4)) have proven the ineffectiveness of simple encoder-decoder
    report generation models and mentioned that adding prior knowledge can be a promising
    method. However, the current utilization of multi-modal data remains under-explored.
    Firstly, the methods for non-image feature extraction and the fusion of multi-modality
    data are often limited and simplistic, such as using graph encoding for the knowledge
    base and attention mechanisms for feature fusion. Secondly, the construction of
    the knowledge base is imperfect. The pre-defined graph (Zhang et al., [2020](#bib.bib150))
    is overly simplistic. Despite Radgraph being a vast knowledge base, it is solely
    derived from reports, lacking the relationship between images and reports, such
    as organ recognition or understanding of typical radiological scenarios, which
    radiologists possess. The Chest ImaGenome dataset (Wu et al., [2021](#bib.bib132))
    provides the organ recognition annotations, alleviating the problem. Additionally,
    as far as we know, publicly available knowledge bases only concentrate on chest
    X-rays, leaving a gap in general medical knowledge databases.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Evaluation of medical correctness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluating the medical correctness of generated reports is crucial for the clinical
    application. Compared to previous works (Messina et al., [2022](#bib.bib75); Liao
    et al., [2023](#bib.bib64)), recent works have paid more attention to it, but
    still have two shortcomings. First, in the reviewed articles, medical correctness
    evaluation has only been applied to chest X-ray reports. Secondly, the evaluation
    is based on the automatic labeler of radiology reports, which are only targeted
    at 14 types of diseases and the average F1 score is around 0.798 (Smit et al.,
    [2020](#bib.bib103)). Thus, improving the accuracy and scale of automatic labeling
    tools can help optimize the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3 Large public datasets and unified comparison benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in Table [1](#S4.T1 "Table 1 ‣ 4 Datasets ‣ A Survey of Deep Learning-based
    Radiology Report Generation Using Multimodal Data"), most of the public datasets
    are limited in size. Deep learning-based techniques require a large amount of
    data. The contemporary prevalence of large language models underscores this need
    for extensive data volumes. Among the datasets, the MIMIC dataset is relatively
    large but only includes Chest X-ray data. Large datasets targeting other image
    modalities and diseases need to be constructed. In addition, while MIMIC-CXR is
    a well-established benchmark compared to IU-XRay, dataset utilization lacks standardization,
    complicating comparisons. We urge papers using the MIMIC dataset to define their
    training, validation, and testing partitions, with explicit disclosure of the
    filtration method, particularly for the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4 Human-AI interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most papers overlook the interaction between users (e.g., clinicians or patients)
    and automated systems. When the system performs as an AI assistant, users may
    want to know the insights of the model regarding specific aspects of medical images.
    In the reviewed works, Tanwani et al. ([2022](#bib.bib113)) constructed a Visual
    Question Answering system for medical report generation, and Tanida et al. ([2023](#bib.bib112))
    linked the output results to image regions using object detection, allowing users
    to select areas of interest and receive corresponding language explanations. Recently,
    dialogue systems (e.g., GPT-4 (Achiam et al., [2023](#bib.bib2)), PaLM (Chowdhery
    et al., [2023](#bib.bib16)), Gemini (Team et al., [2023](#bib.bib114))) based
    on large language models and large multi-modal models have shown people more possibilities
    for human-AI interaction. Although general large language models can provide answers
    to some questions in medical question answering benchmarks, their deployment in
    clinical settings remains unfeasible due to safety concerns within the medical
    domain (Yan et al., [2023](#bib.bib141); Saab et al., [2024](#bib.bib95)).
  prefs: []
  type: TYPE_NORMAL
- en: To enable dialogue systems to comprehend medical knowledge, fine-tuning the
    model with medical data is an intuitive approach, such as LLaVA-Med (Li et al.,
    [2024](#bib.bib55)) and Med-PaLM 2 (Singhal et al., [2023](#bib.bib101)). However,
    they did not test the model’s performance on the report generation task. Saab
    et al. ([2024](#bib.bib95)) proposed Med-Gemini, a series of highly proficient
    multi-modal models tailored specifically for the medical domain. They intuitively
    showed the interactive report generation process for a normal case, while lacking
    quantitative results to evaluate report generation performance. Exploring the
    direction of report generation with human-AI interaction holds significant promise.
    Additionally, fine-tuning large models demands substantial GPU resources, making
    efficient methods crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5 Standardized report generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the existent methods focused on unstructured report generation, while
    structured reporting has several advantages, such as saving time (Hong and Kahn,
    [2013](#bib.bib30); Nobel et al., [2022](#bib.bib82)), preventing errors, decreasing
    communication expenses linked to ambiguous natural language. Recently, Pellegrini
    et al. ([2023](#bib.bib87)) developed a structured template and released a related
    dataset. This promising start could pave the way for further exploration in this
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we have conducted a detailed technical review of 89 papers on
    automatic medical report generation published in the years 2021, 2022, 2023, and
    2024 to showcase both mainstream and novel techniques. Our particular focus lies
    on the utilization and fusion of multi-modality data. The analysis of methods
    is structured based on the components of the report generation pipeline, presenting
    the key techniques for each component. Additionally, we provide a summary of the
    current publicly available datasets and evaluation methods, encompassing both
    quantitative and qualitative assessments. Subsequently, we compare the results
    from a subset of the 89 papers under the same experimental setting. Finally, we
    outline the current challenges and propose future directions for medical report
    generation. Overall, sustained progress is needed to produce standardized and
    clinically accurate reports. This survey aims to offer a comprehensive overview
    of report-generation techniques, emphasize critical issues, and assist researchers
    in promptly grasping recent advancements in the field to build more robust systems
    for clinical practice.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abela et al. [2022] Brandon Abela, Jumana Abu-Khalaf, Chi-Wei Robin Yang, Martin
    Masek, and Ashu Gupta. Automated radiology report generation using a transformer-template
    system: Improved clinical accuracy and an assessment of clinical safety. In *AI
    2022: Advances in Artificial Intelligence: 35th Australasian Joint Conference,
    AI 2022, Perth, WA, Australia, December 5–8, 2022, Proceedings*, pages 530–543\.
    Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alfarghaly et al. [2021] Omar Alfarghaly, Rana Khaled, Abeer Elkorany, Maha
    Helal, and Aly Fahmy. Automated radiology report generation using conditioned
    transformers. *Informatics in Medicine Unlocked*, 24:100557, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Babar et al. [2021a] Zaheer Babar, Twan van Laarhoven, and Elena Marchiori.
    Encoder-decoder models for chest x-ray report generation perform no better than
    unconditioned baselines. *Plos one*, 16(11):e0259639, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Babar et al. [2021b] Zaheer Babar, Twan van Laarhoven, Fabio Massimo Zanzotto,
    and Elena Marchiori. Evaluating diagnostic content of ai-generated radiology reports
    of chest x-rays. *Artificial Intelligence in Medicine*, 116:102075, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Banerjee and Lavie [2005] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic
    metric for mt evaluation with improved correlation with human judgments. In *Proceedings
    of the acl workshop on intrinsic and extrinsic evaluation measures for machine
    translation and/or summarization*, pages 65–72, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beddiar et al. [2023] Djamila-Romaissa Beddiar, Mourad Oussalah, and Tapio
    Seppänen. Automatic captioning for medical imaging (mic): a rapid review of literature.
    *Artificial Intelligence Review*, 56(5):4019–4076, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2022] Yiming Cao, Lizhen Cui, Fuqiang Yu, Lei Zhang, Zhen Li, Ning
    Liu, and Yonghui Xu. Kdtnet: medical image report generation via knowledge-driven
    transformer. In *Database Systems for Advanced Applications: 27th International
    Conference, DASFAA 2022, Virtual Event, April 11–14, 2022, Proceedings, Part III*,
    pages 117–132\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. [2023] Yiming Cao, Lizhen Cui, Lei Zhang, Fuqiang Yu, Ziheng Cheng,
    Zhen Li, Yonghui Xu, and Chunyan Miao. Cmt: Cross-modal memory transformer for
    medical image report generation. In *International Conference on Database Systems
    for Advanced Applications*, pages 415–424\. Springer, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carreira and Zisserman [2017] Joao Carreira and Andrew Zisserman. Quo vadis,
    action recognition? a new model and the kinetics dataset. In *proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*, pages 6299–6308,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2019] Qingyu Chen, Yifan Peng, and Zhiyong Lu. Biosentvec: creating
    sentence embeddings for biomedical texts. In *2019 IEEE International Conference
    on Healthcare Informatics (ICHI)*, pages 1–5\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2022] Weipeng Chen, Haiwei Pan, Kejia Zhang, Xin Du, and Qianna
    Cui. Vmeknet: Visual memory and external knowledge based network for medical report
    generation. In *PRICAI 2022: Trends in Artificial Intelligence: 19th Pacific Rim
    International Conference on Artificial Intelligence, PRICAI 2022, Shanghai, China,
    November 10–13, 2022, Proceedings, Part I*, pages 188–201\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. [2020] Zhihong Chen, Yan Song, Tsung-Hui Chang, and Xiang Wan. Generating
    radiology reports via memory-driven transformer. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 1439–1449,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2021] Zhihong Chen, Yaling Shen, Yan Song, and Xiang Wan. Cross-modal
    memory networks for radiology report generation. In *Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    5904–5914, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. [2023] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 24(240):1–113, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cornia et al. [2020] Marcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and
    Rita Cucchiara. Meshed-memory transformer for image captioning. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    10578–10587, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalla Serra et al. [2022] Francesco Dalla Serra, William Clackett, Hamish MacKinnon,
    Chaoyang Wang, Fani Deligianni, Jeff Dalton, and Alison Q O’Neil. Multimodal generation
    of radiology reports using knowledge-grounded extraction of entities and relations.
    In *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics and the 12th International Joint Conference on Natural
    Language Processing*, pages 615–624, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalla Serra et al. [2023a] Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni,
    Jeff Dalton, and Alison Q O’Neil. Controllable chest x-ray report generation from
    longitudinal representations. In *The 2023 Conference on Empirical Methods in
    Natural Language Processing*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dalla Serra et al. [2023b] Francesco Dalla Serra, Chaoyang Wang, Fani Deligianni,
    Jeffrey Dalton, and Alison Q O’Neil. Finding-aware anatomical tokens for chest
    x-ray automated reporting. In *International Workshop on Machine Learning in Medical
    Imaging*, pages 413–423\. Springer, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demner-Fushman et al. [2016] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman,
    Sonya E Shooshan, Laritza Rodriguez, Sameer Antani, George R Thoma, and Clement J
    McDonald. Preparing a collection of radiology examinations for distribution and
    retrieval. *Journal of the American Medical Informatics Association*, 23(2):304–310,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. [2022] Xin Du, Haiwei Pan, Kejia Zhang, Shuning He, Xiaofei Bian,
    and Weipeng Chen. Automatic report generation method based on multiscale feature
    extraction and word attention network. In *Asia-Pacific Web (APWeb) and Web-Age
    Information Management (WAIM) Joint International Conference on Web and Big Data*,
    pages 520–528\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Endo et al. [2021] Mark Endo, Rayan Krishnan, Viswesh Krishna, Andrew Y Ng,
    and Pranav Rajpurkar. Retrieval-based chest x-ray report generation using a pre-trained
    contrastive language-image model. In *Machine Learning for Health*, pages 209–219\.
    PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gajbhiye et al. [2022] Gaurav O Gajbhiye, Abhijeet V Nandedkar, and Ibrahima
    Faye. Translating medical image to radiological report: Adaptive multilevel multi-attention
    approach. *Computer Methods and Programs in Biomedicine*, 221:106853, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. [2024] Tiancheng Gu, Dongnan Liu, Zhiyuan Li, and Weidong Cai. Complex
    organ mask guided radiology report generation. In *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pages 7995–8004, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. [2021] Zhongyi Han, Benzheng Wei, Xiaoming Xi, Bo Chen, Yilong Yin,
    and Shuo Li. Unifying neural learning and symbolic reasoning for spinal medical
    report generation. *Medical image analysis*, 67:101872, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 770–778, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2020] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
    Momentum contrast for unsupervised visual representation learning. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pages
    9729–9738, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural computation*, 9(8):1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hong and Kahn [2013] Yi Hong and Charles E Kahn. Content analysis of reporting
    templates and free-text radiology reports. *Journal of digital imaging*, 26:843–849,
    2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hou et al. [2021a] Benjamin Hou, Georgios Kaissis, Ronald M Summers, and Bernhard
    Kainz. Ratchet: Medical transformer for chest x-ray diagnosis and reporting. In
    *Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part VII 24*, pages 293–303\. Springer, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. [2021b] Daibing Hou, Zijian Zhao, Yuying Liu, Faliang Chang, and
    Sanyuan Hu. Automatic report generation for chest x-ray images via adversarial
    reinforcement learning. *IEEE Access*, 9:21236–21250, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2017] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q
    Weinberger. Densely connected convolutional networks. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4700–4708, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2021a] Jia-Hong Huang, Ting-Wei Wu, Chao-Han Huck Yang, and Marcel
    Worring. Deep context-encoding network for retinal image captioning. In *2021
    IEEE International Conference on Image Processing (ICIP)*, pages 3762–3766\. IEEE,
    2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2021b] Jia-Hong Huang, C-H Huck Yang, Fangyu Liu, Meng Tian,
    Yi-Chieh Liu, Ting-Wei Wu, I Lin, Kang Wang, Hiromasa Morikawa, Hernghua Chang,
    et al. Deepopht: medical report generation for retinal images via deep models
    and visual explanation. In *Proceedings of the IEEE/CVF winter conference on applications
    of computer vision*, pages 2442–2452, 2021b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2022] Jia-Hong Huang, Ting-Wei Wu, C-H Huck Yang, Zenglin Shi,
    I Lin, Jesper Tegner, Marcel Worring, et al. Non-local attention improves description
    generation for retinal images. In *Proceedings of the IEEE/CVF winter conference
    on applications of computer vision*, pages 1606–1615, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. [2023] Zhongzhen Huang, Xiaofan Zhang, and Shaoting Zhang. Kiut:
    Knowledge-injected u-transformer for radiology report generation. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    19809–19818, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irvin et al. [2019] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana
    Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie
    Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty
    labels and expert comparison. In *Proceedings of the AAAI conference on artificial
    intelligence*, volume 33, pages 590–597, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jain et al. [2021] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong,
    Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y
    Ng, et al. Radgraph: Extracting clinical entities and relations from radiology
    reports. *arXiv preprint arXiv:2106.14463*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jeong et al. [2024] Jaehwan Jeong, Katherine Tian, Andrew Li, Sina Hartung,
    Subathra Adithan, Fardad Behzadi, Juan Calle, David Osayande, Michael Pohlen,
    and Pranav Rajpurkar. Multimodal image-text matching improves retrieval-based
    chest x-ray report generation. In *Medical Imaging with Deep Learning*, pages
    978–990\. PMLR, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. [2021] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Blackley Suzanne,
    Yangyong Zhu, and Chunlei Tang. Radiology report generation for rare diseases
    via few-shot transformer. In *IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM)*, pages 1347–1352\. IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jia et al. [2022] Xing Jia, Yun Xiong, Jiawei Zhang, Yao Zhang, Yangyong Zhu,
    and S Yu Philip. Few-shot radiology report generation via knowledge transfer and
    multi-modal alignment. In *2022 IEEE International Conference on Bioinformatics
    and Biomedicine (BIBM)*, pages 1574–1579\. IEEE, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. [2024] Haibo Jin, Haoxuan Che, Yi Lin, and Hao Chen. Promptmrg:
    Diagnosis-driven prompts for medical report generation. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 38, pages 2607–2615, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. [2019a] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz,
    Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven
    Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs
    with free-text reports. *Scientific data*, 6(1):317, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. [2019b] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum,
    Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J
    Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available database
    of labeled chest radiographs. *arXiv preprint arXiv:1901.07042*, 2019b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaur and Mittal [2022a] Navdeep Kaur and Ajay Mittal. Radiobert: A deep learning-based
    system for medical report generation from chest x-ray images using contextual
    embeddings. *Journal of Biomedical Informatics*, 135:104220, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaur and Mittal [2022b] Navdeep Kaur and Ajay Mittal. Cadxreport: Chest x-ray
    report generation using co-attention mechanism and reinforcement learning. *Computers
    in Biology and Medicine*, 145:105498, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaur et al. [2022] Navdeep Kaur, Ajay Mittal, and Gurprem Singh. Methods for
    automatic generation of radiological reports of chest radiographs: a comprehensive
    survey. *Multimedia Tools and Applications*, 81(10):13409–13439, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational
    bayes. *arXiv preprint arXiv:1312.6114*, 2013.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kong et al. [2022] Ming Kong, Zhengxing Huang, Kun Kuang, Qiang Zhu, and Fei
    Wu. Transq: Transformer-based semantic query for medical report generation. In
    *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    VIII*, pages 610–620\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Langlotz [2006] Curtis P Langlotz. Radlex: a new method for indexing online
    educational materials, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lau et al. [2018] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.
    A dataset of clinically generated visual questions and answers about radiology
    images. *Scientific data*, 5(1):1–10, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. [2022] Hojun Lee, Hyunjun Cho, Jieun Park, Jinyeong Chae, and Jihie
    Kim. Cross encoder-decoder transformer with global-local visual extractor for
    medical image captioning. *Sensors*, 22(4):1429, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. [2020] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
    Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation
    model for biomedical text mining. *Bioinformatics*, 36(4):1234–1240, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2024] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2022a] Jun Li, Shibo Li, Ying Hu, and Huiren Tao. A self-guided
    framework for radiology report generation. In *Medical Image Computing and Computer
    Assisted Intervention–MICCAI 2022: 25th International Conference, Singapore, September
    18–22, 2022, Proceedings, Part VIII*, pages 588–598\. Springer, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2020] Mingjie Li, Fuyu Wang, Xiaojun Chang, and Xiaodan Liang. Auxiliary
    signal-guided knowledge encoder-decoder for medical report generation. *arXiv
    preprint arXiv:2006.03744*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2021] Mingjie Li, Wenjia Cai, Rui Liu, Yuetian Weng, Xiaoyun Zhao,
    Cong Wang, Xin Chen, Zhong Liu, Caineng Pan, Mengke Li, et al. Ffa-ir: Towards
    an explainable and reliable medical report generation benchmark. In *Thirty-fifth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track
    (Round 2)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2022b] Mingjie Li, Wenjia Cai, Karin Verspoor, Shirui Pan, Xiaodan
    Liang, and Xiaojun Chang. Cross-modal clinical graph transformer for ophthalmic
    report generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 20656–20665, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023a] Mingjie Li, Bingqian Lin, Zicong Chen, Haokun Lin, Xiaodan
    Liang, and Xiaojun Chang. Dynamic graph enhanced contrastive learning for chest
    x-ray report generation. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 3334–3343, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023b] Mingjie Li, Rui Liu, Fuyu Wang, Xiaojun Chang, and Xiaodan
    Liang. Auxiliary signal-guided knowledge encoder-decoder for medical report generation.
    *World Wide Web*, 26(1):253–270, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. [2023c] Qingqiu Li, Jilan Xu, Runtian Yuan, Mohan Chen, Yuejie Zhang,
    Rui Feng, Xiaobo Zhang, and Shang Gao. Enhanced knowledge injection for radiology
    report generation. In *2023 IEEE International Conference on Bioinformatics and
    Biomedicine (BIBM)*, pages 2053–2058\. IEEE, 2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023d] Yaowei Li, Bang Yang, Xuxin Cheng, Zhihong Zhu, Hongxiang
    Li, and Yuexian Zou. Unify, align and refine: Multi-level semantic alignment for
    radiology report generation. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 2863–2874, 2023d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. [2023] Yuxiang Liao, Hantao Liu, and Irena Spasić. Deep learning
    approaches to automatic radiology report generation: A systematic review. *Informatics
    in Medicine Unlocked*, page 101273, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin [2004] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In *Text summarization branches out*, pages 74–81, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. [2023] Zhihong Lin, Donghao Zhang, Danli Shi, Renjing Xu, Qingyi
    Tao, Lin Wu, Mingguang He, and Zongyuan Ge. Contrastive pre-training and linear
    interaction attention-based transformer for universal medical reports generation.
    *Journal of Biomedical Informatics*, page 104281, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023a] Chang Liu, Yuanhe Tian, and Yan Song. A systematic review
    of deep learning-based research on radiology report generation. *arXiv preprint
    arXiv:2311.14199*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021a] Fenglin Liu, Shen Ge, and Xian Wu. Competence-based multimodal
    curriculum learning for medical report generation. In *Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages
    3001–3012, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021b] Fenglin Liu, Xian Wu, Shen Ge, Wei Fan, and Yuexian Zou.
    Exploring and distilling posterior and prior knowledge for radiology report generation.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 13753–13762, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021c] Fenglin Liu, Changchang Yin, Xian Wu, Shen Ge, Ping Zhang,
    and Xu Sun. Contrastive attention for automatic chest x-ray report generation.
    In *Findings of the Association for Computational Linguistics: ACL-IJCNLP*, page
    269–280, 2021c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2021d] Fenglin Liu, Chenyu You, Xian Wu, Shen Ge, Xu Sun, et al.
    Auto-encoding knowledge graph for unsupervised medical report generation. *Advances
    in Neural Information Processing Systems*, 34:16266–16279, 2021d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2021e] Guangyi Liu, Yinghong Liao, Fuyu Wang, Bin Zhang, Lu Zhang,
    Xiaodan Liang, Xiang Wan, Shaolin Li, Zhen Li, Shuixing Zhang, et al. Medical-vlbert:
    Medical visual language bert for covid-19 ct report generation with alternate
    learning. *IEEE Transactions on Neural Networks and Learning Systems*, 32(9):3786–3797,
    2021e.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2019] Guanxiong Liu, Tzu-Ming Harry Hsu, Matthew McDermott, Willie
    Boag, Wei-Hung Weng, Peter Szolovits, and Marzyeh Ghassemi. Clinically accurate
    chest x-ray report generation. In *Machine Learning for Healthcare Conference*,
    pages 249–269\. PMLR, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. [2023b] Zhizhe Liu, Zhenfeng Zhu, Shuai Zheng, Yawei Zhao, Kunlun
    He, and Yao Zhao. From observation to concept: A flexible multi-view paradigm
    for medical report generation. *IEEE Transactions on Multimedia*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messina et al. [2022] Pablo Messina, Pablo Pino, Denis Parra, Alvaro Soto, Cecilia
    Besa, Sergio Uribe, Marcelo Andía, Cristian Tejos, Claudia Prieto, and Daniel
    Capurro. A survey on deep learning and explainability for automatic report generation
    from medical images. *ACM Computing Surveys (CSUR)*, 54(10s):1–40, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirikharaji et al. [2023] Zahra Mirikharaji, Kumar Abhishek, Alceu Bissoto,
    Catarina Barata, Sandra Avila, Eduardo Valle, M Emre Celebi, and Ghassan Hamarneh.
    A survey on deep learning for skin lesion segmentation. *Medical Image Analysis*,
    page 102863, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miura et al. [2021] Yasuhide Miura, Yuhao Zhang, Emily Tsai, Curtis Langlotz,
    and Dan Jurafsky. Improving factual completeness and consistency of image-to-text
    radiology report generation. In *Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies*, pages 5288–5304, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mohsan et al. [2022] Mashood Mohammad Mohsan, Muhammad Usman Akram, Ghulam Rasool,
    Norah Saleh Alghamdi, Muhammad Abdullah Aamer Baqai, and Muhammad Abbas. Vision
    transformer and language model based radiology report generation. *IEEE Access*,
    11:1814–1824, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moon et al. [2022] Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim,
    and Edward Choi. Multi-modal understanding and generation for medical images and
    text via vision-language pre-training. *IEEE Journal of Biomedical and Health
    Informatics*, 26(12):6070–6080, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Najdenkoska et al. [2021] Ivona Najdenkoska, Xiantong Zhen, Marcel Worring,
    and Ling Shao. Variational topic inference for chest x-ray report generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th
    International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
    Part III 24*, pages 625–635\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Najdenkoska et al. [2022] Ivona Najdenkoska, Xiantong Zhen, Marcel Worring,
    and Ling Shao. Uncertainty-aware report generation for chest x-rays by variational
    topic inference. *Medical Image Analysis*, 82:102603, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nobel et al. [2022] J Martijn Nobel, Koos van Geel, and Simon GF Robben. Structured
    reporting in radiology: a systematic review to explore its potential. *European
    radiology*, pages 1–18, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI [2023] OpenAI. Chatgpt: Optimizing language models for dialogue. 2023.
    URL [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pahwa et al. [2021] Esha Pahwa, Dwij Mehta, Sanjeet Kapadia, Devansh Jain,
    and Achleshwar Luthra. Medskip: Medical report generation using skip connections
    and integrated attention. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 3409–3415, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pandey et al. [2021] Abhineet Pandey, Bhawna Paliwal, Abhinav Dhall, Ramanathan
    Subramanian, and Dwarikanath Mahapatra. This explains that: Congruent image–report
    generation for explainable medical image analysis with cyclic generative adversarial
    networks. In *Interpretability of Machine Intelligence in Medical Image Computing,
    and Topological Data Analysis and Its Applications for Medical Data: 4th International
    Workshop, iMIMIC 2021, and 1st International Workshop, TDA4MedicalData 2021, Held
    in Conjunction with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings
    4*, pages 34–43\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In *Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pellegrini et al. [2023] Chantal Pellegrini, Matthias Keicher, Ege Özsoy, and
    Nassir Navab. Rad-restruct: A novel vqa benchmark and method for structured radiology
    reporting. In *International Conference on Medical Image Computing and Computer-Assisted
    Intervention*, pages 409–419\. Springer, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pino et al. [2021] Pablo Pino, Denis Parra, Cecilia Besa, and Claudio Lagos.
    Clinically correct report generation from chest x-rays using templates. In *Machine
    Learning in Medical Imaging: 12th International Workshop, MLMI 2021, Held in Conjunction
    with MICCAI 2021, Strasbourg, France, September 27, 2021, Proceedings 12*, pages
    654–663\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Platanios et al. [2019] Emmanouil Antonios Platanios, Otilia Stretcu, Graham
    Neubig, Barnabás Poczós, and Tom Mitchell. Competence-based curriculum learning
    for neural machine translation. In *Proceedings of the 2019 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long and Short Papers)*, pages 1162–1172, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin and Song [2022] Han Qin and Yan Song. Reinforced cross-modal alignment
    for radiology report generation. In *Findings of the Association for Computational
    Linguistics: ACL 2022*, pages 448–458, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image
    generation. In *International conference on machine learning*, pages 8821–8831\.
    Pmlr, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. [2022] Vignav Ramesh, Nathan A Chi, and Pranav Rajpurkar. Improving
    radiology report generation systems by removing hallucinated references to non-existent
    priors. In *Machine Learning for Health*, pages 456–473\. PMLR, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2015] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster
    r-cnn: Towards real-time object detection with region proposal networks. *Advances
    in neural information processing systems*, 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saab et al. [2024] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David
    Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al.
    Capabilities of gemini models in medicine. *arXiv preprint arXiv:2404.18416*,
    2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selivanov et al. [2023] Alexander Selivanov, Oleg Y Rogov, Daniil Chesakov,
    Artem Shelmanov, Irina Fedulova, and Dmitry V Dylov. Medical image captioning
    via generative pretrained transformers. *Scientific Reports*, 13(1):4171, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shamshad et al. [2023] Fahad Shamshad, Salman Khan, Syed Waqas Zamir, Muhammad Haris
    Khan, Munawar Hayat, Fahad Shahbaz Khan, and Huazhu Fu. Transformers in medical
    imaging: A survey. *Medical Image Analysis*, page 102802, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shetty et al. [2023] Shashank Shetty, Ananthanarayana VS, and Ajit Mahale. Cross-modal
    deep learning-based clinical recommendation system for radiology report generation
    from chest x-rays. *International Journal of Engineering*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep
    convolutional networks for large-scale image recognition. *arXiv preprint arXiv:1409.1556*,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. [2021] Sonit Singh, Sarvnaz Karimi, Kevin Ho-Shon, and Len Hamey.
    Show, tell and summarise: learning to generate and summarise radiology findings
    from medical images. *Neural Computing and Applications*, 33:7441–7465, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singhal et al. [2023] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery
    Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal,
    et al. Towards expert-level medical question answering with large language models.
    *arXiv preprint arXiv:2305.09617*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sirshar et al. [2022] Mehreen Sirshar, Muhammad Faheem Khalil Paracha, Muhammad Usman
    Akram, Norah Saleh Alghamdi, Syeda Zainab Yousuf Zaidi, and Tatheer Fatima. Attention
    based automated radiology report generation using cnn and lstm. *Plos one*, 17(1):e0262209,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smit et al. [2020] Akshay Smit, Saahil Jain, Pranav Rajpurkar, Anuj Pareek,
    Andrew Y Ng, and Matthew Lungren. Combining automatic labelers and expert annotations
    for accurate radiology report labeling using bert. In *Proceedings of the 2020
    Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages
    1500–1519, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. [2022] Xiao Song, Xiaodan Zhang, Junzhong Ji, Ying Liu, and Pengxu
    Wei. Cross-modal contrastive attention model for medical report generation. In
    *Proceedings of the 29th International Conference on Computational Linguistics*,
    pages 2388–2397, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics [2020] Statistics. Diagnostic imaging dataset., 2020. URL [https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/](https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2022] Jinghan Sun, Dong Wei, Liansheng Wang, and Yefeng Zheng.
    Lesion guided explainable few weak-shot medical report generation. In *Medical
    Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th International
    Conference, Singapore, September 18–22, 2022, Proceedings, Part V*, pages 615–625\.
    Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2019a] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution
    representation learning for human pose estimation. In *Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition*, pages 5693–5703, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023] Zhaoyi Sun, Hanley Ong, Patrick Kennedy, Liyan Tang, Shirley
    Chen, Jonathan Elias, Eugene Lucas, George Shih, and Yifan Peng. Evaluating gpt-4
    on impressions generation in radiology reports. *Radiology*, 307(5):e231259, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2019b] Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang.
    Rotate: Knowledge graph embedding by relational rotation in complex space. *arXiv
    preprint arXiv:1902.10197*, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. [2016] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, pages 2818–2826, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan and Le [2019] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
    scaling for convolutional neural networks. In *International conference on machine
    learning*, pages 6105–6114\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanida et al. [2023] Tim Tanida, Philip Müller, Georgios Kaissis, and Daniel
    Rueckert. Interactive and explainable region-guided radiology report generation.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 7433–7442, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tanwani et al. [2022] Ajay K Tanwani, Joelle Barral, and Daniel Freedman. Repsnet:
    Combining vision with language for automated medical reports. In *Medical Image
    Computing and Computer Assisted Intervention–MICCAI 2022: 25th International Conference,
    Singapore, September 18–22, 2022, Proceedings, Part V*, pages 714–724\. Springer,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Topol [2019] Eric Topol. *Deep medicine: how artificial intelligence can make
    healthcare human again*. Hachette UK, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tu et al. [2024] Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed
    Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al.
    Towards generalist biomedical ai. *NEJM AI*, 1(3):AIoa2300138, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Van Miltenburg et al. [2018] Emiel Van Miltenburg, Desmond Elliott, and Piek
    Vossen. Measuring the diversity of automatic image descriptions. In *Proceedings
    of the 27th International Conference on Computational Linguistics*, pages 1730–1741,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vedantam et al. [2015] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
    Cider: Consensus-based image description evaluation. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pages 4566–4575, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022a] Jun Wang, Abhir Bhalerao, and Yulan He. Cross-modal prototype
    driven network for radiology report generation. In *Computer Vision–ECCV 2022:
    17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXXV*, pages 563–579\. Springer, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2024a] Jun Wang, Abhir Bhalerao, Terry Yin, Simon See, and Yulan
    He. Camanet: class activation map guided attention network for radiology report
    generation. *IEEE Journal of Biomedical and Health Informatics*, 2024a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022b] Lin Wang, Munan Ning, Donghuan Lu, Dong Wei, Yefeng Zheng,
    and Jie Chen. An inclusive task-aware framework for radiology report generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    VIII*, pages 568–577\. Springer, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Sheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang, and Dinggang
    Shen. Chatcad: Interactive computer-aided diagnosis on medical image using large
    language models. *arXiv preprint arXiv:2302.07257*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023b] Siyuan Wang, Bo Peng, Yichao Liu, and Qi Peng. Fine-grained
    medical vision-language representation learning for radiology report generation.
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 15949–15956, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2022c] Song Wang, Liyan Tang, Mingquan Lin, George Shih, Ying Ding,
    and Yifan Peng. Prior knowledge enhances radiology report generation. In *AMIA
    Annual Symposium Proceedings*, volume 2022, page 486\. American Medical Informatics
    Association, 2022c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2024b] Yixin Wang, Zihao Lin, Zhe Xu, Haoyu Dong, Jie Luo, Jiang
    Tian, Zhongchao Shi, Lifu Huang, Yang Zhang, Jianping Fan, et al. Trust it or
    not: Confidence-guided automatic radiology report generation. *Neurocomputing*,
    page 127374, 2024b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2023c] Yuhao Wang, Kai Wang, Xiaohong Liu, Tianrun Gao, Jingyue
    Zhang, and Guangyu Wang. Self adaptive global-local feature enhancement for radiology
    report generation. In *2023 IEEE International Conference on Image Processing
    (ICIP)*, pages 2275–2279\. IEEE, 2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. [2021] Zhanyu Wang, Luping Zhou, Lei Wang, and Xiu Li. A self-boosting
    framework for automated radiographic report generation. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2433–2442,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022d] Zhanyu Wang, Hongwei Han, Lei Wang, Xiu Li, and Luping
    Zhou. Automated radiographic report generation purely on transformer: A multicriteria
    supervised approach. *IEEE Transactions on Medical Imaging*, 41(10):2803–2813,
    2022d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2022e] Zhanyu Wang, Mingkang Tang, Lei Wang, Xiu Li, and Luping
    Zhou. A medical semantic-assisted transformer for radiographic report generation.
    In *Medical Image Computing and Computer Assisted Intervention–MICCAI 2022: 25th
    International Conference, Singapore, September 18–22, 2022, Proceedings, Part
    III*, pages 655–664\. Springer, 2022e.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023d] Zhanyu Wang, Lingqiao Liu, Lei Wang, and Luping Zhou. Metransformer:
    Radiology report generation by transformer with multiple learnable expert tokens.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 11558–11567, 2023d.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2021] Joy Wu, Nkechinyere Agu, Ismini Lourentzou, Arjun Sharma, Joseph
    Paguio, Jasper Seth Yao, Edward Christopher Dee, William Mitchell, Satyananda
    Kashyap, Andrea Giovannini, et al. Chest imagenome dataset. *Physio Net*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2022] Xing Wu, Jingwen Li, Jianjia Wang, and Quan Qian. Multimodal
    contrastive learning for radiology report generation. *Journal of Ambient Intelligence
    and Humanized Computing*, pages 1–10, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2023] Yuexin Wu, I-Chan Huang, and Xiaolei Huang. Token imbalance
    adaptation for radiology report generation. In *Conference on Health, Inference,
    and Learning*, pages 72–85\. PMLR, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie et al. [2017] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and
    Kaiming He. Aggregated residual transformations for deep neural networks. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 1492–1500,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2020] Chen Xu, Bojie Hu, Yufan Jiang, Kai Feng, Zeyang Wang, Shen
    Huang, Qi Ju, Tong Xiao, and Jingbo Zhu. Dynamic curriculum learning for low-resource
    neural machine translation. In *Proceedings of the 28th International Conference
    on Computational Linguistics*, pages 3977–3989, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. [2023] Dexuan Xu, Huashi Zhu, Yu Huang, Zhi Jin, Weiping Ding, Hang
    Li, and Menglong Ran. Vision-knowledge fusion model for multi-domain medical report
    generation. *Information Fusion*, 97:101817, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xue et al. [2024] Youyuan Xue, Yun Tan, Ling Tan, Jiaohua Qin, and Xuyu Xiang.
    Generating radiology reports via auxiliary signal guidance and a memory-driven
    network. *Expert Systems with Applications*, 237:121260, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. [2022] Bin Yan, Mingtao Pei, Meng Zhao, Caifeng Shan, and Zhaoxing
    Tian. Prior guided transformer for accurate radiology reports generation. *IEEE
    Journal of Biomedical and Health Informatics*, 26(11):5631–5640, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan [2022] Sixing Yan. Memory-aligned knowledge graph for clinically accurate
    radiology image report generation. In *Proceedings of the 21st Workshop on Biomedical
    Language Processing*, pages 116–122, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. [2023] Zhiling Yan, Kai Zhang, Rong Zhou, Lifang He, Xiang Li, and
    Lichao Sun. Multimodal chatgpt for medical applications: an experimental study
    of gpt-4v. *arXiv preprint arXiv:2310.19061*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021a] Shaokang Yang, Jianwei Niu, Jiyan Wu, Yong Wang, Xuefeng
    Liu, and Qingfeng Li. Automatic ultrasound image report generation with adaptive
    multimodal attention mechanism. *Neurocomputing*, 427:40–49, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2022] Shuxin Yang, Xian Wu, Shen Ge, S Kevin Zhou, and Li Xiao.
    Knowledge matters: Chest radiology report generation with general and specific
    knowledge. *Medical Image Analysis*, 80:102510, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2023] Shuxin Yang, Xian Wu, Shen Ge, Zhuozhao Zheng, S Kevin Zhou,
    and Li Xiao. Radiology report generation with a learned knowledge base and multi-modal
    alignment. *Medical Image Analysis*, 86:102798, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2021b] Yan Yang, Jun Yu, Jian Zhang, Weidong Han, Hanliang Jiang,
    and Qingming Huang. Joint embedding of deep visual and semantic features for medical
    image report generation. *IEEE Transactions on Multimedia*, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [2021] Di You, Fenglin Liu, Shen Ge, Xiaoxia Xie, Jing Zhang, and
    Xian Wu. Aligntransformer: Hierarchical alignment of visual regions and disease
    tags for medical report generation. In *Medical Image Computing and Computer Assisted
    Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September
    27–October 1, 2021, Proceedings, Part III 24*, pages 72–82\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You et al. [2022] Jingyi You, Dongyuan Li, Manabu Okumura, and Kenji Suzuki.
    Jpg-jointly learn to align: Automated disease prediction and radiology report
    generation. In *Proceedings of the 29th International Conference on Computational
    Linguistics*, pages 5989–6001, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023a] Junsan Zhang, Xiuxuan Shen, Shaohua Wan, Sotirios K Goudos,
    Jie Wu, Ming Cheng, and Weishan Zhang. A novel deep learning model for medical
    report generation by inter-intra information calibration. *IEEE Journal of Biomedical
    and Health Informatics*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2023b] Ke Zhang, Hanliang Jiang, Jian Zhang, Qingming Huang, Jianping
    Fan, Jun Yu, and Weidong Han. Semi-supervised medical report generation via graph-guided
    hybrid feature consistency. *IEEE Transactions on Multimedia*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2020] Yixiao Zhang, Xiaosong Wang, Ziyue Xu, Qihang Yu, Alan Yuille,
    and Daguang Xu. When radiology report generation meets knowledge graph. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 34, pages 12910–12917,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2022] Yong Zhang, Weihua Ou, Jiacheng Zhang, and Jiaxin Deng.
    Category supervised cross-modal hashing retrieval for chest x-ray and radiology
    reports. *Computers & Electrical Engineering*, 98:107673, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2016] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. Learning deep features for discriminative localization. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 2921–2929,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2021] Yi Zhou, Lei Huang, Tao Zhou, Huazhu Fu, and Ling Shao. Visual-textual
    attentive semantic consistency for medical report generation. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 3985–3994,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. [2017] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
    Unpaired image-to-image translation using cycle-consistent adversarial networks.
    In *Proceedings of the IEEE international conference on computer vision*, pages
    2223–2232, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table A1: Summary of papers in the survey. The overview is based on the findings
    of the survey. The dataset and metrics section focuses on mainstream datasets
    and metrics. ’–’ indicates that the paper either does not detail this process
    or does not include the key techniques summarized in this survey. The following
    abbreviations are used: I-Archi: the architecture of image feature learning, I-Module:
    the enhancement module of image feature learning, NI: the feature learning of
    non-image data, GEI: gastrointestinal endoscope image, RetiI: retinal image, Term:
    terminology, KnowB: knowledge base, RealR: real report, ClinicalI: clinical information,
    Ques: questionnaires, FreFilter: frequency-based filtering, ConAtoL: converting
    all tokens to lowercase, RemoNAT: removing non-alphabetic tokens, AT: auxiliary
    task, ContrasL: Contrastive learning, MM: memory metric, FeatO: feature-level
    operation, OptimS: optimization strategies, H-LSTM: hierarchical LSTM, ReLoss:
    re-weighted loss function, ReinL: reinforcement learning, R-L: Rouge-L, C-D: CIDEr-D,
    CE: clinical efficacy, Com: comparison, Clas: classification, Escore: error scoring,
    ECI: extracting case-related information, IU: IU X-Ray, MIMIC: MIMIC-CXR, ImaGeno:
    Chest ImaGenome, COV: COV-CTR, DeepEye: DeepEyeNet, CCCT: Chinese COVID-19 CT,
    Ret-I: Retina ImBank, and Ret-C: Retina Chinese. In addition, AT-Graph, AT-Class,
    AT-EC, and AT-DS mean the graph-based, classification, embedding comparison, and
    detection/segmentation auxiliary tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | Input data | Data preparation | Feature Learning | Feature Fusion
    | Generation | Training Strategy | Datasets | Metrics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| I-Archi | I-Module | NI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021b](#bib.bib69)] | Chest X-ray, Term,'
  prefs: []
  type: TYPE_NORMAL
- en: RealR | Tokenizing, ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | AT-Graph | Transformer | FeatO | Transformer | – | IU,
    MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2021d](#bib.bib71)] | Chest X-ray | – | ResNet | AT-Graph |
    – | – | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2021a](#bib.bib68)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | CNNs | – | – | – | LSTMs | Curricu- lum lea-
  prefs: []
  type: TYPE_NORMAL
- en: rning | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Chen et al. [[2021](#bib.bib15)] | Chest X-ray | – | ResNet+ Transformer
    | – | – | MM | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| You et al. [[2021](#bib.bib146)] | Chest X-ray | – | ResNet | – | – | Attention
    | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Miura et al. [[2021](#bib.bib77)] | Chest X-ray | – | DenseNet+ Transformer
    | – | – | – | Transformer | ReinL | MIMIC | BLEU, C-D,'
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Alfarghaly et al. [[2021](#bib.bib3)] | Chest X-ray | Resizing | DenseNet
    | AT-Class | – | Attention | Transformer | – | IU | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: Clas |
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2021b](#bib.bib145)] | Chest X-ray | – | ResNet | AT-EC | –
    | – | H-LSTM | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Pahwa et al. [[2021](#bib.bib84)] | Chest X-ray | Resizing, Cropping,'
  prefs: []
  type: TYPE_NORMAL
- en: Flipping | HRNet+ Transformer | – | – | – | R2Gen | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhou et al. [[2021](#bib.bib153)] | Chest X-ray, ClinicalI | Tokenizing,
    FreFilter | DenseNet | AT-Class, AT-EC | One-hot, BioSentVec | Attention | H-LSTM
    | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: nKTD |
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2021b](#bib.bib35)] | RetiI, Term | – | CNNs | – | Embedding
    layer | FeatO | LSTM | – | DeepEye | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Han et al. [[2021](#bib.bib26)] | Spine MRI | – | Self-design | AT-DS | –
    | – | Reasoning | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021e](#bib.bib72)] | Chest X-ray, Chest CT,'
  prefs: []
  type: TYPE_NORMAL
- en: Term | – | DenseNet | AT-Class | BERT | FeatO, Attention | Transformer | AT
    | CCCT | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2021](#bib.bib128)] | Chest X-ray, Chest CT | Tokenizing |
    ResNet+ Transformer | AT-EC | – | – | H-LSTM | AT | IU, COV | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Endo et al. [[2021](#bib.bib23)] | Chest X-ray | – | ResNet | – | – | – |
    Retrieval | – | MIMIC | BLEU, $S_{emb}$,'
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Najdenkoska et al. [[2021](#bib.bib80)] | Chest X-ray | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet+ Transformer | AT-EC | – | – | LSTM | – | IU, MIMIC | BLEU,
    R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2021a](#bib.bib142)] | Breast ultrasound | Tokenizing, FreFilter
    | ResNet | AT-Class | – | FeatO | LSTM | – | BCD2018 | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Pandey et al. [[2021](#bib.bib85)] | Chest X-ray | Resizing | VGG | – | –
    | – | H-LSTM | AT | IU | BLEU, R-L |'
  prefs: []
  type: TYPE_TB
- en: '| Hou et al. [[2021b](#bib.bib32)] | Chest X-ray | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | AT-Class | – | LSTM | H-LSTM | ReinL | IU, MIMIC | BLEU,
    R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Pino et al. [[2021](#bib.bib88)] | Chest X-ray | – | DenseNet | – | – | –
    | Template | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: MIRQI |
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2021c](#bib.bib70)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | ContrasL | – | – | H-LSTM | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Jia et al. [[2021](#bib.bib41)] | Chest X-ray | – | ResNet+ Transformer |
    – | – | – | Transformer | – | IU, MIMIC | BLEU, R-L |'
  prefs: []
  type: TYPE_TB
- en: '| Hou et al. [[2021a](#bib.bib31)] | Chest X-ray | Resizing, Data'
  prefs: []
  type: TYPE_NORMAL
- en: augmentation | DenseNet | – | – | – | Transformer | – | MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2021a](#bib.bib34)] | RetiI, Term | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | CNNs | – | Embedding layer | LSTM | LSTM | – | DeepEye | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Babar et al. [[2021a](#bib.bib4)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | – | – | – | – | Uncondition | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Singh et al. [[2021](#bib.bib100)] | Chest X-ray | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | InceptionV3 | AT-Class | – | – | LSTM | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2022](#bib.bib143)] | Chest X-ray, KnowB,'
  prefs: []
  type: TYPE_NORMAL
- en: RealR | Resizing, Tokenizing,
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | – | RotatE, ECI,
  prefs: []
  type: TYPE_NORMAL
- en: BERT | Attention | Transformer | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Ramesh et al. [[2022](#bib.bib93)] | Chest X-ray | Tokenizing, Filtering
    | ResNet/ Transformer | – | – | – | Retrieval | – | MIMIC | $S_{emb}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Sirshar et al. [[2022](#bib.bib102)] | Chest X-ray | Tokenizing, RemoNAT,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL | VGG | – | – | – | LSTM | – | IU, MIMIC | BLEU |
  prefs: []
  type: TYPE_NORMAL
- en: '| Najdenkoska et al. [[2022](#bib.bib81)] | Chest X-ray | Resizing, RemoNAT,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet+ Transformer | AT-EC | – | – | Transformer, LSTM | – |
    IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: '%Novel,'
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022c](#bib.bib125)] | Chest X-ray | Cropping, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet | AT-Graph | – | – | H-LSTM | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2022b](#bib.bib59)] | RetiI | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | I3D+ Transformer | AT-Graph | – | – | Transformer | – | FFA-IR |
    BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Qin and Song [[2022](#bib.bib90)] | Chest X-ray | – | ResNet+ Transformer
    | – | – | MM | Transformer | ReinL | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022a](#bib.bib120)] | Chest X-ray | Resizing, Cropping | ResNet+
    Transformer | ContrasL | – | MM | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Cao et al. [[2022](#bib.bib9)] | Chest X-ray, GEI,'
  prefs: []
  type: TYPE_NORMAL
- en: Term | – | DenseNet+ Transformer | AT-Graph | BERT | FeatO, Attention | Transformer
    | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Mohsan et al. [[2022](#bib.bib78)] | Chest X-ray | Tokenizing, ConAtoL |
    Transformer | – | – | – | Transformer | – | IU | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Yan [[2022](#bib.bib140)] | Chest X-ray | – | DenseNet+ Transformer | AT-Graph,
    MM | – | – | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: MIRQI |
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2022a](#bib.bib56)] | Chest X-ray | – | ResNet+ Transformer |
    AT-Class | – | – | Transformer | AT | IU | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Sun et al. [[2022](#bib.bib106)] | RetiI | Resizing | ResNet+ Faster-RCNN
    | AT-EC, AT-DS | – | – | Transformer | – | FFA-IR | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| You et al. [[2022](#bib.bib147)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | ResNet+ Transformer | AT-Class | – | Attention, MM | Transformer |
    – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022d](#bib.bib129)] | Chest X-ray | Resizing | Transformer
    | AT-Class, AT-EC | – | – | Transformer | ReLoss | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Lee et al. [[2022](#bib.bib53)] | Chest X-ray | Resizing, Cropping,'
  prefs: []
  type: TYPE_NORMAL
- en: Flipping | ResNet+ Transformer | – | – | – | R2Gen | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Moon et al. [[2022](#bib.bib79)] | Chest X-ray | Resizing, Cropping,'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing | ResNet+ Transformer | AT | – | – | Transformer | – | IU, MIMIC
    | BLEU, CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2022](#bib.bib36)] | RetiI, Term | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | CNNs | – | Embedding layer | Attention | LSTM | – | DeepEye | BLEU,
    R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Kaur and Mittal [[2022a](#bib.bib46)] | Chest X-ray | Resizing, Convert image
    to'
  prefs: []
  type: TYPE_NORMAL
- en: grayscale,
  prefs: []
  type: TYPE_NORMAL
- en: Flipping,
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing,
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | VGG | – | – | – | H-LSTM | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Song et al. [[2022](#bib.bib104)] | Chest X-ray, RealR | – | DenseNet | ContrasL
    | – | Attention | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Jia et al. [[2022](#bib.bib42)] | Chest X-ray | – | DenseNet | – | – | Attention
    | H-LSTM | – | IU, MIMIC | BLEU, R-L |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2022](#bib.bib151)] | Chest X-ray | Resizing | VGG | – | –
    | – | Retrieval | – | MIMIC | Precision |'
  prefs: []
  type: TYPE_TB
- en: '| Dalla Serra et al. [[2022](#bib.bib18)] | Chest X-ray, ClinicalI | Tokenizing,
    Resizing,'
  prefs: []
  type: TYPE_NORMAL
- en: Flipping,
  prefs: []
  type: TYPE_NORMAL
- en: Rotation,
  prefs: []
  type: TYPE_NORMAL
- en: Cropping | ResNet+ Transformer | AT-Graph | Embedding layer | Attention | Transformer
    | – | MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Yan et al. [[2022](#bib.bib139)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | AT | – | Attention | Transformer | – | IU,
    MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Gajbhiye et al. [[2022](#bib.bib24)] | Chest X-ray | Tokenizing, Removing
    irrelevant elements, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet | AT-Class | – | – | LSTM | ReLoss | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022b](#bib.bib122)] | Chest X-ray | Grouping | ResNet+ Transformer
    | AT-Class | – | – | R2Gen | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Abela et al. [[2022](#bib.bib1)] | Chest X-ray | – | DenseNet | – | – | –
    | Template | – | MIMIC | BLEU, CE |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022](#bib.bib133)] | Chest X-ray | – | ResNet | ContrasL | –
    | – | LSTM | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Tanwani et al. [[2022](#bib.bib113)] | Chest X-ray, Ques | Resizing, Image
    transformations,'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing | ResNeXt | AT-Class, ContrasL | BERT | Attention | Transformer |
    – | IU | BLEU |
  prefs: []
  type: TYPE_NORMAL
- en: '| Chen et al. [[2022](#bib.bib13)] | Chest X-ray | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | ResNet+ Transformer | AT-EC MM | – | – | R2Gen | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Kong et al. [[2022](#bib.bib50)] | Chest X-ray | Resizing | Transformer |
    – | – | – | Retrieval | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2022e](#bib.bib130)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | Transformer | AT-Class, ContrasL | – | FeatO | Transformer | – |
    MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Du et al. [[2022](#bib.bib22)] | Chest X-ray | – | ResNet | AT-Class | –
    | – | H-LSTM | – | IU | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Kaur and Mittal [[2022b](#bib.bib47)] | Chest X-ray | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | VGG | AT-Class | – | FeatO | H-LSTM | ReinL | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Tanida et al. [[2023](#bib.bib112)] | Chest X-ray | Resizing, Data'
  prefs: []
  type: TYPE_NORMAL
- en: augmentation,
  prefs: []
  type: TYPE_NORMAL
- en: Removing redundant whitespaces | ResNet+ Faster-RCNN | AT-DS | – | – | Transformer
    | – | MIMIC, ImaGeno | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2023d](#bib.bib131)] | Chest X-ray | Tokenizing | Transformer
    | AT | – | – | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023b](#bib.bib61)] | Chest X-ray, Chest CT | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | DenseNet | AT-Graph | – | Attention | Transformer | – | IU, COV
    | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wu et al. [[2023](#bib.bib134)] | Chest X-ray | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: Removing irrelevant elements,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | – | – | – | Transformer | ReinL, AT | IU,
    MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023a](#bib.bib60)] | Chest X-ray, KnowB,'
  prefs: []
  type: TYPE_NORMAL
- en: RealR | Tokenizing | Transformer | AT-Class, ContrasL | ECI, BERT | Attention
    | Transformer | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Huang et al. [[2023](#bib.bib37)] | Chest X-ray, KnowB | Tokenizing, ConAtoL,'
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | – | ECI, BERT | FeatO, Attention | Transformer
    | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhang et al. [[2023b](#bib.bib149)] | Chest X-ray | – | DenseNet+ Transformer
    | AT-Graph, AT-Class,'
  prefs: []
  type: TYPE_NORMAL
- en: AT-EC | – | – | Transformer | AT | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Xu et al. [[2023](#bib.bib137)] | Chest X-ray, Dermoscopy,'
  prefs: []
  type: TYPE_NORMAL
- en: KnowB | – | DenseNet+ Transformer | AT-Class | ECI, BERT | Attention | R2Gen
    | – | IU | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: Com |
  prefs: []
  type: TYPE_NORMAL
- en: '| Yang et al. [[2023](#bib.bib144)] | Chest X-ray, KnowB | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet | AT-Class, AT-EC | – | Attention | Transformer | – | IU,
    MIMIC | BLEU, C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Zhang et al. [[2023a](#bib.bib148)] | Chest X-ray, Chest CT | Resizing |
    ResNet+ Transformer | – | – | – | Transformer+ MM | – | IU, MIMIC,'
  prefs: []
  type: TYPE_NORMAL
- en: COV | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2023a](#bib.bib123)] | Chest X-ray | – | ResNet+ Transformer
    | AT-Class, AT-DS | – | – | R2Gen +LLM | – | MIMIC | CE |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[2023](#bib.bib66)] | Chest X-ray, RetiI | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter | ResNet+ Transformer | ContrasL | – | – | Transformer | – | IU, MIMIC,
  prefs: []
  type: TYPE_NORMAL
- en: Ret-I,
  prefs: []
  type: TYPE_NORMAL
- en: Ret-C | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Selivanov et al. [[2023](#bib.bib96)] | Chest X-ray | Resizing, Tokenizing,'
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | DenseNet | – | – | – | Transformer | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Cao et al. [[2023](#bib.bib10)] | Chest X-ray, GEI,'
  prefs: []
  type: TYPE_NORMAL
- en: Term | – | DenseNet+ Transformer | – | BERT | FeatO, Attention,
  prefs: []
  type: TYPE_NORMAL
- en: MM | Transformer | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Shetty et al. [[2023](#bib.bib98)] | Chest X-ray | – | Self-design | – |
    – | – | LSTM | – | IU | BLEU |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023c](#bib.bib127)] | Chest X-ray | - | ResNet+ Faster-RCNN+'
  prefs: []
  type: TYPE_NORMAL
- en: Transformer | – | – | – | R2Gen | – | IU, MIMIC,
  prefs: []
  type: TYPE_NORMAL
- en: ImaGeno | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023d](#bib.bib63)] | Chest X-ray | Resizing, Cropping,'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter,
  prefs: []
  type: TYPE_NORMAL
- en: ConAtoL,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | Transformer | – | – | MM +Self-design | Transformer | AT | IU, MIMIC
    | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Liu et al. [[2023b](#bib.bib74)] | Chest X-ray, Term,'
  prefs: []
  type: TYPE_NORMAL
- en: RealR | FreFilter, ConAtol | DenseNet | ContrasL | Transformer | FeatO, Attention
    | Transformer | – | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2023b](#bib.bib124)] | Chest X-ray | – | ResNet+ Transformer
    | ContrasL | – | – | R2Gen | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Pellegrini et al. [[2023](#bib.bib87)] | Chest X-ray, Ques | – | EfficientNet
    | – | BERT | Attention | Classification | - | Rad-ReStruct | CE |'
  prefs: []
  type: TYPE_TB
- en: '| Dalla Serra et al. [[2023a](#bib.bib19)] | Chest X-ray, ClinicalI | Resizing,
    Cropping,'
  prefs: []
  type: TYPE_NORMAL
- en: Grouping | ResNet+ Faster-RCNN | AT-DS | Embedding layer | Attention | Transformer
    | – | MIMIC, ImaGeno | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Li et al. [[2023c](#bib.bib62)] | Chest X-ray, RealR,'
  prefs: []
  type: TYPE_NORMAL
- en: Term | Resizing, Self-design | ResNet+ Transformer | - | ECI, BERT | Attention
    | Transformer | - | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D |
  prefs: []
  type: TYPE_NORMAL
- en: '| Dalla Serra et al. [[2023b](#bib.bib20)] | Chest X-ray, ClinicalI | Resizing,
    Cropping | ResNet+ Faster-RCNN | AT-DS, AT-Graph | Embedding layer | Attention
    | Transformer | - | MIMIC, ImaGeno | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Jeong et al. [[2024](#bib.bib40)] | Chest X-ray | Resizing | Transformer
    | – | – | – | Retrieval | – | MIMIC | BLEU, Escore |'
  prefs: []
  type: TYPE_TB
- en: '| Gu et al. [[2024](#bib.bib25)] | Chest X-ray, Term | Resizing, Cropping,'
  prefs: []
  type: TYPE_NORMAL
- en: Flipping,
  prefs: []
  type: TYPE_NORMAL
- en: FreFilter,
  prefs: []
  type: TYPE_NORMAL
- en: RemoNAT | ResNet | AT-DS, AT | Transformer | FeatO, Attention | Transformer
    | ReinL | IU, MIMIC | BLEU, R-L,
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2024b](#bib.bib126)] | Chest X-ray, Chest CT | – | ResNet+
    Transformer | AT | – | – | Transformer | ReLoss | IU, COV | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: Grading |
  prefs: []
  type: TYPE_NORMAL
- en: '| Xue et al. [[2024](#bib.bib138)] | Chest X-ray, Term | - | ResNet+ Transformer
    | - | Transformer, Attention | FeatO, Attention | Transformer | - | IU, MIMIC
    | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR |
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[2024a](#bib.bib121)] | Chest X-ray | Resizing, Cropping | DenseNet+
    Transformer | AT-Class | – | – | R2Gen | AT | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: C-D,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: '| Jin et al. [[2024](#bib.bib43)] | Chest X-ray, RealR | – | ResNet | AT-Class
    | Transformer | FeatO, Attention | Transformer | – | IU, MIMIC | BLEU, R-L,'
  prefs: []
  type: TYPE_NORMAL
- en: METEOR,
  prefs: []
  type: TYPE_NORMAL
- en: CE |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table A2: Comparisons of the model performance on the MIMIC-CXR Dataset. B1,
    B2, B3, B4, R-L, C-D, P, R, and F represent BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L,
    CIDEr-D, precision, recall, and F1 score, respectively. The best and second best
    results are highlighted. All values were extracted from their papers.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Paper | B1$\uparrow$ | B2$\uparrow$ | B3$\uparrow$ | B4$\uparrow$ | R-L$\uparrow$
    | METEOR$\uparrow$ | C-D$\uparrow$ | P$\uparrow$ | R$\uparrow$ | F$\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| Findings Section |'
  prefs: []
  type: TYPE_TB
- en: '| Chen et al. [[2021](#bib.bib15)] | 0.353 | 0.218 | 0.148 | 0.106 | 0.278
    | 0.142 | - | 0.334 | 0.275 | 0.278 |'
  prefs: []
  type: TYPE_TB
- en: '| Pino et al. [[2021](#bib.bib88)] | - | - | - | - | 0.185 | - | 0.238 | 0.381
    | 0.531 | 0.428 |'
  prefs: []
  type: TYPE_TB
- en: '| Song et al. [[2022](#bib.bib104)] | 0.360 | 0.227 | 0.156 | 0.117 | 0.287
    | 0.148 | - | 0.444 | 0.297 | 0.356 |'
  prefs: []
  type: TYPE_TB
- en: '| Impression + Findings Section |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2022](#bib.bib133)] | 0.340 | 0.212 | 0.145 | 0.103 | 0.270 |
    0.139 | 0.109 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022d](#bib.bib129)] | 0.351 | 0.223 | 0.157 | 0.118 | 0.287
    | - | 0.281 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Jia et al. [[2022](#bib.bib42)] | 0.363 | 0.228 | 0.156 | 0.130 | 0.300 |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Unspecified generated sections |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021a](#bib.bib68)] | 0.344 | 0.217 | 0.140 | 0.097 | 0.218
    | 0.133 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021c](#bib.bib70)] | 0.350 | 0.219 | 0.152 | 0.109 | 0.283
    | 0.151 | - | 0.352 | 0.298 | 0.303 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021b](#bib.bib69)] | 0.360 | 0.224 | 0.149 | 0.106 | 0.284
    | 0.149 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[2023c](#bib.bib62)] | 0.360 | 0.231 | 0.162 | 0.119 | 0.298 |
    0.153 | 0.217 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[2023](#bib.bib66)] | 0.362 | 0.227 | 0.155 | 0.113 | 0.283 |
    0.142 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2023b](#bib.bib149)] | 0.362 | 0.229 | 0.157 | 0.113 | 0.284
    | 0.153 | - | 0.380 | 0.342 | 0.335 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[2022](#bib.bib143)] | 0.363 | 0.228 | 0.156 | 0.115 | 0.284
    | - | 0.203 | 0.458 | 0.348 | 0.371 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023c](#bib.bib127)] | 0.363 | 0.235 | 0.164 | 0.118 | 0.301
    | 0.136 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2021d](#bib.bib71)] | 0.369 | 0.231 | 0.156 | 0.118 | 0.295
    | 0.153 | - | 0.389 | 0.362 | 0.355 |'
  prefs: []
  type: TYPE_TB
- en: '| Xue et al. [[2024](#bib.bib138)] | 0.372 | 0.233 | 0.154 | 0.112 | 0.286
    | 0.152 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2024a](#bib.bib121)] | 0.374 | 0.230 | 0.155 | 0.112 | 0.279
    | 0.145 | 0.161 | 0.483 | 0.323 | 0.387 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[2023a](#bib.bib148)] | 0.376 | 0.233 | 0.157 | 0.113 | 0.276
    | 0.144 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| You et al. [[2021](#bib.bib146)] | 0.378 | 0.235 | 0.156 | 0.112 | 0.283
    | 0.158 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Qin and Song [[2022](#bib.bib90)] | 0.381 | 0.232 | 0.155 | 0.109 | 0.287
    | 0.151 | - | 0.342 | 0.294 | 0.292 |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al. [[2023](#bib.bib134)] | 0.383 | 0.224 | 0.146 | 0.104 | 0.280 |
    0.147 | - | - | - | 0.758 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al. [[2023](#bib.bib144)] | 0.386 | 0.237 | 0.157 | 0.111 | 0.274
    | - | 0.111 | 0.420 | 0.339 | 0.352 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023b](#bib.bib124)] | - | - | - | 0.119 | 0.286 | 0.158 |
    0.259 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2023d](#bib.bib131)] | 0.386 | 0.250 | 0.169 | 0.124 | 0.291
    | 0.152 | 0.362 | 0.364 | 0.309 | 0.311 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu et al. [[2023b](#bib.bib74)] | 0.391 | 0.249 | 0.172 | 0.125 | 0.304
    | 0.160 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Huang et al. [[2023](#bib.bib37)] | 0.393 | 0.243 | 0.159 | 0.113 | 0.285
    | 0.160 | - | 0.371 | 0.318 | 0.321 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022b](#bib.bib122)] | 0.395 | 0.253 | 0.170 | 0.121 | 0.284
    | 0.147 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Jin et al. [[2024](#bib.bib43)] | 0.398 | - | - | 0.112 | 0.268 | 0.157 |
    - | 0.501 | 0.509 | 0.476 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[2022e](#bib.bib130)] | 0.413 | 0.266 | 0.186 | 0.136 | 0.298
    | 0.170 | 0.429 | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Kong et al. [[2022](#bib.bib50)] | 0.423 | 0.261 | 0.171 | 0.116 | 0.286
    | 0.168 | - | 0.482 | 0.563 | 0.519 |'
  prefs: []
  type: TYPE_TB
