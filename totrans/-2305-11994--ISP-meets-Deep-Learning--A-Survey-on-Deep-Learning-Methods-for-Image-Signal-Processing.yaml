- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:39:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.11994] ISP meets Deep Learning: A Survey on Deep Learning Methods for
    Image Signal Processing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.11994] ISP 遇见深度学习：关于图像信号处理的深度学习方法的综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11994](https://ar5iv.labs.arxiv.org/html/2305.11994)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11994](https://ar5iv.labs.arxiv.org/html/2305.11994)
- en: 'ISP meets Deep Learning: A Survey on Deep Learning Methods for Image Signal
    Processing'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ISP 遇见深度学习：关于图像信号处理的深度学习方法的综述
- en: Matheus Henrique Marques da Silva, Jhessica Victoria Santos da Silva, Rodrigo
    Reis Arrais
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Matheus Henrique Marques da Silva, Jhessica Victoria Santos da Silva, Rodrigo
    Reis Arrais
- en: Eldorado Research Institute
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Eldorado 研究所
- en: Campinas, São Paulo - Brazil
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 坎皮纳斯，圣保罗 - 巴西
- en: '{matheus.marques, jhessica.silva, rodrigo.arrais}@eldorado.org.br &Wladimir
    Barroso Guedes de Araújo Neto, Leonardo Tadeu Lopes, Guilherme Augusto Bileki'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{matheus.marques, jhessica.silva, rodrigo.arrais}@eldorado.org.br &Wladimir
    Barroso Guedes de Araújo Neto, Leonardo Tadeu Lopes, Guilherme Augusto Bileki'
- en: Eldorado Research Institute
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Eldorado 研究所
- en: Campinas, São Paulo - Brazil
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 坎皮纳斯，圣保罗 - 巴西
- en: '{wladimir.neto, leonardo.lopes, bilekig,}@eldorado.org.br &Iago Oliveira Lima,
    Lucas Borges Rondon, Bruno Melo de Souza'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{wladimir.neto, leonardo.lopes, bilekig,}@eldorado.org.br &Iago Oliveira Lima,
    Lucas Borges Rondon, Bruno Melo de Souza'
- en: Eldorado Research Institute
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Eldorado 研究所
- en: Campinas, São Paulo - Brazil
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 坎皮纳斯，圣保罗 - 巴西
- en: '{iagolima, lucas.rondon, brunobms}@eldorado.org.br &Mayara Costa Regazio, Rodolfo
    Coelho Dalapicola, Claudio Filipi Gonçalves dos Santos'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '{iagolima, lucas.rondon, brunobms}@eldorado.org.br &Mayara Costa Regazio, Rodolfo
    Coelho Dalapicola, Claudio Filipi Gonçalves dos Santos'
- en: Eldorado Research Institute
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Eldorado 研究所
- en: Campinas, São Paulo - Brazil
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 坎皮纳斯，圣保罗 - 巴西
- en: '{mayara.regazio, rodolfo.dalapicola, claudio.santos}@eldorado.org.br'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '{mayara.regazio, rodolfo.dalapicola, claudio.santos}@eldorado.org.br'
- en: Abstract
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The entire Image Signal Processor (ISP) of a camera relies on several processes
    to transform the data from the Color Filter Array (CFA) sensor, such as demosaicing,
    denoising, and enhancement. These processes can be executed either by some hardware
    or via software. In recent years, Deep Learning has emerged as one solution for
    some of them or even to replace the entire ISP using a single neural network for
    the task. In this work, we investigated several recent pieces of research in this
    area and provide deeper analysis and comparison among them, including results
    and possible points of improvement for future researchers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 相机的整个图像信号处理器（ISP）依赖于多个过程来将来自颜色滤镜阵列（CFA）传感器的数据转换，例如去马赛克、去噪声和增强。这些过程可以通过一些硬件或软件来执行。近年来，深度学习作为解决这些问题或甚至用一个神经网络替代整个
    ISP 的方案出现。在这项工作中，我们调查了该领域的一些最新研究，并提供了更深入的分析和比较，包括结果和未来研究者可能的改进点。
- en: '*K*eywords image signal processing, deep learning, convolutional neural networks'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 图像信号处理，深度学习，卷积神经网络'
- en: 1 Introduction
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The Image Signal Processor (ISP) is a component of digital cameras capable of
    performing various tasks to improve image quality, as demosaicing, denoising,
    and white balance. The set of tasks performed by the ISP is called ISP pipeline,
    divided in preproccessing and postprocessing steps, and may differ from manufacturer
    to manufacturer [[1](#bib.bib1)]. Nowadays, Machine Learning is used to replace
    partially or the entire ISP pipeline. Particulary, Deep Learning is employed to
    replace ISP tasks, working on noise removal or some image feaure that hinders
    processing over the network. Deep Learning network provides an improvement in
    relation to computational efficiency and processing time. This survey paper aims
    to analyze recent studies, 27 research papers, that implemented Deep Learning
    based ISP pipeline.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图像信号处理器（ISP）是数字相机的一个组件，能够执行多种任务以改善图像质量，如去马赛克、去噪声和白平衡。ISP 执行的一组任务称为 ISP 管道，分为预处理和后处理步骤，并可能因制造商而异[[1](#bib.bib1)]。如今，机器学习被用来部分或完全替代整个
    ISP 管道。特别是，深度学习被用来替代 ISP 任务，处理噪声去除或影响网络处理的一些图像特征。深度学习网络在计算效率和处理时间方面提供了改进。本文旨在分析最近的研究，27
    篇研究论文，涉及基于深度学习的 ISP 管道。
- en: 1.1 Image Signal Processing
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 图像信号处理
- en: Traditionally, ISPs are digital signal processors that reconstruct RGB images
    from RAW images. In traditional camera pipelines, complex and proprietary hardware
    processes are used to perform image signal processing  [[2](#bib.bib2)]. It consists
    of several processing steps, including noise reduction, white balance, demosaicing,
    and more. Each step with loss functions in the ISP is usually performed sequentially,
    the residual error accumulates over the runtime  [[3](#bib.bib3)]. Parameter adjustments
    in later stages correct the accumulated errors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，ISP（图像信号处理器）是从RAW图像重建RGB图像的数字信号处理器。在传统的相机处理流程中，使用复杂且专有的硬件过程来进行图像信号处理 [[2](#bib.bib2)]。它包括多个处理步骤，如噪声减少、白平衡、去马赛克处理等。每一步在ISP中的损失函数通常是按顺序执行的，残差误差会在运行过程中累积
    [[3](#bib.bib3)]。后续阶段的参数调整用来纠正累积的错误。
- en: Most of the traditional methods use heuristic approaches to derive the solution
    at each step of the ISP pipeline  [[2](#bib.bib2)], so numerous parameters need
    to be adjusted. Moreover, multiple ISP processes executed sequentially with module-based
    algorithms lead to cumulative errors at each execution step. To minimize those
    errors, new techniques were researched and, among them, algorithms related to
    Deep Learning started to get more focus.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数传统方法使用启发式方法来推导ISP流程中每一步的解决方案 [[2](#bib.bib2)]，因此需要调整众多参数。此外，多个ISP过程按顺序执行且基于模块的算法会在每一步执行中导致累计错误。为了最小化这些错误，研究了新的技术，其中与深度学习相关的算法开始受到更多关注。
- en: 1.2 Deep Learning
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 深度学习
- en: Even though the research of Machine Learning dates back to the decade of 1950[[4](#bib.bib4)],
    it was only in the last decade that the advancements in technology have allowed
    its more complex fields to be extensively explored. The rapid evolution of computational
    power, coupled with the growing amount of data being produced daily, caused a
    subtle renewal of interest in the usage of Machine Learning techniques. For that
    reason, several areas such as Chemistry [[5](#bib.bib5)], Medicine [[6](#bib.bib6)],
    Economics [[7](#bib.bib7)] and Physics [[8](#bib.bib8)], have been able to harness
    its capacities to accelerate or improve their work, directly impacted by this
    evolution in the field known as Deep Learning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习的研究可以追溯到1950年代 [[4](#bib.bib4)]，但只有在过去十年中，技术的进步才允许其更复杂的领域得到广泛探索。计算能力的快速发展，加上每日生产的数据量不断增加，导致了对机器学习技术使用的兴趣的微妙复兴。因此，化学
    [[5](#bib.bib5)]、医学 [[6](#bib.bib6)]、经济学 [[7](#bib.bib7)] 和物理学 [[8](#bib.bib8)]
    等多个领域能够利用其能力加速或改善其工作，直接受到了深度学习领域这一演变的影响。
- en: Deep Learning, as a subset of Machine Learning, is comprised of algorithms based
    on Artificial Neural Networks that use several layers of neurons to extract higher
    level features from the raw data being provided to it [[9](#bib.bib9), [10](#bib.bib10),
    [11](#bib.bib11)]. This class of algorithms requires a huge amount of computational
    power that become available only in recent years. In parallel to the high demand
    of computational power, the capacity of learning of Deep Learning algorithms also
    rises with the amount of data being provided to the system. For this reason, areas
    that have a great influx of data in their operation saw in Deep Learning an interesting
    way to find and understand hidden information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，作为机器学习的一个子集，由基于人工神经网络的算法组成，这些算法使用多个神经层从提供的原始数据中提取更高层次的特征 [[9](#bib.bib9),
    [10](#bib.bib10), [11](#bib.bib11)]。这一类算法需要巨大的计算能力，而这种计算能力仅在近年来才得以实现。与对计算能力的高需求并行，深度学习算法的学习能力也随着系统提供的数据量的增加而提升。因此，那些在操作中数据流入量巨大的领域发现，深度学习是一种有趣的方式来发现和理解隐藏的信息。
- en: One of the fields that found great results with the use of Deep Learning is
    the Image Processing (a sub set of Computer Vision), more specifically with the
    use of Convolutional Neural Networks (CNN). The CNNs are a class of Neural Networks
    more prone to work with visual imagery for being inspired by biological processes,
    created in a manner that the connective pattern of the neurons imitates the pattern
    of the visual cortex of animals [[12](#bib.bib12), [13](#bib.bib13)]. Another
    important aspect of the CNNs is the fact that, following a different path from
    other networks, they use very minimal pre-processing, being able to learn by themselves
    to optimize kernels. These features make the use of CNNs more common for Image
    Processing tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习使用方面，取得了显著成果的一个领域是图像处理（计算机视觉的一个子集），更具体地说，是使用卷积神经网络（CNN）。CNN是一类更倾向于处理视觉图像的神经网络，因为它们受到生物过程的启发，设计方式模仿了动物视觉皮层的神经连接模式[[12](#bib.bib12),
    [13](#bib.bib13)]。CNN的另一个重要方面是，与其他网络不同的是，它们使用非常少的预处理，能够通过自身学习来优化卷积核。这些特性使得CNN在图像处理任务中的使用更加普遍。
- en: 1.3 The ISP and Deep Learning Relation
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 ISP与深度学习的关系
- en: The intention of using a CNN to replace the hardware-based ISP is justified
    by the fact that a CNN can compensate the loss of information in the input images
    making it more reliable than the traditionally implemented ISP, as traditional
    ISP is known to accumulate errors at each step [[1](#bib.bib1)].  [[31](#bib.bib31)]
    was one of the first to propose a CNN in place of a smartphone ISP camera and
    provided a RAW-to-RGB dataset using the PyNet network. These demonstrated the
    potential of CNN for image processing as a replacement for even the most sophisticated
    ISPs.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CNN替代基于硬件的ISP的意图是有依据的，因为CNN可以弥补输入图像中的信息损失，使其比传统实施的ISP更可靠，因为传统ISP在每一步都容易积累错误[[1](#bib.bib1)]。[[31](#bib.bib31)]是最早提出用CNN代替智能手机ISP相机的人之一，并使用PyNet网络提供了一个RAW到RGB的数据集。这些成果展示了CNN作为图像处理替代方案的潜力，即使是替代最复杂的ISP也不在话下。
- en: Network CNNs not only show significant advantages in low-level vision tasks
     [[2](#bib.bib2)], they also show good results in high-level tasks such as object
    detection and segmentation  [[14](#bib.bib14)]. With these advantages, the use
    of CNNs for transforming RAW images into RGB images became possible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 网络CNN不仅在低级视觉任务[[2](#bib.bib2)]中显示出显著优势，在高级任务如目标检测和分割[[14](#bib.bib14)]中也表现良好。凭借这些优势，使用CNN将RAW图像转换为RGB图像变得可能。
- en: Despite the good results, there are few works using CNN as a replacement for
    ISP.  [[3](#bib.bib3)] showed the difficulties in performing the necessary adjustments
    in traditional ISP pipelines and developed a CNN that performs the ISP pipeline.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了良好的结果，但使用CNN替代ISP的研究仍然较少。[[3](#bib.bib3)]展示了在传统ISP流程中进行必要调整的困难，并开发了一个执行ISP流程的CNN。
- en: 1.4 Comparison with Other Works
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 与其他工作的比较
- en: One of the great ways to consolidate a growing field in science is to perform
    surveys about the most current techniques in that field. This way, the access
    for this kind of information is facilitated, making it easier to understand and
    choose which technique to use for one’s individual case. In the field of Deep
    Learning, for example, a great number of surveys is already available in many
    different areas, such as agriculture [[15](#bib.bib15)], cyber security [[16](#bib.bib16)],
    autonomous driving [[17](#bib.bib17)], medical imaging [[18](#bib.bib18)], and
    also on more technical areas, such as on CNN  [[19](#bib.bib19)]. But for more
    recent fields, such as using Deep Learning to replace  ISPs, it might still be
    hard to find this kind of gathered information.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 巩固一个不断发展的科学领域的一个有效方法是对该领域的最新技术进行调查。这样，可以更方便地获取这类信息，使得理解和选择适合自己个案的技术变得更容易。例如，在深度学习领域，已经有大量的调查报告涵盖了许多不同的领域，如农业[[15](#bib.bib15)]、网络安全[[16](#bib.bib16)]、自动驾驶[[17](#bib.bib17)]、医学成像[[18](#bib.bib18)]，以及更技术性的领域，如CNN[[19](#bib.bib19)]。但对于更为新兴的领域，如使用深度学习替代ISP，找到这类汇总信息可能仍然比较困难。
- en: While there are already some surveys about individual steps of an ISP, such
    as demosaicing [[20](#bib.bib20)] and denoising[[21](#bib.bib21)], there is still
    no easy way to find and compare end-to-end ISP deep learning approaches. In this
    paper we summarize many of these approaches, bringing some of the state-of-the-art
    Artificial Neural Networks (ANNs) in this field.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经有一些关于 ISP 单个步骤的调查，如去马赛克 [[20](#bib.bib20)] 和去噪 [[21](#bib.bib21)]，但仍然没有简便的方法来查找和比较端到端
    ISP 深度学习方法。在本文中，我们总结了这些方法中的许多，展示了这一领域的一些最先进的人工神经网络（ANN）。 |
- en: 1.5 Scope of this work
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 本工作的范围
- en: 'For this study, the articles were studied according to three main points:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本研究，文章按照三个主要点进行了研究：
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Novelty: to introduce the most recent and significant works comprising strategies
    for replacing parts or the entire ISP pipeline through deep learning approaches;
    and'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新颖性：引入最新和重要的工作，包括通过深度学习方法替代部分或整个 ISP 流程的策略；以及
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recently developed: all studies considered were published between the years
    2019 and 2021, making this study very up-to-date.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近开发：所有考虑的研究均发表于 2019 年至 2021 年之间，使本研究非常具有时效性。 |
- en: 'Table 1: Summarization of the approaches considered in the survey.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：调查中考虑的方法汇总。 |
- en: '| Short Name | Description |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 短名称 | 描述 |'
- en: '| HerNet [[22](#bib.bib22)] | Combined CNN’s with traditional algorithms to
    reverse the order of the CFA processing pipeline. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| HerNet [[22](#bib.bib22)] | 将 CNN 与传统算法结合，逆转 CFA 处理流程的顺序。 |'
- en: '| CameraNet [[23](#bib.bib23)] | Divides the subtasks with poor correlation,
    creating a network with two stages. |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| CameraNet [[23](#bib.bib23)] | 将相关性差的子任务分开，创建一个具有两个阶段的网络。 |'
- en: '| Deep Camera [[3](#bib.bib3)] | Create a network with four parallel paths
    with convolutional layers and an inverse ISP to synthesize RAW images. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Deep Camera [[3](#bib.bib3)] | 创建一个具有四个并行路径的网络，包含卷积层和反向 ISP，用于合成 RAW 图像。
    |'
- en: '| DRDN [[24](#bib.bib24)] | Color filter array demosaicking based on residual
    learning and densely connected convolutional neural network. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| DRDN [[24](#bib.bib24)] | 基于残差学习和密集连接卷积神经网络的彩色滤光片阵列去马赛克。 |'
- en: '| Deep Demosaicing for Edge Implementation [[25](#bib.bib25)] | Discussed the
    edge implementation of deep learning-based demosaicing algorithms on low-end edge
    devices |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| Deep Demosaicing for Edge Implementation [[25](#bib.bib25)] | 讨论了基于深度学习的去马赛克算法在低端边缘设备上的边缘实现。
    |'
- en: '| BayerUnify and BayerAug [[26](#bib.bib26)] | Create a method to unify different
    Bayer patterns and an effective approach for raw image augmentation. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| BayerUnify 和 BayerAug [[26](#bib.bib26)] | 创建了一种统一不同 Bayer 图案的方法和有效的 RAW
    图像增强方法。 |'
- en: '| VisionISP[[27](#bib.bib27)] | ISP method to increase computer vision applications
    performance. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| VisionISP [[27](#bib.bib27)] | ISP 方法，用于提高计算机视觉应用的性能。 |'
- en: '| RLDD [[28](#bib.bib28)] | Combined CNN’s with traditional algorithms to reverse
    the order of demosaicking and denoising. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| RLDD [[28](#bib.bib28)] | 将 CNN 与传统算法结合，逆转去马赛克和去噪的顺序。 |'
- en: '| DPN [[29](#bib.bib29)] | An efficient deep neural network architecture for
    Quad Bayer CFA demosaicing adopted in submicron sensors. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| DPN [[29](#bib.bib29)] | 一种高效的深度神经网络架构，用于四元 Bayer CFA 去马赛克，适用于亚微米传感器。 |'
- en: '| CycleISP [[30](#bib.bib30)] | Models camera imaging pipeline in forward and
    reverse directions, producing realistic image pairs for denoising. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| CycleISP [[30](#bib.bib30)] | 模拟相机成像流程的正向和反向方向，生成用于去噪的逼真图像对。 |'
- en: '| PyNET [[31](#bib.bib31)] | Uses a novel pyramidal CNN architecture to replace
    the mobile camera ISP. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| PyNET [[31](#bib.bib31)] | 使用一种新型金字塔形 CNN 架构替代移动相机的 ISP。 |'
- en: '| PyNET-CA [[32](#bib.bib32)] | Improves PyNET performance by adding channel
    attention and subpixel reconstruction modules. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| PyNET-CA [[32](#bib.bib32)] | 通过添加通道注意力和亚像素重建模块来提高 PyNET 的性能。 |'
- en: '| SGNet [[33](#bib.bib33)] | Created an adaptative method to treat regions
    with high and low frequencies. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SGNet [[33](#bib.bib33)] | 创建了一种自适应方法来处理高频和低频区域。 |'
- en: '| PatchNet and RestoreNet [[34](#bib.bib34)] | Select the most useful patches
    from an image for the training step using active learning. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| PatchNet 和 RestoreNet [[34](#bib.bib34)] | 使用主动学习从图像中选择最有用的补丁用于训练步骤。 |'
- en: '| AWNet [[35](#bib.bib35)] | Use of wavelet transform and non-local attention
    mechanism in ISP Pipeline. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| AWNet [[35](#bib.bib35)] | 在 ISP 流程中使用小波变换和非局部注意机制。 |'
- en: '| Del-Net [[36](#bib.bib36)] | A multi-scale architecture that learns the entire
    ISP pipeline. Ideal for smartphone deployment. |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Del-Net [[36](#bib.bib36)] | 一种多尺度架构，学习整个 ISP 流程。适合智能手机部署。 |'
- en: '| InvISP [[37](#bib.bib37)] | Reconstruct the RAW data, in addition, to rendering
    sRGB images using an invertible neural network. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| InvISP [[37](#bib.bib37)] | 除了重建 RAW 数据，还使用可逆神经网络渲染 sRGB 图像。 |'
- en: '| ICDC-Net [[38](#bib.bib38)] | An approach with ISP-Net that addresses JPEG
    image compression in network training. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ICDC-Net [[38](#bib.bib38)] | 一种与 ISP-Net 配合的方法，用于解决网络训练中的 JPEG 图像压缩问题。 |'
- en: '| CSANet [[39](#bib.bib39)] | Uses cascaded blocks composed of channel attention
    modules. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| CSANet [[39](#bib.bib39)] | 使用由通道注意模块组成的级联块。 |'
- en: '| LiteISPNet [[40](#bib.bib40)] | Method to align pairs of images captured
    by different cameras during the train. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LiteISPNet [[40](#bib.bib40)] | 方法用于对齐在训练过程中由不同相机拍摄的图像对。 |'
- en: '| TENet [[41](#bib.bib41)] | Reordered the traditional operation sequence to
    denoising + super-resolution -> demosaicing. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| TENet [[41](#bib.bib41)] | 重新排序了传统操作序列，变为去噪 + 超分辨率 -> 去马赛克。 |'
- en: '| ReconfigISP [[42](#bib.bib42)] | Adapted the architecture and parameters
    according to a specific task. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ReconfigISP [[42](#bib.bib42)] | 根据特定任务调整架构和参数。 |'
- en: '| ISP Distillation [[43](#bib.bib43)] | Uses an sRGB image classification model
    and distills the knowledge of an ISP pipeline. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ISP Distillation [[43](#bib.bib43)] | 使用 sRGB 图像分类模型并提炼 ISP 流程的知识。 |'
- en: '| Merging-ISP [[44](#bib.bib44)] | Approach to reconstructing multiple image
    layers LDR in just one image HDR. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Merging-ISP [[44](#bib.bib44)] | 一种将多个图像层 LDR 重建为一个图像 HDR 的方法。 |'
- en: '| GCP-Net [[45](#bib.bib45)] | A joint denoising and demosaicking method for
    real-world burst images, based on a green channel prior neural network. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GCP-Net [[45](#bib.bib45)] | 一种针对实际拍摄的快速图像的联合去噪和去马赛克方法，基于绿色通道先验神经网络。 |'
- en: '| PIPNet [[46](#bib.bib46)] | Uses a deep network to work with joint demosaicing
    and denoising in CFA patterns. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| PIPNet [[46](#bib.bib46)] | 使用深度网络处理 CFA 模式中的联合去马赛克和去噪问题。 |'
- en: '| CURL [[47](#bib.bib47)] | Image enhancement method that can be used in the
    RAW-to-RGB and RGB-to-RAW mapping tasks. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| CURL [[47](#bib.bib47)] | 图像增强方法，可用于 RAW 到 RGB 和 RGB 到 RAW 映射任务。 |'
- en: 'Those considered papers did not seek the same ISP tasks and improvements. Figure [2](#S1.F2
    "Figure 2 ‣ 1.5 Scope of this work ‣ 1 Introduction ‣ ISP meets Deep Learning:
    A Survey on Deep Learning Methods for Image Signal Processing") shows the ISP
    studied functions distribution among the reviewed articles and TableLABEL:tbl:methods
    gives a list of all methods discussed in this work.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '所考虑的论文并未关注相同的 ISP 任务和改进。图[2](#S1.F2 "Figure 2 ‣ 1.5 Scope of this work ‣ 1
    Introduction ‣ ISP meets Deep Learning: A Survey on Deep Learning Methods for
    Image Signal Processing") 显示了评审文章中研究的 ISP 功能分布，表LABEL:tbl:methods 列出了本文讨论的所有方法。
    |'
- en: Around $30\%$ of the papers proposed an entire ISP pipeline framework using
    an end-to-end Deep Learning approach. Others developed deep learning solutions
    for specific stages of an ISP pipeline, such as denoising tasks, joint denoising-demosaicing
    tasks, resolution enhancement tasks, among others. Some of them also proposed
    extra and distinct ISP deep learning techniques, like RAW data augmentation and
    RAW data generation from RGB images by using inversed ISP procedure.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 约 $30\%$ 的论文提出了使用端到端深度学习方法的整个 ISP 流程框架。其他论文则为 ISP 流程的特定阶段开发了深度学习解决方案，例如去噪任务、联合去噪-去马赛克任务、分辨率提升任务等。其中一些还提出了额外和不同的
    ISP 深度学习技术，如 RAW 数据增强和通过反向 ISP 过程从 RGB 图像生成 RAW 数据。 |
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1.5 Scope of this work ‣ 1 Introduction ‣ ISP
    meets Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing")
    shows how we mapped those studied ISP tasks. We labeled them into two groups:
    ISP function, when the proposed solution strikes specifics ISP operations, and
    ISP pipeline, when the proposed solution settles a RAW to RGB or RGB to RAW operation
    and an entire ISP procedure.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S1.F2 "Figure 2 ‣ 1.5 Scope of this work ‣ 1 Introduction ‣ ISP meets
    Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing")
    显示了我们如何映射那些研究的 ISP 任务。我们将它们标记为两个组：ISP 功能，当所提出的解决方案针对特定的 ISP 操作时，以及 ISP 流程，当所提出的解决方案涉及
    RAW 到 RGB 或 RGB 到 RAW 操作以及整个 ISP 过程时。'
- en: '![Refer to caption](img/15e13bba217839ccb7b1a1db36b8dd57.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/15e13bba217839ccb7b1a1db36b8dd57.png)'
- en: 'Figure 1: Reviewed ISP tasks distribution'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 评审的 ISP 任务分布'
- en: '![Refer to caption](img/138f93d34074e84ca4573b5a1c30fdd7.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/138f93d34074e84ca4573b5a1c30fdd7.png)'
- en: 'Figure 2: Mapped ISP tasks'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 映射的 ISP 任务'
- en: 1.6 Work structure
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6 工作结构
- en: 'The rest of this work is structure as follows: Section 2 describes the overall
    concept, traditional pipelines and algorithms for ISP. Section 3 contains the
    resumes for the works collected through this research. Section 4 covers the results
    of the collected works. Section 5 describes in detail the methodology collected
    during this research. Section 6 concludes this work.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分结构如下：第 2 节描述了整体概念、传统的 ISP 流程和算法。第 3 节包含了通过本研究收集的工作的摘要。第 4 节涵盖了收集工作的结果。第
    5 节详细描述了在本研究期间收集的方法论。第 6 节总结了本文内容。
- en: 2 Software ISP
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 软件 ISP
- en: Over the last two decades, since the rise in popularity of embedded devices
    that use digital cameras as a secondary or main feature, the demand for reliable
    digital image capture and processing systems have grown significantly. Nowadays,
    processing speed and image quality are great selling points for most of those
    devices. That being said, studies of image reconstruction systems have never been
    so important.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的二十年里，随着嵌入式设备的普及，这些设备将数字相机作为辅助或主要功能，可靠的数字图像捕捉和处理系统的需求显著增长。如今，处理速度和图像质量是大多数这些设备的主要卖点。也就是说，图像重建系统的研究变得前所未有的重要。
- en: Traditionally, digital cameras are composed by the bond of two subsystems, the
    first one being dedicated to the acquisition of signals measured by a grid of
    photosensitive analog sensors, usually referred to as sensor element [[1](#bib.bib1)].
    Modern sensor elements have high sensitivity to light variation but are unable
    to identify color variation on their own. A possible solution to this problem
    would be to use 3 distinct sensor elements, each with a specific filter to capture
    a certain frequency range of visible light.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，数字相机由两个子系统结合而成，第一个子系统专注于通过一组光敏模拟传感器来获取信号，这些传感器通常称为传感器元件[[1](#bib.bib1)]。现代传感器元件对光的变化具有高灵敏度，但无法独立识别颜色变化。一个可能的解决方案是使用
    3 个不同的传感器元件，每个元件都有一个特定的滤光片，以捕捉可见光的特定频率范围。
- en: 'This strategy would bring many technical issues, related primarily to sensor
    alignment, difference on light incidence, among others, in addition to an increase
    in hardware cost [[1](#bib.bib1)]. Modern sensor elements have a known pattern
    light frequency filter embedded into them, which makes it possible to reconstruct
    a color image with a single sensor element. These filters are known as Color Filter
    Arrays (CFA). The Bayer Color Filter (BCF) is a special type of Red-Green-Blue
    (RGB) CFA pattern that is widely used in modern image sensors. The BCF is build
    on the assumption that the human visual system (HVS) has more sensitivity to colors
    on the green spectrum. Based on that BCF consists of a 2 by 2 grid pattern containing
    two green, one red and one blue sensor [[1](#bib.bib1)] as shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2 Software ISP ‣ ISP meets Deep Learning: A Survey on Deep Learning
    Methods for Image Signal Processing").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略会带来许多技术问题，主要与传感器对准、光照入射差异等相关，此外，还会增加硬件成本[[1](#bib.bib1)]。现代传感器元件内置有已知的光频率滤光片，使得使用单个传感器元件重建彩色图像成为可能。这些滤光片被称为颜色滤光阵列
    (CFA)。Bayer 颜色滤光片 (BCF) 是一种特殊类型的红绿蓝 (RGB) CFA 图案，在现代图像传感器中广泛使用。BCF 基于假设人类视觉系统
    (HVS) 对绿色光谱的颜色更敏感。基于这一点，BCF 由一个 2x2 网格图案组成，包含两个绿色、一个红色和一个蓝色传感器[[1](#bib.bib1)]，如图
    [3](#S2.F3 "图 3 ‣ 2 软件 ISP ‣ ISP 与深度学习相遇：关于图像信号处理的深度学习方法的调查") 所示。
- en: '![Refer to caption](img/e23115c49c2bd4173431806c7c0a088e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e23115c49c2bd4173431806c7c0a088e.png)'
- en: 'Figure 3: (1) Bayer Pattern and (2) extended pattern. Based on [[48](#bib.bib48)]'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3： (1) Bayer 图案和 (2) 扩展图案。基于[[48](#bib.bib48)]
- en: The BCF filter is placed right in front of the sensor element. The signal resulting
    from the capture process, filtered by the BCF is called Bayer Array (BA) and is
    composed of the monochromatic intensity of each pixel, following BCF pattern.
    RAW image files are composed by BA data in addition to metadata acquired at the
    time of capture, in this section, information such as capture time, pre-processing
    strategy, black level, aperture, exposure, ISO, among others, are usually encapsulated
    in standard Exchangeable Image File (EXIF) data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BCF 滤光片放置在传感器元件的正前方。捕获过程产生的信号，经过 BCF 滤波，称为 Bayer 阵列 (BA)，由每个像素的单色强度组成，遵循 BCF
    图案。RAW 图像文件由 BA 数据以及捕获时获得的元数据组成，本节中，信息如捕获时间、预处理策略、黑电平、光圈、曝光、ISO 等通常被封装在标准可交换图像文件
    (EXIF) 数据中。
- en: Modern image sensors, such as OmniVision’s OV5647, uses a sensor element composed
    by a grid of 2624×1956 photosensitive sensors, covered by a layer of BCF. In addition
    to image data acquisition, this device also provides various processing options
    such as Automatic Exposure Control (AEC), Automatic White balance (AWB), Automatic
    Band Filter (ABF), and Automatic Black level Calibration, to provide a RAW output
    with better overall image quality[4].
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现代图像传感器，如 OmniVision 的 OV5647，使用由 2624×1956 个光敏传感器组成的传感器元素，覆盖一层 BCF。除了图像数据采集，这个设备还提供了各种处理选项，如自动曝光控制（AEC）、自动白平衡（AWB）、自动带通滤波器（ABF）和自动黑电平校准，以提供具有更好整体图像质量的
    RAW 输出[4]。
- en: An ISP pipeline usually referred as the second subsystem of a digital camera.
    It is build on a series of processes that aim to convert a RAW image file into
    a visible digital object, in order to display and store the image that was captured.
    Some of the steps of a traditional pipeline will be explored here in general lines.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ISP 管道通常被称为数字相机的第二个子系统。它建立在一系列过程之上，旨在将 RAW 图像文件转换为可见的数字对象，以便显示和存储捕获的图像。这里将一般性地探讨传统管道的一些步骤。
- en: 'Traditionally, ISP pipelines are constructed as a sequential series of operations.
    The Input of an ISP is usually a RAW image and the output is a RGB encoded digital
    image. Commercially used ISPs may vary in order and type of operations, depending
    on the manufacturer needs. This information is not available, however, there are
    some basic operations that are necessary for most ISPs and are used as a basis
    for the study of this type of subsystem. That said, there are known stages common
    to almost all traditional ISPs, these stages are shown in the Figure [4](#S2.F4
    "Figure 4 ‣ 2 Software ISP ‣ ISP meets Deep Learning: A Survey on Deep Learning
    Methods for Image Signal Processing") below.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '传统的 ISP 管道通常是由一系列顺序操作构建而成。ISP 的输入通常是一个 RAW 图像，而输出则是一个 RGB 编码的数字图像。商业上使用的 ISP
    可能在操作的顺序和类型上有所不同，具体取决于制造商的需求。然而，这些信息并不可用，但大多数 ISP 都需要一些基本操作，这些操作作为研究这种子系统的基础。也就是说，几乎所有传统
    ISP 都有一些已知的阶段，这些阶段如下面的图 [4](#S2.F4 "Figure 4 ‣ 2 Software ISP ‣ ISP meets Deep
    Learning: A Survey on Deep Learning Methods for Image Signal Processing") 所示。'
- en: '![Refer to caption](img/6a783eb29196cbf5f1dadb90c45e3c35.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6a783eb29196cbf5f1dadb90c45e3c35.png)'
- en: 'Figure 4: Traditional ISP pipeline. Based on [[49](#bib.bib49)]'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：传统 ISP 管道。基于 [[49](#bib.bib49)]
- en: 'Although modern image sensors are capable to fix many data acquisition issues[4],
    a pre-processing of the data contained in the RAW image is important to verify
    signal integrity, identify and fix acquisition failures, preventing errors from
    being propagated through the following operations. Three stages are commonly mentioned
    in the context of ISP pre-processing: Signal conditioning, defective pixel correction,
    and black level offset. Signal conditioning refers to normalization, linearization
    and other operations necessary to adapt the data obtained by the sensor to be
    processed by the ISP [[1](#bib.bib1)].'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现代图像传感器能够解决许多数据采集问题[4]，但对 RAW 图像中的数据进行预处理是重要的，以验证信号完整性，识别和修复采集故障，防止错误在后续操作中传播。ISP
    预处理的三个常见阶段是：信号调节、缺陷像素校正和黑电平偏移。信号调节是指对传感器获得的数据进行归一化、线性化及其他必要的操作，以使数据适应 ISP 进行处理 [[1](#bib.bib1)]。
- en: 2.1 Black Level Offset Correction
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 黑电平偏移校正
- en: Black level offset correction is a necessity created by the imprecision of image
    sensors, its goal is to correct the ISP input value to reduce black current effects,
    which tend to increase the light intensity measured by the sensor element and
    can cause an blur effect in a processed image [[1](#bib.bib1)]. The black level
    offset aims to ensure that the black tones contained in the image are correctly
    registered. Modern image sensors usually provides an array that contains a mask
    for black level correction[4].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 黑电平偏移校正是由图像传感器的不精确性所产生的必要性，其目标是校正 ISP 输入值，以减少黑电流效应，这种效应会增加传感器元素测量的光强度，并可能在处理后的图像中造成模糊效果 [[1](#bib.bib1)]。黑电平偏移旨在确保图像中的黑色调被正确记录。现代图像传感器通常提供一个包含黑电平校正掩码的阵列[4]。
- en: 2.2 Defective Pixels Correction
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 缺陷像素校正
- en: Defective Pixels are also common and expected acquisition errors up to a certain
    amount [[50](#bib.bib50)], they occur due to measurement issues caused by production
    errors, storage methods and temperature problems. The identification of defective
    pixels is made, in general, from the analysis of the light intensity variation
    of a central pixel in relation to its neighbors [[50](#bib.bib50)]. There are
    several methods of correction of defective pixels, one of them is to apply the
    average value of the neighboring points to the pixel identified as defective [[50](#bib.bib50)].
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 缺陷像素也是常见的预期采集错误，出现这种问题的原因包括生产缺陷、存储方法和温度问题[[50](#bib.bib50)]。缺陷像素的识别通常是通过分析中央像素相对于其邻近像素的光强度变化来进行的[[50](#bib.bib50)]。修正缺陷像素的方法有多种，其中一种是将邻近点的平均值应用于被识别为缺陷的像素[[50](#bib.bib50)]。
- en: 2.3 White Balance
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 白平衡
- en: After the fixing acquisition issues, a usual first step of a conventional ISP
    is to perform a white balance on the imputed data. Although the HVS is able to
    identify the white color of objects illuminated by different types of light sources,
    digital systems do not have this capability, different frequency range of light
    results in different measured values [[51](#bib.bib51)]. White balance is a step
    that aims to ensure that the measured colors have a natural tone for the human
    eye after reconstruction [[1](#bib.bib1)]. A strategy often used by ISPs is the
    AWB strategy, while many image sensors already have this feature built in as in
    OmniVision’s OV5647, many ISPs implement it separately. A usual way of doing this
    is using the gray world assumption, which dictates that the average color of each
    sampled channel tends to be equal in most cases. Based on this principle, the
    ratio between the average light intensity measured in the green channel and the
    others channels used as a basis to make the correction in all pixels [[51](#bib.bib51)].
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在修正采集问题之后，传统ISP的一个常见第一步是对输入数据进行白平衡处理。虽然人类视觉系统能够识别不同光源下物体的白色，但数字系统并不具备这种能力，不同频段的光会导致不同的测量值[[51](#bib.bib51)]。白平衡旨在确保重建后的颜色对人眼具有自然的色调[[1](#bib.bib1)]。ISP常用的策略是自动白平衡（AWB）策略，尽管许多图像传感器如OmniVision的OV5647已经内置此功能，但许多ISP还是单独实现了这一功能。常用的方法是利用灰世界假设，该假设认为在大多数情况下，每个采样通道的平均颜色是相等的。基于这一原理，通过测量绿色通道和其他通道的平均光强度之间的比率来进行像素的修正[[51](#bib.bib51)]。
- en: 2.4 Demosaicing
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 去马赛克
- en: The most computationally heavy step of a digital image reconstruction refers
    to the conversion of CFA data to visible image, this process is called demosaicing.
    A recurrent strategy to perform this operation is by some variation of weight
    interpolation of the absolute values of each pixel in the CFA. Although most of
    the demosaicing techniques used commercially are protected by patent. Some open
    source applications like RawTherapee are transparent with the demosaicing technique
    used. In this application, visible image is reconstructed using algorithms such
    as Adaptive Homogeneity-Directed  [[52](#bib.bib52)].
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 数字图像重建中最计算密集的步骤是将CFA数据转换为可见图像，这一过程称为去马赛克。执行这一操作的一种常见策略是通过某种变体的加权插值来处理CFA中每个像素的绝对值。尽管大多数商业化的去马赛克技术受到专利保护，但一些开源应用程序如RawTherapee对使用的去马赛克技术是透明的。在这个应用程序中，可见图像使用如自适应同质性导向算法进行重建[[52](#bib.bib52)]。
- en: '![Refer to caption](img/49858e2fe0a88e8e8e63ca926c47567c.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/49858e2fe0a88e8e8e63ca926c47567c.png)'
- en: 'Figure 5: Demosaicing a CFA.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：对CFA进行去马赛克处理。
- en: 'Figure [5](#S2.F5 "Figure 5 ‣ 2.4 Demosaicing ‣ 2 Software ISP ‣ ISP meets
    Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing")
    shows the comparison between the ISP pipeline output (third image) of the smartphone
    Samsung G9600 and the RAW image sent by the image sensor (first image). Analyzing
    the metadata of the sensor output it was possible to identify that the CFA color
    pattern used in the capture process was the Bayer Green-Red-Blue-Green pattern.
    This information was used to perform a demosaicing (second image) on the original
    RAW . The comparison between the second and third images highlights the importance
    of all steps in an ISP pipeline in order to reconstruct high quality photos.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#S2.F5 "Figure 5 ‣ 2.4 Demosaicing ‣ 2 Software ISP ‣ ISP meets Deep
    Learning: A Survey on Deep Learning Methods for Image Signal Processing") 显示了智能手机
    Samsung G9600 的 ISP 流程输出（第三张图）与图像传感器发送的 RAW 图像（第一张图）之间的比较。通过分析传感器输出的元数据，可以识别出捕获过程中使用的
    CFA 颜色模式是 Bayer 绿色-红色-蓝色-绿色模式。这个信息被用于对原始 RAW 图像进行去马赛克处理（第二张图）。第二张图和第三张图之间的比较突显了
    ISP 流程中所有步骤在重建高质量照片中的重要性。'
- en: 2.5 Denoising
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 去噪
- en: Image denoising is a complex step of the digital image reconstruction task,
    where the goal is to remove the noise from an input image to estimate the original
    image. This step is usually used in the traditional ISP pipeline because of defects
    or heterogeneity of the image sensor hardware components, and due to image compression.
    Image denoising is very important for several applications in the vision computing
    field and has received a lot of attention over the years [[53](#bib.bib53), [54](#bib.bib54),
    [55](#bib.bib55), [21](#bib.bib21), [56](#bib.bib56)]. Several methods have been
    proposed for image denoising. They can be classified into classical approaches
    and deep learning approaches. The classical approaches encompass the spatial domain
    method, which applies filters in the image to remove the noise, and the transform
    domain method, which changes the domain from the input image and then uses a denoising
    procedure to improve the image. The deep learning approach, in most cases, is
    a CNN-based method. In this survey, some works of deep learning for image denoising
    will be cited [[57](#bib.bib57), [26](#bib.bib26), [58](#bib.bib58), [33](#bib.bib33),
    [59](#bib.bib59), [60](#bib.bib60)].
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图像去噪是数字图像重建任务中的一个复杂步骤，其目标是从输入图像中去除噪声，以估计原始图像。由于图像传感器硬件组件的缺陷或异质性以及图像压缩，这一步骤通常用于传统的ISP流程。图像去噪对视觉计算领域的多个应用非常重要，多年来受到广泛关注[[53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [21](#bib.bib21), [56](#bib.bib56)]。已经提出了几种图像去噪方法，它们可以分为经典方法和深度学习方法。经典方法包括空间域方法，该方法在图像中应用滤波器以去除噪声，以及变换域方法，该方法将域从输入图像中改变，然后使用去噪程序来改进图像。深度学习方法在大多数情况下是一种基于CNN的方法。在这项调查中，将引用一些关于图像去噪的深度学习工作[[57](#bib.bib57),
    [26](#bib.bib26), [58](#bib.bib58), [33](#bib.bib33), [59](#bib.bib59), [60](#bib.bib60)]。
- en: 2.6 Deblurring
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 去模糊
- en: 'Blur is a general artifact that is hard to avoid in digital image processing
    and can be caused by various sources like motion blur, out-of-focus, camera shake,
    extreme light intensity, accumulated error in ISP pipeline, etc. Given this, many
    handcraft deblurring algorithms exist in the literature to mitigate this problem[[61](#bib.bib61),
    [62](#bib.bib62)] and some are included in ISPs. The Razligh and Kehtarnavaz [[63](#bib.bib63)]
    proposed a deblurring method for cell phones that takes into consideration the
    brightness and the contrast to correct the blurred image. On the other hand, Hu
    et al. [[64](#bib.bib64)] consider the use of smartphone’s inertial sensors, gyroscope
    and accelerometer, for kernel estimation and utilize an online calibration to
    synchronize cameras and sensors. However, many of these classical methods englobe
    only some cases, realize handcraft feature extraction and are necessary two previous
    steps[[61](#bib.bib61)]: blur detection and blur classification. Deep learning
    was an alternative to solving these problems in the last years, possibly jointly
    all these steps on a unique step[[65](#bib.bib65)].'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊是一种在数字图像处理中很难避免的伪影，可能由各种来源引起，如运动模糊、对焦不准、相机抖动、极端光强、ISP管道中的累计误差等。鉴于此，文献中存在许多手工去模糊算法来缓解这一问题[[61](#bib.bib61),
    [62](#bib.bib62)]，其中一些已被纳入ISP中。Razligh 和 Kehtarnavaz [[63](#bib.bib63)] 提出了一个针对手机的去模糊方法，该方法考虑了亮度和对比度来校正模糊图像。另一方面，Hu
    等人 [[64](#bib.bib64)] 认为可以使用智能手机的惯性传感器、陀螺仪和加速度计来估计核，并利用在线校准来同步相机和传感器。然而，许多这些经典方法仅涵盖了某些情况，实现了手工特征提取，并且需要两个先前的步骤[[61](#bib.bib61)]：模糊检测和模糊分类。近年来，深度学习成为解决这些问题的一种替代方案，可能将所有这些步骤联合在一个唯一的步骤中[[65](#bib.bib65)]。
- en: 2.7 Post-processing step
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.7 后处理步骤
- en: Each camera manufacturers can use different, often proprietary, processing methods
    to improve image quality. The post-processing step aims to make some adjustments
    to the images that went through the previous processes. Some of the most common
    post-processing steps used are edge enhancement, removal of colored artifacts
    and coring [[1](#bib.bib1)]. These techniques use heuristics and require considerable
    fine-tuning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 各个相机制造商可能使用不同的、通常是专有的处理方法来改善图像质量。后处理步骤旨在对经过先前处理的图像进行一些调整。最常用的一些后处理步骤包括边缘增强、去除彩色伪影和核心处理[[1](#bib.bib1)]。这些技术使用启发式方法，并需要大量的微调。
- en: For example, the demosaicing step can introduce artifacts that can be problematic,
    such as zippered edges and confetti in the rest of the image. In the post-processing
    stage, it is essential to keep these artifacts to a minimum without losing image
    sharpness [[1](#bib.bib1)], some camera manufacturers use edge enhancement techniques
    to make the image more attractive by reducing low-frequency objects contained
    in the image. The solution to these problems involves many variables, from the
    size of the capture sensor to the demosaicing technique used.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，去马赛克步骤可能会引入问题性伪影，比如拉链状的边缘和图像其余部分的彩色碎片。在后处理阶段，必须将这些伪影保持到最小，同时不失去图像的清晰度[[1](#bib.bib1)]，一些相机制造商使用边缘增强技术，通过减少图像中的低频对象，使图像更具吸引力。解决这些问题涉及许多变量，从捕捉传感器的大小到使用的去马赛克技术。
- en: 2.8 Rendered Color Spaces
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.8 渲染颜色空间
- en: Rendered color spaces are generally used as output and have a limited scale,
    unlike unrendered which are based on scenes. The rendered color space is created
    based on data extracted from an image in unrendered space and contains a maximum
    of 8b while the unrendered space has a variable range between 12 and 16b [[1](#bib.bib1)],
    for this reason, the process of transforming unrendered space to rendered space
    contains a loss in dynamic range.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染颜色空间通常用作输出，并且具有有限的范围，与基于场景的未渲染颜色空间不同。渲染颜色空间是基于从未渲染空间中的图像提取的数据创建的，最大为8b，而未渲染空间的范围在12到16b之间[[1](#bib.bib1)]，因此，将未渲染空间转换为渲染空间的过程包含动态范围的损失。
- en: The most common rendered space is the sRGB [[1](#bib.bib1)] color space, which
    has become common for multimedia. Another common rendering space is the ITU-R
    BT.709-3, which was created with high-definition televisions in mind. The sRGB
    standard adopts the primaries defined by ITU-R BT.709-3\. It is these patterns
    that define the methods of transforming unrendered spaces to values of 8b imposed
    by most output media.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的渲染空间是 sRGB [[1](#bib.bib1)] 颜色空间，它已成为多媒体的常见标准。另一个常见的渲染空间是 ITU-R BT.709-3，它是为高清电视设计的。sRGB
    标准采用了 ITU-R BT.709-3 定义的基色。这些模式定义了将未渲染空间转换为大多数输出媒体所施加的 8b 值的方法。
- en: Depending on the preview mode, images need to be converted to proper color space.
    An example is in the case of viewing by a CRT monitor with an additive color system,
    images need to be converted to an 8b output given the display model used, offset
    values, color temperature, and values of gamma.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 根据预览模式，图像需要转换为适当的颜色空间。例如，在 CRT 显示器的加色系统下查看时，图像需要根据使用的显示模型、偏移值、色温和 gamma 值转换为
    8b 输出。
- en: When the goal is storage, we have two solutions, professional cameras that have
    a very large set of sensors and much larger storage space, and generally store
    the images in a proprietary format or as Tag Image File Format/Electronic Photography
    (TIFF/ EP). Images stored as TIFF/EP have additional information such as camera
    settings details and the color transformation matrix [[1](#bib.bib1)]. JPEG2000
    is an international standard that offers a more efficient compression than the
    common JPEG standard, in addition to offering several features such as control
    over data compression size and image resolution. However, despite the benefits
    that JPEG2000 presents, its computational complexity, and high memory cost are
    limiting factors.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标是存储时，我们有两种解决方案：一种是专业相机，它们具有非常大的传感器集和更大的存储空间，通常将图像存储为专有格式或标记图像文件格式/电子摄影（TIFF/EP）。以
    TIFF/EP 格式存储的图像包含附加信息，如相机设置细节和颜色转换矩阵 [[1](#bib.bib1)]。JPEG2000 是一种国际标准，提供比常见的
    JPEG 标准更高效的压缩，并提供多种功能，如数据压缩大小和图像分辨率的控制。然而，尽管 JPEG2000 具有许多优点，但其计算复杂性和高内存成本是限制因素。
- en: Given the high complexity and need for tuning of modern ISP pipelines, many
    studies are being made aiming to use machine learning to convert RAW image data
    into high quality outputs. This work shows state of the art of these studies.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于现代 ISP 流水线的高复杂性和调优需求，许多研究正致力于使用机器学习将 RAW 图像数据转换为高质量输出。本工作展示了这些研究的最新进展。
- en: 3 Know Approaches
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种已知方法
- en: In this section, we review the applications of deep learning-based methods to
    substitute the traditional handcraft ISP pipelines, where include operations as
    demosaicing, denoising, white balance, tone adjustment, and exposure balance.
    Also,we briefly summarize some works that use this approach to increase the performance
    in other computer vision tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了基于深度学习的方法在替代传统手工 ISP 流水线中的应用，包括去马赛克、去噪、白平衡、色调调整和曝光平衡等操作。同时，我们简要总结了一些利用这种方法提高其他计算机视觉任务性能的工作。
- en: 3.1 HERNet
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 HERNet
- en: HighEr-Resolution Network (HERNet) [[22](#bib.bib22)] is a network that can
    learn local and global information about high-resolution image patches without
    excessive consumption of GPU memory. This network has two paths for local and
    global feature extraction and the introduction of the Pyramid Full-Image Encoder
    [[22](#bib.bib22)] that realizes a regularization of the output image and helps
    to reduce the number of artifacts. Besides, this work proposes training the model
    with progressively growing the resolution of inputs, which results in performance
    stability and short training time.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 高分辨率网络（HERNet）[[22](#bib.bib22)] 是一种可以学习高分辨率图像补丁的局部和全局信息的网络，且不会过度消耗 GPU 内存。该网络具有用于局部和全局特征提取的两个路径，并引入了金字塔全图编码器
    [[22](#bib.bib22)]，实现了输出图像的正则化，有助于减少伪影。此外，该工作还提出通过逐渐增加输入分辨率来训练模型，这带来了性能稳定性和较短的训练时间。
- en: The local information path consists of Multi-Scale Residual Blocks (MSRBs)[[66](#bib.bib66)]
    that have two convolutional layers with 3x3 and 5x5 kernels in parallel. Furthermore,
    in the global information path, was applied an Autoencoder mechanism with modified
    Residual in Residual (RIR)[[67](#bib.bib67)] modules for feature extraction. The
    modified RIRs modules aim to decrease the GPU memory usage, especially to high-resolution
    images, then the Channel Attention Units were removed, and stacked the remaining.
    Finally, the authors trained and validated the model with the ZRR dataset[[31](#bib.bib31)]
    and used only the L1 loss in training. Unfortunately, the L1 loss favors blurry
    images in datasets with misalignment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 局部信息路径由多尺度残差块（**MSRBs**）[[66](#bib.bib66)] 组成，这些块具有两个并行的卷积层，分别使用3x3和5x5的卷积核。此外，在全局信息路径中，应用了具有修改的残差中的残差（**RIR**）[[67](#bib.bib67)]模块的自编码器机制用于特征提取。修改后的RIR模块旨在减少GPU内存使用，特别是对高分辨率图像的处理，因此去除了通道注意力单元，并对剩余部分进行了堆叠。最终，作者使用ZRR数据集[[31](#bib.bib31)]进行了模型训练和验证，并在训练中仅使用了L1损失。不幸的是，L1损失在对齐不准确的数据集中会偏向模糊图像。
- en: The local information path consists of Multi-Scale Residual Blocks (MSRBs)[[66](#bib.bib66)]
    that have two convolutional layers with 3x3 and 5x5 kernels in parallel. Furthermore,
    in the global information path, is apply an Autoencoder mechanism with modified
    Residual in Residual (RIR)[[67](#bib.bib67)] modules for feature extraction. The
    modified RIRs modules aim to decrease the GPU memory usage, especially to high-resolution
    images, then the Channel Attention Units are removed and stacked the remaining.
    The authors trained and validated the model with the ZRR dataset[[31](#bib.bib31)]
    and used only the L1 loss in training. Unfortunately, the L1 loss favors blurry
    images in datasets with misalignment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 局部信息路径由多尺度残差块（**MSRBs**）[[66](#bib.bib66)] 组成，这些块具有两个并行的卷积层，分别使用3x3和5x5的卷积核。此外，在全局信息路径中，应用了具有修改的残差中的残差（**RIR**）[[67](#bib.bib67)]模块的自编码器机制用于特征提取。修改后的RIR模块旨在减少GPU内存使用，特别是对高分辨率图像的处理，因此去除了通道注意力单元，并对剩余部分进行了堆叠。作者使用ZRR数据集[[31](#bib.bib31)]进行了模型训练和验证，并在训练中仅使用了L1损失。不幸的是，L1损失在对齐不准确的数据集中会偏向模糊图像。
- en: The progressive training was used to train the network, where the input image
    resolution increased during the training, keeping the same architecture of networks
    all the time. As a result, this process can make the network converges more quickly.
    Besides, HERNet won second place in track 1 of fidelity and first place in track
    2 of perceptual in AIM 2019 RAW to RGB Mapping Challenge. HERNet won second place
    in track 1 of fidelity and first place in track 2 of perceptual in AIM 2019 RAW
    to RGB Mapping Challenge.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 采用渐进式训练来训练网络，其中输入图像分辨率在训练过程中逐渐增加，同时网络架构始终保持不变。因此，这一过程能够使网络更快地收敛。此外，HERNet在AIM
    2019 RAW到RGB映射挑战中获得了第一赛道的第二名和第二赛道的第一名。
- en: 3.2 CameraNet
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 CameraNet
- en: 'The CameraNet[[23](#bib.bib23)] proposes an effective and general framework
    for a deep learning-based ISP pipeline, with two stages of CNN stacked. The motivation
    for this is that some subtasks of the ISP pipeline have poor correlation, then
    the subtasks from the ISP pipeline were divided into two stages: the first stage
    is the restoration stage with tasks like demosaicing, denoising, and white balance,
    and the enhancement stage as second stage performing tasks like exposure adjustment,
    tone mapping, color enhancement, and contrast adjustment. Besides, two ground
    truths were created for each image in the datasets HDR+[[68](#bib.bib68)] and
    FiveK[[69](#bib.bib69)] using Adobe Camera Raw¹¹1https://helpx.adobe.com/br/camera-raw/using/supported-cameras.html
    and Adobe Lightroom ²²2https://www.adobe.com/lightroom. Each ground truth was
    used to train a different stage. In the CameraNet pipeline, before these two main
    stages, the input image is pre-processed with the bad pixels removing, initial
    demosaicing with interpolation, and converting RGB to CIE XYZ space because it
    is related to human perception. Moreover, the U-Net is the base model for these
    two stages, because of the multi-scale extraction features. Some changes were
    made, like the addition of a fully connected layer in the lowest level of the
    network and the use of different processing blocks in each stage. While the restoration
    stage uses plain convolutional blocks, the enhancement stage uses residual connections
    to details improvement. Furthermore, in the experiments, the CameraNet generated
    images with less noise, artifacts, better color mapping, and higher qualitative
    scores than DeepISP[[70](#bib.bib70)] Network in the HDR+ and, mainly, SID[[71](#bib.bib71)]
    dataset. The explanation for this difference can be the high level of noise in
    the SID dataset and the separation of weakly related subtasks in two stages in
    the CameraNet. In the FiveK dataset, both methods achieve comparables results
    in SSIM, but the CameraNet obtain superior results in PSNR and Color Error indices
    because this dataset was captured with high-end cameras, reducing the noise level.
    DCRAW and CameraRAW, which generate images with default settings, have the lowest
    results in comparison with Deep Learning methods, explained by the limitation
    of traditional methods.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CameraNet[[23](#bib.bib23)] 提出了一个有效且通用的深度学习基础的ISP管道框架，其中包含两个叠加的CNN阶段。其动机在于，ISP管道的一些子任务之间相关性较差，因此将这些子任务分为两个阶段：第一阶段是恢复阶段，包括去马赛克、去噪和白平衡等任务；第二阶段是增强阶段，执行曝光调整、色调映射、颜色增强和对比度调整等任务。此外，使用Adobe
    Camera Raw¹¹1https://helpx.adobe.com/br/camera-raw/using/supported-cameras.html和Adobe
    Lightroom ²²2https://www.adobe.com/lightroom在数据集HDR+[[68](#bib.bib68)]和FiveK[[69](#bib.bib69)]中为每张图片创建了两个真实值。每个真实值用于训练不同的阶段。在CameraNet管道中，在这两个主要阶段之前，输入图像会进行预处理，包括去除坏点、初步去马赛克和插值、以及将RGB转换为CIE
    XYZ空间，因为这与人类感知相关。此外，U-Net是这两个阶段的基础模型，因其多尺度特征提取能力。做了一些修改，例如在网络的最低层增加了一个全连接层，并在每个阶段使用不同的处理块。恢复阶段使用普通卷积块，而增强阶段则使用残差连接以改善细节。此外，在实验中，CameraNet生成的图像在HDR+数据集和特别是在SID[[71](#bib.bib71)]数据集中，比DeepISP[[70](#bib.bib70)]网络具有更少的噪声、伪影、更好的颜色映射和更高的质量评分。这种差异的解释可能是SID数据集中的噪声水平较高以及CameraNet中将弱相关的子任务分为两个阶段。在FiveK数据集中，两种方法在SSIM上取得了可比的结果，但CameraNet在PSNR和颜色误差指标上取得了更优的结果，因为该数据集使用高端相机捕捉，减少了噪声水平。与深度学习方法相比，DCRAW和CameraRAW生成的图像在默认设置下具有最低的结果，这可以解释为传统方法的局限性。
- en: 3.3 Deep Camera
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 深度相机
- en: 'As the first CNN Network proposed to substitute the entire ISP pipeline, the
    Deep Camera[[3](#bib.bib3)] is a small network with four parallel paths: a main
    path and other three short paths with a convolutional layer in your middle. The
    explanation for this is the model is very small compared with the ResNet, then
    the network does not generalize well with the copy of the input to the output
    of a block. Furthermore, the authors created an inverse ISP to recreate RAW images
    from a large dataset [[72](#bib.bib72)] with 11,000 images and several types of
    scenes and illuminants, where the training and experiment stages used the resulted
    dataset.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一个提出替代整个ISP管道的CNN网络，Deep Camera[[3](#bib.bib3)]是一个具有四个并行路径的小型网络：一个主路径和其他三个中间有卷积层的短路径。解释是模型相较于ResNet非常小，因此网络在将输入复制到块的输出时泛化能力较差。此外，作者创建了一个逆ISP，从一个包含11,000张图像及多种场景和光源的大数据集中重建RAW图像[[72](#bib.bib72)]，训练和实验阶段使用了生成的数据集。
- en: The CNN model outperformed traditional methods in white balance and image reconstruction
    tasks, delivering many better images. However, in some images with many different
    colors, the algorithms are better. Besides, the Deep Camera can do defective pixel
    correction and be used in other color filter mosaics like the X-Trans color filter
    by Fujifilm.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: CNN模型在白平衡和图像重建任务中优于传统方法，提供了许多更好的图像。然而，在一些颜色较多的图像中，算法表现更好。此外，Deep Camera可以进行缺陷像素修正，并可用于其他颜色滤镜马赛克，如富士胶卷的X-Trans颜色滤镜。
- en: 3.4 DRDN
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 DRDN
- en: The DRDN [[24](#bib.bib24)] proposed a convolutional neural network to color
    filter array demosaicing. Using a mosaiced image as input, the proposed model
    is trained in an end-to-end manner to generate demosaiced images outputs. Compared
    to other conventional convolutional neural network-based demosaicing models, the
    proposed model requires less computational complexity, because it does not require
    the initial interpolation step for mosaicked input images. It also solved the
    vanishing-gradient problem experienced by many deep neural networks, due to the
    residual learning[[73](#bib.bib73)] and densely connected convolutional neural
    network[[74](#bib.bib74)]. Moreover, the proposed model applied block-wise convolutional
    neural networks to consider local features and a sub-pixel interpolation layer,
    generating demosaiced output images more efficiently and accurately.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: DRDN [[24](#bib.bib24)] 提出了一个卷积神经网络用于颜色滤镜阵列去马赛克。该模型以马赛克图像作为输入，采用端到端的方式进行训练，以生成去马赛克的图像输出。与其他传统的卷积神经网络去马赛克模型相比，所提出的模型需要更少的计算复杂度，因为它不需要对马赛克输入图像进行初始插值步骤。它还通过残差学习[[73](#bib.bib73)]和密集连接的卷积神经网络[[74](#bib.bib74)]解决了许多深度神经网络中遇到的梯度消失问题。此外，该模型应用了块级卷积神经网络来考虑局部特征和一个子像素插值层，从而更高效、准确地生成去马赛克的输出图像。
- en: This paper has an exceptional explanation and contextualization about the demosaicing
    challenging task, citing similar previous methods and highlighting what could
    be improved in each one. The study detailed the training parameters and aspects
    related to the considered Datasets. Finally, there is a vast performance comparison,
    in which the proposed method stood out in the vast majority. On the other hand,
    the paper did not reveal the inference time obtained during the validation phase,
    considering that the studied datasets have medium size images. Likewise, the authors
    could discuss deeper the reason that the DRDN did not reach the best PSNR in some
    evaluated cases [[24](#bib.bib24)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文对去马赛克的挑战任务提供了出色的解释和背景分析，引用了类似的前期方法，并突出显示了每种方法可以改进的地方。研究详细介绍了训练参数和与所考虑数据集相关的方面。最后，进行了广泛的性能比较，其中提出的方法在绝大多数情况下表现突出。另一方面，论文没有透露验证阶段获得的推断时间，考虑到研究的数据集具有中等大小的图像。同样，作者可以更深入地讨论为什么DRDN在一些评估案例中未能达到最佳PSNR[[24](#bib.bib24)]。
- en: 3.5 Deep Demosaicing for Edge Implementation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 边缘实现的深度去马赛克
- en: 'In this paper [[25](#bib.bib25)], the authors discussed the edge implementation
    of deep learning-based demosaicing algorithms on low-end edge devices major challenge.
    They provided an extensive search of deep neural network architectures, obtaining
    a Pareto front of Color Peak Signal to Noise Ratio (CPSNR) as the loss versus
    the number of parameters as the model complexity. The article contributed with
    a valuable reference frame about demosaicing methods, divided into six categories:
    Edge-sensitive methods, Directional interpolation and decision methods, Frequency
    domain approaches, Wavelet-based methods, Statistical reconstruction techniques,
    and Deep learning-based methods [[75](#bib.bib75)]. Likewise, the authors reviewed
    other relevant demosaicing aspects, like the unwanted presence of image artifacts,
    and performance evaluation methods.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中[[25](#bib.bib25)]，作者讨论了深度学习去马赛克算法在低端边缘设备上的边缘实现的主要挑战。他们对深度神经网络架构进行了广泛搜索，获得了以颜色峰值信噪比（CPSNR）为损失与参数数量为模型复杂度的帕累托前沿。该文章为去马赛克方法提供了有价值的参考框架，分为六个类别：边缘敏感方法、方向插值和决策方法、频域方法、小波方法、统计重建技术和基于深度学习的方法[[75](#bib.bib75)]。同样，作者还回顾了其他相关的去马赛克方面，如图像伪影的非期望存在以及性能评估方法。
- en: The study comes up with an exhaustive search of architectures, based on discrete
    and well-constructed hyper-parameters, like the number of filters and blocks,
    the skip connections length, and the use of depthwise separable convolutions.
    It presented a proper methodology and mathematical theory about the neural architecture
    search and the Pareto building. The highlights were five brand-new theorems in
    respect to the neural architecture search convergence. Lastly, the designed space
    with a simple exhaustive search outperformed the state-of-the-art and brought
    a range of loss versus complexity for edge implementation with varying resource
    constraints, overcoming drawbacks related to the number of evaluations and the
    search algorithm complexity. As a drawback, the authors should have discussed
    more the use of the given architecture search in other image processing tasks,
    not only in the edge implementation challenge.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究对架构进行了全面的搜索，基于离散且构造良好的超参数，例如滤波器和块的数量、跳跃连接的长度以及深度可分卷积的使用。研究提出了关于神经架构搜索和帕累托构建的适当方法论和数学理论。亮点是关于神经架构搜索收敛性的五个全新定理。最后，设计的空间通过简单的全面搜索超越了最先进的技术，并在边缘实施中带来了不同资源约束下的损失与复杂度范围，克服了与评估次数和搜索算法复杂性相关的缺点。作为缺点，作者应该更多讨论给定架构搜索在其他图像处理任务中的使用，而不仅仅是边缘实施挑战。
- en: 3.6 BayerUnify and BayerAug
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 BayerUnify和BayerAug
- en: Liu Et al. [[26](#bib.bib26)] presented two new techniques for DNN-based RAW
    image denoising. The first is a Bayer pattern unification (BayerUnify) method,
    that effectively deals with varied Bayer patterns from different data sources.
    The second method is the Bayer preserving augmentation (BayerAug), allowing proper
    RAW images augmentation. Associating these two techniques with a modified U-Net,
    the proposed method achieved a satisfactory SOTA PSNR of 52.11 and an SSIM of
    0.9969 in NTIRE 2019 Real Image Denoising Challenge.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Liu等人[[26](#bib.bib26)]提出了两种新的基于DNN的RAW图像去噪技术。第一种是拜耳模式统一（BayerUnify）方法，能够有效处理来自不同数据源的多种拜耳模式。第二种方法是拜耳保持增强（BayerAug），允许适当的RAW图像增强。将这两种技术与修改后的U-Net结合，所提出的方法在NTIRE
    2019真实图像去噪挑战中取得了令人满意的SOTA PSNR 52.11和SSIM 0.9969。
- en: BayerUnify consists of two stages. In the training phase, the study unified
    RAW data with different Bayer patterns via cropping. The technique maps the BGGR
    Bayer format, for example, within the other formats (RGGB, GRBG, and GBRG) and
    crops the selected area, converting any Bayer pattern to BGGR (or any other chosen
    pattern). In the testing phase, as the image pixels need to be processed, the
    technique unified the Bayer patterns via padding. Subsequently, there is the network
    denoising and the extra pixels removal, disunifying the output images and reversing
    the pattern conversion.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: BayerUnify由两个阶段组成。在训练阶段，研究通过裁剪将具有不同拜耳模式的RAW数据统一。该技术将BGGR拜耳格式映射到其他格式（RGGB、GRBG和GBRG）中，并裁剪所选区域，将任何拜耳模式转换为BGGR（或其他所选模式）。在测试阶段，由于图像像素需要处理，该技术通过填充统一了拜耳模式。随后，进行了网络去噪和额外像素的移除，解统一输出图像并逆转模式转换。
- en: Traditional data augmentation methods are frequently based on image flipping
    or cropping. However, for RAW images, the flipping procedure may affect the Bayer
    pattern. BayerAug sorts out this question, combining both flipping and cropping.
    The paper presented three different flipping methods, which enabled data augmentation
    on Bayer RAW images without any issues.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的数据增强方法通常基于图像翻转或裁剪。然而，对于RAW图像，翻转过程可能会影响拜耳模式。BayerAug解决了这个问题，结合了翻转和裁剪。论文展示了三种不同的翻转方法，使得对拜耳RAW图像的数据增强没有任何问题。
- en: The study evaluated the proposed method on the Smartphone Image Denoising Dataset
    (SIDD) [[76](#bib.bib76)] - 320 pairs of noisy and noise-free images that covered
    three different Bayer patterns. The networks were trained with L1 loss, AdamW[[77](#bib.bib77)]
    optimizer, and 200,000 iterations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究在Smartphone Image Denoising Dataset (SIDD) [[76](#bib.bib76)] 上评估了所提出的方法
    - 320对有噪声和无噪声图像，涵盖了三种不同的拜耳模式。网络使用L1损失、AdamW[[77](#bib.bib77)]优化器以及200,000次迭代进行训练。
- en: The paper presented a satisfactory introduction about the Bayer pattern pre-processing
    and augmentation, besides discussing related works. The proposed method is properly
    explained, as well as the network architecture and an exceptional training detailing.
    At that frame reference, the study shown a promising direction on RAW image processing
    with deep learning techniques. As a limitation, the authors could give a comparison
    to other works in the same NTIRE 2019 challenge, and a deeper discussion about
    the application of the proposed method in real devices.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 论文对 Bayer 模式的预处理和增强提供了令人满意的介绍，并讨论了相关工作。所提方法得到适当解释，网络架构和卓越的训练细节也得到了说明。在该框架参考下，该研究展示了使用深度学习技术处理
    RAW 图像的有希望的方向。作为局限性，作者可以提供与 NTIRE 2019 挑战中其他工作的比较，以及对所提方法在实际设备中应用的深入讨论。
- en: 3.7 VisionISP
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 VisionISP
- en: Wu Et al. [[27](#bib.bib27)] proposed a particular ISP method for computer vision
    applications. VisionISP reduced the data transmission needs without relevant information
    loss, optimizing the subsequent computer vision system performance. The framework
    consists of three processing blocks. The first block, the Vision Denoiser, reduces
    the input signal noise and modifies the tuning targets on an existing ISP. The
    study adopted the technique presented by Nishimura[[78](#bib.bib78)] to optimize
    the denoising parameters, constructing the denoiser for the computer vision task,
    not for image quality. The paper also highlighted that the demosaicing step can
    be skipped and the color filter array image usage, instead of a demosaiced image,
    did not improve computer vision task performance. The second block, the Vision
    Local Tone Mapping (VLTM), reduced the bit-depth, achieving similar accuracy with
    fewer bits per pixel. VLTM used a global non-linear transformation followed by
    a local detail boosting operator. Lastly, the Trainable Vision Scaler block (TVS)
    is a generic neural network that processes and downscales the input for a following
    computer vision engine.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人[[27](#bib.bib27)] 提出了一个专门针对计算机视觉应用的 ISP 方法。VisionISP 减少了数据传输需求而不会丢失相关信息，从而优化了随后的计算机视觉系统性能。该框架由三个处理模块组成。第一个模块，Vision
    Denoiser，减少了输入信号噪声并修改了现有 ISP 上的调谐目标。该研究采用了 Nishimura[[78](#bib.bib78)] 提出的技术来优化去噪参数，构建了用于计算机视觉任务的去噪器，而不是用于图像质量的去噪器。论文还强调了去马赛克步骤可以跳过，且使用彩色滤光片阵列图像而不是去马赛克图像并没有改善计算机视觉任务性能。第二个模块，Vision
    Local Tone Mapping (VLTM)，减少了比特深度，在每个像素的比特数更少的情况下实现了类似的准确度。VLTM 使用了全局非线性变换，后跟局部细节增强算子。最后，Trainable
    Vision Scaler 模块（TVS）是一个通用神经网络，处理并缩小输入以供后续计算机视觉引擎使用。
- en: VisionISP was trained and evaluated with the KITTI 2D object detection dataset [[79](#bib.bib79)],
    an autonomous driving benchmark dataset. The study measured the influence of each
    VisionISP block in the mean average precision(mAP). As a computer vision task
    sample in the experiments, the authors used the SqueezeDet [[80](#bib.bib80)]
    framework and its original code.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: VisionISP 在 KITTI 2D 目标检测数据集[[79](#bib.bib79)]上进行了训练和评估，这是一个自动驾驶基准数据集。研究测量了每个
    VisionISP 模块对平均精度均值（mAP）的影响。作为实验中的计算机视觉任务样本，作者使用了 SqueezeDet[[80](#bib.bib80)]
    框架及其原始代码。
- en: The paper provided a proper explanation and evaluation of each VisionISP component.
    As a positive aspect, the experiments show that once TVS is trained, it can be
    used with other computer vision systems. VisionISP as a whole can be trained jointly
    or separately with the computer vision backbone. Besides that, each component
    of the proposed framework enhances a computer vision engine performance and can
    be deployed independently.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 论文对每个 VisionISP 组件提供了恰当的解释和评估。一个积极的方面是，实验表明一旦 TVS 经过训练后，它可以与其他计算机视觉系统一起使用。VisionISP
    整体可以与计算机视觉骨干网络一起或单独训练。此外，所提框架的每个组件都能增强计算机视觉引擎的性能，并且可以独立部署。
- en: Nevertheless, the paper could provide more training details (e.g., epochs number,
    time inference, hardware, etc.) and a deeper comparison of the VisionISP effectiveness
    with other computer vision optimization systems at that time.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，论文可以提供更多的训练细节（例如，训练轮数、推理时间、硬件等）以及 VisionISP 效果与当时其他计算机视觉优化系统的更深入比较。
- en: 3.8 RLDD
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 RLDD
- en: In this paper [[28](#bib.bib28)], the authors combined convolutional neural
    networks with traditional algorithms to reverse the order of the traditional CFA
    pipeline (demosaicking and denoising). The method, which we called here RLDD,
    uses two stages for demosaicking-denoising. The first stage performed the demosaicking
    by composing the gradient-based threshold-free (GBTF) method [[81](#bib.bib81)]
    and a convolutional neural network to overcome the reduction of image resolution
    in the subsampling operation. The second stage performed the denoising using another
    convolutional neural network whose goal was to deal with residual noise. The properties
    of the residual noise were altered due to complex interpolation, and the convolutional
    neural network aimed to remove it without losing the details of an image.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文 [[28](#bib.bib28)]中，作者将卷积神经网络与传统算法相结合，逆转了传统CFA流程（去马赛克和去噪）的顺序。我们在此称之为RLDD的方法，使用两个阶段进行去马赛克-去噪。第一个阶段通过将基于梯度的无阈值（GBTF）方法 [[81](#bib.bib81)]与卷积神经网络相结合来执行去马赛克，以克服在下采样操作中图像分辨率的降低。第二个阶段使用另一个卷积神经网络进行去噪，旨在处理残余噪声。由于复杂的插值，残余噪声的属性发生了变化，而卷积神经网络旨在去除噪声而不丢失图像细节。
- en: The results were validated on the Kodak [[82](#bib.bib82)], McMaster [[83](#bib.bib83)],
    and Urban 100 [[84](#bib.bib84)] datasets and have shown that this model outperforms
    state-of-the-art demosaicking and joint demosaicking and denoising algorithms
    with higher PSNR and SSIM values on all datasets. The results of the visual comparison
    between the methods confirmed the quantitative values achieved, demonstrating
    a better visual quality of the images. However, the average running time of demosaicking
    and denoising of this method did not surpass all the evaluated methods.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果在Kodak [[82](#bib.bib82)]、McMaster [[83](#bib.bib83)]和Urban 100 [[84](#bib.bib84)]
    数据集上进行了验证，并且显示出该模型在所有数据集上都优于最先进的去马赛克和联合去马赛克及去噪算法，具有更高的PSNR和SSIM值。方法之间的视觉比较结果确认了所达到的定量值，显示了更好的图像视觉质量。然而，该方法的去马赛克和去噪的平均运行时间并未超越所有评估的方法。
- en: 3.9 DPN
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9 DPN
- en: This study proposed a duplex pyramid network (DPN [[29](#bib.bib29)]), an efficient
    deep neural network architecture for Quad Bayer CFA demosaicing adopted in submicron
    sensors.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种双层金字塔网络（DPN [[29](#bib.bib29)]），这是一个高效的深度神经网络架构，用于亚微米传感器中的Quad Bayer
    CFA去马赛克。
- en: The article delivers an accurate background in respect to the ISP state-of-the-art
    challenge and a helpful Quad Bayer CFA Analysis, referencing related Deep Learning
    based solutions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 该文章提供了有关ISP最先进挑战的准确背景和有用的Quad Bayer CFA分析，参考了相关的深度学习解决方案。
- en: The proposed architecture consisted of two connected feature map pyramids. One
    of them is composed of downscaling blocks and the other is composed of upscaling
    blocks, combined with dense skip connections.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的架构由两个连接的特征图金字塔组成。其中一个由下采样块组成，另一个由上采样块组成，并结合了密集的跳过连接。
- en: As well as the skip connections, inspired by U-Net [[85](#bib.bib85)], the given
    network also applied residual learning, inspired by ResNet [[73](#bib.bib73)].
    DPN also implemented a Linear Feature Map Growth. Compared to the traditional
    exponential method, this linear method led to a fewer number of parameters, which
    is more precise for mobile applications with limited memory constraints.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 除了跳过连接，受U-Net [[85](#bib.bib85)]启发，给定的网络还应用了受ResNet [[73](#bib.bib73)]启发的残差学习。DPN还实现了线性特征图增长。与传统的指数方法相比，这种线性方法导致了更少的参数，对于具有有限内存限制的移动应用更为精确。
- en: In the results topic, the study brought a comparison against conventional ISP
    algorithm implemented in Samsung mobile phone, observing improvement in sharpness,
    color moiré, edges, texture preservation, and visual artifacts reduction. DPN
    achieved better CPSNR values when compared with other Deep Learning based methods
    at that frame reference. As a limitation in the proposed network architecture,
    the input image width and height must be a multiple of $2^{(L+1)}$, where "L"
    is the resolution level. Otherwise, the input must be cropped before the downscaling
    blocks. Furthermore, the study could have done tests with larger resolution images
    which are also captured in mobile phones of that 2019 frame reference, like FullHD
    and 4K images.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果部分，该研究对比了在三星手机中实现的传统 ISP 算法，观察到在锐度、色彩摩尔纹、边缘、纹理保留和视觉伪影减少方面的改善。在该帧参考中，DPN 在与其他深度学习方法的比较中取得了更好的
    CPSNR 值。作为提议的网络架构中的一个限制，输入图像的宽度和高度必须是 $2^{(L+1)}$ 的倍数，其中 "L" 是分辨率级别。否则，输入必须在下采样块之前进行裁剪。此外，该研究本可以使用更大分辨率的图像进行测试，这些图像也在
    2019 年的手机中捕获，如 FullHD 和 4K 图像。
- en: 3.10 CycleISP
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.10 CycleISP
- en: Zamir [[30](#bib.bib30)] Et al. proposed the CycleISP, a framework that models
    camera imaging pipeline and produces realistic image pairs for denoising both
    in RAW and sRGB spaces. The authors trained a new image denoising network on synthetic
    data and achieved state-of-the-art performance on real camera benchmark datasets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Zamir [[30](#bib.bib30)] 等人提出了 CycleISP，这是一个建模相机成像管道并生成真实图像对以进行 RAW 和 sRGB 空间去噪的框架。作者在合成数据上训练了一种新的图像去噪网络，并在真实相机基准数据集上达到了最先进的性能。
- en: CycleISP is a compound of two stages. First, the framework models the camera
    ISP in forward and reverse directions. Second, it synthesizes realistic noise
    datasets for the RAW and sRGB images denoising tasks. The CycleISP model introduces
    the RGB2RAW network branch, the RAW2RGB network branch, an auxiliary color correction
    network branch, and a noise injection module. The RGB2RAW and RAW2RGB modules
    are trained independently, followed by a joint fine-tuning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: CycleISP 由两个阶段组成。首先，该框架在正向和反向方向上建模相机 ISP。其次，它为 RAW 和 sRGB 图像去噪任务合成真实的噪声数据集。CycleISP
    模型引入了 RGB2RAW 网络分支、RAW2RGB 网络分支、辅助颜色校正网络分支和噪声注入模块。RGB2RAW 和 RAW2RGB 模块独立训练，然后进行联合微调。
- en: The RGB2RAW branch converts sRGB images to RAW data without requiring any camera
    parameters. This module network is composed of convolutional layers, proposed
    recursive residual groups, dual attention blocks, and a final Bayer sampling function,
    generating a mosaicked RAW output. The RAW2RGB network maps clean RAW images to
    clean sRGB images. First, the noise injection module is set as ’OFF’, followed
    by a 2x2 block packaging into four channels (RGGB) and an image resolution reduction
    block. To ensure that the input RAW data may come from any camera and have different
    Bayer patterns, the RAW2RGB branch applies the Bayer pattern unification technique [[26](#bib.bib26)].
    Next, a convolutional layer and a proposed recursive residual group encode the
    packed RAW image into a deep feature tensor. Additionally, the authors proposed
    a color correction branch to the RAW2RGB network, which receives an sRGB image
    input and generates a color-encoded deep feature tensor.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: RGB2RAW 分支将 sRGB 图像转换为 RAW 数据，而无需任何相机参数。此模块网络由卷积层、提议的递归残差组、双重注意力块和最终的 Bayer
    采样函数组成，生成马赛克 RAW 输出。RAW2RGB 网络将干净的 RAW 图像映射到干净的 sRGB 图像。首先，噪声注入模块设置为 'OFF'，然后将
    2x2 块打包成四个通道（RGGB）和一个图像分辨率降低块。为了确保输入 RAW 数据可能来自任何相机并具有不同的 Bayer 图案，RAW2RGB 分支应用了
    Bayer 图案统一技术 [[26](#bib.bib26)]。接下来，卷积层和提议的递归残差组将打包的 RAW 图像编码为深度特征张量。此外，作者向 RAW2RGB
    网络提出了一个颜色校正分支，该分支接收 sRGB 图像输入并生成颜色编码的深度特征张量。
- en: Furthermore, there is a Joint Fine-Tuning that provides optimal-quality images.
    For this, the RGB2RAW’s output becomes the RAW2RGB’s input, and the RGB2RAW branch
    receives gradients from both sub-losses, reconstructing the final sRGB image.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个联合微调，提供最佳质量的图像。为此，RGB2RAW 的输出成为 RAW2RGB 的输入，RGB2RAW 分支接收来自两个子损失的梯度，从而重建最终的
    sRGB 图像。
- en: To synthesize realistic noise image pairs for denoising in RAW space, the noise
    injection module is turned ’ON’ and includes shot and read noise of different
    levels to the RGB2RAW’s output. After this, CycleISP can generate a clean and
    its noisy image pair from any sRGB image. For the sRGB space, the CycleISP model
    is fine-tuned with the SIDD [[76](#bib.bib76)] dataset, which contains clean and
    noisy image pairs in both RAW and sRGB spaces.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 RAW 空间合成逼真的噪声图像对，噪声注入模块被设置为“开启”，并在 RGB2RAW 输出中包括不同水平的拍摄和读取噪声。之后，CycleISP
    可以从任何 sRGB 图像生成干净的及其噪声图像对。对于 sRGB 空间，CycleISP 模型使用 SIDD [[76](#bib.bib76)] 数据集进行微调，该数据集包含
    RAW 和 sRGB 空间中的干净和噪声图像对。
- en: For the training stage, the authors used the MIT-Adobe FiveK dataset [[69](#bib.bib69)],
    followed by the Fine-Tuning. They evaluated CycleISP performance with state-of-the-art
    RAW and sRGB denoising methods, using the DND [[86](#bib.bib86)] and SIDD[[76](#bib.bib76)]
    benchmarks. CycleISP achieved better results in both scenarios, with almost a
    5 times smaller network parameters number than the previous best RAW denoising
    method[[58](#bib.bib58)] and performance gain against the previous best sRGB denoising
    algorithm[[57](#bib.bib57)]. Besides that, in contrast to the other evaluated
    models, the proposed method provided clean and artifact-free results, also preserving
    image details. The proposed framework modules were appropriately described and
    reinforced with an ablation study. The authors provide a decent implementation
    details section, a suitable comparison with related methods, and a solid generalization
    capability study. As a negative aspect, the paper could show more details at the
    ablation study, giving more information about individuals contributions of the
    other CycleISP modules aside from the RAW2RGB branch.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练阶段，作者使用了 MIT-Adobe FiveK 数据集 [[69](#bib.bib69)]，随后进行了微调。他们使用 DND [[86](#bib.bib86)]
    和 SIDD [[76](#bib.bib76)] 基准测试评估了 CycleISP 的性能，比较了最先进的 RAW 和 sRGB 去噪方法。CycleISP
    在这两种情况下都取得了更好的结果，其网络参数数量几乎比之前最好的 RAW 去噪方法 [[58](#bib.bib58)] 小了 5 倍，并且在性能上超越了之前最好的
    sRGB 去噪算法 [[57](#bib.bib57)]。此外，与其他评估的模型相比，所提出的方法提供了干净且无伪影的结果，同时保留了图像细节。提出的框架模块得到了适当描述，并通过消融研究得到了强化。作者提供了详细的实现细节部分，与相关方法的比较也很合适，并进行了坚实的泛化能力研究。作为负面方面，论文在消融研究中可以展示更多细节，提供更多关于其他
    CycleISP 模块的个体贡献的信息，除了 RAW2RGB 分支外。
- en: 3.11 PyNET
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.11 PyNET
- en: PyNET [[31](#bib.bib31)] is a novel pyramidal CNN architecture designed to replace
    the entire ISP pipeline present in smartphones. The proposed method has an inverted
    pyramidal shape and was composed of five different levels trained from bottom
    to top where each level is trained sequentially with the trained output being
    used in the above level training stage. The convolutional filters size in this
    method varies from 3x3 at level five up to 9x9 at level one. Therefore, lower
    levels learn global image manipulation while higher levels learn to reconstruct
    the final image recovering the missing details at lower levels. The network is
    trained using three different loss functions combinations. The lowest levels,
    four and five, are trained with mean squared error (MSE) to learn global color
    and brightness correction. Levels two and three are trained by a combination of
    MSE and VGG-based[[87](#bib.bib87)] to refine the color and shape of objects.
    Finally, level one is trained with MSE, VGG, and SSIM loss[[88](#bib.bib88)] and
    performs corrections in the local color processing, noise removal, texture enhancement,
    etc.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: PyNET [[31](#bib.bib31)] 是一种新颖的金字塔 CNN 架构，旨在替代智能手机中存在的整个 ISP 流水线。所提出的方法具有倒金字塔形状，由五个不同的层级组成，从下到上进行训练，每个层级的训练输出用于上层的训练阶段。该方法中的卷积滤波器大小从第五层的
    3x3 到第一层的 9x9 不等。因此，较低层级学习全局图像处理，而较高层级则学习重建最终图像，恢复较低层级缺失的细节。网络使用三种不同的损失函数组合进行训练。最低层级，即四层和五层，使用均方误差
    (MSE) 进行训练，以学习全局颜色和亮度校正。第二层和第三层则通过 MSE 和基于 VGG 的方法 [[87](#bib.bib87)] 的组合进行训练，以细化对象的颜色和形状。最后，第一层使用
    MSE、VGG 和 SSIM 损失 [[88](#bib.bib88)] 进行训练，并进行局部颜色处理、噪声去除、纹理增强等校正。
- en: Additionally, the authors present the Zurich RAW to RGB dataset, composed of
    20 thousand RAW-RGB image pairs where the RAW images are captured using Huawei
    P20 smartphone and the RGB images are captured using a professional high-end Canon
    5D Mark IV camera.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者还展示了 Zurich RAW 到 RGB 数据集，包含 2 万对 RAW-RGB 图像，其中 RAW 图像使用华为 P20 智能手机拍摄，RGB
    图像则使用专业的高端 Canon 5D Mark IV 相机拍摄。
- en: To evaluate the method, three experiments are conducted. The first, compared
    PyNET with the methods SPADE [[89](#bib.bib89)], DPED [[90](#bib.bib90)], U-Net [[85](#bib.bib85)],
    Pix2Pix [[91](#bib.bib91)], SRGAN [[92](#bib.bib92)], VDSR [[93](#bib.bib93)],
    and SRCNN [[94](#bib.bib94)]. In this comparison, PyNET outperformed all other
    methods in the PSNR and MS-SSIM metric values. The second experiment measures
    the quality of the generated images using the Amazon Mechanical Turk³³3https://www.mturk.com
    platform, compared the images of PyNET, Visualized RAW, and Huawei P20 ISP with
    the images produced by the Canon 5D Mark IV DSLR camera. In this comparison, the
    image produced by PyNET reached the better MOS result in comparison to the target
    DSLR camera. Finally, the last experiment tested the PyNET using RAW images captured
    by the BlackBerry KeyOne smartphone without re-training the network. The image
    produced by PyNET was compared with the image produced by BlackBerry’s ISP, and
    PyNET generated good results, however, the results were not ideal in terms of
    exposure and sharpness.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估该方法，进行了三个实验。第一个实验中，将PyNET与SPADE [[89](#bib.bib89)]、DPED [[90](#bib.bib90)]、U-Net
    [[85](#bib.bib85)]、Pix2Pix [[91](#bib.bib91)]、SRGAN [[92](#bib.bib92)]、VDSR [[93](#bib.bib93)]和SRCNN
    [[94](#bib.bib94)]进行了比较。在这次比较中，PyNET在PSNR和MS-SSIM指标值上表现优于其他所有方法。第二个实验使用亚马逊机械土耳其人平台[[3](https://www.mturk.com)]测量生成图像的质量，将PyNET、Visualized
    RAW和华为P20 ISP的图像与佳能5D Mark IV单反相机拍摄的图像进行了比较。在这次比较中，PyNET生成的图像在与目标单反相机的比较中达到了更好的MOS结果。最后一个实验测试了使用BlackBerry
    KeyOne智能手机捕捉的RAW图像的PyNET，无需重新训练网络。PyNET生成的图像与BlackBerry的ISP生成的图像进行了比较，PyNET生成了不错的结果，但在曝光和清晰度方面的结果并不理想。
- en: 3.12 SGNet
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.12 SGNet
- en: Many methods have joined highly correlated tasks have success, decreasing the
    accumulated error in the several process units in the ISP pipeline. Thus, the
    SGNet[[33](#bib.bib33)] joins demosaicing and denoising in a unique network.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法已经成功地加入了高度相关的任务，减少了ISP管道中多个处理单元的累积误差。因此，SGNet [[33](#bib.bib33)]将去马赛克和去噪合并到一个独特的网络中。
- en: As the correction of high-frequency regions in images is more complicated, the
    authors propose extracting a density map representing the frequency of areas of
    the picture. This density map can help the network know each region’s difficulty
    level and adapt better than other models in areas with high frequency. Furthermore,
    half of the Bayer pattern comprises green pixels; subsequently, it is easier to
    recover the missing pixels from this channel. For this reason, the network has
    a branch to reconstruct the green channel, where consequently helps reconstruct
    other channels. Besides, SGNet uses the Residual-in-Residual Dense Block (RRDB)
    to feature extraction in both branches. Additionally, this network was trained
    in a set of loss functions, which consider the reconstruction fidelity of the
    green channel, full image, the objects, textures edges, and noise removal.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像中高频区域的修正更为复杂，作者提出提取一个表示图像区域频率的密度图。这个密度图可以帮助网络了解每个区域的难度水平，并在高频区域比其他模型更好地适应。此外，Bayer模式的一半由绿色像素组成；因此，从这个通道恢复缺失像素更容易。为此，网络有一个分支用于重建绿色通道，从而有助于重建其他通道。此外，SGNet在两个分支中使用了Residual-in-Residual
    Dense Block (RRDB)进行特征提取。此外，该网络通过一组损失函数进行训练，考虑了绿色通道、完整图像、物体、纹理边缘和噪声去除的重建保真度。
- en: SGNet outperforms state-of-the-art methods in terms of PSNR and SSIM in datasets
    aimed at the super-resolution, denoising, and demosaicing tasks. Furthermore,
    compared with the ADMN[[59](#bib.bib59)], CDM[[95](#bib.bib95)], Kokkinos[[96](#bib.bib96)],
    Deepjoint⁴⁴4Method from paper Deep Joint Demosaicking and Denoising [[60](#bib.bib60)],
    called by [[33](#bib.bib33), [46](#bib.bib46)] as Deepjoint, by[[41](#bib.bib41)]
    as DemosaicNet, and by[[34](#bib.bib34)] as DeepJDD. [[60](#bib.bib60)], and FlexISP[[97](#bib.bib97)],
    the SGNet can remove moiré artifacts more effectively and give images with more
    definition in high-frequency areas than ADMN and Deepjoint. However, as a negative
    point, this work did not show a preoccupation with computational efficiency, which
    is essential to applications that use demosaicing and denoising.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: SGNet 在超分辨率、去噪和去马赛克任务的数据集中，在 PSNR 和 SSIM 方面表现优于最先进的方法。此外，与 ADMN[[59](#bib.bib59)]、CDM[[95](#bib.bib95)]、Kokkinos[[96](#bib.bib96)]、论文《Deep
    Joint Demosaicking and Denoising》中的 Deepjoint⁴⁴4Method[[60](#bib.bib60)]（由[[33](#bib.bib33),
    [46](#bib.bib46)] 称为 Deepjoint，由[[41](#bib.bib41)] 称为 DemosaicNet，和[[34](#bib.bib34)]
    称为 DeepJDD）、以及 FlexISP[[97](#bib.bib97)] 相比，SGNet 能够更有效地去除摩尔纹伪影，并在高频区域提供更高的图像定义。然而，作为负面点，本工作未展示对计算效率的关注，而计算效率对使用去马赛克和去噪的应用至关重要。
- en: 3.13 PatchNet and RestoreNet
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.13 PatchNet 和 RestoreNet
- en: In this paper [[34](#bib.bib34)], the authors proposed a method based on active
    learning (data-driven) that learns to select the most suitable patches from an
    image for the training step without adding additional cost to the inference step.
    To do this, the method, called PatchNet, assigns a weight to each patch that defines
    whether it will be used or ignored during training. This method is a feed-forward
    network with multiple stages where each stage is composed of several convolutions
    blocks and a down-sampling operator. Then gradually the stages transform the image
    into a set of trainability scalars that are finally binarized to obtain the network
    output.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文[[34](#bib.bib34)]中，作者提出了一种基于主动学习（数据驱动）的方法，该方法学习选择最适合训练步骤的图像补丁，而不会增加推断步骤的额外成本。为此，该方法称为
    PatchNet，为每个补丁分配一个权重，以定义在训练过程中是使用还是忽略该补丁。该方法是一个前馈网络，具有多个阶段，每个阶段由多个卷积块和一个下采样操作符组成。然后逐渐将图像转换为一组可训练的标量，最终二值化以获得网络输出。
- en: In addition to PatchNet, the authors also proposed RestoreNet, an architecture
    that applies the structural knowledge extracted from PatchNet and is responsible
    for restoring the original image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 PatchNet，作者还提出了 RestoreNet，这是一种应用从 PatchNet 提取的结构知识并负责恢复原始图像的架构。
- en: 'The results were validated on the Vimeo-90k [[98](#bib.bib98)], MIT Moire[[60](#bib.bib60)],
    and Urban 100 [[84](#bib.bib84)] datasets and compared with the Kokkinos[[96](#bib.bib96)],
    SGNet [[33](#bib.bib33)], CDM [[95](#bib.bib95)], and DeepJDD^†^†footnotemark:
     [[60](#bib.bib60)] methods. The methods were compared at three different noise
    levels (5, 15, 25). In all comparisons, the proposed method achieved better PSRN
    values in the JDD task.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '结果在 Vimeo-90k[[98](#bib.bib98)]、MIT Moire[[60](#bib.bib60)] 和 Urban 100[[84](#bib.bib84)]
    数据集上进行了验证，并与 Kokkinos[[96](#bib.bib96)]、SGNet[[33](#bib.bib33)]、CDM[[95](#bib.bib95)]
    和 DeepJDD^†^†footnotemark: [[60](#bib.bib60)] 方法进行了比较。方法在三种不同噪声水平（5、15、25）下进行比较。在所有比较中，所提出的方法在
    JDD 任务中取得了更好的 PSRN 值。'
- en: The ablation studies analyzed the effects of different patch sizes when PatchNet
    is evaluated on Demosaicing and shown that performance improves as patch size
    increases. It would be interesting to study the computational costs involved in
    increasing patch sizes and how much this would change the network’s complexity.
    The study continued with experiments of PatchNet on JDD and compared it with the
    methods mentioned above. Only the PSNR metric was used for comparison, but it
    would have been interesting to evaluate other metrics.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究分析了在对 Demosaicing 进行 PatchNet 评估时不同补丁大小的效果，并显示随着补丁大小的增加，性能有所提高。研究增加补丁大小的计算成本及其对网络复杂性的影响将是有趣的。该研究继续对
    PatchNet 进行了 JDD 实验，并与上述方法进行了比较。只使用了 PSNR 指标进行比较，但评估其他指标会更有趣。
- en: 3.14 AWNet
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.14 AWNet
- en: In this work [[35](#bib.bib35)], the researchers propose a method capable of
    enhancing smartphone generated images by replacing the base ISP by a U-Net [[85](#bib.bib85)]
    resembled CNN (Convolutional Neural Network), called AWNet.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作[[35](#bib.bib35)]中，研究人员提出了一种通过用类似 CNN（卷积神经网络）的 U-Net[[85](#bib.bib85)]
    替换基础 ISP 来增强智能手机生成图像的方法，称为 AWNet。
- en: The network is divided into two branches, each using different inputs and, thus,
    different models. The first branch, using the RAW model, receives 224 x 244 x
    4 RAW images and the second branch, using the demosaiced model, receives 448 x
    448 x 3 demosaiced images. Both branches are trained separetely and the results
    are averaged during test.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 网络被分为两个分支，每个分支使用不同的输入，因此使用不同的模型。第一个分支使用RAW模型，接收224 x 244 x 4 RAW图像，第二个分支使用去马赛克模型，接收448
    x 448 x 3去马赛克图像。两个分支分别训练，在测试期间结果取平均。
- en: 'The structure of this network, following the U-Net, applies three main modules
    to the inputs, for each branch: global context res-dense, residual wavelet up-sampling
    and residual wavelet down-sampling. The res-dense module is applied to extract
    the low frequency components after the discrete wavelet transform (DWT), which
    are sent to the layer below, while the down-sampling extract all the components.
    After the extraction, both sets of components for each layer are up-sampled and
    concatenated with the layer above. Finally, a pyramid pooling module is applied,
    generating the output for that branch.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 该网络的结构遵循U-Net，为每个分支应用三个主要模块：全局上下文残差密集模块、残差小波上采样模块和残差小波下采样模块。res-dense模块用于在离散小波变换（DWT）后提取低频组件，这些组件被发送到下层，而下采样则提取所有组件。提取后，每层的两个组件集都进行上采样并与上一层进行拼接。最后，应用了一个金字塔池化模块，为该分支生成输出。
- en: For the test phase, a self-ensemble mechanism was applied, made up of 8 ensemble
    variants. Those variants where, then, evaluated using the PSNR (dB) values, which
    would be used as weights to generate the final predictions of the model. The chosen
    PSNR were 21.36 dB for the RAW model and 21.52 dB for the demosaiced model. By
    applying this tunned model to the two tracks of the Zurich dataset from AIM 2020
    Learned Smartphone ISP Challenge [[99](#bib.bib99)], the study reached the 5th
    and 2nd positions, respectivelly.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试阶段，应用了一个自集成机制，由8个集成变体组成。这些变体随后使用PSNR（dB）值进行评估，这些值将作为权重生成模型的最终预测。选择的PSNR值为RAW模型的21.36
    dB和去马赛克模型的21.52 dB。通过将这个调优后的模型应用于AIM 2020 Learned Smartphone ISP Challenge [[99](#bib.bib99)]中的Zurich数据集的两个轨道，该研究分别达到了第5和第2的位置。
- en: Using the results as a justification for the use of the wavelet transform and
    the global context blocks, the researchers compared the results of AWNet against
    other popular network architectures, like U-Net, RCAN [[67](#bib.bib67)] and PyNet[[31](#bib.bib31)],
    with the use of the ZRR dataset. By comparing their performance, the researches
    found that AWNet is able to outperform U-Net, RCAN and the current state of the
    art, PyNet.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 利用结果作为使用小波变换和全局上下文块的理由，研究人员将AWNet的结果与其他流行的网络架构进行比较，如U-Net、RCAN [[67](#bib.bib67)]和PyNet[[31](#bib.bib31)]，使用ZRR数据集。通过比较它们的性能，研究发现AWNet能够超越U-Net、RCAN和当前最先进的PyNet。
- en: 3.15 PyNET-CA
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.15 PyNET-CA
- en: PyNET-CA is an end-to-end mobile ISP deep learning algorithm for RAW to RGB
    reconstruction [[32](#bib.bib32)]. This network improves PyNET [[31](#bib.bib31)]
    performance by adding channel attention and subpixel reconstruction modules and
    decreasing training time. PyNET-CA has an invertible pyramidal structure for considering
    the local and global features of the image. The Basic modules of PyNET-CA are
    the channel attention module based on[[67](#bib.bib67)], the DoubleConv module,
    which has two operations of 2D convolution with a LeakyReLU activation, and the
    MultiConv channel attention module, which concatenates the features from the DoubleConv
    modules and a channel attention module.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: PyNET-CA是一个端到端的移动ISP深度学习算法，用于RAW到RGB的重建 [[32](#bib.bib32)]。该网络通过添加通道注意和子像素重建模块并减少训练时间来提高PyNET [[31](#bib.bib31)]的性能。PyNET-CA具有可逆的金字塔结构，以考虑图像的局部和全局特征。PyNET-CA的基本模块包括基于[[67](#bib.bib67)]的通道注意模块、具有两个2D卷积操作的DoubleConv模块（使用LeakyReLU激活）以及多通道注意模块，后者将DoubleConv模块的特征与通道注意模块的特征进行拼接。
- en: The superpixel reconstruction module helps the network reconstruct the final
    image with quality and better computational efficiency. For this, PyNET-CA upsamples
    the image with the MultiConv channel attention module, followed by a 1×1 convolution
    layer and upsamples the features by subpixel shuffling at the final level of the
    model. The results were presented on Zurich Dataset [[31](#bib.bib31)] where it
    has shown better PSNR and SSIM values when compared to PyNET[[31](#bib.bib31)].
    This paper did not present the number of network parameters and although the authors
    cite the decrease in training time, a table comparing these results to PyNET [[31](#bib.bib31)]
    was not presented.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 超像素重建模块帮助网络以更高质量和更好的计算效率重建最终图像。为此，PyNET-CA 使用 MultiConv 通道注意力模块对图像进行上采样，接着通过
    1×1 卷积层和在模型的最终层通过子像素混洗上采样特征。结果展示在 Zurich Dataset [[31](#bib.bib31)] 上，与 PyNET
    [[31](#bib.bib31)] 相比，显示了更好的 PSNR 和 SSIM 值。本文未提供网络参数数量，虽然作者提到训练时间的减少，但未提供与 PyNET
    [[31](#bib.bib31)] 结果对比的表格。
- en: 3.16 Del-Net
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.16 Del-Net
- en: Del-Net [[36](#bib.bib36)] is a single-stage end-to-end deep learning model
    that learns the entire ISP pipeline to convert RAW Bayer data to high-quality
    sRGB-image. This network uses a combination of Spatial and Channel Attention blocks
    (modified UNet)[[100](#bib.bib100)] and Enhancement Attention Modules blocks[[57](#bib.bib57)].
    The Spatial and Channel Attention blocks allow the network to capture global details
    both spatial-wise and channel-wise, therefore helping with color enhancement.
    The Enhancement Attention Modules blocks help in denoising, which improves the
    PSNR value. The images generated by Del-Net are visually comparable to the state-of-the-art
    networks (PyNET [[31](#bib.bib31)], AWNet [[35](#bib.bib35)], and MW-ISPNet [[99](#bib.bib99)])
    when considering the color enhancement, denoising, and detail retention capabilities
    while presenting a reduction in Mult-Adds (Number of composite multiply-accumulate
    operations for an image). Altogether, this makes the network ideal for smartphone
    deployment. It also has a competitive trade-off between accuracy metrics and complexity.
    The results were presented on Zurich Dataset [[31](#bib.bib31)] where it has shown
    better detail retention compared to PyNET[[31](#bib.bib31)], better denoising
    compared to MW-ISPNet ignatov2020aim, and better colour enhancement compared to
    AWNet [[35](#bib.bib35)]. Although, the detail recovery capability of Del-Net
    is inferior to that of MW-ISPNet [[99](#bib.bib99)].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Del-Net [[36](#bib.bib36)] 是一个单阶段端到端深度学习模型，学习整个 ISP 流水线，将 RAW Bayer 数据转换为高质量的
    sRGB 图像。该网络使用空间和通道注意力块（修改版 UNet）[[100](#bib.bib100)] 和增强注意力模块块[[57](#bib.bib57)]
    的组合。空间和通道注意力块允许网络在空间和通道层面捕捉全局细节，从而帮助色彩增强。增强注意力模块块有助于去噪，从而提高 PSNR 值。Del-Net 生成的图像在色彩增强、去噪和细节保留能力方面与最先进的网络（PyNET
    [[31](#bib.bib31)]、AWNet [[35](#bib.bib35)] 和 MW-ISPNet [[99](#bib.bib99)]) 相当，同时在
    Multi-Adds（图像的复合乘加运算次数）方面有所减少。总体而言，这使得该网络非常适合智能手机部署。它在准确性指标和复杂性之间有一个具有竞争力的折中。结果展示在
    Zurich Dataset [[31](#bib.bib31)] 上，显示了与 PyNET [[31](#bib.bib31)] 相比更好的细节保留，与
    MW-ISPNet ignatov2020aim 相比更好的去噪，与 AWNet [[35](#bib.bib35)] 相比更好的色彩增强。虽然，Del-Net
    的细节恢复能力不如 MW-ISPNet [[99](#bib.bib99)]。
- en: 3.17 InvISP
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.17 InvISP
- en: InvISP [[37](#bib.bib37)] redesigns the ISP pipeline allowing the reconstruction
    of RAW images almost identical to camera RAW images without any memory overhead
    and also generates human-pleasing sRGB images like traditional ISPs. This is interesting
    since end users can only access processed sRGB images because RAW images are too
    large to store on devices. The reconstruction of RAW images in this method is
    done by the compression of RGB images with the inverse process. To achieve this
    goal, the authors designed a RAW-to-RGB and RGB-to-RAW mapping from an invertible
    neural network consisting of a stack of affine coupling layers and an invertible
    1x1 convolution. In addition, a differentiable JPEG compression simulator was
    integrated into the model, allowing the reconstruction of near-perfect RAW images
    from JPEG images by Fourier series expansion. The network was trained bidirectionally
    to jointly optimize the RGB and RAW reconstruction process. Model evaluation was
    performed on the Canon EOS 5D subset and Nikon D700 subset from the MIT-Adobe
    FiveK dataset[[69](#bib.bib69)]. To render the ground truth of sRGB images from
    RAW images, the LibRAW library was used, which allows simulation of the steps
    of an ISP pipeline. The experiments demonstrated an improvement of PSNR over the
    RAW synthesizing methods UPI  [[58](#bib.bib58)] and CycleISP[[30](#bib.bib30)],
    which implies a more accurate retrieval of RAW images. The method was compared
    to Invertible Grayscale [[101](#bib.bib101)] and U-net[[71](#bib.bib71)] baselines
    and the results showed better PSNR and SSIM values, indicating a more robust model
    for RAW image retrieval and RGB image generation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: InvISP [[37](#bib.bib37)] 重新设计了ISP管道，使得RAW图像的重建几乎与相机RAW图像完全相同，且不会产生任何内存开销，同时生成类似于传统ISP的人眼悦目的sRGB图像。这一点很有趣，因为最终用户只能访问处理后的sRGB图像，因为RAW图像过大无法存储在设备上。这种方法中的RAW图像重建是通过RGB图像的逆过程压缩实现的。为实现这一目标，作者设计了一个从可逆神经网络（由一系列仿射耦合层和可逆1x1卷积组成）的RAW到RGB和RGB到RAW映射。此外，将一个可微分的JPEG压缩模拟器集成到模型中，使得通过傅里叶级数扩展可以从JPEG图像中重建几乎完美的RAW图像。网络进行了双向训练，以共同优化RGB和RAW重建过程。模型评估在MIT-Adobe
    FiveK数据集中的Canon EOS 5D子集和Nikon D700子集上进行[[69](#bib.bib69)]。为了从RAW图像渲染sRGB图像的真实情况，使用了LibRAW库，它允许模拟ISP管道的步骤。实验显示，相比RAW合成方法UPI
    [[58](#bib.bib58)]和CycleISP [[30](#bib.bib30)]，PSNR有所提高，这意味着RAW图像的检索更加准确。该方法与可逆灰度方法[[101](#bib.bib101)]和U-net[[71](#bib.bib71)]基线进行了比较，结果显示了更好的PSNR和SSIM值，表明模型在RAW图像检索和RGB图像生成方面更具鲁棒性。
- en: 3.18 ICDC-Net
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.18 ICDC-Net
- en: In this paper, the authors have proposed an ISP-Net that addresses JPEG image
    compression in network training  [[38](#bib.bib38)], which we called here ICDC-Net.
    The fact that images can lose information in the compression process has not been
    addressed on previously ISP pipelines with convolutional neural networks.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，作者提出了一种ISP-Net，解决了网络训练中的JPEG图像压缩问题[[38](#bib.bib38)]，我们在此称之为ICDC-Net。此前的ISP管道中，卷积神经网络并未解决图像在压缩过程中可能丢失的信息问题。
- en: To this end, the authors applied a fully convolutional compression artifacts
    simulation network (CAS-Net). This network can add JPEG compression artifacts
    to an image, and it is trained by inverting the inputs and outputs needed for
    training compression artifact reduction networks. In this work, the authors connected
    the CAS-Net to an ISP network, so the ISP network can be trained with consideration
    to image compression, taking compression artifacts into account. The ISP-Net used
    in this work was U-Net with channel attention module [[102](#bib.bib102)] and
    the architecture of CAS-Net was U-Net[[85](#bib.bib85)] without channel attention
    module.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，作者应用了一个完全卷积的压缩伪影模拟网络（CAS-Net）。该网络可以将JPEG压缩伪影添加到图像中，并通过反转训练压缩伪影减少网络所需的输入和输出进行训练。在这项工作中，作者将CAS-Net与ISP网络连接，以便在考虑图像压缩时训练ISP网络，将压缩伪影纳入考虑。本文中使用的ISP-Net是带有通道注意模块的U-Net[[102](#bib.bib102)]，而CAS-Net的架构是没有通道注意模块的U-Net[[85](#bib.bib85)]。
- en: Results are present on the Nikon D700 subset from the MIT-Adobe FiveK dataset [[69](#bib.bib69)].
    To render the ground truth of sRGB images from RAW images, the LibRaw library
    was used. The sRGB images were compressed with two different QFs, 80 and 90, and
    each model was trained separately. The experimental results have shown that this
    proposed network can produce better quality images when compared to its compression
    agnostic version.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果展示在来自 MIT-Adobe FiveK 数据集的 Nikon D700 子集上[[69](#bib.bib69)]。为了从 RAW 图像渲染 sRGB
    图像的真实情况，使用了 LibRaw 库。sRGB 图像分别以两个不同的 QF 进行压缩，80 和 90，并且每个模型都是单独训练的。实验结果表明，所提议的网络在图像质量上优于其对压缩不敏感的版本。
- en: 3.19 CSANet
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.19 CSANet
- en: 'As the second place in the Mobile AI 2021 Learned Smartphone ISP Challenge[[39](#bib.bib39)]
    and first place in PSNR score, the Channel Spatial Attention Network (CSANet)[[103](#bib.bib103)]
    is a network that aims at computational performance and the quality of final image
    results, inferred at most 90.8 ms per image. The network has three main parts:
    downscale, processing blocks cascaded, and upscale. In the first part, to reduce
    the computational time and number of parameters, the authors did a downscale with
    a strided convolution block following a conventional convolution to feature extraction.
    Sequentially, the network has a Double Attention Module (DAM) inspired by the
    Convolutional Block Attention Module (CBAM)[[104](#bib.bib104)]. The DAM comprises
    a spatial attention module to learn spatial dependencies in the feature maps and
    a channel attention module to learn the inter-channel relationship of features
    maps. And in the last part has the convolution transpose and depth to space to
    upscale to a final RGB image. Another essential part of this work is the loss
    function composed by the Charbonnier loss[[67](#bib.bib67)], the SSIM loss, and
    the Perceptual loss to decrease the perceptual difference between the generated
    image and the ground truth.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 Mobile AI 2021 学习型智能手机 ISP 挑战赛[[39](#bib.bib39)]的第二名和 PSNR 得分第一名，通道空间注意力网络（CSANet）[[103](#bib.bib103)]
    旨在提高计算性能和最终图像结果的质量，每张图像推理最多为 90.8 毫秒。该网络有三个主要部分：降采样、处理块级联和上采样。在第一部分，为了减少计算时间和参数数量，作者使用了带有步幅卷积块的降采样，随后是常规卷积用于特征提取。接着，网络有一个受卷积块注意力模块（CBAM）[[104](#bib.bib104)]
    启发的双重注意力模块（DAM）。DAM 包括一个空间注意力模块来学习特征图中的空间依赖关系和一个通道注意力模块来学习特征图的通道间关系。最后部分则通过卷积转置和深度到空间的方式进行上采样，生成最终的
    RGB 图像。这项工作另一个重要部分是损失函数，由 Charbonnier 损失[[67](#bib.bib67)]、SSIM 损失和感知损失组成，用于减少生成图像与真实图像之间的感知差异。
- en: In quantitative metrics on the validation dataset of Mobile AI 2021 Learned
    Smartphone ISP Challenge, the CSANet outperforms the PUNet, the baseline model
    for this Challenge, and has a shorter runtime. Also, it was comparable results
    and better inference time than the AWNet [[35](#bib.bib35)], a network with a
    high score in the AIM 2020\. Compared with other candidates in the Challenge,
    this work was ranked second with satisfactory runtime e highest quality score.
    The CSANet can be used in embedded systems as the tests in smartphones showed.
    Besides, as expected, the use of the modules of attention helped in better color
    mapping.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mobile AI 2021 学习型智能手机 ISP 挑战赛的验证数据集上的量化指标中，CSANet 超越了该挑战赛的基线模型 PUNet，并且运行时间更短。此外，它的结果与
    AWNet [[35](#bib.bib35)] 相当，但推理时间更好，AWNet 在 AIM 2020 中得分较高。与挑战赛中的其他候选模型相比，该工作以令人满意的运行时间和最高质量得分排名第二。CSANet
    可以用于嵌入式系统，正如在智能手机测试中所示。此外，正如预期的那样，注意力模块的使用有助于更好的颜色映射。
- en: 3.20 LiteISPNet
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.20 LiteISPNet
- en: In some datasets, the RAW and RGB images are captured with different cameras.
    Consequently, the pairs of images have misalignment and color inconsistency, difficulting
    the training process and producing blurry results. Thinking about this problem,
    Zhang et al. [[40](#bib.bib40)] proposed a method to train the networks with misaligned
    images and map RAW to RGB. The authors used the pre-trained optical flow estimation
    network, PWC-Net [[105](#bib.bib105)], to align the image pairs and designed a
    global color mapping (GCM) to match the color between the input and ’target images
    to facilitate alignment. Besides, the LiteISPNet is responsible for mapping the
    RAW-to-RGB. It simplifies MW-ISPNet [[99](#bib.bib99)], which proposed a U-Net
    based multi-level wavelet ISP network, reducing the number of RCAB in each residual
    group and changing the position of convolutional layer and residual group before
    each wavelet decomposition. These changes decreased the model size and running
    time by approximately 40% and 20%, respectively.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些数据集中，RAW和RGB图像是用不同的相机拍摄的。因此，这些图像对存在错位和颜色不一致，使得训练过程变得困难并产生模糊的结果。考虑到这个问题，Zhang等人[[40](#bib.bib40)]提出了一种方法，用于训练具有错位图像的网络，并将RAW映射到RGB。作者使用了预训练的光流估计网络PWC-Net
    [[105](#bib.bib105)]来对齐图像对，并设计了一个全局颜色映射（GCM），以匹配输入图像与目标图像之间的颜色，以便于对齐。此外，LiteISPNet负责将RAW映射到RGB。它简化了MW-ISPNet
    [[99](#bib.bib99)]，该方法提出了一种基于U-Net的多级小波ISP网络，减少了每个残差组中的RCAB数量，并改变了卷积层和残差组在每个小波分解之前的位置。这些变化将模型的大小和运行时间分别减少了约40%和20%。
- en: 'The authors tested the network in two datasets, the ZRR dataset[[31](#bib.bib31)],
    and the SR-RAW[[106](#bib.bib106)], with two variants of ground truth: the original
    GT and align GT. In the ZRR dataset, the LiteISPNet was compared with three states
    of the art (PyNet[[31](#bib.bib31)], AWNet[[35](#bib.bib35)], and MW-ISPNet) and
    outperformed all metrics on the aligned GT but was a little worse than MW-ISPNet
    in the SSIM metric on the original GT. Moreover, the GAN version of this model
    obtains better perceptual results in the LPIPS metric[[107](#bib.bib107)]. Finally,
    in qualitative comparison, the network could retain more fine details than other
    models. With the SR-RAW dataset, the authors also compared with SR methods, as
    SRGAN[[92](#bib.bib92)], ESRGAN [[108](#bib.bib108)], SPSR [[109](#bib.bib109)],
    and RealSR[[110](#bib.bib110)]. It generates images with less noise, less blurry,
    more details, and it had better scores in almost all metrics, losses only in the
    PSNR metric on original GT, which favors blurred images.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在两个数据集上测试了该网络，ZRR数据集[[31](#bib.bib31)]和SR-RAW[[106](#bib.bib106)]，有两个真实值变体：原始GT和对齐GT。在ZRR数据集中，LiteISPNet与三种最先进的模型（PyNet[[31](#bib.bib31)]、AWNet[[35](#bib.bib35)]和MW-ISPNet）进行了比较，在对齐GT上超越了所有指标，但在原始GT的SSIM指标上略逊于MW-ISPNet。此外，这个模型的GAN版本在LPIPS指标[[107](#bib.bib107)]中获得了更好的感知结果。最后，在定性比较中，该网络能够保留比其他模型更多的细节。使用SR-RAW数据集时，作者还与SR方法进行了比较，如SRGAN[[92](#bib.bib92)]、ESRGAN
    [[108](#bib.bib108)]、SPSR [[109](#bib.bib109)]和RealSR[[110](#bib.bib110)]。它生成的图像噪声更少、更清晰、细节更多，并且在几乎所有指标上得分更高，只在原始GT的PSNR指标上失分，该指标偏向于模糊图像。
- en: In conclusion, this work outperformed state-of-the-art models in ISP and SR
    problems, in addition to provide a new method to train DNN models with misalignment
    datasets. Besides, this new method enabled a lightweight network, such as used
    at work, and generated results near or higher than more robust models. However,
    the authors did not test the model in embedded systems.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这项工作在ISP和SR问题上超越了最先进的模型，并且提供了一种用错位数据集训练DNN模型的新方法。此外，这种新方法使得一个轻量级网络（如工作中使用的）能够产生接近或高于更强大的模型的结果。然而，作者没有在嵌入式系统中测试该模型。
- en: 3.21 TENet
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.21 TENet
- en: 'Usually, the ISP pipeline is an operations sequence with three core components
    in a fixed order: demosaicing, denoising, and super-resolution. However, Qian
    et al. [[41](#bib.bib41)], in extensive experiments, shown that a simple reordering
    of the operation sequence can increase the image quality. Then, the authors created
    the Trinity Enhancement Network (TENet), a network that reordered the operation
    sequence to denoising(DN), super-resolution(SR), and demosaicing(DM). The DN block
    is the first because the noise on a RAW image has a Gaussian-Poisson distribution
    [[111](#bib.bib111)], then more straightforward to resolve; the RAW image noise
    can hinder subsequent tasks; also, this noise become complex over image processes
    operations. Furthermore, the DM in higher resolution images results in fewer artifacts,
    and super-resolution algorithms could amplify the artifacts generated by DM. Therefore
    the SR was the second block in this architecture.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，ISP 流水线是一个具有固定顺序的三个核心组件的操作序列：去马赛克、去噪声和超分辨率。然而，Qian 等人 [[41](#bib.bib41)]
    在广泛的实验中表明，简单地重新排序操作序列可以提高图像质量。随后，作者创建了 Trinity Enhancement Network (TENet)，这是一个重新排列操作顺序的网络，将去噪声
    (DN)、超分辨率 (SR) 和去马赛克 (DM) 排列在一起。DN 块是第一个，因为 RAW 图像上的噪声具有高斯-泊松分布 [[111](#bib.bib111)]，因此更容易解决；RAW
    图像噪声可能会阻碍后续任务；此外，这种噪声在图像处理操作过程中会变得更加复杂。此外，高分辨率图像中的 DM 会产生更少的伪影，超分辨率算法可能会放大 DM
    生成的伪影。因此，SR 是这个架构中的第二个块。
- en: 'As the DN produces blur in the image, the authors joined the DN and SR in a
    unique block eliminating the accumulated error over image operations and resulting
    in this final pipeline: DN + SR -> DM. To effectively leverage in consideration
    these two stages, the loss function is composed of two losses: the $\pazocal{L}_{joint}$,
    which is the $l_{2}$-norm loss on the final output image, and LSR, the $l_{2}$-norm
    loss between the DN+SR result and the high-resolution noise-free mosaiced image
    of the input image. Thus, the final loss was the sum of these two losses. Besides,
    the authors used the Residual in Residual Dense Block(RRDB) [[108](#bib.bib108)]
    to construct the central part of all blocks.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 DN 会在图像中产生模糊，作者将 DN 和 SR 合并为一个独特的块，消除了图像操作中的累积误差，最终得到了这个流水线：DN + SR -> DM。为了有效利用这两个阶段，损失函数由两个损失组成：$\pazocal{L}_{joint}$，这是最终输出图像上的
    $l_{2}$-范数损失，以及 LSR，即 DN+SR 结果与输入图像的高分辨率无噪声马赛克图像之间的 $l_{2}$-范数损失。因此，最终损失是这两个损失的总和。此外，作者使用
    Residual in Residual Dense Block (RRDB) [[108](#bib.bib108)] 来构建所有块的核心部分。
- en: 'They also notice that previous datasets that synthesized the DM are sub-optimal
    for three reasons: 1) The images used to synthesize RAW images are the result
    of interpolation by the camera ISP; 2) the model was trained to learn an average
    DM algorithm used in the camera ISP; 3) The synthesized RAW images had less information
    than real RAW images. For this reason the PixelShift200 Dataset citeqian2021rethinking
    was created with 200 2k-resolution full color sampled images. Each pixel of images
    was all color information without demosaicing because of the pixel shift technique.
    Besides, from these high-resolution RAW images was created the low-resolution
    RAW images through the bicubic downsampling kernel [[94](#bib.bib94)], mosaic
    kernel [[58](#bib.bib58)], and addition of the Gaussian-Poisson noise model [[111](#bib.bib111)].'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 他们还注意到，之前合成 DM 的数据集存在三方面的不足：1）用于合成 RAW 图像的图像是通过相机 ISP 进行插值的结果；2）模型被训练成学习相机 ISP
    中使用的平均 DM 算法；3）合成的 RAW 图像的信息量低于真实 RAW 图像。因此，PixelShift200 数据集 citeqian2021rethinking
    采用了 200 张 2k 分辨率的全色彩采样图像。由于像素位移技术，每个图像的像素都包含完整的颜色信息，无需去马赛克处理。此外，从这些高分辨率 RAW 图像中，通过双三次下采样核
    [[94](#bib.bib94)]、马赛克核 [[58](#bib.bib58)] 和加性高斯-泊松噪声模型 [[111](#bib.bib111)] 创建了低分辨率
    RAW 图像。
- en: 'The model was compared with the ADMM [[59](#bib.bib59)], Condat [[112](#bib.bib112)],
    Flex-ISP [[97](#bib.bib97)], and DemosaicNet^†^†footnotemark:  [[60](#bib.bib60)]
    on the commonly used benchmark datasets to denoise and demosaicing tasks: Urban
    100 [[84](#bib.bib84)], Kodak, McMaster [[83](#bib.bib83)] and BSD100 [[113](#bib.bib113)].
    TENet outperforms these models in qualitative metrics, therefore generating much
    less moiré, color artifacts, and more fine-grained textures. The network generated
    clean images with accurate details validated in the datasets with the addition
    of Gaussian white noise, where the ADMM and DemosaicNet generate smooth results,
    FlexISP does not treat the noise correctly, and the ADMM generates color aliasing
    artifacts.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '该模型与ADMM [[59](#bib.bib59)]、Condat [[112](#bib.bib112)]、Flex-ISP [[97](#bib.bib97)]和DemosaicNet^†^†footnotemark:
    [[60](#bib.bib60)]在常用的基准数据集上进行了比较，包括去噪和去马赛克任务：Urban 100 [[84](#bib.bib84)]、Kodak、McMaster
    [[83](#bib.bib83)]和BSD100 [[113](#bib.bib113)]。TENet在定性指标上优于这些模型，因此生成了更少的摩尔纹、色彩伪影，并具有更精细的纹理。该网络生成了清晰的图像，详细信息在添加了高斯白噪声的数据集中得到了验证，其中ADMM和DemosaicNet生成了平滑结果，FlexISP没有正确处理噪声，而ADMM生成了色彩混叠伪影。'
- en: 3.22 ReconfigISP
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.22 ReconfigISP
- en: ReconfigISP [[42](#bib.bib42)] is a reconfigurable ISP where the architecture
    and parameters are adapted according to a specific task. To accomplish this, the
    authors have implemented several ISP modules and given a specific task, an optimal
    pipeline is configured by automatically adjusting hundreds of parameters. This
    method maintained the modularity of the steps in an image reconstruction process,
    where each module performs a clear role in the ISP pipeline and allows back-propagation
    for each module by training a differentiable proxy. The differentiable proxy aimed
    to imitate a non-differentiable module via a convolutional neural network, thus
    allowing the optimization of the module’s parameters. Therefore, the ISP architecture
    was explored with neural architecture search, where modules receive an architecture
    weight and are removed if the weight is below a pre-set threshold. This also reduces
    computational complexity and speeds up the training process. The loss function
    in this network was chosen according to the specific task desired.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ReconfigISP [[42](#bib.bib42)]是一种可重配置的ISP，其中架构和参数根据具体任务进行调整。为了实现这一点，作者们实现了多个ISP模块，并根据具体任务，通过自动调整数百个参数来配置一个最佳管线。这种方法保持了图像重建过程步骤的模块化，每个模块在ISP管线中执行明确的角色，并通过训练一个可微分的代理来允许每个模块的反向传播。可微分的代理旨在通过卷积神经网络模仿一个不可微分的模块，从而允许优化模块的参数。因此，ISP架构通过神经架构搜索进行探索，模块接收一个架构权重，并在权重低于预设阈值时被移除。这也减少了计算复杂性，加快了训练过程。该网络中的损失函数是根据特定任务需求选择的。
- en: To validate the effectiveness of this proposal, the authors performed experiments
    with image restoration and object detection with different sensors, light conditions,
    and efficiency constraints. The results were validated over the SID Dataset [[71](#bib.bib71)]
    and S7 ISP Dataset[[70](#bib.bib70)] and showed that this network outperforms
    the traditional ISP pipelines achieving a higher PSNR value than Camera ISP. When
    compared to U-Net[[71](#bib.bib71)], the method obtained a better PSNR value for
    data with smaller numbers of patches in training but a lower value for larger-scale
    data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一提案的有效性，作者们进行了图像恢复和目标检测实验，使用了不同的传感器、光照条件和效率约束。这些结果在SID数据集[[71](#bib.bib71)]和S7
    ISP数据集[[70](#bib.bib70)]上进行了验证，显示该网络优于传统的ISP管线，达到了比相机ISP更高的PSNR值。与U-Net[[71](#bib.bib71)]相比，该方法在训练数据的补丁数量较少时取得了更好的PSNR值，但在大规模数据中则表现较差。
- en: 3.23 ISP Distillation
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.23 ISP 蒸馏
- en: In ISP Distillation [[43](#bib.bib43)], the authors proposed a model for image
    classification with RAW images using an sRGB image classification model and Knowledge
    Distillation [[114](#bib.bib114)] of an ISP pipeline to reduce the compute cost
    of the traditional ISP. Traditional ISP pipelines focus on human vision, while
    this paper provided a solution for machine vision only. Because of this, the authors
    applied the vision models directly to the RAW data.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在ISP蒸馏[[43](#bib.bib43)]中，作者们提出了一种使用sRGB图像分类模型和知识蒸馏[[114](#bib.bib114)]的RAW图像分类模型，以减少传统ISP的计算成本。传统的ISP管线关注于人类视觉，而本文则仅为机器视觉提供了解决方案。因此，作者们将视觉模型直接应用于RAW数据。
- en: A dataset of RAW and RGB pairs was used to overcome the performance drop that
    occurs when data was trained directly on RAW images. This dataset is used to pre-train
    a model that was subsequently distilled to another model responsible for treating
    directly the RAW data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了一个 RAW 和 RGB 对的数据集，以克服直接在 RAW 图像上训练数据时发生的性能下降。该数据集用于预训练一个模型，随后将其蒸馏到另一个直接处理
    RAW 数据的模型中。
- en: To validate the proposal, two cases were tested. The first was by discarding
    denoising and demosaicing pre-processing on a model pre-trained on the ImageNet
    dataset [[115](#bib.bib115)]. The second was to discard the entire ISP pipeline
    in a model pre-trained on the HDR+ dataset  [[68](#bib.bib68)].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证提案，测试了两个案例。第一个是对在 ImageNet 数据集 [[115](#bib.bib115)] 上预训练的模型，舍弃去噪声和去马赛克的预处理。第二个是对在
    HDR+ 数据集 [[68](#bib.bib68)] 上预训练的模型，舍弃整个 ISP 流水线。
- en: ResNet18 [[73](#bib.bib73)] and MobileNetV2[[116](#bib.bib116)] were used for
    validation. Both experiments demonstrated good performance when evaluated on top-1
    and top-5 metrics. Therefore, ISP Distillation is a step towards achieving similar
    classification performance on RAW images when compared to RGB. Although the paper
    cited that ISP Distillation saves the computation cost of the ISP, the computation
    cost was not presented in the article.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet18 [[73](#bib.bib73)] 和 MobileNetV2 [[116](#bib.bib116)] 被用于验证。两项实验都显示出在
    top-1 和 top-5 评估指标上的良好性能。因此，ISP 蒸馏是实现 RAW 图像与 RGB 图像相似分类性能的一个步骤。尽管论文中提到 ISP 蒸馏节省了
    ISP 的计算成本，但文章中并未提供计算成本的具体数据。
- en: 3.24 Merging-ISP
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.24 Merging-ISP
- en: 'Merging-ISP [[44](#bib.bib44)] consists of a deep neural network responsible
    for reconstructing multiple image layers LDR (low dynamic range) in just one image
    HDR (high dynamic range). Thus, the input data contains RAW images in dynamic
    or static scenes, where the network maps them and convolute all the layers in
    one exit HDR. Before the convolution, a DnCNN[[117](#bib.bib117)] conception applies
    a filter with 5x5 size and 64 layers, then applies two filters, both with 5x5
    size and 64 layers, and, finally, applies three filters with 1x1 size and activation
    sigmoid. The output obtained, the data volume is reduced without applying another
    trainee and the LDR merge in just one HDR, comprising four convolutional layers
    Merging-ISP: Multi-Exposure High Dynamic Range Image Signal Processing 7 with
    decreasing receptive fields of 7 × 7 for 100 filters in the first layer to 1 ×
    1 for three filters in the last layer. Note that it was not necessary to apply
    an optical flow on input data.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Merging-ISP [[44](#bib.bib44)] 由一个深度神经网络组成，该网络负责将多个图像层 LDR（低动态范围）重建为一个图像 HDR（高动态范围）。因此，输入数据包含动态或静态场景中的
    RAW 图像，网络对这些图像进行映射并将所有层卷积为一个输出 HDR。在卷积之前，DnCNN [[117](#bib.bib117)] 概念应用了一个 5x5
    大小和 64 层的滤波器，然后应用两个 5x5 大小和 64 层的滤波器，最后应用三个 1x1 大小的滤波器和激活 sigmoid。所获得的输出数据量减少，而没有应用其他训练器，LDR
    合并为一个 HDR，包含四个卷积层的 Merging-ISP：多曝光高动态范围图像信号处理 7，感受野从第一层的 7 × 7 逐渐减少到最后一层的 1 ×
    1。请注意，输入数据上不需要应用光流。
- en: To train the network, synthetic and real datasets based on Kalantari [[118](#bib.bib118)]
    datasets were used. The data contained dynamic and statics scenes, how was stated
    before. Secondly, rotation techniques were used to increase the dataset, contributing
    to extracting 210000 non-overlapping patches of size 50 × 50 pixels using a stride
    of 50\. Besides, they perform training over 70 epochs with a constant learning
    rate of 10e^(-4) and batches of size 32\. During each epoch, all batches are randomly
    shuffled.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练网络，使用了基于 Kalantari [[118](#bib.bib118)] 数据集的合成和真实数据集。数据包含动态和静态场景，如前所述。其次，使用旋转技术增加数据集，提取了
    210000 个不重叠的 50 × 50 像素大小的图像块，步幅为 50。除此之外，他们在 70 个训练周期上进行训练，学习率恒定为 10e^(-4)，批量大小为
    32。在每个周期中，所有批次都被随机打乱。
- en: In comparison to other merging-ISP methods, this one approach obtained the best
    result in PSNR, SSIM and HDR-VDP-2 parameters.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他合并-ISP 方法相比，该方法在 PSNR、SSIM 和 HDR-VDP-2 参数上获得了最佳结果。
- en: 3.25 GCP-Net
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.25 GCP-Net
- en: Guo Et al.  [[45](#bib.bib45)] studied a CNN-based Joint Denoising and Demosaicing
    method for real-world burst images. For this task, since the green channel has
    twice the sampling rate and better quality than the red and blue channels in CFA
    RAW data, the authors proposed a green channel prior neural network - the GCP
    Net. This model extracted the GCP features from green channels to conduct the
    deep feature modeling, upsampling the image and evaluate the frames offset, relieving
    the noise impact. The given work also sought out realistic noise models[[119](#bib.bib119)],
     [[58](#bib.bib58)], and a set of burst images instead of a single CFA image.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Guo 等人[[45](#bib.bib45)]研究了一种基于 CNN 的联合去噪和去马赛克方法，用于现实世界的突发图像。针对这一任务，由于绿色通道的采样率是红色和蓝色通道的两倍，并且在
    CFA RAW 数据中质量更好，作者提出了一种绿色通道先验神经网络——GCP Net。该模型从绿色通道中提取 GCP 特征，以进行深度特征建模、图像上采样和帧偏移评估，从而缓解噪声影响。该研究还探讨了现实噪声模型[[119](#bib.bib119)]、[[58](#bib.bib58)]，以及一组突发图像而非单一的
    CFA 图像。
- en: The GCP-Net structure is composed of two branches - a GCP branch and a reconstruction
    branch. By using several convolutional and LReLu blocks [[120](#bib.bib120)],
    the GCP branch extracts the green features from the noisy green channels concatenation
    and their noise level map.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: GCP-Net 结构由两个分支组成——一个 GCP 分支和一个重建分支。通过使用几个卷积和 LReLu 块[[120](#bib.bib120)]，GCP
    分支从嘈杂的绿色通道拼接及其噪声水平图中提取绿色特征。
- en: The reconstruction branch estimates the clean full-color image. The branch consists
    of three blocks - the intra-frame module (IntraF), the inter-frame module (InterF),
    and the merge module - and utilizes the burst images, the noise maps, and the
    GCP features as the guided information. The IntraF block models the deep features
    of each frame and guides the feature extraction using the GCP features. The InterF
    uses a deformable convolution [[121](#bib.bib121)] in the feature domain to make
    up for the shift between frames. A pyramidal processing is applied to handle possible
    large motions, just like EDVR[[122](#bib.bib122)] and RViDeNet[[123](#bib.bib123)].
    Furthermore, InterF includes an LSTM regularization in the offset estimation,
    providing the temporal constraint. The merge module provides adaptive upsampling
    for the full-resolution image reconstruction.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 重建分支估计干净的全彩色图像。该分支由三个模块组成——帧内模块（IntraF）、帧间模块（InterF）和合并模块，并利用突发图像、噪声图和 GCP 特征作为引导信息。IntraF
    块对每帧的深度特征进行建模，并使用 GCP 特征指导特征提取。InterF 在特征域中使用可变形卷积[[121](#bib.bib121)] 来弥补帧之间的位移。采用金字塔处理以应对可能的大幅运动，就像
    EDVR[[122](#bib.bib122)] 和 RViDeNet[[123](#bib.bib123)]。此外，InterF 在偏移量估计中包括 LSTM
    正则化，提供了时间约束。合并模块提供了自适应上采样，以实现全分辨率图像重建。
- en: The study synthesized trained data using the Vimeo-90k open high-quality video
    dataset [[98](#bib.bib98)]. The training lasted two days, using the PyTorch framework
    and two Nvidia GeForce RTX 2080 Ti GPU. The authors used the Vid64 [[124](#bib.bib124)]
    and the REDS4[[122](#bib.bib122)] datasets for the ablation experiments.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究使用 Vimeo-90k 开放的高质量视频数据集[[98](#bib.bib98)] 合成了训练数据。训练持续了两天，使用了 PyTorch 框架和两块
    Nvidia GeForce RTX 2080 Ti GPU。作者使用了 Vid64[[124](#bib.bib124)] 和 REDS4[[122](#bib.bib122)]
    数据集进行消融实验。
- en: For the comparison experiments, the authors tested the proposed model on synthetic
    data and real-world data. In both scenarios, the GCP-Net achieved superior quantitative
    and qualitative performance to other state-of-the-art Join Denoising-Demosaicing
    algorithms, such as FlexISP [[97](#bib.bib97)] and ADMM[[59](#bib.bib59)]. The
    Paper provided a complete introduction and related work explanation. There was
    also a proper and detailed experiment section, including fine points about training
    parameters. The ablation study validated the effectiveness of major GCP-Net components.
    The chosen comparison datasets enhanced the proposed model value, especially with
    real-world data verification.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在对比实验中，作者在合成数据和现实世界数据上测试了所提出的模型。在这两种情况下，GCP-Net 在定量和定性性能上均优于其他先进的联合去噪-去马赛克算法，如
    FlexISP[[97](#bib.bib97)] 和 ADMM[[59](#bib.bib59)]。论文提供了完整的介绍和相关工作的解释。同时，实验部分也非常完善和详细，包括有关训练参数的细节。消融研究验证了主要
    GCP-Net 组件的有效性。选择的对比数据集提升了所提出模型的价值，特别是在现实世界数据验证方面。
- en: 3.26 PIPNet
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.26 PIPNet
- en: In this paper [[46](#bib.bib46)], the authors proposed a deep network to work
    with joint demosaicing and denoising in Quad Bayer CFA and Bayer CFA patterns.
    The proposed network uses attention mechanisms and is oriented by an objective
    function, including news perceptual losses to produce pleasure images on a pixel-bin
    image sensor. This network, defined as a pixel-bin image processing network (PIPNet),
    uses UNet as a framework and traverses different feature depths through downscaling
    and upscaling operations to leverage the architecture used. The authors also extended
    the method to reconstruct and enhance perceptual images captured with a smartphone
    camera.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文[[46](#bib.bib46)]中，作者提出了一个深度网络，用于处理Quad Bayer CFA和Bayer CFA模式下的联合去马赛克和去噪。该网络使用注意力机制，并由一个目标函数进行指导，其中包括新的感知损失，以在像素-bin图像传感器上生成令人愉悦的图像。该网络定义为像素-bin图像处理网络（PIPNet），使用UNet作为框架，通过降采样和升采样操作遍历不同的特征深度，以利用所使用的架构。作者还扩展了该方法，以重建和增强用智能手机摄像头捕捉的感知图像。
- en: 'The results were validated on the MSR demosaicing dataset [[125](#bib.bib125)],
    BSD100 [[113](#bib.bib113)], McMaster [[83](#bib.bib83)], Urban 100 [[84](#bib.bib84)],
    Kodak [[82](#bib.bib82)], and WED [[126](#bib.bib126)] datasets and compared with
    Deepjoint^†^†footnotemark:  [[60](#bib.bib60)], Kokkinos [[96](#bib.bib96)], Dong [[127](#bib.bib127)],
    DeepISP [[70](#bib.bib70)], and DPN [[29](#bib.bib29)] methods at three different
    noise levels (5, 15, 25). In all comparisons, PIPNet performed better over the
    PSNR, SSIM, and DeltaE2000 metrics. Qualitatively, the method also outperformed
    the other approaches. However, the network was tested only with data collected
    by traditional Bayer sensors, which may hinder network performance in other scenarios.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在MSR去马赛克数据集[[125](#bib.bib125)]、BSD100[[113](#bib.bib113)]、McMaster[[83](#bib.bib83)]、Urban
    100[[84](#bib.bib84)]、Kodak[[82](#bib.bib82)]和WED[[126](#bib.bib126)]数据集上进行了验证，并与Deepjoint^†^†脚注标记:[[60](#bib.bib60)]、Kokkinos[[96](#bib.bib96)]、Dong[[127](#bib.bib127)]、DeepISP[[70](#bib.bib70)]和DPN[[29](#bib.bib29)]方法在三个不同噪声水平（5、15、25）下进行了比较。在所有比较中，PIPNet在PSNR、SSIM和DeltaE2000指标上表现更好。从定性上看，该方法也优于其他方法。然而，该网络仅在使用传统Bayer传感器收集的数据上进行了测试，这可能会影响网络在其他场景中的性能。
- en: 3.27 CURL
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.27 CURL
- en: In this paper [[47](#bib.bib47)], the authors proposed a method for enhancing
    an image inspired by photographers who perform image retouching based on global
    image adjustment curves. This method, called CURL, can be used in two different
    scenarios. The first is the RGB-to-RGB mapping where an input RGB image is mapped
    to another visually pleasing RGB image and the second scenario is the RAW-to-RGB
    mapping where the entire ISP pipeline is done.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文[[47](#bib.bib47)]中，作者提出了一种增强图像的方法，这种方法的灵感来源于摄影师基于全局图像调整曲线进行图像修饰的方式。该方法称为CURL，可以在两种不同的场景中使用。第一种是RGB到RGB映射，即将输入的RGB图像映射到另一个视觉上令人愉悦的RGB图像；第二种场景是RAW到RGB映射，即完成整个ISP流程。
- en: This method is composed of two architectures called Transformed Encoder-Decoder
    (TED) backbone and CURL block. The TED is similar to U-Net [[85](#bib.bib85)]
    but without the skip connections, except for the level-1 skip connections which
    were replaced by a multi-scale neural processing block that provides enhanced
    images via local pixel processing to the CURL block. The CURL block is a Neural
    Curve Layers block that exploits the representation of the image in three color
    spaces (CIELab, HSV, RGB) intending to globally refine its properties through
    color, luminance, and saturation adjustments guided by a new multi-color space
    loss function. The CURL loss function aims to optimize the final image in its
    different properties such as chrominance, hue, luminance, and saturation.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法由两个架构组成，分别是称为转化编码解码器（TED）骨干网络和CURL块。TED类似于U-Net[[85](#bib.bib85)]，但没有跳跃连接，除了第一级跳跃连接，这些连接被一个多尺度神经处理块所替代，该块通过局部像素处理向CURL块提供增强图像。CURL块是一个神经曲线层块，它利用图像在三种颜色空间（CIELab、HSV、RGB）中的表示，旨在通过颜色、亮度和饱和度调整来全球性地优化其属性，指导由新的多颜色空间损失函数提供。CURL损失函数旨在优化最终图像的不同属性，如色度、色调、亮度和饱和度。
- en: Two experiments were done for the validation of the method, where the medium-to-medium
    exposure RAW to RGB mapping and the predicting the retouching of photographers
    for RGB to RGB mapping was evaluated. In the first, results were validated on
    the Samsung S7 dataset [[70](#bib.bib70)], and CURL scored the best PSNR and LPIPS
    metrics when compared to the U-Net [[85](#bib.bib85)] and DeepISP[[70](#bib.bib70)]
    methods, but tied with the DeepISP method on the SSIM metric. In the second, results
    were validated over the MIT-Adobe FiveK dataset[[69](#bib.bib69)] and compared
    with HDRNet[[128](#bib.bib128)], DPE [[129](#bib.bib129)], White-Box [[130](#bib.bib130)],
    Distort-and-Recover [[131](#bib.bib131)], and DeepUPE [[132](#bib.bib132)] methods
    where CURL scored better on PSNR and LPIPS metrics, but DeepUPE scored the best
    on SSIM. Qualitatively CURL generates images very pleasing to the human eye.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对该方法进行了两项实验验证，其中评估了从中等曝光 RAW 到 RGB 映射和预测摄影师对 RGB 到 RGB 映射的修饰。在第一次实验中，结果在 Samsung
    S7 数据集上进行了验证[[70](#bib.bib70)]，CURL 方法在 PSNR 和 LPIPS 指标上表现最佳，相较于 U-Net[[85](#bib.bib85)]
    和 DeepISP[[70](#bib.bib70)] 方法，但在 SSIM 指标上与 DeepISP 方法持平。在第二次实验中，结果在 MIT-Adobe
    FiveK 数据集[[69](#bib.bib69)] 上进行了验证，并与 HDRNet[[128](#bib.bib128)]、DPE[[129](#bib.bib129)]、White-Box[[130](#bib.bib130)]、Distort-and-Recover[[131](#bib.bib131)]
    和 DeepUPE[[132](#bib.bib132)] 方法进行了比较，其中 CURL 在 PSNR 和 LPIPS 指标上表现更佳，但 DeepUPE
    在 SSIM 上表现最佳。质性上，CURL 生成的图像对人眼非常令人满意。
- en: 4 Methodology
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: This section describes how we made a qualitative comparison among the works
    covered in this survey and the analysis of these papers to highlight points of
    improvement, highlights, and ways to evolve this field of study.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了我们如何在本次调查中对涉及的工作进行定性比较，并分析这些论文以突出改进点、亮点和该研究领域的演变方式。
- en: 4.1 Datasets
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: 'For the quantitative evaluation, we provide a comparison among the works covered
    in this survey with the more explored datasets. To do this, we use the results
    provided by these works and the most commonly used metrics in the image restoration
    task, PSRN and SSIM, as the base for comparison. We provide a brief discussion
    on each most commonly used dataset on the studies we analyzed. Table [2](#S4.T2
    "Table 2 ‣ 4.1 Datasets ‣ 4 Methodology ‣ ISP meets Deep Learning: A Survey on
    Deep Learning Methods for Image Signal Processing") gives a list of all datasets
    discussed in this section with details about the number of images, size, and link
    to download.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '对于定量评估，我们提供了在本次调查中涵盖的工作与更多被广泛探索的数据集的比较。为此，我们使用这些工作提供的结果以及图像恢复任务中最常用的指标，PSRN
    和 SSIM，作为比较的基础。我们对每个最常用的数据集进行了简要讨论。表 [2](#S4.T2 "Table 2 ‣ 4.1 Datasets ‣ 4 Methodology
    ‣ ISP meets Deep Learning: A Survey on Deep Learning Methods for Image Signal
    Processing") 给出了本节讨论的所有数据集的列表，包含有关图像数量、大小和下载链接的详细信息。'
- en: 'Table 2: Summarization of datasets considered in the survey and their respective
    amount of images, size, and link to download.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：调查中考虑的数据集的汇总，以及它们各自的图像数量、大小和下载链接。
- en: '| Dataset name | Nº images | Size | Link to download |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 数据集名称 | 图像数量 | 大小 | 下载链接 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Zurich RAW to RGB [[133](#bib.bib133)] | 20.000 | $\approx$ 22 GB | [http://people.ee.ethz.ch/~ihnatova/pynet.html](http://people.ee.ethz.ch/~ihnatova/pynet.html)
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Zurich RAW to RGB [[133](#bib.bib133)] | 20.000 | $\approx$ 22 GB | [http://people.ee.ethz.ch/~ihnatova/pynet.html](http://people.ee.ethz.ch/~ihnatova/pynet.html)
    |'
- en: '| Urban 100 [[84](#bib.bib84)] | 100 | 1.14 GB | [https://github.com/jbhuang0604/SelfExSR](https://github.com/jbhuang0604/SelfExSR)
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| Urban 100 [[84](#bib.bib84)] | 100 | 1.14 GB | [https://github.com/jbhuang0604/SelfExSR](https://github.com/jbhuang0604/SelfExSR)
    |'
- en: '| McMaster dataset [[83](#bib.bib83)] | 18 | 13.6 MB | [https://web.comp.polyu.edu.hk/cslzhang/CDM_Dataset.htm](https://web.comp.polyu.edu.hk/cslzhang/CDM_Dataset.htm)
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| McMaster dataset [[83](#bib.bib83)] | 18 | 13.6 MB | [https://web.comp.polyu.edu.hk/cslzhang/CDM_Dataset.htm](https://web.comp.polyu.edu.hk/cslzhang/CDM_Dataset.htm)
    |'
- en: '| Kodak | 25 | 119.7 MB | [http://r0k.us/graphics/kodak/](http://r0k.us/graphics/kodak/)
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Kodak | 25 | 119.7 MB | [http://r0k.us/graphics/kodak/](http://r0k.us/graphics/kodak/)
    |'
- en: 4.1.1 Zurich RAW to RGB (ZRR)
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 Zurich RAW to RGB (ZRR)
- en: ZRR dataset was proposed by Ignatov Et. al.  [[133](#bib.bib133)] in order to
    get a large-scale real-world dataset, that deals with the task of converting original
    RAW photos captured by smartphone cameras to superior quality images achieved
    by a professional DSLR camera. The proposed database is publicly available and
    amounts to 22 GB, containing 20K real images captured synchronously by a Canon
    5D Mark IV DSLR camera and Huawei P20 phone, in a variety of places and in various
    illumination and weather conditions. The photos were captured in automatic mode,
    however, some RAW–RGB image pairs are not perfectly aligned, which requires a
    preprocessing and matching performance. ZRR was a recurrent dataset choice for
    some RAW to RGB mapping problem works considered in this survey[[22](#bib.bib22),
    [36](#bib.bib36), [31](#bib.bib31), [40](#bib.bib40), [35](#bib.bib35), [32](#bib.bib32)].
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ZRR数据集由Ignatov等人提出[[133](#bib.bib133)]，旨在获得大规模真实世界的数据集，用于将原始智能手机相机拍摄的RAW照片转换为由专业单反相机获得的高质量图像。该数据库公开可用，总量为22GB，包含由佳能5D
    Mark IV单反相机和华为P20手机同步拍摄的2万张真实图像，涵盖各种地点、照明和天气条件。这些照片是在自动模式下拍摄的，但一些RAW-RGB图像对未完全对齐，需要预处理和匹配性能。ZRR是本调查中一些RAW到RGB映射问题工作的常用数据集[[22](#bib.bib22),
    [36](#bib.bib36), [31](#bib.bib31), [40](#bib.bib40), [35](#bib.bib35), [32](#bib.bib32)]。
- en: 4.1.2 Urban 100
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 城市100
- en: The Urban 100 dataset consists of 100 High-Resolution images with an assortment
    of real-world urban scenarios and structures. It was proposed by Huang Et. al.
     [[84](#bib.bib84)], in order to address the lack of High-Resolution Datasets
    with indoor, urban, and architectural scenes. Urban 100 was constructed with synthetic
    images from Flickr⁵⁵5https://www.flickr.com/, under Creative Commons license,
    resulting in a 1.14 GB dataset. It is a well-known public database for super-resolution
    tasks  [[29](#bib.bib29), [28](#bib.bib28), [34](#bib.bib34), [46](#bib.bib46),
    [33](#bib.bib33), [29](#bib.bib29)].
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 城市100数据集包含100张高分辨率图像，展示了各种真实世界的城市场景和建筑结构。由黄等人提出[[84](#bib.bib84)]，旨在解决缺乏室内、城市和建筑场景的高分辨率数据集的问题。城市100数据集由Flickr⁵⁵5https://www.flickr.com/上的合成图像构成，使用了创作共用许可证，形成了一个1.14GB的数据集。它是一个广为人知的超分辨率任务公开数据库[[29](#bib.bib29),
    [28](#bib.bib28), [34](#bib.bib34), [46](#bib.bib46), [33](#bib.bib33), [29](#bib.bib29)]。
- en: 4.1.3 McMaster dataset
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 麦克马斯特数据集
- en: The McMaster dataset was proposed by Zhang et al. [[83](#bib.bib83)] and consists
    of eighteen sub-images of size 500x500 captured by Kodak film and then digitized.
    The sub-images were cropped from eight high-resolution natural images with size
    2310x1814\. When compared with the Kodak color image dataset, McMaster shows images
    with more saturated colors and more color transitions between the image’s objects.
    However, this dataset is still limited in scene variation and colors gradations.
    The McMaster dataset is used for color demosaicing in some of the articles in
    this survey[[46](#bib.bib46), [28](#bib.bib28), [29](#bib.bib29), [24](#bib.bib24),
    [41](#bib.bib41)].
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克马斯特数据集由张等人提出[[83](#bib.bib83)]，包含十八张大小为500x500的子图像，这些图像由柯达胶卷拍摄并数字化。子图像从八张高分辨率自然图像中裁剪，尺寸为2310x1814。与柯达彩色图像数据集相比，麦克马斯特数据集的图像颜色更加饱和，图像对象之间的颜色过渡更多。然而，该数据集在场景变化和颜色渐变方面仍然有限。麦克马斯特数据集在本调查的一些文章中用于颜色去马赛克[[46](#bib.bib46),
    [28](#bib.bib28), [29](#bib.bib29), [24](#bib.bib24), [41](#bib.bib41)]。
- en: 4.1.4 Kodak
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 柯达
- en: Kodak ⁶⁶6http://r0k.us/graphics/kodak/ is a little dataset composed of 24 photographic
    quality images of size 768x512 or 512x768 with a large variety of locations and
    lighting conditions. Initially, it was created to be a sample Kodak Photo CD,
    with some images captured by Kodak’s professional photographers and others selected
    from the winners of the Kodak International Newspaper Snapshot Awards(KINSA).
    This dataset contains raw images in photo-cd (PCD) format and PNG format with
    24 bits per pixel. Besides, many works use the Kodak dataset for compression tests
    and to validate methods that do tasks like demosaicking, denoising, and full ISP
    pipeline [[46](#bib.bib46), [28](#bib.bib28), [24](#bib.bib24), [29](#bib.bib29),
    [41](#bib.bib41)].
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 柯达数据集⁶⁶6http://r0k.us/graphics/kodak/ 是一个由24张768x512或512x768大小的摄影质量图像组成的小型数据集，涵盖了各种地点和照明条件。最初，它被创建为一个样本柯达照片CD，部分图像由柯达的专业摄影师拍摄，另一些则从柯达国际报纸快照奖（KINSA）获奖作品中选取。该数据集包含照片CD（PCD）格式和PNG格式的原始图像，每像素24位。此外，许多工作使用柯达数据集进行压缩测试，并验证执行如去马赛克、去噪声和完整ISP管道等任务的方法[[46](#bib.bib46),
    [28](#bib.bib28), [24](#bib.bib24), [29](#bib.bib29), [41](#bib.bib41)]。
- en: 4.2 Papers Analysis
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 论文分析
- en: 'The analysis of the works in some fields of study is fundamental to discover
    new ways to its evolution and improvement in future works. In this survey, the
    papers are analyzed about the following points:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些领域工作的分析对于发现其演变和未来改进的新途径至关重要。在本次调查中，论文被分析了以下几点：
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Details of method: The analysis of details of many methods can bring new ideas
    and identification of problems that future works can propose to solve.'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法细节：对许多方法的细节分析可以带来新的想法，并识别未来工作可以提出解决的问题。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Used datasets:The camera hardware has many nuances and generates your own noise
    that is hard to simulate. Then the use of an appropriate dataset to train and
    validate the work is an important part to consider in creating a new ISP method.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用的数据集：相机硬件具有许多细微差别，并产生难以模拟的噪声。因此，使用适当的数据集来训练和验证工作是创建新 ISP 方法时需要考虑的重要部分。
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Preocupation with computational cost: The computational cost is an important
    point to consider in almost all applications of ISP, mainly used in embedded systems
    and mobile devices.'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算成本的关注：计算成本是几乎所有 ISP 应用中需要考虑的重要因素，主要用于嵌入式系统和移动设备。
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Method evaluation: How the method was evaluated may be well planned to indicate
    the contribution of this work in a determined study area.'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法评估：方法如何评估可能经过精心规划，以表明该工作在特定研究领域中的贡献。
- en: 5 Results
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: In this section, we analyzed the works on a quantitative comparison and indicated
    possible reasons for a method to present better than others. We divided this section
    into four subsections, where each one discussed results obtained in a dataset.
    In the first, we discussed the Zurich RAW to RGB dataset, which contains pairs
    of images from differents câmeras and misalignment between the pairs. In the second,
    we discussed the Urban 100 dataset, which is composed of 100 High-Resolution images
    with real-world urban scenarios and structures. Then, in sequence, we discussed
    the works in the McMaster dataset, with 18 images captured by Kodak film. Finally,
    in the fourth section, we discussed the Kodak dataset with 24 photographic quality
    images of size 768x512 or 512x768, generally used in compression tests and to
    validate methods for tasks like demosaicking and denoising, etc.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们对工作进行了定量比较分析，并指出了某种方法表现优于其他方法的可能原因。我们将本节分为四个小节，每个小节讨论了在一个数据集上获得的结果。在第一部分，我们讨论了苏黎世
    RAW 到 RGB 数据集，该数据集包含来自不同相机的图像对以及图像对之间的错位。第二部分，我们讨论了 Urban 100 数据集，该数据集由 100 张高分辨率的真实世界城市场景和结构的图像组成。接着，我们讨论了
    McMaster 数据集中的工作，该数据集包含 18 张由 Kodak 胶卷拍摄的图像。最后，在第四部分，我们讨论了 Kodak 数据集，该数据集包含 24
    张 768x512 或 512x768 尺寸的摄影质量图像，通常用于压缩测试以及验证去马赛克和去噪等任务的方法。
- en: ZURICH RAW2RGB dataset [[31](#bib.bib31)]  Networks PSNR SSIM Del-Net[[36](#bib.bib36)]
    21.46 0.745 PyNet[[31](#bib.bib31)] 21.19 0.746 AWNet (Ensemble)[[35](#bib.bib35)]
    21.86 0.781 AWNet (Demosaiced)[[35](#bib.bib35)] 21.38 0.745 AWNet (RAW)[[35](#bib.bib35)]
    21.58 0.749 PyNet-CA[[32](#bib.bib32)] 21.50 0.743 LiteISPNet[[40](#bib.bib40)]
    21.55 0.748 LiteISPNet[[40](#bib.bib40)] 23.76^a 0.873^a HERNet[[22](#bib.bib22)]
    22.59^b 0.81^b
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ZURICH RAW2RGB 数据集 [[31](#bib.bib31)] 网络 PSNR SSIM Del-Net[[36](#bib.bib36)]
    21.46 0.745 PyNet[[31](#bib.bib31)] 21.19 0.746 AWNet (Ensemble)[[35](#bib.bib35)]
    21.86 0.781 AWNet (Demosaiced)[[35](#bib.bib35)] 21.38 0.745 AWNet (RAW)[[35](#bib.bib35)]
    21.58 0.749 PyNet-CA[[32](#bib.bib32)] 21.50 0.743 LiteISPNet[[40](#bib.bib40)]
    21.55 0.748 LiteISPNet[[40](#bib.bib40)] 23.76^a 0.873^a HERNet[[22](#bib.bib22)]
    22.59^b 0.81^b
- en: a
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a
- en: Align ground truth with RAW image.
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对齐真实值与 RAW 图像。
- en: b
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b
- en: Data with diferent distribution.
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据分布不同。
- en: Urban 100 dataset [[84](#bib.bib84)]  Networks PSNR SSIM DPN [[29](#bib.bib29)]
    37.70 0.9799 PIPNet [[46](#bib.bib46)] 37.51^a 0.9731^a TENet [[41](#bib.bib41)]
    29.37^c 0.9061^c SGNet[[33](#bib.bib33)] 34.54^a 0.9533^a RLDD [[28](#bib.bib28)]
    39.52^b 0.9864^b RestoreNet w/ PatchNet [[34](#bib.bib34)] 34.66^a - DPN [[29](#bib.bib29)]
    37.70 0.9799
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Urban 100 数据集 [[84](#bib.bib84)] 网络 PSNR SSIM DPN [[29](#bib.bib29)] 37.70 0.9799
    PIPNet [[46](#bib.bib46)] 37.51^a 0.9731^a TENet [[41](#bib.bib41)] 29.37^c 0.9061^c
    SGNet[[33](#bib.bib33)] 34.54^a 0.9533^a RLDD [[28](#bib.bib28)] 39.52^b 0.9864^b
    RestoreNet w/ PatchNet [[34](#bib.bib34)] 34.66^a - DPN [[29](#bib.bib29)] 37.70
    0.9799
- en: a
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a
- en: Data with noise.
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有噪声的数据。
- en: b
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b
- en: 10 pixels were removed from image’s border to calculate the PSNR value.
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为计算 PSNR 值，从图像的边界移除了 10 像素。
- en: c
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: c
- en: Downsampled data to do demosaicing + SR task.
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下采样数据用于做去马赛克 + 超分辨率任务。
- en: 5.1 Zurich RAW to RGB
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 苏黎世 RAW 到 RGB
- en: 'The HERNet and LiteISPNet present versions with better results in the [section 5](#S5
    "5 Results ‣ ISP meets Deep Learning: A Survey on Deep Learning Methods for Image
    Signal Processing"), but it uses, respectively, a different distribution of data
    and the RAW image aligned with the ground-truth. With the remain of results, the
    AWNet outperformed other methods in PSNR and SSIM scores, and the use of a self-ensemble
    strategy can explain this, as RAW and Demosaiced versions have scores relatively
    much lower than the ensembled version. Besides, the AWNet RAW version has slightly
    higher results than LiteISPNet and PyNet-CA, which can result from the wavelet
    transform and context global blocks. Furthermore, the LiteISPNet has impressive
    results with introducing an additional method to align the images, improving network
    training.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 'HERNet和LiteISPNet在[第5节](#S5 "5 Results ‣ ISP meets Deep Learning: A Survey
    on Deep Learning Methods for Image Signal Processing")中展示了更好的结果，但分别使用了不同的数据分布和与地面真实图像对齐的RAW图像。根据其余结果，AWNet在PSNR和SSIM得分上超越了其他方法，自我集成策略可以解释这一点，因为RAW和去马赛克版本的得分明显低于集成版本。此外，AWNet
    RAW版本的结果略高于LiteISPNet和PyNet-CA，这可能得益于小波变换和上下文全局块。此外，LiteISPNet通过引入额外的方法来对齐图像，改进了网络训练，取得了令人印象深刻的结果。'
- en: 5.2 Urban 100
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 Urban 100
- en: 'Urban 100 dataset is used for many tasks of image restoration. Then the works
    do modifications to adequate this dataset to a specific task. As shown in Table [5](#S5
    "5 Results ‣ ISP meets Deep Learning: A Survey on Deep Learning Methods for Image
    Signal Processing"), the RLDD had the highest scores compared to other methods
    but was validated with the image’s border removed, and this factor can help increase
    scores. Furthermore, the PIPNet, even being validated with noisy data, has comparable
    results with DPN. The introduction of attention mechanisms that bring good correlations
    in relation to depth and spatial dimensions may be the cause of these promising
    results in this dataset.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 'Urban 100 数据集用于许多图像修复任务。然后，工作会对该数据集进行修改以适应特定任务。如表格[5](#S5 "5 Results ‣ ISP
    meets Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing")所示，RLDD的得分最高，但经过图像边界去除验证，这一因素可能有助于提高得分。此外，尽管PIPNet经过了带噪声数据的验证，但其结果与DPN相当。引入注意机制带来与深度和空间维度的良好关联，可能是该数据集取得这些有希望结果的原因。'
- en: 5.3 McMaster
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 McMaster
- en: 'The DRDN+ and DRDN methods showed better results in the PSNR metric in Table [5.3](#S5.SS3
    "5.3 McMaster ‣ 5 Results ‣ ISP meets Deep Learning: A Survey on Deep Learning
    Methods for Image Signal Processing"). These methods are CNN-based models and
    focus on demosaicking. The DPN method, which also focused on demosaicing, showed
    better results in the SSIM metric and better artifacts reduction in its qualitative
    evaluation. The TENet network had the lowest performance in the PSNR and SSIM
    metrics, however, it is worth noting that the goal of this network is to make
    an entire ISP pipeline enhancement, instead of only the demosaicing task.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格[5.3](#S5.SS3 "5.3 McMaster ‣ 5 Results ‣ ISP meets Deep Learning: A Survey
    on Deep Learning Methods for Image Signal Processing")中，DRDN+和DRDN方法在PSNR指标上显示了更好的结果。这些方法是基于CNN的模型，专注于去马赛克。DPN方法也专注于去马赛克，在SSIM指标上显示了更好的结果，并且在定性评估中表现出更好的伪影减少。TENet网络在PSNR和SSIM指标上的表现最差，但值得注意的是，该网络的目标是对整个ISP管道进行增强，而不仅仅是去马赛克任务。'
- en: McMaster dataset [[83](#bib.bib83)]  Networks PSNR SSIM DRDN [[24](#bib.bib24)]
    38.88 0.9689 DRDN+ [[24](#bib.bib24)] 39.02 0.9697 DPN [[29](#bib.bib29)] 37.6
    0.9842 TENet[[41](#bib.bib41)] 32.40^c 0.9163^c PIPNet [[46](#bib.bib46)] 38.13^a
    0.9612^a RLDD [[28](#bib.bib28)] 36.61^b 0.9725^b
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: McMaster 数据集 [[83](#bib.bib83)] 网络 PSNR SSIM DRDN [[24](#bib.bib24)] 38.88 0.9689
    DRDN+ [[24](#bib.bib24)] 39.02 0.9697 DPN [[29](#bib.bib29)] 37.6 0.9842 TENet[[41](#bib.bib41)]
    32.40^c 0.9163^c PIPNet [[46](#bib.bib46)] 38.13^a 0.9612^a RLDD [[28](#bib.bib28)]
    36.61^b 0.9725^b
- en: a
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a
- en: Data with noise.
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有噪声的数据。
- en: b
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b
- en: 10 pixels were removed from image’s border to calculate the PSNR value.
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图像边界去除10个像素以计算PSNR值。
- en: c
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: c
- en: Downsampled data to do demosaicing + SR task.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下采样数据以进行去马赛克 + SR任务。
- en: Kodak Dataset [[82](#bib.bib82)]  Networks PSNR SSIM DRDN [[24](#bib.bib24)]
    42.43 0.9889 DRDN+ [[24](#bib.bib24)] 42.66 0.9893 DPN [[29](#bib.bib29)] 40.1
    0.9846 TENet[[41](#bib.bib41)] 31.39 ^a 0.8965^a PIPNet [[46](#bib.bib46)] 39.37^a
    0.9768^a RLDD [[28](#bib.bib28)] 42.76^b 0.9893
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Kodak 数据集 [[82](#bib.bib82)] 网络 PSNR SSIM DRDN [[24](#bib.bib24)] 42.43 0.9889
    DRDN+ [[24](#bib.bib24)] 42.66 0.9893 DPN [[29](#bib.bib29)] 40.1 0.9846 TENet[[41](#bib.bib41)]
    31.39 ^a 0.8965^a PIPNet [[46](#bib.bib46)] 39.37^a 0.9768^a RLDD [[28](#bib.bib28)]
    42.76^b 0.9893
- en: a
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a
- en: Data with noise.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 带有噪声的数据。
- en: b
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b
- en: 10 pixels were removed from image’s border to calculate the PSNR value.
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从图像的边缘移除了 10 像素以计算 PSNR 值。
- en: 5.4 Kodak
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 Kodak
- en: 'Table [5.3](#S5.SS3 "5.3 McMaster ‣ 5 Results ‣ ISP meets Deep Learning: A
    Survey on Deep Learning Methods for Image Signal Processing") shows the reviewed
    paper results in the Kodak dataset ⁷⁷7http://r0k.us/graphics/kodak/. RLDD [[28](#bib.bib28)]
    achieved the best PSNR metric performance, whereas DRDN+[[24](#bib.bib24)] had
    the best SSIM metric performance. The RLDD framework combines Denoising and Demosaicing
    techniques, delivering proper quantitative and qualitative results. It is important
    to reinforce that RLDD authors removed 10 pixels from the Kodak image’s borders
    to calculate the PSNR. TENet and PIPNet introduced artificial noise models into
    the dataset for deeper denoising study. DRDN stands out in terms of efficiency
    and accuracy, in large part because of its block-wise convolutional neural networks
    which consider local features and a sub-pixel interpolation layer.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [5.3](#S5.SS3 "5.3 McMaster ‣ 5 Results ‣ ISP meets Deep Learning: A Survey
    on Deep Learning Methods for Image Signal Processing") 显示了在 Kodak 数据集中的评审论文结果
    ⁷⁷7http://r0k.us/graphics/kodak/。RLDD [[28](#bib.bib28)] 实现了最佳的 PSNR 指标性能，而 DRDN+[[24](#bib.bib24)]
    则在 SSIM 指标上表现最佳。RLDD 框架结合了去噪和解彩技术，提供了适当的定量和定性结果。需要强调的是，RLDD 作者在计算 PSNR 时从 Kodak
    图像的边缘移除了 10 像素。TENet 和 PIPNet 在数据集中引入了人工噪声模型，以进行更深入的去噪研究。DRDN 在效率和准确性方面表现突出，主要因为其块级卷积神经网络考虑了局部特征和子像素插值层。'
- en: 5.5 Source Code Links
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 源代码链接
- en: 'The Table [3](#S5.T3 "Table 3 ‣ 5.5 Source Code Links ‣ 5 Results ‣ ISP meets
    Deep Learning: A Survey on Deep Learning Methods for Image Signal Processing")
    presents the links to the source codes of some works mentioned in this survey.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S5.T3 "Table 3 ‣ 5.5 Source Code Links ‣ 5 Results ‣ ISP meets Deep
    Learning: A Survey on Deep Learning Methods for Image Signal Processing") 提供了本调查中提到的一些工作的源代码链接。'
- en: 'Table 3: Summarization of some approaches considered in the survey and their
    respective source codes.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：调查中考虑的一些方法及其相应的源代码总结。
- en: '| Short Name | Source Code |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 简称 | 源代码 |'
- en: '| --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CycleISP [[30](#bib.bib30)] | PyTorch: [https://github.com/swz30/CycleISP](https://github.com/swz30/CycleISP)
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| CycleISP [[30](#bib.bib30)] | PyTorch: [https://github.com/swz30/CycleISP](https://github.com/swz30/CycleISP)
    |'
- en: '| CURL [[47](#bib.bib47)] | PyTorch: [https://github.com/sjmoran/CURL](https://github.com/sjmoran/CURL)
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| CURL [[47](#bib.bib47)] | PyTorch: [https://github.com/sjmoran/CURL](https://github.com/sjmoran/CURL)
    |'
- en: '| LiteISPNet [[40](#bib.bib40)] | PyTorch: [https://github.com/cszhilu1998/RAW-to-sRGB](https://github.com/cszhilu1998/RAW-to-sRGB)
    |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| LiteISPNet [[40](#bib.bib40)] | PyTorch: [https://github.com/cszhilu1998/RAW-to-sRGB](https://github.com/cszhilu1998/RAW-to-sRGB)
    |'
- en: '| PyNet [[31](#bib.bib31)] |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| PyNet [[31](#bib.bib31)] |'
- en: '&#124; PyTorch and Tensorflow: http://people.ee.ethz.ch/ ihnatova/pynet.html
    &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch 和 Tensorflow: http://people.ee.ethz.ch/ ihnatova/pynet.html
    &#124;'
- en: '|'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PyNet-CA [[32](#bib.bib32)] | PyTorch: [https://github.com/egyptdj/skyb-aim2020-public](https://github.com/egyptdj/skyb-aim2020-public)
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| PyNet-CA [[32](#bib.bib32)] | PyTorch: [https://github.com/egyptdj/skyb-aim2020-public](https://github.com/egyptdj/skyb-aim2020-public)
    |'
- en: '| BJDD [[46](#bib.bib46)] | PyTorch: [https://github.com/sharif-apu/BJDD_CVPR21](https://github.com/sharif-apu/BJDD_CVPR21)
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| BJDD [[46](#bib.bib46)] | PyTorch: [https://github.com/sharif-apu/BJDD_CVPR21](https://github.com/sharif-apu/BJDD_CVPR21)
    |'
- en: '| TENet [[41](#bib.bib41)] | PyTorch: [https://github.com/guochengqian/TENet](https://github.com/guochengqian/TENet)
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| TENet [[41](#bib.bib41)] | PyTorch: [https://github.com/guochengqian/TENet](https://github.com/guochengqian/TENet)
    |'
- en: '| AWNet [[35](#bib.bib35)] |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| AWNet [[35](#bib.bib35)] |'
- en: '&#124; PyTorch: https://github.com/Charlie0215/AWNet-Attentive &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PyTorch: https://github.com/Charlie0215/AWNet-Attentive &#124;'
- en: '&#124; -Wavelet-Network-for-Image-ISP &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; -Wavelet-Network-for-Image-ISP &#124;'
- en: '|'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Bayer Methods [[26](#bib.bib26)] | PyTorch: [https://github.com/Jiaming-Liu/BayerUnifyAug](https://github.com/Jiaming-Liu/BayerUnifyAug)
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| Bayer Methods [[26](#bib.bib26)] | PyTorch: [https://github.com/Jiaming-Liu/BayerUnifyAug](https://github.com/Jiaming-Liu/BayerUnifyAug)
    |'
- en: '| HERNet [[22](#bib.bib22)] | PyTorch: [https://github.com/MKFMIKU/RAW2RGBNet](https://github.com/MKFMIKU/RAW2RGBNet)
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| HERNet [[22](#bib.bib22)] | PyTorch: [https://github.com/MKFMIKU/RAW2RGBNet](https://github.com/MKFMIKU/RAW2RGBNet)
    |'
- en: 6 Discussion
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: We provide, in this section, points of improvement in these works about its
    methodology and evaluation. However, much of it has highlights which can contribute
    a lot about the learned cameras ISPs. This section will address the negative and
    positive points of the works described above and ask questions that may evolve
    into new paths to rods in this research field.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了关于这些工作在其方法论和评估方面的改进点。然而，其中很多亮点可以对学习到的相机 ISP 产生很大贡献。本节将讨论上述工作中的负面和正面点，并提出可能发展成新研究路径的问题。
- en: 6.1 Dataset Created with Different Cameras
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 使用不同相机创建的数据集
- en: Some datasets  [[31](#bib.bib31), [39](#bib.bib39)] have the pair images captured
    by different cameras, where one of the main objectives of the works that use these
    datasets is to learn the features of high-quality cameras and apply them in cameras
    with more limitations. Because of the differences between the devices, these datasets
    have alignment problems about the image patch pairs. Doing the RAW to RGB mapping
    can be challenging because the patch pixels do not correspond to theirs pixels.
    The trivial solution is to align the pairs, but how to do this is the main question.
    Zhang et al.[[40](#bib.bib40)] propose the use of Deep Learning where outperforms
    the models trained with the misalignment dataset. Another way that many works
    [[39](#bib.bib39), [35](#bib.bib35), [31](#bib.bib31), [36](#bib.bib36)] use to
    mitigate this problem is to adapt the loss function to not punish the models where
    the generated image is misaligned with the ground truth. Models trained with losses
    that try to get closer to human perception are more invariant to misalignment
    and generate images with more details. Currently, the metrics that are closest
    to this make use of Deep Learning techniques, using extracted features of images
    with the use of pre-trained Deep Learning models [[107](#bib.bib107), [134](#bib.bib134)].
    Thus comes the doubt of how the best misaligment solution should be used to mitigate
    the problem adding no overhead on the computational cost of the training process.
    This problem can become a search point by itself, such as alignment methods to
    RAW and RGB images of differents cameras, facilitating the creation of new datasets
    and improving the performance of the new works.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集[[31](#bib.bib31), [39](#bib.bib39)] 包含由不同相机捕捉的配对图像，其中这些数据集的主要目标之一是学习高质量相机的特征，并将其应用于具有更多限制的相机。由于设备之间的差异，这些数据集在图像补丁配对方面存在对齐问题。RAW
    到 RGB 映射可能具有挑战性，因为补丁像素与其像素不对应。简单的解决方案是对齐这些配对图像，但如何做到这一点是主要问题。张等人[[40](#bib.bib40)]
    提出使用深度学习方法，超越了用不对齐数据集训练的模型。许多工作[[39](#bib.bib39), [35](#bib.bib35), [31](#bib.bib31),
    [36](#bib.bib36)] 采用的另一种方式是调整损失函数，以不惩罚生成图像与真实值不对齐的模型。用试图接近人类感知的损失训练的模型对不对齐更具不变性，并生成更多细节的图像。目前，最接近这种方法的度量指标使用了深度学习技术，利用预训练深度学习模型提取的图像特征[[107](#bib.bib107),
    [134](#bib.bib134)]。由此产生了如何使用最佳不对齐解决方案来减轻问题而不增加训练过程的计算成本的疑问。这个问题本身可以成为一个研究点，例如不同相机的
    RAW 和 RGB 图像的对齐方法，促进新数据集的创建并提高新工作的性能。
- en: 6.2 Methodologies
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 方法论
- en: Seen as a point that helps share knowledge and the research field evolution
    is the justification of each stage, from architecture construction to validation
    of the work or the choice of datasets. Nevertheless, some works do not do it.
    For example, Liu et al. [[33](#bib.bib33)] do not explain well why the use of
    some datasets and Liu et al.[[26](#bib.bib26)] do not indicate the motivation
    to use the U-Net model. Besides, many methods do not make the source code available
    in its paper [[22](#bib.bib22), [24](#bib.bib24), [23](#bib.bib23), [3](#bib.bib3),
    [25](#bib.bib25), [28](#bib.bib28)], making it difficult for future validations
    and comparisons with other methods and making the validation less reliable.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 被视为帮助分享知识和研究领域演变的一个关键点是每个阶段的正当性，从架构建设到工作的验证或数据集的选择。然而，一些工作并没有做到这一点。例如，刘等人[[33](#bib.bib33)]
    没有很好地解释为何使用某些数据集，而刘等人[[26](#bib.bib26)] 也没有说明使用 U-Net 模型的动机。此外，许多方法没有在其论文中提供源代码[[22](#bib.bib22),
    [24](#bib.bib24), [23](#bib.bib23), [3](#bib.bib3), [25](#bib.bib25), [28](#bib.bib28)]，这使得未来的验证和与其他方法的比较变得困难，并使得验证的可靠性降低。
- en: 6.3 Evaluation Protocols
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 评估协议
- en: Train models using synthesized RAW image data when RAW and RGB images are scarce
    in the dataset is a good way to increase generalization. However, this cannot
    be a great option to validate methods proposed to map ISP because the synthesized
    RAW image data results from interpolation of RGB processed images, has pixels
    with less information, and the resulting noises of camera hardware are complex
    and hard to generate. Nevertheless, many works use datasets created for tasks
    like denoising, deblurring, or super-resolution with the generation of RAW images
    from these datasets that disbelieve the applicability of these methods in the
    real world. This problem raises the question of how to create RAW to RGB datasets
    with good representation and high quality for further research.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合成的RAW图像数据进行模型训练，当数据集中RAW和RGB图像稀缺时，是一种提高泛化能力的好方法。然而，这不能成为验证映射ISP方法的一个好选项，因为合成的RAW图像数据是通过插值处理RGB图像生成的，包含的信息较少，且相机硬件的噪声复杂且难以生成。尽管如此，许多工作使用为去噪、去模糊或超分辨率等任务创建的数据集，并从这些数据集中生成RAW图像，这些数据集对于这些方法在现实世界中的适用性产生了怀疑。这个问题引发了如何创建具有良好表现和高质量的RAW到RGB数据集以供进一步研究的疑问。
- en: 'On the other hand, few works tested the models in embedded systems or mobile
    devices. This can be a problem because most applications of ISPs are in devices
    with low computational capacity, and the fault of validation in this way can not
    prove the possible use in real applications. In view of this, the Mobile 2021
    Challenge released the task: "Learned Smartphone ISP on Mobile NPUs with Deep
    Learning."[[39](#bib.bib39)] It considers the processing and time of execution
    in mobile NPUs and the image quality like the other two previous challenges. This
    proposal can encourage future works to care more about these points.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，很少有工作在嵌入式系统或移动设备上测试模型。这可能是一个问题，因为大多数ISP应用都在计算能力较低的设备上，未能在这种方式下验证的缺陷不能证明其在实际应用中的可能使用。鉴于此，Mobile
    2021挑战赛发布了任务：“在移动NPU上使用深度学习进行学习型智能手机ISP。”[[39](#bib.bib39)] 该任务考虑了移动NPU中的处理和执行时间，以及图像质量，就像之前的两个挑战赛一样。这项提案可以鼓励未来的工作更多地关注这些问题。
- en: 7 Conclusion
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: The ISP pipeline is an important combination of techniques, essential for the
    creation of quality digital images from camera sensors. This survey provided an
    in-depth research on the applications of deep learning techniques to ISP tasks,
    regarding the application of networks for solving partial steps or the complete
    pipeline. Additionally, it also provided an introduction to both ISP and deep
    learning areas, alongside with a detailed overview of software ISP, regarding
    its fundamentals and individual steps.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ISP管道是一个重要的技术组合，对于从相机传感器创建高质量数字图像至关重要。本次调查提供了对深度学习技术在ISP任务中的应用的深入研究，涉及解决部分步骤或完整管道的网络应用。此外，它还介绍了ISP和深度学习领域，并详细概述了软件ISP的基础和各个步骤。
- en: The works surveyed in this paper were selected based on their novelty, their
    target task, and the applied deep learning techniques. Among the 27 reviewed papers,
    $30\%$ had applied DNNs for replacing the complete ISP pipeline. This reveals
    a new trend that explores the generalization ability of CNNs to learn all the
    ISP individual tasks, aiming to apply all of them in a single forward operation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 本文调查的工作是根据其创新性、目标任务和应用的深度学习技术进行选择的。在27篇审阅的论文中，$30\%$应用了DNNs来替代完整的ISP管道。这揭示了一个新趋势，探索CNNs学习所有ISP单独任务的泛化能力，旨在将所有任务应用于单次前向操作中。
- en: 'Furthermore, this work also summarized five datasets often used for ISP tasks:
    Urban 100 [[84](#bib.bib84)], ZRR dataset [[133](#bib.bib133)], Kodak⁸⁸8http://r0k.us/graphics/kodak/,
    and McMaster dataset [[83](#bib.bib83)]. The availability of quality datasets
    is necessary for the research and development of new solutions to any deep learning
    application area. In this scenario, new ISP-related datasets can improve some
    limitations existent in the surveyed datasets.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这项工作还总结了五个经常用于ISP任务的数据集：Urban 100 [[84](#bib.bib84)]、ZRR数据集 [[133](#bib.bib133)]、Kodak⁸⁸8http://r0k.us/graphics/kodak/和McMaster数据集 [[83](#bib.bib83)]。高质量数据集的可用性对于研究和开发新的深度学习应用解决方案是必要的。在这种情况下，新的ISP相关数据集可以改善现有数据集中的一些局限性。
- en: 'Finally, during the development of this survey, some critical points were detected
    and are highlighted below:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本次调查的开发过程中，检测到了一些关键点，并在下文中进行了突出显示：
- en: •
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The use of different cameras for producing RAW-to-RGB datasets creates alignment
    issues that require the application of additional techniques during the training
    of DNNs. These mitigators can add computational costs and interference in the
    overall performance of the networks.
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用不同的相机生成RAW到RGB数据集会导致对齐问题，这要求在DNN训练过程中应用额外的技术。这些缓解措施可能增加计算成本并干扰网络的整体性能。
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Better ablation studies, regarding the effect of each architectural component
    present in proposed solutions, alongside with the distribution of the source code,
    can facilitate the future development of new techniques and the overall improvement
    of the research area.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更好的消融研究，涉及提出的解决方案中每个架构组件的影响，以及源代码的分发，可以促进新技术的未来发展和研究领域的整体改进。
- en: •
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As more approaches are proposed to the ISP pipeline replacement task, a more
    consistent evaluation procedure, alongside with the definition of common target
    datasets, will facilitate the comparison between methods, helping to define state-of-the-art
    performance.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着更多方法被提出用于ISP管道替代任务，更一致的评估程序以及通用目标数据集的定义，将有助于方法间的比较，帮助定义最先进的性能。
- en: •
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Besides being one of the main target applications, mobile application for networks
    that replace the complete ISP pipeline are not often discussed. A more in-depth
    evaluation of the proposed methods performance on edge devices is important to
    identify components that can be optimized or new techniques focused on these target
    environments.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了作为主要目标应用之一，完全替代ISP管道的网络移动应用不常被讨论。对提出的方法在边缘设备上的性能进行更深入的评估，重要的是识别可以优化的组件或针对这些目标环境的新技术。
- en: 'As future work, we intend to pursue two main approaches to Deep Learning-based
    ISP applications:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 作为未来的工作，我们打算采用两种主要方法来研究基于深度学习的ISP应用：
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The development of a complete ISP pipeline network focused on execution in a
    Raspberry Pi board, aiming to explore the deployment on edge challenge.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开发一个专注于在Raspberry Pi板上执行的完整ISP管道网络，旨在探索边缘计算挑战。
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The application of vision transformers to the complete ISP task, aiming to explore
    the good results achieved by transformers in other computer vision tasks.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视觉变换器在完整ISP任务中的应用，旨在探索变换器在其他计算机视觉任务中取得的良好结果。
- en: References
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Rajeev Ramanath, Wesley E Snyder, Youngjun Yoo, and Mark S Drew. Color
    image processing pipeline. IEEE Signal Processing Magazine, 22(1):34–43, 2005.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Rajeev Ramanath, Wesley E Snyder, Youngjun Yoo, 和 Mark S Drew. 彩色图像处理管道。IEEE信号处理杂志,
    22(1):34–43, 2005。'
- en: '[2] Yu Zhu, Zhenyu Guo, Tian Liang, Xiangyu He, Chenghua Li, Cong Leng, Bo Jiang,
    Yifan Zhang, and Jian Cheng. Eednet: Enhanced encoder-decoder network for autoisp.
    In Adrien Bartoli and Andrea Fusiello, editors, Computer Vision – ECCV 2020 Workshops,
    pages 171–184, Cham, 2020\. Springer International Publishing.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yu Zhu, Zhenyu Guo, Tian Liang, Xiangyu He, Chenghua Li, Cong Leng, Bo
    Jiang, Yifan Zhang, 和 Jian Cheng. Eednet: 增强型编码解码网络用于autoisp。在Adrien Bartoli和Andrea
    Fusiello编辑的《计算机视觉 – ECCV 2020研讨会》中，第171–184页，Cham，2020年。施普林格国际出版社。'
- en: '[3] Sivalogeswaran Ratnasingam. Deep camera: A fully convolutional neural network
    for image signal processing, 2019.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Sivalogeswaran Ratnasingam. 深度相机：用于图像信号处理的完全卷积神经网络，2019。'
- en: '[4] F. Rosenblatt. The perceptron: A probabilistic model for information storage
    and organization in the brain. Psychological Review, pages 65–386, 1958.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] F. Rosenblatt. 感知机：一种用于信息存储和组织的大脑概率模型。心理学评论, 第65–386页, 1958。'
- en: '[5] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent
    Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W R Nelson, Alex Bridgland,
    Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli,
    David T Jones, David Silver, Koray Kavukcuoglu, and Demis Hassabis. Improved protein
    structure prediction using potentials from deep learning. Nature, 577(7792):706–710,
    January 2020.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent
    Sifre, Tim Green, Chongli Qin, Augustin Žídek, Alexander W R Nelson, Alex Bridgland,
    Hugo Penedones, Stig Petersen, Karen Simonyan, Steve Crossan, Pushmeet Kohli,
    David T Jones, David Silver, Koray Kavukcuoglu, 和 Demis Hassabis. 利用深度学习的潜力改进蛋白质结构预测。自然,
    577(7792):706–710, 2020年1月。'
- en: '[6] Padideh Danaee, Reza Ghaeini, and David A Hendrix. A deep learning approach
    for cancer detection and relevant gene identification. Pac. Symp. Biocomput.,
    22:219–229, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Padideh Danaee, Reza Ghaeini, 和 David A Hendrix. 一种用于癌症检测和相关基因识别的深度学习方法。太平洋生物计算研讨会,
    22:219–229, 2017。'
- en: '[7] Hugo Storm, Kathy Baylis, and Thomas Heckelei. Machine learning in agricultural
    and applied economics. Eur. Rev. Agric. Econ., 47(3):849–892, June 2020.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Hugo Storm、Kathy Baylis 和 Thomas Heckelei。农业和应用经济学中的机器学习。Eur. Rev. Agric.
    Econ.，47(3):849–892，2020年6月。'
- en: '[8] Ammar Askar, Abbas Askar, Mario Pasquato, and Mirek Giersz. Finding black
    holes with black boxes – using machine learning to identify globular clusters
    with black hole subsystems. Mon. Not. R. Astron. Soc., 485(4):5345–5362, June
    2019.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Ammar Askar、Abbas Askar、Mario Pasquato 和 Mirek Giersz。利用机器学习寻找黑洞：识别具有黑洞子系统的球状星团。Mon.
    Not. R. Astron. Soc.，485(4):5345–5362，2019年6月。'
- en: '[9] Faustino J. Gomez. Co-evolving recurrent neurons learn deep memory pomdps.
    In InGECCO-05: Proceedings of the Genetic and Evolutionary Computation Conference,
    pages 491–498.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Faustino J. Gomez。共同进化递归神经元学习深度记忆 POMDPs。见 InGECCO-05: Proceedings of the
    Genetic and Evolutionary Computation Conference，第491–498页。'
- en: '[10] Juergen Schmidhuber. Deep learning. Scholarpedia J., 10(11):32832, 2015.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Juergen Schmidhuber。深度学习。Scholarpedia J.，10(11):32832，2015。'
- en: '[11] K Chen, V Kvasnicka, P C Kanen, and S Haykin. Multi-valued and universal
    binary neurons: Theory, learning, and applications [book review]. IEEE Trans.
    Neural Netw., 12(3):647–647, May 2001.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] K Chen、V Kvasnicka、P C Kanen 和 S Haykin。多值和通用二值神经元：理论、学习和应用 [书评]。IEEE
    Trans. Neural Netw.，12(3):647–647，2001年5月。'
- en: '[12] Y. Bengio and Yann Lecun. Convolutional networks for images, speech, and
    time-series. 11 1997.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Y. Bengio 和 Yann Lecun。用于图像、语音和时间序列的卷积网络。11 1997。'
- en: '[13] K Fukushima. Neocognitron: a self organizing neural network model for
    a mechanism of pattern recognition unaffected by shift in position. Biol. Cybern.,
    36(4):193–202, 1980.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K Fukushima。Neocognitron：一种自组织神经网络模型，用于处理不受位置变化影响的模式识别机制。Biol. Cybern.，36(4):193–202，1980。'
- en: '[14] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and
    Alan L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets,
    atrous convolution, and fully connected crfs, 2017.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Liang-Chieh Chen、George Papandreou、Iasonas Kokkinos、Kevin Murphy 和 Alan
    L. Yuille。Deeplab：使用深度卷积网络、空洞卷积和全连接 CRF 的语义图像分割，2017。'
- en: '[15] Andreas Kamilaris and Francesc X. Prenafeta-Boldú. Deep learning in agriculture:
    A survey. Computers and Electronics in Agriculture, 147:70–90, April 2018.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Andreas Kamilaris 和 Francesc X. Prenafeta-Boldú。农业中的深度学习：调查。Computers
    and Electronics in Agriculture，147:70–90，2018年4月。'
- en: '[16] Daniel Berman, Anna Buczak, Jeffrey Chavis, and Cherita Corbett. A survey
    of deep learning methods for cyber security. Information, 10(4):122, April 2019.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Daniel Berman、Anna Buczak、Jeffrey Chavis 和 Cherita Corbett。深度学习方法在网络安全中的调查。Information，10(4):122，2019年4月。'
- en: '[17] Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.
    A survey of deep learning techniques for autonomous driving. Journal of Field
    Robotics, 37(3):362–386, April 2020.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Sorin Grigorescu、Bogdan Trasnea、Tiberiu Cocias 和 Gigel Macesanu。用于自动驾驶的深度学习技术调查。Journal
    of Field Robotics，37(3):362–386，2020年4月。'
- en: '[18] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso
    Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van
    Ginneken, and Clara I. Sánchez. A survey on deep learning in medical image analysis.
    Medical Image Analysis, 42:60–88, December 2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Geert Litjens、Thijs Kooi、Babak Ehteshami Bejnordi、Arnaud Arindra Adiyoso
    Setio、Francesco Ciompi、Mohsen Ghafoorian、Jeroen A.W.M. van der Laak、Bram van Ginneken
    和 Clara I. Sánchez。医学图像分析中的深度学习调查。Medical Image Analysis，42:60–88，2017年12月。'
- en: '[19] Zewen Li, Wenjie Yang, Shouheng Peng, and Fan Liu. A survey of convolutional
    neural networks: Analysis, applications, and prospects, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Zewen Li、Wenjie Yang、Shouheng Peng 和 Fan Liu。卷积神经网络的调查：分析、应用和前景，2020。'
- en: '[20] Xin Li, Bahadir Gunturk, and Lei Zhang. Image demosaicing: a systematic
    survey. In William A. Pearlman, John W. Woods, and Ligang Lu, editors, Visual
    Communications and Image Processing 2008. SPIE, January 2008.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Xin Li、Bahadir Gunturk 和 Lei Zhang。图像去马赛克：系统调查。见 William A. Pearlman、John
    W. Woods 和 Ligang Lu 编辑，Visual Communications and Image Processing 2008。SPIE，2008年1月。'
- en: '[21] Linwei Fan, Fan Zhang, Hui Fan, and Caiming Zhang. Brief review of image
    denoising techniques. Visual Computing for Industry, Biomedicine, and Art, 2(1),
    July 2019.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Linwei Fan、Fan Zhang、Hui Fan 和 Caiming Zhang。图像去噪技术的简要回顾。Visual Computing
    for Industry, Biomedicine, and Art，2(1)，2019年7月。'
- en: '[22] Kangfu Mei, Juncheng Li, Jiajie Zhang, Haoyu Wu, Jie Li, and Rui Huang.
    Higher-resolution network for image demosaicing and enhancing, 2019.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Kangfu Mei、Juncheng Li、Jiajie Zhang、Haoyu Wu、Jie Li 和 Rui Huang。用于图像去马赛克和增强的高分辨率网络，2019。'
- en: '[23] Zhetong Liang, Jianrui Cai, Zisheng Cao, and Lei Zhang. Cameranet: A two-stage
    framework for effective camera isp learning, 2019.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Zhetong Liang、Jianrui Cai、Zisheng Cao 和 Lei Zhang。Cameranet：一个用于有效摄像头
    ISP 学习的双阶段框架，2019。'
- en: '[24] Bumjun Park and Jechang Jeong. Color filter array demosaicking using densely
    connected residual network. In Color Filter Array Demosaicking Using Densely Connected
    Residual Network, 2019.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 朴凡俊、郑哲昌。使用密集连接残差网络的彩色滤波阵列去马赛克。在《使用密集连接残差网络的彩色滤波阵列去马赛克》，2019年。'
- en: '[25] Vahid Patrovi Nia Ramchalam Kinattinkara Ramakrishnan, Shangling Jui.
    Deep demosaicing for edge implementation, 2019.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 瓦希德·帕特罗维、尼亚·拉姆查拉姆、金纳廷卡拉·拉马克里什南、商凌·崔。用于边缘实现的深度去马赛克，2019年。'
- en: '[26] Yuzhi Wang Qin Xu Yuqian Zhou Haibin Huang Chuan Wang Shaofan Cai Yifan
    Ding Haoqiang Fan Jue Wang Jiaming Liu, Chi-Hao Wu. Learning raw image denoising
    with bayer pattern unification and bayer preserving augmentation, 2019.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 王裕之、徐秦、周宇乾、黄海滨、王川、蔡少凡、丁一凡、范浩强、王珏、刘嘉铭、吴志豪。通过贝叶尔模式统一和贝叶尔保持增强学习原始图像去噪，2019年。'
- en: '[27] Sushma Rao Bhavin Nayak Timo Gerasimow Aleksandar Sutic Liron Ain-kedem
    Gilad Michael O Chyuan-Tyng Wu, Leo F. Isikdogan. Visionisp: Repurposing the image
    signal processor for computer vision applications, 2019.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 苏什玛·拉奥、巴文·纳亚克、蒂莫·杰拉西莫夫、亚历山大·苏提奇、利龙·安-凯德姆、吉拉德·迈克尔·奥·蔡元廷、吴、里奥·F·伊斯克多甘。Visionisp：将图像信号处理器重新用于计算机视觉应用，2019年。'
- en: '[28] Yu Guo, Qiyu Jin, Gabriele Facciolo, Tieyong Zeng, and Jean-Michel Morel.
    Residual learning for effective joint demosaicing-denoising, 2020.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 顾宇、金琪宇、加布里埃尔·法乔洛、曾铁永、让-米歇尔·莫雷尔。有效的联合去马赛克-去噪的残差学习，2020年。'
- en: '[29] Irina Kim, Seongwook Song, Soonkeun Chang, Sukhwan Lim, and Kai Guo. Deep
    image demosaicing for submicron image sensors. Electronic Imaging, 2020(7):60410–1,
    2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 伊琳娜·金、宋成旭、张顺奎、林锡焕、郭凯。针对亚微米图像传感器的深度图像去马赛克。《电子成像》，2020年（7）：60410–1，2020年。'
- en: '[30] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz
    Khan, Ming-Hsuan Yang, and Ling Shao. Cycleisp: Real image restoration via improved
    data synthesis, 2020.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 西耶德·瓦卡斯·扎米尔、阿迪提亚·阿罗拉、萨尔曼·汗、穆纳瓦尔·哈亚特、法赫德·沙赫巴兹·汗、杨名轩、邵凌。Cycleisp：通过改进的数据合成实现真实图像恢复，2020年。'
- en: '[31] Andrey Ignatov, Luc Van Gool, and Radu Timofte. Replacing mobile camera
    isp with a single deep learning model. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops, pages 536–537, 2020.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 安德烈·伊格纳托夫、卢克·范·古尔、拉杜·蒂莫夫特。用单一深度学习模型替代移动摄像头ISP。在IEEE/CVF计算机视觉与模式识别会议研讨会论文集中，第536–537页，2020年。'
- en: '[32] Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, and JaeHyun Baek. Pynet-ca:
    Enhanced pynet with channel attention for end-to-end mobile image signal processing.
    Lecture Notes in Computer Science, page 202–212, 2020.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 金秉勋、宋俊英、郑哲镐、白载贤。Pynet-ca：具有通道注意力的增强Pynet，用于端到端的移动图像信号处理。《计算机科学讲义》，第202–212页，2020年。'
- en: '[33] Lin Liu, Xu Jia, Jianzhuang Liu, and Qi Tian. Joint demosaicing and denoising
    with self guidance. In 2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 2237–2246, 2020.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 刘林、贾旭、刘建壮、田奇。自指导的联合去马赛克和去噪。在2020年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集中，第2237–2246页，2020年。'
- en: '[34] Shuyang Sun, Liang Chen, Gregory Slabaugh, and Philip Torr. Learning to
    sample the most useful training patches from images. arXiv preprint arXiv:2011.12097,
    2020.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 孙帅阳、陈梁、格雷戈里·斯拉巴赫、菲利普·托尔。学习从图像中采样最有用的训练补丁。arXiv预印本arXiv:2011.12097，2020年。'
- en: '[35] Linhui Dai, Xiaohong Liu, Chengqi Li, and Jun Chen. Awnet: Attentive wavelet
    network for image isp, 2020.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 戴林辉、刘晓红、李承奇、陈军。Awnet：用于图像ISP的注意力波纹网络，2020年。'
- en: '[36] Saumya Gupta, Diplav Srivastava, Umang Chaturvedi, Anurag Jain, and Gaurav
    Khandelwal. Del-net: A single-stage network for mobile camera isp, 2021.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 萨乌米亚·古普塔、迪普拉夫·斯里瓦斯塔瓦、乌芒·查图尔维迪、阿努拉格·贾因、戈拉夫·坎德瓦尔。Del-net：用于移动摄像头ISP的单阶段网络，2021年。'
- en: '[37] Yazhou Xing, Zian Qian, and Qifeng Chen. Invertible image signal processing,
    2021.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 邢亚舟、钱子安、陈启峰。可逆图像信号处理，2021年。'
- en: '[38] Kwang-Hyun Uhm, Kyuyeon Choi, Seung-Won Jung, and Sung-Jea Ko. Image compression-aware
    deep camera isp network. IEEE Access, 9:137824–137832, 2021.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 岩光贤、崔奎元、郑承源、许成宰。图像压缩感知深度摄像头ISP网络。《IEEE Access》，第9卷：137824–137832，2021年。'
- en: '[39] Andrey Ignatov, Cheng-Ming Chiang, Hsien-Kai Kuo, Anastasia Sycheva, Radu
    Timofte, Min-Hung Chen, Man-Yu Lee, Yu-Syuan Xu, Yu Tseng, Shusong Xu, Jin Guo,
    Chao-Hung Chen, Ming-Chun Hsyu, Wen-Chia Tsai, Chao-Wei Chen, Grigory Malivenko,
    Minsu Kwon, Myungje Lee, Jaeyoon Yoo, Changbeom Kang, Shinjo Wang, Zheng Shaolong,
    Hao Dejun, Xie Fen, Feng Zhuang, Yipeng Ma, Jingyang Peng, Tao Wang, Fenglong
    Song, Chih-Chung Hsu, Kwan-Lin Chen, Mei-Hsuang Wu, Vishal Chudasama, Kalpesh
    Prajapati, Heena Patel, Anjali Sarvaiya, Kishor Upla, Kiran Raja, Raghavendra
    Ramachandra, Christoph Busch, and Etienne de Stoutz. Learned smartphone isp on
    mobile npus with deep learning, mobile ai 2021 challenge: Report, 2021.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Andrey Ignatov, Cheng-Ming Chiang, Hsien-Kai Kuo, Anastasia Sycheva, Radu
    Timofte, Min-Hung Chen, Man-Yu Lee, Yu-Syuan Xu, Yu Tseng, Shusong Xu, Jin Guo,
    Chao-Hung Chen, Ming-Chun Hsyu, Wen-Chia Tsai, Chao-Wei Chen, Grigory Malivenko,
    Minsu Kwon, Myungje Lee, Jaeyoon Yoo, Changbeom Kang, Shinjo Wang, Zheng Shaolong,
    Hao Dejun, Xie Fen, Feng Zhuang, Yipeng Ma, Jingyang Peng, Tao Wang, Fenglong
    Song, Chih-Chung Hsu, Kwan-Lin Chen, Mei-Hsuang Wu, Vishal Chudasama, Kalpesh
    Prajapati, Heena Patel, Anjali Sarvaiya, Kishor Upla, Kiran Raja, Raghavendra
    Ramachandra, Christoph Busch, 和 Etienne de Stoutz。基于深度学习的智能手机ISP在移动NPU上的应用，移动AI
    2021挑战：报告，2021年。'
- en: '[40] Zhilu Zhang, Haolin Wang, Ming Liu, Ruohao Wang, Jiawei Zhang, and Wangmeng
    Zuo. Learning raw-to-srgb mappings with inaccurately aligned supervision, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Zhilu Zhang, Haolin Wang, Ming Liu, Ruohao Wang, Jiawei Zhang, 和 Wangmeng
    Zuo。通过不准确对齐的监督学习原始图像到sRGB的映射，2021年。'
- en: '[41] Guocheng Qian, Yuanhao Wang, Chao Dong, Jimmy S. Ren, Wolfgang Heidrich,
    Bernard Ghanem, and Jinjin Gu. Rethinking the pipeline of demosaicing, denoising
    and super-resolution, 2021.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Guocheng Qian, Yuanhao Wang, Chao Dong, Jimmy S. Ren, Wolfgang Heidrich,
    Bernard Ghanem, 和 Jinjin Gu。重新思考去马赛克、去噪和超分辨率的管道，2021年。'
- en: '[42] Ke Yu, Zexian Li, Yue Peng, Chen Change Loy, and Jinwei Gu. Reconfigisp:
    Reconfigurable camera image processing pipeline, 2021.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Ke Yu, Zexian Li, Yue Peng, Chen Change Loy, 和 Jinwei Gu。ReconfigISP：可重配置相机图像处理管道，2021年。'
- en: '[43] Eli Schwartz, Alex Bronstein, and Raja Giryes. Isp distillation, 2021.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Eli Schwartz, Alex Bronstein, 和 Raja Giryes。ISP蒸馏，2021年。'
- en: '[44] Prashant Chaudhari, Franziska Schirrmacher, Andreas Maier, Christian Riess,
    and Thomas Köhler. Merging-isp: Multi-exposure high dynamic range image signal
    processing, 2021.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Prashant Chaudhari, Franziska Schirrmacher, Andreas Maier, Christian Riess,
    和 Thomas Köhler。Merging-ISP：多曝光高动态范围图像信号处理，2021年。'
- en: '[45] Zhetong Liang Shi Guo and Lei Zhang. Joint denoising and demosaicking
    with green channel prior for real-world burst images, 2021.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Zhetong Liang Shi Guo 和 Lei Zhang。带有绿色通道先验的联合去噪和去马赛克用于实际世界的连续图像，2021年。'
- en: '[46] SM A Sharif, Rizwan Ali Naqvi, and Mithun Biswas. Beyond joint demosaicking
    and denoising: An image processing pipeline for a pixel-bin image sensor. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 233–242,
    2021.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] SM A Sharif, Rizwan Ali Naqvi, 和 Mithun Biswas。超越联合去马赛克和去噪：一种用于像素-bin图像传感器的图像处理管道。在IEEE/CVF计算机视觉与模式识别会议论文集中，第233–242页，2021年。'
- en: '[47] Sean Moran, Steven McDonagh, and Gregory Slabaugh. Curl: Neural curve
    layers for global image enhancement. In 2020 25th International Conference on
    Pattern Recognition (ICPR), pages 9796–9803\. IEEE, 2021.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Sean Moran, Steven McDonagh, 和 Gregory Slabaugh。Curl：用于全局图像增强的神经曲线层。在2020年第25届国际模式识别大会（ICPR）上，第9796–9803页。IEEE，2021年。'
- en: '[48] Peter Wilson. Chapter 7 - high speed video application. In Peter Wilson,
    editor, Design Recipes for FPGAs (Second Edition), pages 67–77\. Newnes, Oxford,
    second edition edition, 2016.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Peter Wilson. 第7章 - 高速视频应用。在 Peter Wilson 主编的《FPGA设计秘诀（第二版）》中，第67–77页。Newnes，牛津，第二版，2016年。'
- en: '[49] Hakki Can Karaimer and Michael S. Brown. A software platform for manipulating
    the camera imaging pipeline. In European Conference on Computer Vision (ECCV),
    2016.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Hakki Can Karaimer 和 Michael S. Brown。用于操控相机成像管道的软件平台。在欧洲计算机视觉会议（ECCV），2016年。'
- en: '[50] Liu Yongji and Yuan Xiaojun. A design of dynamic defective pixel correction
    for image sensor. In 2020 IEEE International Conference on Artificial Intelligence
    and Information Systems (ICAIIS), pages 713–716, 2020.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Liu Yongji 和 Yuan Xiaojun。用于图像传感器的动态缺陷像素修正设计。在2020年IEEE国际人工智能与信息系统会议（ICAIIS）上，第713–716页，2020年。'
- en: '[51] Georgi Zapryanov, Ivanova, and Iva Nikolova. Automatic white balance algorithms
    for digital still cameras - a comparative study. Information Technologies and
    Control, 1:16–22, 01 2012.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Georgi Zapryanov, Ivanova, 和 Iva Nikolova。数字单反相机的自动白平衡算法 - 比较研究。《信息技术与控制》，1:16–22，2012年1月。'
- en: '[52] K. Hirakawa and T.W. Parks. Adaptive homogeneity-directed demosaicing
    algorithm. IEEE Transactions on Image Processing, 14(3):360–369, 2005.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] K. Hirakawa 和 T.W. Parks。自适应同质性导向去马赛克算法。《IEEE图像处理学报》，14(3):360–369，2005年。'
- en: '[53] Mukesh Motwani, Mukesh Gadiya, Rakhi Motwani, and Frederick Harris. Survey
    of image denoising techniques. 01 2004.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Mukesh Motwani、Mukesh Gadiya、Rakhi Motwani 和 Frederick Harris. 图像去噪技术调查。2004年1月。'
- en: '[54] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A review of image
    denoising algorithms, with a new one. Multiscale modeling & simulation, 4(2):490–530,
    2005.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Antoni Buades、Bartomeu Coll 和 Jean-Michel Morel. 图像去噪算法综述及新算法。多尺度建模与仿真，第4卷第2期：490–530，2005年。'
- en: '[55] Dr.Sabeenian R.S. A survey on image denoising algorithms’ (ida),. Science,
    Measurement and Technology, IEE Proceedings A, 1:456–462, 11 2012.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Dr.Sabeenian R.S. 关于图像去噪算法的调查（ida），. 科学、测量与技术，IEE 会议论文集 A，第1期：456–462，2012年11月。'
- en: '[56] Chunwei Tian, Lunke Fei, Wenxian Zheng, Yong Xu, Wangmeng Zuo, and Chia-Wen
    Lin. Deep learning on image denoising: An overview, 2020.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Chunwei Tian、Lunke Fei、Wenxian Zheng、Yong Xu、Wangmeng Zuo 和 Chia-Wen Lin.
    图像去噪的深度学习：概述，2020年。'
- en: '[57] Saeed Anwar and Nick Barnes. Real image denoising with feature attention,
    2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Saeed Anwar 和 Nick Barnes. 使用特征关注的真实图像去噪，2020年。'
- en: '[58] Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet,
    and Jonathan T. Barron. Unprocessing images for learned raw denoising, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Tim Brooks、Ben Mildenhall、Tianfan Xue、Jiawen Chen、Dillon Sharlet 和 Jonathan
    T. Barron. 为学习的原始去噪处理图像，2018年。'
- en: '[59] Hanlin Tan, Xiangrong Zeng, Shiming Lai, Yu Liu, and Maojun Zhang. Joint
    demosaicing and denoising of noisy bayer images with admm. In 2017 IEEE International
    Conference on Image Processing (ICIP), pages 2951–2955, 2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Hanlin Tan、Xiangrong Zeng、Shiming Lai、Yu Liu 和 Maojun Zhang. 使用 ADMM 的噪声拜耳图像的联合去马赛克与去噪。见
    2017 IEEE 国际图像处理会议（ICIP），页码 2951–2955，2017年。'
- en: '[60] Michaël Gharbi, Gaurav Chaurasia, Sylvain Paris, and Frédo Durand. Deep
    joint demosaicking and denoising. ACM Trans. Graph., 35(6), November 2016.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Michaël Gharbi、Gaurav Chaurasia、Sylvain Paris 和 Frédo Durand. 深度联合去马赛克与去噪。ACM
    Trans. Graph., 第35卷第6期，2016年11月。'
- en: '[61] Boon Tatt Koik and Haidi Ibrahim. A literature survey on blur detection
    algorithms for digital imaging. 2013 1st International Conference on Artificial
    Intelligence, Modelling and Simulation, pages 272–277, 2013.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Boon Tatt Koik 和 Haidi Ibrahim. 数字成像模糊检测算法的文献综述。2013年第一届人工智能、建模与仿真国际会议，页码
    272–277，2013年。'
- en: '[62] Fagun Vankawala, Amit Ganatra, and Amit Patel. A survey on different image
    deblurring techniques. International Journal of Computer Applications, 116:15–18,
    2015.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Fagun Vankawala、Amit Ganatra 和 Amit Patel. 关于不同图像去模糊技术的调查。国际计算机应用期刊，第116卷：15–18，2015年。'
- en: '[63] Qolamreza Razlighi and Nasser Kehtarnavaz. Imaeg blur reduction for cell-phone
    cameras via adaptive tonal correction. 1:I – 113, 09 2007.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Qolamreza Razlighi 和 Nasser Kehtarnavaz. 通过自适应色调校正减少手机相机的图像模糊。第1卷：I –
    113，2007年9月。'
- en: '[64] Zhe Hu, Lu Yuan, Stephen Ching-Feng Lin, and Ming-Hsuan Yang. Image deblurring
    using smartphone inertial sensors. 2016 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), pages 1855–1864, 2016.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Zhe Hu、Lu Yuan、Stephen Ching-Feng Lin 和 Ming-Hsuan Yang. 使用智能手机惯性传感器的图像去模糊。2016
    IEEE 计算机视觉与模式识别会议（CVPR），页码 1855–1864，2016年。'
- en: '[65] Siddhant Sahu, Manoj Kumar Lenka, and Pankaj Kumar Sa. Blind deblurring
    using deep learning: A survey. ArXiv, abs/1907.10128, 2019.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Siddhant Sahu、Manoj Kumar Lenka 和 Pankaj Kumar Sa. 使用深度学习的盲去模糊：综述。ArXiv，abs/1907.10128，2019年。'
- en: '[66] Juncheng Li, Faming Fang, Kangfu Mei, and Guixu Zhang. Multi-scale residual
    network for image super-resolution. In ECCV, 2018.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Juncheng Li、Faming Fang、Kangfu Mei 和 Guixu Zhang. 用于图像超分辨率的多尺度残差网络。见 ECCV，2018年。'
- en: '[67] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu.
    Image super-resolution using very deep residual channel attention networks, 2018.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Yulun Zhang、Kunpeng Li、Kai Li、Lichen Wang、Bineng Zhong 和 Yun Fu. 使用非常深的残差通道注意力网络的图像超分辨率，2018年。'
- en: '[68] Samuel W. Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T.
    Barron, Florian Kainz, Jiawen Chen, and Marc Levoy. Burst photography for high
    dynamic range and low-light imaging on mobile cameras. ACM Transactions on Graphics
    (TOG), 35:1–12, 2016.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Samuel W. Hasinoff、Dillon Sharlet、Ryan Geiss、Andrew Adams、Jonathan T.
    Barron、Florian Kainz、Jiawen Chen 和 Marc Levoy. 移动相机的高动态范围和低光成像的高速摄影。ACM 图形学会会刊（TOG），第35期：1–12，2016年。'
- en: '[69] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fredo Durand. Learning
    photographic global tonal adjustment with a database of input / output image pairs.
    In CVPR 2011, pages 97–104, 2011.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Vladimir Bychkovsky、Sylvain Paris、Eric Chan 和 Fredo Durand. 使用输入/输出图像对数据库学习摄影全球色调调整。见
    CVPR 2011，页码 97–104，2011年。'
- en: '[70] Eli a Schwartz. Deepisp: Toward learning an end-to-end image processing
    pipeline. IEEE Transactions on Image Processing, 28(2).'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Eli a Schwartz. Deepisp：迈向端到端图像处理管道的学习。IEEE 图像处理学报，第28卷第2期。'
- en: '[71] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in
    the dark, 2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Chen Chen, Qifeng Chen, Jia Xu 和 Vladlen Koltun. 学习在黑暗中看见, 2018.'
- en: '[72] Florian Ciurea and Brian V. Funt. A large image database for color constancy
    research. In Color Imaging Conference, 2003.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Florian Ciurea 和 Brian V. Funt. 用于颜色恒常性研究的大型图像数据库. 在颜色成像会议上, 2003.'
- en: '[73] S. Ren K. He, X. Zhang and J. Sun. Deep residual learning for image recognition,
    2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Ren K. He, X. Zhang 和 J. Sun. 深度残差学习用于图像识别, 2016.'
- en: '[74] L. van der Maaten G. Huang, Z. Liu and K. Q. Weinberger. Densely connected
    convolutional networks. EEE Conf. Comput. Vis. Pattern Recognit., pages 2261–2269,
    2017.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] L. van der Maaten G. Huang, Z. Liu 和 K. Q. Weinberger. 密集连接卷积网络. EEE 计算机视觉与模式识别会议,
    页码 2261–2269, 2017.'
- en: '[75] Daniele Menon and Giancarlo Calvagno. Color image demosaicking: An overview.
    Signal Processing: Image Communication, 26, oct 2011.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Daniele Menon 和 Giancarlo Calvagno. 彩色图像去马赛克: 概述. 信号处理: 图像通信, 26, 2011年10月.'
- en: '[76] Michael S. Brown Abdelrahman Abdelhamed, Stephen Lin. A high-quality denoising
    dataset for smartphone cameras. 2018 IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, jun 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Michael S. Brown Abdelrahman Abdelhamed, Stephen Lin. 用于智能手机相机的高质量去噪数据集.
    2018 IEEE/CVF 计算机视觉与模式识别会议, 2018年6月.'
- en: '[77] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization.
    arXiv preprint arXiv:1711.05101, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Ilya Loshchilov 和 Frank Hutter. 解耦权重衰减正则化. arXiv 预印本 arXiv:1711.05101,
    2017.'
- en: '[78] Sushma Rao Aleksandar Sutic Chyuan-Tyng Wu Gilad Michael Jun Nishimura,
    Timo Gerasimow. Automatic isp image quality tuning using non-linear optimization,
    2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Sushma Rao Aleksandar Sutic Chyuan-Tyng Wu Gilad Michael Jun Nishimura,
    Timo Gerasimow. 使用非线性优化的自动 ISP 图像质量调整, 2019.'
- en: '[79] Raquel Urtasun Andreas Geiger, Philip Lenz. Are we ready for autonomous
    driving? the kitti vision benchmark suite. 2012 IEEE Conference on Computer Vision
    and Pattern Recognition, jun 2012.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Raquel Urtasun Andreas Geiger, Philip Lenz. 我们准备好自动驾驶了吗? KITTI 视觉基准套件.
    2012 IEEE 计算机视觉与模式识别会议, 2012年6月.'
- en: '[80] Forrest Iandola Peter H. Jin-Kurt Keutzer Bichen Wu, Alvin Wan. Squeezedet:
    Unified, small, low power fully convolutional neural networks for real-time object
    detection for autonomous driving, 2016.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Forrest Iandola Peter H. Jin-Kurt Keutzer Bichen Wu, Alvin Wan. Squeezedet:
    统一、小型、低功耗完全卷积神经网络用于实时物体检测和自动驾驶, 2016.'
- en: '[81] Ibrahim Pekkucuksen and Yucel Altunbasak. Gradient based threshold free
    color filter array interpolation. In 2010 IEEE International Conference on Image
    Processing, pages 137–140, 2010.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Ibrahim Pekkucuksen 和 Yucel Altunbasak. 基于梯度的无阈值颜色滤波器阵列插值. 在 2010 IEEE
    国际图像处理会议上, 页码 137–140, 2010.'
- en: '[82] R. Franzen. Kodak lossless true color image suite. [http://r0k.us/graphics/kodak/](http://r0k.us/graphics/kodak/).'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] R. Franzen. Kodak 无损真彩色图像套件. [http://r0k.us/graphics/kodak/](http://r0k.us/graphics/kodak/).'
- en: '[83] Lei Zhang, Xiaolin Wu, Antoni Buades, and Xin Li. Color demosaicking by
    local directional interpolation and nonlocal adaptive thresholding. Journal of
    Electronic Imaging, 20(2):1 – 17, 2011.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Lei Zhang, Xiaolin Wu, Antoni Buades, 和 Xin Li. 通过局部方向插值和非局部自适应阈值化进行颜色去马赛克.
    电子成像期刊, 20(2):1 – 17, 2011.'
- en: '[84] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution
    from transformed self-exemplars. In 2015 IEEE Conference on Computer Vision and
    Pattern Recognition (CVPR), pages 5197–5206, 2015.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Jia-Bin Huang, Abhishek Singh 和 Narendra Ahuja. 从变换的自我样本中进行单图像超分辨率. 在
    2015 IEEE 计算机视觉与模式识别会议 (CVPR) 上, 页码 5197–5206, 2015.'
- en: '[85] P. Fischer O. Ronneberger and T. Brox. U-net: Convolutional networks for
    biomedical image segmentation, 2015.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] P. Fischer O. Ronneberger 和 T. Brox. U-net: 用于生物医学图像分割的卷积网络, 2015.'
- en: '[86] Stefan Roth Tobias Plötz. Benchmarking denoising algorithms with real
    photographs, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Stefan Roth Tobias Plötz. 用真实照片对去噪算法进行基准测试, 2017.'
- en: '[87] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for
    real-time style transfer and super-resolution. In European conference on computer
    vision, pages 694–711. Springer, 2016.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Justin Johnson, Alexandre Alahi 和 Li Fei-Fei. 实时风格迁移和超分辨率的感知损失. 在欧洲计算机视觉会议上,
    页码 694–711. Springer, 2016.'
- en: '[88] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural
    similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference
    on Signals, Systems & Computers, 2003, volume 2, pages 1398–1402\. Ieee, 2003.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Zhou Wang, Eero P Simoncelli 和 Alan C Bovik. 用于图像质量评估的多尺度结构相似性. 在第三十七届
    Asilomar 信号、系统与计算会议上, 2003年, 第 2 卷, 页码 1398–1402. Ieee, 2003.'
- en: '[89] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image
    synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 2337–2346, 2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, 和 Jun-Yan Zhu. 使用空间自适应归一化的语义图像合成。载于
    IEEE/CVF 计算机视觉与模式识别大会论文集，第2337–2346页，2019年。'
- en: '[90] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc
    Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks.
    In Proceedings of the IEEE International Conference on Computer Vision, pages
    3277–3285, 2017.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, 和 Luc
    Van Gool. 使用深度卷积网络在移动设备上获得数码单反相机质量的照片。载于 IEEE 国际计算机视觉大会论文集，第3277–3285页，2017年。'
- en: '[91] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image
    translation with conditional adversarial networks. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 1125–1134, 2017.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, 和 Alexei A Efros. 基于条件对抗网络的图像到图像翻译。载于
    IEEE 计算机视觉与模式识别大会论文集，第1125–1134页，2017年。'
- en: '[92] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,
    Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and
    Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial
    network, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham,
    Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, 和
    Wenzhe Shi. 使用生成对抗网络的照片级单图像超分辨率，2017年。'
- en: '[93] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution
    using very deep convolutional networks. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1646–1654, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Jiwon Kim, Jung Kwon Lee, 和 Kyoung Mu Lee. 使用非常深的卷积网络进行准确的图像超分辨率。载于 IEEE
    计算机视觉与模式识别大会论文集，第1646–1654页，2016年。'
- en: '[94] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Image super-resolution
    using deep convolutional networks, 2015.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Chao Dong, Chen Change Loy, Kaiming He, 和 Xiaoou Tang. 使用深度卷积网络的图像超分辨率，2015年。'
- en: '[95] Run xu Tan, K. Zhang, Wangmeng Zuo, and Lei Zhang. Color image demosaicking
    via deep residual learning. 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Run xu Tan, K. Zhang, Wangmeng Zuo, 和 Lei Zhang. 通过深度残差学习进行彩色图像去马赛克。2017年。'
- en: '[96] Filippos Kokkinos and Stamatios Lefkimmiatis. Deep image demosaicking
    using a cascade of convolutional residual denoising networks. In Proceedings of
    the European Conference on Computer Vision (ECCV), pages 303–319, 2018.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Filippos Kokkinos 和 Stamatios Lefkimmiatis. 使用卷积残差去噪网络级联的深度图像去马赛克。载于 欧洲计算机视觉大会（ECCV）论文集，第303–319页，2018年。'
- en: '[97] Felix Heide, Markus Steinberger, Yun-Ta Tsai, Mushfiqur Rouf, Dawid Pająk,
    Dikpal Reddy, Orazio Gallo, Jing Liu, Wolfgang Heidrich, Karen Egiazarian, Jan
    Kautz, and Kari Pulli. Flexisp: A flexible camera image processing framework.
    ACM Trans. Graph., 33(6), November 2014.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Felix Heide, Markus Steinberger, Yun-Ta Tsai, Mushfiqur Rouf, Dawid Pająk,
    Dikpal Reddy, Orazio Gallo, Jing Liu, Wolfgang Heidrich, Karen Egiazarian, Jan
    Kautz, 和 Kari Pulli. Flexisp: 一个灵活的相机图像处理框架。ACM 计算机图形学与交互技术期刊，33(6)，2014年11月。'
- en: '[98] Jiajun Wu Donglai Wei William T. Freeman Tianfan Xue, Baian Chen. Video
    enhancement with task-oriented flow, 2019.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Jiajun Wu, Donglai Wei, William T. Freeman, Tianfan Xue, Baian Chen. 使用任务导向流的视频增强，2019年。'
- en: '[99] Andrey Ignatov, Radu Timofte, Zhilu Zhang, Ming Liu, Haolin Wang, Wangmeng
    Zuo, Jiawei Zhang, Ruimao Zhang, Zhanglin Peng, Sijie Ren, Linhui Dai, Xiaohong
    Liu, Chengqi Li, Jun Chen, Yuichi Ito, Bhavya Vasudeva, Puneesh Deora, Umapada
    Pal, Zhenyu Guo, Yu Zhu, Tian Liang, Chenghua Li, Cong Leng, Zhihong Pan, Baopu
    Li, Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, JaeHyun Baek, Magauiya Zhussip,
    Yeskendir Koishekenov, Hwechul Cho Ye, Xin Liu, Xueying Hu, Jun Jiang, Jinwei
    Gu, Kai Li, Pengliang Tan, and Bingxin Hou. Aim 2020 challenge on learned image
    signal processing pipeline. 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Andrey Ignatov, Radu Timofte, Zhilu Zhang, Ming Liu, Haolin Wang, Wangmeng
    Zuo, Jiawei Zhang, Ruimao Zhang, Zhanglin Peng, Sijie Ren, Linhui Dai, Xiaohong
    Liu, Chengqi Li, Jun Chen, Yuichi Ito, Bhavya Vasudeva, Puneesh Deora, Umapada
    Pal, Zhenyu Guo, Yu Zhu, Tian Liang, Chenghua Li, Cong Leng, Zhihong Pan, Baopu
    Li, Byung-Hoon Kim, Joonyoung Song, Jong Chul Ye, JaeHyun Baek, Magauiya Zhussip,
    Yeskendir Koishekenov, Hwechul Cho Ye, Xin Liu, Xueying Hu, Jun Jiang, Jinwei
    Gu, Kai Li, Pengliang Tan, 和 Bingxin Hou. Aim 2020 挑战关于学习的图像信号处理管道。2020年。'
- en: '[100] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz
    Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image
    restoration and enhancement, 2020.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz
    Khan, Ming-Hsuan Yang, 和 Ling Shao. 学习丰富特征以实现真实图像恢复和增强，2020年。'
- en: '[101] Menghan Xia, Xueting Liu, and Tien-Tsin Wong. Invertible grayscale. ACM
    Transactions on Graphics (TOG), 37(6):1–10, 2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Menghan Xia, Xueting Liu, 和 Tien-Tsin Wong. 可逆灰度。ACM计算机图形学学报（TOG），37(6):1–10，2018年。'
- en: '[102] Kwang-Hyun Uhm, Seung-Wook Kim, Seo-Won Ji, Sung-Jin Cho, Jun-Pyo Hong,
    and Sung-Jea Ko. W-net: Two-stage u-net with misaligned data for raw-to-rgb mapping.
    2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), Oct
    2019.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Kwang-Hyun Uhm, Seung-Wook Kim, Seo-Won Ji, Sung-Jin Cho, Jun-Pyo Hong,
    和 Sung-Jea Ko. W-net: 两阶段U-net与不对齐数据用于原始到RGB映射。2019 IEEE/CVF国际计算机视觉大会研讨会（ICCVW），2019年10月。'
- en: '[103] Ming-Chun Hsyu, Chih-Wei Liu, Chao-Hung Chen, Chao-Wei Chen, and Wen-Chia
    Tsai. Csanet: High speed channel spatial attention network for mobile isp. In
    2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
    (CVPRW), pages 2486–2493, 2021.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Ming-Chun Hsyu, Chih-Wei Liu, Chao-Hung Chen, Chao-Wei Chen, 和 Wen-Chia
    Tsai. Csanet: 高速通道空间注意网络用于移动ISP。发表于2021 IEEE/CVF计算机视觉与模式识别会议研讨会（CVPRW），第2486–2493页，2021年。'
- en: '[104] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional
    block attention module, 2018.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Sanghyun Woo, Jongchan Park, Joon-Young Lee, 和 In So Kweon. Cbam: 卷积块注意模块，2018年。'
- en: '[105] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns
    for optical flow using pyramid, warping, and cost volume, 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, 和 Jan Kautz. Pwc-net: 基于金字塔、变形和代价体积的光流CNN，2018年。'
- en: '[106] Xuaner Cecilia Zhang, Qifeng Chen, Ren Ng, and Vladlen Koltun. Zoom to
    learn, learn to zoom, 2019.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Xuaner Cecilia Zhang, Qifeng Chen, Ren Ng, 和 Vladlen Koltun. 放大以学习，学习以放大，2019年。'
- en: '[107] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver
    Wang. The unreasonable effectiveness of deep features as a perceptual metric,
    2018.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, 和 Oliver
    Wang. 深度特征作为感知度量的非凡有效性，2018年。'
- en: '[108] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change
    Loy, Yu Qiao, and Xiaoou Tang. Esrgan: Enhanced super-resolution generative adversarial
    networks, 2018.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen
    Change Loy, Yu Qiao, 和 Xiaoou Tang. Esrgan: 增强型超分辨率生成对抗网络，2018年。'
- en: '[109] Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, and Jie Zhou.
    Structure-preserving super resolution with gradient guidance, 2020.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, 和 Jie Zhou. 具有梯度引导的结构保持超分辨率，2020年。'
- en: '[110] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue
    Huang. Real-world super-resolution via kernel estimation and noise injection.
    2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
    (CVPRW), pages 1914–1923, 2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, 和 Feiyue Huang.
    通过核估计和噪声注入实现现实世界超分辨率。2020 IEEE/CVF计算机视觉与模式识别会议研讨会（CVPRW），第1914–1923页，2020年。'
- en: '[111] Samuel W. Hasinoff. Photon, Poisson Noise, pages 608–610. Springer US,
    Boston, MA, 2014.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Samuel W. Hasinoff. 光子，泊松噪声，第608–610页。Springer US，波士顿，MA，2014年。'
- en: '[112] Laurent Condat and Saleh Mosaddegh. Joint demosaicking and denoising
    by total variation minimization. In 2012 19th IEEE International Conference on
    Image Processing, pages 2781–2784, 2012.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Laurent Condat 和 Saleh Mosaddegh. 通过总变差最小化进行联合去马赛克和去噪。在2012年第19届IEEE国际图像处理会议上，第2781–2784页，2012年。'
- en: '[113] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database
    of human segmented natural images and its application to evaluating segmentation
    algorithms and measuring ecological statistics. volume 2, pages 416–423 vol.2,
    02 2001.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] David Martin, Charless Fowlkes, Doron Tal, 和 Jitendra Malik. 人体分割自然图像数据库及其在评估分割算法和测量生态统计数据中的应用。第2卷，第416–423页，2001年2月。'
- en: '[114] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge
    in a neural network, 2015.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 提炼神经网络中的知识，2015年。'
- en: '[115] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
    Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge, 2015.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh,
    Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander
    C. Berg, 和 Li Fei-Fei. ImageNet大规模视觉识别挑战，2015年。'
- en: '[116] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
    Chen. Mobilenetv2: Inverted residuals and linear bottlenecks, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, 和 Liang-Chieh
    Chen. Mobilenetv2: 倒置残差和线性瓶颈，2019年。'
- en: '[117] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond
    a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions
    on image processing, 26(7):3142–3155, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] 张凯、左望盟、陈云金、孟德宇和张磊。超越高斯去噪器：深度卷积神经网络的残差学习用于图像去噪。IEEE《图像处理汇刊》，26(7):3142–3155，2017年。'
- en: '[118] Nima Khademi Kalantari, Ravi Ramamoorthi, et al. Deep high dynamic range
    imaging of dynamic scenes. ACM Trans. Graph., 36(4):144–1, 2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] 尼玛·卡德米·卡兰塔里、拉维·拉马穆尔提等。动态场景的深度高动态范围成像。ACM《图形学汇刊》，36(4):144–1，2017年。'
- en: '[119] Jiawen Chen Dillon Sharlet Ren Ng Robert Carroll Ben Mildenhall, Jonathan
    T. Barron. Burst denoising with kernel prediction networks. 2018 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, jun 2018.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] 陈佳文、迪龙·夏勒特、任恩格、罗伯特·卡罗尔、本·米尔登霍尔、乔纳森·T·巴伦。基于核预测网络的爆破去噪。2018 IEEE/CVF计算机视觉与模式识别会议，2018年6月。'
- en: '[120] Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of
    rectified activations in convolutional network, 2015.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] 徐冰、王奈彦、陈天奇和李木。卷积网络中修正激活函数的实证评估，2015年。'
- en: '[121] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and
    Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international
    conference on computer vision, pages 764–773, 2017.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] 戴继峰、祁浩智、熊玉文、李毅、张国栋、胡涵和魏亦辰。可变形卷积网络。发表于《IEEE国际计算机视觉会议论文集》，第764–773页，2017年。'
- en: '[122] Ke Yu Chao Dong Chen Change Loy Xintao Wang, Kelvin C.K. Chan. Edvr:
    Video restoration with enhanced deformable convolutional networks, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] 余可、董超、陈成、罗伊·洛伊、王心涛、陈凯文。EDVR：具有增强可变形卷积网络的视频恢复，2019年。'
- en: '[123] Lei Liao Ronghe Chu Jingyu Yang Huanjing Yue, Cong Cao. Supervised raw
    video denoising with a benchmark dataset on dynamic scenes, 2020.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] 廖磊、楚荣赫、杨静宇、岳焕静、曹聪。基准数据集上的有监督原始视频去噪，动态场景，2020年。'
- en: '[124] Deqing Sun Ce Liu. On bayesian adaptive video super resolution. IEEE
    Transactions on Pattern Analysis and Machine Intelligence, 36(2), feb 2014.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] 孙德庆、刘策。贝叶斯自适应视频超分辨率。IEEE《模式分析与机器智能汇刊》，36(2)，2014年2月。'
- en: '[125] Daniel Khashabi, Sebastian Nowozin, Jeremy Jancsary, and Andrew W Fitzgibbon.
    Joint demosaicing and denoising via learned nonparametric random fields. IEEE
    Transactions on Image Processing, 23(12):4968–4981, 2014.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] 丹尼尔·哈沙比、塞巴斯蒂安·诺沃津、杰里米·詹萨里和安德鲁·W·菲茨吉本。通过学习的非参数随机场进行联合去马赛克与去噪。IEEE《图像处理汇刊》，23(12):4968–4981，2014年。'
- en: '[126] Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang
    Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality
    assessment models. IEEE Transactions on Image Processing, 26(2):1004–1016, 2016.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] 马克德、段睿芳、吴庆博、王周、邹红伟、李洪亮和张磊。滑铁卢探索数据库：图像质量评估模型的新挑战。IEEE《图像处理汇刊》，26(2):1004–1016，2016年。'
- en: '[127] Weishong Dong, Ming Yuan, Xin Li, and Guangming Shi. Joint demosaicing
    and denoising with perceptual optimization on a generative adversarial network.
    arXiv preprint arXiv:1802.04723, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] 董伟双、袁明、李鑫和施光明。基于生成对抗网络的联合去马赛克与去噪的感知优化。arXiv预印本 arXiv:1802.04723，2018年。'
- en: '[128] Michaël Gharbi, Jiawen Chen, Jonathan T Barron, Samuel W Hasinoff, and
    Frédo Durand. Deep bilateral learning for real-time image enhancement. ACM Transactions
    on Graphics (TOG), 36(4):1–12, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] 米歇尔·加比、陈佳文、乔纳森·T·巴伦、塞缪尔·W·哈西诺夫和弗雷多·杜兰。用于实时图像增强的深度双边学习。ACM《图形学汇刊》（TOG），36(4):1–12，2017年。'
- en: '[129] Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung-Yu Chuang. Deep
    photo enhancer: Unpaired learning for image enhancement from photographs with
    gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 6306–6314, 2018.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] 陈宇生、王宇青、柯曼新和庄永宇。深度照片增强器：基于GAN的无配对图像增强学习。发表于《IEEE计算机视觉与模式识别会议论文集》，第6306–6314页，2018年。'
- en: '[130] Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, and Stephen Lin. Exposure:
    A white-box photo post-processing framework. ACM Transactions on Graphics (TOG),
    37(2):1–17, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] 胡元明、何浩、徐晨曦、王宝源和斯蒂芬·林。曝光：一个白盒照片后处理框架。ACM《图形学汇刊》（TOG），37(2):1–17，2018年。'
- en: '[131] Jongchan Park, Joon-Young Lee, Donggeun Yoo, and In So Kweon. Distort-and-recover:
    Color enhancement using deep reinforcement learning. In Proceedings of the IEEE
    Conference on computer vision and pattern recognition, pages 5928–5936, 2018.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] 朴钟灿、李俊英、柳东根和权仁书。扭曲与恢复：使用深度强化学习进行色彩增强。发表于《IEEE计算机视觉与模式识别会议论文集》，第5928–5936页，2018年。'
- en: '[132] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng,
    and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 6849–6857, 2019.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] 王瑞兴，张青，傅志伟，沈晓勇，郑伟诗，贾佳雅。利用深度光照估计进行低曝光照片增强。在《IEEE/CVF计算机视觉与模式识别会议论文集》中，第6849–6857页，2019年。'
- en: '[133] Andrey D. Ignatov, Juncheng Li, Jiajie Zhang, Haoyu Wu, Jie Li, Rui Huang,
    Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita, Yuzhi Zhao, Lai-Man Po, Radu
    Timofte, Tiantian Zhang, Zongbang Liao, Xiang Shi, Yujia Zhang, Weifeng Ou, Pengfei
    Xian, Jingjing Xiong, Changxian Zhou, Wing Yin Yu, Yubin Yubin, Sung-Jea Ko, Bingxin
    Hou, Bumjun Park, Songhyun Yu, Sangmin Kim, Jechang Jeong, Seung wook Kim, Kwang-Hyun
    Uhm, Seo-Won Ji, Sung-Jin Cho, Jun-Pyo Hong, and Kangfu Mei. Aim 2019 challenge
    on raw to rgb mapping: Methods and results. 2019 IEEE/CVF International Conference
    on Computer Vision Workshop (ICCVW), pages 3584–3590, 2019.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] 安德烈·D·伊格纳托夫，李俊城，张佳杰，吴浩宇，李杰，黄锐，穆罕默德·哈里斯，格雷格·沙赫纳罗维奇，乌木诺里，赵宇之，包赖文，拉杜·蒂莫夫特，张天田，廖宗邦，施翔，张雨佳，欧伟峰，冯鹏飞，熊晶晶，周长贤，余永银，俞玉彬，金胜元，朴品俊，柳松贤，金尚敏，郑杰昌，金承旭，严光贤，赵成真，洪俊镐，梅康福。Aim
    2019挑战赛：从原始数据到RGB映射的方法与结果。2019年IEEE/CVF国际计算机视觉会议研讨会（ICCVW），第3584–3590页，2019年。'
- en: '[134] Aamir Mustafa, Aliaksei Mikhailiuk, Dan Andrei Iliescu, Varun Babbar,
    and Rafal K. Mantiuk. Training a task-specific image reconstruction loss, 2021.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] 阿米尔·穆斯塔法，阿列克谢·米哈伊柳克，丹·安德烈·伊列斯库，瓦伦·巴巴尔，拉法尔·K·曼提克。训练任务特定的图像重建损失，2021年。'
